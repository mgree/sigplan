{"article_publication_date": "05-01-2001", "fulltext": "\n Exact Analysis of the Cache Behavior of Nested Loops* Siddhartha Chatterjeet Erin Parkert Philip J. \nHanlon* Alvin R. Lebeck~ sc@cs.unc.edu parker@cs.unc.edu hanton @ math.lsa.umich.edu alvy@cs.duke.edu \ntDepartment of Computer Science *Department off Mathematics ~Department of Computer Science The University \nof North Carolina University of Michigan Duke University Chapel Hill, NC 27599-3175 Ann Arbor, MI 48109 \nDurham, NC 27708 ABSTRACT We develop from first principles an exact model of the behavior of loop nests \nexecuting in a memory hierarchy, by using a nontra- ditional classification of misses that has the key \nproperty of com- posability. We use Presburger formulas to express various kinds of misses as well as \nthe state of the cache at the end of the loop nest. We use existing tools to simplify these formulas \nand to count cache misses. The model is powerful enough to handle imperfect loop nests and various flavors \nof non-linear array layouts based on bit in- terleaving of array indices. We also indicate how to handle \nmodest levels of associativity, and how to perform limited symbolic analy- sis of cache behavior. The \ncomplexity of the formulas relates to the static structure of the loop nest rather than to its dynamic \ntrip count, allowing our model to gain efficiency in counting cache misses by exploiting repetitive patterns \nof cache behavior. Validation against cache simulation confirms the exactness of our formulation. Our \nmethod can serve as the basis for a static performance predictor to guide program and data transformations \nto improve performance. 1. INTRODUCTION The growing gap between processor cycle time and main mem- ory \naccess time makes efficient use of the memory hierarchy ever more important for performance-oriented \nprograms. Many compu- tations running on modern machines are often limited by the re- sponse of the memory \nsystem rather than by the speed of the pro- cessor. Caches are an architectural mechanism designed to \nbridge this speed gap, by satisfying the majority of memory accesses with low latency and at close to \nprocessor speed. However, programs *This work was supported in part by DARPA Grant DABT63-98- 1-0001, \nNSF Grants EIA-97-26370 and CDA-95-12356, NSF Ca- reer Award MIP-97-02547, The University of North Carolina \nat Chapel Hill, Duke University, and an equipment donation through Intel Corporation's Technology for \nEducation 2000 Program. Erin Parker is supported by a Lawrence Livermore Computer Science Graduate Fellowship. \nThe views and conclusions contained herein are those of the authors and should not be interpreted as \nrepresent- ing the official policies or endorsements, either expressed or im- plied, of DARPA or the \nU.S. Government. Permission to make digital or hard copies of all or part of this work for personal or \nclassroom use is granted without fee provided that copies are not made or distributed for profit or commercial \nadvan-tage and that copies bear this notice and the full citation on the first page. To copy otherwise, \nto republish, to post on servers or to redistribute to lists, requires prior specific permission and/or \na fee. PLDI 2001 6/01 Snowbird, Utah, USA &#38;#169; 2001 ACM ISBN 1-58113-414-2/01/06...$5.00 must \nexhibit good locality of reference in their memory access se- quences in order to realize the performance \nbenefit of caches. Optimizing compilers attempt to speed up programs by perform- ing semantics-preserving \ncode transformations. Loop transforma- tions such as iteration space tiling [62] are a major source of \nper- formance benefits. They restructure loop iterations in ways that make the memory reference sequence \nmore cache-friendly. The theory of loop transformations is well-developed in terms of decid- ing the \nlegality of a proposed transformation and generating code for the transformed loop. However, models of \nthe expected per- formance gains of performing a given loop transformation are less well-developed [19, \n38, 45, 48, 50, 51, 61]. Where such models exist, they are often heuristic or approximate. For example, \ntiling requires the choice of tile sizes, and the performance of a loop nest is typically a non-smooth \nfunction of the extents of the loop bounds, the tile sizes, and the cache parameters [13, 19, 38]. The \nmodel we develop in this paper can be used to quantitatively determine the number of cache misses of \na proposed transformation without ex- plicit simulation. Ultimately, such a model could be used to guide \nthe choice of parameters in such program transformations. A complementary method for improving sequential \nprogram per- formance that has been investigated in recent years is that of trans- forming the memory \nlayout of its data structures. Such data layout transformations can vary in complexity; examples include \ntrans- position and stride reordering [32], array merging [39], intra- and inter-array padding [50, 51], \ndata copying [38], and non-linear ar- ray layouts [14]. Once again, proper choice of parameter values \nis of paramount importance in getting good performance out of such transformations, but the models guiding \nthis optimization are of- ten inexact. For instance, Rivera and Tseng [50, 51] use heuristics to determine \ninter-array pad. However, there is empirical evidence that almost every choice of pad can be catastrophically \nbad for a program as simple as matrix transposition [16]. Better models are clearly needed to guide such \noptimizations, Our work in this paper is a step in this direction. An aggressive form of data optimization \nis the use of certain families of non-linear array layouts that are based on interleaving the bits in \nthe binary expansion of the row and column indices of arrays. Previous studies have demonstrated performance \ngains as well as robustness of performance resulting from the use of such layouts [14, 15]. Yet it is \ndifficult to ascertain, short of simulation, the memory behavior of a program given a particular data \nlayout. This paper works towards building an analytical model of cache be- havior for such layouts that \ncan provide insight into the relationship between such data layouts and memory behavior. Our model is \nan alternative to the well-known Cache Miss Equa- tions (CME) model of Ghosh et el. [26]. Compared to \nCME, our model has the following strengths and weaknesses. o Our model is e~act as a consequence of our \nuse of Presburger arithmetic as the underlying formalism. Ghosh et al. [26] use the abstraction of reuse \nvectors to simplify the analysis. Reuse vectors do not exist for all loop nests, and certainly do not \nexist in the presence of non-linear array layouts. Our model accurately determines the state of the cache \nat the end of executing a loop nest. This functionality is important for accurately counting compulsory \nmisses [30], for rapidly leap-frogging up to a certain point in the computation, and for handling multiple \nloop nests. Our model handles imperfect loop nests in addition to perfect loop nests. We apply a transformation \nof Ahmed et al. [2, 3] to an imperfect loop nest, thereby converting it to a perfect loop nest with guards \non statements. Ghosh et al. [26] con- sider only a single perfect loop nest. Our model handles a variety \nof array layout functions, from row- and column-major to non-linear. We will subsequently refer to row- \nand column-major layouts as canonical lay-outs [17]. The formulation for non-linear layouts is new, to \nthe best of our knowledge.  Our model handles caches with modest levels of associativ- ity in a natural \nway. While Ghosh et al. [25] can handle set-associative caches, their solution method is equivalent to \nsimulation in the worst case.  * Our model is capable of symbolic analysis. This is a direct consequence \nof our use of the Presburger formalism. For ex- ample, we can simplify a formula for the cross-interference \nbetween two arrays while keeping the difference of their start- ing addresses symbolic. The simplified \nformula can be rapidly evaluated for specific values of this variable. \u00ae The enhanced capabilities of \nour model come at the cost of computational complexity, in the form of super-exponential worst case behavior \nof aigofithms for satisfiability check- ing and quantifier elimination of Presburger formulas [60]. While \nwe have a prototype implementation of our model as a SUIF [55] pass, and the analysis and formula generation \nportions of the implementation are acceptably efficient, sig- nificant improvements are necessary to \nthe robustness and efficiency of the simplification and counting parts. Compared to explicit simulation, \nour formulas capture temporal patterns of cache behavior that may not be apparent in simulation. Moreover, \nan analytical cache model provides deeper insight into the behavior than what may be learned from simulation. \nWe antici- pate that such information will make it possible to guide the choice of data layouts that \noptimize cache behavior. We validate the re- suits of all our formulas against simulation in Section \n4, thereby confirming their exactness. The remainder of this paper is structured as follows. Section \n2 re- views background material for discussing our approach to the cache analysis problem: basics of \ncache memory (Section 2.1), a new classification of cache misses (Section 2.2), the polyhedral model \n(Section 2.3), and Presburger arithmetic (Section 2.4). Section 3 constructs our model. Section 4 provides \nsome preliminary results obtained using our cache analysis model. Section 5 discusses re- lated work. \nSection 6 presents conclusions and future work. 2. BACKGROUND This section provides background material \nand defines notation for the remainder of the paper. 2.1 Basics of memory hierarchies We assume a simplified \nmemory hierarchy that processes one memory access at a time, with no distinction between memory reads \nand writes. 2.I.I Cache structure The structure of a single level of a memory hierarchy--a cache--is \ngenerally characterized by three parameters [30]: Associativity, Block size, and Capacity. Capacity and \nblock size are in units of the minimum memory access size (usually one byte). A cache can hold a maximum \nof C bytes. However, due to physical constraints, the cache is divided into cache frames of size/3 that \ncontain B contigu- ous bytes of memory---called a memory block. The associativity A specifies the number \nof different frames in which a memory block can reside. If a block can reside in any frame (i.e., A = \n~), the cache is said to be fully associative; if A -~ 1, the cache is direct-mapped; otherwise, the \ncache is A-way set associative. A cache set is the group of frames in which a memory block can reside, \nand the number of cache sets, 5', is given by S = ~-~. o We assume a two-level memory hierarchy, consisting \nof an A- way set associative cache with block size of/3 bytes and total ca- pacity of C bytes followed \nby main memory. We also assume that main memory is large enough to hold all the data referenced by the \nprogram. The function/3 converts a memory byte address into a memory block address (with B(a) = [a//3J). \nThe function S converts a memory block address to the cache set to which it maps (thus, S(b) = b rood \nS). 2.1.2 Cache dynamics For an access to memory address m, the cache controller de- termines whether \nmemory block/3(m) is resident in any of the A cache frames in cache set S(13(m)). If the memory block \nis resi- dent, a cache hit is said to occur, and the cache satisfies the access after its access latency. \nIf the memory block is not resident, a cache miss is said to occur. The state of the cache represents \nthe memory block(s) contained in each set of the cache at any point during a program's execution. Thus, \nin a direct-mapped cache where each set holds one frame, the cache state C maps set s to the address \nof the memory block contained there. In general, C is a map from cache sets to the sets of memory blocks \nthat they contain. C(s) is empty for a cache set s to which no block has been mapped.  2.2 Classification \nof cache misses From an architectural standpoint, cache misses fall into one of three classes: compulsory, \ncapacity, and conflict [30]. Capacity and conflict misses are often combined and called replacement misses. \nThis classification is extremely useful for understanding the role of capacity and associativity in the \nperformance of a cache; however, it does not have the property of composability. Consider two program \nfragments P1 and P2, where, for i E {1, 2}, fragment P/ incurs Ci cold misses and R/ replacement def \n misses. Now consider the program fragment P12 = F1; P2 formed by sequential composition of P~ and P2, \nand suppose that it incurs C12 cold misses and R~ replacement misses. There is no simple relation connecting \nthe misses of the whole to the misses of the parts. In particular, C12 + R12 ~ C1 + Rt + C2 + R2. Composi- \ntion is a fundamental operation in the construction of programs and in the definition of programming \nlanguage semantics. As we wish to count cache misses for individual program fragments and their compositions, \nwe propose a different classification that is compos- able. We classify misses from a program fragment \ninto the following two classes. Interior misses are those data references that are guaranteed to miss, \nindependent of the initial cache state when the frag- ment begins execution. In other words, given the \ncode, the ar- ray layouts, and the structural parameters of the cache, such misses can be identified/enumerated/counted \nby analyzing the fragment in isolation. o Potential boundary misses are those data references that may \neither hit or miss, depending on the initial cache state when the fragment begins execution. The potential \noccurrence of such misses can be identified by analyzing the fragment in isolation, but the actual occurrence \nof the miss can be deter- mined only after considering the initial cache state. Another equivalent view \nof this classification is that we can stati- cally examine a program fragment in isolation and place \neach data memory access that it makes into one of three categories: those that are guaranteed to hit, \nthose that are guaranteed to miss (interior misses), and those that could hit or miss depending on the \ninitial cache state (potential boundary misses). In a second step, we fur- ther partition the potential \nboundary misses into hits and misses by resolving them against the cache state when the program fragment \nstarts executing. We call these misses boundary misses. It follows that, in order to compose program \nfragments, we also need to de- termine the state of the cache after executing a program fragment. For \na given program fragment P and an initial cache state S, we will let ff'(P, S) denote the final cache \nstate after fragment P has completed execution. Theorem 2.1 Let program fragment P1 executing from initial \ncache state Co incur I1 interior misses and B1 (Co) boundary misses and produce final cache state Ca \n= q2(P1, Co). Let program frag- ment P~ executing from initial cache state C1 incur I2 interior misses \nand B2(Ci ) boundary misses and produce final cache state C:z = ffl(P2, Cx ). Let program fragment P12 \n~f ]91; P2 executing from initial cache state Co incur I12 interior misses and B12 (Co) boundary misses \nand produce final cache state C12 = ~ ( P1 ~ , Co). Then the following relations hold. Ii~+B~2(CO) = \nI~+B~(CO)+h+B,(C~) Ci~ = C~ PROOF. The proof follows immediately from the semantics of program composition \nand from the deterministic nature of the pro- gram fragments and of the cache. [] Theorem 2.1 has several \nimportant consequences. . The theorem enables the analysis of cache misses of a com- posite program fragment \nin terms of the cache miss behavior of its parts. Each part can be analyzed in isolation, and the results \nof these analyses can be combined using cache states. We will show later how to efficiently propagate \ncache state across a program fragment. \u00ae Stronger assertions, like 112 = Ia +/2, do not hold in gen- \neral. o The theorem is silent about the nature of program fragments P1 and P2 or about how to calculate \nboundary and interior misses for them. In the remainder of the paper, we will choose loop nests as our \natomic program fragments and use Presburger formulas to codify the various kinds of misses. \u00ae The theorem \nprovides additional leverage if symbolic anal- ysis of the atomic program fragments is possible. For \nex- ample, block-recursive codes [4] employ multiple dynamic instances of the same loop nest differing \nonly in the starting addresses of the data arrays on which they operate. Symbolic analysis of such fragments \nwould allow the cost of analysis to be amortized over multiple uses of the resulting formulas. \u00ae Note \nthat boundary misses for a fragment are bounded from above by the cache footprint of the data structures \nit accesses, which is in turn bounded from above by the number of cache frames. This number is typically \nmuch smaller than the num- ber of interior misses. We could therefore avoid the calcu- lation of cache \nstate and approximate the number of cache misses of the composite program by It + I2, with an accom- \npanying error bound. 2.3 The polyhedral model Our model for analyzing cache behavior of loop nests is \nbased on the well-known polyhedral model [20]. The program fragment whose cache behavior we are trying \nto analyze is a nested normal- ized loop with d levels of nesting, numbered 0 through d -1 from outermost \nto innermost. We first consider perfect loop nests; we will extend the model to imperfect loop nests \nin Section 3.4. The upper bound Uj of ~j, the loop control variable (LCV) for loop j, is an affine function \nof the LCVs co through Lj-1. The iteration space Z is the set of all valid combinations of LCV values \nthat are within the bounds of the loop nest. The notation g = [go,..., ga-1] T de-notes a generic point \nin the iteration space Z. The iteration space possesses a total order ~, which in the polyhedral model \nis the lexicographic ordering. The order specifies the temporal order in which the iteration points in \nthe iteration space are executed. The loop accesses elements of arrays y(O) through y(m-1). Ar-ray variable \ny(1) has di dimensions, with nj being the extent of the array in the (j + 1) th dimension. The data index \nspace Di corre- sponding to array y(O is the Cartesian product [0, no -1] x .. x [0, n,~,_1 -1]. The \nstatements in the loop body make k references to array vari- ables. The ith reference Ha has three components: \nNi, the name of the array referenced (so that N~ = Y(~) for some j E [0, m -1]); Fi, the index expression \nof the reference, which identifies the co- ordinates of the array element accessed by this reference \nat itera- tion point e; and Sh, the statement that contains reference Ha. To include statement Sh in \nthe definition of reference Ha may seem excessive at this point, but it will be useful in Section 3.4 \nwhen we consider imperfect loop nests. The index expression Fi is con- strained to he an affine function \nof g in each of its components. Thus, F~ is a function from the iteration space Z to the data index space \n~DN~. Borrowing terminology from Ghosh et al. [26], we call a static instance of a memory read or write \na reference, and a dynamic in- stance of that read or write an access. A reference and an iteration point \nuniquely define an access. The total order -< on iterations almost induces a similar total order on accesses; \nhowever, two ac- cesses in the same iteration need to be ordered as well. We compose the total order \n-< on the iteration space and the order among refer- ences of an iteration to define a total order \"precedes\" \n(written <1) among accesses. Thus, access (Ri, u) precedes access (Rj, v) iff (u -i v) V (U = v A i < \nj). Several quantities are associated with array Y(i): a layout func- tion \u00a3i, which is a 1-1 map from \n\"Di into the memory address space Zo+; #i, the starting byte address of the array; and ill, the number \nof bytes per array element. Applying \u00a3~ to an element of the array Mathematical Object Representation \n An iteration point ith array reference R~ = (yU~, F~, Sh) Access made by .R/at \u00a3 (Ri, g) Array element \naccessed by/g/at g e~ = YU)[&#38;(e)] Byte address of e~ m, = m + C~(F,(e))5~ ' Block address of ml b~ \n=/3(mi) Cache set to which bi maps si = S(bi)  Table 1: Table of notation. produces an offset, and \nmultiplying the offset by/~ gives the byte offset from the starting address of the array in memory. Adding \nthis offset to #i then gives the byte address of the element. Putting all of this notation together, \nwe have the objects of inter- est and their mathematical representations shown in Table 1. Example 1 \nConsider the following loop nest for matrix multipli- cation, which we present in a stylized pseudo-code \nin an attempt to remain language-neutral. do i = O, n-I do j=O, n-i dok = O, n-i SO: C[i,j] = A[i,k]*B[k,j]+C[i,j] \nend end end This loop nest has depth d = 3. The LCVs are ~o = i, ~a = j, and t2 = k. The loop nest accesses \nthree arrays: y(o) = A, yO) = B, and Y(~) -~ C. Each array is two-dimensional, so that T~o = D1 = ~2 \n= [0, n -1] x [0, n -1]. There are four array references: Ro = A[i,k], R1 = B[k,j], R2 = C[i,j] (the \nread access), and Ra = G[i, j] (the write access). The in- ['001 dex expressions of the four references \nare No = 0 0 1 ' F~ = 0 1 0 ,andF~ =F a= 0 1 0 \" enccs are contained in statement So.   2.4 Presburger \narithmetic Presburger arithmetic [31] is the subset of first order logic cor- responding to the theory \nof integers with addition. Presburger for- mulas consist of affine constraints on integer variables, \nwhich can be either constraints of equality or inequality. The constraints are linked by the logical \noperators -~, A and V, and the quanfifiers V and B. It has been used to model various aspects of programming \nlanguages, as well as in other areas such as timing verification [6, 7]. We use Presburger formulas to \ndefine polytopos whose contents describe interesting events like cache misses. Presburger arithmetic \nis decidable; however, a quantifier elimi- nation decision procedure has a suporexponential upper bound \non performance. More precisely, the truth of a sentence of length n can be determined within 22~p~ time, \nfor some constant p > 1 [46]. The bound is tight [60]. Bounded quantifier elimination has worst-case \nupper and lower bounds of 0(22 ) [50]. The complex- ity is related to the number of alternating blocks \nof V and B quanti- tiers [52] as well as to the numerical values of the integer constants and their co-primality \nrelationships. We use the Omega library [34] to manipulate and simplify our Presburger formulas, and \nhave found its methods reasonably effi- cient for our applications. 3. THE CACHE ANALYSIS MODEL The problem \nof central interest to us is the following. Given a cache configuration as in Section 2.1, a loop nest \nL meeting the conditions of Section 2.3, the layout fimctions of the arrays accessed in L and an initial \ncache state gin: count the interior misses incurred by L\" \u00ae count the boundary misses incurred by L\" \n\u00ae find the cache state C.out after execution of L A simple strategy to accomplish all of these goals \nis through sim- ulation of the code. This is precisely what cache simulators [29, 40, 54, 56] do. The \nmain drawback of simulation is its slowness: it takes time proportional to the running time of the code, \nusually with a significant multiplicative factor (10 - 100 is typical). In the matrix multiplication \nkernel of Example 1, this time is O(na). Our goal is to develop much faster algorithms, whose existence \nis sug- gested by the regularity of the array access patterns and the limited number of cache sets to \nwhich they map. Section 3. I provides the basic Presburger formulas necessary to describe the cache events \nin Section 3.2. Section 3.3 discusses how we count cache misses, given such Presburger formulas. Section \n3.4 extends our model to analyze imperfect loop nests. Section 3.5 shows how to extend our formula for \ninterior misses to handle modest levels of associativity. Section 3.6 reviews array layouts based on \nbit interleaving, and provides the Presburger formulas to describe them. Section 3.7 discusses issues \nrelated to physically indexed caches. 3.1 Describing cache structure using Presbur- ger formulas We now \npresent the basic formulas that will be combined in Section 3.2 to describe cache events. The translations \nare mostly straightforward or well-known [18, 49]. 3.1.1 Valid iteration point The predicate g 6 Z describes \nthe fact that iteration point e = [go ..... gd-1] m belongs to the iteration space. d--1 e~z%f A o<~ \ne~ < u~ (1) i=0 3.1.2 Lexicographical ordering of accesses When considering all accesses that occur \nbefore access (.Rv, m), we include any access occurring at an iteration g, such that ~ m. To be complete, \nwe must also include any access made at it- eration m by a reference that occurs before R,,. The predicate \n(.Ru, e) <1 (R,,, m) describes the fact that the memory access made by reference _Ru at iteration g precedes \nthe memory access made by R,, at m. (R,,, g) <1 (&#38;,m) aedg e Z^ m eZA d-1 i--1 ( V (e, < ~, A A ej \n= m~) v /.=0 j=O d-1 (2) j=O ....... 289  3.1.3 Mapping memory locations to cache sets Let A = associativity, \nB = block size, C = capacity, and S = A~ = number of cache sets. Then memory location m maps to cache \nset s = [~J mod S. This can be translated to the following Presburger formula, where the auxiliary variable \nw represents the \"cache wraparound\". Suppose that Y(~) is the array referencing memory location m, and \nlet oL~ be the number of elements in Y(~). Map(m, w, s) de=f 0 ~< s < S A B(wS + s) ~< m < B(wS + s) \n+ B ^ #~ -13 < B(wS + s) < #, + \u00a2~,a, (3) The last clause in formula (3) bounds the possible values \nof w, and is used to bound certain directions of the underlying polytope that would otherwise be unconstrained. \nThis bounding is needed for efficiency in the counting step that follows formula simplifica- tion. The \nquantity B(wS + s) represents the address of the first byte in the block containing memory location m, \nwhich must be within the memory locations containing array Y(*). However, if the starting address #, \nis not aligned on a memory block bound- ary, asserting that #~ ~< B(wS + s) is wrong. As shown below, \nthe address of the first byte in the memory block containing Y(*)'s first element may actually be less \nthan #~. Restricting w such that #z -B < B(wS+s) is correct whether or not the starting address /z, is \naligned on a memory block boundary. B(~+s) m B(wS+s)+B 3.1.4 Data layouts in memory Row- and column-major \nlayouts are easily expressed using Pres- burger formulas. Consider reference R~ = (Y(*), F~, Sh) and \niteration point g. Let F~(g) = [io ..... id.-1] T. (m = Row-maj(F~(g),#,)) dee m /> 0 d~--2 d.--1 Am \n= #, + ( ~ ( H nk)ij + ia,-t)B, (4) j=o k=j+~ (m = da Col-maj(F~(g), #,)) =m/>0 d~-I j-1 ^m = ~ + (io \n+ ~ (1FI-~)~j)~ (5~ j~l k=0 Section 3.6 discusses nonlinear data layouts. 3.2 Describing cache behavior \nusing Presbur- ger formulas The various pieces described in Section 3.1 fit together to de- scribe events \nin the cache. We now construct Presburger formulas for interior misses, boundary misses, and cache state, \nas defined in Sections 2.1 and 2.2. We consider direct-mapped caches for now, and extend the formulation \nto set-associative caches in Section 3.5. 3.2.1 Interior misses To identify a cache miss, Ghosh et al. \n[26] rely on the notion of a most recent access of a memory block, which they obtain through reuse vectors. \nThis abstraction is valid when the array index ex- pressions are uniformly generated in addition to being \naffine in the LCVs. We avoid this condition by dispensing with the notion of a most recent access in \nour formulas. To determine if an access to a memory block b results in an in-. tefior miss, it is enough \nto know two things: that there is an earlier access to a dif[~rentmemory block mapping to the same cache \nset as b; and that there is no access to b between this earlier access and the current access to b. Let \nreference R~ = (Y(~), F~, Sp) at iteration point i access memory block b~, and let reference ~ = (y(u), \nF~, Sq) at iteration point j access memory block b,. Sup- pose that access (R~, j) precedes access (R~, \ni), recalling the \"pre- cedes\" relation from Section 3.1.2; that b~ and b~, are distinct memory blocks; \nbut that both b~ and by map to the same cache set s. Then, access (Ru, i) suffers an interior miss if \nthere does not exist a reference R~ = (Y(~), Fw, S~) at iteration k access- ing memory block bw, such \nthat (R~, j) ~ (R~, k) <~ (R~, i) and b~ = b~. The following formula expresses this condition. ((R,,, \ni) E IntMiss(L)) Hal = iEZA 34 s : Map(E.(F~(i)), a, s) A 3e, j, v: (R~, j) ,~ (R~,/) ^ Map(\u00a3~(F,,(j)), \ne, s) A -,(3k, ~,: (Ro, j) < (nw, k) < (R~, i) ^ Map(C, (F~ (k)), d, s)) A d # e (6) Note that it is \nnot necessary to have y(z) = y(~) in order to have (R~, i) and (P~,, k) access the same memory block. \nThis flexibility accommodates the possibility of array aliasing. 3.2.2 Boundary misses Recall that boundary \nmisses are those that are dependent on the initial cache state. Therefore, we are interested only in \nthose ac- cesses that are the first to map to a cache set during the execution of the loop nest. For \nall other accesses, the cache set already con- tains a memory block accessed during the execution of \nthe loop nest, and initial cache state is irrelevant. To determine an actual boundary miss for an access \nthat is the first to map to the cache set, it simply remains to check if the memory block accessed is \nresident in the initial cache state of the set. An access (.Ru = (Y(*), Fu, ST), i) to memory block b~ \nsuffers a boundary miss if there does not exist an access (R~,, j) preceding (Ru, i) and accessing a \nmemory block by mapping to the same cache set, and b~ is not in the initial cache state C4n at set s. \nNote that, unlike in the formula for interior misses, there is no constraint ((.P~, i) E BoundMiss(L, \nC4~ )) de=f i E 27 A 3d,~ : Map(E.(F,.(i)),d,~) ^ -(3e,j,v : (Rv,j) <3 (Ru,i) A Map(E u (F~, (j)), e, \ns)) A B(E~(F,,(i))) \u00a2 C.n (s) (7) 3.2.3 Cache state If the loop nest L contains no memory access mapping \nto set s, the final cache state of set s, Cost (s), is the same as the initial cache state Can (s). Otherwise, \nthe final cache state of set s is the address of the memory block that is not subsequently replaced by \nan access to a block of memory mapping to the same cache set s. (cow, = ~(L, ~,~)) %fvs e [0,s -1]: (3i: \ni e z^ (~d: Map(\u00a3a(Fu(i)), d, s) A -~(3e, j, v: (R~, i) <1 (R., j) A Map(\u00a3v(F~(j)), e, s)) A Cout (s) \n= B(/~(Fu(i))))) V (~(3e : Map(E, (F~,(i)), e, s)) ^ CCo~, (s) = CC~,~ (s)) (8) 3.3 Counting cache misses \nWe use the Omega Calculator [33, 34] to simplify the formulas above by manipulating integer tuple relations \nand sets. After sim- plification, we are left with formulas defining a union of polytopes (see Figure \n4 for an example). The number of integer points in this union is the number of misses. We use PolyLib \n[42] to operate on such unions. We first convert the union into a disjoint union of polytopes, and then \nuse Ehrhart polynomials to count the number of integer points [18] in each polytope. 3.4 Extension to \nimperfect loop nests Extending our model to imperfect loop nests involves two steps. 1. We use the transformations \nof Ahmed et al. [2, 3] to convert an imperfect loop nest into a perfect loop nest with guards on statements. \n 2. We extend the notion of a valid iteration point to that of a valid access.  For each statement \nof the loop nest, Ahmed et al. define a state-ment iteration space whose dimension is the number of loops \nthat contain the statement. The product space for the loop nest is a linearly independent subspace of \nthe the Cartesian product of all the statement iteration spaces. Affine embedding functions map a point \nin a statement iteration space to a point in the product space. When multiple statements map to the same \niteration point in prod- uct space, they are executed in program order. In relation to the product space, \nembeddings represent guards on statements, map- ping a statement from its place outside the innermost \nloop to a valid place inside the innermost loop. We emphasize that the guards are conceptual, and for \nanalysis only. They do not result in ran-time conditional tests in the generated code. Kelly and Pugh \n[35, 36] and Lim and Lain [41] have presented other algorithms that embed imperfect loop nests into perfect \nloop nests, with similar end results. The details of the embedding algo- rithms are not important for \nour purpose. Our use of the framework of Ahmed et al. merely reflects our greater familiarity with their \nwork. Figure l(a) is an improved version of Example 1, in which the loop-invariant reference C [ i, j \n] is hoisted out of the k-loop and stored in a scalar x that can be register-resident. In this imperfect \nloop nest, statements SO and s2 occur outside of the innermost loop. Let iX denote the loop index variable \ni pertaining to state- ment SX. Then i0 x j0 and i2 x j2 are the statement iteration spaces of statements \nSO and $2, respectively. The following embedding functions i01 1,21 F0( j0 )= J00 ' F2( j2 )= n j-2\" \n1 ' map points in these statement iteration spaces to points in product space [i,j,k] T. It is clear \nhow the guards on statements SO and $2 of Figure l(b) accomplish this. Statement 51 is already in the \ninnermost loop, and requires no guard on it. The second part of the extension is to insure that our model \ncan handle array references that are guarded in this manner. We accom- plish this effect by extending \nour notion of a valid iteration point (Section 3.1) to that of a valid access. Let R~ = (Y(~),F~,Sh) \nbe the u th reference with 0 ~< u < k. Let Gh (i) be the guard of statement Sh in the product space version \nof the loop nest. We assume that the guards are expressible in Presburger arithmetic. For Figure l(b), \nGo = (i2 = 0), G1 = true, and G2 = (i2 = n2 -1). Then (Ru = (Y(~), F~,, Sh), i)is a valid access if i \nbelongs to the iteration space, and Gh (i) holds. The predicate (R~, i) ~ Z represents this fact, (~,i) \ngI%eieI^0 ~< u <k^Oh(i) (9) With this extension, the formulas from Section 3.2 apply directly, with \nevery occurrence of i E Z replaced by (R~, i) ~ Z. 3.5 Associativity We currently handle associativity \nin a straightforward manner, assuming a Least Recently Used replacement policy. From Sec- tion 3.2.1, \nwe simply need to allow at least A distinct accesses preceding (Ru, i) to unique memory blocks, such \nthat there is no access (Rw, k) accessing the same memory block as (./~, i) and (R,,o ,it) <1 (Rw, k) \n<1 (R~,i) (where (R,.,o, jo ) is the earliest of at least A references to unique memory blocks). The \nfollowing Presburger formula expresses interior misses for an A-way set- associative cache. ((Ru,i) C \nIntMiss) d=ef i E Z A 3d, s : Map(gg(F,,(i)),d,s) A 3eo,jo, vo: (Rvo,jO) <1 (Ru, i) A Map(L:yo (F,o (jo \n) ), co, s) A (3el,...,eA-1 : A-1 A (3jo,vo : (n~o, it) < (Rot, it) <1 (P~, i) ^ Map(Z:u, (F, a (ja)), \nca, s)) ^ d ~ eo T~ ... ~ eA-1) A -~(3k, w : (go,J0) <1 (nw, k) <1 (Re,i) A Map(l:z(Fw(k)),d,s)) (10) \n This method will handle modest values of A, and the complexity of the formulas certainly increases with \nA. Presburger formulas for cache state and boundary misses with associativity A are non- obvious, and \nwill require more work to construct. 3.6 Array layouts based on bit interleaving Previous work [14, 15, \n21] suggests that nonqinear data layouts provide better cache performance than canonical layout functions \nin some numerical codes. Such layout functions are described in terms of interleavings of the bits in \nthe binary expansions of the array coordinates rather than as affiue functions of the numerical values \nof these quantities. We describe such bit interleavings and provide formulations of these layouts in \nPresburger arithmetic. In developing the model of alternative array layouts, we assume that nj = 2 qj \nfor some j E [0, d= -1] (where d~ is the number of coordinates in an array Y(~)). Therefore, the bit \nrepresentation of an array index will have q~ bits, with the least significant bit (LSB) numbered 0 and \nthe most significant bit (MSB) numbered qj -I. We identify the binary sequence sq- 1 ... so with the \nnon- 291 do i : 0, n-i do i : 0, do j : 0, n-I do j : S0: x = C[i,j] do k do k = 0, n-I SO: if Si: x \n= i[i,k]*B[k,j] + x Si: x = end $2: if $2: C[i,j] = x end end end end end n-i 0, n-i = 0, n-I (k == \n0) x = C[i,j] h[i,k]*B[k,j] + x (k :: n-l) C[i,j] : x  Figure 1: (a) An imperfect loop nest for matrix \nmultiplication. (b) The product space version with guards. negative integer s = x\"qJ-~ s~2 ~. We denote \nby z..,~=o Bqj the set ofaU binary sequences of length qj, and extend the above identification to identify \nBq~ with the interval [0, 2 q~ - 1]. We describe a fantily of nonlinear layout functions parameter- ized \nby a single parameter a, as follows. An (qo,... ,qa,-l)- ~d~-i interleaving, or. is a sequence of length \np (where p = z_,i=o qD over the alphabet {0,..., (d, -1)} containing qi i's. It describes the order in \nwhich bits from the d, array coordinates are interleaved to linearize the array in memory. An array layout \nfunctions as a map from d~ array coordinates to a memory address. Therefore, given an (qo,  , qd,- 1)-interleaving \na, define a map 0 : Bqo X... X Bqd \u00ae_l \"-+ Bp in the following way. If x (0 = x (0 (0Xo(1) Bq;Vi E q~-l..'xl \nE [0, d~ - 1], then O(x(\u00b0),..., x (d'-l)) is the sequence obtained by replacing the jth u from the right \nwith x~ ~). We extend this nota- tion to consider O as a map from [0, 2 q\u00b0 -1] \u00d7... x [0, 2 qa~-~ -1] \nto [0, 2 p -1] by identifying non-negative integers and their bi- nary expansions. We call O the mixing \nfunction indexed by a. Note that O(0,..., 0) = 0 for any a. Example2 Let d~ = 2, no = 16 (qo = 4), nl \n= 16 (ql = 4), and a = 10110010. Then O(12, 5) = O(1100, 0101) = 01101010 = 106. Example3 Let d~ = 3, \nno = 8 (qo = 3), nl = 8 (ql = 3), n2 = 4 (q2 = 2), and let cr = 21102001. Then 0(3, 7, 1) = e(011,111, \n001) = 01101111 = 111. The idea behind translating such a data layout into a Presburger formula is to \ndefine the bit values of the binary expansion of the memory address using Presburger arithmetic. Consider \nagain ref- erence R~ = (Y(~), F~, Sh) and iteration point g. For every nj where 0 ~< j < d~, let nj = \n2 q~. Then a is an (qo,..., qd~-l)\" interleaving. Then we can compute the following dx x p matrix Z(a). \nLetting g = try, the fth column of Z(o') consists of 2 e in the gth position, where (r f is the eth g \nfrom the right, and zeros in every other position. Z(a) can be thought of as a transformation that when \napplied to the binary expansion of a memory address m, produces the coordinates of the array element \nat m. Example 4 Given that d~ = 3, no = 8 (qo = 3), nl = 8 (ql = 3), n2 = 4 (q2 = 2), and a = 12102010, \n[00040201] Z(a)= 4 0 2 0 0 0 1 0 0 2 0 0 1 0 0 0 The following formula maps F~ (g), #~, and Z (~r) to \nmemory location m. Letp = ~:~1 qJ, and let M = [rap-l,.. , too] T. (m = lnterleave(F~, (g), #~, X(a))) \nde=Z 3TD.o f f ~ Tl%p-- l ~ . . . ,too : 0~mp-1,...,mo ~ 1Am>/0A m = #~ + moyf/3~ A F~(g) = Z(a)MA p-1 \nmoff= ~_~ mk 2 k (11) k=0 Data layouts such as X-Morton and U-Morton [15] require an X-OR operation \nin addition to bit interleaving. (Note that this for- malism applies only to n x n arrays.) The additional \nX-OR op- eration can also be expressed as a Presburger formula on the bit representation. 3.7 Physically \naddressed caches The techniques described thus far operate on virtual addresses. However, many systems \nutilize physical indexed caches (e.g., sec-ond level caches) whose performance is highly dependent on \npage placement. Fortunately, most operating systems employ page col- oring techniques that minimize this \neffect [37] by creating virtual to physical page mappings such that the virtual and physical cache index \nare identical. It may also be possible to extend our analysis to include the effects of page placement; \nwe leave this as future research. 4. RESULTS In this section, we present and interpret cache behavior \nas ob- tained by our method on five model problems, and validate them against cache miss counts produced \nby a (specially-written) cache simulator. Unless otherwise specified, we use a direct-mapped cache with \ncapacity 4096 bytes and block size of 32 bytes that is initially empty. We assume that all data arrays \ncontain double- precision numbers (so that 13 is eight bytes), and that all arrays are linearized in \ncolumn-major order. The total number of misses for each array match up exactly between our model and \nthe simulator in all cases, but their partitioning differs. We explain the implica- tions of this difference \nin Section 4.1. Problem 1 (Matrix multiplication) We count boundary and in- terior misses for each array \nfor the matrix multiplication kernel shown in Example 1, under four scenarios. 1. Problem size n = 21, \nthe leading dimension of each array is n, and the three arrays are adjacent to each other in memory address \nspace (i.e., tga = 0, pB = /3n 2, and Pc = 2/3n2) We show results for all six possible permutations \nof the loop orders, from both our approach and from explicit cache sim- ulation. This is representative \nof a code where both the it- eration space and the data arrays are tiled. Placing the ar- rays back-to-back \ncauses two memory blocks to be shared between arrays. Figure 2(a) tabulates the results. The j ki loop \norder is seen to be substantially superior in terms of total misses. 2. Problem size n = 20, the leading \ndimension of each array is n, and the three arrays are adjacent to each other in mem- ory address space. \nWe show results for all six possible per- mutations of the loop orders, from both our approach and from \nexplicit cache simulation. This scenario is similar to the previous one, but there is no sharing of memory \nblocks between arrays. Figure 2(b) tabulates the results. The num- ber of misses is somewhat smaller, \nand the jk\u00a3 loop order wins again. 3. Problem size n = 21, the leading dimension of each array is n, \nand the three arrays collide in cache space (i.e., tza = O, #B = 4096, and #c = 8192). This represents \na situation where the arrays do not use the cache effectively (occupying only 111 of the 128 cache sets). \nWe show results for all six possible permutations of the loop orders, from both our ap- proach and from \nexplicit cache simulation. Figure 2(c) tab- ulates the results. The number of misses rises dramatically, \nas expected; the j ki loop order produces the fewest cache misses, but not by as large a margin. 4. \nProblem size n = 20, the leading dimension of each array is kn (for k 6 {1,2,3}), and the three arrays \nare adjacent to each other in memory address space. This represents a situation where the iteration space \nis tiled but the data is not reorganized, resulting in the data tiles not being contiguous in memory \nspace. We show only the ijk loop order. Fig- ure 2(d) tabulates the results. The total number of misses \nfor each array change with the leading dimension, although different arrays behave differently.  Problem \n2 (Multiple loop nests) We count boundary and interior misses for each array for the following variation \non the matrix mul- tiplication kemel. do i = 0, n-i /* Loop nest 1 */ do j = 0, n-i C[i,j] = 0 end end \ndo i = 0, n-I /* Loop nest 2 */ do j = 0, n-I do k = 0, n-i C[i,j] = A[i,k]*B[k,j] + c[i,j] end end \nend The layout constraints are identical to those in Problem 1, sce- nario 1. This demonstrates how \nthe model handles multiple loop nests. The miss counts are as follows. Loop Bnd Int ] Tot Bnd Int [ Tot \nBnd Int [ Tot [ 1 O 0 [ 0 O 0 [ 0 111 0 , 111 ] 2 28 521 549 92 866 958 0 383 [ 383 [ The model correctly \nclassifies all the misses in the first loop nest as boundary misses. The cache contains all of array \nC at the end of the first loop nest, so all of the misses of C in the second loop nest are interior misses. \nFigure 3 graphically represents cache state at the end of the computation. Problem 3 (Imperfect loop \nnest) We count boundary and interior misses for each array for the imperfect loop version of the matrix \nmultiplication kernel of Figure 1 with n = 21, with the leading dimension of each array being n. This \ndemonstrates how the model handles imperfect loop nests. We show two scenarios. The first scenario has \nthe three arrays adjacent to each other in memory address space. The miss counts are as follows. [ l] \nA B C(read) C(write) Bnd 28 92 8 0 Int 521 866 383 0 Total 549 958 391 0 Cold 110 110 111 0 Repl 439 \n848 280 0 The significant observation is that none of the write references to C miss, even though there \nare many references to A and B between the read and the write reference to c [ i, 5 ]- The total number \nof misses is identical to that of Problem 1, scenario 1. The second scenario has the arrays colliding \nin the cache. The miss counts are as follows. I1 A B C (read) C (write) Bnd 20 90 1 0 Int 980 648 440 \n441 Total 1000 738 441 441 Cold 111 111 111 0 Repl 889 627 330 441 Now every read and write reference \nto C [ i, 5 ] misses. However, the total number of cache misses is significantly smaller than the corresponding \ncase in Problem 1, scenario 3, showing the benefit of allocating c [ 2, j ] in a register. Problem 4 \n(Set-associatlve cache) We count interior misses for each array for the matrix multiplication kernel \nshown in Example 1, using two-way associative caches. The layout constraints are iden- tical to those \nin Problem 1, scenario 2. This demonstrates how the model handles associativity. Both scenarios consider \na two-way associative cache with block size of 32 bytes that is initally empty. The cache has a capacity \nof 4096 bytes in the first scenario and 8192 bytes in the second scenario. The miss counts are as follows. \nII C = 4096 C = 8192 A B C A B C Bnd Int Total 75 128 773 1189 213 8 256 0 300 36 Cold Repl 100 0 100 \n757 100 132 100 0 100 0 100 0 The total number of boundary misses in each scenario is deter- mined by \nthe number of cache frames in the footprint of all three arrays in cache. For every cache frame that \nis touched during the matrix multiplication kernel, the first instance of a memory block being mapped \nto the cache frame incurs a boundary miss since the cache is initially empty. In the first scenario, \nthere are 64 cache Bnd Int Tot Cold Repl Int ] Tot [ Cold Repl d Repl ~k 28 521 549 110 439 92 866 t \n958 110 848 383L2 L Jlll (a) i~ jik 18 108 445 590 463 698 110 110 353 588 85 .... i8 1985 [ 2070 502 \n520 110 110 1960 410 25 1563 15~ ]H 14~]L_~_~_~ jki 104 355 459 110 349 18 167 185 110 75 -6 207 I 213] \n]Ff ~ \" ~ k~ 2 184 186 110 76 34 1644 1678 110 1568 92 ~ ]1716__~ 11i 1605 ~___:~_58Qj ~i 9 297 306 110 \n196 31 436 467 I 110 357 88 530 t 618 I 1~ 5~~ Loop [ A B c order Bnd Int Tot Cold Repl Bnd Int Tot t \nCold Repl Bnd int I abtlCold Repi I] Total Uk 25 405 430 100 330 85 661 746 '100 646 18 298 316 100 216j \n149-2 (b) i~ jik 23 97 349 409 372 506 100 i00 272 406 73 28 1533 345 1606, 373 ] 100 100 1506 273 32 \n3 1205 ' 1237 97 100 100 100 1137 0, 3215 979 jki 95 261 356 100 256 28 131 159 ~ 100 59 5 160 165 100 \n65 i 680 k~ 13 146 159 100 59 33 1276 1309 100 1209 82 1254 1336 100 1236 2804 ~i 16 220 236 100 136 \n31 352 383 100 283 81 404 485 100 385 II 1104 Loop order Bnd Int A Tot Cold Repl Bnd Int B Tot Cold [I \nRepl I1 Bnd Int C Tot Cold Repl Grand l Total I ~k 21 964 985 111 874 90 1799 1889 111 1778 0 2393 2393 \n111 2282 5267 (c) ikj jik 1 107 864 578 865 685 111 111 754 574 110 4 1846 1900 1956 1904 111 111 1845 \n1793 0 0 2556 2123 2556 2123 111 111 2445 2012 '5377 4712 jki 111 558 669 111 558 0 1789 1789 111 1678 \n0 2232 2232 111 2121 4690 kij 5 545 550 111 439 20 1866 1886 Ill 1775 86 2299 2385 111 2274 4821 ~i 6 \n577 583 111 472 5 1823 1828 111 1717 ! 100 2229 2329 111 22'18 4740 i A l B C [Grandk Bnd Int I Tot \nI Cold Repl Bnd Int I T\u00b0tlC\u00b0ld Repl Bnd Int Tot Cold Repl Total (d) 1 25 405 430 100 330 '85 661 746 \n100 646 18 298 316 100 216 1492 2 40 305 345 100 245 71 1198 1269 100 1169 17 227 244. 100 144 3350 3 \n40 4.49 489 100 389 68 1119 1187 100 1087 20 311 331 100 231 5357 ! Figure 2: Miss counts from our approach \n(Bnd and Int) and from cache simulation (Cold and Repl). (a) Problem 1, scenario 1. (b) Problem 1, scenario \n2. (c) Problem 1, scenario 3. (d) Problem 1, scenario 4. - Array A Array B Array C Figure 3: Cache state \nat the end of the computation described in Problem 2. The shaded blocks are cache-resident. There are \nexactly 128 shaded memory blocks. Arrays A and B share a block, as do arrays B and C. The block with \nthe heavy outline in each array maps to cache set 0. H~[o~am ol R~n~ 9 T~ sets. Since the cache footprint \nof all arrays 'wraps around' at least twice, we know that all 128 cache frames are touched. Hence, there \nare 128 boundary misses in scenario 1. Similarly, we can determine that there are 256 boundary misses \nin scenario 2. Problem 5 (Symbolic analysis) We analyze the matrix-vector prod- uct example from Fficker \net al. [22] to show the symbolic process- ing capabilities of our approach. The code is as follows. do \njl = 0, N-I reg = Y[jl] do j2 = 0, N-I reg += A[j2, jl] * X[j2] end Y[jl] = reg end We focus on \nthe interior misses on x due to interference from A, assuming that ~A : 0 but that #x is symbolic. For \ncompatibility with Fricker et al., we use a direct-mapped cache of capacity 8192 bytes and block size \nof 32 bytes, and we choose N = 100. The formula shown in Figure 4 is a pretty-printed version of formula \n(6) as simplified by the Omega Calculator. While the formula ap- pears formidable, it should be kept \nin mind that it captures the miss patterns for all possible values of #x. In principle, the Ehrhart polynomial \nof the polytope union represented by this formula can be computed, enabling counting of the number of \nmisses for a par- ticular value of#x by evaluating this polynomial. 4.1 Interpretation off results Two \ngeneral observations on the results are worth mentioning. First, the model results are identical to the \nsimulation results in all cases. This reinforces the exactness of the model, which is a major strength. \nSecond, the classification of cache misses into boundary and in- terior misses rather than cold and replacement \nmisses is a signif- icant departure from previous models. Boundary misses are, in a sense, a cache-centric \nanalog of cold misses. Just as the number of cold misses of a program is no more than the number of mem- \nory blocks occupied by the data, the number of boundary misses is no more than the combined cache footprint \nof the data, which is itself bounded above by the number of cache sets. This can be verified by totaling \nthe boundary or cold misses in any row of Fig- ure 2(a) (where the totals are 128 and 300, respectively) \nor Fig- ure 2(b) (where the totals are 111 and 333, respectively). In gen- eral, then, the number of \nboundary misses should be significantly smaller than the number of cold misses. Thus, our classification \nal- lows for more precise context-free identification of misses, leaving many fewer references to be \nresolved from cache state. 4.2 Running times Figure 5 shows a histogram of the running times required \nby the Omega Calculator [33] to simplify 108 sample cache miss formulas on a 300MHz Sparc Ultra 60. The \nsamples are made up of bound- ary and interior miss formulas for each array in scenarios 1, 2, and 3 \nof Problem 1. The cache miss formulas are generated from source code using a SUIF [55] compiler pass \nthat we developed for this purpose. The time required for formula generation is negligible. Half of the \ncache miss formulas run in less than 10 seconds, and the majority of those formulas run in less than \n1 second. The boundary miss formulas simplified quickly (most in under a sec- ond), while the time required \nto simplify the interior miss formulas varied widely. We have observed the running time of a formula \nto be strongly correlated to the number of cache misses it generates. Given these running times, our \napproach clearly does not yet have enough performance to be practical; however, the value of in- m 4S \n4O ~S i 2O Io s o o-1 $ 2-1o$ 11-305 31-60s 61*t20s t21,1~Oa 151-24.0~ >241B Figure 5: Histogram of \nrunning times of formula simplification using the Omega calculator on a 300 MHz Spare Ultra 60. sight \ngained from our approach should not be overlooked. Further- more, it is not clear how much of the slow \nrunning time is a con- sequence of our formulations and how much is due to the Omega software. We hope \nto make this determination in the immediate future, by investigating other software options. 5. RELATED \nWORK We organize related work by the way in which they handle cache behavior: compiler-centric, language-centric, \narchitecture-centfic, or trace-centfic (including simulation). Compiler-centric. The work of Ghosh et \nal. [23, 24, 25, 26] is most closely related to our framework for analytical cache model- ing. (Zhang \nand Martonosi [64] have recently begun extending this work to pointer data structures.) They introduce \nadditional con- straints to make the problem tractable. We avoid these constraints. Work by Ahrned et \nal. on tiling imperfect loop nests [2, 3] em- beds the iteration space of each statement of a loop into \na product space. We use this transformation in Section 3.4. Ca~caval [12] estimates the number of cache \nmisses using stack distances. His prediction is valid for a fully-associative cache with LRU replacement, \nand requires a probabilistic argument to transfer to a cache with smaller associativity. He assumes that \neach loop nest starts with a cold cache, sacrificing the accuracy gained from knowing the actual cache \nstate at the start of the loop nest. Our simplified formulas resemble LMADs [47] in several re- spects. \nEstablishing a connection between them remains the subject of future work. Language-centric. Alt et al. \n[5] apply Abstract Interpretation to predicting the behavior of instmction cache, for general programs. \nTheir notion of cache state is somewhat different from ours. Prior empirical evidence [14, 15, 21] suggests \nthat alternative array layout functions provide better cache behavior than canoni- cal layout functions \nfor many dense linear algebra codes. Previous work [27] has taken a combinatorial approach to modeling \ncache misses in the presence of such non-linear data layouts. Architecture-centric. Lain, Rothberg, and \nWolf [38] discuss the importance of cache optimizations for blocked algorithms, using matrix multiplication \nas an example. Their simulation-based anal- ysis is exact, but their performance models are approximate. \nFricker et al. [22] develop a model for approximating cache in- terferences incurred while executing \nblocked matrix vector multi- ply in a specific cache. Their analysis is inexact in considering only cross-interferences \nand neglecting redundancies among array pairs. McKinley and Temam [44] examine locality characteristics \nin the SPEC'95 and Perfect Benchmarks. Their discovery most perti- 295 {[jl,0, s,d] : 3(c~ : 1 .~ jl \n-~ 99 A0 ~< s ~< 255 A a < dA 32s + 8192d.~ #x A 800jl + 8192d ~ 792 + #x + 8192a A #x ~ 31 + 32s + 8192d \nA s + 256c~ < 25jl)} t_J{[jl,j2, s,d]:S(a:l~<j~ ~<99A0<~s~<255Al<~j2A925+100jl+j2 ~<1024d+4sA8192d+32s~#x+8j~A \n#x + 8j: ~< 7 + 8192d + 32s A 100jl + j2 ~< 99 + 4s + 1024a A ~ + 256c~ < 25jt)} U{[j~,jz,s,d]:3(a:0 \n~<j~ g. 99A0~<s~<255Aj2 ~.99A25j~ ~<s+256aA8192d+32s~<#x+8j2A #x + 8j2 ~< 7 + 8192d + 32s A 4s + 1024a \n< 100j~ +j2 A 256 + 25jl ~< 256d+ s)} U{[j~,j2,s,d] : 3(a : O ~< j~ ~ 99 A O <~ j: ~<99A0~<s~<255A8192d+32s<~#x+Sj2A#x+8j2 \n<~31+8192d+32sA 1021 + 100j~ + j2 ~< 1024d + 4s A 100j~ + j2 ~< 3 + 4s + 1024a A 4s + 10240 ~ 100j~ + \nj~)} U{[j~,0, s,d]:S(a:l<~j~ ~<99A0~<s~<255A257+s+255d~<25j~A32s+8192d<~#xA 800j~ + 8192d ~ 792 + #x \n+ 8192a A#x ~< 31 + 32s + 8192dA s + 256a < 25j~)} tJ{[j~,j2, s,d]:S(a:l~<j~ g. 99A0~<s~<255A1.~.jeA257+256d+s~25j~A8192d+32s~<#x+Sj2A \n,ux + 8j~ ~< 7 + 8192d + 32s A 100j~ + j: ~< 99 + 4s + 1024a A s + 256c~ < 25j~)} U{[j~,j2,s,d]:~(a:0<~j~ \n~<99A0~<s~<255Aj2 ~<99A25j~ ~s+256aA8192d+32s~#x+8j2A /zx + 8j~ ~< 7 + 8192d + 32s A 4s + 1024a < 100j~ \n+ j2 A 1025 + 1024d + 4s ~ 100j~ + j2)} U{[j~,j2,s,d]:2(a:0~<j~ ~<99A0<~jz <~99A0~<s<~255A8192d+32s.~ttx+8j~A~x+8jz.~.31+8192d+32sA \n1024 + 1024d + 4s ~< 100j~ + j~ A 100j~ + jz ~< 3 + 4s + 1024a A 4s + 1024a .g. 100j~ + j:)} Figure 4: \nA formula describing interior misses on X due to interference from A in Problem 5. Each 4-tuple is of \nthe form [jl, j2, s, d], where (jl, j~) identifies the iteration at which the miss occurs, and (s, d) \nidentifies the set and the cache wraparound of the reference, nent to our work is that most misses are \ninternest capacity misses. Harper et al. [28] present an analytical model that focuses on set-associative \ncaches. Their model approximates the cache miss- ratio of a looping construct and allows imperfect loop \nnests to be considered. They do not attempt to analyze multiple loop nests. Trace.centric, Prior research \n[1, 57] has investigated various analytic cache models by extracting parameters from the reference trace. \nSimulation techniques, such as cache profiling [43, 39], can provide insight on potential program transformations \nby classifying misses according the cause of the cache miss. All trace-centfic approaches usually require \nfull execution of the program. Weikle et al. [58, 59] introduce the novel idea of viewing caches asfilters. \nThis framework is not limited to analyzing loop nests or other particular program constructs, but can \nhandle any pattern of memory references. Brehob and Enbody [11] model locality using distances between \nmemory references in a trace. Wood et al. [63] explore the problem of resolving unknown ref- erences \nin simulation--first-time references to memory blocks that may miss or hit depending on the cache state \nat the beginning of the trace sample--and show that accurate estimation of their miss rate is necessary. \nWe use cache state to resolve such unknown ref- erences, and then categorize them as boundary misses \nor hits. 6. CONCLUSIONS AND FUTURE WORK This work initially began from the intuition that the CME formu- \nlation of Ghosh et al. was not fully exploiting all of the regularity inherent in the problem. The output \nof the Presburger formulas vividly illustrates this regularity, allowing us to employ general- purpose \ntools for counting misses. While powerful mathematical results (such as the existence of the Ehrhart \npolynomial) are known for polytopes, the correspond- ing algorithms are complex and subject to geometric \ndegeneracies. As a result, the software libraries are not very robust. Such de- generacies have prevented \nus, for example, from calculating the Ehrhart polynomial for the formula in Figure 4. Similar comments \napply, with less severity, to the Presburger decision procedures. The robustness of both libraries needs \nto be improved substantially to realize the full potential of our approach. While we have made some progress \nin handling associativity, symbolic constants, and non-linear array layouts, much remains to be done \non all three fronts. Our current handling of associativity is incomplete and unscalable; in particular, \nit is not powerful enough to model TLB behavior. Our ability to handle symbolic constants derives from, \nand is therefore limited by, the corresponding capa- bility in Omega. The constraints introduced in Section \n3.6 to han- dle non-linear data layouts are essentially 0-1 integer programming constraints, which are \nlikely to cause bad behavior in the Presbur- ger decision procedures. We have recently become aware of \nan alternative tool [10] that claims to be more aggressive at formula simplification than Omega, and \nalso of an alternative approach to representing Presburger for- mulas using finite automata [8, 9]. We \nintend to explore both these options to try to improve the efficiency of our system. However, the general \nproblem of simplifying arbitrary Presburger formulas is intrinsically difficult, no matter whether one \nviews it from the perspective of logic, number theory, computational geometry [53], automata theory, \nor something else. In the end, the only practical path to efficiency may involve developing specialized \nalgorithms that exploit some structural constraints of the kinds of formulas that arise in our application. \nIn addition to compiler-related uses, our approach may also sig- nificantly speed up cache simulators \nby enabling them to rapidly leap-frog the computation over polyhedral loop nests that consume most of \nthe running time. The development of such a mixed-mode cache simulator remains the subject of future \nwork. 7. REFERENCES [1] A.Agarwai, M. Horowitz, andJ, Ht.nnessy AnanalytwcalcachenxxleI.ACMTrans. Comput.Sy~t., \n7(2): 18,4-215. May 1989. [2] N.A.hmed~L~calityEnhanceraent~f~mperfe~tly.nestedL~pNesas.PhDthesis~Depa~1me.at~fC~r~put~x \nScience, Cornel| Umverslty. Aug. 2000. [3] N. Ahmed, N. Mateev, and K. Pmgah. Tihng imperfectly.nested \nkx~p nests. Techmcal Report TR2000.1782. Com\u00a2ll Umv~slty, 2(X)O. [4] ~Ahm~dandK.Pmgah~Aut~ma~generati~n~f~ck~re~ursIv~ctx~.h~Pr~ceeding$~Eur~par2~pag~ \n125-134. 2000. [5] M. AIt, C. Ferdinand, F. Martin, ~d R Wilhelm. C~he behavior prediction by abstract \ninterpretation. In R Cotlsot arid D A. Sclhnadt. editors, 8AS'96. Staffc Analysis 8ymFoMum , volume I \n145 of Lecture Notes in ComputerScience, pages 5|~66. Spnnger. September 1996. [6] T. Amon. G. Bornetlo, \nT. Hu, and J Lm Symbohe ttvmng verification of timing chagrarc~ using PresLerge~ foranulas. In Proceedings \nof DAC 97. pages 226--23 I, Anaheun, CA, June 1997, [7] T. Amon, O. Bornello, and L Lm Making complex \nun-ang relationships readable: Fre.sburgcr formula elmphficcatlon tv~mg don't car~, In Pme'eedings of \nDAC 98. pag~ 586-590. Sail Praltclsco. CA, Jvae 1998. [8] B Bolgelot and P Wolper A. automata-theoretic \napproach to Pr~burger arlthmeue. In A Myeroft, editor, Proceedingz of the Second Inlernattonal Sympostum \non Static Analyzis (SAS '9S), volume 983 of Lecture Notes m ComputerScience. pages l-lg Springer Verlag, \nSept. 1995. ]9] A. BoudetandH Comon.DIophanlmeequanons, Presb~xgeranth/aatleandfinlteautomata lnH Klrchaer. \ned~lor. Proc. Coll. on Trees in Algebra a.d Programmb~g (CAAP'96), volume 1059 of Lecture Notes tn Compmer \nScience. pages 30-43. Spnnger Verlag, 1996. [10] P Boulet and X. Redon. SPPoC\" fonctlonnernen et apphcatlons. \nResearch Report 00.04. LIFL (Laboratolre d~ Recherche en In fornmtlque de l'Umversttd des Sciences et \nTechnologt~ de Lille). 2000. In French. Also see http://www.lifl.fr/west/sppoe/.  ] 11 ] M Brehob and \nR. FEabody. A mathematical model of locahty ~ad caching. Tedmtcal Report TR-MSU-CPS-96-TBD,/vhdagan Stale \nUmvers~ly. Nov. 1996 [12] G C. C~jt~val. Compile.~me Performance Prediction of Sciem~c Programs. PhD \nlhesls, Department off Computer Science. Umverstty of Illinois at Urbana-Champatgn. 2000. [131 S~Can.~K~S.MeKm~ey~andC~W'Tseag~CC~mp~er~pt~wazati~nsf~rirr~r~wngdata~cal]ty~nPr~ceeding~of \nthe Sixth Imernational Conference on Architectural Support for Programming Languages and Operating System~, \npages 252...-262, San Jose, CA, Oct. 1994. [14] S~Chatteriee~ V~ V~Jam~A.RLebeek~S~Mundhra,andM~Th~tteth~di.N~nhnear~aray~ay~ut~f~rh~erar~h~ca~ \nmemory systems. In Proceedings of the 1999 ACM lmernational Conference on Supercompuling, pages 444.-453, \nRhodes, Greece, June 1999. [15] S. Chalterj~, A. R. Lebt~;k. E K. Patna]a, and M. ThottefliodL Reoaralve \narmy layouts and fast parallel fflaln x mult]pheatzon In Proceedings o/Eleventh AmluaI ACM Syr~sium on \nParallel Algorithms and Amhttert.rex. page.s 222.--231. Stunt-Main, Fr~ce, June t999. [16] S.Chatte0eeandS~Sen`Cache-e~ctentmatr~xt~ansIx~s~t~n.~nPr~ceedings~fHPCA~6~pages~95-2~5 \n, Toulouse. Prance, Jan 2000. [17] M ~er.makand~L~`Um~ymgd~taan~c~ntr~tran~f~rmat~nsf~rd~str~butedshared~mern~rymaehmes.~n \nProceedings of the ACM SIGPL4N'95 Conference on Programming Language Design and Implememation, pages \n205-217, La Jolla, CA, June 1995. [18] P~auss.C~untmgsulut*~nst~hnearandn~ulmear~nstreantst1w~ughEhrh~rtpulyn~m~als~A~phcat~nst~ \nanaly~ and transform scientific program. In Proceedings of International Conference on Supercomputing, \npages 278-285, May 1996. [19] S.cul\u00a2manandK.S.M~Km~ey.T~es~zese~eetionusmgcache~rgan~zati~nanddat~ay~ut~nPraceedin~s~f \nthe ACM SIGPLAN'95 Conference on Programming Language Design and lmplememation, pages 279~290. La Julia, \nCA, June 1995. [20] PF~autr~er`Data~wanalys~s~farrayandsealarreferenees.~nternati~nalJanrnM~fParMle~Pr~gramming~ \n20( 1):23-54,1991. [21] I.D. Frens and D. S. Wts&#38;#169;. Auto-blcekmg rmtnx-multlphcatlollor tracking \nBLAS3 performance with source code, In Proceedingz of the Sixth ACM SIGPLAN Symposium on Principlez and \nPraetlee of Parallel Programming, pages 206--216, Los Vegas, NV, June 1997. [22] C. Frleker, O. Teraam, \nand W. Inlby. lnfluen~ of cl-o~s.mtefference oft ble~ked loops: A case study with matrix-vector multiply. \nACM Tran~ Prog. Lang. Syzh. 17(4):561-575, July 1995. [231 S.Gh~sh.CacheMlesEquatlans:C~mpllerana~y~isfraraew~rkf~rtuningmera~rybehavlar.PhDthests. \nDepartment of Eleemcal Engmeenng.Princeton Umvetslty, Now 1999. [24] S. Ghosh, M. Martonoal. end S. Mallk. \nCache rmss eqnullons: An analytical representation of ccache wasses. In Proceedings of the 1997 International \nConference on Supemomputing. pages 317-324, Vienna, Austria, July 199% 125] S. Ghosh, M. Martonosl, and \nS. Mallk. Preczse mid analysts for program transforraatlons with c~:hes of arbzlrary 8~x,~l nitrify. \nIll Proceedings off the Eighth lnter~mtional Conferetwe on Architectural Support for Programming Languages \nand Operating @stems, pages 228.-239. So. lo~,e, CA, Oct. 1998. ]26] S. Ghosh. M. Martonosi. and S. Mahk. \nCache mass equations: A compiler frarnework for analyzing and tuning rmmotv behavzor. ACM Trans. Prog. \nLang. Syst., 21(4):703-746, July 1999. [27] RJ.Han~n~D~chung~S.chatte43ee~D~emus~A~Lebeek~d~r~er.Tneunmbmat~ncs~fcstehem~s~es \nduring matrix rnulttphcahon, \u00a3 Comput. ~st, Sci,, 2000. To appear. [28] J S, Haq~er.D.J. Kerbyson, gndG, \nR Nudd. Anedyticalmodehagofset.assoeiatlveeachebehavtor.lEEETrans. Comput., 48(lO): lO09-1024, 0ct. 1999, \n[29] M.D. Hill, J. R. Larus, A R Lebeek, M. TTallun,and D. A. Wc~xl. Wty, coirt~ln archli~ltlral reseatx:h \ntool set. Computer Architecture News, 21(4):8-10, Aagu~t 1993. [30] M~D.H~andA~J~Sral~h~Ev~Inatmgass~at~vaymCPU\u00a2aches~EEETrans~mpu~.~C~38(~2):~6~2-~63~ \nDec. 1989. [31] J E. Hoparoft and J. D. UllramL Introduction to Automata Theory. Languages, and Computation. \nAddtson-W~al~y Pubhshmg Colrfany, 1979, [32] M.T. Kaadeuer. A.N. Choudhary, N. Sheaoy, RBanerjee. undJ. \nRarP2xtwanx A line~ algebra framework for automatic daterrranation of optimal data layouts IEEE Tran~tlans \non Parallel and Dlelributed Syztems, 10(2): 115.--135, Feb. 1999. {33] W. Kelly, V. Masinv, W Pugh, E \nRosser, T. Shpetsman. mad D. Wonnacott. lhe Omega Calculator and bbrary. version 1.1.0, Nov. 1996. [34] \nW.Kal~y~V`Mas~Ov'W.Pagh~KR~sser~T~Shpe~sman~andD.W~nnae~tt.The~megaLibtary~rslan~.L~ Interface Guide, \nNov. 1996. [351 W. Kelly and W. Pugh. A fr~ork Inr Urafymg reordenng transformations. Technical Report \nCS-TR-3193, Dep,~t nleot of Compute Sele~e~ University of Maryland, College Perk, MD, Apr. 1993. [36] \nW. Kelly ~d w, Pugh, I~ndmg lagal reordering trensformatmus using trappings. Te~amcal P.ePort CS'TR'3297, \nDep~tlrt~t of Compllte S\u00a2tunee, University of Mm'yland, College Park, IVH3, June 1994. [37] R.E, Kesaler \nand M, D. HriLPagep|~emeatalgOnflirmfc*lat~ereal-mdexoache~ ACMTmns. Comput.syal., 10(4):338-359,1992. \n[38] M, S. Lain, E, E. Rothberg, and M, E. Wolf, The enehe pe~fornmce ~d optnrazatlOes of blccked algortthtm. \nIn Proceediagz of the Fourth InternalionM Conferetwe on Architee\u00a2iural Support for programming Languages \nand Operating Systems, pag~ 63--74, Apr. 1991. [391 A.R. LebeekandD. A. Wood. Cacheprofihng~dflieSPECbeaehmarks: \nAcesestudy. lEEEComputer, 27(10): 15-26. Oct. 1994. [40] A.R.I..ebeek and D. A. Wood. Aawe mersey: A \nnew nbstractton for memory system smmlatzun ACM Trar~action~ on Modeling and Computer Simulation, 7(I):42-77, \nJam, 1997. [41] A.W. Lira mul M. S. Lain. Maxm~ng parallelism ~d Iramlm~g synchrunlzatlun wtfli dfine \ntransf\u00b0rrm. In proceedings of the 241h ACM SIGPLAN-$1GACT b'~nposium on Principles of Pwgramming Language, \ns, pages 201-214, Palls, Fr~ce, Jan. 1997. [42] V. Loeclmer. polyLdb:ALibmryforManipMatingPammeteri~edPalyhedm, \nMat'1999- [43] M. Mertonotn, A. Gupta, endT. Andetson Metrapy: Analymngmemol3'syatembottleneeksmlr~ograms \nIn MGMETRlCS92, pages 1-i2. June 1992. [44] K.S.M\u00a2Km~eyand~.TermmQuant~fytag~p~est~calityusmg~PEc~95and~IePe~eaben~hmarks \nACM Trans. Comput. ~y~t., 17(4):288--336,Nov. 1999. [45] N. Mitchell, L. Carter, J. Ferrante, and K H0$atedt~ \nQuantifying the multi.level natureof tflmg interactions In Languages and Compilerz for Parallel Computing: \nlOth Annum Workahep, LCPC'97. number 1366 in Lecture Notes in Computer Science, pages 1-15. Spnages, \n1998. [461 D.C.Oppe~.A2 22pn uppetboundunflieunmplexltyofPresburgerarllhmetlc. J. Comp\"l. SYs~.Sci., \n16(3):323-332, July 1978, [47] Y. Pack, J. Hoeflinger, and D. Padug- Slmp hficatl\u00b0n \u00b0f array a~ee~s patte~tl$ \nfct\" eOn~pder \u00b0Pnrte~tt\u00b0ns\" In Proceedings ofACM PLDI. vulurrt~ 33. pages 60-71, May 1998. [481 A.K. \nPorterfield.SoftwareMethedsforlmp r\u00b0vemenl\u00b0fCachePege\u00b0rmance\u00b0nSugerc\u00b0mputerAppticatwns PhD thesis, Rice \nUniversity, Houston, 'IX, May 1989. Available as technical report CRPC.TR$9009 [49] W. Pugh.Countmg~olutlonstoPresbeeg \nerf\u00b0rmulas:H\u00b0wandwhy'InPr\u00b0ceeding~\u00b0ftheACMSIGPLAN'94 Conference on Pmgrammi.g Language Design and Implementation. \npages 121-134, Orlando, FL. June 1994. I50] G. Rivets and C..W. Tseng. Data traxtsformanons for ehmmatiag \nconflict tresses In proceedings of the ACM SIGPLAN'98 Conference on Progroraming Language Design and \nImplememation, pages 38-49, Months]. Canada, Juae 1998. (5 i ] G Rivers and C -W. Tseng EJlmnllutlllg \noonlhct rntsses for high l~rforrna~tct~ archlte..aures In Proceedings of the 1998 International Conference \nan S.petcomputing. pag~ 353.*360, Melbourne. Australia, July 1998 1521 U SchOmag ComplexJtyofPresburgeranthmatlcwlthfixedquantlfierdlmenslon. \nTheory o/ Computing Syslems, 30\"423.-428,1997 [53] N. Shfliata,K. Okaoa. T Hlgashmo.andK TanlgqlctlLAd~.lslonalgonthffidorprenexformratlonall~esbt.lrger \nsentences based on combinatorial geometry. In Proceeding* of the 2nd International Conference on Discrete \nMathematicz and Theoretical Computer Science wld \u00a2he 5th Australasian Theory Symposium (DMTCS'99+CATS\"99),pages \n344-359, Jan. 1999. [54] A. Srtvastava and A Eustace. ATOM: A system for tmtldmg ct~tormzed program ~alalysls \ntools. In Pmceedmga of the ACM SIGPLAN'94 Conference on Progrmnmlag Language Design and Implementation. \npages 196-205. June 1994. 1551 TneStanfordComptlerGroep SUIF:AnlnfrastmclureforResearchonPamllelizingm*dOpttmlzingCompders \nhttp:/Isulf.stan ford edu [56] R. A Sugunxax 8rid S. G Abraham_ Efficltmt $1mtl]atlon of multiple cache \nconfigurations using bmomlal trees Technical Report CSE-TR.I I I-91.1991. [571 D`~%~bautandH.St~ne.F~oIpnntsmthec~he.ACMTrans.C~mput~Sya.~5(4):3~5-329~N~v.~987. \n1581 D.A. B Wetkle. S. A. MeKee. and W. A, Wulf. Ca\u00a2.lwa ~ filters: A new approach to roche mtalysls. \nIn MASCOTS '98, Modeling. Analysis. and Simulation of Computer and Telecommunication Systems, July 1998. \n [59] D.A.B. Welkle, K. Skadron. S. A. McKee, and W. A. Wulf. Caches as filters: A unifying rtxMel for \nmemory hierarchy analysis. Teehmcal Report CS-2000-16, Umverslty of Vzrgmla. June 2000. [60] VW\u00a2ispf\u00a2nnmg~C~rnp~ea~tyandun~f~rtmty~fal~n3inat~n~nPresbuxgerar~thraunc~InPr~ceediags~fthe1997 \nlnternationalA~amposlum on Symbolic and Algebraic Computation. pages 48-.-53, Kthel. Maul, HI, July 1997 \n[61] M E WulfandM. S. LamAdatalocalityopttrmzmgalgortthntlnProceedlngsoftheACMSIGPLAN'91 Conference on \nProgramming Language Design a.d Implementation, pages 30-.44, Toronto. Canada. June 1991. [621 M.LW~f\u00a2.M~re~Ierati~nspacenhng.InPr~ceedings~fSuperc~mpuring~89~pages65~-664~Ren~NV~N~v. \n1989. {631 D.A. Wood, M. D. Hill, and R. E Kes.~ler. A model for eatlmatmg trace-sample rmss ratios. \nIn Proceedings of ACM SIGMETRICS, May 1991. [64] H z~angandM.Mart~n~st.Math~mat~ca~c~h~rmssanalyalsf~rp~mt\u00a2rdatastructures.~nPr~ceedbIgs~fthe \nSIAM Conference on Parallel Pro~ssi.~ for Scientific Computing. Portsmouth, VA. Mar. 2001. CD-ROM 297 \n   \n\t\t\t", "proc_id": "378795", "abstract": "<p>We develop from first principles an exact model of the behavior of loop nests executing in a memory hicrarchy, by using a nontraditional classification of misses that has the key property of composability. We use Presburger formulas to express various kinds of misses as well as the state of the cache at the end of the loop nest. We use existing tools to simplify these formulas and to count cache misses. The model is powerful enough to handle imperfect loop nests and various flavors of non-linear array layouts based on bit interleaving of array indices. We also indicate how to handle modest levels of associativity, and how to perform limited symbolic analysis of cache behavior. The complexity of the formulas relates to the static structure of the loop nest rather than to its dynamic trip count, allowing our model to gain efficiency in counting cache misses by exploiting repetitive patterns of cache behavior. Validation against cache simulation confirms the exactness of our formulation. Our method can serve as the basis for a static performance predictor to guide program and data transformations to improve performance.</p>", "authors": [{"name": "Siddhartha Chatterjee", "author_profile_id": "81100051284", "affiliation": "Department of Computer Science, The University of North Carolina, Chapel Hill, NC", "person_id": "PP14029125", "email_address": "", "orcid_id": ""}, {"name": "Erin Parker", "author_profile_id": "81100169726", "affiliation": "Department of Computer Science, The University of North Carolina, Chapel Hill, NC", "person_id": "P334736", "email_address": "", "orcid_id": ""}, {"name": "Philip J. Hanlon", "author_profile_id": "81100052757", "affiliation": "Department of Mathematics, University of Michigan, Ann Arbor, MI", "person_id": "P324593", "email_address": "", "orcid_id": ""}, {"name": "Alvin R. Lebeck", "author_profile_id": "81100265767", "affiliation": "Department of Computer Science, Duke University, Durham, NC", "person_id": "PP15027760", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/378795.378859", "year": "2001", "article_id": "378859", "conference": "PLDI", "title": "Exact analysis of the cache behavior of nested loops", "url": "http://dl.acm.org/citation.cfm?id=378859"}