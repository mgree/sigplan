{"article_publication_date": "05-01-2001", "fulltext": "\n A Unified Framework for Schedule and Storage Optimization* William Thies t, Pr@d@rie Vivien ~, Jeffrey \nSheldon t, and Saman Amarasinghe t tLaboratory For Computer Science [ICPS/LStIT Massachusetts Institute \nof Technology Universit~ Louis Pasteur Cambridge, MA 02139 Strasbourg, France {thie s, jeff shel, saman}Olcs, \nmit. edu vivienOicps,u-strasbg, fr Abstract We present a unified mathematical framework for analyz- \ning the tradeoffs between parallelism and storage allocation within a parallelizing compiler. Using this \nframework, we show how to find a good storage mapping for a given sched- ule, a good schedule for a given \nstorage mapping, and a good storage mapping that is valid for all legal schedules. We consider storage \nmappings that collapse one dimension of a multi-dimensional array, and programs that are in a sin- gle \nassignment form with a one-dimensional schedule. Our technique combines affine scheduling techniques \nwith occu- pancy vector analysis and incorporates general affine depen- dences across statements and \nloop nests. We formulate the constraints imposed by the data dependences and storage mappings as a set \nof linear inequalities, and apply numerical programming techniques to efficiently solve for the shortest \noccupancy vector. We consider our method to be a first step towards automating a procedure that finds \nthe optimal tradeoff between parallelism and storage space. Introduction It remains an important and \nrelevant problem in computer science to automatically find an efficient mapping of a se- quential program \nonto a parallel architecture. Though there are many heuristic algorithms in practical systems and par- \ntial or suboptimal solutions in the literature, a theoreti-cal framework that can fully describe the \nentire problem and find the optimal solution is still lacking. The difficulty stems from the fact that \nmultiple inter-related costs and constraints must be considered simultaneously to obtain an efficient \nexecutable. While exploiting the parallelism of a program is an im- portant step towards achieving efficiency, \ngains in paral- lelism are often overwhelmed by other costs relating to data locality, synchronization, \nand communication. In particu- lar, with the widening gap between clock speed and mere- *This research \nwas done while Fr~d@ric Vivien was a Visiting Pro- fessor in the MIT Laboratory for Computer Science. \nMore informa- tion on this project can be found at http://compiler.lcs.mit.edu/aov. Permission to make \ndigital or hard copies of all or part of this work for personal or classroom use is granted without fee \nprovided that copies are not made or distributed for profit or commercial advan- tage and that copies \nbear this notice and the full citation on the first page, To copy otherwise, to republish, to post on \nservers or to redistribute to lists, requires prior specific permission and/or a fee, PLD/200I 6/O1 Snowbird, \nUtah, USA &#38;#169; 2001 ACM ISBN 1-55113-414-2101/06...$5.00 ory latency, and with modern memory systems \nbecoming in- creasingly hierarchical, the amount of storage space required by a program can have a drastic \neffect on its performance. Nonetheless, parallelizing compilers often employ varying degrees of array \nexpansion [9, 5, 1] to eliminate element-level anti and output dependences, thereby adding large amounts \nof storage that may or may not be justified by the resulting gains in parallelism. Thus, compilers must \nbe able to analyze the tradeoffs between parallelism and storage requirements in order to arrive at an \nefficient executable. In this paper, we intro- duce a unifying mathematical framework that incorporates \nboth schedule constraints (restricting when statements can be executed) and storage constraints (restricting \nwhere their results can be stored). We consider storage mappings that collapse one dimension of a multi-dimensional \narray, and programs that are in a single assignment form with a one- dimensional schedule. Our technique \nincorporates general affine dependences across statements and loop nests, mak- ing it applicable to many \nscientific applications Using this technique, we present solutions to three im- portant scheduling problems. \nNamely, we show bow to de- termine 1) a good storage mapping for a given schedule, 2) a good schedule \nfor a given storage mapping, and 3) a good storage mapping that is valid for all legal schedules. Our \nmethod is precise and practical in that it reduces to a linear program that can be efficiently solved \nwith standard tech- niques. We believe that these solutions represent the first step towards automating \na procedure that finds the optimal compromise between parallelism and storage space. The rest of this \npaper is organized as follows. In Sec- tion 2 we motivate the problem abstractly, and in Section 3 we \ndefine it concretely. Section 4 formulates the method abstractly, and Section 5 illustrates the method \nwith exam- ples. Experiments are described in Section 6, related work in Secion 7, and we conclude in \nSection 8.  2 Abstract Problem To motivate our approach, we consider simplified descrip- tions of the \nscheduling problems faced by a paratlelizing compiler. We are given a directed acyclic graph G = (V, \nE). Each vertex v E V represents a dynamic instance of an in- struction; a value will be produced as \na result of executing v. Each edge (vl, v2) E E represents a dependence of v2 on the value produced by \nvl. Thus, each edge (vl, v2) imposes the schedule constraint that vl be executed before v2, and the storage \nconstraint that the value produced by vl be stored until the execution time of v2. A[] [] : new int[n] \n[m] $t [] = new in't [Ix] :for j = 1 to m for i = I to n A[i][j] = f(A[i-2][j-1], A[i][j-1], AEi+i][j-1]) \nFigure 1: Original code for Example 1. Our t~k is to output (\u00ae, m), where O is a function map- ping \neach operation v E V to its execution time, and m is the maximum number of values that we need to store \nat a given time. Parallelism is expressed implicitly by assigning the same execution time to multiple \noperations. To sim- plify the problem, we ignore the question of how the values are mapped to storage \ncells and assume that live values are stored in a fully associative map of size m. How, then, might we \ngo about choosing O and m? 2.1 Choosing a Store Given a Schedule Tile first problem is to find the optimal \nstorage mapping for a given schedule. That is, we are given \u00ae and choose m such that 1.) (O, m) respects \nthe storage constraints, and 2) m is as small as possible. This problem is orthogonal to the traditional \nloop paral- lelization problem. After selecting the instruction schedule by any of the existing techniques, \nwe are interested in iden- tifying the best storage allocation. That is, with schedule- specific storage \noptimization we can build upon the perfor- mance gains of any one of the many scheduling techniques available \nto the parallelizing compiler. 2.2 Choosing a Schedule Given a Store The second problem is to find an \noptimal schedule for a given size of the store, if any valid schedule exists. That is, we are given m \nand choose O such that 1) (O, m) respects the schedule and storage constraints, and 2) O assigns the \nearliest possible execution time to each instruction. Note that if m is too small, there might not exist \na O that respects the constraints. This is a very relevant problem in practice because of the stepwise, \nnon-linear effect of storage size on execution time. For example, when the storage required cannot be \naccommodated within the register file or the cache, and has to resort to the cache or the external DRAM, \nrespectively, the cost of storage increases dramatically. Further, since there are only a few discrete \nstorage spaces in the memory hierarchy, and their size is known for a given architecture, the compiler \ncan adopt the strategy of trying to restrict the store to successively smaller spaces until no vMid schedule \nexists. Once the storage is at the lowest possible level, the schedule could then be shortened, having \na more continuous and linear effect on efficiency than the storage optimization. In the end, we end up \nwith a near-optimal storage allocation and instruction schedule. 2.3 Choosing a Store for all Schedules \nThe final problem is to find the optimal storage mapping that is valid for all legal schedules. That \nis, we axe given a (possibly infinite) set = {O1,O2,... }, where each O in respects the schedule constraints. \nWe choose m such that 1) V\u00ae E ~, (O, m) respects the storage constraints, and 2) m is as salall as possible. \nfor j = i \"to m for i ~ I to n A[i] = f(A[i-2], A[i], A[i+l]) Figure 2: Transformed code for Example \n1. The occupancy vector is (0,1). -e .... ----O----O .... O----O----O- Figure 3: Iteration space diagram \nfor Example 1. Given the schedule where each row is executed in parallel, our method iden- tifies (0, \n1) as the shortest valid occupancy vector. A solution to this problem allows us to have the mini- mum \nstorage requirements without sacrificing any flexibility of our scheduling. For instance, we could frst \napply our stor- age mapping, and then arrange the schedule to optimize for data locality, synchronization, \nor communication, without worrying about violating the storage constraints. Such flexibility could be \ncritical if, for example, we want to apply loop tiling [10] in conjunction with storage opti- mization. \nIf we optimize storage too much, tiling could become illegal; however, we sacrifice efficiency if we \ndon't optimize storage at all. Thus, we optimize storage as much as we can without invalidating a schedule \nthat was valid under the original storage mapping. More generally, if our analysis indicates that certain \nsched- ules are undesirable by any measure, we could add edges to the dependence graph and solve again \nfor the smallest ra sufficient for all the remaining candidate schedules. In this way, m provides the \nbest storage option that is legal across the entire set of schedules under consideration. 3 Concrete \nProblem Unfortunately, the domain of real programs does not lend itself to the simple DAG representation \nas presented above. Primarily, loop bounds in programs are often specified by symbolic expressions instead \nof constants, thereby yielding a parameterized and infinite dependence graph. Furthermore, even when \nthe constants are known, the problem sizes are too lacge for schedule and storage analysis on a DAG, \nand the executable grows to an infeasible size if a static instruc- tion is generated for every node \nin the DAG. Accordingly, we make two sets of simplifying assump- tions to make our analysis tractable. \nThe first concerns the nature of the dependence graph G and the scheduling function O. Instead of allowing \narbitrary edge relationships and execution orderings, we restrict our attention to affine dependences \nand affine schedules. The second assumption concerns our approach to the optimized storage mapping. Instead \nof allowing a fully associative map of size m, as above, we employ the occupancy vector as a mechanism \nof storage reuse. In the following sections, we discuss these assumptions in the context of an example. \n233 Figure 4: Iteration space diagram for Example 1. Given an occupancy vector of (0, 2), our method \nidentifies the range of valid schedules. An affine schedule will sweep across the space, executing a \nline of iterations at once. If this line falls within the gray region (as on the left), then the schedule \nis vMid for the occupancy vector of (0,2). If this line falls within the striped region (as on the right) \nthen the schedule is valid for some occupancy vector other than (0, 2). The schedule at right is invalid \nbecause the operation at the tip of the occupancy vector (0, 2) overwrites a value before the operation \nat (2, 1) can consume it.  3.1 Program Domain Primarily, we require an affine description of the depen-dences \nof the program. This formulation gives an accurate description of the dependences of programs with static \ncon- trol flow and ai-fine index expressions [6] and can be esti- mated conservatively for others. As \nwill become clear be- low, restricting our attention to affine dependences allows us to model the infinite \ndependence graph as a finite set of parameters, which is central to the method. In this paper, we further \nassume a single-assignment form where the iteration space of each statement exactly corresponds with \nthe data space of the array written by that statement. That is, for array references appearing on the \nleft hand side of a statement, the expression indexing the i'th dimension of the array is the index variable \nof the i'th enclosing loop (this is formalized below). While techniques such as array expansion [5] can \nbe used to convert programs with affine dependences into this form, our analysis will be most useful \nin cases where an expanded form was obtained for other reasons (e.g., to detect parallelism) and one \nnow seeks to reduce storage requirements. We will refcr to the example in Figure 1, borrowed from [17]. \nIt clearly falls within our input domain, as the depen- dences have constant distance, and iteration \n(i, j) assigns to A[i][j]. This example represents a computation where a one-dimensional array A[i] is \nbeing updated over a time dimension j, and the intermediate results are being stored. We assume that \nonly the element A[n][m] is used outside the loop; the other values are only temporary.  3.2 Occupancy \nVectors To arrive at a simple model of storage reuse, we borrow the notion of an occupancy vector from \nStrout et al. [17]. The strategy is to reduce storage requirements by defining equivalence classes over \nthe locations of an array. Following a storage transformation, all members of a given equivalence class \nin the original array will be mapped to the same loca- tion in the new array. The equivalence relation \nis: ~7 if, for all pairs of locations (i~, ~) in A: Re(h, ~) ~ h and ~ are stored in same location in \nA' We say that an occupancy vector ~7 is valid for an array A with respect to a given schedule O if transforming \nA under everywhere in the program does not change the semantics when the program is executed according \nto @. Given an occupancy vector, we implement the storage transformation using the technique of [17] \nin which the orig- inal data space is projected onto the hyperplane perpen- dicular to the occupancy \nvector. If an occupancy vector intersects multiple (integral) points of the data space, then modulation \nmust be used to distinguish these points in the transformed array. Occupancy vector transformations are \nuseful for reduc- ing storage requirements when many of the values stored in the array are temporary. \nGenerally, shorter occupancy vectors lead to smaller storage requirements because more elements of the \noriginal array are coalesced into the same storage location. However, the shape of the array also has \nthe potential to influence the transformed storage require- ments. Throughout this paper, we assume that \nthe shapes of arrays have second-order effects on storage requirements, and we refer to the \"best\" occupancy \nvector as that which is the shortest. We are now in a position to consider our occupancy vec- tor analysis \nas applied to Example 1. First, assume that we have chosen to execute each row in parallel so as to have \nthe shortest schedule. What is the best storage mapping for this schedule? Our method can identify (0, \n1) as the shortest occupancy vector for this schedule (see Figure 3), yielding the code in Figure 2. \nSecondly, consider the case where we become interested in adding some flexibility to our scheduling. \nIf we lengthen the occupancy vector to (0, 2), what is the range of sched- ules that we can consider? \nAs illustrated in Figure 4, our method can identify all legal affine schedules for the occu-pancy vector \nof (0, 2). We could then use affine scheduling techniques [7] to choose amongst these schedules according \nto other criteria. and we refer to ~ as the occupancy vector. We say that A' 3.3 Affine Occupancy Vectors \nis the result of transforming A under the occupancy vector Finally, we might inquire as to the shortest \noccupancy vector that is valid for all affine schedules in Example 1. An affine Figure 5: Iteration space \ndiagram for Example 1. Here the hol- low arrow denotes an Affine Occupancy Vector that is valid for all \nlegal affine schedules. The gray region indicates the slopes at which a legal affine schedule can sweep \nacross the iteration domain. A[] = new int [2*n+m] :for j = 1 to m for i = i to n A[2*i-j+m] = f(A[2*(i-2)-(j-1)+ia], \n A[2*i-(j-1)+m], A[2* (\u00a3+i)-(j-l)+m])  Figure 6: qYansformed code for Example I. The AOV is (1,2). \n schedule is one where each dynamic instance of a statement is executed at a time that is an affine expression \nof the loop indices, loop bounds, and compile-time constants. To ad- dress the problem, then, we need \nthe notion of an Affine Occupancy Vector: Definition 1 An occupancy vector ~ for array A is an Afflne \nOccupancy Vector (AOV) if it is valid with respect to every affine schedule 0 that respects the schedule \nconstraints of the original program. Note that, in contrast to the Universal Occupancy Vector of [17], \nan AOV need not be valid for all schedules; rather, it only must be valid for affine ones. Almost all \nthe instruction schedules found in practice are affine, since any FOR loop with constant increment and \nbounds defines a schedule that is affine in its loop indices, (This is independent of the array references \nfound in practice, which are sometimes non- affine.) In this paper, we further relax the definition of \nan AOV to those occupancy vectors which are valid for all one-dimensional 1 altine schedules. We also \nobserve that, if tiling is legal in the original program, then tiling is legal after transforming each \narray in the program under one of its AOV's. This follows from the fact that two loops are tilable if \nand only if they can be permuted without affecting the semantics of the program [10]. Since each permutation \nof the loops corresponds to a given affine schedule and the AOV is valid with respect to both schedules, \nthe AOV transformation is also valid with respect to a tiled schedule. Returning to our example, we find \nusing our method that (1, 2) is a valid AOV (see Figure 5). Any affine one- dimensional schedule that \nrespects the dependences in the original code will give the same result when executed with the transformed \nstorage. 1A one-dimensional affine schedule assigns a scalar execution time to each operation as an affine \nfunction of the enclosing loop indices and symbolic constants. Multi-dimensional schedules assign vector- \nvalued execution times, which are ordered lexicographically; certain programs require multi-dimensional \nschedules. See [7, 8, 4] for details. 4 The Method 4.1 Notation We adopt the following notation: o An \niteration vector ~contains the values of surrounding loop indices at a given point in the execution of \nthe program. o The structural parameters g, of domain X, represent loop bounds and other parameters \nthat are unknown at compile time, but that are fixed for any given exe- cution of the program.  \u00ae There \nare n~ statements $1... S,~ in the program. Each statement S has an associated polyhedral do- main 79s, \nsuch that V~ E 79s, there is a dynamic in- stance S(/~ of statement S at iteration { during the execution \nof the program. \u00ae With each statement S is associated a scheduling func- tion Os which maps the instance \nof S on iteration to a scalar execution time. By assumption, Os is an affine function of the iteration \nvector and the struc-tural parameters: Os(~, ~) = gs \" i + bs ~ + cs. The schedule for the entire program \nis denoted by (9 E 8, where \u00a3 is the space of all the scheduling parameters There are np dependences \nP1... Pop in the program. Each dependence Pj is a 4-tuple (Rj, Tj, \"Pj, hi) where Rj and 7~ are statements, \nhj is a vector-valued affine function, and \"Pj C 79Rj is a polyhedron such that: V~ E 79j,Rj(~) depends \non Tj(hj(~,~)) (1) The dependences Pj are determined using an array dataflow analysis, e.g., [6] or the \nOmega test [15]. There are n~ arrays A1...A~ in the program, and A(S) denotes the array assigned to \nby statement S. Our assumption that the data space corresponds with the iteration space implies that \nfor all statements S, S(i') writes to location ~ of A(S), and S is the only statement writing to A. However, \neach array A may still appear on the right hand side of any number of statements, where its indices can \nbe arbitrary affine expressions of ~ and ~.  With each array A we will associate an occupancy vec- \ntor gA that specifies the storage reuse within A. The locations ~ and ~ in the original data space of \nA will be stored in the same location following our storage transform if and only if ~ = ~2 + k ~TA, \nfor some inte- ger k. Given our assumption about the data space, we can equivalently state that the values \nproduced  by iterations i~ and i'~ will be stored in the same lo-cation following our storage transform \nif and only if ~1 = i'2 + k * gA, for some integer k.  4.2 Schedule Constraints According to dependence \nPj (Equation (1)), for any value of in Pj, operation Rj (/~ depends on the execution of opera- tion Tj \n(hi (~', ~)). Therefore, in order to preserve the seman- tics of the original program, in any new order \nof the com-putations, Tj (fhj (~, ~)) must be scheduled at a time strictly earlier than Rj ({), for all \n~ ~ ~Pj. We express this constraint in terms of the scheduling function. We must have, for each dependence \nPh, J E [1, np]: These dependence constraints can be solved using Farkas' lemma as shown by Feautrier \n[7, 8, 4]. The result can be ex- pressed as a polyhedron 7\u00a2: the set of all the legal schedules \u00ae in \nthe space of scheduling parameters \u00a3. Note that Equa- tion (2) does not always have a solution [7]. In \nsuch a case, one needs to use rnultidimensional schedules [8]. However, in this paper, we assume that \nEquation (2) has a solution. Refer to Section 5.1.1 for an example of the schedule constraints.  4.3 \nStorage Constraints The occupancy vectors induce some storage constraints. We consider any array A. Because \nwe assume that the data space corresponds with the iteration space, and by definition of the occupancy \nvectors, the values computed by iterations and ~+ ~'A are both stored in the same location ~. For an \noccupancy vector gA to be valid for a given data object A, effery operation depending on the value stored \nat location l by iteration ~ must execute no later than iteration ~ + ~A stores a new value at location \n~. Otherwise, following our storage transformation, a consumer expecting to reference the contents of \nfproduced by iteration ~ could reference the contents of ~ written by iteration ~ + ffA instead, thereby \nchanging the semantics of the program. We assume that, at a given time step, all the reads precede the \nwrites, such that an operation consuming a value can be scheduled for the same execution time as an operation \noverwriting the value. (This choice is arbitrary and unimportant to the method; under the opposite assumption, \nwe would instead require that the consumer execute at least one step before its value is overwritten.) \nLet us consider a dependence P = (R,T,h,P). Then operation T(f~(~,/i)) produces a value which will be \nlater on read by R(/~. This value will be overwritten by T(fh(~, g) + gA(T)). The storage constraint \nimposes that T(h(~,gg)+ gA(T)) is scheduled no earlier than R(~). Therefore, any schedule ~ and any occupancy \nvector ffA(T) respects the de- pendence P if: v~ e x', v7 e z, 0~(~(7, ~) + ~(T), ~) -- 0R(7, ~) > 0 \n(3) where Z represents the domain over which the storage con- straint applies That is, the storage constraint \napplies for all iterations [ where [ is in the domain of the dependence, and where h([, g) + ffA(T) is \nin the domain of statement T. Formally, Z = {~ I ~ E T' A h(~,g) + V~A(T) E ~DT}. This definition of \nZ is not problematic, since the intersection of two polyhedra is defined simply by the union of the affine \nin- equalities describing each, which obviously is a polyhedron. Note, however, that Z is parameterized \nby both ffA(T) and 'if, and not simply by g. Equation (3) expresses the constraint on an occupancy vector \nfor a given dependence and a given schedule. For an occupancy vector to be an AOV, however, it must respect \nall dependences across all legal schedules. Thus, the following constraint defines a valid AOV ga for \neach object A in the program: Ve e ~, V~ e N', Vj c [1, np], V7 c Z;, % (~j(::, ~) + ~TA(r~, ~) - 0~ \n(i2 ~) -- 1 _> 0 (4) See Section 5.1.1 for an illustration of the storage con-straints. 4.4 Linearizing \nthe Constraints Equations (3) and (4) represent a possibly infinite set of constraints, because of the \nparameters. Therefore, we need to rewrite them so as to obtain an equivalent but finite set of affine \nequations and inequalities, which we can easily solve. Meanwhile, we seek to express the schedule (2) \nand storage (4) constraints in forms afl:ine in the scheduling parameters O. This step is essential \nfor constructing a linear program that minimizes the length of the AOV's.  Section 5.2 contains an illustrative \nexample of the con-straint linearization. 4.4.1 Reduction using the vertices of polyhedra Any nonempty \npolyhedron is fully defined by its vertices, rays and lines [16], which can be computed even in the case \nof parameterized polyhedra [13]. The following theorem ex- plains how we can use these vertices, rays \nand lines to reduce the size of our sets of constraints. Theorem 1 Let D be a nonempty polyhedron. D \ncan be written D = P + C, where P is a polytope (bounded polyhe- dron) and C is a cone. Then any affine \nfunction h defined over D is nonnegative on 7) if and only if i) h is nonnega- tire on each of the vertices \nof P and ~) the linear part of h is nonnegative (resp. null) on the rays (resp. lines) of C. Although \nthe domain of structural parameters Af is an input of this analysis and may be unbounded, all the poly- \nhedra produced by the dependence analysis of programs are in fact polytopes, or bounded polyhedra. Therefore, \nin or- der to simplify the equations, we now assume that all the polyhedra we manipulate are polytopes, \nexcept when stated otherwise Then, according to Theorem 1, an affine function is nonnegative on a polyhedron \nif and only if it is nonneg- ative on the vertices of this polyhedron. We successively use this theorem \nto eliminate the iteration vector and the structural parameters from Equation (3).  4.4,2 Eliminating \nthe Iteration Vector Let us consider any fixed values of O in 'T~ and ~ in .h/' Then, for all j E [1, \nnp], V~A(Ti) must satisfy:  v~\" e z~, 0T~ (gj d, ~) + ~A(T~), ~) --0Rj (~', ~) --1 > 0 (5) which is \nan affine inequality in ~ (as hi, OT~, and OR s are affine functions). Thus, according to Theorem 1, \nit takes its extremal values on the vertices of the polytope Zj, denoted by ~z,j,.. , F~,,j. Note that \nZj is parameterized by ~ and V~A(TD. Therefore, the number of its vertices might change depending on \nthe domain of values of ~ and ffA(Ti). In this case we decompose the domains of g and gArT.~ into subdo- \n, ~ 31 mains over which the number and defimtmn of the vertices do not change [13], we solve our problem \non each of these domains, and we take the \"best\" solution Thus, we evaluate (5) at the extreme points \nof Zj, yield- ing the following: Vk c:. [1,nz],OTi(f~j(~k,j(~A(Ti),~),~) \"~\"VA(T~),~) (6)  -0~ (e~,~(ga(~,~), \n~), ~) -1 >_ 0 According to Theorem 1, Equations (5) and (6) axe equiva- lent. However, we have replaced \nthe iteration vector ~ with the vectors z~,j, each of which is an anne form in g and VA(Tj )\" 4.4.3 Eliminating \nthe Structural Parameters Suppose Af is also a bounded polyhedron. We eliminate the structural parameters \nthe same way we eliminated the iteration vector: by only considering the extremal vertices of their domain \nAf. Thus, for any fixed value of O in 7~, j in [I, np], and k in fl, nz] we must have: w ~ N, o~ (f~ \n(e~j(~'A(r;), ~), ~) + ~a(r~),~) (7) -0~ (e~,~(~(v~), ~), ~) -1 _> 0 Denoting the vertices of Af by \n(~l,...,~n~), the above equation is equivalent to: W ~ [1, n~], 0r~ (fij (~,~ (~a(r~), ~), ~) + ~a(r~), \n~) (8) --OR~(Zk,j(VA(Ti),Wl), l,) --1 k 0 Case of unbounded domain of parameters. It might also be the \ncase that Af is not a polytope but an unbounded polyhedron, perhaps corresponding to a parameter that \nis input from the user and can be arbitrarily large. In this case, we use the general form of Theorem \n1. Let ~1,..., r~, be the rays defining the unbounded portion of A f (a line being coded by two opposite \nrays). We must ensure that the linear part of Equation (8) is nonnegative on these rays. For example, \ngiven a single structural parameter nl E [5, oo), we have the following constraint for the vertex nl \n= 5: o~j (~j (~,~ (~A(~), 5), 5) + ~a(r~), 5)  -0~ (~k,~ (~a(~), 5), 5) - 1 > o and the following \nconstraint for the positive ray of value 1: 0% (h~ (Fa5 (ffA(T~), 1), 1) + gA(T D, 1) -Oe~ (Fad (gA(~), \n1), 1) (9) -ors (~ (e~,~ (~A(r~), 0), 0) + ~(~), 0) +OR~(F~,j(~A(T~), 0), O) > 0 Though this equation \nmay look complicated, in practice it leads to simple formulas since all the constant parts of Equa- tion \n(7) are going away. We assume in the rest of this paper that Af is a polytope. This changes nothing in \nour method, but greatly improves the readability of the upcoming sys- tems of constraints! 4.5 Finding \na Solution After removing the structural parameters, we are left with the following set of storage constraints: \nVj ~ [1, Up], Vk e [1, n~], Vl E [1, nw],  0r~ (~j (e~,j (~A(r~), ~), ~z) + ~a(rj), ~) (~0) --0Ri (Fkd \n(ga(Tj), tg~), ~l) -- 1 > 0 which is a set of affine inequalities in the coordinates of the schedule \nO, with the occupancy vectors ggA(T~)as unknowns. Note that the vertices Fk,j of the iteration domain, \nthe ver- tices Nl of the structural parameters, and the components fhj of the affine functions, all \nhave fixed and known values. Similarly, we can lineaxize the schedule constraints to arrive at the following \nequations: vj e [1, ~p], Vk e [1, n~], Vle [1, n~], (11) ORj (Yk,j (Wt), wl ) -- OT~ (hi (Yk,j (~z), \nwl), Nt) -1 > 0 Where ytd,..., Y~,J denote the vertices of Pj. 4.5.1 Finding an Occupancy Vector Given \na Schedule At this point we have all we need to determine which oc-cupancy vectors (if any) axe valid \nfor a given schedule \u00ae: we simply substitute into the simplified storage constraints (10) the value of \nthe given schedule. Then we obtain a set of affine inequalities where the only unknowns are the compo- \nnents of the occupancy vector. This system of constraints fully and exactly defines the set of the occupancy \nvectors valid for the given schedule. We can search this space for solutions with any Linear Programming \nsolver. To find the shortest occupancy vectors, we can use as our objective function the sum of the lengths \n2 of the com-ponents of the occupancy vector. This metric minimizes the \"Manhattan\" length of each occupancy \nvector instead of minimizing the Euclidean length. However, minimizing the Euclidean length would require \na non-lineax objective function. We improve our heuristic slightly by minimizing the dif- ference between \nthe lengths of the occupancy vector compo- nents as a second-order term in the objective function. That \nis, the objective function is dim(v) dim(v) dim(v) i=1 i=1 j~l where k is large enough that the first \nterm dominates, thereby selecting our vector first by the length of its components and then by the distribution \nof those lengths across its dimen- sions (a more \"even\" distribution having a shorter Euclidean distance.) \nIt has been our experience that this linear objec- tive function also finds the occupancy vector of the \nshortest Euclidean distance. For an example of this procedure, refer to Section 5.1.2.  4.5.2 Finding \na Schedule Given an Occupancy Vector At this point, we also have all we need to determine which schednles \n(if any) exist for a given set of occupancy vec- tors. Given an occupancy vector ffA for each array A \nin the program, we substitute into the linearized storage con- straints (10) to obtain a set of inequalities \nwhere the only unknowns are the scheduling parameters. These inequali- ties, in combination with the \nlinearized schedule constraints (11) completely define the space of valid affine schedules valid for \nthe given occupancy vectors. Once again, we can search this space for solutions with any Linear Programming \nsolver, selecting the \"best\" schedule as in [7]. See Section 5.1.3 for an example. 2To minimize Ixl, \nset x = w -z, w > 0, z ~ 0, and then minimize w+z. Either w or z will be zero in the optimum, leaving \nw+z = Ivl. A [] [] = new int In] [m] B [] [] = new int In] [m] for i = I to n for j = i to m A[i][j] \n= f(B[i-l][j]) (Si) B[i][j] = g(A[i][j-1]) ($2) Figure 7: Original code for Example 2.   ~i ~s2 sl \nFigure 8: Dependence diagram for Example 2. 4.5.3 Finding the AOV's Solving for the AOV's is more involved \n(follow Section 5.1.4 for an example.) To find a set of AOV's, we need to satisfy the storage constraints \n(10) for any value of the schedule O within the polyhedron 7~ defined by the schedule constraints. To \ndo this, we apply the Affine Form of Farkas' Lemma [16, 7, 4]. Theorem 2 (Affine Form of Farkas' Lemma) \nLet D be a nonempty polyhedron defined by p a~ne inequalities gj . Z + bj _> O, jE[1,p], in a vector \nspace g. Then an aff~ne form is nonnegative everywhere in :D if and only if it is an a~fine combination \nof the affine forms defining 9: V~ E g, @(e) ~ Ao + E(Aj(Ej'~+ bj)), Ao...Ap _> 0 J The nonnegative constants \nAj are referred to as Farkas mul- tipliers. To apply the lemma, we note that the storage constraints \nare affine inequalities in O which are nonnegative over the poly- hedron 7~. Thus, we can express each \nstorage constraint as a nonnegative afflne combination of the schedule constraints defining T~. To simplify \nour notation, let STORAGE be the set of ex- pressions that are constrained to be nonnegative by the lin- \nearized storage constraints (10). That is, STORAGE con-tains the left hand side of each inequality in \n(10). Naively, ISTORAGE I = n v x n~ x (n~ + n~); however, several of these expressions might be equivalent, \nthereby reducing the size of STORAGE in practice. Similarly, let SCHEDULE be the set of expressions that \nare constrained to be nonnegative by the linearized schedule constraints (11). The size of SCHEDULE is \nat most np x n~ x (n~ + n~). Then, the application of Farkas' Lemma yields these identities across the \nvector space \u00a3 of scheduling parame- ters in which O lives: All = new int [m+n] B[] = new int[m+n] for \ni = 1 to n for j = 1 to m h[i-j+m] = f(BE(i-1)-j+m]) (Sl) B[i-j+m] = g(A[i-(j-l)+m]) (S2)  Figure 9: \nTransformed code for Example 2. Each array has an AOV of (1,1). ISCHEDULE 1 STORAGEi(~) = Ai,o + E (I{,/ \n. SCHEDULEj (J)) j=1 Ai,~ >_ 0, Y:g 6 g, Vi 6 [1, ISTORAGEI] These equations are valid over the whole \nvector:pace g.Therefore, we can collect the terms for each of compo-nents of x, as well as the constant \nterms, setting equal the respective coefficients of these terms from opposite sides of a given equation \n(el. [7, 4] for full details). We are left with ISTORAGEI x (3 x n~ + 1) linear equations where the only \nvariables are the A's and the occupancy vectors gA- The set of valid AOV's is completely and exactly \ndeter- mined by this set of equations and inequalities. To find the shortest AOV, we proceed as in Section \n4.5.1. 5 Examples We present four examples to illustrate applications of the method described above. \n5.1 Example 1: Simple Stencil First we derive the solutions presented earlier for the 3-point stencil \nin Example 1. 5.1.1 Constraints Let 0 denote the scheduling function for the statement writ- ing to array \nA. We assume that 0 is an affine form as follows: O(i,j,n,m) = a* i+ b* j + c* n + d* m + e There are \nthree dependences in the stencil, each from the statement unto itself. The access functions describing \nthe dependences are hi (i, j, n, m) = (i- 2, j -1), h2 (i, j, n, m) = (i,j --1), and hz(i,j,n,m) = (i \n+ 1,j -1). Because these dependences are uniform-that is, they do not depend on the iteration vector-we \ncan simplify our analysis by considering the dependence domains to be across all values of i and j. Thus, \nthe schedule constraints are: O(i,j,n,m) -O(i- 2,j- 1,n,m)-1 > 0 O(i,j,n,m) -O(i,j- 1, n,m) -1 > 0 O(i, \nj, n, m) -O(i + 1, j -1, n, m) -1 >_ 0 However, substituting the definition of 0 into these equa- tions, \nwe find that i, j, n, and m are eliminated. This is because the constraints are uniform. Thus, we obtain \nthe following simplified schedule constraints, which are affine in the scheduling parameters: 2*a+b-1_>0 \nb-l>0 -a+b-l>_0 imax = a.length jmax = b.length kmax = c. length D[] [] [] = new int[imax] [jmax] [kmax] \n for i = i to imax for j = I to jmax for k = i ~o kmax if (i==l) or (j==l) or (k==l) then D[i] [j] [k] \n= f(i,j,k) ($1) else D[i] [j] [k] = (S2) min(D[i-l] [j-l] [k-l] + w(a[i] ,b[j] ,c[k]), D[i][j-1][k-1] \n+ w(GAP,b[j],c[k]), D[i-l] [j] [k-1] + w(a[i] ,GAP,elk]), D[i-l][j-l][k] + w(a[i],b[j],GAP), D[i-l] [j] \n[k] + w(a[i] ,GAP,GAP), D[i][j-l][k] + w(GAP,b[j],GAP), D[i][j][k-l] + w(GAP,GAP,\u00a2[k]))  Figure i0: \nOriginal code for Example 3~ for multiple sequence alignment. Here f computes the initial gap penalty \nand w com- putes the pairwise alignment cost. Now let ~TA = (vl, v j) denote the AOV that we are seeking \nfor re'ray A. Then the storage constraints are as follows: - O(i 2 + vi,j -1 +vj,n,m) -O(i,j,n,m) k 0 \ne(i + vi,j -1 + vj,n,m) -8(i,j,n,m) > 0 e(i + 1 +vl,j -1 +vj,n,m) -8(i,j,n,m) > 0 Simplifying the storage \nconstraints as we did the schedule constraints, we obtain the linearized storage constraints: a*vi +b*vj \n-2,a-b>0 a*vi +b*vj -b >_O a*v~ +b,vj +a-b > 0  5.1.2 Finding an Occupancy Vector To find the shortest \noccupancy vector for the schedule that executes the rows in parallel, we substitute 8(i,j, n, m) = j \ninto the linearized schedule and storage constraints. Mini-mizing lye + vjl with respect to these constraints \ngives the occupancy vector of (0, 2) (see Figure 3). 5,1.3 Finding a Schedule To find the set of schedules \nthat are valid for the occupancy vector of (0, 2), we substitute v~ = 0 and vj = 2 into the linearized \nschedule and storage constraints. Simplifying the resulting constraints yields: b>l-2*a b>l+a b>_2*a \n Inspection of these inequalities reveals that the ratio a/b has a minimum value of -1/2 and a maximum \nvalue that asymptotically approaches 1/2, thus corresponding to the set of legal affine schedules depicted \nin Figure 5 (note that in the frame of the figure, however, the schedule's slope is -~/5.) 5.1.4 Finding \nan AOV To find an AOV for A, we apply Farkas' Lemma to rewrite each of the linearized storage constraints \nas a non-negative imax = a.length jmax = b.length kmax = c.length D[] [] = new int[imax+jmax] [imax+kmax] \nfor i = 1 to imax for j = i ~o jmax for k = 1 to kmax if (i==1) or (j==l) or (k==l) then D[jmax+i-j] \n[kmax+i-k] = f(i,j,k) (Sl) else D[jmax+i-j] [kmax+i-k] = ($2) min(D[jmex+(i-l) -(j-l)] [kmax+ (i-l) -(k-l) \n] + w(a[i] ,b[j] , c [k] ) , D[jmax+i-(j-l)] [kmax+i-(k-i)] + w(GAP,b[j] ,c[k]), D[jmax+(i-l)-j][kmax+(i-i)-(k-l)] \n+ w(a[i],GAP,c[k]), D[jmax+(i-l)-(j-l)] [kmax+(i-l)-k] + w(a[i] ,b[j] ,GAP), D[jmax+(i-l)-j] [kmax+(i-l)-k] \n+ w(a[i] ,GAP,GAP), D[jmax+i-(j-l) [kmax+i-k] + w(GAP,b[j] ,GAP), D[jmax+i-j] [kmax+l-(k-l)] + w(GAP,GAP,c[k])) \n Figure II: Transformed code for Example 3, using the AOV of (1,1,1). The new array has dimension [imax+jmax][imax+kmax], \nwith each reference to [i][j][k] mapped to [jmax+i-j][kmax+i-k]. affine combination of the linearized \nschedule constraints: [ a*v~+b*vj-2*a-b ] a*vi+b*vj-b = a*vi +b*vj +a-b )`1,1 )`1,2 )`1,3 )`1,4 2 * a \n+ b -- 1    [ ][' ] ),2,1 )`2,2 )`2,3 )`2,4 b-1 )`3,1 )`3,2 Aa,a )`a,4 -a +b-1 Ai,j > 0, Vi 6 [1,3], \nVj e [1, 4] Minimizing Ivi + vii subject to these constraints yields an AOV (vi,vj) = (1,2), which is \nsmaller than the shortest uov of (o, 3) [17]. To transform the data space of array A according to this \nAOV ~7, we follow the approach of [17] and project the orig- inal data space onto the line perpendicular \nto ~. Choosing 77\u00b1 = (2,-1) so that ~7. ~7\u00b1 = 0, we transform the original indices of (i, j) into ~7\u00b1 \n (i, j) = 2 i -j. Finally, to ensure that all data accesses are non-negative, we add m to the new index, \nsuch that the final transformation is from A[i][j] to A[2 * i -j +m]. Thus, we have reduced storage requirements \nfrom n * m to 2 n + m. The modified code corresponding to this mapping is shown in Figure 6. 5.2 Example \n2: Two-Statement Stencil We now consider an example adapted from [12] where there is a uniform dependence \nbetween statements in a loop (see Figures 7 and 8). Letting 81 and 0e denote the schedul- ingfunctions \nfor statements 1 and 2, respectively, we have following schedule constraints: 81(i,j,n,m) -82(i - 1,j,n,m) \n-1 >_ 0 82(i,j,n,m) -81(i,j - 1,n, m) -1 > 0 and the following storage constraints: 02(i -1 + vB,i, \nj + vs,~, n, m) --81 (i, j, n, m) >_ 0 O1 (i ~-VA,i, j --1 q-VA,j, n, m) --02 (i, j, n, 9n) ~ 0 We now \ndemonstrate how to linearize the schedule con-straints. We observe that the polyhedral domain of the \nitera- tion parameters (i, j) has vertices at (1, 1), (n, 1), (1, m), (n, m), 239 A[] [] = new int[n] \n[m] A[]= new ing[n] B[] = new int[n] B = new int for j = 1 to n A[i][j] = B[i-l]+j (Sl) B[i]= A[i][n-i] \n(S2) Figure 12: Original code for Example 4. \\ o ~\"i $ S2 0 sl Figure 13: Dependence diagram for Example \n4. so we evaluate the schedule constraints at these points to eliminate (i, j): 81(1,1, n, m) -- 82(0,1, \nn,m)--i k0 02(1, 1, n, m) -- 81(1, 0, n, m) -- 1 k 0 81 (n, 1, n, m) -82(n -1, 1, n, m) -1 > 0 82 (n, \n1, n, m) -- 81 (n, 0, n, m) -1 k 0 01(1,m,n,m) -82(1 --1,m,n,m) -1 >_ 0 82(1, ra, n,m)- 81(1, m- 1, n,m) \n-1 >_ 0 o~(~,~,n,m) o~(~ -1,.~,~,~) -1 >_ o - 82(n,m,n,m) -81(n,m-1,n,m) -1 >_ 0 Next, we eliminate the \nstructural parameters (n, m). As- suming n and m are positive but arbitrarily large, the do- main of \nthese parameters is an unbounded polyhedron: (n, m) = (1, 1) + j * (0, 1) + k * (1,0), for positive integers \nj and k. We must evaluate the above constraints at the ver-tex (1, 1), as well as the linear part of \nthe constraints for the rays (1, 0) and (0, 1). Doing so yields 24 equations, of which we show the first \n3 (which result from substituting into the first of the equations above): 81(1,1,1,1) --82(0,1,1,1) -- \n1 > 0 81(1, 1, 1, 0) --82 (0, 1, 1, 0) -81 (1, 1,0, 0) + 82 (0, 1, 0, 0) k 0 81(1, 1,0, 1) -- 82(0, 1,0, \n1) -- 81(1, 1,0,0) + 82(0, 1,0,0) > 0 Expanding the scheduling functions as O~(i, j, n, m) = am + b~ \n* i + c~ * j + d~ * n + e~ * m, the entire set of 24 equations can be simplified to: dt = d2 el ~ (32 \nat+b1 +cl-a2-c2+(bl-b2)n-1 kO al +2b~ +cl-as-be-c2-1_>O a2 + b2 + 2c2 -- al -- bl -- cl -- 1 > 0 a2 + \n2C2 --al -- cl + (b2 - bl)n -1 > 0 These equations constitute the linearized schedule constraints. In \na similar fashion, we could linearize the storage con-straints, and then apply Farkas' lemma to find \nthe shortest AOV's of ~TA = gB = (1, 1). Due to space limitations, we do not derive the entire solution \nhere. The code that results after transformation by these AOV's is shown in Figure 9. for j = 1 to n \n A[i] = B+j (SI) B = A[i] (S2) Figure 14: Transformed code for Example 4. The AOV's for A and .B are \n(1,0) and 1, respectively.  5.3 Example 3: Multiple Sequence Alignment We now consider a version of \nthe Needleman-Wunch se-quence alignment algorithm [14] to determine the cost of the optimal global alignment \nof three strings (see Figure 10). The algorithm utilizes dynamic programming to determine the minimum-cost \nalignment according to a cost function w that specifies the cost of aligning three characters, some of \nwhich might represent gaps in the alignment. Using 81 and 82 to represent the scheduling functions for \nstatements 1 and 2, respectively, we have the following schedule constraints (we enumerate only three \nconstraints for each pair of statements since the other dependences fol- low by transitivity): 82(i,j,k,x,y,z) \n-81(i-1,j,k,x,y,z) -1 >_ 0  lot i = 2, j [2, u], k e [2, z] 82(i,j,k,x,y,z) -81(i,j -1,k,x,y,z) -1 \n_> 0 for ~ [2, ~1, j = 2, k e [2, z] 02(i,j,k,x,y,z) -81(i,j,k-1,x,y,z) -1 _> 0 for i [2, x],j [2, \ny],k= 2 82(i,j,k,x,y,z) -82(i-1,j,k,x,y,z) -1 _> 0 for i [3, x], j [2, y], k 6 [2, z] 82(i,j,k,x,y,z) \n-82(i,j -1,k,x,y,z) -1 _> 0 for i [2, x], j e [3, y], k 6 [2, z] 82(i,j,k,x,y,z) -82(i,j,k-1,x,y,z) \n-1 _> 0 for i 6 [2, x], j 6 [2, y], k 6 [3, z] Note that each constraint is restricted to the subset \nof the iteration domain under which it applies. That is, $2 de-pends on $1 only when i, j, or k is equal \nto 2; otherwise, $2 depends on itself. This example illustrates the precision of our technique for general \ndependence domains. The storage constraints are as follows: 82(i --1 + vl,j +vj,k + vk,x,y,z) -82(i,j,k,x,y,z) \n>_ 0 for i 6 [3, x], j 6 [2, y], k 6 [2, z] 82(i + vi,j -1 + vj,k + vk,x,y,z) -- 82(i,j,k,x,y,z) > 0 \nfor i 6 [2, x], j 6 [3, y], k [2, z] 8~(i + vl,j + vj,k- 1 + vk,x,y,z) --82(i,j,k,x,y,z) > 0 for i \n[2, ~], j [2, v], k [3, z] There is no storage constraint corresponding to the depen- dence of $2 on \n$1 because the doma{n Z ofthe constraint is empty for occupancy vectors with positive components, and \noccupancy vectors with a non-positive component do not satisfy the above constraints. That is, for the \nfirst dependence of $2 on $1, the dependence domain is P = {(2, j, k) ]j e [2, y] A k 6 [2, z]} while \nthe existence domain of St isDs~ ----{(i,j,k) l i [1,xlAj 6 [1,y]Ak [1, z]A(i = 1Vj = 1Vk ---- 1)}. \nThen, the domain of the first storage con- straint is Z = {(i, j, k) I (i, j, k) 6 7'A(i--1, j, k)\u00f7~TA \n6 2)Sl }. Now, Z is empty given that ~TA has positive components, be- cause if (i,j, k) 6 P then i = \n2, but if (i - 1,j, k) +ffA Dsl then i -1 + VA,~ = 1, or equivalently i + VA/ = 2. Thus for Z to be \nnon-empty, we would have 2 + VA,i ----2, which con- tradicts the positivity assumption on VA#. The argument \nis analogous for other dependences of $2 on $1. Example2 Speedup Example3 Speedup 40 Transfo'rmed --~--- \nOriginal y\" 3O 25 2o 15 10 5-- 0 i i ~--m 10 20 30 40 56 60 70 Processors Figure 15: Speedup vs. number \nof processors for Example 2. Applying our method for this example yields an AOV of (1, 1, 1). The transformed \ncode under this occupancy vector is just like the original, except that the array is of dimension [imax+jmax][imax+kmax] \nand element [i]~j][k] is mapped to ~imax+i-j][kmax+i-k]. 5.4 Example 4: Non-Uniform Dependences Our final \nexample is constructed to demonstrate the applica- tion of our method to non-uniform dependences (see \nFigures 12 and 13). Let 01 and 02 denote the scheduling functions for statements 5'1 and ,92, respectively. \nThen we have the following schedule constraints: 01(i,j,n) -02(i-1,n) -1 _> 0 02(i, n) -01(i, n -i, \nn) -1 >_ 0 and the following storage constraints: 02(i -1 + VB, n) --01(i, j, n) >_ 0 01(g -~- VA,i, \nn --i \"t- VA,j, n) --02(i, n) ~_ 0 Appljiing our method to these constraints yields the AOV's gA = (1,1)) \nand VB = 1. The transformed code is shown in Figure 14.  6 Experiments We performed preliminary experiments \nthat validate our technique as applied to two of our examples. The tests were carried out on an SGI Origin \n2000, which uses MIPS R10000 processors with 4MB L2 caches. For Example 2, the computation was divided \ninto di- agonal strips. Since there are no data dependences be-tween strips, the strips can be assigned \nto processors with- out requiring any synchronization [12]. Figure 15 shows the speedup gained on varying \nnumbers of processors using both the original and the transformed array. Both versions show the same \ntrend and do not significantly improve past 16 processors, but the transformed code has an advantage \nby a sizable constant factor. Example 3 was parallelized by blocking the computation, and assigning rows \nof blocks to each processor. As shown in Figure 16, the transformed code again performs substan- tially \nbetter than the original code. With the reduced work- ing set of data in the transformed code, the speedup \nis super- linear in the number of processors due to improved caching. 20 Transformed 18 Original $ ~o \nJ 6 j 4 0 I I l I I I 1 I 0 2 4 6 8 t0 12 14 16 Processors Figure 16: Speedup vs. number of processors \nfor Example 3. 7 Related Work The work most closely related to ours is that of [17], which considers \nschedule-independent storage mappings using the Universal Occupancy Vector (UOV). While an AOV is valid \nonly for affine schedules, a UOV is valid for any legal execu- tion ordering. Consequently, sometimes \nthere exist AOV's that are shorter than any UOV since the AOV must be valid for a smaller range of schedules. \nWhile the analysis of [17] is limited to a stencil of dependences involving only one state- ment within \na perfectly nested loop, our method applies to general affine dependences across statements and loop \nnests. Moreover, our framework goes beyond AOV's to unify the notion of occupancy vectors with known \naffine scheduling techniques. Another related approach to storage management for parallel programs is \nthat of [3, 2, 11]. Given an affine sched- ule, [11] optimizes storage first by restricting the size \nof each array dimension and then by combining distinct arrays via renaming. This work is extended in \n[3, 2] to consider storage mappings for a set of schedules, towards the end of capturing the tradeoff \nbetween parallelism and storage. However, these techniques utilize a storage mapping where, in an assignment, \neach array dimension is indexed by a loop counter and is modulated independently (e.g. A [i rood n] [j \nrood m] ). This is distinct from the occupancy vector map- ping, where the data space of the array is \nprojected onto a hyperplane before modulation (if any) is introduced. Tile former mapping-when applied \nto all valid affine schedules- does not enable any storage reuse in Examples 2 and 3, where the AOV did. \nHowever, with a single occupancy vec- tor we can only reduce the dimensionality of an array by one, whereas \nthe other mapping can introduce constant bounds in several dimensions. In the future, we hope to extend \nour method to find multiple occupancy vectors, thereby enabling storage reuse along multiple array dimensions. \nMemory reuse in the context of the polyhedral model is also considered in [18]. This approach uses yet \nanother storage mapping, which utilizes array transformations on the data space to achieve the effect \nof multiple occupancy vectors applied at once. However, the mapping does not have any modulation, so \nit could not duplicate the effect of the (2, 0) occupancy vector we found (for a given schedule) in Example \n1. 241  8 Conclusion We have presented a mathematical framework that unifies the techniques of affine \nscheduling and occupancy vector analysis. Within this tYamework, we showed how to deter- mine a good \nstorage mapping for a given schedule, a good schedule for a given storage mapping, and a good storage \nmapping that is valid for all legal schedules. Our technique is general and precise, allowing inter-statement \naffine de- pendences and efficiently solving for the shortest occupancy vector using standard numerical \nprogramming methods. We consider this research to be the first step towards automating a procedure that \nfinds the optimal tradeoff be- tween parallelism and storage space. This question is very relevant in \nthe context of array expansion, where the cost of extra array dimensions must be weighed against the \nschedul- ing freedom that they provide. Additionally, our framework could be applied to single-assignment \nfunctional languages where all storage reuse must be orchestrated by the com-piler. In both of these \napplications, and even for compil- ing to uniprocessor systems, understanding the interplay be- tween \nscheduling and storage is crucial for achieving good performance. However, since finding an exact solution \nfor the \"best\" occupancy vector is a very complex problem, our method relies on several assumptions to \nmake the problem tractable. We ignore the shape of the data space and assume that the shortest occupancy \nvector is the best; further, we minimize the Manhattan length of the vector, since minimizing the Euclidean \nlength is nonlinear. Also, we restrict the input domain to programs where 1) the data space matches the \niteration space, 2) only one statement writes to each array, 3) the schedule is one-dimensional and affine, \nand 4) there is an affine description of the dependences. It is with these qualifications that our method \nfinds the \"best\" solution. In future work, we aim to relax some of the assumptions about the input domain. \nPerhaps most relevant is the case of arbitrary affine references on the left hand side, since it would \nnot only widen the input domain, but would allow the reduction of multiple array dimensions via application \nof successive occupancy vectors. Many of these extensions are difficult because, in their straightforward \nformulations, the constraints become nonlinear. We consider it to be an open question to formulate these \nextensions as linear pro- gramming problems. It will also be valuable to consider more general storage \nmappings. The occupancy vector method as it stands now can only decrease the dimensionality of an array \nby one, and the irregular shape of the resulting data space could be hard to embed in a rectilinear array \nin a storage-efficient way. However, other storage mappings [11, 18] we discussed also have their limitations. \nThe perfect storage mapping would allow variations in the number of array dimensions, while still capturing \nthe directional and modular reuse of the occupancy vector and having an efficient implementation; it \nshould also lend itself to efficient storage reuse between distinct arrays.  9 Acknowledgements We \nwould like to thank Kath Knobe for her helpful com-ments and suggestions. We appreciate the support of \nthe MARINER project at Boston University for giving us access to its Scientific Computing Facilities. \nThis work was partly supported by NSF Grant CCR0073510, DARPA grant DBT63- 96-C-0036, and a graduate \nfellowship from Siebel Systems. References [1] D. Barthou, A. Cohen, and J. Collard. Maximal static expansion. \nIn Principles of Programming Language% pages 98---106, San Diego, CA, Jan. 1998. [2] A. Cohen. Parallelization \nvia constrained storage map- ping optimization. Lecture Notes in Computer Science, 1615:83-94, 1999. \n[3] A. Cohen and V. Lefebvre. Optimization of stor- age mappings for parallel programs. Technical Report \n1998/46, PRISM, U. of Versailles, 1988. [4] A. Darte, Y. Robert, and F. Vivien. Scheduling and Automatic \nParalIelization. Birkhguser Boston, 2000. [5] P. Feautrier. Array expansion. In A CM Int. Conf. on Supercomputing, \npages 429-441, 1988. [6] P. Feautrier. Datafiow analysis of array and scalar ref- erences. Int. J. of \nParallel Programming, 20(1):23-51, 1991. [7] P. Feautrier. Some efficient solutions to the affine scheduling \nproblem, part I. one-dimensional time. Int. J. of Parallel Programming, 21(5):313-347, Oct. 1992. [8] \nP. Feautrier. Some efficient solutions to the affine scheduling problem, part II. multidimensional time. \nInt. J. of Parallel Programming, 21(6):389-420, Dec. 1992. [9] P. Feautrier, J.-F. Collard, M. Barreteau, \nD. Barthou, A. Cohen, and V. Lefebvre. The interplay of expan- sion and scheduling in paf. Technical \nReport 1998/6, PRISM, U. of Versailles, 1988. [10] F. Irigoin and R. Triolet. Supernode partitioning. \nIn Proc. 15th Annual ACM Symp. Principles of Prog. Lan- guages, pages 319-329, San Diego, CA, Jan. 1988. \n[11] V. Lefebvre and P. Feautrier. Automatic storage man- agement for parallel programs. Parallel Computing, \n24(3-4):649-671, May 1998. [12] A. Lira and M. Lam. Maximizing parallelism and min- imizing synchronization \nwith affine transforms. In Pro-ceedings of the 24th Annual ACM SIGPLAN-SIGACT Syrup. on Principles of \nProg. Languages, Jan. 1997. [13] V. Loechner and D. K. Wilde. Parameterized polyhe- dra and their vertices. \nInt. J. of Parallel Programming, 25(6):525-549, Dec. 1997. [14] S. B. Needleman and C. D. Wunsch. A general \nmethod applicable to the search of similarities in the amino acid sequence of two proteins. Journal of \nMolecular Biology, 48:443-453, 1970. [15] W. Pugh. The Omega test: a fast and practical in- teger programming \nalgorithm for dependence analysis. Communications of the ACM, 8:102-114, Aug. 1992. [16] A. Schrijver. \nTheory of Linear and Integer Program- ming. John Wiley and Sons, New York, 1986. [17] M. M. Strout, L. \nCarter, J. Ferrante, and B. Simon. Schedule-independent storage mapping for loops. In Architectural Support \nfor Programming Languages and Operating Systems, pages 24-33, 1998. [18] D. Wilde and S. Rajopadhye. \nMemory reuse analysis in the polyhedral model. Parallel Processing Letters, 7(2):203-215, June 1997. \n  \n\t\t\t", "proc_id": "378795", "abstract": "<p>We present a unified mathematical framework for analyzing the tradeoffs between parallelism and storage allocation within a parallelizing compiler. Using this framework, we show how to find a good storage mapping for a given schedule, a good schedule for a given storage mapping, and a good storage mapping that is valid for all legal schedules. We consider storage mappings that collapse one dimension of a multi-dimensional array, and programs that are in a single assignment form with a one-dimensional schedule. Our technique combines affine scheduling techniques with occupancy vector analysis and incorporates general affine dependences across statements and loop nests. We formulate the constraints imposed by the data dependences and storage mappings as a set of linear inequalities, and apply numerical programming techniques to efficiently solve for the shortest occupancy vector. We consider our method to be a first step towards automating a procedure that finds the optimal tradeoff between parallelism and storage space.</p>", "authors": [{"name": "William Thies", "author_profile_id": "81100276340", "affiliation": "Laboratory For Computer Science, Massachusetts Institute of Technology, Cambridge, MA", "person_id": "P335208", "email_address": "", "orcid_id": ""}, {"name": "Fr&#233;d&#233;ric Vivien", "author_profile_id": "81408603016", "affiliation": "Universitd Louis Pasteur, Strasbourg, France", "person_id": "PP39034592", "email_address": "", "orcid_id": ""}, {"name": "Jeffrey Sheldon", "author_profile_id": "81100419803", "affiliation": "Laboratory For Computer Science, Massachusetts Institute of Technology, Cambridge, MA", "person_id": "P334852", "email_address": "", "orcid_id": ""}, {"name": "Saman Amarasinghe", "author_profile_id": "81100533031", "affiliation": "Laboratory For Computer Science, Massachusetts Institute of Technology, Cambridge, MA", "person_id": "PP14184970", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/378795.378852", "year": "2001", "article_id": "378852", "conference": "PLDI", "title": "A unified framework for schedule and storage optimization", "url": "http://dl.acm.org/citation.cfm?id=378852"}