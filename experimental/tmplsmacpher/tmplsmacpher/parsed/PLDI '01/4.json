{"article_publication_date": "05-01-2001", "fulltext": "\n On the Importance of Points=To Analysis and Other Memory Disambiguation Methods For C Programs Rakesh \nGhiya Daniel Lavery David Sehr Intel Corporation Intel Corporation Intel Corporation 2200 Mission College \nBlvd 2200 Mission College Blvd 2200 Mission College Blvd Santa Clara CA, 95052 Santa Clara CA, 95052 \nSanta Clara CA, 95052 (408) 765-5807 (408) 765-0884 (408) 765-5372 rakesh.ghiya@intel.com daniel.m.lavery@intel.com \ndavid.c.sehr@intel.com ABSTRACT In this paper, we evaluate the benefits achievable from pointer analysis \nand other memory disambiguation techniques for C/C++ programs, using the framework of the production \ncompiler for the Intel\u00ae Itanium TM processor. Most of the prior work on memory disambiguation has primarily \nfocused on pointer analysis, and either presents only static estimates of the accuracy of the analysis \n(such as average points-to set size), or provides performance data in the context of certain individual \noptiraizations. In contrast, our study is based on a complete memory disambiguation framework that uses \na whole set of techniques including pointer analysis. Further, it presents how various compiler analyses \nand optimizations interact with the memory disambiguator, evaluates how much they benefit from disambiguation, \nand measures the eventual impact on the performance of the program. The paper also analyzes the types \nof disambiguation queries that are typically received by the disambiguator, which disambiguation techniques \nprove most effective in resolving them, and what type of queries prove difficult to be resolved. The \nstudy is based on empirical data collected for the SPEC CINT2000 C/C++ programs, running on the Itanium \nprocessor. 1. INTRODUCTION Pointer analysis has recently been an active topic of research. Its goal is \nto compute potential targets of pointers in the program, and enable more accurate disambiguation of pointer-based \nindirect memory references. Recent research has led to the development of efficient pointer analysis \ntechniques that can effectively analyze very large programs in reasonable time [5,6,7]. Most researchers \nin this area have used the metric of average points-to set size to evaluate the effectiveness of the \nanalysis. While this metric provides a good measure of the static results, it does not reflect the actual \nbenefits achievable from the analysis information in terms of program performance. To evaluate pointer \nanalysis in this context, one requires a framework where points-to information is used by all compiler \nanalyses and optimizations Permission to make digital or hard copies of all or part of this work for \npersonal or classroom use is granted without fee provided that copies are not made or distributed for \nprofit or commercial advan-tage and that copies bear this notice and the full citation on the first page. \nTo copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific \npermission and/or a fee. PLDI 2001 6/01 Snowbird, Utah, USA &#38;#169; 2001 ACM ISBN 1-58113-414-2/01/06,..$5.00 \nthat can benefit from it. Previous work has focused on individual optimizations like parallelization \n[13], common subexpression elimination [14], and redundant load removal [15] to evaluate the benefits \nof points-to information. Cheng et. al conducted a more thorough study [ 16], but for a simulated processor \nand a different set of programs. A detailed study of the overall benefits of pointer analysis has not \nbeen undertaken on real hardware. That is the focus of this paper. Pointer analysis itself is a component \nof the memory disambiguation framework of an optimizing compiler. A memory disambiguator uses a variety \nof techniques like symbol table information, address-taken analysis, base-offset calculations, and use-def \nchains, in addition to pointer analysis. Thus, another interesting data point in evaluating the effectiveness \nof pointer analysis is how often the disambiguator needs to use points-to information, and how crucial \nare those disambiguation queries. This would reflect the added benefits of pointer analysis over simpler \nheuristics and enable a comparison of its contributions versus the cost of the analysis. The memory disambiguation \nframework implemented in the Intel Itanium compiler [ 11,12], provides the required infrastructure for \nsuch a study. All compiler analyses and optimizations that need to disambiguate memory references query \nthe disambiguator. The disambiguator, in turn, uses information from pointer analysis, address-taken \nanalysis, array dependence analysis, language semantics and other sources to answer the query. It also \nprovides a mechanism to translate queries for low-level memory references from optimizations like instruction \nscheduling and software-pipelining into corresponding high-level constructs. The main contributions of \nthis paper include: 1. An optimizing compiler framework that brings together all the clients and sources \nof memory disambiguation. 2. A detailed study of the overall benefits of pointer analysis and its eventual \nimpact on program performance. 3. A comprehensive analysis of the effectiveness of various disambiguation \ntechniques, providing insight into which techniques are most often used. 4. A detailed analysis of the \ncases that prove difficult to disambiguate.  47 5. Experimental evaluation based on data collected from \nthe industry standard SPEC CINT2000 benchmarks running on the Itanium processor. The rest of this paper \nis organized as follows. In section 2, we introduce the overall Itanium compiler framework, presenting \nthe various compiler analyses and optimizations. In section 3, we describe in detail our memory disambiguation \nframework and how it interfaces with its clients. Section 4 provides details on the pointer analysis \nimplemented in our compiler. We present the empirical data on our study in section 5, along with suitable \nobservations. Section 6 discusses related work, and finally we draw our conclusions in section 7. 2. \nINTRODUCTION TO THE INTEL\u00ae ITANIUMTM COMPILER The Intel Itanium compiler is designed to extract the full \npotential of the Itanium architecture [10]. It incorporates a number of leading edge technologies, including \nprofile guidance, multi-file interprocedural analysis and procedure integration, global code scheduling, \nand a large number of optimizations that make use of speculation and predication. One of the key goals \nof the compiler is to eliminate or hide memory latency. One part of this is eliminating as many memory \nreferences as possible and taking advantage of the Itanium processor's large register files, Another \npart is scheduling to hide latency. Register variable promotion and scheduling rely intimately on the \nbest possible memory disambiguation technology. Researchers and compiler writers have developed numerous \ntechniques to prove memory locations independent or non-overlapping. The Itanium compiler incorporates \nthe best- known practical techniques for points-to analysis and data dependence analysis. Points-to computation \nis used as an input to disambiguation, but also performs several other functions. It may convert indirect \nfunction calls to direct function calls, sharpening the analysis and exposing opportunities for procedure \nintegration. Points-to information can also be used to build the basis sets for MOD/REF analysis, which \ncomputes the set of locations modified/referenced by each function in the program. The compiler currently \nperforms limited forms of MOD/REF analysis, such as for standard library functions (e.g., strlen). Disambiguation \nand optimizations interact in many ways, so an effective disambiguator needs to incorporate information \nfrom a variety of semantic levels of the intermediate language (IL). For instance, generating efficient \ncode for the register indirect addressing requires lowering to base and offset early in the compilation. \nDoing so naively may make disambiguation more difficult by obscuring such simple facts as two scalar \nvariables can never conflict. Therefore the disambiguator needs to retain \"high- level\" information about \nstorage locations. Relying solely on high-level information, though, may result in missed information \nas well. For example, if the program contains pointer arithmetic such as the following fragment, we need \nlowered addressing and constant propagation to prove that we can registerize s.b across the store whenever \ni is zero. struct { int a, b; } s; int *p = &#38;s.a; s.b = O; * (p + i) = I; .o. = s\u00b0b;  The interprocedural \noptimizer performs inlining and partial inlining of function bodies into call sites. After either optimization, \npost-inlining cleanup performs forward substitution of variables and indirect to direct reference conversion. \nThis is particularly important for Fortran and C++ by-reference parameters that can become direct references \nafter inlining. This has implications for disambiguation that will become apparent in later sections. \nThe clients of the disambiguator are the optimization and code scheduling modules in the compiler, including \npartial redundancy elimination (PRE) [21, partial dead store elimination (PDSE), dead code elimination, \nstructure copy optimization, the global code scheduler [3], the local scheduler, and the software pipeliner. \nPRE uses the disambiguator to determine if a store kills an available load, while PDSE wants to know \nif a load kills a potentially dead store. Removal of unnecessary loads and stores also enables many other \noptirnizations that operate on temporaries (virtual registers), for example copy propagation or recognition \nof induction variables. Dead code elimination removes stores to local variables that are never read again \nfor the remainder of the function. The local and global schedulers and the software pipeliner query the \ndisambiguator to determine if a load and store or two stores can be reordered. The software pipeliner \nrequests information about both loop-independent and loop-carried dependences, while the other schedulers \nquery only about loop- independent dependences. 3. THE DISAMBIGUATOR FRAMEWORK As described earlier, \nthe disambiguator needs to retain high-level information about memory references. Many of the optimizations \nthat rely on memory disambiguation occur in the compiler backend. Typically, after the program representation \nis lowered and optimizations are performed, much of the source-level information is lost and the code \nis transformed in ways that make it more difficult for the compiler to perform memory disambiguation. \nTo solve this problem, the Itanium compiler maintains a link from each load or store to a high-level \nsymbolic representation of the memory reference and other information that is crucial for disambiguation. \nWe call our disambiguator DISAM, which stands for DISambiguation using Abstract Memory locations. 3.1 \nAbstract Memory Locations DISAM decomposes the storage space (memory and registers) into a set of abstract \nstorage locations called LOCs. There are different types of LOCs representing global variables, local \nvariables, formal and actual function parameters, functions, registers, and dynamically allocated objects. \nEach LOC represents a storage object that is independent of all other storage objects. LOCs are part \nof the symbol table and have links to the symbol table information about the variables that they represent. \nFor dynamically allocated objects, the LOC has a link to the call site in the source where the object \nwas allocated. LOCs can be grouped together in sets called LOC sets. For disambiguation query is trivial. \nIn contrast, the cost to intersect example, the set of memory locations that could be accessed points-to \nsets or to determine the base and offset for an address through a pointer is represented using a LOC \nset. References to expression is larger. As soon as definite dependence o$ the same memory objects can \nbe detected by intersecting LOC independence is determined, disambiguation stops and the result sets. \nis returned. 3.2 Retention of Source Level Information Each memory reference is linked to source-level \ninformation through a DISAM token, which provides access to all the information necessary to perform \nmemory disambiguation. This information includes a high-level symbolic r~presentation of the memory reference, \ntype information, and a link to an array data dependence graph for disambiguation of different elements \nof the same array. For direct memory references, the disam token contains a LOC representing the memory \nobject that is accessed. For indirect references, the token contains a LOC representing the pointer, \nand a dereference level. The disambiguator must use the results of points-to analysis to determine the \nset of locations that could be accessed by dereferencing the pointer. While the IL representation of \nthe memory reference and its associated addressing changes greatly as a result of IL lowering and optimizations, \nit remains linked to its DISAM token, and the LOC representation preserves the high-level source-like \nrepresentation of the memory reference. DISAM tokens are created earlyin the compiler when the high- \nlevel information is still available. DISAM tokens are part of the memory referencing IL, and as such \nare automatically carried along whenever a memory reference is moved or copied. There is a small amount \nof DISAM token maintenance that must be done. For example, if the compiler creates a memory reference \nthat did not exist in the source program (e.g. stack locations for parameter passing) a token is created \nto represent the memory reference. As described earlier, forward substitution of address expressions \ncan cause an indirect reference to become direct. The DISAM tokens are updated after forward substitution \nto reflect this. Overall, DISAM token maintenance is a relatively simple task. 3.3 Performing Memory \nDisambiguation Figure 1 shows a block diagram of the various modules involved in memory disambiguation \nand the interfaces between them. Each arrow is labeled with the type of data structure that is used for \nthe interface. The disambiguator module receives queries from a client, consults the symbol table, array \ndata dependence graph and points-to information if necessary, performs memory disambiguation, and returns \na disambiguation result. The following is an outline of the disambiguation methods employed. A full description \nof the details of each method is beyond the scope of this paper, but this gives th e reader the flavor \nof what is done and sets up the various categories of disambiguation methods for the experimental results. \nThe disambiguator currently applies the methods in the sequence presented in the subsequent paragraphs. \nThis ordering is predominantly driven by the compile time cost of the query portion of the method. For \nexample, determining if the address of a global variable is taken anywhere in the program requires sophisticated \ninterprocedural analysis.' However the cost to lookup this information as an attribute on a variable \nfor each PA\u00b0nl~lt~iTs\u00b0 L I S ymbolTable Y ~\u00a2-,_~L ArrayData ~ T F I Dependence 1~ J Disambiguator ] Graph \nI R Vl I R: DISAM result L: LOC set [ ......... S: Symbol table object I Clients Figure 1 Disambiguation \nSystem Compiler-generated references can often be easily disambiguated from all other memory references. \nFor example, references to read-only storage areas can be disambiguated from all stores. Registers are \nspilled to a special area on the stack, so these loads and stores can easily be disambiguated from each \nother and from all memory references that are not spills. The disambiguator can trivially prove the above \ntypes of references independent; hence they are not considered in the statistics we present later. If \nboth memory references are direct (note that direct vs. indirect is easily determined using the dereference \nlevel in the DISAM token), their LOCs are compared to determine whether or not the same memory object \nis accessed. If the same object is accessed, the disambiguator then attempts to determine if overlapping \nportions of the object are accessed.\" For example, the array data dependence information is used to determine \nif the same array element is accessed and structure type information from the symbol table is used to \ndetermine if overlapping fields of a structure are accessed. If at least one of the memory references \nis indirect, the disambiguator first attempts to prove independence without knowing where the indirect \nreferences point to. For example, an indirect reference off an unmodified parameter could not possibly \naccess a stack allocated local variable from the function in which the two references appear. Also, an \nindirect dereference cannot possibly access a local variable that has not had its address taken. When \nthe compiler is run with interprocedural optimization, it has the ability to automatically detect that \nit is seeing the whole program. That is it can detect whether or not there are calls to functions that \nit has not seen and does not know the behavior of. When the compiler can see the whole program, the disambiguator \nknows that an indirect reference cannot possibly access a global variable that has never had its address \ntaken. If simple rules such as those above do not allow the disambiguator to prove independence of the \nmemory references, 49 s2. So, the points-to information we collect, indicates both sl.p the results \nof points-to analysis are consulted. The compiler and s2.p as pointing to x, and sl.q and s2.q to y. \nWith the contains two points-to analysis phases: an intraprocedural (local) structure collapsing approach, \nwe would have both sl.p and sl.q analysis and a flow-insensitive interprocedural analysis. The pointing \nto x, and similarly both s2.p and s2.q pointing to y. disambiguator checks the results of the local points-to \nanalysis first. If that yields a maybe dependent result then the interprocedural points-to analysis is \nconsulted. With the exception of the compiler-generated references, the disambiguation methods discussed \nabove, all make use of high- level symbol table information and analyses. In cases where the memory references \ncannot be disambiguated by the above methods, the disambiguator resorts to a method that utilizes the \nlowered addressing. It analyzes the address expression of each memory reference and tries to determine \na base and offset. If successful it compares the base and offset for the two memory references. If they \nhave the same base, the disambiguator can use the offsets and sizes of the memory references to determine \nwhether or not they overlap. This simple base and offset analysis is useful for two memory references \noff the same pointer (with the pointer having the value at the two references), references whose addresses \nhave been modified by adding offsets or performing pointer arithmetic in an unstructured way, and array \naccesses with constant subscripts. Finally, for ANSI C compliant programs the disambiguator can perform \ntype-based disambiguation based on the ANSI type aliasability rules. For example, under the ANSI rules, \na reference to an object of type float cannot overlap with a reference to an object of type integer. \nThis method is applied last because it is enabled by a user assertion that the program complies with \nthe ANSI C standard. Applying the other methods first gives the compiler the opportunity to detect potential \ncases where the user may make the assertion for a program that is not truly compliant. In these cases \nit can ignore the assertion, perhaps avoiding a runtime failure. 4. POINTS-TO ANALYSIS FRAMEWORK In our \ncompiler framework, we have implemented the flow-insensitive interprocedural pointer analysis proposed \nby Andersen[4], which we call WPT (whole-program interprocedural points-to analysis). To counter its \ncubic time complexity, we have augmented the analysis with off-line variable substitution[5], which pre-computes \npointers with identical points-to sets, and online cycle elimination[6], which identifies cycles in the \npoints- to graph and collapses them as the analysis proceeds. All these approaches have been proposed \nin the literature. We differ from published work in our handling of structure fields. Standard practice \nis to collapse the entire structure, and let the structure name represent all its fields. This leads \nto very imprecise analysis, as points-to sets of distinct fields of a structure can no longer be distinguished. \nIn our approach we distinguish between distinct fields of a structure type, but do not distinguish between \nits individual instances. For example, consider the following code fragment: struct foo {int *p; int \n*q; } sl, s2; int x,y ; sl.p = &#38;x; s2. q = &#38;y;  With our approach, we distinguish between fields \np and q of structure type foo, but not between its individual instances, sl and In our experience with \nanalyzing large programs, our approach provides substantially more accurate points-to information. This \noccurs because pointer analyses typically are not able to distinguish between different structure instances, \nbecause many structures of a given type tend to be allocated at a given malloc- site. With the structure \ncollapsing approach, we also lose the distinction between fields and the points-to sets of the fields. \nFor example, consider a structure type with one field as a function pointer, and another as integer pointer. \nIf we collapse them, the points-to sets get merged, resulting in very imprecise information. Our approach \nsolves this problem. For correctness in the presence of type-casting, when we encounter a composite structure \nassignment between different structure types, we analyze it as a sequence of assignments between their \ncorresponding/overlapping fields. Note that in the case of a structure copy assignment of the form *a \n= *b, we only need to look-up the underlying type signatures of *a and *b, and not their points-to sets, \nthus reducing the analysis overhead. Another added feature of our points-to implementation is identification \nof malloc-like functions. Typica!ly, programmers use a wrapper function, like my_malloc, for dynamically \nallocating memory via the library call malloc, to check for potential errors and for modularity. This \nis even more evident in C++ programs, where overloaded new operators are common. Points-to analysis abstracts \neach static malloc-site in the program as a distinct memory location. With the use of wrapper functions, \nthe analysis sees only one static malloc-site, and is not able to distinguish between memory locations \nallocated through different calls to this function. To solve this problem, we try to identify if the \nwrapper function behaves like malloc, in that in each invocation it returns a fresh memory location. \nThis is achieved by building the SSA (static single assignment) representation of the function, and walking \nback the use-def chain starting from the return value of the function. If the chain terminates at an \nunconditional rnalloc call, we know the wrapper function to be malloc-like. We also check that the wrapper \nfunction does not store the address of the allocated memory to any variable/structure field that can \nbe live out of the function, We construct the SSA representation and perform the safety checks in the \nabsence of points-to information, and hence need to be conservative in the presence of indirect references. \nStill, we are able to identify malloc-like functions in several benchmarks. That substantially improves \nthe accuracy of points-to information and effectiveness of memory disambiguation. Our final modification \nto the points-to analysis is that we analyze the assignment statements in the program in a particular \norder. We build a directed assignment graph of the program, where an assignment l loc = r loc, is represented \nby adding a directed edge from the node representing r loc to the node representing l loc. Next, we perform \na topological sort on the nodes in the graph. The analysis then proceeds by visiting the nodes in the \ntopological order. When a given node representing x loc is being visited, all assignments where x loc \nappears on the right hand side are analyzed. The analysis continues till all nodes are visited, and iterates \nuntil a fixed-point solution is reached. For example, consider the analysis of the following set of assignments: \n[(p = q), (q = r), (r = &#38;x)}. For this set, the topological order of the nodes in the assignment \ngraph is (&#38;x, r, q, p), and the assignments will be analyzed in the following order: [(r = &#38;x), \n(q = r), (p = q)}. The analysis of assignments in the original order will require four iterations, as \ncompared to only two using the sorted order. In the presence of indirect references, we cannot arrive \nat the best topological order of assignments, but can still improve upon the original sequence. For our \nbenchmark suite, we noticed up to a 50% reduction in the number of iterations with the sorted assignment \nsequence. Finally, note that off-line variable substitution also requires the topologically sorted assignment \ngraph, so we do not incur any additional overhead for this improvement. With the above modifications, \nwe were able to implement a very efficient and effective points-to analysis in our compiler framework. \nWe have found that the time spent in points-to analysis is small compared to the overall compilation \ntime.  4.1 Local Points-to Analysis To achieve better disambiguation in the absence of interprocedural \nanalysis, we have implemented an intraprocedural version of points-to analysis, we term local points-to \nanalysis (LPT). For LPT, we use the same analysis engine as for WPT. Since LPT only sees assignments \nwithin the analyzed procedure, we need to make conservative assumptions about the points-to sets of global \nvariables and formal parameters at procedure entry. Additionally we need to conservatively incorporate \nthe effect of function calls on the points-to relationships. We use the concept of a symbolic location, \nnloc, which stands for non-local location. This is used to represent all locations in the program, excluding \nthe local variables of the analyzed procedure. All global variables and formal parameters of the procedure \nare initialized to be pointing to nloc at the onset of the analysis. This initialization embodies the \nassumption that at the entry of the analyzed procedure, these variables can point to any location in \nthe program, except the set of locations created after the entry to the given procedure. This set includes \nall non-static local variables, and dynamic memory allocated inside the procedure, represented by static \nmalloc-sites within the procedure. Additionally,nloc is initialized to Point to itself, because locations \nrepresented by nloc can point to each other. After this initialization, the analysis proceeds in the \nsame iterative fashion on the set of assignments in the procedure as for WPT. On reaching a fixed-point, \nwe mark all local locations accessible from a global variable or an actual parameter via pointer indirection, \nas address-escaped. The address-escaped locations can be both modified/referenced outside the procedure \nthrough function calls. Thus any location visible outside the analyzed procedure can point to a location \nwhose address has escaped and vice versa\u00b0 To take this into account, we include the symbolic location, \nnloc, in the points-to set of each address-escaped location. Furthermore, nloc is considered to additionally \nrepresent all address-escaped locations. Since points-to sets have been updated, LPT is performed again. \nIdentification of address-escaped locations and LPT is iterated until a fixed-point is reached. Typically, \nit takes fewer than three iterations. LPT is performed on the SSA representation of the procedure. This \nenables it to achieve limited flow-sensitivity: only for local pointer variables whose address is not \ntaken and are replaced with SSA temporaries. In our framework, LPT is performed even when WPT has been \nconducted. This is because LPT can sometimes provide sharper information, as it is performed after inlining \nand SSA construction, providing the benefits of both context- and flow-sensitivity in a limited fashion. \n5. EXPERIMENTS Using the Itanium TM compiler and an Itanium-based system, we have collected data on the \ntwelve C/C++ programs from the SPEC CINT2000 benchmark suite. The Itanium processor [9] contains two \ninteger units, two memory/ALU units, and three branch units. Integer multiplies and divides can be executed \non the processor's two floating-point units, adding additional integer throughput for some programs. \nThe processor has a three-level on-package cache .......... hierarchy. The highest optimization levels \npossible are used to generate the binaries for the experiments in this paper. The SPEC CINT2000 benchmarks \nare aggressively compiled using the switches that are used for a \"peak\" SPEC build, including all of \nthe optimizations and analysis described in sections 2-4. The compiler's support for data speculation \nis turned off for these experiments because the focus of this paper is on traditional compile-time memory \ndisarnbiguation without hardware data speculation support. We have instrumented the disambiguator to \ncollect data on the characteristics of the memory references and points-to sets, the number of queries, \nand the reason for each disambiguation result. Switches have been added to turn on and off the individual \ndisambiguation methods, so that we could see their effect on performance.  5.1 Memory Reference Characteristics \nTable 1 shows for each program, the percentage of static memory references that are direct references \nto local variables, global variables, and indirect references via pointers. The majority of memory references \nare either accesses to global variables (164.gzip, 186.crafty, 256.bzip2), or indirect pointer references \n(176.gcc, 181.mcf, 197.parser, 253.perl, and 254.gap). With the exception of 252.eon and 255.vortex, \nwe do not see many references to local variables, as only accesses to local arrays and address-taken \nlocal scalars are considered memory references. Other local variables are promoted to registerizable \ntemporaries. The address of a local variable is typically taken to effect call-by- reference semantics. \nThe address-exposure is no longer required if the given call is inlined and forward substitution is applied \n(section 2). This further reduces the number of address-taken local variables. Forward substitution also \neliminates indirect references off the formal parameters in the inlined copy of the callee function. \nThis effect is most pronounced for 252.eon (the only C++ program in the benchmark suite), where indirect \nreferences off the C++ \"this\" pointer in the inlined callee, become direct references in the caller. \n51 Table 1 Program Memory Reference Characteristics Program I Local Global Ind Avg Total % % % SetSize \nQueries 164.gzip 7 84 9 ....2.4 ..... 26118 '175.vpr 16 39 45 i.3 40093 176'igcc ......... 8 31 '61 2'2.1 \n1237456 181.mcf 8 11 80 1.3 10195 186.craRy 4 87 9 3.7 321026 197.parser 7 39 5 6.9 67642 252.eon .... \n27 40 '33 147.7 507662 1253.perl ....... 6 36 58 427.3 1192815 254.gap 4 22 ......74 196.3 286053 255.voaex \n34 22 44 39.3 405790 256.bzip2 ..... 15 67 18 1.00 13544 300.twolf ..... 2 46 52 3.4 443028 The colum \nlabeled \"Avg Set Size\" in Table 1 shows the average size of the points-to sets for the indirect references \nin each program as determined by our interprocedural points-to analysis (WPT). It is reasonably small \nfor the majority of the benchmarks, with the exception of 252.eon, 253.perl and 254.gap. The loss of \naccuracy in the first two benchmarks occurs due to the presence of indirect calls, with numerous potential \ntarget functions (assigned to an array of function pointers). This forces WPT to consider all possible \nassignments between the formals of the numerous target functions and the actuals at call-sites, resulting \nin loss of accuracy. The analysis for 252.eon can be improved by more accurate handling of virtual function \ncalls. The benchmark 254.gap primarily uses dynamically allocated structures. However, the memory allocating \nroutine for this benchmark, called NewBag, uses free lists and complex pointer arithmetic, and cannot \nbe automatically identified as a malloc function. Thus WPT cannot distinguish between different structures, \nand provides very inaccurate points-to information. The use of free lists maintained with global pointers \nwould inhibit even a flow- and context-sensitive analysis from identifying that each invocation of the \nfunction NewBag returns a disjoint piece of memory. Thus the loss in accuracy is due to poor basis information \navailable to the points-to analysis, and not because of lack of flow- or context-sensitivity. The last \ncolumn in Table 1 shows the total number of distinct disambiguation queries for each benchmark. Throughout \nthis paper, each pair of memory references is accounted for only once, irrespective of the number of \ntimes the disambiguator is queried with a given pair of references. The filtering out of repeated queries \nis done using a hash table mechanism. Note that we start accounting for unique memory references after \ninlining (this is when the DISAM tokens are created), so the memory references in different inlined copies \nof a given function are considered unique. Compilation of 176.gcc and 253.perl generates the largest \nnumber of unique queries, approximately 1.2 million each. Thus the compile time cost for each query is \nimportant. NIndependent []Maybe []Dependent t 100% Itl 90Ol, O) 80\u00b0/* 0 700 0 60 \u00b0./0 s0o/. 0J 40% o~ \n30Olo \u00a2: ~d 20% 10% o% d Benchmark Figure 2 Disambiguation Result Summary 5.2 Analyzing the Disambiguation \nQueries We now focus on the characteristics of the disambiguation queries received from various compiler \nphases, and the effectiveness of our disambiguation techniques in resolving them. Figure 2 shows, for \neach program, the percentage of unique disambiguation queries for which the disambiguator returns definitely \nindependent, definitely dependent, and maybe dependent. The three categories add up to 100%. Over all \nthe queries received for the same pair of memory references, the best result (definite is better than \nmaybe) is recorded. For example, the simple base and offset analysis is used only after the addresses \nare lowered. Thus for a pair of references that requires this method to determine independence, the disambiguator \nwould return a maybe result before the address-lowering phase, followed by an independent result thereafter. \nThe definitely dependent results are either determined by the direct method or by simple base and offset. \nThe \"average\" bar on the chart shows that on average, the disambiguator returns an independent result \nfor more than 85% of the queries, indicating the effectiveness of our suite of disambiguation techniques. \n5.2.1 Analyzing the Independent Queries In this section, we study the relative contribution of different \ndisambiguation methods described in section 3.3, in resolving disambiguation queries. In Figure 3, the \nportion of each bar representing the definitely independent queries in Figure 2, is expanded and scaled \nto 100%. For a given bar, each section represents the percentage contribution of a particular method \nin resolving queries. Note that the percentage basis is the total number of queries proven independent,and \nnot the total number of queries received. The accounting is done by applying the methods in a specific \norder and crediting the first method that determines independence. For these experiments, the methods \nare applied in an order that is slightly different from that described in section 3.3. We believe this \nrevised order gives better insight into the real benefits of the disambiguation methods. The methods \nare ordered according to \u00b0 100% 90% 80% 0 \"E 70% 60% \"0 50% 40% L2 30% 20% 10% 0% Benchmark J Figure \n3 Breakdown of Independent Queries by Method two criteria. The first is whether the method is used by \ndefault in the compiler without interprocedural analysis. This gives insight into the level of disambiguation \nthat can be achieved without user assertions or interprocedural alias/address analysis. The second criterion \nis the complexity of the method. The rationale here is that the more complex methods should be credited \nonly for queries where that method is really needed. Thus for example, the reader can see the additional \nbenefit provided by interprocedural points-to analysis (WPT) over the simpler methods that would normally \nbe implemented in a compiler before WPT. Type-based disambiguation is the exception here. Even though \nit is simpler than WPT, it requires a user assertion. We believe it is interesting to see how much benefit \nthe user assertion provides over what the compiler analyses can determine on their own. The order of \nthe methods is as follows: 1. direct: Disambiguation of direct references, either different memory objects \nor different parts of the same memory object (not including array element analysis). 2. sbo indirect: \nSimple base and offset analysis, and simple rules to disambiguate indirect references from direct references. \n 3o array: Array data dependence analysis to disambiguate different elements of the same array. 4. LPT: \nIntraprocedural points-to analysis. 5. global: Disambiguation of an indirect reference from a direct \nreference to a global vafable that has not had its address taken.  6. WPT: Interprocedural flow-insensitive \npoints-to analysis. 7. type: type-based disambiguation. Methods 1-4 are enabled at all optimization \nlevels. Methods 5 and 6 are applied only at the highest levels of optimization when multifile interprocedural \nanalysis is enabled. We can make several important observations from the data presented in Figure 3. \nFirst, simple techniques like direct and sbo_indireet, which do not require sophisticated program analysis, \nresolve over 60% of the queries determined independent, on average. The direct technique is especially \neffective for the benchmarks 164.gzip, 186.crafty, 252.eon, and 256.bzip2. This is consistent with the \nfact that the majority of memory references for these benchmarks fall into the direct reference category \n(Table 1). For all benchmarks except 186.crafty, nearly 100% of the queries resolved by the direct method \nare for two references to different memory objects. The preservation of high-level information makes \nresolution of these queries very easy. The LOCs are simply compared to determine independence. For 186.crafty \nabout 25% of the queries resolved by the direct method are to different fields of the same (statically \nallocated) structure. Structure type information accessible from the DISAM token is used to determine \nthe position of the field accessed within the structure. The sbo indirect disambiguation technique makes \nconsistent contributions across all benchmarks. It is useful in disambiguating accesses to different \nStlUCture fields with the same base pointer. This case arises frequently in loops traversing a linked \ndata structure (175.vpr, 176.gcc, and 181.mcf), where the loop body contains accesses to different fields \nwith respect to the navigating pointer: for example, the memory references t->item and t->next in the \nfollowing code fragment. while (t != NULL) { t->item = t->item + i; t = t->next; }  Local points-to \nanalysis (LPT) makes visible contributions for 175.vpr, 176.gcc, 197.parser and 254.gap. In the former \ntwo benchmarks, several procedures initialize locally declared pointers via explicit calls to malloc \n(as opposed to via wrapper functions). Indirect references off these pointers are then easily disambiguated \nagainst other memory references, using their local points-to sets consisting of the distinct malloc-sites. \nIn 197.parser and 254.gap, local points-to succeeds in disambiguating accesses to local pointers used \nto navigate arrays, like pointers p and q in the code fragment below: p = &#38;a[i]; q = &#38;b[j]; while \n(...) { *p = *q + i; p++; q++; }  Array dependence analysis (array) does not make a significant contribution \nto independent queries in the static count for the benchmarks with many arrays like 164.gzip and 256.bzip2. \nHowever, it does prove to be crucial in the runtime performance context because disambiguation of a few \narray-based memory references can enable optimization and scheduling of a critical loop. Array dependence \nanalysis is important statically for 181.mcf. This benchmark contains loops that go through arrays of \narcs and nodes. The software pipeliner queries the disambiguator about loop-carried dependences for some \nof these loops and the array data dependence analysis is able to determine that a new array element is \naccessed for each iteration. Address-taken analysis for global variables (global) proves effective across \nthe majority of the benchmarks. In the given benchmark set, global variables are mostly scalars of integer \nor pointer type, and their address is not typically taken as they can be directly accessed in any section \nof the program. The technique is less effective for 181.mcf with only 11% of memory references as global \naccesses, and for 253.perl, which has address-taken attribute set on several global variables. Interprocedural \npoints-to analysis (WPT) is most effective for the benchmarks 175.vpr, 181.mcf, and 300.twolf. These \nbenchmarks use structures with pointer fields, and the different fields point to dynamically allocated \narrays associated with distinct static malloc- sites. The majority of memory references in these benchmarks \ninvolve indirect accesses to these disjoint arrays, which can be accurately disambiguated. Both 175.vpr \nand 300.twolf use wrapper functions for memory allocation. Our analysis is able to identify them as malloc \nfunctions, leading to accurate points-to information. Otherwise all pointers would be reported as pointing \nto a single malloc site, providing almost no disambiguation. Also note that our strategy of distinguishing \nbetween distinct fields of a given structure type proves critical to obtaining accurate disambiguation \nfor this benchmark set. The contribution of WPT for 176.gee and 256.bzip2 is also attributable to identification \nof dynamically allocated arrays with distinct malloc-sites. For 252.eon and 254.gap, we have very inaccurate \npoints-to information with average points-to set size over 100, and subsequently little contribution \nfrom WPT. However, for 253.perl, even with an average set-size of 427, WPT is responsible for over 20% \nof independent queries. The majority of these queries involve disambiguation of address-taken global \nvariables against indirect pointer references. On the contrary, for 197.parser, even though the set-size \nis small (6.92), the WPT- based disambiguation is ineffective because the majority of pointers in the \nprogram have the same points-to set. Thus average points-to set size is not always a reliable indicator \nof the usefulness of points-to information. Finally, note that simpler techniques like LPT and address-taken \nanalysis for globals (global), steal a significant number of independent queries that would otherwise \nbe attributed to WPT. Thus, disambiguation frameworks that do not use the simpler techniques, may overstate \nthe added benefits of using interprocedural points-to analysis[ 14, 16]. Type-based disambiguation makes \nsignificant contributions for 181.reef and 254.gap. The two benchmarks require frequent disambiguation \nof pointers against objects of types long and short, respectively. The technique also pays off for 176.gcc, \n197.parser, and 253.perl. Again the contribution comes from queries disambiguating pointers against scalar \nobjects of types integer and float. Overall, one can notice that each disambiguation technique pays off \nfor one or another benchmark. The technique that proves most effective depends on the memory reference \nmix of the program, and the type of queries posed by the various optimizations in the compiler. This \ndepends on the regions of the program considered more important by the optirnizations based on program \nstructure and profile information. Thus, our strategy of employing a suite of disambiguation techniques \nis a viable approach to the problem. 5.2.2 Analyzing the Maybe Dependent Queries In this section, we \nanalyze in detail the cases for which our disambiguator reports a maybe dependence. In Figure 4, we highlight \nthe maybe dependent portion of the bars shown in Figure 2. As opposed to Figure 3, each bar in Figure \n4 shows the percentage of maybe dependent queries with respect to the total number of queries receivedby \nthe disambiguator. On an average, we report 12% of queries as maybe dependent. For 6 out of 12 benchmarks, \nit is in the range of 5%, indicating very accurate disambiguation. We have over 20% maybe queries for \n197.parser and 254.gap, with 176.gcc and 253.perl falling in the 15-20% range. Before presenting a detailed \nanalysis, we first explain the breakdown of data presented in Figure 4. The top section of each bar in \nFigure 4 (labeled direct),represents the maybe dependent queries which involve two direct memory references \n(accesses to global or address-taken local variables). The bottom section represents maybe dependent \nqueries involving [mintersect midentity Dunknown Bdirec{ 1 40% 35% i~ 30% 25% 20*/. 15% m 100 o% ,e \nBenchmark Figure 4 Breakdown of Maybe Queries by Method at least one indirect memory reference. The \nmaybe cases for indirect references are further subdivided into three categories, intersect, identity, \nand unknown, explained below: intersect: The sets of locations accessible from the two memory references \nintersect. Since at least one reference is an indirect reference, one of the location sets is obtained \nfrom points-to analysis.  identity: Both memory references are indirect references off the same pointer, \nfor example the two indirect references in the statement: {*p = *p + x; }  unknown: The points-to set \nfor one of the dereferenced pointers is not known, and the pointer is conservatively assumed to be able \nto point to any address-exposed location in the program. This category includes pointers initialized \nvia library Calls like file pointers, pointers to i/o buffers, the argv pointer used as a formal argument \nto function main, and pointers never initialized in the program (dereferences to such pointers are guarded \nby conditions that always evaluate to false at runtime).  The points-to sets considered above are the \nones obtained from WPT analysis. Each category in the above classification pinpoints a specific area \nof potential improvement for points-to analysis. For example, the unknown queries can be better resolved \nby more accurate modeling of pointers, which are not explicitly initialized in the source program. The \nrequirement is improving the basis information for points-to analysis, and not necessarily its propagation \nstrategy. The identity queries can benefit from program point specific points-to information, which requires \nintroduction of flow-sensitivity. Finally, the queries in the intersect category may benefit only from \na more sophisticated points-to analysis. We now focus on the data for individual benchmarks. We first \nconsider 197.parser, that reports the highest percentage of maybe dependent queries (35%). The dominant \ncategories are unknown and intersect. The unknown queries arise from dereferences of file pointers and \npointers to i/o buffers. Currently our analysis does not accurately model such pointers, and assumes \nthem to be possibly pointing to any address-exposed location, resulting in very conservative disambiguation. \nOur hand analysis indicates that with more accurate modeling of these pointers (by making them point \nto specific symbolic locations), the unknown queries can either be disambiguated or identified as identity \nqueries. Further, the benchmark 197.parser uses a graph data structure, with all nodes allocated via \na wrapper function called xalloc. This function uses free lists and pointer arithmetic, and our analysis \ncannot recognize it as a malloc function. As a result, queries involving accesses to disjoint nodes of \nthe graph cannot be disambiguated, and fall in the intersect category. By explicitly recognizing xalloc \nas a malloc function (for experimental purposes), we see a drop in intersect queries from 15% to 5%. \nMost of the added disambiguation is achieved in program regions where a new node is allocated via xalloc, \nand inserted in the graph data structure (all references to the newly allocated node can be disambiguated). \nThe benchmark, 254.gap, also uses a memory allocator which is based on free lists and pointer arithmetic. \nAll data structures in the program are allocated via calls to this function, and majority of pointers \nin the program end up having identical points-to sets as explained in section 5.1. This results in over \n15% queries falling in the intersect set. The queries in the identity set arise from our strategy of \nmerging all instances of a given structure field. For example, the memory references *(p->ptr) and *(q->ptr), \nare both considered as the reference *(ptr) by our disambiguation framework. Such dereferences of structure \nfield pointers frequently occur in 254.gap. For the benchmark, 176.gcc, the majority of maybe queries \narise inside loops traversing linked data structures, like a list of instructions, or a chain of tree \nnodes. To be able to distinguish between different nodes of such data structures; we need to determine \nthe acyclic property of the navigating pointer fields, which requires sophisticated data structure analysis \n[18]. However, even advanced data structure analyses need to identify the statements where new heap nodes \nare allocated. The complex user-defined memory management in this benchmark practically obscures this \ninformation from the analysis. For 253.perl, the points-to information is very inaccurate, and we get \nover 10% queries in the intersect category. In the benchmark 256.bzip2, we have several global pointers, \nwhich are initialized only once in the program through malloc, and are used as dynamic arrays. All identity \nqueries arise from dereferences of a given global pointer at different program points. Note that since \nthe pointer points to the same location across the entire program, flow-sensitive information will not \nbe able to improve disambiguation. For other benchmarks, we see noticeable number of maybe queries in \nthe direct category. These queries mostly involve array references, subscripted with a pointer reference, \na structure field, or another array reference: cases too complex for the array dependence analyzer. Other \nqueries require disambiguation of an array reference outside a loop, with those inside the loop, as illustrated \nbelow. Such a query may be posed by the scheduler attempting to move the post-loop array load above the \nloop, for (i = O; i < x; i\u00f7+) { a[i + k] = rhs; } y = a[m];  Another source of direct maybe queries \nwe have identified, involves accesses to elements of an array of (statically allocated) structures. For \ntwo references of the form a[i].fieldA and a[k].fieldB, our disambiguator returns a maybe dependence. \nSince we are dealing with static structures, different fields can be safely assumed to access disjoint \nmemory locations irrespective of the array indices. The additional checks required to get this disarnbiguation, \ncan be easily implemented. For now, majority of the direct maybe queries in 186.crafty and 252.eon are \nattributable to this reason. LWdirect msbo_indiroct Dlpt 30 25 E > 20 D. E c o Ik *.. 5 o -5 each \nmethod is computed by dividing the execution time for the base case by the execution time with that method \n(and all the methods to the left) enabled. Basic disambiguation of direct references is very important \nfor 164.gzip and 252.eon, consistent with the static data presented in Figure 3. Disambiguation of direct \nreferences is not very important for 256.bzip2 performance, despite the importance of that method in \nthe static data. The frequently executed loops in 256.bzip2 do not contain direct reads and writes of \ndifferent memory objects. Likewise, direct reference disambiguation is not Eiarray Blglobal I~wpt retype] \nBenchmark Figure 5 Performance Improvement from each Disambiguation Method Overall, our maybe analysis \nindicates that majority of maybe dependent queries arise in programs using dynamic data structures, where \nthe user-defined memory allocating routines cannot be automatically identified by our simple SSA-based \nrecognizer. Development of more sophisticated techniques for malloc function recognition, holds the potential \nto substantially improve disambiguation for a large number of programs. This is amply demonstrated by \nthe accurate disambiguation achieved for 175.vpr and 300.twolf, and the improvement we get for 197.parser \non explicitly recognizing the malloc function. Alternatively, the user can be asked to provide this information \nvia an assertion.  5.3 Performance Analysis We now focus on the impact of our memory disambiguation \ntechniques on program performance. Figure 5 shows the percentage speedup obtained by successively enabling \nthe disambiguation methods. The 0% speedup line represents the base performance with no memory disambiguation. \nThe speedup for as important for 186.crafty performance as the static data would indicate. The next method \nturned on is the use of simple base and offset analysis, and simple rules for indirect references. The \nsimple rules for indirect references pay off for 252.eon. There are many queries with a direct reference \nto a local automatic variable whose address is taken, but the indirect reference is off an unmodified \nparameter. In 255.vortex, there are similar cases. In 175.vpr and 186.crafty, the benefit for this method \ncomes from rules for indirect references, as well as from simple base and offset analysis. For 300.twolf, \nthe performance improvement is mostly from simple base and offset analysis. Local points-to analysis \nis able to provide small performance gains for 175wpr, 252.eon, and 256.bzip2: Array data dependence \nanalysis proves to be very important for the performance of 164.gzip and 256.bzip2, both of which include \nreferences to different elements of the same array in the frequently executed loops. However, static \ncontribution from this analysis is very small for these benchmarks (Figure 3). This makes sense because \nthe loops containing these memory references contribute very little to the static number of memory references, \nbut contribute greatly to the dynamic number of memory references and to the execution time. Disambiguation \nof indirect references from direct references to globals that have not had their address taken helps \nmany programs including 175.vpr, 176.gcc, 254.gap, 256.bzip2, and 300.twolf. 175.vpr, 176.gcc, and 300.twolf \nall have a healthy mix of indirect references and direct references to globals. These benchmarks also \nshow large percentage of static disambiguation queries resolved by this technique. The frequently executed \nloops in 256.bzip2 contain indirect writes to an array and direct loads of global scalar variables for \nthe loop bound and array pointer. This method allows these scalar references to be hoisted out of the \nloop by PRE. This method steals much of the thunder from interprocedural points-to analysis (WPT). Without \nthis method, WPT would normally be required to disambiguate these memory references. Thus it is important \nto evaluate the performance contribution of WPT in the context of other disambiguation techniques. WPT \nprovides performance gains for 175.vpr, 176.gce, 252.eon, 253.perl, 256.bzip2, and especially 300.twolf, \nconsistent with the static query data in Figure 3. The benchmark 181.mcf spends most of its time accessing \nmemory, so despite the fact that a very large fraction of the static queries are resolved by WPT, there \nis no performance gain. The final method considered is type-based disambiguation. It provides small gains \nfor 175.vpr, 176.gcc, 254.gap and a larger gain for 252.con. In 252.eon, the type-based technique proves \nto be very effective for the routine mrSurfaceList::viewingHit, which is one of the most frequently executed \nfunctions in the benchmark. The astute reader will notice a few cases where increasing the level of disambiguation \nactually hurts performance slightly, particularly in 254.gap. It is difficult to pinpoint the slight \nperformance loss to a particular optimization. The loss is possibly caused by a complex interaction between \ndifferent optimizations and requires further investigation. 6. RELATED WORK In [ 1 ], Novack et. al. \npresent a method for preserving algorithm- level and source-level semantics information. Their method \ndefines a hierarchical decomposition of the address space using implicit assertions that reflect the \nprogramming language as implemented by the compiler and explicit programmer assertions that reflect the \nalgorithm and the programmer's use of the language. Using this hierarchy, the disambiguator can distinguish \nbetween stack and heap, different types, and different variables. The DISAM approach also preserves source \nlevel semantics and decomposes the address space, but it provides access to a richer set of information, \nincluding full symbol table information about variables, pointers, types, functions, and memory allocation \nsites. In our framework, we use the symbolic representation of memory references as a way to interface \nwith analyses such as points-to analysis that are performed while the program is represented at a high \nlevel. We have also extended this framework for function call MOD/REF analysis. Our implementation of \nthis technique in a product compiler demonstrates that it is possible with very reasonable cost to maintain \nhigh-level information for use by the disambiguator at any compilation phase. The memory disambiguator \nin the IMPACT compiler is described in [8]. This compiler performs array data dependence analysis and \npoints-to analysis [16]. It generates memory dependence arcs, called sync arcs, to represent all the \nmemory dependences in the function and maintains these arcs through all of the optimization phases. This \nrequires that potentially O(N 2) arcs be stored in the worst case where N is the number of memory references \nin the function. In practice the pairs of references that actually have a control flow path between them \n(and thus require an arc) is much smaller. In our method, we maintain information about each memory reference \n(O(N)) plus a data dependence graph containing only arcs for dependences between different elements of \nthe same array in loops. We must store a points-to graph whereas the IMPACT compiler can discard the \npoints-to graph once the sync arcs have been generated. The relative memory usage of the two methods \ndepends on the relative sizes of the information stored in the points-to graph and DISAM tokens verses \nthe sync arcs. We believe that the DISAM approach has a memory usage advantage at\u00b0lower optimization \nlevels where points-to information is not available, as the number of conservative sync arcs will be \nhigher due to the absence of accurate information. In the DISAM approach, on average, each query is relatively \nexpensive compared to looking up a sync arc. While the compile-time cost of the queries has not been \nan issue thus far, the results of the most time-consuming queries could be cached to reduce the cost. \nThe total number of queries received by the disambiguator is much larger than the number of unique queries. \nThe related work on using the results of pointer analysis includes that of Wilson and Lam[13], Ghiya \nand Hendren[14], and Diwan et. al[15]. Wilson and Lam use points-to information for parallelization of \ntwo benchmarks. The speedup is achieved through better disambiguation between pointer-based arrays. Ghiya \nand Hendren study the benefits of a collection of pointer analyses in the context of three optimizations: \nloop-invariant removal, location-invariant removal, and common subexpression elimination. In our compiler \nframework, all three optimizations are subsumed within the PRE and PDSE optimizations. Diwan et. al present \nthe results on redundant load elimination (RLE) for a set of Modula programs, based on information from \na type-based alias analysis. Our PRE optimization also subsumes RLE. Pioli and Hind[17] present a thorough \nstudy of the efficiency and precision of six context-insensitive pointer analyses, using the metric of \naverage points-to set size at indirect references. Our experimental data indicates that this metric is \nnot always a reliable indicator of the effectiveness of points-to information in resolving disambiguation \nqueries. Cheng et. al[16] perform a very thorough study of the impact of memory disambiguation on the \nSPEC CINT92 and CINT95 programs in the context of redundant load/store elimination, loop- invariant location \npromotion, and load/store scheduling. The memory optimizations are subsumed by our PRE and PDSE transformations, \nand we also include acyclic scheduling, software pipelining, and other optimizations in our study. Cheng \net. al individually show the benefits of disambiguation for scheduling vs. load/store optimization. Their \nperformance improvements are higher than those reported in this paper. Their study uses a different set \nof programs and a wider simulated processor. Our base case includes modeling of side effects for library \nfunction calls and disambiguation of compiler generated references such as spill code. Their base case \ndoes not use any disambiguation or side-effect analysis. 7. CONCLUSION In this paper, we have described \na memory disambiguation framework that brings together all the clients and sources of memory disambiguation. \nWe have evaluated the framework using standard benchmarks on an ItaniumTM-based system. The experimental \nresults show that a broad range of disambiguation methods is necessary to handle the varying characteristics \nof different programs and provide the highest overall performance. The results also show that it is important \nto evaluate the performance of additional memory disambiguation techniques such as points-to analysis \nwithin a hierarchical framework that implements the simpler disambiguation methods, because these methods \nsteal some of the thunder of the more complex methods. Further, we demonstrated that there is no direct \ncorrelation between the effectiveness of memory disambiguation as per the static metrics, and its contribution \nto overall program performance. This also applies to individual disambiguation methods, when considered \nin isolation. Finally, we noted that in many cases, loss in accuracy of points-to information occurs \ndue to certain features inherent to the analyzed program, like arbitrary type-casting and user-managed \nmemory allocation, which cannot always be overcome by even applying more sophisticated points- to analyses. \n 8. ACKNOWLEDGMENTS We thank the members of the Intel Itanium TM compiler team for their contributions \nto the compiler technology described in this paper, and the reviewers, whose comments significantly improved \nthe quality of this paper.  9. REFERENCES [1] Novak, S., Hummel, J., and Nicolau, A. A Simple Mechanism \nfor Improving the Accuracy and Efficiency of Instruction-Level Disambiguation. 8th International Workshop \non Languages and Compilers for Parallel Computing, Springer-Vcrlag, August 1995, 289-303. [2] Chow, F., \nChan, S., Kennedy, R., Liu, S., Lo, R., and Tu, P. A New Algorithm for Partial Redundancy Elimination \nBased on SSA form. Proceedings of the ACM SIGPLAN '97 Conference on Programming Language Design and Implementation, \nJune 1997, 273-286. [3] Bharadwaj, J., Menezes, K., and McKinsey, C. Wavefront Scheduling: Path Based \nData Representation and Scheduling of Subgraphs. Proceedings of the 32nd Annual ACM/IEEE International \nSymposium on Microarchiteeture, 1999, 262- 271. [4] Andersen, L. O. Program Analysis and Specialization \nfor the C Programming Language. PhD thesis, DIKU, University of Copenhagen, May 1994. (DIKU report 94/19) \n[5] Rountev, A., and Chandra, S. Off-line Variable Substitution for Scaling Points-to Analysis. Proceedings \nof ACM SIGPLAN Conference on Programming Language Design and Implementation, June 2000, 47-56. [6] Fahndfich, \nM., Foster, J., Su, Z., and Aiken. A. Partial Online Cycle Elimination in Inclusion Constraint Graphs. \nProceedings of ACM SIGPLAN Conference on Programming Language Design and Implementation, June 1998, 85-96. \n[7] Das, M. Unification-based Pointer Analysis with Directional Assignments. Proceedings of ACM SIGPLAN \nConference on Programming Language Design and Implementation, June 2000, 35-46. [8] Hwu, W., Hank, R., \nGallagher, D, Mahlke, S., Lavery, D., Haab, G., Gyllenhaal, J., and August, D. Compiler Technology for \nFuture Microprocessors, Proceedings of the IEEE,December 1995, pp. 1625-1640. [9] Sharangpani, H., and \nArora, K. Itanium Processor Micro- architecture. IEEE Micro, Vol 20, No 5, Sept 2000, 24-43. [10] Huck, \nJ., Morris, D., Ross, J., Knies, A., Mulder, H., and Zahir, R. Introducing the IA-64 Architecture. IEEE \nMicro, Vol 20, No 5, Sept/Oct 2000, 12-23. [11] Bharadwaj, J., Chert, W,, Chuang, W., Hoflehner, G., \nMenezes, K., Muthukumar, K., and Pierce, J. The Intel IA- 64 Compiler Code Generator. IEEE Micro, Vol \n20, No 5, Sept/Oct 2000, 44-53. [12] Krishnaiyer, R., Kulkami, D., Lavery, D., Li, W., Lim, C., Ng, J., \nand Sehr, D. An Advanced Optimizer for the IA-64 Architecture. IEEE Micro, Vol 20, No 6, Nov 2000, 60-68. \n[13] Wilson, R. P., and Lain, M. S. Efficient Context-Sensitive Pointer Analysis for C Programs. Proceedings \nof ACM SIGPLAN Conference on Programming Language Design and Implementation, June 1995, 1-12. [14] Ghiya, \nR., and Hendren, L. Putting Pointer Analysis to Work. Proceedings of ACM SIGPLAN/SIGACT Conference on \nPrinciples of Programming Languages, Jan 1998, 121-133. [15] Diwan, A., McKinley K., and Moss, J. Type-based \nAlias Analysis. Proceedings of ACM SIGPLAN Conference on Programming Language Design and Implementation, \nJune 1998, 106-117. [16] Cheng, B., and Hwu, W., Modular Interprocedural Pointer Analysis using Access \nPaths: Design, Implementation, and Evaluation. Proceedings of ACM SIGPLAN Conference on Programming Language \nDesign and Implementation, June 2000, 57-69. [ 17] Pioli, A., and Hind, M. Evaluating the Effectiveness \nof Pointer Alias Analyses. Science of Computer Programming, Vol. 39 (1) (2001), 31-55. [18] Ghiya, R., \nand Hendren, L. Is it a Tree, DAG, or a Cyclic Graph? A Shape Analysis for Heap-directed Pointers in \nC. Proceedings of ACM SIGPLAN/SIGACT Conference on Principles of Programming Languages, Jan 1996, 1-15. \n   \n\t\t\t", "proc_id": "378795", "abstract": "<p>In this paper, we evaluate the benefits achievable from pointer analysis and other memory disambiguation techniques for C/C++ programs, using the framework of the production compiler for the Intel&#174; Itanium#8482; processor. Most of the prior work on memory disambiguation has primarily focused on pointer analysis, and either presents only static estimates of the accuracy of the analysis (such as average points-to set size), or provides performance data in the context of certain individual optimizations. In contrast, our study is based on a complete memory disambiguation framework that uses a whole set of techniques including pointer analysis. Further, it presents how various compiler analyses and optimizations interact with the memory disambiguator, evaluates how much they benefit from disambiguation, and measures the eventual impact on the performance of the program. The paper also analyzes the types of disambiguation queries that are typically received by the disambiguator, which disambiguation techniques prove most effective in resolving them, and what type of queries prove difficult to be resolved. The study is based on empirical data collected for the SPEC CINT2000 C/C++ programs, running on the Itanium processor.</p>", "authors": [{"name": "Rakesh Ghiya", "author_profile_id": "81100605741", "affiliation": "Intel Corporation, 2200 Mission College Blvd, Santa Clara, CA", "person_id": "P238209", "email_address": "", "orcid_id": ""}, {"name": "Daniel Lavery", "author_profile_id": "81100387329", "affiliation": "Intel Corporation, 2200 Mission College Blvd, Santa Clara, CA", "person_id": "P334687", "email_address": "", "orcid_id": ""}, {"name": "David Sehr", "author_profile_id": "81100461045", "affiliation": "Intel Corporation, 2200 Mission College Blvd, Santa Clara, CA", "person_id": "P334700", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/378795.378806", "year": "2001", "article_id": "378806", "conference": "PLDI", "title": "On the importance of points-to analysis and other memory disambiguation methods for C programs", "url": "http://dl.acm.org/citation.cfm?id=378806"}