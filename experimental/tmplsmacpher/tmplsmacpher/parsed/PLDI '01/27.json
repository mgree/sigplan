{"article_publication_date": "05-01-2001", "fulltext": "\n SPL: A Language and Compiler for DSP Algorithms Jianxin Xiong' dererny Johnson ~ Robert Johnson' David \nPadua 1 1 Department of Computer Science University of Illinois at Urbana-Champaign Urbana, IL 61801 \n{j XiOag, padua}@cs, uiuc. edu Department of Mathematics and Computer Science Drexel University Philadelphia, \nPA 19104 j j ohnson~mcs, drexel, edu 3 MathStar Inc. Minneapolis, MN 55402 rvj @mathstar. edu ABSTRACT \nWe discuss the design and implementation of a compiler that translates formulas representing signal processing \ntransforms into efficient C or Fortran programs. The formulas are rep- resented in a language that we \ncall SPL, an acronym from Signal Processing Language. The compiler is a component of the SPIRAL system \nwhich makes use of formula trans- formations and intelligent search strategies to automatically generate \noptimized digital signal processing (DSP) libraries. After a discussion of the translation and optimization \ntech- niques implemented in the compiler, we use SPL formula- tions of the fast Fourier transform (FFT) \nto evaluate the compiler. Our results show that SPIRAL, which can be used to implement many classes of \nalgorithms, produces programs that perform as well as \"hard-wired\" systems like FFTW.  1. INTRODUCTION \nSince the advent of digital signal processing, there has been an enormous effort to obtain high-performance \nimple- mentations of signal processing algorithms such as the fast Fourier transform (FFT). This effort \nhas produced thou- sands of variants of fundamental algorithms and an equally large number of implementation \ntechniques. There have been more than 4000 papers written on the FFT alone [7] and undoubtedly an even \ngreater number of implementa- tions of this algorithm, many of which have been carefully hand-optimized. \nThe cost of these hand optimizations and their long implementation times are a strong motivation Permission \nto make digital or hard copies of all or part of this work for personal or classroom use is granted without \nfee provided that copies are not made or distributed for profit or commercial advan-tage and that copies \nbear tfiis notice and the full citation on the first page, To copy otherwise, to republish, to post on \nservers or to redistribute to lists, requires prior specific permission and/or a fee. PLDI 2001 6/01 \nSnowbird, Utah, USA &#38;#169; 2001 ACM ISBN 1-58113-414-2/01/0S...$5.00 to automate the implementation \nand optimization of sig- nal processing algorithms [9]. To this end, we have designed SPL [8], a domain-specific \nlanguage for describing and im- plementing fast signal transforms and related computations, and implemented \na compiler that translates formulas writ- ten in this language into C or Fortran programs. SPL stands \nfor Signal Processing Language. The SPL compiler is a component of the SPIRAL system [13], that systematically \nsearches through algorithm and im- plementation choices to find an optimal implementation for a given \ncomputing platform. Figure 1 shows the structure of the SPIRAL system. The algorithmic choices are expressed \nby mathematical formulas expressed in SPL, and the im- plementation choices are explored using the SPL \ncompiler. The mathematical nature of SPL programs aids in the au- tomatic generation of potential algorithms \nusing a process called formula generation. The resulting implementations are compared using the performance \nevaluation component, which returns run times and other performance metrics ob- tained by executing the \ncode in the target machine or esti- mated using models. The search engine looks for the fastest implementation \nout of the set of choices produced by the formula generator and SPL compiler. Due to th e exponen- tial \nsize of the search space, intelligent search strategies are required to make this process feasible. SPL \nis a descendent of the TPL (Tensor Product Lan- guage) [i] that was developed for the automatic generation \nof FFT algorithms. SPL programs are essentially mathe- matical formulas describing matrix factorizations. \nAs such, they are built using operators from linear algebra and fam- ilies of parameterized matrices. \nSuch formulas naturally arise when describing fast signal transforms, where the sig- nal transform corresponds \nto a matrix-vector product and fast algorithms can be represented by a factorization of the matrix into \na product of sparse structured matrices [14, 15, 16]. The SPL compiler translates an SPL expression into \na program to compute the matrix-vector product of the ms-  Given DSP Transforms formed signal. Fast \nalgorithms for computing y = Mx can k_, be obtained by factoring the matrix M into a product of Posstble \nAlgorithms sparse structured matrices. For example, a one-dimensional IFormula Generator) discrete Fourier \ntransform (DFT) is defined as y = F~x, T where the (p,q) element of F, is w~ q with wn = e . The I Possible \nImplementations ~_~ Search 4-point DFT ($_____PL Engine  Compiler) 1 1 1 1  IPerformance Evaluation \n7, DSP LI aries Figure 1: The SPIRAL framework trix given by the SPL expression following a method first \noutlined in [10]. The semantics of the components of a SPL expression are defined using a template mechanism. \nThis template mechanism allows the user to include additional operators and matrices in the SPL language. \nIt also provides a mechanism-to control the optimization and code genera- tion strategies used by the \ncompiler. In this paper, we describe the translation process and opti- mization techniques used by the \nSPL compiler and present performance data for the code it generates. We use the FFT as a benchmark due \nto the availability of very high- performance implementations that can be used to measure the quality \nof the code produced by the SPL compiler. In particular, we show that the code produced is competitive \nwith the best available software packages (FFTW [6] was used in our comparison) for performing similar \ncomputa- tions. Our system is more general than earlier systems, discussed in Section 5, that were hard-wired \nto generate only one class of signal processing programs. The use of SPL enables our system to generate \nany class of algorithm that can be repre- sented as matrix expressions. Knowledge of particular ma- trix \nexpressions is not included in the compiler, instead this knowledge is used by the formula generator \nwhen SPL pro- grams are created and manipulated. As long as the algo- rithms can be described in SPL, \nall the steps needed to get a good implementation are automatic. The rest of this paper is organized \nas follows. First, some background information about matrix factorizations and SPL are presented in Section \n2. The organization and algorithms used by the compiler are presented in Section 3. Section 4 presents \nperformance results. Finally, conclusions and future work are discussed in Section 5. 2. SPL, MATRIX \nFACTORIZATIONS, AND FAST SIGNAL TRANSFORMS In this section we review the relationship between fast s!g- \nnal transforms and matrix factorizations, and we summarize how SPL can be used to represent matrix factorizations. \n 2.1 Matrix Factorizations andFast SignalTrans- forms A signal transform can be represented by the matrix- \nvector product, y = Mx, where the vector x denotes the input signal, the matrix M the transform, and \ny the trans- 1 -i -i i F4 = i -i i -i i i -i -i can be factored as 1 0 oioj1 0 1 1 0 0 1 0 0 0 0 F4= \n1 0 -1 0 0 0 1 0 0 1 0 -1 0 0 0 -i 1 i 0 0 1 0 0 0 1 -i 0 0 0 0 1 0 0 0 1 1 0 1 0 0 0 0 1 -i 0 0 0 1 \nGiven the definitions Im -~ the m x m identity matrix, 0~n W,.(,o.) -- I \"'. Win-1 (can be written as \nW,n whenever it is clear in the context ), L~ s ~ the rs \u00d7 rs stride permutation matrix with stride s, \nthe previous factorization can be written as F, [I2 /2 ]L~. In general, we have l[ 1L2. (1) = 19 -I~ \nW~ a L F~ F~ a and y = F~x can be computed in 4 steps: step(1): ~ = L~x step (2): t2 = F~ t~    \nI'\" ] step (3): t3 = W~ t2 step (4): Y = I~r -I~ This four-step computation, applied recursively, is \nthe well- known Cooley-Tukey FFT algorithm. Step (1) is a stride permutation that can be performed in \nn operations. Step (2) computes two FFTs of size ~. Step (3) is the product of a diagonal matrix with \na vector and therefore requires n operations. Step (4) requires 2 operations per row. The total number \nof operations, f(n), m then n + \u00b0 ~ 2 f(~) + n + 2n f(~) + O(n), and, therefore, f(n) = O(nlogn). 299 \nThe previous factorizations can be represented more con- cisely using the tensor product. Let A be an \nm \u00d7 m matrix and B be an n x n matrix. The tensor product of A and B, denoted as A \u00ae B, is the following \nmn x mn matrix: allB \"\" almB ] A\u00aeB ~. \" \". \" J . (2) aml B \" \" amrnB The tensor product has interesting \ninterpretations when one of the operands is the identity matrix. Thus, In\u00aeA= \". A represents an identical \ntransformation applied to successive sub-vectors of the input vector, and A \u00ae I,~ = \" \".. \" a.lI~ ... \na,.dm represents the same transformation applied to strided sub- vectors of the input vector with stride \nrn. The FFT factorization expressed in Equation 1 can be represented using the tensor product notation \nas follows: F,~ = (F2 \u00ae I~ )T~(I2 @ F~ )L~, (3) whereTO= [ I~ W~ ] , is known as the twiddle matrix. \nThe general definition of twiddle matrix is: Tmm\" = W,~ . (4) W,~ -1 A more general form of the previous \nfactorization, that does not require n to be even, is: F~, = (F~ $ Io)T:\u00b0(L @ Fo)LT'. (5) This is also \ncalled decimation in time FFT. Other factorizations are possible by using the identity: A,~x,~ \u00ae B,~x,~ \n= L~~'~(B~x.\u00ae A . ~x~j~L ~'~ (6) Examples of such factorizations include, decimation in fre- quency \nFFT: Fro = L~'(Ir @ Fo)Tf'(Fr \u00ae/e), (7) a parallel form of the FFT (In \u00ae A can be trivially imple- mented \nas a parallel loop): Fro = L~'(Ir \u00ae Fo)L;'T:\u00b0(I,. \u00ae F\u00b0)L~', (8) and a vector form: Fr8 = (F,. @ h)T~'L~\u00b0(F\u00b0 \n@ It). (9) Finally, the following identity further generalizes the pre- vious factorizations of F~: f. \n= [(a,_ \u00ae F., \u00ae \u00ae T:::'+)]. 1 I-I~=,(Im_ \u00ae n~: TM), (to) where n = ni . n,, hi- = nl . hi-i, and n~+ \n= r~i+ l o. n~. Proof of the above factorizatlon equations can be found in  [10), Applying this last \nfactorization recursively to each F~ until the base case is equal to F2 leads to an algorithm for computing \nF~. The special case when t = 2, nl = 2, and n2 = ~ leads to the standard recursive FFT. Other recursive \nbreakdown strategies are obtained by choosing different val- ues for nl and n= or by choosing other factorizations \nsuch as those shown above. The special case when n~ .... nt = 2 leads to the standard iterative, radix-two, \nFFT. Equation 10 allows us to explore various amounts of reeursion and iteration along with different \nbreakdown strategies. Other transforms can be factored in a similar manner. These include the Walsh-Hadamard \ntransform (WHT) WHT2 = F2, WHT2, = H~=I(I~t...2~,-, \u00ae WHT2,, \u00ae I2\",+,...2,~), and the discrete cosine \ntrans/orm (DOT) DCTII2 = diag(1, l/v/2) . F2, DCTIIn = P. (DCTII~ @ DCTIV~). (I~ \u00ae F2) Q, DCTIVn = S. \nDCTII,~ D. In this last set of equations, P,Q are permutation matrices, D, S are diagonal matrices, \n@ is the direct sum, and A Q means the conjugation Q-iAQ. 2.2 The SPL Language SPL is a convenient \nlanguage for describing matrix factor- izations, and hence fast algorithms for computing matrix- vector \nproducts. In particular, it can be used for describing fast signal transforms such as the FFT. SPL expressions \nare matrix expressions involving general matrices and parame- terized matrices such as Fs and Ls 8~. \nAn SPL expression may involve a variety of operations including composition, direct sum, and tensor product. \nSPL represents matrix operations in Cambridge Polish notation. For example, (compose A B) represents \nthe matrix product A B. A matrix can be specified by giving the value of all its elements, or it can \nbe represented by a parameterized matrix. For example, a 2 \u00d7 2 identity matrix can be represented as \n(matrix (1 0) (0 i)), (diagonal (i i)) or (I 2). An SPL program contains one or more SPL expressions \n (or formulas). Each expression can be optionally preceded by definitions and compiler directives. An \nSPL formula can be interpreted as a matrix or, more importantly for our pur- poses, as a subroutine that \naccepts a vector as the input and output the result of the corresponding matrix-vector prod- uct. In \nthis section, we briefly review the syntax used by SPL. Further details are available in [8]. Typical \ncomponents of SPL expressions are: (1) General matrices, for example: (matrix ((all ... aln) ... (am1 \n... amn)) (diagonal (all ... ann)) (permutation (kl ... kn))  (2) Parameterized matrices, for example: \n(I n) ; identity matrix (F n) ; Fourier transform by definition (Lmnn) ; stride permutation matrix (T \nmu n) ; twiddle matrix  (3) Matrix operations, for exaraple: (compose hl ... An) ; matrix product (tensor \nA1 ... An) ; tensor product (direct-sum A1 ... An) ; direct sum  The elements of a matrix can be real \nor complex num- bers. In SPL, these numbers can be specified as constant scalar expressions, which may \ncontain function invocations and symbolic constants like pi. Thus, expressions like 12, 1.23, sqrt(2), \nand (cos(2*pi/3.0),sin(2*pi/3.0)) are valid scalar SPL expressions. All constant scalar expressions are \nevaluated at compile-time. An SPL program may contain templates, a mechanism to add new parameterized \nmatrices and matrix operations: (4) Template definition (template pattern condition i-code-list)  We'll \ndiscuss templates in Section 3. In SPL, a formula can be assigned a name so that it can be reused elsewhere. \n(5) Name assignment (define name formula)  Lines starting with \"#\" are compiler directives, and the \ncontents between a \";\" and the new line are comments. Compiler directives control some of the compiler \nactions. For example, an SPL formula can be preceded by a #eubname directive to specify the name of the \nsubroutine to be gener- ated. Other directives include: #unroll, which controls the loop unrolling; #datatype, \nwhich specifies the type (real or complex) of the data to be manipulated; #lauguage, that specifies the \ntarget language; and #codetype, that specifies whether the intrinsic type of the input vector and interme- \ndiate operands will be real or complex when the target lan- guage is Fortran. When the #datatype is complex \nand the #codetype is real, the complex values will be represented as pairs of real numbers and, for each \ncomplex operation, the compiler will generate the corresponding real operations. To illustrate SPL, consider \nthe formula: Fxs = (F4 \u00ae h)T~S(I4 \u00ae F,)L~ 8 where F4 = (F2 @ I2)T~(I2 @ F2)L~ The SPL program corresponding \nto this formula is (define F4 (compose (tensor (F 2) (I 2)) (T 4 2) (tensor (I 2)(F 2)) (L 4 2))) #subname \nfftl6 (compose (tensor F4 (I 4)) (T 16 4) (tensor (I 4) F4) (L 16 4))   3. THE COMPILER The SPL compiler \ngenerates a Fortran or C subroutine for each SPL formula. The compiler proceeds in five phases: parsing, \nintermediate code generation, intermediate code re- structuring, optimization, and target code generation. \nEach of these phases is described below in separate subsections. 3.1 Parsing The parser translates each \nSPL formula into an abstract syntax tree (AST) containing matrix operations and matri- ces. The AST is \nbinary. N-ary formulas such as (compose hi ... An) are associated right-to-left. Nested expressions must \nbe used in order to specify a different association order. 3.2 Intermediate Code Generation All SPL operations \nhave to be defined using templates. Templates for pre-defined operations, such as those corre- sponding \nto the parameterize matrices and matrix opera- tions mentioned in Section 2.2, are placed in a start-up \nfile that is read by the compiler before reading the source SPL program. Only when the formula matches \na template does the compiler know the meaning of the formula and can gen- erate code for it. This approach \nprovides great flexibility and extensibility. New parameterized matrices and new matrix operations can \nbe easily added using templates. A template consists of: a pattern, a condition, and an in- termediate \ncode (or i-code) sequence. If an SPL formula matches the pattern and satisfies the condition, then the \nSPL formula can be translated into the intermediate code specified by the template. Matching is attempted \nin the reverse order of definition so that new templates override earlier ones and in particular override \npre-defined templates that are processed by the SPL compiler as if they were de- fined at the beginning \nof the program. The pattern is an SPL formula that can contain pattern variables. All pattern variables \nend with an underscore (_). Pattern variables that start with a lower-case letter can match any integer \nconstant and those that start with an upper-case letter can only match SPL formulas. For exam- pie, the \npattern (I n.) matches (I 1) or (I 2); and the pattern (compose X_ Y.) matches (compose (F 2)(I 3)) or \n(compose (compose A B) (tensor (I 2) C)), where A, B and C are defined symbols representing formulas. \nPattern variables can not match undefined symbols. For example, although the pattern (I n) can match \n(I 2), (I 4), and so on, it can not match a generic (I m). This limitation is consistent with the fact \nthat SPL is designed to describe fixed-size computations. The condition of a template is a C-style boolean \nexpres- sion, enclosed by brackets. For instance, a pattern (L m_ n_) with condition [ m.==2*n_ ] will \nmatch the formula (L 4 2), but not (L 4 1). Here m_ and n- matches 4 and 2, respectively. The condition \nis optional. I-code instructions are Fortran-style \"do\" loop headers, \"end do\" statements, or four-tuples \ncontaining an operator and up to three operands. Operators are mainly arithmetic operations, such as \n+, -, *, / and assignment. Operands can be constants, pattern variables, scalar variables, vec-tor variables, \nor intrinsic functions. If a pattern variable matches an SPL formula, properties of the formula repre- \nsented as \"components\" of the pattern variable can be used as scalar values. For example, if A. is a \npattern variable, then A_. in_size is the size of the input vector of the formula represented by h_. \nSimilarly, A_. out_size is the size of the output vector. Scalar variables can be loop indices (named \n$i0, $il, ...), integer variables (named Sr0, Srl, ...), and floating point or complex variables (named \nSf0, Sfl, ...). Vector variables can be the input vector Sin, output vector Sour, or temporary vectors \n(named St0, Stl .... ). The sub- scripts of vector variables are always linear combinations of loop indices \nwith integer coefficients. Intrinsic functions are parameterized scalar function. For example, W(n,k) \nis an intrinsic function which returns the value a~. Here n and k can be scalar constants or variables. \nAn example template is the following definition of the semantics of (F n_): (template (F n_) [n_>O] \n(do $i0 = O,n_-I $out($iO) = 0 do $ii = 0,n_-i St0 = $i0 * $il $f0 = W(n_ Sr0) * Sin($il) $out($i0) \n= $OUt($i0) + $:f0 end end) )  Each template has six implicit parameters: Sin, Sou% Sin_stride, $out_stride, \nSin_offset and $out.offset, which represent the input vector, the output vector, and the strides and \noffsets to access each of these vectors (these values were assumed to be 1 in this example to make the \ncode easier to understand). Furthermore, the size of the input and output vectors, Sin_size and Sour_size \nrespectively, is inferred by the SPL compiler from the template. Templates can be applied recursively. \nFor example, the following template defines the compose operation. (template (compose A_ S_)) [ A_.in_size \n== B_.out_size ] ( B_( Sin, St0, 0, 0, i, i ) A_( St0, Sour, 0, 0, I, I )))  Here, pattern variables \nA_ and B_ are followed by a list of parameters to be used during expansion of the formula represented \nby the pattern variable. The formula (compose (I 2)(F 2)1 will match this template with A_=(I 21 and \nB_=(F 2). The i-code sequence contains pattern variables, so the compiler tries to find templates which \nmatch the sub- formulas (I 2) and (F 2), replacing the two pattern vari- ables and their parameters with \nthe code generated from the sub-formulas. The first parameter of the pattern variables, Sin in the first \ninstruction and St0 in the second, will be used as the value of Sin in the matching template. The sec- \nond parameter will be used as the value of Sour. The other four parameters correspond to the offsets \nand strides for the input and output vectors. In this case, the offsets are always zero, and the strides \nare always one. The pattern can be as elaborate as any SPL formula. Thus, code generation strategies \ncan be defined for all for- mule patterns regardless of their complexity. By using the appropriate template, \nit is possible to reproduce the effect of some compiler optimizations. For example, the SPL formula (compose \n(tensor (I 8) A) (tensor (I 8) B)) could be translated into two consec- utive loops using a tensor template \n(twice) and a compose template. To merge these two loops into one, we can de- fine a template which recognizes \nthe complete formula and generates a single loop. The effect is the same as loop fusion. Templates can \nbe generated by a search engine. Thus, in a system such as SPIRAL, the search can include different formulas, \ndifferent code generation strategies, and different optimization parameters in a uniform way.  3.3 \nIntermediate Code Restructuring We discuss next three transformations that can be applied to the i-code \nafter it is generated by the template mecha- nism: loop unrolling, intrinsic function evaluation, and \ntype transformation. 3.3.1 Loop unrolling The presence of pattern variables in the templates may force \nthe use of loops in the i-code sequence. Thus, although a loop-free i-code sequence is possible when \nthe pattern rep- resent a constant-size parameterized matrix such as (Y 2), loops are needed for patterns \ncontaining a variable-size term such as (F n_). A template for (F n_) was presented in Sec- tion 3.2. \nHowever, after matching the pattern with a specific SPL formula, these loop bounds become constant values. \nThe compiler may be directed to unroll the loops, fully or par- tially, to reduce loop overhead and increase \nthe number of choices in instruction scheduling. When the loops axe fully unrolled, not only is loop \ncontrol overhead eliminated but it also becomes possible to substitute scalar variables for ar- ray elements. \nThe use of scalar variables tends to improve the quality of the code generated by Fortran and C com-pilers \nwhich are usually unable to analyze codes containing array subscripts even if the subscripts are constants. \nThe downside of unrolling is the increase in code size. In SPL, the degree of unrolling can be specified \nfor the whole program or for a single formula. For example, with the command-line option \"-B 32\", all \nthe loops in those sub- formulas whose input vector is smaller than or equal to 32 are fully unrolled. \nTo see how to control loop unrolling for individual formulas, consider the following SPL program: #datatype \nreal #unroll on (define I2F2 (tensor (I 2)(F 2111 #unroll off #subname I64F2  (tensor (I 32) I2F2) \n It generates the following Fortran sequence where the un-rolled version of I2F2 appears as the body \nof the loop: subroutine I64F2 (y,x) implicit real*8 (f) implicit integer(r) real*8 y(128),x(128) do \niO = O, 31 y(4*iO+l) = x(4*iO+l) + x(4.i0+2) y(4.i0+2) = x(4*iO+l) -X(4\"i0+2) y(4\"i0+3) = x(4\"i0+3) \n+ x(4'i0+4) y(4.i0+4) = x(4\"i0+3) -x(4'i0+4)  end do  In the generated code, the input and output \nvector is named as x and y, respectively.  3.3.2 Intrinsic function evaluation All intrinsic functions \nare evaluated at compile-time. If all the parameters of an intrinsic function are constant, the intrinsic \nfunction invocation is replaced by its value. If one or more of the parameters are loop indices and the \nothers are constant, then the compiler evaluates the intrinsic function for all possible values of the \nloop indices, places these values in a table, and replaces the intrinsic function invocation with a reference \nto the table accessed through the loop indices. w--- 333MHz UP 2oo 180MHz 400MHz traSPARC MIPS Pentium \nII IIi R10000 L1 cache \"~t ;/16KB 32KB/32KB 16KB/16KB L2 cache .....1MB'I 512KB Memory 1281v :B 384MB \n256MB OS Sola~ is 7 IRIX64 6.5 Linux kernel 2.2.18 Work- MIPSpro egcs 1.1.2 shop 5.0 7.3.1.1m Table \n1: Experiment platforms  3.3.3 Type transformation The input and output vectors of the subroutine generated \nby the SPL compiler could be either real or complex. If the type is complex and the target language is \nFortran, the com- piler could generate either complex intrinsic types or a pair of real numbers for each \noccurrence of a complex scalar. The advantage of using complex type is that the resulting code is shorter \nand clearer. However, of the popular imperative lan- guages only Fortran supports complex data type. \nFurther- more, in our experiments we decided to use only Fortran codes based on real numbers because \nthis enabled further optimizations, such as the replacement of multiplication by i (the square root of \n-1) with a swap instruction followed by a negation instruction.  3.4 Compiler Optimizations The SPL \ncompiler applies constant folding, copy propa- gation, common subexpression elimination, and dead code \nelimination. These default optimizations are applied in a single pass using a wlue numbering algorithm. \nBoth scalar variables and array elements are handled by the optimiza- tions. Performing conventional \noptimizations in the SPL compiler was necessary to improve the quality of the code generated by the native \nFortran and C compilers we used in our experiments. To demonstrate the effect of the optimizations applied \nby the SPL compiler, we selected a set of 45 SPL formulas, and generated three versions of Fortran code \nfor each of them. Figure 2 shows the normalized performance measured on the three platforms listed in \nTable 1. The three versions of For- tran code are: (1) no optimization; (2) replacing temporary vectors \nwith scalar variables; (3) default optimization. All these versions are compiled by the corresponding \nback-end compiler with the maximum optimization turned on. The performance data is obtained by taking \nthe inverse of the execution time, and then normalizing with respect to the performance of the version \nwith default optimization. The effect of default optimizations depend on the platform and the back-end \ncompiler. In the SPARC machine (Figure 2), replacing array elements with scalar variables improved the \nperforms by 60 percent, the default optim~ations in- troduced similar amount of improvement. On the Pentium \nII machine, changing array elements to scalar variables did not help much, while the default optimizations \nimproved the performance by a factor of two. On the MIPS machine, however, the effect of these optimizations \nwas insignificant. It means the MIPSpro compiler did a good job in standard optimizations. 0.2 no optimization \n~lar temporary 1; 15 2~0 215 3~) Different SPL fo~nul~s for FFT (N-32) 35 40 4~ (a) SPARC i 0,8 i0,6 \nI0,4 0.2 no optimization scaMr temporary , t\u00f7 I i i i 210 i 5 10 15 255 30 35 40 45 Different $PL fomlukls \nfor FFT (N-32) (b) MIPS i -.e--no opllmlzetlon -t-~tar tempora~ I ..~1.- default optimization O. 0.6 \nDifferent 8PL formulae for FFT (N,,321 (c) Pentium II Figure 2: Effect of basic optimization 303 An additional \neffect of these optimizations is that they improve the readability of the target code by avoiding much \nof the redundant code that would be generated otherwise. The compiler also applies two, machine-dependent, \npeep- hole optimizations. One replaces unary minus operators on double precision operands with subtraction \noperations or negative constants. For example, the compiler generates \"f2=0-fl\" instead of \"f2=-fl\", \nand \"f2=(-7)*fl\" instead of \"f2ffi-7*fl\". The reason is that, on SPARC systems, arithmetic negation is \na single precision instruction and it takes at least six cycles for the floating point unit to switch \nbetween single precision mode and double precision mode. Another optimization declares all temporary \nvariables as \"autemat:[c\" so they will be allocated on the stack. Many Fortran compiler, by default, \nregards all variables as static. This transformation led to significant performance improve- ments on \nSPARC systems. Both of these transformations are machine-specific and may not have a positive effect \non machines other than the SPARC. 3.5 Target Code Generation In addition to generating code in different \ntarget lan-guages, the SPL compiler can add stride and offset infor- mation to the input and output vectors, \nso that the com-putation can be performed and the result stored in vector elements that are not consecutive \nand do not start at the beginning of the vector. For example, if the input stride is 2, the output stride \nis 4 and both offsets are 1, then the code generated for (I 2) will copy x(1), x(3) to y(1), y(5) (suppose \nsubscripts start from 0). The compiler also can vectorize the resulting code by adding an outer loop \nto the the code so the computation changes from A to A \u00ae In, where m is a parameter and A is a formula. \n 4. EXPERIMENTS To evaluate the SPL compiler, we present results on code generation of the FFT transforms \nFa,, with i between 1 and 20. We used a simple search strategy in these experiments. More elaborate strategies \nare possible and they may produce codes with better performance than the codes tabulated in this paper. \nHowever, the simple strategy we used was suf- ficient to match the performance of the codes generated \nby FFTW and demonstrate the viability of this approach. Our search strategy proceeded by first searching \nfor a good implementation for small-size transforms, 2 to 64, and then searching for a good implementation \nfor larger size trans- forms that use the small-size results as basic computation modules. For the small \nsizes, we used dynamic programming over all possible factorizations using Equation 10 (Section 2.1) and, \nfor each size, we selected the factorization with the lowest execution time. For larger sizes, we also \nused dy- namic programming over the factorization obtained by using the equation F,., = (F, @ I,)T~'\u00b0(I,. \n@ F,)L;\" with the restriction that r _< 64. The formula was applied recursively to Fo until s < 64. In \nthe case of large-size FFTs the dynamic programming algorithm kept the three best results at each stage \ninstead of just one as is usually done. To generate code for large size FFTs, templates for F~, 2 < r \n<_ 64, were created using for the intermediate code component the outcome of the search for smMl-size \ntransforms. The reason we chose this strategy is that it parallels the strategy followed by FFTW. The \nexperiments were carried out on the same platforms as described in Table 1. In the rest of this section, \nwe first present the performance results for small-size transforms, then the results for larger sizes. \nIn each case, we compare performance with FFTW. 4.1 Small Size FFFTs For small-size problems, after some \ntrial and error, we found that it is usually better to generate straight-line code because loop control \noverhead is eliminated and all the tem- porary variables can be scalars. Using scalars enables the back-end \ncompiler to do a better optimization job. Further- more, for small-size transforms, code size does not \naffect performance. For a given FFT size, the number of operations gener- ated by the compiler is fixed, \nand the performance depends exclusively on factors such as register allocation, memory access pattern, \nand instruction scheduling. These factors are influenced by the order in which the instructions appear \nin the source program. We found this order to be an impor- tant factor despite the fact that optimizing \ncompilers are supposed to do instruction reordering. In the experiments reported here, the different \ncomputation orders were exclu- sively the result of using different formulas. For example, the following \ntwo formulas are different factorizations of Fs: common definition (define F4 (compose (tensor (F 2)(I \n2) (T 4 2) (Sensor (I 2)(F 2)) (L 4 2)))) ; formula-1 (compose (tensor (F 2)(I 4)) (T 8 4) (tensor (I \n2) F4) (L 8 2)) ; formula-2 (compose (tensor F4 (I 2)) (T 8 2) (tensor (I 4)(F 2)) (L 8 4)) The correspondingFortran \ncode using comp~x arithmetics perform thesame computations but in different order: ; formula-1 ; formula-2 \nfO = x(1) + x(S) fO = x(1) + xCS) fl = x(1) -x(6) fl -x(1) -x(5) f2 = x(3) + x(7) f2 -x(2) + x(6) f3 \n= x(3) -x(7) f3 = x(2) -x(6) f4 = (O,-l)*f3 f4 -x(3) + x(7) f5 = fO + f2 f5 = x(3) x(Z) - f6 = fO -f2 \nf6 -x(4) + x(8) f7 = fl + f4 f7 = x(4) -x(8) f8 \" fl -f4 f8 = (0.7,-0.7)*f3 f9 = x(2) + x(6) f9 = (0,-1) \n* f5 flO = x(2) -x(6) flO = (-0.7,-0.7)*f7 fll = x(4) + x(8) fll = fO + f4 f12 = x(4) -x(8) fir = fO \n-f4 f13 = (0,-1)*f12 f13 -f2 + f6 f14 -f9 + fli f14 -f2 -f6 f15 = f9 -f11 f15 -(0,-1)*f14 f16 = flO \n+ f13 y(1) = fll + f13 f17 = flO -f13 y(5) = fll -f13 f18 = (0.7,-0.7)*f16 y(3) = f12 + f15 f19 = (0,-1) \n* f15 y(7) = f12 -f15  70\" FFTW oodekd   I-m- 8PL N (\u00a2ze of FFD (a) SPARC ,o. FFTW \u00a2~de~t e~ [,+ \n$PL I 500 4C0 10q o~, ; ...... ~ ,,, ~ N (|Ize of FF'r) (b) MIPS 600 [.o..~ 8PLFWIWeedelet [| I= | N \nOdze ~ FFr) (c) Pentium II Figure 3: Performance for small-size FFTs)  f20 = (-0.7,-0.7\")*f17 f20 = \nfl + f9 y(1) = f5 + f14 f21 = fl -f9 y(S) = f5 -f14 f22 = :f8 + fl0 y(2) = f7 + ~18 f23 = f8 -flO y(6) \n= f7 -f18 f24 = (0,-I)*f23 y(3) = f6 + f19 y(2) = f20 + f22 y(7) = f6 -f19 y(6) = f20 -f22 y(4) = :f8 \n+ f20 y(4) = :f21 + f24 y(8) = f8 -f20 y(8) = f21 -:f24 For FFTs of size 2, 4, 8, 16, 32, and 64, dynamic \npro- gramming was used on the formulas generated by Equation 10. Each formula was passed to the SPL compiler \nwith the unrolling flag turned on to enforce the generation of straight- line (loop free) code. We compared \nwith the performance of the FFTW codelets, a set of optimized straight-line code for small-size FFTs. \nThese codelets accept two parameters, \"istride\" and \"ostride\", which are used to control the access to \nthe input and output vectors. For each codelet, we measured the performance of the original code and \nof a modified version involving fewer instructions because it assumes that the stride is always 1. The \nmodified version was expected to be faster because it contains fewer instructions. However, this was \nnot always true. On the SPAI:tC machine, the modified version per- formed no better than the original \nversion and, in some cases, it was much slower. One explanation is that stride computations are integer \noperations that can be executed in parallel with floating point operations and removing the stride does \nnot necessarily reduce execution time. Vgriabil- ity caused by scheduling strategies probably account \nfor the slowdown. Figure 3 compares the performance of the code generated by the SPL compiler for small \nsize FFTs after performing the search described above with the performance of the FFTW codelets. The \nperformance is measured in terms of \"pseudo MFlops\", which is a value calculated by using the equation \n5N~og2N where N is the size of FFT and t is the execution $ time in microseconds. The performance of \nthe codes gener- ated by the SPL compiler is very close to the performance of the FFTW codelets. 4.2 \nLarge Size FFTs We decided to use straight-line code only for FFT sizes less than 64 in part to parallel \nFFTW but also because straight- line code for FFT sizes greater than 64 do not fit in the cache of the \nSPAKC we used in our experiments. For FFT sizes larger than 64, we factored the formula into FFTs of \nsize 2, 4, 8, 16, 32, or 64, and implemented the computation using loops containing the straight-line \ncodes generated for the small-size FFTs as discussed above. We used the best program resulting from the \nprevious search under the assumption that a good formula for small size FFTs also could be a good sub-formula \nfor larger size FFTs. It is possible we could have missed the actual best formula but following this \napproach significantly reduced the search space and made the search for large-size FFTs possible; In \nthis experiment, the search space was restricted to bi- natty Cooley-Tukey style factorization, as expressed \nin Equa- tion 5 (Section 2.1), and to right-most factorization, This means that when a FFT of size N \nis factored into two sub- FFTs of size Ni and N2, the sub-FFT of size Nz will not be factored further. \nOnly the second sub-FFT could be factored again. Again, this decision p~alleled FFTW and 400 L 100 2~ \n| 240 220 i 200 180 160 140 120 2^13 2^16 2^t2 2*20 N (s~z~ of FFT) (a) SPAP,,C .0, FFTW CS.~ [ -,~.. \nFFTW estimate ] ,0 %--~--~--.ts 2^13 ' ' ~7 2~ ~', 2:,o'2:. 2^12' ' 2:,, 2:,. 2:. 2:,, 2^1 B 2^~, 2^20 \nN (slze of FFT') (b) MIPS a ,A \"'.. k \"... o.. 2A-~7 J, i , i f.a. Fcr,u ] 2\"~8 2^9 2:10 27112\"121 2\"13 \n211427152716271727182:19 ' 2^2O N {roOf FFT) (c) Pentium II  Figure 4: Performance for large-size FFTs \nlOC ~:.-. FFTW e.otlmale t :.~i 8PL 80 7C g~ ~G IC 2^7 2.'~8 2*9 2A10 2\"11 2^12 2A13 2^14 2^15 2^16 \n2^17 2A18 2,~19 2^20 N (l~.,~, of FFI~ Figure 5: Memory consumption for large-size FFTs reduced the size \nof the search space. The search strategy we used for large-size FFTs is a mod- ified version of dynamic \nprogramming. Ordinary dynamic programming keeps the best result for each size. Our ver- sion keeps the \nthree best results for each size. This strategy increases the chance of finding the best formulas because \nthe best formula for one size is not necessarily also the best sub-formula for a larger size. In FFTW, \nlarge-size FFTs are computed recursively us- ing three components: the planner, the executor, and the \ncodelets. The planner searches for an optimal factorization at run-time using dynamic programming. This \nfactoriza- tion, called a plan, is then interpreted by the executor. The executor calls to the codelets \nin the order specified by the plan. FFTW also has an option to select plan by \"estimat- ing\" instead \nof measuring the execution time. This option saves time and memory. The performance results for FFT of \nsize 2 r to 2 2o are shown in Figure 4. One line (labeled \"SPL\") represents the performance of the loop \ncode generated by SPL compiler, a second line (labeled \"FFTW\") represents the performance of FFTW when \nthe the plan is chosen by measuring the ex- ecution time, and a third line (labeled \"FFTW estimate\") \nrepresents the performance of FFTW when the plan is cho- sen using estimation. The time for planning \nin FFTW was excluded from the measurement. As was the case for small-size FFTs, the performance of the \ncode generated by the SPL compiler is similar to the performance of FFTW. In addition to the performance, \nwe also measured mem- ory requirements. Figure 5 shows that the memory required to run the code generated \nby the SPL compiler is similar to the memory required by \"FFTW estimate\". More memory is required for \nFFTW to find a plan by measuring the exe- cution time. However, this is not a significant issue, because \nin FFTW one can save the plan to a file and re-use it in later sessions. An interesting observation of \nthe performance curves in Figure 4 is that there are two large drops in each graph. For example, for \nthe SPARC machine, they happened at size 2 9 and 2 zs. By analyzing the size of individual segments of \nthe executables, we found out that the drops are related to the x 10 -~4 ~.2 J~ 0.8 0.6 ,~i/.~.~ &#38;: \n0 0.4 0.2 __~_.~,,_ s,,, . . i f i i i I i 1 I f I i 2^1 2^2 2^3 2\"~4 2^5 21'6 2 ^7 2'~8 2N~ 2^t0 2A11 \n2^12 2^t3 2^t4 2^15 ~^t6 ~^17 2^18 N (\u00a2zo of FFI') Figure 6: Accuracy of FFT computation data cache. \nThe size of data segment exceeded L1 cache limit (16KB) when the problem size reached 29, and ex-ceeded \nL2 cache limit (2MB) when the problem size reached 21~. The next limitation would be the physical memory \nsize. Our SPARC machine has 128MB memory, about 90MB of which can be used to run a specific program. \nFor this rea- son, from size 221 to size 222 we expect to see another per- formance drop. The curves \nof the other two machines have similar shapes. It is not surprising that our loop code matched the perfor- \nmance of FFTW's recursive code. Our loop code was gener- ated by applying recursive factorization rules \nand hence fol- lowed the same computation order and data access pattern of the recursive code used by \nFFTW. One possible drawback of the loop code is that code size may increase faster than recursive code \nbecause recursive code can re-use \"codelets\" while loop code has to duplicate them. Our analysis, how- \never, showed that the increase of code size was very slow. The size of the text segment of the loop code \nfor size 220 was only 50 percent larger than that of size 27 . Finally, we measured the accuracy of the \ncomputation rep- resented by these code by using the package benchfft [4]. Figure 6 shows the relative \nerror of FFT of each size.  5. RELATED WORK Our approach is similar to that used by FFTW [5, 6]. However, \nFFTW is specific to the FFT. Since the algorithm and implementation are mixed in the package, it's not \neasy to extend the ability to other algorithms. A recent work by [17] showed how to modify FFTW to support \ndiscrete cosine transform. However, doing this requires a good un- derstanding of the internal mechanism \nof FFTW, especially the compiler that generates the codelets. This requirement is too much for an ordinary \nuser. Our use of SPL allows us to implement and optimize a far more general set of programs. FFTW also \nutilizes a special purpose compiler. However, their compiler is based on built-in code sequences for \na fixed set of FFT algorithms and is used for generating only codelets. FFTW uses runtime dynamic programming \nto search for efficient implementations, while our approach performs search at compile-time and allows \nmore flexible selection of search algorithm and search space. Similar to the work in FFTW is the package \ndescribed in [11] for computing the Walsh-Hadamard transform (WHT). This work is closer in spirit to \nthe work in this paper in that the different algorithms considered are expressed mathemat- ically and \nthe search is carried out over a space of formulas. Similar to FFTW, the WHT package is restricted to \na spe- cific transform, and a code generator restricted to the WriT is used rather than a compiler. Another \nclosely related work is EXTENT [3], which uses tensor product to represent block recursive algorithms \nand uses a translator to generate programs in high-level lan- guages. Our work differs from EXTENT in \nseveral way: at first, we provide a language for describing algorithms, while in EXTENT an \"application \nsubsystem\" has to be added for every algorithm. Secondly, our use of templates enable us to extend the \nalgorithm defining language and to change the code generation strategy without modifying the system; \nwhile in EXTENT, code generation is built into the translator so that it doesn't have this flexibility; \nthirdly, in EXTENT performance data is fed back to the user for man- ual performance tuning; while we \nuse search to automate the tuning procedure. Also related to our approach is the ATLAS project, whose \ngoal is portable high performance implementation of the Basic Linear Algebra Subroutines (BLAS) [18]. \nThey iso- late machine specific operations to several routines that deal with performing optimized on-chip, \ncache contained matrix multiply. General matrix multiply is built from these basic routines. The basic \nroutines are created by a code generator that searches for the correct blocking and loop unrolling fac- \ntors based on timing information. PHiPAC[2] presents some guidelines for writing compiler friendly high \nperformance C programs and makes use of code generators to create spe- cial purpose (for matrix multiply) \nC code that follows these guidelines. Their code generators are parameterized so that they can tune performance \nby searching over these parame- ters. Our work differs from theirs in that they are focusing on general \nlinear algebra routines, while we are interested in different signal transform algorithms. Furthermore, \ntheir code generators are hand-coded, while we can automate the entire procedure through the use of a \ndomain-specific lan- guage and compiler. We use compile-time search to find optimal implementa- tions. \nKisuki and Knijnenberg [12] presented similar ideas using the term \"iterative compilation\". They experimented \nwith loop tiling, loop unrolling and array padding with dif- ferent parameters, as well as different \nsearch algorithms. Their results appear to be good compared with static se-lection algorithms. One difference \nbetween their work and ours is that we allow a wider range of search objects because, in addition to \ncompiler options, input programs can be in the search space. 6. CONCLUSIONS We have presented a domain-specific \nlanguage, SPL, and compiler for representing and implementing signal process- ing algorithms. The SPL \ncompiler applies several optimiza- tions and transformations and is extensible in that new oper- ators \nand their semantics can be introduced using a pattern matching mechanism and templates. When the compiler \nis combined with an algorithm generator and a search engine, as is the case in SPIRAL, it is possible \nto automatically search for optimal implementations. This paper shows that when the compiler is incorporated \ninto a search of FFT fac- [5] M. ~rigo. A fast fourier transform compiler. In PLD[ torizations, it produces \nlibrary routines that axe competitive in their performance with those generated by FFTW. The SPL compiler, \nhowever, is more general in that it can gener- ate libraries not only for FFT computations, as is the \ncase with FFTW, but also for many other classes of algorithms. The results presented in the paper illustrate \nthe power of domain-specific languages and compilers. By focusing on a particular domain, the process \nof generating highly-tuned code can be automated to a degree that is not possible in more general contexts, \nat least with today's technology. The signal processing domain is particularly attractive because of \nthe many applications of signal processing algorithms and the degree of interest that exist today in \nthese applications. However, signal processing is not necessarily better suited for an approach like \nthe one discussed in this paper than other problem domains. It is likely, therefore, that we will see \nmany more domain-specific projects in other areas in the near future.  7. ACKNOWLEDGMENTS This work \nwas partially supported by DAI:tPA through re- search grant DABT63-98-1-0004 administered by the Army \nDirectorate of Contracting.  8. REFERENCES [1] L. Auslander, J. Johnson, and R. W. Johnson. Automatic \nimplementation of FFT algorithms. Technical Report DU-MCS-96.-01, Dept. of Math. and Computer Science, \nDrexel University, Philadelphia, PA, June 1996. Presented at the DARPA ACMP PI meeting. [2] J. Bilmes, \nK. Asanovic, C. Chin, and J. Demmel. Optimizing matrix multiply using phipac: a portable, high-performance, \nansi c code methodology. In Proc. ICS1997, 1997. h~p : //www. icsi .berkeley. edu/,-~bilmes/phlpac/. \n [3] D. L. Dai, S. K. S. Gupta, S. D. Kaushik, J. H. Lu, 11.V. Singh, C.-H. Huang, P. Sadayappan, and \nR. W. Johnson. EXTENT: A portable programming environment for designing and implementing high-performance \nblock recursive algorithms. In Supercomputing 199~, pages 49-58, 1994. [4] M. Frigo. BenchFFT. http://www, \nfftv. org/benchfft/. '99, pages 189--180, 1999. [6] M. Prigo and S. G. Johnson. FFTW: An adaptive software \narchitecture for the FFT. In [CASSP '98, volume 3, pages 1381-1384, 1998. http ://www. f:ftw, o~:g. \n[7] M. T. H. Henrik V. Sorenson, C. Sidney Burrus. Fast Fourier Transform Database. PWS Publishing Company, \nBoston, 1995. [8] J. Johnson, tL Johnson, D. Padua, and J. Xiong. TPL: Tensor Product Language, 1999. \n http : l/~w. ace. cmu. edul~splral/tpl, html. [9] J. R. Johnson and R. W. Johnson. Challenges of computing \nthe fast Fourier transform. In Proc. Optimized Portable Application Libraries (OPAL) Workshop, A DARPA/NSF \nsponsored workshop in Kansas City, June. P-3, 1997. [10] J. R. Johnson, It. W. Johnson, D. Rodriguez, \nand R. Tolimieri. A methodology for designing, modifying, and implementing Fourier transform algorithms \non various architectures. Circuits, Systems, and Signal Processing, 9(4):449---500, 1990. [11] J. It. \nJohnson and M. Pfischel. In search of the optimal Walsh-Hadam0xd transform. In Proc. ICASSP PO00, 2000. \n[12] T. Kisuki, P.M.W.Knijnenberg, M.F.P.O'Boyle, and H.A.G.Wijshoff. Iterative compilation in program \noptimization. In Proc. CPC$O00, pages 35--44, 2000. [13] J. M. F. Mourn, J. Johnson, R. W. Johnson, D. \nPadua, V. Prasanna, M. Piischel, and M. M. Veloso. SPIRAL: Portable Library of Optimized Signal Processing \nAlgorithms, 1998. http://~nn~, ece. cmu. edu/~spiral. [14] K. R. Rao and P. Yip. Discrete Cosine Transform. \nAcademic Press, 1990. [15] R. Tolimieri, M. An, and C. Lu. Algorithms for discrete Fourier trans/orms \nand convolution. Springer, 2nd edition, 1997. [16] C. Van Loan. Computational Framework o~ the Fast Fourier \nTrans/orm. Siam, 1992. [17] R. Vuduc and J. Demmel. Code generators for automatic tuning of numerical \nkernels: Experiences with fftw. In ICFP $000, 2000. [18] It. C. Whaley and J. Dongarra. Automatically \ntuned linear algebra software (ATLAS), 1998. http://www, netllb, oft/atlas/.   \n\t\t\t", "proc_id": "378795", "abstract": "<p>We discuss the design and implementation of a compiler that translates formulas representing signal processing transforms into efficient C or Fortran programs. The formulas are represented in a language that we call SPL, an acronym from Signal Processing Language. The compiler is a component of the SPIRAL system which makes use of formula transformations and intelligent search strategies to automatically generate optimized digital signal processing (DSP) libraries. After a discussion of the translation and optimization techniques implemented in the compiler, we use SPL formulations of the fast Fourier transform (FFT) to evaluate the compiler. Our results show that SPIRAL, which can be used to implement many classes of algorithms, produces programs that perform as well as &#8220;hard-wired&#8221; systems like FFTW.</p>", "authors": [{"name": "Jianxin Xiong", "author_profile_id": "81100313918", "affiliation": "Department of Computer Science, University of Illinois at Urbana-Champaign, Urbana, IL", "person_id": "P334856", "email_address": "", "orcid_id": ""}, {"name": "Jeremy Johnson", "author_profile_id": "81100452088", "affiliation": "MathStar Inc., Minneapolis, MN", "person_id": "PP39043399", "email_address": "", "orcid_id": ""}, {"name": "Robert Johnson", "author_profile_id": "81408599717", "affiliation": "", "person_id": "PP14159361", "email_address": "", "orcid_id": ""}, {"name": "David Padua", "author_profile_id": "81452612804", "affiliation": "Department of Computer Science, University of Illinois at Urbana-Champaign, Urbana, IL", "person_id": "P63208", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/378795.378860", "year": "2001", "article_id": "378860", "conference": "PLDI", "title": "SPL: a language and compiler for DSP algorithms", "url": "http://dl.acm.org/citation.cfm?id=378860"}