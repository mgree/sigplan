{"article_publication_date": "05-01-2001", "fulltext": "\n inerementalized Pointer and Escape Analysis* Fred6ric Vivien Martin Rinard ICPS/LSIIT Laboratory for \nComputer Science Universit6 Louis Pasteur Massachusetts Institute of Technology Strasbourg, France Cambridge, \nMA 02139 vivien @ icps. u-strasbg.fr rinard@lcs.mit.edu ABSTRACT We present a new pointer and escape \nanalysis. Instead of analyzing the whole program, the algorithm incrementally analyzes only those parts \nof the program that may deliver useful results. An analysis policy monitors the analysis re- sults to \ndirect the incremental investment of analysis re-sources to those parts of the program that offer the \nhighest expected optimization return. Our experimental results show that almost all of the ob- jects \nare allocated at a small number of allocation sites and that an incremental analysis of a small region \nof the program surrounding each site can deliver almost all of the benefit of a whole-program analysis. \nOur analysis policy is usually able to deliver this benefit at a fraction of the whole-program analysis \ncost. 1. INTRODUCTION Program analysis research has focused on two kinds of analyses: local analyses, \nwhich analyze a single procedure, and whole-program analyses, which analyze the entire pro- gram. Local \nanalyses fail to exploit information available across procedure boundaries; whole-program analyses are \npotentially quite expensive for large programs and are prob- lematic when parts of the program are not \navailable in ana- lyzable form. This paper describes our experience incrementalizing an existing whole-program \nanalysis so that it can analyze arbi- trary regions of complete or incomplete programs. The new analysis \ncan 1) analyze each method independently of its caller methods, 2) skip the analysis of potentially invoked \nmethods, and 3) incrementally incorporate analysis results from previously skipped methods into an existing \nanalysis result. These features promote a structure in which the algorithm executes under the direction \nof an analysis pol- icy. The policy continuously monitors the analysis results to direct the incremental \ninvestment of analysis resources *The full version of this paper is available at www.cag.lcs.mit.edu/~rinard/paper/pldi01.fulLps. \nThis research was done while Frdddric Vivien was a Visiting ProfesSor in the MIT Laboratory for Computer \nScience. The research was supported in part by DARPA/AFRL Contract F33615-00-C-1692, NSF Grant CCR00-86154, \nand NSF Grant CCR00-63513. Permission to make digital or hard copies of all or part of this work for \npersonal or classroom use is granted without fee provided that copies are not made or distributed for \nprofit or commercial advan- tage and that copies bear this notice and the full citation on the first \npage. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior \nspecific permission and/or a fee. PLD/2001 6/01 Snowbird, Utah, USA &#38;#169; 2001 ACM ISBN 1-58113.414-Z/01106,,.$5.00 \n to those parts of the program that offer the most attrac-tive return (in terms of optimization opportunities) \non the invested resources. Our experimental results indicate that this approach usually delivers almost \nall of the benefit of the whole-program analysis, but at a fract!on of the cost. 1.1 Analysis Overview \nOur analysis incrementalizes an existing whole-program analysis for extracting points-to and escape information \n[16]. The basic abstraction in this analysis is a points-to escape graph. The nodes of the graph represent \nobjects; the edges represent references between objects. In addition to points- to information, the analysis \nrecords how objects escape the currently analyzed region of the program to be accessed by unanalyzed \nregions. An object may escape to an unanalyzed caller via a parameter passed into the analyzed region \nor via the return value. It may also escape m a potentially invoked but unanalyzed method via a parameter \npassed into that method. Finally, it may escape via a global variable or parallel thread. If an object \ndoes not escape, it is captured. The analysis is flow sensitive, context sensitive, and com- positional. \nGuided by the analysis policy, it performs an incremental analysis of the neighborhood of the program \nsurrounding selected object allocation sites. When it first analyzes a method, it skips the analysis \nof all potentially invoked methods, but maintains enough information to re- construct the result of analyzing \nthese methods should it become desirable to do so. The analysis policy then ex-amines the graph to find \nobjects that escape, directing the incremental integration of (possibly cached) analysis results from \npotential callers (if the object escapes to the caller) or potentially invoked methods (if the object \nescapes into these methods). Because the analysis has complete infor- mation about captured objects, \nthe goal is to analyze just enough of the program to capture objects of interest. 1.2 Analysis Policy \nWe formulate the analysis policy as a solution to an in- vestment problem. At each step of the analysis, \nthe policy can invest analysis resources in any one of several allocation sites in an attempt to capture \nthe objects allocated at that site. To invest its resources wisely, the policy uses empirical data from \nprevious analyses, the current analysis result for each site, and profiling data from a previous training \nrun to estimate the marginal return on invested analysis resources for each site. During the analysis, \nthe allocation sites compete for re- sources. At each step, the policy invests its next unit of analysis \nresources in the allocation site that offers the best 35 marginal return. When the unit expires, the \npolicy recom- putes the estimated returns and again invests in the (po- tentially different) allocation \nsite with the best estimated marginal return. As the analysis proceeds and the policy obtains more information \nabout each allocation site, the marginal return estimates become more accurate and the quality of the \ninvestment decisions improves. 1.3 Analysis Uses We use the analysis results to enable a stack allocation \nop- timization. If the analyis captures an object in a method, it is unreachable once the method returns. \nIn this case, the generated code allocates the object in the activation record of the method rather than \nin the heap. Other opti- mization uses include synchronization elimination and the elimination of ScopedMemory \nchecks in Real-Time Java [6]. Potential software engineering uses include the evaluation of programmer \nhypotheses regarding points-to and escape in- formation for specific objects, the discovery of methods \nwith no externally visible side effects, and the extraction of infor- mation about how methods access \ndata from the enclosing environment. Becmlse the analysis is designed to be driven by an analy- sis policy \nto explore only those regions of the program that are relevant to a specific artalysis goal, we expect \nthe analysis to be particularly useful in settings (such as dynamic com- pilers and interactive software \nengineering tools) in which it must quickly answer queries about specific objects. 1,4 Context In general, \na base analysis must have several key proper- ties to be a good candidate for incrementalization: it \nmust be able to analyze methods independently of their callers, it must be able to skip the analysis \nof invoked methods, and it must be able to recognize when a partial analysis of the program has given \nit enough information to apply the desired optimization. Algorithms that incorporate escape information \nare good candidates for incrementalization be- cause they enable the analysis to recognize captured objects \n(for which it has complete information). As discussed fur- ther in Section 7, many existing escape analyses \neither have or can easily be extended to have the other two key prop- erties [14, 7, 3]. Many of these \nalgorithms are significantly more efficient than our base algorithm, and we would expect incrementalization \nto provide these algorithms with addi- tional efficiency increases comparable to those we observed for \nour algorithm. An arguably more important benefit is the fact that incre- mentatized algorithms usually \nanalyze only a local neighbor- hood of the program surrounding each object allocation site. The analysis \ntime for each site is therefore independent of the overall size of the program, enabling the analysis \nto scale to handle programs of arbitrary size. And incrementalized algorithms can analyze incomplete \nprograms. 1.5 Contributions This paper makes the following contributions: \u00ae Analysis Approach: It presents \nan incremental ap- proach to program analysis. Instead of analyzing the entire program, the analysis \nis focused by an analysis policy to incrementally analyze only those regions of the program that may \nprovide useful results. o Analysis Algorithm: It presents a new combined pointer and escape analysis \nalgorithm bazed on the incremental approach described above. Analysis Policy: It formulates the analysis \npolicy as a solution to an investment problem. Presented with several analysis opportunities, the analysis \npolicy in- crementally invests analysis resources in those oppor- tunities that offer the best estimated \nmarginal return. \u00ae Experimental Results: Our experimental results show that, for our benchmark programs, \nour analysis policy delivers almost all of the benefit of the whole- program analysis at a fraction of \nthe cost. The remainder of the paper is structured as follows. Sec- tion 2 presents several examples. \nSection 3 presents our pre- viously published base whole-program analysis [16]; readers familiar with \nthis analysis cart skip this section. Section 4 presents the incrementalized analysis. Section 5 presents \nthe analysis policy; Section 6 presents experimental results. Section 7 discusses related work; we conclude \nin Section 8.  2. EXAMPLES We next present several examples that illustrate the basic approach of our \nanalysis. Figure 1 presents two classes: the complex class, which implements a complex number pack- age, \nand the client class, which uses the package. The complex class uses two mechanisms for returning values \nto callers: the add and multiplyAdd methods write the re-sult into the receiver object (the this object), \nwhile the multiply method allocates a new object to hold the result. 2.1 The compute Method We assume \nthat the analysis policy first targets the object allocation site at line 3 of the compute method. The \ngoal is to capture the objects allocated at this site and allocate them on the call stack. The initial \nanalysis of compute skips the call to the mulzip1yhdd method. Because the analysis is flow sensitive, \nit produces a points-to escape graph for each program point in the compute method. Because the stack \nallocation optimization ties object lifetimes to method lifetimes, the legality of this optimization \nis determined by the points-to escape graph at the end of the method. Figure 2 presents the points-to \nescape graph from the end of the compute method. The solid nodes are inside nodes, which represent objects \ncreated inside the currently analyzed region of the program. Node 3 is an inside node that represents \nall objects created at line 3 in the compute method. The dashed nodes are outside nodes, which rep- resent \nobjects not identified as created inside the currently analyzed region of the program. Nodes 1 and 2 \nare a kind of outside node called a parameter node; they represent the parameters to the compute method. \nThe analysis result also records the skipped call sites and the actual parameters at each site. In this \ncase, the analysis policy notices that the target node (node 3) escapes because it is a parameter to \nthe skipped call to mult\u00a3plyAdd. It therefore directs the al- gorithm to analyze the multiplyAdd method \nand integrate the result into the points-to escape graph from the program point at the end of the compute \nmethod. Figure 3 presents the points-to escape graph from the ini- tialanalysis of the multiplyAdd method. \nNodes 4 through class complex { double x,y; complex(double a, double b) { x = a; y = b; } void add(complex \nu, complex v) { x = u.x+v.x; y = u.y+v.y; } complex multiply(complex m) { complex r = new complex(x*m.x-y*m.y, \nx*m.y+y*m.x); return(r); } void multiplyAdd(complex a, complex b, complex c) { complex s = b.multiply(c); \nthis.add(a, s); } } class client { public static void compute(complex d, complex e) { 3: complex t = \nnew complex(O.O, 0.0); t.multiplyAdd(d,e,e); } } Figure 1: Complex Number and Client Classes Points-to \nEscape Graph Skipped Method Calls   t--~(~) 0 inside node (:) outside node Figure 2: Analysis Result \nfrom compute Method Points-to Escape Graph Skipped Method Calls this ---~, 4, .., ~8 ,= ',6\",. multiply \n( ,'~;,,' \"\" C---~: 7 ; a--,< 5/ s--*({\" ',4, .add(, 5, , Figure 3: Analysis Result from multiplyAdd \nMethod Points-toEscape Graph Skipped Method Calls d---~ I; ,8 ,=~2 ,.multi.ply(, 2,,) e---~,2) Q  t---,-@ \n Figure 4: Analysis Result from compute Method after Integrating Result from multiplyAdd Points-to Escape \nGraph Skipped Method Calls  d----','l) \" ,8. =, 2/. multiply ( ',~\" ) e----* v,2) / \" / '  t---~(~) \n Figure 5: Analysis Result from compute Method after Integrating Results from multiplyAdd and add 7 are \nparameter nodes. Node 8 is another kind of outside node: a return node that represents the return value \nof an unaaalyzed method, in this case the multiply method. To integrate this graph into the caller graph \nfrom the compute method, the analysis first maps the parameter nodes from the multiplyAdd method to the \nnodes that represent the actual parameters at the call site. In our example, node 4 maps to node 3, node \n5 maps to node 1, and nodes 6 and 7 both map to node 2. The analysis uses this mapping to combine the \ngraphs into the new graph in Figure 4. The analysis policy examines the new graph and determines that \nthe target node now escapes via the call to the add method. It therefore directs the algorithm to analyze \nthe add method and integrate the resulting points-to escape graph into the current graph for the compute \nmethod. Note that because the call to the multiply method has no effect on the es-cape status of the \ntarget node, the analysis policy directs the algorithm to leave this method unanalyzed. Figure 5 presents \nthe new graph after the integration of the graph from the add method. Because the add method does not \nchange the points-to or escape information, the net effect is simply to remove the skipped call to the \nadd method. Note that the target node (node 3) is captured in this graph, which implies that it is not \naccessible when the compute method returns. The compiler can therefore generate code that allocates all \nobjects from the corresponding allocation site in the activation record of this method. 2.2 The multiply \nMethod The analysis next taxgets the object allocation site in-~ side the multiply method. The points-to \nescape graph from this method indicates that the taxget node escapes to the caller (in this case the \nmultiplyAdd method) via the return value. The algorithm avoids repeated method re-analyses by retrieving \nthe cached points-to escape graph for the multiplyAdd method, then integrating the graph from the multiply \nmethod into this cached graph. The result is cached as the new (more precise) points-to escape graph \nfor the multiplyhdd method. It indicates that the target node does not escape to the caller of the multiplyAdd \nmethod, but does escape via the unanalyzed call to the add method. The analysis therefore retrieves the \ncached points-to escape graph from the add method, then integrates this graph into the current graph \nfrom the multiplyAdd method, once again caching the result as the graph for the multiplyAdd method. The \ntarget node is captured in this graph --it escapes its enclosing method (the multiply method), but is \nrecaptured in a caller (the multiplyAdd method). At this point the compiler has several options: it can \ninlirte the multiply method into the multiplyAdd method and allocate the object on the stack, or it cart \npreallocate the object on the stack frame of the multiplyAdd method, then pass it in by reference to \na specialized version of the multiply routine. Both options enable stack allocation even if the node \nis captured in some but not all invocation paths, if the analysis policy declines to analyze all potential \ncallers, or if it is not possible to identify all potential callers at com- pile time. Our implemented \ncompiler uses inlining. 2.3 Object Field Accesses Our next example illustrates how the analysis deals \nwith object field accesses. Figure 6 presents a rational number class that deals with return values in \nyet another way. Each 37 Rational object has a field called result; the methods in Figure 6 that operate \non these objects store the result of their computation in this field for the caller to access. We next \ndiscuss how the analysis policy guides the analy- sis for the Rational allocation site at line I in the \nevaluate method. Figure 7 presents the initial analysis result at the end of this method. The dashed \nedge between nodes I and 2 is an outside edge, which represents references not iden- tified as created \ninside the currently analyzed region of the program. Outside edges always point from an escaped node \nto a new kind of outside node, a load node, which represents objects whose references are loaded at a \ngiven load state- ment, in this case the statement n = r.result at line 2 in the evaluate method. The \nanalysis policy notices that the target node (node 1) escapes via a call to the abs method. It therefore \ndirects the analysis to analyze abs and integrate the result into the re- sult from the end of the evaluate \nmethod. Figure 8 presents the analysis result from the end of the abs method. Node 3 represents the receiver \nobject, node 4 represents the object created at line 4 of the abs method, and node 5 represents the object \ncreated at line 5. The solid edges from node 3 to nodes 4 and 5 are inside edges. Inside edges represent \nref- erences created within the analyzed region of the program, in this case the abs method. The algorithm \nnext integrates this graph into the analysis result from evaluate. The goal is to reconstruct the result \nof the base whole-program analysis. In the base analysis, which does not skip call sites, the analysis \nof abs changes the points-to escape graph at the program point after the call site. These changes in \nturn affect the analysis of the statements in evaluate after the call to abs. The incremen- talized analysis \nreconstructs the analysis result as follows. It first determines that node 3 represented node 1 during \nthe analysis of abs. It then matches the outside edge against the two inside edges to determine that, \nduring the analysis of the region of evaluate after the skipped call to abs, the outside edge from node \n1 to node 2 represented the inside edges from node 3 to nodes 4 and 5, and that the load node 2 therefore \nrepresented nodes 4 and 5. The combined graph therefore contains inside edges from node 1 to nodes 4 \nand 5. Because node 1 is captured, the analysis removes the outside edge from this node. Finally, the \nupdated analysis replaces the load node 2 in the skipped call site to scale with nodes 4 and 5. At this \npoint the analysis has captured node 1 inside the evaluate method, enabling the compiler to stack allocate \nall of the objects created at the corresponding allocation site at line 1 in Figure 6. 3. THE BASE ANALYSIS \n The base analysis is a previously published points-to and escape analysis [16]. For completeness, we \npresent the algo- rithm again here. The algorithm is compositional, analyzing each method once before \nits callers to extract a single pa- rameterized analysis result that can be specialized for use at different \ncall sites.l It therefore analyzes the program in a bottom-up fashion from the leaves of the call graph \ntowards the root. To simplify the presentation we ignore static class variables, exceptions, and return \nvalues. Our implemented algorithm correctly handles all of these features. 1 Recursive programs require \na fixed-point algorithm that may analyze methods involved in cycles in the call graph multiple times. \nclass Rational { int numerator, denominator; Rational result; Rational(int n, int d) { numerator = n; \ndenominator = d; } void scale(int m) { result = new Rational(numerator * m, denominator); } void abs() \n{ int n = numerator; int d = denominator; if (n < O) n = -n; if (d < 0) d = -d; if (d Z n == 0) { 4: \nresult ffi new Rational(n / d, 1); } else { 5: result = new Rational(n, d); } } } class client { public \nstatic void evaluate(int i, int j) { I: Rational r = new Rational(0.0, 0.0); r.absO; 2: Rational n = \nr.result; n.scale(m); } } Figure 6: Rational Number and Client Classes Points-toEscape Graph Skipped \nMethod Calls result --Q r---~ i~ ........ ~i ) ,\", .abs() ,2, scale() n ...\" inside edge ...... outsideedge \nFigure 7: Analysis Result from evaluate Method Points-toEscape Graph Skipped Method Calls Figure 8: \nAnalysis Result from abs Method Points-to Escape Graph Skipped Method Calls result ~G Figure 9: Analysis \nResult from evaluate After Inte- grating Result from abs 3.1 Object Representation The analysis represents \nthe objects that the program ma- nipulates using a set n 6 N of nodes, which consists of a set Ni of \ninside nodes and a set No of outside nodes. Inside nodes represent objects created inside the currently \nanalyzed region of the program. There is one inside node for each object allocation site; that node represents \nall ob- jects created at that site. The inside nodes include the set of thread nodes NT C Ni. Thread \nnodes represent thread objects, i.e. objects that inherit from Thread or implement the Runnable interface. \nThe set of parameter nodes Np C No represents objects passed as parameters into the currently analyzed \nmethod. There is one load node n 6 NL C_ No for each load state- ment in the program; that node represents \nall objects whose references are 1) loaded at that statement, and 2) not iden- tified as created inside \nthe currently analyzed region of the program. There is also a set f E F of fields in objects, a set v \n6 V of local or parameter variables, and a set 1 E L C V of local variables. 3.2 Points-To Escape Graphs \nA points-to escape graph is a pair (O, I), where  O C (N \u00d7 F) \u00d7 NL is a set of outside edges. We write \nan edge ((nl, :f), n2) as nl --~ n2.  I C_ ((N \u00d7 F) x N) U (V \u00d7 N) is a set of inside edges. We write \nan edge (v,n) as v ~ n and an edge ((nl,f),n2)  f as nl ~ T~2. A node escapes if it is reachable in \nOUI from a parameter node or a thread node. We formalize this notion by defining an escape function eo,l \n(n) = {n' 6 NT U Np.n is reachable from n' in O U I} that returns the set of parameter and thread nodes \nthrough which n escapes. We define the concepts of escaped and captured nodes as follows: escaped((O, \nI), n) if cod(n) # O  captured((O, I), n) if eod(n) = O  We say that an allocation site escapes or \nis captured in the context of a given analysis if the corresponding inside node is escaped or captured \nin the points-to escape graph that the analysis produces. 3.3 Program Representation The algorithm represents \nthe computation of each method using a control flow graph. We assume the program has been preprocessed \nso that all statements relevant to the analy- sis are either a copy statement I = v, a load statement \nit = 12.f, a store statement l~.f = 12, an object allo- cation statement 1 = new cl, or a method call \nstatement lo.op(ll,... ,ik). 3.4 Intraprocedural Analysis The intraprocedural analysis is a forward dataflow \nanaly- sis that produces a points-to escape graph for each program point in the method. Each method is \nanalyzed under the assumption that the parameters are maximally unaliased, i.e., point to different objects. \nFor a method with formal parameters vo,... , v~, the initial points-to escape graph at the entry point \nof the method is (0,{(vl,nv~).l < i < n}) where nv~ is the parameter node for parameter vi. If the method \nis invoked in a context where some of the parame- ters may point to the same object, the interprocedural \nanal- ysis described below in Section 3.5 merges parameter nodes to conservatively model the effect of \nthe aliasing. Statement Existing Edges Generated Edges 1 1 l=v ii= 12.f ~f ~f 11 11  = 12.f It 11~ \n11 12 ~@ 12 @~ J~\" -- where (~) escaped O is the load node for 11 = 12.f l~.f= 12 ~0 I / I I= newcl \ni I ' @ where ~ is the inside node for l=new\u00a2l existing inside edge ~ generated inside edge .... ~ existing \noutside edge .... * generated outside edge C) inside node or outside node Figure 10: Generated Edges \nfor Basic Statements The transfer function (O', I') = ~st]((O,I)) models the effect of each statement \nst on the current points-to escape graph. Figure 10 graphically presents the rules that deter- mine the \nnew graph for each statement. Each row in this figure contains three items: a statement, a graphical \nrepre- sentation of existing edges, and a graphical representation of the existing edges plus the new \nedges that the statement generates. Two of the rows (for statements 11 = 12.f and 1 = new cl) also have \na where clause that specifies a set of side conditions. The interpretation of each row is that when- \never the points-to escape graph contains the existing edges and the side conditions are satisfied, the \ntransfer function for the statement generates the new edges. Assignments to 39 a variable kill existing \nedges from that variable; assignments to fields of objects leave existing edges in place. At control- \nflow merges, the analysis takes the union of the inside and outside edges. At the end of the method, \nthe analysis re- moves all captured nodes and local or parameter variables from the points-to escape \ngraph. 3.5 Interprocedural Analysis At each call statement, the interprocedural analysis uses the analysis \nresult from each potentially invoked method to compute a transfer function for the statement. We assume \na call site of the form 10.op(lt,... , lk), a potentially invoked method op with formal parameters vo,... \n,vk, a points-to escape graph <Or, It) at the program point before the call site, and a graph IO2,/2) \nfrom the end of op. A map # C_ N x N combines the callee graph into the caller graph. The map serves \ntwo purposes: 1) it maps each outside node in the callee to the nodes in the caller that it represents \nduring the analysis of the callee, and 2) it maps each node in the callee to itself if that node should \nbe present in the combined graph. We use the notation #(n) = {W.<n, n'> e #} and nt --~ n~ for n2 e #(nt). \nThe interprocedural mapping algorithm (<O, I), #) = map(<Ot,I1), <O2, Is),p) starts with the points-to \nescape graph (Or, It) from the caller, the graph (O2, Is) from the callee, and an initial parameter map \n{ Zt(li) if{n}=Z~(v,) ~(n) = 0 otherwise that maps each parameter node from the callee to the nodes \nthat represent the corresponding actual parameters at the call site. It produces the new mapped edges \nfrom the callee <0, I) and the new map #. Figure 11 presents the constraints that define the new edges \n(O, I) and new map #. Constraint 1 initializes the map # to the initial parameter map/~. Constraint 2 \nextends #, matching outside edges from the cailee against edges from the caller to ensure that # maps \neach outside node from the callee to the corresponding nodes in the caller that it represents during \nthe analysis of the callee. Constraint 3 extends # to model situations in which aliasing in the caller \ncauses an outside node from the callee to represent other callee nodes during the analysis of the callee. \nConstraints 4 and 5 complete the map by computing which nodes from the callee should be present in the \ncaller and mapping these nodes to themselves. Constraints 6 and 7 use the map to translate inside and \noutside edges from the callee into the caller. The new graph at the program point after the call site \nis <I1 U I, Ot U O). Because of dynamic dispatch, a single call site may invoke several different methods. \nThe transfer function therefore merges the points-to escape graphs from the analysis of all potentially \ninvoked methods to derive the new graph at the point after the call site. The current implementation \nobtains this call graph information using a variant of a cartesian product type analysis [1], but it \ncan use any conservative approximation to the dynamic call graph. 3.6 Merge Optimization As presented \nso far, the analysis may generate points-to escape graphs <O, I) in which a node n may have multiple \ndistinct outside edges n ~ nt,... ,n -~ nk E O. We elimi- nate this inefficiency by merging the load \nnodes nl,... , nk. fi(n) g #(n) (1) f f nt --~ n2 E 02,n3 -+ n4 E Ot U It,m --~ n3 (2) n2 ~n4 2 (3) nt \n---~n4 E 02,n2-+n~ E02 U I2 ~(n4) C #(ns) nl~n2~12,nt \">n, n2CNI (4) n~ --% n2 2 nt -+ n2 60s,nt ~ n, \nescaped((O,I),n) (5) n2 ~ n2 nl ~n2 EI2 (6) (~(nt) x {~}) \u00d7 #(ns) C I nl -~ n2 e Os, n2 ---% ns (7) (#(hi) \n\u00d7 {f}) \u00d7 {n2} C_ 0 Figure 11: Constraints for Interprocedural Analysis With this optimization, a single \nload node may be associ- ated with multiple load statements. The load node gener- ated from the merge \nof k load nodes nl ,... , nk is associated with all of the statements of nt,... , nk. 4. THE INCREMENTALIZED \nANALYSIS We next describe how to incrementalize the base algo- rithm --how to enhance the algorithm so \nthat it can skip the analysis of call sites while maintaining enough infor- mation to reconstruct the \nresult of analyzing the invoked methods should the analysis policy direct the analysis to do so. The \nfirst step is to record the set S of skipped call sites. For each skipped call site s, the analysis records \nthe invoked method ops and the initial parameter map ~s that the base algorithm would compute at that \ncall site. To sim- plify the presentation, we assume that each skipped call site is 1) executed at most \nonce, and 2) invokes a single method. Section 4.8 discusses how we eliminate these restrictions in our \nimplemented algorithm. The next step is to define an updated escape function es,oj that determines how \nobjects escape the currently an- alyzed region of the program via skipped call sites: es,o,l(n) = {s \nE S.3nl E Np.nl ~ n2 and n is reachable from n2 in O U I} U eO,l(n) We adapt the interprocedural mapping \nalgorithm from Sec- tion 3.5 to use this updated escape function. By definition, n escapes through a \ncall site s if s E es,oj(n). A key complication is preserving flow sensitivity with re- spect to previously \nskipped call sites during the integration of analysis results from those sites. For optimization pur- \nposes, the compiler works with the analysis result from the end of the method. But the skipped call sites \noccur at var- ious program points inside the method. We therefore aug- ment the points-to escape graphs \nfrom the base analysis with several orders, which record ordering information between edges in the points-to \nescape graph and skipped call sites: e w C S x ((N x {:~}) x NL). For each call site s, w(s) = {n~ -~ \nnz.(s, n~ -\u00a3~ n2) ~ co} is the set of outside edges that the analysis generates before it skips s. \u00ae \nt C S x ((N x {:f}) x N). For each call site s~ t(s) = {nl .L~ nu.(s, nl ~ n2) E t} is the set of inside \nedges that the analysis generates before it skips s. \u00ae v C S x ((N x {f}) x NL). For each call site s, \nT(s) = {nl --~ n2.(s, nl --~ n2) E ~-} is the set of outside edges that the analysis generates after \nit skips s. \u00ae ~, C S x ((N x {f}) x N). For each call site s, u(s) = {nl -~ n2.(s, nl ~ n2) E ~,} is \nthe set of inside edges that the analysis generates after it skips s. \u00ae fl C S x S. For each call site \ns, f~(s) = {st.(s,s') e fl} is the set of call sites that the analysis skips before skipping s. * a C \nSx S. For each call site s, a(s) = {s'.(s,s') e c~} is the set of call sites that the analysis skips \nafter skipping s. The incrementalized analysis works with augmented points- to escape graphs of the \nform (O,I,S,w,t,v,~,,fl, a). Note that because fl and a are inverses, 2 the analysis does not need to \nrepresent both explicitly. It is of course possible to use any conservative approximation of w, t, r, \nu, f~ and a; an especially simple approach uses w(s) = r(s) = O, t(s) ---- v(s) = I, and ;3(s) = or(s) \n= S. We next discuss how the analysis uses these additional components during the incremental analysis \nof a call site. We assume a current augmented points-to escape graph (01,I1,Sl,wl,t1,'rl,vl,t31,al), \na call site s E $1 with in- voked operation ops, and an augmented points-to escape graph (02, I2, $2, \nw2, t2, ~'2, ~'2, ~2, o~2) from the end of opt. 4.1 Matched Edges In the base algorithm, the analysis \nof a call site matches outside edges from the analyzed method against existing edges in the points-to \nescape graph from the program point before the site. By the time the algorithm has propagated the graph \nto the end of the method, it may contain addi- tional edges generated by the analysis of statements that \nexecute after the call site. When the incrementalized algo- rithm integrates the analysis result from \na skipped call site, it matches outside edges from the invoked method against only those edges that were \npresent in the points-to escape graph at the program point before the call site. w(s) and t(s) provide \njust those edges. The algorithm therefore computes (0, I, #) ----map((wl (s), tl (s)), (02, I2),/2~) \n where 0 and I are the new sets of edges that the analysis of the callee adds to the caller graph. 2Under \nthe interpretation f~-I __ {(sl' s~>.(s2,sl) E /~} and oc -1 = {(sl,s2).(s2,sl) a}, ~ ----a -1 and f~--1 \n= a. 4.2 Propagated Edges In the base algorithm, the transfer function for an ana-lyzed call site may \nadd new edges to the points-to graph from before the site. These new edges create effects that propagate \nthrough the analysis of subsequent statements. Specifically, the analysis of these subsequent statements \nmay read the new edges, then generate additional edges involv- ing the newly referenced nodes. In the \npoints-to graph from the incrementalized algorithm, the edges from the invoked method will not be present \nif the analysis skips the call site. But these missing edges must come (directly or indirectly) from \nnodes that escape into the skipped call site. In the points-to graphs from the caller, these missing \nedges are rep- resented by outside edges that are generated by the analysis of subsequent statements. \nThe analysis can therefore use T1 (S) and ul (s) to reconstruct the propagated effect of ana- lyzing \nthe skipped method. It computes (O',I',#') = map((O,I), (71(s),ul(s)), {(n,n).n e N}) where O' and I' \nare the new sets of edges that come from the interaction of the analysis of the skipped method and subsequent \nstatements, and f maps each outside node from the caller to the nodes from the callee that it represents \nduring the analysis from the program point after the skipped call site to the end of the method. Note \nthat this algorithm generates all of the new edges that a complete reanalysis would generate. But it \ngenerates the edges incrementally without reanalyzing the code. 4.3 Skipped Call Sites from the Caller \nIn the base algorithm, the analysis of one call site may affect the initial parameter map for subsequent \ncall sites. Specifically, the analysis of a site may cause the formal pa- rameter nodes at subsequent \nsites to be mapped to addi-tional nodes in the graph from the caller. For each skipped call site, the \nincrementalized algorithm records the parameter map that the base algorithm would have used at that site. \nWhen the incrementalized algorithm integrates an analysis result from a previously skipped site, it muss \nupdate the recorded parameter maps for subsequent skipped sites. At each of these sites, outside nodes \nrepre- sent the additional nodes that the analysis of the previously skipped site may add to the map. \nAnd the map #' records how each of these outside nodes should be mapped. For each subsequent site s' \nE al (s), the algorithm composes the site's current recorded parameter map f~,, with f to obtain its \nnew recorded parameter map #' o/i8,. 4.4 Skipped Call Sites from the Callee The new set of skipped call \nsites S' = ($1 t.J $2) contains the set of skipped call sites $2 from the callee. When it maps the callee \ngraph into the caller graph, the analysis updates the recorded parameter maps for the skipped call sites \nin $2. For each site s' E $2, the analysis simply composes the site's current map/28, with the map # \nco obtain the new recorded parameter map # o/28, for s'. 4.5 New Orders The analysis constructs the new \norders by integrating the orders from the caller and callee into the new analysis result and extending \nthe orders for s to the mapped edges and skipped call sites from the callee. So, for example, the new \norder between outside edges and subsequent call sites (w') consists of the order from the caller (oat), \nthe mapped order from the callee (w2[#]), the order from s extended to the skipped call sites from the \ncallee ($2 xwt (s)), and the outside edges from the callee ordered with respect to the call sites after \ns (oft(s) x O): ~'=~1 u ~2[#] u (s~ \u00d7 ~,(~)) v (~(s) \u00d7 o) ~'=~t u ~2[#] u (222 x ~(s)) u (at(s) x I) \nr'=r~ U ~-~[#] U (S2 X T~(S)) U (,&#38;(s) X O) -'=-~ U -2[#] U (S~ x .~(s)) U (~(s) x /) a'=~ U ~ U \n(s2 x ~(s))u (at(s) x S2) Here w[/z] is the order w under the map #, i.e., w[#] = {(s,n~ -~ n~).(s, \nnl -~ n2) E w, nl ~ n~, and n2 ~ n~}, and similarly for L, r, and u. 4.6 Cleanup At this point the algorithm \ncan compute a new graph (O1 U O U O', It U I U F, S t, w I, el, r', u',/3 t, a l) that reflects the integration \nof the analysis of s into the previous anal- ysis result (O1,11, $1, wl, el, ~'t, ~'1,/31, al). The final \nstep is to remove s from all components of the new graph and to remove all outside edges from captured \nnodes. 4.7 Updated Intraproeedural Analysis The transfer function for a skipped call site s performs \nthe following additional tasks: Record the initial parameter map ~ that the base al- gorithm would use \nwhen it analyzed the site.  Update ov to include {s} x O, update ~ to include {s} x I, update a to contain \nS \u00d7 {s}, and update f~ to contain  {s} x S. \u00ae Update S to include the skipped call site s. Whenever \na load statement generates a new outside edge nt ~ f n2, the transfer function updates T to include S \nx {nl ~ n2}. Whenever a store statement generates a new inside edge nt -~ n2, the transfer function updates \n~, to include S x {nl -~ n2}. Finally, the incrementalized algorithm extends the con-fluence operator \nto merge the additional components. For each additional component (including the recorded param- eter \nmaps #s), the confluence operator is set union. 4.8 Extensions So far, we have assumed that each skipped \ncal! site is ex- ecuted at most once and invokes a single method. We next discuss how our implemented \nalgorithm eliminates these re- strictions. To handle dynamic dispatch, we compute the graph for all of \nthe possible methods that the call site may invoke, then merge these graphs to obtain the new graph. \nWe also extend the abstraction to handle skipped call sites that are in loops or are invoked via multiple \npaths in the control flow graph. We maintain a multiplicity flag for each call site specifying whether \nthe call site may be executed multiple times: \u00ae The transfer function for a skipped call site s checks \nto see if the site is already in the set of skipped sites S. If so, it sets the multiplicity flag to \nindicate that s may be invoked multiple times. It also takes the union of the site's current recorded \nparameter map ,58 and the parameter map ~ from the transfer function to obtain the site's new recorded \nparameter map f~8 U f~. o The algorithm that integrates analysis results from previously skipped call \nsites performs a similar set of operations to maintain the recorded parameter maps and multiplicity flags \nfor call sites that may be present in the analysis results from both the callee and the caller. If the \nskipped call site may be executed mul- tiple times, the analysis uses a fixed-point algorithm when it \nintegrates the analysis result from the skipped call site. This algorithm models the effect of executing \nthe site multiple times. 4.9 Recursion The base analysis uses a fixed-point algorithm to ensure that \nit terminates in the presence of recursion. It is possible to use a similar approach in the incrementalized \nalgorithm. Our implemented algorithm, however, does not check for recursion as it explores the call graph. \nIf a node escapes into a recursive method, the analysis may, in principle, never terminate. In practice, \nthe algorithm relies on the analysis policy to react to the expansion of the analyzed region by directing \nanalysis resources to other allocation sites. 4.10 Incomplete Call Graphs Our algorithm deals with incomplete \ncall graphs as fol-lows. If it is unable to locate all of the potential callers of a given method, it \nsimply analyzes those it is able to locate. If it is unable to locate all potential callees at a given \ncall site, it simply considers all nodes that escape into the site as permanently escaped. 5. ANALYSIS \nPOLICY The goal of the analysis policy is to find and analyze al- location sites that can be captured \nquickly and have a large optimization payoff. Conceptually, the policy uses the fol- lowing basic approach. \nIt estimates the payoff for capturing an allocation site as the number of objects allocated at that site \nin a previous profiling run. It uses empirical data and the current analysis result for the site to estimate \nthe like- lihood that it will ever be able to capture the site, and, assuming that it is able to capture \nthe site, the amount of time required to do so. It then uses these estimates to calcu- late an estimated \nmarginal return for each unit of analysis time invested in each site. At each analysis step, the policy \nis faced with a set of par- tially analyzed sites that it can invest in. The policy simply chooses the \nsite with the best estimated marginal return, and invests a (configurable) unit of analysis time in that \nsite. During this time, the algorithm repeatedly selects one of the skipped call sites through which \nthe allocation site escapes, analyzes the methods potentially invoked at that site (reusing the cached \nresults if they are available), and integrates the results from these methods into the current result \nfor the allocation site. If these analyses capture the site, the policy moves on to the site with the \nnext best esti- mated marginal return. Otherwise, when the time expires, the policy recomputes the site,s \nestimated marginal return in light of the additional information it has gained during the analysis, and \nonce again invests in the (potentially dif- ferent) site with the current best estimated marginal return. \n 5.1 Stack Allocation The compiler applies two potential stack allocation opti- mizations depending \non where an allocation site is captured: o Stack Allocate: If the site is captured in the method that \ncontains it, the compiler generates code to allo- cate all objects created at that site in the activation \nrecord of the containing method. Inline and Stack Allocate: If the site is captured in a direct caller \nof the method containing the site, the compiler first inlines the method into the caller. After inlining, \nthe caller contains the site, and the generated code allocates all objects created at that site in the \nactivation record of the caller. The current analysis policy assumes that the compiler is 1) unable \nto inline a method if, because of dynamic dispatch, the corresponding call site may invoke multiple methods, \nand 2) unwilling to enable additional optimizations by fur- ther inlining the callers of the method containing \nthe alloca- tion site into their callers. It is, of course, possible to relax these assumptions to support \nmore sophisticated inlining and/or specialization strategies. Inlining complicates the conceptual analysis \npolicy de- scribed above. Because each call site provides a distinct analysis context, the same allocation \nsite may have differ- ent analysis characteristics and outcomes when its enclosing method is inlined \nat different call sites. The policy therefore treats each distinct combination of call site and allocation \nsite as its own separate analysis opportunity. 5.2 Analysis Opportunities The policy represents an opportunity \nto capture an alloca- tion site a in its enclosing method op as (a, op, G, p, c, d, m), where G is the \ncurrent augmented points-to escape graph for the site, p is the estimated payoff for capturing the site, \nc is the count of the number of skipped call sites in G through which a escapes, d is the method call \ndepth of the analyzed region represented by G, and m is the mean cost of the call site analyses performed \nso far on behalf of this analysis opportunity. Note that a, op, and G are used to perform the incremental \nanalysis, while p, c, d, and m are used to estimate the marginal return. Opportunities to capture an \nallocation site a in the caller op of its enclosing method have the form (a, op, s, G, p, c, d, m), where \ns is the call site in op that invokes the method containing a, and the remainder of the fields have the \nsame meaning as before. Figure 12 presents the state-transition diagram for anal- ysis opportunities. \nEach analysis opportunity can be in one of the states of the diagram; the transitions correspond to state \nchanges that take place during the analysis of the op- portunity. The states have the following meanings: \n Unanalyzed: No analysis done on the opportunity.  Escapes Below Enclosing Method: The opportu- nity's \nallocation site escapes into one or more skipped call sites, but does not (currently) escape to the caller \nof the enclosing method. The opportunity is of the form (a, op, G, p, c, d, m).  \u00ae Escapes Below Caller \nof Enclosing Method: The opportunity's site escapes to the caller of its enclos- ing method, but does \nnot (currently) escape from this Figure 12: State-Transition Diagram for Analysis Opportunities caller. \nThe site may also escape into one or more skipped call sites. The opportunity is of the form (a, op, \ns, G, p, c, d, m). \u00ae Captured: The opportunity's site is captured. Abandoned: The policy has permanently \nabandoned the analysis of the opportunity, either because its allo- cation site permanently escapes via \na static class vari- able or thread, because the site escapes to the caller of the caller of its enclosing \nmethod (and is therefore um optimizable), or because the site escapes to the caller of its enclosing \nmethod and (because of dynamic dis- patch) the compiler is unable to inline the enclosing method into \nthe caller. In Figure 12 there are multiple transitions from the Es-capes Below Enclosing Method state \nto the Escapes Below Caller of Enclosing Method state. These transitions indi-cate that one Escapes Below \nEnclosing Method opportunity may generate multiple new Escapes Below Caller of Enclos- ing Method opportunities \n--one new opportunity for each potential call site that invokes the enclosing method from the old opportunity. \nWhen an analysis opportunity enters the Escapes Below Caller of Enclosing Method state, the first analysis \naction is to integrate the augmented points-to escape graph from the enclosing method into the graph \nfrom the caller of the enclosing method. 5.3 Estimated Marginal Returns If the opportunity is Unanalyzed, \nthe estimated marginal return is (4 p)/a, where ~ is the probability of capturing an allocation site \ngiven no analysis information about the site, p is the payoff of capturing the site, and, assuming the \nanalysis eventually captures the site, a is the expected analysis time required to do so. If the opportunity \nis in the state Escapes Below Enclosing Method, the estimated marginal return is (El(d) .p)/(a. m). Here \n~1 (d) is the conditional probability of capturing an al- location site given that the algorithm has \nexplored a region of call depth d below the method containing the site, the al- gorithm has not (yet) \ncaptured the site, and the site has not escaped (so far) to the caller of its enclosing method: If the \nopportunity is in the state Escapes Below Caller of Enclosing Method, the estimated marginal return is \n(~2(d) .p)/(c. m). Here ~2(d) has the same meaning as ~l(d), except that the assumption is that the site \nhas escaped to the caller of its 43 enclosing method, but not (so far) to the caller of the caller of \nits enclosing method. We obtain the capture probability functions ~, ~1, and ~2 empirically by preanalyzing \nall of the executed allocation sites in some sample programs and collecting data that al- lows us to \ncompute these functions. For Escapes Below En- closing Method opportunities, the estimated payoff p is \nthe number of objects allocated at the opportunity's allocation site a during a profiling run. For Escapes \nBelow Caller of Enclosing Method opportunities, the estimated payoff is the number of objects allocated \nat the opportunity's allocation site a when the allocator is invoked from the opportunity's call site \ns. When an analysis opportunity changes state or increases its method call depth, its estimated marginal \nreturn may change significantly. The policy therefore recomputes the opportunity's return whenever one \nof these events happens. If the best opportunity changes because of this recomputa- tion, the policy \nredirects the analysis to work on the new best opportunity. 5.4 Termination In principle, the policy \ncan continue the analysis indefi- nitely as it invests in ever less profitable opportunities. In practice, \nit is important to terminate the analysis when the prospective returns become small compared to the analy- \nsis time required to realize them. We say that the analysis has decided an object if that object's opportunity \nis in the Captured or Abandoned state. The payoffs p in the analy- sis opportunities enable the policy \nto compute the current number of decided and undecided objects. Two factors contribute to our termination \npolicy: the percentage of undecided objects (this percentage indicates the maximum potential payoff from \ncontinuing the analy- sis), and the rate at which the analysis has recently been deciding objects. The \nresults in Section 6 are from analy- ses terminated when the percentage of decided objects rises above \n90% and the decision rate for the last quarter of the analysis drops below 1 percent per second, with \na cutoff of 75 seconds of total analysis time. We anticipate the development of a variety of termination \npolicies to fit the particular needs of different compilers. A dynamic compiler, for example, could accumulate \nan analy- sis budget as a percentage of the time spent executing the application --the longer the application \nran, the more time the policy would be authorized to invest analyzing it. The accumulation rate would \ndetermine the maximum amortized analysis overhead. 6. EXPERIMENTAL RESULTS We have implemented our analysis \nand the stack alloca- tion optimization in the MIT Flex compiler, an ahead-of-time compiler written in \nJava for Java. 3 We ran the exper- iments on an 800 MHz Pentium III PC with 768Mbytes of memory running \nLinux Version 2.2.18. We ran the compiler using the Java Hotspot Client VM version 1.3.0 for Linux. The \ncompiler generates portable C code, which we compile to an executable using gcc. The generated code manages \nthe heap using the Boehm-Demers-Weiser conservative garbage collector [4] and uses alloca for stack allocation. \naThe compiler is available at www.flexc.lcs.mit.edu. 6.1 Benchmark Programs Our benchmark programs include \ntwo multithreaded sci- entific computations (Barnes and Water), Jlex, and several Spec benchmarks (Db, \nCompress, and Raytrace). Figure 13 presents the compile and whole-program analysis times for the applications. \nCompile Time Whole-Program Application Without Analysis Analysis Time Barnes 89.7 34.3 Water 91.1 38.2 \nJlex 119.5 222.8 Db 93.6 126.6 Raytrace 118.4 102.2 Compress 219.6 645.1 Figure 13: Compile and Whole-Program \nAnalysis Times (seconds) Tile estimated optimization payoff for each allocation site is the number of \nobjects allocated at that site during a train- ing run on a small training input. The presented execution \nand analysis statistics are for executions on larger produc- tion inputs. 6.2 Analysis Payoffs and Statistics \nFigure 14 presents analysis statistics from the incremen- talized analysis. We present the number of \ncaptured al- location sites as the sum of two counts. The first count is the number of sites captured \nin the enclosing method; the second is the number captured in the caller of the en- closing method. Fractional \ncounts indicate allocation sites that were captured in some but not all callers of the en-closing method. \nIn Db, for example, one of the allocation sites is captured in two of the eight callers of its enclosing \nmethod. The Undecided Allocation Sites column counts the number of allocation sites in which the policy \ninvested some resources, but did not determine whether it could stack al- locate the corresponding objects \nor not. The Analyzed Call Sites, Total Call Sites, Analyzed Methods, and Total Meth- ods columns show \nthat the policy analyzes a small fraction of the total program. The graphs in Figure 15 present three \ncurves for each ap- plication. The horizontal dotted line indicates the percent- age of objects that \nthe whole-program analysis allocates on the stack. The dashed curve plots the percentage of decided objects \n(objects whose analysis opportunities are either Cap- tured or Abandoned) as a function of the analysis \ntime. The solid curve plots the Percentage of objects allocated on the stack. For Barnes, Jlex, and Db, \nthe incrementalized anal- ysis captures virtually the same number of objects as the whole-program analysis, \nbut spends a very small fraction of the whole-program analysis time to do so. Incrementaliza-tion provides \nless of a benefit for Water because two large methods account for a much of the analysis time of both \nanalyses. For Raytrace and Compress, a bug in the 1.3 JVM forced us to run the,incrementalized analysis, \nbut not the whole-program analysis, on the 1.2 JVM. Our experience with the other applications indicates \nthat both analyses run between five and six times faster on the 1.3 JVM than on the 1.2 JVM. 6.3 Application \nExecution Statistics Figure 16 presents the total amount of memory that the applications allocate in \nthe heap. Almost all of the allocated Analysis Captured Abandoned Undecided Total Analyzed Total Time \nAllocation Allocation Allocation Allocation Call Call Analyzed Total (seconds) Sites Sites Sites Sites \nSites Sites Methods Methods Barnes 0.8 3+0 0 2 736 18 1675 13 512 Water 21.7 33+0 4 33 748 94 1799 33 \n481 Jlex 0.9 0+2 1 2 1054 27 2879 12 569 Db 4.5 1+0.25 4 1.75 1118 54 2444 25 631 Raytrace 76.3 8+0.37 \n20.63 54 1067 271 3109 64 699 Compress 79.5 4+0.33 4 19.66 1354 111 4084 40 808 Figure 14: Analysis Statistics \n..... Stack Allocation Percentage, Whole-Program Analysis -- - Decided Percentage, Incrementalized Analysis \n--Stack Allocation Percentage, Incrementalized Analysis 001 o\u00b0 10075 U\", ..... :'~0 50 25 ~0 0.0 0.3 \n0.6 0.9 Analysis Time (seconds) Barnes 100 ..... 75 8\" 50 25 0 ~ I 0.0 0.3 0.6 0.9 Analysis Time (seconds) \nJlex 100 I 75 0 50 ..-,'-25 7== :\":\" .......... 0 25 50 75 Analysis Time (seconds) Compress ......y 0 \ng ~9 50 25 0 0 7 14 21 \u00a22 Analysis Time (seconds) Water 100 .............. , - - - 5o 25 0 ,r\" , , 0.0 \n1.5 3.0 4.5 Analysis Time (seconds) Db o 10o f 75 50 ..................... 0 -f \u00a2 25 ,: S-- 0 25 50 \n75 Analysis Time (seconds) Raytrace Figure 15: Analysis Time Payoffs memory in Barnes and Water is devoted \nto temporary arrays that hold the results of intermediate computations. The C++ version of these applications \nallocates these arrays on the stack; our analysis restores this allocation strategy in the Java version. \nMost of the memory in Jlex is devoted to temporary iterators, which are stack allocated after inlining. \nNote the anomaly in Db and Compress: many objects are allocated on the stack, but the heap allocated \nobjects are much bigger than the stack allocated objects. Figure 17 presents the execution times. The \noptimizations provide a significant performance benefit for Barnes and Wa- ter and some benefit for Jlex \nand Db. Without stack allo- cation, Barnes and Water interact poorly with the conser- vative garbage \ncollector. We expect that a precise garbage collector would reduce the performance difference between \nthe versions with and without stack allocation. 7. RELATED WORK We briefly discuss related work in escape \nanalysis, demand analysis, program fragment analysis, and incremental anal- ysis. See the full paper \nfor a more comprehensive discussion. from Incrementalized Analysis No Incrementalized Whole-Program Application \nAnalysis Analysis Analysis Barnes Water Jlex Db Raytrace Compress Figure 16: 36.0 3.2 2.0 190.2 2.2 \n0.6 40.8 3.1 2.5 77.6 31.2 31.2 13.4 9.0 6.7 110.1 110.1 110.1 Allocated Heap Memory (Mbytes) No Incrementalized \nWhole-Program Application Barnes Analysis 33.4 Analysis 22.7 Analysis 24.0 Water 18.8 11.2 10.7 Jlex \n5.5 5.0 4.7 Db 103.8 104.0 101.3 Raytrace Compress 3.0 44.9 2.9 44.8 2.9 45.1 Figure 17: Execution Times \n(seconds) 7.1 Escape Analysis Many other researchers have developed escape analyses for Java [16, 7, \n14, 3, 5]. These analyses have been presented as whole-program analyses, although many contain elements \nthat make them amenable to incrementalization. All of the analyses listed above except the last [5] analyze \nmethods independently of their callers, generating a summary that can be specialized for use at each \ncall site. Unlike our base analysis [16], these analyses are not designed to skip call sites. But we \nbelieve it would be relatively straightforward to augment them to do so. With this extension in place, \nthe remaining question is incrementalization. For flow-sensitive analyses [16, 7], the incrementalized \nalgorithm must record information about the ordering of skipped call sites relative to the rest of the \nanalysis information if it is to preserve the precision of the base whole-program analysis with re-spect \nto skipped call sites. Flow-insensitive analyses [14, 3], can ignore this ordering information and should \ntherefore be able to use an extended abstraction that records only the mapping information for skipped \ncall sites. In this sense flow-insensitive analyses should be, in general, simpler to incrementalize \nthan flow-sensitive analyses. Escape analyses have typically been used for stack allo- cation and synchronization \nelimination. Our results show that analyzing a local region around each allocation site works well for \nstack allocation, presumably because stack allocation ties object lifetimes to the lifetimes of the captur- \ning methods. But for synchronization elimination, a whole- program analysis may deliver significant additional \nopti-mization opportunities. For example, Ruf's synchronization elimination analysis determines which \nthreads may synchro- nize on which objects [14]. In many cases, the analysis is able to determine that \nonly one thread synchronizes on a given object, even though the object may be accessible to multi- ple \nthreads or even, via a static class variable, to all threads. Exploiting this global information significantly \nimproves the ability of the compiler to eliminate superfluous synchroniza- tion operations, especially \nfor single threaded programs. 7.2 Demand, Fragment, and Incremental Analysis Demand algorithms analyze \nonly those parts of the pro- gram required to compute an analysis fact at a subset of the program points \nor to answer a given query [2, 10, 8, 11]. Our approach differs in that it is designed to temporarily \nskip parts of the program even if the skipped parts poten- tially affect the analysis result. Fragment \nanalysis is designed to analyze a predetermined part of the program [12, 13]. A similax effect may be \nob- tained by explicitly specifying the analysis results for miss- ing parts of the program [9, 15]. \nOur approach differs in that it monitors the analysis results to dynamically deter- mine which parts \nof the program it should analyze to obtain the best optimization outcome. Incremental algorithms up- \ndate an existing analysis result to reflect the effect of pro- gram changes [17]. Our algorithm, in contrast, \nanalyzes part of the program assuming no previous analysis results. 8. CONCLUSION This paper presents \na new incrementalized pointer and es- cape analysis. Instead of analyzing the whole program, the analysis \nexecutes under the direction of an analysis policy. The policy continually monitors the analysis results \nto di- rect the incremental analysis of those parts of the program that offer the best marginal return \non the invested analysis resources. Our experimental results show that our analysis, when used for stack \nallocation, usually delivers almost all of the benefit of the whole-program analysis at a fraction of \nthe cost. And because it analyzes only a local region of the program surrounding each allocation site, \nit scales to handle programs of arbitrary size. 9. ACKNOWLEDGEMENTS The research presented in this paper \nwould have been im- possible without the assistance of C. Scott Ananian, Brian Demsky, and Alexandru \nSalcianu. Scott and Brian provided invaluable assistance with the Flex compiler infrastructure. In particular, \nBrian developed the profiling package on very short notice. Alex provided invaluable assistance with \nthe implementation of the base algorithm. We thank David Karger and Ron Rivest for interesting discussions \nregarding potential analysis policies, and the anonymous reviewers for their helpful and exceptionally \ncompetent feedback. 10. REFERENCES [1] O. Agesen. The cartesian product algorithm: Simple and precise \ntype inference of parametric polymorphism. In Proceedings of the 9th European Conference on Object-Oriented \nProgramming, Aarhus, Denmark, Aug. 1995. [2] G. Agrawal. Simultaneous demand-driven data-flow and call \ngraph analysis. In Proceedings of the 1999 International Conference on Software Maintenance, Oxford, \nUK, Aug. 1999. [3] B. Blanchet. Escape analysis for object oriented languages, application to Java. In \nProceedings of the l~th Annual Conference on Object-Oriented Programming Systems, Languages and Applications, \nDenver, CO, Nov. 1999. [4] H. Boehm and M. Weiser. Garbage collection in an uncooperative environment. \nSoftware--Practice and Experience, 18(9):807-820, Sept. 1988. [5] J. Bogda and U. Hoelzle. Removing unnecessary \nsynchronization in Java. In Proceedings of the l~th Annual Conference on Object- Oriented Programming \nSystems, Languages and Applications, Denver, CO, Nov. 1999. [6] G. Bollella et al. The Real-Time Specification \nfor Java. Addison-Wesley, Reading, Mass., 2000. [7] J. Choi, M. Gupta, M. Serrano, V. Sreedhar, and \nS. Midkiff. Escape analysis for Java. In Proceedings of the 14th Annual Conference on Object-Oriented \nProgramming Systems, Languages and Applications, Denver, CO, Nov. 1999. [8] E. Duesterwald, R. Gupta, \nand M. Sofia. A practical framework for demand-driven interprocedural data flow analysis. A CM Transactions \non Programming Languages and Systems, 19(6):992-1030, Nov. 1997. [9] S. Guyer and C. Lin. Optimizing \nthe use of high performance libraries. In Proceedings of the Thirteenth Workshop on Languages and Compilers \nfor Parallel Computing, Yorktown Heights, NY, Aug. 2000. [10] Y. Lin and D. Padua. Demand-driven interprocedural \narray property analysis. In Proceedings of the Twelfth Workshop on Languages and Compilers for Parallel \nComputing, La Jolla, CA, Aug. 1999. [11] T. Reps, S. Horowitz, and M. Sagiv. Demand interprocedural data.flow \nanalysis. In Proceedings of the ACM SIGSOFT 95 Symposium on the Foundations of Software Engineering, \nOct. 1995. [12] A. Rountev and B. Ryder. Points-to and side-effect analyses for programs built with precompiled \nlibraries. In Proceedings of CC 2001: International Conference on Compiler Construction, Genoa, Italy, \nApr. 2001. [13] A. Rountev, B. Ryder, and W. Landi. Data-flow analysis of program fragments. In Proceedings \nof the ACM SIGSOFT 99 Symposium on the Foundations of Software Engineering, Toulouse, France, Sept. 1999. \n[14] E. Ruff Effective synchronization removal for Java. In Proceedings of the SIGPLAN '00 Conference \non Program Language Design and Implementation, Vancouver, Canada, June 2000. [15] R. Rugina and M. Rinard. \nDesign-directed compilation. In Proceedings of CC 2001: International Conference on Compiler Construction, \nGenoa, Italy, Apr. 2001. [16] J. Whaley and M. Rinard. Compositional pointer and escape analysis for \nJava programs. In Proceedings of the l~th Annual Conference on Object-Oriented Programming Systems, Languages \nand Applications, Denver, CO, Nov. 1999. [17] J. Yur, B. Ryder, and W. Landi. Incremental algorithms \nand empirical comparison for flow- and context-sensitive pointer aliasing analysis. In Proceedings of \nthe 21st International conference on Software Engineering, Los Angeles, CA, May 1999. 46  \n\t\t\t", "proc_id": "378795", "abstract": "<p>We present a new pointer and escape analysis. Instead of analyzing the whole program, the algorithm incrementally analyzes only those parts of the program that may deliver useful results. An analysis policy monitors the analysis results to direct the incremental investment of analysis resources to those parts of the program that offer the highest expected optimization return.</p><p>Our experimental results show that almost all of the objects are allocated at a small number of allocation sites and that an incremental analysis of a small region of the program surrounding each site can deliver almost all of the benefit of a whole-program analysis. Our analysis policy is usually able to deliver this benefit at a fraction of the whole-program analysis cost.</p>", "authors": [{"name": "Fr&#233;d&#233;ric Vivien", "author_profile_id": "81408603016", "affiliation": "ICPS/LSIIT, Universit&#233; Louis Pasteur, Strasbourg, France", "person_id": "PP77031766", "email_address": "", "orcid_id": ""}, {"name": "Martin Rinard", "author_profile_id": "81100087275", "affiliation": "Laboratory for Computer Science, Massachusetts Institute of Technology, Cambridge, MA", "person_id": "PP79028476", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/378795.378804", "year": "2001", "article_id": "378804", "conference": "PLDI", "title": "Incrementalized pointer and escape analysis", "url": "http://dl.acm.org/citation.cfm?id=378804"}