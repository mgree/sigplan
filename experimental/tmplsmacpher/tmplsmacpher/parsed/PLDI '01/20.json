{"article_publication_date": "05-01-2001", "fulltext": "\n The Pointer Assertion Logic Engine Anders M\u00a2ller &#38; Michael I. Schwartzbach BRICS Department of \nComputer Science University of Aarhus, Denmark {amoeller, mis}@brics, dk Abstract We present a new \nframework for verifying partial specifica- tions of programs in order to catch type and memory errors \nand check data structure invariants. Our technique can ver- ify a large class of data structures, namely \nall those that can be expressed as graph types. Earlier versions were restricted to simple special cases \nsuch as lists or trees. Even so, our current implementation is as fast as the previous specialized tools. \nPrograms are annotated with partial specifications ex- pressed in Pointer Assertion Logic, a new notation \nfor ex- pressing properties of the program store. We work in the logical tradition by encoding the programs \nand partial speci- fications as formulas in monadic second-order logic. Validity of these formulas is \nchecked by the MONA tool, which also can provide explicit counterexamples to invalid formulas. To make \nverification decidable, the technique requires ex- plicit loop and function call invariants. In return, \nthe tech- nique is highly modular: every statement of a given program is analyzed only once. The main \ntarget applications are safety-critical data-type algorithms, where the cost of annotating a program \nwith in- variants is justified by the value of being able to automati- cally verify complex properties \nof the program. Introduction We present a new contribution to the area of pointer verifica- tion, which \nis concerned with verifying partial specifications of programs that make explicit use of pointers. In \npractice, there is an emphasis on catching type and memory errors and checking data structure invariants. \nFor data-type implementations, standard type-checking systems, as in C or Java, are not sufficiently \nexpressive. For example, the type of binary trees is identical to the one for doubly-linked lists. Both \nare just records with pairs of pointers, which makes the type checker fail to catch many common bugs. \nIn contrast, pointer verification can vali- date the underlying data structure invariants, for instance, \nto guarantee that doubly-linked lists maintain their shapes Permission to make digital or hard copies \nof all or part of this work for personal or classroom use is granted without fee provided that copies \nare not made or distributed for profit or commercial advan- tage and that copies bear this notice and \nthe full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute \nto lists, requires prior specific permission and/or a fee. PLDI 2001 6/01 Snowbird, Utah, USA &#38;#169; \n2001 ACM ISBN 1-58113-414-2/01106.-$5.00 after pointer manipulations. Memory errors, such as deref- \nerence of null pointers or dangling references, and creation of memory leaks are also beyond the scope \nof standard type checking. There have been several different approaches to pointer verification, but \nnot many that are as expressive as the one we propose in this paper. Clearly there is a trade-off between \nexpressiveness and complexity, since less detailed analyses will be able to handle larger programs. Our \napproach is designed to verify a single abstract data type at a time. Since such implementations often \ncontain intricate pointer manipulations and are trusted implicitly by programmers, they are a fair target \nfor detailed scrutiny. We work in the logical tradition by encoding pro-grams and partial specifications \nas formulas in monadic second-order logic. Formulas are processed by the MONA tool [26, 34] which reduces \nthem to equivalent tree automata from which it is simple to conclude validity or to extract concrete \ncounterexamples. Translated back into the under- lying programming language, a counterexample is an initial \nstore that causes the given program to fail. Program anno- tations, in the form of assertions and invariants, \nare allowed and may prove necessary to obtain the desired degree of precision. This approach can be viewed \nboth as lightweight program verification, since the full behavior of the program is not considered, and \nas heavyweight type checking, since properties well beyond the expressiveness of standard type systems \ncan be checked. We have reported on our approach in two earlier works. In the first we introduce the \nbasic technique applied to lin- ear lists [24]. In the second we provide a generalization to tree-shaped \ndata structures and introduce a new encoding to make the analysis feasible [14]. The current paper takes \na leap forward in generalizing the class of data structures that can be considered, without sacrificing \nprecision or efficiency. Our new framework can handle all data structures that can be described as graph \ntypes [28]. These include data struc- tures that are well-known from folklore or literature, such as \ndoubly-linked lists, trees with parent pointers, threaded trees, two-dimensional range trees, and endless \ncustomized versions such as trees in which all leaves are linked in a cyclic list. Our framework is also \ndesigned to handle the common situation where a data structure invariant must be temporarily violated \nat some program points. Our contributions are: An extension of the results in [24, 14] to the whole class \nof graph types; a language Ibr expressing data structures and opera- tions along with correctness specifications; \n \u00ae a full implementation exploiting intricate parts of the MONA tool to obtain an efficient decision \nprocedure, together with a range of non-trivial examples.  To verify a data type implementation, the \ndesired data structure is specified in an abstract notation, and the pro- gram is annotated with assumptions \nand assertions. It is not necessary to customize or optimize the implementation, and no proof obligations \nare left to be dealt with manually. We rely on a new formal notation, Pointer Assertion Logic (PAL) to \nspecify the structural invariants of graph types, to state pre- and post-conditions for procedures, and \nto fbrmulate invariants and assertions that are given as hints to the system. The PAL notation is essentially \na monadic second-order logic in which the universe of discourse con- tains records, pointers, and booleans. \nPrograms with PAL annotations are verified with the tool PALE, the Pointer Assertion Logic Engine. The \n\"secret\" behind the PALE im- plementation is using the MONA tool to decide validity of Hoare triples \nbased on PAL over loop-free code. Code with loops or recursion is handled by splitting it into loop-free \nfragments using invariants, as in classical Hoare logic. While the MONA logic has an inherent non-elementary \ncomplex- ity [33], we demonstrate that it can efficiently handle many real programs. Furthermore, the \nability to insert assertions to break larger triples into smaller ones suggests that the overall approach \nis modular and thus can scale reasonably. A framework for pointer verification, such as ours, should \nbe evaluated on four different criteria. First, how precise is the analysis? Second, it is fast and scalable? \nThird, does it allow or require programs to be annotated? Fourth, which data structures can be considered \nand how are they described? In the following sections, we will describe a pro- gramming language that \nuses Pointer Assertion Logic for ex- pression of store properties, describe the decision procedure based \non Hoare logic and MONA, and through a number of experiments argue that the Pointer Assertion Logic ap- \nproach provides a productive compromise between express- ibility and efficiency. A Tiny Example Consider \nthe type of linked lists with tail pointers, which as a graph type is expressed as: type Head = { data \nfirst: Node; pointer last: Node[this.first<next*.[pos.next=null]>last];  } type Node = { data next: \nNode;  } The notation is explained in the following section, but in- tuitively the last pointer is \nannotated with a formula that constrains its destination to be the last Node in the list. A candidate \nfor verification is the following procedure which concatenates two such structures: proc concat(data \n11,12: Head): Head { if (ll.last!=null) { ll.last.next = 12.first; } else { ll.first = 12.first; } \nif (12.first!=null) { li.last = 12,1ast; } 12.first = null; 12.1ast = null; return li;  These are tedious \npointer manipulations that are easy to get wrong. However, if we annotate the procedure with the pre-condition \nthat 11 and 12 are not null and run PALE, it will in half a second report that no memory errors occur \nand, importantly, that the data structure invariant is maintained. Related Work General theorem provers, \nsuch as HOL [5], may consider the full behavior of programs but are often slow and not fully automated. \nTools such as ESC [12] and LCLint [15} consider memory errors among other undesirable behaviors but usually \nignore data structure invariants or only support a few predefined properties. Also, they trade soundness \nor completeness for efficiency and hence may flag false errors or miss actual errors. Model checkers \nsuch as Bebop [2] and Bandera [9] ab- stract away the heap and only verify properties of control flow. \nThe JPF [20] model checker verifies simple assertions for a subset of Java, but does not consider structural \ninvari- ants. The constraint solver Alloy has been used to verify prop- erties about bounded initial \nsegments of computation se- quences [23]. While this is not a complete decision proce- dure even for \nstraight-line code, it finds many errors and can produce counterexamples. With this technique, data structure \ninvariants can be expressed in first-order logic with transitive closure. However, since it assumes computation \nbounds, absence of error reports does not imply a guarantee of correctness, and the technique does not \nappear to scale. The symbolic executor PREfix [7] simulates unannotated code through possible executions \npaths and detects a large class of errors. Again, this is not a complete or sound deci- sion procedure, \nand data structure invariants are not consid- ered. However, PREfix gives useful results on huge source \nprograms. Verification based on static analysis has culminated with shape analysis. The goals of the \nshape analyzer TVLA [32, 38, 31] are closest to ours but its approach is radically different. Rather \nthan encoding programs in logic, TVLA performs fixpoint iterations on abstract descriptions of the store. \nRegarding precision and speed, PALE and TVLA seem to be at the same level. TVLA can han-dle some data \nabstractions and hence reason about sort-ing algorithms; we show in Section 6 that we can do the same. \nTVLA analyzes programs with only pre- and post- conditions, where PALE often uses loop invariants and \nasser- tions. This seems like an undisputed advantage for TVLA; however, not having invariants can cause \na loss in precision making TVLA falsely reject a program. Regarding the spec- ification of new data structures \nwe claim an advantage. Once a graph type has been abstractly described with PAL, the PALE tool is ready \nto analyze programs. In TVLA it is nec- essary to specify in three-valued logic an operational seman- \ntics for a collection of primitive actions specific to the data structure in question. Furthermore, to \nguarantee soundness of the analysis, this semantics should be proven correct by hand. TVLA is applicable \nalso to data structures that are not graph types, but so far all their examples have been in that class. \nUnlike PALE, TVLA cannot produce explicit counterexamples when programs fail to verify. There exists \na variety of assertion languages designed to express properties of data structures, such as ADDS [21], \nLr [3], and Shape Types [18]. We rely on PAL since it provides a high degree of expressiveness while \nstill having a decision procedure that works in practice. A drawback of our approach is that detailed, \nexplicitly stated loop invariants often are required. The overhead of adding such annotations can be \nsignificant, so the ap- proach is not applicable for verifying large programs. How- ever, the most complex \npointer operations often occur in data-type implementations, which usually have a manage-able size and \nappear in central libraries. Thus, we primarily aim for the niche of safety-critical data-type implementa- \ntions. For such programs, it is well known that the effort of constructing loop invariants is comparable \nto the effort of designing the data-type [19]. Once the program anno-tations have been added, the PALE \ntool can automatically decide validity. PALE works by splitting the program into disjoint fragments that \nare verified separately by analyzing every statement exactly once. That is, verification depends only \non locally specified properties mad there is no fixpoint iteration involved. In this sense, the approach \nis highly scal- able. On the other hand, the approach relies on a decision procedure with a non-elementary \ncomplexity, so there are programs that cannot be verified in practice. The experi- ments described in \nSection 7 indicate that the annotation overhead is manageable, that the theoretical complexity is not \nnecessarily a problem in practice, and that quite intri- cate properties can be expressed and verified. \nPointer Assertion Logic In this section, we informally present the components of our framework. First, \nwe describe the underlying store model. Second, we use the notion of graph types to describe data structures. \nThird, we employ a simple programming lan- guage to express data structure operations. And, finally, \nwe use program annotations in the form of Pointer Asser- tion Logic formulas, for expressing properties \nof the program store. The programming language and the aanotations have been designed to be simple but \nat the same time as expres- sive as the verification technique allows. In the following, we present the \nframework informally and refer the reader to [35] for formal definitions. To make the expressive power \nof the framework lucid, we show the complete syntax instead of only describing the main ideas. Store \nModel In our model, the store consists of a heap and some program variables. The heap contains records \nwhose fields are either pointers or boolean values. A pointer either has the value null or points to \na record. Program variables are either data variables or pointer variables. A data variable is the root \nof a data structure, whereas a pointer variable may point to any record in the heap. This is a very concrete \nrepresentation. We only abstract away arithmetic values and the actual addresses of records. Memory management \nis not automatically represented, but as in [24, 14], allocation and deallocation primitives could easily \nbe added along with automatic checks for memory leaks and dangling references. Graph Types Collections \nof records and pointers can form any number of interesting data structures, which are generMly expressed \nthrough an invariant on the allowed shapes. We wish to explicitly declare such data structures so that \ntheir invari- ants can be verified by our system. For this purpose, we use graph types [28] which is \nan intuitive notation that makes it feasible to describe complex structures. Invariants of graph type \nstructures can be expressed in monadic second-order logic on finite trees, which allows us to use the \nMONA tool to verify correctness. A graph type is a tree-shaped data structure with ex-tra pointers. The \nunderlying tree is called the backbone. The constituent records have two kinds of fields: data fields \nwhich define the backbone, and pointer fields which may point anywhere in the backbone. To describe a \nstructural invariant, a pointer field is annotated with a routing expres- sion which restricts its destination. \nIn the current work, we have generalized the annotations to be arbitrary formulas that may contain routing \nexpressions as basic predicates. Another difference to [28] is that instead of building types from unions \nand records, we only use records and nullable pointers. Clearly, the two variations can encode each other; \nwe choose the more primitive version, since it turns out to lead to a more efficient decision procedure. \nOur syntax and semantics of graph type declarations is described in the next section. Surprisingly many \ndata structures can be described as graph types. As a simple example, consider the type of binary trees \nwhere all nodes contain pointers to the root. In our notation, it looks like: type Tree = { data left,right \n:Tree; pointer root :Tree [root<(left+right)*>this empty(root^Tree.left union root \"Tree. right) ] ; \n } The syntax for formulas is presented below, but the re-striction on the source, this, and destination, \nroot, of the pointer is read as follows: this must be reachable from root by following a sequence of \nleft or right pointers, and the set of Tree records having left or right pointers to the root must be \nempty. Another example is doubly-linked lists with boolean values: type Node = { bool value; data next \n: Node ; pointer prev:Node[this'Node.next={prev}] ; } Here, the set of of nodes that can reach the \nthis node through a next pointer must only contain the prey node. The convention that {null} is interpreted \nas the empty set handles the first node in the list. Our benchmark programs cover a variety of data struc- \ntures expressed as graph types, including singly-linked lists, doubly-linked lists with tail pointers, \nred-black search trees, and post-order threaded trees with parent pointers. Addi-tional examples are \npresented in [28]. The Programming Language A program consists of a set of declarations of types, vari- \nables, and procedures, specified by the following grammar: typedect ~ type T = { ( field ; )* } field \n~ data p@ : T 1 pointer pO : T [ form ] I bool b 0 progw~r --+ data p@ : T I pointer p0 : T [ bool b \n@ procedure --+ proc n ( progvar \u00ae ) : ( T I void ) ( logicvar ; )* property ( { ( progvar ; )* stm \n} )? property We use the notation @ and \u00ae Ibr comma-separated lists with one-or-more elements and zero-or-more \nelements, re-spectively. T, p, b, and n range over names of types, pointer variables or fields, boolean \nvariables or fields, and proce-dures, respectively. Ignore for now all oecurences of logicvar and property; \nthey are introduced later. A type consists of a number of fields of kind data, pointer, or bool. The \ndata fields span the tree value and the pointer fields define extra pointers whose destinations are constrained \nby a formula. The bool fields are used to model finite values. A procedure has a name, formal parameters, \na return type, and a body consisting of local variable declarations and statements. If the body is omitted, \nthe declaration is considered a proto-type. A statement is one of the following constructs; the assert \nand split statements are described later: stm ~ stm stm asn 0 ; procca11 ; if ( condexp ) { stm } \n( else { stm } )? while property ( condexp ) { stm } return progexp ; assert property ; split property \nproperty ; ash -~ Ibexp = ( condexp [ proccall ) I Iptrexp = ( ptrexp [ procca11 )  The language permits \nmultiple-assignment statements where all right-hand sides are evaluated before assigning-- these are \nuseful for certain program transformations. Expressions have the following form: condexp bexp [ ? [ form \n] bexp \"-~ ( bexp ) ! bexp ] bexp ~ bexp bexp I bexp [ bexp => bexp bexp <=> bexp [ bexp = bexp ptrexp \n=ptrexp [ bexp != bexp ptrexp ! = ptrexp t true [ false Ibexp lbexp b ] ptrexp, b ptrexp --~ null [ Iptrexp \nlptrexp --~ p ptrexp, p proccM1 --~ n ( ( eondexp I ptrexp )\u00ae ) [ formula ] The \"?\" operator stands \nfor nondeterministic boolean choice, which is used to model arithmetic conditions that we cannot capture \nprecisely. The operator \".\" dereferences a pointer, and the other constructs have the expected mean- \nings. The language does not contain arithmetic, since our ap- proach focuses on the structural aspects \nof data types. How- ever, as described in a later section, the technique does per- mit abstractions of \narithmetic properties, for instance for specifying certain ordered data structures, Program Annotations \nPointer Assertion Logic is a monadic second-order logic on graph types. It allows quantification over \nheap records, both of individual elements and of sets of elements, and uses generalized routing expressions \n[28] for convenient navigation in the heap. Formulas are used in pointer fields to constrain their destinations, \nin while loops and procedure calls as invariants, in procedure declarations as pre-and post-conditions, \nand in assert and split statements. The syntax of formulas is as follows: form -~ ( existpos [ allpos \n) p0 of T : form ( existset [ allset ) s e of T : form ( existptr [ allptr ) p0 of T :form ( existbool \n[ allbool ) s 0 : form ( form ) ] ! form form ~ form I form I form form => form [ form <=> form ptrexp \nin setexp [ setexp sub setexp setexp = setexp I setexp != setexp empty ( setexp ) I bexp return [ n. \nb m ((form [ ptrexp I setexp)\u00ae ) ptrexp < routingexp > ptrexp predicate ~ pred m ( logicvar \u00ae ) = form \n The identifiers m and s denote predicates and set variables, respectively. The poe and ptr quantifiers \ndiffer in that the former range over heap records while the latter also includes the null value. A routing \nexpression formula pl <r>p2 is sat- isfied by a given model if there is a path from pl to p2 satis- fying \nr, as defined below. For reuse of formulas, predicates can be defined as top-level declarations. Logical \nvariables can be associated to procedures to allow the pre-and post-conditions to be related, as commonly \nseen in the literature [19, 11]. A logical variable is a universally quantified variable that may occur \nin the pre-and post-conditions of a procedure but not in the procedure body: logicvar --~ pointer p~ \n: T [ bool b 0 [ set s 0 : T  In formulas, ptrexp has two additional forms allowing access in procedure \npost-condition to the returned value and in procedure call formulas to the logical variables of the called \nprocedure: ptrexp --~ ... I return [ n. p Set expressions can contain the usual set operators, along \nwith the up operation x^T.p which denotes the set of records of type T having a p successor to x: setexp \nS [ ptrexp ~ T . p I { ptrexp 0 } 1 setexp union setexp [ setexp inter setexp [ setexp minus setexp \n The syntax of routing expressions is a slightly generalized version of that in [28]. A routing expression \nis a regular expression over routing directives, each being a step down or up a pointer or data field, \nor a formula with the extra free variable pos filtering away those records that cause the formula to \nevaluate to false when poe denotes one of them: routingexp ~ p [ \" T . p I [ form ] [ routingexp, routingexp \nI routingexp + routingexp I ( routingexp ) ] routingexp * By default, a pointer field must satisfy the \nformula given in its type declaration. This can be overridden with pointer directives of the form: ptrdirs \n~ { ( T . p [ form ] )\u00ae } They allows pointer fields to be constrained differently at different program \npoints. This is important because tem- porary but intentional invalidation of data structure invari- \nants often occurs in imperative programs, as noted for in- stance in [21]. Pointer directives, both default \nand overrid- ing, are required to be well-formed. This means that in any store and for any record, the \ndirectives associated to the pointer fields must denote exactly one record. Fortunately, as proved in \n[28] this is decidable. A pair consisting of a formula and a set of pointer directives: property ~ [ \nform ptrdirs ] is called a property and denotes the set of stores where the formula form is satisfied; \n the data variables denote disjoint acyclic backbones spanning the heap; and  each pointer field satisfies \nits pointer directive (which is either the default from the type declaration or the overriding from the \nptrdirs).  Properties occur as procedure pre- aad post-conditions, as while loop invariants, as split \nassertions and assumptions (split contains two properties), and as assert assertions. Semantics of Annotations \nThe program annotations are invariants of the program that must be interpreted as follows: The pre-condition \nof a procedure may be assumed to hold when evaluating the procedure body;  the post-condition must hold \nupon termination of the procedure body;  every while loop invariaat must hold upon entry and after each \niteration, and may be assumed to hold when the loop terminates;  assertions specified with assert must \nhold at those pro- gram points;  for split statements, the assertion properties must hold, and the assumption \nproperties may be assumed to hold (the reason for introducing these statements is explained in Section \n4); and  \u00ae at every procedure call, the invariant conjoined with the pre-condition of the called procedure \nmust hold for some valuation of its logical variables, and the invari- ant conjoined with the post-condition \nmay be assumed upon return, also for some valuation of the logical vari- ables. In later sections, we \nshow that the requirements imposed by the annotations can be verified automatically, provided that valid \nand sufficiently detailed invariants are given. 3 Example: Threaded Trees Before describing our decision \nprocedure, we show a larger example of using PAL. A threaded tree is a binary tree in which all nodes \ncontain a pointer to its cyclic successor in a post-order traversal. As a further complication, we equip \nall nodes with a parent pointer as well. This corresponds to the following graph type: type Node = { \ndata left,right:Node; pointer post:Node[POST(this,post)]; pointer parent:Node[PARENT(this,parent)]; \n} where POST and PARENT are predicates that spell out these relationships. For example, PARENT(a,b) \nabbreviates the formula: a'Node.left union a'Node.right={b} The POST predicate is more involved and \nmakes use of aux- iliary predicates LEAF, ROOT, and LESSEQ. We consider a procedure :fix(x) that assigns \nthe correct value to x.post assuming that this field initially contains the value null and that x is \nnon-null. This is a non-trivial operation that looks like: proc fix(pointer x: Node): void { if (x.left=null \n~ x.right=null) { if (x.parent=null) { x.post = x; } else { if (x.parent.right=null I x.parent.right=x) \n{ x.post = x.parent; } else {  x.post = findsmallest(x.parent.right); } } } else { x.post = findsmallest(x); \n} where the auxiliary procedure findsmallest is: proc findsmallest(pointer t: Node): Node pointer T: \nNode; { while (t.left!=null I t.right!=null) { if (t.left!=null) { t = t.left; } else { t = t.right; \n} } return t; The question is: Does this code verify? Does the resulting tree always satisfy the data \nstructure invariant? Can type or memory errors ever occur? PALE can provide the answers with some help \nfrom us. First, since the argument to fix is not a proper threaded tree, we must state a suitable pre- \ncondition as the property: [x!=null {Node.post[ALMOSTPOST(this,post,x)]}]  Here we require that the \nargument is not null and that the data structure invariant can be temporarily violated. The ALMOSTPOST \npredicate is: (this!=x => POST(this,post)) ~ (this=x => post=null) which simply states the exception \nthat we allow. Second, the while loop in findsmallest needs an invariant, which is the property: [INV \n{Node.post[ALMOSTPOST(this,post,x)]}]  where the pointer directive states that the threaded tree is \nstill messed up, and the proper invariant INV equals: T<(left+right)*>t allpos c of Node: LESSEQ(c,t,T) \n=> t<(left+right)*>c which states that t is a descendant of T and all its post- order successors are \nfurther descendants. See [35] for the full code with all post-conditions. In total, six annotations are \nrequired. In less that 4 seconds PALE verifies that the code contains no errors. Hoare Logic Revisited \n Given an annotated program, we wish to decide whether the program is correct with respect to the annotations. \nThe first step in our decision procedure is to split the given program into Hoare triples [22, 1, 11]. \nThe idea of modeling transformations of the heap with Hoare logic has been studied before [37, 17]. The \nmain novelty of our approach is the choice of PAL as assertion language. Our Hoare \"triples\" have a nonstandard \nform: triple -~ property s~m The statement stm is not allowed to contain while loops, split statements, \nor procedure calls. A triple is valid if executing stm in a store where property is satisfied cannot \nviolate any assertions specified by assert state- ments occurring in stm; and We define the cut-points \nof a program (according to [16]) as the following set of program points: the beginning and end of procedure \nbodies and while bodies, the split state-ments (these do not affect the computation and are con-sidered \nsingle program points), and before each procedure call. For each cut-point in the given program, we generate \na Hoare triple from the property associated with that point and the code that follows until reaching \nother cut-points. Extra assert statements are automatically inserted for these other cut-points, reflecting \nthe assertions they define. In case of split statements, we here use the assertion prop- erty. For procedure \ncalls, we use the pre-condition property of the called procedure conjoined with the call invariant for- \nmula. Recall that we do allow if statements in the Hoare triples. However, if one branch contains a cut-point, \nwe require syntactically that the other branch also contains a cut-point or that the if statement is \nimmediately followed by one. Typically, split statements are used to fulfill this requirement. As a result, \nthe statement part of a Hoare triple in general has a tree shape with one cut-point in the root and one \nin each leaf'. See [35] for more details. We claim without proof that this reduction is semanti- cally \nsound, with two exceptions: For split statements, the assertion property may not be implied by the assumption \nproperty, thereby causing a \"gap\" between the Hoare triples. This is intentional, because it allows to \nrecover from situations where the required properties are beyond what is expressible in Pointer Assertion \nLogic, such as arithmetical proper- ties. Using split statements at a few selected places, one can then \nstill verify properties of the remaining parts of the code. However, none of the examples shown in Section \n7 require this feature. Procedure calls are known to cause complications for Hoare logic [II]. In our \nease, there is in general no guarantee that the call invariant is actually a valid in- variant. However, \nin most situations, simple syntactic requirements suffice, since recursive calls in data type operations \ntypically follow the recursive structure of the graph type backbones. A sufficient condition is that \nthe call invariant only accesses variables and record fields that are not assigned to in the procedure. \nSuch require- ments ensure that the invariant and the procedure's pre- and post-conditions express properties \nof disjoint parts of the store, reminiscent of the \"independent con- junctions\" in [37]. All the examples \nshown in Section 7 can be handled by simple rules, which we plan to build into PALE. the execution always \nterminates in a store consisting of disjoint, acyclic backbones spanning the heap in which all pointer \ndirectives hold. As opposed to normal Hoare triples, these have no explicit post-condition, but the \nstm part may contain assert sub-statements. This simple generalization allows many asser- tions to be \nmade without always breaking triples into smaller parts, as was often the case in [24] and [14]. For \ninstance, an if statement where both branches end in assert statements does not necessarily need to be \nbroken into two parts. Also, using this form of Hoare triples simplifies the encoding in monadie second-order \nlogic described in Section 5. In PALE, this phase is implemented as a desugaring process reducing all \nprocedures, while loops, split statements, and procedure calls to transduction declarations having the \nform \"transduce triple\". In the following section we describe how validity of these simpler transduce \nconstructs can be decided. In contrast to techniques based on generating the weak- est preconditions \nfor all procedures, each program or pro-cedure is not turned into one single verification condition; \ninstead we use the annotations to split the program into Hoare triples that are verified independently. \nAlso, as op- posed to [17], we will not rely on fixpoint iterations. This means that detailed invariants \nmay be required; however, it has the advantage that the technique becomes highly mod- ular and hence \nscalable. Deciding Hoare Triples in MONA We need to decide validity of a Hoare triple of the form property \nstm where the statement stm is without loops and procedure calls. The question is whether every execution \nof stm start- ing from a store satisfying property is guaranteed to satisfy the assertions given by assert \nstatements and to result in stores with disjoint, acyclic backbones spanning the heap in which all relevant \npointer directives hold. A result in [29] shows in a very general setting that this is a decidable ques- \ntion. In essence, we encode each Hoare triple in the logic weah monadic second-order theory of 2 successors, \nwhich is decidable using the MONA tool [26, 25, 34]. Similarly to the previous implementations [24, 14] \nwe use a particular transduction technique. This idea allows us to avoid an explicit construction of \nweakest pre-conditions working backwards through the statement sequence. In-stead, we directly simulate \n(transduce) the statements and mirror their effect by updating a fixed collection of store predicates \nwhich abstractly describes a set of stores. It is shown in [29] that any question about the resulting \nset of stores can be answered by phrasing it in terms of the trans- duced store predicates and checking \nfor validity of the re- sulting formula. The store predicates describe a set of stores in MONA logic. \nThey can be thought of as an interface for asking questions about a store. There are 11 kinds of predicates: \n\u00ae bool_T_b(v) gives the value of the bool field b in a record v of type T; succ_T_d(v,w) holds if the \nrecord w is reachable from the record v of type T along a data field named d; Q null_T_d (v) holds if \nthe data field d in the record v of type T is null; succ_T_p(v,w) holds if the record w is reachable \nfrom the record v of type T along a pointer field named p;  null_T_p(v) holds if the pointer field p \nin the record v of type T is null;  ptr_d (v) holds if the record v is the value of the data variable \nd;  null_d() holds if the data variable d is null;  ptr_p(v) holds if the record v is the destination \nof the pointer variable p;  null_p() holds if the pointer variable p is null;  bool_bO gives the value \nof the boolean variable b;  memfailed() holds if a null-pointer dereference has oc- cured.  All properties \nof a store can be expressed using these pred- icates in MONA logic. The transduction process generates \na collection of such store predicates for each program point. For convenience, we describe this by indexing \nthe predi- cates with program points; for example, for each program point i there is a version of the \nbool_T_b(v) predicate called bool_T_b_i (v). An initial collection of store predicates is defined to \nre- flectthe formula and pointer directives that constitute the pre-condition of the Hoare triple. In \nthe encoding into MONA code, the program variables are modeled as free vari- ables, which are universally \nquantified in the final validity formula that is given to MONA. For example, a bool vari- able is modeled \nas a boolean variable _bool_b in MONA and the corresponding initial store predicate is: bool_b_00 = _bool_b \nSimilarly, a pointer variable p is modeled as a first-order MONA variable _ptr_p and the corresponding \ninitial store predicate is: ptr_p_0(v) = v = _ptr_p A bool field b in a record of type T is modeled \nas a second- order variable _bool_T_b containing the set of records in which b is true. Consequently, \nthe corresponding initiM store predicate is: bool_T_b_0(v) --v in _bool_T_b As a final example, we consider \npointer fields whose initiM store predicate is: succ_T_p_0(this,p) = f where f is the encoding of the \nformula associated with the p field of T. If the pre-condition of the Hoare triple contains the pointer \ndirective T. p [form], then that formula is form, otherwise the default formula from the type definition \nis used. Across a simple statement, two collections of store pred- icates are related in a manner that \nreflects the semantics of that statement. Consider for example a type of linked lists: type Node = { \ndata next: Node; } and a simple statement involving two pointer variables of type Node: p = q.next; \n If this statement is enclosed by program points i and j, then the store predicates are updated as follows \nin MONA code: memfailed_jO = memfailed_i() ~null-q_i 0 ptr_p_j(v) = ex2 ptr_q_i(w) ~ w: succ.Node_next_i(w,v) \n null.p-jO = ex2 w: p1~r_q_i(w)~ null_Node_next(w) while the other store predicates remain unchanged. \nThe PALE tool generates such store predicate updates for all Hoare triples and subsequently generates \nformulas to check the required properties. Between conditionals, routing ex- pressions, and various \nprimitive statements this is a com- plex translation reminiscent of generating machine code in a compiler. \nThe details can be studied in [35]. The way as- signments are handled without losing aliasing information, \nas in the example above, is essentially the same as in [36]. Checking that an assertion property at a \ngiven progrean point cannot be violated can be expressed by encoding the property using the store predicates \nassociated with the pro- gram point together with the pre-condition property en-coded with the initial \nstore predicates. There is a strong con- nection between this transduction technique and the more traditional \nweakest-precondition technique: if the predicate invocations in the MONA formulas are \"unfolded\", one \nes- sentially gets the weakest pre-condition. The main advan- tage of using the \"forward\" transduction \ntechnique instead of a \"backward\" weakest-precondition technique is an im- plicit reuse of intermediate \nresults. Checking that the resulting backbones are disjoint, acyclic, and span the heap is based on formulas \nfor express- ing transitive closure. Checking that a pointer directive holds is in [281 shown to be decidable \nin monadic second- order logic. This result generalizes easily to our extension of graph types, where \narbitrary formulas rather than only routing expressions can be used as pointer directives. ' The MONA \ntool transforms the resulting formulas, which can be quite large, into equivalent minimal Guided Tree \nAutomata [4] represented as BDD structures [6], and from that either deduces validity or generates counterex- \nample models. In the latter case, the PALE tool decompiles that model into a program store which causes \nthe program to fail. The use of Guided Tree Automata rather than ordinary tree automata yields an exponential \nsaving by, factorizing the state space according to the recursive structure of the graph type backbones. \nCompared to the WSRT technique used in [14], our choice of describing the backbones as records with pointers \nrather than as recursive types allow a simpler and more efficient automaton guide to be constructed. \nAlso for efficiency reasons, we compile directly into MONA logic rather than use a more high-level logic, \nsuch as FIDO [30]. Note that a collection of store predicates is vaguely sim- ilar to the abstract store \ndescriptions employed by TVLA. Consequently, it might seem that we could follow their ap- proach and \nuse a fixpoint process to transduce a while loop. However, this is in general not possible, since such \nfixpoints may require transfinite induction. Hence, we resort to using invariants to break up loops. \nThis transduction approach introduces no imprecision; it is both sound and complete for individual Hoare \ntriples. Data Abstractions In [38, 31], abstractions of the data contained in the heap records can \nbe tracked by specifying suitable instrumenta-tion predicates. As an example, a predicate dle(x, y) is \nused to represent \"the data in x is less than or equal to the data in y'. To illustrate the power of \nPAL, we show that a similar approach works for our technique. As an example, we instrument the ubiquitous \nlinked-list reverse example to verify that reversal of a list ordered in increasing order results in \na list ordered in decreasing order: We associate two boolean fields, next_die and next_dge, to the next \nfield in the linked-list type, with the intended meaning: next_dle is true in a given record if the data \nin the record denoted by the next pointer is certain to be less than or equal to the data in the given \nrecord -and likewise for next_dge with greater than or equal. Similarly, for each pair of program pointer \nvariables, two boolean variables are added to keep track of the relative order of the records being pointed \nto. With a subsequent dead-code elimination, a total of three boolean variables suffice. \u00ae For each pointer \nassignment, the new boolean fields and variables are updated accordingly. For instance, list.next = res; \n is replaced by the multiple-assignment statement: list.next = res, list.next die = res_dle_list; reflecting \nthe change of the next field. If arbitrary PAL formulas are allowed as right-hand sides of tile new \nassignments, even complex teachability properties can be captured. For this example, simple assignments \nsuf- fice, though. As in [31t, this is also sufficient to verify for instance that bubblesort actually \nsorts the elements. The intellectuM effort needed to update the data abstrac- tion bits seems to be the \nsame as to define the required oper- ational semantics in TVLA. As hinted in the example, some degree \nof automation is possible for our technique; however, we leave that for future work. Note that many data \nstructures, in particular variations of search trees, can be abstractly described by associating to every \nnode a t~w of bits of information summarizing prop- erties of the tree. Those data structures can also \nbe verified using techniques like these. 7 Implementation and Evaluation Our verification technique \nis implemented in a tool called PALE, the Pointer Assertion Logic Engine. Given an anno- tated program, \nPALE checks that: - the pointer directives are well-formed; \u00ae null pointer dereferences cannot occur; \n at each cut-l~oint that the data variables contain dis- joint, acyclic backbones spanning the heap and \nthat the assertions and pointer directives are satisfied; . all assert assertions are valid; and  all \ncut-point properties are satisfiable.  There is not necessarily an error in the program if a cut- point \nproperty is unsatisfiable, but it usually indicates an error in the specification. As previously mentioned, \nmemory allocation can easily be expressed such that the tool would also check for memory leaks and dangling \nreferences. Using PALE, we have evaluated the technique on a num- ber of examples dealing with a variety \nof data structures. In all cases, we check for memory errors and possible violations of the data structure \ninvariants: Singly-linked lists with the operations reverse, search, zip, delete, insert, and rotate. \nThese examples have been scrutinized before [8, 24, 32]. We also in- elude the concat operation on lists \nwith tail pointers from Section 1. We have tried bubblesort as in [31] but with various degrees of abstraction \nof the data: In bubblesort_simple, the record values are abstracted away so only null pointer dereferenees \nare checked for; in bubblesort_boolean, the values are abstracted to booleans which in the post-condition \nare checked to be properly sorted; and in bubblesort_full, the data ab- straction technique from Section \n6 is used as in [31] to conclude that the resulting lists are sorted. We also use data abstractions in \norderedreverse to show that reverse switches the order of a sorted list. Fi-nally, we try recreverse, \nwhich is a recursive version of reverse. Example name / Lines of l code lnvariants (formulas) GTA operations \nI Largest GTA [ States I BDD nodes Time (seconds) Memory (MB) reverse search zip 16 12 33 1,109 853 1,753 \n35 27 174 ] t 142 85 730 0.52 0.25 4.58 2 11 delete 22 973 73 , 349 1.36 5 insert 33 1,005 103 I 443 \n2.66 7 rotate 11 590 44 213 0.22 1 concat 24 1,056 48 177 0.47 3 bubblesort_simple 43 1,477 373 3,289 \n2.s6 iS bubblesort_boolean 43 2 1,737 357 3,922 3.37 12 bubblesort_full 43 2 2,069 373 3,291 4.13 19 \norderedreverse 24 i 1,091 29 100 0.46 3 recreverse 15 2 1,019 42 176 0.34 doublylinked 72 1 4,163 230 \n796 9.43 13 leftrotate 30 0 1,489 165 1,550 4.62 7 rightrotate 30 0 1,489 165 1,550 4.68 7 treeinsert \n36 I 1,989 137 844 8.27 31 redblackinsert 57 7 4,279 297 2,419 35.04 44 threaded 54 4 3,5{J5 50 248 3.38 \n7 Figure 1: Statistics from PALE experiments. * Doubly-linked lists with tail pointers [28] with the \nop- erations delete, search, insert, and concat. * Red-black search trees [10] with the standard oper-ations \nleftrotate, rightrotate, treeinsert, and redblackinsert. We include the non-arithmetic part of the red-black \nsearch tree invariant, that is, that the root is black and red nodes have black children: BLACK(root) \nallpos q of Node: KOOT<(left+right)*>q => (RED(q) => BLACK(q.left) a BLACK(q.right)); \u00ae Threaded trees \n[28], as shown in Section 3, where every node has a pointer to its post-order cyclic successor and a \npointer to its parent, with a fix operation for reestablishing the correct post pointer for a given node. \n The resources for translation into MONA code and for the automaton analysis are negligible. Figure 1 \nshows the time and space consumptions of the MONA automaton opera-tions (on a 466MHz Celeron PC) for \nthe examples, along with the number of GTA operations (here we count only the essential operations: minimization, \nprojection, and prod- uct), the size of the largest intermediate minimized automa- ton (in number of \nstates and in number of BDD nodes). Note that some examples implement individual operations while others \nimplement whole data types. The lines of code measure the underlying program only, thus disregarding \nthe PAL annotations. \"Invariants\" is the total number of split statements, while statements, and procedure \ncalls that re-quire explicitly stated invariants. This number is an indica- tion of the effort required \nby the programmer to make PALE work, in addition to writing the program and its specifica- tion. The \ninvaxiants for redblackinsert were admittedly hard to get right. However, the programs that require the \nmost complicated invariants axe also those that have the most complicated pointer operations and hence \nare the ones in most need of verification. The table shows that the ex-amples typically run in seconds \ndespite requiring a quite large number of automaton operations. Since the complex- ity is non-elementary \nin the size of the program, intractable examples do exist but they do not seem to occur often in practice. \nThe verification time seems insignificant compared to the time required to design a given data type and \nspecify the invariants, however, it is useful in the design cycle that verification is efficient. The \ncode for the bubblesort examples (excluding anno- tations) is taken from [31]. Interestingly, PALE discovered \na minor bug (a null-pointer dereference) even though the code had allegedly been verified by TVLA, which \nspent 245 sec- onds compared to 4 seconds for PALE. This huge speedup shows an instance where using invariants \nis much faster than performing a fixpoint iteration. This suggests that PALE may be quite scalable. Another \nnoteworthy point discov- ered by PALE is that in [10], the authors forget to require the root to be initially \nblack in redblackinsert. (More precisely, they mention the requirement in the proof of cor- rectness, \nbut not in the specification.) Versions with plausible bugs planted typically take roughly the same time \nto process as the correct programs. For such buggy versions, counterexamples are generated, which is \ncrucial for determining whether the error is in the program, the assumptions, or the assertions. As an \nexam- ple, if a conditional in redblackinsert erroneously tests for a specific node to be black rather \nthan red, PALE produces the following counterexample store for the Hoare triple con- taining the conditional: \nroot Here, the root node is black and the others are red, and we omit field names and all pointer fields. \nSuch a eounterexam- pie is clearly useful for locating the bug. Notice that for this bug, the approach \nin [23] would not find the bug for heap bounds of less than four records. The experiments show that our \napproach does work in practice for non-trivial data structures, and with time and space requirements \nwhich are as good as or better than those fbr the previous more specialized versions [24, 14] and re-lated \napproaches with similar goals [31, 23, 13, 17]. Conclusion It is well known that developing formal program \nspecifica- tions is expensive, but for some safety critical applications a guarantee of partial correctness \nof data type implementa- tions can be worth the effort. A tool such as PALE can be used to verify specifications \nexpressible in Pointer Assertion Logic, and also to guide the programmer by the generation of counterexamples. \nWith verification techniques based on undecidable logics, either the programmer may have to guide a theorem \nprover to the proofY, not even being certain that they exist, or accept that the reply may be \"don't-know\". \nWith less expressive techniques, important aspects of the data types may not be expressible and hence \nnot verifiable. In contrast to traditional program analyses, our technique is highly modular: each statement \nin the given program is analyzed only once. To verify complex properties, the tech- nique often requires \ndetailed invariants to be provided. How- ever, since we primarily aim for data-type implementations, \nwe believe that this annotation overhead is reasonable com- pared to the effort of creating the program. \nIn conclusion, Pointer Assertion Logic may provide a fruitful compromise between expressibility and usability. \nAlthough facing a non-elementary theoretical complex- ity, the examples we provide show that logic and \nautomaton based program verification is feasible. Furthermore, we be- lieve that the efficiency of the \nimplementation can be im- proved by at least an order of magnitude by tuning the MONA tool using heuristics \nas proposed in [27]. As also suggested in [23, 20] we may benefit from an initial sim- plification phase \nthat performs program slicing or partial evaluation of the source programs. Future work will also examine \nthe possibility of incor- porating simple arithmetic into the language. The MONA tool can also be used \nas an efficient decision procedure for Presburger arithmetic [39, 26], which is sufficient for many properties. \nIn [21], abstract data structure descriptions are used to improve program analyses in optimizing compilers. \nPointer aliasing, for instance, can be expressed in PAL, so the detailed knowledge of the heap structure \nprovided by PALE might also be useful for optimization. Another idea is to build a translator from C, \nC++, or Java to PALE to make the tool more practically useful. Finally, it might be interesting to integrate \nthe \"independent conjunctions\" from [37] into PAL to support local reasoning and make the tool easier \nto use. The full source code for the PALE tool, the examples, and a detailed description of the desugaring \nand code gen- eration to MONA are available from the PALE site at http ://www. brics, dk/PALE/.  Acknowledgments \n This work was done while the first author was visiting UC Berkeley. Thanks to Alex Aiken, Jeff Foster, \nand Zhendong Zu for valuable discussions and comments. References [1] Krzysztof R. Apt. Ten years of \nHoare's logic: A survey--part I. ACM Transactions on Programming Languages, 3(4):431-483, 1981. [2] Thomas \nBall and Sriram K. Rajamani. Bebop: A sym- bolic model checker for boolean programs. In Proceed-ings \nof the SPIN Software Model Checking Workshop, volume 1885 of LNCS, 2000. [3] Michael Benedikt, Thomas \nReps, and Mooly Sagiv. A decidable logic for describing linked data structures. In European Symposium \nOn Programming, ESOP~99, vol-ume 1576 of LNCS, 1999. [4] Morten Biehl, Nils Klarlund, and Theis Rauhe. \nAl-gorithms for guided tree automata. In First Interna- tional Workshop on Implementing Automata, WIA \n'96, volume 1260 of LNCS, 1997. [5] Paul F. Black and Phillip J. Windley. Inference rules for programming \nlanguages with side effects in expres- sions. In International Conference on Theorem Proving in Higher \nOrder Logics, 1996. [6] Randal E. Bryant. Graph-based algorithms for boolean function manipulation. IEEE \nTransactions on Comput- ers, C-35(8):677-691, Aug 1986. [7] William R. Bush, Jonathan D. Pincus, and \nDavid J. Sielaff. A static analyzer for finding dynamic program- ming errors. Software: Practice and \nExperience, 30(7), 2000. [8] Stephen A. Cook and Derek C. Oppen. An assertion language for data structures. \nIn Principles of Program- ming Languages, POPL'75, pages 160-166, 1975. [9] C. Corbett, Matthew B. Dwyer, \nJohn Hatcliff, and Robby. A language framework for expressing check-able properties of dynamic software. \nIn Proceedings of the SPIN Software Model Checking Workshop, volume 1885 of LNCS, 2000. [10] Thomas H. \nCormen, Charles E. Leiserson, and Ronald L. Rivest. Introduction to Algorithms. MIT Press, 1990. [11] \nPatrick Cousot. Formal models and semantics. In J. van Leeuwen, editor, Handbook of Theoretical Computer \nScience, volume B, pages 841-993. MIT Press/Elsevier, 1990. [12] David L. Detlefs, K. Rustan M. Leino, \nGreg Nelson, and James B. Saxe. Extended static checking. Re-search Report 159, Compaq Systems Research \nCenter, December, 1998. [13] Nurit Dor, Michael Rodeh, and Mooly Sagiv. Checking cleanness in linked \nlists. In Seventh International Static Analysis Symposium, SAS'O0, 2000. [14] Jacob Elgaard, Anders M\u00a2ller, \nand Michael I. Schwartzbach. Compile-time debugging of C programs working on trees. In European Symposium \non Program- ming Languages and Systems, ESOP'O0, volume 1782 of LNCS, 2000. 230 [15] David Evans. Static \ndetection of dynamic memory er- rors. In Programming Language Design and Implemen- tation, PLDI'96, 1996. \n[16] Robert W. Floyd. Assigning meanings to programs. Mathematical Aspects of Computer Science, American \nMath. Soc., 1967. [17] Pascal Fradet, Ronan Gaugne, and Daniel Le M6tayer. Static detection of pointer \nerrors: An axiomatisation and a checking algorithm. In European Symposium on Programming, ESOP'96, volume \n1058 of LNCS, 1996. [18] Pascal Fradet and Daniel Le Mdtayer. Shape types. In Principles of Programming \nLanguages, POPL'97. ACM, 1997. [19] David Cries. The Science of Programming. Springer-Verlag, 1981. [20] \nKlaus Havelund and Thomas Pressburger. Model checking Java programs using Java PathFinder. In-ternational \nJournal on Software Tools for Technology Transfer, 2(4), April 2000. [21] Laurie Hendren, Joseph Hummel, \nand Alexandru Nico- lau. Abstractions for recursive pointer data structures: Improving the analysis and \ntransformation of impera- tive programs. In Program Language Design and Im- plementation, PLDI'92, 1992. \n[22] C.A.R. Hoare. An axiomatic basis for computer pro- gramming. Communications of the ACM, 12(10):576-580, \nOct 1969. [23] Daniel Jackson and Mandana Vaziri. Finding bugs with a constraint solver. In International \nSymposium on Software Testing and Analysis, ISSTA '00. ACM, 2000. [24] Jacob L. Jensen, Michael E. Jergensen, \nNils Klarlund, and Michael I. Schwartzbach. Automatic verification of pointer programs using monadic \nsecond-order logic. In Programming Language Design and Implementation, PLDI'97, 1997. [25] Nils Klarlund. \nMona &#38; Fido: The logic-automaton connection in practice. In Computer Science Logic, CSL'97, volume \n1414 of LNCS, 1998. [26] Nils Klarlund and Anders M\u00a2ller. MONA Version 1.4 User Manual. BRICS Notes Series \nNS-01-1, Depa/t- meat of Computer Science, University of Aarhus, Jan- uary 2001. [27] Nils Klarlund, \nAnders M\u00a2ller, and Michael I. Schwartzbach. MONA implementation secrets. In FiSh International Conference \non Implementation and Ap- plication of Automata, CIAA '00, LNCS, 2000. [28] Ntis Klarlund and Michael \nI. Schwartzbach. Graph types. In Principles of Programming Languages, POPL'g3. ACM, 1993. [29] Nils Klarlund \nand Michael I. Schwartzbach. Graphs and decidable transduetions based on edge constraints. In Trees in \nAlgebra and Programming, CAAP'9$, volume 787 of LNCS, 1994. [30] Nils Klarlund and Michael I. Schwartzbach. \nA domain-specific language for regular sets of strings and trees. IEEE Transactions On Software Engineering, \n25(3):378-386, 1999. [31] Tal Lev-Ami, Thomas Reps, Mooly Sagiv, and Rein- hard Wilhelm. Putting static \nanalysis to work for ver- ification: a case study. In International Symposium on Software Testing and \nAnalysis, ISSTA '00. ACM, 2000. [32] Tal Lev-Ami and Mooly Sagiv. TVLA: A system for implementing static \nanalyses. In Seventh International Static Analysis Symposium, SAS'O0, 2000. [33] Albert R. Meyer. Weak \nmonadic second-order theory of successor is not elementary recursive. In R. Parikh, editor, Logic Colloquium, \n(Proc. Symposium on Logic, Boston, 1972), volume 453 of LNCS, pages 132-154, 1975. [34] Anders M\u00a2ller. \nwww.brits, dk/mona. MONA project home page. [35] Anders M\u00a2ller. www.brits, dk/PALg. PALE project home \npage. [36] Joseph M. Morris. A general axiom of assignment. In Theoretical Foundations of Programming \nMethodology, 1982. [37] John C. Reynolds. Intuitionistic reasoning about shared mutable data structure. \nIn Millennial Perspec- tives in Computer Science, 2000. [38] Mooly Sagiv, Thomas Reps, and Reinhard Wilhelm. \nParametric shape analysis via 3 valued logic. In Prin-ciples of Programming Languages, POPL'99, 1999. \n[39] Thomas R. Shiple, James H. Kukula, and Rajeev K. Ranjan. A comparison of Presburger engines for \nEFSM reachability. In Computer Aided Verification, CA V'98, volume 1427 of LNCS, 1998.  \n\t\t\t", "proc_id": "378795", "abstract": "<p>We present a new framework for verifying partial specifications of programs in order to catch type and memory errors and check data structure invariants. Our technique can verify a large class of data structures, namely all those that can be expressed as <i>graph types</i>. Earlier versions were restricted to simple special cases such as lists or trees. Even so, our current implementation is as fast as the previous specialized tools.</p><p>Programs are annotated with partial specifications expressed in Pointer Assertion Logic, a new notation for expressing properties of the program store. We work in the logical tradition by encoding the programs and partial specifications as formulas in monadic second-order logic. Validity of these formulas is checked by the MONA tool, which also can provide explicit counterexamples to invalid formulas.</p><p>To make verification decidable, the technique requires explicit loop and function call invariants. In return, the technique is highly modular: every statement of a given program is analyzed only once.</p><p>The main target applications are safety-critical data-type algorithms, where the cost of annotating a program with invariants is justified by the value of being able to automatically verify complex properties of the program.</p>", "authors": [{"name": "Anders M&#248;ller", "author_profile_id": "81100071429", "affiliation": "BRICS, Department of Computer Science, University of Aarhus, Denmark", "person_id": "P16729", "email_address": "", "orcid_id": ""}, {"name": "Michael I. Schwartzbach", "author_profile_id": "81392609511", "affiliation": "BRICS, Department of Computer Science, University of Aarhus, Denmark", "person_id": "P198767", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/378795.378851", "year": "2001", "article_id": "378851", "conference": "PLDI", "title": "The pointer assertion logic engine", "url": "http://dl.acm.org/citation.cfm?id=378851"}