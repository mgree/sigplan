{"article_publication_date": "11-04-2002", "fulltext": "\n Transformation of an Application Data Layer Will Loew-Blosser Grain IT, Cargill, Inc P.O. Box 5664, \nMinneapolis, MN 55440-5664 952-742-7138 will_loew-blosser@cargill.com Abstract Changing a fundamental \ninterface in a large application is typically considered impractical because of high risks and costs. \nThis report demonstrates that with careful use of tools and testing, risks and costs can be significantly \nreduced. Normally, refactoring consists of a series of small safe steps that improve design while preserving \nbehavior. Instead of performing many small transformations, we used the facilities in Brant and Roberts' \nSmalltalk Refactoring Browser to make more significant program transformations by defining our own program \ntransformation rules This allowed us to systematically make 17,200 changes almost bug free. We further \nreduced our risk by rigorously testing our changes using Beck's S-Unit tool. After these transformations, \nthe client data framework was reduced from about 40% to less than 2% of the client execution time. Performance \nIssues A very large Smalltalk application was developed at Cargill to support the operation of grain \nelevators and the associated commodity trading activities. The Smalltalk client application has 385 windows \nand over 5,000 classes. About 2,000 classes in this application interacted with an early (circa 1993) \ndata access framework. The framework dynamically performed a mapping of object attributes to data table \ncolumns. Analysis showed that although dynamic look up consumed 40% of the client execution time, it \nwas unnecessary. The look up system was not needed because the attribute of a class had a strict mapping \nto exactly one data column. To improve performance, we decided to replace dynamic look up with an explicit \nobject to database mapping scheme. Our solution involved changing the data framework as well as the application \nclasses. The challenge was how to change 2,100 classes in a systematic manner. Applying a New Interface \nA new data layer interface was developed that required the business class to provide the object attribute \nto column mapping in an explicitly coded method. Testing showed that this interface was orders of magnitude \nfaster. The issue was how to change the 2,100 business class users of the data layer. Aside from the \nperformance problem, the early framework was well designed. The difficulty in the transformation was \nfinding the constructed mapping definition in the execution path of the original code. Elements of the \nmapping definition could be spread across subclasses or collaborators. The rewrite rules of the Refactoring \nBrowser convert source code into a parse tree structure within the search and replace system. The Refactoring \nBrowser has over 60 built in transformations that should be used as example for new rewrite rules. The \nlimitation (actually key design feature) is all of the provided transformations are behavior safe per \nthe Opdyke thesis, i.e. the refactoring change to improve the design will not change the behavior of \nthe application. However, our goal was to change implementation behavior, not preserve it. The preservation \nof behavior occurred at the more abstract level of preserving the reading and storing of objects to the \ndatabase. Thus, we needed new rewrite rules that were not strictly behavior safe. We did not use the \nGUI of the Refactoring Browser. Instead, we wrote rules in code. A number of transformation steps based \non the rules were constructed that moved the application onto the faster data layer framework. Twenty-three \nsearch rules and eleven rewrite rules were created. A number of the existing refactoring rules were also \nused in the transformation steps. To construct rewrite rules you should closely study the search and \nreplace rules in the Refactoring Browser. An excellent example for learning the usage of rules is found \nCopyright is held by the author/owner(s). in the class ExtractMethodRefactoring as well as other subclasses \nof Refactoring. We found it usually takes one to three days to construct each new rule. We ve gotten \nbetter but it is still not easy. It is difficult because a terse syntax of special symbols is used to \nspecify the variable and substitution aspects of the search and replace expressions. The behavior of \nthe special symbols against a target code expression is hard to understand. However, the effort to gain \nan understanding of the matching and substituting code expressions is well worth the time in a large \ncode base. We were also helped by the development history. Developers coming onto the project were given \nexample code that easily transformed into an explicit column mapping. As a consequence, we could construct \na few rewrite rules to do most of the transformations in a routine way. And fortunately, the dynamic \ncomposition aspect of the data layer was difficult to understand so only two of the more than 100 developers \nused the composition feature over the more explicit example code. In the dynamic composition data requests \na superclass provided a template or incomplete form of a data request. The subclasses were responsible \nfor providing the missing arguments to the data request. In some usages the data requests represented \nthe composition of several cooperating objects, with each supplying one or more parameters, and receiving \none or more columns from the result set of the data request. Performance Improvements The old framework \noften used a pattern of access to attribute values by names in a dictionary. Obtaining the value could \ntake up to six successive dictionary lookups. A similar set of lookups was needed to set the value of \nan attribute from the data result set. The performance was improved by replacing the lookups with a single \nmessage send to the holder of the attribute. The name used by the class always resolved to the same attribute. \nThe transformation process had to change each use of an attribute name to the correct message send. The \nsimple cases consisted of names of attributes that belonged directly to the class making the data request. \nThe complex cases occurred when the use of the attribute name and the source of the value were distant \nfrom each other. Before and After Example 1 This code provides the definition of the parameters used \nin a data request. Old Form TruckAgent#createParms ^(OrderedCollection new add: #(loadArrivalFlag str \nfalse false) At execution time the values were looked up indirectly by the name of the attribute listed \nin the first element of the array. Below in the new form, the arrays are replaced with an object. More \nimportantly the new Parm receives the value by a direct message to the holder of the value. This is the \nsimple case where the attribute belongs to the class making the data request. New Form TruckAgent#createParms \n ^(ParmList new) add: ((Parm name: #loadArrivalFlag type: Str required: false output: false) value: self \ndbGetLoadArrivalFlag); This change was done in a two pass strategy. First, the possible sources of the \nnamed attribute and other parameters were collected by ParseTreeSearch rules, using the same search path \nas the runtime behavior. Then the new code was constructed by assembling the fragments into the new method. \n Before and After Example 2 The old form dynamic look up system allowed the parameters to a data base \nstored procedure to be added at different points in the execution. This is seen below where a data request \nis created (createSurrogate). A mapping for the attribute named #cpItem is then added with the aliasFor: \nstatement. The object holding the cpItemId is actually four messages sends distant from the object executing \nthe data request. Old Form TruckAgent#createRow | surr | surr := self createSurrogate. surr aliasFor: \n#cpItem is: [self client cpItem dbmAgent] knownAs: #id.  New Form TruckAgent#createParms ^ParmList \nnew add: ((Parm name: #cpItem type: Int required: false output: false) value: [self client cpItem dbmAgent \ndbGetId]) The rewrite system must search the callers for the definition of the value and move the definition \nto a new point in the execution. Below is the search expression that finds the aliasFor: code in the \nold form ``@messageReceiver aliasFor: `#attribute is: ``@codeBlock' knownAs: `#realAttributeName => \n[:node :answer | answer add: node; yourself] The codeBlock expression in the old form is pulled out \nin the first pass. When the new form is assembled in a different method the codeBlock expression is added. \n Rewrite Rule Example This is an example of a rewrite rule to illustrate the syntax of the special characters. \nIt has the purpose of adding a wrapper object around global class variables for execution on a server. \n#protectClassVar: aVarName in: targetTree contextString := self findContextTag: targetTree. rewriteRule \n:= ParseTreeRewriter new. rewriteRule replace: aVarName , ' := ``@object' with: 'self ' , (self newSelectorName: \naClassVarName) , ' ' , contextString, 'Value' , ': ' , ' ``@object' rewriteRule executeTree: methodTree. \n Each variable in the replace statement is distinguished with the ` symbol. Two `` and the @ symbol together \nindicate recursive search and search of lists. The ` symbol may be modified by following symbols indicating \nliteral expressions (#) or statements (.). The expression will match an assignment to the class variable \nand replace the assignment with a wrapper statement. The wrapper statement is based upon a tag in the \nmethod. Thus all points in the code that directly access a class variable are changed to the new form. \nBelow is a comparison of the old code and new code produced by the rule. Old Form cache: aCollection \n #taskContext. ^cache := aCollection  New Form cache: aCollection #taskContext. ^self cacheHolder \ntaskContextValue: aCollection  Manual Changes In some cases, code quirks were found that did not merit \nthe investment of writing a rule. These quirks were changed manually. Usually these changes were done \nbecause the narrow scope of the change did not seem to merit the time to construct a new rule. Either \nthe code was changed into a form that an existing rule would handle or it was changed after a transformation \nwas run. Usually the manual changes affected only several methods. The manual changes were saved in the \nform of the actual code, not a rule. In the aggregate however, the number of manual changes still amounted \nto a size that required a scripting system to correctly apply in right order. About 187 manual change \nfiles were run in the script system. The manual changes proved to be the most fragile aspect of the transformation. \nThe number of manual changes should be kept as low as possible to avoid collisions on changes to the \nsame code between the main development effort and the transformation work. Parallel Development A large \napplication under development cannot freeze code while a transformation of an interface is constructed \nand tested. We had to construct and test the transformations in a parallel branch of the code repository \nfrom the main development stream. When the transformation was fully tested, then it was applied to the \nmain code stream in a single operation. During the development of the transformation a developer sometimes \nchanged code in the main stream where there already existed a manual change in the transformation code. \nSince the manual change was based on now obsolete code an execution of the manual change had the effect \nof removing the mainstream fix. A rewrite rule (as opposed to a saved manual change) does not have this \nproblem, since the search statement will evaluate the current code according to the search rule. Risk \nManagement &#38; Testing High Risk / High Cost is the reason that a fundamental interface is usually \nimpossible to change on the practical level. A large number of classes are affected. An error in the \ntransformation could bring down the entire mission critical application. Or it could cause a large number \nof independent errors that overwhelm the development and support staffs. The accuracy of the search and \nreplace rules provides a theoretical basis for reducing risk. The key to moving from high risk to low \nrisk is to show that the transformation actually works. In our case we wanted to prove that the system \nlevel behavior of reading and writing to the database was not altered. Each call to the database should \nhave the same arguments. Each read from the database must put the values of the columns into the same \nobject attributes. Our goal was to exercise each and every call to prove that the transformation worked \nin all cases. We extended the Kent Beck S-Unit Test Framework to provide a test that compared the before \nand after states. We used an unusual technique for the test because a direct value comparison of before \nand after states would fail because our database uses an artificially generated key for each row and \ntimestamp based locking. Thus two successive insert calls will return different id values and timestamp \nvalues. The test used a more abstract condition. It tested the condition that the attribute of the object \nmapped to the same database call argument. The return was tested for the condition that the same column \nmapped to the same business object attribute. A business test case was run with a hook that captured \nand stored the mappings before the transformation. After the transformation was performed, the business \ntest case was run again with the hook capture. The transformation test passed if the stored mappings \nfrom the two executions were the same. Due to the large size of the application we only achieved about \n50% coverage of the business test cases. The entire client development staff spent several weeks developing \ntest cases. At the 50% point an analysis of the remainder showed that most were of a low risk form. A \nfew high-risk transformations were added to the test group. A high-risk data request was one in which \nthe attribute needed for a value was distant from the creator. Building and running the application wide \nbusiness test cases was the most expensive aspect of the project however, it has had long-term benefits \nas well. Overall application stability has greatly improved as the application has matured into a maintenance \nphase. Extensive use of the Beck S-Unit Test Framework and the rewrite rules of the BrantRoberts Rewrite \nRules changed a high-risk activity to low risk. This was borne out by the low number of bugs in release. \nLess than 35 bugs were found in the 17,100 changes. All of the bugs were quickly resolved in a three-week \nperiod. Rewrite Rule Development Time The complete set of Rewrite Rules took about 235 hours to develop. \n1,455 hours were spent on test development and running the tests. We needed a high level of confidence \nbefore the changes were released. Phase Hours % Framework Tuning 288 13.2% Performance tests 107 4.9% \nRewrite rules 235 10.8% Application test cases 600 27.6% Rewrite Testing 748 34.4% Bug fixes 196 9.0% \nTotal Hours 2,173 100.0% If the changes were done manually we estimate that it would have taken 8,500 \nhours, compared with 235 hours to develop the transformation rules. The 8,500 hours is based on an optimistic \nmetric of 30 minutes per change and 17,100 changes. Manual changes would certainly have had more bugs. \nThe manual evaluation of the source of each attribute is not quick because it commonly was not clear \nfrom casual code inspection. It could be argued that a metric of one hour per change would be more realistic. \nIf the changes had been done manually the time for the framework tuning, new test cases and testing would \nbe the same. The effective improvement from using Rewrite Rules is based on the ratio of 235 hours for \nRewrite Rules to 8,500 hours for manual changes. The task was completed in 3% of the expected time by \nusing Rewrite Rules. This is an improvement by a factor of 36. Other Rewrite Rule Development We have \nused Rewrite Rules in three other application enhancements and found similar time savings: VisualWorks \n7 migration Vendor metrics suggested that our very large application would have taken 410 days to migrate \nfrom version 2.52 to version 7. Migration time actually took 27 days by developing and using Rewrite \nRules. Class variable protection framework A framework that provided process safe class variable access \nis estimated to have taken roughly 60 days to manually install around each class variable. Removing it \nwith a rewrite rule took 4 days. If we had reversed the rewrite rules and used them for the installation \nthen the 60-day task would have been completed in 4 days. Error recovery A framework experiment that \nprovided a state commit/rollback feature on errors to all objects in the system was implemented using \nRewrite Rules to add a hook line in front of every variable change. The Rules took about 5 days to develop. \nWe did not estimate the time to manually make such a change, but we suspect it would have been well over \n10,000 hours to touch every variable in the system. This illustrates the low cost of software experiments \nusing Rewrite Rules. Conclusion The BrantRoberts Rewrite Rules and the Beck S-Unit Test Framework are \na powerful tool combination that let us achieve a previously impractical task. The much higher cost due \nto time and errors makes manual changes of a major interface impractical. The application of these techniques \nshowed that rapid major changes to early design decisions are possible even late in the development cycle. \nCompanies can avoid high rework costs and preserve large investments in existing applications by applying \nthese techniques when a tool such as the Refactoring Browser is available. By using the Rewrite Rules \nyou can safely try and change approaches quickly. Careful use of tools and testing instead of brute force \nincreases productivity by large multiples. From a software vendor perspective the issue of backward compatibility \nbecomes almost moot when you can offer a rewrite rule change path for existing applications. Application \nteams will have much greater freedom to change out major components using the rewrite rule technology. \nFor example changing to different GUI or GUI framework, or switching database vendors is now possible \nif you understand how to transform coding using rewrite rules. Language &#38; Tool Vendors should add \nRewrite Compilers Currently only Smalltalk applications have the benefit of the tool. There is a much \nlarger portfolio of other language applications, which could potentially have substantial benefit from \na similar rewrite tool. The search and replace functions of the BrantRoberts Rewrite Rules use a traditional \ntechnique of algorithms based upon generating a compiler parse tree. This technique could be applied \nback to other languages to produce a Rewrite Compiler whose output is rewritten source code, not machine \nlevel code. The key point is that a parse tree is essential to achieving the correct transformation of \ncomplex expressions, and a compiler seems the appropriate place for meta knowledge about the target language \nand system. References Refactoring Browser is a freely distributed tool created by John Brant and Don \nRoberts during their PhD research at the University of Illinois at Urbana-Champaign see http://st-www.cs.uiuc.edu/~brant/RefactoringBrowser/ \nSee also A Refactoring Tool for Smalltalk , Don Roberts, John Brant, and Ralph Johnson http://st\u00adwww.cs.uiuc.edu/~droberts/tapos/TAPOS.htm \nWilliam Opdyke Phd thesis University of Illinois at Urbana-Champaign \u00adftp://st.cs.uiuc.edu/pub/papers/refactoring/opdyke\u00adthesis.ps.Z \nThe Smalltalk examples are only representative of the actual code. They are provided for the purpose \nof illustrating usage of the Refactoring Browser.           Development Metrics Phase Hours \n% Framework Tuning 288 13% Performance tests 107 5% Rewrite rules 235 11% Application Test Cases 600 \n28% Rewrite Testing 748 34% Bug Fixes 196 9% Total 2,173 100 % 20   \n\t\t\t", "proc_id": "604251", "abstract": "Changing a fundamental interface in a large application is typically considered impractical because of high risks and costs. This report demonstrates that with careful use of tools and testing, risks and costs can be significantly reduced.Normally, refactoring consists of a series of small safe steps that improve design while preserving behavior. Instead of performing many small transformations, we used the facilities in Brant and Roberts' Smalltalk Refactoring Browser to make more significant program transformations by defining our own program transformation rulesThis allowed us to systematically make 17,200 changes almost bug free. We further reduced our risk by rigorously testing our changes using Beck's S-Unit tool. After these transformations, the client data framework was reduced from about 40% to less than 2% of the client execution time.", "authors": [{"name": "Will Loew-Blosser", "author_profile_id": "81100188095", "affiliation": "Grain IT, Cargill, Inc, Minneapolis, MN", "person_id": "P414284", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/604251.604258", "year": "2002", "article_id": "604258", "conference": "OOPSLA", "title": "Transformation of an application data layer", "url": "http://dl.acm.org/citation.cfm?id=604258"}