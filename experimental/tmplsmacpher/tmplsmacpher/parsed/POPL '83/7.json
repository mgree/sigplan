{"article_publication_date": "01-24-1983", "fulltext": "\n Permission to make digital or hard copies of part or all of this work or personal or classroom use is \ngranted without fee provided that copies are not made or distributed for profit or commercial advantage \nand that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, \nto post on servers, or to redistribute to lists, requires prior specific permission and/or a fee. &#38;#169; \n1983 ACM 0-89791-090-7 $5.00 (2) For any type in Russell there is an expression of that type; and  (3) \nAny Russell expression can be parameterized with respect to any free identifier occurring in it to yield \na function of a more complex type.  Because Russell is type-complete, it has a simp!e, clearly identifiable \nlanguage core of combining forms and then a larger set of interchangeable parts. These interchangeable \nparts include the builtin types and, most importantly for our purposes, the operations on variables. \nIn most pro\u00adgramming languages variables can be manipulated by pro\u00adgrams (they can be passed as arguments \nto procedures, for example) but the operations that manipulate them are not all part of the surface syntax \nof the language. Assign\u00adment usually is a named primitive operation, but other operations like variable \nallocation usually are not, making it impossible to define the semantics of variables using the same \nlogic that defines the non-variable component of the language. There are compelling reaaons for this \n-few languages have type structures rich enough to describe such operations as variable allocation. Type-completeness \nmakes it possible for all the operations that manipulate variables in Russell to be named functions in \nthe pro\u00adgramming language. Making Russell type-complete allowed us to bring the operations on variables \nup to the surface, so that the same equational theory that defines the language core can also define \nthe parts of the language that manipulate variables. The other important design decision was to begin \ndeveloping an equational proof system for Russell before the language design waa frozen, While developing \nthe equational system, we found places where critical proper\u00adties of the operations on variables could \nnot be expressed as equations among expressions of the language. Our decision to develop an equational \nspecification had an interesting consequence instead of enriching an assertion language for describing \nRussell, we were forced to enrich the programming language itself. It seems to us that this is an important \npoint. A good deal has been written about the need to develop a language and its proof rules simultaneously; \nthe hope is that the language will be molded by the desire to keep its proof rules simple. The use of \nan equational system puts an interesting twist on this idea: the requirements of the proof rules may \npoint out areas where the programming language should be made more expressive. In the following section \nwe describe the Russell language subset used in this paper. We then present the overall structure of \nour equational proof system, and dis\u00adcuss the most important inference rules, The complete set of rules \nis given in [Demer&#38;2]. Next we discuss proof rules for operations on simple variables. We give two \nsample proofs to convey the flavor of reasoning in the sys\u00adtem. Finally, we give rules for composite \nvariables (records) which show how their semantics are straightfor\u00adward compositions of the semantics \nof their components. The Programming Language The language used below is a minor modification of Russell \n[Boehm80]. As we have already remarked, Russell is a type-complete language. It includes function-and \ntype-valued expressions and treats them on an equal foot\u00ading with more conventional expressions. Russell \nis statically typed. As described below, the Russell logic includes rules by which we associate with \neach expression e a syntactic type specifier or signature S. The signature of e tells whether e denotes \na value or a variable, and gives an expression for the type of e. Thus, a signature has the form var \nexp or val exp where, informally, exp must be a type-valued Russell expression. Formally, S is a legal \nsignature iff the formula legal S is provable in the logic. Expressions have a fairly conventional syntax. \nIn the following description, variables beginning with p,q,...,x ,y, z represent identifiers; variables \nbeginning with d,...,h or T represent expressions; and those beginning with S represent signatures. A \nRussell expression may be any of An identifier or constant. Special identifiers such as int, bool and \ntype, which are builtin types, and a number of builtin functions to be introduced below, are treated \nas constants that may not be redeclared. A sequential composition, of the form (e,; .... e.) A let-block, \nof the form let .... Xi = d;;,,.in eni A conditional, of the form if e, then e2 else es tl A loop, \nof the form while e, do e2 od An application eO [ elj -.l en1 of function eO to arguments e, through \nem. When no confusion should arise we shall frequently write appli\u00adcations in infix rather than prefix \nnotation. A ,function type expression, of the form func[...; Xi:Si; ...] S Intuitively, such an expression \ndenotes the type of n\u00adparameter functions, where x, through x, are param~ ter names, SI through SD are \nthe corresponding parameter signatures, and S is the result signature. For example, func[x :val int; \ny:val int] VZd int is an expression for the type of the integer addition operation. A function construction, \nof the form func t[...; Xi,Si; ...] { e } Such an expression denotes a function, where x, through X. \nare parameter names and S1 through S, are the corresponding parameter signatures as above; identifier \nt is a local name for the function, which 60 allows it to call itself recursively; and expression e \nis the function body. For example, func square[x :val int] { if x <O then square[-x] else if x >0 then \nsquare[x 1]+ x+ x 1 else Ofill} is an expression for an integer squaring function. The Structure of the \nLogic The first task in specifying the semantics of variables is to find a set of operations suitable \nfor characterizing their behavior. What sort of behavior must recaptured? That one can assign to a variable \nand extract the ualue possessed by a variable. Thmt variables can alias --two syntactically distinct \nvariable-producing expressions in a program can pro\u00adduce semantically indistinguishable results. That \nnew variables can be allocated. This allocation does not change the value of any existing variable, nor \ndoes the new variable alias with any existing variable. In Russell the operations of assignment and value \nextraction, the predicates that test for aliasing and the operation of allocating a new variable are \nall just particu\u00adlar builtin functions. Thus it seemed natural to apply the ideas of abstract data type \nspecification, where semantics is given by a set of equations relating the results of func\u00adtion applications, \nto defining Russell. In doing so we encountered two problems not common to the usual equa\u00adtional specifications: \n(1) Russell expressions can have effects, as well as pro duce values-that is the whole point of having \nvari\u00adablcs in the language. Thus, the equations of our system must relate not only the values of expressions \nbut also the effects of evaluating them. In a sense, one of the major problems in the development of \nan equational theory for Russell is the semantics of the ; operator. (2) Equivalences between expressions \nmay hold only in certain contexts. For example, the following equivalence is obviously valid:  (let \nx.* 3 in y :=4; xni) = (y:=4;3) Here we have substituted the right-hand side of the declaration ( 4 ) \nfor the declared identifier ( y ) within block body. On the other hand, consider (let x =. ValOf(y) in \ny := 4; x ni) = (y:= 4; ValueOf(y) ni) Again we have just substituted the right-hand side of a declaration \nfor the left-hand identifier. However, this equivalence is clearly invalid. The equations of our system \nmust include premises that allow the first substitution but disallow the second. These constraints led \nus to a Gentzen-style deductive sys\u00adtem having much in common with those of [Bates82] and [Martin-Lof79]. \nIn the remainder of this section we present the basic structure of the system, In later sections we give \nthe rules of inference that define our Russell sub\u00adset. Formulas Our logic is basically an equational \none -formulas can have the form e, = e2, where e, and ez are expressions of the programming language. \nHowever,, to deal with the problems discussed above we need several other kinds of formulas. The formulas \nof our system include: Typings. These are formulas of the form e -S, which assert that e is a legal Russell \nexpression whose signature is S. L ecjalify Assertions. These are formulas of the form legal S, which \nassert that S is a legal signature -S IMS the form var e or val e where e is a type-valued expression \nwhose value is independent of the store. Purity Assertions. These are formulas of the form pure e, which \nassert that expression e is independent of the values of any variables -e neither has an observable effect \nnor produces a value that depends on the current state. An expression e is said to be pure if pure e \nholds. The notion of purity is an important one and is discussed at some length below. Equivalences. \nThese are formulas of the form el = e2, which assert that expressions el and e2 are completely equivalent \n that they have the same value and pro\u00adduce the same observable effects when evaluated. If exp is a Boolean \nexpression we will sometimes abbre\u00adviate Lexp = True by exp . Note that True is a pure, total expression \n its evaluation always ter\u00adminates without error or observable effect so the equivalent expression exp \nmust likewise be pure and total. It is worth noting that the truth or falsity of any for\u00admula is independent \nof the store. For example, if e, = e2 is true the expressions e, and et may reference program variables \nbut the two expressions must be equivalent independent of what values are possessed by those vari\u00adables. \nEnvironments An environment r is a set of formulas r=eu+ where 0 is a set of typings in which only simple \nidentifiers (not arbitrary expressions) may appear on the left hand sides, and @ is set of equivalence \nformulas. When 0 con\u00adtains a typing (x-S) we say that e (and hence r) defines x. I is closed if e defines \nevery identifier that occurs free in a formula of e or @. All environments arising in correct proofs \nare closed. Given r and a set of program identifiers xl through Xn, we may construct A (r is closed) \nA (r defines none of XI,...,Xn) }. i.e., the maximal closed subset of r not defining any of X1 through \nx,. Less formally, this construction simply deletes from r definitions of x, through X. and any formulas \nthat depend on them. BY construction r/xi,...,x, is closed, since an arbitrary union of closed environments \nis easily seen to be closed. Using the above construction, we define (r,xl-&#38;,...,xS,),) d,f r/xl,...,xn \nu {X,-sl,...,sm},m}, which gives the effect of declaring program identifiers on the environment first \nall references to previous instances of the identifiers are deleted from the environment, and then new \ntypings are added. Note that (r,...,xi_Si)...) is closed if all free identifiers of % through &#38; are \namong X1 through x. or are defined in r/xl,...,x.. Similarly, we define r I Xl,...,xn, the var restriction \nof r to X, -through Xa, by r 1X1,.,.,X. d,f U{ r ! (r ~r) A (r is closed) A((y-var T)Er * ~iy = Xl) } \nThis rest;icts r so that any free identifiers with vrm signa\u00ad ture must be among xl through x.. Finally, \nwe define r,,, &#38;f rI i.e., the restriction of I to no var identifiers (leaving only identifiers with \nval typings). This operation is useful because expressions that can be typed using only the information \nin I vll are guaranteed to be pure. Goals A goal G has the form r*F with the meaning that the formula \nF is a consequence of the typings and equivalences in r. Inference Rules An inference rule has the form \nG, ... G, G with the meaning that from G, through G, we may con\u00ad clude G. We call G the conclusion of \nthe rule, and G, through G, its hypotheses or wb~oak We shall frequently abbreviate by allowing lists \nof formulas to appear in hypotheses and conclusions. We also follow the conven\u00ad tion that the variables \nt,u,..., z represent new identifiers, and that an expression of the form d[...xix]...] which represents \nthe result of simultaneously substituting el through em for X1 through Xn, is legal only if no capture \noccurs. Theorems The set of theorems is the smallest set of goals closed under deduction using the inference \nrules. This set is nonempty, since there are inference rules with no hypotheses. Basic Rules The system \nincludes some logical rules stating obvi\u00adous basic facts about equivalences and typings. Among them are \nassumption and modus ponens rules, rules stat\u00ading rej7exivity, symmetry and transitivity of =, and substi\u00adtution \nrules that allow provably equivalent expression to be substituted for one another in equivalence formulas \nand typings. These rules are straightforward, and can be found in [Demers82] The Russell subset we are \nusing includes builtin types int and bool with the Boolean, arithmetic and com\u00adparison operators + , \n-, *, /, ~, A, V, etc. These opera\u00adtions treat values rather than variables, and in a Hoar&#38; style \nsystem there are equivalent, identically-named func\u00adtions in the assertion language. To enable us to \nreason about terms that use these operators, we include all rules of the form L 10 (equivalent terms) \n@~(el-T, e2 -T) where (1) 61 consists of tbe typings contained in r. (2) T is either int or bool, (3) \nExpressions e, and .2 are equivalent terms using only the builtin int and bool operators mentioned above. \nFree identifiers are considered universally quantified.  These rules correspond roughly to the assumption \nall true formulas in the assertion language are axioms fre\u00adquently made when discussing relative completeness \nof systems of proof rules cf. [deBakker80]. Using the complete set of rules from [Demers82], we can \nshow that if el and e2 use only the bool operators listed above, and if is provable, then is provable, \nand so is whenever e$ is a bool term such that e, * ea tautologi\u00ad cally implies es. Purity, Null Effect \nand Legality In giving the semantics of a language in which expressions may have effects, it is necessary \nat times to limit the use of inference rules to only those subexpres\u00adsions that are provably independent \nof state. For exam\u00ad pie, an equivalence such as must be provable only if the meanings of el and e2 are \nindependent of the number of times they are evaluated and the order in which they are evaluated. We can \nguarantee this by requiring that e, and e2 be pure intui\u00adtively, that evaluation of e, or ez neither \ndepend on nor affect the value possessed by any variable. Thus, the logic includes purity awertions and \na number of rules that allow us to deduce that expressions are pure. Purity assertions are perhaps the \nmost novel aspect of this work, and they appear as hypotheses in many of the rules below. Purity is not \nthe same as freedom from side-effects. Consider the expression VatOf[x] which extracts the value possessed \nby variable x. This expression has no effect, but its value manifestly depends on the state; thus it \nis not pure. Conversely, evaluation of a pure expression can have an effect, since it does not necessarily \nterminate without error. Thus the plausible null eflect rule (pure null effect (UNSOUND)) r ~ (e,; e2)-S \nr 1- pure e, r * (e,; e2) = e2 is unsound because it fails to account for possible abortion or nontermination \nin e,. For example, it could be used to show (1/0; 17) = 17 which is clearly untrue because the erroneous \ndivision by zero occurs in the left side of the equation but not the right. Since a pure expression is \nindependent of the state, its effect must be the same abortion, nontermination, or no effect -every \ntime it is evaluated. Similarly, the value of a pure expression, if it has a value, must be the same \nevery time it is evaluated. In short, a pure expression may be evaluated any number of times, provided \nit is evaluated atleast once; The two rules P1 (pure idempotence) r~e-s r 1-pure e r~e=(e; e) and P2 \n(pure reordering) r 1- (el -41; e2 -S2; es -SS) r ~ pure e, r ~ let xl= e,; x2:, e2ine~ni = let x2 :: \ne2; xl Z=e, in e3 ni capture this behavior. N ote that only one of two expres\u00ad sions being reordered \nneeds to be pure. In the rules below, a hypothesis of the form r ~ pure e appears whenever the rule \ns conclusion involves moving or duplicating (but not entirely eliminating) subexpression e. Purity Rules \nBelow are the rules by which one deduces purity of expressions. One of the attractive points of Russell \nis that there is a large class of syntactically identifiable pure expressions. The following two rules \nillustrate this. Intui\u00adtively, they state that any identifier is pure and that any type-correct Russell \nexpression that imports no var identifiers and does not produce a var result is pure. P3 (identifier \npurity) r} x-s r ~ pure x P4 (value purity) rval~ e-valT r ~ pure e Rule P4 is surprisingly powerful. \nAfthough it requires a proof of pure e to begin in an environment without var typings, it does not prohibit \nintroduction of var typings in proofs of the subgoals. Thus, the rule can be used to prove that an expression \nis pure even if the expression allocates variables, provided it does not refer to variables from the \nsurrounding context. P5 (substitution purity) r~e, =e2-S r ~ pure e, r 1- pure e2 This rule allows us \nto prove an expression is pure even if it violates the simple syntactic characterization of purity given \nin the previous two rules, by proving it equivalent to another expression known to be pure. In addition \nto the above rules, there are rules that allow us to deduce purity of composite expressions. These rules \ncan be found in [Demers82]. For the most part they are straightforward they merely state that composite \nexpressions are pure if their components are, and that function type expressions and function constructions \nare pure unconditionally, For applications the purity rule is more subtle, however. P12 (application \npurity) r t-- (..., ei -Val l i, ...) r l-- f(..., ei, ...) -val T r * (pure f, .... pure q, ...) r f \npure f(..., ~i,...) This rule states that an application is pure if the function and argument expressions \nare pure and neither the urgu\u00ad ments noT the result are variables. Although this rule may seem weak at \nfirst glance, it is the strongest rule one could hope for in genera!. Applications that take var argu\u00ad \nments or produce var results may be impure even though all their subexpressions are pure value extraction \nand variable allocation, both of which are obviously impure, are simple examples of this. Applications \nof certain builtin functions may be pure even though they violate the rather stringent hypotheses of \nrule P12 above. For these functions there are special application purity rules. Some of these rules appear \n below; the remaining ones may be found in [Demers82]. Null Effect llules We have argued that our rules \ncannot allow a pure expression to be discarded even if its value is never used. However, there are a \nnumber of Russell constructs whose evaluation is guaranteed to terminate without error or observable \neffect, and such expressions can safely be dis\u00adcarded when their values are unused. For each such expression \ne there is a special null eflect rule of the form (null effect rule scheme) r ~ (e,;et) -S r ~ (cl; e2) \n= ez The most general of these rules is P13 (identifier null effect) rR-(x; e)-S r~(x; e)=e which allows \nus to discard evaluation of an identifier if the value is not subsequently used. There are also null \nef?ect rules for function types and function constructions: P14 (func type null effect) r * (func[...; \nXi:Si; ...] S; e ) -S r ~ (func[...; Xi:Si; ...] S; e ) ~ e P15 (func construction null effect) r ~ (func \nf[...; Xi:si; ...]{e}. e ) -S r ~ (func f[...; Xi:Si; ...]{e}. e ) S e These rules state that building \na function type or a func\u00adtion value in Russell has no effect. Of course, they say nothing about the \neffect of subsequently applying the function there can be no general null effect rule for applications, \nsince this depends on the details of the func\u00adtion body. A number of the builtin functions are guaranteed \nto terminate without error or effect if their arguments do. There is a separate null effect rule for \neach such function. There are no specific rules for proving that a compo\u00adsite expression has no effect \nif its components have none; however, such facts are provable in the system. Signature Legality Rules \nEarlier we remarked informally that a signature var e or val e is legal only if e is a type expression \nwhose value is independent of the store. We now have the mechanism to make this remark formal: S1 (signature \nlegality) r E T-val type r j-pure T r ~ (legal var T, legal val T) Recall that evaluation of a pure expression \ndoes not necessarily terminate without error. Thus, for example, the above rule allows a legal signature \nto contain a non\u00adterminating type expression. This is generally acceptable, We could easily eliminate \nit by adding to the rule the additional hypothesis r 1-(T; True) = True which precludes nontermination \nor abortion of T, This approach is taken in the discussion of records below, aa it simplifies the presentation \nof the rules. Composite Expressions Earlier we described the rules by which we may deduce that a composite \nexpression is pure. We shall also require .signakwe rules for composite expressions, which allow us to \ndeduce that expression e is type-correct by proving a formula of the form e -S. Finally, we shall require \nrules allowing us to deduce equivalences between composite expressions. The most important of the compo\u00adsite \nexpression rules are discussed below; the remaining ones can be found in [Demers82], The simplest signature \nrule is the one for sequential composition: S2 (; signature) r~e,-sl .. I Ken-S, r } (el; .... e,) \nS, Basically, this rule states that a composite expression is type-correct if all its subex pressions \nare. Most of the other signature rules are similar, and may be found in [Demers82]. The signature rules \nfor applications and Ie% blocks are more interesting, however, as they embody the signature calculus \nrules of [Boehm80]. The signature rule for function applications is aa fol\u00adlows: S3 (application signature) \nr~f Vd func[...; X,,S,; ...] S r + (..., q -Si[...CXjX],..], ...) r ~ legal S[...eXjX]...] r ~ f[..., \nei, ...] -S[...eXjX]...] The subgoals and conclusion of this rule involve simul\u00adtaneous substitution \nof the argument expressions q for the corresponding parameter names xi. These substitutions enable the \nrule to deal with type-valued parameters. For example, let p be the polymorphic identity function func \np[t:val type; x:val t] {x} so that P -val func[t:val type; x:val t] val t For the (type-correct) application \np~nt, 17] The second and third subgoals of the function signature rule are instantiated as r ~ int N \nval type~rrt,17/t,x] I ~ 17 -val t@t,17/t,x] r ~ legal val t~nt,17/t,x] or, equivalently, r l-- int -va! \ntype r~17 -val int r ~ Iegal val int These allow the conclusion r l-- pfint,17] -val int a-s desired. \nThe signature rule for let-blocks can be developed from the application signature rule. Following Landin \ns (principle of correspondence \\Landin66], we expect the expression Iet .... xi:Si ==~; ... in d ni to \nbe type-correct iff the application f [ .,., q, ... ] is type-correct, where f has signature func [ .... \nX,:Si; ...] S and S is the signature of the block body d. The rule S4 (let signature) (r,...xsis)...) \n1-d -s r K (..., % -Si[...t3XjX],..], ...) r ~ legal S[...ex;x]...] r ~ let .... xi:Si =. q; .. in d \nni -S[,,,eJ/xJ,,.] follows quite directly. The remaining composite expression rules are those that allow \nus to deduce equivalences. A few of these rules are described below; a complete list appears in the [Demers82]. \nThe most interesting and, for our purposes, the most important of the rules are the ones dealing with \nlet-blocks, function applications and conditionals. Most of the let rules are used to rename bound identifiers, \nto rearrange the components of a let-block, or to combine a block with an adjacent expression, For example, \nthe rule C6 (let absorption (right)) r l-- let .... xi .= di; ... in el ni ---S1 (r/...Xi...) j-ez -S2 \nr,Q R let .... xi ==di; ... in e, ni; e2 = let .... xi =, di; ... in el; e2 ni allows an expression e2 \nto be absorbed into a let-block from the right. The requirement that e2 can be typed with an environment \nfrom which the new identifiers have been eliminated ensures that capture cannot occur. The following \ntwo rules allow let-blocks to be intro\u00adduced and eliminated. C8 (let elimination (a)) r~letx==dineni-S \nrjx~e-S r~letx:= dine ni=(d; e) This rule states that a binding that is never referred to may be eliminated. \n C9 (let elimination (b)) rt--- s-s I +letx.. einxni=e In neither of these rules is there any requirement \nthat the subexpressions be pure. The most complex of the let rules is the following substitution rule: \nC1O (let substitution) r l--let .... xi:Si S*di; ...in e ni N S .. (r,...,xSjS)...) * let .... Xi:Si \n=: di; ...in gj ni s let .... xi:Si :. di; ...in hj ni (r,...,xi~S)...) 1-pure gjj pure hj ... (f ,...,xi~S)U{ \n.,gj~hj,g}~hj,...} l--e ~ e r l--let .... xi:Si .= di; ...in e ni s let .... xi:Si ==di; ...in ef ni \nWhen the number of ~-hi pairs is zero, this rule specializes to the obviously sound rule: C1O. 1 (let \nequivalence) r ~ let .... xi:Si .: di; ...in e ni -S (r,...,XSiS)...) ~ e s e r f-let .... xi:Si =. di; \n...in e ni = let .... xi:Si ==di; ...in e ni Informally, ~ = hi can be used as a predicate describing \nthe bound values; so the general form of the rule allows us to use knowledge of the bound values in proving \nthe equivalence of e and e . For example, letting g, be xl and hl be dl, we can derive the following \nextremely useful rule: C1O.2 (pure /3-conversion) r 1-let XI:SI ., d,; ...in e ni -S r ~ pure d, r f \nlet X1:S1 = d,; ...in e ni = let X1:S, =, d,; ...in e[dl/x,] ni provided no capture occurs. This derived \nrule is related to the ,tkonversion rule of the lambda calculus. Two important rules give the meaning \nof function application: C18 (application argument evaluation) r 1- d[..., q, ...] -S r ~ d[..., ei, \n...] 1 -let x zsd; .... y, .. e,; . in x[..., K, ... ni This rule states that argument evaluation may \nbe done by let-binding; thus, the rule implies that parameters are passed by value, and that argument \nevaluation proceeds from left to right. C19 (application meaning) r h (func p[...; Xi:Si; ...]{e})[ .... \nq, ...] _ S r /-(func p[...; Xi:Si; ...]{e})[ .... e;, ...] = let p :: (func p[...; Xi:Si; ...]{e}). \n.... Xi ==e,; ... in e ni ni The binding for p in the right hand side of the conclusion is necessary \nin case e calls p recursively. If there are no recursive calls, this binding can be eliminated using \nthe let-elimination rule (C8) and the func construction null effect rule (P15). Another important claw \nof rules define conditional expressions. These include a signature rule, absorption rules analogous to \nthe let absorption rules above, and a condition evaluation rule similar to application argument evaluation. \nThere are also three if elimination rules: C13 (if elimination (true)) I j- if True then ezelse e$fi \n-S r ~ if True then ez else ea fi = ez C14 (if elimination (false)) r \\ if False then e2 efee e~ fi ---S \nr K if False then e2 efae ej fi = ei C15 (if elimination (parallel)) r K ifelthen e2elsee2fi-S r f if \nel then e2 efee ez 6 = (e,; e2) The most complex of the if rules is: C16 (if substitution) r ~ ifelthen \ne2eLsee~fi-S .r+ pure e, N{el=True} f e2= dz IIJ{el=False} f es= d~ r 1-if e, then .2 else es 6 = if \ne, then d2 elee d~ fi Like let substitution (C1O), this rule introduces new equivalences into the environments \nof its subgoals, making it possible to use knowledge of the truth or falsity of the condition in proving \nproperties of subexpressions e, and ez. As we illustrate below, the if substitution and parallel if elimination \nrules can be used together to perform case analysis in the system. The Semantics of Variables In Russell, \nthe operations that manipulate variables are generally components of data types. Each builtin type T \nincludes the components: := : func[var T; val T] val T ValOf : func[var T] val T New :func[ ]var T These \noperations perform assignment, value extraction and allocation for variables of type T. The rules below \nspecify precisely how they behave. We give the rules for the builtin type int, but essentially the same \nrules hold for all builtin types, including composite types such as arrays and records. Similar rules \nare expected to hold for user-defined types as WCII. However, one must be some\u00adwhat careful here. The \nfact that a user-defined type hap\u00adpens to have a New operation with the right signature does not mean \nthat it will necessarily obey these rules \u00adthere is nothing in the name of an operation that prescribes \nits semantics. Arw ~nment and Value Extraction The first step in specifying the semantics of variables \nis to give rules relating the effects and values produced by assignment and value extraction. These include \nVI (:= value (left)) r t-- (el := e2) -val int r f pure e, r f (.l := e2)= (e, := e,; ValOf[e,]) V2 (:= \nvalue (right)) r 1- (e, := e2) -val int r 1- (pure el, pure ez) r ~ (e, := e2) = (e, := et; ez) These \ntwo rules give the value of assignment expressions: the value produced is the value of the right hand \nside and the value of the right hand side becomes the value pro\u00adduced by ValOf after the assignment. \nP16 (ValOf null effect) r~x -var int rEe-S r 1- (ValOf[x]; e) = e This rule gives the effect of ValOf \n-it has none. V3 (:= annihilation) r t-- (e:=el) -val int r ~ (e:=ez) -val int r l-- (pure e, pure ez) \nr ~ (e:=el; e:=ez) a (e,; e:=e2) V4 (self :=) r ~ (e:=ValOf[e]) -val int r ~ pure e r f e:= VidOf[e] \n= ValOf[e] These two rules give the effect of assignment. Rule V3 states that if two successive assignments \nare made to the same variable, only the second one can be noticed. The pure e2\u00b0 premise to the rule \nguarantees that the value of ez does not depend on the prior value of the vari\u00ad able. Rule V4 states \nthat assigning the value of a variable to itself has no effect. Aliasing The next step in developing \na specification of vari\u00adables is to give rules that describe when variables share storage. To do so in \nan equational system we must add several new builtin functions to the programming language: alias, contains, \ndisjoint: val func[Tl,Tz:val type] val func[x:var T1, y:val T2] val bool Informally, alias[T1,TJ[e1,e2] \nis true if the expressions e, and e2 evaluate to the same variable; contains[T1,T2] [e,,e2] is true if \nthe expression el evaluates to a variable that contains e2 as a component, so that changing the contents \nof variable e, will change the contents of e2, while chang\u00ading the contents of e2 will change a component \nof the con\u00adtents of el. Finally, disjoint[T,,T2] [el,e2] is true if no changes to the contents of e, \nwill affect the contents of e2 and vice versa. Formally, these functions are related by inference rules \ngiven below. . As mentioned earlier, we feel it is important that our attempt to develop an equational \nspecification of Russell pointed out areas in which the language needed to be made more expressive. The \nalias, contains and disjoint predicates were introduced to make an equational descrip\u00adtion of the language \npossible; but after some consideration they began to seem natural and potentially quite useful in ordinary \nprogramming. For example, a program that manipulates large arrays can use them to test whether an efficient \nin-place algorithm can be used or copying is required. The following inference rules define a!ias, contains \nand disjoint: V5 (alias definition) r l--(el -var Tl, ez -var T2) r h (pure el, pure e2) r 1-alias[T1,T2][e1, \ne2] ~ (crmtaicss[T1,T2] [e1,e2] &#38; contains[Tz,TJ[ez,eJ) V6 (disjoint definition) r ~ (e, -var T,, \ne2-var T2) r t-(pure el, pure e2) r ~ disjoint[T1,T2] [e1,e2] = (-wontains[T,,T2 ][e1,e2] &#38; ~contains[T,,Tl] \n[e2,eJ) These two rules define alias and disjoint in terms of con\u00adtains: two variables alias if each \ncontains the other, and two variables are disjoint if neither contains the other. P17 (contains purity) \nr + contains[T1,T2] [e1,e2] -val bool r E (pure e,, pure ej) r F pure eontains[Tl!Tz] [el,e.J P18 (contains \nnull effect) r ~ (contains[T1,T$] [xl,x2]; e) -S r F (contains[T1,T2 ][xl,x.J; e) e (T,; T,; e) V7 (contains \ntransitivity) r 1- (x, -var T,, X2-var T2, X8-var TJ r F-( ( contains [T1,T.J[x I,x2] A contains[T2,T3] \n[x2,x3] ) + contains [T1,Ta][x ,,x3] ) s (Tl; T,; T$; True) V8 (contains reflexivity) r~x -varT r,~ \n~ contains[T,T][x,x] = (T; True) V9 (nonoverlap) r t- (..., xi -Var Ti)...) r f ((contains[T1,TO] [xl,xo] \nA contains[T2,TO] [x2,xO]) + (contains[T1,T2] [X2,Xl])) [x1,x2] V cOntains[T2,T11-(To; T,; T2; True) \nV1O (acyclic) r f (Xl --var T, X2-var T) r 1- contains[Tl,T2][x ,,x2] ~ contains [T2,Tl][x2,x1] These \nrules give the essential properties of contains. Rule P17 states that an application of contains is independent \nof the store if its arguments are; similarly, by rule P18 an application of contains has no observable \neffect if the argu\u00adment evaluations have none. Rules VT and V8 state the obvious facts that containment \nis transitive and reflexive. Rule V9 states that variables do not overlap -if two vari\u00adables contain \nanything in common, then one of the two variabIes must contain the other. Finally, rule V1O is a well-foundedness \nrequirement, stating that a variable can\u00adnot be properly contained in another variable of the same type. \nThe following three rules give the semantic import of the alias, contains and disjoint tests in the programming \nlanguage. Vll (alias equivalence) r ~ (e, -var Tl, ez-var Tz) r t- (pure el, pure e2) r ~ alias[T1,T.J[el, \nez] = True r~el=ez This rule states that two variables that alias are com\u00ad pletely indistinguishable. \nNote the hypotheses pure e, and pure ez are essential and are not implied by purity of [alias[T1,TJ [e1,e2] \n el and e2 might be impure but have effects that exactly cancel one another. For example, one can prove \nthat alias[int,int] [(x:= ValOf[x]+ 1; y),(x:=ValOf[x]-l; y)] is pure and equivalent to True even though \nthe sub expressions x := ValOf[x] f 1 are obviously impure. V3a (:= annihilation (generalized)) r ~ (x:=el) \nw val T, r ~ (e:=ez) -val Tz r ~ contains[T2,T1] [e,x] = True r 1- (pure e, pure e2) r l-- (x:=el; e:=ez) \n= (el; e:=e2) This rule states that an assignment to variable e hides the effect of a previous assignment \nto x if e contains x. As in the annihilation rule given earlier, the purity hypotheses are necessary \nto ensure that neither e nor e2 depends on the previmrs value of x. V12 (reordering) r &#38; (..., Xi \n-var Ti,...) r &#38; (..., Yj var Uj,...) r I- (..., dSjOint[Ti,uj][x;,yj],...) (r I ...xi...) + ~1--s, \n(r I ...yj...) E e2 -S2 r~ea-sa r ~ let t, :: e,; tz ==ez; in e3 ni = let t2 ==e2; t, =: e,; in ea ni \n This rule allows us to conclude that the effects of two expressions do not interfere with one another, \nso that the expressions may be evaluated in either order. The hypotheses require that the free var identifiers \nof el are among xl through Xn, the free var identifiers of et are among y, through y=, and all x s are \ndisjoint from all y s. There is no requirement that the x s or y s themselves be pairwise disjoint, as \nthis is not necessary to show that el and ez do not interfere. Variable Allocation We can now give the \nrules that define the allocation function for variables of type int: V13 (New disjoint) r~e -varT r ~ \ndisjoint~nt,T][ New[ ],e] a (T; e; True) P17 (New null effect) rt-- e-s r + (Newl]; e)=e V14 (New inaccessible) \nrt-- e-s r K New[ ]:=e a e These three rules give the important properties of the value and effect of \nNew a new variable is disjoint from any other variable, and allocating or even assigning to a new variable \nhas no observable effect. There is no rule stating that New is pure it is not pure, since it produces \na new variable each time it is invoked. However, as noted above, it may be possible in the system to \nprove that an expression is pure even if it creates new variables, pro\u00advided it does not use any variables \nfrom the surrounding context. For example, one can use the value purity rule (P4) to show that the expression \nlet x: var int =. New[ ] in x:=17 ni is pure; and, with some manipulation, one can use the New inaccessible \nrule (V14) to show that it is equivalent to the constant 17. Note that the above rules leave many things \nunsaid about variables in Russell. They do not specify an imple\u00admentation, but only the observable characteristics \nof vari\u00adables in programs. Variable allocation may be done using a stack or a heap; the definition of \nNew is insensitive to such implementation decisions. The operations of assign\u00adment or value extraction \ncould have benevolent side effects [Hoare72] like garbage collection; the only obliga\u00adtion is that after \nan assignment the only variable to yield a new value is the one to which the assignment was made. Two \nExamples To demonstrate the use of our equational system, we give two substantial examples. A Derived \nRule Our first example illustrates how case analysis can be performed within the system. We shall prove \nthe follow\u00ading derived inference rule: r R (x,:=el -val T, xz:=ez ~ val T) r t-(pure el, pure e2) r ~ \n(T; True) = True r F (xl:=el; x2:=e2; ValOf [xl] ~ (x,:=el; x,:=e,; if disjoint[T,T] [xl,x2] then el \nelse e2 6 Intuitively this rule is true because two variables of the same type are either disjoint or \nalias they cannot over\u00adlap, nor can one of them properly contain the other. The proof of this derived \nrule proceeds as follows. Using disjoint detlnition (V6), contains null effect (P18), and if elimination \n(C15), we show xl:=el; x2:=e2; ValOf [xl] = disjoint[T,T][xl; X2]; xl:=e,; x2:=e2; ValOf[xl] = if disjoint[T,T][xl; \nX2] then xl:=el; x2:=e2; ValOf [xl] else x ,:=el; x2:=e2; ValOf[x ~] 6 Next, using if substitution (C16) \nwith subgoals rU{disjoint[T,T][x ~,x@True} ~ (xl:=.,; x,:=e,; ValOf[xl]) -(x,:=el; x,:=e,; e,) rU{disjoint[T;r][x \n~,x2]=False} ~ (x,:=el; x,:=e,; ValOf[xl]) = (xl:=el; x,:=e,; e,) (which are discussed below) we obtain \n= if disjoint [T,T][x ~; X2] then xl:=.,; x2:=e2; e, else xl:=el; x2:=e2; e2 6 from which we can obtain \nthe desired result using if absorption rules. The first of the two subgoals is straightforward, since \nthe assumption that xl and X2 are disjoint allows the assignment expressions to be reordered. The second \nsubgoal is a bit more complicated. Using disjoint definition (V6) and the hypothesis that disjoint[T,T][x,, \nx2] = False we obtain, after some manipulation, contains [T, T][xl,x2] V contains[T,T] [x2,x1] = True \nUsing contains acyclic (V1O) we can show the two dis\u00adjuncts on the left hand side are equivalent. After \nmore manipulation this leads to (contains[T,T][xl,x2] A contains[T,T][x2,xl]) = alias [T, T][xl,x2] a \nTrue Now that xl and x2 have been shown to ali as, the desired result follows directly. A Simple Procedure \nAs a second example of the use of our system, we consider a theorem presented in [Cartwright78]. Let \nSWAP represent the Russell expression func Swap[yl,yz: var int] val int { let t ., New[ ] in t := Va10f[y2]; \nY* := ValOf[yl]; y, := ValOf[t] ni } which is a standard function for swapping the values of two variables. \nWe would like to show that swap works correctly whether or not xl and Xt are disjoint roughly speaking, \nwe would like to prove something analogous to the Hoare-style correctness formula {x,=al A x,=a,} swap[x,,xz] \n{x,=al A XI=%} Since the formulas of our system are equivalences between programming language expressions, \nwe cannot even state precisely this formula. What we cars show is that for any r from which there are \ndeductions r ~ (x, -var int, X2 -var int) r ~ (el -val int, e2 -val int) r t-(pure q, pure e2) r ~ swap \n= SWAP there is also a deduction r ~ (xl:=el; x2:=e2; swap[xl, X2]) = (x2:=e1; x1:=e2) This equivalence \nformula appears similar to the Hoare formula, and a proof of it should convince the reader that calling \nswap actually does exchange the values of the argumcllts. However, the two formulas really are techni\u00adcally \nquite different. For example, if xl and x2 are aliaaed, the Hoare formula {x,=11 A x,=17} swap[xl,x2] \n{X2=11 A X1=17} holds because the precondition cannot possibly be satisfied; the equivalence (x,:=11; \nX2:=17; SWap[X,, X2]) ~ (X$=ll; X1:=17) holds because 17 is stored over 11 on both sides. l he similar \nequivalence (x,:=11; X2:=17; swap[xl, x,]) = (xl:= 17; xz:=ll) does not hold when xl and X2 are aliased. \nThe proof (of the true equivalence) proceeds as fol\u00adlows. Using substitution we can show the left hand \nside of the desired result is equivalent to (xl:=el; xt:=e2; SWAP[X,, X2]) By application meaning (C19) \nthis is equivalent to (xl:=el; let y, ==x,; y, ==x2 in (body of SWAP) ... ni Using the derived /3-conversion \nrule (C1O.2), let elimina\u00adtion ~C8) and identifier null effect (P13) yl and y2 can be eliminated to yield \nxl:=el; x2:=e2; let t .= New[ ] in t:= ValOf [x2]; x2:=Va10f [xl]; x2:= Va10f[t] ni through T. respectively. \nWe shall use the signature rule Now let motion can be used to convert this to let t =.New[ ]in xl:=el; \nx2:=e~; t:=ValOf[xz]; xz:=ValOf [xl]; xl:=ValOf[t] ni (A use of let renaming may be required hereto avoid \ncap\u00adture of free variables of e.) At this point we invoke the let substitution rule in its full generality. \nWe first prove the subgoal let t ,= New[ ] in disjoint ~nt,int] [t,xi] ni = let t ==New[ ] in True ni \nwhich is straightforward; we then prove (r,t var int)U{disjoint~nt,int] [t,xi]=True} 1\u00ad(x,:-e,; X2:==~2;t:=ValOf \n[x2]; x2:= Va10f{xl]; xl:= ValOf[t]) = (t:=e2; x2:=e1; x,:=e2) , This subgoal ismoresubstantial than \nthe first. It is fairly straightforward to transform the left side to (t:=ez; xl:=el; x,:=e,; x2:=Va10f[x \nl]; xl:=ez) At this point we perform a case analysis. Just a-s in the previous example, weobtain onecase \nin which disjoint[T,T][xl,x2] s True and another case in which a1ias[T,T][xl,x2] = True The first case \ncan be dealt with easily by reordering the assignments to xl and X2. The second case can be handled just \nas easily by using := annihilation (V3) to eliminate the unwanted assignments. From the two major subgoals \nabove, let substitution (C1O) allows us to conclude that the original expression is equivalent to let \nt ==New[ ] in t:=e2; xz:=el; x1:=e2 ni This is easily transformed to let t Z=New[ ] in t:=e2 ni; x2:=el; \nxl:=ez from which the let block can be eliminated using argu\u00ad ment evaluation (C18) and New inaccessible \n(V14). This gives us the desired equivalence. Structured Variables The set of operations used above for \nsimple variables forms the basis of the characterization of composite vari\u00adables like records or arrays. \nBelow we present the definition of records in Russell; arrays can be treated by a similar approach. A \nRussell expression for a record type has the form record ( fl:T1; .... f.:Tu ) This is a slight simplification \nof the actual Russell syntax in particular, recursively defined types are not allowed but it will sufllce \nfor our purposes. Such an expression denotes a record type with fields fl through f, of types T, S9 (record \ntype signature) ... I_ ~ legal val Ti r f (Ti; True) s True . r ~ record (...; fi:Ti; ...) N val type \nThe hypotheses of this rule demand that the expressions for the types of the fields (Tl through T,) be \npure and free of side effects. While not strictly necessary, this restric\u00adtion considerably simplifies \nthe rules below; without it the conclusions of the rules would have to contain additional occurrences \nof the Ti with no purpose other than to ensure soundness of the rules in the face of erroneous or nonter\u00adminating \ntype expressions. P18 (record type purity) r f record (...; fi:Ti; ...) -val type r R pure record (...; \nfi:Ti; ...) P19 (record type null effect) r \\ record (...; fi:Ti; ...) -val type r ~ (record (...; fi:Ti; \n...). e) s e These are stand ard purity and null effect rules, reflecting the fact that constructing \na record type value is indepen\u00ad dent of the store and terminates without error. For this section, we \nlet RR =~cf record (...; fi:Ti; ...) be a fixed record type expression. Associated with RR are the usual \noperations of assignment, value extraction and variable allocation: := : func[var RR; val RR] val RR \nValOf : func[var RR] val RR New : func[ ] var RR In addition, there are constructor and field selector \nfunc\u00adtions: Mk : func[val Tl; .... val Tn] val RR f~a : func[val RR] val TI fr : func[var RR] var TI \n. f~i : func[val RR] val T. f~ : func[var RR] var T, In Russell, function names can be overloaded two \nor more fundt.ions can have the same name as long % they have different types so they can be disambiguated. \nThus, the identifying superscripts var and val on the field names are technically unnecessary. We retain \nthem below for clarity. To specify the meaning of the record type construc\u00adtor, we must give meanings \nfor each of the above opera\u00adtions. These meanings are defined in terms of the opera\u00adtions for the types \nTi. In the rules that follow, we use the notation Ti$ :=, Ti$ValOf and Ti$New for the assignment, value \nextraction and allocation functions for variables of type Ti. Construction and field seleetion for record \nvalues are described by two simple rules: V14 (record val selection) r h (..., xi -val Ti, ...) r ~ f~ \n[Mk[..., Xj,...]] = xi V15 (record val construction) r~e -valRR r ~ pure e r 1-Mk[..., f~ [e], ...] = \ne These are the simple McCarthy rules for the LISP con\u00adstructors [McCmthy60] restated in Russell terms. \nThe additional rule V16 (record var selection) rt--e -var RR r + pure e r ~ ValOf[e] ~ Mk[..., Ti$ValOf[f~[e]], \n...] shows how the var and val field selection functions relate to one another. Note that the identity \nf~l[ValOf[e]] G Ti$ValOf[fY[e]] follows immediately from rules V14 and V16. The next few rules describe \nthe aliasing behavior of the fields of record variables. V17 (field new) r j-f~[New[ ]] -Ti$NeW[ ] This \nrule states that a field selected from a new record variable is indistinguishable from a new variable \nof the field type. Thus, by new disjoint (V13), the fields of new records are disjoint from all existing \nvariables. V18 (field disjoint) r ~ ew varRR r l--pure e r ~ disjoint[Ti,Tj] [f~[e],f~[e]]~ True for \nall i#j. This rule states that the fields of a record variable are disjoint from one another. Note this \nfact is not a consequence of the preceding rule. V19 (field containment (a)) r ~ (e, -var U,e2 C-var \nRR) r l--(pure el, pure ez) r + contains[U,RR][el, e2] = . A contains[UjTi][el, f~[ez]] A This rule \nstates that a record variable contains all of its fields, and that it contains nothing else any variable \nthat contains all the fields contains the record. V20 (field containment (b)) r + (el -var RR,e2 -var \nU) r 1-(pure e,, pure ez) r ~ contains [RR ,U][e1,e2] = alias [RR,U] [el,ez] V ,,. v contains[Ti, U][f~[el], \nez] V ... This rule states that there is nothing between a record and its fields a variable that is \ncontained in a record either aliases with the entire record or is contained in one of the fields. Finally, \nthere are purity and null effect rules for the val and var field selector functions. The var rules P22 \n(field purity (var)) r k f~[e] -var Ti r ~ pure e r 1---pure f~[e] P~3 (field ,Iull effect (y~r)) r 1-(f~[e]; \ng) -S r t-(fi %i d = (e; d may appear strange at first glance, but they are both sound and reasonable \n even though a var field selector function is passed a variable as an argument, it neither examines nor \nmodifies the value possessed by that vari\u00adable. As an extended example, we illustrate that the obvi\u00adously \nsound rule I ~ X:=C.-RR r } pure e r t--x:=e = Mfi[,,., f~[x]:=ffl [e], ...] can be derived from the \nabove rules. Beginning from x := e, we first use field containment (V19) and := annihilation (V3) to \nshow x:=e ~ (...; f~ r[x]:=f~t[e]; ...). x:=e we then use record val construction (V15) to obtain ~ (...; \nf~[x]:=f~l[c]; ...). x:= Mk[..., f~][e], ...] By argument evaluation (C18) this becomes G (...; f~[x]:=f~i[e]; \n...). let .... ti =, f~ [e]; ... in Mk[,.., ti, ,..] ni Using let motion rules note that e is pure \nand thus com\u00admutes with any expression we obtain -let tl ==f~ [e]; .... t~.l .= f~~~[e]; t, ==(...; \nf~[x]:=f~ [e]; .... f~ [e]) in h{k[..., tij ...] ni It is easily shown that (f~[x] := f~i[e]; f~ [e]) \n= (f~[x] := f~ [e]) Using this fact and let motion n times we can reduce the above expression to == let \n.... ti ==(f~[x] :== f~ [e]); . in Mk[..., ti) ...] ni which by argument evaluation (C18) finally reduces \nto Mk[ ,,,; (f~[x] := f~a [e]); ... ] as desired Conclusions In this paper we have presented an equational \ntheory for Russell and argued that It presents a useful rnathematica/ specification of vari\u00adables in \nRussell, and This sort of equational theory can be construed to cover all of even a very rich language \nlike Russell. The equational logic presented here is certainly not a complete replacement for a conventional \nHoare-style sys\u00adtem. Most importantly, there are no induction principles to allow proofs of general properties \nof program correct\u00adness. Theorems in this system can give the final results of program execution for \nparticular inputs; but statements such as for any integer n, the program computes n! are outside its \nscope. On the other hand, the fact that it is ahoays possible to deduce the result of a terminating com\u00adputation \nin the system argues that it gives a complete specification of variables; and this form of specification \nis a particularly pithy way to get across important proper\u00adties of the programming language to potential \nusers. In particular, there are components of the Russell equational theory that present the language \nbetter than any of our attempts at informal description. When writing the infor\u00admal description of Russell \nand explaining the language to others, we often found that describing New as allocating a new variable \nwas inadequate. It is far more edifying to give inference rules such as I ~e-var T r ~ disjoint~nt,T] \n[int$New [ ],e] + (T; e; True) and r p (New[l; e)=e which say very clearly that New produces a result \nthat is disjoint from any other variable, and that applying New has no other effect. In addition, as \nwe noted above, the process of pro\u00adducing the equational theory led us to make some impor\u00adtant changes \nin the language. It was only when we began to work on the theory that we found the need to intro\u00adduce \nalias, contains and disjoint into the programming language. We now consider these operations to be per\u00adfectly \nnatural and potentially quite useful. References [Bockus78] Ba.ckus, J. Can programming be liberated \nfrom the vonNeumann style? A functional style and its alge\u00adbra of programs. CXG A421 (1978), pp. 612-641. \n[Bates82] Bates, J. and R. Constable. The definition of pprl. TR-82-492, Department of Computer Science, \nCornell University, 1982. [Boehm80] Boehm, H., A. Demers and J. Donahue. An informal description of Russell. \nTR-80-430, Department of Computer Science, Cornell University, 1980. [Boehm82] Boehm, H. A logic for \nexpressions with side-effects. Proceedings Ninth Symposium on Principles of Pro\u00adgramming Languages (1982), \npp. 268-280. [Cartwright78] Cartwright, R. and D. Oppen. Unrestricted pro\u00adcedure calls in Hoare s logic. \nProceedings I ijth Sym\u00adposium on Principles oj Programming Languages (1978), pp. 131-140. [deBakker80] \nJ. deBakker. Mathematical Theory oj Program Correctness. Prentice-Hall, 1980. [Demers80] Demers, A. and \nJ. Donahue. Type completeness as a language principle. Proceedings Seventh Symposium on Principles of \nProgramming Languages (1980), pp. 234-244. [Demers82] Demers, A. and J. Donahue. An equational theory \nfor Russell. TR-82-534, Department of Computer Science, Cornell University, 1982. [Dijkstra72] Dijkstra, \nE. Notes on structured programming. In Dahl, O-J., E.W. Dijkstra and C.A.R. Hoare, Struc\u00adtured Programming, \nAcademic Press, 1972. [Guttag78] Guttag, J. and J. Horning. The algebraic specification of data types, \nActs Injormatica 10 (1978), pp. 27-52. [Hoare72] Hoare, C.A.R. Proofs of correctness of data represen\u00adtations. \nActs in/ormatica 1 (1972), pp. 271-281. [Landin66] Landin, P.J. The next 700 programming languages. C.ACM \n9 (1966). [Martin-Lof79] Constructive mathematics and computer programm\u00ading. Sizth International Congress \nfor Logic, Metho\u00addology and Philosophy oj Science, Hanover, Germany (1979). [McCarthy 60] hlcCarthy, \nJ. Recursive functions of symbolic expressions and their computation by machine, part I. CACM 3 (1960), \npp. 184-195.\n\t\t\t", "proc_id": "567067", "abstract": "One of the fundamental notions of programming, and thus of programming languages, is the variable. Recently, the variable has come under attack. The proponents of \"functional programming\" have argued that variables are at the root of all our problems in programming. They claim that we must rid our languages of all manifestations of the \"vonNeumann bottleneck\" and learn to live in the changeless world of functional combinators.While we may not, believe all the claims of the functional programming advocates, there is evidence that the treatment of variables in most programming languages leaves much to be desired. In this paper we discuss how to make variables \"abstract\", i.e., how to introduce the notion of variable in to a language so that variables have reasonable mathematical properties. This paper includes:--- a discussion of language design principles that allow a more mathematical treatment of variables in programming language, and--- a description of an equational logic for the programming language Russell [Boehm80]. Although this logic follows much of the development of abstract data types (cf. [Guttag78]), it is novel in its treatment of a language in which expressions not only produce values but also can have effects. We discuss the over all structure of the logic and present in detail the rules particularly relevant to the semantics of variables. A complete presentation of the logic appears in [Demers82], and the rule numbers in this paper agree with the numbers used there.In the next section, we discuss the underlying language principles needed to make an equational specification of variables possible. In the sections that follow, we present a logic that does so for Russell.", "authors": [{"name": "Alan Demers", "author_profile_id": "81100529925", "affiliation": "Cornell University, Ithaca, New York", "person_id": "P12363", "email_address": "", "orcid_id": ""}, {"name": "James Donahue", "author_profile_id": "81100145919", "affiliation": "Xerox Palo Alto Research Center, Palo Alto, California", "person_id": "PP43116770", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/567067.567075", "year": "1983", "article_id": "567075", "conference": "POPL", "title": "Making variables abstract: an equational theory for Russell", "url": "http://dl.acm.org/citation.cfm?id=567075"}