{"article_publication_date": "01-24-1983", "fulltext": "\n Permission to make digital or hard copies of part or all of this work or personal or classroom use is \ngranted without fee provided that copies are not made or distributed for profit or commercial advantage \nand that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, \nto post on servers, or to redistribute to lists, requires prior specific permission and/or a fee. &#38;#169; \n1983 ACM 0-89791-090-7 $5.00 the graph by deleting all edges not in S. A description of the path relationships \namong a set of edges S is sufficient to recognize all valid paths with respect to S. Equivalently, a \ndescription of path relationships among S is sufficient to determine whether you can get from one specific \nelement of S to another specific element without going through any other element. We present an algorithm \nin two parts, followed in the appendix by a complete example. Part one takes a graph and, in time related \nto the size of the graph, constructs some tables. Part two takes as input those tables and a set of edges \n(or nodes) and, in time primarily related to the size of the set, produces a regular expression which \ndescribes the paths between the edges (or nodes) in the set. If we restrict the class of graphs to those \nderived from programs using control flow achievable in Pascal programs written without goto s, then the \ntime for part one of the algorithm is linear in the size of the graph and the time taken for part two \nis the size of the set times the log of the size of the set. In the rest of the paper we will refer to \ngraphs which can be so derived as structured graphs. We believe, and hope to show in a later paper, that \nthe presence of a few goto s does not destroy these time relationships. Many of the most fundamental \ngraph algorithms attempt to answer questions about path relations. We will give three examples: 1) the \ndominator tree allows us to tell, rapidly, whether there is a path from the start node of the graph (the \nroot of the dominator tree) to another node u which does not pass through a third node v. If there is \nno path, in the graph, from s to u which does not pass through v, then v dominates u. 2) Various depth-first \nsearch numberings allow us to classify certain kinds of paths. 3) Transitive closure says whether there \nis a path from one node to another. WeJpropose an algorithm capable of describing the paths between any \nset of edges in a given graph. A path between two edges (u,v) and (w,x) is a sequence of edges (v, al), \n(al,a2) ... (an,w), For technical ~ea\u00adsons we will talk about paths between edges, rather than paths \nbetween nodes. Moreover, after a linear amount of preprocessing on just the graph, the algor\u00adithm will \nproduce, for any set of edges, a description of the paths between those edges. For a restricted class \nof graphs, the size of the description is linear in the number, k, of edges in the set, and the description \nwill be produced in time k log(k). Since for the re\u00adstricted class of graphs we can answer all the questions \nthat a general transitive closure algorithm does more rapidly than any known transitive closure algorithm \ndoes, any generalization that maintained the speed advantage would be a major result. We do describe \nways of generalizing our algor\u00adithm so that it works on arbitrary graphs. The author s intuition is that \non graphs arising from typical programs, this generalization will not noticeably affect the time or size \nof description bounds. However, characterizing such graphs, and proving bounds for them is left to a \nlater piece of work. We will describe the application of this algor\u00adithm to global flow analysis. For \na graph in our re\u00adstricted class, the use of our algorithm for most flow analysis problems, such as clef-use \nchaining or constant propagation, results in an algorithm that as the graph grows, asymptotically, is \nconsiderably faster than any previously formulated ones. This is because it does approximately as many \nsimple operations as ordinary flow analysis algorithms do bit vector operations. In standard algorithms \nthe size of the vector grows as the size of the program. Global flow analysis may be thought of as a \nmeans of proving simple theorems about programs[2,3]. We will sketchia general framework forglobal flow \nanalysis. Wearegiven functions which describe the semantics of the program and a set of propositions, \nP, each of which partially describes the state of the machine during execution of the program. The functions, \none for each edge in the graph of the program, map subsets of P to subsets of P. Suppose the program, \nbefore executing node u, is in some state S, and that propositions TcP are true of S. If the execution \npointer changes by moving along edge (u,v) so that node v is about to be executed, then the prop\u00ad ositions \nin ~(uV)(T) will be true of the new state of the program. For example, let P contain the proposi\u00ad tion \nx=5, let u correspond to a node which y: =x+1, then f ~u,vj(P) may contain the proposition that y=6. \nWe usually assume nothing is true at the start node. All flow analysis algorithms (except [1]) work by \nre\u00adpeatedly applying, composing, or intersecting such semantic functions to show that on all paths from \nthe start node to another node certain propositions must be true. Suppose one is interested in determining \nthe variables that must have a certain, constant value along all paths in the program to particular \ninteresting points. This can be valuable in that it allows you to replace a use of the variable with \na use of a constant. First one gathers a set of propositions. For example if the assignment statement \na: =5: appears in the pro\u00adgram, the proposition a=5 should be an element of the set. If the proposition \na= 5 is true along all paths to a use of the variable a , then that use can be replaced with a use of \nthe value 5 . Ordinary flow analysis algorithms would at least do operations such as intersection, composition, \nappli\u00adcation, and union on the semantic functions to deter\u00admine which propositions are true at every \nnode along all paths to the nodes of interest. Union of two func\u00adtions f and g is defined so that ((fug)(T)) \n= (f (T)ug(T)). Intersection is similarly defined. In fact, almost all algorithms known to the author \nwould de\u00adtermine the truth of all propositions at all nodes in the program. Rosen[5] gives an incremental \nalgorithm which does not recalculate all propositions every time the program changes, but must examine \nthe behavior of each function on all propositions at every node. Allen et al. [6] describe a method for \navoiding the calculation of propositions, providing that sections of the program are derived from integrating \nprocedures in-line and providing that flow analysis was done on those procedures. However, any user of \na flow analysis algorithm such as an optimizing compiler is likely to only be interested in the values \nof a given variable at state\u00adments in the program which include references to that variable. If there \nare m propositions and n nodes, traditional algorithms must take time at least m x n, At a given node \none may only be interested in k<m propositions. The sum of the k s for all the nodes may be considerably \nsmaller than m x n. By doing the kinds of processing we will suggest in this paper, one can hope to take \ntime related to the number of facts one is interested in. Here, a fact is whether or not a proposition \nis true on all paths to a given node. Often a semantic function may be defined by a couple of bit vectors, \ncalled the kill and gen vectors[2]. One vector (the gen vector) defines the propositions which are always \ntrue no matter what the inputs to the function are, and the other (the kill vec\u00adtor) defines those which \nare no longer true, the propo\u00adsitions not so defined are true if and only if they are true in the input \nto the function. The above described operations on functions can be performed with a small number of \nbit vector operations. Very few nodes are contained in the set S of nodes which either generate or kill \na given proposition, and to determine the truth of a given proposition at nodes in a set S, one need \nonly examine the path relations between nodes in s u s . Even if more general functions than kill-gen \nfunctions are used, it is to be expected that for most pairs of functions and propositions (derived from \nreal programs) the proposition will be contained in the result of the function if and only if it is contained \nin the input. We can derive a summary for the places we are interested in the value of a proposition \nplus the places a proposition we are interested in can be changed, plus where other propositions become \nde\u00adpendent on it. We can take the transitive closure of dependency of the propositions and form a summary \nof all the dependencies relevant to a given proposition. We then want to determine which propositions \nare true at all points in our summaries. We can do this by forming the fixed point of these propositions, \nas sug\u00adgested by Kildall[ 14], and in time linear in the size of the summaries gathered as above, determine \nthe truth of the propositions we are interested in. The Graph Problem: We will describe a problem on \na restricted class of graphs and one technique for solving it, and then show how to generalize the solution \nfor a larger class of graphs. Consider a directed graph G= (N,E,s,t). N is a set of nodes, E c NxN a \nset of edges, s the start node that dominates all other nodes in the graph, and t the terminating node \nthat is dominated by all nodes in the graph. Paths through the graph may be described as a sequence of \nedges the first of which leaves s, and the last of which enters t. One can create a regular expression \n(using as operators U , + , , composition) which recognizes all paths through the graph. We will later \ndescribe a generalization of a regular expression, a regular DAG. Until then, the reader is advised to \nassume that a regular DAG is the same as a regular expression. Given a certain kind of graph, and a set \nof edges E c E, we will rapidly construct a regular expression (whose size is linear in I E I ) that \nrecog\u00adnizes all strings in E * which are subsequences of paths in G (obtained by deleting all edges not \ncontained in E ). The first part of our process is to form a regular expression over the edges of a graph. \nThis regular expression recognizes all sequences of edges which correspond to a path through the graph. \nThe method for building the regular expression will be described later. We then form the parse tree, \nT, of this regular expression. To be more precise, we place the opera\u00adtors in the regular expression \nat the internal vertices of the parse tree and the operands at the leaves of the parse tree. The leaves \nof the tree, correspond to the edges of the graph. We will attempt to avoid confu\u00ad sion by referring \nto the edges of the graph as edges, and the connection between a parent and a child in a tree as an arc. \nSimilarly thenodesin the graph will be called nodes, and the vertices in the tree will be called vertices. \nThe reader is advised to examine the exam\u00adple graph and regular expression in the following ex\u00adample. \nWe then preprocess the graph by numbering the vertices in the tree T with both prefix and postfix numbers. \nThus, we can quickly determine whether one vertex is the ancestor of another. At each vertex we put in \ntwo pointers, one to the nearest ancestor which corresponds to either the operator u or, the operator \n*; and the other to the nearest ancestor which corresponds to either the operator + or *. In the above \nexample, vertex 5 has a pointer to vertex to show the nearest u and a pointer to vertex 3 for the nearest \n+ (or in this case *). No pointers are needed to nodes labelled with  (composition). We number the edges \nby the position of the leaf which corre\u00adsponds to them in left to right order, and set up the data structures[4] \nso that we can rapidly find the Low\u00adest Common Ancestor (LCA) of two vertices in 0(1) time. Note that \nthis requires that no edge appear more than once as a leaf of the tree. This is all the preprocessing \nwe do on the graph, Let us describe a slow process for obtaining a regular expression containing only \nedges in E . Take the original regular expression, and replace all edges in E-E with lambda, the empty \nstring. Then simplify the expression using such algebraic identities as 1)lambda composed with lambda \nequals lambda and 2)lambda unioned with the union of lambda and a non-empty string, n, equals the union \nof lambda and n. One can make similar identities for * and +. Using these identi\u00adties in the naive way \nwould require scanning the whole tree. This is not necessary To rapidly find a regular expression which \ncor\u00adresponds to just the edges in E cE, we first form a tree T and define a correspondence between vertices \nof T and T . The leaves of T correspond to the leaves of T which are in E . Let u , v , and w be internal \nnodes of T , with v and w corresponding to v and w inT. Let ubetheLCA ofvandwinT. Then u corresponds \nto u if u is the LCA of v and w . If there is not a vertex r which corresponds to the root of T, we add \none. which becomes the root of T . To compute T we make a list of all the edges in E (they are leaves \nof T) sorted by their left to right position in T. We then find an LCA, i, of a pair of elements in the \nlist, (k,l), so that i is lower than the LCA S of the adjacent pairs of elements in the list (eg. (j,k) \nand (l,m)). We replace the elements k and 1 by their LCA, i, in the list, and again try to find the LCA \nS of the adjacent elements in the list. We also make i the par\u00adent of k and 1 in T . After completing \nthis process we have a tree T which roughly corresponds to the parse tree for the regular expression \nwe wish to obtain. We further amend T as follows. If there are tree vertices m,n in T such that m istheparent \nofninT ,andthere isa+or*onthe path from mtoninT,then avertex with a+or*as an operator must be placed \nbetween n and m in T . Similarly if there is a u between n and m, a u must be inserted whose other operand \nis the null path. This may be accomplished without searching the path from m to n in T. We have a pointer \nfrom n to the nearest vertex above it containing one of these operators. We simply test to see whether \nthat nearest vertex is higher or lower than m. This tells us whether there is such a vertix. We are not \ninterested in whether there is more than one. This comple}es the construction of T and the desired regular \nexpression may be obtained direct\u00adly from T . If we wished to summarize edges b,f, and g, then T would \nlook like the following: . A brief sketch of the proof that the above al\u00adgorithm produces the desired \nregular expression fol\u00adlows. All tree vertices (edges in the graph) in T that are not interesting may \nbe replaced by lambda (the null path). A vertex which corresponds to the com\u00ad position of a vertex, n, \nwith lambda may be replaced by n. The union of lambda with the union of lambda and a vertex, n, may be \nreplaced with the union of n with lambda. Similar rules apply with + and *. The algorithm sketched in \nthe two previous paragraphs is merely applying these transformations more rapidlv. The algorithm described \nabove assumed that a regular expression may be obtained describing the graph. For structured graphs (i.e. \nthose obtained from programs written without goto s and using the con\u00adstructs begin-end, repeat-until, \nif-then, if-then-else, and case) one can easily obtain a regular expression. One can form a regular expression \nfor arbitrary graphs; however, this regular expression may be very large, and it may have many large \nsubexpressions which are duplicated and appear several places in the graph. As soon as goto s are introduced, \nit thus be\u00adcomes important to examine the concept of regular directed acyclic graphs (regular DAG s). \nOur choice of the term Regular DAG is more obvious, if one thinks of regular expressions as being synonymous \nwith regular trees. DAG s are more general than trees. If a DAG is also a tree, then the parent of a \nvertex is the predecessor, and a child of a vertex is a successor, However, a vertex may have several \nprede\u00adcessors in a DAG, but only one in a tree. A regular expression is represented by a tree whose leaves \nare operands and whose internal vertices are operators. A regular DAG is represented by a DAG whose vertices \nwith no successors are operands and whose other ver\u00adtices are operators. In a tree, a vertex may have \nonly one parent. In a DAG it may have several predeces\u00adsors. Thus, duplicated subexpressions can be repre\u00adsented \nby the addition of an arc to the data structure, rather than by actually having two copies of the ex\u00adpression. \nFor an arbitrary graph a regular DAG can be obtained whose size is no larger than N3 [3]. For reducible \ngraphs the size of the regular DAG will not exceed E a(E) (where a is the functional inverse of Ackerman \ns function) provided the DAG is construct\u00aded in an optimal manner [10]. For most programs there is reason \nto believe that the size of the graph is linear[2,3]. Tarjan[7,8] has shown that regular DAGs can be \nused for flow analysis. Cocke[9] much earlier realized this. We will not try to obtain a regular expression \nas the summary of arbitrary graphs. Rather we will ob\u00adtain a regular DAG. When our summary was suppos\u00aded \nto be a regular expression we started by represent\u00ading the graph by a regular expression; now we will \nstart out by representing it by a regular DAG for the whole graph. Our technique for summarizing a regular \nDAG is based on our method of summarizing regular expres\u00adsions. Our first step is to find a spanning \ntree for the DAG. We then preprocess the spanning tree as we did for tree T before. We also add to each \nvertex a point\u00ader to its nearest ancestor which has a predecessor not in the spanning tree (i.e. a vertex \nwith two or more parents. ) These additional data structures enable us to handle a small number of non-tree \narcs with a small penalty. To minimize the number of non-tree arcs we will later discuss how a regular \nDAG is generated for a graph. Assume for the moment that the graph is repre\u00ad sented by a regular DAG, \nwhich is basically a tree, with a few non-tree arcs thrown in. By examining only the spanning tree, T, \nof the DAG, we can con\u00adstruct a tree, T , as before. We examine the paths in T from the children of all \nvertices in T to their parent in T . If a vertex v on such a path has a predecessor u in the DAG, but \nwhich is not in the spanning tree, then u and v become interesting and they are added to the set of vertices \nin T . The number of such interesting vertices that are not in the original set is bounded by the number \nof non-tree arcs in the DAG. In graphs the author has examined, this number is related to the number \nof goto s leaving otherwise one\u00adentry, one-exit regions. Ferrante [11] has outlined a plausible proof \nthat this will always be the case. In practice the author expects this number will be small for most \nprograms. For certain graphs, additional benefit can be obtained by creating the dominator tree for the \nDAG and using that tree rather than the spanning tree. The size of the summaries may be smaller, and \nthe time to obtain them less. Since it is more complex to use dominators, and we have no proof that they \nare gener\u00adally better we will not discuss that in this abstract. We now return to the question of obtaining \na regular DAG from a graph, and minimizing the num\u00adber of non-tree arcs. A regular DAG may be obtained \nby applying a sequence of two basic transformations to the graph; each transformation reduces the graph \nand builds up the expressions. We will describe some\u00adwhat less general transformations than are needed \nfor arbitrary g;aphs. However, these transformations are sufficient to build up regular DAGs for reducible \ngraphs[2,3]. Tarjan[8] has suggested that similar transformations will also build regular DAGs. The more \ngeneral transformations[3] needed for flow anal\u00adysis on arbitrary graphs can also be used to build up \nregular expressions. To make a regular expression for a graph, start out by labelling each edge in the \ngraph with a regular expression which recognizes that edge, One transfor\u00admation takes a node, v, such \nthat 1 ) there is an edge (v,v) and 2) there is only one node, u, such that there is an edge (u,v) and \nsuch that u#v. It then deletes the looping edge (v,v). Suppose before the transformation the regular \nexpression for (u,v) was R ~ and the ex\u00ad pression for (v,v) was R ~. Then this transformation also changes \nthe regular expression for (u,v) to be RI R; R, % The second transformation takes three nodes, U,V, \nand w. (u,v) can be the only edge entering v and there is an edge (v,w). The edge (v,w) is deleted; the \nedge (u,w) is added to the graph, if it does not already exists. Assume that (u,v), (v,w) and (u,w) before \nthe transformation have expressions R 1A z, and R 3. Then after the transformation (u,w) will have transforma\u00ad \ntion R 1R2uR3, and (u,v) will remain the same. This is the transformation which causes the problems, \nin that after the transformation R ~ has been duplicated. Eventually, in structured graphs, all these \nduplicates can be eliminated. In non-structured graphs, many can be. In a structured graph, the two duplicated \nR 1 s will eventually be the prefixes of two larger expres\u00adsions, which are unioned later by either transformation \none or two. At that point they may be merged. This follows from the algebraic property of regular expres\u00adsions \nthat: RR u RR = R(R u R ). On the right hand side R appears only once. Whenever a Union is performed \na check is made to see if this merger is possible. Each expression, that is the result of com\u00adposition, \nmay be viewed as the result of a list of com\u00ad positions, Each of the sub-expressions will be either the \nresult of a union, or a leaf. With each composed expression will be kept the number of sub-expression \n that are composed to form it. By virtue of the order that the transformations are applied, in a structured \ngraph, all expressions with a common prefix will have in their representation a sub-tree which corresponds \nto the largest shared prefix. All that is needed is to find that prefix. An algorithm much like a merge \n(in a merge sort) can find this prefix, in time linear in the number of sub-expressions not in the prefix. \nSince all these sub-expressions are made the children of a un\u00adion, they will not be rescanned by the \nmerge-like al\u00adgorithm, and the total time it takes will be linear in the size of the structured graph. \nA similar type of factoring may be applied to change aa* to a+. We suggest the addition of another binary \noperator, % to regular expressions. a(ba)+ = afi. This operator, which roughly corre\u00adsponds to the notion \nof an n-and-a-half loop, is neces\u00adsary in order to change the DAG s resulting from While-do constructs \ninto trees. The changes to the data structures are left as an exercise for the reader. There may be other \nconstructs which can profitably be added to the regular expression. If the factoring is done properly, \nand the algor\u00adithm suggested in [2,3] is used to choose the order in which the transformations are applied, \nthen structured graphs can be transformed into regular expressions, as opposed to regular DAG s, in linear \ntime. For pro\u00adgrams with goto s the author s intuition is that the number of. goto s out of otherwise \none-entry, one-exit regions will be the number of non-tree arcs in the DAG. Applications, Putative Lower \nBounds, and Open Ques\u00ad tions: We will describe the transitive closure algorithm in detail, since it \nis typical of the applications which follow, Transitive closure of a graph tells whether it is possible \nto reach one node from another node, not whether there is a path from one edge to another. Our tree is \nexpressed in terms of edges. To make the transition, we modify the graph. Each node, a, is split into \ntwo nodes, b and c. All edges which formerly entered a now enter b. All edges which left a now leave \nc. An edge is added between b and c, Now, if in the original graph there was a path to a node from node \na, then there is now a path to that node from edge (b,c). In other words, rather than asking ques\u00adtions \nabout node a, we now ask questions about (b,c). Computing transitive closure of a graph is sim\u00adply computing \nwhether there is a path from one node to another for all pairs of nodes. We modify the graph as above, \nand then ask whether there is a path from one edge to another. If the graph is of our restricted type, \nthen the regular DAG has size proportional to the number of edges in our set, in this case two edges. \nHence, the regular DAG has fixed size for each pair of edges, no matter how large the graph gets. Now \nwe must check to see if the regular DAG can describe a string containing u followed by v. If the regular \nDAG has fixed size, this can be done by table look up. Or, it can be done by the following method, For \neach pair of nodes, u and v, in the graph, each vertex in the regular DAG is evaluated to see if it stands \nfor one of the seven following types of sets of paths: 1) a set of paths where neither u nor v occur; \n2) a set where only u occurs; 3) a set where only v occurs; 4) a set where u and v occur, but neither \nfol\u00adlowing the other; 5) a set where u and v occur, but for all instances where u and v occur on the \nsame path, u precedes v; 6) a set with u and v, but all instances of v precede instances of u; 7) a set \ncontaining paths from u to v and v to u. The characterization of vertices in the DAG proceeds by first \nevaluating all vertices in the DAG which have no successors, and then evaluat\u00ading all vertices all of \nwhose successors have been eval\u00aduated. Thus, the DAG may be evaluated in time linear in its size. The \nalgorithm described above may be used to compute transitive closure. For all pairs of nodes, one forms \nthe regular expression for the pair, and deter\u00ad mines whether there is a path from one node to the other. \nIf the graph is a structured graph, then the transitive closure can be computed in time n 2 since there \nare only n2 questions and each may be answered in unit time. The fact that each question can be an\u00ad swered \nin unit time follows from the fact that the questions are about a fixed number of nodes (in this case \n2) and that in a structured graph the time taken to form the summary is solely dependent on the size \nof the subset, and not related to the size of the original graph. The best known transitive closure algorithm \nfor general graphs runs in time n 249 [12,13] and thus it seems unlikely that the algorithm can be extended, \nwithout a substantial time penalty, to general graphs. It would be interesting to know how much the graph \ncould be generalized. Can the author s intuition that the number of non-tree arcs depends on the number \nof goto s out of otherwise one-entry one-exit regions be formalized and proven? Ferrante[ 11 ] has given \nreason to believe this can be proven, If one goto exited se\u00adveral regions, we would count it several \ntimes. Global Flow Analysis information can be calcu\u00adlated in a manner analogous to the way transitive \nclo\u00adsure is. We will sketch here ideas first described in [7,8 ]., However, the ideas described in [14] \nalso apply and may be much more practical. We will describe them following a description of [7,8]. During \nthe the compilation process, using a single pass and hashing, a list can be made at the edges along which \neach propo\u00adsition can change its value. Suppose, after this and the preprocessing on the modified graph, \none is interested in determining the truth of a particular proposition at a number of edges, A set of \nedges is assembled con\u00adtaining all the nodes at which one is interested in de\u00adtermining the truth of \na particular proposition, and also containing all the edges at which the semantic function for that node \nis not the identity function, A regular DAG is constructed describing the path rela\u00adtionship between \nmembers of the set. Each vertex in the DAG represents part of some set of paths through the graph. A \nfunction, f, can be constructed for each vertex in the DAG such that if the propositions con\u00adtained in \nthe set X are true before any part of the paths represented by the vertex in the DAG are trav\u00adersed then \nf(X) must be true afterwards. These func\u00adtions will be constructed in a bottom up fashion. For example, \na proposition must be true after the union of a number of paths are executed if and only if it must be \ntrue after any of the paths are executed. Hence, a union node may be said to Gen a proposition if all \nits children do. After these functions are constructed, they are applied from the top down, to determine \nwhat must be true before the paths represented by the ver\u00adtices are traversed. If 1) it is known what \nis true be\u00adfore a vertex, u, is traversed, and 2) u is represented by the composition of two vertices, \nv and w, and 3) v and w have no other predecessors, then a) the same things must be true on entry to \nv as are true on entry to u, and b) by applying the function for v, we can determine what is true on \nentry to w. If u represented a union, then v and w would have the same value as u. If u represents a \n*, and v represents the part of the DAG being repeated, then let f be the function for v and let X be \nthe propositions true on entry to u. The propositions provable at v are bounded below by the intersection \nof X , ~(X ), f(f(X )) For most types of functions used in flow analysis, it is easy to com\u00adpute this \nintersection, which is called fastness closure in [2,3]. For functions involving only Kills and Gens, \nthe intersection equals the intersection of X and f(x ). We leave as an exercise for the reader the proper \nhandling of + and ~ Using the techniques in [2,3, or 7] we can show that the above sketch can achieve \nan acceptable assignment (i.e. a solution to the global flow analysis problem). Kildall[ 14] first suggested \napplying a method, called the iterative method, for solving flow analysis prob\u00adlems. This method applies \nequally well to regular DAGs as it does to graphs. The method first makes up a potential solution to \nthe flow analysis problem, With more propositions assumed to be true than are. It iteratively applies \nfunctions and refines the potential solution, throwing out propositions, until all proposi\u00adtions remaining \nare true on all paths. It assumes that no propositions are true at the beginning of the pro\u00adgram and \nassumes that to begin with all propositions are true at all edges other than ones leading from node s. \nIt repeatedly applies functions to what is known to be true before them. If one determines that a proposi\u00adtion \nis not contained in the output of a function asso\u00adciated with an edge, e, it refines what propositions \nare assumed to be true at all the edges which can follow edge e in a path accepted by the regular DAG. \nBy properly manipulating the choice of when to apply functions, one can guarantee to only apply functions \nas often as the product of the number of edges in the summary graph times the number of propositions. \nThe computational overhead of this method may well be less than for the previous method for solving flow \nproblems. Ferrante[ 11 ] has suggested that code genera\u00adtion, and possibly optimizing transformations \ncould be done by pattern matching on the regular expressions. For example, suppose that one wishes to \ngenerate code for a computer with vector operations (move character string instructions may be thought \nof as vector opera\u00adtions). An array reference A(i) may occur in the pro\u00adgram. A regular DAG can be formed \nsummarizing the path relations between all nodes in the program which reference or assign to A or i. \nOne may then discover an assignment in which B(i) gets the value of A(i). If that occurs inside a loop, \nand several other criteria are verified about the path relations of uses and assign\u00adments to the relevant \nvariables, then the vector move operation could be used. The algorithm may be used to check whether or \nnot a collection of nodes is a cut set. What is the relationship between graphs stored as adjacency lists \nand graphs stored as regular DAGs? The representation as regular DAGs can be larger, (by a factor of \na(n)) ignoring the size of pointers. If the regular DAG represents strings of nodes, is there al\u00adways \na small graph that corresponds? This is true if the+regular DAG is a regular expression. Conclusions \nA fundamental problem in graph algorithms is to design a way of preprocessing a graph, and then to quickly \nanswer questions about the paths relating cer\u00adtain nodes. Both s-numbering by depth-first search and \ndominator trees are often used to answer a re\u00adstricted type of path question. We have given a tech\u00ad nique \nfor answering more general questions, but only for a restricted class of graphs. We have also indicat\u00ad \ned why this method might be useful for answering questions about flow analysis. Indeed, often clef-use \nchains are constructed to gain some of the advantages available by our method. In theory, our method \nis superior to clef-use chains in that the number of opera\u00adtions it will do is roughly (exactly in the \ncase of pro\u00adgrams without go to s) proportional to the number of bit-vector operations done with clef-use \nchaining. To calculate clef-use information, one bit is assigned in the bit vector for each assignment. \nThe time taken to calculate clef-use chains would be the number of nodes(number of bit-vector ops) in \nthe program times the number of assignments(number of bits in a bit vector) divided by the size of the \nmachine word. Since we would do one operation for each bit vector operation done for clef-use chaining, \nwe will be asymp\u00adtotically faster. Our method is also superior because questions can be answered when \nthey are found. This is important for optimizations like strength reduction. It is also important when \na program is modified, dur\u00ading optimization, by moving assignments and uses to less frequently executed \nplaces, or when code can be eliminated. Acknowledgements: The author would like to thank many of his \ncolleagues at IBM who have read this paper and com\u00admented on it. Without them the paper would be con\u00adsiderably \nless comprehensible. In particular, Jeanne Ferrante, Barry Rosen, and Karl Ottenstein have made valuable \ncontributions. References: 1. Kou, L. On Live-Dead Analysis for Global Data Flow Problems JACM Vol 24, \nNo. 3 pp. 473-483 1977. 2. Graham, S. L. and Wegman, M. A Fast and Usually Linear Algorithm for Global \nFlow Analysis JACM Vol. 23, No. 1 January 1976 pp. 172-202. 3. Wegman, M. General and Efficient Methods \nfor  Global Code Improvement Ph.D. Dissertation, Univ. of Cal. at Berkeley, Dec. 1981. 4. Harel, Dov, \nA linear time algorithm for the lowest common ancestors problem 21st Annual Symposium on Foundations \nof Computer Science, Syracuse N. Y., Oct. 1980 pp. 308-319 5. Rosen, B. K., Linear Cost is Sometimes \nQuadratic 8th Annual Principles of Programming Languages, Jan. 1981, pp. 117-125 6. Allen, F. E. et. \nal. The Experimental Compiling System IBM Journal of Research and Development, vol 24, no 6, Nov. 1980 \npp. 695-715. 7. Tarjan, R. E., A Unified Approach to Path Problems , J. ACM 28, 3 (July 1981), 581-593. \n 8. Tarjan, R. E., Fast Algorithms for Solving Path Problems , J. ACM 28, 3 (July 1981), 594-614. 9. \nCocke, J., private communication to a number of people, 1961-62(?). 10. Tarjan, R. E., Applications \nof Path Compression on Balanced Trees , J. ACM 26,4(Oct 79) 690-715. 11. Ferrante, J. personal communication. \n 12. Coppersmith, D. and Winograd, S., On the Asymp\u00adtotic Complexity of Matrix Multiplication, to appear \nSIAM Journal on Computing, Aug 82 (also FOCS 1981. 13 Aho Hopcroft and Unman, Design and Analysis of \nAlgorithms, Addison-Wesley, 1975. 14 Kildall, G.A. A unified approach to global pro\u00adgram optimization. \nConf. Records of the Second ACM Symposium on Principles of Programming Lan\u00adguages, Oct. 1973, pp. 193-206. \n Appendix: A complete example: In this section we will do a complete analysis, as suggested in the paper, \nof the value of X in the follow\u00ading program. Start Program X:=5; while (Bool STMT 1 ; if Bool X:=6: 1 \n) do 2 then begin go br anch; if Bool 3 then else STMT STMT 2 3; Use 1 X; end; STMT 4 ; branch: STMT \n5; Use 2 X; Stop program; STMTS 1, 2, 3, 4 and 5 could of course be large and complex. The summaries \nwe will derive would not change in size, no matter how large those statements become (assuming that they \nstill had no goto s to branch ). The first stage of our process is to derive a regular DAG. This is accomplished \nby initially la\u00adbelling each edge with its own name. Then the two transformations are applied until there \nis only one entering edge to the stop node and that edge leaves the start node. The order that we use \nto apply the transformations is that suggested by [2,3], basically, finding an innermost loop, precisely \ndefined as a re\u00adstricted region, and using the transformations to re\u00adduce that region to a trivial loop \nand an acyclic part. The graph derived from the program, with each edge labelled follows: Start ~ <=; \n - , ._.._-.-.- b *, Be:>+ \\ 1 / ? .. STMT ..~ 4 ) c m $52~ 22!? - --- ~\u00ad k P ,e ,/.- .\u00ad w , ~, X:=6 \n~~ stop J --\u00ad _-._..i f ~ ,;E-z/ N. h ~....+ ( \\ STMT 2 ) / -> ( TMT J ----7 ~ . j >, ,&#38;:: Use \n1 X \\ -.. ._-.r --, . --\u00ad I The second transformation on graphs is applied, re\u00ad peatedly, first to \nreplace edge d, then to replace edge e, then f, g, and h. These edges are all replaced by appropriately \nlabelled edges leaving Bool 1. CE3 &#38; a . X:=5 Jb 1 . R1=c*d k GEcY4=R3 h : This point is interesting, \nbecause it is the point where the first union is placed into the regular expression. If we were merely \ninterested in obtaining a regular DAG, we would be content to label the edge to Use 1 X in the following \ngraph with (R3.h)j u (R3.g)~ however, in order to minimize the number of non-tree edges in the graph, \nwe instead label it with the equivalent R3. (hj u gi). This equivalent expremion is found by comparing \nR4. j with R6.$ starting from the right hand side of the expressio% until au equal prefix was found. \n214 . EsStart a \u00adX:=5 \\/ b I 1 / STMT 4 m k &#38; * >7 z TMT 5 z o Use 2 X P stop CIiExY = h rua ~ R7=R \n(hj ---__.-_! U gi) Eventually, the following regular DAG is derived: jk 1 m noP Edges b, f, i, j, \nand o are all the edges related to the use and setting of X, We can derive the summary of A these edges. \nNote, that while the original DAG was not a tree, this regular DAG is in fact a regular ex\u00adpression. \nThis will happen in the process of making some summaries. The summary is built first by finding the LCA, \nu, of i and j. u has a union, and hence the summary contains a union. Then the LCA, v, of u and f is \ntaken. There are no unions or * encounted in finding v, so it is la\u00adbelled with composition. In the process \nof traveling \\ from v to the LCA of v and b, a * is encountered, and b o Now, we build up the effects \nof the paths, on the val\u00ad this must be placed in the summary. Finally the fol\u00adue of X, bottom up. The \nunion of i and j leaves the Iowimg summary is derived. value intact. Edge f followed by that union sets \nX to 6. When that is repeated O, 1, or many times, X is either equal to 6, or to what it was equal to \nbefore. Edge b sets X to 5, and edge b followed by O or more occurrences of f, i, or j sets X to either \n5 or 6. We then determine what is true at edges going top down. At edge o, X is either equal to 5 or \n6. At the entrance to the loop X is equal to 5. After f, it will be equal to 6 when it is used by either \ni or j. \n\t\t\t", "proc_id": "567067", "abstract": "This paper shows how to rapidly determine the path relationships between k different elements of a graph (of the type primarily resulting from programs) in time proportional to k log k. Given the path relations between elements u,v, and w, it is easy to answer questions like \"is there a path from u to w?\" and \"is there a path from u to w which does not go through v?\" (The elements can be either nodes or edges.)This algorithm can be used in a wide variety of contexts. For example, in order to prove that whenever control reaches a point p, the last assignment of a value to a variable, v, has always been of the form v := c, where c is a certain constant, it is only necessary to know the path relations between the point p and all assignments to that variable.Ordinarily one is interested in the possible points, where a variable was assigned its current value (definition points), and at the points at which that value is used. Previously, all flow analysis algorithms would compute the definition points of all variables at all nodes in the graph, despite the fact that any given node may use only one of those variables.This algorithm may also be a generally useful graph algorithm. It can compute transitive closure more rapidly than the standard algorithm using the current best matrix multiplication algorithm on the kinds of graphs resulting from programs. The algorithm proceeds by preprocessing the graph, creating tables that can be used later to rapidly determine flow relationships between any sections of the program. In addition, small changes to the graph need only result in small changes to the tables. The algorithm is therefore suitable for incremental analysis.", "authors": [{"name": "Mark Wegman", "author_profile_id": "81100339949", "affiliation": "IBM Thomas J. Watson Research Center", "person_id": "P191288", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/567067.567088", "year": "1983", "article_id": "567088", "conference": "POPL", "title": "Summarizing graphs by regular expressions", "url": "http://dl.acm.org/citation.cfm?id=567088"}