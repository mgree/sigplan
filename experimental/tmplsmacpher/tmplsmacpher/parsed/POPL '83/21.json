{"article_publication_date": "01-24-1983", "fulltext": "\n Permission to make digital or hard copies of part or all of this work or personal or classroom use is \ngranted without fee provided that copies are not made or distributed for profit or commercial advantage \nand that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, \nto post on servers, or to redistribute to lists, requires prior specific permission and/or a fee. &#38;#169; \n1983 ACM 0-89791-090-7 $5.00 The paper is organized as follows. The remainder of this section describes \nhow to construct our interme\u00addiate program form, called an extended data flow graph (edfg). The folIowing \nsections describe some transfor\u00ad mations of this form. An algorithm for performing code motion on an \nedfg is presented and analyzed in Section 2. Sections 3 and 4 briefly discuss scalar pro\u00adpagation, constant \nfolding and common subexpression elimination. Next, Section 5 sketches induction varia\u00adble defection. \nFinally, Section 6 addresses some issues in code generation and Section 7 gives a summary and some areas \nfor future work. We assume throughout that the reader is familiar with flow analysis and op\u00adtimization \n(1, 13). 1.1 Background. Ottenstein defined a program form which consisted of a cyclic data dependency \ngraph and an associated control flow graph in (18, 19). That form, called a data flow graph (dfg), is \nconstructed by creating DAG s (1) for each basic block and then linking every upwards exposed use to \nall reaching definitions, mak\u00ading definition-use chaining explicit. Each node in the graph is labelled \nwith the number of the block in the associated control flow graph which had originally contained the \ncomputation represented by the node. A dfg makes explicit, via data flow, the ordering constraints which \nmust be imposed on a program. Ot\u00adtenstein (17, 18) developed methods for using the dfg form for scalar \npropagation, common subexpression elimination and reduction in strength. Some transfor\u00admations, such \nas code motion and common subexpres\u00adsion elimination, which require both data dependency and control \nflow (path and regio,n) information are no easier with dfg s than conventional forms due to the usual \ndichotomy of control and data dependency infor\u00admation. This current work eliminates the shortcoming of \ndata flow graphs for programs which are side-affect free and which have so-called structured flowgraphs \n(constructed from begin-end, if-then-else, repeat-until, and do-while control structures). Each operator \nin the data flow graph has as predecessors both its operands as well as a predicate expression which \ndescribes pre\u00adcisely the conditions which must hold for the operator to execute. The control flow graph \nis no longer need\u00aded, as all information is unified in the new form. Our ideas are similar in concept \nto some of the ideas presented in (11, 14, 22). Kuck s work has simi\u00adlar aims, but uses a more complicated \nnotion of de\u00adpendence graph which includes five kinds of depend\u00adence, and does not incorporate predicates. \nIn addi\u00adtion, (14) does not treat any of the transformations with which we are concerned. Treleaven (22) \ncon\u00adstructs a machine model which incorporates both data flow machines and the more standard Von Neumann \narchitectures, and is not concerned with program translation. Dennis data-flow schemata (11 ) realize \none of the same effects as our graph form by eliminat\u00ading any concept of physical statement order which \nis not determined solely by control and data dependence relations. The logical gates in that form make \ntrans\u00ad formations of the kind we describe awkward; however, Montz ( 16) has examined some data flow machine\u00ad \nspecific transformations.  1.2 Predicate Regions. Before showing how an extended daa flow graph is \nconstructed, we will describe in general terms the underlying program model. Consider the program schema \nbelow: S1 if P1 then S2 while P2 do S3 if P3 then S4 else S5 S6 if P4 then S7 else repeat S8 until P5 \nS9 else S 10 Sll while P6 do S12 S13 Each control structure defines a region of influence which may be \nlabeled with the relevent predicate. These regions are nested, reflecting the control struc\u00adture nesting. \nIn Figure 1 (which resembles a Nassi-Schneiderman chart), these regions are shown enclos\u00ading the statements \nS 1,, ,S 13. The predicate defining an if region is shown just outside that region. The predi\u00adcate defining \na repeat region is shown inside the region since the evaluation of the predicate may depend upon itself, \nand is evaluated under the same conditions as the body of the loop. The predicate defining a while region \nis shown outside the region since its execution is not initially dependent on its truth value, and so \nis not evaluated under the same conditions as the body of the loop. Just inside each box is a T or F \nindicat\u00ading that the region is executed only when the corre\u00adsponding predicate is true or false, respectively. \nIf the T or F has a + or * suffix, the region is a loop. In the case of + , the region is executed only \nas long as the predicate has the indicated truth value. In the case of * , the region is executed regardless \nof the truth value initially (the region boundary vanishes) and thereafter the region is as a + region, \nThus the predicate determining a while region should itself be in a T* region, (indicated in Figure 1 \nby a T* tag on the predicate). The outer region is defined by a global condition which we call simply \nentry . The data dependency graph may be thought of as being overlaid on top of this region map, with \neach node residing in the region corresponding to its source program location. If there are no data dependencies \nbetween certain regions, they and the associated com\u00adputations can be moved. In terms of code production, \ncode for statements (such as S1, S11, and S13) or regions (such as P1 and P6) which are within the same \nregion at the same level may be generated as defined by the data dependency graph. If regions are inde\u00adpendent, \nthis information can be used to better place information onto pages or schedule instructions onto a single \nor multi-processor, for example. We next make this intuitive program model more precise.  1.3 The Extended \nData Flow Graph. An extended data flow graph is a graph consisting of simple data flow graphs, which \nrepresent the compu\u00adtations in basic blocks in our form, linked by two kinds of (possibly labelled) edges. \nData dependency edges indicate the flow of data values through the sdfg S . Control dependency edges \nlink the governing predicate of a computation with the computation, and are labelled with the truth values \nof the predicate un\u00adder which the computation would be executed. Sdfg s were first defined and used in \n(19). An sdfg is essen\u00adtially a computational DAG as in (1), except that com\u00admon subexpressions may not \nhave been removed, and explicit assignment operators appear for any assign\u00adment of a constant to a variable. \nThe construction of an extended data flow graph for structured programs is outlined below. Our algorithm \nwill permit the regrouping or reassociation of Fori:=l ton operators which satisfy the associative law. \nOperators for which reassociation is permitted should be marked during construction. Binary expression \ntrees involving such operators should be flattened into n-ary operator trees. This flattening allows \nthe code motion and common subexpression elimination algorithms to re\u00ad group operands as necessary to \nproduce improved code. ALGORITHM: CONSTRUCTION OF AN EXTENDED DATA FLOW GRAPH 1. For a program with body \nB, CONSTR UC T_&#38;_LAREL_SDFG( B,entry, T ), (see below), 2. Compute the set REACH of reaching definitions \nper basic block, and the set INIT of variables ini\u00adtialized on every path to a basic block. 3. Use REACH \nand INIT to link sdfg s together, in\u00adcluding an undefined node as in ( 18), using data\u00addependency edges. \n(Label such edges with the ad\u00additional predicates gathered from the intermediate REACH sets, as outlined \nin Section 4, if common subexpression elimination or code generation is to be performed.) 4. Flatten \nthe expression trees of those operators marked for reassociation.  CONSTR UC T_&#38;_LABEL_SDFG( S,P,L) \n: If S of of the form: a) A simple statement or expression: For a statement A : = X op Y, with operator \nop, we create a node labelled op,A , and give it two predecessors by data-dependency edges, and one control-dependency \nedge from P la\u00ad belled with L. The data-dependency predeces\u00ad sors are the nodes representing the current \nvalues of X and Y in the block, if they exist, or newly created nodes labelled with the varia\u00ad ble name \nif not. Constants are shared be\u00ad tween basic blocks. (Other cases, such as when the expression has more \noperands, or where S is an expression X op Y, are handled similarly. ) ( 18, 1) b) begin Sl,...,Sn end: \nCONSTR UC T_&#38;_LABEL_SDFG( Si,P,L) c) if Q then S1 else S2: CONSTR uCT_&#38;_LABEL_SDFG (Q,P,L) CONSTRUCT_&#38;_LABEL_SDFG( \nSl,Q, T ) coNsTRucT_&#38;_LAB EL_sDFG(s2,Q, F ) d) while Q do S1: CONS TRUCT_&#38;_LABEL_SDFG (Q,P,L) \ntag(Q, while ) CONSTR UCT_&#38;_LABEL_SDFG (Sl,Q, T+ ) e) repeat S 1 until Q: CONSTR UCT_&#38;_LABEL_SDFG \n(Q,P,L) tag(Q, until ) CONS TRUCT_&#38;_LABEL_SDFG (Sl,Q, F* ) Note that there are two kinds of edges \nin the graph: data-dependency and control-dependency edges. Each node has a single explicit control prede\u00adcessor; \nthe tagged loop predicates additionally have themselves as an implied control predecessor. A tagged while \npredicate is in a T* region determined by its own predicate, and a tagged repeat predicate is in an F* \nregion. The T* indicates that the operator will be executed initially, and then only as long as the associated \npredicate is true. The T+ indicates that an operator in the region will be evaluated only as long as \nthe associated predicate is true. The F* indicates that each operator will execute regardless of the \nassociated predicate initially and only as long as it is false thereafter. In what follows, the control \nedges are drawn with dashed lines and the data edges with solid lines. The leaves of the sdfg s become \ngraph nodes which do not represent computations, but rather represent the set of reaching definitions \nfor a particular variable for a par\u00ad ticular basic block. Such a node having only one data predecessor \nis called a data node; one having several such predecessors is called a merge node ( 19). Some transformations \n(e.g., scalar propagation and constant folding) are concerned only with data edges; others will be interested \nin all edges. For the transformations which follow, a tree con\u00adtaining all predicate computation nodes \nis required. This predicate tree should reflect precisely the nesting of predicate regions such that \na postorder traversal would trace the predicate values of the second argu\u00adment determined by the recursive \ncalls to CONSTR UC T_&#38;_LABEL_SDFG of step 1. The tree shoild additionally be threaded such that each \nnode points to the nearest loop predicate ancestor. Note that without these threads, the tree is a subgraph \nof the ed~g. (This tree is easily constructed during step 1,) We should note here, as in (18), that including \nprocedures in the source language requires interproce\u00addural data flow analysis (1) before even simple \nd~g s per basic block can be constructed in 1 above, unless worst-case assumptions are made. To avoid \nworst case assumptions, step 1 would require an initial pass over the source program to gather data flow \ninforma\u00adtion and produce some initial intermediate form such as quadruples. Intra-and interprocedural \ndata flow analysis would follow, at which point sdfg s could be constructed. The representation of structured \nvaria\u00ad bles is discussed in (19, 18). 2. Code Motion. * The region containing a computation gives us \npre\u00ad cisely the predicates for whatever loop and conditional structure determines the execution of the \ncomputation. No other control representation whatsoever is known. We define the predicate chain for a \ncomputation to be the list of predicates on the path from the entry predi\u00adcate to the computation. In \nthe graph as constructed, the predicate chain reflects the conditions necessary for the execution of \nthe basic block in which the com\u00adputation resides. (The initial predicate chain is equal to the path \nin the predicate tree from the root to the computation s control predecessor. ) Code mo~ion ex\u00adamines \nthe predicate regions of the nodes on which a computation is data dependent in order to move com\u00ad putations \nout of loop regions Examine the region map of Figure 1 for the fol\u00ad lowing example. Suppose that S4 \ncontains the expres\u00ad sion a+b/c and that that the reaching definitions for a , b and c come from S3, \nS2 and S1, re\u00ad spectively. Using the notation P3T to refer to the true region defined by P3, we would \ninitially have both the + and / nodes associated with P3T . The predecessors of / would be whatever computa\u00ad \ntions defined b and c : these would be associated with PIT and entry . The predecessors of + 11 II would \nbe the definition of a in p2T+ and the / operator. Figure 2 shows what this partial edfg would look \nlike, overlaid onto a region map to clarify loca\u00adtions Given that all reassociable basic block expression \n trees have been flattened at parse time, code motion would be performed by the recursive graph marking \nprocedure sketched in the Appendix, That algorithm will not move entire loop regions, however. Such \nmovement is described in Section 2.3. In our example, assume that a , b and c are inputs or constants. \nFurther suppose that we have climbed up the graph by recursion to the point of try\u00ading to move the / \n. As the algorithm continues re\u00adcursively, we first try to move all of the predecessors of / , Suppose \nb and c have been moved, and we have not yet tried to move P3 , Each predicate in the predicate tree \nhas a sequence OPDEP associated with it which is initialized to be empty, (Sequences are needed here \nto keep track of actual operands in case we can regroup operands). Suppose that P3 was data dependent \nonly on values defined in S2 (represented here by the ? ), which again have been moved. The algorithm \nproceeds by looking at the data dependency predecessors of P3, represented here by the ? , and adding \nthem to the OPDEP sequence of their control predecessors, here PI . The predicate subtree of P3 is scanned \nto see if any of its OPDEP sequences are nonempty. If not, the algorithm then walks the predicate chain \nbackward from P3, stopping at the first predicate with a nonempty OPDEP se\u00adquence in its predicate tree, \nhere PI. If a predicate with a + or * associated with it is encountered in this backward search, then \nthe computation in ques\u00adtion can profitably be moved from a loop, The com\u00adputation P3 could be moved \nto region P2T from P2T+ simply by changing the control dependency edge label. The value of P3 would still \nbe used to determine region P3T , This is reflected in the graph by creating a reference node which is \ndata depend\u00adent on P3, but control dependent on P2T+. This ref\u00aderence node is given as its successors \nall of the succes\u00adsors which used to belong to P3. (See Figure 3.) Now, when the procedure tries to move \nthe / (having returned from visiting all of the predecessors of / ), it will find P 1 as the first predicate \nwith a nonempty OPDEP sequence when climbing back along the predicate chain for / . During that climb, \na + edge will be traversed, indicating that a (potentially) profitable move can be made out of a loop. \nThe con\u00ad trol edge which connected the P3 reference node to / is then replaced by a T-edge connecting \nthe now\u00admoved P3 to / . Figure 3 shows this final graph. This simple operation is equivalent to stating \nthat an if P2 and P3 then t:=b/c statement be generated in the PIT region. The + -operator cannot be \nmoved due to a being defined in the P2T+ region. To see how reassociation of operands works, con\u00adsider \nthe following example: j:=l; while j < = 10 do begin s:=s+a(i+ j); j:=j+ lend Assume a word-organized \nmachine memory, and that reassociation of subscript expressions is allowed. Af\u00adter exposure of addressing \ncomputations, a(i + j) in\u00adside the loop would be referenced as t:=i+j-l+acfcfr-(a) (1) . Our reassociative \ncode motion algorithm would move the loop-invariant part of this computation, i -1 + addr(a), outside \nthe loop, under control of the predi\u00adcate j <= 10 . After scalar propagation eliminated the test j <= \n10 , code equivalent to tl := i-1+ addr(a) would then be generated outside the loop. Then, inside the \nloop, ( 1) would be replaced with t:=tl+j 2.1 Safety and Correctness. Examination of the algorithm shows \nthat the same data values reach a node n before as after the trans\u00adformation. Hence the value of n, when \nexecuted, is unchanged. We now argue that if n is executed only if control predecessor p is true before \nthe transformation, then n is executed only if control predecessor p is true after the transformation. \nAssume n is executed only if con\u00adtrol predecessor p is true before the transformation. Then the edge \nfrom p must initially be Iabelled T+ or T . If the edge were marked T+ , the + may be deleted by the \nalgorithm, leaving a T . Thus, n would be executed after the transformation only if p were true. If the \nedge p is marked T , then if p has been moved, the algorithm may either create a refer\u00adence node or move \nn to p s region. In either case, since the reference node has the same truth value as p, n would be executed \nafter the transformation only if p is true. 2.2 Code Motion Complexity. The time complexity of this \ntransformation, with or without reassociation, is O(E + N*t + N*d), where E is the number of edges in \nthe graph, N is the num\u00adber of nodes, t is the number of nodes in the predicate tree, and d is the maximal \nin-degree (with respect to data-dependency) of any node. Only one of the first two while loops can be \nexecuted for a given node (depending on its associativity). The while loop thus executed and the remaining \nwhile loop of the program can be executed in total only r times per node, where r is the maximum length \nof a predicate chain (the maxi\u00admum static nesting of conditionals and loops). Factor\u00ading itself can only \nadd a total of d steps per node. At most the entire predicate tree must be examined and the appropriate \nsequence appended per node. There\u00adfo~e, the number of steps needed to perform the while loops for all \nnodes in reassociative code motion is O(N * max(t,d)), since r is less than or equal to t. The rest of \nthe algorithm adds at most O(E) steps. A number of nodes and edges, linear in the number of factoring \nsteps, may be added to the graph. These do not add to the number of steps in the algorithm, however, \nsince these new nodes and edges are never considered further. Without expression flattening, the value \nof d will be a fixed constant. While an if a then if b then if c then... program would have a value \nof t and r propor\u00adtional to N, giving a bound of O(N*N), the predicate tree will only be examined to \nthe extent that actual motion takes place. Our intuition is that typical pro\u00adgrams would not force examination \nof a predicate subtree with more nodes than some small, fixed con\u00adstant nor would such programs exhibit \na value for r or d greater than a small, fixed constant. Thus, the per\u00adformance of our code motion algorithm \nmay well be linear in the number of nodes in the extended data flow graph. 2.3 Movement of Loops. Entire \nloops can be moved in the edfg framework. Non-loop motion is an outer-to-inner transformation. Loop motion \nmust be performed in an inner-to-outer order, however. This is because the movement of an inner loop \ncould remove dependencies from an enclos\u00ading loop, thereby permitting motion of that loop. Fur\u00adthermore, \nnon-loop motion must precede loop motion in order to remove invariant dependencies from the loop regions. \nOur approach to loop motion is similar to the non\u00adloop code motion algorithm in the Appendix; the dif\u00adference \nis that all nodes in a region are examined and marked together, and dependencies within the region are \nignored. We briefly outline our method. For loop motion, we visit each loop predicate in the order defined \nby a postorder traversal of the predi\u00adcate tree. (This tree will be updated as motion oc\u00adcurs. ) For \neach predicate in the predicate subtree root\u00aded at a given loop predicate L, we examine every con\u00adtrol \nsuccessor S, marking the control predecessor of every data predecessor of S. Then we walk the predi\u00adcate \nchain of L, examining only loop predicates P. We examine the subtree of P (or entry if L has no such \nloop ancestor), In this examination, the subtree root\u00aded at L is ignored. The loop region L can be moved \nto the outermost unmarked loop predicate which has no marks in its subtree. Should movement occur, the \npredicate tree is also updated. How might an invariant loop be present in a pro\u00adgram in the first place? \nOne way is through program\u00admer error, and a message should be displayed for the user. Other such loops \ncould be the result of other optimizations such as procedure integration (3,7) or loop splitting (21 \n).  2.4 Comparison with Previous Code Motion Algorithms. The node distinction work of Wegman (25), Ur\u00adschler \n(23), and Wegbreit (24) uses node-splitting (2) techniques for the purpose of simplifying code, one example \nof which is code motion. In our program representation, no splitting takes place, and so there is no \nduplication of code. In final code generation, some of the same effects would be obtained using node \ndistinction as by the application of our techni\u00adques. However, our techniques always result in struc\u00ad \ntured programs whereas the result of (25) is not al\u00adways structured. More importantly, the application \nof node distinction techniques can exponentially increase the size of code, and requires heuristics to \nmake it computationally feasible (25). Unlike the methods presented in (1), our algorithm makes only \nsafe movements, as computations always remain under the influence of the same predicates. In addition, \nours never increases the running time of a program since no code is ever executed in the trans\u00adformed \nprogram unless it would have been executed in the original. 3. Scalar Propagation and Constant Folding. \nScalar propagation and constant folding (1) are optimizations which decrease the size of an ed~g and \nshould thus be performed after edfg creation to de\u00ad crease the running time of subsequent optimizations. \nMoreover, scalar propagation makes transitive data dependencies explicit and is required for the common \nsubexpression recognition strategy of Section 4. Therefore we summarize here the linear time algorithm \nfor scalar propagation and constant folding in ( 17). Scalar propagation and constant folding may be \nperformed together in a single graph walk which pro\u00adceeds in a manner similar to our motion algorithm. \nThis walk only traverses data dependency edges, how\u00adever. That is, the algorithm recurses from outputs \nto inputs or constants, examining a node only after its data predecessors have been visited. If all predeces\u00adsors \nof an operator are constants, the operator and the incident edges are replaced by the result. Scalar \npro\u00ad pagation occurs according to three transformations relative to the operator n: Tl: If n is a data \nnode with predecessor p, make each of n s successors data dependent on p. Delete n and all edges incident \nto and exit\u00ading from it. Alternatively, if n is an identi\u00adty operator and n s successor is not a merge \nnode (i. e., the successor is an operator or a data node), perform the transformation just described. \n T2: If n is a merge node and there is an edge from n to itself, delete that edge. T3 : If n is a merge \nnode, delete any redundant incident edges. (Recall that a data node is a node labelled with a varia\u00adble \nname which has only one predecessor. A merge node is such a node with more than one predecessor. Both \nrepresent the set of reaching definitions for some variable used in a basic block. ) 4. Common Subexpression \nElimination. In order to perform common subexpression elimi\u00adnation, additional information must be collected \nand added to the extended data flow graph during the con\u00adstruction of the REACH sets. Consider the program \nfragment below: a := ... repeat := a+b if Pthen a:= ... ... := a+b until Q; The first and second references \nto a have much in common: they have exactly the same reaching defini\u00adtions and they are contained in \nthe same loop and both will execute at least once and as long as condition Q is true thereafter. However, \nthe definition of a inside the loop can reach the first reference to a only if the condition Q has been \nevaluated and found to be false. This can be represented by associating certain edges with predicate \nregions. Here, the edge from the inner definition to the first use of a can be associat\u00aded with the \npredicate Q in a F+ region, thereby distinguishing it from the second use. The use itself is in a F* \nregion since Q need not be evaluated for the statements in the loop to execute the first time. This association \nof ,predicates with edges can be accomplished by computing the definitions which reach a use only because \nof the presence of a cycle. These reach sets would simply be the set difference between the reaching \ndefinitions for the loop asuzcom\u00adputed by a variant of interval analysis and an interme\u00addiate value of \nthe reach set computation. The edges connecting those definitions to a use would be associ\u00adated with \nthe loop predicate, and would serve to make the necessary distinctions between uses of a variable in \na loop. We could have alternatively associated the edge from the first definition of a to its second \nuse with P in an F region. Although we have chosen not to compute such information, our model could incor\u00adporate \nand use it effectively. A popular method of finding common subexpres\u00adsions relies on the computation \nof the available expres\u00adsions ( 1 ) for each basic block. This framework would distinguish the two instances \nof a+b above since the definition of a in the loop would kill the previous expression. Disadvantages \nof using available expres\u00adsions include the need to recompute them after trans\u00adformations and the fact \nthat reassociation can not be implemented conveniently with them. 4.1 Elimination Method. Ottenstein \ngives a method for detecting common subexpressions in (18) derived from (19) which uses the concept of \ncongruent expressions (26). If two like\u00adnamed operators have exactly the same data predeces\u00adsors with \nthe same edge labels, they are congruent and are common subexpression elimination (cse) candi\u00addates. \nFlattened associative operators give us two additional types of cse candidates: ( 1) the data prede\u00adcessors \nof one operator may be a subset of those of the other and (2) the two operators may share at least two \ncommon data predecessors. An important aspect of common subexpression elimination with (extended) dfg \ns is the fact that merge nodes must be treated as operators in the reduction. Recall that after scalar \npropagation, only merge nodes remain to represent sets of reaching definitions as all data nodes have \nbeen deleted. Treating merge nodes as operators leads to a clean, consistent algorithm. After redundant \nmerge nodes are eliminated, the graph is left with only unique sets of reaching definitions, these sets \nbeing the predecessors of various compu\u00adtational operators. Those operators can then be exam\u00adined for \ncongruent (sub) expressions, followed by their children, etc., using the same graph walk as previously \nused: we climb from outputs to inputs or constants, examining a node for me only after all of its predeces\u00adsors \nhave been marked or examined. The actual exam\u00adination for congruence requires looking at a node s data \npredecessors and then checking the other data successors of those predecessors. Given a set of congruent \n(sub) expressions, we have to determine when one instance can be replaced with a reference to another \nor, if no instance is in a suitable region for value sharing, finding such a region. Given a set of congruent \noperators with the same control predecessor and the same label on that control edge, we can perform a \nreduction. All but one opera\u00adtor can be deleted. adding the successors of the delet\u00aded operators as successors \nof the remaining operator. Suppose that two congruent operators have the same control predecessor C, \nbut the control edge la\u00ad bels have opposite truth values. One operator maybe deleted after its successors \nare made successors of the other congruent operator. The remaining operator should have its control edge \nreplaced with one from C s control predecessor P labelled exactly as (P, C). This movement corresponds \nto hoisting the expression to the nearest dominator of an alternation. Suppose that two congruent operators \nhave differ\u00adent control predecessors Cl and C2. A reduction can be performed if two conditions exist. \nFirst, either C 1 or C2 must be a control ancestor of the other or they must share some common ancestor \nC3. Second, no operands must be defined in a particular region de\u00adfined by a particular set of predicates. \nThis set of predicates consists of all of those on the predicate tree path(s) from the ancestor to the \ndescendent instance(s) along with all predicates in the predicate subtree of any loop predicate in the \nset. If any ope\u00adrand were in the region defined by these predicates, it would mean that the operand could \nbe updated be\u00adtween the ancestor s region and the descendent s re\u00adgion. This would mean that the descendent \ns value on a given execution of the region would not necessarily be the same as its ancestor and its \ncomputation could thus not be eliminated. This distinction is similar to that described in the beginning \nof this section for a single loop. As there, two expressions may be con\u00adgruent because the loop structure \ngives them the same set of reaching definitions, yet the values on a particu\u00adlar execution may differ. \nThus, if the intersection of the predicate set and the set of all operand control predicates is empty, \na reduction may take place. (In the case of a nonempty intersection, factoring may still be possible \nwith an associative operator. ) 5. Induction Variable Detection. 5.2 The Detection Algorithm. 5.1 Introduction. \nGiven a set of basic induction variables, induc. tion variable detection checks whether a variable (or \nan expression) in a loop in the source program can be written as an invariant expression plus a linear \ncombi\u00ad nation of basic variables. This is the first step in the optimization called induction variable \nelimination, where references can be replaced by the obtained ex\u00adpression, thereby eliminating variables \nor facilitating reduction in strength. This problem is also of interest in the context of compiling for \nvector machines (5) since only those statements involving vectors whose subscripts are of this form are \ncandidates for vectori\u00adzation. (One would then have to perform other tests, such as the dependency analysis \nof (6), to safely re\u00adplace the loop with a vector statement.) The definition of a basic induction variable \nwould vary with the optimization problem. One can define a basic induction variable to be any variable \nwhose only assignment is in terms of its old value plus a loop invariant expression. We present an algorithm \nwhich recognizes basic induction variables of this sort below. We should remark here that variables as \nsuch do not appear in our program form except as labels for merge nodes. Thus our basic induction variables \nare those merge nodes whose assignment is in terms of its old value plus a loop invariant expression. \nSimilarly, our algorithm for induction variable detection below actually checks whether a node (which \nmight represent a variable or an expression) can be written as an in\u00advariant expression plus a linear \ncombination of basic! induction expressions. The detection of induction nodes is based on the static \nnesting of loop predicates of the entire program (as given by the predicate tree of the entry predicate. \nSee 1.3). Processing of loop predicate regions pro\u00adceeds innermost to outermost so that all successors \nof a predicate in the predicate tree are processed before the predicate itself. Given a loop predicate \nP, we first mark all predi\u00adcates in the predicate subtree rooted at P with P. Giv\u00aden that code motion \nhas taken place, it is then easy to determine if a node represents an invariant expression in the given \nregion. One need only check to see that the node s control predecessor is not labelled with the region \npredicate. We now sketch an algorithm for detecting basic induction nodes of the region. First find \nall strongly connected components in the region subgraph. We first check that all operators in the strongly \nconnected component are either + or - . (Any operator other than + or -is unallowable in this context. \n) For each node in the strongly connected component, we check that all data flow predecessors of the \nnode out\u00adside the component are invariant in the region. We then conclude that all nodes in the strongly \nconnected component are basic induction nodes. Now assuming basic induction nodes in the region have \nbeen marked, we detect induction nodes by a recursive procedure as follows. The algorithm starts at all \noutputs of the region, determined by a depth first walk of the region. If the node v has already been \nvisited, then v is not an induction node. Otherwise, v is marked as visited. If v is invariant in the \nregion, then v is marked as a constant. If v is marked as a basic induction node. then v is also an induction \nnode. Otherwise, if v is a + or - operator, then v is an induction node iff all its data flow predecessors \nare recursively determined to be induction nodes or con\u00ad stants, and at least one such is an induction \nnode. If v is a * operator, and there is a unique data flow predecessor d of v in the region, and all \nother data flow predecessors of v are constant in the region, then v is to be an induction node if and \nonly if d is recur\u00adsively determined an induction node. If v is any other operator, then v is not an \ninduction node.  5.3 Complexity. Given a region as determined by a loop predicate, marking all predicates \nin the predicate tree of the loop predicate can be done in time proportional to the size of the tree. \nFinding all basic induction nodes in a region, as outlined above, will be at worst linear in the number \nof edges in the region. Strongly connected compo\u00adnents of the region can be determined in time propor\u00adtional \nto the number of edges in the region. The search through all nodes in the strongly connected component \nfor an unallowable operator, and for each node in the component, a check of all data flow prede\u00adcessors \nof the node for invariance in the region, can be done in time proportional to the number of edges in \nthe region. Similarly, finding all induction nodes will also be linear in the number of edges in the \nregion; again, the algorithm involves a predecessor search, with each node marked visited never being \nreexamined.  5.4 Comparison with Previous Detection Algorithms. Only linear combinations of a singIe \nbasic induc\u00ad tion variable are considered in (l), so our method is a generalization of the one presented \nthere. The algor\u00ad ithm of Cocke and Schwartz (10), (9), (4) only de\u00ad tects variables which can be written \nas sums of basic induction variables. However, since their purpose is reduction in strength, multiplication, \nincluding multi\u00ad plication by a constant, is handled separately. The insertion of code and substitution \nof expressions used in their method can be fit into our framework. In fact, substitution just involves \nchanging data-dependency edges, and so does not involve duplicating code, a great advantage of our scheme. \nThe algorithm of ( 10), however, is essentially quadratic in nature, as each time the set of induction \nvariables is changed, a new pass is made over the source program. This re\u00ad flects the essentially sequential \nnature of the data structure chosen, The detection algorithm is linear (in the number of edges in the \ned~g) and only one pass need be made over the subgraph in a region. This reflects the fact that the computationally \nrelevant facts appear explicitly in our data structure. 6. Code Generation. If the target language is \nbased on a Von Neumann architecture, the following strategy may be used in generating target text. The \n(possibly transformed) predicate tree permits the generation of an appropriate control template. Within \neach region, code should be ~enerated iu locations dictated by any control tags on data dependency edges \nand in an order dictated by data flow. The highly parallel ed~g must be sequen\u00adtialized. Methods for \nthis are given in (18). If a machine language is the target, a method such as the coloring scheme of \n(8) seems desirable for reg\u00adister assignment. Each node in the edfg represents a virtual register in \nthat framework. A topic for further work is the construction of the interference graph for an edfg. No \nnodes which are adjacent in the edfg will be adjacent in the interference graph. The difficult question \nis determining those nodes which should be adjacent. Of course, other register assignment and code generation \nstrategies are applicable as well. Also, note that while computations are moved and referred to elsewhere \nin the graph, it is still a register allocation decision as to whether a register is kept busy over a \nlong distance or whether registers are loaded at reference points. 7. Conclusions. 7.1 Future Work. \nOne immediate avenue of investigation is the in\u00adtroduction of unlimited go-to s into our representation. \nIt is clear that in this case, more control flow informa\u00adtion than that given by predicate regions will \nhave to be introduced, but the full extent to which control flow would have to appear remains to be investigated. \nA second area of future research is the application of our techniques in the context of vector machines. \nBoth the vectorization question, and the development of vector optimization techniques, will be investigated. \nWe are interested in investigating whether our form will permit all of the vectorizing transformations \nof (5, 15) without additional dependence types. Our form appears amenable to the problem of scheduling \nin\u00adstructions onto a parallel architecture. This problem will also be investigated. The development of \nother optirnizations in this framework is a third area of research. We believe that our representation \ncaptures the essentials of program information, in that any further information needed can be easily \ngenerated. Further, by operating in this framework, we are directly manipulating the essential information \nstructure which guides the transforma\u00ad tions. A true test of the viability of our representation will \nbe the ease with which one can formulate optimi\u00ad zation algorithms, and the efficiency of their imple\u00ad \nmentation in a realistic framework. Clf particular con\u00ad cern is procedure integration (3), an essential \ntransfor\u00ad mation given the current emphasis on modularization and abstraction in programming. When procedure \nintegration is employed at the source level or at the code generation level (7), powerful optimization \ntech\u00adniques are needed to tailor integrated code to its target environment. 7.2 Summary, In this paper \nwe have proposed a program repre\u00adsentation for structured programs in which the sequen\u00adtial order of \nsource statements is not imposed, but rather only the essential order determined by data flow relationships. \nIn addition, our representation reflects data flow information explicitly, so that when optimiz\u00ading transformations \nare performed, little recomputation of information is needed. This is in contrast to the usual optimization \nsetting. We believe that this will lead to more efficient and powerful c,ptimization tech\u00adniques, demonstrated \nby our reassociative code mot\u00adion, global common subexpression elimination, and induction variable detection \nalgorithms. We expect that in the context of serial machines where powerful optimizations are needed, \nour techniques will prove extremely useful. They also have great potential use in translation for parallel \nmachines, both in the area of scheduling and optimization. REFERENCES 1. Aho, Alfred V. and Unman, Jeffrey \nD. Pr-irzci\u00adples of Compiler Design. Addison-Wesley (1977). Cocke, John. Graph theoretic constructs for \nprogram control flow analysis. IBM Research Report RC-3923 (July 1972) 65 pages. 2. Allen, Frances E. \nand 3. Allen, Frances E. and Cocke, John. A cata\u00adlogue of optimizing transformations in Design and Optimization \nof Compilers (Randall Rtrstin, Ed.) Prentice-Hall (1972) 1-30. 4. Allen, Frances E., Cocke, John., and \nKennedy, K. Reduction of operator strength Program Flow Analysis (Muchnik, S., and Jones, N., Eds.) \nPrentice-Hall (1981) 79-101. 5. Allen, John R. and Kennedy, Ken. PFC: a program to convert Fortran to \nparallel form. Rice MASC TR82-6 (March 1982) 63 pages. 6. Banerjee,U. Data dependence in ordinary programs. \nUniversity of Illinois Department of Computer Science Report 76-837 (November  1976). 7. Carter, J. \nLawrence. A case study of a new code generation technique for comPilers. CACM 20, 12 (Dec. 1977) 914-920. \n 8. Chaitin, Gregory J.; Auslander, Marc A.; Chandra, Ashok K.; Cocke, John; Hopkins, Martin E.; and \nMarkstein, Peter W. Register  allocation via coloring. Computer Languages 6 (1981) 47-57. 9. Cocke, \nJohn and Kennedy, Ken. An algorithm for reduction in strength. CACM 20, 11 (Nov. 1977) 850-856. 10. \nCocke, John and Schwartz, Jack. Program\u00adm ing Languages and Their Compilers, C ourant Institute, New \nYork University (1970). 11. Dennis, Jack B. Data flow supercomputers. IEEE Computer 13, 11 (Nov. 1980) \n48-56. 12. Frailey, Dennis J. Expression optimization using unary complement. (Proc. of a Symposi\u00adum \non Compiler Optimization) ACM SIGPLAN Notices, 5, 7 (July 1970) 67-85. 13. Hecht, Matthew S. Flow Anaijsis \nof Computer Programs. Elsevier-North Holland, Inc. (1977) 232 pages. 14. Kuck, D. J; Kuhn, R. H., Padua, \nD. A.; Lea\u00adsure, B. and Wolfe, M. Dependence graphs and compiler optimizations. Conf. Record of the 8th \nAnn. ACM Symp. on Print. of Prog. Lang. Williamsburg, VA (Jan. 26-28, 1981) 207-218. 15. Kuck, David \nJ. The Structure of Computers and Computations, John Wiley &#38; Sons (1978) Chapter 2. 16. Montz, Lynn \nBarbara. Safety and optimiza\u00adtion transformations for data flow programs. MIT LCS TR-240 (Jan. 1980) \n85 pages. 17. Ottenstein, Karl J. A fast algorithm for scalar propagation and constant folding. Dept. \nof  Math. and Computer Sciences, Mich. Tech. Univ. (August 1982). Submitted for publica\u00adtion. 18. Ottenstein, \nKarl J. An intermediate program form based on a cyclic data-dependency graph. CS-TR 81-1, Dept. of Math. \nand Computer Sciences, Mich. Tech. Univ. (Oct. 1981) Sub\u00admitted for publication. 19. Ottenstein, Karl \nJ. Data-flow graphs as an intermediate program form, Ph.D. Thesis, Computer Sciences Dept., Purdue Univ. \n(August 1978) 283 pages. 20. Pottosin, I. V. Economy of expressions in the alpha-translator, appearing \nin The ALPHA au\u00adtomatic programming system (A. P. Yershov, ed.; J. McWilliams, trans. ) Academic Press \n(1971) 149-159.  21 Standish, T. A., Harriman, D. C., Kibler, D. F. and Neighbors, J. M. The Irvine \nProgram Transformation Catalogue. Dept. of Info. and Computer Science, Univ. of Calif, Irvine (Jan 7, \n1976) 82 pages. 22. Treleaven, Philip C,; Hopkins, Richard P. and Rautenbach, Paul W. Combining data \nflow and control flow computing. The Computer Journal 25, 2 (1982) 207-217.  23. Urschler,G. Complete \nredundant expression elimination in flow diagrams. IBM Research Report RC 4965 T.J. Watson Research Center, \nYorktown Heights, N.Y. (August 1974). 24. Wegbright, B. Property extraction in well founded property \nsets. Software Engineering (September 1975) 270-285. 25. Wegman, Mark. General and efficient me\u00ad  thods \nfor global code improvement, Ph.D. The\u00adsis, Univ. of Calif., Berkley (1981). 26, Wulf, William; Johnsson, \nRichard K.; Wein\u00ad stock, Charles B.; Hobbs, Steven O.; and Geschke, Charles M. The Design of an Optim\u00adizing \nCompiler. Elsevier ( 1975) 165 pages. Acknowledgments. The authors Corporation for given them by Bruss, \nand Mark appreciate the support of the IBM this work and the helpful comments Fran Allen, Marc Auslander, \nAnni Wegman. 231 ,,uy r S1 P1 r S2 2 T. + S3 S6 7 Sll P6 T T+ S13 S9 Slo S12 Figure 1-PredicateRegions \n eo\u00adltry ! ----\u00ad ,, ---\u00ad 8 -----\u00ad \\ -----) c ! ,I 7 I Figure2-PartialedfgBeforeCodeMotion * Itry ,, \n-----\u00ad $, ----\u00ad,c \\ ? ( ,1 1, 1( 11 II 1\u00ad Figure3-PartialedfgAfter CodeMotion Appendix: Code Motion \nAlgorithm. ALGORITHM: CODE MOTION 1. Clear all marks on all nodes.  2. Initialize all operand dependency \n(OPDEP) sequences to <>. 3. For each node n representing a program output, MOVE(n):  procedure MOVE \n(n: graph_ node); procedure CHANGE_ CONTR OL; begin delete the control edge (c,n); cp : = c s control \npredecessor; add the edge (cp,n) with the same labels as the control edge (cp,c); c := Cp end f CHANGE_ \nCONTR OLj; procedure FACTOR (var OPDEP: operand_sequence); begin create a new node s labelled as n; \n c : = n s control predecessor; add the edge (c)s) labelled as (c,n); for each op in OPDEP do begin \nadd the edge (op, s); delete the edge (op,n) end;  add the edge (n,s); j Clear OPDEP since after factoring \nn has no operand dependencies on c. j OPDEP := <> end {FACTOR ]; begin if (n is not a constant) and \n(n s predecessor is not an input procedure) and (n k not marked as moved ) then begin mark n as moved \n; for each data or control predecessor, p, of n do MOVE (p); for each data predecessor d of n with \ncontrol predecessor c do add d to OPDEP,; {Move out of any enclosing repeat or while loops in which \nthe computation n is invariant. Where part of n is not invariant, that part is factored out and made \ndepend\u00adent upon the remaining part which is moved further if possible.] let c be the control predecessor \nof n; if (n is an associative operator) then begin can_move_further : = true; while ((c,n) is marked \n* or + ) and (can_move_further) do begin {See Note 1.} append OPDEP, to OPDEPC for all 1 in the predicate \nsubtree rooted at c; if (OPDEPC # <>)and ((indegree(n) -I OPDEPC ] ) z 2) then FACTOR (OPDEPC); if OPDE.PC \n= <> then if ((c,n) is marked ) then CHANGE_ CONTR OL else delete the + on (c,n) else can_move_further \n: = false end end else begin {not an associative operator] {See Note 1.] while ((c,n) is marked * ) \nand (OPDEP, = <> for all 1 in the predicate subtree rooted at c) do CHANGE_ CONTR OL; if ((c,n) is marked \n+ ) and (OPDEPI = <> for all 1 in the predicate subtree rooted at c) then delete the + on (c,n) end; \n~See Notes 2, 3, 4.] if (c is a reference node ) and (loop(c) # entry ) then begin lC:= c; p := c s \ndata predecessor; Ip := loop (p); can_move_further : = true; while (IC # lp) and can_move_further do \nbegin Ic : = loop (lC); append to OPDEP,r all OPDEPi in the predicate subtree rooted at lc; if n is \nan associative operator then begin if (OPDEP,C # <>) and ((indegree(n)-10 PDEP,C I ) 2 2) then begin \nFACTOR (OPDEP,C) ; make ref(lc,p) the control predecessor of n end; if OPDEPIC # <> then can_move_further \n: = false end else {not associative] if OPDEP)C # <> then begin make ref(lc,p) the control predecessor \nof n; can_move_further : = false end; end {while]; end; {If a predicate computation has been moved \naway from its region of use as a predicate, create a reference node in the old region.] with n the root \nof the (possibly) factored expression above do if (n is the root of a predicate computation) and (n has \nbeen moved above) then begin create a reference node r; make all of n s control successors r s successors; \nmake r the only successor of n by a data-dependency edge; add an edge between n s original control predecessor \nand r labelled exactly as the original edge to n had been end; foreach data predecessordof n with control \npredecessorcdo OPDEPC : = <>; end {if]; end {MOVE}; Note 1---When a subtree of the predicate tree is \nexamined, its root is marked so that the subtree will not be considered again in the current search. \nNote 2---The function loop(p) returns the nearest ancestor of p which is a loop predicate. If no such \nancestor exists, entry is returned. This function is computed in unit time from the threaded predicate \ntree. The function indegree(n) returns the number of data dependency edges incident to n. Note 3---If \nc, the control predecessor of n, is a reference node, we know that c s data predecessor p computes the \nvalue of the predicate in a region as near the root of the predicate tree as possible. The computation \nn (or a factored component of it) may be movable to a region defined by p or it may only be movable to \na region defined by another reference node to p which, if it doesn t exist, would be created in a region \nbetween c s parent region and p s parent region. If p and c are in the same immediate loop, no movement \nis profitable. Note 4---Function ref(l,p) returns the reference node which is data dependent on p in \nregion 1, creating a new node if none exists. Any new node created by ref would be added to the predicate \ntree as a child of 1. When a reference node is created due to the movement of a predicate computa\u00adtion, \nthat reference node replaces the predicate in the predicate tree. If, at any time during move\u00adment, any \npredicate in the predicate tree no longer has any successors, it is deleted.   \n\t\t\t", "proc_id": "567067", "abstract": "A new program representation is presented which permits certain optimizations to be performed at less expense than with other forms. This paper describes code motion, common subexpression elimination and induction variable detection. Scalar propagation and constant folding are sketched here, but detailed elsewhere. The powerful code motion strategy allows entire regions of the program to be moved. The representation described may be used as a compiler intermediate form or simply as a model for program analysis. It has great potential for use in translation for parallel machines.", "authors": [{"name": "Jeanne Ferrante", "author_profile_id": "81100357275", "affiliation": "IBM T. J. Watson Research Center, Yorktown Heights, New York", "person_id": "P137070", "email_address": "", "orcid_id": ""}, {"name": "Karl J. Ottenstein", "author_profile_id": "81100369516", "affiliation": "Michigan Technological University, Houghton, Michigan", "person_id": "P157239", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/567067.567089", "year": "1983", "article_id": "567089", "conference": "POPL", "title": "A program form based on data dependency in predicate regions", "url": "http://dl.acm.org/citation.cfm?id=567089"}