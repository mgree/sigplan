{"article_publication_date": "01-24-1983", "fulltext": "\n Permission to make digital or hard copies of part or all of this work or personal or classroom use is \ngranted without fee provided that copies are not made or distributed for profit or commercial advantage \nand that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, \nto post on servers, or to redistribute to lists, requires prior specific permission and/or a fee. &#38;#169; \n1983 ACM 0-89791-090-7 $5.00 Silicon Compilation The term Silicon Compiler has been applied(1) to various \nhardware tools, although the term is very vague. Some of these pro\u00adgrams encourage or require graphical \ninterac\u00adtion between the program and the designer, in a way most unlike conventional compilers. Other \nprograms are more like assemblers; the designer is given a convenient way of specify\u00ading a particular \ndesign for a particular process technology, and that is all. Other systems (MACPITTS, for example)(2) \nbuild chips from a higher-level description of the desired behavior, and may even be able to fabricate \nthe hardware in several different technologies. It is this latter class of programs, by analogy with \nsoftware, that we will term Silicon Com\u00adpilers. Assuming (and this is a big if) that there is a description \nof the desired circuit, how do we reduce this description to hardware. We can distinguish: 1. Lexical \nAnalysis and Parsing 2, Control Flow Analysis 3, Data Flow Analysis 4. Common Subexpression and Algebraic \nIdentity Recognition 5. Resource Allocation 6. Code Generation 7. Peephole Optimization  These terms \nhave been deliberately chosen to emphasize, the similarity of these tasks to the parts of conventional \ncompilers. The lexical analysis and parsing issues are truly identical, and will not be further treated. \nThe other issues will be treated in a following section. Data and Control Flow Many digital circuits \nconsist of a data\u00adpath, consisting of registers, adders, etc., and a control unit controlling the movement \nof data in the datapath. After the input language has been parsed, it is necessary to understand what \ndata operations will be done (data flow) and how they are to be controlled (control flow). In general, \nthe data opera\u00adtions share functional units and communica\u00adtion paths (e.g., adders, busses, registers); \nto do this correctly, the control flow must be well understood. As a simple example, in a microcom\u00adputer \nwe may wish to add to the program counter. Sometimes this is done by including a special adder closely \ntied to the program cmnter register; in other cases, the addition is done in the main arithmetic unit. \nThe first solution takes more hardware, the second more time. Should we use high-level design languages, \nwhere the compiler itself would make such decisiornls, or low-level design languages, where these decisions \nare made by the designer? One recalls the debates in soft ware about vectorization: should vector operations \nbe part of the programming language, or should the compiler recognize patterns of scalar references that \ncan be vec\u00adtorized? In either case, the arithmetic unit should not be asked to update the program counter \nwhen some (other operation is simul\u00adtaneously in progress. To ensure this, flow analysis techniques can \nbe used to decide when computed quantities are generated and used, The above exa]mple shows a spacdtime \ntradeoff familiar fro]m software. If we wish to do two or more things, we can do them sequentially with \nless hardware, or in parallel with more. Because several data and control paths are usually active at \nonce, the perfor\u00admance can be tuned; the slowest paths can be made faster by adding power to thcrn, while \nthe faster paths can be slowed down some\u00adwhat to save power, without affecting the total system performance. \nThe most subtle and important parts of the design process are probably in this area. Recognizing which \npaths are critical is essential to tuning the design, but typically cannot be done until much of the \ndetailed design is completed. This argues strongly for a high-level approach (e.g., a compiler) that \nwould allow many iterations through the design; a similar view has won out in soft ware. In order fc)r \nthis iterative process to be successful, it is essential that tlhe design language and the compiler be \natnle to do something sensible to the critical paths when they have been founcl; many silicon compilers \n(and, for that matter, programming language compilers as well) give little help to the user who learns \nby measurement where the slow parts are, only to find that improving the speed requires throwing away \nthe compiler ! 15 Common Subexpressions and Algebraic Iden\u00adtities Suppose some boolean expression of \ninput signals is used several different places in the circuit. The expression could be recom\u00adputed each \ntime it is needed, or it could be computed once, and the value distributed to the places that use it. \nThe first solution costs additional hardware, but gains speed; the second saves hardware, but is typically \nslower. In software, we see a similar problem in register allocation, where if we leave a value in a \nregister to be reused later, we can\u00adnot use the register for other purposes in the meantime. However, \nthere are important differences. In software, a common subex\u00adpression may be used any number of times \nwithout significant penalty, while in hardware, each use of a signal slows down the circuit, increases \nthe power requirements, or both. It is possible to be too successful at recognizing common subexpressions! \nHaving recognized common signals, a wire must be used to distribute the value across space, or a register \nor latch to distribute the value across time. This may cost more than recomputa\u00adtion. Algebraic (boolean) \nidentities appear to be more useful in silicon compilation than in programming language compilation. \nA simple example will be revealing. In nMOS, we pay a certain price for a device called a pullup; (the \ncost is in terms of electrical power). Having made a pullup, we can qonstruct a boolean function by pulling \nthe output signal down to ground through a network of transis\u00adtors controlled by the input signals. For \neach pullup, we compute a function of the general form NOR( exp, exp, ... ) where each exp has the form \nx&#38; y&#38;.., Note that this includes the NOT and NAND functions as special cases. Suppose we are \ninterested in computing the expressions: t= a&#38;b ~=!c&#38;t Taking a naive approach, we can write \nthis in terms of the NOR functions as t= NOT( NAND( a,b)) x = NOT( NAND( NOT( c), t) ) ) This costs us \nfive pullups. Now, suppose t is an intermediate variable, and it is used only the one time. Then we may \nsubstitute: x = NOT(NAND( NOT(C), NOT(NAND(a,b)) )) Using deMorgan s laws and simplifying, we obtain \nx = NOR( c,NAND( a, b)) which costs only two pullups. Another way of looking at this is that we have \nsimply implemented t in negative logic (e.g., O is true, one is false). These techniques are similar \nto those used in standard compiling, although the cost function, and the relative importance of vari\u00adous \noptimizations, are rather different. In effect, nMOS code generation looks like code generation for a \nmachine with one opcode (NOR), many different arities (unary, binary, etc.), and a variety of address \nmodes (x, x &#38; Y7 .,. ). Resource Allocation After the basic architectural decisions have been made, \nit is necessary to allocate resources to carry out the functions of the cir\u00adcuit. The resources to be \nallocated may be as diverse as wiring channels, chip sockets, transistors, power, clock phases, etc. \nMuch of the standard compiler lore passes over immediately and, it appears, successfully. For example, \nthe Sethi-Ullman(3) rule that the hardest portion of an expression should be the first to be compiled \nseems appropriate in hardware as well. Consider the problem of connecting two hardware structures together, \nsay an adder and a register. One approach is to optimize the desired properties (power, area, speed, \nor whatever) for the register and adder separately, and then connect them together with a general wire \nrouter. The problem, of course, is that the resulting structure may waste quite a lot of space in the \ninterconnec\u00adtion. At the other extreme, the desired struc\u00adture may be flattened into a collection of \nlow-level gates and wires, and the combina\u00adtion re-designed; in this case, the original sub\u00adstructures \nmay no longer be recognizable when the larger structure is made. In this case, the compilation time rises, \nthe properties of the combination are no longer evident from the properties of the substructures, and, \nmore subtly, the very large combinatorial problems that arise make it likely that heuristic solu\u00adtions \nwill be far from optimal. A number of intermediate solutions have been proposed; for example, Bristle \nBlocks(4) allows substructures to stretch inter\u00adnally to adapt to one another, avoiding expen\u00adsive interconnection \nwiring. Other systems, such as Mulga(5,6) and the Sticks system(7) allow the designer to deal with interconnec\u00adtion \nproblems symbolically; the system com\u00adpacts the designs automatically. The analogy with procedure call \nmechanisms in software is immediate. In both cases, the desire is to create a hierarchy of abstractions, \nmaking both the design and compilation time more manageable. In software, a cost is associated with establishing \nand maintaining the procedure model: regis\u00adters must be saved and restored, a stack\u00adframe maintained, \netc. Also, the cost of cal\u00adling a procedure depends critically on the semantics of the programming language; \ncall\u00adby-value is typically cheaper than call-by\u00adname, and the presence of ON-conditions and displays \ncan increase the cost of subroutine call in certain cases. The tradeoffs, and the lore, associated with \ninter-procedural optimi\u00adzation appear to apply directly to the design of hierarchical languages for VLSI. \nTo draw just one more analogy, a number of the allocation problems that arise in hardware are mathematically \nsimilar to register allocation problems in compiling. Ordinarily, we have a limited number of registers, \nand do large computations by arranging to share the registers among many different computations. Two \ncomputations that need not both be available at the same time may share the same register; this leads \nnaturally to an analogy with graph coloring that has been exploited(8) in a practical com\u00adpiler. Similar \ntechniques apply, at least for\u00admally, to the allocation of wiring channels, clock phases, and bus utilization. \nValues may share latches, busses, or functional units if they do not use them simultaneously. There are \nsome major differences, however. Whereas most computers have a fixed number of registers available to \nthe compiler, in hardware it is usually possible to increase the available resources albeit at some cost. \nA more serious problem arises from the issue of parallelism. In the typical von Neu\u00admann machine, only \none instruction is being executed at a time; this simplifies the interac\u00adtions between the values computed \nby the pro\u00adgram. Parallelism in hardware is not only possible, it is the rule! If a piece of hardware \ncould meet its speed constraints by doing only one thing at a time, there would be little rea\u00adson to \nbuild custom hardware; a microproces\u00adsor or microcontroller would be cheaper to design and build, and \nbe. sufficiently fast. While there have been .a number of tech\u00adniques suggested in software for dealing \nwith parallelism, the confusion that reigns over such simple issues as array and vector opera\u00adtions suggests \nthat the issue will not be resolved quickly. Code Generation At the lowest levels of the design, it is \nnecessary to construct portions of the circuit (registers, gates, etc.) out of basic elements (transistors, \nwires, etc.). This may be possi\u00adble in several tiifferent ways, differing in their speed and power requirements. \nFor example, a= AND(b, c) cannot be directly implemented in nMO$ it must either be done as a = NOT( NAND(b,~) \n) (taking two pullups) or a = NOR( NOT(b), NOT(c) ) which takes thlree PUIIUPS. The second form typically \nruns faste~ thin the fiist, but takes more power. When common subexpressions are introduced, the situation \nbecomes more complex; if there were several uses of NOT(c), for example, the difference between these \ntwo forms becomes less marked. More\u00adover, some elements (e.g., static latches) tend to make availa~ble \nNOT(x) as a side effect of storing x. The best way to make such trade\u00adoffs is to know the critical tilming \npaths, and attempt to make the decisions to tune the cir\u00adcuit performance. The analogy is to variables \nthat want to be kept in registers because they are frequently referenced, or referenced inside loops. \nAs with conventional compilers, 17 there is a communication issue to be addressed; global path information \nmust be collected, remembered, and made available to the code generator so that these local deci\u00adsions \ncan be made effectively. Peephole Optimization Circuits produced by silicon compilers can often be improved \nby local changes, just as with conventional compilers. Two ele\u00adments that are logically or physically \nclose in the final design may have been generated at quite different times in the compiler, and the compiler \nmay fail to take adequate account of their interaction. An improvement phase that looks for local patterns \nand improves them is straightforward to write, and can be quite effective. It may be possible to address \nsome glo\u00adbal timing issues by local modifications as well. If one has identified the critical paths, \nit may be possible to improve the speed of the circuit by local modifications along these paths that \ndo not affect the overall compila\u00adtion strategy. This technique has been applied to compilers for gate \narrays(9) and should be appropriate for most silicon com\u00adpilers; the extent of the improvement will depend \non the compilation model and the intelligence of the earlier compiler stages. Practical Experience This \nsection discusses our experienw with a silicon compiler research project, dur\u00ading which many of the above \nissues came up. Our initial goal was to compile boolean equa\u00adtions into nMOS, using a variant of the \nMead-Conway design rules. It was a simple matter to extend the boolean input language to include simple \nmemory elements, so we could describe finite-state machines. In hindsight, we can now identify several \ndifferent stages in this work. Initially, we concentrated on fabricating individual cells that were area \nefficient; this lead to a sequence of increasingly unpleasant geometric problems, as we tried to cram \nmore and more logic into the same space. It became clear that we were winning the battle and losing the \nwar, since the wiring required to connect these cells was coming to dominate the design. To permit users \nto make designs containing many such cells, we then went to a more rigid geometry, constructing cells \nthat would easily butt together on all four sides, but giving less efficiency within each cell. The accent \nwas still on generating pieces of the design so that a human being could assemble them. At about this \ntime, Stu Feldman and Sandy Fraser specified a high-level design , language called xi, and a compiler \nwas con\u00adstructed. One of the targets of this compiler was the boolean circuit generator; this in turn \nmade a number of changes necessary. The compiler could produce much more compli\u00adcated wiring patterns \nthan any human had patience with, so our routing program, con\u00adstructed by Peter Weinberger, had to become \nmore intelligent. It was also convenient at this time to tell the router how to put the input and output \npads onto the chip, so com\u00adplete designs could be done (albeit ineffi\u00adciently) without human intervention, \nFinally, we taught a short course in which program\u00admers with no silicon design experience were able to \nspecify medium-sized circuits of interest to them. Over two dozen different circuits have been fabricated \nusing the boolean equation compiler and the xi language, These circuits include everything from tiny \npieces of real cir\u00adcuits to entire chip-level projects. At this point, we are entering a phase of worrying \nabout the speed of our designs; the global tim\u00ading issues appear to be best addressed at the xi level, \nand we do not forsee much addi\u00adtional use for the boolean equation generator except as a target for xi. \nsummary Throughout the foregoing project, we took the attitude that writing a compiler for silicon was \njust like writing a compiler for anything else. Of course, not being skilled circuit designers, we had \nlittle choice. Nevertheless, it was striking how many of the instincts of compiler writing appeared to \ncarry over to this exciting and important area. If that indeed proves to be true, compiler writ\u00aders will \nhave yet another hedge against unem\u00adployment. Acknowledgements My colleagues at Bell Laboratories have \nbeen most stimulating collaborators and wil\u00adling users; special thanks to Mischa Buric, Stu Feldman, \nSandy Fraser, Hania Gajewska, Mike Maul, Danny Sleator Tom Szymanski, Brian Kernighan, and Peter Weinberger, \nReferences 1. Werner, J., The Silicon Compiler: Panacea, Wishful Thinking, or Old Hat? VLSl Design, Sept/Ott, \n1982, pp. 46-52 2, Siskind, J. M., J. R. Southard, K. W. Crouch, Generating Custom, High-Performance \nVLSI Designs from Suc\u00adcinct Algorithmic Descriptions. Proceedings of the Conference on Advanced Research \non VLSI, Artech House Inc., 1981, 3. Sethi, R., J. D. Unman, The Genera\u00adtion of Optimal Code for Arithmetic \nExpressions, J. Assoc. Comp. Mach. 17,4, Oct. 1970, pp. 715-728 4. Johansson, D. L., A Design Technique \nfor VLSI Chips. Proceedings of the Cal\u00adtech Conference on VLSI, 1979. 5. Weste, N. H. E., Mulga-An Interac\u00adtive \nSymbolic Layout System for the Design of Integrated Circuits. BeU Sys\u00adtem Technical Journal, 60, #6, \nJulylAug. 1982. 6. Weste, Neil Virtual Grid Symbolic Layout. Proceedings of the 18th Design Automation \nConference, 1980, pp. 225\u00ad233 7. Williams, J., STICKS-A Graphical Compiler for High Level LSI Design. \nProceedings of 1978 NCC, May 1978, pp. 289-95, 8. Chaitlin, Gregory J, et. al., Register Allocation \nvia Coloring. Computer Languages, 6, pp. 47-57. 9. Darringer, J. A., W. H. Joyner, Jr., C.  L. Berman, \nL, Trevillyan, Logic Syn\u00adthesis Through Local Transformations , IBM Journal of Research and Develop\u00adment, \nJuly, 1981 \n\t\t\t", "proc_id": "567067", "abstract": "Compiler writers facea bleak future---soon everyone will be using Ada on the Nebula Architecture and there will be no further need for our services. Luckily, it appears to be possible to recycle compiler writers by encouraging them to build circuit design tools (Silicon Compilers). Many of the standard techniques used in compilers can be immediately applied in circuit design, especially for LSI circuits. In part, this paper attempts to serve as a tutorial, casting some of the problems of designing in nMOS with the Mead-Conway design rules into the vocabulary of the compiler writer. The paper also discusses experience with a Silicon compiler that has fabricated over two dozen different designs.", "authors": [{"name": "S. C. Johnson", "author_profile_id": "81332506933", "affiliation": "Bell Laboratories, Murray Hill, New Jersey", "person_id": "PP48024621", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/567067.567070", "year": "1983", "article_id": "567070", "conference": "POPL", "title": "Code generation for silicon", "url": "http://dl.acm.org/citation.cfm?id=567070"}