{"article_publication_date": "01-24-1983", "fulltext": "\n Permission to make digital or hard copies of part or all of this work or personal or classroom use is \ngranted without fee provided that copies are not made or distributed for profit or commercial advantage \nand that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, \nto post on servers, or to redistribute to lists, requires prior specific permission and/or a fee. &#38;#169; \n1983 ACM 0-89791-090-7 $5.00 The goals of transformational programming are to reduce programming labor. \ntmprove program rehabdlty, and uP9rade program performance In order for labor to be reduced, the effort \nrequred to obtain P, prove It correct, and derive P by transformation should be less than the effort \nrequired ,to code P from scratch. and also to debug n Program reliability wdl be Improved If P can be \ncertlfled correct, and lf each transformation preserves program meanmg Finally, program performance WIII \nbe upgraded if transformations are drected towards increased efficiency, Experimental transformational \nsystems that emphasize one or more aspects of the methodology outhned above have been Implemented by \nCheatnam [51, Darllngton [3], Loveman [271, Standish [41], Feather [14] Huet and Lang [11], and others \nHowever, all of these systems fall short of the goals, because of a number of reasons that include, 1 \nInaullity TO mechamze the checking of transformation appllcablhty conditions 2 reliance on large, unmanageable \ncollections of low level transformations, and long arduous derivation sequences 3 dependency on transformations \nwhose potential for Improvmg program performance is unpredictable 4 use of source languages msufflclently \nhigh level to accommodate perspicuous mltlal program specifications and powerful algorvthmlc transformations \nYet, convincing evidence that thm new methodology WIII succeed has come from recent advances m verlflcatlon, \nprogram transformations, syntax directed edlttmg systems, and high level languages These advances, dlecussed \nbelow, represent partial sol~timm to the problems stated above. and could eventually be integrated mto \na single system 1 The transformational approach to verlflcatlon was lmoneered by Gerhart [19] and strengthened \nby the results of Schwartz [39], Scherlls [36], Broy et al [2], Koenjg and Palge [26, 31] Blausteln [1], \nand others Due mamly to Improved technology for the mechanization of proofs of enabling conditions that \njustify application of transformations, this a~proach IS now at a point where It can be effectively used \nr a system Such mechamzatlon depends strongly on program analysfs, and, m particular, on reanalyses after \na program IS modlfled Attribute grammars [24] have been showr to be espemal!y usefui r facd(tatmg program \nanalysls [23] Moreover, Reps [34] has discovered an algorlthm that reevaluates attributes m optima! ume \nafter a program undergoes syntax directed edlttmg crranges (as are allowed on the Cornell Synthesizer \nE43]1 He has Impiemsmted hls algorlthm recently, ano has reported mtm success 2 There are encouraging \nlndlcatlons that a transformational system can be made to de~end mamly on a small b~t powerfui collection \nof transformations applied top-down fashion to cmograms speclfled at various levels of aostract[on from \nlogic down to assembler We envlslon such a system as a farly Conventional semiautomatic comprler m which \nclasses of transformations are selected sem(mechanlcally m a predetermined order, and are justrfled by \npredicates supplled mechanically bu: proved semlmanually Of particular Importance ts nondetermlnlsm removal \nwhich as formulated b} Sharrr [40~ could Ieac to a technique for turning nawe, nonctetermmstlc programs \ninto deterministic programs with emergent strategies Such programs could then be transformed automatically \nby fmlte dlfferencmg [13, 16, 17, 18, 29 30, 31] and jammmg [28, 31, 20] (which we have Implemented) \nmto programs whose data access paths are fully determmed The SETL optlmlzer could Improve these programs \nfurther by automatically cnoosing efflc,ent data structure representations and aggregations 3 Of fundamental \nImportance to the transformations Just mentioned is the fact that they can be associated w!th speedup \npredictions Fong and Unman [16] were the first to characterize an Important class of algor!thmlc dlfferencmg \ntransformations m terms of accurate asymptotic speedup predictions, eg, they gave conditions under which \nrepeated calculation of a set former {x m slk(x)} could be computed m O(#s) + cost(k) steps By considering \nstronger condmons and special cases for the boolean valued subpart k, Paige [31] later gave sharper speedup \npredictions (eg, either O(1) steps for each encounter of the set former or a cumulatwe cost of O(#s) \nsteps for every encounter) associated with another dlfferencmg method Both Morgenstern [28] and Palge \n[31] prove constant factor Improvements due to their )ammmg transformations (implemented by Morgenstern \nfor the Improvement of fde processing, and by Palge for the optimization of programs) Constant factor \nspeedup has also been observed for data structure selection by the method of basmgs bu? a supporting \nanalytjc study has no? been presented [8, 37] 4 Essential to the whole transformational process IS a \nwide spectrum Rrogrammmg language (or set of languages) that can express a program at every stage of \ndevelopment from the mltlal abstract specification down to Its concrete Implementation realization Since \ntransformations applied to programs written at the highest levels of abstraction are likely to make the \nmost fundamental algorithmic changes, It IS important to stress abstract features r our language In addition \nto supporting transformations, the highest level language dictions should support lucid inltlal speclflcatlons. \nverlficatlon, and even program analysts Of special Importance IS SETL [38, 9], because its abstract set \ntheorettc dlctlons can model data structures and algorithms easily, because its philosophy of avoiding \nhidden asymptotic costs facilitates program analysm, because its semantics conforms to finite set theory \nand can accommodate a set theoretic program logic, and because it is wide spectrum, As is evidenced by \nthe work of Schwartz, Fong, Paige, and Sharir, SETL is also a rich medium for transformation, Il. Main \nResults The original contributions of our work are listed below i. Our mam result is the implementation \nof a prototype transformational programming system that incorporates several of the Ideas mentioned above \nThis system. called RAPTS (Rutgers Abstract Program Transformation System) [321. supports the semiautomatic \ndevelopment of reliable and efficient sohvare using source to source program transformations for an abstract \nvariant of the SETL language. Like the prior transformational systems of Cheatham [4], Standish [41], \nLoveman [27], Darlmgton [3], and Feather [14] RAPTS has modules to perform parsing, unparsing (i.e., \nprettyprinting), search, and transformation application: i? can manipulate libraries of transformations, \nsource programs, and program development states; RAPTS provides 2 variety of user aids, also, global \ncontrol flow and data flow analysls are used to prove the apphcabihty conditions of our transformations \nautomatically However, our system emphasizes the strict stepwise refinement of programs by successive \napplications of powerful correctness preserwng transformations that can be selected, justified, and applied \nwith a much greater degree of mechanization than other systems. We have used RAPTS for experimenting \nwtth algorithm derivation, system construction, and automated database processing. An important conceptual \nadvantage m using SETL as both system implementation language and source language is that RAPTS can be \nused to improve Itself, as was done for its dead code elimination procedure. In Appendix lil we show \nhow an meffictent but clear abstract specification of this procedure is transformed mto a lower level \nSETL variant that runs m Imear time with respect to the use-to\u00ad def links Moreover, the transformational \napproach to verification together with appropriate assert!on control could be used within RAPTS to prove \nItself correct il. RAPTS uses a finite differencing method [33] that genarallzes John Cockes strength \nreduction [61. and provides an efficient implementation of a host of transformations including Jay Earley \ns Iterator mverslon [13]. Our differencing algorithm is an outgrowth of less effuent and less general \nalgorithms due to Cocke, Schwartz, and Kennedy [6 7] The reduction in strength algorithms found m [6 \n7] execute in O(n) steps (where n is the number of nodes m the flow gZiDh of a program IOOPI for a single \n~ass However, their algorithm and also the algordhm used by Paige and Koenig [31] takes 0m**2) steps \nm the worst case to compute, due to successive linear time passes over programs growing successively \nlarger Our new algorithm only equres a single pass, and execu~es in Oh steps overali We obtain this improvement \nby detection of all reducible expressions (that woula be detected w{thm multiple passes of the classical \nalgorithms) in advance of any transformational steps Our algorithm gains greater generality by accepting \ndifferencing transformations as input Based on these differencing rules, we automatically determine categories \nof variable modifications upon which we can detect expressions amenable to reduction Thus, differencmg \ntransformations can be apphed over a wider range of data types than prewously possible. However, m thw \npaper we will stress the important application to set theoretic expressions, first observed by Earley \n[13], Fong [17] first presented an algorithm to implement a subset of Earley s transformations, and her \napproach varied frOm his and Cockes approach by using a deferred update strategy. She also gained more \ninformation by analyzing program paths instead of loops However, her algorithm ran in ttme proportional \nto e log e bit vector operations, where e is the number of edges in the program flow graph. Furthermore, \nlike the classical strength reduction algorithms, her algorithm must be reapplied over programs growing \nsuccessively larger. (As in the case of the classlcal algorithms, this problem !s due to the fact that \nreduction of one expression f can make another expression g(f), which depends on f, reducible; as was \nfirst noted by Cocke and Schwartz [6] and solved by Cocke and Kennedy [7], reduction of f can also introduce \nnew auxihary expressions that must be further reduced.) Even the improvement of Fongs algorithm by Tarjan \n[42] and Rosen [35] to almost linear time in e bit vector operations per pass falls to make it a viable \ncompetitor to the classical approach or our new improvement. It remams an mterestmg open problem whether \nthe path analysis approach introduced by Fong can be modified mto a single pass algorithm without losing \nasymptotic efficiency. We conjecture that this problem can be solved affirmatively. Further comDarlson \no? her worh with ours can be found m [310 Like Fong and Unman [161 our application of set theoretic differencmg \nE based upon reasonable condmons for ensuring asymptotic speedup Although our current implementation \nincludes about 50 groups of concrete differenctng rules [31], recent theoretical improvements provide \nfor a much more compact collection of meta rules (see Appendix 1) that are as easy to specify and implement \nas our current rules, and even more general than those proposed in [30]. iii. RAPTS can perform new and \npowerful set expression Jamming transformations implemented by a hnear time algorithm [30, 31], A much \nmore powerful algorithm than what is currently implemented and that yields optimal expression jamming \nis found in [20]. iv We have designed (but not yet Implemented) a new way to mechanically estimate the \nasymptotic speed of an algorithm derwed by transformation within FIAPTS. Related to this is the earlier \nwork of Wegbreit, who discussed a transformational system that mechanically analyzed the performance \nof Lisp programs as they were Improved by transformation [44], Because we are observing programs specified \nat a h!gher level of abstraction than Wegbreit, and because our transformations deal with more fundamental \nalgorithmic program Improvements, we obtain more global mformatlon. v. We specify our mmal abstract program \na? an unusually high level of abstraction beyond current standard SETL. Illustrations will be included \nin the next section and Appendix Ill RAPTS incorporates an Implementation of a class of abstract static \nto dynamic expression transformations that generalize Earleys itera~or inversion, (See Appendix II for \na samphng of these transformations ! 2. RAPTS Illustrations It Is. perhaps, most convement to explain \nthe transformational capabilities of RAPTS by example, using photo generated excerpts of an actual RAPTS \nderruatlon of to~ologjcal sorting (an example f!rst considered by Eariey [13]) Before proceeding, the \nreader may find it helpful to consult the brief description of SETL operations and their estimated computational \ncosts (based on obvious hash table implementations for sets and maps) gwen [n table 1. ripe-at 10n Remarks \nEst, mated Cost s Wlttl:. x element ada?tlon o(i) s les s,= x element de , etlon 0(1) x>ns 3eT member \nshl F 0(1) s += delta set addltlon 0(.-de ltal s -= ae-, la set ae letlon O(ndelta) f(x) . . y indexed \nmac assl~nment 0(1) -[xl, .,xnl function retr leva? O(n) 1 feral> x In s] fo:-al 1 low 0(. ? % Ccst(aloc.1 \n1 610ck, x) ena forall 1A lr, s I k(x)} set former 0[-s > CcSt (k) ) exists x in s I k(x) existent, \na , quantifier O(PS x Cost(k) ) ?o-all x in s I k(x) universal quantlf, er O(FS X cost(k) ) s~t set union \n0(. s + =tl Z.T set 7ntei-sect70n m~n(O(*sl. Oi *T)) s-t set difference 0(? s) +[s1 image set 0(, .f) \nTABLE : Complex {t> Est~mates o? Setl Operations our initial algorithm specification inputs a set s and \na set of pairs sp representing an Irreflexwe transitive predecessor relation defined on s; as output, \nIt produces a tuple t m which the elements of s are arranged in a total order consistent wkh the partial \norder sp. sp maps each element x of s mto the set sp {x} of predecessor elements The algorithm proceeds \nby repeatedly searching for the minimal elements of the partially ordered set s, addfng such elements \nto the end of t, and then removmg them from s Pretty printed in RAPTS, our initial program is, program \ntopsort ; 1 read (SP) ; 2 print ( sp) ; 3 t:=[1 ; b s := domain sp + range sp ; 5 (while exists ainsl((sp \n{a] )* s)={}) 6 t with := a; sless := a;7 end while ; 8 if s= {} then 9 print (t ) ; else 10 print (O) \n; endif ; end program ; The running time of the mmal program IS slow,, essentially O(#E*%21, which [s \ndue to repeated search for the minimal elements of s each cycle through the while loop Speeding up this \nprogram entails searching for the mlnlmal elements only once, and maintaining the set of minimal elements \nby Inexpensive differential computations within the while loop as s decreases This strategy for program \nimprovement IS captured by a basic program optimization method we call finite differencing In order to \nfacilitate finite differencing, we must first turn the code above into a normal form in which set update \noperations are implemented in terms of element addklons and deletions, set intersections and deletions \nare rewritten as set formers, etc. For our example, the system will carry out several local transformations \n(selected from a production system of $Imple rewrite rules) Application of each transformation is justified \nby an assertion specified by a SETL predicate. If the system can simplify the predicate to true , the \ntransformation m applied automatically, Otherwise, the system asks the user to confirm the partly simplified \npredicate. (In all of our examples presented here, the system has proved these predicates.) The essential \nfragment of the normal form of the topological sorting procedure appears Just below, (while exists a \nin { setlkl in s \\ # { set142 in SP { setlbl ] I set142ins]= O}) twith := a; sless := a; end while ; \nFinite differencing will automatically transform the normal form algorithm mto an equivalent but more \nefficient algorithm that uses the speedup strategy stated earner. In rough terms, differencing will perform \nthe following three steps based on Cockes reduction in strength schema [61. i. Just before the while \nloop, insert code that evaluates the set of mmmal elements {set141 in sl#{set142 in SP{ setlkl l\\set142 \nin s]= O] (1) and stores it into the variable minset, We call this code the initialization for mmset \nii. Within the while loop where s is modified, insert code that recalculates mmset from its old value \nso that it always stores the value of the set of minimal elements at the point (line 5) where it is computed \nWe call the code that updates minset the difference of minset with respect to the modification s /ess:= \na When the difference code is executed just prior to the modlflcatlon, It is called predifference code; \nwhen it is executed just after the modification, it is called posto ifference code. .,. Ill At line 5 \nreplace the mmimal set, which is made redundant by steps (i) and (ii), with the variable minset. For \nthis approach to improve program performance, the overall computational cost of calculating the initialization \nand difference code in the transformed program must be less than the cost of repeated calculations of \nthe minimal set m the unoptimized program Our system makes this analysls based on classical code motion \nassumptions (based on Cocke and Schwartz [6, 31]) and mechanical examination of the minimal set (1) and \nthe while loop wlthm the normal form of our algorithm before differencmg IS applied For this example, \nthe system will predict that dlfferencmg will yield asymptotic improvement m the cost of computmg (1) \n(Note that Fong and Unman [16] reheal on weaker assumptions; see [31] for a comparison.) The intuitive \nideas behind the analysis are based on a declaion procedure for a class of expressions for which the \ncost of computmg difference code relativa to certain kinds of parameter modifications is asymptotically \nless expensive then the cost of full expression evaluations. We say that expressions belonging to this \nclasa are differen?iab/e. To define the class of differentiable expressions, we first define a fitvte \ncollection of elementary differentiable expressions and their associated difference code blocks whose \ncomputational cost is comparatively small. As is shown in [31], the full class of differentiable expressions \nKS formed from composition of the elementary expressions and parameter substitution This extended defmmon \nis justified by a formal calculus that constructs inexpenswe difference code for a nonelementary dlfferent!able \nexpression by combinmg difference code for the elementary differentiable expressions out of which n IS \nformed We now apply the preceding analysis to the mmlmal set (1) using the collection of basic set theoretic \ndifferentiable expressions found m Appendix 1. Examination of the m!n!mal set calculation detects three \npotentially differentiable subexpresslons, newpred{setlhl] = {set142 in sp {Set]41] lset142 in S ] numpred(setlbl) \n= # newpred {setlbl ) minset = {set141 in s I numpred(set 141) = 0] that might permit efficient differencmg \nfor the minimal set Unfortunately, neither newpred{set141) nor numpred(set141) are differentiable, because \nwe cannot form efficient difference code for them relatlve to the arbitrary modifications in the free \nvariable set141 that occur wlthm the while loop of the normal form However, we can overcome thle problem \nusing transformations (listed m Appendix 11! that handle dynamic expression formation, a generalization \nof Earleys Iterator mverston Appl(catlon of transformation (4! of Appendix II converts newpred{set141] \ninto the following differentiable expression newpred = {[x, yl in SP] Y in S] which removes the troublesome \nfree variable setkll, and stores values of newpred{set141} for all relevant values of seti41. Likewise, \ntransformation (26) turns numpred(set141) into the following expression that can be malntamed dynamically \nat low cost, numpred = {[x, # newpred{x]]: x in domain newpreci} Supported by the elementary differentiable \nexpressions, newpred, numpred, and mlnset, the mirvmal Se? ill is seen to be differentiable, and our \nsystem can proceed to carry OU: the main transformational steps that will speed up the normal form of \nthe topological sort, I.e., i. Store initial values Into newpred. numpred. and minset on entry to the \nwhile loop il Update newpred, numpred and mmset Just prior to Ime 7 where s E modified m order to make \nthe computation of the mmmal set at hne 5 redundant Consistent with prewous dwcusslon, we refer to the \nu~date code revolved in tash (ii) as the difference of newpred, numpred, and minset with respect to the \nelement deletion s less:= setll, and we form this difference code using a kmd of chain rule that combines \nthe separate rules for formmg difference code first for newpred, then numpred, and finally minset (I,e., \nfrom Inner to outer subexpression of the minimal set). We will illustrate the chain rule by proceeding \nwith this example The predlfference of newpred relatw e to the modification s less= a is (Forall set\\48 \nin { x indomainsp I a in SP {x] }) (2) newpred{set148] less:= a; end forall ; Observe that the predifference \ncode (2) contains a costly embedded expression succ{a} = {xin domain SPIain sp {x}} that we do not want \nto compute, However, the system will recognize that this expression can itself be reduced by second differencing, \nAt the same time that the three other differentiable expressions are detected, the system will recogmze \nthat dlfferentlatfon of the dynamfc expression succ = {[y,x]: x in domain sp, y in sp{x]] can efficiently \neliminate the costly static expression occurring within the difference code (2) The predifference code \nfor numpred relatlve to the change in newpred w,nhln (2 I IS simply numpred(setlh8) -:= 1; (3) The final \nstep of the chain rule revolves forming the difference of mlnset relative to modifications in both of \nits parameters, s and numpred, These predifference blocks are comment: relative to changes in s (4) if \nnumpred ( a ) = Othen minset less := a ; endif ; and comment: relative to changes in numpred (5) if set148 \nin s then if numpred ( set 148 ) = O then minset less := setllt8 ; elseif numpred ( set 148 ! = O + 1 \nthen minset with := set]48 ; end if : end if : respectively The chain rule combines the preceding blocks \nof difference code to form the following collectwe predifference of newpred, succ, n umpred, and mmset \nwith respect to s less= a ( forall set148 in succ { a j ) (6) if set]48 in s then if numpred ( set]48 \n) = O then minset less := set148 ; elseif numpred ( set]48 ) = O + 1 then rninset with := set148 ; end \nif ; end if ; numpred ( set14&#38; ) -:= 1 ; newpred { seti48 } less := a ; end feral 1 ; if numpred \n( a j = O then minset less := a ; end if ; . Analysis of the overall cost of executing the block (6) \nrests on three easy observations i. Based on the monotomcally decreasing sel s wlthm the while loop, \nwe estlma~e tnat (6) is executed O(#nsI times, where s !s the mmal value i! B2sed on tne complexity estlmales \nstatec In Table 1. the difference blocks !3}. ~4i. and !5) revolve only constant fac~or costs. Sucn costs \nare subsumed by the costs of surrounding code and can be ignored These examples illustrate the following \ngeneral property Definition An expression E = f(st IS strong/y corrlirwous with respect to modlftcatlons \nof the form ds IO s if the cost of the difference code for E with respect to ds is 0(1) Thus, mlnset \nIS strongly continuous wjth respect to indexed assignments to numpred and element deletlons to s Also, \nnumpred IS strongly continuous with respect to element addltlons and deletlons to newpred iii. Repeated \nexecution of the difference code for newpred (Forall setlk8 in succfal) newp-ed{seti48} less:= a: end \nferal I ; relatwe to each distinct element a removed from the monotomcally decreasing set s. has an overall \nasymptotic cost no worse than a single calculation of newpred = {[x,YI in SP I Y in S1 at the i!~itial \nvalue of s: i.e., O(#spl This example illustrates the followlng general property. Definition, An expression \nE = f(s) m weakly continuous with respect to modifications ds to s if for every mmmai length sequence \nof operations dsl,ds2,...,dsn (of the form ds) that constructs the final value 52 from the m!tial value \ns1, the cumulative cost of all difference code for E with respect to all of the operations dsl,...,dsn \nIS O(max(cost(f(s l)l,cost(f(s2))) + n). (Note that all of our speed estimates are based on the heunsttcs \ngwen m Table 1.} Thus, newpred m weakly continuous with respect to element additions to s. Based on tne \npreceding analysis. the asymptotic cumulative cost of (61 E estimated at O($sp! whrch IS an order of \nmagnnude better than the overall COSt of the mmrma! set computation !1; lr~ the normal form algorithm. \nBased on Table 1 and the assumption that initialization of newpred, succ numpred. and minset can be achieved \nby the straightforward assignments newpred := {[x,y] in spl y in s]; (7) Succ := {[Y,xI: x in domain \nSP, Y in SP{XII; numpred := {[x, #neWpred{x]]: x in doinain newpred]; minset := {set141 in s I numpred(setlhl) \n= 0]; we estimate the preprocessing costs to be O(#sp), which justifies our prediction of asymptotic \nspeedup However, we gain a constant factor improvement over this naive initialization by jamming the \nimplicit loops within these set formers [31] The jamming algorithm implemented in RAPTS constructs newpred \nand numpred m a single loop A deeper investigation of this important transformation and an improved algorlthm \nis found in [20] The speedup prediction Just presented IS based on analysls of the normal form algorithm, \nso that it can be determined whether differentiation IS profitable prior to any differencmg tran.sformatlons \nare applied. By analysis of tne normal form, It ts somettmes also possrble to estimate the asymptotic \nspeed of the transformed algorthm Based on the Table 1 estimates and detecrlon of the presence of monotomc \nset growth within while loops (which can prowde an estimate for the loop repetition frequency), It Can \nbe determined that after the minimal set computation IS replaced by mInset, ali code other than that \nwhich has been introduced by differencmg and mltlallzation contributes no more than O(tisp) m overall \ncost Adding m our estimates for cumulatwe differencing and mitdzat!on costs gives us an overall estimate \nof O(#sp) m running t!me for the transformed algorlthm. Further Improvement m ttme and especially space \ncan be realized by performing dead code ellmlnation, which exploits the increase in data independence \nresulting from differencing and jamming. Based on an algorithm due to Kennedy [23, 31], our dead code \nelimination procedure detects all assignments to newpred as superfluous The result of this final step \nis, program topsort : i read (s-p ): 2 pr-lnt ( sp ) ; 3 t:= [l: 4 s = domain SP + range 5P ; 5 Succ \n:= {}, 6 ( foi-a, l set149 In domain SP , set75C Ir, 5P { set149 ! 1 7 succ { set150 ? with := set 14% \n: end ?Orall ; a numpred := { } ; 9 i fo. all [ set143 setlil 1 in sp J 10 ,f set144 In s then ,< numpred \n(9et143 )+= ~: end lf : end forall : 12 mlnset := {) : i3 ( forall set154 ~n s ) 14 lf numpred ( Set154 \n) = O then 15 mjnset w,th ,. set15d : end (f; end forall ; !6 ( wh~le exists a in rn>nsmt ) 17 twith \n:=a: 18 ( forall set14a in succ { a ? ) 19 if set 148 in s then 20 If numpred ( set 148 ) = O then 21 \nm]nset less := set 14a i elself numpred ( set 148 )=O+ 1then 22 mlnset with := set 148 : endlf : end \nIf : 23 numt3reo ( set 148 )-.= -: ena forall : 24 If numpreti ( z ) =Cthen 25 mlnset less := a ; end \n,f; 26 sless := a; end wh~le ; 2? ifs= {} then 2s! print (t ); else 29 print (O); end If ; end program \n; Our current Implementation outputs the code Just above after mechanical application of the preparatory \ntransformations, dynamic expression formation, finite differencing, jamming, and dead code elimmation. \nAs m evident from our example, these transformations treated together function prlmanly to automate the \nformation of data access paths Since the Iangth of such paths traversed during execution IS strongly \nrelated to the asymptotic running time of an algorithm, It IS not surprising that finite dlfferencmg \nand its ancillary transformations yield asymptotic speedup. In addition to speedup the process just illustrated \nsupports verification The soundness of our transformations, along wtth a standard correctness proof af \nthe initial abstract algorithm, proves the correctness of the less perspicuous but more effmient equivalent \nalgorithm above Further improvement will result from manually imtlated assertion propagation and easy \nsyntactic transformations. For example, we can introduce the assertion asser~ set144 m s Just before \nline 10 m order to ellminate the extraneous membership test at Ime 10. It is also worthwhile to place \nthe statements assert set148 m s prior to line 19, assert numpred(set148) /= O just before 20, and asserl \nnumpred(a) = O Immediately before 24, and then exploit these assertions in obvious ways Further automatic \nimprovement by a large constant factor may be achieved by data structure selection and aggregation [8, \n37], transformations that should eventually be Integrated into RAPTS. However, considerable extensions \nto the referenced method are needed to obtain the most desirable data structures for our example (see \nKnuth [25]). A careful semiautomatic approach to select data structures for topological sorting has been \nworked out by Katzenelson [22], who used clusters of abstract data types. Katzenelson observed that the \nmost difficult transformattonai step revolves showing how numpred and minset can share the same space. \nTo solve this problem, we use the following transformation which can be Justified mainly on syntactic \ngrounds. Since numpred IS pointwise monotonically decreasing to O within the whiie loop from lines 16 \nto 26 and since numpred is only referenced when It m nonzero Just before Ime 22 we can release its space \nwhen it goes to 0. Note, however, that when numpredix) becomes O is exactly when minset = {x m SI numpredtx~ \n= 0} is augmented by x Tne result of all these transformational steps yields the following data structures \n lnunlpl.ed base( I e. .unlversal set; ciomaln sp + range 5P SD m,nsel ~...-----------------------------------..-. \n--. . . . . . . ..-- .--. -.. - . --------------------------------------..------..-. .-. .-----.-. - \n - .\u00ad~ x pO>n Te,-tc, numprea( . f or pO1n Ter to 5 ? {.) 9Qv T e,emert of m>nssl qdeke _.. __ ----------------.--_-____--__---------------------------------\u00ad \n1 --------------------------------------------------------------------J SP {X} llst --. -. . . .. ------------\u00ad \nointe-.s to base 1 L.... ............. It is worthwhile to elaborate on the approach used to estimate \nasymptotic program performance for the topological sorting example. Most differentiable expressions m \nAppendix I are either strongly or weakly continuous with respect to element addmons or deletions to set \nor map valued parameters. For example, all elementary differentiable expressions except for (8) and ( \n10) m Appenalx I are strongly continuous with respect to the set S, strong continuity K also exhibited \nby expression ( 1) with respect to the set Q and by expressions (3] ~4! (7) and (?O) wtn respect to the \nfunction F Weak contmultv can be observed m ExpressIons (5) and (6) with respect to the set Q and m expresson \n( 10i with res~ect to S Note that the expression associated with newpred m the topological sort example \n!s of the form (5 I of Appendix 1, and exhibits both strong and weak contmulty. Some properties of contmulty \nare formalized in the theorem below. Theorem: i Strong continuity is closed under arbitrary composition. \nIi, Let E = f(s) be weakly continuous with respect to changes ds to s, and let ds 1,,..,dsn be any minimum \nlength sequence of operations of the form ds that constructs S2 from s 1. If all difference code for \nE with respect to ds 1,...,dsn forms a mimmum length sequence of operations of the form dE that constructs \nf(s21 from f(s 1), and if g(E! is weakly continuous with respect to dE, then g(f !s)1 IS weakly continuous \nwith respect to s. The cumulative cost all the difference code for g(f(s)) is O(max(cost(f (s 1)),c0st(f(S2)l \n+ max(cost(g(f (s 1)),cost(g(f(s2 ))))). ii! If f(s) is strongly continuous with respect to ds, it is \nalso weakly continuous with respect to ds For an example of composmon of weakly continuous expressions, \nconsider nested image sets. The Image set expression E = f [s1 E weakly cormnuous with respect to element \nadditions and deletions to s, and m a program loop where s ts monotonically increasing (resp decreasing), \nso is the value of E after differencmg IS applied Thus if we consluer the nested expression h [g[f [s111 \nin such a LOOP in which h.g. and f are invariant, the cumulative cost of executing difference code within \nthe loop after it m optimized IS estimated to be O(#h+##g+#f). We have used the preceding mechanical \nasymptotic tme estimates successfully on several algorithms that include finding connected components \nm a graph, fmdmg the center of a free tree [31], finding all nonterminals that derive the empty string \nin a context free grammar, computmg attribute closure, and partitioning a flow graph into intervals The \nImplementation of fmlte dlfferencmg wlthm RAPTS uses three main algorithms I. de fmitlon of variable \nmodification categories based on finite differencing rules; ii detection of variables and differentiable \nexpressions (In a program loop! that fall mto the categories defined in step i iii finite dlfferencing \nof the differentiable expressions with respect to the program loop Details of these algorithms can be \nfound in [33]. Step ii which is Ioglcally similar to the proced[]l e presented in [30], passes through \na postordering of a parse tree form of the program loop, At each node a value number is computed [6, \n12] to determine whether an expression is differentiable, and also whether it has been encountered before \nThe goal of this step is to determine all differentiable expressions, Some of these are detected directly \nin the loop, while others occur as auxdlary expressions within differencing rules The time complexity \nis linear In the sum of the parse tree sizes for the loop and the differentiable expressions. Step iii \nuses two lists produced by step )[ a Ijst of places in the loop where each differentiable expression \noccurs, and a 11s? of places each variable is modified. It rapidly replaces each chfferentiable expression \nf by its associated variable name E w]thln the loop by a straightforward bottom up procedure For each \nmodification dx to a variable x on which some differentiable expression depends, the collective pre-and \npostdifference blocks are formed with respect to dx and inserted around dx The time complexity for this \nprocedure is Imea. m the sum of the sizes of the inserted difference code anti the code which !s eliminated \nas redundant Conclusion Interactwe syntactic edmmg systems such as the Cornell Synthesizer have successfully \ndemonstrated a program construction methodology that mitigates compile time error The Synthesizer speeds \nthe process of program construction by dynamically monitoring syntax and, to some extent, semantics while \nthe program IS entered interactively. Transformational programming is a proposed methodology that alms \nto elimmate run time error, so that debugging would be unnecessary It seeks to speed the programming \nprocess by mteractwely monitoring program correctness and efficiency during program construction RAPTS \nis a novel implementation of a prototype transformational system that represents a synthesis of old and \nnew ideas, It incorporates new algorithms (Its dlfferencing algorlthm ts an improvement over classical \nstrength reduction used m conventional compillng systems) and new transformations. We have used RAPTS \nto derwe many simple algorithms such as the one Just presented For a more complicated example see Appendix \nIll. We have Introduced a straightforward mechanism for estlmatmg the speedup that results from finite \ndifferencmg and the speed of a differentiated algorithm prior to differentiation Important followup work \nto thvs would be to determme condmons unde r which these initial performance estimates are preserved \nby a conventional complexity measure after conventional data structures are chosen to Implement the sets \nand maps occurring within the differentiated aigorlthm, Appendix 1, Differentiable Set Expressions Listed \nbelow IS a small, but fairly complete, collection of elementary set-theoretic meta expressions that can \nbe maintamed efficiently by dlfferencmg We assume that each set former in this collechon can also be \nexpressed m terms of multi-lterators, which generalize cartesian product, e.g., {e(X, Y): X in S, Y in \nT(X) I K(X, Y)} Although the difference rules associated with each elementary expression are not shown, \nthey follow easily from standard distributive laws. Out of these meta-expressions and corresponding difference \nrules, the more concrete and numerous elementary expressions and efficient difference rules found in \n[31] can be derwed, 1. S+Q 2. (X1n SIB(Xl}  2. (X in S I F(X) = T) where T IS an Integer valued constant \n, F IS Integer valued. A, {X In s [ F(X) /= T; h>here T >S an Integer valueo constant , F is Integer \nvclued. 5 {X ,n S \\e(X) in Q} G {X tn s \\ e(X) notln Q} 7 {Y, 1P S \\ F(X) relop R) whet. e F[S] IS dense \non the interval of Integers containing the range of R values: F must be > nKeger val ueci; t.e lop can \nbe any of the Combat. ,so s ., >,.=,>= 8 {> In s ~ F(X) relop R} where F[S] IS sparse on the interva? \nof integers Conta, nlng the range of R values, or when F and R can be real ; In th~s case, we also maintain \nthe following two auxil ,ary expressions: V = SO RTED(F IS]) AND K = !JIN){I In [!, ,*V . 1] I NOT (V(I) \n. R)) 9. {e(Y) x in s} 10 F[S] 11. *S !2. .,1s where + represents arl thmet lC sum Appendix IL DYNAMIC \nEXPRESSION FORMATION Below we present rules based on Eariey s iterator inversion [ 13] and Paiges method \nof discontinuity removal [30], for transforming static set formers and other set theoretm expressions \ninto a form swtable for efficient dynamic modification Each basic expression f given below depends on \nfree variables q,q 1,q2,.. that can undergo such modifications that disallow efficient dynamic maintenance \nof the value of f. However, f can be profitably maintained dynamically by eliminating its free variables \nand using a dynamic expresmon f associated with f in the table below. Note that f stores the values of \nf {q} for all useful instantiation of q 5tat1c ExDresslon El,namlc Express Ion 1. {J ,n G(q) I B(X)) \n{[ V,X] In G I B(X)) 2. {X ;n s IF(x) =q) {[F(X),X]: Xin S) 3. {X In G{q2) I F(X) = ql} {[[ F(XI, VI \n,X]: [V , X] ;n G}  J. {Y in G{q) I XIn Q? {[>, v] InG ~V?nQ} {[ I Fy s} 5. {X ]n s IXln F{q?? X,Y] \nI ,r, 6. {X In G{ql) \\ X In F{q2!) {[[x. YI, zI [x, z] In G. v ?n domain F I z in F{>,!} 7. {X In G{q! \nI F(X) In Q) {[Y,X] In G I FIX) in 0} 8. {X In S \\ F(X) In H{q)) {[ Y. x], Y in domain H. x ,n doma, \nn F I F(x) ~n H{!,,}} 9. {X In G{ql) \\ F(X) In H{q2)? {[[ Y.z]. x]:[Y. xI In G, 1 In domain H I F(x) \nIn H{>,?) IG. {x In G{a! I Xnotln Q} {[x, V] In G \\ Ynot, n 0>  11. {1 1n5 I X notln F{q)) {[y, x]: \ny in Ciomaln F. x In S I x notln F{ Y)) 12. {X >n G{ql) I X notln F(q2?) {[[ Y.z I.x I [y, x] ln G.z \nin domain F lx not, n F{2}} 13. {X in G{q) I F(X) notln Q> {[ Y.X] in G I F(x) notln Q! 14 {a in S I \nF(X) notln H{q)) {[y ,x] y In domain h, Y in S I F(x) nOt\\n H{y}?  15 {x ,n G{cI1} IF(X) notln H{q2}) \n{[[ Y.z]. x]:[Y. x] In G.z ~n Uomaln H\\ F(x) notln H(z)) 16, {7 In 5 ~q ,n F{X}) {[Y.X]: x In S. Y In \nF(>i\\ ,7.{> in G{u2 ! a< r, F{>)! {[[ U. X], W] [W.>] Ir, G, L r FiA: 18 {) in G,o I><R; {[A.,] lnG IY<R} \n !9 {X ,nS I><Ftql) {[Y.x I: y in Uoma]n F, YIn S I x < F(y)} 23. {X In G{al} IXsF(q2)) {[[ Y.zI.x I: \n[Y.xI ?nG. z in doma>n FIx<F(z) 21. {Y ,n G{q) I F(x) <R) {[X.Y] in GIF(Y) <R) 22. {X in S I FIx1 < \nH(q)} {[ Y.x]: Y In Uomaln H, x In S I F(x) < H(Y)} 23. {x ~n G{ai} I F(x) < H(q2)) ([[ Y.z] .x]. [s \n.x] in G. z In aomaln H I (xl ~ H(z)) 24. {F( XI: X In G{q)} {[ Y. F(x)I: [Y. x] in G) 25 F[G{q}l {[ \nY.x]: [Y. z] ,n G. x in F(z})  {[Y, .F{.Y}], Y in doma)n F; 27, ./ F{q} {[x, ,/ F{ X}], >, ln DOt !AIN \nF) where . represents arltnme T1c sum 26, FF {q} Appendix Ill. Differencing Applied to Dead Code Elimination \nWithin RAPTS :. Below IS an ,n,t~al abstracl algop>thm SPeclf YlnY z POr Y1On ~f the dead code e] ,m \nI nation PPOCeUUPe useU W>ttl, n RAPTS The set crit ,s the set of crltlcal statements (Inltlallj Oe?lned \nto be the p~lnt statements of a program) The algorlthm works b. .eoez Ted>, l ad~,ng to Crlt the set \n0+ >nstructlons tnat can affect tne value 0+ variable uses w,thln C1-lt Until crl: nc Iongei_ grm>z luses \n{q? IS the set of variable uses w.ttnln statement Q usetodef {u) IS the set of al: Variable de fin, t,ons \ntna: can reach var~able use u ,nstof (d) IS the statement assoc~ateo wlKn a variable def>n:?,on d compound(q) \nIS the Compounu statemen7 )mmed)atel! conta, n,ng st2tement c1 program dead : read ( Instof usetoaef \n,uses compouna cr>t I 2 ( converge ) : Crlt + := ( lnstof [ usetoaef [ luses [ crit ] ] ] + compound \n[ Cr lt ] ): end : \\ 4 D.>nt ( cr]t ) : eno : IT IS k lthln Tne norms-, fern beloh tns: 14 d,$ferent>aole \nexDressl On% are aeTec Ted. Includlng Ist and 2nd a>? ference expressions. Anal ys>s cie Te. m,nes that \nthe maus ,nsto? usetoae?, ]uses, and c>mp~uno a-e z: 1 tueakl~ Cent, n ous w,tt vesoect ta element aod?:,ons \n,n their set Vzlued arguments. that weav Cortlnultb ,s closed for the Se expressions. and flna? 1 y, \nthat the Cumulative COS? of a,+ ference cooe 15 estimated to be 0(, lnstof+, usetodef +=luses+ ncompoundl, \nwhlct >S dom> nated b) O(eusetodefl. Th, s est, mate IS the same for ,-,-,~la,,;zat,on costs, It IS easy \n:a see th&#38;?t after dlfferenclng, tne rema, n,ng costs are proport]onai to the sum of the ,nput and \noutput sizes. program dead : read ( Instor , usetodef IUses cormouns crIt ) : 2 ( wh]le ex)sts se?ll \njr { set10 >n ( Ins To? [ usetodef i luses [ c.i* 1 1 1 ~ COmPOund [ c~~t 1 ) I set10 notln cr, t ) I \n3 cr, t With := setll : end Wh,,le : ~ pr, nt ( C1-lt ) : enu : b+?er Olfferenc>ng and aeac! code el, \nm,rmt, on, the malt . loop of the algo!-? in.> aPDea F% below Note that 4 out of the 14 d, fferent, aDle \n exp. esslons have been el Iml nated as useless. The passage from step 1 to 3 IS done comD1etely automatically \nwlthln RAPTS. 35 ( wn,le ex, sts setll >n ne~, nsts ) 36 ( for-all set15 Tn ,uses { setll } \\ nusep~ed \n( set15 1 = O ) 37 ( forall set118 ?n usetodef { set15 ) ~ ndefpred ( set118 ) = 38 39 40 41 42 43 42 \n45 46 47 48 50 51 52 53 54 55 56 57 58 59 60 o) ( forall set123 ln lnstof { setl18 } I nlnstpred ( set123 \n) =0) If set 123 not~n comps then If set 123 natln Crlt then newlnsts wltn . set 123 ; end?f : Instpnts \nWith = set 123 ; enci I+ ; Insts with := set 123 : end feral 1 . ( forall set130 ln lnstof { setl18 \n) ) nlnstpreu (set130 )+= 1; ena feral 1 : end feral 1 : ( fora?l set127 In usetoaef { set15 ) ) ndefpred \nI set 127 )+ ,. i : end feral 1 end feral 1 ( feral 1 setl 10 jr compound { setl 1 ? I ncompred ( setl \n10 ) = O ) If 3etl10 notln lnsts tnen If setllo not>n C?IX then newlnsts wltn = setllo : endIf : lnstpnts \nwltn = se7110 : end lf : Comps with .= Setllo end forall : I forall setll~ In compouno i setli ? ) ncomored \n( setli.I ) + = 1 : end forall ; ( forall set139 In 1use5 { setll } J nusepred ( set 139 ) -=i. end forall \n: >f setl i ,n lnstpnts then new~nsts less = setll ; end if , . CP1t with = setll : end while ; References \n1. Blaustem, Barbara T. Enforcing Database Assertions Techmques and Appkcatlons Tech Rept TR-21 81 Center \nfor Research m Computng Technology, Harvard Unwersky, Aug, 1981 2. Broy, M, Partsch, H, Pepper, P, and \nWirsmg, M Semantic Relations m Programmmg Languages Information Processing 80 ( 1980) 3. Burstall, R, \nM., and Darhngton, J A Transformation System for Developing Recurswe Programs JACM 24, 1 (Jan 1977} \n 4. Cheatham, T, E,, and Wegbrelt, Ben, A Laboratory for the Study of Automatmg Programming Proc AFIPS \n1972 Spring Joint Computer Conf., 1972 5. Cheatham, T E, Holloway, G H, Townley, J A Program Refinement \nby Transformation Proc 5tP, Im Conf on Software Engineering, Mar, 1981 6. Cocke, John and Schwartz, \nJ T, Programming Languages and Their Compilers. ClMS, New York Unwerslty, 1969 7. Cocke, John and Kennedy, \nKen An Algorlthm for Reduction of Operator Strength CACM 20, 1 I (NOV 1977} 8. Dewar, Robert B K., Grand, \nArthur, Lw Ssu-Cheng Schwartz, Jacob T.,and Schonberg, Edmond, Program by Refinement, as Exemplified \nby the SETL Representation Sublanguage. TOPLAS 7, 1(July 19791, 9. Dewar, Robert The SETL Programmmg \nLanguage. Manuscript 10. Dljkstra, E. W. A Disc/ p/ine of Prograrnrn/ng. Prentice-Hall, 1976. 11. Donzeau \nGouge, V., Huet, G., Kahn, G., Lang. B Programming environments based on structured editors: the Mentor \nExperience Tech Rept Rapport de Recherche No 26, INRIA, Rocquencourt, France, July, 1980.  12, Downey, \nPeter, Sethi, Ravl, and Tarjan, Robert. Varlatlons on the Common Subexpression Problem JACM 27, 4 (Ott \n1980). 13. Earley, Jay. High Level Iterators and a Method for Automatically Designing Data Structure \nRepresentation Journal of Computer Languages 7 ( 1976),321-342  14. Feather, Miartin S A System for \nDeveloping Programs by Transformation, Ph.D. Th., U, of Edinburgh, . 1979 15. Floyd, Robert W, Asslgnmg \nMeaning to Programs, Proceedings of Symposia on Apphed Mathematics Vol XIX, American Mathematics Society, \nProvidence, R !,, 1967  16. Fang, Amelia C. and Unman, Jeffrey D. Induction Variables in Very High Level \nLanguages. Proc. Third ACM Symp on Prlnctples of Programming Languages, Jan, 1976  17. Fong, A C Elim!nat,on \nof Common Subexpresslons m Very High Level Languages, Proc. 4th ACM Symposium on Principles of Programming \nLanguages, Jan, 1977.  18. Fong, A C Inductively Computable Constructs in Verv High Level Languages \nProc 6th ACM Symposium on Prlnclples of Programmmg Languages, Jan, 1979 19. Gerhart. S Correctness Preserving \nProgram Transformations. Proc Second ACM Symposium on Principles of Programming Languages, 1975  20. \nGoldberg, Allen, Palge, Robert, Loop Fusion, unpublished manuscript 21. i-loare C A i? An Axlomatlc \nBasis for Computer Programming CA.CM 12, 10 (196 3) 576 5E?I 22. Katzenelson, J Clusters and Dialogues \nfor Set Implementations. IEEE Trans. on Software Eng\\neer!ng SE-5, 3 (May 1979!  23. Kennedy, Ken. A \nSurvey of Complier Optimlzatlon Techniques. In Program Kiow Analysis, Muchmcl., S. Jones, N, Eds., Prentice \nHall, 1981, pp 5-54. 24. Knuth, D E Semantics of Context free Languages Mathematical Systems T,hecry \n,2 2 (1968, 25. Knuth, D. E.. Fundamental Algorithms. Adulson-Wesley, 1968.  26, Koenig, Shaye, A Transformational \nFramework for Automatic Derived Daia Control and Its Applications m an Entity-Relationship Data Model \nTech Rept LCSR-TR-23, Rutgers University, Dept of Computer Science, 1981. New Brunswick, N. J 27. Loveman, \nD B Program Improvement by Source to Source Transformation JACM 24, 1 (Jan 1977). 28. Morgenstern, Matthew \nAutomated Design-and Optimization of Management Information Sys?em Software Ph.D Th., MIT, Laboratory \nfor Computer Science, Sep 1976,  29. Paige, R, and Schwartz, J T Expression Continuity and the Formal \nDifferentiation of Algorithms Proc Fourth ACM Symp on Principles of Programming Languages, Jan, 1977 \n 30. Paige, Robert Forma/ Differentiation, UMI Research Press, 1981, Revision of Ph,D, thesis, NYU. June \n1979 31. Palge, Robert, and Koemg, Shaye Finke Differencmg of Computable ExpressIons ACM TOPLAS 4, 2 \n(July 1982)  RAPTS Th~ Rutq~r~ Abstract proara~ ~ran~for~,atlon sv~t~~ 32. Palae Robert ComDiler Demonstration, \nSymp o; Compiler Construction, Bost~n  33. Paige, Robert. An Efflclent Implementation of Flnlte Dlfferenclng \nDept of Computer Science, Ru@ers Unwersty, Dec. 1982  34. Reps, Thomas Optimal-time Incremental Semantic \nAnalysis for Syntax. -directed Editors, Proc. Ninth ACM Symp. on Principles of Programmmg Languages, \nJan, 1982  35. Rosen, B K Degrees of Availability In Program F/ow Analysis, Muchmck, S., Jones, N., \nEds., Prentice Hall, 1981, pp 55 -76, 36. Scherlm Willlam L Program Improvement by Internal Specialization \n8th POPL, Jan, 1981 37. Schonberg, Schwartz, and Sharir Automatic Data Structure Selection m SETL. Proc, \nSixth ACM Symp on Principles of Programming Languages, Jan, 1979 38. Schwartz, J T., On Programming: \nAn Interim Report on the SETL Project, Installments / and I I ClMS, New York Umv., New York, 1974, 39, \nSchwartz, J T Correct Program Technology. Tech Rept Courant Computer Science Report Num. 12. New York \nUnwersity. Dept of Computer Science, Sep. 1977 40, Sharlr, M Some Observations on Formal Dlfferentlatlon. \nNew, York Unwersity, Dept of Computer Science, 1980 41. Standish, Thomas An Example of Program Improvement \nUsing Source to Source Transformations Umv of Cal at Irvine, Dept of Information and Computer Science, \nFeb, 1976. 42. Tarjan, R. E A Unified Approach to Path Problems. J ACM 28, 3 (July 1981) 43. Teitelbaum, \nT. and Reps, T. The Cornell Program Synthesizer a syntax-directed programming environment. CACM 24, 9 \nC3ep 1981) 44. Wegbreit, B Goal-directed program transformation IEEE Trans. Software Engineering SE-2, \n2 (June ?976) 45, Wlrth, N Program Development by Stepwlse Ref mement CACM 14, 4 (April 197 1), 221-227. \n\t\t\t", "proc_id": "567067", "abstract": "<p>Ten years ago Cheatham and Wegbreit [4] proposed atransformational program development methodology based on notionsof top-down stepwise program refinement first expressed by Dijkstra[10] and Wirth [45]. A schema describing the process of thismethodology is given in fig. 1. To develop a program bytransformation, we first specify the program in as high a level ofabstraction and as great a degree of clarity as our programminglanguage admits. This high level problem statement program P isproved correct semimechanically according to some standard approach(see Flovd and Hoare [15, 21]), Next, using an interactive systemequipped with a library of encoded transformations, each of whichmaps a correct program into another equivalent program, we selectand apply transformations one at a time to successive versions ofthe program until we obtain a concrete, low level, effecientimplementation version P'. The goals of transformationalprogramming are to reduce programming labor, improve programreliability, and upgrade program performance. In order for labor tobe reduced, the effort required to obtain P, prove it correct, andderive P' by transformation should be less than the effort requiredto code P from scratch, and also to debug it. Program reliabilitywill be improved if P can be certified correct, and if eachtransformation preserves program meaning. Finally, programperformance will be upgraded if transformations are directedtowards increased efficiency.</p><p>Experimental transformational systems that emphasize one or moreaspects of the methodology outlined above have been implemented byCheatham [5], Darlington [3], Loveman [27], Standish [41], Feather[14] Huet and Lang [11], and others. However, all of these systemsfall short of the goals, because of a number of reasons thatinclude,</p><p>1 inability to mechanize the checking of transformationapplicability conditions</p><p>2 reliance on large, unmanageable collections of low leveltransformations, and long arduous derivation sequences</p><p>3 dependency on transformations whose potential for improvingprogram performance is unpredictable</p><p>4 use of source languages insufficiently high level toaccommodate perspicuous initial program specifications and powerfulalgorithmic transformations</p><p>Yet, convincing evidence that this new methodology will succeedhas come from recent advances in verification, programtransformations, syntax directed editting systems, and high levellanguages. These advances, discussed below, represent partialsolution to the problems stated above, and could eventually beintegrated into a single system</p><p>1 The transformational approach to verification was pioneered byGerhart [19] and strengthened by the results of Schwartz [39],Scherlis [36], Broy et al [2], Koenig and Paige [26.31] Blaustein[1], and others. Due mainly to improved technology for themechanization of proofs of enabling conditions that justifyapplication of transformations, this approach is now at a pointwhere it can be effectively used in a system. Such mechanizationdepends strongly on program analysis, and, in particular, onreanalyses after a program is modified. Attribute grammars [24]have been shown to be especially useful in facilitating programanalysis [23]. Moreover, Reps [34] has discovered algorithm thatreevaluates attributes in optimal time after a program undergoessyntax directed editing changes (as are allowed on the CornellSynthesizer [43]). He has implemented his algorithm recently, andhas reported initial success</p><p>2 There are encouraging indications that a transformationalsystem can be made to depend mainly on a small but powerfulcollection of transformations applied top-down fashion to programsspecified at various levels of abstraction from logic down toassembler. We envision such a system as a fairly conventionalsemiautomatic compiler which classes of transformations areselected semimechanically in a predetermined order, and arejustified by predicates supplied mechanically but provedsemimanually. Of particular importance is nondeterminism removalwhich has formulated by Sharir [40] could lead to a technique forturning naive, nondeterministic programs into deterministicprograms with emergent strategies. Such programs could then betransformed automatically by finite differencing [13, 16, 17, 18,29, 30, 31] and jamming [28, 31, 20] (which we have implemented)into programs whose data access paths are fully determined. TheSETL optimizer could improve these programs further byautomatically choosing efficient data structure representations andaggregations</p><p>3 Of fundamental importance to the transformations justmentioned is the fact that they can be associated with speeduppredictions Fong and Ullman [16] were the first to characterize animportant class of algorithmic differencing transformations interms of accurate asymptotic speedup predictions, eg, they gaveconditions under which repeated calculation of a set former {x ins|k(x)} could be computed on O(#s) + cost(k) steps. By consideringstronger conditions and special cases for the boolean valuedsubpart k, Paige [31] later gave sharper speedup predictions (eg,either O(1) steps for each encounter of the set former or acumulative cost of O(#s) steps for every encounter) associated withanother differencing method. Both Morgenstern [28] and Paige [31]prove constant factor improvements due to their jammingtransformations (implemented by Morgenstern for the improvement offile processing, and by Paige for the optimization of programs).Constant factor speedup has also been observed for data structureselection by the method of basings but a supporting analytic studyhas not been presented [8, 37]</p><p>4 Essential to the whole transformational process is a widespectrum programming language (or set of languages) that canexpress a program at every stage of development from the initialabstract specification down to its concrete implementationrealization. Since transformations applied to programs written atthe highest levels of abstraction are likely to make the mostfundamental algorithmic changes, it is important to stress abstractfeatures in our language. In addition to supportingtransformations, the highest level language dictions should supportlucid initial specifications, verification, and even programanalysts. Of special importance is SETL [38, 9], because itsabstract set theoretic dictions can model data structures andalgorithms easily, because its philosophy of avoiding hidden asymptotic costs facilitates program analysis, because its semanticsconforms to finite set theory and can accommodate a set theoreticprogram logic, and because it is wide spectrum. As is evidenced bythe work of Schwartz, Fong, Paige, and Sharir, SETL is also a richmedium for transformation.</p>", "authors": [{"name": "Robert Paige", "author_profile_id": "81100242795", "affiliation": "Rutgers University, New Brunswick, NJ", "person_id": "PP14093563", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/567067.567076", "year": "1983", "article_id": "567076", "conference": "POPL", "title": "Transformational programming: applications to algorithms and systems", "url": "http://dl.acm.org/citation.cfm?id=567076"}