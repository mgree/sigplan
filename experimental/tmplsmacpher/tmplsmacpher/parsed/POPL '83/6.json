{"article_publication_date": "01-24-1983", "fulltext": "\n Permission to make digital or hard copies of part or all of this work or personal or classroom use is \ngranted without fee provided that copies are not made or distributed for profit or commercial advantage \nand that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, \nto post on servers, or to redistribute to lists, requires prior specific permission and/or a fee. &#38;#169; \n1983 ACM 0-89791-090-7 $5.00 axioms. Such introduction is known to be unsafe [BOYER8 1]. New rewrite \nrules are accepted, but only after a proof of the rule has been generated by using a powerful, semi-automatic \ntheorem prover [BOYER79] separate from the verification system proper. Our goals are similar to those \nof German [G ERMAN8 I ] in that we desire primarily to prove shallow properties of sizable programs, \nand certain of our techniques are drawn from his work. Our system, although capable of full proofs of \ncorrectness, is not normally utilized for that purpose. Tbe primary use of the verifier is to obtain \nassurance that tbe program being verified will not engage in gross misbehavior such as subscripting out \nof range, overflowing, or infinitely looping. This goal governed strongly many of our design decisions. \nThe proofs performed during the verification process are performed by a decision procedure which is complete \nwithin its limited scope and fully automatic. This automatic simplifier handles linear arithmetic, propositions, \narrays, records, and type information. [OPPEN7!I]. The application of rewrite rules is also automatic, \nbut the rules are applied only in restricted circumstances, so that it is generally necessary to construct \na specific rule when a rule needs to be applied. THE LANGUAGE SYSTEM Pascal-F is a dialect of Pascal \ndeveloped by E. Nelson at the Ford Scientific Research Laboratories in Dearborn, Michigan. It mmbines \nfeatures of Pascal, Ada, and Modula, to provide a language for programming dedicated computers running \nwithout an operating system. Code generators exist for the Digital Equipment Corporation LS1-11 and the \nFord Electronic Engine Control IV microprocessor (now offered by Intel asthe1nte18061). The basic Pascal-F \nlanguage has been augmented with statements and declarations needed for verification purposes. The syntax \nof these statements is integral to the language, i.e. the verification information is not embedded in \ncomments but is expressed in syntax as close as possible to that of normal Pascal. New keywords, such \nas ASSERT and INVARIANT have bcenintroduced tomake this possible. Thecompiler andverificr have a common \nfirst pass. This first pass processes these statements, syntax and type checking them, and either passes \nthem on to the later passes (if bchsg used as part of the verifier) or discards the information obtained \n(if being used as part of the compiler), USER INTERFACE We attempt to produce diagnostic messages in \nthe style of a compiler. All error messages reference a source line with which the problem isassoeiated. \nAtypical example appears below. Our usual demonstration of tbe system is to present a program which has \nbeen verified, and invite visitors to intrcduce an error which would cause the program to fail, either \nby breaking a rule of the Ianguage or byviolating the assertions in the program text, We then present \nthe modified program to the verifier. We have not yet had it fail to detect an error, and in most cases, \nthe diagnostic messages produced directly reflect the error inserted. AN EXAMPLE This program fragment \nis typical of the sort of processing which takes place in small microprocessor control programs. The \ncode here would be suitable for controlling a stepping motor attached to a load of moderate mass, where \nthe motor must be slowed down as the desired position is reached to avoid overshoot and hunting. stepdrive.pf \n1{ 2 Verifier Demonstration Program 3 4 Stepping motor drive example. 5 6 Version 1.50f 2/24/82 71 \n8 program stepdrive; 9const maxint = 32767; { biggest integer } type 11 dir= (down,up); { direction of \nmotion ) 12 pos = -5000..5000; { range of positions ] 13 rate = 0,,2000; { steps/compute cycle J 14 motion \n= -2000..2000; { signed rate} 1 5{ 16 log2 --extract logtobase 20 fnumber, except at Oand I 17 18 Takes \ninnumbers from Otomaxmt/2 and returns numbers from 19 Oto 15; 21 Note that theexit condltionsof log2 \ndo not fully describe 22 what it does; they just impose some constraints on the result 23 } 24 procedure \nlog2(vaF n: integer); 25exit n<= 15;n>=0; {bounds onn) 26 n <= mold; { 10g2(n) <= n } 27 (rr,old = O) \nimphes (n = O); { log2(o) = o ) 28 (rr.old = 1) implies (n = l); {log2(l) = 1 } 29 (mold > O) implies \n(n > O); { non,ero if nOrrTero input ] entry n >= O; { only on positive numbers } 31 n <= ma~lnt div \n2; { upper bound for n ) 32 type smallint = O .. 15; 33 var i: smallint; { loop counter ) 34 log: smallint; \n( resulting log } 35 twotol: I ..maxlnt; { number to double) 36 begin 37 if n <= 1 then log = smallint(n) \n{ log(O) = O by cmrventlon ] 38 { Iog( I) = 1 by convention ] 39 else begin twotoi := 2; { 2**i ) 41 \nlog := o; { log lags behind i } 42 for i .= 1 to 15 do begin { for maximum needed cycles } 43 44 if twotoi \n<= n then begin { if not big enough yet } 45 log ,= log + 1; { increment log ) 46 t wotoi := t wotoi \n* 2: { double value ] 47 end; 48 49 state (defined (twotoi), defined (log), 0< log, 51 log < twotoi, \nlog <= n, log <= 1); 52 end; 53 end, 54 n = log; { return log) 55 end {log2]: 56 { 57 calcsteprate --calculate \nstepping rate for stepplrrg motor 58 59 Called once per 10Oms to calculate stepping rate to be used \nduring next 10Oms. 6[ J 62 procedure calcsteprate(currentpos, { current shaft position } 63 desiredpos: \npos; { desired shaft position } 64 var steprate mOtiOn; { step rate to US4) 65 var stepdir: dir); { direction \nto step \\ -, 66 exit 67 (currentpos.old = desuedpos.old) implies 68 (steprate = O); { must stop] 69 (currentpos.old \n<> desiredpos.old) implies 70 (steprate <> O); {no stall ) 71 (currentpos.old > desiredpos,old) implies \n72 (stepdlr = down); { direction check 73 (currentpos.old < desiredpos.old) implies 74 (stepdir = up); \n{ direction check 75 (stepdir = up) implies { no overshoot } 76 (currentpos.old + steprate <= desiredpos.old); \n77 (stepdir = down) implies { position ) 78 (currentpos,old + steprate >= desiredpos,old); 79 var move: \ninteger; 80 begin 81 move := currentpos -desiredpos; { steps to goal ) 82 stepdir := up; { assume upward \nmove) 83 if move < 0 then begin { if downward direction } 84 stepdir := down; {so note } 85 move := -move; \n{ make move positive] 86 end; 87 log2(move); { reduce exponentially) 88 steprate := motion (move); { \nrate from 10g2 ) 89 if stepdir = down then ( if down direction ) 90 steprate := -steprate; ~ step other \nway ) 91 end (calcsteprate) ; 92 93 begin {main} 94 {No main program, just a demonstration of procedures \nthis time,) 95 end. The example contains a bug. The verifier s diagnostic messages appear below. Verifying \ncalcsteprate Could not prove {stepdrive pf73J (currentpos.old < desiredpos.old) implies (stepdir = I) \n(exit assertion) for path: {stepdrive.pf67) Start of calcsteprate {stepdrive.pf.83) IF->THEN Could not \nprove {stepdrivepf71 ) (currentpos.old > desiredpos.old) implies (stepdir = O) (exit assertion) for path: \n{stepdrive.pf67) Start of lcalcsteprate {stepdrive.pf83) IF not 2 errors detected Verifying 10g2 No errors \ndetected Verifying stepdrive No errors detected Two errors were diagnosed. Both errors represent failures \nto verify an exit condition of the procedure ca/cs?eprate. Each message references the source file name \n(stepdrive,pj) and source line number for the proof goal, and contains a copy of the proof goal itself. \nThe path being traced is displayed by stating the branch points encountered along the path and giving \nthe branch taken at each cmrditional. Note that the values down and up are now represented as O and 1. \nSince enumerated types are represented in the real machine as integers, we do our verification work on \nthem as integers. Our experience is that the information shown here is enough to make it possible for \nthe programmer to find the problem, without recourse to examining the verification conditions themselves. \nThe reader is invited to find the bug in the example.* Note that nothing was printed about the many proofs \nwhich succeeded. Only the items requiring attention by the user appear. There were 49 goals identified \nby the system which need to be proved. Each of these is a potential trouble spot in the program. Eighteen \nproof goals were generated to insure that variables are defined at the moment of reference. This is a \ncheck for reference to uninitialized variables. In the cases below, all the variables are simple ones \n(not array or record variables) but the verifier would insist, at an array reference, that the specific \nelement being referenced be previously initialized and provably so. The number in braces is the relevant \nsource program line number. {37} n is defined {37) n~ is defined {44] twotoifl is defined (44) ~n is \ndefined {45] log is defined {46) ~twotoi~ is defined {49) detined(twotoi) (STATE assertion) {49] defineif(log) \n(STATE assertion) {54] log is defined {24) defined(n) (exit assertion) (g3) move is defined {85] move \nis defined (87] defined(n) (entry assertion of log2* {24) ) {88) move is defined (89] stepdirfl is defined \n {90} ~stepratefl is defined {62} defined(steprate) (exit assertion) (62} defined(stepdir) (exit assertion) \n Twelve goals were generated by operations involving subrange types of Pascal. Note that the type integer \nis defined as the subrange -32768..32767 in this implementation. No checks were generated for overflow \nof intermediate operations in this example, because the ranges of variables happened to be such that \neither intermediate result overflow was not possible or the check was subsumed by the subrange check \nassociated with the left side variable of an assignment statement. Had such checks been necessary, they \nwould have been generated. (37]n >= O (range check for log? O..15) {37)n<= 15 (range check for log O..15) \n{45]10g + 1 >= o (range check for log 0,, 15) {45)log + I<= 15 (range check for log 0., 15) {46] twotoi \n 2>= I (range check for twotoi 1..32767) /46) twotoi * 2 <= 32767 (range check for twotoi 1..32767) \n{81 } desiredpos -currentpos >= -32768 (range check for move -32768..32767) {81] desiredpos -currentpos \n<= 32767 (range check for move -32768..32767) {88] move >= -2oOO (range check for steprate -2000..2000) \n{88] move <= 2000 (range check for steprate -2000..2000) {90] -steprate >= -2000 (range check for steprate \n-2000..2000) {90] -steprate <= 2000 (range check for steprate -2000..2000) Four goals were generated \nby the loop invariant [the STATE statement. ) {49] O < log (STATE assertion) {49) log < twotoi (STATE \nassertion) {49} log <= n (STATE assertion) {49} log <= i (STATE assertion)  One check was eenerated \nto insure lack of overflow of a FOR Iwo control variable. (In this case the loop bounds were constant, \nso th~ check is redundant, but had they been they variable a check would have been required). {42) i \n<= 14 (FOR loop count) Finally, there are fourteen entry and exit assertions. These have the meaning \nusual in verification; the entry assertions are checked at the start of a call and assumed true at the \nbeginning of the procedure, and the exit assertions are checked at exit from the procedure and assumed \nat return from the call. Hint: examineline S1 {87}n >= O (entry assertion of log2\u00b0 {30} ) {87) n <= 16383 \n(entry assertion of ~log2z {3 I ) ) {25]n<= 15 (exit assertion) {25}n >= O (exit assertion) {26] n <= \nn.old (exit assertion) (27] (n.okl = O) implies (n = O) (exit assertion) {28] (n.old = 1) implies (n \n= 1) (exit assertion) {29) (n,old > O) implies (n> O) (exit assertion) {67] (currentfxx.old = desiredpos \nold) implies (steprate = O) (exit assertion) {69] (currentpos.oId <> desiredpm.old) implies (steprate \n<> O) (exit assertion) {7 I } (currentpos old > desiredpos.old) implies (stepdir = O) (exit assertion) \n {73} (currerrtpos.old < desiredpos.old) implies (stepdir = 1) (exit assertion) {75] (stepdir = 1) implies \n(currentpas.old + steprate <= desiredpos,old) (exit assertion) (77) (stepdir = O) implies (currentpos.old \n+ steprate >= desiredpos.old) (exit assertion) Even in a modest program, the number of possible trouble \nspots is quite large. Any one of these spots has the potential of causing a major failure of a program. \nWe look upon this kind of analysis as the software engineering counterpart of structural stress analysis \nfor buildings and other engineered structures. Economics Our -system runs on a VAX I 1/780, a super-mini \n. Running time for the above example, without previous results being available, is about seven minutes \nThis is not unreasonable for the applications envisioned for the programs being verified. Reverification \nafter minor changes are faster. The example shown requires 141 proofs A separate proof is attempted for \nevery proof goal for every path No user interaction was required during the seven-minute verification. \nIMPLEMENTATION The complexity of a proof of correctness system in which the semantics of the language \nare not grossly restricted is considerable. The Pascal-F verifier is roughly of the complexity of an \ncompiler with extensive global optimization, The system is divided into two separate subsystems. The \nverifier, which takes Pascal-F programs and attempts to verify thcm, is entirely automatic and provides \nfor no user interaction once started. The rule builder, which is used to prove the validity of new rules, \nis a version of the Boyer-Moore theorem prover. The verifier proper is composed of the compiler pass, \nthe decompiler, the path tracer, and the simplifier. The compiler pass takes in Pascal-F and emits I-code \nand a storage allocation map. The compiler pass used in the verifier is the same as the first pass of \nthe Pascal-F compiler used to generate machine cnde, This insures that the definition of the language \nis the same for the verifier and the compiler. The decompiler takes in the I-code and the storage map, \nconstructs a code tree, annotates the code tree with variable names by using the storage map. constructs \nvariable set-used lists and procedure call graphs, performs transitive. closure to determine the side \neffects of each callable routine, and generates output in the form of an assertion language In which \nthe semantics of the program statements have been expressed as assertions placed on a flow graph. In \nthe assertion [angaagc, which wc call .I-code, all statements, including pruecefurc and function calls. \nare expressed as a list of variables changed along with the assertions true after the statement is executed. \nThe path tracer, or verification condition generator, takes in the assertion language text and traces \nout all possible paths by which a proof goal may bc reached in the program. Verification conditions nrc \ngenerated for each possible path to each goal, and submitted to the simplifier, which is called as a \nsubroutine When a proof attempt fails, the path tracer generates an appropriate diagnostic for the user, \nTbe simplifier is based on D. Oppen s earlier simplifier An improved propositional mechamsrn using tableaus, \na type definition facility, and a rule handler have been added. The compiler, decompiler, and path tracer \nare written in Pascal; the simplifier is in Lisp The verifier proper is composed of about 22,000 Imes \nof source code. The rule builder M a version of the Boyer-Moore theorem prover initialized with a theory \ncompatible with that built into the simplifier. The rule builder M run separately from the remainder \nof the verifier, and communicates with the simplifier by placing rewrite rules in a file associated with \na specific program being verified. There is no communication from the verifier to the rule builder. The \nrule builder is an Interlisp program. CAPTURING THE SEMANTICS OF THE LANGUAGE The Ianguagc which we verify \nis essentially taken to be defined by its compiler The compiler is a stmightforward recursive-descent \ncompiler which emits a reverse-Polish language suitable for direct execution on a suitable machine. There \nis an interpreter for this language, but normally the translated program is passed to a code generator \nfor a target machine, The reverse-Polish language is intended for execution by a stack machine. Data \ntypes are integers of vartous bit widths, arrays, and binary fixed-point numbers. Pointers are used only \nfor parameter pass!ng and for implementing Pascal WITH statements. Typical operators in this language \nare IADD (Integer addition) Pop two operands from the stack The operands are 16-bit twos complement integers. \nAdd these operands with twos complement 16-bit signed arithmetic, and push the result on the stack. It \nis an error if the result is not in the range -32768.32767. STOL W (Store). Pop an address from the \nstack. Pop w bits from the stack. Store the w bits at the address obtained. VARBL w a (Variable address \ntake). Push the address a on the stack. The object being addressed is w bits wide. INDEX w n (Array index). \nPop an address from tbe stack. Pop a 16-bit value, which must be nonnegative, from the stack. Multiply \nthe value by the item width w, then add tbe address popped. Pop w bits from the stack Store these bits \nat the address indicated The reverse.Polish Ianguagc, which wc call l-code, contains 10X opcmtors. The \nusual operators for machine ar!thmetlc arc provldcd, along with IF, FOR, LOOP, and CALL operators for \ncontrol. There are no unstructured branching operators. Each operator hm semantics similar in complexity \nto the ones above. As an example, A[i] .=x+y; with the storage allocation map x address 20, width 16 \nbits Y address 40, width 16 bits i address 60, width 16 b!ts A address 100, width 16* 100 bits (array \n0..99) would be translated into VARBL 1620 (Take value of x, which is 16 bits) VARBL 1640 (Take value \nof y, 16 bits wide) IADD (Add x and y) VARBL O 100 (Take address of A, no width) VARBL 1660 (Take value \nof i, width 16 bits) INDEX 16 (Compute address of A[i]) STOL 16 (Store 16-bit result into A[i])  We \nrefer to this technique as the concrete semantics approach. This very low-level representation of programs \nhas rather well-defined semantics. We choose to work through this low level notation because it gives \nus a rigorous definition of the exact semantics of machine arithmetic to be performed during the execution \nof any given program. Since this notation does not even use variable names, working entirely with addresses \nand offsets, it would seem that any verification results would be completely incomprehensible to the \nuser. This being unacceptable, we decompile the I-code representation into a new symbolic representation \nof the program. As we do so, we make note of the constraints imposed by the domain and range of each \nmachine operator. The restrictions of the implementation as to size of Operands are thus captured. J-GRAPHS \nAND J-CODE We divide the problem of generating verification conditions into two parts. We first translate \nthe program to be verified into a representation we call a J-graph, and then trace out the paths in the \nJ-graph. Wehave defineda textual language we call J-code which is used to represent J-graphs. The decompiling \npass of the verifier generates J-code from l-code. Thepath tracer then parses J-codc and builds a J-graph, \nthen traverses the J-graph to produce verification conditions. The J-graph is best thought of as a nondeterministic \nloop-free program. J-graphs contain the primitives NEW, SPLIT, WHEN, BRANCH, JOIN, HANG, and REQUIRE. \nThe NEW instruction is a nondeterministic, simultaneous assignment statement. The format of a NEW instruction \nis NEW <variable-list> <formula>. When a NEW instruction is executed, the variables in the <variab/e\u00adfist> \nare updated in such a way as to make the <~ormula> true. If v is a formula in the variable list, the \nnotation v.old may be used to denote the value of the variable before the NEW instruction is executed. \nAssignment statements and procedure calls are translated to NEW instructions in a straightforward fashion. \nFor example, the assignment statement: X:=x+l is translated to: NEW (X) (X = X.OLD+l) To translate a \nprocedure call, the decompiler determines every variable which could be modified as a result of the call, \neither by direct modification or through side effects, This list of variable forms the <variable-list> \nof the NEW instruction. The <formula> is derived from the EXIT condition declared with tbe procedure. \nThe SPLIT, WHEN, BRANCH, and JOIN instructions are used to model flow of control in J-graphs. SPLIT and \nWHEN are used when the flow of control forks off in different directions, and BRANCH and JOIN are used \nwhen it comes back together again. This notation mere] y allows us to represent a loop-free flowchart \nin a linear fashion, As an example, lFX>YTHEN M:= XELSEM:=M +1; would be represented by SPLIT 1 WHENX>Y \nI NEW (M) (M =X) BRANCH 2 WHEN NOT (X>Y) 1 NEW (M) (M =Mold + 1) BRANCH 2 JOIN 2 All similarly numbered \nstatements are considered to be connected. The REQUIRE statement is used to put verification goals into \nJ\u00adgraphs. The format of a REQUIRE instruction is REQUIRE <formula> Whenever wc must prove that some formula \nis always true when control reaches some point in a program, we put a REQUIRE instruction containing \nthat formula at the corresponding point in the J-graph. For each REQUIRE statement. the path tracer will \ntrace out all paths from the REQUIRE statement back to the beginning of the program, and for each path, \nit will generate and submit to the theorem prover a verification condition. Finally, the instruction \nterminates a path. An execution path which reaches a HANG instruction is not continued past that instruction. \nJ-graphs must be loop-free. Every execution path which starts at the beginning of the J-graph must be \ntraced forward to a HANG instruction without reaching any instruction in the graph more than once. Since \nwe require users to write loop invariant for all imps, we are able to break the loop at the invariant \nand thus end up with a loop-free graph. For the program fragment REPEAT Sl; INVARIANT p; S2; UNTIL b: \n where S1 and S2 are blocks of code, p is the loop invariant, and b is a boolean expression, the corresponding \nJ-code is 54 SPLIT 1 WHEN TRUE 1 BRANCH 2 WHEN TRUE 1 NEW (<list-of-variables-charrged-in-lcmp>) p <J-code \nfor s2> SPLIT 3 WHEN NOTb 3 BRANCH 2 JOIN 2 <J-code for s1> REQUIRE p HANG WHENb 3 (The graph has been \nsimplified for clarity by the omission of the statements usually present for proof of loop termination.) \nThe loop invariant is proved by induction on the number of times the loop body is executed. For the base \ncase of the induction, we must show that p is satisfied when the INVARIANT statement is reached on the \nfirst execution of the loop body. This case is handled by the path which goes back from the REQUIRE through \nthe J-code for SI, the JOIN 2, the first BRANCH 2 and back to the beginning of the program. For the induction \nstep, we must show that if p holds when the INVARIANT statement in the original program, then when the \nINVARIANT is reached again, P will still hold. This case is handled by the path from the REQUIRE back \nthrough the JOIN 2, the second BRANCH 2, WHEN NOT b, the J-code for s2, the NEW statement, and back through \nthe WHEN TRUE 1 and SPLIT 1 to the beginning of the program. Finally, the path taken by the final execution \nof the loop body is represented by the path starting at the end of the J-code fragment and continuing \nback via the WHEN b 3 statement, through the loop invariant in tbe NEW statement, and eventually back \nto the beginning of the program. The J-graph generated thus captures the semantics of the original program. \nIt is worth noting that our implementation generates J-code by a process very similar to that used for \ngenerating machine code in a compiler. The process of generating J-code is much more amenable to such \ntreatment than that of directly generating verification conditions, and allows us to draw heavily on \ntechniques from compiler technology in our verifier implementation. VERIFICATION CONDITION GENERATION \nThe semantics of tbe language having been captured in the J\u00adgrapb, the task of the verification condition \ngenerator is primarily that of tracing out all paths back from every REQUIRE statement back to the beginning \nof the J-graph and generating verification conditions during the prccess. The verification condition \nformally expresses the prqxmition If this path is taken through the program then the formula on the REQUIRE \ninstruction will be satisfied. . As soon as tbe path tracer generates a verification condition, it passes \nthe formula to the theorem prover. If the theorem prover cannot simplify the formula to TRUE, the path \ntracer displays a diagnostic message. The user is not ordinarily exposed to the verification condition \nitself. Our experience is that diagnostic information of the form shown in the example above is usually \nsufficient to allow the user to correct the problem. The actual verification condition is constructed \nfrom formulae on the WHEN, NEW, and REQUIRE statements and the path. The first step of the construction \ninvolves renaming of variables. A unique name is invented for each of the variables in the variable lists \nof NEW instructions along the path. Then one of these unique names is substituted for every variable \nin every formula on tbe path, If v is a variable in a formula, to find the unique name to be substituted \nfor v,search back along the path for the first NEW instruction that has v in its variable list. The unique \nname associated with v in that instruction is used. Special attention is required when v occurs in both \nthe variable list and formula of a NEW instruction. Recall that v refers to tbe value of the variable \nafter the NEW is executed, and v.old refers to the value of the variable before the NEW is executed. \nTherefore, the unique name for v is taken from the NEW, while the unique name for v.old is taken from \nthe previous NEW instruction on the path mentioning v in its variable list. After all the variables have \nbeen renamed, the verification condition is simply pl and p2 and ... pn implies q where q is the formula \nin tbe REQUIRE being processed, and the terms p] pn are the formulas on all the other instructions on \nthis path. It is not strictly necessary to include among the p terms formulas from REQUIRE instructions \npassed over during backwards tracing, but we do so to stop the user from getting an avalanche of diagnostics \nthat all result from a single error. [n effect, after a REQUIRE is passed, it is assumed to be true. \nThis is valid since tbe graph is loop-free. It is also very effective in reducing the number of diagnostic \nmessages and in speeding up the proofs. OPTIMIZING VERIFICATION CONDITIONS The major motivation for our \napproach to verification condition generation is the ability to provide good diagnostics. However, the \ngeneration of a separate verification condition for every REQUIRE/path pair is expensive, since the number \nof paths is exponential in the size of tbe graph. Fortunately, we have discovered some techniques to \nreduce the amount of computation required to process a J-graph. The first optimization we call NEW balancing. \nWe define a graph to be NEW balanced if for every variable v and point P in the graph, every path from \nthe beginning of the graph to P contains the same number of NEW instructions with v in their variable \nlists. If a graph is NEW-balanced, the path tracer does not have to perform a renaming operation each \ntime it processes a verification condition. Instead, renaming need only be performed only once, before \nany verification conditions are generated. The second optimization is a technique for eliminating irrelevant \nclauses in verification conditions. Consider tbe program fragment X:=x+l; y:=y+l; assert(y > O); The \nJ-graph generated for the program fragment above will contain two NEW instructions and a REQUIRE instruction. \nThe verification condition generated for the REQUIRE must make use of the NEW instruction for y, but \nit could ignore the one for X, because it has no effect on the assertion, An effective technique for \nperforming this optimization has been implemented. THEOREM PROVING Providing a sound theorem-proving \nsystem of reasonable speed is quite a challenge, We use two theorem provers, a descendant of Oppen s \nsimplifier and an essentially-unmodified Boycr-M~nrc structural induction prover. Both have been dcscribcd \ncxtcnsivcly elsewhere. [BOYER79], [OPPEN79]. The Oppen prover is an integral part of the verifier but \nthe Boyer-Moore prover is run separately to produce rules for the Oppen prover. This rather unusual approach. \nalthough somewhat difficult (O iIIIPICMeIItI aPP~Jrs LObe productive line of development. The Boyer-Moore \nstructural induction prover is quite powerful but slow, and tends to become lost if the formula being \nproven has a large number of irrelevant terms. The Oppcn prover is much faster, and a complete decision \nprocedure in its restricted universe, but not Powerful enough to allow proofs of anything involving iteration \nor induction. We thus feed the Oppen prover the verification conditions and let the user talk directly \nto the Boycr-Moore prover. The Oppen prover has been augmented with a rather simple rule handler which \napplies rules exhaustively, but only one deep, The intent here is not to provide a general rule-based \nproof system but merely to make it possible to aPPO general theorems proven in the Boyer-Moore system \nas rules in the Oppen system. We allow users to define new functions and predicates as Boyer-Moore recursive \ndefinitions. For example, for a proof of a SJrting program, we might introduce a definition of ordered \nby the following steps. The user defines ordered in Pascal-F as an uninterpreted function (which we call \na rrde Jrcrrcfiorr). The user then defines ordered to the Boyer-Moore system. (The notation here is that \nof Boyer and Moore, with the addition that sekcfa! is the array subscripting operator. ) Definition. \n(ordered A I J) . (IF (LESSP I J) (AND (NOT (LESSP (selects! A (ADDI 1)) (selects! A 1))) (ordered A \n(ADD1 I) J)) T) This recursive definition says that ordered(A,I,j) is true if ./ is less than 1, otherwise \nordered is true if and only if both AII] < A[J] + 1 ordered is true from 1+1 to J. Once this definition \nhas been introduced and found sound by the Boyer-Moore prover (which insists that such recursive definitions \nprovably terminate), we can prove some theorems about ordered. Theorem. ordered .void.rule: (IMPLIES \n(AND (NUMBERP I) (NUMBERP J) (arrayp! A) (LESSP J I)) (ordered A I J)) (i.e. if J < t, ordered is vacuously \ntrue), Theorem. ordered.single.rule: (lMPLlf% (AND (arrayp! A) (NUMBERP l)) (ordered A I 1)) (a single \nclement array is always ordered), Theorem. ordered.unchangcd,rulc (IMPLIES (AND (NUMBf3RP 1) (NUMBERP \nJ) (NUMBERP X) (arrayp! A) (OR (LESSP X 1) (LESSP J X)) (ordered A I J)) (ordered (stores! A XV) I J)) \n(storing into an array outside the limits of the ordering does not destroy the ordering), and Theorem. \nordered .extend.downward. rule: (IMPLIES (AND (NUMBERP I) (NUMBERP J) (arrayp! A) (ordered A (ADDI 1) \nJ) (NOT (LESSP (sclccta! A (ADDI I)) (selects! A 1)))) (ordered A I J)) (the ordering may be extended \ndownward one element if the ncw clement is Icss than the previous first element of the ordering). . All \nof the above rules are provable by the Boyer-Moore prover as written above. Once the rules needed for \nproving the program have been proven with the Boyer-Moore prover, the user then invokes a utility program \nwhich reads the Boyer-Moore database of proved theorems and definitions, finds all rules, (our convention \nis that any Boyer-Moore theorem with a name ending in .rtde is to be transferred to tbe Oppen system) \nand translates them into the form needed by the oppen prover s rule handler. The original verification \nis then rerun. Where applicable, the rules will be invoked automatically, and the verification conditions \nwill be proved. The process is not as automatic as it appears. Although there is no user interaction \nduring proofs in either system, both systems often require help. In the Boyer-Moore system, one provides \nhelp by suggesting lemmas which help the prover to prove the rules needed for tbe program proof. This \nform of manual subgoaling is often difficult, One is required to work strictly upward, devising lemmas \nwhich are both provable by the automatic prover and lead the prover toward reaching the desired goal. \nSimple recursive properties of arrays, such as the ordered example above, are within the power of the \nprover to handle without help, but more complex properties can be difficult. Once rules deemed sufficient \nto prove the program correct have been generated, the problem is not over, Since rules are invoked only \none deep (a rule will not be applied to a form created by another rule), the user may have to provide \nsome assistance to the rule handler in the main verifier, Again, this is done by adding intermediate \nproof goals. In the Pascal-F source program, one adds ASSERT statements immediately ahead of the statement \ngenerating the proof goal with which one is having trouble. [t is always the case that Pascal-F source \nof the form ASSERTp; ASSERT q; will generate a verification condition for which the rule handler would \napply a rule of the form p implies q, if such a rule were present. Further, once an assertion has been \nproven, theverifier will assume its truth in Pascal-F code following the assertion. Thus, when faced \nwith a difficult proof, one adds ASSERT statements to force the verification conditions into forms which \nthe rule-handler will recognize as immediately provable from a known rule. We thus have adequate power \nto handle difficult problems. We usually hold most of this power in reserve, and attempt to prove only \nsimple properties of moderate-sized programs. The level of expertise required to push a difficult proof \nthrough the system is presently rather high compared to the formal mathematical knowledge of most working \nprogrammers. Our current thinking is that programmers will use the verifier with a database of previously-proved \nrules, obtaining the aid of a verification expert when a new rule is necessary. Insuring the soundness \nof the two-prover approach is non-trivial but possible given the power of the Boyer-Moore system. If \nthe semantics of a function were inconsistent in the Bayer-Moore and Oppen systems, unsoundness would \nresult. [t is necessary to prove that thedcfinitions in the Boyer-Moore system arc consistent with the \naxioms built into the Oppen system. Such proofs are necessary for any function symbol known to botb systems. \nThe function symbols so known include only the primitives of arithmetic, the Boolean connective, the \narray operators selecr and ,s(ore (which wc call selectal and srorea!), and some type predicates such \nas arraypl (true for array-valued variables) and rmmberp! (true for the natural numbers). Machine proofs* \nof the axioms of the Oppcn system IMVC by Dr. John Privilera oi Ford Aerospace been generated using the \nBayer-Moore system initialized with our definitions. This crosscheck insures that our definitions of \nthe Oppen objects in the Boycr-Moore system are consistent with the Oppcn built-in rules. Once the built-in \ntheories have been shown to be consistent, we can allow the user to introduce new functions in the Boyer-Moore \nsystem, but such functions will be uninterpreted functions in the Oppen system and thus cannot introduce \ncontradictions. DIRECTIONS FOR FURTHER WORK Our immediate plans call for using the verifier on a pilot \nproject to determine the effectiveness of these tools on certain types of embedded systems. We may then \nconsider further tool development. We have a number of ideas for obtaining substantial speed increases \nin the automatic proof process. We will hint at one of tbcse. By integrating path tracing with theorem \nproving, we can avoid the generation of verification conditions in the traditional sense and take advantage \nof the fact that, on many paths, we need not trace back all the way to the beginning of the program before \ndiscovering that the proof goal has been satisfied. Central to this scheme is the fact that the Oppen \ncombination of methods stores the state of a proof in a form which can be pushed and popped, One could, \nin theory, start by providing the prover with the proof goal, and then tracing backwards through the \nJ-graph, add information to the proof system as each assertion is passed. If a proof succeeds before \nthe head of the program has been reached, one can stop tracing the path at that point. Not only has one \nverified the proof goal for the path being traced; one has verified the proof goal for all paths which \ninclude the part of the path being traced which was actually explored. Even better, one need not throw \naway all tbe information now inside the prover to trace tbe next path. One need only back down the J-graph \ntothelast JOIN point, popping out assertions from the prover state as they are passed over while backing \ndown the tree. When the back-down process reaches the original proof goal node, the proof is complete. \nIn many cases, this will occur long before the entire sheaf of paths leading to the proof goal has been \nexamined. This line of thinking leads us to suspect that future program verification systems will not \ngenerate verification conditions at all, but will operate on graph representations of the program similar \nto our J\u00adgraph. Other benefits besides speed may be obtainable in this way, including improved diagnostic \nmessages and more source-program oriented statements about why the proof is failing. CONCLUS1ONS 1. Proof \nof correctness is currently usable as a technique for improving the reliability of small real-time programs. \n2. The machine resources required for machine proof, while substantial, arc not unremovable. 3. Autcrmatic \nproof systems must, to be sound, refuse to accept new user provided lemmas without proof. 4. A verification \nsystem should operation the assumption that most of the programs fed to it will initially be incorrect. \n 5. Adequate user diagnostics arc not difficult to implement, but must be implemented if tbe system is \nto be usable.  6. When each requirement to be proven generates a separate verification condition for \neach path, and the diagnostic messages produced fora proof failure include the requirement which could \nnot be proven and the path for which it could not be proven, users are able to find tbe problem without \nexamining the verification condition. 7. It is sometimes easier to capture the semantics of the language \nfrom a compiler intermediate form rather than trying to axiomatize the source language. 8. Usable verification \nsystems are not small systems; we conclude that constructing a verifier for a given language is a task \nof roughly equal difficulty with constructing a good optimizing compiler for that language.  REFERENCES \nBOYER79 Boyer, Robert S, and Moore, J. Strother, ,4 Cornputatiomd Logic, Academic Press, New York, 1979. \nBOYER80 Boyer and Moore, private communication. FLOYD67 Floyd, Robert., AssiKninx A4earrinm to Prorrarrrs, \nMa~hematicai Aspect; o~ Comput~r Science,-Proc. Symp. Applied Math. Vol XIX American Mathematical Society, \nProvidence, R,l, 1967 GERMAN81 German, S. M., Verifying the Absence of Common Runtime Errors [n Computer \nPrograms, PhD Thesis, Harvard University, 1981. OPPEN79 Oppen, Derek, Simplifica[icm by Co-operating \nDecision Procedures, Computer Science Department, Stanford University, 1979. STANFORD79 Luckham, German, \nv. Henke, Karp, Milne, Oppen, Polak, Scherlis, Stanford Pascal Verijier User Manua/, Computer Science \nDepartment, Stanford University, 1979, \n\t\t\t", "proc_id": "567067", "abstract": "Despite the attractiveness of the concept, attempts to date to use proof of correctness techniques on production software have been generally unsuccessful. The obstacles encountered are not fundamental. We have implemented a proof of correctness system to be used for improving the realiability of certain small, real-time programs. It appears that many of the problems of past systems can be avoided.This work is supported by the Long Range Research Program of the Ford Motor Company, Dearborn, Michigan.", "authors": [{"name": "John Nagle", "author_profile_id": "81100632377", "affiliation": "Ford Aerospace and Communications Corporation", "person_id": "P145297", "email_address": "", "orcid_id": ""}, {"name": "Scott Johnson", "author_profile_id": "81100449504", "affiliation": "Ford Aerospace and Communications Corporation", "person_id": "PP24017825", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/567067.567074", "year": "1983", "article_id": "567074", "conference": "POPL", "title": "Practical program verification: automatic program proving for real-time embedded software", "url": "http://dl.acm.org/citation.cfm?id=567074"}