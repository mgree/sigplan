{"article_publication_date": "01-24-1983", "fulltext": "\n Permission to make digital or hard copies of part or all of this work or personal or classroom use is \ngranted without fee provided that copies are not made or distributed for profit or commercial advantage \nand that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, \nto post on servers, or to redistribute to lists, requires prior specific permission and/or a fee. &#38;#169; \n1983 ACM 0-89791-090-7 $5.00 We now expand on each of these paradigm shifts. 3. From Effect to Entity \nTraditionally computation has been viewed, both by programmers and by mathematicians, as being performed \nfor its effect The classical basic instruction is the state-modi~ing assignment statement. The classical \nmodel of computation is an automaton (finite-state, pushdown, Turing machine, etc.) with a state or configuration \ntransition function that describes how each state or configuration leads to its successor. This view \nof computation is gradually being superseded by a more entity or object onentcd view. The computing universe \nis regarded as being populated with entities. The dynamics of the universe is no longer described in \nterms of the state-to-state transitions of automata but rather in terms of the site-to-site transitions \nof entities, for which two of the more popu!ar settings are recursive function evaluation and dataflow \narchitecture. There are several forces acting to bring about this paradigm shift. First, mathematics \ntends to emphasize entities over effects: consider the classical mathematical model, the algebraic structure, \nconsisting of a set together with operations and relations. Second, there is the increasing prevalence \nof parallelism, as nets connect computers into inherently parallel configurations, as cheap microprocessors \nstart showing up several to a system, and as silicon compilers that translate algorithms into VLSI designs \nbecome a reality. The sequence-of-states model, well suited to serial computation, rapidly becomes unworkable \nin the presence of parallelism. Third, there seems to be a psychological advantage to being able to externalize \nconcepts, that is, to treat them all conceptually as nouns instead of verbs, relations, etc. We speculate \nthat this advantage comes from the simplicity of the typelessnew resulting from complete externalization. \nWe will raise the issue of typelessness again in more detail in a later section. 4. From Serial to Parallel \n4.1. Which is the Basic Concept? One issue here is which is the more central concept, serial or parallel \ncomputation? There is an analogous question for determinism versus nondeterminism. For both questions \nthere are ways of formalizing the question to make the answer come out either way. For the latter question \nthe predominant view of automata these days, which is to represent them in terms of constraints on their \nstate transitions, favors nondeterminism, with determinism being merely a special case of nondeterminism. \nThus instead of dividing automata into two classes, the deterministic and the nondeterministic, we treat \ndeterminism as a property a nondeterministic automaton might or might not have, the linguistic awkwardness \nof the prefix non notwithstanding. This poinl of view has been firmly supported by the essentially universally \naccepted formal definition of automaton for the past two decades. Our own intuition about serial versus \nparallel is that serial is a special case of parallel, rather than vice versa, However unlike the situation \nwith determinism vs. nondetcrminism, there has not been a similarly universally accepted formal definition \nof the notions serial and parallel. This makes it much harder to resolve the question by appeal to a \ndefinition. One can see how hard it is to relate the two notions formally by considering how they relate \nin current programming languages. Let us consider C [under Unix), Ada, and Hoare s CSP, which are among \nthe better known languages offering parallelism. In each of these languages the basic computational paradigms \nare serial. Parallelism is introduced by modclling the computing universe as a set of serial computers \ncommunicating with each other. (In the case of C under Unix concurrency and interprocess communication, \nlike 1/0, is supplied by Unix kernel calls, and is not part of the C language proper. However it is a \nfine example of a language in current use that in practice doea offer concurrency.) These examples strongly \nsuggest that the relation between the two notions is that serial computation is a necessary prerequisite \nto defining a notion of parallel computation. ~is view of parallelism obviously does not reflect what \nactually happens inside a machine. Within any serial computer one can observe parallelism at many levels \nin its implementation: in the many electrons that flow through a wire (to go to an absurdly low level), \nin the many wires making up a bus, and in the many microprocessors that can be found in today s large \nmainframes, to name just some examples. For whatever reason the programming language of a mainframe is \nserial, it is not because the hardware itself is serial. What we would like is a model of computation \nthat not only reflected this ubiquity of parallelism but that at the same time subsumed the notion of \nserial computation, making it merely a special case of parallel computation. The chief advantage of this \nwould be in simplifying both our theoretical models of computation and our programming languages. A program \nwould then be serial more as an accident than through not using parallel constructs, just as determinism \narises more by accident than by avoidance of nondeterministic constructs. Hewitt [Hew] has advocated \njust this simplification of programming languages, in the form of his message-passing Actors theory. \nAlthough some of the details differ (actors have no output), the underlying rationale appears to be similar. \n4.2. Need for a Formal Model Comparing the situation once again with (non)determinism, there is still \none missing ingredient, namely a formal semantica that converts the precedence of parallel over serial \nfrom a matter of taste to a mathematical definition. The candidates for a model of parailel computation \nthat we take at atl seriously are Petri nets pet], Milner s Calculus of Concurrent Systems (CCS) [Mil], \nthe Kahn-MacQueen model of determinate processes [KM], the Brock-Ackerman Scenarios model [BA], and our \nown model of processes [Pra82]. Among these models the greatest unity, and the longest history, can be \nfound among [KM], [BA], and ~ra82], which together constitute a monotonically improving sequence of models \n(in that order). We consider the resulting model to supply exactly the missing ingredient. The Kahn-MacQueen \nmodel defines an n-port process to be an n-ary relation on the set of all histories (sequences of data). \nThis model was developed only for determinate processes, and was suspected by its authors of not being \ndirectly usable for nondctcrminism, This suspicion was formally contirmed by an enlightening countcrcxamplc \ndue to Rock and Ackerman, who also proposed the necessary modification to the model to extend it to nondeterminism \n[ltA]. The modification was to introduce inter-history tcrnporal precedence information. The Brock- Ackerman \nmodel was adopted and further extended by the present author ~ra82] to cxmer for process composition \nin a satisfactorily formal way, and to support an rdgebraic view of process composition analogous to \nthe afgebraic view of serial\u00ad program composition mandated by the structured-programming movement. Besides \nthe greater economy of subsuming the scnal with the parallel, there is also the issue of irrelevant serialization \nforced in a serial language. Thus x: =a; y:= b is a pair of assignments whose order must be given even \nthough it is clearly not needed. This issue can bc met piecemeal by adding yet more constructs to the \nlanguage, e.g. parallel assignment. However starting with an inherently parallel language from the beginning \nsolves this and related problems just as effectively and more generally. fl re Viron programming language \nis noteworthy in having NO explicitly saial constructs. 4.3. Data Structures Auxiliary to the vertical \nintegration of processes into Viron s control structures is its incorporation into the data structures \nas well, fhe dcnotational semantics of processes given in ~ra82] imbues them wilh the status of object, \npermitting prccesses to be thought of as data with the same charter of rights pop] or mobility pra79] \nas integers. Taking this development one step fi,rrther, we have chosen to make processes not the organic \nmolecules of our language but rather the elementary particles. That is, every datum, whether of the complexity \nnormally associated with processes, or as simple as a character or an integer, is defined to be a process. \nin this we are again following Hewitt [Hew], in whose development everything is an actor. The main conceptual \nobstacle to thinking of an atom as a process is that atoms seem too simple to be thought of in this way. \nHowever essentially the same argument was made for millennia excluding zero m a legitimate number. Yet \ntoday zero is almost universally acknowledged to be, though less than 1, no less a number than 1. Of \ncourse one might come up with an unconvincing behavior for numbers viewed as processes. Hewitt embeds \nthe knowledge that 3+2= 5 and 3x2=6 in the actor that is the number 3, which makes 3 a much more complex \nprocess than seems intuitively necessary. The Viron idea of a number, and more generally of any arom, \nas a process is that, afthough the atom does output something in response to each input, the output is \nindependent of the value of the input and consists of the atom itself. The main reason for this choice \nis to tit in with our extensional view of processes, in which two processes with the same behavior must \nbe the same process. If an atom was unresponsive all atoms would collapse to the same atom. A useftd \nfringe benefit of this convention is that, following our straightforward definition of addition, the \naddition of a nurnbcr n to an array of numbers results in the addition of n to each of the elements of \nthe array, as will be seen in the account below of Viron. 4.4. The Process Compiler One might well ask \nwhy can t the notion of process be excluded from the programming language proper and made a part of the \nsubroutine library, on the principle that the programming Ianguagc only need supply a basis from which \nto extend via the subroutine library. All process-oriented notions in Unix arc supplied in this way. \nfor cxmnplc. (It should bc realized that C was dcvclopcd by the dcvclopcm of Unix as part of the Unix \neffort: thus this expulsion of the notion of process to the library was a consciously made decision in \nthk case, not an accident resulting from an inherited language.) A plausible motivation for putting the \nnotion in the language is that parallelism is not definable in purely serial terms, much as one might \nargue that nondetcrminism is not dcftnable using only deterministic concepts. 1lowcver this argument \nassumes that the library is a true language cxtcnsirm in the sense that all its funclions could have \nbeen wrhtcn in lhc Ianguagc. This is actually not the case in the Unix example, which requires non-C \nassembly code in its system calls in order to access the kernel, which is the source of parallelism in \nUnix. Thus it is possible to introduce parallelism into the language via the library even if parallelism \nis not definable using just the bisic language, given that the library is permitted to step outside the \nbasic language. Our actual motivation is that we want to expose parallelism to the optimizing compiler. \nThe state of the art of parallelism forces it to be an interpreted concept, duc to its having a purely \noperational definition, one which admits only titeral interpretation of parallel constructs by an interpretive \nmachine. If a more abstract definition of parallelism is given, it becomes possible for an optimizing \ncompiler to choose from a variety of equivalent implementations in compiling a given parallel construct. \nThe definition should be maximally abstract; only necessary detail should be retained in the definition, \n5. From Partition to Predicate Types Another paradigm shitl has to do with the nature of types. The partition \nview of types considers type to be a function from the universe onto a partition of that universe: each \nindividual is mapped to the block of that partition containing that individual. Thus type(3) = integer, \ntype(3.14) = real, typc([3,1,4]) = list, type(cos) = real->real, ami so on. In contrast to the pantition \nview, the predicate view of types abandons the attempt to keep types disjoint, and permits each individual \nto be of many types. For example 3 may simultaneously be of type real, integer, positive integer, integer \nmod 4, mod 5, mod 6, etc. You yourself may simultaneously be a human, a teacher, an American, a Democrat, \na Presbyterian, a non-smoker, and so on. There is no such thing in the physical world as THE type of \nan object, although any given context may suggest a particular predicate as being the most appropriate \npredicate to be called the type of that object in that context. The partition view can admittedly be \nmade to work in the simple environments that come with to{day s programming languages. However as the \nenvironment gets richer the partition view becomes progressively more intractable. Imagine a programming \nIanguagc in which for every pair i,j of integers with i<j there is a type i. .j of integers in the interval \nfrom i to j. A pure partition view of types would require that the integer 3 not-be one individual but \nmany, onc lfor each internal containing 3. This may secm laughable, yet it is a logical extension of \nthe more readily accepted idea that the real 3.0 is distinct from the integer 3. (It is noteworthy that \nPascal adopts a prcdicatc-like approach to its treatment of the range subtype, while remaining partition\u00adoncnted \nelsewhere, thereby avoiding this problem in its more extreme forms.) The predicate approach to types \nsimplifies this by having only one individual recognizable as 3, common to all intervals containing 3. \nThis individual can even be identified with the individual 3.0 if one wishes to make the integers a subset \nof the reals, a simplifying view of the integer-real relationship which has much to recommend it. The \npredicate view has a certain amount of support from modern mathcma[ical logic. There has been much study \nof logical theories incorporating various notions of type. Indeed l<USSCI1 Sapproach to comlrolling the \nlogical paradoxes of l~rcgc s theory was to introduce a type hierarchy of the partition kind. However \nthis approach eventually supcrscdcd by the typeles$ Wiis thconcs of Zcrmelo-l:racnkcl and Bernays-Goedcl. \nAdmittedly the llcrnays-Gocdel thcor,y did go so far as to postulate a two-type hierarchy of sets and \nclasses, but it is noteworthy that the typcless (but not prcdicatclcss) Zermclo-Fraenkel theory is the \nonc that today is taken (nnodtdo details) as the formal definition of set theory, which in lU rn is acccptcd \nby many mathematicians as supplying the formal basis for all or tnalhcmatics. While Zcrmclo-lJraenkcl \nset theory may from time to time be subjected to at(acks, it is rarely if ever because of its typelcssnesa. \nThe entity oriented approach that we wish to explore will be characterized by the typclessoess of the \npredicate approach, in that afl entities will belong to a single domain. fhus our approach will have \nthe flavor of LISP S typelessness, though with what we feel is a sounder rationale than has been advanced \nby the Lisp community to date for typelessness. 6. From Computable to Definable It is unthinkable today \nto propose a noneffective model for a computing environment. How would you implement it? It is unimplcmentable \nby definition. Nevertheless we feel that this insistence on effectiveness produces inarticulate programmers. \nWe propose to include noneffective concepts in our models to simultaneously enhance the expressive power \nof and simplify the language. To begin with, consider the set of even integers and the set of primes. \nThese are objects that are very natural to bc able to refer to in a program; certainly in natural language \nthey are referred to all the time. Having these objects in one s domain is only noneffective if one insists \non a traditional representation of sets as bit vectors or linked lists of elements. If those two objects \nwere all there were in the domain one bit would serve to represent each. However suppose we CIOSCthis \ntiny domain under Boolean operations. We now want to manipulate Boolean combinations of these two sets. \nCan this be done effectively? Yes: equality between expressions is decidable since it reduces trivially \nto the decision problem for two-variable propositional calculus, with the two sets playing the role of \nthe two variables. Four bits suftlce (exercise: and arc necessary) to represent the sixteen possible \nBoolean combinations of these two sets. Now let us go a little further and add a unary operation to the \nlanguage that adds onc to every element in a set. Suddenly we can express infinitely many distinct subsets \nof the integers, even without the evens. Ncvcrthcless can we still compute in. this language? In particular \ncan we always decide whether two expressions denote the same set? Maybe, maybe not (let us know if you \nfind out), but clearly we cannot continue to add such reasonable constructs to the language for long \nwithout arriving at a non-effective domain, one in which not even equality is decidable. R. Popplcstone \nran into this predicament when drawing up his charter of rights for data [Pop], where his notion of datum \nwent bcyood just itttegcrs and boolcans. Hc wanted every datum, including objects such as arrays and \nfunctions, to be assignable to a variable, passable as a parameter, and returnable as the value of a \nprocedure. 1Iowever he did not require that it bc possible to tell whether two data were equal. More \ngenerally, he did not require that procedures behave the same with different representations of the same \ndata. Why? Because equality is undecidable for functions, inter alia. We consider Popplestone s charter \nof rights to bc substandard. Under that charter data is not abstract A programming Ianguagc should assign \nabstractness higher priority than cffcc[ivcncss. Ibis iss logical extension of (hc programming rrzlc, \nMake it work before you make it fast. Ile extension is to treat Lhc programming Ianguagc as being primarily \na descriptive tool, and only secondarily as a medium for achieving performance or even effectiveness. \nOur approach to implementing a noneffective domain is to irnp]cmcnt succinctly spcci tied decidable language \nfragments. The key here is the exisbmcc of easily recognized decidable fragmcnls of undecidable Ianguagcs. \nWc have dcvclopcd this idea in [Pra80] and [Pra8 L] for the cmc of program vcn tication. The idea is \nnot spccitic to vcrilication however, and can be applied just as readily to execution in a noncffcctivc \ndomain. Those fragments may grow in size and number as the supply of algorithms improves; all that noneffectiveness \ndoes here is to prevent a complete implementation of Viron. I%e progmer should accept such incompleteness \nwith the same good grace that the mathetnatician accepts it for his logical took, which inevitably must \nbe incomplete. Making the break not only with performance but with effectiveness removes a source of \nworry from the programmer much as having an undo key reassures the user of a word processor. I_he programmer \ncan get on with the job without the distraction of whether a given way of saying something will run fast, \nor even will run at all. There is a feeling in some programming circles that the burden of performance \nshould be placed on the compiler. This is possible up to a point, although no compiler can assume the \nfbll burden, since there are always new algorithms to be discovered. Our position is that exactly this \nsituation holds for effectiveness as WCI1as for performance. A compiler can deal with some of the issues \nof finding an effective way to execute a program, but no one compiler can discover every such effective \nway on its own, it must sometimes depend on the programmer. Just as the impossibility of the perfect \noptimizer does not imply the uselessness of optimizers, so does the impossibility of the perfect automatic \nprogrammer not imply the uselessness of compilers that can find effective methods in many cases. 7. From \nSyntactic to Semantic Consistency Effectiveness is only one of the inhibitors of articulate expression. \nThe current approaches to controlling inconsistency constitute another. Russell s tbcory of types was \ndesigned to avoid the inconsistencies Russell and others found in Frege s logical theories. The introduction \nof a hierarchy of types into the A-calculus serves a similar end. In contrast to these syntically cautious \napproaches are the syntactically casual languages of Schoenfinkel [Sch] (combinatory logic) and Church \n[Chu] (the untyped A calculus). Here paradoxes of the traditional kind maybe obtained at the drop of \na hat; for example either language may express the seemingly nonsensical concept of a fixed point of \nthe integer successor function. Yet the languages are more user-friendly than ones which introduce typing \nrcstlictions aimed at preventing such paradoxes. Are such languages merely syntactic curiosities devoid \nof referential significance, or can they bc considered to actually denote, despite the inconsistencies? \nSurely they could not denote, or they would not be inconsistent. Dana Scott has worked out the details \nof an approach to making semantic sense of paradoxical and hence ostensively meaningless languages, which \nis to computation as complex numbers are to electrical impedance. The idea is to augment an otherwise \nnormal domain with fimy or information-lacking elements. Fuzziness is represented with a partial ordering \nof the domain in which x dominating y indicates that x has more information than y, which can be rephrased \nwithout using the word information by saying that y might on closer examination turn out to bc x. A very \nsimple example of the shift from syntactic to semantic consistency is provided by Roolcan circuits. A \nsimple syntactic constraint on a circuit that guarantees predictable stalic behavior is that it bc acyclic. \nThis condition may be relaxed with caution to yield more interesting behaviors. I Iowever if in the interests \nof simplicity all conditions on circuits are dropped, we can then connect the oulput of an invertcr (a \ndcvicc realizing the unary Boolean operation of complcmcntation) to its inpu~ Ilis provides a simple \nphysical model of the logical paradox implicit in the equation x = -x. Classically a paradox means an \ninconsistency, which in turn means there is no model of the paradox -the universe should disappear when \nwe feed the inverter s output back to its input! This actually does happen, at least in the sense that \nthe universe of pure truth values no longer provides an adequate account of the circuit behavior. With \nthe feedback loop the inverter firnctions like an amplifier with negative feedback, with its common input \nand output stabilizing at a voltage somewhere between logical Oand 1. fhis intermediate voltage is not \na part of the O-1 Boolean universe, but it is a part of a more detailed model that admits invalid or \nuninformative data in addition to the regular data. Thus if we postulate three vafues, O, *, and 1, with \nO and 1 considered maximally informative and * uninformative, and take the response of an inverter to \nthe inputs O, *, 1 to be respectively 1, *,0, then we may solve x = -x with x = *. The key feature of \nthis simple example is that we have moved from a syntactic to a semantic solution to the problem of paradox. \nInstead of relying on the absence of cycles or some other syntactic constraint to prevent paradoxes, \nScott s approach is instead to expand the universe to account for and hence dispose of paradoxes. Scott \ns approach was motivated by just the sort of %er\u00adfnendly syntactic sloppiness that actually arises in \nreal programming languages, such as the abifity in Algol 60 to pass as a parameter to the fi,mction f \nany function including f itself. More recently Saul Knpke [Kri] has made a very similar proposal to the \nphilosophical community with a paradox-explaining theory of truth that has been received with remarkable \nenthusiasm by the philosophical community. Kripke s theory of truth is founded on the existence of tixpoints \nof monotone functional in a complete partial order, just as with Scott s theory. It should be observed \nthat the Scott-Strachey school of mathematical semantics that developed at Oxford has made two distinct \ncontributions to programming semantics: the notion of denotational semantics as a homomorphism from expressions \nto vahrcs, and the notion of the information order as a basis for a fixpoint-of-monotone-functional semantics \nfor resolving paradoxes. Yet little attempt is made by computer scientists to distinguish these two contributions. \nand the tarn denotational semantics is frequently applied to both of them as a single package, with the \nimplication that the latter is a vital component of the former. In fact one can carry out a very comprehensive \nprogram of semantics without any reference to rur information ordering. This is done for example in such \nprogram logic schools as algorithmic logic, dynamic logic, and temporal logic, where the semantics is \nof a homomorphic character but with no dependenm on ordered domains. It is afso done in @?ra82],the foundations \non which the semantics of Viron are buih. When paradoxes emerge however in response to lax syntax, Scott \ns information order becomes a key ingredient of a successful semantics. In the commonest account of Scott \ns theory (not Scott s own account however), based on complete partial orders (cpo s, partial orders in \nwhich every directed set has a sup), the maximal elements of the cpo can be considered the normaf or \nideal elements, the objcc~$ we consider to normally populate the universe. The other elements are approximations \nto the ideal elements, in the same sense as intervals with rational endpoints on the real Iinc arc approximations \n10 reals. In the cpo account, unlike in Scott s account, lhcrc arc no ovcrspcciticd clcrnents containing \nmore information than the ideal elements. The simple expressions of the language, e.g. the numerals, \narithmetic expressions over numerals, etc., are considered to denote ideal elements. However some of \nthe more complex expressions will only denote approximations. In particular the paradoxical expressions \nare guaranteed to denote approximations: no matter how closely you inspect a paradoxical element you \ncannot tell what ideal element it should denote. By withholding information in this way, the model prevents \nyou from arriving at a contradiction. I?or example an expression denoting a fixed point of the successor \nfunction will denote an approximation to integers, usually onc that approximates afl integers (integer \nbottom ). The main advantage of Scott s approach is the way it can simplify the language, which no longer \nneeds to be sensitive to inconsistencies. On the other hand it does complicate the model. Yet even here \nthere is an advantage, for the model can be used to permit the relocation of the irnplementability boundary \nfrom syntax to semantics, a novel concept for programming languages but one that we believe can be used \nto good effect Lxt us see how this works. Normally a system designer choclses an implementable language, \nand as new needs arise augments the language with additional implementable constructs. With Scottish \nmodels it is possible to fix an abmrdly over-expressive yet simple language once and for afl, and to \naugment not the language but the interpretation of the language, by increasing the information available \nto the language interpreter about the interpretations of expressions in the language. (Interpretation \nI, mapping expressions to domainl elements, is considered an augmentation of interpretation J when I \ndominates J, i.e. I(e) dominates J(e) for all expressions e in the language.) As a trivial example, one \ncould start out with a semantic function that mapped numerals to integers, and afl other expressions \nto the bor,tom element of the domain. Although the language might have addition, that function would \nin effect start out as the everywhere undefined function. Then one could add some set of compu@ble arithmetic \nfunctions by raising from bottom to integers the interpretations of all expressions containing only those \nfunctions and numerals, at the same time providing the necessary implementation of this increase. At \nsome point one might raise the interpretations of set of evens and set of primes to the appropriate sets, \nalso ideal elements. As algorithms for evaluating various linguistic fragments of set theory came to \nlight one could implement them and so raise the interpretations of corresponding expressions. (If desired \none might also add hctuistics for noneffective fragments, thereby firther raising some interpretations, \nthough by .ill-cbaractcrized amounts for an ill-characterized subset of the language.) The advantage \n(of putting language subsetting in the semantiw instead of in the syntax is that it decouples language \ndevelopment from implementability considerations. This in turn makes it possible to make the full language \navailable immediately for development of algorithms without waiting for full implementation support for \nthose algorithms. These would sometimes be noncftisctive algorithms when they referred to as\u00adyct undefined \nfunclimrs, but they still would serve the useful purpose of specifying problems that could then bc rewritten \nmanually in an effective sublanguage. A language as powertirl as this can be built up until it subsumes \nany given lrequircments language. From this point of view implementation reduces to translation within \nthe language to achieve a raising of the interpretation (meaning) of the translated expression. The raising \nhappens because, for example, some noneffective function or concept (e.g. quantification) is translated \nto a more effective :forrn. The definition of correctness of an implementation is lhat it dominate the \nexpression it was translated from (where the ordering bctwccn expressions is just that induced by the \nordcnng on the interpretations of those expressions, i.e. for cx.prcssions c and f, u[f when I(e)< I(t)). \nIf one views an automatic prograrnmm as a function mapping expressions to expressions in this language \nthen the automatic programmer is correct just when it is monotonic. This one-language view of the relation \nbetween requirements and implementation is appealingly simple. Yet it fits naturally into the real world \nof requirements and implementations, which typically form a hierarchy in which implementations turn into \nrequirements as one programs from top to bottom. fhe homogeneity of our rcquircmcnts and implcmcrttations \nsimplifies this dual view of rcquircmcntdprosrams by expressing them atl in a common language. 8. Lisp \nas a Benchmark Lisp is a good benchmark against which to measure progress in language design. Despite \nits age (approaching the quarter century mark) it still ranks as one of the primary sources of insight \ninto the principles of programming language design. Lisp, at least pure Lisp, emphasizes entity over \neffect Lisp treats its complex data, lists and (to art extent) functions, as objects to be moved around \nthe computing environment with the same mobility as integers, putting demands on the storage management \nalgorithms beyond what sutTces for a domain of say integers. Furthermore Lisp emphasizes the homogeneity \nor typelessness of the predicate approach to typing. However pure Lisp does not gracehrlly handle the \nprocess\u00adoncntcd notions of state, memory, coroutine, or concurrency, concepts that we at best feebly \ncaptured in a domain of recursively defined funclions and fimctionals on a basis of lists and atoms, \nIt is usual to think of these as only recently being demanded, but we ase of the opinion that their need \nhas always been present, and that only the lack of the necessary concepts has prevented the Lisp designers \nand users from recognizing these nwds as process-oriented needs long ago. We believe that the impurities \nof Lisp -PROG, SETQ, GOTO, RPLACA, RPLACD, etc. -arose in response to such needs, and met them by reverting \nfrom the entity paradigm to the effect paradigm, where it was afready understood intuitively how to implement \nprccess oriented notions. The price for this step backwards was the loss of mathematical meaning for \ntic concepts of Lisp, to the extent that being effect-onented leads to clumsier definitions than being \nentity-oriented. The similarity between pure Lisp and Viron is that both are entity oriented. The difference \nia that Viron entities are specifically intended to model the notions of state, memory, coroutines, and \nconcurrency, 9. Foundations As stated in the introduction, this is the second paper of a series whose \nfirst paper [Pra82] dcscnbed the mathematical foundations for a notion of process. We repeat here the \nbare definitions. There are two views of processes, internrrl and exfemal. he internal view is the more \ndetailed one, and depends on the notion of a ner of processes, without regard for what actual data flows \nbetwccm them. All processes have two countable wts of inpuf and ouIpur porfs, 11,12,11,,..and 01,02,0,..., \natl but finitely many of which will norrnalry go u nuscd. fiis arrangement avoids the encumbrance of \na syntactic classification of processes according to their port structure.) The net consists of zero \nor more disjoint communication links each connecting one output port to one input port; each port is \nconnected to at most one link. A net can be studied in its own right, or as a means of implementing a \nprocess, in which case certain of its processes are associated with ports of the implemented process. \nIn [Pra82] each port-associated process was assumed to use only onc of its own ports. Onc minor improvement \nwc make here to Ihat model is to collect all lhc port-associated proecsses of a net into a single process, \ncalled the exterior process of the net. Port 1, of this process corresponds to pon O. of the implemented \nproe~ in the sense that data sent by the n&#38; to 1. will appear as output from port O. of the process \nimplemented ~ this net. Dually data arriving at ~ort I. of the implemented proccsscs enters the net of \nthe implementation of that process via port Oj of the net s exterior process. To ask how the extcnor \nproms of net N is implemented is to ask what rwtwork the process implemented by N is embedded in. This \nviewpoint rcftects a certain symmetry bctwccn the exterior and interior of proccsscs that sharpens the \nrole of the process as network interface. A link is to be thou ght of not in the information theoretic \nsense of a channel having capacity, or affecting its messages, but rather merely as an arbitrary boundary \nbetween two processes. A datum flowing between two processes must at some time cross that boundary; this \nis called an net event. It either happens or dots not happen; there is no probability, distortion, delay, \nor queuing associated with the event Imperfections in the net must always bc associated with processes, \nA transmission link that accumulates, permutes or distorts messages must bc modelled as a process in \nour nets. The question of whether a link has a finite queue, an infinite queue, or no queue, is translated \nto the question as to what buffering mechanisms a process provides at each of its input and output ports. \nThis in turn is captured abstractly in the reliability of a process -finite buffers will reveal themselves \nthrough the possibility of imperfect behavior. Formally, a nef evenf is a link-datum pair, interpreted \nas the traversing of that link by that datum. A net frace is a partially ordered multisct of net events, \ninterpreted as a possible computation, with the order specifying which events necessarily premded which \nother events in time. Ncccssary temporal precedence is a primitive notion in this theory., A nef behavior \nis a set of net traces. These three notions, net event, net trace, and net behavior, constitute the internal \nview of a process. In the external view, a process everrf is a port-datum pair, a process frace is a \npartially ordered mtrltisct of process events, and a process behavior is a set of process traces. (The \ntight correspondence between the internal and external views of a process should be noted.) There are \ntwo connections to be made between the internal and external views of a process, Network traces need \nto be consistent with the behavior of the constituent processes of the net, achicvcd by requiring that \nthe restriction of each net tice to any constituent (i.e. non.extcnor) process of the net be a process \ntrace of that process. And the process behavior implemented by a net is obtained as the restriction of \nthe net behavior to the exterior process, with 1 and O interchanged. In both cases restriction involves \na renaming of links to ports, selection of the relevant events, and corresponding restriction of the \npartial order; details are in [Pra82]. We adopt an extensional view of processes, identifying them with \ntheir process behavior, just as one identifies a firnction with its graph (set of ordered pairs). Thus \nwe may abbreviate process behavior implemented by a net to process implemented by a net. A network] of \nn proccsscs numbered 1 through n defines an n-ary operation mapping each n-tuple of prcccsscs to the \nprocess irnplcmcntcd by that network having those n processes as constituents. rhc nef-definable operations \narc those operations on processes definable in this way, A nef algebra is any setof processes closed \nunder the net-dcfrnable operations. 10. The Programming Language Viron The goal of Viron is to bc maximally \nuseful with a minimum of machinery. 10.1. At the interface In the word-object dichotomy, the concept \nof language seems to belong as much to the word as to the object it names. In this paper however wc shall \nplay down the syntactic part of Viron, leaving that to other papers, and focus instead on Vhon s domain \nof discourse, lneR is ~ ~~tinction ~ade in p K+s2] between simple nets and fvmcral nets-We have since \ndecided 10 mnsidcr only simple nets. In the interests of brevity and readability, and in keeping with \nthe introductory nature of this paper, the description of Viron will remain at an informal level. A more \nrigorous treatment of the language would entail the use of a formal description language. It is our intent \nto use Vlron to describe itself formally, just as an informal description of ZF set theory may be formalized \nin the language of ZF. (One reason for not using ZF instead of Viron is that they have quite different \ninconsistency-avoidance mechanisms. Viron evadesinconsistencyby being noncommittal, cautiously raising \nits definitions as far as its algorithms permi~ whereas ZF sets itself up with fingers crossed as a fixed \ntarget that either is or is not consistent) The Viron universe is simply a set of processes, ranging \nin complexity from simple atoms through firnctional objects such as application and composition to large \nand/or complex systems. The Viron user interacts with processes: he manipulates them, watches them, talks \nto them, listens to them, and discourses on them (with an occasional break for coffee). No one of these \nactivities is intended to be the dominant one, nor is this list of what one can do with processes intended \nto be complete. Abstract programming languages generafly start out with one or another basic combining \nprimitive. One popular such primitive is application; the domain of discourse of such a language is called \na combinalory aIgebra, and the language itself is characterized as being applicative. All other combining \noperatom, or combinarors, are provided as elements of the combinat.ory afgebra. Church s A-calculus [Chu] \nprovides a familiar example of a combinatory afgebra; the set of proofs of propositional calcuhIs, with \nmodus ponens as the analogue of application, provides another. The informal interface between Viron and \nits user takes the place of application in an applicative language. The precise definition of the processes \nthemselves makes it possible to provide a formal definition of any given mode of user interaction on \ndemand. Manipulation of processes may be formalized in terms of whatever combinators are supplied by \nthe manipulation fanguage -composition when processes can be assembled ihto a net, application when date \ncan be input to processes, etc. Watching a process execute can be described formally in terms of viewing \na trace. Talking to a process is the same as inputting data to a process, while listening to one is the \nconverse -output from a process is sent to the user. Dkcourse on plocesses characterizes a user-Viron \ntaIk-listen loop since all transactions are themselves processes. The fact that afl data and computing \nagents are processes need not be pointed out to the beginner, who will encounter numbers, lists, functions, \nand so on well before the general notion of a process makes its appearance. However since this paper \nis for a more sophisticated audience we can afford to make the basic process representation explicit \nThe least likely candidates for representation as processes are atomic data such as integers and characters. \nSomewhat more plausible are fimctions, which amount to memoryless processes. We have chosen to reprcstnt \nn-ary functions as processes that send one datum to output 1 when one datum has been consumed at each \nof the first n inputs, the output being the desired hrnction of the consumed inputs, 10.2. Basic Data \n Having ensured that functions are processes, to make an atom a process it suffices to make it a function, \nwhich we do by defining the atom b to bc the constant ti.rnction b satisfying b(x) = b for all x. (Type \ncirculan[~ is no problem here since we are not using a conventional type hierarchy of functions and ftrnctionals.) \nAtoms: We take m the atoms of Viron the set Z of integers. (It is tempting to have other atoms such as \ncharacters, but the notion of set of characters is not Suf%ciently universal to justifi its inclusion \nin Viron as a primitive.) n-dimensional Array: a function with domain a contiguous rectangular strbspacc \nof Zn. List: a l-dimensional array starting at 1. ( This agrees with the definition of list cm p.43 of \nMaclanc and Birkhoff Nat], and makes no attempt to relate lists to pointers. Making the pointer implementation \nof lists visible to the user, despite its obvious advantages in term of control, makes the fist concept \nunduly complicated) Filter a restriction of the identity r%nction to a partial function. Sec a one-input \ntwo-output process whose two outputs in effect implement two filters with complementary domains, i.e. \na stcenng mechanism. As such a set is net a firnction only in that it has two asynchronous outputs (as \nopposed to one output synchronously yielding a pair). Predicate: a set Record: a fi.rnctimr with domain \na finite set of symbols. Memory cell: a process that when sent any value on its second input outputs \nthe most recent value seen on its first. (Cells are defined more formally in ~ra82].) It is a nontrivial \nexample of a process that is not a function, One may link n processes into a net with the help of various \nn-ary fimctions for lkat purpose. For example there is a binary function Sequence(a,,b), which yields \na process implemented by a net that connects its input 1 to a s input 1, a s output 1 to b s input 1, \nand b s output 1 to output 1 of Sequcrrce(a,b). (This is made more formal using the definition of process \ncomposition in pra82].) The quaternary function Fork(a,b,qd) yields a process implemented by a net that \nconnects its input 1 to a s input 1, a s output 1 to b s input 1, a s output 2 to c s input 1, b s output \n1 to ds input 1, c s output 1 to ds input 2, and d s output 1 to output 1 of Fork(a,b,c,d). The ternary \nfunction Loop(a,b,c) connects its input 1 to that of a, a s output 1 tcl bs input 1, bs output 1 to c \ns input 1, bs output 2 to output 1.of Loop(a,b,c), and c s output 1 to a s input 2. The process Mwge \npasses atl data received on inputs 1 and 2 straight through tcl output 1, merging them subject to no \nparticular rule. The above process-combining operations form a usefl,d, though surely incomplete, basis \nfor parallel programming. However they can Ibe seen to easily subsume the conventional serial constructs \nas well, if we consider flow of control in a Ma] machine to mean the flow of the entire state of the \nmachine as a single giant datum through a net. fhus begin z b end may be written as Sequence(a,b), if \np then a else b as Fork(p,a,b,Merge), and while p do a as Loop(Merge,p,a), where the predicate p is as \ndefined above (a set, i.e. a pair of filters). There is no explicit notiort of type declaration in Viron, \nHowever one can always insert a filter into a data stream to achicvc the effect of a declaration, which \nit does by blocking any object not of that type. If type error reporting is desired, this may be accomplished \nby using a set instead of a filter and routing the false output to a suitable error handler at run time. \nCompile time type error reporting amounts to testing at compile time whether it is possible for any errors \nto reach the error handler. (As usual with compile time computation, such a test maY need to be conservative, \nsometimes predicting errors when none can happen, but never overlooking a possible error.) Recursion \nis introduced into our model at the semantic level via the notion of least frxcd point. (It is noteworthy \nthat in our semantics the notion of minimality, whether of fixed points or anything efse, is not used \nin the definition of Loop and hence of .\u00ad while. ) Operationally, this becomes the usual substitution \nof the process definition for the recursive use (invocation) of that process. The notion of a passing \na parameter to a function corresponds in Won to the notion of inputting data to a process. In this sense \nonly call by value is provided. Call by reference and call by name are avoided as being too unpredictable: \nit is dlff cult to prove a program correct when nearby programs hold pointers to objects of that program. \ncall by need should be treated as an implementation issue. The effects of these parameter-passing disciplines \nare best handled by passing objects of higher type by value. 11. Impact of the Paradigm Shifts on Viron \nWe now give a more detailed discussion of the impact of the paradigm shifts on the structure of the language. \nMuch of the impact should already be apparent given the discussion of the paradigm shifts and the nature \nof the language. Thus this section is just a short Viron-specific supplement to the main discussion of \nthe shifts. 11.1. From Effect to Entity Viron is in one sense an applicative language. Every communication \npath is brought out into the open, instead of being hidden by references to shared variables. Applicative \nlanguages are normally inherently entity oriented. However in another sense Vlron is effect-oriented, \nin that data entering a process can have an effect on that process. Yet the typical effect is to after \nthe set and/or arrangement of entities existing inside the process. Thus Vhon is at once entity oriented, \nlike an applicative language, and effect oriented, like an imperative fanguage. By making every concept \nan object, and by having processes that can take processes as their input we get the effect of a language \nof higher type. This provides a mathematically attractive way of getting expressive power that in other \nlanguages either cannot be attained or is strained for with a fmily of esoteric parameter-passing mechanisms. \n11.2. From Partition to Predicate Types The role of types in conventional programming fanguages is on \nthe one hand to make clearer to the reader what the program does, and on the other to tell the compiler \nwhat data representation and type of operations to use in the translation. In Viron, filters, which arise \nnaturally in Viron as simple process objects, are used to achieve both of these ends. This takes much \nof the mystery out of types, and at the same time provides a more flexible approach to types in that \nany Viron-definable predicate may be used as a type. 11.3. From Serial to Parallel The net-definable \noperations provide all the needed control structures. This makes Viron an easy language to teach -once \nthe notion of a net is in place, a variety of control structures, whether serial or parallel, can be \nintroduced simply by exhibiting the appropriate net. In Vlron every datum, regardless of its complexity, \nis a process. Thus adoption of parallelism over serialism permits a uniform treatment of data, whether \natomic, structured, or active. 11.4. From Effectiveness to Definability One result of replacing effectiveness \nby definability is that it makes sense to think of Viron as a requirements tanguage as well as a programming \nlanguage. In this respect an implementation of Viron can be viewed as either a compiler/interpreter of \nViron or an inference engine. The boundary between execution and inference is not a sharp one, and we \nfeel is best characterized in terms of how much optimization is performed. Code motion, where an operation \nthat the program shows as executing n times is actually only executed once thanks to the optimizer, is \nclearly an execution-related notion. Induction, where an operation that is shown as executing over all \nnatural numbers is reduced to one step, is clearly part of inferencing. Yet the difference between these \ntwo optimizations is really only quantitative, if we accept infinity as a quantity. In between, we have \nin logic the notion of arguing by cases, which is indistinguishable from a program set up to deal with \neach of those cases. Another consequence of deemphasizing effectiveness is that it changes the status \nof lazy evaluation. Normally lazy evaluation is thought of as part of the operational or interpretive \nsemantics of the language, giving it the extra power needed to compute with infinite objects without \ngoing into an infinite loop trying to generate the object all at once. In Vlron lazy evaluation disappears \nas a language concep~ resurfacing if at all as an implementation concept. The Vhon user is not meant \nto be aware (other than via performance) of whether lazy evaluation or some other method is used to deal \nwith such infinite objects as the set of all primes. It should be possible to interchange such methods \nand have no effect on the semantics of any Vlron program. 11.5. From Syntactic to Semantic Consistency \nIn Viron it is unnecessary to restrict how expressions may be built up and where data maybe sent. One \nconsequence of this is that a single least fixed point operator is possible in Vlron, rather than a fixed \npoint operator at each type as would be required in a more traditionally cautious language. Viron is \nto the untyped A\u00adcalculus as a cautious language would be to the typed A-calculus. 12. Conclusion We \nhave proposed several changes to the way in which we view our programming languages, only some of which \nare presently advocated by others. These changes are not SB obvious ones to make. Nevertheless we believe \nthat they are all changes for the better. We believe our arguments defending them to be sound. Thus the \nchanges certainly should not be rejected without first disposing of our arguments. 13. Bibliography \n[AD] Ackerman, W.B. and J.B. Dennis, A Value-Oriented Algorithmic Language, MIT LCS TR-218, June 13,1979. \n [BA] Brock, J.D. and W.B. Ackerman, Scenarios: A Model of Non-Determinate Computation. In hcttrre Notes \nin Computer Science, 107; Formalization of Programming Concepts, J, DIaz and I. Ramos, Eds., Springer-Verlag, \nNew York, 1981. [Chu] Church, A., The Calculi of Lamb&#38;-conversion, Princeton University Press, 1941. \n[Hew] Hewitt, C. and H.G. Baker, Laws for Communicating Parallel Processes, IFIP 77, 987-992, North-llolland, \nAmsterdam, 1977. [Hoa] IIoare, C. A.R., Communicating Sequential Processes, CACM, 21,8,666-672, August, \n1978, [Kah] Kahn, G., The Semantics of a Simple Language for Parallel Programming, IFIP 74, North-Holland, \nAmsterdam, 1974, [KM] Kahn, G. and 1).B. MacQueen, Coroutincs and Networks of Parallel Processes,IFIP \n77,993-998, North-Holland, Amsterdam, 1977, [Kri] Kripke, S.,Outline of a Theory of Truth, J.of Phil., \n690\u00ad716,1975, [Lip] Star Graphics: An Object-Oriented Implementation, SIGGRAPH-82 Conference Proceedings, \n115-124, ACM, July 1982. Mac] MacLane, S., and G. Birkhof~ Algebra, Macmillan, NY, 1967. Nil] Milner, \nR., A Ca[culus of Communicating Systems, Spnnger-Verlag Lecture Notes in Computer Science,.92, 1980. \nNor] Morns, J.B., Types are Not Sets, 1st Annual ACM Symposium on Principles of Programming Languages, \nBoston, MA, October 1973. pet] Petri, C. A., Introduction to General Net Theory, Springer-Verlag Lecture \nNotes in Computer Science, 1981. pop] Popplestone, R., The Design Philosophy of POP-II, Machine Intelligence \n3, Edinburgh University Press, 1968. ~ra79] Pratt, V. R., A Mathematician s View of Lisp, Byte Magazine, \nAugust 1979, p.162 ~ra80] Pratt, V. R., On Specifying Verifiers, 7th Annual ACM Symposium on Principles \nof Programming Languages, Jan. 1980. pra81] Pratt, V. R., Progmm Logic Without, Binding is Decidable, \n8th Annual ACM Symposium on Principles of Programming Languages, Jan. 1981. pra82] Pratt, V.R., On the \nComposition of Processes, 9th Annual ACM Symposium on Principles of Programming Languages, Albuquerque, \nNM, Jan. 1982. [Sch] Schoenfinkel, M, Ueber die Bausteine der Mathematischen Logik, Math. Ann. 92, 305-316, \n1924. English translation in From Frege to Goedel, Harvard University Press, 1967.  \n\t\t\t", "proc_id": "567067", "abstract": "We describe five paradigm shifts in programming language design, some old and some relatively new, namely Effect to Entity, Serial to Parallel, Partition Types to Predicate Types, Computable to Definable, and Syntactic Consistency to Semantic Consistency. We argue for the adoption of each. We exhibit a programming language, Viron, that capitalizes on these shifts.", "authors": [{"name": "Vaughan Pratt", "author_profile_id": "81100298352", "affiliation": "Stanford University", "person_id": "PP43123527", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/567067.567068", "year": "1983", "article_id": "567068", "conference": "POPL", "title": "Five paradigm shifts in programming language design and their realization in Viron, a dataflow programming environment", "url": "http://dl.acm.org/citation.cfm?id=567068"}