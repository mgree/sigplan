{"article_publication_date": "06-01-1993", "fulltext": "\n Balanced Scheduling: Instruction Scheduling When Memory Latency is Uncertain Daniel R. Kernst and Susan \nJ. Eggers Department of Computer Science and Engineering University of Washington, FR-35 Seattle, WA \n98195 E-Mail: kerns@ pure.com &#38; eggers@cs.Washington.edu Abstract Traditionat list schedulers order \nitxwructions based on an op\u00adtimistic estimate of the load delay imposed by the implemen\u00adtation. Therefore \nthey cannot respond to variations in load latencies (due to cache hits or misses, congestion in the mem\u00adory \ninterconnect, etc.) and cannot easily be applied across different implementations. We have developed \nan altern\u00adative algorithm, known as balanced scheduling, that sched\u00adules instructions based on an estimate \nof the amount of in\u00adstruction level parallelism in the program. Since scheduling decisions are program-rather \nthan machine-based, balanced scheduling is unaffected by implementation changes. Since it is based on \nthe amount of instruction level parallelism that a program can support, it can respond better to variations \nin load latencies. Performance improvements over a tradi\u00adtional list scheduler on a Fortran workload \nand simulating several different machine types (cache-based workstations, large parallel machines with \na multipath interconnect and a combination, atl with non-blocking processors) are quite good, averaging \nbetween 3910and 18%.  Introduction Instruction schedulers for conventional machines generate code assuming \na machine model in which load latencies are well-defined and fixed. Usually the latencies reflect the \nmost optimistic execution situation, e.g., the time of a cache hit rather than a cache miss. Compiler \noptimizations intended to This researchwassupportedby NSFGrantNo. CCR-9114167,NSF PYI AwardNo. MIP-9058-439,ONRGrantNo. \nNOO014-92-J-1395and theWashingtonTechnologyCenter. tAuthor s currentaddress:PureSoftware,1309S.Mary Ave,Sunnyvale, \nCA 94087 Permission to copy without fee all or part of this material is granted provided that the copies \nare not made or distributed for direct commercial advantage, the ACM copyright notice and the title of \nthe publication and its date appear, and notice is given that copying is by permission of the Association \nfor Computing Machinery. To copy otherwise, or to republish, requires a fee and/or specific permission. \nACM-S lGPLAN-PLDl-6/93 /AlbuquerqueJ N.M. @ 1993 ACM 0-89791 -598 -4/93 /0006 /0278 . ..$1 .50 improve \nperformance through instruction scheduling, such as reordering instructions to avoid pipeline stalls, \ninsert inde\u00adpendent instructions after loads to keep the CPU busy while memory references are in progress. \nThe number of instruc\u00adtions inserted (in the best case) depends on this predefine latency value. When \na load reference must exceed the implementation\u00addefined latency, the processor architecture generally \nstipu\u00adlates that instruction execution be stalled. The advantage of this design (called blocking loads) \nis that it requires a simple and straightforward hardware implementation. The conse\u00adquence for compiler \ntechnology is that the compiler does not have to consider multiple memory latencies during instruc\u00adtion \nscheduling. Two architectural innovations make it worthwhile to re\u00adconsider how to schedule behind load \ninstructions, The first is processor designs that do not stall on unsatisfied load ref\u00aderences (called \nnonblocking loads) through the use of lockup free caches[17, 18,15, 13], multiple hardware contexts[2, \n1] or an instruction lookahead scheme[2]. Nonblocking loads allow a processor to continue executing other \ninstructions while a load is in progress. Although the design requires more complex hardware, more instruction \nlevel parallelism can be exploited, and therefore programs execute faster. The second innovation is machines \nthat have a large variance in memory response time. They may be due to congestion in a multipath interconnector \na hierarchy of memory, including both cache hierarchies and local and global memories. Variable load \ninstruction latencies, coupled with non\u00adblocking loads, complicate scheduling, because the instruc\u00adtion \nscheduler does not know how many instructions to schedule after a load to maintain high processor utilization. \nIf the memory reference is delayed beyond the scheduler s latency estimate, the processor will stall \nand processor uti\u00adlization will drop. However, if the load latency is shorter than the estimate, the \ndestination register of a load instruc\u00adtion will be tied up longer than necessary. This may increase \nregister pressure enough to cause unnecessary spills to mem\u00adory and a consequent increase in program \nexecution time. In addition, an excessive number of instructions may migrate to the top of the schedule, \nleaving an insufficient number to hide load latencies near the bottom, In this case the CPU will also \nbe needlessly idled. In this paper we present a code scheduling algorithm, called balanced scheduling, \nthat has been specifically de\u00adsigned to tolerate a wide range of variance in load Iateney over the entire \nexecution of a program. Balanced schedul\u00ading works within the context of a traditional list scheduler[9, \n14,21,8, 6], but uses a new method for calculating load in\u00adstruction weights. Rather than using weights \nthat are deter\u00admined by the implementation and therefore are fixed for all programs, the weight of each \nload is based on the amount of instruction level parallelism that is available to it. (We refer to this \nas load level parallelism.) This assignment is effec\u00adtive, since load instructions are scheduled for \nthe maximum latency that can be sustained by the amount of load level par\u00adallelism in the code, In essence, \nour algorithm schedules for the code instead of scheduling for the machine. Locik\u00ading at it another way, \nbalanced scheduling amortizes the cost of incorrectly estimating actual load latencies over all loi~d \ninstructions in the program. To validate the atgorithm we compared the performance of several programs \nscheduled via batanced scheduling artdl a traditional list scheduler on a variety of processor and menn\u00ad \nory architectures. The processor models differed in their ability to exploit load level parallelism; \neach was coupled with three different memory systems, that exhibit dissimilar latency behavior. Both \nthe balanced scheduler and the tra\u00ad ditional scheduler were incorporated into the GCC[19] comp\u00ad iler \nand generated code for the Perfect Club benchmarks[41]. Performance improvements for balanced scheduling \naver\u00ad aged 3% to 18% over the traditional list scheduler, for dif\u00ad ferent processor and system model \ncombinations. The remainder of this paper is organized as follows. Sec\u00ad tion 2 introduces balanced scheduling, \nand section 3 de\u00ad scribes the algorithm in more detail. Section 4 explains our experimental methodology; \nsection 5 presents the experi\u00ad mental results. Section 6 discusses extensions and other ap\u00ad plications \nof the balanced scheduling rdgorithm. A summary follows in section 7. Balanced Scheduling The traditional \napproach to instruction scheduling that con\u00adsiders machine resource constraints is list schedtding[9, \n14, 21,8, 6]. The primary data structure used by list schedulers is the code DAG, in which nodes represent \ninstructions and edges represent dependence between them. Each node is la\u00adbeled with a weight reflecting \nthe latency of the instructioml At each iteration of its algorithm a list scheduler creates a ready list \nof instructions that are eligible for scheduling, i.e., . 1Edges can also be labeled, allowing latencies \nto dffer among successor nodes of a given node, as on dte Intel i860. those whose predecessors in the \ncode DAG have been sched\u00aduled or have had their latencies met. A set of heuristics is then applied to \ndecide which instruction from the ready list should be scheduled next the heuristics used depend on the \nparticular list scheduler. For example, Gibbons and Much\u00adnick[8] first schedule the instruction with \nthe greatest oper\u00adation latency. If more than one instruction qualities, their scheduler breaks the tie \nby choosing the instruction(s) with the greatest number of successors. The final heuristic picks the \ninstruction with the largest sum of the latencies along the longest path from the instruction node to \na leaf node. Other styles of list schedulers include those that combine several levels of heuristics \ninto a single weight and schedule in de\u00adcreasing weight order[ 16,22] and update scheduling weights dynamically \n[21]. Our heuristics are described in detail in Section 4.1. If a processor exposes the variations in \nactual memory ref\u00aderence latency to the compiler through non-blocking load instructions, instruction \nscheduling becomes more compli\u00adcated. Traditionrd list schedulers use a single constant for the weight \nof all load instructions, usuatly art implementation\u00addefined latency (e.g., cache hit time). They then \nschedule in\u00adstructions independent of that load until the load latency has been consumed. As expected, \ntraditional schedulers work best when the actual latency of each load matches the pre\u00addefine (and optimistic) \nvalue. When it does not, a longer latency (e.g., the time of a cache miss) penalizes the pro\u00adgram by \nstalling the CPU. This fixed estimate of memory latency prevents the scheduler from hiding latencies \nlarger than the nominal value. Therefore, when the optimistic ex\u00adecution scenario does not occur, performance \nsuffers. The worst scheduling situation exists when the actual latencies change over time, for example, \nas congestion in the inter\u00adconnect varies. In contrast, the balanced scheduler computes load instruc\u00ad \n tion weights based on a measure of instruction level paral\u00ad lelism in the code rather than on an implementation-defined \nvalue. This measure, which we call load level parallelism, defines the number of instructions that may \nexecute in par\u00ad allel with each load instruction. The weight for each load is calculated separately, \nas a function of the number of in\u00ad structions that may initiate execution during the load and the number \nof other loads that could also use them to hide laten\u00ad ties. Both the balanced scheduling algorithm and \nthe traditional scheduler operate on a basic block by basic block basis. The balanced scheduler simply \nincorporates the new method of computing weights for each load instruction into a traditional list scheduler. \nUsing the code DAG of Figure 1 as an example, Figure 2 illustrates the schedules generated by the traditional \nand the balanced schedulers. Nodes labeled LrJ represent load in\u00ad structions and nodes labeled Xn represent \nother non-load instructions of weight 1. The schedules in Figures 2a and 2b result from scheduling the \ngraph of Figure 1 with a tra\u00ad Begin x4 Figure 1: Code DAG of a hypothetical program. Traditional Traditionat \nBalanced Scheduling Scheduling Scheduling W=5 W=l 11 LO LOLO Xo L1Xo xl Xoxl x2xl L1 x3 x2x2 L1x3 x3 \nx4 x4x4 (a) (b) (c) Figure 2: Schedules generated from the code DAG in Figure 1, using the traditional \nand balanced schedulers. The traditional scheduler is illustrated with load instruction weights of 5 \nand 1, respectively. Load Latencv vs. Interlocks m Figure 2 (a) o Figure 2 (b) m Figure 2 (c)   Jw.Ldll \n 1 8 Figure 3: Interlocks generated from Figure 2 for various load Iatencies. ditional scheduler, assuming \nload instruction weights of 5 and 1, respectively. These two schedules illustrate the ef\u00adfect of over-and \nunder-estimating load instruction latency. In Figure 2a, if L1 incurs an actual latency greater than \none, hardware interlocks will be inserted before X4. We say the scheduler is greedy in this case, because \nLO captured all of the load level parallelism and left none for L1. The oppo\u00adsite situation occurs when \nload instruction weights are too small. Figure 2b illustrates the schedule produced when a weight of \none is used. In this case we have not taken advan\u00adtage of the load level parallelism with respect to \nLO, We say the scheduler was lazy, because it passed over opportunities for parallelism. Should the actual \nlatency be greater than the scheduling assumption, the processor will needlessly stall. Figure 2C is \nthe schedule that the balanced scheduler gen\u00aderates. The balanced scheduler has measured the load level \nparallelism in the DAG and determined that a weight of 3 assigned to each load instruction would generate \nan efficient schedule. Figure 3 summarizes the number of interlocks that accrue when these schedules \nare executed with varying memory la\u00adtencies. The chart shows that, for latencies in the range of 2-4, \nthe balanced schedules are faster than both the greedy and lazy traditional schedules illustrated in \nFigure 2. Out\u00adside this range the balanced and traditional schedules per\u00adform equivalently. In summary, \nbalanced scheduling s strength is its ability to look beyond fixed latencies, thereby exposing additional \ninstruction level parallelism. Whereas traditional sched\u00adulers plan for the optimal latency, balanced \nschedulers make scheduling decisions based on the amount of load level par\u00adallelism the code can support. \nIt therefore produces fewer interlocks when the optimal case doesn t occur. 3 The Balanced Scheduling \nAlgorithm This section presents the balanced scheduling algorithm, The algorithm is first illustrated \nthrough two simple exam\u00adples. The examples depict the two relationships load instruc\u00adtions can have with \neach other, i.e., occurring in series and in parallel, and, therefore, the two cases the algorithm must \nhandle. When presented with load instructions in series, the bal\u00ad anced scheduling algorithm equally \ndistributes among them all instructions with which they can execute in parallel. Re\u00ad ferring again to \nthe code DAG of Figure 1, the two load in\u00ad structions, LO and L1, may execute independently of XO, Xl, \nX2 and X3. Since L1 is dependent on LO, the obvious par\u00ad titioning would schedule two instructions after \nLO and two after L1. The weight on each load instruction is simply one (for the issue slot of the load), \nplus the number of instruction issue slots that may be initiated independently of the load di\u00ad vided \nby the number of loads in series or, 1 + (4/2) = 3. Issue slots are measured, because instruction weights \nrepre\u00adBegin x4 Figure 4: Code DAG in which LO and L1 are independent and execute in parallel with all \nother instructions.  Ezll LO L1 Xo xl x2 x3 x4 El Figure 5: Balanced schedule generated from the code \nDAG in Figure 4. sent the number of machine cycles that should pass before an instruction that uses \nthe result of the load is initiated. When load instructions are not dependent on each other, i.e., they \noccur in parallel, their latencies can be hidden, us\u00ading instructions drawn from the same set. Referring \nto Ihe code DAG in Figure 4, the balanced scheduling algorithm takes advantage of the fact that LO and \nL1 can, and should, share the same set of padding instructions. In Figure 4 each load instruction may \nexecute in parallel with five other in\u00adstructions, so they are each assigned a weight of six ( 1+5/ 1). \nThe final schedule is shown in Figure 5. For a balanced scheduling algorithm to be successful, any combination \nof loads in series and loads in paratlel must be accommodated. A balanced scheduler operates by measuring \nload level parallelism and assigning weights accordingly. The algo\u00adrithm, shown in Figure 6, examines \neach instruction i in the code DAG (G) and computes the set of instructions with which it may execute \nin parallel. It first eliminates frc~m G those instructions that are predecessors or successors, re\u00adcursively, \nproducing G~.d (line 3). The resulting connected components of G~nd contain the SetSof load instructions \nth2tt may execute in parallel with i. Within each connected com\u00adponent, C, the path with the largest \nnumber of load instruc\u00adtions is located (lines 4-5), (We examine the longest load path, because loads \non other paths can be overlapped with it.) Since the loads on this path execute in series, their sum \n(called Chances) represents the number of opportunities for scheduling i. Finally, the number of issue \nslots in the instruc\u00adtion execution pipeline that are required by i is divided by the number of loads \nin series (IssueSlots(i)/C hances), and is added to the accumulating weight of each load instruction \nin C (lines 6-7). Figure 7a illustrates the balanced scheduling algorithm on a more challenging basic \nblock. Using i=Xl, step 4 gen\u00aderates the three connected components shown in Figure 7b. (L2 does not \nappear in a connected component because it is a predecessor of Xl). The maximum path length in thecompo\u00adnent \ncontaining L1 is 1; therefore Xl contributes 1/1 to L1 s weight. The maximum path length in the second \ncompo\u00adnent is 3, and X 1 contributes 1/3 to the weights of each load instruction, L3, L4, L5 and L6. \nThe third connected com\u00adponent has no load instructions. Table 1 shows the weight contributed by each \ninstruction to each load at the comple\u00adtion of the algorithm. The latencies assigned to the five load \ninstructions represent a distribution of load level parallelism that is representative of the load level \nparallelism in Figttre7. If n is the number of nodes in the DAG, steps 4 and 5 together may be done in \na worst case time of O(n a n)2, using the set union algorithm. First, each node in Gind is labeled with \nits level from the farthest leaf. Next, it is com\u00adbined with the nodes to which it is connected, using \nthe set union function. Each time we perform set union, the set label is updated to reflect both the \nminimum and maximum level number that has been seen in that set. Therefore, the largest path length for \neach connected component is simply the max\u00adimum level number minus the minimum level number plus 1, Steps \n6-7 are performed in O(n) time and, therefore, do not impact the worst case time complexity. Connected \ncompo\u00adnent anatysis is done for each instruction in the code DAG; therefore, the entire atgorithm has \na worst case time com\u00adplexity of 0(n2 a n). Since the worst case time complexity of list scheduling is \n0(n2 ), the batartced scheduling atgo\u00adrithm is nearly as efficient. An attemate technique for assigning \nweights might com\u00adpute a weight based on the average load level parallelism over atl load instructions \nin a basic block. However, since load level parallelism typically varies within a basic block, this method \ndoes not consider those imbalances, often ignor\u00ading load level parallelism that is greater than the average \nfor some loads, while unrealistically allocating nonexistent par\u00adallelism to others. Our early experiments \nindicated that this attemative produced schedules that executed no faster than schedules from the traditionat \nscheduler. 2 a k the inverse Ackerman function. As a function of n, it increases very slowly and may \nbe considered constant[20]. Term Definition G the code DAG. Pred(z) the transitive closure of the predecessor \nfunction on node i. SUCC(2) the transitive closure of the successorfunction on node i. Chances the maximum \nnumber of loads on any path in a connected component. IssueSlots(i) the number of issue slots in the \ninstruction execution pipeline required by instruction i. 1. Initialize the latency of each load instruction \nto 1. 2. for each instruction i in G  3. G,~d = G (Pred(i) U Succ(z)) 4. for each connected component \nC in ~,nd 5. Find the path with the maximum number of load instructions. 6. for each load instruction \n1 ~ C 7. add IssueSlots(i)/Chances to the weight of /  Figure 6: Balanced scheduling algorithm (a) \n0) Begin L2 r v~ L1 L3 xl x2oL1 nL3 x2 v 1 n \\i4 L5 X2X4 L4 L5 x4 (3 Figure 7: Balanced scheduling \nexample, with the code DAG placements for Xl (b).  Experimental Methodology We designed a series of \nexperiments to compare balanced scheduling with a traditional scheduling approach. These experiments \nmodeled the execution of real programs running on several different architectures. This section describes \nthe methodology of these experiments. The integration of the balanced scheduler into the GCC compiler, \nthe workload and the simulator we used for our measurements are described, in turn, in sections 4.1 through \n4.3. For our experiments we classify the target machine char\u00adacteristics into two groups. The processor \ncharacteristics are those that control how the processor exploits parallelism with respect to load instructions. \nThe system characteristics are the attributes of the memory system in a particular im\u00adplementation. We \nused several alternatives for each model, to demonstrate that balanced scheduling works well on ar\u00adchitectures \nthat contribute to latency uncertainty in different (a) and the connected components which determine \nthe possible ways. The processor and system models we used are de\u00adscribed in sections 4.4 and 4.5. \n4.1 Compiler We modified the GNU GCC version 2.2,2 compiler[19] to perform batanced instruction scheduling. \nThe default in\u00adstruction scheduler within GCC was replaced by a new mod\u00adule that can schedule using either \nthe traditional or balanced approaches. In addition, several changes were made to GCC to increase scheduling \neffectiveness and improve instruction level parallelism. The chartges include alleviating the effect \nof dependence in spill code introduced by register alloca\u00adtion, our heuristics for picking instructions \nfrom the ready list (one of which helps control register pressure) and modi\u00adfications to GCC S RTL intermediate \nlanguage. Both sched\u00adulers take advantage of these modifications. Contribution by Total Load L1 L2 L3 \nL4 L5 L6 Xl X2 X3 X4 Weight L1OII 1111 111 10 L2 1/40000000 00 1 1/4 L3 1/4 o 0 0 0 0 1/31/31/31/3 25/12 \nL4 1/4 o 0 0 1 1 1/31/31/31/345/12 L5 1/4 o 0 1/2 o 0 1/3 1/3 1/3 1/3 211/12 L6 1/4 o 0 1/2 o 0 1P 1/3lp \n1/3211/12 E Table 1: How instruction weights are calculated for Figure 7. The total weight is one plus \nthe sum of the weight contribution of each instruction to each load. GCC performs instruction scheduling \nboth before andl af\u00adter register allocation. Since register allocation may add spill code and/or copy \ninstructions, the second scheduling pass serves to integrate these additional instructions into the finat \nschedule. However, the effectiveness of the second schedul\u00ading pass is restricted because of dependence \nintroduced by register allocation. These false dependence negatively effect schedule lper\u00adformance in \ntwo ways. First, the final assignment of lreg\u00adister numbers severely limits the code motion that a sched\u00aduler \ncan perform. Second, when adding spill instructions,, the GCC compiler always uses register numbers selected \nfrom a small pool of spill registers. The net effect is that spill code cannot be scheduled effectively \nwith other instructions. We improve performance by increasing the size of GCC S spill register pool by \ntwo and implementing a FIFO queue-like or\u00addering of the registers in the pool. An alternative appra~ach \nwould use software register renaming after register alloca\u00adtion to better integrate spill instructions. \nAs previously mentioned, both the balanced and tradit\u00adional schedulers use the same list scheduler. Some \nlist schedulers place instructions onto the ready list when all their predecessors in the code DAG have \nbeen scheduled. In contrast, our scheduler defers adding these instructions to the ready list until each \npredecessor has exhausted its expected latency. In the case of starvation the scheduler inserts virtual \nno-op s into the instruction stream. This delayed insertion of instructions into the ready list increases \nthe accuracy of instruction placement within the schedule, Since our pro\u00adcessors use the hardware interlock \nmodel of execution, the virtuat no-ops are removed before actual code generation. List schedulers select \ninstructions from the ready list in priority order. In our case, the priority of an instruction is equal \nto its weight plus the maximum priority among its suc\u00adcessors. In the event of ties we select instructions \nusing, al\u00adternate heuristics in the following order. The first selects the instruction that has the largest \ndifference between con\u00ad sumed and defined registers; this heuristic helps control reg\u00adister pressure. \nThe second ranks instructions based on the number of successors in the code DAG that would be ex\u00adposed \nfor scheduling if that instruction were to be selected; it gives the list scheduler more instructions \nfrom which to se\u00adlect. The final heuristic selects the instruction that was gener\u00adated the earliest. \nOur list scheduler is a bottom-up scheduler, therefore we generate schedules in reverse order by schedul\u00ading \nfrom the leaves of the code DAG toward the roots. The compiler has been configured for the MIPS RISC \npro\u00adcessor[12]. GCC S intermediate language, RTL, is not suf\u00adficiently RISC-like for an instruction scheduler \nto get max\u00adimum benefit, since some primitive operations in RTL are actually multi-cycle macros. In the \ncontext of this work, memory-to-memory copies are the most notable, since it is load instructions that \nwe are concentrating on scheduling. Our implementation extracts GCC S intermediate language after optimization \nbut before register allocation and modifies it to replace certain non-RISC patterns, such as memory-to\u00admemory \ncopy, with their RISC equivalents. The modified RTL is at a lower level and therefore more suitable for \nin\u00adstruction scheduling. Loop unrolling is an optimization that increases instruc\u00adtion level parallelism. \nDue to a conflict with the way we use profiling information (section 4.3), GCC S unrolling capabil\u00adity \nis not usable for these experiments. Therefore, unrolling was performed manually. 4.2 Workload The workload \nconsisted of the Perfect Club suite of bench\u00admarks[4]. Since these programs are written in FORTRAN, they \nwere converted to C using j2c[7]. The Fortran-to-C converter produces C programs that correctly represent \nthe semantics of the original FORTRAN programs. However, these C programs are conservative translations: \nafter being compiled by a C compiler, they will most likely execute more slowly than if they were compiled \nby a FORTRAN compiler. For example, since almost all data is referenced through pointers in the C program, \nit is nearly impossible for a C compiler to do the memory reference disambiguation that might be obvious \nto a FORTRAN compiler. Instmc\u00adtion scheduling is affected, because load instructions are not free to \nmove above stores. Since this problem severely re\u00adstricts a scheduler s ability to exploit load level \nparallelism, we apply a transformation which more correctly models the dependence in the FORTRAN program \nand increases the available parallelism. float a[HUGE], b[HUGE]; float func(a, b) float *a, *b; float \nnewfunc(ar b) { + float *a, *b; a[l] = b[21; { a[2] = b[31; a[l] = b[21; } a[2] = b[31; } Figure 8: \nAn exampleflc program showing the disambiguation problem and our transformation. In f unc the loadofb[31 \nmust be considered dependent on the store of a [1]. Our transformation results in newf uric. The resulting \nprogram produces incorrect results, but accurately models the code that would be generated by a FORTRAN \ncompiler. The FORTRAN standard[3] specifically disallows alias\u00ading among dummy arguments (formal parameters) \nif there will be any stores to the dummy arguments. If the function f unc in Figure 8 were produced byflc, \nthe FORTRAN stan\u00addard would assume that array a and array b were disjoint therefore the load for b [ \n31 could be scheduled before the store of a [1]. However, the C semantics for func insert a true data \ndependence between the store of a [11 and the load ofb[31. This dependence is an artifact of the Fortran-to-C \ntranslation and does not exist in the original program. Our compiler takes advantage of the FORTRAN seman\u00adtics \nby performing a parallelism-exposing transformation on the input C programs. The transformation would \nreplace f unc with newf uric, as illustrated in Figure 8, New global variables are inserted with the \nsame names as the original subroutine parameters. The formal parameters are replaced with names that \nare never referenced. The program is no longer semantically correct, but the compiler is now able to \ncorrectly model the FORTRAN independence between ref\u00adererices to array a and array b. The net effect \nis the genera\u00adtion of code that is comparable to that generated by a FOR-TRAN compiler. This transformation \nis a conservative rep\u00adresentation of the data dependence that a FORTRAN com\u00adpiler could discover, since \nFORTRAN is quite specific about when aliasing may occur.  4.3 Simulator After the second scheduling \npass, the machine instructions are extracted and run through an instruction level simulator. Given a \nparticular model for load instruction latencies (ex\u00ad plained in section 4.5), the simulator simulates \ninstruction issue and completion for each basic block and computes its execution time in cycles. As the \nsimulator encounters load instructions, it draws la\u00adtency samples from a random distribution that represents \nthe system-level characteristics being modeled (see seetion 4.5). The output of the simulator is one \nsample of the number of instruction and interlock cycles that comprise the execution time of the program \non the modeled system. Because the results of the simulation are based on an independent and identically \ndistributed random variable, we can take several steps to both reduee the execution time of the simulation \nand improve the quality of the results. Our method executes the full instruction-by-instruction simulation \n30 times with new random numbers on eaeh it\u00aderation. The number 30 represents an arbitrary choice which \nis large enough to avoid statistical noise. Second, we measure the accuracy of our results by gen\u00aderating \nconfidence intervals. Confidence intervals are com\u00adputed for percentage improvement using a bootstrapping[5] \nprocedure. From the 30 sample runtimes, we randomly draw 30 samples, with replacement, in order to generate \na second sample mean. This process is repeated until we have 100 sample means for the block. These 100 \nsample mean rtm\u00adtimes are scaled by the profiled execution frequency to com\u00adpute the actual runtime of \nthe block, The sample means for each block are summed giving 100 sample rtmtimes for the entire program. \nThe mean runtime reported is the mean of the 100 sample mean runtimes. In order to report a percentage \nimprovement for balanced scheduling, the 100 sample means from the balanced sched\u00aduler are paired with \nan equal number from the traditional scheduler, and the calculation is performed. After sorting, a 95% \nconfidence interval is directly extracted.  4.4 Processor-level model Processor-level attributes model \na processor s ability to ex\u00adploit load level parallelism. We model three different config\u00adurations. The \nfirst is unrealistically aggressive and serves as a best case reference. The second two are restricted \nin ways that make them implementable, All of our processor models are assumed to maintain store/load \nconsistency, i.e., if a load instruction follows a store, and they reference the same ad\u00ad dress, the \nload instruction receives the data that was written by the store instruction. The first processor model \n(called UNLIMITED) cart dis\u00adpatch non-blocking load instructions with no limit on the number of loads \noutstanding. This model is similar to the\u00adoretical datatlow machines[ 10]. It is of interest because \nit exposes the maximum benefit that processor parallelism etm achieve. The second (called MAX-8) allows \na maximum of eight load instructions to be simultaneously executing. If a ninth load instruction is issued, \nthe processor blocks until one of the eight outstanding loads completes. The third pro\u00adcessor model (called \nLEN-8) restricts the maximum number of cycles a load instruction can take before blocking, as in the \nTera Computer[2].3 In this model, if a load instruction has been outstanding for eight cycles, the processor \nblocks until the data is returned. The balanced scheduler has not been specifically con\u00adfigured for \nany of the processor models. In particular, it may schedule more than eight load instructions before \nusing loaded data (as is prohibited in MAX 8), and it might assign load instructions weights greater \nthan eight (not effective in LEN-8). If this information were available to the compiler, the results \nfor MAX-8 and LEN-8 would improve. We used a processor-independent version of balanced scheduling to \ndemonstrate that a code scheduling approach that was not associated with a particular implementation, \nbut instead was based solely on program characteristics, such as the amount of load level parallelism, \nwould generate efficient code. 4.5 System-1evel model Three memory systems are modeled and simulated, \nrepre\u00adsenting different latency behavior in both current and future architectures. The first has a data \ncache. A load instruc\u00adtion s data is returned after 2 cycles on a cache hit and ei\u00adther 5 or 10 cycles \non a cache miss. The model represents a typical workstation-class RISC processor that implements nonblocking \nload instructions, such as the Motorola 88000 series [15]. It is simulated with cache hit rates of 80% \nand 95%, modeling first level caches of 4K and 32K bytes, re\u00adspectively 11]. Four configurations are \nmodeled, and ~are referred to as Lhr(hl,ml), where Lhr stands for lockup-free caches with a hit rate \nof hr, and hl and ml are hit and miss latencies, respectively. The second model has a memory interconnection \nnetwork and no cache. The interconnection scheme uses a hashimg function to assign addresses to memory \nmodules, effectively randomizing memory access locations. In this architecture, memory latencies modeled \nby one of two zero-based prob\u00adability mass functions, depicting normal distributions with standard deviations \nof 2 or 5. A standard deviation of 2 rep\u00adresents a machine in a relatively stable state (uniform net\u00adwork \nload, low to medium uncertainty). A standard devia\u00adtion of 5 represents one with unpredictable memory \nlatencies (changing network load, high uncertainty). The network ma\u00adchine is modeled in seven different \nconfigurations. Each dk.\u00adtribution is combined with a mean of 2, 3 or 5, representing different base \nload levels. In a multithreaded processor suich as the Tera, the different means are related to the number \nof active threads; the more threads, the lower the mean mem\u00adory access time. We refer to these models \nas N(p ,a) where 3 me Tera restricts the number of instmctions rather than Cycles:Since we assume that \ninstmctions orher than loads execute in a single cycle, the two are equivalent. p is the mean of the \ndistribution and o is the standard de\u00adviation, All six configurations are reasonable design points for \nthe machine. A seventh configuration models an unbal\u00adanced system, with a mean access time of 30 cycles \nand a standard deviation of 5 (N(30,5)), Although we recognize that a compiler would not likely generate \ncode specifically for such an unbalanced configuration, we include it in order to gauge balanced scheduling \ns ability to handle a workload that has t~ little load level parallelism to hide the average latency. \nThe third machine has both a data cache and a Tera-style memory interconnection network. A cache hit \noccurs 80% of the time and takes two cycles. A cache miss is represented by a normal distribution with \na mean of 30 and a standard de\u00adviation of 5. This configuration is referred to as L80-N(30,5) and has \na mean latency of 7.6. In this case the 30 cycle la\u00adtency is a reasonable design point, since the cache \nsatisfies most requests. The model is intended to be representative of Alewife-like systems[l], where \na commodity processor might be incorporated into a shared memory machine,  5 Experimental Results The \nfirst set of results is the percentage improvement in execution time of the balanced scheduler over the \ntradi\u00adtional scheduler. (The results for the UNLIMITED proces\u00adsor model appear in Table 2. Positive values \nindicate an im\u00adprovement due to balanced scheduling.) For these experi\u00adments, the traditional scheduler \nuses load Iatencies equat to the cache hit time or effective access time for models with caches and the \nmean of the normal distribution for mod\u00adels without caches (labeled Optimistic Latency in the table). \nThe percentage improvement of balanced scheduling over traditional scheduling is quite good. The average \ndecrease in execution time for the UNLIMITED model varies from 3 to 18 percent for individual system \nmodels, with a mean im\u00adprovement of 9.9%, The results for MAX-8 and LEN 8 are similar, with ranges of \n7qo to 1670 and 3% to 16%, and means of 10.O7Oand 8.7%, respectively. These results demonstrate that \nbalanced schululing works well for several architec\u00adtures, each of which contributes to latency uncertainty \nin a different way. It is important to emphasize that the bal\u00adanced scheduler has not been customized \nfor the restricted processors; these results represent the improvement from a machine-independent scheduler \nand would be better if the processor dependence were taken into account. The balanced scheduler does \nrelatively better (over the tra\u00additional scheduler) as the uncertainty of the load instruction latencies \nincreases. This can be seen in three different situ\u00adations: when the cache hit rate is low (L80 vs. L95): \nwhen the cache miss penatty is high (L8O(2,1O) vs. L80(2,5) and L95(2,1O) vs. L95(2,5)); and when the \nstandard deviation of the normat is high (N(2,5) vs. N(2,2), etc.). To better understand the reasons \nfor the performance im\u00ad 28.5 Processor modeh UNLIMITED Unlimited loads System Optimistic Percentage \nimprovement from balanced schedulig Latency ADM I ARC2D FL052Q MDGI BDN A I -I WD2 I T RACK llM~ Data \ncachq bus-based in terconnection L80(2,5) 2 5.8 6.7 6.0 4.9 9.8 7.0 19.3 7.2 8.3 2.6 4.0 6.2 5.2 3.6 \n8.7 6.2 18.6 2.6 6.9 L8O(2,1O) 2 9.9 13.1 10.6 8.7 14.4 11.9 27.8 6.7 12.9 3.6 7.5 11.7 8.1 6.7 11.6 \n10.7 25.8 2.2 10.5 L95(2,5) 2 3.4 3.9 4.4 2.8 6.9 3.7 16.9 6.1 6.0  2.2 2.1 4.0 4.0 2.1 6.2 3.9 16.2 \n2.0 5.1 L95(2,1O) 2 4.6 5.8 5.8 3.9 8.0 5.4 19.9 4.9 7.3 2.4 3.2 6.1 5.6 3.8 7.2 5.8 19.0 1.7 6.6 No \ncach~networkinterconnection N(2,2) 2 8.0 9.3 8.0 6.5 11.3 9.2 21.3 9.4 10.4 N(3,2) 3 6.4 8.9 4.0 5.0 \n12.0 8.6 22.7 3.5 8.9 N(5,2) 5 4.8 5.5 3.4 3.6 13.5 6.5 20.0 3,9 7.7 N(2,5) 2 14.2 17.7 14.4 11.9 20.9 \n16.1 32.7 16.6 18.1 N(3,5) 3 11.5 18.2 10.8 10.9 20.0 14.9 35.9 3.9 15.8 N(5,5) 5 9.2 12.0 9.3 7.6 18.3 \n10.3 27.8 4.9 12.4 N(30,5) 30 -3.5 -5.0 1.9 4.1 19.3 -0.9 7.1 0.6 3.0 Mixed L80-N(30,5) 2 12.4 20.4 15.8 \n12.7 20.0 13.3 39.6 11.3 18.2 7.6 7.0 9.3 18.4 6.3 14.3 4.5 19.4 -2.5 9.6 Table 2: Percent improvement \nin execution time from simulations using processor model UNLIMITED Program: MDG (Bins = 5,144 million) \nSystem Optimistic Tins UNLIMITED MAX 8 LEN 8 Latency ]rnp% T170 ] BI % II Imp% I H%] BI% 1]Imp%~o I \nBI% Data cache bus-based intercon nection L80(2,5) 2 5,358 9.8 10.4 5.6 7.8 13.9 10.9 9.6 10.4 5.7 2.6 \n5,351 8.7 9.6 7.4 13.7 8.2 9.3 L8O(2,1O) 2 5,358 14.4 21.6 13.6 10.8 25.2 20.6 14.6 22.5 14.7 3.6 5,299 \n11.6 20.2 8.9 24.7 11.6 21.2 L95(2,5) 2 5,358 6.9 5.9 3.4 6.0 8.8 7.1 6.8 5.8 3.4  2.15 5,351 6.2 5.5 \n6.0 8.9 6.2 5.3 L95(2,1O) 2 5,358 8.0 9.4 6.1 6.4 12.1 10.2 8.0 9.9 6.6 2.4 5,351 7.2 8.9 6.8 12.5 7.2 \n9.4 No cache; n etwork inter tonne ction N(2,2) 2 5,358 11.3 12.8 6.9 10.2 16.6 11.8 11.2 13.1 7.2 N(3,2) \n3 5,351 12.0 16.1 9.7 10.0 21.1 16.5 12.0 16.0 9.5 N(5,2) 5 5,297 13.5 24.4 16.8 10.8 32.1 27.0 12.9 \n24.3 17.0 N(2,5) 2 5,358 20.9 30.0 18.7 15.9 35.6 28.3 19.2 30.4 20.4 N(3,5) 3 5,351 20.0 31.8 21.3 14.5 \n37.2 30.9 17.4 31.9 23.0 N(5,5) 5 5,297 18.3 35.5 25.9 13.5 42.3 36.4 16.4 36.3 28.0 N(30,5) 30 5,393 \n19.3 71.8 67.9 14.5 79.4 77.5 18.7 73.1 69.6 Mixed L80-N(30,5) 2 5,358 20.0 49.9 42.3 13.6 52.3 47.9 \n13.9 49.5 44.7 7.6 5,405 14.3 46.9 11.5 50.9 10.4 47.4 Bins = The number of instructions executed, in \nmillions, for the balanced scheduler. Tins = The number of instmctions executed, m millions, for the \ntraditional scheduler. Imp% = The percentage improvement of the balanced scheduler over the traditional \nscheduler. TI% = The percentage of cycles which were interlock cycles for the traditional scheduler. \nBI% = The percentage of cycles which were interlock cycles for the balanced scheduler. Table 3: Detailed \nanalysis of performance in MDG provements, we did a component anatysis of the execution times. All of \nour instructions execute in a single cycle; theref\u00adore the runtime of a program is the sum of the number \nof instructions executed and the number of interlocks incurred. Table 3 presents interlock information \non the performance of one of the benchmarks, MDG. In this table, the percelmt\u00adage of the total number \nof cycles that were interlock cy\u00adcles is reported for both the traditional and balanced sched\u00adulers. \nMDG s performance gain with balanced scheduling (and also that of the other programs) is a result of \nboth exe\u00adcuting fewer instructions (Bins < Tins) and incurring fewer interlocks (BI% < TIYo). Balanced \nschedules execute fewer instructions because their schedules often contain less spill code. Table 4 presents \ndata on the percentage of total instructions executed that was classified as spill code. (A spill instruction \nis defined to be any instruction that is inserted by the register allocator.) Bal\u00adanced scheduling incurred \nfewer spills than the traditional scheduler for virtually all implementation-defined latenciies on all \nprograms. (The sole exceptions were ARC2D with an optimistic latency of 30 cycles and FL052Q with 3.6 \ncycles.) We hypothesize that the reduction in interlocks and spill code when using the balanced scheduler \nis a direct conse\u00adquence of its always considering load level parallelism when calculating latency weights. \nIt measures the parallelism, and, whether it is high or low, tries to use it to hide all load laten\u00adcies \nin a basic block. When there is significant load level parallelism, code DAGs tend to be bushy, causing \nall list schedulers to sched\u00adule independent instructions in parallel. The balanced sched\u00aduler manages \nthis by assigning load instruction weights in such a way that load latencies are hidden by the other \nin\u00adstructions. Traditionat schedulers lack the guidance for ef\u00adficient load placement. Therefore they \nincur similar register pressure, but also more interlocks. When there is little load level parallelism, \ntraditionat schedulers greedily let independent instructions float to one end of the basic block. Therefore \nthey incur spills at that end, and interlocks at the other. In contrast, the balanced sched\u00aduler spreads \nout the few independent instructions behind all loads. In all cases uses quickly follow definitions, \nand little or no spill code is generated. If the load level parallelism is less than the latency assumed \nby the traditional scheduler, balanced scheduling generates fewer spill instructions than the traditionat \ntechnique. In both situations (high and low load level parallelism) balanced scheduling contributes either \nlittle additional or less register pressure. When actual latencies differ from the optimistic latency, \nbalanced scheduling incurs fewer inter\u00adlocks; when both Iatencies are equal, the number of inter\u00adlocks \nproduced by the two schedulers is similar. When load latencies are much larger than the amount of load \nlevel parallelism and therefore cannot be hiddlen via instruction scheduling, there is no guarantee the \nbal\u00adanced scheduler will do better. In this case, register pres\u00adsure can be a problem, and balanced scheduling \ncan insert more spill code than the traditional scheduler. The situa\u00adtion is illustrated in Table 5, \nwhich summarizes the results for the N(30,5) model. This model assumes a mean latency much larger than \nthe amount of load level parallelism of the programs in our workload. Two interrelated factors con\u00adtribute \nto balanced scheduling s poor performance with this model. First, as latencies get long, interlocks account \nfor an increasingly large proportion of execution time. Both sched\u00adulers do poorly, and often equally \npoorly (for example, see TRACK). Second, a consequence of long load Iatencies is that each load instruction \nconsumes more cycles relative to other instructions, and its contribution to execution time is greater, \nTherefore whichever scheduler generates more spill loads will have the poorer performance. Occasionally \nbal\u00adanced scheduling chooses load instruction weights that cause higher than necessary register pressure \nand consequently is\u00adsues more spill instructions (for example, see ARC2D), In summary, these results \nindicate that balanced schedul\u00ading reduces execution time relative to traditionat list schedul\u00ading in \nmost cases. Because its schedules are based on the amount of load level parallelism that a program can \nsupport, they cause fewer interlocks during program execution and contain less spill code. The benefits \nare most apparent when memory latency uncertainty is high, as evidenced by greater miss rates and penalties, \nand larger standard deviations from mean Iatencies.   6 Extensions Balanced scheduling has been presented \nin a specific form (weights calculated based on load level parallelism) to solve a specific problem (scheduling \nwith uncertain load instruc\u00adtion latencies). The technique should be applicable to a wider set of problems, \nsuch as other multi-cycle instructions (e.g., floating point operations coupled with asynchronous floating \npoint units), disabling balanced scheduling when the latency is known (e.g., for the second access to \na cache line), techniques that enlarge basic blocks (trace scheduling and software pipelining) and superscalar \narchitectures. 7 Summary This paper describes an instruction scheduling algorithm, called balanced scheduling, \nthat is appropriate for com\u00adputers that expose uncertain memory latencies. Balanced scheduling is fundamentally \ndifferent from previous list schedulers in two respects. First, it ignores the optimistic, implementation-determined \nmemory latency when assign\u00ading scheduling priorities, basing them instead on the amount of paratlel execution \nthat is achievable in the program. Sec\u00adond, it computes individual scheduling weights for each load instruction \nseparately, rather than using a single value for all Percentage of Spilt Instructions B&#38;m ced Tradmonat \nSch eduler w]th Op tlmlsttc La tency of Program Bins Scheduter 2 2.15 2.4 2.6 3 3.6 5 7.6 30 ADM 2,494 \n7.43 9.59 9.15 9.15 9.15 9.22 9.42 9.50 8.70 7.49 ARC2D 11,149 10.47 13.52 13.74 13.74 13.68 13.27 13.46 \n13.89 12.25 10.11 BDNA 2,391 22.84 26.50 26.32 26.32 26.32 24.17 24.94 24.68 24.73 25.54 FL052Q 3,323 \n4.61 7.14 6.82 6.82 6.82 6.97 3.91 6.55 5.89 4.90 MDG 5,144 7.49 7.86 8.04 8.04 8.04 8.04 8.13 8.00 8.86 \n9.21 MG3D 60,784 7.38 9.73 10.36 10.36 10.36 10.36 10.86 10.36 8.85 7.88 QCD2 1,176 19.91 29.30 28.92 \n28.92 28.92 28.92 28.78 28.02 26.89 28.02 TRACK 398 15.78 20.41 17.85 17.85 17.85 17.85 17.85 17.84 17.45 \n17.46 Table 4: Spill Instructions Executed . UNLIMITED MAX 8 LEN 8 Program TrrtS Bitts Imp% m% BI% \nImp% m% BI% Imp% TWO BI% ADM 2,496 2,494 -3.5 67.8 69.0 -1.7 76.1 76.5 -2.7 69.9 70.7 ARC2D 11,108 11,149 \n-5.0 67.3 68.9 -3.9 78.4 79.1 -4.7 70.9 72.1 BDNA 2,478 2,391 1.9 65.0 65.6 12.5 85.3 84.0 2.5 71.1 71.4 \nFL052Q 3,332 3,323 4.1 67.0 65.7 1.4 76.6 76.4 3.7 69.0 67.9 MDG 5,393 5,144 19.3 71.8 67.9 14.5 79.4 \n77.5 18.7 73.1 69.6 MG3D 61,116 60,784 -0.9 63.1 63.7 1.5 86.6 86.5 -3.8 67.4 68.8 QCD2 1,270 1,176 7.1 \n69.0 69.2 23.4 86.4 84.5 6.3 72.2 72.6 TRACK 40+5 398 0.6 81.6 81.9 4.7 85.6 85.2 2.3 82.3 82.3 Table \n5: Analysis of N(30,5) results the effect of spill code. loads in a basic block. Balanced scheduling \nthus insulates program execution from machine uncertainties by generat\u00ading schedules that are optimized \nfor the program rather than the machine. To validate the algorithm balanced scheduling was in\u00adcorporated \ninto the GCC compiler and the performance of the Perfect Club benchmarks scheduled with both bal\u00adanced \nscheduling and a traditional list scheduler was com\u00adpared. Three processors were modeled, representing \nma\u00adchines with varying abilities to exploit instruction level par\u00adallelism. Each of the processor models \nwas coupled with several memory systems that exhibit dissimilar latency be\u00adhavior. Execution time reductions \nof batanced scheduling over the traditional list scheduler averaged between 370 and 18%, depending on \nthe processor model, system model and program. The results demonstrate that, if the capability to exploit \nuncertain memory latency is architected in future ma\u00adchines, balanced schedulers can effectively take \nadvantage of the additional flexibility to generate faster schedules.  Acknowledgements We would like \nto thank David Bradlee, Craig Chambers and Robert Henry for their valuable comments on an earlier draft \nand the compiler group at Tera Computers for their design review.  References [1] [2] [3] [4] [5] [6] \n[7] [8] [9] Anant Agarwal, Beng-Hong Llm, David Kranz, and John Kubiatow\u00adicz. APRIL A processor architecture \nfor multiprocessing. In F ro\u00adceedings of the 17th Annual International Symposium on Computer Architecture, \npages 104-114. IEEE, May 1990. Robert Alverson, David Callahan, Daniel Cummings, Brian Koblenz, Allan \nPorterfield, and Burton Smith. The Tera Computer System. In 1990 International Conference on Supercomputing, \npages 1-6. SIGARCH, June 1990. ANS X3.9-1978. American National Stanohrd Progratnmin g lan\u00adguage FORTRAN. \nAmerican National Standards Institute, New York, 1978. M. Berry, D. Chen, D. Kuck, S. Lo, Y. Pang, L. \nPointer, R. Roloff, A. Samah, E. Clementi, S. Chin, D. Schneider, G. Fox, P. Messina, D. Walker, C. Hsiung, \nJ. Schwarzmeier, K. Lue, S. Orszag, F. Seidl, O. Johnson, R. Goodmm, and J. Martin. The perfect club: \nEffective performance evaluation of supercomputers. The InfernatiomalJournal of Supercomputer Applications, \n3(3), Fatt 1989. Bradley Efron. The jackknife, the bootstrap, and other resampling plans. SIAM/CBMS-NSF \nRegional conferen~ series in applied mathematics, volume 38, 1982. John R. Ellis. Bulldog: A Compiler \nfor VLJW Architectures. ACM doctoral dissertation award; 1985. The MIT Press, 1986. S. I. Feldman, David \nM. Gay, Mark W. Maimone, and N. L. ScIrryer. A Fortran-to-C converter. Computer Science Technicat Report \n149, AT&#38;T Belt Laboratories, Murray Hill, NJ 07974, April 1991. PhiKtp B. Gibbons and Steven S. Muchnick. \nEfficient instmction scheduling for a pipelined architecture. Proceedings of the SIG-PLAN 1986 Symposium \non Compiler Construction, SIGPLAN No\u00adlices, 21(7), July 1986. John L. Hennessy and Thomas R. Gross. Code \ngeneration and re\u00ado~anization in the presence of pipeline constraints. In Symposium on Principles of \nProgramming Language.r, pages 120-127, January 1982. [10] John L. Hennessy and David A. Pattersott. \nComputerArchitecture: A Quantitative Approach. Morgan Kaufmann, 1990. [11] Mark Donald Hill. Aspects \nof Cache Memory and instruction B@er Performance. PhD thesis, University of Cdlfomia, Berkeley, Novem\u00adber \n1987. [12] Gerry Kane. rnips RISC Architecture. Prentice.HalI, 1988. [13] D. Kroft. Lockup-free instmction \nfetch/prefetch cache organization. hr 8th Annual [nternaiional Symposium on Computer Architecture, pages \n81-87,1981, [14] E. Lawler, J. K. Ixmstra, C. Mattel, B. Simons, and L. Stockmeyer. Pipeline schedrding: \nA survey. Research Report RJ-5738, IBM, July 1987. [15] Motorola. MC881OO RISC Microprocessor User s \nManual. Prentice Hall, 1990. [16] Krishna V. Palem and Barbara B. Shuons. scheduling time-critical instmctions \non RISC machines. In ACIU Symposium on Principles of Programming Latguages, January 1990. [17] C. Scheurich \nandM. Dubois. Lockup-free caches in high-performance multiprocessors. Journal of Parallel and Distributed \nProcessing, 11(1):25-36, January 1991. [18] G. S. Sohi and M. Franklin. High-bandwidth data memory systemls \nfor superscalar processor. In Fourth JnternationaI Conference on Archi\u00adtectural Support for Programming \nLarrguagesand Operating System (ASPLOS), pages 53-62, April 1991. [19] Richard Stallrnan. The GNU project \noptimizing C compiler. Free Software Foundation, Inc. [20] Robert Endre Tarjan. Data Structures and Network \nAlgorithm, vol\u00adume 44 of Regional Conference Series in Applied Mathematics. So\u00adciety for Industrial and \nApplied Mathematics, 1983. [21] H. S. Warren, Jr. Instruction scheduling for the IBM RISC Sys\u00adtem/60W \nprocessor. IBM JournaI of Research and Development, 34(l), January 1990. [22] MichaelJ. Woodard. Personal \ncommunication. Scheduling techniques used in Sun SPARC compilers, September 1992.  \n\t\t\t", "proc_id": "155090", "abstract": "<p>Traditional list schedulers order instructions based on an optimistic estimate of the load delay imposed by the implementation. Therefore they cannot respond to variations in load latencies (due to cache hits or misses, congestion in the memory interconnect, etc.) and cannot easily be applied across different implementations. We have developed an alternative algorithm, known as balanced scheduling, that schedule instructions based on an estimate of the amount of instruction level parallelism in the program. Since scheduling decisions are program-rather than machine-based, balanced scheduling is unaffected by implementation changes. Since it is based on the amount of instruction level parallelism that a program can support, it can respond better to variations in load latencies. Performance improvements over a traditional list scheduler on a Fortran workload and simulating several different machine types (cache-based workstations, large parallel machines with a multipath interconnect and a combination, all with non-blocking processors) are quite good, averaging between 3% and 18%.</p>", "authors": [{"name": "Daniel R. Kerns", "author_profile_id": "81100171871", "affiliation": "", "person_id": "P58963", "email_address": "", "orcid_id": ""}, {"name": "Susan J. Eggers", "author_profile_id": "81100262930", "affiliation": "", "person_id": "PP15027685", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/155090.155117", "year": "1993", "article_id": "155117", "conference": "PLDI", "title": "Balanced scheduling: instruction scheduling when memory latency is uncertain", "url": "http://dl.acm.org/citation.cfm?id=155117"}