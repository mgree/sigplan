{"article_publication_date": "06-01-1993", "fulltext": "\n Improving the Cache Locality of Memory Allocation Dirk Grunwald Benjamin Zorn Robert Henderson Department \nof Computer Science Campus Box #430 University of Colorado, Boulder 80309-0430 Abstract The allocation \nand disposal of memory is a ubiquitous operation in most programs. Rarely do programmers concern themselves \nwith details of memory allocators; most assume that memory allocators provided by the system perform \nwell. This paper presents a per\u00adformance evaluation of the reference locality of dynamic storage allocation \nalgorithms based on trace-driven simulation of five large allocation-intensive C programs. In this paper, \nwe show how the design of a memory allocator can significantly affect the reference locality for various \napplications. Our measurements show that poor locality in sequential-fit allocation algorithms reducesprogram \nper\u00adformance, both by increasing paging and cache miss rates. While increased paging can be debilitating \non any architecture, cache missesrates are also important for modem computer architectures, We show that \nalgorithms attempting to be space-efficient by coa\u00adlescing adjacent free objects show poor reference \nlocality, possibly negating the benefits of spaceefficiency, At the other extreme, algo\u00adrithms can expend \nconsiderable effort to increase reference locality yet gain little in total execution performance. Our \nmeasurements suggest an allocator design that is both very fast and has good locality of reference, 1 \nIntroduction The allocation and disposal of memory is a ubiquitous operation in most programs, yet one \nlargely ignored by most programmers. Some programmers use domain-specific knowledge to improve the speed \nor memory utilization of memory allocators; however, the majority of programmers use the memory allocator \nprovided in a given programming environment believing it to be efficient in time and/or space, In virtual \nmemory systems, space efficiency is usu\u00adally a secondary concern, although important in some applications. \nRarely are programmers concerned with secondary effects, such as cache locality. In this paper, we show \nthat the structure of dynamic storage allocation (DSA) implementations contribute to the page and cache \nmissrate. Ourmeasurementsshowthatincreasedcachemissescan increase program execution time on conventional \narchitectures by UP to 25Y0. Permission to copy without fee all or part of this material is granted provided \nthat the copies are not made or distributed for direct commercial advantage, the ACM copyright notioe \nand the title of the publication and ite date appear, and notice is given that copying ie by permission \nof the Association for Computing Machinery. To copy otherwise, or to republish, requires a fee and/or \nspecific permission. ACM-SlGPLAN-PLDl-6 /93/Albuquerque, N.M. e 1993 ACM 0-89791 -598 -4/93 /0006 /0177 \n. ..$1 .50 Inapreviouspaper[8], wemeasuredandcomparedthetime and space efficiency of standard memory \nallocators. Based on empiri\u00adcal observations[29], we designed a program (WSTOMALLOC) that synthesizes \na memory allocator customized for a specific applica\u00adtion, Empirical measurements show that the synthesized \nallocators are uniformly faster than the allocators distributed with widely-used operating systems (e,g., \nBSD) and are also more space efficient. In that study, we measured the number of machine instructions \nused to allocate and free memory, and did not account for the effects of the memory hierarchy, which \nis the focus of this paper, 1.1 The Importance of Rafarence Locality In recent years, faster processor \nspeedshave shifted interest in mem\u00adory system performance from slower main memories to high-speed cache \nmemories. Cache memories must be able to provide instruc\u00adtions and data to processors at increasingly \nfaster rates. AS proces\u00adsors and cache memories increase in speed, the cost of missing in the cache and \nretrieving data from main memory increases, Jouppi estimates the cost of a cache miss will increase to \nover 100 cycles if current trends continue [11], Mogul and Borg evaluate the effect of context switches \non a hypothetical two-level cache that requires 200 cycles to service a second-level cache miss [19]. \nAlthough the performance of cachesis improving, new processorscommonly use a smaller on-chip primary \ncache, with a larger secondary cache. Increased cache misses are difficult to detect, Some recent tools \n[7, 17] indicate what regions of a program incur excessive cache misses. However, they do not indicate \nthe reason for those misses. Although the cache misses may be seen in one region of the program, the \ncause may arise elsewhere, More insidiously, the increased cache misses may be spread over all program \nsections that reference heap allocated objects, belying the true influence of the DSA algorithm. We believe \nthat some algorithmic and imple\u00admentation decisions in DSA design influence the overall reference locality \nof a program. Furthermore, unlike the related problem of virtual memory locality, it is difficult to \ndevote more resources to reduce cache misses. Although main memory can be expanded to reduce page faults \nin an existing system, it is often impossible to expand the size of a primary or secondary cache, Thus, \nfor existing computers, improved management of the cache resource is the only alternative. In this paper, \nwe investigate the effects of DSA algorithms and implementation choicesoncachelocality andtotalperformance \nina number of significant allocation-intensive programs, We show that these choices affect the overall \nperformance of allocation-intensive programs by up to 25%. Misses are increased directly by DSA im\u00adplementations \nwith poor reference locality and indirectly by imple\u00admentations that fill valuable cache spacewith storage \nmanagement overhead. Combining the results of this paper with our previous work on synthesizedDSA implementations, \nwe expect it is possible to design fas~ space-conservative DSA implementations that have verygoodcachelocality. \nReferencelocality andallocator speedare two aspectsof memory allocation that must be balanced to achieve \nhigh performance. 1.2 Structure of the Paper In the next section, we describe the DSA implementations \ncon\u00adsidered in this paper and review the previous work in measuring the locality of DSA implementations. \nIn $3, we describe our ex\u00adperimental methodology including the suite of programs used to compare the \ndifferent DSA implementations. Section 4 presents the results of our simulation study, both for page \nlocality and cache locality. Finally, in $5we summarize the salient features of memory allocators that \nare both fast and have high reference locality. 2 Previous Work 2.1 Memory Allocation Algorithms General \npurpose algorithms for dynamic storage allocation have been proposed, analyzed and tuned for many years. \nKnuth [13], Bozman et al [3], and Kom and Vo [14] all carefully describe and evaluate various allocation \nstrategies and implementations. Tradi\u00adtionally, these algorithms have sought to minimize CPU overhead \nand reduce total memory usage. Standish [22] divides algorithms for dynamic storage allocation into three \nbroad categories: sequential-fit algorithms (e.g., first-fit and best-fit), buddy-system methods (e.g., \nbinary-buddy and Fi\u00adbonacci), and segregated-storage algorithms, which is a catch-all category involving \na number of different approaches. Our previous research on DSA customization [8] has given us first-hand \nexperi\u00adence with several of the most widely-used implementations. In this paper, we compare the reference \nlocality of five algo\u00adrithm implementations that are described below. These particular algorithms were \nchosen because they are well-known, considered to be efficient (either in time, space, or both), and \nwidely-used. FIRSTFIT:Thisalgorithm isanimplementation ofafirst-fit strategy with optimizations suggestedby \nKnuth [13] and implemented by Mark Moraes. In this algorithm, free bIocks are connected together in a \ndoubly-linked freelist that is scanned during allocation for the first free block that is sufficiently \nlarge, This block is split into two blocks, one of the appropriate size that is returned, and the other \nthat is placed back in the fieelist. As an optimization, if the extra piece is too small (in this case \nless than 24 bytes), the block is not split, The freelist pointer is implemented as a roving pointer, \nwhich eliminates the aggregation of small blocks at the hont of the freelist. Allocated blocks in this \nalgorithm require two extra words of overhead(boundary tags),oneateachendoftheblock,which containthesizeoftheblock \nanditscurrentstatus(allocatedor free). Boundary tags allow objects to be freed and coalesced with adjacent \nfree storage in constant time. GNU G++: This algorithm, implemented by Doug Lea [16], en\u00adhances the standard \nfirst-fit algorithm by using an array of freelists segregatedby object size. In eachfreelist, freeblocks \nare connected together in a doubly-linked list. An appropri\u00adatefreelist isselectedbasedonthelogarithm \noftheallocation reques~ this is done to increase the probability of a better tit. In other respects, \nthis algorithm is similar to FIRSTFIT. BSD: Chris Kingsley implemented a fast segregated-storage algo\u00adrithm \nthat was distributed with the 4.2 BSD Unix release [12]. Kingsley s algorithm rounds object size requests \nto powers of two minus a constant and a freelist of objects of each size class is maintained, If no objects \nof a particular size class are available, more storage is allocated. No attempt is made to coalesce objects. \nBecause this algorithm is so simple, its implementation is very fast. On the other hand, it also wastes \nconsiderable space, especially if the size requests are often slightly larger than the size classesprovided. \nGNU LOCAL: Mike Haertel implemented a hybrid of the first\u00adfit and segregated-storage algorithms that \nis available to the public as the Free Software Foundation implementation of malloc/free [9]. In Haertel \ns algorithm, requests larger than a specific size (e.g., 4096 bytes) are managed using a first\u00adfit strategy, \nwhile objects smaller than this are allocated in specific size classes(powers of two), like the BSD algorithm. \nHaertel s approach actively seeks to improve the locality of reference during allocation by dividing \nallocated storage up into page-sized chunks and storing information about these chunks in small, highly-localized \nchunk headers. Instead of traversing the entire heap attempting to find a fit, only the information in \nthe chunk headers must be traversed. This algorithm alsoreducestheper-object spaceoverheadrequired by \nother algorithms (such as the boundary-tags in the first-fit algorithms) in the following way. Chunks \nare allocated so that all objects in a chunk are the same size. The address of any object can be used \nto locate the chunk header, which contains information about the object size associatedwith the chunk. \nThe chunk header also contains a count of the number of free blocks within a chunk and deallocates entire \nchunks when all the objects in the chunk have been freed, QUXCKFIT:Weinstock and Wulf describe a fast \nsegregated-storage algorithm based on an array of fleelists[24, 22]. Like the GNU LOCAL algorithm, QUICKFIT \nis a hybrid algorithm that aIlocates small and large objects in different ways. Large objects are handled \nby a general algorithm, while objects smaller than a certain threshold are allocated and deallocated \nvery quickly. The algorithm is very fast because the object requestsizeisusedasanindexintothefreelist \narray,returning the appropriate fieelist in a small number of instructions. Deallocation requires identifying \nthe size of the object being freed (using a boundary @g in our implementation), indexing the fieelist \narray to get the appropriate freelist, and placing the free object on that list. Both the BSD and QUICKFIT \nalgorithms make no attempt to coalesce free objects. Unlike the BSD algorithm, which rounds sizes to \npowers of two, QUICKFITrounds to multiples of word sizes (e.g, 4,8, or 16 bytes), reducing internal fragmentation. \nThe configuration of QUICKFIT that we measured handles allocation requests of 4-32 bytes (rounded to \nword size) with the fast array and larger requestsusing a general purpose first-fit algorithm (GNU G++, \nin our case).  2.2 Measurements of Locality Most studies of reference locality in DSA implementations \nconcern garbage collection (GC) algorithms [15, 18,20,23,25,26,27, 28]. i ?lmtFk 1 The main reason for \nthis emphasis is that references made during OukkFii garbage collection have poor locality compared \nto program ref-40,[ :, a++ BSD erences. This poor locality can result in thrashing of the virtual QNUIx@ \nmemory paging system when a program s address space is larger than the physical memory. Experiences with \nthrashing led to the development of generational GC algorithms, which focus the efforts of garbage collection \non a relatively small part of the total address space[18], A number of reporta, including those of Moon \n[20] and Ungar [23], indicate that generational GC successfully reduces the page-fault ratesingarbage-collected \nlanguagesystemssuchasLisp and SmallTalk. Other attempts to improve the reference locality of garbage \ncollection hrwe focused on improving the traversal or\u00adder of objects in memory during a collection. Techniques \nsuch as breadth-first, depth-first, approximate depth-first hierarchical de\u00adcomposition, and type-directed \ntraversals have been implemented and measured [20, 15, 26], More recently, there has been greater emphasis \non the cache locality of these algorithms, Zorn measured the cache locality of generational copying and \nnon-copying garbage collection [27, 28]. Wilson also reports on issuesrelated to the cachelocality of \ncopying generational collection, including the effect of cache aasociativity, cache size, and algorithm \ndesign on miss rate [25], Unfortunately, many of the locality issues related to GC al\u00adgorithms are not \nrelevant for explicit dynamic storage allocation because there is no need for explicit algorithms to \ntraverse the set of living objects. Furthermore, garbage collection techniques that copy data are also \nnot applicable in languages such asC, where dy\u00adnamicly allocated objects cannot be relocated, One possible \nreason for the dearth of prior work on the reference locality of explicit DSA algorithms is the perception \nthat the reference locality of existing algorithms k good enough. We seein \\4 that reference locality \ncan have a dramatic effect on program performance, even with explicit DSA algorithms. 3 Experimental \nDesign In this section, we describe our measurement methods, The results in $4 are based on measurements \nof actual allocation-intensive pro\u00adgrams. We believe that dynamic memory allocation will play an increasingly \nimportant role in existing and future systems. We only consider allocation-intensive programs to better \nillustrate the prob\u00adlems encounted. Here, we provide information about the programs measured and the \ntools used to measure them, 3.1 Application Programs Our goal is to investigate the effect of DSA algorithms \non cache locality in programs that are of interest to a wide class of users. We collected five allocation \nintensive programs, described in Table 1 and Table 2, representing a variety of allocation-intensive \napplica\u00adtion domains including language interpreters (GAWK and GS), a lan\u00adguagetranslator(PTc),aprogram \ndependenceanalysistool(MAKE), and a PLA logic optimizer (ESPRESSO).As Table 2 shows, the in\u00adput sets \nused exercise the storage allocation system significantly, resulting in hundreds of thousands of allocations \nwhile executing more than a billion instructions. In this study, we did not modi~ the applications; any \nchange in cache misses using different DSA algorithms arises either from references in the DSA subroutines \nor becausethe representation of allocated objects has some effect on the locality of the application. \nIn general, it is difficult to measure the exact contribution of these direct and indirect effects on \nthe overall cache miss rate. Aa with sac i 120,, ! # 10.0 0,0 ---n. upmso 05 m. Qawk Make Figure 1: \nPercent of Time in Malloc and Free (as % of Execution Time) any programmer, we are concerned with the \ninfluence of DSA implementation on total execution time, Figure 1 shows the fraction of execution time \nthe application programs spend doing dynamic storage allocation (both allocation anddeallocation), asmeasuredbycounting \ninstructions andassum\u00ading no cache miss penalty; in $4.2, we measure the influence of cache miss penalties. \nAs the figure shows, the choice of allocator dramatically affects the fraction of time spent doing allocation. \nFor the programs considered, the time spent doing allocation ranges from a few percent to = 30%, depending \non the allocator and application. 3.2 Measurement Tools We measured execution time in terms of machine \ninstructions us\u00ad ing Larus and Ball s QPutility [1]. This tool provides a dynamic execution count for \neach subroutine in terms of instructions, Using this tool removes any variabtiity in counting program \ninstructions, greatly simplifying the experimental design. To measure cache locality, we instrumented \nthe programs using PIXIE [6]. Although PIXIE can also provide execution information similar to that of \nQP,we found the output format of QPmore usefid in our study. Using PIXIE, the instrumented programs emit \ncoded information allowing us to reconstruct the data references. We modified a version of the ~CHO [10] \ncache simulator to implem\u00adent execution-driven cache simulation. Since the cache simulator directly consumed \nthe trace horn the instrumented application, we were able to quickly trace a large number of references \nwithout storing large trace files. We used traces varying from 17 million references to almost 600 million \nreferences (seeTable 2 for the data references generated by the FIRSTFIT allocatoq other allocators have \na similar number of references.) We chose to simulate a direct-mapped cache with a 32-byte block size, \nAll cache miss rates presented are the miss rates for the data cache; in all cases,we assume the instruction \ncache miss rate is O%,making our predictions of the cache effect on execution time conservative. Likewise, \nour miss rate results are conservative be\u00adcausewe intentionally avoid introducing the effects of intermittent \ncache flushes on the miss rate. Because the tools we use generate deterministic results, our experiments \ndid not require statistically averaging multiple runs. We used VMSIM, a fast implementation ESPRESSOEspresso,version2.3,isalogicoptimization \nprogram.Theinputfileisanexampleprovidedwiththe release code. GS GhostScript, version 2.1, is a publicly \navailable interpreter for the PostScript page-description lan\u00adguage. The input files were a variety of \nsmall and large files, including a 126-page user manual. This execution of GhostScript did not run as \nan interactive application as it is often used, but instead was executed with the NODISPLAY option that \nsimply forces the interpretation of the Postscript(without the makefile of another large application. \nTable 1: General Information about the Test Programs Program ESPRESSO GS pTC GAWK MAKE 13zE Time (sec.) \n155.1 131.3 25.1 76,7 4.0 Total Instr. (x lo ) 2506 1344 367 1215 56 Data Refi, (xlO ) 595 421 125 \n374 17 Max. Heap Size (Kbytes) 396 4129 3146 60 380 Objects Allot ed (1000s) 1673 924 103 1704 24 m6@iF \nFreed (1000s) 1666 898 0 1702 13 Table 2: Test Program Performance Information. Execution times were \nmeasured on a DECstation 5000/120 workstation (MIPS architecture) with 24 megabyt=s of memory. All values \nprovided are for the FIRSTFITallocator, which is used as a baseline in later figures. of a stack simulation \nalgorithm [27], to measure page fault rates; a page size of 4 kilobytes was used for all page fault measurements. \n4 Comparison We traced the execution of the five applications using the five dif\u00adferent DSA implementations \ndescribed. We used these traces to measure the page fault rate and cache miss rate. 4.1 Page Reference \nLocality We measured the page fault rate of five applications using the dif\u00adferent DSA implementations. \nFigures 2 and 3 show the page fault ratesexpressedin faults per memory reference for two applications, \nGSand PTC. In both figures, the symbols on the horizontal axis in\u00addicate the total amount of memory requested \nby the program using the different DSA implementations. Although each application has different behavior, \nthese datasets effectively capture the range of effects of DSA implementation on the page fault rate. \nFigures 2 and 3 indicate two interesting metrics for each DSA implementation: the maximum required space \nand the slope of the pagingrateforeachDSAimplementation. Largepagefaultratesare so debilitating that \nfew systems can tolerate even the small amount of paging implied by the subtle differences between the \nefficient allocators shown in Figures 2 and 3 for a fixed memory size. It is generally true that reducing \nthe amount of memory needed by an application has the greatest inff uence on the paging rate. Thus, the \nmost effective measure to reduce paging is to select a very spaceefficient allocator. TheslopeofthepagingrateforeachDSA \nimplementation in Figures 2 and 3 indicates the resiliency of the allocators to restricted resources. \nFor example, the performance of the FIRSTFIT algorithm in the GhostScript application would rapidly degrade \nif memory was unavailable, while the performance of QUICKFITwould degrade less rapidly. Rvo of the DSA \nimplementations, FIRSTFITand GNU G++, are variants of the classic first-fit algorithm. They find free \nregions of memory by traversing a linked-list data structure and coalesce contiguous free regions. These \nalgorithms tend to have poor page locality becausethe allocator examines several objects in the linked \nlist and those objects maybe scattered throughout the addressspace. Furthermore, when searching for a \nlarge objec~ all entries in the linked list may be visited. Recall that the FIRSTFITimplementation uses \na single doubly-linked list structure to hold all objects, while GNU G++ uses multiple doubly-linked \nlists, segregated by the size of the free objects. On average, the GNU G++ allocator examines fewer objects \nthan FIRSTFITwhen allocating memory. The effect of this simple algorithmic change is dramatic. By searching \nless objects in the ileelist, the GNU G++ algorithm is more resilient: and would fault less frequently. \nAs shown in Figures 1 and 2, the conventional FIRSTFITalgorithm increasesboth the raw execution time \nand the page fault rate. Another factor that affects both FIRSTFITand GNU G++ is the decision to coalesce \ncontiguous objects on the fleelist. This is usually done either by maintaining a sorted freelist or by \nusing a doubly-linked freelist and boundary tags. Both decisions have drawbacks. Maintaining a sorted \nlist takes considerable CPU time and many pages will be visited when objects are inserted in order. Although \nthe overhead of doubly-linked freelista is smaller, they require that three objects be modified to insert \nan item in the list (the newly heed object, its predecessor and successor), and these references may \nbe to different pages, In our previous studies [29, 8], we found that most allocation requests were for \none of a few FlrstFlt  ___________________ i%%k~l~ - _____________ etw~     _____ Bai2__ 10,000 \n IN [ [ n--IHE ~ ----------------9W!=1 1,000 0.100 BSD 0.010 I4 0.001 o~ Memory Size (Kbytee) Figure2: \nPage Fault Rate for GhostScript (m)asafunct.ionof physical memory size 100,000 ~ 10.000 ~ 1.000 0100 \nO--O Fket Fit QulckFlt _ ONU g++ 0!01o M BSD GNU 10Cd f 1 0.001 0 1000 2000 3000 1 4000 Memory Size \n(Kbytea) Figure3: Page Fault Rate for Pascal To C(PTC)as afunctiorsof physical memory size different \nobject sizes, Consequently, ifanobject hasalready been allocated, coalescing an object when it is deallocated \nmay have little benefit becauseit will be re-used immediately, This design decision influences both the \nbasic execution time of the algorithms and their reference locality. To one extent or another, this observation \nof frequent re-use is exploited by the remaining allocators, which form a distinctly sep\u00adarate classwith \ngreater resiliency, However, even in this group, the page fault rate evinces algorithmic dtierences. \nThe BSD imple\u00admentation spends relatively little time actually allocating memory, but the BSD implementation \nsuffers from severeinternal fragmenta\u00ad tion becauseit allocates w 2r*8 1 bytes for an iV byte objec~ \nmuch of the allocated space maybe wasted. This increases the page fault rate for BSD, because more pages \nmust be resident to accessthe same set of allocated objects. On the other hand, allocating exactly iV \nbytes implies that the number of size classes, or distinct groups of allocation request sizes, will increase. \nThis can increase the CPU time to allocate objects and decreaseobject re-use. For some applications, \nsuch as GS, increased paging caused by the spacewasted by BSD may offset the advantagesof the faster \nal\u00adgorithm, For other applications, such asPTC,there is little effective difference between the space \nneeded by the different DSA imple- Eepresso GS Ptc Gawk Make Figure4: Normalized program execution time \nwith 16K direct\u00admappedcache,25-cyclecachemisspenalty,overlaid onnormalized execution time when we ignore \nthe memory hierarchy, 1m z 1= c 1.25 ij % &#38;! 3 1,00 E s 0.75 Eepreeso GS Ptc Gawk Make Figure5: Similar \nnormalized program execution time, assuminga 64K direct-mapped cache with 25-cycle cache miss penalty. \nmentations, For example, the design of the GNU LOCAL allocator attempts to explicitly increase reference \nlocality, at considerable expense in execution performance (see Figure 1), but appears to gain little \nby this careful design, The impact of DSA design on the page fault rate is dramatic, Some of the DSA \ndesign principles that reduce cache misses will also reduce page faulta, but the most important DSA design \ngoal should be to reduce space requirements without sacrificing execu\u00adtion speed, We now consider how \nto improve cache miss rates because it is difficult to expand the size of an existing cache and it is \nmore difficult for programmers to detect the influence of DSA design on cache locality. 4.2 Cache Reference \nLocality In auniprocessor, cachemissesoccur either becauseof a cold start (when an an item is brought \ninto cache for the first time) or cache overflow (where a referenced item was forced out of the cache \nto make room for a new item), Becausewe have simulated reasonably long tracesandwe areconsidering moderate \ncachesizes,we believe that cold start misses play a minor role in our study. Cache misses can be reduced \nby prefetching data. Prefetching may be directed by software [4], but usually ariseswhen cachelines contain \nmultiple words [21] referencing one word automatically brings other words into the cache. In this paper, \nwe consider only the effect of hardware prefetching. We first measured a base execution time, ignoring \neffects of the memory hierarchy. For each application, the execution time for the different DSA implementations \nwas normalized relative to the FIRSTFITDSA implementation. The shaded values in Figure 4 and 5 show the \nnormalized execution time for each application and allo\u00adcator. These values are overlaid with the normalized \nexecution time when we include the additional delays introduced by the memory hierarchy. Although some \napplications execute significantly longer than others, the execution time for different DSA implementations \nwithin a single application usually differs by less than 10%-20Y0, as indicated by Figure 1. The clear \nblocks overlaying the nor\u00admalized execution time in Figures 4 and 5 show the normalized execution time \nfor each application when we consider cache misses in small (16K) and medium (64K) direct-mapped caches \nusing a modest cache miss penalty (25 cycles). These figures include only the effects of data cadre misses. \nIf an application executed 1 in\u00adstructions with D data references, a data cache miss rate of M and a \nmiss penalty of P, we estimated the total execution time to be 1 + (ill x P)D. We assume all instructions, \nincluding loads and stores, complete in a single machine cycle, and ignore the effects of page faults, \ninstruction cache misses and other cache design issues. Page faults are not considered for reasons described \nin \\4.1, and other effects (such as instruction cache misses) are relatively con\u00adsistent across the different \nDSA implementations and less relevant to our study. Figures 4 and 5 show that the reference locality \nof DSA imple\u00admenta~ionscan significantly influence the overall execution time of a program. However, \nthere is no single DSA implementation that increases reference locality in any significant way across \nall appli\u00adcations, Figures 4 and 5 show the effect of cache misses for only a smallrangeofcachesizesandfor \nasingleinput setfor eachapplica\u00adtion. When considering all applications, allocators and cachesizes, the \nquantity of data precludes an exhaustive presentation, and we will use one application (GhostScript) \nto illustrate several points in the remainder of this section becauseFigures 4 and 5 show the most variance \nfor the GhostScript application. We ran the GhostScript application using three different input sets. \nTable 3 shows that each input set evoked significantly different execution characteristics. Figures 6, \n7 and 8 show the cache miss rates for these different input setswhile we vary the size of the direct-mapped \ncache tlom 16K to 256K. These figures illustrate several important points for this appli\u00adcation. First, \nthere are significant differences in the cache miss rate for different DSA implementations acrossall \nthe input setsand cache sizes. Quite naturally, these differences are muted for the smaller input set \nthat ostensibly allocates and references a smaller amount of data. For each input set, we see that the \nDSA imple\u00admentation with thelargestcachemissratio isFIRSTFIT.Recall that this implementation also suffered \nfrom extremely poor page ref\u00aderence locality, The remainder of the DSA implementations have markedly \nbetter cache locality, although the other first-fit imple\u00admentation (GNU G++) has the second highest \nmiss rate. In the other applications, no other DSA implementation has such exceptionally poor behavior. \nFrom this data, we can conclude that searching a freelis~ as done by FIRSTFm and GNU G++, is disastrous \nfor page reference and cache locrdity. As noted, our earlier study [8] indicated that this searching \nbestowed little advantage on these algorithms; they Cache Size (Kbytes) Figure 6: Data cache miss rate \nfor GhostScript (rlW3mall) 6.0 5!0 - w C?uickFlt s ~ 4.0 - A---A BSD +++ GNUlocal 2! .2 3.0 - x $ 2.0 \ne 1.0 - 0,0 0 64 i 28 192 256 Cache Size (Kbytes) Figure 7: Data cache miss rate for GhostScript (@-Medium) \nj!j 3.0 \u00adz g 2.0 \u00ad0 1.0 - Oto o 64 128 192 256 Cache Size (Kbytee) Figure 8: Data cache miss rate for \nGhostScript (GS-Large) Exec. Total Data Max. Heap Objects Objects Program Time Irrstr. Refs. Size Allot \ned Freed (sec.) (x 106) (xlO ) (Kbytes) (1000s) (1000s) es-Small 17.0 195 66 1092 109 102 Gs-Medium 51.3 \n539 172 2721 567 551 GS-LSrge 131.3 1344 421 4129 924 898 Table 3: Characteristics of Different Input \nSets for GhostScript PSPRESSO GS Total Total Allocator time (see)/ time (see)/ Miss Miss  time (see) \ntime (see) FIRSTFIT 199.67/43.01 113.13/29.11 QUXCKFIT 192.16/41.85 90.18/12.22 24.84/2.62 72.02/12.12 \n3.57/0.21 GNU G++ 188.14/34.94 91.38/15.09 25.50/2.82 77.25/14.87 3.70/0.27 BSD 184.80/34.39 89.65/14.65 \n24.9312.62 70.35/10.14 3.55/0.18 GNULOCAL 213.07/35.40 100,74/16.44 25.36D.57 I 89.25/13.84 I 3.67/0.13 \nI Table 4: Total estimated execution time and time waiting for a 16-kilobyte direct-mapped cache miss \nin five allocation-intensive programs. ESPRESSO GS PTc GAWK MAKE Total Total Total Total Total Allocator \ntime (see)/ time (see)/ time (see)/ time (see)/ time (see)/ Miss Miss Miss Miss Miss time (see) time \n(see) time (see) time (see) * FIRSTFIT 164.74/8.08 24,16/1,21 79.18/3.27 3.6910.14 QUICKFIT 159.16/8,85 \n81;29/3.32 23.2711.04 61.83/1.92 3.45/0.08 GNU G++ 163.74/10.55 82.96/6.67 23.83/1.16 65.20/2.82 3.53/0.09 \nBSD 163.14/12,72 78.95/3.95 23.45/L15 62,40/2.19 3.4310.06 GNULOCAL 185.33/7.67 88.15/3.85 23.77/0.98 \n76.70/1.29 3.6010.05 Table 5: Total estimated execution time and time waiting for a 64-kilobyte direct-mapped \ncache miss in five allocation-intensive programs. Metric ESPRESSO GS PTc GAWK MAKE GNULOCAL (w/tags) \nMiss rate 0.880 0,580 0,600 0.250 0.240 GNU LOCAL (w/tags) Miss penalty (% of total (no tags) exec. time) \n5.27 4.51 4,91 1.99 1.78 GNULOCAL (no tags) Miss rate (%) 0.680 0,560 0.500 0.210 0,200 GNU LOCAL (no \ntags) Miss penalty (% of total (no tags) exec. time) 4.14 4,37 4s3 1.68 1.49 Penalty due to boundary \ntags (% of total (no tags) exec, time) 1.13 0.14 0.78 0.31 0.29 Table 6: The effect of boundary tags \non execution time in the GNU LOCAL allocator with a 64-kilobye direct-mapped cache. The rows show themisspenaltyandexecutiontimedegradationfortheGNULOCALallocatorbothwithandwithout \nboundarytags.Thefinalrowshowsthe percentageincreaseinexecutiontimeduetocachemissescausedbyboundary tags. \ntendtouseslightly lessspacethanotherDSAimplementations, but at a sizable execution time penalty. Although \nthese allocators have attributes that are theoretically appealing, they are not suitable DSA implementations \nfor modern architectures and programming styles. This is not caused by a naive implementation; both FIRSTFrTand \nGNU G++ are well crafted and exploit many of the improvements that have been suggested for first-fit \nallocators. The differences between the remaining DSA implementations we considered is inconclusive, \nIndeed, Figures 6, 7 and 8 show that each implementation has the smallest miss rate in one of the different \ndata sets. This is surprising, because one implemerrta\u00adtion, GNU LOCAL expends considerable effort to \ngarner reference locality. Tables 4 and 5 show that, for 16K and especially for 64K direct-mapped caches, \nthis is largely successful. Each table shows the total estimated execution time for each allocator and \napplication and the portion of that execution time attributable to cache misses.1 The GNU LOCAL allocator \ndoes reduce delays from cache misses, but it doesn t take advantage of the rapid allocation and dealloca\u00adtion \ntechniques used in the QUXCKFITor BSD allocators and thus the overall execution time for GNU LOCAL is \nlarger than QUICKFIT or BSD, However, for caches with very high miss penalties, the reduced miss rate \nmay have a more significant effect. The algorith\u00admicchangestoenhancelocality aremoreeffectiveformoderatesize \ncaches.Smallcachescannotcontainenoughoftheworking setand suffer from high cachemisses. Large cachescontain \nenough of the working set that all algorithms begin to perform well, discounting the additional effort \nexpended by GNU LOCAL. Clearly, large: moderate and small are not absolute measure -the important cache \nsizes depend on the application behavior.    4.3 Design Decisions for Improving Reference Locality \nThe focus of this paper is to highlight the design principles for developing a DSA implementation that \nachieves high reference lo\u00adcality. Reference locality, both at the level of pages and caches, is important \nbecause it can increase the actual program execution time, as shown by Figures 4 and 5, Efficient storage \nmanagement for DSA implementations can affect the page reference locality, as shown in Figure 2. Many \nDSA algorithms, such as FIRSTFIT,GNU G++ and GNU LOCAL attempt to reduce the amount of memory requested \nfrom the operating system. In computers lacking virtual memory or having very limited physical memory, \nthis is an ap\u00adpropriate consideration. However, on most modem architectures, spaceconsiderationsshould,within \nreason,besecondarytooverall performance. If an allocator requests memory pages, but they are not actively \nreferenced, then they have little effect on the program performance. As mentioned earlier, the BSD allocator \ncan waste considerable memory, enough so to warrant the within reason caveat. This wastedspaceoccursbecauseBSD \nwasnot designedwith sufficient knowledge of the way programs actually behave [29]. Similarly, QUICKFIT \nis efficient for a pre-determined range of size classes; determined by experience and anecdotal evidence; \nvarious tech\u00adniques can make QUXCKFITsuitable for most programs [8]. We believe the structure of the \nQUICKFIT allocator should be the foundation for high-performance DSA implementations. Are there ways \nto improve on QUICKFIT? We believe some techniques Theseare eshded execution times; however, the execution \ntime for Oreconfigura\u00adtion moat similar to our test vehicle (a DEC.stetion-SOOO/120with 64K direct-mapped \ndata cache), closely mstch the actual execution times, some of which sre listed in Table 2. Figure 9: \nMapping Allocation Requests used in the GNU LOCAL allocator are applicable, and will further improve \nthe reference locality of the QUICr@IT-style allocators. Recall that the QUICKFIT allocator manages a \nsubset of the possible allocation request sizes; the remaining requests must be managed by another, more \ngeneral allocator. When deallocating an objec~ we must determine which allocator is responsible for that \nobject. The implementation of QUICKFIT we considered uses boundary tags to indicate the allocator responsible \nfor the object. Boundary tags pollute the cache; the information in the bound\u00adary tag is only useful \nto the allocator, and Figure 1 indicates that a properly designed allocator should contribute only a \nsmall frac\u00adtion to the total execution time. With boundary tags, the memory before and after each allocated \nobject records the size of the ob\u00adject and indicates whether that object is currently allocated or free, \nThis information will typically be brought into the cache when the object is referenced. This wasted \nspace increases the cache miss rate by reducing the effectiveness of prefetching; boundary tags, rather \nthan useful data, may be prefetched, Our measurements in related work [30] confirm previous observations \nthat programs tend to allocate many small objects; we found that 24 bytes was a very common allocation \nrequest size. Common boundary tag implemen\u00adtations add eight bytes of overhead information to each allocated \nobject; thus, w25r70of the cache may hold information useful only to the memory allocation subroutines. \nAs mentioned in j2.1, the GNU LOCAL allocator does not use boundary tags. Not only does this reduce the \ntotal memory needed because less memory is devoted to boundary tags, it also slightly improves the reference \nlocality. Table 6 shows the cache miss rates for a modified version of the GNU LOCAL allocator that allocates \nan additional eight bytes of data for each object. This extra space emulatestheeffectofcachepollution \nbytheboundary tagswithout otherwise influencing the DSA implementation, Boundary tags increase total \nexecution time by 0.lYo-1.170, and the contribution would increase as cache miss penalties increase. \n 4.4 An Architecture for Efficient M emery Allocation We can use the results of our experimentation \nto guide the design of efficient DSA implementations by discarding inefficient designsand identifying \nthe most important characteristics present that provide efficiency. The inefficient design elements are: \nSearch &#38; Coalescing: Algorithms that search for free space, such asFIRSTFITandGNUG++aregenerally \nslowerthanotheral\u00adgorithms and have poor reference locality. Fortunately, many programs do not need these \ngeneral allocators for every al\u00adlocation; most programs have patterns of behavior that can be handled \nin more efficient ways. However, these algo\u00adrithms are needed to allocate infrequently allocated objects \nor objects that deviate from the normal program behavior. Explicit Cache Management: Our measurements \nshow that the GNU LOCALallocator, which was carefully crafted to provide good cache locality, did not \nhave miss rates significantly lower that the BSD or QuIcKFrT algorithms, Even though the miss rates of \nthe GNU LOCAL allocator were sometimes marginally lower that the BSD or QUICKFIT allocators, the added \nCPU overhead in the GNU LOCAL allocator resulted in longer total execution times based on cache miss \npenalties associatedwith existing computer architectures. In the future, if cache miss penalties increase \ndramatically, the added CPU overhead required to obtain the marginal increase in locality may then be \nwarranted. Our measurements have alao shown that techniques intended primarily to decreaseexecution time \nalao result in increased refer\u00adence locality. In particular, segregated-storage allocators such as BSD \nand QuICKFrTsolve two problems simultaneously: they allow very rapid allocation and deallocation and \nat the same time they promote rapid object re-use leading to higher reference locality. A central aspect \nof the design of segregated-storage algorithms is the choice of object sizeshandled by the fast freeliats. \nAlgorithms can choose to merge many object sizes together (e.g., rounding up to apower of two in the \nBSD allocator) handling the entire classof sizes with a single freelist, or algorithms can handle each \ndistinct size request with a different freelist. Merging sizes enhances rapid objectre-usebutwastesstoragespaceduetointernal \nfragmentation. The BSD algorithm is an example of an allocator that shows good re-use but excessive fragmentation. \nThe alternative, using many distinct size freeliats, reduces object re-use but eliminates internal fragmentation \nproblems, The best allocator strikes a balance between too few and too many size classes, The choice \nof these size classes can be based on several approaches. Firat, anecdotal evidence about the best choice \nof size classescan be used and that kind of evidence was the basis for the QUICKFIT implementation that \nwe measured. Second, size classescan be chosen in such a way that the amount of internal fragmentation \nisbounded (e.g.,if25% orlessinternal fragmentation is tolerated, then objects of size 12-16 bytes are \nrounded to 16 bytes) [5]. Finally, we advocate basing the choice of size classes on empirical measuremenfi \nof a particular program s behavior. In previous work [8], we have shown that allocator customization \nresults in very fast allocators. Such customized allocators could also be designed to promote the most \neffective object re-use, leading to enhanced cache locality, Implementing non-uniform size-merging operations \nrequires al\u00adlocators to implement an arbitrary mapping between the object re\u00adquest size and the associated \nsize class size. One reason for the crude powers-of-two mapping used in the BSD algorithm is that it \nis easy to compute. However, arbitrary mappings can be im\u00adplemented efficiently using a size-mapping \narray, as illustrated in Figure 9. With such an array, size requesta can be rounded-up to arbitrary sizes. \nOne way to reduce the space overhead and increase the refer\u00adence locality of programs is to eliminate \nthe per-object boundary tag information for objects needed in the BSD and QUICKFIT im\u00adplementations. \nWhile the GNU LOCAL implementation does elimi\u00adnate per-object tags, measurements in Table 6 show that \nthe cache performance improvement associatedwith eliminating even 8-byte boundary tags is quite small. \nWe conclude that boundary-tag elim\u00adination hasmixed performance advantageson current architectures and \nia not warranted if the elimination increases the cost of alloca\u00adtion and deallocation significantly. \n5 Conclusions In this paper, we investigated the effect of dynamic storage alloca\u00adtion on program reference \nlocality. We conclusively showed that the choice of DSA algorithm has a strong impact on program reference \nlocality, This impact can significantly reduce program performance in modern computer architectures. \nUsing trace-driven simulation, we measured the cachemiss rates and page fault rates for a broad range \nof cache and memory sizes in five allocation-intensive programs. We conclude that allocators based on \nsequential-fit methods, such as first-fit, best-fit, etc, have poor reference locality. Even though these \nalgorithms reduce the total memory requirements of programs, this reduction does not result in increased \nreference locality. We conclude that efforts to reduce total memory utilization in DSA implementations, \nsuch as coalescing adjacent free blocks, will in most casesboth increase total execution time and reduce \nprogram reference locality. Our measurements show that the most CPUefficient allocators,suchasBSDorQUICKFIT,alsoprovidethe \nbest locality of reference. Locality in these algorithms is enhanced because they are designed on the \nprinciple that programs allocate objects with a small number of distinct sizes, and the allocators rapidly \nrecycle free objec~ of those sizes. 5.1 Future Work We are extending our previous work in synthesized \nallocators using the information we gained in this study. Ideally, ourgeneral-purpose allocator will \nwork well for many programs and can be improved further using customization. We alao hope to include \nother work in program behavior prediction based on call site information [2] in the synthesized allocators. \n6 Acknowledgements This material is based upon work supported by the National Science Foundation under \nGrants No. CCR-901O624, CCR-9121269 and CDA-892251O. We would like to thank James Larus for develop\u00adment \nof the QPutilit y, which greatly simplified our experimentation, and Doug Lea, Mike Haertel and Mark \nMoraes for the use of and information about their allocators, We also thank the reviewers for their comments. \nReferences [1] Thomas Ball and James R. Larus, Optimally profiling and tracing programs. In Conference \nRecord of the Nineteenth ACM Symposium on Principles of Programming Languages, pages 59-70, January 1992. \n[2] David Barrett and Benjamin Zorn. Using lifetime predictors to improve memory allocation performance, \nIn SIGPLAN 93 Conference on Programming Language Design and Imple\u00admentation, Albuquerque, June 1993. \n[3] Gerald Bozman. The software lookasizebuffer reducessearch overhead with linked lists. Communications \nof the ACM, 27(3):222-227, March 1984. [4] David Callahan, Ken Kennefy, andAllan Porterfield. Software \nprefetching. In Fourth Intl Con{ on Arch. Support for Pro\u00adgramming Languages and Operating Systems, pages \n40-52, Ap~ 1991. [5] John DeTreville. Heap usagein the Topaz environment. Tech\u00adnical Report 63, Digital \nEquipment Corporation System Re\u00adsearch Center, Palo Alto, CA, August 1990. [6] Digital Equipment Corporation. \nUnixManualPagefor PIXIE, UL~IX V4.2 (rev 96) edition, September 1991. [7] A. J. Goldberg and J, Hennessy. \nPerformance debu~ing shared memory multiprocessor programs with MTOOL. In Proceedings Supercomputing \n91, pages 481491,1991. [8] Dirk Grunwald and Benjamin Zom. Cus mhW.lOc:Efficient Synthesized Memory Allocators. \nTechnical Report CS-CS\u00ad602-92, Department of Computer Science, University of Col\u00adorado, Boulder, Boulder, \nCO, July 1992. [9] Mike Haertel. Description of GNU malloc implementation. Personal communication, August \n1991. [10] Mark D. Hill, TYCHO. Univemity of Wisconsin, Madison, WI. Unix manual page. [11] Norman P. \nJouppi, Improving direct-mapped cache perfor\u00admance by the addition of a small fully-associative cache \nand prefetch buffers. In Proceedings of the Seventeenth Annual International Symposium on Computer Architecture, \npages 364-373, Seattle, WA, May 1990, [12] Chris Kingsley. Description of a very fast storage alloca\u00adtor. \nDocumentation of 4.2 BSD Unix malloc implementation, February 1982. [13] Donald E. Knuth. FundamentalAlgorithms, \nvolume 1 of The Art of Computer Programming, chapter 2, pages 435-451. Addison Wesley, Reading, ~ 2nd \nedition, 1973. [14] David G. Kom and Klem-Phong Vo. In search of a better mal-Ioc. In Proceedings of \nthe Summer 1985 USENIXConference, pages 489-506,1985. [15] M.S. Lam, P.W, Wllaon, and T, G.Moher, Object \ntype directed garbage collection to improve locality. In Proceedings of the International Workshop on \nMemory Management, St. Male, FRANCE, September 1992. Springer Verlag. [16] Doug Lea. An efficient first-fit \nmemory allocator. (From comments in source and personal communication). [17] Alvin R. Lebeck and David \nA. Wood. CPROF: A cache per\u00adformance profiler. Technical report, Computer SciencesDept., Univ. of Wisconsin-Madison, \nJuly 1992. [18] Henry Lieberman and Carl Hewitt. A real-time garbage col\u00adlector based on the lifetimes \nof objects. Communications of the ACM, 26(6):419-429, June 1983. [19] Jeffrey C. Mogul and Anits Borg. \nThe effect of context switches on cache performance. In Proceedings of the Fourth International Conference \non Architectural Support for Pro\u00adgramming Languages and Operating Systems @iSPLOS-~, pages 75-84, Santa \nClara, CA, April 1991. [20] David A. Moon. Garbage collection in a large Lisp system. In Conference Record \nof the 1984 ACM Symposium on LISP and Functional Programming, pages 235-246, Austin, Texas, August 1984. \n [21] A,J. Smith. Line (block) size choice for CPU cache mem\u00adories. IEEE Transactions on Computers, 36(9)1063-1075, \nSeptember 1987. [22] Thomas Standish. Data Structures Techniques. Addison-Wesley Publishing Company, \n1980. [23] David Ungar. Generation scavenging: A non-disruptive high performance storage reclamation \nalgorithm. In SZG\u00adSOFT/SIGPLANPractical Programming Environments Con. ference,pages 157-167, April 1984. \n[24] Charles B. Weinstock and William A. Wulf. Quickfit: An efficient algorithm for heap storage allocation.ACMSIGPLAN \nNotices, 23(10)141 144, October 1988. [25] Paul R. Wilson, Michael S. Lam, and Thomas G. Moher. Caching \nconsiderations for generation garbage collection. In Proceedings of the 1992 ACM Conference on LISP andFunc\u00adtional \nProgramming, pages 32-42, San Francisco, CA, June 1992, ACM. [26] Paul R. Wilson, M.S. Lam, and T.G. \nMoher. Effective static graph reorganization to improve locality in garbage-collected systems. In Proceedings \nof the ACM SIGPLAN 91 Confer\u00adence on Programming Language Design and Implementation, volume 26, pages \n177-191, Toronto, Ontario, Canada, June 1991. [27] Benjamin Zorn. Comparative Performance Evaluation \nof Garbage Collection Algorithms. PhD thesis, University of California at Berkeley, Berkeley, CA, November \n1989. Also appears as tech report UCB/CSD 89/544. [28] Benjamin Zom. The effect of garbage collection \non cacheper\u00adformance. Technical Report CU-CS-528-91, Department of Computer Science, University of Colorado, \nBoulder, Boulder, CO, May 1991. [29] Benjamin Zom and Dirk Grunwald, Empirical measurements of six allocation-intensive \nC programs. SIGPLAN Notices, 27(12):71-80, December 1992. [30] Benjamin Zom and Dirk Grunwald, Evaluating \nmodels of memory allocation. Technical Report CU-CS-603-92, Depart\u00adment of Computer Science, University \nof Colorado, Boulder, Boulder, CO, July 1992.   \n\t\t\t", "proc_id": "155090", "abstract": "<p>The allocation and disposal of memory is a ubiquitous operation in most programs. Rarely do programmers concern themselves with details of memory allocators; most assume that memory allocators provided by the system perform well. This paper presents a performance evaluation of the reference locality of dynamic storage allocation algorithms based on trace-driven simualtion of five large allocation-intensive C programs. In this paper, we show how the design of a memory allocator can significantly affect the reference locality for various applications. Our measurements show that poor locality in sequential-fit allocation algorithms reduces program performance, both by increasing paging and cache miss rates. While increased paging can be debilitating on any architecture, cache misses rates are also important for modern computer architectures. We show that algorithms attempting to be space-efficient by coalescing adjacent free objects show poor reference locality, possibly negating the benefits of space efficiency. At the other extreme, algorithms can expend considerable effort to increase reference locality yet gain little in total execution performance. Our measurements suggest an allocator design that is both very fast and has good locality of reference.</p>", "authors": [{"name": "Dirk Grunwald", "author_profile_id": "81100218381", "affiliation": "", "person_id": "PP15026204", "email_address": "", "orcid_id": ""}, {"name": "Benjamin Zorn", "author_profile_id": "81100190820", "affiliation": "", "person_id": "PP40024661", "email_address": "", "orcid_id": ""}, {"name": "Robert Henderson", "author_profile_id": "81332503747", "affiliation": "", "person_id": "PP31079303", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/155090.155107", "year": "1993", "article_id": "155107", "conference": "PLDI", "title": "Improving the cache locality of memory allocation", "url": "http://dl.acm.org/citation.cfm?id=155107"}