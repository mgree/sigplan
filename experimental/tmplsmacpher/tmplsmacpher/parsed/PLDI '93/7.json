{"article_publication_date": "06-01-1993", "fulltext": "\n Dependence-Based Program Analysis Richard Johnson Keshav Pingali Department of Computer Science Cornell \nUniversi@, Ithaca, NY 14853  Abstract Program analysis and optimization can be speeded up through the \nuse of the dependence flow graph (DFG), a represen\u00adtation of program dependence which generalizes clef-use \nchains and static single assignment (SSA) form. In this pa\u00adper, we give a simple graph-theoretic description \nof the DFG and show how the DFG for a program can be constructed in O(EV) time. We then show how forward \nand back\u00adward dataflow analyses can be performed efficiently on the DFG, using constant propagation and \nelimination of partial redundancies as examples. These analyses can be framed as solutions of dataflow \nequations in the DFG. Our construction algorithm is of independent interest because it can be used to \nconstruct a program s control dependence graph in O(E) time and its SSA representation in O(EV) time, \nwhich are improvements over existing algorithms. Introduction A number of recent papers have focused \nattention on the prob\u00adlem of speeding up program optimization [FOW87, BM090, CCF91, PBJ+ 91, CFR+91, \nDRZ92]. Most optimization al\u00adgorithms are based on dataflow analysis. Classic examples are Kildall s \nconstant propagation algorithm [Ki173], and Morel and Renvoise s algorithm for elimination of partial \nredundancies [MR79]. These algorithms are usually ~mple\u00admented using vectors of boolean, integer or real \nvalues to represent sets of assertions, such as x is 5 here, or y+ z is available here. One vector is \nassociated with each point in the control flow graph and initialized appropriately. Vector values are \ncomputed iteratively by propagating information 1This research was supported by an NSF presidential Young \nliwestigator award CCR-8958543, NSF grant CCR-90138526, ONR grant NOOO14-93-1\u00ad0103, end a grant from \nHewlett-Packard Corporation. Permission to copy without fee all or part of this material is granted \nprovided that the copias are not made or distributed for direct commercial advantage, the ACM copyright \nnotice and the title of the publication and its date appear, and notice is given that copying is by permission \nof the Association for Computing Machinery. To copy otherwise, or to republish, requires a fee and/or \nspecific permission. ACM-S IGPLAN-PLDl-6/93 /Albuquerque, N.M. @ 1993 ACM 0-89791 -598 -4/93 /0006 /0078... \n$ 1.50 from the inputs of statements to their outputs in the case of forward analysis, and from outputs \nto inputs in the case of backward analysis. The analysis terminates when all state\u00adments have a consistent \nset of input and output assertions. Although easy to implement, this approach has a number of disadvantages. \n Information is propagated throughout the control flow graph, not just to where it is needed for optimization. \nFor example, inconstant propagation it suffices to prop\u00adagate information from definitions of variables \nto their uses. In common subexpression elimination, it is un\u00adnecessary to propagate availability of an \nexpression to points where the variables of the expression are dead,  When the vector at some point \nin the program is updated, the entire control flow graph below that point (or above it, in backward analysis) \nmay be re-amdyzed, even if there are few points in that region affected by the update.  Many optimization \nbenefit from analysis performed in stages, but this is difficult to do in the standard approach. Consider \nredundancy elimination in the following pro\u00adgram. To deduce that the computation of y is redun\u00addant, \nwe must first deduce that the computation of w is redundant. This kind of analysis in stages is contrary \nto the standard approach, which considers all assertions simultaneously.  ... z=a+b w=a+b ... X=z+l \ny=w+l ... Def-use chains provide a partial solution to these prob\u00adlems. They permit information to flow \ndirectly between defi\u00adnitions and uses without going through unrelated statements. However, clef-use \nchains suffer from three drawbacks. First, clef-use chains cannot be used for backward dataflow prob\u00adlems, \nsuch as the elimination of redundant computations, because they do not incorporate sufficient information \nabout the control structure of the program. Second, this lack of control flow information in clef-use \nchains affects the preci\u00adsion of analysis even in forward dataflow problems such as constant propagation \n[WZ85, PBJ+ 91 ]. Finally, the worst\u00adcase size of clef-use chains is O(E2 V) where E is the number of \nedges in the control flow graph and V is the number of variables [RT81]. The size problem can be overcome \nby using a factored representation of clef-use chains called static single assign\u00adment (SSA) form, which \nhas worst-case asymptotic size O(EV) [CFR+89, CFR+91]. However, SSA form camnot be used for backward \ndataflow problems. A generalization of the SSA approach, called the sparse datapow evaluation graph, \nhas been proposed to address this problem, but sparse graphs take 0(IV3) time to construct, where N is \nthe number of nodes in the control flow graph [CCF91, DRZ92]. In this paper, we show how these problems \ncan be solved using the dependenceflow graph (DFG), which can be viewed as a generalization of clef-use \nchains and SSA form. The DFG was suggested to us by the work of Cartwright and Felleison who showed the \nadvantages of an executable representation of program dependence [CF89]. We previously introduced the \nDFG using a dataflow machine style operational seman\u00adtics [PBJ+ 91, Bec92]. Dataflow ~chlne graphs are \nalso the basis of the program dependence web (PDW) of Ballance, McCabe and Ottenstein [BM090], as well \nas the ori@d SSA graphs due to Shapiro and Saint [SS70]. However, our experience in implementing and \nusing a representation based on these ideas is that a full-blown dataflow graph representa\u00adtion is neither \nnecessary nor desirable. The main contribution of this paper is the distillation and incorporation of \nthe essence of the dataflow graph represen\u00adtation into a traditional optimizing compiler framework. We \naccomplish this as follows. In Section 2, we give a simple graph-theoretic charac\u00adterization of dependence \nflow graphs. This characterization permits the exorcism of the dataflow execution model from the description \nof DFGs. It also brings out key connections between our work and prior work on representing control and \ndata dependence. In Section 3, we describe how to construct DFGs. An important step in this construction \nis determining when two nodes in a control flow graph have the same control depen\u00addence. We describe \nhow to do this in O(E) time. This algorithm is of independent interest since it can be used to build \na program s control dependence graph in O(E) time and its SSA representation in O ( EV) time, which are \nim\u00adprovements over existing algorithms [CFS90, CFR+89],, In Section 4, we show how to use the DFG in \na forward dataflow problenx constant propagation with dead code elim\u00adination. This algorithm is faster \nthan the standard control flow graph algorithm, yet it does as good a job of optimizing programs. In \nSection 5, we show how to solve a backward dataOow problenx anticipatability of expressions, which is \nan impor\u00adtant step in the elimination of partial redundancies [MR 79]. Finally, in Section 6, we describe \nwhat we have learned so far in our implementation. 2 A graph-theoretic characterization of dependence \nflow graphs We give a graph-theoretic characterization of clef-use chains, static single assignment form, \nand the dependence flow graph. This characterization formalizes the relationship be\u00adtween these program \nrepresentations, permits the design of an efficient construction algorithm for DFGs, and aids in proving \noptimization algorithms correct. 2.1 Terminology Definition 1 A control graph with distinguished all \nnodes are reachable Row nodes from graph start start (CFG) and and all is a directed end such that nodes \nhave a path toe nd. start is the only node with no predecessors, and end is the only node with no successors. \nFor convenience in describing algorithms that operate on CFGS, we introduce explicit switch and merge \nnodes to sepa\u00adrate branching and merging of control flow from computation. A switch node is essentially \na conditional jump that redkects control flow to one of multiple outgoing edges based on the value of \nan expression computed within the node, A merge node performs no computation but simply serves as the \ntarget of multiple control flow edges, An assignment statement node performs any general, non-branching \ncomputation, It is useful to extend the standard notions of dominance, postdominance and control dependence \nso that they apply to edges as well as to nodes in the CFG. Definition 2 A node or edge x is said to \ndominate node or edge y in a directed graph if every path from start to y includes x. A node or edge \nx is said to postdominate node or edge y in a directed graph if every path from y to end includes x. \nA node or edge x is said to be control dependent on node n if x postdominates all edges on some path \nfrom n to x, but x does not postdominate n. (Intuitively, n is a conditional branch that determines if \ncontrol will pass through x.) 2.2 Def-use chains We begin by recalling the standard definition of clef-use \nchains, Definition 3 A definition of variable x is said to reach a use of x if there is a control jlow \npath from the definition to the use that does not pass through any other definition of x. A clef-use \nchain for variable x is a node pair (nl, n2) such that nl dejines x, n2 uses x, and the definition of \nx at n 1 reaches the use of x at nz. For our purpose, it is convenient to recast this in terms of control \nflow edges rather than nodes. ...........* ------* control flow dependence forx ~ dependence for y \ns ,,;&#38;$ ,:=i~) !/ ! , ;, :, / ;, ~~Y. Y.. L x x., &#38; H (a) CFG with clef-use chains (b) SSA \nFigure 1: A Comparison of Definition 4 A clef-use chain for variable x is an edge pair (e~, e~) such \nthat 1. the source of el dejines x, 2. the destination of e2 uses x, and 3. there is a control$ow path \nfrom el to e2 with no assign\u00ad  ment to x. Figure 1(a) shows a control flow graph with clef-use chains. \nThe limitations of clef-use chains have been discussed exten\u00adsively in the literature [WZ85, PBJ+ 91 \n], and we summarize them here. Consider the problem of constant propagation using clef-use chains: the \nstandard algorithm replaces a use of a variable with a constant if the right hand side of every definition \nreaching that use is that constant [ASU86]. In Fig\u00adure 1(a), this algorithm determines that the use of \nx in the conditional branch can be replaced by the constant 1, and this fact is determined without propagating \nthe vahre of x through the assignment to y that is between the definition and use of x. Similarly, it \ndetermines that the right hand side of the statement y: = y+ 1 can be replaced by the constant 3. However, \nthis algorithm cannot determine that the last use of y in this program can be replaced by the constant \n3, since there are two clef-use edges carrying different constants that reach this use. By contrast, \nthe standard dataflow analysis algorithm for constant propagation on the control flow graph will find \nthis constant, since it deduces correctly that the false side of the conditional branch is dead2. z~~ls \nof ~ls algotiti aredkcussed in Section 4. :~b) . ~.. \u00ad / Y. ~ / ,.X. d form (c) DFG form Program Representations \nTo summarize, algorithms using clef-use chains can produce less optimized code than algorithms performing \ndataflow analysis directly on the control flow graph. In addi\u00adtion, clef-use chains cannot be used for \nbackward dataflow analysis, and the worst-case size of clef-use chains is 0(E2V) [RT81], which is rather \nlarge.  2.3 Static single assignment form Static single assignment form solves the size problem of def\u00aduse \nchains by introducing a so-called ~-function to com\u00adbine clef-use edges having the same destination [CFR+ \n89, CFR+91]. In an SSA representation, each use of a variable is reached by exactly one definition or \n~-function. Figure 1(b) shows the SSA form for the previous example. Notice that the clef-use edges for \nvariable y are combined by a d-function placed at the merge in the control flow graph. SSA edges have \nthe following graph-theoretic characterization. Definition edge pair 1. there 2. there 3. there from \n 5 An SSA edge for variable (el, e2) such that exists a definition exists a use of x is no assignment \nel to e2) and 4. el dominates e2. The first two conditions two points on some path of x that reachable \nto x on x corresponds to an reaches el, from e2, any control pow path assert that an SSA edge connects \nfrom a definition of x to a use of x reached by that definition. Conditions 3 and 4 ensure that the \nonly definitions that reach ez are those that reach el; otherwise there would be @functions between el \nand ez and there would be no SSA edge from e1 directly to ez. The worst-case size of the SSA representation \nis O(EV). This solves the size problem of clef-use chains, but the SSA form cannot be used directly in \nbackward dataflow ana~ysis problems. 2.4 Dependence flow graph Figure 1(c) shows the dependence flow \n~aph for the running example. Unlike clef-use chains, which go directly from definitions to uses, a DFG \nedge for a variable x can bypass a region of the control flow graph only if this region is a single\u00adentry \nsingle-exit region that does not contain an assignment to x, since such a region has neither data nor \ncontrol informal.ion that is of interest to program analysis. The following theorem defines single-entry \nsingle-exit regions formally, and states an important connection between control dependence and DFGs. \nTheorem 1 The following are equivalent. el and e2 enclose a single-entry single-exit region.  el dominates \ne2, ez postdominates e~, and every cycle containing et also contains e2 and vice versa.  e1 and ez have \nthe same control dependence.  For lack of space, we omit the proof of this theorem. A related structure \ncalled a hammock has been discussed in the literature [Kas75]. Hammocks are not the same as sin,gle\u00ad \nentry single-exit regions since the exit node in a hammock can be the target of edges outside the hammock; \nbesides, the algorithm for finding hammocks is O (EN). Just as clef-use chains are intercepted by ~-functions \nin the SSA representation, they are intercepted by swit ch and merge operators in the DFG. DFG edges \ncan be character\u00adized as follows: Definition 6 A DFG edge for variable x corresponds to an edge pair \n(e 1, ez) such that 1. there exists a definition of x that reaches el, 2. there exists a use of x reachable \nfrom ez, 3. there is no assignment to x on any control flow path from e~ to ez, 4. el dominates ez, \n 5. ez postdominates el, and 6. every cycle containing el also contains ez and vice versa.  Conditions \n1 through 4 are the same as in the SSA rep\u00adresentation. In the DFG, the merge operator plays the same \nrole as d-functions do in SSA form. Conditions 4 through 6 formally specify that the region of the control \nflow graphbe\u00adtween el and ez must be a single-entry single-exit region. For example, in F@ure 1(c) the \nregion of the control flow graph 81 between the assignment to x and the use of x is a single-entry single-exit \nregion containing no definition of x. Thus there is a dependence edge from the definition of x directly \nto the use of x. However, this region contains a definition of y, so dependence edges for y cannot bypass \nthis region; they are intercepted by a switch operator at the conditional branch. In this way, dependence \nin the DFG are intercepted by merges and switches at merge points and branches respectively in the control \nflow graph.  3 Constructing DFGs We now describe the construction algorithm for depen\u00addence flow graphs. \nWe lint outline our algorithm for identi\u00adfying single-entry single-exit regions and then show how this \ninformation is used to build the DFG. 3.1 Finding single-entry single-exit regions Abstractly, decomposing \na control flow graph into single\u00adentry single-exit regions can be viewed as providing a parse tree of \nthe control flow structure. In structured pro\u00adgrams, syntactic constructs such as if-the n-e 1 se and \nwhile loops provide information for determining single\u00adentry single-exit regions. For general control \nflow graphs, however, we need an efficient algorithm for discovering this information. Consider any two \nsingle-entry single-exit regions. It is easy to show that if they overlap, then either one is nested \nwithin the other, or the intersection is itself a single-entry single-exit region. If we only consider \nregions that are not formed by sequentially composing smaller regions, then we can show that single-entry \nsingle-exit regions are pairwise either nested, disjoint, or sequentially ordered. Thus these regions \ngive a hierarchical decomposition of a control flow graph s structure. For lack of space, we sketch our \nO(E) algorithm for find\u00ading single-entry single-exit regions. A longer paper giving the details of this \nalgorithm can be obtained from the au\u00adthors. Given a control flow graph, we want to find sets of edges \nhaving the same control dependence. Such edges are totally ordered, and each pair of consecutive edges \nare the entry/exit of a single-entry single-exit region. To find sets of edges having the same control \ndependence, note that we can insert a dummy node on each edge and then compute the property for nodes3. \nWe then reduce the problem to one of finding sets of cycle equivalent nodes in a related graph. Definition \n7 Control~ow nodes a and b are said to be cycle equivalent if every cycle containing a also contains \nb and vice versa. 3Adding E nodes does not change the O(E) time complexity of our algorithm. ........... \n control flow E  g - e n&#38; y,=2 (a) example CFG (b) base-level Figure 2: An Illustration Claim 1 \nNodes a and b have the same control dependence if and only if a and b are cycle equivalent in the strongly \nconnected component formed by adding the edge e nci --+ staick to the CFG. The proof is straightforward \nusing Theorem 1. Comput\u00ading cycle equivalence in directed graphs appears difficult, but we further reduce \nthe problem to that of finding cycle equivalence in a related undirected graph. Claim 2 Nodes a and b \nare cycle equivalent in a strongly connected component S if and only if the corresponding nodes a and \nb are cycle equivalent in the undirected graph G formedfrom S as follows: expand S into S by splitting \neach node n into nodes ni, nt and no such that inedges to n connect to ni, out\u00adedges from n originate \nfrom nO, and there are directed edges n~~ n and n! ~ no. e undirect all edges in S to form G. Note that \nany cycle in S has a corresponding cycle in G, and although G contains cycles not in S, these cycles \ndo not af%xt the cycle equivalence relation on nodes in G that correspond to nodes in S. Our algorithm \nfor finding undi\u00adrected cycle equivalence is based on depth-first search and runs in O(E) tim~ the details \nare omitted. In a companion paper, we show that this algorithm can be used to construct the factored \ncontrol dependence graph of a program in O(E) time. A surprising aspect of this algorithm is that it \ndoes not DFG (c) after region bypassing and dead edge removal of DFG Construction require the computation \nof the dominator or postdominator relations.  3.2 The DFG construction algorithm Suppose single-entry \nsingle-exit regions are discovered by the algorithm sketched above. The following steps outline the DFG \nconstruction algorithm and are illustrated in Fig\u00adures 2(b)-(c). In the example, each assignment statement \nis a single-entry single-exit region, as is the i f t he n e 1s e construct. 1. Determine the variables \ndejined within each single\u00adentry single-exit region. This is accomplished by an inside-out traversal \nof the regions. This information will be used in a later step to allow dependence flow paths to bypass \nregions not relevant to the dependence path. In our example, each assignment statement defines one variable, \nand the i f then el se defines y. 2. Create a base-level DFG with no region bypassing. Simply insert \nV dependence edges in parallel with each control flow edge. Figure 2(b) shows the base-level DFG. 3. \nPerform region bypassing using the information found in Step 1. Use a forward flow algorithm that maintains \nthe most recent dependence source for each variable. When a region is bypassed, some dependence are cut. \n 4. Remove dead flow edges generated by bypassing. Use a backward propagation starting from edges cut \nduring  the region bypassing. Figure 2(c) shows the graph after region bypassing and dead edge removal. \n 3.3 Discussion Multiedges: Due to region bypassing, it is often the case that many DFG edges for a \nsingle variable share a com\u00admon source. For example, in Figure 2(c) two dependence edges start at the \nassignment x: =1. We find it convenient to refer to such a collection as a multiedge. We refer to the \nsource of a multiedge as its tail, and its successors as heads. We will use the term edge to refer to \nthe tail and a piurtic\u00adular head of a multiedge. From Theorem 1, it follows that the tail and all the \nheads of a multiedge are totally ordwed by dominance/postdominance, In the following sections on dataflow \nanalysis using DFGs, our use of multiedges allows us to separate the flow of information between a node \nand its dependence flow successors into two parts: propagation be\u00adtween anode and a multiedge tail, and \npropagation between a multiedge tail and its multiple heads. Control edges It is convenient to ensure \nthat the DFG is connected and rooted at start. To do this, we introduce a dummy variable defined at st \nart and used in each statement that has no other variables on its right hand side. Note that these additional \nedges are simply control edges indicating a node s control dependence region. In F@ure 2(c), the dependence \nfor this dummy variable are indicated by a solid arc with a circle. Region Bypassing: Bypassing single-entry \nsingle-exit re\u00adgions of the control flow graph is useful because it speeds up optimization. However, \nthe DFG-based optimization al\u00adgorithms described in this paperwork correctly even if some or no bypassing \nat all is performed. Abstractly, this means that any equivalence relation on CFG edges that is finer \nthan control dependence equivalence can be used to construct the DFG. For example, we can use a relation \nin which two edges are equivalent if and only if they are in the same basic b] ock this will permit bypassing \nof assignment statements but not of control structures. Constructing the SSA Representation: If the SSA \nrep\u00adresentation of a program is desired, we can construct it in O(EV) time by first building the DFG \nrepresentation and then eliding switches and converting merges to d-functions. Unlike the standard algorithm \n[CFR+ 89], our algorithm does not require computation of the dominance relation or dmni\u00adnance frontiers \nand is therefore much simpler to implement.  4 Forward dataflow analysis In this section, we present \nconstant propagation as an example of forward dataflow analysis using the DFG. Consider Figure 3(a). \nThe first use of z can be repla~ced by 1 and the second by 2. The right hand sides of the two definitions \nof x can now be simplified to the constant 3, and the final use of x can be replaced by 3. Most constant \npropagation algorithms in the literature, such as the clef-use chain algofithm [ASU86], discover such \nall-paths constants. However, additional constants may be found if we ignore definitions inside dead \nregions of code. In Figure 3(b), the predicate of the conditional can be determined to be constant. By \nignoring the definition on the unexecuted branch, the use of x in the last statement can be determined \nto have value 1. Suchpossible-paths constants are common in code generated horn irdine expansion of procedures \nor macros [WZ85], but algorithms that use clef-use chains alone do not find these if (p) {Z:=l; x else \n{2:=2; x Y :=x then := := 2+2 2+1 } } P := true if (p) then {X:=l} else {x:=2} Y-x .= (a) all-paths (b) \npossible-paths Figure 3: Exmples of Constant Propagation constants. We will discuss the standard CFG \nalgorithm, which solves a set of dataflow equations in the control flow graph, and the DFG algorithm, \nwhich solves a set of equations in the dependence flow graph. Both algorithms find all-paths and possible-paths \nconstants, but the DFG algorithm is asymp\u00adtotically faster by a factor of O(V). We use Kildall s framework \nfor constant propaga\u00adtion [Ki173]. Define a lattice consisting of all constant values and two distinguished \nvalues T and 1. Uses of variables are assigned values ffom the lattice during constant propagation. Initially, \neach use is mapped to J_, meaning that we have no information yet about the values of the variable at \nruntime. A use is mapped to T when the algorithm cannot determine that the use is a constant (e.g. if \nthe use is reached by two definitions whose right hand sides are 3 and 4.) At the end of constant propagation, \nthe interpretation of the lattice value assigned to a use of a variable x is as follows: 1-This use was \nnever examined during constant propaga\u00ad tion; it is dead code. c This use of x has the value c in all \nexecutions. T This use of x may have different values in different executions. 4.1 The CFG algorithm \nAt each edge, we maintain a vector of lattice values having an entry for each variable. Intuitively, \nthese vectors summarize the possible values of variables at each program point. These vectors are initialized \nto UL, the vector with -1-in every entry, and they are updated monotonically as the algorithm start \nassignment multiedge switch merge m :%! h , ,#?. /+  ?( 2 A Vi B c t f c ifp[aA] =true Vp=T ~B= \nA OL otherwise { UA = Ul_ aB=aA[XH e{UA}] UC= UAUUB ifP[~A]=fa19e V p= T ~c= A OJ. otherwise { (a) \ncontrol flow graph dataflowequations I! I v if Vp= true VVp=T Vt = -1 otherwise { Vi=T V. = e{ Vi} \nVi=V V= VIHV2 V if Vp= false V Vp=T Vf = L otherwise { (b) dependence flow graph dataflow equations \nFigure 4: Dataflow Equations for Constant Propagation proceeds. 4.2 The DFG algorithm Computing these \nvectors can be viewed as solvinga system The DFG algorithm propagates values for each variable sep\u00adof \ndataflow equations. One equation describing the output arately along edges in the DFG. The set of DFG \ndataflowvectors(s) in terms of the input vector(s) is associated with equations has one equation for \neach DFG edge. As discussed each node. Figure 4(a) shows the equation scheme for con\u00adin Section 2, a \nDFG edge d can be viewed as representing stant propagation on the control flow graph. In this figure, \na pair of CFG edges (e 1, ez), and the dataflow equation for 0 represents the vector of lattice values \nstored at edge A. d computes the lattice value of the associated variable in Since the values of variables \nat start are unknown, we set the region between el and e . Figure 4(b) shows the equa\u00ad the vector ats \ntart to T for all variables. The output vector tion scheme for solving constant propagation using the \nDFG. of an assignment statement x: =e is obtained by evaluating These equations are similar to the ones \nfor the control flow expression e using the values available at the statement input, graph algoritluw \nthe only new feature is the rule that prop\u00ad and updating the x entry of the output vector to reflect \nthis agates the value at the tail of a DFG multiedge to its heads. value. Expression e evaluates to L \n(or T) if any operand of For example, in the program of Figure 1(c), this rule can be e is 1 (or T) \nin the input vector. At a switch, the output used to propagate the value of x from the assignment x: \n=1 vector is identical to the input vector for each direction that to the two uses of x in the program. \ncontrol flow can take; the output vector is UL for directions As with the control flow algorithxq a simple \nworklist\u00ad that control flow cannot possibly take. At a merge, the based algorithm can be used to solve \nthe dataflow equations. output vector is simply the least upper bound of the input Whereas the control \nflow algorithm performed O(V) work vectors. each time a node is processed, the DFG algorithm performs \nThese equations can be solved using a standard worklist work only for the relevant dependence at each \nnode. There\u00ad algorithm. Unfortunately, the asymptotic complexity of this fore, the asymptotic complexity \nof the DFG algorithm is algorithm is poor. If V is the number of program variables O (EV). In addition, \nthe algorithm avoids propagating in\u00ad and E is the number of CFG edges, then the algofithm re\u00adformation \nthrough single-entry single-exit regions in which quires O(EV) space and 0(EV2) time. The inefficiency \nthere are no assignments to the relevant variable. arises because lattice values must be propagated along \ncon-A variety of enhancements to this algorithm are possible. trol flow paths from definitions of variables \nto their uses. For example, the MuMflow compiler performed predicate analysis to determine additional \nconstants: if the predicate at a switch is x=1, we can propagate the constant 1 for x on the true side \nof the conditional even if we cannot determine the value of x for the false side [LFK+ 93]. It is easy \nto extend both theDFG and CFG algorithms to accomplish this, but this extension seems difficult in SSA-based \nalgorithms [WZ91] since SSA edges bypass switches in the CFG. We omit the proof of correctness of the \nDFG algorithm. Given the structural properties of DFG edges, it is a simple matter to project values \nfrom the DFG onto the corresponding CFG edges and then show that these values are consistent with the \nvalues determined by the CFG algorithm.  5 Backward dataflow analysis In this section, we describe the \nuse of the DFG in performing backward dataflow analysis, using the computation of an\u00adticipatable expressions \nas an example. We then show how anticipatable expressions can be used in a powerful optimiza\u00adtion called \nelimination of partial redundancies, which sub\u00adsumes common subexpression elimination and loop-invaxiant \nremoval MR79]. 5.1 Anticipatability Definition 8 An expression e is totally (partially) antici\u00adpatable \nat a point p if, on every (some) path in the CFG from p to end, there is a computation of e before an \nassign\u00adment to any of the variables in e. We denote total (partial) anticipatability as ANT (PAN). ANT \nand PAN are usually computed for all expression in the program simultaneously, but we will focus attention \non a single expression to keep the discussion simple. The CFG equations for the computation of anticipatability \nare shown in Figure 5. The solution of the equations for ANT can be obtained iteratively, starting with \nan initial approximaticm in which ANT is true everywhere in the program except at e n d. This initial \napproximation permits ANT to propagate through loops correctly while ensuring that the boundary condition \nat end is satisfied. Similarly, the equations for PAN can be solved iteratively, starting with an initial \napproximation in which PAN is false everywhere in the program. We now discuss the solution of dataflow \nequations for ANT and PAN in the DFG. We first discuss expressions involving a single variable (such \nas x+1, y* 3 ) and then generalize to multivariable expressions (such as x+ y, x* y). Figure 5 shows \nthe DFG equations for ANT and PAN com\u00adputations for an expression x+ 1. These equations are similar to \nthe CFG equations. The rule for multiedges propagates anticipatability information from the heads to \nthe multiedge tail. The intuition behind this rule is the following by the definition of the DFG, the \nheads of a multiedge postdominate its tail, and there can be no definitions of variable x in the portion \nof the CFG between the tail and any of the heads; therefore, if the expression is totally (partially) \nanticipatable at any head, then it is also totally (partially) anticipatabl e at the tail. Another way \nof looking at this is that the tail of a multiedge accumulates the contributions to anticipatability \nfrom nodes with the same set of control dependence. In the solution of the CFG equations for ANT, the \ninitial ap\u00adproximation has ANT true everywhere except ate nd which provides the boundary condition where \nANT is false. DFG edges do not go to end, but the role of e n d in providing the boundary condition can \nbe played by statements that use x but do not compute the expression x+1 dependence for x at these statements \nare set to false. Similarly, if a variable x is live on one side of a conditional branch but dead on \nthe other, then the dependence for x is initialized to false on the dead side of the switch. Once the \nDFG propagation is done, the values of ANT at points in the CFG can be found by projecting from the DFG \ninto the CFG: simply set ANT to true at every point in the the single-entry single-exit region between \nthe head and tail of every dependence edge for which ANT is true at the head. An example of ANT computation \nis shown in F@ure 6. We start with ANT being true at all DFG edges except at expressions other than x+1 \nthat use x; in the example, ANT is true at all edges except d4, where it is false. The values at d4 and \nd5 are combined together by the multiedge rule, so ANT at d2 is true. Since ANT is true at d6, ANT at \nd3 remains true. ANT at d2 and d3 are true, so ANT is true at dl. Projecting ANT onto the CFG sets ANT \nto true at every point between the definition of x and the two computations Ofx+l. We can reduce the \nproblem of computing ANT for mul\u00adtivariable expressions like x+ y to the single-variable ANT discussed \nabove as follows. Definition 9 An expression e is totally anticipatable rela\u00adtive to variable x at a \npoint p if, on every path in the CFG from p to end, there is a computation of e before an assign\u00adment \nto x. It follows that an expression is totally anticipatable at a point p if it is totally anticipatable \nrelative to all of its vari\u00adables at p. A similar notion can be defined for partial antici\u00adpotability. \nTo compute ANT for x+ y, we initialize all dependence for x and y to true, except for the boundary dependence \nwhich are set to false. Propagation along dependence for x and y proceeds independently using the single-variable \nrules described earlier. Once DFG propagation is completed, we project ANT relative to x and ANT relative \nto y onto the CFG edges as before, and assert that ANT is true wherever it is true relative to both x \nand y separately. Figure 7 gives an example of multivariable ANT for the expression x+ y. We solve the \nsingle-variable ANT problems relative to x and y independently. Solving single-variable ANT relative \nto x yields true at dl and d3 and false at d2; projecting the results onto control flow edges yields \nANT true relative to x at e2 through e7. Single-vtiable ANT relative to y is true at d6 through d8 and \nis false at d4 and d5; projecting the results onto control flow edges yields ANT statement expression \nmultiedge switch merge ,I EL &#38; .~ck /4 + y Bc c ~.4 = false A.NT,4 = true MA = ~ ~B,C ANTA,B \n. ANTc p~A = false pANA = tr Ue pA.NA = ~ PANB,c PANA,B = PANc (a) control flow graph dataflow e uations \nANTA = t~ue ANTA = ~ #@/Tc, PANA = true PANA = ~ PANc; PPtiKA) ~f ANTc . AVB PPtii[B) :f ANTc . AVA \nI I AVC. = PP. AVB,C = AVA AVC == AVA . AVB DELETEA ~f AVA IIW.ERTA ~f PPA. N AVA (b) dependence flow \ngraph Wallow equations Figure 5: Dataflow Equations for AN 17PAN and EPR  L+S-l v x := ... x:= . dl \ndl e2 \\ .. . .,d2 d3 ............. .  d4 .., ,,\u00ad &#38;* .... ,.. ..$. ,. ,. @~ ,.. :. e3: d4 ./d2 \n..X.?.2.. M  x  ,  &#38;~&#38;J .-., e4 .. ..;; ... . .. #. . ,.. .-Ci5 . . .,X+1.. A; .(.. \n,: .. ,. ,.. mes! d6; a ~.,. , ,,\\ ,. ...  ,.. .. -. d6 ... .. ... . . dl / / ..,. ............. , \n ..,. ..X+l .. Q ,, -.$. ,., .,. , ;d k 3 ds , x (j bend L..lEEJ F@ure 6: An Example of Single-Variable \nAnticipatability Figure 7: An Example of Multivariable Anticipatability true relative toy at e5 throughe7. \nWe combine these sepacate results to get x+ y anticipatable at e5, e6 and e7. In our approach, the propagation \nof ANT occurs only in that portion of the program in which at least one of the vawi\u00adables in the expression \nis live. More elaborate approaches are possible. For example, we can avoid propagating false from expressions \nother than x+ y that use x or y by com\u00adputing ANT in two phases as is done in some ClW-based dataflow \nanalyses [DRZ92]. It is also possible to compute ANT directly by simultaneously following dependence \nfor all variables used in the expression, relying on a depth-first numbering scheme to order these dependence. \nWe will not discuss these more complex alternatives any further due to lack of space. 5.2 Elimination \nof partial redundancies Partial and total anticipatability can be used in a power\u00adful optimization called \nelimination of partial redundancies (epr), which subsumes common subexpression elimination and loop-invariant \nremoval. A computation is said to be redundant if it follows a computation of the same value on some \nexecution path. If a redundant computation is preceded by computations of the same value on all execution \npaths, we say the computation is totally redundan~ otherwise we say the computation is partially redundant. \nA classic dataflow algorithm for the removal of partial redundancies is due to Morel and Renvoise [MR79]. \nWe complete our discussion of DFG-based analysis by showing how we can implement epr. The basic idea \nis to insert new computations into the CI?G where it is safe and profitable to do so, thereby making \npar\u00adtially redundant computations totally redundant. Totally re\u00addundant computations may be replaced \nby a use of a new temporary variable that is properly assigned at the preced\u00ading computations. After \nremoving these totally redundant computations, no execution path will contain more instances of a computation \nthan it did originally, and some paths will contain fewer instances of the computation. A program point \nis safe for insertion of a computation of e if e is anticipatable at that point, but what about profitabil\u00ad \nity? Inserting computations of e wherever it is anticipatable, and then deleting computations of e wherever \nit has become available eliminates partial redundancies; however, this strat\u00ad egy may perform superfluous \ncode motion. For example, in Figure 6 this strategy will place x+1 immediately after the assignment to \nx and delete the other computations of x+ 1, even though there is no redundancy in the original program. \nThere has been much discussion in the literature about code motion strategies [DS88, Dha91, KRS92], but \nto our knowl\u00ad edge there is no experimental data showing the superiority of any single strategy. Our \napproach to epr has the virtue of simplicity. F@ure 5 shows the dataflow equations for the dependence-based \nal\u00ad gorithm. ANT and PAN are backward dataflow proble~ms, while AV is a forward problem. PP, INSERT, \nand DELETE are local definitions that do not propagate. The rationale be\u00adhind these rules is as follows. \nOnce we have computed ANT and PAN as described in Section 5.1, we determine where it may be profitable \nto place (PP) computations. There are two rules. The merge rule inserts a computation into a region if \nit is anticipatable and partially available at the output of the merge after insertion, the expression \nbecomes totally available at the output of the merge, so computations below the merge can be deleted. \nThe multiedge rule eliminates re\u00addundancies within a control region: it is profitable to place a computation \nat the tail of a multiedge if the expression is anticipatable at the tail and partially anticipatable \nat two or more heads. This is equivalent to saying that placement is profitable at the tail if the computation \nis totally anticipat\u00adable at one head and partially anticipatable at another head. Finally, we insert \na computation at a point if placement is profitable but the expression is not available, and we delete \ncomputations where the expression is available. The rule for multiedges can be refined further to fine-tune \nthe placement of code. Instead of hoisting a computation to the tail of the multiedge, it may be desirable \nto place it at the head where the value of the expression is first used in some other computation, since \nthat is the earliest place where the computation must be available. This kind of refinement is easy to \ndo in the DFG since the dependence edges tell us exactly where we must look to find the desired information, \nWe are evaluating these heuristics and we refer the interested reader to the forthcoming thesis of one \nof the authors [Joh93]. Our epr algorithm is simple in part because it is edge\u00adbased rather than node-based \nlike conventional presentations of epr. Placing computations at nodes is complicated by the presence \nof control flow edges whose source is a switch and whose destination is a merge, such as the back edge \nof repe at unt i 1 loops. This complication is eliminated by adding empty basic blocks to split such \nedges [MR79], but these blocks must later be removed if no code is moved into them. DFG algorithms are \nnaturally edge-based and avoid these complications. Our epr algorithm propagates information only through \nthe portion of the control flow graph where the variables in the expression are live. It can also skip \nover single-entry single-exit regions of the control flow graph where there is no definition or use of \nthe variables in the expression. This is not possible in CFG-based approaches. Fhtally, the DFG is built \nonly once prior to optimization (it is, of course, updated as optimization is performed). This approach \nis simpler to implement than other approaches that build a special-purpose graph for each expression \nfor which partial redundancies must be eliminated PRZ92]. 6 Conclusions In this paper, we have presented \nan approach to speeding up program analysis and optimization that is based on the use of the dependence \nflow graph. First, we gave a graph-theoretic characterization of the DFG that tied together related work \non clef-use chains, static single assignment form, and control dependence. Then we sketched a fast algorithm \nfor con\u00adstructing the DFG, which is of independent interest because it can be used to construct a factored \ncontrol dependence graph of a program in 0(E) time, a factor of N improvement over the best existing \nalgorithm. Finally, we showed how the DFG can be used for both forward and backward dataflow analy\u00adses. \nOur approach avoids inefficiencies by (1) propagating information for a variable x only where needed, \nbypassing single-entry single-exit regions of the graph which contain no definition of x, and by (2) \nperforming work proportional to the number of variable references at each assignment state\u00ad ment. We \nhave not addressed the problem of optimization in stages described in Section 1. Note that performing \nredun\u00addancy elimination in dependence order in the example of Section 1 achieves the desired ordering. \nThe general picture is more complex because of merges, but we believe that a dependence-based approach \nis the right one. In this paper, we have focused on the use of the DFG for optimizations. For parallelization, \nthe simple picture of the DFG in this paper can be extended to include aliasing, data structures, anti-and \noutput dependence, loop recog\u00adnition, and distance/direction information for loop-carried dependence. \nOur treatment of aliasing, and anti-and out\u00adput dependence is discussed in an earlier paper llMP91]. \nWe are implementing a DFG tool-kit for parallelization and optimization, and we will report on experimental \nresults in another paper. We conclude with a discussion of what we have learned so far from our implementation. \nFirst, we have found that it is neither necessary nor de\u00adsirable to use a full-blown dataflow graph representation \nof imperative language programs as an intermediate form. Our current representation retains control flow \ninformation and permits us to expose only relevant dependence in any phase of the compiler; for example, \nan optimization phase could expose only clef-use information, as we have done in this paper. We are aware \nof successful functional language com\u00adpilers (for dataflow machines) that represent programs as full-blown \ndataflow graphs, but we attribute their success to the relative simplicity of functional languages and \nthe close\u00adness between the intermediate and machine languages. Second, we believe that renaming of variables \nto accom\u00adplish single assignment (static or dynamic) is orthogonal to dependence representation. In our \nopinion, single assign\u00adment has nothing to do with the DFG (or for that matter, static single assignment \nform) it is best to view these represen\u00adtations as a way of knitting control dependence information \nwith clef-use information. This view has two advantages. First, aliasing can be handled very simply [BJP91], \nSecond, control dependence can also be combined with anti\u00ad and output dependence without the need for \na new conceptual framework. Finally, in the context of optimization, control dependence equivalence \nis more important than control dependence per se. The construction of the DFG requires an equivalence \nre\u00ad lation on control flow edges, and control dependence equiv\u00adalence is the coarsest equivalence relation \nthat can be used for this purpose. Realizing this led us to invent an algorithm that computes control \ndependence equivalences directly, and this in turn led us to an O(E) algorithm for constructing a factored \ncontrol dependence graph. To place our work in perspective, it is useful to understand the differences \nbetween the DFG and the program depen\u00addence graph (PDG) [FOW87]. The PDG of a program is the union of \nits control and data dependence. There have been many efforts to give a formal semantics to the PDG, \nwith the objective of using the semantics in correctness proofs of pro\u00adgram transformations [HPR88, Se189, \nCF89]. However, this has proved to be difficult. For example, it has been shown that two programs with \nthe same PDG have the same input\u00adoutput behavior, but the proof is long and intricate even for structured \nprograms. Other efforts give a constructive defini\u00adtion of the PDG by transforming the denotational semantics \nof an imperative language, but the construction and the final result are hard to decipher. These difficulties \nin giving se\u00admantics to a program, once it has been broken down into its control and data dependence, \nare not unlike the difficulties in giving semantics to a program once it has been broken down into assignments \nand GOTOS. Like quarks in nuclei [GM64] or conditional jumps in the stored program computer, con\u00adtrol \ndependence is a deep and fundamental notion. However, quarks do not exist in isolation, and conditional \njumps are im\u00adplicit and hidden in modern programming language control structures. Similarly in the context \nof optimization, con\u00adtrol and data dependence should be fused together to give structure to dependence \nas we have done in the DFG. It is straightforward to give a semantics to the DFG if one is desired, but \nmore importantly, such a semantic account is un\u00adnecessary since DFG edges have precise structural properties \nthat can be used in correctness proofs. Put simply, the DFG gives a way of propagating information in \nthe control flow graph while bypassing uninteresting regions, and the amount of bypassing is a useful \ncompromise between too much and too little.  7 Acknowledgements Thanks to Mayan Moudgill for early work \non DFG-based optirnizations and to Micah Beck for his stimulating input throughout the entire project. \nThanks to Richard Huff, Wei Li, Mayan Moudgill, Paul Stodghill and Mark Charney for their comments on \nthe paper.  References [ASU86] A. V. Aho, R. Sethi, and J. D. Unman. Compilers: Principles, Techniques, \nand Tools. Addison-Wesley, Reading, MA, 1986. /J3ec92] Micah Beck. Translating FORTRAN to Datajlow Graphs. \nPhD thesis, Department of Computer Science, Cornell University, May 1992. [BJP91] Micah Beck, Richard \nJohnson, and Keshav Pingali. From control flow to dataflow. Journal of Parallel and Distributed Computing, \n12:118-129,1991. [BM090] Robert A. Ballsnce, Arthur B. Maccabe, and Karl J. Ottenstein. The program Dependence \nWeb: A repre\u00adsentation supporting control-, data-, and demand-driven interpretation of imperative languages. \nIn Proceed\u00adings of theSIGPLAN 90 Conference on Programming Language Design and Implementation, pages \n257-271, June 20-22,1990, [CCF91] Jong-Deok Choi, Ron Cytron, and Jeanne Fermnte. Automatic constmction \nof sparse data flow evaluation graphs. In Conference Record of the 18th Annual ACM Symposium on Principles \nof Programming Languages, pages 55-66, January 21-23, 1991. [CF89] Robert Cartwright and Matthias Felleisen, \nThe semant\u00adics of program dependence. Jn Proceedings of the SIG-PLAN 89 Conference on Programming Language \nDe\u00adsign and Implementation, pages 13-27, June 2 1 23, 1989. [CFR+ 89] Ron Cytron, Jeanne Ferrante, Barry \nK. Rosen, Mark N. Wegman, and F. Kenneth Zadeck. An efficient method of computing static single assignment \nform, Jn (70n \u00adferenceRecordof the 16th Annual ACh4Symposium on Principles of Programming Languages, \npages 25-35, January 11-13,1989. [CFR+ 91] Ron Cytron, Jeanne Ferrsnte, Barry K. Rosen, Mark N. Wegman, \nand F. Kenneth Zadeck. Efficiently comput\u00ading static single assignment form and the control de\u00adpendence \ngraph. ACM Transactions on Programming Languages and Systems, 13(4):451-490, October 1991. [CFS90] Ron \nCytron, Jeanne Ferrante, and Vivek Sarkar. Com\u00adpact representations for control dependenc&#38; In Pro\u00adceedings \nof theSIGPLAN 90 Conference on Program\u00adming Language Design and Implementation, piiges 337-351, June \n20-22,1990. ~ha91] Dhansnjay M. Dhamdhere. Practical adaptation of the global optimization algorithm \nof Morel and Renvoise. ACM Transactions on Programming Languages and Systems, 13(2):291-294, April 1991. \n~RZ92] DhsnanjayM. Dharndhere, Barry K. Rosen, andF. Ken\u00adneth Zadeck. How to analyze large programs efficiently \nand informatively. In Proceedings of the SIGPLAN 92 Conference on ProgrammingLanguage Design andlm\u00adplementation, \npages 212-223, June 17-19, 1992. [DS88] K.-H. Drechsler and M. P, Stadel. A solution to a problem with \nMorel and Renvoise s Global Optimiza\u00adtion by Suppression of Partial Redundancies . ACM Transactionson \nProgrammingLanguagesandSystems, 10(4):635-640, October 1988. [FOW87] J. Ferrsnte, K. J. Ottenstein, and \nJ. D. Warren. The program dependency graph and its uses in optimization. ACM Transactions on Programming \nLanguages and Systems, 9(3):3 19-349, June 1987. [GM64] Murray Gell-Msnn, A schematic model of baryons \nand mesons. Physics Letters, 8(3):214 2 15, February 1964, [HPR88] Sussn Horwitz, Jan Prins, and Thomas \nReps. On the adequacy of program dependence graphs for represent\u00ading programs. Jn ConferenceRecordof \nthe 15th Annual ACM Symposium on Principles of Programming Lan\u00adguages, pages 146-157, January 13-15,1988. \n[Joh93] Richard Johnson. Dependence-Based Compilation (working title). PhD thesis, Department of Computer \nScience, Cornell University, 1993. Expected in Septem\u00adber. ~as75] V, N. Kas jsnov. Distinguishing hammocks \nin a directed graph. Soviet Math. Doklady, 16(5):448-450, 1975. DW73] Gary A. Kildall. A unitled approach \nto global program optimization. In Conference Record of the ACM Sympo\u00adsium on Principles of Programming \nLanguages, pages 194-206, October 1-3,1973. llCRS92] Jens Knoop, Oliver Riithing, and Bernhard Steffen. \nLazy code motion. In Proceedings of the SIGPLAN 92 Conference on Programming Language Design and Implementation, \npages 224-234, June 17-19,1992. lLFK+ 93] P. GeoiTrey Lowney, Stefan M. Freudenberger, Thomas J. Karzes, \nW. D. Liechtenstein, Robert P. Nix, John S. O Domell, and John C. Ruttenberg. The Mul\u00adtifiow trace scheduling \ncompiler. Journal of Supercom\u00adputing, 7(1/2), January 1993. [MR79] Etienne Morel and Claude Renvoise. \nGlobal optimiza\u00adtion by suppression of partial redundancies. Communi\u00adcations of the ACM, 22(2):96-103, \nFebruary 1979. ~BJ+ 91] Keshav Pingali, Micah Beck, Richard Johnson, Mayan Moudgill, and Paul Stodghill. \nDependence Flow Graphs: An algebraic approach to pro~am dependen\u00adcies. In Conference Record of the 18th \nAnnual ACM Symposium on Principles of Programming Languages, pages 67-78, January 21-23, 1991. BT81] \nJohn H. Reif and Robert E. Tsrjsn. Symbolic program analysis in almost-liness time. SIAM Journal on Com\u00adputing, \n11(1):81-93, February 1981. [Se189] Rebecca P. SeIke. A rewriting semantics for program dependence graphs. \nIn Conference Record of the 16th AnnualACMSymposium on Principles ofProgramming Languages, pages 12-24, \nJanuary 11-13, 1989. [ss70] R. M. Shapiro and H. Saint. The representation of algorithms. Technical Report \nCA-7002-1432, Mas\u00adsachusetts Computer Associates, February 1970. ~85] Mark N. Wegman and F. Kenneth Zadeck. \nConstant propagation with conditional branches. In Conference Record of the 12th Annual ACM Symposium \non Princi\u00adples of Programming Languages, pages 291-299, Jsn-USry 14-16, 1985. pwz91] Mark N. Wegman and \nF. Kenneth Zsdeck. Con\u00adstant propagation with conditional branches. ACM Transactions on ProgrammingLanguages \nand Systems, 13(2):181-210, April 1991.  \n\t\t\t", "proc_id": "155090", "abstract": "<p>Program analysis and optimization can be speeded up through the use of the <italic>dependence flow graph</italic> (DFG), a representation of program dependences which generalizes def-use chains and static single assignment (SSA) form. In this paper, we give a simple graph-theoretic description of the DFG and show how the DFG for a program can be constructed in <italic>O(EV)</italic> time. We then show how forward and backward dataflow analyses can be performed efficiently on the DFG, using constant propagation and elimination of partial redundancies as examples. These analyses can be framed as solutions of dataflow equations in the DFG. Our construction algorithm is of independent interest because it can be used to construct a program's control dependence graph in <italic>O(E)</italic> time and its SSA representation in <italic>O(EV)</italic> time, which are improvements over existing algorithms.</p>", "authors": [{"name": "Richard Johnson", "author_profile_id": "81406600453", "affiliation": "", "person_id": "PP31083369", "email_address": "", "orcid_id": ""}, {"name": "Keshav Pingali", "author_profile_id": "81100554731", "affiliation": "", "person_id": "PP39048331", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/155090.155098", "year": "1993", "article_id": "155098", "conference": "PLDI", "title": "Dependence-based program analysis", "url": "http://dl.acm.org/citation.cfm?id=155098"}