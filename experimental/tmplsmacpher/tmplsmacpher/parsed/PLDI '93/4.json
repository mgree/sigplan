{"article_publication_date": "06-01-1993", "fulltext": "\n Abstract debugging of higher-order imperative languages Frangois DIGITAL Paris Research Laboratory \n85, avenue Victor Hugo 92500 Rueil-Malrnaison France Tel: +33 (1) 47142822 bourdoncle@prl Abstract \nAbstract interpretation is a formal method that enables the static determination (i.e. at compile-time) \nof the dynamic properties (i.e. at run-time) of programs. We present an abstract interpretation-based \nmethod, called abstract debug\u00adging, which enables the static and formal debugging of pro\u00adgrams, prior \nto their execution, by finding the origin of po\u00adtential bugs as well as necessmy conditions for these \nbugs not to occur at run-time. We show how invariant asser\u00adtions and intermittent assertions, such as \ntermination, can be used to formally debug programs. Finally, we show how ab\u00adstract debugging can be \neffectively and efficiently applied to higher-order imperative programs with exceptions and jumps to \nnon-local labels, and present the Syntox system that en\u00adables the abstract debugging of the Pascal language \nby the determination of the range of the scalar variables of programs. 1 Introduction Even though software \nquality is becoming more and more im\u00adportant, relatively few methods have been proposed to help programmers \ndebug their programs, and debugging is typi\u00adcally done post-mortem , that is, after a bug has occurred. \nThe main drawback of this approach is that it is often very difficult, if not impossible, to find the \norigin of a bug just by looking at the memory state after the abortion of a program. Methods have been \nproposed to allow the reverse execution of higher-order functional languages such as ML [24], but these \nmethods do not seem to be quite applicable to imper- Permission to copy without fee all or part of this \nmaterial is granted provided that the copies are not made or distributed for direct commercial advantaga, \nthe ACM copyright notice and the title of the publication and its date appear, and notice is given that \ncopying is by permission of the Association for Computing Machinery. To copy otherwise, or to republish, \nrequires a fee and/or specific permission. ACM-S lGPLAN-PLDl-6/93 /Albuquerque, N.M. a 1993 ACM 0-89791 \n-!598-41931000610046 . ..$1 .50 Bourdoncle Centre de Math6matiques Appliques Ecole desMines de Paris \nSophia-Antipolis 06560 Valbonne France .dec. com ative languages such as Pascal, Modula-2, Modula-3, \nC or C++, since they have to keep track of every variable as\u00adsignment. Even more important, post-mortem \ndebugging can fail to detect bugs when programs are not extensively tested, since some parts of the code \nmay have never been executed. Abstract interpretation, as defined by Patrick and Radhia Cousot [8, 11, \n13], is a formal method that enables the static determination of the run-time properties of programs. \nSo far, absmact interpretation has only been applied to fairly simple languages (first-order imperative, \nfunctional, logic or parallel languages) and has been used to build sophis\u00adticated optimizing compilers. \nIn this paper, we propose a novel, semantic-based approach to the debugging of pro\u00adgrams, called abstract \ndebugging, which combines several traditional, although rarely used, abstract interpretation tech\u00adniques \nto allow the static and formal determination of the origin of certain bugs in higher-order imperative \nprograms. This method enables the programmer to insert assertions into the source-code of the program \nbeing debugged, and vio\u00adlations of these assertions are treated as run-time errors by the debugger. There \nare two kinds of assertions. Invariant asser\u00adtions are properties which must always hold at a given control \npoint, and are similar to the classical assert statement in C programs. Intermittent assertions are properties \nwhich must eventually hold at a given control point. Differently stated, intermittent assertions are \ninevitable properties of programs, that is, properties such that every execution of the program inevitably \nleads to the control point with a memory state sat\u00adisfying the intermittent property. For instance, the \ninvariant assertion false can be used to specify that a particular control point should not be reached, \nwhereas the intermittent assertion true at the end of a pro\u00adgram specifies the termination of the program. \nInvariant and intermittent assertions can be freely mixed and give the pro\u00adgrammer a great flexibility \nto express correctness conditions of programs. Abstract debugging differs from traditional abstract in\u00adterpretation \ntechniques in that a very precise interprocedural analysis of programs is needed in order to locate \nbugs. There\u00adfore, many traditional approximations, which are acceptable for optimizing compilers, cannot \nbe used here, In particukw, aliasing cannot be approximated if one wants to compute a precise information \nabout the value of the scalar variables of programs, and methods [2, 7, 14, 15, 16, 20, 22] used to determine \nthe set of possible alias pairs of programs, that is, pairs of distinct variables for which there exists \nat least a procedure activation in which both variables have the same address, are inappropriate here, \nsince they do not describe the simultaneous aliasing of three or more variables, and wcmld lead to a \nvery unprecise abstract assignment primitive. This paper is organized as follows. In the first section, \nwe give several examples of the kind of bugs that can be found by an abstract debugger. Then, in sections \n3 and 4, we describe the basic techniques of abstract debugging, and show how they can be combined to \neffectively debug programs. In section 5, we address the problem of the abstract interpretation of higher-order, \nPascal-like imperative languages, and briefly sketch a non-standard, copy-in/copy-out semantics of these \nlanguages that is well suited to abstract debugging. Finally, in section 6, we present the prototype \nSyntox system that enables the abstract debugging of first-order Pascal programs by the determination \nof the range of scalar variables. We discuss implementation and complexity issues, show that even very \nsimple properties such as the range of variables enable the determination of non-trivial bugs, and show \nhow this system can also be used to safely suppress most array bound checks during the execution of Pascal \nprograms.  Examples The programs of figure 1 exemplify several common pro\u00adgramming mistakes that can \ntypically be discovered and re\u00adported by an abstract debugger. For instance, program For will obviously \nexit on a run\u00adtime error when accessing TIO], unless n < 0 at point ~. Moreover, if the index i ranges \nfrom 1 to n instead of O to n, then the program will exit when accessing T[ 101] unless n < 100 at point \n~. Similarly, program While will IIoop unless b = false at point @, and program Fact will loop unless \nx > Oat point ~. It might seem quite difficult to automatically discover these kinds of bugs. However, \nour abstract debugger Symox, described in section 6, will automatically discover and report the above \nnecessary conditions of correctness. A compiler could use these conditions to issue a warning or generate \na call to a specific error handler to do some clean\u00adup and exit safely, or else could enter a special \ndebugging mode to do a step-by-step execution of the program until the error actually occurs. The interesting \nfact about abstract debugging is that it predicts bugs before they actually happen, which permits a safe \nhandling of these bugs. Whenever possible, an abstract debugger finds the origin of bugs, rather than \ntheir occurrences, and back-propagates necessary conditions of correctness as far as possible in or\u00adder \nto minimize the amount of information delivered to the programmer. This feature makes abstract debugging \nmuch more useful than traditional methods and global flow analyz\u00aders such as Lint for instance, which \nis well known for the large number of warnings it generates. For example, it is much more interesting \nto know that variable n of program For must be lower than 100 at point @ than to know that i must be \nless than 100 at point @ since the former test can be done once and for all after n has been read, whereas \nthe latter must be done for every access to T . Moreover, if n > 100 at point ~, then it is certain that \nthe program will either loop or exit on a run-time error later on. As we shall see in the next section, \nbackwardpropagation is an essential component of abstract debugging, since it is responsible for the \ndiscovery and the factorization of the correctness conditions of programs. As an example, consider the \nfollowing sequence of Pascal statements, where T is an array of 100 integers: read(i); 0 j:=i+l; @) k:=j; \nQ) read(T[k]) Starting from the end of the sequence, a backward analysis will determine that k s [1, \n100] at point ~, j 6 [1, 100] at point Q, and finally that i E [0, 99] at point ~. This information can \nthen be combined with the forward data flow, which shows that the post-condition i c Z of the first call \nto read does not imply the pre-condition i 6 [0, 99] determined by the backward analysis. Hence, a warning \ncan be issued to inform the programmer that if i $! [0, 99] at point O, then his program will certainly \nfail later on. As stated in the introduction, an important feature of ab\u00adstract debugging is that programmers \ncan freely insert invari\u00adant assertions and intermittent assertions into their programs to either statically \ncheck that important invariance properties, such as calling conditions of library functions, are satisfied, \nor check under which conditions a program eventually reaches a control point while satisfying a given \nproperty. Intermittent assertions allow for a very powerful form of debugging. As an example, if the \nintermittent assertion i = 10 is inserted at point @ of program Intermittent , then Syntox shows that \na necessary condition for the program to eventually reach control point @ with i = 10 is that i < 10 \nat point o. It is thus possible to determine the set of program states (and, in particular, of input \nstates) from which a program eventuzdly reaches a given control point, by simply inserting the intermittent \nassertion true at this point. So for instance, if the intermittent assertion true is inserted at point \n@ of program Select , then Syntox shows that a necessary condi\u00adtion for the program to terminate is that \nn <10 at point ~. Differently stated, if n >10, then the program will certainly loop or exit on a run-time \nerror. Further more, if the invariant assertion false is inserted at point ~, then Syntox shows that \nn < 10 at point @ is a necessary condition for the program to terminate without control ever reaching \npoint ~. Now, if the intermittent assertions = 1 is inserted at point Q), Syntox shows that a necessary \ncondition for this assertion to eventually hold is that n 6 [ 10, 10] at point @ To conclude this section, \nwe can remark that invariant as\u00adsertions are normally used to express normal conditions of the execution \nof a program, whereas intermittent assertions are either used to enforce termination or to determine \nnec\u00adessary conditions for the failure of invariant assertions. For instance, if a necessary condition \nof correctness is reported for a given program, then it is interesting to negate, one after the other, \nevery invariant assertion inserted into the source code and determine necessmy conditions for control \nto reach the corresponding point and satisfy the negated assertion, i.e. necessary conditions for the \nassertion to be surely violated. Finally, it should be clear that although the standard ab\u00adsmact interpretation \nframework determines flow-insensitive program properties, invariant assertions such as false can be used \nto restrict the control flow and examine the behavior of a program along specific execution paths.  \nStatic debugging In this section, we shall explain the mathematical framework behind abstract debugging, \nleaving decidability and com\u00adputability issues to the next section. We shall refer to this mathematical \nframework as static debugging, reserving the term abstract debugging for its tractable counterpart. Static \ndebugging is a combination of several different techniques, namely: Forward propagation  Backward propagation \n o Least fixed point computation Greatest fixed point computation Forward propagation is the most classical \ntechnique. It has been used for a long time in data-flow analysis to propagate program properties by \nfollowing the normal flow of programs. Backwtwd propagation does the same as forward propagation but \nreflects the backwards execution of programs. Note that, contrary to a common belief, the backward semantics \nof a program is not more complex nor very different from the forward semantics, the only real difference \nbeing that it is not deterministic. Static debugging uses the abstract interpretation frame\u00adwork pioneered \nby the Cousots in which the operational se\u00admantics of a program is defined by a transition relation r \nover a set of program states S. This framework aims at computing safe approximations ofjixed points of \ncontinuous functions over complete lattices, such as the lattice (P(S ), @,S, <, u, n) of the subsets \nof S. In what follows, we shall denote by ~+ (X) theset{y ESI3x GX :x ~ y}ofdescendants ofstates in X~S, \nand by~-(Y) theset{x eS13y~Y:x~y} of ancestors of states in Y < S. Moreover, we shall assume that every \nstates has a unique descendant, i.e. IT+ ( {s}) I = 1, which is the case for classical imperative languages, \nand that every output state s 6 S..f and every error state s E Se,, is stable, i.e. s ~ s. Note that \ndenotational semantics cannot be used here since the backward semantics of a program in this framework \nis not definable from the forward semantics. Finally, we shall implicitly identify a property over S \nwith the set of states for which this property holds. It is now well known [13] that the set of descendants \nof a set of states Z ~ S by a finite number of program steps is the least fixed point (w.r.t. the subset \nordering) of function @ defined by: o = AX, (X u T+(x)) that is, the function which takes a set of states \nX and returns the union of Z and the set of descendants of states in X. This fixed point always exists \nand can be iteratively computed as the following limit: (J @ (o) l~o Similarly, the set of ancestors \nof a set of states Z is the least fixed point of AX. (7Lu T-(x)) Finally, the set of states which do \nnot lead to an error is the greatest fixed point of  Ax. (T-(x) sew) More generally, let II be a property \n(i.e. a set of states H ~ S) that one wishes to prove about a program. Two sets are of interest for static \ndebugging: o The set always(lI ) of states whose descendants satisfy n. The set eventually(H) of states \nfor which there exists at least one descendant satisfying II. It can be shown that: always@I) = gfp ~x \n(rI n ~-(x))) eventually = lfp JX. (FI u r-(X))) { [ where lfp and gfp respectively denote the least \nfixed point and the greatest fixed point operators. Intuitively, since the least fixed point of a continuous \nfunction over a lattice can be computed by an increasing iterative computation starting from the least \nelement 0, then: eventually = H U T-(H) U (T-)2(H) U is the set of ancestors of the states satisfying \nH. Similarly, since a greatest fixed point can be computed by a decreasing program Selecq program Fact; \nprogram McCarthy; var n,s : integer; var x, y: intege~ var m, n: integer; function Select(n: integer): \ninteger; function F(n: integer): integer; function MC(n: integer) : integer; begin begin begin if n> \n10 then if n=Othen if (n> 100) then Select:= Select(n + 1) F:=l MC:=n 10 else if n > 10 then else else \nSelect:= Select(n 1) F:=n*F(n 1) MC:= MC(MC(MC(MC( else if n = 10 then end; MC(MC(MC(MC( @ Select:= \n1 begin MC (n + 81))))))))) else read(x); Q end; Select:= 1 y:= F(x) @ begin end; end. read(n); Q begin \nm := MC(n); read(n); writeln(m) @ @ s:= Select(n); end. writeln(s); @ end. program While; program Intermittent \nprogram FOG var i: integer; var i: integer; var i, n: integer; b: boolean; begin T: array [1..100] of \ninteger; begin @ read(i); @ begin i := 0; while (i < 100) do read(n); O read(b); Q @ i:=i+l@ fori:=Otondo \nwhile b and (i < 100) do @ @ read(T[i]) @ i:=i-1 end. end. end. Figure 1: Examples iterative computation \nstarting from the maximum element S, then: always(II) = II n T-(H) n (r-)2 (1 1) n . . . is the set \nof states satisfying 11 whose ancestors satisfy II. So for instance, eventually(SOUt) is the set of states \nfor whlich the program terminates, eventually (S.v) is the set of states leading to a run-time error, \nalways(S Sou,) is the set of states which either cause the program to loop or to exit cm a run-time \nerror, and always(S Se,,) is the set of states which do not lead to a run-time error. Note that under \nthe assumption that IT+ ( {s}) I = 1 for everys E S, it can be shown that always(~) = eventually where \nII denotes the negation (or complement) of Il. How\u00adever, since approximate lattices used to finitely \ncompute safe approximations of program properties are usually not com\u00adplemented, this equality is of \nno practical use. For instance, the complement of an interval, say [1, 5], is not an interval in general. \nThe two sets always(II) and eventually @I) can be used to perform the static debugging of programs as \nfollows. Suppose for instance that a programmer wants to prove that a set of properties {T~}~e~a always \nhold at control points {c~}~e~=. We shall assume that every state other than an error state consists \nin a pair (c, m) of a control point c and a memory state m. Then, the global invariant assertion to prove \nis: ~a={(c, m) ES[Vk EKa:c=ck* rn~irk} that is, II. enforces ~k at point ck E K. and is true ev\u00aderywhere \nelse. Similarly, if the programmer wants to prove that at least one control point of a set {ck}keKe, \nK. f Kg = O, will be eventually reached at run-time with a memory state satisfying { ~k}k~Ke, then the \nglobal intermittent assertion to prove is: 11. ={(c, m)6S13k CKe:c=c~A m=n} that is, He enforces ~k \nat point ck E K. and is false ev\u00aderywhere else. The program invariant, which represents the set of program \nstates that can be reached during program executions that are correct with respect to the programmer \ns specifications, is then the limit I of the decreasing chain 1~ defined by IO = S and iteratively computed \nby applying the following steps in sequence 49 1) Compute the set 1~+1 of descendants of the states in \n1~ by computing the least fixed point of AX. (1~ f (Si. U T+(X))) where Si. is the set of input states. \n2) Compute the set I~+Z of states in Ik+l whose descen\u00ad dants satisfy II. by computing the greatest fixed \npoint of Ax. (1~+1 n n= n T-(X)) 3) Compute the set I~+s of states in I~+Z for which there exists at \nleast one descendant satisfying 11 by comput\u00ading the least fixed point oti AX. (1~+2 n (H, u 7-(X))) \nStep 1 is a forward analysis, whereas steps 2 and 3 are back\u00adward analyses. Note that the chain (1~)~20 \ncan be infinitely strictly decreasing, since each step can refine the previous one. For instance, steps \n2 and 3 can back-propagate a condi\u00adtion on the input states (i.e. remove a few input states) that will \nbe then propagated forward by step 1, etc. When the invariant I has been reached, it is very easy to \ndetermine the source of potential bugs in the program and issue warnings to the programmer. First, every \ninput states that is not in I is an erroneous state and any execution starting from this state will lead \nto a violation of at least one of the programmer s invariants. Moreover, it is easy to see that every \nstate s @ I that is a descendant of a states E I is erroneous, since it follows from the forward data \nflow but is not part of the backward flow (section 2). The characterization of this set of erroneous \nfrontier states is the negation of the correctness condition reported to the programmer. Finally, note \nthat in real programming languages, the hy\u00adpothesis IT+ ({s} ) I = 1 for every states c S does not hold, \nand some states can have several descendants. This is the case, for instance, for input statements and \nprocedure calls (since local variables have undefined values upon procedure entry), but the same property \nholds for logic programs, which are intrinsically non-deterministic. It can be shown that the framework \nremains valid but that the invariant I becomes a necessary condition of correctness and is no longer \na sufficient condition.  Abstract debugging Since each invariant 1~is not computable in general, abstract \ninterpretation techniques must be used to finitely compute a safe approximation of I. The standard framework, \ndefined in Cousot [8], consists in llefining a Galois connection (a, T) between the exact lattice (P(S), \n0, S, <, U, n) and a finitely represented approximate lattice (P# (S), 1, T, E, U, fl) used to represent \nsafely approximated program properties. The abstraction function a : P(S) -+ P# (S) maps sets of states \nto their best approximation in the abstract lattice, whereas the concretization or meaning function y \n: P# (S) -+ P(S) maps every abstract property to its meaning , i.e. the set of states satisfying the \nproperty. Note that a loses infor\u00admation, whereas y does not, that is: y Oa ~ 1P(s) and a o y = lp#(,$) \nThe advantage of using Galois connections is that if a function @# is a safe approximation: @# g ao@oy \nof a semantic function 0, e.g. @ = AX. (S,. U T+ (X)), then the least fixed point (for instance) of @# \nis automatically a safe approximation of the least fixed point of cD,that is: 7(lfp(@#)) 2 lfp(@) Note \nthat a and T are never actually implemented and only serve the purpose of establishing the semantic correctness \nof @#. Also, note that the semantic functions @ and @# depend on the program. However, it is easy to \nsee that@ is built-up on a standard way from abstract primitives which are program independent and are \nthe only semantic functions actually implemented in an absmact debugger. For instance, program Intermittent \nof figure 1 is asso\u00adciated with the forward system of semantic equations, corre\u00adsponding to the least \nfixed point equation X = cD#(X): X()=1 xl = [read X2 = [i< 1OO](XI) H [i< 100](x~) x3 = [i:= i+l](x2.) \nX4 = [i> 1OO](X1) H [i> 100](x3) where, for example, [i < 100] denotes the abstract test primitive, which \nmust satisfy, for every x c P*(S): [i< 100](x) z a({i 6 ~(x) : i < 100}) Similarly, when the intermittent \nassertion i = 10 is inserted at point Q, the backward system of semantic equations to be solved iteratively, \nstarting from 1, is: xl) = ~read(i)]- (xl) xl = [i< 1OO](XZ) U [i> 1OO](X4) X2 = CY({1O}) U [i:=i+ 1]-1(x3) \nx3 = [i< 1OO](X2) U [i> 100](xq) x4 = x4 where [i: = i + 1] 1 denotes the backwmd abstract assign\u00adment \nprimitive which, in this particular case, is equivalent to the [i := i 1] forward primitive. Finally, \nif the invariant assertion i 2 O is inserted at point Q, the backward system of semantic equations to \nbe solved, starting from T, becomes: X(l = [read(i)] -l(xl) xl = [i< 1OO](XZ) U [i> 1OO](X4) X2 = a({O,l \n,2,3, ...}) n [i:= i+l]-l (x3) x3 = [i< 1OO](XZ) U [i> 1OO](X4) x4= x4 Note that the last two systems \nare obtained by a trivial in\u00adversion of the forward system and by adding the appropri\u00adate unions (resp. \nintersections) to the right-hand sides of the equations, e.g. Ua({lO}) , totakeinto account thepro\u00adgrammer \ns intermittent (resp. invariant) assertions. When the approximate lattice is of finite height, iterative \ncomputations of solutions of these systems always terminate, but when the lattice is of infinite height, \nor when its heighlt is finite but very large, speed-up techniques must be used. These techniques, known \nas widening and narrowing [4, 5,8, 11, 13], allow the determination of safe approximations of approximate \njixed points, while enforcing jinite iterative computations. They allow trade-offs to be made between \ncomputation time and precision. We recall that a widening operator V is a safe approximation of the union, \nthat is: b x, yEP#(s) : Xvy g Xuy and is such that for every increasing chain (xj),~o, the chain (xj),~II \ndefined by: x; = X(J and Xj+, = x:Vxi+l (i ~ O) is always eventually stable. A narrowing operator A \nsatisfies: and is such that for every decreasing chain (xi )1>0, the chain (x~)i20 defined by: is always \neventually stable. For every continuous semantic function @#, it can be shown that the increasing chain: \nXo=l and X,+l = Xi V@#(Xi) (i ~ 0) is eventually stable and that its limit d~ is a post-fixedpoint of \n@# (i.e. @#(@#) ~ 4#) and, therefore, a safe approximation of the least fixed point of cJ#. Similarly, \nit can be shown that the decreasing chain: xl) = qP and X,+l = xi A@#(xi) (i ~ O) starting from any \npost-fixed point 4# of @# is always a safe approximation of the least fixed point of @# and, choosing \n@#= T, that this limit is a safe approximation of the greatest fixed point of @#. These properties show \nthat safe approximations of IIeast fixed points of continuous functions can be finitely computed by \na combination of a widening phase starting from J_, fol\u00ad lowed by a narrowing phase starting from the \nresult of the previous phase, and that safe approximations of greatest fixed points can be computed by \na single narrowing phase starting from T. Note that, when working with systems of equations, it is not \nnecessary to apply widening operators to each equation, but only to a set IV of equations such that every \ncycle in the dependency graph of the system is cut by at least an element w E W, called a widening point. \nFor instance, for the first system above, only the third equation needs to be replaced by: X2 = X2 v \n([Z< 1OO](X1) u [2<1OO](X3)) Of course, since widening operators lead to a loss of precision, the smaller \nthe cardinal of W, the better, but since the problem of finding minimum sets W is NP-complete, safe heuristics \nhave to be developed to select good sets of widening points and iteration strategies (section 6.3). Finally, \nnote that the approximation of I implies that the correctness conditions determined by an abstract debugger \nare necessary, but not always sufficient. 5 Higher-order languages As stated in the introduction, one \nof the constraints of ab\u00adstract debugging is to efficiently determine precise informa\u00adtion about the \ndynamic behavior of programs in order to be able to detect bugs. It is thus desirable that the abstraction \nused to approximate program invariants be highly tunable and arbitrarily precise. Earlier works [13] \nshow that this is the case for simple , first-order languages without reference parameters. However, \nreference parameters and procedure parameters are the source of two distinct problems: aliasing and environment \nsharing, which both lead to very unprecise and expensive abshact interpretations. Intuitively, environment \nsharing, i.e. the fact that the same variable can be accessible, at the same time, to different pro\u00adcedure \nactivations in the run-time stack of a block-structured program with local procedures, implies that the \nabstract as\u00adsignment of non-local variables, to be safe, must be additive rather than destructive as \nis normally the case ([5], p. 105). Similarly, if the partition of the set of identifiers accessi\u00adble \nto a given procedure activation into subsets of identifiers having the same address is not known exactly, \nthen the ab\u00adstract assignment primitive has an exponential complexity, since the assignment must be simulated \nfor every possible partition, and is also very unprecise ([5], p. 106). We have thus designed a non-standard, \ncopy-in/copy-out semantics of higher-order imperative languages, and shown that this semantics is equivalent \nto the standard, stack-based semantics (as can be found for instance in Aho et al. [1]) of second-order \nPascal programs with jumps to local and non-local labels. An early version of this semantics for first-order \nPascal programs can be found in Bourdoncle [3] and the version for higher-order imperative languages \ncan be found in Bourdon\u00ad cle [5], p. 113 196. We have shown that this semantics is also equivalent to \nthe standard semantics for an undecidable sub-class of higher\u00ad order programs containing, in particulm, \nprograms with ex\u00ad ceptions but without local procedures (which allows for the treatment of the set jmp \nand long jmp primitives of C). ~ascalAbstract Debugger 6 Ie: Mdir/bourdonc!MacCarthy.p rogram Maccarthy{ \ninput, output), var x, m : intager, function MC(n integer) integer; begin If (n > 100) then MC = n 10 \nelse begin tic := MC(MC(n + 11)) end; end; $1 top ? I m LJd . : I egati n El3El   I terati ns .**Checking \nsyntax A.hForward1S amlys .*+ Widening (84) *** Narrowing (56) A &#38; * Inter,~~ttent a~~e~f Ions \n*** Widening (140) .** Narrowing (28 ) , k k ~o, ~r,j anal s IS *** Wldenmg (81) *** Narrowing (28) AA \nA corre&#38;ness condit ions .== ====.= -=.======= ====== ,** Flags : nart-owmg *** CPU: 0.6 secmnds \n:*A M+mry, 46 Kb *** Control points . 32 **+ Equations. 448 (2104 unions, 814 widen ings) *** CO~ple~ity: \n2 1 ~ n Figure 2: The Syntox system Moreover, we have shown that for programs for which the two semantics \ncoincide, it is possible to determine variable aliasing exactly, that is, it is possible to determine, \nfor every procedure, ail the possible run-time partitions of the local and global variables of this procedure \ninto subsets of variables having the same address. The knowledge of these partitions is much richer than \nwhat can be found for instance in Banning [2] since there may be a procedure activation where x and y \nare aliases and another activation where y and z are aliases, but no activation where x, y and z are \naliases simultaneously. As a matter of fact, the classical problem of finding all possible aliaspairs \nof a program is an abstraction (namely, graph union and transitive closure) of the real aliasing information. \n The Syntox system The Syntox system is an interprocedural abstract debugger that implements the ideas \nof section 3 and 4, and the non\u00adstandard semantics of section 5 for a subset of Pascal. This system, \nwhich is only a research prototype, can be used to find bugs which are related to the range of scalar \nvariables, such as array indexing, range sub-types, etc. The lattice of program properties used is thus \nnon-relational, but we have shown ([51, p. 197-216) that any relational lattice can be chosen, and that \nresults can be arbitrarily precise, even in the presence of aliasing, local procedures passed as parameters, \njumps to non-local labels and exceptions (which do not exist in Pascal). Relational lattices can be used \nto determine properties, such as linear inequalities of the form i < 2 * j + 1, that exist between different \nvariables of a program [10]. Even though the interval lattice is quite simple, we shall see in section \n6.5 that it allows to determine non-trivial bugs. 6.1 Interval lattice The interval lattice I(Zb) used \nby Syntox is a safe approxima\u00adtion of p(~b ) where ~b denotes the set of integers between w = 2b-l and \nw+ = 2b-1 1. This lattice being of height 2b, fixed point computations can require up to 2b iterations. \nTherefore, we use the standard widening and narrowing op\u00aderators of Cousot [8, 13] defined, for every \nx E I(Zb), by: J_vx= xvJ_ =X, lAx=x A1 = L and, for every [al, bl], [az, bz] 6 I(~b), by: [al, b,] V \n[az, bz] = [if a2 < al then w-else al, if b2 > bl then w+ else bl] [aI, bl] A [a2, b2] = [if al = w-then \naz else rnin(al, az), if bl = w+ then b2 else rnax(bl, b2)] It is easy to see that these operators enforce \nthe convergence of iterative computations of least fixed points in four steps at most. For instance, \nthe values taken by X2 during the iterative computation of the solution of the first system of section \n4 are: 1, [0,0], [O,o]v([o,o]u [1, 1]) = [O,w+] for the widening phase and: [O,w+], [O,w+] A ([0,0] U \n[0, 100]) = [0, 100] for the narrowing phase, which gives the optimum results: X2 = [0, 100] and z = \n[10, 10]. Therefore, the approximate computation of a least fixed point over I(~b ) using a widening \nphase and a narrowing phase is only four times more complex than constant propagation! Finally, note \nthat more sophisticated widening and nar\u00ad rowing operators can be easily designed [4] to integrate ad\u00ad \nhoc heuristics which are appropriate for the class of programs considered, as long as they satisfy the \ngeneric requirements stated above.  6.2 Language restrictions For historical reasons, Syntox does not \nyet allow procedures to be passed as parameters to other procedures, but the theoret\u00adical results of \nsection 5 show that this feature could be added without major problems. Variant records and the with \ncon\u00adstruct are not allowed in programs. Only the most standard Pascal library functions are predefine. \nPrograms with point\u00aders to heap-allocated objects are accepted, but are not always handled safely with \nrespect to aliasing; other works on the abstract interpretation of heap-allocated data structures such \nas Deutsch [15, 16] could be used to handle pointer-induced aliasing. Records are accepted, but no information \nis given on their fields. This decision was made LOsimplify the de\u00adsign of the debugger, but records \ncan be handled without much trouble. Jumps tolocal andnon-local labels are fully supported. 6.3 Algorithms \nandcomplexity Two different fixed point computation algorithms, described in Bourdoncle [6], are used \nby Syntox. Both algorithms are based on a weak topological ordering decomposition of the dependency graph \nof the system of equations which gen\u00aderalizes topological ordering to directed graphs containing cycles. \nIn particular, we have shown that the hierarchical de\u00adcomposition of a reducible graph [1] obtained by \ncomputing its limit graph is a weak topological ordering, and that any weak topological ordering of a \ndependency graph gives an admissible set of widening points (section 4) as well as two good iteration \nstrategies, that is, algorithms for iteratively and asynchronously solving the system of equations [9]. \nThe first strategy is used to compute intraprocedural fixed points and takes advantage of the fact that \nthe intraproce\u00addural dependency graph is known in advance to achieve an excellent complexity. Theoretical \nresults [5, 6] show that the complexity of this strategy is the product of the height h of the abstract \nlattice by the sum of the individual depths of the n nodes in the decomposition of the graph. The maximum \ncomplexity is thus n when the graph is acyclic, and is at most: hn(n+l) 2 Note that the use of widening \nand narrowing operators cwer the lattice of intervals leads to the same complexity with h = 4v, where \nv is the number of variables, and that h = 4 is a good approximation of the empirical average complexity. \nThe second smategy is used during the interprocedural analysis, for which the dependency graph is not \nknown in advance, and is based on a depth-first visit of the interproce\u00addural call graph. We have shown \nthat the overall complexity of a fixed point computation over a program with n control points, c procedure \ncalls, p procedures and 1intraprocedurai loops is at most: hn(c+p+l) = phn2 where p < 1 is the sum l/n+ \nc/n of the densities of intrapro\u00adcedural loops and procedure calls in the program, and of the inverse \nof the average size n/p of procedures. However, practice shows that complexity is rarely quadratic, except, \nfor programs which consist in tightly coupled mutually recursive procedures or ad-hoc programs such as \nprogram McCarthy of figure 1, which is equivalent, in terms of complexity, to a program with 10 mutually \nrecursive procedures or to a linear program 100 times longuer. program BinarySearch; type index = 1..100; \nvar n: index; key : integer; T: array [index] of integer; function Find(key: integer) : boolean; var \nm, left, right: integer; begin left:= 1; right:= n; repeat m := (left+ right) div 2; if (key < T[m]) \nthen right:= m 1 else left:= m+ 1 until (key = T[m]) or (left > right); Find:= (key = T[m]) end; begin \nread(n, key); writeln( F ound = , Find(key)) end. Figure 3: Binary search  6.4 Implementation Syntox \nconsists of approximately 20.000 lines of C, 4.000 of which implement a user-friendly interface under \nthe X Window system. The system has its own integrated editor shown in figure 2. Once a program has been \nsuccessfully parsed and analyzed, the user can click on any statement and the debugger pops up a window \ndisplaying the abstract memory state right after the execution of this statement. If needed, the window \ncan be dragged to a permanent position on the screen. When a procedure has reference parameters, Synmx \ngives a description of all the possible alias sets of this procedure [3]. Intermittent and invariant \nassertions can be inserted before any statement. The analysis of a program is done in several steps. \nThe first step consists in writing the intraprocedural semantic equations associated with each procedure \nof the program. The forward system of equations directly follows from the syntax of the program, and \nthe backward equations are built by a trivial inversion of the forward system as described in section \n4. The debugger then repeatedly performs a forward analysis and two backward analyses (one for each kind \nof assertion) and stops after a user-selectable number of passes. The default is to perform a forward \nanalysis, two backward analyses and a final forward analysis. Each analysis consists of a fixed point \ncomputation (either a least fixed point or a greatest fixed point) with a widening phase and a narrowing \nphase, which implies that a complete analysis is, in practice, 4 x 4 = 16 times more complex than constant \npropagation. Note that when the program has recursive procedures, the interprocedural call graph is dynamically \nunfolded during the analysis, and each procedure activation is duplicated accord\u00ading to the value of \nits token [3, 4, 5, 19, 23] which consists in the static calling site of the activation and the set of \nall its aliases. When this duplication is too costly in terms of time and memory, it is possible to avoid \nit, at the cost of a loss of precision.  6.5 Results Although the lattice of intervals used by Syntox \nis rather simple, the use of tokens to unfold the interprocedural call graph and the use of widening \nand narrowing techniques allow for the discovery of very subtle and non-trivial bugs. As an example, \nconsider program McCarthy of figure 1. This program implements, for k = 9, a generalization kfck of McCarthy \ns91 function defined by: It can be shown that this function has the following meaning: n 10 ifn> 100 \nMck(n) = 91 if n< 100 { If the invariant assertion n < 101 is inserted at point ~, Syntox proves that \nm = 91 at point @, which shows that if a call MC9 (n) terminates and n < 101, then MCg (n) = 91. Even \nmore interesting, if the intermittent assertion m = 91 is inserted at point Q, then Syntox shows that \na necessmy condition for this property to hold is that n < 101 at point @. Finally, if the correct value \n81 is replaced by any value less than 80, for instance 71, and that the intermittent assertion true is \ninserted at point @, then Syntox shows that a neces\u00adsary condition for the program to terminate is that \nn > 101 at point ~. Experimentally, it can be shown that this erroneous program loops for every value \nn <100 by calling MC9 with a finite set of wguments n. For instance, the call MC9(0) leads to a cycle \nof length 81. Interestingly, this bug was effec\u00adtively discovered while attempting to generalize McCarthy \ns 91 function with an incorrect formula. Another use of Syntox is to prove that every array access in \na program is statically correct, so that a compiler need not generate code to check that array indices \nare correct at run\u00adtime. We have been able to show automatically that every array access is statically \ncorrect in particular implementa\u00adtions of HeapSort and Binary Search (figure 3), and that most accesses \n(i.e. all but one or two) are also correct in other implementations of various sorting algorithms. The \nexper\u00adimental comparison between these programs compiled with and without array bound checking shows \na speed-up ranging from 30% to 40%. These results are clearly superior to previous ones. For in\u00adstance, \nHarrison [17] computes the greatest fixed point of the forward system of semantic equations, which has \nno seman\u00adtic justification and gives poor results (section 3). Moreover, since he does not use a narrowing \noperator, his analysis can be extremely costly. More recently, Markstein et al. [21] use Program Size \nI Memory I Time Fact 24 44 kb 0.5 sec Select 61 64 kb 0.9 sec Ackermann 72 99 kb 1.9 sec QuickSort 92 \n98 kb 2.1 sec HeapSort 96 108 kb 2.4 sec McCarthyg 176 230 kb 5.4 sec McCarthy30 m1184 3387 kb 153.3 \nsec Figure 4: Statistics strength reduction, code motion and subexpression elimina\u00adtion to move range \nchecks outside loops, and Gupta [18] uses monotonicity to improve their results. These works are par\u00adtial \nattempts to perform backward analyses, but they are not semantically founded, and rely on ad-hoc heuristics \nrelated to the way induction variables are computed. On the contrary, our method works without such assumptions, \ncan be applied to arbitrzwy recursive procedures, back-propagates assertions much further, and gives \nbetter results. For instance, every ar\u00adray access in programs Matrix and Shuttle of Markstein et al. \n[21] is statically proven correct by Syntox. Finally, note that Syntox can be used to statically check \nthat variables having range sub-types, such as 1..100, are used consistently. As a matter of fact, range \nsub-types act as permanent invariant assertions and have proven to be very useful for abstract debugging. \nThe time and memory requirements for the abstract de\u00adbugging of programs are reasonable. The table in \nfigure 4 shows the size of different examples (i.e. the total number of control points after having unfolded \nthe interprocedural call graph), the allocated memory in kbytes, and the analysis time in seconds for \na DEC 5000/200 Ultrix workstation. These results show that, in practice, the amount of time and memory \nrequired is closer to the linear case than the quadratic case, except for very complex programs, and \nthere\u00adfore invalidate a common belief according to which static analysis of programs would be exponential. \n 7 Conclusions and future work We have presented a new static, semantic-based approach to the debugging \nof programs, called abstract debugging , that allows programmers to use invariant and intermittent assertions \nto statically and formally check the validity of a program, test its behavior along certain execution \npaths, and find the origin of bugs rather than their occurrences. This method is based on the operational \nabstract interpre\u00adtation framework and cannot be translated to the denotational framework used by systems \nsuch as SPME [25]. We have shown that abstract debugging can be efficiently implemented with a worst-case \nquadratic complexity, and shown that non-trivial bugs can be automatically discovered even when the lattice \nof abstract properties is fairly simple. Finally, we have presented the prototype abstract debug\u00adger \nSyntox which cart be used to debug first-order Pascal programs. The methods we have developed are not \nrestricted to Pascal, and could be easily applied to other safe imper\u00adative languages such as Modula-3 \nor safe subsets of C-I-+, and to functional or logic programming languages. Although we have not tried \nto debug large, real-life pro\u00adgrams with Syntox, all experiments done to date indicate that the time \nand space complexity of abstract debugging lies somewhere between linear and quadratic, and that only \nin\u00adtrinsically complex programs tend to be complex to analyze. We are therefore confident that this technique \ncan be effec\u00adtively applied to reasonably sized, real-life programs. Acknowledgements I wish to thank \nPatrick and Radhia Cousot for their support and helpful comments on this work. References [1]Alfred \nV. Aho, Ravi Sethi and Jeffrey D. Ullrnan: Compilers Principles, Techniques and Tools , Addison-Wesley \nPub\u00adlishing Company (1986) [2] John P. Banning: An Efficient Way to Find the Side Effects of Procedure \nCalls and the Aliases of Variables , Proc. of the 6th ACM Symp. on POPL (1979) 29-41 [3] Fran~ois Bourdoncle: \nInterProcedural Abstract Interpreta\u00adtion of Block Structured Languages with Nested procedures, Aliasing \nandRecursivity , Proc. of the International Workshop PLZLP 90, Lecture Notes in Computer Science 456, \nSpringer-Verlag (1990) 307-323 [4] Frangois Bourdoncle: Abstract Interpretation By Dynamic Partitioning \n, Journal of FunctionaiProgramming, Vol. 2, No. 4 (1992) 407-435 [5] Frangois Bourdoncle: S4mantiques \ndes langages impdmtifs d ordre sup~rieur et interpretation abstraite , P/z.D. disserta\u00adtion, Ecole Polytecbnique \n(1992) [6] FramgoisBourdoncle: Efficient Chaotic Iteration Strategies with Widenings , Proc. of the International \nCon&#38; on For\u00admal Methods in Programming and their Applications, Lecture Notes in Computer Science, \nSpringer-Verlag (1993) to appear [7] Keith D. Cooper: Analyzing Aliases of Reference Formal Parameters \n, Proc. of the 12th ACM Symp. on POPL (1985) 281 290 [8] Patrick andRadhia Couso~ Abstract Interpretation: \naunified lattice model for static analysis of programs by construction or approximation of fixpoints \n, Proc. of the 4th ACM Symp. on POPL (1977) 238-252 [9] Patrick Cousot: Asynchronous iterative methods \nfor solving a fixpoint system of monotone equations , Research Report IMAG-RR-88, Universit6 Scientifique \net M6dicale de Greno\u00adble (1977) [10] Patrick Cousot and Nicolas Halbwachs: Automatic discovery of linear \nconstraints among variables of a program , Proc. of the 5th ACM Symp, on POPL (1978) 84-97 [11] Patrick \nCOUSOL M&#38;hodes it&#38;atives de construction et d approximation de points fixes d op6rateurs monotones \nsur un treillis. Analyse s6mantique de programmes , Ph.D. disser\u00adtation, Universit6 Scientifique et M6dicale \nde Grenoble (1978) [12] Patrick and Radhia COUSOL Static determination of dynamic properties of recursive \nprocedures , Formal Description of Programming Concepts, North Holland Publishing Company (1978) 237-277 \n[13] Patrick Couso~ Semantic foundations of program analysis in Muchnick and Jones Eds., Program Flow \nAnalysis, Theory and Applications, Prentice-Hall (198 1) 303 343 [14] Alan J. Demers, Anne Neirynck and \nPrakash Panrmgaden: Computation of Aliases and Support Sets , Proc. of the 14th ACM Symp. on POPL (1987) \n274-283 [15] Alain Deutsch: On determining lifetime and aliasing of dy\u00adnamically allocated data in higher-order \nfunctional specifica\u00adtions , Proc. of the 17th ACM Symp. on POPL (1990) [16] Alain Deutsch: A Storeless \nModel of Aliasing and its Abstractions using Finite Representations of Right-Regular Equivalence Relations \n, Proc. of the IEEE 92 International Con&#38; on Computer Languages, IEEE Press (1992) [17] William H. \nHarrison: Compiler Analysis of the Value Ranges for Variables , IEEE Transactions on sojlware engineering, \nvol. SE-3, No. 3, (1977) 243-250 [18] Rajiv Gupta A Fresh Look at Optimizing Array Bound Checking , Proc. \nof SIGPLAN 90 Conf, on Programming Language Design and Implementation (1990) 272-282 [19] Neil D. Jones \nand Steven Muchnick: A Flexible Approach to Jnterprocedural Data Flow Analysis and Programs with Re\u00adcursive \nData Structures , in Proc. of the 9th ACM Symp. on POPL (1982) [20] William Landi and Barbara G. Rydec \nPointer-induced Alias\u00ading: A Problem Classification , Proc. of the 18th ACM Symp. on POPL (1991) 93 103 \n[21] Victoria Markstein, John Cocke and Peter Markstein: Opti\u00admization of Range Checking , Proc. of the \nSIGPLAM82 Symp. on Compiler Construction (1982) 114 119 [22] Jan Stransky: A lattice for Abstract interpretation \nof Dy\u00adnamic (Lisp-like) Structures , Information and Computation 101 (1992) 70-102 [23] Micha Sharir \nand Amir Pnueli: Two Approaches to Inter\u00adprocedural Data Flow Analysis in Muchnick and Jones Eds., Program \nFlow Analysis, Theory and Applications, Prentice-Hall (1981) 189-233 [24] Andrew P. Tohnach and Andrew \nW. Appel: Debugging Stare dard ML Without Reverse Engineering , Proc. 1990 ACM Corf on Lisp and Functional \nProgramming, ACM Press (1990) 1-12 [25] G.A. Venkatesh and Charles N. Fishe~ SPARE: A Develop\u00ad ment Environment \nFor Program Analysis Algorithms , IEEE Transactions on software engineering, Vol. 1%, No. 4, ( 1992) \n304-318  \n\t\t\t", "proc_id": "155090", "abstract": "<p>Abstract interpretation is a formal method that enables the static determination (i.e. at compile-time) of the dynamic properties (i.e. at run-time) of programs. We present an abstract interpretation-based method, called <italic>abstract debugging</italic>, which enables the static and formal debugging of programs, prior to their execution, by finding the origin of potential bugs as well as necessary conditions for these bugs not to occur at run-time. We show how <italic>invariant assertions</italic> and <italic>intermittent assertions</italic>, such as termination, can be used to formally debug programs. Finally, we show how abstract debugging can be effectively and efficiently applied to higher-order imperative programs with exceptions and jumps to non-local labels, and present the <italic>Syntox</italic> system that enables the abstract debugging of the <italic>Pascal</italic> language by the determination of the range of the scalar variables of programs.</p>", "authors": [{"name": "Fran&#231;ois Bourdoncle", "author_profile_id": "81100353323", "affiliation": "", "person_id": "PP39069685", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/155090.155095", "year": "1993", "article_id": "155095", "conference": "PLDI", "title": "Abstract debugging of higher-order imperative languages", "url": "http://dl.acm.org/citation.cfm?id=155095"}