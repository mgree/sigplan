{"article_publication_date": "06-01-1993", "fulltext": "\n Interprocedural Constant Propagation: A Study of Jump Function Implementations Dan Grove Sun Microsystems \n2550 Garcia Avenue Mountain View, California 94043 Abstract An implementation of interprocedural constant \nprop\u00adagation must model the transmission of values through each procedure. In the framework proposed \nby Callahan, Cooper, Kennedy, and Torczon in 1986, this intraprocedural propagation is modeled with a \njump function. While Callahan et al. propose several kinds of jump functions, they give no data to help \nchoose between them. This paper reports on a com\u00adparative study of jump function implementations. It \nshows that different jump functions produce different numbers of useful constants; it suggests a particu\u00adlar \nfunction, called the pass-through parameter jump function, as the most cost-effective in practice. Introduction \nProcedure calls have long been recognized as an im\u00adpediment to performance in compiled code. In part, \nthis happens because procedure calls hide informa\u00adtion. Information about the environment that a pro\u00adcedure \ninherits and about the side effects of proce\u00addures that it invokes is unavailable to the optimizer. Interprocedural \ndata-flow analysis attempts to over\u00adcome this problem by propagating information about data usage and \ncreation among the procedures in a program. The compiler can then take advantage of contextual information \nto generate better code for This research was supported by DARPA through ONR Grant NOO014-91-J-1989. \n E-mail addresses: Dan Grove: dgrove@eng.sun. corn Linda Torczon: torczontlkice.edu Permission to copy \nwithout fee all or part of this material is granted provided that the copies are not made or distributed \nfor direct commercial advantage, the ACM copyright notice and tha title of the publication and its date \neppear, and notica is given that copying is by permission of the Association for Computing Machinery. \nTo copy otherwise, or to republish, requires a fee and/or specific permission. ACM-SIG PLAN-PLDl-6/93/Albuquerque, \nN.M. @ 1993 ACM 0-89791 -598 -4/93 /0006 /0090 . ..$1 .50 Linda Torczon Department of Computer Science \nRice University Houston, Texas 77251-1892  any single procedure. In particular, a compiler can generate \nbetter code for a procedure if it knows at compile time which parameters and global variables will have \nconstant values, and what those values will be, when the procedure is invoked. These constants are called \ninterprocedural constants. Interprocedural constants have been used to im\u00adprove the results of analysis \nor optimization in a va\u00adriet y of different situations, including computation of dependence information \n[14], automatic paralleliza\u00adtion of sequential routines [9, 15], and procedure cloning [6, 13]. They \nalso improve many scalar opti\u00admization, including intraprocedural constant propa\u00adgation, dead code elimination, \nand certain loop trans\u00adformations. Shen, Li, and Yew examined the improvement in dependence information \nthat is obtained when the values of interprocedural constants are known [14]. In their experiment, they \nperformed the interproce\u00addural constant propagation manually and added as\u00adsertions to the program text \nabout constant variable values. In their examination of FORTRAN library routines, they found that knowledge \nof interprocedu\u00adral constants significantly decreased the number of nonlinear subscript references. They \nfound that ap\u00adproximately 50 percent of the subscripts which had previously been considered nonlinear \nwere found to be linear in the presence of interprocedural constant information. Because many dependence \nanal yzers are incapable of analyzing nonlinear subscripts, this is an important application of interprocedural \nconstants. Another arena in which interprocedural constants are valuable is automatic detection of parallelism. \nEigenmann and Blume suggested that interprocedu\u00adral constants are often used as loop bounds [9]. Loop \nbounds are important for two reasons. First, they improve dependence information, as described above. \nSecond, knowing their values allows the compiler to make informed decisions about the profitability of \nparallel execution the number of iterations executed T any A any Al c~ A c~Acj=l T cj = =-L = any &#38;i \nif if Ci Ci = # Cj Cj C.-n <  Figure 1: The constant propagation lattice. by a particular loop is an \nimportant factor in deter\u00admining both the amount of work it represents and the number of processors that \nit can profitably employ. Several techniques for discovering interprocedural constants have been proposed \n[3, 4, 13, 16] (See Sec\u00adtion 5, Other Work ). The study presented in this paper is based on the work \nof Callahan et al. [4]. They suggest several jump functions for their frame\u00adwork. Those jump functions \nvary in complexity and in the class of interprocedural constants that they are expected to propagate. \nThis study examines the tradeoff between the complexity of the jump func\u00adtions used and the number of \ninteger constants de\u00adtected when interprocedural constant propagation is performed on scientific FORTRAN \ncodes. Empirical results are presented. The remainder of this paper is divided into five sections. Section \n2 outlines the framework employed for interprocedural constant propagation. Section 3 describes the jump \nfunctions examined. Section 4 discusses both the results of the study and the imple\u00admentation used. Section \n5 sketches related research. Section 6 presents our conclusions. Interprocedural Constant Propaga\u00adtion \n The goal of interprocedural constant propagation is to discover, for each procedure p in the program, \nthe set of constants that always hold on entry to p. In data-flow terms, we want to annotate p with a \nset CONSTANTS(p) containing (name, value) pairs. A pair (z, v) 6 CONSTANTS(p) indicates that z always \nhas value v when p is invoked. This set conservatively approximates reality; each pair in CONSTANTS(p) \ndenotes a run-time constant, but not all run-time con\u00adstants can be found (for example, values read from \na file may be combined to form a constant that propag\u00adates through the program). For the purpose of computing \nCONSTANTS(p), we formulate the problem in a lattice theoretic frame\u00adwork. With each entry point p, we \nassociate a func\u00adtion that maps formal parameters of p into elements of the constant propagation lattice \nL. Each element of L is one of three types: the lattice top element (T), a constant value c, or the lattice \nbottom element (1). The lattice s structure is shown on the right of Figure 1. It is defined by the rules \nfor the lattice meet operation (A). These rules are shown on the left of Figure 1. Here, an y denotes \na valid element of L. The lattice, although infinite, has bounded depth. In particular, the value associated \nwith some formal parameter x can be lowered at most twice. For any formal parameter z, let VAL(X) represent \nthe best current approximation to the value of z on entry to the procedure. After the analysis is com\u00adplete, \nif VAL(Z) = L, the analysis was not able to determine that z is constant. (z may or may not be constant. \nIf z is constant, the analyzer was unable to discover it.) If V L(~) = c, z has the constant value c. \nThe value T is used as an initial approximation for all parameters; z retains the value T only if the \nprocedure containing z is never called. Given VAL(Z) for each of p s formal parameters, CONSTANTS(p) \nis simply the set of formal parameters for which VAL is equal to some constant value. Interprocedural \nconstant propagation is performed on a call graph, G. Each node in G represents a pro\u00adcedure p and each \nedge (p, q) represents a call from p to q. Interprocedural constant propagation has two principal parts: \n(1) modeling the transmission of val\u00adues through each procedure and (2) propagating in\u00adformation in VA.L \nsets around the call graph. 1To simplify the presentation, we extend the definition of a parameter to \ninclude global variables. Recall that formal parameter refers to the name used inside the called procedure. \nThe reference in the parameter list at the call site is an actual parameter. To model the flow of constant \nvalues through a pro\u00adcedure p, we associate with each call site s a set of forward jump functions, J:. \nFor an actual parameter y, J: gives the value of y at s as a function of p s formal parameters, where \ns is a call site in p. The support of .J# is the exact set of p s formal parame\u00adters whose values on \nentry to p are used in the com\u00adputation of J;. We denote the support set of J! as support(J$ ). Procedures \ncan also return constant values. To ac\u00adcount for this effect, we must model the transmission of constant \nvalues from within a procedure back to the call site that invoked it. With each procedure, we associate \na set of return jump functions. For each formal parameter c that may be modified by p, a re\u00adturn jump \nfunction R; is created to compute the best approximation to a s value on return from p. The support of \nthe function R: is the exact set of formal parameters of p whose values on entry to p are used in the \ncomputation of R;. As before, we denote the support set of R; as support. Given the call graph G and \nthe forward and return jump functions for each procedure, we can propagate the VAL sets around the graph \nusing a standard it\u00aderative technique [11, 12]. The results presented in this paper were computed using \na simple worklist it\u00aderative scheme. Alternative formulations based on the binding multi-graph are possible \n[7]. The method presented by Callahan et al. essentially models the binding graph computation on the \ncall graph [4]. Callahan et al. showed a single simple example to demonstrate that different jump function \ntechniques produced different results. They fail, however, to pro\u00advide any experimental evidence that \nsuggests where the tradeoff lies between jump function complexity and the precision of the CONSTANTS \nsets. This pa\u00ad per examines the effectiveness of several jump func\u00ad tions. From this comparative data, \nwe can assess the value of spending additional compile time in jump function evaluation. Jump Functions \nTo study the tradeoffs between compile time and pre\u00adcision that arise in an implementation, we built \nan analyzer that can use one of several different jump functions. Our choice of implementation was driven \nby the goals set for our study: understanding the tradeoff between more complex jump functions and the \nnumber of interprocedural constants discovered. Thus, we chose an implementation strategy that dif\u00adfers \nfrom the one described by Callahan et al. We built a set of jump functions on top of an existing framework \nfor global (intraprocedural) value number\u00ading [1]. Callahan et aL envisioned constructing jump functions \nbefore any interprocedural data-flow anal\u00adysis is performed. We found it convenient to simu\u00adlate the \nvarious jump functions on top of global value numbering after the computation of interprocedural summary \nsets (MOD and REF). Section 3.1 describes the four forward jump functions that we considered and the \ncosts associated with each jump function. Section 3.2 describes the single return jump function that \nwe implemented. 3.1 Forward Jump Functions The four forward jump functions vary both in the complexity \nof their construction and in the class of interprocedural constants that they can detect, In increasing \norder of complexity, they are: the literal constant jump function, the intraprocedural constant jump \nfunction, the pass-through parameter jump function, and the polynomial parameter jump func\u00adtion. Note \nthat the set of constants propagated by one of these forward jump functions is a subset of the constants \npropagated by the more complex models that follow it. Each of the forward jump functions selected for \nthis study can be constructed as initial information be\u00adfore the interprocedural propagation phase begins. \nIt is not necessary to reconstruct the jump functions on each iteration over G. For simplicity in defining \nthese forward jump functions, let gcp(y, s) represent a func\u00adtion, computed prior to the interprocedural \nconstant propagation phase, that returns the constant value of parameter y at call site s when that value \ncan be determined with intraprocedural constant propaga\u00adtion or value numbering coupled with interprocedural \nMOD information. 3.1.1 The Literal Constant Jump Function The literal constant jump function is defined \nas the following: if y is the literal constant c at call J:= c site s 1 otherwise { Because it does \nnot take into account the values of incoming formal parameters, this forward jump func\u00adtion misses opportunities \nto propagate interprocedu\u00adral constants through the procedure body. As a re\u00adsult, interprocedural constants \ncan only be propa\u00adgated over a single edge in G. The only interproce\u00addural constants propagated by this \nmethod are those that appear as a literal constant at a call site. (There\u00adfore, this jump function misses \nany constant globals which are passed implicitly at the call site. )  3.1.2 The Iniraprocedural C onstant \nJump Function 3.1.5 Costs of Forward Jump Functions The intraprocedural constant jump function is de\u00adfined \nin the following manner: if gcp(y, s) = c J:= c J_ otherwise{ Like the literal constant jump function, \nthis forwi~rd jump function ignores any values known on entry to the procedure for incoming formal parameters. \nThus, it will only propagate an interprocedural constimt along a single edge in G. Because it uses intraproce\u00ad \ndural constant propagation to produce the constants propagated at the call sites, it can potentially \nde\u00ad tect more interprocedural constants than the literal constant jump function. It can also detect constant\u00ad \nvalued global variables. 3.1.3 The Pass-through Parameter Jump Function The pass-through parameter jump \nfunction is defined as the following: c if gcp(~, s) = c if y= zat sand zisaformal pa- J:= ~ rameter \nof the calling procedure { 1-otherwise This jump function detects situations where a formal parameter \nis passed unmodified through the proce\u00addure body to a call site where it is used as an ac\u00adtual parameter. \nBecause this forward jump function allows interprocedural constants to be propagated through a procedure \nbody, it can propagate interpro\u00adcedural constants along paths of length greater than one in G. 3.1.4 \nThe Polynomial Parameter Jump Function The polynomial parameter jump function is defined in the following \nmanner: c if gcp(y, s) = c if y can be represented J; = .f(support(J$)) w a Polynomial func\u00adtion ~ of \nszJpport(Jj ) I -1 otherwise This jump function takes a more comprehensive approach towards propagating \ninterprocedural con\u00adstants through a procedure body than the pa.ss\u00adthrough parameter jump function. Actual \nparanne\u00adters are represented aa polynomial functions of the incoming values of the formal parameters \nof p when such a polynomial function can be constructed. .All standard integer operations are supported \nby the rou\u00adtines which generate and evaluate these jump func\u00adtions. The four forward jump functions vary \nboth in the cost of construction and the time required for solv\u00ading the interprocedural problem. The \nliteral con\u00adstant jump function is constructed with a minimal amount of effort a textual scan of the \ncall sites provides all the required information. Building the other three forward jump functions involves \nsome in\u00adtraprocedural analysis, so they are more expensive to construct. Our implementation used an SSA-based \nvalue numbering scheme to generate these jump func\u00adtions. Building the SSA graph for a procedure typ\u00adically \nrequires O(N) steps, where IV is the size of the procedure (the maximum of the number of nodes, edges, \nvariable assignments, or variable references in the procedure) [8, page 487]. While the time required \nfor construction of these three jump functions is simi\u00adlar, the complexity of the data structures used \nby the polynomial parameter jump function is significantly greater than for either the intraprocedural \nconstant or pass-through parameter jump function. The time required for the interprocedural propa\u00adgation \nstep also varies among these jump functions. The analysis has three distinct cases. 1. The literal constant \nand intraprocedural con\u00adst ant jump functions can only propagate con\u00adstants along paths of length one \nin the call graph. Thus, the interprocedural propagation can be solved in 0(~~ ~Y cost(Jj )) time, where \ns ranges over all call sites, y ranges over all pa\u00adrameters at each call site, and cost (JJ ) is the \ncost of evaluating J:. For the intraprocedural constant jump function, y includes both actual parameters \nand global variables. 2. The pass-through parameter jump function can pass constants along arbitrary \npaths in the call graph. Because the constant lattice is shallow, each value can be lowered at most twice \n from T to a constant to -L. When a formal parame\u00adter s value is lowered, each actual parameter that \nuses its value must be re-evaluated. However, each actual parameter depends on exactly one formal parameter. \nTaken together, these facts suggest that the interprocedural propagation can be completed in O(X$ EY \ncost(J# )) time. Calla\u00adhan et al. present an algorithm that achieves that time bound [4]. The implementation \nused in our experiment waa less efficient. Note that cost(~~ ) is greater for the pass-through jump function \nthan for the literal constant and intraprocedu\u00adral constant jump functions.  3. The polynomial parameter \njump function also passes constants along arbitrary paths in the call graph. Each element in support(J# \n) can be low\u00adered at most twice, resulting in the re-evaluation of J:. Thus the time required for interprocedural \npropagation is [4]: O(Z ~ cost(Jg) . lsuppo7+(J:)l) s Y In practice, we found that the number of com\u00adplex \npolynomial jump functions actually con\u00adstructed is small. Taken over the program, cost(J# ) approaches \nthe cost of pass-through pa\u00adrameter jump functions and lsuppoti(J# ) I ap\u00adproaches 1. These effects allow \nthe solution time for the interprocedural constant propagation to approach that of the simpler forward \njump func\u00ad  3.2 The Return Jump Function To understand the value of return jump functions, we implemented \na single type of return jump function. It is similar to the forward polynomial jump function. The values \ncomputed for return jump functions are calculated during an initial bottom-up pass through the call graph. \nInterprocedural MOD information, re\u00adturn jump functions from routines already analyzed, and intraprocedural \nconstants are used in the con\u00adstruction of each return jump function polynomial. For simplicity in defining \nthis return jump function, let rgcp(z, p) represent a function, computed prior to the interprocedural \nconstant propagation phase, that returns the constant value of formal parameter x after invocation of \np when that value can be de\u00adtermined with intraprocedural constant propagation coupled with interprocedural \nMOD information and information provided by return jump functions that have already been computed for \ncall sites within p. Formally, the return jump function is defined as fol\u00adlows: c rgcp(z, p) = c I if \nx can be represented R; = as a polynomial func\u00ad dwwo~(R;)) tion g of support(R~) \\ (O- therwise Because \nan SSA-based value numbering scheme is used to generate return jump functions, the amount of time required \nto construct them is typically O(iV) the same as that needed for the polynomial parameter forward jump \nfunction. The data structures needed in this stage are also identical to those required by the polynomial \nparameter jump function. Each re\u00adturn jump function is evaluated exactly twice at each call site. It \nis first evaluated during the generation of return jump functions for the calling procedure. This is \ndone in order to expose as many return jump functions as possible in the calling procedure. The second \nevaluation of the return jump function takes place during the generation of forward jump func\u00ad tions. \nDuring this phase, any return jump function that cannot be evaluated as constant using intrapro\u00ad cedural \ninformation coupled with other return jump function values is set to -1-. The effect of this limi\u00ad tation \nis that return jump functions that depend on parameters to the calling procedure can never be eval\u00ad uated \nas constant. 4 study To study the relative effectiveness of the jump func\u00adtions described in Section \n3, we built an analyzer that implements each of them and used it to analyze a collect ion of FORTRAN \nprograms. This allows a fair comparison of the set of constants discovered with each jump function. We \nlimited the implementation in two ways. 1. The implementation only propagates integer constants. There \nare two principal reasons for this. First, we were most interested in discover\u00ading constants that affect \ncontrol flow. These in\u00adclude loop bounds, loop strides, and conditions that control if-then-else statements. \nSecond, compile-time evaluation of floating-point num\u00adbers is problematic, particularly so in an envi\u00adronment \nlike ours where the compiler and appli\u00adcation may run on different architectures. 2. The analyzer does \nnot try to track constants in and out of arrays. Dependence analysis might al\u00adlow the analyzer to track \nsome constant-valued array elements. We elected not to attempt this because the information from interprocedural \nconstant propagation is used as input to the de\u00adpendence analyzer. Thus, using dependence in\u00adformation \ninside a jump function would either introduce a phase-ordering problem or require iterative application \nof dependence analysis and interprocedural constant propagation.z Any ref\u00aderences to array elements are \ninitialized to 1.  2VJebelieve that tracking the flow of values in and out of FORTRAN arrays would yield \nfew additional interprocedural constants. These limitations are in line with the goals of our study \n to compare the sets of integer constants disc\u00adovered with the various jump functions described in Section \n3. 4.1 Implementation The interprocedural constant propagation system used in this study was built on \ntop of an SSA-based value number graph [8] and the call graph abstrac\u00adtion from ParaScope, a set of tools \ndesigned to aid in interactive parallelization of FORTRAN programs [5]. Its execution proceeds in four \nstages: generation of return jump functions, generation of forward jump functions, interprocedural propagation \nof constants, and recording the results. Generating return jump functions. In the first phase, a return \njump function for each formal lpa\u00adrameter modified by a procedure is calculated in a bottom-up walk over \nthe call graph. At each nc)de in the call graph, the analyzer builds an SSA gralph for the corresponding \nprocedure [8]. To determine which parameters can have constant values on return from the routine, we \nvalue number the SSA gralph. The value numbering pass incorporates information provided by already-computed \nreturn jump functions for subroutines invoked by the current routine. After the return jump functions \nhave been built, the SSA graph and the value number graph are discarded to conserve memory. The data \nstructures which hold the expressions representing the return jump func\u00adtions are stored for later use. \nGenerating forward jump functions. In the second phase, forward jump functions are generated for every \ncall site. In a top-down pass over the call graph, the analyzer builds the SSA graph and the value number \ngraph for each routine. This analy\u00adsis uses the return jump function expressions built in the first stage. \nEach parameter associated with a call site receives a value number. The value numbering proceeds in the \nstandard way; it can build an arbi\u00adtrarily complex representation for an arithmetic ex\u00adpression. The \nanalyzer then constructs an expression tree to represent the parameter s jump function from the expression \nproduced by value numbering.3 The resulting expression tree is converted into a context\u00adindependent representation \nand stored in the list of jump functions for the parameters of this call site. Our implementation was \nintended for experimenta\u00ad tion. Of course, simpler representations are possible 3 The user sPecifie~ \nwhi&#38; forward jump function is desired. The appropriate function is constructed from the information \nproduced by value numbering. for the literal constant, intraprocedural constant, and pass-through parameter \njump functions. For exam\u00adple, the pass-through parameter jump functions can be easily derived from either \nan SSA graph or DEF-USE chains. They can be represented with a simple map from actuals to formals and \nglobals at each call site. A production implementation should consider such simpler represent ations. \nInterprocedural propagation of constants. Once forward jump functions have been built, the compiler must \nperform interprocedural propagation. Rather than build a specialized solver for this prob\u00adlem, we used \nParaScope s interprocedural data-flow solver. Meet and transfer functions were written for this solver; \nthey evaluate the jump functions. The solver uses an iterative method. Even with this less efficient \nsolver, the problems converged quickly. In our implementation, the cost of intraprocedural anal\u00adysis \ndominates the cost of the interprocedural phase. The meet function is called, in turn, for every pa\u00adrameter \nalong each edge entering a procedure. Meet combines the values (produced by jump functions) flowing from \neach call site with the current values of the parameters in the procedure. Meet is defined as shown in \nFigure 1; any non-constant jump function evaluates to -L. Once the meet has been performed, the values \nof constant parameters must be transmitted to call sites within the node. If an incoming parameter which \nwas previously evaluated as non-constant is now found to be constant, all non-constant parameters at \neach call site within that procedure are re-evaluated. This pro\u00adcess uses a symbolic expression evaluator \nto interpret the expression tree that represents the jump function. It uses all available information \nabout incoming pa\u00adrameter values. If this process finds that a previously non-constant actual parameter \nis now constant, the iterative data\u00adflow analyzer makes another pass over the call graph. This process \ncontinues until no new constants are found during an entire iteration. At this point, the CONSTANTS sets \nfor the nodes are stable, and the data-flow solver halts. Recording the results. The CONSTANTS sets are \nwritten to a single file. Optionally, the analyzer can produce a transformed version of the original \nsource in which the interprocedural constants are textually substituted into the code. The numbers reported \nin the next section count the number of constants that this option substituted into each program. Metzger \nand Stroud suggest that this is the right measurement of effectiveness [13]. In their experience, FORTRAN \nI Program Line< II m II I matrix300 I ocean II 1728 I simple II 805 I snasa7 II 696 spec77 2904 65 31 \n45 trfd 401 8 40 50 Table 1: Characteristics ofprogram test suite. procedures often have constant-valued \nglobal vari\u00adables that are known but irrelevant that is, they are not referenced inside the procedure. \nThis mea\u00adsure relates more directly to code improvement; it also fact ors out procedure length and modularity. \n 4.2 Results To evaluate the relative effectiveness of the jump functions, we chose a set of scientific \nFORTRAN pro\u00adgrams. We elected to evaluate the SPEC and PER-FECT benchmark suites. Because ParaScope does \nnot current ly support multiple entry points, some of the PERFECT suite had to be removed from consideration.4 \nSome characteristics of the programs which were tested are shown in Table 1. In general, the programs \nare of small to medium size, with a fairly high degree of modularity. The line counts ex\u00adclude comments \nand and blank lines. With the excep\u00adtions off pppp and simple, a fairly even distribution of code throughout \nthe procedures in each program was found (this is shown by the closeness of the mean lines per procedure \nand the median lines per proce\u00addure in each program). In these two programs, the distribution of the \nlines per procedure was skewed a single routine made up a large part of the code in f pppp and simple. \nWhile these programs may not be representative of all styles of scientific computing, the two suites \nof programs are well known and widely used in studies like this one. Table 2 shows the number of constants \ndiscovered and actually substituted into the code by the ana\u00ad 4Because the multiple entry points in ocean \nwere never called, they were eliminated in order to include ocean in the study. lyzer using each of \nthe jump functions. The first four columns show the numbers of constants obtained by using both forward \nand return jump functions. The final two columns show the results for polynomial and pass-through jump \nfunctions without a return jump function. In our test suite, the polynomial and pass\u00ad through parameter \ntechniques found the same set of constants. Both outperformed the intraprocedural jump function. The \nintraprocedural jump function performed better than the literal jump function. Return jump functions \nmade no noticeable differ\u00adence in ten of the thirteen programs. In doduc and mdg, return jump functions \nlet the analyzer find a few more constants. In ocean, the return jump func\u00adtions more than tripled the \nnumber of constants that the analyzer found. By recognizing that the initial\u00adization routine at the start \nof ocean resulted in the assignment of constant values to many variables, the analyzer was able to propagate \nadditional constants to routines throughout the program. Table 3 shows several different techniques. \nThe first column shows results from a propagation using polynomial parameter forward and return jump \nfunc\u00adtions and no MOD information. The second column of numbers is identical with the first column in \nTa\u00adble 2. The third column shows the result of a (com\u00adplete propagation. To obtain these numbers, the \nanalyzer was run with polynomial parameter forward and return jump functions, using MOD information. \nAfter each run, dead code elimination was performed. If any dead code was found, the propagation was \nperformed again from scratch all of the values in CONSTANTS sets were reset to T.5 The final column \nshows the results of performing a purely intraproce\u00addural constant propagation on each procedure in the \nprogram. No constants were propagated between pro\u00adcedures, but interprocedural MOD information was used \nduring the intraprocedural propagation. To assess the import ante of MOD information, we eliminated it \nfrom both the initial information and the jump functions. Comparing the results (see col\u00adumn 1) against \nthose obtained with MOD informa\u00adtion (see column 2), we see a substantial difference. In any program \nwhere constants were found, using MOD information exposed additional constants. The numbers are particularly \nstriking in adm, linpackd, matrix300, ocean, simple, and spec77. Without MOD information, the value numbering \npass (see Sec\u00adtion 4.1) had to use worst case assumptions about any call sites. Thus, the presence of \nany call in a routine eliminated potential constants along paths 5In each case, only one pass of dead \ncode elimination was needed. The second propagation did not expose any newly dead code. Using Return \nJump Functions No Return Jump Functions Program Polynomial Pass-through Intraprocedural Literal Polynomial \nPass-through adm 110 110 110 110 110 110 doduc 289 289 289 288 287 287 fpppp 60 6(I !54 49 56 56 linpackd \nIi () 170 170 94 170 170 matrix300 138 138 122 71 138 138 mdg 41 41 4(I31 40 40 ocean 194 194 194 57 \n62 62 qcd 180 180 180 180 180 180 simple 183 183 179 174 183 183 snasa? 336 336 336 254 336 336 spec?? \n137 137 137 104 137 137 trfd 16 16 1616 16 16 Table 2: Constants found through use of jump functions. \nPolynomial Polynomial Compieie Intraprocedural Program without MOD with MOD Propagation Propagation adm \n25 110 110 105 doduc 288 289 289 3 f pppp 34 60 60 38 linpackd 33 170 170 74 matrix300 18 138 138 69 \nmdg 3141 41 31 ocean 79 194 204 56 qcd 169 180 180 179 simple 2 183 183 174 snasa7 303 336 336 254 spec77 \n76 137 141 83 trfd 10 16 16 15 Table 3: Comparison of most precise jump function with other propagation \ntechniques. leaving the call site, even when no modification oc\u00adcurred. The data indicates that incorporating \nMOD information significantly increases the number of con\u00adstants that can be detected in scientific FORTRAN \ncodes. Incorporating interprocedural constants into a rou\u00adtine can expose segments of code that can never \nbe executed. Removing this dead code can poten\u00adtially eliminate conflicting definitions of variables \nand expose additional constants. Combining dead code elimination with interprocedural constant propaga\u00adtion \nin the complete propagation (see column 3) exposed few additional constants. The additional ef\u00ad fort \nof a complete propagation does not appear jus\u00adtified by the additional constants found. In fact, the \nresults that we obtained in this study with complete propagation can be achieved by bas\u00ading the jump-function \ngenerator on a gated single\u00adassignment form [2, 10]. An analyzer based on gated single-assignment form \nwould never consider the dead assignments that we found in the com\u00adplete propagations This would let \nthe standard polynomial jump function produce the results seen with complete propagation. Finally, the \nresults of an intraprocedural constant propagation are presented to show the number of con\u00adst ants that \nit found. For fair comparison, MOD infor\u00admation was used in the intraprocedural propagation. For programs \nthat contained constants, the interpro\u00adcedural propagation (see column 2) always detected more constants \nthan strictly intraprocedural propa\u00adgation (see column 4). In some cases, the difference was substantial. \nIn scientific FORTRAN codes, inter\u00adprocedural constant propagation exposed many con\u00adstants not found \nby the intraprocedural propagation. Other Work Several researchers have proposed techniques for propagating \ninterprocedural constants. Wegman and Zadeck propose combining procedure integration with intraprocedural \nconstant propagation to detect inter\u00adprocedural constants [16]. Because procedure integra\u00adtion makes \npaths through the program s call graph explicit, the interprocedural information computed along a particular \npath may be improved. By com\u00adbining the results of the improved analysis with dead code elimination, \nit may be possible to eliminate some paths through the program. Data is not yet available to indicate \nwhether or not the proposed algorithm 6Note that information from return jump functions is used during \nthe construction of the gated single-assignment graph. Hence, a limited number of interprocedural constants \nmay be available for this construction. would perform efficiently in practice Callahan, Cooper, Kennedy, \nand Torczon present an interprocedural constant propagation framework that consists of an algorithm for \npropagating con\u00adstants across call boundaries and a family of meth\u00adods, called jump functions, used to \npropagate the constants through procedure bodies [4]. Both inter\u00adprocedural constants passed to called \nprocedures and those returned by a called procedure can be detected using this technique. Because this \ntechnique does not make paths through the call graph explicit, it poten\u00adtially detects fewer constants \nthan the method pro\u00adposed by Wegman and Zadeck. This technique has been implemented in several compilation \nsystems and has proved to be efficient in practice. Burke and Cytron propose a similar approach for computing \ninterprocedural constants within a uni\u00adfied framework that integrates dependence analysis with interprocedural \nanalysis [3]. Intraprocedural constant propagation is employed to determine which constants are available \nfor propagation at call sites. Interprocedural constants are propagated from call sites to the called \nprocedure and used as initial infor\u00admation in the intraprocedural constant propagation. The method described \ndoes not handle returned con\u00adst ants or make paths through the call graph explicit. Metzger and Stroud \nimplemented interprocedural constant propagation in the CONVEX Application Compiler based on ideas presented \nin Callahan et al. [13]. They used interprocedural constants to guide procedure cloning. Their empirical \nresults indicate that goal-directed cloning of procedures based on in\u00adterprocedural constants can substantially \nincrease the number of interprocedural constants available for use by later analysis and optimization \npasses. Experience with the CONVEX Application Compiler haa shown that interprocedural constants can \nbe used to increase the effectiveness of intraprocedural constant propa\u00adgation, dependence analysis, \ndead code elimination, loop strip mining, loop interchange, and interproce\u00addural optimization. 6 Conclusions \nWe have examined the effectiveness of four for\u00ad ward jump functions and one return jump function. The \npass-through and polynomial parameter forward jump functions were equivalent in the number of con\u00adstants \nthat they found; in practice, the former tech\u00adnique should prove less expensive to build and eval\u00ad uate. \nIncorporating MOD information is important; it led to the discovery of many additional constants. Incorporating \ndead code elimination into the process did not expose many additional constants. Scientific FORTRAN \nprograms use constants that cannot be detected with intraprocedural constant propagation. Our study showed \nthat interprocedu\u00adral constant propagation can discover such constants, even with simple jump functions. \nThe difference be\u00adtween jump function implementations is significant. Acknowledgements We would like \nto thank the members of the Para-Scope programming environment group at Rice, pilr\u00adticularly Paul Havlak \nand Mary Hall, for providing the infrastructure used in this study. We would alko like to thank Keith \nCooper and Preston Briggs jfor their constant encouragement and support. References [1] Bowen Alpern, \nMark N. Wegman, and F. Ken\u00adneth Zadeck. Detecting equality of variables in programs. In Conference Record \nof the Fifteenth Annual ACM Symposium on Principles of Pro\u00adgramming Languages, pages 1 1 1, January 1968. \n[2] Robert A. Ballance, Arthur B. Maccabe, and Karl J. Ottenstein. The program dependence web: A representation \nsupporting control, data, and demand-driven interpretation of imperative languages. SIGPLAN Notices, \n25(6):257-271, June 1990. Proceedings of the ACM SIGPL14N 90 Conference on Programming Language 17e\u00adsign \nand Implementation. [3] Michael Burke and Ron Cytron. Interprocedural dependence analysis and parallelization. \nSIG-PLAN Notices, 21(7):162-175, July 1986. Pro\u00adceedings of the ACM SIGPLAN 86 Symposium on Compiler \nConstruction. [4] David Callahan, Keith D. Cooper, Ken Kennedy, and Linda Torczon. Interprocedural constant \npropagation. SIGPLAN Notices, 21(7):152-161, July 1986. Proceedings of the ACM 5 IGPL.4N 86 Symposium \non Compiler Construction. [5] Keith D. Cooper, Mary W. Hall, Robert T. Hood, Ken Kennedy, Kathryn McKinley, \nJc)hn Mellor-Crummey, Linda Torczon, and Scott K. Warren. The ParaScope parallel programming environment. \nProceedings of the IEEE, Febru\u00adary 1993. (to appear). [6] Keith D. Cooper, Mary W. Hall, and Ken Kennedy. \nProcedure cloning. In Proceedings of the IEEE Computer Society 1992 International Conference on Computer \nLanguages, pages 96\u00ad105, April 1992. [7] Keith D. Cooper and Ken Kennedy. Interpro\u00adcedural side-effect \nanalysis in linear time. SIG-PLAN Notices, 23(7):57-66, July 1988. Proceed\u00adings of the ACM SIGPLAN 88 \nConference on Programming Language Design and Implemen\u00adtation. [8] Ron Cytron, Jeanne Ferrante, Barry \nK. Rosen, Mark N. Wegman, and F. Kenneth Zadeck. Effi\u00adciently computing static single assignment form \nand the control dependence graph. ACM Trans\u00adactions on Programming Languages and Sys\u00adtems, 13(4):451-490, \nOctober 1991. [9] Rudolf Eigenmann and William Blume. An ef\u00adfectiveness study of parallelizing compiler \ntech\u00adniques. In Proceedings of the 1991 International Conference on Parallel Processing, St. Charles, \nIL, August 1991. [10] Paul Havlak. Interprocedural Symbolic Analysis. PhD thesis, Rice University, expected \nMay, 1993. [11] Matthew S. Hecht. Flow Analysis of Computer Programs. Programming Languages Series. Else\u00advier \nNorth-Holland, Inc., 52 Vanderbilt Avenue, New York, NY 10017, 1977. [12] Gary A. Kildall. A unified \napproach to global program optimization. In Conference Record of the ACM Symposium on Principles of Program\u00adming \nLanguages, pages 194 206, October 1973. [13] Robert Metzger and Sean Stroud. Interprocedu\u00adral constant \npropagation: An empirical study. (To appear in ACM Letters on Programming Languages and Systems). [14] \nZhiyu Shen, Zhiyuan Li, and Pen-Chung Yew. An empirical study of FORTRAN programs for parallelizing compilers. \nIEEE Transactions on Parallel and Distributed Systems, 1(3):356-364, July 1990. [15] Jaswinder P. Singh \nand John Hennessy. An empirical investigation of the effectiveness of and limitations of automatic parallelization. \nIn Proceedings of the International Symposium on Shared Memory Multiprocessors, Tokyo, Japan, April 1991. \n[16] Mark N. Wegman and F. Kenneth Zadeck. Con\u00adst ant propagation with conditional branches. ACM Transactions \non Programming Languages and Systems, 13(2):181 210, April 1991. \n\t\t\t", "proc_id": "155090", "abstract": "<p>An implementation of interprocedural constant propagation must model the transmission of values through each procedure. In the framework proposed by Callahan, Cooper, Kennedy, and Torczon in 1986, this intraprocedural propagation is modeled with a <italic>jump function</italic>. While Callahan <italic>et al.</italic> propose several kinds of jump functions, they give no data to help choose between them. This paper reports on a comparative study of jump function implementations. It shows that different jump functions produce different numbers of useful constants; it suggests a particular function, called the <italic>pass-through parameter jump function</italic>, as the most cost-effective in practice.</p>", "authors": [{"name": "Dan Grove", "author_profile_id": "81100575934", "affiliation": "Sun Microsystems, Mountain View, CA", "person_id": "P58058", "email_address": "", "orcid_id": ""}, {"name": "Linda Torczon", "author_profile_id": "81332513281", "affiliation": "Rice Univ., Houston, TX", "person_id": "PP40035980", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/155090.155099", "year": "1993", "article_id": "155099", "conference": "PLDI", "title": "Interprocedural constant propagation: a study of jump function implementation", "url": "http://dl.acm.org/citation.cfm?id=155099"}