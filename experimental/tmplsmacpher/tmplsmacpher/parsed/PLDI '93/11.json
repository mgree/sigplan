{"article_publication_date": "06-01-1993", "fulltext": "\n Communication Optimization and Code Generation for Distributed Memory Machines Saman P. Amarasinghe \nand Monica S. Lam Computer Systems Laboratory Stanford University, CA 94305 Abstract This paper presents \nseveral algorithms to solve code generation and optimization problems specific to machines with distributed \naddress spaces. Given a description of how the computation is to be partitioned across the processors \nin a machine, our algorithms produce an SPMD (single program multiple data) program to be run on each \nprocessor. Our compiler generates the necessary receive and send instructions, optimizes the communication \nby eliminating redundant communication and aggregating small mes\u00adsages into large messages, allocates \nspace locally on each proces\u00adsor, and translates global data addresses to local addresses. Our techniques \nare based on an exact data-flow analysis on indi\u00advidual array element accesses. Unlike data dependence \nanalysis, this analysis determines if two dynamic instances refer to the same value, and not just to \nthe same location. Using this information, our compiler can handle more flexible data decompositions \nand find more opportunities for communication optimization than sys\u00adtems based on data dependence analysis. \nOur technique is based on a uniform framework, where data decompositions, computation decompositions \nand the data flow information are all represented as systems of linear inequalities. We show that the \nproblems of communication code generation, local memory management, message aggregation and redundant \ndata communication elimination can all be solved by projecting polyhedra represented by sets of inequalities \nonto lower dimen\u00adsional spaces. 1 Introduction Compiling to machines with a distributed address space \ncan be separated into two major phases. The first phase determines how to decompose the computation and \ndata across the processors. The goal is not just to parallelize the applications, but also to ensure \nthat the programs have data locality so as to minimize the commu\u00adnication overhead. This step is common \nto all large-scale parallel machines with non uniform memory access times. If the machine has a distributed \naddress space, then the compiler is faced with the additional problem of managing the memory and communication \nexplicitly. The second phase of the compilation is to generate the code so that each processor will execute \nits allotted computation and communicate correctly and efficiently. These This research was supported \nin part by DARPA contract DABT63-91-K\u00ad0003, an NSF Young Investigator Award and a fellowship from Intel \nCor\u00adporation. Permission to copy without fee all or part of this material is granted provided that the \ncopies are not made or distributed for direct commercial advantage, the ACM copyright notice and the \ntitle of the publication and its date appear, and notice is given that copying is by permission of the \nAssociation for Computing Machinery. To copy otherwise, or to republish, requires a faa and/or specific \npermission. ACM-S lGPLAN-PLDl-6/93 /Albuquerque, N.M. @ 1993 ACM 0-89791 -598 -419310006101 2 6... $ \n1.50 algorithms can also be useful to enhance the memory system per\u00adformance of machines with a shared \naddress space. Many systems being developed rely on the user to supply the data decompositions. Languages \nsuch as High Performance FOR-TRAN[24], FORTRAN-D[16] and Vienna FORTRAN[8] allow the programmer to annotate \nthe sequential program with data decompositions. The compiler is responsible for generating the computation \ndecomposition and an SPMD (Single Program Multi\u00adple Data) program with explicit communication. We are \ndeveloping a compiler system that automatically parallel\u00adizes a sequential program for shared and distributed \nmemory machines. The input language to our compiler is sequential FOR-TRAN-77, although many of the techniques \ndeveloped are also applicable to optimizations within and especially across FOR-TRAN-90 statements[14]. \nOur data and computation decomposition phase tries to maximize parallelism and minimize communication[3]. \nIf all the available parallelism cannot be exploited without communication, this phase first tries to \ntrade off excess degrees of parallelism to eliminate communication. The algorithm understands doacross \nparallel\u00adism, where processors are organized as a pipeline and they syn\u00adchronize and communicate during \nthe course of the computation. The algorithm will choose to use the doacross form of parallel\u00adism if \nsuch a choice minimizes the overall communication cost. Finally, if necessary, the algorithm will reorganize \nthe data dynam\u00adically; it tries to insert data reorganizations into the least frequently executed parts \nof the program. The decomposition phase generates a full specification of the com\u00adputation decompositions. \nIt also specifies the data decompositions before and after major data reorganizations; it does not generate \ndata decompositions for those sections of code using doacross form of parallelism, where data are often \nallocated to different pro\u00adcessors at different times. Data reorganizations such as matrix transposes \nare implemented using collective communication routines[18]. The algorithms pre\u00adsented in this paper \nfocus on generating the code and communica\u00adtion between reorganizations. Within each region of code requiring \nno major data reorganization, there may still be fine-grained com\u00admunication, Sections of the arrays \nmay be replicated and allocated to different processors at different times. The given data decompo\u00adsitions \nare used as specifications for the initial and final layouts to each code region. The algorithm uses \nthe computation decomposi\u00adtion to guide the local memory management process. Whhin each region, our algorithm \nmay change the data layout according to the demands of the given computation decompositions. The data \nand computation decomposition schemes that a user can specify in languages like High Performance FORTRAN \non dense matrices is a subset of those generated by our compiler. Thus, the techniques described in this \npaper are also applicable to generating code from user-supplied data decompositions. Given a computation \ndecomposition, and an initial and a final data decomposition, the techniques described in this paper \nautonnati\u00adcally produce an SPMD (single program multiple data) progralm to be run on each processor. \nOur compiler generates the necessary receive and send instructions, optimizes the communication by eliminating \nredundant communication and aggregating small mes\u00adsages into large messages, allocates space locally \non each process\u00ador and translates global data addresses to local addresses. This paper presents three \nmain results. First, we propose a value\u00adcerrtric approach to deriving the necessary communication for \nmachines with a distributed address space. Previous approaches are locah on-cerrtric communication is \nderived from data decom\u00adposition optimizations are performed using data dependencc[5], an analysis that \ndetermines if accesses may refer to the same loca\u00adtion. We derive our code generation from computation \ndecomposi\u00adtions using a data-flow analysis technique that is based on values instead of locations. This \napproach enables a more general set of data and computation decompositions and allows for more com\u00admunication \noptimization. Second, we have developed a mathematical model for code gener\u00adation which is applicable \nto both the location-and value-centric approaches. We represent data decompositions, computations decompositions, \nand communication as systems of linear inequali\u00adties. We show that the various code generation and communication \noptimization problems can be solved by projecting the polyhedra represented by systems of inequalities \nonto lower dimensional spaces. Third, we have developed several communication optimization within the \nvalue-oriented framework. These optimizations include eliminating redundant messages, aggregating messages, \nand hiding the communication latency by overlapping the communication with computation. These optimizations \nare essential to achieving an acceptable performance on distributed memory machines[22]. The organization \nof our paper is as follows. In Section 2, we describe the conventional way of generating communication \ncode from user-specified data decompositions and some of the limita\u00adtions of the approach. In Section \n3, we describe our value-centric approach to communication generation. We formally describe the domain \nof our technique and present a mathematical representation of the problem in Section 4. We describe our \ncode generation tech\u00adnique and our communication optimizations in Sections 5 and 6. We finally close \nwith a detailed example to illustrate the entire code generation process,  2 A Location-Centric Approach \n Many of the existing compilers developed for distributed memory machines have a similar basic approach \nin how they generate code from user-specified data decompositions[4] [9][16][21][22] [26]. The FORTRAN-D \ncompiler appears to be the most mature; it employs techniques such as data dependence analysis and regular \narray sections for communication optimizations, and substantial experimentation results have been reported. \nIn the following, we first review the techniques used in the FORTRAN-D compiler. We then discuss some \nof the limitations of this approach to motivate the need for more advanced analysis and optimization. \n2.1 The Basic Technique For simplicity, in the following discussion we assume that there is only one \nloop nest which contains one read access and one write access to the same array. The argument obviously \nholds for the general case with multiple loops and arrays. There are three domains that are manipulated \nin the compilation process: the itera\u00adtion space S, the array elements space A, and the processor space \nP Each array access function in the source program specifies the data used by each iteration in the loop. \nThat is, each read or write access function, denoted by fr, fm: S -A respectively, maps an iteration \nto the array indices of the data read or written. The user\u00adspecified data decomposition D:A -D@ maps \neach array location to a processor. From the read and write access functions and the data decomposition, \nthe compiler automatically derives the computa\u00adtion decomposition C: S + P which maps each iteration \nin the loop to a processor. To derive the computation decomposition, the compiler applies the owner-computes \nrule: each assignment statement is performed by the processor that owns the data. Therefore, given a \nwrite access function fw and a data decomposition D, the computation decom\u00adposition is C = Dfw Under \nthe owner-computes rule, no communi\u00adcation is needed to impIement the write accesses. Communication is \nneeded for a read access in iteration i if the data read is resident on a different processor, i.e. Ci \n* Dfri. The relationships between iteration space, array space, and the processor space are shown in \nFigure l(a); processor p, receives data from processor p, if P,* P,. Array Space A  I ff Internal External \nLocation-centric Value-centric (a) (b) Figure 1: Different approaches to code generation for distributed \nmemory machines. To minimize the communication cost, the compiler tries to maxi\u00admize the intervals between \ncommunication, All the data needed within the interval are sent in one message. This optimization is \nbased on data dependence analysis. IWO accesses are data depen\u00addent if one of the accesses is a write \noperation, and if they may refer to the same location, The dependence is carried at level k if the dynamic \ninstances accessing the same location belong to the same iteration of the first k -1 outermost loops \nbut not the kth loop. The maximum level of a dependence between two references is simply the maximum \nloop nest level that carries a dependence between the references. If the maximum depth of all dependence \ninvolving a read access is k, the compiler needs to communicate only once in each iteration of the kth \nloop. Thus, the maximum depth information is useful for reducing the communication fre\u00adquency. All the \ndata accessed within the interval requiring commu\u00adnication are summarized by a regular section description[15]. \nIn this way, the same data used multiple times within the interval are only transferred once. In summary, \nthis approach deduces the computation decomposi\u00adtions from the user-specified data decompositions, using \nthe owner-computes rule; it uses data dependence analysis to reduce the number of message$ finally, it \nuses the concept of regular sec\u00adtions to reduce redundant data transfers. 2.2 Limitations of Existing \nApproaches While the approach described above can handle many regular dense matrix computations, there \nare still many aspects in which this approach can be improved. 2.2.1 Data Decompositions and the Owner-computes \nRule Implicit in the use of the owner-computes rule is the assumption that iterations writing to the \nsame location must be executed by the same processor. This is unnecessarily restrictive. Consider the \nfol\u00adlowing example: fori=Otondo forj=l tondo X[i, O] = X[i, O] + X[i, j] Suppose the array X has been \ndistributed as blocks of columns to facilitate the parrdlelization of other phases. The best way to paral\u00adlelize \nthis code is to pipeline the execution of the second loop. That is, the variable X[i,O] passes from processor \nto processor, and each processor contributes the sum of its section of the ith row to the value of X[i,O]. \nIt is easy to express the computation decomposi\u00adtion of this parallelization scheme. It is not possible \nto achieve this scheme by specifying a data decomposition, unless we relax the owner-computes rule. The \nowner-computes rule also assumes that the written data can\u00adnot be replicated; i.e. data decompositions \nmap each written loca\u00adtion to a unique address. Consider the following example: fort= Otomdo for i=1to \nn-1do X[i] =1/3* (X[i] +X[i -1]+X[i +1]) A typical parallelization is to partition X into contiguous \nblocks, and assign to each processor the computation of one of these blocks. Since the range of data \nread extends beyond the block of data written, the borders of the blocks are often replicated on adja\u00adcent \nprocessors. Such data decompositions, however, would render the owner-computes rule inoperative, since \nthe written data are replicated on multiple processors. The use of the owner-computes rule plays a major \nrole in simplify\u00ading the generation of communication code. The sender of any data is always the processor \nthat owns the data. If we relax the owner\u00adcomputes rule, deciding which processor has the desired value \nat any one time is much harder. If we were to replicate the written data, or move the written data dynamically, \nor choose not to com\u00adpute at the owner s site, we would not be able to just derive the sender of the \ndata from the data decomposition. 2.2.2 Data Dependence Analysis Data dependence analysis is an alias \nanalysis. A read operation is dependent on a write operation as long as they may refer to the same location, \neven if none of the written values are used by any of the read instances[5]. The lack of value information \nmay reduce the opportunities for parallelism and communication optimization. Consider the followiog example: \nfor i=Oto 100do for j=Oto 100do work[j] = ... (s1) forj = Oto 100 do . = work[j] (s2) The data dependence \nanalyzer does not know if a write operation covers, or kills, an earlier write operation. Data dependence \nanaly\u00adsis will yield a level 1 dependence between statements S1 and S2 because the locations written \nin each outer iteration of the loop (work[O:100]) overlap with the locations read (work[O:100]). This \ninformation would serialize the outermost loop. On analyzing the flow of the data, it is obvious that \ndata written within each iter\u00adation in the outer loop affect only the computation within the same iteration. \nThis outer loop can be parallelized by giving each pro\u00adcessor a private copy of the work array. This \noptimization, known as array privatization, has been shown to be essential to parallelize many real programs \nsuccessfully[6]. Data dependence analysis does not distinguish between the differ\u00adent instances of the \naccesses. The analyzer only knows that a dependence exists; it does not know which pairs of instances \nare dependent. When using data dependence information for commu\u00adnication optimizations, the maximum depth \nof the dependence determines the communication interval, All the non-local data accessed within the interval \nmust be communicated, even though some of the data may not participate in the data dependence at the \nsame maximum depth. Thus, the same value may be transferred multiple times unnecessarily. Consider the \nfollowing example: fori=O tondo X[i] = ... (s1) forj=itondo Y[j] = Y[j] + X[j -1] (s2) Suppose matrices \nX and Y are distributed across the processors by blocks. Using the owner-computes rule, statement S1 \nin iteration i of the outer loop is executed by the owner of X[i]; iteration j of the inner loop is executed \nby the owner of Yti]. The data dependence between S1 and S2 indicates that communication is needed in \neach iteration of the outermost loop. Processors executing the second loop may need to make one non-local \ndata reference. However, except for the processor executing the first iteration of the inner loop, none \nof the non-local data accessed has changed its value from the last time the loop was executed. Thus, \nat most one word needs to be transferred in each iteration of the outermost loop. Data dependence analysis \ncannot identify the instances that carry the dependence. Thus, if only data dependence information is \navailable, we must conclude that communication is needed for every processor executing the inner loop. \nFinally, compilers that use data dependence analysis to aggregate messages are limited by the resolution \nof the data dependence analysis. Since only the loop level information is available, the communication \nmust necessarily take place at the iteration bound\u00adaries of the loop carrying the dependence. Potentially, \nif the com\u00adpiler knows the precise iteration that produces the data to be communicated, we can send the \ndata to the receiver as soon as all the data in a message have been generated. Such optimization is significant \nas it reduces the chances a processor is stalled waiting for data to arrive. 2.2.3 Regular sections \nThe contents of a message are defined by the regular section descriptor that encompasses all the non-local \narray elements touched between communication points. This technique may cre\u00adate additional traffic when \nthe set of data accessed cannot be pre\u00adcisely described by a regular section. Consider the following \nsimple example: for i=1to 100do forj =ito 100do ...= A[1OOO i + j] Representing the data accessed as \na regular section descriptor would increase the amount of communication by a factor of 20.   3 Our \nApproach We have been exploring the use of a more sophisticated analysis technique that determines if \ntwo accesses refer to the same value, and not just to the same location. In the following, we describe \nthe information obtained from the analysis and show how this can be used to eliminate the limitations \nof data dependence analysis dis\u00adcussed above. 3.1 Accurate Data-Flow Analysis Instead of data dependence \nanalysis, our optimization are based on finding exact data-flow information on individual instances of \narray accesses. Let us use the simple example in Figure 2 to illus\u00adtrate our technique. fort= Oto Tdo \nfori=3to Ndo X[i] =X[i -3] Figure 2: A simple 2-deep loop nest. Data dependence analysis on this program \nwill produce the depen\u00addence vectors {[+, 3], [0, 3]}, meaning that the read access in itera\u00adtion [t,, \ni,] may be data dependent on all iterations [ ~W,iw] such that iW -i, -3, and twstr.Perfect data flow \nanalysw, however, is able to determine precisely that the first three iterations of the innermost loop \nread data defined outside the loop, and the rest of the iterations use the value defined three iterations \nearlier,, i.e. [tW, iw] = [f,, i,-3]. The problem of finding precise array data-flow information was \nfirst formulated by Feautrier[lO][ll] [12]. Feautrier proposed a parametric integer programming algorithm \nthat can find such per\u00adfect information in the domain of loop nests where the loop bounds and array indices \nare affine functions of loop indices. We have developed an algorithm that can find the same precise information \nfor many common cases more efficiently[19] [20]. The perfect data-flow information is captured by a representation \nwe called a Last Write Tree (LWT). The LWT for the above e:~am\u00adple is shown in Figure 3. The leaf Ml \nrepresents the first three iter\u00adations that read data X[O:2], whose values are not generated by this \nprogram. The leaf M2 represents the rest of the iterations where the data read are written three iterations \nearlier. Ost, sT 3si, sN true i,z6% Jib true Dependence false Level tw= t, M2 2 im = ir-3 LMl o Figure \n3: The Last Write Tree for the example in Figure 2. For simplicity, our discussion below assumes that \nthere is on~y one loop nest and one write access in the program. The LWT algorithm can handle multiple \nwrite operations and multiple loop nests in general. The LWT is a tinction that maps an instance of a \nread operation to the very write instance that produces the value read, provided such a write exists. \nWe denote a read and a write instance by the values of their loop indices, ;r and iv, respectively. The \ndomain clf the LWT function is the set of read iterations ~, that satisfy the con\u00adstraints imposed by \nthe loop bounds. Each internal node in the tree contains a further constraint on the value of the read \ninstance ;,. It partitions the domain of read instances into those satisfying the constraint, represented \nby its right descendants, and those not satis\u00adfying the constraint, represented by its left descendants. \nWe clefine the context of a node to be the set of read iterations that satisfy the constraints of a node \ns ancestors. The tree is constructed such that either all the values read within the context of a leaf \nare written within the loop, or none of them are. In the former case, the leaf node defines a last-write \nrelation that relates each read instance ~r within the context to the write instance ;U that produces \nthe value read. AU the read-write pairs share the same dependence levels. If the instances within a context \ndo not read any value written within the loop, we denote the lack of a writer by J-. The LWT information \ndiffers from the data dependence analysis in two major ways. First, LWTS can distinguish between the \ndifferent instances of the same array access. For example, the LWT analysis can determine that the first \nthree iterations of the inner loop in Fig\u00adure 2 have different dependence relationships than all the \nother iterations. Second, the LWT is a function that specifies precisely the last write instance that \ngenerates the value read by a particular read instance. Data dependence analysis, on the other hand, \ncannot discriminate between writes to the same locations.  3.2 Using LWTS in a Distributed Memory Compiler \nWe have developed an array privatization algorithm based on the LWT analysis[19]. In this algorithm, \nparallelization is based on only the data-flow dependence generated by the LWT informa\u00adtion. If the parallelized \nloop carries any anti-dependences or output dependence, then privatization is necessary. On machines \nwith a shared address space, the compiler needs to create a private copy of the data for every processor. \nThe private copies may need to be initialized and the final results may be written back to the original \narray. On machines with a distributed address space, the privatized array in a processor s local memory \nis no different from any other data allocated in its local memory. By generating communication only for \nthe data-flow dependence, writes irrelevant to the compu\u00adtation on other processors will not affect the \nother processors. Besides enhancing parallelization by enabling optimizations such as array privatization, \nLWTS are especially useful for efficient code generation for distributed memory machines, as explained \nbelow. 3.2.1 Deriving Communication from LW Ik There are two kinds of leaves in an LWT L leaves that \ndo not read any of the values written within the code being analyzed, and leaves that do. In the former \ncase, the compiler can simply load all the non-local data onto a processor before executing any of the \ncode. Given a computation decomposition and an initial data decomposition produced by an earlier compiler \nphase, the tech\u00ad nique to generate the necessary communication code is no differ\u00adent from that used in \nthe location-centric approach. Communication and computation are more tight-coupled for the other kind \nof leaves. The LWT specifies all the pairs of iterations that share a producer and consumer relationship. \nBy applying the computation decomposition function on the related iterations, we can derive the identity \nof the processors that write and read the same value. If the writer and reader are different processors, \nthen communication is necessary. Our technique is depicted in Figure l(b). The data decompositions generated \nfrom the earlier compiler phase serve only as interfaces with other sections of the program. In general, \nwe generate the necessary communication from LWTS and computation decompositions, and not data decompositions. \nOur algorithm will change the data layout when called for by the computation decompositions. Thus, our \ncompiler can support a wider range of data decompositions. Locations written to can be replicated or \nmapped to different processors across time. Further\u00admore, our algorithm does not rely on the restriction \nof the owner\u00adcomputes rule. 190 3.2.2 Communication Optimization Using the LWT information, we can \neasily eliminate redundant data transfers. While accessing the same location may require mul\u00adtiple data \ntransfera since the value at the location may or may not have changed, each value needs to be transferred \nonce and only once. Moreover, the perfect producer and consumer information enables the compiler to issue \nthe send immediately after the data are produced and to issue the receive just before the data are used. \nThis maximizes the chances that the communication is overlapped with computation. Even when a source \nprogram appears to be very simple, handling the iterations at the boundary of the loops properly can \nbe tricky. The LWT analysis automatically partitions the read instances into sets that share similar \ncommunication characteristics. This parti\u00adtioning not only makes generating correct code routine, it \nalso enhances optimizations. For example, all iterations of each context in an LWT have the same dependence \nlevel, and read instances requiring communication within a loop are separated from those that do not. \nThe uniformity in each of the contexts allows us to develop simple and powerful algorithms to optimize \nthe communi\u00adcation. Without perfect information, the compiler may need to treat all instances uniformly \nand conservatively, and observe the most stringent of the requirements for all the instances.  4 Problem \nFormulation In this section, we formally define the scope of our technique. We show how we can represent \nall the information useful for commu\u00adnication and computation code generation as sets of linear inequal\u00adities. \nThis model and our techniques discussed in Section 5 are useful for both value-centric and location-centric \napproaches. 4.1 The Problem Domain The scope of our technique is limited to programs consisting of a \nset of loop nests, where the bounds of the loop nests are affine expressions of outer loop indices and \nsymbolic constants. The array accesses are also affine functions of loop indices and sym\u00adbolic constants. \nOur technique can also handle conditional state\u00adments that do not contain any loops. Each assignment \nwithin the conditional statement is treated as an unconditional assignment; depending on the outcome \nof the condition, it assigns to the vari\u00adable either the newly computed value or the variable s current \nvalue. We can handles loops of the following form: for il = ll(Z) to hl(ti) do for i2 = 12(;, il) to \nhz (5, il) do .... for in = ln(fi, il, .... in-l) to hn(fi, il, ....in_l) do where fi is a symbolic constant \nvector (variables unchanged within the loop) and /k, hk are affine functions. The iteration set of a \nloop is thus -. ik21k(v, ) 21, . . ..tk_1 I = j= (ii,...,in) 63 Vk= 1,..,, n -, ikshk(v, ~1, ..., ) { \nk-l } The index set of a data array with dimensions Ul, .... Um is A -{ti= (al, .... am) CJl I V/c= \n1,..., tn Osak<uk}. we read and write access functions, ~, = (j,,.. .,fr~) and fro= (fm,!..,,fwm)are \naffine functions of the form f (i 11,...,in) = (al, .... am) , where (il, .... in) Cl, (al, .... am) \nG A and fi is a symbolic con\u00ad stant vector. To support cyclic decompositions where data or computation \nare distributed to processors in a round-robin manner, we introduce the notion of a virtual processor \narray. The computation and data decompositions map the computation and data to the virtual pro\u00ad cessor \nspace. Let Ul, .... Uq be the dimensions of the virtual pro\u00ad cessor space, the index set of this virtual \nprocessor array is thus P = {p= (pI,..., pq)=5?l Vk=l, ..,, qOsp~<uJ. Our physical processor array has \nthe same number of dimensions as the virtual processor array. Let u ~, .... u ~ be the physical pro\u00ad \ncessor array dimensions, U ks Uk,Vk = 1, .... q. The physical pro\u00ad cessor index set is P = {~= (Pal,..., \nq)q) ET lVk=l,..., q OSpk<U k}, The kth dimension of data elements or loop iterations are distrib\u00aduted \nacross the physical processors in a cyclic manner whenever u k < u ~. The mapping from the virtual to \nthe physical processor space, n: ~ ~ ~ is defined as ~ (;) = ~ where Vk = 1>..)q P k = pkmodu k. Since \nit is only in the latter stages of the optimizations will the compiler be operating in the physical processor \nspace, we will sim\u00adply refer to virtual processors as the processors.  4.2 Data Decompositions Definition \nZ: The data decomposition relation D is a set of array element and processor pairs (ii, ~), such that \n(~, ~) G D iff the processor ~ has a copy of the array element Z. Data decompositions can be written \nas u(fi-i) >(; +Bil)p-;l D= (~,j) 6Ax P {} LI(ii-i) <(t+ Bii)(p+l)+ijh where U is an extended unimodular \nmatrix, t, d,, d~, b, are integer vectors, B is an int:ger matrix an-d j is a vector of symbolic constants \nsuch that b + Bii * O and d[, dk z O. Our scope of data decompositions is larger than those typically \nused in existing distributed memory machine compilers. The matrix U determines if the array is reversed \nor skewed. When the array have more dimensions than that of the processor space, the O columns of the \nextended unimodular matrix U choose the dimen\u00adsions to be mapped on the same processor. The entire array \ncan be shifted with respect to the processor array using the integer vector i. Since the data block size \nis often a function of the number of processors engaged in the computation, it is useful not to deter\u00admine \nthe block size at co~pile time. We can handle some symbolic block sizes of the form b + B ii; the scope \nof our technique is dis\u00adcussed in Section 5.1. The overlap of array elements between pro\u00adcessors is determined \nby vectors d,, d~ >0. Figure 4 illustrates how we can use this scheme to describe several common data \ndecompo\u00adsitions. The 2 x 2 grid in each example represents the first 2 x 2 processors in the system; \neach paoel is a picture of the entire data array, and the shaded portion represents the data allocated \nlocally to that specific processor.  4.3 Computation Decompositions Computation decompositions have \na similar scope as data decom\u00adpositions, except that an iteration can only be mapped onto one processor. \nDefinition 2: The computation decomposition relation C is a set of iteration and. processor pairs (~, \n~) such that processor ~ 130 ttrecutes iteration ~ iff (~, ~) G C, Computation decompositions can be \nwritten as c= {( f,p)GIx P I (i+ Bfi)psu(i-i)<(~+Ijti)(p+i)} where U is an extended unimodular matrix, \nt, b are integer vectors, B is an in(eger matrix and ii is a vector of symbolic constants such that b \n+ Bii >0.  I i a, ii~ .   ,,;.,,,.,,, , , $,,,.,;,: ,,.;,, ,., ,,,: ,, , .. .,,,*. y,-, ,, , $ I ,$ \n, ,: ,,? ,, . $,,;,+;,: ,, , ~,,, .;: .,,, ., , , , , ,-, ., , ,.,, .,, -,j,,,. , , ,,. 10 ooo .. ,., \n, ,,, ,,:;,y; , , , N $:!.,,,. . .. . . . ~,, ,,,;:,, , 01 00 0 N ,;, /$ ;,, ,,. ,,,,, ,,, . . f, , , \n,,: ,,, , ; ,: YB (a) Full replication. L...............~l................l L&#38;d 10 150o 1 e,,,, \n!.:,. 01 00 0N %&#38;:....! . .* ...... ... .$. .... .... .. .... .... .. (c) Square blocks with symbolic \nblock sues, shijled right by 1. 1-1 8 1oo 21 16 -100 (d) Skewed rectangular blocks. Figure 4: Examples \nof some data decompositions for an NxN array onto a 2-dimensional processor space.  In our compiler, \ncomputation decompositions are generated auto\u00admatically by an earlier phase[3]. For systems that rely \non the user to specify the data decompositions, Theorem 1 shows how to derive computation decompositions \nfrom data decompositions. Theorem 1: Assuming that written data are not replicated the computation decomposition \nas derived from data decomposition D, using the owner-computes rule, is Cm {(~,@~IxP13a~A s,t. (ti, @G \nDAa=~w(i,;). 4.4 Communication We define the communication between processors formally as a communication \nset: Definition 3: A communication set M is a set of elements ( ir, 7,,1,, F,, ii) GIx Pxlx PxA, where \n(;,, ~,, i,, ~,, i) EM iffprocessor ~~needs to send the va~ in location z in iteration j~ to processor \npr for use in iteration tr. 4.4.1 Data Decompositions and the Owner-computes Rule . If we use the owner-computes \nrule, no communication is necessary for write operations. We use Theorem 2 to find all the necessary \ncommunication for each read access within the loop nest. We use the user-specified data decomposition \nto find the owner of the data rea~ we use the computation decomposition derived from Theo\u00adrem 1 to find \nthe processor reading the data; if these two processors are not the same, communication is needed. Theorem \n2: The communication set required by the access fUnCtiOn f fOr a set of iterations L under computation \ndecomposition C and data decomposition D is the set of elements -ii) G1x PxIx PxA,( ~p p,> l,? p~> where \n(;,,~r) =C, (~,~,) CD, ir~L, i = jr(ti, ir), i$ = ;,, ~,#~r.  4.4.2 Computation Decompositions and \nLast Write lkees Definition 4: An LWTpartitions the iteration set of a loop nest into the contexts of \nits leaves. If the values read by the iterations in a context L G I are written within the loop, then \nthe context has a last-write relation p. The last-write relation p of the context L is a set of iteration \npairs (~,, ~J such that (~,, ~J G p iff ;, G L and ~~G 1 is the iteration that generates the value read \nin iteration ;,, A contert L can be written as {~ G 1 \\ j (;, ~) ah} and a read\u00adwrite relation p can \nbe written as {iipi,) eIx I[7(ti, ir*is) =6}) where ~ and q are vectors of ajj%e expressions. It is sometimes \nnecessary to introduce auxiliary variables so that the last-write relations can be represented as linear \ninequalities. Some of the read-write relations produced by the LWT analysis algorithm are expressed as \ni = B (mod a) or i x ~ (mod a) , where a, ~ are integers. We can introduce an auxiliary variable u, and \nrewrite these relations as i -F = au and au < i -13 c au + a, respectively. If a context of an LWT reads \ndata written within the loop, then communication is needed only if the iterations sharing the read\u00adwrite \nrelation are executed by different processors (Theorem 3). If the context of an LWT uses data written \noutside the loop, then we use the initial data decomposition to determine the owner of the data (Theorem \n4). Theorem 4 is similar to Theorem 2, except that the sends can precede the computation of the loop \nsince the values needed are not generated within the loop. Theorem 3: The communication set that satisfies \nthe last-write relation p under computation decomposition C is the set of elements (;r,fir,;~,~~,i) GIXPxIxp \nx~ yhere (ip ~r-)$ (k? ~,) EC, (!,>@ ~~, ii -jr(;, ir) -js (Z, i~), ps*Pr. Theorem 4: The communication \nset required by an access jintction ~ within a 1-context L of an L~ under computation decomposition C \nand an initial data decomposition D, is the set of elements (;r, jr, ;$,j~,;) G 1x P x 1 x P x A, where \n(ipp,) cc, (ip,) GD, I, EL, a = j,(ti, i,), 1, = 0, F,*F,. Communication decompositions, data decompositions, \niteration contexts, access functions and last-write relations can all be expressed as systems of linear \ninequalities. The ~~ *F, constraint, however, cannot be expressed as a conjunction of inequalities. We \nbreak down the inequality into a set of disjunctive conditions. For example, for a one-dimensional processor \narray, the constraint p, e p, is represented by p,> p, v p,< p,. We represent the neces\u00adsary communication \nas a set of communication sets, with each one satisfying all the other inequalities and one of the disjunctive \ncon\u00additions. Suppose the second loop in our program in Figure 2 is distributed as blocks of 32 iterations \nacross a linear array of processors. That is, processor executes iteration [t, i] iff f 32p s [o ~ <32(p+l) \ni [1 (We will use this computation decomposition throughout the rest of the paper.) Figure 5 shows the \ncommunication sets for context M2 from-Figure 3.  trzo T-tr>O ir-320 N-i, aO Context  ir-6a0 f,-tr>o \ntr-t, =o i~-ir+3z0 ir-i, -3z0 Access function ir-3-a20 a-ir+3z0 Computation decomposi\u00ad ir-32prz0 32p, \n+31-i, zO tion for read iterations Computation decomposi\u00ad i,-32p, >0 32p, +31-i, >O tion for write iterations \nConstraint p,* p, P,> P, P,< P, Figure 5: Inequalities defining the communication sets for context M2 \nin Figure 3. \u00ad 4.4.3 Finalization Data produced within the loop nest may need to be written back to their \nhome locations in the final data layout. The problem of identifying which written values are live at \nexit is a sub-problem in calculating last write trees[19]. The set of inequalities generated by this \nsub-problem, in conjunction with the final data distribution, defines the communication set for finalization. \n 5 Code Generation Before we describe how to generate the computation and commu\u00adnication code, we first \nreview the techniques of projection and scanning a polyhedron in Section 5.1. We then show how to gener\u00adate \na working program. We first generate the code for each com\u00admunication set individually, merge these code \nfragments into an SPMD program, and manage the local memory on each processor. Optimizations are discussed \nin the next section. 5.1 Projection We can represent all possible values of a set of variables (v~, v~, \n. . .. Vn) G Zn as an n-dimensional discrete cartesian space, where the kth axis corresponds to variable \nv~. Coordinate [xl, .... XH] G Zn corresponds to the value VI = xl, ....vn=xn. A set of linear inequalities \nin (Vl, Vz, .... Vn) defines a polyhedron S in this n-dimensional space. All the solutions satisfying \nthe ine\u00ad qualities correspond to the integer points within the polyhedron. Suppose we project the polyhedron \nonto the n-l dimensional sub\u00ad space orthogonal to the axis representing variable Vfl. The result\u00ad n-l \ning polyhedron in the (n -1)-dimensional subspace, S , can be represented by a set of linear inequalities \ninvolving (vpv~, . ..>vn_~) ~ If [xl, .... Xn] E Sn then [xl, ,.., Xn-~] = Sn-1. However, given [xl, \n....Xn-~] G S -1, there mayor may not exist an X. such that x ] G S Consider the example where S has \na single .. . ~~;lt~a;n;mvolv;ng v : VI = 2vn. We know that VI must be even. However, this c~nstraint \nis not captured in the projected polyhedron S -1 and VI can be an odd number in Sn -1. Projection of \nan n-dimensional polyhedron onto an (n -1 )-dimen\u00adsional space can be achieved using a single step of \nFourier-Motz\u00adkin elimination[23]. Fourier-Motzkin elimination can produce a large number of superfluous \nconstraints. We can determine if a constraint is superfluous as follows. We replace the constraint in \nquestion with its negation, and if the new system does not have an integer solution then the constraint \nis superfluous. To check if a system has an integer solution we again use Fourier-Motzkin elim\u00adination. \nSince the Fourier-Motzkin elimination algorithm checks if a real solution exists for a system, a branch-and-bound \ntechnique is needed to check for the existence of integer solutions[23], We have extended the technique \nof projection to handle some sim\u00adple non-linear inequalities so that we can handle symbolic block sizes. \nWe allow the coefficients in the linear inequalities to be of the form *( bO+blul+ . . . +bmum) where \n&#38;o, . . .. bm>O are inte\u00adgers and Ul, .... Um >0 are symbolic constants. The scope of our technique \nis limited to those cases where the result of the projec\u00adtion also has coefficients that are linear combinations \nof symbolic constants. This is an important extension because it enables the compiler to handle symbolic \nblock sizes in data and computation decompositions; otherwise, the value of the blocks would have to \nbe determined at compile time.  5.2 Scanning a Polyhedron Definition 5: Given ~,~ E Z , ~ is lexicographically \nless than ~ iff there exists ks n such that VO s 1< k il = jl and ik<jk. Given a system of linear inequalities \n,S with unknowns (V*, V2, ..C, vn ), we wish to create a 100P nest that will enumerate all the solutions \nto S in lexicographic order, The desired code is in the form of an n-deep loop nest. The index of the \nkth outermost loop is v~; it is incremented by one every iteration, and has a finite lower and upper \nbound. The loop bounds are expressions of sym\u00adbolic constants in the code and outer loop indices. This \nloop struc\u00adture guarantees that the indices of the iterations executed are in lexicographic order, The \nproblem that remains is: what should the bounds of the loops be such that the loop nest contains an iteration \nwith indices [xl, ,.., Xn] iff [xl, ,,., Xn] is a solution to S ? Ancourt and Irigoin showed that the \nproblem can be done by a series of projections 1] [2]. In the following, we briefly describe their algorithm, \nthen discuss a minor domain. We find the bounds of the loop nest bounds for Vn, we rewrite the c;vns/%l,...,crn_l \n) and c~vnzlr qualities not involving Vn need not be considered here. The integer lower and upper bounds \nfor Vn are given simply by: extension that is useful in our in reverse order. To findl the constraints \nin the form of (VI, . . ..vn.l). Any ine\u00ad tion code for the communication sets in Figure 5. Note that \nno communication is necessary when p~ > pr. To generate the receive and send code for a communication \nset M, we scan M lexicographically in (~,, ~,, 7,, ~,, ~) a~d (~,, ~,, ~,, ~,, ii) order, respectively. \nIn the recewe loop nest, the ~, loops enumerate the processors involved in receiving data. The t, loops \nspecify the iterations when processor F, needs to receive data. By definition, the ~~, ~, and ri loops \nare degenerate loops containing only one iteration. The data to be received is the value in location \nti on processor ~~ in iteration ~~. Conversely, the ~~ fxR1 vn M NHlloops in the send loop nest enumerate \nall the senders. The ~, loops We next project the original polyhedron onto the (Vy V-J ...,vn-~ ) space, \nwe obtain an (H -1)-dimensional poly\u00adhedron represented by a set of constraints involving only variables \n(v~,v~,...,v~-~ ) . We can then repeat the process for varia~bles v n-l  % An example illustrating \nthe algorithm is shown in Figure 6. In the example, we have a polyhedron defined by inequalities involving \nvariables i and j. Steps 1 and 2 produce the bounds for the loop nest that scans the solution lexicographically \nin (i, j) ordeq whereas steps 3 and 4 produce the bounds for the loop nest that scans the solution lexicographically \nin (j, i) order. As discussed in Section 5.1, there may exist an [xl, ....Xn-~] in the projected polyhedron \nin (VI, Vz,....Vn-~) space that does not correspond to any solution in the original system. Since all \ncon\u00adstraints involving Vn are used in the derivation of its lower and upper bounds, the lower bound of \nVn will be larger than the upper bound when VI =xl, ....rrn _~= Xn-*. llms, there isaone-to-one correspondence \nbetween the iterations executed and the solutions in the system. While the algorithm deseribed above \nis correct, the generated code can be inefficient. An outer loop may contain iterations that do not have \nany useful computation; they simply compute the bounds of their inner loop just to find that the inner \nloop has no iterations. The auxiliary variables introduced by the LW functions can often cause such inefficiency. \nWe can usually eliminate this form of inef\u00adficiency as follows. We need not create a loop nest for Vn, \nif the bounds on Vn can be expressed as avk -f3s Vns avk -13, where a and @ are integers. We can simply \nreplace all references to Vn by avk-~. If the bounds on Vn are of the form Vk -13s avns Vk -13 where \nIal >1, we can still eliminate the loop nest for Vn by updating the bounds of Vk to: l-p forvk=a _ +fitohstepado \na H where 1 and h are the original lower and upper bounds of Vk. 53 Generating Computation and Communication \nCode To find the computation allotted to each processor, we scan the elements in a computation decomposition_re~tion \nC lexicognaphi\u00adcally in (pi, ..., pq, il, . . . in), or simply (p, I), order. The P loops enumerate the \nprocessors. The inner i loops enumerate the iterat\u00adions to be executed for each value of ~. The SPMD \ncode to be executed by each processor is as follows. Each processor checks if its processor number is \nwithin the bounds of the ~ loops. If so, the code it executes is simply the ; loops parametrized by its \nproces\u00adsor number. In the case where the computation decomposition is cyclic, each processor must iterate \nthrough the virtual processors it represents. F@res 7(a) and (b) show the computation code fcw our example \nin Figure 2. The rest of the figure shows the communica\u00adspecify the iterations when processor ~~ needs \nto send some mes\u00adsages. The jr loops identify the receivers of each message. The ~, loops specify the \niterations when processor ~r need the data. The ~ loop is a degenerate loop containing the address of \nthe data to be sent. If auxiliary variables have been introduced to handle modulo constraints in the \nLWTS, the auxiliary variables are placed last in the lexicographic order for both loops. i 2;M 1 m **** \n$ VI *** w o *    (H .... u d &#38; 1 ]6_isj 2jsi+12 Figure 6: Example of two projection sequences \non a 2-ditnensional polyhedron. The table contains the bounds on the variables eliminated by each projection. \n 5.4 Merging Loop Nests To generate the complete program for a processor, we need to merge together a \nprocessor s computation code, and its receive and send codes for each communication set. A naive technique \nis to make each processor iterate through the entire loop nest in the source program. In each iteration, \na proces\u00adsor checks if the iteration belongs to its computation domain, and if it is to take part in \neach of the communication sets. Checking such conditions in the innermost loop would be exorbitantly \nexpensive. Some of this inefficiency can be eliminated by standard data-flow optimization such as algebraic \nsimplification, invariant code motion, strength reduction and common subexpression elimi\u00adnation. Since \nall the conditions tested are affine expressions, we can poten\u00adtially eliminate all run-time checks by \nsplitting loops. Suppose we need to merge the following loops: 133 ifp>=Oand pc. N/32then fort= Oto \nTdo for i = MAX(32 p, 3) to MIN(32 p + 31, N) do X[i] =X[i -3] (a) Computation code: scanning C in (p, \nt, i) order, for p.= PPtoN / 32step Pdo fort= Oto Tdo for i = MAX(32 pv 3) to MIN(32 p, + 31, N) do \nX[i] =X[i -3] (b) Computation code when virtual processors poare mapped to P physical processors (pP). \n if pr >= 1and p, <= N / 32then fortr=Oto Tdo for i,= 32p, to MIN(32 p, + 2, N) do p,=pr-l t,=; i~=ir.3 \na=ir-3 receive X[a] from iteration (tY iJ in processor (pJ (c) Receive code: scanning M2 in (p,, t,, \ni,, p,, t,, i,, a) ordec if p~ >= Oandp~ <= N/ 32-1 then fort, =Oto Tdo for i,= 32p, +29 to MIN(32 p, \n+ 31, N -3) do pr=p, +l t,= t, ir=i~+3 a=i~ send X[a] to iteration (tr ir) in processor (pr) (d) Send \ncode: scanning M2 in (p,, t,, i,, p,, t,, i,, a) order Figure 7: Computation and communication code \nfor the example in Figure 2. for i =Oto 200 do receive...) for i=100to 300do send(...) Instead of generating: \nfor i=Oto 300do if Oc= iand ic= 200then receive (...) if 100c= iand i<= 300then send (...) we can generate: \nfori=Oto99do receive...) for i =100to 200 do receive...) send(...) for i=201to 300do send(...) We \nhave developed an algorithm that merges multiple nested loops together using the technique of loop splitting. \nDetails of the algo\u00adrithm are beyond the scope of this paper. If the relative magnitude between the bounds \nof the individual loops is not known at com\u00adpile time, loop splitting can expand the program size by \na signifi\u00adcant amount. Our compiler only uses loop splitting on inner loops, and also when the relative \nmagnitudes between the loop bounds are known. We have also developed a dynamic splitting scheme that \nwe use on the outer loops. The compiler does not generate all the possible combinations statically. Instead, \neach processor deter\u00admines its bounds for all the iteration sets, sorts the bounds, and interprets the \nsorted list to determine the loops it has to execute. Finally, for iteration sets that are a function \nof only outermost loop variables, we insert dynamic checks into the bodies of the outer loops. 5.5 Local \nAddress Space Typically, a processor on a parallel machine touches only a part of an array. Since data \nsets processed by these programs are often very large, it is essential that the compiler only allocates, \non each processor, storage for the data used by the processor. The following is a simple approach to \nthe memory allocation prob\u00ad lem. We allocate on each processor the smallest rectangular region that covers \nall the data read or written by the processor, and we copy all the received data from the communication \nbuffer to their home locations in the array before they are accessed. Given a com\u00ad putation decomposition \nC and an access function f, the set of loca\u00ad tions touched by processor is {ti GA [~i(i, ~) Cc Aa=j(ti, \ni)}. By scanning the inequalities lexicographically in (~, ak, ;) order, the bounds we obtain on ak are \nthe bounds for the kth dimension of the bounding box covering access ~ If there are multiple accesses \nto the same array, we simply find the bounding box of the rectan\u00adgular boxes for all the accesses to \nthe same array. Note that this formulation allows local data spaces on different processors to overlap. \nThe above algorithm is inadequate if the rectangular bounding box of the data accessed is larger than \nthe available local memory on the processor, while the data actually used fit in the local memory. Also, \na processor s local memory may not be large enough to fit all the data that a processor will eventually \nuse in a computation. In that case, we need to manage the memory dynamically. The LWT information provides \na more efficient way to manage the data that have been received from other processors. The compiler knows \nprecisely which values are read by every instance of the read access. Instead of first copying all the \nreceived data to their home locations, a processor can simply read the values directly from the communication \nbuffers. The compiler also has the infor\u00admation on when the buffer is no longer needed, and can manage \nthe buffer space effectively. 6 Qtimizations Since, the above algorithms generate a receive and a send \nmessage for every read access to remote data, the code is correct but ineffi\u00adcient. It is essential that \nwe eliminate the redundant messages and amortize the message sending overhead by batching the communi\u00adcation. \n6.1 Eliminating Redundant Communication Ancourt has also studied the problem of eliminating redundant \ncommunication[2]. Given a set of iterations and accesses, Ancourt s algorithm can construct a set of \nloop nests that fetches all the data touched without any duplication. This algorithm is ade\u00adquate for \nremoving redundant traffic if no communication is required within the loop nest. In general, transfers \nof data with the same address are redundant only if the values transferred are iden\u00adtical. 134 We separate \nredundancy self reuse when multiple into two categories. We say that there is instances of a single read \naccess use the Ostrs T same data and group reuse when instances of different read 3si, sN accesses use \nthe same data. We discuss each of these in turn below. OSUS3 6.1.1 Redundant Communication Due to Self \nReuse Read instances that have different data-flow relationships often are amenable to different communication \noptimizations. By partition\u00ading the read instances into different contexts according to their data-flow \npatterns, the LWT makes it easier to detect and eliminate redundancy. Our algorithm applies Theorem 5 \nto the communica\u00adtion set of each context to detect redundancy due to self reuse. All elements in a communication \nset with identical ,~, ,-~, ~nd i refer to the same values all elements with identical IS, p~, p ~ and \nii are redundant messages. Thus, we wish to replace the set of redund@ messages with ( m in ( ;r), ~r, \n~$L~~zZ) . This can be achieved by projecting the set onto the (p~, i$, ~r, ~) space, and constraining \nthe upper bound of jr to be identical to its lower bound. There are two complications. First, if the \nlower bound of ~, is expressed as a conjunction of multiple inequalities involving outer loop indices, \nthen the communication set containing the min\u00adimum ;. s is no longer convex. fbe algorithm needs to divide \nthe communication set into multiple convex sets. The second compli\u00adcation arises from the fact that a \nprojected image may contains points that do not correspond to a solution to the original system. In many \ncases, a simple test can determine that no such degemera\u00adcies are present[19]. 6.1.2 Redundant Communication \nDue to Group Reuse Detection of reuse between arbitrary accesses to the same matrix can be expensive. \nHowever, there is a prevalent form of reuse that can be incorporated and exploited easily within our \nmodel and that is the set of uniformly generated references[13]. Array index func\u00adtions of uniformly \ngenerated references are affine functions of loop indices and symbolic constants, and they differ only \nin the constant terms. For example, X[i] and X[i+3] are uniformly generated refer\u00adences; so are B[2i+3j+l, \n3j+n+3] and B[2i+3j+10, 3j+n+2], but not C[i] and C~]. Reuse between uniformly generated references has \nbeen exploited successfully in improving cache Iocal\u00adit y[27] [28]. Uniformly generated references are \nquite common in real programs, so much so that specialized languages and cc]mpil\u00aders have been built \nto translate them to efficient code[7][17]. We can represent a set of uniformly generated references \nby their convex hull, and describe the data flow information for all the ref\u00aderences by a single LWT. \nFor exampIe, suppose the example in Figure 2 has three more read accesses: fort= Oto Tdo fori=3 toNdo \nX[i] = f(X[i], X[i -1], X[i -2], X[i -3]) Figure 8: The example in Figure 2 with multiple read accesses. \nThe set of accesses X[i], X[i -1], X[i -2], X[i -3] can be repre\u00adsented by the access function f(i), \nwhere f(i) = i-IJ and Os us 3. The LWT for all the accesses is given in Figure 9. Note that the convex \nhull may contain more data than those accessed within an iteration, however, since a processor is typi\u00adcally \nresponsible for a contiguous block of iterations, this method is unlikely to cause any significant unnecessary \ntraffic. b true ir-u23 true fals Figure 9: The Last Write Tree of the example in Figure 8. 6.13 Other \nforms of redundancies Redundancy may also arise from cyclic decompositions, where a physical processor \nemulates multiple virtual processors. Given a virtual to physical processor mapping n: P + P, communication \nii) c M can be eliminated if n (jr) = n ( ~J . Also, :jm~;~;c:;ion5 ( ~r)Fr$j, P,, ~), 1~,, ~,, ij, i,, \n~ ~ ~ are redundant if rr(jir) = n(p r), ~~ = p s, IS -i s and u -a . Communication sets derived from \ndata decompositions that repli\u00adcate data may also contain redundancy. In our definition of com\u00admunication \nsets, we consider communication to be necessary as long as there is a processor that owns a copy of the \ndata needed by another processor. That means communication is generated even if the processor already \nowns a copy of the data. To eliminate this redundancy, we eliminate all the communication elements (jr, \n~r, ~~,~~, ii) ~ M such that (E, ~J G D. Furthermore, two communication elements (;,, F,, ~$,~$, a) , \n(~=~,, ~~, ~~, ~) G_M are redundant due to replicated data if ~r = p r, ~~ * p s, IS = i s and i = ~. \nThe technique to eliminate this redundancy is similar to that of removing redundant communication due \nto self-reuse.  6.2 Communication Aggregation Whether aggregation of small messages into large messages \nis nec\u00adessary depends on the machine architecture. For example, machines such as the iWarp and CM-5 support \nfine-grain commu\u00adnication, while machines such as the Intel iPSC have significant overhead in processing \nevery message. Again, we classify message aggregation into two kinds: self aggregation, where messages \ngen\u00aderated by different instances of the same access are aggregated, and group aggregation, where messages \ngenerated by different accesses are aggregated. For group aggregation, we simply aggre\u00adgate all messages \nthat have the same sender, receiver and depen\u00ad dence level into one message. While group aggregation \nreduces the number of messages by a small constant, self aggregation can potentially eliminate many more \nmessages. Our self aggregation algorithm also takes advan\u00adtage of the partitions created by the LWT analysis. \nAll instances 135 within the same communication set have the same dependence level. If the dependence \nlevel of a communication set is k, it is obviously legal to batch all the messages within an iteration \nof loop k and send the data at the end of the iteration. This can result in significant overhead reduction \nif loop k is not the innermost loop. The algorithm to aggregate the communication of a communica\u00adtion \nset at level k is as follows. To generate the send code, we scan the communication set lexicographically \nin (~,, i,,, .... i,k_l, P,, i,J_..., i, , ~r,ti)  order. Each instance of the loops p~, i$ , .... i \n, ~, produces one message, and each instance of the loo&#38; i , ~!~, i contributes an item to the message. \nRedundancy elimina~~on wc%ld have caused ~. to take on only the value of the earliest iteration on the \nreceiver side using the value. Similarly, we create the receive loop nest by scanning the polyhedron \nin (jr, ir, .... i ~~-1,7$, ~s,ir{ .... irm,;) order. Iterations in loops j i , .... ir * use the data \nfrom the same s rk message. Note that for each message,nthe order in which the sender packs the data \nis the same as the unpacking order. Figure 10 shows the receive and send code for the M2 context in Figure \n3 after aggregation. if p, >= 1and p, <= N / 32then fortr=Oto Tdo p,=pr-l receive data into buffer from \nprocessor p~ index = O foris=32pr -3to MIN(32pr-l, N-3) do t,= t, ir=is+3 a=ir-3 X[a] = buffer[index] \nindex = index + 1 if p~ >= Oandp~ <= (N -32)/ 32then fort$=O toTdo pr=p, +l index = O fori,= 32p~+29to \nMIN(32 p,+31,N -3)do t,= t, ir=is+3 a=i~ buffer[index] = X[a] index = index + 1 send the data in buffer \nto processor pr Figure 10: Aggregated communication for context M2 in Figure 3. 6.2.1 Multi-casting \nMany systems provide optimized routines for multi-casting. To take advantage of these routines, we need \nto determine if the same message is sent to multiple processors. We scan a communication set to be aggregated \nat level k lexicographically in (~,, i,, .... i,, ,Ljir, ti, i,, .... i,,, ;,) order. If the bounds of \nZ are indep&#38;dent of pr, the data sent to each processor are identical.  7 A Detailed Example We \nhave implemented the communication optimization and code generation algorithms described in this paper \nin the Stanford SUIF compiler system[25]. However, these algorithms have not been fully integrated with \nall the other phases in the compiler. We have been able to experiment with our algorithms on the Intel \niPSC/860 machine by doing the following. We enter the decompositions manually. From the decompositions, \nour compiler pass generates all the necessary communication and computation loop nest struc\u00adtures and \ntheir bounds in C. We then insert in the original loop body and compile the program using the vendor \ns C compiler. In this section, we show how our algorithm compiles and optimizes the LU decomposition \nkernel in Figure 11. foril=Oto Ndo fori2=il+lto Ndo X[i2][il] = X[i2][ilJ / X[il][il] fori3=il+lto Ndo \nX[iJ[i~] = X[i2][i~J -X[i2][i~] * X[il][i3] Figure 11: Sequential version of LU decomposition. To improve \nload balance, we use a cyclic computation and data decomposition in this program. The computation decomposition \nand initial data decomposition D first map the computation and data, respectively, to a virtual processor \nspace, which is later mapped to the physical processor space. Let ~ = ( il, i2, i3) , the decompositions \nused are C= {(i,p)=lxl lps[ol~;cp+l} and D5 {(ti, p) GAx Plpsr@<p+l} That is, the kth virtual processor \nexec~tes-the kth iteration of the second loop and owns the kth row of array X. First, our algorithm generates \nthe LWTS for all the five read accesses in the program. Every tree has two leaves, one -!-leaf and one \nleaf reading from either the X[i2][i3] or X[i2][i1J write opera\u00adtion. The LJVT for the read access X[il] \n[i3] is shown in Figure 12. Osilrs N il+lsi SN r Zr il +lsi3rs N r ilr z1 false values from the write \nx[i21[i31 values outside Figure 12:TheLWTfor thereadaccessX[il ][i3], Second, our compiler ~alculates \nthe communication set for every leaf of each tree. It determines that both leaves on the LWTS for X[il][i3] \nand X[il][il] have non-empty communication sets. All other leaves need no communication, either because \nthe values read are produced by the same processor or they are already in the local memory due to the \ninitial data decomposition, The communication sets specify the related read and write dynamic instances \nprecisely. We know precisely that it is the first iteration of the second loop that produces all the \ndata used by X[il] [iq] and X[il] [il] in the next iteration of the outermost loop. Knowing this information \nenables us to issue the send immediately after the first iteration of the second loop. If we had used \nonly data dependence analysis, we would have to delay the send until all the 136 iterations of the second \nloop have been executed. Sending the data early maximizes the potential for overlap between communication \nand computation. Next, the compiler performs several communication optimizatians. It first aggregates \nall the messagea within each of the two comrmr\u00adnication sets of X[il] [i3]. Then, the communication sets \nfor the L leaves of both X[i1][i3] and X[il] [i3] are aggregated. The same is done for the other two \nleaves. At this point, each virtual processor receives only a single message in each iteration of the \noutermost loop. The compiler then discovers that the message contents are independent of the identity \nof the receiving processor, thus the dlata can be multi-cast. Since the computation decomposition is \ncyclic, the compiler optimizes the communication further by sending the data only to one virtual processor \nin each physical processor. Lastly, the compiler decides on how to manage the 10CS1memury. While each \nvirtual processor always writes to the same row in the matrix, every processor would have read the entire \nmatrix by the end of the computation. The compiler creates a local array for the write accesses, and \nuses a buffer to hold the data a processor receives. The program uses the data in the buffer directly. \nThe dimensions of the local array on each physical processor are ((N + P) / P) x 1 x (N+ 1) where P is \nthe number of physical processors in the system, Note that the middle dimension of this local array can \nbe trivially eliminated. The communication buffer is of size N+l, the size of the largest aggregated \nmessage. Figure 13 is a pretty-printed version of the information genersted by the compiler. In this \ncode, the variable myp identifies the physical processor executing the code. Most of the loop overhead \npresent in this code can be eliminated using classical optimiza\u00adtion. float X[(N + P)/P][N+l], BIN+l] \n{ First processor sends initial data} if N>=landmyp=Othen fort. Oto Ndo B[t] = XIO][t] for p,= 1 to MIN(P \n-1, N) do send Bof size N +1to (prmod P) foril=Oto N-ldo { iterate over the virtual processor set} forp=P*((il \n+l-myp+P-l )/P)+ mypto Nstep Pdo i2=p c = iZ/P {local address } { receive data, once for each physical \nprocessor} ifil>=i2-P+l then receive toBof size N -il +1 X[c][il] = X[c][il] / BIO] fori3=l+i1to Ndo \nX[c][i3] = X[c][i3] -X[c][il]*B[i3 -il] { send data} ifilc=N -2andi2=il+lthen fort= i2to Ndo B[t -i2 \n] = X[c][t] forpr=i2+ lto MIN(i2+P-l, N)do sendBof sizeN-i2+1to (p,mod P)  Figure 13: Compiler generated \nSPMD code for LU decomposition. Our compiler pass took 2.9 seconds to generate the computation and communication \ncode. Our 2048 x 2048 LU-demmposition code runs at 250 MFLOPS (single-precision) on a 32-procmsor 32-\u00ad\\, \n@ 512 - - Achieved Time (N. 2048) j 256. \u00ad , , Achieved Time (N. Periect Speedup 1024) 8 ~ 128. \u00ad ~w \n 16. \u00ad 8-\u00ad.\u00ad... 4.- ., .. 2 t 1,2 Number of Processors Figure 14: Performance for a single-precision \nLU decomposition on an Intel iPSC/860. Intel iPSC/860 machine. Figure 14 shows the performance of our \ncode for two matrix sizes on different numbers of processors. 8 Summary and Conclusion This paper introduces \na value-centric approach to communication generation for machines with a distributed address space. We \nuse a precise data-flow analysis that determines the exact value read by every instance of a read access; \nwe derive communications from computation decompositions. This approach has several advan\u00adtages over \nthe more conventional approach of deriving communi\u00adcations from data decompositions, and optimizing communications \nusing data dependence analysis. Our approach supports more flexi\u00adble data decomposition schemes: data \nmay not have a static home, data may be replicated, and the owner-computes nde can be relaxed. Our approach \ncan also support more communication opti\u00admization. The LWT analysis automatically partitions the read \ninstances into sets that share similar communication characteris\u00adtics. This simplifies the handling of \nboundary conditions and also exposes more opportunities for parallelism. We have developed a uniform \nframework to solve the problems of communication code generation, local memory management, mes\u00adsage aggregation \nand redundant data communication elimination. We represent data decompositions, computation decompositions \nand the data flow information all as systems of linear inequalities. The various code generation and \ncommunication optimization problems can be solved by projecting polyhedra represented by sets of inequalities \nonto lower dimensional spaces. Our techniques are applicable to both the value-centric approach presented \nin this paper as well as the conventional location-centric approach used in other systems. Acknowledgments \nWe would like to thank Jennifer Anderson, Amy Lim, Dror May\u00addan and Daniel Scales for their contributions \nto this work. We acknowledge the support of the SUIF compiler group, and Robert Dutton and Dan Yergeau \nfor providing and supporting the iPSC/ 137 860 machine. Finally, we thank Mary Hall$ Amy Llm, Martin \nRinard, Prabhat Samaratunga and Bob Wilson for providing invaluable comments on the paper.  References \n[1] C. Ancourt and F. Irigoin. Scanning Polyhedra with DO LQops. In Third ACM SIGPLAN Symposium on Principles \nand Practice of Parallel Programming, PPOP~ pp. 39-50, April 1991. [2] M. Ancourt. Generation automatique \nde codes de transfert pour multiprocesseurs a memoires locales. PhD thesis, University of Paris VI, March \n1991. [3] J. M. Anderson and M. S. Lam. Global Optimizations for Parallelism and Locality on Scalable \nParallel Machines. In Proceedings of the SIGPLAN 93 Conference on Program Language Design and Implementation, \nJune 1993. [4] F. Andr6, O. Ch6ron and J.-L. Pazat. Compiling Sequential Programs for Distributed Memory \nParallel Computers with Pandore 11. In Third Workshop on Compilers for Parallel Computers, pp. 231-242, \nJuly 1992. [5] U. Banerjee. Dependence Analysis for Supercomputing. Kluwer Academic, 1988. [6] W. Blume \nand R. Eigenmann. Performance Analysis of Parallelizing Compilers on the Perfect Benchmarks Programs. \nIn IEEE Transactions on Parallel and Distributed Systems, vol. 3, no. 6, pp. 643-656, November 1992. \n[7] M. Bromley, S, Heller, T. McNemey and G. L. Steele Jr. Fortran at Ten Gigaflops: The Connection Machine \nConvolution Compiler. In Proceedings of ACM Conference on Programming Language Design and Implementation, \npp. 145-164, June 1991. [8] B. Chapman, P. Mehrotra, and H. Zima. Programming in Vienna Fortran. In Third \nWorkshop on Compilers for Parallel Computers, pp. 121-160, July 1992. [9] c. Koelbel. Compile-time generation \nof regular communication patterns. In Proceedings of Supercomputing 91, pp. 101-110, November 1991. [10] \nP. Feautrier, Array expansion. In hrternational Conference on Supercomputing, pp. 429-442, 1988. [11] \nP. Feautrier, Parametric integer programming. Technical Report 209, Laboratoire Methodologies and Architecture \nDes Systems Informatiques, January 1988. [12] P. Feautrier. Dataflow analysis of array and scalar references. \nIn International Journal of Parallel Programming, 20(1):23\u00ad52, February 1991. [13] G. Gannon, W. Jalby \nand K Gallivan. Strategies for cache and local memory management by global program transformation. In \nJournal of Parallel and Distributed Computing, 5:587-616, 1988. [14] G. R. Gao, R. Olsen, V. Sarkar and \nR. Thekkath. Collective loop fusion for array contraction. In Proceedings of the Fifth Workshop on Programming \nLanguages and Compilers for Parallel Computing, pp. 171-181, August 1992. [15] P. Havlak and K. Kennedy. \nAn implementation of interprocedural bounded regular section analysis. In IEEE Transactions on Parallel \nand Distributed Systems, vol. 2, no. 3, pp. 350-360, July 1991. [16] S. Hiranandani, K. Kennedy and C. \nTseng. Compiling Fortran D for MIMD Distributed-Memory Machines. In Communications of ACM, vol. 35, no. \n8, pp. 66-80, August 1992. [17] L. G. C. Hamey, J. A. Webb and 1. C. Wu. An architecture independent \nprogramming language for low-level vision. Computer Esion, Graphics, and Image Processing, vol. 48, no. \n2, pp. 246-264, November 1989. [18] J. Li and M. Chen. Index domain alignment: Minimizing cost of cross-referencing \nbetween distributed arrays. In Proceedings of Frontiers 90: The Third Symposium on the Frontiers of Massively \nParallel Computation. pp. 424-432. October 1990. [19] D. E. Maydan, S. P. Amarasinghe and M. S. Lam. \nArray Data-Flow Analysis and its Use in Array Privatization. In Proceedings of ACM Conference on Principles \nof Programming Languages, pp. 2-15. January 1993, [20] D. E. Maydan, Accurate Analysis of Array References. \nPhD thesis, Stanford University, September 1992. Published as CSL-TR-92-547. [21] P. Mehrotra and J. \nVan Rosendale. High Level Programming of Distributed Memory Architectures, In A. Nicolau, D. Gelemter, \nT. Gross and D. Padua editors, Advances in Languages and Compilers for Parallel Processing, pp. 364\u00ad384, \nThe MIT Press, Cambridge, Massachusetts, 1991. [22] K. Pingali and A. Rogers. Process decomposition through \nlocality of reference. In Proceedings of the SIGPLAN 89 Conference on Program Language Design and Implementation, \npp. 69-80, June 1989. [23] A. Schrijver, Theory of Linear and Integer Programming, Wiley, Chichester \n1986. [24] G. L. Steele. Proposal for alignment and distribution directives in HPF. Draft presented at \nHPF Forum meeting, June 1992. [25] S. Tjiang, M. Wolf, M. Lam, K. Pieper and J. Hennessy. Integrating \nscalar optimizations and parallelization. In U. Banerjee, D. Gelemter, A. Nicolau and D. Padua editors, \nLanguages and Compilers for Parallel Computing, pp. 137\u00ad151, Springer-Verlag, Berlin, Germany, 1992. \n[26] C.-W. Tseng. An Optimizing Fortran D Compiler for MIA4D Distributed-Memo~ Machines. PhD thesis, \nRice University, January 1993. Published as Rice COMP TR93-199. [27] M. E. Wolf and M. S. Lam. A data \nlocality optimizing algorithm. In SIGPLAN Notices, vol. 26, no. 6, pp. 30-44, June 1991, [28] M. E. Wolf. \nImproving localip and parallelism in nested loops. PhD thesis, Stanford University, August 1992. Published \nas CSL-TR-92-538, 138  \n\t\t\t", "proc_id": "155090", "abstract": "<p>This paper presents several algorithms to solve code generation and optimization problems specific to machines with distributed address spaces. Given a description of how the computation is to be partitioned across the processors in a machine, our algorithms produce an SPMD (single program multiple data) program to be run on each processor. Our compiler generated the necessary receive and send instructions, optimizes the communication by eliminating redundant communication and aggregating small messages into large messages, allocates space locally on each processor, and translates global data addresses to local addresses.</p><p>Our techniques are based on an exact data-flow analysis on individual array element accesses. Unlike data dependence analysis, this analysis determines if two dynamic instances refer to the same value, and not just to the same location. Using this information, our compiler can handle more flexible data decompositions and find more opportunities for communication optimization than systems based on data dependence analysis.</p><p>Our technique is based on a uniform framework, where data decompositions, computation decompositions and the data flow information are all represented as systems of linear inequalities. We show that the problems of communication code generation, local memory management, message aggregation and redundant data communication elimination can all be solved by projecting polyhedra represented by sets of inequalities onto lower dimensional spaces.</p>", "authors": [{"name": "Saman P. Amarasinghe", "author_profile_id": "81100533031", "affiliation": "", "person_id": "P258981", "email_address": "", "orcid_id": ""}, {"name": "Monica S. Lam", "author_profile_id": "81100237956", "affiliation": "", "person_id": "PP14092336", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/155090.155102", "year": "1993", "article_id": "155102", "conference": "PLDI", "title": "Communication optimization and code generation for distributed memory machines", "url": "http://dl.acm.org/citation.cfm?id=155102"}