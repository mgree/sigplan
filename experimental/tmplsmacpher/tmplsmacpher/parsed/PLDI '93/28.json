{"article_publication_date": "06-01-1993", "fulltext": "\n Branch Prediction For Free THOMAS BALL JAMES R. LARUS tom@cs.wkc.edu larus@cs.wise.edu Computer Sciences \nDepartment University of Wisconsin Madison ABSTRACT Many compilers rely on branch prediction to improve \nprogram performance by identifying frequently executed regions and by aiding in scheduling instructions. \nProfle-based predictors require a time-consuming and inconvenient compile-profile-compile cycle in order \nto make prdlctions. We present a program-based branch predictor that performs welt for a large and diverse \nset of programs written in C and Fortran. In addition to using natural loop analysis to predict branches \nthat control the iteration of loops, we focus on heuristics for predicting non-loop Lmstches, which dominate \nthe dynamic branch count of many programs. The heuristics are sim\u00adple and require little program analysis, \nyet they are effective in terms of coverage and miss rate. Although program-based predic\u00adtion does not \nequal the accuracy of profile-based Prdlction, we believe it reaches a sufficiently high level to be \nuseful. Additional type and semantic information available to a compiler would enhance our heuristics. \n 1. INTRODUCTION In this paper we study the behavior of branches in programs and show that simple, static \nprogram-bused heuristics can predict branch directions with surprisingly high accuracy. Our heuristics \ngo beyond simply identifying loop branches, because in many programs, non-loop branches execute more \nfrequently than loop branches. These heuristics are inexpensive to employ and simple to implement, yet \naccu\u00adrately predict a high percentage of loop and non-loop branches for a large and diverse set of programs, \nincluding many programs with complex conditional cent.ml flow. Our measurements show that a perfect static \npredictor has the potential to predict dynamic loop and non-loop Authors address: 1210 W. Dayton St. \nMadison, WI 53706. This work was supportad by the Naticmd Science Foundation under grants CCR-8958530 \nand CCR-9101O35. Permission to copy without fee all or part of this material is granted provided that \nthe copies are not made or distributed for direct commercial advantage, the ACM copyright notice and \nthe title of the publication and its date appear, and notice is given that copying is by permission of \nthe Association for Computing Machinery. To copy otherwise, or to republish, requires a fee and/or specific \npermission. ACM-SlGPLAN-PLDl-6 /93/Albuquarque, N.M. @ 1993 ACM 0-89791 -598 -4/93 /0006 /0300 . ..$1 \n.50 branches with a miss rate of approximately 10%. Naive strategies that always predict the target \nor fall-thru succes\u00adsor of a non-loop branch have a miss rate of approximately 50%. Our heuristic has \nan average miss rate of 26% for non-loop branches, with good performance on benchmarks that appear, at \ntirst, difficult to predict. Taking into account loop brrmches-for which we employ a more accurate heuristic \nthan the common technique of simply identifying backwards branches+ur heuristics have an average miss \nrate of 20?4. Many compiler optirnizations rely on branch prediction to identify heavily-executed paths \n[6, 12, 14]. In addition, recently introduced architectures, such as the DEC Alpha [15] and MIPS R4000 \n[9], exact a heavy pipeline penalty for mispred.icting a branch (up to 10 cycles [15]). To help alleviate \nthis problem, some architectures predict that forward conditional branches are not taken and back\u00adward \nconditional branches are taken, thereby relying on a compiler to arrange code to conform to these expectations. \nRun-time profile information [2, 8] from a program execu\u00adtion typically is used to statically predict \nbranch directions. Fisher and Freudenberger observed that profiled-based static branch prediction works \nwell because most branches take one direction with high probability and the highly probable direction \nis the same across different program exe\u00adcutions [7]. Profile-based branch prediction can be quite accurate, \nbut it is inconvenient and time-consuming to use. First, a pro\u00adgram is compiled. To be profiled, a program \nmust be instrumented with counting code, which may be done by the compiler or another tool. The instrumented \nprogram exe\u00adcutes, possibly several times, producing a profile. Finally, the program can be recompiled \nwith the aid of the profile. This process requires two compilations and an execution. Furthermore, when \nthe program changes, the entire process must be repeated. On the other hand, program-based pred\u00adiction \ncan be employed during the original compilation to make branch predictions. Although program-based predic\u00adtion \nis a factor of two worse, on the average, than pro6le\u00adbased prediction, we believe it reaches a sufficiently \nhigh level to be useful. This paper is organized as follows. Section 2 contains background material and \ndescribes the benchmark pro\u00adgrams. Section 3 classifies loop and non-loop branches and compares their \nbehavior. Section 4 presents several simple heuristics for non-loop branches and measures their effec\u00adtiveness \nin isolation. Section 5 considers combining these simple heuristics into a complete heuristic and contains \nthe results for this heuristic. Section 6 presents results on how our heuristic performs at finding sequences \nof instructions without a mispedicted branch. We compare profile-based methods for measuring this quantity \nwith trace-based methods and show why trace-based methods are preferable. Section 7 examines the performance \nof our heuristic on dif\u00adferent datasets. Section 8 reviews related work and Section 9 concludes the paper. \n 2. BACKGROUND We restrict our heuristics to predicting two-way conditional branches with fixed targets. \nThroughout the paper, the word brunch refers to such branches. We do not consider branches whose target \nis dynamically determined (by lookup in a jump table, for example). Associated with leach conditional \nbranch instruction is its target successor--the instruction to which control passes if the branch condition \nevaluates to &#38;u&#38;and its fall-thru successor-the instruc\u00adtion to which control passes if the branch \ncondition evalu\u00adates to false. We used our profiling and tracing tool QPT [2] both as a platform for \nstudying branch behavior and for making branch predictions. QPT takes as input a MIPS executable file \nand produces an instrumented program that generates an edge profile (i.e., for each branch, a count of \nhow many times control passes to the target and fall-thru successor) when run. QPT can also instrument \na program to produce an instruction and address trace. Since QPT operates on an executable file, all \nprogram procedures are analyzed. The numbers in this paper include DEC Uttrix 4.2 library pro\u00adcedures \nas well as application procedures. In order to instrument an executable file, QPT builds a control flow \ngraph for each procedure in the executable file. Each vertex in the control flow graph represents a basic \nblock of instructions. A basic block ending with a ccmdi\u00adtionrd branch corresponds to a vertex in the \ncontrol flow graph with two outgoing edges. The root vertex of the con\u00adtrol flow graph is the entry point \nof the procedure. A basic block containing a return (procedure exit) has no successom in the control \nflow graph. Some of our heuristics make use of the control flow graph s domination and postdomination \nrelations [1]. A vertex v dominates w if every path from the entry point of the procedure to w includes \nv. A vertex w postdomimtes v if every path from v to any exit vertex includes w. If the successor of \na branch postdominates the branch, then no matter which direction the branch takes, the successor even\u00adtually \nexecutes. We analyzed the programs in the SPEC89 benchmark suite [4], along with a number of other programs. \nThese benchmarks (23 of them) are listed in Table 1, along with a size Program Description Lng. (lKb) \ncongress Interp. for Prolog-like lang. c* 856 ghostview X postscript previewer c 831 gcc * GNU C compiler \nc 688 lCC Fraser &#38; Hanson s C cmplr. c 254 m Net news reader c 221 espresso * PLA minimization c \n188 qpt Profiling and tracing tool c 143 awk Pattern scanner &#38; processer c 102 Xlisp * Lisp interpreter \nc 78 eqntott * Boolean eqns. to truth table c 45 addalg Integer program solver c 33 compress File compression \nutility c 25 Search file for regular expr. c 20 qP Polydominoes game c 16 Py splce2g6 * Cwcmt smmlahon \nF 385 doduc * Hydrocode simulation F 184 Two-electron integral denv. F 168 fPPPP* dnasa7 * Floating \npoint kernels F 90 tomcatv * Veetorized mesh generation F 66 mstrix300 * Matrix multiply F 61 costScale \nSolve minimum cost flow c 41 dcg Conjugate gradient c 41 sgefat Gaussian elimination c 33  Table 1. \nBenchmarks, sorted by code size. SPEC benchmarks are marked with *. FortrsrI benchmarks are marked with \nan F. Benchmarks were optimized (-O or -02). description of their function. We have broken the bench\u00admarks \ninto two major groups: programs that perform little to no floating point computation and programs that \nperform many floating point operations. Within each group, pro\u00adgrams are sorted by the size of their \nobject code. All of the benchmarks were compiled and analyzed on a DECstation (a MIPS R2000/R3000 processor) \nwith -O optimization. The results presented in Sections 3 to 6 are for a single execution of each benchmark. \nPrevious work has shown that most branches behave similarly over different execu\u00adtions [7]. That is, \nif a branch takes one direction with high probability during one execution of a program, it most likely \ntakes the same direction with high probability in other executions. The goal of this work is to show \nthat static prediction can accurately determine these branch directions, rather than confirm previous \nresults. However, we also tested our predictor on multiple datasets per bench\u00admark and found similar \nresults to those of [7]. Section 7 summarizes the results of these experiments. We are concerned with \nstatic branch prediction. That is, for each branch either the target or fall-thru successor is prextictcd, \nand this prediction does not change during the execution of the program. Predicting a branch corresponds \nto choosing one of the two outgoing edges from the vertex containing the branch in the control flow graph. \nFor an exe\u00adcution, the standard for how well static branch prediction can potentially perform is the \nperject stafic predictor, which predicts the more frequently executed outgoing edge of each branch in \na program. If the perfect predictor has a low miss rate, then most branches follow one direction with \nhigh probability. If most branches take both directions with approximately equal probability, a perfect \nstatic predictor would do no better than a 50 %0miss rate. The perfect predictor provides an upper bound \non the performance of any static predictor. Branches for which the perfect predictor performs poorly \nwill not be ptedicted well by any static predictor. To measure how well a predictor P performs (for a \ngiven set of branches) we use two pemen\u00adtages (notated C/D), where C is the percentage of the dynamic \nbranches that are mispredicted by P (i.e., miss rate), and D is the miss rate for the perfect predictor. \n 3. LOOP AND NON-LOOP BRANCHES In this section we show that predicting non-loop branches is key to good \nbranch prediction for many programs. In addi\u00adtion, we show that a static predictor can potentially do \nvery well at predicting non-loop branches. First, we precisely classify branches as loop or non-loop. \nBackwards branches in code (a backwards branch passes control to an address that is before the address \nof the branch instruction) usually control the iteration of loops. However, many non-backwards branches \ncan also control the iteration of loops either by exiting the loop or continuing the itera\u00adtion [13], \nFor many of the benchmarks, loop branches that are not backwards branches account for a very high percen\u00adtage \nof loop branches (for example, 40% of dymmic loop branches in xlisp were not backwards branches and 45% \nof lcmp branches in doduc were not backwards branches). Such branches can be easily identified by natural \nloop analysis [1] of the control flow graph, as we now review. Each vertex that is the target of one \n(or more) loop back\u00adedges (as identified by a depth-fist search of the control flow graph from the root \nvertex) is a loop heud. Removing the backedges from a control flow graph eliminates all directed cycles. \nThe natural loop of a loop heady is: nat-loop(y) = {y) u { w I there exists a backedge x + y and a y-free \npath from w to x ) An edge V+W is an exit edge if there is a loop nat-loop~) such that v = nat-loop(y) \nand w 4 nat-loop(y). It is clear from the definition of natural hop that for any vertex v in nat-loop(y), \nat least one of v s successors must be in nat\u00adloop(y). Therefore, for any vertex, either none of its \noutgo\u00ading edges are exit edges, or exactly one of its outgoing edges is an exit edge. We classify branches \nas follows: * a branch is a loop branch if either of its outgoing edges is an exit edge or a loop backedge. \n * a branch is a non-loop branch if neither of its outgo\u00ading edges is an exit edge or a backedge.  Loop \nbranches can be very accurately predicted, as follows: if either of the outgoing edges is a backedge, \nit is predicted.l Otherwise, the non-exit edge is predicted. The intuition is that loops iterate many \ntimes and only exit once. The loop predictor chooses iterating over exiting. Figure 1 illustrates loop \nand non-loop branches. Edges D +B and E+B are backedges. There is one natural loop (with loop head B) \nwhich contains the vertices B, C , D, and E. The exit edges are C +F and E +F. Vertices A and B are non\u00adloop \nbranches, while C, D and E are loop branches. The predictions for the three loop branches are C+D, D \n+B, and E+B, respectively. Table 2 shows the breakdown of dynamic branches in each benchmark according \nto this loop classification scheme. Within each group, programs are ordered by the percentage of dymmic \nbranches that are non-loop branches ( % AU ). Many programs executions are dominated by non-loop branches. \nWe Iirst consider how the loop branch predictor performs. Under Loop , the column Prd/Prf contains the \nmiss rates for the loop predictor and the perfect predictor, applied to lcmp branches. The results are \nnot too surprising: the loop predictor does very well, and in some cases approaches the perfect predictor \n(compress, uddulg, eqntott). The mean miss rate for the loop predictor is 12% + Ioyo. We now consider \nnon-loop branches. The perfect predic\u00adtor ( TM ) performs very well for all benchmarks, implying that \nmost non-loop branches take one direction with high probability. For some benchmarks, non-loop branches \nare better behaved than loop branches! For instance, for gcc and xlisp, the perfect static predictor \ndoes better on non\u00adloop brartches than on loop branches. Tgt shows the 1 Figure 1. Control flow graph \nwith loop. Bold edges are back\u00ad edges and dashed edges are exit edges. Vertices C, D, and E are loop \nbranches. Although it is theoretically possible for fxxtr the outgoing edges from a branch TObe loop \nbackedges, this never occurred in our analysis of the benchmarks. If it did occur, one could predict \nrhe edge that leads to the in\u00adnennost loop. I Loon II Non-Loot) 1 Program Prd@f % TgtJPrf Rnd&#38;rf \nBig (%/%) Atl (%/%) (%/%) (x %) gcc 22/15 73 46/1 1 lCC 18/14 71 47/12 52/12 1 13 qpt 19/14 70 56I9 52I9 \n00 50 1 ~ compress 12/12 66 72/18 66/18 6 69 Xlisp 28/19 62 67P 50P 00 addrdg 7t7 52 43/30 43/30 7 \n67 ghostview 8/6 52 45/4 47/4 4 53 eqntott 312 49 73125 50/25 2 92 m 7r3 48 51/1 51/1 3 25 grep 26/2 \n44 34/0 3/0 3 96 congress 21/12 40 37/3 57/3 2 10 espresso 18/12 37 59/13 42/13 324 awk 4/3 29 51/3 57P \n4 29 yoly 1 1/10 20 50/3 3 1/3 3 54 1 34/34 86 41p costScale 7/6 71 48/21 49/21 6 52 doduc 8/7 52 62I3 \n49/3 00 tomcatv 1/1 38 2/0 50/0 2 98 41P ~ fPPPP dcg 212 21 40/4 46/4 4 51 sp{ce2g6 9/8 21 53/8 52/8 \n2 27 sgefat 2/2 18 28/8 61/8 8 73 dnasa7 1/1 10 68/4 55/4 4 i58 matrix300 1/1 4 99/0 66/0 3 99 MEAN 12/8 \n43 51/10 49/10 Std.Dev. 10/8 23 19/8 13/8 4 Table 2. Dynamic breakdown of loop branches vs. non-loop \nbranches. Prd shows the miss rate for the loop predictor on loop branches. For each class of branck M \nis the miss rate for the perfect predictor. All is the percentage of all branches 7. that are non-loop \nbranches. Tgt shows the results for predicting the target successor of each non-loop branch and Rnd shows \nthe results for predicting each non-loop branch randomly. Finally, Big shows how many non-loop branches \nin the program contri\u00adbuted more than 5 prcent of all dynamic non-loop branches, and what percentage \nis accounted for by these big branches. performance of a simple strategy that always predicts the target \nsuccessor. Not surprisingly, this heuristic does not fare well. Sometimes it pays to choose the target \n(grep}, sometimes the fall-thru (compress), and sometimes neither does well (qpt, m). The mean miss rate \nis 51% * 19%. In fact, for many benchmarks, random prediction ( Rnd ) per\u00adforms as well as or better \nthan predicting the target (mean of 4990 k 13Yo). These numbers show that simply predict\u00ading the target \nor fall-thru produces results of varying quality with mediocre overall performance. For a compiler to \npredict non-loop branches well for an architecture such as the DEC Alpha, a more sophisticated strategy \nis necessary. Correctly predicting a frequently executed branch hms a high payoff. The column Big shows \nhow many distinct non-loop branches in each program generate more than 5 percent of the dynamic non-lcmp \nbranch executions and the percentage of dynamic non-loop branches accounted fcx by these branches. For \nsome benchmarks (eqntou, grep, tomcatv, wzatrix300) a handful of non-loop branches in the program produce \nmost of the dynamic non-loop branches. For such programs, the performance of a predictor depends crucially \non predicting these branches correctly. Other pro\u00adgrams (gee, lCC, qpt, xlisp, congress, doduc) execute \nmany different branches, each of which contributes a small per\u00adcentage of the dynamic non-loop branches. \nTo summarize the main points of this section:  Branches that control the iteration of loops can be identified \nand predicted accurately with natural loop analysis. Any branch not identified as a loop branch by natural \nloop analysis cannot dircxx.ly control the iteration of a loop.  For many programs, non-loop branches \ndominate the loop branches and must be predicted accurately to get good overall branch prediction. Naively \npredicting the target or fall-thru successor for non-loop branches produces middling results.  Static \nprediction has the potential to accurately predict non-loop branches since most non-loop branches choose \none direction with high probability.  4. HEURISTICS FOR NON-LOOP BRANCHES This section examines a number \nof simple heuristics for predicting non-loop branches. The heuristics are com\u00adpletely automatic and only \nmake use of information avail\u00adable from an executable tile. Some heuristics could clearly be refined \nand made more accurate with source-level infor\u00admation available to a compiler. This section examines \nthe performance of each heuristic in isolation. The next section discusses how we combined these heuristics. \nTable 3 summarizes the results for each benchmark and heuristic. Each entry in the table presents the \npercentage of (dynamic) non-loop branches to which the heuristic applies (bold number) and the miss rates \n(for the heuristic and per\u00adfect predictors). A table entry is left blank if the percentage of branches \ncovered is less than one percent. For reference, the second column of the table ( NL ) repeats the percen\u00adtage \nof all branches that are non-loop branches. It is useful to keep this percentage in mind when examining \nthe effec\u00adtiveness of these heuristics. 4.1. Opcode Heuristic We predict some branches based on the branch \ninstruction opcode. The MIPS R2000 has integer branch instructions that branch if an operand register \nis less than, less than or equal, greater than, or greater than or equal to zero (bl t z, blezr bgt z, \nbge z). Because many programs use negative integers to denote error values, the heuristic predicts that \nblt z and ble z are not taken and that bg-t z and bge z are taken. The heuristic also identifies floating \npoint comparisons that check if two floating point numbers are equal, predicting that such tests usually \nevalu\u00adate false. As Table 3 shows, the Opcode heuristic performs very well for most benchmarks, although \nits coverage varies executed rather than avoided. Marty compilers generate widely. The heuristic performs \npoorly on spice2g6 because code for while loops and for loops by generating an if-then of a high number \nof integer branches that compare against a around a dwntil loop, replicating the loop test in the if\u00adnegative \nvalue. then condition (this strategy avoids generating an extra unconditional branch). The heuristic \ncatches these cases as 4.2. Loop, Call, Return, Guard, and Store Heuristics well as branches around loops \nexplicitly specified in the program. The performance of this heuristic is quite good The following heuristics \nare based on properties of the basic except on compress, ghostview, and matrix300. It hasblock successors \nof a branch. Each heuristic consists of two excellent coverage and/or performance on xlisp andpieces \nof fixed information, a selection property and a pred\u00ad espresso and doduc. ic[or. If neither successor \nto the block containing the condi\u00adtional branch has the selection property or both have the Call Heuristic \nproperty, no prediction is made. If exactly one successor The successor block contains a call or unconditional\u00adhas \nthe property, the predictor chooses either the successor ly passes control to a block with a call that \nit dom\u00ad with the property, or the successor without the property, depending on the heuristic. inates, \nand the successor block does not postdom\u00adinate the branch. If the heuristic applies, predict the Loop \nHeuristic successor without the property. The successor does not postdominate the branch and is either \na loop head or a loop preheader (i.e., passes This heuristic surprised us. Initially, we had believed \nthat a control unconditionally to a loop head which it dom\u00ad branch that decided between executing or \navoiding a call inates). If the heuristic applies, predict the successor would execute the cdl, as programs \ntypically make calls to with the property. perform useful work. However, the numbers strongly show the \nexact opposite, especially for the first set of programs. The loop heuristic determines if a branch chooses \nbetween In examining the programs we found that many conditional executing or avoiding a loop and predicts \nthat loops are calls are to handle exceptional situations. Just one example Program NL Opcode Loop call \nReturn Guard Store Point gcc 73 12 26/5 8 34/8 18 28/8 10 32/10 41 36/ 12 27 33/7 9 46/ 16 lCC 71 2 33/2 \n10 36/12 23 14/5 9 33/6 48 29/14 47 47/13 32 37/19 qpt 70 7 31/14 10 30/10 31 18/6 22 28/5 46 16/5 24 \n39/8 10 15;10 compress 66 56 29/13 4 87/12 32 6/6 53 41/26 58 78/9 Xlisp 62 3 1/1 20 12/2 44 25/7 35 \n20/1 41 13/3 29 65/16 20 14/0 addalg 52 19 19/15 8 16/5 8 42/35 2 43/1 57 58/29 30 39/21 64 51/38 ghostview \n52 47 1/1 17 73/1 1 22 18/1 7 64/5 51 60/5 29 64/1 18 19/10 eqntott 49 2 21 3 10/1 2 0/0 1 63/13 2 88/12 \nm 48 37 8/1 ]6 11/6 I45 9/1 I6 36/0 \\35 33/1 ]47 55/1 I6 77/1 grep 44 68 0/0 33 2/1 I wngress 40 3 12J4 \n4 16/5 46 29/3 31 47/3 42 38;4 26 42/6 34 15/6 espresso 37 21 22J3 3 27/4 5 33t28 27 69/16 16 34/21 awk \n29 11 0/0 9 4/1 53 4/1 16 17/5 38 912 25 58/1 21 10/2 poly 20 25 0/0 34 0/0 32 36/4 46 36/3 2 100/0 31 \n29/6 fPPPP 86 22 20/6 5 9/0 11 54/30 15 29/4 52 27/8 25 4//6 13 91/2 wstscale 71 8 2/0 3 13/3 5 91 2 \n2 8/0 39 45/26 39 40/17 1 23/16 doduc 52 42 31/6 7 4J0 10 58/2 17 27/1 44 25/3 27 30/4 tomcatv 38 99 \n99/0 99 1/0 dcg 21 13 7/36 14/57 5/151 4/128 9 68/4 1 51/2 spice2g6 21 40 81/2 3 36/1 23 9/4 15 20/5 \n33 2;; 29 15/3 sgefat 18 8 12J4 10 1/1 5 26/1 3 42/1 49 34/5 44 16/9 dnasa7 10 23 1 1/2 7 1/1 1s 32P \n78 33/4 2 73/9 rnatrix300 4 67 0/0 33 100/0 66 0/0 99 33/0 MEAN 16/4 25/4 22/6 28/4 38/8 45/8 41/10 \n- Std.Dev. 19/5 28/4 17/10 16/6 26I9 20/6 29/1 1 Table 3. The effectiveness of each heuristic for predctirtg \nnon-loop branches, applied individually. For each heuristic, the table shows the percentage of dynamic \nnon-loop branches to which the heuristic applies (in bold) and the miss rates for those branches. A table \nentry is left blank if the coverage is less than one percent of all non-loop branches. Blank entries \nare not counted in the mean and standard deviation. of this is printing. For many programs, printing \nis an exceptional occurrence. Even in applications that print to standard output or to a file, most printing \nis done uncondi\u00adtionally rather than conditionally. Return Heuristic The successor block contains a return \nor uncondi\u00adtionally passes control to a block that contains a re\u00adturn. If the heuristic applies, predict \nthe successor without the property. There are several justifications for this predictor, the most compelling \nof which is recursion. Because programs must loop or recurse to do useful work, we expect that loops \niterate and that recursive procedures recurse. A return is the base case, which is the exception in recursion, \njust as a loop exit is the exception in iteration. In addition, many returns fi-om procedures handle \ncases which occur infrequently (i.e., error and boundary conditions). The performance of the return heuristic \nis good over most of the benchmarks. Guard Heuristic Register r is an operand of the branch instruction, \nre\u00ad gister r is used in the successor block before it is defined,2 and the successor block does not postdom\u00ad \ninate the branch. If the heuristic applies, predict the successor with the property. This heuristic \nanalyzes both integer and floating point branches. It attempts to find instances in which a branch on \na value guards a later use of that value. The intuition is that the function of many guards is to catch \nexceptional condi\u00adtions and that the common case is for a guard to allow the value to flow to its use. \nThe coverage for this heuristic is quite high over most of the benchmarks (we remind that reader that \nthe heuristic applies only if exactly one of the successors has the pro\u00adperty). The performance is fairly \ngood over most of the benchmarks, with stronger performance on the secondl set of programs. The heuristic \nperforms well on most of the pointer-chasing programs (gee, lCC, qpt, xlisp, congress) because the common \ncase for a null pointer test guarding the use of the same pointer is that the pointer is not null. The \nheuristic performs very poorly on romcatw, mispredict\u00ading the two branches that account for 99 %oof all \nnon-loop branches. These branches are inside a loop that determines the maximum value in an array of \nvalues (i.e., if (a[i,j] > max) then max := a[ij] fi). In this program the common case is to avoid updating \nthe maximum, but the guard heuristic predicts the opposite. The heuristic does not analyze past calls \nsince no interprocedural register use or definition information is computed. We note that global register \nallocation can greatly affect the coverage of this heuristic. Without performing global register alk.ation \non the benchmarks, the coverage for this heuristic would be much lower due to reloads of values, which \nthe heuristic does not detect. Store Heuristic The successor block contains a store instruction and does \nnot postdominate the branch. If the heuristic applies, predict the successor without the property. We \ntried this heuristic more out of curiosity than intuition about how it might perform. On the first set \nof benchmarks, the store heuristic has very poor performance and coverage is very high. However, the \nperformance improves on the floating-point intensive benchmarks. On tomcatv, the heuristic performs perfectly, \ncorrectly predicting the two branches that the guard heuristic mispredicted. 4.3. Pointer comparisons \nPointer comparisons either compare a pointer to null or compare two pointers. As mentioned before, in \npointer\u00admanipulating programs, most pointers are non-null. Furth\u00adermore, we expect equality comparison \nof two pointers to rarely be true. To distinguish pointer comparisons from other comparisons requires \ntype information that we did not have. But because the MIPS is a RISC architecture, there are very few \npossible code sequences for pointer comparis\u00adons. Two of the code sequences are shown below: load rM, \n. . . load rM, . . . . . . load rN, . . . beq ro, rM, . . . . . . beq rM, rN, . . . The pointer heuristic \nlooks for these two cases in the basic block containing the branch and predicts that the fall-thru is \ntaken. It also looks for the same patterns with a bne branch and predicts that the branch is taken.3 \nOf course, similar code sequences may be generated for comparisons that do not involve pointers. Our \nheuristic does not distin\u00adguish these cases ffom those involving pointers. However, a compiler could \neasily make the distinction. We made one small optimization to the heuristics, noting that many pointers \nare either local variables and addressed off the SP register (stack pointer), or are in the heap and \naddressed off a register other than SP or GP (pointer to global storage). If either load instruction \nloads off GP, the branch is not considered. If a local pointer variable is allocated a regis\u00adter, then \nthe heuristic will miss comparisons involving that pointer. 31rrboth cases, the heuristic does not apply \nif there is a call instmction between the load and the branch. The results for some pointer-chasing programs \nsuch as lCC,xlisp, qpt, and congress are fairly good. For some other pointer-manipulating programs, such \nas gee, the heuristic does not perform as well. This is because the heuristic picks up comparisons of \nvariables that are not pointer types. The heuristic could certainly be improved by incorporating type \ninformation. As expected, the pointer heuristics per\u00adforms poorly on the floating point benchmarks, which \ncon\u00adtain little to no pointer manipulation.  4.4. Discussion Unsuccessjid Heuristics We tried many heuristics \nthat were unsuccessful. These included heuristics that were based on the number of instructions between \na branch and its targe~ and the domi\u00adnation and postdomination relations between a branch and its successors. \nGeneralizations All of the heuristics discussed above are very local in nature. Excluding the information \nmade available from natural loop, domination, and postdomination analysis, they examine only information \nfrom the basic block containing the conditional branch or in successors of the block (at most two steps \naway). Some of the heuristics could clearly be generalized to consider more basic blocks. For example, \nthe guard heuristic could look farther away from the branch to see if the branch value is reused by an \ninstruction whose execution is conmolled by the branch. Other heuristics could be similarly generalized. \nIt remains to be seen how such generalizations affect the coverage and performance of the heuristics. \n 5. COMBINING THE HEURISTICS This section describes how we combined the heuristics from the previous \nsection into a single heuristic procedure for predicting non-loop branches. It is clear that more than \none heuristic can apply to a branch. We chose to combine the heuristics by totally ordering them. To \npredict a branch, the combined heuristic simply marches through the heuris\u00adtics until one applies and \nuses it to predict the branch. We will discuss later what to do for branches for which no heuristic applies. \nMany other approaches for combining the heuristics are possible, such as a voting protocol with weighings. \nHowever, for any such approach there is the central problem of prioritizing the heuristics. As Graph \n1 shows (see the Appendix for graphs), the ordering of the heuristics can have quite an impact on miss \nrate. The graph shows the average miss rate (for all bench\u00admarks except matrix300) for non-loop branches \nfor every possible order (there are 7! = 5040 possible orderings), where the orders have been sorted \nby miss rate. How does one choose an order for the heuristics? The best one can do is to analyze available \nbenchmarks to select a good order and hope that the order works well in the future when additional benchmarks \nare encountered. We performed the following experiment to see if it is reason\u00adable to expect that orders \npicked for a subset of the bench\u00admarks will perform well over all benchmarks. To get an even number of \nbenchmarks we eliminated matrix300, the least interesting of the benchmarks in terms of non-loop branch \nprediction. For each subset of cardinality 11 of the remaining 22 benchmarks, we computed the order that \nminimized the average miss rate (for non-loop branches) for the benchmarks in the subset.4 These 11 \nbenchmarks represent the known benchmarks. Using the chosen order, we computed the average miss rate \nfor all 22 bench\u00ad marks. The experiment consisted of ((~~) = 705,432) trials, one for each subset. Of \nthe possible 5040 orders, only 622 appeared in the trials. Graph 2 shows the 101 most fre\u00adquently occurring \norders (ordered by frequency) versus the cumulative percentage of all trials that these orders appeared \nin. As can be seen, almost 9070 of the trials are accounted for by the 40 most frequently occurring orders. \nGraph 3 shows the average miss rate (for all 22 bench\u00admarks) for each of the 101 most frequently occurring \nord\u00aders. The results of our analysis are encouraging. The 40 most common orders account for approximately \n90% of all trials and the average miss mte for most of these orders is below 277.. The order that occurred \nthird most frequently was also the order that minimized the average miss rate for all benchmarks. Table \n4 shows the 10 most common orders from the experiment and the percentage of the trials that each one \naccounts for. The Opcode, Call, and Return heuristics are consistently among the top 3 heuristics in \nthese orders. Computing the order that minimizes the miss rate for a set of benchmarks is not an inexpensive \nproposition, espe\u00adcially as the number of heuristics grows. A less expensive approach that we explored \nwas pair-wise analysis: we examined pairs of heuristics and for the set of branches in the intersection, \ncompared the performance of the two heuristics to determine a pair-wise ordering. The orders we found \nwith this analysis were generally inferior to those found by the previous experiment, but were in the \ntop quar\u00adter of performers. Table 5 presents the results for the simple heuristics applied in the order \nPoint + Call + Opcode + Return + Store + Loop + Guard. Recall that as soon as a heuristic applies, the \nprediction is made and the next branch is con\u00adsidered. If no heuristic applies to a branch, then a Defaulr \nprediction is made, which is simply a random prediction. Each benchmadc gets equal weight in this average. \nIt would also be in\u00adteresting to use a weighted average that accounts for the percentage of dynamic non-loop \nbranches in and number of predictions made for each benchmark. % of Trials I Miss Rate Order I 8.92 \nI 26.00 I OocOde call Return Store pOillt LOOD Guard {J 8.50 25.52 Cil Opcode Return Store Point Loo; \nGuard 7.52 25.50 Point call Opmde Return Store Loop Guard Point cdl 5.82 2s.59 I.Amp Rem Opcode Return \nStore Guard 5.22 27.12 Opcode Cdl Store Point Guard LOOp 4.99 2s.99 Point Oocode Call Retura Store LOrm \nGuard t,  4.86 26.64 I call O~ode Return Store Point Gu;d LOOp 4.82 26.04 I LOOtl Call @code Return \nStore Point Guard L J  4.58 25.55 cdi Opcode R&#38;rn Point Store Loop Guard 4.40 26.02 Opeode Call \nReturn Point Store Loop Guard Table 4. The 10 most common orders born the (1 ~ experiment. Shown with \neach order are the percent of all triids in which the order aP\u00ad 23 peared and the average miss rate (all \n22 benchmarks) for that order. Program I Point call Gpcode Return Store I Loop Guard I Default gcc 46/16 \nI 17 28fl I 10 28/5 16 30/10 I 15 28/8 32/5 I 19 33/14 1 56/14 im 32 37/19 21 12/3 2 29/1 4 45P 7 31P \n2 65/20 11 32J11 20 41/11 qpt 10 15/10 26 14/6 6 30/14 13 15/6 5 47/7 3 13/12 11 10/3 24 55/14 compress \n32 6/6 24 60/22 18 62/19 16 51/49 10 43/0 Xlisp 20 14/0 37 23/9 15 18/2 11 71/21 7 7/1 10 59/12 addalg \n64 51/38 1 36/5 19 19/15 4 14/1 2 3519 10 60127 ghostview 18 19/10 21 15/1 33 1/1 2 33/9 9 8/1 14 40/13 \nI 12 45fl eqntott 2 88/12 2 1/1 95 50/26 m 6 77/1 42 10/1 6 1/1 5 31/0 13 50/3 5 83/0 21 57/1 grep 68 \n0/0 congress 34 15/6 32 22/3 2 23/7 13 50/1 1 25/2 3 1/0 4 20/8 11 68/1 espresso 12 26/4 I 14 31/30 \nI 14 35/22 16 6/3 6 48/26 S6 26/12 = awk [21 10/2 I38 3/2 I 18 11/5 I3 33/0 1 29/3 4 17/2 24 34P poly \n32 36/4 21 0/0 24 69/6 12 65/4 11 39/0 13 91/2 11 54/30 18 14/1 12 28/5 16 54/6 12 8/3 18 50/ 9 costScale \n1 23/16 5 8/1 4 4/0 2 3/0 33 32/19 25 38/36 29 26/:8 doduc 10 58/2 39 33/6 14 25/0 14 14/3 15 31/1 8 \n55/1 tomcatv 99 1/0 dcg 1 51/2 7 3/0 8 11/4 48 3t2 4 82J4 spice2g6 23 9/4 20 76/1 14 20/5 5 12/3 sgefat \n5 27/1 5 19/5 2 46/1 40 11/10 dnasa7 22 11/1 ~= 13 22/7 w matrix300 67 0/0 33 100/0 41/10 21/5 20/5 \n28/6 36/7 35/5 33/12 45/1 1 Std.Dev. 29/1 1 17/7 21/6 17r7 23/7 36/6 19/14 1719 Table 5. The performance \nof the simple heuristics, when applkd in a prioritized ordering (left to right). If no heuristic applies \nto a branch, it is covered by the Default heuristic, which prdlcts ran&#38;mdy (the same prediction is \nmade as in Table 2). A table entry is left blank if the coverage is less than one percent of all non-loop \nbranches. Blank entries are not counted in the mean and standard deviation. For these branches, the same \nprediction is made as the ran-adds in predictions for loop branches, as discussed in Sec\u00addom prediction \nfor Table 2. tion 3. For comparison, the column Loop+Rand shows the miss rate for loop prediction on \nloop-branches and ran- Table 6 presents the results of the combined heuristic for dom prediction on non-loop \nbranches. non-loop branches. The column Heuristics shows the percentage of dynamic non-loop btanches \ncovered by the Table 7 contains the means and standard deviations of the heuristics excluding the default, \nand the miss rates for those above results for all benchmarks and for the set of bench\u00adbranches. As this \ncolumn shows, the combined heuristic is marks excluding eqntott, grep, tomcatv, and matrix300 (the effective \nin terms of coverage and miss rate, even for pro-programs for which over 90% of the non-loop branches \nare grams with much conditional control flow such as gee, accounted for by a few branch instructions). \nWe also xlisp, and doduc. The column +Default adds in predic-include the results for target and random \nprediction of non\u00adtions for branches covered by the default prediction. All Loop Program Heuristics +Default \nAll +Rand gcc 79 32/10 37/1 1 33/12 43m icc I 80 30/12 I 32/12 I 28/12 I 42J12 &#38;ptto. I 5 3iJ5 I \nsotis I 26/i3 I 26/i3 m 1 79 27/1 34/1 2012 ~ 2a/2 grep 168 1/1 1/0 15/1 16/1 congress 89 23/4 28/3 24I9 \n3519 espresso 44 25/15 26/13 21/13 27/13 awk 76 8/2 14/3 7/3 19r3 poly 89 40/4 40/3 17/9 15/9 fPPPP 82 \n40/7 42/9 41/ 13 40/13 wstScale 71 30/22 29/21 22/17 37/17 doduc I 92 31/3 3313 21/5 30/5 tomcatv 100 \n1/0 2/0 ] 1/1 20/1 dcg I73 11/2 I 15/4 I 5/2 I 12/2 spice2g6 75 33/5 36/8 14/8 18/8 sgefat 1% 25~ 26/8 \nI 7/3 13/3 dnasa7 95 29/4 3244 I 4/1 6/1 matrix300 100 33/0 33/0 3/1 I 4/1 I  Table 6. Fhsl results. \nHeuristics shows the percent of non\u00adloop branches covered by the heuristics (bold) and the miss rates. \n+Default adds in the predictions for non-loop branches not covered, and All adds in predictions for loop \nbranches. For comparison, Loop+Rand is the miss rate for loop pre&#38;ction on loop branches and random \nprediction on non-loop branches. loop branches (from Table 2) for comparison. On average, our heuristics \nprovide a miss rate of 26% on non-loop branches. 6. INSTRUCTIONS PER BREAK IN CONTROL The previous sections \nmeasured the performance of branch prediction by miss rate. Such a metric is useful because most modem \narchitectures exact a performance penalty for mispredicting a branch. However, such a metric does not \nidentify the performance benefit that can be realized when the percent of mispredicted branches decreases. \nFor exam\u00adple, with good branch prediction instruction-level parallel architectures can find more data-independent \nthreads to execute in parallel [5] and compilem can globally schedule code to improve program performance \n[14]. This section meastues the performance of branch predic\u00adtion based on its ability to find sequenees \nof instructions without a mispredicted branch. Fisher and Freudenberger have proposed a metric [7]: instructions \nexecuted per break in control (a break in control is a mispredicted branch instruction, an indirect jump \nother than procedure return, or an indirect call; correctly predicted branch instructions are not breaks \nin control). As they argue, the ability of branch prediction to find long sequences of instructions without \na break in control depends not just on the branch predictor s miss rate, but also on the density of mispredicted \nbranches in the program s instruction stream. Fisher and Freudenberger computed instructions per break \nin control (IPBC) based on execution profiles (i.e., total number of instructions executed / number of \nbreaks in control in the execution). We used instruction traces of pro\u00adgram executions to colleet more \ndetailed data on IPBC than is available from an execution profile. With instruction traces, we were able \nto measure the number of instructions executed between each pair of consecutive breaks in con\u00adtrol. This \ninformation is simply not available from an exe\u00adcution profile. Our data shows that the profile-based \nIPBC average underestimates the length of available sequences and fails to accurately distinguish different \nbranch predic\u00adtion strategies. We collected instruction traces for the following bench\u00admarks: gee, lee, \nqpt,xlisp, doduc, fpppp and spice2g6. For the most pm we chose benchmarks that contain complex control \nflow and are hard to predict. The instruction traces were generated by the same datasets as in the previous \nsec\u00adtions. We used three branch predictors in this experiment: @ the perfect predictor (Perfect); e loop \nprediction on loop branches and the ordering Point + Call+ Opcode + Return + Store + Loop + Guard on \nnon-loop branches (Heuristic). loop prediction on loop branches and random predic\u00adtion on non-loop branches \n(Loop +Rand). Each branch predictor defines a set of breaks in control in a program s execution. Each \nbreak in control B defines a Table 7. Means and standard deviations of results from Table 6, for two \nsets of programs. (all) is for all the benchmarks. (most) excludes the programs eqntott, grep, torncatv, \nand tnatrk?OO. Results for target and random prediction of non-loop branches are included for com\u00adparison. \nsequence of instructions from (but not including) the break in control preceding B up to and including \nB. These sequences partition the instruction trace of an execution. For each predictor, we recorded the \nfollowing information: V j, O S j <999, the number of sequences whose leng,th is in the interval (lOj, \n10j+9). The last bucket (j= 999) records all sequences of length greater than or equal to 9990. For each \nbucke~ we also recorded the sum of the length of the sequences associated with that bucke~ We graph the \ndistribution of sequence lengths by platting sequence length (x) versus the percentage of the executed \ninstructions accounted for by sequences of length less than x. The graphs (4-11) can be found in the \nAppendix. IEaeh branch predictor contributes a plot to each graph. The slower the growth rate of a plo~ \nthe better. For each predic\u00adtor, the graph also shows the miss rate (for ail branches) and the IPBC average. \nIn many eases, the Heuristic plot is closer to Loop+Rand than to Pe~ect because very high accuracy is \nnecessary to obtain long sequences, especially in programs in which there is conditional control flow \nin most loops and the basic block size is small (i.e., gee, /cc, qpt, xlisp and doduc). In these programs \nnon-loop branches are distributed fairly regularly over the entire execution, so the miss rate (on non-loop \nbranches) must be very loIw to get long sequences. A very simple model captures this behavior. The model \nassumes that every basic block is of unit length and ends with a conditional branch, branches are independent, \nand every branch has a miss rate of m. Ifs is a sequence length (s 21) then the function s-1 f(m,s) = \nm ~ (l-m)i = 1-(l-m)s i=(.1 represents the percentage of the executed instructions accounted for by sequences \nof length less than or equal tos. Graph 12 shows plots of this function for miss rates between 2.5% and \n30% by increments of 2.5%. The payoff in sequence length comes not from moving from 30% to 15%, but fmm \nreducing the miss rate to less than 15%. Similar behavior can be found in gcc (Graph 6), kc (Graph 7), \nand qpt (Graph 8). While the growth rates of the plots and accompanying absolute miss rates are different \nthan the model (due to the simplifying assumptions of the model), the relationship between miss rate \nand plot roughly folllows that of the model. We now turn our attention to the IPBC average, the profile-based \nmetric defined previously. Because the IPBC average evenly distributes mispredicted branches over the \nentire execution, it tends to underestimate the available sequence length. This can lead to underestimation \nor overestimation of the difference between predictors, depending on the control flow complexity of the \nprogram. We use the benchmark spice2g6 tQ illustrate some of these points. As Graph 4 shows, the IPBC \naverage for the perfect predictor is 183 (instructions per break in conmol). However, for the perfect \npredictor, sequences of length less than 183 account for approximately 30% of the executed instructions. \nIf we examine Graph 5, the reason for this disparity becomes clear. This graph shows sequence length \n (x) versus the percentage of breaks in control accounted for by sequences of length less than x. Sequences \nof length less than 183 account for approximately 80% of the breaks in control. Because the profile-based \nIPBC average distributes the breaks in control evenly over the enthe execution, the highly skewed distribution \nof sequence lengths causes the average to underestimate the available sequence length. This skew occurred \nto varying degrees in all the bench\u00admarks. For many of the benchmarks we examined it was more informative \nto look at the sequence length at which 50% of the executed instructions were accounted for (we refer \nto this length as the dividing length). For the perfect predictor for spice2g6, the dividing length is \napproximately 800 instntctions. When a high percentage of the sequences (i.e., breaks in control) account \nfor a low percentage of the execution, sub\u00adstantial differences in the IPBC average may not accurately \nreflect the differences between pr@lctors. In the case of spice2g6, the IPBC average overestimates the \ndifference between the perfect predictor and other predictors. As Graph 4 shows, the IPBC averages are \n87, 108, and 183 while the dividing lengths are approximately 550, 650, and 800 instructions for the \nLoopi-Rand, Heuristic and Perfect predictors, respectively. The reason for this disparity has to do with \nthe control flow complexity of the benchmark. In spice2g6 predicting loop branches gives a big payoff. \nCorrectly predicting non-loop branches is not crucial to tinding sequences of long length. However, differences \nin miss rates for non-loop branches can have a large impact on the IPBC average because the average distributes \nmispredicted branches evenly over the entire execution. The IPBC average can also underestimate the difference \nbetween predictors. This is especially true for benchmarks in which there is conditional control flow \ninside most loops. In these cases, the non-loop branches are truly more evenly distributed in the execution \nand the IPBC average tends to underestimate the dividing length for the perfect predictor. For example, \nin the lcc benchmark, the perfect predictor has an IPBC average of 58 and a dividing length of 100, while \nthe Heurisfic predictor has an IPBC average of 28 and a dividing length of 40. To summarize the results \nof this section: High accuracy on non-loop branches is crucial to get\u00adting long sequences for programs \nwith conditional control flow in loops. e Instruction traces provide a much more accurate view than execution \nprofiles of the impact of branch pred\u00ad iction on instructions executed per mispredictcd branch. The skewed \ndistribution of sequence lengths causes the profile-based IPBC average to underesti\u00ad mate the length \nof available sequences. Depending on the control flow complexity of a program, this may cause the IPBC \naverage tooverestimate orunderesti\u00admate the difference between predictors. 7. OTHER DATASETS If a program-based \npredictor is to be useful it must have good performance over different executions of the same program, \nas well as over executions of different programs. We ran a number of the benchmarks on different datasets \nto examine how well the Heuristic predictor performs. Graph 13 presents the results. The first plot for \neach benchmark is for the dataset used in the previous sections. For each benchmark and dataset, the \ngraph shows the miss rate for the perfect predictor and for the Heuristic predictor. We emphasize that \nthe heuristic predictor makes the same pred\u00adictions no matter which dataset is used, while the perfect \npredictor is dataset dependent, making the best possible static prediction per dataset. For many of the \nbenchmarks, such as gee, lCC, qpt, compress, xlisp, ghostview, grep, espresso, costScale, and doduc, \nthe miss rates do not vary too widely. The con\u00adsistent results for benchmarks with large amounts of condi\u00adtional \ncontrol flow is encouraging. It is often the case that a difference in miss rates for the heuristic predictor \nis accom\u00adpanied by a similar difference in the miss rates for the per\u00adfect predictor. For example, in \nthe fist two datasets for spice2g6 the miss rate for both the heuristic and perfect predictor approximately \ndouble. 8. RELATED WORK Related work on static branch prediction falls into two categories: profile-based \nand program-based. McFarling and Hennessy reported that profile-based static prediction yields results \ncomparable to dynamic hardware-based methods [11]. As mentioned before, Fisher and Freuden\u00adberger examined \nprofile-based static prediction in detail, showing that most branches behave similarly over different \nexecutions of the same program and that profiles can be used to effectively predict branch directions \nin other execu\u00adtions [7]. J. E. Smith discusses several static prediction strategies based on instruction \nopcodes, applied to six FORTRAN programs with success [16]. Program-based static predic\u00adtion was used \nby Bandyopadhyay, et al, in a C compiler for the CRISP microprocessor [3]. They identified loop tests \nas those in the boolean expression associated with a loop con\u00adstruct. Branch prediction for tests associated \nwith if state\u00adments was accomplished by a table lookup based on the comparison operator and operand types. \nThe authors reported an extremely high success rate but gave no numbers or details on this table lookup \nstrategy. Wall used program-based heuristics to estimate various program profiles (the number of times \na particular program com\u00adponent executes) rather than to predict individual branches [17]. He reported \npoor results for his estimator, compared to a randomly generated profile. Lee and A. J. Smith s paper \non branch prediction stra\u00adtegies reported that for the workloads they considered (IBM 370, DEC PDP-11, \nand CDC 6400) branches were talen twice as often as they fell through [10]. Lee and Smith con\u00adsidered \nbranch prediction based on instruction opcodes and dynamic branch history. They found that the miss rates \nfor opcode prediction ranged from 20.270 to 44.8 ZO with an average of 30.170. 9. CONCLUSIONS We have \npresented a simple set of program-based heuristics for statically predicting branches and combined these \ninto a single branch prediction heuristic that performs well for a large and diverse set of programs. \nIn addition to using natural loop analysis to predict branches that control the iteration of loops, we \nfocus on heuristics for predicting non-loop branches, which dominate the dynamic branch count in many \nprograms. These heuristics are local in nature, requiring litl.le program amlysis, yet are effective \nin terms of coverage and miss rate. We believe that many of these heuristics could be generalized and \nrefined with infor\u00admation available in a compiler to produce even better results. ACKNOWLEDGEMENTS We \nwould like to thank Susan Horwitz for her support of this work. Thanks to Paul Adams for providing the \ncongress benchmark, to Stefan Freudenberger for his extra SPEC datasets, and to Steve Kurlander for help \nwith some benchmarks. Josh Fisher, Mark Hill, and Guri Sohi pro\u00ad vided helpful comments on earlier drafts \nof this work. Thanks also to Todd Austin for his input. REFERENCES 1. A. Aho, R. Sethi, and J. Ulhrr~ \nCompilers: Principles, Tech\u00adniques curd Tools, Addison-Wesley, Reading, MA (1986). 2. T. Ball and J. \nR. Larus, Optimally Profiling and Tracing Pro\u00adgrams, Conference Record of the Nineteenth ACM Sympo\u00adsium \non Principles of Programming Languages, (AKru \u00adquerque, NM, January 19-22, 1992), pp. 59-70 ACM, (1992). \n 3. S. Bandyopadhyay, V. S. Begwani, and R. B. Murray, Com\u00ad piling for the CRISP Microprocessor, Spring \nCompcon 87, pp. 96-100 IEEE Computer Society, (February 1987).  4. Systems Performance Evahration Cooperative, \nSPEC Newsletter (K. Mendoza, editor) 1(1)(1989). 5. J. A. Fisher, Trace Scheduling: A Technicme for \nGlobal Microccde Compaction, IEEE Transactions-on Computers C-M)(7) pp. 478-490 (July 1981).  6. J. \nA. Fisher, J. R. Ellis, J. C. Ruttenberg, and A. Nicolau, Parallel Processing: A Smart Compiler and a \nDumb Machine, Proc. of the ACM SIGPLAN 1984 Symposium on Compiler Construction (SIGPLAN Notices) 19(6) \npp. 37-47 (June 1984).  7. J. A. Fisher and S. M. Freudenberger, Predicting Conditionat  Branch Directions \nFrom Previous Runs of a Program, Proceedings of the 5th International Conference on Architectural Support \nfor Progr arnmrning Languages and Operating Systems (ACM SIGPLAN Notices) 27(9) pp. :B5-95 (October \n1992). 8. S. L. Graham, P. B. Kessler, and M. K. McKusick, An Exe\u00adcution Profiler for Modular programs, \nSo@are-Practice and Experience 13 pp. 671-685 (1983). 9. G. Kane and J. Heinrich, MIPS RISC Architecture, \nPrentice Hall (1992). 10. J. K. F. Lee and A. J. Smith, Branch Prediction Strategies and Branch Target \nBuffer Design, Computer 17(1) pp. 6-22 (January 1984). 11. S. McFarling and J. Hennessy, Reducing the \nCast of 13ranches: Proceedings of the 13th Annual International Symposium on Computer Architecture, pp. \n396-403 ACM and IEEE Computer Society, (June 1986). 12. W. G. Morns, CCC: A Prototype Coagulating Co&#38; \nGenera\u00adtor, Proceedings of the SIGPLAN 91 Co@rence on Pro\u00adgramming Language Design and Implementation, \n(Toronto, June 26-28, 1991), ACM SIGPLAN Notices 26(6) pp. 45-58 (June, 1991). 13. D. A. Patterson and \nJ. L. Hennessy, Computer Architecture: A Quantitative Approach, Morgan, Kaufmann Publishers Inc. (1990). \n 14. K. Pettis and R. C. Hanson, Profile Guided Code Position\u00ading, Proceedings of the ACM SIGPLAiV 90 \nCo#erer~e on Programming Language Design and Implementation (SIG-PLAN Notices) 25(6) pp. 16-27 ACM, (June, \n1990). 15. R. L. Sites, Alpha Architecture Reference Manual, Digital Press, Burlingto~ MA (1992). 16. \nJ. E. Smith, A Study of Branch Prediction Strategies, Proceedings of the 4th Annual Internatwnal Syrnposi~~m \non Corr@er Architecture (SIGARCH Newsletter) 9(3) pp. 135-148 ACM and IEEE Computer Society, (May 1981). \n 17. D. W. Wall, Predicting program Behavior Using Real or Estimated profiles; Proceedings of the SIGPLAN \n91 Confer\u00adence on Programming Language Design and Implementation, (Toronto, June 26-28, 1991 ), ACM SIGPLAN \nAlotices 26(6) pp. 59-70 (June, 1991).  APPENDIX -GRAPHS ~1 2sI J 0 1000 2000 W30cl 4000 \u00ad5040 Order. \nGraph 1. Average miss rates for each of the 7! = 5040 possible orderings, sorted by miss rate. 100I 1 \n.~ 0 1020304050 607060S0100 303 Most common orders+ Graph 2. The most common 101 orders from the (~~) \nexperi\u00adment and their cumulative dkxribution in the trials. \u00ad 40 II 1. ! 2!5 . ..~ i 00 301 Most Common \nOrder. 0 30203040 506070S0 90 Graph 3. Average miss rates (all 22 benchmarks) for the most AA common \n101 orders from the ( fi) eXPe.fimeIIt. SPie4?S6 100 so 00 70 60 50 40 ---------Loop+Rand 3 a% 67 OPbG \n Hwvrtatic 1 4Y. 1 0S ipt.e_ -----P-dsct 6-/. 1 S3S ipbc 30 20 10 I 10 6*O ?0,0 1530 20~0 2s70 30?0 3si0 \n-1o-o as-o 5070 =-w.. -~..eth r.. . 0 Graph 4. spice2g6: cumulative d~~ibution of sequence lengths. \n.pi.-2*6 i 00 100 ,=.e----------... . . . ...-..:..: ----------.---..--\u00ad 90 90 .\u00ad -~~--\u00ad 80 #s ---\u00ad~ \n _. --\u00ad70 70 /e\u00ad -------I-oop+f land ,.\u00ad60 Heuristic ii 60 Y .G -----Perfect ,. :~ ,\u00ad 1 * 50 ~ 0 \n+ ,, 40 . / so so ~ 40 ---------LOOp+Fland 42-/. 16 tpbc ,, H.u.istic 24-/. 25 ipk.c , -----Perfect \n1 o-% -1= ipb , 20 20 f ,/ 10 ?0 0 10 so,,0 Z,o..0 o s-w.. ---.9tfi Graph 5. spice2g6: cumulative \ndistribution of breaks. Graph 8. qpt: cumulative distribution of sequence lengths, 100 ..4. .. .... .... \n.... ... .... .... ... .... ...\u00ad .. ........ .. ..-..-.....-.*..*....------------.. ~.e.. . so w ~. \n-----\u00ad ---- , /\u00ad ,--- / .,.  ,. 80 So r\u00ad.. --- ,. ---\u00ad *, 70 70 ,- 60 60/ ,, F\u00ad 50 /~ -----------Leop+Rand \n4a-/. 16 ipk ,,< 40 ,. 40 Heuristic SW% 21 i pbc --- -Pc3rfc3ct 1 2-/. 50 i+c f . ..........-Loop+ \nRand 42-/. 1S ipk.c  30 30 Heuristic 2e-/. 26 Ipbc =~ ? -----Perfect 1 2-/. 5=$ Ipbc , 20 20/ ,: \n 10,,- *O / , e , ,/ 0 0,0 So1,0 + 7 So 210 Ze.o ,0 60 ,,0 160 270 ,0 Seq e ce Length s-q.e.ca L#a.~th \n Graph 6. gee: cumulative distribution of sequence lengths. Graph 9. xlisp: cumulative distribution of \nsequence lengths. 300 400 .................. ................ .......  .................   < r 90 \n/ 90 ....... , ______  so ao ,-. _-. 70 ,,/----------- ------ - ,\u00ad70 -,- ,----, _.. so 60 50 / So \n40 40 -------l-oop+ Rand 42-/. 19 tpbc -------I-oop+flamd 30 -/ 57 ,pk.c 30 Heuristic 2e-/. 2e ,pbc \n30 He nstic 21 -/. 79 lpk.c -----Per felct 12 -/. 5f3 ipbc -----Perfect 5 -/ 331 lpbc 20, 20 7,; \n 1 >0 10 0. 0 ,0 2,0 4,0 S,o Ix, c. 10,0 lZ, O 7 +,0 ,e, o ,,3,0 20,0 =-==7...\u00ad L...9th Graph 7. lcc \ncumulative distribution of sequence lengths, Graph 10. doducz cumulative distribution of sequence lengths. \n 300 -------l_oop+ Rand 40 % =7 ipbc Heuristic 41 A 242 ipbc -----Psrkmt 13-% 963 ipbc ,---\u00ad ............. \n........ ., ~ , -- -------------J ,-------------. , ., :1 u1[ I 00 ~000 2000 =0 4000 -S000 7000 aOOO \nS000 10000 *1 21 -i 43 Si 61 7* 61 91 10? s-we.-L9nmll Sequcwlm I-cwwth Graph 11. fpppp:cumulative distribution \nof sequence lengths. Graph 12. A simple model for cumulative distribution of se\u00adquence lengths. The function \ny = l (l+n)x is plotted for miss rates (m) between 0.025 and 0.3 by increments of 0.025. 50 40 ml 30 \nJl ~ s CO . = 20 10 0 Graph 13. Miss rates for different runs of various benchmarks. Miss rate is for \nall branches, loop and non-loop.  \n\t\t\t", "proc_id": "155090", "abstract": "<p>Many compilers rely on branch prediction to improve program performance by identifying frequently executed regions and by aiding in scheduling instructions.<italic>Profile-based</italic> predictors require a time-consuming and inconvenient compile-profile-compile cycle in order to make predictions. We present a <italic>program-based</italic> branch predictor that performs well for a large and diverse set of programs written in C and Fortran. In addition to using natural loop analysis to predict branches that control the iteration of loops, we focus on heuristics for predicting non-loop branches, which dominate the dynamic branch count of many programs. The heuristics are simple and require little program analysis, yet they are effective in terms of coverage and miss rate. Although program-based prediction does not equal the accuracy of profile-based prediction, we believe it reaches a sufficiently high level to be useful. Additional type and semantic information available to a compiler would enhance our heuristics.</p>", "authors": [{"name": "Thomas Ball", "author_profile_id": "81100472343", "affiliation": "", "person_id": "PP39073493", "email_address": "", "orcid_id": ""}, {"name": "James R. Larus", "author_profile_id": "81100277326", "affiliation": "", "person_id": "P132790", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/155090.155119", "year": "1993", "article_id": "155119", "conference": "PLDI", "title": "Branch prediction for free", "url": "http://dl.acm.org/citation.cfm?id=155119"}