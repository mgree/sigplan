{"article_publication_date": "06-01-1993", "fulltext": "\n Practical Data Breakpoints: Design and Implementation* Robert Wahbet Steven Luccot Susan L. Grahamt \nComputer Science Division, 571 Evans Hall UC Berkeley, Berkeley CA, 94720 Abstract A data breakpoint \nassociates debugging actions with programmer-specified conditions on the memory state of an executing \nprogram. Data breakpoints provide a means for discovering program bugs that are te\u00addious or impossible \nto isolate using control breakpoints alone. In practice, programmers rare] y use data break\u00adpoints, because \nthey are either unimplemented or pro\u00adhibitively slow in available debugging software. In this paper, \nwe present the design and implementation of a practical data breakpoint facility. A data breakpoint facility \nmust monitor all memory updates performed by the program being debugged. We implemented and evaluated \ntwo complementary techniques for reducing the overhead of monitoring memory updates. First, we checked \nwrite instructions by inserting checking code directly into the program being debugged. The checks use \na segmented bitmap data structure that minimizes address lookup cc)m\u00adplexity. Second, we developed data \nflow algorithms that eliminate checks on some classes of write instruc\u00adtions but may increase the complexity \nof the remaining checks. We evaluated these techniques on the SPARC using the SPEC benchmarks, Checking \neach write instruc\u00ad This research was sponsored in part by the Defense Advanced Research Projects Agency \n(DARPA) under grant MDA972-92-J-1028 and contract DABT63-92-C-O026. The con\u00adtent of the paper does not \nnecessarily reflect the position or the policy of the Government and no official endorsement should be \ninferred. tEmail: {ruahbe, lUCCO, grahsm}~cs .berkeley, edu Permission to copy without fee all or part \nof this material is granted provided that the copies are not made or distributed for direct commercial \nadvantage, the ACM copyright notioe and the title of the publication and its date appear, and notice \nis given that copying is by permission of the Association for Computing Machinery. To copy otherwise, \nor to republish, requires a fee and/or specific permission. ACM-SlGPLAN-PLDl-6 /93/Albuquerque, N.M. \n@1993 ACM 0-89791 -!598-4193/0006 /0001 ...$1.50 tion using a segmented bitmap achieved an average overhead \nof 42?lo. This overhead is independent of the number of breakpoints in use. Data flow analysis elim\u00adinated \nan average of 79% of the dynamic write checks. For scientific programs such the NAS kernels, analysis \nreduced write checks by a factor often or more. On the SPARC these optimizations reduced the average \nover\u00adhead to 25%. 1 Introduction Breakpoints are user-specified rules that associate de\u00adbugging actions \nwith break conditions that arise dur\u00ading program execution. Control breakpoints specify the break condition \nin terms of the program s control flow, for example stop on call to f unct ion main. Data breakpoints \nspecify the break condition in terms of the program s memory state, for example stop when field f of \nstructure s is modified. Data breakpoints support debugging tasks such as print the value of field f \nof structure s every time it is updated. To perform this task using only control, breakpoints, the programmer \nmust find all statements in the program that might update f. In the presence of pointera or reference \nparameters, this search is both tedious and error-prone. At present, efficient data breakpoint facilities \nare not available to programmers. Using efficient runtime data structures and ideas from compiler optimization, \nwe have developed and evaluated several methods for providing practical data breakpoints. The key difference \nbetween control break conditions and data break conditions is in the complexity of the mapping from the \nbreak condition to the set of pro\u00adgram instructions that can trigger the condition. We \u00adcall an instruction \nthat may trigger a break condi\u00adtion an unsafe instruction. For control break condi\u00adtions, the mapping \nto unsafe instructions is one-to\u00adone, or one-to-few in the presence of funct ion inlining. Hence, debuggers \ncan guarantee detection of each con\u00ad trol break condition by monitoring a handful of unsafe instructions. \nIn contrast, the mapping from data break condi\u00ad tions to unsafe instructions is one-to-many, because \nthe same memory location can be updated by write instructions scattered throughout a program. A de\u00adbugger \nmust either watch a memory location corre\u00adsponding to a data breakpoint, or it must check each write \ninstruction in the program that might update that memory location. We say a write instruction is known \nif static analysis can resolve its target address. A debugger can deter\u00admine the set of known instructions \nby inspecting target addresses prior to execution. Programs written in lan\u00adguages that include pointers \nor permit runtime type violations (e.g. out-of-bounds array indices) generally contain a large number \nof unknown write instructions. If any data breakpoint is active, all unknown write instruct ions must \nbe considered unsafe, and must be checked at runtime. Implementation Strategies Some commercially available \nprocessors provide direct support for data breakpoints. Examples include the Intel i386 [10], the MIPS \nR4000 [12], and the SPARC[16]. Special-purpose hardware can monitor memory efficiently. Unfortu\u00adnately, \nthe hardware approach inherently limits the number of data words simultaneously monitored. The Intel \ni386 can monitor four words; the MIPS R4000 and the SPARC can only monitor a single word. The UNIX debuggers \ngdb [17] and dbx [14, 18] pro\u00advide data breakpoints. Both systems conservatively assume all instructions \nare unsafe. The possible side\u00adeffects of each instruction are checked through dynami\u00adcally inserted trap \ninstructions. Due to context switch and trap costs, this approach incurs very high over\u00adhead. We measured \nthe overhead of dbx to be a factor of 85,000, independent of the program being debugged. VAX DEBUG provides \ndata breakpoints using vir\u00adtual memory page protection [3]. Like gdb and dbx, VAX DEBUG assumes that \nall instructions are unsafe. However, rather than check each instruction, VAX DE-BUG protects each virtual \nmemory page containing data that is part of a data break condition. hlagpie is a programming environment \nfor Pascal that allows debugging actions to be associated with variable updates [6]. This functionality \nis implemented by inserting checks during compilation. Magpie does not support monitoring of heap objects. \nTo this date, no performance information has been reported for Magpie or for VAX DEBUG. Several au\u00adthors \nhave speculated that efficient data breakpoints require special-purpose hardware [4, 11, 15]. To quantify \nthe differences among data breakpoint implement ation strategies, Wahbe [19] compared fa\u00adcilities based \non specialized processor support, virtual memory page protection, checking the destination ad\u00address of \nmachine instructions via an operating system trap, and checking the destination address of machine instructions \nvia a procedure call. Virtual memory and the use of traps were shown to be too slow for practical use. \nSpecial-purpose hardware, while efficient, could not support all test cases due to limits on the number \nof memory words simultaneously monitored. Checking each write instruction via a procedure call emerged \nas the most promising method for realizing efficient data breakpoints. Its most significant disadvantage \nwas the expected overhead of between 209% and 642%. In this paper, we adopt a code patching approach \nto checking write instructions and significantly reduce its overhead. We investigate two complementary \nap\u00adproaches to reducing the overhead of write checks. First, we develop an efficient data structure, \nthe seg\u00admented bitmap, for checking whether an individual target address is part of a data breakpoint \ncondition. We compare several techniques for optimizing these checks. Second, we investigate the additional \nperformance benefit of eliminating write checks through compile\u00adtime analysis. Our strategy for eliminating \nwrite checks has three components. First, we use a sym\u00adbol table matching algorithm to find as many known \nwrite instructions as possible. We check those instruc\u00adtions only when the variables to which they write \noccur in break conditions. Second, we use data flow analy\u00adsis techniques to remove checks on write instructions \nwithin loops. We replace such checks by checks that execute only once, on entry to the loop. Third, we \nsupport these loop optimizations by using data struc\u00adtures that provide efficient checks on contiguous \nranges of memory locations. We demonstrate that, at the cost of considerable implementation complexity, \nthese techniques dramatically diminish the dynamic count of checked write instructions. The remainder \nof this paper has the following struc\u00adture. Section 2 describes a monitored region service abstraction \nthat defines the specific functionality pro\u00ad vided by our system and outlines its implementation. Section \n3 describes the segmented bitmap data struc\u00ad ture, its optimization through caching and inlining, and \nits overhead for the SPEC benchmarks. Section 4 provides the details of our write-check elimination algorithms, \nand evaluates their effectiveness. Finally, Section 5 draws some conclusions. St %oo, rlfp-20] ! Write \ninstruction sub xfp,20,%g5 ! Address passed via %g5 call check-1-word,O ! Write check procedure nop ! \nDelay slot Figure 1: Simplified example ofawrite check.  2 Monitored Region Service We encapsulate the \ncore functionality needed to im\u00adplement data breakpoints within an abstraction callled a monitored region \nservice (MRS). A monitored region service detects writes to contiguous regions of memory called monitored \nregions. To simplify the implementa\u00adtion, monitored regions are assumed to be word aligned and non-overlapping. \nThe interface to the monitored region service consists of the following three functions: Create!fonitoredRegion \n(MonitoredRegion) DeleteMonitoredRegion (MonitoredRegion) NotificationCallBack (TargetAddress, Size) \n It is the responsibility of the debugger to map source language names used in the break conditions tomon\u00aditored \nregions, and to create and delete monitored re\u00adgions as necessary. Ifthe target ofawrite instruction \nintersects amon\u00aditored region, there is a monitor hit, otherwise there is a monitor miss. The function \nNotificationCallBack is used tonotify MRS clients, such as the debugger, of monitor hits. The monitored \nregion service does not monitor writes to registers or memory updates due to system calls. Because registers \ncannot be aliased, detecting writes to registers is straightforward and incurs negligi\u00adbleruntime overhead. \nAdebugging system can detect memory updates due to system calls by replacing the library portion ofeach \nsystem call with an equivalent call that reports memory updates to the debugger. 2.1 Implementation We \nnow outline our basic approach to implementing a monitored region service. In the next two sections we \nwill show how this straightforward implementation can be optimized. Our system consists of a program \nanalysis tool and a runtime library. The analysis tool acts as an extra processing stage between the \ncompiler and the assem\u00adbler, patching each write instruction with a function call that checks the target \naddress to detect monitor hits. An example SPARC assembly code fragment, gen\u00aderated by our MRS implementation, \nis shown in Fig\u00adure 1. In this example, the write instruction is a one word store to memory (st ), and \nthe target address is %f p-2o. The runtime monitor library contains the data structures necessary to \ncheck whether a target address represents a monitor hit. Since our simple MRS implementation does not \nmonitor indirect jump instructions during program execution, it must be prepared for arbitrary control \ntransfers. Indirect control transfers pose two potential problems for the MRS. First, as a result of \ndata corrup\u00adtion, control might be transferred directly to a write in\u00adstruction. To insure that all monitor \nhits are detected, checks are placed after, rather than before, the write instruction. Second, the program \nmight jump into the middle of a monitor library routine. This situation is detected by maintaining a \ncheck-in-progress flag which is set on entry to a monitor routine and cleared on exit. The above scheme \ndoes not prevent the rou\u00adtine from referencing an invalid address before check\u00ading the flag. However, \nthe MRS may use an operating system signal to detect this situation gracefully. The write check implementations \ndescribed in Sec\u00adtion 3 and Section 4 reserve a minimum of three reg\u00adisters for the monitored region \nservice. One register holds the check-in-progress flag described above. The MRS uses a second register \nto hold a global disabled flag. The MRS sets this flag whenever no data breakpoints are active. The code \nfor each write check branches around the body of the check when the disabled flag is set, reducing runtime \noverhead. Fi\u00adnally, a third register is used by write checks to hold the target address of the write \ninstruction. For etliciency, the monitor library data structures are maintained in the address space \nof the program being debugged. The MRS creates monitored regions as necessary to insure the integrity \nof these data struc\u00adtures,  3 Write Check Implementa\u00adtions In this section, we describe our basic SPARC \nimple\u00admentation of a monitored region service, and present several possible optimizations for reducing \nthe over\u00adhead of checking write instructions. On the SPARC, all write instructions update either one \nor two memory words. Because the two word write instructions are suitably aligned, one-word and two-word \nchecks incur identical overhead for the implementation techniques that we tested. We will discuss only \nsingle-word write checks. The basic operation that a write check performs is to determine whether a \nwrite instruction s target ad\u00address references a monitored region. We call this oper\u00adation address lookup. \nWe found that the best strategy for implementing efficient address lookup is to mini\u00ad SegmentTable mize \nthe average number of memory accesses required. The write checks tested in Wahbe s pilot study of data \n Addres breakpoint implementations used a hash table for ad\u00ad dress lookup [19]. This data structure uses \nmemory i >> efficiently, consuming space proportional to the num\u00ad 1 ber of monitored regions. However, \nit requires sev\u00aderal memory accesses for each address lookup. We tested this data structure using the \nSPEC benchmarks and verified that the write check overhead generally matched the 209% to 642% reported \nin the previous study for a different set of benchmarks. The overhead of hash table address Iookups is \ndue mostly to the memory accesses performed in matching a target address against a list of hash table \nentries. The write checks described in this section use a seg\u00admented bitmap data structure to implement \naddress lookup. This data structure uses one bit to represent each word allocated by the program being \ndebugged. Each bit indicates whether or not the corresponding word is monitored. Hence a segmented bitmap \ncon\u00adsumes more space than a hash table -roughly 3% of the total memory used by the program. However, \nit in\u00adcurs at most two memory accesses per address lookup. Further, these memory accesses are more likely \nto be cached than the hash table memory accesses. Since one bitmap word represents 32 words of memory \nallo\u00adcated by the program being debugged, and since cache lines on the SPARC contain 32 bytes, any lookup \nof an address within 512 bytes of a recently checked address is very likely to require only cached memory \naccesses. Conceptually, the bitmap contains one bit for ev\u00adery word of addressable memory. To reduce \nits space overhead, we organize the bitmap into segments of size SEGMENT-SIZE. Segments are allocated \nlazily in re\u00adsponse to monitored region inst all at ions. Right shift\u00ading the target address by logz \n(SEGMENT-SIZE) bits yields its segment number. Segments are accessed via a segment table, which is an \narray of segment pointers, indexed by segment number. All segment pointers are initialized to a single \nzeroed bitmap segment. Figure 2 depicts a segmented bitmap. Breaking the bitmap into segments requires \nan extra load instruction to index into the segment table. How\u00ad ever, the target of this load instruction \nis very likely to be cached, again assuming spatial locality among checked addresses. 3.1 Reserving \nRegisters for the Moni\u00adtored Region Service On modern RISC architectures the naive compilation typically \nused during debugging requires only a sub\u00adset of the available registers. We can take advantage of this \nsituation by reserving registers for use by the MRS. Optimization based on reserved registers are I 7 \nSegment SegmentNmnber+ e entp h er   -UE1 I I SingleWordin Segment Figure 2: A Segmented Bitmap less \napplicable to architectures, such as the i386, which have small register sets [10]. Also, as compilation \nfor debugging incorporates more sophisticated register al\u00adlocation, there will be a tradeoff between \nfreeing an additional register for the compiler, and reserving that register for the MRS. On the SPARC \nwe found two techniques that can benefit from reserved registers. The segmented bitmap code requires \nthree regis\u00adters to hold intermediate values during address lookup. The MRS can use reserved registers \nto avoid pushing a register window. The MRS can use a fourth register to hold the base address of the \nsegmented bitmap table. Although this value is constant, it is too large to be an immediate assembly \noperand and therefore placing it in a register requires extra register instructions during address lookup. \nA more sophisticated technique, called segment caching, uses registers to cache the results of previous \nwrite checks on the same bitmap segment. To sup\u00adport caching, each write instruction is assigned a write \niype. The goal of the write type is to identify groups of write instructions which are likely to exhibit \nspa\u00adtial locality. For C programs, we used the write types BSS, STACK, and HEAP. All target addresses \ncomputed using the frame or stack pointer were assigned type STACK. Writes with constant target addresses \nwere as\u00adsigned type BSS; the remaining writes were assigned type HEAP. For FORTRAN, in addition to the \nabove types, we also used the type BSS-VAR. The Sun FOR-TRAN compiler used a simple idiom to calculate \nrelated B SS accesses. By recognizing this idiom we were able to increase the effective cache hit rate. \nFor each write type we maintain a segment cache. The segment cache holds the segment number of the last \nchecked segment to have no monitored regions. By keeping the segment cache in a reserved register, we \ncan check the cache in four register instructions. To support segment caching we must be able to de\u00adtermine \nefficiently whether a bitmap segment contains any monitored regions. We do this by maintaining, for \neach bitmap segment, a boolean flag unrnonit ol:ed, indicating whether the segment has any monitored \nre\u00adgions. By suitably aligning the bitmap segments, we can store the unmonitored flag in the unused low \norder bit of the corresponding segment pointer. In addition to supporting segment caching, the un\u00admonitored \nflag saves the address calculation and load of the correct bitmap segment when the segment con\u00adtains \nno monitored regions. To support efficient cre\u00adation and deletion of monitored regions in the pres\u00adence \nof the unmonitored flag, an auxiliary data struc\u00adture maintains a count of monitored regions for each \nbitmap segment. The algorithm for maintaining the segment caches is as follows: if Segmen tNum (TargetA \nddress) = Segmen t Cache Done elsif Segmen ti VotMonitored(TargetAddress)) then Segment Cache + SegmentNum(TargetAddress) \nelsif MonitorHit (TargetAddress) NotificationCallBack(TargetAddress, Size) endif Note that the segment \ncache is only updated if t here is a cache miss and the new segment contains no mon\u00aditored regions. While \nlarger segments improve segment cache lcjcal\u00adity, smaller segments reduce the number of full lookups. \nFull lookups are monitor misses that require checking the cache, the unmonitored flag, and finally the \nappro\u00adpriate bitmap segment word. A full lookup occurs for target addresses whose bitmap segment contains \nmoni\u00adtored regions; the impact of full lookups is discussed in Section 3.3. Segment size also determines \nthe number of segments in the segment table. For a 232 byte ad\u00address space, a 128 word segment size requires \n1 million segments. While the segments themselves are allocated lazily, the segment table is not. Thus, \nto decide seg\u00adment size, one must consider tradeoffs among segment cache locality, the expected number \nof full lookups, and the size of the segment table. To limit table size, we restricted our choice of \nseg\u00adment sizes to 128 words or greater. Figure 3 graphs segment cache locality as a function of segment \nsize. Segment sizes greater than 128 words did not offer enough gain in cache locality to justify the \npossible increase in full lookups. Hence, all experiments re\u00adported in Section 3.3 were performed with \na 128 word segment size. 90.00% -/--\u00ad / :.-\u00ad,/ . \u00ad 80.00% ./ .~ /. l /.\u00ad 0./:-- C Average . . ---F Average \n--Average40.00% {n 30.00% tI Segment Size (Bytes) Figure 3: Segment Cache Locality 3.2 Inlining In addition \nto reserving registers, we evaluated the im\u00adpact of inlining write checks. On the SPARC, inlining eliminates \nas many aa six instructions. Section 3.3 demonstrates, however, that inlining can increase over\u00adhead \ndue to instruction cache misses. To evaluate the effectiveness of inlining, we compared inlined and non\u00adinlined \nversions for both simple bitmap lookup and seg\u00adment cached implementations. For segment caching, the \nfour instructions necessary to check the cache are always inlined. The non-inlined version makes a pro\u00adcedure \ncall when there is a segment cache miss.  3.3 Evaluation Table 1 presents monitored region service overhead \nfor the following write check implementations: Bitmap. Address lookup executed via procedure call. Bitmaphdine. \nAn inlined version of Bitmap. BitmapInlineRegisters. An inlined version of Bitmap that makes use of reserved \nregisters to avoid spilling and the recalculation of address constants. Cache. Segment caching implementation \nusing four segment caches. On a cache miss, lookup is ex\u00adecuted via a procedure call. CacheInJine. An \ninlined version of C7ache. In addition to the above implementations, we mea\u00adsured the overhead of branching \naround checks when Programs Disabled Bitmap (C) 023. EQNTOT~ -3.2% 0.2% (C) 008.ESPRESSO 22.2% 70.4% \n(C) 001. GCC1.35 28.1% 75.4% (C) 022.LI 60.2% 128.5% (F) 015. DODUC 19.3% 58.6% (F) 042. FPPPP 33.8% \n55.4% (F) 030. MATRIx300 7.5% 39.1% (F) 020. NASKER 9.2% 44.5% (F) 013. SPICE2G6 7.1% 30.9% (F)047.TOMCATV \n13.6% 44.7970 C AVERAGE 26.8% 68.6% FORTRAN AVERAGE 15.1% 45.5 %0 OVERALL AVERAGE 19.8% 54.8% Table \n1: Monitored region service overhead the disabled flag is set. The column labeled aisex\u00adplained below. \nAll routines were carefully hand coded in SPARC assembly code. The standard libraries were not patched \nfor these experiments; using gprof [8], we measured the percentage of time spent in library rou\u00adtines \nto be an average of 2.6~o for C programs and 1.6% for FORTRAN programs, excluding the SPARC library routines \nfor integer multiplication and division which do not update memory. 3.3.1 Cache Effects Our measurements \nshow a number of interesting anomalies. The most obvious are the negative over\u00adheads for 023 .EQNTOTT. \nFor a number of programs, the savings due to reserving registers is somewhat higher than we had estimated. \nFinally, while inlin\u00ading the segment caching routine slightly improved the average overhead on C programs, \ninlining the simple bitmap lookup routine had essentially no effect. We conjecture that these anomalies \nare due to cache effects, The SPARC used in our experiments has a direct-mapped combined instruction \nand data cache with 32 byte cache lines. Inserting write checks affects cache performance in two ways. \nFirst, because write checks increase the size of the code, the cache size is effectively reduced. Second, \nadding or moving instruc\u00adtions changes the alignment of code and data relative to cache line boundaries. \nTo evaluate the impact of caching we performed an experiment in which we inserted 2, 4, 8, 16, or 32 \nnop instructions before each write instruction. In the absence of cache effects, the overhead should \nbe linearly dependent on the number of instructions in-for different write check implementations. Bitmap \nBitmap Cache Cache o Inline Inline Inline Registers -0.5% -1.770 -3.7% -4.4% 2.3% 66.2% 40.4% 29.6 %0 \n22.2% 4.9% 83.6% 63.1% 49.7?70 53.3% 6.1% 124.2% 94.8% 77.2% 62.3% 19.4% 73.3% 45.2% 21.170 37.8~o 5.6% \n68.7% 56.1% 41.2% 53.8% 3.3% 31.8% 25.3% 15.4% 13.8% 1.1% 40.0% 37.2% 17.2% 19.6% 1.6% 29.1% 25.1% 15.970 \n15.7% 4.1% 36.6% 32.5% 19.2% 27.8~o 1.3% 68.4% 49.2~o 38.2% 33.3% 8.2% 46.6$Z0 36.9% 21.7~o 28.1 ?ZO \n2.8% 55.3% 41.8% 28.3% 30.2% 5,0% serted. We make the simplifying assumption that the effect ive cache \nsize is also linearly reduced. For each program we performed a simple linear regression on the measured \noverhead for the different number of in\u00adserted nop instructions. Under the above assumptions, any deviation \nfrom the expected linear behavior must be caused by cache alignment effects. The last col\u00adumn of Table \n1 shows the standard deviation of the differences between expected and observed overhead. In two ways, \nthese cache effects alter the conclu\u00adsions we can draw from comparing different write check implementations. \nFirst, the ranking of a given ap\u00adproach could change given a different cache organiza\u00adtion. Second, we \nmust rely more heavily on average measurements over all SPEC benchmarks, as individual measurements may \ninclude anomalous overhead due to cache performance variation. 3.3.2 Inlining Inlining had little effect \non the measured overheads for our benchmark programs; overall, it slightly increased overhead. Because \ninlining dramatically changes the alignment characteristics of the program, the small dif\u00adferences observed \nfor individual programs are not sig\u00ad nificant. We conclude that inlining write checks on the SPARC is \nnot necessary. 3.3.3 Segment Caching In contrast, segment caching did reduce the effec\u00adtive overhead \nof write checks. However, because full lookups may occur when monitored regions are present, the savings \nfrom segment caching is depen\u00addent on the debugging situation. If there are too many full lookups, the \nadditional overhead incurred for checking the cache and unmonitored flag cancels the benefit of caching. \nTo address this issue, we com\u00adpared the cycle counts for BitmaplnlineRegisters and Cache. BitmapInlineRegisters \nexecutes 12 register in\u00adstructions and 2 loads. Cache executes 6 register in\u00adstructions if there is a \ncache hit, 18 register instruc\u00adtions and 1 load if there is a cache miss, and 26 reg\u00adister instructions \nand 2 loads if there is a full lookup. Assuming that loads take between 2-8 cycles, the break-even point \nfor C programs occurs when the per\u00adcentage of write instructions requiring a full lookup is 24.3-44.0%. \nFor FORTRAN programs, the break-even point is 16.4-36.7%. Segment caching reduced overhead by an average \nof 13.5%. For short lived programs, we do not think the savings justify the scheme s variability. Consider \nthat 1470 overhead represents only about 50 seconds for a program that normally runs for 6 minutes. For \ncompute intensive applications, improving the naive compilation typically used during debugging would \nhave more performance impact than reserving registers for segment caching [1, 2, 9]. In particu\u00adlar, \nregister allocation would speed program execution, while reducing the number of write checks. Because \nregister allocation targets scalar variables found on the stack and stack writes exhibited excellent \nlocality in our tests, we conjecture that register allocation would reduce the effect ive hit rate of \nsegment caching, nar\u00adrowing the performance gap between simple bitmap lookup and segment caching.  \n4 Eliminating Write Checks The implementation described in the previous section is simple but may incur \nsignificant overhead when the dynamic count of write instructions is large relative to the total instruction \ncount. The optimizations de\u00adscribed in this section concentrate on reducing this overhead by eliminating \nunnecessary y write checks. Write check elimination is based on dynamic inser\u00adtion and deletion of write \nchecks. For certain classes of write instructions, data flow analysis can determine runtime conditions \nunder which a given write instruc\u00adtion is safe. For example, if a particular write instruc\u00adtion w can \nonly write to a specific program variable x, then the MRS need only check w while x is being monitored. \nHence, the analysis tool can eliminate the check on w, and arrange for the hIRS to re-insert the check \nat runtime upon creation of a monitored region that includes x. Similarly, the analysis tool can often \ndetermine that a particular write instruction W1 within a loop will up\u00addate a contiguous range of memory \nlocations. Given this information, it can arrange for the MRS to check, on loop entry, whether the range \nof memory locations to be updated intersects any monitored regions. If this range check succeeds, the \nMRS can dynamically re\u00adinsert the eliminated write check on wl. Kessler [13] describes a method for dynamically \npatching a running program. To insert a check be\u00adfore an instruction, the instruction is replaced with \na branch to a write check patch. The write check patch, in addition to checking for a monitor hit, is \nresponsible for executing the displaced instruction. At compile\u00adtime, for each write instruction, a write \ncheck patch is constructed. By having dedicated patches we in\u00adsure that inserting checks is extremely \nefficient. On most architectures only a handful of instructions are required. Unfortunately, the MRS \nmust incur additional run\u00adtime overhead to support write check elimination. For example, both write check \noptimizations outlined above require that the MRS check all indirect jump instructions, to verify that \nthe control flow graph used in data flow analysis matches the actual control flow of the program being \ndebugged. Whether these addi\u00adtional sources of overhead cancel the benefit of elim\u00adinating write checks \ndepends on both the target pro\u00ad cessor architecture and the type of program being de\u00adbugged. 4.1 Analysis \nTo support write check elimination, we augmented our code patching and analysis tool to include a machine \nindependent optimizer. As before, the analysis tool takes as input a sequence of SPARC assembly instruc\u00adtions. \nIt then converts this sequence into an interme\u00addiate representation (IR) which is defined as a set of \n3-address codes. In addition, the analysis tool con\u00adverts symbol table entries (e.g. STAB) into IR form. \nThe optimizer takes as input the IR, and converts it to static single assignment (SSA) form [5]. It then \nper\u00adforms several IR transformations that eliminate checks on the target addresses of individual write \ninstructions.  4.2 Symbol Table Pattern Matching The first pass of the optimizer identifies known write \ninstructions through symbol tabJe pattern matching. The optimizer creates an expression DAG for each \ntarget address, matching the DAG against debugging symbol table entries. JVhen the expression for a target \naddress matches a symbol table expression, the opti\u00admizer eliminates the expression from the IR instruction \nsequence and replaces it with a pseucboperancl. For example, if the expression %fp-20 matches a symbol \ntable entry, the optimizer will replace all instances of Xfp-20 with a unique variable name v. Instructions \nthat read from Xfp-20 are converted to IR move in\u00adstructions with v as the source. Instructions that \nwrite into Xfp-20 are converted into IR move instructions with v as the target. This transformation has \ntwo benefits. First, it en\u00adables a substantial portion of the write checks to be eliminated, since there \nis no uncertainty about which variable is being written. Second, substituting pseudo\u00adoperands for target \naddress expressions such as Xfp-20 simplifies the recognition of induction variables, a nec\u00adessary step \nin the loop optimizations described below. During this first pass, the optimizer generates a table which \nis used at runtime to translate a symbol name x into a list of associated write instructions. When the \nmonitored region containing x is created, each in\u00adstruction w in this list must be patched to detect \na monitor hit when w is executed. To support this pro\u00adcedure, the monitored region service interface \nexports two additional operations: PreMonitor(symbol) PostMonitor(symbol) The Premonitory operation \nperforms this code patch\u00ading procedure; the PostMonitor operation reverses it. When a break condition \ninvolving x is set, the debug\u00adger calls Premonitory to patch the write instructions for x and then calls \nCreateMoni.toredRegion on the memory region associated with z. The debugger must create a monitored region \nfor x because x could be written through aliases as well as through instructions patched to detect a \nmonitor hit directly. To support this symbol table optimization we must check all definitions of registers \nthat appear in symbol table expression DAGs. For example, whenever Xfp is modified, we must ensure that \nit points to the correct stack frame. Hence, eliminating the uses of !!fp will be profitable only if \nthe number of uses of Xfp in write instructions is greater than the dynamic count of its definitions. \nFurther, to check whether !!fp is reset to its correct value following a function return requires a pair \nof memory accesses to save and retrieve the cor\u00adrect Xfp value. A dedicated register will not suffice, \nexcept for leaf procedures. Hence, checking a defini\u00ad tion of %f p will be as expensive as checking two \nor three write instructions that use %fp. Further, to avoid still greater overhead (an extra load instruction \nfor each Xf p check), the hIRS must reserve a register for this check. Because the %fp check reserves \na register, the remaining write checks in the optimized implementa\u00ad tion must push a register window. \nAs a whole, the optimized implementation requires four dedicated reg\u00ad isters. Elimination of known write \ninstructions also de\u00adpends on the control flow of the program. For example, if the program erroneously \njumps into the middle of a procedure, the Xf p can contain an incorrect value. To prevent such an occurrence, \nwe must also check all in\u00addirect jumps in the program being debugged, to ensure that they transfer control \nto legitimate targets. Finally, this optimization requires compiler support for the correct treatment \nof exceptions. If an exception causes stack unwinding, the MRS must be notified so that it can unwind \nits stack of correct Ifp values, 4.3 Loop Optimization For many programs, writes performed in loops \ncan dominate the dynamic write-count, even if loop writes make up only a small minority of the static \nwrite\u00ad count. Because of the importance of loop writes, the optimizer uses additional data flow analysis \nto elimi\u00adnate checks for some of the writes found in loops. The optimizer performs two loop-based optimiza\u00adtion: \nloop invariant check motion and monotonic write check elimination. First, the optimizer detects all loop \ninvariant target addresses. It eliminates the checks for these loop invariant addresses and replaces \nthem with write checks in a pre-header block that dom\u00adinates all entrances to the loop. If one of these \nchecks succeeds at runtime, the MRS will insert the elimi\u00adnated write check within the loop. Second, \nthe optimizer detects write instructions that will generate a monotonic sequence of target addresses \nduring the execution of a loop. We call such instruc\u00adtions monotonic writes. The optimizer replaces checks \non monotonic writes with range checks in the loop pre\u00adheader. We use an efficient data structure to imple\u00adment \nrange checks. For ranges of 225 bytes or less, the lookup requires at most three memory accesses. As \nwith loop invariant checks, if a range check succeeds at runtime, the MRS will dynamically restore the \nelim\u00adinated write check. To detect monotonic writes, the optimizer deter\u00admines the monotonic variables \nfor each loop. The value of each monotonic variable must increase or decrease monotonically during the \nexecution of the loop. 4.3.1 Assert Definitions To support loop optimization, the post-processor con\u00adverts \nthe SPARC condition code and conditional branch instructions into IR assert statements. An assert state\u00adment \nhas the form DESTI ,DEST2 := ASSERT_OP SRCI ,SRC2 where ASSERTDP is one of the relational operators. \nIn an assert statement, DEST1 is the same operand as SRCI, and DEST2 is the same as SRC2. The role of \nthe assert statement is to update the data flow information about DESTI and DEST2 to reflect the condition \ncode setting. The purpose of this re-definition is to deter\u00admine precisely, for each use of a variable, \nthe symbolic lower and upper bounds of the value of the variable. 4.3.2 Bound Propagation The optimizer \nuses a single bound propagation algo\u00adrithm to detect both loop invariant and monotonic writes. Bound \npropagation is performed once per loop. Loop nests are processed from inner to outer loops, so that checks \nmoved out of inner loops can becolme candidates for further optimization. The loop being processed is \ncalled the current loop. Following bound propagation, the optimizer processes each write in\u00adstruction \nin the current loop, replacing the checks on all bounded writes with checks in the current loop s pre-header. \nA bounded write is a write instruction whose target address has both an upper bound and a lower bound. \nTo detect bounded writes, the optimizer tags each SSA variable with bounds (L, U), where L represents \nthe lower bound on the variable, and U the upper bound. L can have one of five values: LC, LL1, LM, LA, \nor l-. Lc represents a lower bound derived from constants. A variable tagged with Lc is either a con\u00adstant \nor derived from an expression DAG containing only constants. LL1 designates a lower bound derived from \nloop invariants or constants. LA~ designates a lower bound derived from monotonic variables, loop invariants \nor constants. LA designates a lower bound derived from assert statements, monotonic variables, loop invariants \nor constants. Finally, 1-designates that the variable has no known lower bound. Similarly, U can have \nthe values Uc, ULI, UM, UA, or 1. The possible values for L are totally ordered accord\u00ading to the usefulness \nof the bounds these values rep\u00adresent: Lc > LLZ > LM > LA > 1. For example, a bound derived only from \nconstants or loop invariamts (LL1) is more useful than a bound derived from mcmo\u00adtonic variables, constants, \nand loop invariants (LM ). The latter type of bound requires a range check in the pre-header of the current \nloop, while the former re\u00adquires only a standard write check. The values for U are ordered analogously. \nBefore bound propagation begins, the bounds of all SSA variables are initialized. Constants and variables \nwith constant values have bounds (Lc, Uc ). Loop in\u00advariant variables have bounds (LLI, ULZ), Members \nof monotonic groups have bounds (LM, -1-)or (L, ~~) de\u00adpending on whether their direction is increasing \n(LM) or decreasing (UM). After this initialization step, bound propagation Def = statements that define \nvariables while (Def # 0) changed = false remove S from Def newJower.bound = max(LowerBound(Dest(S)), \nComputeLowerBound(Operands(S))) new.upper.bound = max(UpperBound(Dest(S)), ComputeUpperBound(Operands(S))) \nif (LowerBound(Dest(S)) # newJower-bound) changed = true LowerBound(Dest(S)) = newJower-bound if ( UpperBound(Dest(S)) \n# new.upper.bound) changed = true UpperBound(Dest(S)) = new-upper-bound if (changed) add all statements \nusing Dest(S) to Def Figure 4: Bounds propagation algorithm. proceeds using the algorithm shown in Figure \n4. Here Dest(S) denotes the destination operand for statement S. LowerBound and UpperBound select the \nappr\u00adpriate component of the bounds associated with the variables. This algorithm iterates to a fixed-point. \nIt places all statements defining SSA variables into a set Def. It then processes every statement S in \nDef, comput\u00ading bounds for Dest(S). If the bounds for Dest(S) change, then all statements using Dest(S) \nare added to Def, By using the max operator to combine the bounds on Dest(S) with the bounds computed \nfrom the source operands of S, the algorithm propagates the computed bounds only when they are more useful \nthan the current bounds for Dest(S). The ComputeLowerBound and ComputeUpper-Bound functions depend on \nthe type of S. For ex\u00adample, the ADD and SHIFT statements require only the simple conjunction rule 1 \n= min(/_srcl, /-src2) to compute the a new lower bound 1 from the two source operand lower bounds l-srcl \nand l-src2.  4.4 Generation of Checks Once bound propagation has completed, the optimizer visits each \nwrite instruction in the current loop. If the target address a of a write instruction w is a bounded \nvalue, then the optimizer can replace the check on a with a check in the loop pre-header. The particular \nop\u00adtimization performed depends on the symbolic bounds for a. Let a have bounds (1, u). Then if 1 ~ LLI \nand u > ULI, a k loop invariant and the optimizer can re\u00ad Checks Checks Runt ime Program Eliminated \nGenerated Overhead Symbol LI Range Tot al LI Range Full Sym (C) 023. EQNTOTT 71.9% 0.0% 0.6% 72.5% 0.0% \n0.0% 0.5% 4.0% (C) 008. ESPRESSO 23.1% 19.5% 15.4% 58.0% 0.9% 7.4% 27.8% 39,9% (C) 001. GCC1.35 49.0 \n%0 1.3% 1.8 ?ZO 52.1% 0.0% 0.8% 80.4% 109.2% (C) 022.LI 75.9% 0.0% 0.0% 75.9% 0.0% 0.0% 89,2% 156.4% \n(F) 015. DODUC 84.7% 0.1% 10.6% 95.4% 0.1% 4.6% 3.1% 80.8% (F) 042. FPPPP 70.4% 0.0% 10,8% 81.2% 0.0% \n0.0% 11.9% 39.5% (F) 030. MATRIx300 51.7% 0.0% 48.3% 100.0% 0.2% 0.2% 0.4% 18.8% (F)020.NAsKER 42.6% \n17.3% 34.5% 94.4% 0.1% 0.2% 13.9% 26.9% (F) 013.sPIcE2G6 77.7% 0.2% 1.0% 78.9% O.o ?zo 0.4% 11.4% 34.4% \n (F)047.TOMCATV 70.4% 0.0% 10.8% 81.2% 0.0% 0.0% 8.2% 40.6% CAVERAGE 55.0% 5.2% 4.5% 64,6% 0.2% 2.1% \n49.5% 77.4% FORTRAN AVERAGE 66.3% 2.9% 19.3% 88.5% 0.1% 0.9% 8.1% 40.2% OVERALL AVERAGE 61.7% 3.8% 13.4% \n79.0% 0.1% 1.4% 24.7% 55.1% Table2: Results of write check elimination. place the check on awith astandard \nwrite check inthe 4.5.1 Overflow pre-header ofthe current loop. Ifl=LM andu>UA For range checks, the \noptimizer must also guard oru= UMandi~ LA, then a is derived fromamono\u00adagainst overflow. Overflow occurs \nwhen the monotonic tonic variable and the optimizer can replace the check variable is incremented or \ndecremented to a value that on a with a range check in the loop pre-header. is not in the domain of the \nvariable s type. For ex-Togenerate code for themovedchecks, theoptimizer ample, a 16-byte signed integer \nmight be incremented walks the expression DAGfora, generating statements past 215 1, yielding a non-monotonic \nsequence of val\u00aduntil it reaches loop invariant or constant operands. ues. Detecting overflow requires \nthe compiler to pro-For monotonic write check elimination, the optimizer vide type information. The optimized \ncode would use walks the DAG twice, generating code for the lower this type information to verify the \ntype consistencybound and then the upper bound. If the write check of each sub-expression leading to \na loop-optimized ad\u00adfor a ever succeeds during program execution, the mon\u00address. itored region service \ndynamically restores the elimi\u00adnated write check inside the loop. 4.5.2 Reserved Registers 4.5 Implementation \nComplexities The MRS implementation that uses both symbol table and loop optimization reserves five registers. \nThe extra In performing these transformations, the optimizer register beyond what is needed to support \nsymbol table must take into account possible aliases that might af\u00adoptimization is used to hold one of \nthe two bounds fect the value of a. As the optimizer generates code, computed by the range check code. \nit maintains an alias list of all memory operands en\u00adcountered while walking the expression DAG for a. \n 4.6 Evaluation The optimizer precedes the range check generated for a with a sequence of statements \nthat create monitored We evaluated the symbol table and loop optimizations regions for each address on \nthe alias list. At all exits to in two ways. First, we provide detailed dynamic count the current loop, \nthe optimizer inserts a code sequence data for write checks eliminated as a result of opti\u00ad that deletes \nthese monitored regions. Thus, alias de\u00admization, Second, we measured the runtime overhead tection, like \nsymbol table optimization, requires veri\u00adof the monitored region service for symbol table opti\u00ad fication \nof program control flow. Further, it requires mization and for symbol table optimization combined compiler \nsupport for notification of exceptions, as the with loop optimization. MRS may need to delete monitored \nregions when an exception transfers control outside of a loop. 10  4.6.1 Dynamic Write C1~eck Counts \n5 Conclusion To compare the counts of write checks executed with and without optimization, we measured \nthe number of checks that optimization was able to eliminate while still insuring that all monitor hits \nare detected. We also measured the number of dynamic write checks executed in loop pre-headers generated \nas a result of monotonic variable and loop invariant optimization. Under the headings Checks Eliminated \nand Checks Generated, Table 2 reports these results as percent\u00adages of total write instructions executed. \nFor seven of ten programs our optimizations were able to eliminate more than 75% of the checks. 001 .GCC1.35 \nand 008. ESPRESSO have the lowest per\u00adcentage of checks eliminated. Both programs mtike extensive use \nof C s register declaration. During de\u00adbugging, the C compiler keeps register declared vari\u00ad ables in \nregisters. Because registers are not aliased, these declarations reduce both the need and the op\u00ad portunity \nfor optimization.  4.6.2 Expected Performance We now turn to the expected overhead of the moni\u00adtored \nregion service, This overhead includes all une\u00adliminated write checks and loop pre-header checks gen\u00aderated \nas a result of loop optimization. In addition, as stated earlier, the MRS must check all indirect jumps \nand definitions of the lif p. The column Sym in Table 2 shows the effect of us\u00ading symbol table optimization \non the SPEC benchmark. Comparing the overheads from Section 3 reported in Table 1, we observe that for \nsome programs, checking every write instruction incurs less overhead than the analysis based implementations. \nThis is due to the added overhead of checking ?!fp definitions and con\u00adtrol flow. The column Full in \nTable 2 reports the overhead of monitoring the SPEC benchmarks with checks elim\u00adinated through both symbol \ntable and loop optimiza\u00adtion. These measurements are optimistic in that our implementation does not check \nfor either overflow or aliases. Since all work performed for these checks is done only on loop entry \nor exit, overhead for this check will be insignificant for loops that have large iteration spaces. The \nscientific programs in the benchmark suite gain the most from loop optimization. For these programs, \nthe costs of checking control flow and symbol table lceg\u00adister definitions are subsumed by the benefit \nof elim\u00adinating most of the write checks. However, for some system codes such as 001 .Gcc 1.35, these \ncosts do,mi\u00adnat e, and checking every write instruction emerges as the better choice. Among the data \nbreakpoint implementation methods we studied on the SPARC architecture, we believe that the best method \nis to check all write instructions using a segmented bitmap, reserving registers to hold inter\u00admediate \nvalues during address lookup. This implemen\u00adt ation choice has several advantages. First, its over\u00adhead \nis independent of the number and distribution of monitored regions. Second, its average overhead on the \nSPEC benchmarks is 42Y0, which is small when com\u00adpared to the cost of using unoptimized code for debug\u00adging. \nFinally, this choice simplifies both the monitor library and the assembly language analysis tool. The \nmonitor library need not initialize and maintain data structures that support fast lookup on address \nranges. The analysis tool can simply insert checks after every write instruction; it does not need to \nperform data flow analysis on the assembly language code. As debugging systems evolve to support more \nso\u00adphisticated register allocation, the dynamic count of write instructions executed by a typical program \nshould diminish relative to the total instruction count. This development will reduce the overhead of \ncheck\u00ading every write instruction. It may also dictate that freeing more registers for the compiler, \nrather than re\u00adserving them for write checks, will minimize the over\u00adhead of providing a monitored region \nservice. This development will also decrease the importance of some write check elimination techniques. \nFor example, an optimizing compiler will eliminate many of the same write instructions whose write checks \ncan by elimi\u00adnated through symbol table pattern matching. The majority of these instructions access local \nvariables that, in optimized code, will reside in registers. On processor architectures such as the i386, \nthe dy\u00adnamic count of write instructions will be far greater relative to the total instruction count \nthan on RISC architectures such as the SPARC. Further, some appli\u00adcations of data breakpoints, such as \ndetecting access anomalies [7] in parallel programs, require the moni\u00adtoring of read instructions as \nwell as write instructions. Since the dynamic count of read instructions is typi\u00adcally two to three times \nthat of write instructions, the overhead of monitoring every read and write can be significant. The data \nflow analysis techniques outlined in Section 4 successfully address this problem by pro\u00adviding a means \nfor eliminating checks on the majority of write instructions dynamically executed by the pro\u00adgram. Straightforward \nextensions of these techniques will handle read instructions as well. On the SPARC both implementation \napproaches yielded data breakpoint services whose overhead is low enough for practical use. In addition \nto support\u00ading important debugging queries such as stop when field f of structure s is modified, a practical \ndata breakpoint service opens the door for higher level applications of data breakpoints. Data breakpoints \n[8] can be combined with control breakpoints to support fault isolation. Using this technique, programmers \ncan prevent a subset of their program s code from access\u00ad ing a given data structure. For example, a \nprogrammer [9] could detect corruption of library data structures such as those used by a memory allocator. \nOther applica\u00adtions of data breakpoints include access anomaly de\u00ad[10] tection, data structure animation, \ncheckpointing data for replayed execution, and support for runtime type [11] checking. We are currently \ninvestigating several of these applications. Acknowledgements [12] We wish to thank Oliver Sharp for \nhis valuable com\u00adments on earlier drafts of this paper. [13] References [1] A. Adl-Tabatabai and T. \nGross. Detection and Re\u00adcovery of Endangered Variables Caused by Instruction [14]Scheduling, . In Programming \nLanguage Design and Implementation, 1993. [2] A. Adl-Tabat abai and T. Gross. Evicted Variables [15]and \nthe Interaction of Global Register and Symbolic Debugging, . In Principles of Programming Lan\u00ad guages, \npages 371-383, 1993. [3] B. Beander. Vax DEBUG: an Interactive, Symbolic, Multilingual Debugger, . In \nProceedings of the ACM SIGSOFT/SIGPLA N Software Engineering Sympo\u00ad [16] sium on High-Level Debugging, \npages 173 179, August 1983. Appeared as SIGPLAN Notices 18(8). [4] T. Cargill and B. Locanthi. Cheap \nHardware Support [17] for Software Debugging and Profiling, . In Proceedings of t}ae Second International \nConference on Architec\u00adtural Support for Programming Languages and Oper\u00ad ating Systems, pages 82 83, \nOctober 1987. Appeared [18] as SIGPLAN Notices 22(10). [5] R. Cytron, J. Ferrante, B. K. Rosen, M. N. \nWeg\u00adman, and F. K. Zadeck. Efficiently Computing Static [19] Single Assignment Form and the Control Dependence \nGraph, . ACM Transactions on Programming Lan\u00adguages and Systems, 13(4):451-490, October 1991. [6] N. \nM. Delisle, D. E. Menicosy, and M. D. Schwartz. Viewing a Programming Environment As a Sin\u00adgle Tool, \n. In Proceedings of the ACM SIG- SOFT/SIGPLAN Software Engineering Symposium on Practical Software Development \nEnvironments, pages 49 56, May 1984. Appeared as SIGPLAN No\u00adtices 19(5). [7] A. Dinning and E. Schonberg. \nAn Empirical Com\u00adparison of Monitoring Algorithms for Access Anomaly Detection, . In ACM Symposium on \nPrinciples and Practice of Parallel Programming, pages 1-10, 1990. S. L. Graham, P. B. Kessler, and M. \nK. McKu\u00adsick. An Execution Profiler for Modular Programs, . Software-Practice H Experience, 13:671 685, \nAugust 1983. J. L. Hennessy. Symbolic Debugging of Optimized Code, . ACM Transactions on Programming \nLan\u00adguages and Systems, 4(3):323 344, July 1982. Intel Corporation, Santa Clara, California. Inte/ 80386 \nProgrammer s Reference Manual, 1986. M. S. Johnson. Some Requirements for Architectural Support of Software \nDebugging, . In Symposium on Architectural Support for Programming Languages and Operating Systems, pages \n140-148, April 1982. Ap\u00adpeared as SIGPLAN Notices 17(4). G. Kane and J. Heinrich. MIPS RISC ARCHITEC-TURE. \nPrentice Hall, New Jersey, 1992. P. B. Kessler. Fast Breakpoints: Design and Im\u00adplementation, . In Proceedings \nof the ACM SIG-PLAN 90 Conference on Programming Language De\u00adsign and Implementation, pages 78 84, White \nPlains, New York, June 1990. Appeared as SIGPLAN Notices 25(6). M. A. Linton. The Evolution of Dbx, . \nIn Proceedings of the 1990 Useniz Summer Conference, pages 211 220, Anaheim, CA, June 1990. J. M. Mellor-Crummey \nand T. J. LeBlanc. A Soft\u00adware Instruction Counter,n. In Proceedings of the Third International Conference \non Architectural Sup\u00adport for Programming Languages and Operating Sys\u00adtems, pages 78-86, April 1989. \nAppeared as SIGPLAN Notices 24( Special Issue). Spare International. The Spare Architecture Manual. Prentice-Hall, \nInc., Menlo Park, CA, version 8 edition, 1992. R. M. Stallman and R. H. Pesch. A Guide to the GNU Source-Level \nDebugger. Free Software Founda\u00adtion, 4.01 revision 2.77 edition, January 1992. Sun Microsystems, Inc. \nProgrammer s Language Guide, revision a edition, March 1990. Part Number: 800-3844-10. R. Wahbe. Efficient \nData Breakpoints, . In F ijth International Conference on Architectural Support for Programming Languages \nand Operating Systems, pages 200-212, October 1992. Appeared as SIGPLAN Notices 27(9).   \n\t\t\t", "proc_id": "155090", "abstract": "<p>A data breakpoint associates debugging actions with programmer-specified conditions on the memory state of an executing program. Data breakpoints provide a means for discovering program bugs that are tedious or impossible to isolate using control breakpoints alone. In practice, programmers rarely use data breakpoints, because they are either unimplemented or prohibitively slow in available debugging software. In this paper, we present the design and implementation of a practical data breakpoint facility.</p><p>A data breakpoint facility must monitor all memory updates performed by the program being debugged. We implemented and evaluated two complementary techniques for reducing the overhead of monitoring memory updates. First, we checked write instructions by inserting checking  code directly into the program being debugged. The checks use a <italic>segmented bitmap</italic> data structure that minimizes address lookup complexity. Second, we developed data flow algorithms that eliminate checks on some classes of write instructions but may increase the complexity of the remaining checks.</p><p>We evaluated these techniques on the SPARC using the SPEC benchmarks. Checking each write instruction using a segmented bitmap achieved an average overhead of 42%. This overhead is <italic>independent</italic> of the number of breakpoints in use. Data flow analysis eliminated an average of 79% of the dynamic write checks. For scientific programs such the NAS kernels, analysis reduced write checks by a factor of ten or more. On the SPARC these optimizations reduced the  average overhead to 25%.</p>", "authors": [{"name": "Robert Wahbe", "author_profile_id": "81100055503", "affiliation": "", "person_id": "PP31025078", "email_address": "", "orcid_id": ""}, {"name": "Steven Lucco", "author_profile_id": "81100431977", "affiliation": "", "person_id": "PP42051841", "email_address": "", "orcid_id": ""}, {"name": "Susan L. Graham", "author_profile_id": "81452606376", "affiliation": "", "person_id": "PP95034797", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/155090.155091", "year": "1993", "article_id": "155091", "conference": "PLDI", "title": "Practical data breakpoints: design and implementation", "url": "http://dl.acm.org/citation.cfm?id=155091"}