{"article_publication_date": "06-01-1993", "fulltext": "\n Global Optimization for Parallelism and Locality on Scalable Parallel Machines Jennifer M. Anderson \nand Monica S. Lam Computer Systems Laboratory Stanford University, CA 94305 Abstract Data locality is \ncritical to achieving high performance on large-scale parallel machines. Non-local data accesses result \nin communica\u00adtion that can greatly impact performance. Thus the mapping, or decomposition, of the computation \nand data onto the processors of a scalable parallel machine is a key issue in compiling programs for \nthese architectures. This paper describes a compiler algorithm that automatically finds computation and \ndata decompositions that optimize both par\u00adallelism and locality. This algorithm is designed for use \nwith both distributed and shared address space machines. The scope of our algorithm is dense matrix computations \nwhere the array accesses are affme functions of the loop indices. Our algorithm can handle programs with \ngeneral nestings of parallel and sequential loops. We present a mathematical framework that enables us \nto sys\u00adtematically derive the decompositions. Our algorithm can exploit parallelism in both fully parallelizable \nloops as well as loops that require explicit synchronization. The algorithm will trade off extra degrees \nof parallelism to eliminate communication. If communica\u00adtion is needed, the algorithm will try to introduce \nthe least expensive forms of communication into those parts of the program that are least frequently \nexecuted. 1 Introduction Minimizing communication by increasing the locality of data ref\u00aderences is \nan important optimization for achieving high perfor\u00admance on all large-scale parallel machines. The long \nmessage\u00adpassing overhead of multicomputer architectures, such as the Intel Touchstone[17J, makes minimizing \ncommunication essential. LO\u00adcality is also important to scalable machines that support a shared address \nspace in hardware. For example, local cache accesses on the Stanford DASH shared-memory multiprocessor \nare two orders of magnitude faster than remote accesses[261. Improving locality can greatly enhance the \nperformance of such machines. The mapping of computation onto the processors of a parallel machine is \ntermed the computation decomposition of the program. Similarly, the placement of data into the processors \nlocal memories This research was supported in part by DARPA contract NOO039-91-C-0138, m NSF Young Investigator \nAward and a fellowship from Digital Equipment Corporation s Western Research Lab. Permission to copy \nwithout fee all or part of this material is granted provided that the copies are not made or distributed \nfor direct commercial advantage, the ACM copyright notice and the title of the publication and its date \nappear, and notioe is given that copying is by permission of the Association for Computing Machinery. \nTo copy otherwise, or to republish, requires a fee and/or specific permission. ACM-SlGPLAN-PLDl-6 /93/Albuquerque, \nN,M. 01993 ACM 0-89791-598-4/93/0006/01 12...$1.50 is called the data decomposition. This paper describes \na compiler algorithm that automatically finds the computation and data de\u00adcompositions that optimize \nboth the parallelism and locality of a program. This algorithm is designed for use with both distributed \nand shared address space machines. For machines with a distributed address space, the compiler must follow \nthis phase with a pass that maps the decomposition to explicit communication code[2]. While it is not \nnecessary to manage the memory directly for machines with a shared address space, many of the techniques \nused to manage data on distributed memory machines can be used to improve cache performance. The choices \nof data and computation decomposition are inter\u00adrelated; it is important to examine the opportunities \nfor parallelism and the reuse of data to determine the decompositions. For example, if the only available \nparallelism in a computation lies in operating on different elements of an array simultaneously, then \nallocating those elements to the same processor renders the parallelism unusable. The data decomposition \ndictated by the available parallelism in one loop nest affects the decision of how to parallelize the \nnext loop nest, and how to distribute the computation to minimize communication. It maybe advantageous \nto abandon some parallelism to create larger granularity tasks if the communication cost overwhelms the \nbenefit of parallelization. A popular approach to this complex optimization problem is to solicit the \nprogrammer s help in determining the data decomposi\u00adtions. Projects using this approach include SUPERB[40], \nAL[34], ID Noveau[31], Kali[22], Vienna Fortran[7] and Fortran D[14, 33]. The current proposal for a \nHigh Performance Fortran extension to Fortran 90 also relies upon user-specified data decompositions[ \n13]. While these languages provide significant benefit to the programmer by eliminating the tedious job \nof managing the distributed memory explicitly, the programmer is still faced with a very difficult pro\u00adgramming \nproblem. The tight coupling between the mapping of data and computation means that the programmer must, \nin effect, also analyze the parallelization of the program when specifying the data decompositions. As \nthe best decomposition may change based on the architecture of the machine, the programmer must fully \nmaster the machine details. Furthermore, the data decompositions may need to be modified to make the \nprogram run efficiently on a different architecture. The goal of this research is to automatically derive \nthe data and computation decompositions for the domain of dense matrix code where the loop bounds and \narray subscripts are affine functions of the loop indices and symbolic constants. Our algorithm finds \ndecompositions for loops in which the number of iterations is much larger than the number of processors. \nThe emphasis of this paper is on finding the first-order, or shape , of the decompositions. We do not \naddress issues such as load balancing, choosing the block size for a block-cyclic decomposition, determining \nthe number of physical processors to lay out in each dimension, and fitting the computation and data \nto the exact number of physical processors. Even though these issues impact the performance of parallel \nmachines, their effect is secondary and we do not address them in this paper. We have developed a mathematical \nframework for expressing and calculating decompositions. This framework is general enough to handle a \nbroad class of array access patterns, including array sec\u00adtions, and is also used to calculate the replication \nof read-only data. As there are many possible decompositions for a program, a system\u00adatic solution must \nsuccessfully reduce this complex problem into a manageable one. Our model is based on the property that \nequivalent decompositions have the same data and computation allocated to a single processor. Once this \naspect of the decomposition has been determined, we show that an assignment to specific processors can \neasily be calculated. The cost of communication is determined by the data movement pattern. If the communication \npattern is nearest-neighbor shifts of data, then the amount of data transferred can be significantly \nreduced by blocking. This form of communication is inexpensive compared to communication patterns that \nrequire general movement of the entire data stmcture (e.g. a transpose). We further differentiate between \ncommunication that occurs within a parallel loop with explicit synchronization, or across loops due to \nmismatches inl de\u00adcompositions. We call communication within a loop nest pipehzed communication. Communication \ndue to mismatches in deconnpo\u00adsitions, and that require moving the entire data structure, is culled data \nreorganization communication. If a single data decomposition can be found for an array such that there \nis no reorganization com\u00admunication in the program, then we consider that decompositicm to be static \n(even though there maybe some minor nearest-neiglhbor communication between parallel loop nests). Section \n2 briefly presents the background on optimizing paral\u00adlelism and locality within a loop nest. We then \nintroduce the issues involved in automatically calculating decompositions, and formu\u00adlate the problem \nmathematically. Section 3 describes the conlpo\u00adnents of a decomposition and gives an overview of our \napproach. To illustrate the basic ideas behind our decomposition model,, we first discuss a simplified \nsubproblem in Section 4. We present an algorithm that finds data and computation decompositions that \nhave neither data reorganization nor pipelined communication. We then reapply the concepts to find decompositions \nwith pipelined commu\u00adnication in Section 5. Section 6 uses the algorithms in Section 4 and 5 as building \nblocks to develop an algorithm that takes intc~ ac\u00adcount both pipelined and data reorganization communication. \n!Sec\u00adtion 7 presents additional techniques for handling replication, and for minimizing the number of \nidle processors and the amount of replication. We have implemented the algorithms described in this paper \nin the SUIF compiler at Stanford. Section 8 describes some experimental results using the compiler. Section \n9 discusses re\u00adlated work, and we conclude in Section 10 with a summary of the contributions of this \npaper. 2 Problem Overview This section briefly diacuases optimizations for parallelism anti lo\u00adcality \nwithin a single loop nest, and introduces the issues involved in finding decompositions by way of a simple \nexample. After pre\u00adsenting a mathematical formulation of decompositions, we then formally state the problem. \n2.1 Background Techniques for maximizing parallelism and locality within a single loop nest have been \npresented in the literature[20, 25, 36]. A number of researchers have also looked at the specific problem \nof mapping a single loop nest onto parallel machines[15, 23, 24]. We refer to such loop-level techniques \nas local analysis. The global analysis is responsible for optimizing parallelism and locality across \nmultiple loop nests. First, our compiler normalizes the loops and performs loop distribution before \nexecuting the decomposition algorithms[l]. The compiler runs a loop fusion pass after decomposition to \nregroup compatible loop nests[5, 10]. Our compiler uses the algorithm developed by Wolf and Lam[25, 36] \nto apply unimodular transforms to find the coarsest granularity of parallelism within a loop nest. This \npass leaves the loop nests in a canonical form consisting of a nest of filly per\u00adnzutable loop nests. \nThe nests are as large as possible, starting from the outermost loops. A loop nest is fully permutable \nif any arbitrary permutation of the loops within the nest is legal. A fully permutable loop nest of depth \nj can be transformed to get j -1 degrees of par\u00adallelism. Our compiler positions the loops in each fully \npermutable nest such that any parallel loops are outermost. For example, given the following code: (1) \nforii :=Oto Ndo for h :=Oto N do Y[iI,N 22] += x[zI,i2];  (2) for zz:= 1to N do  for il :=1to Ndo \nZ[il,iz] :=z[21,a2-1] + Y[iz,al -1];  The local analysis would produce the code shown at the top of \nFigure 1. The keyword forall indicates that the iterations of the loop can be executed in parallel. For \na loop nest of depth i, let ~k be the outermost parallelizable loop (the first loop in the outermost \nfully permutable loop nest of size greater than 1). Loops il . . . ik-1 are sequential (degenerate fully \npermutable nests of size 1), thus there must be dependence between iterations of these loops. The local \nphase is responsible for finding transformations that minimize communication of loops ik . . . it with \nrespect to the outer sequential loops. Parallelizable loops are allocated such that any neighboring loops \nin the iteration space are neighbors when mapped onto the processor space. 2.2 A Simple Example Consider \nthe code shown at the top of Figure 1. In the figure, the array elements at (O, O) and (O, N) are shaded \nlight grey and dark grcy, respectively, to identify the position of the arrays. The loop iterations are \nshaded similarly. The figure assumes that arrays are stored in row-major order. A naive approach that \nconsiders each loop nest individually would distribute the outermost loop il in the first loop nest, \nto get the coarsest granularity of parallelism. Each processor then accesses rows of arrays X and Y, \nIn the second loop nest, the parallel ZI loop would be distributed, and each processor accesses columns \nof array Y and rows of array Z. Communication will occur, since a processor accesses a different section \nof array Y in each of the two loop nests. A solution that has no communication is to only parallelize \nthe 22 loop in the first loop nest. Each processor would then access columns of X, rows of Z, and columns \nof Y in both loop nests. The loop nests must also be analyzed to determine the relative positions of \nthe arrays so that no communication is necessary, A complete communication-free decomposition is shown \nin Figure l(c). The rest of the figure shows the mathematical representation of the decompositions and \nwill be discussed in later sections. 2.3 Problem Formulation (1) forall i, := Oto Ndo forall iZ := O \nto N do Y[il,N -iz] += X[i1,i2]; (2) forall il :=1 toNdo for iz :=lto Ndo z[il,i2] := z[21,i2 1] + Y[i2,21 \n 1];  Figure 1: A simple decomposition example. Squares represent array elements and circles represent \niterations. Lines connect the array elements and iterations that are allocated to the same processor, \n114 This section presents a mathematical model of the decomposition problem. We represent data and computation \ndecompositions as affine transformations. In this discussion, all loops are normalized to have a unit \nstep size, and all arrays subscripts are adjusted to start at O. A loop nest of depth 1, with loop bounds \nthat are affine fimctions of the loop indices, defines an iteration space Z, a polytope in l-dimensional \nspace. Each iteration of the loop nest corresponds to an integer point in the polytope and is identified \nby its index vector z= (21, i2, . . ..2l). An amay of dimension m defines an array space A, an m-dimensional \nrectangle.. Each element in the array is accessed by an integer vector a+ = (al, a2, . . . , am ) Similarly, \nan n-dimensional processor array defines a processor space P, an n-dimensional rectangle. We write an \naffine array index function ~: Z + A as ~(i ) = F;+ ~, where F is a linear transformation and ~ is a \nconstant vector. Definition 2.1 For each index 2 of an m-dimensional arra~ the data decomposition of \nthe array onto an n-dimensional processor array k a fimction d (t?) : A ~ P, where D is an n x m linear \ntransformation matrix and $ is a constant vector. Definition 2.2 For each iteration i of a loop nest \nof depth 1, the computation decomposition of the loop nest onto an n-dimensional processor array is a \nfunction ;(i) : T -+ P, where C is an n x 1linear transformation matrix and ~ is a constant vectoz The \nproblem can now be stated formally as follows. We want to find the computation decomposition Z(i) for \neach loop nest, and the data decomposition ~(d) for each array in each loop nest, such that parallelism \nis maximized and communication is minimized. The formal decompositions for the simple example from the \nprevious section are shown in Figure l(c).  3 Basic Concepts The problem of finding data decompositions \n~(d) = Dii + $ and computation decompositions Z(O = Ci + ~ can be broken down into three distinct components \nusing the affine model. The parti\u00adtion determines the computation and data that are allocated to the \nsame processor. Mathematically, data and computation partitions are described by the nullspace of the \nmatrices D and C from Def\u00adinitions 2.1 and 2.2. The orientation, represented by the matrices D and C, \ndescribes the mapping between the axes of the array elements and loop iterations, and the processors. \nLastly, the dis\u00adplacement gives the offset of the starting position of the data and computation, and \ncorresponds to the constant vectors ~ and ~. We illustrate the partition, orientation and displacement \nby developing a communication-free decomposition for the sample program in Figure 1. The Partition. There \nis a data dependence of (0,1) in loop nest 2 which serializes the iz loop. No communication is necessary \nwhen all the elements in each column of array Y and each row of array Z are assigned to the same processor. \nSince the elements in each column of array Y are on the same processor, iterations of il in loop nest \n1 are also assigned to the same processor and execute sequentially (even though there are no dependence \ninl loop nest 1). In turn, the columns of array X are allocated to the same processor as well. The partitions \nfor this example are shown in Figure l(a). Informally, the data and computation partitions specify which \namay elements and iterations, respectively, are assigned to the same processor, but not which processor. \nFormally, the subspace of the array space accessed by an array referenced in a loop nest is denoted by \nS and is the range clf the array index matrix F: S = range(F) (1) that there exist many different communication-free \norientations, all with the same partition. For this example, we could just have easily chosen to allocate \nthe columns of X in reverse order, and the columns of Y and the rows of Z in forward order. This alternative \norientation would result in Dx = [0 1], DY = [0 1] and Dz = [1O],with CL= [0 1]andCz = [1O]. The Displacement. \nThe displacement specifies the offsets of the array elements and iterations with respect to the processors. \nIn loop nest 2, accesses by the il loop to the columns of array Y are offset by one tlom the rows of \narray Z. In loop nest 1, accesses to arrays X and Y have no offset. Assigning columns O..N of array X \non processors O.. N, the columns of Y on processors N..O and the rows Z on processors N + 1..1 satisfies \nthis requirement. Iterations 1..N of loop il in the second loop nest are then assigned to procesaora \nN.. 1. The complete decompositions with displacements are illustrated in Figure l(c). For an array of \ndimension m, whenever rank(F) < m, then S CA. Let D be the data decomposition matrix from Def. 2.1. TWO \narray elements dl, ~z G S are allocated to the same processor if and only if DtfI = Driz, that is, D(iil \n-dz) = O,ordl -&#38; EkerD. Conversely, any two array elements such that (tfl, G26 S) A (dl \u00ad&#38; @ \nker D) may be assigned to different processors and are con\u00adsidered distributed. Lst C be the computation \ndecomposition matrix from Def. 2.2. TWO iterations 5, G G Z are executed on the same processor if and \nonly if C;l = CZ2 that is, C(fi -~z) =0, ori r -V EkerC. Any two iterations ii, ti c Z such that fi \n-G @ kerC are said to be distributed and may run on different processors. The mathematical representation \nof the partitions for the example is also shown in Figure l(a), The data partitions for X and Y indicate \nthat all array elements along the direction (1, O) (i.e. each column) are assigned to the same processor. \nSimilarly, the data partition for Z means that all elements along the direction (O, 1) are assigned to \nthe same processor. The computation partitions indicate that all iterations of the il loop in the first \nloop nest, and all iterations of the iz loop in the second loop nest, are executed on the same processor. \nThe Orientation. The partition determines which array elements and iterations are local to a single processor. \nThe orientation to\u00adgether with the displacement can now specify the processor on which the data and computation \nare allocated. In particular, the orientation gives the correspondence between the data and computation \ndimen\u00adsions and the processor dimensions. In loop nest 1, the colulmns of array Y are accessed in the \nreverse order from the columns of X. In loop nest 2, the columns of array Y are accessed in the same \norder as the rows of array Z. One solution that satisfies all these requirements is to allocate the columns \nof X in forward order, and the columns of Y and the rows of Z in reverse order. The iterations of the \nil loop in loop nest 2 must now be reversed as well. The orientation is illustrated in Figure l(b). Formally, \nthe matrix D from Def. 2.1 defines the data orientation and the matrix C from Def. 2.2 is the computation \norientation. The matrices for the example are also shown in Figure l(b). Note Formally, the displacements \n~ and ~ are the constant vectors from Definitions 2.1 and 2.2, respectively. The orientation matrix derived \nfrom the partition, plus the displacement forms the complete decomposition. Figure l(c) also shows the \ndata and computation displacements and the final decompositions for the example. As was the case with \norientations, there are also many possible dis\u00adplacements that lead to communication-fkee decompositions. \nWe can now summarize the basis of our approach. There are many different, yet equivalent, decompositions \nwith the same par\u00adtition. We reduce the complexity of finding the decomposition functions ~(d) for each \narray and Z(i) for each loop nest by first finding a partition that is guaranteed to lead to the desired \ndecompo\u00adsition. Then a simple calculation can be used to find the appropriate orientations and displacements \nthat completely specify the decom\u00adpositions. 4 Static Decompositions In this section, we present an \nalgorithm to find data and computation decompositions that have neither pipelined communication nor data \nreorganization communication. This simplified problem illustrates the basic ideas of our decomposition \nmodel. The algorithm finds a single, static decomposition for each array and each loop nest, and only \nconsiders the parallelism available in forall loops. 4.1 Relationship Between Data and Computa\u00adtion No \ncommunication will occur when the data is local to the proces\u00ad sor that references that data. This relationship \nbetween data and computation is expressed by the following theorem. Theorem 4.1 Let the computation decomposition \nfor loop nest j be .. 23 and the data decomposition for array x be d=. Let fzJ bean array index jimction \nfor array x in loop nest j. For all iterations < the elements of the array will be local to the processor \nthat references those elements if and only if D@j(~) + ~. = Cj(n + ?J (2) Communication at the displacement \nlevel is inexpensive since the amount of data transferred can be significantly reduced by block\u00ading. \nThus the priority is in finding the best partitions and orienta\u00ad tions, and we first focus on the version \nof Eqn. 2 that omits displace-The array index firnctions for X and Y in the first loop nest are ments. \nLetting iz~ (~ = F =)(i) + t=), D. F.j(q = C,(q (3) Given the array index function matrices Fzj for each \narray x in each loop nest j, a decomposition is free of reorganization commu\u00ad nication if the data decomposition \nmatrix D= and the computatiori decomposition Cj are such that Eqn. 3 is true. A trivial solution that \nguarantees no communication is to execute everything sequen\u00ad tially by setting all computation decomposition \nmatrices C = O and all data decomposition matrices D = O. Therefore ker C would span the entire iteration \nspace Z and ker D would span the entire array space d. However, maximizing parallelism means finding \ndata and computation decompositions such that the partition ker C , the nullspace of C, for all loop \nnests is as small as possible.  4.2 Partition Constraints We firat consider the base case of at most \none outer sequential loop containing a number of perfectly nested loops. We assume that the local phase \nhas analyzed and transformed the perfectly nested loops individually into canonical form. In section \n6.4 we will discuss the general case of multiple nesting levels. A collection of loops nests and arrays \nis represented as a bipartite interference graph, G = (Vl, V~, E). The loop nests form one set of vertices \nV., and the arrays form the other set of vertices Vd. There is an undirected edge e E E between an array \nand a loop nest if the array is referenced in the loop nest. Each edge contains one or more array index \nfunctions for all accesses of the array in the loop nest. Each connected component of the interference \ngraph corre\u00adsponds to a set of arrays and loop nests that have inter-related de\u00adcompositions. The algorithms \npresented later in this section operate on a single connected component at a time. To find a static decomposition, \nthe following constraints are placed on the data and computation partitions. (1) Single Loop Nest. The \nconstraints on a single loop nest char\u00adacterize the loops that are assigned to the same processor. These \nconstraints are used to initialize the computation partition for each loop nest. As this algorithm considers \nonly forall loops, the initial computation partition for a loop nest of depth 1 is the span of a set \nof l-dimensional elementary basis vectors* representing the sequential loops in the loop nest. If a loop \nat nesting level k is sequential, then ek is included in the initial computation partition. (2) Multiple \nArrays. The role of the constraints due to multiple arrays is to ensure that there exists a single data \ndecomposition matrix D for each array. These constraints are used to initialize the data partitions. \nSuch constraints are necessary if there are two distinct paths in the interference graph from an array \nz to another array y. For example, consider the following code fragment: (1) forall i, := Oto Ndo forall \niz := Oto N do X[il,iz] += Y[il,iz];  (2) forall il := Oto N do  ford i2 := Oto Ndo Y[i2,il] := X[il,iz]; \n Thekthelementaryvector,writtenek, hasa1inthekth positionandzeroinall otherpositions. FxI=FY1= ; ~ The \narray index functions for X and [1 Y in the second loop nest are FXZ = FXI and FY2 = ~ ~ , [1 respectively. \nA decomposition that is free of reorganization communication will have decomposition matrices DX and \nDY (for arrays X and Y, respectively) such that Eqn. 3 holds for some CI and CZ (for loop nests 1 and \n2, respectively). For iterations i I in loop nest 1 and &#38; in loop nest 2, we have the following: \nDxFxl(i 1) = DYFYI(i 1) = CI(~I) DxFxz(G) = =   DY17YZ(~2)CZ(7Z) Since the array index functions are \ninvertible, these equations pro\u00adduce the following equations for DY: DY = Dx FXIFY1 1 and Dy = DXFX2FY2-1. \nThUS, DX(FXIFY1-l FX2FY2-1) =6. (4) and (FXIFY1-l FX2FY2 1)G ker DX. If a[l the array access functions \nfor each array are equal then the above equation will yield DX6 = 6 and ~ E ker Dx, and no additional \nconstraints are placed on the partitions. In the xample qn 4 s X([ ~ ~1-[~~1)=5 us DX :1 ;1 =6.Simplifying \nthe equation gives a con\u00ad  ([ 1) straint on the partition of array .Y: ker DX ~ span{ ( 1, 1) }. Sim\u00adilar \nanalysis yields the same constraint on the partition of array 1 : ker DY ~ span{ (l, l)}. This partition \nmeans that elements along the diagonal are allocated to the same processor. In general, when the array \nindex functions are not invertible, we must introduce aux\u00adiliary variables and use a pseudo-inverse function. \nThe techniques we use are similar to those presented in other literature [3, 29]. This analysis is run \non all pairs of arrays involved in a cycle in the interference graph (including the degenerate case of \nmulti\u00adple access functions for one array in a loop nest). If an array is involved in multiple cycles \nand multiple constraints are found, then the constraints are summed. In general, when computing the data \nconstraints on an array used in multiple loop nests, it is possible that the loop nests access different \nsubsections of the array. If this is the case, each loop nest only contributes to the constraints for \nthe section of the array that it references. (3) Data-Computation Relation. This constraint ensures that \nthe relationship between data and computation from Eqn. 3 holds. If two iterations 11and 3 in loop nest \nj are mapped to the same processor, then the data of ~rray x they access must also be mapped to the same \nprocessor. For t = i 1 h, then from section 3, ~ c ker CJ, Using Eqn. 3: D,FZJ; = Cl ~= 6 and thus Fzl \n~ E ker D=. Formally, ker DZ ~ span{; I i = FcJ~;= ker Cj} (5) Similarly, two iterations ;l and ;Z in \nloop nest j must be mapped to the same processor if the data of ~rray x they access are mapped to the \nsame processor. Again, let t = G 72. If [E ker(DcFmj ) then ~ ~ ker CJ. Since ker(Dm F=j ) ~ ker Fzj, \nif t 6 ker F~J then the two iterations reference the same array location and must be mapped to the same \nprocessor. Let SZJ = range(FaJ ). In general, kerCj ~ span{~l (i?e kerFZj)V(Fmj~e (kerDmnSm3))} (6) The \nsequential loops in each loop nest cause elements of the array referenced in that loop nest to be allocated \nlocal to the same processor. The local array elements cause iterations of the loop nests that access \nthose elements to be executed sequentially. 4S Calculating Partitions Figure 1, the array index functions \nfor arrays X and Z are Fxl = H The index functions for array Y in the first andTo find partitions that \nmaximize parallelism and have neither FZ2=H pipelined nor data reorganization communication, we find \nthe mini\u00admum partitions that satisfy constraints (1) -(3). Constraint 1 (single  ecOnd;OOpne;sare Fyl[~ \n11 =[~~1 =andFy2 loop) is used to initialize the computation partitions and constraint respectively. \nker Cl is initialized to 0 and ker C2 is initialized to 2 (multiple arrays) is used to initialize the \ndata partitions. An it-span{ (O, l)}. The data partitions ker DX,Y,Z are initialized to 0. erative algorithm \nis used to satisfy constraint 3 (data-computation The routine UpdateArrays is called with loop nest 2. \nEqn. 5 is relationship). An overview of this algorithm is shown in Figur(e 2. applied to arrays Y and \nZ, resulting in ker DY = span{( 1, O)} and ker DZ = span{ (O, l)}. Next, routine UpdateLoops is called \nwith arrays Y and Z. Because of array Y, Eqn. 6 is applied to loop nest 1, resulting in ker Cl = span{ \n( 1, O)}. Finally, UpdateArrays isalgorithm UpdateArrays called again with loop nest 1 and ker DX = span{( \n1, O)}. (j: LOop_Nes~ lG: Interference-Graph; / lG = (V=, V~, E) /  Lemma 4.2 Thepartition algorithm \nfinds the maximumparallelism DPSet: set of vector~pace) (minimumpartitions) that satisjj constraints \n1 3, and is guaranteed to terminate. foreach x c referencedin(j) do Proof: The partition algorithm satisfies \nconstraints 1 and 2 because kern= := ker Da + span{? I ?= F., ? IC ker CJ}; the data and computation \npartitions are initialized with these con\u00adend foreach; straints. The algorithm finds the minimum partition \nthat satisfies theend algorithm, data-computation relation constraint 3 because the algorithm only ever \nincreases the partitions in order to ensure that the constraintalgorithm UpdateLoops is satisfied. To \nprove termination, we use the fact that the spaces (x: Array; ker D (data partitions) and ker C (computation \npartitions) increaseIG: Interference-Graph; / IG = (V., V~, E) / in size monotonically as the algorithm \nprogresses, In the worstCPSet: set of vector~pace) case, the partitions will span the entire space and \nthe algorithm will terminate. 0foreach j E loopsmsirrg(x) do After a data partition has been found for \neach array and a com\u00adker CJ := kerCJ + putation partition for each loop nest, the next step is to determinespan{~l \n(~c kerFsJ) V (F=J~c (ker D= n S.J))}; the number of virtual processor dimensions. The number of virtual \nend foreach; processor dimensions n is end algorithm; n = ~~A~~Y,(dim(Sm) dim(ker D=)) algorithm CalcJlelation \n(1G : Interference-Graph; / lG = (V., V~, ./3) */ Here S. = range(FZJ ) is the total array space ac\u00ad \nCP_Set: set of vectorspace; z DPSet: set of vectorspace) Vj @30ps-using(z) cessed, typically the entire \narray. This equation will yield a value while changes do of n such that all the parallelism found in \nthe partition algorithm is exploited. In the example from Figure 1, n = 1. if changed(z c V~) then Updatelmops(xJG, \nUpset); if changed(j IE VC) then Update_Arrays(jJG,DPJJet); end while;  4.4 Calculating Orientations \nend algorithm; Once the partition and number of virtual processor dimensions have algorithm Partition \nbeen found, the algorithm finds the orientations. The partitions de\u00ad(1G : Interference.Graph; /* IG = \n(~, V~, E) / termine the kernels of each of the decomposition matrices. Since the orientations in a connected \ncomponent of the interference graph / Computation and data partitions / CPSet: set of vectorspace;  \nare all relative to one another, we can choose one arbitrary decom-DPJSet: set of vectorspace) position \nmatrix and derive the rest of the decomposition matrices in the component. The algorithm starts by choosing \nan n x m data decomposition matrix D% for an array x of dimension m such P Satisfy constraints 1 foreach \nj c V= do ker Cl := singledoop-constraint(j}  that the nullspace of D. is the data partition ker Ds. \nAccording multiple_loop_constraint(lG, DPSet); to Eqn. 3, the computation decomposition matrix for a \nloop nest j CalcJlelation(lG, CPSet,DP3et~ that references the array is CJ = DaF.j. Again, for simplicity \nof end algorithm; presentation we assume that the array index functions are invertible. The data decomposition \nmatrix, DY, for another array y accessed in the same loop nest is calculated using DY = Cj FY~l = D, \nFmj FY_ . Figure 2: Algorithm for calculating partitions. The remaining decompositions in the connected \ncomponent are cal\u00adculated in a similar fashion. When an array index function only accesses a subsection \nof the array (i.e. SU3 c AU), auxiliary vari- The iterative algorithm calculates the effects of the loop \nnests ables are used temporarily in the unspecified dimensions of the data on the arrays using Eqn. 5 \nand of the arrays on the loop nests u:sing decomposition matrix, Note that when calculating the orientations, \nEqn. 6. This continues until a stable partition is found. Informally, non-integer entries in the decomposition \nmatrices can result. Be\u00ad the partition algorithm trades off extra degrees of parallelism to cause orientations \nare relative, the matrices can be multiplied by the eliminate communication. Going back to the simple \nexample in least common multiple to eliminate the fractions. Lemma 4.3 The orientation algorithm jinds \ndecomposition ma\u00adtrices for all arrays x and all loop nests j that have exactly the nrdlspace found by \nthe partition algorithm, and such that D=F=J = C,. Proofi We only outline the proof here because of space \nconsidera\u00adtions. We prove this lemma by induction. The base case is the array ~ for which we chose an \narbitrary decomposition matrix that has the specified kernel. Using partition constraints 2 and 3, we \nthen show that as each decomposition matrix is calculated, it has the correct nullspace and DZFEJ = Cl \nholds. 0 Theorem 4.4 The partition and orientation algorithms together find decomposition matrices for \nall arrays and all loop nests that maximize parallelism when there is no communication within loops and \nno reorganuation communication across loops. Proofi This theorem follows directly from Lemmas 4.2 and \n4.3. 0 4.5 Calculating Displacements As we expect communication at the displacement level to be rel\u00adatively \ninexpensive nearest-neighbor communication, we do not consider sacrificing parallelism to avoid communication \ndue to dis\u00ad placements. However, the algorithm minimizes any communication caused by conflicting displacements \nwhenever possible. The dis\u00ad placements are calculated after the partitions and orientations have already \nbeen determined. Our compiler uses a simple greedy strat\u00adegy that takes into account branch predictions \nand the offset sizes to find displacements that minimize communication along the most frequently executed \npaths. Eqn. 2 says that given the full data de\u00ad composition, Dz + ~=, for array x referenced in a loop \nnest j (with the array index function F=j (i ) + ~=j ) the computation displace\u00ad -1 ment JJ = D=kzj + \n~.. The data displacement, ~y, for another array g accessed in the same loop nest can be calculated using \n:9 =?, -Q/zw. 5 Blocked Decompositions In this section we discuss the problem of finding data and compu\u00adtation \ndecompositions that have pipelined communication, but no data reorganization communication. The previous \nsection only considered the parallelism available in forall loops. However, it may be the case that it \nis not possible to legally transform the iteration space so that there are outermost forall loops. For \nexample, consider the four point difference oper\u00adation: foril:=ltoiV-ldo foriz:=lto N ldo X[il,q := f(x[il,22], \nX[il -l,iz] + X[il + l,iz] + X[il,iz 1]+ X[21,22+ l]); Here the parallelism is only available along \na wavefiont, or di\u00ad agonal, of the original loop nest. Tlling[37, 39] (also known as blocking, unroll-and-jam \nand stnpmine-and-interchange) is a well\u00adknown transformation that allows both parallelism and locality \nto be exploited within a loop nest. The original iteration space is shown in Figure 3(a) and Fig\u00adure \n3@) demonstrates how this loop can be executed in parallel using doacrnss parallelism. The iterations \nin each shaded block are assigned to different processors. The computation proceeds il il i2(c) iz (d) \nFigure 3: (a) Original iteration space. (b)--(d) Iteration spaces showing the parallel execution of tiled \nloops. The arrows represent data dependence. along the wavefront dynamically, by using explicit synchronization \nto enforce the dependence between the blocks. When all dimensions of the iteration space are blocked, \nthere will be idle processors as only blocks along the diagonal can exe\u00adcute in parallel. We can gain \nthe advantages of tiling without idle processors by assigning entire rows (Figure 3(c)) or columns (Fig\u00adure \n3(d)) to different processors. In these two cases, each processor is assigned a strip of the iteration \nspace, and all processors can start executing in parallel. For example, the tiled code that corresponds \nto Figure 3(d) is as follows: for ii; :=lto N lby Bdo for 21 :=lto N ldo for ij :=zij tomin(N -l,ii~ \n+ B 1)do X[il ,2!]:=f(x[il,ij], X[il -l,i;] + x[i~ + 1,24] + x[il,ij 1]+ X[il,ij + 1]) When this loop \nnest is tiled, the original iz loop is split into two dimensions: the outer ii; loop and the inner i; \nloop. Allocating each shaded strip from Figure 3(d) to a different processor spreads iterations of the \niij loop across processors, while iterations of the i; loop reside on the same processor, Tiling can \nalso be used to reduce communication across loop nests, even when feral! parallelism is available in \nboth nests. Con\u00adsider the following example of an ADI (Alternating Direction Im\u00adplicit) integration: \n(1) forall i, := O to IV do for 22:= 1to N do X[21,22]:=fl(x[il,iz], X[21,22 l]); (2) for Z,:=1 to N \ndo forall iz := Oto N do X[21,22]:=fz(x[il,iz], X[21 l,iz]] In the first loop nest, the sequential loop \naccesses columns of X. In the second loop nest, the sequential loop accesses rows of X. For there to \nbe no communication, then the data partition for X must be ker Dx = span{ (O, 1), (1, O)}, and the computation \npar\u00adtition for both loops must be ker Cl,z = span{ (O, 1), (1, 0)}. This partition specifies that the \nentire array X is allocated on the same processor, and that both loops run sequentially. Only considering \nthe parallelism available in the forall loops provides only two op\u00adtions: either run the loops sequentially, \nor incur data reorganization communication between the two loops. However, if the compiler tiles both \nloops to extract wavefront parallelism, then the reorganization communication is reduced to inexpensive \npipelined communication. A tiled vecsion of the AD1 code is shown below. (1) forii~ := 1to N by B do \nfor al :=Oto N do for i; := ii: to min(N,ii~ + B 1) do X[i~,ij] := ~l(X[il,ij], X[il,i~ 11);  (2) \nfor ii: :=Oto N by B do for il :=1 toNdo  for il:= iij to min(N,iij + B -1) do X[il,i~] := ~2(X[il,ii], \nX[il l,i~]); 5.2 Calculating Blocked Decompositions We now present an algorithm to find data and computation \npartitions that may have pipelined communication. Our algorithm first tries to apply the partition algorithm \nas specified in section 4.3, considering only the parallelism available in the outermost forall loops. \nThis will try to find a solution such that every parallelizable loop has parallelism, and there is neither \nreorganization nor pipelined com\u00admunication. If such a solution cannot be found, the compiler then tries \nto exploit doacross parallelism. Recall that the local phase of our compiler transforms each loop nest \nsuch that the largest possible fully permutable loop nests are outermost. Also within each fully per-mutable \nnest, any forall loops are positioned outermost. A loop nest that is fully permutable can also be fully \ntiled[18, 38]. If the dependence vectors in the fully permutable loop nest are all distance vectors, \nthen the pipelined communication is inexpensive because only the data elements at the block boundaries \nneed to move. Otherwise, the cost of the communication within the loop must be weighed against the cost \nof reorganization communication between the loops. In both loop nests, the outer ii! is distributed across \nprocessors, and the inner ii and i; loops are executed on the same processor. Therefore, each processor \nis assigned a block of columns of the array. In the first loop nest, there are dependence across the \nblocks and there is pipelined communication within the loop nest. In the second loop nest, the data dependence \nare within the block so no communication is necessary. 5.1 Blocked Decomposition Model Our decomposition \nmodel is easily extended to incorporate the con\u00adcept of tiling. In general, tiling creates two sets of \nloops: the inner loops iterate within the block and the outer loops iterate across the blocks. The inner \nloops are allocated to the same processor, while the outer loops are distributed across the processors. \nIn this way, we achieve locality within the block, and parallelism across the blocks. Mathematically \nwe have represented the computation that is allocated to the same processor as a vector space ker C. \nFocusing now on the loops within an inner block, the iterations that are allocated to same processor \nalso form a vector space, L.. The vector space Z&#38; is called the localized vector space in [37], where \nLC is used to represent tile iterations that have cache locality. In our model the localized vector space \nLc contains all dimensions of the iteration space that are local to a processor, be they completely local \nor blocked. Thus ker C ~ Lc. Any dimension of the iteration space that is in L= -ker C is blocked. Only \nthe iterations within a finite block are allocated to the same processor, not the entire dimension. The \nblocks themselves are then distributed across the processors. Similarly, we define a vector space&#38;to \ncharacterize the array dimensions within a block that are allocated to the same processor. The relationship \nbetween the data partition ker D and space L,t is kerD ~ L&#38; In the ADI example, the blocked computation \npartitions are kerCl,z = 0 and Le?,? = span{ (O, 1), (1, O)}. Simi\u00adlarly, the blocked data partltlon \nis ker Dx = 0 and &#38;x = span{ (O, 1), (1,0)}. Our algorithm finds the computation and data partitions \nker C and ker D; these spaces correspond to those dimensions that must be entirely mapped onto the same \nprocessor. If blocking is desired, the algorithm also finds L= and Ld; the iterations in Lc -ker C, and \nthe data in &#38; ker D are distributed, but must be blocked. algorithm Partition.with_Blocks (1G : \nInterference.Graph; / lG = (V,, V., E) / / Computation and data partitions *I CP3et: set of vector-space; \nDPBet: set of vector-space; / Computation and data localized spaces / CLSet: set of vectorspace; DLJlet: \nset of vector~pace) I* Try to find solution with no communication 1 Partition(ZG,CPSet,DP2et); if no \nparallelism then / Record localized spaces / foreach ker Cl E CPSet do L=i := ker CJ; foreach ker D. \nc DPliet do L&#38; := ker D%;  I* Find blocked iterations and data *I foreach j G V. do ker CJ := singleAlockedloop-constraint(~); \nmultipleJoop-constraint(lG, DPSet); Calc_Relation(lG, CPSet,DPSet); end ifi end algorithm; Figure 4: \nAlgorithm for calculating partitions with blocks. An overview of the algorithm is shown in Figure 4. \nOnce the algorithm determines that a solution with neither reorganiza\u00adtion nor pipelined communication \n(and with at least one degree of parallelism) cannot be found, it recalculates ker C and ker D. The partition \nalgorithm in Figure 2 is reapplied the only change is in the single loop constraint used to initialize \nthe computation parti\u00adtions. Any dimensions that can be tiled are not considered in the initial computation \npartitions. Thus the initial computation partition for a loop nest of depth 1 is again the span of a \nset of l-dimensional elementary basis vectors. If a loop at nesting level k is sequential and cannot \nbe tiled, then ek is included in the initial computation partition. The multiple array constraints are \nused as before to ini\u00adtialize the data partition ker D. The iterative partition algorithm is then run \nto find the data and computation partitions. The final ker C and ker D represent the computation and \ndata that must be allocated to the same processor. We now need to find L. and Ld to determine the iterations \nand array elements that are either completely local to a processor or blocked. Note that these vector \nspaces have already been calculated. When the partition algorithm was called to find a solution with \nno pipelined communication, the resulting ker C is exactly the vector space L. for each loop nest. Similarly, \nthe resulting ker D is exactly. the vector space Ld for each array. Once the partitions have been found, \nthen an orientation and displacement are calculated as dis\u00adcussed in sections 4.4 and 4.5. When the iteration \nand data spaces are the blocked, the orientations and displacements are found for entire blocks. In the \nADI example above, both loop nests are fully permutable and can be completely tiled. When the compiler \ndiscovers that the forall parallelism cannot be exploited without communication, it tries to exploit \nthe doacross parallelism in these loops. The initial computation partitions are ker Cl)z = 0, and the \ninitial data partition is ker DX = 0. Running the iterative partition algorithm does not change the partitions. \nSince L=,,, = span{ (O, l), (1, O)} and &#38;x = span{ (O, 1), (1, O)}, the spaces are completely tiled. \nNote that the algorithm yields a solution that allows the entire iteration and data space to be tiled, \nand does not over-constrain the partitions unnecessarily. This solution can have idle processors, as \nwas shown in Figure 3@) above. The optimizations our compiler uses to reduce idle processors are described \nin Section 7.1. When we exploit doacross parallelism, it is possible that dif\u00adferent processors will \nwrite to the same array location within a loop nest. Thus, there is no single static data decomposition \nfor the array at that loop nest. In these cases, however, the amount of data that must be communicated \nis small with respect to the amount of com\u00adputation. A code generator for a shared address space machine \ndoes not need to know exactly which processor has the current value of the data. C)ur code generator \nfor distributed address space machines uses data-flow analysis on individual array accesses to find efficient \ncommunication when the data moves within a loop nest[2]. 6 Dynamic Decompositions In this section we \nsolve the problem of finding data and compu\u00adtation decompositions that maximize parallelism when both \ndata reorganization and pipeline communication are allowed. Data reor\u00adganizations occur when the decomposition \nfor an array in one loop nest differs from the decomposition of the same array in another loop nest. \nWe find a data decomposition for each array at each loop nest, and a computation decomposition for each \nloop nest. 6.1 The Communication Graph To model decompositions that change dynamically, we use a com\u00ad \nmunication graph G = (V, E). The nodes in the graph correspond to the loop nests in the program. The \nedges in the graph represent places in the program where data reorganization communication can occur. \nThe edges in the graph are calculated using information that is similar to the reachingdecompositions[ \n14, 33] used in the Fortran D compiler. In Fortran D, the reaching decompositions are defined to be the \nset of decomposition statements that may reach an array reference that uses the decomposition. In our \ncase, all loop nests may define a decomposition. Thus, the decomposition for an array in one loop nest \nreaches another loop nest if it is possible for the values of the array in the two loop nests to be the \nsame. This problem can be calculated in a manner similar to the standard reaching definitions data flow \nproblem. The edges in the communication graph are the chains formed by the reaching decompositions, and \nare not directed. For simplicity of presentation, this discussion assumes each array is both read and \nwritten in the loop nests that access the array. Associated with each edge e ~ E is the probability that \nthe decomposition in one loop nest will reach the other loop nest. Each loop node has a weight that is \na function of the number of instructions in the loop and an estimate of the number of times the loop \nexecutes. Our implementation currently uses profile information to calculate the probabilities for the \nedge weights as well as the loop execution counts for the loop node weights. 6.2 Problem Formulation \nWe can now formally state the dynamic decomposition problem. For a given communication graph G = (V, \nE) we want to find the data decomposition of each array at each loop node, and the corresponding computation \ndecomposition of the loops. The computation decomposition determines the degree of par\u00adallelism in a \nloop nest. For each loop node, we use the computation decomposition and the loop node weight to estimate \nthe benefit to the execution time as a result of the parallelism in the loop nest. Note that if there \nis tiling, the parallelism benefit of the loop takes into account the cost of pipeline communication \nwithin the loop. Data reorganization can occur if the decomposition for an array in one loop differs \nhorn the decomposition of the same array in another loop. Thus the data decompositions, together with \nthe probabilities on the edges, are used to estimate the communication time. The value of the graph is \nthe sum of the parallelism benefits of all the loop nodes minus the total communication cost. The goal \nis to label the arrays and loops with decompositions such that the overall value of the graph is maximized. \nFor example, consider the following program fragment. (1) forall i, := Oto N do forall i2 := Oto N do \nX[il,iz] := fl(x[il,iz], Y[il,iz]); Y[il,iz] := f2(x[21,22], Y[il ,i2]); if (expr) then (2) forall i, \n:= Oto N do for iz:=oto N do X[21,22] := f3(x[il,91(i2)l); eke (3) fOld] i, := Oto N do for i2:=Oto \nNdo Y[22,21] := f4(Y[g2(i2),21]); end it (4) forall i, := Oto N do forall zz := Oto N do X[il,iz] := \nf5(X[il,i2], Y[iI ,i2]); YIZI ,ZZ] := f6(X[i1,i2], y[iI ,22]); Figure 5(a) shows the communication graph, \nassuming the ex\u00adpression is true 75% of the time, and that both arrays are of size 10 x 10. The edges \nare labeled with the estimated communication time assuming that none of the decompositions match, and \nall the data must be reorganized between each loop nest. The value on the edge between nodes 1 and 4 \nis the sum of the communication estimates for arrays X and Y. From loop nest 1, the decomposition @@ \n(c) Figure 5: (a) A communication graph. (b) The components resulting from the dynamic decomposition. \n(c) The final decompositions. of X has a 2570 probability of reaching loop nest 4, and the decom\u00adposition \nof Y has a 75% probability. Figures 5(b) and (c) illustrate the final decompositions for this example, \nand are discussed in the next section. Theorem 6.1 The dynamic decomposition problem is NP-hard. Proofi \nWe prove the dynamic decomposition problem NP-hard by transforming the known NP-hard problem, Colored \nMultiway Cut[9], into a subproblem of this problem. The Colored Multi,way Cut problem is given a graph \nG = (V, E) with weighted eclges, and a partial k-coloring of the vertices, i.e., a subset V <~ V 9 9..., \n and a function f : V ~ 12k. Can f be extended to a total function such that the total weight of edges \nthat have different colored endpoints is minimized? Consider the subproblem of the dynamic decomposition \nproblem in which the program accesses only a single array X. If parallelized, each loop node has a value \nthat is greater than the sum of the weights of all the edges; otherwise it has a value of O. We can reduce \nan instance of Colored Multiway Cut into an instance of our subproblem in polynomial time. We only give \na brief overview of the reduction here. Each node in the original problem becomes a loop nest of depth \nkin our subproblem s input program, and the edge weights in G become branch probabilities. The amay X \nis k-dimensional, where each dimension represents a color in G. We write the input program such that \nfor each node v 6 V of color i, only the ith loop is parallel in the loop nest representing v. For each \nnode v c V -V all loops in the comesponding loop nest are parallel. D After finding the dynamic decompositions, \nthe edges that lhave communication correspond to the cutset of edges in the Colored Mukiway Cut problem. \nIf the edges are removed, then an array will have the same decomposition across all loop nests in a connected \ncomponent of the communication graph. 6.3 Dynamic Decomposition Algorithm Given that the dynamic decomposition \nproblem is NP-hard, our compiler algorithm uses heuristics to find dynamic decompositions. Our dynamic \nalgorithm uses a greedy approach that eliminates the largest amounts of communication first. The algorithm \njoins the loop nodes that have the greatest edge costs into the same compo\u00ad nent, thus eliminating the \npossibility of data reorganization between those two loop nodes. We only consider loop nests that have \nsome degree of parallelism when joining components. Purely sequen\u00ad tial loops are treated as being in \na component by themselves. An overview of the algorithm is shown in Figure 6. The routine ,Sin\u00ad gle_Level \ndescribes the algorithm for the base case of a single nesting level. The rest of the algorithm deals \nwith the multiple level case and is described in the next section. At each nesting level, the algorithm \noperates on a communica\u00ad tion graph. The algorithm initializes the components such that each node in \nthe communication graph G = (V, E) is its own compo\u00ad nent, and then calculates the edge weights, The \nedge weights are a worst-case approximation of the actual communication cost. The worst case occurs when \nnone of the decompositions match and all the data must be reorganized between each loop nest, The edges \nin E are examined in decreasing order of their weights. For each edge (u, v) c E, the algorithm tries \nto join the current component of u and the current component of v into a single component. An interference \ngraph is created from the loop nodes (and arrays referenced in the loops) in the new, joined com\u00adponent. \nThe partition algorithm from section 5 is called with the interference graph to find the new partitions. \nIn forming the new component, the algorithm eliminates the data reorganization cost of the edge. However, \nthe union operation may cause some (or all) of the loop nodes to execute sequentially, or it may generate \npipeline communication within loop nodes (as a result of tiling). The algorithm finds the value of the \ngraph before and after the new partitions have been calculated. If the value of the graph is greater \nafter the join, then the new component is saved. The algorithm then records the new partitions of all \nloops and arrays within the new component. Otherwise, there is communication along the edge (u, v), and \nthe m?w component is discarded. Consider the communication graph of Figure 5(a). For this example, we \nassume that the loop node weights are very large. As none of depenckmces in the code are distance vectors, \nwe assume that tiling is not practical. The edge between nodes 1 and 4 is examined first. The partition \nalgorithm determines that there is at least one degree of parallelism without data reorganization communication \nbetween the two loops, so the nodes are joined. Next the algorithm examines either the edge between nodes \n1 and 2 or the edge between 2 and 4. In this case the partition algorithm can still find parallelism \namong the nodes 1, 2 and 4. Next, the algorithm tries to add node 3 into the component. This time the \npartition algorithm finds that the only way to eliminate reorganization communication is to run all four \nloops sequentially and the algorithm decides not to add node 3. Thus, nodes 1,2 and 4 form one component \nand node 3 is in another component. Figure 5@) illustrates the resulting components and Figure 5(c) shows \nthe final decompositions within each component, 6.4 Putting It All Together So far we have only considered \nthe base case of at most one outer sequential loop containing a number of perfectly nested loops. The \nDynamicDecomposition routine shown at the bottom of Figure 6 gives an overview of the decomposition algorithm \nin the general case. Each nesting level is examined in a bottom-up order. This has the effect of pushing \ncommunication into the outermost loops as much as possible. The components are re-initialized at each \nlevel so that each loop nest is considered in the context of its sequential outer nest at the current \nlevel. The partitions found at each level are used to initialize the partitions for the next level. All \nreferences to algorithm Single-Level (CG : Communication.Graph; / CC= (V, E) / 1 Computation and data \npartitions 1 CP3et: set of vector~pace; DP3et: set of vector~pace; / Computation and data localized spaces \n/ CLJ$et: set of vector~pace; DLJlet: set of vectorspace) joined~omp, compl, comp2: Component; lG : Interference.Graph; \nval: integeq initialize components; foreach (u, u) G E do calculate T.u(u, u); val:= value(CG); foreach \n(u, v) c E in decreasing order of weight do compl := tind~omponent(u); comp2:= findzomponent(v} joined_comp:= \nunion~omponents(compl, comp2); IG := createinterference~raph(joined-comp] Partition.withJ310cks(IG, CP2et,DP&#38;t,CL3et,DL \nSet); if value(CG) > val then val:= value(CG); install joined~omp; else discard joined~omp, record \ndata reorganization between u and v; end ifi end foreach; end algorithm; algorithm DynamicDecomposition \nCC: Communication.Graph; I* Computation and data partitions *I CP3et: set of vectorspace; DPSet: set \nof vector_space; / Computation and data localized spaces / CLSet: set of vector-space; DL_Set: set of \nvector~pace; foreach nesting level i in bottom-up order do CC:= create_comm%raph(i,CPXet,DPJlet, CL~et,DLSet); \nSingle_Level(CG, CPJe~DPSet,CL&#38;t,DLSet~ end Foreach; calculate orientations; calculate displacements; \nend algorithm; Figure 6: Algorithm for calculating dynamic decompositions. loop indices that are outside \nthe current nesting level are treated as symbolic constants when finding the partition constraints. In \nthis manner, only the constraints for the current level are considered. In the bipartite interference \ngraph, there is an edge between each array node and each loop node that accesses the array. Each array \nnode in the bipartite interference graph only has a single decomposition. Thus, if the dynamic algorithm \ndiscovers that an array s decomposition changes, the node corresponding to that array in the interference \ngraph is split at all subsequent levels. The edges are adjusted so that loops using the reorganized decomposition \nnow point to the proper array node. Once the components have been formed, the algorithm finds the orientations \nand displacements. The orientations of the arrays and loops within a component are relative to one anothe~ \nfor example, no additional communication would result from transposing all the decompositions within \na component. We use this observation to reduce the amount of communication when finding the orientations \nacross components. Our compiler chooses an orientation for each component that matches as closely as \npossible to the other com\u00adponents that are connected by edges. Again, the compiler uses a greedy strategy \nbased on the edge weights to decide which compo\u00ad nents to orient first. The orientations and displacements \nwithin a component are found using the algorithms described in Section 4.4 and Section 4.5, respectively. \nThe dynamic decomposition algorithm shown in Figure 6 is the driver algorithm for finding decompositions \nin the general case. It finds data and computation decompositions that maximize paral\u00ad lelism and minimize \ndata reorganization and pipelined communi\u00ad cation. The partitioning algorithms from the previous two \nsections are used as subroutines to the dynamic algorithm. In particular, if a static decomposition exists, \nthen the dynamic algorithm will be able to successfully join all the loop nodes into a single component. \nIn general, the algorithm reports a data decomposition for each array at each loop nest, and a computation \ndecomposition for each loop nest, 7 Optimization There are several ways to improve upon the program decompositions \nfound in the previous section. This section briefly summarizes how to minimize the number of idle processors \nand how to find and minimize replication of read-only data. 7.1 Idle Processors When a loop nest accesses \nonly a subsection of an array, the number of virtual processor dimensions maybe larger than the nesting \ndepth of a loop nest. As a result, only a fraction of the processors will be busy during the execution \nof the loop nest. To avoid idle proces\u00ad sors, we use the computation decomposition to find those processor \ndimensions that have parallelism for all loops. The equation for the number of virtual processor dimensions \nn is modified so that n is limited to the minimum distributed iteration space: n = min(=~~a#Y,(dim( S=) \n dim(ker Dz)), ~r&#38;Ip(n dim(ker C3))) Here .SZ = range(~$~ ) is the array space accessed. x Vj Gloops.usmg(z) \nWe then reduce the number of virtual processor dimensions by projecting the n-dimensional virtual processor \nspace onto an n \u00ad dimensional processor space. In choosing the dimensions in the virtual proc:ssor space \nto pr:ject onto, n vectors are selected (i, z,..., tn~)such that Vi, ti~ k~ CT for all computation ti\u00adcomposition \nmatrices C. This means that there are no projections onto a processor dimension that is idle during the \nexecution of any loop nest. 7.2 Replication Replication of readady data is a common technique used to \nim\u00adprove the performance of parallel machines. Our algorithms findl the amount of read-only data replication \nneeded to maintain the degree of parallelism inherent in the read-write data without introducing additional \ncommunication, We consider two types of replication constant replication and dimenswn replication. Constant \nreplication occurs when there are multiple data decompositions for an array. Dmension replication means \nthat ail processors along a given dimension have a copy of the same data. The increase in the space requirement.s \nto accommodate constant replication is a linear function of the array size, whereas dimension replication \ncan cause the space needed to grow by the number of processors. Thus we focus on dimension replication. \nTo allow the necessary replication, we tirat run the deconlpo\u00adsition algorithm without takirtg into account \nthe read data in the program. From the resulting computation partition, we can then find the data partitions \nof the read-only arrays using Eqn. 5. We must now find the processor dimensions which contain replicated \ndata. Dimension replication is modeled using a reduced processor space, which is then expanded into the \nfull n-dimensional pro\u00adcessor space. All processors in the expanded dimensions have copies of the data \nthat are on the corresponding processor in the reduced space. Let x be a replicated mdirnensional may, \nwith data partition ker Dc. The dmensionality of the reduced processor space, nr, is n~ = dn(Sz ) -dm(ker \nD.), where s= = range(~.~ ) is the array space accessed. The x vj@qWJsillg(.) &#38;gree of replication \nfor an array (the number of processor dimen\u00adsions along which the data is copied) is n -n,. The data \ndecomp\u00ad osition matrix for array x is an n, x m decomposition mati D=, which maps array elements onto \nthe reduced processor space. Let Cj be the computation decomposition matrix for a loop nest j that accesses \namay z. Cj maps iterations onto the full processor space. To relate the full processor space to the reduced \nprocessor space, we use an n, x n matrix R=j. Eqn 3 from section 4.1 is modified to express the relationship \nbetween computation and data with replication D.x Fzj(ij = R=JCJ (~ (7) The matrix R$j maps Cj onto \nthe reduced space, and the nulkqpace of R.J, ker R.,, corresponds to dimensions along which there is \nreplication. The algorithm uses the data partition ker D= to find ker ,%,. In section 4.4, we used Eqn. \n3 to find tie data and computation decomposition matrices. Similarly, we use Eqn. 7 to @-id the de\u00adcomposition \nmatrices with replication. The computation and data decompositions are initially derived without any \nconsideration for the amount of replication needed. As a resul~ the amount of replication called for \ncould be much greater than is practical on the target machine. Thus, we alsoI use the techniques in section \n7.1 to liit the degree of replication by projecting the virtual processor space onto a smaller processor \nspace. 8 Experimental Results We have implemented rhe algorithms described in this paper in the SUIF \ncompiler at Stanford. The experiments described in this section were performed on the Stanford DASH shared-memory \nmultiprocessor[26]. Since we do not have a code generator for DASH at this poinL we implemented by hand \nparallel SPMD pro\u00adgrams with the decompositions generated by our compiler. All programs were compiled \nwith the SGIf17 compiler at the -02 op\u00adtimization level. The DASH multiprocessor is made up of a number \nof physi\u00adcally distributed clusters. Each cluster is based on Silicon Graphics PGWER Station 4D/340, \nconsisting of 4 MIPS R3OOWR3O1O pro\u00adcessors. A dwectory-based protocol is used to maintain cache coherence \nacross clusters. It takes a processor 1 cycle to retrieve data from its cache, 29 cycles from its local \nmemory and 100-130 cycles horn a remote memory. The DASH operating system allo\u00ad cates memory to clusters \nat the page leve~ if a page is not assigned to a speciiic cluster then it is allocated to the fist cluster \nthat touches the page. We compare the decomposition our algorithm finds with the decomposition the SGI \nPower Fortran Accelerator (version 4.0.5) parallelizing compiler used. We also compare our results with \nother possible decompositions. We ran our programs on an 8-cluster DASH multiprocessor, with 28MB of \nmain memory per cluster. We looked at the heat conduction phase of the application SIMPLE, a twodnenaional \nLagrangianhydrodynarnics code from Lawrence Livermore National Lab. The heat conduction routine conduct \nis 165 lines long and has about 20 loop nests. Within this routine is a set of loops that performs m \nADI integration where the parallelism is first across the rows of the arrays and then across the columns \nof the arrays. In all cases, we used a blocked distribution scheme. Figure 7 shows the speedups (over \nthe best sequential version) of four different decompositions of this routine, for a problem size of \n1K x 1K using double precision. ~ 32 0 o no optimization s o 0 static 1~ 28 0 n dynamic, no pipdining \n3( x dynamic and pipelirdng 24 20 16 12 8 4 0 048 121620242832 Number of Processom Figure 7: Speedup \nover sequential execution time for conduct. The problem size is lK x lK, double precision. The total \namount of data used by this routine is on the order of 128MB. When the amount of memory needed by a \ncluster exceeds the memory available on that cluster, the DASH operating system allocates the memory \non the next available cluster. Thus when executing on four or fewer clusters, the data used in the application \nmay actually be allocated to another cluster. The first curve labeled no optimization shows the results \nof the SGI Power Fortran compiler. We allowed the DASH operating system to allocate the pages to the \nfirst cluster that accessed the data. Since Fortran arrays are allocated column major, this resulted \nin blocks of columns being allocated to the clusters. When the only available parallelism is by row, \nthe processors perform remote reads and writes. The second curve labeled static shows the perfor\u00admance \nif a single data decomposition is used for each array. In this case blocks of rows were made contiguous \nin the shared address space. This represents the best possible static decomposition if only forall parallelism \nis exploited. The third curve labeled dynamic, nopipelining reallocates the data when the dimension of \nthe paral\u00adlelism changes. However, in this case the program incurs the cost of reorganization communication \nwhen the data is reallocated. This curve represents the best possible overall decomposition with only \nforall parallelism. The fourth curve labeled dynamic andpipelin\u00ading shows the results of allocating blocks \nof rows contiguously and using explicit synchronization between processors when the paral\u00adlelism is by \ncolumn. In this version the processors only synchronize between blocks of columns (we used a block size \nof 4). This is the decomposition our compiler finds when considering both pipeline and reorganization \ncommunication. Related Work A number of researchers have addressed problems that are related to the \ndecomposition problem. Sarkar and Gao have developed an algorithm that uses collective loop transformations \nto perform array contraction[32]. They use loop interchange and reversal transforma\u00adtions to orient the \ncomputation. Ju and Dietz use a search-based al\u00adgorithm to find data layout and loop restructuring combinations \nthat reduce cache coherence overhead on shared memory machines[19]. Hwang and Hu describe a method for \nfinding the computation map\u00adping of two systolic array stages that share a single array [16]. Their algorithm \nworks by first calculating the projection vector, which is similar to what we call the partition, of \nthe computation mapping. Many projects have examined the problem of finding array alignments (what we \ncall data orientations and displacements) for data parallel programs[8, 11, 21, 30, 35]. These approaches \nfocus on element-wise array operations, and try to eliminate the commu\u00adnication between consecutive loops. \nLi and Chen prove the problem of finding optimal orientations NP-complete[28], and have developed a heuristic \nsolution which is used to implement their functional language Crystal on message\u00adpassing machines[271. \nIn contrast to these approaches, our model supports loop nests containing both parallel and sequential \nloops and general affine array index functions. These approaches all optimize for a fixed degree of parallelism, \nwhereas we make explicit decisions about which loops are run in parallel. Several researchers have developed \ndata decomposition algo\u00adrithms based on searching through a fixed set of possible decompo\u00adsitions. Gupta \nand Banerjee have developed an algorithm for auto\u00admatically finding a static data decomposition[12]. \nTheir approach is based on an exhaustive search through various possible decomposi\u00adtions using a system \nof cost estimates. Carle et. al. have developed an interactive tool, as part of the Fortran D project, \nthat finds data decompositions within and across phases of a procedure[6]. Data can be remapped dynamically \nbetween phases. Their approach uses a static performance esfimator[4] to select the best decompo\u00adsitions \namong a fixed set of choices. In comparison, our algorithm avoids expensive searches by systematically \ncalculating the decom\u00adpositions. As a result of our mathematical model, we are able to derive decompositions \nthat take into account pipeline communica\u00adtion within loop nests and data reorganization communication \nacross loop nests. 10 Summary and Conclusions The decomposition problem is very complex, as there are \nmany inter-related issues that must be addressed. This paper addresses the full problem of automatically \ncalculating data and computation decompositions for programs in a systematic way. Our algorithms are \nbased on the mathematical model of decom\u00adpositions as affine functions. This framework is general enough \nto handle abroad class of array access patterns. Using the affine model we structure decompositions into \nthree components: partition, ori\u00adentation and displacement. Since equivalent decompositions have the \nsame partition, we solve for the partition first and can therefore evaluate many possible decomposition \ndesigns simultaneously. To maximize parallelism, our algorithm exploits forall paral\u00adlelism, as well \nas doacross parallelism using tiling, To minimize communication, the algorithm tries to find a static \ndecomposition that exploits the maximum degree of parallelism available in the program such that there \nis no reorganization nor pipeline commu\u00adnication. The algorithm will trade off extra degrees of parallelism \nto eliminate communication. If communication is needed, the algo\u00adrithm will try to reduce expensive reorganization \ncommunication to inexpensive pipelined communication by tiling. Finally, any nec\u00adessary data reorganization \ncommunication is inserted into the least frequently executed parts of the program. References [1] J. \nR. Allen and K. Kennedy. Automatic translation of Fortran programs to vector form. ACM Transactions on \nProgramming .Languages and Systems, 9(4):491 542, October 1987. [2] S. P. Amarasinghe and M. S. Lam. \nCommunication optimiza\u00adtion and code generation for distributed memory machines. In Proceedings of the \nSIGPLAN 93 Conference on Programming Language Design and Implementation, June 1993. [3] C. Ancourt and \nF. Ingoin. Scanning polyhedra with DO loops. In Proceedings of the Third ACM/SIGPLAN Symposium on Principles \nand Practice of Parallel Programming, pages 39 50, April 1991. [4] V. Balasundaram, G. Fox, K. Kennedy, \nand U. Kremer. A static performance estimator to guide data partitioning decisions. in Proceedings of \nthe Third ACMJSIGPLAN Symposium on Principles and Practice ofParallel Programming, pages 213 222, April \n1991. [5] D. Callahan. A Global Approach to Detection of Parallelism. PhD thesis, Rice University, April \n1987. Published as COMP\u00adTR-87-50. [6] A. Carle, K. Kennedy, U. Kremer, and J. Mellor-Crummey. Automatic \ndata layout for distributed-memory machines in the D programming environment. Technical Report CRPC\u00adTR93-298, \nRice University, February 1993. [7] B. Chapman, P. Mehrotra, and H, Zima. Programming in Vienna Fortran. \nScierrtifc Programming, 1(1):31 50, Fall 1992. [8] [9] [10] [11] [12] [13] [14] [15] [16] [17] [18] \n[19] [20] [21] [22] [23]  S. Chatterjee, J. R. Gilbert, R. Schreiber, and S.-H. Teng. Automatic array \nalignment in data-parallel programs. In Pro\u00adceedings, 20th AnnualACM Symposium on Principles of l>ro\u00adgramming \nLanguages, pages 16-28, January 1993. E. Dahlhaus, D. S. Johnson, C. H. Papadimitriou, P. D. Sey\u00admour, \nand M. Yannakakis. The complexity of multiway cuts. In Proceedings of the 24th ACM Symposium on the Theory \nof Computing, pages 241 251, May 1992. G. R. Gao, R. Olsen, V. Sarkar, and R. Thekkath. Collec\u00adtiveloop \nfusion for array contraction. lnProceedings of the Fifih Workrhop on Programming Languages and Compilers \nfor Parallel Computing, pages 171-181, August 1992. J. R. Gilbert and R. Schreiber. Optimal expression \nevalua\u00adtion for data parallel architectures. Journal of Parallel and Distributed Computing, 13(1):58-64, \nSeptember 1991. M. Gupta and P. Banerjee. Demonstration of automatic data partitioning techniques for \nparallelizing compilers on multi\u00adcomputers. Transactions on Parallel and Distributed Systems, 3(2):179-193, \nMarch 1992. High Performance Fortran Forum. High Performance Fortran Language Specification, November \n1992. Version 0.4. S. Hiranandani, K. Kennedy, and C.-W. Tseng. Compiling Fortran D for MIMD distributed-memory \nmachines. Commu\u00adnications of theACM, 35(8):66-80, August 1992. C. H. Huang and P. Sadayappan. Communication-free \nhyper\u00adplane positioning of nested loops. In U. Banerjee, D. Gelernter, A. Nicolau, and D. Padua, editors, \nLanguages and Compil\u00aders for Parallel Computing, pages 186-200. Springer-Verlag, Berlin, Germany, 1992. \nY.-T. Hwang and Y. H. Hu. On systolic mapping of muki\u00adstage algorithms. In Proceedings of the IEEE International \nConference on Application Specific Array Processors, pages 47-61, August 1992. Intel Corporation, Santa \nCJara, CA. iPSC/2 and iPSC/860 User s Guide, June 1990. F. Irigoin and R. Triolet. Supemode partitioning. \nIn Pro\u00adceedings of the SIGPLAN 88 Conference on Programming Language Design and Implementation, pages \n319 329, !Jan\u00aduary 1988. Y. Ju and H. Dietz. Reduction of cache coherence overhead by compiler data layout \nand loop transformation. In U. Ban\u00aderjee, D. Gelemter, A. Nlcolau, and D. Padua, editors, Lan\u00adguages \nand Compilers for Parallel Computing, pages 344\u00ad 358. Springer-Verlag, Berlin, Germany, 1992. K. Kennedy \nand K. S. McKinley. Optimizing for parallelism and data locaJity. In Proceedings of the 1992 ACM Interna\u00adtional \nConference on Supercomputing, pages 323-334, July 1992. K. Knobe, J. D. Lukas, and G. L. Steele. Data \noptimiza\u00adtion: Allocation of arrays to reduce communication on SIMD machines. Journal of Parallel and \nDistributed Compwing, 8:102-118,1990. C. Koelbel, P. Mehrotra, and J. Van Rosendale. Supporting shared \ndata structures on distributed memory architectures. In Proceedings of the SecondACM/SIGPLAN Symposium \non Principles and Practice of Parallel Programming, pages 177 186, March 1990. D. Kulkami, K. G. Kumar, \nA. Basu, and A. Paulraj. Loop partitioning for distributed memory multiprocessors as uni\u00admodular transformations. \nIn Proceedings of the 1991 ACM International Conferenceon Supercomputing, pages 206-215, June 1991. \n[24] K. G. Kumar, D. Kulkami, and A. Basu. Deriving good trans\u00adformations for mapping nested loops on \nhierarchical parallel machines in polynomial time. In Proceedings of the 1992 ACMInternational Conference \non Supercomputing, pages 82 91, July 1992. [25] M. S. Lam and M. E. Wolf. Compilation techniques to achieve \nparallelism and locality. In Proceedings of the DARPA Soft\u00adware Technology Conference, pages 150-158, \nApril 1992. [26] D. Lenoski, K. Gharachorloo, J. Laudon, A. Gupta, J. Hen\u00adnessy, M. Horowitz, and M. \nLam. The Stanford DASH Mul\u00adtiprocessor. IEEE Computer, 25(3):63 79, March 1992. [27] J. Li and M. Chen. \nGenerating explicit communication from shared-memory program references. In Supercomputing 1990, pages \n865-876. IEEE, May 1990. [28] J. Li and M. Chen. Index domain alignment: Minimizing cost of cross-referencing \nbetween distributed arrays. In Proceed\u00adings of Frontiers 90: The Third Symposium on the Frontiers of \nMassively Parallel Computation, pages 424-432. IEEE, October 1990. [29] D. E. Maydan. Accurate Analysis \nof Array References. PhD thesis, Stanford University, September 1992. Published as CSL-TR-92-547. [30] \nJ. F. Prins. A framework for efficient execution of array-based languages on SIMD computers. In Proceedings \nof Frontiers 90: The Third Symposium on the Frontiers of Massively Par\u00adallel Computation, pages 462-470. \nIEEE, October 1990. [31] A. Rogers and K. Pingali. Compiling for locality. In Pro\u00adceedings of the 1990 \nInternational Conference on Parallel Processing, pages 142 146, June 1990. [32] V. Sarkar and G. R. Gao. \nOptimization of array accesses by col\u00adlective loop transformations. In Proceedings of the 1991 ACM International \nConference on Supercomputing, pages 194-204, June 1991. [33] C.-W. Tseng. An Optimizing Fortran D Compiler \nfor MIMD Distributed-Memory Machines. PhD thesis, Rice University, January 1993. Published as Rice COMP \nTR93-199. [34] P.-S. Tseng. A Parallelizing Compiler for Distributed Memory Parallel Computers. PhD thesis, \nCarnegie Mellon University, May 1989. Published as CMU-CS-89-148. [35] S. Wholey. Automatic Data Mapping \nfor Distributed-Memory Parallel Computers. PhD thesis, Carnegie Mellon University, May 1991. Published \nas CMU-CS-91-121. [36] M. E. Wolf. Improving Locality and Parallelism in Nested Loops. PhD thesis, Stanford \nUniversity, August 1992. Pub-Iished as CSL-TR-92-538. [37] M. E. Wolf and M. S. Lam. A data locality \noptimizing al\u00adgorithm. In Proceedings of the SIGPLAN 91 Conference on Programming Language Design and \nImplementation, pages 3044, June 1991. [38] M. E. Wolf and M. S. J-am. A loop transformation theory and \nan algorithm to maximize parallelism. Transactions on Par\u00adallel and Distributed Systems, 2(4):452-470, \nOctober 1991. [39] M. J. Wolfe. Optimizing Supercompilers for Supercomputers. MIT Press, Cambridge, MA, \n1989. [40] H. P. Zima, H.-J. Bast, and M. Gemdt. SUPERB: A tool for semi-automatic MIMD / SIMD parallelization. \nParallel Computing, 6(1):1 18, January 1988.   \n\t\t\t", "proc_id": "155090", "abstract": "", "authors": [{"name": "Jennifer M. Anderson", "author_profile_id": "81452613603", "affiliation": "", "person_id": "PP14046415", "email_address": "", "orcid_id": ""}, {"name": "Monica S. Lam", "author_profile_id": "81100237956", "affiliation": "", "person_id": "PP14092336", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/155090.155101", "year": "1993", "article_id": "155101", "conference": "PLDI", "title": "Global optimizations for parallelism and locality on scalable parallel machines", "url": "http://dl.acm.org/citation.cfm?id=155101"}