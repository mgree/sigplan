{"article_publication_date": "06-01-1993", "fulltext": "\n Register Allocation with Instruction Scheduling: a New Approach Shlomit S. Pinter Dept. of Electrical \nEngineering Technion Israel Institute of Technology Haifa 32000, Israel shlomit~ee.technion. ac.il Abstract \nby the instruction scheduler. When instruction reorder\u00ading is carried out after register allocation the \nselection of registers may limit the possibilities to reorder instruc- We present a new framework in \nwhich considerations tions due to false dependence that are introduced with of both register allocation \nand instruction scheduling the reuse of registers. On the other hand, when reorder\u00adcan be applied uniformly \nand simultaneously. In this ing instructions precedes register allocation the span of framework an optimal \ncoloring of a graph, called the live values is increased implying longer register lifetimes parallel \ninterference graph, provides an optimal regis\u00adthus more registers are needed and more spills may be ter \nallocation and preserves the property that no false introduced. In addition, in some cases register alloca\u00addependence \nare introduced, thus all the options for par\u00adtion must precede instruction scheduling since the exact \nallelism are kept for the scheduler to handle. For this register assignment is needed by the scheduler \n[12]. framework we provide heuristics for trading off parallel scheduling with register spilling. The \nabove tradeoff is manifested in the following short example. Program (b) of Example 1 is a representation \nof the code fragment generated for (a) with symbolic registers. In (c) a possible allocation of values \nto phys\u00adical registers is provided where ri and rz are reused. In this case only 3 registers are needed \nbut a false de- Introduction Register allocation is an important task of every com\u00adpendence is introduced \nbetween the second and fourth piler. The problem of efficiently allocating values to reg\u00adinstructions \nforbidding the parallel execution (schedul\u00ad isters was investigated extensively for sequential proces\u00ading) \nof the two instructions. There is a way to allocate sors (for a survey see [2]). The most widely used \nsolution three registers and not generate the false dependence to the problem is by coloring the interference \ngraph [5], between the second and the fourth instructions (using which represents conflicts between live \nranges of values, the mapping si-ri, .sZ-rz, s3-r2, s+rs, .sS-rz); with a given number of colors (registers) \nand spilling to in this case these instructions can be executed simulta\u00ad memory those values that do \nnot fit. Many heuristics neously. for this global allocation problem (which in general is NP-complete) \nwere investigated and implemented dur-Example 1: ing the years. 51 := load z Lately, with the development \nof new processors which 52 := i offer instruction level parallelism, an optimal coloring x := a[i] S3 \n:= a[s2] of the interference graph does not necessarily correlate Y :=2+2 S4 := SI + SI with good machine \nutilization. To take advantage of the z :=X*5+2 S5 := S3*5+SI parallel capabilities instruction reordering \nis carried out (a) (b) rl := load z Permission to copy without fee all or part of this material is r2 \n:=i granted provided that the copies are not made or distributed for direct commercial advantage, the \nACM copyright notice and the r3 := a[r2] title of the publication and its date appear, and notice is \ngiven r2 := rl + rl that copying is by permission of the Association for Computing rl :=r3*5+rl Mechinery. \nTo copy otherwise, or to republish, requires a fee (c) and/or specific permission. ACM-SlGPLAN-PLDl-6 \n/93/Albuquerque, N.M. Q J 993 ACM Q-~979J-~98.4j93 /0006 /0248 . ..$~ .50 In view of the tradeoff between \ngood register alloca\u00ad tion, and good instruction scheduling in some compilers, like those for the MIPS \nprocessors, register allocation precedes instruction scheduling [6], where= in others, like the one for \nthe IBM RISC S/6000 [14], instruction scheduling is carried out first. Register allocators and instruction \nschedulers each use a graph model of the input program. Since the meanings of the nodes and edges in \nthose graphs are different and partially conflict (as we shall see), a sim\u00adple combination of the graphs \nis impossible. In this work we provide a simple common framework for represent\u00ading the input program \nfor both tasks. In this framework a register allocation with minimum number of registers (without spilling) \ndoes not decre=e the parallelism em\u00adbedded in the program. We also investigate the case when spilling \noccurs. The relation between code scheduling and register al\u00adlocation was lately considered in few studies. \nA method for alternating between two scheduling techniques in which one tries to schedule instructions \nso as to mini\u00admize the number of registers required whereas the c]ther schedules for pipelin,ed processors \nis presented in [1 O]. Another solution in [10] considers modifying the depen\u00addence graph whenever a \nregister must be reused (by adding the related dependence considering all possi\u00adble registers as candidates), \nand choosing one register. In [4] three strategies are studied. In the first, a limit is put on the number \nof registers to be used in every basic block; in the second, some scheduling cost esti\u00admates are used \nby the register allocator; and in the last method a pre-stage is used for estimating the limits to be \nused in the first method. Their basic algorithm nlain\u00adtains both graphs and alternates between running \nthe restricted versions of both the allocator and the sched\u00aduler. In [15] some registers are colored \nduring optimiza\u00adtion phase and the final coloring procedure is called from the scheduler. Our approach \nis different from all of the above in that it provides a single framework in which optimal utilizat\u00adion \ncan be achieved when there are enough registers and both heuristics of the scheduler and the allocator \nmay be applied when the number of registers is small. In particular, a parallel interference graph which \ntakes into account scheduling constraints is generated first. With this graph register allocation is \ncarried out; we prove that an optimal coloring of this graph provides a register allocation which does \nnot generate false depen\u00addence. In case of a need for spill, heuristics for both scheduling and register \nallocation are performed on the graph at the same time. The scheduling itself takes place after the register \nallocation: nevertheless, the rel\u00adative order of the non-constrained statements need not be the one used \nduring the register allocation process. We present some new heuristics and show how to, im\u00adplement existing \nones in this framework. Currently, our implemental ion effort is underway and we shall have some experi \nlnmtal results by the time the full paper is due. 2 Definitions In this section we define the general \nsetting for both the register allocation and the instruction scheduling prob\u00adlems. A simple model of \na superscalar machine is pro\u00advided, followed by a symbolic register level based de\u00adscription of a program. \nThese models are used in defin\u00ading our problem and algorithms. Although the results are presented for \na superscalar machine model, they are all applicable to regular single-issue, pipelined unipro\u00adcessors \naa well. Machine Model. In the description of the model we assume a RISC type processor (memory reference \nin\u00adstructions are only load and store while computations are done in registers) with finitely many registers. \nAn instruction level parallel processor is a RISC type pro\u00adcessor comprising a collection of functional \nunits that potentially can each execute one instruction in the same machine cycle. Examples of such processors \nare the MIPS R3000 and the IBM RISC S/6000 comprising three functional units: fixed point, floating point \nand branch units. The source code of the program is first translated into register based intermediate \ncode where an infinite num\u00adber of symbolic registers is assumed (one symbolic reg\u00adister per value)l. \nIn what follows we use data depen\u00addence between instructions of the intermediate code. We define the \ndata dependence between instructions as follows: Let u, and v be two instructions. A data depen\u00addence \nfrom u to v exists if one of the following holds: Data fiow dependence the register (variable) defined \nin u is used in v.  Data-an.tz-dependezlce a register used in u is later redefined in v (destroying \nthe value used in u).  Data output dependence the register defined in u is redefined in v (destroying \nthe value de\u00adfined earlier in u).  ~During this translation some obvious machine related trans\u00adformations \ndeviate form thk assumption by reusing registers (e.g. incrementing induction variables ). Instruction \nScheduling for Superscalar Ma\u00adchines. Instruction scheduling is the process of mov\u00ading instructions so \nthat they can be scheduled to the different units of the processor such that the total ex\u00adecution time \nis minimized. Instructions are moved in a way that preserves the program s semantics on one hand, and \nprovides a better utilization of the machine on the other hand. The input for the scheduler is presented \nby a schedule graph, G, = (V,, E.), constructed as follows: Every vertex v E V$ corresponds to an instruction \nin a register based description of the program.  There exists a directed edge (u, v) c 138, from u to \nv, if u must be executed before v. This happens in one of the following three cases: (i) there is a data \ndependence of v on u, (ii) there is a control dependence from u to v, (iii) there is a machine constraint \nthat enforces the precedence of u over v.  Control dependence exist only between basic blocks where \nthe corresponding edges are derived from the prc\u00adgram s flow graph (following if branches etc.). The \nmachine based constraints imposed on the program may forbid the use of the same functional unit within \na short time interval (resource contention), or the simultane\u00adous access to the same memory address, \nor similar con\u00adstraints. A legal scheduling must preserve the execution order described by the edges \nof the graph. Thus, adding prece\u00addence edges to G, may restrict the potential for parallel scheduling. \nTraditional instruction scheduling is done per basic block. The precedence constraints in such a case \nare the data dependence based constraints and the machine based constraints. Register Allocation. Register \nallocation is the pro\u00adcess of mapping symbolic registers into physical ones in a way that variables stay \nin registers as long as they are live (if at all possible). For this mapping an znierfer\u00adence graph, \nGr = (Vr , Er), representing the interactions between live variables, is constructed as follows: e Every \nvertex v c V, corresponds to a distinct pro\u00adgram interval in which a definition of a variable s value \nis live. e There exists an (undirected) edge {u, v} E E. if one definition is live (its value is used \nor subsequently used) in a statement where the other is defined (the two intervals intersect). In practice, \nin many compilers the end point of the live interval of a symbolic register (i.e. the statement correspond \ning to its last use) is not considered part of the interval; this, later on, enables the reuse of the \nreg\u00adister in the same statement that last uses it (like the increment of a register). A coloring of the \ninterference graph, in which no two nodes that are incident upon an edge have the same color, induces \na register allocation (if the number of reg\u00adisters in the machine is not smaller than the number of colors) \nin which every value is in a register as long as it is alive. We say that such a coloring of an interference \ngraph is optimal and induces optimal register allocation if it uses the smallest coloring set over all \nlegal colorings. In addition to the fact that the minimum coloring prob\u00adlem is NP-complete, in general \nthe number of registers is smaller than the number of colors. Thus, in prac\u00adtice a sptlling stage is \ncarried out in which the values of some variables (symbolic registers) are temporarily stored in memory. \nThe problem of register allocation is to find a register mapping in which the cost of spilling is minimum. \nAs discussed in the introduction register allocation may restrict the capabilities to schedule instructions \nin parallel for superscalar machines; the issue raised is whether the capabilities to schedule instructions \nin par\u00adallel can be defined together with register allocation so that one does not restrict the other. \n3 Regkter Allocation for Superscalar Machines As can be seen from the definitions in the previous sec\u00adtion, \nthe vertices in the two graphs do not have the same meaning, thus, a simple combination of the graphs \nis im\u00adpossible. In this section we provide a simple common framework for representing the two frameworks. \nWe start with an example that demonstrates some of the main ideas of our technique. Consider the code \nin Example 2 which is to be run on a processor with two arithmetic units (fixed-point and floating-point), \nwhere the si s represent symbolic registers. Example 2: S1 := load z (fixed) S2 := load y (fixed) S3 \n:=Si +S2 S4 :=Si *S2 S5 := S3+S4 S6 := load x (float) S7 : = load w (float) S8 := S7 S6 * S9 := S5+S8 \n The dependence edges of the schedule graph of Ex\u00adample2 are presented in Figure 1. S7 S6 W S8 S3S4 w \nS5 \\/ S9 Figure 1: The dependence edges of the schedule graph of Example 2. At this point when we have \nall the precedence type constraints we generate the transitive closure of the scheduling graph and remove \nthe directions of the e<iges. To this new graph we now add all the machine related dependence that are \nnot of a precedence type. For ex\u00adample, in ourmachine there is only one fixed-point unit, thus operations \nS3 and S4 cannot be executed together; this constraint will be represented by adding the edge {s3, s4}. \nIn some scheduling algorithms such an edge is not present in the graph but instead the algorithm it\u00adself \nwill take it into account by trying to schedule an op\u00aderation on the floating-point unit following an \noperation on the fixed-point unit and vice versa (see for example [16]). In our framework we will make \nall such constraints explicit at this stage. We note that the more edges are present in this graph the \nbetter the results will be; this is so since we are actually going to use only the edges that are in \nthe compl~ment of the constructed graph. The edges in the complement graph present the actual parallelism \navailable to our machine for the given pro\u00adgram. To continue our example, since we have only one fetching \nunit we will also generate all the possible edges between the four load instructions s 1, S2, s6, and \ns7. The only edges in the complement graph of the example are between S8 and each of the five statements \ns 1, S2, S3, S4, S5, and all the edges between the two sets S7, S6 and S3, s4, s5. Indeed those are the \nonly opera\u00adtions that can be executed in parallel according to the definition of our machine. The next \nstage of our tech\u00adnique is to integrate those edges with the interference graph so that the register \nallocation algorithm will take this available parallelism into account. We will continue the example \nafter the formal presentation of our frame\u00adwork. The formal presentation starts with the definition \nof false dependence whose existence limits the poten\u00adtial parallelism implemented by the scheduler. When \nscheduling is done after register allocation the depen\u00addence between the registers provide the edges \nof the scheduling graph. The register allocation may cause the generation of data dependence that were \nnot present before. Let (u, v) be a data dependence edge in the scheduling graph generated after register \nallocation; the edge (u, v) is a false dependence edge iff u and v can be scheduled together according \nto the schedule graph for the code when presented with symbolic registers. The problem of register allocation \nfor super\u00adscalar machines is to find an optimal register allo\u00adcation (with minimum number of registers \nand no spills for live values) whose scheduling graph does not have a false dependence. We next derive \na new graph (relation) comprising ex\u00adactly all the false dependence of a given program. The edges of \nthe graph will be used in the generation of the parallel interference graph whose optimal coloring pro\u00adduces \nan optimal register allocation without introducing false dependence. For a given basic block define the \nfalse dependence undirected graph, Gj = (~, l?,), as follows: Let Gc = (Vc, E C) be the scheduling graph \nof the basic block prior to register allocation, i.e. instructions are presented with symbolic registers \nand all the precedence based constraints are drawn.  Vj= t; Consider the set of edges in the transitive \nclosure of G. and define Et to be the same set of edges after the removal of the directions of the edges. \nAdd to Et all the non-precedence basqd constraints that de\u00adscribe the restrictions on the machine capabilities. \nThe set of edges Ej comprises the pairs {u, v} such that U,V E Vj, u # v, and {u, v} @Et. is clear that \nthe schedule graph, G,, of the b=ic block has no output and anti data dependence edges (with symbolic \nregisters no register is redefined); thus, the set Et contains exactly the real constraints on the scheduler. \nI.n case that the false dependence graph is constructed after partial register allocation (some regis\u00adters \nare reused), it is assumed that every anti or output dependence thus introduced is part of a single machine \ninstruction (e.g. incrementing a register) tlius no real output or anti dependence is introduced. Lemma \n1 An edge (u, v) of a scheduling graph G, is a false dependence iff {u, v} E Ef. 2 The cOn~tr~nt5 ~O&#38;]ed \natthis stage are of the form: These two instructions may not be scheduled in parallel. Positive forms \nof constrains can be introduced on the complemented graph which is generated next. l?roofi Clearly if \n(u, u) is present in the transitive clo\u00adsure of EC then u and v cannot be scheduled together. Since when \nu cannot be scheduled with v then v can\u00adnot be scheduled with u (symmetry), no pair in J?3tcan be scheduled \ntogether when considering the code pre\u00adsented with symbolic registers. Now, if {u, v} ~ Ej then {u, v} \nc Et thus neither (u, v) nor (v, u) can be a false dependence. For the other direction: an edge {u, v} \nc Ej iff there is no directed path from u to v or from v to u in Et. Thus, from the semantics of the \nscheduling graph u and v can be scheduled together and the lemma holds. i Since anti and output dependence \nare generated only when a memory location (or a register) is redefined they are often considered as non-real \ndependence. Indeed, when an unbounded number of symbolic registers is used no redefinition is needed \nand these dependence need not distract parallelism. This leads to the idea that if we keep the two definitions \ncorresponding to a false de\u00adpendence edge in different registers, then no false output dependence will \nbe introduced; we will also show that our technique does not generate false anti dependence. In Figure \n2 (a) we present only the data dependence edges of the scheduling graph, G$, corresponding to Ex\u00adample \n1(b). The instructions (vertices) are marked with the defined symbolic registers. The graph in Figure \n2 (b) presents the edges in the set E,. Lastly, Figure 2 (c) shows the interference graph, G,, of the \nsame example. S1 n S2  6$ 6$ (c) Figure 2: A scheduling graph (a), the edges in the set E* (b), and \nthe interference graph (c) of Example l(b). Note that the edges {sI ,s3} and {s4, s5} in Fig\u00adure 2 (b) \nare machine dependent constraints, and the only false dependence edges are {s1 , s2}, {s2, s4} and {s3, \ns4}; indeed, the operations in every pair (false edge) can be scheduled on the same cycle (according \nto our example). With the false dependence graph we next define the parallelizable interference graph. \nIt basically includes both edges of the false dependence graph and those of the interference graph; thus, \nits optimal coioring assures (as we later prove) that no spills and no false depen\u00addence are introduced. \nThe following observation and claim are used in the construction: In the interference graph a definition \n(assignment of a value) marks the beginning of an interval and also potentially defines a new symbolic \nregister, thus a vertex may correspond to the defining instruction and to the symbolic register. Claim \n1 Let G, and G, be the scheduhng and zn,terfer\u00ad ence graphs respectively of a baste block. Then V, ~ \nV,. Proofi Every definition corresponds to a single in\u00adstruction. Note that only branch instructions \nare not directly represented in the interference graph. This can be re\u00adlaxed somewhat since the predicate \nof the branch con\u00addition is computed separately and thus is present in the interference graph. Other \nnon-register instructions are changed to be register oriented; a call instruction is changed to be a \nmultiple register assignment, a load defines a new value and a store instruction indicates the end of \na live interval (it may also be considered a spill). Also, the right number of names analysis, see [5], \nis used to discover the live ranges and unify those that share the same use. Within a basic block this \nmay have an effect only in some redefinitions in the body of a loop; it is simple to see that Claim 1 \nis preserved in this case. The case of the global interference graph is discussed at the end of this \nsection. Note that in general there may be edges in E. that are not in E7 and vice versa. For example, \nan interfer\u00adence edge in Er which is present due to the sequential ordering of the input code may not \nbe present in E$; for the other direction: machine based constraints may not be present in E., or a data \nflow dependence edge (u, v) E E, may not be present in E. since it corre\u00adsponds to the last use of u \n(which is not considered in the live interval of the symbolic register). At this point we are ready to \ndefine the parallelizable interference graph which is used for solving the regis\u00adter allocation problem \nwithout generating false depen\u00addence. In such a process we need all the edges in E. even though some \nof them only present interferences due to the sequentiality of the input code (but not real de\u00adpendence \nor machine constraints). Thus, our solution is relative to the order of instructions in the input code. \nIn some compilers pre-scheduling is used to somewhat improve the input ordering. In our final algorithm \nwe introduce a step that improves the initial ordering of statements based on scheduling heuristics. \nDefine the parallelizable interference graph, G = (V, E), of a basic block (the definition will later \nbe ex\u00adtended to more complex control structures) as follows: Let G, be the interference graph of the \nblock and Gj its false dependence graph. V=vr  E= ErU{{U, V}:{ U,V}~~j and U, VCV}  In Figure 3 we \nsee the parallelizable interference graph (a) of Example 1 and a possible register allocation. (a) \nrl =load z r2 i = r2 = a[r21 r3 .rl+rl r2 =r2*5+rl (b) Figure 3: .The parallelizable interference graph \n(a) of Example 1 and a possible register allocation. At this point we note that the parallel interference \ngraph may be augmented to include extra nodes, like stores (see Claim 1), and edges in order to represent \nmore information for the instruction scheduler. Never\u00adtheless, those parts of the augmented graph do \nnot take part in the coloring (register allocation) algorithm. A possible augmented parallelizable interference \ngraph (for a basic block) is: V=v$  E=&#38; U{{ U, V}:{ U, V}CEj and U, VCV}  In this graph an edge \nbetween two nodes means that the two operations may be scheduled at the same cycle or the two nodes represent \nlive ranges that are not disjoint. Thus, at each node v the edges {v, u} e Ej ~ E provide the list of \navailable instructions (with v) as used in list scheduling algorithms such aa in [9]. Theorem 1 Every \noptimal coloring (minimum number of colors) of the parallelizable interference graph G pro\u00advides a register \nallocation without spilling whose schedul\u00ading graph does not have a false dependence. Proofi From Lemma \n1, only an edge from Ej which is also in E can generate a false dependence. From the coloring no two \nvertices incident upon an edge in E are allocated to the same register. This assures that no false output \ndependence is generated. The edges of E. ~ E assure that no false flow dependence is generated. To complete \nthe proof, we next prove that no false anti dependence is generated. Let {u, v} ~ V such that the definition \nin u precedes to that of v, and v has a color (register) that were used in u. We have to show that (u, \nv) is not a false anti dependence edge. Let x be the value (register) used in u and node x, that precedes \nu, represents the definition of that value. Clearly, from the construction of the interference graph \nGr, v is not in the live interval of the definition of z. Thus, the color (register) of z may be reused \nin v. i Going back to Example 2, if we assume that no value is live on the entrance and exit from the \ncode fragment then from the interference graph in Figure 4 we see that only three registers are needed; \nbut there is no restric\u00adtion to assign the same register, for example, to opera\u00adtions S8 and S3 or to \noperations S8 and S2 thus prevent\u00ading the possible parallel scheduling of these operations. Such an assignment \nis impossible with the parallel in\u00adterference graph. S9 o Figure 4: The interference graph of Example \n2. With the parallel interference graph four registers are needed. One of the possible assignments is \nprovided in Figure 5. The next theorem proves the optimality of our con\u00adstruction with respect to the \ninput graph; i.e. no smaller graph will have the property of Theorem 1. Example 2: rl := load z (fixed) \nr2 := load y (fixed) r3 :=rl +r2 r2 :=rl *r2 r3 :=r3 +r2 rl := load x (float) r4 := load w (float) r4 \n:=rl *r4 rl :=r3 +r4 Figure5: A register assignment to Examp1e2. Theorem 2 Let G = (V, E) be a pamlleiizable \ninter\u00adference graph of a baszc block and G = (V, E ) where E =E {u, v}forsome edge{v,v}EE.LetC be an \noptimal colorzng of G. Then, ustng colorlng C on G with the change C(u) = C(v) provides a r-egis\u00ad tera[locationfor \nG whose scheduling,qraph has afalse dependence or aspill is introduced. Proofi If{u,v} c E, ~ Ethen aspill \nis introduced. Otherwise {u,v}~ .EfthusfromLemma Iuandv could rescheduled together according to the schedule \ngraph for the code when presented with symholicregisters. # The results of Theorems 1 and 2 is that, \noptimal col\u00adoring of the parallelizable interference graph is an opti\u00admal register allocation for superscalar \nmachines in the following sense: It provides the assignment of a mini\u00admum number of registers that are \nneeded in order not to gener~te a spill for a live value and it keeps all the parallelization options \nfor the scheduler (no false depen\u00addence in the resulting graph). The case in which spills happen, i.e. \nwhen the number of registers is small, is presented in the next section. We next expand our framework \nto cover scheduling across basic block boundaries. In such scheduling, con\u00adtrol dependence edges between \nbasic blocks are used. In global code motion or percolation based scheduling [8, 13] the instructions \nare moved across branches and join points (inter basic blocks); this is also the case in region scheduling \nbased techniques [1 I] and later on in [3]. For these last two scheduling techniques moving instructions \nis possible only within a region which is a maximal acyclic fragment of code. The scheduling is done \nby logically ignoring the control dependence edges between two basic blocks that are considered as a \nsingle block for scheduling. There are a few criteria for select\u00ading such two blocks and we say that \nthey are piausiljle for being scheduled together. An example of such a case is when one block is executed \nif and only if the other one is also executed. This condition holds when one block dominates the other \nand the second one postdominates the first, and can be verified by observing the donlina\u00adtors tree [1] \nand the postdominators tree (constructed like a dominators tree when the edges in the program flow graph \nare reversed). When generating the global interference graph, the right number of names analysis is used \nto combine live intervals in those cases in which there is a use whose value depends on more than one \ndefinition (i.e., several clef-use chains reach a single use; e.g., when coming from different branches \nof an if-then-else statement). See for example Figure 6. Figure 6: Three live intervals for variable \nx that are used in a single point. Indeed, although the combination of intervals (def\u00aduse chains) into \none non-linear live interval generates a node in the interference graph whose edge degree is higher than \nany of its constituents, such a combination is desirable since we need the result in a single register. \nThere are no negative implications of such a process on the scheduler; observe that such a combination \ndoes not decrease the options for parallelism since operations on two branches of a conditional may never \nexecute simul\u00adtaneously and in the other cases (anti or output) data dependence prevent parallel executions. \nAt this point our observation that every definition corresponds to a live interval is no longer true, \nbut we may view a node v in G~ as representing all the live intervals of tle definitions v; (nodes of \nG~ ) which com\u00adprise the combined non-linear interval. We later on use the notation vi E v where vi c \nV, and v ~ V, to denote a compound interval v composed of the linear intervals in {2 ;}. For inter basic \nblock scheduling the parallelizable in\u00adterference graph G = (V, E) is constructed as follows: * V is \nthe set of vertices in the global interference graph. e EaEGr U{{ulv}:{u; ,vj} CEGJ, uiEuandvj v}  Where \nEG~ are the edges in the global interference graph and EGj are the edges in I he global false dependence \ngraph. Claim 2 Let G. and G, be the scheduling and inter\u00adference graphs respectively of a progrom segment \nsuch that vi # vj G Va and vi, t.j E v E VT. Then the cov-re\u00ad sponding instructions of vi and Vj may \nnwer ezecuie in parallel. With Claim 2, Theorems 1 and 2 can be shown to be true for this generalization \nto regions, thus enabling the use of the technique also with region (or global) based schedulers. With \nsome caution one can generate an augmentation of the global parallelizable interference graph similarly \nto the one for basic blocks. 4 More on Algorithms From Theorem 1 we know that a minimum size ccJor\u00ading \nset of the parallel interference graph provides a legal register allocation, and from Theorem 2 we know \nthat a the minimum size coloring set of the parallelizable interf\u00aderence graph is the smallest one that \nwill not generate false dependence. Since the number of registers lmay be smaller than the minimum coloring \nset, or the Ibest coloring achieved, heuristics that are being used during either register allocation \nor scheduling may be applied on the parallel interference graph. We next present some heuristics and \nthose properties of the graph that they use. The heuristics are embedded in a Chaitin based coloring \nalgorithm. The first type of heuristic eliminates edges from the graph. The question of which edge to \neliminate may in\u00advolve consideration of both the scheduler and the allo\u00adcator. If we consider removing \nsome edges that prevent false dependence we are doing the job of the sched\u00aduler when, due to register \npressure, some parallelizatiou opt ions are given away. We can also choose to preserve some edges that \npromise good parallel ization and decide to remove interference edges which may lead to spill /(the spill \nmay be avoided only when two such nodes get dif\u00adferent colors in spite of the absence of the interfering \nedge). The following rules are used: . Remove those edges of E Er for which the parallel scheduling of \nthe two instructions have the small\u00adest priority for the scheduler. Such a priority lmay involve early \nscheduling of an instruction which is last on a critical path of the precedence edges (in E.). Avoid \nthe removal of edges in Ej n Er. They are used by both the scheduler and the allocator Lemma 2 A llocatlng \na single regwt er for dejinitxons u and v where {u, v} c E Ep may not cause a spill bui restricts parallelism. \n Lemma 3 Allocating two registers for definitions u and v where {u, v} E Ej n Er both prevents a spill \nand en\u00adables the parallelization of the two definitions. Another possible approach is to generalize the \nheuris\u00adtic function used in many register allocators. It is cus\u00adtomary to use the following h function \nfor selecting which definition to spill: h(v) = cost(v) /cleg(v) where cost(v) is the cost of keeping \nthe value v in a register rather than in memory, and c-leg(v) is the degree of v in the graph. It is \nsuggested to improve this function by considering weights on the edges of the parallel interference graph. \nThe weightfunction w : E + 72 can be generated by using Lemmas 2, 3 that distinguish between those edges \nthat preserve parallelism and those that prevent spills. In general, parallelism that will eventually \nmaterialize is preferred over the cost of spilling some extra value, since it utilizes a free unit compared \nto the cost of a spill. Each such type of an edge has its price for any given machine. In particular, \nif all the edges in E E, have weight O then we get the traditional h function. . Let in(v) denote the \nset of vertices that each share an edge with v. The new h function can be: h*(v) = cost(v)/ ~ w({u, V}) \nucin(u) The cost function, in general, is a function of the instruction s nesting level and may not be \nchanged. Since the interference graph of the code uses the se\u00adquential ordering of the instructions \nwe will add a pre\u00adliminary scheduling heuristic for selecting one such or\u00adder. The parallel interference \ngraph G = (V, E) is first extended by adding to every node v c V a number EP(v) representing the earliest \npossible time for schedul\u00ading v (in [7] EP stands for early partition). The EP numbers are computed from \nthe scheduling graph (G,); during this stage the delay numbers (on the edges) that are part of the scheduling \ngraph (they were ignored in our presentation of GJ ) may be used for generating more accurate EP numbers. \nA registers allocation Algorithm Given a program segment represented with symbolic registers, the schedule \ngraph, Gs, is first generated. The mapping EP : V -N of the earliest possible time for scheduling is \ncomputed. Scheduling heuristics ~averse tile graph pro\u00adcessing nodes by increasing EP numbers (smallest \nfirst ). Whenever all the operations with the same EP number cannot be scheduled together (machine limitations) \nselect the operations to he postponed; increase the EP number of each node in the post\u00adponed set and \nupdate the EP numbers on all the paths -(in G,) leaving the node. When this process terminates select \na linear order Which is consistent with the partial order of the new EP numbers and reorder the program \nsegment accord ingly. Coloring procedure Generate the parallelizable interference G = (V, E) of the \ncode fragment. De\u00adnote by r the number of registers in the machine and initialize a spill list to be \nempty. G := G while G is not empty do ;;; simplify while there is a node v E V with degree smaller than \nr do delete v and update G end do while there is a node in V withdegree smaller than r when only interference \nedges are considered do use scheduling considerations to remove from both G and G a false dependence \nedge not in E, (e.g. an edge {v, u} for which scheduling u with v contributes the least ), thus giving \nup some possible parallelization (this can be done globally or with respect to a selected node)3; while \nthere is a node v c V with degree smaller than r do delete v and update G end do end do if V is not empty \nthen { choose v C V with min h*(v) = cost(v) /&#38; E,,,(t,)u)({u,v}); delete v and update G; place \nv on the spill list } end do 3 Currently, it is ako advisable to use the knowledge of multiple units \nof the same type when such units exist if 1]Ie spill list is empty then ;;; select color the nodes in \nreverse order of deletion (this is done by rebuilding G a node at a time) else { ;;; spill spill each \nv in spill list; repeat the coloring procedure } Remarks and Discussion The desire to allocate a small \nnumber of registers to be used at any point during the execution of a program is driven both by the limited \nnumber of registers and by the fact that temporary sav\u00ading of registers values, for example, when a procedure \nis called, is time consuming. The second while loop (in the simplify part ) guarantees that the convergence \nproperty of the algorithm will be similar to the one proved for the original algorithm. It is pointed \nout that since we keep all the possible parallelism up to that stage, for most of the instructions there \nwill be many scheduling pos\u00adsibilities, thus scheduling pruning is a must during the simplify stage. \nAcknowledgment. We would like to thank David Bernstein, Itai Nahshon and Nathan Srebro for fruitful discussions. \nReferences [1] A. V. Aho, R. Sethi, and J. D. Unman. Co7npil\u00aders Principles, Techniques, and Tools. \nAddison-Wesley, Reading, MA, 1986. [2] F. Allen, B. K. Rosen, and K. Zadeck. (Eds.) Optimization tn Compilers. \nACM Press/Addison-Wesley, to appear 1992. [3] D. Bernstein and M. Rodeh. Global instruc\u00adtion scheduling \nfor superscalar machines. In SIG\u00adPLAhr 9.1 conference on programming Languoge Design and Implementation, \npages 241-255. ACM, June 1991. [4] D. G. Bradlee, S. J. Eggers, and R. R. Henry. Inte\u00adgrated register \nallocation and instruction schedul\u00ading for RISCS. In The 4th International Con\u00adference on Architectural \nSupport for Programming Languages and Operating Systems, pages 122-131, April 1991: [5] G. J. Chaitin, \nM. A. Auslander, A. K. Chanclra, J. Cocke, M. E. Hopkins, and P. W. Markstein. Register allocation and \nspilling via graph coloring. Computer Languagesj 6:47-57, 1981.  [6] F. Chow, M. Himelstein, E. Killiam, \nand L. Weber. Engineering a RISC compiler system. In COMP-CON Proceedings. IEEE, March 1986. [i] S. \nDavidson, D. Landskov, B. D. Shriver, and P. W. h~allett. Some experiment in local microcode com\u00adpaction \nfor horizontal machines. IEEE Transac\u00adtions on Computers, C-30( 7):460-4 i7, July 1981. [8] J. A. Fisher. \nTrace scherl ul ing: A technique for global microcode compaction. ACM Transaction on Computer Systems, \nC-30(7):478-490, July 1981. [9] P. B. Gibbons and S. S. Muchnick. Efficient instruc\u00adtion sceduling for \na pipelined architecture. In Pro\u00adceedings of the SIGPLAN fifi Sympsoiurn on Com\u00adpiler Construction, volume \n21, pages 11 16, July 1986. [10] J. R. Goodman and W. Hw]. Code scheduling and register allocation in \nlarge lmsic blocks. In lnierna\u00adtional Conference on Supcri-ompu.ting, pages 442\u00ad 452. ACM, July 1988. \n [11] R. Gupta and M. L. Sofia. Region scheduling: An approach for detecting and redistributing paral\u00adlelism. \nIEEE Transaction ov ,~nfiware Engm eering, 16(4):421-431, April 1990. [12] J. Hennessy and T. Gross. \nPostpass code opti\u00admization of pipline constrai tIIs. ACM Transacl!ion on Programming Languages 071 d \n,qystems, 5(3):422\u00ad448, July 1983. [13] A. Nicolau. Uniform parallelism exploitation in or\u00addinary programs. \nIn Intcrnofional Conference on Parallel Processing, pages 614-618. ACM and IEEE Computer Society, August \n198.5. [14] K. O Brien, B. Hay, J. llinish, H. SchalTer, B. Schloss, A. Shepherd, and Xl. Zaleski. Advanced \ncompiler technology for the R lSC System/6000 ar\u00adchitecture. In .IBh[ RISC Sy.sirnl\\6000 Tech.??olt~gy, \npages 154-161. IBM SA23-2619, 1990. [15] R. F. Touzeau. A Fortran compiler for the FPS\u00ad164 scientific \ncomputer. IJ) Prnrcedings of the SIG-PLAh 8.4 S7JIJ2PSOZU7TI On ~O177pl/(Y cOIJSf.TUCt:iOll, volume \n19, pages 48-57, June 1984. [16] H. S. Warren. Instruction scheduling for the IBM RISC system/6000 processor. \nIBhf Joarml Re\u00adsearch and Development, 34(1 ), January 1990. \n\t\t\t", "proc_id": "155090", "abstract": "<p>We present a new framework in which considerations of both register allocation and instruction scheduling can be applied uniformly and simultaneously. In this framework an optimal coloring of a graph, called the <italic>parallel interference graph</italic>, provides an optimal register allocation and preserves the property that no false dependences are introduced, thus all the options for parallelism are kept for the scheduler to handle. For this framework we provide heuristics for trading off parallel scheduling with register spilling.</p>", "authors": [{"name": "Shlomit S. Pinter", "author_profile_id": "81100157492", "affiliation": "", "person_id": "PP39077339", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/155090.155114", "year": "1993", "article_id": "155114", "conference": "PLDI", "title": "Register allocation with instruction scheduling", "url": "http://dl.acm.org/citation.cfm?id=155114"}