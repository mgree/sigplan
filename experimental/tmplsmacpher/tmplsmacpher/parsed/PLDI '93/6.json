{"article_publication_date": "06-01-1993", "fulltext": "\n A Practical Data Flow Framework for Array Reference Analysis and its Use in Optimizationst Evelyn Duesterwald \nRajiv Gupta Mary Lou Soffa Department of Computer Science University of Pittsburgh, Pittsburgh, PA 15260 \ndueste@cs.pitt.edu Abstract Data flow analysis tecluiques have traditionally been restricted to the analysis \nof scalar variables. Thk restriction, however, imposes a limitation on the kinds of optirnizations that \nCm be performed in loops containing array references. We present a data flow framework for array reference \nanalysis that provides the information needed in various optimization targeted at sequential or fine-grained \nparallel architectures. The framework extends the traditional scalar framework by incorporating itera\u00adtion \ndistance values into the analysis to qualify the computed data flow solution during the fixed point iteration. \nAnalyses phrased in this framework are capable of discovering recurrent access patterns among array references \nthat evolve during the execut;on of a loop. The framework is practicat in that the fixed point solution \nrequires at most three passes over the body of structured loops. Applications of our framework are discussed \nfor register allocation, load/store optimizations, and con~olled loop unrolling. 1. Introduction Traditionat \nprogram analysis and optimization techniques are typically restricted to scalar variables. Interest in \nthe analysis of array references is growing with the recognition of its impor\u00adtance in detecting loop-level \nparallelism. The development of parallel architectures motivated the study of data dependence tests for \narray references to fully exploit the available parallelism [17, 25, 26]. These tests are designed to \ndissmbiguate array references, that is, to determine whether two array references may refer to the same \nmemory location. However, sequential or fine-grained parallel architectures (e.g., pipelined, superscalsr, \nor VLIW architectures) do not always benefit from this kind of dependence information and the complex \ncode transformations used in psrallelization. Instead, these architectures offer their own class of loop \noptirnizations and analysis requirements. Recurrent accesses to subscripted data are a major source in \noptimizing the execution of loops. For example, an important optimization of recurrent access patterns \namong the subscripted references in a loop is the avoidance of redundant memory accesses through appropriate \nregister allocation strategies. When dealing with scientific programs that spend most of their T Pardatty \nsupported by National Science Fourrdatlon Prrsdrmial Young Investi\u00adgator Award CCR-91 57371 and Grant \nCCR-9 109089 to tbe University of Pitts\u00adburgh Permission to copy without fee all or part of this material \nis granted provided that the copies are not made or distributed for direct commercial advantage, the \nACM copyright notice and the title of the publication and its date appear, and notice is given that copying \nis by permission of the Association for Computing Machinery. To copy otherwise, or to republish, requires \na fee and/or specific permission. ACM-SlGPLAN-PLDl-6 /93/Albuquerque, N.M. o 1993 ACM 0-89791 -598 -4/93 \n/0006 /0068 . ..$1.50 time executing loops, reducing the overhead in memory accesses contributed by array \nreferences can greatly improve the overall program performance. The information needed to support these \nfine-grairted optimization of individual array references differs from data depmdence information in \ntwo ways. Firs~ flow-sensitive infor\u00ad mation is needed. A potential overlap in the referenced memory \nlocations as determined by conventional data dependence tests, does by itself not imply a flow of values \nbetween the involved references. However, optirnizations such as register allocation are based on information \nabout reuse of previously accessed values as opposed to information about re-accessed memory locations. \nTo determine if two references that access the same memory location actually reference the same value \nmandates the inclusion of control flow into the analysis. Condkional con~ol flow is often ignored when \ntesting for data dependencies in a psratlelizing compiler, as a potential (loop-camied) dependence between \ntwo arbitrary memory references may prevent psralleli\u00ad zation. Hence, conventional data dependence information \nis inadequate for fine-grained optirrtizations. Second, dependence information can only be exploited \nfor fme-grained optirnizations if the precise identity of the instances that are involved in the dependence \nis known. For example, in order to assign registers to array elements, the live range of an array reference \nmust be determined, which includes the precise number of iterations that the computed value is live. \nThus, the analysis we need should be focused on the dependen\u00adcies among array references that are sepwated \nby constant itera\u00adtion distances, that is, on a class of dependencies whose detec\u00adtion requires less \ncomplex analysis methods. This paper presents a data flow framework that addresses these two requirements \nto systematically exploit fine-grsined optimization opportunities for array references. The framework \nextends the traditional scalar framework [16] to the analysis of subscripted variables to meet the requirement \nof flow-sensitivity which includes the proper handling handling conditional control flow. The detection \nof recurrent access patterns among sub\u00adscripted references that evolve during the execution of a loop \nis modeled in the framework as a fixed point computation. The solution to classical scalar data flow \nproblems, such as reaching definitions, is adapted to subscripted references by qualifying the solution \nwith iteration distance vshtes. Specifically, the com\u00adputed fixed point at a program point denotes the \nmaximal itera\u00adtion distance for which the respective data flow information holds at that point. Analyses \nphrased in this framework we prac\u00adticat in that at most three passes over a structured loop are required \nto find the fixed point solution. This efficiency makes the snatyses attractive for use in optimizing \ncompilers for sequential or fine-grained parallel architectures. The opttilzations supported by the framework \nextend traditional scalar optirnizations to individual array elements. We demonstrate three classes of \napplications, Firs~ we show how the computed information can be used to extend register rtlloca\u00adtion \ntechniques to subscripted variables. We outline our tech\u00adnique for register pipeliniig [9] that preserms \nan integrated approach to register allocation for both scalar and subscripted variables. The second class \nof applications is simple load/store related optimization of array references. Finally, we show how the \ntiamework is used to efficiently provide the information needed to control loop unrolling, a loop transformation \naimed at uncovering fine-grained parallelism across loop iterations. In our analysis, we consider Fortran-like \nDo-loops, that may contain conditionals or nested loops, but no arbitrary gotost. A loop is controlled \nby a basic induction vmiable, i, and we assume that no statement in the loop contains an assignment to \ni. Furthermore, we assume that prior to the analysis, non-basic induction variables have been identified \nand removed [1] and that all loops are normalized, i.e., the induction variable ranges from 1 to an upper \nbound UB with increment one. The rrnray references in a loop are restricted to references whose subscripts \nare affine functions of the loop s induction variable, i e., refer\u00adences of the form X ~ (i )], where \n~ (i ) = a x i+ b with induction variable i and integer constants a and b. Section 2 introduces the notion \nof data flow information for array references. The data flow framework to compute this information is \npresented in Section 3. The analysis is described in detail for one-dimensional arrays. Multi-dimensional \narray references are discussed in Section 3.6. Section 4 discusses applications of the framework. Related \nwork and conclusions are given in Section 5 and Section 6. 2. Data Flow of Array References A data flow \nanalysis of array references in a loop provides the solution to a specific data flow problem, that is, \nto a specific question about the flow of array element values inside the loop. To express the access \npatterns of array references the computed solution is qualified with iteration distance values to denote \nthat the solution at a point holds only up to a maximal iteration dis\u00adtance. As an example, consider \nthe computation of reaching definitions. In contrast to scalars, a single textual detlnition of a subscripted \nvariable represents definitions of an entire range of memory locations. Reaching definition analysis \nfor subscripted variables determines at each point in the loop the range of previ\u00adous definition instances \nthat reach that point (i.e, are not redefined prior to the point). Thus, instead of the binary informa\u00adtion \nreaching and not reaching that is computed in sc;alar reaching definition analysis, array reference analysis \nrequires more refined information. Specifically, the information reach\u00ading is qualified with respect \nto a maximal iteration distance ~t. A definition d reaches a point p with iteration distance 6 if the \nlatest 8 instances of d reach p . The iteration distance 6 is muxirnal if 6 is the largest distance vahre \nfor which definition d reaches point p . The upper bound for the maximal iteration dktnce in a loop with \nUB iterations is UB -1 denoting the complete range of iteration instances. Instances of a definition \nmay not reach a point if there was a 7 Note that only the gotos that destroy the smgte-emry, singte-exit \npro. perty of loops cause problems in our analysis. Gotos that keep control inside the loop can be handled. \nHowever, backward gotos inside a lcmp affect the efficiency of the analysis. Nested cycfes created by \nbackward gotos result in additional passes over the loop during the fixed point iteratmn. do i=l,UB (1) \nC[ih2] := C[i] x 2; (2) B[2 x i] := C[i]~X; (3) ~C[i]=O then C[i]:=B[i-l]; (4) B[i] := C[i+l]; enddo \n  Fig. 1. Subscripted references inside a loop prior redefinition of the same array element. For example, \nthe definition C [i ] in statement (3) in Fig. 1 redefines the instance of definition C [i+2] from two \niterations earlier. Thus, defirthion C [i ] kills all instances of C [i +2] with an iteration distance \ngreater than one. As a consequence, only the instances of C [i +2] with a distance up to 1 reach the \nexit of statement (3) and in tom the entry of statement (4). Thus, the reference C [i+l] in state\u00adment \n(4) is always a use of the value defined by statement (1) one iteration earlier. 3. A Framework for Array \nReference Analysis Our array reference analysis operates on a loop flow graph FG =(N, E) thatrepresents \nthe body of a loop. The nodes in N denote statements in the loop or summary nodes, and the edges in E \nare control flow edges. A node, exit, is added to the graph to explicitly represent the increment operation \nof the loop induc\u00adtion variable, i.e., exir contains the operation i :=i +1 for induc\u00adtion variable i. \nThe loop flow graphs are constructed in a hierarchical fashion, startiig with the graphs for the innermost \nnested loops. A single loop is analyzed at a time. When moving to the analysis of array references that \nare contained in an enclosing loop, the nested loops are replaced by summary nodes. Thus, no loop flow \ngraph has nested cyclic control flow at the time it is being analyzed. The data flow framework defines \na system of equations over the flow graph of a loop body that is solved by fixed point iteration. The \nfixed point at each node in the loop flow graph describes the maximal iteration dtstance for which the \nsolution holds at that node. Informally, instances of subscripted refer\u00adences we propagated throughout \nthe loop from points where they are generated until points are encountered that kill the instances. Which \nsubscripted references act as generating or killing refer\u00adences (e.g., uses and/or definitions of subscripted \nreferences) depends on the specific data flow problem, and is thus a parame\u00adter of the framework. We \ndescribe the analysis in a meet data flow framework [16], which most naturally models data flow problems \nthat pro\u00advide must-information, i.e., an underestimate of the actual irtfor\u00admation that holds. However, \nas in the scalar case, problems to provide may-information, i.e., an overestimate of the actual information, \ncart also be formulated in the framework. The equation system in the framework defines for each subscripted \nreference in the loop a lattice value that denotes a maximrd itera\u00adtion distance. Thus, instead of the \nconventional binary lattice {T, J-) used in the classical scalar data flow problems, a chain lattice \nL of iteration distance vahtes, as shown in Fig. 2, is used. A lattice value x EL for a subscripted reference \nr denotes the range of the latest x instances of r. The meet operator A in this lattice is the usual \nminimum operator min defined for integers, where Vx CL: min(x, J_)=_L tutdmin(x,T)=x. Wealsouseadualrnax \noperator which corresponds to the maximum operator for integers: Vxe L:max(x, l)=x andmax(x,T)=T. . 69 \ntop = all instances instances up to max. distance -L bottom = no instance Fig. 2. Lattice L of maximal \niteration distance values 3.1. The Flow Functions To model a specific data flow question about the way \nthe sub\u00adscripted variables are referenced inside a loop, a pair of parame\u00adters (G, K) must be supplied. \nG is a set describing the sub\u00adscripted references in a loop that generate instances and K is a set describing \nthe references that kill instances. The subscripted references that occur in the loop are identified \nby integers. Thus, the sets G and K are sets of integers. We use the notation G [n ] and K [n ] to denote \nthe generating and killing references that occur locally in node n. The pair (G, K) uniquely determines \na space of flow functions in the framework. Let IG I = m. Then a flow function f.: Lm + Lm that operates \non tuples of iteration distance values is defined for each node n. The function f. describes how the \niteration distance values associated with the m refer\u00adences in G are affected when control passes through \nnode n. For ease of presentation, we describe the definition of the flow functions using the example \nof determining the must-reachmg definitions. The generalization to other must-problems is immediate. \nDetermining must-reaching definitions is the problem of computing the instances of subscripted variable \ndefinitions that reach a node along all paths in the loop flow graph, In this prob\u00adlem the set G [n ] \ncontains the definitions of subscripted variables that occur in node n. Each definition may also act \nas a killing reference. Hence, the set K [n ] also consists of the definitions in node n. The flow function \nf : Lm + L m operates on tuples (x,,..., x~ ) of maximal iteration distance values. For each definition \nd, f. determines a lattice element XdeL denoting the range of previous instances of d that must reach \nthe exit of node n, All flow functions operate independently on each component in Lm, i,e., there are \nfunctions f~, . . . . f ~ such that V(xl, . . ..x~)eLm. fn(x,,..., %)=( fnl(xl)> . ..> f.(xl? l)). We \ndefine the component f$ L &#38; L with respect to a single definition d. The definition off. form definitions \nresults as the Cartesian product over the m components. There are two types of flow functions: f~(x)=m(x, \n0) or f!(x) =min(x, pj), where p: is a constant in L. 3.1.1. Generate Functions If ds G [n ] then node \nn generates definition d with distance 0, The flow function f # is of the first type, i.e., f j(x ) \n= max (x, O), and is called a generate function. 3,1,2, Preserve Functions Functions of the second type, \ni.e., f~(x ) = min (x, p:), are called preserve functions. The constant pf denotes the maximal itera\u00adtion \ninstance of definition d that is preserved, i.e., not killed, by an element in K [n 1.Let definition \nd be of the form d =X[fl(i)]. \u00ad (i) If node n contains no definition to array X, then p:= T, i.e., all \ninstances of d are preserved. In this case, the preserve function f # is simply the identity, i.e., f \n#(x ) = x. (ii) If node n contains a definition d = X[ f Ji )]~K[n], a safe underestimate of the maximal \nnumber of previous itera\u00adtion instances of d that are not redefined by d must be deter\u00admined. The range \nof previous iteration instances of d =X ~l(i )] with respect to node n is described by X ~ I(i 6)], \nwhere fi 21. If definition d occurs in a predecessor node of n in the loop body, the instance of d with \nd~tance O is also included in range of previous instances, i.e., in thii case 6 ? O. To uniformly model \nthese two cases we define a predicate pr: O if referenced occurs in a predecessor node of n pr (d ,n \n) = I otherwise { Let I denote the iteration range [1, . . . . UB ] of induction vari\u00ad able i, then p~=max{i5<UB \nIV iel, Vfi 3pr(d,n) 56 <&#38; f2(i)#fl(i-V ) Let the subscripts f ~ and f2 be fl(i)=al xi +bl and fz(i)=azxi \n+bzandlet al-a2 xi+ bl b2 k(i)= al al The equation can then be rewritten as: pj=max(i5<UB lVi~I, V Y3pr(d,n)< \n&#38;S&#38; i$ #k(i)). To determine the constant pf we distinguish three cases for func\u00adtion k. If k \nis the constant pr (d ,n ) (i.e., V i ~19k (i ) = pr (d ,n )), then every &#38;finition generated by \nd is killed by d . In this case: p:= 1. Examples are textually identi\u00adcal references for pr (d ,n ) = \nO. 0 If k is below pr (d ,n ) in the iteration range I (i.e., Vie I: k(i)< pr (d ,n )), then there are \nno instances of definition d that redefine a previous instance of definition d. Hence, p#=UB 1 =T. An \nexample of this case is d =X[i] and d =X[i+2]. E Otherwise, k is a function that has positive values \nabove pr(d,rz) in the iteration range I (i.e., 3ieI: k(i) >pr(d,n)). If there are positive integer values, \nthen there are instances of d that kill some earlier instrmces of d. An example is d = X [ 2 x i ] and \nd = X [i ]. We obtain a safe approximation of p: from the minimum (integer) value of k above pr (d ,n \n), i.e., pf=[min{ k(i) I i~I, k(i) >pr(d,n) )1 1. Ifk is a positive integer constant this approximation \nis accurate. In summary for case (ii) we obtain -L if Vie I: k(i)= pr(d,n) p:= if Vie I: k(i) <pr(d,n) \n[rein{ k(i) I ;I, k(i) >pr(d,n) ]1-1 otherwise [ 3.1.3. Exit Function Based on the constants pf the flow \nfunctions for statement nodes are fully defined. Consider now the flow function for the addi\u00adtional node \nexit in the loop. Node cril represents the increment operation of induction variable i, which expresses \nthe transfer to a higher iteration. Thus, propagating information across node exit reflects the propagation \nto future iterations. The increment in node exif affects the propagated lattice values as follows: An \ninstance of a definition that reaches the exit of the loop body with distance fi reaches the entry of \nthe next loop iteration with distance fi + 1. Thus, the flow function J cti, increments the itera\u00adtion \ndktance vahre of each reference in preparation for the next iteration, i.e., ~~n., (x ) = x++, where \nthe increment operator -t-+ is defined as: ifx=T x+-t={ -L ifx=l x+1 otherwise I T [ For an example \nof the flow functions see Section 3.5 that illus\u00adtrates the analysis for the loop in Fig. 1. 3.2. The \nEquation System Based on the parameters (G, K) all flow functions are defined locally at each node for \nthem references in the loop. The global data flow solution is determined in a greatest fixed point compu\u00adtation \nthat iteratively lowers lattice elements associated with each node until information stabilizes. The \nresults are the assign\u00adments IN[n]= (xl, ,.. ,x~) and OUT[n]=(yl, ..., y~) at each node n. IN [n ,d ] \n= xd describes the maximal iteration dis\u00adtance for which instances of definition d must reach the entry \nof node n. OUT [n ,d ] describes the corresponding information at node exit. The iteration is started \nwith a safe initial guess, that is, one that overestimates the greatest fixed point. We overestimate \nthe solution by assuming that IN [n ,d ] = T for each definition that is generated along the meet-over-all-paths \nin the loop body (i.e., along all paths in the loop body leading to n in a must\u00adproblem). The initial \nguess is obtained in the following initiali\u00ad zation pass that visits the loop nodes in reverse postorder. \nLet pred (n ) denote the set predecessor nodes of node n. hen if n = 1 (loop entry) otherwise T ifd~G[n] \nOUT[n,d]O= IN [n ,d ]0 otherwise { Based on the initial guess, the fixed point is computed iteratively \nover the following equation system. V d e { 1, . . . . m ): IN [n,d]i+ = A OUT[m,d]i m=ped (n) OUT[n,d]i \n=ff(lN[n,d]i) Note that the computed information describes the access patterns among the subscripted \nvariables that evolve during the loop iterations. Thus, the information determined in the tuples IN [n \n] and OUT [n ] holds after some inhial start-up iterations. Specifically, let the lattice values for \na definition d be x on entry to node n, i.e., IN [n ,d ] =x. Instances of d must reach the entry of node \nn up to iteration distance ~, where pr (d ,n )s 6 S x, for all but the first 8 iterations of the loop. \nClearly, the number of required start-up iterations must be taken into account when applying optirnizations \nto the subscripted references. An important property of the equation system is that in addition to the \ninitialization pass, only two passes through the loop body are needed to reach the fixed poin~ provided \nthe nodes in the loop flow graph are visited during each pass in reverse postorder. This efficiency results \nfrom the fact that a loop flow graph contains no nested cycles and from properties of the flow functions. \nThere are two types of statement node flow functions, ~(x) = min (x, c ) and ~ (x ) = ma (x, O) with \nconstant c EL, and the exit function is ~Cti, (x ) = x++. As can easily be shown, the statement node \nflow functions are idernpoten~ i.e., ~ ~ =! ~d ~,ti, ~ weakly idempotenti i.e., ~,fi,~.xti > ~. Weak \nldempotence unplies that one traversal around a cycle is sufficient to compute the contribution of the \ncycle [18]. Thus, in addition to the initialization pass, two passes over the loop body are sufficient \nfor the fixed point convergence. Let N denote the number of statements in the loop. The fixed point is \nreached in 3 x N nodes visits and the space required is O (N2) to maintain the setsIN and OUT at each \nnode. The overall analysis of a program is performed hierarchi\u00adcally starting with the innermost nested \nloops and working towards the outermost loops and the main program. The analysis provides information \nof the evolving access patterns that arise in the currently analyzed loop. If induction variables of \nenclosing loops occur in subscript expressions, they are treated as symbolic constants. After the analysis \nof a loop 11 with induction variable i 1 is completed, the references in the enclos\u00ading loop 1~ with \ninduction variable iz are analyzed. The nested loop 11 is incorporated into the analysis of 12 in the \nform of a summary node that replaces 11in the loop flow graph of 1z. Dur\u00ading the analysis of 12, a summary \nnode is treated as any other node. Thus, sets G [1~] and K [1J are determined for the loop 11 and a corresponding \nflow function $1, is defined. The set G [1~] contains only references in 1~ whose subscripts are functions \nof the outer induction variable i2, i.e., references of the form X [a x i2 + b ]. Other references in \n11 do not act as generating references in the outer loop. However, all references in 11 may act as killing \nreferences. We conservatively assume that a refer\u00adence X[al x il + bl] in the inner loop kills all instances \nof a reference X [a z x i z + b ~ in an enclosing loop. We are currently investigating how knowledge \nabout the iteration bounds of the inner induction variable i 1 can be incorporated to provide more accurate \nkilling information in an enclosing loop. 3.3. May-Information The meet framework as defmed in the previous \nsection naturally models must-problems, i.e., all-paths problems. To formulate may-problems, i.e., any-path \nproblems, such as conventional reaching definition analysis or live variable analysis, some modifications \nare needed in both the meet lattice L and the space of flow functions. The lattice for a may-problem \nis simply the reverse of the lattice L, i.e., T denotes no instance and _L denotes all instances . The \nmeet in the reverse lattice is the dual operator m, i.e., A . max. Changes in the flow functions involve \nthe definition of the constant pf in a preserve function ~f(x ) = min (x, p:). Unliie a must-problem \nwhere the constant pi describes an underestimate of the actual information, pf describes an overestimate \nin a may-problem, so m to ensure that no information is missed. Thus, pj denotes the instances of d that \nmay be preserved by elemen~ in K [n ]. Unless there is a definhe kill that regularly kills instances \nof d in each iteration, we assume that all instances of d are preserved. Let d = X [f(i)] and let d be \na reference in K [n ]. A definite kill occurs if d is of tbe form d =X~(i)+c]. In this case, only instances \nof d up to d~\u00adtance c 1 are preserved by d . Let the function k be defined as in Section 3.1.2, then \npj is determined W: -r ifviei: k(i) =pr(d,n) P:= / c 1 if Vie I: k(i) =c, c >pr(d,n) -L otherwise *1 \n[ A further modification in a may-problem concerns the estimate of an initial guess for the fixed point \niteration. A safe initial guess for a may-problem would be T (no instance) for all definitions. However, \nwhen starting the iteration with this initial\u00adization, the fast fixed point convergence can no longer \nbe guaranteed. The reason is that the function ~=ti( is no longer weakly ldempo ten; I.e., ~=m.~.~e~,i \n_ ~=, m the reverse lattice. <f. Starting with an initial value T may require UB 1iterations to obtain \nthe lattice value -L for a definition. As the bound UB may not be known, the iteration could continue \ninfinitely. Fortunately, it is possible to predict the increment effect of function ~.ti~. The initial \nguess in a may-problem predicts the maximal effect of the increment function, i.e., rdl lattice values \nare initialized to 1 t. It can easily be shown that the composi\u00adtion of any flow function with ~,ti, \nis idempotent with respect to the initial guess -L, i.e., ~,,il ~ ) ~~.X~ ~ ) (-L ) = ~,~,.~ ) (-L ), \nwhich implies that information converges within two passes over the loop body. Note, that although the \ninitialization is lower in the lattice than the fixed poin~ no information is lost since the preserve \nfunctions will eventually raise the inhial lattice value all instances to the greatest fixed point. An \ninitialization pass is not required in a may-problem and the greatest fixed po~t is computed in 2 x N \nnode visits. 3.4. Backward Problems The framework models a forward flow of information. That is, information \nis propagated from control predecessors to control successors and from earlier iterations to later iterations. \nIn a backward problem, such as live variable analysis, the flow of information is in the opposite direction. \nAs usual, to phrase the backward flow from control successors to control predecessors in a forward framework, \nthe reverse graph of the loop flow graph is used. In addition, to model the backward flow from later \nto earlier iterations, the role of negative and positive iteration dis\u00adtances is interchanged. This interchange \nonly involves the definition of preserve functions and is modeled by redefining the function k used to \ndetermine the maximal distance of preserved instances as: az al xi+ bz bl k(i)=   3.5. An Example: \nMust-Reaching Definitions We illustrate the framework by an instance to compute the must-reaching definitions. \nMust-reaching definitions provide information about the guaranteed use of previously computed values, \nwhich can, for example, be exploited by register alloca\u00adtion techniques. A definition d at a pointp must \nreach pointp with maxi\u00admal iteration distance 8 if d reaches p along all paths starting at p that extend \nup to b iterations. As described earlier, the framework parameters G [n ] and K [n ] both contain the \ndefinitions of subscripted variables that occur in node n. The solution IN [n ,d ] = x denotes that definition \nd must reach node n with iteration distance 6 for pr (d ,n ) s ISs x, t The increment effect of function \n~.., can be fommlty medeled using a wkiening operation [6]. B [2xi ]:=C [i ]+X 2 ifC[i] C[i]:=ll [i-l]; \n3 B[i]:<[i+ l]; 4 Ei+ i:=i+l; 5 0 Fig. 3. Loop flow graph for the loop in Fig. 1. The data flow analysis \nfor must-reaching defirtkions is illustrated for the loop in Fig. 1 whose flow graph is depicted in Fig. \n3. Each subscripted defirthion is numbered by the contain\u00ading node, i.e., {1: C [i+2], 2: B [2 xi], 3: \nC [i], 4: B [i]). Thus, we obtain G~]=K~]= { j). In the five node flow functions that follows the component \nxi denotes the lattice element for definition i according to the above numbering. fl(xl, x2, x3, ~4)=(-(xl,0), \nX2, X3, X4) j_2(xl, x2,x3,x4) = ( :1, U(% O), X3, X4) f3(xl, x2, X3,X4)= (fnl~(x~, 1), X2, -(X3, ()), \nX4) f4(xbx2. x3, b) = ( xl, m~~(x2, 0), X3, -(X4,0)) f5(xl, x2,x3,x4) = ( xl++, X2++ ,X3++, X4++) initialization \npass tuples (C[i+2],B[2xi],C[i],B[i]) lN[l] (J_, _L, -L, -L) Of/T[l] (T, J-, L,L) IN [2] (T, L,L, L) \n0UT[2] (T, T,_L,l) IN [3] (T, T, L, J_) 0UT[3] (T, T, T, J_) IN [4] (T, T,L, L) 0UT[4] (T, T,L, T) . \nIN [5] ( 1, I ,1,7-) 0UT[5] (T, T,L, T) (i) Initialization pass. 1. pass 2.pms tuples (C[i+2],B [2xi],C[i],B[i]) \n(C[i+2],B [2xi],C[i],B[i]) IN [1] (T, T, J-, T) (2,1, J_, -i-) OVT[l] (T, T,l, T) (2,1. L,T) IN [2] (T, \nT,L, T) (2,1, L,T) 0UT[2J (T, T,_L, T) (2,1, -L, T) IN [3] (T, T,_L, T) (2, l, L,T) 0UT[3] (I, T, O,T) \n(I, I,o, T) IN [4] (I, T,_L, T) (l, 1,1, T) 0VT[4] (1, 0,-L., T) (l, o, L,T) IN [5] (1, 0,1, -i-) (1,0,1,7-) \nI 0UT[5] (2, l,_L, T) (2,1; L;T) (ii) Iteration over two passes. Table 1. Data flow tuples for must-reaching \ndefinitions in Fig. 3. The data flow computation is illustrated in Table 1 (i) showing the initialization \npass and in Table 1 (ii) showing the iteration over two passes that visit the loop nodes in reverse pos\u00adtorder. \nTo determine the guarantmd use of previously computed vahres based on the results of must-reachmg definition \nanalysis, the tuples JN [n ] are inspected. Let u be a use of a subscripted variables at node n, i.e., \nu = X ~ (i )]. Use u has a must-reaching definition in the loop with iteration distance 8 if the loop \nccm\u00adtains a definition d =X~(i 8)] and pr(d,n)s 5< N[n,d], i.e., 8 is within the range of previous instances \nof d that must reach node n. Consider the computed must-reaching definkions for the loop in Fig. 3. The \nuses of C [i ] in nodes 1 and 2 reuse the value computed by definition C [i +2] two iterations earlier \nsince C [i+2] must reach these nodes with iteration dktance 2. The solution at node 3 indicates that \nall previous instances of definkion B [i ] must reach node 3. Hence, the reference B [i --1] uses the \nvalue computed in node 4 one iteration earlier. Finally, the definition C [i +2] must reach node 4 with \ndistance 1, imply\u00ading that the reference to C [i + 1] uses the value computed by C [i +2] one iteration \nearlier. 3.6. Multi-Dimensional Arrays The framework models the analysis of recurrent access pattenns \nthat arise with respect to one enclosing loop li with induction variable i. Multi-dnensional array references \nin ii are of the form X[~l(i), ..,, f. (i )], where the subscript fk (i ) in each dimension k is an affine \nfunction of induction variable i. frtdn.sc\u00adtion variables of other enclosing loops that occur in f ~(i \n) act as symbolic constants during the analysis of 1~. Multi-dimensional references are incorporated \ninto the analysis by linearizing them to a single-dimensional reference X ~ (i )]. The presence of symbolic \nconstants in the linearized subscript f (i ) may compli\u00adcate the determination of the preserve functions. \nTo prevent overly conservative information, the loop bounds that restrict the range of values for these \nsymbolic constants may be incor\u00adporated into the computation of the preserve functions. Consider for \nexample the recurrence among the refer\u00adences in statement (1) in Fig. 4. The linearized references are \nof the form XINxi +(N+j)] and X[N~i +j], where N is the size of the first dimension. By symbolically \nevaluating the distance between the two references during reaching definition analysis of loop 1:, a \nreuse with distance 1 can be discovered. To discover recurrences that are due to an outer induction variable, \na separate analysis of the loop body is performed with respect to the corresponding outer loop. For example, \nthe recurrence among the references in statement (2) is determined during an analysis of 100p /j. During \nthe analysis of lj, induction variable i acts as a constan~ i.e., the linearized subscrip~ are interpreted \nas func\u00adtionsofj: Y[j +(Nxi+l)] and Y[ j +(Nxi 1)]. lj: do j:=l,UB, 1,: do i:=l,VBl (1) X[i+l,j]:=X[i,j]; \n (2) Y[i,j+l]:=Y[i,j-l]; (3) Z[i+l,j]:=Z[i,j-l];  enddo enddo Fig. 4. Multi-dimensional array references \nA separate analysis of each enclosing loop in a tight loop nest is capable of discovering recurrent access \npatterns that are due to a single induction variable. However, a recurrence that arises with respect \nto multiple induction variables simultane\u00adously, as the one among the two references in statement (3), \ncan\u00adnot be determined. Discovering these recumences requires a combmed analysis of the loop nest. The \ndistance information that is computed for each reference must be expanded to a vector of distance vahtes, \none for each induction variable of an enclos\u00ading loop. We are currently investigating how our framework \ncao be extended in thk way for the analysis of tight loop nests. 4. Applications This section dkcusses \napplications of the rmay analysis frame\u00adwork that cover a number of optimization problems for array references. \n  4.1. Register Allocation for Subscripted Variables Conventional register allocators typically ignore \nthe potential benefits of keeping array elements in registers for reuse. This is in part due to the fact \nthat standard data flow analysis tectilques used in register allocation are not expressive enough to \ncompute the live ranges of array elements. Consider, for example, the loop in Fig. 5 (i). Conventional \ncompilers typically generate load and store instructions for each reference to an array elemen~ as shown \nin the generated code in Fig. 5 (ii). However, the com\u00adputed values cars be preserved in registers for \nreuse, thereby avoiding memory load instructions, by allocating a register pipe\u00adlirse. do i=l,1000 (1) \nA[i+2]:=A[i]+X; enddo (i) load rX +-X load rl + A(2) rl+-1 load r2 +-A(l) L1: load r +-A(rI) load rX \ne X r + r+rX rI+-3 store A(rI+2) ~ r L2: rO + r2+rX rI + rI+l store A(rI) + rO ifrI<41000 goto LI rI \n+-rl+l r2e-rl rl+ rO if rIs 1000 goto L2 (ii) (iii) Fig. 5. A sample loop (i), the conventional code \ngenerated (ii), and the improved version using a register pipeline (rO, rl, r2) for elements of array \nA (iii). A register pipeline is a set of registers constituting the stages of the pipeline. A computed \nor loaded value enters the pipeline at the tirst stage and progresses through the pipeline, one stage \nper iteration. The code in Fig. 5 (iii) illustrates the use of a three-stage register pipeline (rO, rl, \nr2). In each iteration a value computed in statement (1) enters the pipe~me in stage 10 (instruction: \nrO -r2+rX ) and at the end of each iteration the values currently in the pipeline progress one stage \nfurther. With proper initialization of the pipeline, the values of the array ele\u00adments referenced by \nA[i] are always found in the third stage of the pipeline (i.e., in register r2) and memory load instructions \ninside the loop are entirely avoided. The major challenge in allocating register pipelirtes is the construction \nof live ranges for subscripted variables, as the enti\u00adties of the register assignment. Once live ranges \nfor both the scalar and subscripted variables have been determined it is pos\u00adsible to generalize traditional \nregister assignment strategies [4,5] to effectively include subscripted variables. We have developed \n4.1.2. Construction of the IRIG such an integrated register assignment strategy based on a regis\u00adter \nallocation tectilque using priority-based coloring to enable a fair and uniform competition of both classes \nof variables for the available registers [9]. The overall allocation task for a loop is achieved in four \nphases: (i) Live range analysis (ii) Construction of the integrated register interference graph  (iii) \nMulti-coloring (iv) Code generation First, the loop is analyzed to determine the live ranges of the scalar \nand subscripted variables in the loop. The live ranges of both classes of variables are represented in \na common graph: the integrated register interference graph. Based on this graph we generalize the conventional \npriority based coloring heuristic to a multi-coloring strategy. Finally, the code to implement register \npipelines is generated. We show how the developed framework is used for live range analysis, which presents \nan improvement over the analysis used in [9]. The remainder of thk section overviews the remain\u00ading three \nphases. Details of the integrated allocation technique are found in [9]. 4.1.1. Live Range Analysis The \nlive range of a variable value v starts at a point where v is generated (either be a definition or a \nuse site) and extends up to the last use of v. Live ranges of scalar variables are determined using conventional \nmethods [1]. To construct live ranges for array elements, the individual reuse points of a generated \nvalue must be identified. However, a generated value can only be preserved in a register for reuse if \nall instances of the generation site are available at the reuse point. This information is captured by \nan instance of our framework that computes a must problem: the &#38;available values inside a loop. This \nproblem is an exten\u00adsion of the classical scalar problem of computing available subexpression [1]. A \nvalue v of an array element e generated at @nt p is &#38;avai/able at point p if there is no redefinition \nof e along all paths leading to p that start at p and extend up to 5 iterations. The problem of computing \n&#38;available values is similar to the must-reaching definition analysis described in Section 3.5, except \nthat in addition to the definition sites of subscripted vari\u00adables, the use sites also act as generating \nreferences. To specify the framework for &#38;available values the parameters G and K are defined. G \n[n ] contains both the definitions and uses of subscripted variables that occur in node n and K [n ] \ncontains only the definitions. Based on G and K the flow functions associated with nodes in the loop \nflow graph are determined as described in Section 3.1 and the fixed point is computed over the equation \nsystem IN and OUT. The solution IN [n ,r ] = x at node n denotes that the value of the subscripted reference \nr is &#38;available at node n for dktance 5 where pr(r,n)<Fi9. Live ranges for subscripted variables \nare constructed by inspecting each node that contains a use site for the &#38;available values. Let r \n= X ~ (i)] be a reference that is &#38;available on entry of node n. A use r in node n reuses the value \ngenerated by reference r, if r is of the form r = X ~ (i 8 )] and pr (r ,n ) < 8 s 8. Reference r reuses \nthe value generated by r with the constant iteration distance &#38; . By collecting the indivi\u00addual reuse \npoints in a live range in this fashion, the complete ranges inside the loop are determined. Traditionally, \nthe problem of assigning registers to live ranges is formulated as the problem of k-coloring the register \ninterference graph [1], where k is the number of available registers. The nodes in this graph represent \nthe live ranges of variables in the loop. Two nodes are connected by an edge, i.e., itierfere, if the \ncorresponding live ranges overlap and cannot be assigned the same register. Live ranges are assigned \npriorities that express the benefits of keeping the corresponding variables in registers. Registers are \nassigned to live ranges by coloring the correspond\u00ading nodes in the graph based on their priorities. \nWe extend the traditional structure of the register interfer\u00adence graph to represent live ranges of subscripted \nvariables as well as scrdar live ranges. The resulting graph is called the integrated register interference \ngraph (IRIG). A priority function P is defined over the set of live ranges represented by the nodes in \nthe IRIG. The priority of a live range 1 is based on the resource requirements of 1, which are expressed \nby a second function depth. Depth (1) is the depth of the register pipeline that is needed to preserve \nthe generated values in 1. Let 50(1) denote the iteration distance between the generation point and the \nlatest reuse point in 1, then 1 if 1 is scalar ffeplh (z ) = ~o(~)+1 otherwise { The priori~ P(1) expresses \na savings/costs ratio of allocating registers to live range 1. The savings result from memory load instructions \nthat are avoided normalized with respect to the length of the live range, so as to favor live ranges \nthat require the register resources for only short periods. Priorities are deter\u00admined uniformly over \nthe scalar and subscripted live ranges by calculating the utilization of each required register. Let \nCm be the average cost of executing a memory load instnsction, access (1) the number of reuse points \nin 1, and let 11I be the length of 1. The priority P (1) is computed as: [access (1) -1]XCm P(I)= Ill \nxdepth (l)  4.1.3. Multi-Coloring Register pipelines are assigned to live ranges by multi-coloring the \nIRIG based on the calculated priorities. In the standard prioritybased coloring strategy for scalars \n[5], the coloring of nodes that have fewer interferences (i.e., neighbors in the graph) than available \nregisters is postponed knowing that these nodes can always be colored. These nodes are called the unconstrained \nnodes. A node that has more interferences than available regis\u00adters is called a constrained node. The \nunstrained nodes Me sulit. .. creating two or more nodes with fewer neighbors. This process continues \nuntil there are no more constrained nodes in the graph and all nodes can be assigned a color. To adapt \nthis coloring strategy to the IRIG, it must be acknowledged that nodes in the IRIG that represent subscripted \nlive ranges may require more than one register (i.e., more than one color). Specifically, to determine \nwhether a node is an unconstrained node, the possibly varying register requirements of the neighbor nodes \nas well as the register requirements of the node itself must be taken into account. A node n in the IRIG \nis an unconstrained node if depth (n) + ~ depth (m )s k. m nneightor If the above inequality holds, there \nwill always be depth(n) colors to multi-color node n. The remainder of the coloring pro\u00adcedure is essentially \nunchanged and details are found in [9]. 4,1.4. Code Generation The use of a register pipeline involves \nthree phases. First, the pipeliie is initialized prior to entering the loop. Let 1 be a live range and \nlet the generating reference of 1 be of the form X ~ (i )]. The pipeline stages for 1 are initialized \nby generating for each stage rj, where j = 1, ..., depih (1) 1, a load instruc\u00ad tion load rj ~ X ~ (1-j \n)]. The pipeline is used inside the loop by replacing each reuse point with a reference X ~ (i 6)] by \nart access to pipelhte stage r8. A load may only be needed for the generating reference in 1, i.e., if \nthe generating reference is a use. The final phase of using a pipeline consists of implement\u00ading the \npipdine progression at the end of the loop body. A value located in stage rj progresses to stage rj+l \nfor 1 s j <d l for use in the next iteratton. Note that physically moving values among the stages of \nthe pipeline is not necessary if the loop is urtrol~ed depth (1) times. However, loop unrolling has the \ndisadvantage of increasing the number of instruction in the loop body. If the unrolled loop body is too \nlarge for the instruction cache, more additional memory accesses may be introduced than saved by the \nregister assignment. one way to implement the pipeline progression at the end of each iteration of the \noriginal loop body is to generate depth (1) 1 register-to-register move instruction rj + rj.l. If these \ninstructions cannot be executed concurrently with other operations in the loop, the pipeline progression \nhas an overhead that is absent if register pipelines are not used. For large pipe\u00adlines, this overhead \nmay undo some of the savings gained by using the pipeline. Thk problem of potential overallocation can \nbe avoided by incorporating the costs of the pipeliie progression as a negative factor into the priority \ncalculation. Alternatively, hardware support may be exploited to per\u00adform the pipeline progression at \nconstant costs. The Cydra 5 archhecture [7] provides iteration control pointer (ICP) that is implicitly \naddressed by each register reference. By appropriate ely updating the ICP at the end of each iteration, \nthis architecture cart be used to implement the pipeline progression as a register windowing scheme. \nAnother hardware solution provides a con\u00adcurrent register-to-register move instruction, that performs \nthe individual move instructions simultaneously provided the pipe\u00adline consists of a set of consecutive \nregisters. 4.2. Load/Store Related Optimization The data flow framework supports the extension of traditicmaf \noptirnizations to subscripted variables. As an exarn~le, thk sec\u00adtion presents load/store optimization \nin relation to kntbscripted references. 4.2.1. Eliminating Redundant Stores A store (i.e., a definition \nsite) of an array element is redundant if, on all paths, it is followed by another store to the same \narray ele\u00adment without an intermediate use of the stored value. Fig. 6 shows an example of a redundant \nstore that arises across one iteration (i) and the transformed loop with the redundancy elim\u00adinated (ii). \nThe data flow problem of detecting redundant stores is a generalization of the scalar problem of determining \nvery busy expressions [1] applied to definitions of subscripted variables. We refer to this problem as \ncomputing the &#38;busy stores. A store s at point p is &#38;busy at point p , if the store is executed \nwithout a preceding use of the stored array ele\u00adment along all paths leading to p that start at p and \nextend up to 6 iterations. do i=l,1000 do i=l,999 A[i]: =...; {store) A[i]:= ...; {1-redund. store} if \ncord then ..,; ~~cond then A[i+l]:...; enddo enddo A[1OOO]:=...; ifcond then AIIOOI]:=...; (i) (ii) \nFig. 6. Example of a l-redundant store across one iteration (i) and the transformed loops with the redundancy \neliminated (ii). Like the problem of computing very busy expressions, determin\u00ading &#38;busy stores is \na musr-problem and a backward problem. The candidates for redundant stores we the subscript expressions \nthat occur in definition sites, i.e., G is the set of textually distinct subscript expressions in definition \nsites. A busy store is killed when it meets a use of the same array element. Thus, the set K consists \nof the uses of subscripted variables in the loop. The fixed point solution to &#38;busy stores determines \nat each node n for each store s c G a lattice value IN [n ,s ] = x denoting that stores is &#38;busy \non exit of node n forpr (s ,n )s 65 Xt. The solution to &#38;busy stores is used to detect opporturti\u00adties \nfor eliminating redundant stores. Lets = X ~ (i )] be a store at a node n. The store s at node n is &#38;redundant \n(i.e., redun\u00addant with iteration distance 6), if there is another store s = X ~ (i -6)] in the loop and \ns is &#38;busy at node n, i.e., IN [n ,s ] = &#38; Thus, a &#38;redundant store is followed by another \nstore to the same array element 5 iterations later. It follows that a &#38;redundant store is redundant \nin all but the finaf 6 loop itera\u00adtions. Hence, the store can be eliminated from rdl but the final 8 \niterations. This is achieved by removing the store from the loop and by unpeeling the final 5 loop iterations \nas shown in Fig. 6 (ii).   4.2.2. Eliminating Redundant Loads A load of an array element e is redundant, \nif, on all paths, it is preceded by a statement that either loads or defines the same array element without \nart intermediate redefinition. The problem of detecting and eliminating redundant loads is a special \ncase of the register allocation problem for subscripted variables described in Section 4.1. Thus, the \nsame framework instance for computing the fiavailable values is used. Using this informa\u00adtion, redundant \nloads carI be eliminated in a similar way as avail\u00adable subexpressions are eliminated for scalars [1]. \nThus, for each redundancy a scalar temporary is created that can be held in a register to avoid memory \naccesses. Fig. 7 shows an example of a l-redundant load and i~ removal. do i=l,1000 t:=A[l]; {1-redund. \nload) do I=l,IGOO tfcorui then ....=A[i]. if cond then ....=t. A[i+l]:=...; {store) A[i+l]:=...; enddo \nt:=A[i-tl]; enddo (i) (ii) Fig. 7. Example of a l-redundant load (i) and the transformed loops with the \nredundancy eliminated (ii). t Recall, hat, in a backward problem, IN denotes node exit information. \n 4.3. Controlled Loop Unrolling When wmpiling a loop body for execution on a fine-grained parallel \narchitecture, it maybe beneficial to unroll the loop [8]. The original loop body may not have enough \nparallelism to exploit the available architecture. Sufficient parallelism could then be provided by the \nlarger loop body that is obtained by unrolling. However, loop unrolling may also impact the code in a \nnegative way. Loop unrolling leads to an increase in code size and to an increase in register pressure \ndue to longer live ranges in the unrolled loop body, Moreover, loop unrolling should only be performed \nif more parallelism is actually created. There may be loop-carried dependencies in the original loop \nthat become loop-independent in the unrolled loop, and thus may prevent an increase in parallelism. Ideally, \nthe effects of loop unrolling are predicted in advance, so as to avoid this transformation if no sufficient \nimprovements of the code can be achieved. Clearly, to be of use, such a prediction must be obtainable \nefficiently. We describe in this section, how the developed array analysis frame\u00adwork can be used to \napproximate the impact of loop umolling on the parallelism in the loop body. A similar strategy may be \nused to predict the effect of loop unrolling on the register pressure in the loop. To measure the amount \nof parallelism consider the length 1 of the critical path (i.e., the longest chain) in the dependence \ngraph for the loop body. The length lU~Otl of the critical path in the unrolled loop is 1< lUWOIIs 2 \nx 1, where lUWOI1=1 if there are no loop-camied dependencies with distance 1, and lM~Oll= 2 x 1 if there \nis a dependence from the last statement in the chain in one iteration to the first statement in the chain \nin the next iteration. A strategy to control loop unrolling based on the advance calculation of the critical \npath length lW,Oll was described for region scheduling [13]. In this strategy, unrolling is performed \nincrementally under the assumption that all relevant dependence information is available. During each \nstep the length /uWOll is calculated from information about loop-carried dependencies with dktance 1. \nOnly if 1~,011is below a certain threshold value ~, where 1 s ~ <2 x 1 is unrolling actually performed. \nThis pro\u00ad cess continues until either sufficient parallelism has been created or until no more usable \nparallelism is created by further tmrol\u00ad ling the loop. For such a controlled loop unrolling strategy \nto be efficient, it is critical that information about loop-cwried depen\u00addence with distance 1 can quickly \nbe made available during each step. We briefly show how our framework can be used to compute this dependence \ninformation. There are three types of loop-carried dependencies: flow dependencies, anti-dependencies, \nand output dependencies [17]. Flow and anti-dependencies can be detected based on reaching definition \ninformation. To include the detection of anti\u00addependencies the uses of subscripted variables are propagated \nin addition to the definitions. The data flow analysis for dependence detection computes the &#38;reaching \nreferences. A reference r at point p is S-reaching at point p if there is a path from p to p along which \nthe array element referenced by r is not redefined. The framework for &#38;reaching references is specified \nby the set G [n ] containing the definitions and uses of subscripted vari\u00adables, and by the set K [n \n] that contains only the definitions in node n. Reaching reference analysis is a may-problem, i e., potential \ndependencies are discovered. The fixed point solution IN [n ,r ] describes at each node n and for each \nreference r e G the maximal iteration distance up to which reference r reaches the entry of node n. Dependencies \nare determined by examining the computed reaching information at each node Let r ~= X ~ ~(i)] be a refer\u00adence \nin node n, and let r ~ = X ~ Ji )] be another reference that reaches node n up to distance &#38; A dependence \nfrom r ~ to rz exists if 38 30S5 Sfi: ~i=l: fl(i)=fJi -5 ). A conservative test of the above condition \nproceeds in a similar way as described in Section 3.1.2 for the definition of preserve functions. Let \nfio be the smallest value for 3 for which the two references may overlap. A dependence from r ~ to r \n~ with itera\u00adtion distance &#38; is determined. Instances of r ~ and rz with an iteration distance less \nthan &#38; are dependence-free. Finally, the type of dependence between the two references, i.e., flow, \nanti-, or output dependence, merely depends on whether r ~ and r ~ are definitions or uses of subscripted \nvariables. 5. Related Work Previous extensions to the scalar data flow framework to devise a flow-sensitive \nanalysis of array references can be categorized into one of two approaches. The target of frameworks \nin the tirst approach is to improve the accuracy of wnventional data depen\u00addence information and the \nframeworks operate on array regions. The target of the second approach, and the approach taken in this \nwork is to provide the data flow information needed in fine\u00adgrained optimization of individual array \nreferences. Therefore, the framework operates on individual reference instances. Both approaches share \nrhe advantage that data flow frameworks can handle conditional control flow inside loops. Techniques \nthat follow the first approach include the frameworks described by Gross and Steenkiste [12], Granston \nand Veidenbaum [1 1], Rosene [24], and Hanxleden et al [14]. In these techniques, the array regions accessed \nby an individual array reference are locally approximated by some form of area descriptor. The locally \ndetermined summary information is glo\u00adbally propagated during an interval-based or iterative data flow \ncomputation. The accuracy as well as the wsts of these tech\u00adniques depends on the choice of representation \nfor the accessed array region and the resulting complexity of the meet operation. Although it is in general \npossible to extract from the access sum\u00admaries the instance information that is of interest for fine-grained \noptimization, our framework shows that this information-can be computed directly in a more efficient \nway. An array analysis framework that follows the second approach and operates on reference instances \nas opposed to access summaries was pro\u00adposed by Rau [22]. This analysis propagates textual names of referenced \narray elements throughout the program. Unlike the analysis presented in this paper, the number of iterations \nover the program is in general unbounded and is thus, in practice, limited by a chosen upper bound resulting \nin a limited amount of irtfor\u00admation. In contrast to our analysis, this technique would fail to recognize \nrecurrent access patterns in all-paths problems if they arise only after some initial start-up iterations \nof an enclosing loop. Other work on the anatysis of array references is targeted at improving the accuracy \nof conventional data dependence tests used in parallelization. By incorporating flow information as an \naddhional wnstraint on the presence of a dependence, false dependence repmts are avoided. These techniques \nprovide more accurate data dependence information at the wsts of additional complexity, which is acceptable \nin relation to the performance improvemen~ of loop parallelizar.ion. A general framework for obtaining \nflow-sensitive data dependence information by inwr\u00adporating sequencing information was presented by Feautrier \n[10]. Other work to obtain flow-sensitive data dependence infor\u00admation includes the techniques by Maydan \net al [19], Pugh and Wormawtt [21], Brandes [2], Ribas [23], and by Kallis and Klappholz [15]. Some research \nspecifically addresses the exploitation of reuse opportunities for individual array elements. Scalar \nreplacement [3] improves register allocation for array elements by introducing new scalar temporaries \nfor each dependence in a similar way as we have described for eliminating redundant loads. This method \nis based on conventional data dependence information and thus may miss reuse opportunities in the pres\u00adence \nof conditional control flow. Register allocation for ar~ay elements across loop iterations in the context \nof software pipe\u00adlined loops is described in [20]. 6. Conclusions In this paper we addressed the problem \nof efficiently and effec\u00adtively analyzing array references to provide the information needed for various \noptimization problems targeted at sequential or fine-grained parallel tmchitectures. A data flow framework \nwas presented that extends the traditional scalar framework to analysis of subscripted variables by incorporating \niteration dis\u00adtances. The framework models the detection of recurrent access patterns among subscripted \nreferences that arise during the exe\u00adcution of loops that may contain condhionals. Analyses phrased in \nthis framework are practical in that they require only a small number of passes over a loop body. This \nefficiency makes the analyses attractive for use in optimizing compilers for sequential or fine-grained \nparallel architecture. Applications of the frame\u00adwork were demonstrated in various optimizations of indhridkl \narray references. The framework, as stated in this paper, models the analysis of a single loop as the \nprimary source for fine-grafined optimizations. We are currently investigating how the frame\u00adwork can \nbe extended to the analysis of tight loop nests to pro\u00advide information about recurrent access patterns \nthat arise simul\u00adtaneously with respect to multiple enclosing loops. Acknowledgements We thank the referees \nfor their comments which helped in improving this paper. References 1. A. V. Aho, R. Sethi, and J. D. \nUlhnan, in Compilers, principles, techniques, and tools, Addison-Wesley Pub\u00adlishing Company, Massachusetts, \n1986. 2. T. Brandes, The importance of direct dependence for automatic parallelism, Proc. of the 88 \nInt. Conf. on Supercomputing, pp. 407-424, 1988. 3. D. Callahan, S. Carr, and K. Kennedy, Improving \nregis\u00adter allocation for subscripted variables, Proc. of the ACM SIGPLAN 90 Cor$ Programming Language \nDesign and Implementation, pp. 53-65, htn. 1990. 4. G. J. Chaitin, Register allocation and spilling \nvia graph coloring, Proc. of the ACM SIGPLAN 82 Symp. on Compiler Construction, pp. 201-207, Jun. 1982. \n 5. F. Chow and J. Hennessy, Register allocation by priority-based coloring, ACM SIGPLAN Notices, vol. \n19, no. 6, pp. 222-232, 19S4, 6. P. Cousot and R. Cousot, Abstract interpretation: a unified lattice \nmodel for static analysis of programs by consmction or approximation of fixpoints, Conf. Record of the \n4th Annual ACM Symp. on Principles of Programming Languages, pp. 238-252, Jan. 1977. 7. J.C. Dehner~ \nP.Y.-T. Hsu, and J.P. Brat4 Overlapped loop support in the Cydra 5, Prwc. of the 3rd Int. Corf on Architectural \nSupport for Programming Languages and Operating Systems., pp. 26-39, Apr. 1989.  8. J.J. Dongarra and \nA.R. Hinds, Unrolling loops in FOR-TRAN, So@rre-Practice and Experience, vol. 9, no. 3, Mar. 1979. 9. \nE. Duestcrwald, R. Gupta, and M.L. Soffa, Register pipeliig: An integrated approach to register allocation \nfor scalar and subscripted variables, Proc. of the 4th Int. Workshop on Compiler Construction, LAJCS \n641 Springer Verlag, pp. 192-206, Oct. 1992. 10. P. Feautrier, Data flow analysis of array and scalar \nreferences, Int. Journal of Parallel Programming, vol. 20, no. 1, 1991. 11. E. Granston and A. Veidenbaum, \nDetecting redundant accesses to wray data, Proc. of Supercomputing 91, pp. 854-865, NOV. 1991. 12. T. \nGross and P. Steenkiste, Sh-uctured dataflow analysis for arrays and its use in an optimizing compiler, \nSoftware -Practice and Experience, vol. 20, no. 2, pp. 133-155, Feb. 1990. 13. R. Gupta and M.L. Soff&#38; \nRegion scheduling: an approach for detecting and redistributing parallelism, IEEE Trans. on Sojhvare \nEngineering, vol. 16, no. 4, pp. 421-431, Apr. 1990. 14. R. Hanxleden, K. Kennedy, C. Koelbel, R. Das, \nand J. Saltz, Complier analysis for irregular problems in For\u00adtran D, 5th Workshop on Languages and Compilers \nfor Parallel Computing, pp. 65-77, Aug. 1992. 15. A. D. Kallis and D. Klappholz, Reaching definitions \nanalysis on code containing array references, Conf. Rec. of the 4th Worhhop on Languages and Compilers \nfor Parallel Computing, Aug. 1991. 16. J.B. Karn and J.D. Unman, Monotone data flow analysis frameworks, \nActs Irt$ormatica, vol. 7, no. 3, pp. 305\u00ad317, Jul. 1977. 17. D.J. Kuck, R.H. Kuhn, D. Padua, B.R, Leisure, \nand M. Wolfe, Dependence graphs and compiler optimization, Proc. of the 8th ACM Symp. on Principles of \nProgram\u00adming Languages, pp. 207-218, Jan. 1981. 18. T.J. Marlowe and B.G. Ryder, Properties of data \nflow frameworks, a unified model, Acts Informatica, vol. 28, no. 2, pp. 121-163, Dec. 1990. 19. D.E. \nMaydan, S.P, Amarasinghe, and M.S. Lam, Array data-flow analysis and its use in array privatization, \nProc. of the 20th ACM Symp.on Principles of Program\u00adming Languages, pp. 2-15, Jan. 1993. 20. Q. Ning \nand G.R. Gao, A novel framework of register allocation for software pipelining, Proc. of the 20th ACM \nSymp. on Principles of Programming Languages, pp. 29-42, Jan. 1993. 21. W. Pugh and D. Wonnacott, Eliminating \nfalse data dependence using the omega test, Proc. of the SIG-PLAN 92 Conf. on Programming Language Design \nand Implementation, pp. 140-151, Jun. 1992. 22. B. R. Rau, Data flow and dependence analysis for instruction-level \nparallelism, Conf Rec. of the 4th Workshop on Languages and Compilers for Parallel Computing, Aug. 1991. \n 23. H. Ribas, Obtaining depedence vectors for nested-loop computations, Proc. of the 1990 Int. Conf. \non Parallel Processing, pp. 212-219 (IJ), 1990. 24. C. Rosene, Incremental dependence analysis, Ph.D. \nthesis, Rice University, March 1990.  25, M. Wolfe and U. Banerjee, ~ Data dependence ~d ib application \nto parallel processing, Int. Journal of l aml\u00adlel Programming, vol. 16, no. 2, pp. 137-178, 1987. 26. \nM. Wolfe, Optimizing supercompilers for supercomput\u00aders, Pitman Publishing Company, London, MIT Press, \nCambridge, Massachusetts, 1989. \n\t\t\t", "proc_id": "155090", "abstract": "<p>Data flow analysis techniques have traditionally been restricted to the analysis of scalar variables. This retriction, however, imposes a limitation on the kinds of optimizations that can be performed in loops containing array references. We present a data flow framework for array reference analysis that provides the information needed in various optimizations targeted at sequential or fine-grained parallel architectures. The framework extends the traditional scalar framework by incorporating iteration distance values into the analysis to qualify the computed data flow solution during the fixed point iteration. Analyses phrased in this framework are capable of discovering recurrent access patterns among array references that evolve during the execution of a loop. Applications of our  framework are discussed for register allocation, load/store optimizations, and controlled loop unrolling.</p>", "authors": [{"name": "Evelyn Duesterwald", "author_profile_id": "81100028046", "affiliation": "", "person_id": "PP39023899", "email_address": "", "orcid_id": ""}, {"name": "Rajiv Gupta", "author_profile_id": "81100027751", "affiliation": "", "person_id": "PP39077357", "email_address": "", "orcid_id": ""}, {"name": "Mary Lou Soffa", "author_profile_id": "81452611636", "affiliation": "", "person_id": "PP39083704", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/155090.155097", "year": "1993", "article_id": "155097", "conference": "PLDI", "title": "A practical data flow framework for array reference analysis and its use in optimizations", "url": "http://dl.acm.org/citation.cfm?id=155097"}