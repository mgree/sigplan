{"article_publication_date": "06-01-1993", "fulltext": "\n First-class Data-type Representations in SCHEMEXEROX iVorman Adams, Pavel Curtis, Alike Spreitzer Norman, \nXerox PARC 3333 Coyote Hill Rd. l%do Alto, CA 94304 Pavel, ~preitzer@PARC. Xerox. Corn Abstract In most \nprogramming language implementations, thecom\u00adpiler has detailed knowledge of the representations of and \noperations on primitive data types and data-type construc\u00adtors. In SCHEMEXEROX, this knowledge is almost \nentirely external to the compiler, in ordinary, procedural user code. The primitive representations and \noperations are embodied in first-class representation types that are constructed and implemented in an \nabstract and high-level fashion. Despite this abstractness, a few generally-useful optimizing transfor\u00admations \nare sufficient to allow the SCRE~XEROX compiler to generate efficient code for the primitive operations, \nes\u00adsentially as good as could be achieved using more contorted, traditional techniques. 1 Introduction \nand Motivation Typically, the compiler for a given programming language embodies detailed knowledge of \nthe syntax and semantics of that language s data-type specifications. This knowl\u00adedge includes, for example, \nalgorithms for bit-level layout of data-type instances, the object-code implementations of the primitive \naccess, modification, and allocation operations, and (in languages with run-time type checking) the runtime \nsystem s protocols for type testing and new type creation. Putting all of this knowledge into the compiler \nallows it to more easily generate efficient object code for manipulating values. There are, however, \na number of drawbacks to placing the type representation knowledge in the compiler. First, code in the \ncompiler must necessarily be rneta\u00adcode , in the sense that it does not directly perform the operations \nin question but rather generates code that will perform the operations. This level of conceptual indirection \ncan make such code more difficult to write, to understand, and to test. Second, when the knowledge is \nembedded in the com\u00adpiler, it is difficult or impossible for users to experiment with variations of that \nknowledge, such as new styles of data rep\u00adresentation or layout. Finally, in languages like Scheme, it \ncan be very difficult for compilers to discover sufficient information to generate Permission to copy \nwithout fee all or part of this materinl is granted provided that the copies are not made or dietributedl \nfor direct commercial advantage, the ACM copyright notice and the titla of the publication and Its date \nappaar, and notice is given that copying is by permission of the Association for Computing Machinery. \nTo copy otherwise, or to republish, raquires a fae end/or specific permission. ACM-SlGPLAN-PLDl-6 /93/Albuquerque, \nN.M. ~ 1993 ACM 0-89791-598-419310006101 39...$1.50 good code; the problem is that user-defined data \ntypes are created by procedural manipulation of first-class type val\u00adues rather than by static program \nsyntax [1, 2]. The usual solution is to give the compiler detailed knowledge of the built-in types and \noperations (e.g., cons, pair?, car, and act-car ! in Scheme) and to let all of the implementation of \nuser-defined types take place in essentially unoptimized run-time code. In such systems, operations on \nuser-defined types can be significantly less efficient than thoee on built-in types. In SCHEMEXEROX, \nwe ve taken a new approach in which we place even more of the data representation knowledge in procedural \nuser code. In fact, essentially all of the support for data representation, including that for all of \nthe built-in types except procedures, is implemented in normal run-time code. New types are defined by \ncomposing first-class type and layout values procedurally and then extracting, from these first-class \ntypes , procedures for allocating and ma\u00adnipulating instances of the new types. We then rely only upon \nthe compiler s general-purpose transformations and optimizations to make uses of the type system efficient. \nBecause there is a great deal of regularity to the con\u00adstruction of new types and the extraction of the \ntype-specific procedures, SCHEMEXEROX provides a concise syntactic ex\u00adtension for the purpose. Most programmers \nwill use the type system through this synt attic extension, rather than proce\u00addural interface introduced \nlater. As examples demonstrating most of its features, here are the SCHEME XEROX definitions of three \nstandard Scheme data types:l (define-type pair (field car (accesaor car) (modifier act-car ! ) ) (field \ncdr (acceaaor cdr) (modifier aet-cdr!)) (constructor (cons car cdr)) (tag 2)) (define-type string (sequence \ndata (field elt (type char) (acceasor string-ref) (modifier string-set!)) (length atring-length) )) lFor \nbrevity, these definitions are slightly simplified from the ones used in our system; for example, the \nlatter include clauses defining the specialized value-printing procedure8 for the types. (define-type \nchar (field tag (type (unsigned 15)) (constant ttx4FF)) (field code (type (unsigned 16)) (accessor char->integer)) \n(constructor (integer->char code)) (immediate)) In SCHEMEXEROX, pairs are represented by tagged pointers \nto two-word cells, strings are tagged pointers to atypecode word followed by a length word and some number \nof8-bit character codes (there is no (tag . . . ) clause in the def\u00adinition because all coded types share \na single tag), and characters are tagged words containing an 8-or 16-bit code. Even though this syntax \nis declarative, the reader should keep in mind that it is not perceived as such by the SCHEME-XEROX compfier; \nuses of define-type are simply expanded into a series of definitions invoking the procedural interface. \nIn the remainder of this paper, we briefly describe some salient Scheme language extensions in SCHEMEXEROX, \ndis\u00adcuss the procedural interface to the first-class types imple\u00admentation, and explain how the SCHEMEXEROX \ncompiler s general-purpose code transformations are sufficient to gen\u00aderate highly-efficient code for \nuses of that interface. 2 Some Extensions to Scheme in SCHEMEXEROX The SCHEmXEROX programming language \nincludes a num\u00adber of extensions to standard Scheme, two of which are rel\u00adevant here. Very much in the \nstyle of languages like Modula, ML, Ada, or Cedar/Mesa, SCErEMEXEROX programs are struc\u00adtured into lexically-isolated \nmodules that export implemen\u00adt ations of variables described in textually-separate :nter\u00adfaces. Curtis \nand Rauen describe the module system in detail [3], but for the purposes of this paper it is enough to \nknow two facts. All inter-module references are made via qualified names like at ack#push !, in which \nstack is the name of an interface and push! is the name of a variable described in that interface and \nimplemented in some un\u00adspecified other module. Within a module, standard Scheme definition syntax is \nused to define variables that are local to the module and inaccessible from without; exported vari\u00adables \nare defined with a similar syntax, but using the key\u00adword public instead of define, In standard Scheme, \nprocedures that accept a variable number of arguments are defined using a rest parameter after all of \nthe parameters corresponding to required argu\u00adments; that parameter is bound on invocation to a list \nof any extra arguments to the call. As an alternative to this, SCFIEMEXEROX offers a lambda syntax in \nwhich two special parameters are specified after the keyword others ; the first is bound to a function \nmapping a non-negative argu\u00adment index into the corresponding extra argument, and the second is bound \nto a natural number specifying how many extra arguments were provided. This procedural style is frequently \nmore convenient for the programmer and almost always more easily optimized by the compiler. 3 The Procedural \nInterface to Type Definition Uses of the define-type form shown in the introduction expand into code \nthat creates and uses first-class values that describe the types representations. For example, (define \npair-type (type#make-tagged 2 (layout#make-product (layout #make-value car) (layout#make-value cdr))) \n(define pair? (type#predicate pair-type)) (define car (typet$accessor pair-type car)) This code defines \npair-type to be a Type. A Type is a first-class value that describes a representation. From a Type, one \ncan extract a basic constructor, a printer, a membership predicate, and accessor, modifier, and address\u00adtaking \nfunctions for fields. The description of representations is organized into four levels ofabstraction, \ncded Bits, Structure, Layout, and Type (in increasing order). I Type: SCHEMEXEROX conventions 1 I Layout: \nnaming, converting eltreps, curried access 1 ! Structure: hierarchical structure, indexed by integers \n1 Bits: read and write memory Each level of abstraction is object-oriented, and centers on one object \ntype; there is a SCHEMEXEROX interface for each level. The Type and Layout provide the public interface \nto the type system; the Structure and Bits are internal to the implement ation. A Bits object stands \nfor a sequence of bits. A Struc\u00adture object stands for anode in a hierarchical structure im\u00adposed on \na bit sequence, d la Queinnec [5, 6]. A Layout object also stands for a node in a hierarchical structure; \na Layout object differs from a Structure object in: (l) intro\u00adducing naming, (2) converting element representations, \nand (3) organizing the access of atomic elements into one name\u00adoriented and one subscript-oriented step. \nA Type object stands for a primitive Scheme data type and its representa\u00adtion in SCHEMEXEROX; a Type \ninitiates and terminates the recursions involving Layouts and Structures, and specifies the details of \nhow memory is allocated and how SCHEME-XEROX organizes the space of representations. To define a new \nrepresentation, one first constructs a Layout for the in\u00adternal structure, and then a Type from that \nLayout. Then the methods of the Type are invoked to produce the predi\u00adcate, the accessor functions, and \nso on. In the interest of brevity, the following presentation omits some details concerned with error-handling \nand advanced features; the omitted details introduce no new concepts. A Bits object stands for a sequence \nof bits, either a con\u00adstant word-long sequence (a direct Bits) or a sequence of mutable bits starting \nat some memory address (an Mirect Bits). There aretwoways to create a Bits: make direct: (v: Uord) -+B \nmake-indirect: (addr:Addr, fl:Int) +B The first takes a machhe word;z the second takes a starting address \n(in the machine s native format) and an offset (ns a Scheme integer) to subtract from the starting address. \n Each Bits has methods to read and write subsequences of those bits. The following operations are available \non a Bits: read (b: B, 8: Int, length: Int, signed: Int) ~ Uord write: (b: B, 6: Int, length: Int, data: \nWord) -B is-direct? (B) + Bool The read operation takes a Bits and three more arguments: (1) an offset \n6 where the subsequence begins, (2) the length of the subsequence, and (3) an indication of whether to \nsign\u00adextend the subsequence. The read operation returns the indicated subsequence of bits in the low-order \nposition of the result word. The m-it e operation takes an offset and length indicating a subsequence, \nand a word carrying mew bits for the indicated subsequence. A direct Bits returns a new direct Bits that \ndiffers by the indicated alteration; an indirect bits makes the indicated alteration as a side\u00adeffect, \nand ret urns an uninteresting value. Because the read and writ e operations use machine words to carry \nthe su~bse\u00adquences read or written, such subsequences cannot be longer than a machine word. A Structure \nobject stands for a node in a hierarchical structure imposed on a bit sequence, d la Queinnec [5, 6]. \nEach Structure is either atomic or composed of a number of other Structures, indexed by integers starting \nwith O. We currently implement composite Structures for records and fixed-and variable-length arrays; \nvariant-record struc\u00adture could be added easily within the existing framework. The following constant \nand operations are used to construct Structures: empty S rnake-atoix (width: Int, signed. Bool) + S make-product: \n(sl: S, .92: S) + S rnake-powe~ (base: S, counti Int) + S raake-sequence (ba.$e: S) + S Hake-atonr makes \natomic Structures, make-product makes composite Structures with two components, andrnake-power and make-sequence \nmake fixed-and variable-length array Structures, respectively. Each Structure supports the following \noperations: skip. (tr:S, &#38;B,6:Int)+ Int constant-size?(S) +Bool init:(s: S, bl:B,8: Int, is: Int*)+ \n(b2:B, is : Int~f) allot-siz= (s: S, lengths: Int*) + (size: Int, /engths : 1A*) Some of the operations \non a Structure ,. oDerate on an actual . instance of such a structure, passed ss the Structure plus a \nBits and an offset into that Bits where the instance begins. skip is such a method, and returns the number \nof bits oc\u00adcupied by the instance. Structures are classified as either constant-size or variable-size. \nAll instances of a constant\u00adsize Structure have the same size; variable-size Structures have no such \nguarantee. For example, a fixed-length a,llay 21n our system, a11Scheme values are represented by a single \nma\u00adchine word (wbich may, of course, encode an address of wheretbe rep\u00adresentation continues). Each procedure \nargument or result is passed as a single word. In the lower levels of the type representation sys\u00adtem, \nwe sometimes pass words as arguments or results that are not interpreted as Scheme values, but es machine \nwords. of constant-size elements is itself constant-size; a variable\u00adlength array (called a sequence) \nis variable-size. When a variable-size structure is first instantiated, it may need to be initialized \nthis involves setting some of its bits a cer\u00adtain way (e.g., storing the length of a sequence). Constant\u00adsize \nStructures need not be initialized. The init method is present in every Structure; it takes an instance \nand a se\u00adquence of integers3 called an instance speci$cation. Init consumes some (possibly empty) prefix \nof the sequence in the course of initializing the instance; this includes recur\u00adsively initializing all \nthe components of the instance. Init returns two values: (1) either the newly-initialized Bits, if direct, \nor an uninteresting value, if the Bits is indirect, and (2) the unconsumed tail of the instance specification. \nThe allot-s ize method takes an inst ante specification, but no instance, and returns the number of bits \nthat would be oc\u00adcupied by such an instance, plus the un-consumed tail of the instance specification. \nSome Structures also support some of the following operations: read (s: S, b: B, 6: Int) + Herd write: \n(s: S, b: B, 6: Int, data: Word) + B count: (s: S, b: B, 6: Int) + Int offset: (s: S, b: B, 6: Int, index: \nInt) + Int An atomic Structure can be read and written. A sequence also can be read; the result is the \n(machine representation of) the length of the sequence. A composite Structure can count its components. \nA non-empty composite Structure can compute the offset where the component with a given index begins. \nA Lavout obiect also stands for a node in a hierarchi\u00adcal struc~ure. Ii fact, the Layout hierarchy has \nthe same structure as the Structure hierarchy. A Layout object dif\u00adfers from a Structure object in: (1) \nintroducing naming, (2) converting element representations, and (3) organizing the access of atomic elements \n(called fields) into two steps: first one that is based on naming and normally involves literals at compile \ntime, and then one that is based on subscripting and normally manipulates variables whose values are \nnot known until run time. The following constant and operations are used to con\u00adstruct Layouts: empty \nL make-f ielct (name: Sym, width: Int, signed: Bool, insert: (Val) + Word, extract: (Uord) ~ Val) -L \nmake-value (name: Sym) ~ L make-boole&#38; (name: Sym) + L ... make-product (1I: L, 12: L) + L make-~ower \n(base: L; cou~t: Int) + L make-sequence (base: L, name: Sym) + L make-f ield t akes a name (represented \nby a Scheme symbol), the atomic Structure parameters (width, signed), and a pair of functions (insert \nand extract) used to convert between the SCHEMEXEROX representation for an element and the representation \nused for that element in the Layout. Layouts can thus use compact representations in their storage. For \nexample, a string could use a Layout that is a sequence of 8-bit fields. 3 The sequence is actually passed \nas two arguments: a length and a fetch-function. There are several functions, make-value, nake-boolean, \n,.. , that are specializations of nake-f ield, fixing the values for width, signed, insert, and extract; \nthese specializations are usually used in place of make-field. Make-product, make-pouer, andrnake-sequence \nare analogous tothe Struc\u00adture operators of the same names. A Layout object supports the following operations: \nstructure-of:(L) + S accesso~ (1: L, name: Sym) + [(h B, 6: Int, is: Int*) + Val] modifier (J: L, name: \nSym) + [(lx B, 6: Int, isu: Int*Val) + B] fields: (/: L, consume: (name: Syrn) + Bool) + Bool  The \nstructure-of a Layout is the corresponding Structure. Each field, and each sequence, has a name. No two \nfields or sequences in a Layout have the same name. Thus, each field or sequence can be identified by \none name and a series of integer subscripts, one for each level of power or sequence structure above \nthe field or sequence in question. The access or method takes a field or sequence name and returns either \n#f or an accessor function. An acces\u00adsor function takes a Layout/Structure instance (i.e., a Bits and \nan offset into that Bits) and a sequence of subscripts (as Scheme integers), and returns the value of \nthe field, or length of the sequence, identified by that name and subscripts. The modifier method maps \na name to either #f or a modifier function. A modifier function takes a Layout/Structure in\u00adstance and \na sequence consisting of the subscripts followed by the new value for some field, and sets the identified \nfield to the new value. The fields method is used to check the uniqueness of names in a Layout; it enumerates \nthe names in a Layout by calling back its argument consume once for each symbol, stopping if and when \nconsume returns a true value. A Type object stands for a SCHEMEXEROX data type and its representation. \nThere are three kinds of Types: immedi\u00adate, tagged, and coded. They correspond to the three levels of \norganization that SCHEMEXEROX imposes on the space of represent at ions of Scheme values. An immediate \nType s representation fits entirely within a word, and uses a di\u00adrect Bits in the lower levels of this \nsystem. A tagged Type s representation consists of a tagged address of the real rep\u00adresentation, and \nuses an indirect Bits. A coded Type is a special case of a tagged type, where the real representa\u00adtion \nstarts with a typecode. The following operations are used to create Types: make-immediate: (1: L) + T \nmake-tagged (tag: Int, i: L, @z: Pred, ix: Ini.t]) + T make-coded (1: L) + T All three Type-creation \nprocedures take a Layout argument. make-tagged also takes the tag as an argument; make-coded doesn t \ntake a tag argument because all coded types share one particular tag. make-t agged also takes two optional \nar\u00adguments that can (1) further specialize the Type s mem\u00adbership predicate, and (2) extend the initialization \nstep. make-t agged uses these optional arguments to implement coded types as a special case of tagged \ntypes. A Pred is a procedure of signature (read: Reader) + BOO1 used to further specialize the predicate \nof a type. The pred\u00adicate of a tagged type first tests the tag, and if that passes then calls pz (if \ngiven) to make further tests. pz uses its argument read, which has the signature (name: Syra, il, . . \n. . in: Int) + Val to read fields by name and subscripts, An Init is a procedure that extends the initialization \nof new values; if ix is given, it is applied to the new instance after the Structure-level initialization. \nA Type supports the following operations: constructor (t: T) + (is: Int*) + Val[t] predicate (t: T) + \n(Val) + Bool accessor (t: T, name: Sym) + (v: Val(t), is: Int*) + Val modifier (t: T, name: Sym) + (v: \nVal(t), iw: Int*Val) + Val[t] where Val[t] denotes a Scheme value known to be of Type t, and Val(t) denotes \na Scheme value that should be of type t(an exception is raised if it is not). The construct or of a Type \ncreates and initializes an instance according to the given instance specification; the result is a Scheme \nvalue of type t. The predicate of a Type tests an arbitrary Scheme value for membership in the Type. \nThe accessor and modifier operations map a field or sequence name to an accessor function and a modifier \nfunction (respectively) for that field or sequence. 4 Optimizing Compilation in SCK-ENEXEROX The type \nsystem s generality and its modular implementa\u00adtion have the potential to make the data structure oper\u00adations \nthat it defines unacceptably slow. This can be ex\u00adplained with a closer look at the type system implementa\u00adtion. \nThe constructors at each level of abstraction in the type system are implemented in a simple object-oriented \nstyle. This excerpt from the implementation of layouts is repre\u00adsent ative: (public (make-field my-name \nwidth signed encode decode) \u00ad (clef ine (fields consume) ; one method ... ) (clef ine (accessor name) \n; another method ... ) ... (clef ine (self msg) ; the object definition (case msg , method lookup bu \ncase .. ((f iel~s) fields) ( (accessor) accessor) ... )) self) ; return the object To operate on a field, \nfor example, we call the field object with a single argument, the method selector. The object returns \na method, which we then call with the appropriate arguments. When pairs are defined using the type system, \ncar per\u00adforms a number of obiect constructions and method invo\u00adcations. A rough trace of the execution \nof car on a value purported to be a pair is as follows: Construct a direct Bits object out of the Scheme \nvalue, and then call structure#read to fetch the tag. Structure#read does a bits$tread, which calls a \nutility function to extract a field from the word in the Bits object. Compare the tag to the expected \ntag for pairs. Assuming the test succeeds, construct an indirect Bits object from the Scheme value, and \ncall a layout accessor that has been stored in the type accessor. The lay\u00adout accessor calls structure#read \nwhich in turn calls bits#read. Reading from an indirect Bits calls a helper passing two closures. The \nhelper computes an address and calls one of the passed closures. The called closure fetches a word from \nmemory at the computed address and then calls a utilitv to extract a field from the word. The layout \nlevel calls the encoder (in this case, the identity) on the field and resulting value is re\u00adturned. Bits#read \nand structure#read are small routines that serve to hide the object protocol; they retrieve the appropriate \nmethod from an object and invoke the method. Creating a bits object constructs at least 3 closures. In \nthis sketch we can count 20 procedure calls and the construction of 6 closures. Omitted from the sketch \nis the work performed in constructing the accessor car itself the type system computes the field s offset \nin the record, ;and the layout accessor that the type accessor will need. To make the type system of \npractical use, the ScmIm-XEROX compiler must turn all of this into a handful of in\u00adstructions. Though \nof modest complexity at 75OO lines of Scheme, the compiler succeeds in this task. It does so with extensive \ncross-module inline substitution, Rabbit-style op timizations [8], and a bit of help from a few language \nexten\u00adsions: the module system, the others extra arguments facility, and programmer-supplied irdining \ndeclarations. In addition to this, the type system is written in a style which takes the compiler s optimization \nstrategy into ac\u00adcount. In particular, the code contains no side effects, and data structures are represented \nprocedurally. The type sys\u00adtem implementor included declarations for procedure imdin\u00ading, and was careful \nto avoid non-trivial recursion among inlined procedures. 4.1 Compiler Overview SCHEMEXEROX is built on \nthe Xerox Portable Common Run\u00adtime [9]. PCR provides garbage collection, dynamic load\u00ading, and threads. \nPCR S conservative garbage collector per\u00admits SCHEMEXEROX code to generate ill-formed Scheme ob\u00adjects \nas intermediate results without having to lock out the garbage collector. ThM a convenience for the compiler \n(it need not maintain non-pointer/point er distinction), ancl for the type system (it is constructing \nScheme values in user code). The SCHEMEXEROX compiler is similar in structure to Orbit [4], though it \nis not as ambitious in closure analysis; the compiler doesn t even recognize simple loops. Unlike many \nLisp compilers, t bough, the SCHEME XEROX compiler operates on whole modules by default. Since the module \nsystem identifies what definitions escape from the module, any other definitions are subject to the full \nforce of the op timizer. Non-escaping definitions may be substituted irdine, or removed entirely when \nno longer referenced. The front end of the compiler converts source into an abstract syntax tree (AST), \nremoves assignments to vari\u00adables, and converts the AST to continuation passing style, The simplifier \nthen applies a number of transformations to the code. After simplification, the ASTS of public items \nare saved in the object file, lambdas are annotated with a list of free variables, and C code is generated. \nSince the SC~~-XEROX compiler generates C code, it does no register allo\u00adcation, instruction selection, \nor instruction scheduling. A header file included in each generated C file defines a number of C macros \nthat expand into irdine assembly code. These macros are used to implement mechanisms that would be inefficient \nto implement in plain C. The compiler uses this mechanism for arithmetic using tagged add and subtract \ninstructions, making tail recursive calls, and for receiving multiple return values. The remaining sections \ndescribe the simplifier in more detail, and present sample output from the compiler. 4.2 The Simplifier \nDoes the Work The simplifier does its work with a tree walk. It collects subsititutions to make as it \nwalks down the tree, and makes changes to the tree as it returns back up. Whenever the sim\u00adplifier makes \na change in the tree, it resimplifies the subtree rooted at the changed node. Among the conventional \ntrans\u00adformations performed are beta reduction, constant folding, boolean short-circuiting, and the elimination \nof dead code. Simplifying modules individually will not yield adequate performance for the type system. \nTo address this, SC~~-XEROX supports esposures [3], a mechanism that permits a user to give the compiler \naccess to interface implementations in other modules. With the implementation of an imported item exposed, \nthe simplifier can exploit the context of each use of the item to further simplify the code. 4.3 Exposures \nExposures provide a framework for sharing implementation\u00adlevel information between separately compiled \nparts of a SCHEMEXEROX program. An exposure is a pledge that a user makes to the compiler that certain \nassumptions will hold true in the eventual runtime environment of the code being compiled. The compiler \ncan then use that information to generate better code. The linker can verify the assump tions that the \ncompiler made are actually true when the whole program is linked together. Though a wide variety of kinds \nof exposures are possible, SCHEMEXEROX currently supports just one kind: a user may expose the complete \nim\u00adplementation of public items of an already compiled module. The compiler will use an item if its value \nis an eqv?-safe lit\u00aderal, or a procedure that was declared to be inlinable. (Dec\u00adlarations about irdining \nare described below.) During compilation of a module, the optimized abstract syntax trees of public items \nare saved in a special section of the object file. When compiling a client of such a module, the user \ncan direct the compiler to use those saved imple\u00adment ations.  4.4 Inline Substitution and Declarations \nThe simplifier performs inline substitution for procedures in a number of cases. For example, when the \nsimplifier can determine that a singly-referenced local variable is bound to a lambda-expression, it \nwill substitute the lambda for the variable, and remove the variable from the code. But, the simplifier \nis not free to inline every use of a procedure for which it has an imp1ementation4. Doing so could result \nin a large incre= in the size of the code, or even an infinite loop of irdines. Therefore, the simplifier \nuses user-supplied declarations to control inlining when code size may be a problem. In these cases, \na variety of user-supplied declarations can control the three possible states a lambda-expression can \nhave with respect to irdining: inline, not inline and don t care. A lambda that is marked inline will \nbe substituted whenever a variable to which it is bound occurs in procedure position of a call, or the \ntest of an if. A lambda that is marked don t care will be substituted if there is only one reference \nto the variable to which it is bound. A lambda that is marked not inline will never be substituted. The \nsimplifier disregards an irdine mark on a self-recursive lambda. Though the declaration facility allows \nthe user fine con\u00adtrol over which procedures in a module are inlinable, we found we did not need this \nflexibility. The type system im\u00adplementation uses only one declaration in each module to indicate that \nall lambdas in the source should be marked inline.  4.5 Letrectification One novel transformation important \nto the simplifier s suc\u00adcess involves turning certain patterns of assignments into bindings thus enabling \nfuture beta-reductions; we call this transformation letrectijication. Scheme requires that top level \ndefinitions have the se\u00admantics of assignments, except that a new binding is pro\u00advided for the variable \naround the entire program. For the sake of consistency, definitions in SCHEMEXEROX modules and internal \ndefinitions behave the same way. As a result, what is conceptually a binding operation is actually repre\u00adsented \nw cell operations. Consider a module that exports push to the stack interface, and has one definition \ninternaJ to the module: (nrodule ((export stack)) (define (helper ...) ...) (public (push . . . ) ( . \n. . (helper . ..) . ..))) After module translation and assignment conversion we have: (lambda () (let \n( (helper-cell (make-cell #unspecified))) (cell-set! helper-cell (lambda (...) . ..)) (cell-set! stack#push \n(lambda (x) (... ((cell-ref helper-cell) . ..) . ..)))) Inorder for the simplifier to beableto substitute \nthe helper function, it must first convert the cell operations into vari\u00adable bindings and variable references. \nTo do this, the simplifier looks for calls to Are-cell where the resulting cell is assigned a meaningful \nvalue ex\u00adactly once, and that value is aliteralor a lambda. In the case 4Such an implementation may \ncome either from a definition in the module itself, or from an exposure of a literal, a let is inserted \nin the tree at the point of the make-cell, the assignment is removed, and any cell-refs are converted \nto variable references. The transformation is similar when the assigned value is a lambda-expression, \nbut it is legal only if the all variables free in the lambda are in scope at the point in the tree where \nits corresponding make-cell cal.l appears. Actually, the transformation is a bit more complicated. The \nsimplifier looks for sequences of calls to make-cell, and attempts to create a letrec containing all \nthe assigned val\u00adues. Moving the lambdas in a block increases the chance that all the free variables \nwill be in scope at the destination. This transformation was originally performed in a sep\u00adarate pass \nbefore conversion to continuation passing style. The transformation in that case is much simpler. Unfor\u00adtunately, \ncross-module inlining causes the same pattern to arise in the course of simplification, so the simpler \napproach was not adequate.  4.6 Example Returning to the example of pairs, here is the C code gen\u00aderated \nfor set-cdr!. This is not the compiler output ver\u00adbatim. To improve readability we have expanded Cmacros, \nrenamed some variables, and removed redundant caats. static SX-Value GJiteral-4; extern SX-Global-Cell \nSX-G-type-err; static SX-Value G-pair-Tilde-set-cdr_Bang-O(self, nargs, v, i) SX-Value self, v, i; unsigned \nnargs; { SX-Value proc, r4, r3, r2, w2, rl, w1, a, test, rO, wO; if (nargs != 2) SX-Arity_Erroro; Wo=(29 \n>=32) ?O : (V << 29); rO = (29 >=32) ?O: (wO>> 29); test =rO==2; if (test) { a= v+2; WI = (* (unsigned \n*) a); rl=wl&#38;O; W2 = i &#38; OXFFFFFFFF; r2=(O>=32) ?0 : (w2<<O); r3 =r2 Iri; (* (unsigned*) a) = \nr3; return SX-UIJSPECIFIED; else { r4 = SX-G-type-err.value; if (! SX-procedure-p(proc = r4)) SX-Procedure-Error(proc) \n; SX-For_Effect ( SX-Procedure-Code (proc) (proc, 2, V, GJiteral-4)); return v; 1; 1 Compiling this code \nwith GCC [7] yields the following SPARC assembly code. Notice that GCC has converted two shifts into \nan and and crop, and eliminated a useless AND, OR, shift, and fetch. .G-pair.Tilde.set-cdr-Bang.0: save \n%sp,-li2,%p GCCfunction prolag call .--builtin.save~egs,0 nop nov %iO,XoO at Xii, ~Lfp+72] mov %il,%ol \ncmp %01,2 ; arity check be L15 rnov zi2,Zi0 call -SX-Raise-Arity-Error,O add %fp,76,%02 L15: and li0,7,%o0 \n; check for pair tag cmp ZO0,2 bne L16 ; raise non-pair error sethi %hi(-SX-G-type-err+4) pxo0 ; delay \nat %i3,MiO+2] ; set the cdr! b L19 ; branchto exit mov 3327,1i0 ; return#!unspecified L16: . . . code \nto handle the type error. . . L19: ret restore  In code that checks argument types explicitly, the \ntype error code in the data structure operation may become un\u00adnecessary, aa in the following example: \n(public (zero-tail! x) (cond ((pair#pair? x) (pair#set-cdr! x O) x) (else #f)))  The GCC-generated assembly \ncode looks like thw: .G-simple.Tilde-zero-tail-Beng-0: . . . GCCfunctionprolog and arity check.. . and \n~i0,7,%o0 tag check for pair 9 cmp %00,2 bne,a L19 mov 1023,%i0 ; (delay) return false st Xg0,UiO+2] \n; set the cdr L19: ret restore  GCC removed a redundant pair? test and type error code. Our compiler \nremoved the useless aritychecks for the calls topair? and set-cdr!. This code is about as good aa can \nbe produced fc,r a SPARC without using the addressing hardware toperforrn some of the type checks. In \nsuch a scheme, one carefully assigns pointer tags such that an access through an ill-typed pointer causes \nan alignment trap. The trap handler can inspect the offending instruction to determine the details of \nthe error. We chose not to include this complexity in ScHEmXEItOx, though the type system could support \nit. 4.7 Practicalities Four problems remain in the implementation: (1) thesim\u00adplifier is slow, (2) letrectification \ndoesn t always work, (3) procedure eqv?-ness is not always maintained, and (4) error cases are verbose. \nThe simplifier is slow. The simplifier currently does far more work than is necessary, aa a result of \nthe its simple\u00adminded control structure. For example, when the simplifier inlines the definition for \none of the type system objects, it first fully optimizes all of the methods before it tries to optimize \nthe case expression that performs the method dis\u00adpatch. When the case is eventually considered, thesimpli\u00adfier \nthrows away the arduously optimized code for all but one of the methods. This happens at each of the \n4 levels of abstraction. We believe this will be straightforward to fix. Letrectification doesn t aiways \nwork. Our implementa\u00adtionofletrectification knot robust wehave had to rewrite some code in the type system \nto use binding constructs in\u00adstead of internal definesin order to get the desired output from thesimplifier. \nThis is partly due tothesimplifier s sim\u00adple control structure, and partly due to the transformation \nitself not doing enough work. Procedure eqv?-ness isnot always maintained. Insubsti\u00adtuting procedures, \nit is important to maintain eqv?-ness as Scheme requires. There is one situation that arises during simplification \nwhere the constraint may not be satisfied. If the simplifier substitutes a procedure into argument position \nofaca.11 within the body ofan irdinable procedure P, then P should no longer be considered inlinable \nby the simpli\u00adfier. Otherwise, P may itself be substituted multiple times, possibly defeating theeqv?-ness \nconstraints of the first sub\u00adstituted procedure. We believe that prohibiting procedures from being substituted \nin argument position eliminates the problem and produces the same output. Error cases are verbose. The \ncode to check for and re\u00adport type errors insufficiently verbose that we may not want to irdine calls \nto commonly-used field accessors and modi\u00adfiers. In some cases, machine-dependent solutions such as skipping \ntag checks and then trapping on any misaligned memory references would solve the problem nicely, but \na general solution would still be needed. 5 Conclusions Placing all knowledge of data-type representation \nin the compiler requires that the author write more meta-code , code that generates code to perform operations. \nSuch code is harder to write, understand, and test than direct code. Code in the compiler is a.lso difficult \nfor users to experiment with; external code allows them to try out new ideas for representations. In \na language like Scheme, in which new data types are created procedurally instead of declaratively, either \nthe representation knowledge in the compiler must be very complex or else user-defined types will get \nshort shrift and not be as efficiently compiled as built-in types. The approach we ve taken in SCHEMEXEROX \nistogoeven further than what s normal for Scheme implementations, making more of thedata-type representation \nknowledge pro\u00adcedural; our data-type representations are themselves first\u00adclaes values, manipulable in \nthe usual way. Because all prim\u00aditive operations, from the bit-level up, are written in high\u00adlevel, modular \nScheme code, we can be more sure of their correctness; the code is easier to understand and maintain \nthan would be meta-level code in the compiler. Further, users can experiment with new representation \ntypes (such as various styles of variant records) either within the frame\u00adwork used for most SCHEMEXEROX \ntypes or else in entirely new styles. In all cases, straightforward compiler support suffices to produce \nhighly efficient object code. [1] Clinger, William and Jonathan Rees, editors, Revised Report on the \nAlgorithmic Programming Language Scheme, LLSPPointers 4(3), 1991. [2] Curtis, Pavel, The Scheme of Things, \nLL5 P Pointers 4(l), 1991. [3] Curtis, Pavel and James Rauen, A Module System for Scheme, in Proceedings \nof the 1990 ACM Conference on Lisp and Functional Programming, Nice, France, 1990. [4] Kranz, David, \nRichard Kelsey, Jonathan A. Rees, Paul Hudak, James Philbin, and Norman I. Adams, Orbit: An Optimizing \nCompiler for Scheme, Proceedings of the SIGPLAN 86 Symposium on Compiler Construction, pp. 219 233, published \nas SIGPLAN Notices 21(7), July 1986. [5] Queinnec, Christian and Pierre Cointe, An Open Ended Data Representation \nModel for EUIISP, in Proceedings of the ACM Conference on Lisp and Functional Pro\u00adgramming, pp. 298 308, \nSnowbird, Utah, 1988. [6] Queirmec, Christian, A Specification Framework for Data Aggregates, unnumbered \ntechnical report from Laboratoire d Informatique de l hcole Polytechnique, 1989. [7] Stallman, Richard, \nUsing and Porting GNU CC, Free Software Foundation, 1989. [8] Steele, Guy Lewis, Jr., Rabbit: a Compiler \nfor Scheme, MIT AI Memo 474, Massachusetts Institute of Technol\u00adogy, Cambridge, Mass., May 1978. [9] \nWeiser, Mark, Alan Demers, and Carl Hauser, The Portable Runtime Approach to Interoperability, in Pro\u00adceedings \nof the Twelfth ACM Symposium on Operating Systems Principles, pp. 114 122, published as Operating Systems \nReview 23(5), December 1989.  \n\t\t\t", "proc_id": "155090", "abstract": "<p>In most programming language implementations, the compiler has detailed knowledge of the representations of and operations on primitive data typed and data-type constructors. In SCHEMEXEROX, this knowledge is almost entirely external to the compiler, in ordinary, procedural user code. The primitive representations and operations are embodied in first-class &#8220;representation types&#8221; that are constructed and implemented in an abstract and high-level fashion. Despite this abstractness, a few generally-useful optimizing transformations are sufficient to allow the SCHEMEXEROX compiler to generate efficient code for the primitive operations, essentially as good as could be achieved using more contorted, traditional techniques.</p>", "authors": [{"name": "Norman Adams", "author_profile_id": "81100312784", "affiliation": "", "person_id": "PP37026305", "email_address": "", "orcid_id": ""}, {"name": "Pavel Curtis", "author_profile_id": "81100112864", "affiliation": "", "person_id": "P222736", "email_address": "", "orcid_id": ""}, {"name": "Mike Spreitzer", "author_profile_id": "81100538508", "affiliation": "", "person_id": "PP31095550", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/155090.155103", "year": "1993", "article_id": "155103", "conference": "PLDI", "title": "First-class data-type representations in SCHEMEXEROX", "url": "http://dl.acm.org/citation.cfm?id=155103"}