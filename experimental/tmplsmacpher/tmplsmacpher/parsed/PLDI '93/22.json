{"article_publication_date": "06-01-1993", "fulltext": "\n The Essence of Cornpiling with Continuations Cormac Flanagan* Amr Sabry k Bruce F. Duba Matthias Felleisen* \nDepartmentof Computer Science Rice University Houston, TX 77251-1892 Abstract In order to simplify the \ncompilation process, many com\u00adpilers for higher-order languages use the continuation\u00adpassing style (CPS) \ntransformation in a first phase to generate an intermediate representation of the source program. The \nsalient aspect of this intermediate fcmm is that all procedures take an argument that represents the \nrest of the computation (the continuation ). Since the naive CPS transformation considerably increases \nthe size of programs, CPS compilers perform reductions to produce a more compact intermediate representation. \nAlthough often implemented as a part of the CPS trans\u00adformation, this step is conceptually a second phase. \nFi\u00adnally, code generators for typical CPS compilers treat continuations specially in order to optimize \nthe inter\u00adpretation of continuation parameters. A thorough analysis of the abstract machine for CPS terms \nshows that the actions of the code generator 2n\u00advert the naive CPS translation step. Put differently, \nthe combined effect of the three phases is equivalent to a source-to-source transformation that simulates \nthe compaction phase. Thus, fully developed CPS compil\u00aders do not need to employ the CPS transformation \nbut can achieve the same results with a simple source-level transformation. 1 Compiling with Continuations \nA number of prominent compilers for applicative higher\u00adorder programming languages use the language of \n*Supported in part by NSF grants CCR 89-17022 and ~CCR 91-22518 and Texas ATP grant 91-003604014. Permission \nto copy without fee all or part of this material is granted provided that the copias are not made or \ndistributed for direct commercial advantage, the ACM copyright notice and the title of the publication \nand its date appear, and notice is given that copying is by permission of the Association for Computing \nMachinery. To copy otherwisa, or to rapublish, requires a fee and/or specific permission. ACM-SlGPLAN-PLDl-6 \n/93/Albuquerque, N.M. 01993 ACM 0-89791 -598 -41931000610237 . ..$1 .50 continuation-passing style (CPS) \nterms as their inter\u00admediate representation for programs [2, 14, 18, 19]. This strategy apparently offers \ntwo major advantages. First, Plotkin [16] showed that the A-value calculus based on the ~-value rule \nis an operational semantics for the source language, that the conventional jull A-calculus is a semantics \nfor the intermediate language, and, most importantly, that the A-calculus proves more equations between \nCPS terms than the &#38;-calculus does between corresponding terms of the source language. ~anslated \ninto practice, a compiler can perform more transforma\u00adtions on the intermediate language than on the \nsource language [2:4-5]. Second, the language of CPS terms is basically a stylized assembly language, \nfor which it is easy to generate actual assembly programs for different machines [2, 13, 20]. In short, \nthe CPS transformation provides an organizational principle that simplifies the construction of compilers. \nTo gain a better understanding of the role that the CPS transformation plays in the compilation process, \nwe recently studied the precise connection between the Au-calculus for source terms and the Lcalculus \nfor CPS terms. The result of this research [17] was an extended A.-calculus that precisely corresponds \nto the A-calcu\u00adlus of the intermediate CPS language and that is still semantically sound for the source \nlanguage. The ex\u00adtended calculus includes a set of reductions, called the A-reductions, that simplify \nsource terms in the same manner as realistic CPS transformations simplify the output of the naive transformation. \nThe effect of these reductions is to name all intermediate results and to merge code blocks across declarations \nand conditionals. Direct compilers typically perform these reductions on an ad hoc and incomplete basis.l \nThe goal of the present paper is to show that the true purpose of using CPS terms as an intermediate \nrepre\u00adsent ation is also achieved by using A-normal forms. We base our argument on a formal development \nof the ab\u00adstract machine for the intermediate code of a CPS-based compiler. The development shows that \nthis machine is 1Personal communication: H. Boehm (also [4]), K. D ybvig, R. Hieb (April 92). M .._,. \n I ~et I (ifO I(MM, I(ofkf, (z MI Ml) M,) M, M,) . . . Jfn) . . . &#38;fn) V c x O C c c C Values constants \nVariables Primitive Operations v .. CIZI(AZ1. .,%M) Figure 1: Abstract Syntax of Core Scheme ( CS) identical \nto a machine for A-normal forms. Thus, the back end of an A-normal form compiler can employ the same \ncode generation techniques that a CPS compiler uses. In short, A-normalization provides an organiza\u00adtional \nprinciple for the construction of compilers that combines various stages of fully developed CPS compil\u00aders \nin one straightforward transformation. The next section reviews the syntax and semantics of a typical \nhigher-order applicative language. The follow\u00ading section analyses CPS compilers for this language. Section \n4 introduces the A-reductions and describes A\u00adnormal form compilers. Section 5 proves the equiva, lence \nbetween A-normal form compilers and realistic CPS compilers. The benefits of using A-normal form terms \nas an intermediate representation for compilers is the topic of Section 6. The appendix includes a linear \nA-normalization algorithm. 2 Core Scheme The source language is a simple higher-order applicative language. \nFor our purposes, it suffices to consider the language of abstract syntax trees that is produced by the \nlexical and syntactic analysis module of the com\u00adpiler: see Figure 1 for the context-free grammar of \nthis language. The terms of the language are either val\u00adues or non-values. Values include constants, \nvariables, and procedures. Non-values include let-expressions (blocks), conditionals, function applications \nand prim\u00aditive operations.2 The sets of constants and primitive procedures are intentionally unspecified. \nFor our pur\u00adposes, it is irrelevant whether the language is statically typed like ML or dynamically typed \nlike Scheme. The language Core Scheme has the following context\u00ad sensitive properties, which are assumed \nto be checked by the front-end of the compiler. In the procedure (Ax, . . . zn, iM) the parameters Z1, \n. . . . Zn are mutually distinct and bound in the body M. Similarly, the ex\u00ad pression (let (z Ml) Mz) \nbinds z in M2. A variable that 2The language is overly simple but contains all ingredients that are necessary \nto generate our result for full ML or Scheme. In par\u00adticular, the introduction of assignments, and even \ncontrol opera\u00adtors, is orthogonal to the analysis of the CPS-based compilation strategy. is not bound \nby a A or a let is free; the set of free vari\u00adables in a term M is FV(M). Like Barendregt [3:ch 2,3], \nwe identify terms modulo bound variables and we as\u00adsume that free and bound variables of distinct terms \ndo not interfere in definitions or theorems. The semantics of the language is a partial function from \nprograms to answers. A program is a term with no free variables and an answer is a member of the synt \nattic category of constants. Following conventional tradition [1], we specify the operational semantics \nof Core Scheme with an abstract machine. The machine we use, the CEK machine [10], has three components: \na control string C, an environment E that includes bind\u00adings for all free variables in C, and a continuation \n1{ that represents the (rest of the computation . The CEK machine changes state according to the transition \nfunction in Figure 2. For example, the state transition for the block (let (z Ml) ib12 ) starts the eval\u00aduation \nof Ml in the current environment E and modifies the continuation register to encode the rest of the com\u00adputation \n(It x, Mz, E, K). When the new continuation receives a value, it extends the environment with a value \nfor z and proceeds with the evaluation of Mz. The re\u00admaining clauses have similarly intuitive explanations. \nThe relation -* is the reflexive transitive closure of the transition function. The function y constructs \nmachine values from syntactic values and environments. The notation E(z) refers to an algorithm for looking \nup the value of x in the environment E. The operation E[xl := V;, . . ..$m := V;] extends the environment \nE such that subsequent lookups of Zi return the value Vi . The object (cl xl . . .xn, M, E) is a closure, \na record that contains the code for M and values for the free variables of (Axl . . . Zn .M). The partial \nfunction 6 abstracts the semantics of the primitive operations. The CEK machine provides a model for \ndesigning di\u00ad rect compilers [6, 11, 15]. A compiler based on the CEK machine implements an efficient \nrepresentation for en\u00ad vironments, e.g., displays, and for continuations, e.g., a stack.3 The machine \ncode produced by such a compiler 3The machine also characterizes compilers for first-order lan\u00adguages, \ne.g., Fortran. In this case, the creation and deletion of the environment and continuation components \nalways follows a stack-like behavior. Hence the machine reduces to a traditional stack machine. Semantics: \nLet M G CS, e?dd(kf) = c if (M, 0, stop)% (stop, c). Data Specifications: S ~ Stated = 6 S X Eravd X \n6 Odd I Codci X Vdbed (machine states) E G Envd = Varhbles ++ Vdtted (environments) v c valued= cl(clzl. \n..z~, M,E) (machine values) ~e ~Odd = stop I (ap (..., V*, *, M), E,K)E, K) I (lt z, M, E,K) (continuations) \n[ (if J41, M., E,K) I (pr O,(..., V*, @,M,), E,K), K) Transition Rules: (V, E, K) +---+ (K, -Y(V, E)) \n((let (s MI) MQ), E, K) x (Jfl, E, (lt ~, Jfz, E, K)) ((ifO,Ml Lfz Jfs), E, K) ++ (MI, E, (if M,, Ms, \nE, K)) ((MM, . . . Mn), E,K) +-+ (M, E, (ap (c, Ml,..., M~), E, K)) ((OM, M, ... Mn), E,K) +--+ (M1, \nE, (pr 0,(0,M2,...,~n))E)~)) ((it z, M, E, K), V*) _ (M, E[z := V*], K) ((if MI, M2, E, K),0) _ (MI, \nE,K) ((if MI, Mz, E, K), V ) + (M,, E, K) where V* # O ((ape..., ~*, e, M,... ), E, K), T&#38;,) +--+ \n(M, E,(ap (..., K*, Ul,, C), E,~)), ~)) ((ap V*, ~*,..., .), E, K),V;) + (M , .~ [$1 := VI*, ....Zn := \nv;], ~) if V* = (cl ZI ... T~. Jf , E ) ((pr O,(..., V*, o,M ,...), E, K), K$I) t---+ (M, E,(pr 0,(..., \nK*, K:,, ), E,K)), K)) if 6(0, VI*, . . . . V:) is defined((pr O,(V*,.. .,c), E, K ), V:) * (K,6(0, Vi*,. \n... V;)) Converting syntactic values to machine values: 7(c,E) = C T(z, E) = E(x) 7((AX1 . ..xn.M). \nE) = (CIZ,L . ..zn. M,E) Figure 2: The CEK-machine realizes the abstract operations specified by the \nCEK code segment machine by manipulating these concrete representations IV~(+ (+2 2) (let (z 1) (f z))) \n of environments and continuations. into the CPS term ((I/c,. ((~k,. (~ 2))  3 CPS Compilers (At,. ((M4. \n(kq 2)) (Xtz.(+ h f,~z)))))) Several compilers map source terms to a CPS intermedi\u00ad  (Xts.((m~ ate \nrepresentation before generating machine code. The (Qk,. (k, 1)) function f [12] in Figure 3 is the \nbasis of CPS trans\u00ad (Atq.(letJz t~) formations used in various compilers [2, 14, 19]. It uses ((M,. \n((II%$. (~ f))  special A-expressions or continuations to encode the rest (x,. ((M,, (k,z))  of the \ncomputation, thus shifting the burden of main\u00ad (Ro.(tslbt~))))))  taining control information from the \nabstract machine k,)))))  to the code. The notation (~z. . . .) marks the adntin\u00ad (X7. (+ k t3h)))))) \n istrattve A-expressions introduced by the CPS transfor\u00admation. The primitive operation 0 used in the \nCPS By convention, we ignore the context (M.[ ]) enclosing language is equivalent to the operation O \nfor the source all CPS programs. language, except that O takes an extra continuation ar-To decrease \nthe number of administrative ~\u00adgument, which receives the result once it is computed. abstractions, realistic \nCPS compilers include a simplifi-The transformation f introduces a large number of cation phase for compacting \nCPS terms [2:68 69, 14:5 administrative ~-expressions. For example, f maps the 6, 19:49 51]. For an analysis \nof this simplification phase, its optimality, and how it can be combined with Y, we refer the reader \nto Danvy and Filinski [9] and Sabry and Felleisen [17]. This phase simplifies administrative redexes \nof the form ((~z .P) Q) according to the rule: ((~z.P) Q) + P[z := Q] (P) The term P[z := Q] is the \nresult of the capture-free substitution of all free occurrences of z in P by Q; for example, (Az.zz)[z \n:= (Ay. x)] = (Au.u(~y. z)), Apply\u00ading the reduction @ to all the administrative redexes in our previous \nexample produces t] e following ~-normal form term: cps(N) = (+ (Itl. (let (z 1) (j Xtz.(+ k i~f~))z))) \n2 2) The reduction ~ is strongly-normalizing on the lan\u00adguage of CPS terms [17]. Hence, the simplification \nphase of a CPS compiler can remove all ~-redexes from the output of the translation 7.4 After the simpli\u00adfication \nphase, we no longer need to distinguish be\u00adtween regular and administrative J-expressions, and use the \nnotation (~. ) for both classes of A-expression. With this convention, the language of ~-normal forms, \nCPS(CS), is the following [17]: P ::= (k w) (return) \\ (let (z w) P) (btnd) I (ifO W PI P,) (branch) \n[(wkw, . ..wn) (ta~l call) I (w (A$.P) w, . . . w.) (call)  I(o kw, . ..wn) (pn?n-clp) \\ (0 (AZ.P) w, \n. . . Wn) (pram-q) 4The CPS translation of a conditional expression contains two references to the continuation \nvariable k. Thus, the ~\u00adnormalization phase can produce exponentially larger output. Modifying the CPS \nalgorithm to avoid duplicating kremoves the potential for exponential growth. The rest of our technical \ndevel\u00adopment can be adapted mutatis mutandts. w ::= c I z I (Akz?l. ..8P)P) (values) Indeed, this language \nis typical of the intermediate rep\u00adresentation used by CPS compilers [2, 14, 19]. Naive CPS Compilers \nThe abstract machine that characterizes the code generator of a naive CPS com\u00adpiler is the C.P,E machine. \nSince terms in CPS(CS) contain an encoding of control-flow information, the machine does not require \na continuation component (K) to record the rest of the computation. Evalua\u00adtion proceeds according to \nthe state transition func\u00adtion in Figure 4. For example, the state transition for the tail call (W k \nW1 Wn) computes a closure (cl k xl . . ~n, P , E ) corresponding to W, extends E with the values of k, \nWI, . . . . Wn and starts the inter\u00adpretation of P . Realistic CPS Compilers Although the CCP,E ma\u00adchine \ndescribes what a na~ve CPS compiler would do, typical compilers deviate from this model in two re\u00adgards. \nFirst, the naive abstract machine for CPS code repre\u00adsents the continuation as an ordinary closure. Yet, \nreal\u00adistic CPS compilers mark the continuation closure as a special closure. For example, Shivers partitions \npro\u00adcedures and continuations in order to improve the data flow analysis of CPS programs [18:sec 3.8.3]. \nAlso, in both Orbit [14] and Rabbit [19], the allocation strategy of a closure changes if the closure \nis a continuation. Sim\u00adilarly, Appel [2: 114 124] describes various techniques for closure allocation \nthat treat the continuation closure in a special way. In order to reflect these changes in the machine, \nwe tag continuation closures with a special marker (ar that describes them as activation records. Second, \nthe CPS representation of any user-defined procedure receives a continuation argument. However, Steele \n[19] modifies the CPS transformation with a continuation variable hack [19:94] that recognizes in\u00adstances \nof CPS terms like ((~kl . . . .P) k2 . . .) and trans\u00adSemantics: Let P G CPS(CS), ev&#38;(P) = c if \n(P, o[k := (cl x, (k $), O[k := stop])]) +--+* ((k Z), o[z := c, k := stop]). Data Specifications: s. \nC State. = CPS(CS) x Envn (machine states) E c Envn = Variables +-+ Value. (environments) W g Value. \n= Cl(clkxl . . . x~, P, E) I (cl o, P, E) I stop (machine values) Transition Rules: ((k W), E) w (P \n, E [x := ~(W, E)]) where E(k)= (cl z, P , E ) ((let (z W) P), E) w (P, E[z := @V, E)]) ((ifO W P, Pz), \nE) _ (Pi, E) where P(W, E) = O (Pz, E) where ,u(W, E) # O ((W k W, . . . Wn), E) ~ (P , E [k := E(k), \nq := W:,.. .,x~ := W;]) where P(W, E) = (cl k zI . . . x~, P , E ) and for 1< i < n, W,* = p(W,, E) ((W \n(Ax.P) W, . . . Wn), E) +--+ (P , E [k := (cl z, P, E), q := W;,.. .,xn := W;]) where p(W, E) = (cl k \nzl . ..z~. P , E ) and for 1 < z < n, W,* = IJ(W, ,E) ((0 k W, . . . Wn), E) _ (P , E [z := &#38;( O \n, W;, . . . . W;)]) if 6.(0 , W;, . . ., W;) is defined, where E(k) = (cl z, P , E ) and for 1 < z< n, \nW,* = p(W,, E) ((0 (Az.P) W, . W.)j E) _ (P, E[Z := 8.(0 , w;, ....w:)]) if 6C(0 , W?, ..., W;) is defined, \nand for 1< i < n, W, = I.J(W,, E) Converting syntactic values to machine values: P(C,E) = C P(:z, E) \n= E(x) P((NW1 ...zn.P).E) = (cl kzl . ..zn. P,E) Figure 4: The naive CPS abstract machine: the C.p, E \nmachine. forms them to ((A. ~~.P[/cl := kz]) . .). This optimiza-in Figure 5. The new CCP,EK machine \nextracts the tion eliminates some of the register shuffling [19:94] information regarding the continuation \nfrom the CPS during the evaluation of the term. Appel [2] achieves terms and manages the continuation \nin an optimized the same effect without modifying the CPS transforma-way. For example, the state transition \nfor the tail tion by letting the variables kl and kz share the same call (W k W1 . . . W.) evaluates \nW to a closure register during the procedure call. (cl kc, . . . X., P , E;), extends E; with the values \nof In terms of the CPS abstract machine, the optim-WI,..., Wn and starts the execution of P . In particu\u00adization \ncorresponds to a modification of the oper-lar, there is no need to extend E; with the value of k as this \nvalue remains in the environment component Ek. ation E [k := E(k), xl := W;, . . .,zn := W:] to J!7[lrl: \n=w; ,...,z n:= W;] such that E and E share the binding of k. In order to make the sharing explicit, we \nsplit the environment into two components: a com-4 A-Normal Form Compilers ponent Ek that includes the \nbinding for the continu- A close inspection of the C=P,EK machine reveals that ation, and a component \nE-that includes the rest of the control strings often contain redundant information the bindings, and \ntreat each component independently. considering the way instructions are executed. First, a This optimization \nrelies on the fact that every control string has exactly one free continuation variable, which return \ninstruction, i.e., the transition WC, dispatches implies that the corresponding value can be held in \na on the term (k W), which informs the machine that the special register.5 return address is denoted \nby the value of the variable k. The machine ignores this information since a re- Performing these modifications \non the naive abstract turn instruction automatically uses the value of register machine produces the \nrealistic CPS abstract machine Ek as the return address . Second, the call instruc\u00ad tions, i.e., transitions \n~, and ~., invoke closures 5 This fact also holds in the presence of control operators as there is always \none identifiable current continuation. that expect, among other arguments, a continuation k. Semantics: \nLet P G CPS(CS), ew&#38;(P) = c if (P, O, (ar z, (k z), O, stop)) w: ((k z), O[z := c], stop). Data \nSpecifications: SC E Statec = CPS(CS) x Envc x Cont. (machine states) E-E Envc = Variables m Valuec \n(environments) W* G Valuee = c I (CllCZI... Z~, P,)-) (machine values) Ek E Contc = stop I (ar z, P, \nE-, Ek) (continuations) Transition Rules: ((k W), E-, Ek) @+c (P E;[z := p(W, E-)], Ef) where Ek = (ar \nx, P , El , E!) ((let (z W) P), E-, E ) F@+= (P, E-[z := ,u(W, E-)], E ) {(if O W PI .P2), E-, Ek) ~c \n(Pi, E-, E ) where K(W,.E-) = O or (Pz, E-, E ) where K(W, E-) # O ((wkw, ... W~), E-, Ek) w= (P , E~[z] \n:= W:, . . ..z. := WJ, Ek) where v(W, E ) = (cl k zl . . .xn, P , El-) and for 1 5 t < n, W, = P(W,, \nE ) ((w (AS.P) w, . . . Wn), E-, E~) ~c (P , E~[zl := WY,. ... zn := W:], (ar z, P, E-, Ek)) where p(W, \nE-) = (cl k zl . ..~n. P , E1-) and for 1< i < n, W,* = p(W; ,E ) ((0 k W, . . Wn), E-, Ek) WC (P , Efl[z \n:= &#38;( O , W:,..., W;)], E;) if 8C(0 , W~, . . . . W;) is defined, where Ek = (ar x, P , E;, E!) and \nfor 1< i < n, W? = ,U(W,, E-) ((o (/kz.P) w, . . . W~), E-, Ek) UC (P, E-[x := &#38;( O , W:,..., W~)], \nEk) if ISC(O , W:, . . . . W;) is defined, and for 1< i< n, W,* = p(W,, E ) Figure 5: The realistic CPS \nabstract machine: the C,-P, EK machine. CPS b Again, the machine ignores the continuation parameter \n(7s $3 o in the closures and manipulate the global register Ek I I instead. Al @normalization I Undoing \nCPS The crucial insight is that the I I elimination of the redundant information from the t un-CPS CCP,EK \nmachine corresponds to an inverse CPS trans-A(C S) i e CPS(CS) formation [7, 17] on the intermediate \ncode. The func-The diagram naturally suggests a direct translation A tion Z./ in Figure 6 realizes such \nan inverse [17]. The in\u00adthat combines the effects of the three phases. The iden\u00ad verse transformation \nformalizes our intuition about the tification of the translation A requires a theorem re\u00ad redundancies \nin the CCP.EK machine. It eliminates the lating ~-reductions on CPS terms to reductions on the variable \nk from return instructions as well aa the param\u00adsource language. This correspondence of reductions was \neter k from procedures. The latter change implies that the subject of our previous paper [17]. The resulting \nset continuations are not passed as arguments in function of source reductions, the A-reductions, is \nin Figure 7.6 calls but rather become contexts surrounding the calls. Since the A-reductions are strongly \nnormalizing, we can For example, the code segment cps (IV) in Section 3 be\u00adcharacterize the translation \nA aa any function that aP\u00ad comes: plies the A-reductions to a source term until it reaches a normal form \n[17: Theorem 6.4]. A(iV) = (let (f~ (+ 2 2)) The definition of the A-reductions refers to the con\u00ad(let \n(z 1) cept of evaluation contexts. An evaluation context is a (let (t,(f%)) term with a hole (denoted \nby [ ]) in the place of one (+ -t,t2)))) subterm. The location of the hole points to the next 6Danvy \n[8] and Weise [21] also recognize that the compaction Based on the above argument, it appears that CPS \nof CPS terms can be expressed in the source language, but do not compilers perform a sequence of three \nsteps: explore this topic systematically.  The inverse CPS transformation: I?J: CPS(CS) + A(CS) U[(k \nw)] = V[w] U[(let (z W) P)] = (let (z iU[W]) U[P]) U[(if O W P1 Pz)] = (if (l Q[W] UIP1] U[l%]) Z.f[(w \nk WI . . . w.)] = (w[w] 3qw,] ... V[wn]) U[(W (AZ.P) WI . . . w.)] = (let (z (V[W] Ui[Wl] . . . If[W.])) \nU[P]) U[(o k-WI . . . Wn)] = (o W[w,] ... W[wn]) q(o (XZ.P) WI ... Wn)] = (let (z (O VIW1] . . . f17[Wn])) \nL/[P]) V2:w+ v W[c] =c !qx] =x qMx] . ..%. q = Axl . ..xn.u[itq The language A(CS) M ::= V (return) \nI (let (z V) M) (bind) l(if13V MM) (branch) I(VK . . . Vn) (tad call) I (let (x (V K Vn)) M) (call) I(ovl... \nvn) (prira-op) \\ (let (x (O Vi . . . V~)) M) (prim-op) v ::= clxl(kc]. ..x M)M) (values) Figure 6: The \ninverse CPS transformation and its output Evaluation Contexts: &#38; ::= l(let(z$)M) l(ifO~MM) I(FV. \nV8M.. .M) where F= Vor F=O [1 The A-reductions: ~[(let (X M) AT)] + (let (x M) S[N]) where&#38;# [ ], \nz @ FV(f) (AI) t[(i~ v M, M,)] -+ (ifO V SIMI] S[MZ]) where ~ # [ ] (Az) t[(F VI ... Vn)] + (let (t (F \nVI ... V~)) t[t]) (A) where F = V or F = O,: # ~ [(let (Z []) M)], $ # [],t CFV(~) Figure 7: Evaluation \ncontexts and the set of A-reductions subexpression to be evaluated according to the CEK se-The A-reductions \ntransform programs in a natu\u00admantics. For example, in an expression (let (% Ml) flfz), ral and intuitive \nmanner. The first two reductions the next reducible expression must occur within Ml, merge code segments \nacross declarations and condi\u00adhence the definition of evaluation contexts includes the t ionals. The \nlast reduction Iifis redexes out of eval\u00adclause (let (z ~) M). uation contexts and names intermediate \nresults. Us\u00ad ing evaluation contexts and the A-reductions, we can Semantics: Let M G A(CS), ewda(M) \n= c if (M, 0, (ar z, z, O,stop)) ++; (x, O[z := c], stop). Data Specifications: s. C State. = A(CS) x \nEnva x Conta (machine states) E G Enva = Variables++ Vakea (environments) v* E Valuea = c I (cl Z1 . \n. . z~, M, E) (machine values) K E Conta = stop I (ar z, M, E, K) (continuations) Transition Rules: (~ \nE, K ) Q. (M , E [z := Y(V, E)], K ) where K = (ar z, M , E , K ) ((let (z V) M), E, K) @+a (M, E[z := \nT(V, E)], K) ((ifll V Ml Mz), E, K) u. (Ml, E, K) where 7(V, E) = O or (Mz, E, K) where ~(~ E) # O ((vu \n... V~), E, K) ~. (M , E [q := V;, .... xn := V:], K) where T(V, E) = (cl ZI .z~, M , E ) and for 1< \nt < n, ~ = ~(~,E) ((let (z (V VI . . . Vn)) M)j E, K) ~. (M , E [zl := ~ , ,z~ := V~], (ar z, M, E, A \n)) where Y(V, E) = (cl . . .z~, M , E ) and for 1 < z< n, V,* = y(V,, E) ZI ((OU . . . V~), E, K) ma \n(M , E [z := c$(O, V;,..., V:)],) ) if 6(0, VI*, . . . . V;) is defined, where K = (ar z,M , E j K ) \nand for 1< i < n, V,* = -y(V,, E) ((let (z (O VI . . Vn)) M), E, A-) Ua (M, E[z := 8(0, ~ , ,V~)], K) \nif 8(0, VI*, . . . . VJ) is defined, and for 1 < i < n, V,* = -y(V, jE) Figure 8: The C.EK machine rewrite \nour sample code segment IV in Section 3 as fol\u00adlows. For clarity, we surround the reducible term with \na box: N = I(+(+22) (let (x 1) (~ z))) I -(let (t, (+2 2)) (A,) I(+ t, (let (z 1) (j z))) I) + (let (t, \n(+2 2)) (AI) (let (x 1) ~)) (let (t, (+2 2)) (A,) (let (x 1) (let (t, (f x)) (+ ~1 b)))) The appendix \nincludes a linear algorithm that maps Core Scheme terms to their normal form with respect to the A-reductions. \nCompilers In order to establish that the A\u00adreductions generate the actual intermediate code of CPS compilers, \nwe design an abstract machine for the lan\u00adguage of A-normal forms, the C. EK machine, and prove that \nthis machine is equivalent to the CPS machine in Figure 5. The C.EK machine is a CEK machine specialized \nto the subset of Core Scheme in A-normal form (Figure 6). The machine (see Figure 8) has only two kinds \nof con\u00adtinuations: the continuation stop, and continuations of the form (ar z, M, E, K). Unlike the CEK \nmachine, the C. EK machine only needs to build a continuation for the evaluation of a non-tail function \ncall. For exam\u00adple, the transition rule for the tail call (V VI . . . Vn ) evaluates V to a closure (cl \nxl . . . ~n, M , E ), extends the environment E with the values of VI, . . . . Vn and continues with \nthe execution of M . The continuation component remains in the register K. By comparison, the CEK machine \nwould build a seperate continuation for the evaluation of each sub-expression V, VI, . . . . Vm. 5 \nEquivalence of Compilation Strategies A comparison of Figures 5 and 8 suggests a close relationship between \nthe CCP,EK machine and the CaEK machine. In fact, the two machines are identi\u00adcal modulo the syntaz of \nthe control strings, as cor\u00adresponding state transitions on the two machines per\u00adform the same abstract \noperations. Currently, the tran\u00adsition rules for these machines are defined using pattern matching on \nthe syntax of terms. Once we reformulate these rules using predicates and selectors for abstract syntax, \nwe can see the correspondence more clearly. For example, we can abstract the transition rules (5) (5) \n-a and I---+C from the term syntax as the higher-order functional 75: T5[ca11-var, call-body, cal!?, \ncall-args, ca!l-fn] = (c, E,K) -. if ca//?(C) where = ca/Lvar(C) ~ = call-body(C) V = ca/Lfn(C) Vl, . \n. ..vn = ca!l-argsl(C) The arguments to 75 are abstract-syntax functions for manipulating terms in a \nsyntax-independent manner. Applying 75 to the appropriate functions produces ei\u00ad ther the transition \nrule ~. of the C. EK machine or the rule WC of the C.P, EK machine, i.e., ma = T5[A-ca/Lvar, . . . . \nA-calLfn] &#38;Zc = T~~cps-call-var, . . . . cps-call-fn] Suitable definitions of the syntax-functions \nfor the language A(CS) are: A-calLvar[(let (z (V VI . . . V~)) M)] = x A-call-body [(let (z (V V1 . . \n. Vn)) M)] = M ... A-call-fn[(let (z (V VI . . . Vn)) M)] = V Definitions for the language CPS(CS) \nfollow a similar pattern: cps-calLvar[(W (kc. P) WI . . . IVn)] = x cps-calLbody[(W (Az. P) W1 . . . \nWn)] = P ... cps-ca/Lfn[(W (~z.P) W1 . . . Wn)] = W In the same manner, we can abstract each pair of \ntransi\u00adtion rules tia and WC as a higher-order functional T.. Let S. and $C be abstract-syntax functions \nappro\u00adpriate for A-normal forms and CPS terms, respectively. Then the following theorem characterizes \nthe relation\u00adship between the two transition functions. Theorem 5.1 (Machine Equivalence) For 1< n< 7,~a \n= 7n[Sa]and @c = 7n[SC]. The theorem states that the transition functions of the CaEK and CCP,EK machines \nare identical modulo syn\u00adtax. However, in order to show that the evaluation of an A-normal form term \nM and its CPS counterpart on the respective machines produces exactly the same behav\u00adior, we also need \nto prove that there exists a bijection M between machine states that commutes with the transi\u00adtion rules. \nDefinition 5.2. (M, 7?, V, and K) M : Statec -Statea M((P, E-, Ek)) = (U[P], R(E-), K(Ek)) EnvC + Enva \nXI(E-) = E where E($) = V(E-(x)) Value. -Valuea v(c) =c V((cl klzl . . .~n, P, E-)) = (cl z,. . .rn,U[P],7?(E-)) \n- Conic * Cont. )qstop) = stop K((ar z, P, E-, Ek)) = (ar LZ,U[P], R(E-), K(E )) Intuitively, the function \nM maps C.P,EK machine states to C. EK machine states, and l?, V and K perform a similar mapping for environments, \nmachine values and continuations respectively. We can now formalize the previously stated requirement \nthat O and 0 behave in the same manner. Requirement For all W;, . . . . W: E Vaiuec, V(6. (0 , W:,..., \nw;)) = 6(0, V(W; ), . . . . V(wq)). The function M commutes with the state transition functions. Theorem \n5.3 (Commutativity Theorem) Let S E Statec: S ~c S if and only zf JU(S) @+. M(S ). fic s, s AA II II \nM(i ) ma M(s) Proofi The inverse CPS transformations U is bijec\u00ad tive [17]. Hence by structural induction, \nthe functions M, 7?, V and K are also bijective. The proof proceeds by case analysis on the transition \nrules. ~ Intuitively, the evaluation of a CPS term P on the CCP,EK machine proceeds in the same fashion \nas the evaluation of U [P] on the CaEK machine. Together with the machine equivalence theorem, this implies \nthat both machines perform the same sequence of abstract operations, and hence compilers based on these \nabstract machines can produce identical code for the same input. The A-normal form compiler achieves \nits goal in fewer passes.  6 A-Normal Forms as an Inter-A Linear A-Normalization mediate Language The \nlinear A-normalization algorithm in Figure 9 is written in Scheme extended with a special form match, \nOur analysis suggests that the language of A-normal which performs pattern matching on the syntax of \npro\u00ad forms is a good intermediate representation for compil\u00ad gram terms. It employs a programming technique \nfor ers. Indeed, most direct compilers use transformations CPS algorithms pioneered by Danvy and Filinski \n[9]. similar to the A-reductions on an ad hoc and incomplete To prevent possible exponential growth in \ncode size, the basis. It is therefore natural to modify such compilers to algorithm avoids duplicating \nthe evaluation context en\u00ad perform a complete A-normalization phase, and analyze closing a conditional \nexpression. We assume the front\u00ad the effects. We have conducted such an experiment with end uniquely \nrenames all variables, which implies that the non-optimizing, direct compiler CAML Light [15]. the condition \nz @ FV(S) of the reduction Al holds. This compiler translates ML programs into bytecode via a A-calculus \nbased intermediate language, and then in-Acknowledgments We thank Olivier Danvy, Preston terprets this \nbytecode. By performing A-normalization Briggs, and Keith Cooper for comments on an early on the intermediate \nlanguage and rewriting the inter\u00adversion of the paper. preter as a C. EK machine, we achieved speedups \nof be\u00adtween 50 ?ZOand 100 % for each of a dozen small bench\u00admarks. Naturally, we expect the speedups \nto be smaller References when modifying an optimizing compiler. A major advantage of using a CPS-based \nintermedi-[1] AHO) A., SETHI, R., AND ULLMAN, J. ate representation is that many optimizations can be \nCompile rs Prmciples, Techniques, and Took. expressed as sequences of /3 and q reductions. For Addison-Wesley, \nReading, Mass., 1985. example, CPS compilers can transform the non-tail [2] AIWIZL) A. Compiling with \nContinuations. Cam\u00adcall (W (Az.kx) WI . . w ~ ) to the tail-recursive call bridge University Press, 1992. \n(Wkw, . . . W.) using an q-reduction on the con\u00adtinuation [2]. An identical transformation [17] on the \n[3] BARENDREGT, H. The Lambda Calcu!us: Its Syn\u00ad language of A-normal forms is the reduction f?,d: tax \nand Semantics, revised ed. Studies in Logic and the Foundations of Mathematics 103. North\u00ad (let (x (V \nV, .. Vn)) %)+ (V V, ... Vn), Holland, 1984. where V, Vi,..., V. are the A-normal forms correspond-[4] \nBOEHM, H.-J., AND DEMERS, A. Implement\u00ading to W, W1, . . . . Wn respectively. Every other opti-ing Russel. \nIn Proceedings of the ACM SIG\u00admization on CPS terms that corresponds to a sequence PLAN 1986 Sympostum \non Comptler Construction of ~~-reductions is also expressible on A-normal form (1986), vol. 21(7), Sigplan \nNotices, pp. 186-195. terms [17]. [5] BONDORF, A. Improving binding times without The A-reductions also \nexpose optimization opportu\u00ad explicit CPS-conversion. In Proceedings of the 1992 nities by merging code \nsegments across block declara- ACM Conference on LMp and Functional Program\u00ad tions and conditionals. \nIn particular, partial evaluators ming (1992), pp. 1 10. rely on the A-reductions to improve their specializa\u00adtion \nphase [5]. For example, the addition operation and [6] CLINGER, W. The Scheme 311 compiler: An ex\u00ad the \nconstant O are apparently unrelated in the following ercise in denotational semantics. In Proceedings \nof term: the 1984 ACM Conference on Lzsp and Functional Programmmg (1984), pp. 356-364. (addl (let (3 \n(~ 5)) O)) [7] DANVY, O. Back to direct style. In Proceedings The A-normalization phase produces: of \nthe ith European Symposium on Programming (Rennes, 1992), Lecture Notes in Computer Sci\u00adence, 582, Springer \nVerlag, pp. 130 150. (let (z (f 5)) (addl O)), [8] DANVY, O. Three steps for the CPS transforma\u00ad which \nspecializes to (let (x (f 5)) 1). tion. Tech. Rep. CIS-92-2, Kansas State University, In summary, compilation \nwith A-normal forms char\u00ad 1992. acterizes the critical aspects of the CPS transformation relevant to \ncompilation. Moreover, it formulates these [9] DANVY, O., AND FILINSKI, A. Representing con\u00adaspects in \na way that direct compilers can easily use. trol: A study of the CPS transformation. Mathe-Thus, our \nresult should lead to improvements for both matical Structures in Computer Science, 4 (1992), traditional \ncompilation strategies. 361-391. (define na-rndize-terrn(M) (w-make M (lambda (z) (lambda z)))) (define \nnormalize (lambda (M k) (match M ( (lambda ,paranu ,My) (k (lambda ,pararns ,(norrnalize-terrn ZIody)))] \n~(let (,z ,MI) ,Mz) (rwrrmdize Ml (lambda (Nl) (let (,z ,Nl) ,(normalize M, k))))] ~(ifO ,MI ,Mz ,M3) \n(normalize-name Ml (lambda (t)(k (ifO ,t,(normalize-term M2) ,(norrmdize-term Ms)))))] ~(,Fn ,M*) (if \n(PrirnOp? Fn) (normcdwe-name* M* (lambda (t*) (k (,Fn . ,t )))) (normalize-name Fn (lambda (t)(normalize-nom. \nM* (lambda (t*) (k (jt . ,t )))))))]  [v (k v)]))) (define normalize-name (lambda (M k) (rzormahze \nM (lambda (N) (if ( Value? N) (k N) (let([t (neuwar)]) (let (,t ,N) ,(k t)))))))) (define normaiize-name \n(lambda (M* k) (if (null? M*) (k ()) (normalize-name (car M*) (lambda (t) (normalize-name (cxtr M*) \n(lambda (t*)(k (,t . at ))))))))) Figure 9: A linear-tilme A-normalization algorithm [10] FELLEISEN, \nM., AND FRIEDMAN, D. Control op-[15] LEROY, X. The Zinc experiment: An economical erators, the SECD-machine, \nand the A-calculus. [n implementation of the ML language. Tech. Rep. Formal Description of Programmmg \nConcepts III 117, INRIA, 1990. (Amsterdam, 1986), M. Wirsing, Ed., Elsevier Sci\u00ad [16] PLOTKIN, G. Call-by-name, \ncall-by-value, and the ence Publishers B.V. (North-Holland), pp. 19;3 A-calculus, Theoretical Computer \nScience 1 (1975),217. 125-159. [11]FESSENDEN, C., CLINGER, W., FRIEDMAN, [17] SAE+RY, A., AND FELLEISEN, \nM. Reasoning about D. P., AND HAYNES, C. T. Scheme 311 version 4 programs in continuation-passing style. \nIn Pro\u00adreference manual. Computer Science Technical R,e-ceedings of the 1992 ACM Conference on Lisp port \n137, Indiana University, Bloomhy.$on, Indi-and Functional Programming (1992), pp. 288 298. ana, Feb. \n1983. Technical Report 92-180, Rice University. [18] SHIVERS, O. Control-Flow Analysis of Higher\u00ad [12] \nFISCHER, M. Lambda calculus schemata. In Pro- Order Languages or Tamtng Lambda. PhD thesis, ceedings \nof the ACM Conference on Proving As\u00adCarnegie-Mellon University, 1991. sertions About Programs (1972), \nvol. 7(l), Sigplim Notices, pp. 104-109. [19] STEELE, G. L. RABBIT: A compiler for Scheme. MIT AI Memo \n474, Massachusetts Institute of Technology, Cambridge, Mass., May 1978. [13] KELSEY, R., AND HUDAK, P. \nRealistic com\u00adpilation by program transformation. In Confer\u00ad [20] WAND, M. Correctness of procedure representa\u00ad \nence Record of the 16th Annuai ACM Symposium tions in higher-order assembly language. In Pro\u00ad on Principles \nof Programming Languages (Austin, ceedings of the 1991 Conference on the Mathemat- TX, Jan. 1989), pp. \n281-292. ical Foundations of Programing Semantics (1992), S. Brookes, Ed., vol. 598 of Lecture Notes \nan Com\u00ad [14] KRANZ, D., KELSEY, R., REES, J., HUDAK, E ., puter Scaence, Springer Verlag, pp. 294-311. \nPHILBIN, J., AND ADAMS, N. Orbit: An op\u00ad [21] WEISE, D. Advanced compiling techniques. timizing compiler \nfor Scheme. In Proceedings of Course Notes at Stanford University, 1990. the ACM SIGPLAN 1986 Symposium \non Compi/er Construction (1986), vol. 21(7), Sigplan Notices, pp. 219-233.  \n\t\t\t", "proc_id": "155090", "abstract": "<p>In order to simplify the compilation process, many compilers for higher-order languages use the continuation-passing style (CPS) transformation in a first phase to generate an intermediate representation of the source program. The salient aspect of this intermediate form is that all procedures take an argument that represents the rest of the computation (the &#8220;continuation&#8221;). Since the nai&uml;ve CPS transformation considerably increases the size of programs, CPS compilers perform reductions to produce a more compact intermediate representation. Although often implemented as a part of the CPS transformation, this step is conceptually a second phase. Finally, code generators for typical CPS compilers treat continuations specially in order to optimize the interpretation of continuation parameters.</p><p>A thorough analysis of the abstract machine for CPS terms show that the actions of the code generator <italic>invert</italic> the nai&uml;ve CPS translation step. Put differently, the combined effect of the three phases is equivalent to a source-to-source transformation that simulates the compaction phase. Thus, fully developed CPS compilers do not need to employ the CPS transformation but can achieve the same results with a simple source-level transformation.</p>", "authors": [{"name": "Cormac Flanagan", "author_profile_id": "81100538763", "affiliation": "", "person_id": "PP14187273", "email_address": "", "orcid_id": ""}, {"name": "Amr Sabry", "author_profile_id": "81100016804", "affiliation": "", "person_id": "P16266", "email_address": "", "orcid_id": ""}, {"name": "Bruce F. Duba", "author_profile_id": "81100311653", "affiliation": "", "person_id": "P33389", "email_address": "", "orcid_id": ""}, {"name": "Matthias Felleisen", "author_profile_id": "81100323458", "affiliation": "", "person_id": "PP39037684", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/155090.155113", "year": "1993", "article_id": "155113", "conference": "PLDI", "title": "The essence of compiling with continuations", "url": "http://dl.acm.org/citation.cfm?id=155113"}