{"article_publication_date": "06-01-1993", "fulltext": "\n Compiling Real-Time Programs into Schedulable Code* S~~ngsoo Hong and Richard Gerber Department of Computer \nScience University of Maryland College Park, MD 20742 (301) 405-2710 s~hongf?fcs .umd. edu rlchrl)cs.umd.edu \nAbstract We present a programming language with first-class timing constructs, whose semantics is baaed \non time\u00adconstrained relationships between observable events. Since a system specification postulates \ntiming relation\u00adships between events, realizing the specification in a pro\u00adgram becomes a more straightforward \nprocess. Using these constraints, as well as those imposed by data and control flow properties, our objective \nis to transform the code so that its worst-csse execution time is consistent with its real-time requirements. \nTo accom\u00adplish this goal we first translate an event-based source program into intermediate code, in \nwhich the timing constraints are imposed on the code itself, and then use a compilation technique which \nsynthesizes feasible code from the original source program.  1 overview The construction of a hard real-time \nsystem can, in practice, be a painful process. Many factors conspire to make this the case, among which \nare inflexible schedul\u00ading paradigms and the lack of high-level programming language support. This last \nfactor frequently forces pro\u00adgrammers to use assembly language modules for some of the key components \nof their systems. Real-time perfor\u00admance is subsequently achieved by manually counting instruction-cycle \ntimes, hand-optimizing the code, and experimenting with various orderings of operations to help achieve \nschedulability. *This research is supported in part by NSF grant CCR\u00ad9209333 and DARPA contract Noo014-91-C-0195. \nPermission to copy without fee all or psrt of this material is granted provided that the copies are not \nmade or distributed for direct commercial advantage, the ACM copyright notice and the title of the publication \nand its date appear, and notics is given that copying is by permission of the Association for Computing \nMachinery. To copy otherwise, or to republish, requires a fee end/or specific permission. ACM-SIGPLAN-PLDl-6 \n/93/Albuquerque, N.M. 01993 ACM 0-89791 -598 -4/93 /0006 [01 66...$1.50 Recently, experimental languages \nhave been pro\u00adposed which provide first-class, real-time constructs [9, 12, 13, 14, 17]. An example of \nsuch a construct is within 10ms do B , where the block of code B must be executed within 10 milliseconds. \nThis constraint is, in turn, conveyed to the real-time scheduler as a direc\u00ad tive. These languages, while \nproviding a convenient framework for expressing time in programs, have done little to ease the process \nof translating a real-time spec\u00adification into schedulable code. Thus, their timing con\u00adstructs have \nnot been adopted in any production-level programming languages. We believe the reason is straightforward: \nLanguage constructs such as within 10ms de) B establish con\u00adstraints on blocks of code. However, true \nreal-time properties establish constraints between the occurrences of events [5, 10]. These constraints \ntypically arise from a requirements specification, or from a det ailed anal\u00adysis of the application environment. \nWhile language\u00adbased constraints are very sensitive to a program s ex\u00adecution time, specification-based \nconstraints must be maintained regardless of the platform s CPU character\u00adistics, memory cycle times, \nbus arbitration delays, etc. Our approach is to treat a real-time program as (1) an event-based, timing \nspecification, which represents the system s real-time requirements; and (2) a func\u00adtional implementation, \nthat is, the system s code. In\u00adstead of presenting yet another real-time program\u00adming language, we carry \nout this approach by using constructs quite similar to those found in the above\u00admentioned languagea. \nHowever, in our approach their interpretation is quite different. Instead of constrain\u00ading blocks of \ncode, the timing constructs establish con\u00adstraints between the observable events within the code. As \nan example, consider the following specification frag\u00adment, which is rendered pictorially in Figure 1: \n(1) The motion-sensor emits obj .coords on port p, (2) Transformation function F converts obj .coords \ninto next .cmd for controHer. (3) The controller receives next _cmd on port q,  rthM0r6@dto3.51m \nfrml ntwr 00n&#38;llu tion under the traditional semantics. If F is a CPU\u00ad t p---... \\ ! ./ i .,::,:,,,,,:,,,:,,, \n:,,,:,: ,M>;J&#38;&#38;...,:,, .Iwt.anld m F(Ob@Ords) ,,,,,,:,,,,.,Y ,W* ,,,,, ,, ,,, . : , , ..................... \n... .... ....................................... . lw. thm0r*wa!t04.Om* Figure 1: Event-Based Specification \nof Sensor-Controller System (4) To achieve steady state, transmission of next.cmi is made no earlier \nthan 3.5 ma after receipt of obj-coords. (5) To guarantee response-time threshold, transmis\u00adsion of \nnext .cmd is made no later than 4.0 ms af\u00adter receipt of obj -coords.  Within our event-based framework, \nthe following pro\u00adgram fragments realize the specification: /* Program A*/ do receive(p,obj-coords); \nstart after 3.5 ms finish within 4.o ms { next-cmd = F(obj-coords); send(q,next-cmd); } ...................... \n/*ProgramB*/ do { receive(p,obj-coords); next-cmd = F(obj-coords); } , start after 3.5 ms finish \nwithin 4.0 ms send(q,next-cmd);  The system s only observable events occur instan\u00adtaneously during the \nexecutions of the send and receive operations. The do statement estab\u00adlishes timing constraints only \nbetween these two op\u00aderations. On the other hand, the local statement next-cmd = F(obj .coords ) is \nonly constrained by the program s natural control and data dependence. Armed with this interpretation, \nour compiler treats both programs as having equivalent semantics! This is quite different from the approaches \nmentioned above, where timing constructs establish constraints on code. In that interpretation, program \nA would first receive its data, then delay for 3.5 ms and finally, evaluate F and send the result within \nthe remaining 0.5 me. Program B would receive its data, evaluate F, then delay for 3.5 ms and finally, \nsend the result within 4.0 ms of evaluating F! Both programs will fail to implement the specifica\u00ad intensive \nfunction (and thereby requires over 0.5 ma of execution time), program A is inherently unschedula\u00adble, \nOn the other hand, program B establishes a con\u00adstraint between the evaluation of F and the send oper\u00adation, \nand not between the two specified events. Both programs would have to be rewritten to achieve the de\u00adsired \neffect. The necessary corrections would include manually decomposing F, as well as adjusting the tim\u00ading \nconstraints. The actual changes would heavily de\u00adpend on the particular characteristics of the computer, \nand thus, the very reason for using high-level timing constructs would be defeated. There are several \nimmediate benefits to our seman\u00adtics for real-time constructs. First, a source program is not hardware-specific, \nand thus maintains the abstract, portable spirit of a high-level language. Since the tim\u00ading constraints \nrefer only to specification-based events, they need not be hand-tuned for an individual CPU. Second, \nthis decoupling of timing constraints from code blocks enables a more straightforward implementation \nof an event-based specification. But of most importance, much of the arduous, assembly-language level \nhand-tuning can now be accom\u00adplished automatically by compiler optimization tech\u00adniques. In this paper \nwe present several such techniques, most of which rely on code-motion methods similar to those used in \ninstruction scheduling. Here, however, the objective is to achieve consistency between the real\u00adtime \nconstraints and the execution characteristics of the code. In doing this we use the observable events \nas signposts, which constrain the places where code can be moved. These events, as well as data dependence, \nestablish the limiting constraints for the optimization algorithm. From Source Programs to Schedulable \nCode. While an event-based semantics makes sense at the source-program level, most real-time schedulers \nonly ac\u00adcept timing constraints on the start and finish times of tasks. Thus, the role of the compiler \nis to transform event-driven source programs into constrained blocks of code. The challenge is to achieve \na task set which is fea\u00adsible, i.e., whose tasks have execution times consistent with the timing constraints. \nFor example, Program A is infeasible if F executes longer than 0.5 ms. As a SO\u00adlution, F(ob j _coords \n) can be inlined, and then de\u00adcomposed into two parts one which is flow-dependent on the parameter obj \n-coords, and another which is invariant of ob j -coords. This second part can be lifted out of the do \nstatement altogether. The compilation stages are outlined in Figure 2. The machine-independent pass translates \nthe source pro\u00adgram into intermediate code in single static assignment form (for an overview of the SSA \nform, see [3]). ~-.--.\u00ad   Ipwf&#38;=J s- p### L..+ . .---\u00ad -1 ! =m @?nbll Figure2: Stages of the Compilation \nProcess Then, consistency analysis is performed on the tim\u00ading constraints themselves, under the assumption \nthat the code s execution time is O. The constraints are con\u00adsistent if and only if the within modifiers \ndo not contradict the after modifiers. As a simple exam\u00adple, consider Program A. If start after 3.5 ms \nwere changed to start after 4.1 ins, the program would be inconsistent. Since timing constructs can be \narbitrarily nested, as well as placed within conditionals, less triv\u00adial inconsistencies can easily arise. \nWhen a program is found inconsistent, no amount of optimization will help it adhere to its timing constraints \n thus, a program\u00adming error is reported. Program synthesis is the process of relocating unob\u00adservable \ninstructions between observable events, while maintaining functional correctness and attempting to achieve \nfeasibility y. The synthesis phase requires, in ad\u00addition to the code itself, an event specification \nand a set of timing parameters. The event specification denotes the observable events within the program; \nfor example, ports connected to external devices, or variables corre\u00adsponding to memory-mapped IO. In \nthe sequel, we con\u00adsider all send and receive) operations to be observ\u00adable. That is, observable events \noccur instantaneously during the executions of all send and receive op\u00aderations. The timing parameters \nare the minimum and maximum execution times for each instruction on the given CPU. These times are generated \nby a timing anal\u00adysis tool, such as those found in [15, 18, 19]. Our synthesis phase depends heavily \non the tech\u00adnique of code motion, one of the most frequently used transformations in optimizing compilers \n[4, 7, 8]. Our algorithm pushes this transformation to the limit, in that it greedily performs inter-basic \nblock optimiza\u00adtion. Such a technique proves quite fruitful when at\u00adtempting to achieve real-time feasibility. \nThe code generation pass completes the compilation of a real-time program. This module is the back-end \nof a C compiler, and it allocates registers, generates machine-dependent code, etc. In this paper we \nonly treat the synthesis of sequen\u00adtial programs, albeit with an arbitrary degree of nested timing constraints. \nHere, we strive only to make a se\u00ad quential program s constraints consistent with its exe\u00adcution time. \nThis is certainly a necessary condition to achieve real-time schedulability. In our future work, we plan \nto extend our approach to concurrent programs as well, where code transformations will be made using \nguidance from the real-time scheduler. z Syntax and Semantics of the Language Constructs In this section \nwe introduce two simple language con\u00adstructs to express timing constraints within a program. Both constructs \nare syntactic descendants of the tempo\u00adral scope, first introduced in [13]. However, as we have stated, \nour semantics is quite different, in that it relies on constrained relationships between observable events. \nWe first elaborate the do construct used in Sec\u00adtion 1, which establishes several types of relative timing \nconstraints. Its general form is as follows. do (reference block) [start after tmin][start before tm..i] \n[finish within t~acz ] (constraint block) The reference block (R13) and the constraint block (CB) are \nsimply C statements, or alternatively, timing constructs themselves. The do construct induces the following \ntiming constraints: e start after tmin:There is a minimum delay of tminbetween the last event executed \nin the RB, and the first event executed in the CB. start before tm==l: There is a maximum delay of t~a.l \nbetween the last event executed in the RB, and the first event executed in the CB. e finish within tmaz2:There \nis a maximum delay of tmaz2between the last event executed in the RB, and the last event executed in \nthe CB. Since either block may contain conditionals, depending on the program s state there may be several \nsuch events executed either first or last . For example, consider the fragment from a typical control \nflow graph in Fig\u00adure 3. Depending on the path taken, the last event executed in the reference block \nmay be either El or E2. Simi\u00adlarly, the first event in the constraint block will be E3 or E4, while the \nlast event will be either E4 or E5. To denote such possibilities, we introduce two mappings FIRST and \nLAST, from code blocks to sets of events. That is, LAST (RB) = {El, 132}, FIRST(CB) = {E3, 134} and LAST \n(CB) = {JY4, J!?5}. Thus, the do construct introduces two potential constraints between LAST(RB) and \nFIRST(CB), as well as one constraint between LAST(RB) and LAST(C B). (2) Each message contains the dimensions \nof an object Rdema B-(RB) m currently approaching the robot, If no object is M approaching, the message \nis tagged as null. B] Sz. B3 (3) The controller must receive every message. m M (4) To achieve steady-state, \nthe controller delays at Ss least 1.5 rns after receiving the message, Comlr.imtBbck (Cll) m M BI M B9 \n+ Figure 3: Typical Control Flow Graph Figure 4: Behavior of Periodic Timing Construct The second real-time \nconstruct denotes a statement with cyclic behavior of a positive periodicity: every p [while (condition) \n] [start after t~,~ ] [start before t~~.1 ] [finish within t~a.z ] (constraint block) As long as the \nwhile condition is true, the observ\u00adable events in the constraint block execute every p time units. Akin \nto an untimed while-loop, when the condi\u00adtion evaluates to false the statement terminates. In its real-time \nbehavior, the interpretation of the every construct is similar to that of do. For example, as\u00adsume that \nthe statement is first scheduled at time t,and that the while condition is true for periods O through \ni. As depicted in Figure 4, the following constraints are induced for period i: start after tmin:The \nfirst event executed in the CB occurs after t+ ip + tmin.  start before tm..l:The first event executed \nin the CB occurs before t+ ip + tma=l.  finish within tm.=z: The last event executed in the CB occurs \nbefore t + ip + tma=z.  As we have stated, timing constraints may be arbi\u00adtrarily nested. For example, \nconsider the two-arm robot control program in Figure 5(A), which monitors a con\u00adveyer belt and gives \ncommands to the robot s arms. The specification is as follows: (1) Every 10 ms, a position-sensor sends \na message to the controller. (5) If an object was detected, new commands are sent to arml and arm2. \n(6) Both commands must be sent within 4.0 ma of receiving the sensor s message, and within 8.0 ms of \nthe beginning of the period.  Figure 5(B) shows the program s representation in the form of its control-flow \ngraph (CFG), which is gen\u00aderated by the compiler s machine-independent pass, Af\u00adter this phase is complete, \nthe resulting code would be in SSA form; however, for the sake of brevity, here we have left the code \nin its original C form. We extend the traditional structure of a CFG to help represent both real-time \nconstraints and observable events. Thus a node may be one of the following: (1) a basic block (i.e., \nstraight-line code consisting of internal computation statements); (2) an observable event; (3) an entry \nnode, denoting the start of a timing block, e.g., a reference or constraint block; or, (4) an exit node, \ndenoting the end of a timing block. 3 Program Synthesis The synthesis phase involves repeatedly applying \nopti\u00admizing transformations to each timing construct, until either the program achieves feasibility, \nor is determined to be infeasible. This must account for a program s nested timing constraints, as well \nas loops, procedure calls, etc. We approach the synthesis problem in a bottom\u00adup manner, and manage constructs \nfrom the innermost to outermost level of nesting. After a timing constraint (or a loop) has been synthesized, \nit gets treated as a single node in the surrounding control flow graph. Of course, to claim that loops \ncan be reduced into single represent atives, the flow graph of the program must be reducible [1]. Since \nstructured programs without un\u00adrestricted goto s lead to reducible flow graphs, without loss of generality \nwe assume that our programs possess this property. While our algorithm is bottom-up, there is also a \ntop-down component to handle nested constraints. That is, the innermost constructs are synthesized first. \nWhen their timing constraints are determined feasible, they are reduced to single representatives. Then, \nthe surrounding construct is handled. If thislevel is found infeasible, the inner nodes are opened up \nonce again, and more aggressive optimization is carried out. . i .  II [-1;1s4  every 10 ms finish \nwithin 8 ms do { receive (Sensor, dim); msg.cnt ++;  } start after 1.5 ms finish within 4 ms { if (!null(dlm)) \n{ Z1 = convert (dim, 10C1); Z2 = convert (dim, loc2); send(arml, z1); send(arm2, z2); } } Figure 5: \n(A) Source Code for Robot In this section we discuss the heart of our algorithm, which is our approach \nto synthesizing a single timing construct. We do this in a four-step process. First, the construct is \ndecomposed into several sections, de\u00adnoted by its control flow structure (Section 3.1). Next, code based \ntiming constraints are derived from the con\u00adstruct s event-based timing constraints (Section 3.2), and \nchecked for their consistency with the execution time. Finally, code-scheduling transformations are used \nto reduce the worst-case execution time of the infeasible sections (Section 3.3). 3.1 Section Generation \nA timing construct is divided into five code sections, as portrayed in Figure 6. As can be seen, the \nreference block is decomposed into three sub-blocks. The unob\u00adservable code before the first observable \nstatement be\u00ad comes an interface section (S1). The code cent aining the observable statements becomes \nthe reference sec\u00adtion (S2). The unobservable code after the observable statements becomes the first \npart of the delay section (S3). Consequently, the topmost unobservable code of the constraint block becomes \nthe second part of S3, and so on. Recall the discussion of the FIRST and LAST func\u00adtions in Section 2. \nSince a code block may contain Is Ftefermce Block @~ ~ II II reewe(saww,.srn) II II II Ine.g@l++ II \nII L_    r-----&#38; l&#38; i l ii 1, II 1, II II II 1, 1, II 1, J, . I i________ _________! + e \ntuiut-1 tnmu Sxlt Ndc Sntiy Node Controller, and (B) Control-Flow Graph complicated control structures, \nwe require a convenient means of defining the boundaries of S2 and S4 the sec\u00adtions that contain observable \nevents. For exmple, the boundary between S1 and S2 should be an unobserv\u00adable statement that most closely \ndominates all possible events first executed in S2 (i.e. 171RS T(S2)). Simi\u00adlarly, the boundary between \nS2 and S3 should be an un\u00adobservable statement that most closely post-dominates the events in LAST(S2). \nWe delimit section boundaries through the use of pseudo-events -domFirst(B), domLast(B), pdomFirst(13) \nand pdomLast(B) which consume no time and are not visible. For a block B, domFirst(13) is a pseudo-event \ninserted directly af\u00adter the unobservable instruction most closely dominat\u00ading F1RST(13). On the other \nhand, pdomIW st(13) is inserted directly before the unobservable instruction most closely post-dominating \nFIRST(B). The pseudo\u00adevents dornLast(B) and pdomLast(B) are similarly de\u00adfined. For example, consider \nthe constraint block in Fig\u00adure 6. Although B9 also post-dominates LAST(~B), itconsumes non-zero execution \ntime. Thus, its logical place is in the interface section S5, which is not sub\u00adject to the construct \ns timing constraints. Hence the need for the pseudo-event pdomLast (CB), which is the *WRdewmce BI ) \n-fm  S1 - . . -@S) B1 B2 B2 B3 ---- S4 .-.-... pdO&#38;uqRB) m -... M .-. dLwmn@21) B7 B4 B: . \n. . .-. . p&#38;+n@qa) imtOcc B9 s -E!!f!_l o~ Figure 6: The CFG of a Timing Construct and its Sec\u00adtion \nDivision. unique exit point for the constrained section S4. To properly decompose the original timing \nblock, the compiler ensures that each path in a timing block has at least one event. This is done by \ninserting imag\u00adinary events on paths which do not have any event. Since imaginary events do not impose \nany data or con\u00adtrol dependence on local computations, the movement of instructions on such paths is \nstill restricted only by the original dependence. Observe that imaginary event insertion makes the dominator \nand the post-dominator of a timing block always post-dominate the entry and dominate the exit, respectively. \nThough we use domFirst(CB) and pdomLast(CB) aa the upper and the lower boundaries of the section S4 in \nFigure 6, there are cases where domLast(CB) can be used for the upper boundary, or pdomFirst(CB) can \nbe used for the lower boundary. Such cases occur when either both the start after and the start before \ncon\u00adstraints are missing, or the finish within constraint is missing. Of course, if all these constraints \nare miss\u00ading, then S4 becomes an empty section, since the two pseudo-events representing the boundaries \nmeet. 3.2 Deriving Code-Based Timing Ccm\u00adstraints At this point, code-based timing constraints are derived \nfrom the language-based timing construct. That is, us\u00ading the code s execution-time characteristics, \nwe trans\u00adform the event-based timing constraints into temporal relationships between the start and finish \ntimes of the sections. These new code-based timing constraints must satisfy the original semantics of \nthe event-based con\u00adst raints. To do this, we first make two definitions: For nodes a and b in the control-flow \ngraph, Pa.ths(a, b) is defined as the set of all paths start\u00ading from a and ending at b.  For a fragment \nof code c, wt (c) is defined as the worst-case execution time of c.  We use the timing schema approach \nin [19] to deter\u00admine the wt function. Now, consider a typical con\u00adstruct such as do RB start after tmin \nstart before t~azl finish within tma=2 CB. In establishing the code-based timing constraints, is not \nsufficient to sim\u00adply postulate that, for example, S4 must start within a maximum delay of tma=lafter \nS2 ends. The reason is that there are many possible events in LAST(S2) and FIRST(S4), and each may execute \nat a different time. Yet the event-based semantics is clear: the time between any event in LAST(S2) and \nany event in FIRST(S4) is at most tm==l.To guarantee that this occurs, we must account for all possible \nexecution scenarios. Specifi\u00adcally, we must compensate for the maximum amount of execution time between \nan event in LAST(S2) and pdomLast(S2), as well as the maximum amount of ex\u00adecution time between domFirst \n(S4) and an event in FIRST(S4). We must similarly adjust tma=2. To do this, we make the following definitions: \no A~wn~ir.9t(~) % maz{wt(p) I e G FIRST(B), P E Paths(domFirst(B), e)}. Apciornw(B) % maz{wt(p) I e E \nLAST(B), p G Paths(e, pdomLast(B))}. Now, the code-based timing constraints can be adjusted as follows, \nwhere we assume that the code has been decomposed into its sections S1 -S5. (1) S4 starts at least Tmin \nafter S2 ends, where Tmin = tmin. (2) S4 starts at most Tmazl after S2 ends, where T~aZl = tmacl _ ApdO~Ld.t(S2) \nA&#38;mFir$* (S4). (3) S4 ends at most T~a.z after S2 ends, where Tma=x = tma=z &#38;omLa,,(S2),  \n These timing constraints are strong enough to guarantee the original event-based timing constraints, \nAlong with the precedence ordering of the five sections, these de\u00adrived constraints yield the following \nconditions for fea\u00adsibility y: (1) wt(S3) ~ Tmdxl. (2) wt(S4) < Tmanz maz{wt(S3), T~in}.  In the \nnext subsection we discuss our code-scheduling techniques to handle the csses in which one of these conditions \nfails to hold. code. In doing so, we attempt to achieve the following Section ] Duration Constraint (D \nUl?(.S)) I . S1 none (co) S2 none (co) S3 T~azl S4 Tmazz maX{~~in, ~t(S3)} S5 none (co) Table 1: Timing \nconstraints of each section. Using the derived constraints, we annotate the sec\u00adtions with their code-based \ntiming properties. We adopt mechanism similar to that found in Flex language [11], in which blocks are \ndesignated with a label, and tagged with two sublabels, start and finish. Timing con\u00adstraints are expressed \naa conjunctions of linear inequal\u00adities between the start and finish sublabels of differ\u00adent sections. \nFor example, the constraint expression of S4 is as follows: S4.start > S2.finish + T~,~, S4.start ~ S2.finish \n+ T~~zl, S4.finish ~ S2.finish + T~~~2. Recall the robot controller program from Fig\u00adure 5(A). Figure \n7(A) illustrates the intermediate pro\u00adgram after sections are generated, and the derived tim\u00ading constraints \nare annotated. Again, for the sake of brevity we have left the code in its C form instead of converting \nit to SSA form.  3.3 Code Scheduling The final step is to rearrange instructions across sections in \nsuch a way that all the sections satisfy their derived timing constraints. Such a process is similar \nto that of code scheduling, which is a well-defined problem for au\u00adtomatic fine-grain (instruction level) \nparallelization for superscalar and VLIW processors [2, 6, 7, 8, 16, 20]. However, we cannot directly \nuse the techniques such as Trace Scheduling [7] or Percolation Scheduling [2, 6], as our problem context \nhas a different goal. In what follows, we present a code scheduling algorithm which achieves the advantages \nof both techniques by the means of SSA form translation and section-wise trace schedul\u00ading. To summarize \nthe results of the last section, only sections S3 and S4 are subject to maximum duration constraints \n(ss shown in Table 1). The code scheduling problem involves copying or re\u00adlocating unobservable instructional \nto new locations, while preserving the functional semantics of the original goal: o Satisfy wt(Si) < \nDUR(Si) for i = 3,4. The algorithm works by inspecting the sections S4 and S3, checking whether they \nsatisfy the goal. If either vi\u00adolates its duration constraint, the algorithm attempts to reduce the surplus \nexecution time of the section. To accomplish this, we have adapted a technique from the approach to Trace \nScheduling found in [7]. In our al\u00ad gorithm, instructions lying on paths that exceed their section s \nduration constraints are considered for code motion. We distinguish such paths as cm tical traces. Formally, \nthe critical trace t of section Si is defined as a path starting from the head of Si and ending at the \ntail such that wt(t) > D UIZ(Si). Figure 8 sketches the algorithm. First, note the com\u00adputation of D \nUR(S3). Since D UR(S4) partially depends on wt(S3), the algorithm requires the assumption that wt(S3) \nwill not change. Unfortunately, this may not al\u00adways be the case if some code of S4 is moved into S3. \nTo adjust, D UR(S3) is set to the previous wt(S3) in one of the cases; otherwise, the synthesis of S4 \nwould be no longer valid. Note also that the code of S3 is moved to S1, while that of S4 is moved to \nS3. We dis\u00adallow code from being moved into S2, because it would change Apdomhst (S2), which we assume \ninvariant. Figure 7 illustrates the code scheduling process for our robot example. The duration constraint \nfor section S4 is violated, since its predicted worst case execution is 2.82 ms (the rightmost column \nof Figure 7(A) shows a range for execution time of each statement). How\u00adever, S4 is allowed only 2.1 \nms to execute its body.z Figure 7(B) yields the result of code scheduling: an as\u00adsignment is moved into \nS3, and the test guarding it is copied. After the transformation, the implementation satisfies the necessary \ncondition for feasibility, since the body of S3 requires at most 1.82 ms. In addition to such an instant \nbenefit, the transfor\u00admation converts the possibly wasteful delay into use\u00adful computation time, since \nthe new code in S3 can be scheduled within the delay interval between S2 and S4. Instructions on a critical \ntrace t may only be moved if they maintain the data-flow integrity of the program. To maintain the property, \nwe first compute the data precedence DAG for t.Since the graph imposes only a partial ordering on instructions, \nthere might exist sev\u00aderal instructions which have no incoming edges. In this case, the worst-case execution \ntimes and depths3 of such instructions prioritize the selection of instructions to be moved. 22.lms \n= S4.fiuish-S4.start = (S2.finish+3.6ms) - (S2.finish+ 1We conservatively prohibit event-generating instructions \nfrom 1 .Sms) being moved, so that the timing relationships between events is 3 The depth of a node is \nthe maximum length of all paths from preserved. the node to those having no successors. /* A.*/ S6: \n(S6.start[p]~p x 10ms, S6.finish[p]<px 10ms+8ms) { Sl: {/* null*/} S2: { receive(Sensor, dim); } [0.2ms,0.4ms] \nS3: { msg.cnt ++; [0.01 ms,0.02ms] c = !null(dim); [0.lms,O.2ms] } S4: (S4.start~S2.finish+ l.5ms, S4.finish<S2.finish+ \n3.6ms) { if (c) { [0.01 ms,0.02ms] Z1 = convert (dim, Iocl); [0.5ms,l.Oms] 22 = convert (dim, 10C2); \n[0.5ms,l.Oms] send(arml, z1); [0.2ms,0.4ms] send(arm2, z2); [0.2ms,0.4ms] } S5: \\/* null*/} } Figure \n7: Program synthesis: (A) After section 3.3.1 The Bookkeeping Problem The most formidable problem with \nTrace Scheduling is code explosion, which results from the bookkeeping in\u00ad structions required to maintain \nthe program s semantic integrity. This is illustrated in Figure 9. After an assign\u00ad ment is moved up \non a given critical trace, it is copied into all jozns4 having originally preceded it, but now fol\u00adlowing \nit (transformation from (A) into (B)). Similarly, after a test (split5) is moved, the code having originally \npreceded it, but now following it, is copied into its asso\u00ad ciated branch (transformation from (C) into \n(D)). Such unrestricted bookkeeping occurs because Trace Schedul\u00ad ing fails to first consider data and \ncontrol dependence between instructions lying off of the selected trace. In addition to the code explosion \nproblem, book\u00adkeeping is additionally undesirable in our case, since it could potentially increase the \nworst case execution times for non-critical traces, and thereby make them critical. Fortunately, we can \navoid unnecessary bookkeeping for joins, if the following conditions are satisfied: 1. The new position \nof the moved instruction domi\u00adnates its original position. 173 /* B.*/ S6: (S6.start[p]~pxlOms, S6.finish[p]<p \nx 10ms+8ms) S1: { {/* null*/} S2: { receive(Sensor, dim); } S3: { msg-cnt ++; c = !null(dim); if (c) \nZ1 = convert (dim, 10C1); } S4: (S4.start~S2.finish+ l.5ms, S4.finish<S2.finish+3 .6ms) { if (c){ 22 \n= convert(dim, 10C2); send(arml, z1); send(arm2, 22); } S5: i/* null */ } } generation, and (B) After \ncode scheduling 2. The instruction is free of data dependence with any other instruction on paths starting \nfrom its new position, and ending at its original position. Code motion in our framework always satisfies \ncondition (l), since an instruction is moved beyond the head of its original section. Fulfillment of \ncondition (2) can easily be tested using SSA form. Figure 1O(A) and (B) demonstrate our approach to bookkeeping. \nThe initial translation to SSA form produces # functions at the join node containing q = r+l , where \nmultiple values of the single variable a merge. If any instruction potentially uses a value of a, and \nis then moved beyond a join node having # for a, a bookkeeping copy is only made at that particular join. \nIn Figure 1O(A), the join (or equivalently joining path) containing a3 = a2\u00b0 falls into the place which \nneeds bookkeeping copy of x = a3+i 6. Further, we can avoid bookkeeping for splits by copying a test \ninstruction, and leaving the original test behind instead of moving it (unlike the approach in [7]). \nIn Figure 1O(D), no bookkeeping is needed, since noth\u00ading has been changed in code having data precedence \nfrom x = a+ 1 as a result of copying if c . Bookkeeping copies of an instruction leave multiple 6a3 = \na2 is a part of ~ function for a. Algorithm Synthesis(T) /* T is a timing construct */ input: the ordered \nset of sections {S1, S2,. ... S5} in T { S3.old = S3; dur = T~a.2 -~~~{Tmin, w~(s3)}; compute tsuch that \nd(t)= maz{wt(path) I path in S4}; whHe (wt(t) > dur) { /* t is a critical trace */ perform Code-Scheduling \ninto S3 on t; if (t is still critical) then exit ( Unable to synthesize, ); recompute tsuch that wt(t) \n= max {wt(path) I path in S4}; } if (no code was moved into S3) then dur = Tma=I; else if (wt(S3-old) \n> T~~sI ) dur = T~~~l; else dur = maz{Tm,n, wt(S3.old)}; compute t such that wt(t) = maz{ wt(path) I \npath in S3}; while (d(t)> dur) { perform Code.Scheduling into S1 on t; if (tis still critical) then \nexit( Unable to synthesize.))); recompute t such that wt(t) = rrmz{wt(puth) I path in S3}; } } Figure \n8: Algorithm for the synthesis of a timing construct, assignments on a single variable in the program, \nand On the other hand, transformations which are strict in thereby break the SSA form assumption. Consequently, \nexecution time (ET) are conservative with respect to bookkeeping mandates additional transformations \ntore-decreasing the real execution time of the program. store the SSA form of the program after code \nmotion Figure 11 shows our three basic transformations. is applied. To be specific, bookkeeping on a \njoin leads Their application strategy is summarized as follows: the associated join node to become the \npoint where the 1. Type 1 code motion is applied in an unrestricted values defined by the moved instruction \nand its book\u00admanner (Figure 1l(A)). keeping copy merge. Thus, a new # function is inserted 2. Type 2 \ncode motion is attempted next, since itat that join node. is likely to preserve the WCET of the entire \npro-Restoration of SSA form is completed only after re\u00adgram (while decreasing the criticality of a con\u00adname \ntransformation assures that a unique name is as\u00adstrained section). In Figure 11(B), for instance,signed \nto the target variable of each assignment. Fur\u00adif the movement of y = b+ i follows that of xther, all \nuses reached by that assignment must be cor\u00ad= a+ i the resulting code has the same WCETrectly renamed. \nA naive approach is to repeatedly per\u00ad(Figure 11(B)). form the rename transformation immediately after \neach code motion. Fortunately, we may postpone applying 3. Type 3 code motion is attempted only when \nthe rename transformations until the end of scheduling an current critical trace is still critical, and \neither entire critical trace. The generated ~ functions alone type 1 or 2 motion is no longer applicable. \nWhile can sufficiently represent data dependence information transformations of type 3 result in increasing \nthe needed for successive code motions, even before the re-overall WCET, they decrease the criticality \nof a name t ransformat ion. section which is still a net gain in hard real-time systems (Figure 11(C)). \n 3.3.2 Types of Code Motion 3.3.3 Eliminating Redundant Instructions We classify transformations based \non the notion of strictness. Transformations which are strict in worst Movement of an instruction may \nleave several book\u00adcase execution time (WCET) are conservative with re-keeping copies which are identical \nexcept for some re\u00adspect to decreasing the WCET of the entire program. named variables. Percolation Scheduling \n[2] addresses no (A)  (B) nace Nm-3hec saaimnOwldwy %+ x-a+1 MO -\u00ad(c) * / / #*H X.4+1 y.x+l (m Boo&#38;upimshmctbn \nx-a+1 ya+l Figure 9: Bookkeeping: (A) Original code, (B) Join bookkeeping, (C) Original code, and (D) \nSplit bookkeeping. Figure 10: (A) Original code, (B) Hc x =a+l\\ X.x+l \\q%w f\\ -+\u00adl+-;c ,// WIOvA / \n  I--\\ +\u00ad \\ ,L08Wd \\ / y.b+l $ (A)Type 1 w m (0(m lhx  NLm-mcf 0 4\u00ad  s8c&#38;uB0wI&#38;y o nOOk&#38;qing \nInsnutwl  Figure 11: (A) Strict code motion, (B) Strict in WCET but nonstrict in ET, and (C) nonstrict \ncode motion. this problem, and exploits the strategy of unification transformation as a solution. This \ntransformation al\u00adlows identical instructions from different nodes in a CFG to be merged, if possible. \nUnlike the original Trace Scheduling algorithm, we can easily incorporate unifica\u00adtion transformations \ninto our framework, while exploit\u00ading the advantages of Trace Scheduling (e.g., efficiency and simplicity). \nThis is possible because all instruc\u00adtions moved from a section share the same destination section. For \nexample, when a bookkeeping instruction is being moved to a section, the compiler checks if its identical \ncopy is already there. If so, the compiler sim\u00adply discards the instruction being moved (which is the \ninverse transformation of bookkeeping); otherwise, the compiler moves the instruction into the appropriate \nsec- Moving an assignment, (C) Original code, and (D) Copying a test. tion. Similar optimization can \nbe done for creating test instructions. After the compiler haa scheduled all traces tion, it may also \nleave some redundant ~ A ~ function is redundant if all bookkeeping tions which originally required the \n~ function in a sec\u00adfunctions. instruc\u00adhave been (C)TYPO 3 moved. The compiler, therefore, deletes the \n# function, and renames all references to the target variable of the ~ function into that of the original \ninstruction. 4 Remarks The reason we use the trace scheduling approach is straightforward: Optimizing \nto avoid hard real-time ex\u00adceptions demands scheduling only the critical traces, and no others. Besides \nit is very cheap and efficient for the compiler to move code only on a trace compared with relocating \nthe code in the trol flow graph. However, the code explosion with trace scheduling mandated our technique \nbookkeeping. The proof that our algorithm terminates at a time, whole con\u00adassociated to reduce is similar \n to the termination proof for trace scheduling found in [16]. In our case termination will either occur \nwhen all movable code is depleted, or preferably, when all traces in a section become non-critical. 175 \n References [13] Insup Lee and Vijay Gehlot. Language con\u00ad [1] [2] [3] [4] [5] [6] [7] [8] [9] [10] [11] \n[12] Alfred V. Aho, Ravi Sethi, and Jeffrey D. Unman. Compilers: Principles, Techniques, and Tools. Ad\u00addison \nWesley Publishing Company, 1986. Alexander Aiken and Alexandru Nicolau. A de\u00advelopment environment for \nhorizontal microcode. IEEE fiansactions on Software Engineering, pages 584-594, May 1988. R. Cytron, \nJ. Ferrante, B, K. Rosen, M. N. Weg\u00adman, and F. K. Zadeck, Efficiently computing static single asignment \nform and the control de\u00adpendence graph. ACM fiansactions on Program\u00adming Languages and systems, 9:319 \n345, July 1987. Ron Cytron, Andy Lowry, and Kenneth Zadeck. Code motion of control structures in high-level \nlan\u00adguages. In Conference Record 19th Annual ACM Symposium on Principles of Programming Lan\u00adguages, pages \n70-85. ACM, January 1986. B. Dasarathy. Timing constraints of real-time sys\u00adtems: Constructs for expressing \nthem, method for validating them. IEEE Transactions on Sofiware Engineering, 11(1):80-86, January 1985. \nKemal Ebcioglu and Alexandru Nicolau. A global resource-constrained parallelization tech\u00adnique. In International \nConference on Supercom\u00adputing, pages 154-163. ACM, June 1989. Joseph A Fisher. Trace scheduling: A technique \nfor global microcode compaction. IEEE Transactions on Computer, 30:478 490, July 1981. Franco Gasperoni. \nCompilation techniques for VLIW architectures. Technical Report RC 14915 (#66741), IBM T. J. Watson Research \nCen\u00adter, September 1989. Y. Ishikawa, H. Tokuda, and C. W. Mercer. Object-oriented real-time language \ndesign: Con\u00adstructs for timing constraints. In Proceedings of 00PSLA-90, pages 289-298, October 1990. \nF. Jahanian and Al Mok. Safety analysis of timing properties in real-time systems. IEEE 7kansactions \non Software Engineen ng, 12(9) :890 904, Septem\u00adber 1986. Kevin B. Kenny and Kwei-Jay Lin. Building flex\u00adible \nreal-time systems using the Flex language. IEEE Computer, pages 70-78, May 1991. E. Kligerman and A. \nD. Stoyenko. Real-time Euclid: A language for reliable real-time sys\u00adtems. IEEE l kansactions on Software \nEngineering, 12:941-949, September 1986. structs for real-time programming. In Proceedings IEEE Real-Time \nSystems Symposium, pages 57-66. IEEE, 1985. [14] K. J. Lin and S. Natarajan. Expressing and main\u00adtaining \ntiming constraints in FLEX. In Proceed\u00adings IEEE Real-Time Systems Symposium, Decem\u00adber 1988. [15] Al \nMok and et al. Evaluating tight execution time bounds of programs by annotatiorw. In 6th Work\u00adshop on \nReal-Time Operating Systems and Soft\u00adware, pages 74 80. IEEE, May 1989. [16] Alexandru Nicolau. Parallelism, \nMemory Anti\u00adaliasing and Correctness Issues for a fiace Scheduling Compiler. PhD thesis, Yale University, \nJune 1984. [17] V. Nirkhe, S. Tripathi, and A. Agrawala. Language support for the Maruti real-time system. \nIn Pro\u00adceedings IEEE Real-Time Systems Symposium, De\u00adcember 1990. [18] ChangYun Park and Alan C. Shaw. \nExperimenting with a program timing tool based on source-level timing schema. In Proceedings IEEE Real-Time \nSystems Symposium, pages 72-81, December 1990. [19] Alan C. Shaw. Reasoning about time in higher level \nlanguage software. IEEE Transactions on Software Engineering, pages 875-889, July 1989. [20] Michael \nSmith, Mark Horowitz, and Monica Lam. Efficient superscalar performance through boost\u00ading. In Fi@h International \nConference on Archi\u00adtectural Support for Programming Languages and Operating Systems, pages 248-259. \nACM, October 1992. \n\t\t\t", "proc_id": "155090", "abstract": "<p>We present a programming language with first-class timing constructs, whose semantics is based on time-constrained relationships between observable events. Since a system specification postulates timing relationships between events, realizing the specification in a program becomes a more straightforward process.</p><p>Using these constraints, as well as those imposed by data and control flow properties, our objective is to transform the code so that its worst-case execution time is consistent with its real-time requirements. To accomplish this goal we first translate an event-based source program into intermediate code, in which the timing constraints are imposed on the code itself, and then use a compilation technique which synthesizes feasible code from the original source program.</p>", "authors": [{"name": "Seongsoo Hong", "author_profile_id": "81452597405", "affiliation": "", "person_id": "PP95030137", "email_address": "", "orcid_id": ""}, {"name": "Richard Gerber", "author_profile_id": "81406598745", "affiliation": "", "person_id": "PP39076687", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/155090.155106", "year": "1993", "article_id": "155106", "conference": "PLDI", "title": "Compiling real-time programs into schedulable code", "url": "http://dl.acm.org/citation.cfm?id=155106"}