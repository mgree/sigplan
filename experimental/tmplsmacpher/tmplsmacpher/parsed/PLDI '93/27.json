{"article_publication_date": "06-01-1993", "fulltext": "\n Reverse If-Conversion Nancy J. Warter Scott A. Mahlke Wen-mei W. Hwu B. Ramakrishna Rau Center for Reliable \nand High-Performance Computing University of Illinois Urbana-Champaign, IL 61801 Abstract In this paper \nwe present a set of isomorphic control trans\u00adformations that allow the compiler to apply local scheduling \ntechniques to acyclic subgraphs of the control flow graph. Thus, the code motion complexities of global \nscheduling are eliminated. This approach relies on a new technique, Reverse If-Conversion (RI C), that \ntransforms scheduled If-Converted code back to the control flow graph representation. This paper presents \nthe predicate internal representation, the al\u00adgorithms for RIC, and the correctness of RIC. In addition, \nthe scheduling issues are addressed and an application to software pipelining is presented. Introduction \nCompilers for processors with instruction level parallelism hardware need a large pool of operations \nto schedule from. In processors without support for conditional execution, branches present a scheduling \nbarrier that limits the pool of operations to the basic block. Since basic blocks tend to have only a \nfew operations, global scheduling techniques are used to schedule operations across basic block boundaries. \nGlobal scheduling consists of two phases, inter-block code motion and local (basic block) scheduling. \nThe engineering problem of global scheduling is to determine how to properly order these phases to generate \nthe best schedule. In this paper we present a set of isomorphic control trans\u00adformations (ICTS) that \nsimplify the task of global schedul\u00ading to one that looks like local scheduling. This is achieved by \ndefining a predicate intermediate representation (predi\u00adcate IR) that embodies the code motion properties \nand thus eliminates the need for an explicit code motion phase dur\u00ading scheduling. The ICTS convert from \nthe control flow graph representation to the predicate IR and vice-versa. If\u00adconversion, a well known \ntechnique [I] [2], is used to convert an acyclic control flow graph into an enlarged basic block Permission \nto copy without fee all or part of this material is granted provided that the copies are not made or \ndistributed for direct commercial advantage, the ACM copyright notice and the title of the publication \nand its date appear, and notice is given that copying is by permission of the Association for Computing \nMachinery. To copy otherwise, or to republish, requires a fee and/or specific permission. ACM-S lGPLAN-PLDl-6/93 \n/Albuquerque, N.M. Q 1993 ACM o-89791 -598 -4193 /0006 /0290 . ..$~ .50 Hewlett Packard Laboratories \nPalo Alto, CA 943o3 (hyperblock) Figure 1: Overview of the Isomorphic Control Transforma\u00adtion (ICT) approach \nfor global scheduling. of predicated operations called a hyperblock [3] [4]. After scheduling, a new \ntechnique, Reverse If-Conversion (RI C), is used to convert from the scheduled hyperblock to the sched\u00aduled \ncontrol flow graph. Figure 1 shows an overview of the ICT approach to global scheduling. If-Conversion \nis considered an isomorphic control transfor\u00admation because an operation in the original acyclic control \nflow graph will have the same condition for execution in the hyperblock. Likewise, RIC is isomorphic \nbecause an oper\u00adation in the scheduled hyperblock will have the same con\u00addition for execution in the \nregenerated acyclic control flow graph. Furthermore, we assume that no operations are in\u00adserted during \nRIC. If an operation is inserted during RIC, it may violate the schedule. This assumption is particularly \nimportant for VLIW processors which rely on precise timing relationships. This paper is organized as \nfollows. In Section 2, we derive the predicate IR. If-Conversion, the scheduling issues, and Reverse \nIf-Conversion are presented in Sections 3 through 5, In Section 6 we prove the correctness of the ICTS, \nand in Section 7, present an application of the ICT approach for software pipelining acyclic loop bodies. \nWe conclude with a discussion of the strengths and limitations of scheduling un\u00adder ICT as compared to \nother global scheduling techniques, 2 Intermediate Representation The primary program representation \nis the control flow graph. The problem addressed in this paper is to define an intermediate representation \nfor acyclic subgraphs of the control flow graph that allows the compiler to generate a globally-scheduled \ncontrol flow graph by applying local scheduling techniques. Other researchers have noted the inadequacies \nof the control flow graph for applying com\u00adpiler transformations and have proposed more powerful in\u00adtermediate \nrepresentations such as the Program Dependence Graph (PDG] [5][6][7] [8] [9]. Our intermediate representation \n Rules of Code Motion Bounds 17 1)idantlcal operations from B and C merged Into A (hoisting) 2) operation \nfrom A copied to both B snd C B c 3) operation moves up from B to A (speculative) 6 [ 4) operstlon from \nA copied only to B (destinstlon is not in Ilve-in set of C) DE 5) operstlon from F copied Into C, D, \nand E db I 6) identical operations from C, D, snd E merged into F 7) oDeration from D cot)led into F \n(Dredkated).. % F u Figure 2: Rules of code motion for global scheduling, builds on the PDG concepts \nbut is designed specifically to assist global instruction-level scheduling. Before presenting the intermediate \nrepresentation, we want to analyze the difficulty of scheduling an acyclic control flow graph. Definition \n1: An acyclic control fiow graph is a directed acyclic graph G with a unique entry node START and a set \nof exit (EXIT) nodes such that, each node in the graph has at most two successors. For any node N in \nG there exists a path from START to N and a path from N to one of the EXIT nodes. The nodes of a control \nflow graph are commonly referred to as basic blocks. Applying global scheduling to an acyclic control \nflow graph involves two phases, code motion be\u00adtween basic blocks, and scheduling within basic blocks. \nFig\u00adure 2 illustrates the rules of code motion for global schecLul\u00ading [10][1 1]. These rules can also \nbe viewed as the basic steps needed for moving operations in the control flow graph. For example, an \noperation in basic block F can be moved into basic blocks C, D, and E by applying rule 5. The operatims \ncan then be scheduled in these basic blocks or the identical operations in D and E can be merged into \nB by applying rule 1. Again, the operations can be scheduled in these basic blocks or the identical operations \nin B and C can be meriged into A. This simple example illustrates the phase ordering problem between \nthe code motion and scheduling phases in global scheduling. It is difficult to determine when to stop \ncode motion and schedule the operations in order to generate the best schedule. In some code motion cases, \nit is necessary to create a ba\u00adsic block into which an operation may be copied. Consider the control \nflow graph in Figure 3(a). It is not possible to simply apply rule 2 to copy an operation from B into \nD and E. Because E has multiple predecessors, the operation from B cannot be placed in basic block E. \nInstead, a basic block (Dl) must be created for the copy of the operation. Fig\u00adure 3(b) shows an augmented \ncontrol flow graph with all of the possible basic blocks (Dl, D2, and D3) that could be created as a \nresult of applying the code motion rules [10][ 1l]. By applying the code motion rules to operations from \nev\u00adery basic block in the control flow graph, the range of possi ble destination blocks can be determined \nfor each operation. The table in Figure 3(c) presents the range assuming no speculative or predicated \ncode motion. For example, con\u00adsider an operation b in basic block B. Since we do not con\u00adsider speculative \nexecution, b cannot be moved to A. By A Op;oa:.m Range .,,,, ~efore bran ch of nlerg, at A all BC B B,D,M \nA E,F c C,D2,03 A E,J DE DD BF F E Dl, D2,E B,C F F B,0, Di, D2,E,F E,C J JJ all f) (a) (b) (c) Figure \n3: Bounds on code motion for example control flow graph. (a) Example control flow graph. (b) Augmented \ncontrol flow graph. (c) Code motion bounds and ranges assuming no speculative or predicated execution. \napplying rule 2, b can be copied into D and D1. However, since we assume no predicated execution, b cannot \nbe copied from D to F or from D1 to E. Thus, the range of allowable basic blocks for operation b is B, \nD, and D1. From this range, we can see that b is bounded from above by the con\u00ad ditional branch in A \nand bounded from below by the merges atEand F. During local scheduling, only the precedence relations \nbe\u00adtween operations are considered. Thus, if the bounds on code motion can be represented as precedence \nrelations, the task of global scheduling can be simplified to local schedul\u00ading. The upper bound on code \nmotion defines the bound for upward code motion and corresponds to control deperz\u00ad dence [1]. An operation \nx that is bounded by a conditional branch operation y is said to be control depersdenton U. From the \nscheduler s viewpoint, z cannot be scheduled until after y has been scheduled. Given the following definition \nof post\u00addominance, control dependence can be defined [1][5]. Definition 2: A node X is postdominated \nby a node Yin G if every directed path from X to an EXIT node (not including X) contains Y. Definition \n3: All the operations in node Y are control de\u00adpendent upon the conditional branch operation in node \nX if and only if (1) there exists a directed path P from X to Y such that every node Z in P (excluding \nX and Y) is post\u00addominated by Y and (2) Y does not postdominate X. The lower bound on code motion defines \nthe bound for downward code motion. There is no existing precedence re\u00adlation that can be used to define \nthe lower bound on code motion. One problem in defining such a precedence relation is that precedence \nrelations are between two operations, and there is no explicit merge operation. However, we can define \nan imphczt merge operation to be an implied operation which proceeds the first explicit operation in \nevery merge node. Now, the lower bound on code motion can be represented by a control antt-dependence \nrelation. From a scheduler s viewpoint, a merge operation ~ cannot be scheduled until every operation \nx upon which it is control anti-dependent has been scheduled. Using the following definition for dom\u00adinance \n[12], control anti-dependence can be defined. Definition 4: A node X dominates a node Yin G if every \ndirected path from START to Y contains X. . (tmlch d) OPERATION SYNTAX ABC predicated operation <p>op \nA 8X w predicate define < p > pd [cond] {false} {true} -. (qmrwm Wm) predicate merge pm {no.jump} {jump} \nFEH .A ABC DE FJDI D2D3 Dx *, xx xx  ) : x (&#38;Lld): F xx Jxx x J mx D2 x D3 x B (a) (b) Figure 4: \nControl precedence relations for operations in the example control flow graph. (a) Control dependence. \n(b) Control anti-dependence. Definition 5: The implied merge operation in node Y is control anti-dependent \nupon all the operations in node Xif and only if (1) there exists a directed path Pfrom Xto Y such that \nevery node Z in P (excluding Y) is dominated by X and (2) X does not dominate Y. The matrices in Figure \n4 show the non-redundant control dependence and control anti-dependences for the example control flow \ngraph in Figure 3(b). Given the control dependence and control anti\u00addependences, we need to define an \nIR that preserves these de\u00adpendence. Since our goal is to apply local scheduling tech\u00adniques, an operation \nis the basic unit of the IR. A predicate, which represents an operation s condition for execution, can \nbe assigned to each operation. Every operation in the acyclic control flow subgraph becomes a predicated \noperation. To preserve control dependence, conditional branches become operations that define the predicates. \nThese operations are referred to as predicate define operations. To preserve control anti-dependences, \nimplied merge operations become opera\u00adtions that merge predicates, referred to as predicate merge operations. \nIt is sufficient to represent the control dependence and control anti-dependences using one set of predicates \nper predicate define operation and one set of predicates per pred\u00adicate merge operation. However, in \norder to regenerate the control flow graph, the RIC algorithm must be able to de\u00adtermine which predicates \nwhere defined along the two paths of the conditional branch. Thus, two sets of predicates are needed \nfor the predicate define operations, one for predicates defined along the true path and one for predicates \ndefined along the false path of the conditional branch. Likewise, there are two types of predicates paths \nentering a merge node, those that have jump operations and those that do not. The predicate merge operation \nhas two sets of pred\u00adicates, those corresponding to paths being merged that re\u00adquire a jump, and those \nthat do not. Table 1 shows the syn\u00adtax of the predicate IR. Note that since the implicit merge operation \nis not an actual operation in the original control flow graph, the predicate merge operation is not predicated. \nBefore discussing the If-Conversion transformation, we re\u00adturn to the idea of augmenting the control \nflow graph with Table 1: Predicate Intermediate Representation. empty basic blocks. As discussed above, \nduring code mo\u00adtion, basic blocks may need to be generated to hold copies of moved operations. Since \nlocal scheduling is applied to the predicate IR, RIC eventually generates the scheduled control flow \ngraph and the effects of code motion will be present. To avoid inserting operations during RIC, the con\u00adtrol \nflow graph is augmented with dummy blocks before If-Conversion. Given an edge X + Y, if X has multiple \nsuc\u00adcessors and Y has multiple predecessors, a dummy node is inserted on X -+ Y. If Y is not on the fall-through \npath of X, insert a jump operation in the dummy node. Algorithm DUM in Appendix A is used to insert dummy \nnodes. 3 If-Conversion The If-Conversion algorithm presented in this paper is based on the RK algorithm \n[13]. The basic RK algorithm presented in [13] decomposes the control dependence using two func\u00adtions \nR and K. The function R(X) assign a predicate to basic block X such that any basic block that is control \nequiva\u00adlent [9] to X is assigned the same predicate. The function K(p) specifies the condition under \nwhich a predicate p is de\u00adfined. We have added two functions, St,Ue (X) and Sfal,e (X), to merge all \nthe predicates defined under the true and false conditions of conditional branch of X, The S algorithm \nis presented in Appendix A. Figure 5 shows the predicated example control flow graph and corresponding \nR, K, StrUc, and Sfal,. functions. Note that basic blocks A and J are control equivalent and thus, have \nthe same predicate pO. Since A and J always exe\u00adcute, K(pO) is the empty set since no conditions define \ntheir execution. In this paper we assume that when a node has two succes\u00adsors, the left edge corresponds \nto the false condition and the right edge corresponds to the true condition of the branch. Consider the \nconditional branch in basic block A. From the control dependence matrix in Figure 4(a), we know that \nba\u00adsic blocks B, C, and F are control dependent on the branch in A. Using the R function, the predicates \nof basic block B, C, and F are pl, p2, and p5, respectively. The K func\u00adtion is {~} for pl, {A, ~} for \np5, and {A} for p2. Thus, Sf~l,~(A) is {P1,P5} and St,~. (A) is {p2}. Another way of viewing control \nanti-dependences, is to say that an operation is reverse control dependent on a branch. Definition 6: \nAll the operations in node X are reverse con\u00adtrol dependent upon the implied merge operation in node \nY if and only if (1) there exists a directed path P from X to Y such that every node Z in P (excluding \nY) is dominated by X and (2) X does not dominate Y. Reverse control dependence can be calculated by modi\u00adfying \nthe algorithm for calculating control dependence pre\u00ad <po> Op_al Nook x, A8c0 EFJ DI D2 W <po> 0p_a2 \n@edlcnt. R(x) po pi p2 p2 p4 p5 po p6 p7 ,,8 7 <po> op-a3 11: <p@ Op_al p,dlcat. p, po pi pz p3 p4 P5 \nP6 P7 P8 K(P) @ ~ (A) @l {E@) ~,~ {s) {~} (C)1  EGGzcrl Mock x EFJ A-D, D7.D3 M.noJ.mp+) fpl,p2,p7) \n@i,p3) @2,p5) m *4)WV 6 @ ~ MJ.W.) fp6) d- Figure 5: Solutions of R, K, S, and M for example control \nflow graph. sented in [13]. The algorithm RCD in Appendix A calcu\u00adlates the reverse control dependence \nof each node in the directed graph. RCD(X)calculates theset of implied merge operations which every operation \nin node X is reverse con\u00adtrol dependent upon. The algorithm M in Appendix A uses RCD(X) to determine \nwhich predicates are being mergecl at each predicate merge operation. Once it has been deter\u00admined which \npredicates to merge, the predicates are divided into the no.jump and jump sets for each predicate merge \noperation. Given a merge node t in RCD(X) and the predi\u00adcate, p, of X, p is placed in the jump set of \nthe the predicate merge operation of t if (1) t is an immediate successor of X, and (2) t is not on the \nfall-through path of X. Otherwise, p is placed in the no.jump set of the predicate merge operation of \nt. Figure 5 shows the merge functions MJu~P and Mn._j.rnP of the implicit predicate merge operations \nat nodes E, l?, iind J. Consider the implicit predicate merge operation at node F. From the control anti-dependence \nmatrix in Figure 4(b), we know that operations in basic blocks B, D, and E are control anti-dependent \non the implied merge operation of F. Node E is an immediate predecessor of F and is assumed to have a \njump operation (e.g., F is not on the fall-through path) and thus its predicate is in the jump predicate \n(set. Thus, M1u~P (F ) is {P4} and Mn..jumP(F ) is {P1,P3}. After If-Conversion, each operation is predicated, \ncondi\u00adtional branch operations are replaced by predicate define op\u00aderations, implied merge operations \nare replaced by predicate merge operations, an d jump operations are deleted. Fig\u00adure 6(a) shows the \nhyperblock of the example control fl~ow graph after If-Conversion. Note that the R function defines the \npredicate of each operation; the St,U. and Sfal,, func\u00adtions define the true and false sets of the predicate \ndeiine operations; and the M.._j.mP and Mj.mp functions deiine the no_jump and jump sets of the predicate \nmerge opera\u00adtions. To illustrate the scheduling and regeneration ccjm\u00adplexities, Figure 6(a) shows predicated \noperations for each basic block such that the lower case operations are from the corresponding upper \ncase basic block. The condition of the predicate define operation is specified as the basic block iden\u00adtifier. \nAlso, the predicate merge operations indicate from which basic block they originate. Not e that since \ndummy nodes, Dl, Dz, and D3 do not have any operations before <PO> 0p_a4 12 <pb 0p_a2 <PO> pd [A] (pl \n,p5xP2) 13. <p@ OpJ i <pi > op-bl 14: <P* pd [A] (p i ,P5HP2) <PI> op_b2 15: <PI> op_bl <pi > op_b3 \n16, <pb 0p,2 <pi > pd [B] {P3HP4,@) 17: <P&#38; OP_Cl I <PI> op_b2 <p2> Op_cl 18: <PI> pd [8] {P3Hp4,p6) \n<p2> 0p_c2 19: <p3> op_dl / <p2> pd [C] (p4,p5,p7xp8) <p2> oP_c3 110. <p4> op_el <P2> pd [C] (P4,P5,p7xp8) \n111: <p5> Op_fl <p3> op_dl 112: <pa 0P_c2 / <P3> op_d2 <p3> op_d2 113: <P4> 0p_e2 <P3> op_d3 114: <pi> \nop_b3 I <P2> 0P_c3 pm_E {p i ,P2,P7)(P6) 115: pm_E (p 1,P2,P7)(P6) .P4> op-e 1 116, <P3> op_d3 .p4> cp_e2 \n117: <p5> Qp_f2 <P4> 0p_e3 118: <P4> 0p_e3 pm_F (PI ,P3HP4) 119. pm_F (PI ,P3)414) <p5> Op_fi 120: <pb \noP_a3 <p5> 0p_f2 121: <pb 0P_13 <p5> 0P_f3 122, <P5> oP_f3 / ~_J (P2,P5HP8) pm_J (P2,P5XP8) [23 <P&#38;. \nOP.a4 <po> OpJ i [24 <pb 0PJ4 <PO> C9J2 <PO> qLj3 <PO> 0Pd4 (a) (b) Figure 6: (a) Hyperblock of example \ncontrol flow graph. (b) Hyperblock after scheduling. scheduling, there are no predicated operations correspond\u00ading \nto these blocks. However, their predicates are placed in the appropriate predicate define and predicate \nmerge oper\u00adations. 4 Scheduling Issues By using the set of isomorphic control transformations, local \n(basic block) scheduling techniques can be used to perform global scheduling. The control dependence \nand control anti\u00addependences are used to preserve the control flow properties during scheduling. Control \ndependence prevents operations from being scheduled before the corresponding predicate de\u00adfine. In terms \nof the control flow graph, control dependence prevent the operations from being moved above a conditional \nbranch. Control dependence may be removed if the proces\u00adsor has hardware support for speculative execution. \nIn this case, the predicate of an operation can be promoted to the predicate of the predicate define \noperation and thus, the operation can be scheduled before the predicate define [3]. Control anti-dependence \nensures that the predicate merge operation is scheduled before any operations that are con\u00adtrol anti-dependent \nupon it. Effectively, this prevents any operations from being scheduled after a merge point in the control \nflow graph. Control anti-dependences may be re\u00admoved if the processor has hardware support for predicated \nexecution [14][2]. The disadvantage of speculative and predicated execution is that the processor will \nfetch the operation even when the result of the operation is not used. The hyperblock forma\u00adtion can \nbe used to ensure that operations are only moved along the most frequently executed paths. The hyperblock \nis formally defined as follows. Definition 7: A hyper+lock is an If-Converted region formed from an acyclic \nsubgraph of the control flow graph which has only one entry (START) block but may have multiple exit \n(EXIT) blocks. Using this definition, if some basic blocks are known to be infrequently executed, they \ncan be excluded from the hyper\u00adblock. This allows the compiler to more effectively optimize and schedule \nthe operations from the more frequently exe\u00adcuted basic blocks that form the hyperblock. In order to \nexclude basic blocks, a technique called tail duplication is used to remove additional entry points into \nthe hyperblock. The details of selecting basic blocks to form hyperblocks are provided in [3] [4]. Before \nscheduling, a hyperblock is a sequence of consec\u00adutive operations. After scheduling, a hyperblock is \na se\u00adquence of consecutive instructions. For a RISC plocessor, an instruction has one operation. For \na VLIW processor, an instruction has w operations, where w is the issue rate of the processor. In the \nscheduled hyperblock, there may be several operations scheduled on top of one another. Since operations \nwith mutually exclusive predicates will be placed in different basic blocks in the scheduled control \nflow graph, they can be scheduled to the same resource in the same cycle. We use the Predicate Hierarchy \nGraph described in [3][4] to determine if two predicates are mutually exclusive. The predicate merge \noperation also has special scheduling characteristics. A predicate merge is scheduled as a jump operation \nunder the jump set predicates and as a null op\u00aderation under the no-jump set predicates. Since there \nmay be multiple predicates in the jump set, multiple jump oper\u00adations can be scheduled per predicate \nmerge. However, all predicates in the jump set are mutually exclusive and thus their operations can be \nscheduled to the same resource in the same cycle. A null operation does not require any re\u00adsources or \ncycles. To schedule a predicate merge operation at a given time to a given resource, only the predicates \nin the jump set must be mutually exclusive to the predicates of other operations already scheduled to \nthe resource. Figure 6(b) shows the example hyperblock after schedul\u00ading. The schedule assumes a single-issue \nprocessor with\u00adout interlocking. We present the most restrictive processor model in order to illustrate \nhow no-op operations are in\u00adserted if needed. Note that the control dependence (control anti-dependences) \nbetween predicate define (merge) oper\u00adations and their respective predicated operations are pre\u00adserved. \nWe assume that all other data dependence between operations are preserved. Also note that the predicate \nmerge operation pmJ is scheduled in instruction 122. It can be scheduled in instruction 122 since all \noperations that are predicated on p2, p5, and p8 are scheduled at or earlier than 122. Also, p8 and p5 \nare mutually exclusive and thus both op_R3 and pm-.J can be scheduled in the same cycle. Note, that although \na predicate merge operation is scheduled as a jump operation, it is not converted to a jump operation \nuntil RIC. Similary, a predicate define operation is scheduled as a conditional branch but is not converted \nto a conditional branch operation until RIC. Before applying If-Conversion, the control flow graph was \naugmented with dummy nodes. During scheduling, it can be determined whether a dummy node is needed or \nnot. If a dummy node is not needed, the scheduler should apply a jump optimization to remove unnecessary \njump operations when predicate merge operations are scheduled. When a predicate merge operation is scheduled, \na jump is required if any operations have been scheduled along the control path between the predicate \ndefine, pd, operation that defines a predicate p and the predicate merge operation, pm that contains \np in its jump set, This can be done be checking all scheduled operations between pcl and the cycle in \nwhich pm is being scheduled. If any operations have a predicate that is not mutually exclusive with the \npredicate p, a jump IS needed. For VLIW processors, a further condition is needed. To remove a jump, \npm must be scheduled in the same cycle as pd. If a jump is not needed, jump optimization is per\u00adformed \nby deleting the predicate p from the jump set and inserting it into the no-jump set of the predicate \nmerge op\u00aderation 5 Reverse If-Conversion After scheduling, the hyperblock represents the merged schedule \nfor all paths of the control flow graph. The task of RIC is to generate the correct control flow paths \nand to place operations into the appropriate basic blocks along each path. Whereas a basic block in the \noriginal control flow graph was assigned one predicate, the basic blocks in the regenerated control flow \ngraph have a set of predicates associated with them. The allowable predzcatc set specifies the predicates \nof the operations that can possibly be placed in the corre\u00adsponding basic block. Intuitively, the allowable \npredicate set is the mechanism that accounts for the code motion phase of global scheduling. The RIC \nalgorithm is p~esented in Appendix A. The R,e\u00adverse If-Conversion algorithm processes the operations \nin the hyperblock in a sequential manner. A leaf node set, L, is maintained that consists of the basic \nblock nodes in which operations can be placed. Thus, at a given cycle, L con\u00adtains one node from every \npossible execution path. Initially, L consists of the root node of the regenerated control flow graph. \nEach node X in L has an allowable predicate set P(X). When an operation op in the hyperblock is processed, \nit is placed in every node X in L if P(op) c p(X). For a VLIW processor, it is necessary to insert no-op \noperations into empty slots. Since each operation slot may have mul\u00adtiple operations with mutually exclusive \npredicates, no-op operations are not, inserted until after the entire instruction has been processed. \nLet P(pd) be the predicates defined by the predicate de\u00adfine operation pd. When a predicate define operation \nis encountered, it is inserted into every node X in L where F (yd) E p(X). Each X is then deleted from \nL. For each such X, two successor nodes S?Lcct and SILCCf correspond\u00ading to the true and false path of \nprl are created and inserted Hypert+oti Imsbucuon Being Proceswd (0) 11 Opal 12 0p_a2 n 13 14 ch_A (02] \n15 16 0pJ2 17 Op.cl 18 ),1,3,5] ,,, , q 19 op.dl cbr.c {0,2,4,5,7] ,2,8] 110 op.ei >p.ei 111 Op_fi op.fl \n>p.fl 112 op_d2 >p_C2 op_c2 113 op_e2 >p_e2 114 op_b3 op_b3 >p.cz op_c3 ,15 pnlp 1 [0,4,6] 116 op_d3 \n1i7 op_f2 Op.fz 118 op_e3 119 + jump + c. 120 op_a,3 121 OPJ 3 122 lump 123 IOp.a4 124 OPd4 I L.---J \n Figure 7: Control flow graph of scheduled hyperblock a,fter RIC. into L. The allowable predicate sets \nof Sztcc~ and Succf are, p(Succt) = p(X) U true.predicates of pd, and p(Succf) = p(X) U false.predicates \nof pd. When a predicate merge operation is encountered, the fol\u00adlowing is done for each node X in L. \nIf a predicate p(X) is in the jump or no.jump sets, the predicates specified in the jump and no_jump \nsets are deleted from p(X) to cre\u00adate p~e~ (X). If it was in the jump set, a jump operation is inserted \ninto X. L is searched for a node Y with the same al\u00adlowable set as p(X). If one is found, Y is the successor \nof X. Otherwise, a successor is created for X with the allowable set pneW(X). Figure 7 shows the control \nflow graph generated from the scheduled hyperblock in Figure 6(b). The target machine in this example \nis a single-issue processor without interlockhg. Thus, no-op operations are required and are represented \nby a dash. The allowable predicate set is indicated on tolp of each basic block. 6 Correctness of ICT \napproach In this section we prove the correctness of the isomorphic control transform approach. First, \nwe show that DUM pre\u00adserves the control dependence of the original control flow graph G. Lemma 6.1 Given \na directed graph, G, the augmented con\u00ad trol flow graph created by DUM preserves the control depen\u00ad dence \nand reverse control dependence of G. Proofi Given a directed path P between node X and node Y of G. First \nconsider control dependence, where Y is control dependent on X. Assume that DUM inserts a node Z in P. \nSince Z only has one successor and Z is in P, every path from Z to an EXIT node contains Y. Thus, Z is \npostdominated by Y, and hence, Y remains control dependent on X. A similar argument can be made for reverse \ncontrol dependence. 0 Lemma 6.2 Assuming no speculative or predicated execu\u00adtton, an operation m the \nregenerated control flow graph is executed if and only if it would be executed in the original control \nflow graph. Proofi During If-Conversion, every operation is assigned a condition for execution (predicate). \nThe control depen\u00ad dence and control anti-dependences are preserved using predicate define and predicate \nmerge operations respectively. We assume that the scheduler does not violate these depen\u00ad dence. There \nare three cases in which an operation can execute in the original control flow graph and not in the regenerated \ncontrol flow graph: 1) the operation is moved from above to below a branch but only placed on one path \nof the branch, 2) the operation is moved from below to above a merge but not along every path, and 3) \nthe operation is deleted from a path. Since the scheduler does not allow these cases, all three cases \ncan only occur if RIC places an operation on the wrong path or fails to place an operation. There are \nthree cases in which an operation can execute in the regenerated control flow graph but not in the original \ncontrol flow graph: 1) the operation is moved above a branch it is control dependent upon, 2) the operation \nis moved below a merge it is control anti-dependent upon, and 3) the oper\u00adat ion is executed along a \nmutually exclusive path. The first two cases can only occur if RIC violates the dependence. The last \ncase can only occur if RIC places an operation on the wrong path. RIC preserves the control dependence \nand control anti\u00addependences in the following manner. During RIC, a predi\u00adcate define operation is converted \ninto a conditional branch. The predicates defined alpng one path of the branch are inserted into the \nallowable predicate set of that patli. An operation ,is inserted only along the path that has its pred\u00adicate \nin the allowable predicate set. Thus, an operation will not be placed before a branch upon which it is \ncon\u00adtrol dependent or along the wrong path of the branch. The predicate merge operation is scheduled \nafter every opera\u00adtion upon which it is control anti-dependent. During RIC, paths are not merged until \na predicate merge operation is encountered. Thus, an operation will not be placed after a merge. Since \nRIC preserves the control dependence and control anti-dependences, the predicate of an operation will \nbe in the allowable predicate set of a block when the op\u00aderation is processed. Thus, an operation cannot \nbe deleted during RIC. 0 Lemma 6.3 Given a VLIW processor, the precise t~mzng relationships of scheduled \ncode in the predicated IR is pre\u00ad served between any two operations tn the regenerated control flow graph. \nProofi During RIC, instructions are processed according to the schedule. Thus, instructions will be generated \nwith the proper number of cycles between them unless operations are added or deleted. Only jump operations \nwould need to be added during RIC. Dummy node insertion and the predicate merge operation ensure that \nno jump operations need to be added during RIC. After all VLIW instructions are processed, empty operation \nslots are filled with no-op operations. Thus, no operations will be inserted or deleted during RIC.n \nTheorem 6.1 Assuming the schedule is correct, the ICT approach generates a correct globally scheduled \ncontrol flow graph. Proofi This theorem follows from Lemmas 6.1, 6.2, and 6.3. Software Pipelining Application \n One of the benefits of the ICT approach is that existing com\u00adpiler techniques for processors with support \nfor Predicated Execution [14] can be used for processors without PE sup\u00adport. One such technique is Modulo \nScheduling for software pipelining developed by Rau et. al. [2][15]. By performing If-Conversion before \nModulo Scheduling, loops with condi\u00adtional branches can be scheduled in the same manner as those without. \nHowever, without RIC, Modulo Scheduling for processors without PE support have relied on techniques such \nas Hierarchical Reduction, which apply prescheduling to remove conditional branches [16]. Prescheduhng \nlimits the effectiveness of Modulo Scheduling [17] [18]. Further\u00admore, Hierarchical Reduction can only \nbe applied to struc\u00adtured loop bodies whereas the ICTS can be applied to any acyclic loop body. In this \nsection we present the Enhanced Modulo Scheduling technique (EMS) which uses the ICTS to simplify scheduling. \nFor further details about EMS, refer to [17]. Figure 8 shows the hyperblock after software pipelining. \nThe target machine is a VLIW processor with two opera\u00ad tion slots. Effectively, two iterations of the \nhyperblock (the schedule from Figure 6(b)) have been overlapped. Note that the operations within each \ninstruction have been reordered to illustrate how the iterations are overlapped. Modulo Vari\u00adable Expansion \n[16] and renaming have been applied to the predicate variables. The kernel of the software pipeline is \nunrolled once so that the predicate variable lifetimes of p5 and p8 do not overlap themselves. In the \nsecond copy of the kernel, p5 is renamed to p9 and p8 is renamed to P1O. Although no operations are predicated \non p8, which corre\u00adsponds to D3 in Figure 5, it must be renamed to ensure that the control paths are \nmerged correctly. Note that op.j4 is the loop back branch. Thus, it is deleted from alf but the last \nstage in the kernel. Figure 9 shows the corresponding control flow graph after regeneration. The allowable \npredicate set is indicated for each basic block. The dashes indicate no-op operations. ,1 <po> op..< \n12 <p> op_a2 P 13 <po> OpJ i r ,4 <PO> Pd [A [P1,P5)[P2] t o 15 <pi> .p~ / 16 <po> opJ2 0 17 <P2> OP.CI \n/ @> cQ_b2 9 18 <pi> pd ~] (p3](p4,PS) u 19 <P3> op_dl / <P2> pd [C] [P4,P5,P71(P8) : e 110: <p> 0p_8t \n 111 <~> Op_fi 112 <P2> 0P_c2 I +3> q_d2 , 113 +4> Op.ez +0> op_ai, 114 <pl > 0P_b3 ( <P2> 0P_C3 *O> \nop_a2 115 pm_E (pt,p2,P7J(P6) +0> OPJV 116 <P3> op_d3 +0> pd [A~ (pl ,P!3)(P2) 117 <*> Op.e +!! wj~y \n 118 +4> 0p_e3 119 pm_F @i,P3](P4) +2> Op_C1,/ 41> cf_b2 120 $&#38; :;3:3 +> pd ~~ (P3)(p4,P6) 121 43) \nop_dl, / <p2> pd [CY (P4,P9,P7)(PI 0] 122 .P5> oP_f$ I pm_J @2,P5] @3) +4> Op_ell ,23 <pa> op_a4 +9> \nOp_fi, e 124 -*2> .p.cz$ I +3> c+_d2 K r n e I 125 <po> Op_. i ,, +4> op.ez, 126 <w> op.az q31 > op.b? \n/ +2> oP_c3, 127, <PO> Op 111 $m&#38;E,o$y ;, P7)[P6] + 123 <pO> pd A~ [p t P5]{P2] /, 129 <pi > opbl,, \n130 <po> .pJ2 s T% 131 <p2> Op.ci ,,/ pm_F lpi ,P3)@4) 132 <pi > pd ~~ (p3@,;$-bz +0> op_a3 133 <p3) \nop.dl,, / 42> pd [C~ [P4,p5,p7)[p6] +0> OPJ? r34 <p!> op.ev +9> .p.t? / Pm_Jz (P2,P9)[Pi0] 135 <*> op_lr, \n+0> op.a s 136 ,-<P2> 0P_C2\u00b0 / +3> OP_&#38;? .+0> opj# 1 137 .P4> op.e~ 138 <pi> o _b3,, / +2> op_c3,, \n139 pm-E r Pi ,P2,P7)(P6) 140, <p3> op.d? F 141 <*> op.v 142 +4> op_e3,, 0 143 pm_FJ[pi,p3][p4] 9 144. \n<po> Op.a? 145 <po> 0pJ3 146 +6> OP.W / pm_J ) [P2,P5)(P3) 147 <po> op_a4J$ 148 ; w .pl# IS the loop \nbeck branch (oPJ4 and OPJ.V we .Wtbd) Figure 8: Hyperblock of example control flow graph after software \npipelining. The hyperblock is divided to show the four stages of the pipeline. The modified RK If-Conversion \nalgorithm and RIC have been implemented in the IMPACT-I C compiler. The EMS technique uses these transformations \nto convert the acyclic loop body to and from the hyperblock which is modulo scheduled. Modulo Scheduling \nwith Hierarchical Reduction has also been implemented. To analyze the benefits of the ICT approach, EMS \nand Modulo Scheduling with Hierarchi\u00ad cal Reduction were applied to 26 doall loops from the Perfect benchmarks. \nAll loops contain conditional constructs. The target machine is a VLIW processor without interlocking. \nThe processor allows any combination of non-branch opera\u00ad tions per cycle, but only one branch per cycle. \nDue to space limitations, we summarize the results. EMS performs 18%, 17%, and lg~o better than Modulo \nSchedul\u00ad ing with Hierarchical Reduction for issue rates 2, 4, and 8, respectively. The code expansion \nfor EMS is sz~o, 60~0, and 105% larger than for Modulo Scheduling with Hierar\u00ad chical Reduction for issue \nrates 2, 4, and 8, respectively. Since EMS has a tighter schedule, the conditional constructs overlap \nmore, causing larger code expansion. These results show that the ICT approach can be used to significantly \nin\u00ad crease the performance of Modulo Scheduling when there is no special hardware support for PE. However, \nthe effect of code expansion on performance still needs to be analyzed. A more in-depth analysis of these \ntwo techniques including a comparison with Modulo Scheduling with P E is presented in [17]. 101 m  \nH=-F%&#38;  I IEd w, ,,-,,.,,,,.,.,,0,.,., q u..4 1I Figure 9: Control flow graph of software pipelined \nhyper\u00adblock after RIC. Discussion The ICT approach presented in this paper extends the con\u00adcept of a \npredicate IR to processors without support for Predicated Execution (PE) [2][14]. Thus, in addition to \napplying If-Conversion [1][2][13], an inverse isomorphic con\u00ad trol transformation, RIC, is applied to \nregenerate the control flow graph structure. The ICT approach is also very similar to the the Pro\u00adgram \nDependence Graph (PDG) approach [5]. In addition to representing the data dependence, the PDG explicitly \nrepresents control dependence. Whereas the predicate IR assigns predicates to operations, the PDG assigns \npredicates to regions, each of which contains operations with the same execution condition. Thus, global \nscheduling techniques for the PDG require explicit code motion and thus need to de\u00adtermine the proper \nphase ordering between code motion and local scheduling in order to achieve the best schedule [8][9]. \nWhile the ICTS remove the phase ordering problem faced by other global acyclic scheduling techniques, \nwe do not claim that it is possible to find the best schedule for an acyclic subgraph. Rather, we simply \nstate that the com\u00ad piler can schedule a much larger scope of operations and not have to worry about \ncode motion. In addition to the code motion rules mentioned in Section 2, the code motion com\u00ad plexity \nincurred by moving branch operations above other branch operations [11] is eliminated. The ICT approach \nal\u00ad lows branches and merges to be scheduled in the same man\u00adner as other operations. There are some \ndrawbacks to the scheduling approach pre\u00adsented in the paper. First, an operation in the original flow \ngraph is never replicated during scheduling. Thus, an op\u00aderation is not scheduled until all control dependence \nare resolved. This may cause an operation to be scheduled later than necessary on one or more of the \ncontrol paths. Further\u00admore, it is difficult to associate data flow analysis, such as live-in sets, with \nthe operations within a hyperblock. Thus, it is currently not possible to move an operation from above \nto below a branch and schedule it on only one path of the branch. This is allowed when the result of \nthe operation is in the live-in set of only one successor of the branch. These drawbacks are not necessarily \ndrawbacks of the ICT approach to scheduling, but they do exist in the scheduling scheme presented in \nthis paper. Similar problems exist for cyclic scheduling. During mod-U1O scheduling, since all paths \nare scheduled at once, the initiation interval is the same for all control paths through the loop [15][16][18]. \nWhile some global scheduling-based software pipelining techniques do not require a fixed initi\u00adation \ninterval [19], we are not aware of any methods that support processors with limited resources and non-uniform \nIatencies. In this paper we have assumed that no operations are in\u00adserted during RIC and thus the predicate \nmerge operations are used to specify when jump operations should be sched\u00aduled. Currently, the merge \npoints are described in terms of the original control flow graph. In some sense, this is an arbitrary \ndecisionl . Consider the case where three control paths enter the merge node of a control flow graph. \nAfter scheduling, two paths may be able to be merged earlier than the third. However, since we do not \nmerge any paths until all control anti-dependences are resolved, an early merge will not occur in our \ntechnique. It is possible to split the merge operations into individual predicate kill operations, where \neach predicate kill is scheduled as a jump operation. Since predicate merges, an d hence predicate kills, \nare not pred\u00adicated, there is no ordering imposed on the merge points and it is possible to merge identical \ncontrol paths in any arbitrary fashion. The cost of this solution is that a larger number of jump operations \nwill be scheduled than required. If they are not used, they are nullified with no-op opera\u00adtions. Thus, \nthere is a tradeoff between performance and code expansion. Alternatively, if operations can be inserted \nduring RIC, then it is possible to merge control paths when they become the same by detecting when a \npredicate is no longer used in the scheduled hyperblock. In this case, a predicate merge operation is \nnot needed. 1Basic block layout can be performed as a preprocessor step to make this less arbitrary. \nConclusion In this paper we have presented a predicate IR that pre\u00adserves control dependence and control \nanti-dependences. A set of isomorphic control transformations (ICTS) are used to convert an acyclic control \nflow graph to and from the pred\u00adicate IR. If-Conversion, modified to support control anti\u00addependences, \nis used to transform acyclic control flow sub\u00adgraphs into predicated hyperblocks. A Reverse If-Conversion \ntechnique was presented which transforms scheduled hyper\u00adblocks into scheduled acyclic control flow subgraphs. \nUsing this set of transforms, the task of global scheduling is re\u00adduced to local scheduling. In this \npaper we have shown that the ICTS can be used to significantly improve the perfor\u00admance of Modulo Scheduling \nfor processors without Pred\u00ad icated Execution support. We are currently investigating using the ICT methodology \nfor other hyperblock optimiza\u00adtion and scheduling techniques. Acknowledgments The authors would like \nto thank Grant Haab along with all members of the IMPACT research group for their comments and suggestions. \nSpecial thanks to the program commit\u00ad tee whose comments and suggestions helped to improve the quality \nof this paper significantly. This research has been supported by JSEP under Contract NOO014-90-J-1270, \nDr. Lee Hoevel at NCR, the AMD 29K Advanced Processor De\u00advelopment Division, Matsushita Electric Industrial \nCo. Ltd., Mazda Motor Co., Hewlett-Packard, and NASA under Con\u00adtract NASA NAG 1-613 in cooperation with \nICLASS. Scott Mahlke is also supported by a fellowship provided by the Intel Foundation. References [1] \nJ. R. Allen, K. Kennedy, C. Porterfield, and J. War\u00adren, Conversion of control dependence to data depen\u00addence, \nin Proceedings of the 10th ACM Symposium on Principles of Programming Languages, pp. 177 189, January \n1983. [2] J. C. Dehnertj P. Y. Hsu, and J. P. Bratt, Overlapped loop support in the Cydra 5, in Proceedings \nof the Thmd International Conference on Architectural Sup\u00adport for Programming Languages and Operating \nSys\u00adtems, pp. 26 38, April 1989. [3] S. A. Mahlke, D. C. Lin, W. Y. Chen, R. E. Hank, and R. A. Bringmann, \nEffective compiler support for pred\u00adicated execution using the hyperblock, in Proceedings of 25th Annual \nInternational Symposzum on Microar\u00adchitecture, December 1992. [4] D. C. Lin, Compiler support for predicated \nexecution in superscalar processors, Master s thesis, Department of Electrical and Computer Engineering, \nUniversity of Illinois, Urbana, IL, 1992. [5] J. Ferrante, K. J. Ottenstein, and J. D. Warren, The program \ndependence graph and its use in optimiza\u00adtion, ACM Transactions on Programming Languages and Systems, \nvol. 9, pp. 310 349, July 1987. [6] R. Cytron, J. Ferrante, and V. Sarkar, Experience us\u00ading control \ndependence in PTRAN, in Proceedings of the Second Workshop on Languages and Compzlers for Parallel Computzng, \nAugust 1989. [7] W. Baxter and I. H. R. Bauer, The program depen\u00ad dence graph and vectorization, in Conference \nRecord of the Sixteenth ACM Symposium on the Principles of Programming Languages, pp. 1-10, January 1989. \n[8] R. Gupta and M. L. Sofa, Region scheduling: An approach for detecting and redistributing parallelism, \nIEEE Transactions on Software Engineering, vol. 16, PP. 421-431, April 1990. [9] D. Bernstein and M. \nRodeh, Global instruction scheduling for superscalar machines) in Proceedings of the ACM SIGPLAN 1991 \nConference on Program\u00adming Language Design and Implementation, pp. 241 255, June 1991. [10] J. A. Fisher, \nTrace scheduling: A technique for global microcode compaction, IEEE Transactions on Com\u00adputers, vol. \nc-30, pp. 478-490, July 1981. [11] J. Ellis, Bulldog: A Comptler for VLIW Architectures. Cambridge, MA: \nThe MIT Press, 1985. [12] A. Aho, R. Sethi, and J. Unman, Compilers: Prtnctples, Techniques, and Tools. \nReading, MA: Addison-Wesley, 1986. [13] J. C. H. Park and M. Schlansker, On Predicated Exe\u00adcution) Tech. \nRep. HPL-91-58, Hewlett Packard Soft\u00adware Systems Laboratory, May 1991. [14] B. R. Rau, D. W. L. Yen, \nW. Yen, and R. A. Towlej The Cydra 5 departmental supercomputer, IEEE Computer, pp. 12 35, January 1989. \n[15] B. R. Rau and C. D. Glaeser, Some scheduling tech\u00adniques and an easily schedulable horizontal architec\u00adture \nfor high performance scientific computing, in Pro\u00adceedings of the 20th Annual Workshop on Micropro\u00adgramming \nand Mtcroarchitecture, pp. 183 198, October 1981. [16] M. Lamj A Systolic Array Optimizing Compder. PhD \nthesis, Carnegie Mellon University, Pittsburg, PA, 1987. [17] N. J. Warter, J. W. Bockhaus, G. E. Haab, \nand K. Subramanian, Enhanced Modulo Scheduling for loops with conditional branches, in Proceedings of \nthe 25th International Symposium on Microarchitecture, PP. 170 179, November 1992. [18] N. J. Warter, \nD. M. Lavery, and W. W. Hwu, The ben\u00adefit of Predicated Execution for software pipelining, in Proceedings \nof the 26th Hawaiz International Confer\u00adence on System Sciences, vol. 1, pp. 497 506, January 1993. \nAppendix A Algorithm DUM: Given arootedgraph G, (N, E, START), insert dummy nodes where necessary. Let \nVX~N numsucc(X) = the number of successors of X num_pred(X) = the number of predecessors of X for[X,Y] \nc E if (numsucc(X) > 1) and (num+red(Y) > 1) insert dummy node on edge [X,Y] if(Yis not the fall-through \npath of X) insert jump operation in dummy node Algorithm S: Given (l)arooted graph G,(N, E, START),(2) \nan ordered set of predicates, P, determined by R(n) V n (i N, and (3) the mapping V p E P, compute S. \nFor any nocle X that contains aconditionalbranch operation, St,.~(X) (Si.lsc(x)) specifies the set of \npredicates defined along the true (false) path of x. for X c K(p) if conditions TRUE strue(x) = Strue(x) \nu p else Sfal.e (x) =Sfak(x)up  Algorithm RCD: Givena rooted graph G, (N, E, START), compute thereverse \ncontrol dependence. Assume that dominators of G have been calculated. Let VX~N dom(X) = {Y g N: Y dominates \nX} idom(X ) = the immediate dominator of X for [X,Y] G E such that X @ dom(Y) LUB = idom(Y) t=x while \n(t # LUB) RCD(t) = RCD(t) U Y t = idom(t)  Algorithm M: Given a rooted graph G, (N, E, START) and RCD \nof G, compute M. Ml specifies the A Of predicates to b. merged whose corresponding blocks require an \nexplicit jump to be inserted. Mnl specifies those that do not. Let VX~N P(X) = predicate that X is control \ndependent upon for X~N for ;f~ R#C@D(X) if (t fall-through path of X) or (t not immediate successor \nof X) M.](t) = M.,(t) up(X) else Ml(t) = Mj(t) U F (X) Algorithm RIC: Given hyperblock H, regenerate \na correct con\u00adtrol flow graph. For VLIW processors, insert mo-op fills each [1~] K. Ebcioglu and A. Nicolau, \nA global resource\u00adconstrained parallelization technique, in Proceedings of the International Conference \non Supercomputing, pp. 154-163, June 1989. empty operation slot in an instruction with a no-op operation. \nLet VX~H P(X) = predicate that X is control dependent upon create root node L = {root} p(root) = {PO} \nfor op E H in scheduled order for X6L if op is predicate define operation if P(op) 6 P(X) insert conditional \nbranch operation in X create successor nodes Succt and Succf p(Succt) = P(X) U true(op) p(Succf) = P(X) \nU false(op) L = (L X) USucct USuccf if op is predicate merge for p 6 (jump(op) U no.jump(op)) Pnew(x) \n= P(x) nO-jumP(OP) judop) if p EP(X) if p E jump(op) insert jump operation in X for YeL if pneW(X) \n~ P(Y) if p c jump(op) Succt = Y else Succf = Y L=L X if successor not found if p c jump(op) create successor \nnode Succt p(succt) = pnew(x) L = (L X) USucct 1 e se create successor node Succf /J(succf) = pn.w(x) \nL = (L X) USuccf if op is predicated operation if P(op) G P(X) insert 0p in X if target processor is \nVLIW and last op of instruction insert -no-op  \n\t\t\t", "proc_id": "155090", "abstract": "<p>In this paper we present a set of isomorphic control transformations that allow the compiler to apply local scheduling techniques to acyclic subgraphs of the control flow graph. Thus, the code motion complexities of global scheduling are eliminated. This approach relies on a new technique, Reverse If-Conversion (RIC), that transforms scheduled If-Converted code back to the control flow graph representation. This paper presents the predicate internal representation, the algorithms for RIC, and the correctness of RIC. In addition, the scheduling issues are addressed and an application to software pipelining is presented.</p>", "authors": [{"name": "Nancy J. Warter", "author_profile_id": "81100308771", "affiliation": "", "person_id": "P207080", "email_address": "", "orcid_id": ""}, {"name": "Scott A. Mahlke", "author_profile_id": "81100622742", "affiliation": "", "person_id": "P260981", "email_address": "", "orcid_id": ""}, {"name": "Wen-Mei W. Hwu", "author_profile_id": "81406592242", "affiliation": "", "person_id": "PP39073529", "email_address": "", "orcid_id": ""}, {"name": "B. Ramakrishna Rau", "author_profile_id": "81451599881", "affiliation": "", "person_id": "PP95036584", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/155090.155118", "year": "1993", "article_id": "155118", "conference": "PLDI", "title": "Reverse If-Conversion", "url": "http://dl.acm.org/citation.cfm?id=155118"}