{"article_publication_date": "06-01-1993", "fulltext": "\n Orchestrating Interactions Among Parallel Computations* Susan L. Grahamt Steven Luccot Oliver Sharpt \nComputer Science Division, 571 Evans Hall UC Berkeley, Berkeley CA, 94720 Abstract Many parallel programs \ncontain multiple sub-compu\u00ad tations, each with distinct communication and load balancing requirements. \nThe traditional approach to compiling such programs is to impose a processor syn\u00ad chronization barrier \nbetween sub-computations, op\u00ad timizing each as a separate entity. This paper de\u00ad velops a methodology \nfor managing the interactions among sub-computations, avoiding strict synchroniza\u00ad tion where concurrent \nor pipelined relationships are possible. Our approach to compiling parallel programs has two components: \nsymbolic data access analysis and adaptive runtime support. We summarize the data ac\u00adcess behavior of \nsub-computations (such as loop nests) and split them to expose concurrency and pipelining opportunities, \nThe split transformation has been in\u00adcorporated into an extended FORTRAN compiler, which outputs a FORTRAN \n77 program augmented with calls to library routines written in C and a coarse-grained dataflow graph \nsummarizing the exposed parallelism. The compiler encodes symbolic information, includ\u00ading loop bounds \nand communication requirements, for an adaptive runtime system, which uses runtime infor\u00admation to improve \nthe scheduling efficiency of irreg\u00ad This research has been sponsored in part by the Defense Advanced \nResearch Projects Agency (DARPA) under contract DABT63-92-C-O026 and by a Hertz Fellowship to Oliver \nSharp. The content of the paper does not necessarily reflect the position or the policy of the Government \nand no official endorsement should be inferred. tEmail: {graham, lUCCO, oliver@cs .berkeley. edu Permission \nto copy without fee all or part of this matarial is granted provided that the copies are not made or \ndistributed for direct commercial advantage, the ACM copyright notice and the title of the publication \nand its date appear, and notice is given that copying is by permission of the Association for Computing \nMachinery. To copy otherwise, or to rapublish, requires a fee and/or specific permission. ACM-S lGPLAN-PLDl-6/93 \n/Albuquerque, N.M. o 1993 ACM 0-89791 -598 -4193 /000610100 . ..S1 .50 ular sub-computations. The runtime \nsystem incorpo\u00adrates algorithms that allocate processing resources to concurrently executing sub-computations \nand choose communication granularity. We have demonstrated that these dynamic techniques substantially \nimprove performance on a range of production applications in\u00adcluding climate modeling and x-ray tomography, \nespe\u00adcially when large numbers of processors are available. 1 Introduction The manual parallelization \nof large scientific applica\u00adtions is a complex process; a common goal is to encap\u00adsulate some of this \ncomplexity within automatic tools. Research toward this goal has focused on recognizing parallelism within \na particular loop nest and reorganiz\u00ading loops to improve the use of vector hardware, mem\u00adory hierarchies, \nand multiple processors [2, 5, 8, 12, 21]. In this paper, we consider the effect of interactions among \nsub-computations (including loop nests) on the efficiency of parallel program execution. Several ap\u00adplication \nstudies [7, 15, 19] have identified such inter\u00adactions as a key target for manual optimization. Our contribution \nis to automate these transformations. For example, suppose we have two loop nests with the potential \nto execute concurrently. If each compu\u00adtation is handled independently, with synchronization between \nthe two loop nests, efficiency of CPU usage is limited by the less regular of the two. One possible remedy \nis to use loop fusion, coalescing the two loops into one. However, the resulting parallelization is in\u00adcomplete, \nsince fusion discards information about the more regular component of the new loop. With the computations \nseparated, a runtime system can use the additional parallelism from the more regular loop nest to smooth \nthe load balance of the computation as a doi = l,n integer mask[l ..n], CO1,i, j float result[l ..n], \nq[l ..n,l ..n], output[l ..n,l ..n] Data Behavior of Loop: P do CO1= 1,n where (mask[col] c> O) A mask \n0000111001 q compute result[i] from i th column of q doi = l,n rJi,col] = result[i] 1 I write I read \nI Figure 1: Sample Interacting Computations whole. To expose this type of interaction automatically, \nwe use a new method for data access summarization called a symbolic data descriptor. We use symbolic \ndata descriptors to implement a key transformation, called split, which reduces synchronization constraints \nby sub-dividing computations. We have incorporated split into a compiler which outputs FORTRAN 77 aug\u00admented \nwith calls to library routines written in C and a coarse-grained dataflow graph summarizing the ex\u00adposed \nparallelism. The compiler encodes symbolic information such as loop bounds and data sizes. The runtime \nsystem uses this information, as well as information gathered from the running program, to guide two \nkey scheduli\u00adng decisions: grain-size selection and processor allo\u00adcation. The grain-size selection algorithm \nis detailed elsewhere [14]. In this paper, we present the run\u00adtime processor allocation algorithm, which \nuses sub\u00adcomputation finishing time estimates to correctly ra\u00adtion processing resources among concurrently \nexecut\u00ading sub-computations. The structure of the paper is as follows: in Sec\u00adtion 2, we present a more \ncomplete example of sub\u00adcomputation interaction. Section 3 introduces tech\u00adniques for building symbolic \ndata descriptors, and presents the details of split. Section 4 describes the adaptive scheduling techniques \nused by the runti.me system. Performance results are given in Section 5. 2 Example Interaction Figure \n1 shows interaction among sub-computations. It is a loop whose induction variable CO1 iterates over the \ncolumns of a data array q. If the associated element of the array mask is non-zero, the iteration performs \nthe computation A. A reads all of q and modifies col\u00adumn CO1. After the loop is completed, B computes \nthe array output from q. Because of the conditional in A, the compiler cannot statically determine an \nefficient schedule for A. De\u00adpending on the values in that array, adaptive tech\u00adniques may find an efiicient \nschedule. However, on large numbers of processors efficiency may still be poor if there is not enough \nparallelism (i.e. too few mask elements are non-zero) or if the time to process each column varies too \nwidely. Our approach is to transform the code to expose further concurrency; there are three sources \nthat our strategy can reveal. The first of these is to divide B into three pieces, BI, BD, and BM, where \nBI processes the columns of q which are not touched by any of the instances of A and BD processes the \nrest. BM merges the results into a single output array. In Figure 2, we show the effect of applying this \ntransformation. The notation is equivalent to do ... where <expr> do ... 100P body if (<expr>) 100P body \ninteger float rmw.k[l..n], CO1, i, j rmuk[l..n], q[l..n,l.m], output[l..n,lm] ouputl[l..n,l..n], 0utput2[l..n,l..n] \nde CO1 = l,n where (maak[ml] o do i = l,n com$ute result[i] frem lthwkmmofq do i = l,n q[i,col] = readt[i] \nO) . a I doi=l,rr if (maak[i] = O) doj= l,n outputti,i] = efae doj=l~ outputfi,i] = Figure 2: Code For \nclarity, we show an explicit merge of the two output arrays in BM. In practice, merging can often be \nhandled implicitly by the runtime system during data communication. The second transformation we can \napply is to pipeline one iteration of the col loop with subsequent iterations. We show the code with \nthis further opti\u00admization in Figure 3. The body of the loop has been converted into three computations \nAD, AI, and AM. AD represents the code that is dependent on the pre\u00advious iteration of the loop; the \nruntime system waits for the previous iteration to complete before schedul\u00ading AD. AI, on the other hand, \nis independent of the previous iteration and can be scheduled concurrently. By weakening the synchronization \nconstraint between iterations, we are able to pipeline them. AI computes the result vector for all but \nthe missing column of q that comes from the previous iteration; AD computes the one missing element into \nthe vari\u00adable prev.val. AM takesthe almost complete vector and the missing value and combines them into \na single vector. Again, this kind of merge can often be done im\u00adplicitly. Note that we use the form do \nvar=<range> and <range> to denote a discontinuous sequence of values, rather than duplicating the entire \nloop for both ranges. B1 do i = l,n where (mask[i] = O) doj= l,n outputltij] = f(qti.il) outputlti,i] \n0utput2fj j] After Split The third transformation which we could perform is to pipeline iterations of \nA with corresponding itera\u00ad tions of BD, exposing a further source of concurrency. The form of pipelining \ndescribed in these last two transformations is different from loop pipelining opti\u00ad mization defined \nelsewhere. Previous approaches fall into two classes. The first is software pipehirg [13], which seeks \nto reduce overhead by reorganizing the code. The transformed loop performs fewer iterations but has a \nlarger body that handles more than one of the original loop s iterations at once. This strat\u00ad egy works \nwell in improving performance of a regular loop nest. The second approach, suggested by Bala\u00ad sundaran \nand Kennedy [3], uses post and wait prim\u00ad itives to allow more than one loop iteration to exe\u00ad cute concurrently. \nSince this strategy imposes a ,jlxed synchronization discipline, it does not admit adaptive scheduling \ntechniques.   3 compiler context Our compilation environment combines split with source-to-source transformations \nlike loop fusion [12] and loop interchange [2] to expose additional concur\u00adrency. The system begins by \nperforming an extensive symbolic analysis of the input. The analysis, which is integer mesk[l..n], CO1,i, \nj float rmtdt[l..n], [1..m,l.m], ou ut[l..n,l..n], rmultl[l..n , outputl[l..n, ..n], 0Wput2[l..n,l..n], \npmv_val If B. t I do eel . lJI where (mesk[ud] o 0) 1 do i = I,n where (mesk[i] = O)AD A dej=l~ outputlti,i] \n= f(qtij]) compoteprw.val do i = l,u31-2 and col,n D frenl column com@e result l[i] f.. (coI-l) of q \ni th CUIUUIUI of q /= AM doj= l,ecd-2 and CObl qf.j,col] = re.mltl[i] q[eol-l,col] = plev_val I do \ni=l~ where (mask[i] o O) doj = l,n 0utfmt2ti,i] = f(q~,i]) II ~~ /BM dei=l,n if (mask[i] = O) doj-lo \noulputti,i] = outputlfi,i] else do j =:IJI outputtij] = 0utput2~,i] r I Figure 3: Cocle After also used \nto identify independence and improve tradi\u00adtional optimizations like dead code elimination, is cru\u00adcial \nfor split. One particularly useful outcome of split is to divide the iterations of a loop into two sets \nbased on memory access behavior, a transformation that ramely can be done without symbolic information. \nFollowing the analysis, we calculate summary information and use it to carry out the transformations. \n3.1 Symbolic Analysis The primary purpose of symbolic analysis is to anno\u00adtate variables with the range \nof values they can take on during execution. Annotations are determined by con\u00adverting the code into \nstatic single-assignment (SSA) form [6] and propagating branch conditions andl as\u00adsignment information. \nSymbolic information comes in a few different forms. An sssertion is a disjunction of conjunctions of \nineclual\u00adities. In the example, the computation A executes un\u00adder the assertion that mask [co1] <> 0. \nInequalities express the relationship of metic symbolic expression. symbolic expressions to be set of \nSSA names, each with a constant (either integer an SSA name to an arith- We have chosen to IIimit a sum \nthat may include a an integer coefficient, and or floating point). A :~ym- I Split and Pipeline bolic \nvalue is either a symbolic expression or a range, where a range has a symbolic expression for both start\u00ad \ning and ending values and an integer skip. Analysis consists of several steps: 1. Call Site Analysis \n-because accuracy is often cru\u00adcial for the transformations that use symbolic in\u00adformation, the compiler \nperforms aggressive inter\u00adprocedural analysis. Rather than summarizing a given procedure once and using \nthat summary at every call site, we classify the sites into groups based on profile information and argument \nchar\u00adacteristics. Call sites that represent a signifi\u00adcant amount of computation will only be grouped \nwith others that have the same aliasing pattern and constant values. Less important calls are grouped \ntogether less aggressively, based on a tun\u00adable heuristic. 2. Memory Usage Analysis the compiler builds \na control flow graph (CFG); each CFG node is an\u00adnotated with the scalars that it reads and writes and \na descriptor for its aggregate usage. 3. SSA the code is converted into static single as\u00adsignment form. \n  4, Aggregate Propagation the compiler generates temporary SSA names for values that are assigned \nthrough aggregates. For example, if a value V is assigned to A [i] and then A [i] is assigned to a scalar, \nthe compiler creates an SSA name for V. 5. Alias Elimination -when aliases may cause an as\u00adsignment to \noverwrite uses of other SSA names, the uses that may be affected are marked in\u00advalid. Potential aliases \nare detected in a top down traversal of the CFG, using the description of each node s memory behavior \nto determine which SSA names it may invalidate. 6. Value Propagation the system attempts to con\u00advert \neach conditional branch test into an asser\u00adtion. If successful, the assertion is propagated through the \nCFG links controlled by the condi\u00adtional. In addition, symbolic values are propa\u00adgated from each def \nto all use sites that have not been invalidated.   3.2 Symbolic Data Descriptors Once the code has \nbeen annotated with symbolic in\u00adformation, the compiler can summarize memory usage behavior with a symbolic \ndata descriptor. Symbolic data descriptors are similar in intent to other summa\u00adrization strategies that \nhave been proposed, including regular sections [4] and Data Access Descriptors [3]. The key difference \nis that symbolic data descriptors include symbolic information, which other approaches either limit or \ndiscard. Previously, if symbols were incorporated into the summarization, they could only appear as the \nendpoints of iteration ranges. Split ma\u00adnipulates code fragments containing unresolved sym\u00adbols and requires \na summarization strategy that can analyze them. Descriptors consist of two sets of triples, a set repre\u00adsenting \ndata locations read and one for data locations writ ten. The read set cent sins locations which are live \non entry to the code being annotated; reads known to be dominated by writes in the write set are not \nin\u00adcluded. Each triple describes access to a given block of mem\u00adory and is represented in the form < \nG > B[P]. G is an optional symbolic guard expression; the access represented by the triple is known not \nto occur if the guard is proven false. B is the memory block accessed. P, also optional, describes the \npattern of access; if P is not specified, the triple refers to the entire memory block. Patterns have \nan expression for each dimension of the memory block, representing the range of data touched. Patterns \ncan optionally include a masking expression to further limit access. When assembling a descriptor for \na set of CFG nodes, the analyzer chooses the set of SSA names that may remain unresolved. Here is a sample \ncode frag\u00adment representative of irregular scientific code: doi=l,10 if (miss (i.) 0 1) then doj=l,10 \nq[i, jl = q~i, jl + x[jl To determine whether loop iterations are indepen\u00addent, we sssemble the descriptor \nfor an individual it\u00aderation. The iterations are independent if a change to the value of the induction \nvariable yields a new descrip\u00adtor that intersects with the original only in their read sets. To describe \nthe behavior of the entire loop, we specify that the induction variable may not appear in the descriptor \nso it is promoted to be its entire range. Ignoring scalar variables, here is the descriptor for the i \nth iteration of the entire loop nest in the example: write: < miss[i] # 1> q[i, 1..10] read: < miss[i] \n# 1> q[i, 1..10] Z[l..lo] The expression within angle brackets is a guard and the variable i is an unresolved \nconstant within the it\u00aderation. When the descriptor for the entire loop nest is computed, the guard is \nconverted into a mask across the first dimension of q: write: q[l..lO/(miss[*] # 1), 1..10] read: q[l..lO/(rniss[*] \n# 1), 1..10] X[l..lo] The expression following the slash is a mask on the range accessed in that dimension \nof the array. The symbol * in rniss[*] is a pattern matching symbol that represents the current element \nin a range. Accesses to q[5, 1..10], for example, are masked by the element miss [5] . Given a descriptor \nD, we will refer to its read set as Dread and its write set as Dwrite. A descriptor A interferes with \na descriptor B if (A~,it. n .&#38;M. #0) or (AWAe n B,ead # 0) or (Ar..d n E&#38;,,e # 0)  Interference \ncaptures the notion of dependency be\u00adtween descriptors. The three cases represent output-, flow-, and \nanti-dependencies respectively. When two descriptors do not interfere, the computations they represent \nare independent. We compute interfer\u00adence conservatively; descriptors interfere unless we can prove otherwise. \nTo simplify discussion, we will say that two computations do not interfere if their descrip\u00adtors do not. \nOriginal Code: ~1 G  1,-I After SdiE do i = l,a-1 and a+lJI doi=lfl doj=l~ X[aj] = X[aj] + Y[i] sum2 \n= som2 + Xtij] mm = Suml + SOM2 Figure 4: Simple 3.3 The Transformations 3.3.1 Split The split transformation \nis used when two computa\u00adtions interfere. Figure 4 demonstrates a simple exi_\u00adple, the division of H \ninto three parts (assuming that we can treat addition as an associative operation). Let DG be the descriptor \nfor G and DH be the descriptor for It. Then (omitting induction variables) DGwriie == { X[a,l..n]}, DG,~~d \n= { X[a,l.,n],Y[l..n]}, DHwrite = { sum}, and 12Hread Q {X[l..n,l..n], sum}. Since DGwrite fl DH,ead \n# 0, the two code blocks interfere. H is flow dependent on G because part of H reads the column of X \nmodified by G. However, the depen\u00addency hides potential concurrency because most of H can be computed \nsimultaneously with G; only the com\u00ad putation on column a must be deferred until (we assume for simplicity \nthat a is known tween 1 and n). H can be divided into the must wait for G and the piece that need not; \nparts have finished, the results are merged G is done to be be\u00adpiece that after both together. This \nis exactly what split does on this example, di\u00adviding one computation into three parts based on its interactions \nwith a second computation. More generally, split takes as input a computa\u00adtion C and a descriptor D of \na computation (usu\u00adally a predecessor or successor), and attempts to find I Example of Split sub-computations \nin C that do not interfere with D. The effect of the transformation is to convert C into three computations: \nthe dependent sub-computation CD, the independent sub-computation C1, and the merging sub-computation \nCM. CI contains the sub\u00adcomputations that do not interfere with the computa\u00adtion whose descriptor is \nD and could be separated. CD holds the rest of C, except for those sub-computations that rely on values \nnow computed in Cz. The remain\u00ading sub-computations, along with any needed post\u00adprocessing code, are \nput into CM. The split algorithm begins by subdividing C into primitive computations. Primitive computations \nare the blocks of code that are managed by the transforma\u00adtion; the choice of primitive computation determines \nthe granularity of the split. We have chosen to con\u00adsider basic blocks, function calls, and loops as \nprimi\u00adtive computations. 1 Thus in Figure 4, code block G hsa two primitive computations, a loop and \na basic block, whereas block H has three. Subdividing basic blocks would yield a more aggressive split, \nat the cost of more communication overhead and more cleanup code in CM. The next step is to categorize \neach primitive com\u00ad 1When ~ro~ng i~omation indicates that a blo~ of code is executed infrequently, the \nsystem may choosenot to decompose a loop nest into its constituent computations. putation into one of \nthree sets based on its memory usage behavior. In defining the sets, we will say that a computation C \n(directly) interferes with a set S if and only if there exists s E S such that C interferes with s. We \nwill also use the property of transitive interfer\u00adence; C transitively interferes with S if there exists \na sequence of computations II, IZ, ... In, n > 0, such that C interferes with 11, Im interferes with \n1~+1, and In interferes with S. We say C transitively interferes with S1 using S2 when II through In \nare members of S2. Notice that the property holds when C directly interferes with S. The three categories \nof memory usage are: Bound computations that interfere with D.  Linked computations that do not interfere \nwith D directly but do transitively interfere with it.  Free computations that do not interfere with \nD directly or transitively.  The following algorithm takes C, a collection of primitive computations, \nand assigns each member to a usage category with respect to a descriptor D: Bound = MaybeFree = 0 for \neach cEC if inter~ere(c, D) Bound = Bound U {c} else MaybeFree = MaybeFree U {c} Linked = transitiveinter \nfere(MaybeFree, Bound) Free = MaybeFree transitiveinterfere( initial, Target) Result = 0, Testset = Target \nwhile (Testset) Newbound = 0 for each c E Initial if interfere(c, Testset) Initial = Initial {c} Result \n= Result U {c} Newbound = Newbound U {c} Testset = Newbound return Result The transitive.inter fere \nprocedure is given an initial set Initial and a reference set; it returns a set Result containing the \nmembers of Initial that transitively in\u00adterfere with Target using Initial, The members of Result are \nremoved from Initial. The procedure it\u00aderates to fixpoint; the number of iterations depends on the interference \npattern between sub-computations. Each iteration looks at every sub-computation that re\u00admains in the \nInitial set and moves at least one into Result or the algorithm terminates. An upper bound on execution \ntime is 0(n2) interfere or union compu\u00adtations, where n is the number of sub-computations in C. In general, \nn is small because a computation that is worth moving will be fairly coarse grained. Applying the algorithm \nto the computation H and the descriptor DG, all three primitive computations are in Bound, reflecting \nthe fact that .Wwrite n DHread # 0. As in this csse, it is often possible to split the iter\u00adations of \na loop in Bound into two sets, one of which interferes with D and one of which does not. It is le\u00adgal \nto split iterations when we have nests of loops that are either independent or computing a reduction; \nthey can be split by placing a conditional on the induction variable. By looking at the intersection \nof the loop descriptor with D, we determine whether a restriction on the induction variable yields a \nset of iterations that do not interfere with D. If a loop is successfully split, we have two sets of \nloop iterations; one is still in Bound, but the other is either in Free or in Linked, depending on whether \nit interferes transitively with D. In our example, by restricting i so that it never equals a, we are \nleft with a Free computation. Note that we are computing a reduction, so to split the iterations we must \nreplicate the reduction variable and do the final reduction step in HM. A simple assignment of sub-computations \nto output sets could be done based on the three categories, by putting Free computations into C1 and \nall the rest into CD. However, it is often important to handle Linked computations more carefully, so \nthe set is sub\u00addivided further. This involves one final interference property called flow interference, \nwhich unlike inter\u00adference is not symmetric. A successor computation B has a flow interference from a \npredecessor computa\u00adtion A if Awrite (1 Bread # 0. This is equivalent to the notion of a flow dependency. \nTransitive flow interfer\u00adence is analogous to transitive interference. The new sub-divisions are: NeedsBound \n Linked computations with a tran\u00ad sitive flow interference from Bound. GenerateLinked Linked computations \nfrom which Bound or NeedsBound has a transitive flow interference. ReadLinked Linked computations which \nare neither NeedsBound or Gen&#38;ateLinked. In Figure 5 we show a more complicated example to demonstrate \nthe types of Linked computations (but do not show transformed code). Suppose we wish to split T with \nrespect to W s descriptor. For this example, we will consider the named computations indivisible.  \n m /i==i7  0=i51Elc Rsult=l esuk *sum &#38;; DEiiG3E Figure 5: Enhanced E is Free because it does \nnot have any relation\u00adship to W; B is Bound because it reads the values of array X that are written by \nW. The rest of the sub-computations are Linked. A generates values c)f Y that are used by the Bound computation \nB and hence is GenerateLinked. C is ReadLinked, because it also needs values from A. D is NeedsBound \nbecause it uses the value of sum computed by the Bound computation B. Here is the algorithm that assigns \neach Linked ccjm\u00ad putation to a set: Unrestricted = Linked NeedsBound = transitiveflow.up( Unrestricted, \nBound) GenerateLinked = transitive flow.down(Unrestricted, Bound U NeedsBound) ReadLinked = Unrestricted \nThe computation of transitive_ flow_{ up, down) is analogous to transitive. inter f ere, except that \nthe calls to interfere are replaced by calls to flow.interfere. flow-interfere is not symmetric, so there \nare two versions of the transitive routine to check for interference from the candidate set (down) land \nfrom the target set (up). ReadLinked computations can be moved into the independent set at the cost of \nadditional replication or movement. From the example, we could move the ReadLinked computation C into \nT1 if we are willing to replicate A or to move both B and D into TM, In general, to move a ReadLinked \ncomputation r into the independent set, every computation s from which r has a transitive fiow interference \nmust also be put in that set. Example of Split In our current implementation, we use a heuristic to decide \nwhether moving a member of ReadLinked is worthwhile. The heuristic goes ahead with the move if both of \nthe following are true: the number of floating point and integer compu\u00adtations in the code that is to \nbe replicated can be calculated and it is below a threshold o profiling data shows that the computation \nis ex\u00adpensive enough to justify moving it Note that when a sub-computation created by split\u00adting a loop \nnest is moved, some code must be added to the merge. If a nesting of independent loops is split, the \nmerge glues the results together. If a reduction is involved, a temporary scalar or array must be created \nand the results stored in it (as was done in Figure 4 for the reduction variable sum). As a final step \nin merging, the last reduction is performed. 3.3.2 Applications of Split In the discussion of the example \nin Section 2, three sources of concurrency are described, the first of which is a simple split. The other \ntwo, however, are pipelin\u00ad ing transformations that go beyond split. To pipeline a loop with split, first \nthe descriptor for one iteration of the loop is computed. If the induction variable is i, D~_ 1, the \ndescriptor for iteration i 1, is computed. Then the loop body is split using Di _ 1; the resulting independent \ncomputation does not interfere with iteration i 1. As iteration i is computed, the next iteration s independent \ncomputation can be executed concurrently, By transforming the code, that oppor\u00ad tunity is exposed to \nthe runtime scheduler. If deeper pipelining is desired, the descriptor for iteration i 2 can be computed, \netc.  3.4 The Intermediate Form The compiler outputs the transformed program in three forms. The first \nis a dataflow graph representing the parallel control structure. The graph is expressed in the coordination \nlanguage Delirium, a functional language with special support for describing data par\u00adallel operations \n[16]. The second form of output is a series of parallel and sequential sections in the original source \nlanguage. The sequential computations can invoke Delirium functions representing parallel computations \nvia the re-entrant runtime system. Dat aflow within each parallel section is described in Delirium. The \nfinal form of output is a set of annotations on each argument and return value of the Delirium functions, \ngiving data size and type information. The Delirium compiler translates this information into run\u00adtime \ncode for estimating communication costs. 4 Ituntime Support One consequence of using Delirium as an intermedi\u00adate \nform is that the source language compiler fixes the minimum grain size of the computation. The set of \nnon-re-entrant operators determines the minimum units of scheduling. Henceforth, we ll call these indi\u00advisible \nscheduling units tasks. While the minimum grain size of the computation is fixed by the front end analysis \ntool, the Delirium compiler and runtime system retain crucial flexibility in determining the grain size \nat which a data paral\u00adlel operation is scheduled. In a previous paper [14], we developed a quantitative \nrelationship between to\u00adtal available parallelism, optimal grain size, and execu\u00adtion time variance of \na parallel operation. We applied this relationship to the problem of choosing grain sizes for parallel \noperations, and demonstrated that adap\u00adtive scheduling is often both necessary and sufficient for efficient \nexecution of irregular parallel operations. In this section and the next, we demonstrate that interactions \namong parallel operations, both regular and irregular, can also benefit from adaptive strategies. 4.1 \nOrchestrating Interactions Among Parallel Operations Consider a runtime scenario in which the transformed \nparallel operations A and BI from Figure 3 are execut\u00ading simultaneously. A begins executing first and \nhas partially completed when B1 begins executing. The runtime system must decide how many processors \nto reallocate. If we change the example so that all proces\u00adsors must synchronize upon completion of A \nand 131, an ideal processor allocation would minimize the ex\u00adpected finishing time of these two parallel \noperations. The expected finishing time of a parallel operation is a function of the number of tasks \nthat make up the operation (iV), the number of processors cooperating to execute the operation (p), the \nvariance in task exe\u00adcution times, and the overhead of scheduling [11, 14]. Thus, approximating the ideal \nprocessor allocation re\u00adquires information available only at runtime. For this reason, we extended adaptive \nalgorithms de\u00adveloped for single irregular parallel operations to man\u00adage interactions among multiple, \nsimultaneously exe\u00adcuting parallel operations. There are three main exten\u00adsions. First, we developed \na method for improving the accuracy of estimated finishing times that works for a wide range of scheduling \nalgorithms. Second, we ap\u00adplied this method to the runtime processor allocation problem, using an iterative \nalgorithm to equalize fin\u00adishing time estimates. Finally, we combined finishing time estimates with runtime \ncommunication cost esti\u00admates to choose communication granularity for pairs of pipelined parallel operations. \n4.1.1 Individual Parallel Operations To provide a basis for describing these extensions, we review in \nthis section our adaptive runtime algorithms for executing single parallel operations on distributed \nmemory machines.2 We use a probabilistic algorithm called TAPER to select the grain-sizes at which tasks \nare scheduled. The runtime system samples task execution times to com\u00adpute their statistical mean (p) \nand variance (a2). It uses this information to reduce overhead by schedul\u00ading large chunks (groups of \ntssks) at the beginning of a parallel operation and successively smaller chunks as the computation proceeds. \nIf we define a scheduling event as the moment when a processor finishes execut\u00ading a chunk, then for \neach scheduling event i, TAPER computes Ki, the number of tasks in the iih chunk. The runtime system \ndoes additional sampling of task costs to build a cost function, which estimates task ex\u00adecution times \nas a function of iteration number within the parallel operation. We use the cost function to scale a \nchunk size Ki by s == p~ [PC. In this expres\u00adsion, pg is the global mean execution time and pc is the \nmean for the tasks in the current chunk. Regular parallel operations execute according to the owner-computes \nrule described in [9]. To execute irregular parallel operations, we begin with some orig\u00adinal data decomposition \nand assign tasks to processors zThe shared memory algorithm is described in [14] according to the owner-computes \nrule. During execut\u00adion, as the runtime system gains information about the work distribution, it refines \nthe data decomposi\u00adtion. In the distributed TAPER algorithm the p proces\u00adsors are logically connected \nas a binary tree with p leaves. Some of the processors act as both leaves and internal nodes of the tree. \nAll processors start in epoch O. When a processor begins executing a chunk it sends its current epoch \nvalue (called a token) to its parent, which pssses the token to its parent (possibly combin\u00ading messages \nfrom both children). When the root re\u00adceives p tokens from the same epoch, it increments the global epoch \nvalue and broadcasts a message through the tree to all processors. The message tells the pro\u00adcessors \nto increment their epoch value and may also tell some processors to transfer a chunk of tasks and their \nassociated data to another processor. Processors compete for the p chunks of each epoch. If processor \na can get two tokens of value i to the root before processor b can send one token of value i, then the \nroot will re-assign processor b s chunk of size K~ to processor a. Processor b is then forced to re-interpret \nthe chunk it is currently executing as be\u00adlonging to some later epoch (and thus containing fewer tasks). \nIf most of the actual task cost is on a few ;pro\u00adcessors, this scheme will degenerate into the central\u00adized \nTAPER algorithm. If task costs are independent then we expect most tasks to remain on the processor owning \nthem at the beginning of the parallel opera\u00adtion; thus, the algorithm reduces task transfer costs and \nmaintains communication locality. 4.1.2 Interacting Parallel Operations Now, we return to our runtime \nscenario in which the parallel operations A and BI are executing concur\u00adrently. When BI begins executing, \nthe runtime system must reallocate some of the p processors executing A. To accomplish this, the runtime \nsystem uses the fol\u00ad lowing iterative algorithm: epsilon = 570 pl =p/2, p2 =p pl, count = O eA = finish-estimate(A, \npi), eB = finish.estimate( B, p2) while ((count < maz.count) and (leA eB\\ > epsdon)) if(eA >eB) pl \n=pl +p2/2 p2=p pl else p2 = p2 +pl/2 pl=p p2 eA = finish-estimate(A, pl) eB = finish_estimate(B, p2) \ncount = count + 1 We limit the number of iterations to control the amount of overhead imposed. In practice, \nusing a max_count of four has been sufficient. In estimating finishing time, the runtime system uses \nthe expression: finish = setup+ compute+ lag+ comm + sched (1) setup is the maximum of the time to contract \nthe data required by A onto PI processors and the time to expand the data required by B1 onto PZ processors. \ncompute is the expected mean time to perform a por\u00adtion of the computation: Np/p (where p = pl for A \nand p = p2 for BI). lag is the expected maximum fin\u00adishing time to perform a portion of the computation; \nit is related to the distribution of task execution times for the computation (p,u) [14]. comm represents \nthe communication overhead of executing the given paral\u00adlel operation on p processors. To estimate comm \nat runtime, we use an algorithm like that suggested by Sarkar and Hennessy [18], which performs a weighted \nsum of dataflow graph edges that cross processor boundaries. Rather than perform this computation statically, \nthe Delirium compiler gener\u00adates code blocks that perform the estimate given run\u00adtime parameters such \nas N and pt. Finally, we must estimate the scheduling overhead (sched). To do so, we need to predict, \nat runtime, the number of chunks that will be scheduled for the parallel operation (hence the number \nof epochs in the distributed algorithm given above). The method for predicting this parameter is discussed \nelsewhere [14]; it applies to TAPER and many other algorithms for generating chunk sizes [10, 17, 20]. \nBy balancing the estimated finishing times of A and BI, the runtime system uses the extra concurrency \nfrom B1 to compensate for A s irregular execution be\u00adhavior. 5 Performance Results We validated the runtime \nalgorithms using a set of ap\u00adplications including the EMU circuit simulator [1], the UCLA General Circulation \nModel, an adaptive vor\u00adtex method for modeling turbulent fluid flow, and an image reconstruction program \nfor x-ray tomography (Psirrfan). More detail on these applications is given elsewhere [14]. We hand-transformed \neach program with split 3. The result was a Delirium description of the exposed parallelism and a transformed \nFORTRAN program (or 3 At the time we took the measurements, the integration be\u00adtween the run time system \nand compiler had not been finished 1000I I 11 I 1 900 -800 - TAPP,R wit,h 8pllt 700 - TAPER 600 - SW \n- static 400 - 300 -200 &#38; 100 I o~ 200 400 600 8(XI 1000 1200 Ncabe-2Processors Figure 6: Psirrfan \nperformance C program in the case of EMU). The Delirium compi\u00adlation was done automatically. With the \nruntime sys\u00adtem using the processor allocation algorithm described above, we were able to double the \nnumber of proces\u00adsors used for each application, with a loss of only five to fifteen percent in efficiency. \nFor example, using just the TAPER algorithm with cost functions, we could run the UCLA climate model \non 512 processors of an Ncube-2 multiprocessor at 87% efficiency, where efficiency is defined as the \nperfor\u00admance given the 512 processors divided by the sequen\u00adtial performance of the program. When we \nmodified the climate model using split wherever applicable, we were able to run the same input data set \n(about 3200 latitude-longitude grid cells) at 83% efficiency on 1024 processors. Hence the total speedup \nincreased from 445 to 850. Without this modification, the climate model s speedup on 1024 processors \nis only 581 (5770 efficiency) because of the irregular task execution times found in the cloud physics \nsection of the code. As shown in Figure 6, Psirrfan with just the TAPER algorithm and cost functions \nis highly eflicient on 512 processors but does not sustain this efficiency (with same input size) through \n1024 processors. However, by exposing additional coarse-grained parallelism and two opportunities for \npipelining, we transformed Psirrfan to achieve sustained efficiency of over 80~0 using up to 1024 processors. \n6 Conclusion Many parallel applications contain multiple, concur\u00adrently executing sub-computations with \ndistinct ex\u00adecution behaviors. To achieve efficient execution of such programs on large numbers of processors \nre\u00adquires a new approach to orchestrating the interac\u00adtions among program sub-computations. When two \nsub-computations are allowed to execute concurrently, a runtime scheduler can use the additional parallelism \nof one sub-computation to compensate for communi\u00adcation constraints or load imbalance in the other. To \nsupport this type of optimization, we have de\u00adveloped symbolic data summarization techniques and used \nthese techniques to implement the split transfor\u00admation, which can identify and expose concurrent and \npipelined interactions among sub-computations. We have also developed runtime algorithms which take advantage \nof this transformation by adaptively assign\u00ading processing resources to concurrently executing sub\u00adcomputations. \nWe have demonstrated the benefits of the transformation on a set of application programs in\u00adcluding x-ray \ntomography, circuit simulation, and cli\u00admate modeling. 7 Acknowledgements We gratefully thank David Bacon \nfor his helpful com\u00adments on an earlier draft. References [1] Bryan Acldand, Steven Lucco, Tom London, \nand Eric DeBenedictis. CEMU: A Parallel Circuit Simula\u00adtor, . In Proceedings of the International Conference \non Computer Design, October 1986. [2] John R. Allen and Ken Kennedy. Automatic Loop Interchange, . In \nProceedings of the SIGPLAN Sympo\u00adsium on Compiler Construction, pages 233 246, Mon\u00adtreal, Canada, June \n1984. [3] Vasanth Balasundaram. A Mechanism for Keeping Useful Internal Information in Parallel Programming \nTools: The Data Access Descriptor, . ~ournaJ of Par\u00adallel and Distributed Computing, 9:154 170, 1990. \n [4] David Callahan. A Global Approach to Detection of Parallelism. PhD thesis, Rice University, April \n1987. [5] Ron Cytron and Jeanne Ferrante. What s in a Name? -or-The Value of Renaming for Parallelism \nDetection and Storage Allocation, . In Proceedings of the inter\u00adnational Conference on Pamllel Processing \n(ICPP), pages 19-27, St. Charles, Illinois, August 1987. [6] Ron Cytron, Jeanne Ferrante, Barry K. Rosen, \nMark N. Wegman, and F. Kennth Zadeck. Efficiently Computing Static Single Assignment Form and the Control \nDependence Graph, . ACM Transactions on Programming Languages and Systems, 13(4):451-4,90, October 1991. \n[7] Geoffrey C. Fox. Domain Decomposition in Dis\u00adtributed and Shared Memory Environments?. In Pro\u00adceedings \nof the First International Conference on Su\u00adpercomputing, pages 1042-1073, June 1987. [8] Milind Girkar \nand Constantine Polychronopoulos. Compiling Issues for Supercomputers, . In Su\u00adpercomputing 88, pages \n164-173, Orlando, Florida, November 1988. [9] Seems Hiranandani, Ken Kennedy, and Chau-Wen Tseng. Compiler \nOptimizations for Fortran D on MIMD Distributed Memory Machines, . In Proceed\u00adings of Supercomputing \n91, pages 86 loo, Novem~ber 1991. [10] Susan Hummel, Edith Schonberg, and Lawrence Flynn. Factoring: \nA Practical and Robust Method for Scheduling Parallel Loops, . In Proceedings of Su\u00adpercomputing 91, \npages 610-619, November 1991. [11] C. Kruskal and A. Weiss. Allocating Independent Subtasks on Parallel \nProcessors, . IEEE Transactions on Software Engineering, SE-11, October 1985. [12] David J. Kuck, Robert \nH. Kuhn, David Padua, Bruce Leasure, and Michael Wolfe. Dependence Graphs and Compiler Optimizations, \n. In Eighth Annual ACM Symposium on Principles of Programming Languages (POPL), pages 207-218, Williamsburg, \nVirginia, Jan\u00aduary 1981. [13] Monica Lam. Software Pipelining: An Effective Scheduling Technique for \nVLIW Machines, . In ~Dro\u00adceedings of the ACM SIGPLA N 88 Conference! on Programming Language Design and \nImplementation (PLDI), pages 318-328, Atlanta, GA, June 1988. [14] Steven Lucco. A Dynamic Scheduling \nMethod for Irregular Parallel Programs, . In Proceeding:! of the ACM SIGPLAN 92 Conference on Programming \nLanguage Design and Implementation (PLDI), San Francisco, CA, June 1992. [15] Steven Lucco and Oliver \nSharp. Delirium: An lEm\u00adbedding Coordination Language, . In Supercomputing 90, pages 515-524, New York \nCity, New York, Novem\u00adber 1990. [16] Steven Lucco and Oliver Sharp. Parallel Program\u00adming With Coordination \nStructures, . In Eighteenth Annual ACM Symposium on Principles of Program\u00adming Languages (POPL), Orlando, \nFlorida, January 1991. [17] Constantine D. Polychronopoulis and David Kuck. Guided Self-Scheduling: A \nPractical Scheduling Scheme for Parallel Supercomputers, . IEEE Trans\u00adactions on Computers, C-36(12), \nDecember 1987. [18] Vivek Sarkar and John Hennessey. Partitioning Par\u00adallel Programs for Macro Dataflow, \n. In ACM Con\u00adference on Lisp and Functional %ogmmrning, pages 202-211, Cambridge, Mass., 1986. [19] Jaswinder \nP. Singh, C. HoIt, Takashi Totsuka, Anood Gupta, and John L. Hennessy. Load Balancing and Data Locality \nin Hierarchical N-body Methods, . Technical Report CSL-TR-92-505, Stanford Univer\u00adsity, February 1992. \n[20] Peiyi Tang and Pen-Chung Yew. Processor Selt-Scheduling for Multiple Nested Parallel Loops, . Pro\u00adceedings \nof the 1986 International Conference on Par\u00adallel Processing (ICPP), pages 528-535, August 1986. [21] \nMichael E. Wolf and Monica S. Lam. An Algorith\u00admic Approach to Compound Loop Transformation, . In Proceedings \nof the 3rd Workshop on Programming Languages and Compilers for Parallel Computing, Au\u00adgust 1990.   \n\t\t\t", "proc_id": "155090", "abstract": "<p>Many parallel programs contain multiple sub-computations, each with distinct communication and load balancing requirements. The traditional approach to compiling such programs is to impose a processor synchronization barrier between sub-computations, optimizing each as a separate entity. This paper develops a methodology for managing the interactions among sub-computations, avoiding strict synchronization where concurrent or pipelined relationships are possible.</p><p>Our approach to compiling parallel programs has two components: symbolic data access analysis and adaptive runtime support. We summarize the data access behavior of sub-computations (such as loop nests) and split them to expose concurrency and pipelining opportunities. The split transformation has been incorporated into an extended FORTRAN compiler, which outputs a FORTRAN 77 program augmented with calls to library routines written in C and a coarse-grained dataflow graph summarizing the exposed parallelism.</p><p>The compiler encodes symbolic information, including loop bounds and communication requirements, for an adaptive runtime system, which uses runtime information to improve the scheduling efficiency of irregular sub-computations. The runtime system incorporates algorithms that allocate processing resources to concurrently executing sub-computations and choose communication granularity. We have demonstrated that these dynamic techniques substantially improve performance on a range of production applications including climate modeling and x-ray tomography, expecially when large numbers of processors are available.</p>", "authors": [{"name": "Susan L. Graham", "author_profile_id": "81452606376", "affiliation": "", "person_id": "PP95034797", "email_address": "", "orcid_id": ""}, {"name": "Steven Lucco", "author_profile_id": "81100431977", "affiliation": "", "person_id": "PP42051841", "email_address": "", "orcid_id": ""}, {"name": "Oliver Sharp", "author_profile_id": "81100266702", "affiliation": "", "person_id": "PP31093950", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/155090.155100", "year": "1993", "article_id": "155100", "conference": "PLDI", "title": "Orchestrating interactions among parallel computations", "url": "http://dl.acm.org/citation.cfm?id=155100"}