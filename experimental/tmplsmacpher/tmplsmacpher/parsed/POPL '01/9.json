{"article_publication_date": "01-01-2001", "fulltext": "\n Efficient Deductive Methods for Program Analysis Haxald Ganzinger Max-Planck-Institut fiir Informatik \nSaarbriicken, Germany One of the goals in automated reasoning on a computer is to find a good balance \nbetween the level of abstraction at which a problem can be formulated and the efficiency by which it \ncan be solved. Deductive processes are not necessar- ily inefficient. In particular for propositional \n(variable-free) problems, efficient methods have become available. Satisfia- bility for ground Horn clauses \ncan be implemented in linear time [5]. A recent generalization of this result is given in [10], where \nit is shown that finite data bases can be saturated with respect to certain non-ground Horn theories \nin time linear in the number of prefix firings. (This is the number of ways a prefix of the antecedent \nof a Horn clause can pos- sibly match atoms is the saturated data base.) Congruence closure, the problem \nof deciding implication between sets of ground equations, can be computed in time O(m log m) [6]. These \nresults make two fundamental algorithmic concepts -- dynamic programming and balanced binary trees --avail-able \nat a rather high level of abstraction. In his work on \"lo- cal theories\" [9], McAllester has shown that \nmany inference problems for Horn theories with variables can be reduced to propositional problems and, \nhence, can be efficiently solved. A second line of research was concerned with improving saturation-based \nmethods for automated reasoning in first- order theories of transitive relations. As a result, better \nin- ferences systems were developed for equational theories and also for first-order theories of non-symmetric \ntransitive rela- tions [1, 2, 3, 11]. These methods employ inference systems that are parameterized by \norderings on ground terms and by selection functions. Orderings and selection functions re- strict inferences \nto only few positions in terms and clauses. Proof-theoretically such restricted inference systems avoid \ninefficiencies caused by searching for trivial variants of proofs as they can be obtained from proof \nrotation. In other words, term orderings and selection functions help to better distin- guish between \ndon't-care and don't-know nondeterminism in proof search. In addition, formal concepts were devel-oped \nthat capture many cases of global redundancy as they arise from subsumption on the level of proofs and \npartial proofs. These formal notions of redundancy allow one to justify elimination and simplification \ntechniques. For in- Permission to make digital or herd copies of all or part of this work for personal \nor classroom use is granted without fee provided that copies are not made or distributed for profit or \ncommercial advan- tage and that copies bear this notice and the full citation on the first page. To copy \notherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission \nand/or a fee. POPL'01 1101 London, UK &#38;#169; 2001 ACM ISBN 1-58113-336-7/01/0001 ...$5.00 stance, \na clause, the proof of which is subsumed by a proof of some equivalent clause, can be deleted without \naffecting completeness of proof search. When restricting the general first-order methods to variable-free \nHorn clauses, one often obtains decision pro- cedures that are competitive --in terms of computational \ncomplexity -- to the best known special purpose algorithms. As argued in [4], congruence closure can \nbe computed by ground completion in time O(m log m) and, hence, in time competitive with [6], if the \nordering on terms is chosen such that the length of rewriting chains is in O(log m). As demon- strated \nin [7] for the case of set inclusion constraints, ordered chaining, a proof system that generalizes Knuth/Bendix- \ncompletion to first-order theories of non-symmetric tran-sitive relations [2], can help break the cubic \nbottleneck in certain cases of dynamic transitive closure computation. We argue that program analysis \nis an application domain where these two strands of efficient automated deduction can be brought together \nand successfully applied. In many cases, a program analysis problem can be naturally formu- lated as \na problem of saturating a finite data base (a set of facts derived from a program) under a suitable Horn \ntheory. We also argue that in many cases a non-naive treatment of transitive relations is crucial for \nan efficient analysis. For example -- following the exposition in [10] --the typability problem in a \nvariant of the Abadi-Cardelli object calculus considered by Henglein [8] can be represented by the follow- \ning inference system: input(a.l) input( r.l) accepts(w, l) a < r accepts(r, l) rEp aEr erect a E p a.l \n= r.l That a = [ .... l : r,...] is an object type in a source pro- gram is assumed to be represented \nby typing .facts of the form accepts(a,l), input(o'.l), and a.l ~ r specifying the components of a and \ntheir types. Additionally we may be given subtyping facts of the form a < r. The main prop- erty to check \nis expressed by the third rule which requires component types a.l and r.l of (transitive) subtypes cr \nE r, so they both exist, to be equal. Type equality is supposed to be an equivalence which also satisfies \nthis subset of the 102 congruence axioms: tTET ff -.~-- T ~ --~- o \"t a ~ \"r accepts(a, l) 7\" --'-- \nr' a.l ~ ~'.l accepts(r, l) One of the typability criteria to be checked is that no new facts of the \nform accept(v, l) can be derived from the given typing facts. This would, otherwise, indicate that the \nthird rule is violated in at least one case. This theory is local, and from the first three inference \nrules only O(m 2) ground instances need to be generated when given m typing facts as input. Using recent \nresults about the complexity of sat- urating data bases with respect to a local equational Horn theory \n(satisfying also the structural axioms for ='), this im- plies that typability can be checked in quadratic \ntime. One can show that many examples of program analysis can be specified in a similarly natural way \nand that effi-cient analyses can be obtained by applying general purpose deductive methods for transitive \nrelations. References [1] L. Bachmair and H. Ganzinger. Rewrite-based equa- tional theorem proving with \nselection and simplifica- tion. J. Logic and Computation, 4(3):217-247, 1994. Revised version of Research \nReport MPI-I-91-208, 1991. [2] L. Bachmair and H. Ganzinger. Ordered chaining cal- culi for first-order \ntheories of transitive relations. J. As- sociation for Computing Machinery, 45(6):1007-1049, 1998. [3] \nL. Bachmair, H. Ganzinger, Chr. Lynch, and W. Sny- der. Basic pararnodulation. Information and Computa- \ntion, 121(2):172-192, 1995. Revised version of Research Report MPI-I-93-236, 1993. [4] Leo Bachmair and \nAshish Tiwari. Abstract congru-ence closure and specializations. In David McAllester, editor, Automated \nDeduction -CADE-17, 17th Inter- national Conference on Automated Deduction, LNAI 1831, pages 64-78, Pittsburgh, \nPA, USA, June 17-20, 2000. Springer-Verlag. [5] William F. Dowling and Jean H. Gallier. Linear-time algorithms \nfor testing the satisfiability of propositional Horn formulae. J. Logic Programming, 3:267-284, 1984. \n[6] P. J. Downey, R. Sethi, and R. E. Tarjaa. Variations on the common subexpressions problem. J. Association \nfor Computing Machinery, 27(4):771-785, 1980. [7] M. F~ihndrich, J.S. Foster, S. Zhendong, and A Aiken. \nPartial online cycle elimination in inclusion constraint graphs. In Proceedings of the ACM SIGPLAN '98 \nCon- ference on Programming Language Design and Imple- mentation, pages 85-96, Montreal, Canada, June \n17 - 19 1998. ACM. [9] D. A. McAllester. Automated recognition of tractabil- ity in inference relations. \nJ. Association for Computing Machinery, 40(2):284-303, 1993. [10] David McAllester. On the complexity \nanalysis of static analyses. In A. Cortesi and R. Fil6, editors, Static Anal- ysis --6th International \nSymposium, SAS'99, LNCS 1694, pages 312-329, Venice, Italy, September 1999. Springer-Verlag. [11] R. \nNieuwenhuis and A. Rubio. Theorem proving with ordering and equality constrained clauses. J. Symbolic \nComputation, 19(4):321-352, 1995. [8] F. Henglein. Breaking through the n 3 barrier: Faster object type \ninference. Theory and Practice of Object Systems, 5(1):57-72, 1999. A preliminary version ap- peared \nin FOOL4. 103 \n\t\t\t", "proc_id": "360204", "abstract": "", "authors": [{"name": "Harald Ganzinger", "author_profile_id": "81100475310", "affiliation": "Max-Planck-Institut f&#252;r Informatik, Saarbr&#252;cken, Germany", "person_id": "PP39044530", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/360204.360212", "year": "2001", "article_id": "360212", "conference": "POPL", "title": "Efficient deductive methods for program analysis", "url": "http://dl.acm.org/citation.cfm?id=360212"}