{"article_publication_date": "01-01-2001", "fulltext": "\n A Compiler Technique for Improving Whole-Program Locality Mahmut Taylan Kandemir Department of Computer \nScience and Engineering Pennsylvania State University University Park, PA 16802, USA  kandemir@cse.psu.edu \nABSTRACT Exploiting spatial and temporal locality is essential for ob- taining high performance on modern \ncomputers. Writing programs that exhibit high locality of reference is difficult and error-prone. Compiler \nresearchers have developed loop transformations that allow the conversion of programs to exploit locality. \nRecently, transformations that change the memory layouts of multi-dimensional arrays---called data transformations--have \nbeen proposed. Unfortunately, both data and loop transformations have some important draw- backs. In \nthis work, we present an integrated framework that uses loop and data transformations in concert to ex-ploit \nthe benefits of both approaches while minimizing the impact of their disadvantages. Our approach works \ninter- procedurally on acyclic call graphs, uses profile data to elim- inate layout conflicts, and is \nunique in its capability of re- solving conflicting layout requirements of different references to the \nsame array in the same nest and in different nests for regular array-based applications. The optimization \ntechnique presented in this paper has been implemented in a source-to-source translator. We eval- uate \nits performance using standard benchmark suites and several math libraries (complete programs) with large \ninput sizes. Experimental results show that our approach reduces the overall execution times of original \ncodes by 17.5% on the average. This reduction comes from three important charac- teristics of the technique, \nnamely, resolving layout conflicts between references to the same array in a loop nest, deter- mining \na suitable order to propagate layout modifications across loop nests, and propagating layouts between \ndifferent procedures in the program --all in a unified framework. Keywords Optimizing Compilers, Memory \nLayouts, Static Optimiza- tions, Cache Locality, Data Reuse. Permission to make digital or hard copies \nof all or part of this work for personal or classroom use is granted without fee provided that copies \nare not made or distributed for profit or commercial advan- tage and that copies bear this notice end \nthe full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute \nto lists, requires prior specific permission and/or a fee. POPL'O1 1/01 London, UK &#38;#169; 2001 ACM \nISBN 1-58113-336-7/0110001 ...$5.00  1. INTRODUCTION Over the last decade, the gap between memory latency \nand processor speeds has continued to widen. Main mem- ory latencies for new machines are now more than \nhundred cycles. This has resulted in the increasing reliance on caches as a means to increase the overall \nmemory bandwidth and reduce memory latency. From an application point of view, programs that process \nlarge data sets can achieve high levels of performance only if they exploit data caches: data once read \nfrom the main memory to the high-speed cache must be used as many times as possible before being written \nback to main memory. Unfortunately, the straightforward coding of many programs results in poor data \nlocality. Also, hand- tuning programs in order to improve cache performance is a tedious and error-prone \ntask. Therefore, compiler optimiza- tions aimed at improving locality have been very attractive, particularly \nfor scientific programs that exhibit regular data access patterns [46]. Several loop-level transformations \nhave been proposed for improving the cache locality of programs [28, 31, 29, 10, 34, 44]; these aim to \nimprove locality by deriving better data access patterns as a result of changing the order of execu- \ntion of loop iterations. Consider the Fortran-like loop nest shown in Figure l(a). With a column-major \nstorage order for a large array U, the cache performance of this code might be very poor. Consecutive \niterations of the innermost loop (the j loop) access elements of U from different columns and between \nsuccessive reuses of the same column, the cache lines containing the elements will most probably have \nbeen evicted from the cache. This is a function of several fac- tors such as array sizes, cache replacement \npolicy, cache size, and associativity. In this example, interchanging the i and j loops might result \nin significantly better cache performance, since consecutive iterations of the new inner loop (the i \nloop now) access contiguous data in memory. While loop trans- formations can improve both temporal and \nspatial locality, are well-understood, and are effective in many cases, they have at least three important \ndrawbacks: (1) they are con- strained by data dependences [46]; (2) complex imperfectly nested loops \npose a challenge for loop transformations; and (3) they affect the locality characteristics of all the \ndata sets (e.g., arrays) accessed in a nest, some perhaps adversely. In order to illustrate the last \npoint, consider the code in Fig- ure l(b). Here, assuming column-major memory layouts, the locality for \narray U is poor whereas that for array V is good. Interchanging the loops will improve the locality for \narray U while severely degrading the locality for array V. do i = I, N do i = l, N do i = l, N do u = \n-N+l, N-1 do j --1, N do j --1, N do j --1, N do v --max(l-u,1), min(N-u,N) .... U(i,j) .... U(i,j) + \nY(j,i) .... U(i,j) + U(j,i) .... U(u+v,v) + U(v,u+v) end do end do end do end do (a) (b) (c) (d) Figure \n1: Example loop nests. (a) A loop nest that can be optimized by loop or data transformations. (b) A loop \nnest that can be optimized by data transformations. (c) A loop nest that requires an integrated (loop \nplus data) approach. (d) Optimized version of (c). Recently, data transformations [8, 24, 23, 20, 30, \n38] have been proposed to improve locality in situations where loop transformations are not effective. \nInstead of changing the or- der of loop iterations, data transformations modify the mem- ory layouts \nof multi-dimensional arrays (from a language-defined default such as column-major in Fortran and row- \nmajor in C into a desired form). Let us look at the code in Figure l(b) again. Suppose we could choose \na row-major layout for array U and a column-major layout for array V, the cache performance of the code \nwould be significantly better. Many of the techniques for data transformations proposed so far (e.g., \n[8, 24]) are limited to dimension re- indexing (e.g., conversion from column-major to row-major and vice-versa); \npowerful techniques that consider more com- plex layouts (e.g., diagonal layouts) have also been shown \nto be useful [23, 38, 30]. For example, diagonal layouts are very effective in improving locality in \nbanded-matrix codes [30, 23]. Data transformations have some drawbacks in that they cannot improve temporal \nlocality directly and (in the case of a single layout for an array for the whole program) a data transformation \naffects the locality performance of all the loop nests that access the array in question; that is, the \nimpact of a layout transformation is global. Nevertheless, they have the following advantages: (1) they \nare not con-strained by data dependences and are easily applicable to perfect and imperfect nests [8]; \n(2) in a single loop nest, the data layout choices for different arrays are independent; and (3) where \nloop transformations fail, data transforma- tions might be able to improve spatial locality. Several \ntechniques have been proposed for combining loop and data transformations in a single framework [3, 8, \n24, 20, 21]. Some of these techniques limit the scope of data and loop transformations [3, 8, 24] used \nin their framework; for example, they do not consider diagonal memory layouts for arrays. Others [30, \n24] have focused mainly on a single loop nest; it is not clear how their techniques extend to whole programs \nconsisting of multiple loop nests. The main limitation of the previous work on pure loop, pure data, \nand combined loop/data transformations is that they optimize only a single procedure at a time, meaning \nthat the layouts should be explicitly transformed between procedures. Note that these explicit transformation \n(re-mapping) operations (called copy loops) can offset any gains that may be obtained from layout optimizations. \nThe sec-ond problem with the previous work is that they do not propose a well-defined framework that \ntells us the order in which the loop nests in a procedure should be processed. This is an important problem \nas a wrong processing order can easily lead to unnecessary layout conflicts between nests and eventually \nto unoptimized memory layouts. And finally, within a single nest ff there are different references to \nthe same array, they fail to propose a solid conflict resolution (or reconciliation) scheme. We believe \nthat unless these three problems in the inter-procedural, intra-procedural, and loop nest level are addressed \nappropriately, any locality optimiza- tion framework that involves data layout transformations will not \nobtain strong acceptance in the optimizing compiler community. This paper presents a compiler technique \nthat uses data and loop transformations in concert to optimize data locality of the entire program. In \ntheory, such a framework is clearly superior to those that use just loop or just data transfor- mations. \nIn practice, several problems such as dependence constraints, global effect of data transformations, \nand the issue of propagating layouts across procedures pose serious challenges to the task of integrating \nloop and data trans- formations for improving locality. To the best of our knowl- edge, none of the proposed \napproaches can optimize the code in Figure l(c) using a combination of linear (dimension-preserving) \nloop and data transformations. This code ex- hibits a non-trivial access pattern, despite its apparent \nsim- plicity. As we move from one iteration of the inner loop to the next iteration, the code accesses \nthe next elements in the same row and the same column; this results in conflicting preferences for the \nmemory layout of the array. Loop inter- change also cannot avoid this conflict in layout preference. \nExcept for tiling [29, 10], most of the loop transformation techniques will not be able to improve the \ncache performance for this code. Nevertheless, it is possible to optimize this nest for locality using \nthe framework developed in this pa- per. Consider the transformed version of this code as shown in Figure \nl(d). Here, the locality is good for both the refer- ences, provided that array U has a diagonal memory \nlayout. This means that---except near the array boundaries--array elements U(i,j) and U(i + 1,j \u00f7 1) \nshould be stored con- tiguously in memory. Essentially, what we have done here is to skew the iteration \nspace (defined by the loop iterations) as well as the data space (defined by the array elements) such \nthat consecutive loop iterations will access contiguous array elements in memory. Notice that although \nthe result- ing references are not the same, both access the array in the same (diagonal) fashion, leading, \nhopefully, to decent spa- tial locality. In [20, 21], unified approaches for optimizing locality have \nbeen proposed. In order to determine a good layout/loop order combination, these approaches need some \nhints. For example, the approach in [20] assumes that the most costly nest in the program will be optimized \nby either pure loop or pure data transformations, but not both. On the other hand, the approach in [21] \nassumes that aggressive op- timization for temporal locality using loop transformations will result in \nthe best possible code, which may not always be the case. Therefore, neither [20] nor [21] can derive \nthe code in Figure l(d). In addition, both [20] and [21] can op- timize a single procedure at a time, \nresorting, practically, to explicit layout transformations in procedure boundaries. The problem that \nwe tackle in this paper is to find a good combination of loop and data transformations to optimize cache \nlocality program-wide. Specifically, we make the fol- lowing contributions:  We describe a very general \nframework which uses loop and data transformations in a systematic way such that overall cache locality \nacross the entire program is enhanced.  We characterize the cases where it is possible to optimize all \nthe references to the same array in a nest in a conflict-free manner.  We show how our approach handles \nmultiple loop nests in a given procedure and multiple procedures in a given pro- gram.  We present experimental \nresults that demonstrate clear benefits.  We believe that this study is also the first extensive and \nprogram-wide evaluation of a cache locality optimization tech- nique that involves both loop and data \ntransformations. The results reveal that all three important aspects of our ap-proach, namely, resolving \nlayout conflicts, propagating lay- outs across the nests, and propagating layouts across pro- cedures \nare critical for obtaining reasonable performance on deep memory hierarchies. The rest of this paper \nis organized as follows. Section 2 presents the background needed. Section 3 gives an outline of our \napproach to integrating loop and data transforma- tions. In Section 4, we present and discuss experimental \nresults. In Section 5, we review related work, and we con- clude in Section 6 with a summary and discussion. \n 2. BACKGROUND AND NOTATION We consider the case of references to arrays with affine subscript functions \nin nested loops, which are common in regular scientific codes. Consider such an access to an m- dimensional \narray in an n-deep loop nest. Let .t denote the iteration vector (consisting of loop indices starting \nfrom the outermost loop). Each array reference can be represented as Ei+ 6, where the m x n matrix \u00a3 \nis called the access (or reference) matrix [44] and the m-element vector 6 is called an offset vector. \nIn order to illustrate the concept, we consider the loop nest shown in Figure l(a). For the reference \nshown in the nest, we have i An element is reused if it is accessed (read or written) more than once \nin a loop. There are two types of reuses: temporal and spatial. Temporal reuse occurs when two references \n(not necessarily distinct) access the same memory location; spa- tial reuse arises between two references \nthat access nearby memory locations. The notion of nearby locations is a func- tion of the memory layout \nof elements. In this paper, we focus primarily on self-reuses (i.e., reuses originating from individual \nreferences); our approach, however, easily extends to handle group-reuses. It is important to note that \nthe most important reuses (whether temporal or spatial) are the ones exhibited by the innermost loop. \nIf the innermost loop ex- hibits temporal reuse for a reference, then the element ac- cessed by that \nreference can be kept in a register throughout the execution of the innermost loop (assuming that there \nis no aliasing [36]). Similarly, spatial reuse is most beneficial when it occurs in the innermost loop \nbecause, in that case, it may enable unit-stride accesses to consecutive locations in memory. It is also \nimportant to observe that reuse is an intrinsic property of a program. Whenever a data item is present \nin the cache at the time of reuse, we say that the ref- erence to that item exhibits locality. The key \noptimization problem is then to convert patterns of reuse into locality, which depends on a number of \nfactors such as cache size, associativity, and block replacement policy. Consider again the loop nest \ngiven in Figure l(a). In this loop nest, each it- eration accesses a distinct array element and thus \nthere is no temporal reuse. However, assuming a column-major mem- ory layout, successive iterations of \nthe i loop (for a fixed value of j) access the neighboring elements on the same col- umn. In this case, \nwe say that the reference exhibits spatial reuse in the i loop; in other words, the i loop carries spatial \nreuse. However, assuming that array size and loop bounds are sufficiently large, it will be difficult \nto convert this reuse into locality since between the two successive iterations of the/-loop that access \nthe same cache line, all the iterations of the j-loop need to be executed. Notice that, if we can make \nthe i loop innermost, then (since this loop carries the reuse), the chances for converting reuse into \nlocality will be much higher. Consider the application of a non-singular linear loop trans formation \nto an n-deep loop nest that accesses an array with the subscript function represented as/:i + &#38; This \ntransfor- mation can be represented by an n x n non-singular trans- formation matrix T, and maps the \niteration _7 of the original loop nest to the iteration _P = T.r of the transformed loop nest [44, 31, \n46]. On applying the transformation, the new subscript function is ET-1i ' + 6. The transformation also \naffects the loop bounds of the iteration space that can be computed using techniques such as Fourier-Motzkin \nelimi- nation [46]. In this paper, we denote T -1 by Q, and column j of Q = [q~] is written as ~j.  \n3. OUR APPROACH Our approach to the locality problem is based on deter- mining the access patterns exhibited \nby different references in loop nests. For ease of exposition, we focus mainly on two-dimensional arrays, \nbut our results easily generalize to higher-dimensional cases. Given a reference represented by access \nmatrix E, the access pattern imposed by this refer- ence can be determined by looking at the rightmost \ncolumn (last column) of \u00a3. Note that this column corresponds to the innermost loop in the nest. In particular, \nassume that ( /11 /12 \"'\" lln ) = ~21 ~22 \"'\" ~2n for a reference to a two-dimensional array in an n-deep \nloop nest. In this case, In = (ix,~,12,~) T determines the access pattern exhibited by this reference \n[4]. For example, if T~ = (a, 0) T, it means that when we increase the innermost loop index by one we \nhave a displacement in the (a, 0) direction; that is, we move along the same column by an amount of a \nelements. Similarly, a displacement in the (0, b) direction corresponds to row-wise accesses. On the \nother hand, an [, such as (a, b) T causes a displacement in both directions. An important case arises \nwhen l,~ = (0, 0) T. Notice that this corresponds to a situation where the innermost loop index does \nnot appear in any subscript position of the ref- erence. In this case, the access pattern for this reference \nis imposed by Tj, where llj or 12j is non-zero and j is the largest of all which satisfy this condition. \nIt is easy to see that zero displacement corresponds to temporal reuse. Note 181 that our definition \nof displacement is not limited to just the innermost loop. It can be defined with respect to any loop \nindex enclosing the reference in question. If there is just one reference to an array, then this refer- \nence alone imposes an access pattern for the entire array. A problem occurs when two or more references \nto the same array impose conflicting access patterns. An example loop nest where this happens is shown \nin Figure l(c). In this nest, one displacement is along (0, 1) (row-wise) direction and the other is \nalong (I, 0) (column-wise) direction. In order to rec- oncile these two displacement directions, our \napproach uses loop transformations as follows. We first consider the sce- nario in which the innermost \nloop index is increased from j to j + 1 while the outermost loop index is fixed at a specific i value. \nWe define two iteration vectors: Y = (i, j)T and i + = (i,j + 1) T, where j < N. Essentially, what we \nwant is to transform the loop nest such that (after the transforma- tion) the two references represented \nby \u00a31 and \u00a32 will have the same displacement direction. In mathematical terms, {/:iQi +} -{\u00a31Qi} = {\u00a32Qi \n+} -{\u00a32Qi} should hold. From this equation, we obtain ~lq~ = \u00a32q,~, where qn = (qx2, q22) T is the last \ncolumn of Q. Since in our   case (10) (o 1) /~1 ---- 0 1 and ~2 = 1 0 , we have (1 0)(0 1) 0 1 q~= \n1 0 qn, which results in q12 = q22. Setting q12 = q22 = 1, we derive an example solution Q--0 1 \" Using \nthis matrix as the inverse of the loop transformation matrix T, we obtain the code shown in Figure l(d). \nIn this code, both the references have the same displacement direction. However, now the displacement \ndirection is not along columns; recall that the default layout in Fortran is column-major. This is exactly \nwhere data trans]ormations come in to the picture. If we can choose a memory layout in which the consecutive \nelements are stored along this new displacement direction, then we get spatial reuse for all the references \nto array U. Therefore, we transform the memory layout of array U from the default column-major to the \ndi- agonal layout, where successive data elements reside along the vector (1, 1), which is the new displacement \ndirection for both the references. This example, although quite simple, illustrates our ap-proach to \nthe locality problem in a single nest. First, con- sidering all references to all arrays in the nest, \nwe strive to come up with a suitable loop transformation that imposes the same displacement direction \nfor as many references to the same array as possible. Of course, different arrays in general may have \ndifferent displacement directions. After finding the loop transformation, considering each array in- \ndividually, we apply data layout transformations to comply their memory layouts with the new displacement \ndirections. Experimental data show that our approach is superior to those proposed earlier [8, 20, 21, \n38, 30] in resolving conflicts between access patterns of different references to the same array in a \ngiven nest. Once the desired memory layout is determined, the question of how this layout is implemented \n(in a given language) is relatively easy to answer. We omit the details of this and refer the interested \nreader to relevant works on code generation for data layout transformations [30, 38, 23]. Some layouts \nmay require more memory space than the default layouts and in such cases data space reduc- tion techniques \n(that do not affect spatial locality) proposed by Leung and Zahorjan [30] will be used. Of course, things \nare not always so straightforward in practice. In the following, we first discuss some problem- atic \naspects, and then present our approach for the most general case. Data Dependences: The first important \nissue is the se- lection of a ~ from among many possible candidates; and (once selected) how to complete \n~,~ to a matrix Q such that all the data dependences in the original nest will be pre- served. For example, \nin the case of ql --q2 in the previous example, we could easily have chosen ql = q2 = 2 instead of ql \n= q2 = 1. Our rule is to select a q~ (among all suitable candidates) such that gcd(ql~,q2,~, ..., q,~) \nis minimal. We have a good reason for such a selection. In most of the cases where gcd{ql,~,q2,~, ..., \nq,,~} is minimal we have at least one i such that qin ----1. In those cases, it is possible to complete \nthe inverse of the loop transformation matrix to a unimodu-lar [46] matrix, which is, in general, preferable \nas it involves less loop overhead compared to a non-unimodular transfor-mation. Notice that once we select \na ~n, we automatically determine a suitable tl (first row of the loop transforma- tion matrix) as tl \ne Ker(q,~}. Once we have a ~1, we use dependence-aware matrix completion methods proposed by Li [31] \nand Bik and Wijshoff [5] (which guarantee a legal transformation) to complete it to a full T without \nviolating any data dependences. Global Impact of Layout Transformations: The second important issue is \nthat of minimizing the global impact of a memory layout transformation. This is important since a particular \nchoice of memory layout of an array affects the spatial locality characteristics of all loop nests that \naccess the said array. The approach that we choose to handle this problem is based on maximum branching \n[35] and is explained in Section 3.2. Inter-procedural Issues: The third issue is that the ap- plicability \nof data transformations is constrained by the pa- rameter passing rules and the sequence and storage \nassoci- ation rules of the language in question as well as the use of pointer arithmetic [8]. In some \ncases, it might be nec-essary to apply runtime checks to determine whether it is safe to apply a candidate \ndata transformation. We do not investigate these issues in this paper and assume that the data transformations \nwe apply are always legal. Of course, this may not always be true; in those cases, we can use the techniques \nproposed by Chandra et al. [6]. Alternatively, we may choose to apply our technique only in cases where \ncom- piler could detect (statically) that there exists no aliasing between arrays of interest. A related \nimportant issue is the propagation of layout transformations across procedures. In Section 3.3, we illustrate \nhow to propagate memory layouts across procedures using call graph representation of the pro- gram. In \na very recent work, O'Boyle and Knijnenburg [39] have proposed an elegant technique to propagate data \ntrans- formations in the presence of reshaping between procedure boundaries. When array reshaping occurs, \nour layout prop- agation scheme can be extended to include their techniques.  Step (1): Using data reuse \ntheory [44], determine all optimization alternatives. Step (2).\" For each alternative (where in general \nsome arrays have temporal reuse and others have spatial reuse): (2.a) For each array i with temporal \nreuse in the current op- timization alternative, set up (2.b) For each array j without temporal reuse \nin the current optimization alternative, set up \u00a3jlgl,~ = Ej2qn = ... : Ejtjq,~ (2.c) If these conditions \nyield a non-trivial solution, then com- plete this q~ to a Q, determine the new layouts, and move to \nthe next alternative; else eliminate an equality (corre- sponding to least frequently accessed reference) \nand re- peat steps (2.a), (2.b), and (2.c). Step (3): Compare the locality coefficients of the alternatives, \nand select the solution (alternative) with the largest locality coef- ficient. Figure 2: Algorithm for \noptimizing a single loop nest. a2 n2 a2 2~._.n2 a2~-~ n2 a2~1 n2 a3 n3 a3 ~ ~)13 a3 ~Arl \"~ n3 a3 ~3 \nI~ n3 (a) (b) (c) (d) Figure 3: (a) An example movement graph (MG). (b-d) Three different solutions. \nMG is a bi-partite graph where each node represents a loop nest or an array; there is an edge between \na loop node n\u00b1 and an array node aj if and only if the array represented by aj is accessed in the nest \nrepresented by ni. The direction of the edge indicates whether a loop transformation for n\u00a3 determines \nthe layout of aj (a directed edge from ni to aj) or a memory layout for aj determines the loop transforma- \ntion for n\u00b1 (a directed edge from aj to hi). For example, Figure 3(a) shows an MG for a procedure with \nthree nests (nl, n2, and n3) and three arrays (al, a2, and a3). Initially, all the edges are bi-directional, \nmeaning that all the possible movements are possible (i.e., no layout or loop transforma- tion has been \ndetermined yet). Figure 3(b) depicts a solution that can be described informally as follows. First, using \nthe algorithm given in Figure 2, we determine the loop order for nl and the memory layouts for al and \na2. This step corresponds to edges labeled 1 in Figure 3(b). Then, in the second step (using the layout \nof a2), we determine the neces- sary loop transformation for nest n2 to obtain good locality (with the \nnew layout of the array a2); also, using the layouts of both al and a2, we determine the required loop \ntransfor- mation for n3. The edges corresponding to these activities are marked by 2 in the figure. In \nthe last step, using the loop order for n2 (after the transformation), we specify the memory layout for \na3. This step is indicated by the edge la- beled 3, going from n2 to a3. Thus, Figure 3(b) corresponds \nto a solution that starts with the loop nest nl. Similarly, Figures 3(c) and (d) correspond to solutions \nthat start with nests n2 and n3, respectively. In each solution (b through d), possible conflicts are \nalso marked using dashed circles. For example, in Figure 3(b), we might have a conflict in de- termining \nthe loop transformation for n3, as both al mad a2 might require different loop transformations for n3 \nfor their respective layouts. Similarly, in Figure 3(c), we may have a conflict in determining the layout \nof al, as nl and n3 can demand different (preferred) layouts for this array. In the solution shown in \nFigure 3(d), on the other hand, we might have a conflict in determining a suitable loop transformation \nfor nl. Our approach tries to find a solution (that is, specifying the directions of the edges in a given \nMG) that minimizes the number of conflicts. In case this is not possible, it derives a solution with \nthe minimum number of conflicts. In an MG representation, this directly corresponds to minimizing the \nnumber of nodes with an in-degree of 2 or more; note that this problem is a variant of maximum branching \n[35] where, given a graph, we find a directed tree such that each node has (at most) an in-degree of \n1. Our approach performs three important optimizations as well. First, it distinguishes between conflicts \noccurring in an array node and conflicts occurring in a loop node. If a conflict occurs in an array node, \nwe have to sacrifice locality of the array in question in one of the nests unless the conflicting nests \ndemand the same layout for that array. If a conflict occurs in a loop node, however, we might still have \nthe chance of coming up with a suitable loop transformation that satisfies the layouts of the arrays \ninvolved. This is particularly true in cases where only two or three arrays are conflicting. Consequently, \neverything else being equal, we prefer loop conflicts over layout conflicts. The second optimization \nthat our approach performs is aimed at breaking a tie that cannot be broken using the first optimization. \nIn case we have, lets say, two alternative solutions, each with a single conflicting loop node, we prefer \nthe one with the minimum total weights of the edges incident to it. These weights are assigned to the \nedges (using profile data) when we are building the MG, and correspond to the importance of optimizing \nthe array in question in the nest. Currently, we set the weight of the edge between aj and n\u00b1 to the \nnumber of accesses to the array represented by aj in the nest represented by n\u00b1 in a typical run. This \noptimiza- tion attempts to reduce the impact of the conflict on the overall locality behavior of the \nprocedure. Also, note that this second optimization indirectly takes into account the array sizes and \nnumber of loop iterations as well, because, the larger these values, the higher the weights of the edges \ninvolving them. Finally, during the search for a suitable so- lution, if we find a solution that exhibits \nno conflict, we stop the algorithm and return this solution, thereby reducing the compile-time. It should \nbe noted that during the solution, we might come across a situation where we need to optimize a loop \nnest in which the layouts of some of the arrays the loop ac- cesses have already been fixed (during processing \nof another nest). For example, in Figure 3(b), when we start processing n2, the layout of a2 is fixed \nand that of a3 is not yet fixed. What we need to do is to find a suitable loop transformation for n2 \n(taking into account the already determined layout of a2), and to find a suitable layout for a3. In the \nfollowing, we explain how to solve this constrained optimization problem in the general case. Assume \nthat (during the optimization process) we arrive at a nest that accesses 6 arrays. Assume further that \nthe memory layouts of a of these arrays have already been de- termined during the processing of previous \nnests. Also, as- sume that the displacement direction determined for array i (1 < i < a) is ~i. In this \ncase, for optimizing the current nest, the compiler sets up the following two sets of equalities: Vi \n [1...a] /~ilq,~ = Z~i24~ ..... /~it, e/n = ~i (3) Vj [(a + 1).-6] ~jlqn = \u00a3j2~n ..... \u00a3jt~, (4) Notice \nthat set of equalities given by (3) takes the layouts found so )ear into account, whereas the equalities \ngiven by (4) involve the arrays whose layouts are yet to be determined. Solving these two sets of equalities \ntogether gives us a suit- able ~, that in turn can be completed to a Q matrix for the loop in question. \nThen, using the new loop access pattern, we can determine the layouts for the arrays a + 1 through 6. \nNote that our approach subsumes pure loop transformations by fixing the same value for the ~ vectors \nfor all the arrays in each and every nest. To illustrate our approach for the multiple loop nest case, \nwe consider the program fragment in Figure 4(a). This frag- ment contains two loop nests (nl and n2 from \ntop) and three arrays (U, V, and W). The MG for this fragment is shown in Figure 4(c). The access matrices \nfor nl are as follows: (1 0 0) ( ) /~Vl = 0 1 i ; /~Vl = 0 0 1 0 1 0 0 I -1 ; and 0 0 i)/~vz = 0 1 \n0 \" And for the second nest (n2),  ( oo) (o01) ,t~U2 = 0 1 1 ; ~W1 = 0 1 0 ; 0 1 --1 1 1 1 (011) (1 \n10) Lw2 = 0 0 1 ; and ~w8 = 0 0 1 . 1 0 1 1 1 0 Let Q = [ql, q2, (la] and R = [~x, ~2, ~a] be the inverses \nof the loop transformation matrices for nl and n2, respectively. Our objective is to determine Q and \nR as well as memory layouts for the arrays U, V, and W. Assuming the solu- tion process depicted in Figure \n4(d) (returned by maximum branching), we first focus on nl. The data reuse theory [44] says that the \narray V has temporal reuse. Using the condition ~vlq3 = /:v2qa = 00, we obtain ~a = (1,0,0) T. Therefore, \na suitable Q is (001) 0 I 0 , 1 0 0 which, in turn, leads to a displacement vector (1,0, 0) for array \nU, meaning that its layout should be column-major. Note that since array U is referenced in this nest \nonly once, no condition is set up for it. Note also that the other alter- native, namely exploiting for \nboth arrays only spatial locality will not result in a better code here. Next, we process n2 and in optimizing \nit, we take the layout of U found in the previous nest into account. First, we try to solve the following \nsystem: {Z:u2f3 = (1, 0, 0) T /~wlr3 =/~Wzff3 =/:wara}- Step (1): Pre-process the loop nests in the procedure \nusing loop fusion, loop distribution, and code sinking to isolate as many nests as possible. Step (2): \nBuild an MG for the resulting procedure. Step (3): Starting from each nest node in turn (3.a) Using maximum \nbranching determine a solution process. (3.b) Evaluate the solution found using the number of con- flicts, \ntype of conflicts, and (where necessary) weights of the edges incident on the nodes involved in conflicts, \nStep (4): Select the best solution alternative. Step (5)~ Once the best solution alternative is chosen, \napply the algorithm given in Figure 2 and its constrained version to the nests. Figure 5: Procedure-wide \nlocality optimization al-gorithm. Since this system has no solution, we need to ignore some equations. \nIgnoring the last equation (using, for example, profile data), and solving the system {\u00a3U2~3 = (1, 0, \n0) T ~Wl~3 = \u00a3w2r3}, gives us ffz = (1, 0, 0) T which results in R = Q. This result implies that the \nlayout of array W should be row-major. The transformed code is shown in Figure 4(b). Note that since \n(after the transformation) array V in the first nest has tem- poral locality in the innermost loop (w), \nthe memory layout for V is of secondary importance. However, our approach can be extended to determine \nthat the layout for array V should be row-major in order for good spatial locality in the second innermost \nloop (v). A sketch of the overall algorithm for optimizing locality in the multiple-nest case (procedure-wide) \nis shown in Figure 5. 3.3 Multiple nests, multiple procedures Our approach for handling multiple procedures \nis based on a two-pass analysis on the call graph representation of the program. In a call graph [36], \neach procedure is rep- resented by a node and an edge from a node p to a node q indicates that p calls \nq. We currently do not handle the programs with recursive function calls or procedure-valued variables. \nIn order to optimize locality inter-procedurally, we need to propagate layouts across procedure boundaries. \nNote that this approach is fundamentally different from in- lining. While we slightly add to the complexity \nof the overall locality optimization framework, we avoid the potential code explosion that might be caused \nby in-lining. Our optimization process has two passes: a bottom-up pass and a top-down pass. In the bottom-up \npass, we first handle the leaves in the call graph. However, instead of solving the equations and determining \nlayouts and loop transformations right away, we just build an MG for each leaf procedure and propagate \nit up in the call graph to its parent(s). In the par- ent node, we combine the local MG with the MGs \nobtained from children nodes. In that way, we process each node in the call graph if and only if all \nof its children have been processed. At each propagation step, we combine the MGs being moved up in the \ncall graph with the local MG. When we reach the root (the main procedure in the program), we have a global \nMG that provides us with all the relations between arrays and loop nests that can somehow have an do \ni = l, N do j=l, N dok--1, N V(i,j+k,j) = V(k,j-k) + V(k,j) end do doi= l,N do j = 1, N do k = l, N U(i,j+k,j-k)= \nW(k,j,i \u00f7k \u00f7j) + W(j-kk, k,i\u00f7k)\u00f7 W(i-j,k,i\u00f7j) end do (a) do u = 1, N do v = i, N do w = 1, N V(w,~+v,v)= \nV(u,v-u)+ y(=,,) end do u~ : ~.~ (C) nl ~a do u = l, N dov=1, N do w= 1, N V(w,~t+v,v-u)=W(u,v,~+v+w) \nu~..j 2\\_~ n~ end do4\" W(u+v,u,u+w)+ W(w-v,u,v+w) : ~ .2 (b) (d) Figure 4: (a) A program fragment. (b) \nOptimized version of (a). (c) MG for (a). (d) A solution corresponding to (b). r--lm, (a) \"~xk (e) (c) \n~t ira ~\"........... !ml 1/JD nl bl ~'~fO'iml i \" al, bl ~--~ n2 I ,, b2 1 im2 2 b2i e----e~ im~ b3 \nm3 a2. b2 n3 b3 ~:~--j~ m3 b4 m4 a3 ~2 ml b4 ~f-~:--~ m4 (b) (d) w m2 (0 Figure 6: (a) MG for the root \nprocedure (caller). (b) MG for the callee. (e) Combined MG at the root. (d) Solution for the combined \nMG. (e) Part of the solution that affects the root. (f) Solution for the callee. impact on the locality \ncharacteristics of the root procedure. At this point, we solve the problem (the MG at the root) using \nthe approach of Section 3.2 and determine all the lay- outs and loop transformations that can affect \nthe perfor- mance of the root. Note that, in solving the constraints at the root, the approach also takes \nthe profile data into account whenever necessary (This profile data includes, in addition to those mentioned \nearlier, the number of times each procedure is called from each call site). Afterwards, the top-down \npass starts. In this pass, each node propagates already determined layouts to its children. The children, \nin turn, using these layouts, determine the layouts of their lo- cal arrays as well as the loop transformations \nfor the nests whose orders do not affect the locality performance of their parents (otherwise, their \nloop orders (transformations) are determined by parent(s)). The optimization process halts when we process \nthe leaves in the call graph. The overall inter-procedural optimization framework is similar to the inter-procedural \ndata decomposition technique proposed in [21. As an example, let us focus on Figure 6. Assume that we \nhave the MG shown in Figure 6(a) for our main procedure (root) and the MG shown in Figure 6(b) belongs \nto a routine called by the root. Also assume that the arrays al and a2 are passed as the actual parameters \nand the corresponding for- mal parameters in the callee are bl and b2, respectively. Af- ter the bottom-up \npass, we obtain the combined MG shown in Figure 6(c). Note that, from callee to caller, we pass all the \narray parameters and all the involved nests (the portion delimited using a dashed box in Figure 6(b)). \nFigure 6(d) depicts an example solution. What this solution means to the root is shown in Figure 6(e). \nDuring the top-down pass, we propagate the layouts (for bl and b2) and loop trans- formations determined \nso far (for ml and m2) down to the callee. Using these, the callee finds the remaining layouts (for b3 \nand b4) and loop transformations (for m3 and m4) as shown in Figure 6(f) (dashed arrows). The sketch \nof the overall approach to inter-procedural locality optimization is shown in Figure 7. Algorithmic Complexity: \nThe complexity of our approach to locality can be calculated as follows. The intra-nest opti- mization \ntakes e(kn~2 r) time, where k is the total number of constraints in the nest, n is the number of loops \nin the nest, and r is the total number of references in the nest. The term kn 2 is due to the solution \nof the linear system, and 2 T represents the extreme case where each reference might (po- tentially) \nbe optimized for temporal locality. Note that k can be bounded by rm, where m is the maximum dimen- sionality \nof any array. The procedure-wide (inter-nest) al- gorithm has a complexity of e(veS), where v is the \nsum of the number of loops and number of arrays in the procedure, e is the number of references in the \nprocedure, and s is a constant (if the graph has a maximum spanning branch tree, then s = 1). Finally, \nthe inter-procedural part (bottom-up and top-down passes) takes 0(/9), where p is the number of procedures \nin the code. Consequently, the entire approach has an algorithmic complexity of O(pveSn2mr2r). As will \nbe discussed later, in practice, it only incurs a small compile- time overhead. 4. EXPERIMENTS In this \nsection, we present experimental results obtained on a single node of an SGI/Cray Origin 2000 multiprocessor. \nThe important characteristics of our platform are given in Table 1. We used seventeen complete programs \nwhose salient char- acteristics are given in Table 2. The x Sz column shows how (on an average) the program \ninputs are scaled with respect to original inputs. For example, a value a in this column indicates that \nthe input size is set to aN, where N is the  Table 3: Different optimized versions used in the experiments. \ni Version [ Description vl pure loop transformation technique without loop tiling v~2 pure loop transformation \ntechnique with loop tiling v3 pure data transformation technique with most costly nest and no conflict \nresolution v4 IIII g'i~ll l*I*l *ll~. I iI* II ~ tl I ~,1 ill I*ll lll~,l P1 Ill iI ~7~ i~l [* i i i \n~,li/,.l i~ ill I ii*1.1 |(l~pl I'l ll~-lll I I~ i l l|~ll I i i i lil i~(l] I I I v5 unified loop and \ndata transformation technique with most costly nest and conflict resolution v6 unified loop and data \ntransformation technique with maximum branching and conflict resolution v7 unified loop and data transformation \ntechnique with maximum branching, conflict resolution, and inter-procedural analysis v8 unified loop \nand data transformation technique with maximum branching, conflict resolution, inter-procedural analysis, \nand tiling v9 unified loop and data transformation technique without conflict resolution but with inter-procedural \nanalysis Step (1): Build a call graph for the program. Step (2): Build an MG for each procedure including \nthe main pro- cedure (root). Step (3): Perform bottom-up pass: (3.a) If the procedure is leaf, propagate \nMG to parents. (3.b) If the procedure is neither leaf nor root, combine the MGs taken from children with \nthe local MG and propa- gate the resulting MG to parents. (3.c) If the procedure is root, combine the \nMGs taken from children with the local MG. Step (4): Solve the resulting combined MG at the root. Step \n(5): Perform top-down pass: (5.a) If the procedure is root, propagate down all the layouts and loop transformations \ndetermined so far to children. (5.b) If the procedure is neither leaf nor root, inherit the memory layouts \nand loop transformations found so far from parents; determine the unknown layouts and loop transformations; \npropagate down all the layouts and loop transformations (including the new ones) to children. (5.c) If \nthe procedure is leaf, inherit the memory layouts and loop transformations found so far from parents; \ndeter-mine the unknown layouts and loop transformations. Figure 7: Inter-procedural locality optimization \nal- gorithm.  Table 1: Platform used in the experiments. Processor 195 MHz MIPS R10000 Out-of-Order \nExecution 4 instructions Functional Units LlCache 32 KB split, 2-way associative L2 Cache 4 MB unified, \n2-way associative L1 latency 2 to 3 cycles L2 latency 8 to I0 cycles Memory Latency 60 to 200 cycles \nOperating System IRIX 6.5 Compiler lll'~'lll Table 2: Programs used in the experiments. For math libraries, \nthe data sizes used exceed the ca-pacity of the L2 cache. Spe\u00a2 Benchmarks hydro2d 4,227 2.2 Navier-Stokes \nHydrodynamics tomcatv 190 2.0 Mesh Generation nasa7 1,105 3.2 Nasa Ames Fortran Kernels su2cor 2,271 \n2.4 Quantum Physics Perfect Club Benchmarks tsf 1,986 2.2 'IYansonic Flow Analysis oce 4,343 2.4 2D \nFluid Flow Solver ape 6,105 2.0 Mesoscale Hydro Model tis 485 2.0 Electron Integral Transform mrs 3,784 \n1.7 Sensor Computation wss 3,885 2.0 Molecular Dynamics of Water Math Libraries eispack 1,473 Eigenvector \nComputation odepack 1,695 Solvers for Initial Value Prob. hompack 1,673 Solver for Nonlinear systems \nminpack 1,012 Solvers for Nonlinear LSP Miscellaneous cholssky 34 3.2 Cholesky Factorization ADI 56 \n4.0 Alternate Direction Integral original input size. This modification was necessary as the original \ninput sizes (except for that of nwchem) were very small for the 4 MB L2 cache of R10000. As compared \nto most of the previous research on locality enhancement, we collected complete programs from different \nprogram domains such as benchmarks, math libraries, and real applications. Note that the library codes \nare, in general, difficult to op- timize using loop transformations alone so they are good candidates \nto evaluate the strength of data transformations. The code nwchem is part of a large computational chemistry \napplication used at the Pacific Northwest Lab (PNL) [37]. Measuring the degree of success of our approach \nin library and application codes allows us to see whether our inte-grated approach makes a difference \nover pure loop and pure data transformation techniques. For each code, we built a main program that calls \nit with appropriate parameters. We used nine optimized versions of each code as shown in Table 3. The \nfirst entry (vl) corresponds to classical loop transformations including linear transformations, loop \nfu- sion, and loop distribution. The next entry (v2) involves all these optimizations plus tiling. We \nallowed the native com- piler to choose a suitable tile size. The compiler achieves this using a sophisticated \nalgorithm as partially explained in [45]. The third entry uses pure layout transformations to improve \nlocality. This version is similar to that presented in [23] and (as all transformation schemes based \non pure layout optimizations) is not effective in optimizing temporal local- ity. Since it does not use \nany loop transformations, it does not solve conflicts within a single nest. In handling multi- ple nests~ \nthis version first orders nests according to a cost measure (obtained through profiling), and then tries \nto de- termine memory layouts starting from the most important nest (this is called most costly nest \n(mcn) strategy in the re- mainder of this paper). The next version (v4) uses both loop and data transformations \nin a unified framework. As in the previous version, this version does not solve conflicts within the \nnest; instead, it gives preference to optimizing temporal reuse aggressively. Previous work [21] shows \nthat this ap- proach generally results in better codes than those could be obtained using a unified approach \nthat does not give priority to optimizing temporal reuse [8, 24]. Also, for the multiple nest case, v4 \nuses the mcn strategy. From v5 to vT, we incre- mentally evaluate the three important characteristics \nof our approach presented in this paper. In v5, we activate con- flict resolution, and in v6 we activate \na maximum branching solution for multiple loop nest case. In v7, we use our inter- procedural layout \npropagation scheme. In order to enable this scheme, we apply a pre-pass step that de-linearizes [33] \none dimensional arrays whenever appropriate. If we cannot eliminate array reshaping through de-linearization, \nwe sim- ply do not optimize the array. In the experiments, however, we were always able to de-linearize \nthe one-dimensional ar- rays. As mentioned earlier, in v5, v6, and vT, whenever we need to favor one \nreference over other, we use profile data. Similar profile data is also utilized (when necessary) with \nother versions as well. In all the versions explained so far (except vT), no inter-procedural analysis \nis used and the arrays are explicitly layout-transformed (re-mapped) across procedure boundaries to preserve \ncorrect execution (this is done only for input and/or output arrays, though, as temporary arrays need \nnot be re-mapped). The last two versions are studied to see the impact of loop tiling in our approach \n(v8) and to measure the performance of an inter-procedural optimization scheme (similar to [22]) without \nintra-nest and inter-nest conflict resolution (vg). We also use selective cloning [11, 17] in our inter-procedural \noptimizer. Cloning might be necessary whenever a procedure is called in two or more sites, each with \ndifferent layout requirements for the best locality. An example is shown in Figure 8(a), where the procedure \nR is called from P and Q. Depending on the layout requirement of the arrays passed as parameters, we \nmay need to clone R rather than sacrificing locality in one of the procedures. Figure 8(b) shows the \nsame code when cloning is applied. Note that, in selective cloning, if both P and Q demand the same layouts, \nwe do not clone the procedure R. All of the versions explained above are obtained from the original codes \nusing a source-to-source translator [1] with the help of Omega library [25]. The resulting codes are \nthen compiled using the native compiler on the Origin with 03 op- tion, allowing all major scalar optimizations. \nIn cases where we do not want the native compiler to perform a specific optimization (as in, for example, \nv3, where we do not want to apply any loop transformations), we explicitly disabled the said optimization \nusing the native compiler's flags. Let us first present some compile-time (static) information obtained \nduring code generation using our technique (vT). The second column in Table 4 shows the average number \nof alternative Q matrices per nest for each program in our procedure main call P procedure main call \nQ call P end procedure call Q ... end procedure procedure P call R procedure P end procedure call R end \nprocedure procedure Q ... call Rcio~e procedure Q end procedure call R ... end procedure procedure R \nprocedure R end procedure end procedure procedure Rctone (a) end procedure (b) Figure 8: (a) A program \nfragment. (b) The same fragment with cloning. suite. The average over all programs is approximately 2, \nindicating a significant reduction in search space. The next column gives the number of intra-nest conflicts \nper nest. Note that, for six codes, we observe no intra-nest conflict; consequently, these codes will \nnot benefit from our conflict resolution scheme. The next column gives the per nest con- flicts that \nresulted in the use of profile data. Although not presented here, we observed that not using profile \ndata (i.e., randomly eliminating constraints whenever necessary) de- graded the performance of these \ncodes by approximately 8.5% on average. Note that we did not utilize profile data very often. In other \nwords, our intra-nest conflict resolu- tion scheme works quite well in practice. The fifth column shows \nthat, in most of the conflicts, the compiler (during a maximum branching solution) was able to choose \na suit- able loop transformation to satisfy the different layout re- quirements of different arrays. \nThe next column indicates whether the solution that we found using maximum branch- ing was the same found \nwhen using the most costly nest (mcn) approach. This result indicates that six of our codes did not benefit \nfrom maximum branching. Finally, the last column gives the average number of arrays that would be explicitly \n(layout-)transformed (re-mapped) per procedure call (had we not used the inter-procedural analysis). \nThe higher this number, the more benefit we can expect from inter-procedural layout propagation. Due \nto space limitations, we present only percentage im-provements on overall execution times with respect \nto the original version of each code (i.e., the version without any locality optimization). From Table \n5, we can conclude the following. Neither pure loop (vl) nor pure data optimiza- tions (v3) bring significant \nimprovements. In fact, v3 in- creases the execution time in five cases, as the cost explicit layout transformations \n(copy loops) exceeds the gains ob- tained from optimized locality. Tiling, on the other hand, performed \nquite well, bringing a 7.3% improvement on the average. (In general, data dependences in these codes \npre- vent tiling from performing even better). Combining loop and data transformations in a unified framework \nresulted in an 8.6% improvement. This is, although not bad, still less than one might expect from a framework \nthat uses both loop and data transformations; the main reasons for this Table 4: Information collected \nduring compilation. Number of Program Q matrices intra-nest conf profile use inter-nest conf == arrays \ntrans. per nest per nest per nest per proc men ? per call Spec Benchmarks hydro2d tomcatv 2.5 2.0 0.70.0 \nI 0.20.0 1.0 0.0 [ No No [ 1.9 0.0 nasa7 2.7 0.3 0.1 1.3 No 2.3 su2cor 1.7 0.9 0.1 1.2 No 2.7 Perfect \nClub Benchmarks tsf 2.3 0.3 0.I 1.6 No 1.8 ocs 1.5 0.5 0.I 1.6 Yes 1.9 ape 1.5 0.2 0.I 1.0 No 1.2 tis \n2.7 0.0 0.0 0.0 No 0.0 mrs 1.5 0.0 0.0 1.2 No 1.8 wss 2.6 0.0 0.0 1.0 Yes 2.5 Math Libraries eispack \n2.0 0.8 0.0 1.4 Yes 2.4 odepack 2.2 0.8 0.1 0.9 Yes 2.0 hempack 1.6 0.0 0.0 1.6 No 1.6 minpack 2.3 0.6 \n0.2 1.4 No 2.3 Miscellaneous nwchem 1.5 0.9 0.0 0.7 Yes 2.9 cholesky 2.0 0.9 0.0 1.2 No 3.0 ADI 2.5 0.0 \n0.0 0.0 Yes 1.0 average: 2.1 0.4 0.06 1.0 1.8 are explicit copy loops and lack of conflict resolution. \nWhen we applied intra-nest conflict resolution (vS), we obtain an 11.2% improvement. Adding inter-nest \nconflict resolution (based on maximum branching) takes the improvement to 12.9%, which is quite good, \nconsidering the fact that we handle complete programs. The complete version of our al- gorithm (v7) results \nin a 17.5% improvement; that is, prop- agating layout across procedures caused a 4.6% additional improvement. \nAt this point, we need to make the following comments. In two codes (tomcatv and tis), our approach was \nnot able to do anything as the locality of the original codes was very good to begin with. In six codes, \nwe did not observe any benefit from maximum branching, as it generated the same code obtained from the \nmost costly nest (men) approach. Propagating layouts across procedures was found to be use- ful for all \ncodes except two. The results obtained from the last two versions, on the other hand, reveal two things. \nFirst, combining tiling with our approach brings over a 3% additional improvement; there- fore, we suggest \napplying tiling following our framework. In fact, in two codes, only tiling was able to improve the per- \nformance. In two other codes (ocs and aps), however, over- heads caused by tiling degraded the performance \nof the v7 version, motivating the research in better integrating lin- ear loop/data transformations with \ntiling. Another obser- vation is that using an inter-procedural optimization tech- nique without first \nresolving conflicts (v9) may not be very good idea, as in that case, we may end up with non-optimal layouts \n(although the layouts are propagated between pro- cedures without copy loops). Overall, these results \nshow that in order to exploit the inherent data reuse in whole programs, integrated loop and data transformations, \nresolving conflicts at the nest, proce- dure, and entire program granularity, and propagating mem- ory \nlayouts across procedures (inter-procedural locality op- timization) are necessary.  4.1 Overheads Our \nexperimental results clearly establish the usefulness of our approach. In Table 6, we quantify the overheads \ncaused by our technique. The second column gives the compilation time for the vl version in milliseconds \n(For the math library codes, the given values are averages over all routines in the respective library). \nThe third column gives the compilation time difference (in percentage) between v7 and vl, and the next \ncolumn gives the same difference between v7 and v4. As compared to vl, the increase in compilation time \nis always below 20% except for one case. Finally, the last column shows the percentage increase in memory \nsize when our technique is applied. This increase is due to non-traditional memory layouts (e.g., diagonal \nlay- outs) and is bounded by 4% for all codes, averaging only a 1.3%. These overheads are not great (due \nto the space min- imization technique [30] that we employed) and we believe that they are easily outweighed \nby the benefits coming from improved whole-program locality.  5. RELATED WORK A majority of the related \ncompiler work on cache local- ity optimization is based on loop transformations. Wolf and Lain [44] define \nreuse vectors and reuse spaces and show how these concepts can be exploited by an iteration space optimization \ntechnique. Li [31] also uses reuse vectors to detect the dimensions of loop nest that carry reuse. McKin-ley \netal. [34] use a simple locality criterion to re-order the computation to enhance data locality. The \nloop based lo- cality enhancing techniques also include tiling [44, 29, 10, 28, 27, 16]. All these techniques \nfocus on the iteration space and attempt to improve data locality indirectly by modifying the loop access \npatterns; all are constrained by data depen- dences. We, instead, demonstrated in this paper that data \ntransformations can also play a significant role in enhancing cache locality, and an integrated approach \nmay obtain even better results. More recently, new techniques based on memory layout transformations \nhave been proposed. O'Boyle and Knijnen- Table 5: Percentage improvements in execution times. These \nimprovements are with respect to the original version of each code (i.e., the version without any locality \noptimization). Spec Benchmarks hydro2d 0.0 6.3 -2.5 4.0 4.0 5.1 8.3 8.8 6.8 tomcatv 0.0 6.1 0.0 0.0 0.0 \n0.0 0.0 6.1 0.0 nasa7 6.3 10.9 4,1 18.4 19.3 24.9 30.1 34.1 20.8 su2cor 1.3 10.3 -3.5 -0.2 5.5 6.2 10.1 \n16.6 8.9 Perfect Club Benchmarks tsf 13.6 18.4 7.9 27.6 30.3 32.0 36.5 38.2 22.8 ocs -3.7 1.1 -2.8 -2.0 \n3.1 3.1 8.9 8.7 7.0 ape -2.4 -1.5 6.5 10.2 12.7 21.0 24.3 22.0 12.7 tie 0.0 4.6 0.0 0.0 0.0 0.0 0.0 4.6 \n0.0 mrs 11.4 11.8 12.7 18.0 18.0 23.2 28.8 32.2 24.9 web 5.9 5.7 6.4 7.1 7.1 7.1 13.7 13.7 13.7 Math \nLibr~ies eispack 2.4 6.3 7.0 11.0 16.7 16.7 21.4 22,9 17.5 odepack 0.0 4.0 9.6 9.9 13.2 13.2 18.4 21.7 \n13.9 hompack 0.0 2.8 -1.0 6,4 6.4 8.5 10.0 13.0 8.5 minpack 0.0 2.0 8.2 9.5 14.3 17.1 21.0 21.0 16.6 \nn--cholesky MiscellaneousI 801123p 9 112 1180 12.8 13.4 -I. 0.3 7.3 9.3 19.0 26.8 7 8 ADI 9.3 10.0 7. \n13.5 13.5 13.5 20.7 27.2 15 0 burg [38] explain how to generate code given a data transfor- mation matrix. \nLeung and Zahorjan [30] focus more on min- imizing memory consumption after a layout transformation. \nKandemir et al. [23] have proposed a layout optimization technique based on explicit layout representation. \nThese techniques, however, are more oriented toward exploiting  Table 6: Compilation time and memory \noverheads. spatial reuse rather than temporal reuse. More importantly,  The second column gives the \ncompilation time for since the impact of a layout change may penetrate into mul-  the vl version in \nmilliseconds. The third column tiple nests, the scope considered becomes much larger; un-  gives the \ncompilation time difference (in percentage) fortunately, the said approaches do not provide satisfactory \n between v7 and vl, and the next column gives the solutions to this global locality optimization problem. \n same difference between v7 and v4. The last column Cierniak and Li [8] were among the first to offer \na scheme  shows the percentage increase in memory size when that unifies loop and data transformations. \nThere are a  our technique is applied. number of differences between their work and ours. First, Pro~ \nI ~1 I vz-vl I vz-v4 II H-Ovhd we use general loop and data transformations whereas they Spe\u00a2 Benchmarks \nrestrict the search space for possible loop and data trans- hydro2d 2,187 17.4 8.4 0.0 formations. Second, \ntheir optimization methodology is cen- tomcatv 2,772 12.9 8.0 0.0 nasa7 3,331 9.8 6.1 0.0 tered around \na concept called the stride vector, whose entries eu2cor 3,015 16.5 7.0 2.3 should be guessed by the \ncompiler before the optimization Perfect Club Benchmarks process. Third, they focus largely on a single \nloop nest. tsf 3,405 13.0 6.4 0.0 ocs 2,982 8.6 5.4 0.0  Their extension to multiple loop nests is not \nvery clear, aps 2,780 18.2 8.0 0.0 whereas we can handle the multiple loop nest case as well tis 3,411 \n13.0 8.2 0.0 as the inter-procedural layout optimization problem. Kan- mrs 3,093 I0.0 4.9 0.0 demir et \nal. [24] have also considered loop and data layout wss 3,200 15.4 7.0 1.7 Hath Libraries optimizations. \nLike Cierniak and Li [8], their solution re- eispack 1,925 11.9 6.0 3.6 stricts the search space of data \ntransformations to dimen- odepack 1,226 18.9 10.4 3.2 sion permutations (re-indexing). Anderson et al. \n[3] pro- hompack 1,354 15.4 8.8 4.0 minpack 1,384 20.3 11.5 2.8 pose a transformation technique that \nmakes data elements Miscellaneous accessed by the same processor contiguous in the shared ad- dress space. \nThey use permutations (of array dimensions) ohol.k, 1,902 13.5 8.2 0.0 and strip-mining for possible \ndata transformations. O'Boyle ADI 1,896 . 5.4 0.0 and Knijnenburg [39] discuss a nice technique that \ncan al- l aver'g e: I \" I 14'0 I 6.8 II 1.3 I low us to propagate layouts across procedures in the most \ngeneral case but do not show how to integrate it with a unified layout transformation framework. As compared \nto the techniques discussed in [20] and [21], the integrated ap- proach proposed in this paper handles \nthe multiple nest case much better and proposes an inter-procedural optimization scheme. Both [21] and \n[20] are based on the most costly nest strategy, do not handle inter-procedural optimization, and are \nnot as powerful in resolving intra-nest conflicts. Our experimental results show that intra-and inter-nest \ncon-flict resolutions and inter-procedural layout optimizations are important to obtain decent improvements. \n Very few papers have addressed the problem of inter- procedural optimization of locality. Cierniak and \nLi [9] pro- posed an efficient array re-mapping scheme to improve inter- procedural data locality but \nthey do not show how to in- tegrate it with intra-procedural loop and data transforma- tions. In [22], \nfour small codes were hand-optimized to propagate layouts across procedures. The algorithm pre- sented \nthere (called v9 here) does not resolve conflicts and uses a different technique to handle multiple nest \ncase. Our work also relates to recent work on inter-procedural data distribution. Anderson [2] was among \nthe first to perform inter-procedural analysis to derive data distributions. Her work does not address \ncache locality explicitly; rather, it is intended to reduce the inter-processor communication in- curred \ndue to data distribution. Although our inter- pro- cedural optimization scheme is similar to [2], our \nsolution mechanism (using maximum branching) and the type of con- straints we handle are different. Finally, \nthere is a huge body of work on automatic data distribution on distributed memory machines (e.g., [7, \n13, 14, 15, 26, 32, 40, 43, 41]). We believe that many of the ideas presented in these papers can be \nadapted to the inter- nest locality optimization. 6. SUMMARY AND CONCLUSIONS We presented an integrated \napproach for optimizing lo- cality in regular scientific codes. Given a loop nest which references a \nnumber of arrays, our approach first tries to find a suitable loop transformation such that the last \ncolumns of as many access matrices as possible to the same array will be the same. Then, using data layout \ntransformations, we selected a new memory layout for each array that meshes well with the new access \npattern imposed by the loop trans- formation. We also discussed how this technique can be adapted to \nwork with multiple loop nests (using a variant of maximum branching) and with multiple procedures (us- \ning a two-pass layout propagation technique). In a suite of programs, we achieved 17.5% improvement compared \nto the original codes. Our current work includes enhancing our approach to eliminate conflict misses \nas well. Rivera and Tseng ob-served that eliminating conflict misses would enable better exploitation \nof spatial locality and proposed elegant data transformation techniques to eliminate conflict misses \n[42]. We hope to adapt these algorithms to further improve data locality. We also study the ways to unify \nthe approach pro- posed here with a recently proposed technique [12] that in- terleaves multiple arrays \nto improve spatial locality. An-other interesting topic is combining our approach with the parallelization \ntechniques used by NUMA machines. Since effective parallelization also requires carefully chosen loop \nand data transformations, we believe that reconciling the different layout and loop order requirements \nof parallelism and locality optimizations is a challenging task.   Acknowledgments The author would \nlike to thank Prith Banerjee, Alok Choud- hary, and J. Ramanujam for their insightful comments on 'resolving \nintra-nest layout conflicts' and Flo Thomas for proofreading the manuscript. The author would also like \nto thank the reviewers for their helpful and constructive com- ments.  7. REFERENCES [1] S. P. Amarasinghe, \nJ. M. Anderson, M. S. Lain, and C. W. Tseng The SUIF compiler for scalable parallel machines. In Proc. \nthe Seventh SIAM Conference on Parallel Processing for Scientific Computing, February, 1995. [2] J. Anderson. \nAutomatic Computation and Data Decomposition for Multiprocessors. Ph.D. dissertation, Stanford University, \nCA, March 1997. Also available as Technical Report CSL-TR-97-179, Computer Systems Laboratory, Stanford \nUniversity. [3] J. Anderson, S. Amarasinghe, and M. Lain. Data and computation transformations for multiprocessors. \nIn Proc. 5th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, July 1995. [4] \nA. Bik, P. Knijnenburg, and H. Wijshoff. Reshaping access patterns for generating sparse codes. In Proc. \nthe 7th International Workshop on Languages and Compilers for Parallel Computing, pages 406-422, 1994. \n[5] A. Bik and H. Wijshoff. On a completion method for unimodular matrices. Technical Report 94-14, Dept. \nof Computer Science, Leiden University, 1994. [6] 1~. Chandra, D. Chen, R. Cox, D. Maydan, N. Nedeljkovic, \nand J. Anderson. Data-distribution support on distributed-shared memory multi-processors. In Proc. Programming \nLanguage Design and Implementation, Las Vegas, NV, 1997. [7] S. Chatterjee, J. Gilbert, R. Schreiber, \nand S. Teng. Optimal evaluation of array expressions on massively parallel machines. A CM Transactions \non Programming Languages and Systems, 17(1):123-156, January 1995. [8] M. Cierniak and W. Li. Unifying \ndata and control transformations for distributed shared memory machines. In Proe. SIGPLAN '95 Conference \non Programming Language Design and Implementation, June 1995. [9] M. Cierniak and W. Li. Inter-procedural \narray re-mapping. In Proc. the International Conference on Parallel Architectures and Compilation Techniques, \nNov 1997. [10] S. Coleman and K. McKinley. Tile size selection using cache organization and data layout. \nIn Proc. SIGPLAN '95 Conference on Programming Language Design and Implementation, June 1995. [11] K. \nCooper, M. W. Hall, and K. Kennedy. A methodology for procedure cloning. Computer Languages, 19(2), April \n1993. [12] C. Ding and K. Kennedy. Inter-array data regrouping. In Proc. Languages and Compilers for \nParallel Computing, San Diego, CA, August 4-6, 1999. [13] J. Garcia, E. Ayguade, and J. Labarta. A novel \napproach towards automatic data distribution. In Proc. Supercomputing'95, San Diego, December 1995. [14] \nJ. Garcia, E. Ayguade, and J. Labarta. Dynamic data distribution with control flow analysis. In Proc. \nSupercomputing'96, Pittsburgh, November 1996. [15] M. Gupta and P. Banerjee. Demonstration of automatic \ndata partitioning techniques for parallelizing compilers on multicomputers. IEEE Transactions on Parallel \nand Distributed Systems, 3(2):179-193, March 1992. [16] S. Ghosh, M. Martonosi, and S. Malik. Precise \nmiss analysis for program transformations with caches of arbitrary associativity. In Proc. 8th International \nSymposium on Architectural Support for Programming Languages and Operating Systems, October, 1998. [17] \nM. W. Hall, S. P. Amarasinghe, B. R. Murphy, S. Liao, and M. S. Lain. Detecting coarse-grain parallelism \nusing an inter-procedural parallelizing compiler. In Proceedings of Supercomputin9 '95, December 1995. \n[18] M. Kandemir, P. Banerjee, A. Choudhary, J. Ramanujam, and E. Ayguade. An integer linear programming \napproach for optimizing cache locality. In Proc. 1999 ACM International Conference on Supercomputing, \nRhodes, Greece, June 1999, pp. 500-509. [19] M. Kandemir, A. Choudhary, J. Ramanujam, and M. Kandaswamy. \nLocality optimization algorithms for compilation of out-of-core codes. Journal of Information Science \nand Engineering, 14(1):107-138,March 1998. [20] M. Kandemir, A. Choudhary, J. Ramanujam, and P. Banerjee. \nA matrix-based approach to the global locality optimization problem. In Proc. International Conference \non Parallel Architectures and Compilation Techniques, October 1998, Paris, France. [21] M. Kandemir, \nA. Choudhary, J. Ramanujam, and P. Banerjee. Improving locality using loop and data transformations in \nan integrated framework. In Proc. International Symposium on Microarchitecture, Dallas, TX, December, \n1998. [22] M. Kandemir, A. Choudhary, J. Ramanujam, and P. Banerjee. A framework for inter-procedural \nlocality optimization. In Proe. International Conference on Parallel Processing, University of Aizu, \nJapan, Sept 1999. [23] M. Kandemir, A. Choudhary, N. Shenoy, P. Banerjee, and J. Ramanujam. A hyperplane \nbased approach for optimizing spatial locality in loop nests. In Proc. 1998 A CM International Conference \non Supercomputing, July 1998. [24] M. Kandemir, J. Ramanujam, and A. Choudhary. A compiler algorithm \nfor optimizing locality in loop nests. In Proc. 11th ACM International Conference on Supercomputing, \npages 269-276, Vienna, Austria, July 1997. [25] W. Kelly, V. Maslov, W. Pugh, E. Rosser, T. Shpeisman, \nand David Wonnacott. The Omega Library interface guide. Technical Report CS-TR-3445, CS Dept., University \nof Maryland, College Park, March 1995. [26] K. Kennedy and U. Kremer. Automatic data layout for High \nPerformance Fortran. In Proe. Supercomputing'95, San Diego, CA, December 1995. [27] K. Kennedy and K. \nMcKinley. Optimizing for parallelism and data locality. In Proc. the 1992 ACM International Conference \non Supercomputing, ACM, New York. [28] I. Kodukula, N. Ahmed, and K. Pingali. Data-centric multi-level \nblocking. In Proc. SIGPLAN Conf. Programming Language Design and Implementation, June 1997. [29] M. Lam, \nE. Rothberg, and M. Wolf. The cache performance of blocked algorithms. In Proc. 4th Int. Conf. Architectural \nSupport for Programming Languages and Operating Systems, April 1991. [30] S.-T. Leung and J. Zahorjan. \nOptimizing data locality by array restructuring. Technical Report TR 95-09-01, Dept. of Computer Science \nand Engineering, University of Washington, September 1995. [31] W. Li. Compiling for NUMA Parallel Machines. \nPh.D. Thesis, Computer Science Department, Cornell University, Ithaca, NY, 1993. [32] J. Li and M. Chen. \nCompiling communication efficient programs for massively parallel machines. Journal of Parallel and Distributed \nComputing, 2(3):361-376,1991. [33] V. Maslov. De-linearization: An efficient way to break multi-loop \ndependence equations. In Proe. the SIGPLAN'92 Conference on Programming Language Design and Implementation, \nSan Francisco, CA, June 1992. [34] K. McKinley, S. Carr, and C.W. Tseng. Improving data locality with \nloop transformations. ACM Transactions on Programming Languages and Systems, 1996. [35] E. Minieka. Optimization \nAlgorithms for Networks and Graphs, Marcel Dekker, Inc., 1978. [36] S. S. Muchnick. Advanced Compiler \nDesign Implementation. Morgan Kaufmann Publishers, San Francisco, California, 1997. [37] NWChem: a computational \nchemistry package for parallel computers, version 1.1, 1995. High Performance Computational Chemistry \nGroup, Pacific Northwest Laboratory. [38] M. O'Boyle and P. Knijnenburg. Non-singular data transformations: \nDefinition, validity, applications. In Proc. 6th Workshop on Compilers for Parallel Computers, pages \n287-297, Aachen, Germany, 1996. [39] M. O'Boyle and P. Knijnenburg. Integrating loop and data transformations \nfor global optimisation. In Proc. International Conference on Parallel Architectures and Compilation \nTechniques, October 1998, Paris, France. [40] D. Palermo and P. Banerjee. Automatic selection of dynamic \ndata partitioning schemes for distributed-memory multicomputers. In Proc. 8th Workshop on Languages and \nCompilers for Parallel Computing, Columbus, OH, pages 392-406, 1995. [41] J. Ramanujam and P. Sadayappan. \nCompile-time techniques for data distribution in distributed memory machines. In IEEE Transactions on \nParallel and Distributed Systems, 2(4):472-482, October 1991. [42] G. Rivera and C.-W. Tseng. Data transformations \nfor eliminating conflict misses. In Proe. the 1998 A CM SIGPLAN Conference on Programming Language Design \nand Implementation, Montreal, Canada, June 1998. [43] S. Tandri and T. Abdelrahman. Automatic partitioning \nof data and computations on scalable shared memory multiprocessors. In Proc. 1997 International Conference \non Parallel Processing, Bloomingdale, IL, pages 64-73, August 1997. [44] M. Wolf and M. Lain. A data \nlocality optimizing algorithm. In Proc. ACM SIGPLAN 91 Conf. Programming Language Design and Implementation, \npages 30-44, June 1991. [45] M. Wolf, D. Maydan, and D. Chen. Combining loop transformations considering \ncaches and scheduling. In Proe. International Symposium on Mieroarehitecture, pages 274-286, Paris, France, \nDecember 1996. [46] M. Wolfe. High Performance Compilers for Parallel Computing, Addison-Wesley Publishing \nCompany, 1996.   \n\t\t\t", "proc_id": "360204", "abstract": "Exploiting spatial and temporal locality is essential for obtaining high performance on modern computers. Writing programs that exhibit high locality of reference is difficult and error-prone. Compiler researchers have developed loop transformations that allow the conversion of programs to exploit locality. Recently, transformations that change the memory layouts of multi-dimensional arrays---called data transformations---have been proposed. Unfortunately, both data and loop transformations have some important draw-backs. In this work, we present an integrated framework that uses loop and data transformations in concert to exploit the benefits of both approaches while minimizing the impact of their disadvantages. Our approach works inter-procedurally on acyclic call graphs, uses profile data to eliminate layout conflicts, and is unique in its capability of resolving conflicting layout requirements of different references to the same array in the same nest and in different nests for regular array-based applications.The optimization technique presented in this paper has been implemented in a source-to-source translator. We evaluate its performance using standard benchmark suites and several math libraries (complete programs) with large input sizes. Experimental results show that our approach reduces the overall execution times of original codes by 17.5% on the average. This reduction comes from three important characteristics of the technique, namely, resolving layout conflicts between references to the same array in a loop nest, determining a suitable order to propagate layout modifications across loop nests, and propagating layouts between different procedures in the program --- all in a unified framework.", "authors": [{"name": "Mahmut Taylan Kandemir", "author_profile_id": "81100186744", "affiliation": "Department of Computer Science and Engineering, Pennsylvania State University, University Park, PA", "person_id": "P186191", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/360204.360219", "year": "2001", "article_id": "360219", "conference": "POPL", "title": "A compiler technique for improving whole-program locality", "url": "http://dl.acm.org/citation.cfm?id=360219"}