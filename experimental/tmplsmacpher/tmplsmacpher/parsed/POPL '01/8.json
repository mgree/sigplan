{"article_publication_date": "01-01-2001", "fulltext": "\n An Abstract Monte-Carlo Method for the Analysis of Probabilistic Programs* David Monniaux \u00b4 Ecole \nNormale Sup\u00b4erieure Laboratoire d Informatique 45, rue d Ulm 75230 Paris cedex 5 France  David.Monniaux@ens.fr \nABSTRACT We introduce a new method, combination of random test\u00ading and abstract interpretation, for the \nanalysis of programs featuring both probabilistic and non-probabilistic nondeter\u00adminism. After introducing \nordinary testing, we show how to combine testing and abstract interpretation and give for\u00admulas linking \nthe precision of the results to the number of iterations. We then discuss complexity and optimization \nis\u00adsues and end with some experimental results. 1 INTRODUCTION We introduce a generic method that lifts \nan ordinary ab\u00adstract interpretation scheme to an analyzer yielding upper bounds on the probability of \ncertain outcomes, taking into account both randomness and ordinary nondeterminism. 1.1 Motivations It \nis sometimes desirable to estimate the probability of cer\u00adtain outcomes of a randomized computation process, \nsuch as a randomized algorithm or an embedded systems whose environment (users, mechanical and electrical \nparts ... ) is modeled by known random distributions. In this latter case, it is particularly important \nto obtain upper bounds on the probability of failure. Let us take an example. A copy machine has a comput\u00aderized \ncontrol system that interacts with the user through * This work was partially funded by Commissariat \na`l \u00b4 Energie Atomique under contract 27234/VSF. Permission to make digital or hard copies of all or \npart of this work for personal or classroom use is granted without fee provided that copies are not made \nor distributed for profit or commercial advantage and that copies bear this notice and the full citation \non the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, \nrequires prior specific permission and/or a fee. POPL '01 1/01 Londo, UK Copyright 2001 ACM 1-58113-336-7/01/0001 \n... $5.00  some control panel, drives (servo)motors and receives infor\u00admation from sensors. In some \ncircumstances, the sensors can give bad information; for instance, some loose scrap of paper might prevent \nsome optical sensor from working cor\u00adrectly. It is nevertheless desired that the probability that the \nmachine will stop in an undesired state (without having returned the original, for instance) is very \nlow given some re\u00adalistic rates of failure from the sensors. To make the system more reliable, some sensors \nare redundant and the control\u00adling algorithm tries to act coherently. Since adding sensors to the design \ncosts space and hardware, it is interesting to evaluate the probabilities of failure even before building \na prototype. A similar case can be made of industrial systems such as nuclear power plants were sensors \nhave a limited life time and cannot be expected to be reliable. Sound analysis methods are especially \nneeded for that kind of systems as safety guidelines are often formulated in terms of maximal probabilities \nof failures [10]. 1.2 Nondeterminism and Probabilities Treating the above problem in an entirely probabilistic \nfash\u00adion is not entirely satisfactory. While it is possible to model the user by properties such as the \nprobability that the user will hit the C key during the transfer of double-sided doc\u00aduments is less than \n1% , this can prevent detecting some failures. For instance, if pressing some unlikely key combi\u00adnation \nduring a certain phase of copying has a good chance of preventing correct accounting of the number of \ncopies made, certain users might use it to get free copies. This is cer\u00adtainly a bug in the system. To \naccount for the behavior of inputs that cannot be reliably modeled by random distribu\u00adtions (for instance, \nmalicious attacks) we must incorporate nondeterminism. 1.3 Comparison to other works An important literature \nhas been published on software test\u00ading [13, 19, ... ]; the purpose of testing techniques is to discover \nbugs and even to assert some sort of reliability cri\u00adterion by testing the program on a certain number \nof cases. Such cases are either chosen randomly (random testing) or according to some ad hoc criteria, \nsuch as program state\u00adment or branch coverage (partition testing). Partition-based methods can be enhanced \nby sampling randomly inside the partition elements. Often, since the actual distribution in production \nuse is unknown, a uniform distribution is as\u00adsumed. In our case, all the results our method gives are \nrelative to some .xed, known, distributions driving some inputs. On the other hand, we will not have \nto assume some known distribution on the other inputs: they will be treated as nondeterministic. We thus \navoid all problems pertaining to arbitrary choices of partitions or random distributions; our method, \ncontrary to most testing methods, is fully mathe\u00admatically sound. There exists a domain called probabilistic \nsoftware engi\u00adneering [14] also aiming at estimating the safety of software. It is based on statistical \nstudies on syntactic aspects of source code, or software engineering practices (programming lan\u00adguage \nused, organization of the development teams ... ), trying to estimate number of bugs in software according \nto recorded engineering experience. Our method does not use such considerations and bases itself on the \nactual software only. Our analysis is based on a semantics equivalent to those proposed by Kozen [8, \n9, 2nd semantics] and Monniaux [11]. We proposed a de.nition of abstract interpretation on prob\u00adabilistic \nprograms, using sets of measures, and gave a generic construction for abstract domains for the construction \nof an\u00adalyzers. Nevertheless, this construction is rather algebraic and, contrary to the one explained \nhere, does not make use of the well-studied properties of probabilities. Ramalingam [15] proposed an \nabstraction using vectors of upper bounds of the probabilities of certain properties, the resulting linear \nsystem being solved numerically. While his approach is sound and e.ective, it is restricted to programs \nwhere probabilities are only introduced as constant transi\u00adtion probabilities on the control .ow graph. \nFurthermore, the class of properties is limited to data-.ow analyses. Several schemes of guarded logic \ncommands [5] or re.ne\u00adment [12] have been introduced. While these systems are based on semantics broadly \nequivalent to ours, they are not analysis systems: they require considerable human input and are rather \nformal systems in which to construct derivations of properties of programs. 1.4 Contribution We introduce \nfor the .rst time a method combining statisti\u00adcal and static analyses. This method is proven to be math\u00adematically \nsound. While some other methods have been re\u00adcently proposed to statically derive properties of probabilis\u00adtic \nprograms in a general purpose programming language [11], ours is to our knowledge the .rst that makes \nuse of statistical convergences.  1.5 Structure of the paper We shall begin by an explanation of ordinary \ntesting and its mathematical justi.cation, then explain our abstract Monte-Carlo method (mathematical \nbases are given in ap\u00adpendix). We shall then give the precise concrete seman\u00adtics that an abstract interpreter \nmust use to implement our method, .rst for a block-structured language then for arbi\u00adtrary control graphs. \nWe shall .nish with some early results from our implementation. 2 ABSTRACT MONTE-CARLO: THE IDEA In \nthis section, we shall explain, in a mathematical fashion, how our method works. 2.1 The Ordinary Monte-Carlo \nTesting Method The reader unfamiliar with probability theory is invited to consult appendix A. Let us \nconsider a deterministic program c whose input x lies in X and whose output lies in Z. We shall note \n[ c] : X . Z the semantics of c (so that [ c]](x) is the result of the computation of c on the input \nx). We shall take X and Z two measurable spaces and constrain [ c] to be measurable. These measurability \nconditions are technical and do not ac\u00adtually restrict the scope of programs to consider [11]. For the \nsake of simplicity, we shall suppose in this sub-section that c always terminates. Let us consider W \n. Z a measurable set of .nal states whose probability we wish to measure when x is a random variable \nwhose probability measure is \u00b5. The probability of W is therefore \u00b5([[c] -1(W)). Noting W 1 if[ c]](x) \n. W tW (x)= 0 otherwise, this probability is the expectation EtW . Let us apply the Monte-Carlo method \nfor averages to this random variable tW (see appendix B). EtW is then approx\u00adimated by n random trials: \nc . 0 for i =1 to n do x . random(\u00b5) run program c on input x. if program run ended in a state in W then \nc . c +1 end if end for p . c/n A con.dence interval can be supplied, for instance using the Cherno.-Hoe.ding \nbound (Inequ. 11): there is at least a 1 - e probability that the true expectation EtW is less than I- \nlog e p= p + b2n (Fig. 1 we shall see the implications in terms of complexity of these safety margins \nin more detail in section 4). This method su.ers from two drawbacks that make it un\u00adsuitable in certain \ncases: It supposes that all inputs to the program are either constant or driven according to a known \nprobability distribution. In general, this is not the case: some in\u00adputs might well be only speci.ed \nby intervals of possible values, without any probability measure. In such cases, it is common [13] to \nassume some kind of distribution on the inputs, such as an uniform one for numeric in\u00adputs. This might \nwork in some cases, but grossly fail in others, since this is mathematically unsound.  It supposes that \nthe program terminates every time within an acceptable delay.  We propose a method that overcomes both \nof these prob\u00adlems.  2.2 Abstract Monte-Carlo We shall now consider the case where the inputs of the \npro\u00adgram are divided in two: those, in X, that follow a random distribution \u00b5 and those that simply lie \nin some set Y . Now [ c] : X \u00d7 Y . Z. The probability we are now trying to quantify is \u00b5{x . X |.y . \nY [ c] (x, y). W}. Some techni\u00adcal conditions must be met so that this probability is well\u00adde.ned; namely, \nthe spaces X and Y must be standard Borel s spaces [7, Def. 12.5].1 Since countable sets, , products \nof sequences of standard Borel spaces are standard Borel [7, \u00a712.B], this restriction does not concern \nmost semantics. Noting W 1 if .y . Y [ c] (x, y). W tW (x)= 0 otherwise, this probability is the expectation \nEtW . While it would be tempting, we cannot use a straight\u00adforward Monte-Carlo method since, in general, \ntW is not computable.2 Abstract interpretation (see appendix C) is a general scheme for approximated \nanalyses of safety properties of pro\u00adgrams. We use an abstract interpreter to compute a function TW : \nX .{0, 1} testing the following safety property: TW (x) = 0 means that no value of y . Y results in \n[ c]](x, y) . W;  TW (x) = 1 means that some value of y . Y may result in [ c]](x, y) . W.  This means \nthat for any x, tw(x) = TW (x). Let us use the following algorithm: c . 0 for i =1 to n do x . random(\u00b5) \nc . c + TW (x) 1Let us suppose X and Y are standard Borel spaces [7, \u00a712.B]. X \u00d7 Y is thus a Polish space \n[7, \u00a73.A] so that the .rst projection p1 is continuous. Let A = {x . X |.y . Y [ c] (x, y). W}; then \nA = p1([[c] -1(W)). Since [ c] is a measurable function and W is a measurable set, [ c] -1(W) is a Borel \nsubset in the Polish space X \u00d7 Y . A is therefore analytic [7, Def. 14.1]; from Lusin s theorem [7, Th. \n21.10], it is universally measurable. In particular, it is \u00b5-measurable [7, \u00a717.A]. \u00b5(A) is thus well-de.ned. \n2Let us take a Turing machine (or program in a Turing\u00adcomplete language) F. There exists an algorithmic \ntransla\u00adtion taking F as input and outputting the Turing machine F computing the total function .F so \nthat W if F terminates in y or less steps on input x .F (x,y) =1 otherwise. Let us take X = Y = Nand \nZ = {0, 1} and the program F, and de.ne t{1} as before. t{1}(x)=1 ifandonlyif F terminates on input x. \nIt is a classical fact of computability theory that the t{1} function is not computable for all F [16]. \nend for p . c/n With the same notations as in the previous sub-section: (c)(n) tW = TW and thus the \ncon.dence interval is still valid: there is at least a 1- e probability that the true expectation I- \nlog e EtW is less than p = p + b2n . We shall see in the following section how to build abstract interpreters \nwith a view to using them for this Monte-Carlo method.  3 A CONCRETE SEMANTICS SUIT-ABLE FOR ANALYSIS \nFrom the previous section, it would seem that it is easy to use any abstract interpreter in a Monte-Carlo \nmethod. Alas, we shall now see that special precautions must be taken in the presence of calls to random \ngenerators inside loops or, more generally, .xpoints. 3.1 Concrete Semantics We have for now spoken of \ndeterministic programs taking one input x chosen according to some random distribution and one input \ny in some domain. Calls to random gener\u00adators (such as the POSIX drand48() function) are usually modeled \nby a sequence of independent random variables. If a bounded number of calls (= N) to such generators \nis used in the program, we can consider them as input values: x is then a tuple (x1,... ,xN ,v) where \nx1, ... , xn are the values for the generator and v is the input of the program. If an unbounded number \nof calls can be made, it is tempting to consider as an input a countable sequence of values (xn)n.N where \nx1 is the result of the .rst call to the generator, x2 the result of the second call ... ; a formal description \nof such a semantics has been made by Kozen [8, 9]. Such a semantics is not very suitable for program \nanalysis. Intuitively, analyzing such a semantics implies tracking the number of calls made to number \ngenerators. The problem is that such simple constructs as: if (...) { random(); } else {} are di.cult \nto handle: the countings are not synchronized in both branches. We shall now propose another semantics, \nidentifying oc\u00adcurrences of random generators by their program location and loop indices. The Backus-Naur \nform of the program\u00adming language we shall consider is: instruction ::= elementary || instruction ; instruction \nif boolean expr then instruction else instruction endif | while boolean expr do instruction done We \nleave the subcomponents largely unspeci.ed, as they are not relevant to our method. elementary instructions \nare deterministic, terminating basic program blocks like as\u00adsignments and simple expression evaluations. \nboolean expr boolean expressions, such as comparisons, have semantics as sets of acceptable environments. \nFor instance, a boolean expr expression can be x<y+4; its semantics is the set of exe\u00adcution environments \nwhere variables x and y verify the above comparison. If we restrict ourselves to a .nite number n of \ninteger variables, an environment is just a n-tuple of integers. The denotational semantics of a code \nfragment c is a map\u00adping from the set X of possible execution environments be\u00adfore the instruction into \nthe set Y of possible environments after the instruction. Let us take an example. If we take environments \nas elements of3, representing the values of three integer variables x, y and z, then [ x:=y+z] is the \nstrict function (x, y,z) .(y + z, y, z). Semantics of basic constructs (assignments, arithmetic operators) \ncan be easily dealt with this forward semantics; we shall now see how to deal with .ow control. The semantics \nof a sequence is expressed by simple com\u00adposition [ e1; e2] =[ e2] . [ e1] (1) Tests get expressed easily, \nusing as the semantics [ c] of a boolean expression c the set of environments it matches: [ if c then \ne1 else e2]](x)= if x . [ c] then [ e1]](x) else [ e2]](x) (2) and loops get the usual least-.xpoint \nsemantics (considering the point-wise extension of the Scott .at ordering on partial functions) [ while \nc do f] = lfp(.f..x. if x . [ c] then f . [ f]](x) else x). (3) Non-termination shall be noted by .. \nAs for expressions, the only constructs whose semantics we shall precise are the random generators. We \nshall con\u00adsider a .nite set G of di.erent generators. Each generator g outputs a random variable rg with \ndistribution \u00b5g; each call is independent from the precedent calls. Let us also con\u00ad * sider the set \nP of program points and the set Nof .nite * sequences of positive integers. The set C = P \u00d7 Nshall denote \nthe possible times in an execution where a call to a random generator is made: (p, n1n2...nl) notes the \nexecution of program point p at the n1-th execution of the outermost program loop, ... , nl-th execution \nof the innermost loop at that point. C is countable. We shall suppose that inside the inputs of the program \nthere is for each generator g in G a family ( g(p,w))(p,w).C of random choices. The semantics of the \nlanguage then become: [ e1; e2] =[ e2] . [ e1] (4) Tests get expressed easily, using as the semantics \n[ c] of a boolean expression c the set of environments it matches: [ if c then e1 else e2] .(w, x) = \nif x . [ c] then [ e1] .(w, x) else [ e2] .(w, x) (5) Loops get the usual least-.xpoint semantics (considering \nthe point-wise extension of the Scott .at ordering on partial functions): [ while c do f] .(w0,x0) = \n lfp (.f..(w, x).if x . [ c] then f . S . [ f] (w, x)) else x).(1.w0,x0)(6) where S.(c.w, x) = ((c + \n1).w, x). The only change is that we keep track of the iterations of the loop. As for random expressions, \n[ p : randomg] .(w, x) = g(p,w) (7) where p is the program point. This semantics is equivalent to the \ndenotational semantics proposed by Kozen [8, 9, 2nd semantics] and Monniaux [11], the semantic of a program \nbeing a continuous linear opera\u00adtor mapping an input measure to the corresponding output. The key point \nof this equivalence is that two invocations of random generators in the same execution have di.erent \nindices, which implies that a fresh output of a random gen\u00aderator is randomly independent of the environment \ncoming to that program point. 3.2 Analysis Our analysis algorithm is a randomized version of an ordi\u00adnary \nabstract interpreter. Informally, we treat calls to ran\u00addom generators are treated as follows: calls \noccurring outside .xpoint convergence iterations are interpreted as constants chosen randomly by the \ninterpreter;  calls occurring inside .xpoint convergence iterations are interpreted as upper approximations \nof the whole do\u00admain of values the random generator yield.  For instance, in the following C program: \nint x; x = coin_flip(); /* coin_flip() returns 0 or 1 */ /* each with probability 0.5 */ for(i=0; i<5; \ni++) { x = x + coin_flip(); } the .rst occurrence of coin_flip() will be treated as a ran\u00addom value, \nwhile the second occurrence will be treated as the least upper bound of {0} and {1}. This holds for naive \nabstract interpreters; more ad\u00advanced ones might perform dynamic loop unrolling or other semantic transformations \ncorresponding to a re.ne\u00adment of the abstract domain to handle execution traces: [ while c do e]](x)= \n[w[ ) .l..N1 (x) . .(l)) ) ) .k(x) . .N2 lfp n [ c] C k<N1+N2 (8) where .(x)=[ e]](xn[ c]]) and N1 \nand N2 are possibly decided at run-time, depending on the computed values. In this case, the interpreter \nuses a random generator for the occurrences of randomg operations outside lfp computations and abstract \nvalues for the operations inside lfp s. Its execution de.nes the .nite set K of (p, n1 ...nl) tags uniquely \nidentifying the random values chosen for g(p,n1...nl), as well as the values (.gc)c.K that have been \nchosen. This yields .( gc)g.G,c.C .y . Y (.c . Kg c =.gc) . [ c] (( gc)g.G,c.C,y). .Z(z.) (9) which means \nthat .( gc)g.G,c.C (.c . Kg c =.gc) . tW (( gc)g.G,c.C) = tW (z) (10) If we virtually choose randomly \nsome .gc for c/. K, we know that tW ((.gc)g.G,c.C) = tW (z.). Furthermore, (.gc) follows .C the product \nrandom distribution \u00b5g (each .gc has been cho\u00adsen independently of the others according to measure \u00b5g). \nLet us summarize: we wish to generate upper bounds of experimental averages of a Bernoulli random variable \ntW : X .{0, 1} whose domain has the product probability t.C measure \u00b5I . \u00b5 where \u00b5I is the input measure \nand g.Gg the \u00b5g s are the measures for the random number genera\u00adtors. The problem is that the domain \nof this random vari\u00adable is made of countable sequences; thus we cannot generate its input strictly speaking. \nWe instead e.ectively choose at random a .nite number of coordinates for the countable se\u00adquences, and \ncompute a common upper bound for tW for all inputs identical to our chosen values on this .nite number \nof coordinates. This is identical to virtually choosing a ran\u00addom countable sequence x and getting an \nupper bound of its image by tW . Implementing such an analysis inside an ordinary abstract interpreter \nis easy. The calls to random generators are inter\u00adpreted as either a random generation, or as the least \nupper bound over the range of the generator, depending on a ran\u00addomize .ag. This .ag is adjusted depending \non whether the interpreter is computing a .xpoint. The interpreter does not track the indices of the \nrandom variables: these are only needed for the proof of correctness. The analyzer does a cer\u00adtain number \nn of trials and outputs the experimental average \u00af tW (n) . As a convenience, our implementation also \noutputs the \u00af t(n)+t upper bound so that there is at least a probability 1-e W that this upper bound \nis safe according to inequation (11). This is the value that is reported in the experiments of sec\u00adtion \n5. While our explanations referred to a forward semantics, the abstract interpreter can of course combine \nforward and backward analysis [2, section 6], provided the chosen random values are recorded so that \nsubsequent passes of analysis can reuse them. Another related improvement, explained in section 4, uses \na preliminary backward analysis prior to random generation.  3.3 Arbitrary control-.ow graphs The abstract \ninterpretation framework can be extended to logic languages, functional languages and imperative lan\u00adguages \nwith recursion and other complex programming mechanisms (call-by-reference, local procedures passed as \nparameters, non-local gotos, exceptions) [1]. In such cases, the semantics of the program are expressed \nas a .xpoint of a system of equations over parts of the domain of environ\u00adments. The environment in that \ncase includes the program counter, call stack and memory heap; of course a suitable abstract lattice \nmust be used. Analyzing a program P written in a realistic imperative language is very similar to analyzing \nthe following inter\u00adpreter: s . initial state for P while s is not a termination state do s . N(s) end \nwhile where N(s) is the next-state function for P (operational se\u00admantics). The abstract analyzer analysis \nthat loop using an abstract state and an abstract version of N. Most analy\u00adses partition the domain of \nstates according to the program counter, and the abstract interpreter then computes the least .xpoint \nof a system of semantic equations. Such an analysis can be randomized in exactly the same fashion as \nthe one for block-structured programs presented in the previous section. It is all the same essential \nto store the generated values in a table so that backwards analysis can be used.  4 COMPLEXITY The complexity \nof our method is the product of two inde\u00adpendent factors: the complexity of one ordinary static analysis \nof the program; strictly speaking, this complexity depends not only on the program but on the random \nchoices made, but we can take a rough average estimate that de\u00adpends only on the program being analyzed; \n the number of iterations, that depends only on the re\u00adquested con.dence interval; the minimal number \nof it\u00aderations to reach a certain con.dence criterion can be derived from inequalities [18, appendix \nA] such as in\u00adequation (11) and does not depend on the actual pro\u00adgram being analyzed.  We shall now \nfocus on the latter factor, as the former de\u00adpends on the particular case of analysis being implemented. \n(n) Let us recall inequation (11): Pr (EtW = t\u00af+ t = W 2 e -2nt. It means that to get with 1 - e probability \nan ap\u00adproximation of the requested probability \u00b5, it is su.cient to - log e compute an experimental \naverage over r2t2 itrials. This exponential improvement in quality (Fig. 1) is nev\u00adertheless not that \ninteresting. Indeed, in practice, we might want e and t of the same order of magnitude as \u00b5. Let us take \ne = at where a is .xed. We then have n ~- log t , t2 which indicates prohibitive computation times for \nlow prob\u00adability events (Fig. 2). This high cost of computation for low-probability events is not speci.c \nto our method; it is true of any Monte-Carlo method, since it is inherent in the speed of convergence \nof averages of identically distributed random variables; this relates to the speed of convergence in \nthe cen\u00adtral limit theorem [18, ch 1]. It can nevertheless be circum\u00advented by tricks aimed at estimating \nthe desired low prob\u00adability by computing some other, bigger, probability from which the desired result \ncan be computed. 0.8 0.6 0.4 0.2 n Figure 1: Upper bound on the probability that the computed probability \nexceeds the real value by more than t, for t = 0.01. log10 scale 6.5 5.5 4.5 3.5 2.5 t Figure 2: Numbers \nof iterations necessary to achieve a prob\u00adability of false report on the same order of magnitude as the \nerror margin. int x, i; know (x>=0 &#38;&#38; x<=2); i=0; while (i < 5) { x += coin_flip(); i++; } \nknow (x<3); Figure 3: Discrete probabilities. The analyzer es\u00adtablishes that, with 99% safety, the probability \np of the outcome (x< 3) is less than 0.509 given worst-case nondeterministic choices of the precondition \n(x = 0.x = 2). The analyzer used n = 10000 random trials. Formally, p is . Pr coin flip .{0, 1}5 |.x \n. [0, 2] n [ P ]](coin flip,x) < 3). Each coin flip is chosen randomly in {0, 1} with a uniform distribution. \nThe exact value is 0.5. Fortunately, such an improvement is possible in our method. If we know that p1([[c] \n-1(W)) . R, with a mea\u00adsurable R, then we can replace the random variable tW by its restriction to R: \ntW |R; then EtW = Pr(R) .EtW |R. If Pr (R) and EtW are on the same order of magnitude, this means that \nEtW |R will be large and thus that the number of required iterations will be low. Such a restricting \nR can be obtained by static analysis, using ordinary backwards ab\u00adstract interpretation. A salient point \nof our method is that our Monte-Carlo computations are highly parallelizable, with linear speed\u00adups: \nn iterations on 1 machine can be replaced by n/m iter\u00adations on m machines, with very little communication. \nOur method thus seems especially adapted for clusters of low\u00adcost PC with o.-the-shelf communication \nhardware, or even more distributed forms of computing. Another improvement can be to compute bounds for \nseveral W sets simultaneously, doing common computations only once. 5 PRACTICAL IMPLEMENTATION AND EXPERIMENTS \nWe have a prototype implementation of our method, imple\u00admented on top of an ordinary abstract interpreter \ndoing for\u00adward analysis using integer and real intervals. Figures 3 to 5 show various examples for which \nthe probability could be computed exactly by symbolic integration. Figure 6 shows a simple program whose \nprobability of outcome is di.cult to .gure out by hand. Of course, more complex programs can be handled, \nbut the current lack of support of user-de.ned functions and mixed use of reals and integers prevents \nus from supplying real-life examples. We hope to overcome these limitations soon as implementation progresses. \n 6 CONCLUSIONS We have proposed a generic method that combines the well-known techniques of abstract \ninterpretation and Monte-Carlo program testing into an analysis scheme for probabilis\u00adtic and nondeterministic \nprograms, including reactive pro\u00ad double x; know (x>=0. &#38;&#38; x<=1.); x+=uniform()+uniform()+uniform(); \nknow (x<2.); Figure 4: Continuous probabilities. The analyzer es\u00adtablishes that, with 99% safety, the \nprobability p of the outcome (x< 2) is less than 0.848 given worst-case nonde\u00adterministic choices of \nthe precondition (x = 0 . x = 1). The analyzer used n = 10000 random trials. Formally, p is Pr . uniform \n. [0, 1]3 |.x . [0, 1] [ P ]](uniform,x) < 2.. Each uniform is chosen randomly in [0, 1] with the Lebesgue \nuniform distribution. The exact value is 5/6 0.833. double x, i; know(x<0.0 &#38;&#38; x>0.0-1.0); \ni=0.; while (i < 3.0) { x += uniform(); i += 1.0; } know (x<1.0); Figure 5: Loops. The analyzer establishes \nthat, with 99% safety, the probability p of the outcome (x< 1) is less than 0.859 given worst-case nondeterministic \nchoices of the precondition (x< 0 . x> -1). The analyzer. used n = 10000 random trials. Formally, p is \nPr uniform . [0, 1]3 |.x . [0, 1] [ P ]](uniform,x) < 1.. Each uniform is chosen randomly in [0, 1] with \nthe Lebesgue uniform distribution. The exact value is 5/6 0.833. { double x, y, z; know (x>=0. &#38;&#38; \nx<=0.1); z=uniform(); z+=z; if (x+z<2.) { x += uniform(); } else { x -= uniform(); } know (x>0.9 &#38;&#38; \nx<1.1); } Figure 6: The analyzer establishes that, with 99% safety, the probability p of the outcome \n(x> 0.9 . x< 1.1) is less than 0.225 given worst-case nondeterministic choices of the precondition (x \n= 0 . x = 0.1). Formally, p is Pr . uniform . [0, 1]2 |.x . [0, 0.1] [ P]](uniform,x) . [0.9, 1.1].. \nEach uniform is chosen randomly in [0, 1] with the Lebesgue uniform distribution. grams whose inputs \nare modeled by both random and non\u00addeterministic inputs. This method is mathematically proven correct, \nand uses no assumption apart from the distributions and nondeterminism domains supplied by the user. \nIt yields upper bounds on the probability of outcomes of the program, according to the supplied random \ndistributions, with worse\u00adcase behavior according to the nondeterminism; whether or not this bounds are \nsound is probabilistic, and a lower-bound of the soundness of those bounds is supplied. While our ex\u00adplanations \nare given using a simple imperative language as an example, the method is by no means restricted to imper\u00adative \nprogramming. The number of trials, and thus the complexity of the com\u00adputation, depends on the desired \nprecision. The method is parallelizable with linear speed-ups. The complexity of the analysis, or at \nleast its part dealing with probabilities, in\u00adcreases if the probability to be evaluated is low. However, \nstatic analysis can come to help to reduce this complexity. We have implemented the method on top of \na simple static analyzer and conducted experiments showing interesting re\u00adsults on small programs written \nin an imperative language. As implementation progresses, we expect to have results on complex programs \nakin to those used in embedded systems. REFERENCES [1] Fran\u00b8cois Bourdoncle. S\u00b4emantiques des Langages \nIm\u00ad p\u00b4eratifs d Ordre Sup\u00b4erieur et Interpr\u00b4etation Abstraite. \u00b4 PhD thesis, Ecole Polytechnique, 1992. \n [2] Patrick Cousot and Radhia Cousot. Abstract interpre\u00adtation and application to logic programs. J. \nLogic Prog., 2-3(13):103 179, 1992. [3] Patrick Cousot and Nicolas Halbwachs. Automatic dis\u00adcovery of \nlinear restraints among variables of a program. In Proceedings of the Fifth Conference on Principles \nof Programming Languages. ACM Press, 1978. [4] J.L. Doob. Measure Theory, volume 143 of Graduate Texts \nin Mathematics. Springer-Verlag, 1994. [5] Jifeng He, K. Seidel, and A. McIver. Probabilistic mod\u00adels \nfor the guarded command language. Science of Com\u00adputer Programming, 28(2 3):171 192, April 1997. For\u00admal \nspeci.cations: foundations, methods, tools and ap\u00adplications (Konstancin, 1995). [6] Wassily Hoe.ding. \nProbability inequalities for sums of bounded random variables. J. Amer. Statist. Assoc., 58(301):13 30, \n1963. [7] Alexander S. Kechris. Classical descriptive set theory. Graduate Texts in Mathematics. Springer-Verlag, \nNew York, 1995. [8] D. Kozen. Semantics of probabilistic programs. In 20th Annual Symposium on Foundations \nof Computer Sci\u00adence, pages 101 114, Long Beach, Ca., USA, October 1979. IEEE Computer Society Press. \n[9] D. Kozen. Semantics of probabilistic programs. Journal of Computer and System Sciences, 22(3):328 \n350, 1981. [10] N. G. Leveson. Software safety: Why, what, and how. Computing Surveys, 18(2):125 163, \nJune 1986. [11] David Monniaux. Abstract interpretation of probabilis\u00adtic semantics. In Seventh International \nStatic Analysis Symposium (SAS 00), number 1824 in Lecture Notes in Computer Science. Springer-Verlag, \n2000. &#38;#169;cSpringer-Verlag. [12] Carroll Morgan, Annabelle McIver, Karen Seidel, and J. W. Sanders. \nRe.nement-oriented probability for CSP. Formal Aspects of Computing, 8(6):617 647, 1996. [13] Simeon \nNtafos. On random and partition testing. In Michal Young, editor, ISSTA 98: Proceedings of the ACM SIGSOFT \nInternational Symposium on Software Testing and Analysis, pages 42 48, 1998. [14] Panel on Statistical \nMethods in Software Engineering. Statistical Software Engineering. National Academy of Sciences, 1996. \n[15] G. Ramalingam. Data .ow frequency analysis. In Pro\u00adceedings of the ACM SIGPLAN 96 Conference on \nPro\u00adgramming Language Design and Implementation, pages 267 277, Philadelphia, Pennsylvania, 21 24 May \n1996. [16] H. Rogers. Theory of recursive and e.ective computabil\u00adity. MGH, 1967. [17] Walter Rudin. \nReal and Complex Analysis. McGraw-Hill, 1966. [18] Galen R. Shorack and Jon A. Wellner. Empirical Pro\u00adcesses \nwith Applications to Statistics. Wiley series in probability and mathematical statistics. John Wiley \n&#38; Sons, 1986. [19] P. Th\u00b4evenod-Fosse and H. Waeselynck. Statemate ap\u00adplied to statistical software \ntesting pages 99-109. In Pro\u00adceedings of the 1993 international symposium on Soft\u00adware testing and analysis, \npages 99 109. Association for Computer Machinery, June 1993.  A PROBABILITY THEORY Throughout this paper \nwe take the usual mathematical point of view of considering probabilities to be given by measures over \nmeasurable sets [17, 4]. A s-algebra is a set of subsets of a set X that contains \u00d8 and is stable by \ncountable union and complementa\u00adtion (and thus contains X and is stable by countable intersection). For \ntechnical reasons, not all sets can be measured (that is, given a probability) and we have to restrict \nourselves to some su.ciently large s-algebras, such as the Borel or Lebesgue sets [17].  A set X with \na s-algebra sX de.ned on it is called a measurable space and the elements of the s-algebra are the measurable \nsubsets. We shall often men\u00adtion measurable spaces by their name, omitting the s\u00adalgebra, if no confusion \nis possible.  If X and Y are measurable spaces, f : X . Y is a measurable function if for all W measurable \nin Y , f-1(W) is measurable in X.  A positive measure is a function \u00b5 de.ned on a s-algebra sX whose \nrange is in [0, 8] and which is countably additive. \u00b5 is countably additive if, taking (An)n.Na disjoint \ncollection of elements of sX , then  o \u00b5 (.88 n=0An)= n=0 \u00b5(An). To avoid trivialities, we assume \u00b5(A) \n< 8 for at least one A. If X is countable, sX can be P(X), the power-set of X, and a measure \u00b5 is determined \nby its value on the o singletons: for any A . X, \u00b5(A)= \u00b5({a}). a.A A probability measure is a positive \nmeasure of total weight 1; a sub-probability measure has total weight less or equal to 1. We shall note \n=1(X) the sub\u00adprobability measures on X.  Given two sub-probability measures \u00b5 and \u00b5 I (or more generally, \ntwo s-.nite measures) on X and XI respec\u00adtively, we note \u00b5 . \u00b5 I the product measure [17, de.ni\u00ad tion \n7.7], de.ned on the product s-algebra sX \u00d7 sXI . The characterizing property of this product measure \nis that \u00b5 . \u00b5 I(A \u00d7 AI)= \u00b5(A).\u00b5 I(AI) for all measurable sets A and AI . It is also possible to de.ne \ncountable products of measures; if \u00b5 is a measure over the mea\u00ad  .N N surable space X, then \u00b5 is a measure \nover the set X of sequences of elements of X. For instance, let us take \u00b5 the measure on the set {0, \n1}with \u00b5({1})= p and \u00b5({0})=1 - p. Let us take S the set of sequences over {0, 1} beginning with (0, \n0, 1, 0). .N \u00b5 (S)= p(1 - p)3 is the probability of getting a sequence beginning with (0, 0, 1, 0) when \nchoosing at random a count\u00adable sequence of {0, 1} independently distributed follow\u00ading \u00b5. B ESTIMATING \nTHE PROBABIL- ITY OF A RANDOM EVENT BY THE MONTE-CARLO METHOD We consider a system whose outcome (success \nor failure) depends on the value of a parameter x, chosen in the set X according to a random distribution \n\u00b5. The behavior of this system is described by a random variable V : X .{0, 1}, where 0 means success \nand 1 failure. The law of large numbers says that if we indepen\u00addently choose inputs xk, with distribution \n\u00b5, and com\u00ad o pute the experimental average V (n) = 1 n V (xk), then n k=1 limn.8 V (n) = EV where \nEV is the expectation of fail\u00adure. Intuitively, it is possible to estimate accurately EV by e.ectively \ncomputing V (n) for a large enough value of n. Just how far should we go? Unfortunately, a general fea\u00adture \nof all Monte-Carlo methods is their slow asymptotic convergence speed. Indeed, the distribution of the \nexper\u00adimental average V (n) is a binomial distribution centered around EV . With large enough values \nof n (say n = 20), this binomial distribution behaves mostly like a normal (Gaus\u00adsian) distribution (Fig. \n7) with means p = EV and standard p(1-p) deviate v n . More generally, the central limit theorem t \n Figure 7: The Gaussian normal distribution centered on 0, with standard deviate 1. predicts that the \naverage of n random variables identically distributed as V has the same expectation EV as V and standard \ndeviate vswhere s is the standard deviate of V . n The standard deviate measures the error margin on \nthe com\u00adputed result: samples from a gaussian variable centered on x0 and with standard deviate s fall \nwithin [x0 - 2s, x0 +2s] about 95% of the time. We can better evaluate the probability of underestimating \nthe probability by more than t using the Cherno.-Hoe.ding [6] [18, inequality A.4.4] bounds: 2 EV = V \n(n) -2nt Pr (+ t = e (11) This bound, fully mathematically sound, means that the probability of underestimating \nV using V (n) by more than t 2 -2nt is less than e . Any Monte-Carlo method has an inherent margin of \nerror; this margin of error is probabilistic, in the sense that facts such as the value we want to compute \nis in the interval [a, b] are valid up to a certain probability. The size of the interval of safety for \na given probability of error varies in v 1/n. C ABSTRACT INTERPRETATION Let us recall the mathematical \nfoundations of abstract inter\u00adpretation [3, 2]. Let us consider two preordered sets A and Z so that there \nexist monotone functions .A : A . P(A), where A = X \u00d7 Y , and .W : Z . P(Z), where P(Z) is the set of \nparts of set Z, ordered by inclusion. .W is called the concretization function. The elements in A and \nZ represent some properties; for instance, if X = m and Y = n , A could be the set of machine descriptions \nof polyhedra in m+n and .A the function mapping the description to the set of points inside the polyhedron \n[3]. A simpler case is the intervals, where the machine description is an array of integer couples (a1,b1; \na2,b2; ... ; an,bn) and its concretization is the set of tuples (c1; ... ; cn) where for all i, ai = \nci = bi. We then de.ne an abstract interpretation of program c to be a monotone function [ c] : A . Z \nso that .a . A, .a . Aa . .A(A ) . [ c]](a) . .Z . [ c] (a ). In the case of intervals, abstract interpretation \npropagates intervals of variation along the computation. Loops get a .x\u00adpoint semantics: the abstract \ninterpreter heuristically tries to .nd intervals that are invariant for the body of the loop. Such heuristics \nare based on widening and narrowing oper\u00adators [2]. It is all the same possible to de.ne backwards abstract \ninterpretation: a backwards abstract interpretation of a program c is a monotone function [ c] -1 : Z \n. A so that .z . Z, .z . Zz . .Z(Z ) . [ c] -1(z) . .A . [ c] -1 (z ). Further re.nement can be achieved \nby iterating forwards and backwards abstract interpretations [2].    \n\t\t\t", "proc_id": "360204", "abstract": "We introduce a new method, combination of random testing and abstract interpretation, for the analysis of programs featuring both probabilistic and non-probabilistic nondeterminism. After introducing \"ordinary\" testing, we show how to combine testing and abstract interpretation and give formulas linking the precision of the results to the number of iterations. We then discuss complexity and optimization issues and end with some experimental results.", "authors": [{"name": "David Monniaux", "author_profile_id": "81100428259", "affiliation": "&#201;cole Normale Sup&#233;rieure, Laboratoire d'Informatique, 45, rue d'Ulm, 75230 Paris cedex 5, France", "person_id": "P62994", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/360204.360211", "year": "2001", "article_id": "360211", "conference": "POPL", "title": "An abstract Monte-Carlo method for the analysis of probabilistic programs", "url": "http://dl.acm.org/citation.cfm?id=360211"}