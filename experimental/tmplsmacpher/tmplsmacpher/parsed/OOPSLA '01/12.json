{"article_publication_date": "10-01-2001", "fulltext": "\n Partial Method Compilation using Dynamic Profile Information John Whaley Computer Systems Laboratory \nStanford University Stanford, CA 94305 jwhaley@alum.mit.edu ABSTRACT The traditional tradeoff when performing \ndynamic compi- lation is that of fast compilation time versus fast code per- formance. Most dynamic compilation \nsystems for Java per- form selective compilation and/or optimization at a method granularity. This is \nthe not the optimal granularity level. However, compiling at a sub-method granularity is thought to be \ntoo complicated to be practical. This paper describes a straightforward technique for per- forming compilation \nand optimizations at a finer, sub-method granularity. We utilize dynamic profile data to de- termine \nintra-method code regions that axe rarely or never executed, and compile and optimize the code without \nthose regions. If a branch that was predicted to be rare is actu- ally taken at run time, we fall back \nto the interpreter or dynamically compile another version of the code. By avoid- ing compiling and optimizing \ncode that is rarely executed, we axe able to decrease compile time significantly, with little to no degradation \nin performance. Furthermore, ignoring rarely-executed code can open up more optimization opportunities \non the common paths. We present two optimizations -- partial dead code elimination and raxe-path-sensitive \npointer and escape analysis -- that take advantage of rare path information. Using these op- timizations, \nour technique is able to improve performance beyond the compile time improvements. 1. INTRODUCTION \nDynamic compilation systems explore an interesting trade- off. On one hand, we would like to have code \nperformance that is comparable to static compilation techniques. How- ever, we would also like to avoid \nlong startup delays, long latencies, and slow responsiveness, which implies that the dynamic compiler \nshould be fast. Permission to make digital or hard copies of all or part of this work fbr personal or \nclassroom use is granted without fee provided that copies are not made or distributed for profit or commercial \nadvantage and that copies bear this notice and the full citation on the first page, \"1\"o copy otherwise, \nto republish, to post on servers or to redistribute to lists, requires prior specific pctmisskm and/or \na fbe. OOPSLA 01 Tampa Florida USA Copyright ACM 2001 1-58113-335 -9101/10...$5.00 Many dynamic compilation \nsystems attack this problem by using an interpreter and an optimizing compiler. They begin by interpreting \nthe code, and when the execution count for the method reaches a certain threshold or by some other heuristic, \nthey use the optimizing compiler to dynamically compile the code for the method I42, 38]. Some systems \nuse a fast code generator (baseline compiler) rather than an interpreter [15, 9]. The problem with these \nsystems is that the execution speed of the interpreted or baseline compiled code is significantly worse \nthan that of fully optimized code -- typically 30% to ten times slower for baseline compiled code [15, \n9] and ten to a hundred times slower for interpreted code [42, 38]. Therefore, we would like to transfer \ninto the optimized ver- sion as quickly as possible. However, the optimizing com- piler can take a long \ntime to compile. Waiting for the op- timizing compiler to finish hurts program startup and re- sponse \ntimes. Some systems use a multi-level compilation approach, whereby they progress through a number of \ndif- ferent compilation \"levels\", and thereby slowly \"accelerate\" into optimized execution [3, 38, 43]. \nHowever, this simply exacerbates the problem of having a long delay until the program runs at full speed. \nUnlike interpretation, compilation takes time that is propor- tional to the amount of code that is being \ncompiled. Many analyses and optimizations are superlinear in the size of the code (basic blocks, instructions, \nregisters, etc.) This can cause the compilation time to increase significantly as the amount of code \nbeing compiled gets large. Compilation of large amounts of code is the cause of undesirably long com- \npilation times. However, when compiling a method at a time, we do not really have much choice in the \nmatter. Some methods are large to begin with, and others grow large after performing inlining. Even when \nbeing frugal and inlining only when it will make a noticeable difference in performance, methods can \nstill grow large, and excessively restricting inlining can significantly hurt performance [42, 4]. The \nroot of the problem is that method boundaries do not correspond to the code that would most benefit from \nopti- void read db(String fn) { int n = O, act = O; byte buffer[] = null; try { FilelnputStream'sif \n= new FilelnputStream(fn); n = sif.getContentLength(); buffer = new byte[n]; int b; while ((b = sif.read(buffer, \nact, n-act))>O){ act = act + b; } sir.close(); if (act != n) { /* lots of error handling code, rare \n*/ }  } catch (IOException ioe) { /\u00a2 lots of error handling code, rare */ } } int read(byte b[], int \noff, int len) { try /* ... */ } catch (lOException Joe) { /* lots of error handli~ code, rare */  \n} } Figure 1: From spec db. Method boundaries do not correspond well to where the time is actually spent. \nmizing compilation. Even \"hot\" methods typically contain some code that is rarely or never executed, \nbut often con- tain frequently-executed call sites to methods (which in turn, contain their own rarely-executed \ncode.) Figure 1 contains a paraphrased example from the spec db benchmark [44]. In this example, the \nread_rib method is hot due to the while loop that it contains. However, the error handling code guarded \nby the if and the exception handler are rarely executed. Likewise, the call to read() is in the loop \nand therefore a good candidate for inlining. However, read() itself contains rarely-executed error handling \ncode. The re- gion that is important to compile -- the while loop and the hot path in read() --have nothing \nto do with the method boundaries. Using a method granularity causes the compiler to waste time compiling \nand optimizing large pieces of code that do not matter. This paper describes a technique to selectively \ncompile and optimize partial methods. This gives us much better control over what we spend time compiling \nand optimizing. This technique uses dynamic profile data to make a prediction of what code will actually \nbe executed, and selectively com- piles and optimizes only that code. If the program actually attempts \nto branch to code that was not compiled (so-called \"rare code\" ), the system falls back to interpretation \nor an- other dynamically compiled version. We also describe some optimizations that take special ad- \nvantage of rare path information to improve their effective- ness. We describe two optimizations -- partial \ndead code elimination and rare-path-sensitive pointer and escape anal- ysis --that make special use of \nrare path information to optimize for the common paths. The idea of optimizing with respect to \"hot\" \ncode regions is not new. The Multiflow compiler uses trace scheduling [33] to optimize code with respect \nto the most frequently ex- ecuted path. The IMPACT compiler [13] uses superblock scheduling, dividing \nthe program along the most frequently executed paths into single-entry multiple-exit regions called superblocks, \nand performs scheduling on those regions. Dy- namo [5] uses dynamic profile information to perform in- \nstruction scheduling optimizations on instruction trace frag- ments. There are two main differences between \nour tech- nique and prior techniques. The first difference is that our technique operates on arbitrary \ncontrol-flow graph regions, whereas prior techniques only operate on a single trace. The second difference \nis that our technique does not generate code for rare traces until they are actually executed at run \ntime. Our technique allows all analyses and optimizations to suc- cessfully ignore' the rare code, improving \ncompilation time. We use three techniques to guarantee the safety of optimiz- ing code while ignoring \nrare code. First, like superblock scheduling [13], we do not allow control flow merges from the rare \ncode back into the optimized path. Second, during analysis we conservatively summarize the effect of \nthe rare code in order to prevent potentially invalid transformations. The third technique is to generate \ncompensation code on the rare paths to undo the effects of some transformations, ~ la trace scheduling. \nBecause there are no control flow merges from rare code back into non-rare code, our technique avoids \none of the major problems with trace scheduling -- the com- plexity of generating the compensation code. \nBecause code is generated for rare paths in an on-demand basis, we can also avoid the explosive code \ngrowth associated with tail- splitting in superblock scheduling. Our technique is able to significantly \nreduce compilation time while simultaneously improving code quality. Ignoring rare code improves the \ndata flow information on the com- mon paths and thereby opens up more optimization oppor- tunities. Furthermore, \nignoring rare code improves cache locality, and the decrease in compile time potentially al- lows room \nfor the compiler to use more aggressive, time-consuming analyses and optimizations.  1.1 Contributions \nThis paper makes the following contributions: Partial method compilation technique: It presents the \ngeneral technique of partial method com- pilation, including the necessary modifications to the compiler \nand the method of falling back to interpreta- tion or to another compiled version.  Partial dead code \nelimination exploiting rare code information: It gives specific details of a new partial dead code elimination \noptimization. This algo-  rithm differs from other partial dead code elimination interpreted algorithms \nin that it is an optimistic approach. It ef- Stage 1: ficiently identifies operations that are only necessary \ncode in rare blocks and determines when it is safe to move them into the rare blocks. Pointer and escape \nanalysis exploiting rare code information: It gives details of a pointer and escape analysis algorithm \nthat exploits rare code in- formation. This algorithm is able to effectively ignore the existence of \nthe rare path and fully optimize the common case, \u00ae Frequency of rare code in programs: It gives ex- \nperin]ental results showing that a significant amount of code in typical programs is never executed or \nonly exe- cuted during startup. This differs from previous work in that it ignores program initialization, \nwhich has a very different profile than the rest of the execution. Effectiveness of this technique: It \npresents experi- mental results from a preliminary implementation that shows that this technique is very \neffective in reducing compilation time, and can also improve optimization opportunities on the common \npaths. The remainder of the paper is organized as follows. Sec-tion 2 describes how we use profile data \nto distinguish be- tween commonly-executed and rarely-executed code. It also presents experimental results \nthat show the frequency of rare code in programs and the effectiveness of using dynamic pro- file data \nto predict rarely-executed code. Section 3 describes the general technique of partial method compilation. \nIt de- tails how to safely perform data flow analysis in the pres- ence of rare blocks, and it describes \ntwo optimizations that use compensation code to reverse transformations --partial dead code elimination \nand pointer and escape analysis. It also describes the method of falling back to the interpreter. Section \n4 presents experimental results from our preliminary implementation of this technique. Section 5 compares \nour work to related work, and we conclude in Section 6. 2. RARE CODE We begin by providing an overview \nof our dynamic com-pilation system. In Section 2.2, we describe the technique that we use to predict \nrare blocks. Then, in Section 2.3, we provide experimental data showing the frequency of rare blocks \nin actual programs. In Section 2.4, we evaluate the effectiveness of our prediction technique. 2.1 Overview \nof dynamic compilation system Our system is a multilevel compilation system, with an in-terpreter and \ntwo compilation levels. See Figure 2. Transi- tion between the three execution types is triggered by \nper- method dynamic execution counts. A dynamic execution count is the sum of the number of times that \nthe method was invoked and the number of backward branches taken within the method. Execution begins \nwith the interpreter. Once a method execution count reaches a certain thresh- old (tl), we upgrade it \nto a compiled version. This version ~ r - ~ I when execution i count = t 1 compiled Stage 2: code T \nJ when execution It count = t2 fully optimized Stage 3: code Figure 2: Overview of the multilevel compilation \nsystem. Execution transfers from interpreter to compiled code to fully optimized code. includes some \nsimple optimizations and limited method in- lining. Later, after another threshold (t2) is reached, we \nup- grade to a fully-optimized compiled version, which uses more aggressive optimizations and intining. \nNote that our system can transition to a newly compiled version of a method at any backward branch. Therefore, \neven long-running meth- ods can be upgraded. 2.2 Identifying rare code Our technique for identifying \nrare code is very simple: we say that any basic block executed during the second stage, the first compiled \nversion, is not rare, and any basic block that is not executed in that version is rare. Because of our \nuse of threshold values to trigger recompilation, this essen- tially means that we use the set of executed \nbasic blocks in execution numbers tl through t2 of a method as the set of non-rare blocks. We gather \nthe code coverage information by adding instru- mentation code to basic blocks. By using the same threshold \nvalues for profile data collection and compilation transition, we can avoid having to make modifications \nto the interpreter and/or the fully-optimizing compiler, and we do not need to worry about enabling or \ndisabling profiling. These are han- dled implicitly by transitioning to a new compiled version. Figure \n3 contains code paraphrased from the addElement method of java.util.Vector. This code causes an element \nto be added to a Vector; when the array backing the Vector overflows, the code reallocates a new array \ntwice the size. In method foe, we create a Vector with an initial capacity of i0 and then begin adding \nelements to it with addElement. Assume that the threshold tl is 3000 and the threshold t2 JavaCUP~ ] \n,IavaLEX~-S\"~ng~-~ ........ ...... javac ] mpegaud ] mtrt ] jack:kJ check ] comp-~ss db ] 54.24% 66.94% \n46.24% 50.79% 64.49% 61.05% 56.66% 56.89% 58.19% 65.04% 45.19% 52.19% 30.07% 15.07% 60.00% 46.82% 55~65% \n50.13% 51.88% 64.81% ...... 471(1'5% 14.10% 4.25% 45.31% 39.33% 53.33% 49.71% 46.13% 61.19% --38.61% \n35.43% 5.32% 2,07% 45.31% 26.97% 49.79% 47.87% 43.03% 55.70% 35.76% 33.76% 2.84% 2.07% 43.27% 17.98% \n47,45% 46.20% 42.92% 49.41% 29.30% 26.96% 1.56% 2.07% 43.27% 17.98% 44.86% 44.70% 42.92% 46.74% 23.44% \n0.59% i.22% 41.63% 15.36% 39.56% 44.28% 42.37% 38.85% Figure 4: Table that shows how the percentage of \ntouched basic blocks of executed methods varies by changing the threshold at which we begin measuring. \nRows correspond to threshold values; columns correspond to benchmarks. [-'-~l Linpack I ~aCUP I JavaLEX \nI SwingSet ] check] compress ] db ] javac I mpegaud [ mtrt I jack] i 100100% lOO.OO% lOO,OO% lOO.OO% \nlOO.OO% 10o.oo% lOO.OO% lO0.OO% lOO.OO% lOO.OO% lO6.OO% 10 89.51% 82.91% 83.18% 59.06% 26.00% 88.98% \n73.78% 97.45% 81.79% 84.85% 99.54% 100 89151% 82.72% 72.29% 25.19% 7.05% 59.18% 58.80% 92.09% 81.04% \n75.88% 91.77% 500 64.20% 70.00% 57.55% 8.86% 3.28% 59.18% 40.82% 84.01% 78.53% 71.35% 81.59% lOOO 64,20% \n67.22% 55.47% 4.62% 3.28% 55.10~ 28.46% 79.08% 76.11% 7L35% 71.37% 2000 64.20% 58.99% ~il.59% 2.50% ....3.28% \n55.10% 28.46% 74.35% 73.02% 7L35% 66.79% 5000 64.20% 53.42% 35.43% 0.93% 1.94% 52.65% 24.72% 65.78% 71.76% \n69.69% 55.62% Figure 5: Table that shows how the percentage of basic blocks in methods exceeding the \nexecution threshold varies as we change the execution threshold. 100% corresponds to all basic blocks \nin all executed methods. void addElement(0bject obj) { int oldCapacity= elementData.length; int minCapacity \n= elementCount+l; if (minCapacity > oldCapacity) { Object oldData[] = elementData; int newCapacity = \noldCapacity*2; if (newCapacity < minCapacity) newCapacity = minCapacity; ) elementData = new 0bject[newCapacity]; \nSystem.arraycopy(oldData, 0, elementData, O, elementCount); } elementData[elementCount++] = obj;  \nvoid leo(...) { Vector v = new Vector(10); for (i=0; i<5000; ++i) { v.addElement(o); } } Figure 3: \nCode paraphrased from java.util.Vector. is 5000.1 After the 3000th element is added, execution will change \nfrom the interpreter to the instrumented compiled version and it will begin collecting profile data. \nAfter the 5000th element is added, execution will transfer to the fully optimized version. However, even \nthough the code to grow the backing array has executed many times before, it will still be marked as \nrare, because it was not executed between the 3000th and 5000th call (it executed on the 10th, 20th, \n1This example is only used to illustrate how rare code is determined; it is not intended to be realistic. \n..., 2560th call). There are many techniques to minimize the placement and execution of instrumentation \ncode [6]. Furthermore, be-cause we only care about code coverage, each instrumen- tation point only needs \nto be executed once; we can rewrite code or branch targets such that each instrumentation point is disabled \nafter it is executed the first time. Our implemen- tation is incredibly simple --we instrument the targets \nof conditional forward branches, and do not bother disabling instrumentation points. However, even with \nthis naive tech- nique, we saw no substantial performance degradation. This is probably due to the fact \nthat a majority of the benchmark execution time was spent executing fully optimized code, which does \nnot contain any instrumentation code. 2.3 Frequency of rare code Next, we give experimental data that \nshows that when com- piling at a method granularity, a large amount of code is never executed, or only \nexecuted during initialization. We use a various set of GUI and non-GUI programs, includ-ing the industry-standard \nSPECjvm98 benchmark suite [44]. The numbers reported here only include the applications themselves --class \nlibraries are not included: See the table in Figure 4 and the corresponding graph in Figure 6. Note that \nthe graphs use a semi-log scale for the x-axis. In the tables, the numbers along the left correspond \nto the number of initial method invocations or iterations that were ignored (profile start threshold). \nThe benchmarks are listed across the top. Each entry in the table is the percentage of touched basic \nblocks of executed methods after the given number of invocations or iterations are performed. The first \nthing to notice when looking at this data is that even when the profile start threshold is one, less \nthan 67% of the basic blocks of executed methods are ever executed. Furthermore, increasing the threshold \ncauses the percent- 100.00% --e--Linpaek --=--dava(:YdP 80.00% --&#38;-- davaLEX -m- SwingSet I]0.00% \ncheck compress 40.00% I ' i(~sS db ~jSVS=C 20.00% mpellaud mtrt 0.00% --~-jack 1 10 100 500 1000 2000 \n5000 Figure 6: Graph of the data in Figure 4. 100.00% Linpaok --m--- davaCAJP 80.00% davaLEX Swim~et \n 60.00% eheok -e--oompress ---p*--je~ 40.00% db ~jaVSC 20.00% mpei[aud --a-- mtrt 0.00% --~--jack 1 \n10 100 500 1000 2000 5000 Figure 7: Graph of the data in Figure 5. age to drop rapidly. As the threshold \ngets large, the per- centage eventually stabilizes. This corresponds to the fact that a large amount \nof code is only executed during program startup, and the program eventually enters a \"stewdy-state\" where \nthe same set of code is being executed. Now let us compare this to the percentage of basic blocks that \nwould be compiled using a method-at-a-time compila- tion strategy. The table in Figure 5 and the corresponding \ngraph in Figure 7 show the ratio of the number of basic blocks in methods exceeding the execution threshold \nto ba- sic blocks in all executed methods. This corresponds to the number of basic blocks that a method-at-a~time \ncompilation strategy would compile. When we compare these numbers to those in Figure 4, we see that the \nmethod-at-a-time strat- egy typically compiles 50% to 100% more basic blocks, even when the threshold \nis high. Therefore, there is little correla- tion between method boundaries and executed basic blocks, \neven for frequently-executed methods. This data shows that a significant amount of code is never executed \nafter program startup, and that method bound-aries are not a good determination of what code actually \nexecutes. Therefore, a partial method compilation strategy has the potential for significant savings \nin compilation time and code size. 2.4 Accuracy of rare code prediction tech-nique Next, we evaluate \nthe effectiveness of our prediction tech- tuque. The table in Figure 8 shows the accuracy of the prediction \nat various 1;2 (end profiling) threshold levels. We used a value of tl=2000 for these experiments. The \nfirst and second columns give the program name and the t2 (end profiling) threshold. The third column \nshows the percent- age of blocks that were found to be rare by our profiling in Section 2.2. The fourth \ncolumn shows the percentage of non-rare blocks that were executed after the t2 thresh- old was reached. \nThe fifth column shows the percentage of rare blocks that were executed after the t2 threshold was reached. \nThe table in Figure 9 shows the accuracy of the prediction in terms of dynamic execution counts. In this \ntable, the third column shows the percentage of times that a non-rare block was executed, and the fourth \ncolumn shows the percentage of times that a rare block was executed. As we can see from Figure 8, the \nmisprediction rate for rare blocks decreases as we increase the amount of profile data that we collect, \nsometimes decreasing all the way down to zero. The misprediction rate varies a lot between bench- marks, \nbut is in general very low. Figure 9 shows us that rare blocks have extremely low dynamic execution counts \ncompared to non-rave blocks. This meaaas that the execu-tion time of rare blocks should have almost no \neffect on the total execution time. 3. TECHNIQUE This section describes the technique of partial method \ncom- pilation and optimization. Section 3.1 describes the gen- eral technique. Section 3.2 argues the \ncorrectness of our technique. Sections 3.3 and 3.4 describe two optimizations, escape analysis and partial \ndead code elimination, that at-tempt to optimize for the common paths. Section 3.5 de- scribes the process \nof reconstructing the interpreter state. 3.1 Overview The general idea of the technique is to replace \nall entries into rare blocks with stubs that transfer control to the in- terpreter. The rare blocks are \ncompletely removed from the compiler's intermediate representation. Only very minimal changes to the \ncompiler are necessary; optimizations can optionally use rare block information to attempt to better \noptimize the common paths. At the end of compilation, we store a map corresponding to each interpreter \ntransfer point, which specifies how to reconstruct the interpreter state at that point. We now describe \neach step of the process in detail. 1. Based on profile data, determine the set of rare blocks. The technique \nof determining rare blocks is described in Section 2.2. The entry points of the rare basic blocks are \nmapped to abstract program locations, which are Benchmark Threshold Percentage rare blocks N0n-rare \nblocks\" executed Rare bl0cks executed ..... Linpaek 5000 iii 50.96% lO0:00% 26.42% 10000 56,25% 100.00% \n44.44% JavaCUP 25000 5000 10000 25000 30,77% 54,27% 41.90% 41.717o 92.59% 99,74% 97,38% 97.25%' 0.00% \n5.68% 6,82% 6.41% JavaLEX 5000 45.69% 96.40% 26.69% lOOOO 42.05% 86.60%\" 16.22% 25000 33.25% 85.02%' \n18.80% SwingSet 5000 40.09% 99.26% 9.89% 10000 33.64% 100.00% 2.78% 25000 19.23% 100.00% 20.00% compress \n5000 41.86% 100.00% 50.00% lO00O 42.28% i00.00% 51.92% 25000 42.28% 100.00% 51.92% db 5000 53.03% 100.00% \n23.57% I0000 53.0370 100.00% 28.57% 25000 41.67% lO0.OO% 16.00% javac 5000 45.37% 99.26% 15.61% 10000 \n43.11% 99.85% 12.55% 25000 41.15% 100.00% 3.40% mpegaud 5000 39.81% 100.00% 3,80% 10000 \" 35~20% 100.00% \n3.27% 25000 32.91% 100.00%\" 3.40% mtrt 5000 49.68% 99.37% 21,09% iO000 48.56% 98.13% 19.80% 25000 43.79% \n99.37% 6.48% jack 5000 38.73% 100.00% 22.12% I0000 3~.ia% 100.00% 1,60% 25000 31.76% 100.00% L48% Figure \n8: Accuracy of rare code prediction technique at different thresholds, tl=2000 and t2 varies as the value \nin the second column. Benchmark iA~})ack \"' JavaCUP JavaLEX SwingSet compress db javac mpeg~ud mtrt jack \nThreshold 5000 10000 25000 5000 i0000 25000 5000 10O00 25000 5000 10000 25000 5000 10000 25000 5000 10000 \n25000 5000 10000 25000 5000 i0000 25000 5000 10000 25000 5000 10000 25000 Non-rare block execution frequency \n94.9564% 94.'6827% lO6~booo% 99.4227% ...... 99.8612% 99.'9863% 97.4265% 99.0399% 99.9690To 99.9769% \n99.'9999% ' 9~'9998~o 99.9260% 9919260~ 99',9259~ 99,9967% 99',9967% ' 99,9998% 99.6938% ..... 99.8573% \n99,8805~77 99.999~ 99,9997% 99.9997% 98.5938% 9g.6618% 99.8954% 94~8186% \" 99.9981% 99.9981% l~re block \nexecution frequency 5.0436% 5'.3173% 0.0000% 0,5773% 0.1388'% 0.0137% 2.5735% 0.9601% 0.0310% 0.023f% \n0.0001% 0.0002% 0.0740% 0.0740% 0.0741% 0.0033% 0.0033% 0.0002% ....... 0.3062% 0.1427% 0.1195% 0.0003% \n0.0003% 0.0003% 1.4062% 1.338~% 0.1046% 5.1814% 0.0019% 0.0019% Figure 9: Accuracy of rare code prediction \ntechnique in terms of dynamic execution counts, tl=2000 and t2 varies as the value in the second column. \nthen used to mark basic blocks as rare in the compiler's intermediate representation. 2 2. Perform live \nvariable analysis. Before any transformations are performed, we perform live variable analysis to determine \nthe set of live vari- ables at rare block entry points. 3. Redirect the control flow edges that targeted \nrare blocks, and remove the rare blocks. For each control flow edge from a non-raxe block to a rare block, \nwe generate a new basic block containing a single instruction that transfers control to the inter- preter. \nThis instruction uses all local variables and Java stack locations from the Java bytecode that axe live \nat that point, a We redirect the control flow edge to point to this new block, and add an edge from the \nnew block to the exit node. See Figure 10 for an ex- ample. After this process, rare blocks can be removed \nas unreachable code. 4. Perform compilation normally. All analysis, optimization, and code generation \npro- ceeds normally. Analyses treat the interpreter transfer point as an unanalyzable method call. Code \ngenera- tion treats the transfer point as a call to a glue routine, described in Section 3.5. 5. Record \na map for each interpreter transfer point. When generating the code to call the glue routine, we also \ngenerate a map that specifies the location, in reg- isters or memory, of each of the local variables \nand Java stack locations used in the original Java bytccode. This map is used by the glue routine to \nreconstruct the interpreter state. The map is stored immediately af- ter the call in the instruction \nstream. Each map is typically under 100 bytes long. Recently, server-oriented virtual machines like Jalapefio \n[9] and the MRL VM [15] have abandoned interpretation and gone instead with a compile-only approach. \nAlthough our system utilizes an interpreter, this technique can easily be used with compilation-only \nsystems. Instead of recovering interpreter state, the glue routine can use the map to recover compiled \ncode state at the rare path entry point. A mech- anism to enter compiled code at basic block entry points \nis necessary in any case to support Java run time features such as exceptions and debugging [35]. 3.2 \nCorrectness This technique relies on a few simple observations. The first observation is that performing \ncontrol-flow tail duplication does not change program semantics. Control flow tail du- plication is a \ntechnique by which control flow merges are eliminated by duplicating the code after the merge point. \nIt 2If the method entry point is marked as rare, we mark all rare blocks reachable from the root via \nother rare blocks as non-raxe. 3This may include variables from multiple Java methods, if the transfer \npoint is part of inlined code. to interpreter.., / i T EXIT EXIT Figure 10: An example of redirecting \nthe rare path. On the left, the dotted block is rare, so we redirect it to a block that calls the interpreter. \nis typically used to allow more precise data flow information after the merge point and thereby increase \noptimization op- portunities [13, 40, 1]. By removing edges from rare blocks to non-rare blocks, we are \nsimply splitting the rare path from the common path, which obviously does not change the program semantics \non the common path. The second observation is the code that we replace the rare path with --the instruction \nto transfer to the interpreter --is a conservative summary of all of the code on the rare path. Analyses \ntreat this instruction as a method call to an unanalyzable method, which takes all of the live variables \nas arguments. The body of the conceptual unanalyzable method can execute arbitrary code, which is by \ndefinition more general than the code on the rare path and is therefore a conservative summary. The third \nobservation is that reconstructing the interpreter state and continuing execution in the interpreter \nis computa- tionally equivalent to continuing execution on the rare path. Assuming that the implementations \nof the compiler and in- terpreter are correct, and the reconstructed interpreter state is equivalent \nto the state if the interpreter was executing and entered the rare path, our technique is correct. The \ninterpreter state can always be equivalently reconstructed because the live local variables and Java \nstack locations at that program point are marked as used and therefore their correct values are available; \nlocations that are not marked as live contain values that will not be used and are therefore unimportant. \n3.3 Partial dead code elimination We modified our dead code elimination algorithm to treat rare blocks \nspecially. This allows us to move computation that is only live on a rare path into the rare block, saving \ncomputation in the common case. Our dead code elimination uses an optimistic approach sim- ilar to the \none described by Muchnick [36], originally due to Kennedy [29]. That analysis begins by marking all instruc- \ntions that compute essential values, and then recursively marking all instructions that contribute to \nthe computation of essential values. Any non-essential instructions are then eliminated. Our analysis \noperates on SSA form. It first computes the essential instructions in all non-rare blocks, completely \nig- noring all rare blocks. An essential instruction computes a value that is used in a predicate, returned \nor output by the method, or has a potential side-effect. 4 It then visits each rare block to discover \ninstructions that are essential for that rare block, but not essential for non-rare blocks. If these \ninstructions are recomputable at the point of the rare block, they can be safely copied there. For each \ninstruction in the rare block, it recursively visits all instructions that contribute to the computation \nof values for that instruction. If an instruction is marked as essential, it is skipped. If it is a \u00a2 \nfunction, it depends on an earlier predicate, and is therefore (recursively) marked as essential. Otherwise, \nthe instruction is added to a set of instructions associated with the rare block. After computing sets \nfor all rare blocks, it adds each of the non-essential instructions in a set to its corresponding rare \nblock. Then, all instructions in non-rare blocks that are not marked as essential are eliminated. Now \nwe make an argument of correctness. Any instruction that was eliminated on the main path either computed \na value that was not essential anywhere, in which case it is obviously correct to eliminate it, or it \nwas only essential in some number of rare blocks, in which case it would have been copied into those \nrare blocks. Copying the instruction into a rare block is legal because, as the instruction is not a \n\u00a2 function, the instruction dominates and is in the same loop as the rare block and therefore would have \nexecuted exactly once. Also, any instruction with a potential side effect or that read from or wrote \nto memory would have been marked as essential on the main path and therefore executed in its original \nlocation. Therefore, moving the instruction to a rare block does not violate exception or memory semantics. \n3.4 Pointer and escape analysis Treating an entrance to the rare path as a method call can be an overly \nconservative assumption. This conservative- ness is normally not an issue because there are no con- trol \nflow merges from the rare path back into the common path, and therefore it has no effect on forward data \nflow. However, in the case of escape analysis [46], the conserva- tiveness prevents objects from being \nstack-allocated because they may escape into the \"method call\" on the rare path. Therefore, we modified \nour pointer and escape analysis to take advantage of rare block information. Objects can be stack-allocated \nas long as they do not escape in the common blocks; if a branch to a rare block is taken, stack-allocated \nobjects are copied to the heap and pointers to them are updated. 4Reads from the heap are treated as \nhaving a side-effect because they can trigger null pointer exceptions. However, many of the reads from \nthe heap are eliminated by a com- moning optimization pass [42, 27]. Stack / Heap stack ~ copy object \n~ ,~,,~, p J rewrite stack obj cot Figure 11: An example of copying an object from the stack to the \nheap. It copies the object and updates the pointers to point to the new location. We present a modified \nversion of the algorithm by Whaley mad Pdnard [46] that takes advantage of rare path informa- tion. This \nalgorithm is a flow-sensitive, context-sensitive, compositional pointer and escape analysis. It discovers \nob- jects that can be accessed by only one thread and eliminates the synchronizations on them. It also \ncomputes the lifetimes of objects, and when the lifetime of an object is bounded by the activation of \na method, it allocates the object in the stack frame of the method, avoiding heap allocation and garbage \ncollection costs. This algorithm works by building points-to escape graphs. A points-to escape graph \ncharacterizes how objects can refer to each other at run time. It also encodes escap e information, which \nis information about which objects can be accessed by code outside of the scope of the analysis and how \nthey can be accessed. Our implementation performs more optimizations than the algorithm originally described \nby Whaley and Rinard [46]. First, it replaces the fields of non-escaping objects and the elements of \nnon-escaping arrays by local variables whenever possible; this optimization is called scalar replacement \nof aggregates [31, 36, 10]. This transformation is possible for a given location when MI loads and stores \nthat can possibly access the location can access no other locations. In this case, the location is changed \ninto a local variable; loads and stores axe transformed into simple register move operations. Second, \nit uses the points-to information generated by the algorithm to eliminate redundant type checks and resolve \nthe target of dynamic calls. In the analysis, objects that are created at a specific creation site are \ngrouped into a sin- gle node. Because an object's creation site precisely defines its type, we can use \nthis type information along with the points-to information to eliminate redundant type checks and resolve \nthe targets of dynamic calls. The algorithm proceeds as follows. During the analysis phase, the algorithm \ntreats the interpreter transfer instruc- tion as a simple use, rather than a method call. Then, after \nperforming a stack allocation transformation, we visit each of the interpreter transfer instructions \nin each of the rare blocks. If the object to be stack-allocated is potentially pointed to by one of the \nvariables used by the interpreter transfer instruction, we insert a call before the interpreter transfer \ninstruction to a runtime routine that copies the ob- ject from its location on the stack to a newly allocated \narea on the heap. ~Ve then insert conditional update instructions for every location that can potentially \npoint to the stack- allocated object. Because our garbage collector cannot deal with references on the \nheap to stack objects, objects on the heap cannot refer to objects on the stack. Therefore, the only \nlocations that can possibly refer to a stack object are local variables and fields of other stack objects. \nThis list of locations is obtained from the points-to data at the point of the interpreter trmmfer instruction. \nSee Figure 11 for a graphic example. Scalar replacement transformations proceed similarly, but rather \nthan inserting an instruction to copy the object from the stack, we insert instructions to allocate space \non the heap and manually initialize the object using the local variables associated with the fields of \nthe object. When performing synchronization elimination on thread-local objects, we add synchronization \nenter operations on the object to every rare block in the synchronization region. A small discussion \non the legality of postponing synchro- nization enter operations until the rare block is executed is \nin order. According to our interpretat!on of the Java mem- ory model [32], a memory barrier is required \nwhen successive synchronizations occur by different threads on the same ob-ject. Because the object is \ncaptured at the entry point to the rare block, it cannot be accessed by another thread, and therefore \nthe synchronizations (and their associated mem-ory barrier operations) can legally be postponed until \nthe rare block is executed. No change is necessary for the eliminated type checks and resolved method \ninvocations; these transformations are valid even after an object escapes. 3.5 Reconstructing interpreter \nstate We use a runtime \"glue\" routine to construct a set of stack frames that represent the interpreter \nstate, and then resume execution in the interpreter. Our system uses a mixed mode interpretation strategy \nwhere the interpreter uses the same stack as the compiled code. The glue routine first gener- ates a \nsequence of interpreter stack frames corresponding to the inlining state at the rare block entry point. \nThese stack frames are initialized with their corresponding method and bytecode pointers. Then, it iterates \nthrough each location pair in the map, and copies the value at the location to its corresponding position \nin the interpreter stack frames. Finally, it branches into the interpreter, and execution con- tinues. \nRuntime support for the glue code (to support stack walking, exception handling, etc.) was already in \nplace be- cause of our support for multilevel compilation. It is worth noting that execution does not \nstay in interpreted mode forever. The interpreter continues to keep track of the number of invocations \nand backwards branches. When this threshold hits tl again, the method is recompiled once again, and we \nrecollect the profile information. Therefore, the system is able to dynamically adjust when the program \nprofile changes. 4. RESULTS We have implemented the partial method compilation tech- nique in the joeq \nvirtual machine [45], an open-source multi- platform virtual machine. However, the joeq Java compiler \ndoes not perform significant optimizations and therefore the effectiveness of our technique is limited. \nIn order to evalu- ate the effectiveness of our partial-method compilation tech- nique on a fully-optimizing \ncompiler, we also implemented the technique as an ofltine step, using instrumented class files to collect \nthe rare block information described in Section 2.2 and using that information to refactor the affected \nclasses as described in Section 3.1. Interpreter transfer points at rare block entry points are implemented \nas method calls to synthetic methods that contain the code reachable from the transfer point. We then \nrely on the virtual machine's default method-at-a-time compilation policy to avoid the compila- tion \nof the code in the synthetic method. We also perform the partial dead code elimination and pointer and \nescape analysis described in Sections 3.3 and 3.4 on the refactored classes in a separate ofltine step. \nImplementing our tech-nique as a otttine step also makes our technique virtual ma- chine independent, \nand thereby it can be used as an effective standalone optimizer for improving application startup time \non existing virtual machines. 5 We used the joeq virtual machine [45] to collect the statistics in Section \n2 and the profile data to perform the code trans- formations. We only transformed the application classes; \nfurther gains can be realized by applying the same technique to the class library and virtual machine \ninternal classes. We used threshold values of tl=2000 and t2=25000. We used the Bytecode Engineering \nLibrary [16] to output the refactored classes. The escape analysis and partial dead code elimination \npasses are based on analyses in the FLEX compiler infrastructure [2], changed to work with joeq. To test \nthe effectiveness of our technique, we used various pro- grams, including the industry-standard SPECjvm98 \nbench- mark suite [44]. SPECjvm98 benchmarks were executed in command line mode with the large input \nsize. s Our per- formance information was collected using the IBM Devel- oper Kit, Java(TM) Technology \nEdition [25] build cx130- 20010626 on a Pentium 3 600 mhz machine with 512MB RAM running RedHat Linux \n7.1. 4.1 Performance Figure 12 and the associated table in Figure 13 show the im- provement in performance \nfrom using our technique. Each 5The system will be available at press time as a standalone module at \nhttp://joeq.sourceforge.net. SThe SPECjvm98 benchmarks were not executed using the test harness and are \ntherefore not official SPEC results. 100.00% 90.00% i 80.00% 70,00% 60.~% 50.00% 40.O0% 30.00% W ; \n,' 20.f~)%- lO.OO% - J 0.00% check compress j ess db , javac mpegaud mtrt ack b'wingSet linpack JLex \nJCup Figure 12: Normalized run time performance for each of our benchmarks. The first bar for each benchmark \nis the execution time using a standard whole-method compilation technique. The second bar is the execution \ntime using our partial method compilation technique. The third bar adds partial dead code elimination \nand escape analysis. The length of each bar is the time of the first run; the bottoms light-colored part \nof each bar is the time of the best run. The difference can mostly be attributed to compile time. I II \ncheck I compress I jess [ db I javac I mpegaud I mtrt I jack I SwingSet I Linpaek [ JLex I JCup I \"Whole \nmethod 03081,09371207,9,2 0721,3,,, 1120, 1,0\u00b0471 188, I First run Best run Difference 0.146 0.021 0.125 \n,,,, 19'.422 0.976 10.394 5.543 25.088 1.631 16,084 9.888 11.852 1.692 7.378 4.669 12.247 4.300 N/A N/A \n2.750 ] 3.690 2.620 2.237 0.130 1.453 I 4.986 4.718 0:268 First run Best run Difference 0.125 0.020 0.105 \n, 18.100 13.704' 10.086 3.570 24.137 1.076 17.725 8.708 Partial method i2.097 7.242 1.594 ' 4.626 11.592 \n3.568 N/A N/A b 7101393 14 ,42,550 2.209 4:122 0.160 1.724 0.192 First run 0.100 I Partial method plus \noptimizations 2,710 ] 3.634 } 4.250 Best run Difference 0.019 0.081 18 322:1 0.512 9,674 I 23.458 16.994, \n3'580. I 0\"905:8.810 12.060 .... 1.578 6.251 2.508 11.501 3.509 N/A N/A 2.550 0.160 2.016 i.618 4.051 \n0.199 Figure 13: A comparison of whole method compilation versus partial method compilation. All times \nare in seconds.  benchmark has three bars --the first bar is the total execu- tion time using a standard \nwhole-method compilation strat- egy. Its run time is normalized to 100%. The second bar is the total \nexecution time using our partial-method compi- lation technique. The third bar adds the partial dead \ncode elimination and escape analysis optimizations from Sections 3.3 and 3.4, respectively. The height \nof each bar corresponds to the time of the first run of each benchmark. The bottom, light-colored portion \nof each bar corresponds to the time of the best run. The difference between these two times can mostly \nbe attributed to compile time. SwingSet was mea- sured by using wall-clock time, so we do not have best \ntimes for it. Although the times here do not include the processing in the offtine step, our prior experiments \nindicate that the time spent for these is negligible, and that enabling partial- method sensitivity in \nthe optimizations in Section 3 does not significantly add to their run time. As we see from the figure, \nthe effectiveness of our technique varies between the benchmarks. In most cases, it is effective in reducing \ncompile time; however, in a few cases, compile time does increase due to a rare path being taken enough \ntimes to trigger compilation. This effect is very pronounced with JLex because the run time of the benchmark \nis small, so the recompilations have a larger effect. It is also worth noting that the optimizations \nimprove compile time further, mostly due to the fact that the pointer analysis is able to resolve many \nmore call targets, reducing code size by alle- viating the need for generating extra code for virtual \ncalls. This is the cause of the significant improvement in the com- pile time of mtrt when enabling the \noptimizations. Also, in some cases it is able to reduce best run time considerably, as is the case for \ncompress and JCup. This improvement is due to improved cache locality and improved data flow in- formation \non the common paths. In mtrt and JLex, escape analysis was able to scalar replace an object in one of \nthe core routines, which improved their best run time. Overall, the partial method compilation technique \nimproved total run time by an average of 5%, and up to 15%. Enabling the optimizations improved performance \nby, on average, an- other 5%, and up to 17%. 4.2 Code size In Figure 14, we show the reduction in the \nnumber of bytes of bytecode compiled using our technique. The first row of the table contains the code \nsize of all executed methods. The second row is the code size using a method-at-a-time strat- Figure \n14: Number of bytes of bytecode compiled. The first row is the code size of all executed methods. The \nsecond row is the code size if we compile a method-at-a-time. The third row is the code size using the \npartial method compilation technique. The fourth and fifth rows show recompilation statistics from entering \nthe rare path. We used threshold values of tl----2000 and t2-----25000. check compress jess db javac \nmpegaud mtrt jack [ SwlngSet Linpack JLex JCu \"7 All executed methods 12441 4623 37983 4757 114608 56504 \n17570 53772 [ 29894 3048 37838 3395 Method-at-a-time 360 2576 [ 15114 1429 88798 20224 13178 36537 1814 \n1682 23349 2083 Partial method 175 2209 I 8467 912 50844 15222 10713 14669 1172 1744 14358 904 SizeNUmberofrccompiles\u00b0frecompiles \n00 2606 221 00 51t87 00 411950 I\u00b0 L 0__1____2_2l\u00b0l 11 1618318~1___2_j egy. The third row is the code \nsize using the partial method compilation technique, including any recompilations. The fourth row shows \nthe number of recompilations triggered by entering the rare path, and the fifth row shows the code size \nof those recompitations. We used threshold values of tl:2000 and t2:25000. Our technique is able to significantly \nreduce the number of bytes of bytecode compiled. This translates into a direct compilation time speedup \nand code size reduction. 5. RELATED WORK Our technique of partial method compilation has similarities \nto prior work. In this section, we compare and contrast our technique to related areas. Dynamo [5] is \na run-time optimizer that tries to improve performance by identifying and optimizing frequently taken \npaths (traces) through a program. Dynamo initially inter- prets the program, keeping track of execution \nfrequencies. When an execution frequency hits a certain threshold, Dy- namo records a dynamic trace of \ninstructions, called a frag-ment, and compiles it as a single-entry, multi-exit contiguous sequence. \nThis allows them to achieve sub-method and ar- bitrary cross-method granularity. Like our system, Dynamo \nuses dynamic execution counts to trigger upgrade compila- tion. However, Dynamo uses only the most recently \nexe- cuted trace to determine what to compile, and it can only operate on a single trace at a time. Our \ntechnique uses code coverage information from a number of executions, and op- erates on arbitrary code \nregions. The Self dynamic optimizing compiler used a technique of deferring compilation of uncommon cases \nto help improve compilation time by over an order of magnitude [11]. Their technique uses type information \nto defer compilation for messages sent to receiver classes that are presumed to be rare. When a rare \npath is executed, the compiler reuses the current stack frame. Our technique is similar, but handles \nrare paths through arbitrary control flow. Like us, they also found that deferring compilation of the \nrare path improved optimization opportunities on the common path. In their case, they were able to use \na simpler, faster register alloca- tor because most of the calls were on the rare path. The HotSpot server \nVM is able to avoid code generation for uncommon virtual method targets by using an \"uncommon trap\" mechanism \n[38]. Their mechanism of falling back to the interpreter is similar to our technique. Ishizaki et al. \nuse a code patching mechanism to fall back when an assumption about devirtualization is invalidated [26]. \nOgasawara et al. use a profiling technique to optimize exception handling [37]. Trace scheduling is \na technique that predicts the outcome of conditional branches and then optimizes the code 0~ssuming the \nprediction is correct [18, 33]. The compiler generates compensation code to recover the state after an \nunpredicted branch is taken. Trace scheduling suffers from the complex- ity involved in the generation \nof compensation code. Su-perblock scheduling is a technique utilized by the IMPACT compiler [24]. Superblock \nscheduling simplifies the complex- ity of compensation code generation by using tail duplica- tion to \ncreate superblocks --single-entry, multiple-exit re- gions. Like superblock scheduling, our technique \ndoes not allow control to flow from the rare path back onto the com- mon path and thereby simplifies \nthe generation of compen- sation code. However, unlike trace scheduling or superblock scheduling, our \ntechnique operates on arbitrary control-flow graph regions, not just a single trace. Furthermore, our \ntech- nique does not generate code for rare traces until they are actually executed at run time. As long \nas our predictions are correct and the rare paths are not taken, we can avoid the code growth associated \nwith tail duplication. Poletto used tail duplication to increase the amount of data flow information \nthat is available about a piece of code [40]. Like us, he found improved code quality due to better data \nflow information. Chang et al. describe a method of using edge profile information to assist classical \noptimizations [12]. They use edge profile information to discover frequently- executed paths and form \nsuperblocks from those paths, us- ing tail duplication to avoid joins. Our technique achieves a similar \neffect. Ammons and Larus describe a technique of identifying and duplicating hot paths in order to improve \nthe precision of data flow analysis along those paths [1]. The hot paths are constructed using an acyclic \npath profile [7]. Their technique uses qualified data flow analysis [22], which cou- ples a conventional \ndata flow problem with a deterministic finite automaton, to recognize hot paths. To avoid unnec- essary \ncode growth, the duplications that ended up to be unprofitable are removed. Our technique uses node profil- \ning rather than path profiling, and therefore does not dis- tinguish between different paths leading \nto the same point; extending our technique to use path information would be straightforward. There are \nsome techniques that use hardware extensions to allow an optimizer to ignore exceptional control flow \n[13, 41]. Delayed exceptions [17] and sentinel scheduling [34] are hardware extensions that allow speculative \nexecution of excepting instructions. They allow an instruction scheduler to schedule as if exceptions \ndid not exist. They add check and fix up code to deal with the case if the trap actually occurs. We achieve \nthis in software by limiting optimization by making more conservative assumptions about exceptional control \nflow. Gupta et al. describe a technique for performing optimiza- tions in Java while ignoring dependence \nconstraints among potentially excepting instructions [19]. They first identify the subset of program \nstate that needs to be preserved in the case that an exception is thrown. They then remove depen- dencies \nbetween exceptions; when exceptions are reordered, compensation code is inserted to catch the exception \nand throw the correct one. This technique is complementary to ours; modeling rare code as a method call \nmakes it a potentially excepting instruction, and so we could use this technique to remove dependencies \nbetween exceptions and the rare code entry point. Bruening et al. explored the issue of finding optimal \ncom- pilation unit shapes in an embedded Java JIT compiler [8]. They also pointed out that method boundaries \nare a poor choice for compilation. They suggest compiling only the hot traces or loops within a method. \nBecause they did not have a working JIT compiler, they provided estimates of the code size benefits by \nusing trace-based and loop-based compila- tion units. Like us, they found that a majority of the code \nin methods is rarely or never executed, and that they could drastically reduce code size with only a \nnegligible change in performance. Our work improves on their work by providing the details of an actual \nimplementation -- how to determine rare code, how to compile partial methods, how to fall back to interpretation, \nperformance results, etc. Hank et al. also claim that functions are not good compilation units [21] and \nthey propose partitioning the program into disjoint regions that are analyzed and optimized seperately. \nOur partial dead code elimination optimization is similar to other profile-sensitive code motion optimizations. \nHor-spool and Ho present a version of partial redundancy elim- ination based on a cost-benefit analysis \nfrom edge profile information [23]. There has also been work on performing optimizations while making \nuse of path profile information. Gupta et al. describe methods of using path profile informa- tion in \na cost-benefit data flow analysis to perform partial redundancy elimination and partial dead code elimination \nand guiding speculation and predication decisions [30, 20]. The main difference between our algorithm \nand earlier algo- rithms is that because our algorithm is a sparse analysis and only makes a distinction \nbetween rare and non-rare blocks, it can avoid performing cost-benefit analysis, and therefore it has \nnearly the same run time as normal dead code elimi- nation. There has been some research devoted to using \nprofile data to guide code placement strategies to improve instruction cache locality [39]. Our technique \ncan be thought of as a very extreme profile-directed code placement strategy --rare code is not only \nplaced away from common code, it is not even generated at all. Chen and Leupen describe a sys- tem of \njust-in-time code layout that dynamically loads and lays out procedures as they are called [14]; this \nis similar to our method, but we support this at the basic block level, and we also support deferring \ncompilation. 6, CONCLUSION In this paper, we presented a new technique for performing partial method \ncompilation. This technique allows most op- timizations to completely ignore the rare paths. This allows \nthem to fully optimize for the common case. Furthermore, this technique allows us to avoid generating \ncode for the rare path until it is actually executed. Because most rare paths are never executed, this \ndecreases total code size and total compile time dramatically. It Mso improves the effectiveness of analyses, \nespecially pointer and escape analysis. Our experimental results are encouraging. With our bench- mark \nsuite, compile times were reduced drastically, and over- all first run times improved by an average of \n100/0, and up to 32%. The trend in compiler research has often been towards more and more powerful analyses, \nwithout regard for compilation time. This paper attempted to explore an oft-neglected area of compiler \nresearch --how to improve compilation time without giving up too much performance. With the advent of \nJava and corresponding surge in popularity of Just-In- Time compilers, we expect the interest in this \narea to grow. Acknowledgements The idea for ignoring the rare path during escape analysis and then copying \nthe object to the heap if the rare path was taken is originally due to Akira Koseki of IBM Tokyo Re- \nsearch Laboratory. Part of this work was performed while I was a Student Fellow at IBM Tokyo Research \nLaboratory under the guidance of Toshio Nakatani, who was indispens- able in supporting and promoting \nmy research and providing a stimulating research environment. He, Toshio Suganuma, and Tamiya Onodera \nof IBM TRL Mso provided valuable input and comments on this paper. The research was also funded in part \nby an NSF Graduate Fellowship. I would also like to acknowledge all of the people who have worked on \nthe IBM Java virtuM machine and Just-In-Time compiler for providing the initial infrastructure in which \nthis research was performed, and the developers of BCEL, FLEX, and the other tools that were used to \ncollect the data contained in this paper. Finally, I would like to thank Monica Lam for her comments \nand assistance and the anonymous reviewers for their exceptionally helpful comments. 7. REFERENCES [1] \nG. Ammons and J. R. Larus. Improving data-flow analysis with path profiles. In Proceedings of the ACM \nSIGPLAN'98 Conference on Programming Language Design and Implementation (PLDI), pages 72-84, Montreal, \nCanada, June 17-19, 1998. [2] C. S. Ananian. FLEX compiler infrastructure. http://www.flex-compiler.lcs.mit \n.edu, 2001.  [3] M. Arnold, S. Fink, D. Grove, M. Hind, and P. F. Sweeney. Adaptive optimization in \nthe Jalapefio JVM. ACM SIGPLAN Notices, 35(10):47-65, Oct. 2000. [4] M. Arnold, S. Fink, V. Sarkar, and \nP. F. Sweeney. A comparative study of static and dynamic heuristics for inlining. In Dynamo '00 workshop, \nHeld in conjunction with POPL '00: The 27th ACM SIGPLAN-SIGACT Symposium on Principles of Programming \nLanguages, 2000. [5] V. Bala, E. Duesterwald, and S. Banerjia. Dynamo: a transparent dynamic optimization \nsystem. ACM SIGPLAN Notices, 35(5):1-12, May 2000. [6] T. Ball and J. R. Larus. Optimally profiling and \ntracing programs. In Conference record of the Nineteenth Annual ACM SIGPLAN-SIGACT Symposium on Principles \nof Programming Languages: papers presented at the symposium, Albuquerque, New Mexico, January I9-22, \n1992, pages 59-70, New York, NY, USA; 1992. ACM Press. [7] T. Ball and J. R. Larus. Efficient path profiling. \nIn Proceedings of the 29th Annual International Symposium on Microarchitecture, pages 46-57, Paris, France, \nDec. 2-4, 1996. IEEE Computer Society TC-MICRO and ACM SIGMICRO. [8] D. Bruening and E. Duesterwald. \nExploring optimal compilation unit shapes for an embedded just-in-time compiler. In Proceedings of the \n2000 A CM Workshop on Feedback-Directed and Dynamic Optimization FDDO-3, Dec. 2000. [9] M. G. Burke, \nJ. D. Choi, S. Fink, D. Grove, M. Hind, V. Sarkar, M. J. Serrano, V. C. Sreedhar, H. Srinivasan, and \nJ. Whaley. The Jalapefio dynamic optimizing compiler for Java. In Proceedings of the ACM SIGPLAN '99 \nJava Grande Conference, June 12-14, 1999. [10] S. Carr and K. Kennedy. Scalar replacement in the presence \nof conditional control flow. Software: Practice And Experience, 24(1):51-78, Jan. 1994. [11] C. Chambers \nand D. Ungar. Making pure object-oriented languages practical. In N. Meyrowitz, editor, Proceedings of \nthe Conference on Object-Oriented Programming Systems, Languages, and Applications (OOPSLA), volume 26, \npages 1-15, New York, NY, 1991. ACM Press. [12] P. P. Chang, D. M. Lavery, and W. M. Hwu. The effect \nof code expanding optimizations of instruction cache design. Technical Report CRHC-91-18, Coordinated \nScience Lab, University of Illinois, Jan. 1992. [13] P. P. Chang, S. A. Mahlke, W. Y. Chen, N. J. Wafter, \nand W. W. Hwu. IMPACT: An architectural framework for multiple instruction issue processors. In Proceedings \nof the 18th International Symposium on Computer Architecture (ISCA), volume 19, pages 266-275, New York, \nNY, 1991. ACM Press. [14] J. B. Chen and B. D. D. Leupen. Improving instruction locality with just-in-time \ncode layout. In USENIX, editor, The USENIX Windows NT Workshop 1997, August 11-13, 1997. Seattle, Washington, \npages 25-32, Berkeley, CA, USA, Aug. 1997. USENIX. [15] M. Cierniak, G.-Y. Lueh, and J. M. Stichnoth. \nPracticing JUDO: J~va under dynamic optimizations. In SIGPLAN Conference on Programming Language Design \nand Implementation, pages 13-26, 2000. [16] M. Dahm. Byte code engineering library. http://bcel.sourceforge.net, \n2000. [17] M. A. Ertl and A. Krall. Delayed exceptions -- speculative execution of trapping instructions. \nLecture Notes in Computer Science, 786:158-171, 1994. [18] J. A. Fisher. Trace scheduling : A technique \nfor global microcode compaction. IEEE Transactions on Computers, C-30(7):478-490, 1981. [19] M. Gupta, \nJ. D. Choi, and M. Hind. Optimizing java programs in the presence of exceptions. In l$th European Conference \non Object-Oriented Programming (ECOOP 2000), June 12-16, 2000. [20] R. Gupta, D. A. Berson, and J. Z. \nFang. Resource-sensitive profile-directed data flow analysis for code optimization. In Proceedings of \nthe 30th Annual IEEE/A CM International Symposium on Microarchitecture (MICRO-97), pages 358-368, Los \nAlamitos, Dec. 1-3 1997. IEEE Computer Society. [21] R. Hank, W. Hwu, and B. Ran. Region-based compilation: \nAn introduction and motivation. In Proceedings of the 28th Annual A CM/IEEE International Symposium on \nMicroarchitecture, pages 158-168, Ann Arbor, MI, Nov. 1995. [22] L. H. Holley and B. K. Rosen. Qualified \ndata flow problems. IEEE Transactions on Software Engineering, 7(1):60-78, Jan. 1981. [23] R. Horspool \nand H. Ho. Partial redundancy elimination driven by a cost benefit analysis. In 8th Israeli Conference \non Computer Systems and Software Engineering, pages 111-118, June 1997. [24] W. M. Hwu, S. A. Mahlke, \nW. Y. Chen, P. P. Chang, N. J. Wafter, R. A. Bringmann, R. G. Ouellette, R. E. Hank, T. Kiyohara, G. \nE. Haab, J. G. Holm, and D. M. Lavery. The superblock: An effective technique for VLIW and superscalar \ncompilation. The Journal of Supercomputing, 7(1-2):229-248, May 1993.  [25] International Business \nMachines. IBM Developer Kit, Java(tm) Technology Edition, 2001. http: //www.ibm.com/j ava/jdk/index.html. \n[26] K. Ishizaki, M. Kawahito, T. Yasue, H. Komatsu, and T. Nakatani. A study of devirtualization techniques \nfor a Java Just-In-Time compiler. In Proceedings of the A CM SIGPLAN Conference on Object.Oriented Programming \nSystems, Languages and Applications (OOPSLA '00), Oct. 2000. [27] K. Ishizaki, M. Kawahito, T. Yasue, \nM. Takeuchi, T. Ogasawara, T. Suganuma, T. Onodera, H. Komatsu, and T. Nakatani. Design, implementation, \nand evaluation of optimizations in a just-in-time compiler. In Proceedings of the ACM SIGPLAN '99 Java \nGrande ConfeT~ence, pages 119-128, June 12-14, 1999. [28] M. Kawahito, H. Komatsu, and T. Nakatani. \nEffective null pointer check elimination utilizing hardware traps. In Proceedings of the 9th International \nConference on Architectural Support on Programming Languages and Operating Systems (ASPLOS-IX), Nov. \n2000. [29] K. Kennedy. A Survey of data flow analysis techniques. In S. S. Muchnick and N. D. Jones, \neditors, Program Flow Analysis: Theory and Applications, chapter 1, pages 5-54. Prentice-Hall, 1981. \n [30] J. Knoop, O. Riithing, and B. Steffen. Partial dead code elimination. In Proceedings of the Conference \non Programming Language Design and Implementation, pages 147-158, New York, NY, USA, June 1994. ACM Press. \n [31] D. A. Kranz, R. Kelsey, J. A. Rees, P. Hudak, J. Philbin, and N. I. Adams. Orbit: An optimising \ncompiler for scheme. In Proceedings of the SIGPLAN '86 Symposium on Compiler Construction, pages 219-233. \nACM, June 1986. [32] T. Lindholm and F. Yellin. The Java Virtual Machine Specification. Addison-Wesley, \nReading, MA, USA, second edition, 1999. [33] P. G. Lowney, S. M. Freudenberger, T. J. Karzes, W. D. Lichtenstein, \nR.. P. Nix, J. S. O'Donnell, and J. C. Ruttenberg. The Multifiow Trace Scheduling compiler. The Journal \nof Supercomputing, 7(1-2):51-142, 1993. [34] S. A. Malflke, W. Y. Chert, R. A. Bringmann, R. E. Hank, \nW.-M. W. Hwu, B. R.. Rau, and M. S. Schlansker. Sentinel scheduling: A model for compiler-controlled \nspeculative execution. ACM Transactions on Computer Systems, 11(4):376-408, 1993. [35] S. Microsystems. \nJava virtual machine debug interface reference. http: / /java.sun.com /'products/ jdk/1.3 / docs/ guide/ \njpda/jvmdi-spec.html. [361 S. Muchnick. Advanced Compiler Design and Implementation~ Morgan Kaufmann, \nSan Francisco, CA, 1997. [37] T. Ogasawara, H. Komatsu, and T. Nakatani. A study of exception handling \nand its dynamic optimization in java. In Proceedings of the ACM SIGPLAN Conference on Object-Oriented \nProgramming Systems, Languages and Applications (OOPSLA '01), Oct. 2001. [38] M. Paleczny, C. Vick, and \nC. Click. The Java HotSpot(TM) server compiler. In USENIX Java Virtual Machine Research and Technology \nSymposium (JVM'01), 2001. [39] K. Pettis and R. C. Hansen. Profile guided code positioning. SIGPLAN Notices, \n25(6):16-27, June 1990. Proceedings of the ACM SIGPLAN '90 Conference on Programming Language Design \nand Implementation. [40] M. Poletto. Path splitting: a technique for improving data flow analysis. Master's \nthesis, Massachusetts Institute of Technology, May 1995. [41] M. D. Smith, M. Lain, and M. A. Horowitz. \nBoosting beyond static scheduling in a superscalar processor. In Proceedings of the 17th Annual Symposium \non Computer Architecture, pages 344-354, 1990. [42] T. Suganuma, T. Ogasawara, M. Takeuchi, T. Yasue, \n M. Kawahito, K. Ishizaki, H. Komatsu, and T. Nakatani. Overview of the IBM Java Just-in-time compiler. \nIBM Systems Journal, Java Performance Issue, 39(1), 2000.  [43] T. Suganuma, T. Yasue, M. Kawahito, \nH. Komatsu, and T. Nakatani. A dynamic optimization framework for java just-in-time compiler. In Proceedings \nof the A CM SIGPLAN Conference on Object-Oriented Programming Systems, Languages and Applications (OOPSLA \n'01), Oct. 2001. [44] The Standard Performance Evaluation Corporation. SPEC JVM98 Benchmarks, 1998. http://www.spec.org/osg/jvm98/. \n[45l J. Whaley. joeq virtual machine. http://joeq.sourceforge.net, 2001. [46] J. Whaley and M. Rinard. \nCompositional pointer and escape analysis for Java programs. In Object Oriented Programing: Systems, \nLanguages, and Applications (OOPSLA'99), Denver, CO, 2-5 Nov. 1999. 179   \n\t\t\t", "proc_id": "504282", "abstract": "The traditional tradeoff when performing dynamic compilation is that of fast compilation time versus fast code performance. Most dynamic compilation systems for Java perform selective compilation and/or optimization at a method granularity. This is the not the optimal granularity level. However, compiling at a sub-method granularity is thought to be too complicated to be practical. This paper describes a straightforward technique for performing compilation and optimizations at a finer, sub-method granularity. We utilize dynamic profile data to determine intra-method code regions that are rarely or never executed, and compile and optimize the code without those regions. If a branch that was predicted to be rare is actually taken at run time, we fall back to the interpreter or dynamically compile another version of the code. By avoiding compiling and optimizing code that is rarely executed, we are able to decrease compile time significantly, with little to no degradation in performance. Futhermore, ignoring rarely-executed code can open up more optimization opportunities on the common paths. We present two optimizations---partial dead code elimination and rare-path-sensitive pointer and escape analysis---that take advantage of rare path information. Using these optimizations, our technique is able to improve performance beyond the compile time improvements", "authors": [{"name": "John Whaley", "author_profile_id": "81100268439", "affiliation": "Computer Systems Laboratory, Stanford University, Stanford, CA", "person_id": "PP37025809", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/504282.504295", "year": "2001", "article_id": "504295", "conference": "OOPSLA", "title": "Partial method compilation using dynamic profile information", "url": "http://dl.acm.org/citation.cfm?id=504295"}