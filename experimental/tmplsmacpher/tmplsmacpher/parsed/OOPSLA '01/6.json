{"article_publication_date": "10-01-2001", "fulltext": "\n A Study of Exception Handling and Its Dynamic Optimization in Java Takeshi Ogasawara Hideaki Komatsu \nToshio Nakatani takeshi@jp.ibm.com komatsu@jp.ibm.com nakatani@jp.ibm.com Tokyo Research Laboratory, \nIBM Japan 1623-14 Shimotsuruma, Yamato-shi, Kanagawa, Japan 242-8502  ABSTRACT Optimizing exception \nhandling is critical for programs that fre\u00adquently throw exceptions. We observed that there are many \nsuch exception-intensive programs in various categories of Java programs. There are two commonly used \nexception handling techniques, stack unwinding and stack cutting. Stack unwinding optimizes the nor\u00admal \npath, while stack cutting optimizes the exception handling path. However, there has been no single exception \nhandling technique to optimize both paths. We propose a new technique called Exception-Directed Optimiza\u00adtion \n(Edo), which optimizes exception-intensive programs with\u00adout slowing down exception-minimal programs. \nEdo, a feedback\u00addirected dynamic optimization, consists of three steps, exception path pro.ling, exception \npath inlining, and throw elimination. Ex\u00adception path pro.ling attempts to detect hot exception paths. \nEx\u00adception path inlining compiles the catching method in a hot excep\u00adtion path, inlining the rest of \nmethods in the path. Throw elimina\u00adtion replaces a throw with the explicit control .ow to the corre\u00adsponding \ncatch. We implemented Edo in IBM s production Just\u00adin-Time compiler, and obtained the experimental results, \nwhich show that, in SPECjvm98, it improved performance of exception\u00adintensive programs by up to 18.3% \nwithout a.ecting performance of exception-minimal programs at all. Categories and Subject Descriptors \nD.3 [Software]: Programming Languages; D.3.4 [Programming Languages]: Processors incremental compilers, \noptimization, run\u00adtime environment General Terms Performance, Experimentation, Languages  Keywords \nFeedback-directed dynamic optimization, dynamic compilers, ex\u00adception handling, inlining Permission to \nmake digital or hard copies of part or all of this work or personal or classroom use is granted without \nfee provided that copies are not made or distributed for profit or commercial advantage and that copies \nbear this notice and the full citation on the first page. To copy otherwise, to republish, to post on \nservers, or to redistribute to lists, requires prior specific permission and/or a fee. OOPSLA 01 Tampa \nFlorida USA Copyright ACM 2001 1-58113-335-9/01/10 $5.00 1. INTRODUCTION Language-supported exception \nhandling mechanisms, such as in Ada [10], Modula-3 [13] and C++ [43], allows a programmer to write exception \nhandlers for a region of the program. Java [29] is one of the modern languages that support exception \nhandling. Us\u00ading exceptions to change the control .ows of the program is popu\u00adlar in Java [54, 56], and \nmore and more Java programs tend to use exceptions. There are two commonly used techniques to implement \nexception handling [53, 55]: stack unwinding and stack cutting. When an exception is thrown, stack unwinding \nis to unwind the stack frames to search for the corresponding exception handler. Here there is no penalty \nin the normal path, while there is a substantial overhead in the exception handling path to .nd the right \nexception handler by unwinding stack frames one by one 1. On the other hand, stack cutting is to search \nfor the list of the registered exception handlers. Here there is some overhead to register and deregister \nexception handlers in the normal path, while there is a smaller overhead in the exception handling path \nto .nd the right exception handler owing to the registered list. Earlier research focused on optimizing \nthe normal path by sac\u00adri.cing the performance of the exception handling path, assum\u00ading that exceptions \nare rarely thrown in programs written in ma\u00adjor programming languages [49, 34, 25]. In Java, however, \nthere are many programs that frequently throw exceptions. For example, the SPECjvm98 benchmark suite \nhas two exception-intensive pro\u00adgrams, 228 jack and 213 javac, out of seven programs [19, 46]. Other \n.ve programs, such as 227 mtrt, 202 jess, 201 compress, 209 db, and 222 mpegaudio, throw much fewer exceptions. \nAs shown in Section 4.4, the performance of 228 jack becomes 8.40% slower while that of 213 javac becomes \n21.7% faster, when our Java Just-in-Time (JIT) compiler [57] switches the exception han\u00addling mechanism \nfrom stack cutting to stack unwinding. There has been no single exception handling mechanism to opti\u00admize \nboth the normal path and the exception handling path. In this paper, we propose a new optimization technique, \ncalled Exception-Directed Optimization (Edo), based on the feedback directed dy\u00adnamic optimizations. \nEdo consists of three steps: 1) exception path pro.ling, 2) exception path inlining, and 3) throw elimina\u00adtion. \nWhen an exception is thrown and caught, exception path 1Here, the normal path is the code that is executed \nwhen no excep\u00adtions are thrown, while the exception handling path is the code that is executed only when \nan exception is thrown. pro.ling records into a repository the exception path, which in\u00adcludes all the \nmethods from the thrower to the catcher, and incre\u00adments the counter associated with the path. Exception \npath inlin\u00ading searches the repository for hot exception paths. For each hot exception path, it inlines \ninto the catcher the rest of the methods. Finally, given a pair of throw and catch, throw elimination \nreplaces the throw with the explicit control .ow to the catch. Our new ap\u00adproach does not incur any penalty \nin the normal path, while it opti\u00admizes the frequently-executed exception handling paths by aggres\u00adsive \nmethod inlining. It is applicable to any Java virtual machine (VM) regardless of whether it uses both \nan interpreter and a com\u00adpiler [41, 59] or has only a compiler [7, 19]. With an interpreter, the .rst \ncompilation can exploit the pro.ling information to com\u00adpile the target method. In this paper, we collected \ninteresting runtime statistics; such as the exception counts and the lengths of the top three hot excep\u00adtion \npaths, for various categories of programs. We show that there are two types of programs, exception-intensive \nones and exception\u00adminimal ones, and that the majority of exceptions in most of the exception-intensive \nprograms, except for 228 jack, are handled by a few hot exception handling paths of four or fewer invocations. \nThe top three hot exception handling paths of 228 jack throw only 19% of the total exceptions. We implemented \nthree techniques, Edo, stack unwinding, and stack cutting, in our Java JIT compiler. Our preliminary \nexperiment us\u00ading SPECjvm98 shows that Edo improves the best performance of 228 jack by 18.3% (8.40%), \njavac by 13.8% (38.4%), and the ge\u00adometric mean by 4.37% (6.91%), over stack unwinding (stack cut\u00adting). \nIt also shows that Edo reduced the number of exceptions to occur at runtime for jack by 83.2% and javac \nby 100%. We also show that our results are better than those with Sun s HotSpot VMs [59] for the majority \nof the benchmark programs. This paper makes the following contributions: A new approach, called Exception-Directed \nOptimization (Edo), for optimizing exception-intensive programs.  A collection of runtime statistics, \nsuch as exception counts and the lengths of the top three hot exception handling paths, for various categories \nof programs.  Evaluation of our new approach against two common tech\u00adniques, stack cutting and stack \nunwinding, for the perfor\u00admance of SPECjvm98.  The rest of this paper is organized as follows. In Section \n2, we explain the exception model of Java and discuss the overhead in the normal path and the exception \nhandling path for stack unwind\u00ading and stack cutting. In Section 3, we introduce our new tech\u00adnique, \nEdo, for optimizing exception-intensive programs. In Sec\u00adtion 4, we present a collection of statistics \nabout exception behav\u00adiors for various categories of programs, and show performance re\u00adsults for SPECjvm98 \nin order to compare three approaches. We ran SPECjvm98 with three versions of IBM s production JIT compiler, \nwhich implement Edo, stack unwinding, and stack cutting, respec\u00adtively. In Section 5, we discuss the \nrelated work. Finally, in Section 6, we give our conclusions.  2. RUNTIME OVERHEAD OF EXCEPTION HANDLING \nTwo common approaches to exception handling are stack unwind\u00ading and stack cutting. Stack unwinding is \nnot as e.cient as stack cutting in the exception handling path, while the former approach is more e.cient \nin the normal path than the latter. The ine.ciency of stack unwinding is due to the overhead for unwinding \nthe stack itself. In this section, we present the exception-handling model of Java, followed by the detailed \ndescriptions of two approaches. 2.1 Exception handling in Java In Java, an exception is an object from \na subclass of the Throwable class. Programmers can de.ne their exceptions and exception han\u00addlers by \ncreating subclasses of the Throwable class. An exception is thrown with a throw statement. In a Java \nprogram, a try state\u00adment speci.es a try block, which is a program region associated with one or more \nexception handlers. A catch statement speci\u00ad.es an exception handler and associates an exception class \nand its subclasses with the exception handler. These exception classes as\u00adsociated with an exception \nhandler restrict exceptions that the han\u00addler can catch. An exception handler can catch only the exceptions \nthat are instances of these exception classes. When an exception is thrown within a try block, if the \nexception cannot be caught by any exception handlers that are associated with the try block, an outer \ntry block of the current try block is examined. The outer try block may exist within the same method \nor in a caller method. This search of try blocks continues until the handler that can catch the exception \nis found. A complication arises in Java s exception handling because of syn\u00adchronized methods. Before \nthe handler found is executed, if the stack frames for synchronized methods are discarded, the excep\u00adtion \nhandling mechanism must release the locks acquired by these methods [30]. Figure 1 illustrates an example \nof a Java program, where an exception, whose class is Exc, is thrown at method work and is caught by \nthe handler of method main, which calls work. 2.2 Stack unwinding Searching for the handler is performed \nduring the execution of pro\u00adgrams when an exception is thrown. There are three key steps in handling \nan exception: 1) mapping a program context, typically described by a program counter (PC), through which \nan exception is thrown to the corresponding try block, 2) .ltering the exception, and 3) searching for \nthe try blocks of caller methods. Figure 2 describes an abstract procedure of searching for the handler \nin the stack unwinding approach [53]. First, we explain each step in stack unwinding and then compare \nstack unwinding with stack cutting. When an exception is thrown through a PC, we .rst examine which try \nblock covers the program range including that PC. For the table\u00addriven exception handling [55, 20], which \nis commonly used to im\u00adplement exception handling, as in the Java VM [48], the compiler constructs the \ntable that maps a range of PCs to a try block. With the table, the PC is searched for and is mapped to \na try block. If a try block is found, we then examine whether the handler can catch the exception by \ncomparing the class of the handler with that of the exception. We need this exception .ltering because \nin Java a han\u00addler can only catch exceptions whose classes are subclass of the handler s class. A try \nblock can have more than one handler, so 1: class Exc extends Throwable { 2: static void work() throws \nExc { 3: try { 4: try { 5: throw new Exc(); 6: } 7: catch (NullPointerException e) { 8: /* some action \n*/ 9: } 10: } 11: catch (ArrayIndexOutOfBoundsException e) { 12: /* some action */ 13: } 14: } 15: public \nstatic void main(String arg[]) { 16: try { 17: work(); 18: } 19: catch (Throwable e) { 20: /* some action \n*/ 21: } 22: } 23: } Figure 1: An example of a Java program with try blocks begin Searching for the handler \nException = the current exception Frame = the frame where Exception occurs Pc = the program counter where \nException occurs Handler = the default handler do while TryBlk = Search for a try covering Pc /*maping*/ \nfor each HandlerCandidate of TryBlk /* .ltering class begin */ if HandlerCandidate catches Exception \nthen Handler = HandlerCandidate goto Done endif /* .ltering class end */ endfor endwhile /* searching \nfor a try block (unwinding) begin */ restore the context, such as PC and callee-saved registers release \nobjects that are locked but not released in Frame Frame = the pointer to Frame s previous frame Pc = \nthe last PC of Frame /* searching for a try block (unwinding) end */ while Frame is not empty Done: release \nlocked objects to be released before Handler transfer the control to Handler end Figure 2: Searching \nfor the handler in the stack unwinding ap\u00adproach that the exception .ltering is performed for each handler \nof the try block. For example, in Figure 1, the exception at line 5 is not caught by any handlers in \nthe method work(), but it is caught by the han\u00addler in method main because the exception s class Exc \nis a subclass of class Throwable but not of class NullPointerException or ArrayIndexOutOfBoundsException. \nIf the exception handler that can catch the exception is not found in the current method, we unwind the \ncurrent stack frame and continue the above procedure in the caller method. Stack unwinding includes restoring \nthe con\u00adtext of the caller s frame, such as the stack pointer and callee-saved non-volatile registers \n(NVRs), and releasing any unreleased locks on the objects that have been locked during the execution \nof the current frame. Figure 3 shows an example of the call stack for the example of Figure 1. We use \nthis example to show why unwinding the stack costs much more than a normal method return, particularly \nif the housekeeping data has been removed. Method work is called by method main in the last two frames. \nAs shown in Figure 1, work has two try blocks and main has one try block. Let us assume that the runtime \nlibrary unwinds the stack. When an exception is thrown at PC1 in the top frame, the control is transferred \nto the code for handling the exception. If stack unwinding is implemented by compiler-generated code, \nthe code is placed as the dummy epilog corresponding to the innermost try block of work. Otherwise, if \nit is implemented by the runtime library code, the code searches for the handler information for PC1 \nin the code database, which maps each PC of the compiled code to its corresponding code de\u00adscriptor. \nIn both cases, the code performs the exception .ltering, but it cannot .nd any handler that can catch \nthe exception. As a result, it starts to unwind the current frame of work. It restores all callee-saved \nregisters, but this example does not release any locked objects because no object is locked in work. \nAs the .nal step, the code searches for the code descriptor of PC2, which is restored as the program \ncounter of the call site. If the code descriptor is found, it provides the handler information or the \ndummy epilog. Otherwise, main has not been compiled, so that the code performs unwinding the stack and \nsearching for the handler like as the inter\u00adpreter does. In both cases, the code eventually .nds the \nhandler.  2.3 Stack cutting Stack cutting [53] generates the housekeeping code in the normal path in \norder to improve performance of the exception handling path. The housekeeping code maintains a try list \nand a lock list. When the execution enters into a try block, the code pushes the try block into the try \nlist. When the execution exits from the try block, the code pops the try block from the try list. Likewise, \nwhen calling a synchronized method, the code pushes the corresponding lock in the lock list. When returning \nfrom the synchronized method, the code pops the lock from the lock list. An indirect overhead in the \nnormal path is saving non-volatile registers (NVRs) before calling a method in a try block, since the \ncallee-save convention cannot be used for such NVRs in stack cutting [12] In handling an exception, stack \ncutting does not have to do anything in the mapping step, while it simply scans the try list in the search\u00ading \nstep. Thus, stack cutting allows faster exception handling than stack unwinding. Stack cutting is typically \nimplemented with setjmp [12, 22, 20], exception handler registration [38], or structured exception han\u00ad \n try normal path execution non\u00advolatile registers (NVRs) sync try search exception handling path execution \nPC\u00adto\u00adtry map class .lter try search NVR restore locked objects release Unwinding no callee-save no stack \nyes yes stack yes frame Cutting yes (try list) caller-save yes (lock list) try list no yes try list no \nlock list Table 1: Exception handling techniques and associated overhead Figure 3: An example of optimized \nframes and unwinding them during exception handling. dling (SEH) [50]. Table 1 summarizes di.erences \nbetween stack unwinding and stack cutting.  3. EXCEPTION-DIRECTED OPTIMIZATION In this section, we \nexplain our technique called Exception-Directed Optimization (Edo) to optimize exception-intensive programs. \nAl\u00adthough our approach is applicable to stack cutting as well as stack unwinding, we use stack unwinding \nas a basis in this paper because our interest is not only to optimize exception-intensive programs but \nalso to avoid extra overhead in the normal path. Our approach consists of the runtime part and the compiler \npart. We .rst show the overview of our JIT compiler in Section 3.1, and then explain the runtime part \nin Section 3.2 and the compiler part in Sections 3.3 and 3.4. 3.1 JIT compiler overview Our JIT compiler \nselectively compiles methods that are frequently invoked or have hot loops [57]. Methods are .rst executed \nby the interpreter. When the invocation count of a method reaches a threshold, the method is compiled \nby the .rst level compilation [58], and the compiled code is executed. Here the .rst level com\u00adpilation \nquickly generates code by restricting time-consuming op\u00adtimizations, such as method inlining. When the \ninterpreter predicts that a loop in a method will be hot, the interpreter invokes the com\u00adpiler and transfers \nto the compiled code. There are multiple pro.ling systems in the JIT compiler, such as the sampling-based \npro.ler [61] and the instrumenting pro.ler [58]. They accumulate information of which method should be \ncom\u00adpiled or recompiled during the program execution. If a pro.ler detects that a method that was already \ncompiled should be further optimized, it requests recompilation of the method to the compile thread. \nWhen the sampling-based pro.ler identi.es a hot method, the JIT compiler aggressively inlines methods \nincluding virtual methods using class hierarchy analysis [41] along with other time-consuming optimizations \n[58]. Inlining methods is very important to improve the performance of Java programs. For example, the \nJIT compiler improves the best SPEC ratio of 228 jack by 11.3% and that of 213 javac by 6.47% even without \nEdo. The code expansion caused by inlining methods is controlled by heuristics about the depth of method \ninvocations and the increase from the original method size. 3.2 Pro.ling exception paths To optimize \nexception-intensive programs, we collect information on the behavior of each exception handling and save \nit in the pro.le repository as a pro.le. There are three values in each pro.le: ex\u00adception path, the \nstart time, which is the time when the pro.le was created, and the counter, which is the number of occurrences \nof the same exception path. An exception path is the stack trace from the method that throws an exception \nthrough a method that catches the exception and is denoted by ({m0, pc0}, .., {mn-2, pcn-2}, {mn-1,f}). \nmi denotes the i-th method from the root method on the exception path. mi invokes mi+1. pci denotes the \nprogram counter of the call site where mi invokes mi+1. Exceptions are thrown at some program points \nwithin mn-1 and are caught by some handlers within m0. A pro.le is allocated or updated as follows. During \nexception han\u00addling, the runtime library unwinds the stack and it also saves the program counter for \neach stack frame. In an environment using selective compilation, the program counter can point to bytecode \nor to compiled code. After the runtime library .nds the handler, it searches the pro.le repository for \nthe pro.le whose exception path is the same as the current exception path. This search should be as quick \nas possible so that the pro.ling does not add an extra penalty to the cost of exception handling. For \ninstance, the pro.le repos\u00aditory can be implemented by using a hash table using as keys the pairs consisting \nof the PC of a program address throwing an ex\u00adception and the PC of the corresponding handler. If the \npro.le is not found, a pro.le is created in the pro.le repository. The current exception path and the \ncurrent time are saved as the exception path and the start time of the pro.le, respectively, and the \ncounter of the pro.le is zeroed. If there is already a pro.le whose exception path is the same as the \ncurrent exception path, the counter of the pro.le is incremented. The runtime library also checks if \nthe pro.le has not been passed to the dynamic compilation system as a request for dynamic compilation. \nThis check avoids not only redundant pro\u00ad.ling but also redundant requests for dynamic compilation because \nthere can be a time lag between a request and its processing. If the bu.er of pro.les over.ows, these \npro.les which have smallest counts are discarded. If the pro.le has never been used to request dynamic \ncompilation, we examine the pro.le to determine whether to request dynamic compilation using the pro.le. \nWe can calculate the frequency of the exception path from the pro.le with the counter and the start time \nof the pro.le and the current time as follows: Pro f ileCounter PathFrequency = . CurrentTime - Pro f \nileS tartTime Here we need a Pro f ileCounter large enough to keep the accuracy. If the frequency reaches \na certain threshold, the pro.le is passed to the dynamic compilation system. At the same time, those \npro.les, whose catching methods are the same as that of the pro.le and whose frequencies are near the \nthreshold, are searched for, and if any such a pro.le is found, it will also be sent for compilation. \nThose pro.les, whose catching methods are the same as that of the pro.le but whose frequencies are still \nlow, become useless after the compilation of the method. They will be reinitialized with the newly compiled \nmethod. The threshold should be adjusted depending on the speed of the executed code. The frequency of \nan exception path executed by the interpreted code tends to be lower than that by the compiled code. \nSimilarly, the frequency by the unoptimized code tends to be lower than that by optimized code. 3.3 \nException path inlining When the compiler is invoked to compile a method m1, it searches the pro.le repository \nfor the exception paths whose catching meth\u00adods are the same as the target method. If such exception \npaths are found, the compiler examines each of the exception paths to make a decision on inlining. Previous \nwork on inlining [15, 17, 9, 8] re\u00adduced it to the knapsack problem, as explained in the related work \nsection above. The framework in these projects is based on a dy\u00adnamic call graph, whose edge represents \nthe relationship between a call site and its callee and whose node represents a method. Each edge has \na frequency that represents how often the method is in\u00advoked from a call site. In contrast, the frequency \nof an exception path represents how often the method at the end of the path returns to the other end \nof the path with an exception. The compiler should treat method invocations on the exception path as \na whole. This is because the total bene.t of inlining the whole exception path is not a simple summation \nof the bene.ts of inlining each invocation on the exception path. Unless all the invocations on the exception \npath are inlined, we cannot eliminate the cost of PC-to-try mapping and exception .ltering by replacing \nthe athrows with gotos. We inline all the methods on each exception path, ignoring the code expansion. \nAlthough we currently inline the whole method body, we can apply a technique, called partial inlining \n[33], to reduce the code size expansion by selecting program regions to be inlined. We evaluate how inlining \nthe whole exception path a.ects the total compilation time in Section 4.4. Overall, the increase in the \ncom\u00adpilation time caused by inlining all exception paths is small in our experiments. An exception path \ncan include virtual invocations. The exception path pro.ling collects information on which methods are \ninvoked by the virtual invocations in the stack when an exception is thrown. To inline these virtual \nmethods, the compiler applies devirtualiza\u00adtion with tests [11, 31, 6, 24], with dynamic patching [41], \nor with the combination of these two approaches [19]. In devirtualization with tests [11, 31, 6], the \ncompiler generates the code that guards the inlined code and then inlines each virtual method. The guard \ncode checks if the class of each receiver object for the inlined vir\u00adtual method is the same as that \nof the inlined virtual method itself. If the guard code test fails, a normal virtual invocation is performed. \nIf di.erent virtual methods are invoked at the same call site, the compiler may generate guard code for \nevery class of the virtual methods to inline each of them. Instead of testing the class of a receiver \nobject, the guard code may test the method of a receiver object speci.ed at the call site [24]. Type \ninference [52, 5, 14, 27] computes the set of possible classes for every object reference in a program \nand narrows down the set of the classes reachable at each virtual call site. In conjunction with class \nhierarchy analysis [23], which tracks how methods are overridden in the whole program, the guard code \ncan be removed if it can be asserted that only a sin\u00adgle method is invoked at a virtual call site. However, \nsince Java has dynamic class loading to allow a method to be dynamically over\u00adridden, it is hard to say \nthat such an assertion is valid throughout the program execution, except for final methods. Earlier research \npresented techniques of canceling optimizations that are no longer valid because of changes in the class \nhierarchy [36, 40]. By apply\u00ading these techniques, the compiler can inline the method without the guard \ncode. If the method is overridden later, the compiler will cancel inlining by either discarding the whole \nmethod or switching to the original virtual method. An exception path can include invocations of synchronized \nmeth\u00adods or methods with try blocks. For synchronized methods, there can be two approaches: the one without \nand the other with an ex\u00adtension of the Java VMs. One speci.es a try block that covers the code of an \ninlined method and associates the try block with a new finally block that simply releases the lock for \nthe receiver ob\u00adject of the inlined method and re-throws the same exception. This implementation is the \nsame as that of Java s synchronized block [29], and it needs no extensions of the Java VMs. However, \nit re\u00adquires a control transfer for entering and leaving a .nally block in order to release locks. Also, \nit introduces extra work in the data.ow analysis due to the try block. The other approach maintains spe\u00adcial \ndata that keep track of locked receiver objects. For example, receiver objects are saved in the stack \nin the order of their locks. The compiler can generate a lock control table that describes how many objects \nhave been locked at a given program point. If an exception is thrown through a PC, the system does a \nlookup in the lock control table to obtain the number nsync(pcxcpt) of locks at that PC. If the handler \nthat catches the exception is found, the system also obtains the number nsync(pchdlr) of locks at the \nhan\u00addler. Since inlined methods are nested, the handler releases the last /* Case 1 */ new #7 <Class \nExc> dup invokespecial #18 <Method Exc()> athrow /* Case 2 */ getstatic #5 <Field Exc s_var> athrow Figure \n4: Two bytecode sequences that throw exceptions. /* Case 1 */ L1 = new Exc() = invokespecial L1.Exc() \n= athrow L1 /* Case 2 */ L2 = s_var = athrow L2 Figure 5: Two 3-address-code sequences that throw exceptions. \n(nsync(pcxcpt)-nsync(pchdlr)) locks in the stack frame. Otherwise, the system releases all the locks. \nIn an actual implementation, the num\u00adber nsync(pchdlr ) is available from the exception .ltering. Searching \nfor the number nsync(pcxcpt) is typically a binary search of the lock control table. However, by caching \nthe search results, the overhead of the search can be reduced. As a result, we took the second ap\u00adproach \nsince it releases locks faster than the .rst approach without using a control transfer. For methods with \ntry blocks, exception tables for the try blocks are merged while the program ranges are adjusted. This \nis possible because Java uses table-driven exception handling [55, 20] as ex\u00adplained in Section2. To \nallow succeeding optimizations to reorder the basic blocks, the compiler assigns an identi.er, called \na try ID, to each basic block. The try ID is associated with information on the handlers that cover the \nbasic block. The compiler compiles merged exception tables and generates a try ID table, which maps a \ntry ID to the corresponding handler information. Any basic block that is not associated with any try \nblock will have the default try ID. Once the try ID table is generated and try IDs are assigned to each \nbasic block, the exception table will no longer be accessed. Additionally, other method inlining optimizations, \nsuch as the one based on the frequency of edges in the dynamic call graph [21, 9, 8, 7], may be performed \nalong with the exception path inlining. 3.4 Throw elimination After exception paths are inlined within \nthe same code block, the compiler examines each athrow in the code block and, if possible, removes it \nand link the basic block that ends with the athrow to the corresponding handler. To perform such code \ntransformation, called throw-catch linking, the compiler performs exception class resolution and compile-time \nintra-method exception handling anal\u00adysis. 3.4.1 Exception class resolution Figure 5 shows the 3-address-code \ncorresponding to the bytecode in Figure 4. The compiler then resolves the class of every exception 1: \nclass ExactClass { 2: static void work2() { 3: try { 4: /* some code */ 5: throw new Exc(); /* may be \nguarded with if-statements */ 6: /* some code */ 7: } 8: catch (Exc e) { 9: /* some action */ 10: } 11: \n} 12: } Figure 6: Java source that throws exceptions of the exact class. object that is thrown at each \nathrow in the code. This is called ex\u00adception class resolution (ECR). This information is used in the \nnext step. To resolve these classes, the compiler performs type inference [52, 5, 14, 27]. In our exception \nclass resolution, the compiler at\u00adtempts to compute the static type of each object reference within a \nmethod using the similar approach to [27]. This is less expen\u00adsive and thus more suitable for dynamic \ncompilers than computing concrete types [52, 5], which would require analyzing the whole program to compute \nthe set of possible classes of every object ref\u00aderence. The compiler converts the bytecode to a 3-address-code \nas in [27]. Figure 4 shows two typical bytecode sequences that throw exceptions. In Case 1, an exception \nobject of class Exc is created and then immediately thrown. In Case 2, the static variable s var holds \nan object of class Exc or its subclass, and the object held in s var is thrown. Unlike receiver objects, \nthese two pat\u00adterns are dominant in throwing exceptions, so that the compiler can e.ciently compute the \nstatic type of each exception object that is thrown in most cases. 3.4.2 Compile-time intra-method exception \nhandling analysis With the types of exception objects provided by ECR, within the code where exception \npaths are inlined, the compiler attempts to .nd one or more handlers that can catch exceptions from each \nathrow. That is called Compile-time Intra-method Exception Han\u00addling Analysis (CIEHA). CIEHA is similar \nto the normal exception handling, explained in Section 2.2, except for the fact that CIEHA knows only \nthe set of possible classes (or types) of exception ob\u00adjects but not the class of the actual exception \nobject. CIEHA saves the result of its analysis for use in the next step. In the simplest case, ECR computes \nthe exact class of the exception objects and CIEHA .nds the corresponding handler. For example, in Case \n1 of Figure 5, ECR computes the exact class of L1, or Exc. Assuming that a try block covers the code \n= athrow L1 and its handler can catch the object of class Exc, CIEHA .nds the same handler at compile \ntime as the one exception handling can at runtime. Fig\u00adure 6 shows an example of Java source corresponding \nto this trivial situation. In this case, CIEHA saves the analysis that = athrow L1 is always caught by \nthis handler. Unless ECR computes the exact class for the exception objects, CIEHA needs to do more work \nthan what exception handling must do at runtime. A typical case is that ECR computes a set of classes \nbelonging to a class hierarchy and CIEHA .nds the handlers cor\u00adresponding to those classes. For example, \nin Case 2 of Figure 5, assume that: 1) ECR determines two possible classes of L2 such 1: class NotExactClass \n{ 2: Exc s var; 3: static void work3() { 4: try { 5: /* some code */ 6: throw s var; /* may be guarded \nwith if-statements */ 7: /* some code */ 8: } 9: catch (SubExc e) { 10: /* some action */ 11: } 12: catch \n(Exc e) { 13: /* some action */ 14: } 15: } 16: } Figure 7: Java source that potentially throws exceptions \nof mul\u00adtiple classes. Figure 8: Throw-catch linking without compensation code. as Exc and its subclass, \nSubExc, 2) a try block covers the code = athrow L2 , and 3) two handlers are associated with the try \nblock in the exception table: the .rst one can catch SubExc and the sec\u00adond one can catch Exc, which \nare examined by exception .ltering in this order, Figure 7 shows an example of Java source code that \nsatis.es these assumptions. In this case, the saved CIEHA result is that = athrow L2 is always caught \nby the .rst handler, if the class of a thrown exception is SubExc, or by the second handler, if the exception \nclass is Exc. 3.4.3 Throw-catch linking Using the results of CIEHA, the compiler attempts to transform \neach athrow to an unconditional branch or a conditional branch to the corresponding handler in the code. \nThis is called throw-catch linking (TCL). We illustrate how TCL transforms the code in this section. \nFigure 8 is a diagram that shows the blocks of code and the control .ow where TCL links the block that \nends with an athrow, denoted by Bt, with the handler block that catches exception Lt, denoted by Bh. \nBm denotes the block at which the control .ow merges from the handler and from the normal path. Code \nLh = loadExc makes the current exception visible in the scope of the Figure 9: Throw-catch linking with \ncompensation code. code. TCL extracts the code from Bh as a new handler, denoted by Bh-1. Also, TCL replaces \nthe code = athrow Lt with the code Lh = Lt , which copies the exception Lt to Lh. TCL then links Bt with \nthe rest of Bh, denoted by Bh-2. If such a shortcut goes across the boundary of two synchronized methods \nthat are inlined, TCL generates compensation code to release a lock. Figure 9 is the same as Figure 8 \nexcept that it has a block, denoted as Br, in\u00adserted between Bt and Bh-2 to release the locks of the \nsynchronized methods. If the exceptions caught by the handler are thrown only by the removed athrow, \nblock Bh-1 becomes unreachable and the compiler can eliminate it after TCL. CIEHA may .nd more than one \nhandler for an athrow. In these cases, TCL appends the code to perform conditional branches to these \nhandlers after Bt. CIEHA could also detect that some exceptions from the athrow are caught within the \ncode. In this case, TCL appends the code to perform an athrow only if the classes of exceptions are not \nhandled within the code. Thus, the compiler simpli.es the complex control .ows, which require runtime \nintra-method exception handling.   4. PERFORMANCE EVALUATION In this section, we evaluate Edo using \nour system. We .rst ex\u00adperimented with ten programs, which include various categories of applications, \nto investigate how many exceptions are thrown dur\u00ading the execution of programs. Table 2 shows those \nresults, where the .rst two columns describes information about programs, the third describes the parameters \nused for executing the programs, the fourth column shows total exception counts, the .fth to sev\u00adenth \ncolumns show top three exception counts per exception path and the corresponding path lengths, and the \nlast column describes what operations were performed or which benchmark was exe\u00adcuted. According to the \nresults, there are two typical types of pro\u00adgrams, exception-intensive programs and exception-minimal \npro\u00adgrams. Exception-intensive programs are programs that throw more than ten thousand exceptions throughout \ntheir execution or throw thousands exceptions in some phases such as the startup phase or the .le input \nphase. For example, 228 jack and 213 javac in SPECjvm98, Euler in Java Grande Forum Benchmark, and XML \nparser benchmark are typical exception-intensive programs. In con\u00adtrast, exception-minimal programs throw \ntens to hundreds of ex\u00adceptions throughout their execution. For example, the section three Category \nProgram Parameters Total Exception counts per exception path Top (path len.) 2nd(path len.) 3rd(path \nlen.) Scenario Web browsers ICE Browser 5.05 XBrowser 3.0 n.a. n.a. 1245 2892 406 (1) 1018 (1) 406 (3) \n1018 (3) 344 (3) 356 (3) open www.acm.org open www.acm.org word processor Ichitaro Ark 1.0 n.a. 3632 \n4040 1082 (1) 1169 (1) 1082 (3) 1169 (3) 691 (4) 718 (4) startup startup and open pubform.doc multimedia \nplayer JOS Media Player 2.0 n.a. 1075 5321 8205 506 (1) 2512 (2) 2516 (2) 506 (3) 1066 (1) 2506 (1) 26 \n(3) 1066 (3) 2506 (3) startup startup and load an AVI .le startup and load an MP3 .le XSLT processor \nbench\u00admark XSLTMark 1.2.1 run once 7116 2718 1865 2143 (3) 349 (3) 439 (3) 2143 (1) 305 (1) 439 (1) 1099 \n(3) 305 (3) 439 (3) Saxon (Michael Kay) XalanJ (Apatch Project) XT (James Clark) XML parser benchmark \nXML benchmark run once 17640 17640 11961 (3) 11961 (3) 3944 (3) 3944 (3) 1129 (2) 1129 (2) XML4J (IBM) \nXP (James Clark) chat server benchmark VolanoMark 2.1.2 count=100 736 217 200 (4) 169 (4) 110 (4) 16 \n(1) 105 (2) 16 (3) server client business benchmark SPECjbb2000 1.0 1 warehouse 506 99 (1) 96 (1) 96 \n(3) n.a. client benchmark SPECjvm98 1.03 1st run; com\u00adpliant mode 240 226 1324 100 19 451 242318 23849 \n99 (1) 75 (3) 441 (3) 33 (3) 6 (3) 150 (3) 18935 (4) 19108 (3) 39 (3) 75 (1) 441 (1) 33 (1) 6 (1) 150 \n(1) 14110 (3) 2656 (2) 39 (1) 25 (2) 147 (2) 11 (2) 2 (4) 50 (2) 14110 (4) 608 (2) 200 check 227 mtrt \n202 jess 201 compress 209 db 222 mpegaudio 228 jack 213 javac large application bench\u00admark Java Grande \nForum Benchmark Suite v2.0 Section 3 size A (small data set) 27 41222 24 54 48 9 (3) 41198 (2) 8 (3) \n18 (3) 16 (3) 9 (1) 8 (3) 8 (1) 18 (1) 16 (1) 9 (3) 8 (1) 8 (3) 18 (3) 16 (3) Search Euler MD MC Ray \nTracer Table 2: Exception counts and exception path lengths in various programs benchmark programs in \nthe Java Grande Forum Benchmark ex\u00adcept for Euler and SPECjbb2000, which run for relatively long times, \nare obviously exception-minimal programs. Also, Table 2 shows that the exception counts for just the \ntop three exception paths cover about 90% of the total exceptions in those exception\u00adintensive programs \nand that the path lengths of those exception paths are shorter than .ve. Therefore, there is a good opportu\u00adnity \nfor optimizing those exception-intensive programs using Edo. However, in 228 jack, the top three exception \npaths throw only 19% of the total exceptions. Therefore, optimizing 228 jack is more challenging for \nEdo. In the rest of this section, we used SPECjvm98 as a typical example of a set of exception-intensive \nprograms. Section 4.1 explains the methodology of our evaluation. Section 4.2 explains the system used \nin our experiments. Section 4.3 explains the details of the benchmark programs used in our ex\u00adperiments, \nfocusing on the exceptions thrown. Section 4.4 presents the results and discusses them. 4.1 Experimental \nmethodology For practicality, we used the SPECjvm98 benchmark suite [60] in the compliant mode throughout \nour experiments. We satis.ed all program execution rules2. There are two modes in the applets of SPECjvm98, \nthe compliant mode and the test mode. The compliant mode is used when publishing the scores of SPECjvm98 \nwhile the test mode can be used for other purposes such as research [19, 7, 2In terms of the reporting \nrules, we did not satisfy this speci.c rule: The entire system, including the hardware and all software \nfeatures, is required to be publicly available within three months of the date of publication of the \nresults. 40]. We compare Edo, stack unwinding (denoted as Unwind), and stack cutting (denoted as Cut) \nwith each other. We found that Edo im\u00adproves Unwind in exception-intensive programs and still adds no \npenalty to exception-minimal programs. Note that sections with di.erent exception characteristics coexist \nin the same program in our experiments. As explained in Section 2, Cut may be better than Unwind in exception-intensive \nprograms. We found support for this expectation in our experiments. We also show that those results of \nEdo are practical by comparing them to two state-of-the\u00adart Java VMs, the HotSpot Client VM and the HotSpot \nServer VM (both are build 1.3.1-b24, mixed mode) [59]. 4.1.1 Exception handler registration We used exception \nhandler registration (EHR) [38], also known as structured exception handling (SEH) [50], as Cut. EHR \nis sup\u00adported by operating systems such as IBM OS/2 and Microsoft Win\u00addows [45]. In EHR, the compiler \ngenerates prolog code that regis\u00adters an exception registration record (ERR) as well as epilog code that \nderegisters it for any compiled code segment that has try blocks [38]. Each thread has its own ERR list, \nwhich is the last-in-.rst-out list of ERRs. When the code is executed in a thread, it maintains the ERR \nlist of the thread by adding and removing its ERR from the ERR list at runtime. Also, the code maintains \na .eld in its ERR that identi.es the current try block at runtime. Non-volatile regis\u00adters (NVRs) (i.e. \nEBX, ESI, EDI, and EBP in our compiler) that are not used in the code and are exposed to callees are \nconservatively saved in the code, because those exposed NVRs that could be saved in the callee code are \nnot restored during exception handling. For synchronized methods, the compiler generates code that not \nonly obtains the lock on an object but also registers the object to the top\u00admost ERR in the ERR list. \nAs a result, the system can release the locks when the ERR catches an exception without traversing the \nstack. When an exception is thrown, the system traverses the handler list, performing exception .ltering, \nuntil a handler that can catch the exception is found. Thus, no unwinding the stack occurs during exception \nhandling in EHR.  4.2 Environment We implemented Edo on our Java Just-In-Time (JIT) compiler [57, 41, \n40, 42], which works with the IBM Developer Kit for Windows, Java 2 Technology Edition [37], Version \n1.3.1. Throughout the measurements, we used the same parameters for a Java VM and SPECjvm98 that Sun \nused to submit the results of their HotSpot VMs to the SPEC. As for the Java parameters, the applets \nof SPECjvm98 were executed with a loopback network, such as appletviewer.exe http://localhost/SpecApplet.html. \nThe initial and maximum amounts of Java heap space were 42 MB, speci.ed with the parameters -Xms42m -Xmx42m. \nAlso, to measure the HotSpot VMs, the tuning options -XX:NewSize=13m -XX:MaxNewSize=13m were speci.ed. \nAs for SPECjvm98 parame\u00adters, spec.initial.automin=5 causes each benchmark program to be executed for \na minimum of .ve times. In the compliant mode, SPEC ratios are calculated by the applets after the benchmarks \nare .nished. SPEC ratios for benchmark pro\u00adgrams and their geometric mean for the second execution of \nthe applet immediately after rebooting the machine were reported in the rest of this section. The measurements \nwere performed on an IBM Personal Computer 300PL Model 6892-44J, which has a Pentium-III 800MHz CPU, \nand 256 MB physical memory, running Microsoft Windows NT Server Version 4.0 and the Microsoft Internet \nInformation System 4.0 with Service Pack 6. 4.3 Characteristics of the benchmark programs This section \ngives an overview of the characteristics of the bench\u00admark programs. SPECjvm98 is a suite of benchmark \nprograms [60] and is currently accepted as one of the major Java bench\u00admarks for evaluating Java VMs \n[46, 57, 42, 41, 40, 7, 19]. SPEC\u00adcompliant runs, whose SPEC ratios can be submitted to the SPEC if the \nreporting rules are satis.ed, should be performed via the applet SpecApplet. Table 3 shows the number \nof exception paths and the exception counts for each benchmark program of SPECjvm98 during the two classes \nof runs, the .rst run and any run after the .rst run (i.e., the second through the .fth runs). During \none ex\u00adecution of the SPECjvm98 applet, each benchmark program was executed for a minimum of .ve times, \nas explained in the previous section. We collected the data in Table 3 during each execution of those \n.ve runs. As Table 3 shows and earlier research [46, 19] re\u00adports, 213 javac and 228 jack are exception-intensive \nprograms in the SPECjvm98 benchmark. The other programs (exception\u00adminimal programs: 227 mtrt, 202 jess, \n201 compress, 209 db, and 222 mpegaudio) throw few exceptions once initialized. These exception-minimal \nprograms and exception-intensive programs run the 1st run after the 1st run Total Num of Total Num of \nexcept. except. except. except. counts paths counts paths 227 mtrt 226 6 10 1 202 jess 1324 5 0 0 201 \ncompress 100 5 0 0 209 db 19 5 0 0 222 mpegaudio 451 5 0 0 228 jack 242318 170 241877 165 213 javac 23849 \n8 22373 3 Table 3: The original characteristics of SPECjvm98 bench\u00admark programs benchmark 1st run total \nexception counts 2nd 3rd 4th 5th 227 mtrt 226 10 10 10 10 202 jess 1309 0 0 0 0 201 compress 66 0 0 0 \n0 209 db 12 0 0 0 0 222 mpegaudio 300 0 0 0 0 228 jack 113715 74097 59938 43505 40724 213 javac 3885 \n44 0 0 0 Table 4: The total exception counts using EDO in the same execution environment. 4.4 Results \nand discussion 4.4.1 Total exception counts and numbers of excep\u00adtion paths Table 4 shows the total number \nof exceptions for SPECjvm98 bench\u00admark programs during each run. During the .ve runs, Edo reduced 83.2% \nof the original value (241877) for 228 jack and 100% for 213 javac. Table 5 shows that Edo reduced the \nnumber of exception paths for SPECjvm98 benchmark programs. The number of hot execution paths, 954, is \nlarger than the original 170 hot execution paths in Table 3 because selective compilation changes the \naddress of the same call site from the bytecode address to the compiled code ad\u00address. As a result, many \nexception paths can be collected for some logically equivalent exception path. In other words, the number \nof exception paths can increase because of compilation and recompi\u00adlation, while Edo normally reduces \nthe number of exception paths. benchmark total exception paths 1st run 2nd 3rd 4th 5th 227 mtrt 6 1 1 \n1 1 202 jess 6 0 0 0 0 201 compress 4 0 0 0 0 209 db 4 0 0 0 0 222 mpegaudio 4 0 0 0 0 228 jack 954 301 \n320 264 229 213 javac 11 1 0 0 0 Table 5: The number of exception paths using EDO benchmark Edo Unwind \nCut HSC HSS 227 mtrt 143 144 146 59.4 128 202 jess 77.3 77.0 76.0 69.4 86.6 201 compress 78.0 77.8 78.0 \n63.4 73.1 209 db 26.9 26.8 26.2 27.5 29.1 222 mpegaudio 153 152 148 82.1 103 228 jack 142 120 131 102 \n122 213 javac 56.9 50.0 41.1 34.7 40.5 Geom. Mean 83.6 80.1 78.2 57.5 73.9  Table 6: SPEC ratios: Best \nbenchmark Edo Unwind Cut HSC HSS 227 mtrt 82.6 79.6 85.1 58.3 59.1 202 jess 56.2 56.1 57.1 61.1 60.1 \n201 compress 76.6 76.1 76.1 62.2 68.5 209 db 26.4 26.3 25.8 23.6 24.4 222 mpegaudio 123 124 124 78.1 \n79.1 228 jack 80.1 85.6 90.1 88.4 37.0 213 javac 34.3 32.0 29.6 29.5 14.5 Geom. Mean 61.1 60.8 61.1 52.3 \n42.6 Table 7: SPEC ratios: Worst The increase from the second run to the third run for 228 jack in Table \n5 is an example of such a case. As the JIT compiler compiles more bytecode, most of the addresses of \ncall sites on exception paths become those of JIT-compiled code. However, the number of exception paths \nwas eventually reduced by Edo to 24.0% of the value (954) for the .rst run. 4.4.2 SPEC ratios We present \nSPEC ratios for three approaches: Edo, Unwind, and Cut. We also present SPEC ratios for the HotSpot Client \nVM (de\u00adnoted as HSC) and the HotSpot Server VM (denoted as HSS). The SPECjvm98 applet calculates the \nbest and the worst SPEC ratios for each benchmark and the geometric mean of all the SPEC ratios. Ta\u00adble \n6 and Table 7 show the best and the worst SPEC ratios, respec\u00adtively. The SPEC ratio of each benchmark \nin Table 6 is calculated based on the best time for .ve runs of the benchmark. Similarly, the SPEC ratio \nin Table 7 is calculated based on the worst times. With these tables, we evaluate how much Edo improves \nSPEC best ratios, while it introduces only small penalties. We compare Edo with Unwind. Edo improved \n228 jack by 18.3%, 213 javac by 13.8%, and the geometric mean of seven programs by 4.37% in terms of \nthe best SPEC ratios (see Figure 10). In 213 javac, the compiler eliminated the code that frequently \nallo\u00adcates and initializes exception objects. Once Edo replaces a throw with the explicit control .ow \nto the corresponding catch, if an exception object that reaches the throw is no longer used by the catch, \nthe compiler eliminates the allocation and initialization code for the object. The mechanism of this \nelimination is di.erent from lazy exceptions [19], which postpones creation and initialization of an \nexception object until they are actually used. However, both can reduce the overhead for exception object \ncreation. For the worst SPEC ratios, Edo degraded 228 jack by 6.43%, although it re\u00adduced the overhead \nfor exception handling path by 50%. The de\u00adgree of this degradation depends on how the system recompiles \nhot methods. In the .rst run of 228 jack, where the worst SPEC ratio was measured, time-consuming optimizations \nsuch as DAG-based optimizations and data.ow-based optimizations [58] consumes ad\u00additional overhead (19%) \nfor hot methods, however, the resulting code cannot compensate for the overhead within the .rst run. \nEdo does not a.ect the other programs. Thus, the results demonstrate that the performance of exception-intensive \nprograms is dramati\u00adcally improved while that of exception-minimal programs is not degraded. Table 6 \nalso shows the performance trade-o. between Unwind and Cut. Cut shows an 8.40% improvement over Unwind \nin 228 jack, however, it degrades 213 javac by 21.7% . This is because methods that have try blocks are \nheavily invoked and the overhead of main\u00adtaining the ERRs is not negligible in these programs. Thus, \nthe results show that the runtime overhead speci.c to Cut and its fast exception handling are traded \no. and the drawback of the former counteracts the bene.t of the latter.   5. RELATED WORK Lang et al. \n[45] includes a survey of exception handling mech\u00adanisms, both those based on programming languages and \nthose based on operating systems. Implementation techniques of programming-language-based exception handling \nmechanisms are described for CLU [49], Modula-2 [25], C++ [43, 12, 55, 20], a dialect of C [28], and \nintermediate languages [22, 53]. Ramsey and Jones [53] categorize these techniques into two, stack unwinding \nand stack cutting, based on whether or not the stack is walked. We use these terminologies in this paper. \nDrew et al. [25] show that unwinding the stack is expensive if the normal path is highly optimized and \ncontains no housekeeping code for exception handling. They propose an e.cient implemen\u00adtation of stack \nunwinding for Modula-2. Their compiler generates a dummy epilog for each procedure, which restores callee-saved \nnon-volatile registers and discards the stack frame. The dummy epilog includes the code to search for \nthe caller s dummy epilog. To eliminate the overhead of searching, they also mention the hid\u00adden argument \napproach, where a special extra argument is passed at each function call. However, this adds a penalty \nto the normal path. Krall and Probst [44] propose an implementation of stack unwind\u00ading for JIT-based \nJava VM, called CACAO, with an optimization similar to the hidden argument approach [25]. JUDO [19], \nwhich is a compile-all Java VM, uses stack unwinding with no house\u00adkeeping code in the normal path. They \nperform a binary search of the lookup table to map the program counter to the unwind data structure. \nTo reduce the search overhead, JUDO caches the lookup results, which gives a signi.cant improvement in \n228 jack. Notice that our base compiler used in Section 4 includes this optimization. They also present \na technique to eliminate unused exception objects that have no side e.ects. Lee et al. [47] propose exception \nhandler prediction to reduce the overhead of Java s exception .ltering in their Java VM, LaTTe. Their \ntechnique optimizes the cases where they could predict that a handler in the same method catches a throw. \nIn such cases, they simply generate the jump instruction together with the code to check if the current \nexception s class is the same as the pre\u00addicted one. By contrast with their dynamic approach, our approach \nuses exact results from the data.ow analysis. Thus, no checking code needs to be generated. In [46], \nthey experimented with ex\u00adtending their technique across multiple methods by controlling the method inlining \nheuristics. While they mention that such an ex\u00adtension is possible, they conclude that it is impractical \nbecause it spends more than twice of the compile time. They do not describe how they changed the method \ninlining heuristics; however, their re\u00adport implies that aggressive method inlining can cause an explosion \nof code size and compilation time. Method inlining is one of the most widely applied optimizations in \nboth static compilers [15, 17, 9] and dynamic compilers [35, 7, 51]. The central issue in the optimization \nis how aggressively inlining should be performed. Di.erent compilers adopt di.erent heuristics for addressing \nthe issue. However, to the best of our knowledge, there is no heuristics that takes exceptions into account. \nThere are many studies that focus on optimizing the normal path of programs in the presence of exceptions \n[34, 16, 18, 26, 32]. Our focus is on reducing the overhead due to exception handling rather than such \ncode optimization. Their results and our approach can complementarily improve exception-intensive programs. \n 6. CONCLUSIONS This paper has presented a novel approach to adaptive optimiza\u00adtion of exception-intensive \nprograms. It consists of exception path pro.ling, exception path inlining, and throw elimination. Excep\u00adtion \npath pro.ling records an exception path into a repository and updates the pro.le associated with the \npath when an exception is thrown. Exception path inlining searches the repository for hot exception paths \nand inlines the methods on these exception paths into the catcher. Finally, given a pair of throw and \ncatch, throw elimination replaces the throw with the explicit control .ow to the catch. Our new approach \ndoes not incur any penalty in the normal path, while it optimizes the frequently-executed exception handling \npaths. We have presented experimental data about exception counts and the number of exception paths, \nusing various categories of pro\u00adgrams. Using those data, we have shown that some of the pro\u00adgrams throw \nmany exceptions during their executions and perfor\u00admance su.ers from the overhead for exception handling. \nWe have presented experimental results with IBM s production Just-in-Time compiler and discussed the \ne.ectiveness of our approach. Experi\u00admental results show that, in exception-intensive programs, our ap\u00adproach \ngreatly reduced exception counts by 83.2% for 228 jack and by 100% for 213 javac and greatly improved \nthe best SPEC ratios for stack unwinding by 18.3% for 228 jack and by 13.8% for 213 javac, and that these \nresults are practical compared with Sun s HotSpot VMs. 7. ACKNOWLEDGMENTS We are grateful to the people \nin the Network Computing Platform group at Tokyo Research Laboratory for implementing our special JIT \ncompiler. 8. REFERENCES [1] Proceedings of the 10th annual conference on Object-oriented programming \nsystems, languages and applications (New York, NY, USA, Oct. 1995), ACM Press. [2] ACM SIGPLAN 00 Conference \non Programming language design and implementation (New York, NY, USA, May 2000), ACM Press. [3] Proceedings \nof the ACM 2000 Conference on Java Grande (New York, NY, USA, June 2000), ACM Press. [4] Proceedings \nof the ACM SIGPLAN Conference on Object-Oriented Programming Systems, Languages and Applications (OOPSLA-2000) \n(New York, NY, USA, 2000), ACM Press. [5] Agesen, O., and H\u00a8 olzle, U. Type feedback vs. concrete type \ninference: a comparison of optimization techniques for object-oriented languages. In ACM [1], pp. 91 \n107. [6] Aigner, G., and H\u00a8 olzle, U. Eliminating virtual function calls in C++ programs. In Proceedings \nof the 10th European Conference on Object-Oriented Programming -ECOOP 96 (Lecture Notes in Computer Science, \nVol. 1098) (Berlin, July 1996), Springer-Verlag, pp. 142 166. [7] Arnold, M., Fink, S., Grove, D., Hind, \nM., and Sweeney, P. F. Adaptive optimization in the Jalape no JVM. In ACM [4], pp. 47 65. [8] Arnold, \nM., Fink, S., Sarkar, V., and Sweeney, P.F.A comparative study of static and pro.le-based heuristics \nfor inlining. In Proceedings of ACM SIGPLAN workshop on Dynamic and adaptive compilation and optimization \n(New York, NY, USA, Jan. 2000), ACM Press, pp. 52 64. [9] Ayers, A., Gottlieb, R., and Schooler, R. Aggressive \ninlining. In ACM SIGPLAN 97 Conference on Programming language design and implementation (New York, NY, \nUSA, June 1997), ACM Press, pp. 134 145. [10] Baker, T. P., and Riccardi, G. A. Implementing Ada exceptions. \nIEEE Software 3, 5 (Sept. 1986), 42 51. [11] Calder, B., and Grunwald, D. Reducing indirect function \ncall overhead in C++. In Proceedings of the ACM SIGPLAN 94 symposium on Principles of programming languages \n(New York, NY, USA, Jan. 1994), ACM Press, pp. 397 408. [12] Cameron, D., Faust, P., Lenkov, D., and \nMehta, M.A portable implementation of C++ exception handling. In Proceedings of the C++ Conference (Aug. \n1992), USENIX Association, pp. 225 243. [13] Cardelli, L., Donahue, J., Glassman, L., Jordan, M., Kalsow, \nB., and Nelson, G. Modula-3 report (revised). Tech. Rep. SRC Research Report 52, Digital Equipment Corporation, \nSystems Research Center, 1989. [14] Carini, P. R., Srinivasan, H., and Hind, M. Flow-sensitive type analysis \nfor C++. Tech. Rep. Research Report RC20267, IBM, 1995. [15] Chang, P. P., Mahlke, S. A., Chen, W. Y., \nand Hwu, W. W. Pro.le-guided automatic inline expansion for c programs. Software: Practice and Notices \n22, 5 (May 1992), 349 369. [16] Chatterjee, R., Ryder, B. G., and Landi, W. Complexity of concrete type-inference \nin the presence of exceptions. In ESOP 98, 7th European Symposium on Programming, Proceedings (Lecture \nNotes in Computer Science, Vol. 1381) (Berlin, Apr. 1998), Springer-Verlag, pp. 57 74. [17] Chen, W. \nY., Chung, P. P., Conte, T. M., and Hwu, W. W. The e.ect of code expanding optimizations on instruction \ncache design. IEEE Trans. Comput. 42, 9 (Sept. 1993), 1045 1057. [18] Choi, J., Grove, D., Hind, M., \nand Sarkar,V. E.cient and precise modeling of exceptions for the analysis of Java programs. In Proceedings \nof the ACM SIGPLAN-SIGSOFT workshop on Programming analysis for software tools and engineering (New York, \nNY, USA, Sept. 1999), ACM Press, pp. 21 31. [19] Cierniak, M., Lueh, G., and Stichnoth, J. M. Practicing \nJUDO: Java under dynamic optimizations. In ACM [2], pp. 18 21. [20] de Dinechin,C. C++ exception handling. \nIEEE Concurrency 8, 4 (2000), 72 79. [21] Dean, J., and Chambers, C. Towards better inlining decisions \nusing inlining trials. In Proceedings of the 1994 ACM conference on LISP and functional programming (New \nYork, NY, USA, June 1994), ACM Press, pp. 273 282. [22] Dean, J., DeFouw, G., Grove, D., Litvinov, V., \nand Chambers, C. Vortex: an optimizing compiler for object-oriented languages. In Proceedings of the \n7th annual conference on Object-oriented programming systems, languages and applications (OOPSLA-1996) \n(New York, NY, USA, Oct. 1996), ACM Press, pp. 83 100. [23] Dean, J., Grove, D., and Chambers, C. Optimization \nof object-oriented programs using static class hierarchy. In Proceedings of the 9th European Conference \non Object-Oriented Programming -ECOOP 95 (Lecture Notes in Computer Science, Vol. 952) (Berlin, Aug. \n1995), Springer-Verlag, pp. 77 101. [24] Detlefs, D., and Agesen, O. Inlining of virtual methods. In \nProceedings of the 13th European Conference on Object-Oriented Programming -ECOOP 99 (Lecture Notes in \nComputer Science, Vol. 1628) (Berlin, June 1999), Springer-Verlag, pp. 258 278. [25] Drew, S., Gouph, \nK. J., and Ledermann, J. Implementing zero overhead exception handling. Tech. Rep. Technical Report 95-12, \nFaculty of Information Technology, Queensland University of Technology, 1995. [26] Fitzgerald, R., Knoblock, \nT. B., Ruf, E., Steensgaard, B., and Tarditi, D. Marmot: An optimizing compiler for java. Software: Practice \nand Notices 30, 3 (Mar. 2000), 199 232. [27] Gagnon, E. M., Hendren, L. J., and Marceau, G. E.cient inference \nof static types for Java bytecode. In Static Analysis. 7th international symposium, SAS 2000, Proceedings \n(Lecture Notes in Computer Science, Vol. 1824) (Berlin, June 2000), Springer-Verlag, pp. 199 219. [28] \nGehani, N. H. Exceptional C or C with exceptions. Software: Practice and Notices 22, 10 (Oct. 1992), \n827 848. [29] Gosling, J., Joy, B., and Steele, G. The Java Language Speci.cation. The Java Series. Addison-Wesley, \nReading, Massachusetts, Aug. 1996. [30] Gosling, J., Joy, B., and Steele, G. The Java Language Speci.cation. \nIn The Java Series [29], Aug. 1996, ch. 11. [31] Grove, D., and Dean, J. Pro.le-guided receiver class \nprediction. In ACM [1], pp. 108 123. [32] Gupta, M., Choi, J., and Hind, M. Optimizing Java programs \nin the presence of exceptions. In Proceedings of the 14th European Conference on Object-Oriented Programming \n-ECOOP 00 (Lecture Notes in Computer Science, Vol. 1850) (Berlin, June 2000), Springer-Verlag, pp. 422 \n446. [33] Hank, R. E., Hwu, W. W., and Rau, B. R. Region-based compilation: an introduction and motivation. \nIn Proceedings of the 28th annual international symposium on Microarchitecture (New York, NY, USA, Nov. \n1995), ACM Press, pp. 158 168. [34] Hennessy, J. Program optimization and exception handling. In Conference \nrecord of the 8th annual ACM symposium on Principles of programming languages (New York, NY, USA, Jan. \n1981), ACM Press, pp. 200 206. [35] H\u00a8olzle, U., Chambers, C., and Ungar, D. Optimizing dynamically-typed \nobject-oriented languages with polymorphic inline caches. In Proceedings of the 5th European Conference \non Object-Oriented Programming -ECOOP 91 (Lecture Notes in Computer Science, Vol. 512) (Berlin, July \n1991), Springer-Verlag, pp. 21 38. [36] H\u00a8olzle, U., Chanmbers, C., and Ungar, D. Debugging optimized \ncode with dynamic deoptimization. In ACM SIGPLAN 92 Conference on Programming language design and implementation \n(New York, NY, USA, June 1992), ACM Press, pp. 32 43. [37] The IBM Developer Kit, Java 2 Technology Edition. \nhttp://www.ibm.com/developerworks/java/jdk/. [38] IBM VisualAge C++ for OS/2 Programming Guide, third \ned. In IBM [39], May 1995, ch. 14. Signal and OS/2 Exception Handling. [39] IBM VisualAge C++ for OS/2 \nProgramming Guide, third ed., May 1995. [40] Ishizaki, K., Kawahito, M., Yasue, T., Komatsu, H., and \nNakatani, T. A study of devirtualization techniques for a Java Just-In-Time compiler. In ACM [4], pp. \n294 310. [41] Ishizaki, K., Kawahito, M., Yasue, T., Takeuchi, M., Ogasawara, T., Suganuma, T., Onodera, \nT., Komatsu, H., and Nakatani, T. Design, implementation, and evaluation of optimizations in a Java Just-In-Time \ncompiler. Concurrency: Practice and Experience 12, 6 (2000), 457 475. [42] Kawahito, M., Komatsu, H., \nand Nakatani,T. E.ective null pointer check elimination utilizing hardware trap. In Proceedings of the \n9th international conference on Architectural support for programming languages and operating systems \n(ASPLOS-IX) (New York, NY, USA, Nov. 2000), ACM Press, pp. 118 127. [43] Koenig, A., and Stroustrup, \nB. Exception handling for C++ (revised). In Proceedings of the C++ Conference (Apr. 1990), USENIX Association, \npp. 149 176. [44] Krall, A., and Probst, M. Monitors and exceptions: How to implement Java e.ciently. \nIn ACM 1998 Workshop on Java for High-Performance Network Computing (New York, NY, USA, 1998), ACM Press, \npp. 15 24. Also published as Concurrency: Practice and Experience, 10(11 13), September 1998, CODEN CPEXEI, \nISSN 1040-3108. [45] Lang, J., and Stewart, D. B. A study of the applicability of existing exception-handling \ntechniques to component-based real-time software technology. ACM Trans. Program. Lang. Syst. 20, 2 (March \n1998), 274 301. [46] Lee, S., Yang, B.-S., Kim, S., Park, S., Moon, S.-M., and Ebcio. glu, K. E.cient \nJava exception handling in Just-in-Time compilation. In ACM [3], pp. 1 8. [47] Lee, S., Yang, B.-S., \nKim, S., Park, S., Moon, S.-M., Ebcio. glu, K., and Altman, E. On-demand translation of Java exception \nhandlers in the LaTTe JVM just-in-time compiler. In Proceedings of the 1999 Workshop on Binary Translation \n(Oct. 1999). [48] Lindholm, T., and Yellin, F. The Java Virtual Machine Speci.cation. The Java Series. \nAddison-Wesley, Reading, Massachusetts, Sept. 1996, ch. 4.7.4. [49] Liskov, B., and Snyder, A. Exception \nhandling in CLU. IEEE Trans. Softw. Eng. 5, 6 (1979), 546 558. [50] Microsoft. MSDN Online. http://msdn.microsoft.com/. \n[51] Paleczny, M., Vick, C., and Click, C. The Java HotSpot server compiler. In Proceedings of the Java \nVirtual Machine Research and Technology Symposium (Apr. 2001), USENIX Association, pp. 1 12. [52] Palsberg, \nJ., and Schwartzbach, M. I. Object-oriented type inference. In Conference proceedings on Object-oriented \nprogramming systems, languages and applications (New York, NY, USA, Oct. 1991), ACM Press, pp. 146 161. \n[53] Ramsey, N., and Peyton Jones, S. A single intermediate language that supports multiple implementations \nof exceptions. In ACM [2], pp. 285 298. [54] Ryder, B. G., Smith, D., Kremer, U., Gordon, M., and Shah, \nN. A static study of Java exceptions using JESP. Tech. Rep. dcs-tr-406, Rutgers University, Department \nof Computer Science, 1999. [55] Schilling, J. L. Optimizing away C++ exception handling. ACM SIGPLAN \nNotices 33, 8 (Aug. 1998), 40 47. [56] Sinha, S., and Harrold, M. J. Analysis and testing of programs \nwith exception handling constructs. IEEE Trans. Softw. Eng. 42, 9 (2000), 849 871. [57] Suganuma, T., \nOgasawara, T., Takeuchi, M., Yasue, T., Kawahito, M., Ishizaki, K., Komatsu, H., and Nakatani, T. Overview \nof the IBM Java Just-In-Time compiler. IBM Syst. J. 39, 1 (2000), 175 193. [58] Suganuma, T., Yasue, \nT., Kawahito, M., Komatsu, H., and Nakatani, T. A dynamic optimization framework for a Java Just-In-Time \ncompiler. In Proceedings of the ACM SIGPLAN Conference on Object-Oriented Programming Systems, Languages \nand Applications (OOPSLA-2001) (New York, NY, USA, 2001), ACM Press. [59] Sun. Java HotSpot technology. \nhttp://java.sun.com/products/hotspot/, Oct. 2000. [60] The Standard Performance Evaluation Corporation \n(SPEC). JVM Client98 (SPECjvm98). http://www.spec.org/osg/jvm98/, 1998. [61] Whaley, J. A portable sampling-based \npro.ler for Java virtual machines. In ACM [3], pp. 78 87.  \n\t\t\t", "proc_id": "504282", "abstract": "Optimizing exception handling is critical for programs that frequently throw exceptions. We observed that there are many such exception-intensive programs iin various categories of Java programs. There are two commonly used exception handling techniques, stack unwinding optimizes the normal path, while stack cutting optimizes the exception handling path. However, there has been no single exception handling technique to optimize both paths.", "authors": [{"name": "Takeshi Ogasawara", "author_profile_id": "81100012455", "affiliation": "Tokyo Research Laboratory, IBM Japan, 1623-14 Shimotsuruma, Yamato-shi, Kanagawa, Japan 242-8502", "person_id": "PP14017438", "email_address": "", "orcid_id": ""}, {"name": "Hideaki Komatsu", "author_profile_id": "81100557247", "affiliation": "Tokyo Research Laboratory, IBM Japan, 1623-14 Shimotsuruma, Yamato-shi, Kanagawa, Japan 242-8502", "person_id": "PP39048455", "email_address": "", "orcid_id": ""}, {"name": "Toshio Nakatani", "author_profile_id": "81100311827", "affiliation": "Tokyo Research Laboratory, IBM Japan, 1623-14 Shimotsuruma, Yamato-shi, Kanagawa, Japan 242-8502", "person_id": "PP14113792", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/504282.504289", "year": "2001", "article_id": "504289", "conference": "OOPSLA", "title": "A study of exception handling and its dynamic optimization in Java", "url": "http://dl.acm.org/citation.cfm?id=504289"}