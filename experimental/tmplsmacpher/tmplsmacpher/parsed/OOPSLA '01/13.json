{"article_publication_date": "10-01-2001", "fulltext": "\n A Dynamic Optimization Framework for a Java Just-in-Time Compiler Toshio $uganuma, Toshiaki Yasue, \nMotohiro Kawahito, Hideaki Kornatsu, Toshio Nakatani IBM Tokyo Research Laboratory 1623-14 Shimotururna, \nYamato-shi, Kanagawa 242-8502, Japan Phone: +81-46-215-4658 Email: {suganuma, yasue, jl25131, komatsu, \nnakatani}@jp.ibm.com ABSTRACT The high performance implementation of Java Virtual Machines (JVM) and \nJust-In-Time (JIT) compilers is directed toward adaptive compilation optimizations on the basis of online \nruntime profile in- formation. This paper describes the design and implementation of a dynamic optimization \nframework in a production-level Java JIT compiler. Our approach is to employ a mixed mode interpreter \nand a three level optimizing compiler, supporting quick, full, and spe- cial optimization, each of which \nhas a different set of tradeoffs be- tween compilation overhead and execution speed. A lightweight sampling \nprofiler operates continuously during the entire program's execution. When necessary, detailed information \non runtime behav- ior is collected by dynamically generating instrumentation code which can be installed \nto and uninstalled from the specified recom- pilation target code. Value profiling with this instrumentation \nmechanism allows fully automatic code specialization to be per- formed on the basis of specific parameter \nvalues or global data at the highest optimization level. The experimental results show that our approach \noffers high performance and a low code expansion ra- tio in both program startup and steady state measurements \nin com- parison to the compile-only approach, and that the code specializa- tion can also contribute \nmodest pertbrmance improvements. 1. INTRODUCTION There has been a significant challenge for the implementation \nof high performance virtual machines for Java [18] primarily due to the dynamic nature of the language, \nand many research projects have been devoted to developing efficient dynamic compilers for Java [10, \n13, 17, 23, 24, 26, 30, 36, 37, 43]. Since the compilation time overhead of a dynamic compiler, in contrast \nto that of a con- ventional static compiler, is included in the program's execution time, it needs to \nbe very selective about which methods it decides to compile and when and how it decides to compile them. \nMore spe- cifically, it should compile methods only if the extra time spent in compilation can be amortized \nby the performance gain expected Permission to make digital or har~t copies of all or part of this work \nfor personal or classroom use is granted without fee provided that copies are not made or distributed \nfor profit or commercial advantage and that copies bear this notice and the full citation on the first \npage. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior \nspecific permission and/or a llze. OOPSLA (71 Tampa Florida USA Copyright ACM 2001 1-58113-335 -9/01/10...$5.00 \nfrom the compiled code. Once program hot regions are detected, the dynamic compiler must be very aggressive \nin identifying good op- portunities for optimizations that can achieve higher total perform- ance. This \ntradeoff between compilation overhead and its perform- ance benefit is a crucial issue for dynamic compilers. \nIn the above context, the high performance implementation of Java Virtual Machines (JVM) and Just-In-Time \n(JIT) compilers is mov- ing toward exploitation of adaptive compilation optimizations on the basis of \nruntime profile information. Although there is a long history of research on mntime feedback-directed \noptimizations (FDO), many of these techniques are not directly applicable for use in JVMs because of \nthe requirements for programmer intervention. Jalapefio [3] is the first JVM implementing a fully automatic \nadap- tive compilation framework with feedback-directed method inlining, and it demonstrated a considerable \nperformance improve- ment benefit. The Intel research compiler, JUDO [13], also employs dynamic optimization \nthrough a recompilation mechanism. Both of these systems use the compile-only approach, and it can result \nin relatively higher costs in compilation time and code size growth. In this paper, we present a different \napproach for the dynamic opti- mization framework implemented in our production-level JIT com- piler. \nWe use a combination of an interpreter and a dynamic com- piler with three levels of optimization to \nprovide balanced steps for the tradeoffbetween compilation overhead and compiled code qual- ity. A low-overbead, \ncontinuously operating sampling profiler iden- tifies program hot regions for method reoptimization. \nTo decide on the recompilation policy, we use a value profiling technique, which can be dynamically installed \ninto and uninstalled from target code, for collecting detailed runtime information. This technique does \nnot involve target code recompilation, and is reasonably lightweight and effective for the use of collecting \na fixed amount of sampled data on program hot regions. In the highest level optimization for program \nhot methods, we apply code specialization using impact analysis. This is performed fully automatically \non the basis of the parameter values or global object data, which exhibit runtime in- variant or semi-invariant \nbehavior [11 ] through the dynamically in- strumented value profiling. Our experimental results show \nthat this approach provides significant advantages in terms of performance and memory footprint, compared \nto the compile-only approach, both at program start-up and in steady state runs. 1.1 Contributions This \npaper makes the following contributions: 180 @ System architecture: We present a system architecture \nfor a simple, but efficient and high-performance dynamic optimiza- tion framework in a production-level \nJava JIT compiler with a mixed mode interpreter. Extensive experimental data is pre- sented for both \nperformance and memory footprint to verify the advantages of our approach. @ Profiling techniques: We \npresent a program profiling mecha- nism combining two different techniques. One is a continuously operating, \nlightweight sampling profiler for detecting program hot methods, and the other is a dynamically installed \nand unin- stalled instrumenting profiler that collects detailed information for the methods gathered \nby the first profiler. @ Code specialization: The design and implementation of code specialization, an \nexample of FDO, is described, using the dy- namically instrumented profiling mechanism for value sampling. \nThis is a fully automated design with no programmer interven- tion required. The effectiveness of this \ntechnique is evaluated using industry standard benchmark programs. The rest of this paper is organized \nas follows. The next section sum- marizes related work, comparing our system to prior systems. Sec- tion \n3 describes the overall system architecture of our dynamic compilation system, including the multiple \nlevels of the execution model divided between the mixed mode interpreter and recompila- tion framework. \nSection 4 discusses recompilation issues, includ- ing profiling techniques and instrumentation-based \ndata sampling. The detailed description of code specialization appears in Section 5. Section 6 presents \nsome experimental results using a variety of ap- plications and industry standard benchmarking programs \nto show the effectiveness of our dynamic compilation system. Finally we conclude in Section 7. 2. RELATED \nWORK This section discusses prior dynamic optimization systems for Java and other related work.   2.1 \nDynamic Optimization Systems There have been three major dynamic systems for automatic, profile-driven \nadaptive compilers for Java. These can be roughly broken into two categories; the Intel research compiler, \nthe JUDO system [13], and the Jalapefio JVM [3, 4], all follow a compile-only approach, while HotSpot \nTM [30, 37] is provided with an interpreter to allow a mixed execution environment with interpreted and \ncom- piled code, as in our system. The Intel compiler employs dynamic optimization through recom- pilation, \nby providing two different compilers: a fast code generator [1] and an optimizing compiler. As a way \nof triggering reeompila- tion, it inserts counter updating instructions for both method entry point and \nloop backward branches in the first level compiled code. It incurs a continuous bottom-line performance \npenalty. The target code has to be recompiled if we want to remove these instructions overhead. Since \nthe recompiled code is not instrtunented, further re- optimization is not possible in this system. In \ncontrast, our system uses a low-overhead profiling system for continuous sampling op- eration throughout \nthe entire program execution, and thus allows for further reoptimizations, such as specialization. Jalapefio \nis another research JVM implemented in Java itself. They implemented a multilevel reeompilation framework \nusing a baseline and an optimizing compiler with three optimization levels, and they presented good performance \nimprovements in both startup and steady state regimes compared to other non-adaptive configurations or \nadaptive but single level reeompilation configurations. Profile- directed method inlining is also implemented, \nand considerable per- formance improvement is obtained thereby for some benchmarks. Their overall system \narchitecture is quite similar to ours, but the major difference lies in its compile-only approach and \nin how the profiling system works. The compilation-only approach can incur a significant overhead for \nthe system. Although their baseline com- piler was designed separately from the optimizing compiler for \nminimum compilation overhead, the system can result in a large memory footprint. Our system features \na mixed mode interpreter for the execution of many infrequently called methods with no cost in compile \ntime or code size growth. This allows the recompilation system to be more flexible and aggressive in \nits reoptimization pol- icy decision. Their profiling system continuously gathers full run- time profile \ninformation on all methods, including information for organizer threads to construct data structures \nsuch as dynamic call graphs. Our system employs two separate profiling techniques to re- duce the overall \nprofiling overhead. That is, a lightweight sampling profiler focuses on detecting hot methods, and then \nan instnunent- ing profiler collects more detailed information only on hot methods. HotSpot is a JVM \nproduct implementing an adaptive optimization system. It runs a program immediately using an interpreter, \nas in our system, and detects the critical \"hot spots\" in the program as it runs. It monitors program \nhot-spot continuously as the program runs so that the system can adapt its performance to changes in \ntile program behavior. However, detailed information about the program moni- toting techniques and the \nsystem structure for recompilation is not available in the literature. The notion of mixed execution \nof interpreted and compiled code was considered as a continuous compiler or smart JIT approach in [31], \nand the study of three-mode execution using an interpreter, a fast non-optimizing compiler, and a fully \noptimizing compiler was reported in [2]. In both of these papers, it was proven that there is a performance \nadvantage by using an interpreter in the system for bal- ancing the compilation cost and resulting code \nquality, but the problem of the generated code size was not discussed. The Self-93 system [20, 21] pioneered \nthe on-line profile-directed adaptive recompilation systems. The goal of this system is to avoid the \nlong compile pauses and to improve the responsiveness for in- teractive applications. It is based on \na compile-only approach, and for the method recompilation, an invocation counter is provided and updated \nin the method prologue in the unoptimized code. The counters decay over time for reflecting the invocation \nfrequencies to avoid eventually reaching the invocation limit for many unimpor- tant methods. The reeompilation \nalso takes advantage of the type feedback information for receiver class distributions using the pro- \nfiling in the previous version code. Dynamo [8] uses a unique approach, focusing on native-to-native \nruntime optimization. It is a fully transparent dynamic compilation system, with no user intervention \nrequired, which takes an already compiled native instruction stream as input and reoptimizes it at runtime. \nThe use of the interpreter here is to identify the hot paths for reoptimization rather than to reduce \nthe total compilation cost as in our system. Another profile-driven dynamic recompilation sys- tem is \ndescribed in [9] for Scheme. They use edge-count profile in- formation for basic block reordering in \nthe recompiled code for im- proved branch prediction and cache locality. 2.2 Instrumentation Ephemeral \ninsmxnentation [34, 39] is, in principle, quite close to our dynamically installed and uninstalled instrumentation \ntechnique for value profiling. Their method is to dynamically replace the tar- get addresses of conditional \nbranches in the executing code with the pointer to a general subroutine that updates a frequency counter \nof the corresponding edge. The collected data is then used off-line for a static compiler. Our profiling \nsystem, on the other hand, is not limited to the branch target, but applicable to any point of the pro- \ngram by generating the corresponding code for value sampling. Also the instrumentation system is integrated \ninto the fully auto- mated dynamic compilation system. A framework for reducing the instrumentation overhead \nin an on- line system [5] is prototyped in Jalapefio. This technique introduces a second version of the \ncode, called checking code, to reduce the frequency of executing the instrumented code. This will allow \na va- riety of profiling techniques to be integrated in the framework. The main concern is the space \noverhead caused by duplicating the whole method for extra versions for both checking and instru- mented \ncode, although some space saving techniques are described. Our system dynamically attaches only a small \nfragment of the code for value sampling at the method entry points, and thus it is more space efficient. \n2.3 Code Specialization There has been much work in the area of dynamic code generation and specialization, \nmost of which require either source language ex- tensions, such as tee system [32], or programmer annotations \nsuch as Tempo [28], the dynamic compiler developed at the University of Washington [6], and its successor \nsystem, DyC [19]. In these systems, a static compiler performs the majority of optimization work and \nprepares for a dynamic compilation process by generating templates, and a dynamic compiler instantiates \nthe templates at runtime. As a restricted form of specialization, called customization [ 12], the Self \nsystem creates a separate version of a given method for each possible receiver class, relying on the \nfact that many messages within a method are sent to self object. The selective specialization technique \n[15] then corrected the problem of both overspecializa- tion by specializing only heavily-used methods \nfor their most bene- ficial argument classes, and underspecialization by specializing methods on arguments \nother than the receiver. This system resem- bles ours in that it combines static analysis (corresponding \nto our impact analysis) and profile information to identify the most profit- able specialization. However, \ntheir work was focused on converting dynamic calls to static calls to avoid the large performance over- \nhead caused by dynamic method dispatch. Our specialization allows not only method call optimizations, \nbut also general optimizations, such as type test elimination, strength reduction, and array bound check \nelimination, on the basis of specific values dynamically collected. The inlining trials [16] in the Self \nsystem is an attempt to predict the benefit of inlining based on type group analysis. Inlining method \ncalls with special parameter values can be considered an ex- treme case of specialization to a particular \ncall site. Our impact analysis is more general in the sense that it can handle not only method parameters \nbut also global variables such as object instance fields. An analysis to identify so called glacial variables \n[7] is proposed to find good candidates for specialization. However, their analysis is static, and the \nexecution frequency is estimated only by loop nesting level, without using the dynamic profile infolrnation \nas in our system. 3. SYSTEM ARCHITECTURE The goal of our dynamic optimization system is to achieve the \nbest possible performance with a set of currently available optimization capabilities for varying phases \nof application programs, including program startup, steady state, and phase shifts. It also needs to \nbe robust for continuous operation for long-mrming applications. The overall architecture of our system \nis as depicted in Figure 1. We de- scribe each of the major components of the system in the following \nsections. 3.1 Mixed Mode Interpreter Most of the methods executed in Java applications are neither fre- \nquently called nor loop intensive as shown in the results in Section 6.2, and the approach of compiling \nall methods is considered ineffi- cient in terms of both compilation time and code space. The mixed mode \ninterpreter (MMI), written in assembler code, allows the effi- cient mixed execution of interpreted and \ncompiled code by sharing the execution stack and exception handling mechanism between them. It is roughly \nthree times faster than an interpreter written in C. Initially, all methods are interpreted by the MMI. \nA counter for method invocation frequencies and loop iterations is provided for each method and initialized \nwith a threshold value. Whenever the method is invoked or loops within the method are iterated, the counter \nis decremented. When the count reaches zero, it is known that the method has been invoked frequently \nor is computation in- tensive, and JIT compilation is triggered for the method. Thus the JIT compilation \ncan be invoked either from the top entry point of the method or from a backward branch within a loop. \nIn the latter case, the control is directly transferred to the JIT compiled code from the currently interpreted \ncode, by dynamically changing the frame structure for JIT use and jumping to specially generated com- \npensation code. The JIT compilation for such methods can be done without sacrificing any optimization \nfeatures. If the method includes a loop, it is considered to be very perform- ance sensitive and special \nhanding is provided to initiate compila- tion sooner. When the interpreter detects a loop's backward \nbranch, it snoops the loop iteration count on the basis of a simple bytecode pattern matching sequence, \nand then adjusts the amount by which the counter is decremented depending on the loop iteration count. \nIn the case where the iteration count is large enough, the JI.T compi- lation is immediately invoked \nwithout waiting until the counter value reaches zero. The collection of nmtime trace information is another \nbenefit of the MMI for use in JIT compilation, For any conditional branches ] Compile Request Reconapilation \nI ~ ~ Compilation tlot Method Li:~ Profile PI~ a~rofile Data _ Compile Plan ....... Oen ati\u00b0 J,; ' //Z \n ...... Figure 1. System architecture of our dynamic optimization system. encountered, the interpreter \nkeeps the information of whether it is taken or not to provide the JIT compiler with a guide for the \nbranch direction at basic block boundaries t. The trace information is then used by the JIT compiler \nfor ordering the basic blocks in a straight- line manner according to the actual program behavior, and \nfor guid- ing branch directions in partial redundancy optimizations [26].   3.2 Dynamic Compiler The \ndynamic optimizing compiler has the following optimization levels. Quick (1st level) optimization employs \nonly a limited set of the optimizations available. Basically, optimizations causing higher costs in compilation \ntime or greater code size expansion are dis- abled. For example, only those methods whose bytecode size \ndoes not exceed the size of a typical method invocation se- quence will be inlined. This saves the compilation \noverhead not only of the method inlining process, but for the later optimiza- tion phases which traverse \nthe entire resulting code block. The guarded or unguarded devirtualization of method calls is applied \nbased on the class hierarchy analysis [23, 24]. The maximum number of iterations in the dataflow-based \noptimizations is also reduced. These optimizations involve iterations over several components, such as \ncopy propagation, array bound check elimination, null pointer check elimination, common subexpres- sion \nelimination, and dead code elimination. \u00ae Full (2nd level) optimization employs all optimizations avail- \nable. Additional and augmented optimizations at this level in- elude full-fledged method inlining, escape \nanalysis (including stack object allocation, scalar replacement, and synchronization elimination), an \nadditional pass for code generation and code scheduling, and DAG-based loop optimization. The iteration \ncount for dataflow-based optimizations is also increased. \u00ae Special (3rd level) optimization applies \ncode specialization, a feedback-directed optimization, in addition to the same set of optimizations as \nin the previous level. This is described in detail t Since keeping trace information every time can cause \nadditional over- head, the branch instruction is converted to the corresponding quick in- struction after \nbeing executed a fixed number of times in order to mini- mize the performance penalty. in Section 5. \nThe intemal representation is common for all optimization levels. The differences in compilation time, \ngenerated code size, and gen- erated code's performance quality between the quick optimization and full \noptimization versions can be found in the experimental re- suits presented in Section 6.1 and 6.2. The \nreason that we provide three optimization levels in our dy- namic compiler is twofold. First, one level \nof compilation model is, in our experience, simple and still effective until a eertain level of optimization \nin the presence of MMI. However, as more so- phisticated and time-consuming optimizations are added for \npur- suing higher performance, more of the negative side of the dy- namic compilation (that is, the compilation \noverhead and code size growth problems) starts to appear. Even if more expensive optimizations are implemented, \nthe return is diminishing and the net performance gain becomes marginal. This is considered to be due \nprimarily to the larger gap between the interpreter and the compiler regarding the level of tradeoff \nbetween compilation cost and the resulting performance. If we set a lower threshold for trig- gering \ncompilation, we may have better performing compiled code earlier but more total compilation cost is incurred. \nIf we set a higher threshold value, we may miss some opportunities for gain- ing performance for some \nmethods due to delayed compilation. There is also a problem with application start'up performance deg- \nradation with one level of a highly optimizing compiler. It is therefore desirable to provide multiple, \nreasonable steps in the compilation level with well-balanced tradeoffs between the cost and the expected \nperforrnanee, from which an adequate level of optimization can be selected corresponding to the eurrent \nexecu- tion context. Secondly, it is not clear whether it would be effective to have more than three \nlevels of optimization in the dynamic compilation system, without knowing the exact relationship between \neach component of the optimization on performance and compilation cost. Having more levels of optimization \nwould make more choices available for reeompilation. However it would complicate the selection process \nand more informative profiling data would be necessary to make correct decisions, which might add more \noverhead. Furthermore, the resulting code may or may not be of better quality depending on the target \nmethods. The gradual pro- motion with finer steps of optimization can result in more code expansion rather \nthan any overall performance benefit. The re- suits shown in Section 6.1, combined with the interpreter \navail- able in ottr system, suggest that the current classification of three levels of optimization can \nprovide an adequate tradeoff for reason- able promotion for recompilation decisions. 3.3 Sampling-Based \nProfiler The sampling-based profiler [40] gathers information about the program threads' execution. This \nprofiling collector keeps track of methods where the application threads are using the most CPU time \nby periodically snooping the program counters of all of the threads, identifying which methods they are \ncurrently executing, and incrementing a hotness counter associated with each method. Since the MMI has \nits own counter-based profiling mechanism, this sampling profiler only monitors compiled methods for \nreopti- mization. The hot methods identified by the profiler are kept in a linked list, sorted by the \nhotness counter, for use by the recom- pilation controller in deciding on method recompilation. To mini- \nmize the bottom-line overhead, the profiler doesn't operate by constructing and maintaining a call context \ntree for every sam- pling time interval, which would involve traversing the stack to a certain depth. \nInstead, additional information such as caller-callee relationships is collected by instrumentation code \nonly for meth- ods considered as candidates for recompilation as described in Section 3.5. This two-stage \nprofiling design results in low overhead for the sampling profiler and hence allows continuous operation \nduring the entire program execution with virtually no performance penalty. 3.4 Reeompilation Controller \nThe recompilation controller, which is the brain of the recompila- tion system, takes as input from the \nsampling profiler the list of hot methods and makes decisions regarding which methods should be recompiled. \nThe recompilation requests, as the results of these decisions, are put into a queue for a separate compilation \nthread to pick up and compile asynchronously. The controller also directs the instrumenting profiler \nto install in- strumentation code for further profile information such as method return addresses for \ncollecting call site distribution for those hot methods. Some parameter values can also be collected \ndepending on the results of impact analysis done in the full optimization compilation phase (described \nin Section 5). This additional profile data is useful in guiding more effective optimizations, such as \nmethod inlining and code specialization.  3.5 Instrumenting Profiler The instrumenting profiler, according \nto an instrumentation plan from the recompilation controller, dynamically generates code for collecting \nspecified data from a target method, and installs it into the compiled code. The entry instruction of \nthe target code, after it is copied into the instrumenting code region, is dynamically patched with an \nunconditional branch instruction in order to direct control to the generated profiling code. The instrumentation \ncode records the caller's return address or values of parameter and object fields in a table and then \njumps back to the next instruction after the entry point. The data table or a counter for storing infbr- \nmation is allocated separately to be passed back to the controller. After collecting a predetermined \nnumber of samples, the generated code automatically uninstalls itself from the target code by restor- \ning the original instruction at the entry point. The information that is collected and recorded by the \ninstrumenta- tion code can range from a simple counter (such as zero, non-zero, or array type), which \njust counts the number of executions, to a form of table with values or types of variables and their \ncorre- sponding frequencies. Unlike instrumentation code found in other systems [13], this technique \nallows dynamic installation and unin- stallation without involving target code recompilation. Since the \ntarget method and sampling numbers are controllable, the over- head of the instrumentation is relatively \nlightweight, unlike sys- tems where the instrumentation code is generated as part of the compiled code \nwhich always incurs overhead. 4. RECOMPILATION The key to our system is to make correct and reasonable \ndecisions to selectively and adaptively choose methods for each level of opti- mization. From the mixed \nmode interpreter to the 1st level compila- tion, the transfer is made on the basis of the dynamic count \non invo- cation frequencies and loop iterations, with additional special treat- ment for certain types \nof loops. The request for 2nd level and 3rd level recompilation from lower level compiled code is through \nthe sampling profiler. The compilation and reeompilafion need to be done from one level to the next, \nand there is currently no direct path skipping intermediate levels from interpreter mode or compiled \ncode. The reason we chose two different ways of method promotion comes from the consideration of the \nadvantages and disadvantages of the two profiling mechanisms: sampling-based and counter- based. For \nthe interpreter, the cost of counter updates is not an issue, given the inherently higher overhead of \ninterpreted execution, compared to additional code for counter maintenance. Instead, the accuracy of \nthe profiling information is rather important, because the large gap in performance between interpreted \nand compiled code means the performance penalty could be large if it misses the optimum point to trigger \nthe 1 st level compilation. This tradeoffbe- tween efficiency and accuracy can be measured using counter-based \nprofiling. On the other hand, compiled code is very performance sensitive, and inserting counter updating \ninstructions in this com- piled code could have quite a large impact on total performance. Lightweight \nprofiling is much better for continuous operation. Since the target method is already in a compiled form, \na certain loss of accuracy in identifying program hot regions, which may cause a delay in recompilation, \nis allowable. Sampling-based profiling is superior for this purpose. 4.1 Reeompilation Request Since \nthe quick optimization compiler generates code with virtu- ally no method inlining, the sampling profiler \ncollects information on the set of hot methods as individual methods. A simple-minded recompilation request \nfor those methods can result in unnecessary recompilations, since some methods included in the list may \nbe inlined into another during, the full optimization compilation [21 ]. 184 This can happen because \nthe hot methods appearing in the list come from sampling during the same stage in the program's exe- \ncution, and therefore can be closely interrelated. Instead of simply requesting recompilation for each \nof the methods, the controller first constructs call graphs, structures representing the caller- callee \nrelationships, from the list of hot methods. This requires in- formation about the call sites' distributions \nfor each method. Then only those methods which are roots in one of the graphs are pushed into the compile \nrequest queue with appropriate inlining directions for methods appearing in the subgraph below the root. \n 4.2 Multiple Version Code Management After the recompilation is done for a method, it is registered \nby a runtime system called the code manager, which controls and man- ages all the compiled code modules \nby associating them with their corresponding method structures and with a set of information such as \nthe compiled code optimization level and specialization context. This means all the future invocations \nto this method through indirect method lookup will be directed to the new ver- sion of the code, instead \nof the existing compiled code. Static and nonvirtual call sites are exceptions, where direct binding \ncall in- structions to old version code have been generated. A dynamic code patching technique is used \nin order to change the target of the direct call. This is done by putting a jump instruction at the entry \npoint of the old code to a runtime routine, which then up- dates the call site instruction to direct \nthe flow to the new code us- ing the return address available on the stack, so that the direct in- vocation \nof the new code will occur from the next call. For those threads currently executing old version code, \nthe new optimized code will be used from the next invocation. We cur- rently do not have a mechanism \nfor on-stack replacement [21 ], the technique of dynamically rewriting stack frames from one optimi- \nzation version to another. Also the problem of cleaning up the old version code remains in our system. \nThe major difficulty with old code is how to guarantee that no execution is currently or will be in the \nfuture performed using the old compiled code. The apparent solution would be to eagerly patch all the \ndirect bound call sites, rather than to patch lazily as in our current implementation, and then to traverse \nall the stack frames to ensure no activation record exists for this old code. This traversal would be \ndone at some ap- propriate time (like garbage collection time). 5. CODE SPECIALIZATION In this section, \nwe describe the design and implementation of code specialization, as an interesting feedback-directed \noptimization technique. This optimization is applied for the methods already compiled with the full optimization \nlevel. Figure 2 shows the flow of control regarding how the decision on code specialization will be made. \nCurrently code specialization is applied for an entire method, not for a smaller region in the method \nsuch as a loop. 5.1 Impact Analysis Since overspecialization can cause significant overhead in terms \nof both time and space, it is important to anticipate before its ap- plication how much benefit it can \nbring to the system and for what values. Impact analysis, a dataflow-based routine that detects op- pomanities \nand estimates the benefit of code specialization, is used to make a prediction as to how much better \ncode we would l Full Opt Compiler-] F Codooneration ~ ' ~ Hot I Sampling ~ - ~ --Installl/Uninstall \nI P\u00b0cializati\u00b0nl Sa gDa \"< ........ ~.Nanning ) Variable--': - - m High Impact .... Specialization \nRequest Figure 2. Flow of specialization decisions. be able to generate if we knew a specific value \nor type for some variables. The impact analysis is done during the 2nd level compi- lation and the result \nof the analysis is stored into a persistent data- base, so that the controller can make use of it for \nthe next round of reeompilation plans. The specialization targets can be both method parameters and non-volatile \nglobal variables 2, such as static fields and object in- stance fields. The set of specialization targets \nfor global variables within a method can be computed from In (n) and Kill (n) for each basic block n \nafter solving the forward dataflow equations given below: In (entry_bb): All non-volatile global variables \nwithin the method. Kill (n): The set of global variables that can be changed by in- structions in basic \nblock n: Out (n) = In (n) -Kill (n) In (n) = fq Out (m) (for n e entry basic block) m ~ Pred(n) This \nmeans that In (n) is the set of global variables referenced within the method and guaranteed not be updated \nalong any paths reachable from the top of the method to the entry of the basic block n. Each global variable \nreference within the basic block can be checked as to whether it can be included in the specialization \ntarget from the Kill set for each instruction. That is, the above equation computes all the global variables \nthat are safe forprivati-zation at the entry point for each method. The set of specialization targets \nfor the global data, together with the argument list, is then fed to the impact analysis. The pseudocode \nof the impact analysis is shown in Figure 3. Each specialization target can be expressed by a triple \n(L, S, V), where L denotes the defined local variable from a parameter or a z For a variable declared \nvolatile, a thread must reconcile its working copy of the field with the master copy every time it accesses \nthe variable [18]. Specialization Target (L, S, V) L: defined local variable from a parameter or a global \nvadable S: statement where L is defined V: parameter or global variable for specialization for each \nelement (L, S, I/) in Specialization Target List { Derived[ V] = { (L, S) }; Weight[ V, * ] = O; for \neach variable (L, S) in Derived [ V] { for each operation Opwhich uses L and is reachable from S { if \n(Op can be simplified or eliminated by a specialization type T ) Weight[ V, T ] += Impactof T on Op; \nif ( Opis converted to a constant assignment ) { Derived[ V] U = (LHSvar of Op, location of Op) }  \n } } } } Figure 3. Pseudocode for impact analysis. global variable, S denotes the statement in the \nmethod in which the variable L is defined, and V denotes the parameter or global variable for specialization. \nThe algorithm traverses the dataflow through a defuse chain for each use of the variable L and its de- \nrived variable, tracking any possible impact on each operation in V. The impact of the specialization \ntype T on operation Op ap-pearing in the pseudoeode can be expressed by lmpact(Op, T) = saved cost(Op, \nT)/SST(E, T) * f(loop nest level) The baseline of the impact is the execution cost of the Specializa-tion \nSafety Test (SST), which is the guard to be generated at the entry of the specialized method. This can \nvary from a simple ecru- pare and jump instruction to a set of multiple instructions depend- ing on the \nvariable V and type T. The impact is then the relative cost that can be saved on operation Op with the \nspecialization type T against its SST eost. If the operation is located within a loop, then the impact \nis sealed with a factor representing the loop iterations to reflect the greater benefit that can be expected. \nThe cost saving can be quite different for each of the operations. The elimination of checkcast or instancoof \noperations can have a large effect, while it would be much smaller when getting a con- stant operand \nin a binary operation. The final result of impact analysis for the estimated specialization benefit is \nthe Specializa-tion Candidate SC; SC = { V I Weight(V, T) > minimum threshold for any type T } The factors \ncurrently considered in the impact analysis include the following: A constant value of a primitive type, \nwhich can lead to addi- tional opportunities for constant folding, strength reduction, the replacement \nof floating point transcendental operations with a computed value, and the elimination of the body of \na Candidate Sample Bias Weight Expected Benefit SC= R= W= B, SCb R~ Wh Bb SC~ R, We ] Bc (a) spe, Vel't \nSC, ^B c Wlm 0 t~ a\"')~ b specialized version (b) with SCa ^ SCb ^ SC c Figure 4. (a) Multiple specialization \ncandidates available, (b) Construction of decision tree on generating three specialized versions. conditional \nbranch. O An exact object type, which allows removal of some unneces- sary type checking operations \nand leads to more opportunities for class-hierarchy-based devirtualization 3. Q The length of an array \nobject, which allows us to eliminate ar- ray bound checking code. This can also contribute to loop transformations, \nsuch as loop unrolling and simplification, if the loop termination condition becomes a constant. \u00ae The \ntype of an object such as null, non-null, normal object, or array object, which can be used for removing \nsome unneces- sary null checks and for improving the code sequence for some instructions (e.g. invokevirtualobject, \ncheckcast). Equality of two parameter objects, allowing method bodies to be significantly simplified. \n\u00ae A thread local object, which allows us to remove unnecessary synchronizations. 5.2 Specialization Decision \nWhen a hot method has been identified in the 2nd level compiled code, the controller checks the results \nfrom the impact analysis stored in the code manager database. If a candidate in SC for the method looks \npromising as a justification for performing speciali- zation, then the controller dynamically installs \nthe instrumentation code into the target native code to decide whether it is indeed worth specializing \nwith the specified type. This is based on the same mechanism described in Section 3.5. Currently a minimum \nthreshold is used to allow all candidates to be selected as an in- strumentation installation target. \nUpon the completion of the value sampling, the controller then makes a final decision regarding whether \nit is profitable to spe- cialize with respect to a specialization candidate in SC. The metric 3 With \nthe preexistence optimization [17], some of this opportunity for parameter variables may be disappeared. \nTable t. List of benchmarks and applications used in the evaluation. Program Description SwingSet GUI \ncomponent, version 1.1 Java2D 2Dgraphics library ICE Browser [42] Simple intemet browser, version 5.05 \nHot Java [38] Hot Java browser, version 1.1.5 r,o IchitaroArk [25] Japanese word processor Web,Sphere \n[22] Web applica!ion server, version 3.5.3 227 mtrt Multi-threaded image rendering _202 jess Java expert \nsystem shell _201compress LZW compression and decompression 209 db Database function execution [ 222 \nmpegaudio Decompression of MP3 audio file 228 jack Java parser generator _213 javac Java source to bytecode \ncompiler in JDK1.0.2 SPECjbb2000-1.0 [ Transaction processing benchmark we use for this decision can \nbe expressed as follows: f (Weight, Sample Ratio, Code Size, Hotness Count) This function indicates \nthat the impact analysis result, the ratio of bias in the corresponding sample data, the size of the \nrecompile target code, and the method hotness count are all considered for the final specialization decision. \nThe code size affects the maxi- mum number of versions that can be produced for specialization, since \nthe larger the code size for recompilation, the more costly it would be to generate multiple versions. \nThe method hotness count is used for adjusting the number of versions allowed for some im- portant methods. \nThe construction of the specialization plan then proceeds as follows. Suppose there are three specialization \ncandidates SC~ (e SC for i = a, b, c) available for a method as in Figure 4 (a). The expected benefit \nfor each candidate using specialized code, based on the probability of the specialized code hit ratio, \nis computed as Bi = Ri * W~. The plan on how to organize the specialized code can then be viewed as constructing \na decision tree. That is, each internal node of the tree represents the specialization safety test SST~ \nguarding each specialization. The right subtree is for the version where the benefit Bi is applied, and \nthe left subtree tracks the ver- sion where it is not used. The number of leaf nodes is two raised to \nthe number of candidates. The specialization is then organized by selecting leaf nodes, from right to \nleft, for as many as the num- ber of versions allowed as calculated from the code size and method hotness \ncount. All the nodes that are not selected for spe- cialization are contracted to a single node that \nrepresents a general version of the code (that is, the original 2nd level compiled code). Two strategies \ncan be considered for the tree construction: benefit ordered and sample ratio ordered. In the benefit-ordered \nconstruc- tion, a specialization candidate having a larger value of B~ moves to a higher level internal \nnode, reflecting a greater expected bene- fit when the condition holds true. In the sample-ratio-ordered, \na Measurement Condition Run the demo program as an application to bring up the initial window. Run the \ndemo program as an application with options -runs=l -delay=0 Run the browser application to bring up \nthe welcome window. Run the application to bring up the initial input window Attach administration console \nafter starting administration server Run SPECjvm98 benchmark harness from appletviewer with the following \nsettings (the order of these tests is specified in SpecApplet.html). - initial heap size 96m and max \nheap size 96m. -run individual benchmarks in the experiments from Section 6.1 to 6.3, or run complete \nbenchmarks with a single JVM in the experiment for Section 6.4. , -select input size=100, then run with \nautorun, and test mode (not SPEC compliant mode). - the number of executions in each autorun sequence \nis 10. Run with warehouse 1 only, with initial and max heap size 256m. value of R~ is regarded as a more \nimportant factor with expectation of a higher rate of executing the specialized versions. In Figure 4 \n(b), assuming that SCI is in the order of a, b, c in either criteria, the decision-tree is constructed \nwith the number of specialized versions limited to three. The recompilation with code specialization \ncan introduce addi- tional opportunities for many other optimizations, such as con- stant folding, null-check \nand array-bound-check elimination, type check elimination or simplification, and virtual method inlining. \nIn the case of SST failure upon specialized code entry, the control is directed to the general version \nof the code. This can generally occur as the result of changes in the execution phase of the pro- gram, \nso that the specialized methods can be called with a differ- ent set of parameters. In this situation, \nthe general version of the code may be identified as hot again, and the next round of recom- pilation \ncan be triggered for this method, possibly with specializa- tion using different values. Thus the new \nversion of specialized code can then become active, by replacing the previous code. The maximum number \nof versions for specialized code for a method is limited to N, a number which is based on the target \ncode size, set by the recompilation controller to avoid excessive code growth.  6. EXPERIMENTAL RESULTS \nThis section presents some experimental results showing the effec- tiveness of our dynamic optimization \nsystem. We outline our ex- perimental methodology first, describe the benchmarks and applica- tions used \nfor the evaluation, and then present and discuss our per-formance results. 6.1 Benehmarking Methodology \nAll the performance results presented in this section were obtained on an IBM IntelliStation (Pentium \nIII 600 MHz uni-processor with 512 MB memory), nmning Windows 2000 SP 1, and using the JVM of the IBM \nDeveloper Kit for Windows, Java Technology Table 2. Comparison of compilation time with three different \noptimization levels on SPECjvm98 benchmark programs. All results are in seconds. mtrt jess compress db \nmpegaudio jack ] ,, !avac first run 22.14 16.02 no opt best run 21.25 14.31 fast run 8.38 10.5 quick \nopt best run 6,53 7.83 compile 1.85 2.67 time (22.1%) (25.5%) first run 11.34 20.91 full opt best run \n5.03 7.06 compile 6.31 13.85 time (55.6%) (66.2\u00b0/,) first rtm 13.21 21.88 special opt best mn 4.89 6.51 \ncompile 8.32 15.37 time (63.0%) (70.2%) Edition, Version 1.3.1 prototype build. The benchmarks we chose \nfor evaluating our dynamic optimization system are shown in Table 1 (the size of each benchmark is indicated \nin the MMI-only row in Tables 3 and 4). We conducted two sets of measurements: startup runs and steady \nstate runs. For the startup performance evaluation, we selected a variety of real-world applications, \nranging from a sim- ple Intemet browser to a complex Web application server. For evaluating the steady \nstate performance, we use SPECjvm98 and SPECjbb2000 [35], two industry standard benchmark programs for \nJava. Table 2 shows the compilation times for the three different opti- mization levels used in our system. \nThe numbers for no-opt com- pilation are additionally presented to evaluate our system against the compile-only \napproach. These measurements were done by nmning each level of optimization without MMI. We assume that \nthe difference between the first nm and the best run of SPECjvm98 can be regarded as the compilation \ntime. We ignored the cache and file system effects, which we tried to minimize by executing some warm-up \nruns before the measurements. The per- centages shown in the compilation time fields are the ratios of \ncompilation time over the time of the first run. The following configuration and parameters were used \nthroughout the experiments. The threshold in the mixed mode interpreter to initiate dynamic compilation \n(quick optimization) was set to 2,000.  The timer interval for the sampling profiler for detecting hot \nmethods was 10 milliseconds. With this interval, the overhead introduced is below the noise and the accuracy \nis considered adequate for identifying reeompilation candidates [40].  The sampling profiler was operated \ncontinuously to build a list of hot methods, while the controller examined and purged the data every \n100 sampling ticks for recompilation decisions.  \u00ae For instrumentation-based sampling for hot methods, \na compile 0.89 1.71 time (4.0%) (11.0%) 29.52 36.81 28.36 17.22 22.58 29.08 36.52 26.97 I5.94 19.81 \n0.44 0.29 1.39 1.28 2,77 (1.5%) (0.8%) (4,9%) (7.4\u00b0/6) (12.3%) 17.82 29.63 14.01 10,47 17.92 17.39 29.14 \n11.89 8.31 12.18 0.43 0.49 2.12 2.16 5.74 (2.4%) (1.7%) (15.1%) (20.6%) (32,0%) 16.81 30.22 14.17 15.95 \n37.25 15.84 28.83 9.91 7,88 12.22 0.97 1,39 4.26 8.07 25.03 (5.7%) (4.6%) (30.1%) (50.6%) (67.2%) 16.88 \n30.81 14.56 16.56 40.41 15.83 28.51 9,47 7.46 12,08 1.05 2.30 5.09 9.10 28.33 (6.2%) (7.5%) (35.0%) (55.0%) \n(70.1%) maximum of 512 values were collected for each of the target pa- rameters, global variables, \nor return addresses. The maximum number of data variations recorded was 8. @ The priority of the sampling \nprofiler thread and the compilation thread was set to above and equal to that of the application threads, \nrespectively. The number of code duplications allowed for specialization was set to one, regardless \nof the target method code size. Jn the measurement for Section 6.3, the decision-tree construction was \nbased on the benefit-ordered strategy. @ Exception-directed optimization (EDO) [29] was enabled. The \nrecompilation request from EDO profiler was processed with quick optimization with special treatment \nfor method inlining for the specified hot exception paths.  6.2 Evaluation of Reeompilation System \nThere are different requirements for the best performance between the two phases of application execution, \nprogram startup and steady state. During the startup time, many classes are loaded and initialized, but \ntypically these methods are not heavily executed. When the program enters a steady state, a working set \nof hot methods will appear. In our experiment, we evaluated the startup performance by running each of \nthe listed applications individu- ally and measuring the timing from the issuing of the command until \nthe time the initial window appeared on the screen. For Java2D with the options indicated in the table, \neach tab on the window is automatically selected and the execution proceeds until the whole component \nselection is done. That is, we measured the timing from the issuing of the command until the program \ntermi- nates. For WebSphere, we first started the server process, and then measured the time to bring \nup the administration console window which runs on the same machine. For the steady state measure-ment, \nwe took the best time from 10 repetitive automns for each test in SPECjvm98. For SPECjbb2000, we chose \nthe ~] MMI-only [] noopt-full(reeompile) [] MMI-full .> Method Size Method Size Method MMI-only 7,273 \n440.5 7,327 531.5 3,410 no-opt 7,273 3,218.5 7,327 3,884.3 3,410 ~[ quick-opt 6,444 3,096.7 6,511 3,473.6 \n2,905 full-opt 3,701 4,518.6 3,985 5,003,8 t,848 quick opt 439 233.8 523 281.1 75 full-opt 416 347.6 \n463 416.2 69 configuration of one warehouse with 60 seconds of rampup time, longer than the standard \nSPEC rule, to give the system enough warm-up time. We compared the following sets of compilation schemes. \nI. MMI only 2. no optimization compilation with no MMI (noopt-only) 3. quick optimization compilation \nwith no MMI (quick-only) 4. full optimization compilation with no MMI (full-only) 5. no optimization \ncompilation with no MMI and recompilation using full optimization compilation (noopt-full) 6. quick \noptimization compilation with MMI (MMI-quick) 7. full optimization compilation with MMI (MMI-full) \n8. all levels of compilation with MMI for adaptive re, compilation (MMI-all)  The fifth case is provided \nas a comparison of our system to the corresponding recompilation system with the compile-only ap- proach \nas in other systems [3, 13]. In this case, the MMI was not executed and all methods were first compiled \nby the compiler with no optimization applied (level 0), and then some hot methods identified by the sampling \nprofiler were reoptimized with the full optimization compilation. Our no-opt compilation may have SwingSet \nJava2D ICE Browser HotJaval15 lchitaroArk WebSphere Geometric Mean Figure 5. Startup performance comparison. \nEach bar indicates the total execution speed relative to no opt compiler without MMI. Therefore higher \nbar shows better performance. Table 3. Comparison of number of compiled methods and generated code size \n(Kbytes) in program startup. The MMI-only row indicates the number of executed methods and bytecode size. \nNative methods are not counted. SwingSet Java2D ICE Browser HotJava IchitaroArk WebSphero Size Method \nSize Method Size Method Size 275.7 4,110 297.5 7,282 473.7 9,029 615.7 1,791.7 4,110 2,110.5 7,282 3,581.1 \n9,029 4,511.3 1,556.2 3,599 1,945.8 6,193 3,307.3 7,951 4,190.2 2,438.6 2,191 3,019.1 3,910 4,826.3 4,898 \n6,165.4 78.1 91 65.6 205 124.7 321 1653 131.5 85[ 128.9 194 217.7 301 350.9 different characteristics \nin terms of both compilation overhead and code quality from the baseline compiler [3] or the fast code \ngen- erator [1, 13], especially because our no-opt compilation system is not a separate compiler, while \ntheirs are designed and imple- mented differently from the optimizing compilers. Nevertheless, we think \nthe comparison with this configuration can be an indica- tion as to how well our system can compete against \na compile- only system. Figures 5 shows the comparisons of program startup performance. The base line \nin this graph is the second ease above, no optimiza- tion compilation with no MMI (noopt-only). The chart \nindicates that the performance of our dynamic optimization system, MMI- all, is almost comparable to \nthat of the lightweight configuration of the MMI-quiek. On the other hand, no-MMI configurations show \npoor performance in all the programs, probably due to the high cost of compiling all executed methods. \nThe recompilation system with the compile-only approaeh, noopt-full, shows better performance than the \nother no-MMI configurations, but it still cannot compete against other top performing configurations; \nThe fact that the performance of MMI-only is nearly two times faster in average than that of noopt-only \n(base line of the graph) 3 --- 2 -- > 1 -- mtrt jess compress db mpegaudio jack javac SPECjbb Geo. \nMean Figure 6. Steady state performance comparison. Each bar indicates the total execution speed relative \nto no opt compiler without MMI. Therefore higher bar shows better performance. Table 4. Comparison of \nnumber of compiled methods and generated code size (Kbytes) in steady state. The MMI-only row indicates \nthe number of executed methods and bytecode size. Native methods are not counted. H indicates that our \nMMI is reasonably fast and is comparable to no- opt compiled code. Table 3 shows both the number of compiled \nmethods and the code size in these program startup runs. In this table, the numbers for reeompilation \nsystem, both noopt-full and MMI-all, are not pre- sented, since no recompilation activity occurred in \nthe program startup phase and therefore the numbers are mostly similar to those of their corresponding \nbaseline configurations, noopt-only and MMl-quick, respectively. The differences in the number of compiled \nmethods among no MMI cases are primarily caused by the varying degrees of applying method inlining at \neach optimiza- tion level. The table shows the significant differences in the num- ber of compiled methods \nbetween no-MMI and with-MMI bb ,,o93 153.7 267.5 447.4 configurations. With MMI, only 2-7% of the executed \nmethods are compiled, even considering the effects of method inlining. As for the generated code size, \nit is an order of magnitude larger with no-MMI configurations than with MMI configurations. The code \nexpansion factor from the bytecode size can be up to 10x without MMI, while it is less than lx for with-MMI \nconfigurations. Figure 6 is the corresponding performance chart for the steady state program runs, and \nTable 4 shows the numbers for compiled methods and code size at that time. In Table 4, the two configura- \ntions that involve recompilation are shown with the numbers for each level of compilation separately. \nAlso MMI-all configuration is indicated with the number of methods targeted by the instru- menting profiler \nand its code and table space for specialization 7 .... 6 &#38;#169; 4 .................................................~ \n.................. ~ ................ t B N=tmlimited ~ ---- 3 ........ 2 1 _ 0  mtrt jess compress \ndb mpegaudio jack javac SPECjbb Geo. Mean Figure 7. Performance improvement by code specialization for \nthree different N (max. number of specialized versions for a method). Each bar indicates the percentage \nof improvement over the system with code speeialization disabled. Table 5. Statistics of code specialization \non SPECjvm98 and SPECjbb2000. mtrt jess # of specialized versions (methods) 6 (6) 8 (8) it % of code \nsize increase 18.9 7.7 Z % of hit ratio for speeializeal versions 100 80.5 # of specialized versions \n(methods) 10 (6) 12 (8) % of code size increase 21.3 12.7 % of hit ratio for specialized versions 100 \n92.7 # of specialized versions (methods) 20(6) 18(8) % of code size increase 40.4 19.3 % of hit ratio \nfor specialized versions 100 98.6 value profiling. From Figure 6, three configurations, full-only, MMI-fuU \nand MMI-alI, are top performers in this category. The recompilation system with compile-only approach, \nnoopt-full, also works quite well for many of the tests here by applying full optimization on performance-critical \nmethods. Currently the same parameters for reeompilation decision are used in this configura- tion as \nthose in MMI-all, and thus we think the performance can be further improved to the level of other full \noptimization configu- rations by adjusting the parameters more appropriately. The following observations \ncan be made from Table 4 for our MMI-enabled dynamic recompilation system. First the number of methods \ncompiled with quick optimization is, except for jack and javac, at most 30% of the total number of methods, \namong which the number of recompiled methods is roughly 10 to 15%. There- fore we can achieve the high \nperformance attained by the full-only configuration by focusing on merely 3 to 4% of all methods. As \ndescribed in [2], javac has a fiat profile, involving many methods that are executed, and thus poses \na challenge for the recompilation system. This characteristic caused a relatively higher number of quick \noptimizations in our system, 70% of the total methods exe- cuted. However, the number of fully optimized \nmethods is around 4%, similar to the other test cases, showing that our recompilation decision process \nworks quite well. As for Jack, the higher count of methods with quick optimization is caused by the additional \nre- compilation request from EDO, after detecting some hot exception paths as inlining candidates, and \nthis results in the better performance as shown in Figure 6. compress db mpegaudio jack javac SPECjbb \n2 (2) 3 (3) 15 (15) 9 (9) 13 (13) 13 (13) 6.9 13.8 23.6 [ 8.6 11.7 9.4 60.2 100 99.7 95.7 51.8 90.3 3 \n(2) 5 (3) 29 (15) 13 (9) 16 (12) 18 (13) 9.7 18.6 33.4 12.6 23.3 10.3 99.9 I00 99.7 96.8 79.2 93.5 4(2) \n7(3) 63(15) 17(9) 22(12) 22(13) 14.2 27.8 79.4 16.7 26.2 13.4 100 100 99.9 97.5 89.8 97.8 Second, the \nincrease in the size of the compiled code is small in comparison to that of the MMI-quick configuration, \nand the total size of all levels of compilations and the instrumenting profiler combined is well below \nthat of the MMI-full configuration. The largest size for the reeompiled code (for 2nd and 3rd) is for \nSPECjbb2000, but it is still much less than that of the bytecode. Again the compile-only approach shows \na problem with the large size of the compiled code. The expansion factor from the bytecode size with \nno MMI configurations is from 7x to 10x, while it is around 3.5x on average, including the space overhead \nby the in- strumenting profiler, with our MMI enabled recompilation system. Overall our dynamic optimization \nsystem adapts very well to the requirements of both program startup and steady state perform- mace, and \nalso has strong advantages in terms of the system mem- ory footprint.   6.3 Code Specialization Figure \n7 shows the percentage performance improvement from the code specialization over the system with 3rd \nlevel optimization disabled. The measurement was done with the same conditions as in the steady state \nperformance runs in the previous subsection. Three cases are shown for the specialization parameter N, \nthat is the maximum number of specialized versions per method was set to one, two and unlimited. Table \n5 shows the number of special- ized versions produced, the percentage increases in code size, and 86/410 \n132/13/0 I17/14/5 1201210 45619/0 Q 2d o -0.5 > O r,O mtrt jess compress db mpeg jack javac Figure 8. \nChange of compilation activity and the program execution time as program shifts its execution phase. \nBar chart (left Y axis) shows the number of compiled methods for quick, full, and special optimizations, \nand line graph (right Y axis) shows execution time ratio from 1st to 10th runs quick-opt full-opt special-opt \n the ratio showing the frequency of specialized code entry test, SST, succeeded, for all of the three \ncases. A modest performance improvement, from 3 to 6%, can be ob- served for four benchmarks, while others \ndo not show any signifi- cant difference. For those benchmarks which are sensitive to this specialization, \napproximately half of the 2nd level compiled code was specialized, and a 7 to 30% code size growth was \nobserved for the cases of small number of specialization parameter N. The increased code size for mpegaudio, \ndb, and javac seems to be excessively high relative to the resulting perfbrmance gain. One of the reasons \nfor this problem is that the target of specialization in our current implementation is the whole method, \nrather than a smaller region in the method. When specialization is applied for a part of the method, \na technique called method outlining (in con- trast to method inlining) needs to be explored to allow \nmultiple versions of specialized code to be generated for that part of the method. The hit ratio of \nspecialized versions code is quite high overall, considering the fact that only a limited amount of data \nsampling is performed in our instrumentation-based value profiling. This is because the variation of \ndata for parameters or global variables is relatively small within a single benchmark. Jess is the only \nex- ception, that shows significant performance gain by producing multiple specialized versions from \na single method. Three benchmarks do not show any performance improvement from code specialization. Two \nof them, compress and db, have spiky profiles and only a few methods are heavily executed. But our impact \nanalysis could not find any good candidates for spe- cialization among the hot methods. On the other \nhand, javae has many equally important methods, and specializing only a few of them does not seem to \nprovide any additional speedup. 6.4 Compilation Activity In Figure 8, we show how the system reacts \nto changes in the pro- gram behavior with our dynamic optimization system. This was measured by running \nall the tests included in the SPECjvm98 with autorun mode, ten times each with a single JVM. The horizontal \naxis of the graph is equally partitioned by each run of the tests. The bar chart indicates the number \nof compiled methods with each level of optimization, and the line graph indicates the changes of the \nexecution time from the first to the tenth run nor- malized by the time differences 4. That is 1 corresponds \nto the first run and 0 corresponds to the tenth ran. In the case of compress and db, the best timing \nappeared in the earlier runs, however, the irregularities in the graph after the best runs can be considered \nnoise, since no compilation activity occurred. The graph shows that the system tracks and adapts to the \nchanges in the application program behavior quite well. At the beginning of each test, a new set of classes \nis loaded and the system uses the quick optimization compilation for a fair number of methods. As the \nprogram executes several runs in the same test, the system identifies a working set of hot methods, and \npromotes some of them to full or special optimization compilation. The execution time, on the other hand, \nis consistently improved after the first run for many of the tests by successful method promotions from \nthe interpreted code to the 1st level, and then to the 2nd and 3rd level compiled code. In two of the \ntests, jack and javac, the cost of re- compilation seems to appear in the execution time. This is partly \nbecause we performed the measurement on a uni-processor ma- chine and cannot hide the background compilation \ncost com-pletely. No significant overhead can be observed in other tests, since the execution time usually \ndecreases steadily. When one test program terminates and another test begins, the system reacts quickly \nto drive compilation for a new set of methods. 7. CONCLUSION We have described the design and implementation \nof our dynamic optimization framework, that consists of a mixed mode interpreter, a dynamic compiler \nhaving three levels of optimization, a sampling profiler, a recompilation controller, and an instrumenting \nprofiler. Performance results show that the system can effectively work for initiating each level of \ncompilation, and can achieve high 4 This looks similar to the corresponding figure in [3], but note that \nthe horizontal axis here is the number of runs, while it represents time parti- tioned into fixed-size \nintervals in [3]. pertbrmance and a low code expansion ratio in both program startup and steady state \nmeasurements in comparison to the compile-only approach. Owing to its zero compilation cost, the MMI \nallowed us to achieve an efficient recompilation system by setting appropriate tradeoff levels for each \nlevel of optimizations. We also described the design and implementation of automatic code specialization, \nwhich is used for the highest level of optimization compilation. This exploits the impact analysis and \nthe dynamically- generated instrumentation mechanism for runtime parameter and global variable value \nsampling. The experiment shows that the tech- nique can make a modest performance, improvement for some \nbenchmark programs. In the future, we plan to further refine the system to improve the to- tal performance \nby employing feedback-directed optimizations in- cluding more effective specialization, context sensitive \nmethod in- lining using mntime profile data, and some optimizations based on runtime exception profiling. \n8. ACKNOWLEDGMENTS We would like to thank all the members of the Network Computing Platform group in \nIBM Tokyo Research Laboratory for helpful dis- cussions and comments on an earlier draft of this paper. \nWe are also grateful to John Whaley for prototyping the sampling profiler sys- tem. Finally the anonymous \nreviewers provided many valuable sug- gestions and comments on the presentation of the paper. 9. REFERENCES \n [1] A.R. Adl-Tabatabai, M. Ciemiak, C.Y. Lueh, V.M. Parikh, and J.M. Stichnoth. Fast, Effective Code \nGeneration in a Just-in- Time Java Compiler. In Proceedings of the ACM SIGPLAN '98 Conference on Programming \nLanguage Design and Imple- mentation, pp. 280-290, Jun. 1998. [2] O. Agesen and D. Detlefs. Mixed-mode \nByteeode Execution. Technical Report SMLI TR-2000-87, Sun Microsystems, 2000. [3] M. Arnold, S. Fink, \nD. Grove, M. Hind, and P.F. Sweeney. Adaptive Optimizations in the Jalapefio JVM. In Proceedings of the \nACM SIGPLAN Conference on Object-Oriented Pro- gramming, Systems, Languages &#38; Applications, OOPSLA \n'00, Oct. 2000. [4] M. Arnold, S. Fink, D. Grove, M. Hind, and P.F. Sweeney. Adaptive Optimizations \nin the Jalapefio JVM: The Controller's Analytical Model. In Proceedings of the ACM SIGPLAN Workshop on \nFeedback-Directed and Dynamic Optimization, FDDO-3, Dec. 2000. [5] M. Arnold, B.G. Ryder. A Framework \nfor Reducing the Cost of Instrumented Code. In Proceedings of the A CM SIGPLAN '01 Conference on Program \nLanguage Design and Implementa- tion, pp. 168-179, Jun. 2001. [6] J. Auslander, M. Philipose, C. Chambers, \nS.J. Eggers, and B.N. Bershad. Fast, Effective Dynamic Compilation. In Proceedings of the ACM SIGPLAN \n'96 Conference on Programming Lan- guage Design andlmplementation, pp. 149-158, May 1996. [7] T. Autrey \nand M. Wolfe. Initial Results for Glacial Variable Analysis. In Proceedings of the 9th International \nWorkshop on Languages and Compilers for Parallel Computing, Aug. 1996. [8] V. Bala, E. Duesterwald, and \nS. Banerjia. Dynamo: A Trans- parent Dynamic Optimization System. In Proceedings of the ACM SIGPLAN '00 \nConference on Programming Language Design andlmplementation, pp. 1-12, Jun. 2000. [9] R.G. Burger and \nR.K. Dybvig. An infrastructure for Profile- Driven Dynamic Recompilation, In ICCL '98, the IEEE Com- \nputer Society International Conference on Computer Lan- guages, May 1998. [10] M.G. Burke, J.D. Choi, \nS. Fink, D. Grove, M. Hind, V. Sarkar, M. Serrano, V.C. Sreedhar, H. Srinivasan, and J. Whaley. The Jalapefio \nDynamic Optimizing Compiler for Java, In Proceed-ings' of the ACM SIGPLAN Java Grande Conference, pp. \n129- 141, Jun. 1999 [11] B. Calder, P. Feller, and A. Eustace. Value Profiling. In 30th International \nConference on Microarchitecture, pp. 259-269, Dec. 1997. [ 12] C. Chambers and D. Ungar. Customization: \nOptimizing Com- piler Technology for SELF, a Dynamically-Typed Object- Oriented Programming Languages. \nIn Proceedings of the A CM SIGPLAN '89 Conference on Programming Language Design andlmplementation,pp. \n146-160, Jul. 1989. [13] M. Ciemiak, G.Y. Lueh, and J.M. Stiehnoth. Practicing JUDO: Java Under Dynamic \nOptim/zations. In Proceedings of the ACM SIGPLAN '00 Conference on Programming Language Design andlmplementation, \npp. 13-26, Jun. 2000. [14] C. Consel and F. Noel. A General Approach for Run-Time Specialization and \nits Application to C. In Conference Record of the 23rd ACM SIGPLAN-SIGACT Symposium on Principles of \nProgramming Languages, pp. 145-156, Jan. 1996 [15] J. Dean, C. Chambers, and D. Grove. Selective Specialization \nfor Object-Oriented Languages. In Proceedings of the ACM SIGPLAN '95 Conference on Programming Language \nDesign andlmplementation, pp. 93-102, Jun. 1995. [16] J. Dean and C. Chambers. Towards Better Inlining \nDecisions Using Inlining Trials. In Proceedings of the ACM SIGPLAN '94 Conference on LISP and Functional \nProgramming, pp. 273-282, Jun. 1994. [17] D. Detlefs and O. Agesen. Inlining of Virtual Methods. In the \n13th European Conference on Object-Oriented Programming, 1999. [18] J. Gosling, B. Joy, and G. Steele. \nThe Java Language Specifi- cation. Addison-Wesley, 1996. [19] B. Grant, M. Philipose, M. Mock, C. Chambers,and \nS.J. Eg- gers. An Evaluation of Staged Run-Time Optimizations in DyC. In Proceedings of the A CM SIGPLAN \n'99 Conference on Programming Language Design and Implementation, pp. 293- 304, May 1999. [20] U. Hrlzle. \nAdaptive Optimization for SELF: Reconciling High Performance with Exploratory Programming. Ph.D. Thesis, \nStanford University, CS-TR-94-1520, Aug. 1994. [21] U, Hrlzle and D. Ungar. Reconciling responsiveness \nwith per- formanee in pure object-oriented languages. ACM Transac- tions on Programming Languages and \n@stems, 18(4):355-400, Jul. 1996. [22] IBM Corporation Inc. \"WebSphere Software Platform\", docu- mentation \navailable at http://www.ibn~eom/websphere 2000. [23] K. Ishizaki, M. Kawahito, T. Yasue, M. Takeuehi, \nY. Ogasa- wara, T. Suganuma, T. Onodera, H. Komatsu, and T. Nakatani. Design, Implementation, and Evaluation \nof Optimizations in a Just-In-Time Compiler. In Proceedings of ACM SIGPLAN 193 Java Grande Conference, \npp. 119-128, Jun. 1999. [24] K. Ishizaki, M. Kawahito, T. Yasue, H. Komatsu, and T. Naka- tani. A Study \nof Devirtualization Techniques for a Java Just-In- Time Compiler. In Proceedings of the ACM SIGPLAN Confer- \nence on Object-Oriented Programming, Systems, Languages &#38; Applications, OOPSLA '00, pp. 294-310, \nOct. 2000. [25] Just System Corp. \"IchitaroArk for Java\", available at http://www.justsystem.corn/arldindex.html~ \n1998. [26] M. Kawahito, H. Komatsu, and T. Nakatani. Effective Null Pointer Check Elimination Utilizing \nHardware Trap. In Pro-ceedings of the 9th International Conference on Architectural Support on Programming \nLanguages and Operating Systems, Nov. 2000. [27] A. Krall. Efficient JavaVM Just-in-Time Compilation. \nIn Pro-ceedings of International Conference on Parallel Architecture and Compilation Technique, Oct. \n1998. [28] R. Marlet, C. Consel, and P. Boinot. Efficient Incremental Run-Time Specialization for Free. \nIn Proceedings of the ACM SIGPLAN '99 Conference on Programming Language Design and Implementation, pp. \n281-292, Jun. 1999. [29] T. Ogasawara, H. Komatsu, and T. Nakatani. A Study of Ex- ception Handling and \nits Dynamic Optimization for Java. In Proceedings of the ACM SIGPLAN Conference on Object- Oriented Programming, \nSystems, Languages &#38; Applications, OOPSLA '01, Oct. 2001. [30] M. Paleezny, C. Viek, and C. Click. \nThe Java HotSpot Server Compiler. In Proceedings of the Java Virtual Machine Re- search and Technology \nSymposium (JVM '01), pp. 1-12, Apr. 2001. [31] M.P. Plezbert and R.K. Cytron. Does \"Just in Time\" = \"Better \nLate than Never\"?. In Conference Record of the 24th ACM SIGPLAN-SIGA CT Symposium on Principles of Programming \nLanguages, pp. 120-131, Jan. 1997. [32] M. Poletto, D. Engler, and M.F. Kaashoek. tee: A System for Fast, \nFlexible, and High-Level Dynamic Code Generation. In Proceedings of the ACM SIGPLAN'97 Conference on \nPro- gramming Language Design and Implementation, pp. 109-121, Jun. 1997. [33] V.C. Sreedhar, M. Burke, \nand J.D. Choi. A Framework for In. terprocedural Optimization in the Presence of Dynamic Class Loading. \nIn Proceedings of the ACM SIGPLAN '00 Confer- ence of Program Language Design and Implementation, pp. \n196-207, Jun. 2000. [34] M.D. Smith. Overcoming the Challenges to Feedback-Directed Optimization. In \nProceedings of the ACM SIGPLAN Workshop on Dynamic and Adaptive Compilation and Optimization (Dy- namo \n'00), pp. 1-11, Jan. 2000. [35] Standard Performance Evaluation Corporation. SPECjvm98 Benchmarks, available \nat http://www.spec.org/osg/jvrn98 and SPECjbb-2000 available at http://www.spec.org/osg/jbb2000. [36] \nT. Suganuma, T. Ogasawara, M. Takeuchi, T. Yasue, M. Ka- wahito, K. Ishizaki, H. Komatsd, and T. Nakatani. \nOverview of the IBM Java Just-in-Time Compiler, 1BM Systems Journal, 39(1), 2000. [37] Sun Microsysterus. \nThe Java Hotspot Performance Engine Ar- chitecture. White paper available at http://java.sun.com/products/hotspot/index.html, \nMay. 2001. [38] Sun Microsystems. Hot Java TM Browser available at http://java.sun.eorn/products/hotj \nava/index.html 1997. [39] O. Traub, S. Schechter, and M.D. Smith. Ephemeral Instru- mentation for Lightweight \nProgram Profiling. Technical Report, Harvard University, 1999. [40] J. Whaley. A Portable Sampling-Based \nProfiler for Java Vir- tual Machines. In Proceedings of the ACM SIGPLAN Java Grande Conference, Jun. \n2000. [41] J. Whaley. Dynamic Optimization through the Use of Auto- matie Runtime Specialization. Master's \nthesis, Massachusetts Institute of Technology, May 1999. [42] Wind River Systems Inc. \"IceStorm Browser \n5\", available at http://www.iceso ft.no/ieebrowser5/index.htm12000. [43] B.S. Yang, S.M. Moon, S. Park, \nJ. Lee, S. Lee, J. Park, Y.C. Chung, S. Kim, K. Ebeioglu, and E. Altman. LaTTe: A Java VM Just-in-Time \nCompiler with Fast and Efficient Register Allocation. In Proceedings of International Conference on Parallel \nArchitecture and Compilation Technique, Oct. 1999.   \n\t\t\t", "proc_id": "504282", "abstract": "The high performance implementation of Java Virtual Machines (JVM) and just-in-time (JIT) compilers is directed toward adaptive compilation optimizations on the basis of online runtime profile information. This paper describes the design and implementation of a dynamic optimization framework in a production-level Java JIT compiler. Our approach is to employ a mixed mode interpreter and a three level optimizing compiler, supporting quick, full, and special optimization, each of which has a different set of tradeoffs between compilation overhead and execution speed. a lightweight sampling profiler operates continuously during the entire program's exectuion. When necessary, detailed information on runtime behavior is collected by dynmiacally generating instrumentation code which can be installed to and uninstalled from the specified recompilation target code. Value profiling with this instrumentation mechanism allows fully automatic code specialization to be performed on the basis of specific parameter values or global data at the highest optimization level. The experimental results show that our approach offers high performance and a low code expansion ratio in both program startup and steady state measurements in comparison to the compile-only approach, and that the code specialization can also contribute modest performance improvement", "authors": [{"name": "Toshio Suganuma", "author_profile_id": "81100185644", "affiliation": "IBM Tokyo Research Laboratory, 1623-14 Shimoturuma, Yamato-shi, Kanagawa 242-8502, Japan", "person_id": "PP14074580", "email_address": "", "orcid_id": ""}, {"name": "Toshiaki Yasue", "author_profile_id": "81100210831", "affiliation": "IBM Tokyo Research Laboratory, 1623-14 Shimoturuma, Yamato-shi, Kanagawa 242-8502, Japan", "person_id": "PP14082638", "email_address": "", "orcid_id": ""}, {"name": "Motohiro Kawahito", "author_profile_id": "81100231881", "affiliation": "IBM Tokyo Research Laboratory, 1623-14 Shimoturuma, Yamato-shi, Kanagawa 242-8502, Japan", "person_id": "P203163", "email_address": "", "orcid_id": ""}, {"name": "Hideaki Komatsu", "author_profile_id": "81100557247", "affiliation": "IBM Tokyo Research Laboratory, 1623-14 Shimoturuma, Yamato-shi, Kanagawa 242-8502, Japan", "person_id": "PP39048455", "email_address": "", "orcid_id": ""}, {"name": "Toshio Nakatani", "author_profile_id": "81100311827", "affiliation": "IBM Tokyo Research Laboratory, 1623-14 Shimoturuma, Yamato-shi, Kanagawa 242-8502, Japan", "person_id": "PP14113792", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/504282.504296", "year": "2001", "article_id": "504296", "conference": "OOPSLA", "title": "A dynamic optimization framework for a Java just-in-time compiler", "url": "http://dl.acm.org/citation.cfm?id=504296"}