{"article_publication_date": "10-01-2001", "fulltext": "\n Regression Test Selection for Java Software* Mary Jean Harrold James A. Jones Tongyu Li Donglin Liang \nharrold@cc.gatech.edu jjones@cc.gatech.edu tongyu@cc.gatech.edu dliang@cc.gatech.edu Alessandro Orso \nMaikel Pennings Saurabh Sinha S. Alexander Spoon orso@cc.gatech.edu pennings@cc.gatech.edu sinha@cc.gatech.edu \nlex@cc.gatech.edu Ashish Gujarathi ashish.gujarathi@citrix.com ABSTRACT Regression testing is applied \nto modi.ed software to pro\u00advide con.dence that the changed parts behave as intended and that the unchanged \nparts have not been adversely af\u00adfected by the modi.cations. To reduce the cost of regres\u00adsion testing, \ntest cases are selected from the test suite that was used to test the original version of the software \nthis process is called regression test selection. A safe regression\u00adtest-selection algorithm selects \nevery test case in the test suite that may reveal a fault in the modi.ed software. Safe regression-test-selection \ntechniques can help to reduce the time required to perform regression testing because they se\u00adlect only \na portion of the test suite for use in the testing but guarantee that the faults revealed by this subset \nwill be the same as those revealed by running the entire test suite. This paper presents the .rst safe \nregression-test-selection tech\u00adnique that, based on the use of a suitable representation, handles the \nfeatures of the Java language. Unlike other safe regression test selection techniques, the presented \ntechnique also handles incomplete programs. The technique can thus be safely applied in the (very common) \ncase of Java software that uses external libraries or components; the analysis of the external code is \nnot required for the technique to se\u00adlect test cases for such software. The paper also describes Retest, \na regression-test-selection system that implements our technique, and a set of empirical studies that \ndemon\u00adstrate that the regression-test-selection algorithm can be ef\u00adfective in reducing the size of the \ntest suite.  Keywords Regression testing, testing, test selection, software evolu\u00adtion, software maintenance \n* All authors except Gujarathi are a.liated with the Col\u00adlege of Computing, Georgia Institute of Technology, \nAt\u00adlanta, GA. Gujarathi is a.liated with Citrix Systems, Inc, Ft. Lauderdale, FL. Permission to make \ndigital or hard copies of part or all of this work or personal or classroom use is granted without fee \nprovided that copies are not made or distributed for profit or commercial advantage and that copies bear \nthis notice and the full citation on the first page. To copy otherwise, to republish, to post on servers, \nor to redistribute to lists, requires prior specific permission and/or a fee. OOPSLA 01 Tampa Florida \nUSA Copyright ACM 2001 1-58113-335-9/01/10 $5.00 1. INTRODUCTION Regression testing is the process of \nvalidating modi.ed software to provide con.dence that the changed parts of the software behave as intended \nand that the unchanged parts of the software have not been adversely a.ected by the modi.cations. Because \nregression testing is expensive, researchers have proposed techniques to reduce its cost. One approach \nreduces the cost of regression testing by reusing the test suite that was used to test the original version \nof the software. Rerunning all test cases in the test suite, how\u00adever, may still require excessive time. \nAn improvement is to reuse the existing test suite, but to apply a regression\u00adtest-selection technique \nto select an appropriate subset of the test suite to be run. If the subset is small enough, sig\u00adni.cant \nsavings in time are achieved. To date, a number of regression-test-selection techniques have been developed \nfor use in testing procedural languages (e.g., [3, 8, 22, 29, 35, 37]) and for use in testing object-oriented \nlanguages (e.g., [14, 16, 19, 31, 36]). Testing professionals are reluctant, however, to omit from a \ntest suite any test case that might expose a fault in the modi.ed software. A safe regression-test-selection \ntechnique is one that, under certain assumptions, selects every test case from the original test suite \nthat can expose faults in the modi.ed program [32]. To date, several safe regression\u00adtest-selection techniques \nhave been developed [3, 8, 29, 31, 35]. These techniques use some representation of the origi\u00adnal and \nmodi.ed versions of the software to select a subset of the test suite to use in regression testing. Empirical \nevalua\u00adtion of these techniques indicates that the algorithms can be very e.ective in reducing the size \nof the test suite while still maintaining safety [5, 12, 15, 30, 31, 35]. These studies also show that \nfactors such as the structure of the code, location of the changes, granularity of the test suite, and \nfrequency of the testing a.ect the reduction in test-suite size that can be achieved by the techniques. \nRegression-test-selection techniques are particularly e.ec\u00adtive in environments in which changed software \nis tested fre\u00adquently [15]. For example, consider an environment in which nightly builds of the software \nare performed and a test suite is run on the newly built version of the software. In this case, regression \ntest selection can be used to select a subset of the test suite for use in testing the new version of \nthe software. The main bene.t of this approach is that, in many cases, a small subset of the test suite \nis selected, which reduces the time required to perform the testing. For another example, consider a \ndevelopment environment that includes such a regression-test-selection component. In this case, after \nde\u00advelopers modify their software, they can use the regression test selector to select a subset of the \ntest suite to use in testing. With this approach, developers can frequently test their software as they \nmake changes, which can help them locate errors early in development [15]. The techniques are also e.ective \nwhen the cost of test cases is high. An example is the regression testing of avionics software. In this \ncase, even the reduction of one test case may save thousands of dollars in testing resources. Although \nobject-oriented languages have been available for some time, only two safe regression-test-selection \nalgo\u00adrithms that handle features of object-oriented software have been developed [31, 36]. However, both \napproaches are lim\u00adited in scope and can be imprecise in test selection. Rother\u00admel, Harrold, and Dedhia \ns algorithm [31] was developed for only a subset of C++, and has not been applied to soft\u00adware written \nin Java. The algorithm does not handle some features that are commonly present in object-oriented lan\u00adguages; \nin particular, it does not handle programs that con\u00adtain exception-handling constructs. Furthermore, \nthe algo\u00adrithm must be applied to either complete programs or classes with attached drivers. For classes \nthat interact with other classes, the called classes must be fully analyzed by the al\u00adgorithm. Thus, \nthe algorithm cannot be applied to applica\u00adtions that call external components, such as libraries, unless \nthe code for the external components is analyzed with the applications. Finally, because of its treatment \nof polymor\u00adphism, the algorithm can be very imprecise in its selection of test cases. Thus, the algorithm \ncan select many test cases that do not need to be rerun on the modi.ed software. White and Abdullah s \napproach [36] also does not handle certain object-oriented features, such as exception handling. Their \napproach assumes that information about that classes that have undergone speci.cation or code changes \nis avail\u00adable. Using this information, and the relationships between the changed classes and other classes, \ntheir approach iden\u00adti.es all other classes that may be a.ected by the changes; these classes need to \nbe retested. White and Abdullah s ap\u00adproach selects test cases at the class level and, therefore, can \nselect more test cases than necessary. This paper presents the .rst safe regression-test-selection technique \nfor Java that e.ciently handles the features of the Java language, such as polymorphism, dynamic binding, \nand exception handling. Our technique is an adaptation of Rothermel and Harrold s graph-traversal algorithm \n[29, 31], which uses a control-.ow-based representation of the origi\u00adnal and modi.ed versions of the \nsoftware to select the test cases to be rerun. Our new graph representation e.ciently represents Java \nlanguage features, and our graph-traversal algorithm safely selects all test cases in the original test \nsuite that may reveal faults in the modi.ed software. Thus, unlike previous approaches, our technique \ncan be applied to com\u00admon commercial software written in Java. Another novel feature of our technique \nis that the representation models (under certain conditions) the e.ects of unanalyzed parts of the software, \nsuch as libraries. Thus, the technique can be used for safe regression test selection of applications \nwith\u00adout requiring complete analysis of the libraries that they use. Because most Java programs make \nfrequent use of li\u00adbraries, such as the AWT [28], our technique s ability to select test cases for applications \nwithout requiring analysis of the library may provide signi.cant savings during regres\u00adsion testing. \nFinally, the technique provides a new way to handle polymorphism that can result in the selection of \na smaller, but still safe, subset of the test suite. The paper also describes our regression-test-selection \nsys\u00adtem, Retest, and a set of empirical studies that we per\u00adformed using the system. Retest includes \na pro.ler to de\u00adtermine coverage information for a program, a module that compares two program versions \nand identi.es the parts of code that are a.ected by the modi.cation, and a module that combines information \nfrom the other two modules to select regression test cases. Using Retest, we performed two empirical \nstudies on a set of Java subjects. These stud\u00adies show that, for the subjects we considered, our algorithm \ncan provide signi.cant reduction in the size of the test suite and suggest that the technique scales \nwell to larger software systems. The studies also indicate that the granularity of the code entities \non which the selection is based (e.g., edges, methods) can result in considerable di.erences in the sizes \nof the test suites selected by the technique. In the next section, we discuss regression test selection \nin general; we also give details about Rothermel and Harrold s regression-test-selection algorithm, and \nillustrate it with an example. In Section 3, we discuss our algorithm, including details about the assumptions \nfor safety. In Section 4, we discuss our regression-test-selection system, Retest, and in Section 5, \nwe present the results of our empirical studies to evaluate our technique. In Section 6, we present related \nwork and compare it to our work. Finally, in Section 7, we summarize our results and discuss future work. \n 2. BACKGROUND Software testing is the activity of executing a given pro\u00adgram P with sample inputs selected \nfrom the input space for P, to try to reveal failures in the program. The under\u00adlying idea is that, under \nthe hypothesis that a test case is well-designed, its inability to reveal failures increases our con.dence \nin the tested code. To perform testing, it is nec\u00adessary to select a set of input data for P, to determine \nthe expected behavior of P for such data with respect to a given model, and to check the results of P \ns execution against the expected behavior. A test case consists of the input data that is provided to \nP, together with the corresponding ex\u00adpected output. A test suite is a set of test cases, and a test \nrun is the execution of P with respect to a test suite T.For each test case t in T, a test run for T \nconsists of the following three steps: (1) initialization of the environment, (2) execu\u00adtion of P with \ninput speci.ed in t, and (3) checking of the output of the execution with respect to the expected output. \nTo assess the adequacy of a given test suite T, we mea\u00adsure the level of coverage achieved by the test \nrun. Al\u00adthough functional coverage can be measured as well, cov\u00aderage is usually computed for structural \nentities (i.e., en\u00adtities in the code). Common examples of entities consid\u00adered for structural coverage \nare statements, branches, and conditions. Coverage is measured as a percentage. For ex\u00adample, statement \ncoverage is de.ned as the percentage of statements covered by the test run with respect to the total \nnumber of executable statements. The coverage information can be obtained in several ways. One method \nproduces an procedure avg 1. int count = 0 2. fread(fileptr,n) 3. while (not EOF) do 4. if (n<0) \n5. return(error) else  6. numarray[count] = n 7. count++ endif  8. fread(fileptr,n) endwhile  9. \navg = calcavg(numarray,count) 10. return(avg)   Figure 1: Example program, avg and its control-.ow \ngraph (left); modi.ed version, avg. and its control-.ow graph (right). instrumented version of P such \nthat when this instrumented version of P is executed with a test case t, it records the en\u00adtities in \nthe program, such as statements or branches, that are executed with t. An alternative method for obtaining \nthe coverage information modi.es the runtime environment so that when P is executed with t, the environment \ngathers the information about the entities covered. Let P. be a modi.ed version of P, and T be the test \nsuite used to test P. During regression testing of , T and in\u00ad P. formation about the testing of P with \nT are available for use in testing P. . In attempting to reuse T for testing P. , two problems arise. \nFirst, which test cases in T should be used to test P. (the regression-test-selection problem). Sec\u00adond, \nwhich new test cases must be developed to test parts of P. such as new functionality (the test-suite \naugmentation problem). Although both problems are important, in this paper we concentrate on the regression-test-selection \nprob\u00adlem. Regression-test-selection techniques attempt to reduce the cost of regression testing by selecting \nT., a subset of T, and using T. to test P. .  Figure 2: A general system for regression test selection. \nA number of safe regression-test-selection techniques that vary in precision and e.ciency have been presented \n(e.g., [3, 8, 29, 31, 35]). We can view these techniques as a family of regression-test-selection techniques \nthat use information about the program s source code to select T. . Figure 2 illus\u00adtrates a general regression-test-selection \nsystem. In this system, a program P is executed with a test suite T. In addition to the results of the \nexecution the pass/fail information the system records coverage informa\u00adtion about which entities in \nP are executed by each test case t. The types of entities recorded depend on the speci.c regression-test-selection \ntechnique. After all test cases have been run, the coverage information is compiled into a cover\u00adage \nmatrix that associates each t in T with the entities that it executes. In addition to computing coverage \ninformation, these techniques compare P and P., and identify in P a set of dan\u00adgerous entities [4]. We \nde.ne P(i) as the execution of P with input i.A dangerous entity is a program entity e such that for \neach input i causing P to cover e, P(i) and P.(i) may behave di.erently due to di.erences between P and \nP. . 1 The technique ensures that any test case that does not cover a dangerous entity will behave in \nthe same way in both P and P., and thus, cannot expose new faults in P. . Thus, it is safe 1Although \nan entity is not dangerous in itself, and the term a.ected (by a change) may be more appropriate, we \nuse the term dangerous entity to be consistent with the terminology used in Reference [4]. 2In a control-.ow \ngraph, nodes represent program state\u00adments and edges represent the .ow of control between the statements. \nto the set of dangerous entities. To illustrate, consider Figure 1, which shows an example program avg \nand its control-.ow graph (left) and a mod\u00adi.ed version avg . and its control-.ow graph (right). From \navg to avg ., statement 5a has been inserted and statement 7 has been deleted. The algorithm begins the \ntraversal at entry nodes in avg and avg ., and traverses like paths in the two programs by traversing \nlike-labeled edges until a di.er\u00adence in the target nodes of such edges is detected. When the algorithm \nconsiders node 4 in avg and node 4 in avg . , it .nds that the targets of the edges with label T dif\u00adfer, \nand it adds edge (4, 5) to the set of dangerous entities. The algorithm stops its traversal along this \npath: any test case that traverses other dangerous entities, if any, that can be reached only through \nthis edge (i.e., dominated3 by this edge) would necessarily traverse this edge, and thus, is al\u00adready \nselected. The algorithm then considers the F la\u00adbeled edges from node 4 in avg and node 4 in avg . . \nWhen the algorithm considers node 6 in avg with node 6 in avg .,it discovers that the targets of the \nout edges di.er; therefore, it adds edge (6, 7) to the set of dangerous entities, and stops the traversal \nalong this path. Subsequent traversals .nd no additional dangerous edges. In the example, the nodes la\u00adbeled \nD are declaration nodes. In the algorithm, declara\u00adtion nodes are used to model variable declarations \nexplicitly; such nodes contain information about all declarations. By doing so, the algorithm can identify \nchanges that involve only declarations of variables, which do not appear in the CFG. For example, if \nthe declaration of variable count in avg . changed from int to long, the algorithm would .nd a di.erence \nbetween the two declaration nodes and add edge (entry, D) to the set of dangerous entities. After dangerous \nedges have been identi.ed, the select\u00adtests component of the regression-test-selection system uses the \ndangerous entities and the coverage matrix to select the test cases in T to add to T. . Continuing with \nthe above example, suppose the test suite shown in Table 1 were used for avg. Table 1: A test suite for \nprogram avg of Figure 1. Test Case Input Expected Output 1 empty .le 0 2 -1 error 3 123 2 When the program \nexecutes, the edge covered by each test case in the test suite are recorded. For this example, test cases \n1, 2, and 3 cover edge (entry, 1), test cases 1 and 3 cover edge (2, 9), and test cases 2 and 3 cover \nedge (3, 4). Table 2 shows the edge-coverage matrix for this test suite and program avg. Table 2: Edge-coverage \nmatrix for test suite of Table 1 and pro\u00adgram avg of Figure 1. Edge Test Case (entry, 1), (1, 2), (2, \n3) 1, 2, 3 (3, 9), (9, 10), (10, exit) 1, 3 (3, 4) 2, 3 (4, 5), (5, exit) 2 (4, 6), (6, 7), (7, 8), (8, \n3) 3 3An edge ei dominates a edge ej if every path from the beginning of the program to ej goes through \nei. Using the edge-coverage matrix and the set of dangerous entities computed by the regression-test-selection \nalgorithm, the .nal step in the test selection is performed by simply indexing into the matrix using \nthe dangerous entities, and returning the corresponding test cases. In our example, the dangerous edges \nare (4, 5) and (6, 7). Thus, test cases 2 and 3 need to be rerun. 3. REGRESSION TEST SELECTION FOR JAVA \nThe regression-test-selection technique for Java that we present is also control-.ow based. Like the \ntechnique de\u00adscribed in Section 2 [29], our technique performs three main steps. First, it constructs \na graph to represent the control .ow and the type information for the set of classes under analysis. \nThen, it traverses the graph to identify danger\u00adous edges. Finally, based on the coverage matrix obtained \nthrough instrumentation, it selects, from the test suite for the original program, the test cases that \nexercise the dan\u00adgerous edges identi.ed in the previous step. For e.ciency, our technique avoids analyzing \nand instru\u00admenting code components, such as libraries, that are used by the program but have not been \nmodi.ed. Therefore, we can consider the program being tested as divided into two parts: one part that \nwe analyze and the other that we consider as external and do not analyze. For convenience, in the rest \nof the paper we refer to the set of classes that are analyzed and instrumented by our regression-test-selection \nsystem as internal classes, and those that are not analyzed and instru\u00admented by our system as external \nclasses. Analogously, we refer to a method in an internal class as an internal method and a method in \nan external class as an external method. In the rest of this section, we present the details of our regression-test-selection \ntechnique for Java: we describe the assumptions on which the technique is safe; we illus\u00adtrate the representation \nused to model Java programs; we present the traversal algorithm; and we discuss two alterna\u00adtive approaches \nfor Java-program instrumentation that al\u00adlow for gathering the coverage information required by our regression-test-selection \ntechnique. 3.1 Assumptions To be safe without being too ine.cient and too conser\u00advative, our technique \nmust rely on some assumptions about the code under test, the execution environment, and the test cases \nin the test suite for the original program. Bible and Rothermel call this set of assumptions the regression \nbias [4]. Re.ection. Our technique assumes that re.ection is not ap\u00adplied to any internal class or any \ncomponent of an internal class. Re.ection allows programmatic access to informa\u00adtion about the .elds, \nmethods and constructors of loaded classes, and the use of re.ected .elds, methods, and con\u00adstructors \nto operate on their underlying counterparts on ob\u00adjects, within security restriction [27]. In this paper, \nwe consider methods that inspect the information about a spe\u00adci.c class, such as the methods in java.lang.Class,asa \nform of re.ection as well. If a statement uses information obtained through re.ection about either an \ninternal class or its members, the behavior of this statement may be a.ected by several kinds of changes \noccurring in the class and/or its members. In such cases, the identi.cation of all the points in the \ncode a.ected by a change may require sophisticated and expensive analysis of all the re.ection constructs \nin the code. Moreover, if a statement in an external class uses re\u00ad.ection to inspect the information \nabout an internal class, then the external class must be analyzed to identify the code a.ected by a change \nof the internal class. Independent external classes. Our technique assumes that the external classes \ncan be compiled without the inter\u00adnal classes, and that external classes do not load any inter\u00adnal class \nexplicitly by invoking a class loader with the class name as a parameter. In other words, we assume that \nexter\u00adnal code has no knowledge of the internal classes. This as\u00adsumption guarantees that the external \nclasses interact with the internal classes only through a set of prede.ned virtual methods. Thus, this \nassumption reduces the types of in\u00adteractions between the internal and external classes that we must \nconsider. In practice, this assumption holds in most cases because the external classes are often library \nclasses that are developed independent of, and prior to, the devel\u00adopment of the applications that use \nthem. Deterministic test runs. Our technique assumes that a test case covers the same set of statements, \nand produces the same output, each time it is run on an unmodi.ed program. This assumption guarantees \nthat (1) the coverage informa\u00adtion contained in the coverage matrix, obtained by running the original \nprogram with the test cases, does not depend on the speci.c test run, and (2) the execution of a test \ncase that does not traverse a.ected parts of the code yields the same results for the original and the \nmodi.ed programs. Under this assumption, if our representation of the internal classes is correct (i.e., \nit correctly models all language constructs addressed and their e.ects), based on the information in \nthe coverage matrix, we can safely exclude test cases that do not traverse modi.cations. One possible \nthreat to this assumption is a change in the execution environment. If the execution environment changes \nbetween the test run on the original program and the test run on the modi.ed program, the outcome of \na test case may change even if the test execution does not tra\u00adverse the parts of code that are a.ected \nby the modi.cation. Therefore, for our technique to be applicable, the tester must ensure that elements \nsuch as the operating system, the Java Virtual Machine, the Java compiler, the external classes, databases \nand network resources possibly interacting with the program are .xed. This requirement, however, is not \noverly restrictive it is a common requirement for testing in general because it guarantees that, in case \nof a failure, the problem can be reproduced. Another possible threat to this assumption is the presence \nof nondeterministic behavior. The assumption holds for se\u00adquential programs, which contain only one thread \nof exe\u00adcution, and for those multithreaded programs in which the interaction among threads does not a.ect \nthe coverage and the outputs (e.g., an ftp server whose multiple threads are just clones created to handle \nmultiple clients). The assump\u00adtion, however, does not hold in general for programs that contain multiple \nthreads of execution. For our technique to be applicable to such programs, we must use special execu\u00adtion \nenvironments that guarantee the deterministic order in which the instructions in di.erent threads are \nexecuted [24]. This threat too, like the previous one, is a requirement for testing in general, and therefore, \nis not unduly restrictive.  3.2 Representation for Java Software To adequately handle all Java language \nconstructs, a rep\u00adresentation based on simple CFGs, such as the one presented in Section 2, is inadequate. \nA CFG is suitable for repre\u00adsenting the control .ow within a single procedure, but can\u00adnot accommodate \ninterprocedural control .ow (i.e., control .ow across procedure boundaries) or features of the Java language \nsuch as inheritance, polymorphism and dynamic binding, and exception handling. We de.ne a representation \nthat, although general enough to represent software written in other object-oriented lan\u00adguages, is tailored \nto Java. For convenience, we refer to this representation as Java Interclass Graph. A Java In\u00adterclass \nGraph (JIG) accommodates the Java language fea\u00adtures and can be used by the graph-traversal algorithm \nto .nd dangerous entities by comparing the original and modi\u00ad.ed programs. A JIG extends the CFG to handle \n.ve kinds of Java features: (1) variable and object type information; (2) internal or external methods; \n(3) interprocedural inter\u00adactions through calls to internal or external methods from internal methods; \n(4) interprocedural interactions through calls to internal methods from external methods; and (5) ex\u00adception \nhandling. We next discuss how the JIG represents each of these characteristics and constructs of Java. \nWe also show that the representation is suitable for modeling both complete and partial programs, such \nas classes, clusters, or components in general, because it accounts for the possible e.ects of missing \nparts of the system. Variable and object types. The graph-traversal algorithm described in Section 2, \nuses a representation that contains declaration nodes to represent declarations in the program. If a \nchange is made in a global declaration, then the edge from the entry node to the corresponding declaration \nnode is marked as dangerous, thus causing all test cases in the test suite to be selected. To achieve \nbetter precision, in our representation we as\u00adsociate the type of a variable that is of primitive type \nwith the name of the variable by augmenting the name with the type information. For example, a variable \na of type double, is identi.ed as a double in our representation. This method for representing scalar \nvariables essentially pushes any change in the variable type down to the location where that variable \nis referenced, which gives more precise test se\u00adlection than the use of the declaration node. The graph-traversal \nalgorithm described in Reference [31] models classes and class hierarchies explicitly. In our rep\u00adresentation, \ninstead of explicitly representing class hierar\u00adchies, we encode the hierarchy information at each point \nof instantiation (e.g., at each point in the code where new is invoked) using a globally-quali.ed class \nname. A globally\u00adquali.ed class name for a class contains the entire inher\u00aditance chain from the root \nof the inheritance tree (i.e., from class java.lang.Object) to its actual type.4 The interfaces that \nare implemented by the class are also in\u00ad 4For e.ciency, a globally-quali.ed class name can exclude external \nclasses and interfaces. // A is externally defined // and has a public static method foo() // and a public \nmethod m() 1 class B extends A { 2 }; 3 class C extends B { 4 public void m(){...}; 5 }; 6 void bar(A \np) { 7 A.foo(); 8 p.m(); 9 } CFG edge Call edge Path edge  Figure 3: Example of internal method call \nrepresentation. cluded in globally-quali.ed names. If a class implements more than one interface, the \nnames of the interfaces are inserted in the globally-quali.ed name in alphabetical or\u00adder. For example, \nif a class B in package foo extends a class A in the same package, and A implements interface I in package \nbar, then the globally-quali.ed name for B is java.lang.Object:bar.I:foo.A:foo.B. Using globally-quali.ed \nclass names, our technique can identify the changes in class hierarchies. This method for representing \nclass hierarchies also pushes the changes in class hierarchies to the locations where the a.ected classes \nare instantiated. Therefore, our technique can be very precise in accounting for such changes. Internal \nor external methods. A JIG contains a CFG for each internal method in the set of classes under analysis. \nThe CFG di.ers from the one described in Section 2 in two ways. First, each call site is expanded into \na call and a return node. Second, there is a path edge between the call and return node that represents \nthe path through the called method. The graph on the left of Figure 4 illustrates this representation. \nThe node labeled p.m() represents a call node; it is connected to the return node with a path edge. // \nB is an internal class // A is an external class Path edge Figure 4: Example of internal method representation \n(left) and external method representation (right). A JIG contains a collapsed CFG for each external \nmethod that is called by any internal method. Usually, the source code for the external classes is not \navailable, and, even if it were available, we do not want to analyze it. Because we assume that external \nclasses do not change, there is no need to represent and analyze such code. Thus, each collapsed CFG \nconsists of a method entry node and a method exit node along with a path edge from the method entry node \nto the method exit node. The path edge summarizes the paths through the method. The graph on the right \nin Figure 4 illustrates this representation. Interprocedural interactions through internal method calls. \nThe JIG represents each call site as a pair of call and return nodes that are connected with a path edge. \nThe call node is also connected to the entry node of the called method with a call edge. If the call \nis not virtual, the call node has only one outgoing call edge. To illustrate, consider Figure 3(a), which \nshows three classes, A (external), B, and C, a method bar, and the corresponding JIG. Class B extends \nclass A without overriding any method; class C extends class B and overrides method m. In the example, \nmethod bar invokes static method A.foo; there is no dynamic binding at this call site because A.foo is \na static method. Thus, the call node (node 7) has only one outgoing call edge connected to the entry \nnode for method A.foo. If the call is virtual, the call node is connected to the entry node of each method \nthat can be bound to the call. Each call edge from the call node to the entry node of a method m is labeled \nwith the type of the receiver instance that causes m to be bound to the call. In the example, the call \nto p.m in method bar is a virtual call. Depending on the dynamic type of p (whose static type is A), \nthe call to m is bound to di.erent methods: if p s type is either A or B, the call is bound to A.m;if \np s type is C, the call is bound to C.m. Consequently, the call node has three outgoing call edges: two \nof them, labeled A and B , are connected to the entry node for A.m; the third edge, labeled C , is connected \nto the entry node for C.m. //A is externally defined // and has a public method foo() // and a public \nmethod bar() class B extends A { public void foo() {...}; } class C extends B { public void bar() {...}; \n}; CFG edge Call edge  Figure 5: Example of external method call representation. To represent virtual \nmethod calls correctly, we must com\u00adpute, for each virtual call site, the set of methods to which the \ncall may be bound. Such information can be computed using various type-inferencing algorithms (e.g., \n[2, 10, 34]) or points-to analysis algorithms (e.g., [23]). The precision of this computation determines \nthe e.ciency of the represen\u00adtation. In our technique, we use the class hierarchy analy\u00adsis [10] to resolve \nthe virtual calls. Using this representation, our algorithm can identify, by traversing the JIGs constructed \nfor the original and modi\u00ad.ed programs, the internal method calls that may be a.ected by a program change. \nFigure 3(b) shows a modi.cation to class B that adds a new method m() in B (statement 1a). This change \na.ects the method that is invoked at state\u00adment 8 when p s type is B. By comparing the outgoing edges \nfrom the call node associated with statement 8 in the JIG in Figure 3(a) and the outgoing edges from \nthe call node associated with statement 8 in the JIG in Figure 3(b), our algorithm identi.es that the \ntarget of the edge labeled B has changed. Thus, our algorithm identi.es this edge as dangerous. Interprocedural \ninteractions through external method calls. Potential subtle interactions between internal classes and \nexternal classes may lead to di.erent behavior in the program, as a consequence of apparently harmless \nchanges in the internal classes. Therefore, in the case of incomplete programs, we must consider the \npossible e.ects of the un\u00adanalyzed parts of the system. In particular, unforeseen in\u00adteractions between \ninternal classes and external classes may be caused by calls from external methods to internal meth\u00adods. \nTo handle this situation, we explicitly represent, in a summarized form, potential calls to internal \nmethods from external methods. Figure 5(a) provides an example of such representation. External code \nis represented by a node labeled ECN (Ex\u00adternal Code Node). For each internal class that is accessible \nfrom external classes (both classes B and C in the example), the JIG contains an outgoing edge from the \nECN node. Each of these edges is labeled with the name of the class it repre\u00adsents and terminates in \na class entry node. A class entry node for an internal class A represents an entry point to the internal \ncode through an object of type A, and is connected to the entry of each method that can be invoked by \nexternal methods on objects of type A. The only internal methods that can be invoked by external code \nare those methods that override an external method.5 There\u00adfore, we must create a class entry node for \n(1) each class that overrides at least one external methods, and (2) each class that inherits at least \none method overriding an exter\u00adnal method. For example, in Figure 5(a), class C overrides A.bar and inherits \nB.foo, which in turn overrides A.foo; thus, we must create a class entry node for C and connect it to \nB.foo and C.bar, which can both be invoked on ob\u00adjects of type C by external code through polymorphism \nand dynamic binding. In addition, for each class entry node, we create an out\u00adgoing default call edge \nlabeled * (see Figure 5(a)), and we connect it to a default node.The default node for a class A represents \nall methods that can be invoked through an object of a type A, but that are externally de.ned. This representation \nlets us correctly handle modi.cations that in\u00advolve addition or removal of internal methods that override \nexternal methods, Using this representation, our algorithm can identify, by traversing the JIGs constructed \nfor the original and modi.ed programs, the external method calls that may be a.ected by a program change. \nFigure 5(b) shows the modi.cation to classes B and C that deletes bar() from C and adds a new bar() in \nB. This change may a.ect an external call to these methods when the receiver type is either B or C. By \ncomparing the outgoing edges from the node labeled C entry in the graph in Figure 5(a) with the outgoing \nedges from node labeled C entry in the graph in Figure 5(b), our algorithm identi.es that the target \nof the edge labeled bar() has changed. Thus, our algorithm identi.es this edge as dangerous. Exception \nhandling. The JIG uses an approach similar to that described in Reference [33] to model exception-handling \nconstructs in Java code. A JIG explicitly represents the try block, the catch blocks, 5As stated in \nSection 3.1, we assume that external code has no knowledge of internal methods and, therefore, can call \ninternal methods only through polymorphism and dynamic binding. (a) foo(), bar() and their representations \n(b) a modified version of foo() and its representation Figure 6: Example of exception handling representation. \nand the .nally block in each try statement. Figure 6(a) shows an example JIG for a method that contains \nexception\u00adhandling constructs. For each try statement, we create a try node in the CFG for the method \nthat contains the statement (node labeled 2 try in the example). We represent the try block of the try \nstatement using a CFG. There is a CFG edge from the try node to the entry of the CFG of the try block. \nWe create a catch node and a CFG to represent each catch block of the try statement. A catch node is \nlabeled with the type of the exception that is declared by the corresponding catch block. A CFG edge, \nlabeled caught , connects the catch node to the entry of the catch CFG. A path edge, la\u00adbeled exception \n, connects the try node to the catch node for the .rst catch block of the try statement. That path edge \nrepresents all control paths, from the entry node of the try block, along which an exception can be propagated \nto the try statement. For example, path edge (2, 4) in the graph in Figure 6(a) represents all control \npaths that traverse state\u00adment 2 and reach statement 16 or 18, or a throw statement in Integer.valueOf() \nthat causes Integer.valueOf() to propagate an exception. A path edge labeled exception connects the catch \nnode for a catch block bi to the catch node for catch block bi+1 that follows bi. This path edge represents \nall control paths, from the entry node of the try block, along which an exception is (1) raised, (2) \npropagated to the try statement, and (3) not handled by any of the catch blocks that precede bi+1 in \nthe try statement. We create a .nally node and a CFG to represent the .\u00adnally block of the try statement. \nA CFG edge connects the .nally node to the entry of the CFG. For each CFG that rep\u00adresents the try block \nor a catch block, a CFG edge connects the exit node of this CFG to the .nally node. The exit of the CFG \nfor the .nally block is connected to the statement that follows the try statement. If there are exceptions \nthat cannot be caught by any catch block of the try statement, we create a copy of the .nally node and \nof the CFG for the .nally block. A path edge labeled exception connects the catch node of the last catch \nblock to this .nally node. If the try statement is not enclosed in another try statement in the same \nmethod, a CFG edge connects the exit of this duplicated .nally CFG to the exceptional exit node. An exceptional \nexit node models the e.ect of an uncaught ex\u00adception causing exit from the method. If no .nally block \nis present and the try statement is not enclosed in any other try statement in the same method, a path \nedge labeled ex\u00adception connects the catch node of the last catch block of the try statement to the exceptional \nexit node. Using this representation, our algorithm can identify the changes in exception-handling code \nby traversing the JIGs constructed for the original and the modi.ed programs. Fig\u00adure 6(b) shows a modi.ed \nversion of foo() in which we delete statements 4 and 5 and change the type of exception han\u00addled by statement \n6. By comparing the outgoing edges from the try node in the graph in Figure 6(a) and the outgoing edges \nfrom the try node in the graph in Figure 6(b), our algorithm discovers that the target of the edge labeled \nex\u00adception has changed. Thus, our algorithm identi.es this edge as dangerous. Procedure Compare(N,N.) \ninput N: a node in the JIG for original program P N.: a node in the JIG for modi.ed program P . global \noutput E: set of dangerous edges for P begin Compare 1. mark N N.-visited  2. foreach edge e leaving \nN. do 3. e = match(N,e .) 4. if e is null then continue 5. C = e.getTarget()  C. 6. = e . .getTarget() \n 7. if \u00acNodesEquiv(C,C.) then 8. E= E.e 9. elseif C is not marked C.-visited 10. Compare(C,C.)  11. \nendif 12. endfor 13. foreach edge e leaving N and not matched to any edge leaving N. do  14. E= E.e \n 15. endfor end Compare  Figure 7: Compare procedure.  3.3 The Traversal Algorithm Our algorithm that \ntraverses the JIGs and identi.es dan\u00adgerous edges is similar to the algorithm in Reference [29] that \ntraverses the CFG of a procedure. Our algorithm starts the traversal by invoking Compare() on the entry \nnode of method main(), on the ECN node, and on the entry nodes of all methods called static. 6 Compare() \naccepts as inputs anode N in the JIG constructed for the original program P andanode N. in the JIG constructed \nfor the modi.ed program P ; it traverses the JIGs and adds the dangerous edges that it .nds to E. Compare() \n.rst marks N as N. \u00advisited (line 1) to avoid comparing N with N. again in a subsequent iteration. Compare() \nthen examines each outgo\u00ading edge e . from N. and calls match() to .nd an outgoing edge from N that matches \ne . s label (line 3). match() .rst looks for an outgoing edge from N that has the same label as e ..If \nmatch() .nds such an edge, it returns this edge. Otherwise, match() looks for the edge whose label is \n* . If match() .nds such an edge, it returns this edge. Otherwise, it returns null. After Compare() .nds \nthe edge e that matches e ., it com\u00adpares C, the target of e, with C., the target of e . (lines 5 7). \nIf C is not equivalent to C. , Compare() adds e to set E (line 8). Otherwise, if C has not been marked \nwith C.-visited , Compare() invokes itself on C and C. to further traverse the graph (line 10). One way \nto determine the equivalence of two nodes is to examine the lexicographic equivalence of the text associated \nwith the two nodes [29]. For nodes that represent program statements, we can examine the lexicographic \nequivalence of the program statements associated with the nodes; for nodes, such as exit nodes, that \ndo not represent program statements, we can examine the lexicographic equivalence of the labels associated \nwith the nodes. After Compare() .nishes processing the outgoing edges 6The Java compiler creates, for \neach class containing initial\u00adizers for static .elds, a special method called static, which contains \nall such initializations and is executed when the class is initialized. from N., it searches for outgoing \nedges from N that have not been matched with any outgoing edge from N. (line 13). These edges appear \nalong paths that have been deleted in P. . Thus, Compare() adds these edges to set E (line 14).  3.4 \nInstrumentation for Test Selection Given a JIG for a program P, we can instrument P or mod\u00adify the execution \nenvironment to record the edges covered by each test case (the task of gathering coverage was discussed \nin Section 2). The coverage information lets the regression\u00adtest-selection technique select test cases \nthat cover the dan\u00adgerous edges identi.ed by the traversal algorithm. However, because some edges (e.g., \npath edges for exception handling) do not represent actual control .ow from one statement to another, \nwe cannot instrument the program or the execution environment to .nd the test cases that cover such edges. \nMoreover, because recording the coverage information for each edge can be very expensive, we may want \nto record coverage information for coarser-grained entities, such as methods, classes, or modules. Thus, \nsome dangerous edges must be mapped to another set of entities whose coverage information is recorded \nin the coverage matrix. In general, we need an adaptor that takes a set of dan\u00adgerous edges from the \ntraversal algorithm and maps them to a set of dangerous entities whose coverage information is recorded \nin the coverage matrix. The adaptor must be designed together with the instrumenter because it needs \nto know the entities whose coverage information is being recorded. To get a better trade-o. between precision \nand e.ciency, the instrumenter also needs to know which en\u00adtities are of interest to the adaptor. In \nthe following, we discuss a possible approach for building an adaptor and an instrumenter in the case \nof instrumentation at the edge and at the method level. Edge-level instrumentation techniques record, \nfor the in\u00adternal methods, the CFG edges that are covered by each execution of a program P. To determine \nthe virtual call edges that are covered by the execution, we require the in\u00adstrumenter to also record \nthe receiver type of each virtual method call in internal methods. In a JIG, edges representing calls \nfrom external meth\u00adods (see Figure 5) and path edges representing the control paths on which exceptions \nare raised (see Figure 6) need to be mapped to actual CFG edges and nodes, so that the instrumenter can \nrecord the test cases that cover such edges. For an edge e1 representing a method call from an external \nmethod and whose receiver is an instance of an internal class C, if the target of e1 is the entry of \na method, the adaptor maps e1 to the corresponding entry node. However, if the target of e1 is the default \nnode created for C (e.g., the nodes labeled default in Figure 5), the adaptor maps e1 to all new-instance \nstatements in the internal classes that create instances of C. To record the coverage information for \nexception han\u00addling, we require the instrumenter to insert a catch block at the end of each try statement \nto catch and to re-throw all exceptions that are not caught by other previous catch blocks. Let e2 be \na path edge that represents the control paths on which an exception is raised. If the target of e2 is \na catch node for a catch block b, then e2 is mapped to the entry of b and the entry of each of the catch \nblocks that T Program P Program P Figure 8: Our regression-test-selection system, Retest. appear after \nb in the try statement. e2 is also mapped to the entry of the catch block added by the instrumenter. \nMethod-level instrumentation techniques record the inter\u00adnal methods that are covered by each execution \nof the pro\u00adgram. Using this instrumentation technique, the adaptor maps each dangerous edge in the JIG \nfor a method m to m. For each call edge e, if the target of e is the entry node of an internal method \nm, the adaptor maps e to m. Otherwise, if the target of e is the entry node of an external method, the \nadaptor maps e to the method that contains the source of e. However, if e is an edge that represents \na call from external methods and e s target is the default node (e.g., the nodes labeled default in Figure \n5) created for an internal class C, the adaptor maps e to all the constructors (including the default \nconstructor) of C. Instrumentation at a coarser level of granularity is more e.cient than instrumentation \nat a .ner level of granular\u00adity. In particular, method-level instrumentation is more e.cient than edge-level \ninstrumentation. Also, the cover\u00adage matrix computed using method-level instrumentation is smaller than \nthe one computed using edge-level instru\u00admentation. However, using a coverage matrix computed by method-level \ninstrumentation, a test-selection algorithm may select more test cases than using a coverage matrix computed \nby edge-level instrumentation. In practice, another viable solution is a hybrid instrumen\u00adtation approach \nthat, for example, records coverage informa\u00adtion for all internal methods, and also records coverage \nin\u00adformation for those statements, such as exception handling, that are rarely covered (even when the \nmethod that contains the statements is executed). Hybrid approaches may yield a good trade-o. between \nprecision and e.ciency.  4. REGRESSION TEST SELECTION SYS-TEM To investigate empirically the regression-test-selection \ntechnique presented in this paper, we implemented a regression-test-selection system named Retest, which \nis a specialization of the general regression-test-selection system shown in Figure 2. Retest consists \nof three main compo\u00adnents: a component named Pro.ler, which gathers dynamic information, a static analysis \ncomponent named DejaVOO, and a test-selection component named SelectTests. Retest also consists of a \ncomponent, Adaptor, which adapts the output produced by DejaVOO to the type of coverage in\u00adformation \ngathered by the pro.ler. Figure 8 presents the system architecture of Retest. The pro.ler component \nin Retest uses the Java Virtual Machine Pro.ler Interface (JVMPI) [26] to gather coverage information \nabout P when P is run using each test case in T. However, due to restrictions in JVMPI, the current pro.ler \ncannot record information about the execution of individual statements other than class instantiation, \nmethod entries, and method calls. The pro.ler also cannot record infor\u00admation for exception throws and \ncatches. Therefore, the pro.ler uses a hybrid instrumentation approach: it records class instantiation, \nmethod calls, and method entries. At a virtual method call, the pro.ler also records the receiver type \nso that it can determine the method that is actually invoked. The DejaVOO module implements the analysis \nalgorithm described in Section 3, using the Java Architecture for Byte\u00adcode Analysis (Jaba) [1] to construct \nJIGs and other neces\u00adsary information about P and P . The output from DejaVOO is a list of dangerous \nedges that can be either CFG edges or call edges. The adaptor module treats CFG edges and call edges \ndif\u00adferently. The adaptor maps each dangerous CFG edge e in a method m to the entry of m, unless the \ntarget of e is a class instantiation or a method call. If this is the case, the adaptor maps e to the \nnode representing the class instanti\u00adation or to the call node, respectively. The adaptor maps each call \nedge (mi, me), which represents a call from an in\u00adternal method mi to an external method me,to mi s entry. \nThe adaptor maps each call edge (A entry, mi), which rep\u00adresents a call from an external method to an \ninstance of A that is dynamically bound to an internal method mi,to mi s entry. Finally, the adaptor \nmaps each call edge (A entry, default), which represent a call from an external method to an instance \nof A that is dynamically bound to an external method, to all the instantiation sites for A. The current \nimplementation of Retest is less precise than it could be using the technique described in this paper. \nThis imprecision occurs because the pro.ler cannot record infor\u00admation for each individual statement. \nTherefore, if a change Table 3: Software subjects used for the empirical studies. Subject Description \nMethods Versions Test cases Method coverage Siena Internet-based event noti.cation system 185 7 138 70% \nJEdit Text editor 3495 11 189 75% JMeter Web-applications testing tool 109 5 50 67% RegExp Regular-expression \nlibrary 168 10 66 46% occurs only in statements that are traversed by a small frac\u00ad tion of the test \ncases that cover the method, Retest can select more test cases than necessary. For example, if an ex\u00ad \nception handler that catches infrequently-raised exceptions has changed, each test case going through \nthe method that contains the handler is selected, even though the exception is not raised by a majority \nof these test cases. We plan to investigate techniques to enhance our pro\u00ad .ler by using the Java Virtual \nMachine Debug Interface (JVMDI) [25] instead of the JVMPI. The JVMDI allows more detailed examination \nof programs as they run, and should let us reduce the imprecision described above. We will also investigate \nthe possibility of instrumenting the bytecode using a tool such as Bit (Bytecode Instrument\u00ad ing Tool)7 \n[9], Soot (a Java Optimization Framework),8 or BCEL (Byte Code Engineering Library),9 to gather the CFG-edge \ncoverage information. With this approach, we may be able to improve the precision of the test selection. \n 5. EMPIRICAL EVALUATION To evaluate our approach for regression test selection, we used Retest to perform \ntwo empirical studies. This section describes the software subjects and the empirical studies that we \nperformed. 5.1 Software Subjects Our study utilized four software subjects: Siena, JEdit, JMeter10 , \nand RegExp. Each software subject consists of an original version P, several modi.ed versions (V1, ... \nVn), and a set of test cases that was used to test P. Table 3 shows the subjects and, for each subject, \nlists the number of methods in the original program, the number of versions, the size of the test suite, \nand the percentage of methods covered by the test suite. The .rst subject for our studies is the Java \nimplementa\u00ad tion of the Siena server [6]. Siena (Scalable Internet Event Noti.cation Architecture) is \nan Internet-scale event noti.\u00ad cation middleware for distributed event-based applications deployed over \nwide-area networks. We obtained seven suc\u00ad cessive versions of the system from the authors, along with \na test suite that had been used to test the software. We also added test cases to the test suite to increase \nits coverage (at the method level); the resulting test suite contains 138 test cases, which provide 70% \nmethod coverage of the system. The second subject for our studies is JEdit, a Java-based text editor. \nJEdit is a versatile, customizable text edi\u00ad tor, which provides several advanced text-editing features, \nsuch as syntax highlighting, regular-expression search and 7See http://www.cs.colorado.edu/~hanlee/BIT/. \n8See http://www.sable.mcgill.ca/soot/. 9See http://bcel.sourceforge.net/. 10Copyright 1999-2001, Apache \nSoftware Foundation. replace, multiple clipboards, and macro recording. We ob\u00adtained two successive development \nreleases of the software version 3.0-pre4 and version 3.0-pre5 and, based on the changes between the \nreleases, created 11 versions of the soft\u00adware. The original version of the software contains 3495 methods. \nWe then developed a test suite to exercise various features of the text editor; the test suite consists \nof 189 test cases and provides 75% method coverage. The third subject for our studies is Apache JMeter. \nJMeter is a Java desktop application designed to load test the functional behavior and measure performance; \nit was originally designed for testing web applications but has since expanded to other test applications. \nWe obtained two suc\u00adcessive releases of the system from the code repository and, based on the modi.cations \nbetween the releases, created .ve versions of the software. We created 50 test cases by con\u00adsidering \ncombinations of features available through the user interface of the system; the test cases provide 67% \nmethod coverage of the system. The .nal subject for our studies is RegExp, a GNU li\u00adbrary for parsing \nregular expressions. Like for JEdit and JMeter, we obtained two successive releases and built sep\u00adarate \nversions based on the di.erences between the releases; we created 10 versions of the software. We used \nthe three drivers and the 22 test cases that are provided with the library. The test suite thus contains \n66 test cases, which exercise 46% of the methods in the library. 5.2 Studies To evaluate our technique, \nwe performed two studies; this section presents the results of those studies. For Siena,we compared each \nversion to the previous versions, whereas for JEdit, JMeter, and RegExp, we compared each version to \nthe original version. Study 1: Test Suite Reduction. The goal of this study was to determine the reduction \nin the number of test cases that could be achieved using our regression-test-selection tech\u00adnique. For \neach subject program P, and each version P , we used Retest, shown in Figure 8, to select test cases \nfor regression testing. Figure 9 shows the results for the four software subjects. For each subject, \nthe .gure shows the percentage of test cases that were selected for each version of that subject. The \ndata in the .gure illustrate that the reduction in the number of selected test cases varies widely both \nacross and within the subjects. For example, for Siena, the technique selected less than 2% of the test \ncases for one version, but between 60% and 90% of the test cases for the remaining six versions. Similarly, \nfor JEdit, the technique selected fewer than 15% of the test cases for four versions, but over 90% of \nthe test cases for .ve versions. The extremely low number of test cases selected for some versions, such \nas version V6 100 90 80 70 60 50 40 30 20 10 0 Siena JEdit JMeter RegExp Software subjects Figure 9: \nRegression test selection results for our software subjects. Pct. of Test Cases Selected of Siena and \nversion V5, V10, and V11 of JEdit, depends on the fact that, for those versions, the changes are minor, \ninvolving few methods, and methods encountered by only a few test cases. The test reduction illustrated \nin Figure 9 is similar to results of other studies that have evaluated regression-test\u00adselection techniques \nfor procedural software [12, 30]. The data does not reveal any trends that may be peculiar to the object-oriented \nparadigm; further experimentation with a bigger and diverse set of subjects will help deter\u00admine whether \nsuch trends exist. The success of code-based regression-test-selection techniques depends not only on \nthe magnitude of changes to the modi.ed software which, in turn, depends of the frequency of regression \ntesting but also on the location of those changes and the characteris\u00adtics of the software. For example, \na modi.cation in the startup code of a software causes each test case to be se\u00adlected. For further example, \nthe characteristics of certain classes of software, such as language parsers, are such that reduction \nin the test suite can be achieved by considering changes at the edge level over changes at the method \nlevel. However, if there exists some path in M over which a dan\u00adgerous edge in M is not reached, then, \nassuming that the test suite covers all edges in P, there is at least one test case through M that the \nedge-level version of Retest will not select. Thus, in this case, additional reduction of the test suite \ncan be achieved by considering regression test selection at the edge level. The total number of dangerous \nmethods in all four sub\u00adjects is 51; of these, in 51% of the methods, the dangerous edges are reached \nalong all paths from the entry of the re\u00adspective methods. Thus, in many cases, performing the test selection \nat a .ner granularity at the edge level can re\u00adduce the size of the test suite selected. Previous empirical \nstudies that have examined procedural software have also found edge-level selection to di.er from method-level \nselec\u00adtion [5]. most of the test cases exercise a signi.cant percentage of the 100 code in the software; \ntherefore, for such software, most of 90 000000000000 111111111111  000000000000 111111111111 0000000000 \n1111111111 0000011111 method level edge level 00 11 00 11 00 11 00 11 00 11 00 11 00 11 00 11 00 11 00 \n11 00 11 V1 V2 V3 V4 V5 V6 V7 Siena the modi.cations cause a majority of the test cases to be selected. \nIn such cases, code-based regression-test-selection techniques may fail to provide any bene.t in terms \nof re\u00ad duced regression test suites. Study 2: Test Selection Granularity. The goal of the sec\u00ad ond study \nwas to determine whether any additional reduc\u00ad tion in the size of the selected test suite could be achieved \nby selecting dangerous edges instead of dangerous methods. Pct. of Test Cases Selected 80 70 60 50 40 \n30 20 The results of this study will guide our development of pro-10 .lers in the future.11 0 To perform \nthis study, we used DejaVOO to select dan\u00adgerous edges in P. We then used a data-.ow analysis that considers \neach method M, and determines whether some dangerous edge in M is reached on all paths from the entry \npoint of M. If a dangerous edge is reached over all paths in M, all test cases that enter M will be selected \nby an edge-level version of Retest. Thus, in this case, no further 11The development of a pro.ler for \nrecording information at the edge level requires signi.cant e.ort, and the results of this study can \nhelp us decide whether this e.ort is worth\u00adwhile for regression test selection. Modified Version Figure \n10: Percentage of test cases that are selected for Siena using method-level and edge-level test selections. \nWe also wanted to determine the accuracy of our esti\u00admates for edge-level test selection. Thus, for Siena,we \nmanually determined the test cases that would be selected by the edge-level version of DejaVOO. The graph \nin Figure 10 shows the results of this study. For all but three of the versions of Siena, the test suite \nselected by the method-level version of DejaVOO and the edge-level version of DejaVOO selected the same \nnumber of test cases. For those versions that di.ered V2, V6, and V7 the graph shows that there can be \na signi.cant di.erence in the size of the reduction. Thus, in cases where running test cases is particularly \nex\u00adpensive, the additional overhead in selection required by an edge-level version of DejaVOO may be \njusti.ed.  6. RELATED WORK Many researchers have considered the problem of regres\u00adsion test selection \nfor procedural software. A number of regression-test-selection algorithms .t into the general sys\u00adtem \nshown in Figure 2, and are thus related to our work. Ball [3] presents an edge-optimal regression-test-selection \nal\u00adgorithm that, under certain conditions, provides more preci\u00adsion than Rothermel and Harrold s algorithm. \nHis algorithm also identi.es dangerous edges. Ball also presents additional algorithms based on control \n.ow that are even more precise than edge-based algorithms, at greater computation cost. Vokolos and Frankl \npresent a regression-test-selection algo\u00adrithm based on text di.erencing [35]. Their algorithm main\u00adtains \nan association between basic blocks and test cases in T, and compares the source .les of P and P to identify \nthe modi.ed program statements. We can think of this algo\u00adrithm as selecting dangerous blocks of code \nin the program. Their algorithm performs the comparison using UNIX diff utility, and is based on statements, \nnot control .ow; thus, it may select more test cases than the control-.ow-based al\u00adgorithms, at a lesser \ncomputation cost. Chen, Rosenblum, and Vo [8] present a regression-test-selection algorithm that detects \nmodi.ed code entities (i.e., dangerous code entities), which are de.ned as functions or as non-executable \ncompo\u00adnents, such as storage locations. The technique selects all test cases associated with changed \nentities. Because this technique is based on entities that are coarser-grained than those used by statement-or \ncontrol-.ow-based techniques, it may select more test cases than those techniques, with lesser computation \ncost. Other researchers have developed regression-test-selection techniques for object-oriented software. \nIn the Introduc\u00adtion, we discussed two techniques: Rothermel, Harrold, and Dedhia s technique for C++ \n[31] and White and Abdullah s .rewall technique [36]. We compared both techniques to our approach: our \napproach is more precise, can be applied to Java programs, handles exception-handling constructs, can \nbe applied to incomplete programs, and provides a new method for handling polymorphism. Several techniques \nhave been developed to reduce the ef\u00adfort required to test subclasses (e.g., [7, 11, 13]). These techniques \nuse information associated with a parent class in the design of test suites for derived classes. However, \nthe techniques do not address the problem of regression testing of modi.ed classes. Kung et al. [17, \n18] and Hsia et al. [14] present a technique for selecting regression test cases for class testing. This \ntech\u00adnique is based on the concept of .rewalls de.ned originally by Leung and White for procedural software \n[21, 20, 37] and later extended to object-oriented software [36]. The tech\u00adnique of Kung, Hsia et al. \n(here called the ORD technique ) constructs an object relation diagram (ORD) that describes static relationships \namong classes. The represented relation\u00adships include inheritance, aggregation (the use of compos\u00adite \nobjects), and association (the existence of data depen\u00addence, control dependence, or message passing \nrelationships between classes). The ORD technique instruments code to report the classes that are exercised \nby test cases. The .re\u00adwall for a class C is de.ned as the set of classes that are directly or transitively \ndependent on C (by virtue of inheri\u00adtance, aggregation, or association) as described by an ORD. When \nclass C is modi.ed, the ORD technique selects all test cases that were determined through instrumentation \nto exercise one or more classes within the .rewall for C. The ORD technique and the technique presented \nin this paper are similar in that they both select all test cases as\u00adsociated with some set of code components, \nand the associ\u00adation of test cases with code components is determined dy\u00adnamically through instrumentation. \nThe primary di.erence between the techniques is the granularity at which they con\u00adsider components. The \nORD technique selects all test cases associated with classes within the .rewall; it performs no further \nanalysis within classes and methods to attach test cases to entities at a .ner granularity. Not all of \nthose test cases necessarily execute changed code, or code that accesses changed data objects. Similarly, \na class D may be deter\u00admined by the ORD technique to be dependent by message passing on some class C, \nand if C is modi.ed, all test cases associated with D will be selected. Not all of these test cases necessarily \nexercised code involving interactions with C.In such cases, the ORD technique selects test cases that \ncould be omitted from retesting with no ill e.ects test cases that our technique does not select. Thus, \nour technique is more precise than the ORD technique. 7. CONCLUSIONS In this paper, we have presented \nthe .rst safe regression\u00adtest-selection algorithm that handles the Java language fea\u00adtures, that can \nbe applied (under certain conditions) to par\u00adtial programs, and that is more precise than existing ap\u00adproaches \nfor object-oriented software. We also presented a regression-test-selection system for Java, called Retest, \nthat implements our technique. With Retest, we per\u00adformed empirical studies to evaluate the e.ectiveness \nof our technique. Our empirical studies indicate that the technique can be e.ective in reducing the size \nof the test suite but that the reduction varies across subjects and versions. These re\u00adsults are consistent \nwith results reported for C programs, and, for our subject programs, do not show any trends that are \npeculiar to Java software. Additional studies are re\u00adquired to identify such trends. Our studies also \nindicate that regression-test-selection at a .ner granularity may provide further reductions in the test-suite \nsize. However, additional evaluation is required to determine the granularity of the test selection that \nprovides the best trade-o.s in precision and e.ciency. Our future work will include investigating viable \nalterna\u00adtives for instrumenting at di.erent levels, gathering addi\u00adtional subjects, and performing empirical \nstudies to evalu\u00adate the e.ectiveness of our technique. We will also perform studies to determine the \ne.ciency of our technique in prac\u00adtice. 8. ACKNOWLEDGMENTS This work was supported in part by grants \nto Georgia Tech from Boeing Aerospace Corporation, by National Sci\u00adence Foundation awards CCR-9988294, \nCCR-0096321, and EIA-0196145, and by the State of Georgia under the Ya\u00admacraw Mission. The work was also \npartially supported by the ESPRIT Project TWO (EP n.28940) and by the Italian Ministero dell Universit`a \ne della Ricerca Scienti.ca e Tecnologica (MURST) in the framework of the MOSAICO Project. Antonio Carzaniga, \nDavid Rosenblum, and Alexan\u00adder Wolf provided the Siena system and its versions. The anonymous reviewers \nprovided many comments and sugges\u00adtions that helped improve the presentation of the paper.  9. REFERENCES \n[1] Aristotle Research Group, Georgia Institute of Tech\u00adnology. Java Architecture for Bytecode Analysis. \n2000. [2] D. F. Bacon and P. F. Sweeney. Fast static analysis of C++ virtual function calls. In Proceedings \nof the 11th Annual Conference on Object-Oriented Program\u00adming Systems, Languages, and Applications, pages \n324 341, Oct. 1996. [3] T. Ball. On the limit of control .ow analysis for re\u00adgression test selection. \nIn ACM Int l Symp. on Softw. Testing and Analysis, pages 134 142, Mar. 1998. [4] J. Bible and G. Rothermel. \nA unifying framework sup\u00adporting the analysis and development of safe regres\u00adsion test selection techniques. \nTechnical Report 99-60\u00ad11, Oregon State University, Dec. 1999. [5] J. Bible, G. Rothermel, and D. Rosenblum. \nA com\u00adparative study of coarse-and .ne-grained safe regres\u00adsion test selection. ACM Transactions on Software \nEn\u00adgineering and Methodology, 10(2):149 183, Apr. 2001. [6] A. Carzaniga, D. S. Rosenblum, and A. L. \nWolf. Design and evaluation of a wide-area event noti.cation service. ACM Transactions on Computer Systems, \n19(3):332 383, Aug. 2001. [7] T. Cheatham and L. Mellinger. Testing object-oriented software systems. \nIn Proceedings of the 1990 Computer Science Conference, pages 161 165, 1990. [8] Y. F. Chen, D. S. Rosenblum, \nand K. P. Vo. TestTube: A system for selective regression testing. In Proceed\u00adings of the 16th International \nConference on Software Engineering, pages 211 222, May 1994. [9] B. F. Cooper, H. B. Lee, and B. G. Zorn. \nProfbuilder: A package for rapidly building java execution pro.lers. Technical report, University of \nColorado. [10] J. Dean, D. Grove, and C. Chambers. Optimizations of object-oriented programs using static \nclass hierachy analysis. In European Conference on Object-Oriented Programming, pages 77 101, 1995. [11] \nS. P. Fielder. Object-oriented unit testing. Hewlett-Packard Journal, 40(2):69 74, Apr. 1989. [12] T. \nGraves, M. J. Harrold, J.-M. Kim, A. Porter, and G. Rothermel. An empirical study of regression test \nse\u00adlection techniques. In Proceedings of the International Conference on Software Engineering, pages \n188 197, Apr. 1998. [13] M. J. Harrold, J. D. McGregor, and K. J. Fitzpatrick. Incremental testing of \nobject-oriented class inheritance structures. In Proceedings of the 14th International Conference on \nSoftware Engineering, pages 68 80, May 1992. [14] P. Hsia, X. Li, D. Kung, C-T. Hsu, L. Li, Y. Toyoshima, \nand C. Chen. A technique for the selective revalidation of OO software. Software Maintenance: Research \nand Practice, 9:217 233, 1997. [15] J.-M. Kim, A. Porter, and G. Rothermel. An empirical study of regression \ntest application frequency. In Pro\u00adceedings of the 22nd International Conference on Soft\u00adware Engineering, \npages 126 135, Jun. 2000. [16] D. Kung, J. Gao, P. Hsia, Y. Toyoshima, and C. Chen. Firewall regression \ntesting and software maintenance of object-oriented systems. Journal of Object-Oriented Programming, \n1994. [17] D. Kung, J. Gao, P. Hsia, Y. Toyoshima, C. Chen, Y- S. K im, and Y-K. Song. Developing an \nobject-oriented software testing and maintenance environment. Com\u00admunications of the ACM, 38(10):75 87, \nOct. 1995. [18] D. Kung, J. Gao, P. Hsia, F. Wen, Y. Toyoshima, and C. Ch en. On regression testing \nof object-oriented pro\u00adgrams. The Journal of Systems and Software, 32(1):21 40, Jan. 1996. [19] D. Kung, \nJ. Gao, P. Hsia, Y. Wen, and Y. Toyoshima. Change impact identi.cation in object-oriented soft\u00adware maintenance. \nIn Proceedings of the International Conference on Software Maintenance 94, pages 202 211, Sep. 1994. \n[20] H. K. N. Leung and L. J. White. A study of integration testing and software regression at the integration \nlevel. In Proceedings of the Conference on Software Mainte\u00adnance -1990, pages 290 300, Nov. 1990. [21] \nH. K. N. Leung and L. J. White. Insights into testing and regression testing global variables. Journal \nof Soft\u00adware Maintenance: Research and Practice, 2:209 222, Dec. 1990. [22] H. K. N. Leung and L. J. \nWhite. A cost model to com\u00adpare regression test strategies. In Proceedings of the Conference on Software \nMaintenance 91, pages 201 208, Oct. 1991. [23] D. Liang, M. Pennings, and M. J. Harrold. Extending and \nevaluating .ow-insensitive and context-insensitive points-to analyses for java. In Proceedings of the \nACM Workshop on Program Analyses for Software Tools and Engineering, Jun. 2001. [24] C. E. McDowell and \nD. P. Helmbold. Debug\u00adging concurrent programs. ACM Computing Surveys, 21(4):593 622, Dec. 1989. [25] \nSun Microsystems. Java Vir\u00adtual Machine Debug Interface. http://java.sun.com/products/jdk/1.2/docs/guide/jvmdi/. \n[26] Sun Microsystems. Java Virtual Machine Pro\u00ad.ler Interface. http://java.sun.com/products/jdk/\u00ad1.2/docs/guide/jvmpi/jvmpi.html. \n[27] Sun Microsystems. Java2 Platform, API Speci.cation. http://java.sun.com/j2se/1.3/docs/api/. [28] \nSun Microsystems. The Java Founda\u00ad tion Class Abstract Window Toolkit. http://java.sun.com/products/jdk/awt/. \n [29] G. Rothermel and M. J. Harrold. A safe, e.cient re\u00ad gression test selection technique. ACM Transactions \non Software Engineering and Methodology, 6(2):173 210, Apr. 1997. [30] G. Rothermel and M. J. Harrold. \nEmpirical studies of a safe regression test selection technique. IEEE Trans\u00adactions on Software Engineering, \n24(6):401 419, Jun. 1998. [31] G. Rothermel, M. J. Harrold, and J. Dedhia. Regres\u00adsion test selection \nfor C++ software. Journal of Soft\u00adware Testing, Veri.cation, and Reliability, 10(6):77 109, Jun. 2000. \n[32] Gregg Rothermel and Mary Jean Harrold. Analyzing regression test selection techniques. IEEE Transactions \non Software Engineering, 22(8):529 551, Aug. 1996. [33] S. Sinha and M. J. Harrold. Analysis and testing \nof programs with exception-handling constructs. IEEE Transactions on Software Engineering, pages 849 \n871, Sep. 2000. [34] F. Tip and J. Palsberg. Scalable propagation-based call graph construction algorithms. \nIn Proceedings of the Conference on Object-Oriented Programming Systems, Languages, and Applications, \npages 281 293, Oct. 2000. [35] F. Vokolos and P. Frankl. Pythia: A regression test se\u00adlection tool based \non text di.erencing. In International Conference on Reliability, Quality, and Safety of Soft\u00adware Intensive \nSystems, May 1997. [36] L. J. White and K. Abdullah. A .rewall approach for regression testing of object-oriented \nsoftware. In Pro\u00adceedings of 10th Annual Software Quality Week,May 1997. [37] L. J. White and H. K. N. \nLeung. A .rewall concept for both control-.ow and data-.ow in regression integra\u00adtion testing. In Proceedings \nof the Conference on Soft\u00adware Maintenance -1992, pages 262 270, Nov. 1992.  \n\t\t\t", "proc_id": "504282", "abstract": "Regression testing is applied to modified software to provide confidence that the changed parts behave as intended and that the unchanged parts have not been adversely affected by the modifications. To reduce the cost of regression testing, test cases are selected from the test suite that was used to test the original version of the software---this process is called regression test selection. A <i>safe</i> regression-test-selection algorithm selects every test case in the test suite that may reveal a fault in the modified software. Safe regression-test-selection technique that, based on the use of a suitable representation, handles the features of the Java language. Unlike other safe regression test selection techniques, the presented technique also handles incomplete programs. The technique can thus be safely applied in the (very common) case of Java software that uses external libraries of components; the analysis of the external code is note required for the technique to select test cases for such software. The paper also describes RETEST, a regression-test-selection algorithm can be effective in reducing the size of the test suite.", "authors": [{"name": "Mary Jean Harrold", "author_profile_id": "81100639551", "affiliation": "College of Computing, Georgia Institute of Technology, Atlanta, GA", "person_id": "PP14219545", "email_address": "", "orcid_id": ""}, {"name": "James A. Jones", "author_profile_id": "81100555440", "affiliation": "College of Computing, Georgia Institute of Technology, Atlanta, GA", "person_id": "PP33027216", "email_address": "", "orcid_id": ""}, {"name": "Tongyu Li", "author_profile_id": "81100475483", "affiliation": "College of Computing, Georgia Institute of Technology, Atlanta, GA", "person_id": "P344307", "email_address": "", "orcid_id": ""}, {"name": "Donglin Liang", "author_profile_id": "81100044201", "affiliation": "College of Computing, Georgia Institute of Technology, Atlanta, GA", "person_id": "P69014", "email_address": "", "orcid_id": ""}, {"name": "Alessandro Orso", "author_profile_id": "81100565192", "affiliation": "College of Computing, Georgia Institute of Technology, Atlanta, GA", "person_id": "PP14195884", "email_address": "", "orcid_id": ""}, {"name": "Maikel Pennings", "author_profile_id": "81100259391", "affiliation": "College of Computing, Georgia Institute of Technology, Atlanta, GA", "person_id": "PP33024732", "email_address": "", "orcid_id": ""}, {"name": "Saurabh Sinha", "author_profile_id": "81100336015", "affiliation": "College of Computing, Georgia Institute of Technology, Atlanta, GA", "person_id": "PP39038238", "email_address": "", "orcid_id": ""}, {"name": "S. Alexander Spoon", "author_profile_id": "81100256936", "affiliation": "College of Computing, Georgia Institute of Technology, Atlanta, GA", "person_id": "P739832", "email_address": "", "orcid_id": ""}, {"name": "Ashish Gujarathi", "author_profile_id": "81100054306", "affiliation": "Citrix Systems, Inc, Ft., Lauderdale, FL", "person_id": "P344291", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/504282.504305", "year": "2001", "article_id": "504305", "conference": "OOPSLA", "title": "Regression test selection for Java software", "url": "http://dl.acm.org/citation.cfm?id=504305"}