{"article_publication_date": "10-01-2001", "fulltext": "\n The Java Syntactic Extender (JSE) Jonathan Bachrach Keith Playford Arti.cial Intelligence Laboratory \nFunctional Objects, Inc. Massachussetts Institute of Technology 86 Chandler Street Cambridge, MA 02139 \nSomerville, MA 02144 jrb@ai.mit.edu keith@functionalobjects.com ABSTRACT The ability to extend a language \nwith new syntactic forms is a powerful tool. A su.ciently .exible macro system al\u00adlows programmers to \nbuild from a common base towards a language designed speci.cally for their problem domain. However, macro \nfacilities that are integrated, capable, and at the same time simple enough to be widely used have been \nlimited to the Lisp family of languages to date. In this pa\u00adper we introduce a macro facility, called \nthe Java Syntactic Extender (JSE), with the superior power and ease of use of Lisp macro systems, but \nfor Java, a language with a more conventional algebraic syntax. The design is based on the Dylan macro \nsystem, but exploits Java s compilation model to o.er a full procedural macro engine. In other words, \nsyn\u00adtax expanders may be implemented in, and so use all the facilities of, the full Java language. 1. \nINTRODUCTION A macro de.nes a syntactic extension to some core language and allows users to de.ne the \nmeaning of one construct in terms of other constructs. These declarations are called macro de.nitions \nand their uses are called macro calls. Macros provide the power of abstraction where functional abstraction \nwon t su.ce, a.ording: clarity, concision, and implementation hiding. As an example consider a Java loop \nsyntactic extension called forEach whose use would look like: forEach(Task elt in tasks) elt.stop(); \nand whose expansion would be: Iterator i = tasks.iterator(); while (i.hasNext()) { Task elt = (Task)i.next(); \nelt.stop(); } Permission to make digital or hard copies of part or all of this work or personal or classroom \nuse is granted without fee provided that copies are not made or distributed for profit or commercial \nadvantage and that copies bear this notice and the full citation on the first page. To copy otherwise, \nto republish, to post on servers, or to redistribute to lists, requires prior specific permission and/or \na fee. OOPSLA 01 Tampa Florida USA Copyright ACM 2001 1-58113-335-9/01/10 $5.00 The use of forEach is \nmore succinct than its expansion and hides implementation details that if changed would be di.\u00adcult to \nupdate without syntactic abstraction. Macros provide brevity which empowers users to do the right thing \nin situations that would otherwise be too taxing. Con\u00adsider Sun s standard JavaTest framework which is \nvery te\u00addious from the point of view of test authoring. For example, it s very verbose to write robust \ntests and tests involving ex\u00adceptions just go on and on. Furthermore, extra code must be written in order \nto be able to tell from a test log exactly where a failure occurred. In fact, because it s so long-winded \nto test for unexpected exceptions on a check-by-check basis like the .rst case, people don t tend to \nbother, causing test suites to bomb out at the .rst such failure. As an alternative consider a check \nmacro whose uses would look like: check foo.equals(bar); check foo.equals(bar) throws NullPointerException; \nand whose expansions would be respectively: try { logCheck(\"foo.equals(bar)\"); checkAssertion(foo.equals(bar)); \n} catch (Throwable t) { unexpectedThrowFailure(t); }; try { logCheck(\"foo.equals(bar) throws NullPointerException\"); \nfoo.equals(bar); noThrowFailure(); } catch (NullPointerException e) { checkPassed(); } catch (Throwable \nt) { incorrectThrowFailure(t); }; This macro makes the job of writing test suites much eas\u00adier and encourages \nthe test suite author to do the proper bookkeeping. It is sometimes clumsy in Java to de.ne data declaratively, \nbecause of the lack of optional arguments and restrictions about where array initializers can appear. \nFor example, when de.ning a Lisp-style list object, it is very convenient to have a list function that \nwould create a list with elements the same as the arguments. For example, it would be con\u00advenient to \ndo: f(list(x, y, z)); where f is called on a list with elements x, y, and z. More generally, allowing \nusers to de.ne a declarative syntax for de.ning data makes inputting, manipulating, and maintain\u00ading \ndata much more manageable (see [15] for another exam\u00adple). Macros also provide an e.ective vehicle for \nplanned growth of a language (see [3]). In Guy Steele s OOPSLA-98 invited talk entitled, Growing a Language \n[11], he says From now on, a main goal in designing a language should be to plan for growth. A large \npart of Lisp s success and longevity can be linked to its powerful macro facility, which, for example, \nallowed it to easily incorporate both Object-Oriented and Logic programming paradigms. Beyond merely \nproviding syntactic extension, allowing ar\u00adbitrary computation during the construction of replacement \nphrases and more generally having syntax expanders gener\u00adate and interoperate with Java code has a number \nof bene\u00ad.ts: The analysis and rewriting possible in a syntax ex\u00adpander is no longer constrained by a \nlimited pattern matching and substitution language.  The pattern matching and rewrite rule engine does \nnot have to be as complex and capable as it would otherwise have to be, since standard Java control and \niteration constructs can be used along with it.  It is possible to package and re-use syntax expansion \nutilities in the same way as any other useful Java code.  Elements of the pattern matching engine are \nopen to programmer extension.  1.1 Overview First, we describe a library which provides a code representa\u00adtion, \ncalled skeleton syntax tree (SST), source-level pattern matching and code construction utilities, and \nsource code I/O. Next, we present parsing in the face of syntactic exten\u00adsion. Then, we discuss how the \nsyntax expander mechanism can be layered on top of the previous building blocks and we discuss the syntactic \nextension execution model. Next, we discuss how our system ensures that variable references copied from \na macro call and from a macro de.nition mean the same thing in an expansion. Then we discuss source level \ndebugging in the face of macros. From there we dis\u00adcuss related syntactic extension systems. Next, we \ndescribe the current JSE implementation. Finally, we propose future directions. 2. SST AND FRAGMENTS \nThe fragments library provides a collection of classes suit\u00adable for representing fragments of source \ncode in what we call a skeleton syntax tree (SST) form. Skeleton syntax trees may be constructed and \npulled apart manually using the exported interface of these classes. Source level tools are provided \nfor easier parsing and construction. I/O facilities permit the reading and writing of a textual representation \nof SST s. A simpli.ed fragment class hierarchy is shown below: There are essary for f(x, y) + g[0] volved \nare 2.1 Source Level Tools for Fragments Working with Fragment classes directly is tedious and error\u00adprone. \nWhere possible, it is desirable for a programmers to be able to work in terms of the source code shapes \nthey are already familiar with. 2.1.1 Code Quotes An intuitive and accessible way of generating parameterized \ncode is with code quotes. These allow a programmer to write a prototype of the form they want to generate, \nbut with substitutions in place of the variable parts of the code. The evaluation of a code quote yields \nthe skeleton form represen\u00adtation of code. Tokens within a code quote are substituted as themselves, \napart from ? which indicates a parameterized insertion. A ? followed by a name substitutes the fragment \nbound to the local variable of that name (typically bound by pattern matching). A ? followed by an expression \nin parens indicates the insertion of the fragment which is the result of evaluating the expression inside \nthe parens. Evaluation of these substitutions occurs in the lexical environment in force where the code \nquote appears. Simple folding rules are applied to commas to make generating lists easier. As an example \nof code quotes and pattern variable evaluation consider: Fragment test = #{ isOn() }; Fragment then = \n#{ turnOff(); }; return #{ if (?test) ?then }; ==> #{ if (isOn()) turnOff(); } As example of code generation \nfor code quotes, consider the expansion of the above code snippet: Fragment test = Template.processTemplate \n(FragmentList.fnil .fpush(Template.substituteIdentifier(\"isOn\")) .fpush(Template.processParens(FragmentList.fnil)) \n.freverse()); Fragment then = Template.processTemplate (FragmentList.fnil .fpush(Template.substituteIdentifier(\"turnOff\")) \n.fpush(Template.processParens(FragmentList.fnil)) .fpush(Template.substituteSemicolon()) .freverse()); \nreturn Template.processTemplate (FragmentList.fnil .fpush(Template.substituteIdentifier(\"if\")) .fpush(Template.processParens \n (FragmentList.fnil.fpush(test))) .fpush(then) .freverse()); where Fragment is the most general code \nfragment class, and FragmentList is a Lisp-like list object used for containing sequences of fragments. \nTemplate objects represent code quotes and the Template class contains a number of facilities for their \nconstruction. As an example of expression evaluation consider: Fragment getter (IdentifierFragment id) \n{ return new IdentifierFragment (\"get\".concat(id.asCapitalizedString())); } Fragment name = new IdentifierFragment(\"width\"); \nreturn #{ x.?(getter(name))()} ==> #{ x.getWidth() } 2.1.2 Nesting Sometimes it is useful to be able \nto represent code quotes which when evaluated yield other code quotes. These nested code quotes can be \nused in situations such as macro-de.ning macros (see section 6.2 and [2]). The evaluation of pattern \nvariables and expressions is con\u00adtrolled through the use of multiple question marks: ??x, ??(f(x)), ???y. \nEvery #{ } wrapper introduces another level of nesting, and variables and expressions are evaluated when \nthe number of question marks equals the current code quote nesting level. Otherwise the variable or expression \nis left unevaluated. For example, return#{#{?x}}; ==>#{?x} Fragment x = #{ a }; return #{ #{ ??x } }; \n==> #{ a } Multiple and delayed evaluation can be achieved using a combination of ? and () s. For example: \nFragment x = #{ y }; return #{ Fragment y = #{ a }; return #{ ?(??x) }; }; ==> Fragment y = #{ a }; \nreturn #{ ?y }; ==> #{ a } Self Generating Code Quote One amusing and instructive use of nested code \nquotes is in writing self generating programs. There is a long standing tradition of writing self generating \ncode fragments [2], and nested code quotes can be used to express elegant solutions. Alan Bawden [2] \npresents a beautiful quasiquote solution written by Michael McMahon: (let ((let (let ((let ,let)) ,let))) \n(let ((let ,let)) ,let)) Following the same general format, a code quote solution would look like the \nfollowing: Fragment f = #{ #{ Fragment f = #{ ??f }; ?f; }; }; #{ Fragment f = #{ ??f }; ?f; }; We present \nthe following stand-alone self generating Java program, because the usual self-generating goal is to \nwrite a complete stand-alone self generating program. class selfish { static public void main (String \nargs[]) { Fragment f = #{ #{ class selfish { static public void main (String args[]) { Fragment f = #{ \n??f }; ?f.pprint(); } } } }; #{ class selfish { static public void main (String args[]) { Fragment f \n= #{ ??f }; ?f.pprint(); } } }.pprint(); }} For comparison s sake, Klil Neori s stand-alone self generat\u00ading \nJava program [14] is a typical Java solution: class P{public static void main(String args[]){ String \na=\"class P{public static void main (String args[]){String a=;System.out.println (a.substring(0,56)+((char)0x22)+a+((char)0x22)+a \n.substring(56));}}\"; System.out.println(a .substring(0,56)+((char)0x22)+a+((char)0x22)+a .substring(56));}} \nwhere this is meant to be all on one line. Although this isn t a typical Java application, we can still \nsee how poorly suited vanilla Java is for manipulating program representations. 2.1.3 Pattern Matching \nAn intuitive and accessible way of expressing a parser is with patterns. This allows a programmer to \nwrite out the gen\u00aderal shape of the expected input, but with pattern bindings in place of the variable \nparts of the code. The source-level parsing tools provided in the fragment library take this ap\u00adproach. \nThe primary parsing tool o.ered is syntaxSwitch (which is the moral equivalent of Lisp s destructure-bind). \nIt im\u00adplements pattern matching based on a simpli.ed version of Dylan s constraint-directed rewrite rule \nsystem. It is like Java s switch statement and looks as follows: syntaxSwitch (?:expression) { ?rules:* \n} where rules look like: case ?pattern:codeQuote : ?:statement with each pattern looking like the construct \nto be matched. Patterns are augmented by pattern variables which match and lexically bind to appropriate \nparts of the construct. As an example consider: Fragment input = #{ when (isOn()) turnOff(); }; syntaxSwitch \n(input) { case #{ when (?test:expression) ?body:statement }: return #{ if (?test) ?body }; } ==> #{ if \n(isOn()) turnOff(); } where the pattern variable ?test binds to isOn() and ?body binds to turnOff();. \nThe evaluation of a syntaxSwitch statement proceeds as follows. The input expression must evaluate to \na valid frag\u00adment. During evaluation, this expression is tested against each rule s pattern in turn. \nIf one of the patterns matches, its pattern variables are bound to local variables of the same name and \nthe corresponding right hand side statement is run in the context of those bindings. If multiple patterns \nmatch, then the rule corresponding to the .rst pattern to match is chosen. No provision is made to .ag \nrule ambiguities. If no patterns are found to match, then a SyntaxMatchFailure exception is thrown. Pattern \nvariables are denoted with ? pre.xing their names. Each pattern variable has a required constraint that \nrestricts the syntactic type of fragments that it matches (e.g., name, expression, body). A constraint \nis denoted with a colon separated su.x (e.g., ?class:name). A variable name de\u00adfaults to the given constraint \nname (e.g., ?:type is the same as ?type:type). A wildcard constraint (*) matches any\u00adthing and an ellipsis \n(...) is an abbreviation for a wildcard constrained pattern variable. Pattern matching on a particular \npattern proceeds from left to right. It employs a shortest .rst priority for wildcard variables. A simple \nbackup and retry algorithm is used to try to .nd a match by binding the wildcard to more and more tokens. \nA largest .rst priority is used for matching non-wildcard variables with a similar backup and retry algo\u00adrithm. \nPatterns match if and only if all of their subpatterns match. Non-pattern variable tokens appearing in \na pattern match only the same token in the input. Simple folding rules are applied to commas to make \nmatching lists easier. To illustrate one possible code generation strategy for the simplest usage of \nsyntaxSwitch, consider the following min\u00adimally cleaned up version of the generated code for the syntaxSwitch \nexample from the above example: Fragment __exp0 = input; try { FragmentList __e1 = __exp0.getInsideFragments(); \nSequenceFragment test = null; SequenceFragment body = null; FragmentList __fs2 = __e1.matchName(\"when\"); \nSplitList __tmp3 = __fs2.matchParens(); FragmentList __ns4 = __tmp3.getInside(); SplitList __tmp7 = __ns4.matchConstraint(\"expression\"); \ntest = new SequenceFragment(__tmp7.getBefore()); FragmentList __fs6 = __tmp7.getAfter(); __fs6.matchEmpty(); \nFragmentList __fs5 = __tmp3.getAfter(); SplitList __tmp9 = __fs5.matchConstraint(\"statement\"); body = \nnew SequenceFragment(__tmp9.getBefore()); FragmentList __fs8 = __tmp9.getAfter(); __fs8.matchEmpty(); \nreturn Template.processTemplate (FragmentList.fnil .fpush(Template.substituteIdentifier(\"if\")) .fpush(Template.processParens \n(FragmentList.fnil.fpush(test))) .fpush(body) .freverse()); } catch (SyntaxMatchFailure __e10) { throw \nnew SyntaxMatchFailure(); } where the code for creating test has been omitted. The ba\u00adsic strategy is \nto allocate all pattern variables up front and then step across the input matching against the pattern \n.ll\u00ading in pattern variables as the match proceeds. SplitList objects are used for pattern matching functions \nthat need to return both the matching continuation point (getAfter) and the fragments matched (getInside). \nFailures are han\u00addled with Java s exception mechanism and potentially raised inside each pattern matcher \nutility. In the case of multiple syntaxSwitch rules, subsequent rules would be emitted in a nested fashion \ninside nested catch code bodies. Wildcard pattern matching is handled with a try/catch inside a loop \nimplementing the shortest .rst match policy. 2.1.4 Built-in Constraint Types Several useful constraints \ncorresponding to oft-used Java grammar terminals and non-terminals are provided as a starting point for \nusers. The name constraint matches a sin\u00adgle Java identi.er. The type constraint matches a single well-formed \ntype declaration. The expression constraint matches a single well-formed expression. The statement constraint \nmatches either a single {} block or a single semi\u00adcolon terminated statement. In situations where the \n{} s are mandatory, it is necessary to put a {} in the pattern so that it matches literally and to use \nthe body constraint. The body constraint matches zero-or-more semicolon-terminated statements. Finally, \nthe switchBody constraint matches a sequence of zero or more switch statement clauses. 2.1.5 User-de.ned \nConstraint Types Beyond the builtin contraints, a mechanism is provided for users to de.ne their own \nconstraints. When a constraint is found in a pattern, the constraint implementation is re\u00adsolved by looking \nup a class with the constraint name suf\u00ad.xed with SyntaxConstraint. By making a class imple\u00admenting SyntaxConstraint \navailable on the CLASSPATH and following the naming convention described above, program\u00admers can introduce \ntheir own constraints. For example, the provided statement constraint is resolved to the class statementSyntaxConstraint. \nA canonical instance of a constraint class is consulted during pattern matching. For example, its admissibility \npredicate (implemented as a con\u00adstraint class s isAdmissable method) is called during pat\u00adtern matching \nwith decreasing numbers of input fragments such that the constrained pattern variable matches the max\u00adimal \nnumber of input fragments. A constraint de.nition itself minimally consists of a name to be used in pattern \nvariable declarations and an admissibility predicate that takes a list of fragments and returns true \nif it matches exactly. A mechanism is also provided for making available Java grammar nonterminals as \nconstraints.  3. PARSING AND SYNTAX EXPANSION Now that we have a representation for code, and we have \nsource level construction and parsing tools, we are ready to introduce syntactic extensions. First, we \nneed to under\u00adstand the context under which syntactic extensions are pro\u00adcessed. We assume that compilation \ninvolves the following phases: 1. Parse source code into skeletal syntax tree 2. Recursively expand \nmacros top-down 3. Create IR 4. Optimize IR 5. Emit code  Macro processing occurs during the .rst \ntwo phases from above. First, a surface parse of macro calls is performed to get a complete representation \nof calls in SST form. Sec\u00adond, this parsed form is handed to the macro expander for top-down pattern \nmatching and rewriting. Each individ\u00adual macro expansion replaces a given macro call with other constructs, \nwhich themselves can contain macro calls. This top-down process repeats until there are no macro calls \nre\u00admaining in the program. 3.1 Call Parsing Macro calls are limited to a few contexts and shapes cor\u00adresponding \nto existing Java syntactic contexts and shapes. Shapes serve to allow easy location of the end of a macro \nbefore handing it to the macro expander; shapes are a way to .nd the closing bracket . 3.1.1 Call Macros \nCall macros have function call syntax: name(...) and can occur in expression and statement position. \nThe start of a function call macro is the function name and the end is the last matching argument parenthesis. \nOne typical example is assert: assert(a >= 0, \"Underflow\"); where an exception will be thrown unless \na>= 0. 3.1.2 Statement Macros Statement macros can occur in positions where Java state\u00adments occur and \nhave the following basic shape: ... clause clause clause etc where the leading ellipsis stands in for \nzero-or-more mod\u00adi.ers (like public, private, or final) and a clause is one of: name ... terminator where \nname is a clause name (with the .rst clause name being the name of the macro) and terminator is either \na semicolon or a curly bracketed form. Examples of builtin Java statements are class, interface, if, \nwhile, for, try, do, and switch. In order to parse a statement macro, JSE needs to know up front all \nof the clause names in order to determine where a macro ends, there being no common end marker. The beginning \nof the macro call is the .rst token past the last terminator and the end is the .rst terminator not followed \nby an associated clause name. For example, to do the initial parse of try, the steps are something like: \n1. Find the token try and resolve it to a syntax expander. 2. Get a list of clause words from the expander, \nin this case exception and finally. 3. Read the lead clause, try{ ... }, in fragment form. 4. While \nthe next identi.er is a clause word, read the next clause.  Taking another example, class seems more \ncomplicated than try, yet from a form shape point of view, it s much simpler in that it has no clauses \nin this sense; all structure is internal to the lead clause, isolated within a {} block. Java itself \nhas 4. Now that we Macros are .rst de.ned and compiled by JSE producing .class .les containing class \nrepresentions of the de.ned macros. An application that uses these macros is then sub\u00adsequently compiled \nby JSE augmented by demand-loaded class .les corresponding to actually used macros. This then results \nin the .nal application class .les. It would be possible to manually write expander classes against the \ncore code fragment manipulation API provided, but the idea is that a higher level syntax form be used \n(see section 4.4.1), which provides source-level pattern matching and code generation facilities and \nwhich expands into an appropriate class de.nition. 4.1 Package Scoped Syntax De.ning a top level syntax \nexpander, then, allows new syn\u00adtax to be de.ned with the same apparent status as built-in forms like \nclass, interface, and while. For example, to get forItems syntax like: forItems (ContactCard card in \ndatabase) { // ... } ==> { Iterator _it = database.items(); while (_it.hasNext()) { ContactCard card \n= (ContactCard)_it.next(); // ... } } a top level class forItemsSyntaxExpander must have been de.ned \nand made visible through the .java .le s import declarations. When JSE sees an identi.er in a context \nwhere syntax ex\u00adpansion may apply, it attempts to resolve that identi.er, suf\u00ad.xed with SyntaxExpander, \nto a class according to the con\u00adtaining .le s import spec and target program CLASSPATH. If found, the \nclass is loaded into the language processor and the syntax following the leading name is parsed and expanded \naccording to its de.nition. If no de.nition is found, the identi.er is parsed as it would be in standard \nJava. 4.2 Class Scoped Syntax De.ning a nested syntax expander allows new class-speci.c syntax to be \nde.ned without polluting the top level package namespace. For example, to de.ne utility syntax like the \nabstraction of the open operation for a ContactDatabase class one does the following: ContactDatabase.withOpen \n(db = \"/Documents/Contacts\") { // ... } ==> { ContactDatabase db = ContactDatabase.open(\"/Documents/Contacts\"); \ntry { // ... } finally { db.close(); } } where a visible nested static class of ContactDatabase must \nhave been de.ned called withOpenSyntaxExpander. Processing is as above, except that identi.er resolution \nonly kicks in after the pre.x ContactDatabase has been seen. 4.3 Execution Model Issues Note that the \nexecution model described so far precludes the use of a nested syntax de.nition within the code of its \nenclosing class. This restriction could possibly be removed by having JSE extract and separately compile \nsyntax de.ni\u00adtions as they re encountered, but it is hard to get the scop\u00ading right in the resulting \ncode without cooperation from the compiler. Nesting a compile-time class within a run-time class has \nfurther potential for problems because, with a single set of imports, compile-time and run-time dependencies \nare con\u00ad.ated. Most of the time it is unlikely to be a problem, but if the enclosing class makes numerous \nstatic references, or worse if reference is made to something that can t be loaded at compile-time (e.g. \nif cross-compiling and JNI is involved), this could become an issue. Finally, using a single class path \nfor both looking up run\u00adtime classes and compile-time macro expanders could be generalized in order to \nbetter separate the two worlds. The simplest .x would be to provide a separate class path and compilation \narea for syntax expanders to avoid the above mentioned problems. 4.4 Syntax Expander Classes A syntax \nexpander de.nition consists of: Access speci.er  Macro name  List of clause names  Main rule set \n Extra class declarations  whose basic structure is de.ned in the SyntaxExpander in\u00adterface and whose \ndetails are speci.ed in a class de.nition implementing the SyntaxExpander interface. For example, the \nfollowing is a class de.nition for the forEach syntax expander: public class forEachSyntaxExpander implements \nSyntaxExpander { private static String[] clauseNames = {}; public String[] getClauseNames() { return \nclauseNames; } public Expansion expand (Fragment fragments) throws SyntaxMatchFailure { syntaxSwitch \n(fragments) { case #{ forEach (?:type ?elt:name in ?:expression) ?:statement }: return #{ Iterator i \n= ?expression.iterator(); while (i.hasNext()) { ?elt = (?type)i.next(); ?statement } }; }}} where clause \nnames are accessed with getClauseNames and the main rule is invoked with the expand method. 4.4.1 Syntactic \nExtension De.nitions In order to make it more convenient for de.ning syntactic extension we provide a \nsyntax expander named syntax hav\u00ading the following form: #{ ?modifiers:* syntax ?:name ?clauseNames:* \n{ ?mainRules:switchBody ?definitions:* }} It permits the forEach syntactic extension to be more suc\u00adcinctly \nwritten as: public syntax forEach { case #{ forEach (?:type ?elt:name in ?:expression) ?:statement }: \nreturn #{ Iterator i = ?expression.iterator(); while (i.hasNext()) { ?type ?elt = (?type)i.next(); ?statement \n} }; }  5. AUTOMATIC HYGIENE Two desirable properties of a Syntactic Extension system are hygiene and \nreferential transparency [17] [16] [13] [5] which both roughly mean that variable references copied from \na macro call and from a macro de.nition mean the same thing in an expansion. The mechanism attempts to \navoid acciden\u00adtal collisions between macro bindings and program bindings of the same name. In order to \nsupport these properties, each template name records its original name, lexical context, and speci.c \nmacro call context. A named value reference and a binding connect if and only if the original name and \nthe speci.c macro call occurrences are both the same. A new hygiene context is dynamically bound during \nexpansion, but for more precise control, hygiene contexts can also be manually established and dynamically \nbound. References to global bindings should mean the same thing as they did if looked up from within \nthe originating macro de.nition. Unfortunately, this is hard to do in Java without violating security. \nWe choose to force users to manually export macro uses. Getting this to work well in the absence of compiler \nor lan\u00adguage support is hard in places in Java, and simply im\u00adpossible in others. When implemented as \na preprocessor, avoiding shadowing local variables requires renaming and a detailed code walk. Ensuring \nthat any methods or other bindings referenced by code generated by a macro refer to the right things \nat a call point is di.cult in Java. Again a full code walk and insertion of fully-quali.ed names is required. \nHowever, this only works if the things referenced are accessible at the call point: private or package \naccess elements accessed from a public macro at a call point outside their scope present a problem. This \nwas solved for inner classes in Java by hav\u00ading the compiler implicitly liberalize the access declarations \nof anything private used by the inner class. It s possible something similar could be done here, but \nthen we re start\u00ading to substantially modify non-macro code in such a way as could violate security boundaries. \nWe may have to compromise here and guarantee correct res\u00adolution of references, but not necessarily their \naccessibility. 5.1 Circumventing Hygiene Sometimes it is necessary to circumvent hygiene in order to \npermit macros to introduce variables accessible by the lexi\u00adcal context from which the macro was called. \nFor example, imagine writing an if-like macro which executes the then statement when its test expression \nevaluates to a non-null value. Furthermore, as a convenience, it binds a special vari\u00adable named it to \nthe result of the test. The following is an example nif usage: nif (moby()) return it; else return false; \nand the following is the de.nition of nif: public syntax nif { case #{ nif ?test:expression ?then:statement \n?else:statement }: return #{ Object ?=it = ?test; if (?=it == null) ?else else ?then }; } where ?= defeats \nautomatic hygiene and makes the pre.xed variable available by way of variable references in the macro \ncall.  6. A FEW EXAMPLES In this section we present several JSE examples. 6.1 Parallel Iteration This \nexample expands on the previous forEach expander (in section 4.4.1) to include parallel iteration over \nmultiple collections. public syntax forEach { case #{ forEach (?clauses:*) ?:statement }: Fragment inits \n= #{ }; Fragment preds = #{ true }; Fragment nexts = #{ }; return #{ ?(loop(clauses, statement, inits, \npreds, nexts))}; private Fragment loop (Fragment clauses, Fragment statement, Fragment inits, Fragment \npreds, Fragment nexts) throws SyntaxMatchFailure { syntaxSwitch (clauses) { case #{ }: return #{ ?inits \nwhile (?preds) { ?nexts ?statement } }; case #{ ?:type ?:name in ?c:expression, ... }: Fragment newInits \n= #{ ?inits Iterator i = ?c.iterator(); }; Fragment newPreds = #{ ?preds &#38; i.hasNext() }; Fragment \nnewNexts = #{ ?nexts ?type ?name = (?type)i.next(); }; return #{ ?(loop(..., statement, newInits, newPreds, \nnewNexts)) }; } } } The basic strategy is to create an iterator state variable for each collection over \nwhich to iterate, to create a predicate that determines when any one of the collections is exhausted, \nand then to bind each given element name to subsequent col\u00adlection values. For example, the following \nmacro call would expand as follows: forEach (Point e1 in c1, Color e2 in c2) f(e1, e2); ==> Iterator \ni1 = c1.iterator; Iterator i2 = c2.iterator; while (true &#38; i1.hasNext() &#38; i2.hasNext()) { e1 \n= (Point)i1.next(); e2 = (Point)i2.next(); f(e1, e2); } The values of the expander variables are constructed \nthrough iterating over each forEach clause and when no more clauses remain, returning a code quote including \nthe .nal expander variable values. 6.2 The Accessible Macro It is recommended to create a functional \ninterface to .elds so as to hide their implementation as .elds to allow for im\u00adplementation changes without \na.ecting client code. The fol\u00adlowing example shows how to add a functional interface to .elds using a \nsyntax expander called accessible. The fol\u00adlowing is an example usage of the accessible macro: public \nclass RepeatRule { public accessible Date startDate; public accessible Date endDate; public accessible \nint repeatCount = 0; } and the macro itself is de.ned as follows: public syntax accessible { case #{ \n?mods:* accessible ?:type ?:name ?init:*; }: { Fragment getterName = new IdentifierFragment (\"get\".asCapitalizedString()); \nFragment setterName = new IdentifierFragment (\"set\".asCapitalizedString()); return #{ private ?type ?name \n?init; ?mods ?type ?getterName() { return ?name; } ?mods ?type ?setterName(?type newValue) { ?name = \nnewValue; } }; }} Note the creation of getter and setter identi.ers using an Identi.erFragment constructor. \n 6.3 A Macro De.ning Macro We now present an example of a macro which de.nes other macros. Suppose we \n.nd it useful to create aliases for partic\u00adular methods. Instead of de.ning a macro for each alias, we \ncould simplify our task by instead de.ning an alias de.ning macro which itself de.nes a macro for each \nalias. syntax defineAlias { case #{ defineAlias ?new:name = ?old:name }: return #{ syntax ?new { case \n#{ ??new ... }: return #{ ??old ... }; } }; }  7. TRACING AND DEBUGGING Without tracing and debugging \nmechanisms, macros can be di.cult to use. We provide a number of facilities which make it much easier \nto understand macros and when they re not working to .gure out why. JSE provides a macro expand facility \nwhich permits it to input a string containing a macro call and to output the resulting macro expansion. \nA macro call can either be fully expanded or expanded one level at a time. This can be used in smart \neditors to selectively macro expand marked regions of program source. Even with this selective macro \nexpansion support, it still can often be daunting to understand exactly why macros are failing. Often \nit is necessary to understand exactly what patterns are matching and how pattern variables are then subsequently \nbound. JSE provides both a global and per macro tracing .ag which controls this output. Finally, when \ncompiler errors occur, JSE maintains source locations through macro expansion such that the original \nmacro call source can be printed instead of the correspond\u00ading macro expansion. This gives programmers \ncompiler feed\u00adback based on the code they actually wrote. This can also be used during source level debugging. \nUnfortunately, be\u00adcause JSE supports procedural macros, it is possible that macros can arbitrarily rewrite \noriginal program source and thus in uses of more complicated macros, source level step\u00adping, for example, \ncould be a bit Disorienting. In practice though, we have found that simple source location propaga\u00adtion \ngives reasonable results. A number of facilities are made to gracefully handle the case of macro expanders \nthat crash or otherwise fail dur\u00ading expansion. Top level macro expansion exceptions raised during macro \nexpansion are handled and reported through a compiler error message along with appropriate context in\u00adformation. \nExpanders that fail to terminate will hang the compiler. One possible solution would be to provide a \nuser speci.ed timeout (implemented with a parallel timer thread) which, if reached, would again report \nnon-termination fail\u00adures through an expansion-time error message. 8. COMPARISONS 8.1 Dylan Macros Dylan \nmacros were the main inspiration for JSE, but unlike Dylan s rewrite-rule only macro system, JSE exploits \nJava s compilation model to o.er a full procedural macro engine. In other words, syntax expanders may \nbe implemented in, and so use all the facilities of, the full Java language. Because of this, JSE has \na simpler pattern matching and rewrite rule en\u00adgine utilizing standard Java control and iteration constructs \nfor instance. JSE includes a more complicated mechanism for .nding the extent of a macro call, because \nthe basic Java syntax does not include an easy mechanism for .nding the end bracket. 8.2 Lisp Macros \nLisp s destructuring and quasiquote facilities inspired Dy\u00adlan s macro system design. Their advent played \na big part in popularizing macros and Lisp itself [2]. We maintain that our patterns and templates are \nas natural to use as Lisp s destructuring and quasiquote. In fact, we feel that our splic\u00ading operator \n(?) is an easier to use uni.cation of quasiquote s unquote (,) and splicing (,@) operators. The reason \nthis is possible in JSE is because pattern variables can be bound to the actual elements of a sequence \nand not the sequence itself. For example, in Lisp, in order to splice in elements to the end of a parameter \nlist, one uses the splicing operator on a whole sequence as follows: (let ((more (c d e))) (a b ,@more)) \nwhile in JSE, one could do the same with the following: Fragment more = #{c, d, e}; return #{a, b, ?more}; \nwithout the need for a di.erent splicing operator. In order to better relate JSE s code quotes to Lisp \ns quasi\u00adquote, we present the following table showing a loose corre\u00adspondence between the two: Name Lisp \nJSE quasiquote () #{} unquote , ? splicing ,@ ? quote  N/A unquote-quote-unquote , , ?? unquote-unquote \n,, ?(??) We feel that for the typical nested code quote situation, JSE s multiple question mark is much \nmore intuitive to use. Unfortunately, several limitations restrict Lisp macros ease of use. First, variable \ncapture is a real problem and leads to di.cult to debug macros. Second, macro calls are di.cult to debug \nas Lisp macros do not o.er a mechanism for cor\u00adrelating between macro expanded code and a user s original \nsource code. Several other researchers have reported on systems that gen\u00aderalize the quasiquote mechanism \nto in.x syntaxes. Weise and Crew [21] are discussed below. Engler, Hsieh, and Kaashoek [8] employ a version \nof quasiquote to support a form of partial evaluation in C. 8.3 Scheme Macros Macro systems for Scheme \n(e.g., syntax-rules) come the closest to o.ering the power and ease of use of JSE. The major restriction \nwith Scheme macro systems is that they are restricted to languages with sexpr-based syntax. Ignor\u00ading \nthis major restriction, we feel that JSE provides a much more cohesive whole that gracefully progresses \nfrom a self\u00adcontained pattern language to the full power of procedural macros. In this section we will \nbrie.y compare JSE to R5RS s [12] syntax-rules rewrite-only and the syntax-case [7] proce\u00addural macro \nsystems . It is worth mentioning that other Scheme systems exist (e.g., [5]), but they are beyond the \nscope of this paper. syntax-rules is restricted to a rewrite\u00adonly system and thus arbitrary Scheme code \ncan not be uti\u00adlized during macro expansion. The syntax-case extension lifts this restriction. In JSE, \nthe same basic pattern match\u00ading language naturally incorporates the full Java language when writing \nprocedural macros. Even without considering this major limitation, we feel that JSE makes typical macro \nwriting more natural. For exam\u00adple, in syntax-case, a programmer is required to introduce local pattern \nvariables using with-syntax whereas, in JSE, a programmer merely introduces them with the usual local \nvariable declaration syntax. For example in syntax-case one must write the following: (lambda (f) (with-syntax \n((stuff f)) (syntax stuff))) whereas in JSE, one could do the equivalent with the fol\u00adlowing: Fragment \ndoit (Fragment f) { return #{ ?f }; } without having to explicitly introduce f as a pattern vari\u00adable. \nFurthermore, both syntax-rules and syntax-case re\u00adquire users to specify reserved intermediate words \nup front; otherwise names occurring within a pattern or template are interpreted as pattern variables. \nIn JSE, pattern variables have a special notation and thus reserved intermediate words do not need to \nbe declared ahead of time. syntax-case and syntax-rules provide nice solutions to hygiene that automatically \navoid most variable capture er\u00adrors. JSE improves on this by providing an intuitive no\u00adtation (i.e., \n?=) for circumventing hygiene. Consider the syntax-case version of the following JSE nif macro de.ned \nabove: (define-syntax nif (lambda (x) (syntax-case x () ((nif test then else) (with-syntax ((it (datum->syntax-object \n(syntax nif) it)) (syntax (let ((it test)) (if test then else))))))))) Notice how in syntax-case, one \nmust use a long-winded call and a with-syntax binding to produce the desired it variable. Note that R5RS \ns syntax-rules provides no way to disable hygiene. One advantage Scheme macros have over JSE is that \none can limit a macro s visibility to a lexically local region of code using let-syntax and letrec-syntax. \nAlthough not as precise, JSE does provide class-scoped macros. 8.4 Grammar Extension Macros Grammar \nextension macros allow a programmer to make incremental changes to a grammar in order to extend the syntax \nof the base language. JSE is less ambitious in that it provides a convenient and powerful mechanism for \nextending the syntax in limited ways. In particular, it provides only a limited number of shapes and \nrequires that macros must always commence with a name. We feel that this tradeo. is justi.ed because \nit makes the vast majority of macros easier to write. 8.4.1 Programmable Syntax Macros Weise and Crew \n[21] describe a macro system for in.x syntax languages such as C. Their macro system is programmable \nin an extended form of C, guarantees syntactic correctness of macro produced code, and provides a template \nsubsti\u00adtution mechanism based on Lisp s quasiquote mechanism. Their system lacks support for hygiene, \nbut instead requires programmer intervention to avoid variable capture errors. Unfortunately, their system \nis restrictive. Their macro syn\u00adtax is constrained to that describable by what is, essentially, a weak \nregular expression. In contrast, in our system, within the liberal shapes of an SST, just about anything \ngoes, and further, the fragments can be parsed using any appropriate technique. Finally, because templates \nare eagerly parsed, it s not clear that forward references within expanders (and so mutual recursion, \nsay) works in their system. It could be made to work, however (by forward declaring the input syn\u00adtax \napart from the transformer), but this would be awkward to use. Their system requires knowledge of formal \nparsing intrica\u00adcies. They say: The pattern parser used to parse macro invoca\u00adtions requires that detecting \nthe end of a repe\u00adtition or the presence of an optional element re\u00adquire only one token lookahead. It \nwill report an error in the speci.cation of a pattern if the end of a repetition cannot be uniquely determined \nby one token lookahead. We think that it is di.cult for programmers to understand and solve static grammar \nambiguities like this. We believe that as far as possible, programmers should only have to know the concrete \nsyntax, not the abstract syn\u00adtax. Unfortunately, their system requires programmers to use syntax accessors \nto extract syntactic elements, whereas our system allows access through pattern matching concrete syntax. \nTheir system requires templates to always be consistent. We feel that it s often useful to be able to \ngenerate incomplete templates containing macro parameters for instance, that are spliced together at \nthe end of macro processing to form something recognizable. Weise and Crew s insistence that the result \nof evaluating a template should always be a rec\u00adognizable syntactic entity defeats that mechanism. Being \nable to work easily with intermediate part-expressions that have no corresponding full-AST class (e.g. \na disembodied pair of function arguments) is a win for the SST approach. 8.4.2 Metamorphic Syntax Macros \nBraband and Schwartzbach [3] introduce a macro system that improves on Weise and Crew s system but is \nstill laden with similar restrictions. In particular, it is tedious to write complicated macros because \ntheir system is based solely on rewrite rule extensions to a base grammar. Although, their system can \nexpress almost all macros expressible in JSE, having full access to Java control and data structures \nmakes writing demanding macros in JSE much easier. Their ad\u00advantage is that they can within their system \nprove at macro de.nition time that macros will always produce admissible expansions. 8.4.3 Camlp4 Camlp4 \n[6] is a pre-processor-pretty-printer for Objective Caml. Grammar tools and o.ers the ability to modify \nthe concrete syntax of the language including new quotations and syntactic extensions. It uses a recursive \ndescent parser and users extend the syntax by adding new rewrite rules. Users can specify levels of precedence \nso as to help order rules. Rewrite rules are written in terms of pattern match\u00ading with replacement patterns \nand similar restrictions apply. Finally, they o.er no guarantees about parseability and/or correctness. \n 8.5 Java Macro Systems 8.5.1 Jakarta Tool Set The Jakarta Tool Set [1] (JTS) provides a set of tools \nfor creating domain speci.c languages. The .rst of those tools, named Jak, adds support for meta-programming \nto Java. In particular, Jak supports the de.nitions of AST constructors and o.ers hygiene facilities. \nAST s are created using typed code quotes, and are manipulated using a tree walk. This is in contrast \nto our procedurally extensible rewrite-rule sys\u00adtem which mostly shields the user from the details of \nthe underlying AST. The second JTS tool, named Bali, is essentially a parser generator, addressing the \nneed for creating syntactic exten\u00adsions in a more familiar BNF style with regular-expression repetitions. \nThe Bali productions are associated with the class of objects created when a production .res. The result \nof parsing is an AST which can be further modi.ed through a tree walk. This system is similar to the \nabove mentioned grammar extension macro systems. 8.5.2 EPP EPP [9] is a Java system for extending a \nrecursive descent parser and lexical analyzer using the composition of mix\u00adins. The parser consists of \nfunctions which parse each of the non-terminals returning corresponding AST s. Users can de.ne parser \nmixins in order to extend these functions by de.ning their own non-terminal parsers which can aug\u00adment \nand/or override inherited functions. No guarantees are made about the interactions between these mixins. \nSimilar concerns about ease of use apply to their system. Users are asked to understand the idiosyncrasies \nof a grammar and the interactions of grammar extension combinations. Although more limited, we consider \nthe manipulation of surface syn\u00adtax (as in JSE) to be a much less daunting enterprise. 8.5.3 JPP JPP \n[18] implements a .xed set of language extensions rather than a means for Java programmers to add their \nown. It supports cpp-style conditional compilation, but not substi\u00adtution macros: Macros like the standard \nC preprocessor has are the cause of many bugs, and because of this I have no plans to support them . \n8.5.4 Open Java OpenJava [20] is a compiler supporting a compile-time meta object protocol (MOP) for \nJava. A number of protocols for adding or modifying features of the Java language are provided, including \na limited syntax extension mechanism. Syntactic extension is limited to only a few certain places in \nclass de.nitions (e.g., class adjectives) and their uses (e.g., after class names in callers). This allows \nthe system to parse programmer input without potential grammar con.icts. Al\u00adthough an impressive system, \nits main focus is on seman\u00adtic rather than syntactic extension. While limited in the amount of potential \nsurface syntax extension, their system allows for more powerful extensions that can depend on log\u00adical \nor contextual information such as types. A number of other MOP-based systems are worth mention\u00ading. In \nparticular, Chiba introduces OpenC++ [4] which is very similar to OpenJava and shares most of its relavent \nstrengths and weaknesses. MPC++ [10] is a powerful system which de.nes a compile\u00adtime metalevel architecture \nincluding a mechanism for ex\u00adtending limited parts of the C++ grammar. Syntactic ex\u00adtensions are de.ned \nby writing what amount to non-terminal mixins similar to EPP. Code quotes are introduced, but gen\u00aderally \nthe user is required to construct code fragments using object constructors. Furthermore, MPC++ does not \no.er any sort of high level pattern matching facilities nor auto\u00admatic hygiene support.   9. IMPLEMENTATION \nSTATUS JSE is currently implemented as a preprocessor taking .jse .les and producing .java .les. Hygiene \nis currently unim\u00adplemented, although we have con.dence in the the described design as it is based on \nour Dylan procedural macro system used in Functional Objects production compiler. A full SST library \nis implemented and documented. JSE is available as open source software from www.ai.mit.edu/~jrb/jse. \n10. FUTURE WORK Many important future directions remain. We would like to see JSE provide more guarantees \nabout macro de.nitions always producing admissible expansions. We would like to extend JSE to allow for \nsymbol macros and to support gener\u00adalized variables such as CommonLisp s setf. Furthermore, JSE seems \nlike a very nice substrate for staged compilation as exhibited in C (see also [19]). Finally, we think \nthat our approach could be fruitfully applied to other languages such as C and Scheme. 11. CREDITS A \nlarge amount of inspiration for JSE came from the Dy\u00adlan macro system. Much of the initial design work \non Dylan macros was done at Apple. The syntax of macro de.nitions, patterns, constraints, and templates \nused in Dylan s stan\u00addard macro system was developed by Mike Kahl. Dylan s very .rst loose grammar was \ndue to David Moon, who also designed a pattern matching and constraint parsing model suitable for such \na grammar. Earlier, Moon had proposed a model of compile-time evaluation for Dylan in the context of \na pre.x-syntax macro system from which ours takes some terminology. Generalizations of Dylan s loose \ngrammar enabling it to de\u00adscribe all Dylan s syntactic forms, along with the .rst imple\u00admentation of \nthe macro system, were developed by the au\u00adthors while at Harlequin Ltd. The procedural macro system \nwas initially designed and implemented as a component of Harlequin s Dylan compiler, now owned by Functional \nOb\u00adjects, Inc. Many other Dylan Partners made contributions to the design of Dylan macros, particularly \nthe Gwydion team at CMU. This paper bene.tted from helpful discussions with Alan Bawden, Andrew Blumberg, \nTony Mann, Scott McKay, Dave Moon, and Greg Sullivan. Finally, we would like to thank the anonymous OOPSLA \nreviewers for their many helpful suggestions. 12. REFERENCES [1] Don Batory, Bernie Lofaso, and Yannis \nSmaragdakis. JTS: Tools for implementing domain-speci.c languages. In P. Devanbu and J. Poulin, editors, \nProceedings: Fifth International Conference on Software Reuse, pages 143 153. IEEE Computer Society Press, \n1998. [2] Alan Bawden. Quasiquotation in Lisp. In SIGPLAN Workshop on Partial Evaluation and Semantics-Based \nProgram Manipulation, pages 4 12. ACM, January 1999. [3] Braband and Schwartzbach. Growing languages \nwith metamorphic syntax macros. available on their web site, 2000. [4] Shigeru Chiba. A metaobject protocol \nfor C++. In Proceedings of OOPSLA 95, pages 285 299, October 1995. [5] William Clinger. Hygienic macros \nthrough explicit renaming. ACM LISP Pointers, 4(4):25 28, 1991. [6] Daniel de Rauglaudre. Camlp4. http://caml.inria.fr/camlp4/, \n2001. [7] R. Kent Dybvig, Robert Hieb, and Carl Bruggeman. Syntactic abstraction in scheme. Lisp and \nSymbolic Computation, 5(4):295 326, 1993. [8] E.R. Engler, W.C. Hsieh, and M.F. Kaashoek. c:A language \nfor fast, e.cient, high-level dynamic code generation. In Proceedings of Symposium on Principles of Programming \nLanguages, January 1996. [9] Yuuji Ichisugi. Modular and extensible parser implementation. http://www.etl.go.jp/~epp/edoc/epp-parser.pdf, \n2000. [10] Yutaka Ishikawa, Atsushi Hori, Mitsuhisa Sato, Motohiko Matsuda, Jorg Nolte, Hiroshi Tezuka, \nand Hiroki Konaka. Design and implementation of metalevel architecture in C++ MPC++ approach . In Proceedings: \nRe.ection 96, 1996. [11] Guy Lewis Steele Jr. Growing a language. Lisp and Symbolic Computation, 1998. \n[12] R. Kelsey, W. Clinger, and J. Rees. Revised5 report on the algorithmic language scheme. Higher-Order \nand Symbolic Computation, 11(1):7 105, 1998. [13] E. Kohlbecker, D. P. Friedman, M. Felleisen, and B. \nDuba. Hygenic macro expansion. In Proceedings of the 1986 ACM Conference on Lisp and Functional Programming, \npages 151 161. ACM, ACM, August 1986. [14] Klil Neori. Self generating java program. available on a web \nsite, 1999. [15] Peter Norvig. DEFTABLE: A macro for implementing tables. ACM LISP Pointers, 5(4):32 \n38, 1992. [16] C. Queinnec. Lisp In Small Pieces. University Press, Cambridge, 1994. [17] A. Shalit. \nThe Dylan Reference Manual. Addison Wesley, 1996. [18] Nik Shaylor. Java preprocessor (JPP). http://www.geocities.com/CapeCanaveral/Hangar/4040/jpp.html, \n1996. [19] Walid Taha and Tim Sheard. Multi-stage programming with explicit annotations. In SIGPLAN workshop \non partial evaluation and semantics-based program manipulation, pages 203 217, 1997. [20] Michiaki Tatsubori. \nOpen Java. http://www.hlla.is.tsukuba.ac.jp/~mich/openjava/, 2000. [21] Daniel Weise and Roger Crew. \nProgrammable syntax macros. In Proceedings of the SIGPLAN 93 Conference on Programming Language Design \nand Implementation, pages 156 165, June 1993.   \n\t\t\t", "proc_id": "504282", "abstract": "The ability to extend a language with new syntactic forms is a powerful tool. A sufficiently flexible macro system allows programmers to build from a common base towards a language designed specifically for their problem domain. However, macro facilities that are integrated, capable, and at the same time simple enough to be widely used have been limited to the Lisp family of languages to date. In this paper we introduce a macro facility, called the Java Syntactic Extender (JSE), with the superior power and ease of use of Lisp macro sytems, but for Java, a language with a more conventional algebraic syntax. The design is based on the Dylan macro system, but exploits Java's compilation model to offer a full procedural macro engine. In other words, syntax expanders may be implemented in, and so use all the facilities of, the full Java language", "authors": [{"name": "Jonthan Bachrach", "author_profile_id": "81100076377", "affiliation": "Artificial Intelligence Laboratory, Massachussetts Institute of Technology, Cambridge, MA", "person_id": "P344300", "email_address": "", "orcid_id": ""}, {"name": "Keith Playford", "author_profile_id": "81100619866", "affiliation": "Functional Objects, Inc., 86 Chandler Street, Somerville, MA", "person_id": "P159118", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/504282.504285", "year": "2001", "article_id": "504285", "conference": "OOPSLA", "title": "The Java syntactic extender (JSE)", "url": "http://dl.acm.org/citation.cfm?id=504285"}