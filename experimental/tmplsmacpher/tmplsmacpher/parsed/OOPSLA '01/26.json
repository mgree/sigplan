{"article_publication_date": "10-01-2001", "fulltext": "\n An On-the-Fly Reference Counting Garbage Collector for Java * Yossi Levanoni Microsoft Corporation \nOne Microsoft Way Redmond, WA 98052 USA  ylevanon@microsoft.com ABSTRACT Reference counting is not naturally \nsuitable for running on multiprocessors. The update of pointers and reference counts requires atomic \nand synchronized operations. We present a novel reference counting algorithm suitable for a multipro\u00adcessor \nthat does not require any synchronized operation in its write barrier (not even a compare-and-swap type \nof syn\u00adchronization). The algorithm is e.cient and may compete with any tracing algorithm. We have implemented \nour algorithm on SUN s Java Virtual Machine 1.2.2 and ran it on a 4-way IBM Net.nity 8500R server with \n550MHz Intel Pentium III Xeon and 2GB of physical memory. It turns out that our algorithm has an extremely \nlow latency and throughput that is comparable to the mark and sweep algorithm used in the original JVM. \nKeywords: Runtime systems, Memory management, Garbage collection, Reference counting. 1. INTRODUCTION \nAutomatic memory management is well acknowledged as an important tool for a fast development of large \nreliable soft\u00adware. It turns out that the garbage collection process has an important impact on the overall \nruntime performance. The amount of time it takes to handle allocation and reclamation of memory spaces \nmay reach as high as 30% of the overall running time for realistic benchmarks. Thus, a clever design \nof e.cient memory management and garbage collector is an important goal in today s technology. *Most \nof this work was done while the author was at the Dept. of Computer Science, Technion -Israel Institute \nof Technology, Haifa 32000, Israel. This research was supported by the Coleman Cohen Aca\u00addemic Lecturship \nFund and by the Technion V.P.R. Fund -Steiner Research Fund. Permission to make digital or hard copies \nof part or all of this work or personal or classroom use is granted without fee provided that copies \nare not made or distributed for profit or commercial advantage and that copies bear this notice and the \nfull citation on the first page. To copy otherwise, to republish, to post on servers, or to redistribute \nto lists, requires prior specific permission and/or a fee. OOPSLA 01 Tampa Florida USA Copyright ACM \n2001 1-58113-335-9/01/10 $5.00 Erez Petrank Dept. of Computer Science Technion \u00adIsrael Institute of \nTechnology Haifa 32000 Israel erez@cs.technion.ac.il 1.1 Automatic memory management on a mul\u00adtiprocessor \nIn this work, we concentrate on garbage collection for multi\u00adprocessor machines. Multiprocessor platforms \nhave become quite standard for server machines and are also beginning to gain popularity as high performance \ndesktop machines. Many well studied garbage collection algorithms are not suitable for a multiprocessor. \nIn particular, many collectors (mmong them the collector supplied with Javasoft s Java Virtual Machine) \nrun on a single thread after all program threads have all been stopped (the so-called stop-the-world \nconcept). This causes bad processor utilization, and hinders scalability. In order to make better use \nof a multiprocessor, concurrent collectors have been presented and studied (see for exam\u00adple, [6, 37, \n17, 3, 13, 14, 8, 19, 31, 18, 34]). A concurrent collector is a collector that does most of its collection \nwork concurrently with the program without stopping the pro\u00adgram threads. Most of the concurrent collectors \nneed to stop all program threads at some point during the collec\u00adtion, in order to initiate and/or .nish \nthe collection, but the time the mutators must be in a halt is short. Stopping all the threads for the \ncollection is an expensive operation by itself. Usually, the program threads cannot be stopped at any \npoint. Rather, they should be stopped at safe points at which the collector can safely determine the \nreachability graph and properly reclaim unreachable objects. Thus, each thread must wait until the last \nof all threads cooperate and come to a halt. This hinders the scalability of the system, as the more \nthreads there are the more delay the system su.ers. Furthermore, if the collector is not running in parallel \n(which is usually the case), then during the time the program threads are stopped, only one of the processors \nis utilized. Therefore, it is advantageous to use on-the-.y collectors [17, 19, 18]. On-the-.y collectors \nnever stop the program threads simultaneously. Instead, each thread cooperates with the collector at \nits own pace through a mechanism called (soft) handshakes. We remark that another alternative for an \nadequate garbage collection on a multiprocessor is to perform the collection in parallel (see for example \n[25, 12, 30, 26, 21, 28]). We do not explore this avenue further in this work. 1.2 Reference counting \non a multiprocessor Reference counting is a most intuitive method for automatic storage management. As \nsuch, systems using reference count\u00ading were implemented starting from the sixties (c.f. [11].) The main \nidea is that we keep for each object a count of the number of references that reference the object. When \nthis number becomes zero for an object o, we know that o can be reclaimed. At that point, o is added \nto the free list and the counters of all its predecessors (i.e., the objects that are referenced by the \nobject o) are decremented, initiating perhaps more reclamations. Reference counting seems very promising \nto future garbage collected systems. Especially with the spread of the 64 bit architectures and the increase \nin usage of very large heaps. Tracing collectors must traverse all live objects, and thus, the bigger \nthe usage of the heap (i.e., the amount of live objects in the heap), the more work the collector must \nper\u00adform. Reference counting is di.erent. The amount of work is proportional to the amount of work done \nby the user pro\u00adgram between collections plus the amount of space that is actually reclaimed. But it \ndoes not depend on the space consumed by live objects in the heap. The study and use of reference counting \non a multiproces\u00adsor has not been as extensive and thorough as the study of concurrent and parallel tracing \ncollectors. The reason is that reference counting has a seemingly inherent problem with respect to concurrency: \nthe update of the reference counts must be atomic since they are being updated by all program threads. \nFurthermore, when updating a pointer, a thread must know the previous value of the pointer-slot being \nupdated, in spite of many such writes occuring in par\u00adallel. Otherwise, a confusion occurs in the bookkeeping \nof the reference counts. Thus, the naive solution requires a lock on any update operation. More advanced \nsolutions have re\u00adcently reduced this overhead to a compare-and-swap opera\u00adtion, which is still a time \nconsuming write-barrier. 1.3 This work In this work, we present a new on-the-.y reference counting garbage \ncollector with extremely .ne synchronization. In particular, we avoid any synchronization in the write \nbar\u00adrier. We proceed with an overview on the novel ideas in our algorithm, with which we could obtain \nthis advantage. A detailed precise description of these ideas is given in the rest of the paper. Our \nalgorithm, following Deutsch and Bobrow s Deferred Reference Counting [15], does not keep account of \nchanges to local pointers (in stack and registers) since keeping this account is too expensive. Instead, \nit only keeps account of pointers in the heap (denoted heap reference count). Once in a while (when garbage \ncollection is required), the collector inspects all objects with heap reference count zero. Those not \nreferenced by the roots may be reclaimed. Our .rst ob\u00adservation is that many more updates of the reference \ncounts are redundant and may be avoided. Consider a pointer slot that, between two garbage collections \nis assigned the values o0,o1,o2,... ,on for objects o0,... ,on in the heap. There are 2n updates of reference \ncounts made for these assign\u00adments: RC(o0)--, RC(o1)++, RC(o1)--, RC(o2)++, ... , RC(on)++. However, \nonly two are required: RC(o0)--and RC(on)++. Building on this observation, we note that in order to update \nall reference counts of all objects before a garbage collection, it is enough to know which pointer slots \nhave been modi.ed between the collections, and for each such slot, we must be able to tell what its value \nin the pre\u00advious garbage collection was, and what its current value is. In our algorithm, we keep a record \nof all pointer slots that have been modi.ed. We also keep the old value that ex\u00adisted in the slot before \nit was .rst modi.ed. It may seem that we have a problem to obtain this value in a concurrent setting, \nand indeed, special care must be used to make sure that this value is properly registered. However, we \ndo that without any synchronization operation. We denote the al\u00adgorithm resulting from the discussion \nso far as the snapshot algorithm. The exact details of the snapshot algorithm are presented in Section \n3 below. Next, we look at the collection itself. The naive implemen\u00adtation of the above approach is to \nstop all the threads and read the values currently kept in all modi.ed slots. This translates to taking \na snapshot of the heap (or only a snap\u00adshot of the interesting .elds in the heap). Such an approach does \nnot allow full concurrency of the collector (it is not on\u00adthe-.y), although it is sound. In order to \nmake the collector on-the-.y, we borrow ideas from the world of distributed computing. When taking a \nsnapshot in a distributed en\u00advironment, one does not stop all the computers over the distributed environment. \nInstead, one takes a snapshot of each computer at a time, but as the snapshots are being recorded, special \ncare is taken to avoid confusion due to the non-instantaneous view. For example, all messages between \ncomputers are recorded as well. In our case, we will use a similar solution. We will take a non-instantaneous \nview of the interesting pointer slots in the heap, but while checking these slots, we will use a special \nmechanism to avoid confu\u00adsion. We denote this view of the heap the sliding view. The sliding view algorithm \nis brie.y described in Section 4. Due to lack of space, we provide the details of the sliding view algorithm \nas well as implementation details, the allocator mechanism, and a proof of correctness for the algorithm \nin the technical report [29]. 1.4 Cycle collection A major disadvantage of reference counting is that \nit does not collect cycles. We have chosen to collect cycles with an on-the-.y mark-and-sweep collector. \nThe Mark-and-sweep algorithm is run seldom to collect cycles and restore stuck reference counts. (Like \nin [35, 43, 39, 9], we use only two bits for the reference count and thus, stuck counters are created \non the .y, and are restored by the mark-and-sweep algorithm.) We use a novel on-the-.y mark-and-sweep \ncollector that we have designed especially for our reference counting al\u00adgorithm. Note that it is quite \nnatural to base a mark-and\u00adsweep collector on a snapshot of the heap. The marking can be done on the \nsnapshot view of the heap, and since unreach\u00adable objects remain unreachable, changes in the heap do \nnot foil the collection of garbage. We adapt this basic idea to the sliding view notion, thus obtaining \na tracing collector perfectly .tting into our setting. We do not elaborate on the mark-and-sweep collector \nin this paper. The algorithm is described in our technical report [29]. All measurements of throughput \nand latency in this paper are reported for the reference count collector run most of the times and the \non-the-.y mark-and-sweep run seldom. 1.5 Memory consistency The algorithm presented in the paper requires \na sequentially consistent memory. However, three simple modi.cations can make the algorithm suitable \nfor platforms that do not pro\u00advide sequentially consistent memory. We remark that we did not encounter \nproblems in the runs we made on the In\u00adtel platform. We list the modi.cations required and discuss their \ncost in Section 5 below. 1.6 Implementation We have implemented our algorithm on SUN s Reference Release \n1.2.2 of the Java Virtual Machine and ran it on a 4-way IBM Net.nity 8500R server with a 550MHz Intel \nPentium III Xeon processor and 2GB of physical memory. We used the two standard Java multithreaded benchmarks: \nSPECjbb2000 and the mtrt benchmark from SPECjvm98. These benchmarks are described in detail in SPEC s \nWeb site[38]. It turns out that our algorithm has an extremely low latency. It improves over the original \nJVM by two or\u00adders of magnitude. As for e.ciency, the JVM with our ref\u00aderence counting collector bits \nthe original JVM by up to 10% improvement in the overall running time with the mtrt benchmark. As for \nSPECjbb, if we allow a large maximum heap (which is the target of our collector), then our collector \nslightly improves over the running time of the original JVM. With smaller heaps the original JVM does \nbetter than ours for SPECjbb. 1.7 Results In Section 6 we report the measurements we ran with our collector. \nWe note that the throughput is basically the same as the original tracing collector. For the multithreaded \nmtrt benchmark our collector improved the throughput up to a 10% di.erence in the overall running time. \nFor SPECjbb00 our collector had a throughput similar to the original col\u00adlector. In terms of latency, \nwe got one of the best reported results in the literature. The measure we report is the one reported \nby SPECjbb00: the maximum time it takes to com\u00adplete a transaction. It is the same measure that was reported \nby Domani et al. [20]. But whereas Domani et al. ran an IBM JVM with a JIT compiler, our collector gave \nsimi\u00adlar maximal transaction times (20-120ms depending on the number of threads) with the assembler loop \n(and no com\u00adpilation). Incorporating our collector with a JIT compiler would no doubt improve the transaction \ntime substantially. We remark that our result and the result of Domani et al. are incomparable with those \nof Bacon et al. [4] since Ba\u00adcon et al. report the exact pause times, measured by the Jalapeno JVM. 1.8 \nRelated work The traditional method of reference counting, was .rst de\u00adveloped for Lisp by Collins [11]. \nIt was later used in Small talk-80 [23], the AWK [1] and Perl [40] programs. Im\u00adprovements to the naive \nalgorithm were suggested in sev\u00aderal subsequent papers. Weizman [41] studied ameliorating the delay introduced \nby recursive deletion. Deutsch and Bobrow [15] eliminated the need for a write barrier on lo\u00adcal references \n(in stack and registers). This method was later adapted for Modula-2+ [13]. Further study on reduc\u00ading \nwork for local variables can be found in [7] and [32]. Several works [35, 43, 39, 9] use a single bit \nfor each ref\u00aderence counter with a mechanism to handle over.ows. The idea being that most objects are \nsingly-referenced, except for the duration of short transitions. DeTreville [13] describes a concurrent \nmultiprocessor ref\u00aderence counting collector for Modula-2+. This algorithm adapts Deutsch and Bobrow \ns ideas of deferred reference counting and transaction log for a multiprocessor system. However, the \nupdate operation is done inside a critical sec\u00adtion that uses a single central lock. This implies that \nonly a single update can occur simultaneously in the system, plac\u00ading a hard bound on its scalability. \nPlakal and Fischer in [33] propose a collection method based on reference counting for architectures \nthat support explicit multi-threading on the processor level. Their method re\u00adquires co-routine type \nof cooperation between the program thread and corresponding shadow collector threads and therefore is \nprobably not suitable for stock SMPs, as SMP architectures do not support this kind of interaction in \na natural and e.cient manner. Algorithms that perform garbage collection using a snapshot of the heap \nappear in [22, 44]. In terms of synchronization requirements and characteristics our work is similar \nto that of Doligez-Leroy-Gonthier [19, 18] in the sense that we use only .ne synchronization, we never \nrequire a full halt of the system (the mutators are required to cooperate a few times per collection \ncycle). In our tracing algorithm we have used an object sweeping method similar to that presented in \n[19, 18]. Our garbage collection algorithm builds on a pragmatic con\u00adcept of an atomic snapshot. The \nsame concept is also a basic block in other garbage collectors as well as in replication and checkpoint/restart \nalgorithms. The pragmatism of this ap\u00adproach is dictated by the design of contemporary multipro\u00adcessor \nsystems. We would like to also point that there exists a wide and profound theoretic reseach dealing \nwith concur\u00adrent wait-free shared memory algorithms that builds on the snapshot notion. The task in both \nresearch .elds is com\u00admon: obtain a consistent picture of a system taken while it is conceptually frozen. \nHowever, whereas much of the theoretical work assumes that threads are inherently faulty (hence these \nalgorithms strive to be wait-free), the practi\u00adcal approach assumes that threads are not only inherently \nreliable, but are also controllable by privilidged roles (e.g., the operating system scheduler). Another \ndi.erence is in the formulation of the problem the theoretic community is more interested in the strain \nof the problem in which there are n threads, each with a single register to be shared among each other \n(a scatter-gather problem). The practical research, on the other hand, is concentrated on retrieving \nthe value of all shared memory locations at a given time instance, regard\u00adless of the number of running \nthreads. For more reading on related theoretic research, the reader is refered to the work in [2] which \nprovides an exciting attempt to bridge these two worlds as well as a comprehensive survery of previous \natomic snapshot research. 1.9 The work of Bacon et al. Independently of this work, Bacon et al. [4, 5] \nhave built an on-the-.y reference counting algorithm appropriate for a multiprocessor. Their work presents \na big step towards making reference counting practical for servers. Since our work and theirs are closely \nrelated (both introduce an on\u00adthe-.y reference counting collector with extremely low pause times), we \nwould like to elaborate on the relations between these two collectors. Reducing synchronization. A naive \napproach to multi\u00adprocessor reference counting requires at least three compare\u00adand-swaps in the write \nbarrier. One for the update of the pointer and two for the updates of the two reference count. DeTreville \n[13] has used a lock on each update to make sure that no two pointer updates are executed concurrently. \nBa\u00adcon et al. [4] made a signi.cant step into exploiting multi\u00adprocessor concurrency by reducing the \nnumber of synchro\u00adnizing operations to a single compare-and-swap. While sig\u00adni.cantly reducing the cost \nof synchronization, their write barrier still contains a compare-and-swap for each pointer update. In \nour work, using our novel sliding view idea, we have managed to completely eliminate synchronization \nin the write barrier. This major improvement is one of the more important contributions of this work. \nImproving throughput: It is not possible to compare the throughput of the two collectors since they have \nbeen run on di.erent platforms and compared against di.erent base JVM s. Our collector demonstrate a \nthroughput which is comparable to the original SUN JVM. Bacon et al. [4] report a reduction of around \n10% in the throughput of their JVM compared to the original Jalapeno JVM. Improving latency. With respect \nto pause times, the mea\u00adsured results provided by Bacon et al. are incomparable with ours. Bacon et al. \nused the Jalapeno JVM to mea\u00adsure the exact pause times. Unfortunately, we did not have the means to \nget such a measure, which is provided by the Jalapeno JVM. Instead, like in [20], we use the report out\u00adput \nby the SPECjbb00 benchmark. It reports the maximum time it takes to complete a transaction. Our results \nshow excellent latency with respect to previous reports of this nature. We believe that measuring the \nmaximum time for a transaction is more meaningful than the shortest garbage collection pause, because \nit takes into account the slowdown imposed by the collector also. E.g., a collector with very frequent \nbut short pause times might be less good than a collector with slightly longer but much less frequent \npause times. This is an important issue in concurrent and incre\u00admental collection. Sequential memory \nconsistency vs. .oating garbage. Our algorithm requires sequential memory consistency. How\u00adever, as explained \nin section 5 below, this limitation can be overcome at a negligible cost. The algorithm in [4] can run \non any platform. But the cost of this robustness is .oating garbage. In their algorithm, an unreachable \nobject cannot be collected unless it has been unreachable for two consec\u00adutive collections. Thus, although \nmemory coherence is not an issue, the drag time of objects [36] increases signi.cantly, resulting in \na substantial amount of .oating garbage com\u00adpared to our collector. Collecting cycles. Finally, the two \npapers take di.erent avenues for collecting cycles. Bacon and Rajan [5] provide a novel approach for \non-the-.y cycle detection. Their algo\u00adrithm can be run with any algorithm and in particular with ours, \nand it demonstrates that the entire collection can be run with a pure reference counting algorithm. In \ncontrast to their approach, we have chosen to develop an on-the\u00ad.y mark-and-sweep collector that exploits \nthe sliding view mechanism and uses the same data structure as the refer\u00adence counting algorithm. This \nmark-and-sweep collector is run seldom in order to collect cycles and restore stuck ref\u00aderence counts \n(see below). Our on-the-.y mark-and-sweep collector is a stand-alone collector that can be run also with\u00adout \nthe reference counting algorithm, and is interesting on its own. It is di.cult to compare the e.ciency \nof the two approaches, and since the algorithm is run seldom, such a comparison is not so interesting. \nHowever, we note that the use of a tracing collector allows saving space. We use two bits for the reference \ncounts. Reference counts that exceed the value of 2, get stuck and are restored by the tracing col\u00adlector. \nIn Bacon et al., it is necessary to keep the counters correct, since there is no mechanism to restore \ncorrupted counts. Thus, more space is used for the counter, and a cache is used to place objects whose \nreference counts exceed the maximum allowed value. 1.10 Organization In Section 2 we present de.nitions \nand terminology to be used in the rest of the paper. In Section 3 we present our Snapshot Algorithm. \nIn Section 4 we present the sliding view algorithm. In Section 5 we discuss adaptation of the algorithm \nto platforms that do not provide sequentially con\u00adsistent memory and in Section 6 we present performance \nresults. We conclude in 7. 2. SYSTEM MODEL, DEFINITIONS For an introduction on garbage collection and \nmemory man\u00adagement the reader is referred to [27]. We assume the reader is familiar with the concepts \nsuch as heap, object, roots, reachability, etc. Note that in a multithreaded environment each thread \nhas its own roots on top of the global roots. Fields in objects in the heap that hold references are \ncalled heap reference slots but most of the time we will just call them slots. We will count references \nto objects by summing over all slots in the heap. We will not consider the threads local stack and registers \nfor the count. We assume all slots are initialized with a null pointer. We denote the reference count \nassociated with an object o by o.rc. Coordination of threads. We assume that the garbage collector thread \nmay suspend and subsequently resume user threads. When a thread is suspended, the collector may in\u00adspect \nand change its local state with the e.ects taking place after the thread is resumed. In our algorithm, \nwe assume threads are not stopped during execution of protected code. In particular, in our algorithm, \nthe only pieces of code which are protected are procedures Update and New, which are in charge of updating \nheap-slots and allocating new objects, respectively. 3. THE SNAPSHOT ALGORITHM For clarity of presentation, \nwe start with an intermediate al\u00adgorithm called the snapshot algorithm. Here, we present the ideas required \nfor an e.cient write barrier with no synchro\u00adnization. In this intermediate algorithm, the threads are \nstopped for part of the collection. The length of this pause is not too long (the bottle neck is clearing \na bitmap with dirty .ags for all objects in the heap), but it is long enough to hinder scalability on \na multiprocessor. In Section 4, we extend this intermediate algorithm making it on-the-.y. The idea, \nas presented in Section 1.3, is based on computing di.erences between heap snapshots. The algorithm operates \nin cycles. A cycle begins with a collection and ends with another. Let us describe the collector actions \nduring cycle k (throughout the paper we let the subscript k denote the number of a garbage collection \ncycle). Our .rst goal is to record all pointer slots in the heap that have changed since the previous \ncollection cycle k - 1. We let the mutators do the recording with a write-barrier. In order to avoid \nrecording slots again and again, we keep a dirty .ag for each such slot. When a mutator updates a pointer, \nit checks the dirty bit. If it is clear, the mutator sets the dirty bit and records the slot s information \ninto a local bu.er. The recorded information is the address of the slot and its value before the current \nmodi.cation. Recording is done in a local bu.er with no synchronization. When a collection begins, the \ncollector starts by stopping all threads and marking as local all objects referenced directly by the \nthreads stack at the time of the pause. Next, it reads all the threads local bu.ers (in which modi.ed \nslots are recorded), it clears all the dirty bits and it lets the mutators resume. After the mutators \nresume, the collector updates all the heap reference counts to re.ect their values at the time of the \npause. (Recall that the heap reference count is the number of references to the object from other objects \nin the heap.) The algorithm for this update is presented and justi.ed in the remainder of this section. \nHowever, assuming that the heap reference counts are properly updated, the col\u00adlector may reclaim all \nobjects whose reference counts drop to zero by this update and are not marked local. As usual, the reference \ncounts of objects referenced by reclaimed objects are decremented and the reclamation proceeds recursively. \nProcedure New(size: Integer): Object begin 1. Obtain an object o from the allocator, according to the \nspeci.ed size. // add o to the thread local ZCT. 2. Newi := Newi .{o} 3. return o end  Figure 1: Mutator \nCode: for Allocation Procedure Update(s: Slot, new: Object) begin 1. local old := read(s) // was s written \nto since the last cycle ? 2. if \u00acDirty(s) then // ... no; keep a record of the old value. 3. Bufferi[CurrP \nosi] := .s, old. 4. CurrP osi := CurrP osi +1 5. Dirty(s) := true 6. write(s, new) end  Figure 2: \nMutator Code: Update Operation A standard zero-count table (ZCT) [15] keeps track of all objects whose \nreference count drops to zero at any time. These objects are candidates for reclamation. We remark that \nwhenever an object is created it has a zero heap ref\u00aderence count. Thus, all created objects are put \nin (a local) ZCT upon creation. The code for the create routine appears in Figure 1. It remains to discuss \nupdating the reference counts accord\u00ading to all modi.ed slots between collection k - 1 and k. As explained \nin Section 1.3, for each such slot s, we need to know the object O1 that s pointed to at the pause of \ncollec\u00adtion k - 1 and the object O2 that s points to at the pause of collection k. Once these values \nare known, the collector decrements the reference count of O1 and increments the reference count of O2. \nWhen this operation is done for all modi.ed slots, the reference counts are updated and match the state \nof the heap at the kth collection pause. We go on now and describe how to obtain the addresses of objects \nO1 and O2. We start with obtaining O1. If no race occured when the slot s was .rst modi.ed during this \ncycle, then the write barrier recorded the address of O1 in the local bu.er. It is the value that s held \nbefore that (.rst) modi.cation. But suppose a race did occur between two (or more) threads trying to \nmodify s. The code of the write barrier appears in Figure 2. If one of the updating threads sets the \ndirty .ag of s before any other thread reads the dirty .ag, then only one thread records this address \nand the recording will properly re.ect the value of s at the k - 1 pause. Otherwise, more than one thread \n.nds the dirty bit clear. Looking at the code, each thread starts by recording the old value of the slot, \nand only then it checks the dirty bit. On the other hand, the actual update of s occurs after the dirty \nbit is set. Thus, if a thread detects a clear dirty bit, then it is guaranteed, since sequential consistency \nis assumed, that the value it records is the value of s before Procedure Collection-Cycle begin 1. Read-Current-State \n 2. Update-Reference-Counters 3. Read-Bu.ers 4. Fix-Undetermined-Slots 5. Reclaim-Garbage end  Figure \n3: Collector Code any of the threads has modi.ed it. So while several threads may record the slot s in \ntheir bu.ers, all of them must record the same (correct) information. To summarize, in case a race occurs, \nit is possible that several threads record the slot s in their local bu.ers. However, all of them record \nthe same correct value of s at the k - 1st pause. When collecting the local bu.ers from all threads, \ncare is taken to avoid multiple records of a slot. (For implementation details, see the technical report \n[29]). We conclude that the address of object O1 can be properly obtained. We now explain how the collector \nobtains the address of O2, the object that s references at the pause of collection k. Note that at the \ntime the collector tries to obtain this value the threads are already running after the k pause. The \ncollector starts by reading the current value of s. It then reads s s dirty .ag. If the .ag is clear \nthen s has not been modi.ed since the pause of collection k and we are done. If the dirty bit of s is \nset, then it has been modi.ed. But if it has been modi.ed, then the value of s at pause k is currently \nrecorded in one of the threads local bu.ers. This value can be obtained by searching the local bu.ers \nof all threads. Note that the threads need not be stopped for peeking at their bu.ers. We know that this \nslot has a record somewhere and it will not be changed until the next (k + 1) collection. The collector \noperation is given in Figure 3. In Read\u00adCurrent-State the collector stops the threads, takes their bu.ers, \nmark objects directly referenced from the roots as local, takes all local ZCT s (including records of \nnewly cre\u00adated objects), and clears all the dirty marks. The threads are then resumed. While the threads \nrun, the collector up\u00addates the reference counts as much as it can (excluding slots that were modi.ed \nsince the pause). It then reads the cur\u00adrent bu.ers of the threads (without stopping them) to get information \non all slots modi.ed since the last pause, and .nish updating the reference counts. Finally, it (recursively) \nreclaims all objects with zero reference count that are not marked local. We remark that for the correctness \nof the algorithm, it is re\u00adquired that the suspension of all mutators does not interrupt any of the mutators \nin the middle of a pointer modi.cation. We do not elaborate on implementation issues. More discus\u00adsion \nof this intermediate algorithm together with full code is given in the technical report [29]. The main \ngoal of this section is to explain the write barrier that avoids synchro\u00adnization. We now turn to the \non-the-.y algorithm. 4. THE SLIDING VIEW ALGORITHM In the snapshot algorithm we have managed to execute \na major part of the collection while the mutators run concur\u00adrently with the collector. The main disadvantage \nof this algorithm is the halting of the mutators in the beginning of the collection. During this halt \nall threads are stopped while the collector clears the dirty .ags and receives the mutators bu.ers and \nlocal ZCTs. This halt hinders both e.ciency, since only one processor executes the work and the rest \nare idle, and scalability, since more threads will cause more delays. While e.ciency can be enhanced \nby paralleliz\u00ading the .ags clearing phase, scalability calls for eliminating complete halts from the \nalgorithm. This is indeed the case with our second algorithm, which avoids grinding halts com\u00adpletely. \nA handshake [19, 18] is a synchronization mechanism in which each thread stops at a time to perform some \ntransac\u00adtion with the collector. Our algorithm uses four handshakes. Thus, mutators are only suspended \none at a time, and only for a short interval, its duration depends on the size of the mutator s local \nstate. In the snapshot algorithm we had a .xed point of time, namely, when all mutators were stopped, \nfor which we com\u00adputed the reference counts of all objects. Thus, it was easy to claim that if an object \nhas a zero heap reference count at that time, and it is not local at that time, then it can be re\u00adclaimed. \nBy dispensing with the complete halting of threads we no longer have this .xed point of time. Rather, \nwe have a fuzzier picture of the system, formalized by the notion of a sliding view which is essentially \na non-atomic picture of the heap. We show how sliding views can be used instead of atomic snapshots in \norder to devise a collection algorithm. This approach is similar to the way snapshots are taken in a \ndistributed setting. Each mutator at a time will provide its view of the heap, and special care will \nbe taken by the system to make sure that while the information is gathered, modi.cations of the heap \ndo not foil the collection. 4.1 Scans and sliding views Pictorially, a scan s and the corresponding sliding \nview Vs can be thought of as the process of traversing the heap along with the advance of time. Each \npointer slot s in the heap is probed at time s(s); Vs(s) is set to the value of the probed pointer. For \nan object o and a sliding view Vs we de.ne the Asynchronous Reference Count of o with respect to Vs to \nbe the number of slots in Vs referring to o: ARC(Vs; o) def = |Vs -1(o)| Sliding views can be obtained \nincrementally, which will get us the bene.t of not having to stop all mutators simulta\u00adneously in order \nto compute the view. But in order to use this information to safely collect garbage we need to be care\u00adful. \nTrying to use the snapshot algorithm when we are only guaranteed that logging and determining re.ects \nsome slid\u00ading view is bound to fail. For example, the only reference to object o may move from slot s1 \nto slot s2, but a sliding view might miss the value of o in both s1 (reading it after modi.cation) and \ns2 (reading it before modi.cation). We avoid these problems via a snooping mechanism. While the view \nis being read from the heap, we let the write-barrier mark any object that is assigned a new reference \nin the heap. We mark these objects as local, thus, preventing them from being collected in this collection \ncycle. (Recall that objects directly referenced by the roots are marked local to pre\u00advent collecting \nthem because of a zero heap reference count.) We remark that there is nothing preventing the collection \nof these snooped objects in the next cycle. Assuming this snooping mechanism throughout the scan of the \nheap, we observe the following. Observation: If object o has ARC(Vs; o) = 0, i.e., it is not referenced \nby any pointer slot in the heap as re.ected by the sliding view, and if object o is not referenced directly \nby the roots of the threads after the scan was completed, and if ob\u00adject o has not been marked local \nby the snooping mechanism while the heap (and the roots) were being scanned, then at the time the heap \nscan is completed, object o is unreachable and may be reclaimed. Proof idea: If the object is referenced \nby a heap slot in the end of the scan, then this slot has either been pointing to this object when the \nscan of the heap read it, or it has been written to that slot later. Both cases do not fall in the criteria \nof unreachable objects in the observation. Finally, if no reference is written into the heap while the \nroots are scanned, and there is no reference from the roots to this object, then it is unreachable. Here \nwe rely on the fact that a mutator is stopped while reading its stack, so no pointer may move while the \nthread stack is being read; and furthermore, in Java, a reference cannot be moved from the stack of one \nthread to another without being written to the heap. The full argument is given in the technical report. \nKeeping this observation in mind, we are ready to present the sliding view algorithm. We break the description \ninto two. We .rst describe (in Section 4.2 below) how a sliding view of the heap may be used to reclaim \nunreachable ob\u00adjects. We call it a generic algorithm since it may use any mechanism for obtaining the \nsliding view. Then, we describe how the reference counts of all objects can be updated ac\u00adcording to \na sliding view that is not actually taken. This is an extension of the ideas in the snapshot algorithm, \nstill preserving the light write barrier. 4.2 Using sliding views to reclaim objects Based on the above \nobservation we present a generic garbage collection algorithm: Each thread Ti has a .ag, denoted Snoopi \nwhich signi.es whether the collector is in the midst of constructing a sliding view. Thread Ti executes \na write barrier in order to perform a heap slot update. The generic algorithm requires that after the \nstore proper to the slot is performed, i.e., the reference to o is written into slot s, the thread would \nprobe its Snoopi .ag and, if the .ag is set, would mark o as local. We call this probing of the Snoopi \n.ag and the subsequent marking snooping. Any speci.c implementation of the generic algo\u00adrithm may require \nadditional steps to be taken as part of the write barrier. As usual, threads may not be suspended in \nthe midst of an update. A collection cycle contains the following stages: 1. the collector raises the \nSnoopi .ag of each thread. This indicates to the threads that they should start snoop\u00ading. 2. the collector \ncomputes, using an implementation-speci.c mechanism, a scan s and a corresponding sliding view, Vs, concurrently \nwith threads computations. The ac\u00adtual manner using which the collector computes Vs is immaterial, it \ns just important that it arrives at a sliding view. 3. each thread is then suspended (one at a time), \nits Snoopi .ag is turned o. and every object directly reachable from it is marked local. The thread is \nthen resumed. 4. now, for each object o we let o.rc := ARC(Vs; o). 5. at that point, we can deduce \nthat any object o that has o.rc = 0 and that was not marked local is garbage.  Consider an object o \nwith ARC(Vs; o) = 0 and which is not marked local. Since for each thread the Snoopi .ag is set for the \nentire duration of the sliding view computation we conclude that o s true reference count at the end \nof the heap scan is zero as well. It may be, however, that o is directly reachable from some thread at \nthat time. Nevertheless, since no local reference to o was observed by any thread when its state was \nscanned (in stage (3) of the collector) and it was not snooped prior to it, any thread which possessed \nsuch a local reference must have discarded it prior to responding to the handshake of stage (3) without \never raising the heap reference count of o above zero. We conclude that by the time the handshake of \nstage (3) ends, o is garbage. The snooping mechanism may lead to some .oating garbage as we conservatively \ndo not collect objects which are marked local, although such objects may become garbage before the cycle \nends. However, such objects are bound to be collected in the next cycle. We have termed this algorithm \ngeneric since the mech\u00adanism for computing the sliding view is unspeci.ed. We next present an algorithm \nfor updating the reference counts for an implicitly de.ned sliding view of the heap. When the algorithm \nis done, it holds for each object that o.rc = ARC(V ; o), where V is the sliding view that was constructed \nimplicitly. Since we are not interested in the sliding view it\u00adself but rather on its manifestation through \nthe rc .elds, this implicit computation su.ces for collection purposes. 4.3 Obtaining the sliding view \nWe use four handshakes during the collection cycle. The sliding view associated with a cycle spans from \nthe beginning Procedure Update(s: Slot, new: Object) begin 1. Object old := read(s) 2. if \u00acDirty(s) \nthen 3. Bufferi[CurrP osi] := .s, old. 4. CurrP osi := CurrP osi +1 5. Dirty(s) := true 6. write( \ns, new) 7. if Snoopi then 8. Localsi := Localsi .{new}end  Figure 4: Sliding View Algorithm: Update \nOpera\u00adtion of the .rst handshake up to the end of the third handshake. The sampling timing of each individual \nslot in the scan is determined by mutators logging regarding the slot. The snooping .ags are raised prior \nto the .rst handshake and are turned o. at the fourth handshake. Thus, they are set for the entire duration \nof the scan, adhering to the snooping requirement of the generic sliding view algorithm. Any slot which \nis changed between collection cycles is logged along with its value in the most recent sliding view, \nhence there is no loss of information regarding old values. It turns out from our analysis that inconsistent \nlogging of slots is only possible between responding to the .rst and third handshakes of a cycle. Just \nafter the fourth handshake, the collector employs a consolidation mechanism to consolidate any inconsistently \nlogged slot into a .xed value. No thread would log a con.icting value after responding to the fourth \nhandshake, hence no inconsistencies will be visible in the history for the next cycle. In addition, the \ncollector always consolidates any slot which has been logged between the .rst and third handshakes, so \nthere is no risk that the collector would use one value of a slot (before it is consolidated), and that \nvalue will be modi.ed later by the consolidation mechanism. Hence the collector and mutators always agree \non the values of slots in the sliding view. We refer the reader to the full version in [29] where we \nsys\u00adtematically de.ne the sliding view associated with a cycle and prove its properties. We now present \nthe algorithm and the code. 4.4 Mutator s code Mutators use the write barrier of the snapshot algorithm \nwith the additional snooping and marking added after the store proper (see procedure Update in .gure \n4). Object creation is unchanged from the snapshot algorithm. 4.5 Collector s code We now go over the \nmain steps of the collection cycle. The code for each step is provided as well. 1. Signaling snooping. \nThe collection starts with the collector raising the Snoopi .ag of each thread, signaling to the mutators \nthat it is about to start computing a sliding view. Procedure Initiate-Collection-Cycle begin 1. for \neach thread Ti do 2. Snoopi := true 3. for each thread Ti do 4. suspend thread Ti // copy (without \nduplicates). 5. Histk := Histk.  Bufferi[1 . . . CurrP osi - 1] // clear bu.er. 6. CurrP osi := 1 \n7. resume Ti end  Figure 5: Sliding View Algorithm: Procedure Initiate-Collection-Cycle Procedure Clear-Dirty-Marks \nbegin 1. for each .s, o.. Histk do 2. Dirty(s) := false end  Figure 6: Sliding View Algorithm: Procedure \nClear-Dirty-Marks 2. Reading bu.ers (.rst handshake). During the hand\u00adshake threads bu.ers are retrieved \nand then are cleared. (These are the same thread bu.ers as in the .rst (snap\u00adshot) algorithm.) The slots \nwhich are listed in the bu.ers are exactly those slots that have been changed since the last cycle. However, \nin the sliding view scenario this no\u00adtion requires more care. The meaning of changing in this asynchronous \nsetting is de.ned as follows. A slot is changed during cycle k if some thread changed it after responding \nto the .rst handshake of cycle k and before responding to the .rst handshake of cycle k + 1. Steps (1) \nand (2) are carried out by procedure Initiate\u00adCollection-Cycle (.gure 5). 3. Clearing. The dirty .ags \nof the slots listed in the bu.ers are cleared. Note that the clearing occurs while the mutators are running. \nThis step is carried out by procedure Clear-Dirty-Marks (.gure 6). This step may clear dirty marks that \nhave been concurrently set by the running mutators. Since we want to keep these dirty bits set, we will \nuse the logging in the bu.ers (which contain all objects that have been marked dirty since the .rst handshake) \nto set these dirty bits on again. 4. Reinforcing dirty marks (second handshake). dur\u00ading the handshake \nthe collector reads the contents of the threads bu.ers (which contain slots that were logged since the \n.rst handshake). The collector then reinforces, i.e., sets, the .ags of the slots listed in the bu.ers. \n 5. Assuring reinforcement is visible to all mutators (third handshake). The third handshake is carried \nout. Each thread is suspended and resumed with no further ac\u00adtion. By the time all threads resume, we \nknow that they view correctly all dirty bits. Namely, a slot is dirty i. it was  Procedure Reinforce-Clearing-Con.ict-Set \nbegin Procedure Consolidate 1. ClearingConflictSetk := . begin 2. for each thread Ti do 1. local T emp \n:= . 3. suspend thread Ti 2. ColLocalsk := . 4. ClearingConflictSetk := 3. for each thread Ti do ClearingConflictSetk. \n4. suspend thread Ti Bufferi[1 . . . CurrP osi - 1] 5. Snoopi := false 5. resume thread Ti // copy and \nclear snooped objects set 6. for each s . ClearingConflictSetk do 6. ColLocalsk := ColLocalsk . Localsi \n7. Dirty(s) := true 7. ColLocalsi := . 8. for each thread Ti do // copy thread local state and ZCT. 9. \nsuspend thread Ti 8. ColLocalsk := ColLocalsk . Statei 10. nop 9. ZCTk := ZCTk . Newi 11. resume Ti 10. \nNewi := .end // copy local bu.er for consolidation. 11. T emp := T emp. Bufferi[1 . . . CurrP osi - 1] \n// clear local bu.er. Figure 7: Sliding View Algorithm: Procedure Reinforce-Clearing-Con.ict-Set 12. \nCurrP osi := 1 13. resume thread Ti // consolidate T emp into Histk+1. 14. Histk+1 := .  modi.ed by \na thread that responded to the .rst handshake. 15. local Handled := . 16. for each .s, v.. T emp  Steps \n(4) and (5) are executed by procedure Reinforce\u00ad 17. if s/. Handled then 18. Handled := Handled .{s} \n Clearing-Con.ict-Set (.gure 7). 19. Histk+1 := Histk+1 . {.s, v.} end 6. Consolidation (fourth handshake). \nDuring the fourth handshake thread local states are scanned and objects di\u00adrectly reachable from the \nroots are marked local. Threads bu.ers are retrieved once more and are consolidated. Consolidating threads \nbu.ers amounts to the following. For any slot that appears in the threads bu.ers accumulated be\u00adtween \nthe .rst and fourth handshakes, pick any occurrence of the slot and copy it to a digested consistent \nhistory. All other occurrences of the slot are discarded. The digested history replaces the accumulated \nthreads bu.ers. i.e., the history for the next cycle is comprised of the digested history of threads \nlogging between the .rst and fourth hand\u00adshakes of the current cycle, uni.ed with threads bu.ers rep\u00adresenting \nupdates that will occur after the fourth handshake of the current cycle but before the .rst handshake \nof the next cycle. Consolidation is carried out by procedure Consoli\u00addate of .gure 8. 7. Updating. The \ncollector proceeds to adjust rc .elds due to di.erences between the sliding views of the previous and \ncurrent cycle. This is done exactly as in the snapshot algorithm (see .gure 9). The collector fails to \ndetermine the current value of all slots that were modi.ed (i.e., are dirty). These slots will be treated \nlater and are now marked as undetermined. 8. Gathering information on undetermined slots.  The collector \nasynchronously reads mutators bu.ers (using the procedure Read-Bu.ers of .gure 10). Then, in pro\u00adcedure \nMerge-Fix-Sets (.gure 11) it uni.es the set of read pairs with the digested history computed in the consolida\u00adtion \nstep. The set of undetermined slots is a subset of the slots appearing in the uni.ed set so the collector \nmay now proceed to look up the values of these undetermined slots. Figure 8: Sliding View Algorithm: \nProcedure Con\u00adsolidate Procedure Update-Reference-Counters begin 1. Undeterminedk := . 2. for each .s, \nv. pair in Histk do 3. curr := read(s) 4. if \u00acDirty(s) then 5. curr.rc := curr.rc +1 6. else 7. \nUndeterminedk := Undeterminedk .{s} 8. v.rc := v.rc - 1 9. if v.rc =0 . v/. ColLocalsk then 10. ZCTk \n:= ZCTk .{v}  Figure 9: Collector Code: ProcedureUpdate\u00adReference-Counters Procedure Read-Bu.ers begin \n1. P eekk := . 2. for each thread Ti do 3. local P robedP os := CurrP osi // copy bu.er (without duplicates.) \n 4. P eekk := P eekk. Bufferi[1 . . . P robedP os - 1]  end Figure 10: Collector Code: Procedure Read-Bu.ers \nProcedure Merge-Fix-Sets begin 1. P eekk := P eekk . Histk+1 end Figure 11: Sliding View Algorithm: Procedure \nMerge-Fix-Sets Procedure Fix-Undetermined-Slots begin 1. for each pair .s, v. pair in P eekk 2. if s \n. Undeterminedk do 3. v.rc := v.rc +1 end  Figure 12: Collector Code: Procedure Fix\u00adUndetermined-Slots \n9. Incrementing rc .elds of objects referenced by undetermined slots. In procedure Fix-Undetermined-Slots \n(.gure 12) any undetermined slot is looked up in the uni.ed set and the rc .eld of the associated object \nis incremented. 10. Reclamation. Reclamation generally proceeds as in the previous algorithm, i.e., \nrecursively freeing any object with zero rc .eld which is not marked local. We should be careful, however, \nnot to reclaim objects whose slots appear in the digested history. i.e., objects which were modi.ed since \nthe cycle commenced but became garbage before it ended. The reclamation of such objects is deferred to \nthe next cycle. Reclamation is carried out using the procedures Reclaim-Garbage (.gure 13 and Collect \n(.gure 14).   5. MEMORY COHERENCE As mentioned in the introduction, two simple modi.cations can make \nthe algorithm suitable for platforms that do not guarantee sequential memory consistency. We list these \nmodi.cations here and discuss their cost. We .rst note that most platforms provide a sequentially con\u00adsistent \nview of the memory for reads and writes made to the same word. Furthermore, this guarantee is provided \nmore generally within the coherence granule. Namely, a guaran\u00adtee on a sequential consistency is made \nfor an entity that is Procedure Reclaim-Garbage begin 1. ZCTk+1 := . 2. for each object o . ZCTk do \n 3. if o.rc > 0 then 4. ZCTk := ZCTk -{o} 5. else if o.rc =0 . o . ColLocalsk then 6. ZCTk := ZCTk \n-{o} 7. ZCTk+1 := ZCTk+1 .{o}  8. for each object o . ZCTk do 9. Collect(o) end  Figure 13: Collector \nCode: Procedure Reclaim-Garbage Procedure Collect(o: Object) begin 1. local DeferCollection := false \n 2. foreach slot s in o do 3. if Dirty(s) then 4. DeferCollection := true 5. else 6. val := read(s) \n 7. val.rc := val.rc - 1 8. write(s, null) 9. if val.rc = 0 then 10. if val /. ColLocalsk then 11. \nCollect(val) 12. else 13. ZCTk+1 := ZCTk+1 .{val} 14. if \u00acDeferCollection then 15. return o to the \ngeneral purpose allocator. 16. else 17. ZCTk+1 := ZCTk+1 .{o}  end Figure 14: Sliding View Algorithm: \nProcedure Col\u00adlect larger than one word of memory. It is made on the coher\u00adence granule of the platform, \nwhich is usually of the size of the cache line. We keep this guarantee in mind and turn to the algorithm. \nThere are three dependencies on instruction ordering in the algorithm. Dependency 1: in the write barrier, \nthe reads and writes of the dirty .ag and the pointer slot must be executed in the order stated in the \nalgorithm. To solve this dependency, we note that in most cases the dirty bit and the pointer slot re\u00adside \non the same coherence granule. In our implementation, we keep the dirty bit in the header of the object. \nThus, we only need to perform a memory synchronization barrier for objects whose dirty bit does not reside \non the same coher\u00adence granule with the modi.ed slot. Furthermore, the write barrier begins with a check \nwhether the object is not dirty. The synchronization barrier is required only if the check is validated, \ni.e., the object is not dirty. Cost: As reported in a study of the SPECjvm98 bench\u00admarks [16] and is \nimplied by the results of Chilimbi and Larus [10], most objects are small. For example, the me\u00addian of \nthe object size runs between 12 to 24 bytes [16]. The size of the cache line ranges between 32 to 128 \nbytes de\u00adpending on the platform. Furthermore, our measures show that objects tested in the write barrier \nrarely turn out not dirty. For the javac benchmark this happens less than once in a hundred, and for \nthe SPECjbb benchmark and all the other SPECjvm98 benchmarks this happens less than once in a thousand. \nSo the vast majority of the pointer updates require no cost for handling memory coherence. To sum\u00admarize, \nthe number of actual pointer modi.cations whose write barriers require a synchronization overhead (i.e., \nlarge objects that are not dirty) is tiny and we expect to see neg\u00adligible impact on the running time. \nDependency 2: the modi.cation of the snoop .ag. We assume that the modi.cation of the snoop .ag is visible \nto all threads before we actually start the .rst handshake. Heap Size (MB) 600 1200 score in JBB s throughput \nunits Original 1,131.3 1,101.0 RC 1,101.7 1,108.3 Change in JBB score -2.6% 0.7% Maximal response time \n(milliseconds) Original 7763 16,100 RC 115 110 Times RC is more responsive \u00d767.5 \u00d7146.4 Figure 15: Throughput \nand latency of the reference counting collector and the original collector in stan\u00addard SPECjbb runs, \nwith 600 MB and 1200 MB heaps.  To make sure this is the case, we can add a preliminary handshake in \nthe beginning of the cycle in which the snoop .ags are raised (currently this is done without stopping \nthe threads). Cost: this is done once per collection cycle and is thus neg\u00adligible compared to the overall \nrunning time of the collection cycle (and to the running time of the program). We remark that we have \nnot implemented these two modi.\u00adcations, yet, we have not witnessed any problem caused by reordering \ninstructions by the Intel platform. 6. AN IMPLEMENTATION FOR JAVA We have implemented our algorithm \non SUN s Reference Re\u00adlease 1.2.2 of the Java Virtual Machine. The implementation was done for the interpreter \n(no JIT). In both the original and modi.ed JVM we used the assembler loop (it was mod\u00adi.ed to take into \naccount the write barrier and a modi.ed object layout). We ran the measurements on a 4-way IBM Net.nity \n8500R server with a 550MHz Intel Pentium III Xeon processor and 2GB of physical memory. We measured our \nalgorithm s performance characteristics compared to the original algorithm used in the JVM. We also measured \nthe run of our collector on a client machine: a single Pentium III at 500Mhz with 256MB of physical memory. \nWe used two standard testing suites: SPECjbb2000 and JPECjvm98. These benchmarks are described in detail \nin SPEC s Web site[38]. We target our reference counting al\u00adgorithm for use with big heaps. Thus, we \nused a 1,200MB Java heap for the JBB server benchmark. We also present the (slightly worse) results for \na Java heap of size 600MB. For the jvm98 client benchmarks we used a 64MB heap. 6.1 Server performance \nA standard execution of SPECjbb requires a multi-phased run with increasing number of threads. Each phase \nlasts for two minutes with a ramp-up period of half a minute before each phase. Prior to the beginning \nof each phase a syn\u00adchronous GC cycle may or may not occur, at the discretion of the tester. We decided \nnot to perform this synchronous garbage collection as we believe it defeats capturing real world scenarios \nin which the server is not given a chance for this o.ine behavior so often. The results presented here \nare averaged over three standard runs. Threads Time to completion (seconds) % Improvement Original RC \n1 93 88.6 4.9% 2 71.9 68.5 5.0% 3 56.3 52.5 7.2% 4 57.2 54.2 5.6% 8 58.2 52.3 11.4% 12 58 57.9 0.2% 16 \n59 59.1 -0.1% Figure 16: Time to completion, in seconds, of the MTRT benchmark, with varying number of \nthreads.  Threads 1 2 4 6 8 10 15 20 Original 24 39 70 100 139 160 236 312 RC 27 44 77 108 170 171 \n251 329 Figure 17: MB allocated for heap objects and their headers (does not include space allocated \nfor auxil\u00adiary data structures) at the end of a SPECjbb run with a .xed number of threads and a heap \nof 600 MB. Figure 15 shows throughput and latency of the reference counting algorithm compared to the \noriginal JVM: while we essentially retain the throughput attained by the original JVM, we improve the \nmaximal response time by two orders of magnitude. To illustrate, the original JVM may take as long as \n16 seconds to complete a JBB transaction while we never require more than 130 milliseconds. To get some \nmore measurements of the latency, we also checked the latency as a function of the number of threads \nran by SPECjbb00. We compared the response time of the original JVM with our reference counting collector. \nThis non-standard run of the benchmark is reported in Figure 20. The second benchmark that we have used \nis MTRT (multi\u00adthreaded ray tracer). This benchmark does not measure response time, only elapsed running \ntime, which corresponds to the JVM s throughput. As can be seen from .gure 16 the reference counting \ncollector outperforms the original JVM with an improvement of up to 10% in the total running time. In \nFigure 17 we present measurements of the heap consump\u00adtion. The reason for increased consumption in the \nreference counting algorithm is the lack of compaction yielding more fragmentation and the space required \nfor the dirty bit (which is implemented as an extra pointer per object, refer to the technical report \n[29] for details). We remark that since we do not move objects for compaction, we can get most of this \nwaste back by joining the object and its handle and getting rid of the handle pointer to the object. \nIn addition to the space occupied directly by objects we also allocate memory from the operating system \nfor the following data structures: ZCT: we implement the ZCT as a bitmap with each poten\u00adtial object \naddress having an associated bit in the bitmap. Since the object alignment is eight bytes the ZCT requires \na sixteenth of the heap. Benchmark Time to completion (seconds) Original RC Total 2582.2 2676.0 compress \n720.8 723.3 db 374.0 383.7 jack 264.6 299.7 javac 225.0 235.2 jess 181.7 209.7 mpegaudio 607.1 610.6 \nFigure 18: Elapsed time for the execution of the entire SPECjvm98 suite and intermediate execution time \nof a double-run for each of the suite s members. All measurements are in seconds.  Benchmark % Reclaimed \nby tracing % Reclaimed by RC jbb 97.5% 96.5% compress 73.5% 72.1% db 99.6% 90.5% jack 99.6% 96.8% javac \n99.6% 66.1% jess 99.8% 99.5% mpegaudio 74.2% 69.6% Figure 19: Percentage of objects reclaimed by (1) \nthe original collector and (2) the reference counting collector when used without the backing mark-and\u00adsweep \ncollector.  Snoop/local marks: similarly, we mark objects as snooped using a bitmap which requires an \nadditional sixteenth of the heap. Reference counters: the reference counters are imple\u00admented in a bitmap \nwhich associates each potential object address with a two bit counter. Hence this bitmap requires an \nadditional eighth of the heap. Bu.ers: local ZCTs, update bu.ers and snoop bu.ers are all allocated from \nthe same pool of bu.ers. We have used a .xed bu.er size of 64 KB and usually the working set of bu.ers \ndidn t exceed twenty simultaneously allocated bu.ers, which amounts to less than two MB of additional \nmemory. 6.2 Client performance While we have targeted our collector for multi-processor en\u00advironments \nwe still wanted to verify that it is performant in a single-processor setting. To that end we have used \nthe SPECjvm98 benchmark suite. We used the suite using the test harness, performing standard1 automated \nruns of all the benchmarks in the suite. In a standard automated run, each benchmark is ran twice and \nall benchmarks are ran on the same JVM one after the other. Figure 18 shows the elapsed time of the entire \nautomated run and the time for each double run of each benchmark. As can be seen from .gure 18, the reference \ncounting collector was only 3.6% percent slower than the original JVM. Given that we pay the overheads \nof concurrent collection while we re not ben\u00ade.ting from the availability of multiple processors these \nare very good results. 6.3 Collector characteristics We also include some measurements of the collector \nchar\u00adacteristics. Due to lack of space, we only mention a couple of them brie.y. First, we measured the \nnumber of objects that have reached a stuck count (i.e., o.RC = 3). Recall 1The standard run requires \nrunning the harness through a Web server while we performed the tests directly o. the disk. Aside from \nthat, the executions were standard. that we keep only two bits for the reference count and an object \nwhose RC is increased to 3 is considered stuck. This reference count is resolved only in the following \nrun of the mark and sweep collector. It turns out that for most bench\u00admarks this happens to less than \n1% of the objects. For compress, javac, and mpegaudio this number was higher: between 3.7% to 4.7% of \nthe objects. Stuck pointers and cycles prevent the reference counting col\u00adlector from collecting all \ndead objects. We check the ef\u00adfectiveness of the collector in Figure 19. Except for javac, which uses \nmany cyclic structures, and to a lesser degree the db benchmark, the benchmarks have demonstrated a low \ndegree of sensitivity to reference counting. This sup\u00adports the assumption that we may use reference \ncounting for most garbage collection cycles and only occasionally re\u00adsort to tracing.  7. CONCLUSIONS \nWe have presented a novel on-the-.y reference counting garbage collector with low latency and high throughput. \nThe al\u00adgorithm uses extremely low synchronization overhead: the barriers for modifying a reference and \nthe barrier for creat\u00ading a new object are very short and in particular, require no strong synchronized \noperations such as a compare-and\u00adswap instruction. Furthermore, there is no particular point in which \nall threads must be suspended simultaneously. In\u00adstead, each thread cooperates with the collector by \nbeing shortly suspended four times during each collection cycle. We have implemented our collector on \nSUN s Reference Re\u00adlease 1.2.2 of the Java Virtual Machine and presented mea\u00adsurements showing excellent \nlatency and a throughputthat is comparable to the original mark-sweep-compact collector. 8. ACKNOWLEDGMENT \nWe thank Hillel Kolodner for helpful discussions.  9. REFERENCES [1] Alfred V. Aho, Brian W. Kernighan, \nand Peter J. Weinberger. The AWK Programming Language. Addison-Wesley, 1988. [2] Y. Riany, N. Shavit, \nD. Touitou. Towards a Practical Snapshot Algorithm. Proceedings of the Third Israel Symposium on Theory \nand Computing Systems (ISTCS), Tel Aviv, (1995), 121-129. [3] Andrew W. Appel, John R. Ellis, and Kai \nLi. Real-time concurrent collection on stock multiprocessors. ACM SIGPLAN Notices, 23(7):11 20, 1988. \n[4] D. Bacon, C. Attanasio, H. Lee, V. Rajan, and S. Smith. Java without the co.ee breaks: A nonintrusive \nmultiprocessor garbage collector. To appear in the ACM SIGPLAN Conference on Programming Language Design \nand Implementation (PLDI), Snowbird, Utah, June 20-22 2001. [5] D. Bacon and V. Rajan. Concurrent Cycle \nCollection in Reference Counted Systems. To appear in the Fifteenth European Conference on Object-Oriented \nProgramming (ECOOP), University E\u00a8otv\u00a8os Lorand, Budapest, Hungary, June 18-22 2001. [6] Henry G. Baker. \nList processing in real-time on a serial computer. Communications of the ACM, 21(4):280 94, 1978. [7] \nHenry G. Baker. Minimising reference count updating with deferred and anchored pointers for functional \ndata structures. ACM SIGPLAN Notices, 29(9), September 1994. [8] Hans-Juergen B\u00a8ohm, Alan J. Demers, \nand Scott Shenker. Mostly parallel garbage collection. ACM SIGPLAN Notices, 26(6):157 164, 1991. [9] \nT. Chikayama and Y. Kimura. Multiple reference management in Flat GHC. ICLP, pages 276 293, 1987. [10] \nTrishul M. Chilimbi and James R. Larus. Using generational garbage collection to implement cache-conscious \ndata placement. In Proceedings of the First International Symposium on Memory Management, volume 34(3) \nof ACM SIGPLAN Notices, October 1998, pages 37-48. [11] George E. Collins. A method for overlapping and \nerasure of lists. Communications of the ACM, 3(12):655 657, December 1960. [12] Jim Crammond. A garbage \ncollection algorithm for shared memory parallel processors. International Journal Of Parallel Programming, \n17(6):497 522, 1988. [13] John DeTreville. Experience with concurrent garbage collectors for Modula-2+. \nTechnical Report 64, DEC Systems Research Center, Palo Alto, CA, August 1990. [14] John DeTreville. Experience \nwith garbage collection for modula-2+ in the topaz environment. In OOPSLA/ECOOP 90 Workshop on Garbage \nCollection in Object-Oriented Systems, October 1990. [15] L. Peter Deutsch and Daniel G. Bobrow. An e.cient \nincremental automatic garbage collector. Communications of the ACM, 19(9):522 526, September 1976. [16] \nSylvia Dieckmann and Urs H\u00a8olzle. A Study of the Allocation Behavior of the SPECjvm98 Java Benchmarks. \nProceedings of the European Conference on Object-Oriented Programming (ECOOP 99), Lecture Notes on Computer \nScience, Springer Verlag, June 1999. [17] Edsgar W. Dijkstra, Leslie Lamport, A. J. Martin, C. S. Scholten, \nand E. F. M. Ste.ens. On-the-.y garbage collection: An exercise in cooperation. Communications of the \nACM, 21(11):965 975, November 1978. [18] Damien Doligez and Georges Gonthier. Portable, unobtrusive garbage \ncollection for multiprocessor systems. In POPL 1994. [19] Damien Doligez and Xavier Leroy. A concurrent \ngenerational garbage collector for a multi-threaded implementation of ML. In POPL 1993. [20] Tamar Domani, \nElliot K. Kolodner, Ethan Lewis, Elliot E. Salant, Katherine Barabash, Itai Lahan, Yossi Levanoni, Erez \nPetrank, and Igor Yanover. Implementing an On-the-.y Garbage Collector for Java. The 2000 International \nSymposium on Memory Management, October, 2000. [21] Toshio Endo, Kenjiro Taura, and Akinori Yonezawa. \nA scalable mark-sweep garbage collector on large-scale shared-memory machines. In Proceedings of High \nPerformance Computing and Networking (SC 97), 1997. [22] Shinichi Furusou, Satoshi Matsuoka, and Akinori \nYonezawa. Parallel conservative garbage collection with fast allocation. In Paul R. Wilson and Barry \nHayes, editors, OOPSLA/ECOOP 91 Workshop on Garbage Collection in Object-Oriented Systems, 1991. [23] \nAdele Goldberg and D. Robson. Smalltalk-80: The Language and its Implementation. Addison-Wesley, 1983. \n[24] Atsuhiro Goto, Y. Kimura, T. Nakagawa, and T. Chikayama. Lazy reference counting: An incremental \ngarbage collection method for parallel inference machines. ICLP, pages 1241 1256, 1988. [25] Robert H. \nHalstead. Multilisp: A language for concurrent symbolic computation. ACM TOPLAS, 7(4):501 538, October \n1985. [26] Maurice Herlihy and J. Eliot B Moss. Non-blocking garbage collection for multiprocessors. \nTechnical Report CRL 90/9, DEC Cambridge Research Laboratory, 1990. [27] Richard E. Jones and Rafael \nLins. Garbage Collection: Algorithms for Automatic Dynamic Memory Management. Wiley, July 1996. [28] \nElliot K. Kolodner and Erez Petrank. Parallel copying garbage collection using delayed allocation. Technical \nReport 88.384, IBM Haifa Research Lab, November 1999. Available at http://www.cs.princeton.edu/~erez/publications.html. \n[29] Yossi Levanoni and Erez Petrank. A scalable reference counting garbage collector. Technical Report \nCS0967, Technion, Israel Institute of Technology, November 1999. Available at http://www.cs.technion.ac.il/~erez/publications.html. \n[30] James S. Miller and B. Epstein. Garbage collection in MultiScheme. In US/Japan Workshop on Parallel \nLisp, LNCS 441, pages 138 160, June 1990. [31] James W. O Toole and Scott M. Nettles. Concurrent replicating \ngarbage collection. Also LFP94 and OOPSLA93 Workshop on Memory Management and Garbage Collection. [32] \nYoung G. Park and Benjamin Goldberg. Static analysis for optimising reference counting. IPL, 55(4):229 \n234, August 1995. [33] Manoj Plakal and Charles N. Fischer. Concurrent Garbage Collection Using Program \nSlices on Multithreaded Processors. ISMM 2000. [34] Tony Printezis and David Detlefs. A generational \nmostly-concurrent garbage collector. ISMM 2000. [35] David J. Roth and David S. Wise. One-bit counts \nbetween unique and sticky. ACM SIGPLAN Notices, pages 49 56, October 1998. ACM Press. [36] Ran Shaham, \nElliot K. Kolodner, and Mooly Sagiv. On the E.ectiveness of GC in Java. The 2000 International Symposium \non Memory Management (ISMM 00) 2000. [37] Guy L. Steele. Multiprocessing compactifying garbage collection. \nCommunications of the ACM, 18(9):495-508, September 1975. [38] Standard Performance Evaluation Corporation, \nhttp://www.spec.org/ [39] Will R. Stoye, T. J. W. Clarke, and Arthur C. Norman. Some practical methods \nfor rapid combinator reduction. In LFP, pages 159 166, August 1984. [40] Larry Wall and Randal L. Schwartz. \nProgramming Perl. O Reilly and Associates, Inc., 1991. [41] J. Weizenbaum. Symmetric list processor. \nCommunications of the ACM, 6(9):524 544, September 1963. [42] David S. Wise. Stop and one-bit reference \ncounting. IPL, 46(5):243 249, July 1993. [43] David S. Wise. Stop and one-bit reference counting. Technical \nReport 360, Indiana University, Computer Science Department, March 1993. [44] Taichi Yuasa. Real-time \ngarbage collection on general-purpose machines. Journal of Software and Systems, 11(3):181 198, 1990. \n Threads 1 2 4 6 8 10 15 Original 7433 8037 8463 6923 7857 7536 6593 RC 16 16 47 78 110 146 250 Times \nRC is more responsive \u00d7464 \u00d7502 \u00d7180 \u00d788 \u00d771 \u00d751 \u00d726 Figure 20: Maximal response time, in milliseconds, \nof the original JVM and our reference counting col\u00adlectors in a series of SPECjbb2000 runs with a .xed \nnumber of threads per run and a 600 MB heap.   \n\t\t\t", "proc_id": "504282", "abstract": "Reference counting is not naturally suitable for running on multiprocessors. The update of pointers and reference counts requires atomic and synchronized operations. We present a novel reference counting algorithm suitable for a multiprocessor that does not require any synchronized operation in its write barrier (not even a compare-and-swap type of synchronization). The algorithm is efficient and may complete with any tracing algorithm.", "authors": [{"name": "Yossi Levanoni", "author_profile_id": "81100058547", "affiliation": "Microsoft Corporation, One Microsoft Way, Redmond, WA, ", "person_id": "PP14031440", "email_address": "", "orcid_id": ""}, {"name": "Erez Petrank", "author_profile_id": "81100377919", "affiliation": "Dept. of Computer Science, Technion - Israel Institute of Technology, Haifa 32000, Israel", "person_id": "PP39040138", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/504282.504309", "year": "2001", "article_id": "504309", "conference": "OOPSLA", "title": "An on-the-fly reference counting garbage collector for Java", "url": "http://dl.acm.org/citation.cfm?id=504309"}