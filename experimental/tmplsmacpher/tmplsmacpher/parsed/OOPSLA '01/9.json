{"article_publication_date": "10-01-2001", "fulltext": "\n Multitasking without Compromise: a Virtual Machine Evolution Grzegorz Czajkowski Laurent Dayn\u00e8s Sun \nMicrosystems Laboratories 2600 Casey Avenue Mountain View, CA 94043, USA +1 650 336 6501 grzegorz.czajkowski@sun.com \nABSTRACT The Multitasking Virtual Machine (called from now on simply MVM) is a modification of the Java \nvirtual machine. It enables safe, secure, and scalable multitasking. Safety is achieved by strict isolation \nof applications from one another. Resource control mechanisms augment security by preventing some denial-of\u00adservice \nattacks. Improved scalability results from an aggressive application of the main design principle of \nMVM: share as much of the runtime as possible among applications and replicate everything else. The system \ncan be described as a no compromise approach all the known APIs and mechanisms of the Java programming \nlanguage are available to applications. MVM is implemented as a series of carefully tuned modifications \nto the Java HotSpot virtual machine, including the dynamic compiler. This paper presents the design of \nMVM, focusing on several novel and general techniques: an in-runtime design of lightweight isolation, \nan extension of a copying, generational garbage collector to provide best-effort management of a portion \nof the heap space, and a transparent and automated mechanism for safe execution of user-level native \ncode. MVM demonstrates that multitasking in a safe language can be accomplished with a high degree of \nprotection, without constraining the language, and with competitive performance characteristics.  Keywords \nJava virtual machine, application isolation, resource control, native code execution. 1 INTRODUCTION \nIt is not uncommon to come across a computer installation where a majority of executing computations \nare written in the Java programming language [1,11]. The Java virtual machine [19] is used to execute \na widely diversified mix of programs from applets in Web browsers to standalone applications to Enterprise \nJavaBeans components executing in application servers. No matter whether the program is a tiny applet \nor a complex +1 650 336 5101 laurent.daynes@sun.com application, it effectively views the JVM as an \nersatz operating system (OS). However, the capabilities of the environment fall short of what an OS should \nprovide to applications. The existing application isolation mechanisms, such as class loaders [16], do \nnot guarantee that two arbitrary applications executing in the same instance of the JVM will not interfere \nwith one another. Such interference can occur in many places. For instance, mutable parts of classes \ncan leak object references and can allow one application to prevent the others from invoking certain \nmethods. The internalized strings introduce shared, easy to capture monitors. Sharing event and finalization \nqueues and their associated handling threads can block or hinder the execution of some application. Monopolizing \nof computational resources, such as heap memory, by one application can starve the others. Some of these \ncases of interference are subtle while others are easy to detect. Some manifest themselves rarely while \nothers are notorious. All are undesired. Their existence perpetuates the current situation, where the \nonly safe way to execute multiple applications, written in the Java programming language, on the same \ncomputer is to use a separate JVM for each of them, and execute each JVM in a separate OS process. This \nintroduces various inefficiencies in resource utilization, which downgrades performance, scalability, \nand application startup time. The benefits the language can offer are thus reduced mainly to portability \nand improved programmer productivity. Granted, these are important features, but the full potential of \nlanguage-provided safety and protection is not realized. Instead, there exists a curious distinction \nbetween language safety and real safety , where the first one slips more and more into the academic domain \nand the other means hardware-assisted, OS-style multitasking, preferable for everyday use. While the \ncontributions of related research are invaluable [2,3,4,5,12,24], from a pragmatic standpoint the resulting \nprototypes are often impractical: either the semantics of the language is constrained, some features \nor mechanisms are excluded, performance is unsatisfactory, or all of the above. To a large extent the \nproposed techniques and solutions have not yet translated into an industry-strength, widely used implementation \nof a multitasking-safe JVM because of a lack of a complete, well\u00adperforming prototype. Permission to \nmake digital or hard copies of part or all of this work or personal or classroom use is granted without \nfee provided that copies are not made or distributed for profit or commercial advantage and that copies \nbear this notice and the full citation on the first page. To copy otherwise, to republish, to post on \nservers, or to redistribute to lists, requires prior specific permission and/or a fee. OOPSLA 01 Tampa \nFlorida USA Copyright ACM 2001 1-58113-335-9/01/10 $5.00 This paper presents MVM, a multitasking version \nof the JVM. Applications that run in the MVM are protected from one another, and sources of inter-application \ninteraction currently present in the JVM are removed. A very lightweight design promotes sharing of as \nmuch of the runtime as possible among applications, improving performance and scalability. Unused memory \nis managed in a best-effort fashion, further improving performance. The contributions of this work are: \n(i) the design of lightweight isolation, (ii) an approach to best-effort memory management in a multitasking \nenvironment with a shared, garbage-collected heap, and (iii) a safe method for executing user-supplied \nnative code in such an environment. MVM demonstrates that it is possible to build a high quality, full-featured, \nsafe-language multitasking environment. The rest of this paper is organized as follows. Section 2 presents \nan overview of the goals and design of MVM. Sections 3 through 5 focus on selected details of the system: \nlightweight isolation, memory management, and native code execution, respectively. Selected areas of \nrelated work are discussed whenever appropriate in Sections 2 to 5. Several recent papers [2,5,6,9] contain \nan up-to-date overview of related projects. We will only describe details directly related to the techniques \npresented in this paper, and refer interested readers to the cited work for broader exposition. Similarly \nto dispersing the discussion of related work throughout the paper, the performance issues are analyzed \nin Sections 3 to 5. The experimental setup consists of a Sun Enterprise 3500 server with four UltraSPARC \nII processors, with 4GB of main memory, running the Solaris Operating Environment, version 2.8. The Java \nHotSpot virtual machine [26] (referred to from now on as HSVM), version 1.3.1, with the JDK version 1.3.1, \nis the code basis for MVM. The SPECjvm98 benchmarks [25] are used.  2 OVERVIEW OF THE DESIGN MVM is \na general-purpose environment for executing Java applications, also referred to as tasks1. MVM-aware \napplications, such as application server engines, can use the provided API to create tasks, to terminate, \nsuspend and resume them at any point of their execution, and to control the computational resources available \nto tasks. The main task does not have to be a server it can be any Java application. Three goals dictate \nour design choices: (i) no form of interference among executing applications should be allowed, (ii) \nan illusion of having the JVM (with all core APIs and standard mechanisms) to itself should be provided \nfor each task, and (iii) MVM should perform and scale well. The motivation is to make the system attractive \nfrom the practical point of view. The key design principle of MVM is: examine each component of the JVM \nand determine whether sharing it among tasks can lead to any interference among them. In some cases this \napproach yields a clear verdict that the given component can be shared without jeopardizing the safety \nof the tasks. Other components are either replicated on a per-task basis or made task re-entrant,that \nis, usable by many tasks without causing any inter-task interference. This builds on the ideas described \nin [6]. The technique presented in that work replicating static fields and class monitors has been \ngeneralized in MVM to classify all components of the JVM as shareable or non-shareable . Shareable components, \nwhich require some modifications to become task re-entrant, include the constant pool, the interpreter, \nthe dynamic compiler, and the code it produces. An arbitrary 1 The term task is perhaps not the most \nfortunate one, since it is already used in other contexts. The JSR 121 [15], under discussion at the \ntime of this writing, is likely to come up with better terminology. number of tasks in MVM can share \nthe code (bytecode and compiled) of both core and application classes. Non-shareable components include \nstatic fields, class initialization state, and instances of java.lang.Class. Runtime modifications presented \nin Section 3 make these replications transparent and ensure that the garbage collector is aware of them. \nThe heaps of tasks are logically disjoint, but at the physical data layout level they may be interleaved. \nAn efficient implementation of flexible per-task heap memory guarantees, enforceable limits, and transparent \nre-use of surplus memory is provided (Section 4). The separation of tasks data sets in MVM implies that \ntasks cannot directly share objects, and the only way for tasks to communicate is to use standard, copying \ncommunication mechanisms, such as sockets or RMI. This is a conscious and long deliberated decision. \nExisting models of controlled direct sharing, such as [2,4,5,12], are not convincing from the practical \nstandpoint. First, some of them complicate the programming model, since there are now two kinds of objects \n non-shared, with unconstrained access, and shared, with some restrictions on their use. Second, some \napproaches have been implemented only via bytecode editing, and it is not clear what inter-task dependencies \nwould arise from the proposed sharing if it were implemented in the runtime of a complex, high-performance \nvirtual machine. Such dependencies can greatly complicate resource reclamation, accounting, and task \ntermination. Finally, some proposed inter-task communication mechanisms introduce up-front overheads, \naffecting all tasks, even those that never communicate. Native methods of core classes are well tested \nand are shared by all tasks. However, an audit of their sources was necessary to identify global state, \nand some global variables are replicated in MVM. For instance, in the JDK 1.3.1 a global buffer for error \nmessages generated by native methods of the java.util.zip package is declared as: static char errbuf[256]; \n In MVM the buffer is replicated so that each task using the package has its own copy to avoid accidental \ngarbling of error messages. But two other native global variables, associated with the same package \na list of currently open zip files and an associated access lock, are not replicated in MVM, since they \nare a part of a global, task-safe service managing zip files. Overall, global variables in the core native \nlibraries are rather infrequent, but the role of each of them has to be analyzed before deciding whether \nto replicate it or not. User-defined native code is neither known a priori nor known to be well behaved. \nSuch libraries are not shared. A new technique to transparently execute them in a separate process is \npresented in Section 5. Global state of Java classes is transparently replicated by runtime modifications \n(Section 3). However, an audit of core classes is still necessary to prevent interference related to \nusing OS services. In particular, the System and Runtime classes of the java.lang package define services, \nwhich should be customized on a per\u00adtask basis. A case-by-case analysis is necessary. The resulting modifications \nare similar to those reported in [3,12], and we will not describe them here. Other issues not further \ndiscussed in this paper include replicating event queues to deal with multiple tasks using the graphics \nsubsystem, equipping each task with its own finalizer thread and finalization queue in order to avoid \nhijack the finalizer thread attacks, and accounting for most resources, since all of them are rather \nstraightforward when performed in the modified runtime. The modifications to the bootstrap sequence are \nalso not discussed, since even though many things happen between launching of the virtual machine and \nstarting to execute an application, to a large extent they are implementation-specific and their description \nwould not convey a general insight. Asynchronous task termination, available in MVM, is also very implementation-dependent, \nbut considerably easier than a single thread termination, since no application state is shared among \ntasks. Finally, the issues related to security are beyond the scope of this paper. Let us only say that \neach task has its own set of permissions, and that MVM neither prevents nor clashes with user-defined \nclass loaders and the notion of protection domains [10]. 2.1 High-Level Design Choices Such JVM evolution \ntowards multitasking leads to a system in which the known sources of inter-task interference are removed. \nThe examination and replication of problematic components preserves their behavior from the applications \nperspective, and makes their execution in MVM indistinguishable from their execution in the standard \nJVM. It is illustrative to stress the differences between this work and other approaches addressing various \nissues related to multitasking in the JVM. The results of related projects greatly advanced the understanding \nof the potential and limitations of multitasking in language-based systems. From the practical standpoint, \nthough, existing research and resulting prototypes exhibit at least one of the following problems: (i) \nthe black box approach, (ii) using inferior quality systems, (iii) dropping language features, or (iv) \nreplicating OS architecture. The black box approach. Various issues have been addressed via bytecode \nediting for example, resource control [8], isolation [6,12], and termination [23]. Transforming programs \nto achieve desired properties is elegant, portable, and is a great tool for testing new ideas. However, \ntreating the JVM as a black box has not yet led to a satisfactory solution to these issues. For instance, \nabove-the-runtime isolation and resource control are far from perfect because of various interactions \nand resource consumptions taking place in the JVM, and task termination is problematic when a thread \nof a terminated task holds a monitor. Moreover, source\u00adlevel transformations complicate application debugging. \nUsing inferior quality systems. The benefits offered by new functionality of various extensions to the \nJVM can sometimes exceed the associated performance and space costs. But this is often not the case, \nand it is important to understand the sources of overheads and the impact on application performance. \nExperimenting with an under-performing implementation of the JVM leads to qualitative understanding of \nproposed improvements, but quantitative studies do not necessarily translate into high-end implementations. \nLow-quality compilers may make the performance impact of a particular modification look small or negligible, \nand in extreme cases (no compiler, just the interpreter) the costs of modifications may be completely \ndwarfed by the slowness of the runtime. This distorts the cost-benefit calculus when a given feature \nis seriously considered for inclusion in the JVM. Dropping language features. None of the known to us \nmultitasking systems based on the JVM safely handles all the mechanisms of the Java programming language. \nThis is because certain features, such as non-terminating finalizers or user\u00adsupplied native code, are \nparticularly troublesome to make task\u00adsafe. Many applications will run correctly without these mechanisms, \nwhich is often used as a mandate to ignore or prohibit some legitimate parts of the language. From the \npractical standpoint this cannot be accepted. Otherwise, not all existing code can be executed in such \nenvironments, and new code has to be constrained to use only the approved subset of the language. A JVM-based \nmultitasking system must be feature-complete to avoid this. Replicating OS architecture. Our goal is \nto turn the JVM into an execution environment akin to an OS. In particular, the abstraction of a process, \noffered by modern OSes, is the role model in terms of features: isolation from other computations, resource \naccountability and control, and ease of termination and resource reclamation. However, this does not \nmean that the structure of modern OSes should be a blueprint for the design of multitasking in the JVM. \nThe runtimes of safe languages manage resources differently than an OS: memory management is automatic \nand object-centric; memory accesses are automatically safe; interpreted bytecodes co-exists with compiled \ncode, and the runtime has control over which form to choose/generate. This can be taken advantage of, \nwithout undue replication of any of the mechanisms and components of the JVM. The above are serious concerns \nfor systems aiming at being practical, full-featured, and scalable, and that is why the design of MVM \navoids all of them.  3 LIGHTWEIGHT ISOLATION This section presents the details of lightweight task \nisolation in MVM. The runtime is modified according to the principle of sharing as much of it as possible \namong tasks and replicating everything else. The sources of complexity are: (i) class initialization, \nwhich must be done once for every task, upon first use of the class by a task [11], (ii) ensuring that \nthe appropriate copy of a replicated item is accessed, (iii) efficient retrieval of per-task information \n(e.g., static variables and initialization state of classes), and (iv) making the retrieval scale with \nthe number of tasks. The changes affect the runtime class meta-data layout, the runtime sequence for \nloading, linking and initializing classes, the interpretation of a few bytecodes, the compiler, and the \ngarbage collector. The design of MVM forces all tasks to use the same source for their bootstrap and \napplication class loaders. This simplifies sharing of the runtime representation of classes (including \nbytecodes and code produced by the dynamic compiler), which is currently supported only for classes loaded \nby a task s bootstrap or application class loaders. This design choice does not prevent class loaders \nfrom being used in MVM, but it means that the runtime representation of classes loaded with application-defined \nloaders are not shared, and each such class is separately compiled. 3.1 Runtime Data Structures The \nHSVM maintains two representations of a class: a heap\u00adallocated instance of java.lang.Class, and an internal, \nmain memory representation of the corresponding class file, optimized for efficient use by both the interpreter \nand the code generated by the dynamic compiler. The two representations reference each other, and static \nvariables are embedded in the internal representation of the class. Synchronization on a class object \n(either explicit via certain uses of synchronized blocks or implicit via synchronized static methods) \nis performed on the monitor of the java.lang.Class instance representing this class. In MVM, all internal \nclass representation information that needs to be replicated is stored in a task class mirror (TCM) object. \nEach TCM contains the static variables, a back pointer to the java.lang.Class object representing the \nclass for this TCM s task, caches for speeding up type check operations, and the initialization state \nof the class (data indicating whether the class has been loaded, linked, initialized, etc.). Note that \ninterference between tasks on class monitors is avoided since each task is provided a separate instance \nof java.lang.Class.A table of TCM objects is associated with each internal class representation. A unique \ntask identifier indexes the table, so that each table contains one entry per task. Each entry in a TCM \ntable consists of two references to the same TCM: the first one is set at class load-time, the second \nonce the class is fully initialized. The reason for this will be clarified later. The internal representation \nof array classes is similarly changed. Threads are augmented with a field indicating the identifier of \nthe task on behalf of which they execute. The identifier is also used as an offset into TCM tables for \nfast retrieval. Additionally, MVM maintains a table of virtual machine objects, each containing a copy \nof the global variables previously maintained by HSVM, and which must be replicated for each task in \nMVM. Examples of such global variables include references to special objects (e.g., instances of common \nruntime errors and exceptions such as java.lang.OutOfMemoryError), counters of daemons and live threads, \netc. Figure 1 illustrates the organization of MVM. All TCM tables, as well as the table of virtual machine \nobjects, initially have the same size and reflect the current maximum number of concurrent tasks that \nMVM can support. This maximum can be adjusted at runtime to respond to peak loads.   For each class \nHSVM maintains a constant pool cache, which is a compact representation of the original constant pool, \noptimized for the interpreter and the dynamic compiler. Constant pool caches are built at class link \ntime. MVM modifies the entries corresponding to static methods and static variables as follows: direct \nreferences to the internal representation of the class are replaced with direct references to their TCM \ntable; offsets to static variables are changed to refer to offsets relative to TCMs. The interpreter \nuses this organization of runtime data structures to access static variables and to invoke static methods \n(Section 3.4). Note that the only outgoing pointers from the shared runtime data structures to heap data \nof a particular task come from the objects in TCM tables, the table of virtual machine objects, and from \nthe system dictionary (a mapping of class names to shared class representations).  3.2 Fast Class Loading \nand Linking Before being used, a class must first be loaded, linked and initialized. Since in MVM most \nof the class representation is shared, so is the class loading and linking effort. Not all loading and \nlinking steps can be entirely eliminated though: the JVM needs, at least, to keep track of the initialization \nstate of a class for each task, and to carry the class loader information of a class from the time the \nclass has been loaded. Static variables must also be accessible from other classes before the class is \ninitialized, in order to support some legal circularity in the sequence of class initialization (it is \npossible for a class to see an non-initialized, but prepared, static variable of another class during \nthe execution of a static initializer [19]). Loading of a class consists essentially of fetching a class \nfile from a specified source and parsing its content in order to build a main\u00admemory representation adequate \nfor the JVM. This needs to be done only once, by the first task to load the class, regardless of how \nmany tasks use the class. This one-time loading sequence creates the internal representation of a class, \nwhich, in MVM, is shared by all classes, and comprises a TCM table. All the entries of the TCM table \nare initially set to a null value. The task-specific part of class loading creates a Task X Task Y  \nTCM, prepares its static variables, enters its reference at the proper entry of the TCM table, and marks \nthe TCM as loaded . Class loading for all but the first task to load a class is reduced to setting up \na TCM as just described. Substantial parts of class linking can be eliminated too. Linking consists mainly \nof class verification, checking and updating of loader constraints, and, in the case of HSVM, building \na constant pool cache for the class and rewriting its bytecodes so that they refer to the entries of \nthe cache instead of entries of the original constant pool. In MVM all these steps need to be done only \nby the first task to link this class. For all other tasks, linking requires only verifying that the class \nand all its dependents have already been linked, and then marking the TCM linked . 3.3 Class Initialization \nand Resolution Barriers JVM implementations commonly use dynamic code rewriting techniques, such as \nbytecode quickening [18] and native code patching, to dynamically remove class initialization and symbolic \nlink resolution barriers. A class initialization barrier tests whether a class has been initialized and \ntriggers its initialization if it has not. Similarly, a symbolic link resolution barrier test whether \na symbolic link (i.e., an entry in a class constant pool) has been resolved, and if not, proceeds to \nresolve it. Both types of barriers always succeed except the first time the barrier is encountered, hence \nthe use of code rewriting techniques to remove them. These barriers cannot always be eliminated in MVM \nbecause both bytecodes and the code generated by the dynamic compiler are shared by multiple tasks, which \nmay each be at different stages of initialization for a given class. More precisely, a class initialization \nbarrier must be executed at least once per task per class. However, most link resolution barriers need \nonly be executed once in MVM: in order to resolve a symbolic link, the class this symbol refers to (directly \nor indirectly) must be loaded and linked; but, for a given class, only one link resolution can trigger \nits loading and linking. MVM needs a link resolution barrier for a symbol already resolved by another \ntask only if the barrier may trigger a class load. Most bytecode instructions that require a link resolution \nbarrier cannot trigger a class load or link by themselves, either because they are always preceded by \nanother instruction that must have already done it (e.g., both getfield and putfield must have been preceded \nby a new of the corresponding object), or because they refer to constants of the current class (e.g., \nldc) which must have been loaded and linked prior to the execution of the method that contains this instruction. \nThus, most link resolution barriers need to be executed only once by MVM, and most of HSVM s bytecode \nand native code rewriting can remain unchanged. When modifying HSVM runtime data structures, special \nattention was paid to minimize the overhead of both class initialization barriers and access to static \nvariables. Testing whether a task has initialized a class amounts to checking if the entry in the TCM \ntable of that class for that task is non-null. On a SPARC V9 platform [30], the testing part of the class \ninitialization barrier takes three instructions: two loads and one branch on register value. ld [gthread \n+ task_id_offset], task_id ld [tcm_table + task_id], tcm brnz,pt,a,tcm end_barrier <delay slot filled \nwith something useful> call task_class_initialization_stub nop end_barrier: The first load fetches a \nunique internal task identifier from the current thread data structure (a register is allocated to permanently \nstore the address of this structure). The task identifier is then used as an index into the TCM table, \nto fetch the address of the corresponding TCM (second load). A null address means that the class has \nnot yet been initialized by the current task, and a branch on null register value appropriately dispatches \nto the interpreter runtime for initializing the class for the current task. Augmenting TCMs with fields \nrecording the initialization status of the class and the thread initializing this class for the Internal \nclass s task table task class mirror loaded linked initialized representation  of a class   loading \nlinking initializing Figure 2. Fast initialization testing. corresponding task is enough to make the \nexisting HSVM code task-re-entrant. The main issue is to locate the TCM of the initializing task. The \nentry in the TCM table cannot be used to store the TCM created by the task during class load but before \nthe class is initialized for this task, because this would invalidate the null pointer test performed \nupon a class initialization barrier. This is why the TCM table holds two references to the same TCM for \neach task. As illustrated by Figure 2, each reference is set up at a different time: the first one is \nset up during class loading, whereas the second one is set up once the class is fully initialized. Class \ninitialization barriers test the first entry only: when the test performed by the barrier fails, the \nTCM for the current task can be obtained simply.  3.4 Bytecode Interpretation MVM leaves the interpretation \nof all standard bytecodes unchanged. Changes are required only for the interpretation of some of the \nquick versions of standard bytecodes, and for the handling of synchronized static methods, which requires \nfinding the instance of java.lang.Class that represents the class for the current task in order to enter \nits monitor. Quickened bytecodes result from the execution of standard bytecodes that require a link \nresolution or a class initialization barrier. The interpretation of such standard bytecodes happens only \nonce and typically consists of executing a barrier, rewriting the bytecode instruction with a quick version \nof it, and executing the quick version. In particular, the standard getstatic and putstatic bytecodes \ndon t access static variables; this is done by their quick versions. As explained in section 3.3, all \nclass initialization barriers that are eliminated by bytecode quickening need to be re-introduced. This \naffects four bytecodes only: the quick versions of new, invokestatic, getstatic,and putstatic. The first \ntwo require an extra load instruction before the barrier code described in section 3.3, in order to fetch \nthe task table of the class. This increases the path\u00adlength of these bytecodes with 4 instructions. The \nquickened versions of getstatic and putstatic need, in addition to the class initialization barrier, \naccess to the TCM of the current task to access the static variables of the class. A cost-free side effect \nof the barrier is to set a register to the TCM of a class. Thus, once the barrier is passed, the static \nvariable can be obtained with a single memory load. This adds 3 instructions to the path-length of getstatic \nand putstatic.  3.5 Sharing Compiled Code Although our practical experience is based on the client compiler \nof HSVM (known as Compiler 1, or C1), the principles described below are generally applicable to most \nruntime compilers. HSVM invokes C1 to compile a method to native code once the method has reached some \nusage threshold (expressed as a number of method invocations). The threshold itself is chosen to correspond \nto a trade-off between the cost of compilation and the perceived performance pay-off. C1 operates in \ntwo steps. First, it generates, into a code buffer, a version of the code that assumes that all classes \nused by the method have been resolved and initialized. This means that the code thus produced does not \ncontain any link resolution or class initialization barriers. The second step scans the code  buffer \nand evaluates the assumptions made. If the assumption is invalid (e.g., Figure 3. Memory footprint of \nMVM runtime. Numbers in brackets indicate the percentage of the the class has not been total footprint \nattributed to report the percentage the code cache and permanent generation initialized), the optimized \ncode is copied into a special area at the end of the generated code and then replaced with a call to \na patching stub that will properly call the JVM runtime. Patching stubs arrange for proper saving of \nregisters and enforce runtime calling conventions, such as setting up garbage\u00adcollection safe-point information. \nCalls to the JVM runtime from a patching stub resume by atomically writing back the optimized code. This \ncode patching mechanism is problematic in MVM because it dynamically eliminates class resolution and \ninitialization barriers. As discussed earlier in section 3.3, the elimination of some of these barriers \nby one task may be incorrect for another one. We opted for a simple solution whereby class initialization \nbarriers are always planted in the generated code. Only those link resolution barriers that could trigger \nclass loading and linking are inserted (Section 3.3). These barriers are planted even for classes that \nare already loaded and initialized at the time the method is being compiled. Behind its apparent simplicity, \nthis approach has a major advantage over more complex schemes that allow elimination of class initialization \nbarriers: because the generated code is re-entrant, new tasks entering the system can immediately start \nexecuting the native code of a method already compiled by other tasks, without having to interpret the \nmethod at all. The downside is that the compiled code is sub-optimal because of the barriers left in \nit. Under this approach, making HSVM task re-entrant requires only three types of changes to the code \ngenerated by the compiler. First, manipulation of static variables is changed to include the level of \nindirection introduced by multitasking. Second, code generated to enter/exit the monitor of a class is \nmodified to locate the appropriate instance of java.lang.Class. Third, for all bytecode instructions \nthat require a class initialization or link resolution barrier systematically, the compiler has been \nmodified to generate code that handles two different cases: (i) at compile time, the class has never \nbeen loaded by any task, and (ii) the class has already been loaded by at least one task. The sequence \nof instructions generated for the first case is essentially a call to a patching stub that will be overwritten \nat runtime with another sequence of instructions handling the second case, using the patching mechanisms \nalready used by HSVM. The difference is that this time, the code generated by the patching stub includes \na barrier. The code for a barrier, and for accessing static variables, is the same for the interpreter \nand the compiled code, and has been described in Section 3.3.  3.6 Performance Two performance goals \nhave steered the design of lightweight isolation: (i) reducing the per-program footprint of the JVM runtime \nin order to better scale with the number of tasks; and (ii), amortizing across program executions the \noverhead incurred by the runtime itself, principally virtual machine startup, dynamic loading, and runtime \ncompilations. The bulk of the runtime data of HSVM resides in two memory areas: the code cache, where \nnative code produced by the dynamic compiler is stored, and a portion of the heap known as the permanent \ngeneration, which mainly holds the runtime representation of class files (class metadata, constant pool, \nbytecode, etc.) and other runtime data structures. MVM s design of lightweight isolation seeks to share \nbetween tasks the parts of both areas that correspond to core and application classes (i.e., classes \nwhose defining loader is not an application-defined class loader). Figure 3 compares the memory footprint \nof MVM with that of HSVM for three benchmarks from SPECjvm98 (raytrace, db, javac). Each bar reports \nthe accumulated sum of the code cache size and the permanent generation size for execution of a benchmark \nwith HSVM (bars labeled hsvm ), or for execution of multiple instances of the same benchmark by concurrent \ntasks with MVM (bars labeled with a numeric value indicating the number of tasks; a value of 0 means \nthat the program was run as the main task; a value n>0 means that an application manager task controls \nthe execution of n programs, each executed as a task). MVM is started with a parameter indicating the \ninitial number of tasks it can support. This parameter determines the default size of the TCM tables \nthat are part of the runtime representation of classes in MVM. Three configurations were used for our \nmeasurements: 64, 256, and 1024. The permanent generation portion of each bar includes the whole of the \npermanent generation sized for 64 tasks. The two topmost components of bars related to MVM indicate how \nmuch additional space is needed to support, 256 and 1024 tasks (i.e., 64+192, and 64+192+768 tasks, respectively). \nTCM tables only consume  Figure 4. The performance of MVM. Time overheads are reported as relative to \nthis additional space. Since none of the HSVM. benchmarks uses application-defined class loaders, the \nnumbers reported for MVM reflect exactly the size of the shared part of both the permanent generation \nand code cache. Several observations based on Figure 3 can be made. First, the code cache can contribute \nto up to 36% (javac) of HSVM runtime footprint, and is therefore worthwhile to share. Second, the size \nof MVM s shared code cache increases with the number of program executions, as the number of compiled \nmethods increases, although this increase is very modest. Similarly, the size of the permanent generation \nalso slightly increases with the number of running tasks. This increase corresponds to the TCM objects \n(mostly, storage for static variables) added by each running task. Although these are allocated in the \nold generation, we reported them here for comparison with HSVM, which holds the corresponding information \nin its permanent generation. In contrast, running n programs with HSVM would require n times the amount \nreported on Figure 3 for HSVM. The space saving with MVM is immediate: as soon as 2 programs are executed \nwhen MVM is sized for 256 tasks or less, and as soon as 4 programs are executed when MVM is sized for \n1024 tasks. Savings in space quickly reaches one order of magnitude as the number of programs increases. \nThese numbers have to be contrasted with the overall footprint of a program (total heap + code cache): \nacross the benchmarks used for Figure 3, the runtime data of HSVM (code cache + permanent generation) \namounts to between 6% (db) to 13% (raytrace) of the program maximum memory footprint. Sharing the JVM \nruntime can also improve performance in many ways. First, application startup time is improved when compared \nto HSVM, because several steps necessary to launch an application are substantially shortened (e.g., \nloading and linking of all the bootstrap classes), or completely eliminated (e.g., the initialization \nof many internal runtime structures, such as heap, dictionary of loaded classes, etc). Only the first \napplication, typically an application manager, will pay all these costs. The application execution time, \ndefined here as time needed to execute the static main method (the application entry point), also benefits \nfrom similar savings (i.e., loading and linking of classes already loaded and linked by other tasks is \nfaster). More importantly, sharing compiled code means that a task can immediately re-use the code of \nmethods compiled by other tasks, thus shaving off compilation and interpretation costs from its execution \ntime. Furthermore, in MVM, methods infrequently executed by a single execution of a program may still \nbe compiled after several executions of the same or other programs. This is so because method invocation \ncounters are global in MVM. Thus, the code quality may be continually improving, amortizing earlier efforts. \nThis benefits code of popular applications as well as frequently used core classes. On the other hand, \nmultitasking introduces class initialization barriers, and a level of indirection when accessing static \nvariables. The code produced by the dynamic compiler is typically bigger in MVM than in HSVM, because \nof barriers and the associated stubs to call the runtime. Since the costs of some of the compiler operations \ndepend on the number of these stubs and the size of the code produced, MVM indirectly increases the costs \nof dynamic compilation. Figure 4 reports results from experiments that illustrate the trade\u00adoff between \nthe costs of making the JVM task re-entrant and the performance benefits of multitasking. The experiments \nconsisted of executing a program repeatedly as a sequence of tasks. For each sequence, measurements are \nreported as time overhead (or improvement) relative to HSVM. Bars labeled with 0 report the overhead \nof running one instance of a benchmark in MVM as a main application. Bars labeled with a value n>0 report \nthe overheads of running n benchmarks as n tasks started by a simple application manager (the manager \nis the main application in this case). When n>0, the data reported is the overall execution times of \nMVM divided by n. In other words, the overhead reported includes the overhead of the application manager \ntask. When only one task is executed, the overhead of the application manager task is not amortized, \nand performance is usually worse than when no application manager is used (bars corresponding to 0 tasks \n). The overheads of this initial prototype of MVM are within 1-7% when only one instance of a given benchmark \nis run. As soon as more instances of the same code are executed, though, MVM outperforms HSVM in several \ncases, and remains under 2% of overhead in all cases. The performance gain grows with the number of executed \napplications, mainly because of the elimination of runtime compilation and interpretation due to the \nimmediate availability of compiled code. After 50 tasks, the average execution time can decrease by as \nmuch as 16%. MEMORY MANAGEMENT An ability to manage computational resources is necessary to guarantee \ncertain amounts of resources needed for tasks, and to foil certain denial-of-service attacks. Controlling \nand managing heap memory is one of the hardest problems, if not the hardest one, in this area, mainly \nbecause of the difficulties associated with revoking or reclaiming the resource from an uncooperative \ntask. In MVM, all memory of a task is reclaimable once the task has completed. Each task has a guaranteed \namount of memory. Tasks are not allowed to allocate more memory than their guaranteed limit. The only \nexception to this is temporary, transparent use of surplus memory (i.e. memory not backing up any guaranteed \namount Section 4.1). Reclamation of surplus memory from a task is transparent, non-disruptive, and efficient. \nAccounting for memory consumption, needed for enforcing the limits, is accurate and introduces neither \nperformance nor space overheads (Section 4.4). The associated API allows privileged tasks, such as an \napplication manager, to set limits on how much memory a task can use. The limits can be dynamically changed, \nin order to improve overall memory utilization. A privileged task can also set overuse callbacks [8], \nwhich are triggered whenever a particular task attempts to violate its memory limit. The callbacks decide \nwhat to do next with an offending task; one option is instantaneous termination. Privileged tasks can \ncontrol the amount and the recipients of surplus memory, by assigning surplus memory priorities to tasks. \nOnly minor modifications to HSVM s heap management were necessary to incorporate the mechanisms needed \nfor memory accountability, flexible management of surplus memory, and per\u00adtask garbage collections. In \nHSVM the application portion of the heap is organized as two generations, old and new. The new generation \nfollows a design suggested in [28], and is sub-divided into the creation space (eden) and aging space, \nwhich consists of a pair of semi-spaces to and from. New objects are allocated in eden, except for large \narrays, which are allocated directly in the old generation. When the new generation is garbage collected \n(scavenged), the survivors from eden and the objects from the from-space whose age is below a certain \nthreshold are copied into the to-space. Mature objects (i.e. with age above or equal to the threshold) \nare copied to the old generation. The age of each object remaining in to-space is incremented. Finally, \nthe roles of to\u00adspace and from-space are swapped. Whenever the old generation fills up, a global four-phase \npointer-forwarding mark-and\u00adcompact collection is triggered. MVM gives to each task its own private new \ngeneration, consisting of the to-, from-, and eden spaces. The old generation remains shared among all \ntasks. The rationale for these choices is that most action takes place in the young generation the vast \nmajority of objects are allocated there, and garbage collection happens there much more frequently than \nin the old generation. Multiplying the number of young generations is simple and eliminates most of the \nheap-related interference between tasks. The implications of this decision are further discussed in Section \n4.3. 4.1 Management of Surplus Memory Surplus memory is memory not used at the moment as a part of guaranteed \nheap space for any task. Using it may improve application performance. For short-lived tasks, adding \nmemory can postpone GC until after the task completes and all of the task s memory can then be quickly \nreclaimed, potentially avoiding any GC activity. For long-lived, non-interactive programs, additional \nmemory can reduce the frequency of GC. Using surplus memory may thus improve performance but can also \nlead to increased collection pause times. This is undesirable for some applications, such as the interactive \nones, and may be avoided by assigning an appropriate surplus memory priority. Modern automatic memory \nmanagement systems typically copy data to make allocation faster and to improve data locality. This is \noften combined with generational techniques, where long-lived (also known as old or tenured) data is \nmoved around less frequently, and new (young) objects are allocated in the new generation, where they \neither die quickly or are copied and finally get promoted (tenured) to the old generation. Such designs \noptimize for the weak generational hypothesis, which states that most objects die young [28]. The new \ngeneration is a good candidate for receiving surplus memory. Since allocations take place there, more \nmemory available means more allocations without triggering a collection. Less frequent collections allow \nmore objects to die before collections, which can improve performance. Surplus memory can be easily integrated \ninto, used by, and promptly cleared of data and taken away from the new generation. The memory for new \ngenerations is under the control of the New Space Manager (NSM). NSM ensures that each newly started \ntask has a new generation, and is in charge of the best-effort management of currently unused memory. \nWhenever an allocation fails because of eden being full, instead of triggering new generation collection, \nNSM may give the allocating task another memory chunk, extending eden. Thus, each new generation now \nconsists of a from-space, to-space, and a linked list of eden chunks. The last chunk is called current \neden,and new E -chunk 1 object allocations are performed there. The ability to grow eden may postpone \nthe need for a scavenge, letting more objects become garbage before a collection takes place. Figure \n5 explains this: a full initial eden chunk is extended, avoiding a scavenge (top of the figure). When \nthe second chunk is full (middle), a third one is given to the task, further postponing the collection. \nFinally, the third chunk fills up and NSM either does not have more chunks or the task s surplus memory \npriority does not allow further extensions. The scavenge is necessary (bottom). Delaying a collection \nuntil then avoids the copying of objects that have been live after the first or second extension but \ndied before the scavenge, and avoids copying some non-tenured objects back and forth between from-space \nand to-space, because the scavenge did not take place twice. The age of objects is adjusted accordingly \n(e.g. in our example the ones in from-space will age by 3, the ones in the first chunk by 2, etc), but \nonly during the actual copying of objects out of eden or out of from\u00adspace. Extending eden ages objects \nimplicitly, based on what chunk they are in. Because the cost of scavenging depends on the number of \nlive objects and not on the eden size, this scheme can improve the performance by decreasing the total \ntime spent in scavenging. It can also increase the times of those scavenges that actually take place. \nAlso, since this scheme spreads the objects to be scavenged over a larger memory area, different locality \nproperties with respect to caches hold than in the original scheme, which can degrade performance. For \nthese reasons a task can opt out of this scheme. 4.2 Memory Accounting The memory usage counter of a \ntask keeps track of the amount of the old generation occupied by the objects of this task. It also includes \nthe sizes of from-space, to-space, and the initial eden chunk of the task, regardless of how many objects \nare in the new generation, because these spaces are always reserved for the task. The value of the counter \nis incremented only during large array allocations or upon promotion of young objects to the old generation \nduring a scavenge, since only then does the volume of the task s data in the old generation increase. \nIt is decremented during global collections, to reflect the actual space taken by lived tenured objects \nof that task. The sizes of eden extensions introduced by surplus memory management are not included in \nthe usage counter because they are temporary and meant to improve performance when there is no contention \nfor memory. MVM isolation properties guarantee that data sets of different tasks are disjoint. Thus, \nthe collection roots of different tasks are disjoint too. The garbage collector is restructured to take \nadvantage of this property. During global collections the marking starts from all the roots of the first \ntask, then the second task s objects are marked, and so on. It is thus inexpensive to determine how much \nlive data the task has after the collection. New generation collections do not need any additional re-ordering \n whenever an object is tenured, it is known which task s new generation is collected, and the appropriate \ncounter is incremented. No accounting takes place at object allocation time (Figure 6), since the size \nof the new generation is already included in the value of the usage counter. Whenever the usage counter \nis incremented, it is compared against the task s heap memory limit. An old generation collection is \nstarted when this increase makes the usage counter exceed the  Figure 7. Execution time as a function \nof an eden size in MVM. overheads of replicated new generations are virtually zero (they do not record \nin our measurements). This is so since only one indirection, to fetch a reference to the task s new generation, \nis added to object allocations. Second, the overheads introduced by memory accounting do not exceed 0.5% \nfor any benchmark. The overheads are proportional to the number of promoted objects each promotion includes \nadding the already computed (HSVM) object size to a per-task counter and then comparing the new value \nwith a limit. Most objects get promoted in javac (almost two million for 2MB of eden space) and the overheads \nincurred in this case are 0.5%. Many fewer objects are promoted in other benchmarks from a few thousand \n(compress, mpeg) to about half a million (jack), which explains why the overheads are negligible. Finally, \nthe costs of extending eden and associated aging-related bookkeeping during promotions are also low. \nOverall, total memory-related overheads do not exceed 1% of execution time. To quantify the benefits \nof surplus memory management, all benchmarks were run with an MVM eden chunk size equal to 512KB (larger \nsizes yield similar results; smaller are impractical). It is also the initial size of eden. The parameter \nof the experiment was the number of chunks the application can use. The maximum number of chunks was \nfixed for the whole run of the benchmark, so effectively the union of all eden chunks formed eden, managed \nby NSM. The execution times are presented in Figure 7 (mtrt is omitted, since it is similar to raytrace; \ncompress is omitted, since the results were very similar to mpegaudio). Each benchmark was run in MVM, \nand the baseline for comparisons was the run with one, initial chunk only, but including all the costs \nof memory accounting. The unsurprising conclusion is that the execution time can benefit from larger \nedens. The gains can be as high as 4% (javac, jess), but, as surplus memory insensitive mpeg shows, they \nmay just not exist. A task can get more eden space for only a portion of its execution time, and still \nbenefit from it, although less than if the eden was permanently extended. Overall, the scheme is simple \nand inexpensive, and may bring about a few per-cent performance improvements when there is surplus memory \navailable. In view of the fact that HSVM is very well tuned, an ability to gain even 2% improvement is \nattractive. 5 USER-LEVEL NATIVE CODE The coexistence of programs written in a safe language with user\u00adsupplied, \nunsafe (native) code is convenient (it enables direct access to hardware and OS resources and can improve \napplication performance). But the inherent lack of memory safety in native code may break the contract \noffered by a safe language. In the case of a single application executing in the JVM, a bug in an application \n(user-level) native library will disrupt or abnormally terminate this particular application only. The \nconsequence of an errant native library carelessly loaded into MVM can be much more serious. In addition \nto causing malfunctioning of the loading application, such a library may corrupt the data of other applications \nor crash the whole virtual machine. Worse yet, an uncontrolled malicious native library may read the \ndata of other applications, perhaps leaking out privileged information. Several techniques for ensuring \nmemory safety have been proposed, such as augmenting native code with safety-enforcing software checks \n[29], statically analyzing it and proving it safe [21], or designing a low-level, statically typed target \nlanguage to compile native code to [20]. Although these approaches have their success stories, and at \nthe current state of the art they are practical in many circumstances, their usefulness for addressing \nproblems with an arbitrary native library is rather limited. In particular, ensuring memory safety with \nthese techniques requires source code of native libraries [20], generating safety proofs [21], which \nis impossible in general, or may incur non-negligible performance penalties [29]. Memory safety is not \nthe only issue, though. Guaranteeing the safe use of system resources by the JVM and native code is equally \nimportant. Native code is written against two interfaces: the Java Native Interface (JNI) [17], which \nis its sole interaction with the JVM and the application, and the host OS interfaces, involving the usual \nlibraries for I/O, threading, math, networking, etc. The latter is also the interface against which the \nJVM is written, and therein lies a problem. The JVM makes certain decisions regarding the use of the \nhost OS interface and of available resources. Examples include the following: signal handlers may need \nto be instantiated to handle exceptions that are part of the operation of the JVM (e.g., to detect memory \naccess and arithmetic exceptions, etc.); the JVM must choose a memory management regime for its own purposes, \nsuch as the allocation of thread stacks and red zones; threads accessible in the language are typically \nmapped onto the underlying system's threading mechanism; the JVM adopts a convention to suspend threads \nfor garbage collection; the JVM decides how to manage I/O (e.g., using blocking or non-blocking calls). \nFew, if any, of these mechanisms are composable, in the sense that it is not possible to take an arbitrary \nJava program and a user\u00adsupplied native library, put them together into one virtual machine, and expect \nthe resulting system to work correctly. The implicit conventions in which the JVM uses system resources \nare rarely documented, are highly dependent on the implementation decisions, and are usually thought \nof as private to the JVM. Thus, while programmers may have very legitimate reasons, for instance, to \ncustomize signal handling in a native method, doing this may interfere with the JVM, depending on unknown \nimplementation details. In MVM user-supplied native code is executed in a separate process. Each task \nthat needs this and has the necessary permissions has one such process. This means that the only interface \nbetween the JVM and native libraries becomes JNI. There is then no implicit contract concerning memory \nmanagement, threading, signal handling, and other issues. This solves the composability problem neatly. \nThe native code in a separate process has full control of its own resources. There are no unexpected \ninteractions with MVM via memory, signals, threads, and so on. The challenges were to make this execution \nof native code in a separate process transparent to the native libraries and to tasks. We also wanted \nto avoid any modifications to the JVM, so that our design can be easily reused in contexts other than \nmultitasking. 5.1 JNI Essentials JNI interacts with the JVM via downcalls (when a Java application calls \na native method) and upcalls (when a native method calls up to the JVM). Upcalls enable accessing static \nand instance fields and array elements, invoking methods, entering and exiting monitors, creating new \nobjects, using reflection, and throwing and catching exceptions. Downcalls result in calls to C or C++ \nfunctions, the names of which are generated, according to a known convention, from the names of Java \nmethods declared as native. Upcalls are invoked via a JNI environment interface, a pointer to which is \nalways passed as the first argument to all JNI upcalls and downcalls. Objects, classes, fields, and methods \nare never accessed directly, but via appropriate opaque references or identifiers. These references are \nmeaningful only to the JNI functions, and shield native code from the details of particular implementations \nof JNI. 5.2 The Design of Native Code Isolation Native code isolation works by interposing an isolation \nlayer between the JVM and native method libraries such that native methods are transparently executed \nin a process different from the process executing the JVM. For simplicity of exposition, let us call \nthe former n-process, and the latter the j-process.Figure 8 gives a high-level view of the interposition \nmechanisms. The interposition layer maintains one n-process per task actually issuing native method calls, \nand directs all native calls issued from a task to its corresponding n-process. Let us assume a simple \nscenario where one task performs calls to one native library, called hereafter l-orig. A native library \nwith the same name and exported symbols as l\u00adorig is produced so that it can be loaded by the Java application \nexecuting in j-process. Let us call this library l-proxy.Itis Figure 8. Various conflicts are possible \nbetween user\u00adsupplied native code and the JVM (left). In MVM, the native code isolation scheme prevents \nthem (right). generated through an automated analysis of the symbol table of l\u00adorig. All extracted function \nnames that comply with the downcall naming convention are used to generate a source file in which these \nfunctions are redefined to ship their arguments, along with an integer uniquely identifying the function, \nto n-process. Upon receipt of such a message, n-process executes the requested function with the supplied \narguments. Prior to that, n-process replaces the first word in the list of received arguments with its \nown version of the JNI environment pointer. This custom JNI environment redefines all JNI functions so \nthat each of them ships all of its arguments along with its unique identifier back to j\u00adprocess, where \nthe upcall is dispatched to the original JNI method. Generating proxy libraries and replacing the original \nJNI environment pointer with a custom one are two main techniques making the resulting interposition \nmechanism transparent to the JVM and to native libraries. Implementing the upcalls in the custom JNI \nenvironment interface is straightforward for those taking a fixed, known number of arguments of primitive \nor opaque reference or identifier types. Invoking methods or constructors is handled by JNI upcalls that \nexpect a variable number of arguments. Each such upcall has three forms: (i) using the  construct of \nthe C programming language, (ii) expecting arguments as a variable-length list (va_list), and (iii) expecting \na vector of unions of a JNI\u00addefined type, each of which holds one argument for the upcall. Computing \nthe number of the upcall s arguments at call time by analyzing the current stack call frame is possible \nonly in the first case. An elegant, portable, and uniform solution applicable to all three cases takes \nadvantage of the following fact. Before invoking a Java method via a JNI upcall, the method identifier \nhas to be obtained first. This is done by calling a JNI upcall (e.g. GetMethodID) and supplying it with \nthe signature of the Java method or constructor to be executed by the upcall. The number and size of \narguments is obtained by analyzing the signature after successfully obtaining the method identifier. \nThe original method identifier and the computed argument information are then cached by n-process for \nlater use. It is important to ensure that upcalls be handled in the context of the thread that originally \nissued the downcall. For instance, an exception thrown in an upcall has to be dispatched to the thread \nthat caused the downcall; requests to obtain a stack trace should look identical to those generate by \ntraditional JNI systems; monitor acquisitions by native code should be done in the context of the same \nthread that issued the downcall, or synchronization between native and Java code may be impossible. How \nthis is guaranteed depends on the inter-process communication chosen. Details on this aspect of native \nmethod isolation are extensively discussed in [7]. 5.2.1 Portability The implementation of native code \nisolation is highly portable across different hardware/OS platforms. There are two places, which require \nplatform-specific code: (i) determining how many arguments are passed to a downcall in j-process, and \n(ii) returning the correct value after executing the original native method in n\u00adprocess. The reason \nis that in general neither the number of arguments nor the type of return value of a native method can \nbe inferred from the native library. On our platform, the first issue is handled by analyzing the stack \nframe of a downcall. The second is dealt with by obtaining, after executing a downcall in n-process, \nthe contents of both registers (%o0 and %of) that may contain the return value of the downcall. These \nvalues are then restored in j\u00adprocess, after a return from the proxy call. Since the calling convention \nrequires spilling of these two registers before function calls, this approach is correct.  5.3 Performance \nA key factor for the performance of native method isolation is the inter-process communication mechanism \nchosen between the JVM process and the native method servers. Our implementation uses doors [13], a fast \ninter-process communication mechanism available on the Solaris Operating Environment. Doors achieve low \nlatency by transferring control back and forth between the caller's and the callee's threads directly, \nwithout passing by the scheduler. Table 1 summarizes the overheads. The downcall column reports the cost \nof a trivial downcall (no arguments, immediate return), while downcall + upcall additionally issues the \nGetVersion upcall, which returns immediately with an integer specifying the JNI version. The 54\u00b5s overhead \nof a downcall breaks down as follows: 18.5\u00b5s is taken by a door call and return, 30.8\u00b5sisthe cost of \ntwo ucontext swaps and the rest (4.7\u00b5s) are various bookkeeping and data copying overheads. Similar analysis \napplies to the overheads associated with upcalls. This number would be much higher if sockets were used \n a one-word round\u00adtrip message takes about 128\u00b5s. The overheads (about 400 times higher than the plain \nin-proc JNI downcall) seem very large. And they are large indeed, for downcalls that do not compute much. \nTo see how quickly the overheads become tolerable, let us analyze a simple program that performs A=A2 \nfor a floating-point matrix A. The squaring requires one downcall (to initiate the native call) and two \nupcalls (to fetch the matrix entries and to set the computed result). Figure 9 summarizes the overheads \nas a function of array size (e.g. size 25 means a 25x25 array). For small arrays the overheads are prohibitive. \nFor 40x40 arrays they become tolerable, and virtually disappear when the array size approaches 90. These \nnumbers are no substitute for more realistic benchmarks, but serve as a useful estimate of the costs \nof this approach. Overall, the overheads depend on the frequency of native calls issued by an application. \nThe proposed scheme enables safe, reliable, and interference-free composition of native libraries and \nthe MVM runtime. No changes to the MVM were necessary as the infrastructure is transparent to and independent \nof the implementation of the JVM, its vendor or version. The transparency and automation of the presented \ntechnique are major improvements over such designs Microsoft s Common Object Model (COM) [22], where \ncomponents can execute in the same process or in a separate process. This is accomplished with the help \nof an interface definition language (IDL), used to describe data passed into and JNI in-proc JNI out-of-proc \nDowncall 0.136.s 54 .s Downcall + upcall 0.163.s 106.s Table 1. Overheads of executing native code in \na separate process. out of out-of-proc components. The utility of our approach reaches beyond MVM [7]. \nFor instance, we are using it for mixed-mode debugging, where a C/C++ debugger (e.g. gdb) executes n-process \nand the Java code is debugged with a Java debugger (e.g. Forte for Java [27]).  6 CONCLUSIONS MVM is \na complete system, where any existing Java application can execute and can use all abstractions, mechanisms, \nand standard libraries of the JVM. Known sources of inter-task interference, present in the JVM, are \nremoved in MVM. The safety, scalability and uncompromising approach to offering each feature of the language \nmake MVM an attractive platform for environments that require the execution of numerous programs written \nin the Java programming language, such as, application servers, extensible web servers, or even user \ndesktop environments. Special attention has been paid to three issues. First, sharing the native code \nthat results from runtime compilation of methods of both core and application classes. This is crucial \nfor performance, since it amortizes compilation costs across all tasks, and scalability, since it increases \nthe shareable portion of a program memory footprint. Second, heap memory, a resource notorious for unaccountability \nand irrevocability in the JVM, is subject to explicit guarantees and is inexpensively accounted for. \nMoreover, surplus memory is transparently given to task whenever available, to improve overall performance. \nThird, native libraries a feature typically forbidden in safe language multitasking environments can \nbe safely used by tasks, and transparently execute in a separate OS process, to prevent any interference \nwith MVM. The code base for implementing MVM is the Java HotSpot virtual machine. The overheads introduced \nby multitasking rapidly disappear or, in many cases, are replaced by significant performance gains, as \nrepeated executions of methods enjoy accumulated compilation effort. This paper is an initial report \non MVM. The system has just become operational, and more study is needed (and planned!) to evaluate various \naspects of its performance and functionality, especially under very large workloads such as those incurred \nby application servers. 7 ACKNOWLEDGEMENTS The authors are grateful to Godmar Back, Dave Dice, Robert \nGriesemer, Wilson Hsieh, Mick Jordan, Hideya Kawahara, Peter Kessler, Doug Lea, Tim Lindholm, Nate Nystrom, \nFred Oliver, Glenn Skinner, Pete Soper, Ricky Robinson, Pat Tullman, Dave Ungar, and Mario Wolczko for \ntheir comments, suggestions and help. 8 TRADEMARKS Sun, Sun Microsystems, Inc., Java, JVM, Enterprise \nJavaBeans, HotSpot, and Solaris are trademarks or registered trademarks of Sun Microsystems, Inc., in \nthe United States and other countries. SPARC and UltraSPARC are a trademarks or registered trademarks \nof SPARC International, Inc. in the United States and other countries. UNIX is a registered trademark \nin the United States and other countries, exclusively licensed through X/Open Company, Ltd. 9 REFERENCES \n[1] Arnold, K., and Gosling, J. The Java Programming Language. 2nd Edition. Addison-Wesley, 1998. [2]Back,G., \nHsieh, W.,and Lepreau,J.Processes in KaffeOS: Isolation, Resource Management, and Sharing in Java. 4th \nSymposium on Operating Systems Design and Implementation, San Diego, CA, 2000. [3] Balfanz, D., and Gong, \nL. Experience with Secure Multi-Processing in Java. Technical Report 560-97, Department of Computer Science, \nPrinceton University, September, 1997. [4] Bryce, C. and Vitek, J. The JavaSeal Mobile Agent 3rd Kernel. \nInternational Symposium on Mobile Agents, Palm Springs, CA, October 1999. [5] Bryce, C. and Razafimahefa \nC. An Approach to Safe Object Sharing. ACM OOPSLA 00, Minneapolis, MN, October 2000. [6] Czajkowski, \nG. Application Isolation in the Java Virtual Machine. ACM OOPSLA'00, Minneapolis, MN, October 2000. [7] \nCzajkowski, G., and Dayn\u00e8s, L., and Wolczko, M. Automated and Portable Native Code Isolation. Sun Microsystems \nLaboratories Technical Report, TR-01\u00ad96, April 2001. [8] Czajkowski, G., and von Eicken, T. JRes A Resource \nAccounting Interface for Java. ACM OOPSLA 98, Vancouver, BC, October 1998. [9] Dillenberger, W., Bordwekar, \nR., Clark, C., Durand, D., Emmes, D.,Gohda,O., Howard,S., Oliver,M., Samuel, F., and St. John, R. Building \na Java virtual machine for server applications: The JVM on OS/390. IBM Systems Journal, Vol. 39, No 1, \n2000. [10] Gong, Li. Inside Java 2 Platform Security. Addison Wesley, 1999. [11] Gosling, J., Joy, B., \nSteele, G. and Bracha, G The Java Language Specification. 2nd Edition. Addison-Wesley, 2000. [12] Hawblitzel, \nC., Chang, C-C., Czajkowski, G., Hu, D. and von Eicken, T. Implementing Multiple Protection Domains in \nJava. USENIX Annual Conference, New Orleans, LA, June 1998. [13] Hamilton, G., and Kougiouris. The Spring \nNucleus: a Microkernel for Objects. Summer USENIX Conference, June 1993. [14] Hudson, R., and Moss, E. \nIncremental Collection of Mature Objects. International Workshop on Memory Management, September 1992. \n[15] Java Community Process. JSR-121: Application Isolation API Specification. jcp.org/jsr/detail/121.jsp. \n[16] Liang S., and Bracha, G. Dynamic Class Loading in the Java Virtual Machine. ACM OOPSLA'98, Vancouver, \nBC, Canada, October 1998. [17] Liang, S. The Java Native Interface. Addison-Wesley, June 1999. [18] Linholm, \nT., and Yellin, F. The Java Virtual Machine Specification. 1st Ed. Addison-Wesley, 1996. Also: java.sun.com/docs/books/vmspec. \nDiscusses bytecode quickening. [19] Lindholm, T., and Yellin, F.. The Java Virtual Machine Specification. \n2nd Ed. Addison-Wesley, 1999. [20] Morrisett, G.,Crary,K., Glew,N., Grossman,D., Samuels, R.,Smith,F., \nWalker,D., Weirich, S.,and Zdancewic, S. TALx86: A Realistic Typed Assembly Language. ACM SIGPLAN Workshop \non Compiler Support for System Software, Atlanta, GA, May 1999. [21] Necula, G., and Lee, P. Safe Kernel \nExtensions 2nd without RuntimeChecking. Symposium on Operating Systems Design and Implementation, Seattle, \nWA 1996. [22] Rogerson. D. Inside COM. Microsoft Press, 1997. [23] Rudys, A., Clements, J., and Wallach, \nD. Termination in Language-based Systems. Network and Distributed Systems Security Symposium, San Diego, \nCA, February 2001. [24] Suri, N., Bradshaw, J., Breedy, M., Groth, P., Hill, G., Jeffers, R., and Mitrovich, \nT. An Overview of the 2nd NOMADS Mobile Agent System. International Symposium on Agent Systems and Applications, \nASA/MA2000, Zurich, Switzerland, September 2000. [25] Standard Performance Evaluation Corporation. SPEC \nJava Virtual Machine Benchmark Suite. August 1998. http://www.spec.org/osg/jvm98. [26] Sun Microsystems, \nInc. Java HotSpot Technology. http://java.sun.com/products/hotspot. [27] Sun Microsystems, Inc. Forte \nTools: Forte for Java . http://www.sun.com/forte/ffj. [28] Ungar. D. Generational Scavenging: A Non-Disruptive \nHigh Performance Storage Reclamation Algorithm. ACM SIGPLAN Notices, 19(5), April 1984. [29] Wahbe, R., \nLucco, S., Anderson, T., and Graham, S. Efficient Software Fault Isolation. 14th ACM Symposium on Operating \nSystems Principles, Asheville, NC, December 1993. [30] Weaver, D., and Germond, T. The Sparc Architecture \nManual Version 9. Prentice Hall, 1994.    \n\t\t\t", "proc_id": "504282", "abstract": "The multitasking virtual machine (called from now on simply MVM) is a modification of the Java virtual machine. It enables safe, secure, and scalable multitasking. Safety is achieved by strict isolation of application from one another. Resource control augment security by preventing some denial-of-service attacks. Improved scalability results from an aggressive application of the main design principle of MVM: share as much of the runtime as possible among applications and replicate everything else. The system can be described as a 'no compromise'approach --- all the known APIs and mechanisms of the Java programming language are available to applications. MVM is implemented as a series of carefully tuned modifications to the Java HotSpot virtual machine, including the dynamic compiler. this paper presents the design of MVM, focusing on several novel and general techniques: an in-runtime design of lightweight isolation, an extension of a copying, generational garbage collector to provide best-effort management of a portion of the heap space, and a transparent and automated mechanism for safe execution of user-level native code. MVM demonstrates that multitasking in a safe language can be accomplished with a high degree of protection, without constraining the language, and and with competitive performance characteristics", "authors": [{"name": "Grzegorz Czajkowski", "author_profile_id": "81100400752", "affiliation": "Sun Microsystems Laboratories, 2600 Casey Avenue, Mountain View, CA, ", "person_id": "P100088", "email_address": "", "orcid_id": ""}, {"name": "Laurent Dayn&#233;s", "author_profile_id": "81100320237", "affiliation": "Sun Microsystems Laboratories, 2600 Casey Avenue, Mountain View, CA, ", "person_id": "P344303", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/504282.504292", "year": "2001", "article_id": "504292", "conference": "OOPSLA", "title": "Multitasking without comprimise: a virtual machine evolution", "url": "http://dl.acm.org/citation.cfm?id=504292"}