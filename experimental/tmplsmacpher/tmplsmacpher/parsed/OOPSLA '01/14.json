{"article_publication_date": "10-01-2001", "fulltext": "\n Dynamic Optimistic lnterprocedurai Analysis: a Framework and an Application igor Pechtchanski Vivek \nSarkar Department of Computer Science IBM Research New York University T. J. Watson Research Center \npechtcha@cs, nyu. edu vsarkar@us, ibm. com ABSTRACT In this paper, we address the problem of dynamic \noptimistic interprocedural analysis. Our goal is to build on past work on static interprocedural analysis \nand dynamic optimization by combining their advantages. We present a framework for performing dynamic \noptimistic interprocedurai analysis. The framework is designed to be used in the context of dynamic class \nloading and dynamic compilation, and in- dudes mechanisms for event notification (on class loading and \nmethod compilation) and dependence tracking (to en- able invalidation of optimistic assumptions). We \nillustrate the functionality of the framework by using it to implement a dynamic optimistic interprocedural \ntype (DOIT) analysis algorithm. The DOIT algorithm uses a new global data structure called the Value \nGraph. The framework and DOIT analysis are implemented as part of the IBM Jalapefio Java Virtual Machine. \nOur experi- mental results for the SPECjvm benchmarks and two larger programs show promising benefits \ndue to dynamic optimistic analysis. Compared to pessimistic analysis, the reduction in the number of \nmethods and fields analyzed was in the 2.7\u00d7~4.6\u00d7 and 1.6\u00d7-2.4\u00d7 ranges respectively. The average fraction \nof polymorphic virtual calls decreased from 39.5% to 24.4% due to optimistic analysis, with a best-case \ndecrease from 47.0% to 8.1%. The average fraction ofpolymorphic in-terface calls decreased from 96.4% \nto 36.2% due to optimistic analysis, with a best-case decrease from 100.0% to 0.0%. These benefits were \nobtained with a low dynamic analysis overhead in the range of 570-930 bytecode bytes/millisecond (about \n2.5\u00d7-5.4\u00d7 faster than the Jalapefio baseline com- piler). 1. INTRODUCTION Program analysis techniques \nare widely used in modern pro- gramming systems to help achieve one of two goals ~ im-proved performance \nvia code optimization or improved ~ro- Permission to make digital or hard copies of all or part of this \nwork [br personal or classroom use is granted without lee provided that copies arc not made or distributed \nfor profit or commercial advantage and that copies bear this notice and the full citation on the first \npage. To copy otherwise, lo republish, to post on servers or to redistribute to lists, requires prior \nspecific permission and/or a fee. OOPSLA Ol ]'ampa Florida USA Copyright ACM 2001 1-58113-335 -9/01/10...$5,00 \n9rammer productivity via programming tools. The bulk of past work has been on program analysis techniques \nthat are static (performed on a specified set of classes/modules prior to program execution) and pessimistic \n(conservatively assuming that all possible execution paths are feasible). De- spite their limitations, \nmany static and pessimistic analyses have proven to be effective in revealing useful control and data \nflow properties of procedural (non-00) programs for exploitation by code optimizers and programming tools. \nCompared to procedural programs, OO programs present additional challenges for program analyses due to \na num- ber of reasons. These challenges impact both the precision and the efficiency of analyses. First, \nthe increased presence of indirect calls (virtual/interface calls) in OO programs greatly increases the \nnumber of execution paths that are assumed to be feasible. Second, the possibility of dynamic class loading \nin OO languages such as Java [14] makes it impossible to even bound the number of feasible execution \npaths in a static analysis. Third, the widespread usage of large shared libraries in OO programs greatly \nincreases the scope and overhead of whole program analysis, even though any single application typically \nuses only a small fraction of the functionality present in a shared library. Some of these challenges \nhave been addressed in recent work on static interprocedural analysis (IPA) of OO programs. There have \nbeen a number of algorithms developed for im- proving the precision of interprocedural call graph construc- \ntion by combining type analysis with analysis of indirect calls [15, 23, 29]. However, a key limitation \nof static analysis is that it is performed on a conservative approximation of the whole program, which \nis usually much larger than the set of methods invoked in a single execution. As a result, static analysis \ntechniques are forced to be pessimistic and they also incur large compile-time overheads. Another approach \nto addressing the challenges posed by O0 programs is to use dynamic optimization instead of, or in conjunction \nwith, static analysis. In contrast with static analysis, dynamic optimization is performed at runtime, \nand thus a common characteristic of past dynamic analysis ~ech- niques is that they are limited in scope \nbecause they axe usually performed on a single method (or a small number of methods) at a time. Early \nforms of dynamic optimization include customization [8] of methods and the addition of guards (runtime \ntests) [16] to enable optimization of code that is specialized to a likely target of an indirect call. \nMore recent approaches enable analyses to be performed dynami-cally (while the program is executing) \nand optimistically (by ignoring feasible execution paths that have not been taken in the current program \nexecution). To ensure correctness, using the results of optimistic analysis requires the ability to invalidate \nassumptions made for a method when neces- sary (e.g, when a new class is loaded or a new method is executed), \nand to recompile the method if needed. If invalidation/recompilation becomes necessary for a method invocation \nthat has not completed, then techniques such as stack rewriting [17, 18] or code patching [20] are neces- \nsary to ensure that the remainder of the method invocation is correctly executed. In preexistence analysis \n[13], stack rewriting is avoided by restricting analysis to preexisting objects, so that invalidation \ndue to dynamic class loading triggers recompilation but does not require stack rewriting. In this paper, \nwe address the problem of dynamic optimistic interprocedural analysis. Our goal is to build on past work \non static interprocedural analysis and dynamic optimization by combining their advantages. The advantage \nof static inter- procedural analysis is that it can examine a whole program. Its limitation is that it \nis pessimistic in its analysis of feasible program paths. The advantage of dynamic optimization is that \nit can make optimistic assumptions which can later be invalidated if necessary. Its key limitation is \nthat it is restricted in scope to a single method (or a small number of methods) at a time, for the sake \nof efficiency. As we will see, our approach enables efficient analysis of a \"dynamic whole program\" consisting \nonly of methods that are actually in- voked in a program execution, rather than an approximation of methods \nthat may be invoked. We present a framework for performing dynamic optimistic interprocedural analyses. \nThe framework is designed to be used in the context of dynamic class loading and dynamic compilation, \nand includes mechanisms for event notification (on class loading and method compilation) and for depen- \ndence tracking mechanisms (to enable invalidation of opti- mistic assumptions). We illustrate the functionality \nof the framework by using it to implement a dynamic optimistic interprocedural type (DOIT) analysis algorithm. \nType anal- ysis has received a lot of attention in past work, because it is key to enabling optimization \nof virtual method calls and elimination of dynamic type checks in O0 programs. Compared to past work, \nthe DOIT approach is unique in its dynamic, incremental collection and use of optimistic interprocedural \nanalysis information. The framework and DOIT analysis are implemented as part of the IBM Jalapefio Java \nVirtual Machine. Our experimen- tal results for the SPECjvm benchmarks and two larger pro- grams show \nsignificant benefits due to dynamic optimistic analysis. Compared to pessimistic analysis, the reduction \nin the number of methods and fields analyzed was in the 2.7x-4.6x and 1.6x-2.4x ranges respectively. \nThe average fraction of polymorphic virtual calls decreased from 39.5% to 24.4~o due to optimistic analysis, \nwith a best-case de-crease from 47.0~ to 8.1% in mtrt. The average fraction of polymorphie interface \ncalls decreased from 96.4% to 36.2% due to optimistic analysis, with a best-ease decrease from 100.0% \nto 0.0% in db. These benefits were obtained with a low dynamic analysis overhead in the range of 570-930 \nbytecode bytes/millisecond (about 2.5x-5.4x faster than the Jaiapefio baseline compiler). The rest of \nthe paper is organized as follows. Section 2 contains a brief problem statement for our work. Section \n3 describes our framework for dynamic optimistic analysis. Section 4 describes a specific application \nof our framework: the DOIT analysis algorithm. Section 5 presents experi- mental results obtained from \na prototype implementation of the framework. Section 6 discusses related work, and Section 7 contains \nour conclusions. Appendix A contains the complete set of abstract interpretation rules used by the DOIT \nanalysis. 2. PROBLEM STATEMENT In this paper, we focus on the problem of dynamic optimistic analysis \nin the context of dynamic compilation and adaptive optimization [4]. Figure 1 shows the timeline for \na typical application execution scenario in this context. JVM events, such as method execution, are shown \nbelow the timeline, and events associated with a dynamic optimistic analysis framework are shown above \nthe timeline. Program execu-tion begins with the initial class being loaded and execution starting with \na designated method (main, in the case of Java). The first execution of any method is unoptimized i.e., \nis performed via interpretation or via execution of un- optimized code. These method executions trigger \nloading of new classes and calls to new methods. After an initial startup period, a frequently executed \nmethod Mi is selected for optimized compilation. The heuristics employed to make this selection vary \nfrom system to system, and usually rely on profiling techniques such as method counters [18] or sta- \ntistical sampling [4]. The key point is that optimistic IPA information is used during this optimized \nrecompilation. If no new methods are invoked after optimized recompilation of M~ and other methods, then \nthe optimistic assumptions will never be violated. However, as illustrated in Figure 2, there may be \nscenarios where it becomes necessary to invali- date optimistic assumptions. After method M~ is optimized, \nit is possible that a new method M, is executed, and that method M~ contains operations that may invalidate \nsome of the IPA information used when optimizing method M~. In this case, it becomes necessary to recompile \nmethod M~ to ensure that all future calls will be correct. In addition, stack- rewriting and online conversion \nto unoptimized code might be necessary to correctly complete the execution of any existing calls to this \nmethod. Invalidation mechanisms have been studied quite extensively in past work on the SELF [9] and \nHotSpot [18] virtual machines; any of those mechanisms can be utilized in our framework. Though invalidation \nand recompilation can be a source of additional runtime over-head, they are (relatively) rare events \nin practice because optimized compilation is usually triggered only when the application enters the \"steady \nstate phase\" [4] after which dynamic class loading occurs at lower frequencies compared to its \"startup \nphase\". Use IPA info for M1 ... Mk Initialize framework Incorporate IPA information for M1 .... Incorporate \nIPA information for Mk when optimizing Ms; also register verification requests for new methods . . . \n. t I I JVM startup First execution of main method MI (unoptimized) .... First execution of method M~ \n(unoptimized) First optimized recompilation of a method (M~) t Figure 1: Application execution in the \ncontext of Dynamic Compilation and Adaptive Optimization Incorporate info for M~; verification of M~ \ntriggers Keeompile M~; also fix up invalidation of optimized any active invocations of .... compilation \nof M~ M~ if necessary ....... I [ I New classes .... First execution t get loaded of method M~ (unoptimized) \n Figure 2: Scenario in which invalidation may be necessary 3. THE ANALYSIS FRAMEWORK Our framework provides \nsupport for verification of arbitrary properties in dynamically loaded code. Different subsystems of \nthe JVM register verification requests with the framework; all pending properties for the method are \nverified on its first execution. It also tracks reverse dependences of each prop- erty by allowing the \nuser to associate an invalidation action (invalidation) with a property, so that if the verification \nof that property of a particular method fails, all invalidation actions for that property of that method \nare performed. Figure 3 shows a schematic of the Method-Action mapping data structure used by our framework. \nThere is a set of actions associated with each method that is yet to be com- piled. Whenever a new method \nis inserted in the structure, it inherits the actions of any methods that it overrides. For convenience, \nthe Method-Action mapping has an ~e pseudo-method which is assumed to be overridden by all methods. The \n\u00a2ltt method stores default actions that should be performed for all methods. Using actions, this framework \ncan naturally serve as an extensible interprocedural analysis driver with four phases that are invoked \nat different stages of program execution: 1. Initialization: this phase initializes the method-action \nmapping and reverse dependence tracking data struc- ture at the start of JVM execution. Once initialization \nis complete, any analysis can register itself with the framework as described in the registration phase \n(see item 2). Various analyses may associate corresponding actions with the root method or set of methods \nduring the initialization phase. 2. Registration: during this phase an analysis can reg- ister an unconditional \naction or a property veriflca-  tion request for a particular method or set of meth- ods. A property \nverification request consists of the actual verification action and any number of reverse dependences \n(as in Figure 3). The reverse dependence list can also be augmented after registration. Each newly loaded \nmethod will inherit all of the actions and property verification requests from the method it overrides. \nRegistration can occur at any point during JVM execu- tion, e.g., initialization, verification or compilation. \nIf, while compiling some method, optimistic assumptions are made about methods that haven't been loaded \nyet, the registration phase is invoked to register a cus- tom property verification request with future \nmeth- ods; invalidation actions are also constructed to undo whatever operations or optimizations resulted \nfrom the optimistic assumptions. 3. Verification: the verification and analysis phase is triggered for \nevery method on its first invocation (which may be interpreted or compiled). All actions (analysis and \nverification) for that method are performed. If any of the verification actions falls, the invalidation \nphase is invoked (see item 4). Verification and analysis procedures may also invoke the registration \nphase (see item 2) to add new properties to other methods, e.g., targets of call sites currently being \nanalyzed. This ability allows the analysis and verification actions to make optimistic assumptions about \nthe properties of these unknown methods. 4. Invalidation: this phase is performed when verifica- tion \nof a new method (on its first invocation) reveals that one or more properties registered with it were \nvio- lated, which means that a previous optimistic assump- tion was violated. During this phase the framework \nit-  ....'\"~ methods{ [~ DeferredVerification~ T:: ]% [ registered actions to ............ [[::7:: \n:::::::::: ::5::::: .................. on i voc ~r./f. ~....... /. ,. ion Ag Figure 3: Method-Actlon \nMapping invalidation action erates over the reverse dependences for the verification action that resulted \nin failure, invoking each invalida- tion action in turn. These actions may trigger recompi- lation of \nthe methods that were optimized using those optimistic assumptions, ensuring that all future calls to \nthese methods will be correct. In addition, stack- rewriting and online conversion to unoptimized code \nmight be necessary to correctly complete the execution of any calls to these methods that are currently \nin progress. Whenever the optimizing compiler uses optimistic informa- tion, it may add new properties \nand invalidation actions, as necessary. This allows optimistic assumptions to be made, with the guarantee \nthat the code that violates them will be detected before it executes, which will result in invalidation. \nOf course, invalidation in this case can also be expensive, so there is a natural tradeoff between using \nless optimistic as- sumptions and risking multiple invalidations. This risk can be minimized by delaying \nthe use of optimistic information until the application reaches a \"steady state\", as in adaptive optimization \n[4]. The possibility of invalidation presupposes that any opti- mizations performed on either code or \ndata should be possi- ble to Undo. Therefore, any optimization that, for example, deletes fields from \nobjects, or performs any sort of data restructuring, should keep enough information around to restore \nthe original state if necessary. This may not always be practical for more aggressive optimizations. \nDiscussing the exact solutions to this problem depends on the specific optimizations, and is beyond the \nscope of this paper. 4. AN APPLICATION: DOIT We now describe a specific application of the framework \nfrom Section 3, namely a new algorithm for Dynamic Op- timistic Interprocedural Type (DOIT) analysis. \nInterproce- dural type analysis has received a lot of attention in past work on compilation of OO programs, \nbecause it is key to enabling optimization of virtual method calls and elimina- tion of dynamic type \nchecks. Past work on class-hierarchy analysis (e.g., [11, 12]) and call graph construction (e.g., [22, \n15]) included analysis of types and fields. Attention has also been paid to performing these analyses \nefficiently e.g., by using Rapid Type Analysis (RTA) [5] and fast algorithms for call graph construction \n[22, 29, 25]. As we will see, the major difference between the DOIT algorithm and past work on type analysis \nlies in its efficiency and its integration within a framework for dynamic optimistic analysis. The DOIT \nalgorithm has the following characteristics. It is dynamic i.e., it only analyzes methods invoked in \nthe current execution. It is optimistic because it uses a framework that supports invalidation of prior \nassumptions. It is interproce-dural because it analyzes all methods in the \"dynamic whole program\". The \nresults of interprocedural analysis are stored incrementally in the Value Graph (defined in Section 4.1), \na global data structure that is usually smaller in size than a call graph. Since it is important to keep \nthe overhead of dynamic analysis as low as possible, the DOIT analysis was designed to be performed in \na single flow-insensitive pass over the input bytecodes of each method. The DOIT algorithm consists of \nthree stages that are in- voked at different execution phases of the framework de- scribed in Section \n3: 1. Initialization: this stage is performed at the start of JVM execution, after the framework initialization \nphase. It initializes the Value Graph and other DOIT data structures, and registers DOIT as an action \nthat must be performed for each method on its first execu- tion. 2. Analysis and verification: the DOIT \nanalysis and verification stages axe performed as part of the verifica- tion phase in the framework. \nDOIT analysis is enabled for every method on its first execution. It is performed as a linear pass through \nthe input bytecodes of the method being analyzed. The results of the analysis stage axe stored incrementally \nas additional nodes and edges in the Value Graph. Verification is enabled for every new method executed \nafter a verification property has been registered. As described below in item 3, verification properties \naxe registered when DOIT analysis information is used for optimization. 3. Optimization: the optimization \nstage is performed for every method that is selected by the JVM for op- timized compilation. This stage \nis invoked by the op- timizing compiler, and consists of a) a demand-driven  traversed of the Value \nGraph to obtain optimistic re- fined type information for fields, b) use of refined type information \nfor enhanced optimization, c) registration of a verification property to ensure that the type infor- \nmation used for code optimization is not violated by new methods, and d) registration of an invalidation \nac- tion to be performed if the verification in c) fails. The exact invalidation procedure depends on \nhow much information was used in optimization, and how aggres- sive the optimization was. The following \nsubsections give a detailed description of the Value Graph data structure and the Analysis and Optimiza- \ntion stages of the DOIT algorithm. 4.1 Value Graph The DOIT algorithm stores the results of type analysis \nin a global Value Graph. Each node n in a Value Graph denotes a set of types and closure types, \u00a3(n). \nValue Graph nodes are classified as location nodes and operator nodes. Directed edges between nodes denote \nrelationships between type sets, as described below. We define pred(n) to be the Value Graph predecessors \nof node n, and unionpred(n) to be the union of the type sets of n's predecessors i.e., unionpred(n) = \n(J c(no) naE~red(n) Location nodes: A location node represents an abstract location [7] in the program \nthat can hold an object reference. It denotes the set of types that can be stored in the location. There \nare four kinds of location nodes: \u00ae Local --a local node represents a local variable in a method. Different \nmethods have distinct local nodes in the value graph. After analysis of a method is completed, all its \nlocal nodes can be eliminated by performing a simplifica- tion of the Value Graph based on transitive \nclosure. There is an engineering tradeoff involved in deciding when this simplification is worthwhile. \n(Performing the simplification reduces the number of nodes in the Value Graph, but may increase the number \nof edges and may be too expensive for its benefits to outweigh its overheads.) * Field --a field node \nrepresents a specific static or instance field which has a reference declared type. For instance fields, \nall dynamic instances of the same field are represented by a single node, although they could be further \nrefined by a separate alias analysis. Field nodes are shared interproceduraUy. Element --for each local \nnode or field node that has an array type, a number of element nodes are created to represent the array \naccess operations that can be performed on the abstract location. One element node is created per array \ndimension; thus, k element nodes are created for each location that has a declared type of a k-dimensional \narray. If the elements of the array are of a primitive or final type, the element nodes are redundant \nand need not be created. We use \u00a3 as a notation for the single-level \"element-of\" mapping that relates \nthese nodes. For example, if a field f is a two,dimensional array of objects of type T1, then a field \nnode, nf, is created to track the types stored in field f. In addition, an element node, E(nf), is created \nto to track the types stored in f[*], and a second element node, E(E(nf)) (or ~2(nf)), is created to \nto track the types stored in f[*][*]. Element nodes are shared interprocedurally. Constant --a constant \nnode denotes a constant, or exact, type. Since the abstract location represented by a constant node can \nnever be modified, constant nodes are always source nodes in the Value Graph --they have no incoming \nedges. Constant nodes are shared inte~procedurally. Operator nodes: An operator node represents an oper- \nation that can be performed on type sets. There are four kinds of operator nodes in the Value Graph: \n* Closure --a closure node denotes the set of closures of the types in its predecessor type sets. The \nclosure of a type T, written as T*, is defined to be the set of subclasses of T in the class hierarchy \n(including T itself). Note that (T*)* =T*. * Subscript --a subscript node denotes the set of sub- script \ntypes for the array types in its predecessor type sets. We define E(T1) to be the element type of T1 \nif T1 is an array type, and the empty set ~ otherwise. An array type in DOIT analysis is represented \neither as an array of an exact type (e.g., T1 =- T2[]) or as a closure of an array type (e.g., TI = (T2[])*). \nIn either case, the element type is a closure type i.e., ~(T1) -T*  Union --a union node denotes the \nunion of the type sets of all its predecessors. Union nodes are used to ag- gregate multiple types that \nmay be stored in a location node.  Intersection --an intersection node denotes the in- tersection of \nthe type sets of all its predecessors. In-tersection nodes are used to represent explicit con- stralnts \narising from CHECK-CAST and STORE-CHECK operations, as well as implicit constraints arising from type \ndeclarations.  0 iconst_l 1 anewarray Object[] 4 putstatic A.a ? getstatic A.a 10 dup 11 dup 12 ifnonnull \n21 15 new T1 18 goto 27 21 new T2 24 dup 25 astore k 27 iconst_0 28 swap 29 aastore 30 iconst _0 31 aaload \n32 astore k 34 aload k 36 checkcast T2 39 putstatic B.b    [Bbl Figure 4: Value Graph for a bytecode \nsequence We can now define the type set denoted by each node n in a Value Graph recursively as follows: \n{T } n is a constant node with type T unionpred(n) n is a local, field, or element node ~.(n) : unionpred(n) \nn is a union node Nn~epred(n) ff.(na) n is an intersection node ~JTeunionpr~d(n){ T* } n is a closure \nnode ~JTe~,io~,~d(,~){ E(T) } n is a subscript node The goal of our type analysis is to compute the type \nset \u00a3(ne) for each field, local, or element location, \u00a3, of interest to the optimizing compiler. This \ncan be easily accomplished by a demand-driven reverse DFS (depth-first search) traver-sal of the Value \nGraph starting from the desired location node, hi.  Object[] l ,Object[] Object[] i Figure 5: A reverse-DFS \ntraversal 4.2 Example Figure 4 shows an example sequence of bytecodes and its Value Graph (location \nnodes and operator nodes are rep- resented by solid rectangles and circles respectively). The declared \ntypes of fields it. a and B. b are assumed to be Object[] and T2 respectively. Bytecode 4 assigns a new \narray of Object to static field h. a; this assignment is represented by the edge from the constant node \nfor Object[] to the field node for h.a. Since field A. a is declared to be of type Object, all assignments \nto it. a are gated by an implicit intersection with Object*. Similarly, all assignments to B.b axe gated \nby an (implicit) intersection with T~ since B.b is declared to be of type T2. These implicit intersections \nare often no-ops, but they may be necessary when the source of an assignment has an ambiguous type, e.g., \nan untyped local. Bytecode 25 assigns a new object of type T2 to local k; this assignment is represented \nby the edge from the constant node for T2 to the local node for k. Bytecode 28 stores an object of type \nT1 or T2 as an element of the array stored in field h.a. This assignment is represented by the input \nedge to C(A.a), the element node for b.a. A union node is used to combine T~ and T2 as possible element \ntypes, and az~ intersection with the element type of A. a models the STORE-CHECK operation that is normally \nperformed when storing into reference array elements. Bytecode 39 stores the value of k in field B .b, \nafter a CHECK- CAST operation is performed for type T2. Note that there are two intersection nodes gating \nassignments to B .b. The lower node is an implicit intersection because B. b is declared to be of type \nT2. The upper node models the CHECK-CAST operation. This example illustrates that a Value Graph may contain \nredundant operations e.g., the intersection with Object* can be eliminated, or the two intersections \nwith T~ at the bot- tom can be replaced by a single intersection. If we can tell when a node is being \ncreated that it must be redundant, then we avoid adding the node to the Value Graph. Also, the dotted-line \nboxes in Figure 4 show how some operator nodes can be combined with location nodes to obtain macro-location-nodes. \nFor example, each location node can use a union node to combine all assignments to the location, and \nan implicit intersection for its declared type (if any). Our implementation of the Value Graph uses macro-location-nodes, \nand therefore avoids creating many explicit operator nodes. We conclude this section with an example \nof a reverse-DFS traversal. Figure 5 shows a reverse-DFS traversal of the Value Graph from figure 4 starting \nat field B.b. The DFS traverses edges in the reverse direction, from destination to source, to identify \nall types that can be propagated into B.b. Type propagation occurs from source to destination, along \nthe return control flow of the reverse-DFS. The type set is computed for each node visited, using the \nequations introduced earlier. In Figure 5, each edge is labeled with the type set propagated along the \nedge. Thus, the type of field A.a will be Object[], since that was the only type assigned to that field. \nThe element of that array could have been assigned an object of either type T1 or T2, so the type set \nfor element node \u00a3(A.a) is {T1,T2}. The value retrieved from E(A. a) was assigned to local k, which, \ncom- bined with a previous assignment of T2, gives local k the type set {T1,T~}. The CHECK-CAST intersection \nnarrows the set down to {T2} (assuming T1 is not a subclass of T2), which is the resulting type set for \nfield B.b. Therefore, any optimization can (optimistically) assume that a reference loaded from B.b must \npoint to an object of type T2, as opposed to the closure of the declared type, T~. The example in this \nsection described a local (intraprocedu-ral) Value Graph for a sequence of bytecode instructions in a \nsingle method. The global (interproeedural) Value Graph for the dynamic whole program is very similar \nin structure. Constant, field, and element nodes are shared across all local Value Graphs; however, the \nsets of local nodes for distinct methods are disjoint. Whenever a new method is analyzed, its local Value \nGraph is merged into the global Value Graph by inserting nodes and edges as needed. 4.3 DOIT Analysis \nPhase The job of the DOIT analysis stage is to perform a quick traversal of the bytecodes for a given \nmethod on its first invocation, so as to obtain the answers to the following two questions: 1. What types \nof values are assigned to globally accessible fields and array elements? 2. Does this information invalidate \nany assumptions that were used in optimization?  Since this stage is performed dynamically for every \nmethod that is executed, it is desirable that the algorithm be per- formed in a single linear-time pass. \nFigure 6 contains a high-level description of the analysis algorithm that is performed on first invocation \nof method M. A location node is created for each local in the Value Graph, and default edges are added \nfor location nodes that represent parameters. A flow-insensitive algorithm is employed for the sake of \nefficiency. The algorithm separately processes each extended basic block [10] by making conservative \nassump- tions about the state of the Java operand stack on entry to the extended basic block. The action \nperformed for each bytecode instruction b is defined by Rule[b], where Rule[] is the set of rules defined \nin Appendix A (Tables 7-11). These rules describe the impact of each bytecode instruction on the Value \nGraph Q and the abstract stack 8. For conve- nience, Table 1 contains an informal summary of the rules \nfor selected bytecode instructions. 4.4 Optimization The results of DOIT Analysis can be used by the \noptimizing compiler in the adaptive optimization model to perform cer- tain type-dependent optimizations, \nsuch as devirtualization (and subsequent inlining), type check eliminations (includ- ing array store \nchecks), and others. Most of these opti- mizations make an assumption that the type of the objects assigned \nto the field will not change, and will create code that will be incorrect if that assumption is violated. \nTherefore, performing these optimizations requires a means of detecting that an assumption is violated, \nand a means of invalidating the code created based on that assumption. Our framewor k supports both. \nWhenever an optimization is performed, the optimizing compiler registers a property with the framework \n(Fig. 7). It also attaches an invalida- tion action to that property. Whenever subsequent analysis is \nperformed (for newly loaded/executed methods), that property is verified, and if it turns out to be wrong \n(i.e., the assumption is violated), the invalidation action is executed, performing whatever invalidation \nis necessary to undo the optimization. 5. RESULTS In this section, we present experimental results obtained \nfrom a prototype implementation of the DOIT algorithm in the IBM Jalapefio Java Virtual Machine [6, 1], \nThe goal of this study was to understand the extent to which Create a location node for each local in \nmethod M (including parameters) for each parameter p of method NI do Let T be the declared type of parameter \np Insert an edge from the closure node n~ to n~ (the location node for parameter p) end for Identify \nextended basic blocks in the bytacodes of method M for each extended basic block B in method M do Let \nz := stack depth on entry to B // Assume wo~'st-case information for stac~ s~ots on entity to B Let \n$ := abstract stack with z slots initialized to ni for each bytecode instruction b in block B do Update \nS and perform Value Graph actions dictated by Rule[b] end for end for (Optional) Simplify Value Graph \nby eliminating all local and redundant operator nodes Figure 6: High-level algorithm for DOIT Analysis \nstage performed on method M I] Bytecode instruction (b) Rule[b] Any instruction that pushes a single-word \nprimitive or Push the empty type node n\u00a2 onto 8 null value on the stack e.g., aconst_uull Any instruction \nthat pushes a double-word primitive Push two empty type nodes n\u00a2 onto S (long/double) on the stack new \nT; newarray T; anewarray T; multianewarray T Push type node nT onto S invokevirtual M; invokespecial \nM; invokestatic M; (Conservative solution) invokeinterface M (r~(M) is reference) Push the closure node \nn~(M) onto S aload_(k); [wide] aload k Push local node n~ onto 8 getstatic f; getfield f (dr(f) is reference) \nPush field node nf for field $\u00a2 onto 8 Pop array node na from S aaload Push element node \u00a3(n~) onto \nS Pop node n from ~q if n \u00a2 n~ then astore_{kl; [wide] ast0re k Insert an edge from node n to local \nnode n~ end if Pop node n from 8 if n \u00a2 n\u00a2 then putstatic f; putfield f (dr(f) is reference) Insert an \nedge from node n to implicit intersection node n~ for field node n/ end if Pop array node na from ~q \nPop node n from 3 if n ~ n\u00a2 then aastore Insert an edge from node n to intersection node n~ for element \nnode \u00a3(na) end if Pop node n from 8 if n \u00a2 n$ then Create a new intersection node n~ c checkcast T Insert \nan edge from the closure node n~ to n~ \u00a2 Insert an edge from node n to n~ \u00a2 Push node n~ \u00a2 onto 8  end \nif Table 1: Selected Transformation Rules. Only relevant stack operations are shown. For complete rules \nsee appendix. for each field f that needs to be analyzed for optimization do Perform a demand-driven \n~eve~se-DFS ~a~ersa~ starting with field node n I end for Perform optimization for each field f for which \noptimistic info that \u00a3(f)=T was used do Register a property verification request ~ that performs a ~e~e~ee-DFS \nto check that \u00a3(]) = r after the method is analyzed by DOIT (this will apply to all newly executed methods); \nRegister an invalidation action ~ with ~ to invalidate the compiled code for method M and recompile without \nassuming that f(f)=r. This action will be performed if the property verification ~ fails. end for  \nFigure 7\": High-level description of the use of DOIT analysis results in optimization DOIT Analysis can \nyield more precise type information for fields (compared to their declared types), and how that type \ninformation can benefit optimizations. To that end, we focused our attention on static and instance fields \ncontaining object references and ignored fields with primitive types. Table 2 summarizes benchmark characteristics \nthat are most relevant to the DOIT algorithm, namely the number of meth- ods and fields analyzed. The \nbenchmark suite used in our study consists of the seven SPECjvm98 [28] benchmarks (executed with 100%-sized \ninputs), and two larger codes: HyperJ, the Hyper-J [21, 19] composition tool, and DOMCotmt, the Xerces \nvl.2.3 [26] XML parser. For convenience, the method and field counts are presented separately for the \napplication classes and the Java standard library classes. All classes named java. * were counted as \nlibrary classes, and all remaining non-Jalapefio classes were counted as application classes. The counts \nin Table 2 are presented for two cases: pes-simistic analysis and optimistic analysis. In optimistic \nanal- ysis, only methods that are dynamically invoked are ana- lyzed. In pessimistic analysis, all methods \nin all reachable classes are analyzed. A field is analyzed if it is referred to by at least one method \nthat is analyzed. As mentioned ear- lier, a key advantage of optimistic analysis over pessimistic analysis \nis that it causes fewer methods and fields to be analyzed, thus resulting in more precise dynamic analysis \nas well as reduced analysis overhead. As can be seen in Table 2, the reduction due to optimistic analysis \nin the number of application methods analyzed ranged from a factor of 2.1\u00d7 to a factor of 8.2 x, and \nthe reduction in the number of fields analyzed ranged from 1.1x to 2.0x. A unique feature of the Jalapefio \nJVM is that it is itself implemented in Java. For convenience, we did not dynam- ically compile any Jalapefio \nmethods in this study. (It is difficult to obtain repeatable measurements when the JVM classes themselves \nare being dynamically compiled, since many JVM services are performed nondeterministically.) In- stead, \nthe Jalapefio classes were statically precompiled into a variant of a FullAdaptive boo't image [27] for \nboth pes- simistic and optimistic analyses. The methods and field counts for these Jalapefio classes \nare shown in Table 3. The difference between the counts for optimistic vs. pessimistic analysis is insignificant, \nand arises solely from the differ- ent runtime services need for lazy vs. non-lazy compilation. However, \neven though the Jalapefio classes were not dy-namically compiled, each Jalapefio method was subjected \nto DOIT analysis with nodes and edges added to the global Value Graph as needed. Table 4 summarizes the \nstatistics for the Value Graphs con- structed when executing each of the benchmarks. The node counts \nare broken down into field nodes 1 , type nodes, and other (local and operator) nodes. Despite the fact \nthat more than 10,000 methods were analyzed for each bench- mark (when including the Jalapefio methods), \nthe number of nodes and edges in the Value Graph is only in the 3,200 -4,400 range. This shows that the \nValue Graph is a promis- ing representation for use in efficient, scalable and dynamic whole program \nanalysis. As mentioned in Section 4.1, all local nodes and many operator nodes can potentially be eliminated \nby Value Graph simplification. However, the results in Table 4 suggest that the Value Graph is already \nsmall enough, so that elimination of these (other) nodes may not have sufficient impact to justify the \nsimplification. Table 4 also contains data on the average traversal size measured across reverse-DFS \ntraversals performed on each field. An attractive feature of the DOIT algorithm is that the traversal \ncan be performed on demand. However, for the sake of completeness, the average presented is for traversals \nperformed on all application, library and Jalapefio fields, instead of just the fields that are of interest \nto the optimizing compiler when compiling a specific method. The average traversal sizes are very encouraging \nbecause they show that the analysis results for any field can usually be obtained in constant time from \nthe Value Graph (via a traversal of < 3 nodes and 3 edges on average). Table 4 also shows the maximum-length \ntraversal observed across all fields. The maximum traversal size is the same for all benchmarks (21 nodes \nand 35 edges) because it was a traversal that was obtained for a non-application field. We now present \nresults on how the improved type infor- mation obtained by optimistic analysis can benefit opti- mizations. \nFigure 8 summarizes the impact of optimistic 1As discussed in Section 4.2, we implement field nodes as \nmacro-location nodes, so that each field node can contain an implicit union and intersection operator. \nApplication classes Library classes # methods analyzed ~ fields analyzed methods analyzed ~fields analyzed \nBenchmark Pessimistic I Optimistic Pessimistic I Optimistic Pessimistic I Optimistic Pessimistic] Optimisti \ncompress 303 74 165 139 2724 848 '373 9 jess 908 417 217 187 2760 860 379 9 db 293 72 149 126 2744 860 \n377 9 javac 1418 684 382 305 2724 864 373 9 mpegaud\u00b1o 528 192 277 246 2724 852 373 9 mtrt 432 142 185 \n160 2724 857 373 9 jack 561 193 221 196 2724 851 373 9 HyperJ 6604 1704 1249 879 2883 907 400 14 DOMC0unt \n2707 331 851 432 2834 863 394 13 Table 2: Benchmark characteristics. For each benchmark, the Table gives \nthe number of methods and fields analyzed in application and library classes using pessimistic and optimistic \nanalyses. # methods analyzed ~ fields analyzed Pessimistic Optimistic Pessimistic Optimistic Jalapefio \nclasses 9703 9696 1871 1868 Table 3: Jalapefio characteristics Size of the Value Graph Traversal Statistics \nNodes I Average traversal Maximum traversal Benchmark Field I RXype I Other )Total Edges nodes I # edges \nnodes I ~ edges compress 2100 418 791 3309 3640 2.51 2.70 21 35 jess 2152 436 805 3393 3730 2.51 2.70 \n21 35 db 2091 413 788 3292 3623 2.51 2.70 21 35 javac 2268 453 877 3598 3948 2.52 2.70 21 35 mpegaudio \n2207 450 791 3448 3788 2.49 2.68 21 35 mtrt 2122 434 810 3366 3703 2.52 2.71 21 35 jack 2159 438 812 \n3409 3758 2.51 2.71 21 35 HyperJ 2890 556 1015 4461 4871 2.47 2.64 21 35 DOMCoun\u00a2 2439 502 886 3827 4262 \n2.51 2.72 21 35 Table 4: Value Graph characteristics (optimistic analysis) Benchmark w/o DOIT analysis \nw/ DOIT analysis Percentage improvement compress 21.0 21.0 -0.3% jess 8.2 8.1 1.6% db 21.3 19.7 7.5% \njavac 12.2 11.8 3.6% mpegaudio 21.0 20.9 0.3% mtrt 4.8 4.7 0.5% jack 15.8 15.7 0.8% HyperJ 3.1 3.0 2.4% \nDOMCount 2.4 2.0 13.8% Table 5: Execution times (in seconds) with and without the analysis 3.le8 3.0e8 \n3.3e8 1.1e9 9.5e7 7.9e7 2.7e7 3.5e9 10(J 8C o o m Unguarded inline .~ 6G Guarded inline t:mVirtual dispatch \n r Static Pessimistic N 4~ o Dynamic Optimistic Q 2C PO PO PO PO PO PO PO P 0 PO PO Figure 8: Impact \nof DOIT analysis on virtual call handling 4.8e2 2.8e6 6.0e7 1.4e7 7.9e5 2.4e2 1.9e7 4.7e7 7.2e6 1.5e8 \n100 I 8O \"~ Virtualized and unguarded tm Virtualized and guarded - 60 is Static guarded inline t:= Virtualized \nt= Interface dispatch ~ 4o- Static Pessimistic I o Dynamic Optimistic i 20, l I 0 P\"O' P 0 P 0 P 0 \nP 0 P 0 P 0 P 0 P 0 P 0 Figure 9: Impact of DOIT analysis on interface call handling i Bytecode size \nProcessing rates (bcb/ms) Benchmark (,kilobytes) , DOIT analYsis I Baseline compiler Ratio compress 19.7 \n592.82 234.37 2.53 jess 41.9 571.76 127.25 4.49 db 20.9 668.56 237.71 2.81 j avaC 85.8 614.73 126.30 \n4.87 mpegaudio 68.2 931.47 332.67 2.80 m~rt 31.3 666.94 123.60 5.40 jack 49.4 790.45 187.37 4.22 Table \n6: Analysis and compilation rates (bytecode bytes per millisecond) analysis on the optimization of virtual \ncalls. We only present virtual method statistics for the application classes (by ex- cluding Jalapefio \nclasses from our measurements), so as to provide results that are JVM-independent. For each bench- mark, \nFigure 8 shows the breakdown of virtual calls into three categories --unguarded inline, guarded inline, \nand virtual dispatch --for pessimistic vs. optimistic analysis. The total number of virtual calls is \nalso shown on the top of each bar. The unguarded inline category includes calls that are provably monomorphic \nin both pessimistic and op- timistic analysis frameworks. The guarded inline category is monomorphic \nfor optimistic analysis 2, and polymorphic for pessimistic analysis. The virtual dispatch category includes \ncalls that are considered to be polymorphic for both pes- simistic and optimistic analyses. In some cases, \na monomor- phic call may also be included in the third category; this occurs if the call target is too \nlarge to be a candidate for inlining, or when the inlining depth reaches a certain preset limit. When \ncomparing the dynamic number of virtual calls in the pessimistic and optimistic cases in Figure 8, we \nsee that the average fraction of polymorphic virtual calls decreased from 39.5% to 24.5% due to optimistic \nanalysis, with a best-case decrease from 47.0% to 8.1% in mtrt. (The average was obtained by taking a \nweighted mean across all bench- marks, using the dynamic number of virtual calls as the weight.) In most \ncases, the decrease was primarily due to \"guarded inline\" calls being counted as polymorphic virtual \ncalls in pessimistic analysis and as monomorphic virtual calls in optimistic analysis. However, in some \ncases (e.g., jack) the refined type information from DOIT analysis also contributed to a reduction in \npolymorphic calls. Figure 9 shows analogous results for interface calls. For each benchmark, the figure \nshows the breakdown of interface calls into five categories -- virtualized and unguarded, virtualized \nand guarded, static guarded inline, virtualized, interface dis- patch --for pessimistic vs. optimistic \nanalysis. The first cat- egory includes calls that are provably monomorphic in both pessimistic and optimistic \nanalysis frameworks. The second and third categories are monomorphic for optimistic anal- ysis, but polymorphic \nfor pessimistic analysis. The fourth and fifth category includes calls that are considered to be polymorphic \nfor both pessimistic and optimistic analyses. As with virtual calls, a monomorphic interface call may \nbe included in the fourth or fifth category if the call target is too large to be a candidate for inlining, \nor if the inlining depth reaches a preset inline depth limit. This is the reason for the anomaly in the \nresults for HyperJ in Figure 9, where the number of polymorphic calls appears to be slightly larger for \noptimistic analysis than for pessimistic analysis -- the refined type information produced by DOIT analysis \nallowed the optimizing compiler to inline calls higher in the call chain, reaching the inline depth earlier, \nand thus precluding inlining of more frequently executed methods lower in the call chain. ~In our experiment, \nguarded inlining was only performed when there was a single target in the dynamic whole program. When \ncomparing the dynamic number of interface calls in the pessimistic and optimistic cases in Figure 9, \nwe see that the average fraction of polymorphic interface calls decreased from 96.4% to 36.1% due to \noptimistic analysis, with a best. case decrease from 100.0% to 0.0% in db. (The average was obtained \nby taking a weighted mean across all benchmarks, using the dynamic number of interface calls as the weight.) \n~Ve can see from Figure 9 that DOIT analysis had a more significant impact on interface calls than virtual \ncalls. Since both virtual and interface calls are known to be sig- nificant sources of performance overhead \nin object-oriented programs, it is reasonable to expect that reducing the num- ber of these calls will \nresult in reductions in execution time as well. Table 5 summarizes the impact of the DOIT algorithm on \nthe execution times for the benchmarks used in our study. These execution times were measured using the \nJalapefio adaptive optimization system [4] with 2 virtual processors; the time reported for each benchmark \nis the best observed in four runs. All timing measurements reported in this paper were obtained using \nthe Jalapefio JVM on an IBM RS/6000 Enterprise Server F80 running AIX v4.3. The machine has 4GB of main \nmemory and six 500MHz PowerPC RS64 III processors each with 4MB of L2 cache. In adaptive optimization \nmode, optimistic information is only used when a method is selected for optimized compilation; no invalidation \nwas triggered in this mode when executing the benchmarks in our study. The execution time improvements \nin Table 5 were notice- able for two of the benchmarks (7.5% for db and 13.8% for DOMCount), and less \nsignificant (3% or under) for the others. There are three reasons why significant reductions in virtual \nand interface calls did not translate to significant reductions in execution times. First, the Jalapefio \nJVM has a very efficient implementations of virtual and inter- face calls [3]. Second, the Jalapefio \noptimizing compiler [6] performs intraprocedural flow-sensitive type analysis, even in the absence of \nDOIT information. Third, virtual and interface calls account for a fraction of the total benchmark execution \ntimes. Finally, Table 6 summarizes the analysis overhead measured for the DOIT algorithm in units of \nbytecode bytes/ms, when analyzing methods in the SPECjvm98 benchmarks. The compilation rates for the \nJalapefio baseline compiler are glso presented as a comparison point. We see that the per- method analysis \ntime for DOIT is very small: about 2.5x- 5.4x faster than the Jalapefio baseline compiler, which in turn \nruns about 100x faster than the Jalapefio optimizing compiler [4]. 6. RELATED WORK Type analysis has \nreceived a lot of attention in past work on static compilation of OO programs, because it is key to enabling \noptimization of virtual method calls and elimina- tion of dynamic type checks. Past work on class-hierarchy \nanalysis (e.g., [11, 12]) and call graph construction (e.g., [22, 15]) included analysis of types and \nfields. There has also been attention paid to performing these analyses efficiently e.g., by using Rapid \nType Analysis [5] and fast algorithms for call graph construction [22, 29, 25]. The major difference \nbetween the DOIT algorithm presented in this paper and past work on type analysis lies in our algorithm's \nefficiency, and its integration with a framework for dynamic optimistic analysis, Most static analysis \nalgorithms in past work have been re- stricted to the case when dynamic class loading does not occur. \nOne exception is e~tant analysis [24], an approach that identifies a subset of call sites whose targets \ncan be bounded at compile-time, even if dynamic class loading is permitted. However, the effectiveness \nof extant analysis depends greatly on the nature of the \"closed world\" that is provided as the starting \npoint for analysis. There has also been a significant amount of past work on optimistic adaptive optimization \nand invalidation e.g., in the Self [9], Hotspot [18], and Jalapefio [2] virtual machines. Invalidation \nin the Self and Hotspot virtual machines in- cludes recompilation of methods, as well as deoptimiza~ion \ni.e., rewriting an optimized stack frame so that execution can continue in interpreted/unoptimized mode. \nCompared to past work on dynamic compilation and adaptive optimiza- tion, the DOIT approach is unique \nin its collection and use of whole program dynamic optimistic analysis information. 7. CONCLUSIONS In \nthis paper, we presented a framework for performing dynamic optimistic interprocedural analysis. The \nframe- work is designed to be used in the context of dynamic class loading and dynamic compilation, and \nincludes mechanisms for event notification and dependence tracking. We illus- trated the functionality \nof the framework by using it to implement a new dynamic optimistic interprocedural type (DOIT) analysis \nalgorithm. The framework and DOIT analysis axe implemented as part of the IBM Jalapefio Java Virtual \nMachine. Our experimen- tal results for the SPECjvm benchmarks and two larger pro- grams show significant \nbenefits due to dynamic optimistic analysis. Compared to pessimistic analysis, the reductions in the \nnumber of methods and fields analyzed were in the 2.7\u00d7-4.6\u00d7 and 1.6x-2.4x ranges respectively. The average \nfraction of polymorphic virtual calls decreased from 39.5% to 24.4% due to optimistic analysis, with \na best-case de- crease from 47.0% to 8.1% in mitt. The average fraction of polymorphic interface calls \ndecreased from 96.4% to 36.2% due to optimistic analysis, with a best-case decrease from 100.0% to 0.0% \nin rib. These benefits were obtained with a low dynamic analysis overhead in the range of 570-930 bytecode \nbytes/millisecond (about 2.Sx-5.4x faster than the Jalapefio baseline compiler). For future work, we \nplan to explore the use of our framework to obtain dynamic and optimistic implementations of other program \nanalyses, such as escape, immutability, and value analyses. Our future interest is in using these analyses \nto both drive optimizations and to verify program invariants. Acknowledgments The authors would like \nto acknowledge the contributions of the entire Jalapefio team for building the JVM infrastruc- ture used \nin this research, and the OTI VAME team for providing an implementation of the Java class libraries for \nuse in conjunction with Jalapefo. Early presentations on this research were given in two internal meetings \nat IBM, one at the T.J. Watson Research Center and one at the IBM Toronto Laboratory, and we are grateful \nfor the feedback that we received from both audiences. Thanks especially to David Grove for his comments \non previous drafts of this paper. 8. REFERENCES [1] B. Alpern et al. The Jalapefio virtual machine. IBM \nSystems Journal, 39(!), 2000. [2] B. Alpexn, D. Attanasio, J. J. Barton, A. Cocchi, D. Lieber, S. Smith, \nand T. Ngo. Implementing Jalapefio in Java. In ACM Conference on Object-Oriented Programming Systems, \nLanguages, and Applications, pages 314-324, 1999. [3] B. Alpern, A. Cocchi, S. Fink, D. Grove, and D. \nLieber. invokeintefface considered harmless. In ACM Conference on Object-Oriented Programming Systems, \nLanguages, and Applications, Oct. 2001. [4] M. Arnold, S. Fink, D. Grove, M. Hind, and P. Sweeney. Adaptive \noptimization in the Jalapefio JVM. In ACId Conference on Object-Oriented Programming ~ystems, Languages, \nand Applications, Oct. 2000. [5] D. F. Bacon and P. Sweeney. Fast static analysis of C-I-+ virtual function \ncalls. In ACM Conference on Object.Oriented Programming Systems, Languages, and Applications, pages 324-341, \nOct. 1996. [6] M. G. Burke, J.-D. Choi, S. Fink, D. Grove, M. Hind, V. Sarkar, M. J. Serrano, V. C. \nSreedhar, H. Srinivasan, and J. Whaley. The Jalapefio dynamic optimizing compiler for Java. In ACM 1999 \nJava Grande Conference, pages 129-141, June 1999. [7] C. Chambers, I. Pechtchanski, V. Sarkar, M. J. \nSerrano, and H. Srinivasan. Dependence analysis for Java. In 1~th International Workshop on Languages \nand Compilers for Parallel Computing, Aug. 1999. [8] C. Chambers and D. Ungar. Customization: Optimizing \ncompiler technology for Self, a dynamically-typed object-oriented programming language. In ACM Conference \non Object-Oriented Programming Systems, Languages, and Applications, pages 146-160, July 1989. SIGPLAN \nNotices, ~4(7). [9] C. Chambers, D. Ungar, and E. Lee. An efficient implementation of Self - a dynamically-typed \nobject-oriented language based on prototypes. In Proceedings OOPSLA '89, pages 49-70, Oct. 1989. Published \nas ACM SIGPLAN Notices, volume 24, number 10. [10] J.-D. Choi, D. Grove, M. Hind, and V. Sarkar. Efficient \nand precise modeling of exceptions for the analysis of Java programs. In ACM SIGPLAN-SIGSOFT Workshop \non Program Analysis for Software Tools and Engineering, pages 21-31, Sept. 1999. [11] J. Dean, D. Grove, \nand C. Chambers. Optimization of object-oriented programs using static class hierarchy analysis. In 9th \nEuropean Conference on Object-Oriented Programming, 1995. [12] G. DeFouw, D. Grove, and C. Chambers. \nFast interprocedural class analysis. In 25th Annual ACM SIGA CT-SIGPLAN Symposium on the Principles of \nProgramming Languages, pages 222-236, Jan. 1998. [13] D. Detlefs and O. Agesen. Inlining of virtual methods. \nIn 13th European Conference on Object-Oriented Programming, 1999. [14] J. Gosling, B. Joy, and G. Steele. \nThe Java Language Specification. Addison Wesley, 1996. [15] D. Grove, G. DeFouw, J. Dean, and C. Chambers. \nCall graph construction in object-oriented languages. In A CM Conference on Object-Oriented Programming \nSystems, Languages, and Applications, pages 108-124, Oct. 1997. [16] U. HSlzle and D. Ungar. Optimizing \ndynamically-dispatched calls with run-time type feedback. In SIGPLAN '9~ Conference on Programming Language \nDesign and Implementation, pages 326-336, June 1994. SIGPLAN Notices, ~9(6). [17] U. HSlzle and D. Ungar. \nA third generation SELF implementation: Reconciling responsiveness with performance. In A CM Conference \non Object-Oriented Programming Systems, Languages, and Applications, pages 229-243, 1994. [18] The Java \nHotspot Virtual Machine. White paper available from http://java.sun.com/products/hotspot/, May 2001. \n[19] IBM Research. Multi-Dimensional Separation of Concerns: Software Engineering using Hyperspaces. \nhttp://www.research.ibm.com]hyperspace/, 2001. [20] K. Ishizaki, M. Kawahito, T. Yasue, H. Komatsu, and \nT. Nakatani. A study of devirtualization techniques for a Java Just-In-Time compiler. In ACM Conference \non Object-Oriented Programming Systems, Languages, and Applications, Oct. 2000. [21] H. Ossher and P. \nTart. Multi-dimensional separation of concerns and the hyperspace approach. In Software Architectures \nand Component Technology: The State of the Art in Research and Practice. Kluwer, 2001. to appear. [22] \nN. Oxh~j, J. Palsberg, and M. $chwartzbach. Making type inference practical. In the 6th European Conference \non Object-Oriented Programming, July 1992. [23] J. Plevyak and A. A. Chien. Precise concrete type inference \nfor object oriented languages. In ACM Conference on Object-Oriented Programming Systems, Languages, and \nApplieations~ pages 324-340, Oct. 1994. [24] V. C. Sreedhar, M. Burke, astd J.-D. Choi. A framework for \ninterprocedural optimization in the presence of dynamic class loading. In SIGPLAN 2000 Conference on \nProgramming Language Design and Implementation, June 2000. [25] V. Sundaxesan et al. Practical virtual \nmethod call resolution for Java. In ACM Conference on Object-Oriented Programming Systems, Languages, \nand Applications, Nov. 2000. [26] The Apache XML Project. Xerces Java Parser. http://xml.apache.org/xerces-j/, \n2001. [27] The Jalapefio Team. Jalape~o User's Guide. IBM Research Division, Apr. 2001. [28] The Standard \nPerformance Evaluation Corporation. SPEC JVM98 Benchmarks. http://www.spec.org/osg/jvm98/, 1998. [29] \nF. Tip and J. Palsberg. Scalable propagation-based call graph construction algorithms. In A CM Conference \non Object-Oriented Programming Systems, Languages, and Applications, Nov. 2000. APPENDIX  A. TRANSFORMATION \nRULES In this section, we provide the full set of analysis rules for the DOIT algorithm in Tables 7-11. \n(A representative subset was presented earlier in Table 1.) Each row in a table consists of a bytecode \ninstruction and its correspond- ing transformation rule. The generic format for the rules is (S,~) =~ \n(S',~' ~, where \u00a2q and ~ are, respectively, the stack state and the value graph before the transformation, \nand 8' and G' are, respectively, the stack state and the set of edges in the Value Graph after the transformation. \nThe values on the stack are value graph nodes represent- ing types. Stack push and pop operations are \nimplicitly defined by transformations of S, such as S =~- S : nf and S : v =~ $ respectively. An insertion \nof an edge n~ --~ nb (say) in the Value Graph is represented by the transforma- tion, ~ =~ ~U{na --}nb}. \nThere are predefmed type nodes 7 ~ (primitive) and J~f (null), which are both equivalent to he, a node \nwith an empty type set. There is also a predefined type node B, which is equivalent to n\u00b1, the node for \nObject*. The stack on entry to each basic block is initialized to the appropriate number of B values. \ndt(*) represents the declared type of location ~, and rt(M) represents the return type of method M. E(r) \nrepresents an element node for array type r. We will use n I and ni to denote the Value Graph nodes for \nfield f and lo- cal i respectively. Intersection nodes for STORE-CHECK, CHECK-CAST, and IMPLICIT putstatic/putfield \ntype-cheek operations are denoted by n~, n~ c, and n/n respectively. [~By~ecode instruction (b) ..... \nR~ie[b] getstatic f getfield f aaload aload_(k); aload k; wide alo'ad'\"k putstatic f putfield f aastore \nastore_{k); astore k; wide astore k ifa~(f) e {lo.~. do,,~z.} then (s;,~) ~ <~: p: p,g) else if dr(f) \nis primitive then (8,~) ~ (8:7.,~) else end if if dr(f) e {long, doubte} then (2:p,G) ~ (s:p:?.,~) \nelse if dr(f) is primitive then (S:p,~) ~ (s:p,~) else <s:p,~) ~ (8:~,~) end if .... i ifdr(i) e {Io,~9. \ndo.~l.} then (s:v:v,g) ~ (s,g) else if d~(f) is primitive then (8:v,g) ~ (s,g) else (,S : n,g) ~ (,S,~ \nkJ {n--~ nn,nd~(f) -.+ n*n,n~n -+ny}) end if if' d~(f) e \"{lo'n9:\" d';~b'~ei' then .................. \n {S:p:v:v,g) =~ (8, g) else if dr(f) is primitive then (s:p:.,~) ~ (8,~) else * \" \" -*ns}) ( S : p \n: n, ~ ) :* (8, g u {n -~ n~, nd~(1) \"+ n~, nn end if  (s: n~: i--n,a) :. (s,~ u {n --*n\"&#38;n.~ ~ \n~(nD}) (s:n,~) ~ (SiGU{n-+n~}) Table 7: Transformation Rules for Loads and Stores of Locations Bytecode \ninstruction (b) Rule[b] aconst_aull (8,g) =~ (8:N,g) ...... if const[i] is primitive then (S,~) ~ (S:P,~) \nldc i; ldc_w i else (8,g) :* (S:ns~rl.g,~) end if new T (8,~) ~ ($:nT, g) newarray Ti anewarray T multianewarray \nT d (8:sl..sd,G) ~ (8:nT, g) if rt(M) e {void} then ($ : po..p~,g) =* (8,g) else if r~(M) E {long, double} \nthen (8:po..p~,~) ~ (8:P:P,g) invokevirtual M; invokespecial M; else if rt(M) is primitive then invokestatic \nM; invokeinterface M (S : po..pk,6) :=~ (8::P,G) else (8 : Po..P~,~) :=~ (8: n~(M),G) end if Table 8: \nTransformation Rules for Creation of Known Types 209 Byteeode instruction (b) U Rule[b] dup (S:r,g) \n~ (8:r:r,g) (S:v:r,~) ~ (S:r:v:r,~) dup_x2 (S:v~:v::r,g) ~ (S:r:v~:v::'c,g) (S:n:r~,G) m (S:~l:r~:n :r~,O) \ndup2.xl dup2_x2 (8:m:v~:n:r~,g) ~ (8:n:r~.:m:v~:n:r~,g} swap ($:n:r~,~) ~ ($:~:n,g) pop (8:v,~) ~ (8,~) \npop2 Table 9: Transformation Rules for Stack Operations I{ Bytecode instruction (b) return; ret k;wide \nret k ireturn; freturn lreturn; dreturn monitorenter; monitorexit; areturn athrow '~f(c) offset; ifaull \noffset; ifnonnull offset ~f_icmp(c)offset;iLacmp(c)offset jsr offset; jsr_w offset tableswiteh do, hi, \nlo, olo..ohl; lookupswitch do, sz, ( vo,oo }..( v~,o~, ) I Rule[b] I (s,g) ~ (8,g) (s v:v,g) ~ (s,g) \n(s:~,g) ~ (s,g) (8 : n,g) :* (8,g) (S:v,a) ~ (s,~) (s:v:v,g) ~ (s,g) ($,g) =~ ((),~) //clear stack \n(s,g) ~ (s,~) {8:v,g) =~ (8,~) Table 10: Transformation Rules for Control Flow Operations Bytecode instruction \n(b) hop \"]const_(i); fconst_(f); bipush v; sipush v iload_(k); iload k; wide iload k; fload_(k); fload \nk; wide fload k lconst_(l}; dconst_(d}; ldc2_w i; lload_(k); lload k; wide lload k; dload_{k); dload \nk; wide dload k laload; daload istore_(k); istore k; wide istore k; fstore_(k); fstore k; wide fstore \nk; iinc k i; wide iinc k i \"ist0re_{k); lst0re k; wide lst0re k; dstore_(k); dstore k; wide dstore k \niastore; fastore; bastore; castore; saztore lastore; dastore arraylength; instanceof T neg; fneg; i2f; \ng2i; i2b; i2c; i2s iadd; fadd; isub; fsub; imul; fmul; idly; fdiv; ires; frem; ishl; ishr; iushr; iand; \nior; ixor; 12i; 12f; d2i; _d2f; fcmp(c) ladd; dadd; lsub; dsub; lmul; dmul; ldiv; ddiv; lrem; drem; lshl; \nlshr; lushr; land; lor; lxor Rule[b] ($,g) =~ ($,~) (8,G) =~ ($:P,G) (8,g) :* (8:p:p,g) (8:n~:i,g) \n~ (8:P,g) (S:n~:i,g) ~ ($:p:p,g) (8:v,g) =~ ($,g) (8:v:v,~) ~ (S,~) (S:n~ :i:v,g) =~ ($,g) ($ : n~ : \ni : v :v,~) :~ ($,G) ($ : na,~ ) =~ ($ : 79,~ ) (S:v,~) ~ (8:P,6) (S:v:~,g) ~ (S:p:7~,g) ($:v:v,g) ~ \n(8:P,g) (8:v:v:v:v,a) :* (8:P:P,~) (8:v:v:v:v,g) =~ (8:p,g) Table 11: Transformation Rules for Primitive \nOperations 210     \n\t\t\t", "proc_id": "504282", "abstract": "In this paper, we address the problem of <i>dynamic optimistic interprocedural analysis</i>. Our goal is to build on past work on <i>static interprocedural analysis</i> and <i>dynamic optimization</i> by combining their advantages. We present a framework for performing dynamic optimistic interprocedural analysis. the framework is designed to be used in the context of dynamic class loading and dynamic compilation, and includes mechanisms for event notification (on class loading and method compilation) and dependence tracking (to enable invalidation of optimistic assumptions). We illustrate the functionality of the framework by using it to be used in the context of dynamic class loading and dynamic compilation, and includes mechanisms for event notification (on class loading and method compilaton) and dependence tracking (to enable invalidation of optimistic assumptions). We illustrate the functionality of the framework by using it to implement a <i>dynamic optimistic interprocedural type</i> (DOIT) analysis algorithm. The DOIT algorithm uses a new global data structure called the <i>Value Graph</i>. The framework and DOIT analysis of the IBM Jalape&#241;o Java virtual machine. Our experimental results for the SPECjvm benchmarks and two larger programs show promising benefits due to dynamic optimistic analysis. Compared to pessimistic analysis, the reduction in number of methods and fields analyzed was in the 2.7x-4.6x and 1.6-2.4x ranges respectively. The average fraction of polymorphic virtual calls decreased from 39.5% to 24.4% due to optimistic analysis, with a <i>best-case</i> decrease from 47.0% to 8.1%. The average fraction of <i>polymorphic interface calls</i> decreased from 96.4% to 36.2% due to optimistic analysis, with a <i>best-case</i> decrease from 100.0% to 0.0%. These benefits were obtained with a low dynamic analysis overhead in the range of 570-930 bytecode bytes/millisecond (about 2.5x-5.4x faster than the Jalape&#241;o baseline compiler)", "authors": [{"name": "Igor Pechtchanski", "author_profile_id": "81100371293", "affiliation": "Department of Computer Science, New York University", "person_id": "P115951", "email_address": "", "orcid_id": ""}, {"name": "Vivek Sarkar", "author_profile_id": "81100597290", "affiliation": "IBM Research,T. J. Watson Research Center", "person_id": "PP14206121", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/504282.504297", "year": "2001", "article_id": "504297", "conference": "OOPSLA", "title": "Dynamic optimistic interprocedural analysis: a framework and an application", "url": "http://dl.acm.org/citation.cfm?id=504297"}