{"article_publication_date": "01-15-1984", "fulltext": "\n A Comblnator-based Compiler for a Functional Language Paul Hudak Yale Department 1. Introduction Functional \nlanguages are known primarily for their elegance, clarity, and expressive power. Unfortunately, this \nsame elegance and power has been a bottleneck to building efficient implementations for such languages, \nespecially those with fully \"lazy\" evaluation semantics. David Turner in his seminal paper [19] showed \nhow the language SASL, a functional language with lazy evalua- tion, could be implemented simply and \nelegantly using combinators; yet such an implementation on conventional machines seems to be unbearably \nslow, as do implemen- tations based on Landin's SECD machine, as described by Peter Henderson [6]. There \nhas seemed to be an inherent inefficiency in implementing such languages, and few practical functional \nprogramming systems have been built. From the outset the goal of our research has been to dispel this \nbelief. We wish to demonstrate that a func-tional language with the full semantic power of lazy evaluation \ncan be implemented efficiently with an ap-propriate optimizing compiler. Furthermore, we wish to show \nthat combinators offer a convenient intermediate language with which to perform program transformations \nand optimizations. Through the use of combinators, traditional optimizations such as constant-folding \nand global common-subexpresssion elimination become trivial, and we are able to eliminate other sources \nof inefficiency that conventional compilers do not typically deal with. In p'articular, all procedure \nboundaries are eliminated from a program, leaving a single combinator expression that can be manipulated \nat will. One implication of this Permission to copy without fee all or part of this material is granted \nprovided that the copies are not made or distributed for direct commercial advantage, the ACM copyright \nnotice and the title of the publication and its date appear, and notice is given that copying is by permission \nof the Association for Computing Machinery. To copy otherwise, or to republish, requires a fee and/or \nspecific permission. &#38;#169; 1983 ACM 0-89791-125-3/84/001/0122 $00.75 and David Kranz University \nof Computer Science is that there is almost no penalty to the programmer for building modular, hierarchical \nprograms that agree with \"good design practice\". Data may be \"packaged\" into lists for passing between \nprocedures, with no run-time overhead. Once the single combinator expression is created, our compiler \ncreates its own \"optimized\" procedures that do not necessarily correspond to those defined in the source \nprogram. Further optimizations are possible to overcome the shortcomings of lazy evaluation. Normal-order \ngraph reduction is able to accomplish lazy evaluation quite naturally, at the expense of incurring an \nabundance of heap manipulations; more conventional implementations require the use of \"thunks', \"closures\", \nor some equiv-alent object. Although lazy evaluation increases the ex-pressive power of a language, most \napplications do not re- quire it. Our compiler takes advantage of this fact: through a relatively simple \nanalysis we are able to deter- mine most cases where an argument to a function is \"strict\", and may thus \nbe computed prior to invoking the function. When necessary, the use of self-modifying thunks ensures \nus that the overhead of non-strict ar-guments is kept to a minimum. The use of these special objects \nalso effectively accomplishes code-motion from \"loops\". In the next section we provide an overview of \nALFL, the source language of our compiler. In Section 3 we dis- cuss Phase I of the compiler, which includes \nthe details of our abstraction and simplification algorithms. Phase H, optimization and code-generation, \nis discussed in Section 4. The results of our work are presented in Section 5, and in Section 6 we discuss \nthe relationship of our work to others. This research was supported in part by NSF Grant MCS-8302018 \nand ONR Grant 87999. 122 2. ALFL: an Experimental Functional Language ALFL is a block-structured, lexically-scoped \nfunctional language with lazy evaluation semantics; it is similar in style to Keller's FEL [8] and Turner's \nSASL [18] (with most influence from the former). Although ALFI, is not the central issue in this paper, \nit is described here to provide a framework for our compilation techniques. The largest syntactic object \nin ALFL is an equation group, which is delimited by curly brackets (\"{...}'). Within an equation-group \nis a collection of equations that define local identifiers, and a single result clause that expresses \nthe value to which the equation-group will evaluate. A double equal-sign (\"==') is used in equations \nto distinguish it from the infix operator for equality. An equation-group is a special ease of an expression, \nand is thus valid wherever an expression is allowed. A con-ditional expression has the form \"pred -* \ncons, sit\" and is equivalent to the more conventional \"If pred then cons else sit\". Here is a simple \nexample: { fse n == n=0 --* 1, n*fae(n-1); x == 10; result fsc x } ALFL seeping rules are similar to \nthose for most block- structured languages, in that expressions may reference any identifier defined \nlocally in the current equation-group, or in any surrounding equation-group. However, local references \nare allowed to be mutually reeursive. This implies that equations may appear in any order, consistent \nwith lazy evaluation in which expressions are evaluated \"by demand\". In the above example, the two equations \nplus the result clause could appear in any of six different orders. All function applications are \"curried\" \n[5, 13]. That is, all functions are assumed to take just one argument, which is no restriction since \nthat function may return another function that takes one argument, etc. If we define function application \nto associate to the left, curried functions facilitate the use of higher-order functions, as in: { twice \nf x == f (f x); twofscs == twice fsc; result twofacs 10 } which is precisely equivalent to: { twice \nf == { result g; gx == f(fx) } twofscs == twice fae;  result twofaes 10 } ALFL also has a powerful \npattern-matcher through which complex functions may be defined more easily. For example, factorial could \nhave been defined by: { fsc o := l; fsc n == n'fat(n-l); ... } The pattern-matcher is even more useful \nwhen used with lists. Lists are constructed \"lazily\", and are denoted by \"[x,y,z]', which is equivalent, \nusing the infix tensing operator .... , to \"xAyAz \" []\". The selector functions are hd (\"head\"} and tl \n(\"tail\"), corresponding to LISP's CAR and CI)R, respectively. The pattern-matcher knows about lists; \nfor example, the function member may be defined by: { member x [] := false; member x (x'L) == true; member \nx L == member x (tl L); result member 2 [1,2,3] } When using the pattern-matcher note that the order \nof eqnati(,ns defining the same function does matter. \"Infinite lists\" are defined in the obvious way. \nThe in- finite stream of numbers starting at n, for example, can be defined by \"numsfrom n =: nAnunmfrom(n+l) \n\". As a more interesting example, consider this definition of the Fibonacci sequence: { fib == 1 \"1 A \naddstreams [fib,tl fib] ; ad dstreams ix\" S l,y ^ $2 ] == (x+y)\" addstresms [S 1 ,$2 ] ; ... } Of course, \nelements of an infinite list are not computed until they are selected (\"demanded\") for evaluation. The \ninfix operator .... is used to append lists together, and it tot) is lazy. Thus one can even append infinite \nlists together, as in \"fib\" \"fib\"; the second infinite stream of Fibonacci numbers is simply never reached! \nThere are other syntactic and semantic features of ALFL, but they are beyond the scope of this paper. \nOur purpose here is to give a framework on which to base later examples, in addition to pointing out \nthe interesting semantic issues of lazy evaluation, so that the reader may appreciate the implementation \nproblems thus imposed. 3. Phase I: Parsing, Abstraction and Simplification In this section we describe \nPhase I of our compiler, which parses the ALFL source program, simultaneously removing all bound variables, \nand returning in a single pass a partially optimized combinator expression. Phase I1 consists of four \npasses and is described in the next sec- tion. 5.1. The Abstrsctlon AIgorlthm Abstraction is the means \nby which free variables are removed from an expression. Let \"[x]exp\" denote the 123 abstraction of free \nvariable x from expression exp. An abstraction process is valid if it obeys the law of abstraetlon which \nstates that ([x]exp) x = exp. In the lambda calculus [x]exp = ),x.exp, which is consistent with the law \nof abstraction through the definition of //-reduction. In the combinatory calculus [5]: [x] x = I [x] \ny = K y, where y is a constant or variable other than x [x] (el e2) : S ([xlel) ([x]e2), where el and \ne2 are arbitrary combinations  which is consistent with the law of abstraction through the definitions \nof S, K, and I, which are primitive func- tions called combinators. Their definitions, or reduction rules \nare: Ix =x Kxy =:~x Sfgx =~ fx (g x) where, as in ALFL, function application associates to the left. \nTurner describes in [20] an abstraction algorithm for SASL, a functional language similar to ALFL. This \nalgo- rithm is deficient for our purposes for two rem~ons: 1. It does not explicitly handle the lexical \nscop- ing required when nesting equation-groups to an arbitrary depth. 2. On mutually recursive equations \nthe method generates a combinator expression that fails to terminate when evaluated.  The latter problem \nlies in the use of a special combinator U to handle mutual recursion. 1 It turns out that no spe- cial \ncombinator is needed, and the non-termination problem can be avoided. Our abstraction algorithm works \nfor any block-structured, lexically-scoped, functional language. The pattern-matcher first combines all \nequations defining the same function into a single equation. For example, the definition of member given \nearlier is transformed into: member xl x2 == x2: [] ~ false, xl=(hd x2) --* true, member xl (ti x2) \nwhere xl and x2 are new, unique identifiers. Now consider the following equation group in which equations \ndefining the same function have been com-bined, and in which there are no nested equation groups: { fl \nargll argl$ \"'\" arglmt := exPl; fz argsl argzs \"'\" argzm= == exp2; fn argnl argn2 \"'\" argnmn == eXPn; \nresult E } There are n equations, each equation i having m i ar-guments (m I may be zero), and having \ndefinition exPi. \"/'he abstractor first normalizes each equation i by abstracting each argument argix \n(but not fl) from exPr For example, for fl this yields: r I = [arsll]([ars12] ( ... ([arglma]exPl))) \nLetting E i denote the result of normalizing equation i, then once all equations have been normalized \nwe have: { fl == El f:~ == E:I fn==E result ]~ } Note that a free variable in an expression E! can only \nbe one of the variables fl through fn' or any variable that is free in the entire equation-group. Next, \nthe abstraetor eliminates the equations one at a time. That is, each variable f! is abstracted out of \nthe entire equation-group, replacing it with an equivalent to fr llere is the result of eliminating fl: \n{ fs == ([fl]E2) sir *o* f. == ([fI]En) El'' result ([f,]E) E, } where ElW = Y ([fl]E1). 2 Y, of course, \nis the fixpoint combinator defined by Y f = f (Y f). This process is repeated until all equations are \neliminated and a single result clause remains. Now consider a program containing equation-groups nested \nto an arbitrary depth. If the equation-groups are abstracted from the inner-most group out (i.e., \"bottom-up'), \nthe lexical scoping rules are automatically preserved. This is because an identifier is bound to the \ninner-most surrounding equation-group in which it is Iln Turner's scheme the special combinator U is \ndefined so that U f {x'y) ~ f x y, and abstraction of a list from an expression is defined to be: Ix'Y] \nE = u (ixl (Lvl E) ) To see why Turner's scheme fails to terminate, consider a program E where f = %, \ng = e#. Turner's algorithm first combines the equa-Lions into a list, then abstracts the list as defined \nabove. That is: E where f'lg = el'eS =+ ([f'll] E) (Y (If'El (el'e#)) ) =* u ([rl fig] E)) (Y (U ([q \n([sl (el'e=))))) Writing this last expression as U E I (Y (U el)) and applying the reduction rule for \nU it is seen that Y (U I) must be reduced first. Doing so yields U e I (Y (U er)), but now the reduction \nrule for U tells us that Y (U e I) must be reduced again! This process clearly does not terminate. If \none insists on using this technique it seems necessary to introduce another version of Y that evaluates \nits ar- guments in a different order to guarantee termination. ~ln our implementation all of the ~currences \nof En ~ are shared. defined. The reader may verify that the overall algo- rithm obeys the law of abstraction \nby reversing each step to be sure that the original expression results. Note that: 1. Mutual recursion \nis handled naturally as the equations arc sequentially eliminated. 2. If fl is not recursive (i.e., \nfl is not free in El), then ElS = Y (K El) : E r Similarly, if fl is not free in E I, i~l, then fi = \n(ill]El) Eft = K E l EI' = E l .  3. The above steps are always taken, even for equations with no arguments, \nsince the equa- tion might still be recursive. For example, the equation \"x == l~x\" is recursive, and \nwell-defined (i.e., the infinite list of ones}.  In our implementation an additional technique is used \nto improve the performance of the abstracter: a free-variable list is kept for each subexpression. Thus \ngiven a very large expression E, if x is not contained in the free-variable list for E, then [x]E immediately \nreturns K E. Without the free-variable list, x would need to be abstracted from every subexpression of \nE, and many simplifications would be required before the final result K E is returned. 3.2. Simplifications \nand Optimizations Given a combinator (or lambda} expression, several reducible subexpressions (called \nredexes [17]) may appear at once. There are two standard ways of choosing which to reduce first. In applicative-order \nevaluation the inner-most redexes are reduced first. This corresponds roughly to the call-by-value function-calling \nmechanisms used by most Algol-like languages, where the arguments to a function are computed prior to \ninvoking the func- tion. In normal-order evaluation the left-most redex is reduced first, meaning that \nouter redexes are reduced be- fore inner ones. It can be shown that some programs will terminate under \nnormal-order evaluation, but not under applicative-order, 3 but any terminating applicative-order evaluation \nwill also terminate under normal-order. It is thus often said that the expressive power of normal-order \nevaluation is greater than that of applicative-order. Lazy evaluation corresponds roughly to normal-order \nevalua- tion, but generally carries the additional connotation that no subexpression is reduced more \nthan once --this has also been called \"fully lazy evaluation\" [7]. 4 ~'lnfinite lists\" are one example, \nbut a more straightforward one is the ALFL program {f x y == y=0 ~ 0, f (f x y) {y-l); result f 0 1). \n4Other terms have included procrastinating, lenient, demand-driven, and call-by-need. The call-by-namestrategy \nused in ALGOL is also closely related. Turner describes a method for evaluating a combinator expression \nby normal-order fraph-redudioa [19], which results in a fully lazy implementation. The technique has \ncertain \"seif-optindzing properties\", such as constant-folding and code-motion from \"loops\". Our goal \nis to ac- complish as many of these same optimizations as possible prior to I)rogram execution --the \nmachine code that we eventually generate will then benefit accordingly. As the source text is parsed \nin recursive-descent form, com-binator exl)ressions are created from the inner-most ex- pression out. \nRather than wait until run-time, these ex- pressions are simplified as they are constructed, using the \nfollowing rules: I. SK ~KI 2. S (K I) # I 3. s (K (K ,,)) ~ K (K ,,) 4.8 (Kx) l =*x 5. S (K x) (K y)~ \nK (x y) 6. Sfgx =* fx (gx) 7. Kxy ~x 8.1x ~x  9. Y (K x) =* x 10. binop cl c2 =* z, where el and \nc2 are con- stants, z = el binop e2, and blnop is one of -t-, *,-,/, etc.  I I. typepred ob =----* bool, \nwhere ob is an object whose type can be inferred, and bool is the result of applying typepred (one of \npair?, integer?, string?, etc.) to ob. 12. EQ x y ~ true, if x is the same object as y, or if they have \ntile same value. 13. EQ x y =:* false, if x and y are of different types, or if they have the same type \nbut dif- ferent values. 14. NOT true ~ false, NOT false ~ true 15. AND true ~ I, AND false ~ K false \n 16. AND x true ~ x, AND x false ~ false 17. OR true ~ K true, OR false ~ I 18. OR x true ~ true, OR \nx false ~ x 1O. IF true m K, IF false ~ K I  20. IF (NOT x) ~ S(K(S(IF x)))K 21. (IFpca) x =~ IF p \n(c x) (a x) 22. f(IF pc a) ~IF p(fc)(fa), provided f is strict in its first argument 23. HD(Pxy) ~x, \nTL(Pxy)=~y  Rules 1-5 are optimal, in that they were obtained from an exhaustive case-by-case analysis \nof abstraction using S, K, and I. Rules 6-8 are simply the reduction rules for S, K, and I, and rule \n9 eliminates the Y operator when no recursion is present. Rule 10 accomplishes arithmetic 125 constant-folding, \nand rule 11 folds type-testing when pos- sible. Rule 12 or 13 is applicable only if it can be deter- \nmined either that x and y are equal, or that they are not equal, respectively. Rules 14-20 are boolean \nsimplifica- tions (rulre 20 may not seem to \"simplify\" much, but NOT has been eliminated, and the S's \nand K's will dis- sappear a.\u00a2., a result of other simplifications). The purpose of rules 21 and 22 is \nto cause as much function applica- tion as possible to allow further simplifications. Note that to retain \nnormal-order reduction semantics, rule 22 cannot be applied unless f is a strict function; more will \nbe said about this later. Finally, rule 23 forces selection of elements from lists. /Ls a further simplification, \nthe ex- tra combinators B, C, S I, B', and (3' [20] are used in our implementation to reduce the \"intermediate \ncode\" size. As the combinator expressions are created, eommon-subexpression elimination is also performed. \nUsing stan- dard techniques, a harsh-value is computed for each ex-pression and saved for comparison \nto subsequent ones. If a match is found, the common subexpressions are col-lapsed into one. Since there \nare no side-effects, this al- lows global common subexpression elimination. 3.3. Implications 3.3.1. \nAdvantages Phase I of the compiler essentially performs a partial evaluation of the entire source program, \ncollapsing it into a single expression, s The abstraction algorithm automatically preserves the block-structured \nseoping rules, and mutually recursive references are eliminated as ea_gily as any other. Constant-folding \noptimizations occur automatically, and the same mechanisms that fold arith- metic expressions such as \n+ x y also fold binding expres- sions such as S f g x. lligher-order functions such as twofacs defined \nearlier collapse into precisely the same combinator expression that would have resulted if the function \ntwice were not used. Indeed, the abstraction al- gorithm actually \"collapses\" all procedure boundaries \nex- cept recursive ones. Thus there is no penalty for employ- ing \"good design rules\" that suggest, for \nexample, naming subexpressions and using procedural abstractions {especially unshared ones} to modularize \nand clarify code. Another implication of our algorithm is that there is no penalty for \"packaging\" data \ninto lists that are passed between procedures, such as the \"destructuring\" capability found in some LISPs \n{which generally does not come without penalty). This advantage is accrued not only with non-recursNe \nprocedures, but also with recur- SThe only difference between our strategy and applicative-order reduction \nis that we do not perform Y reductions, since that would amount to program execution. sive ones. For \nexample, for this contrived version of member: { member [ Ix], [ [L] ] ] == L= [] ~ false, x=hd L ---, \ntrue, member [[x],[[tl L]]]; result member [[2],[[[1,2,3]]]] } our compiler generates precisely the \nsame code as the ver- sion defined earlier! All conses are eliminated completely except for those in \n[1,2,3]. 3.3.2. Disadvantages The chief disadvantage of our simplification strategy is that all objects \ndefined by the user via an equation get embedded, or integrated, into the expressions that refer- enee \nthem. In the case of common sub-expressions, this is no problem, since the object is still shared --all \nreferences to tile object point to the same thing. The same is true for a recursive function, since it \nremains intact because no simplifications are made across the Y combinator. ttowever, this is not true \nwith non-recursive functions, in that the body of the function is essentially duplicated wherever it \nis applied. This does not result in a detect- able common-subexpression, but rather creates a common \ninterior portion of the graph that represents the function body. Of course our reason for doing all this \nin the first place is to induce as much constant-folding as possible, and to eliminate the overhead of \na procedure call. The two chief complaints about large amounts of code are that it places a strain on \nthe address space and that it might induce excessive paging. Note, however, that when executing a program \nby normal-order graph reduc- tion (for example as described in [4] or [lg]), the same code explosion \noccurs, except that it only happens if the function is actually applied, and the garbage collector may \ncollect the space at some point after application. In essence we are trading static space for dynamic \nspace, and trading utilization of the address space for load on the garbage collector. The paging performance \nwould be roughly the same in both cases. Even though the code explosion only happens with non- recu~ive \nfunctions, we view it as a problem, and would like to avoid it. One obvious solution is to be judicious \nas to when we integrate a function. A much better solu- tion, and the one we are pursuing, is to integrate \nall procedures as we have described, and then abstract out common interior subtrees. Generally not all \ncommon in- terior subtrees would be abstraeted. For example, im-plementation details may indicate that \nthe extra code caused by a function call offsets the gain made by abstracting a particular function. \nAlso, user supplied per- formance requirements may dictate that the time over-head of a particular function \ncall cannot be tolerated. 126  We are currently investigating ways to detect and abstract common interior \nsubtrees. Efficient algorithms have been developed and are being integrated into the compiler. The results \nof this work to date, however, are too preliminary to report here. Another disadvantage of our approach \nis that the com- pilation strategy is not guaranteed to terminate! This seems surprising, since we don't \nperform any Y reduc- tions, which correspond to the recursions that are nor- mally the cause of non-terminating \nprograms. The problem can arise, however, if one performs certain forms of self-application. For example, \nthe simple ALFL program: {fx:=xx; result f f } corresponds to the eombinator expression S I I (S 1 I) \n(and the ]ambda expression (kx.xx)()~x.xx)), and does not terminate, since our applicative-order partial \nevaluator applies f to f, reducing to an expression trying to do the same thing. We do not view this \nis a serious problem, for two reasons: First, self-application is a rare programming event, and not all \nself-applications will fail to terminate (we have never encountered a non-terminating compilation). Second, \na program that does not terminate during compilation will probably not ter-minate when executed. 4. Phase \nH: Comblnator Translation and Code Generation Once the optimized combinator expression is con-structed \nas described in the last section, how is it ex-ecuted? One could use a normal-order reducer such as described \nin [19], but this approach has certain annoying inefficiencies. The primary problem is the way in which \nactual parameters are bound in function calls. Consider a variable v that appears at depth n in the body \nof a func- tion (by depth we do not mean lexical depth --rather, we mean the depth in the expression \ntree). When the cor-responding combinator expression is applied to a value for v, at least n reductions \nare required before the value arrives at its proper place (and more if the variable ap- pears in more \nthan one place). 7 In a standard procedure call on a conventional machine , the value of v Would be loaded \ninto a register or stack location from which it is directly accessible (perhaps through a small number \nof indirections). Clearly the latter is much more efficient. 8 TDespite this inefficiency it is conceivable \nthat if the reduction mechanisms were made an integral part of a machine's architecture, combinator reduction \ncould be a feasible alternative to conventional machines [4]. Furthermore, most implcmentors of normal-order \ncom- binator reduction (or, for that matter, lambda reduction) make two iml)licit assumptions that make \nthe job easier: First, it is :L~sumed that all expressions must be com- puted lazily, so that the normal-ordering \nof the redexes can be followed \"blindly\". The overhead paid for this high, although can be subtly hidden \nin the reduction strategy. Second, all expressions are assumed to be shared, which means that all intermediate \nresults are \"stored away\" by overwriting the appropriate vertices in tile graph, r(~gardless of whether \nthey will be needed or not. The converses of the above arguments, of course, point out the advantages \nof graph reduction. The variable binding mechanisms obviate the need for an environment structure, and \nin those cases where expressions must be treated lazily or shared, the graph reduction strategy works \nquite well. The approach we have taken is to com- bine the best features of both strategies, using a \nconven- tional machine as our target architecture. 9 Specifically, our first goal is to utilize the efficient \nvariable referencing mechanisms of standard procedure-calling strategies. Furthermore, although we believe \nin making normal-order evaluation the default conceptually in a functional lan- guage, we also recognize \nthat most applications do not re- quire it. 10 Since applicative-order reduction can be imple- mented \nmore efficiently on conventional machines, a great savings would result by detecting those cases where \nlazy evaluation is not needed. Finally, empirical studies show that only a small percentage of all objects \nare ac-tually shared in most programs [3], so another significant savings would result by retaining the \nvalues of only those objects that are shared. Phase II of our compiler accomplishes the above op- timizations \nby making four passes over the program, starting with the combinator expression from Phase I and ending \nin machine code: I. Phase lla: An environment structure is em- bedded into the optimized eombinator ex-pression \nby converting it into a nested graph of lambda expressions that we call maero-eomblnators. (See Section \n4.1.) SAlthough the study in [10] seems to indicate that combinator reduction does about as well (if \nnot better) than lambda reduction, care must be taken in drawing any conclusions from this, for implementation-dependent \nreasons. For example, the lambda reducer used in that study employed an ~sociation-list lookup strategy \nto resolve variable references, which is clearly non-optimal, since one can determine at compile*time \nthe precise lexical environment and loca-tion in that environment of each variable. \u00b0Another way at looking \nat our approach is that we are trying to combine the best features of a pure \"fixed-program machine\" \nwith those of a pure \"substitution*, or \"reduction machine\". IOThe success or conventional languages \nis sufficient testimony to this fact! 2. Phase lib: Two simple analyses are then done, one to locate \nall 8hares (those expres- sions whose value might be shared, which is more than just the set of common \nsubexpressions), and the other to determine in which variables and shares each macro-corabinator is strict. \n{See Section 4.2.) 3. Phrase llc: In order to observe a stack dis- cipline for function calls whenever \npossible, an analysis is made to determine when the extent of a variable or share might exceed the lifetime \nof its activation record. (See Section 4.3.) 4. Phase lid: Code generation: \"Lazy\" shai~es and arguments \nare implemented as self-modifyin 9 thanks to preserve normal-order semantics, heap-allocated environments \nare created when necessary, and a few other standard optimizations are done to improve code efficiency. \n(See Section 4.4.)  Each of the above passes is described in detail in the fol- lowing sections. 4.1. \nUneurrying the Combinator Graph In order to compile the optimized combinator graph into code for a conventional \nmachine, we first need to uncurry 11 all functions in order to gain the efficiency of passing more than \none argument in a function call. In anticipation of this, as the combinator expression is con- structed \nin Phase I, we maintain a redex-arlty for each subexpression, defined to be the number of arguments needed \nby that expression to form a redex at the out-ermost level. For example, the redex-arity of S is 3, of \nS (K f) is 2, and of \"t- x is 1. Note that the redex-arity of an expression may actually increase after \nit is applied and reduced, as with K S. Now consider a combinator expression expl having a redex-arity \nof at least 1. If expl is applied to a series of \"dummy variables\" until the redex-arity of the resultant \nexpression is zero, we will have embedded variable names into the expression. If x I through x n are \nthe dummy variables supplied, and expl w is the resulting expression, then the combinator expression \nexpl corresponds to the lambda expression ),xl...x n . expl'. Next consider the combinator expression \nY exp2, whose redex-arity is zero. The redex-arity of exp2 must be at least one. To obtain a lambda expression \nfrom this we first apply exp2 to a \"dummy function\" f, and then supply dummy arguments as above. From \nthis we even- tually obtain a recursive equation defining the function f; viz.: f ~ ~Xl...X n e xp2 \nw We refer to functions so generated as macro-eombin ators. Phase lla of the compiler essentially converts \nall expres- sions in the comhinator graph to lambda expressions, un- covering macro-combinators in preparation \nfor code generation. The above algorithm is used to do this, ex-cept that the sharing of higher-order \nfunctions must be maintained. For example, the combinator graph shown in Figure 4-I contains a subexpression \nY fun ezpl that is shared by two nodes {in this figure each node is an apply node, the left branch being \nthe function, the right the argument), if f-),xl...Xn,eXp is the macro-combinator corresponding Y fun, \nthen it is transformed into f ~ Xx l.(Xxs...xn.exp) as shown. Y f-n Figure 4-1: Shared lligh-Order Functions \n We refer to the result of this pass of the compiler as the maero-combinator graph, or MCG. The MCG is \nbuilt with slots to be filled in with optimization infor-mation gleaned from the next two passes, in \npreparation for a final pass to perform code-generation. Also in this pass a free-variable list is maintained \nfor every subexpres- sion as the dummy arguments are supplied --this is needed to compute shares as described \nin the next sec-tion. 4.2. Computing Shares and Converting Normal-order to Applicative-order Evaluation \nPhase lib locates shares in a top-down pass over the IVICG, and then bottom-up in the same pass computes \nin- formation to convert normal-order to applicative-order evaluation wherever possible. 4.2.1. Shares \nConsider the function (macro-combinator) f --'~ Xn. if n=fac{x) then 1, else f(n-l), where fan is the \nfactorial function, and x is free. This lambda expression represents a closure for f, which can only \nbe fully created at run-time since x may take on different values depend- ent on the particular invocation \nof the context in which it is defined. A conventional implementation of f might compute fac{x) every \ntime the closure is called. On the other hand, a fully lazy graph reduction strategy would only compute \nit the first time the closure is called, shar- ing the result with subsequent calls. This effect of \"code \nllOur use of this word is quite a bit different from that in [19], but our use is more faithful to \"undo \nthe effects of currying\". motion from a loop\" (with the recursion behaving as the loop} comes free in \ngraph reduction, but requires a little more work in a more conventional implementation. Specifically \nwe must identify, for each macro-combinator f having set of bound variables args, the largest sub- expressions \nin the body of f whose free-variable list is dis- joint from args 13 {f}. This is easily computed by \na top- down scan of the MCG --when a maero-eombinator is encountered its arguments are remembered and \ncompared to the free-variabh lists of subexpressions found in the body. The top-down traversal guarantees \nthat the first subexpressions thus found are maximal. These expressions are similar to the maximally-free \nexpressions discussed in [7]. In our implementation they are treated in the same way as common subexpressions, \nand we refer to them collectively as shares. Storage is allocated for a share's value in the local environment \nof the outer-most surrounding macro-combinator from which all of the share's free variables can be accessed. \nThe notation shares~f] denotes the set of all shares al- located space in rs environment. Conventional \ncode-motion strategies would move all shares outside of (i.e., \"in front of') any loop (recursion) in \nwhich they lie. This is satisfactory under an applicative-order evaluation, but under normal-order such \ncode motion may result in evaluating something that was not intended to be, and the semantics may be \nviolated if that evaluation fails to terminate. Thus code motions are performed only if that function \nis strict in the computa- tion of that share; 12 that is, if it can be determined that the share's value \nwill be computed if that function is called and terminates. A share that is not strictly com- puted we \ncall a lazy share, for which it becomes neces- sary to dynamically create an object that can be evaluated \non demand, and whose result can be shared. We accomplish this with \"self-modifying thunks \", to be described \nin Section 4.4. The one other by-product of this pass of the compiler is the computation of a free-object \nlist for every sub-expression --free-obs~exp~ denotes the set of all free variables and free shares contained \nin the expression exp. 4.2.2. Function Strictness Whether or not a maero-combinator is strict in one \nof its shares, of course, is not all we wish to determine. Using applicative-order evaluation, a conventional \ncall to a function f would involve computing the required num- ber of actual parameters, pushing them \nonto a stack, and IlMathematically speaking, a function f(xs,xs,...,xn) is strict in x i it f'(x i ..... \nXi.i,.l.,xi+ 1 ..... xa) --_L for all values of xp i#i. [17] We extend this definition somewhat by saying \nthat a function may be strict in one of its subezpressions, namely a share. jumping to tile code for \nf. llowever, lazy evaluation re- quires that one push some sort of closure for an ar-gument, so that \nit is not computed unless the called func- tion actually needs its value. Since, as disussed earlier, \nmost functions do indeed evaluate their arguments, there is room for optimization here. To avoid the \noverhead of a closure we need to determine in which of its arguments each macro-combinator is strict. \nIn addition to locating shares, Phase lib determines the strictness information that we need. 13 This \nis done through a simple bottom-up analysis by first computing the used-objects list of every subexpression, \ndefined as follows: 1. used-obsllc]] = D, where c is a constant. 2. used-obs[Iv ] ----- {v}, where v \nis a variable.  &#38;used-obsIIx op y] = used-obs~x] U used-obsIIy~, where op is any strict binary operator. \n4. used-obs~op x]l = used-obs~x~, where op is any strict unary operator.  5. used-obs~if x y z~ = used-obs~x~ \nU (used-obs~,~ N used-obsesS)  6. used-obs[]f argl...argn~ = if f is a macro-eombinator ),Xl...Xn.eXp, \nthen {Xl,...,Xn,f}; if f is a primitive function with args xl,...,xn, then {f} U used-obslIarg i] for \neach x t in which f is strict; otherwise O. 7. used-obsllf:~,xl...x .body] ----- used-obs~body~ - ~nXl,...,Xn)f} \n- sharesllf]   Intuitively, used-obslln~ contains all variables and shares whose value will definitely \nbe computed if n's value is computed, with one important exception: In order to determine the strictness \nof recursive functions in a single bottom-up sweep over the MCG, the used-object list of a function call \nto a macro-combinator is simply the set of bound variables for that function (see rule 6). The idea is \nthat if such a variable v propagates to the used-object list of the body of the function r having v as \nan ar-gument, then r must be strict in v. More specifically, the strict-object list of a macro-combinator \nf is defined by: strict-obs [lf=Xxi...x n.body]] = used-obs~body]] CI ({xl,...,x n} U shares[q) This \nstrategy allows us, for example, to determine that this tail-recursive version of factorial is strict \nin both of its arguments: file n s == n=O --. Sr fac (n-l) (n'a) 13Earlier designs of our compiler performed \nshare detection and strictness analysis on the combinator graph itself. Since our target machine requires \na lambda representation, it turned out to be more convenient to do the analysis on the MCG. 4.3. Escape \nAnalysis tlere is the code skeleton for a macro-combinator f: The purpose of Phase He is to determine \nwhen the ex- tent of a variable or share might exceed the lifetime of the activation record in which \nit resides. Many of the issues faced here are not too different from those faced by implementors of any \nlanguage that treats functions as \"first-class citizens\" and in addition uses a stack im-plementation \nof function calls for efficiency. 14 We say that a variable or share escapes the context (i.e., macro-combinator) \nin which it is defined if it is a free object in either a function appearing in a non-functional position \n(i.e., it appears in a function that is not in the function position of function call), or it is free \nin the expression for a lazy argument in a function call (which includes the arguments to a CONS). Phase \nllc simply walks through the MCG, marking all variables and shares that escape, which for a macro-combinator \nf, is denoted escapes~f~. 4.4. Code Generation: Self-modlfylng Thunks At this point in the compilation \nwe are ready for code generation. To summarize, from the previous three passes the following information \nis known about every macro-combinator f: 1. args[Tf~ -- the arguments of f. 2. shares~f] --the set of \nshares allocated in f's local environment.  3. free-obsfff~ --the set of free variables and free shares \nin f (which is disjoint from args[~q U shares[lf~). 4. strict-obs[If] -- the set of arguments and local \nshares that f strictly computes (a subset of args[Tf]] U shares[Try). 5. escap,e-obs~f~ --the set of \narguments and local shares that escape from f (a subset of args[If~ U shares~f~).  Also, for each subexpression \nexp we know free-obs1[exp~. Phase IId generates code for each macro-combinator, which can be thought \nof as generating code for a conven- tional function with a local environment of argument values and shares. \nThe default environment mechanism is a stack-allocated activation record, with access to ob- jects in \nsurrounding contexts accomplished through a register-allocated display [1]. The environment for func- \ntional values is kept on the stack whenever possible (dictated by the escape analysis) --when not possible \na mini-environment for that object is ceased in the heap. <entry code> <for each share s E shares[q- \nstrict-obs[f~, creaW self-modifying-thunk for s> ;;; for lazy shares <for each share s E strict-obs[f~, \ncompute its value> ;;; for strict shares <code for body> ;;; for the body of f <exit code> Shares are \naccessed from lower contexts by indexing through the appropriate display register. If it is a strictly \ncomputed share, its value may be directly fetched. If it is a lazy share, the self-modifying-thunk is \nused to compute its value, in a way to be described shortly. Now consider a function call f exPl ... \neXPn where f is a macro-combinator kXl...xn.body. If x i E strict-obs~f~ then the value of exPl is computed \nprior to calling f. On the other hand, if x t ~ strict-obs[f]], then a self-modifying-thunk is created \nfor the computation of exPl , just as was done for a lazy share. (Of course, as a simple optimization, \nif the value of expl is already known (such as for a variable or constant), it can be pushed onto the \nstack directly.) How is a self-modifying-thunk for an expression x created, and how is it evaluated? \nFirst we must use es- cape analysis to determine whether its environment needs to be conscd in the heap. \nIf x is a share, this arises if x E escape-obs[rf]], where f is the macro-combinator in which x is defined. \nIf x is the ith argument in a call to a macro-combinator g, this arises if the correspon'ding for- mal \nparameter xl E escape-obs[Ig~. In either case there can be no guarantee that the objects on which x depends \nwill be on the stack when it is eventually evaluated. A mini-environment env containing free-obs~x~ is \nceased in the heap, along with a closure containing a pointer to env and a pointer to the code for x. \nThis allows us to share environments ceased in the heap. The strategy is depicted in Figure 4-2a, both \nbefore and after evaluation, where \"X\" is the stack location (either local or in a sur- rounding context) \nof x, and \"smt tc\" is the type-code of a self-modifying-thunk. Note that once the value is com- puted, \nthe stack location for x and the env slot in the closure are overwritten with the result. In addition, \nthe code-pointer of the closure is overwritten with a pointer to an identity function that simply retrieves \nx's value from the env slot if the thunk's computation is at-tempted by some other reference. 15 a41a \nparticular, our problem is similar to that faced by implemen- 15This could be viewed as \"self-modifying \ncode\", normally frowned tors of lexically-seoped LISPS such as SCHEME [16] or T [12]. upon, but in a \npurely functional setting there is no harm -indeed the object never changes in functionality, and in \na sense is not modified at all. If x does not escape, then (1) all of the objects needed to compute its \nvalue will be somewhere on the stack at the time x is evaluated, and (2) all references to x will be \nto the same stack location. Thus there is no reason to create a mini-environment, and no reason to create \na closure. Instead, the self-modifying-thunk simply con-thins a pointer to the code for x, where accesses \nto x's Before: smt in stack: closure in heap: ...... , thunk After: smt in stack: closure in heap: value_ \n/ X value *---~-->code ' \" for identity function (a) Escaping Thunk Before: smt in stack: X! : r smt_tc \n*..... >code After: smt in stack: X value I ' (b) Non-escaping Thunk Figure 4-2: Self-Modifying Thunks \nfree objects are made through the display as in a normal function call. The value thus computed is simply \nwritten into the stack location for x. Figure 4-2b depicts this situation. 5. Results Everything described \nin Sections 3 and 4 has been im- plemented, although space precludes providing further details. The compiler \nwas written in T[12], and generates MACRO code for the PDP-IO. Our primary goal was to demonstrate the \nfeasibility of the ideas described in this paper, and we thus paid little attention to more conventional \noptimizations. Nevertheless, the results are very encouraging. As can be expected, execut- ing our compiled \ncode is a vast improvement over execut- ing combinatory code using a normal-order reducer. Ilowever, \nit is difficult to make comparisons with, for ex- ample, LISP code, for two rea.sons. First, as mentioned \nabove, we have ignored more traditional optimizations --a just comparison is difficult to make until \nwe do bet- ter at such things as register allocation and tail-recursion optimizations. Second, such comparisons \nare very problem-dependent. For example, in cases where an AI,FL program packages data into lists to \npass between procedures, the equivalent I,ISP program naturally incurs a greater overhead. Our compiler \nalso does better in han- dling \"lazy lists\", which must be implemented as explicit closures in l,ISl \n). On the other hand, lists that do not re-quire lazy evaluation are handled more efficiently in LISP. \nSimilarly, our compiler does worse when it cannot determine when an argument to a function is strictly \ncomputed, such ~.s a in: g a == { result f 4; f x =--x=O -~ a, f (x-a) } As is true of most compilers, \nours has much room for improvement. Many of the optimization analyses are too conservative, and as mentioned, \nwe paid little attention to certain more conventional optimizations. We see the following as particular \nareas for improvement: I. The common interior subtree analysis described in Section 3.3.2 needs to be \nimple- mented. 2. The strictness analysis can be improved. For example, the strategy in Section 4.2 \ncan be extended by adding an extra pass over the MCG in which the free objects in the ar-guments in a \nfunction-call are included in the used-obs analysis (this is sufficient to deter- mine that g defined \nabove is strict in s). Or, the technique described in [11] could be used for a more thorough analysis. \n 3. The escape analysis can also be improved, as well as the mechanism for ceasing environ- ments onto \nthe heap. Better safety analyses [9], environment structures [2], and calling strategies [14] could \nimprove the handling of closures. 4. Commutative, associative, and distributive laws could be used to \nimprove code genera- tion as well as to reorganize shares (to make them larger, for example}. 5. Certain \nother conventional optimizations could have a great impact, in particular con- verting tail recursions \ninto loops, and doing a better job of register assignment and alloca-  tion. 131 6. Relationship to \nOther Work We received most of our inspiration from Turner's work with combinators and normal-order graph-reduction \n[19, 20]. Our shares (except for common subexpressions) are similar to the maximally-free-expressions \nidentified by Hughes cite[hugh82], although we make no attempt to make them larger. Also, if the shares \nare abstracted out from our macro-combinators (that is, made arguments to the function), then one essentially \ngenerates llughes' super-combinators, although they are arrived at from quite different approaches. There \nis no need in our im- plementation t'o abstract the shares, since they are acces- sible via the environment \nstructure; i.e., by indexing through a display register. It should be noted that the \"fixed-program\" \ncode described in [10] is simply an itera- tive version of standard graph reduction, and does not have \nthe fixed-program flavor that our compiler pos- sesses. Some of our compilation techniques were in-fluenced \nby Steele's work with lexically-scoped LISPs [15]. 7. Acknowledgements Thanks to Bob Paige for helpful \ncomments, to Cathy Van Dyke for her support, to Cristina for her inspiration, to Leslie for the details, \nand to the Macaroni Group at Yale (especially Jonathan Rees, Norm Adams, and Jim Philbin) for many a \nmouthful. References [1] Aho, A.V. and Ullman, J.D. Principles of Compiler Desigu. Addison-Wesley, 1977. \n[2] Bobrow, D.G. and Wegbreit, B. A model and stack implementation of multiple en- vironments. CACM 16(10):591-003, \nOctober, 1973. [3] Clark, D.W. An empirical study of list structure in LISP. CACA4 20(2):78-87, February, \n1977. [4] Clarke, T., Gladstone, P., Macl,ean, Norman, A. SKIM - the S, K, 1 reduction machine. In Davis, \nR.E., and Allen, J.R. (editors), The 1980 LISP Conference, pages 128-135. Stanford University, August, \n1980. [5] Curry, H.K., and Feys, R. Combinatoroy Logic. Noth-Holland Pub. Co., Amsterdam, 1958. [61 \nHenderson, P. Functional Programming: Application and Im- plementation. Prentice-Hall, Englewood Cliffs, \nN J, 1980. , [7] Hughes, R.J.M. Super-combinators: A new implementation method for applicative languages. \nIn Park et al. (editors), Sym. on Lisp and Func- tional Prog., pages 1-10. ACM, August, 1982. [8] Keller, \nR.M. FEL programmer'8 guide. AMPS TR 7, University of Utah, March, 1982. [9] McDermott, D. An efficient \nenvironment allocation scheme in an interpreter for a lexically-scoped LISP. In Davis, R.E., and Allen, \nJ.R. (editors), The 1980 LISP Conference, pages 154-162. Stanford University, August, 1080. [10] Muchnick, \nS.S. and Jones, N.D. A fixed-program machine for combinator expres- sion evaluation. In Park et al. (editors), \nSym. on Lisp and Func- tional Frog., pages 11-20. ACM, August, 1982. [11] Mycroft, A. Call-by-need .~- \ncall-by-value 4- conditional. Draft, Dept. of Comp. Sc., Univ. of Edinburugh, 1981. [12] Rees, J.A., \nand Adams, N.I. T: a dialect of LISP or, Lambda: the ultimate software tool. In Park et al. (editors), \nSym. on Lisp and Func- tional Prog., pages 114-122. ACM, August,1982. [13] Schonfinkel, M. Uber die bausteine \nder mathematischen logik. Mathematische Annalen 92:305, 1924. [14] Steele, G.L. Debunking the expensive \nprocedure call myth. In Proc. ACM National Conference, pages 153-162. ACM, 1977. [15] Steele, G.L. RABBIT: \nA Compiler for SCHEME. AI 474, MIT, May, 1978. [16] Steele, G.L. and Sussman, G.J. The Revised Report \non Scheme. AI 452, MIT, January, 1978. [17] Stoy, J.E. Denotational Semantics: The Scott-Strachey Ap- \nproach to Programming Language Theory. The MIT Press, Cambridge, Mass., 1977. [18] Turner, D.A. SASL \nlanguage manual. Technical Report, University of St. Andrews, 1976. [19] Turner, D.A. A new implementation \ntechnique for applicative languages.\" Software-Practice and EXperience 9:31-49, 1979. [20] Turner, D.A. \nAnother algorithm for bracket abstraction. The Journal of Symbolic'Logic 44(2):267-270, June, 1979. 132 \n  \n\t\t\t", "proc_id": "800017", "abstract": "", "authors": [{"name": "Paul Hudak", "author_profile_id": "81100539650", "affiliation": "Yale University, Department of Computer Science", "person_id": "PP40028396", "email_address": "", "orcid_id": ""}, {"name": "David Kranz", "author_profile_id": "81100273046", "affiliation": "Yale University, Department of Computer Science", "person_id": "PP39035469", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/800017.800523", "year": "1984", "article_id": "800523", "conference": "POPL", "title": "A combinator-based compiler for a functional language", "url": "http://dl.acm.org/citation.cfm?id=800523"}