{"article_publication_date": "01-15-1984", "fulltext": "\n Implementation of an Interpreter for Abstract Equations Christoah M. tlcffmann Michael J: O'Donnell \n Purdue University The Johns Hopkins University ABSTRACT This paper summarizes a project, introduced \nin [HO79, I-IO82b], whose goal is the implementation of a useful inter- preter for abstract equations \nthat is absolutely faithful to the logicalsemantics of equations. The Interpreter was first dis- tributed \nto Berkeley UNIX VAX sites in May, 1983. The main novelties of the interpreter are (1) strict adherence \nto semantics based on logical conse- quences; (2) \"lazy\" (outermost)evaluation applied uniforml~ (3) \nan implementation besed on table-driven pattern match- in8, with no run-time penalty for large sets of \nequa- tions; (4) strict separation of syntactic and semantic processing, so that different syntaxes \nmay be used for different prob- lems.  I. Introduction The prime motivation for the equation interpreter \npro-jeet was to develop a programming language whose semantics can be described completely in terms of \nsimple mathematical concepts. We chose equations as the notation for the project because E --F has (1) \nan obvious mathematical Interpretation -E and F are different mines for the same thing, (2) a natural \nand simple computational interpretation -replace E by F whenever possible, and (3) well-documented theoretical \nresults on the equivalence  of these two interpretations Church-Rosser or confluence theorems. A/to, \nram for the equation interpreter Is a list of symbols to be used, followed by a lbt of equations involving \nthose sym- bols and variables. The mean/rig of a program is completely described by the following: This \nresearch was supported in pert by the National S~ierce Foundation undergrams MCS 78-01812, and MCS 82-17996. \n Permission to copy without fee all or part of this material is granted provided that the copies are \nnot made or distributed for direct commercial advanhage, the ACM copyright notice and the title of the \npublication and its date appear, and notice is given that copying is by permission of the Association \nfor Computing Machinery. To copy otherwise, or to republish, requires a fee and/or specific permission. \nO 1983 ACM 0-89791-125-3/84/001/0111 $00.75 Definition 1.1 A term containing no imtance of a left-hand \nside of an equa- tion is in normal form. An interpreter for a set of equations is a program that, given \nan input term E, produces a term F, in normal form, such that E--F is a logical consequence of the equations, \nif such an F exists. If no such F exists, the interpreter must not produce output. 1.1 Syntax of the \nEquational Programming Language Input to the equation interpreter is in the following form: Symbols \nsykes ,; on*des ~; symdesm * For all vat i,var 2, \" \" \" vm'n: equn z; equ n 2; equn v Symbol descriptors \nindicate one or more symbols in the language to be defined, and give their arities. Intuitively, symbols \nof arity 0 are the constants of the language, and symb()ls of higher arity are the operators. A s~zbol \ndescriptor (snndes) is either of the form s~=,.wm2, '  symm: amy m )1 or of the form Include s~nclass \n=,  .wmcla~n n 91 Syntactically, ~nbols and ~nbol classes (symclass ) are identifiers. A symbo ! class \nindicates the inclusion of a predefined class of symbols. The classes available are atomic_symbols, Integer_numerals, \nand truth values. Symbols that have been explicitly declared in the Symbols section are called literal \n~ym~ls, to distinguish them from members of the predefined classes. Variables (vat) are identifiers, \nof the same sort as sym- bols. An equation (equn) is either of the form term l=\"/etTfl2 of the form term \nz~-term 2 where qualification end where 111 or of the form Include equclass I, \" ' \" equclaSSm Equation \nclasses (equclass) are identifiers indicating the inclu-sion of a large number of predefined equations. \nThese classes include the defining equations for the standard arith- metic operations. For example, addition \nis defined by' add(1,l)=.2, add(l,2)=3, etc. Of course, such equations are not stored ,explicitly, but \ntheir effect is produced by efficient machine opemtions. A qualification is of the form quahtem ~,  \nqualitem m m ~ I l arid qua/items are of the forms var i~ qua~term vt~ I, ' \" \" var~ are qua/term and \nqua/terms are of the forms in predefined_s~tbol__class term qualterm where qual(fication end where either \n~altermj or.  qua/term, end or Qualifications, as defined above, restrict the ranges of variables to \nterms of given forms. Normally, variables range over all terms in the language de.,cribed by the Symboh \nsec- tion\u00b0 Different syntaxes for terms may be chosen to suit different problems. In this paper, we will \nuse two different term syntaxes: 1) the standard mathematical notation in which function application \nis denoted f(a,b), and 2) LISP notation, in which such application is fla;b], and parentheses are used \nto abbreviate uses of the special binary operator cons to build lists and trees. A lambda notation is \nalso avail- able, and it is straightforward to add yacc and lex programs for other syntaxes as needed. \nThe following example illustrates all of the constructs described above, using LISP notation. Example \n!.1 Symbols : List constructors cons: 2; nil: 0; : Standard arithmetic operators add: 2; : nonils[x] \nis the list containing all of the nonnil : elements of the list x no nils: l; : leafcountlx] is the number \nof leaves in the tree x leafcou nt: I; include atomic_symbols, integernumerals. For all x, y, z, rein: \nnonils[()] --(); nonils[(() . rem)] --nonils[rem]; nonilsl(x, rem)] --(x. nonilslrem]) where x is either \n(y. z) or in atomicsymbols end or leafcou nti()l --0; leafcountl(x, y)l --add[leafcount[x]; leafcountly]]; \ninclude addint. Notice that the symbols cons, nil, and add must be declared. They appear implicitly in \nthe equations. (x. y) is merely an abbreviation for cons[x;y]. \"include addint\" is semantically equivalent \nto the set of equations defining addition of integer numerals. In order for the reduction strategies \nused by the equa- tion interpreter to be correct according to the logical-consequence semantics, some \nrestrictions must be placed on the equations. Presently, 5 restrictions are erfomed: I. No variable may \nbe repeated on the left side of an equa- tion. For instance, !l( x ,y ,r )=y is prohibited, because of \nthe 2 instances of y on the left side. 2. Every variable appearing on the right side of an equa- tion \nmust also appear on the left. For instance, f(x).,,y is prohibited. 3. Two different left sides may \nnot match the same expres- sion. ~ the pair of equations g(0,x)=0; g(x,I)=l is prohibited.  4. When \ntwo (not recessarlly different) left-hand sides match two different parts of the same expression, the \ntwo parts must not overlap. E.g., the pair of equations  f lrst (pred (x ) )--pred f unc ; pred ( ~ucc \n( x ) )--x is prohibited, since the left-hand sides overlap in .first (lied (suet (O)). 5. It must be \npossible, in a left-to-right preorder traversal of any term, to identify an instance of a left-hand side \nwithout traversing any part of the term below that instance. For example, the pair of equations f(g(x,a \n),y )==O; g(b,c)--t is prohibited, since after scanning f(g it is impossible to decide whether to look \nat the first argument to g in hopes of matching the b in the second equation, or to skip it and try to \nmatch the first equation. Sets of equations satisfying 1-4 above are called regular. The property described \nin (5) is strong left--sequentiality. Viola. tions of strong left-sequentiality may often be avoided \nby reordering the arguments to a function. Strong left-seque ntiality is treated in more detail in Section \n4. Restrictions 1-4 guarantee that normal forms are unique, and that outermost \u00a2~,eluation will find \nell .norn~l forms. Restriction 5, which technically subsumes the other four, will be removed in a later \nversion with an implementa- tion of parallel evaluation. Restriction 3 will also be relaxed to allow \ndifferent left-hand sides to match when the corresponding right-hand sides agree, as in or(True ~c )==True \n, or ( x ,True )==True.  1.2 Contents The remainder of the paper discusses the mawr ideas of the pro~ect \nand its execution, as well as our experience with using the system. Section 2 gives the history of the \nproject design decisions and their consequences. Section 6 discusses 3. Semantic Strictness and its Consequences \nour experience in using the system, and Section 7 demon- The first and foremost design decision was to \nbe abso- strates one of the programming adva,=tages of the outermost lutely faithful to the logical semantics. \nThe only type of (lazy) evaluation strategy. failure tolerated in the process of reducing a term to normal \n2. History of the Project The logical foundations of the pro k~ct, results concern- ing uniqueness of \nnormal forms and correct orders of evalua- tion, come from [O'D77]. From 1978 to 1981, work toward an \nimplementation, both in theoretical development of algo- rithms and in the building of prototype systems, \nwas per-formed by the authors, with programming aid from two stu- dents, Giovanni ~ and Paul Golick lHO82b]. \nFinal preparation for distribution was done by O'Donneli in 1982- 1983, while Hoffmann ported an earlier \nversion to Kiel, Ger- many, and led two projects involving alternative approaches to pattern-raatching \nin the interpreter, and use of the inter- preter to define interpreters and compilers for PASCAL. Among \nother nonprocedural programming languages, the one which is very similar in flavor to the equation inter- \npreter is PROL(XI [Ko79]. PROLOG accepts Horn clauses in the first-order predicate calculus as programs. \nAll existing PROLOG interpreters and compilers are incomplete -they sometimes fail to produce output \neven though an output fol- lows logically from the program. This is acortc, equence of PROLOG's computationaUy \nvery expensive semantics: a complete implementation of PROLOG requires a breadth-first evaluation of \nthe proof tree, but this would require an unacceptable amount of space. PROLOG implementors have therefore \nchosen to evaluate the proof tree depth-first, with back tracking. This evaluation strategy introduces \na pro- cedural element absent in the strict semantics, and is respon- sible for the implementations' \nfailure to produce logically enta,led output in certain cases. Since equation semantics is computationally \nmuch simpler, our equation interpreter can produce all of the logically entailed outputs without making \nunacceptable resource demands. Another language processor sirnilar syntactically to the equation interpreter \nis HOPE [BMS80], which also uses equations as programs. HOPE has more stringent restrictions on equations \nthan our interpreter. For example, HOPE:dis-tinguishes function and constructor symbols and prohibits \nequations in which subexpressions involve function symbols. This restriction greatly simplifies the pattern \nmatching required to find instances of equation lefthand sides. We believe that in view of our pattern \nmatching algorithms such a simplification does not lead to a significant performance improvement. HOPE \nuses conventional innermost evalua- tion, instead of lazy evaluation, for all operators except the conditional \nand cons. SO, it is possible to write equations, involving constructors other than cons, for which HOPE \nwill fail to find a logically entailed normal form because it follows an irrelevant infinite evaluation \nof a subterm not included in the fi~tl output. There have been hybrid approaches to equational pro- gramming \nin which the equations are assigned a priority, e.g., [CIMWS0, MonS0]. If several reductions are possible \nat the same position, then the one whose equation has the highest priority is chosen. Such programming \nsystems do not have a neat, well-understood semantics, but their proponents con-sider them easy to use \nand of practical importance. If one were to compare this approach to ours, it should be remem- bered \nthat we wish to obtain a practically useful programming system without sacrificing semantic rigor. form \nis exhaustion of the available space re:wmrces. The main consequence of this decision was the necessity \nof implementing lazy evaluation uniformly. Nearly all program- ruing languages evaluate conditionals \nin this way, and like treatment of the LISP function cons has been proposed in [HM76, FW76, BMS80I, but \nwe kr~w of no other language processor which implements lazy evaluation in all ~s. Kahn and McQueen IKM771 \nhave a PASCAL-like dataflow language in which all communication between coroutines is performed in a \ndemand-driven fashion equivalent to lazy evaluation, but expressions inside routines are evaluated con- \nventionally. Lazy evaluation has advantages for the user, allowing straightforward use of a certain type \nof parallel pro- gramming. [FW76, HM76] demonstrated some of these advantages in the case of LISP. Section \n7 shows how lazy evaluation automatically performs one of the design tasks in dyrumic programming which \notherwise must be explicitly programmed. Amther important cortc, equence of semantic strictness involves \nthe inclusion of efficient machine operations as primitives. Take, for example, integer addition. In \nprinciple, addition may be defined from zero and successor by equa- tions such as add(0,x) --x add(s(y),x) \n--s(add(y,x)) These equations produce an addition that is semantically correct, but unacceptably inefficient. \nThe conventional course is to invoke the machine's addition operation to evaluate add(i,j) whenever i \nand j are integer numerals. The latter course is efficient, but cannot be explained very well by logical \nconsequence semantics because of the possibility of overflow. In the equation interpreter, we may combine \nthe good points of both approaches. What the machine addition really does is to implement the large, \nbut finite set of equa- lions add( I, I )--2, add( 1,2)-..3, etc., representing those addi- tions not \ncausing overflow. Those machine-implemented equations may be augmented by equations for addition in ba.~. \nmaxint, where maxim is the largest integer represented by the machine. Thus, the user has the benefit \nof the pre- cise semantics of integer addition on arbitrarily large nun~bers, and the efficiency of machine \naddition in the usual ease where the numbers are not large. Hecause of lazy evaluation and the pattern-matching \ntechniques described in Section 4, single-precision arithmetic does not have to pay the overhead of checking \nwhether the inputs are single preci- sion. The current version of the equation interpreter allows use \nof the machine-implemented single-precision arithmetic, and leaves to the user the definition of multiprecision \narith- metic. Operations that, in conventional programs, would cause overflow, are simply not performed, \nso that the even the user who has not written multiprecision equations sees correct, but possibly less \nhelpful, output. Of course, the equations for arithmetic and other natural primitives should be written \nonce and saved away to avoid duplication of effort. The facility to do so seems to be a special case \nof the general need for facilities to structure and combine equational definitions, discussed in .Section \n5. We have chosen to await results in the more general area, rather than to perform an ad hot- extension \nfor primitive operations. 4. The Importance of Pattern-Matching Algorithms 4.1 Motivation In order that \nprogramming with equations be really different from more conventional programming styles, it is important \nto be able to write many equations, preferably between small terms, rather than a few huge ones. If all \nof the i=\u00a3orrr~tion about a function f is given by a single equa- tion, f(x)==T, then the term T is essentially \njust a lazy LISP program for f. In order for equational programming to serve a purpose not already served \nby LISP, one must have an interpreter capable of processing many equations giving different pieces of \nthe definitions of functions. The imple-mentation must not penalize programs for using a large number \nof equations by sequential checking of the left-hand sides of equations to see which ones apply. In order \nto com- pete in performance with conventional LISP interpreters, the process of finding the next subexpression \nto replace must have a cost comparable to the cost of manipulating the recur- sion stack in LISP. Instead \nof sequential checking, we preprocess the equa- tions and produce tables to drive the reduction. These \ntables describe state transitions during a traversal of the term that indicate immediately when an instance \nof an equation lefl-hand side is found, and tell which equation is involved. The overhead of each traversal \nstep at run time is only a table lookup. For multiple-pattern string matching, the Aho-Corasick generalization \nof the Knuth-Morris-Pratt algorithm [AC75, KMP77] solves a problem closely analogous to ours. Exten-sion \nof these string-matching techniques to terms (equivalently, trees) was treated separately in [HO82a]. \nIn the last year of the project, we discovered that the restric-tiom already imposed upon equations for \nother reasons allow for a much simpler extension of string matching techniques. The following subsection \nassumes an understanding of the Aho-Corasick algorithm. 4.2 A Specialized Pattern-Matching Algorithm \nThe current version of the equation interpreter is left-sequential. That is, a term to be reduced is \ntraversed to the left first, and any left-hand side that is found is replaced before the traversal continues. \nSuch a strategy cannot deal with certain equations, such as the parallel or equations: or(True pc \")'=--True; \nor ( x ,True )=--True. The interpreter preprocessor detects and rejects such equa- tions. For left-sequential \nequations, a special and simple pattern-matching algorithm 'may be used. Tree patterns are flattered \ninto preorder strings, omit- ting variables. The Aho-Corasick algorithm [AC75] is used to produce a finite \nautomaton recognizing those strings. Each slate in the automaton is annotated with a description of the \ntrec moves needed to get to the next symbol in the string, or the pattern that is matched, if the end \nof the string has been reached. Such descriptions need only give the number of edges (~0) to travel upwards \ntoward the root, and the left-right number of the edge to follow downwards. For example, the patterns \n(equation left-hand sides) f(f(a.x ),g(a,y)) and g(x,b ) generate the strings .[faga and gb, and the \nautomaton given in Figure 4.1. The automaton cannot be annotated consistently if conflicting moves are \nassociated with the same state. Such conflicts occur precisely g ' / ' / J //' l\" J /\" forward edge \nfailure edge failure edges not shown all lead to the start state lu means move up or= level in the tree \ndl means move down to son number 1 ml means a match of pattern number 1 Figure 4.1 the two strings are \nd~f'erent. These differences are discovered directly by attempts to reassign state i~ormation in the \nautomaton when = is the empty string, and by com- paring states at opposite ends of failure edges when \n(, is not empty. When 1\" and 8 are not empty, the conflicting annota- tions are both tree moves, and \nindicate a violation of restric- tion (5) of ,~ction !.!. When one of 1,,8 is the empty string, the cx~rresponding \nannotation reports a match, and indicates a violation of restriction (3) or (4). In the example above, \nthere is a conflict with c~-ffa,/].=g, 1\".-a, 8-b. That is, after scanning .[fag, the first pattern directs \nthe traversal down edge number I, and the second pattern directs the traversal down edge number 2. This \nconflict is discovered because there is a failure arc between states with those two annota- lions. 4.3 \nCompleteness of the Pattern-Matching Algorithm The restriction imposed on equations by the pattern- matching \nstrategy above may be justified in a fashion similar to the justification of deterministic parsing strategies. \nThat is, we show that the algorithm succeeds (generates no conflicts) on every set of equations that \nis left-sequential according to a reasonable abstract definition of sequentiality. In order to define \nsequentiality, we need some special terms for discussing computation steps in the interpreter. All of \nthe discussion in this subsection refers to an arbitrarily given set of equations. Definition 4.1 A set \nof equations is regular if it satisfies restrictions 1-4 of Section i.I (but not necessarily restriction \n5). A context is a term built from the constants and operators in the given set of equations, as well \nas the new constant sym- bol =. An instance of a context C is any term or context S resulting from the \nreplacement of one or more occurrences of \u00a2e in C. A left conte.xt is a context C such that there is \na path from the root of C to a leaf, with no occurrences of o, on or to the left A I(~--lraver~l COOteXt \nis a pair <C,I>, where C is a left 114 context, and I is a node on the path dividing ,,s from other \nsymbols in C. A redex is an occurre~'ce of an insta,x,-.-~e of a left-hand side of an equation (letting \neach variable occurrence be treated as an ~'J)), A term S /--reduces to a term T ifS may be transformed \ninto T by replacing redexes by arbitrarily chosen terms. A redex R in a term S is essential if there \nis no way to /- reduce S to normal form without reducing R. A term S is root stable if there is no redex \nT such that S /- reduces to T. A redex R in a term S is root essential to S if there is no way to/-reduce \nS to a redex or a root stable term without reduc- ing R. A context represents the information known \nabout a term after a partial traversal. The oJs stand for unknown portions. A left-traversal context \ncontains exactly the part of a term that has been seen by a depth-first le~t traversal that has pro- \ngressed to the specified node. /-reduction is the best approx- imation to reduction that may be derived \nwithout knowing the right-hand sides of equations. In the process of reducing a term by outermost reduc- \ntiot~% our short-term goal is to make the whole term into a redex. If that is impossible, then the term \nis root stable, and nnay be cut down into independent subproblems by removing the root. Definition 4.'~ \nA set of equations is slrongly left--sequential if there is a set of left-traversal conte~'ts L such \nthat the following conditions hold: I. For all <C J> in I., the subtree of C rooted at I is a redex. \n2. For all <C,I> in I., S an instance of C, / is essential to S. 3. For all left-traversal contexts <C,/> \nnot in L, S an instance of C, / is not mot-essential toS. 4. Every term is either root stable or an \ninstance of a left context in I..  In a strongly left-sequential system, we may reduce a term by traversing \nit in preorder to the left. Whenever a redex is reached, the left-traversal context specifying that redex \nis checked for membership in I,. If the left context is in I,, the redex is reduced. Otherwise, the traversal \ncontirnaes. When no left context in I, is found, the term must be root stable, so the root may be removed, \nand the resulting subterms pro- cessed independently. (I) and (2) guarantee that only essen- tial redexes \nare reduced. (3) guarantees that no root-essential redex is skipped. (4) guarantees that the reduction \nnever hits a dead end by failing to choose any redex. The analogous property to strong left-sequentiality, \nusing reduc- tion instead of /-reduction, is undecidable. Notice that strong left-sequentiality depends \nonly on the left-hand sides of equations, not on the right-hand sides. Strong left-sequentiality is a \nspecial case of the strong sequentiality defined by Huet and Lt~vy IHL791, who give a thorough technical \ntreatment of these concepts. Huet and L~vy have a pattern-matching algorithm that is much rnore general \nthan ours, but its practical implementation has not yet been studied. We expect that our algorithm will \ncontinue to be useful because of its simplicity, even when implemen- tations of the Huet-l~vy method \nare available to cover their wider class of sequential systems. Strongly left-sequential sets of equations \nare intended to include all of those systents that one might reasonably expect IO process by scanning \nfrom left to right. Notice that definition 4.3 d(ms not require L to be decidable. Also, a strongly left-sequential \nsystem may not necessarily be. pro- cessed by leftmost-outermost evaluation. Rather than requiring us \nto reduce a leftmost redex, definition 4.3 merely requires us to decide whether or not to reduce a redex \nin the left part of a term, before looking to the right. Every redex that is reduced must be essential \nto finding a normal form. When the procedure decides not to reduce a particular redex, it is only allowed \nto reconsider that choice after producing a mot-stable term and breaking the problem into smaller pieces. \nWhile strongly left-sequential systems are defined to allow a full depth-first traversal of the term \nbeing reduced, the algorithm of Section 4.2 avoids searching to the full depth of the term in many cases \nby recognizing that certain subterms are irrelevant to choosing the next step. Theorem 4.1 The pattern-matching \nalgorithm of Section 4.2 succeeds (i.e., generates no conflicts) if and only if the input patterns are \nleft-hand sides of a regular and strongly left-sequential set of equations. Proof sketch: (~\u00a2') If the \npattern matching-automaton is built with no conflicts, then L may be defined to be the set of all left- \ntraversal contexts <C,/> such that I is the root ofa redex in C, and / is visited by the automaton, when \nstarted at the root of C. (<=') If a conflict is found in the pattern-matching automa- ton, then there \nare two flattened preorder strings a/8~, and/88 derived from the patterns, with conflicting tree moves \nat from /9 to y and from /8 to B. Without loss of generality, assume that there are no such conflicts \nwithin the two (~currences of/8. ~,/8, with its associated tree moves, defines a context C, which is \nthe smallest left context allowing the traversal specified by a/8. B defines a smaller left-traversal \ncontext D in the same way. D is contained as a subterm in C, in such a way that the last nodes visited \nin the two traver- sals coincide. If one or both of 7,8 is empty, then C demon- strates a violation of \nrestriction (4) or (3), respectively. So, assume that ~,,8 are not empty, and the annotations at the \nencls of the/gs are both tree moves. Consider the two positions to the right of C specified by the two \nconflicting traversal directions for aB and/8. Expand C to E by filling in the leftmost of these two \npositions with an arbitrary redex, and let n be the root of this added redex. Let equ t be the equation \nassociated with whichever of a/8.|,, /8~ directed traversal toward this leftmost position, and let equ2 \nbe the equation associated with the remaining one of ~x/87, /88. <E,n> cannot be chosen in L, because \nthere is an instance S of E in which a redex occurs above n matching the left-hand side of equ2, and \nS may be/-reduced to normal form at this redex, without reducing the redex at n in E. <E,n > cannot be \nomitted from L, because there is another instance T ore in which everything but n matches the rexlex \nassociated with equ ~, and n is therefore root-essential to T. I-q For example, the pair of equation \nleft-hand sides f(g(x,a ),y) and g(b,c) have the preorder strings fga and gbc. A conflict exists with \na-.f, B--g, ~,=-a, 8--,c. The first equation directs the traversal down edge 2 after seeing .fg, and \nthe second equation directs it down edge 1. The conflicting prefixes fg and g produce the context ./(g(aJ,=),~). \nThe context above is expanded to the left-traversal context consisting of f(g(g(b,c),to),to) with the \nroot ofg(b,c) specified. This left- traversal context cannot be chosen in I. (i.e., it is not safe to \nreduce the redexg(b,c) in this case), because the leftmost \u00a2e could be filled in with a to produce .[(g(g(b,c),a),oJ), \nwhich is a redex of the form/'(&#38;(x,a),y), and can be/-reduced to normal form in one step, ignoring \nthe smaller redex g(b,c). But. this Icft-traversal context may not be omitted from I. (i.e., it is not \nsafe to omit reducing x,(h,c )), because the left- most = may also be filled in with c to produce f(g(g(b,c),c),~,~), \nand reduction ofg(b,c) is essential to get a normal form or a root-stable term. 4.4 Interpreting Nonsequential \nSets of Equations. In the future, an improved version of the equation interpreter should eliminate the \nrestriction to strongly left- sequential systems, and allow definitions of\" (:onstructs such as the parallel \nor. The pattern-matching algorithm of Section 4.2 may be extended to handle nonsequential systems by \nannotating each state in the automaton with a nonempty set of tree moves. When more than one move is \nspecified, parallel processes must be initiated to follow the different possibilities. This approach \nkeeps the degree of parallelism low (but not always the lowest possible), which is desirable on sequential \nhardware. To =naintain acceptable perfor- mance, these processes must be able to wait for results pro- \nduced by other such processes when two or more of them wander into the same region (else work will be \nduplicated), and a process must be killed whenever a second process creates a redex containing the first \none (else wasted work may be done on a subterm that has been discarded). Solu- tions to these problems \nare well-known in principle, but care- ful study is required to implement them with a very small time \nand space overhead. Even when such an implementa- tion is accomplished, sequential algorithms such as \nours and Huet and I_~vy's will be useful because they can avoid the overhead of the parallel methods. \n 5. Separation of Syntactic Processing From Semantics  One of the main problems in making the equation \ninter- preter useful to a human programmer, is the syntactic form of the terms written within equations, \nand those presented for reduction to normal form. Prefix notation is the standard of reference in mathematics, \nbut is almost never convenient for a specific application. We discovered this problem with a prototype \ninterpreter, when we tried to write equations defining LISP. Most of our time was spent wrestling with \nhairy expressions for simple lists, such as cons(l,cons(2,cons(3,cons(4,nil)))), for (1 234), instead \nof thinking about semantic issues. Unfortunately, different domains of computation seem to have developed \ndifferent notation, and we know of none that is universally acceptable. So, we decided to communicate \nwith the equation interpreter through a number of different front ends, stored in a stan- dard library. \nA user may, of course, use his or her own if the ones provided do not suffee. It is important to be able \nto use the same syntactic definitions of terms to parse terms in equations, and to parse terms before \nevaluation. A way to separate syntax and semantics thoroughly is to use an explicit uniform internal \nform for the abstract syntax of terms and equations, into which special syntaxes are translated. This \ninternal syntax is string-based which greatly simplifies porting the system to a new machine. These front \nends may be written in a any programming language. Struc-ture editors are the ideal front ends in our \nview, but at present we use lex and yacc to produce parsers. Of course, for consistency the interpreter \nalso produces its output in internal form, and the output is then sent to one of a library of pretty-printers \nfor display. Current parsing technology t~kes it easy to use the same grammar for terms in parsing both \npreprocessor and interpreter input, but the (much easier) pretty-printers are written separately. While \na pro-gram to generate par.~rs and unparsers (pretty-printers) from the same grammar would be very nice, \nwe prefer to await the availability of grammar-driven structure editors, with which the only syntactic \ntransformation required will be the pretty-printing. Several advantages result from the discipline of \nusing an explicit intermediate form between text produced by the user and semantic processing by the \nsystem. First is the complete separation of syntactic and semantic modules. Conventional use of grammars \nto generate parsers requires a complex interface between the parser and the semantic processor, spe- \ncialized to the particular parser generator. We require no internal connection whatsoever between syntactic \nand seman- tic processors. Second, once a context-free parser has done its task, there may remain issues, \nsuch as checking symbol declarations against use, that are purely syntactic (in spite of compiler-writer's \njargon), but are not expressible by a context-free grammar. By letting the parser produce an expli- cit \nsyntax tree, we are at liberty to process that tree further before submitting it to the semantic processor. \nIn fact, we have implemented the nort-context-free parts of syntactic analysis in the equation interpreter \nitself by equational pro- grams that transform the abstract syntax after context-free parsing and before \nsemantic processing. Systematic encod- ings of notation, such as Currying (transforming f(a,b,c) into \napply(apply(apply(f,a),b),c)) may be implemented at this level. Last, and perhaps most important in the \nlong run, the use of an explicit abstract syntax allows applications of the system to develop far beyond \nthe simple context of a user who types in a program, preprocesses it, types in an input, and awaits the \nresults at his terminal. Many future applica- tions of our interpreter may involve input terms, and even \nequational programs, that are themselves produced automati- cally by other programs, and the outputs \nmay often be sub- ject to other processing before, or instead of, being displayed. The very syntactic \nsugar that makes program and input entry easier for a human, makes it harder to produce automatically. \nSimply by omitting the syntactic pre-and postprocessors when appropriate, we may build useful systems \ncontaining equational programs, and the communication within these systems need not deal with the inefficiencies \nand notational problems (especially quoting conventions) of the humanly readable syntax. We have already \ntaken advantage of this feature, by omitting the pre- and postprocessin8 steps from the equational programs \nthat do syntactic analysis of equa- tional programs. A more important use of this feature to extend the \nusefulness of equational programming is described below. Although equational programs require substantial \ntrans- lation to be executed on conventional machinery, our current language is very low level in the \nsame that no facilities are provided for organizing or moduiarizing large programs. The implementation \nof a high-level approach to equational pro-gramming should include the ability to combine separately \nwritten equational programs into larger ones, in a semanti-(:ally meaningful, rather than purely lexical, \nway. Combining forms such as those described by Burstall and Goguen [BG65] should provide a good starting \npoint for development of higher-level techniques in equational programming. We expect to implemem such \ncombining forms by equational programs that transform the abstract syntax of other equa- tional programs. \nOnce we have chosena pleasant mechanism for resolving name clashes, this capability is integrated into \nthe system between the front end and the semantic part of the equation preprocessor. equational program \nI 1  preprocessor I semantics 1 term term Interpreter Figure 5.1 TI~e considerations above, along \nwith the separate preprocessing step for pattern matching, lead naturally to the system configuration \nshown in Figure 5.1. Communication between modules is always by UNIX text fries.  6. Experience with \nthe System In [HO82b], we reported our experiences with an earlier version of the system. Briefly, we \nconcluded that the bottom-up matching strategy is extremely fast permitting reductions at very high rates. \nSince then we conducted two major experiments in graduate seminars. The purpose of the first experiment \nwas to evaluate the practical performance of the various pattern matching algo- rithms proposed in [HO82a]. \nWe found that the top-down method with counter coordination is inferior to the other two methods, because \nit is slightly slower in detecting matches and requires more processing after reductions to maintain \nmatching information. In particular, the matching time is prOportional to the number of patterns to be \nmatched. Since we wish to encourage writing many small equations, the large number of resulting patterns \nis noticeable in the perfor-mance. The top-down method with bitstring coordination performed better in \ndetecting matches and update processing, but its match time also Increases in proportion to the number \nof patterns matched. The perceived performance differential is probably due to the smaller locality In \nwhich update pro- cessing has to be performed. Top-down matching wRh bitstring coordination did rot offer \na clear advantage over the bottom-up method, despite its cheap proprocessing. Bottom-up matching has \nmore expensive preprocessing and requires tables to direct the matching algorithms which can be fairly \nlarge, however, it affords better diagnostics and is fastest in locating matches and update processing. \nThis comparative appraisal of the bottom-up technique is corrotx}ramd by the work of Wilhelm (e.g., [GMW80]), \nwho has used this matching method extensively in his equational approach to compiler writing. In Wilhelm's \nexperience (as in ours), the patterns which give rise to poor preprocessing times do not normally arise \nin applications. Moreover, there are heuristics to reduce space demands and compress the tables needed \nby the matching algorithm resulting in acceptable sizes. In the case of left-sequential equations, the \nnew method derived from string matching is in our opinion the best choice, as it is as fast as the bottom-up \napproach at run time and usually as space efficient as the top-down methods. A second experiment was \nto investigate the suitability of equational programs for writing compilers for procedural languages. \nWe chose PASCAL as compromise between source language complexity and the time constraints in a class \nroom situation. Results indicate both pros and cons of writing compilers with equations: On the one hand, \nfor attri- bute maintenance equations are not especially converdent, but on the other, the equational \ncompiler was very concise and the students felt that their programming of it was much lass error-prone. \nThe project also pointed out a need for a structured specification technique similar to the ones advo- \ncated in I1~J65] (e.g., \"derive'), which allow a single, com- mon specifu:ation of subtasks whose equations \ndiffer only in inessential ways. 7. Avoiding Repeated Evaluation of Subterms. Outermost evaluation, while \navoiding evaluation of sub- terms that am irrelevant to the final result, allows unn~,.,es- sary duplication \nof relevant subterms. Whenever a variable appears more than once on the right-hand side of an equa- tion, \ninnermost evaluation would evaluate the term substi- tuted for that variable once, before applying the \nequation in question. Outermost evaluation appears to create multiple copies of such a term, which apparently \nwill be evaluated separately. It is easy to avoid this particular duplication of effort by implementing \nmultiple instames of the same vari-able by multiple pointers to the same subterm. Such collaps- ing, \nof course, makes future implementation of parallel reductions (Section 4.4) more diff~ult, because several \nprocesses may simultaneously occupy the same subterm. We have gone farther in avoiding repetition. Whenever \nan instam\u00a2 of a right-hand side is created, the newly created nodes are hashed, and coalesced with any \nexisting identical nodes. This innovation was introduced as an optimization by Paul Golick in a prototype \nversion of the interpreter. As a result, if a subterm T is created repeatedly, it is still evaluated \nonly once. Further improvements are possible. If, as a result of reduction of one of its proper subterms, \nT becomes identical with an existing subterm, we do not detect such an identity. To do so would require \nrestructuring of the hash table, and a noticeable extra overhead. Such a dynamic detection of identical \nsubterms would lead to an implementa- tion of the directed congruence ctoaa'e algorithm of Paul Chew \n[ChLS0], and is left to future work. The current level of identity detection already has interesting \nconsequences for programming. 7.1 Automatic Dynamic Programming. Dynamk; programming may be viewed as \na general tech- nique for transforming an inefflcient recursive program into a more efficient, iterative \none which stores some portion of the graph of the recursively defined function in a data structure, in \norder to avoid recomputation of function values. In a typ- ical application of dymmic programming, the \nprogrammer must specify how the graph of the function is to be stored, as well as the order in which \nthe graph is to be computed. The latter task may be handled automatically by the equation interpreter. \nWe illustrate this automation on equations to solve the optimal matrix multiplication problem of [AHU74]. \nThe input to the problem is a list of integers (do''. am)m 91, representing a sequence Ml,... Mm of matrices \nof dimen- siors doxdt,dsxd2,...d,,_l\u00d7d,, respectively. The problem is to find the cost of the cheapest \norder for multiplying such matrices, assuming that multiplication of an /xJ by a jxk matrix costs toj, \nk. There is an obvious recursive solution given by cost [(do\" d,, ~l= min{cost[ (do .  d~)'f\"cost[ \n(d~ \" dm )]'hd0*d~*dm ] 0<i ~m } cost[(do d I)]-0 This recursive solution, implemented dlrecgy, requires \nexpor~ntial time, because it recomputes the same values of the cost function many times. Dymmic programming \nachieves a polynomial solution by producing the graph of the cosz function as a static data structure, \ninto which each value is stored only once, but inspected repeatedly. Instead of\" the conventional approach \nof defining only a small finite part of the graph of the coat function, we define the infinite graph, \nand the outermost evaluation strategy of the equation inter- prefer guarantees that only the relevant \npart of the graph is actually computed, and in the right order. The more converb tional solution of this \nproblem requires the programmer to specify just the right finite portion of the graph of cost to compute, \nand the precise order of its computation. The following equational program solves the optimal matrix \nmultiplication problem, using LISP notation. Lines begin-ning with colon are comments. : In the following \nequations, the function cost is represented : by an infinRe.-dimensio nai infinite list giving the graph \nof : the functiotx : costgraph[O] -,. : (0 (cost[(l)] (cost[(] 1)1 (cost[(! ! 1)] ... ) : (cost[(l ! \n2)] ... ) : .\u00b0\u00b0) : (cost[(l 2)] (cost[(l 2 1)1 ... ) : (cost[(l 2 2)] .,. ) : .o.) : \u00b0,.) : (~Sd(2)l \n(reSt[(2 1)1 (coSt[(2 I i)] ... ) : ..) .,. ) : ..\u00b0) : That is, cost[(d0 ... dm)] is Ihe first element \nof the list : which is element dm + 1 of element dm-I + 1 of ... : element dO + 1 of costgraph[()], cost[0)] \nis always 0, but : inclusion of these 0s simplifies the structure of costgraph. : costgraph[a], for a< \n> O is the fragrnent of costgraph[ O] : whose indexes are all prefixed by a. Symbols : operators directly \nrelated to the computation of cost cost: I; costgraph: I ; costrow: 2~ reccost: I; su tx~osts: 2; : list-manipulation, \nlogical, and arithmetic operators coi~: 2; nil: 0; min: 1; index: 2; length: I; element: 2; first n: \n2; first: I; tail: !; aftern: 2; last: 1; addend: 2; cond: 3; add: 2; equ: 2; less: 2; subtract: 2; \nmultiply: 2; include integer_numerals, truth_values. For all a, b, i, j, k, x, y: cost[a] --index[a; \ncostgraph[O]]; : costgraph[a] is the infinite graph of the cost function for : arguments starting with \nthe prefix a. costgraph[a] --(reccost[a]. costrow[a:, 1]); : costrow[a; i] is the infinite list : (costgraphlai] \ncostgraph[ai+ 1 ] ... ) : where al is a with i added on at the end. costrow[a; fl -- (costgraph[addend[a; \ni]]. costrow[a; add[i; !]1), : reccost[a] has the same value as cost[a], but is defined : by lhe recursive \nequatiom from the header. reccost[(I J)l -0; reccost[(i)] -.. 0; recoost[()] -0; recxostI(i j. a)l -min[subcx)stsl(i \nj. a); lengthla]]l --where a is (k. b) eixl where; : su~-osts[a; i] is a finile list of the recursively \ncomputed : costs of (dO ... din), fixing the last index removed at :i, i-I .... I. subcosts[a; i] --condlequli; \n0]; O; (addladd[costlfirstn[addli; I ]; all; cost[afternli; a]]]; mu Itiply[ mult iply[ firstla]; element[add[i; \nI]; all; iastla]l] subcosts[a; add[i; -1]])]; : Definitions of list-manipulation operators, : logical \narvd arithmetical operators. min[(i)l --i; min[(i, a)] --cond[lessli; minla]l; i; min[a]l where a is \n(k. b) end where; indexl(); (x. b)] -. x; index[(i, a); x] -indexla; elementladd[i; 1]; x]]; length[()] \n--0; lengthI(x, a)] --add[length[a]; I ]; elementli; (x. a)] ,- cond[ectuli; 1 ]; x; element[subtract[i; \n11; a]]; firstn[i; a] --cond[equ[i; 01; O; (first[al. firsm[subtractli; 1 ]; tailla[l)];  firstI(x, \na)] --x; tail[(x, a)] --a; afternli; a] --cond[equ[i; 0]; a; aftern[subtractli; 1]; tail[a]]]; lastl(x)l \n-x; last[(x y. a)] --last[(y, a)]; adclend[(); yl '-(y); addend[(x, a); yl --(x. addendIa; y]); cond[true; \nx; y] --x; cond[false; x; y] --y; include addint, ec~int, subint, multint. While understanding the mapping \nof the graph of the func- tion cost onto the structure costgralCall is somewhat tedious, such tediousness \nmight be greatly ameliorated by a special- ized notation for such problems, without losing the advan- \ntage of automatic discovery of the correct order of computa- tion. The efficiency (but l~t Ihe correctness) \nof the program at'~ve depends on the fact thai all instances of costgraphl()] will be detected and coalesced \nby the interpreter. A future inlplec~lentation of the clyuramic identity detection embodied in the directed \ncongruence closure algorithm [ChL80] would allow the .came efficiency to be achieved by the straightfor- \nward recursive program. Bibliography and References AC75 Aho, A., and M. Corasick, Efficient String Matching: \nan Aid to llibliographic Search, C.4CA4 18:6 (1975) 333-343 AI-tU74 Aho, A., J. E. Hopcroft, J. D. Ullman, \nThe Design and Analysts ed\" C'omlntter illgorithms. Addison-Wesley, 1974. ALJ72 Aho, A. and J. Llllmart, \nThe Theory of Parsing, Trans- lation, and Cot~zpiling, Volume 1: Parsing, Prentice-Hall, 1972. AW76 Ashcroft, \nE. and W. Wadge, Lucid -A Formal Sys- tem for Writing and Proving Programs. SIAM Jour- nal on Computing \n5:3, 1976, 336:354. AW77 Ashcroft, E. and W. Wadge, Lucid, a Nonproce.,dural Language with Iteration, \nCACM 20:7, 1977, 519-526. I~.74 I~ckus, J. Programming Language Semantics and C'losed Applicative Languages. \nACM Symposium on Principles of Programming Languages, 1974, 71-86. Ba78 l~.ckus, J. Can Programming Be \nLiberated from the yon Neumann Style? A Functional Style and its Algebra of Programs, CACM 21:8, 1978, \n613-641. ltl379 I'lauer, F. L., M. 13roy, R. Gx~tz, w. Hesse, B. Krieg- Bruckner, H. Partsch, P. Pepper, \nH. Wossner. Towards a Wide Spectrurn Language to Support Pro- gram I)evelopn,ent by Trartsformatior~. \nProd'am Construction: International Suntmer School, Lecture Notes in Complaer Science v. 69, Springer-Verlag, \n1979, 543-552. BL77 116rry, C;. and L~vy, J.J. Minimal and Optimal Com- putations of Rocursive Programs. \n4th ACM Sympo- sium on Principles of Programming Languages, 1977, 215.226. BL79 I~rry, G. alxl i.~vy, \nJ. J. Letter to the Editor, SICiACT News, v. I 1, no. i, Summer 1979, 3-4. Itj72 13jorner, D. Fi,ite \nState Tree Computations (Part I). IBM Research Technical Report RJ 1053 (#17598), 1972. llr76 13ruylx~oghe, \nM., An Interpreter for Predicate Logic Programs Part I, Report CWl0, Applied Mathematics and Programming \nDivision, Katholieke Universiteit, Leuven, Belgium, 1976. 1K;65 13urstall, R. M. and Cioguen, J. A. Putting \nTheories Together to Make Specifications. 5th International Joint Conference on Artificial Intelligence, \nCam-bridge, Mass., 1965. IIMS80 13urstall, R., MacQueen, D., Sannella, D. HOPE: An Experimental Applicative \nLanguage. Internal Report CSR-62-80, University of Edinburgh, 1980. Ca J72 Cadiou, J,, Recursive Definitions \nof Partial Functions and Their Computations, Ph.D. Dissertation, Com- puter Science l\"Jept., Stanford \nUniversity, 1972. CAT76 Cargill, T., Deterministic Operational Semantics for Lucid, Research Report C'S-76-19, \nUniversity of Waterloo, 1976. ChL80 Chew, L. P. An [reproved Algorithm for Computing With Equaliom. 21st \nAnnual Symlx>sium on Founda- liOnS of Computer Science, 1980, 108-117. ChA41 Church, A. The Calculi of \nLambda-Conversion. Princeton University Press, Princeton, New Jersey, 1941. deBT2 de Bmijn, N. Ci. Lambda \nC\u00a31cuius Notation with Nameless Dummies, Nederl. Akad. Wetensch. Prec. Series A 75, 1972, 381-392. CF58 \nCurry, H. B., and Feys, R., Combirwtory Logic volume I. North-Holland, Amsterdam, 1958. DS76 Downey, \nP. and R. Sethi, Correct Computation Rules for Recurslve Languages. SIAM Journal on Computing 5:3, 1976, \n378-401. Fa77 Farah, M., Correct Compilation of a Useful Subset of Lucid, Ph.D. Dissertation, Department \nof Computer Scieme, University of Waterloo, 1977. FW76 Friedman, D., and D. Wise, Cons should not evalu- \nate its arguments, 3rd International Colloquium on Automata, Languages and Programming, Edinburgh, Edinburgh \nUniversity Press, 1976, 257-284. GMWS0 Glasner, I., M6ncke, U., and Wilhelm, R. OPTRAN, a language for \nthe specification of program transformations lttformatik-Fachberichte. Springer-Verlag 1980, 125-142 \nGo77 Goguen, J., Abstract Errors for Abstract Data Types, IFIP Working Conference on Formal Description \nof Programming Concepts, E. J. Neuhold, e,d., North- Holland, 1977, GS78 Ciuibas, L. and R. Sedgewick, \nA Dichromatic Frame- work for Balanced Trees, 19th Symposium on Foun- dalions of computer Science, 1978, \n8-21. GHM?6 Guttag, J., E. Horowitz and D. Musser, Abstract Data T~0~es and Software Validation, Information \nSci- ence Research Report IS1/RR-76-48, University of Southern California, 1976. HM76 Henderson, P., \nand J. H. Morris, A Lazy Evaluator, 3rd ACM Symlx~sium on Principles of Programming Languages, 1976, \n95-103. Ho78 Hoffmann, C., Design and Correctness of a Compiler for a Nonprocedural Language, Acta ltlforrnatica \n9, 1978, 217-241. HO79 Hoffmann, C. and O'Donnell, M. J., Interpreter Gen- eralion Using Tree Pattern \nMatching. 6th Annual Symposium on Principles of Programming Languages, 1979, 169-179. HO82a Hoffmann, \nC. and O'Donnell, M. J., Pattern Match- ing in Trees, 2ACM. January 1982, 68-95. HO82b Hoffmann, C. and \nO'Donnell, M. J., Programming With Equations, ACM TOPLAS. January 1982, 83- 112. HL79 Huet, G. and J.-J. \nLdvy, Computations in Non-ambiguous Linear Term Rewriting Systems, IRIA Technical Report #359, 1979. \nJo77 Johnson, S. D., An Interprelive Model for a Language Based on Suspended Construction, Techni- cal \nReport #68, Dept. of Computer Science, Indiana University, 1977. KM77 Kahn, G. and MacQueerg D. B. Coroutines \nand Net- works of Parallel Processes, Information Processing 77, B. Gilchrist ed., North-Holland, 1977, \n993-998. KMP77 Knuth, D., J. Morris and V. Pratt, Fast Pattern Matching in Strings, SIAM J. on Comp. \n6:2 (1977) 323-350 K[80 Klop, J. W. Combimtory Reduction Systems, Ph.D. dissertation, Mathematisch Centrum, \nAmsterdam, 1980. KB70 Knuth, D., and P. l]endix, Simple Word Problems in Universal Algebras. Computational \nProblems in Ab.~ract A/gebra. J. Leech, ed., Pergammon Press, Oxford, 1970, 263-297. Ko79 Kowalski, R. \nAlgorithm --Logic + Control. CACM 22:7, 1979, 424-436. McC60McCarthy, J., Recursivo Functions of Symbolic \nExpressiom and Their Computation by Machine, CACM 3:4, 1960, 184-195. McI68 McIlroy, M. D., Coroutinas, \nInternal report, Bell Telephone Laboratories, Murray Hill, New Jersey, May 1968. Mm~0 M6ncke, U. An Incremental \nand Decrernental Gen- erator for Tree Analysers Bericht Nr. A 80/3, Fachber. lnformatik Univ. des Saarlandes, \nSaarbrficken, April 1980 NO78 Nelson, G. and D. C. Oppen, A Simplifier Based on Efficient Decision Algorithms, \n5th Annual ACM Symposium on Principles of Programming Languages, 1978, 141-150. NOB0 Nelson, G. and D. \nC. Oppen, Fast Decision Algo- rithms Based on Congruence Closure, JACM 27:2, 1980, 356-364. O'D77 O'Donnell, \nM. J., Computing in systems Described by Equation.s, Lecture Notes in Computer Science v. 58, Spri nger-Verlag, \n1977. O'D79 O'Donnell, M. J. Letter to the Editor, SIGACT News, v. 11, no. 2, Fall 1979, p. 2. RoG77 \nRoberts, G., An lmplen~ntation of PROLOG, M.S. Thesis, Dept. of Computer Science, University of Waterloo, \n1977. ROB73 Rosen, B. K., Tree Manipulation Systems and Church-Rosser Theorems, JACM 20:1, 1973, 160- \n187. S177 Staples, J., A Class of Replacement Systems with Simple Optimality Theory, Bulletin of the \nAustralian Mathematical Society, 17:3, 1977, 335-350. St79 Staples, J. A Graph-Like Lambda Calculus For \nWhich Leftmost-Outermost Reduction Is Optimal. Graph Grammars and Their Application to Computer Science \nand Biology, Lecture Notes in Computer Science. volume 73, V. Claus H. Ehrig, G. Rosenberg eds., Spri \nnger-Verlag, 1979. St72 Stenlund, S. Combinators, Lambda-Terms, and Proof Theory. D. Reid\u00a2l Publishing \nCompany, Dordrecht, Holland, 1972. Vu74 Vuillemin, J., Correct and Optimal Implementations of Recursion \nin a Simple Programming Language, JCSS 9:3, 1974, 332-354. 120 WaM76 Wand, M., First Order Identities \nas a Defining Language, Technical Report #29, Dept. or\" Computer Science, Indiana University, 1976. WAD77 \nWarren, D., Implementing PROLOG, Research Reports #39, 40, Dept. of Artificial Intelligence, University \nof F.,dinbvrgh, 1977.  \n\t\t\t", "proc_id": "800017", "abstract": "<p>This paper summarizes a project, introduced in [HO79, HO82b], whose goal is the implementation of a useful interpreter for abstract equations that is absolutely faithful to the logical semantics of equations. The Interpreter was first distributed to Berkeley UNIX VAX sites in May, 1983. The main novelties of the interpreter are</p> <p>(1) strict adherence to semantics based on logical consequences;</p> <p>(2) &#8220;lazy&#8221; (outermost)evaluation applied uniformly;</p> <p>(3) an implementation based on table-driven pattern matching, with no run-time penalty for large sets of equations;</p> <p>(4) strict separation of syntactic and semantic processing, so that different syntaxes may be used for different problems.</p>", "authors": [{"name": "Christoph M. Hoffmann", "author_profile_id": "81100074601", "affiliation": "Purdue University, The Johns Hopkins University", "person_id": "PP14036210", "email_address": "", "orcid_id": ""}, {"name": "Michael J. O`Donnell", "author_profile_id": "81339520288", "affiliation": "Purdue University, The Johns Hopkins University", "person_id": "P332466", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/800017.800522", "year": "1984", "article_id": "800522", "conference": "POPL", "title": "Implementation of an interpreter for abstract equations", "url": "http://dl.acm.org/citation.cfm?id=800522"}