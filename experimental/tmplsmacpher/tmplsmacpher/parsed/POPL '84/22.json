{"article_publication_date": "01-15-1984", "fulltext": "\n Applicative Programming and Digital Design Steven D. Johnson Computer Science Department Indiana University \nLindley Hall 101 Bloomington, IN 47405 USA 1. Introduction This paper adapts applicative programming \ntechniques to the synthesis of mynchronaua system descriptions. It presents a unifying perspective on \nhardware and software engineering and shows that a functional design paradigm is suitable in both realms. \nEstablished techniques for program synthesis are ex-tended to yield hardware descriptions. This work \nstems from research in general programming methods for systems. Synchro- nous systems are a valuable \nspecial case. They are a preferred basis for hardware design: logical synchrony makes circuits easier \nto conceptualize, and physical synchrony makes them easier to test. Applicative notation is well suited \nto the description of digi- tal circuits because it has an immediate interpretation in terms of wiring. \nDigital circuits are a reasonable target for synthesis because they are functional in character. The \ndesign approach is conventional: a circuit is viewed as a sequential control algorithm acting on an architecture \n[17]. This should sound familiar to programmers who view their pro- duct as an algorithm acting on an \nabstract type. An engineer starts with an abstractly descriptive specification and derives a concretely \ndescriptive realization. These notations should be suited not only to what they describe but also to \neach other. Functional recursion equations serve here as specifications. The realization language defined \nin Section 3 uses recursion to state the connectivity of a circuit's components. The engineer's task \nis to translate the one form of recursion to the other. The analogous task in programming for a van Neumann \nrealization is to trans- late an initial specification into iterative form (i.e. construct a flowchart). \nTo design circuits a corresponding characterization of hardware is needed. The major steps in the method \nare (1) transform an ini- tlul specification into iterative form; (2) construct an equivalent synchronous \nsystem description; and (3) make refinements. Step 1 formalizes the task of developing a control algorithm \nfor the circuit, it gives the engineer a setting to attack control decisions abstractly (for example, \nthrough the use of continuations [15]). Step 2 can be automated. It is shown below that a subclass of \niterative specifications, called simple loops, translate immediately into detailed circuit descriptions. \nA simple loop can be derived from any iterative recursion equation by a familiar construction. Step 3 \nis a subject for more research. However, the basic al- gebra for manipulating specifications carries \nover to realizations. Standard design decomposition techniques, such as information hiding, carry across \nnotations. This Material is based on work supported by the National Science Foundation under grants \nnumber MCS77-22325 and MCS82-03978. permission to copy without fee all or part of this material is granted \nprovided that the copies are not made or distributed for direct commercial advantage, the ACM copyright \nnotice and the title of the publication and its date appear, and notice is given that copying is by permission \nof the Association for Computing Machinery. To copy otherwise, or to republish, requires a fee and/or \nspecific permission. &#38;#169; 1983 ACM 0-89791-125-3/84/001/0218 $00.75 A process is often modeled \nas a producer of streams [11] [8]. A general purpose applicative modelling language called DAISY, presented \nin Section 5, supports this abstraction. Section 5 illustrates the ease with which u reasonable circuit \ndescription can be expressed in this language. Execution of the description simulates logical circuit \nbehavior. Thus, the engineer's notation can serve directly as u medium for experimentation, a major benefit \nin a setting that assumes human involvement in the design process. 218 2. Specifications A specification \nis a collection of recursion equations (e.g. F(z) *~ e) in some underlying type. Defining expressions \nare built from conditionals (e.g. p--*eo, \u00a21 ) and applicative terms (e.g. f(q ..... e,)). Least-fixed-point \nsemantics is assumed. In general discussions we will use the type VAL with carrier Vd, operations f, \ng, and h; predicates p and q; and constants a, b, and c. We include an operation called muz that implements \na strict version of the conditional. In linear specifications we may replace p-.*r, t by muz(p, r, t) \nif p, r, and t are trivial, that is, if they contain no symbols defined by the specification. The conditions \nguarantee that in any reduction arguments to mu: remain trivial, hence convergent. Val is dosed under \nfinite combination; that is, there is a simple operation in Val for any combination of the following \nfunctionals: K':(z) == \u00a2 (constant) +i(vl ..... v.) ~ vi (projection) f o g(z) ~ f(g(z)) (aerial combination \n) fJ \"\" fn > (z) ~ (fdz),...,fn(z)) (parallel combination ) Any trivial term over identifiers zt .... \n, zn can be trans- lated to the form \"t(:), where ~t is a combined operation and z identifies the argument \nvector (zt,...,z,). Given any linear specification, it is possible to construct an instance of the canoni- \ncal iterative scheme r(~) ,+= ~,,).-+ I('+). r(g(z)), where p, f, and g may involve muz and finite combinations. \nNon-linear specifications are not generally translatable into iterative form (they are not \"flowchartable') \nunless more is assumed about V&#38;I. There are numerous methods to remove or implement recursion, and \nwe will not attempt a full review. Our general approach is to make transformations on a specification \nby unfolding, folding, and simplifying, with the goal of reaching a characteristic final form. It is \nnot assumed that these deriva- tions are effectively computable but rather that they are done under the \nengineer's guidance, like the transformation system of Darlington and Burstall [2]. We address control \neither through the introduction and analysis of continuations [15] or through the use of standard constructions \n[16, 14]. 3. Realizations A circuit description language must convey what parts a circuit contains and \nhow they are connected. Parts are either storage elements, called reglaters, or computational components. \nIn the language defined below, the symbols of Val stand for components. To avoid confusion between operation \nsymbols and corn ponent symbols, component symbols are surrounded by boxes. The symbol \"!\" designates \na register. Terms express component connectivity; a circuit is described by a system of equations that \nidentifies certain component outputs, or dgnab. Signal names are written in upper ease since they are \ndefined by equations. Definition 1. A SIGNAL EXPRESSION is one of i. ~ , where c is a constant. il. X, \nwhere z is an identifier. iii. e ! s, where c is a constant and is a signal ezpres. sign. iv. ['f_~(sl \n..... am), where f ie an re.place operation and each el is a signal ezpression. A SIGNAL DEFINITION \niS an equation of the form X ~ e, where X is an identifier and s is a signal ezpresdon. A CIR-CUIT DESCRIPTION \niS a system o/signal definitions, {Xi ~ si ] 1 <i~k}, each defining a unique signal identifier. The semantics \nof this language assigns to each signal a function from discrete time to values. This is the usual for- \nmalization. Registers are enabled by a common synchronizing signal called \"the clock\". Since it is shared \nby all registers, the clock is not mentioned in the description language. Components are assumed to operate \ninstantaneously. Definition 2. Let Ls denote the language of signal ezpressions in Definition 1, andlet \nR m (Xi u sl [ 1 ~ i~ k;ol E Ls} be a circuit description. The BEHAVIORAL INTERPRETATION of R io fiz(XZ.(H[ \nsi I ..... )4[ ek ])) where )4 : Ls \"* (o: --* Val) according to )41~]! = X..c )41X, l = Xn.(,',(z)x.) \n)41 c y I --x..(. = o)--+ c,)41 o1(.- ~) Jilts, ..... o,.) i = Xn.f()4l ~1 l(n} ..... )41o., !(\")) \n4. Synthesis of Realizations The fundamental connection between specifications and realizations relates \nthe canonical iterative specification to a ca- nonical synchronous system. The latter is characterized \nas a register transfer system in which all feedback loops pass through a register, and all registers \nare enabled by the same clock [12[. A subgoal induction proves 219 Theorem IL. Let specification S be \n ,V(x) ,ffi p(x)-, f(z),F(g(z)) and realization R be x = : ! I~ (x) RDr = [~ (x) ANS = ~] (X) Then \nif F(z o) is defined, then there is an. such that in R i. )\u00a2[ RDY ](n) =true ii. ~/[ RDY |(k) =false, \nif k <, iii. JIIANSI(n)----F(z\u00b0) | By convention, a circuit description R will be called a realization \nof specification S only when R has signals RDY and ANS that announce and produce the value specified \nby S. The theorem states that R realizes S if its register is properly initialized. How registers are \ninitialized is not addressed here. The basic tool of functional programming, combination, is often described \nin terms Of wiring. This intuition is supported by the formalism above. Proposition 2. ~z)--~ [To93 \nz ) = ~[~ z )) ~-_.~g~.~-~__qz) f[/~[~](z) ..... g~.~z)) [.,o < g,-.. g,. > ]~z) --Lg~-](z) | Hence, \ncombination of terms is transparent to operat- ion/component distinction; at least some of the algebra \ndone on specifications carries over to their realizations. In particular, this transparency gives a class \nof specifications that immediately yield detailed circuit schematics. Theorem 3. The specification F(z, \n..... z.) ',= p-.r,F(t, .....t.) where p, r, and ti are trivial terma, is realized by Xs -----z \u00b0 ! \nt, \u00b0t t, fl~.T n . RDY .~ p ANS~ That is, ANS contaim o o as soon contains F(x t .... ,an) as RDY true. \nBy replacing individual identifiers with projections, the given specification translates to an instance \nof the canonical specifica- tion: F(z) *ffi f(z)-.r'(z),Fo < t~ ...tw. > (z). whence to a canonical \nrealization. The equations of the given realization are exactly the ones needed to reconstruct the original \nterms. Details of the proof can be found in [10]. Specifications in the form shown in Theorem 3 are called \nsimple loops. It is hardly surprising that simple loops charac- terize synchronous systems. Iteration \n(with parallel assignment) is the basis of structured digital design. What we mean to exploit is the \ntransliteration between applicative notations given by the theorem. ExLmple 1. A specification for the \ngreatest-common-divisor function: GCD(z, V) *= cq?(z, V) -\" z, It?(x, y) --* GG D(=, ,ub(V, z)), GCD(y, \nsub(z, II)). can be made into a simple loop by distributing the less-than test through the call to GCD. \nin fact, Itf is pushed through both GCD and sub; we notice below a common expression that results. Since \nthe resulting specification is linear the interior conditionals can be replaced with multiplexors. GCD(z,V) \nt= eq?(z, y) --* z, GCD( muz(It?(z, y), z,y)), aub( muz( It?( z, Y ), V, z), muz(lt?(z, y), =, y)). \nThis specification is realized by X,= =o ! ~z--](~(X,Y),X,Y) Y ffi v o : ~([~&#38;~-]([~](x,r),r,x), \n[-6~(~x,r),x,r))  RDV [7~x,r) = ANS ----X.  Of course, like signals can he identified and thus shared: \nXffiffiz\u00b0! U r = ,,o ,. [;JV~Jv, r, x), u) u = ~v,x,r)  v = [~Bx, r) nDr = [~x,r) ANS ~ X 220 A \nschematic for the realization is drawn from the input- output relationships stated in these equations: \n  U~ :, r~S  By Theorem 3 this circuit is correct with respect to its specifica- tion. 6. An Executable \nDesign L,mgusge In this section the realization for Example 1 is coded in DAISY (see below) and a simple \nexperiment is run to observe its behavior. The exercise illustrates that such experiments are n useful \ntool in design, not only as a debugging aid, but also as a means to explore and refine realizations. \nDAISY is interpreted by graph reduction in a data space of binary list cells; it is n descendant of statically \nseeped PulsE: LISP. DAISY is a normal order language because its interpreter runs in a demand-driven \ndata space [3]. For the same reason, its interpreter can solve data recursions such as L ffi <1 ! L>, \nby which L evaluates to an infinite list of l's. DAISY'S syntax is fairly conventional, but it does have \nsome frills. Angle-brackets specify value lists. An exclamation- point is an infix list constructor; \nAn asterisk circularizes a list; the expression L <t ! L) above could have been written <Is>. Applications \nof a list to an argument results in a form of con- struction called \"functional combination\" [4]. The \nargument, taken to be a list-of-rows, is transposed and the function-list is applied element-wise to \nthe resulting columns. The rule works in infinite cases; for example, <adds>:< <1,7 <9 ! <3*77 > eval- \nuates to (3 4 4 4 ...). By representing both signals and components as infinite lists it is straightforward \nto write a DAISY program that simu- lates circuit behavior. The representation suggests Kahn's model \nof process semantics [11] with LUCJD'S treatment of operators [1]. The component [] is represented as \n<t.>; the constant [] is represented as \u00a2c*>. and the register expression c!S translates to a list concatenation \n<e ! ,>. The DAISY version of the GCD realization is 6CD:[xO 70] <== rec ??? where ][ = (z0 : U> T ffi \n<yO I <sub*>:< <nnx*>:<\u00a5 T X> U > U = <nuxs>:<V lr T) V \u00a2lt?s>:<1 y) RDT : \u00a2eq?s) : \u00a21r T) Ags : I. \n We must now design an expression 77? that states what we want to observe about the system. The experiment \nwill be to display all the identified signals. Unfortunately, the first thing that comes to mind, namely \n<I T O V P, DT Aria>, leads to faih,re; DAISY prints in preorder and X is infinite. To display the sig- \nnals in parallel they must be transposed, so the desired ex-periment is Trunposo:cg T O V RDT AIS) (Function \nTranspose is readily defined as <(X v.v) st). Execution of this form shows that the circuit behaves correctly \nwhen its registers are initialized to (15, 24): J CCD:<I6 24) ((16 24 16 true [] 16) (15 9 9 [] [] lS) \n(9 e e [3 [] g) (e 3 a [] [] e) (3 $ S [] true S) (a o o [] [] s) (0 a 0 true [] O) (0 3 0 true [] O) \n(0 S 0 true [] O) (0 ( 0 a S 0 true 0 true [] [] O) O) tC (inte,~pted) The experiment also shows that \nthe system becomes st- able after it arrives at its answer, and that the answer is retained indefinitely \nin the Y-register. A minor refinement to the realiza- tion, defining ANS = Y and RDY = ~X) exploits this \nproperty at the expense of two clock cycles. A few experiments give convincing evidence that cor-rectness \nis preserved, tlowever, retrofitting the derivation to es- tablish correctness formally is no small matter. \nMoreover, there is no obvious way to describe \"stability\" in the specification law guagel While we await \ntheories that address such issues, the f~ility to bridge the gaps empirically by executing descriptions \nis n substantial benefit. 8. Design Decomposition Example 1 in Section 4 suggests a basic design strategy, \nand it is easy to derive circuit descriptions for small specifications that can be managed in their entirety. \nLarge designs must be decomposed if synthesis is to succeed in a setting that involves in- telligent \nguidance. Proposition 2 foreshadows the conclusion that standard functional program decomposition techniques \nadapt well to the circuit implementation realm. To motivate and explore this issue we present n more \ncomplex derivation. Before proceed- 221 inK to the example let us summarize the design approach: Strategy. \nThe initial specification is a non-linear recursion equation with several defined functions. To make \nuse of Theorem 3 we must derive an equivalent instance of a simple loop. The main subgoal is to place \nthe initial specification in iterative form; from there, a simple loop follows by a familiar construction. \nThus, the initial steps of the derivation realize a sequential pro- gram and use established methods \nfor program synthesis. In the example below we will remove only some of the reeursion and implement the \nrest by introducing a stack. This tack has an ulterior motive: it introduces an abstract type to the \nderivation. Our goal will be to maintain the new abstraction in the realiza- tion. Tactics. A functional \nprogrammer's use of eombinators, or \"help functions\", corresponds exactly to the digital designer's use \nof higher-level symbols to stand for repeated wiring patterns. These abbreviations in the specification \nare carried directly to the realization; the applicaltion pattern kv.t is realized immediately by a packaged \ncomponent ~ . The introduction of a complex type, stacks in the ex- ample, produces signals over complex \nvalues. The defining equa- tions for these signals can be factored in such a way as to recover and hide \nthe original abstraction and reduce the controller's sig- nals to range over basic values. As an illustration, \nconsider the signal equations s ffi cmp*y !~e,~v,s),[-~-~s)) r = [~--~s) Where P is a signal of truth \nvalues and V is a signal of stack- able values. We shall define an abstract component STACK that takes \ninstructions, push and pop, and manages the stack abstraction. The original circuit's signals become \nT= STACK(I,V) Intuitively, the factorization distributes ~ over application. In an intermediate step \nthe circuit ~generates\" a component [] whose operation varies according to P: S ~ empty ! ~V,S) T --[~-'~S) \nNotice that the operation pop has been generalized to accept an extraneous argument. [] seems to violate \nthe notion of a component as something electronically fixed. The physical interpretation must be that \nit is a signal of encodings. STACK is then specified as an instruction interpreter: STACK(I,V) *~ [V~']{S) \nwhere S ~ empty ! ~I, V, S) interpret(i, v, s) *= (i =~ push) \"-* push(v, s), (i ~ pop) ---. pop(s). \nLeaving the implementation of STACK as a nubproblem, we can return to the realization of the stack controller, \nnow given in terms of basic signals. Example 2. We begin with a specification, due to Wand 116], for \nan algorithm to do minimax searching of a game tree. Each node of the tree is either a leaf with a numeric \nvalue or it has a son. Each son except the last has a next brother.  F+(~) .-Icup.(~)-- ~.t(~l,G+(0ons(xl}. \n1 G+(z) *ffi Iost~.(x)\"* F-(z),maz(F-(z),G+(nczt{z))).} } [  F-(z) *= leaf?(z)-.~vol(z),G-(sons(z)). \nI a-(~) ,ffi I.t:(~)--.r+(~),mln(F+(~),G,(next(x))). I1 STEP 0. Highlights, The derivation below involves \nnine transformation steps, some of which are incidental to the main topic. Steps 1 through 3 review Wand's \nelegant partial linearization of the ini- tial form through an analysis of its continuations. Step 4 \nuses a construction to complete the transformation to iterative form. Steps 5 and 6 are simplifications \nleading to the construction or a realization in Step 8. The final step uses the factorization dis- cussed \nabove to decompose the system into abstract components. The first few steps are applied simultaneously \nand sym- metrically to the maximization and minimization functions. Only the maximization versions are \nshown. The operations maz and rain are associative with identities -vo and + ~; they also commute with \neach other under conditions that will be main- rained in this derivation. This suggests the introduction \nof an accumulator~ to move the calls to maz and rain inside the recur- tWe are generalizing the transformation \nfrom L[z) \u00a2. p[z)-* v(z),l(h(z},L(s(z))} to L'(u, z) 4= p(z) -. l(u, v(z)), L'(y(u,A(z)), 0(z)) which \nguarantees that L(z) = L'(ll,z) whenever .f is an associative operation with identity I t . 222 sire \ncall to G. In the case at hand the required output assertion is F+(a,b,z) = maz(a, min{b,F+(z)) if a<L \n and accordingly the initial form is transformed to F+(a,b, z) *ffi lea.n(z)-~ max(a, min(b, val(z))), \nG+(a, b, sons(x)). [ G+(a, b, x) last?(z)-~ F-(a,b,z), max(a, min(L maz(Fg(z), G+(next(z))))).l STEP \n1. Let u = Fg(z) and v = G+(next(x)). In G + we have max(a, rain(b, max(u, v))) = max(a, max(rain(b, \nu)', rain(b, v))) [ b distributes = max(max(a, rain(b, u)), max(a, rain(b, v))) ] a distributes = max(F'f{a, \nb, x), max(a, rain(b, v)))) [ induction Ilence, the call to the old version of F~ is eliminated. Elimination \nof G + is deferred; we introduce an auxiliary function H + to clarify the next step. F+(a,Lz) ~ leaf!(z)-, \nmaz(a, min(Lvai(x))), G+(a, b,sons(z)). G+(a,b,x) last?(x)---. F-(a,b, x), H+(a, b, x, F-(a, b, x)) It+(a, \nb, x, v) ~ max(v, max(a, rain(b, G+(ncxt(x))))). STEP 2. If a __ b then a _ max(a,min(b,v)) = min(b, \nmax(a,v)) </~, for any v. Ilence, F-(a,b,x) ~ max(a,min(LFg(x)) is also bet- ween a and b. We may therefore \nreplace the detlning expression for H + by the conditional H+(a,b,z,v) ~ (v>_b)-.v, maz(v, min(b,G+(nezt(z)). \nH + performs the Ualpha-beta search cutoff\". Now eliminating the old version of G +, It + becomes lt+(a,b,x,v) \n*= (v'>b)-.v,G+(v,b, next(x))). Formal parameter a is no longer used in H + and can be eliminated.f \nf The corresponding minimization functions are F-(a,b,z) = /ea$?(z) -- maz(a, rain(b, vat(z))), G-(a, \nb, sons(z}). G+(a, b, z) last?(z) -, F+(a, b, z), H-(a, F+(a, b, z), z). H+(a, v, z) (v~_a).-., v,G-(a,v, \nnezt(z)). t af:(x) -. maxla, miniS, rot(z)))' 1 G+(a, b,sons(x)). G+(a,b,z) iast?(x) F-(a, b, x), I \nH+(F-(a, b,z), b, z). ] ll+(v,b,z) ~= (v>_b) v'G+(v'b'nezt(z))\" I STEP 3. This brings us to the end \nof Wand's treatment of the alpha-beta search strategy and we note, as he does, that one can justify the \nstrategy by analyzing the control structure of mini- max searching rather than by contemplating the data \nstructure of the game tree. To reach a realizable form the non-linear term in G + must be eliminated. \nWe elect to implement this recursion rather than remove it. A standard technique is to introduce a continua- \nt.ion and to look for a factorizatiou of G + that linearizes its defining expression modulo the continuation \n[15]. Then, code is added to the system that supports, say, a stack representation of the continuation. \nWe do both steps at once here using a con- struction similar to that of Wand and Friedman [16]. In this \ncase the faetorization is obvious: we should eval- uate F-(a,b,z) with continuation G +. A return function \nR is added to implement the continuation. The stack is special- ized to accept an intermediate value \nz, a node x, and a \"return token\" r, in a single push. A notation that suggests field ex- traction is \nused to access stacked values. For example, the term top.z(c) returns the node last pushed on c. We take \nthe liberty of initializing our stack with a halt return token. Let mm(a,b,x) *= max(a, min(b, val(x))). \nF+(a, b, x, c) Icaf?(z) --~ R(mm(a, b, x), top.z(e), top.x(c), top.r(c),pop(e)), G+( a, b, sons(x), c) \n G+(a,b,z,c) ~ F-(a,b,x, push(b,z, lax, c)). ll+(a,b,z,c) (a> b) -~ R(a, top.z(c), top.z(c}, top.r(c),pop(c)), \nG+(a, b, next(x), c). R(v,z,x,r,c) {r ~ halt)-~ v, (r ----=ax) - last?(x) -, R(v, top.z(c), top x(c), \ntop.r(+), pop(c)), H+(v, z, x, c) (r = min) -- tast?(x) -. n(v, top.z(c), top.x(c), top.r(c), pop(c)), \nH-(z,v,x,c) STEP 4. 223 This transformation is inetBcient when considered as a program transformation. \nIt would be better to place the unstarking opera- tions in the procedure R, thus sharing redundant argument \ncom- putations [16]. Of course, this has no bearing on the correctness of the transformation and it will \nturn out that these computa- tions are in fact shared in the realization. The main point here is the \nintroduction of a new type abstraction to the specification, although the added type con- veniently helps \nus to obtain an iterative version of the algorithm. Our concern is how to maintain this abstraction in \nthe coming realization. Now is a good time to do some simplification. The specification below is obtained \nfrom Step 4 and the symmetric minimization functions by \"unfolding\" (replacing a function by its definition) \nH + and H~'; unfolding G + and G~\" twice each; and rearranging R's conditional to combine like branches. \n F+(a,b,z,e) *= lea/?(z) --* R(mm(a, b, x), top.z(c), top.z(c), top.r(c), pop(c)), F-(a, b, so.s(z), \npush(b, so.s(z),--ffi, c)) F-(a,b,z,c) 4= leaf?(z).-~ R(mm(a,b,z),top.z(c),top.:r(e),top.r(e),pop(c)), \nF+(a, b, sons(z), push(a, sons(z),-,in, e)) R(,,,z,z,r,e) 4=. (r = halt) -* v, last:(z) --. R(,,, top.z( \nc), top.z(e), top.r(e), vop(c)), (r = ,.,z)-- (~>_:) -. R(~, top.:(e), top.x(e), top.de),pop(e)), F-(~, \nz, .eat(=), push(z, next(x), .u, e)) (r = sin) -. (vc~z) \"\" R(u, top.z(e), top.z(c), top.r(e),pop(c)), \nF+(z, t,, ,,cat(z), push(z, .ezt(x).,.in, c)) STEP 5. A realization is now at hand; since Step 4 the \nspecifications have been in iterative form. By a familiar construction [7] Specifica- tion 6 can be transformed \nto a simple loop. Introduce a \"control token ~ , p, to represent the defined function symbols. This is \njust state generation [17] without the secondary notation of a flowchart. At the same time, generalize \neach function definition to obtain a uniform state descriptor. By convention a \"don't-care\" value, \u00a2, \nis passed as a dummy argument. We do some register optimization in R by renaming formal parameters v \nand x to a and b. S(p,a,b,z,r,c) ... (p =- r.)-. leap(x) --S(R, ram(a, b, z), top.z(c), top.z(e), top.r(c), \npop(e)), S(r-, a, b, sons(x), \u00f7,push(6, sons(z),au, e))  (p = r-)-. leap(z) -, .~(It, mm(a, b, z), top.s(c), \ntop.z(c), top.r( c ),pop(c)), S(F\u00f7, a, b, sons(x), ~, push(a, sons(z),-.in, c)) (p = It) -. (r .... halt) \n--. a, last,'(x) -. s(It, a, top...(c), top.z(c), top.r(c),pop(e)), (r =..ax) -. (a >~) -. s(a, a, app.<c), \ntop.z(c), top.r(c), pop(e)), S(V-, a, b, next(z), #, push(b, neat(z), max, c)) (r --..in) --. (a <b) \n-. s(n, a, top.~(c), top.x(e), top.r(c), pop(c)), S(F+, b, a, next(z), \u00f7, push(6, nezt(z),,.in, c)) \nSTEP 6. By Theorem 3 this is essentially a realization, once we perform the ungainly task of distributing \nthe tests through the call to S. Define a combinator for the declsion-maklng mechanism: select(p, r, \na, b, z, v,, v~, vs, v., vs, vs, vT, vs, v.) *= (p -F\u00f7)-. [leap(x)-. ,,,,,,,)], (p -r-) --[leap(x) -. \n',3,,~,)], last:(z)-, vs, (r ---.)--[(a >~)-. ~s, ~], [(a_<~) -. ~., ~.l, This conditional can be implemented \nwith muz operations. With select, Step 6 is rewritten as a simple loop S(p,a,b,z,r,c) ~= and((pffi-R),(r=.halt))-'-,a, \n5( select(p, r, a, b, z, R, F-, It, F% It, It, F-, It, F+), select(p, r, a, b, z, ram(a, b, z),a, ram(a, \nb, z), a, a, a, a, a, b), select(p, r, a, b, z, t., b, G, b, t., t., b, t., a ), select(p, r, a, b, \nz, ts, s, t~, s, G, ts, n, as, n ), select(p, r, a, b, z, tr, ~, tr, #, t,, G, ~, tv, #), select(p, r, \na, b, z, pop(c), push(b, sons(z), .,,z, c), po~ c), push(a, sons(:), aln, e), pop(c), pop(c), push(b, \n.eat(x), rex, e ), pop(c), p.sh(b, next(z), ..in, c) tu~ere t, --top.z(c) s --sons(x) t. --top.z(e) n \n.= neat(x) tv =\" top.r( e ). STEP 7. 224 In writing the realization the combinators mm and select \nmay be carried over as packaged components. Like terms are now identified as signals. We dispense with \nthe use of boxes to dis- tinguish components. Let c o == push(hnl t, O, \u00a2, emptystack ). P F+ ! select(P, \nR, A, B, X, It, P-, it, F\u00f7, it, it, F-, it, F+), A ==-co ! select(P,R,A,B,X,M,A,M,A,A,A,A,A,B) B ==+co \n! select(P,R,A,B,X, Ts,B,T,,B, Tz,Tz,B, Tz,A) X = z \u00b0 ! select(P,R,A,B,X, Tz, S, Tz, S, Ts, T,,N,T=,N) \nR : 4! seicct(P,R,A,B,X,T,,C,T,,4,T,,T,,C,T,,\u00a2) C = e\u00b0 ! select(P, R, A, B, X,pop(C),push(B, S,asx, C), \n pop(C), push(A, S, nin, C), pop(C), pop(C),push(B,N,--x, c), pop(C), push(B, N, =,in,C)). S : son4X) \n N nezt(X) M == mm(A, B, X)  7\", : top.r( C ) 7\", := top z(C) 7\", : top.=( C ) RDY =, ana]eq?(P, \nIt),eq?(R,halt)) ANS == A STEP 8. This circuit computes S(F*, - co, + co, z o, ~, e \u00b0) by Theorem 3. \nS is partially correct (totally correct if z \u00b0. is finite) with respect to Fs+( -co, + co, z e) because \ncorrectness-preserving transforma- tions were used to derive S. Hence, S realizes F~'(z\u00b0), Steps 1 through \n3 being in effect a subgoal induction [13]. By symmetry, S also realizes F~\" if P is initialized to F-. \nWe now introduce an almtract component to recover the stack abstraction as discussed earlier. The instructions \nneeded are push and pop; stacked values are selected by reading the defining equation for C. C is replaced \nby the multiple output component (T., T., T,) -- STACK(\u00a3 V., V., V,) I m select(p, r, a, 6, z, pop, push, \npop, push, pop, pop, push, pop, push) V, == seleet(P,R,A,B,X,O,B,O,A,\u00a2,O,B,O,B) V, == select(P, R,A, \nB, X, \u00a2, S, \u00a2, S, \u00a2, \u00a2, N, \u00a2, N) Vr == select(P, R, A, B, X, \u00f7, Bx, 4, sin, 4, 4, max, 4, aim) 225 STACK(I,Z,X,R) \n*= ([~z-~C),~-~C),~-~C)) oJ~ere c .=  c o !~I,Z,X,R,C) interpret(i, z, z, r, c) \u00a2= (i ~ push) \"* push(z, \nz,r,c), (i = pop) ---* pop(c). Using STACK in place of C we obtain the realization P == F* ! select(P, \nR, A, B, X, It, F-, It, F*, it, it, F-, R, F*), A = -co ! 8elect(P,R,A,B,X,M,A,M,A,A,A,A,A,B) B = +co \n! 8cleet(P,R,A,B,X,T,,B,T,,B,T,,T,,B, Tz,A) X == z \u00b0 I sclect(P,R,A,B,X,T,,S, Tz, S, Tz, T,,N, Tz, N \nR == \u00a2! select(P,R,A,B,X,T,,\u00a2,T,,\u00a2,T,,T,,\u00a2,T,,\u00a2) S = so.s(X) N == nezt(X) M ~ ram(A, B, X) RDY ~ ond(eq?( \nP, It), eq?(R,halt)) ANS ==A I == select(p, r, a, b, z, pop, push, pop, push, pop, pop, push, pop, push) \n V, == 8elect(P,R,A,B,X,\u00a2,B,\u00a2,A,\u00f7,h,B,\u00a2,B) Vz ~, select(P, R, A, B, X, \u00a2, S, ~, S, \u00a2, \u00a2, N, \u00a2, N) V, \n== select(P, R, A, B, X, O, max, \u00a2, nin, O, \u00a2, ,.ix, \u00a2, nin) (T,, T,, 7\",) *,* STACK(I, V,, V,, V,) STEP \nO. The corresponding scbematie is shown in Figure 1. Figure 3 is a D^IsY experiment on the game tree \nin Figure 2, trac- ing signals P, R, A, B, and X. The experlment-expression is Transpose'.< P it A B \n<Format*> :<X>> where Format prints X's value if it is a leaf and (s tree) otherwise. There is, of course, \nplenty left to do. The select com-ponents can in most cases be simplified; signal R, for example, can \nbe defined as \u00a2 ! Tr since its value doesn't matter unless the stack is popped. For the same reason, \nV, isn't needed; the output of the selector for register X can be used. Signal Vr can be replaced by \nP if the tokens F*, F-, atx, and minare encoded carefully. Moreover, the signal X, should get the same \ntreat- ment as the signal C did; the game tree should be managed by an abstract component. 7. Conclusions \nEstablished applicative programming techniques adapt to the derivation of synchronous digital realizations. \nSince in- dependence of time is basic to both structured digital design and functional programming, perhaps \nit is not so surprising that the approach works. However, the real benefit seems to lie in the suitability \nof functional notation as a description language for connectivity. The basic algebra of terms commutes \nfreely with interpretation of symbols as components, as do the most com- monplace decomposition techniques. \nD^lsY's ability to compute data recursions leads to a representation of signals as streams. It is fairly \nstraightforward to experiment with logical circuit behavior. A facility for rapid prototyping is important \nin the absence of a mechanical trans- formation system, if only to give evidence that there are no typographical \nerrors in a derivation. Moreover, as the engineer focuses on design refinements the global sense of what \na circuit does is lost. A simple experiment denying the validity of a local change can save the considerable \neffort of a futile attempt at for- mal verification. Of course, an executable description language is \nalso a hospitable environment for informal design synthesis. Considered as a formal system, signal equations \ndescribe local and discrete temporal relationships among behaviors. Our derivations add a single global \nassertion, name.ly, that a specified value eventually appears in the system. A formal treatment of circuit \nrefinement seems to require a temporal logic (e.g. an approach like [6]) that allows the engineer to \nfocus on local be- havior. In the setting presented here, designs are synthesized from an inital output \nassertion in the form of a recursion equa- tions. Of interest is how temporal assertions might arise \nfrom, or be synthesized through, this design method. The realization language is related in spirit to \nGordon's denotational model for register transfer [5]. The relationship is most clearly seen in DAISY's \nsequential representation of be- havior. One notational difference is the use here of argument positions \nrather than port identifiers to describe connection. To its possible detriment, applicative notation \nimplies a direction in behavior. Components map inputs to outputs making the treat- ment of tri-state \nlogic and input-output ports problematical. The immediate obstacle to advancement of the method proposed \nhere is its assumption of synchrony. For example, what happens if the abstract component STACK comes \nwith its own clock? We have the choice of moving to a self-tlmed [12] interpretation of the realization \nlanguage but self-timed be- havior, too, must be implemented. One would expect there to be a \"handbook\" \nof standard transformations that coordinate autonomous systems; this seems to be the way digital designers \napproacl, such problems. Acknowledgements ! am grateful to Daniel Friedman, Franklin Prosser, Mit- chell \nWand, and David Wise for their guidance in the investiga- tions that lead to the approach presented here; \nand to John O'Donncll for his encouragement and helpful suggestions. Anne Kohlstaedt is co-implementor \nof DAiSY--whose development con- tinues. References [1] Asheroft, Edward A. and William W. Wadge. \"Lucid, \na nonprocedural language with iteration, ~ Comm. ACM, g0(7):519-590 (July, 1977). [2] Darlington, J. \nand R.M. Durstall. \"A system which au- tomatically improves programs,\" Acta lnformatica, 6:41-60, (March, \n1976). [3] Friedman, Daniel P. and David S. Wise. ~CONS should not evaluate its arguments, ~ in Automata, \nLanguage8 and Programming, ode. S. Michaelson and R. Milner~ Edin-burgh Univ. Press, Edinburgh, 1976, \n257-284. [4] Friedman, Daniel P. and David S. Wise. \"Functional combination,\" Computer Languagem 8(1):31-35, \n1978. [5] Gordon, Michael J.C.. A Model of Register Transfer Systems with Applications to Mirocode and \nVLSI Cor- rectness. Department of Computer Science Technical Re- port CSR-82-81, Univ. of Edinburgh, \n1981. [6] llalpern, Joseph, Zohar Manna, and Ben Moszkowski. \"A hardware semantics based on temporal \nintervals,\" Lecture Notes in Computer Science No 15~, Springer, New York, 1983, pp. 278-291. [7] llarel, \nDavid. \"On folk theorems,\" Comm. ACM, 23 (7) :379-389 (July, 1980). [8] Henderson, Peter. \"Purely functional \noperating systems,\" in Functional Programming and ira Applications, eds. J. Darling'ton, P. ttenderson, \nand D.A. Turner, Cambridge University Press, Cambridge, 1982, 180-195. [9] Jackson, Philip C.. Introduction \nto Artificial Intelligence, Perochelli/Charter, New York, 1974 [10] Johnson, Steveu D.. Synthesis of \nDigital Designs from Rccurslon Equations, Ph.D. dissertation, Indiana Univer- sity Computer Science Dept., \nBloomington, 1983. [11] Kahn, Gilles. A preliminary theory for parallel programs. Rapport de Recherche \nNo. 6. IRIA l, aboria, (January, 1973). [12] Mead, Carver and Lynn Conway. Introduction to VLSI Sllstems, \nAddison-Wesley, Reading, 1980. [13] Morris, James H. Jr., and Benjamin Wegbreit. \"Subgoal induction,\" \nComm. ACM20(4):209-222 (April, 1977). [14] Sethi, R. and Tang, A.. \"Constructing call-by-value con- tinuation \nsemantics,\" J. ACM 87(-):580.597, (1980). [15] Wand, Mitchell. \"Continuation based program trans-formation \nstrategies,\" J. ACM, ~7(1j:104-180, (January,   1980). [1G] Mitchell Wand and Daniel P. Friedman. \n\"Compiling lamb- da expressions using continultions and factorizations,\" Journa/of Comp. Lang. $, (1978) \n241-263. [17] Winkel, David and Franklin Prosser. The Art of Digital Dealgn, Prentice-Hall, Englewood \nCliffs, New Jersey, 1080. 226  [ P R A e x ] ((doF+ ? -9 10 (a tree)) (doF-? -9 10 (a tree)) (doF+ \n? -g 10 (a tree)) (doF-? -g 10 (a tree)) (doF* ? -9 10 (leaf 2)) (dnR man 2 -9 (leaf 2)) (doF* ? -9 2 \n(leaf 9)) (doR man 2 -9 (leaf 9)) (doF+ ? -9 2 (leaf 3)) (doR nln 2 -9 (leaf 3)) (doR aax 2 10 (a tree)) \n(doF-? 2 I0 (a tree)) (doF+ ? 2 10 (leaf 1)) (doR nin 2 2 (leaf 1)) (doR max 2 10 (a tree)) (doF-? 2 \n10 (a tree)) (doF+ ? 2 I0 (leaf -I)) (doR nin 2 2 (leaf -I)) (doR max 2 10 (a tree)) (doR nan 2 -g (s \ntree)) (doF* ? -9 2 (a tree)) (doF-? -0 2 (a tree)) (doF+ ? -9 2 (leaf 6)) (doR nln 2 -9 (leaf 6)) (doF+ \n? -9 2 (leaf 8)) (doR nin 2 -9 (leaf 8)) (doR max 2 2 (a tree)) (doR ntn 2 -9 (a tree)) (doR nax 2 10 \n(a tree)) (doF-? 2 10 (a tree)) 21GURE !. Schematic of Step 9. (doF+ ? 2 10 (a tree)) (doF-? 2 10 (a \ntree)) (doF+ ? 2 10 (leaf 0)) (doR nin 2 2 (leaf 0)) (doR max 2 10 (a tree)) (doF-? 2 10 (a tree)) (doF+ \n? 2 I0 (leaf -5)) -rJ/~ 'D L1/~ ~'~ ~n (doR nin 2 2 (leaf -5)) /\\ /\\ /\\ /\\ (doR aax 2 10 (a tree)) (doR \nain 2 2 (a tree)) (doR max 2 10 (a tree)) (doR halt 2 ? (a tree)) FIGURE 2. Game tree used in the experiment \nin Figure 3. (e/ FIGURE 3. DAISY experiment with the game tree in Figure2 [g], Fig. 4-5, pg. 133.) Sons \nare leftmost. Leaves that should be using the realization in Step 9. Signals P, R, A,B, and Xare visited \nare marked. traced. 227   \n\t\t\t", "proc_id": "800017", "abstract": "", "authors": [{"name": "Steven D. Johnson", "author_profile_id": "81332507154", "affiliation": "Computer Science Department, Indiana University, Lindley Hall 101, Bloomington, IN", "person_id": "PP39073920", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/800017.800533", "year": "1984", "article_id": "800533", "conference": "POPL", "title": "Applicative programming and digital design", "url": "http://dl.acm.org/citation.cfm?id=800533"}