{"article_publication_date": "01-15-1984", "fulltext": "\n Editing by Example (extended abstract) Robert Nix Yale University f Abstract An editing by example \nsystem is an automatic program synthesis facility embedded in a text editor that can be used to solve \nrepetitive text editing problems. The user provides the editor with a few examples of a text transformation. \nThe system analyzes the examples and generalizes them into a program that can perform the transformation \nto the rest of the user's text. This paper presents the design, analysis, and implementation of a practical \nediting by example system. In particular, we study the problem of synthesizing a text processing program \nthat generalizes the transformation implicitly described by a small number of input/output examples. \nWe define a class of text processing programs called gap programs, characterize their computational power, \nstudy the problems associated with synthesizing them from examples, and derive an efficient heuristic \nthat provably synthesizes a gap program from examples of its input/output behavior. We evaluate how well \nthe gap program synthesis heuristic performs on the text encountered in practice. This evaluation inspires \nthe development of several modifications to the gap program synthesis heuristic that act both to improve \nthe quality of the hypotheses proposed by the system and to reduce the number of examples required to \nconverge to a target program. The result is a gap program synthesis heuristic that can usually synthesize \na target gap program from two or three input examples and a single output example. The editing by example \nsystem derived from this analysis has been embedded in a production text editor. The system is presented \nas a group of editor commands that use the standard interfaces of the editor to collect examples, show \nsynthesized programs, and run them. By developing an editing by example system that solves a useful class \nof text processing problems, we demonstrate that program synthesis is feasible in the domain of text \nediting. *This research was supported by NSF grant No. MCS8002447. \"['his paper summarizes the results \nof the author's Ph.D. dissertation, submitted to the Department of Computer Science. Yale University. \nAuthor's current address: Computer Science Laboratory, Xerox Pale Alto Research Center. 3333 Coyote Hill \nRoad, Pale Alto. California, 94304. Arpanet: Nix.PA@PARC. Permission to copy without fee all or part \nof this material is granted provided that the copies are not made or distributed for direct commercial \nadvantage, the ACM copyright notice and the title of the publication and its date appear, and notice \nis given that copying is by permission of the Association for Computing Machinery. To copy otherwise, \nor to republish, requires a fee and/or specific permission. An editing by example system, or EBE system, \nis an automatic program synthesis facility embedded in a text editor that can be used to solve repetitive \ntext editing problems. The user provides the editor with a few examples of a text transformation. The \nEBE system analyzes the examples and generalizes them into a program that can perform the transformation \nto the rest of the user's text. This paper reports on a theory, a design, and an implementation of an \nEBE system that has been implemented within U [30], a production text editor. To demonstrate the system, \nsuppose that a user of U wants to transform a long list of baseball scores like these: Yankees 3, Orioles \n1. Brewers 12, Cardinals 5. Dodgers 5, Braves 4. Braves 3, Dodgers 0. Reds 4, Mete 2.  Pirates 2, Phillies \n1. to a database input format: Game[winner 'Yankees', loser 'Orioles', scores [3, 1]]; Game[winner 'Brewers', \nloser 'Cardinals', scores [12, 5]]; Game[winner 'Dodgers', loser 'Braves', scores [5, 4]]; Game[winner \n'Braves', loser 'Dodgers', scores [3, 0]]; Game[winner 'Reds', loser 'Mets', scores [4, 2]]; Game[winner \n'Pirates', loser 'Phillies', scores [2, 1]]; He could write a program to make this transformation, or \nhe could perform the transformation manually using his text editor, but he decides instead to use U's \nediting by example facility. He enters U and begins his EBE session by selecting, or marking, his first \nexample: Yankees 3, Orioles 1. He then issues a command to the editor that tells it that the selected \ntext is an example of the sort of thing that he wants to change. That is, this line of text is the sort \nof input that he wants his transformation to affect. He then manually transforms that text to the database \nformat, using the editor commands he would normally use to make the change on a single instance of the \ntext: Game[winner 'Yankees', loser 'Orioles', scores [3, 1 ]]; Once he has finished changing the line, \nhe selects it and issues another command that informs the editor that the selected text is the output. \nThat is, this line of text is the sort of thing he would like the editing by example system to produce \nwhen it finds some text resembling the input. At this point, he could give a command telling the editor \nto synthesize a program that generalizes the transformation expressed by his example. However, he knows \nthat the system's generalization of a single example is a trivial program that transforms all instances \nof the literal input text of the example to the literal output text, and since the rest of the scores \nare not simple repetitions of Yankees 3, Orioles 1., he gives another example by selecting and transforming \nthe Brewers score in the same way. &#38;#169; 1983ACMO-89791-125-3/84/001/0186 $00.75 186  After providing \nthe second example, the user feels that the system should be able to make a decent generalization, so \nhe asks to see it. The editor shows him the synthesized program in a specialized notation for string \nsearch and transformation: bol-I- LJ -2- ,LJ -3- U -4-. eel =\u00a2\" Game[winnen..l' -I- ',uIosen..l' -3- \n',L.JscoresLi[ -2- ,Id -4- ]]; eel This notation defines a simple program called a gap program. Gap programs \nhave two parts: the part preceding the \"=*\", which is called the gap pattern, and the part following \nthe \"=>\", which is called the gap replacement. The gap pattern is a string matching pattern composed \nof constants and variables that describes the format of the fragments of text that the user wants to \nchange. The constants in the expression above are the characters in the sans serif font, like \",LI\" and \n\"Game[winner'\", which match their literal text (U is a visible space character), along with the special \nconstants bol and eel that match the unprintable text fragments beginning-of-line and end-of-line. The \nvariables in the input pattern are signified by the numbers between hyphens. A variable (also called \na gap) matches any sequence of characters up to the constant string that follows the variable in the \npattern. In this pattern, the variable -I- matches the characters between the beginning of the line and \nthe first space. Each of the elements of the output replaccment is a constant string or a variable from \nthe input pattern. Programs in this language execute by searching for some part of the user's text that \nmatches the input pattern. When matching text is found, it is replaced with a concatenation of the constants \nof the output pattern together with those parts of the text that are matched by variables contained in \nthe output pattern. For example, when the input pattern fragment \"LJ-3- El\" is matched against the text \n\"', Orioles \"', the variable -3- is bound to the text \"Orioles\". This text will be copied over to every \nplace that a -3- occurs in the output replacement expression. The searching process is continued after \nthe point of replacement. and the program stops when no matching text is found. Back to the scenario: \nThe user decides that the synthesized program looks like it will work, so he gives a command that runs \nit in single-stepping mode. [n this mode, the editor asks the user for confirmation before transtbrming \nthe text that matches the pattern. The \"Dodgers...\" score is selected as the next part of the file that \nlooks something like the two examples given so far. He confirms that this is a good choice, so the system \nreplaces the \"Dodgers...\" line with the appropriate \"Game[...\" line and asks him whether the transformation \nwas correct. It was, and the user thinks that the program will work for the rest of scores, so he finishes \nthe job by telling the EBE system to continue and transform the rest of the file. A practical EBE system \nThis paper presents the design of a practical editing by example system. Perhaps the simplest kind of \nEBE systems are the program transcription facilities, or keystroke macros, that are present in many text \neditors such as I.'.MACS and Z [28, 38, 43]. Other programming systems have been designed along these \nlines [11, 19, 26, 37, 44, 45]. These program transcription systems all represent interesting and useful \nresearch in user interfaces for programming: however, we are interested in building an EBE system that \ncan perform more ambitious generalizations of the behavior shown it. Given this desire to perform ambitious \ngeneralizations, there are still an enormous number of different ways to design and build an EBE system. \nOur first stab at reducing the scope of the problem is to form a simple and not too restrictive model \nof the process carried out by an editing by example system: The goal of the EBE system is to find a target \nprogram that will solve the user's text processing problems. Towards this end, the EBE system collects \nsample data that describes the desired behavior of the target program and uses its synthesis procedure \nto map from the sample data to a runnable program. If the user is completely satisfied with the synthesized \nprogram, he can run it over and over again until he is through with his editing task. On the other hand, \nif the program does not satisfy him, he can cause the system to create a better program by supplying \nmore data to the synthesis procedure and beginning the process anew. This view of editing by example \nraises several questions: What sort of programs does the EaE system synthesize? What sort of information \ndoes the user provide the EBE system? How does the system synthesize the programs from the information? \nWhat sort of interface does the user see? The answers to these four questions are closely interrelated, \nbut we will attempt to treat them one at a time. We begin by discussing the kind of program that we will \nsynthesize, and we then decide upon the information upon which we base the synthesis. These two decisions \ngreatly determine the structure of the system, and within that framework we then describe the development \nof an algorithm for text program synthesis. We close with a brief sketch of the user interface and a \nconclusion. This paper is a summary of a somewhat lengthier document [31]: as such, it omits many examples \nand all proofs, algorithm listings, literature surveys, and test results.  What sort of programs? '['he \ngoal of an EBE system is to synthesize programs that help a user transform his text in some regular \nmanner. Text is a ubiquitous data structure that can be used in a natural way to represent almost anything, \nso it is possible that these programs could be called on to perform arbitrary computation. However. in \norder to build an effective and practical EBE system, we restrict our attention to solving some of the \ntypical problems encountered while editing texL Many problems come to mind. The user might be performing \na pattern directed scan and edit as was shown in the opening example. The user might be performing some \nknowledge-based function on his text. such as renumbering a list or changing digits like \"9\" to names \nof months like \"September\". The user might be performing a specialized procedure on his text: sorting \nsome lines, adding up columns of numbers, filling and justifying paragraphs, or performing the join of \na database relation. Or the user might be manipulating the text as if it represented a more complicated \ndata structure such as a program parse tree. Although many problems come to mind. if we are to make \nprogress on a practical editing by example system, we must concentrate on one class of them. We have \nchosen to concentrate on synthesizing programs that scan and edit text, as in the introductory baseball \n score example. While such text processing problems can be solved using general-purpose programs, AI \nresearch in automatic programming has not yet yielded a practical method for reliably, robustly, and \nefficiently synthesizing general purpose programs from example information. Thus the approach of adapting \na general purpose program synthesis strategy to the synthesis of general purpose programs that just happen \nto be scanning text would probably not yield a practical system. The approach that we take instead is \nto consider synthesizing programs of limited power that are specialized to string scanning. String scanning \nprograms are often specified using a grammatical pattern matching notation. The study of grammatical \ninference is concerned with the problem of synthesizing patterns from examples; some surveys of grammatical \ninference include those of Biennann and Feldman [7], Fu and Booth [16], or Angluin and Smith [4]. Perhaps \nthe two best known formalisms for describing the syntactic structure of text are regular expressions \nand context free grammars. Unfortunately, although many algorithms have been developed for synthesizing \nregular expressions, context free grammars, and their subclasses from examples [2, 4. 5, 6, 8, 9, 10, \n14. 15, 17, 1.8. 22, 25, 32, 40, 42] none of these algorithms performs well enough in terms of running \ntime, amount of data required, and quality of hypotheses proposed to be used as the pattern synthesis \ncomponent of a useful EBE system. However, pattern matching formalisms of less power than regular expressions \nhave been shown to be quite useful in text processing applications (cf awk [1] and POPLAR [29D. Guided \nby these systems. and by our own experience with building and using text processing tools [12, 13], we \ndecided to use a text processing language of limited power whose, programs could be effectively synthesized \nfrom examples. The programs in the limited language are called gap programs. Gap programs Gap programs \nare the class of pattern ==~ replacement programs introduced in the opening example: the pattern is a \ngap pattern, and the replacement is a gap replacement. Gap patterns bear a resemblance to Angluin's regular \npattern languages [3] and Shinohaxa's extended regular pattern languages [35. 36]. A gap pattern G over \nan alphabet Y- is a sequence of alternating strings and gaps, soglstg2s~..gnsn. The strings si are drawn \nfrom ~+ (although so may be the null string, except when n=O), the gaps gi are distinct symbols drawn \nfrom a gap alphabet E\u00a2; that is disjoint from Y-, and the number of gaps n is greater than or equal to \n0. The constant subsequence of a gap pattern, denoted c(G), is the string sosis2...sn. Similarly, the \ngap subsequence of a gap pattern, denoted g(G), is the string glg2...ga Our first san~ple gap pattern \nis made up from a single constant string: thiaUisUal_Igapupattern This gap pattern matches the constant \nstring \"this is a gap pattern\". Our notation for constant strings will be those strings appearing in \na font like \"thisLl\" one, where \"U\" is meant to be a visible representation of the space character. The \nsecond sample gap pattern contains a gap: thiSlJ -I- LJgapupattern The characters \"-I-\" together make \nup a single gap symbol. This pattern will match strings that begin with \"thisU\" and end in \"LlgapLIpattern\", \nwith the gap spanning the characters in between. The next gap pattern does not have a leading constant \nstring: -i- Llpattern This pattern will match strings ending in \"Llpattern\". Another example, OearLI \n-1-, eol Congratulationsu -2-!LJLiYouubaveubeenuselected denotes a gap pattern that uses two gaps to \nmatch the first few lines of a form letter. Each gap symbol must be distinct; for example, the symbol \n-I- must occur only once in the gap pattern. This text thisUisLInotUaLJgapLJpattern \"I- is not a gap \npattern because all gap symbols in a gap pattern must be followed by a constant string. This is also \nnot a gap pattern thisU -!- -2- nottJalJgapLJpattern because gap symbols must be separated by a constant \nstring. When a gap pattern is matched against a piece of text, each of the gap symbols in the pattern \nis bound to the substring of the text that is matched by the gap. This substting is defined so that the \nmatching process is deterministic; a gap symbol gi that is followed in the gap pattern by a constant \nstring sl will match text only so long as it does not include si. Formally, define the set of legal substitutes \nfor a gap, gi that is followed by the string si to be the set of all strings aEY- in which the lef~nost \noccurrence of s~ as a substring in the string asi is at the end of asi. Then the language [.(G) defined \nby a gap pattern G=soglSlg2s2...gns n is the set of all strings of the form soalSla2s2...ans n where \neach ai is a legal substitute for gi. A string s is matched by a gap pattern G if sEL(G); a set of strings \nS is matched by G if SC_I.(G). For example, when the gap pattern \"-!- abe\" is matched against \"'xyzabc\", \nthe gap -I- matches \"'xyz\" and the pattern matches the entire string. When the pattern matches the string \n\"'abe\", it binds the gap -I- to the null string. When it matches \"'ababc\". it binds -I-to \"ab\". However, \nwhen the pattern is matched against the string \"abcxabc\". it matches only the prefix \"'abe\" and fails \nm match the entire string because the gap -I- is defined so as not to match a string that includes \"abe\". \nFormally, define a parse of string sEL(G) relative to a gap pattern G=sogtstg2s~..gns, to be a sequence \nof n strings PI,P2,...,Pn such that S=soPlSlP2S2...pns n and Pi is a legal substitute for g~. The gap \npattern of the gap program matches text and parses it into those pieces that match the constants and \nthose pieces that match the gaps. Then the gap replacement expression is used to compute the new string \nthat will replace the string matched. A replacement expression R for a gap pattern G over an alphabet \nY- with gap symbols {gl,g~,...,gn} is a string from (Y.U{gl,g2,...,gn}) \u00b0. Gap replacement expressions \nare not interesting objects in isolation; they are only of interest when they have been combined with \na gap pattern G into a gap program. A gap program P is a pair consisting of a gap pattern G and a replacement \nexpression R for G: the program is denoted G=>R. Here is an example of a gap program that will change \nthe phone number \"(20,3) 436-0715.\" to \"203-4,36-0715.\": (203) 4,36-0715. ==, 20,3-436-0715. This gap \nprogram generalizes the transformation to apply to all phone numbers of that form: ( -I- )El -2- -3-. \n=> -I---2---3-. This gap program replaces an area code with the word \"Call\": ( -I- )U -2- - -3-. =*. \nCalIu -2-. -3-. This gap program will delete a phone number entirely: ( -/- )u -2-. -3-. ==> This one \nwill interchange the area code with the first three digits: ( -I- )u -2---3-. ~ ( -2- )u -I---3-. This \nprogram performs another nonsensical transformation, duplicating the digits of the number in a plea~nt \npattern:  ( -I- )LJ -2- --3-. ~ -I--2- -3- -3- -2- -/- \"['his gap program matches phone numbers in the \n\"'(203)\" area code. and deletes the area code: (203)12 -I-- -2-. ===,-I- -2-. And this last changes numbers \nfrom area code \"'(211 )\" to \"(203)': (211)U -!---2-. =~ (203)LJ -I---2-. The intuitive descriptions of \nthe effects of these gap programs may be formalized as follows. If x and y are strings in Z\u00b0 and P- G=~R \nis a gap program, then P(x)=y if and only if G matches x yielding the parse Pl,PZ,...,Pnand if y is equal \nto R with Pi substituted for each occurrence of g~. Gap programs axe a fairly weak text transformation \nlanguage; some elementary properties of gap patterns and programs include: 1) Gap patterns parse strings \nuniquely into constants and gaps. 2) Gap patterns may be matched against text in linear time. 3) Gap \npatterns define languages that are a proper subclass of the regular languages. 4) The language generated \nby a gap pattern is either a singleton or infinite. 5) l'he set of languages defined by gap patterns \nis not closed under union, intersection, or complement. 6) Gap programs are not closed under composition; \nthere are transformations computable by the composition of two gap programs that arc not computable by \na single gap program. 7) Given an arbitrary finitely specified function, i.e. a finite collection of \narbitrary input/output pairs, there exist two gap programs that can be composed together to transform \neach of the given inputs to the corresponding output.  From what sort of information? A fundamental \naspect of the design of an editing by example system is the sort of information that the system requires \nfrom the user. Two types of information about the target program are easy for an EBE system user to provide: \none is the sequence of commands that the user employed while editing an example, and the other is the \nappearance of the example text before and after the edit. The sequence of commands is called a Irate \nof the target program, and the change in appearance is called the program's input~output behavior. The \ninformation contained in the command trace can be made to include everything contained in the input/output \nsamples, and more besides, so at first glance it seems obvious that an editing by example system should \nuse traces as its principal source of information. However, there are some problems with using traces. \nThe major problem is that traces are an unreliable source of information about the user's intent, e.g. \nstring search and cursor movement commands can often play the same role in moving to the next example, \nyet the commands are different. Another problem is that a system that uses traces might require the user \nto use similar commands in a similar order when giving two examples -a requirement that would be counter \nto the free-form nature of interactions in a well-designed editor. And a final problem is that a system \nthat generalizes traces would have to have knowledge of the semantics of every editor command, which \nwould inhibit editor extensibility and EBE system portability. The problems associated with using traces \nas the principal source of information in the EBE system led us to concentrate our efforts on algorithms \nthat work from input/output examples. Although input/output examples contain less information, they have \nnone of the problems that traces do. Given that we are going to use input/output examples, how many should \nbe required of the user to specify a function? While the amount of example data that a user can be imposed \nupon to provide is subject to many factors, such as the smoothness of the user interface, his knowledge \nof programming, and his mood. we suspect that people's tolerances are small. We suspect that five is \ntoo many examples to have to provide, that four is too many, that three is probably too many, and that \ntwo may well be too many. A single input/output example would be ideal. Unfortunately, a single example \ncannot impart the pattern that describes the text that the user would like to transform, and there is \nalso not much of a basis for'deciding on a non-trivial transformation that maps the input string of the \nsingle example to the single output. It is unreasonable to expect synthesis from a single example to \nyield useful programs. The gap program synthesis algorithms that we develop can usually converge to the \ntarget gap program after the user provides two or three examples of the target function's input, and \nwe describe a heuristic below that results in a single output example being all that is usually required. \n Synthesizing gap programs from !/0 examples An algorithm for gap program synthesis takes as input a \ncertain number of input/output examples that describe the behavior of the text transformation that the \n[!BI\" system user would like to perform. It analyzes these examples and proposes a gap program that can \ntransform each of the inputs to the corresponding output. The algorithm succeeds if it proposes a gap \nprogram that performs the desired transformation to the rest of the user's text. If the algorithm can \nidentify an arbitrary gap program after being given adequate data, then we say that the algorithm identifies \nlhe class of gap programs in the limit. Identification in the limit was introduced by Gold [17], and \nhas been explored by other researchers in inductive inference; Angluin and Smith give a good survey [41. \nAn algorithm that synthesizes gap programs in the limit from input/output examples could work by examining \nthe set of samples given by the user and returning the \"best\" gap program that can perform the transformation \nshown. The \"best\" gap program can be defined so that as more and more examples are given to the system, \nthe \"best\" program would be guaranteed eventually to be the program that the user bad in mind. In an \neffort to understand the problems involved with building such a system, we investigated the complexity \nof various subproblems related to gap program synthesis. One of our findings was Theorem: The problem \nof deciding whether a set of input/output samples describe a transformation that can be effected by a \ngap program is an NP-complete problem. This problem remains NP-complete even when there are only three \ninput/output pairs. An algorithm that synthesizes a gap program that performs the transformation described \nby a set of input/output samples must solve an NP-hard problem. Moreover, this problem remains NP-hard \neven when there were only three input/output pairs in the example set. This negative result coincided \nwith our intuition that tackling the gap program synthesis problem monolithically was too hard. However, \nwe still wanted to build an EBE system that could synthesize gap programs. Towards this end, we finessed \nthe problem by dividing and conquering, and decomposed the problem of synthesizing a gap program into \ntwo steps. The first step is to find the \"best\" gap pattern that matches all of the input strings in \nthe sample set. This gap pattern yields a parse of the the input samples, and the second step of the \nalgorithm is to attempt to find a replacement expression that can rearrange the parsed inputs to yield \nthe output samples. This decomposed process differs from the process of finding a gap program as a whole \nin two respects. The first is that the decomposed process can be done more efficiently. The problem as \na whole is NP-hard even when there are only three samples; however, we have been able to show that although \neach of the two steps of the decomposed process is also NP-hard, they can be solved in time polynomial \nin the size of the input when the number of samples is bounded. We have developed efficient beuristics \nthat the EBE system actually uses to perform each of the phases, and we have been able to prove that \nthe heuristics still identify gap programs in the limit from positive data. The second way that the decomposed \nprocess differs is that the first step, that of finding a gap pattern, is performed independently of \nthe second step of finding a gap replacement. It may be that the gap pattern found in the first step \nparses the input strings in such a way that it is impossible for any replacement expression to rearrange \nthe parsed fields to form the output. In this case, the algorithm terminates with the answer \"more data \nrequired\". We prove that the addition of new relevant data can make the decomposed process work, by making \nit find a gap pattern that parses the input so that a replacement expression can be found. The decomposed \nprocess gains efficiency by sometimes requiting the user to supply more data than a monolithic gap program \nsynthesis pn~,ess would require; in practice, this penalty is rarely paid. The gap program synthesis \nalgorithm that we describe below finds the \"best\" gap pattern that matches the input samples, and then \nfinds a replacement expression that can map the inputs to the outputs. Descriptive ,gap pattern synthesis \n Our definition of a \"best\" gap pattern that matches a set of strings S is that the \"best\" gap pattern \nis one that finds the greatest number of common distinctive features in the set. We call such a pattern \ndescriptive:, a gap pattern G is a descriptive gap pattern for a set of strings S if: l) G matches all \nof the strings in S. 2) G has the greatest number of constant symbols of any gap pattern that matches \nS. 3) Of the patterns that satisfy the previous two constraints, G has the fewest number of gaps. The \nfi~t two criteria form the core of the definition; we want gap patterns that find the largest number \nof common constants in the sample data. The third criterion is intended to remove from consideration \nthose gap patterns that find all of the common constants, but comain extraneous gaps. The name \"descriptive\" \nis justified by a result that shows that a descriptive gap pattern for a set of strings defines a minimal \ngap pattern language that contains the strings. The following results help to characterize the difficulty \nof the descriptive gap pattern synthesis problem: Theorem: Finding a descriptive gap pattern for a given \nset of sample strings is NP-hard. Theorem: There is an algorithm that can find a descriptive gap pattern \nfor n strings of length at most I in time O(I ~n\u00f7! log 1). The algorithm of the second theorem is of \ntheoretical interest only. We actually synthesize descriptive gap patterns using a combination of two \nheuristics: one that approximates the constant substring of the pattern, and one that tries to insert \ngaps into those constants to form a gap pattern. Finding the constants The constants are found using \nan algorithm that approximates the Longest Common Subsequence of a set of strings. The Longest Common \nSubsequence (LCS) of a pair of strings can be found in polynomial time [20, 23, 41] using a dynamic programming \nalgorithm, although the problem is NP-hard when the number of strings in the set is not bounded [27]. \nWe approximate the LCS of a set of strings {sl, s}...s~} by first ordering the set from shortest string \nst to longest string Sro and then computing the iterative pai_rwise approximation LCS(sn, LCS(sn-I,...LCS(s2,sl))). \nThis heuristic seems to produce good results in practice. Our current implementation of the heuristic \nmakes use of a pairwise LCS algorithm due to Hirschberg [20], and runs in O(n/\") time and linear space. \nAs an example of its output, when the system is set to analyzing the strings: Yankees 3, Orioles 1. Brewers \n12, Cardinals 5. Dodgers 5, Braves 4. it finds the common constants: esu,ursu. In this case, these common \nconstants are actually the LCS of the samples, although in general the heuristic is not guaranteed to \nfind the LCS of a set of more than two strings. Inserting the gaps The next step of the descriptive \ngap pattern synthesis heuristic is to insert gaps into the constant string to make it into a gap pattern. \nWe have not been able to classify the complexity of the gap insenior problem, although we suspect that \nthe problem is NP-hard in general. However, we have been able to find an algorithm that runs in polynomial \ntime for fixed n: Theorem: Given a constant string c and a set of n strings S. each of length bounded \nby I, there is an algorithm for inserting the minimal number of gaps into c to make it match S that runs \nin time O(/3n\u00f7~ log 1). This algorithm is also solely of theoretical interest. We actually perform gap \ninsertion using a simple heuristic that does a leftmost match of the constants against the ~rnple strings \nand inserts gaps where they are required. For example, the \"e\" of the constant string is matched against \nthe first \"e\" that occurs in each of the samples. The samples do not all contain \"e\" as a first character, \nso a gap is required before the \"e\" in the pattern. A gap is also required before the first \"s\", but \nnot before the first \"Lf' because there is a \"LI\" immediately following the \"s\" in each sample. This \nheuristic also runs in time O(n/2) and results in the pattern: -!- -2-~ -3- ,LI -4- r -5- sU -6-. This \ngap pattern looks a little noisy; in a few sections, we will describe heuristics that the EBE system \nuses to remove this noise. The constant synthesis and gap insertion heuristics can be combined into a \nheuristic algorithm which will identify gap patterns in the limit from positive data. Theorem: The descriptive \ngap pattern synthesis heuristics will identify the gap pattern of the target program in the limit from \npositive data. Replacement expression synthesis algorithm Once a descriptive gap pattern is found that \ncan describe the structure of the input strings, we must then find a way to produce the col'responding \noutput strings using that structure. For example, if these three lines were our input samples: Yankees \n3, Orioles 1, Brewers 12, Cardinals 5. Dodgers 5, Braves 4. Then the following descriptive gap pattern \nwould be found: -I- e -2- SLJ -3- ,U -4- r -5- sU -6-. The first gap in the pattern matches the string \n\"Yank\" in the first sample, \"Br\" in the second, and \"Dodg\" in the third. The second matches \"e\", \"wer\", \nand \"'r\", and so on: -/- -2- -3- -4- -5- -6- Yank e 3 O iole 1 Br wer 12 Ca dinal 5 Dodg r 5 B ave 4 \n This collection of input fragments is called the input sample parse. The replacement expression synthesis \nproblem is that of taking the fragments of text matched by gaps from the input, and a collection of output \nsamples, say: Game[winner 'Yankees', loser 'Orioles', scores [3, 1]]; Game[winner 'Brewers', loser 'Cardinals', \nscores [12, 5]]; Game[winner 'Dodgers', loser 'Braves', scores [5, 4]]; and finding a replacement expression \nthat will produce each of the outputs from the corresponding input parse: Game[winnerLJ' -i- e -2- s'LJIoserLf \n-4- r -5- s',LlscoresLJ[ -3- ,u -6- ]]; eel 190 While this example does not make the problem appear \ndifficult, synthesizing a replacement expression from the given fragments is in general a difficult task: \nTheorem: The problem of synthesizing a replacement expression that maps a given input sample parse to \na given set of outputs is NP-hard. However. our data does not seem to exercise the features that make \nthe problem intractable, and in practice we solve this problem exactly, without recourse to approximations. \nThe algorithm we use for finding a replacement expression has two phases. The first phase constructs \na finite automaton for each input/output example that describes all of the different replacement expressions \nthat yield the output example. The next phase of the algorithm finds a replacement expression that can \nproduce the output strings by intersecting the machines derived in the first step. The classical finite \nstate machine intersection algorithms can be used [21], and a replacement expression that simultaneously \nproduces each of the outputs from the corresponding input can be recovered by finding a path from the \nstart state of the intersected automaton to the accepting state. For example, if the two sample input/output \npairs were: abxbay ==> ababa cddxddcy ==, addcddo Then the descriptive gap pattern matching the two \ninputs would be \"-I- x -2- y\". When this pattern is matched against the first input it parses that input \ninto two gaps, \"ab\" and \"ba\". The algorithm then constructs the following machine that represents all \npossible ways of writing the first output using those inputs and the constants \"a'\" and \"b\": -1--2--1--2- \n It also constructs a similar machine that describes the second output: -2--1--2- In the second stage, \nit intersects these two machines, which results in a machine that encodes all possible ways that the \nconstraints of both input/output pairs can be simultaneously satisfied: -2--2- This particular machine \ngenerates only one string, the string \"a -2- -2-\", which corresponds to the single replacement expression \nthat can perform the transformation. The gap program that transforms the inputs to the outputs is then \n\"-/- x -2- y => a -2- -2-\". The finite state machines of the first phase can be built in time proportional \nto the lengths of the output strings multiplied by the number of gaps in the input pattern. If I is a \nbound on the length of the output strings, then a particular gap from the input can occur at no more \nthan I different points in the output. Thus the machines constructed in the first phase of the algorithm \nhave no more than l states and O([g(G)[/) transitions. The worst case running time of the second phase \ncan be proportional to the product of the sizes of the machines constructed in the first phase, and so \nwe have shown Theorem: A replacement expression that produces n output strings by mapping [g(G)l gap \nfragments taken from n input strings of length at most I can be constructed in time O(Ig(G)l'r). This \nproblem is NP-hard, which implies that we probably should not expect to improve on this worst case performance \nby very much; however, the algorithm seems to perform well in practice. In practice, the machines constructed \nin the first phase are long and skinny, because the string contained in a particular gap usually does \nnot occur in the output in very many places. The intersection of two of these skinny machines M/and M~ \ncan be implemented to run in time roughly proportional to the size of the resulting machine MfIMk, and \nthe intersection is a machine that is usually skinnier than either Mj or Mk. So this algorithm performs \nwell in practice, running in time closer to O(n/) than O([g(G)l'/n), and in fact is exactly the replacement \nexpression synthesis algorithm that is used by the Erie system. The following can be shown to be true, \nTheorem: The replacement synthesis algorithm converges to the target replacement expression once the \npattern synthesis heuristic has found the target pattern. The descriptive gap pattern synthesis heuristic \nand the replacement expression algorithm together make up a gap program synthesis heuristic that can \nidentify gap programs in the limit from positive data. Descriptive gap program synthesis performance \nThe gap program synthesis algorithm that we have sketched above can be shown to identify gap programs \nin the limit from positive data; however, it would be nice to have some assurance that it will do so \nwithin the limit of the user's patience. Unfortunately, this assurance is impossible to come by, because \nthe speed with which the system converges to a target gap program depends upon the quality of the user's \nsample data. The identification of the gap pattern is the part of the EBE process that is most susceptible \nto variations in the quality of sample data, since the replacement synthesis algorithm computes the gap \nreplacement exactly. In an attempt to gain a better understanding of how well the algorithm performs \non a small amount of data, say two or three examples, we studied the algorithm's performance on several \ndifferent sets of randomly generated test data. The trends found in the study may be simply summarized: \nfewer gaps, longer constants, and shorter gap substitutes make target gap patterns easier to identify; \nmore gaps, shorter constants, and longer gap substitutes make them harder. These studies were helpful \nin identifying a common aspect of sample data that slows the gap pattern synthesis procedure's convergence \nto the target pattern. As an example, these two input samples might be given by the user in an effort \nto make the system synthesize the target pattern \"bol -I- eo/\": the first example a second one The descriptive \ngap pattern for this sample set is \"bol -I- LI -2- t_J -3- eoT, and the system will not converge to the \ntarget program until the user supplies an example that does not contain a \"LI\". However, the system will \nstill be able to synthesize a useful gap program, albeit a noisy one, because the pattern fragment \"-I- \nLJ -2- I.J -3-\" matches exactly the same text in this particular sample set as the gap \"-!-\" does in \nthe target pattern. We say that patterns like \"bol -!- LI -2- I.J -3- eor are strongly compatible with \npatterns like \"bo/-!- eor' on a given set of samples. Patterns that are strongly compatible with the \ntarget are tolerable hypotheses, because the system will still be able to find a replacement expression \nthat can produce the outputs, and it will thus be able to synthesize a useful gap program. When the system \nmerely fails to find the target gap program, it usually finds one with a strongly compatible pattern: \nhowever, the system does occasionally fail in more serious ways. The most common of these is when it \nis unable to find any gap pattern at all to match the sample data: from experience, such failures usually \noccur because the user is asking the system to perform a task that cannot be handled by gap programs. \nAnother, less common, manner of failure is fbr the system to find a gap pattern that is not strongly \ncompatible with the target. Such patterns usually do not parse the input samples in a way that lets them \nbe transformed to the output samples; so while the system has been able to generate a gap pattern, it \nwill not be able to generate a replacement expression. Adding another input example will oRen allow the \nsystem to converge to the target gap pattern, or to one that is strongly compatible with the target. \n Descriptive gap program synthesis heuristics The ~E system uses four heuristics in addition to the basic \ngap program synthesis algorithm. The first two heuristics help to make strongly compatible gap patterns \nmore like the target, the third makes the synthesized gap patterns more \"reasonable\", and the last reduces \nthe number of output examples that are normally required to specify a replacement expression. 1) Tokenization. \nTokenization performs an a priori grouping on the characters of the samples so that the system will not, \nfor example, notice that \"Yankees\", \"Brewers\", and \"Dodgers\" all share an \"e\" and an \"s'. The system \nfirst analyzes tokenized samples\u00b0 and if no transformation can be found that respects the tokenization, \nit reanalyzes the samples using a character-at-a-time tokenization. 2) Pattern reduction. Pattern reduction \nis a heuristic for making descriptive gap patterns less descriptive; it examines a gap program for blocks \nof constants and gaps that are copied en masse into the replacement expression and coalesces such blocks \ninto a single gap, as long as the resulting program will still transform the examples. For example, the \ngap program \"x -I- y -2- z =~ a -I- y -2- b\" would be pattern reduced to \"'x -I- z ===, a -1- b\", so \nlong as the new gap program could still parse and correctly transform the samples. 3) Gap bounding. Gap \nbounding limits the number of end-of-line boundaries that a gap is allowed to cross while matching the \ntext of a file, in order to keep the program from running amok and matching 1,000 lines when it matched \nonly 2 in the examples. Ifa particular gap spans at most lend-of-line boundaries in any input sample, \nthen the system restricts it to spanning no more than LI.SIJ end-of-line boundaries when searching the \ntext. 4) Fewer output samples. The fourth heuristic reduces the number of output samples that are normally \nneeded by removing the requirement that an output sample be given for every input sample. The system \nanalyzes all of the input samples and produces a gap pattern. It uses this pattern to parse those inputs \nthat have a corresponding output, and generates the class of replacement expressions that can produce \nthose outputs from the input parses. It then chooses the shortest expression in this class, which is \nusually the one with the largest number of gap symbols. This is a very important heuristic: input examples \nare probably already present in the user's text, and are thus easy for the user to provide; on the other \nhand, output examples usually have to be created from scratch (or by editing the input examples), and \nare thus much more expensive for the user to provide. Descriptive gap program synthesis 'Paese four \nheuristics can be combined with the gap pattern and replacement synthesis procedures to form a practical \ngap program synthesis procedure. This procedure is invoked whenever the EBE system's current program \nhypothesis fails to perform the function specified by a given input or output sample. The algorithm can \nusually synthesize a target gap program from two or three input examples and one output example. Inputs: \n a set of input/output pairs S = {<iy,o 1>,<i2,o2>....,(in, On>}; a set of unpaired inputs I = {i n + \n1,in +2,...,in +m}; and a tokenization function T, Output: a gap program P or an indication of failure \n(not shown). Tokonize the samples in S and I using T; Approximate a descriptive gap pattern G common \nto {i~,i2,...oin,in+ 1,in +2,...,in +m}; Use G tO parse {i~,i2,...,G}; Synthesize the shortest replacement \nexpression R that maps ik to ok for k= 1,2,...,n; Perform pattern reduction on G and R; Bound the gaps \nof G;  return P=G =~ R; We demonstrate the algorithm with an example; many more examples may be found \nin the full paper [31]. In this example, the user would like to take a program filled with Lisp function \ndefinitions: (define (factorial n) (if (<= n 1) 1 (* n (factorial (- n 1)))))  (define (halts f) ,,.) \nand insert a \"comment template\" before each function: ;*** (factorial n) ;see ................................... \n ;'* * (perspicuous description here> .*ee (define (factorial n) (if (< = n 1) 1 (* n (factorial (- \nn 1))))) ;\u00b0*\u00b0 (halts f) ;*\u00b0* ................................... ;*\u00b0\" <perspicuous description here> \n (define (halts f) ,..) To specify this transformation to the EBE system, the user chooses to give \nthe two \"(define\" lines as input examples and the comment template for factorial as an output example. \n(The user could have chosen to give the full text of the function bodies as input examples; this would \nhave worked, but the synthesized programs would have been noisier, e.g. because the function bodies both \ncontain many right parenthesis.) \"l~e system tokenizes the two input examples: bo! ( define LJ ( factorial \nU n ) eol bol ( define LI ( halts L_l f ) eol finds the constants that they have in common: bol ( define \nLI ( LI ) sol and. then inserts gaps to form a descriptive gap pattern G: bol (defineu( -I- ii -2- ) \neol 192 It tokenizes the single output example in the same way, parses the inputs using the gap pattern, \nand builds a finite automaton that describes all possible replacement expressions that can produce the \nsingle output from the first input. It chooses the shortest such replacement expression R (preferring \nto use gaps in lieu of constants when there is a conflict): ;***LI( -I-U -2-) eol ;***U ................................... \neol ;***u<perspicuousudoscriptionuhere> eol ;0\"* eo[ (dofineU( -I- i l -2- ) eol The pattern reduction \nheuristic merges the fragment \"-I- U -2-\" into a single gap \"-/-\" in both G and R, and the gap bounding \nheuristic limits this gap to matching characters within a single line, yielding the gap program: bol \n(defineLJ( -I- ) eol ;'*'t J( -1- )eol ;***U ................................... eol ; * * * u(perspicuou~_JdescriptionLIhere> \neol '*** eol t (defineu( -i- )eol This gap program will serve to insert comment templates in the rest \nof the user's text.   Implementation The gap program synthesis algorithm that we have developed is \nembedded within the EBE subsystem of a screen editor called U [30]. U is a full function screen editor \nimplemented within the T programming system [33, 34]. a LiSP-like programming environment. The principal \nimplementation of U is on the Apollo Domain MC68000-based workstation, although implementations also \nexist for VAX/Onix and VAX/VMS. (Domain, MC68000. Unix, VAX, and VMS are trademarks of Apollo Computer, \nMotorola, Bell Laboratories, DEC, and DEC, respectively.) The user interface of the U editor is similar \nto that of Wood's Z [43], which in turn was inspired by the work of Irons [24]. The Uuser interacts with \nthe EBE subsystem using five U commands: ;' a command that initializes an EBE session, which creats an \nEBE session window or clears the existing one of any previously given samples; a command that specifies \nthat a selected piece of text is an input sample; I, a similar command that specifies that some text \nis an output sample, which will usually be paired with the last input sample supplied; t, a command that \nruns the current gap program hypothesis, either by single-stepping it or by applying its transformation \nto a selected context; and a command that allows the user to modify the state of the EBE subsystem: fixing \nor deleting examples, specifying gap programs by hand, etc. The Erie subsystem commands are implemented \nin the style of the rest of the editor commands. As one example, the standard editor selection mechanism \nis used to specify examples. Another example is that the user interface of the command for running the \ngap program closely resembles that of the editor's querying global-replace command. The EBE subsystem \ninvokes the gap program synthesis procedure every time the state of the example set changes, that is, \nevery time an input or output example is given. The synthesized program is immediately displayed in a \nwindow, so the user gets immediate feedback about whether she has given adequate samples. This arrangement \nis feasible because the gap program synthesis procedure is quite efficient: it usually takes a second \nor two of elapsed time to produce a program (in an untuned implementation). If the gap program synthesis \noperation were more costly, it would probably have been better to have the user invoke it explicitly \nwith a separate command. An implementation of an EBE system within the Cedar programming environment \n[39] is underway. Conclusion The general form of the ~E system sketched in this paper was determined \nby a sequence of design decisions; these decisions, in order of importance, were: to try to develop a \nuseful and practical system for automating repetitive text processing tasks; to automate the tasks through \na program synthesis system, rather than through a novel user interface to a program transcription system; \n), to take a formal approach to solving this problem, rather than, for instance, a knowledge-based approach; \nto concentrate on automating the solution to problems solvable by simple text scanning and replacement \nprograms; k to develop a system that would base its hypotheses on positive data, rather than taking advantage \nof negative data as well; t, to base the system's analyses on the input/output behavior of the target \nfunction, rather than on traces or other sources of information; to require more than one example of \nthe target function's behavior in order to form interesting generalizations, rather than trying to intuit \ninteresting generalizations from a single example; I, to use formal language style patterns in the text \nscanning programs, rather than using control-structure oriented pattern matching as in SNOBOL; k to use \ngap patterns to describe the structure of the text to transform; to use gap replacement expressions to \ndescribe how to perform the transformation; and to use a heuristic gap program synthesis procedure, rather \nthan one that always guarantees m find a gap program concomitant with the demonstrated behavior. An \nenumeration of extensions to the EeE system that each take a slightly different tack may be found in \nthe full paper [31]. The primary contribution of this work lies in demonstrating the feasibility of program \nsynthesis in the domain of text editing. We developed, analyzed, and implemented an editing by example \nsystem and embedded it in a production text editor. The system seems to be an effective aid in automating \nthe solution of a useful class of text processing problems. Most of the credit for this success should \ngo to the choice of the domain. Text editing is an interactive activity that is oriented around the incremental \nand (usually) unstructured manipulation of a.large collection of data. Small-scale text processing problems \nconstantly crop up during the course of these manipulations, and many of these problems can be solved \nby simple, syntactically oriented text processing programs. A few examples suffice to specify a good \nfraction of these programs, and the text editing environment makes these examples easy to produce and \nprovide. The text editing domain is ideal for programming by example research; as a result, we have been \nable to develop one of the few instances of a programming by example system that performs interesting \ngeneralizations of its examples. 193  Another contribution of this work is in providing an application \nfor the techniques of inductive inference, an area of research that has seen a great, deal of theoretical \ndevelopment but heretofore has had very few applications. This application, and the others that hopefully \nwill follow, may help to focus inductive inference research on addressing problems of practical importance. \nThis work indicates a direction for program synthesis research: to find and develop applications for \nthe programs that lie within the range of the program synthesis techniques that have been developed. \nIf such research proves fruitful, it may spur further development in this area. which may help the field \nto evolve towards the eventual goal of automating the programming process. The greatest weakness of this \nwork is that the EBE system has not been used by a large community, because the U editor did not become \ngenerally usable until this project was nearly complete (it is quite usable now). The design and evaluation \nof the system is based on the author's personal experience with building, using, and supporting text \nprocessing tools; while this experience is not inconsequential, it still represents only one man's view. \nIt would have been better to have had more feedback on the system, both from knowledgeable programmers \nand from naive word-processors. Another weakness is that gap programs are not powerful enough to express \nthe solution of many text processing problems. The full paper considers some extensions to gap programs \n[31], but while these extensions increase the capabilities of the system, it is clear that much more \nsophisticated programs cannot be derived from a few input/output examples. Different approaches must \nbe taken. The future of this research lies in studying other ways in which the power of programming can \nbe brought smoothly out into the user interface. Advertisement The EBE system we have described can \nbe implemented within a production screen editor with only a modest amount of effort, and should perform \nwell on the current breed of microprocessors. I urge those interested in implementing such a system to \nconsult the full paper [31]. and I would very much enjoy hearing about their efforts, either from them \nor from their users. Acknowledgements This work benefited from many discussions with my thesis advisor, \nDana Angluin. I would also like to thank the other two members of my reading conunittee. Michael Fischer \nand Alan Periis. The U editor was written by the author, but Nat Mishkin is largely responsible for the \ncurrent status of its implementation on the Apollo. [ gratefully acknowledge the editorial advice of \nJudy Martel and Mary-Claire van Leunen. References [1] A.V. Aho, B.W. Kernighan, and P.J. Weinberger. \nAWK -A pattern matching and scanning language. ,Software-Practice &#38; Experience, 9(4)'267-280, April, \n1979. [2] D. Angluin. On the complexity of minimum inference of regular sets. Information and Control, \n39:337-350, 1978. [3] D. Angluin. Finding patterns common to a set of strings. JCSS, 21:46-62, 1980. \n[4] D. Angluin and C. Smith. A survey of inductive inference: theory and methods. Research report No. \n250, Yale University Department of Computer Science, October. 1982. [5] D. Angluin. A note on the number \nof queries needed to identify regular languages. Information and Control. 51(l):76-87, October. 1982. \n[6] A.W. Biermann and J.A. Feldman. On the synthesis of finite-state machines from samples of their behavior. \nIEEE Transactions on Computing, C-21:592-597, 1972. [7] A.W. Biermann and J.A. Feldman. A survey of results \nin grammatical inference. In Frontiers of Pattern Recognition, Academic Press, N.Y, 1972. [8] C.M. Cook, \nA. Rosenfeld and A.R. Aronson. Grammatical inference by hill-climbing. ISJ. 10:59-80, 1976. [9] S. Crespi-Reghizzi, \nM.A. Melkanoff and L. Lichten. The use of grammatical inference for designing programming languages. \nCACM, 16:83-90, 1973. [10] S. Crespi-Reghizzi, G. Guida and D. Mandrioli. Noncounting context-free languages. \nJACM, 25:571-580, 1978. [11] Gael Curry. Programming by Abstract Demonstration. University of Washington \nPh.D. dissertation, 1977. Appeared as Computer Science Department Tech. Rep. 77-08-02. [12] John R. Ellis, \nNathaniel Mishkin, Robert P. Nix, and Steven R. Wood. A BLISS programming environment. Research report \nNo. 231, Yale University Department of Computer Science, June. 1982. [131 John R. Ellis, Nathaniel Mishkin, \nMary-Claire van Leunen. and Steven R. Wood. Tools: An environment for timeshared computing and programming. \nResearch report No. 232. Yale University Department of Computer Science. To appear in Software -- Practice \n&#38; Experience. [14] J.A. Feldman. First thoughts on grammatical inference. Research report No. 55. \nStanford University Artificial Intelligence Laboratory, 1967. [15] J.A. Feldman, J. Gips. J.J. Homing, \nS. Reder. Grammatical complexity and inference. Stanford University Computer Science Department research \nreport. 1969. [16] K.S. Fu and T.L. Booth. Grammatical inference: introduction and survey, parts I and \n2. IEEE Transactions on System&#38; Ma~ and Cybernetics, SMC-5:95-11.1 and 409-423, 1975. [17] E.M. Gold. \nLanguage identification in the limit. Information and Control, 10:447-474, 1967. [18] E.M. Gold. Complexity \nof automaton identification from given data. Information and Control, 37:302-320, 1978. [19] D.C. Halbert. \nAn Example of Programming by Example. Masters thesis, Department of Electrical Engineering and Computer \nSciences, University of California at Berkeley, 1981. Also an internal report of Xerox Office Products \nDivision, Palo Alto CA, 1981. [201 D. Hirschberg. A linear space algorithm for computing maximal common \nsubsequences. CACM, 18(6):341-343, June, 1975. [21] J.E. HopcroR and J.D. UIlman. Introduction to Automata \nTheory, Languages. and Computation. Addison-Wesley, Reading, M as_~H_'husetts. 1979. [22] JJ. Homing. \nA Study of Grammatical Inference. Ph.D. thesis. Stanford University Computer Science Department. 1969. \n[23] James W. Hunt and Thomas G. Szymanski. A fast algorithm for computing longest common subsequences. \nCACM, 20(5):350-353, May, 1977. [24] E.T. Irons and F. M. Djorup. A CRT editing system. CACM, 15(l):16-20. \nJanuary. 1972. [251 B. Knobe and K. Knobe. A method for inferring context-flee grammars. Information \nand Control, 31:129-146, 1976. [26] H. l.ieberman and C. Hewitt. A session with Tinker: interleaving \nprogram testing with program design. Conference Record of the 1980 I.ISP Conference, Stanford University, \n1980, pp. 90-99. [27] D. Maier. \"lhe complexity of some problems on subsequences and supersequences. \nJACM. 25:322-336. 1978. [28] N. Meyrowitz and A. van Dam. Interactive editing systems: Part ll. Computing \nSurveys, 14(3):353-415. 1982. [29] James H. Morris, Eric Schmidt` and Philip Walder. Experience with \nan applicative string processing language. Proceedings of the 7th ACM Conference on Principles of Programming \nLanguages, January. 1980. pp. 32-46. [30l R.P. Nix and N.W. Mishkin. U Editor User's and Programmer's \nManual. Internal memorandum of the Yale University Department of Computer Science. 1983. [31] R.P. Nix. \nEditing by Example. PhD thesis, Yale University Department of Computer Science, 1983. Also appeared as \nYale University Department of Computer Science Research Report No. 280. [32] T.W. Pao and J.W. Cart IlL \nA solution of the syntactical induction-inference problem for regular languages. Computer Languages, \n3:53-64, 1978. [33] Jonathan A. Rees and Norman I. Adams IV. T: a dialect of l.isp or, lambda: the ultimate \nsoftware tool. Proceedings of the 1982 ACM Symposium on Lisp and Functional Programming, AugusL 1982. \n [34] Jonathan A. Rees and Norman l. Adams IV. T User's Manual. Internal memorandum of the Yale University \nDepartment of Computer Science, 1982. [35] T. Shinohara. Polynomial time inference of pattern languages \nand its application. In Proceedings of the 7th IBM Symposium on Mathematical Foundatwns of Computer Science, \n1982. [36] T. Shinohara. Polynomial time inference of extended regular pattern languages,. In Proceedings \nof Software Science and Engineering of Computer Science, Kyoto. Japan, 1982. 137l D.C. Smith. Pygmalion: \nA Computer Program to Model and Stimulatc Creative Thought. Ph.D. thesis, Stanford University Computer \nScience Department, 1975. Appeared as AIM-260, 1975, and as a book fTom Birkhauser Verlag\" 1977. [38l \nR.M. Staliman. EMACS. the extensible, customizable, self-documenting display editor. In Proceedings of \nthe ACM SIGPLAN/SIGOA Symposium on Text Manipulation, Portland. Oregon, June, 1981, pp. 147-160. Appeared \nas SIGPLAN Notices, Volume 16. Number 6. June 1981. [39] Warren Teitelman. The Cedar programming environment: \nA midterm report and examination. Research report No. CSL-83-11. Computer Science l.aboratory. Xerox \nPalo Alto Research Center. December. 1983 (in press). [40] B.A. Trakhtenbrot and Y.M. Barzdin. Finite \nAutomata. North-Holland. Amsterdam. 1973. pp. 98-99. [411 R.A. Wagner and M. J. Fischer. The string-to-string \ncorrection problem. JACM, 21:168-173. 1974. [421 R.M. Wharton. Grammar enumeration and inference. lnformalion \nand Control. 33:253-272, 1977. [43] S.R. Wood. Z: The 95% program editor. In Proceedings of the ACM SIGPLAN/SIGOA \nSymposium on Text Manipulation. Portland. Oregon, June, 1981, pp. l-7. Appeared as SIGPLAN Notices, Volume \n16, Number 6, June 1981. [44] M.M. Zloof, Query-by-example: a data base language. IBM Systems Journal, \n16(4):324-343. 1977. [451 M.M. Zloof. Office-by-example: a business language that unifies data and word \nprocessing and electronic mail. IBM Systems Journal, 21(3):272-304, 1982.  \n\t\t\t", "proc_id": "800017", "abstract": "<p>An editing by example system is an automatic program synthesis facility embedded in a text editor that can be used to solve repetitive text editing problems. The user provides the editor with a few examples of a text transformation. The system analyzes the examples and generalizes them into a program that can perform the transformation to the rest of the user's text.</p> <p>This paper presents the design, analysis, and implementation of a practical editing by example system. In particular, we study the problem of synthesizing a text processing program that generalizes the transformation implicitly described by a small number of input/output examples. We define a class of text processing programs called <italic>gap programs</italic>, characterize their computational power, study the problems associated with synthesizing them from examples, and derive an efficient heuristic that provably synthesizes a gap program from examples of its input/output behavior.</p> <p>We evaluate how well the gap program synthesis heuristic performs on the text encountered in practice. This evaluation inspires the development of several modifications to the gap program synthesis heuristic that act both to improve the quality of the hypotheses proposed by the system and to reduce the number of examples required to converge to a target program. The result is a gap program synthesis heuristic that can usually synthesize a target gap program from two or three input examples and a single output example.</p> <p>The editing by example system derived from this analysis has been embedded in a production text editor. The system is presented as a group of editor commands that use the standard interfaces of the editor to collect examples, show synthesized programs, and run them. By developing an editing by example system that solves a useful class of text processing problems, we demonstrate that program synthesis is feasible in the domain of text editing.</p>", "authors": [{"name": "Robert Nix", "author_profile_id": "81100460870", "affiliation": "Yale University", "person_id": "P245976", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/800017.800530", "year": "1984", "article_id": "800530", "conference": "POPL", "title": "Editing by example", "url": "http://dl.acm.org/citation.cfm?id=800530"}