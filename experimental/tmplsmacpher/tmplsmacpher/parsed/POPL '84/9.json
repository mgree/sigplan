{"article_publication_date": "01-15-1984", "fulltext": "\n SYSTEMS PROGRAMMING IN CONCURRENT PROLOG Ehud Shapiro Department of Applied Mathematics The Weizmann \nInstitute of Science Rehovot 76100, Israel Abstract Concurrent Prolog [28] combines the logic pro- gramming \ncomputation model with guarded-command in- determinacy and dataflow synchronization. It will form the \nbasis of the Kernel Language [21] of the Parallel Inference Machine [36], planned by Jap~.m's Fifth Generation \nCom- puters Project. This paper explores the feasibility of pro- gramming such a machine solely in Concurrent \nProlog (in the absence of a lower-level programming language), by im- plementing in it a representative \ncollection of systems pro- gramming problems. 1. Introduction The process of turning a bare yon Neumann \nmachine into a usable computer is well understood. One of the more elegant techniques to do so is to \nimplement a cross-compiler for a systems programming language (say C) on a usable computer. Then implement \nin that language ~n operating system kernel (say Unix), device drivers, a file system, and a programming \nenvironment. Then boot the operating system on the target computer. From that stage on the computer is \nusable, and application programs, com- pilers and interpreters for higher-level languages (say Franz \nLisp and CProlog) can be developed on it. This paper addresses the question of turning a bare computer \ninto a usable one, but for a machine of a different type, namely, a parallel logic programming m~chine. \nIn particular, it explores the suitability of Con- current Prolog [28] as the kernel programming language \n1 of such a computer, by asking the question: permissionto copy without fee all or part of this material \nis granted provided that the copies are not made or distributed for direct commercial advantage, the \nACM copyright notice and the title of the publication and its date appear, and notice is given that copying \nis by permission of the Association for Computing Machinery. To copy otherwise, or to republish, requires \na fee and/or specific permission. &#38;#169; 1983 ACM 0-89791-125-3/84/001/0093 $00.75 1. Will a machine \nthat implements Concurrent Prolog in hardware or firmware be usable as a general-purpose, multi-user, \ninteractive computer? or, stated slightly differently, 2. Is Concurrent Prolog exprvssive enough to be \na kernel language of a general-purpose computer? We investigate these questions only from the software, \nnot from the hardware, side. Our concern with efficiency is limited by the assumption that the machine \nwill execute at least as many Megalips 2 as today's computers execute Mips. These questions are far reaching, \nbut not purely speculative, given the Fifth Generation Computers Project's plans to design and build \na parallel logic programming machine, and to use Concurrent Prolog as the basis for the machine's kernel \nlanguage [21]. The paper attempts to give some evidence towards affirmative answers to these questions. \nTo do so, we assume a computer that behaves like a virtual Concurrent Prolog machine, but has no other \n(lower-level or otherwise) programming constructs, specIM instructions, hardware in- terrupts, etc. We \nalso assume that basic drivers for I/O devices are provided, which make each device understand Concurrent \nProlog streams. This assumption is elaborated further below. We then develop a collection of Concurrent \nProlog programs that can run on such a machine, including: ~\" A Concurrent Prolog interpreter/debugger. \nA top-level crash/reboot loop, that reboots the operating system automatically upon a software (and perhaps \nalso a hardware) crash. IThe term Kernel Language denotes a hybrid between a machine language and a systems \nprogramming language, implemented by hardware or firmware. As far ~s I know it was introduced by the \nFifth Generation Project !23]. 2LIPS: Logical Inferences Per Second. In the context of Concur- rent Prolog, \nit means process reductions per second. A Unix-like shell, that hmldles foreground and back-ground processes, \npipelining, and an abort (\"ControI-C') interrupt for foreground processes. ~\" A multiple-process manager \n= ia MUF [ol, that nlanages the creation of, and eommm0cation with, multiple inter- active processes. \n Programs for merging streams, using various scheduling strategies.  A solution to the readers-and-writers \nproblem, based on the concept of monitors [16],  Shared queues, and their ~pplication to implementing \nmanagers of shared resources, such as a disk scheduler.  The programs have been developed and tested \nusing a Concurrent Prolog interpreter, written in Prolog [28]. They show Cohen;rent Prolog's ability \nto exprcss process creation, termination, communication, synchroniza- tion, and indeterminacy. They suggest \nthat a \"pure\" Con- current Prolog machine is self-qontained, and that it will be be usable =as is', without \ntoo many extraneous features. Even if such a machine is usable, it is not neces- sarily useful. For Concurrent \nProlog to be a general purpose programming language, it has t,o solve conveniently a broad range of \"real-life \n~ problems. However, determining what is a real-life problem depends upon one's point of view. One may \nsuggest implementing the algorithms in Aho, tlopcroft and Ullman's book as such a problem. Concurrent \nProlog will exhibit a grand failure if sqch an implementation is at- tempted, and for a reason. The algorithms \nin that book (and, most other sequential algorithms) are deeply rooted in the yon Neumann computer. One \nof the basic operations they use is destructive assignment of values to variabh;s (including destructive \npointer manipulation). This opera- tion is cheap on a yon Neumar~n machine, but is not avail- able directly \nin the logic programming computation aiodcl, is prohibitively expensive to simulate, and thinking in \nterms of it results in an awkward programming style. On the other hand, the operations that are cheap \nin Concurrent Proiog--proccss crcation and communication--are not used m conventional sequential al- \ngorithms, almost by definition. Hence Concurrent Prolog (or any other logic programming language) is \nnot adequate for implementing most yon Neumann algorithms. One may ask : Is there anything else to implement \n(besides payroll programs)? Or: What is Concurrent Prolog good for, then? Our experience to date suggests \nthat, in contrast to yon Neumann languages and algorithms, Concurrent Prolog exhibits strong affiliation \nwith four other \"trends\" in computer science: Object-oriented programming, dataflow and graph-reduction \nlanguages, distributed algorithms, and systolic algorithms. In [30] A. Takeuchi and the author show that \nConcurrent Prolog lends itself very naturally to the pro- gramming style and idioms of object-oriented \nprogramming languages such as Smalltalk [18] and Actors [14] . Many applications are easier to implement \nin this framework. The synchronization mechanism of Concurrent Prol,q~.-read-only variables--m a natural \ngeneralization of d~ttall'~w synchronization from functional to relational lang~ages. The basic operation \nof a Concurrent Prolog l~rogram--process reduction--is basically a graph reduction operation, since a \nprocess is a DAG, and a clause can he viewed as specifying how to rely|ace one DAG by a (possibly empty) \ncollection of other DAGS. It is interesting to ob- serve that the synthesis of dataflow and graph reduction \nmechanisms has been attempted by hardware rcsearchers, independently of logic programming [19] . We have \nsome experience with implementing dis- tributed algorithms in Concurrent Prolog. Thc implemcn- ration \nof the \"Lord of the Ring\" algorithm [24] in Concur- rent Prolog, described ill [26], exhibits a striking \nsimilarity to tile English description of the algorithm, where every rule of process behavior corresponds \nto one Concurrent Prolog clause. That paper also reports on an implementation of a complicated distributed \nminimum spanning tree algorithm. An implementation of Shiloach and Vishkin's MAXFLOW algorithm [31] den~,onstrates \nthe ability of Con- current Prolog to implement complex parallel algorithms without loss of efficiency \n[1~] . Numerical computations are quite remote from Artilicial Intelligence, the original ecological \nniche of logic programming. Nevertheless, we find that the natural Con- current Prolog solutions to l)umerical \nproblems have a \"systolic touch\" to them, and vice-versa: that implement- ing systolic algorithms [22] \nin Concurrent Prolog is easy. A forthcoming paper will include Concurrent Prolog im- plementations of \nseveral systolic algorithms, including the hexagonal band-matrix multiplication algorithm [22]. Other \nrecent applications of Concurrent Prolog include tile implementation of a parallel parsing algorithm \n[12], an or-parallel Prolog interpreter [13], a hardware specification and delmgging sy:,tem [33], and \na l,OOP.q-like [2] nbject-oriented knowledge representationlanguage [9].  2. Concurrent Prolog Concurrent \nProlog m a logic programming lan- guage, in that a program is a collection of universally qnantificd \nIIorn-clause axioms, and a computation is an at- tempt to prove a guM--an existentially quantilicd conjunc- \ntivc statement---from the axioms in tile program. The goal statement uescribes an input/output relation \nfor which the input is known; a successful (constructive) proof provides a corresponding output. The \ndifference between Concurrent Prolog and  other logic programming langttages (e.g. pure Prolog) is in \nthe mechanism they provide for controlling the construction of the proof. Prolog uses the order of clauses \nin the program and the order of goals in a clause to guide a sequential search for a proof, and uses \nthe cut operator to prune undesired portions or the search space. Concurrent Prolog searches for a proof \nin parallel. To col:trol the search, Concurrent 94 Pr~lvg embodies two familiar concepts: guarded-command \nmd,.terminacy, attd datallow ~ynchronization. They are implemented using two constructs: the commit operator \n\"[\" and tile read-only annotation ~'. The commit operator is similar to l)ijkstra's guarded command [7], \nand was first introduced to logic pro- gramming by Clark and Gregory [4]. It allows a process to make \npreliminary computatiors (specilled in tile guard of a clause), before choosing which action to take, \ni.e. which clause to use for reduction. Read-only annotations on oc- currences of variables are the basic \n(and only) mechanism for process synchronization. Roughly speaking, a process that attempts to instantiate \na variable through a re'd-only occurrcnce of it suspends until the variable is instan- tiatcd by another \nprocess. The other componcnts of con- current programming: process ercation, termination, and communication, \nare already a~ailablc in the abstract com- putation model of logic programming. A unit goal cor-rcsp(,uds \nto a process, and a conjunctive goal to a system oJ processes. Aproccss is created via goal reduction, \nand ter- minated by being reduced to tile eml)ty (true) goal. Con-juncLivc goals may share variables, \nwhich are used as com- munication ~hannels between processes. More precisely, a Concurrent Prolog program \nis a lil:itc set of guarded-clauses. A guarded-clause is a univer- sally quantitled axiom of the form \nA \" GhG~,...,Gr~ I B1,B2,...,B,,. m,n >_ O. where tile G's and the B's are atomic formulas, also called \nunit goals. A is called tile clause's head, the G's are called its guard, and tile B's its body. When \nthe guard is empty Lhe commit operator \"1\" is on~itted. Clauses may contain variables marked read-only, \nsuch as \"X?'. The Edinburgh i'rolog syntactic conventions are followed: constants begin with a lower-ease \nletter, and variables with an upper-ease letter. The special binary term [X I Y] is used to denote the \nlist whose head (ear) is X and ~,ail (cdr) is Y. The constant [] denotes the empty list. Concerning the \ndqelarative semantics of a guarded clause, the commit operator reads like a conjunc- Lion: A is implied \nby the G's lind the B's. The read-only annotations can be ignored in the declarative reading. Procedurally, \na guarded-clause specilles a be-havior similar to an alternative in a guarded-command. 'lb reduce a process \nA using a clause AI *-- (.tl B, unil'y A with A1, and, if successful, recursively reduce (_/to i,he empty \nsystem, and, if successful, commit to that clause, at,i, if\" successful, reduce A to /3. The unification \nof ~, process against the head of a clause serves several functions: passing parameters, assigning values \nto variables, selecting and constructing data-structures, and sending and receiving messages. The example \nprograms below demonstrate all these uses of un itlcation. The reduction of a process may suspend or \nfail durilq,; almost any of the steps described above. The unilic:~tion of the process against the head \nof a clause :;uspcnds if it requires the inst:mtiation of variables occur- ring as read-only in A. It \nfails ifA and A1 are not unifiable. The computation of thc guard system G suspends if any of the processes \nin it suspends, and fails if any of them fails. As in guarded-commands, at most one of tile process's \nor- parallel guard-systems may commit. Prior to commitment, partial results computed by the lirst two \nsteps of the reduction---unifying the process against the head of the clau.~e and solving the guard--are \nnot accessible to other processes in A's system. This prewmts interference between L~rother or-parallel \ncomputa- tions~ and eliminates the need for distributed backtracking. This completes the informal description \nof Con- current l'rolog. The simplicity of the language is an asset whclt attempting a hardwarc or firmware \nimplementation of it. 3. A Meta-Interpreter for Concurrent Prolog One of the simpler ways to implement \na program- ruing environment for a programming language L is aug- taunting l,'s interpreter. Among the \nprogram development tools that can be implemented in this way are sophisti-cated debuggers [27] , runtime,statistics \npackages, and ex- tensions to the language, and new embedded languages. Tile difliculty of implementing \nthese tools grows with the complexity of that interpreter. l,br reasons of boot.~Lrapping and elegance, \nthe preferred implementation language for L's programming en- vironment is L itself, as argued eloquently \nby Sandewall[25]. ltence the ease in ~hich an L interpreter can be implemented if L is of clear practical \nimportance, as \u00a2eii as a useful criteria for evaluating the ezpreesiveness and eompletenets of the language, \nas argued by Sussman and Ste\u00a2lc in [32] . l)esigning an expres.~ave language with a simple recta-interpreter \n3 is like solving a fixpoint equation. If the language L is too weak, then L's data-structures may not \nbe rich enough to represent L programs conveniently. If the control constructs of L are incomplete they \ncannot be used to simulate themselves conveniently. On ttle other hand, if tile control structures of \nL are awkward and unrestricted and the data-structures are too baroque, then its interpreter becomes \nvery large and unintelligible (e.g. goto cannot be used in a simple way to simulate u urestricted goto, \nbut the easiest way to simulate a tuhile statement is using a ~h/ie statement in the interpreter). A \nrecta interpreter for pure sequential Prolog can be written in three Prolog claupes, and, indeed, implement- \ning software tools and embedded languages via extending this interpreter is a common activity for Prolog \nprogram- 3CMIed ~ meta-circular interpreter in [32 I. nlel'~ A meta interpreter for Concurrent Prolog \nis desclibed below. It assumes th~ existence of a built-in sys- tc.r:j predicate clauseB(A, Cs), th:,t \nreturns in Cs the list of all clat~aes in the interpreted progr ~m whose head is potentially ul~lli~ble \nwith A. 4 The constant true signifies an empty .guard or an empty body. rcduec(true). reduce((A,B)) *- \nreduce(A?), reduce(B?). reduce(A) +-- clauses(A,C, lauses) [ resolve(A,Ciauses,Body), reduce(Boay?). \nresolve(A,[(A*--Guar(~lBody)lCs],Body) --- reduce(Guard) I true. resolve(A,[CIClauses],Body)+-resolve(A,Clauses,Body) \nI true. Program ls A Meta-interpreter for Concurrent Prolog Like any other Concurrent Prolog program, \nPro- gram I can be read both deelaratively, i.e. as a set of axioms, and operationally, i.e. as a set \nof rules defining the behavior of processes. Declaratively, reduce(A) states that A is true (provable) \nwith respect to the ~xioms defined in the predi- cate clauses. Operationally, the process reduce(A) attempts \nto reduce the system of processes A to the empty (halting) system true. Declaratively, the axtoms of \nreduce read: true is true. The conjunction A,B is true if A is true and B is true. The goal A is true \nif there are clauses Cs with the same head predicate of A, resolving A wil.h Cs gives B, and B is true. \nThe predicate resolvc(A,[Cl Ce].B) reads, declaratively, that resolving A with the axioms [C]Cs] gives \nB if the clause C has head A, guard G and body B, and the guard G is true, or if recursively resolving \nA with Cs gives B. Operationally, the clauses of reduce say that the process true halts. The process \nA, B reduces itself to the processes A and B, and that the process A, with clauses Cs, reduces itself \nto B if the result of resolving A with Cs is B. The reader not familiar with logic-programming may be \npuzzled by this interpr~ter. It seems to capture the control part of the computatiobl, but does not seem \nto deal at :~!1 with unification, the data component. The answer to the puzzle is that the call ~o the \nfirst clause of resolve is doing the work, by unifying.7 the process with the head of the clause. Their \nunification is achieved by calling them with the same name, A. This interpreter assumes one global program, \nwhose axioms are accessible via the system predicate clan#el, 4in our current implementation Cs is the \nlist of all clauses with the same head predicate as A. Eetter indexing mechanism can male the predicate \nmore selectiTe. Another possible optimiza- tion is to use the bouuded-but~er technique of Takeuchi and \nl\"urLkawa [34], to generate clauses on a demand-driven basis. as in conventional Prolog implementations. \nIn a real im- plementation of Concurrent Prolog, programs would be ob- jects that can be passed as arguments, \nand reduce and dau#es would have an additional argxtment, the program being simulated. This interpreter \ncannot execute Concurrent Prolog programs that use built-in system predicates, such as itself (it uses \nthe predicate clauses). The current implemen- tation of Concurrent Prolog contains several (13) system \npredicates: metalogical predicates (clau#es and system), con- trol predicates (othertpiBe and ==), interface \nto the underly- ing prolog, I/O (read and ~rite), and arithmetic predicates (the lazy evaluator :----~ \nand 5 arithmetic test predicates). To handle system predicates, the interpreter can be augmented with \nthe clause reduce(A) *-- system(A) I A.  su#tem(X) is a system predicate that succeeds if X is a Concurrent \nProlog system predicate, and fails other- wise. For example, the call sustera(system(X)) succeeds. The \nclause demonstrates the use of the recta, variable, a facility also available in Prolog, which allows \nto pass processes to other processes as data-structures. It is used extensively in the shell programs \nbelow. One may suggest that using the metavariable facility, a Concurrent Prolog meta-interpreter can \nbe imple- mented via the clause reduce(A) 4-- A. This claim is true, except that it will be rather difficult \nto implement the software tools mentioned earlier as an extensions to this interpreter, whereas implementing \na Concurrent Prolog single stepper by extending Program 1 is a trivial matter. The interpreter in Program \n1 is 10 to 20 times slower than the underlying Concurrent Prolog implementa- tion. We feel that it is \nreasonable to pay a 10-fold slow- down during the program development phase for a good programming environment. \nBesides, a default to the un-derlying Concurrent Prolog can be incorporated easily, as in the case of \nsystem predicates, so that in developing large systems only the portion of the code that is under develop- \nment needs the extra layer of simulation. 4. Streams Concurrent Prolog processes communicate via shared \nlogical-variables. Logical variables are single-assignment: they can be either uninstantiated or instan-tiated, \nbut, once instantiated, their value cannot be destruc- tively modified. Hence the Conc/lrrent Prolog \ncomputation model is indifferent to the distinction between the shared- memory computation nmdei m,d \nthe communication based model. A shared logical-variable can be viewed as a shared memory cell that can \naccept only one value, or as a com- munication channel that can transmit only one message. The distinction \nbetween the \"reader\" and \"wrEer\" of a shared variable (or the \"sender\" and \"receiver\" of the message) \nis done via read-only annotations. A process p(...Xf...) cannot instantiate X. Attempts of p to reducc \nitself to other processes using clauses that require the in- stantiation of X, such as p(...f(a)...} \n.... suspend, until X is instantiated by some other process. If X is instantiated to f(Y), then the \nprocess p can unify with that clause, even though it instantiated Y to a, since the scope of a re~,~d-only \nannotation is only the main functor of a term, but not variables that occur inside the term. This property \nenables a powerful programming technique that uses incomplde me##a#e# [28] . Even though logical variables \nare single-assignment, two processes can communicate with each other via a single shared variable, by \ninstantiating a variable into a term that contains both the message and another variable, to be used \nin subsequent communications. This program- ruing techniques gives the effect of streams. The cleanest \nway to implement I/O functions in a Concurrent Prolog machine is for I/O devices to generate and/or consume \nConcurrent Prolog streams. The current implementation of Concurrent Prolog, which is an inter-preter \nwritten in Prolog [28] , supports only terminal I/O (the rest is done by the underlying Prolog). It implements \nthe stream abstraction for the user terminal via two predi- cates, imtrsam(X), which generated the stream \nX of terms typed in by the user, and out#tream(X), that outputs to the screen the stream X. They are \nimplemented using the un- derlying Prolog read and terite predicates. instream(IXlXs]) *-- read(X) I \ninstream(Xs).  outstream([]). outstream(lX[Xs]) write(X), outstream(Xs?). Program 2t Implementing terminal \n1/O streams using reed and ~rite If we want in#treats to allow the user to signify the end of the stream, \nthe program has to be complicated a little. Using these programs, a \"device-driver\" that implements a \nstream interface to the terminal can be specified: terrain al(Keyboard,S,:reen) ~- instream(KeyBoard), \noutstream(Screen) In a virtual Concurr( nt l'rolog machine in which interfaces to I/0 device drivers \nare implemented as streams, there will be no need for spccmlized l/O primitives. One p<):~:~ii, le exception \nis a screer,-output primitive (write or bdbit), which may be needed for convenience and efficiency. 5. \nBooting an Operating System Assume that device orivers for a terminal (screen, keyboard and a mouse), \ndisk, and a local network have been defined for a personal workstation. Then the following program can \nbe uscd to boot il,s operating system: hooL +- monitor(KeyBoard?, MouseY, Screen, DiskIn?, Dis- kOut, \nNetln?, Netout), ter minal(KeyBoard,Mousc,Screen?), disk(DiskIn,DiskOu t?), nct(Netln,NetOut?) ] true. \nboot +- otherwise [ boot. Program 3: Booting ~n operating system The first clause invokes the device \ndrivers and the monitor. The second clause automatically reboots the sys- tem upon a software crash of \neither the monitor or the device drivers, othertaise is a Concurrent Prolog system predicate that succeeds \nil' and when all of its brother or-para!h:l guards fail. l)eclarativ,:iy, it may read as the nega- tion \nof the disjunction of the guards of the brother clauses 5. 6. A Unix-like Shell A shell is a process \nthat receives a stream of com- mands l'rom tile terminal and executes them. In our context the commands \narc processcs, and executing them means in- yoking them. A simple shell can be implemented using the \nmctavariable facility, shell(lXlXs]) .-- X, shell(Xs?). shell(ll).  This shell is batch-oriented. It \nbehaves like a Unix-shell that executes all commands in \"background\" mode, in the sense that it does \nnot wait for the completion of tile previous process before accepting the next command. 5'rhe predicate \nothertoise is not implemented correctly in the cur- rent Concurrent Prolog interpreter [28]. It may succeed \nwhen it has '~uspended brother or-paralle! guards, instead of suspending, and succeeding only when MI \nsuch guards fail. Hence programs using it are not fully debugged. As is, it achieves the cffect of Unix-like \npipes, using conjunc- an abort command in the input stream (most probably be- tive goals with shared \nvariables as commands. For example, the Unix command p]q,r can be simulated with the conjunctive system \np(X), q(X?,Y), r(Y?). provided that the Unix command p does not read from its primary input and q does \nnot write to its primary output. External I/O by user programs is handled below. Note that since the \nprocess's I/O streams have explicit names, we are not c<mfined to linear pipelining, and any desired \nI/O configuration of the processes can be specified. One of this shell's drawbacks is that it will crash \nifthe user process X crashes, since X and shell(Xs) are part of the same conjunctive system, which fails \nif one of its members fails. This can be remedied by calling enwlopcfX) instead of X. envelope(X) ,-- \nX I write(halted(X)). envelope(X) .-- otherwise I write(failed{X)). It is easy to angmen~ the shell to \ndistinguish be- tween background and foreground processes, assuming that every command X is tagged be{X) \nor .fg(X), as done in Pro- gram 4. (I)shell([]). (2) shell(Ifg(X)lXsl) .--envelope(X) I shell(Xs?). \n(3)shell([bg(X)lXs]) --envelope(X), sh~ll(Xs?). Program 4: A shell that handles foreground and back- \nground processes Note that foregrouncl processes are executed in the shell's guard. This allows a simple \nextension to shell so it will handle an abort (\"control-C\" on decent computers) interrupt for foreground \nprocesses. Upon the reception of an abort command the currently running foreground process (if there \nis one) is aborted, and the content of the input stream past the abort command is flushed. This is achieved \nby the clauses in Program 4a. (4) shell(Xs) *- seek(abort,Xs,Ys) [ shell(Ys?). seek(X,[X[Xs],Xs). seek(X,[Y[Xsl,Ys) \n X\\----Y I seek(X,Xs?,Ys).  Program 4a: An extension to the shell that handled an abort interrupt. The \nprogram operates as follows. When an ,fg(X) command is received, the two guards, envdope and #eric are \nspawned in parallel, and begin to race. The first to commit aborts the second, so if en~lope terminates \nbefore #ed~ found cause the user hasn't typed such a command yet) then the envdope commits, seek is aborted, \nand #hdl proceeds nor- mally with the next command. On the other hand, if tee succeeds in finding an \nabort command before envdope ter-minates, then envelope is aborted, and shell proceeds with the input \npast the abort command, as returned by seek. A more general interrupt, grand-abort, that aborts all processes \nspawned by shell, both foreground and back- ground, can also be implemented quite easily: topshell(Xs) \n+-- shell(Xs) I true. topshell(Xs) +- seek(grand_abort,Xs,Ys) I topshell(Ys?). The distinction the snell \nin Program 4 makes be- tween background and foreground processing is not of much use, however, since \nforeground processes are not interac- tive, i.e. they do not have access to the shell's input stream. \nOne problem with the shell giving a user program its in- put stream is that upon termination the user \nprogram has to return the remaining stream back, so that the shell can proceed. Since we cannot expect \nevery interactive user pro- gram to obey a certain convention for halting (cf. quit, exit, halt, stop, \nbye, etc.) the shell has to implement a uniform command, say e~t to \"softly\" terminate an interactive \nses- sion with a user program (in contrast to aborting it). A filter, called switch monitors the input \nstream to the pro- grand. Upon the reception of an ez/t command it closes the output stream to the program, \nreturns the rest of the input stream to the shell, and terminates. A reasonable interactive user program \nshould terminate upon encounter- ing the end of the input stream. If it is not reasonable, an abort interrupt \nwill always do the job. The following code implements this idea. Commands to interactive foreground processes \nare of the form lg(P, PiJ, where P is the process and Pi is its input stream. For example, a command \nto run the process .foo(X) with input stream Xwill be given as lg(loo(X~),X). (~) shell([fg(X,Xi)lXs]) \n.- envelope(X), switch(Xs?,Xi,Ys) I shell(Ys?). switch([exit[Xs],[],Xsl. switch([XlXs],[XlYs ],zs) ,- \nX\\=exit }switch(Xs?,Ys, Zs). Program 4b: An extension to the shell that handles interactive user programs. \n7. A manager of Multiple Interactive Processes The shell described above can handle only one interactive \nprocess at a time, like the DEC supplied TOPS- 20 EXEC. MUF (Multiple User Forks) is a popular DEC-20 \nprogram, developed at Yale university [6], which overcomes this limitation. It can handle multiple interactive \nprocesses, and has a mechanism for easy context switching. It cannot 98 returning tile updated streams \nin Pil and Input1. muf itseff compete, of course, with the convenience of a system with is suspended \non Input1. a bitmap display and a pointing device. l MUF associates names with processes. It has commands \nfor creating a new process, freezing or killing a process, resuming a frozen process, and others. Program \n5 achieves some of this functionality. (0) tour(x) ,- muf{X,l]).  (I) mu f{[create(Pn ame,Process,Pin,Pou \nt)l[nput],Ps ) Process, tag(Pname,Pout), muf{[resume(Pname)[Input?],[(Pname,Pin)IPs D.  (2) muf([resume(Pname)llnput],Ps) \n.-- find_ process(Pname,Ps,Pin,Psl)[ distribute(Input?,Pin,Inputl,Pin 1), muf(Inputl?,[(Pname,Pint)lPsl]). \n (3) muf([exitllnputl[,[(ename,[])lPsl) ,-- muf(Input?,Ps). (4) mur(ll,Ps) .- close_input(Ps). (1) \nlind- process(Pn ame,[(Pn ame,Pin)lPs],Pin,Ps). (2) find- proeess(Pnanae,[Pr [Ps],Pin,[Pr [Psi])*-- \notherwise I find- process(Pname,Ps,Pin,Ps 1). (1) distribute(l],Pin,ll,Pin). (2) distribute{[Xllnput],Pin,[XI \nlnputl,Pin),-- muf-command(X)l true. (3) distribute(IXllnputl,[XlPinl,lnpuU,Pinl)~ otherwise [ distribute(Input?,Pin,lnputl,Pinl). \n (I)close_input(l]). (2) close-input([(Pn ame,[])lPs]),--- close_input(Ps). (I) muf-eommand(create(-,_,_,_)). \n (2) mu f-command(resume(_)). (3) muf-command(exit).  Program 6: mini-MUF The muf process is invoked \nwith the call muffX~ where X is its input stream. It first initializes itself with the empty process \nlist, using Clause (0), then iterates, serving user commands. On the command ertate(Pname, Proees#,Pi, \nPo) it creates a process Process, and a process t~(Pname, Po), that tags the process's output stream \nelements with the process's name, and displays them on the screen. It also adds a record with the process's \nname, Pname, and input stream, P/, in its process list, and sends itself the command resume(Pname). On \nthe command re#urns(Prisms), mufuses Clause (3). It searches its process list for the input stream of \nthe process Pname, and puts this process record first on the list. This is done by find-proce##. If successful, \nit invokes distribute(lttput~,Pitt,Ittputl,PittI), which copies the elements of the stream Input to the \nstream P/ (Clause 3) until it reaches the end of the stream (Clause 1), or encounters a command to muf \n(Clause 2 7. In that event it terminates, On the command ezit, mufcloses the input stream of tile current \nprocess, and rentoves it from the process list (Clause 3). When encountering the end of its input stream, \nmu/'closes the input streams of all the processes in its list, and terminates (Clause 4). Some of the \nfrills of the real MUF can be easily incorporated in our mini-implementation. For example, the freeze \ncommand resumes the previously resumed process, without having to name it explicitly. This is implemented \nby the following clause: nluf(lfrcezel h, putl,[l'r,(Pn ame,l'i)lPsl),- mu f(lresume(l'n ame)lln pu t?l,[(P \nname,Pi),Pr IPs]). which rew~rses the order of the first two process records on ~he process list, :rod \nsends itself a resume command with the name of the previously resumed process. A similar default for \ne:ritc:m bc added likewise. Note that if the length of the process list is less then two, this clause \nwould not apply, since its head would not unify with the muf process. Similarly, if a resume com-mand \nis given with a wrong argument, Clause (2) wouldn't apply, since the guard, find-process, would fail. \nmuf, as (lefined in Program 5, would crash upon receiving such erroneous commands. Adding the following \nclause wouhl cause it to defaulL in such eases, to an error- message routine: mur([Xllnput l,Ps) .- otherwise \n[ muf-errorl.X,Ps), muf{Input?,Ps).  toni_erroranalyses the command with respect to the process list, \nand reports to the user the typc of error it's made. Similarly easy to imolcment are queries concern- \ning the names of the processes in the process list, and the identity of the currently resumed process. \n 8. Merging streams A Concurrent Proiog process can have several in- put and/or output streams, ;~nd \nuse them to communi-cate with several other processes; but the number of these streams is fixed for any \ngiven process. It is sometimes con- venient to determine or change at runtime the number of processes \ncommunicating with another process; this can be achieved by merging communication streams. In some functional \nand dataflow languages merge is a built in operator [1,11]. Logic programs, on the other hand, can express \nit directly, as shown by Clark and Gregory [4]. Program 6 adapts their implcmentation to Concurrent Prolog. \nIt implcments the process merge(Xf, Yf, Z], which computes the relation \"Z is the interleaving of X and \nY'. 99 mergef[XIXs], Ys, [X, Zs])~ merge(Xs?, Ys, Zs). mcrge(Xs, [YIYsl, [YlZsl)~ merge(Xs, \u00a5s?, Zs). \nnerge(Xs,[], Xs). merge([l, Vs, Ys). Program O: Merging two streams Using stream-merge ~ the basic method \nof many- to-one communication poses three major problems: t. ilow to provide a fair acce:.;s to the shared \nprocess? 2. How to minimize communication delay? 3. ltow to route a response b;~ck to the sender? The \nbuilding-block of a lair con;munication network is a fair merge operator. The abstract computation model \nof Con- curr,-ut Prolog is under-specified, and does not determine whictl of the first two clauses oi' \nProgram 6 would be chosen for reduction if both input sh'eams have elements ready. Dijk:;tr;x {[7], p. \n204) has consiclered this under-specification :~ d~.ir:~blc property of the gut~rded-command, and recom- \nme,ideal simulating a totally erratic demon when choosing be~w,zen two applicable guards. Nevertheless, \nfor reasons of ,;flicicncy and expressiveness, we prefer to work in a more :~tabi,.~environment, and \nallow the programmer to control i, hc chosen clause in the special case in which there are ap- plict~.ble \nclauses with empty gu.xrds. A stable Coneurren~ Prolog machine alCc4),s reduces a process using the first \nunillable clause with an empty guard, if such a clause exists. A stable implements;son is a natural consequence \nof h;~\u00a2ing a sequential dispatcher for the guards of a process. Such ~, dispatcher would perform the \nunification of the proc.'.~ss ab, ainst the clauses' heads sequentially, and dispatch |,he guard of a \nclause if the unification with its head suc-ceeds. It would commit as sotm as it succeeds in unifying \nthe process with the head of a ,dause whose guard is empty. The definition of a s~able machine'assumes \nthat :~ome order (say, text order) is imposed on the clauses of each ?roc, dure in the program. No~,e \nthat a stable implementa- tl.olL guarantees nothing about die selection of clauses with non-\"mpty guards. \nOn a stable machine, Program 0 above will always ;~rel~;r the first stream over :he second, if both streams \nnave elements ready. Hence it does not guarantee bounded waiting (however, it may be used to implement \na notion oJ hlterrupts with different relative priorities). To achieve fairness, this program is modified \n:;li,gh~ly, so it switches the positions of the two streams on ~:ztch reduction, as specified in Program \n7. On a stable :n~c!dne, this would ensure 2-bounded-waiting. mer,cetII\u00d7l\u00d7s], Vs, [XlZsl)+- m,:rge(Ys, \nXs?, Zs). m,,rge(Xs, [YIYs], [YlZsl)'-- merge(Ys?, Xs, Zs). mer~g,;IXs, l], Xs). merge(H, Vs, Ys). Pvosvam \n7\" Fairly n, erging two streams This program is a satisfactory solution to the problem of merging two \nstre ns. More than two streams ~an be merged by constructing a tree of merge operators. It is not difficult \nto see (el. [29])that a balanced merge tree composed of fair binary merge operators ensures linear bounded-waiting, \nand has a logarithmic communication delay. The construction of ~L static balanced merge tree is easy. \nTo allow a dynamicafly changing set of processes st fifir and efficient access to a shared resource, \na more in- novative solution ;s required. In [29]we define self-balancing binary and ternary merge operators. \nThese operators com- pose dynamically into a balar~ced merge-tree, using algo- rithms similar to 2-3-tree \ninsertion and deletion. The algo- rithms require sending messages that contain communica- tion channels, \nin order to reshape the tree. In other words, the :algorithm uses incomplete messages. 2-3 merge trees \nalso ensure linear bounded-waiting and logarithmic com- munication delay, hence we believe they provide \nan accept- able solution to the problem of dynamic many-to-one com- m u n ication. The problem of routing \nback the response to a mes.~age is solved, at the programming level, using incom- plete messages. A message \nthat requires a response typically contains an uninstantiated variable; the sender or tile mes- .~age \nsuspends, looking at the variable in read-only mode. The recipient of the message responds to it by instantiating \ntlmt variable. This technique i~ used in the monitor, queue, and disk scheduling programs below. 8.1 \nA note on abstract stream operations A more abstract ~but also longer and less efficient) implementation \nof merge can be obtained using the 8end(X,S, S1) and reeeive(X,S,51) operations on streams. They define \nthe relation \"the result of sending (receiving) X Jn stream S is the stream SI\" as follows: send(X,[XlXs \n],xs). reeeive(X,[XIXsl,Xs?), Such an implementation hides the internal representation of the stream, \nand eliminates the need to use the read-only anno~ation almost entirely in the calling program, since \nthe resulting stream of receive is already annotated as read-only. We n',d the use of eend and receive.explicitly, \ninstead of achieving this effect implicitly via unification, essential for the readability of programs \nwith complex communication patterns, such as the ones described in [15,26]. The send and receive calls \ncan be eliminated for the sake of efficiency using standard partial-evaluation and program-transformation \ntechniques [20,35]. 100 9. Monitors and the readers-and-writers prob- (0) queue_monitor(S: ~create_queue(Q), \nlem queue-mmdtor(S,Q). The Concurrent Pr(,Iog solution to the readers and writers problem uses this method \nof many-to-one com- munication. It is very similar, in spirit, to the idea of monitors [16]. A designated \nprocess (a 'monitor') holds the shared data in a local argument, and serves the merged in- put stream \nof 'read' and 'write' requests ('monitor calls'). It responds to a 'read' request through the uninstantiated \nresponse variable in it ('result argument'). A schematic implementation of a monitor is shown in Program \n8. Note that it serves a sequence o\u00a3 read requests in parallel, since the reeursive invocation of monitor \nin Clause (2) is not suspended on the result of serve, in contrast to Clause (1). (1) monitor([write(Args)lS], \nData) +- serve(write(Args), Data, NewData), monitor(S?, NewData?). (2) monitor([read(Args)]S], Data) \nserve(read(Args), Data, _), monitor(S?, Data). (3) monitor([],-).  Program 8: A schematic implementation \nof a monitor In monitor-based programming languages, a pro- cedure call and a monitor call are two basic, \nmutually ir- reducible operations. In Concurrent Prolog, on the other hand, there is one basic construct, \na process invocation, whereas a monitor call is a secondary concept, or, rather, a programming technique. \nConcurrent Prolog monitors and merge operators can implement operating systems in a functional style \nwithout side-effects, using techmques similar to lfenderson's 111]. I0. Queues Merged streams allow \nmany client processes to share one resource; but when several client processes want to share several \nresources effectively, a more complex buffering strategy is needed. Such buffering can be obtained with \na :dmple FIFO queue: a client WhO requires the scrvice of a resorrce enqueues its request. When a resource \nbecomes available it dequeues the next request from the queue and serves it. The following implementation \nof shared queues is a canonical example of Concurrent Prolog programming style. It exploits two powerful \nlogic programming tech- uiques: incomplete messages, and difference-lists. A shared queue m~mager is \nan instance of a monitor, enqueue is a \"write' operation, and \"dcqucuc\" in- volves both \"read\" and ~write \n~. An abstract implementa- tion of a queue monitor is shown in Program O. (1) queue-monitor([I equestlS],Q) \n~-serve(Requ,~st,Q,Ql ), queue_moLitor(S?,Ql?). (2) queue-monitor([],q).  Program 9: A queue monitor \n Clause (0) creates ~.'l empty queue; Clause (1) itera;,es, serving queue request:s; and Clause (2) halts \nthe queue monitor upon reaching the end of the requests stream. The implementation of the queue operations \n ,:mploys difference-lists [3]. A difference-list represents a list of elements (in this context, the \nqueue's content) as the difference between two lists, o\" streams. For example, the difference I)etwcen \n11,2,3,41X ] ~[t(l X is the list [1,2,3,4]. As a .ot:,.tional convention, we us(: the binary term X\\Y \n(readj \"the difference I)etween X and Y'), to denote the list that is the difference between the .;ist \nX and the list Y. Note that this term has no special properties predefiend, and any binacy term will \ndo, as long as it is used consistently. crea~c-queue(X\\X). servc(enqueue(X),l lead \\X] NewTail ,tlead \n\\NewTail). serve(dequeue(X), [XlNewllead :,Taii,NewHead \\Tail). Program 9a: Qt, eue operations create-queue(O \n) states that Q is an empty differencc,-list. The clauses for serve define the relation be- twe,:..k \nthe op('ration, tile old queue, and the new queue. On a~, c;,:queue(X) message, X is un.tied with the \nfirst element o~ the Tail stream, and in the ne# queue NewTail is the rest of the stream. On a dequeue(Jt:) \nmessage, X is unified with tile tirst clement of the llea(l stream, and in the new queue Newllead is \nthe rest of the old Head stream. Operationally, tile program mimics the pointer twiddling of a conventional \n(tm:ue program. One difference is the simplicity and uniformity of the way in which vari- al)h;s \"~re \ntransmitted into slid from the queue, using unilication, compared to any other method of parameter passing \nand message routing. Another is the behavior of the program when more dequeue messages have arrived then \nerulueue mes-sag'cs. In this ease the content (,f the difference-list becomes \"neg:~tive\". The llead \nruns ahead of the Tail, and the nega- tive difference between them is a list of uninstantiated vari- \nables, e;mh for an excessive dequeue message. Presumably, a process who sends such a message then suspends \non its variables in a read-only mode. One consequence is that ex- cessive dequeue requests are served \nexactly in the order in which they arrived. The program can be, ondensed and simplified, us- mg program \ntransformation techniques [20,35]. The result- mg orogram is more ellicient, and reveals more clearly \nthe declarative semantics of the queue monitor. Its operational 101 ~cm~ntics, however, seems to ~eeome \n'~ bit more obscure, and its does not hide the internal representation of the ~jueue, as Program 10 does. \n(l) queue-monitor(S) *--queue_monitor(S?, X'xX). (1) queue_monitor([dequeue(X)tS]], [\u00d7lNewlIeadX Tail) \n~-queue_monitor(S?, Newllead\\Tail).  (:~) queue- monitor([enqueue(X !IS], [I lead\\[XJNewTail]) --queue-monitor{S?, \nHead\\NewTail). (3) queue_monitor([],_). Program 10: A simpfified queue monitor I)eclaratively, the v~ \nqueuc_monitor program computes the relation queue(S), which says that S is a legal stream of queue operations, \nh. uses an auxiliary relation queue-monitor(S, Dequeue\\Enqueue), which says that Dequeue ;s the: list \nof all elements X su,:h that dequeue(X) occurs in S (Clau';e I), and that Enqueue is the list of all \nelements X such that enqueue(X)occurs in S (Clause 2). The interface between these two relations (Clause \n0), constrains the list of enqueucd elements to be ideatical to the list of dequeued elements, by calling \nthem with the same name. 11. Bounded-buffer communication Bounded buffers wer:.' introduced into logic \npro- tramming by Clark and Gregory [4]as a primitive con-struct. Their principal use in !ogle-programming \nis not to utilize a Iixed memory-area for communication, but rather Lo en.%rce tighter synchronizatt.3n \nbetween the producer and the consumer of a stream. Takeuchi and Furukawa [34]have shown how to implement \nbounded-buffers Cmcurrent I'rolog, hence it need not be considered a primitive. Their implementation \nre~:rcsents the buffer using a difference-list, and uses incom- Mete messages to synchronize the producer \nand the con-sumcr of the stream. 12. An implementation of the SCAN disk-arm scheduling algorithm The \ngoal of a disk-arm scheduler is to satisfy disk 1/O requests with minimal arm movements. The simplest \n:algorithm is to serve the next 1/O request which refers to the track closest to the current arm position. \nThis algorithm may result in unhoundcd waitmg--a disk I/O request may be postponed indetinitely. The \nSCAN algorithm trics to minimize the arm movement, while guaranteeing bounded waiting. The algorithm \nreads as follows: \"while there remain requests in the current direc- '..ion, the disk arm continues to \nmove in that direction, serv- ing ~,he request(s) at the nearest cylinder; if there are no pending requests \nin that direct,on (possibly because an edge of tile disk surface has been encountered), the arm direction \n,'handles, and the disk arm begins its sweep across the sur-C,ee in the opposite direction\" [From [17]p.94). \n(0) disk_scheduler(DiskS, UserS) --disk_scheduler(DiskS?, UserS?, ([], []), C 0, up)). (I) disk-scheduler([Request \nI DiskS], UscrS,Queucs,ArmStatc)~--  dcqucue(l~,cque~t,O, ueues,Qu ~ues l,ArmSt~te,Ar m State i}[ d \nisk_:mhcd ulcr(l)isk S?,Uscrg,Queues I ,Ar mSt,~tc I ). (2) di;;k_schcduh:r(DiskS,[ltequcsqUserS], Queu~,ArmState) \n+-cnqueue(l~eque:~t,O~ucueu,Qucues I,Arm S tgte)[ disk_schcduler(l)iskS,User Si',Queucs l,ArmState). \n (3) disk_scheduler([io(0,halt)[ -1, [], ([],[]), -)- (I) dcqueue(io(T,X), ([io(T,\u00d7)lUpQ],[]), (upq,[l), \n-, iT,up)). (2) dcq ucue(io(T,X), ([io(T,\u00d7)lUpQ],DownQ), (UpQ,DownQ), (-,up), (T),up)).  (3) dv:luc,,e(io(T,X), \n(H, [~o(T,\u00d7)I Downq]), ([I,D,,~(~ ~, .... (T,d~w ,)). (4) dc,lucuc(io('l',X).[tj.q,[io('r,x)lDow.ql), \n(IJpQ,l)ownQ), (_,,iowa), \" ,~' ,~ ~')). (I) on,1 ucuc(io(T,Args), (UpQ,I)ownO.), ([io(T,Args)pJpQ],DownQ), \n(T,down)). (2) c~ Ilu euc(io(T,Args), (tJpQ,I)ownQ), (UpQ,[io(T,l~gs)lDowuQ]), (T,up)).  (:1) czi~mcue(io(T,Args), \n(U pQ,DownQ),(Upql,I)own~ ~),('r l,I)ir))*- \"I\">TI [ insert(to(T, Args), ,JpQ, UpQI, up). (4) enqueuc(io(T,Args), \n(U pQ,DownQ),(UpQ,Downq t),(TLI)ir))+-- T<T! [ inser t(io(T,Args),DownQ,DownQl ,dow n). (I) in:;ert(io(T,X), \n[], lio('r,X)l,-). (2) in.;er t(io(T,X), [~o(Wl,\u00d7i)lql, [io(W,X), io(T i,XI)IQI,up}~- T<:TI J true. \n t:;) im,:~e.r t(i,,,('r,'<), [;o('r i,x I)IQ] , [io(T,X),io(T~ .Xl)lql,do,,,,)+- T~-Ti I t~',,e. (4) \nin.~er t(io('[',X), iio(T I,\u00d7 01Q], [io(T t, X t)lQ !],up) +-T > ='1\" 1 I i nser t(io(%x),q,:~1,up). \n(5) il,:;er t(io('l',X~), [io(T I,X I)IQ], [io(T I,Xl }IQ ~. ;,down) T-- <'tq I insert(io(T,X),q,ql,dowa). \n Program *: The SCAN disk-arm scheduler The disk scheduler has two input streams--a ~;tream of [/O requests \nfrom the user(s) of the disk, and a str~;am of incomplcte messages from the disk itself. Thc scheduler \nhas two priority que:.'es, represented as lists: one for requests to be servcd at ti,.e up3weep of the \narm, and one for the requests to be served at the downsweep. It rep- resents thc arm state with the pair \n(~'ack, Direction), where 7~aek is the current track number, and Direction is up or dov~ The disk scheduler \nis invoked with the goal: disk_scheduler(Diskb?, UserS?) where UIerS is a stream of I/O requests from \nthe user(s) of the disk, and D/skS is a stream of partially determined (incomplete) messages from the \ndisk controller. I/O re-quest.s are of the form io(Tractc, Argl), where Track is the :,r~'~ck number \nand Args contain all other necessary informa- tion. The first step of the .~cheduler is to initialize \nitself with two empty queues and the arm positioned on track O, ready for an upsweep; this is done by \nClause (0). Following the initialization, the scheduler proceeds using three clauses: P\" Clause (1) handles \nrequests from the disk. If such a request is ready in the disk stream, the scheduler tries to dequeue \nthe next request from one of the queues. If successful, that request is unified with the disk request, \nand the scheduler iterates wi~h the rest of the disk stream, the new queues, and the new arm state. The \ndequeue operation fails if both queues are empty. Clause (2) handles requests from the user. If an I/O \nrequest is received from the user it is enqueued in one of the queues, and the scheduler iterates with \nthe rest of the user stream and the new queues. P Clause (3) terminates the scheduler, if the end of \nthe user stream is reached and if both queues are empty. Upon termination, the scheduler sends a 'halt' \nmessage to the disk controller. The dequeueprocedure has clauses for each of the following four cases: \nClause (1): If DownQ is emW, y then it dequeues the first request in UpQ,and changes ~he new state to \nbe upsweep, where the track number is the track of the I/O request. P Clause (2): If the arm is on the \nupsweep and UpQ is nonempty then it dequeues Lhe first request in Up@ The new state is as in Clause (1~. \nt, Clauses (3) and (4): Are the symmetric clauses for DownQ. Note that no clause applies if both queues \nare empty, hence in such a case the dequeue procedure fails. Since the disk scheduler invokes dequeue \nas a guard, it must wait in this case for the next user request, and use Clause (3) to enqueuc it. If \nsuch a request is received and enqueued then in the next iteration the disk request can be served. The \nenquene procedure also handles four cases. If the 1/O request refers to the current arm track, than according \nto the SCAN algorithm it must be postponed to the next sweep. Clauses (1) and {2) handle this situation \nfor the upsweep and downsweep eases. If the request refers to a track number larger than the current \ntrack, then it is inserted to UpQ by Clause (3}, otherwise it is inserted to DotmQ, by Clause (4). To \ntest the disk scheduler, we have implemented a simulator for a 10-track disco controller. The controller \nsends a stream of partially del;ermined I/O requests, and, when the arguments of the previous request \nbecome deter- mined, it serves it and sends t~:e next request. (0) disk_controller([io(Track,ArgsJlS]) \n*-- (hsk_ controller(Track?,Args\",S, [0,0,0,0,0,0,0,0,0,0D. (I) disk_coatroller('lYack,Args,[ioiTrackl,Argsl) \nIS],D) -- disk('lYack,Args,D,D !) ] disk_controller(Track I?, Argsl?,S,D1). (2) disk_controller(_, halt,t], \n_). (1) di~k(-, (-,false),[],[]). (2) di~k(O, (read(\u00d7),true), [XID], :-<IDI). (3) disk(0,(write(X),true),[_ \n[D],[XtD]). (4) di,~k(N,IO,[X[D],[X[Dl]) +- N>0 I NI:=N-I,disk(NI?,IO,D,D1).  Program *\" A simulator \nof a 10-track disk controller When invoked with t~ stream S, the controller ini- tializes the disk content \nand ~ends the first request using Clause (0). It then iterates wit.~ Clause (1), serving the pre- vious \n1/O request and sending the next partially determined request, until a halt message is received, upon \nwhich it closes its output stream and terminates, using Clause (2). The disk simulator aesumes that the \narguments of an !/O request are pairs (Operlztion, Re#altCode), where the operations are read(X)and write(X). \nOn read(X)Clause (2) unifies X with the content of ~ae requested track number. On terite(X) Clause {3) \nreplaces the requested track content with X. The RestdtCode is unifi,.'d with true if the operation completed \nsuccessfully (Clauses (2) and (3)), and with [alee otherwise (Clause (1)). An exalaple of an unsuccessful \ncom- pletion is when the requested track number exceeds the size of the disk. 13. Conclusion We have \nprovided some evidence that a machine that ;mplements Concurrent Prolog in hardware or firmware will \nbc self-contained, usable, and useful, without much need to resort to reactionary concepts and techniques. \nThe next logical step is to build it.    Acknowledgements This research is supported in part by IBM \nPough- keepsie, Data Systems Division. Part of it was carried while tile author was visiting ICOT, t~e \nInstitute for New Genera- tion Computer Technology, Tokyo. The paper benefited from a critical survey \nof an earlier paper of mine, written by David Gelenter [10]. References The insertion operation is a \nstraightforward Arvind and J. Dean Bro<:k, Ill ordered-list insertion. More efficient data-structures, \nsuch Streams and Managers, in Semantic# of Con-as 2-3-~reea, can be used if necessary. current Computations, \nG. Kahn (ed.) pp.45~ 465, LNCS 70, Springer-Verlag, 1979. [2] Daniel G. Bobrow and l~ark Stefik The LOOPS \nManual (prei:~minary version), Memo KB- VLSI-81-13, Xerox PAR(:. 1983. [16] [3] Keith L. Clark and Stan-Ake \nTarnlund, A first-order theory of data and programs, in Infor- mation Proeesdng 77, B. Gilechrist (ed.), \npp.939-944, North-Holland, 1977. [17] [4] Keith L. Clark and Stew., Gregory A relational language fc.r \nparallel programming, in Proceedings of the the ACM Conference on FunciionaZ Languages and Computer Architecture, \nOctober, 1981. [18] [5] Keith L. Clark and Stew.~ Gregory PARLOG: A Parallel Logic Programming Longuage \nRe- search report DOC 83/5, Department of Comput- ing, Imperial College of Science and Technology, May \n1983. [19] [61 J.R. Ellis, N. Mishkin, and S.R. Wood Tools: an Environment for Timcshared Computing an \nProgramming, Research Report 232, Department of Computer Science, Yale University, 1982. [20] [7] E.W. \nDijkstra, A Disc@line of Programming, Prentice-Hall, 1976. [8] Daniel P. Friedman and David S. Wise An \nIndeterminate Constructor for Applicative Pro- gramming in Conference Record of the Seventh An. nual \nACM Symposium on Principle of Programming Lan. guages, pp. 245-250, 1980. [21] [9] K. Furukawa, A. Takeuchi, \nand S. Kunifuji Mandal~\" A Knowledge Programming Language on Con- current Pralog, ICOT Technical Memorandum \nTM- 0P28 (in Japanese), 1983. [22] [10] David Gelenter A Note on Systems Programming in Concurrent Prolog, \nUnpublished manuscript, Yale University, 1983. [11] Peter Henderson [23] Purely Functional Operating \nSystems in Functional Programming and its Applications, P. Henderson and D.A. Turner (eds.), Cambridge \nUniversity Press, 1982. [12] Hideki Hirakawa Chart Parsing in Concurrent Prolog, ICOT Technical Report \nTR-008, 1983. [24] [13] Hideki Hirakawa etal. Implementing an Or-Paralld Optimizing Prdog System (POPS) \nin Concurrent Prolog, ICOT Technical Report TR-020, 1983. [25] [14] Carl C. Hewitt A universal modular \nActor formalism for artificial [26] intelligence. In Proceedings of the Third lnternationaZ Joint Conference \non Artificial Intelligence, IJCAI, 1973. [15] Lisa Hellerstein and Ehuu Shapiro Algorithmic Programming \nin Concurrent Prolog: [27] the MAXFLOW ezperience. Technical Report CS83-12, Department of Applied Mathematics, \n The Weizmann Institute of of S~ience, 1983. C.A.R. Hoare Monitors: an operating systems structur- ing \nconcept Communications of the ACM, 17(10), pp.4549-557, 1974. R.C. Holt, G.S. Graham, E.D. Lazowska, \nand M.A. Scott Structured Programming with Operating System# Applica-tions Addison Wesley, 1978. Daniel \nH. Ingalls The SmallTalk-76 programming system: design and implementation. In Conference record of the \nFifth Annual ACM Symposium on Principles of Pro. tramming Languages, pages 9-16, ACM, January 1976. Robert \nM. Keller, Gary Lindstrom, and Elliot I. Or- ganic Rediflow: a multiproce#nng architecture combining \nreduc. tion with data.flow. Unpublished manuscript, Depart- ment of Computer Science, University of Utah, \n1983. H.J. Komorowski Partial evaluation as a means for inferenc-ing data-structures in an applicative \nlanguage: a theory and implementation in the case of Prolog. In Conference Record of the Ninth Anneal \nACM Sympodum on Principles of Programming I, anguage#, pp.255-268, ACM, 1982. S. Kunifuji etal.  Coneeptuat \nSpecification o/the Fifth Generation KerneZ Language Vereion I (preliminary draft) ICOT Technical Memorandum \nTM-0028, 1983. It.T. Kung Let's Design Algorithms for VLSI St~tcms, Tech-nical Report CMU-CS-79-151, \nDepartment of Computer Science, Carnegie-Mellon University, 1979. T. Moto-Oka et at. Challenge for knowledge \ninformation process-ing systems (preliminary report on fifth genera-tion computer systems) In Proceedings \nof In-ternational Conference on Fifth Generation Com-puter Systems, pages 1-85, JIPDEC, 1981. Danny Dolev, \nMaria Klawe and Michael Rodeh An O(nlogn) Uni-directional distributed algorithm for extrema finding in \na circle, Journal of Algorithm 3, pages 245-260, 1982. Erik Sandewall Programming in an interactive environment: \nthe Lisp experience. Computing Surveys, ACM, March 1978. Avner Shafrir and Ehnd Shapiro Distributed Programming \nm Concurrent Prolog, Tech-nical Report CS83-12, Department of Applied Math- ematics, The Weizmann tnstitute \nof of Science, 1983. Ehud Shapiro Algorithmic Program Debugging, ACM Distin-guished Dissertation Series, \nMIT Press, 1983. 128] [29] i30] [31] i32] 3:~ i 134] [35] i36] Ehud Shapiro A Subset o] Concur.~'ent \nProlog and its In-terpreter, Technical i~eport TR-003, ICOT--Institute for New Generation Computer Tech-nology, \n1983. Also available as Tech-nical Report CS83-06, Department of Applied Mathematics, The Weizrilann \nInstitute of of Science. Ehud Shapiro, Fair, Biase~ and Self-Baluneing Merge Operators: Thei. Specification \nand Implementation in Concurrent Prolog, Technical Report CS83-t2, Department of Applied Mathematics, \nThe Weizmann institute of of Science, J[983. Ehud Shapiro and Akikazu Takeuchi Object Oriented Pro~[ramrning \nIn Concurrent Prolog, Journal of New Generation Comput- ing Volume 1, Number 1, 1983. Yeast Shiloach \nand Uzi Vishkin An O(n21ogn)parallel ~4AX-FLOW algorithm, J. of Algor~thras, Vol. 3, ~2, J~me 1982, pp. \n128-147. I Guy L. Steele Jr. and G~,rald J. Sussman Tfic Art of the Interpreter or, The Modularity, Com-~lez \nTechnical Memoranflum MM-453, Artificial In- telligence Laboratory, MIT, May 1978. Nohirisa Suzuki Experience \nwith specilication and verification of complex computer using Concurrent Prolog in Logic Programming \nand its Applications, D.H.D. Warren and M. van Caneghem (eds), Lawrence Erlbaum Press, To appe~,r. A. \nTakeuchi and K. Furl~kawa lnterprocess Communication in Concurrent Prolog, in Prec. Logic Programming \nWorkshop 83, pp. 171-185, Albufeira, Portugal, June 1983. Also ICOT Technical Report TR-006, 1983. Hisao \n'ramaki and Taisu~e Sate .4 Transformation System for Looic Programs which Preserves Equivalence, ICOT \nTechnical Report TR-018, 1983. Sunichi Uchida Inference machine: front sequential to parallel. In Proceedings \no/the 10 ~h Ar,~nual International Symposium o~ Co,~patcr Architecture, pages 410-416, Stockholm, 1983. \nAlso ICOT Techmcal Report TR-011.  \n\t\t\t", "proc_id": "800017", "abstract": "<p>Concurrent Prolog [28] combines the logic programming computation model with guarded-command indeterminacy and dataflow synchronization. It will form the basis of the Kernel Language [21] of the Parallel Inference Machine [36], planned by Japan's Fifth Generation Computers Project. This paper explores the feasibility of programming such a machine solely in Concurrent Prolog (in the absence of a lower-level programming language), by implementing in it a representative collection of systems programming problems.</p>", "authors": [{"name": "Ehud Shapiro", "author_profile_id": "81100431580", "affiliation": "Department of Applied Mathematics, The Weizmann Institute of Science, Rehovot 76100, Israel", "person_id": "PP40036563", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/800017.800520", "year": "1984", "article_id": "800520", "conference": "POPL", "title": "Systems programming in concurrent prolog", "url": "http://dl.acm.org/citation.cfm?id=800520"}