{"article_publication_date": "01-15-1984", "fulltext": "\n A Hierarchical Basis for Reorderln8 Transformations Joe Warren T. J. Watson Research Center * In this \npaper, we propose a new dependence baaed program representation. This representation is the union of \ntwo previously separate concepts: loop carried dependence and hierarchical abstrac-tion. The resulting \nform has the property that all information necessary to reorder the set of all exe- cutions of the statements \ncontained in a given loop exists in the representation of that loop. Thus, this representation provides \nan ideal basis for reordering transformations such as vectorisation and loop fusion. As evidence of this, \nwe give efficient algo- rithms for these two transformations based on this representation. 1. Introduction \nMany compilers perform optimisations which reorder the statements in a program. Of course, reordering \nthe statements of program should not change the semantics of the program. One use- ful tool for statement \nreordering is the concept of dependence. A statement S 2 depends on t state- ment S s if S t must be \nexecuted before S s to preserve the semantics of the original program. The set of all dependences for \na program may be viewed u s partial ordering on the sequence of exe- cution of the statements in the \nprogram which preserves the semantics of the original program. Dependences arise as the result of two \nseparate effects. First, a dependence exists between two statements whenever a variable appearing in \none statement may have an incorrect value if the two statements are reversed. Current address is : Depsrtment \nof Computer Science Cornell University Ithaca. NY 148,53 Permission to copy without fee all or part \nof this material is granted provided that the copies are not made or distributed for direct commercial \nadvantage, the ACM copyright notice and the title of the publication and its date appear, and notice \nis given that copying is by permission of the Association for Computing Machinery. To copy otherwise, \nor to republish, requires a fee and/or specific permission. &#38;#169; 1983 ACM0-89791-125-3/84/001/0272 \n$00.75 A--B'C S t D=-A'E+I S 4 in this example, S s depends on S t since executing S s before S s would \nresult in S s using an incorrect value for A. Dependences of this type are data depen- dences. Second, \na dependence exists between two statements whenever the results of one statement control the execution \nof the other statement. IF (A) THEN S I B--enD S e ENDW In this case, Sg depends on St since the value \nyielded by A determines whether S s is executed. Dependences of this type are control dependences. In \nthis paper, we restrict our attention entirely to d t dependence and transformations based on d t dependence. \nTo guarantee the representation and algorithms given in this paper apply to all programs, the transformations \ngiven in [AKPW 82] should be applied to input programs as preproceseing phase. The resulting programs \ncon- sist of three types of statements: assignments, FOR loops and WHILE loops. Moreover, all control \ndependences in the original program have been transformed into an equivalent set of data depen- dences. \nAs a result, any subsequent transformation may be based 8olely on dato dependence. Finally, we restrict \nFOR loops to those in which the initial \u00b0value and the increment of the index variable are one. This \nis not a serious restriction, since say more general FOR loop can be converted into this form using the \ntechniques in [AIIK 82]. The representation proposed in this paper is refinement of the dependence graph. \nFormally, a dependence graph for program S is a directed graph O .0 (N,E). N is set of nodes with each \nnode corresponding to a unique statement in S. E is the set of directed edges (Ns, No) such that N s \ncan be executed before N s and either (1) Statement N s uses a variable defined by statement N s (true \ndependence) or (2) Statement N s defines a variable defined by statement lq t (output dependence) or \n(3) Statement Ns defines a variable used by statement N s (antidependence) [Kuck 80]. In the rest of \nthis paper, we refer to s state- ment in s program and its corresponding node in the dependence graph \ninterchsngetbly. Likewise, we refer to an edge in the dependence graph and s dependence interchangeably. \nLater in the paper, we refer to dependence graph of this type as s name dependence graph since the edges \nlink statement, using and defining variables with common names. This paper fuses two important concepts \nfrom the theory of dependence, layered dependence for arrays [Kenn 80] [Alle 83] and graph abstraction \n[KKLP 80]. The result of this fusion is s new type of dependence graph known as s hierarchical depen- \ndence graph. In the next two sections, we review layered dependence and graph abstraction. |, Layered \nDependence Layered dependence is a refinement of the concept of dependence to provide more precise information \nconcerning the effects of loops and arrays. A layered dependence tony he classified as being one of two \ntypes, loop carried or loop independent. [Kenn 80] defines a loop carried dependence as a dependence \nthat arises because of the iteration ofloops. DO 200 I--1,100 C(1)== E(1) * 2 S, DO IO0 J..l,lO0 A(I,J) \n-- C(I-I) + A(I,J-I) S s I00 CONTINUE CONTINUE In this example, S s depends on S s since S s calculates \nvalues of C used by Ssr However, on any given iteration of the outer loop, S t and S o refer to separate \nelements of C. Only on different iterations of the outer loop do S s and Sg actually refer to common \nelements of C. Since this dependence exists as s result of the iteration of the outer loop, it is clamified \nas loop carried. Likewise, there exists a loop carried dependence from S e to itself sines S a uses values \nit computes on previous iterations of the inner loop. Kennedy defines the level of s loop carried dependence \nas being the nestinl; level of the loop whose iteration causes the dependence. The loop carried dependence \nfrom S s to Sg has a level of one since the iteration of the outer loop causes the dependence. Similiarly, \nthe loop carried dependence from Sg to itself has a level of two since it is s result of iteration of \nthe inner loop. The second type of layered dependence, loop independent dependence, exists even in the \nabsence of loop iteration. Loop independent dependence is solely a result of textual ordering in the \noriginal program. Dependences between statements con-tained in no common loops must be loop indepen- \ndent. Moreover, dependences between statements contained inside common loop may also be loop independent. \nDO I00 I~1,100 A(I) --B(I) + t Ss C(I+I) --A(I) + E(I) S, 100 CONTINUE In this example, S a &#38;pen&#38; \non S t. However, the dependence would exist even in the absence of the containing FOR loop. [Alle 83] \ndescribes depen-dences of this type as being loop independent. Together, the loop carried and loop independent \ndependence partition all pomible dependences I le hi. The above descriptions give only a rough ides of \nloop carried and loop independent depen- dence. To be more precise, we must flint introduce some notation. \nAn iteration vector [Kuhn 801 for a statement is u vector containing one entry for each loop surrounding \nthe statement. The elements of an iteration vector correspond to the values of the loop induction variablce \nfor u particular execution of that statement. Given the following piece of code, DO 200 I--1,10 DO 100 \nJ--l,10 St 100 CONTINUE 20O CONTINUE possible iteration vectors for S t are (!,1), (2,5) and (10,1) \nwhere the first element of the vector represents the value of ! and the second element represents the \nvalue of J. In general, L,, the k'th ieftmost element of iteration vector J (boldface for vectors), is \nthe value of the induction variable of the containing FOR loop whose header is st nesting level k- !. \nOne useful tool for relating iteration vectors is u direction vector [Wolf 80 I. If i and j are itera- \ntion vectors, D(i~j) is s direction vector each of whose elements is defined as follows: D(i,j)k -- '<' \nif it, < Jk '--' if it. .\" j~ '>' if i h > Jk For example, D((!,1),(2,$)) --(4~,~) and D((5,8),(5,1)) \nm (,,,). Now using direction vec- tors, we define both loop carried and loop indepen- dent dependence. \nLet it be s vector consisting of the k leftmost elements of i. Let RPT(Z,q) be a vector consisting of \nq Z'a Let JJ be the concatena- tion operator for vectors 273 Definition Let S x and S I be two statements \nnested in k common loops. There exist a loop carried dependence at level p (l<==p<--k) from S x to Sy \nif and only if There exist iteration vectors i and j such that 1) S a and Sy reference a common memory \nlocation on iterations i and j respectively, with at least one of S z and Sy modifying that location. \n2) D(ikJk) is a vector of the form  RPT(-f,p-I)II(<)IIRPT(*,k-p) where * represents any of <,,,,,,,>. \nThere exists a loop independent dependence from S= to Sy if and,,only if There exist iteration vectors \ni and j such that 1) S z and Sy reference a common memory location on iterations i and j respectively, \nwith at least one of S m and Sy modifying that location. 2) D(ik,Jk) iS a vector of the form RPT(~,k) \n3) S m textually precedes Sy Direction vectors provide a firm mathemati- cal basis for dealing with \nlayered dependence. They also can provide a practical basis for dependence testing. The definition above \nequates the existence of a particular layered dependence with the existence of a particular direction \nvector. One approach to dependence testins would be to calcu- late the set of all possible direction \nvectorsfor a given pair of statements. Definition Let S ! and S 2 be statements nested in k common loops. \nV(SvSe) == {D(i,.,jk) I such that S t references some location M on iteration i and S t references M \non iteration j, with at least one of S t and S t modifying M} Testing for a layered dependence would \nproceed as follows. Construct a name dependence graph as described in the introduction. For each edge \nin the name dependence graph, test for existence of the iteration vectors i and j in part I of each definition \nusing a GCD test as in [Alle 83J. If no such i and j exist, no dependences can exist between the statements \nFor each remaining edge, calculate the set V defined above. For arrays each of whose subscripts are linear \nfunctions of a single induction variable, the calculation of this set is relatively easy nsln S techniques \nsimilar to those in [Lamp 76]. For loops which have no induction vari- able (e.g. WHILE loops) the corresponding \nentry in set of direction vectors would always be e. Now, the test for the existence of specific lay- \nered dependences is simply s set membership test. Consider the following piece of code. DO 200 I.-1,100 \nDO 100 J==l,100 D(I) =, A(I-I) * 2 S 1 E(I,J) --D(I) \u00f7 E(I,J-1) S s 100 CONTINUE 200 CONTINUE  In this \nexample, V(Sl, Ss) --((--,<), (--,m), (.,>)). A loop carried dependence from S 1 to S a at level one \nexists if and only if V(Sl, Ss) contains (<,0). Thus, there exists no level one dependence. Similarly, \nV(SI, Sg) contains (--,<), 8o there must exist a loop carried dependence at level two. The test for the \nexistence of a loop indepen- dent dependence consists of two parts. First, deter- mine if S z textually \nprecedes S\u00a2 Second, check if V(St, Ss) contains (--,--). Both conditions are satisfied so there exists \ns loop independent depen- dence. V(S~Ss) consists of the single element (--,<) which implies a single \nloop carried depen- dence at level two. As we have seen, given the set of direction vectors that can \nexist between two statements, the tests for both loop carried and loop independent dependence translate \ninto set membership tests. In addition, tests for further properties of a depen-dence such as interchange \nprevention or inter-change sensitivity falls 83] can also be translated into simple tests of set membership. \nThe set of direction vectors provides a concise, compact sum- mary of the dependence information relevant \nto two statements 8. Graph Abstraction [KKLP 80] describes graph abstraction as a process in which given \na graph a mt of nodes and their internal arcs may be merged to form a com-pound node. Any edps incident \nto (or from) the set are made incident to (or from) the compound node. Our goal k to perform a modified \ntype of graph abstraction on a layered dependence graph. We col- lapse the set of statements contained \nin a FOR or WHILE loop into a compound node that 274 summarizes the effects of that loop. We refer to \nthese compound nodes as loop nodes. Nodes representing statements are referred to as statement nodes. \nThe result of this graph abstraction is a col- lection of related graphs. Each ~aph is a depen-dence \ngraph for the statements and loops contained in a given loop. We refer to each of these depen- dence \ngraphs as a component dependence graph or CDG. The CDG's are related as follows. There exists a single \nCDG for all statements and loops st nesting level sero. Given n CDG G representing n set of statement8 \nand loops at nesting level k, asso- ciated with each loop node L in O there exists CDG representing \nthe statements sad loops at nest- in s level k+! in L. We refer to this hierarchy of component dependence \ngraphs as hierarchical dependence graph or HDG. The depth of a CDG in the HDG is equivalent to the nesting \nlevel of the statements sad loops contained in the CDG. 4. Hierarchical Dependence Graphs To construct \nthe HDG, we perform the above sraph abstraction on layered dependence graph. However, we add one slight \nmodification. This modification reflects the desire to have loop node summarize the effects of its corresponding \nloop. Consider statements S t sad S s nested in exactly k common loops with level p (p~k) loop carried \ndependence from S z to Sg. Let L t sad Ls be the p'th sad p+l'st loops contsinins S t sad S s, After \nnor- mal graph abstraction, the edge from S t to S s would appear in CI)G at depth k. However, on a \nspecific iteration of L s, S t and Sz do not refer to say common memory locations sad ,as result, are \nindependent. The edge should reflect that a value created inside L z on given iteration of L t is used \nby L s on subsequent iteration of L r As rceult~ the edge should appear in the CDG at depth p link- \nins Lz to itself. To accomplish this, the ~aph abstraction procedure moves loop carried depen-dencce \nupward durins con~ruction of the HDG. Figure 1 outlines the steps neceusJ7 to construct the layered dependence \n~aph for program. Fig-ure 2 gives the algorithm for creation of the hierarchical dependence graph siven \ns fist layered dependence graph. Given program as follom. X --10 S s DO 800 1 ,,- 1,100 L s DO 100 J \n== !,100 L s A(I,J) --X + B(J,J) S s D(J,J) ,.- A(J,J-I) 2 S, 100 CONTINUE DO 200 J,,-l,100 L s F(l+l,J) \n-D(I,J) * 2 S 4 C(J,J) --rp, J) S. 200 OONTIN~ 800 OONTIN~ The hierarchical dependence sraph corresponding \nto this prosrsm is given below.  Depth 00DG ODG for L s CDG for L s ODG for L 8  \u00ae &#38;#169; \u00ae @ \nt ! I i &#38;#169;c@ \u00ae @ ; loop carried dependences -- -~ loop independent dependences This example \nis relatively etrslghtforward. The only subtle point is the loop carrisd depen- dence from S 4 to Se \nat level one now links loop node L s to itself. As stated previously, this reflects the fact that values \nproduce by L e on a siren itera- tion of L s are then used by L e on n subsequent iterltion of L r if \nNG is the name dependence ~sph created in CONSTRU~VF sad k is the maximum nesting depth in the program, \nthen the time spent procem- ins the FOR loop (0) in CONSTRU~'~F is O(koJNE D. We exprece the efficiency \nof this sad other algorithms given in this paper in terms of the eke of NG to allow comparison to slgorithms \nnot based on layered dependence. The time spent pro- eemdns GRAPH..ABSTRAOT as given in flsure 2 is O(k'.(JNVJ+JNEJ))since \neach edge in NE can give rise to up to k+! edges in LE and each edge in LE can be moved up to k tlmea \nHowever, in practice, GRAPH,ABSTRACT csa he implemented in O(ko(JNVJ+JNEJ)) time by simply insertins \neach layered dependence into its proper CDG as it is created by CONSTRUCT. After performing GRAPH.Y~BSTRACT, \n dependence may no longer link the actual statements givins rise to the depen- dence. To insure that \nthese statements may be 275 procedure CONSTRUCT(S) /* S b~ abstract syntax tree representation */ /* \nof input program */ perform transformations in [AKPW 82] on S and normalize resulting FOR loops to iterate \nfrom I to an upper bound by I [AIIK 82] build the name dependence graph NO =.. (NV, NE) for S as described \nin the introduction let LG --(LV,LE) with LV --NV and LE empty (1)/* Expand each edge in NG into */ /* \nlayered dependences in LG */ for each edge (Sj,S~ in NE do begin let k be the common nesting depth of \nS t and Sj /* Test for existence of i and j */ if there exist iteration vectors satisfying GCDTEST in \n~Alle 83] then begin compute V(Si,Sj) /* Test for loop independent dependence */ if S t textually precedes \nSj and RPT(m,k) is in V(S,,$j) then add edge (SeSi) marked loop independent to LE /\" Test for loop carried \ndependences ./ for i:==l to k do if RPT(--,i-I)H( < )[[RPT(o,k-i)is in VlS,,Sj) then add edge (Si, Sj) \nmarked loop carried at level i to LE end end /. return layered dependence graph ./ return(LG) end Figure \n1. Construction of Layered Dependence Graph procedure GRAPH.,ABSTP,.ACT(S, LG,L,k) /e S is abstract syntax \ntree representation of e/ /0 set of statements st nesting level k el /e LO is layered dependence graph \nfor S e/ /* L is the loop node with which the computed e/ /e CDOIS associated ./ /0 Procem each contained \nloop first e/ for each loop header LH at level k in S do begin create loop node L' with the same upper \nhound is LH and ndd to LO let S' be nbtres representing the statements in the loop headed by LH let LG' \nbe the subgr ph of O induced by S' /0 Relink to create compound node ./ for each edp (Sj,Sj) betweenLG' \nand LG-LG' begin remove edge (SvSI) from LE it Sj in O' then add (L',$) to LE else add (S,,L') to LE \n end let LG :-- LG - LG' GRAPi.I.,ABSTRACT(S',LG',L',k + 1) end [* Move loop carr. dependences to higher \nCDG *[ for each edge D in LE do if D is loop carried st level p and p<k then begin remove D from LE add \n(L,L) to CDO containing L end make LG the Cq)G mmoci ted with loop node L. end Figure 2. Construction \nof the HI)(] referred to if necesmuT, we amume GRAPH.,~STRACT also malntslns pointers to the them istemente \nus the dependence is moved. 6. Slnsle Loop Transformations The layered dependences in the HDG ace partitioned \nso that s CI)G st depth k contslns only loop independent dependences and kvel k loop car- tied dependences. \nThis partitioning causes the HI)G to he especially malted for certain types of program transformations. \nJAils 83] defines reordering transformation as any transformation which changes the order of execution \nor a set of statements without adding or deleting any execu-tions of any statement in the set. A reordering \ntransformation preserves a dependence from S t to S s if after the transformation S s is still executed \nafter S t. A reordering transformation is valid if and only if it preserves all dependences in a pro- \ngram. We now identify a certain subset of reorder- ink transformations which we will refer to as single \nloop transformations. Definition Let L be a loop iterating from 1 to some upper bound N by 1 containing \na set of atements {or loops) Sl, .. Let S~ be the j'th execution of S i in L. A single loop transformation \nof L is a reordering transformation in which the sequence of execution of the SU's is permuted. Later \nin the paper, we show complex transforma- tions, such as vectorisation, can actually be expressed as \na sequence of single loop transforma- tions. The following theorem states the key pro- perty of the HDG \nwith respect to the effects of a single loop transformation. THEOREM Let C be a loop node in the HDG \ncorresponding to the loop L. A single loop transformation for a loop L is valid if and only if the transformation \npreserves all dependences present in the CDG associated with C. PROOF Let L be a loop whose header \nis at nesting level k-l. Let G be the CDG associated with G. Implication from left to right: By definition, \nany reordering transformation is valid if and only if it preserves all dependences in the original program. \nThus, if a single loop transformation for L is valid, this implies that the transformation preserves \nall dependences including those in *G. Implication from right to left: We must show that this single \nloop transformation preserves all depen- dences in the program. First, only dependences between statements \ncontained in L (or possibly nested in loops in L) can violated by a reordering transform ,ion restricted \nentirely to L. So consider a dependence from S x to Sy existing in a flat layered dependence graph with \nboth S x and S r contained in L. After construction of the HDG, the edges may exist in one of three places. \nFirst, it could exist in G. By hypothesis, the transformation preserves these dependences. The dependence \ncould also occur in a CDG G' st depth p (p>k) which is a descendent of some loop node S t contained in \nG. This dependence must be either loop independent or loop carried at level p. In either case, the frst \nk elements of the direc- tion vector associated with this dependence are all ~.'s. Thus, any memory locations \nreferenced in S. ,J by S x must also be referenced by S in SiX Thus, any single loop transformation \non L must preserve these dependences. Finally, the dependence could exist in CDG G\" at depth q (q<k) \nwhich is an ancestor of G in the HDG. This implies the dependence must be loop carried at level q. However, \nthe direction vec- tor associated with this dependence consists of q-I --'s followed by a <. This implies \nthat the memory locations referenced by S must be com- pletely independent of the locations referenced \nby S~ for any given execution of the loop L. Thus, any single loop transformation of L must preserve \nthese dependences. QED This theorem allows algorithms for single loop transformations to restrict their \nscope to one part of the HDG at a time. More importantly, a single loop transformation can be extended \nto han- dle programs with multiple nested loops by simply performing the transformation on each component \ndependence graph in the ItDG. As a result, we can derive compact, efllcient algorithms for more gen- \neral transformations by expressing them as s sequence of single loop transformations. 0. Veetorlsatlon \nThe first transformation for which we formu- late an algorithm is vectorisation. In vectorisation, scalar \ncode inside a FOR loop is transformed into an equivalent set of vector statements. This is not straightforward \nprocess since the semantics of a sequential FOR loop differ from those of a vector statement. Consider \nthe following example: DO 100 I\"1,100 A(1) --A(I-S) \"e(1)  100 CONTINUE A direct translation to vector \nstatement would produce: .~1:100) .-X(O:09) * B(l:lO0) 277 However, this translation would introduce \ns seman- tic difference, in the origlnal code, the value fetched by A(I-I) on one itsrntion is the value \netored by A(1) on the previous iteration. The vector statement, however, fetches all of its operands \nbefore storing any operan&#38;, in general, direct translation to vector statement is valid if the statement \nin question does not depend, either directly or indirectly, on itself. JKI(I,P 80] gives an algorithm \nfor vectorislns set of statements contained inside single FOR loop. The alsorithm consists of five steps \n(I) Constructs dependence graph G for the statements in the loop (2) Find the strongly connected components(SCC) \nin G (3) For each statement not part of SCC in G vectorise the statement (4) For each SCC in G generate \na FOR loop around the statements in the SCC and col- lapse the SCC into 8in Is node (5) Generate the \nloops and vector statements in the new G in topologlcal order  Vectorisation is reordering of the set \nof all executions of the statement in loop 8o that ale executions of a given statement are Srouped together, \nbut with no specified ordering among that group. Thus, vectorisstion of a set of |tatemente contained \nin one loop is a single loop transforma- tion. Extending the above algorithm to vector'me statements \ncontained in multiple loops is now easy. The extended algorithm performs steps two, three and four of \nthe above algorithm on each component graph of the hierarchical dependence Srsph. While applying these \nsteps to CDG, it updates the sraph to reflect the new vector statements and dis- trlbuted loops being \ngenerated. The loop indepen- dent dependences in the updated HI)G reflect the new topolosical statement \norder required by step five. Finally, the extended algorithm procemes the CDG's in bottom up order. \nThis order Suarantse8 compound node C contained in a CDG t depth k represents a CI)G at depth k+l which \nis single strongly connected component. Hence, we conclude  statement is contained in SCC of the flat \nlaY-ered dependence graph for loop L if' and only if it is contained in SCC in the CDG representins \nthe statements in L.  Fisure 3 ~ves the complete alsorithm for vectorlsstion. Given the G' m. (V'~') \ncreate by step (1) in VECT, the time spent procemln S step (2) is O([V'J+JE'J) usinS Tsrjan'e algorithm \niT re 72]. Similinrly, the time spent processing steps (a) and (4) is ~ O(IV'J+JE'J). Thus, the time \nspent in a single call to VECT is O([V'J+JE'J).  procedure VECT(L,k) /e L is loop node for Seep to be \nvectorised e/ /e k is ncetins depth of statements in L e/ let O be the (X)G umcisted with L let G' :-- \nO (I)/0 Call VECT on each contained loop 0/ for each node L' in G do if L' is loop node /e Update \nG' to reflect vectorised loop o/ then repince L' by VECTOL',k+I) in G' /0 If L b WHILE, no vectorkation \npomibk ./ if L b WHILE no&#38; then besin let the CI)G mmociated with L be G' return(the srsph coneisting \nof L) end  I' use Tacj n', alprithm [Tacj 72] ,/ find the strongly connected resiGns in G' (8)/0 rectories \n-I! ,tatemente not in t SCC o/ for each node N in G' not contained in s SCC besJn if N is n statement \nnode then let N' be N vectorised in k'th dimension else besin let N' be loop node with the same upper \nbound u L let the CDG nseoclated with N' be N end replace N by N' in G' for each edp E incident to (or \nfrom) N' do mark E u loop independent end (4)/* Dbtribute L around each 8CC o/ for each strongly connected \ncomponent S in G' do he~n let L' be a copy of loop node L replace S by L' in G' let 8 become the 01)G \nmmoclated with L' for each edse E incident to (or from) L' do mark E u loop independent end /o Return \ngraph of vectorked L e/  return(O') end Fisum 8. Vactorisation A~|orithm i 278 To construct a time \nbound for sum of all calls to VECT, let NG --(NV, NE) be the name dependence graph from which the layered \ndepen- denes graph and the HDG were constructed. Let k be the depth of the deepest CDG in the HDG. A \ngiven edge in NE can result in at most k loop car- ried dependences. These k edges exist in k different \nCDG'e, Oi, G 7 ..... Gk, where G, is the direct ances- tor of Gt+ t in the ltD(]. The given edge in NE \ncan also result in single loop independent dependence in the deepest of these CDG'S\" G t. In the worst \ncase, VF, CT may transfer both the loop carried dependence and the loop independent dependence in G t \ninto Of. i during the update of G k. However, since both these dependences are now loop indepen- dent, \nwe may collapse them into single loop independent dependence. Now, Oh. 1 contains st must two layered \ndependences for the edge in NE. By inductively repreating this argument for each Gi, we conclude that \nVECT examines at most 2ok layered dependences for the given edge in NF~ Each node in NV is processed \nonce. However, the update process can result in k new nodes for every node in NV. Thus, the total time \nrequired for nil eaib to VF~T is O(ko(INV]+I~I) }. This algorithm compares well with previous algorithms. \nFor example, [Kenn 80] gives veetod- sation algorithm based on loop carried dependence. Nots that this \nalgorithm and VECT produce the same vector code. The worst case time for this algorithm is O(ko(JLVI+JLEJ) \n) where LG --(LV~E) is the layered dependence graph for program and k is the maximum nesting depth for \nthat program. Sines single edge in the name dependence graph can actuaily result on k+i edges in the \nlayered dependence graph, this algorithm is O(kt*INEI+koINVI) ). Thus, for programs in which JLEI is \nO(k*lNEI) (NE consi~ primarily of edge linking scalars), VF, CVF performs better, in the ease of vectorisation, \nthe hierarchical dependence graph makes possible an efficient algorithm. T. Loop Fudon Another transformation \nwell suited for use with the EIDG is loop fusion. As the name suggests, loop fnsion is transformation \nin which the bodies of FOR loops are fused together to form a single FOR loop. While not single loop \ntransformation, loop fusion can be viewed as the inverse of a single loop transformation in which a single \nFOR loop is split in to two separate FOR loops. Taking advan- tMe of this fact, we formulate the general \nfusion algorithm as a sequence of calls to simplified fusion algorithm performed on each component graph \nin the iIDG. One might uk why loop fusion is useful [AIIC T21 suggests the use of loop fusion in eombl- \nnation with loop unrolling. The loop fuaio algorithm would automatically fuse copies or inner loops created \nby unrolling outer loops. JAlle 83] and [KKLP 80] suggest performing loop fusion to optim- ise the use \nof vector registers. Another use, sug- gested by John Cocks, is in compiling APL for sequential machines. \nA naive translator could pro- dues set of sselgnmente each nested inside severs/ loops. The loop fusion \nalgorithm could then automatically collect these statement8 inside com-mon loops whenever pomible. After \nfusion, more optimiJatious such as the replacement of temporary arrays by scalars could be applied to \nthe resulting code. in general, two adjacent FOR loops may fused together if two conditions are satisfied. \n(I) The induction variables for both loops iterate to common upper bound. (2) Fusing the bodies of \nthe two FOR loops preserves Ill dependences from the first loop to the second loop.  For programs containing \nnested loops, cri-terion two can be expressed in terms of layered dependence. Two adjacent FOR loops \nwboee headers are at nearing level k may be fused it the fusion does not induce any level k+l loop carried \ndependences from the body of the second loop to the body of the &#38;rat loop. To understand this, remember \nthe direction vector for a level k+l loop curried dependence consist, of k ,-'e followed by ~. Before \nfusion, the direction vector consisted of k --'S\" 8o the dependence must have been loop independent \ndependence from the first loop to the second loop. Than, the fudq of the two loops revenmd this dependence \ncausing the transforma- tion to be inv~d. Definition Let D be loop independent dependence both incident \nto and incident from s FOR node in CDG at depth k. Let S t and yj be the actual statements giving rue \nto D. D is fusion preventlq if V(Sj,S,) for the new fused loop contains the direction vector RPT(--,k}[[(<). \n Previous disemmlons of loop fusion have only given the conditions under which adjaes t FOR loops may \nbe fused. We formulate an algorithm which reorders statements to bring as many FOR loops together m, \npomible. Sines the concept of adjaes cy is nebulous for n dependence graph, we use the topological order \nimposed by the loop independent dependences to guide our efforts. This assumption is valid sines only \nloop independent dependences force textual ordering. Now, two loops are candidates for loop fusion it \nthe 279 topolngical order imposed by the loop independent dependences allows the two compound nodes \nto be generated sequentially. The goal of the algorithm is to generate together as many fusible FOR nodes \nas po~ible. Informally, the algorithm consists of the following steps applied to each CDG. (I) Topologically \ngenerate as many statement nodes as possible (2) Let L be a loop node that can be topologi- caily generated. \n (3) Fuse L with as many other loop nodes as topologically available. (4) Generate L. (5) Repeat this \nprocess until all nodes are gen-erated.  By generating all possible scalar statements, the algorithm \nguarantees that as many FOR nodes as possible are available for steps (2) and (8). Moreover, after step \n(I), only FOR nodes can be generated. Since at least one FOR node must now be generated, the algorithm \nchooses FOR node and attempts to fuse it with as many FOR nodes as topologically possible. This process \nis repeated for the remaining dependence graph. Figure 4 contains precise specification of the loop \nfusion algorithm. This algorithm performs the transformation above, but in more el~cient manner. Step \n(1) in FUSE topologically generates as many statement nodes and WHILE nodes as pos- sible. Note, whenever \n new WHILE node or state- ment node becomes available to be generated, it is added to STATQUEUE. When \n FOR node becomes available, the algorithm calls the pro-cedure add. Add looks up the upper bound of \nthe loop in LOOPTABLE. If another FOR node with the same upper bound already exists in LOOPT- ABLE, add \nfuses the two FOR nodes and reinserts them in LOOPTABLE. Otherwise, add inserts the new FOR loop in LOOPTABLE. \nNo fusion preventing dependences can exist between the fused FOR nodes, since both nodes being available \nto be generated implies no loop independent dependences exist between them. Since termination of step \n(1) forces some FOR node to be generated, step {2) chooses FOR node, RL, to be generated. Step (3) attempts \nto fuse as many successors of RL to RL as topologically possible. FUSEQUEUE holds FOR nodes ready to \nbe fused to RL. To analyse the time required by FUSE, we consider the time for one call to FUSE on the \nCDG G --(V,E) above. Both STATQUEUE and FUSE- QUEUE are normal queues so operations such as insert, delete, \nand empty take constant time. The inner most loop in step (1) is iterated at most ~EJ times with the \nadd operation being done at most iN[ times. So, the time for step one is O([E[ + [NJ*cost(add)). The \ntime for step (2) is O(\u00a2ost(remove)). To find a bound for step (3), we must analyse the time required \nto test for fusion preventing dependence. Since we restricted the tests used in calculating sets of direction \nvectors to linear subscripts, we can update the set of direction vec- tors to reflect the new fused loop \nin constant time. Moreover, since the original set of direction vectors contained a loop independent \ndependence, we can also test for fusion preventing dependence in con- stant time. The iteration of the \ninnermost loop in step (3) is bounded by JEJ with at most JNJ acld's being performed. The worst case \ntime for (3) is O([E [ + [Niecnst(sdd)). The total for one call to FUSE k O(JE[+JNJecost(add) \u00f7 cost(remove)). \nNow, we addrem the cost of the add's and the remove. If these operations are implemented using 2-3 tress \n[AhHU V4], then O([NJ*cost(add) + cost{remove)) is O([NJ*I~JNJ). in practice, the algo. rithm above could \nuse s hash table to. achieve linear average case time. For our analysis of worst case time, we use 2-$ \ntrees. To bound the cost of all calls to FU~E, we let again NO R (NV, NE) be the original name dependence \ngraph. Using an argument similiar to the one given in the previous section, we can show that the time \nprocessing all layered dependences associated with a given name dependence is O(k) where k is the depth \nof the deepest CDG. To bound the time processing nodes, we only need note that each node in NV is processed \nonly once. Addition- ally no new nodes are created by the update in FUSE. Thus, the total time proceming \nnodes is O(INVI*IgINVI).The worst cue time bound for all calis to FU~ is O(k*INEI+INVI*lgINVl). This \nalgorithm does not always yield maxl- really fused loops. It may fail for one of two re~- sons, First, \nthe choice of s FOR node to fuse in step (2) can aiTect the success of the fusion algo- rithm in later \nstages. However, since most programs tend to have FOR loops st s given nesting level iterate to the same \nupper bound, the set of avail- able FOR nodes to choose from in step (2) will usu- ally contain only \none member. Second, the algo-rithm alsu limits its scope to only one component dependence graph st a \ntime. To guarantee maximal loop fusion, the algorithm should, when choosing which FOR nodes to fuse, \nconsider the success or failure of attempts to fuse subsequent loops con-tained in new fused loop*. However, \naltering the algorithm to attempt this type of look ahead would severely affect its performance. In general, \nthe above algorithm should perform well on tasks such as those given earlier in this section. 8. Conclusion \nThe IID(] stills needs work in few areas. First, the use of a normalisation routine prior to construction \nof the HDG is too restrictive. The code produced by this routine may not be as efficient as the original \ncode because of the simplified control else add(LP', LOOPTABLE) else inserqLP',STATQUEUE) end procedure \nFUSE(L) 1. L is loop node containing loops to be fused ./ let G be the nodes and loop independent dependences \nin the CDG for loop node L let G' be the empty graph let STATQUEUE, FUSEQUEUE be empty queues let LOOPTABLE \nbe an empty table for each node N in G which has no predecessors do if N is a FOR node then add(N, LOOPTABLE) \nelse insert(N, STATQUEUE)  while ('empty(STATQUEUE) or \"empty(LOOPTABLE)) do begin (s) /. Topologically \ngenerate statements ./ while('empty(STATQUEUE)) do begin let N : -- delete(STATQUEUE) move N from G to \nG' for each M a successor of N do if M has no predeceseom in O then if M is s FOR node then add(Y, LOOPTABLE) \nelse insert(Y, STATQUEUE) end (2) /* Choose s FOR loop to generate 0/ if \"empty(LOOPTABLE} then begin \nlet RL :-- re, move(LOOPTABLE) sdd(RL, FUSEQUEUE) let L' be loop node with empty CDG and same upper bound \nas RL end  (3) /* Topologically fuse FOR loops to RL ,1 while ('empty(FUSEQUEUE)) do begin let LP :-- \ndelete(FUSEQUEUE) let L' :-- merge(L', LP) /0 Merge CDG's 0/ remove LP from G for each LP' s successor \nof LP in O do if LP' has no predecessors in O then if LP' is FOR node then /. UPB is upper bound for \nloops 0/ it (UPB(LP') -,, UPB(L')) and (\"fusion_preventing(L',LP')) then insert(LP', FUSEQUEUE) fuss(L') \n/* Recureive call on nested loops */ add L' to G' end let O' be the CDG amociated with loop node L end \nFignre 4. Loop fusion Algorithm structure. One approach being explored is the incor- poration of the \ncontrol dependence structure described in [FeOW 83]. Using thi~ approach, the control dependences are \nrepresented as they exist in the original program. Another ires of concern is the design of an implementation \nof the HDG. The techniques used in [AIIK 82] should provide a firm basis for an implementation of the \nllDG. The most crucial deci- sion concerning any implementation is how loop nodes and their associated \nCDG's are represented. Finally, more transformations suitable for use with HDG need to be identified. \nOne candidate transfor- mation is the automatic conversion of a sequential FOR loops into the LOOP CONC \nconstruct described in [Love 77]. Since this is s single loop transformation, an efficient algorithm \nfor transforming nested FOR loops into LOOP CONC's should be possible. In summary, the HDG provides an \nattractive intermediate representation for program transfor-mations. By carefully partitioning the information \navailable from layered dependence into CDG's, the hierarchical dependence graph allows construction of \nefficient algorithms for s variety of reordering transformations. In addition, the representation of \nloops as compound nodes allows the formulation of these algorithms in more elegant terms. ThuB, this \nform satisfies one of the fundamental requirements of any intermediate form: algorithms based on it should \nbe both efficient and elegant. Acknowledgements i would like to thank Mike Burke, Jeanne Ferrante and \nFran Allen for their time and willing- hess to discuss the ideas in this paper. I also would like to \nthank Randy Allen and Doug Moore for their help in reading this paper. 9. Blbllousphy JAhlIU 741 A.V. \nAlto, J.R. Hopcroft, J.D. Ullman, The Design and Analysis of Computer Algorithms, 281 Addison-Wesley, \nReading, Musachusetts, 1074 [Kuek 78] D.J. Kuek, The Structure of Computers Ind Compute, ions, Volume \n1, John W'dey and Sons,  V~cPw s2] New York, 1978. J. R. Allen, K. Kennedy, C. Porterfleld, J. War- \nren, \"Conversion of Control Dependence to Data Dependence,\" Conference Record of the Tenth [Pa s01 Annual \nACM Symposium on Principles of Pro- D.A. Padua, D.J. Kuck, and D.IL Lswrle, STammins Languages, Austin, \nTX., Jan. 83 \"High-Speed Multiproc_,~-~ rs stud Compilation Techniques,\" Special Imue on Parallel Procem- \nin&#38; ~ Trssm. on Computers, Vol. 0-29, No.  ~c 72J 0, pp. 708-770, Sep. 1080 F.E. Allen sad J. Cocke, \n\"A cstalosue of optim- ising transformations,\" Design and Optimisation of Compilers, R. Rue,in, ed., \nPrentice-Hall, ITari Englewood Cliffs, New Jersey, 1072, 1-30 R.E. Tarian, \"Depth first eesrch and linear \nSrsph algcrithms,\" SIAM J. Computing I, 2, 1072, 146.180 [Alle 83] J.R. Allen, \"Dependence analysis \nfor subscripted varisbles and its pplicstion to proKrsm transformations,\" Ph.D thesis, Rice University, \nDepartment of Mathemstical Science, Ms)' 1983   [AZIK S~] J.R. Allen, K. Kennedy, \"PFC: program to \nconvert Fortran to parallel form,\" Report MASC TR 82-6, Department of Mathematical Sciences, Rice University, \nHouston, Texas, March, 1082  FeOW 831 J. Ferrante, K. Ottermtein, and J. Warren, \"The Prosrsan Dependence \nGraph and it~ uses in optimisstion', IBM Technical Report RC 10208 August 12, 1083 [Lamp 715] Lamport, \nLeslie, \"The Coordinste Method for the Parallel Execution of lterative Do loops,\" SRI Technical Report \nCA-7606.0~1, August 2, 1076 [Love 77] D.B. Loveman, \"Program improvement by source to source transformation,\" \nJ. of the ACM, Vol. 24, No. I, pp. 121-145, Jan. 77 [I enrlt80] K. Kennedy, \"Automatic translation of \nFortran programs to vector form,\" Rice Technical Report 476-029-4, Rice University, Oct. 1900   [KKLP \nS0] D.J. Kuck, R.H. Kuhn, B. Leuure, DJL Padua, and M. Wolfe, \"Dependence Ip'aphs and corn- plier optimization \n,\" Conference Record of Eighth Annual ACM Symposium on Principles of Pro~amming Languages, Williamsburg, \nV~., Jan. 81.  282  \n\t\t\t", "proc_id": "800017", "abstract": "", "authors": [{"name": "Joe Warren", "author_profile_id": "81100611449", "affiliation": "Department of Computer Science, Cornell University, Ithaca, NY", "person_id": "PP14210854", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/800017.800539", "year": "1984", "article_id": "800539", "conference": "POPL", "title": "A hierarchical basis for reordering transformations", "url": "http://dl.acm.org/citation.cfm?id=800539"}