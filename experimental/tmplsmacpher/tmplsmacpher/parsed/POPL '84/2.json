{"article_publication_date": "01-15-1984", "fulltext": "\n Reflection and Sentantics in Lisp Brian Cantwell Smith XEROX Pale Alto Research Center 3333 Coyote Hill \nRoad. Pale Alto. CA 94304; and Center for the Study of l.anguage and Information Stantbrd University. \nStanlbrd. CA 94305 1. Introduction l\"or three reasons, bi.';p's self-refi;rential properl.ies have not \nled to a general un(h:rst.auding of what it is fro\" a cmuputational system to reason, in substantial \nway~, about its; owe operations a,ul structures. First., there is more to reasoning than reference; one \nalso needs a theory, in terms of which to make .,~ense of the referenced domain. A comln, ter system \nable to reason about it.:;elf-what I will call a reflective system --will therefore need an account of \nitself embedded within it. Second, there most he a systematic relationship between that embedded account \nand the system it describes. Without such a connection, the account would be useless --as disconnected \nan the words of a haple~;s drunk who carries on about the evils of inebriation, without reali~iug that \nhis story applies to himself. Tl'aditional embeddiugs of IAsp in Lisp are inadequate in just this way; \nthey provide no means for the implicit state of the Lisp process to he reflected, moment by moment, in \nthe explicit terms of the embecbled account. Tlaird, a reflective system nmst be given an appropriate \nvantage point at which to stand, far enough away to have itself in focus, and yet close enough to see \nthe important details. This paper presents a general architecture, called procedurcd refh'ctio,, to support \nsell'directed reosoning in a serial programming lmaguage. Tim architecture, illustrated in a revamped \ndialect called 3-Lisp, solves all three problems with a single mechanism. The basic idea is to define \nan infinite tower of procedural self-nmdels, very much like mctacircular interpreters [Steele and Sussman \n1978b], except connected to each other in a simple but critical way. In such an architecture, any aspect \nof a procc~s's state that can be described in terms of One theory can be rendered explicit, in program \naccessihle structures. Furthermore, as we will see, this apparently infinite architecture can be finitely \nimplemented. The architecture allows the user to define complex programming constructs {such as escape \noperators, deviant variableqmssing protocols, and dehugging primitives), by writing direct analogues \nof those metalinguistie semantical expressions that would normally be used to describe them. As is always \ntrue in semantics, the metatheoretie descriptions must be phrased in terms of some particular set of \nconcepts; in this case I have used a theory of Lisp ba:;ed on environments and continuations. A 3-Lisp \nprogram, therefore, at any point during a computation, can obtain representations of the environment \nPermission to copy without fee all or part of this material is granted provided that the copi~ are not \nmade or distributed for direct commercial advantage, the ACM copyright notice and the title of the publication \nand its date appear, and notice is given that copying is by permission of the Association for Computing \nMachinery. To copy otherwise, or to republish, requires a fee and/or specific permission. &#38;#169; \n1983 ACM 0-89791-125-3/84/001/0023 $00.75 and continuation char;wtcrising the state of the computation \nat that pui,~t. Thus, such constructs as ttmow and C,~TCII, which must otherwise be providt,d primitively, \ncan in 3-Lisp be easily defined a:; user procedures (and defined, furthermore, in code that is ~,!most \nisomorphic to the ~-calculus. equations one normally writes, in the metalal'$,3~a!,'c, to describe them). \nAnd all this can be dolte wilhout writing the entire program in a centinuation-pas:;iz~g :~tyle, o!' \nthe sort illu,;trated in [Steele 197til. The point is no!. to decide at the outset what should and what \nshould not be explicit (in Steele's example, continuations must be passed arouml explicitly from the \nhcgim, ing). Rather, the retlective architecture provides a method of making some aspects of the computation \nexplicit, right in the midst of a computation, even if they were implicit a moment earlier. It provides \na mech'mism, in other wo~'ds, of reaching up and \"pulling information out of the sky\" when unexpected \ncircumstances warrant it, without having to worry about it otherwise. The overall claim is that retlection \nis simple to build on a semantically sound hase, where 'semantically sound' means more than that the \nsemantics be earefl~lly formulated. Rather, I assume throughout that computational structures have a \nsemantic significance that transcends their behavioural import --or, to put this another way, that cmnputatinnal \nstructures are about something, over anti above the effects they have on the systems they inhabit. Lisp's \nNft. for example, not only ev~tluates to itself forever, but also (and somewhat independently) stands \nfor Falsehood. A reconstruction of Lisp semantics, therefore, must deal explicitly with both declarative \nand procedural ospects of the overall significance of computational structures. This distinction is different \nfrom (though I will coutrast it with) the distinction between operalional and denotational semantics. \nIt is a reconstruction has boca developed within a view that programming languages are properly to be \nunderstood in the same theoretical terms used to analyse not only other computer languages, but even \nnatural languages. This approach forces us to distinguish between a structure's wdue and what it returns, \nand to discriminate entities, like numerals and numbers, that are isomorphic but not identical (both \ninstances of the general intellectual hygiene of avoiding use/mention errors). Lisp's basic notion of \nevaluation, I will argue, is confused in this regard, and should be replaced with independent notions \nof designation and simplification. The result is illustrated in a semantically rationalised dialect, \ncalled 2-Lisp, based on a simplifying (designation-preserving) term-reducing processor. The point of \ndefining 2-Lisp is that the reflective 3-Lisp can be very simply defined on top of it, whereas defining \na reflective version of a non-rationalised dialect would be more cmnplicated and more difficult to understand. \nThe strategy of presenting a general architecture by developing a concrete instance of it was selected \non the grounds that a gemfine theory of reflection (perhaps analogous to the theory of rccursion) would \nbe difficult to motivate or defend without taking this first, more pragtnatic, step. In section lO, 23 \nhowever, we will sketch a general \"recipe\" for adding reflective capabilities to any serial language; \n3-Lisp is the result of applying this conversion process to the non-reflective 2-Lisp. It is sometimes \nsaid that there are only a few con'~truc~.s fi'om which languages are a,~sembled, ihcluding for example \npredicates, terms, functions, composition, recursion, abstraction, a branching eulnctor, end quantification. \nThough differellt from Ihe:~e notions (and not definable iJ~ terms of them), reflection is perhaps best \nviewed as a preposed addition to that family. Given this view, it is helpfid to understand relleci.ion \nby comparing it, ia particular, with L'ecursion --a construct with which it shares many features. Specifically, \nrecursion can seem viciously circldar to the uninitiated, and can lead to confused implementations if \npoorly understood. The mathematical theory ef recursion, however, underwrites our ability to usa reeursion \nin programmiug languages without doubting its fundamental soundness (in thct, for many programmers, without \nunderstanding much about the formal theory at all). Reflective systems, similarly, initially seem viciously \ncircular (or at least infinite), and are difficult to implement without an adequate understanding. The \nintent of this paper, however, is to argue that reflection is as well-tamed a concept as recursion, and \npotentially as efficient to use. Tim long.range goal is not to force programmers to understand the intricacies \nof designing a reflective dialect, but rather to enable them to use reflection and recursion with equal \nabandon. 2. Motivating Intuitions Before taking up technical details, it will help to lay out seme motivations \nand assumptions. First, by 'reflection' in its most general sense, I mean tire ability of an agent to \nreason not only introspectively, about its self and internal thought processes, but ~.lso externally, \nabout its behaviour and situation in the world. Ordinary reasoning is external in a simple sense; the \npoint of reflection is to give an agent a more sophisticated stance from which to consider its own presence \nin that embedd:,ng world. There is a growing consensus I that reflective abilities underlie much of the \nplasticity with which we deal with the world, both in language (such as when one says Did you understand \nuhat I meant?) and in \"thought (such as when one wenders how to deliver bad news compassionately). Common \nsense suggests that reflection enables us to master new skills, cope with incomplete knowledge, define \nterms, examine assumptions, review and distill our experiences, learn from unexpected situations, plan, \ncheck for consistency, and recover from mistakes. In spite of working with reflection in formal languages, \nmost of the driving intuitions about reflection are grounded in human rationality and language. Steps \ntowards reflection, however, can also be found i,l much of current computational practice. Debugging \nsystems, trace packages, dynamic code optimizers, run-time compilers, macros, metacircular interpreters, \nerror handlers, type declarations, escape operators, cerements, and a variety of other programming constructs \ninvolve, in one way or another, structures that refer to or deal with other ourts of a computational \nsystem. These practices st~ggest, as a first step towards a more general theory, defining a limited and \nrather intro~,pcctive notion of 'procedural reflection': self-referential behaviour itJ procedural languages, \nin which expressions are pr:.marily used iu.,~tructionally, to engender behaviour, rather than assertionally, \nto make claims. It is the hope that the lessons learned in this smaller task will serve well in the larger \naccount. We mentioned at the outset that the general task. in defining a reflective system, is to embed \na theory of the system in the system, so as to support smooth shifting between reasuning directly about \nthe worhl and reasoning about timt reasoning. Because we are talking ef reasoning, not merely of language, \nwe added aa additional requirement on this embedded theory, beyond its being descriptive aml true: it \nmust also be what we will call ca,sally conm,ch.d, so that accounts of objects anti events are tied directly \nto those objects and events. Tim Figure l : A Serial Medel of Cemputation | causal relationship, htrtherinore, \nmust go both ways: fi'om event to description, and from description back to event. (It is as if we were \ncreating a magic kingdom, where fl'om a cake you could automatically get a recipe, and from a recipe \nyou could automatically get a cake.) In mathematical cases of self-reference, including both self-referential \nstatements, and models of syntax and proof theory, there is of course no causation at all, since there \nis no temporality or behaviour (mathematical systems don't run). Causation, however, is certainly part \nof any reflective agent. Suppose, for example, that you capsize while canoeing through dit/icult rapids, \nand swim to the shore to figure out what you did wrong. You need a description of what you were doing \nat the moment the mishap occurred; merely having a name for yoursell, or even a general description of \nyourself, would be usele~. Also, your thinking must be able to have some effect; no good will come from \nyour merely contemplating.a wonderful theory of an improved you. As well as stepping back and being able \nto think about your behaviour, in ether words, you must also be able to take a revised theory. and \"dive \nback in under it\", adjusting your behaviour so as to satisfy the new account. And finally, we mentioned \nthat when you take the step backwards, to reflect, you need a place to ~tand with just the right combination \nof connection and detachment. Computational reflective systems, similarly, must provide both directions \nof causal connection, and an appropriate vantage point. Consider, for example, a debugging system that \naccesses stack frames and other implementation-dependent representations of processor state, in order \nto give the user an account of what a program is up to in the midst of a computation. First, slalck-l'rames \nand implementation codes are really just descriptions, in a rather inelegant language, of the state of \nthe process they describe. Like any description, they make explicit some ef what was implicit in the \nprocess itself (this is one reason they are useful in debugging). Furthernmre, because of the nature \nof implementation, they are always available, and always true. They have these properties because they \nplay a causal role ia..~hever\u00a5 existence el' the process they implement; they therefore automatically \nsolve the \"event-to-description\" direction of causal connection. Second, debugging systems must solve \nthe \"description to reality\" problem, by providing a way of making revised descriptions of the process \ntrue of that process. They carefully provide facilities for altering the underlying state, based on the \nuser's description of what that state should be. Without this direction el: causal connection, the debugging \nsystem, like an abstract nmdel, could have no effect on the process it was examining. And finally, programmers \nwho write debugging systems wrestle with the problem of providing a proper vantage point. In this case, \npractice has been particularly atheoretical; it is typical to arrange, very cautiously, fur the debugger \nto tiptoe around its own stack frames, in order to avoid variable clashes and other unwanted interactions. \nAs we will see in developing 3-Lisp, all of these concerns can be dealt with in a reflective language \nin ways that are both simple and implementation-independent. The procedural code in the metacircular \nprocessor serves as the \"theory\" discussed above; the causal connection is provided by a mechanism whereby \nprocedures at one level in the reflective tower are run in the process one level above (a clean way, \nessentially, of enabling a program to define subreutines to be rux~ in its own ~ S~l_~act,c Ooma~n S \n1 ~3F\"Semant. i c\" Do.._.mmatn D Figure 2: A Simple Seman.tic lntepretalion Function m~plemeutation). \nIn one sense it is all straightforward; the subtlety of 3-Lisp has to do not so much with the power of \nsuch a mechanism, which is evidi~nt, but with how such power can be finitely provided --a question we \nwill examine in section 9. Some final assumptions. I assume a simple serial model of computation, illustrated \nin Figure 1, in which a computational process as a whole is divided into an internal assemblage of program \nand data structures collectively called the structural field, coupled with an internal process that examines \nand manipulates these structures. In computer science this inner process (or 'homunculus') is typically \ncalled the intelpreter; in order to avoid confusion with semantic notions of interpretation, I will call \nit the processor. While models of reflection for concurrent systems could undoubtedly be formulated, \nI claim here only that our particular architecture is general for calculi of this serial (i.e., single \nprocessor) sort. I will use the term 'structure' for elements of the structural field, all of which are \ninside the machine, never for abstract mathematical or other \"external\" entities like numbers, functions, \nor radios. (Although this terminology may be confusing for semanticists who think of a structure as a \nmodel, I want to avoid calling them expressions, since the latter term connotes linguistic or notational \nentities. The aim is for a concept covering both data structures and internal representations of programs, \nwith which to categorize what we would in ordinary English call the structure of the overall process \nor agent.) Consequently, I call metastructural any structure that designates another structure, reserving \nmetasyntactic for expressions designating linguistic entities or O expressmns.-Given our interest in \ninternal self-reference, it is clear that both structural field and processor, as well as numbers and \nfunctions and the like, will be part of the semantic domain. Note that metastructaral calculi must be \ndistinguished from those that are higher-order, in which terms and arguments may designate functions \nof any degree (2-Lisp and 3-Lisp will have both properties). 3 3. A Framework for Computational Semantics \nWe turn, then, to questions of semantics. In the simplest case, semantics is taken to involve a mapping, \npossibly contextually relativized, from a syntactic to semantic domain, as shown in Figure 2..The mapping \n(,1)) is called an interpretation function (to be distinguished, as noted above, from the standard comlmter \nscience notion of an interpreter). It is usually specified inductively, with respect to the compositional \nstructure of the elements of the syntactic domain, which is typically a set of syntactic or linguistic \nsorts of entities. The semantic domain may be of any type whatsoever, including a domain of behaviour; \nin reflective systems it will often include the syntactic domain as a proper part. We will use a variety \nof different terms for different kinds of semantic relationship; in the general case, we will call s \na symbol or sign, and say that s signifies d, or conversely that d is the significance or interpretation \nof s. In a computational setting, there are several semantic relationships --not different ways of characterizing \nthe same relationship (as operational and denotational semantical ~counts are sometimes taken to be), \nfor example, but genuinely distinct relationships. These different relationships make for a more complex \nsemantic framework, as do ambiguities in the use of words like 'program'. In many settings, such as in \npurely extensional functional programming languages, such distinctions are inconsequential. But when \nwe turn to reflection, self-reference, and metastructural processors, these otherwise minor distinctions \nplay a much more important role. Also, since the semantical thi~ory we adopt will be at least partially \nembedded within 3-Lisp, the analysis will aflbct the formal design Our approach, therefore, will be start \nwith basic and simple intuitions, and to identify a finer-grained set of distinctions than are usually \nemployed. We will consider very brielly the issue of how current programming language semantics would \nbe reconstructed in these terms, but the complexities involved in answering that question adequately \nwould take us beyond the scope of the present paper. At the outset, we distinguish three things: a) the \nobjects and events in the world in which a comlmtational process is embedded, including both real-world \nobjects like cars and caviar, and set-theoretic abstractions like numbers and functions (i.e., we \"ldopt \na kind of pan-platonic idealism about mathematics}; b) the internal elements, structures, or processes \ninside the computer, including data structures, l~rogram representations, execution sequences and so \nforth {these are all formal objects, in the sense that computation is formal symbol manipulation}; and \nc) notational or communicational expressions, in some externally observable and eonsensually established \nmedium of interaction, such as strings of characters, streams of words, or sequences of display images \non a computer terminal. The last set are the consP.ituent3 of the communication one has with the computational \nprocess; the middle are the ingredients of the process with which one interacts, and the first (at least \npresumptively) are the elements of the world about which that communication is held. In the human case, \nthe three domains correspond to world, mind, and language. It is a truism that the third domain of objects \ncommunication elements --are semantic. We claim, however, that the middle set are semantic as well (i.e., \nthat structures are bearers of meaning, information, or whatever). Distinguishing between the semautics \nof communicative expressions and the semantics of internal structures will be one of main features of \nthe framework we adopt. It should be noted, however, that in spite of our endorsing the reality of internal \nstructures, and the reality of the embedding world, it is nonetheless true that the only things that \nactually happen with computers (at least the only thing we will consider, since we will ignore sensors \nand manipulators} are communicative interactions. If, for example, I ask my Lisp machine to calculate \nthe square root of 2. what I do is to type some expression like (SQRr Z.0) at it, and then receive back \nsome other expression, probably quite like I. 414, by way of response. '['he interaction is carried out \nentirely in terms of expressions; no structures, numbers, or functions are part of the interactional \nevent. The participation or relevance of any of these more abstract objects, therefore, must be inferred \nfrom, and mediated through, the communicative act. We will begin to analyse this complex of relationships \nusing the terminology suggested in Figure 3. By O, very simply, we refer to the relationship between \nexternal notational expressions and internal structures; by ,1, to the processes and behaviours those \nstructural tield elements engender (thus I, is inherently temporal), and by ,1, to the entities in the \nworld that they designate. The relations 4, and t, are named, for mnemonic convenience, by analogy with \nphilosophy and psychology, respectively, since a study of ,I, is a study of the relationship between \nstructures and the world, whereas a study of ,1, is a study of the relationships among symbols, all of \nwhich, in contrast, are \"within the head\" (of person or machine). Computation is inherently temporal; \nour semantic analysis, therefore, will have to deal explicitly with relationships across the passage \nof time. In Figure 4, therefore, we have unfolded the diagram of Figure 3 across a unit of time, so as \nto get at a full configuration of these relationships. The expressions n I and n2 are intended to be \nlinguistic or communicative entities, as described above; Sl and s2 are internal structures over which \nthe internal processing is defined. The relationship o, which we will call internalisation, relates these \ntwo kinds of object, as appropriate for the device or process in question (we will say, in addition, \nthat nl ,otates sl) For example, in first-order logic nl and n2 would be expressions, perhaps written \nwith letters and spaces and '3\" signs; st and s2. t~ '~he extent they can even be said to exist, would \nbe something like abstract derivation tree 25 7J Figure3: Sem~lntic Relationships in a Computollonal \nProcess types of the corresponding first-order formulae, hi Lisp, as we will see, n I and n 2 would be \nthe input and output expressions, written with letters and parent.hoses, or perhaps with boxes and arrows; \nsl and s2 would be the cons-cells in the s-expre,q.qion heap. In contrast, dl and d 2 are elements oz\" \nfragments of the embedding world, and 4, is the relationship that internal structures bear to them. q~, \nin other words, is the interpretation function thst makes explicit what we will call the designation \nof intern,d structures (not the designation of linguistic terms, which would be described by ~,oO). The \nrelationship between my mental token for T. S. Eliot, for example, and the poet himself, would be formulated \nas part of ~, whereas the relationship between the public name ~I'. S. Eliot\" and the poet would be expressed \nas 4~(O(\"T.S.EI.IOT')) T.S.I.:I.IOT. Similarly, 4, would relate an internal \"numeral\" structure (say, \nthe numeral 3) to the corresponding number. As mentioned at the outset, our focus on ,1, is evidence \nof our permeating semantical assumption that all structures have designations --or, to put it another \nway, that the structures are all symbols. 4 The ~1, relation, in contrast to O and ~, always (and necessarily, \nbecau~ it dosen't hove access to anything else) relates some internal structures to others, or at least \nto behaviours over them. To the extent that it would make sense to talk of a '\u00a2 in logic, it would approximately \nbe the formally computed derivability relationship (i.e., I-); in a natural deduction or resolution ~hemee, \n,I, would be a subset of the derivability relationship, picking out the particular inference procedures \nthose regimens adopt. In a computational setting, however, ,l, would be the function computed by the \nprocessor (i.e., * is evaluation in Lisp). The relationships O, ,I,, and q have differeat relative importances \nin different linguistic disciplines, and different relationships among them have been given different \nnames. For example, O is usually ignored in logic, and there is little tendency to view the study of \n~', called proof theory, as semantical, although it is always related to semantics, as in proving soundness \nand completsner~ (which, incidentally, can be expressed as the equation ~,(Sl,S 2) m [ dl ~ d2 ]. if \none takes ,If to be a relation, and <, to be an inverse satisfaction relationship between sentences and \npossible worlds that satisfy them). In addition, there are a variety of \"independence\" claims that have \narisen in different fields. That ,I, does not uniquely determine 4,, for example, is the \"psychology \nnarrowly construed\" and col~comitant methodological solipsism of Putnam, Fodor, and others [Fodor 19801. \nThat O is usually specifiable compositionally and independently of 4, or is essentially a statement \nof the autonomy thesis for language. Similarly, when 0 cannot be ~pecified indepently of ,I,, computer \nscience will say that a programming language \"cannot be parsed except at runtime\" (Teco and the first \nversions of Smalltalk were of this character). A thorough analysis of these semantic relationships, however, \nand of the relationships among them, is the subject of a different paper. For present purposes we need \nnot take a stand on which of O, q', or has a prior claim on being semantics, but we do need a little \nterminology to make sense of it all. For discussion, we will refer to the \"~\" of a structure as its declaratit~e \nimport, and to its \"q,\" as its procedural J Figure 4: A Fra mework for Computational Semantics consequence. \nIt is also convenient to identify some of the situations when two of the six entities (nt, n2, sl, s2, \nall, and do) are identical. In particular, we will say that sl is self-referential if dl sl, that ,I, \nde-references s! if s2 ffi dr, and that is designatioa.preser~iag (at st) when d t d 2 (as it always \nis, for example, in the ~,-calculus, where t, --a- and #-reduction do not alter the interpretation in \nthe standard model). It is natural to ask what a program is, what programndng language semantics gives \nan account of, and how (this is a related question) and ,Z, relate in the programming language case. \nAn adequate answer to this, however, introduces a maze of complexity that will be considered in future \nwork. To appreciate some of the difficulties, note that there are two different ways in which we can \nconceive of a program, suggesting different semantical analyses. On the one hand, a program can be viewed \nas a linguistic object that de~riboa or signifies a computational process consisting of the data structures \nand activities that result from (or arise during) its execution. In this sense a program is primarily \na communicative object, not so much playing a role within a computational process as existing outside \nthe process and representing it. Putting aside for a moment the question of whom it is meant to communicate \nto, we would simply say that a program is in the domain of O, and, roughly, that ~oO of such an expression \nwould be the computation described. The same characterization would of course apply to a specification; \nindeed, the only salient difference might be that a specification would avoid using non-effective concepts \nin describing behaviour. One would expect specifications to be stated in a declarative language (in the \nsense defined in footnote 4), since specifications aren't themselves to be executed or run, even though \nthey speak about behaviours or computations. Thus, for program or specification b describing computational \nprocess c, we would have (for the relevant language) something like ~(O(b)l -c. If b were a program, \nthere would be an additional constraint that the program somehow play a causal role in engendering the \ncomputational process c that it is taken to describe. There is, however, an alternative conception, that \nplaces the program inside the machine as a causal participant in the bchsviour that results. This view \nis closer to the one implicitly adopted in Figure 1, and it is closer (we claim) to the way in which \na Lisp program must be semantically analysed, especially if we are to understand Lisp's emergent reflective \nproperties. In some ways this different view has a yon Neuman character, in the sense of equating program \nand data. On this view, the more appropriate equation would seem to be \u00a2/(O(b)) --e, since one would \nexpect the .processing of the program to yield the appropriate behaviour. One would seem to have to reconcile \nthis equation with that in the previous paragraph; something it is not clear it is possible to do. But \nthis will require further work. What we can say here is that programming language semantics seems to \nfocus on what, in our terminology, would be an amalgam of q' and @. For our purposes we need only note \nthat we will have to keep q, and strictly separate, while recognising (because of the context relativity \nand nonlocal effects) that the two parts cannot be told independently. Formally, one needs to specify \na general significance function Z, that recursively specifies and together. In particular, given any \nstructure Sl, and any state of the processor and the rest of the field (encoded, say, in an environment, \ncontinuation, and perhaps a store), ~ will specify the structure, configuration, and state that would \nresult (i.e., it will specify the use of st), and also the relationship to the world that Sl signifies. \nFor example, given a I,isp structure o\u00a3 the form (\u00f7 I (PRO~ (SZTQ A 2) A)), X would specify that the \nwhole structure designated the number three, that it would return the numeral 3, and that the machiue \nwould be left in a state in which the binding of the variable A was changed to the numeral z. Before \nleaving semantics completely, it is instructive to apply our various distinctions to traditional Lisp. \nWe said above that all interaction with computational processes is mediated by cmnmunication; this can \nbe stated in this terminology by noting that O and O \"t (we will call the latter e.rternalisation) are \na part of any interaction. Thus Lisp's \"read- eval-print\" loop is mirrored in our analysis as an iterated \nversion of O'1o*oO (i.e., if nj is an expression you type at Lisp, then n 2 is o'l(*(O(nll))). The Lisp \nstructural field, as it happens, has an extremely simple compositional structure, based on a binary directed \ngraph of atomic elements called cons-cells, extended with atoms, numerals, and so forth. The linguistic \nor communicative expressions that we use to represent Lisp programs --the formal language objects that \nwe edit with our editors and print in books and on terminal screens --is a separate lexicai (or sometimes \ngraphical) object, with its own syntax (of parentheses and identifiers in the lexical case; or boxes \nand arrows in the graphical). There is in Lisp a relatively close correspondence between expressions \nand structures; it is one-to-one in the graphical case, but the standard lexical notation is both ambiguous \n(because of shared tails) and incomplete (because of its inability to represent cyclical structures). \nThe correspondence need not have been as close as it is; the process of converting from external syntax \nor notation to internal structure could involve arbitrary amounts of computation, as evidenced by read \nmacros and other syntactic or notational devices. But the important point is that it is structural field \nelements, not notations, over which most Lisp operations are defined. If you type (RPLACA '(A e I 'el, \nfor example, the processor will change the . CAR of a field structure; it will not back up your terminal \nand erase the eleventh character of your im~ut exvreseion. Similarly, Lisp atoms are field element% not \nto be confused with their lexical representations (called P.names). Again, quoted forms like (QUOTE AOC) \ndesignate structural field elements, not input strings. The form (QUOrE ...), in other words, is a structural \nquotation operator; notational quotation is different, usually notated with string quotes ('ABe'). 5 \n4. Evaluation Considered Harmful The claim that all three relationships (O, ~, and ,v) figure crucially \nin :m account of Lisp is not a formal one. It makes an empirical claim on the minds of programmers, and \ncannot be settled by pointing to any current them'ies or implementations. Nonetheless, it is unarguable \nthat l,isp's numerals designate numbers, and that the atoms T and NIL (at least in predicative contexts) \ndesignate truth and falsity --no one could learn Lisp \"l~ree \"lhrce Tmthl x ,b \u00a2,1,Intctnal Structures \n~ ... cdgc of the machinc ,: ExternalWorld Figure 6: LISP's \"De-reference If You Call\" Evalunlion Protocol \nwithout learning Lhis fact. Similarly, (EQ 'A '8) designates falsity. Furthermore, the structure (CAR \n'(A . nil designates the atom A; this is manifested by the fact that people, in describing Lisp, use \nexpressions such as \"i\u00a3 the C^lt of the list is I At~nOA, tl~cn it's a procedure\", where the term \"the \nCAR of the list\" is used as an English referring expression, not as a quoted fragment of Lisp (and English, \nor natural language generally, is by definition the locus of what designation is). (ouorE A), or 'A, \nis another way of designating the atom A; that's just what quotation is. Finally, we can take atoms like \nCAR and \u00f7 to designate the obvious functions. What, then, is the relationship hetween the declarative \nimport (,I,) of Lisp structures and their procedural consequence (,v)? Inspection of the data given in \nFigure 5 shows that Lisp obeys the following constraint (more must be said about * in those cases for \nwhich ~(*(s)) = ,P(s), since the identity function would satisfy this equation): VS E ,S'[ if [~P(SlC \nS] then [\u00a2/(S) = 4b(S) ] (1) else ['~(\u00a2/(S)) = 4)(S)I] All Lisps, including Scheme [Steele and Sussman \n1978a], in other words, dereference any structure whose designation is another structure, but will return \na co-designating structure for any whose designation is outside of the machine (Figure 6). Whereas evaluation \nis often thought to correspond to the semantic interpretation function q,, in other words, and therefore \nto have type EXeRESSIONS -~ VALUES,evaluation in Lisp is often a designation-preserving operation. In \nfact no computer can evaluate a structure like (~ 2 3), if that means returning the designation, any \nmore than it can evaluate the name Ilesperus or peanut b,tter. Obeying equation (t) is highly anomolous. \nIt means that even if one knows what Y is, and knows X evaluates to Y, one still doesn't know what X \ndesignates. It licences such semantic anomalies as (\u00f7 I 'z), which will evaluate to 3 in all extant Lisps. \nInformally, we will say that Lisp's evaluater crosses semantical levels, and therefore oh~ures the difference \nbetween simplification and designation. Given that processors cannot always de-reference (since the co-domain \nis limited to the structural field), it serous they should always simplify, and therefore obey the following \nconstraint (diagrammed in Figure 7): VS E S [,b(*(s)) : ,P(S) A NOIINAL-FORM(~P(S))] (2) The content \nof this equation clearly depends entirely on the contentof the predicale'NonHAL-rOaN (if ~ORH^L-rOnN \nwere kx. true then * could be the identity function). In the k-calculus, the ~ / normal form Falsityl \na th nction Figure 5: LISP Evaluation vs. Designation: Some Examples Figure 7: A Normalisation Protocol \nReduction [ vo: valut~ Dos l~l.li Application Figure 8: Appliceaion vs. Reduction notion of normal-formedness \nis defined in terms of the processing protocols (~- and p-reduction), but we cannot use that definition \nhere, on threat of circularity. Instead, we say that a structure is in normal iorm if and only if it \nsatisfies the following three independent conditions: 1. It is context-independent, in the sense of having \nthe same declarative (,I,) and procedural (,1,) import independent of the context of use; 2. It is side-effect-free, \nimplying that the processing of the structure will have no effect on the structural field, processor \nstate, or external world; and 3. It is stable, meaning that it must normalise to itself in all  contexts, \nso that * will be idempotent. We would then have to prove, given a language specification, that equation \n(2) is satisfied. Two notes. First, I won't use the terms 'evaluate' or 'value' for expressions or structures, \nreferring instead to normalisation for *, and designrttion for \u00a2. I will sometimes call the result of \nnormulising a structure its result or what it retur~ts. There is also a problem with the terms 'apply' \nand 'application'; in standard Lisps, APPLY is a function from structures and arguments onto values, \nbut its use, like \"evaluate', is rife with use/mention confusions. As illustrated in Figure 8, we will \nuse 'apply' for mathematical function application --i.e., to refer to a relationship between a function, \nsome arguments, and the value of the function applied to those arguments ---and the term 'reduce' to \nrelate the three expressions that designate functions, arguments, and values, respectively. Note that \nI still use the term 'value' (as for example in the previous sentence), but only to name that entity \nonto which a function maps its arguments. Second, the idea of a normalising processor depends on the \nidea that symbolic structures have a semantic significance prior to. and independent at: the way in which \nthey are treated by the processor. Witlmut this assumption we could not even ask about the semantic character \nof the Lisp (or any other) processor, let alone suggest a cleaner version. Without such an assumption, \nmore generally, one cannot say that a given processor is correct, or coherent, or incoherent; it is merely \nwhat it is. Given one account of what it does (like an implementation), one c~n compare that to another \naccount (like a specification). One can also prove that it has certain properties, such as that it always \nterminates, or uses resources in certain ways. One can prove properties of programs written in the language \nit runs (from a specification of the ALGOL processor, for example, one might prove that a particular \nt)rogram sorted its input). However none of these questions deal with the fundamental question about \nthe semantical nature of the processor itself. We are not looking for a way in which to say that the \nsemantics of (CA~ '(a . s)) is A because that is how the language is defined; rather, we want to say \nthat the language was defined that way because A is what (CAR '(^ . 8)) designates. Semantics, in other \nwords, can be a tool with which to judge systems, not merely a method of describing them. 5. 2-Lisp: \nA Semantically Rationaliscd Dialect Since we lmve torn apart the notion of ewduation into two constituent \nnotions, we must start at the beginning and build Lisp over again. 2-Lisp is a proposed result. Some \nsummary comments can be made. First, I have reconstructed what I call the category structure of Lisp, \nrequiring that the categories into which Lisp structures are sorted, for various purposes, line up (giving \nthe dialect a property called category alignment). More specifically, Lisp expressions are sorted into \ncategories by notation, by structure (atoms, cons pairs, numerals), by procedural treatment (the \"dispatch\" \ninside EVAL), and by declarative semantics (the type of object designated). Traditionally, as illustrated \nin Figure 9, these categories are not aligned; lists, a derived structure type, include some of the pairs \nand one atom (Nzt); the procedural regimen treats some pairs (those with LAMSDA in the CAR) in one way, \nmost atoms (except T and ~It) in another, and so forth. In 2-Lisp we require the notational, structural, \nprocedural, and semantic categories to correspond one-to-one, as shown in Figure l0 (this is a bit of \nan oversimplification, since atoms and pairs --representing arbitrary variables and arbitrary function \napplication structures or redexes --can designate entities of any semantic type). A summary of 2-Lisp \nis given in Figure 11, but some comments can be made here. Like most mathematical and logical languages, \n2-Lisp is almost entirely declaratively extensional. Thus (+ 1 z), which is an abbreviation for (+ . \n[t 2]), designates the value of the application of the function designated by the atom + to the sequence \nof numbers designated by the rail fl 2]. In other words (+ I z) designates the number three, of whici~ \nthe numeral 3 is the normal-form designator; (\u00f7 1 2) therefore normelises to the numeral 3, as expected. \n2-Lisp is also usually call-by-value (what one can think of as \"procedurally extensional\"), in the sense \nthat procedures by and large normalise their arguments. Thus, (+ ! (BLOCK (PnZNT \"hello')Z) will normalise \nto 3, printing 'hello \u00b0 in the process. Many properties of Lisp that must normally be posited in an ad \nhoc way fall out directly from our analysis. For example, one must normally state explicitly that some \natoms, such as v and NZL and the numerals, are self-evaluating; in 2-Lisp, the fact that the boolean \nconstants are self-normalising follows directly from the fact that they are normal form designators. \nSimilarly, closures are a natural category, and distinguishable from the functions they designate (there \nis ambiguity, in Scheme, as to whether the value of + is a function or a closure). Finally, because of \nthe category alignment, if x designates a sequence of the first three numbers (i.e., it is bound to the \nrail [z 3]), then (+ . x) will designate five and normalise to 5; no metatbeoretic m.'zchinery is needed \nfor this \"uncurrying\" operation (in regular Lisp one must use (APPLY '+ X); in Scheme, (aPPLY \u00f7 X)). \n'['here are numerous properties of 2-Lisp that we will ignore in this paper. The dialect is defined (in \n[Smith 82]) to izmlude side-effects, inte||sional procedures (that do not uot~nalise their arguments), \nand a variety of other sometimes- shunned properties, in part to show that our semantic reconstruction \nis compatible with the full gamut of features found in real programming languages. Reeursion is handled \nwith explicit fixed-point operators. 2-Lisp is an eminently ' usable dialect (it subsumes Scheme but \nis nmre powerful, in part because of the met^structural access to closures), although it is ruthlessly \nsemantically strict. 6. Self-Reference in 2.l,isp We turn now to matters of ~elf-reference. Traditional \nI,isps provide names U=V^L and APPLY) for the primitive proce&#38;~or procedures; the 2-Lisp analogues \nare UORHALZSF and n[DUCE. Ignoring for a moment context argument~ such as environments and continuations, \n(I~OR~ALISE '(\u00f7 Z 3) ) designates the normal-form structure to which\" (\u00f7 z 3) normaliscs, and therefore \nreturns the handle '5. Similarly, I.exical Structural Ih'ocedural I)cclarative Lexical I)cr. Str. Proccdural \nDeclarative Numbers I . A I\" or NIL H T.Values lruth Values I ,,o.r.,s Funct ions I ... [ Labels ~ Atoms \n.... flails Sequences [ loot tea P. LiStS ~..-~(quote :.) ~ Sexprs-J' :~ ~or,.al Eorm Structures [ I \n\"L!st\" \"~\" N~. Lists ,~Se~uence, L~mcric s ~s._J~-~____A_A tom s '1 Appl'ns \" I ( ^1 .-^z )' [--t___pal~_~__J \nI Pairs Jl Figure 9: The Categol:y Structure of LISP 1.5 Figure I0: The Category Structure of 2-LISP \nand 3-LISP Figure I 1: A n Overview of 2-Li~p We begin with the objects. Ignoring input/output categories \nsuch as characters, strings, and streams, there are seven 2-Lisp structure types, as illustrated in Table \n1. The numerals (notated as usual) and the two boolean constants (notated 'ST' and '$f') are unique (i.e., \ncanonical), atomic, normal-form designators of numbers and truth-values, respectively. Rails (notated \n'[A~ Az ... AA]') designate sequences; they resemble standard Lisp lists, but we distinguish them from \npairs in order to avoid category confusion, and give them their own name, in order to avoid confusion \nwith sequences (or vectors or tuples), which are normally taken to be platonic ideals. All atoms are \nused as variables (i.e., as context-dependent names); as a consequence, no atom is normal-form, and no \natom will ever be returned as the result of processing a structure (although a designator of it may be). \nPairs (sometimes also called redexes, and notated '(A~ . Az)') designate the value of the function designated \nby the CAR applied to the arguments designated by the CDR. By taking the notational form '{A~ Az ... \nA~)' to abbreviate '(A 1 . I:Az Aa ... Akl)' instead of '(A~ . (Az . ( ... (A~ .NIL)...)))', we preserve \nthe standard look of Lisp programs, without sacrificing category alignment. (Note that in 2-Lisp there \nis no distinguished atom NIL, and *()' is a notational error ~ corresponding to no structural field element.) \nClosures (notated '(CLOSURE: ... }') are normal-form function designators, but they are not canonical, \nsince it is not generally decidable whether two structures designate the same function. Finally, handles \nare unique normal-form designators of all structures; they are notated with a leading single quote mark \n (thus \"'A' notates the handle of the atom notated 'A', \"(A . St' notates the handle of the pair notated \n'(A . s)', etc.). Because designation and simplification are orthogonal, quotation is a structural \nprimitive, not a special procedure (although a QUOTE procedure is easy to define in 3-Lisp). We turn \nnext to the functions (and use '~' to mean 'normalises to'). There are the usual arithmetic primitives \n(+, -, .and /). Identity (signified with =) is computable over the fall semantic domain except functions; \nthus (- 3 (+ I z)) =* ST, but (= + (LAMOOA [X] (+ X X)))will generate a processing error, even though \nit designates truth. The traditionally unmotivated difference between E0 and EOUAL turns out to be an \nexpected difference in granularity between the identity of mathematical sequences and their syntactic \ndesignators; thus: (= It 2 3] [-1 z 3]) =, Sr (= '[I Z 3] '[1 2 3]) =~ $F (= (z z 3] '[I z 3]) =~ $F \n (In the last case one structure designates a sequence and one a rail.) IST and REST are the CAR/CDR \nanalogues on sequences and rails; thus, (tat It0 20 30]) ~ t0; (REST El0 20 30~]) ~ r20 30]. CAR and \nCaR are defined over pairs; thus (CAR '(a . S)) ~ 'A (because it designates A), and (COR '(+ 1 2)) = \n'[1 z]. The pair constructor is called PC0NS (thus (PCONS 'A 'a) ~ ' (A . a)); the corresponding constructors \nfor atoms, rails, and closures are called AEONS, aeONS, and CC0NS. There are 11 primitive characteristic \npredicates, 7 for the internal structural types (AlOM, PAll|, RAIl., i;OOLEAN, NUMERAL, CLOSURE, and \nIIAFJDLE)and 4 fo~ the external types (NUMBER, TRurtI-VALUE, SEOUENCE, and FuNcrIo~J). Thus: (NUMOER3) \n~ $T (NUMERAL '3) =~ ST (NUMBER '3) ~ Sf (FUNCTION +l ==> ST (FUBCTION '*) =-~ Sf Procedurally intensional \nIF and CONO are defined as usual; BLOCK (as in Scheme) is like standard Lisp's PROGN. BODY, PATTERN, \nand fNVta0NMENT are the three selector functions on closures. Finally, functions are usually \"defined\" \n(i.e., conveniently designated in a contextually relative way) with structures of the tbrm (LAM8OA SIMPLE \nAReS BOOY) (the keyword SIMPLE will be explained presently); thus (LAMBDA SIMPLe IX] (+ X Xll returns \na closure that designates a function that doubles numbers; ((LAblBflA SIMPLE IX] (+ X X)) 4) ~ 8, 2-Lisp \nis higher order, and therefore lexically seeped, like the X-calculus and Scheme. However, as mentioned \nearlier and; illustrated with the handles in the previous paragraph, it is also metastructural, providing \nan explicit ability to name internal structures. Two primitive procedures, called uP and DOWN (usually \nnotated with the arrows %' and \"C) help to mediate this metastructural hierarchy (there is otherwise \nno way to add or remove quotes; ~z will normalise to \"2 forever, never TO z). Specifically, tSTAVC designates \nthe normal~form designator of the designation of SrRUC; i.e., tSreUC designates what STRUC normalises \nto (therefore t(+ z 3)~ 's). Thus: (LAMBDA SIMPLE IX] X) designates a function, ' (LAMaDA S I MPLE [ \nX ] X) designates a pair or redex, and t(LAMODA SIMPLE [xJ x) designates a closure. (Note that 't' is \ncall-by-value but not declaratively extensional.) Similarly, ~sTeuc designates the designation of the \ndesignation of STROC, providing the designation of STRUC is in normal-form (therefore *'2 ==* z). ~,*STRUCis \nalways equivalent to SrRoc, in terms of both designation and result; so is t~.srRvC when it is defined. \nThus if 00URLE is bound to (the result of normalising) (I^MBO^ IX] (* x x)), then (BODY OOURLE) generates \nan error, since BODYis extensional and DOUBLEdesignates a function, but (RODe tDOUrJLE) will designate \nthe pair (+ x x). Type Designq/ion Norm,d Canonical Notation Numerals Numbers Yes Yes --digits Booleans \nTruth-Values Yes Yes --STor SF Handles Structures Yes Yes --' STRUC Closures Functions Yes No CC0NS (closure} \nRails Sequences Some No RC0NS [STRUC... srRv~ Atoms (,~ of Binding) No --AC0NS alphamerics Pairs (ValueofApp.) \nNo --PCONS (STRUC. STRUC Table 1: The 2-LISP(and 3-LISP) Categories ===~ ~ ..o Figure 12: Meta-Circtdar \nProcessors .! (NORgAL[SE '(CAR '(A . B))) ~ ''A (NORNALISE (PCONS '= '[2 3])) =~ '$1 r (REDUCE 'IST '[~10 \n20 30]) =*, '10. More generally, the basic idea is that ~(NOIIMALISE) ~, tO be contrasted with o(~,), \nwhich is approximately o, except that because ,t is a partial function we have @(~, o NORHALISE) = ~. \nGiven these equations, the behaviour illustrated in the foregoing examples is forced by general semantical \nconsiderations. In any computational formalism able to model its own syntox and ~structures, 6 it is \npossible to construct what are commonly known as metacircular interpreters, which we call ,lelacireular \nprocessors (or MCPs) ~ \"meta\" because they operate on (and therefore terms within them designate) other \nformal structures, ~nd \"circular\" because they do not constitute a definition of the processor. They \nare circular for two reasons. First, they have to he run by that processor in order to yield any sort \nof behaviour (since they are programs, not processors, strictly). Second, the behaviour they would thereby \nengender can be known only if one knows beforehand what the processor does. (Standard techniques of fixed \npoints, furthermore, are of no help in discharging this circularity, because this kind of modelling is \na kind of ~lf-mention, whereas reeursive definitions are more ~lf-use.) Nonetheless, such processors \nare pedagogically illuminating, and play a critical role in the development of procedural reflection. \nThe role of MCPs is illustrated in Figure 12, showing how, if we ever replace P in Figure 1 with a process \nthat results from P processing the metacircular processor MCP, it would ~till correctly engender the \nbehaviour of any overall program. Taking processes to be functions from structures onto behaviour (whatever \nbehaviour is --['unctions from initial to final states, say), and calling the primitive processor P, \nwe should be able to prove that. P(MCP) = P, where by '=\" we mean behaviourally equivalent in some appropriate \nsense. The equivalence is, of course, a global equivalence; by and large the primitive processor and \nthe processor resulting from the explicit running of the MCP cannot be arbitrarily mixed. If a variable \nis bound by the underlying processor P, it will not be able to be looked up by the metacircular code, \nfor example, Similarly, if the metacircular processor encounters :: control-structure primitive, such \nas a Till'tOW or a 0nil, it wid not cause the metacircular processor itself to exit prematurely, otto \nterminate. The point, rather, is that if an entire computation is run by the process that results from \nthe explicit prece.~qing of the MCP by P, the results will be tbe same (modulo time) as if that entire \ncomputation had been carried out directly by P. MCPs are not causally connected with the systems they \nmodel. The reason that we cannot mix code for the underlying processor and cede for the MCI ) and the \nrea~a that we ignored context arguments in the definitions above both have to do with the state of the \nprocessor P, In very simple systems (unordered rewrite rule systems, for example, and hardware architectures \n).hat put even the program counter into a memory location), the processor has no internal state, in the \nsense that it is in an identical configuration at every \"click point\" during the running of a program \n(i.e., all information is recorded explicitly in the structural field). But in more complex circumstances, \nthere is always a certain amount of state t~) the processor that affects its behaviour with respect to \nany particular embedded fragment of code. In writing an MCP one must demonstrate, more or less explicitly, \nhow the proce.~qor state affects the processing of object-level structures. By \"more or less explicitly\" \nwe mean that the designer of the MCP has options: the state can be represented in explicit structures \nthat are passed around as arguments within the processor, or it can be absorbed into the state of the \nprocessor running the MCP. (I will say that a property or feature of an object language is obsorbed in \na metalanguage or theory ju:;t in case the mctatbeory uses the very same property to explain or describe \nthe property of the object language. Thus conjunction is absorbed in standard model theories of first-order \nlogics, because the semantics of p A 0 is explained simply by conjoining the explanation of P and 0 --specifically, \nin such a fornmla as: 'P A 0' is true just in case 'P' is true and '0' is true.) The state of a processor \nfor a recursively-embedded functional language, of which Lisp is an example, is typically represented \nin an environment and a continuation, both in MCPs and in the standard metatheoretic accounts. (Note \nthat these are notions that arise in the theory of Lisp, net in Lisp itself; except in self-referential \nor self-modelling dialects, user programs don't traffic in such entities.) Most MCPs make the environment \nexplicit. The control port of the state, Imwever, encoded in a continuation, must also be made explicit \nin order to explain non-standard control operations, but in many MCPs (such as in [McCarthy 1965] and \nSteele and Sussman's versions for Scheme (see for example [Sussman and Steele 1978b}), it is absorbed. \nTwo versions of the 2-Lisp metacircular processor, one absorbing and one making explicit the continuation \nstructure, are presented in Figures 13 and 14. Note, however, that in both cases the underlying agency \nor a#ima is not reified; it remains entirely absorbed by the processor of the MCP. We have no mechanism \nto designate a process (as opposed to structures), and no method of obtaining causal access to an independent \nlocus of active agency (the reason, of course, being that we have no theory of what a process is). 7. \nProcedural Reflcctlon and 3-Lisp Given the met~tcircular processors defined above, 3-I,isp can be non-cffectively \ndefined in a series of steps. First, imagine a dialect of 2-[,isp, called 2-l,isp/1, where user progr'xms \nwere not run directly by the primitive processor, but by that proces~r running a copy of an MCP. Next, \nimagine 2-Lisp/2, in which the MCP in turn was not run by the primitive processor, but was run by the \nprimitive processor running another copy of the MCP. Etc. 3-Lisp is essentially 2-Lisp/Do, except that \nthe MCP is changed in a critical way in order to provide the proper connection between levels. 3-Li..,p. \nin ether words, is what we call a reflective lower, defined ad an infinite number of Ct)l)ies of an MCP-like \nprogram, run at the \"top\" by an (infinitely fleet) processor. The claim that 3-Lisp is well-founded is \nthe claim that the limit exists, as n-.oo, of 2-Lisp/n. We will look at the revised MCP presently, but \nsome general properties of this tower architecture can he pointed out first. A rough idea of the levels \nof processing is given in Figure 15: at each level the processor code is processed by an active process \nthat interacts with it (locally and serially, as usual), but each processor is in turn composed of a \nstructural field fragment in turn processed by a reflective processor on top of it. The implied infinite \nregress is not problematic, and the architecture can be efficiently realised, since only a finite amount \nof information is encoded in all but a finite number of the bottom levels, There are two ways to think \nabout reflection. On the one hand, one can think of there being a primitive and noticeable reflective \nact, which causes the i)rocessor to shilZ levels rather markedly (this is the explanation that best coheres \nwith some of our pre-theoretic intuitions about reflective thinking in the sense of contemplation). On \nthe other hand, the explanation 30 (define READ-NORHALISE-PRINT (lambda simple [env stream] (block (prompt&#38;reply \n(normalise (prompt&#38;road stream) env) stream) (road-normalise-prlnt one stream)))) (define NORMALISE \n(lambda simple [str'uc e.v] (rend [(normal struc) struc] [(atom sLruc) (binding sLruc env)] [(rail struc) \n(normaltse-rail struc env)] [(pair struc) (reduce (carstruc)(cdrstruc) env)]))) define REOUCE (lambda \nslmple [proc args env] (let [[proc! (normalise proc env)]] (selectq (procedure-type procl) [simple (let \n[[args! (eormaltse args env)]] (if (primitive procl) (reduce-primitive-simple proc! argsl env) (expand-closure \nprocl argsl)))] [intensional (if (primitive proc!) (reduce-primtttve-lntenslonal proc! targs any) (expand-closure \nprocl targs))] [macro (normalise (expand-closure procl targs) env))])))) (define NORMALISE\u00b0RAIL (lambda \nsimple [rail env] (If (empty rail) (rears) (prep (normalise (lst rail) env) (normaiise-ratl (rest rail) \nonv))))) define EXPAND-CLOSURE (lambda simple [proc! argsl] (normalise (body, procl) (bind (pattern procl) \nargsi (environment procl)))) Figure 13:ANon-C(mtinuation-Passblg 2-LISPMCP given in the previous paragraph \nleads one to think of an infinite number of levels of reflective processors, each implementing the one \nbelow. 7 On such a view it is not coherent either to ask at which level the tower is running, or to ask \nhow many retlective levels are running: in some sense they are all running at once. Exactly the same \nsituation obtains when you use an editor implement, ed in APL. It is not as if the editor and the APL \ninterpreter are both running together, either side-by-side or independently; rather, the one, being interior \nto the other, SUl)plies the anima or agency of /.he outer one. To put this another way, when you implement \none process in another process, you might want to say that you have two different processes, but you \ndon't have concurrency; it is more a part/whole kind of relation. It is just this sense in which the \nhigher levels in our rcllective hierarchy are always running: each of them is in some sense within the \nprocessor at the level below, so that it can thereby engender it. We will not take a principled view \non which account --a single locus of agency stepping between levels, or an infinite hierarchy of simultaneous \nprocessors --is correct, since they turn out to be behaviourally equivalent. (The simultaneous infinite \ntower of levels is often the better way to understand processes, whereas a shi|!,ing-level viewpoint \nis sometimes the better way to understand programs.) 3-Lisp, as we said, is an infinite reflective tower \nbased on 2-Lisp. The cede at each level is like; the continuation-passing 2- Lisp MCP of Figure 14, but \nextended to provide a mechanism whereby the user's program can gain access to fully articulated descriptions \nof that program's operations and structures (thus extended, and located in a reflective tower, we call \nthis code the 3-Lisp reflective processor). One gains this access by using what are called reflective \nprncedures ~ procedures that, when invoked, arc run not at the level at which the invocation define READ-NORNALISE-PRINT \n(lambda simple lone stream] (normailse (prompt&#38;read stream) oily (lambda simple [result] (block (prompt&#38;reply \nresult stream) (read-normalise-print env stream)))))) (define NORHALISE (lambda simple [strc one cent] \n(rend [(normal struc) (cent strc)] [(atom sire) (cent (binding strc env))] [(rail strc) (normaltse-rail \nstrut env cont)] [(pair strc)(reduce (carstrc)(cdcstrc)envcont)])} (define REDUCE (lambda simple [proc \nargs env coat] (normalise proc env (lambda slmpte [proc!] (selectq (procedure-type procl) [simple (normaltse \nargs any (lambda simple [args!] (if (primitive procl) (redece-primtttve-stmple pratt args! env cent) \n(expand-closure proc! args! cost))))] [intensional (if (primitive procl) (reduce-primitive-intenslonal \nproc! targs env cent) (expand-closure procl ~args cont))] [macro (expand-closure pros! targs (lambda \nsimple [result] (normallse result any cont)))])))))) (define NORMALISE-RAIL (lambda simple [rail env \ncent] (if (empty rail) (cent (rcons)) (normalise (lst rail) env (lambda simple [ftrstl] (normalise-rall \n(rest rat1) env (iambda simple [rest!] (cent (prep first! rest!))))))))) define EXPAND-CLOSURE (lambda \nsimple [proc! ergs! cent] (normalise (body procl) (bind (pattern proc!) args! (one procI)) cent))) Figure \n14: A Continaation-Passing 2-LISP MCP Reflective procedures are essentially analogues of subroutines \nb be run \"in tile implementation\", except that they are in the same dialect as that being implemented, \nand can use all the power o(' the implemented language in carrying out their function (e.g., reflective \nprocedures can themselves use reflective procedures, without limit). There is not a tower of different \nlanguages --there is a single dialect (3-Lisp) all the way up.  L ve,,co .J l''l  occurred, but one \nlevel higher, at the level of the reflective Figure 15: The 3-LISP Reflective Tower processor running \nthe program, given as arguments those structures being passed around in the reflective processor. Rather, \nthere is a tower of processors, necessary because there is different processor state at each reflective \nlevel. Some simple examples will illustrate. Reflective procedures are \"defined\" (in the sense we described \nearlier) using the form (LAMBOA REFLECT ARGS BODY), where ARG$ typically the rail fAnGS ENV coNr] --is \na pattern that should match a 3-element designator of, respectively, the argument structure at the point \nof call, the enviromnent, and the continuation. Some simple examples are given in the \"Programming in \n3-Lisp\" overview in Figure 16, including a working definition of Scheme's CATCH. Though simple, these \ndefinitions would be impossible in a traditional language, since they make crucial access to the full \nprocessor state at point of call. Note also that although Tlm0w and CMC, deal explicitly with continuations, \nthe code that uses them need know nothing about such subtleties. More complex routines, such as utilities \nto abort or redefine calls already in process, are almost ns simple. In addition, the reflection mechanism \nis so powerful that many traditional primitives can be defined; C^MBOA, IF, and QUOTE are all non-primitive \n(user) definitions in 3-Lisp, again illustrated in the insert. There is also a simplistic break package, \nto illustrate the use of the reflective machinery for debugging purposes. It is noteworthy that no reflective \nprocedures need be primitive; even LAHBDA can bc built up from scratch. The importance of these examples \ncomes from the fact that they are causally connected in the right way, and will therefore run in the \nsystem in which they defined, rather than being models of another system. And, since reflective procedures \nare fully integrated into the system design (their names are not treated as special keywords), they can \nhe passed around in the normal higher-order way. There is also a sense in which 3-Lisp is simpler than \n2-I,isp, as well as being more powerful; there are fewer primitives, and 3-[,isp provides much more compact \nways of dealing with a variety of intensional issues (like macros). 8. The 3-Lisp Reflective Processor \n3-Lisp can be understood only with a close inspection of the 3-l,isp reflective processor (Figure 17). \nthe promised modification of the continuation-passing 2-Lisp met~lcircular processor mentioned above. \nNOnMALISE(line 7) takes an structure, cnviromnent, and continuation, returning the structure unchanged \n(i.e., sending it to the continuation) if it is in normal lbrm, looking up the binding if it in an atom, \nnormalising the elements if it is a rail (NORMALISE-RAILis 3-I,isp's tail-recursive continuation-passing \nanalogue of Lisp 1.5's EVilS). and otherwise reducing the CAR (procedure) with the CDIt (arguments). \nREOUCE (line 13) first aormalises the procedure, with a continuation (C- I'ROC!) that checks to see whether \nit is reflective (by convention, we use exclamation point suffixes on atom names used as variables to \ndesignate normal form structures). If it is not rellcctive, C.PltOC~ normalises the arguments, with a \ncontinuation that either expands the closure (lines 23-25) if the Figure 16: Programming in 3-Lisp: For \nillustration, we will look at a handful of simple 3-Lisp programs. The first merely coils thc Continuation \nwith the numeral 3; thus it is semantically identical to the simple numeral: (define THREE (lambda reflect \n[[1 env cent] (cent '3))) Thus (three) ~ 3; (+ It (three)) ~ 14. The next example is an intensional \npredicate, true if and only if its argument (which must be a variable) is hound in the current context: \n(define BOUND (lambda rerlect [[var] one cent] (tf (bound-in-env ear one) (cent 'ST) (cent 'Of)))) or \nequivalently (define SOUND (lambda reflect [[var] env cent] (cent t(bound-in-env vat envl})) Thus (LET \n[[X 31] (BOUND X)) ~ St, whereas (Donne x) ~ SF in the global context. The following quits the computation, \nby discarding the continuation and simply \"returning\": (define QUIT (lambda reflect [[] env cont] 'QUIT!)) \nThere are a variety of ways to implement a TtlROW/CATCHpair; the following defines the version used in \nScheme: (define SCHEME-CATCH (lambda reflect [[tag body] catch-ear catch-cent] (normalise body (bind \ntag t(lambda reflect [[answer] throw-env throw-cent] (normal tso answer throw-ear catch-cent)) catch-earl \ncatch-cent))) For example:  (let [ix 111 (+ 2 (scheme-catch punt (* 3 (/ 4 (if (: x I) (punt 15)  (-x \nl))))))) would designate seventeen and return the numeral 17. In addition, the reflection mechanism \nis so powerful that many traditional primitives can be defined; LN4BDA,If, and QUOTE are all non-primitive \n(user) definitions in 3-Lisp, with the following definitions: (define LNdBDA (lambda reflect [[kind \npattern body] env cent] (cent (coons kind tony pattern body))))  (define If (lambda rerlect [[promise \nthen else] env cent] (normal tse premise env (lambda stmple [preml:ol] (normalise (or 4premtse! then \nelse) env cent)l))) (define QUOTE (lambda reflect [[arg] nay cent] (cent targ))) Some comments. First., \nthe definition of tA..OAjust given is of course circular; a non-circular but effective version is given \nin Smith and des Rivi&#38;res [1984]; the one given in the text, if executed in 3-Lisp, would leave the \ndefinition unchanged, except that it is an innocent lie; in real 3-Lisp kind is a procedure that is called \nwith the arguments and environment, allowing the definition of (lambda macro ... ), etc. COONS is a closure \nconstructor that uses SIMPLE and nEFLECT to tag the closures for recognition by the reflective processor \ndescribed in section 6. ZF is an extensional conditional, that normalises all of its arguments: the definition \nof IF defines the standard intensional version that normalises only one of the second two, depending \non the result of normalising the first. Finally, the definition of QUOTE will yield (QUOTE A) ~ 'A. Finally, \nwe have a trivial break package, with ENV and C0Nr bound in the break environment for the user to see, \nand nFivnn bound to a procedure that will normalise its argument and pass that out as the result of the \ncall to SNEAK: (define BREAK (lambda reflect [[arg] env cent] Iblock (print arg primary-stream) (read-normallse-prlnt \n\">>\" (bind' ['env tenv] ['cent trent] ['return t(lambda reflect [[a2] 02 c2] (normaltse a2 e2 cent))] \nenv) pr Imary-stream) ) ) ) If viewed 'as models of control constructs in a language being iinplemented, \nthese definitions will look innocuous; what is important to remember is that they work in the very language \nin which they are defined. i 32 l ..... (define READ-NOnMALISE-PRINT 2 ........... (lambda simple \n[level say stream] 3 ................. (no~miise (prompt&#38;read level stream) env 4 .......................(lambda \nsimple [result] ;ContinuationCRElq,Y 5 ............................ (block (prompt&#38;reply result level \nstream) 6 ............................................... (read-normaltse-prtnt level env s~ream)))))) \n 7 ..... (define NORMALISE 8 ........... (lambda simple [struc env coat] 9 ................. (cond [(normal \nstruc) (cent struc)]  IO ............................ [(atom struc) (cent (binding struc env))] II ............................ \n[(rail struc) (normalise-rsll struc env cont)] 12 ............................ [(pair struc) (reduce \n(car strut) (cdr struc) env cent)i))) 13 ..... (define REDUCE 14 ........... (lambda simple [proc args \near coat] 15 ................. (normsllse proc env 16 ........................ (lsmbda simple [procl] \n;ContinuationC-PROC! [7 .............................. (tr (reflective procl) 18 ....................................... \n(4(de-reflect procl) ar~s env cont~ 19 ...................................... (normaltse args env 20 \n.............................................. (lambde simple [argsl] ;Continuation C-ARGS! 21 ................................................. \n(If (prhntttvo proci) 22 .......................................................... (cent *lCprocl . \n$argsl)} 23 .......................................................... (normsltse Ibody procl) 24 .................................................................................. \n(bind (pattern proc!) args! (environment proc!) 2S .................................................................................. \ncoat))))))))) 26 ..... (define NORMALISE-RAIL 27 ........... (lambda stmple trail env coat] 28 ................. \n(tf (empty rail) 29 ..........................(COOt (teens)) 30 ..........................(normeltso \n(let rat1) env 31 ................................ (lsmbds simple [flrstl] ;ContinuatlonC-FIRST! 32 ....................................... \n(normsltso-rail (rest rail) ear 33 .............................................. (lambde simple [rest]] \n;Continuation C-RESTI 34 .................................................... (cent (prep first] restl))))))))) \n Figure 17: The 3-Lisp Refleclive Processor: procedure is non-primit, ve, or else directly executing \nit if it is primitive (line 22). Consider (REOUCE '+ 'ix 3] ENV IO), for example, where x is be, end \nto the numeral z and + to the primitive addition closure in [NV. At the point of line 22, PaOC! will \ndesignate that primitive closure, and ARG$! will designate the normal-form rail [z 3]. Since addition \nis primitive, we must simply do the addition. (Peoc!. ARGS!)won't work, because PROC! and AflGSl are \nat the wrong level; they designate structures, not functions or arguments. So, for a brief moment, we \nde-reference them (with ~), do the addition, and then regain our meta-structural viewpoint with the ,.8 \nIf the procedure is reflective, however, it is (as shown in line 18 of Figure 17) called directly, not \nprocessed, and given the obvious three arguments (AnGS, [W, and CONI) that are being passed around. The \n\u00a2(o[-nrFLECT PROC:) is merely a mechanism to purify the reflective procedure so that it doesn't reflect \nagain, and to de-reference it to be at the right level (we want to use, not mention, the procedure that \nis designated by PROCO. Note that line 18 is the only place that reflective procedures can ever be called; \nthis is why they must always be prepared to accept exactly those three arguments. Line 18 is the essence \nof 3-Lisp; it alone engenders the full reflective tower, for it says that some parts of the object language \n--the code processed by this program --are called d~rectly in this program. It is as if an object level \nfragment were included directly in the meta language, which raises the question of who is processing \nthe meta language. The 3-Lisp claim is that an exactly equivalent reflective processor can be processing \nthis code, without vicious threat of infinite ascent. A reflective procedurc,.in sum, arrives in the \nmiddle of the processor context. It is handed environment and continuation structure that designat~ the \nprocessing of the code below it, but it is run in a different context, with its own (implicit) environment \nand continuation, which in turn is represented in structures passed around by the processor one level \nabove it. Thu~ it is given causal access to the state of the process that was in progress (answering \none of our initial requirements), and it can of course cause any effect it wants, since it has complete \naccess to all future processing ot that code. Furthermore, it has a safe place to stand, where it will \nnot conflict with the code being run below it. These various protocols illustrate a general point. As \nmentioned at the outset, part of designing an adequate reflective architecture involves a trade-off between \nbeing so connected that one steps all over oneself (as in traditional implementations of debugging utilities), \nand so disconnected (as with metacircular processors) that one has no effective access to what is going \non. The 3-Lisp tower, we are suggesting, provides just the right balance between these two extremes, \nsolving the problem of vantage point as well as of causal connection. The 3-Lisp reflective processor \nunifies three traditionally independent capabilities in Lisp: the explicit availability of EVAL and APPLY, \nthe ability to support metacircular processors, and explicit operations (like Maclisp's RETFUN~nd Interlisp's \nFRETURN) for debugging purposes. It is striking that the latter facilities are required in traditional \ndialects, in spite of the presence of the former, especially since they depend crucially on implementation \ndetails, violating portability and other natural aesthetics. In 3-Lisp, in contrast, all information \nabout the state of the processor is fully available within the language. 9. The Threat of Infinity, and \na Finite Implementation The argument as to why 3-Lisp is finite is complex in detail, but simple in outline \nand in substance. Basically, one shows that the reflective processor is fully tail-recursive, in two \nsenses: a) it runs programs tail-recursively, in that it does not build up records of state for programs \nacross procedure calls (only on argument passing), and b) it itself is fully tail-recursive, in the sense \nthat all recursive calls within it (except for unimportant subroutines) occur in tail-recursive position. \nThe reflective processor, can be executed by a simple finite state machine. In particular, it can run \nitself without using any state at all. Once the limiting behaviour of an infinite tower of copies of \nthis processor is determined, therefore, that entire chain of processors can be simulated by another \nstate machine, of complexity only moderately greater than that of the reflective processor itself. (It \nis an interesting open research question whether that \"implementing\" processor can be algorithmically \nderived from the reflective processor code.) A full copy of such an implementing processor --about 50 \nlines of 2-Lisp --is provided in {Smith and des Rivi~res 1984J\" a more substantive discussion of tractability \nwill appear in [Smith forthcoming]. 10. Conclt~slons and Morals Fundamentally, the use of Lisp as a language \nin which to explore semantics and reflection is of no great consequence; the ideas shouhi hold in any \nsimilar circumstance. We chose Lisp because it is familiar, because it has rudimentary self-referential \ncapabilities, and because there is a standard procedural self-theory (continuation-passing metacircular \n\"interpreters\"). Work has begun, however, on designing reflective dialects of a side-effect-free Lisp \nand of Prolog, and on studying a reflective version of the X-calculus (the last being an obvious candidate \nfor a mathematical study of reflection). Furthermore, the technique we used in defining 3-Lisp can be \ngeneralised rather directly to these other languages. In order to construct a reflective dialect one \nneeds a) to formulate a theory of the language analogous to the metacircular processor descriptions we \nhave examined, b) to embed this theory within the language, and c)to connect the theory with the underlying \nlanguage in a causally connected way, as we did in line 18 of the reflective processor, by providing \nreflective procedures invoeable in the object language but run in the processor. It remains, of course, \nto implement the resulting infinite tower; a discussion of general techniques is presented in [desRivi~res, \nforthcoming]. It is partly a consequence of using Lisp that we have used non-data-abstracted representations \nof functions and environments; this facilitates side-effects to processor structures without introducing \nunfamiliar machinery. It is clear that enviromnenta could be readily abstracted, although it would remain \nopen to decide what modifying operations would be supported (changing bindings is one, but one might \nwish to excise bindings completely, splice new ones in, etc.). In standard X-calculus-based metatheory \nthere are no side effects (and no notion of processing); environment designators must therefore be passed \naround (\"threaded\") in order to model environment side effects. It should be simple to define a side-effect-free \nversion of 3-Lisp with an environment-threading reflective processor, and then to define s~rQ and other \nsuch routines as reflective procedures. Similarly, we assume in 3-Lisp that the main structural field \nis simply visible from all code; one could define an alternative dialect in which the field, too, was \nthreaded through the processor as an explicit argument, as in standard metatheory. The representation \nof procedures as closm'es is troublesome (indeed, closures are failures, in the sense that they encode \nfar more information than would be required to identify a function in intension; the problem being that \nwe don't yet know what a function in intension might be.). 3-Lisp unarguably provides far too fine-grained \n(i.e., metastructural) access to function designators, including continuations, and the like. Given an \nabstract notion of procedure, it would be natural to define a reflective dialect that used abstract structures \nto encode procedures, and then to define reflective access in such terms. We did not follow this direction \nhere only to avoid taking on another very difficult problem, but we will move in this direction in future \nwork. These considerations all illustrate a general point: in designing a reflective processor, one can \nchoose to bring into view more or less of the state of the underlying process. It is all a question of \nwhat you want to make explicit, and what you want to absorb. 3-Lisp, as currently defined, reifies the \nenvironment and continuation, making explicit what was implicit one level below. It absorbs the structural \nfield (and lmrtly absorbs the global enviromnent); as mentioned earlier, it completely absorbs the animating \nagency of the whole computation. If one defines a reflective procSssor based on a metacircular processor \nthat al.~o absorbs the representation of control (i.e., like the MCP in Figure 13, which uses the control \nstructure of the processor to encode the control structure of the code being processed), then reflective \nprocedures couhl not affect the control structure, In any real application, it would need to be determined \njust what parts of the underlying dialect required reification. One could perhaps provide a dialect in \nwhich a reflective procedure could specify, with respect to a very general theory, what aspects it wanted \nto get explicit access to. Then operations, for example, that needed only environment access, like 9ouNo, \ncould avoid having to traffic in continuations. A final point. I have talked throughout about semantics, \nbut have presented no mathematical semantical accounts of any of these dialects. To do so for 2-Lisp \nis relatively straightforward (see Smith [forthcoming J), but I have not yet worked out the appropriate \nsemantical equations to describe 3- Lisp. It would be simple to model such equations on the implementation \nmentioned in section 9, but to do so would be a failure: rather, one should instead take the definition \nof 3-Lisp in terms of the infinite virtual tower (i.e., take the limit of 2- Lisp/n), and then prove \nthat the implementation strategies of section 9 are correct. This awaits further work. In addition, I \nwant to explore what it would be to deal explicitly, in the semantical account, with the anima or agency, \nand with the questions of causal connection, that are so crucial to the success of any reflective architecture. \nThese various tasks will require an even more radical reformulation of semantics than has been considered \nhere. Acknowledgements I have benefited greatly from the collaboration of Jim des Rivi~res on these questions, \nparticularly with regard to issues of effective implementation. The research was conducted in the Cognitive \nand Instructional Sciences Group at Xerox PARC, as part of the Situated Language Program of Stanford's \nCenter for the Study of Language and Information. Notes 1. See ]Doyle 1980], ]Wcyrauch 1980], [Genesereth \nand Lenat 1980], and {Batali 1983]. 2. In the dialects we consider, the metastructural capability must \nbe provided by primitive quotation mechanisms, as opposed to merely by being able to model or designate \nsyntax -- something virtually any calculus can do, using Godel numbering, for exomple --for reasons \nof causal connection. 3. Most programming languages, such as Fortran and Algol 60, are neither higher-order \nnor metastructura]; the ~,-calculus is the first but not the second, whereas Lisp 1.5 is the second but \nnot the first (dynamic .seeping is n contextual protocol that, coupled with the mete-structural facilities, \npartially allows Lisp 1.5 to compensate for the fact that it is only first- order). At least soma incarnations \nof Scheme, on the other hand, are beth (although Scheme's metastructural imwers are limited). As we will \nsee, 2- Lisp and 3-Lisp are very definitely both metastructural and higher-order. 4. For what we might \ncall declarative languages, there is n natural account of the relationship between linguistic expressions \nand in.the-world designations that need not make crucial.reference to issues of processing (to which \nwe wiU turn in a moment). It is for such languages, in particular, that the composition ~PoO, which we \nmight call ep,, would be formulated. And this, for obvious reasons, is what is typically studied in mathematical \nmodel theory and logic, since those fields do not deal in any crucial way with the active use of the \nlanguages they ~tudy. Thus, for example, 4J' in logic would be the interpretation function of standard \nmodel theory. In what we will call cnmpototionol languages, on the other hand, questions of processing \ndo arise. 5. The string '10tmTE Ae\u00a2]' notates a structure that designates another structure that in \nturn could be notated with the string \"ABe'. The string '\"ABC\"', on the other hand, notates a structure \nthat designates the string 'ABe' directly. 6. Virtually any language, of course, has the requisite power \nto do this kind of modelling. In a language with mete-structural ahilities, the mete-circular processor \ncan represent programs for the MCP as themsolsee --  this is always done in Lisp MCPs --but we need \nnot define that to be an essential property. The term 'metocircular processor\" is by no means strictly \ndefined, and there arc various constraints that one might or might not put on it. My general approach \nhas been to view as metacircular any non.causally connected model of a calculus within itself; thus the \n3-Lisp reflective processor is nut mete-circular, because it does have the requisite caused connl,ction~, \nand therelbrc an essential Imrt of the 3-Lisp architecture. 7. Curiously, there are also intuition~ about \nconlemplative thinking, where one is both detoched and yet directly present, that fit more with this \nview. 8. One way to undcr~tand thi~ is tn realize that the reflective processor simply asks its processor \nto do any primiHves that it encounters. I.e., it passes responsibility up to the processor running it. \nIn other words, each time one level uses a primitive, its proceg~or runs around setting everything up, \nfinally re~whing the point at which it must simply do the primitive action, whereup~n it asks its own \nprocessor for help. Bul of course the processor runnin~ that processor will else come racing towards \nthe edge of the same  cliff, and will similarly duck responsibility, handing the primitive up yet anolher \nlevel. In fact every primitive ever ex,~cutcd is handed all the way to the tap of the tower. There is \na magic moment, when the thing actually happ~ms, and then the answer filters all the way back down to \nthe level that stortt.d tile whole procedure. It is as if tile deus ex mrwhina, living at the tap of \nthe tower, sends a lightning bolt down to some level or other, once every intervening level gets appropria~x~ly \nlined up (rather like the sun, at the stonehenge and pyramids, reaching down through a long tunnel at \njust one particular moment during the year). Except, of course, that nothing ever h[Ippens, ultimately, \nexcept primitives. In other words tile enabling agency, which must flow down from the top of the tower, \nconsists of an infinitely dense series of these lightning bolts, with something like 10% of the ones \nthat reach each level being allowed through to the level below. All infinitely fast. References Batali, \nJ., \"Computational Introspection\", M.I.T. Artificial lntenigeneo Laboratory Memo AIM-TI{-701 (1983). \ndt, sll.ivi~,r~,s, J. '\"l'he Implenl~ntotlon of Procedurally Reflective Languages\", (forthcoming). Doyk,, \nJ., A Model [or I)elihr,rali~m, Action, and Introspection, M.I.T. Artificial Intelligence L~boratory \nMemo AIMJI'R-581 (1980). Fodor, J. \"MethodologicM Solipsism Considered as a Research Stralegy in Cognitive \nPsychology\", The Beh~wiour~d end Brian Sciences, 3:1 (1980) pp. 63-73; reprinted in Fodor, J., Iteprcsent~ttlons, \nCambridge: Bradford (1981). Gmresereth, M., and l,cnat, D. I:|., \"Self-Description end -Modification \nin a Knowledge Representation I,anguage\", |leuristic Programming Project Report IIPI'-B0-10, Stanford \nUniversity CS Dept., (1980), McCarthy, J. at el,, Lh~P 1.5 Progrummcr'$ Munuul, Cambridge, Mass.: The \nMIT PRess (1965). Smith, B., Reflection and ,~emaltlics in a Prueetlural Language, M.I.T. latboratory \nfor Computer Science Report MIT-TR-272 (1982). Smith, B. and desRivi~'res, J. \"Interim 3-LISP Reference \nManual ~, Xerox PARC Report CIS-nn, Pale Alto (1984. forthcoming). Steele, G., \"I,AMI|DA: q'i~e Ultimate \nDeclaralive\", M.I.T. Artificial Intelligence Laboratory Memo AIM.a79 (1976). Steele, G., and Sussman, \nG. \"The Revised Report an SCHEME, a Dialect of IJSI >'', M.I.'I ~. Artificial Intelligence Imboratory \nMemo AIM-452, (1978a). Steele, G., and Sussman, G. \"The Art of the Interpreter, or, The Modularity Complex \n(Parts Zero, One, and Two)\", M.I.T. Artificial Intelligence Imboratory Memo AIM-.153, (1978b). Wcyhrauch, \nR. W., \"Prolegomena to a Theory of Mechanized Formal Reasoning\", Artificial I~:lrlligem'e 13:1,2 (1980) \npp. 133-170. 35  \n\t\t\t", "proc_id": "800017", "abstract": "", "authors": [{"name": "Brian Cantwell Smith", "author_profile_id": "81100079547", "affiliation": "XEROX Pale Alto Research Center, 3333 Coyote Hill Road, Pale Alto, CA and Center for the Study of language and Information, Stantbrd University, Stanford, CA", "person_id": "PP14038393", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/800017.800513", "year": "1984", "article_id": "800513", "conference": "POPL", "title": "Reflection and semantics in LISP", "url": "http://dl.acm.org/citation.cfm?id=800513"}