{"article_publication_date": "01-15-1984", "fulltext": "\n Efficient Applicative Data Typest Eugene W. Myers Department of Computer Science The University of Arizona \nTucson, Arizona 8~721 1. Introduction Applicative programming has long been advo- cated on theoretical \ngrounds because the formal pro- perties of such programs are simple and elegant. Recently, there has \nbeen a trend to use the applica- tive approach in software development tools [3] and programming languages \n[2,7]. Unfortunately, the requirement that operations be free of side-effects makes it difficult to achieve \nefficient implementa- tions [7]. To date, there are only two published algo- rithms [5,8], which treat \nthe applicative manipulation of queues and stacks. This work presents O (IgN) time and space algo- rithms \nfor the applicative manipulation of linear lists. A generalization of an AVL tree, called an AVL dag, \nis used. While the result is simple, its consequences are far reaching. Since almost every non-scalar \ndata type can be modeled with lists, the results presented here are a powerful method for improving the \nimple- mentations of applicative languages. The results also provide a fast and space efficient method \nfor con- structing history systems such as editors with unlim- ited \"undos\" and version control systems. \nFinally, lists can be realized in value-semantic programming languages, such as PASCAL, with worst case \nperfor- mance superior to any previously proposed solution. Space is at a premium in history systems. \nAn enhancement that improves absolute space perfor- mance by a factor of two for applicative editor opera- \ntions (e.g. move, transfer, etc.) is also presented. The O (1) time and space results in [5], suggest \nthe possi- bility of yet more efficient applicative algorithms for 1\"This work was supported by the National \nScience Foundation under Grant MCS-8210096. Permission to copy without fee all or part of this material \nis grant- ed provided that the copies are not made or distributed for direct commercial advantage, the \nACM copyright notice and title of the publication and its date appear, and notice is given that copying \nis by permission of the Association for Computing Machinery. To copy otherwise, or to republish, requires \nn fee and/or specific per- mission. the simpler sub-abstractions of list. Algorithms are presented for \narrays that perform in O (KN ~/K ) time and O (K) space where K may be chosen arbitrarily. 2. Motivation \nIn many applications it is desirable to view a list data structure as an instance of a \"linear list\". \nFor example, programming languages manipulate \"integers\", \"arrays\", \"strings\", etc. Formally, we assume \nthat a value-semantic linear list data abstrac- tion consists of: an arbitrary and time varying number \nof objects of type linear list; a fixed finite cob lection of operators that access and manipulate objects; \nand a time varying set of variables, each of which refers to an object. Variables can be created, destroyed, \nand their reference relation can be modi- fied by assignment. A variable denotes the value of the object \nto which it refers. Only assignment can change the value of a variable. The state of the data abstraction \nis the collection of objects referred to by the current set of variables. A typical linear list operation \nrepertoire [6] consists of: (1) LEN(L):Integer Determine the number of elements in list L. (2) SEL(L,k):X \nSelect the ld h element of list L. (3) RNK(L,x):Integer Determine the rank of x in an ordered list L. \n (4) ADD(L,k,x):List_of_X Insert x after the k th element of list L. (5) DEL(L,k):List_of_X Delete the \nk th element of list L. (6) CON(Li ,l.,z):List_of-X Concatenate L I and L 2. (7) SUB(L,ij):List_of_X \nSelect the i th through jt~ elements of L.  The primitives have been formulated as functions. The constraint \nthat only an assignment can change the value of a variable forces an implementation in which an operator \nmay not modify the values con- &#38;#169; 1983 ACM 0-89791-125-3/84/001/0066 $00.75 66 tained in its \noperands, i.e. it must operate applica-tively. Such implementations are applicative data types. Height-balanced \nmodels such as AVL trees [I,6] are frequently used to model linear lists because of their \"smooth\" O(IgN) \nworst-case behavior. They are, however, unsuitable in a value-semantic frame- work because of their procedural \nformulation. For example, the concatenation algorithm is usually for- mulated as a procedure, CA TENATE(LI,L2,L3), \nwhich places the concatenation of L 2 and L 3 in L I. The procedure doesn't consume space, but it destroys \nits operands L 2 and L 3 through rebalancing and join operations. The presumed origin of this semantic \nchoice was the assumption that the preservation of the operand trees would require that copies be made \nat an intolerable cost of O(N) time and space. As shown in the next section, this assumption is false. \n3. AVL Dags The observation that led to the concept of an AVL dag and its applicative algorithms is illustrated \nby an example. Consider the AVL tree at the left in Figure I. Suppose that the vertex labeled X is to \nbe added as the right child of vertex 5. The procedure- oriented AVL algorithm modifies vertices 4, 5, \nand 6 to produce the result shown at the upper right. The list represented by their ancestor, 3, is also \nindirectly affected. Let A be the set of vertices directly modi- fied by an AVL operation. Let A* be \nthe set consist- ing of A and every ancestor of a vertex in A. It fol- lows that A* is exactly the collection \nof vertices, v, whose list values are modified by the operation. Thus, the result of the operation can \nbe represented (while still preserving the original tree) by using copies of just the vertices in z~*. \nIn the example, this leads to the dag shown at the lower right of Figure I. Note that in Figure !, A* \nis exactly the set of ver- tices on the search path from the root to the point of insertion. For other \nAVL operations, A* more gen- erally depends on the balances of search path vertices and their children \nand grandchildren. Nonetheless, the cardinality of A must be O(IgN) for any AVL algorithm because it \noperates in O(IgN) time. Moreover, 4\" is O(IgN) because AVL algorithms modify a tree along just one or \ntwo search paths. The tree resulting from an AVL operation is to be represented augmentatively and consequently \nit may refer to unaltered vertices in its operand trees. Thus the representation for the state of an \nAVI. data type is a dag with the following special properties. ,,;; 5 :7 ,;;' k', FIGURE 1: THE CENTRAL \nIDEA (a) The dag is a binary dag. There is a special vertex A that has out-degree 0. Every other vertex \nhas out-degree two and its two children are further ordered into \"left\" and \"right\".  (b) Let the height \nof a vertex, v, in the dag be the length of the longest path from v to A. The dag satisfies the height-balance \nproperty: the heights of the left and right children of every vertex (except A) differ by at most I. \n  A dag satisfying these properties is an A VL dag. Fig- ure 2 gives an example. q ...... ~_ _ _ ~L,.~.~.,,Ib_ALCLO,,A.,_C>_/~\"~ \n--)_ ~%~ V ........... _f  1 VAt.'~~ ...... .~--S o. V--........ AVL DAG AVL TREE(uNaRA]DED COUNTERPART) \nFIGURE 2: AVL DAG EXA/tl~ An AVL dag encodes a collection of linear lists as follows. As for AVL trees, \neach vertex v (except A) is labeled with a list element. The value of v, Val(v), is 67 the list of labels \nencountered in the symmetric and unmarked traversal of the subdag dominated by v. Equivalently, one can \nimagine \"unbraiding\" this sub- dag by replicating every vertex with in-degree greater than one until \na tree rooted at v is obtained. This tree is an AVL tree and its symmetric label list is Val(v). Thus \nevery vertex in the AVL dag represents an AVL tree whose symmetric order list is the linear list being \nmodeled. It then follows [I] that the length of every path from v to A is O(IgN)where N =lVal(v)~. 4. \nApplicative AVL Algorithms An AVL dag models the state of the list data type. Each variable refers to \na dag vertex v; the list denoted by the variable is Val(v). The list operations require new vertices \nand cause current state vertices to become unused, i.e. the vertex cannot be reached from any vertex \nreferenced by a variable. Thus a storage management scheme that allocates and gar- bage collects vertices \nis needed. An incremental reference counter method [10] provides the O(!) allocation and collection algorithms \n--NEW, INC, DEC. References to vertex v are created and des- troyed with INC(v)and DEC(v). NEW(I,x,r)gen-erates \nan initial reference to a new vertex with label x and left and right sons i and r. The efficiency of \nthe method in [10] permits the on-line cost of storage management to be included in subsequent complex- \nity claims (see Appendix A). Each AVL list space vertex is modeled by the record: Type vertex = Record \nRC: integer (* Reference Count *) LN,H : integer (* Length and Height *) L,R : )vertex (* Left and Right \nSiblings *) V : X (* Label (List Element) *) End Type list = )vertex The LN field contains IVal(v)~+! \nand is used in searching for a list element. The fields LN and H are maintained by the list space primitive \nNEW. For simplicity, the INC and DEC calls needed to main- tain reference counts will be ignored. The \napplicative nature of AVL dags suggests that a recursive and functional \"bottom-up\" approach be used \nin the exposition of the algorithms. It has long been recognized that the functional style of program- \nming leads to terse and conceptually clear algo-rithms.t Consider the operation ADD(v,k,x) which 1\" The \nentire AVL dag implemented list abstraction (in- cluding storage management) was written in 193 lines \nof C.  inserts x after the k th element of Val(v). For the moment, ignore the constraint that the result \nbe height-balanced. If v --A then Val(ADD(v,k,x)) = <x> and thus NEW(A,x,A) generates the desired reference. \nProceeding inductively, if v ~ A and k < v.L.LN then Val(ADD(v.L,k,x)) --Val(A DD(v,k,x))[ i..v.L. LN] \nimplying that NEW(ADD(v.L,k,x),v.V,v.R) generates the correct refereence. Similarly, when k _> v. L. \nLN, the desired reference is generated by NEW(v.L,v.V,ADD(v.R,k--v.L.LN,x)). These facts lead to the \nalgorithm: Function ADD(v: list; k: integer; x: base) : list I. If v= AThen 2. ADD --NEW(A,x,A) 3. Else \nIf k < v.L.LN Then  4. ADD --NEW(ADD(v.L,k,x),v.V,v.R)  5. Else  6. ADD ~ NEW(v.L,v.V,ADD(v.R,k-v.L.LN,x)) \n  The recursion descends along a search path to A and produces its result by adding new vertices to \nthe AVL dag as it proceeds back up this path. While applicatively producing the correct list value, \nADD does not retain the height balance pro- perty because the heights of search path vertices may be \nincremented producing a number of locally unbal- anced sights. These local imbalances are rectified by \na central utility algorithm, BAL(I,x,r), which per- forms the applicative equivalent of the single and \ndouble rotations used in conventional AVL algo- rithms. The function BAL(l,x,r) has the same effect \nas the primitive NEW (i.e. returns Vai(l),<x>*Val(r)), but in the event that I.H-r.H ~ [-2,2], BAL guarantees \na height-balanced result. Function BAL(I: list: x: base: r: list) : list I. lfI.H-r.H E [-!,1] Then 2. \nBAL --NEW(I,x,r) 3. Else If I.H > r.H Then  4. If I.L.H > I.R.H Then  5. BAL -'- NEW(I.L,I.V,NEW(I.R,x,r)) \n 6. Else  7. BAL \"- NEW(NEW(I.L,I.V,I.R.L), I.R.V,N EW(I.R. R,x,r))  8. Else  9. If r.R.H > r.L.H \nThen  I0. BAL \"-- NEW(NEW(I,x,r.L).r.V,r.R) I I, Else 12. BAL \"-- NEW(NEW(I,x,r.L.L),r.L.V, NEW(r.L.R,r,V,r.R)) \n OPERATOR TIME Z~:ORDER ~:ABSOLUTE L~:EXPECTED SEL(v,K) O(LG N) 0 0 0 RNK(v,x) O(LG N) 0 0 0 ADD(v,K,x) \nO(LG N) O(LG N) s v.H+I .995 LG N + 1.23 DEL(v,K) O(LG N) O(LG N) ~(v.H-I) .998 LG N - .275 CON(L,R)* \nO(LG M) O(LG M) +\u00bd(R.H-1)_<L.H .966 LG M + .520 SUB(v,I,J) O(LG N) O(LG R) s 3v.H-5 1.94 LG R - .170 \n WHERE N = IVAL(v) I M = IVAL(L)I R = J-I * WLOG ASSUME L.H z R.II TABLE I: AVL DAG PERFORMANCE SUMMARY \nBAL is O(l) and it adds a maximum of three new vertices to the dag. To maintain the height-balance property \nin the algorithm ADD above, simply use use BAL instead of NEW. The algorithms for deletion, concatenation, \nand substring selection are similarly obtained by formu- lating the applicative equivalents of their \nprocedural AVL tree counterparts (see Appendix A). The selec- tion and rank primitives are elementary \nbinary search algorithms. By analogy with the classic AVL algorithms, these AVL dag algorithms require \nO(IgN) time. Moreover, it follows that BAL is called at most O (IgN) times and thus A\" is O (igN) for \nevery operator. A more detailed analysis yields the performance bounds presented in Table I. The issue \nof space performance is new to AVL dag algo- rithms. Table 1 includes tight upper bounds for the absolute \nvalue of A\" and its experimently deter- mined expected value. The derivations of the tight upper bounds \nare given in [9]. The expected perfor- mance statistics were obtained by running a linear regression \non the average value of A\" for a geometric series (5 to 2134 in steps of 1.4) of 19 parameter values. \nEach average was obtained by running 500 experiments on randomly constructed AVL trees of the required \nparameter size. The augmentative approach employed here is not specific to AVL trees, but can be applied \nto any tree- based method yielding new data structures such as B dags, 2-3 dags, heap dags, and dynamic \nbinary search dags. The corresponding applicative algorithms have the same time performance as their \nprocedural coun- terparts; the space performance of a transforma-tional primitive (e.g. ADD vs. SEL) \nis the same as its time performance. 5. Applications A. A Value Semantic List Data Abstraction The immediate \nconsequence of AVL dags is a superior implementation of list algorithms in a value-semantic framework. \nCorollary I: Let A be a list algorithm whose com- plexity is O(F) when list operations are assumed to \nbe atomic (i.e. O(l)). Further suppose that A uses 1 elements of input and constant data and that A out- \nputs O elements of output. Using the AVL dag algo- rithms, the real-time complexity of algorithm A is \nO(FlgL+l+O) where L is the average length of a list operand. Any program must spend time proportional \nto the size of its input and output. O(N) algorithms to build and write AVL dag encoded lists of length \nN can be easily constructed. Using AVL dags all opera- tions require O (lgN) time and space. Variables \ncan be introduced and removed with O(l) INC and DEC operations. Of paramount importance is the fact that \nassignment is also O (I): DECrement the current variable reference and INCrement a new one to the desired \nvalue. The most pathological possibility for the parame- ter L is represented by the algorithm that reads \na list of length I and then doubles its length in the remain- ing F-! steps. That is, L can be I'I(/2F). \nThus, in terms of F, l, and O, the AVL dag implementation guarantees that a list algorithm's real time \ncomplex- ity is O (F(F +lgl)+l +0). The best performance of previous applicative list algorithms is a \nreal time complexity of O (FL +1 +0 ) or O ( FI 2 F +I +0 ). Let the size of the current state S be the \nsum of the lengths of every variable's value. The correct maintenance of reference counts implies that \nthe 69 number of vertices in the AVL dag is less than S. Moreover, the size of the dag may be as small \nas IgS. Thus the method is asymptotically space optimal. In practice, it may be more space efficient \nthan co0ven- tional methods if the incidence of sharing is high enough to compensate for the incumbent \noverhead of the AVL dag model (-20 bytes per vertex). The fundamental improvement expressed in Corollary \n1 immediately leads to a plethora of new results for a variety of specific data abstractions. An applicative \nordered list data type is obtained by employing the primitive RNK. As a result, the method provides applicative \nO(IgN) implementa-tions of the numerous sub-abstractions of list or ordered list, such as arrays, tables \nand any complex structure encoded as lists. In practical terms, the method applies to structures such \nas text files (a list of strings), program states (an array of words), and relational data bases (a finite \nset of ordered lists of ordered pairs). B. Maintaining Histories A history tree of an object is a rooted \noriented tree in which each vertex denotes a value (called a version) of the object. The original version \nof the object is denoted by the root. The version of every other vertex is assumed to have been obtained \nby applying a transformational operator (called an update) to the version of its parent. The application \nof a passive operation (i.e. one that does not change the value of the object) is called a query. The \nprob- lem is to maintain a history tree that permits efficient on-line algorithms for: adding a new leaf \nversion through the application of an update to an existing version; deleting leaf versions; and querying \nan arbi- trary existing version. Any applicative implementa- tion solves this problem since the invocation \nof an update is guaranteed not to affect the version being operated upon. Thus, the AVL dag method provides \nthe corollary: Corollary 2: A history tree of lists can be maintained in O(igN) time per query or update \nand O(IgN) space per update, where N is the size of the version upon which the operator is applied. Versions \ncan be deleted in O (1) time. The generality and efficiency of the AVL dag method is illustrated by contrasting \nit with the earlier work of [4]. Their history problem constrains the history tree to be a line-graph \nand their list abstrac- tion only permits the operations ADD, DEL, RNK, and SEL (i.e. a table). Their \nupdate and query algo- rithms require O(ig z N) time. In [4], these algo- rithms were applied to a number \nof geometric inter- section problems. The use of AVL dags immediately gives better results and holds \npromise for other prob- lems in computational geometry. Practical and efficient history systems can \nbe con- structed with AVL dag history trees. For example, an editor with complete history maintains a \nhistory line-graph of versions where each version is a text file; an update operation is a line-oriented \neditor command that changes the text; and a query opera- tion is a passive editor command on a version \nin the current history. A text file is a list of lines and is implemented as an AVL dag. Each line-oriented \nedi- tor command can be implemented as a finite compo- sition of the AVL dag algorithms. This history \neditor performs updates in O(IgN) time and space and queries in O(IgN) time where N is the number of \nlines in the text file. Another class of history systems currently of practical interest are version \ncontrol systems. For example, a source code control system maintains a history tree of versions where \neach version is a source code text file; an update operation is the action of an editing session on a \nversion; and a query operation searches or accesses an arbitrary version of the source code. Modeling \ntext file as above, an update operation is realized by using the history editor above to compute the \nAVL dag representing the final text file of the editing session. This requires O(ElgN ) time and O (min \n(N ,AIgN )) space where E is the number of edit commands in the session; N is th~ maximum number of lines \nin the text file; and A is the number of lines changed by the session. Query operations can access K \ncontiguous lines of an N line version in 0 ( K +lgN ) time. 6. Further Results A. Saving Space in Applicative \nEditor Operations The line-oriented editor of the last section mani- pulates applicative \"text files\". \nA typical operator repertoire consists of: (I) LEN(T):Integer Return the number of lines in text file \nT. (2) FIND(T,a):Line Return the a th line of T. (3) SCROLL(T,a,~) Starting with the a th line, pass \nsuccessive lines ofT to the \"handler\" procedure until it returns a halt signal. (4) REPLACE(T,a,s):Text_File \nReplace the a th line of T with line s. (5) DELETE(T,a,b):Text_File Delete lines a through b of T. \n(6) INSERT(T,c, ~):Text_File Insert the sequence of lines returned by the procedure immediately after \nline c of T.  70 (7) MOVE(T,a,b,c):Text_File Move lines a through b oft immediately after line c. (8) \nTRANS FE R(T,a,b,U,c):Text_File Place a copy of lines a through b of T immediately after line c of U. \n While these operations can be expressed in terms of the list operations given in Section 2 (e.g. DELETE(T,a,b) \n= CON(SU B(T,I,a-I), SUB(T,b+I,LEN(T))) ), a closer examination of the underlying mechanism reveals an \napproach that uses half as much space. Although this improvement is only by a constant factor, it is \nof great practical value because it doubles the size of histories that can be maintained. The basic action \nused to achieve the list functions CON and SUB is embodied in the applicative func- tion JO1N(l,x,r), \nwhich has the same effect as BAL (i.e. returns Val(l)*<x>*Val(r) ), but is correct regardless of the \nheights of I and r. The procedural version of JOIN is treated in [6] where x is called the juncture value \n(see also Appendix A). The applica- tive version of JOIN adds a maximum ofll.H-r.H]+l vertices to the \nAVL dag. Consider the operator DELETE. The portion of the tree to be retained is represented by the height- \nmonotone slices depicted in Figure 3. The desired result is obtained by successively joining the subtrees \nof the slices together with their interspersed juncture values. When formulated as above, DELETE first \njoins the subtrees of the left slice together in height increasing order. DELETE then joins the subtrees \nof the right slice. A final application of JOIN on the results of the proceeding two steps produces the \ndesired list. The sum of the space bounds of each application of JOIN telescopes to give a total worst- \ncase bound of 2H\u00f7V vertices, where H is the height of the heighest subtree in either slice and V is the \nnumber of subtrees in both slices. Since concatenation is associative, any order of joins is correct. \nThe improvement is obtained by using a better \"join order\" than the one above. Specifically, the subtrees \nin both slices are simultane- ously joined in order of increasing height. This merged joining is analogous \nto the sorting technique of merging two ordered lists. With this order, only H\u00f7V vertices are added in \nthe worst case. For DELETE, the height profile of the slices form a \"V\". For other operations the height \nprofiles are more complex and merging opposing slices is more subtle. For example, the operator TRANSFER \nrequires the joining of four slices as depicted in Figure 4. For this \"W':profile the optimum join procedure \nis as follows: (1) in a T i ..o\u00b0 .. .. \u00b0 .. ..\u00b0..o \".. LEFT SLICE RIGHTSLICE FIGURE ~: DELETE(T,A,I) \nSLICES  merged fashion join the slice R a,h and the portion of slice U,+~,** whose subtrees have height \nless than lea.H; (2) as in (1) join the lower portion of Uo. , and all of La. h ; (3) join the results \nof (1) and (2) with juncture value Ica. V; and (4) in a merged fashion join the upper portions of Uo, \n, and U,+j.** using the result of (3) to \"seed\" the merge. With this join order, at most H+L+V vertices \nare used where V is the number of subtrees in all the slices, H is the height of the heighest subtree, \nand L is the height of the vertex h'a. U \" ......... , ,* ~(!T[A~'~ ~ \"\u00b0\" .   ,,\"LCA'\", 'L~U[[B!~ \nUo,c LA,r, RA,m Uc+l,- FIGURE q. TRANSFER (T,A,s,U,c) SLICES  The number of subtrees in any slice is \nless than the difference in height between the lowest and highest subtrees of the slice. Thus V < 2H \nin the analysis of DELETE This implies an upper bound of 3H vertices when joins are merged and 4H vertices \notherwise. However, these bounds are somewhat coarse as the space efficiency of the JOIN operations is \nnot independent of the parameter V. As V OPERATOR FIND(v,A) SCROLL(v,A,P) REPLACE(v,A,S) DELETE(v,A,B) \nINSERT(v,c,p) TRANSFER(w,A,B,V,C) MOVE(v,A,B,C) TIME /~:ORDER O(LG N) 0 O(LG N + K) 0 g(LG N) g(LG N) \nO(LG N) O(LG (N-R)) g(LG N + K) g(LG N + K) g(LG N + LG M) g(LG N + LG R) B(LG N) g(LG N)  ~':EXPECTED \n --'(UNOPTIMIZED) 0 0 .99 LG N + .28 1.89 LG (N-R) -.99 1.85 LG N + K -.89  ~Q:EXPECTED --(OPTIMIZED) \n0 0 .95 LG N -.28 1.04 LG (N-R) + .28 1.00 LG N + K + .23  1.83 (LG N + LG R) - .47 1.07 (LG N + LG \nR) + 1.11 1.95 (LGN +LG R$)- 1.19 1.13 (LGN+ LG RS) + .41 WHERE N = IVAL(v) I M = IVAL(w) I R =IB-AI \nS =Ic-BJ K = No. OF LINES PROCESSED BY P, ~: APPLICATIVE EDITOR PERFORMANCE SUMMARY approaches 2H, the \nheight differences between suc- cessive subtrees in each slice (and their merged sequence) become smaller \nuntil almost every such difference is 0 or I. JOIN(I,x,r) requires ]I.H-r.HI+I vertices only when ]l. \nH-r.H] >2; otherwise it uses just ]l.H-r.H] vertices. Thus as V approaches 2H, the joins become more \nspace efficient. It can be shown that at most 2H vertices are used when joins are merged and 3H otherwise. \nThese bounds are tight and imply that merging joins improves space perfor- mance by ! / 3 in the worst \ncase. Table lI lists the asymptotic performance bounds for the applicative editor operators and shows \nthe results of experiments designed to determine the expected-case space usage of both optimized and \nunoptimized operators. The experiments reveal that merging improves space utilization by roughly 45% \nin the expected case. A prototype editor using these primitives has been built. Its speed is comparable \nto that of \"ed\" under UNIX. With a megabyte of memory, a history of !,750 to 3,500 versions of a 10,000 \nline text file.can be maintained. B. Applicative Arrays An array is a fixed length list with the operator \nrepertoire: (1) SEL(A,k):X Select the k th element of array A. (2) ASN(A,k,x):Array_of_X Assign x to \nthe k th element of array A.  While arrays could be modeled applicatively by encoding them as lists, \na more space efficient scheme is desirable as the length of arrays are frequently very large. (For example, \nconsider a history system that maintains versions of a program's state modeled as an array of words.) \nFirst consider modeling an array as a reference to a singly-linked list in which each cell encodes an \nele- ment of the array as an <index,value> pair. ASN(A,k,x) is easy: push a cell containing the pair \n<k,x> onto the front of the linked list referenced by A. This is applicative (the original linked list \nis not modified) and requires only O(l) time and space. SEL(A,k) is achieved by searching A for the first \ncell containing index k and returning the associated value. This requires time proportional to the length \nof the linked list which can, unfortunately, be made arbitrarily large through the repeated use of ASN \noperations. The next refinement guarantees a worst-case time of O (N) for SEL where N is the size of \nthe array. Let each cell further contain an auxiliary <index,value> pair with the following properties. \nThe auxiliary index of a cell is one more (modulo N+l) then the auxiliary index of its successor in the \nlinked list. The auxiliary value of a cell v is SEL(v,a) where a is the auxiliary index of v. From these \npro- perties it follows that SEL(A,k) can be achieved by returning the value associated with the first \nauxiliary or regular index that matches k. Moreover, at most N cells must be searched before a cell with \nauxiliary index equal to k is found. The one drawback is that now ASN must engage in an O(N) SEL in order \nto establish the auxiliary value of the cell it pushes onto the linked list. Thus this second approach \nrequires O (i) space and O (N) time for both operators. 72 B. That is, let A[i] = B[il][iz].  [iK] where \nill2  i K is the K-digit representation of i in the radix-W number system. Now view B as a W-element \narray of (K-l)-dimensional arrays, each of which is a W-element array of (K-2)-dimensional arrays, and \nso on. Applicatively model each of the W-element arrays using the simple method above. Observe that: \nFinally, consider modeling an N-element array 3. Broy, M. and Pepper, P. \"'Combining Algebraic A[0..N-I] \nwith a K-dimensional array and Algorithmic Reasoning: An Approach to B[0..W-I]\" \"'[0..W-I] and for simplicity \nassume the Schorr-Waite Algorithm.\" A CM Trans. on N =W r. Let the i th element of A correspond to the \nProg. l.anguages and ,~vstems 4, 3 (1982), 362- i th element in the lexicographical order of elements \nin 381. SEL(A,i) = B K where Bj = SEL(Bj_ I,iJ) for J = I to K and B 0 = B Thus a selection into the \nN-element array requires K selections into the W-element arrays for a total of 0 (KN I/x ) time. Further \nobserve that: ASN(A,i,x) = C I where Cj = ASN(Bj_I,ij,Cj+I) for J = I to K and CK+ I =x Thus applicatively \nassigning an element in the N-element array requires K applicative assignments and selections into the \nW-element arrays for O (KN j/A\" ) time and O(K) space. Observe that K can be chosen arbitrarily. Space \nconsumption is constant when K is fixed at a small integer, e.g. K =3 gives O(N I/3) time and O(I) space. \nLogarithmic search time and less than 0 (lgN) space are simultaneously attained by choos- ing K=/gN/ig \nlgN (giving O(/g 2N / Ig IgN) time). When K =igN performance coincides with that of the list algorithms. \nAcknowledgements The author would like to thank Chris Fraser, Gary Levin, and David Hanson for their \nmany helpful suggestions. References I. Adel'son-Vel'skii, G.M. and Landis, E.M. \"An Algorithm for \nthe Organization of Informa-tion.\" Dokl. Akad. Nauk SSSR 146 (1962), 263-266 (Russian). English translation \nin Soviet Math. Dokl. 3 (1962), 1259-1262. 2. Backus, J. \"Can Programming Be Liberated from the yon Neumann \nStyle? A Functional Style and Its Algebra of Programs.\" Comm. ACM21, 8 (1978), 613-641. 4. I)obkin, l).P. \nand Munro, J.i. \"Efficient Uses of the Past.\" Proc. 21st A CM Syrup. on Foun- dations of Computer Science \n(1980), 200-206. 5. Hood, R. and Melville, R. \"Real-Time Queue Operations in Pure LISP.\" Inform. Process. \nLett. 13, 2 ( 198 I), 50-54.  6. Knuth, D.E. The Art of Computer Program- ruing (Vol. 3) Sorting and \nSearching. Addison-Wesley, Reading, M ass. (1973), 451-468. 7. Morris, J.H., Schmidt, E. and Wadler, \nP. \"Experience with an Applicative String Process- ing Language.\" Proc. /h ACM Symp. on the Print'. of \nProg. Languages (1980), 32-46. 8. Myers, E.W. \"An Applicative Random-Access Stack\" lnfi~rm. Process. \nLett. (to appear). 9. Myers, E.W. \"AVL Dags\" TR 82-9, Dept. of Computer Science, U. of Arizona, Tucson, \nAZ (1982). 10. Weizenbaum, J. \"Symmetric List Processor.\" Comm..4 CM 6, 9 (1963), 524-536.   Appendix \nA: The List Algorithms This appendix contains a complete specification of the applicative list algorithms \ndiscussed in Sec- tions 2 through 4. The following conventions insure that reference counts are correctly \nmaintained: (i) every function returns a new (INCremented) refer-ence to its result, and (2) every reference \npassed as an argument to a function is consumed (DECremented) by the function. In what follows, the abbreviation \n\"@<list>\" denotes the expression INC(<list>). The list data abstraction is assumed to be imple- mented \nin a work space consisting of some suffi-ciently large, say DAGMAX, array of vertex records. A special \nrecord is set aside to model A. its H-field is 0, its LN-field is i, and its RC-fieid is initially 2*I)AGMAX-I. \nAt the outset, every other record is in a free list with its RC-field set to 0 and its L- and R-fields \nreferencing A. The work space is manipulated through the prim- itives INC, DEC, and NEW. The absence \nof cyclic substructures guarantees that a reference counter strategy suffices to detect all unused vertices. \nIf a vertex v becomes free in a call to DEC, it is added to the free list. The processing of v's internal \nreferences does not proceed at this time but is deferred until v is reallocated in a call to NEW. At \nthe time of realloca- tion, v's offspring are collected but the processing of their internal references \nis again deferred. This stra- tegy yields the following O (i) on-line implementa-tions of NEW, INC, and \nDEC: Function INC(v: list) : list I. v.RC --v.RC+i 2. INC --v Procedure DEC(v: list) !. v.RC --v.RC-I \n2. If v.RC = 0 Then Push v onto the free list Function NEW(I: list; x: base; r: list) : list Var v: list \n!. If free list is empty Then 2. Abort :\"Overflow\" 3. Else 4. Pop v from the free list 5. DEC(v.L) \n 6. DEC(v.R) 7. NEW --INC(v) 8. v.V ~x 9. v.L ~1 10. v.R ~ r I1. v.H --max(I.H,r.H)+l  12. v.LN \n~ v.L.LN + v.R.LN The functions BAL(I,x,r) and JOIN(I,x,r) per-form the fundamental operation of concatenating \nAVL subtrees (dags) in a manner that preserves the height-balance property. Both return a list whose \nvalue is Vai(l)*<x>,Val(r). BAL produces a height- balanced result whenever ]i.H-r.H] < 2. JOIN uses \nBAL to produce a height-balanced result regardless of the heights of I and r. Function BAL(I: list; x: \nbase; r: list) : list i. If I.H-r.H E [-I,I] Then 2. BAL ~ NEW(@l,x,@r) 3. Else if I.H > r.H Then 4. \nif I.L.H _> i.R.H Then 5. BAL ~ NEW(@I.L,I.V,NEW(@I.R,x,@r)) 6. Else 7. BAI. --NEW(NEW(@I.L,I.V,@I.R.L), \n I.R. V, N E W(@I. R. R,x,@r)) 8. Else 9. If r.R.H _> r.L.H Then 10. BAL ~ NEW(NEW(@I,x,@r.L),r.V,@r.R) \n I I. Else 12. BAL --NEW(NEW(@I,x,@r.L.L),r.L.V, N EW(@r.L. R,r.V,@r.R)) 13. DEC(I,r)  Function JOIN(i: \nlist; x: base; r: list) : list 2. if I.H-r.H ~ [-2..2] Then 2. JOIN --BAL(@I,x,@r) 3. Else if r.H > \nI.H Then 4. JOIN --BAL(JOIN(@I,x,@r.L),r.V,@r.R) 5. Else 6. JOIN --BAL(@I.L,I.V,JOIN(@I.R,x,@r)) \n7. DEC(I,r)  The remaining algorithms constitute the opera- tors of the applicative list data type. \nThey are simply applicative adaptations of their procedural counter- parts. However, note that the change \nin perspective has permitted these algorithms, considered difficult by many, to be tersely and clearly \nexpressed. Function SEL(v: list; k: integer) : base 1. if k < v.L.LN Then 2. SEL --SEL(@v.L,k) 3. Else \nif k > v.L.LN Then 4. SEL --SEL(@v.R,k-v.L.LN) 5. Else 6. SEL ,-- v.V 7. DEC(v)  Function RNK(v: \nlist; x: base) : integer I. Ifv=AThen 2. RNK ~ 0 3. Else if x < v.V Then 4. RNK ~ RNK(@v.L,x) 5. Else \n 6. RNK ~ v.L.LN + RNK(@v.R,x) 7. DEC(v)  Function ADD(v: list; k: integer; x: base) : list !. ifv=AThen \n2. ADD --BAL(@A,x,@A) 3. Else if k < v.L.LN Then  4. ADD ~ BAL(ADD(@v.L,k,x),v.V,@v.R) 5. Else 6. \nADD --BAL(@v.L,v.V,ADD(@v.R,k-v.L.LN,x)) 7. DEC(v)  Function DEL(v: list; k: integer) : list I. Ifv.L=Aandv.R=AThen \n2. DEL --@A 3. Else If k < v.L.LN and v.L # A Then 4. if k = v.L.LN Then 5. DEL --BAL(DEL(@v.L,k-i), \nSEL(@v.L,k-i),@v.R) 6. Else 7. DEL --BAL(DEL(@v.L,k),v.V,@v.R)  74 8. Else 9. if k = v.L.LN Then \n I0. DEL --BAL(@v.L,SEL(@v.R,I), DE L(@v. R, I )) II. Else 12. DEL ~ BAL(@v.L,v.V,DEL(@v.R,k-v.L.LN)) \n13. DEC(v) Function CON(I,r: list) : list I. lfI.H <r.H Then 2. CON ----JOIN(DEL(@I,I.LN-I), SEL(@I,I.LN-I),@r) \n 3. Else 4. CON .- JOIN(@I,SEL(@r,i),DEL(@r,I)) 5. DEC(I,r)  Function SUB(v: list; i,j: integer) : \nlist Function Left(v: list; k: integer) : list I. lfv=AThen 2. Left --@A 3. Else if k < v.L.LN Then \n 4. Left --JOIN(Left(@v.L,k),v.V,@v.R) 5. Else 6. Left --Left(@v.R,k-v.L.LN) 7. DEC(v)  Function \nRight(v: list; k: integer) : list ..... Symmetric analog of Left\" I. lf j < i Then 2. SUB --@A 3. Else \nlfj < v.L.LN Then 4. SUB --SUB(@v.L,i,j) 5. Else if i > v.L.LN Then 6. SUB --SUB(@v.R,i-v.L.LN,j-v.L.LN) \n 7. Else 8. SUB --JOIN(Left(@v.L,i),v.V, Right(@v. R,j-v. L. LN))  9. DEC(v)   \n\t\t\t", "proc_id": "800017", "abstract": "", "authors": [{"name": "Eugene W. Myers", "author_profile_id": "81406593964", "affiliation": "Department of Computer Science, The University of Arizona, Tucson, Arizona", "person_id": "PP77041037", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/800017.800517", "year": "1984", "article_id": "800517", "conference": "POPL", "title": "Efficient applicative data types", "url": "http://dl.acm.org/citation.cfm?id=800517"}