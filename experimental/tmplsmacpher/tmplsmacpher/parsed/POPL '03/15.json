{"article_publication_date": "01-15-2003", "fulltext": "\n A Type Theory for Memory Allocation and Data Layout * Leaf Petersen Robert Harper Karl Crary Frank \nPfenning Carnegie Mellon University Abstract Ordered type theory is an extension of linear type theory \nin which variables in the context may be neither dropped nor re-ordered. This restriction gives rise \nto a natural notion of adjacency. We show that a language based on ordered types can use this property \nto give an exact account of the layout of data in memory. The fuse constructor from ordered logic describes \nadjacency of values in memory, and the mobility modal describes pointers into the heap. We choose a particular \nallocation model based on a common implementation scheme for copying garbage collection and show how \nthis permits us to separate out the allocation and initialization of memory lo\u00adcations in such a way \nas to account for optimizations such as the coalescing of multiple calls to the allocator. Categories \nand Subject Descriptors D.3.1 [Programming Languages]: Formal De.nitions and The\u00adory; D.3.3 [Programming \nLanguages]: Language Constructs and Features; D.3.4 [Programming Languages]: Processors Com\u00adpilers, Memory \nmanagement General Terms Languages, Theory  Keywords Ordered Logic, Type Theory, Memory Management, \nData Repre\u00adsentation * This material is based on work supported in part by NSF grants CCR-9984812 and \nCCR-0121633. Any opinions, .ndings, and conclusions or recommendations in this publication are those \nof the authors and do not re.ect the views of this agency. Permission to make digital or hard copies \nof all or part of this work for personal or classroom use is granted without fee provided that copies \nare not made or distributed for pro.t or commercial advantage and that copies bear this notice and the \nfull citation on the .rst page. To copy otherwise, to republish, to post on servers or to redistribute \nto lists, requires prior speci.c permission and/or a fee. POPL 03, January 15 17, 2003, New Orleans, \nLouisiana, USA. Copyright 2003 ACM 1-58113-628-5/03/0001 ...$5.00 1 Introduction High-level programming \nlanguages such as ML and Java allow pro\u00adgrammers to program in terms of abstractions such as pairs, records, \nand objects, which have well-de.ned semantics but whose realiza\u00adtions in terms of the underlying concrete \nmachine are left unspeci\u00ad.ed and unobservable. Sometimes, it is necessary to program without these abstractions. \n A programmer may need to interact with an operating system or a network or another programming language \nin such a way as to require exact knowledge of, and control over, the manner in which data is laid out \nin memory.  A compiler must choose a concrete implementation for the high-level abstractions provided \nby the source level language such as the actual layout of data in memory and the manner in which such \nmemory gets allocated and initial\u00adized.  Traditionally, both of these needs have been addressed in an \nun\u00adtyped, or a weakly typed fashion. Languages such as C give pro\u00adgrammers relatively precise control \nover data layout and initializa\u00adtion at the expense of type and memory safety. Traditional compil\u00aders \nrepresent programs internally using un-typed languages, relying on the correctness of the compiler to \npreserve any safety properties enjoyed by the source program. Recently, research in the areas of typed \ncompilation and certi.ed code [12, 21, 11] has focused on providing type systems for low\u00adlevel languages \nin which abstractions such as control .ow and data layout are made explicit. These ideas have been used \nin a number of compilers [12, 21, 9, 2, 19, 6]. However, some of the mechanisms that have been invented \nto describe low-level operations are fairly ad hoc and do not yet have an interpretation in standard \ntype the\u00adory. For example, in the typed assembly language formalism[11], allocation and initialization \ncan be separated, but at the expense of having to annotate each type with a .ag indicating whether or \nnot the value it classi.es has been initialized. This kind of low-level technique seems unlikely to integrate \nwell with a high-level pro\u00adgramming language. In this paper, we attempt to give a type theoretic account \nof data layout that provides a foundation for de.ning how high-level con\u00adstructs such as pairs are laid \nout in memory. We realize our system with a concrete allocation model based on a common implementa\u00adtion \nof a copying garbage collector and show that we can separate out the process of allocating a block of \nmemory from the process of initializing the individual memory words. Our system is .ex\u00ad 1. 2. 3.  \n 3 4 5 Figure 1. Three possible layouts for the term (3,(4,5)) ible enough to permit multiple allocation \ncalls to be coalesced so that memory for multiple source level objects can be allocated si\u00admultaneously, \nwhile ensuring that calls to the allocator can never invalidate assumptions made about the state of partially \ninitialized data. An important contribution of this work is that it remains completely within the framework \nof a lambda calculus which enjoys the stan\u00addard meta-theoretic properties. In this way, we reconcile \nthe very low-level notion of allocated memory with the substitution proper\u00adties expected of a high-level \nprogramming language. This is of par\u00adticular interest because it suggests the possibility that these \nideas could be made available to programmers, so that even programs requiring detailed control of memory \nlayout could be written in a typed, high-level language.  2 Data layout and allocation Specifying the \nlayout of data in memory is an essential part of real\u00adizing a high-level program as a concrete collection \nof machine in\u00adstructions and data, but one which is usually not of direct interest to programmers. The \nprogrammer cares about the ability to construct objects, but most of the time cares about the layout \nin memory only insofar as it affects the performance of operations on an object. How terms should be \nlaid out in memory is therefore a matter of policy for the compiler writer. For example, the lambda calculus \nterm (3,(4, 5)) of type int \u00d7(int \u00d7int) de.nes a pair whose .rst element is 3 and whose second element \nis a pair containing 4 and 5. Figure 1 shows several possible representations for this term. One compiler \nmight choose to represent this as a pointer to a pair, whose elements are an integer and a pointer to \nanother pair. However, another might choose to add an indirection to integers, or to attempt to .atten \nthe whole term into three adjacent cells in memory. The high level notion of pairing captures certain \noperational proper\u00adties that are useful to the programmer, but does not uniquely specify an implementation \nstrategy. Commonly, a compiler simply chooses to interpret the pair type as meaning one particular strategy. \nFor the purposes of giving a general account of data layout, this is clearly unsatisfactory as it does \nnot permit us to break the high-level con\u00adcept into its constituent concepts. A .rst step to a more general \ntype theory for data layout is to ob\u00adserve that there seem to be two key concepts used by the different \ninterpretations of pairing given in .gure 1: adjacency and indirec\u00adtion. Each of the different choices \nof representation corresponds to a different choice as to which data is to be represented by physi\u00adcally \nadjacent bytes in memory and which data is to be represented via an indirection into another portion \nof memory. This is the .rst notion that we shall attempt to capture in our type system. 2.1 Allocation \nOnce the layout of data in memory has been made explicit, it be\u00adcomes possible to consider the process \nby which new memory is created and initialized. We suggest that it is useful to think of this in terms \nof three stages, regardless of the mechanism employed. Reservation is the process by which a new block \nof uninitialized memory is created. Initialization is the process by which values get written into the \nreserved memory, potentially changing its type. It is important for type safety that either the memory \nbe treated linearly in this stage, or else that the initialization operations be such that they only \nre.ne the type [3]. Allocation is the process by which a section of reserved (and pre\u00ad sumably initialized) \nmemory is made available as an ordinary unrestricted object. Different memory-management systems combine \nthese stages in different ways. For example, in the TAL framework [11], reser\u00advation and allocation are \ndone atomically, and hence initialization is very restricted in how it can change the type. The concrete \nmemory management system that we choose to model is one commonly used in practice by copying garbage \ncollectors and hence is of particular interest. This choice is not essential other systems can be expressed \nusing similar techniques to those we present here. In a copying garbage collector, the available memory \ncan be divided into two adjacent contiguous sections: a heap containing data that has been allocated \nsince the last garbage collection (or perhaps just the youngest generation thereof), and a possibly empty \nfreespace containing memory that has not yet been allocated. The allocator maintains an allocation pointer \n(or freespace pointer), which points to the end of the allocated data and the start of the free memory, \nand a heap-limit pointer, which points to the end of the free memory. To create a new heap object requiring \nn bytes, the program .rst compares the allocation pointer to the heap-limit pointer to ensure that there \nare at least n bytes available in the freespace. If not, it calls the garbage collector to free up enough \nspace. This step cor\u00adresponds to the reservation phase discussed above. Once suf.cient memory has been \nfound either in the existing freespace or by call\u00ading the garbage collector the program may assume that \nn bytes of free space exist in front of the allocation pointer. We refer to this initialized area as \nthe frontier. Once space has been reserved on the frontier, values can be written into the individual \ncells of memory via offsets from the allocation pointer. This corresponds to the initialization phase. \nAt any point, the program may move a pre.x of the frontier into the heap. The value of the allocation \npointer becomes the pointer to the new heap value, and the allocation pointer is advanced past the allocated \nspace. This corresponds to the allocation phase. Figure 2 gives an example of this process. The .rst \nline shows a schematic diagram of the heap and the freespace, where a.p. stands for the allocation pointer \nand l.p. stands for the limit pointer. The  a.p. l.p. abcd  a.p. l.p. Figure 2. Reservation, initialization, \nand allocation of (3,(4,5)) ragged boundary of the freespace indicates that we have no infor\u00admation about \nits extent it may potentially be exhausted. The second line of the .gure shows the result of reserving \nfour words of space suf.cient for allocating the term (3,(4,5)) using the .rst layout strategy from Figure \n1. We refer to the individual cells of the frontier by the names a,b,c and d. Note that this step may \nhave invoked the garbage collector to free up more memory if the freespace from the previously line was \nin fact exhausted. To create the pair (4,5) we assign 4 to a,5 to b, and then allocate a and b into the \nheap getting back a heap pointer x as shown on the third line of the .gure. We can then initialize the \ntop-level pair by writing 3 to c and x to d. A .nal allocation step gives us a pointer y which refers \nto a heap allocated structure of the form pictured in the .rst line of Figure 1. As this example shows, \nwe do not require that the entire frontier be allocated as a single object. The program may choose to \nreserve space for several objects at once and then initialize and allocate them individually. This optimization \navoids multiple checks against the heap-limit pointer. There are two constraints on this process that \nmust be captured by our type system to ensure safety. Firstly, the manner in which we move objects into \nthe heap means that objects cannot be allocated from the middle or end of the fron\u00adtier. Only pre.xes \nof the frontier that is, contiguous blocks of memory adjacent to the allocation pointer may be allocated. \nSecondly, reserved space in the frontier cannot persist across suc\u00adcessive reservations nor across function \ncalls. When the garbage collector is called it will copy the live data to a new heap and change the allocation \npointer to point to this new location. Any partially initialized data that was previously in the frontier \nwill be lost in the process. This corresponds to a kind of destructive effect: the state of the frontier \ncannot be assumed to be preserved across the evaluation of any term that could potentially call the allocator. \nThe type sys\u00adtem must therefore ensure that no assumptions about the state of the frontier can persist \nacross the evaluation of any term that might reserve or allocate memory.  3 Ordered linear type theory \nOrdered (or non-commutative) linear logic is a variant of standard linear logic in which hypotheses must \nnot only be used exactly once, but must also be used in order [17, 16, 18, 15]. The corre\u00adsponding proof \nterms make up an ordered lambda calculus that is characterized by the lack of an exchange property for \nthe ordered context in addition to the usual linearity restrictions. We present a small fragment of the \nordered lambda calculus by way of intro\u00adduction to the these ideas. The presentation here is simpler \nthan previous work, in that it omits the linear context, retaining only the ordered and unrestricted \ncontexts. The modal therefore moves di\u00adrectly from the ordered terms to unrestricted terms. Typing rules \nfor the ordered lambda calculus have the form G;. f M : t, indicating that the M has type t under the \nvariable assump\u00adtions declared in the unrestricted context G and the ordered con\u00adtext .. We distinguish \nsyntactically between ordered variables a which must be used linearly and in order, and unrestricted \nvariables x which may be used arbitrarily often. G;a:t fa : tG,x:t,G';\u00b7fx : t Unlike standard linear \ntype theory, the ordered comma operator .1,.2 is interpreted as simple list concatenation and does not \nper\u00admit the intermingling of hypotheses. Where unambiguous, we write a:t instead of a:t,\u00b7for singleton \ncontexts. (.1,.2) def .2 if .1 = \u00b7 = a:t,(.' 1,.2) if .1 = a:t,.' 1 t :: = int integers | t1 .t2 unrestricted \narrow | t1 t2 ordered multiplicative | !t modal type . :: = \u00b7|a:t, . ordered contexts G :: = \u00b7|x:t, G \nunrestricted contexts M :: = a ordered variables | x unrestricted variables | n integer literals | M \nM fuse intro | let a1 a2 = M in M fuse elim | .(x:t).E lambda intro | MM lambda elim | !M modal intro \n| let !x = M in M modal elim Figure 3. Standard ordered lambda calculus syntax This de.nition means that \nconcatenation of contexts preserves the order of the entries in the contexts. The multiplicative connective \n(fuse) demonstrates a use of this con\u00adcatenation operator. G;.1 fM1: t1 G;.2 fM2: t2 G;.1,.2 fM1 M2: \nt1 t2 The elimination rule for fuse splits it into components and places them in the ordered context. \nNotice that the variables representing the components of M1 go into the ordered context in place of .. \nG;.fM1: t1 t2 G;.L,a1:t1,a2:t2,.R fM2: t G;.L,.,.R flet a1 a2 = M1 in M2: t Finally, the mobility modal \npermits terms that are orderedly closed to be moved to the unrestricted context. ' G;\u00b7fM : tG;.fM :! \ntG, x:t;.L,.R fM ' : t ' G;\u00b7f!M :!tG;.L,.,.R flet ! x = M in M ' : t 3.1 Size preservation and adjacency \nThere are three interesting observations that we can make about ordered lambda calculus terms that motivate \nthe application of or\u00addered type theory to data layout. 1. Because ordered variables may not exchange \nposition in the context, we may think of ordered variables as simply standing for locations in the ordered \ncontext. 2. We may break ordered terms down into their components and re-form them, but we may not change \ntheir order. In particu\u00adlar, the term that splits apart an ordered pair and reforms it in the opposite \norder is not well-typed.  let a1 a2 = a in a2 a1 Viewed as a linear (rather than ordered) term, this \ncode would be well-typed. 3. The ! modality takes an ordered term whose location is .xed and moves it \ninto the unrestricted context, where its location become indeterminate. Based on these observations, \nwe propose the following three intu\u00aditions as the basis for our system. 1. An ordered context may be \nthought of as describing a particu\u00adlar region of memory under consideration. Ordered variables correspond \nto locations, or offsets into the region. Adjacent variables in the context correspond to physically \nadjacent lo\u00adcations, with extents given by the types of the variables. 2. The fuse constructor t1 t2 \ndescribes terms that are physically adjacent in memory. The fact that we cannot reorder ordered terms \ncorresponds naturally to the fact that we cannot reorder bytes in memory. 3. The ! modality !tcorresponds \nto an indirection out of the re\u00adgion of memory described by the ordered context into another (unspeci.ed) \npart of the heap.  The standard ordered lambda calculus does not entirely justify these intuitions. \nOrdered terms preserve the order of sub-components, but they do not in general preserve their adjacency. \nThe essence of this problem can be seen in the derived ordered substitution princi\u00adple. ' G;.fM : tG;.1,a:t,.2 \nfM ' : t ' G;.1,.,.2 flet a = M in M ' : t Notice that the portion of the ordered context that is passed \nto the term being bound is replaced with the variable itself when type\u00adchecking the rest of the body. \nOur intention is that operations such as this should be done in-place on the memory described by the \nor\u00addered context. However, the following term demonstrates that this does not hold in the general ordered \nlambda calculus. ' G;\u00b7f3: int G;.1,a:int,.2 fM ' : t ' G;.1,\u00b7,.2 flet a = 3in M' : t The problem is that \nwe are able to insert unrestricted terms into the ordered terms in arbitrary places. While this does \nnot violate our notion that ordered variables correspond to locations, it does mean that these locations \nare not .xed. Operationally, it would seem that we would be forced to shift all of .2 over in memory \nto make room for the new term in the context. An alternative way of looking at this is that the general \nordered lambda calculus is not size preserving: the sub-derivation G:\u00b7f 3: int produces a term of size \none from a context of size zero. If we interpret the ordered context as describing a region of mem\u00adory, \nthen the above term inserts a word-sized value into an empty region of memory! In order to prevent such \nproblematic terms, it is necessary to carefully restrict the calculus in such a way as to ensure that \noperations on memory preserve size. The notion of size preservation is the last insight necessary to \nfor\u00admulate a lambda calculus in which we can give a full account for data layout. We will use the fuse \ntype to describe adjacency and the modal type to describe indirection, while restricting the terms in \nsuch a way as to enforce various key size preservation properties. The allocation model described in \nsection 2 will be accounted for by using an ordered context to describe the frontier. Ordered variables \nthen become offsets into the frontier, and reservation, initialization, and allocation become operations \non ordered terms. The linearity of k :: = Treg |Th t :: = 1 |int |t1 .t2 |t1 t2 |!t |NS Q :: = a |*|Q \nQ V :: = *|ns |n |V V |.(x:t).E |!V M :: = x |ns |n |.(x:t).E |!V E :: = ret M |MM |let x : t =E in E \n|reserven as ain E |alloc Q as x in E |Q :=M as ain E |let a =Q in E |let a a =Q in E |let *=Q in E |let! \n(x x)=M in E |let !x =M in E G :: = \u00b7|x:t, G . :: = \u00b7|a:t, . . :: = . \u00b7|a .V, . Figure 4. Syntax the \nordered context will permit destructive operations on the fron\u00adtier (such as initialization), and the \nsize preservation property will ensure that all operations on the frontier may be done in-place.  4 \nThe orderly lambda calculus We now have all of the ideas that we need to de.ne a language for data layout \nand allocation, which we shall call the orderly lambda calculus,or .ord for short. For the sake of brevity, \nthis paper will focus on a small core language that captures the essential ideas. The syntax of the core \nlanguage is given in .gure 4. We use the notation tn for an n-ary fuse of t. t0 = 1 tn+1 t tn = For data \nlayout purposes, we only require a few new types from the ordered lambda calculus: the fuse constructor \nwhich models adjacency; the modal constructor, which models indirection; and the multiplicative unit. \nOther types include a base type of integers and the type of unrestricted functions. The NS (nonsense) \ntype is the type of a single un-initialized word of memory. It is important for our purposes to distinguish \nbetween types which are of unit size and hence can be kept in registers or on the stack, and other types \nthat must be heap allocated. This is accomplished by a kinding distinction ft : k. The kind Treg classi.es \nthe types of values which may be loaded into registers, whereas the kind Th classi.es types that may \nbe heap-allocated (a strict super-set of the former). An important property of this language is that \ntypes uniquely deter\u00admine the size of the data they classify. . . 0 if t =1 def |t|= |t1|+|t2| if t =t1 \nt2 . 1 if t =t1 .t2, int, NS or !t' For simplicity, the smallest unit of size we consider is a single \nma\u00adchine word. The multiplicative unit type has size zero, since it is inhabited by a single value which \ntherefore does not need to be rep\u00adresented. We view the function type as having unit size, since we expect \nthat a practical implementation would use closures to rep\u00adresent functions. Under closure conversion, \nlambdas become ex\u00adistentially quanti.ed records allocated on the heap, and hence are represented by a \npointer of unit size. We assume that the actual code for the function will be statically allocated. Ordered \ncontexts . map ordered variables a to types t, and are used to describe regions of memory (in particular, \nthe frontier). The notion of sizing for types extends naturally to ordered contexts. def0 if . =\u00b7 |.|= \n|t|+|.'| if . =a:t,.' As before, exchanging, discarding, or duplicating variables in the ordered context \nis not permitted. Unrestricted contexts G map ordinary variables x to their types. The well-formedness \njudgement for unrestricted contexts checks that all unrestricted variables have unit-sized types that \nis, types whose kind is Treg . Ordinary variables correspond to registers or stack slots in the underlying \nmachine, and so are restricted to have word size via this kinding mechanism. This is a key point about \nthe orderly lambda calculus: all large objects are required to be explicitly allo\u00adcated and initialized. \nThe term level of .ord is split into four separate syntactic classes: coercion terms Q, heap values V \n, terms M and expressions E. The main typing judgements are described in .gure 5, along with com\u00adments \nabout the size properties which they enjoy. Complete de.ni\u00adtions of the typing rules can be found in \nappendix A. Making allocation explicit introduces a kind of effect into the lan\u00adguage. Reserving and \nallocating memory is an effectful operation, and as we saw in the previous section these effects may \ninterfere. In order to control these effects and their interaction we introduce a distinction between \nterms M and expressions E in the style of Pfenning and Davies [14], but without an explicit modal type \nfor computations. (The computation type does not seem useful in our setting since we do not have the \ninclusion of expressions into terms, instead taking the partial arrow as primitive). The syntactic form \nwe impose is not overly restrictive: it is actually related to, but more permissive than, the A-normal \nor CPS forms that many compilers typically use. 4.1 Terms Terms M correspond to values that do not reserve \nor allocate in the course of their evaluation, but that may contain free references to ordered variables \n(that is, to the frontier). In this presentation, all terms are values but it is straightforward and \nuseful to include other primitive operations that do not allocate (such as integer oper\u00adations) at this \nlevel. The typing judgement for terms is of the form G;. ftrm M : t. The term M may refer to variables \nin G arbitrarily often, but must refer to each variable in . exactly once, and in an ordered fashion. \nThe typing rules for terms are for the most part unsurprising. For the .-abstraction case, the body of \nthe function is checked as an expression, with the argument placed in the unrestricted context. G,x:t;. \nfexp E : t' G;. ftrm .(x:t).E : t .t' Notice that we permit free references to the frontier in functions. \nSince function application lies in the category of expressions, we Judgement Size properties Meaning \nf. . is a well-formed ordered context. fG .x .G,|G(x)|= 1 G is a well-formed unrestricted context. ft \n: k if k = Treg then |t|= 1 t is a well-formed type. . fcrc Q : t |.|= |t| Q coerces . to look like t \n. G;. ftrm M : t |t|= 1 M is a non-allocating/non-reserving term of type t. G;. fexp E : t |t|= 1 E is \na well typed expression of type t which consumes .. fval V : t |V |= |t| V is a closed value of type \nt. f. : . . is a well-typed frontier for the ordered context .. |.|= |.| will defer discussion of the \nelimination form to Section 4.4. All other terms must be closed with respect to the ordered context. \nThe most non-standard term is !V . This term corresponds to a pointer into the heap to a location occupied \nby the heap value V , and is the canonical form for terms of type !t. fval V : t G;\u00b7ftrm !V :!t An interesting \nfacet of our presentation is that we account for heap allocation without requiring an explicit heap (for \nexample in the style of Morrisett and Harper [10]). In a heap semantics, a pointer to a value V is represented \nby a label e, with e bound to V in an ex\u00adplicit heap data-structure. Since sharing is not observable \nin our simple calculus, we avoid this extra complexity by representing such values directly as !V , denoting \na pointer to a location occu\u00adpied by V . We stress that this is purely a technical convenience it is \nstraightforward to give a heap semantics in which the sharing is made explicit in the usual fashion. \n 4.2 Heap Values Heap values V represent terms that may occur in memory. It is therefore essential that \nthey be closed. An open heap term would require that a new copy be implicitly allocated every time differ\u00adent \nvalues were substituted into it, which is contrary to the aims of .ord. The typing judgement for heap \nvalues, fval V : t, enforces this property. The primary motivation for having heap values comes from \nthe op\u00aderational semantics of the language. However, it is not intended that they should play the role \nof so-called semantic objects that are only permitted to be introduced in the course of evaluation. It \nis perfectly reasonable for a programmer to write heap values in the source program. Doing so corresponds \nprecisely to the notion of statically allocated data that is, data that is present in the heap at the \nstart of the program. The important difference between heap values and terms is that heap values may \nbe of arbitrary size. This is re.ected in the syn\u00adtax by the value V1 V2, denoting a contiguous block \nof memory in which V1 is laid out adjacent to the value V2. The fact that fused terms are adjacent means \nthat the construc\u00adtor is associative in the sense that the term 3 (4 5) has the same representation \nin memory as the term (3 4) 5. Both terms de\u00adscribe three successive words of memory, occupied by the \nintegers 3, 4, and 5 respectively. This is a fundamental difference from ordi\u00adnary lambda calculus pairing, \nin which (3,(4,5)) is almost certain Figure 5. Typing judgements for .ord to have a different representation \nfrom ((3, 4),5). This associativity is just one example of values which have different types but the \nsame representations. Other examples include values involving the ordered unit, *. Since we do not choose \nto represent this value, we expect that the representations of 3 *, * 3, and 3 will all be the same at \nruntime. Coercion terms exist to provide a mechanism by which to convert between such values which have \ndifferent typing structure but the same underlying representation.  4.3 Coercions The level of coercion \nterms in this fragment of the language is ex\u00adtremely simple, consisting only of variables a, the ordered \nunit *, and fuse Q1 Q2. Coercion binding and elimination forms are pro\u00advided at the expression level \n(Section 4.4). Intuitively, coercion terms package up the frontier into new forms without changing the \nunderlying representation. For example, the term a1 a2 takes the section of the frontier described by \na1 and the section described by a2 and combines them into a single fuse which could then be bound at \na new name using the expression level coercion let. The orderedness of the terms ensures that the two \nsections were already adjacent, and hence combining them into a fuse does not change their representation. \nThe typing judgement for coercion terms is of the form . fcrc Q : t, signifying that Q re-associates \n. to have the form t. The coercive nature of the terms is exhibited in the size preservation property \nthat holds of this judgement: that |.|= |t|. .1 fcrc Q1: t1 .2 fcrc Q2: t2 a:t fcrc a : t.1,.2 fcrc Q1 \nQ2: t1 t2 The unit term is well-typed in the empty context. \u00b7fcrc *:1  4.4 Expressions So far we have \nonly seen the value forms that occupy or coerce memory, but that do not modify it. The memory operations \nreservation, allocation, and initialization are all done at the level of expressions. The well-formedness \njudgement for expressions is given by G;. fexp E : t. The ordered context . in the typing judgement de\u00adscribes \nthe current state of the frontier. Because of the destructive nature of the reserve and allocate operations, \nthe interpretation is that the frontier is consumed by the expression E. That is, any space that is on \nthe frontier must either be allocated by E, or explic\u00aditly destroyed. As we saw in section 2, memory \noperations are effectful, and so the type system for expressions must be carefully designed to en\u00adsure \nthat these effects do not interfere. This is enforced by always passing the entire ordered context (and \nhence the entire frontier) to each sub-expression (but not sub-term). In this way, we ensure that every \npossibly allocating/reserving expression has a correct view of the entire frontier when it is evaluated. \nThe expressions can be conceptually divided into four basic cate\u00adgories. Ordinary expressions The inclusion \nof values into expressions is given by the expression ret M. G;\u00b7ftrm M : t G;\u00b7fexp ret M : t This is \nthe only value form for expressions, and consumes no re\u00adsources. It is unsound to permit the term M to \ncontain ordered vari\u00adables, since it may be substituted for an unrestricted variable by the primitive \nlet form discussed below. Function application is an expression, since the evaluation of the body of \nthe function may engender memory effects. Applications are syntactically restricted to permit only application \nof a term to another term. G;.ftrm M1: t1 .t2 G;\u00b7ftrm M2: t1 G;.fexp M1 M2: t2 The term being applied \nis permitted to refer to ordered variables, but the argument must be closed since unrestricted functions \nmay duplicate or drop their arguments. Application allows us to de.ne a term-level let construct with \nthe following derived typing rule. ' G;\u00b7ftrm M : tG,x:t;.fexp E : t ' G;.fexp let x:t= M in E : t This \nlet is not fully general, since there is no way to bind the result of an application to a variable. Therefore, \nwe introduce a primitive let form to bind expressions to variables. G;.fexp E1: t1 G,x:t1;\u00b7fexp E2: t2 \nG;.fexp let x:t1 = E1 in E2: t2 Notice that we pass the entire ordered context to the .rst sub\u00adexpression. \nThis is a crucial point: E1 may have memory effects that could invalidate any previous assumptions about \nthe state of the frontier that E2 might make. Therefore, E2 cannot assume anything at all about the state \nof the frontier that is, it must be well-typed in an empty ordered context. Somewhat surprisingly, it \nis safe to permit E1 to have free refer\u00adences to the ordered context. This is reasonable because expres\u00adsions \nconsume resources, but do not contain them. By this we mean that the value form for expressions (ret \nM) is well-typed only in an empty ordered context. Therefore, if the ordered context .is not empty, then \nE1 must explicitly destroy or allocate all of the memory described by .before it reaches a value. Since \nthis value will be orderedly closed, it is safe to substitute it freely for the unrestricted variable \nx. Memory expressions The most interesting and non-standard expressions are those deal\u00ading directly with \nthe frontier. Recall that there are three operations of interest: reserving space on the frontier, initializing \npieces of the frontier, and allocating pre.xes of the frontier into the heap. These three operations \nare captured directly as primitives. As we shall see later, this is not entirely necessary by extending \nthe type system somewhat we can give types to these primitives as constants. For simplicity however, \nwe .rst present them as primitive notions. The .rst operation, reservation, discards any resources that \nwere previously mentioned in the ordered context, and introduces n words of nonsense into the frontier. \nG;a:NSn fexp E : t G;.fexp reserven as ain E : t This corresponds exactly to the reservation operation \ndescribed in Section 2.1, which destroys any existing data on the frontier and provides a block of new \nuninitialized space. Memory must be written using assignment. .fcrc Q : t ft: Treg '' '' G;\u00b7ftrm M : \ntG;.L,a:t,.R fexp E : t '' G;.L,.,.R fexp Q := M as ain E : t The ordered term Q gives the location in \nthe ordered context to which the value should be written. This location is then referred to by a in the \nbody of the expression. The linearity of the ordered context is important here, since we are destructively \nchanging the type of a memory location. At any point, space can be allocated from the left side of the \nfrontier with the alloc construct. ' .L fcrc Q : tG, x:! t;.R fexp E : t ' G;.L,.R fexp alloc Q as x \nin E : t The coercion term Q describes a section of the frontier to be pack\u00adaged up as a boxed heap value. \nThe splitting of the ordered context ensures that the term to be allocated is a pre.x of the frontier. \nThe new heap value is given a pointer type and permitted to be used unrestrictedly for the rest of the \nprogram. Coercion expressions The memory expressions manipulate the frontier using ordered variables, \nwhich stand for offsets into the frontier. Coercions are used to manipulate ordered variables, combining \nthem into bigger terms or breaking them into smaller pieces. The simplest coercion expression is the \nelimination form for unit. .fcrc Q :1 G;.L,.R fexp E : t G;.L,.,.R fexp let *= Qin E : t . G;\u00b7ftrm M \n:!t ft[i]: Treg G,x:t[i];.fexp E : t2 [i]def . t1[i] if t=t1 t2 and |t1|> i where t= t2[i -|t1|] if t=t1 \nt2 and |t1|=i . G;.fexp loadtx =M[i] in E : t2 t if tis not a fuse and i =0 . .  let! x1 x2 =M in . \nif t=t1 t2 and |t1|> i . . loadt1 x =x1[i] in E def loadtx =M[i] in E = let! x1 x2 =M in . if t=t1 \nt2 and |t1|=i . loadtx =x2[i -|t1|] in E . 2 . let ! x =M in E if |t|=1 and tis not a fuse Figure 6. \nAn example of a direct-load de.ned in terms of split Since the unit term is considered to have zero size, \nwe may elimi\u00adnate it freely from the ordered context without changing the size or adjacency properties \nof the terms in the frontier. The elimination form for fuse is also a coercion expression. .fcrc Q : \nt1 t2 G;.1,a1:t1,a2:t2,.2 fexp E : t G;.1,.,.2 fexp let a1 a2 =Qin E : t The intuition is that since \nt1 t2 describes two adjacent blocks of memory, we are free to view the single block of memory described \nby Q as two adjacent blocks at offsets named by a1 and a2. The last coercion operation is the simple \nordered let form, which permits ordered terms to be packaged up or renamed. ' .fcrc Q : tG;.1,a:t,.2 \nfexp E : t ' G;.1,.,.2 fexp let a =Qin E : t Load expressions The memory operations account for the creation \nof heap objects. Equally important is the ability to load values out of the heap. Once an object is in \nthe heap, we must have some way of accessing its components. Pointers to small objects can be de-referenced \ndi\u00adrectly. G;\u00b7ftrm M :!t1 ft1: Treg G,x:t1;.fexp E : t2 G;.fexp let !x =M in E : t2 The kinding restriction \nensures that the only values that can be loaded with this operation are those that will .t into a register. \nTo access the .elds of larger objects, we provide a composite elim\u00adination construct that takes a pointer \nto a large object, and produces two pointers to the immediate subcomponents of the object. G;\u00b7ftrm M \n:!(t1 t2) G,x1:! t1,x2:!t2;.fexp E : t G;.fexp let!(x1 x2)=M in E : t Notice that the variables are \nbound not to the components of M themselves, but rather to pointers to the components of M. Using this \nexpression we may successively iterate over large composite objects until we arrive at a pointer to a \nsmall object which can be loaded directly. This construct is somewhat disturbing from a practical standpoint \nfor two reasons. In the .rst place, it seems to require pointers into the interior of objects (sometimes \ncalled locatives) in order to be implemented ef.ciently. While not completely out of the question, interior \npointers can be quite problematic for copying garbage col\u00adlectors (at least when implemented as direct \npointers into the inte\u00adrior of heap objects). More importantly however, this construct does not permit \nconstant time access to .elds of a heap-allocated record. For example, to access the last element of \na n-ary tuple in right-associated form re\u00adquires n computations before we arrive at a term that can be \nloaded directly. This is clearly impractical. We choose to use this split operation as the primitive \nnotion be\u00adcause it provides a simple and natural elimination form. In practice however, it is likely \nthat this term would be eliminated in favor of one of a number of direct-load constructs that are de.nable \nin terms of split (.gure 6). By taking such a direct-load as primitive and giv\u00ading it a direct implementation, \nthe need for the interior pointers is eliminated and .elds of records can be loaded in constant time. \n 4.5 Frontier semantics In order to make the connection between the orderly lambda calcu\u00adlus and the \nfrontier model of allocation clear, the semantics keeps an explicit frontier. This means that the reduction \nrelation is de.ned not just on expressions, but rather on a frontier and an expression together. Frontier \nterms .(as de.ned in .gure 4) map ordered variables (that is, offsets) to values V . From the standpoint \nof the operational se\u00admantics, the frontier plays a role very similar to an explicit substi\u00adtution. The \ntyping judgement for the frontier, f.: ., asserts that the ordered context .describes a frontier that \nlooks like .. fval V : t f.: .(a ./.) f\u00b7: \u00b7f(a .): (a:t,.).V,. The evaluation relation for the orderly \nlambda calculus is given in terms of frontier/expression pairs. f.: .\u00b7;.fexp E : t f(.,E): t ' The relation \n(.,E)..(.,E ')indicates that in frontier ., the ex\u00ad pression E reduces in a single step to the expression \nE ', with new ' frontier .. The complete de.nition of this relation is given in Ap\u00adpendix B. It is straightforward \nto show that reduction preserves typing, and that well-typed terms that are not values may always be \nreduced further. Theorem 1 (Progress &#38; Preservation) If f (.,E) : t then . (.' 1. Either (.,E) .,E \n') or E is a value. . (.' 2. if (.,E) .,E ') then f (.' ,E ') : t PROOF. The proof proceeds by induction \non the derivation of \u00b7;. fexp E :t, with the help of several substitution lemmas and some auxiliary lemmas \nproving properties of ordered contexts and fron\u00adtiers. 4.6 Size properties An important property of \nthe orderly lambda calculus is that types uniquely determine the size of the data that they represent. \nWe have informally mentioned a number of sizing properties of the calcu\u00adlus: in particular that coercion \nterms preserve size, and that terms and expressions are always of unit size (so that they can be kept \nin registers). These properties can be formalized as follows. Theorem 2 (Size) 1. If f t : Treg then \n|t| = 1 2. If f t : Th then .i such that |t| = i 3. If . fcrc Q : t then |.| = |t| 4. If fval V : \nt then |V | = |t| 5. If G;. ftrm M : t then |t| = 1 6. If G;. fexp E : t then |t| = 1 7. If f . : \n. then |.| = |.|.  PROOF. For each clause we proceed separately by induction on typing derivations. \n  5 Representing the lambda calculus One of the intended uses of .ord is as a target language for translation \nfrom higher-level languages. To show how this can be done, and to provide some intuition into how the \nlanguage is used, we present in this section a translation from the simply typed lambda calculus with \nproducts and unit into the orderly lambda calculus. We begin by de.ning a translation .t.that maps each \nordinary lambda calculus type to a .ord type. .int.= int .unit.= !1 .t1 . t2.= .t1.. .t2. .t1 \u00d7 t2.= \n!(.t1. .t2.) The product case is unsurprising: we represent a pair as a pointer to a heap-allocated record \ncontaining the sub-components. As dis\u00adcussed in section 2, other representations are possible. We represent \nthe ordinary lambda calculus unit as a pointer to the orderly lambda calculus unit. Recall that |1| = \n0in .ord. This means that our chosen representation of unit is as a pointer to a zero-word object. This \ncorresponds precisely to the standard implementation of values of type unit as a distinguished pointer \nto nothing (e.g. the null pointer). An analogous translation is de.ned at the term level. The interest\u00ading \ncase is the translation of pairing, since pairs are the only terms requiring allocation. We begin by \nde.ning a .ord function pair. pair : t1 . t2 . !(t1 t2) def = .(x1: t1)..(x2: t2). reserve2 as a (1) \nin let a1 a2* = a (2) in let a2 a* = a2* (3) in let * = a* (4) ' in a1:= x1 as a (5) 1 ' in a2:= x2 \nas a (6) 2 '' in alloc(a1 a2) as x (7) in ret x (8) The .rst line of the function reserves the space \non the frontier from which the pair will be created. This binds a single ordered variable a which points \nto the beginning of this space. Line 2 gives the names a1 and a2* respectively to the .rst and second \nwords of the newly allocated space. From the typing rule for reserve we can see that the second location \nhas an extra zero-byte value of type unit attached, so lines 3 and 4 serve to split out and eliminate \nthis. ' Lines 5 and 6 initialize the two locations, renaming them to a1 and ' a2. Finally, line 7 allocates \nthe initialized space into the heap and names the result x, which becomes the return value of the function \nin line 8. This de.nition demonstrates how the various operations interact to permit low-level code to \nbe written in a relatively high-level man\u00adner. In particular, there is no mention of offsets at all: \neverything is done in terms of standard alpha-varying variables. It may seem that this code is somewhat \nverbose, but it is simple to de.ne syn\u00adtactic abbreviations and composite terms that eliminate much of \nthe verbosity. For example, in the common case for initialization terms where the coercion term Q is \na variable, we may take advantage of alpha-conversion to simply re-use the old variable name, yielding \na more standard looking assignment syntax. def a1:= M in E = a1:= M as a1 in E It is also trivial to \nde.ne a composite reserve operation that pre\u00adcomputes the offset variables. G;a1:NS,...,an:NS fexp E \n: t G;. fexp reserven as[a1,...,an]in E : t Working out the de.nition of this term is left as an exercise \nto the reader, but using these abbreviations, we can write the pair con\u00adstructor quite succinctly. pair \n: t1 . t2 . !(t1 t2) def = .(x1: t1)..(x2: t2). reserve2 as[a1,a2] in a1:= x1 in a2:= x2 in alloc(a1 \n a2) as x in ret x The elimination forms for pairs can be given succinct de.nitions using the direct \nload de.ned in Figure 6. fst :!(t1 t2) . t1 def = .(x :!(t1 t2)) loadt1 t2 x1 = x[0] in ret x1 snd \n:!(t1 t2) . t2 def = .(x :!(t1 t2)) loadt1 t2 x2 = x[1] in ret x2 The remainder of the translation \nof the simply typed lambda calcu\u00adlus is straightforward. All variables introduced by the translation \nare assumed to be fresh. def .x.= ret x def .n.= ret n def .().= ret !* def ..(x:t).e.= ret .(x:.t.)..e. \ndef .e1 e2.= let x1 = .e1. in let x2 = .e2. in x1 x2 def .(e1,e2).= let x1 = .e1. in let x2 = .e2. in \nlet xt = pairx1 in let x = xt x2 in ret x def .p1e.= let x = .e. in fst x def .p2e.= let x = .e. in snd \nx 5.1 Coalescing reservation Translating simply typed lambda calculus terms into the orderly lambda calculus \nbreaks the high level memory abstractions and ex\u00adposes a .ner grain of detail. Exposing these details \ncan enable op\u00adtimizations not expressible at the more abstract level. A simple ex\u00adample of this is the \nability to coalesce multiple calls to the allocator. For example, consider the result of translating \nthe term (3,(4,5)) under the above translation (with some minor simpli.cations). .(3,(4, 5)).= let xt \n= reserve2 as[a1,a2] ' in a1:= 4 as a1 ' in a2:= 5 as a2 '' in alloc(a1 a2) as x in ret x in reserve2 \nas[a3,a4] ' in a3:= 3 as a3 ' in a4:= xt as a 4 '' in alloc(a3 a4) as x in ret x This code fragment \nmakes two separate calls to the allocator, each reserving two words of space. It is easy to see that \nthe second re\u00adserve operation can be coalesced with the .rst, reducing the total number of calls to the \nallocator. opt .(3,(4, 5)).= reserve4 as[a1,a2,a3,a4] ' in a1:= 4 as a1 ' in a2:= 5 as a2 '' in alloc(a1 \n a2) as xt ' in a3:= 3 as a3 ' in a4:= xt as a 4 '' in alloc(a3 a4) as x in ret x This kind of optimization \nis commonly done in untyped compilers, but here we can easily express it in a typed setting. A further \nstep to consider is to try to coalesce the two allocation op\u00aderations, in addition to coalescing the \nreservations. Unfortunately, this is not in general possible in our setting. The problem is that we currently \ncannot express pointers into the frontier such point\u00aders would be dif.cult to typecheck since the types \nof locations in the frontier can change. Therefore we are unable to initialize the second .eld of the \ntop level pair until we have moved the other pair into the heap.  6 Extensions and future work This \npaper has given a detailed presentation of the core of the or\u00adderly lambda calculus, developing a high-level \nframework for dis\u00adcussing issues of allocation and data-layout. The full language in\u00adcludes an account \nof sums and recursive types that permits sum al\u00adlocation and tagging to be done using only the memory \nmechanisms already described. In addition, we have extended the coercion level to include ordered functions \nand application forms and shown that a rich language of coercions is de.nable in this setting. Finally, \nwe have shown how the reserve, alloc, and write primitives can be replaced by typed constants, eliminating \nthe need to incorporate special memory-management primitives into the language. The full language is \ndescribed in a separate technical report [13]. The most important question that we have not yet addressed \nis how to give an account of the allocation of objects with dynamic extent. The system we have developed \nso far is predicated on the ability to statically predict the size of an object based on its type. For \nobjects such as arrays however, this is clearly not true. While an ad-hoc treatment of arrays can be \nfairly easily integrated into the language, this is unsatisfactory since the intention is to make all \nallocation explicit through the same mechanism. A more interesting possibility is to use a dependent \ntype formalism [23] or a type analysis formalism [4] to introduce a notion of dynamic extent into the \ntype system. We intend to explore this avenue further in the future. Another important area for future \nresearch is to attempt to account for pointers into the frontier itself. As we saw in Section 5 we are \nforced to allocate an object into the heap before we can initialize other objects with a pointer to it, \nwhich prevents some useful opti\u00admizations such as the destination passing style optimization [8]. 7 \nRelated work Ordered logic and ordered type theory have been explored exten\u00adsively by Pfenning and Polakow \n[16, 15]. There is a signi.cant amount of previous work applying ordinary linear type theory to memory \nmanagement [1, 22, 5, 7], but none of it addresses (nor is intended to address) the question of separat\u00ading \nout allocation and initialization, and of giving a foundational account of data layout. The work that \nmost closely addresses the issues that we discuss here is the alias type formalism of Smith, Walker, \nand Morrisett [20]). Alias types allow aliasing information to be tracked exactly in the type system. \nA quasi-linear type system allows memory locations to be destructively updated. Since aliasing is tracked \nexactly, an ex\u00adplicit free operation is provided which de-allocates space. Some very useful optimizations \nsuch as the destination passing style op\u00adtimization can be encoded fairly easily in this language. The \nalias type formalism does not seem to provide for the explicit coalesc\u00ading of allocator calls, nor does \nit provide an explicit type theory for describing data layout in the manner that we have attempted to \ndo. 8 References [1] Jawahar Chirimar, Carl A. Gunter, and Jon G. Riecke. Ref\u00aderence counting as a computational \ninterpretation of linear logic. Journal of Functional Programming, 6(2):195 244, 1996. [2] Christopher \nColby, Peter Lee, George C. Necula, Fred Blau, Mark Plesko, and Kenneth Cline. A certifying compiler \nfor Java. In Proceedings of the Conference on Programming Lan\u00adguage Design and Implementation (PLDI 00), \npages 95 107, Vancouver, Canada, June 2000. ACM Press. [3] Karl Crary and Greg Morrisett. Type structure \nfor low-level programming langauges. In Twenty-Sixth International Col\u00adloquium on Automata, Languages, \nand Programming, volume 1644 of Lecture Notes in Computer Science, pages 40 54, Prague, Czech Republic, \nJuly 1999. Springer-Verlag. [4] Karl Crary and Stephanie Weirich. Flexible type analysis. In 1999 ACM \nInternational Conference on Functional Program\u00adming, Paris, France, September 1999. ACM Press. [5] A. \nIgarashi and N. Kobayashi. Garbage collection based on a linear type system, 2000. [6] T. Jim, G. Morrisett, \nD. Grossman, M. Hicks, J. Cheney, and Y. Wang. Cyclone: A safe dialect of c, June 2002. [7] Naoki Kobayashi. \nQuasi-linear types. In Conference Record of POPL 99: The 26th ACM SIGPLAN-SIGACT Sympo\u00adsium on Principles \nof Programming Languages, San Antonio, Texas, pages 29 42, New York, NY, 1999. [8] Y. Minamide. A functional \nrepresention of data structures with a hole. In Conference Record of the 25th Symposium on Principles \nof Programming Languages (POPL 98), 1998. [9] Greg Morrisett, Karl Crary, Neal Glew, Dan Grossman, Richard \nSamuels, Frederick Smith, David Walker, Stephanie Weirich, and Steve Zdancewic. TALx86: A realistic typed \nassembly language. In Second Workshop on Compiler Sup\u00adport for System Software, pages 25 35, Atlanta, \nGeorgia, May 1999. [10] Greg Morrisett and Robert Harper. Semantics of memory management for polymorphic \nlanguages. In A. Gordon and A. Pitts, editors, Higer Order Operational Techniques in Se\u00admantics. Newton \nInstitute, Cambridge University Press, 1997. [11] Greg Morrisett, David Walker, Karl Crary, and Neal \nGlew. From System F to typed assembly language. ACM Transac\u00adtions on Programming Languages and Systems, \n21(3):527 568, 1999. [12] George C. Necula and Peter Lee. The design and implemen\u00adtation of a certifying \ncompiler. In Keith D. Cooper, editor, Proceedings of the Conference on Programming Language Design and \nImplementation (PLDI 98), pages 333 344, Mon\u00adtreal, Canada, June 1998. ACM Press. [13] Leaf Petersen, \nRobert Harper, Karl Crary, and Frank Pfen\u00adning. A type theory for memory allocation and data layout. \nTechnical Report CMU-CS-02-171, Department of Computer Science, Carnegie Mellon University, December \n2002. [14] Frank Pfenning and Rowan Davies. A judgmental reconstruc\u00adtion of modal logic. Mathematical \nStructures in Computer Science, 11(4):511 540, 2001. [15] Jeff Polakow. Ordered linear logic and applications. \nPhD thesis, Carnegie Mellon University, June 2001. Available as Technical Report CMU-CS-01-152. [16] \nJeff Polakow and Frank Pfenning. Natural deduction for in\u00adtuitionistic non-commutative linear logic. \nIn J.-Y. Girard, editor, Proceedings of the 4th International Conference on Typed Lambda Calculi and \nApplications (TLCA 99), pages 295 309, L Aquila, Italy, April 1999. Springer-Verlag LNCS 1581. [17] Jeff \nPolakow and Frank Pfenning. Relating natural deduction and sequent calculus for intuitionistic non-commutative \nlin\u00adear logic. In Andre Scedrov and Achim Jung, editors, Pro\u00adceedings of the 15th Conference on Mathematical \nFounda\u00adtions of Programming Semantics, New Orleans, Louisiana, April 1999. Electronic Notes in Theoretical \nComputer Sci\u00adence, Volume 20. [18] Jeff Polakow and Frank Pfenning. Properties of terms in continuation-passing \nstyle in an ordered logical framework. In J.Despeyroux, editor, Proceedings of the 4th Interna\u00adtional \nConference on Typed Lambda Calculi and Applications (TLCA 99), Santa Barbara, California, June 2000. \n[19] Zhong Shao. An overview of the FLINT/ML compiler. In 1997 Workshop on Types in Compilation, Amsterdam, \nJune 1997. ACM SIGPLAN. Published as Boston College Com\u00adputer Science Department Technical Report BCCS-97-03. \n[20] Frederick Smith, David Walker, and Greg Morrisett. Alias types. Lecture Notes in Computer Science, \n1782, 2000. [21] David Tarditi, Greg Morrisett, Perry Cheng, Chris Stone, Robert Harper, and Peter Lee. \nTIL: A type-directed opti\u00admizing compiler for ML. In ACM SIGPLAN Conference on Programming Language Design \nand Implementation, pages 181 192, Philadelphia, PA, May 1996. [22] David N. Turner and Philip Wadler. \nOperational interpreta\u00adtions of linear logic. Theoretical Computer Science, 227(1 2):231 248, 1999. [23] \nHongwei Xi and Frank Pfenning. Eliminating array bound checking through dependent types. In Keith D. \nCooper, editor, Proceedings of the Conference on Programming Language Design and Implementation (PLDI \n98), pages 249 257, Mon\u00adtreal, Canada, June 1998. ACM Press.  A Static semantics De.nitions 0 t= 12)def \n.2 if .1 =\u00b7 (.1,.= tn+1 = t tna:t,(.' 1,.2) if .1 =a:t,.' 1 Well-formed contexts and frontier fG, f., \nf.: . ft: Treg fG (x ./G) f. (a ./.) fval V : t f.: .(a ./.) f\u00b7 fx:t,G f\u00b7 fa:t,. f\u00b7: \u00b7fa .: a:t,. .V,. \n Register and heap types ft: T, ft: Th reg ft: Th ft1: Treg ft2: Treg ft1: Th ft2: Th ft: Treg fint \n: Treg fNS : Treg f! t: Treg ft1 .t2: Treg f1: Th ft1 t2: Th ft: Th Coercion terms .fQ : t crc .1 fcrc \nQ1: t1 .2 fcrc Q2: t2 a:tfcrc a : t \u00b7fcrc *:1 .1,.2 fcrc Q1 Q2: t1 t2 Terms G;.fM : t trm ' G(x)=t fval \nV : t ft: Treg G,x:t;.fexp E : t ' G;\u00b7ftrm x : tG;\u00b7ftrm n : int G;\u00b7ftrm ns : NS G;\u00b7ftrm !V :!tG;.ftrm \n.(x:t).E : t.t Values fV : t val ' fval V : t ft: Treg x:t;\u00b7fexp E : tfval V1: t1 fval V2: t2 ' fval \nn : int fval ns : NS fval !V :!t fval .(x:t).E : t.tfval *:1 fval V1 V2: t1 t2 Expressions G;.fE : t \nexp G;\u00b7ftrm M : tG;.ftrm M1: t1 .t2 G;\u00b7ftrm M2: t1 G;.fexp E1: t1 G,x:t1;\u00b7fexp E2: t2 G;\u00b7fexp ret M \n: tG;.fexp M1 M2: t2 G;.fexp let x:t1 =E1 in E2: t2 ' G;a:NSn fexp E : t.L fcrc Q : tG, x:!t;.R fexp \nE : t ' G;.fexp reserven as ain E : tG;.L,.R fexp alloc Q as x in E : t '' '' .fcrc Q : t ft: Treg G;\u00b7ftrm \nM : tG;.L,a:t,.R fexp E : t.fcrc Q :1 G;.L,.R fexp E : t '' G;.L,.,.R fexp Q :=M as ain E : tG;.L,.,.R \nfexp let *=Qin E : t ' .fcrc Q : tG;.1,a:t,.2 fexp E : t.fcrc Q : t1 t2 G;.1,a1:t1,a2:t2,.2 fexp E : \nt ' G;.1,.,.2 fexp let a = Qin E : tG;.1,.,.2 fexp let a1 a2 = Qin E : t G;\u00b7ftrm M :!(t1 t2) G,x1:! \nt1,x2:!t2;.fexp E : tG;\u00b7ftrm M :!t1 ft1: Treg G,x:t1,;.fexp E : t2 G;.fexp let! x1 x2 = M in E : tG;.fexp \nlet !x = M in E : t2 B Dynamic semantics De.nitions *[\u00b7]= * V 0 = * 2) def .if .1 = \u00b7 (.1,.= 2 ' a[a \n..V ]= V ' Vn+1 = V Vn .1,.if ..a .V,(.2) 1 = a .V,. 1 (Q1 Q2)[.1,.2]= Q1[.1] Q1[.2] '' Expressions \n(.,E) ..(.,E) (.,(.(x:t).E)Mv) ..(.,E[Mv/x]) ' (.,E1) ..(.,E1') '  (.,let x:t= E1 in E2) .,let x:t1 \nin E2)(\u00b7,let x:t= ret Mv in E) ., E[Mv/x]) .(.= E '.(\u00b7 Qv[.1]= V n (.,reserven asain E) ..(a ..ns ,E) \n((.1,.2),alloc Qv as x in E ..(.2,E[!V /x]) ' 1,a .V,..1,a .Mv,.2),E) ((..2),a := Mv as a ' in E) .((..(.,let \na = Qv in E) ..(., E[Qv/a]) (.,let a1 a2 = Q1 Q2 in E) ., E[Q1,Q2/a1,a2]) .(. ((..2),let a1 a2 = ain \nE) ..((... 1,a .V1 V2,.1,a1 .V1,a2 .V2,.2),E) (.,let *= *in E) ..(., E) ((.1,a ..*,.2),let *= ain E) \n..((.1,.2),E) (.,let! x1 x2 = !(V1 V2)in E) ., E[!V1,!V2/x1,x2]) (.,let !x = !V in E) .(.,E[V /x]) .(.. \n \n\t\t\t", "proc_id": "604131", "abstract": "Ordered type theory is an extension of linear type theory in which variables in the context may be neither dropped nor re-ordered. This restriction gives rise to a natural notion of <i>adjacency</i>. We show that a language based on ordered types can use this property to give an exact account of the layout of data in memory. The fuse constructor from ordered logic describes adjacency of values in memory, and the mobility modal describes pointers into the heap. We choose a particular allocation model based on a common implementation scheme for copying garbage collection and show how this permits us to separate out the allocation and initialization of memory locations in such a way as to account for optimizations such as the coalescing of multiple calls to the allocator.", "authors": [{"name": "Leaf Petersen", "author_profile_id": "81100348011", "affiliation": "Carnegie Mellon University, Pittsburgh, PA", "person_id": "PP31038281", "email_address": "", "orcid_id": ""}, {"name": "Robert Harper", "author_profile_id": "81100140064", "affiliation": "Carnegie Mellon University, Pittsburgh, PA", "person_id": "PP39029370", "email_address": "", "orcid_id": ""}, {"name": "Karl Crary", "author_profile_id": "81100253026", "affiliation": "Carnegie Mellon University, Pittsburgh, PA", "person_id": "P157139", "email_address": "", "orcid_id": ""}, {"name": "Frank Pfenning", "author_profile_id": "81100157780", "affiliation": "Carnegie Mellon University, Pittsburgh, PA", "person_id": "PP39030152", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/604131.604147", "year": "2003", "article_id": "604147", "conference": "POPL", "title": "A type theory for memory allocation and data layout", "url": "http://dl.acm.org/citation.cfm?id=604147"}