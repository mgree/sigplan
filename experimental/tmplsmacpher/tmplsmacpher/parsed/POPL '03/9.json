{"article_publication_date": "01-15-2003", "fulltext": "\n Folklore Con.rmed: Reducible Flow Graphs are Exponentially Larger * Larry Carter Computer Sci. &#38; \nEng. Dept. UC San Diego 9500 Gilman Drive La Jolla, CA 92093-0114 USA  carter@cs.ucsd.edu Jeanne Ferrante \nComputer Sci. &#38; Eng. Dept. UC San Diego 9500 Gilman Drive La Jolla, CA 92093-0114 USA  ferrante@cs.ucsd.edu \nABSTRACT Many program analysis techniques used by compilers are applicable only to programs whose control \n.ow graphs are reducible. Node-splitting is a technique that can be used to convert any control .ow graph \nto a reducible one. However, as has been observed for various node-splitting algorithms, there can be \nan exponential blowup in the size of the graph. We prove that exponential blowup is unavoidable. In par\u00adticular, \nwe show that any reducible graph that is equivalent to the complete graph on n nodes (or to related bounded\u00addegree \ncontrol .ow graphs) must have at least 2n-1 nodes. While this result is not a surprise, it may be relevant \nto the quest for .nding methods of obfuscation for software protection. Categories and Subject Descriptors \nD.3 [Programming Languages]: Processors  General Terms Algorithms, Security, Languages, Theory Keywords \nCompilers, computational complexity, programming languages, safety/security in digital systems 1. INTRODUCTION \nControl .ow graphs [1] are frequently used in optimizing compilers as the basis of program analysis and \noptimiza\u00adtion. Intuitively, a control .ow graph of a program is a directed graph that represents the \nprogram s potential .ow of control. Reducible .ow graphs, de.ned in Section 2, are * Work performed while \nthe .rst two authors were on sab\u00adbatical at University of Auckland. Permission to make digital or hard \ncopies of all or part of this work for personal or classroom use is granted without fee provided that \ncopies are not made or distributed for pro.t or commercial advantage and that copies bear this notice \nand the full citation on the .rst page. To copy otherwise, to republish, to post on servers or to redistribute \nto lists, requires prior speci.c permission and/or a fee. POPL 03 January 15 17, 2003, New Orleans, Louisiana, \nUSA. Copyright 2003 ACM 1-58113-628-5/03/0001 ...$5.00. Clark Thomborson Dept. of Computer Science University \nof Auckland Private Bag 92019, Auckland New Zealand cthombor@cs.auckland.ac.nz a subclass of control \n.ow graphs that include all those de\u00adrived from structured programs, i.e., programs generated using a \nrestricted set of constructs such as if-then-else, while-do, continue and break. Many static analyses \n[1, 13] either require the code to have a reducible .ow graph, or may be too costly to analyze when applied \nto a large, irre\u00adducible graph. As noted in [13], reducible .ow graphs allow a divide-and-conquer approach \n(such as interval analysis) which reduces the complexity of the analysis. Nevertheless, irreducible .ow \ngraphs do arise, often through the use of go to constructs, and must be handled by optimizing compil\u00aders. \nThey also appear in the machine code generated by some optimizing compilers. The most commonly used technique1 \nfor handling irre\u00adducible .ow graphs is node-splitting [1, 13, 2, 9]. This technique eliminates unstructured \nprogram constructs by making duplicate copies of selected nodes in the control .ow graph. Figure 1 gives \nan example. As has been observed in the literature that for various node splitting algorithms, there \nis an exponential blowup in the size of the program or .ow graph.2 However, there are di.erent ways to \nchoose which nodes to split, and to choose an order of splitting these nodes [14, 16]. This leaves open \ntwo questions, which we have been unable to .nd answered in the published literature. Is it possible \nthat some (perhaps as yet undiscovered) node splitting algorithm avoids exponential blowup?  Is it possible \nthat for any .ow graph, there is an equiv\u00adalent, reducible .ow graph (even one that might not be the \nresult of a node splitting algorithm) that avoids exponential blowup?  This paper answers both questions \nnegatively. Although this will not come as a surprise to people familiar with the .eld, it .lls in a \ngap that has been considered folklore . Until now, there was little concern about the possible blowup, \nsince it has often been observed that most programs written by humans are reducible [1]. However, we \nare motivated to 1Another technique adds extra predicate variables to guard statements in a loop, e.g. \n[5, 6, 12, 4]. Although the resulting program is in some sense the same, it is not equivalent in the \ntechnical sense de.ned later in this paper. Thus, our re\u00adsults do not apply to such program restructuring \ntechniques. 2See, for example, [1], p. 680 and [15], p. 197. e: if (cond) goto b a: x = .... b: y \n= ... goto a  Figure 1: On the left is an unstructured program and the corresponding irreducible control \n.ow graph. The right .gure is an equivalent structured program and its (reducible) .ow graph. take another \nlook at this question by our interest in software obfuscation [10, 11]. A software obfuscator is an automatic \ntechnique that trans\u00adforms programs into functionally equivalent variants that are more di.cult to understand \nor analyze [10, 11]. Obfusca\u00adtion can be shown to be theoretically impossible [7], given enough computational \npower and memory. This has not de\u00adterred attempts to construct obfuscators that have embed\u00added instances \nof hard problems. For example, [8] shows that an instance of a PSPACE-hard problem may be embedded in \nthe control .ow of the obfuscated program. As another ex\u00adample, [17] introduce additional variable aliases \nby indirect addressing, making control .ow di.cult to determine. This latter work uses the fact that \nalias detection is an NP-hard problem. We, too, have explored making the control .ow of a pro\u00adgram harder \nto analyze. A resourceful attacker can be ex\u00adpected to attempt to automatically decompile the target \ncode, and to perform other automated analyses. As noted in [8], such automatic analysis can statically \nobtain global information about the structure of the program, enabling re\u00adverse engineering. Thus, our \nhope is to defend against this attack by obfuscating the control .ow (or data .ow) of the original program. \nThis can be done by adding extra edges to make the control .ow graph irreducible. This can be ac\u00adcomplished \nby inserting extra conditional branches into the program. If the conditional branches use values not \nknown until run time, or perhaps use mathematical identities that are beyond the scope of an optimizing \ncompiler to verify, then static analysis would be unable to eliminate them. While we were able to develop \ntechniques for construct\u00ading a complete (irreducible) .ow graph from the original .ow graph of a program, \nthus confounding static analysis, we were unable to .nd techniques that couldn t easily be un\u00adscrambled \nby symbolic execution or by analyzing a runtime trace. Our main result is that for any code whose .ow \ngraph is equivalent to the complete graph with n nodes, any equiv\u00adalent reducible .ow graph must have \nat least 2n-1 nodes. A reducible .ow graph of exponential size is unlikely to be of much use to an attacker. \nStarting with a program of modest size, the reducible equivalent may not even .t in a computer s memory, \nand its sheer mass will frustrate any human s attempt to understand what the code is doing. 2. REDUCIBLE \nFLOW GRAPHS The term reducible .ow graph,3 originally de.ned in [3], means that the graph can be reduced \nto a single node by a sequence of applications of the two transformations, T1 and T2, shown in Figure \n2. In the context of data .ow analysis, .ow graphs are annotated with information on the nodes (such \nas There is an assignment to X and a use of Y in this block of code. ) Many data .ow algorithms work \nby manipulating this information appropriately as the graph is reduced via T1 and T2, producing summary \ninformation about the program (such as The assignment to X in Block 3is usedinBlocks 4 and8, and The \nuse of Y in block 3 may be uninitialized .) The details of these algorithms need not concern us; however, \nthey motivate the following de.nitions: Definition 1. A .ow graph is a directed graph with adistinguished \ninitial node n0, together with a function L that assigns to each node of the graph a label chosen from \nan alphabet U. The labels need not be unique. Definition 2. A .nite string of labels sis said to be pro\u00adduced \nby a .ow graph G if s = L(n0)L(n1)L(n2) ...L(nk) where n0,n1,n2...,nk is a path in G beginning with the \nini\u00adtial node. L(G) denotes the language of G, i.e. the set of strings produced by G. 4 Definition 3. \nTwo .ow graphs G1 and G2 are equiva\u00adlent if L(G1)= L(G2). For example, the two .ow graphs of Figure 1 \nare equiva\u00adlent. In general, node splitting techniques transform an arbi\u00adtrary .ow graph into an equivalent \nreducible .ow graph. We note that .ow graphs and the set of strings they produce are very similar to \n.nite automata and regular languages the di.erence is that .ow graphs have labeled nodes, whereas .nite \nautomata have labeled edges. In fact, the language of a .ow graph G is regular,5 but .ow graphs produce \nmore 3There are several equivalent de.nitions of reducible .ow graph; see [13, 1, 14]. 4We overload the \nsymbol Lto mean, in di.erent contexts, the label of a node, the language of a .ow graph, or other ways \nof associating a set of strings to various objects (such as leftist trees or annotated .ow graphs) that \nwill come later. 5If the label of each node is put on each outgoing edge, the resulting .nite automaton \nproduces L(G).  Figure 2: Two transformations used to simplify .ow graphs. T1 eliminates self-loops. \nT2 merges a single\u00adentry node into its parent. If there is an arc from the child back to the parent, \nit becomes a self-loop. A graph G is reducible if repeated applications of T1 and T2 reduce G to a single \nnode. restricted languages for instance, every string in L(G) must begin with the label of the initial \nnode. Before continuing with our study of .ow graphs, we intro\u00adduce leftist trees, which provide another \nway of producing a set of strings. We then prove a lower bound on the size of any leftist tree that produces \nthe set of all strings over a given alphabet that begin with a given symbol. This result will allow us \nto prove a similar result for .ow graphs. 3. LEFTIST TREES A leftist tree T is simply a rooted binary \ntree (i.e., each node has either zero or two children), with labels on the leaves. |T| denotes the number \nof nodes in T.Welet Tl and Tr denote the left and right subtrees6 of T.Note that Tl and Tr are either \nboth empty or both non-empty. De.ne nl(T), nr(T) as follows: if T is a single node, both these denote \nthat node. Otherwise, they are .rst (leftmost) leaf of Tl and Tr, respectively. Finally, let sl(T)and \nsr(T)bethe labels of nl(T)and nr(T), respectively. Whenever the tree T is understood from context, we \nmay drop the (T) from our notation. Thus, for instance, Tl, nl, and sl denote the left subtree of T (or \nT itself if |T| =1), the leftmost leaf of that subtree, and the symbol labeling that leaf. We now give \na method of associating a set of strings with a leftist tree. If P = p1p2 ...pk is a non-empty sequence \nof leaves of tree T,then wesay P is a leftist leaf sequence (LLS) of T if for all i<k, there exists a \nsubtree Ti of T such that pi . Ti and pi+1 is either nl(Ti)or nr(Ti). Intuitively, starting from pi, \nyou can move up the tree as far as you want, go down to either child, but then you must keep going left \nuntil you get to the leaf pi+1. As an example, see Figure 3. Given a LLS P = p1p2 ...pk of T, we say \na string of symbols s = s1s2 ...sk is produced by P if for i =1,...,k, si is the label of pi. Finally, \nwe de.ne the language of T, denoted, L(T), to be the set of strings produced by LLS s in T that start \nwith nl(T), the leftmost leaf of T. The following three observations follow easily from the de.nition \nof leftist leaf sequence. 6By a subtree we mean a node together with all of the nodes that it is an ancestor \nof. n1 n3 n0 n2  Figure 3: For this tree, the sequence n0n2n2n1n0n1 is a legal leftist leaf sequence, \nbut n0n3 is not. Lemma 1. If P is a sequence of leaves of a subtree T. of T,then P is aLLS of T. if and \nonly ifitis aLLS of T. Lemma 2. If p1 ...pj and pj+1 ...pk are LLS s of T,and pj+1 is either nl or nr,then \np1 ...pjpj+1 ...pk is a LLS of T. Lemma 3. If p1 ...pjpj+1 ...pk is a LLS of T and pj+1 is in some subtree \nT. that doesn t include pj,then pj+1 must be nl(T'). We now can prove an alternate characterization of \nthe set of strings that are produced by a tree T. Theorem 1. L(T) can be expressed recursively by L(T)= \nsl(T)+ if |T| =1 and L(T)= L(Tl)(L(Tl) . L(Tr)) * other\u00adwise.7 Proof. By induction on |T|. (Base case) \nIf |T| = 1, i.e., if T is a single node nl,thenby letting Ti = T, we see that any sequence of one or \nmore nl s is a LLS starting with nl. Together these LLS s produce the language sl(T)+ . (Inductive step) \nAssume that the theorem holds for Tl and Tr.Given any s. L(Tl)(L(Tl) . L(Tr)) * , we can .nd a LLS in \nTl or Tr for each of the substrings that are implied by this formula. By Lemma 1, these LLS s are also \nLLS s 7We use standard notation: if x and y are strings, then xy is their concatenation; if A and B are \nsets of strings, then AB represents the set {xy|x . A,y . B}, A * is the set of strings that are the \nconcatenation of zero or more strings in A,and A+ = AA * . of T. Since each such LLS begins with either \nnl(Tl)= nl or nl(Tr)= nr, their concatenation is an LLS by Lemma 2. It is also clear that this sequence \nbegins with nl.Thus, s is in L(T). Conversely, suppose s . L(T). Then s is generated by a LLS P of T \nthat begins with nl. We can decompose P into subsequences according to when P switches from Tl to Tr \nor back again. By Lemma 1, these subsequences are SSL s of Tl or Tr. By Lemma 3, each such subsequence \nbegins with nl or nr. By the inductive hypothesis, they produce substrings in L(Tl)or L(Tr). Since T \nbegins with nl,the .rst substring is in L(Tl). Thus s, which is the concatenation of the substrings, \nis in L(Tl)(L(Tl) . L(Tr)) * . A simple consequence of Theorem 1 is that for any leftist tree T, L(T)+ \n= L(T). We need a few more de.nitions: Definition 4. We say that a string tis a su.x of L(T) if .s. U \n* such that st. L(T). Definition 5. Given an alphabet S . U, we say that T is S-complete if .t. S * , \nt is a su.x of L(T). In our examples, the structure of a leftist tree T will be presented using parentheses \nrecursively. We write T = s if T is a single node labeled by s,and T =(TlTr)if |T| > 1. For example, \nT =(a(bc)) is a tree whose left subtree is a and whose right subtree is (bc). Example 1. Let T =(ab). \nThen L(T)= a{a,b} * ,thus T is S-complete for S = {a,b}. Example 2. Let T =(a(bc)). Then L(T)= a(a * \n. b{b,c} * ) * . The string ac is not a su.x of L(T), so T is not complete for S = {a,b,c}. However T \nis both {a,b}\u00adcomplete and {b,c}-complete. Example 3. T =((ab)(cb)) is {a,b,c}-complete. Another example \nof a S-complete tree is shown in Fig\u00adure 4. Lemma 4. Astring t is a su.x of L(T) if and only if t is \nproduced by a LLS of T (with no constraint on the .rst node of the LLS). The only non-trivial part of \nthe proof is to show that you can construct a LLS from nl(T) to the .rst symbol in the string. This can \nbe done by using the leftmost leaves of the trees rooted at the nodes that form a path from the root \nof T to the leaf corresponding the .rst symbol of t. The next two lemmas provide insight into the structure \nof S-complete trees. Lemma 5. Given an alphabet S,if T is a S-complete tree of minimal size, then its \nright subtree Tr is (S-sl)-complete. Proof. By contradiction. Assume Tr is not complete over the reduced \nalphabet (S - sl). Then there exists a string u . (S - sl)+ such that u is not a su.x of L(Tr). Because \nT is of minimal size, Tl is not S-complete, so there exists a shortest string t in S+ that is not a su.x \nof Tl. Let j and k be the lengths of t and u.Because T is S\u00adcomplete, we know that tu is a su.x of L(T). \nDenote tu by the string v = v1v2 ...vj+k. By Lemma 4, theremustbea LLS p1p2 ...pjpj+1 ...pj+k of T that \nproduces v. We will derive a contradiction by considering where pj, the leaf corresponding to the last \nsymbol of t,is. First, suppose that pj . Tr. For all i>j, vi = sl,since vi is a symbol in uand ucontains \nno sl s. Thus, applying Lemma 3 inductively tells us that for all i>j, pi must be a leaf of Tr. But this \nwould mean that the LLS pj+1 ...pj+k produces u. This contradicts that we chose u to not be a su.x of \nL(Tr). On the other hand, suppose that pj . Tl.By construction, p1 ...pj is not a LLS of Tl (since t \nis not a su.x of Tl), so theremust besome nodes of p1 ...pj-1 in Tr.Choose m to be the largest integer \nm<j such that pm . Tr.This ensures that pm+1pm+2 ...pj is a LLS of Tl.Furthermore, since pm . Tr and \npm+1 . Tl,wemust have vm+1 = sl by Lemma 3. Now we use the fact that tis a shortest non-su.x of Tl to \nconclude that there must be some LLS p1p2 ...pm in Tl that produces t1t2 ...tm. Now consider the sequence \np . 1 ...p . mpm+1 ...pj. By construction, it is entirely in Tl and it produces t. It also is a LLS by \nLemma 2. Thus, we conclude that tis produced by a LLS in Tl, which contradicts the fact that it is not \nasu.xof Tl. Lemma 6. Given an alphabet S,if T is a S-complete tree of minimal size, then its left subtree \nTl is (S - sr)-complete. Proof. The proof is the same as for Lemma 5, with the roles of Tl and Tr reversed. \nThe abbreviated proof follows. Suppose there were a string u . (S - sr)+ of length j that is not a su.x \nof L(Tl). Tr is not S-complete, so there is a shortest string t . S+ that is not a su.x of Tl.Let p1p2 \n...pjpj+1 ...pj+k bea LLSof T that produces tu. pj must be in Tr; otherwise, the remainder of the leaf \nsequence would be stuck in Tl and produce u. We .nd m<jsuch that pm+1 ...pj is a LLS of Tr. The initial \nsubstring p1 ...pm, being shorter than t, has a LLS in Tr. Extending this LLS with pm+1 ...pj gives a \nLLS in Tr that produces t a contradiction. Lemmas 5 and 6 enable us to prove a lower bound on the size \nof S-complete trees. Theorem 2. Let Sn be the minimal number of leaves in a S-complete tree with |S| \n= n.Then Sn = 2n-1 . Proof. By induction on n. The induction basis n =1 is established by noting that \nT = ais the minimal-sized tree to generate the 1-symbol complete language L(T)= a +.The inductive step \nSn = 2Sn-1 is established by considering the subtrees Tl and Tr for a S-complete tree T of minimal size \nwith |S| = n. By Lemma 6, the subtree Tl is (ST - sr)\u00adcomplete, so by the inductive hypothesis it has \nat least 2n-2 leaves. By Lemma 5, the subtree Tr is (S - sl) complete, so by induction it has at least \n2n-2 leaves. This gives us the desired inequality. Theorem 3. Sn =2n-1 . Proof. A recursive construction \nwill match the bound of Theorem 2. Our basis, for alphabet S = {x1},is the one-node tree T = x1. We build \na tree T with 2n-1 leaves for alphabet S = {x1,x2,...,xn} from a left subtree of size Sn-1 with L(Tl)= \nx1(S - xn) * and a right subtree also of size Sn-1 with L(Tr)= xn(S - x1) * . The resulting lan\u00adguage \nL(T)is, by Theorem 1, L(Tl)(L(Tl) . L(Tr)) * ,which is x1(S-xn) * (x1(S-xn) * .xn(S-x1) * ) * = x1(S) \n* .Thus, T is S-complete and has 2n-1 leaves. Figure 4: This tree is S-complete for S= {e, a, b, c}, \nand illustrates a recursive construction for such trees that have 2n-1 leaves when S is an alphabet of \nsize n. 4. EXPONENTIAL BLOWUP Given a .nite alphabet S of size n and e . S, let KS,e be the .ow graph \ncomprising the complete directed graph on n nodes, where each node is labeled with a distinct element \nof S and e is the label of the initial node. Let L(KS,e)be the language produced by KS,e. It s not hard \nto see that L(KS,e) is the regular language eS * , that is, the set of all strings over S that begin \nwith e . In this section, we will prove the main result of this paper: Theorem 4. If R is a reducible \n.ow graph that is equiv\u00adalent to KS,e (i.e. that is, if L(R)= eS * ), then R must have at least 2n-1 \nnodes, where n is the cardinality of S. Proof. We .rst show how to associate a leftist tree T with any \nreducible .ow graph R.Each leaf of T will corre\u00adspondtoadistinct nodeof R,and L(T ) will be a superset \nof L(R). We do this by annotating each node ni of R with a leftist tree Tni . As we reduce R using the \ntransformations T1 and T2, we will build up the trees that annotate the nodes. By the time R has been \nreduced to a single node, its associated tree will be the desired leftist tree. To each intermediate \nannotated .ow graph there corresponds a lan\u00adguage in the natural way it is the set of strings of the \nform s1s2...sk,where si . L(Tni )and n1,n2, ... is a path through the .ow graph. Over the course of the \nconstruction, the following invari\u00adants will hold: 1. The language associated with the current annotated \n.ow graph is a superset of L(R). 2. There is a one-to-one correspondence between the nodes of R and \nthe leaves in a forest formed of all the leftist trees in the annotations.  The .rst step of the construction \nis to annotate each node ni of R with the leftist tree Tni consisting of a single (leaf) node labeled \nby L(ni), the label of ni.It is easy to verify that properties (1) and (2) hold initially. Whenever a \nT1 (self-loop elimination) transformation is applied to a node ni of an annotated graph, we don t need \nto adjust the annotations. Property (1) will still hold since the only paths through the original .ow \ngraph that are no longer paths of the transformed one include repetitions of node ni, but the fact that \nL(Tn)+ = L(Tn) ensures that arbitrary repetitions are represented even after eliminating the self-loop. \nA T2 transformation can be applied to nodes n1 and n2 only when the edge (n1,n2) is the only edge into \nn2.When a T2 transformation is applied, we fuse the two nodes into a single node and update its annotation \nas shown in Figure 5. In any path through the pre-T2 .ow graph, every oc\u00adcurrence of n2 must be immediately \npreceded by an n1. Thus, for each substring si-1si where si-1 . L(Tn1 )and si . L(Tn2 ) that contributes \nto strings in the pre-T2 lan\u00adguage, that same substring will be included in L(Tn1/n2 in the post-T2 .ow \ngraph. Thus, property (1) holds.8 Property (2) also remains vaild throughout the construction because \nthe T1 and T1 transformations do not a.ect the total num\u00adber of leaves in the forest of annotations. \nFor example, in Figure 6, the annotation forest always has four leaves (e, a, b, and c). Once we have \nreduced R to a single node via T1 and T2, the annotation of that node is T , the leftist tree we asso\u00adciate \nwith R.As noted, L(T ) may contain strings not in L(R). However, if R is equivalent to KS,e, then its \nlan\u00adguage already contains all strings starting with e.Thus, L(T )= L(R)= eS * . By Theorem 2, T has \nat least 2n-1 leaves. Since there is a one-to-one correspondence between the leaves of T and the nodes \nof R, the proof is completed. 8New strings may be have been added to the language; for instance, in the \nexample there was no edge from n1 to n4 or from n2 to n3 before T2 was applied. n3  Figure 5: AfteraT2 \ntransformation, the annotation of the new fused node is a tree that has the old annotations as its left \nand right subtrees. In Figure 6, we show the steps of transforming the exam\u00adple graph, resulting in its \nleftist tree. Theorem 4 settles the two open questions posed in Section 1 negatively. In the next section, \nwe will extend our results to show that even .ow graphs which have outdegree 2 su.er from exponential \nblowup when converted to reducible .ow graphs. 5. LIMITING OUTDEGREE Suppose ni is a node in a .ow graph \nthat has outdegree k>1. We can replace ni with a tree of k- 1nodes, where each node in the new tree has \noutdegree exactly 2. We begin by creating a new tree of k- 1 nodes in which the outdegree of each node \nis at most 2, and each new node is given the same label as the original node, L(ni). We direct the edges \nof this new tree from its root towards its leaves. Now attach all of the edges that went into ni to the \nroot of this new tree, and attach the k outedges that originally came from ni to nodes in the new tree \nin a way that makes each node has outdegree exactly 2. This construction is analogous to implementing \na multi-way case statement as a sequence of binary branches. It doesn t matter for our purposes whether \nthe tree is balanced or not. Given KS,e, the complete .ow graph on n nodes used earlier, if we apply \nthis procedure to every node, we will obtain a .ow graph JS,e that has n(n- 1) nodes, each of outdegree \n2. We will show that any reducible .ow graph that is equivalent to JS,e has at least 2n-1 nodes. Figure \n7 shows this construction for S = {e,a,b,c}. The language of JS,e is related to S * in that for each \nstring . r1 r2 s = s1s2 ...sk . S * , there is a string s = s1 s2 ...skrk . L(JS,e). In other words, \nthe symbols of s can be duplicated a certain number of times9 to produce s . . We will show that any \nreducible .ow graph that produces 9The number of repetitions depends on the method used for expanding \nnodes of KS,e into binary trees. 2n-1 L(JS,e) (or a superset thereof) must have at least nodes, where \nn is the size of S. We will do so by prov\u00ad ing results similar to Lemmas 5 and 6 about the structure \nof any reducible graph producing such a language. First, we need some more de.nitions. Given two strings \nu and u ',wesay that u is a contraction of u . if uis the result of eliminating some adjacent duplicates \nin u . . For instance, abbac is a contraction of aaabbacc. However, abba is not, since a contraction \nmust contain at least one symbol from each string of duplicates. Given a leftist tree T over an alphabet \nS, and a set of strings S . S * ,we say that T covers S if for every s . S, there exists a su.x s of \nT such that s is a contraction of s . Lemma 7. Given an alphabet S,if T is a minimum-size leftist tree \nthat covers S * , then its right subtree Tr covers (S - sl) * . Proof. By contradiction. Assume Tr does \nnot cover (S- sl) * . Then there exists a string u . (S - sl)+ such that u is not a contraction of any \nsu.x u . of L(Tr). Because T is of minimal size, Tl does not cover S * . Consequently, there exists a \nshortest string t in S+ that is not a contraction of any su.x of Tl.Because T covers S * , we know that \ntu is a contraction of some su.x v of L(T). We can therefore write v = v1v2 ...vj vj+1 ...vj+k,where \nt is a contraction of v1v2 ...vj and u is a contraction of vj+1 ...vj+k.By Lemma 4, there must be a LLS \np1p2 ...pjpj+1 ...pj+k of T that produces v. We need to handle a messy detail. It could happen that \nfor some i, vi = sl and pi+1 = nl. 10 Whenever this hap\u00ad pens, we replace pi by nl. Notice that the modi.ed \nstring ...pi-1nlpi+1 ... is still a LLS, since nl is allowed to follow any node (and in particular pi-1), \nand nl to pi+1 is a valid 10This condition means that v has two adjacent occurrences of the label of \nT s leftmost node, and the LLS that produces v uses that leftmost node to produce the second occurrence. \n Figure 6: This examplegraph is successivelytransformedby T2 and T1 transformations, resulting in its \nleftist tree. Figure 7: On the left is a .ow graph with outdegree 3. The right .gure is an equivalent \n.ow graph of outdegree 2 that would result by applying our procedure. transition since they are the same \nnode. After making as many such substitutions as possible, we can assume with\u00adout loss of generality \nthat whenever the sequence p1p2 ... moves from Tr to Tl, that is when pi . Tr and pi+1 = nl, then vi \nWe will need this property in a moment. = sl. We can now mimic the proof of Lemma 5 to derive a contradiction. \nConsider where pj, a leaf of T corresponding to the last symbol of t,isin T. First, suppose that pj . \nTr. Note that for all i>j, vi = sl,since vi is a symbol in u and ucontains no sl s. Thus, applying Lemma \n3 inductively tells us that for all i>j, pi must be a leaf of Tr. But this would mean that vj+1 ...vj+k \nis a su.x of L(Tr). Summarizing, ' we know u is a contraction of the string u= vj+1 ...vj+k, ' and \nthat uis a su.x of L(Tr). This contradicts how we chose u. On the other hand, suppose that pj . Tl.By \nconstruction, p1 ...pj is not a LLS of Tl (since t is not a contraction of asu.x of Tl), so there must \nbe some nodes of p1 ...pj-1 in Tr.Choose m to be the largest integer m<j such that pm . Tr. This ensures \nthat pm+1pm+2 ...pj is a LLS of Tl. Furthermore, since pm . Tr and pm+1 . Tl,wemust have pm+1 = nl by \nLemma 3. By the messy detail given above, we can assume that vm = sl. This ensures that the portion ' \ntof t that is a contraction of v1 ...vm is indeed shorter than t. Now we use the fact that t is a shortest \nnon-su.x of ''' Tl to conclude that there must be some LLS p1p2 ...pm' in ' Tl that produces a string \nthat covers t.Now consider the '' sequence p1 ...pm' pm+1 ...pj. By construction, it is entirely in \nTl and it covers t. It also is a LLS by Lemma 2 and the fact that pm+1 = nl. Thus, we conclude that t \nis covered by a string produced by Tl, which contradicts how t was originally chosen. QED We can also \nprove this analog of Lemma 6 Lemma 8. Given an alphabet S,if T is a minimum-size leftist tree that covers \nS * ,then its left subtree Tl covers (S- sr) * . Proof. The proof is the same as Lemma 7, with the roles \nof Tl and Tr reversed. We note that the corresponding messy detail , where L(ni)= sr and pi+1 = nr is \nhandled by replacing pi with nr. The resulting leaf sequence is still an LLS since pr can legally follow \nany note in T. We are now ready for our .nal result. Theorem 5. If S is an alphabet of size n and R is \na reducible .ow graph that is equivalent to JS,e (or more gen\u00aderally, if L(R) is a superset of L(JS,e)), \nthen R has at least 2n-1 nodes. Proof. Let T be the leftist tree associated with JS,e and T' the tree \nassociated with R, using the construction of The\u00adorem 4. We know that L(T') . L(R) . L(JS,e). Further, \nwe know that for every string s . S * , s is a contraction of a su.x of a string in L(JS,e). Thus, T' \ncovers S * . Using an induction proof as in Theorem 2, Lemmas 7 and 8allow us toconclude that T' has \nat least 2n-1 leaves. Since R hasasmanynodes as T' has leaves, this completes the theorem. 6. CONCLUSION \nWe have formalized and proven a portion of the folklore of compiler analysis. Some .ow graphs, in particular \nthe ones equivalent to the complete .ow graph, have no equiva\u00adlent reducible .ow graph that is not exponential \nin size. In particular, our results show that no node splitting technique can avoid this exponential \nblowup. While there has been little concern about such exponen\u00adtial blowup, since most programs written \nby programmers are reducible [1], automatic compiler analyses would .nd handling such large programs \ndi.cult. Such analyses either require the code to have a reducible .ow graph, or would be very costly \nto analyze when applied to an irreducible graph of very large size. Thus, a line of defense against re\u00adverse \nengineering of programs is to distribute code whose .ow graph is irreducible. Such code is unlikely to \nbe eas\u00adily automatically analyzable, and thus would be di.cult to reverse engineer. We have already noted \nthat the technique of adding guard predicates to obtain a reducible .ow graph from an irre\u00adducible one \nis not covered by our results. However, this technique may also considerably complicate program anal\u00adysis. \nWe leave it as an open question as to whether such analysis can be shown to be provably hard. 7. ACKNOWLEDGMENTS \nWe would like to thank the CSE 238 class at UCSD (in Fall, 2002) for their helpful comments. 8. REFERENCES \n[1] A. V.Aho,R. Sethi,and J. D. Ullman. Compilers: Principles, Techniques and Tools. Addison-Wesley, \n1986. [2] F. Allen and J. Cocke. Graph-theoretic constructs for program control .ow analysis. Technical \nReport RC-3923, IBM Research, 1972. [3] F. E. Allen. Control .ow analysis. ACM Sigplan Notices, 5(7):1 \n19, 1970. [4] R. Allen and K. Kennedy. Optimizing Compilers for Modern Architectures. Morgan Kaufmann, \n2001. [5] R. Allen, K. Kennedy, C. Porter.eld, and J. Warren. Conversion of control dependence to data \ndependence. In Conference Record of the ACM Symposium on the Principles of Programming Languages (POPL), \nJanuary 1983. [6] Z. Ammarguellat. A control-.ow normalization algorithm and its complexity. IEEE Transactions \non Software Engineering, 18(3), Mar. 1992. [7] B.Barak, O.Goldreich,R.Impagliazzo, S. Rudich, A. Sahai, \nS. Vadhan, and K. Yang. On the (im)possibility of obfuscating programs. In Proceedings of CRYPTO, 2001. \n [8] S. Chow, Y. Gu, H. Johnson, and V. Zakharov. An approach to the obfuscation of control-.ow of sequential \ncomputer programs. In Proceedings of ISC, 2001. Published in Springer-Verlag LNCS 2200. [9] J. Cocke \nand R. Miller. Some analysis techniques for optimizing computer programs. In Proceedings of the Second \nIntl. Conf. of Systems Science, January 1969. [10] C. Collberg, C. Thomborson, and D. Low. Breaking \nabstractions and unstructing data structures. In IEEE International Conference on Computer Languages \n(ICCL), May 1998. [11] C. Collberg, C. Thomborson, and D. Low. Manufacturing cheap, resilient and stealthy \nopaque constructs. In Conference Record of the ACM Symposium on the Principles of Programming Languages \n(POPL), January 1998. [12] A. Erosa and L. Hendren. Taming control .ow: A structured approach to eliminating \ngoto statements. In International Conference on Computer Languages, May 1994. [13] M. S. Hecht. Flow \nAnalysis of Computer Programs. North Holland, 1977. [14] J. Janssen and H. Corporaal. Controlled Node \nSplitting. In Proceedings of the 6th International Conference on Compiler Construction, pages 44 58, \nLinkoping, Sweden, Apr. 1996. volume 1060 of Springer Lecture Notes in Computer Science. [15] S. S. Muchnick. \nAdvanced Compiler Design &#38; Implementation. Morgan Kaufmann, 1997. [16] S. Unger and F. Mueller. Handling \nIrreducible Loops: Optimized Node Splitting vs. DJ-Graphs. Technical Report TR 01-146, Institut f. Informatik, \nHumboldt-University, Jan. 2001. [17] C. Wang, J. Hill, J. Knight, and J. Davidson. Software tamper resistance: \nObstructing static analysis of programs. Technical Report CS-2000-12, University of Virginia, 12 2000. \n \n\t\t\t", "proc_id": "604131", "abstract": "Many program analysis techniques used by compilers are applicable only to programs whose control flow graphs are <i>reducible</i>. Node-splitting is a technique that can be used to convert any control flow graph to a reducible one. However, as has been observed for various node-splitting algorithms, there can be an exponential blowup in the size of the graph.We prove that exponential blowup is unavoidable. In particular, we show that any reducible graph that is equivalent to the complete graph on <i>n</i> nodes (or to related bounded-degree control flow graphs) must have at least 2<sup><i>n-1</i></sup> nodes. While this result is not a surprise, it may be relevant to the quest for finding methods of obfuscation for software protection.", "authors": [{"name": "Larry Carter", "author_profile_id": "81392591424", "affiliation": "UC San Diego, La Jolla, CA", "person_id": "PP94030696", "email_address": "", "orcid_id": ""}, {"name": "Jeanne Ferrante", "author_profile_id": "81100357275", "affiliation": "UC San Diego, La Jolla, CA", "person_id": "P137070", "email_address": "", "orcid_id": ""}, {"name": "Clark Thomborson", "author_profile_id": "81100064244", "affiliation": "University of Auckland, Auckland, New Zealand", "person_id": "PP94030285", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/604131.604141", "year": "2003", "article_id": "604141", "conference": "POPL", "title": "Folklore confirmed: reducible flow graphs are exponentially larger", "url": "http://dl.acm.org/citation.cfm?id=604141"}