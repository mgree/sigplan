{"article_publication_date": "01-15-2003", "fulltext": "\n Destructors, Finalizers, and Synchronization Hans-J. Boehm Hewlett-Packard Laboratories 1501 Page Mill \nRd. Palo Alto, CA 94304 Hans Boehm@hp.com  ABSTRACT We compare two di.erent facilities for running \ncleanup ac\u00adtions for objects that are about to reach the end of their life. Destructors, such as we .nd \nin C++, are invoked syn\u00adchronouslywhen an object goes out of scope. Theymake it easier to implement cleanup \nactions for objects of well\u00adknown lifetime, especiallyin the presence of exceptions. Languages like Java[8], \nModula-3[12], and C#[6] provide a di.erent kind of .nalization facility: Cleanup methods maybe run when \nthe garbage collector discovers a heap ob\u00adject to be otherwise inaccessible. Unlike C++ destructors, \nsuch methods run in a separate thread at some much less well-de.ned time. We argue that these are fundamentallydi.erent, \nand po\u00adtentiallycomplementary, language facilities. We also tryto resolve some common misunderstandings \nabout .nalization in the process. In particular: The asynchronous nature of .nalizers is not just an \naccident of implementation or a shortcoming of tracing collectors; it is necessaryfor correctness of \nclient code, fundamentallya.ects how .nalizers must be written, and how .nalization facilities should \nbe presented to the user.  An object maylegitimatelybe .nalized while one of its methods are still running. \nThis should and can be addressed bythe language speci.cation amd client code.  Categories and Subject \nDescriptors D.3.3 [Programming Languages]: Language Con\u00adstructs and Features Dynamic storage management; \nD.4.2 [Operating Systems]: Storage Management Garbage Collection; D.4.1 [Operating Systems]: Process \nManage\u00adment Threads Permission to make digital or hard copies of all or part of this work for personal \nor classroom use is granted without fee provided that copies are not made or distributed for pro.t or \ncommercial advantage and that copies bear this notice and the full citation on the .rst page. To copy \notherwise, to republish, to post on servers or to redistribute to lists, requires prior speci.c permission \nand/or a fee. POPL 03 January 15 17, 2003, New Orleans, Louisiana, USA Copyright 2003 ACM 1-58113-628-5/03/0001 \n...$5.00.  General Terms Languages, design Keywords Deadlock, destructor, .nalization, garbage collection, \nsyn\u00adchronization, thread 1. INTRODUCTION The following sections describe two mechanism for asso\u00adciating \ncleanup code with objects: C++ destructors and Java/Modula-3/C# .nalizers. C++ destructors are cleanup \nactions executed primarily when a program variable goes out of scope. The prevailing opinion is that \nC++ destructors are clearlyuseful. And there is some agreement on how theyshould be used (cf. Resource \nManagement in [14]). Manyprogramming languages that provide automatic, garbage-collected memorymanagement \nalso provide .nal\u00adizers to clean up heap objects sometime before collection. Unlike destructors, .nalizers \nare often viewed as ... un\u00adpredictable, often dangerous, and generallyunnecessary... can cause erratic \nbehavior, poor performance, and portabil\u00adityproblems ([1], taken slightlyout of context, in section entitled \nAvoid .nalizers ). A common view is that the asynchronous nature of .\u00adnalization is a consequence of \nthe use of a tracing garbage collector which detects garbage onlyat periodic intervals, and that it might \nbecome more useful if traditional refer\u00adence counting were used as much as possible, so that most .nalizers \ncould be run synchronously at the precise program point at which an object became inaccessible.1 It has \nalso occasionallybeen claimed that if .nalization must be used, one should avoid locking in .nalizers. \nWe point out that both claims are wrong, though the latter is somewhat moti\u00advated bycommon language implementation \nbugs. We argue that although .nalization is rarelyneeded, it is often critical when it is needed. In \nthese cases, its absence would render the garbage collector useless. Furthermore, although .nalization \ntiming is certainlyindeterminate, it should not lead to unreliable programs. Some of the observations \nunderlying this paper have been previouslymade byothers. We attempt to point out such prior work. But \nwe are not aware of anypublished work that thoroughlydiscusses the implications of those obser\u00advations, \neither on the writing of correct client code, or on 1This has been repeatedlyclaimed in various C++ discus\u00adsion \ngroups, and less frequentlyon gclist@iecc.com. the underlying design of the .nalization facility, and \npartic\u00adularlyon the futilityof designing systems for synchronous .nalization.2 Although our observations \nshould perhaps be obvious, there is no shortage of empirical evidence that theyare not. Nearlyeverynew \nJava implementation seems to initially execute .nalizers synchronously from the garbage-collecting thread, \neven though this is prohibited bythe language spec\u00adi.cation. The current C# speci.cation[6] appears to \nallow .nalizers to be run synchronously3, in spite of the fact that, as we argue below, this makes it \nlargelyimpossible to write deadlock-free .nalizers. A number of common memoryman\u00adagement implementations \nfor C++ (e.g. Boost shared ptr4) implement essentiallysynchronous .nalization. The issues are certainlyno \nbetter understood among the authors of client code than among language implementors. A large fraction \nof the .nalization uses that we have seen involved unintended races due to insu.cient synchronization \nin the .nalizer. To make this discussion more concrete, we will assume a multi-threaded environment. \nAs we will see below, .nalizers (like Unix signals or hardware interrupts) essentiallyintro\u00adduce their \nown thread of control. Thus various issues be\u00adcome easier to express if threads are alreadypresent. None \nof our issues disappear in a single-threaded environment, but theymust be addressed with di.erent, and \nprobably less natural, techniques[7]. Although our approach to .nalization will deviate slightly from \nJava semantics in ways we discuss below, we will use mostlyJava terminologythroughout this paper. In \npartic\u00adular, a method or an object is synchronized if it acquires a lock. It is worth remembering throughout \nall of this that cleanup actions, like memorymanagement issues in general, really become of interest \nonlyfor at least medium-sized programs manipulating relativelycomplex data structure. And they become \nfar more interesting for systems that try to rely on modularityso that theydo not have to relyon a single \nim\u00adplementor understanding the entire system. Unfortunately none of these lend themselves to inclusion \nas examples in a paper. Thus our examples will tend to be somewhat incom\u00adplete.  2. C++ DESTRUCTORS \nThe C++ language allows objects to have an associated destructor method.5 When a stack allocated object \ngoes out of scope or is explicitlydeallocated using delete,the object s destructor is automaticallycalled. \nThis mechanism can be viewed as largelya syntactic convenience, but it does 2Some of this paper is an \nexpansion of the web pages we previouslymade available at http://www.hpl.hp.com/personal/Hans Boehm/ \ngc/det destr.html and http://www.hpl.hp.com/ personal/Hans Boehm/gc/.nalization.html 3To our knowledge, \nC# implementations generally do the right thing, and use a separate .nalization thread. One of them temporarilydid \nnot. See http://bugzilla.ximian.com/show bug.cgi?id=31333 . 4See http://www.boost.org/libs/smart ptr/ \nshared ptr.htm 5Note that the term destructor in the C# language de.\u00adnition corresponds to our usage \nof the term .nalizer .     have several advantages over explicit calls to a destroy method: 1. Class \ninheritance is handled correctlybyalso invoking superclass destructors. 2. It can be guaranteed that \nanystack allocated object will be destroyed exactly when the object goes out of scope. There is no opportunityto \nforget to make the call. 3. As a particularlyimportant instance of the last point, destructors signi.cantlysimplifycleanup \nof stack allo\u00adcated objects in the presence of exceptions.  Since it is easier to handle object cleanup \nthis wayin C++, it is common to tryto move anykind of resource ac\u00adquisition into a constructor, and the \ncorresponding dealloca\u00adtion into the destructor. This approach is commonlyreferred to as resource acquisition \nis initialization or RAII[14]. 6 As a particularlyillustrative and common example, de\u00adstructors are often \nused to release locks.7 Thus { L.lock(); f(); L.unlock(); } would be replaced by { scoped_lock sl(L); \nf(); } where the constructor for scoped lock acquires the argu\u00adment lock and saves its identity, so that \nthe destructor can release it. If f() raises an exception, the destructor is in\u00advoked during stack unwinding, \nso that L is still released. Thus it is guaranteed that L will be released at block exit.8 Destructors \nare guaranteed to be executed at a well\u00adde.ned program point. This propertymakes them useful for cleanup \nactions that have immediatelyvisible side-e.ects, such as releasing locks or closing windows. Destructors \nare also executed when heap objects are ex\u00adplicitlydeallocated, using delete. As we will argue below, \nthis works well if the programmer is aware of exactlywhich objects are deallocated at what point. However, \nit becomes highlyproblematic if some automatic or semi-automatic mechanism is used to perform deallocations \nimplicitly. 6Although this is designed to applyto all objects, it seems to be more useful for stack allocated \nobjects than for statically allocated ones. The author knows of more than one large project that has \nabandoned use of this facilityfor statically allocated variables due to the di.cultyof ensuring that \ncon\u00adstructor invocations occur in a safe order. Its applicability to heap objects is discussed below. \n7See for example http://www.boost.org/libs/thread/ doc/mutex.html. 8In our opinion, this raises a stylistic \nissue, in that there maybe no wayto see whether a lock is being released at the end of a block without \nreading the entire block. Thus we assert onlythat it s common practice, and leave the reader to judge \nwhether it s desirable.  3. FINALIZERS Xerox PARC Cedar[13]9 , Modula-3[10], and our garbage collector \nfor C/C++[2], among others, allow a .nalization function to be associated with an object. The .nalization \nfunction is executed sometime after an object can no longer be accessedexcept bythe .nalizationmethodfor \nthe object itself.10 It is given access to the object itself. Finalizer invocation maybe arbitrarilydelayed \nin unspeci.ed ways. Unlike destructors, .nalizers are attached to heap allo\u00ad cated objects, typically \nin contexts in which object deallo\u00ad cation is implicit. Theyare not used with stack allocated objects, \nor objects whose lifetime always ends at a syntacti\u00ad callydeterminable program point.11 We will refer \nto objects with an associated nonempty.nal\u00ad ization action as .nalization-enabled. Finalization-enabled \nobjects which are not reachable from ordinaryroots will be referred to as .nalization-ready. 12 Typically \n.nalizers are implemented by keeping pointers to all objects with nonempty.nalizers in a separate set \ndata structure F . During initial tracing, the garbage collector ignores this data structure. The garbage \ncollector then also traces (i.e. marks or copies) objects reachable byfollowing chains of one or more \npointers from the objects in F .Any objects that are in F but were not traced during either phase become \neligible for .nalization. Finally F and the objects it references directlyare traced to ensure that objects \nneeded by.nalizers are retained bythe garbage collector. The .nalizer for an object p mayassign p to, \nfor exam\u00ad ple, a static class member. In that case p mayremain live inde.nitelyafter its .nalizer has \nrun. This is sometimes, perhaps incorrectly, referred to as resurrection.It di.ers from destructor behavior, \nbut causes no real problems.13 3.1 Alternatives PARCPlace Smalltalk (see [9] for a discussion) provides \na slightlydi.erent .nalization model. An object can be referenced bya weak array. Such references are \ninitially ignored bythe garbage collector when determining object reachability, but cleared once an object \nis collected. When this happens, the weak arrayobject is noti.ed that one of itsentries hasdisappeared. \nThis di.ers from our model in two ways: Finalization is combined with a weak pointer facil\u00adity. (Weak \npointers are pointers that do not prevent a garbage collector from viewing an object as inacces\u00adsible.) \nThe same combination is used in later versions of the Cedar facility14 or in Java s java.lang.ref. 9Rovner[13] \ndescribes an earlyversion of Cedar .nalization. Later versions are closer to the Modula-3 version. 10To \nsimplifymatters, we ignore special weak references that are ignored in making this determination. Such \na fa\u00ad cilityis often combined with a .nalization facility, and the combination can be quite useful. \n11Ada s use of the term .nalization corresponds to our destruction . 12 We avoid the term .nalizable \nsince it has been used to mean either of these. See for example [9] and [8]. 13Nonetheless it is often \nviewed as problematic. The only reason we can identifyis that at least in the original Java speci.cation, \nthere was no wayto reenable .nalization to be run again when the p becomes inaccessible (again). This \nrestriction appears to be fairlyarbitraryand Java-speci.c. It is not shared by, for example, Modula-3 \nor Xerox Cedar. 14Earlyversions used package refs . E.ectivelythe garbage More interestingly, a di.erent \nobject is noti.ed of the unreachabilityof an object. E.ectively, the noti.cation message can be viewed \nas a .\u00ad nalizer invocation, but with the di.erence that the .nalizer function does not have access to \nthe unreachable object it\u00ad self, and hence that object can be immediatelyreclaimed. Others have also \nargued in favor of notifying another ob\u00ad ject15 . The approach has its advantages in terms of pro\u00ad gramming \nstyle, and may well make .nalizers more compre\u00ad hensible, since object resurrection can no longer occur. \nA priori it has some advantages when it comes to ordering issues. (See appendix A). However, it can be \nemulated in our model, at least at this level of detail, byadding a possi\u00ad blyempty executor object p \n.,apointerfrom p to p .,and registering a .nalizer onlyfor p . . Thus it doesn t reallyaf\u00ad fect our discussion, \nand we do not consider this alternative further here. Java .nalizers di.er from Modula-3 .nalizers in \nthat ob\u00ad jects maybe .nalized even if theyare reachable from other .nalization-enabled objects. This \na.ects the implementa\u00ad tion onlyslightly. The impact on client code is discussed in appendix A. C# destructors \nare really.nalizers in our terminology, and behave essentiallylike Java .nalizers. 3.2 Example uses \nof .nalization The addition of a .nalizer-like mechanism is well moti\u00ad vated in [5]. Here we look at \na few additional examples to motivate and clarifyour discussion. The third one will be used to discuss \nsome of the synchronization issues which are central to our discussion. In general, .nalizers should \nbe used to reclaim resources, other than garbage collected memory, when timing of the resource reclamation \nis not critical. In the case of scarce resources, explicit deallocation is usuallypreferable. But as \nwe see in the second example below, it is not always practical. 3.2.1 Legacy libraries Perhaps the most \ncommon use of .nalization is to accom\u00admodate libraries written for malloc/free memorymanage\u00adment in a \ngarbage collected environment. Typically such li\u00adbraries provide an explicit deallocation function d \nthat must be invoked when an object l managed bythe libraryis no longer needed. If the object l is used \nbya garbage collected object g, then it is common to invoke d from g s .nalizer. This mayunnecessarilydelayreclamation \nof l, but this is typically no more of a problem than the delayed dealloca\u00adtion of memoryinherent in \na tracing garbage collector. 3.2.2 Files in ropes Occasionallycomplex data structures contain embedded \nnon-memoryresources. A good example of that is the rope data structure described in [3].16 collector \nignored a predetermined number of references to .nalization-enabled objects.[13] 15See for example, the \ndiscussion of death notices on gclist@iecc.com. 16A similar, but explicitlyreference counted, data structure \nis included in the SGI and GNU ver\u00adsions of the C++ standard template library. See http://www.sgi.com/tech/stl/Rope.html. \n Figure 1: Slightly edited .le as a rope Here astringis represented as abinarytree or DAG. Each leaf \nis a string constant. Interior nodes represent the con\u00adcatenation of the strings represented bythe left \nand right subtrees. This representation allows constant time concate\u00adnation of arbitrarilylong strings. \nIf some care is taken to keep leaves small and trees balanced, it also allows e.cient substring operations \non long strings. These make it pos\u00adsible, for example, to build an e.cient text editor which represents \nthe entire .le being edited as a single rope. A common extension is to allow .le descriptors instead \nof explicitlystored strings as leaves, and to allow interior nodes representing unevaluated substring \nexpressions. These allow an editor to operate on a large .le represented as a rope, without reading the \nentire .le into memory. After a single character insertion of the letter z after the Nth position in \na large .le, the in-memoryrepresentation of the resulting .le might be as in .gure 1. In general it is \ndi.cult to predict when the .le nodes in such a data structure will be dropped. For example, in the Cedar \nprogramming environment[13] ropes are used as the standard string representation, and are thus themselves \nem\u00adbedded in manyother data structures. Even in the editor case, an embedded .le descriptor will be dropped \nif all char\u00adacters in the original .le are replaced. Thus if we wanted to explicitlyclose a .le, we would \nhave to be able to determine when a .le node is no longer accessible from anyaccessible ropes; e.ectivelywe \nwould have to redo exactlythe work alreadyperformed bythe collector. Finalizers are well-suited to handling \nthis problem. We simplyattach a .nalizer to each leaf containing a .le de\u00adscriptor. Sometime after that \nleaf becomes unreachable, the .nalizer closes the .le. This maylead to .les being kept open longer than \nnecessary; but for typical applications such as the text editor example, veryfew .les tend to be embedded \nin ropes in this manner, so this is unlikelyto be an issue. As we will see below, we can potentiallyreclaim \n.le descriptors more promptlywith some help from the client code and .le open routines.  3.2.3 External \nobject data Assume that we have a class C, such that all instances of C maintain at least some of their \nstate (a C impl instance) in a permanentlyreachable array. This allows us to easily share a single C \nimpl between multiple Cs containing the same data. The global arrayallows us to easilysearch for an existing \nC impl to reuse. We ll see another reason to do this in the next example. This looks something like: \n class C_impl { // Some stuff needed by C instances. T data; public C_impl(T d) {data = d;} } class C \n{ // The following two arrays are protected // by the lock on the impls array. static C_impl impls[] \n= new C_impl[N]; // The number of Cs sharing the // corresponding C_impl. static int impl_use_count[] \n= new int[N]; int my_index; // impls index for my rep. public C(T t) { synchronized(impls) { for (int \ni=0; i <N; ++i) { if (impl_use_count[i] > 0 &#38;&#38; <impls[i] reusable>) { my_index = i; return; \n} } my_index = first_available(); impls[my_index] = new C_impl(t); impl_use_count[my_index] = 1; } } \n protected void finalize() { synchronized(impls) { --impl_use_count[my_index]; if (impl_use_count[my_index] \n== 0) { impls[my_index] = null; } } } static int first_available() { // Caller must hold impls lock. \nfor (int i= 0;i <N; ++i) { if (impl_use_count[i] == 0) return i; } throw ... } } The data structure \nis pictured in .gure 2. E.ectively C impl_use_count impl C_impl s C s Figure 2: Data structure for example \n3.2.3 objects contain onlyan index my index into a global table impls. The real instance data is referenced \nbythe table impls.When a C is no longer otherwise reachable, its .nal\u00ad izer is invoked. This updates \nimpl use count and possibly explicitlyremoves a C impl from the table so that it can be reclaimed. If \nwe had put the C impl state directlyinto C objects instead, we would have needed an auxiliarytable to \n.nd candidates for reuse. This table would have prevented the garbage collection of C objects without \na mechanism such as weak pointers. Note that the .nalizer here must acquire the class lock since it updates \nshared data structures.  3.2.4 Removal of temporary .les Occasionallyit is necessaryto provide for reliable \ncleanup of certain resources before process exit. Removal of tempo\u00ad rary.les is often an example of this. \nAs is discussed in appendix A, .nalizers bythemselves cannot provide this facilitysince the .nalization \nmechanism doesn t have the necessaryordering information. Here we outline how to use .nalizers to remove \ntemporary .les as theyare dropped during program execution, while making it possible for an explicit \nroutine to remove those that are remaining at process exit. Since .nalizers cannot guarantee to remove \nall such .les, we will need an explicit routine cleaner to be called before process exit. It will be \nthe clients responsibility(as it must be) to ensure that this is onlycalled after the last use of a temporary.le, \nbut before the removal of anyother objects needed by cleaner. This cleaner routine will need access to \nthe system re\u00ad sources which are still allocated. One wayto accomplish this is analogous to our previous \nexample. We keep an ex\u00ad plicit, always reachable, table T of temporary.les. Clients are not given direct \naccess to T . These clients in\u00ad stead access .nalization-enabled temporary.le objects con\u00ad taining primarilyan \nindex of, or a reference to, the corre\u00ad sponding .le information in T . The data structure is out\u00ad lined \nin .gure 3.17 Finalizing a temporary.le object causes removal of the .le, and the corresponding entryfrom \nT . 17Note that references to the client-visible object are synchro\u00adnized, i.e. acquire the lock on \nthe object. This is necessary    T: References from client Object representing temporary file. (Finalizer \nremoves table entry.) (Synchronized for reachability.) Object containing temporary file data. (No finalizer.) \nTable of temporary files needing cleanup. Figure 3: Guaranteed cleanup of temporary .les Cleaner removes \nanyentries left in T at process termina\u00adtion.  3.3 Locking of .nalizers Many.nalizers should guard against \nconcurrent access to the underlying object by acquiring a lock. There are two reasons for this: 1. There \nis usuallynot much of a point in writing a .nal\u00adizer that touches onlythe object being .nalized, since \nsuch object updates wouldn t normallybe observable. Thus useful .nalizers must touch global shared state \n(e.g. static .elds of Java classes). Generallythis shared state can be concurrentlyupdated byclient code \nout\u00adside .nalizers. In a language such as Java, multiple .nalizers for the same class mayalso run concurrently \nin separate threads (even if the original application is single-threaded!), so that we must ensure mutual \nex\u00adclusion between .nalizers. Sometimes, as in our ropes example above, the only shared state that is \ntouched is inside the operating systems kernel or low level system libraries, so that the necessarysynchronization \nis hidden. In manyother cases, e.g. in our external object data examples, user\u00advisible locks are needed. \n2. Object x maybe .nalized before x.foo() has .nished executing. This is possible since x.foo() mayno \nlonger need access to x past a certain point in the method, and thus there is no reason for x to be treated \nas reach\u00adable bythe garbage collector past that point. Consider speci.cally: class X { Y mine; public \nX() { mine = new Y(); ... } public foo() { ...; mine.bar(); } for reasons discussed below. Accesses \nto T must also be synchronized, since it must support concurrent access. public void finalize() { mine.baz(); \n} } where mine is a subobject not visible to the outside world, and whose methods can thus not be called \nex\u00adcept through the corresponding X object. Assume that onlyone thread uses object x of class X,and that \nthe last action on x is to call x.foo(). When foo calls mine.bar, x mayno longer be reachable. It is \nquite possible that, at this point, the register holding x will have been reused, and the stack frame \nfor foo no longer exists due to tail call optimization. Thus the .nalize method maybe invoked at this \npoint, and mine.bar() mayrun concurrentlywith mine.baz(). If both update the same data structure, this \nis nearly certain to be unacceptable. We can ensure that mine.bar() and mine.baz() run sequentiallybyadding \nlocking, e.g. bymarking foo and finalize as synchronized.18 This is at odds with the occasional advice \nto avoid syn\u00adchronization in .nalizers.  3.4 Reachability and Optimization Most speci.cations for languages \nsupporting .nalization are intentionallysomewhat vague about .nalizer execution timing in both directions. \nTheynot onlyallow .nalization to be delayed, but also allow .nalizers to be executed earlier than might \nbe expected, as the result of compiler optimiza\u00adtions. We .rst concentrate on the latter. We saw in the \npreceding section whythis might happen with private subobjects. However, things can get even less pleasant. \nConsider an example resembling the external object data example from section 3.2.3. In this case, assume \nthat we have objects of class D containing an index my index to the external array A.Each entryin A is \nused byat most one D object. It contains either a null pointer, if it s not currently used byanyobject, \nor otherwise a reference to an object containing a counter .eld. The .nalizer for D resets the arrayentryto \nnull. Assume that D has a method foo that simplycounts the number of its invocations byincrementing the \ncorresponding counter: void foo() { ++A[my_index].counter; } Now consider what happens when an instance \nd of D is accessible onlybya single thread, and it is last accessed in the loop for (int i = 0; i < 1000000; \n++i) { d.foo(); }  18Interestingly, as we discuss in the next section, it would probablysu.ce to declare \nonlyfoo as synchronized. That would ensure that x remains live until the release of the associated lock, \nand thus su.cientlydelay.nalization to ensure that the methods are run sequentially. As discussed brie.yin \n[9], other methods for guaranteeing the liveness of x maybe thwarted bycompiler optimizations. A compiler \nmight inline foo,observe that d.my index is loop invariant, and thus simplykeep it in a register. It \nmay then observe that d is now dead, and no longer needs to be kept anywhere. If the compiler eliminates \nd in this manner, and the collector then runs while the loop is executing, it will discover that d is \nunreachable, and should thus be .nalized. The .nalizer will set d.my index to null, and cause the next \nincrement to fail. The apparent conclusion from this is that we need to be a bit more precise about what \nreferences can be eliminated bythe compiler, and how soon .nalizers can be run. Un\u00adfortunately, we are \nnot aware of any language speci.cations that are su.cientlyprecise about this. As an example, the Java \nLanguage Speci.cation[8] spec\u00adi.es that A reachable object is anyobject that can be ac\u00adcessed in anypotential \ncontinuing computation from any live thread. Optimizing transformations can be designed that reduce the \nnumber of objects that are reachable to be less than those which would naivelybe considered reach\u00adable. \nWe believe that in light of the above example, this is not su.cientlyprecise. A minimal solution to this \nproblem for Java would be to insist that an object is reachable at least until the lock on the object \nhas been released for the last time. This would eliminate the problem in the above example once we make \nfoo synchronized. It could also be used to eliminate corre\u00adsponding problems in examples 3.2.3 and 3.2.4. \nSome of the constructions in appendix A of this paper, as well as, for example, Finalizer Guardian construction \nin [1], require that certain other potentiallyunaccessed objects need to be considered reachable. In \nparticular, if there is a reference from A to B,and A is reachable, we need to know that B will not be \n.nalized. We will continue to make this assumption were necessary, since existing code relies on it and, \nto our knowledge, all existing implementations satisfy it. (It would be acceptable to restrict this assumption \nto final and volatile references.) The situation in C# appears to be verysimilar, and in need of similar \nsolutions. For some other languages, the de.nition of reachability is even less precise. 3.5 Finalization \nmust be asynchronous. Java requires .nalizers to run inside a thread holding no user-visible locks. Those \nvirtual machines that implement this correctlyappear to universallydo so byrunning .nal\u00adizers in a separate \nthread or threads, once the collector has determined, at its leisure, that an object is eligible for \n.nal\u00adization. To understand the reasoning behind this requirement, it is useful to consider, instead \nof Java, the case in which locks cannot be reacquired bya thread. (For example, pthread locks[11] work \nthis waybydefault.) If .nalizers can be run at anypoint in anythread, a .nalizer requiring lock L may \ncoincidentallybe run in a thread that alreadyholds lock L. The attempt to reacquire the lock results \nin deadlock. If a .nalizer needs to acquire a lock, there is verylittle the programmer can do to avoid \nthis scenario. Even if the pro\u00adgramming language, like Java, allows locks to be reacquired bythe same \nthread, the situation does not improve. Instead of an obvious failure, we will allow two logicallyseparate \noperations, namelythe client hread holding L and the .nal\u00adizer, to hold L at the same time. The example \nin appendix B illustrates that this can result in intermittent incorrect  execution. This issue was \napparentlyknown to the authors of [8]. Unfortunately, most major Java implementations appear to have \nimplemented this incorrectlyin their initial versions.19 We argue that this observation is equallyapplicable \nto environments that traditionallyuse reference-count based garbage collection, and even if .nalization \nis implemented basedondestructors. For example, it has often been argued in the context of automatic \nmemorymanagement for C++, that destructors for heap objects should behave just like destructors for stack \nobjects, and that theymust be run the instant the last refer\u00ad ence to an object is dropped, e.g. in response \nto a reference counter decrement. And this is exactlywhat is commonly done byC++ reference count implementations \n(cf. [4]). Python[16] uses a similar approach. In both cases destruc\u00ad tors20 are used to implement .nalizers. \nBut this encounters exactlythe same problems that were encountered byearlyJava implementations: Due to \nthe presence of reference counting, which is designed to free the programmer from worryabout deallocation \ntiming, the tim\u00ad ing of .nalizer execution is no longer transparent. Finalizers mayrun in response to \na reference decrement at anyassign\u00ad ment or block exit, and hence appear to the programmer to be asynchronous. \nDeadlocks involving a lock held by the thread processing the reference count decrement and re\u00ad quired \nbythe .nalizer are neither predictable nor avoidable. It is impractical for a tracing garbage-collector \nto run .\u00ad nalizers at exactlythe point in thread execution when an object becomes .nalization-ready. \nBut even if it could do so, this would in fact make it much harder, rather than eas\u00ad ier, to write correct \ncode. Appendix B expands on the external object data exam\u00ad ple from section 3.2.3 to give a concrete \nillustration of how synchronous .nalization in response to a reference count decrement can fail unexpectedly. \nThe example also illus\u00ad trates that module abstraction boundaries e.ectivelymake it impossible to avoid \nsuch disasters, even with determin\u00ad istic reference-counting garbage collection. The often repeated complaints \nthat .nalizers are unpre\u00ad dictable (cf. [1]) is a necessaryfeature, not a de.ciency. 3.5.1 Explicit \nFinalizer Invocation Java provides a method System.runFinalization() which explicitlyforces .nalizer \ninvocation. Various .nalizer im\u00adplementations and proposals (cf. Cedar [13], Guardians[5], the Ellis-Detlefs \nsafe C++ proposal[7] or java.lang.ref in Java2) do not always run .nalization procedures implicitly, \nbut mayinstead simplyenqueue .nalization-readyobjects, 19See for example a typical complaint at http: \n//www.geocrawler.com/archives/3/196/1997/ 9/0/1089518/ or the more detailed discussion at http://www.cs.arizona.edu/sumatra/hallofshame/ \nmonitor-.nalizer.html The latter overlooks the require\u00ad ment on java.long.Object..nalize that the thread \nthat invokes finalize will not be holding anyuser-visible synchronization locks ... , and thus confuses \nan implemen\u00ad tation bug for a speci.cation bug. The GNU Java compiler corrected a similar implementation \nbug after we pointed out the problem. We believe this was a common mistake in this context, which has \ncontributed to the bad reputation of .nalizers. 20 methods in Python del leaving it to the client to \nread the queue and invoke the appropriate procedures. These allow .nalizers to be used safelyin single-threaded \nenvironments, byrequiring the client to explicitlyinvoke .\u00adnalizers when it is safe to do so, i.e. outside \ncode sections that should be executed atomically. This appears to be the safest wayto handle .nalization \nin single-threaded code, al\u00adthough it has some clear disadvantages in certain contexts: Potentiallylarge \namounts of code, especiallylong-running routines, must be explicitlysprinkled with .nalizer invoca\u00adtions. \nBut such code must not be called in contexts in which .nalizer invocations are unsafe. Another potential \nuse for an explicit call to run .nalizers is to reclaim certain system resources which are particu\u00adlarlyscarce. \nFor example, it is common to force a garbage collection and invoke .nalizers when a .le open call would \notherwise fail due to lack of .le descriptors. However, this is complicated byseveral issues, which often \nseem to be over\u00adlooked: 1. Running one .nalizer maycause other objects to be eligible for .nalization. \nFor example, a bu.ered .le mayneed to be .nalized (and .ushed) before the un\u00adderlying raw .le can be \n.nalized (and the descriptor reclaimed). In the case of Modula-3 style ordered .nal\u00adization, the .nalizers \nwould implicitlybe run in con\u00adsecutive collection cycles. As we explain in appendix A, this behavior \ncan, and often must be, emulated in Java. 2. The resource allocation call, e.g. the .le open call, maybe \nmade from client code that holds locks. These locks maybe needed by.nalizers.  The .rst issue is easilyaddressed \nwith careful interface design. Rather than attempting to run all .nalizers with System.gc(); System.runFinalization(); \nwe should use do { System.gc(); System.runFinalization(); } while (<resource unavailable> &#38;&#38; \n<System.runFinalization() did something>) Thesecond issueismoredi.cult to resolve. Assume that System.runFinalization() \nis called from a thread holding lock L and no other locks. It is possible to, at least logically, run \neach .nalizer in its own thread. This will cause exactlythose .nalizers that do not need L to run to \ncompletion. That s perhaps the best we can do, but it certainlyleaves open the possibilitythat some blocked \n.\u00adnalizers would have caused other, possiblymore interesting, objects to be dropped. A second possibilitywould \nbe to insist that runFinalization() not be called from contexts that might hold locks shared by.nalizers. \nThat would require similar restrictions on .le open or other resource allocation routines, which might \ncall runFinalization() internally. This appears to us to be a viable alternative.  The current Java \nAPI speci.cation[15] is unfortunately unclear on the intended usage model, or even whether it is acceptable \nto run .nalizers in the thread invoking runFinalization(). We conclude that it is probablyuseful to explicitlyinvoke \n.nalizers to reclaim needed resources in this manner. We are aware of several Java implementations that \nimplicitlytryto to do so. But it is not clear to us that anyof these systems do so completelysafely, \nor through the right interfaces.   4. CONCLUSIONS We have pointed out that C++ destructors and Java \n.nal\u00ad izers are completelydi.erent facilities. C++ destructors are used to provide guaranteed cleanup \nactions at well-de.ned program points, especiallyin the presence of exceptions. In our view21 theyare \nmore closelyrelated to Java synchro\u00ad nized and try { ... } finally { ...} blocks or, in the case of locks, \nsynchronized blocks, than theyare to Java finalize methods. In contrast, .nalization in languages like \nJava is necessary in order to manage resources other than garbage-collected memorybased on garbage-collector-discovered \nreachability. Without this facility, it would often be necessary to basically redo the garbage collectors \nwork in order to manage these resources. Although it has clearlyintroduced a signi.cant amount of confusion, \na properlydesigned facilitycan be used safely, and does not add signi.cant complexity beyond that inherent \nin multi-threaded programming. Although we believe that .nalization is an essential facil\u00ad ityin manylarge \nsystems, we do not believe that it should be used frequently. Based on the limited statistics we have \nseen, one use per 10,000 lines or more of well-written code is probablytypical. But eliminating use of \n.nalization would touch nearlyeverymodule in such systems. Even the small amount of complexityinherent \nin .nalization can normally be isolated to a few modules of a large system. This is again di.erent from \nnormal C++ destructor usage, which tends to be far more pervasive, while each individual use tends to \nbe far less essential. The issues for application programmers center on the fact that .nalization e.ectivelyintroduces \nan additional thread of control, and thus concurrencyissues must be considered, even for otherwise single-threaded \napplications. Arguably no fundamentallynew issues are introduced, but the im\u00ad portance of understanding \nconcurrencyissues is elevated. Synchronization is essential for .nalizers. Language implementors must \nalso respect the fact that .\u00ad nalizers naturallyconstitute a separate asynchronous thread of control.22 \nThis applies to any.nalizers on heap objects, whenever these become inaccessible at a point that is not \ncompletelyapparent to the programmer. In particular, .\u00ad nalizers should never be run implicitlyas part \nof a client thread which maybe holding other locks. As was known to (some of) the Java community, this \napplies if the objects are managed byan automatic tracing garbage collector. But it also applies to semi-automatic \n(e.g. manuallyreference 21A similar point is made in e.g. [1] 22It must be asynchronous in the sense \nthat the .nalizer can\u00ad not always run at the speci.c point at which a thread dis\u00ad covers the object to \nbe inaccessible. This does not byitself preclude cooperative multi-threading, with potentiallyde\u00ad terministic \nexecution.  counted) memorymanagement schemes, since in these sys\u00ad tems the programmer has also chosen \nto abstract awaythe details about when objects become inaccessible. It is oth\u00ad erwise impractical to \nensure that .nalizers cannot be run inside a client holding locks. More speci.cally, language designs \nsupporting .nalizers should ensure the following: In a multi-threaded environment, it must be guaran\u00adteed \nthat .nalizers will run in a thread in which no locks are held. Typically this means that either .naliz\u00aders \nare run in their own thread(s), or that .nalization\u00adreadyobjects are enqueued and then run explicitly \nfrom programmer-initiated threads. Thus .nalizers must be allowed to, and encouraged to, acquire locks. \n In purelysingle-threaded environments the program\u00admer must be given explicit control over when to run \n.nalizers. Typically this will be accomplished by ex\u00adplicitlyenqueueing .nalization-readyobjects, as \nin [5].  The language speci.cation must provide the program\u00admer with a wayto ensure that objects remain \nreachable long enough to prevent premature .nalization. The last run-time representation of a pointer \nto an object maydisappear long before the last logical access to one of its .elds. There seem to be clean \nways to provide the necessaryguarantees, but it does not appear to us that anylanguage speci.cation currentlydoes \nso.  Librarycalls to explicitlyinvoke the garbage collector and .nalizers are apparentlyquite useful \nin manag\u00ading relativelyscarce non-memoryresources. However primitives such as Java s System.runFinalization \nmust be clearlyspeci.ed with respect to synchroniza\u00adtion behavior, and theymust accommodate the fact \nthat there maybe dependencies among .nalizers. Cur\u00adrent language speci.cations generallyappear to fail \non both counts.  As far as .nalization is concerned, there is no qualitative reason to prefer a reference-counting \ncollector over a tracing collector. A reference counting collector mayhave a quan\u00adtitative advantage \nin that it maydefer their execution for a shorter period of time. But their execution must still be deferred, \nat least in some cases. 5. ACKNOWLEDGEMENTS The observations about the necessityfor synchronization \n in .nalizers grew out of discussions, notablywith Barry Hayes, many years ago. The problem, though not \nthe solu\u00ad tion, is partiallyoutlined in [9]. Some of the conclusions here grew out of discussions on \n the gcj (GNU Java compiler) mailing list. NotablyAndrew Haley23 pointed out the danger in calling runFinalization \nfrom a thread holding locks, e.g. while trying to allocate a .le descriptor. (Several bug reports on \nSun s web site appar\u00ad entlymade similar observations somewhat earlier, though not known to the author \nat the time.) GuySteele contributed to several discussions on .nalizer ordering in Java, which are partiallyre.ected \nhere. The anonymous reviewers provided many useful suggestions. 23See http://gcc.gnu.org/ml/java/2001-12/ \nmsg00390.html. 6. REFERENCES [1] J. J. Bloch. E.ective Java Programming Language Guide. Addison-Wesley, \n2001. [2] H.-J. Boehm. A garbage collector for C and C++. http://www.hpl.hp.com/personal/Hans Boehm/gc/. \n [3] H.-J. Boehm, R. Atkinson, and M. Plass. Ropes: An alternative to strings. Software Practice and \nExperience, 25(12):1315 1330, December 1995. [4] G. Colvin, B. Dawes, P. Dimov, and D. Adler. Boost \nsmart pointer library. http://www.boost.org/libs/smart ptr/. [5] R. K.Dybvig,C. Bruggeman, and D.Eby \n.Guardians in a generation-based garbage collector. In SIGPLAN 93 Conference on Programming Language \nDesign and Implementation, pages 207 216, June 1993. [6] ECMA. Standard ECMA-334: C# Language Speci.cation. \nECMA, December 2001. [7] J. R. Ellis and D. L. Detlefs. Safe, e.cient garbage collection for C++. Technical \nReport CSL-93-4, Xerox Palo Alto Research Center, September 1993. [8] J. Gosling, B. Joy, and G. Steele. \nThe Java Language Speci.cation, Second Edition. Addison-Wesley, 2000. [9] B. Hayes. Finalization in the \ncollector interface. In International Workshop on Memory Management (IWMM 92, LNCS 637), pages 277 298, \n1992. [10] J. Horning, B. Kalsow, P. McJones, and G. Nelson. Some useful modula-3 interfaces. Technical \nReport 113, Digital Systems Research Center, December 1993. [11] IEEE and The Open Group. IEEE Standard \n1003.1-2001. IEEE, 2001. [12] G. Nelson, editor. Systems Programming with Modula-3. Prentice-Hall, 1991. \n[13] P. Rovner. On adding garbage collection and runtime types to a strongly-typed, statically-checked, \nconcurrent language. Technical Report CSL-84-7, Xerox Palo Alto Research Center, July1985. [14] B. Stroustrup. \nThe Design and Evolution of C++. Addison-Wesley, 1994. [15] Sun Microsystems. Java 2 platform, standard \nedition, v 1.4.0 api speci.cation. http://java.sun.com/j2se/1.4/docs/api/, 2002. [16] G. van Rossum. \nPython reference manual. http://www.python.org/doc/current/ref/ref.html.  A. APPENDIX: FINALIZER ORDERING \nJava .nalizers and C# destructors di.er from the Modu\u00adla-3/Cedar approach in two important ways: 1. Objects \nmaybe .nalized even if theyare reachable from other .nalization-enabled objects. Thus Java objects with \nnonempty .nalize methods must explicitlytake care to keep objects theyneed accessible through some other \npath. 2. In C#, a reachable object, e.g. an object direct refer\u00adenced from a static member of a class \nmaybe .nalized at program termination. Java had a facilityto do the same, though that has since been \ndeprecated.  Before we discuss these, we can observe that .nalizers gen\u00aderallyfall into two categories: \nThose that simplydeallocate operating system resources, and those that touch other user data structures. \nA useful .nalizer must generallydo one of  D: Permanently reachable table Figure 4: Enforcing .nalizer \ndependence of A on B these: Onlytouching the object itself is useless, since it is no longer reachable, \nand thus can t be seen bythe rest of the program. The above changes don t a.ect the simple deallocation \nof system resources. For other kinds of .nalizers, both changes add complications, in that a .nalizer \ncan no longer assume that the objects it needs have not yet been .nalized. We believe that change (2) \nabove is a defect that has been remedied in Java with the deprecation of System.runFinalizersOnExit, \nand should be repaired in C#. For a well-designed operating system it is typically not a signi.cant improvement \nfor .nalizers that onlyreturn re\u00ad sources to the operating system, e.g. byclosing .le descrip\u00ad tors. \nOperating systems generally reclaim those on process exit anyway to avoid resource leaks when programs \ncrash. But in this environment we do not know how to write reli\u00ad able .nalizers that do much more than \nthis: There can no longer be a guarantee that anyother objects, even if they are clearlyreachable through \nordinaryroots, have not been .nalized before a given .nalizer runs.24 Even if a .nalizer simplywants \nto generate output to the standard error stream it is hard to see how it can guarantee that it will \nnot have been previouslyclosed. Since the exis\u00ad tence of a .nalizer for a particular class is generallyviewed \nas a private implementation detail for that class, every.nal\u00ad izer would have to be prepared for everyobject \nit touches to have alreadybeen invalidated. It is hard to see how it would be practical to write .nalizers \nunder this kind of assumption. The situation is aggravated in that anyerrors along these lines are likelyto \nresult in intermittent symptoms. We believe that change (2) was motivated bya desire to guarantee that \n.nalizers run eventuallyto enable them to be used for tasks such as removing temporary.les. We showed \nin our last example in section 3.2.4 that this can be done correctlywithout this change. Change (1) is \nmore interesting, especiallysince it has per\u00ad sisted in Java. It requires that if the .nalizer for object \nA depends on another object B, which mayalso have a .nal\u00ad izer, object A will generallyneed to explicitlyensure \nthat object B remains accessible, where in the Modula-3 case the collector implicitlyhandled the ordering. \nThis is possible bysimplyhaving A s constructor add B to a reachable data structure D, and then having \nA s .nalizer remove the reference25 as is shown in .gure 4. 24 As is is pointed out in the Java API documentation \n[15], the real reason for deprecating this call in Java was apparently onlymildlyrelated to this. If \nother threads are still running (as is likelyin the presence of daemon threads), there is also no wayto \nguarantee that .nalizers won t be called on objects that are still being activelyused. 25This relies \non our earlier assumption about reachability. A: A: A : 11111 11111 * 11111 11111 00000000000 11111111111 \n00000000000 11111111111 00000000000 11111111111 00000000000 11111111111 00000000000 11111111111 11111111111 \n11111111111 11111111111 11111111111 11111111111 11111111111 11111111111 11111111111 11111111111 11111111111 \n11111111111 11111111111 11111111111 11111111111 11111111111 11111111111 11111111111 11111111111 11111111111B: \n11111111111 B : 0000000000011111111111 11111 11111 * 11111 11111 Figure 5: Elimination of .nalization \ncycle Change (1) does have the advantage that long lists or cycles of .nalization-ready objects can automatically \nbe .\u00adnalized in a single cycle. But the same e.ect can usually be achieved with Modula-3 style .nalizers \nby breaking objects into pieces, one of which contains onlythose .elds accessed bythe .nalizer. Consider \nagain the case in which A s .\u00adnalizer needs access to B. Now assume also that B holds a reference to \nA,which is not needed by B s .nalizer. This can be transformed as in .gure 5.26 Here .elds needed by \n.nalizers have been indicated byshading, and * indicates that an object is .nalization-enabled.27 In \nthe transformed version, A. is not reachable from a .nalization-enabled ob\u00adject, and can thus be .nalized \n.rst. On the other hand, B. is reachable from A. and must therefore wait for B. s .nalization. In general, \nfor Modula-3 style .nalization, the program\u00ad mer should keep .nalization-enabled objects small, and avoid \nreferences in .nalization-enabled objects that are not fol\u00ad lowed by.nalizers. If this rule is followed, \nwe believe that neither cycles nor long chains of objects are a serious issue. (Anyremaining cycles between \n.nalization-readyobjects are easilydetectable bythe runtime.)  B. APPENDIX: SYNCHRONOUS FINAL-IZATION \nFAILS We illustrate how synchronous .nalization, such as one might obtain with a C++ reference counting \ncollector that naivelyuses C++ destruction to implement .nalization, can unexpectedlydeadlock. A runnable \nversion of this ex\u00adample in C++, using Boost shared ptr as the garbage collector and Boost synchronization \ncan be found at http: The current Java Language Speci.cation[8] does not guar\u00adantee this to be correct. \nSince the reference from D to B is never actuallyfollowed from a non-.nalizer thread, there is no guarantee \nthat the collector must consider it when de\u00adtermining reachability. As in the previous construction, \nwe are assuming that ob\u00adject reachabilityis de.ned so that A. is not .nalized while A is accessible. \n27We have also assumed that, as in Modula-3, a .nalization\u00ad enabled object maybe .nalized even if it \npoints to itself. This is not the default for our collector, though perhaps it should be. Otherwise, \nand in some more complex cases, it might be necessaryto expose B. and to make A. reference B. directlyto \nmake it apparent to the GC that there is no real reference cycle. //www.hpl.hp.com/personal/Hans Boehm/popl03 \n/c++example. Consider adding the following to the external object data example from section 3.2.3. The \ncall c.update(x)updates the information stored in the impls arrayfor c, based on some information associated \nwith x.If c s representation is not shared, the update is performed in place. Otherwise, it is .rst copied: \npublic synchronized void update(C other) { synchronized(impls) { int count = impl_use_count[my_index]; \nT new_val = combine(impls[my_index].data, X.messy_fn(other)); if (count > 1) { // Clone my C_impl. \nint new_index = first_available(); impl_use_count[new_index] = 1; impls[new_index] = new C_impl(new_val); \nimpl_use_count[my_index] = count -1; my_index = new_index; } else { impls[my_index].data = new_val; } \n} } Assume that 1. X.messy fn computes some propertyof its class C ar\u00adgument, which is expensive to \ncompute. This property never changes for a given C instance. 2. X waswritten bysomeoneelsewhomwe haven \nt heard from in .ve years. 3. Unknown to us, X maintains a small cache of C values it was recentlypassed, \ntogether with the corresponding return values. When the cache becomes too large, an old value maybe dropped \nbybeing overwritten.  Recall that so far we assume that locks cannot be reac\u00adquired bya thread. Now \nconsider the scenario in which 1. We call update. 2. It acquires the lock, reads a count of 2, and then \ncalls X.messy fn. 3. X.messy fn s cache does not yet contain other.It thus computes the result and replaces \na previous cache entry c. 4. c happened to share a C impl with the entrybeing updated. 5. c becomes \neligible for .nalization.  With a simple reference counting garbage collector (e.g implemented like \nBoost shared ptr [4]) the Scenario con\u00adtinues: 1. Finalizers are invoked immediatelywhen a zero refer\u00adence \ncount is detected, from the thread that caused the reference count to be decremented. 2. c s .nalizer \nwill be invoked from the thread that called update. 3. This thread still holds the lock for class impl.Dead\u00adlock! \n If instead we assume Java lock semantics, we mayend up with data structure corruption instead of deadlock. \nInstead of the above, the scenario continues: 1. c s .nalizer will be invoked from the thread that called \nupdate. 2. The .nalizer reacquires the lock, ignoring the fact that we were alreadyin the middle of \nupdating the under\u00adlying data structure. 3. The .nalizer decrements the count (but not update s local \ncopy). 4. Update installs a count of 1 into the old entry, when it should have been zero.  Clearly, \nwith full insight into the entire system, we could easilyprogram around this problem (though it maybe \nharder for more realistic examples). But the code, as written, should run correctly. The programmer correctlyused \nlocks to ensure mutual exclusion, and correctlycounted on the garbage collector to take care of memorydeallocation \nand .nalization behind the scenes. In order to .x the problem, we need to understand that the collector \nwill potentiallyin\u00advoke a .nalizer for a C object as part of an invocation of messy fn. But the whole \npurpose of the collector was to make it unnecessaryto reason about deallocation timing in this way. \n The onlywayto preserve the programmer s abstraction is to decouple the .nalizer invocation from the \nthread that happened to drop the last object reference, i.e. to run the .nalizer asynchronously, e.ectively \nin its own thread. Thus even if we use a simple reference counting collector, which detects inaccessibilityimmediately, \nwe should still enqueue the .nalization call and carryit out later in a di.erent thread.  \n\t\t\t", "proc_id": "604131", "abstract": "We compare two different facilities for running cleanup actions for objects that are about to reach the end of their life.Destructors, such as we find in C++, are invoked synchronously when an object goes out of scope. They make it easier to implement cleanup actions for objects of well-known lifetime, especially in the presence of exceptions.Languages like Java[8], Modula-3[12], and C\\#[6] provide a different kind of \"finalization\" facility: Cleanup methods may be run when the garbage collector discovers a heap object to be otherwise inaccessible. Unlike C++ destructors, such methods run in a separate thread at some much less well-defined time.Languages like Java[8], Modula-3[12], and C\\#[6] provide a different kind of \"finalization\" facility: Cleanup methods may be run when the garbage collector discovers a heap object to be otherwise inaccessible. Unlike C++ destructors, such methods run in a separate thread at some much less well-defined time.We argue that these are fundamentally different, and potentially complementary, language facilities. We also try to resolve some common misunderstandings about finalization in the process. In particular: <ul><li>The asynchronous nature of finalizers is not just an accident of implementation or a shortcoming of tracing collectors; it is necessary for correctness of client code, fundamentally affects how finalizers must be written, and how finalization facilities should be presented to the user.</li> <li>An object may legitimately be finalized while one of its methods are still running. This should and can be addressed by the language specification amd client code.</li></ul>", "authors": [{"name": "Hans-J. Boehm", "author_profile_id": "81423595101", "affiliation": "Hewlett-Packard Laboratories, Palo Alto, CA", "person_id": "PP39052937", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/604131.604153", "year": "2003", "article_id": "604153", "conference": "POPL", "title": "Destructors, finalizers, and synchronization", "url": "http://dl.acm.org/citation.cfm?id=604153"}