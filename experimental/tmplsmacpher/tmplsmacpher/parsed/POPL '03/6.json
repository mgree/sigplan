{"article_publication_date": "01-15-2003", "fulltext": "\n Discovering Af.ne Equalities Using Random Interpretation Sumit Gulwani George C. Necula University \nof California, Berkeley {gulwani,necula}@cs.berkeley.edu ABSTRACT We present a new polynomial-time \nrandomized algorithm for discovering a.ne equalities involving variables in a program. The key idea of \nthe algorithm is to execute a code fragment on a few random inputs, but in such a way that all paths \nare covered on each run. This makes it possible to rule out invalid relationships even with very few \nruns. The algorithm is based on two main techniques. First, both branches of a conditional are executed \non each run and at joint points we perform an a.ne combination of the join\u00ading states. Secondly, in the \nbranches of an equality condi\u00adtional we adjust the data values on the .y to re.ect the truth value of \nthe guarding boolean expression. This increases the number of a.ne equalities that the analysis discovers. \nThe algorithm is simpler to implement than alternative deterministic versions, has better computational \ncomplexity, and has an extremely small probability of error for even a small number of runs. This algorithm \nis an example of how randomization can provide a trade-o. between the cost and complexity of program \nanalysis, and a small probability of unsoundness. Categories and Subject Descriptors D.2.4 [Software \nEngineering]: Software/Program Veri.\u00adcation; F.3.1 [Logics and Meanings of Programs]: Spec\u00adifying and \nVerifying and Reasoning about Programs; F.3.2 [Logics and Meanings of Programs]: Semantics of Pro\u00adgramming \nLanguages Program analysis General Terms Algorithms, Theory, Veri.cation This research was supported \nin part by the National Science Foundation Career Grant No. CCR-9875171,and ITR Grants No. CCR-0085949 \nand No. CCR-0081588,and gifts from Mi\u00adcrosoft Research. The information presented here does not nec\u00adessarily \nre.ect the position or the policy of the Government and no o.cial endorsement should be inferred. Permission \nto make digital or hard copies of all or part of this work for personal or classroom use is granted without \nfee provided that copies are not made or distributed for pro.t or commercial advantage and that copies \nbear this notice and the full citation on the .rst page. To copy otherwise, to republish, to post on \nservers or to redistribute to lists, requires prior speci.c permission and/or a fee. POPL 03, January \n15 17, 2003, New Orleans, Louisiana, USA. Copyright 2003 ACM 1-58113-628-5/03/0001 ...$5.00.  Keywords \nA.ne Relationships, Linear Equalities, Random Interpreta\u00adtion, Randomized Algorithm 1. INTRODUCTION In \nthis paper, we take a fresh look at program analysis and explore what can be learned about a program \nby running it on a small number of input values. At .rst sight, the answer appears to be discouraging. \nAfter all, we know that testing is only as good as the coverage of the test cases, and that even loop-free \nprograms have an exponential number of paths. The problem is that with a small number of inputs we cannot \ncover the entire program. This is because at each branching point we make a binary decision: if the decision \nvariable has value 0 we take one path and if it has value 1 we take the other. But what if we relaxthe \nsemantics of the program and take a middle path instead? For example, we could take 30% of one path and \n70% of the other. This means that we execute both paths, and at the join point we combine the two values \nfor each variable using a factor 30/70 (i.e., the value of variable x after the join is 0.3\u00d7x1 +0.7\u00d7x2 \nwhere x1 and x2 are the values on the joining branches). This gives us a continuum of choices at each \nbranch, among which we choose randomly, with the overall e.ect that each such run of the program involves \nall of the paths. It is not obvious that such a contrived execution has much in common with a real run \nof the program. However, we prove in this paper that this strategy captures most of the a.ne relationships \namong variables at any point in a pro\u00adgram. An a.ne relationship among variables xi (i =1 ..n) . n is \na relationship of the form + c =0, where i=1 aixi ai (i =1..n)and c are some constants. Several classical \ndata .ow analysis problems can be modeled as the problem of detecting a.ne relationships among variables. \nExam\u00adples are: constant propagation (such as x =2), discovery of symbolic constants (such as x =5 \u00d7 N \n+ 1), detection of common sub-expressions. Several loop invariant compu\u00adtations and loop induction variables \ncan also be identi.ed by detecting a.ne relationships. Translation validation [11, 10] also requires \nchecking the equivalence of variables in two versions of a program before and after optimization. Consider, \nfor example, the program shown in Figure 1 (ig\u00adnoring for the moment the annotations shown on the side). \nOf the two assertions at the end of the program, the .rst is true on all four paths, and the second is \ntrue on three of them (it is false when the .rst conditional is false and the second is true). Regular \ntesting would have to exercise that precise path to avoid inferring that the second equality holds. Instead, \nwe propose to use a non-standard execution model. At branches we proceed on both the true and false branch. \nAt joins we choose a random weight w, and we use it to combine the values v1 and v2 of variable v in \nthe two branches, as follows: w\u00d7v1 +(1-w)\u00d7v2. In the example, all variables are dead on entry, so we \nstart the execution with some arbitrary values (shown as * in the .gure). We use the random weights w1 \n= 5 for the .rst branch and w2 = -3 for the second branch. We can then verify easily that the resulting \nstate satis.es the .rst assertion but does not sat\u00adisfy the second. Thus, in one run of the program we \nhave noticed that one of the exponentially many paths breaks the invariant. The price for the simplicity \nof this analysis is that some\u00adtimes it may be unsound, meaning that it may incorrectly claim that a relationship \nholds, even when there are execu\u00adtion paths on which the relationship does not hold. However, we prove \nthat the probability that this happens can be made in.nitesimally small, so that for practical purposes \nwe can assume that the analysis is a sound one. A close analysis of our example shows that there are \nquite a few choices for w1 and w2 that would make it appear that the second assertion also holds (precisely \nthose when either w1 =1 or w2 =0). If the random choice of weights were restricted to 0 or 1 (that is, \nthose modeling actual executions in the program) then the probability of unsound results in one run would \nbe 34 in this case, or 2n-1 worst case in general for a program of size 2n n. However, if we relaxthe \nchoice of weights and let w1 and w2 be 32-bit numbers, there are 233 - 1 choices for which we obtain \nincorrect results. Since the total number of choices for choosing w1 and w2 are 264, the probability \nof obtaining an unsound conclusion is less than 2-31 . If we try to discover relationships by analyzing \nthe values of the variables, we may draw incorrect conclusions. For example, in Figure 1, it may appear \nthat a = -4isan in\u00advariant at the end of the program, which is incorrect. To avoid this problem, we execute \nthe program several times and then look for common relationships among all those ex\u00adecutions. A close \nanalysis of our example shows that if we execute the program once more, the probability of a evalu\u00adating \nto -4 again is precisely the probability that we choose the random weight w1 to be 5 again, which is \nequal to 2-32 , if the weights are 32-bit numbers. There are numerous algorithms in the literature [2, \n5, 9] that can verify or discover a.ne relationships. Our pro\u00adposed algorithm di.ers from these in several \nrespects. It is simpler to implement since it resembles an interpreter, and does not involve complexcomputations \nof convexhulls [5] or a.ne unions of spaces [9]. Our algorithm can be wrong on rare occasions, but by \nrepeated runs we can reduce the probability of failure to a negligible value. Finally, the in\u00adferred \nrelationships are represented implicitly in the form of all the linear relationships that the set of \nruns satis.es. However, the information is easy to extract when needed, and we can quickly answer questions \nabout whether a cer\u00adtain relationship holds by simply checking whether all runs satisfy that relationship. \nIn contrast to the simplicity of the algorithm, the for\u00admal proofs that the soundness probability is \nhigh are subtle, as is usually the case with randomized algorithms. In this paper we present the formal \nproofs for the case when the program involves only linear computations. We discuss .rst how the algorithm \nhandles basic control-.ow elements such Figure 1: A code fragment with four paths. Of the two equations \nasserted at the end the .rst one holds on all paths but the second one holds only on three paths. The \nnumbers shown next to each edge repre\u00adsent values in a random execution. The join points are a.ne combinations \nof the inputs to the join with the weights w1 =5 and respectively w2 = -3. as assignments (Section 3), \njoins (Section 4) and branches (Section 5). Then, in Section 6, we put these pieces together and we state \nand prove the main completeness theorem and the probabilistic soundness theorem. In Section 7 we make \na number of observations related to possible extensions of these techniques beyond linear arithmetic. \n 2. NOTATION In this paper we work with a simple imperative language containing the following a.ne expressions \nover integers Z (here q . .and x is one of n variables): e ::= x | q | e1 + e2 | e1 - e2 | e \u00d7 q '' We \nwrite e[e/x] to denote the result of substituting efor x in e. The random interpreter needs to perform \ndivisions and hence it must work with values chosen from a .eld I(recall that a .eld is a mathematical \nstructure whose elements have multiplicative inverses). For the purpose of the discussion in the following \nthree sections, the reader may consider that the .eld Iis the set of rationals Q. However, as we will \nsee later, another choice for Iappears to be more appropriate both for technical as well as for implementation \nreasons. A state . is an assignment of .eld values to the n vari\u00adables. Occasionally, in order to expose \nthe geometric intu\u00aditions behind the algorithms, we also refer to the n variables as coordinates and \nto states as points in In .We write [ e]]. for the meaning of e in the state . (using the usual interpre\u00adtation \nof the arithmetic operations over I). The notation .[x . q] denotes the state obtained from . by setting \nthe value of variable x to q. We say that a state . satis.es an equation e =0 when [ e]]. =0. We write \n. |= e =0 when this is the case. Our algorithm requires several runs with random input values. The random \ninterpreter can be thought of as exe\u00adcuting the program in parallel on a collection of states. We refer \nto such a collection of states S as a sample and we write Si to refer to the ith element of the sample \nS (the state corresponding to the ith run). In the geometric inter\u00adpretation, a sample is a sequence \nof points. Throughout this paper we assume that all samples have r elements, where r is a parameter of \nthe algorithm. We say that a sample sat\u00adis.es a linear equation e = 0 when all of its states satisfy \nthe equation. We write S |= e= 0 when this is the case. Whenever the interpreter must choose some .eld \nvalue at random, it does so independently of the previous choices and uniformly at random (u.a.r.) from \nsome .nite subset F of I of size d, which is another parameter of the algorithm. With a larger value \nof d, the probability of errors in the algorithm is smaller, but the interpreter must have more random \nbits to make the choices. Throughout the paper we use the a.ne combination oper\u00adation on .eld values \nand on states. An a.ne combination is a weighted average of a number of values such that the sum of the \nweights is 1. We perform this operation most often on two values, in which case we write q.w q ' for \nthe value q\u00d7 w+ q ' \u00d7 (1 - w). We refer to w as the weight of the com\u00adbination. We extend this operation \nto states, in which case we perform the combination with the same weight for each variable. If the states \n.1 and .2 are viewed as points in In then their a.ne combinations are the points situated on the line \npassing through .1 and .2. We further extend the a.ne combination to samples, in which case we combine \neach pair of corresponding states using a separate weight factor: '' (S.[w1,...,wr] S )i = Si .wi Si \n(i =1 ..r) In the following sections we consider separately the oper\u00adation of the random interpreter \non various nodes of a .ow\u00adchart. Then, in Section 6 we put the pieces together and we de.ne precisely \nthe random interpreter algorithm. 3. THE ASSIGNMENT OPERATION In the case of assignments the random \ninterpreter behaves exactly as a concrete interpreter. For the assignment x:= e, it transforms each state \nin the sample by setting x to the value of e in that state. If the sample before the assignment is S \nthen the sample after the assignment is S ' such that: Si ' = Si[x. [[e]]Si] 4. THE UNION OPERATION \nThe random interpreter executes both branches of con\u00additionals. Assume that the interpreter reaches a \njoin point with two samples S and S ' . Each of these samples encodes implicitly a number of a.ne relationships \nbetween variables. In order to continue with only one sample after the join point, we perform a union \noperation in which the resulting sample Su encodes (implicitly) all of the relationships that S and S \n' have in common, and no other relationships. In previous work, the union operation is quite involved, \nrequir\u00ading complexalgorithms for computing the a.ne union of a.ne spaces [9] or convexhulls when a.ne \ninequalities are also handled [5]. In contrast, a random interpreter simply chooses r random weights \nw1,...,wr from F and computes Su ' = S.[w1,...,wr] S Figure 2: Example of union operation on two 4\u00adpoint \nsamples, S and S ' with result Su.The sample S satis.es the equation y =4, and the sample S ' satis.es \nthe equation y = x- 2. The points in the resulting sample Su are marked with stars and are chosen at \nrandom on the lines joining corresponding points in S and S ' . All the points lie in the plane z =0. \nThe union operation has a simple geometric intuition as shown in Figure 2 for 4-point samples. The sample \nS is ob\u00adtained after the sequence of assignments y := 4; z := 0 and S ' after x := x+1; y := x- 2; z \n:= 0 . In both cases, the samples are obtained with the initial random values of x being 0, 2, 3 and \n4. For each pair of corresponding points in the two samples, a point is chosen at random on the line \njoining the points. For example, the weight w1 is 0.5and thus S1 u is at midway between S1 and S1' .The \nweight factor can be greater than one or less than zero, as is the case for the third and fourth points. \nThis example demonstrates two important aspects of the union operation. First, the states in both original \nsamples satisfy the equation z = 0 (and no other common a.ne relationship among variables). Notice that \nSu also satis.es this relationship. Thus, the union op\u00aderation preserves common a.ne relationships. This \nis the completeness property that we state and prove below. The other notable aspect is that it is possible \nbut highly unlikely for the Su points to satisfy some other a.ne relationship (i.e., for all the points \nin Su to be aligned). This is the probabilistic soundness property that we state precisely be\u00adlow. An \nexample using the union operation is shown in Fig\u00adure 1 for a 1-point sample. The initial values of variables \nare not relevant because the variables are not live on input (we show these values in .gure as *). The \n.rst union oper\u00adation is performed with the weight w1 = 5. Notice that the common relationship a+ b = \n1 is preserved after the .rst union operation. The second union operation is performed with the weight \nw2 = -3. On the resulting state, we can verify the .rst assertion but not the second. In fact, we prove \nbelow that the probability that the second assertion would be satis.ed accidentally is extremely small \nbecause there is at least one path on which it is not satis.ed. We conclude the discussion of the union \noperation with the statement and proof of the completeness and probabilis\u00adtic soundness. The completeness \nlemma for the union operation states that the resulting sample satis.es all a.ne relationships that are \nsatis.ed by both of the original states. Lemma 1 (Union Completeness Lemma). Let S and S ' be two r-point \nsamples whose points satisfy the a.ne equation g =0. Then, for any choice of weights w1 ,...,wr the union \nSu = S.[ w1,...,wr] S ' also satis.es the same equa\u00adtion. Proof. It is easy to verify that if g is a.ne, \nthen for any a.ne combination of two states ..w . ' ,we have [ g]](..w . ' )=[[g]]..w [[g]]. ' .Thus \nif the value of g is zero in both the states . and . ' , then the value of g is zero also on their a.ne \ncombination. From here the completeness statement follows immediately. The following probabilistic soundness \nlemma for the union operation states that the probability of choosing the com\u00adbination weights such that \na new a.ne relationship is in\u00adtroduced is extremely small (for a large enough choice of weights). Lemma \n2 (Union Soundness Lemma). Let S and S ' be two samples and let g be an expression such that S | = g \n= 0. More speci.cally, let t be the number of points in S that do not satisfy g =0.Let w1 ,...,wr be \nchosen u.a.r. from F and independently of each other and of the expression g.Let Su = S.[ w1 ,...,wr] \nS ' . Then the probability that Su |= g =0 is at most ( d 1 )t . Proof. Without any loss of generality, \nlet S1 ,...,St be the tstates in S that do not satisfy g = 0. For any i .1 ...t, consider the line joining \nthe points Si and Si' .If[ g]]Si = [[g]]Si' , then this line is parallel to the hyperplane g =0, and \nhence, no point on this line satis.es the equation g =0. In other words, for any choice of wi,[[g]]Siu \n=[[g]]Si =[[g]]Si ' =0 and thus the probability that Su |= g =0 is zero. If on the other hand [ g]]Si \ni' , then this line intersects the =[[g]]S hyperplane g = 0 in exactly one point. In other words, [[ \ng]] S. there is only one choice for wi (i.e., i ) such that [[ g]] S.-[[ g]] Si i [[g]]Siu = 0. Thus, \nthe probability that the weight wi was chosen such that state Siu satis.es the equation g =0 is precisely \nd 1 .Since w1 ,...,wt are all independent, it follows that the probability that all the states S1 u ,...,Stu \nsatisfy the equation g = 0 is less than or equal to ( d 1 )t.This is an upper bound on the probability \nthat all the states in Su satisfy g =0.  5. THE INTERSECTION OPERATION Next we consider the a.ne relationships \nthat are intro\u00adduced by conditionals. Consider the following program: a:=x +y ; if x =ythen b:= aelse \nb := 2 *x ; assert (b =2 *x) The assert statement is true but in order to prove it we must notice that \nx=y in the true branch of the conditional. The random interpreter reaches the conditional with some sample. \nIn order to re.ect the conditional in the branches, we must change the sample (since all relationships \nare ex\u00adpressed implicitly as those satis.ed by the sample). We could try to restart the interpretation \nwith values that sat\u00adisfy the conditional, but .nding such initial values is hard. Or we could split \nthe sample into two parts, one that satis\u00ad.es the conditional and one that does not. But splitting is \nundesirable because working with smaller samples increases Figure 3: A code fragment showing the use \nof 3\u00adpoint sample intersection. The numbers shown next to each edge represent random samples. Adjust\u00adment \nis used to obtain the sample S ' from S,as detailed in Figure 4. the probability that some accidental \nrelationship holds. Fur\u00ad thermore, as we shall see, the probability that enough points satisfy the equality \nis extremely small anyway. Notice that we could not do something cheap such as replacing the oc\u00ad currences \nof x with y in the true branch; this would not help in this case. We have to somehow adjust all of the \npreviously computed variables as well, such as a in this example. One of the novel aspects of this work \nis a procedure for transforming the sample in such a way that all of the pre\u00ad viously existing relationships \nstill hold, and additionally ex\u00ad actly one new relationship holds: the one given by the con\u00ad ditional. \nWe do this by essentially projecting the sample points onto the plane given by the conditional. Orthogonal \nprojection does not work since it destroys a.ne relation\u00ad ships. Instead we use the following function \nAdjust(S,e)to adjust the sample S according to the conditional e=0: Adjust(S, e)= Pick Si and Sj in \nS such that [[e]]Si =[[ e]]Sj . Pick w .Isuch that .0 = Si .w Sj has the property that [[e]].0 =0 and \n[[e]].0 for all k .{1 ...r}. =[[e]]Sk For all k, let S ' be the intersection of the plane k e=0 with \nthe line passing through .0 and Sk, ' [[ e]] .0 i.e., S = Sk ..0 , where wk = . kwk [[ e]] .0-[[ e]] \nSk The result is [S1 ' ,...,Sr' ]. An example of such an adjustment is shown in Figure 3. Here the program \nmentioned at the beginning of this section is executed on the 3-point sample shown at the top of the \n.gure. Adjustment is used to obtain the sample S ' from S. Notice that all of the states in S satisfy \na = x+ y (due to the assignment). Now consider the distribution of the points in S when viewed inside \nthe plane a = x+ y (as shown in  Figure 4: The detailed adjustment operation used in the example from \nFigure 3. The adjusted points are obtained as the intersections of the lines connecting the original \npoints with .0 . Figure 4). We pick the points S1 and S2 to play the role of Si and Sj from the de.nition \nof Adjust (since the expression x -y has di.erent values on those points). Then we pick another point \n.0 on the line determined by these two points. We picked .0 at the intersection with the y axis but the \nonly requirements for .0 are that it is not in the plane x -y =0, and that the lines passing through \nit and all the points in S are not parallel to the plane x -y = 0. Then we obtain the points Sk' (k =1, \n2, 3) as the intersections of the lines that pass through .0 and Sk with the plane x = y.Notice that \ntwo of the points will coincide. The intuition behind this construction is that all of the points Sk \n' are obtained as a.ne combinations of three points Si,Sj and Sk. As such they will satisfy all a.ne \nrelation\u00adships between variables that are satis.ed by all points in the original sample S. Furthermore, \nit is intuitive that since the original points are spread in the plane a = x+y the resulting points are \nspread in the intersection of that plane with x = y (with the exception that one of the points is sacri.ced \nand will be equal to some other point). Returning to the example from Figure 3, we see that the sample \nis adjusted only in the true branch of the equality conditional, since there is no a.ne equality that \nwe can infer from a disequality. Notice that after adjustment the sample satis.es both the original relationships \n(a = x + y)and also the new one due to the conditional (x = y). Finally, the join operation is done using \nthe random weights -2, 3 and -4 and the resulting sample now clearly re.ects the desired assertion b \n=2 \u00d7x (precisely because both sides of the join re.ect the same assertion). There are a few details in \nthe de.nition of Adjust that deserve discussion. The .rst step of the algorithm presumes the existence \nof two points at which e has di.erent values. If there are no such points, it means that e has the same \nvalue q on all the points in the sample. In such a case we need not perform any adjustment. Instead we \ndeclare that e = q holds before the conditional and accordingly we consider only one branch depending \non whether the constant q is zero or not. When this is the case we say that Adjust is not de.ned on the \npair (S, e). Thesecond linein the Adjust algorithm .nds a point on the line Si to Sj that makes e have \na non-zero value distinct from the values at the original points. Since e has di.er\u00adent values at Si \nand Sj this is always possible and .nding such a value is a linear-time operation. Also, .nding the intersection \nof a line with a plane is a simple computation. To complete the discussion of the intersection operation, \nwe state and prove below the completeness and then the soundness lemmas. The completeness lemma states \nthat the adjusted sample satis.es all of the a.ne relationships satis\u00ad.ed by the original sample and \nsatis.es also the relationship for which the original sample was adjusted. Lemma 3 (Intersection Completeness \nLemma). Let e and g be expressions and let S be a sample such that S ' = Adjust(S, e) is de.ned. Then \nfor any choice of the intermediate point .0 we have that S ' |= e =0 and if S |= g =0 then S ' |= g =0. \nProof. By de.nition of S ' we have that each Sk ' from S ' satis.es e =0. Since .0 is an a.ne combination \nof Si and Sj (the two points picked in the .rst step of Adjust) then it satis.es all a.ne relationships \nthat both Si and Sj satisfy, hence also g =0. Now each Sk ' from S ' is an a.ne combination of Sk and \n.0 and therefore it also satis.es g = 0. The following soundness lemma implies that the adjusted sample \nsatis.es exactly one more linearly independent a.ne relationship than the original sample. Lemma 4 (Intersection \nSoundness Lemma). Let e and g be expressions and let S be a sample such that Adjust(S, e) is de.ned. \nFor any choice of the intermediate point .0 ,there exists a . Isuch that if any t states in the sample \nAdjust(S, e) satisfy the equation g =0, then the corresponding t states in the sample S satisfy the equation \ng + ae =0. [[ g]] .0 Proof. Let S ' = Adjust(S, e)and let a be -(which [[ e]] .0 is de.ned by the choice \nof .0 ). Without any loss of general\u00adity, let S1 ' ,...,S t ' be the t states in the sample S ' that \nsatisfy the equation g = 0. For any k .{1,...,t},we have that [[ e]] .0 S ' = Sk ..0 ,where wk = .Since \nS ' satis\u00ad kwk [[ e]] .0-[[ e]] Sk k .es the equation g = 0, we can verify that Sk satis.es the equation \ng -[[ g]] .0 e = 0. This completes the proof. [[ e]] .0 The geometric intuition behind the soundness \nlemma is that if some subset of the adjusted points lie in the hyper\u00adplane g = 0, then the corresponding \nsubset of the original points lie in the hyperplane that contains the point .0 and passes through the \nintersection of the hyperplanes g =0 and e = 0. The soundness lemma implies that any equa\u00adtion g = 0 \nthat is satis.ed by the adjusted sample can be expressed as a linear combination of the equation e =0 \nand some equation g ' = 0 that is satis.ed by the original sample. Note that the soundness lemma indicates \none such g ' (i.e., g + ae). We can see from the proofs of the lemmas that as long as the adjusted points \nare a.ne combinations of the orig\u00adinal points, the completeness lemma will be satis.ed. Ini\u00adtially, we \ntried to compute a.ne combinations of just two points (by computing the intersection of the line they \ndeter\u00admine and e = 0). This made it impossible to state a clean soundness result in some corner cases; \nhence the three-point combination that we use at the moment. More complicated a.ne combinations could \nalso be used and they might have the bene.t of avoiding the overlap of two adjusted points. Notice that \nduring adjustment two distinct points in the original sample are transformed into the same point in the \nadjusted sample, thereby e.ectively reducing the number of points in a sample after each adjustment. \nThis is not entirely unexpected. Since each adjustment crowds the sample into one fewer dimensions (e.g. \nfrom 3-d space into a 2-d plane), we expect that not all of the r points are going to remain independent. \nWe shall see in Section 6 that because of this we need to start our random interpreter with a sample \nlarger than the maximum number of adjustments on any path through the program. If we don t, then it is \nvery likely that our algorithm will announce false relationships. For example, if we redo the example \nfrom Figure 3 with just the .rst two states (meaning that we ignore the third column in all samples), \nthen after adjustment we will falsely infer many relationships, such as a = 3 and many others (there \nare many planes that pass through the point S1 ' = S2' ).  6. THE RANDOMIZED INTERPRETER We now put \ntogether the ideas mentioned in the previous sections to de.ne the randomized interpreter R.For nota\u00adtional \nconvenience, let us extend the de.nition of a sample as follows. A sample is either a sequence of r states \nor is unde.ned, in which case we write it as ..We also say that .|= g = 0 for any expression g. Essentially, \nRinterprets a program like an abstract interpreter. The action of R over the various nodes of a .ow-chart \nis de.ned below. Assignment Node: See Figure 5 (a). S = .,if S ' = .. Si = Si' [x .[[e]]Si], otherwise. \n  Conditional Node: See Figure 5 (b).  S1 = .and S2 = .,if S ' = . S1 = S ' and S2 = .,if S ' |= e \n=0 S1 = S ' = . and S2 ,if S ' |= e -q = 0 for some non\u00adzero constant q S1 = Adjust(S ' ,e)and S2 = \nS ' ,otherwise Join Node: See Figure 5 (c). S = S1,if S2 = . S = S2,if S1 = . S = S1.[w1,...,wr]S2,otherwise, \nwhere {wi}are chosen independently and u.a.r. from F . If Rconcludes that a conditional is always true \n(or always false), then it executes only the true (or false) branch of the conditional. Otherwise, it \nexecutes both branches of the conditional. To start the execution of the program, R chooses r points, \neach with n variables, independently and F n uniformly at random from as the initial sample. The resulting \nsamples can then be used to verify whether desired a.ne relationships hold at certain points in the pro\u00adgram. \nMoreover, a sample can also be used to discover a.ne relationships by computing the a.ne subspace in \nwhich all the points of a sample lie. It is possible that the random interpreter is unsound, meaning \nthat the resulting samples satisfy a.ne relationships that are false on some concrete execution. We prove \nin the rest of this section that the probability of this happening is extremely small, and can be reduced \neven further by repeating the experiment several times. 6.1 Error Probability Analysis In our proof \nof an upper bound on the error probability we make use of the fact that Iis a .nite .eld. (However, we \nfeel that it may be possible to prove similar results without this assumption). Working with a .nite \n.eld is also desir\u00adable from an implementation point of view; otherwise the size of the values involved \nduring the random interpretation doubles with each adjust operation. For these reasons we choose d to \nbe a prime number and Ito be the .eld of in\u00ad tegers modulo d.We let F = Iand thus d also represents \nthe size of the .eld I. The arithmetic operations over Iare performed modulo the prime number d and division \nof a by b is implemented as multiplication of a by the multiplicative inverse of b in the .eld I. However, \nour results are valid only if the concrete arithmetic operations of the program are in\u00adterpreted over \nthe same .eld Ias opposed to the intended domain of integers. Thus we choose d larger that any value \narising in a concrete execution of the program. For example, if the program operates on 32-bit numbers \nand if we make the assumption that its operations do not over.ow, then we can choose d to be any prime \nlarger than 232,and we make the interpreter use enough precision to be able to represent the entire \n.eld I. In these conditions we can let F = I. For the purpose of the probability analysis we introduce \nan abstract interpreter A that computes a sound approxi\u00admation of the set of a.ne relationships in a \nprogram. In the following de.nition we use the letter U to range over sets of a.ne relationships. We \nwrite U . g =0 to say that the conjunction of the relationships in U imply g =0. We write U1 nU2 for \nthe set of relationships that are implied by both U1 and U2. (This operation is sometimes called the \nunion of a.ne spaces [9]). Finally, we write U[e/x]for the relationships that are obtained from those \nin U by substitut\u00ading e for x. For notational convenience, we let . represent an inconsistent set of \na.ne relationships. We also say that ..g = 0 for any expression g. With these de.nitions we can de.ne \nthe action of Aover the nodes of a .ow-chart as follows: Assignment Node: See Figure 5 (a). U = .,if \nU ' = . U = {x = e[x ' /x]}.U ' [x ' /x], otherwise, where x ' is a fresh variable  Conditional Node: \nSee Figure 5 (b). U1 = U ' and U2 = .,if U ' .e =0  U1 '' = . and U2 = U ,if U . e -q =0for some non-zero \nconstant q U1 ' = U .{e =0} and U2 = U ,otherwise Join Node: See Figure 5 (c). U = U1,if U2 = .  Figure \n5: Flow-chart nodes U = U2,if U1 = . U = U1 nU2,otherwise The abstract interpreter starts with the empty \nset of a.ne relationships between variables. Implementations of abstract interpretations such as Ahave \nbeen described in the literature. The major concern there is the concrete representation of the set U \nand the imple\u00admentation of the operation U1 nU2. For example, Karr s algorithm [9] uses the union of \na.ne spaces, while Cousot s algorithm [5] uses convexhulls to implement the stronger operation that also \nhandles a.ne inequalities. Here we use Aonly to state and prove the soundness and completeness results \nof the random interpreter R. Given a program, the sets of a.ne relationships U com\u00adputed by A at every \npoint in the program is uniquely de\u00ad.ned. Corresponding to each such U, there is a random sample S (which \ndepends on the random choices made by R). We state below the relationship between U and S,in the form \nof completeness and soundness theorems. Theorem 5 (Completeness Theorem). Let U be a set of a.ne relationships \ncomputed by A at a given point in the program and let S be the corresponding sample. For any expression \ng,if U .g =0,then S |= g =0. The proof of Theorem 5 is based on Lemma 1 and Lemma 3, and is given in \nAppendixA.1. The completeness theorem implies that the random interpreter Rdiscovers all the a.ne relationships \nthat the abstract interpreter Adiscovers. The next de.nition is necessary for the statement of the soundness \ntheorem. Definition 6. For any set of a.ne relationships U = . computed by A for a program point, any \nsubset S of the random sample S computed by R for the same point, and any expression g such that U S, \ng) be the .g =0,let E(U, event that all points in S satisfy the equation g =0.Let Pr(E(U, S, g)) denote \nthe probability of this event over the random choices made by the random interpreter R.Let P (U, t)= \nmax{Pr(E(U, S .S, | . S, g)) | S|= t} Note that the undesirable event E(U, S, g) occurs only when the \nrandom interpreter claims that g = 0 holds but the abstract interpreter claims that it does not. The \nproba\u00adbility of this event is bounded by the quantity P (U, r). The soundness theorem provides a bound \nfor P (U, r). However, in order for the inductive proof to work, the soundness the\u00adorem actually provides \na bound for P (U, t), where 1 =t =r is a parameter. Theorem 7 (Soundness Theorem). For any set of a.ne \nrelationships U computed by A for a program point, \u00d7( j+1 P (U, t) =(2d)bd )t,where b and j are the maximum \nnumber of intersection (branches) and union (join) opera\u00adtions respectively performed by A on any path \nbefore com\u00adputing U. According to Theorem 7, given any relationship not ver\u00adi.ed by the abstract interpreter, \nthe probability (over the random choices made by the random interpreter) that the re\u00adlationship is veri.ed \nby the random interpreter is extremely small. The proof of Theorem 7 is non-trivial, but is easy to follow \nonce the reader is comfortable with the above no\u00adtation. The proof is given in AppendixA.2. We use this \nsoundness theorem in the next section to prove Theorem 8, which establishes an upper bound on the probability \nthat R is unsound even for programs involving loops. The bound on P (U, t) can be interpreted as follows. \nEach intersection operation increases the probability of error by afactor of2d while all the j join operations \ntogether con\u00adtribute a factor of ( j+1 )t . The latter can be explained in a d rather interesting manner. \nIf R does not perform any ad\u00adjust operations (b = 0), then it can be viewed as evaluating multivariate \npolynomials of degree j + 1 whose variables are the input variables of the program along with a variable \ncor\u00adresponding to each join operation. For example, in Figure 1 the multivariate polynomial corresponding \nto variable a is w1(0) + (1 -w1)(1) =1 -w1, and that corresponding to b is w1(1) + (1 -w1)(0) =w1. Similarly \nthe polynomial cor\u00adresponding to variable c is w2(b -a)+(1 -w2)(2a + b) = w2(w1-1+w1)+(1-w2)(2-2w1+w1) \n=3w1w2-w1-3w2+2 and that corresponding to d is w2(1-2b)+(1-w2)(b -2) = w2(1 -2w1)+(1 -w2)(w1 -2) =-3w1w2 \n+ w1 +3w2 -2. Note that the multivariate polynomial corresponding to the expression c + d is identically \nequal to 0 and hence Ralways succeeds in verifying the assertion c+d = 0, while the multi\u00advariate polynomial \ncorresponding to the expression c -a -1 is 3w1w2 -3w2 which is not identically equal to 0 and hence Rdeclares \nthe assertion c-a-1 = 0 to be invalid with high probability. The classic random testing procedure to \ncheck whether a multivariate polynomial of degree j + 1 is iden\u00adtically equal to 0 or not has an error \nprobability at most ( j+1 )t d ,where d is the size of the set from which random values are chosen and \nt is the number of trials [13]. This is precisely the factor contributed by j join operations to the \nbound for P (U, t). For the informal intuition behind the factor corresponding to intersections consider \nthe case when j =0. Then each adjust operation has the e.ect of reducing the number of useful points \nby 1 (along with an additional factor of 2 that is necessary to accommodate some corner cases).  6.2 \nFixed Point Computation The lattice of sets of a.ne relationships (under the set union operation, and \nthe intersection operation as described in Section 6.1) has .nite depth nsince there can be at most nlinearly \nindependent a.ne relationships involving nvari\u00adables. Thus the abstract interpreter A interpreting a \npro\u00adgram with loops is guaranteed to reach a .xed point. Given the close relationship between A and R \nas mentioned in Theorem 5 and Theorem 7, R also reaches a .xed point with high probability. One way to \ndetect when R has reached the .xed point is to compare the rank of the samples (viewed as matrices) at \nrelevant locations in two successive iterations of a loop. The rank of a sample is always equal to the \nnumber of variables minus the number of linearly independent relationships that the sample satis.es. \nThus, if the rank has stabilized, the number of linearly independent relationships satis.ed by S has \nbeen stabilized, and so has the set of a.ne relationships satis.ed by S. We now state and prove the .nal \nresult that bounds the probability of error in the operation of R.In particular, it also bounds the error \nprobability that R does not reach a .xed point, or that the rank testing mechanism fails to correctly \ndetect the .xed point. Theorem 8 (Probabilistic Soundness Theorem). The probability that some random \nsample S models a rela\u00adtionship g=0 that is not implied by its corresponding set of n \u00d7 ( j+1 a.ne relationships \nU,is atmost m\u00d7 2d\u00d7 (2d)bd )r , where m is the total number of operations performed by A before reaching \nthe .xed point (among which there are bin\u00adtersections and j union operations). Proof. Consider one particular \nsample Sand the corre\u00adsponding set of a.ne relationships U. There are less than 2dn di.erent a.ne relationships \nwith coe.cients from Ibe\u00adtween the nvariables and hence this is an upper bound on the number of relationships \nnot implied by U. Thus, it fol\u00adlows from Theorem 7 that the probability that there exists an expression \ng such that S |= g =0and U . g =0 is n \u00d7 ( j+1 at most 2d\u00d7 (2d)bd )r . And since there are msuch samples, \nthe desired result follows easily. Note that if all samples S model only relationships that are implied \nby the corresponding set of a.ne relationships U,then R reaches a .xed point when A does so, and the \nrank testing mechanism faithfully tells whether or not the .xed point has been reached. Theorem 8 says \nthat the prob\u00adability that this happens is high. Theorem 8 implies that r must be O(n+ b)to achieve a \nsmall error probability. In particular, if d>max{j3 ,2m,8}and we pick r>1.5(n+1) +2b, then the error \nprobability 3 (r-1.5(n+1)-2b) is bounded above by ( d 1 ) 2 . 6.3 Computational Complexity The cost \nof each intersection and union operation per\u00adformed by the randomized interpreter is O(nr). Each as\u00adsignment \noperation takes O(r) time assuming that each as\u00adsignment operation involves a constant number of arithmetic \noperations. On the other hand, Karr s deterministic algo\u00adrithm [9] incurs a cost of O(n 3) for each union \noperation and O(n 2) for each intersection operations. Computing the rank of a sample takes O(n 3) time. \nThis may be expensive considering that the other operations per\u00adformed by R are at most O(nr). An alternative \nway to ensure that a .xed point was reached is to estimate an up\u00adper bound on the number of iterations \nrequired by A to reach a .xed point for a loop and then let R go around that loop that many number of \ntimes. Note that since n is the depth of the lattice of the sets of a.ne relationships, it determines \nan upper bound on the number of iterations required to reach a .xed point.  7. BEYOND AFFINE EQUALITIES \nThe language of expressions that we have considered so far allows only for a.ne arithmetic expressions. \nIn this section we speculate about the uses of randomized algorithms for programs containing other features \nas well. We have experimented successfully with the algorithm presented here even for arithmetic expressions \nthat are non\u00adlinear. We believe that the probabilistic soundness results still hold but we have yet to \nprove this formally. Another dif\u00ad.culty is that completeness would be seriously compromised by the union \noperation and it is not clear how to implement e.ciently an appropriate intersection operation. In the \nab\u00adsence of union and intersections (e.g. for a basic-block anal\u00adysis) both completeness and probabilistic \nsoundness can be attained. Consider, for example, the following basic block: a:=(y + 1) \u00d7 (y-1) ; b:=y \n\u00d7 y-1; assert (a = b) To prove the assert statement, we need to prove that (y+ 1) \u00d7 (y- 1) = y\u00d7 y- 1. \nThere is no known deterministic polynomial-time algorithm to solve the above problem (the full monomial \nexpansion of a polynomial can be exponential in the size of the original polynomial). However, there \nis a very simple and fast randomized algorithm (see [13]) that is complete and probabilistically sound: \ncompare the values of the two polynomial expressions on a few random inputs. We consider in this paper \nonly equality and disequality conditionals. The algorithm can be applied to other kinds of conditionals, \nby not doing any adjustment. The soundness results still hold but this weakens the analysis since it \nis not able to exploit the information from such conditionals. In future work we plan to explore alternatives \nfor the union and intersection procedures that work with inequalities as well. Interestingly, if we restrict \nthe weights to the range [0,1] then not only a.ne equations but also a.ne inequalities are preserved. \nIt is worth mentioning that the random interpreter ap\u00adproach seems to be ill suited for verifying disequalities. \nFor example, just because a number of random runs for a pro\u00adgram yield non-zero results we cannot conclude \nthat there is no run in which the result is zero. However, we can in\u00adfer some disequalities that are \nconsequences of a.ne equal\u00adities. For example, if x, y and z are integer variables and x-y= a \u00d7z+bsuch \nthat either a=0 and b=0or 0 <b<a (for some integer constants aand b), then we know that it cannot be \nthe case that xand yare equal. The analysis that we have described is a path-insensitive analysis. However, \nby choosing the random values for each join carefully, we can capture some path-sensitive informa\u00adtion. \nFor example, consider verifying the assert statement at the end of the following program: if (x =y) then \na:= 1else a:= 4; if (x =y) then b:= 2else b:= 5; assert (b= a+ 1) If R chooses di.erent random values \nfor the two joins in the above program, it fails to verify the assert statement. However, if the same \nrandom values are chosen for the two joins, R is able to verify the assert statement. Note that choosing \nsame random values for joins that correspond to equivalent conditionals does not break any of our proba\u00adbilistic \nsoundness results. 8. COMPARISON WITH RELATED WORK Blum, Chandra and Wegman [3] showed how to compute \n.ngerprints of read-once branching programs in order to de\u00adcide their equivalence in probabilistically \npolynomial time. Their idea was to assign random values to boolean variables instead of the usual 0 or \n1 boolean values and then evalu\u00adate the branching program by performing multiplication in place of conjunction, \naddition in place of disjunction and subtraction from 1 in place of negation. Our technique for handling \njoins is reminiscent of their idea, where we also as\u00adsign a random value instead of the typical 0 or \n1 boolean values at join points. Aiken, F\u00a8ahndrich and Su [1] have used random sampling for race detection \nin Relay Ladder Logic programs with probabilistic guarantees. F\u00a8ahndrich, Foster, Su and Aiken [7] have \nused randomization for e.ciently solving general in\u00adclusion constraints in the context of pointer analysis \nfor C programs. Value numbering is a technique whereby hash values are assigned to expressions and variables \nwith equivalent hash values are declared to be equal [12]. However, the problem with this technique is \nthat it is very closely tied to the struc\u00adture of expressions rather than their semantics. For example \nit cannot detect that (x + y)+ z =(z + x)+ y.In some sense, a sample can be thought of as a set of hash \nvalues for the program variables at that location, except that we can maintain it across assignments, \nunion and intersection operations. Wegman, Sreedhar and Bodik [14] have inde\u00adpendently extended value \nnumbering to be less sensitive to the syntaxof the expressions. They are also using an a.ne combination \nfor joins but do not have an intersection oper\u00adation. Random testing [8] is most commonly used to verify \nas\u00adsertions in a program, and this technique has recently been used to discover useful invariants from \nprogram traces [6]. However, the greatest problem with this kind of approach is its lack of soundness \nsince it is only practical to explore a limited number of paths. Our technique is similar in spirit to \nthis technique but avoids these problems by executing both sides of a branch (and locally adjusting the \nvalues of variables to account for the latest path predicate) and then merging the results at join points. \nSymbolic analysis techniques have also been used to dis\u00adcover linear relationships among variables. For \nexample, Karr [9] describes an abstract interpretation on a lattice of a.ne relationships between variables. \nHis analysis is able to infer the same relationships as the one presented in this paper, in the case \nwhen the program has only linear compu\u00adtation. In the presence of non-linear computations our al\u00adgorithm \nis slightly more complete as explained in Section 7. Karr s algorithm works on a lattice of .nite depth \nwhose union and intersection operations require O(n 3)and O(n 2) arithmetic operations respectively while \nour algorithm re\u00adquires O(nr) arithmetic operations for both joins and inter\u00adsection. The real complexity \nhowever in Karr s algorithm is in the implementation. The computation of an a.ne union of spaces is signi.cantly \nmore involved than our join opera\u00adtion. Just like for our algorithm, their union and intersection operations \nrequire multiplication of two numbers of the same size. Although the paper is silent about this aspect, \nan im\u00adplementation of the algorithm must deal with exponentially large numbers. The abstract interpretation \nalgorithm used by Cousot and Halbwachs [5] goes a step further and dis\u00adcovers also linear inequality \nrelationships among variables. This algorithm also appears to su.er from the presence of exponentially \nlarge numbers. It is interesting to compare the random interpreter ap\u00adproach described in this paper \nwith abstract interpretation [4] in general. In both cases there are union and intersection operations \n(called join and meet in abstract interpretation). The samples used by the random interpreter are represen\u00adtatives \nof the sets of a.ne relationships (satis.ed by them) which form a lattice (under the set union operation, \nand the intersection operation as described in Section 6.1). However, on rare occasions, the random interpreter \nmight perform an unsound join operation (returning a sample whose represen\u00adtative is not greater than \nthe lowest upper bound of the representatives of the joined samples). 9. CONCLUSION We have presented \nin this paper the preliminary results of our investigation of the use of randomized algorithms in program \nanalysis. We have found that by running the pro\u00adgram on random inputs and by relaxing the semantics of \nthe conditionals, such that we execute both branches of each conditional, we can quickly compute a .ngerprint \nof the program that re.ects a.ne invariants of the program and, with high probability, nothing more. \nThe surprising result is that with this form of testing, a single run through the program captures information \nabout all the possible paths, thus making it possible to .lter out quickly the invariants that hold only \non certain paths. The algorithms discussed in this paper are a selection from a set of algorithms that \nwe are exploring. A general char\u00adacteristic of the progress in our project has been that the algorithms \nfor random interpretation are fairly easy to de\u00adsign and most often trivial to implement. In each case \nour intuition suggests that the probability of unsound results is extremely small, and in fact experiments \ndo not reveal any unsoundness. But proving an upper bound for the probabil\u00adity of unsoundness has been \nan extremely challenging task, and most often we have to settle with conservative bounds. Nevertheless, \nwe believe that randomization has much to o.er to program analysis and this area is worth of future research. \nProgram analysis is provably hard, and we have all learned not to expect perfect results. However, this \nattitude has manifested itself mostly in a large number of static analy\u00adsis approaches in which completeness \nis sacri.ced and false positives are accepted as a fact of life, while soundness re\u00admains the sine qua \nnon of program analysis. The results of this paper show that it might be pro.table to relaxthis strict \nview of soundness, and trade o. minuscule amounts of soundness in return for other advantages such as \nbetter computational complexity, or simplicity, or even more pre\u00adcise results. And when we think that \nin the grand scheme of things, program analyses are used to produce software that interacts with other \npotentially buggy libraries running on fallible hardware, we realize that maybe a minuscule proba\u00adbility \nof unsoundness in the analysis is tolerable after all. 10. ACKNOWLEDGMENTS We would like to thank Mark \nWegman for providing useful directions and helpful discussions. We would also like to thank the members \nof the Open Source Quality Group for carefully reviewing early drafts and for helping us improve the \npaper. 11. REFERENCES [1] A. Aiken, M. F\u00a8ahndrich, and Z. Su. Detecting races in relay ladder logic programs. \nInternational Journal on Software Tools for Technology Transfer, 3(1):93 105, 2000. [2] B. Alpern, M. \nWegman, and F. Zadeck. Detecting equality of variables in programs. In Fifteenth Annual ACM Symposium \non Principles of Programming Languages, pages 1 11, Jan. 1988. [3] M. Blum, A. Chandra, and M. Wegman. \nEquivalence of free boolean graphs can be decided probabilistically in polynomial time. Information Processing \nLetters, 10:80 82, 1980. [4] P. Cousot and R. Cousot. Abstract interpretation: A uni.ed lattice model \nfor static analysis of programs by construction or approximation of .xpoints. In Proceedings of the 4th \nACM Symposium on Principles of Programming Languages, pages 234 252, 1977. [5] P. Cousot and N. Halbwachs. \nAutomatic discovery of linear restraints among variables of a program. In Proceedings of the 5th ACM \nSymposium on Principles of Programming Languages, pages 84 97, 1978. [6] M. D. Ernst, J. Cockrell, W. \nG. Griswold, and D. Notkin. Dynamically discovering likely program invariants to support program evolution. \nIEEE Transactions on Software Engineering, 27(2):1 25, Feb. 2001.  [7] M. F\u00a8ahndrich, J. Foster, Z. \nSu, and A. Aiken. Partial online cycle elimination in inclusion constraint graphs. In Proceedings of \nthe ACM SIGPLAN Conference on Programming Language Design and Implementation, pages 85 96, Montreal, \nCA, June 1998. [8] D. Hamlet. Random testing. In J. Marciniak, editor, Encyclopedia of Software Engineering, \npages 970 978. Wiley, New York, 1994. [9] M. Karr. A.ne relationships among variables of a program. In \nActa Informatica, pages 133 151. Springer, 1976. [10] G. C. Necula. Translation validation for an optimizing \ncompiler. In ACM SIGPLAN 00 Conference on Programming Language Design and Implementation (PDLI), pages \n83 94, Vancouver, BC, Canada, 18 21 June 2000. ACM SIGPLAN. [11] A. Pnueli, M. Siegel, and E. Singerman. \nTranslation validation. In B. Ste.en, editor, Tools and Algorithms for Construction and Analysis of Systems, \n4th International Conference, TACAS 98,volume LNCS 1384, pages 151 166. Springer, 1998. [12] B. K. Rosen, \nM. N. Wegman, and F. K. Zadeck. Global value numbers and redundant computations. In POPL 88. Proceedings \nof the conference on Principles of programming languages, pages 12 27, Jan. 1988. [13] J. T. Schwartz. \nFast probabilistic algorithms for veri.cation of polynomial identities. JACM, 27(4):701 717, Oct. 1980. \n[14] M. Wegman, V. C. Sreedhar, and R. Bodik. Probabilistic value numbering. Unpublished manuscript, \n2001. APPENDIX  A. PROOF OF SOUNDNESS AND COMPLETENESS THEOREMS We now give the proofs for the completeness \nand sound\u00adness theorems stated in Section 6. Both the abstract inter\u00adpreter A and the randomized interpreter \nR perform simi\u00adlar operations for each node in the .ow-graph. The proofs are by induction on the number \nof operations performed by the interpreters. The computation performed by the inter\u00adpreters can be viewed \nas going forward in the sense that the outputs of a .owchart node are determined by the inputs of the \nnode. Hence, for the inductive case of the proof, we prove that the required property holds for the outputs \nof the node given that it holds for the inputs of the node. A.1 Proof of Completeness (Theorem 5) The \nproof is by induction on the number of operations performed by the interpreters. The base case is trivial \nsince initially U = \u00d8 and hence for any expression g, U . g =0. For the inductive case, the following \nscenarios arise. Let g be any expression. Assignment Node: See Figure 5 (a). Assume that U . g =0. We \nprove that S |= g =0. Consider the expression g ' = g[e/x]. Since U .g =0, U ' .g ' = 0. It follows from \nthe induction hypothesis on U ' and S ' that S ' |= g ' =0. Hence, S |= g =0.  Conditional Node. See \nFigure 5 (b). We prove that  (a) if U1 .g =0, then S1 |= g =0, and (b) if U2 .g =0, then S2 |= g =0. \nThree possibilities arise here.  (i) U ' . e = 0. It follows from induction hypothesis on U ' and S \n' that S ' |= e = 0. By de.nition of A,   U1 ' = S ' = U and U2 = .. Similarly, S1 and S2 = .. (a) \nAssume that U1 .g =0. Thus, U ' .g =0 and by induction hypothesis S ' |= g =0. Thus, S1 |= g =0. (b) \nThe proof obligation for S2 follows immediately. (ii) U ' . e -q = 0 for some non-zero constant q.It \nfollows from induction hypothesis on U ' and S ' that  S ' ' |= e -q = 0. By de.nition, U1 = . and U2 \n= U . Similarly, S1 = .and S2 = S ' . The proof for this case is similar to the symmetric case shown \nabove. (iii) U ' .e -q =0 for any constant q. By de.nition, U1 = U ' .{e =0} and U2 = U ' . (a) Assume \nthat U1 .g = 0. There exists an expres\u00ad ''' ' sion g such that U .g =0 and g = g + .e for some constant \n.. It follows from induction hypothesis on U ' and S ' that S ' |= g ' = 0. It follows from Lemma 3 that \nS1 |= g ' =0 and S1 |= e =0. Hence, S1 |= g ' + .e =0. (b) Assume that U2 . g =0. Thus, U ' . g =0. It \nfollows from induction hypothesis on U ' and S ' that S ' |= g =0. Either S2 = S ' or S2 = ..In either \ncase, S2 |= g =0. Join Node: See Figure 5 (c). Assume that U .g =0. We prove that S |= g =0. By de.nition \nof A, U1 .g =0 and U2 .g =0. By induction hypothesis on U1 and S1 and on U2 and S2 , we have that S1 \n|= g =0and S2 |= g =0. It now follows from Lemma 1 that S |= g =0. A.2 Proof of Soundness (Theorem 7) \nThe proof is again by induction on the number of op\u00aderations performed by the interpreters. For the base \ncase, j = b =0and U = \u00d8.Let g be any expression which is identically not equal to 0. We have S = , all \nof whose S0 F n states are chosen independently and u.a.r. from .The probability that a particular point \nin sample S0 satis.es the equation g =0 is at most d 1 . Thus, the probability that some particular tpoints \nfrom sample S0 satisfy the equation g =0 is at most ( d 1 )t.Hence, P(U,t) =( d 1 )t . For the inductive \ncase, the following scenarios arise. Assignment Node: See Figure 5 (a). We prove that P(U,t)= P(U ' ,t) \nand the result follows from induction hypothesis on U ' and S ' . Let g be any expression such that U \n.g =0. Consider ' the expression g = g[e/x]. Let S be any subset of ' sample S.Let S be the corresponding \nsubset of sample ' ' ' S .Note that event E(U, , ' ,g ) S,g)happens i. E(UShappens. Thus, P(U,t)= P(U \n' ,t). Conditional Node: See Figure 5 (b). We prove that (a) P(U1,t) =(2d) \u00d7P(U ' ,t), and (b) P(U2,t) \n=(2d) \u00d7P(U ' ,t) and the result follows from the induction hypothesis on U1 and S1,and on U2 and S2 . \nThe following possibil\u00adities arise:  (i) U ' .e =0 It follows from Theorem 5 that S ' |= e=0.  '' \n' (a) U1 = U . S1 = S .Thus, P(U1,t)= P(U ,t). (b) U2 = ..Thus, P(U2,t)=0. (ii) U ' .e-q = 0 for some \nnon-zero constant q It follows from Theorem 5 that S ' |= e-q =0. Hence, (a) U1 = ..Thus, P(U1,t)=0 \n '' ' (b) U2 = U and S2 = S .Thus, P(U2,t)= P(U ,t). (iii) U ' .e-q = 0 for any constant q It is possible \nthat S ' |= e-q = 0 for some constant q (note that Theorem 5 does not help us here). This may be a cause \nof unsoundness and hence we consider this possibility while computing P(U1,t)and P(U2,t) below. (a) U1 \n= U ' .{e =0}.Let g be any expression such that U1 ' .g =0. Note that U .g+ .e =0 for any constant ..Let \nbe any subset of t points of sample S1 S1 '' .Let S be the corresponding subset of sample S . S 1 ' The \nevent E(U1 , ,g) occurs only if S |= e-q =0 for some constant q,or S '| = e-q =0for any con\u00adstant q (hence, \nAdjust(S ,e) is de.ned) and the event E(U ' ,S ' ,g+ .e) occurs for any constant . (this follows from \nLemma 4). Thus, Pr(E(U1 ,S 1 ,g)) . . ' ' ' = Pr(S |= e-q =0) + Pr(E(U ,S ,g+ .e)) q.. ... d-1 d-1  \n  .' .' = P(U ,r)+ P(U ,t) q=0 .=0 = d\u00d7P(U ' ,r)+ d\u00d7P(U ' ,t) =(2d) \u00d7P(U ' ,t) Hence, P(U1,t) =(2d) \n\u00d7P(U ' ,t) (b) U2 = U ' .Let g be any expression such that U2. g =0. Let be any subset of t points of \nsample S 2 S2 '' .Let S be the corresponding subset of sample S . ' The event E(U2 ,S2 ,g) occurs only \nif S |= e =0, or ' ' ' S | = e =0 (hence S2 = S ' )and the event E(U ,S ,g) occurs. Thus, S 2 '' ' Pr(E(U2 \n, ,g)) =Pr(S |= e =0) + Pr(E(U ,S ,g)) =P(U ' ,r)+ P(U ' ,t) =2P(U ' ,t) = (2d) \u00d7P(U ' ,t) Hence, P(U2,t) \n=(2d) \u00d7P(U ' ,t) Join Node: See Figure 5 (c). \u00d7( j+1 )t We prove that P(U,t) = (2d)td ,where b and j \nare the maximum number of intersection and union operations performed by A for computing U.Let b1 and \nj1 be the maximum number of intersection and union operations performed by A for computing U1 . Let b2 \nand j2 be the maximum number of intersection and union operations performed by A for computing U2 . Clearly, \nj =1+ max(j1,j2)and b = max(b1,b2). Let g be any expression such that U Thus, . g =0. either U1 = 0. \nConsider the case . g =0 or U2.g when U1 = 0. (The other case is symmetric).  . g Let S be any subset \nof t points of sample S.We are going to compute the probability that S |= g =0 (or, Pr(E(U,S,g ))). Let \nS 1 be the corresponding subset of t points of sample S1.For i .{0,...,t}, let event S 1 Ei be the event \nthat exactly i states in satisfy the equation g = 0. The events Ei form a disjoint partition of the event \nspace. This means that: t Pr(E(U,S,g )) = Pr(Ei) \u00d7Pr(E(U, S,g) |Ei) i=0 By Lemma 2, Pr(E(U, S,g) |Ei) \n(the probability that S |= g = 0 if it is known that exactly i states in S 1 satisfy g =0, or equivalentlythat \nt-i states do not )t-i satisfy g = 0)is atmost( d 1 . In order to compute Pr(Ei) we observe that there \nare (D t S 1 iways to choose a subset of i states from .For each such subset, the probability that it \nsatis.es g =0 is at most (2d)b1 \u00d7( j1d +1 )(i D(by induction hypothesis). In conclusion, Pr(Ei) = ti\u00d7(2d)b1 \n\u00d7( j1d +1 )i . t . Thus, Pr(E(U, = S,g) |Ei) S,g)) Pr(Ei) \u00d7Pr(E(U, i=0 . t (D t (2d)b1 ( j1+1 )i )t-i= \nid \u00d7( d 1 i=0 =(2d)b1 \u00d7( j1d +2 )t \u00d7( j+1 =(2d)bd )t \u00d7( j+1 )t Thus, P(U,t) =(2d)bd .  \n\t\t\t", "proc_id": "604131", "abstract": "We present a new polynomial-time randomized algorithm for discovering affine equalities involving variables in a program. The key idea of the algorithm is to execute a code fragment on a few random inputs, but in such a way that all paths are covered on each run. This makes it possible to rule out invalid relationships even with very few runs.The algorithm is based on two main techniques. First, both branches of a conditional are executed on each run and at joint points we perform an affine combination of the joining states. Secondly, in the branches of an equality conditional we adjust the data values on the fly to reflect the truth value of the guarding boolean expression. This increases the number of affine equalities that the analysis discovers.The algorithm is simpler to implement than alternative deterministic versions, has better computational complexity, and has an extremely small probability of error for even a small number of runs. This algorithm is an example of how randomization can provide a trade-off between the cost and complexity of program analysis, and a small probability of unsoundness.", "authors": [{"name": "Sumit Gulwani", "author_profile_id": "81100315615", "affiliation": "University of California, Berkeley, CA", "person_id": "PP14115174", "email_address": "", "orcid_id": ""}, {"name": "George C. Necula", "author_profile_id": "81100295630", "affiliation": "University of California, Berkeley, CA", "person_id": "PP14109324", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/604131.604138", "year": "2003", "article_id": "604138", "conference": "POPL", "title": "Discovering affine equalities using random interpretation", "url": "http://dl.acm.org/citation.cfm?id=604138"}