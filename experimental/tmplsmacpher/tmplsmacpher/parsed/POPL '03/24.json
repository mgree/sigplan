{"article_publication_date": "01-15-2003", "fulltext": "\n A Real-time Garbage Collector with Low Overhead and Consistent Utilization David F. Bacon Perry Cheng \nV.T. Rajan dfb@watson.ibm.com perryche@us.ibm.com vtrajan@us.ibm.com IBM T.J. Watson Research Center \nP.O. Box 704 Yorktown Heights, NY 10598 ABSTRACT Now that the use of garbage collection in languages \nlike Java is be\u00adcoming widely accepted due to the safety and software engineering bene.ts it provides, \nthere is signi.cant interest in applying garbage collection to hard real-time systems. Past approaches \nhave gener\u00adally suffered from one of two major .aws: either they were not provably real-time, or they \nimposed large space overheads to meet the real-time bounds. We present a mostly non-moving, dynami\u00adcally \ndefragmenting collector that overcomes both of these limita\u00adtions: by avoiding copying in most cases, \nspace requirements are kept low; and by fully incrementalizing the collector we are able to meet real-time \nbounds. We implemented our algorithm in the Jikes RVM and show that at real-time resolution we are able \nto obtain mutator utilization rates of 45% with only 1.6 2.5 times the ac\u00adtual space required by the \napplication, a factor of 4 improvement in utilization over the best previously published results. Defragmen\u00adtation \ncauses no more than 4% of the traced data to be copied. General Terms Algorithms, Languages, Measurement, \nPerformance Categories and Subject Descriptors C.3 [Special-Purpose and Application-Based Systems]: Real\u00adtime \nand embedded systems; D.3.2 [Programming Languages]: Java; D.3.4 [Programming Languages]: Processors \nMemory management (garbage collection) Keywords Read barrier, defragmentation, real-time scheduling, \nutilization 1. INTRODUCTION Garbage collected languages like Java are making signi.cant in\u00adroads into \ndomains with hard real-time concerns, such as automo\u00adtive command-and-control systems. However, the engineering \nand product life-cycle advantages consequent from the simplicity of Permission to make digital or hard \ncopies of all or part of this work for personal or classroom use is granted without fee provided that \ncopies are not made or distributed for pro.t or commercial advantage and that copies bear this notice \nand the full citation on the .rst page. To copy otherwise, to republish, to post on servers or to redistribute \nto lists, requires prior speci.c permission and/or a fee. POPL 03, January 15 17, 2003, New Orleans, \nLouisiana, USA. Copyright c &#38;#169;2003 ACM 1-58113-628-5/03/0001 $5.00. programming with garbage \ncollection remain unavailable for use in the core functionality of such systems, where hard real-time \ncon\u00adstraints must be met. As a result, real-time programming requires the use of multiple languages, \nor at least (in the case of the Real-Time Speci.cation for Java [9]) two programming models within the \nsame language. Therefore, there is a pressing practical need for a system that can provide real-time \nguarantees for Java without imposing major penalties in space or time. We present a design for a real-time \ngarbage collector for Java, an analysis of its real-time properties, and implementation results that \nshow that we are able to run applications with high mutator utilization and low variance in pause times. \nThe target is uniprocessor embedded systems. The collector is therefore concurrent, but not parallel. \nThis choice both complicates and simpli.es the design: the design is complicated by the fact that the \ncollector must be interleaved with the mutators, instead of being able to run on a separate processor; \nthe design is simpli.ed since the programming model is sequentially consistent. Previous incremental \ncollectors either attempt to avoid overhead and complexity by using a non-copying approach (and are there\u00adfore \nsubject to potentially unbounded fragmentation), or attempt to prevent fragmentation by performing concurrent \ncopying (and therefore require a minimum of a factor of two overhead in space, as well as requiring barriers \non reads and/or writes, which are costly and tend to make response time unpredictable). Our collector \nis unique in that it occupies an under-explored por\u00adtion of the design space for real-time incremental \ncollectors: it is a mostly non-copying hybrid. As long as space is available, it acts like a non-copying \ncollector, with the consequent advantages. When space becomes scarce, it performs defragmentation with \nlim\u00adited copying of objects. We show experimentally that such a design is able to achieve low space and \ntime overhead, and high and con\u00adsistent mutator CPU utilization. In order to achieve high performance \nwith a copying collector, we have developed optimization techniques for the Brooks-style read barrier \n[10] using an eager invariant that keeps read barrier overhead to 4%, an order of magnitude faster than \nprevious soft\u00adware read barriers. Our collector can use either time-or work-based scheduling. Most previous \nwork on real-time garbage collection, starting with Baker s algorithm [5], has used work-based scheduling. \nWe show both analytically and experimentally that time-based scheduling is superior, particularly at \nthe short intervals that are typically of in\u00adterest in real-time systems. Work-based algorithms may achieve \nshort individual pause times, but are unable to achieve consistent utilization. The paper is organized \nas follows: Section 2 describes previ\u00adous approaches to real-time collection and some of the common problems \nencountered. Section 3 presents an informal overview of our collector. Section 4 analyzes the conditions \nunder which real\u00adtime bounds can be met. Section 5 analyzes the space requirements of our collector and \ncompares them to other real-time collectors. Section 6 describes the implementation of the collector, \nand Sec\u00adtion 7 presents our experimental results. Section 8 discusses issues in real-time garbage collection \nthat are raised by our work. Finally, we present our conclusions. 2. PROBLEMS WITH PREVIOUS WORK Previous \napproaches to real-time garbage collection have gener\u00adally suffered from a variety of problems. In this \nsection we will describe these problems. 2.1 Fragmentation Early work, particularly for Lisp, often assumed \nthat all memory consisted of CONS cells and that fragmentation was therefore a non\u00adissue. Baker s Treadmill \n[6] also only handles a single object size. Johnstone [17] showed that fragmentation was often not a \nmajor problem for a family of C and C++ benchmarks, and built a non\u00admoving real-time collector based \non the assumption that frag\u00admentation could be ignored. However, these measurements are based on relatively \nshort-running programs, and we believe they do not apply to long-running systems like continuous-loop \nembedded devices, PDAs, or web servers. Fundamentally, this is an average\u00adcase rather than a worst-case \nassumption, and meeting real-time bounds requires handling worst-case scenarios. Furthermore, the use \nof dynamically allocated strings in Java combined with the heavy use of strings in web-related processing \nis likely to make object sizes less predictable. Dimpsey et al. [14] describe the compaction avoidance \ntech\u00adniques in the IBM product JVM, which are based on Johnstone s work. They show that these techniques \ncan work quite well in prac\u00adtice. However, when compaction does occur it is very expensive. Siebert [23] \nsuggests that a single block size can be used for Java by allocating large objects as linked lists and \nlarge arrays as trees. However, this approach has simply traded external fragmen\u00adtation for internal \nfragmentation. Siebert suggests a block size of 64 bytes; if there are a large number of 8-byte objects, \ninternal frag\u00admentation can cause a factor of 8 increase in memory requirements. 2.2 High Space Overhead \nTo avoid the problems resulting from fragmentation, many re\u00adsearchers have used copying algorithms [5, \n10] as the basis for real-time collection. Such collectors typically have a high space overhead. First \nof all, when a full collection is performed a com\u00adplete semi-space is required for the target data, so \nthe minimum space overhead is a factor of 2. Secondly, space is required so that the mutator can continue \nto run (and allocate) while the collector operates. In order to achieve good mutator utilization while \nthe collector is running, a space overhead of a factor of 3-5 is typical [12]. For Johnstone s non-copying \ncollector [17], space overhead is often a factor of 6 8. 2.3 Uneven Mutator Utilization Much of the \nliterature has focused on maximum pause times in\u00adduced by collection, but in fact an equally important \nmetric is mu\u00adtator utilization (the fraction of the processor devoted to mutator execution). If there \nis a period of low utilization, the mutator may be unable to meet its real-time requirements even though \nall indi\u00advidual pause times are short. Uneven utilization is endemic to collectors that use a to-space \nin\u00advariant (that is, the mutator only sees objects in to-space). Such col\u00adlectors are implemented with \na read-barrier that checks if an object being accessed is in from-space, and if so, copies it into to-space \nbefore returning the pointer to the mutator. There is therefore a tight coupling between the operations \nof the mutator and the scheduling of operations by the collector. Examples are Baker s copying algorithm \n[5] which uses an ex\u00adplicit read-barrier, and the Appel-Ellis-Li collector [2], which uses virtual memory \nprotection. Both of these collectors have the prop\u00aderty that mutator utilization is very poor right after \nthe collector starts, when the fault rate is high. An alternative is to use a replicating collector which \nmaintains a from-space invariant, and to perform mutator updates on both from-space and to-space, as \nin the ML collectors of Nettles and O Toole [21] and Cheng and Blelloch [12]. However, this requires \na fairly costly replication for all updates, rather than a simple write barrier on pointer updates. As \na result, the strategy is better suited to mostly functional languages like ML, and less well-suited \nto im\u00adperative languages like Java. 2.4 Inability to Handle Large Data Structures Some algorithms attempt \nto avoid the factor of 2 space overhead in copying collectors by doing the work incrementally collecting \nonly a portion of the heap at a time. The most notable example is the Train algorithm [16]. Recently, \nBen-Yitzhak et al. [7] have im\u00adplemented a parallel incremental collector that operates on a .xed fraction \nof the heap at a time to minimize pause times for large heaps. The fundamental problem with all algorithms \nthat attempt to col\u00adlect a subset of the heap at a time is that they can be defeated by adversarial mutators. \nLarge cyclic structures, objects with high in\u00addegree, and high mutation rates are ways to force such \ncollectors to perform work without .xed bound.  3. OVERVIEW Our collector is an incremental uni-processor \ncollector targeted at embedded systems. It overcomes the problems of the previous section by using a \nhybrid approach of non-copying mark-sweep (in the common case) and copying collection (when fragmentation \noccurs). The collector is a snapshot-at-the-beginning algorithm that allo\u00adcates objects black (marked). \nWhile it has been argued that such a collector can increase .oating garbage, the worst-case performance \nis no different from other approaches and the termination condition is easier to enforce. Other real-time \ncollectors have used a similar approach. 3.1 Overview of Our Collector Our collector is based on the \nfollowing principles: Segregated Free Lists. Allocation is performed using segregated free lists. Memory \nis divided into .xed-sized pages, and each page is divided into blocks of a particular size. Objects \nare allocated from the smallest size class that can contain the object. Mostly Non-copying. Since fragmentation \nis rare, objects are usu\u00adally not moved. Defragmentation. If a page becomes fragmented due to garbage \ncollection, its objects are moved to another (mostly full) page. Read Barrier. Relocation of objects \nis achieved by using a for\u00adwarding pointer located in the header of each object [10]. A read barrier \nmaintains a to-space invariant (mutators always see objects in the to-space). Incremental Mark-Sweep. \nCollection is a standard incremental mark-sweep similar to Yuasa s snapshot-at-the-beginning al\u00adgorithm \n[24] implemented with a weak tricolor invariant. We extend traversal during marking so that it redirects \nany point\u00aders pointing at from-space so they point at to-space. There\u00adfore, at the end of a marking phase, \nthe relocated objects of the previous collection can be freed. Arraylets. Large arrays are broken into \n.xed-size pieces (which we call arraylets) to bound the work of scanning or copying an array and to avoid \nexternal fragmentation caused by large objects. Since our collector is not concurrent, we explicitly \ncontrol the interleaving of the mutator and the collector. We use the term col\u00adlection to refer to a \ncomplete mark/sweep/defragment cycle and the term collector quantum to refer to a scheduler quantum in \nwhich the collector runs. 3.2 Object Allocation and Fragmentation Allocation is performed using a simple \nsegregated free-list ap\u00adproach. When a free list is empty, a new page is chosen, broken into equal-size \nblocks, and the resulting blocks are placed onto that free list. Note that the allocator page size Iis \nnot necessarily the 14 same as the operating system page size. We use I==l KB. Internal fragmentation \nis regulated by using a geometric progres\u00adsion of free list sizes, such that if there is a free list \nwhose blocks are of size s, the next larger size is s(l+p). We generally choose p=l/8, resulting in worst-case \nfragmentation of 12.5%. How\u00adever, the measured internal fragmentation in our collector has never exceeded \n2% at p=l/8. Most programs obey a locality of size property, that is, the ob\u00adject sizes allocated frequently \nin the past will tend to have a high correlation with object sizes allocated in the future. Therefore, \nwe expect that in the normal case, the garbage collector will .nd un\u00adused blocks in a particular size \nclass that can simply be re-used. Only in relatively rare cases will object allocation cause external \nfragmentation. Because our collector performs defragmentation, we can choose a pthat results in low internal \nfragmentation, but allows a relatively large number of size classes. Most collectors based on segregated \nlists must be concerned about external fragmentation, and therefore keep the number of size classes small \nby choosing p=l, leading to power-of-two size classes with high internal fragmentation. The only overhead \nto decreasing pis that we may need to have one under-utilized page per size class. Assuming a 4-byte \nword size, the number of size classes Cis bounded by InIp/4 C< In(l+p) The free lists are actually kept \nas chains of pages, rather than chains of blocks. Each page has an associated mark array. The allocation \ncursor is actually a pair pointing to a page and a block within the page. This organization allows formatting \nof pages to be performed lazily and therefore avoids a full sweep of the memory on each collection. \n3.3 Defragmentation At the end of the sweep phase, we determine whether there is a suf.cient number of \nfree pages to allow the mutator to continue to execute for another collection cycle without running out \nof mem\u00adory, assuming a worst-case selection of object sizes by the mutator (that is, we assume that the \nmutator will act adversarially to maxi\u00admize external fragmentation). If the number of free pages drops \nbelow this threshold, we per\u00adform a defragmentation that will free at least that many pages. De\u00adfragmentation \nis performed as follows: for each page, we compute the number of live objects. Then the pages within \neach size class are sorted by occupancy. Finally, we move objects from the least occupied to the most \noccupied pages within a list (note that this never causes new pages to be allocated; it only transfers \nobjects between pages within a size class). 3.4 Read Barrier We use a Brooks-style read barrier [10] \nto maintain a to-space invariant in the mutator: each object contains a forwarding pointer that normally \npoints to itself, but when the object has been moved, points to the moved object. Our collector thus \nmaintains a to-space invariant, but the sets comprising from-space and to-space have a large intersection, \nrather than being completely disjoint as in a pure copying collector. Note that while we use a read barrier \nand a to-space invariant, our collector does not suffer from variations in mutator utilization because \nall of the work of .nding and moving objects is performed by the collector. Read barriers, especially \nwhen implemented in software, are fre\u00adquently avoided because they are considered to be too costly. We \nwill show that this is not the case when they are implemented care\u00adfully in an optimizing compiler and \nthe compiler is able to optimize the barriers. A fundamental design choice for the read barrier is whether \nit is lazy or eager . A lazy barrier has the property that registers and stack cells can point to either \nfrom-space or to-space objects, and the forwarding operation is performed at the time of use. An eager \nbarrier, on the other hand, maintains the invariant that registers and stack cells always point into \nto-space: the forwarding operation is performed eagerly as soon as the quantity is loaded. Eager barriers \nhave a major performance advantage in that if a quantity is loaded and then dereferenced many times (for \ninstance, a reference to an array of integers loaded and then used in a loop), the eager barrier will \nonly perform the forwarding operation once, while the lazy barrier will perform the forwarding operation \nfor ev\u00adery array access. Of course, there is a cost: because the eager invariant is more strict, it is \nmore complex to maintain. Whenever the collector moves objects, it must .nd all outstanding register \nand stack cells and re-execute the forwarding operation on them. We apply a number of optimizations to \nreduce the cost of read barriers, including well-known optimizations like common subex\u00adpression elimination, \nas well as special-purpose optimizations like barrier-sinking, in which we sink the barrier down to its \npoint of use, which allows the null-check required by the Java object deref\u00aderence to be folded into \nthe null-check required by the barrier (since the pointer can be null, the barrier can not perform the \nforwarding unconditionally). This optimization works with whatever null-checking approach is used by \nthe run-time system, whether via explicit comparisons or implicit traps on null dereferences. The important \npoint is that we avoid introducing extra explicit checks for null, and we guarantee that any exception \ndue to a null pointer occurs at the same place as it would have in the original program. The result of \nour optimizations is a mean cost of only 4% for the read barriers, as is shown in Section 7. 3.5 Arraylets \nLarge objects pose special problems for garbage collectors. In copying collectors, if they are repeatedly \ncopied, the performance penalty can be very high. In non-copying collectors, external frag\u00admentation \ncan make it impossible to allocate a large object. For instance, a single small object in the middle \nof the heap can make it impossible to satisfy a request for an object slightly larger than half the heap. \nFurthermore, in incremental and real-time collectors, large ob\u00adjects pose an additional problem because \nthey can not be moved in a reasonably bounded amount of time. Siebert [23] has suggested using .xed-size \nblocks of 32 or 64 bytes for all object allocations, and creating large arrays by using a tree structure. \nUnfortunately, this requires rewriting every array access as a loop, and can have a severe performance \npenalty for array-intensive programs since common loop optimizations are de\u00adfeated. Our mostly non-copying \ncollector allows a different approach: we represent small arrays contiguously, and large arrays as two\u00adlevel \nstructures consisting of a sequence of arraylets. Each arraylet (except the last) is of a .xed size, \nwhich is chosen to be a power of two so that the division operation required for indexing can be implemented \nwith a shift. In our case, arraylets are E=Ip= KB. We therefore have the advantage of never needing to \nallocate large objects contiguously, and are therefore not subject to external fragmentation. On the \nother hand, access to array elements is still ef.cient, and when combined with strip-mining optimizations \nis usually as ef.cient as contiguous layout. The arraylet size must be chosen carefully and there are \nsome tradeoffs involved. With a suf.ciently large size, one can assume that all objects will be contiguous \nand smaller than the arraylet size, simplifying the implementation. The maximum array size that can be \nrepresented with a single root of size Eis E2/4,or1MBin our case. However, note that if necessary we \ncan simply allocate an entire block to be the root of the array, because the wasted space at the end \nof the block will be negligible compared to the total size of the array. Thus we can accommodate arrays \nof size up to IE/4 or 8 MB. For larger objects we can scan the free block list for the necessary number \nof contiguous free blocks. If the system must be able to return objects larger than 8 MB in real time, \nthe maximum size can be tuned by varying Iand p. All arrays are represented in a uniform manner: arraylet \npointers are laid out in reverse order to the left of the array header. If the array is contiguous, there \nis only one arraylet pointer and it points to the data .eld to the right of the header. Arraylets are \nimplemented in the system presented in this paper, but not yet highly optimized. However, we can use \nArnold s thin guards [3] to eliminate the indirection for array types that do not exist as arraylets, \nso that most array accesses will operate at full speed. For arraylets, we can strip-mine regular iterations \nto the ar\u00adraylet size. Thus arraylets should only suffer performance penalties when they are used and \nwhen the access pattern is irregular. 3.6 Open Issues The main issue we have not addressed in our collector \nis making stack processing incremental. This is an issue in two parts of the system: root scanning and \nmaintenance of the eager invariant for the read barrier. Stacklets [13] break stacks into .xed-size chunks \nto quantize the associated work. However, they only provide a partial solution: if we only copy the top \nstacklet of the running thread and return to the mutator, the mutator can then begin either pushing or \npopping at a very high rate. A high rate of popping is problematic because the collector must halt the \nmutator while it copies each popped stacklet, and if many stacklets are popped in a short interval the \nmutator utilization will temporarily become very low. It can also force the memory con\u00adsumption of the \nstack to double (due to the snapshots). A high rate of pushing is problematic because the collector may \nhave trouble keeping up with the mutator. In this case, the solution is to model stack pushes that enter \nnew stacklets to be modelled as allocation, and to use the associated methods for measuring and controlling \nallocation rates. For the benchmarks available to us the stacks remained small, and the limiting factor \nin pause time was the resolution of the oper\u00adating system clock. Therefore the implementation presented \nin this paper does not include stacklets. We intend to address the issue of incrementalizing stack operations \nin future work, in particular by exploring alternative write barriers and termination conditions.  4. \nREAL-TIME SCHEDULING In this section we derive the equations for CPU utilization and memory usage for \nour collector using two different scheduling poli\u00adcies: one based on time, the other based on work. We \ncan de.ne the real-time behavior of the combined system comprising the user program and our garbage collector \nwith the following parameters: A*(7)is the instantaneous memory allocation rate at time 7 (MB/s).  \nG*(7)is the instantaneous garbage generation rate at time 7 (MB/s).  Pis the garbage collector processing \nrate (MB/s). Since ours is a tracing collector, this is measured over live data.  A time 7is on an idealized \naxis in which the collector runs in\u00ad.nitely fast we call this mutator time. As a practical matter this \ncan be thought of as time measured when the program has suf.cient memory to run without garbage collecting. \nBy convention, upper-case letters refer to primitive quantities; lower-case quantities are derived. The \nonly other primitive param\u00adeters required are the relative rates of mutator and collector. From these \nbasic parameters we can de.ne a number of impor\u00adtant characteristics of the application relevant to real-time \ngarbage collection. The amount of memory allocated and garbage generated during the interval (7172)are \n . ** (7172)=A(7) 7(1) . . *G* (7172)=(7) 7 (2) . The maximum memory allocation for an interval of \nsize 7is * *( 7)= x (77+7) (3) and the maximum memory allocation rate is ** ( 7)= ( 7)/ 7 (4) The instantaneous \nmemory requirement of the program (exclud\u00ading garbage, overhead, and fragmentation) at time 7is *** (7)= \n( 7) ( 7) (5) 4.1 Mapping Between Mutator and Real Time Now consider a realistic execution in which the \ncollector is not in.nitely fast. Execution will consist of alternate executions of mu\u00adtator and collector. \nTime along real time axis will be denoted with the variable t. The function <(t)-7maps from real to mutator \ntime, where * 7<t. Functions that operate in mutator time are written r(7) while functions that operate \nin real time are written r(t). The live memory of the program at time tis thus (t)= *(<(t)) (6) and the \nmaximum memory requirement over the entire program execution is * =x(t)=x(7)(7) t 4.2 Time-Based Scheduling \nTime-based scheduling interleaves the collector and mutator us\u00ading .xed time quanta. It thus results \nin even CPU utilization but is subject to variations in memory requirements if the memory al\u00adlocation \nrate is uneven. A time-based real-time collector has two additional fundamental parameters: QTis the \nmutator quantum: the amount of time (in seconds) that the mutator is allowed to run before the collector \nis al\u00adlowed to operate.  CTis the time-based collector quantum (in seconds of col\u00adlection time).  For \nthe time being, we assume that the scheduler is perfect, in the sense that it always schedules the mutator \nfor precisely QTsec\u00adonds. A typical value for QTmight be 10 ms. In Section 7 we will show how close we \nare able to get to this ideal in practice. Cheng and Blelloch [12] have de.ned the minimum mutator uti\u00adlization \nor MMU for a given time interval tas the minimum CPU utilization by the mutator over all intervals of \nwidth t.From the parameters QTand CTwe can derive the MMU as lJ /t QT.+x Q. C. UT(t)= (8) t where the \n.rst term in the numerator corresponds to the number of whole mutator quanta in the interval, and the \nxterm corresponds to the size of the remaining partial mutator quantum, which is de.ned as (lJ) t x=xt(QT+CT).CT(9) \nQT+CT While this expression is fairly awkward, as the number of intervals becomes large, it reduces to \nthe straightforward utilization expres\u00adsion QT IiUT(t)=(10) /t-o QT+CT A plot of the MMU for a perfectly \nscheduled system using 10 mil\u00adlisecond mutator and collector quanta is shown in Figure 1. It is important \nto note that at the small time scales of interest in real\u00adtime systems, the xterm is very signi.cant: \nat t=ms the MMU is l/(the maximum value), while at t=3ms, it drops to l/3. Also, the higher the scheduling \nfrequency of the collector, the more quickly it converges to the theoretical limit. In practice, at large \ntime intervals UT(t)is only a lower bound on the utilization, since in most cases the collector only \nruns inter\u00admittently. Utilization 1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 Delta t (s) Figure 1: MMU \nfor a perfectly scheduled time-based collector. QT=CT=.l(10 ms). Now consider the space utilization of \na time-scheduled collector. Since we are assuming the collection rate is constant, at time tthe collector \nwill run for (t)/Pseconds to process the (t)live data (since our collector is trace-based, work is essentially \nproportional to live data and not garbage). In that time, the mutator will for QT seconds per CTseconds \nexecuted by the collector. Therefore, in order to run a collection at time t, we require excess space \nof *((t)QT) eT(t)=<(t)<(t)+.(11) PCT We further de.ne the maximum excess space required as eT=xeT(t) \n(12) t Freeing an object in our collector may take as many as three collections: the .rst is to collect \nthe object; the second is because the object may have become garbage immediately after a collection began, \nand will therefore not be discovered until the following col\u00adlection cycle; and the third is because \nwe may need to relocate the object in order to make use of its space. The .rst two properties are universal; \nthe third is speci.c to our approach. As a result, the space requirement of our collector paired with \na given application (including unreclaimed garbage, but not includ\u00ading internal fragmentation) at time \ntis sT(t)<(t)+3eT (13) and the overall space requirement is sT<+3eT (14) However, note that the expected \nspace utilization is only +eT, and the worst-case utilization is highly unlikely; this is discussed in \nmore detail below. 4.3 Work-Based Scheduling Work-based scheduling interleaves the collector with the \nmutator based on .xed amounts of allocation and collection. A work-based real-time collector is parameterized \nby Qwis the work-based mutator quantum: the number of MB the mutator is allowed to allocate before the \ncollector is al\u00adlowedtorun.  Cwis the work-based collector quantum: the number of MB the collector must \nprocess each time the mutator yields to it.  Then the excess space required to perform a collection \nat time tis Qw ew(t)=(t). (15) Cw and the excess space required for a collection over the entire exe\u00ad \ncution is ew = . Qw Cw (16) Note therefore that it must be the case that Qw C wor else the space may \ngrow without bound. Consequently, the space requirement of the program at time tis sw(t)<(t)+3ew (17) \nand the space requirement for the entire program execution is sw =+3ew (18) 4.3.1 Work-based CPU Utilization \nComputing mutator CPU utilization when collector scheduling is work-based is inherently problematic, \nbecause the operations of the mutator may affect the amount of time allocated to the mutator. In other \nwords, there is a time dilation from tto 7that is linear and .xed in time-based scheduling but variable, \nnon-linear, and application-dependent in work-based scheduling. Due to these problems it is not possible \nto obtain a closed-form solution for the utilization. We begin by noting that each muta\u00adtor pause involves \nthe collector processing Cwmemory at rate P. Hence each mutator pause will be =Cw/P. In our simpli.ed \nmodel, this will be a constant. Each mutator quantum will involve allocation of Qwmemory, so the minimum \ntotal mutator time 7i for iquanta will be given by the minimum 7ithat solves the equa\u00adtion * (7i)=iQw \n(19) As the time interval increases the maximum amount of allocation in that time does not decrease, \nso * (7)is a monotonically in\u00adcreasing function and hence 7i>7i-1. Therefore, the solution to (19) can \nbe found with an iterative method. This is analogous to the iterative solution to rate monotonic scheduling \nin real-time sys\u00adtems [18]. Let kbe the largest integer such that k+7k<t (20) so the minimum mutator \nutilization over an interval of size tis 7k+y Uw(t)= (21) t where the .rst term in the numerator is the \ntime taken by kwhole mutator quanta in the interval and the yterm corresponds to the size of the remaining \npartial mutator quantum (if any), which is de.ned as y=x(t7k(k+l).)(22) Note that in a work-based collector, \nutilization will be zero for t. In fact, any large allocation of nQwbytes will lead to zero utilization \nfor time n. This simply expresses analytically the fact that in a work-based collector, there is a much \nlarger burden on the programmer to achieve real-time bounds by making sure that memory allocation is \nsuf.ciently discretized and evenly spaced.  4.4 Mutation In addition to allocation, the other form of \nwork by the mutator that can interact with the operation of the collector is the actual heap mutation. \nMutation can be thought of as an alternate way for roots to be added, along with stack scanning. We impose \nthe following division of labor between the muta\u00adtor and the collector: the mutator s write barrier is \nresponsible for making sure that only non-null, unmarked objects are placed into the write buffer. This \nensures that the work performed by the col\u00adlector attributable to mutation is O(N),where Nis the number \nof objects, while keeping the overhead of the write barrier constant. The collector periodically processes \nthe write buffer and treats the entries like any other potential roots: it marks the objects gray and \nplaces them into the work queue for scanning. Note that in the worst case, the work queue can reach size \nN. Now we must account for mutation in the formulas for collec\u00adtor performance that we have derived, \nbecause mutation consumes memory just like allocation by the mutator. To do this, we simply rede.ne A \n* (7)to comprise both directly allocated memory and indirectly allocated memory due to mutation, where \neach mutation consumes memory of the size of one object pointer. If desired, the formulas could all be \nbroken up to account for each kind of space consumption individually. 4.5 Sensitivity to Parameters \nThe degree to which each collector is able to meet its predicted behavior will depend quite strongly \non the accuracy of the param\u00adeters which are used to describe the application and the collector strategy. \nThese are the application parameters A * (t)and G * (t), and the collector parameters, Pand either QTand \nCTor Qwand Cwfor the time-based or work-based collectors, respectively. In practice, the user describes \nthe application in terms of its max\u00adimum memory consumption and its maximum allocation rate * (7). 4.5.1 \nSensitivity of the Time-based Collector The CPU utilization rate UTof the time-based collector is strictly \ndependent on the quantization parameters QTand CT, so the uti\u00adlization will be very steady (depending \nonly on implementation\u00adinduced jitter, and subject to the minimum quantization that the implementation \ncan support). On the other hand, the space required to perform a collection eT(t)which determines the \ntotal space sTrequired to run the ap\u00adplication is dependent on both the maximum memory usage by the application \nand the amount of memory allocated over an interval. Thus if the user under-estimates either or *, then \nthe total space requirement sTmay grow arbitrarily. In particular, time-based col\u00adlectors are subject \nto such behavior when there are intervals of time in which the allocation rate is very high. Furthermore, \nthe estimate of the collector processing rate Pmust also be a lower bound on the actual rate. However, \nnote that the space consumed by the application is over a relatively long interval of time, namely the \namount of time the application runs while a single collection takes place or (t)QT 7=. PCT and therefore \nthe allocation rate in that time will typically be close to the average allocation rate of the program \nand the variation will tend to be low. Therefore, to a .rst order, a time-scheduled collector will meet \nboth its time and space bounds as long as the user estimate of is correct. 4.5.2 Sensitivity of the \nWork-based Collector In the work-based collector, the space overhead for collection ew(t)is straightforward \nto compute, and it will be accurate as long as the user estimate of the total live memory is accurate. \nOn the other hand, the CPU utilization rate for a given interval * tdepends on the allocation rate (7)where \n7<tas well as on the collector processing rate P. The interval tis the interval over which we require \nreal-time performance, for instance s. Since this interval is small, the peak allocation rate for this \ninterval size is likely to be quite high, as we will show in Section 7. Thus we expect that the CPU uti\u00adlization \nof the work-based collector will vary considerably with the allocation rate. In particular, note that \nthe 7in which the time-based collector is dependent on allocation rate is on a much larger scale, namely \nthe amount of time for a garbage collection. Therefore, to a .rst order a work-scheduled collector will \nmeet its space bound as long as the user estimate of is correct, but its CPU utilization will be heavily \ndependent on the allocation rate over a real-time interval. 4.5.3 A Robust Collector A robust real-time \ncollector should primarily use a time-based scheduling policy, but as memory resources become scarce \n(indi\u00adcating that the input parameters to the collector may have been incorrect), if graceful degradation \nis desirable then the collector should begin slowing down the allocation rate. This can be done in a \nnumber of ways. A classical approach in real-time systems is to separate threads into priority classes, \nand as the system becomes unable to meet real-time bounds, low-priority threads are successively suspended \n[15]. Another approach is to begin using a hybrid strategy which be\u00adcomes progressively more work-based \nas the collector comes closer to its memory limit. This approach will not guarantee that real-time bounds \nare met, but is robust even if the allocation rate and memory utilization of the top-priority threads \nhave been underestimated. We have not done this; instead we have implemented pure time\u00adbased and work-based \ncollector scheduling policies, and in Sec\u00adtion 7 we compare them experimentally so that the tradeoffs \ncan be evaluated.  5. SPACE COSTS We now compare the relative space costs of the different types of \nreal-time collectors. Since purely non-copying algorithms are subject to high (and often unbounded) fragmentation, \nthey are not suitable for use in true real-time systems. Since our collector has a signi.cantly different \narchitecture from copying real-time collectors, its space bounds are quite different. Incremental semi-space \ncopying collectors have an inherent space overhead of .(+e)+r+.,where is the maximum live heap memory, \neis the space required to allow allocation to proceed during a single garbage collection, ris the maximum \nstack depth, and .is the maximum size of the global variable area. Our collector has an expected-case \nspace requirement of +e+ r+.and a worst-case cost of +3e+r+.+N,where Nis the maximum number of uncollected \nobjects (live or dead). The extra e+Nspace is incurred when: a data structure of size close to is freed \nimmediately after the beginning of a collection (the collector must run again to .nd it, requiring eextra \nspace); all garbage found causes external fragmentation (requiring an extra collection cycle to relocate \nthe data and make it available, which requires another eextra space); and the program traverses the heap \nin a pessimal fashion (forcing a maximum number of pointers to be pushed onto the work queue for each \nmark operation, which requires Nextra words of memory). There are two things to note about the worst-case \nmemory re\u00adquirements of our collector. First, the difference in the worst-case between our collector \nand a copying collector is e+Nversus . The space erequired to run a collection is typically lower than \nthe maximum live memory (and can be tuned). The maximum num\u00adber of uncollected objects is the maximum \nuncollected space di\u00advided by the average object size in words A,or (+e)/A.Since Ais typically on the \norder of 8 for Java programs, Nis typically small relative to . Thus for most programs, the worst-case \nspace requirements of our collector will still be smaller than those of a copying semi-space collector. \nSecond, the likelihood of more than one of these worst-case sce\u00adnarios occurring concurrently is very \nlow. In practice, this means that the amount of memory devoted to the system can be varied be\u00adtween the \nexpected-and worst-case space requirements depending on the acceptable failure rates for the system in \nquestion. These .gures do not include the extra space overhead required to bound internal fragmentation \nwith the parameter p, which we have set to l/8in our implementation. This parameter can be further reduced \nat the expense of potentially requiring additional partially used blocks for the extra size classes. \nFor p=l/8, the number of size classes C=4.and the measured fragmentation does not exceed 2% for our benchmarks. \nWe do not include the space overhead due to the forwarding pointer, since all high-performance copying \nalgorithms also use a forwarding pointer. Bacon et al. [4] have shown that an extra header word leads \nto a 14% increase in space utilization (assuming one uses an object model with a single-word header as \na basis). 6. IMPLEMENTATION ISSUES We implemented a real-time collector based on the ideas intro\u00adduced \nin the previous sections. Implementing the collector required both coding the collector proper as well \nas adding read barriers to the compiler. In certain cases, it was infeasible to introduce a read barrier. \nOmitting the barrier is correct as long as we pin the ob\u00adject to guarantee that it never moves. Fortunately, \nmost objects that fall into this category are run-time data structures that are immor\u00adtal. By maintaining \na separate immortal heap, we can omit moving such objects without introducing any fragmentation. 6.1 \nTriggering a Collection In the worst-case analysis of the collector, we can run the pro\u00adgram in space \n+3ewhere is the amount of maximum live data and eis the space required to run a single collection (e=eT \nor ewdepending on the scheduling policy). However, executing with these boundary conditions will result \nin the collector always running. Even if the application utilization is at 50% during a col\u00adlection, \nthis will lead to an overall slowdown of the program by a factor of 2 which is likely unacceptable. For \ncomparison, running a stop-the-world collector at will result in a virtually in.nite slow\u00addown. The solution \nis to provide headroom so that the program can run for some time before a collection must occur. For \nexample, if enough headroom is provided so that the collector runs only 25% of the time, then the overall \nutilization rises to 87.5%. In our implementation, we have set the headroom to be e.A collection is thus \ntriggered when the amount of memory in use is +e. 6.2 Control of Interleaving Ideally, in the time-scheduled \ncollector we would use a precise timer to control the scheduling of the mutator and collector pro\u00adcesses. \nUnfortunately, AIX does not allow user-level access to timers with a resolution of less than 10 ms. Therefore, \nwe must Barrier Overhead 12% 10% 8% 6% 4% 2% 0% Figure 2: Relative overhead of lazy and eager read barriers \nin the Jikes RVM optimizing compiler. use an approximate method based on polling. The mutator polls the \ntimer on the slow path of allocation (when it moves to a new page) or when the mutation buffer .lls up. \nThis keeps the polling out of the fast, in-lined cases, but is subject to some inaccuracy. However, as \na practical matter, this is acceptable because we are increasing mutator utilization and doing it at \na time when resource consumption is low. The collector on the other hand performs work in progressively \n.ner work quanta as it gets closer to the end of its time quantum CT. When the time consumed is close \nto or exceeds the quantum, the mutator is resumed. The work-scheduled collector is also subject to some \ninaccuracy because scheduling is only performed in the slow path through the allocator, even though a \nprecise count of bytes allocated is kept on the fast (inlined) path.  7. MEASUREMENTS We present empirical \nresults in this section. All results were obtained on an IBM RS/6000 Enterprise Server F80 running AIX \n5.1. The machine has 4 GB of main memory and six 500 MHz PowerPC RS64 III processors each with 4 MB of \nL2 cache. The virtual machine runs on a single CPU. Experiments are run on an unloaded multiprocessor \nso that operating system processes are performed on different CPUs to avoid perturbing our measure\u00adments. \nOur system is implemented as part of the Jikes Research Virtual Machine (RVM) version 2.1.1 at the IBM \nT.J. Watson Research Center [1]. All methods were compiled with the optimizing com\u00adpiler (since the system \nis real-time, adaptive compilation is turned off). Measurements were started after a dummy run of the \nbench\u00admark which forces all methods to be compiled. Because the optimizing compiler often requires more \nspace than the applications themselves, the heap is resized after compilation to the heap sizes given. \nIn this way, we measure the intrinsic prop\u00aderties of the application rather than of the compilation. \n7.1 Read Barrier Costs Since our collector makes use of read barriers, and read barriers are often considered \nprohibitively expensive, we begin by showing that our optimized implementation of the Brooks-style read \nbarrier with the eager invariant can achieve very low overhead. We have implemented both lazy and eager \nbarriers in the IBM Jikes RVM [1] and present their relative performance, both to each other and to a \nsystem without barriers. Read barriers were initially considered so expensive as to only be practical \nwith hardware support, as was done in a number of com\u00admercially available machines such as the Symbolics \nLisp Machine [20]. The .rst implementation we know of the Brooks read barrier is that by North and Reppy \n[22] in their concurrent collector for Pegasus ML. However, they do not measure barrier cost but only \nthe total cost. Zorn [25] compared the cost of hardware, software, and page protection-based read barriers, \nand determined that software read barriers were much better than protection based read barriers, but \nstill cost about 20%. Zorn measured Baker-style read barriers that require on average four ALU/branch \ninstructions. The straightforward implementa\u00adtion of our read barrier requires a compare, a branch, and \na load. However, in most cases we are able to optimize away the compare and branch, and to perform common \nsubexpression elimination on the remaining loads. The results are shown in Figure 2. The geometric mean \nof the lazy barrier overhead is 6%, with a maximum of 11% overhead for javac. This is signi.cantly better \nthan previous results, but still not acceptable in our opinion. On the other hand, the geometric mean \nof the eager barrier over\u00adhead is only 4%, with a maximum of less than 10% for compress. The mean overhead \nis an order of magnitude better than previous results, and in our opinion low enough for incorporation \ninto a highly aggressive optimizing compiler, given the potential bene.ts in space utilization and incrementality, \nas shown in the following sections. On the other hand, the variance is still too large: we do not con\u00adsider \nthe slowdown for compress to be acceptable. It turns out that the problem with compress is due to a shortcoming \nin the op\u00adtimizer which is preventing it from performing loop-invariant code motion. Once this bug is \n.xed, we expect the overhead in com\u00adpress to drop below 5%.  7.2 Collector Performance We tested our \nreal-time collector on the SPECjvm98 benchmarks and a synthetic fragger benchmark designed to act adversarially: \nit allocates at a high rate, uses a maximal amount of memory, and creates maximal fragmentation. Of the \nSPEC benchmarks, mpegaudio was excluded because it performed little allocation and would have no necessary \ngarbage collections. In addition, compress was excluded because our current implementation does not fully \nsupport arraylets and com\u00adpress makes frequent use of large arrays. Table 1 presents overall results \nfor the benchmarks when run with a target utilization UT(.ms)=.4., mutator quantum QT =lms, and a collector \nquantum CT =l.ms. For each program, we include the high watermark of live data and the maxi\u00admum memory \nactually used. The average allocation rate is the allo\u00ad * cation rate over the entire execution (.)whereas \npeak allocation measures the maximum allocation rate during a mutator quantum * (QT). The collection \nrate Pshows how quickly the collector can trace through the live data of that application. For each program, \nwe show the target application utilization and the worst actual uti\u00adlization that occurred. Average and \nmaximum pause times are in\u00adcluded. Finally, we show the total amount of moved and traced data as an indication \nof how much defragmenting work is necessary. Benchmark Maximum Memory Allocation Rate Coll. Rate P Min. \nUtil. UT(t) Pause Time Copied Traced Live Used sT Ratio sT/ Avg. * (.) Peak * (QT) Avg. Max. javac 34 \n69.3 2.0 14.2 258.0 39.4 0.446 11.3 12.3 12.1 299.4 jess 21 52.4 2.5 19.2 94.2 53.2 0.441 11.0 12.4 2.0 \n324.0 jack 30 59.3 2.0 16.0 105.1 57.4 0.441 10.7 12.4 3.5 321.7 mtrt 28 44.4 1.6 9.6 114.3 45.1 0.446 \n11.0 12.3 2.3 176.9 db 30 54.8 1.8 14.2 82.1 36.7 0.441 11.4 12.4 1.3 144.6 fragger 20 47.7 2.4 17.5 \n185.9 38.4 0.441 11.0 12.4 12.6 307.0 Table 1: Overall Results for the Time-Based Collector. .is the \ntotal run-time of the program. Target mutator quantum QT =l ms, target collector quantum CT =l.ms, target \nutilization is 0.45, t=.ms. All sizes in MB, all rates in MB/s, all times in milliseconds. All of our \nbenchmarks had a similar amount of maximum live data (between 20 and 30 MB) but they required anywhere \nfrom 45 to 70 MB at some point in their execution. The variance in space usage arises from several factors. \nThe heap size requirement ap\u00adpears to be primarily correlated to the average allocation rate for instance \nnote the high allocation rate for jess and the correspond\u00adingly high maximum memory ratio. The measured \nvalues for the rate of collection Prange from 36.7 to 57.4 MB/s. This is primarily due to variation in \npointer density in the data structures of the programs, and shows that while our theoretical assumption \nthat Pis constant does not introduce large error, it is nonetheless signi.cant. Average allocation rates \nranged from 9.6 to 19.2 MB/s while peak allocation rates ranged from 82.1 to 258 MB/s. These spikes in \nallocation rates demonstrate the infeasibility of using a purely work-based scheduling policy for the \ngoal of maintaining a high minimum utilization. For all benchmarks, we ran the collector with a target \napplication utilization of 0.45 and obtained a minimum utilization of 0.441 to 0.446. Thus the maximum \ndeviation is only 2%. The last two columns in Table 1 show the amount of data copied and traced over \nthe entire execution of the program. The maxi\u00admum amount of data copied is about 4% of the data traced \n(interest\u00adingly, javac introduces about the same amount of fragmentation as fragger, which we wrote speci.cally \nas a fragmenting adver\u00adsary program). Note that the amount of data traced by our collector is roughly \ncomparable to the amount of data that would be copied by a semi-space collector, although such a collector \nwould require a signi.cantly larger heap to obtain the same performance. Table 2 summarizes the results \nwhen we changed from time-to work-based collector scheduling. The table only shows those quan\u00adtities \nthat changed appreciably from the time-based collector. Also, since the utilization at our target twas \noften zero, we also give the utilization for an interval of 50 ms. Even at this longer interval, the \nbest case is only half the target value. While average pause times are considerably lower, the maximum \npause times for the work-based collector are much higher (up to 92 ms for fragger)and at t=.ms the minimum \nmutator uti\u00adlization is very poor. These measurements con.rm experimentally the analytic results from \nSection 4.  7.3 Detailed Evaluation We examine three benchmarks in detail: mtrt, javac,and fragger. \nThese three were chosen because they represent a range of dif.culty for the collector. For both time-and \nwork-based schedul\u00ading, we compare the distribution of pause times, the utilization over time, the MMU \nover a full range of intervals, and the space con\u00adsumption of these three benchmarks. The pause time \ndistributions are shown in Figures 3 through 8. These .gures show that our time-based collector achieves \nhighly uniform pause times, with the majority of all pauses at 12.2 ms. By comparison, the work-based \ncollector has a much more uneven distribution (note the differences in scale on both the xand yaxes). \nThe work-based collector has considerably shorter average pauses, but the distribution is much more uneven \nand there is a much longer tail in the distribution. The adversarial nature of fragger is clearly seen \nin Figure 8: while the work-based collector keeps the vast majority of pauses below 10 ms, the tail extends \nto almost 100 ms. If one only considered maximum pause time, the pause time dis\u00adtribution graphs would \ngive the impression that utilization under the work-based collector would be about 2-3 times worse for \nnon\u00adadversarial programs. However, Figures 9 through 14 show that for a short interval on the order likely \nto be of interest in real-time sys\u00adtems (22.2 ms), work-based scheduling produces very large vari\u00adance \nin mutator utilization, often dropping to almost zero. This can easily occur when a single large object \nis allocated, forcing the collector to perform many collector quanta in a row. On the other hand, the \ntime-based collector performs extremely well. There is a small amount of jitter due to slight imprecision \nin our work predictor, but utilization during collection is almost exactly on or above the target. For \nmtrt and javac, after the .rst collection the application enters a fairly regular cycle in which the \nconcurrent collector is off for l/3to l/of the time. However, the adversarial nature of fragger is once \nagain apparent: in the time-based collector, it collects continuously, and in the work-based collector \nthe utiliza\u00adtion frequently drops to zero. Figures 15 through 17 show the minimum mutator utilization \n(MMU [12]) of both time-and work-based collectors superimposed on one graph. The time scale ranges from \n10 milliseconds to the length of the program run. At small time scales, the MMU for the time-based collector \nalmost precisely matches the shape of the perfect curve shown in Figure 1. At larger time scales, the \neffect of the mutator being off come into play, and utilization rises above the target. MMU for the work-based \ncollector is much lower, and interest\u00adingly has much less of a sawtooth shape. At the time scale of a \nsmall number of collections, the work-based collector may brie.y exceed the time-based collector in utilization, \nbut as the number of collections becomes large they appear to approach the same asymp\u00adtotic cost. We \ncompute the MMU precisely below t=lseconds using a quadratic algorithm; above 10 seconds we use an approximate \nalgorithm which has very small error. Cheng and Belloch [12] used Benchmark Minimum Utilization Pause \nTime Uw(t) Uw(.ms) Avg. Max. javac 0 0.118 5.1 31.9 jess 0 0.180 3.1 26.2 jack 0.001 0.152 2.8 13.2 mtrt \n0.002 0.227 5.0 18.8 db 0 0.141 6.0 28.0 fragger 0 0 3.7 92.1 Table 2: Overall Results for the Work-Based \nCollector. Mu\u00adtator allocation quantum Qw =4KB, collector processing quantum Cw =lKB, t=.ms. All times \nin millisec\u00adonds. a sampling technique and only plotted the MMU for certain values of t, thus hiding \nsome of the irregularity of the curve. Blackburn et al. [8] use a variant of the MMU which produces a \nmonotonic curve which is strictly derivable from the MMU curve. This de.nition of utilization is appropriate \nat the large time scales at which these collectors operate (several hundred milliseconds and above) but \nhides information that is important at short time intervals of interest in true real-time systems. Finally, \nFigures 18 through 20 show space consumption over time for both time-and work-based collectors. The maximum \nlive data and the collector trigger threshold are also shown. What is surprising is how little difference \nthere is between time-and work\u00adbased memory consumption given the large differences in behavior seen \nin the previous graphs. There is some variation, but there is no clear winner: each type of scheduling \nsometimes requires slightly more or slightly less space, but the shape of the space curves is very similar \nand only slightly translated.  8. REAL-TIME ISSUES In Section 2 we outlined some of the problems common \nto real\u00adtime collectors. The design choices made in our collector avoid these problems in the following \nways: Fragmentation is avoided through a combination of means: internal fragmentation is limited by \nchoosing a small ratio for adjacent size classes. External fragmentation is prevented by defragmenting \nthe heap as needed, and by breaking up large arrays into arraylets.  Space overhead is limited by using \na mostly non-copying al\u00adgorithm, so that from-space and to-space are mostly sharing physical storage. \n Uneven mutator utilization is avoided because we use a time\u00adbased scheduling policy, which is not sensitive \nto variations in average allocation rate at small (real-time) intervals but only at large intervals on \nthe order of a full collection.  Large data structures are handled by using arraylets, which effectively \nturns large objects into small objects.  8.1 Flaws in Baker s Real-time De.nition Baker [5] begins his \nseminal paper on real-time garbage collec\u00adtion by stating that a real-time list processing system is \none in which the time required by the elementary list operations . . . is bounded by a small constant. \nThis approach has been the basis for most of the later work on real-time collection [2, 6, 10, 11, 17, \n19, 24]. However, this is implicitly a work-based approach, and as we have seen in Sections 4 and 7, \nat the small time intervals that are typically of interest in real-time systems, work-based collectors \nmay be subject to very poor utilization. Baker attempts to .nesse this problem by interleaving the col\u00adlector \nwith the mutator in a very .ne-grained manner, but this only hides the problem: it keeps individual pauses \nlow, but does not pre\u00advent numerous closely-spaced pauses. In the case of Baker s copy\u00ading collector, \nthe read barrier converts what was originally a simple load instruction into a sequence of tests, loads, \nand possibly a copy of the object. Let us say that the cost of such a read with barrier is .times the \ncost of the original read operation. Then if we consider a short interval tcontaining only read operations, \nthe utilization will be l/.. Ultimately, it comes down to a question of what one means by small . If \n., then the utilization will probably be acceptable. However, more typical values are 10 to 20. In such \nshort intervals, utilization may drop so low as to be useless, as we saw experimen\u00adtally in Table 2. \nThere are fundamentally three ways to ameliorate this problem: increase t, decrease .,ormake .bimodal. \nIncreasing tis de\u00adpendent on the real-time requirements of the application. An exam\u00adple of decreasing \n.is Brooks variant [10] of Baker s algorithm: a read only requires one extra load instruction, and the \ncostly barrier is only performed on writes, which are considerably less frequent. However, at a resolution \nof 1 ms, there could be a lot of writes, and the .for the write barrier is unlikely to be less than 10 \n(and is often much higher), so utilization could still be very low. Attempts have been made to further \nreduce the cost of the write barrier by using a store buffer [24] or by pre-allocating the space for \nthe copied object and deferring the actual copy to collection time [15]. Nettles and O Toole [21] introduced \nreplicating copying collec\u00adtors [12, 16], which represent another point in the tradeoff space. In these \ncollectors, there is no read barrier, but the overall cost of the write barrier is more expensive because \nit may have to update both to-and from-space objects. Baker attempted to keep performance uniform by \ninterleaving the allocator with each CONS, CAR,and CDR operation. However, the more .ne-grained the interleaving, \nthe higher the relative cost of the operations. Many subsequent collectors have attempted to reduce the \ntime overhead of concurrent collection by batching the work (the Appel-Ellis-Li collector [2], which \nuses virtual memory page traps, is an extreme example). However, this limits the reso\u00adlution of t, and \ndoes not function well when the cost of the quan\u00adtized work varies widely (for example, due to variation \nin object sizes) or when the quanta occur irregularly. If the variation is low, it should be possible, \nfor a given t, to determine the best batch size analytically. Ultimately, the distinction that is generally \nmade in the literature between hard real-time and soft real-time is an over-simpli.cation. There is really \na continuum that depends on the required response time and the cost and variability of collector operations. \n 8.2 Time-based Collectors While most previous work on real-time collection has focused on work-based \nscheduling, there are some notable exceptions. In par\u00adticular, Henriksson [15] implemented a Brooks-style \ncollector [10] in which application processes are divided into two priority lev\u00adels: for high-priority \ntasks (which are assumed to be periodic with bounded compute time and allocation requirements), memory \nis pre-allocated and the system is tailored to allow mutator operations to proceed quickly. For low-priority \ntasks, no response-time goals are set. Henriksson gives a schedulability analysis using the real-time \nscheduling techniques of Joseph and Pandya [18]. While his anal\u00adysis is work-based, his formula for utilization \nis similar to our for\u00admula for time-based scheduling. This is because in his collector the high-priority \nmutators can always interrupt the collector when they are ready to run. Thus we see that interrupt-driven \nwork-based scheduling is essentially the same as periodic time-based schedul\u00ading. The garbage collectors \nof Nettles and O Toole [21] and North and Reppy [22] run the collector in a separate thread, which appears \nto be a time-based approach. However, Nettles and O Toole dy\u00adnamically detect situations in which the \nmutator is allocating faster than the collector, in which case they pause the mutator while a .xed amount \nof work is performed. North and Reppy s collector does not have any feedback, nor is there any way of \nbalancing the mutator/collector quanta, so muta\u00adtors with high allocation rates may fail. 9. CONCLUSIONS \nWe have presented a hybrid real-time collector that operates pri\u00admarily as a non-moving incremental mark-sweep \ncollector, but pre\u00advents fragmentation via the use of limited copying (no more than 4% of traced data \nin our measurements). Because fragmentation is bounded, the collector has a provable space bound yet \nretains a lower space overhead than a fully-copying real-time collector. The key to fully incremental \ndefragmentation is a low-overhead read barrier that maintains consistency without compromising the real-time \nbounds. We have shown that in an optimizing Java com\u00adpiler, a highly ef.cient software read barrier can \nbe implemented and will only cause a 4% mean slowdown. We have implemented the collector and shown that \nfor real ap\u00adplications it can achieve highly predictable mutator utilization rates with highly stable \npause times at real-time resolution. It is gener\u00adally able to achieve 45% utilization while the collector \nis on with only 1.6 2.5 times the actual memory high water mark of the ap\u00adplication. Acknowledgements \nWe thank David Grove for his assistance in implementing the read barrier optimizations, and the entire \nJikes RVM team for provid\u00ading the research platform which made this work possible. We also thank Rob \nO Callahan, David Grove, Mike Hind, and the anony\u00admous referees for their helpful comments. 10. REFERENCES \n[1] ALPERN,B., ET AL. The Jalape no virtual machine. IBM Syst. J. 39, 1 (Feb. 2000), 211 238. [2] APPEL,A. \nW., ELLIS,J. R., AND LI, K. Real-time concurrent col\u00adlection on stock multiprocessors. In Proceedings \nof the SIGPLAN 88 Conference on Programming Language Design and Implementation (Atlanta, Georgia, June \n1988). SIGPLAN Notices, 23, 7 (July), 11 20. [3] ARNOLD,M., AND RYDER, B. G. Thin guards: A simple and \nef\u00adfective technique for reducing the penalty of dynamic class load\u00ading. In Proceedings of the Sixteenth \nEuropean Conference on Object-Oriented Programming (M\u00b4alaga, Spain, June 2002), B. Magnusson, Ed., vol. \n2374 of Lecture Notes in Computer Science, pp. 498 524. [4] BACON,D. F., FINK,S. J., AND GROVE, D. Space-and \ntime\u00adef.cient implementation of the Java object model. In Proceedings of the Sixteenth European Conference \non Object-Oriented Programming (M\u00b4alaga, Spain, June 2002), B. Magnusson, Ed., vol. 2374 of Lecture Notes \nin Computer Science, Springer-Verlag, pp. 111 132. [5] BAKER, H. G. List processing in real-time on a \nserial computer. Com\u00admun. ACM 21, 4 (Apr. 1978), 280 294. [6] BAKER, H. G. The Treadmill, real-time garbage \ncollection without motion sickness. SIGPLAN Notices 27, 3 (Mar. 1992), 66 70. [7] BEN-YITZHAK,O., GOFT,I., \nKOLODNER,E. K., KUIPER,K., AND LEIKEHMAN, V. An algorithm for parallel incremental com\u00adpaction. In Proc. \nof the Third International Symposium on Memory Management (Berlin, Germany, June 2002), pp. 100 105. \n[8] BLACKBURN,S. M., JONES,R., MCKINLEY,K. S., AND MOSS,J. E. B. Beltway: getting around garbage collection \ngridlock. In Proc. of the SIGPLAN Conference on Programming Language Design and Implementation (Berlin, \nGermany, 2002), pp. 153 164. [9] BOLLELLA,G., GOSLING,J., BROSGOL,B. M., DIBBLE,P., FURR,S., HARDIN,D., \nAND TURNBULL,M. The Real-Time Speci\u00ad.cation for Java. The Java Series. Addison-Wesley, 2000. [10] BROOKS, \nR. A. Trading data space for reduced time and code space in real-time garbage collection on stock hardware. \nIn Conference Record of the 1984 ACM Symposium on Lisp and Functional Pro\u00adgramming (Austin, Texas, Aug. \n1984), G. L. Steele, Ed., pp. 256 262. [11] CHEADLE,A.M., FIELD,A.J., MARLOW,S., PEYTON JONES, S. L., \nAND WHILE, R. L. Non-stop Haskell. In Proc. of the Fifth In\u00adternational Conference on Functional Programming \n(Montreal, Que\u00adbec, Sept. 2000). SIGPLAN Notices, 35, 9, 257 267. [12] CHENG,P., AND BLELLOCH, G. A parallel, \nreal-time garbage collec\u00adtor. In Proc. of the SIGPLAN Conference on Programming Language Design and Implementation \n(Snowbird, Utah, June 2001). SIGPLAN Notices, 36, 5 (May), 125 136. [13] CHENG,P., HARPER,R., AND LEE, \nP. Generational stack collection and pro.le-driven pretenuring. In Proc. of the Conference on Pro\u00adgramming \nLanguage Design and Implementation (June 1998). SIG-PLAN Notices, 33, 6, 162 173. [14] DIMPSEY,R., ARORA,R., \nAND KUIPER, K. Java server perfor\u00admance: A case study of building ef.cient, scalable JVMs. IBM Syst. \nJ. 39, 1 (2000), 151 174. [15] HENRIKSSON,R. Scheduling Garbage Collection in Embedded Sys\u00adtems. PhD \nthesis, Lund Institute of Technology, July 1998. [16] HUDSON,R. L., AND MOSS, E. B. Incremental garbage \ncollection for mature objects. In Proc. of the International Workshop on Memory Management (St. Malo, \nFrance, Sept. 1992), Y. Bekkers and J. Cohen, Eds., vol. 637 of Lecture Notes in Computer Science. [17] \nJOHNSTONE,M. S. Non-Compacting Memory Allocation and Real-Time Garbage Collection. PhD thesis, University \nof Texas at Austin, Dec. 1997. [18] JOSEPH,M., AND PANDYA, P. K. Finding response times in a real\u00adtime \nsystem. Computer Journal 29, 5 (1986), 390 395. [19] LAROSE,M., AND FEELEY, M. A compacting incremental \ncollector and its performance in a production quality compiler. In Proc. of the First International Symposium \non Memory Management (Vancouver, B.C., Oct. 1998). SIGPLAN Notices, 34, 3 (Mar., 1999), 1 9. [20] MOON, \nD. A. Garbage collection in a large LISP system. In Confer\u00adence Record of the 1984 ACM Symposium on LISP \nand Functional Programming (Austin, Texas, Aug. 1984), pp. 235 246. [21] NETTLES,S., AND O TOOLE, J. \nReal-time garbage collection. In Proc. of the SIGPLAN Conference on Programming Language Design and Implementation \n(June 1993). SIGPLAN Notices, 28, 6, 217 226. [22] NORTH,S.C., AND REPPY, J. H. Concurrent garbage collection \non stock hardware. In Functional Programming Languages and Com\u00adputer Architecture (Portland, Oregon, \nSept. 1987), G. Kahn, Ed., vol. 274 of Lecture Notes in Computer Science, pp. 113 133. [23] SIEBERT, \nF. Eliminating external fragmentation in a non-moving garbage collector for Java. In International Conference \non Compilers, Architecture, and Synthesis for Embedded Systems (San Jose, Califor\u00adnia, Nov. 2000), pp. \n9 17. [24] YUASA, T. Real-time garbage collection on general-purpose ma\u00adchines. Journal of Systems and \nSoftware 11, 3 (Mar. 1990), 181 198. [25] ZORN, B. Barrier methods for garbage collection. Tech. Rep. \nCU-CS\u00ad494-90, University of Colorado at Boulder, 1990. 250 mtrt.time.opt: Distribution of pause time \n18 mtrt.work.opt: Distribution of pause time 16 200 14 12 150 10 Count Count Count 100 50 0 0 2 4 6 \n8101214 Pause Time(ms) Figure 3: Time-based collector pause times for mtrt javac.time.opt: Distribution \nof pause time 600 500 400 300 200 100 0 0 2 4 6 8101214 Pause Time(ms) Figure 4: Time-based collector \npause times for javac torture.time.opt: Distribution of pause time 500 450 400 350 300 Count Count Count \n8 6 4 2 0 0 2 4 6 8 1012141618 Pause Time(ms) Figure 6: Work-based collector pause times for mtrt javac.work.opt: \nDistribution of pause time 50 45 40 35 30 25 20 15 10 5 0 0 5 101520253035 Pause Time(ms) Figure 7: Work-based \ncollector pause times for javac torture.work.opt: Distribution of pause time 120 100 80 60 200 40 150 \n100 20 50 0 0 0 2 4 6 8 10 12 14 0 102030405060708090100 Pause Time(ms) Pause Time(ms) Figure 5: Time-based \ncollector pause times for fragger Figure 8: Work-based collector pause times for fragger 1 1 0.8 0.8 \n Mutator CPU Utilization Mutator CPU Utilization Mutator CPU Utilization 0.6 0.4 0.2 0.2 0 0 Time (s) \nTime (s) Figure 9: Time-based utilization for mtrt, t=.ms. Figure 12: Work-based utilization for mtrt, \nt=.ms. 1 1 0.8 0.8  0.6 0.4 0.6 0.4 0.2 0.2 0 0 Time (s) Time (s) Figure 10: Time-based utilization \nfor javac, t=.ms. Figure 13: Work-based utilization for javac, t=.ms. 1 1 0.8 0.8  0.6 0.6 0.4 0.2 0.2 \n0 0 Time (s) Time (s) Figure 11: Time-based utilization for fragger, t=.ms. Figure 14: Work-based utilization \nfor fragger, t=.ms. Mutator CPU Utilization Mutator CPU Utilization Mutator CPU Utilization mtrt.work.opt: \nSpace usage vs time 50  0.9 45 0.8 40 0.7 35 0.6 Utilization Utilization Utilization Memory (Mb) Memory \n(Mb) Memory (Mb) 30 25 0.3 0.2 0.1 0 Figure 15: MMU for mtrt. 1 20 15 10 5 0 Time (s) Figure 18: Space \nconsumption by mtrt. javac.work.opt: Space usage vs time  80 0.9 70 0.8 60 0.7 0.6 50 0.5 40 30 0.3 \n20 0.2 0.1 10 0 0 Delta t (s) Time (s) Figure 16: MMU for javac. Figure 19: Space consumption by javac. \ntorture.work.opt: Space usage vs time 1  0.9 50 0.8 0.7 40 0.6 300.5 0.4 20 0.3 0.2 10 0.1 0 0 Figure \n17: MMU for fragger. Figure 20: Space consumption by fragger.   \n\t\t\t", "proc_id": "604131", "abstract": "Now that the use of garbage collection in languages like Java is becoming widely accepted due to the safety and software engineering benefits it provides, there is significant interest in applying garbage collection to hard real-time systems. Past approaches have generally suffered from one of two major flaws: either they were not provably real-time, or they imposed large space overheads to meet the real-time bounds. We present a mostly non-moving, dynamically defragmenting collector that overcomes both of these limitations: by avoiding copying in most cases, space requirements are kept low; and by fully incrementalizing the collector we are able to meet real-time bounds. We implemented our algorithm in the Jikes RVM and show that at real-time resolution we are able to obtain mutator utilization rates of 45% with only 1.6--2.5 times the actual space required by the application, a factor of 4 improvement in utilization over the best previously published results. Defragmentation causes no more than 4% of the traced data to be copied.", "authors": [{"name": "David F. Bacon", "author_profile_id": "81100628167", "affiliation": "IBM T.J. Watson Research Center, Yorktown Heights, NY", "person_id": "P60470", "email_address": "", "orcid_id": ""}, {"name": "Perry Cheng", "author_profile_id": "81451593218", "affiliation": "IBM T.J. Watson Research Center, Yorktown Heights, NY", "person_id": "PP43116113", "email_address": "", "orcid_id": ""}, {"name": "V. T. Rajan", "author_profile_id": "81331502483", "affiliation": "IBM T.J. Watson Research Center, Yorktown Heights, NY", "person_id": "PP43115622", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/604131.604155", "year": "2003", "article_id": "604155", "conference": "POPL", "title": "A real-time garbage collector with low overhead and consistent utilization", "url": "http://dl.acm.org/citation.cfm?id=604155"}