{"article_publication_date": "01-15-2003", "fulltext": "\n Incremental Algorithms for Dispatching in  Dynamically Typed Languages o Yoav Zibin Joseph (Yossi) \nGil Technion Israel Institute of Technology zyoav yogi @ cs.technion.ac.il ABSTRACT A fundamental problem \nin the implementation of object-oriented languages is that of a frugal dispatching data structure, i.e., \nsup\u00adport for quick response to dispatching queries combined with com\u00adpact representation of the type \nhierarchy and the method families. Previous theoretical algorithms tend to be impractical due to their \ncomplexity and large hidden constant. In contrast, successful prac\u00adtical heuristics, including Vitek \nand Horspool s compact dispatch tables (CT) [16] designed for dynamically typed languages, lack theoretical \nsupport. In subjecting CT to theoretical analysis, we are not only able to improve and generalize it, \nbut also provide the .rst non-trivial bounds on the performance of such a heuristic. Let nv ,ml ,\u00a3denote \nthe total number of types, messages, and dif\u00adferent method implementations, respectively. Then, the dispatch\u00ading \nmatrix, whose size is n m, can be compressed by a factor of at most v r ,n mh o/s\u00a3. Our main variant \nto CT achieves a compression factor of l . More generally, we describe a sequence of algo\u00ad 0rithms CT, \nCT , CT , ..., where CTdachieves compression by a l factor of (at least) ld lo-.l d, while using dmemory \ndereferencing operations during dispatch. This tradeoff represents the .rst bounds on the compression \nratio of constant-time dispatching algorithms. A generalization of these algorithms to a multiple-inheritance \nsetting, increases the space by a factor of Ilo-al d, where Iis a met\u00adric of the complexity of the topology \nof the inheritance hierarchy, which (as indicated by our measurements) is typically small. The most important \ngeneralization is an incremental variant of the CTd scheme for a single-inheritance setting. This variant \nuses at most twice the space of CTd, and its time of inserting a new type into the hierarchy is optimal. \nWe therefore obtain algorithms for ef.cient management of dispatching in dynamic-typing, dynamic-loading \nlanguages, such as SMALLTALK and even the JAVA invokein\u00adterfaceinstruction. Research supported by the \nBar-Nir Bergreen Software Technol\u00adogy Center of Excellence o Research supported in part by the fund for \nthe promotion of re\u00adsearch at the Technion. Permission to make digital or hard copies of all or part \nof this work for personal or classroom use is granted without fee provided that copies are not made or \ndistributed for pro.t or commercial advantage and that copies bear this notice and the full citation \non the .rst page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires \nprior speci.c permission and/or a fee. POPL 03, January 15 17, 2003, New Orleans, Louisiana, USA. Copyright \n2003 ACM 1-58113-628-5/03/0001 ...$5.00.   Categories and Subject Descriptors D.3.4 [Programming Languages]: \nProcessors Run-time envi\u00adronments; D.1.5 [Programming Techniques]: Object-oriented Pro\u00adgramming; D.3.3 \n[Programming Languages]: Language Constructs and Features Inheritance; G.4 [Mathematical Software]: Algo\u00adrithm \ndesign and analysis General Terms Algorithms, Design, Measurement, Performance, Theory Keywords CT, \nDispatch, Dynamic-typing, Hierarchy, Incremental, Message 1. Introduction Message dispatching stands \nat the heart of object-oriented pro\u00adgrams, being the only way objects communicate with each other. To \nimplement dynamic binding during dispatch, the runtime sys\u00adtem of object-oriented languages uses a dispatching \ndata structure, in which a dispatching query .nds the appropriate implementation of the message to be \ncalled, according to the dynamic type of the message receiver. A fundamental problem in the implementation \nof such languages is then a frugal implementation of this data struc\u00adture, i.e., simultaneously satisfying \n(i) compact representation of the type hierarchy and the families of different implementations of each \nmethod selector, and (ii) quick response to dispatching queries. Virtual function tables (VFT) are a \nsimple and well known (see e.g., [14]) incremental technique which achieves dispatching in con\u00adstant \ntime (two dereferencing operations), and very good compaction rates. The VFT of each type is an array \nof method addresses. A lo\u00adcation in this array represents a message, while its content is the address \nof an implementing method. The VFT of a subtype is an extension of the VFT of its supertype, and messages \nare allocated locations at compile time in sequential order. The static type of the receiver uniquely \ndetermines the location associated with each message. VFTs rely on single-inheritance. Multiple-inheritance \nimplementations exist [9], but they are not as elegant or ef.cient. The challenge in the dispatching \nproblem is therefore mostly in dealing with dynamically typed and/or multiple-inheritance lan\u00adguages. \nAlso very important is the incremental version of this prob\u00adlem, in which types (together with their \naccompanying messages and methods) are added at the bottom of the hierarchy. Our contribution (described \nin greater detail in Section 1.3) in\u00adcludes a provable tradeoff between space and dispatching time with \nextensions to multiple-inheritance hierarchies. The pinnacle of the results is an incremental algorithm \nfor maintaining a compact dis\u00adpatch table in dynamically typed languages. 1.1 The Problem We de.ne the \ndispatching problem in a similar fashion to the colored-ancestors abstraction described by Ferragina \nand Muthukr\u00adishnan [7]: a hierarchy is a partially ordered set (T,:) whereTis a set of types1 and :is \na re.exive, transitive and anti-symmetric subtype relation. The yi hoperator return the set of smallest \ntypes in any given set: yi hhXe ={et E X i {ta' E X bt ' i tat ' :gt } a(1.1) Let FFTdenote the family \nof types which have a method imple\u00admentation for the same message.2 For example, consider the single-inheritance \nhierarchy in Fig\u00adure 1.1a. Type names are uppercase and messages are lowercase, e.g., type D implements \nthe messages c, e and f. Then, {ADE}is the family of method implementations of c. a b c d e f A B C \n(a) (b) D E F b G Figure 1.1: (a) A small example of a single-inheritance hierarchy, and (b) its dispatching \nmatrix Given a family Fand a type t, cand(F t,is the set of candidates in F, i.e., those ancestors of \ntin which an implementation of the given message exists: cand(F t,=F nbhn c e atbs(t,)a(1.2) In the .gure, \nwe have for example cand({ADE}G) ={AD}. A dispatching query dispatch(Fatoreturns either the smallest \ncandidate orhn nl lif no such unique candidate exists. (Ahn nl lresult represents either the message \nnot understood or message ambigu\u00adous error conditions.) Speci.cally, dispatch(Fatot'if yi hcand(F t, \no) ={ t'}(1.3)hn ul lotherwisea DEFINITION 1.1. Given a hierarchy (T,:) and a family col\u00adlection FN T \n(T, the dispatching problem is to encode the hi\u00aderarchy in a data structure supporting dispatch(Fat,queries \nfor all FFE F,t E T. A solution to the dispatching problem is measured by the fol\u00adlowing three metrics: \n(i) space, (ii) query time, and (iii) encoding creation time. We would like to express these as a function \nof the following problem parameters t,nv om,\u00a3e}: the number of types, fam\u00adilies, and implementations \n(or family members). Speci.cally, nhT mF h (1.4) \u00a3=LF The distinction between type, class, interface, \nsignature, etc., as it may occur in various languages does not concern us here. We shall refer to all \nthese collectively as types. We abstract away from the nomenclature of different languages, and use the \nterms message (also called selectors or signature) for the unique identi.er of a family of implementation \n(also called methods, member functions, operations, features, etc.) In Figure 1.1 for example, we have \nnm\u00a3 n =7, m =6and \u00a3 =1 6. The incremental version of the problem, is to maintain this data structure \nin the face of additions of types (with their accompanying methods) to the bottom of the hierarchy, as \ndone in languages such as JAVA [1]. 1.2 Simple solutions The most obvious solution is an nm n dispatching \nmatrix, stor\u00ading the outcomes of all possible dispatching queries. Figure 1.1b shows the dispatching \nmatrix of Figure 1.1a, where the \u00a3gray en\u00adtries correspond to (non-inherited) family members. In the \ndispatching matrix representation, queries are answered by a quick indexing operation. However, the space \nconsumption is prohibitively large, e.g., 512MB for the dispatching matrix in the largest hierarchy in \nour benchmarks (8,793 types and 14,575 families). Note that an encoding that does not try to compress \npointers must use at least \u00a3cells for representing the \u00a3different method addresses. We would like to \nget as close as possible to this space requirement while preserving a constant and small query time. \nThe dispatching matrix can be potentially compressed by a factor of v r ,n mh o/\u00a3(1.5) ba We shall refer \nto as the optimal compression factor, and to schemes attempting to reach as duplicates-elimination schemes. \nIn our data-set of 35 large hierarchies (see Section 5), =7b2 5. Let hdenote the number of non-hn nl \nlentries in the dispatching matrix, i.e., h{{t(Fat }dispatch(Fat,)i =hn nl l }ca(1.6) By eliminating \nhn ul lmemory cells, the dispatching matrix might be compressed by a factor of ,nmo/,h, which is around \n150 in our data-set. Examples of hn nl l-elimination schemes are row displace\u00adment [5, 6], selector coloring \n[4, 12], and virtual function tables (VFT) [14]. In single-inheritance and static-typing setting of the \nproblem, the VFT technique uses precisely hmemory cells. In the more general setting, the matrix can \nalso be compressed into 0hhcells (with fairly large constants) by using perfect hash\u00ading [8] or one of \nits variants. Even though dispatching time is con\u00adstant in perfect hashing, it is complicated by the \n.nite-.eld arith\u00admetic incurred during the computation of the hash function. With additional increase \nto the complexity of dispatching, there are variations to the famous FKS [8] scheme which usehi g hh \ncells. There is also a dynamic version of perfect hashing [3] which can support incremental dispatching. \nThe memory toll is even larger, with constants in the range of a thousand. Notice that even complete \nnull-elimination gives suboptimal com\u00adpression, since hmight be substantially larger than \u00a3. In our bench\u00admark \nof 35 large hierarchies, h/\u00a3is on average 8.3, and in one hierarchy it is 122.4! It is not dif.cult to \ncome close to complete duplicates-elimination, i.e., a space of 0\u00a3, with a simple representation of the \nhierarchy as a graph where types are nodes and immediate inheritance rela\u00adtions are edges. The cost is \nof course the query time, which be\u00adcomes 0,n., since we must traverse all the ancestors of a receiver \nin order to .nd the nearest family member. Sophisticated caching algorithms(asemployedintheruntimesystemof \nSMALLTALK [2]) make the typical case more tolerable than what the worst case indi\u00adcates. 1.3 Contribution \nThere is a large body of research on the dispatching problem (see e.g., [2, 4 6, 10, 12, 15 18]). The \nfocus in these was on prac\u00adtical algorithms, which were evaluated empirically, rather than by provable \nupper bound on memory usage. The main theoretical research on the topic [7, 11] produced algorithms (for \nthe single\u00adinheritance setting) which using minimal space (0\u00a3cells) sup\u00adported dispatching in doubly \nlogarithmic, 0hllgllgn., time. How\u00adever, the hidden constants are large, and the implementation is com\u00adplicated. \nIn this paper, we describe a different tradeoff: constant-time dis\u00adpatching in dsteps, while using at \nmost d\u00a3\u00a3f0cells. Stated differ\u00adently, our results are that dsteps in dispatching (provably) achieve a \ncompression rate ofd f. For example, with id2the com\u00ad pression is by a factor of at least half of the \nsquare root of , the optimal compression rate. Also, the compression factor is close to optimal, 'p, \nwhen the dispatching time is logarithmic, 1glm. An important advantage of these results in comparison \nto pre\u00advious theoretical algorithms is that they are simple and straight\u00adforward to implement, and bear \nno hidden constants. In fact, our algorithms are based on a successful practical technique, namely compact \ndispatch tables (CT), which was invented by Vitek and Horspool [16]. Viewed differently, the results \npresented here give the .rst proof of a non-trivial upper bound on practical algorithms. Even though \nthe algorithms carry on to multiple-inheritance with the same time bounds of dispatching, the memory \nconsumption in\u00adcreases by a factor of at most 2Ilo-.ld, where Ican be thought of as a metric of the complexity \nof the topology of the inheritance hierarchy. (In a benchmark of 19 multiple-inheritance hierarchies \nwith 34,810 types, we found the median value of an upper bound for Iis 5, the average is 6.4, and the \nmaximum is 18.) Our pre\u00advious work [18] on dispatching gives an implementation of a dis\u00adpatching data \nstructure whose space was only 0I\u00a3, but the dis\u00adpatching time was logarithmic. The results presented \nhere com\u00adplete the tradeoff spectrum, giving constant time dispatching with any number of steps. We give \nempirical evidence that the algo\u00adrithms perform well in practice, in many cases even better than the \ntheoretically obtained upper bounds. We also describe an incremental version of the algorithms in a single-inheritance \nsetting, and prove that updates to the dispatching data structures can be made in optimal time. The cost \nis in a small constant factor increase (e.g., 2) to the memory footprint. A variant of the this algorithm \nfor the multiple-inheritance set\u00adting is not described for lack of space. Readers may also take interest \nin some proof techniques, including the representation of dispatching as search in a collection of partitionings, \nthe elegant Lemma 4.1, and the amortization analysis of the incremental algo\u00adrithm. Outline The remainder \nof this article is organized as follows. Sec\u00adtion 2 presents the generalized CT schemes for single-inheritance \nhierarchies. Section 3 shows how these schemes can be made incre\u00admental. A (non-incremental) version \nof these schemes for multiple\u00adinheritance hierarchies is described in Section 4. Section 5 presents the \nexperimental results: timing and compression values on a data\u00adset of 35 hierarchies collected from both \nsingle and multiple dis\u00adpatching languages. Open problems and directions for future re\u00adsearch are the \nsubject of Section 6. 2. Generalization of Compact Dispatch Tables for Single-Inheritance Hierarchies \nFor simplicity, assume w.l.o.g. that the hierarchy is a tree (rather than a forest) rooted at a special \nnode E=T. There cannot be a message ambiguous in a single-inheritance setting. To avoid the other error \nsituation, namely message not understood, we assume that for all FFE F =EnF. With this assumption, every \ndispatching query returns a single family member. The cost is in (at most) dou\u00adbling the number of implementations \n\u00a3. (At the end of this section we will see that the memory toll is in fact much smaller.)  Vitek and \nHorspool s CT algorithm [16] partitions the family  collection Finto kdisjoint slices FFla aeaFkaThese \nFslices break the dispatching matrix into ksub-matrices, also called chunks. The authors experience was \nthat chunks with 14 columns each give best results, and this number was hard-coded into their algorithm. \nFigure 2.1 shows the three chunks of the dispatching matrix of Figure 1.1b for following partitioning: \nF={Fa F}Fl={FFd }(2.1) b c F={Fe F}{a f As Vitek and Horspool observed, and as can be seen in the .gure, \nthere are many identical rows in each chunk. Signi.cant compres\u00adsion can be achieved by merging these \nrows together, and introduc\u00ading, in each chunk, an auxiliary array of pointers to map each type to a \nrow specimen. a b c d e f A  B A B A A C D D D B A E D F F D D G Figure 2.1: Three chunks of the dispatching \nmatrix of Figure 1.1b Why should there be many duplicate rows in each chunk? There are two contributing \nfactors: (i) since the slices are small, there are not too many columns in a chunk, and (ii) that the \nnumber of distinct values which can occur in any given column is small, since, as empirical data shows, \nthe number of different implementations of a selector is a small constant. Hence, there could not be \ntoo many distinct rows. However, these considerations apply to any random distribution of values in the \ndispatching matrix. The crucial observation we make is that a much stronger bound on the number of distinct \nrows can be set relying on the fact that the values in the dispatching ma\u00adtrix are not arbitrary; they \nare generated from an underlying struc\u00adtured hierarchy. Consider for example a chunk with two columns, \nwith nand n l distinct implementations in these columns. Simple counting con\u00adsiderations show that the \nnumber of distinct rows is at most nn. l Relying on the fact that the hierarchy is a tree we can show \nthat the number of distinct rows is at most nn. l To demonstrate this observation, consider Figure 2.2a \nwhich fo\u00adcuses on the .rst chunk, corresponding to slice Fl={,Fa Fb }. As can be seen in the .gure, the \nrows of types A, D, and F are identical. Figure 2.2b shows the compressed chunk and the auxil\u00adiary array. \nWe see that this auxiliary array maps types A, D, and F to the same row. We call attention to the (perhaps \nsurprising) fact that it is possible to select from the elements of each row in Figure 2.2b a distinguish\u00ading \nrepresentative. These representatives are members of what we call the master-family F'=FaF={ABCG}{a \nb The representatives of the four rows in the .rst chunk are A, B, C and G, in this order. The .gure \nhighlights these in gray. Also note a b A A A B B a b B a b C C C A D D D B E E E C F F F G G G G \n(a) (b) (c) Figure 2.2: (a) The .rst chunk of Figure 1.1c, (b) the chunk compressed using an auxiliary \narray of pointers, and (c) the chunk compressed using an array of labels that each member of the master-family \nserves as a representative of some row. Figure 2.2c gives an alternative representation of the chunk, \nwhere each row is labeled by its representative. The auxiliary array now contains these labels instead \nof pointers. For example, the second row is labeled B EnFb; the second and the .fth entry of the auxiliary \narray store B rather than the row specimen address. Our improvement is based on the observation that \nthe distin\u00adguishing representatives phenomenon is not a coincidence and on the observation that CT applies \na divide-and-conquer approach to the dispatching problem: The search .rst determines the relevant master-family, \nand then continues to select the appropriate result among its members. Let AFFdenote the compressed ith \nchunk of the dispatching ma\u00adtrix, and let ebe the master dispatching matrix, whose columns are the auxiliary \narrays of the chunks. Figure 2.3 shows matri\u00adces AliAiAand e, which constitute the complete CT represen\u00adtation \nfor the hierarchy of Figure 1.1. Note that the .rst column of eis the auxiliary array depicted in Figure \n2.2c. A 1 A 2 A 3 A a b c d e f B A A A C B D B D C E D E G F F A 1 A 2 A 3 G B Figure 2.3: CT representation \nfor the hierarchy of Figure 1.1 For each slice FFlet the master-family FF'be the union of fam\u00adilies in \nthat slice, i.e., FF'mtm F. Then, answering the query dispatch(Ft,at runtime requires three steps: 1. \nDetermine the slice of F. That is, the family collection FFF, such that FEFF. If the partitioning into \nslices and the se\u00adlector Fare known at compile-time, as it is usually the case in dispatching of static-loading \nlanguages, then this stage in\u00adcurs no penalty at runtime. 2. Fetch the .rst dispatching result t'dispatch(FF'ato. \nThis value is found at the row which corresponds to type tand the column which corresponds to the master-family \nFF', i.e., t' eF[tums]. 3. Fetch the .nal dispatching result t''dispatch(Fato. This type is found in \nthe row of t'and the column of Fin the compressed chunkA, i.e.,fhaA[t'F].  FtFF The algorithm merges \ntogether all the different messages in F FF. At step 2, we .nd tm{'t, which is the smallest candidate \nin the merged master-family. Matrixe(of size nk) is the dispatching matrix of the types Tand the master-family \ncollection {F'eaeacaFk'}. The search then continues with t', to .ndoht{lt', the smallest candidate in \nF, the original family. Each matrix AtF(of size FF'nFF) is the dispatching matrix of the types in FF'and \nthe family collection F FF. To understand the space saving, consider just two families Fand F. The naive \nimplementation of dispatch is using two arrays, l each of size nT, which map each type tto two types \ntEnF ntFfhlland thEF, such that iF/hdispatch(ttF,i1)2. A more compact representation can be obtained \nby using a single array of size n, to dispatch .rst on the merged master-family F'=FlF. Let t'EF'be the \nresult of this dispatch. The crucial point is that the smallest candidate for t', in either Flor F, is \nthe same as for t. Since there are F'atFFdifferent values of t', a continued search from t'(for either \nlFlor F) can be implemented using two arrays, each of size F'. The .rst such array maps F'to F; the second \nto F. Total memory used is n2F'instead of 2nlcells, while the cost is an additional dereferencing operation. \nMore generally, given a dispatching problem for a family collec\u00adtion F, the CT reduction partitions Finto \nkdisjoint slices F=Flaea aF(2.2) Fk and merges together the families in each slice by de.ning a master\u00adfamily \nF'F(2.3) F=Um for all i=1ea aeak. Let AFFbe the matrix whose dimensions are FF'nFF(2.4) th corresponding \nto the islice. Then, the query dispatch(Fat,is realized by the fetch AF[dispatch(FF'atoF(2.5) F] where \nFFEFFF. Since both steps 2 and 3 in the dispatching are in essence a dis\u00adpatching operation, better compaction \nof the dispatching data struc\u00adture might be achieved by applying the CT technique recursively to either \nthe matrix e, or all the matricesAF. It is not dif.cult to see that each of the recursive applications \nwill yield the same dis\u00adpatching data structure, in which the set of selectors is organized in a three-level \nhierarchy of partitions: families, master-families, and master-master-families (so to speak). We chose \nto describe this 3\u00adlevel system by applying the CT technique to the matrix e. The (potential) saving \nin space comes at a cost of another dereferenc\u00ading step during dispatch. Clearly, we could recursively \napply the reduction any number of times. We need the following notation in order to optimize these recur\u00adsive \napplications. Let y ed,nv oml ,\u00a3denote the memory required for solving the dispatching problem of ntypes, \nmfamilies and \u00a3 method implementations, using ddereferencing operations during dispatch. A simple dispatching \nmatrix representation gives y el,nv om,\u00a3)n ma(2.6) Each application of the CT reduction adds another \ndereferencing, while reducing a dispatching problem with parameters te} ,nv oml ,\u00a3to a new dispatching \nproblem with parameters t,nko\u00a3'}, where kkmm \u00a3'LFF'LUFaFi lFi lU m  Note that \u00a3't\u00a3. To see this recall \nthat \u00a3LFLFi k l) bLtF and apply the fact that the cardinality of the union of sets is at most the sum \nof cardinalities of these sets \u00a3' LFi k lmUbtFmtLFi k l) bLtF\u00a3,a(2.7) m m The reduction generates the \nmatrices AeacaeaiAk. To estimate l their size suppose that all slices are equal in size, i.e., they all \nhave x families. (For simplicity we ignore the case that mis not divisible by x, in which slices are \nalmost equal.) Then, the total memory generated by the reduction is kkk FFL'':xbaFiFiFi LF F'nFLF'uxxy=xFF+x\u00a3t\u00a3 \nlll To conclude, the costs of the CT reduction are another deref\u00aderencing and an additional space of \nx\u00a3. In return, a dispatching problem with parameters t\u00a3}is reduced to a new dispatching ,nv oml problem \nwith parameters tk\u00a3'}, where kmh//xand \u00a3't\u00a3 ,nv . Formally, y e d,n,ml ,\u00a3t\u00a3xy ed,nv omh//xa ,\u00a3(2.8) +\u00a3 \nlwhere xis arbitrary. Let CTdbe the dispatching data structure and algorithm obtained by applying the \nCT reduction d1 d-times to the original dispatching problem. The recursion is ended by applying simple \ndispatching matrix at the last step. Thus, CTis simply the dispatching matrix, l while CT is similar \nto Vitek and Horspool s algorithm (with x w 1). By making dd-=1substitutions of (2.8) into itself, and \nthen using (2.6), we obtain y e d,nv ,m\u00a3t\u00a3+. . ., xdn m(2.9) \u00a3x\u00a3l-alxx. . . xd l-.l th where xFis the \nslice size used during the iapplication of the CT reduction. Symmetry considerations indicate that the \nbound in (2.9) is minimized when all xFare equal. We have, n m y e d,nv oml ,\u00a3t,dd-1\u00a3xd-.l(2.10) s\u00a3x \nd which is minimized when xr ,n mh/\u00a3l. Table 1 summarizes the space and time requirements of algo\u00adrithms \nCTd, where v r ,nmo/s\u00a3is the optimal compression factor. CTN/A _f _ No CTlNo 22\u00a3f o NNCT i 33\u00a3fi i \u00a3\u00a3 \nCTdNf dd fNf df\u00a3\u00a3     CT m)f p p lo( lo Table 1: Generalized CT results for single-inheritance \nhierarchies The last row in the table is obtained by applying the CT reduc\u00adtion a maximal number of times. \nIn each application the slice size is x(typically, x=2). The collection Fis then organized in a hierarchy \nof lgmlevels, which is also the number of derefer\u00adencing steps during dispatch. The memory used in each \nlevel is \u00a3 \u00a3x (see (2.8)). The generalizations (Table 1) of CTdover Vitek and Horspool s algorithm is \nin the following directions: (i) a sequence of algo\u00adrithms which offer a tradeoff between the size of \nthe representa\u00adtion and the dispatching time, and (ii) precise performance anal\u00adysis, which dictates \nan optimal slice size, instead of the arbitrary w universal recommendation, x=1. In re.ecting on the \ngeneralized CT algorithm we see that they are readily adapted to the case where message not understood \nare allowed as is the case in dynamically typed languages. Whenever the search in a master-family F'returns \n, we can be certain that the search in every constituent of F'will also return . Therefore, it is possible \nto check after each dereferencing operation whether the fetched type is , and emit the appropriate error \nmessage. A more appealing alternative is to continue the search with , using an array which maps into \nitself for each constituent of F'. Now, since this array does not depend on the identity of F', we can \nstore only one such copy for each application of the CT reduction. The memory toll that CTdbears for \nthese arrays is ,dd-1s)xcells. Note also that Vitek and Horspool s idea of using selector color\u00ading [4,12] \nin each chunk is still applicable with a slight variation to our generalization. If certain columns in \na chunk contain many elements, it might be possible to collapse these columns together. 3. Incremental \nvariants for Single-Inheritance hierarchies This section describes an incremental variant of the CT scheme \nin the single-inheritance setting, achieving two important proper\u00adties: (i) the space it uses is at most \ntwice that of the static algo\u00adrithm, and (ii) its total runtime is linear in the .nal encoding size. \n(We cannot expect an asymptotically better runtime since the algo\u00adrithm must at least output the .nal \nencoding.) Section 3.1 describes ICT , the incremental variant of CT . Sec\u00adtion 3.2 gives the generalization \nfor CTd. The main idea is to rebuild the entire encoding whenever the ratio between the current slice \nsize and the optimal one reaches a high-or low-water mark (for example 2 and 1/2). Therefore, some insertions \nwill take longer to process than others. We therefore obtain bounds on the amortized time for an insertion.3 \nThe amor\u00adtized time of an insertion is asymptotically optimal since the total runtime is linear in the \n.nal encoding size. Using techniques of background copying [3], it is possible to amend the algorithms \nso that the worst case insertion time is optimal as well. Note that unlike the static version of the \nproblem, we cannot assume that the families always include the root . The reason is that this assumption \nwould require to include implementation of all families, and the initial value of the number of families \nwill jump to m. 3.1 ICT in a single-inheritance setting The CT scheme applies a single CT reduction and \nuses a dis\u00adpatching matrix for the resulting master-families. This process di\u00advides the dispatching problem \ninto independent sub-problems: one dispatching matrix, and a set of matrices AtF, i1aeaeak, which (in \na single-inheritance setting) are in fact dispatching matrices as well. We .rst note that it is relatively \neasy to maintain a plain, single\u00adlevel, dispatching matrix subject to type insertions. The cost is in \nan We remind the reader that the amortized time of an operation is c,n., if a sequence of nsuch operations \nrequires at most n,n. nctime. The worst case time of any single operation can however be much greater \nthan c,n.. For more information on amortized com\u00adplexity see [13]. additional comparison to guard against \narray over.ows. Consider a newly added type t. The row of tin the dispatching matrix is identical to \nthe row of its parent, except for entries corresponding to families in which tis a member. Note that \nthe insertion time of a type is linear in its row size, and the total runtime is therefore linear in \nthe .nal encoding size. There is a slight dif.culty when tintroduces new families. (A new type tintroduces \na family F, tEiF, iff no other type was a member of F.) Observe that the dispatching result for such \na newly introduced family and every other type is always hn ul l. Therefore, we extend the row of tand \nplace the entries for these new fami\u00adlies at the end. However, instead of extending all the other rows \nwith hn nl lentries, we perform a range-check before accessing any given row. In the case of array-over.ow \nwe return hn ul l, otherwise we proceed as usual. The space requirement of CT in a single-inheritance \nsetting is (see Table 1) y cx)\u00a3\u00a3xn mh//xa which is minimized when the slice size is xOPT n mh/\u00a3ba (3.1) \n(3.2) Algorithm ICT will maintain the following invariant xOPT2t:x t 2/xOPT (3.3) and will rebuild the \nencoding whenever this condition is violated. Algorithm 1 shows the procedure to apply whenever a new \ntype is added to the hierarchy. 1: Let xbe the current slice size. 2: Let t ,\u00a3be the current problem \nparameters. ,nv ome} 3: xOPT n m/s\u00a3// The optimal slice size. OPT 4: If not txt2xOPT then 5: x xOPT 6: \nRebuild the entire CT encoding 7: end If 8: Insert tto the CT encoding Algorithm 1: Insertion of a new \ntype tin ICT Substituting (3.2) in (3.1) we .nd the optimal encoding size y exOPT ) =20nm\u00a3ba Let us write \nthis as a function of the problem parameters, ,nv ,ml\u00a3= y exOPT ) =20n m\u00a3ba and study the properties \nof this function. FACT FACT 3.1. Functionis monotonic in all three arguments nv ,ml ,\u00a3.  3.2. There \nare constants ccc, such that l LFi a(2nFoml ,\u00a3n t:cl ,nv ,ml ,\u00a3 fL L (nvm2,\u00a3 :ct ,nv ,ml ,\u00a3(3.4) F Fi \nfL LFin,ml2 ) fL f \u00a3t:c ,nv ,ml ,\u00a3)a F PROOF. Note that LFi ( 2nFom,\u00a3 L0L Fi 2nFm\u00a3 fL 0n m\u00a3L 1FFi 2 \nfL t2 0n m\u00a3E 0 ,nv ,ml\u00a3o)a 2-02The proof for parameters mand \u00a3is identical. LEMMA 3.3. The space requirement \nof ICT is at most 2 ,nv ,ml ,\u00a3)a PROOF. From the algorithm invariant (3.3) it follows that y ex)\u00a3\u00a3xn \nmh//x (xOPT t\u00a32xOPT n mh/2 =2\u00a3\u00a3xOPT n mh//xOPT =2y ex) =2 ,nv ml\u00a3)a OPT Our next objective is to prove \nthat the total runtime of ICT is  linear in ,nv om,\u00a3. To do so, we will breakdown the sequence of insertions \ncarried out by the algorithm into phases, according to the points in time where rebuilding took place. \nNo rebuilding occurs within a phase, and all that is required is to maintain several plain dispatching \nmatrices. Hence, the total runtime of the insertions in a phase is linear in the encoding size at the \nend of this phase. The main observation is that rebuilding happens only when at least one of the problem \nparameters is doubled. We distinguish between three kinds of rebuilds, depending on the parameter which \nwas doubled. We then show that the total runtime of rebuilds of the  same kind is linear in ,nv ,ml \n,\u00a3. Formally, phase ibegins immediately after phase in1-, and ends th after the encoding was built for \nthe itime (the last phase ends when the program terminates). Let ,nto ,mFFF}, i) T1aeacad , o\u00a3, be the \nproblem parameters at the end of phase i. Observe that the problem parameters can only increase, i.e., \nnmF+, lnF, mF+l a mF, and \u00a3F+\u00a3F, Phase i.nishes with an encoding size of at l most 2,nFF, therefore \nits runtime is linear ino om, ,\u00a3sF Thus, the total runtime is linear in L,nFo ,mF, ,\u00a3sF)aFi lWe need \nto show that this sum is linear in ,nom ,n, omFF, ,\u00a3F. (3.5) \u00a3. LEMMA 3.4. Invariant (3.3) is violated \nonly when at least one of the problem parameters is doubled, i.e., one of the following holds nF+l 2noF \nmF+ll 2mF(3.6)\u00a3sF+l2s\u00a3sFa l PROOF. Let x denote the slice size at the beginning of phase , i.e., z x \nns j \u00a3ma(3.7) At the end of phase ione of the following conditions must hold xF+2xF xF+ll t21xFa(3.8) \n From (3.7) and (3.8), we have w nF+mF+nFmF ll\u00a3F+\u00a3F l (3.9) nF+mF+nFmF lltwa \u00a3F+\u00a3F l Since the problem \nparameters can only increase, w nF+mF+nFmFo l l lw(3.10) \u00a3F+\u00a3F l which implies that at least one of the \nparameters was doubled. LEMMA 3.5. The total runtime of ICT is linear in ,n,m,\u00a3)a PROOF. Let {(Nl,Ml,Llcaeaea(N \n,M ,L }be the prob\u00adlem parameters of phases where nFl2F was doubled, i.e., N+2N. l Therefore, N 22N \n-alaeaea2-alNla(3.11) Using Fact 3.2, the total runtime of these phases is linear in L(NF,,MF,,LFtL(NFo,M \n,L FiFi l l N (3.12) tL2 ,M ,L ) - Fi E0l(N ,M ,L s o)a The same consideration applies to phases in \nwhich the number of methods or the number of families was doubled. So, the runtime of the entire algorithm \nis the total runtime of the three kinds of phases, which is linear in ,nomo\u00a3.  3.2 ICTdin a single-inheritance \nsetting The generalization to d2 d>is mostly technical, as outlined next. Function y ex, the space requirement \nof CTdas de.ned in (2.10) is minimized when the slice size is xOPT fn mh/\u00a3ba Let function ddenote the \noptimal encoding size d,nv ,ml\u00a3= y exOPT )d\u00a30f Algorithm ICTdwill preserve the following invariant xOPT \nXd txt2xOPT a(3.13)2l-al LEMMA 3.6. The space requirement of ICTdis at most 2d,nv om,\u00a3)a PROOF. Similar \nto that of Lemma 3.3 FACT 3.7. There are constants ccc, such that l LFi0Ld(2nFoml ,\u00a3nt:cld,nv om,\u00a3 Ld(nvmF,\u00a3:ctd,nv \nom,\u00a3(3.14)Fi0L2 \u00a3 Ldn,mlF)t:cd,nv om,\u00a3)aFi0L2   LEMMA 3.8. Rebuilding only takes place when at least \none of the problem parameters is doubled. PROOF. Similar to that of Lemma 3.4 LEMMA 3.9. The total runtime \nof ICTdis linear in d,nv oml ,\u00a3)a PROOF. Similar to Lemma 3.5.  4. Generalization of Compact Dispatch \nTables for Multiple-Inheritance Hierarchies This section explains how to generalize the CT reduction \nas de\u00adscribed in Section 2 to the multiple-inheritance setting. In a single\u00adinheritance hierarchy, there \ncould never be more than one most spe\u00adci.c family member in response to a dispatch query. The fact that \nthis is no longer true in multiple-inheritance hierarchies makes it dif.cult to apply the CT reduction \nto such hierarchies. Even if the original families are appropriately augmented to remove all such ambiguities, \nambiguities may still occur in the master-families as they are generated by the reduction. We will therefore \nuse a notion of a generalized dispatching query, denoted g-dispatch(Fat,, which returns the entire set \nof smallest candidates, rather than hn ul lin case that this set is not a singleton. Formally, g-dispatch(Fato= \nyi hcand(Fato o)a(4.1) Generalized dispatching is a data-structure transaction rather than an actual \nruntime operation which must result in a single method to execute. Consider for example the hierarchy \nof Figure 4.1. a b  a b Figure 4.1: A small example of a multiple-inheritance hierarchy with two \nfamilies The .gure shows two families of methods, Fa and Fb, Fa ={AB} (4.2) Fb ={AC} a The dispatching \nmatrix of these two families is depicted in Fig\u00adure 4.2a. Note that the results of all dispatching queries \non types D and E (for example) are the same. The corresponding rows in the table are identical and can \nbe compressed. Figure 4.2b shows a rep\u00adresentation of the dispatching matrix obtained by merging together \nall identical rows and an auxiliary array of pointers to all different rows specimens. This compressed \nrepresentation can be understood in terms of the master-family F'=Fa F={ABC} a b The auxiliary array \nrepresents all the possible results of a general\u00adized dispatch on this master-family. For example, g-dispatch(F'D)g-dispatch(F'E) \n={BC}{a a b A  A a b A a b B {A} B B {B} C C C D {C} E D D B C E {B,C} E F F F (a) (b) (c) Figure \n4.2: (a) The dispatching matrix of Figure 4.1, (b) the matrix com\u00adpressed using an auxiliary array of \npointers, and (c) the matrix compressed using an array of set-labels Therefore, the D and E entries in \nthe auxiliary array point to the same row specimen whose label is the set {BC}. In total there are four \ndifferent results of generalized dispatch\u00ading with respect to F'. Family F'therefore partitions the types \nin the hierarchy into four sets, as shown in Figure 4.2c. The .gure shows the same compressed representation \nof the dispatching ma\u00adtrix, where the results of generalized dispatch are used to label row specimens \ninstead of pointers in the auxiliary array. In order to derive bounds on the quality of the CT compression \nin the multiple-inheritance setting we need to estimate the number of distinct rows in chunks. The dif.culty \nis that the result of a generalized dispatch is a set rather than a singleton, and hence this number \nmight be exponential in the family size. To show that this is not the case, we .rst de.ne the notion \nof a partition imposed by a family, and then show the size of this partition is at most 2Itimes the size \nof the family, where 1tItnis a (usually small) metric of the complexity of the hierarchy. 4.1 Family \nPartitionings Given a partially ordered set of types Tand a family of imple\u00admentations FT, the partitioning \nof Tby F, also called the family partitioning due to F, is F={Tlcaeaea Tn} such that all types in partition \nTFhave the same generalized dis\u00adpatch result. In other words, types ETare in the same parti\u00adtion TFE \nFif and only if g-dispatch(F )g-dispatch(Fh )a(4.3)  (a) (b)    f ' (c) Figure 4.3: The family partitionings \nof the familiesa,b of (4.2) and their master-familyab Types D and E, for example, are in the same partition \nin F' since g-dispatch(F'D g-dispatch(F'E {BC}. The partitionings are F={ {ACF}c{BDE}b} a Fb ={ {AB}){CDEF}b}(4.4)F'={ \n{A}c{B}c{CF}c{DE} }{a b of Figure 4.3 In comparing Figure 4.3c with Figure 4.4, we see that the parti\u00adtioning \nF'can be obtained by a simple overlay of the two parti\u00adtionings Fa and Fb. We will next prove that this \nwas no coinci\u00addence. Given two partitionings , ', their overlay m.h 'is the coarsest partitioning consistent \nwith bothand. Constructively, the over\u00ad ' lay is obtained by intersecting all partitions ofwith all partitions \nof ': m. '={,TF{nT'eT)EF T' E'}{a(4.5) For example, the overlay of Fa and Fb of (4.4) is F. Fb ={ {ACF} \nn {AB}){ACF} n{CDEF} a {BDE}\u00a3n {AB}){BDE} n{CDEF}b}={ {A}){CF}c{B}){DE} } a (4.6) LEMMA 4.1. Fl. F (FlFfor \nall Fl, F. PROOF. Itisawellknownfactthatforeverypartitioningthere is a binary equivalence relation whose \nset of equivalence classes are the same as the partitioning. Instead of proving that the par\u00ad titioning \n(FFand F. Fare equal, we will prove that their equivalence relations are the same. l l On the one \nhand, typesare in the equivalence relation of (FlF iff they have the same generalized dispatching results \nwith respect to FlF(see (4.3)), i.e., g-dispatch(FF eg-dispatch(FF )a(4.7) ll On the other hand, the \noverlay partitioning, F. F, is de.ned by intersecting all partitions of Fwith those ofl F(see (4.5)). \nl Therefore, types are in the equivalence relation of F. F iff the following two conditions hold g-dispatch(F \n)g-dispatch(F g-dispatch(Fl )g-dispatch(Fl )a(4.8) We must show that (4.7) holds iff (4.8) holds. Formally, \nusing the de.nition of generalized dispatch (4.1), we must show that yi hcand(FlF o) = yi h.cand(FlFw \no (4.9) yi h.cand(Fl o) = yi h.cand(Fl o yi h.cand(F o) = yi h.cand(F o)a Since two sets of candidates \n(for the same family) have the same smallest elements iff they are equal, our objective is to prove (see \nthe de.nition of candidates in (1.2)) (FlFn,hn e c atb)r(FlFnbhn c e atbs (4.10) Fln,hn e c atb) =Fln,hn \ne c atbFn,hn e c atb) =Fn,hn e c atb)aGiven two sets X , their symmetric difference is de.ned asX hX \nt hXn )a  Observe that) n X) n ) lnhX ) na(4.11) By combining (4.10) and (4.11) we .nd that we need \nto prove that (FlFln( bhu e e atbs = bhu e e atbo) (4.12) Flnbs bo) l( bhu e e at= bhu e e atFnbs bo) \nna l( bhu e e at= bhu e e at The above trivially holds since for all sets X ) ,hX n) Xn  ) na n  4.2 \nMemory requirements of the reduction As in the single-inheritance version, the CT reduction partitions \nF into disjoint slices FlaeaeaF, and generates for the ith slice the Fk master-family FF'by merging the \nfamilies in this slice. To answer the generalized dispatching query g-dispatch(Fat,, where FNEFF, we \n.rst (recursively) answer the query g-dispatch(FF'at,, in the collection of master-families, {,Fl'aeaeaFk'}. \nThis recursive call returns one of the partitions of FF'. The next step is to .nd the unique containing \npartition of F. To understand this better, recall that FFFF'. To apply Lemma 4.1 note that there exists \na set Xsuch that FF'=FX, and hence FF'(FX)F.X a Therefore, every partition of FF'is contained in a partition \nof F. A matrix AFwith FF'rows and FFcolumns is used to map each of the partitions of FF'to a partition \nof F, for all FE FFF. Matrices Aca aeaiAkare nothing other than the dispatching data l structure of the \nCT reduction. (Clearly, there is an additional data structure which the recursive call uses.) To bound \nthe size of these matrices, we need to estimate F. In single-inheritance, FF. An easy, but not so useful \nbound in multiple-inheritance, is Ft=2n . A better bound is given by de.ning I, the complexity of a hier\u00adarchy, \nand then showing that Fat2IFa(4.13) Using slices with xfamilies in each, the total memory of matri\u00adces \nAeaeacaiAkis l kkk 'FF, tt,a LFFnFLFF'uxx:xL2IFF'=2/xI\u00a3 FiFiFi lll The recursive equations then become \ny cl,nv oml\u00a3)n ml (4.14) y ed+,nv oml\u00a3t2I\u00a3. xg y cd,nv omh//xa ,\u00a3)a By using 2I\u00a3linstead of \u00a3, the analysis \nof the previous section holds. COROLLARY 4.2. Let ,n mh o/2I\u00a3. In a hierarchy whose complexity is I, \nCTdperforms dispatching in ddereferencing op\u00add erations, and reaches a compression factor of at least \ndllo-al d (when using a slice size ofl).  In other words, in a hierarchy whose complexity is I, the \nspace requirements of CTdin the multiple-inheritance setting is worse than the single-inheritance setting \nby a factor of at most 2Ilo-ald . 4.3 Hierarchy Complexity DEFINITION 4.3. The complexity of a hierarchy \nis the minimal number Isuch that there exists partitioning of Tinto sets TeaeaeaTl , l and an ordering \nFof TF, i=1aeaea,I, such that for every type t ET, the setn e e ehh n bh{t(ton TFis an interval in F. \nClearly, the complexity of a hierarchy is 1 if there exists an order\u00ading of Tin which descendants of \nany type de.ne an interval. All single-inheritance hierarchies have complexity 1 since in a simple preorder \nthe descendants of any type are consecutive.     Figure 4.5: An example of a multiple-inheritance \nhierarchy of complexity 1 Figure 4.6 shows the family partitioning of the family F{ABE}in the hierarchy \nof Figure 4.5. Observe that F=5.  in the hierarchy of Figure 4.5 Since the complexity of this hierarchy \nis 1, the descendants of each type de.ne an interval. Therefore the familyde.nes the three intervals \ndepicted in Figure 4.7. Those intervals partition the types into 5 segments. (We will show that there \nare at most s t segments.) Types in the same segment have the same set of candi\u00addates and therefore belong \nto the same partition. So we conclude that the number of partitions is at most the number of segments, \nto running the experiments, all degenerate families, i.e., families of which in turn is at most n t . \nIn our example,size one, were pruned from the input. The reason for doing so is t n= Tso =n t that sending \na message whose family is degenerate requires no dis\u00ad  patching, and is the same as static procedure \ncall. (In dynamically typed languages there is an earlier step, which is equivalent to a  subtyping \ntest, in which it is made sure that the message is valid for the receiver type.) We stress that by eliminating \ndegenerate families, compression becomes more dif.cult for the CT schemes. The reason is that this pruning \nreduces both mand eby the same number. Therefore, the optimal compression factor L b((nemh/'e, which \nwe aimed at reach\u00ading, becomes smaller. On the other hand, the compression factor ofin g , -elimination \nschemes ((nemo/2may or may not decrease. Table 2 gives the essential properties of the pruned hierarchies. \nThe .rst two row blocks in the table correspond to single-inheritance (SI) and multiple-inheritance (MI) \nhierarchies. The last block is for hierarchies drawn from multi-dispatch languages. (We regard each multi-dispatch \nquery as several independent single-dispatch queries on each of the arguments, as done in the .rst step \nof the major al\u00ad gorithms for multi-dispatching [18].) PROOF. We need the following fact, whose proof \nis elementary. A set of fintervals partition any consecutive set into at most 21fn+ AA segments. Out \nof these segments at most 21fTiare con- Let f =. Recall (De.nition 4.3) the partitioning of Tinto t sets \nTe1'I.I,Twith their respective ordering. Let iwrite the list of members of the set TTi, enumerated in \nits respective tained in one interval or more. (See illustration in Figure 4.7.) be .xed. We     \nVisualworks1 774 1,170 0.91 79.14 4.62 1 Visualworks2 1,956 3,196 6.25 289.67 13.58 1 Digitalk2 535 962 \n0.51 72.27 3.33 1 order i. Digitalk3 1,357 2,402 3.26 362.11 9.44 1 Consider a type ttEETTi. The result \nof g-dispatch(FI,t is uniquely determined by the subset of all types t, iE IBM Smalltalk 2 2,320 4,335 \n10.06 204.97 16.29 1 , such that Single-InheritanceMultiple-Inheritance Multiple-Dispatching VisualAge \n2 3,241 6,529 21.16 594.98 26.21 1 thetis among the descendant of t . From De.nition 4.3, we have NextStep \n311 499 0.16 16.24 2.12 1 that the descendants are consecutive in the list of Ti. Family de\u00ad.nes therefore \nfintervals partition the list into at mostyfb+e1 ET++ 371 296 0.11 12.20 1.41 1 SI: JDK 1.3.1 6,681 4,392 \n29.34 128.26 23.82 1 intervals (which may be empty) in this list. These SI: Corba 1,329 222 0.30 6.94 \n2.59 1 segments such that the (FI,tt is uniquely determined by the segment SI: HotJava 644 690 0.44 23.86 \n2.91 1 t result of g-dispatch t to Tbi. SI: IBM SF 6,626 11,664 77.29 287.38 88.28 1 s . These segments \ngive the restriction of lc(nf +f1' . of SI: IBM XML 107 131 0.01 1.30 0.59 1 We have thus obtained SI: \nOrbacus 1,053 980 1.03 18.66 3.82 1 To obtain a tighter SI: Orbacus Test 579 368 0.21 5.67 2.39 1 bound \nwe need a more careful counting. Let us remove from TTi SI: Orbix 1,278 535 0.68 10.90 2.90 1 all types \nwhich are not descendants of any of the members of . Self 1,802 2,459 4.43 234.04 21.75 4 The remaining \ntypes are divided by into yft1segments. Gen- Unidraw 614 360 0.22 8.11 2.33 2 eralized dispatching on \nthe removed types returns the empty set, LOV 436 663 0.29 14.09 2.84 12 irrespective of i. The total \nnumber of equivalence classes in is therefore lc(yf .11 e+t1s2lmf Geode 1,318 1,413 1.86 122.27 9.52 \n19 MI: JDK 1.3.1 7,401 5,724 42.36 140.91 28.68 11 . l MI: Corba 1,699 396 0.67 13.58 3.20 7 We are unaware \nof any non-exponential method for .nding . Instead we use a greedy heuristic which gives an upper-bound \non l. MI: HotJava 736 829 0.61 24.90 3.40 8 MI: IBM SF 8,793 14,575 128.16 390.35 116.15 13 On a benchmark \nof 19 large multiple-inheritance hierarchies, the median value on that bound was 5, the average was 6.4, \nand the MI: IBM XML 145 271 0.04 2.33 0.95 3 MI: Orbacus 1,379 1,261 1.74 24.82 5.00 5 maximum was 18. \nMI: Orbacus Test 689 379 0.26 7.49 2.75 5 MI: Orbix 2,716 786 2.13 22.44 3.70 4 REMARK 4.1. The actual \npartitioning T 1 I'.ItTis not re\u00adquired in order to apply the CT reduction; only the integer k Cecil \n932 1,009 0.94 72.89 4.21 7 is needed for determining the slice size. We found that in practice the single-inheritance \nanalysis closely models even hierarchies which use multiple-inheritance heavily. (Therefore there is \nno need even to .nd k.) Dylan 925 428 0.40 70.38 1.78 3 Cecil\u00ad 473 592 0.28 16.06 2.36 5 Cecil2 472 131 \n0.06 17.17 0.56 6 Harlequin 666 229 0.15 23.11 1.02 8 Vor3 1,660 328 0.54 15.44 1.86 7 Vortex3 1,954 \n476 0.93 305.50 2.50 7 5. Experimental Results In this section we compare the theoretical prediction \non the al\u00adgorithms with their empirical performance. Our benchmark com\u00adprises 35 hierarchies totaling \n63,972 types, 70,680 messages and 418,839 methods. Out of these, there were 16 single-inheritance hierarchies \nwith 29,162 types, 12 multiple-inheritance hierarchies with 27,728 types, and 7 multiple-dispatch hierarchies \nwith 7,082 types. This data-set includes all hierarchies previously used in the lit\u00aderature in benchmarks \nof dispatching algorithms. However, prior Table 2: Essential parameters of the pruned hierarchies in \nour data-set The .rst two data columns in the table give the values of n and mfor each of the hierarchies \nin the data-set. We see that the hierarchies span a range of sizes: the number of types is be\u00adtween 107 \nand 8,793 while the number of messages is between 131 and 14,575. A more detailed description of the \ndata-set, includ\u00ading the source of the hierarchies and their respective programming languages is available \nelsewhere [18]. ne The column entitled 1 0;gives the memory requirement of the dispatching matrix, measured \nin millions of cells. We see that this matrix can be huge. Suppose that each cell uses four bytes (an \nassumption we make henceforth), then this matrix consumes about 160MB of memory in the MI: JDK 1.3.1 \nhierarchy and about 500MB in the MI: IBM SF hierarchy. The next column in the table, entitled 1 0 3w, \ngives the number of non-null entries in the dispatching matrix, measured in thousands. The column indicates \nthat this matrix is sparse: In most cases, 90% or more of its cells are empty. We shall use this column \nas a baseline for comparison of the CT algorithms, since it shows the memory requirement of an optimal \nin g ,-elimination scheme such as VFT on single-inheritance hierarchies. Note that in hierarchies such \nas MI: JDK 1.3.1 and MI: IBM SF the potential compression is by a factor of 300 or more. But still, the \nVFTs may consume a lot of space: 1 2MB on some single-inheritance hierarchies. The column entitled 1 \n0C3gives the number of method implemen\u00adtations, which ranges between 562 and 116,152. This column also \nsets a lower bound on the memory used by an optimal duplicates\u00adelimination compression scheme. Comparing \nthis column to the previous one, we learn that duplicates-elimination is potentially much better than \nnull-elimination. However, it is much more dif.\u00adcult to come close to optimal duplicates-elimination \nthan to optimal null-elimination. We shall use this column as another comparison standard for the performance \nof the CT algorithms. The .nal column entitled lshows an upper bound on lwhich was found by our greedy \nheuristic. (Recall that we do not have an ef.cient algorithm for computing l.) In single-inheritance \nhierar\u00adchies, l=l=1. The median of lin the remaining hierarchies is 7. The hierarchy whose topology seems \nto be the most complex is Geode, followed by MI: IBM SF , LOV and then JDK 1.3.1. The implementation \nof the various CT schemes was run on 900Mhz Pentium III computer, equipped with 256MB internal memory \nand controlled by a Windows 2000 operating system. On this ma\u00adchine, the runtimes for generating the \nencoding (without actually copying the values into matrices) of the .rst four schemes (CTn through CT5) \nwere 0.7 Sec, 1.4 Sec, 2.1 Sec and 2.9 Sec. Since our data-set included in total 418,839 methods we .nd \nthat the time per implementation is measured in microseconds. For ex\u00adample, we found that the creation \ntime per implementation ranged between 0.3 and 1.7 JSec in CTnin single-inheritance hierarchies (the \nmedian being 0.6 JSec). These times increase in multiple\u00adinheritance hierarchies: the range being 1.1 \nto 6.7 JSec; the median being 2.4 JSec. Figure 5.1 shows the memory used by the .rst four CT schemes \nrelative to the baseline in the 35 hierarchies in the data-set. Mem\u00adory usage of the CT schemes were \nobtained using the empirically found best slice size (which may be different than the prescription of \ncolumn 2 of Table 1). The .gure shows that compared to the optimal in g ,-elimination, n CTis better \nin 6 hierarchies, CT3in 13 hierarchies, CT4in 15 hierarchies, and CT5in 16 hierarchies. In a few cases, \nthe im\u00adprovement is by an order of magnitude from the baseline. We also see that CTnis at most one order \nof magnitude worse than this ide\u00adalized baseline. We can also learn from Figure 5.1 that the incremental \nimprove\u00adment by the series of CT schemes is diminishing. In fact, exam\u00adining the actual memory requirements, \nwe .nd that the median incremental improvements are: CT3over CTn: 44%, CT4over CT3: 18%, and CT5over \nCT4: 8%. This .nding is consistent with the theoretical prediction. The .gure also plots another idealized \nalgorithm, i.e., the opti\u00admal duplicates-elimination scheme, which uses ecells. We see that this ideal \nis about one order of magnitude better than the various CT schemes. Finally, we see a certain correlation \nbetween eand the series of CT schemes, as predicted by the theoretical analysis. When ethe CT schemes \noutperform even an optimal null\u00ad \u00a3< elimination scheme. n We now turn to comparing the actual performance \nof the various CT schemes with the theoretically obtained bounds. In single-inheritance hierarchies, \nthe upper bound on the mem\u00adory requirement are given by the fourth column of Table 1. Fig\u00adure 5.2a shows \nthe memory requirement relative to these values. We see that in all schemes and in all hierarchies, the \nmemory re\u00adquirement is signi.cantly smaller than the upper bounds. Also, the extent of improvement of \nCTdover the upper bound increases with d. Corollary 4.2 provides the upper bounds in multiple-inheritance \nhierarchies depending on their complexity l. Figure 5.2b shows the memory, relative to these upper bounds, \nof the actual CT per\u00adformance. Again, we see that the extent of improvement of CTd over the upper bound \nincreases with d. Interestingly, in comparing Figure 5.2b with Figure 5.2a, we see that the improvement \nof the implementation upon the upper bound is much greater in multiple\u00adinheritance vs. single-inheritance \nhierarchies. A possible explanation for this seemingly better performance in multiple-inheritance hierarchies \nis exaggerated upper bounds. Examining Corollary 4.2, we see that the upper bounds increase with l. Since \nour heuristics only .nds an upper approximation of l, it could be that the true upper bounds are actually \nsmaller, and hence the improvement upon the upper bound is not as great. Figure 5.2c tries to test this \nhypothesis, by comparing the perfor\u00admance on multiple-inheritance hierarchies with the upper bounds obtained \nby assuming l =1(as in single-inheritance hierarchies).4 We see that the improvement upon the upper bounds \ncomputed thus is almost the same as in single-inheritance hierarchies (Fig\u00adure 5.2a). Such a similarity \ncould not be explained by an overesti\u00admation of l.  of a master-family by the sum of sizes of its constituents, \ni.e., zz zzz z  ziz=s z ULz LUt In fact, especially when the families are large, the probability of \n.nding shared elements may be signi.cant, and the master family is likely to be smaller. As a result, \ne., the number of implemen\u00adtations after the reduction, may be much smaller than the original n value \ne. For example, with X.=a9for CTin Digitalk3, the CT reduction transforms the problem (I,e1}b=2(,113 \nmIa4k0nIt9 4 4 4k} (nLI mto ,1(13 mI,8 3In41}, i.e., the number of implementations decreased by a factor \nof more than 2. Our analysis assumed (see (2.7)) how\u00adever that e=oe. This effect increases also with \nslice size, which is the reason that choosing a slice size greater than the theoretical prescription \nmay improve the performance of the reduction. In IBM SF, for exam\u00adple, the theoretical analysis suggested \nthat XOPT = M3 0as optimal slice size for CTn. However, by using instead a slice size Xo=m0, we were \nable to further reduce the number of cells from 3.3M to about 2.4M. n Figure 5.3 compares the actual \nmemory used by the CTscheme with the theoretical prediction (2.10) in the Digitalk3 hierarchy. (The graphs \nof other hierarchies and higher order schemes are sim\u00adilar.) We see that the extent by which the empirical \nperformance is superior to the theoretically obtained bound increases with the slice size.  2 7 12 \n17 22 27 32 37 42 47 52 57 62 67 72 77 82 87 92 97 slice size n archy of Digitalk3 for CTand its theoretical \nupper bound (2.10) Figure 5.2: The memory requirement of CTn, CT3, CT4and CT5rela\u00adtive to the theoretically \nobtained upper bounds in single-inheritance hierar\u00adchies (a), multiple-inheritance hierarchies where \nthe upper bound was com\u00adputed using K(b), and multiple-inheritance hierarchies, where the upper A bound \nis computed as in single-inheritance hierarchies (K) (c) The reason that algorithms perform better than \nthe theoretically obtained is that the analysis of the CT reduction bounded the size In fact, we used \nthe bound for single-inheritance in Table 1, fo1c1d which is smaller by a factor of than the bound for \nmultiple\u00adinheritance in Corollary 4.2. 6. Conclusions and Open Problems The incremental algorithm described \nin Section 3 is in many ways the pinnacle of this paper. This algorithm assumes the single\u00adinheritance, \ndynamically typed, and dynamic loading model, de\u00adnoted SDTDL. A prime example for the model is the SMALLTALK \nprogramming language. Note that the VFT method is unsuitable in an SDTDL model. Curiously, even though \nJAVA is in essence a statically typed lan\u00adguage, the implementation of the invokeinterface bytecode instruction \nis a very close match of this model. To see this recall that all implementations of a method de.ned in \nan interface must reside in classes, and that these classes take a tree topol\u00adogy. The locations of these \nimplementations in this tree are how\u00adever totally unrelated, and additional implementations can be intro\u00adduced \nas a result of dynamic class loading. Even though there is a possibility of using static information \nof the interface type, many implementations of the invokeinterfacebytecode instruction assume an SDTDL \nmodel. Incorporating the algorithm into a runtime system requires care\u00adful attention to details, including \nselecting a heuristic of determin\u00ading the optimal slice size, which might perform better than the the\u00adoretical \nvalue, a wise strategy for background copy to avoid stag\u00adnation, tweaking and .ne tuning of the partitioning \nalgorithm, etc. We leave this empirical evaluation to continuing research. Also, the incremental algorithm \ncan be generalized to the multiple\u00adinheritance setting, but there are subtle issues in the theoretical \nanalysis of the performance of this generalization. Observe that the static algorithm for multiple-inheritance \nhierar\u00ad .... chies, achieves ylemspace when do=m. Type slicing [18] e(.... however uses only .lecells, \nwhile achieving .nc e(dis\u00adpatching time. There is therefore a reason to believe that the trade\u00adoff offered \nby our technique can be improved, especially for higher values of d. 7. REFERENCES [1] K. Arnold and \nJ. Gosling. The Java Programming Language. The Java Series. Addison-Wesley, Reading, Massachusetts, 1996. \n[2] P. Deutsch and A. Schiffman. Ef.cient implementation of the Smalltalk-80 system. In 11th Symposium \non Principles of Programming Languages, POPL 84, pages 297 302, Salt Lake City, Utah, Jan. 1984. ACM \nSIGPLAN SIGACT, ACM Press. [3] M. Dietzfelbinger, A. R. Karlin, K. Mehlhorn, F. Meyer auf der Heide, \nH. Rohnert, and R. E. Tarjan. Dynamic perfect hashing: Upper and lower bounds. SIAM J. Comput., 23(4):738 \n761, Aug. 1994. [4] R. Dixon, T. McKee, M. Vaughan, and P. Schweizer. A fast method dispatcher for compiled \nlanguages with multiple inheritance. In Proceedings of the 4th Annual Conference on Object-Oriented Programming \nSystems, Languages, and Applications, pages 211 214, New Orleans, Louisiana, Oct. 1-6 1989. OOPSLA 89, \nACM SIGPLAN Notices 24(10) Oct. 1989. [5] K. Driesen. Selector table indexing &#38; sparse arrays. In \nProceedings of the 8th Annual Conference on Object-Oriented Programming Systems, Languages, and Applications, \npages 259 270, Washington, DC, USA, Sept. 26 -Oct. 1 1993. OOPSLA 93, ACM SIGPLAN Notices 28(10) Oct. \n1993. [6] K. Driesen and U. H\u00a8olzle. Minimizing row displacement dispatch tables. In Proceedings of the \n10th Annual Conference on Object-Oriented Programming Systems, Languages, and Applications, pages 141 \n155, Austin, Texas, USA, Oct. 15-19 1995. OOPSLA 95, ACM SIGPLAN Notices 30(10) Oct. 1995. [7] P. Ferragina \nand S. Muthukrishnan. Ef.cient dynamic method-lookup for object oriented languages. In J. D\u00b4iaz and \nM. Serna, editors, Algorithms ESA 96, Fourth Annual European Symposium, volume 1136 of Lecture Notes \nin Computer Science, pages 107 120, Barcelona, Spain, 25 27 Sept. 1996. Springer. [8] M. L. Fredman, \nJ. Koml\u00b4os, and E. Szemer\u00b4edi. Storing a sparse table with .worst case access time. J. ACM, e(,11 31(3):538 \n544, July 1984. [9] J. Y. Gil and P. Sweeney. Space-and time-ef.cient memory layout for multiple inheritance. \nIn Proceedings of the 14th Annual Conference on Object-Oriented Programming Systems, Languages, and Applications, \npages 256 275, Denver, Colorado, Nov.1 5 1999. OOPSLA 99, ACM SIGPLAN Notices 34(10) Nov. 1999. [10] \nU. H\u00a8olzle, C. Chambers, and D. Ungar. Optimizing dynamically-typed object-oriented languages with polymorphic \ninline caches. In Proceedings of the 5th European Conference on Object-Oriented Programming, number 512 \nin Lecture Notes in Computer Science, Geneva, Switzerland, July15 19 1991. ECOOP 91, Springer Verlag. \n[11] S. Muthukrishnan and M. M\u00a8uller. Time and space ef.cient method-lookup for object-oriented programs. \nIn Proceedings of the Seventh Annual ACM-SIAM Symposium on Discrete Algorithms, pages 42 51, New York/Philadelphia, \nJan. 28 30 1996. ACM/SIAM. [12] A. Royer. Optimizing Method Search with Lookup Caches and Incremental \nColoring. In Proceedings of the 7th Annual Conference on Object-Oriented Programming Systems, Languages, \nand Applications, pages 110 126, Vancouver, British Columbia, Canada, Oct.18-22 1992. OOPSLA 92, ACM \nSIGPLAN Notices 27(10) Oct. 1992. [13] D. Sleator and R. Tarjan. Self-adjusting binary search trees. \nJ. ACM, 32(3):652 686, July 1985. [14] B. Stroustrup. The Design and Evolution of C++. Addison-Wesley, \nReading, Massachusetts, Mar. 1994. [15] J. Vitek and R. N. Horspool. Taming message passing: Ef.cient \nmethod lookup for dynamically typed object-oriented languages. In Proceedings of the 8th European Conference \non Object-Oriented Programming, number 821 in Lecture Notes in Computer Science, Bologna, Italy, July \n4-8 1994. ECOOP 94, Springer Verlag. [16] J. Vitek and R. N. Horspool. Compact dispatch tables for dynamically \ntyped object oriented languages. In T. Gyimothy, editor, Compiler Construction, 6... International Conference, \nvolume 1060 of Lecture Notes in Computer Science, pages 309 325, Link\u00a8oping, Sweden, 24 26 Apr. 1996. \nSpringer. [17] O. Zendra, C. Colnet, and S. Collin. Ef.cient dynamic dispatch without virtual function \ntables: The SmallEiffel compiler. In Proceedings of the 12th Annual Conference on Object-Oriented Programming \nSystems, Languages, and Applications, pages 125 141, Atlanta, Georgia, Oct. 5-9 1997. OOPSLA 97, ACM \nSIGPLAN Notices 32(10) Oct. 1997. [18] Y. Zibin and J. Y. Gil. Fast algorithm for creating space ef.cient \ndispatching tables with application to multi-dispatching. In Proceedings of the 17th Annual Conference \non Object-Oriented Programming Systems, Languages, and Applications, pages 142 160, Seattle, Washington, \nNov. 4 8 2002. OOPSLA 02, ACM SIGPLAN Notices 37(10) Nov. 2002.  \n\t\t\t", "proc_id": "604131", "abstract": "A fundamental problem in the implementation of object-oriented languages is that of a frugal <i>dispatching data structure</i>, i.e., support for quick response to dispatching queries combined with compact representation of the type hierarchy and the method families. Previous theoretical algorithms tend to be impractical due to their complexity and large hidden constant. In contrast, successful practical heuristics, including Vitek and Horspool's <i>compact dispatch tables</i> (CT) [16] designed for dynamically typed languages, lack theoretical support. In subjecting CT to theoretical analysis, we are not only able to improve and generalize it, but also provide the first non-trivial bounds on the performance of such a heuristic.Let <i>n</i>,<i>m</i><i>l</i> denote the total number of types, messages, and different method implementations, respectively. Then, the dispatching matrix, whose size is<i>nm</i>, can be compressed by a factor of at most &#953; &#8801; (<i>nm</i>)/<i>l</i>. Our main variant to CT achieves a compression factor of &#189; &#8730;&#953;. More generally, we describe a sequence of algorithms CT<inf>1</inf>, CT<inf>2</inf>, CT<inf>3</inf>,..., where CT<inf><i>d</i></inf> achieves compression by a factor of (at least) <sup>1</sup>over<inf><i>d</i></inf>&#953;<sup>1&#8212;1/<i>d</i></sup>, while using <i>d</i> memory dereferencing operations during dispatch. This tradeoff represents the first bounds on the compression ratio of constant-time dispatching algorithms.A generalization of these algorithms to a <i>multiple-inheritance</i> setting, increases the space by a factor of &#954;<sup>1-1/<i>d</i></sup>, where &#954; is a metric of the complexity of the topology of the inheritance hierarchy, which (as indicated by our measurements) is typically small. The most important generalization is an <i>incremental</i> variant of the CT<inf><i>d</i></inf> scheme for a single-inheritance setting. This variant uses at most twice the space of CT<inf><i>d</i></inf>, and its time of inserting a new type into the hierarchy is optimal. We therefore obtain algorithms for efficient management of dispatching in dynamic-typing, dynamic-loading languages, such as <sc>Smalltalk</sc> and even the <sc>Java</sc> invokeinterface instruction.", "authors": [{"name": "Yoav Zibin", "author_profile_id": "81100037778", "affiliation": "Technion---Israel Institute of Technology", "person_id": "PP36023074", "email_address": "", "orcid_id": ""}, {"name": "Joseph (Yossi) Gil", "author_profile_id": "81100349003", "affiliation": "Technion---Israel Institute of Technology", "person_id": "P149595", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/604131.604143", "year": "2003", "article_id": "604143", "conference": "POPL", "title": "Incremental algorithms for dispatching in dynamically typed languages", "url": "http://dl.acm.org/citation.cfm?id=604143"}