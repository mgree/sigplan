{"article_publication_date": "01-15-2003", "fulltext": "\n Bitwidth Aware Global Register Allocation Sriraman Tallam Rajiv Gupta Department of Computer Science \nThe University of Arizona Tucson, Arizona 85721 {tmsriram,gupta}@cs.arizona.edu ABSTRACT Multimedia \nand network processing applications make extensive use of subword data. Since registers are capable of \nholding a full data word, when a subword variable is assigned a register, only part of the register is \nused. New embedded processors have started sup\u00adporting instruction sets that allow direct referencing \nof bit sections within registers and therefore multiple subword variables can be made to simultaneously \nreside in the same register without hinder\u00ading accesses to these variables. However, a new register allocation \nalgorithm is needed that is aware of the bitwidths of program vari\u00adables and is capable of packing multiple \nsubword variables into a single register. This paper presents one such algorithm. The algorithm we propose \nhas two key steps. First, a combina\u00adtion of forward and backward data .ow analyses are developed to determine \nthe bitwidths of program variables throughout the pro\u00adgram. This analysis is required because the declared \nbitwidths of variables are often larger than their true bitwidths and moreover the minimal bitwidths \nof a program variable can vary from one pro\u00adgram point to another. Second, a novel interference graph \nrepresen\u00adtation is designed to enable support for a fast and highly accurate algorithm for packing of \nsubword variables into a single register. Packing is carried out by a node coalescing phase that precedes \nthe conventional graph coloring phase of register allocation. In contrast to traditional node coalescing, \npacking coalesces a set of interfering nodes. Our experiments show that our bitwidth aware register allo\u00adcation \nalgorithm reduces the register requirements by 10% to 50% over a traditional register allocation algorithm \nthat assigns separate registers to simultaneously live subword variables. Categories and Subject Descriptors \nC.1 [Computer Systems Organization]: Processor Architectures; D.3.4 [Programming Languages]: Processors \ncompilers  General Terms Algorithms, Measurement, Performance  Keywords subword data, minimal bitwidth, \npacking interfering nodes, embed\u00added applications Permission to make digital or hard copies of all or \npart of this work for personal or classroom use is granted without fee provided that copies are not made \nor distributed for pro.t or commercial advantage and that copies bear this notice and the full citation \non the .rst page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires \nprior speci.c permission and/or a fee. POPL 03, January 15 17, 2003, New Orleans, Louisiana, USA. Copyright \n2003 ACM 1-58113-628-5/03/0001 ...$5.00. 1. INTRODUCTION Programs that manipulate data at subword level, \ni.e. bit sections within a word, are common place in the embedded domain. Ex\u00adamples of such applications \ninclude media processing as well as network processing codes [12, 19]. A key characteristic of such applications \nis that at some point the data exists in packed form, that is, multiple data items are packed together \ninto a single word of memory. In fact in most cases the input or the output of an ap\u00adplication consists \nof packed data. If the input consists of packed data, the application typically unpacks it for further \nprocessing. If the output is required to be in packed form, the application com\u00adputes the results and \nexplicitly packs it before generating the out\u00adput. Since C is the language of choice for embedded applications, \nthe packing and unpacking operations are visible in form of bitwise logical operations and shift operations \nin the code. In addition to the generation of extra instructions for packing and unpacking data, additional \nregisters are required to hold values in both packed and unpacked form therefore causing an increase \nin register pressure. New instruction set architectures for embedded and network pro\u00adcessors allow bit \nsections within a register to be directly referenced [6, 15, 18]. For example, the following instruction \nadds a 4 bit value from R2with a 6 bit value from R3and stores a 8 bit result in R1. The operands are \nextended by adding leading zero bits to match the size of the result before the addition is carried out. \nR1o..7.R2o..3+R3o..5 In our recent work we incorporated bit section referencing into the popular ARM \nprocessor. We have shown that the proper use of such instructions eliminates the need for explicit packing \nand unpacking operations and thus reduces the number of executed instructions signi.cantly [13]. Another \nimportant consequence of having this instruction set support is that multiple subword sized variables \ncan be made to simultaneously reside in the same register without hin\u00addering access to the variables. \nThus this approach reduces register requirements of the program. Since embedded processors support a \nsmall number of registers (e.g., ARM [16] supports 16 registers and even fewer are directly accessible \nby most instructions in Thumb mode) ef.cient use of register resources is extremely important. To illustrate \nthe potential for reduction in register requirements, let us consider the examples shown in Fig. 1 that \nare typical of em\u00adbedded codes. These code fragments, taken from the adpcm (au\u00addio) and gsm (speech) \napplications respectively, perform unpack\u00ading and packing. Each code fragment references three variables \nwhich have the declared size of 8 bits each. The live ranges of the variables, including their widths, \nare shown. By examining these live ranges we .nd that a traditional register allocator must use two registers \nto hold their values. However, bitwidth aware register al\u00adlocation can dramatically reduce the register \nrequirements. We can Unpacking inbuffer=... deltal=inbufferxf Packing alphal=... alpha2=... Unpacking \nharinbuffer; hardeltal, delta2; inbuffer delta1 Packing haroutbuffer; haralphal, alpha2; alpha1 alpha2 \nlastuseofdeltal delta2=(inbuffer\u00bb\u00bb4) outbuffer= delta2 xf lastuseofdelta2 (alpha2((alphal x7)I xf)\u00ab\u00ab3)) \n4 4 4 4 4 4 1 7 5 34 4 outbuffer Figure 1: Subword variables in multimedia codes. hold the values of \nthese variables in 8 bits and 7 bits of a single reg\u00adister respectively for the two code fragments. The \nremaining bits of this register can be used to hold additional subword sized variables. In this paper \nwe describe an approach for achieving register al\u00adlocations that use a part of a single register, as \nopposed to multiple registers, for the above code fragments. There are two key compo\u00adnents of our approach. \nFirst, we employ algorithms for constructing live ranges of variables such that the minimal bitwidths, \nor widths for short, of the live ranges at various program points are also com\u00adputed. Second, we employ \na fast and effective method for packing together multiple live ranges. The packing phase essentially \nper\u00adforms coalescing of interfering nodes on an enhanced interference graph representation for the program. \nFollowing packing, register allocation is carried out using a conventional graph coloring algo\u00adrithm \nthat assigns a single register to each node in the graph [1, 3, 8]. There are a number of challenges \nof developing fast and yet effective algorithms for each of the above components that are de\u00adscribed \nbelow: Live range construction The .rst challenge is to identify the minimal width of each live range \nat each relevant program point. Analysis must be developed for this purpose due to two reasons that are \nillustrated by our exam\u00adples: variables are declared to be of larger than needed bitwidths (e.g., delta1 \nis declared as an 8 bit entity while it only uses 4 bits); and the bitwidth of a variable can change \nfrom one pro\u00adgram point to another as a variable may contain multiple data items which are consumed one \nby one (e.g., inbuffer initially is 8 bits of data, after delta1 is assigned it contains only 4 bits \nof use\u00adful data). We present a combination of forward and backward data .ow analysis to .nd the minimal \nwidths. The second challenge is to ef.ciently identify the minimal widths. An obvious way is to develop \nan analysis which, for a given vari\u00adable that is declared to be bbits wide, determines the need for keeping \neach of the bbits in a register at each program point. The cost of such bitwise analysis will be high \nas it is directly dependent upon the bitwidths of the variables. To achieve ef.ciency, we de\u00advelop an \nanalysis which views each variable, regardless of its size, as made up of three bit sections called the \nleading, middle, and trailing sections. The goal of the analysis is to determine the min\u00adimal sized middle \nsection that must be kept in the register while the leading and trailing sections can be discarded. This \napproach is effective in practice because the unneeded bits of a variable at a program point typically \nform leading and/or trailing bit sections. Packing multiple variables into a register When variables \nare packed together through coalescing of two nodes in the interference graph, the shapes of the live \nranges must be taken into account to determine whether or not the live ranges can be co\u00adalesced, and \nif coalescing is possible, the characteristics of the coa\u00adlesced live range must be determined to perform \nfurther coalescing. A simple approach to this problem may not be accurate leading to missed opportunities \nfor coalescing. For example, in our earlier example the maximum width of inbuffer and delta1 were 8 and \n4 bits respectively. A simple method that ignores their shapes and assigns a width of 12 bits to the \nlive range resulting from coa\u00adlescing the two, overestimates the width by 4 bits. Therefore while this \napproach is simple, and thus fast, it will miss coalescing op\u00adportunities. A completely accurate approach \ncan be developed which com\u00adpares the shapes of the live ranges at all relevant program points to determine \nwhether they can be coalesced, and if that is possible, it computes the compact shape of the resulting \nlive range. While this approach will not miss coalescing opportunities, it is too expensive. We present \na fast and highly accurate approach for node coalescing based upon an enhanced labeled interference graph. \nThe shapes of interfering live range pairs are compared exactly once to generate the labelling. Node \ncoalescing is driven by the labelling which is updated in constant time following each coalescing step. \nWhile the labelling is approximate, it is highly accurate in practice and there\u00adfore missed coalescing \nopportunities are rare. Outline The remainder of the paper is organized as follows. In section 2 we present \nthe live range construction algorithm. The enhanced in\u00adterference graph representation and the node coalescing \nalgorithm based upon it to affect variable packing is described in section 3. Experimental evaluation \nis presented in section 4. Related work is discussed in section 5 and conclusions are given in section \n6. 2. LIVE RANGE CONSTRUCTION De.nition 1. (Live Range) The live range of a variable vis the program \nregion over which the value of vis live, that is, for each point in the live range, a subset of bits \nin v s current value may be used in a future computation. Each right hand side reference of variable \nvin some program statement does not need to explicitly reference all of the bits in v during the execution \nof the statement. As a consequence, at each point in v s live range, not all of the bits representing \nvare live. Therefore, different amounts of bits may be needed to hold the value of vin a register at \ndifferent program points. De.nition 2. (Dead Bits) Given a variable v, which according to its declaration \nis represented by sbits, a subset of these sbits, say d,are dead at program point pif all computa\u00adtions \nfollowing point pthat use the current value of vcan be performed without explicitly referring to the \nbits in d. De.nition 3. (Live Range Width) Given a program point p in variable v s live range, the width \nof v s live range at point p, denoted by Wv(p), is de.ned such that the bits representing variable vaccording \nto its declaration can be divided into three contiguous sections as follows: a leading section of lv(p)dead \nbits; a middle section of Wv(p)live bits; and a trailing section of tv(p)dead bits. v l (p) w (p) t \n(p) v vv Let svdenote a statement that refers to the value of variable v. We de.ne (sv,v)as an ordered \npair (l,t)such that the leading lbits and trailing tbits of vneed not be explicitly referred to during \nexecution of sv. The conditions under which only a sub\u00adset of, and not all, bits of a variable vare suf.cient \nfor evaluating an expression are given in Fig. 2. The .rst three situations exploit the use of compile \ntime constants in left shift, right shift,and bit\u00adwise and operations. The results computed by these \nexpressions are only dependent upon subset of bits of vand thus the remaining bits are considered as \nnot having been used. The next two situations ex\u00adploit presence of zero bits in v. Leading zero bits \npresent in vneed not be explicitly held in a register to perform arithmetic and rela\u00adtional operations \nas the results of these operations can be correctly computed without explicitly referring to these bits. \nSimilarly, the results of the bitwise or operation can be computed without explic\u00aditly referring to the \nleading and trailing zero bits of v. Therefore, we consider these zero bits as not having been used. \nFinally, in all other cases a right hand side reference to vis considered to use all bits of v,i.e. (sv,v)is \n(0,0). To identify dead bits and hence the width of the live range at each program point in the live \nrange, we perform the following analy\u00adsis. First we carry out forward analysis to compute a safe estimate \nof leading and trailing zero bit sections in each program variable at each program point. This information \nis needed in the com\u00adputation of (sv,v)in two of the cases described above (arithmetic/relational operations \nand bitwise or operation). Hav\u00ading computed (sv,v)information fully, second we carry out backward analysis \nto identify the dead bit sections in each pro\u00adgram variable at each program point. We describe these \nanalyses next. Without loss of generailty, we assume in our discussion that all variables have the same \ndeclared bitwidth. Leading and trailing zero bit sections As described above, leading and trailing zeros \nneed to be found because results of some operations can be computed without ex\u00adplicitly referring to \nthem and thus these sections can be treated as dead bit sections. Forward analysis is employed to determine \nthe leading and trailing zero bit sections for each variable, at each pro\u00adgram point. When a variable \nvis being assigned, in some cases, by examining the expression on the right hand side we can deter\u00admine \nthe leading and trailing zero bit sections of vfollowing the assignment. In case of a constant assignment \nv=, by looking at the value of constant , we can determine the zero bit sections of v. In case of signed \nnumbers the leading zero bit section is formed by sign extension bits (i.e., from zeros or ones). A right \n(left) shift by a constant amount, i.e. v=x (v=x ), results in the creation of leading (trailing) bit \nsections. A bitwise logical or (and) operation, i.e. v=xIy(v=x&#38;y), results in propagation of zero \nbit sections. For a copy assignment v=x, the zero bit sections of xare simply propagated to v. If nothing \ncan be asserted about the value being assigned to v, the analysis conservatively as\u00adsumes that there \nare no leading or trailing zero bit sections. Note that since zero bit sections of one variable may depend \nupon zero bit sections of another variable, all variables must be an\u00adalyzed simultaneously. The meet \noperator for this forward analysis safely computes the smallest leading and trailing zero bit sections \nthat are present in each variable across all incoming edges. The data .ow equations for computing the \nzero bit sections are given in Fig. 3, where in/out[n,v]represents the zero bit sections of variable \nvat entry/exit of node n. Recall that for simplicity we only list the situations involving variables \nof the same bitwidth. When variables of different size are considered, additional opportunities arise. \nFor example, when an unsigned short integer is assigned to an unsigned long integer, a leading zero bit \nsection is created in the latter. More situations can be found to enhance the analysis. Leading and trailing \ndead bit sections Fig. 3 also gives the data .ow equations for computing the dead bit sections. D in/out[n,v]represents \nthe leading and trailing dead bit sections of variable vat entry/exit of node n. As expected, determination \nof dead bit sections is based upon backward analy\u00adsis which examines each statement svto identify the \nsubset of bits of variable vwhose values are not used by the statement. This in\u00adformation is represented \nby (sv,v)as described earlier, which can be computed for each statement given the results of the zero \nbit sections analysis. If the bit sections in (sv,v) are dead at the point after statement sv, then they \nare also dead im\u00admediately before statement sv. If a statement de.nes vand does not use it, then all \nbits of the variable are dead which is indicated by (T,T), i.e. leading and trailing dead bit sections \nof size equal to the width of the variable. The join operator conservatively com\u00adputes those bit sections \nas dead at the exit of a node that are also dead at entry points of all successor nodes. An example The \nresults of applying the above analyses are illustrated using an example shown in Fig. 4. For simplicity \nwe use a straightline code example although our technique applies to programs with complex control .ow \nstructures. For the given code fragment, .rst we show the zero bit sections of each variable at the point \nit is assigned some value. Next we show the results of the dead bit sections analysis where the set of \ndead variables immediately following each state\u00adment are given. For example, immediately following statement \n3 the higher order 4 bits of variable Dare zero. In case the entire vari\u00adable is dead we simply list \nthe name of the variable (e.g., all of the involved variables are fully dead immediately preceding the \ncode fragment). The results of the dead bit sections analysis are equiva\u00adlent to the live ranges shown \nwhere the area enclosed in solid lines corresponds to the bit section that is not dead. If we examine \nthe above ranges, it is easy to see that the maximum combined width of these live ranges at any program \npoint is 32 bits. Therefore a single 32 bit register is suf.cient to handle all these variables. Note \nthat a traditional register allocator which ignores the widths of the variables will need four registers \nfor this code fragment. sv Characteristics of sv (sv,v) vtvlv&#38; vop...vI... f(v) tis a compile time \nconstant lis a compile time constant is a compile time constant with lleading and ttrailing zero bits \n opis an arithmetic or relational operator; vhas at least lleading zero bits vhas at least lleading zero \nbits and ttrailing zero bits other forms of statements that use v (0,t)-ttrailing bits of vare not used. \n(l,0)-lleading bits of vare not used. (l,t)-lleading bits and ttrailing bits of vare not used. (l,0)-lleading \nbits of vare not used. (l,t)-lleading bits and ttrailing bits of vare not used. (0,0)-all bits of vare \nused. Figure 2: Partial use of a variable s bits. Input: control .ow graph =(N,E,start,end), where each \nnode contains a single intermediate code statement. de.nitions: (hl,tl)/(h2,t2)=(min(hl,h2),min(tl,t2)). \n(hl,tl)V(h2,t2)=(max(hl,h2),max(tl,t2)). (hl,tl)L(h2,t2)=(min(hl,h2),max(tl,t2)). (hl,tl)\\(h2,t2)=(max(hl,h2),min(tl,t2)). \n boundary conditions: for each variable v,Sin[start,v]=Sout[end,v]=(T,T), where Tis the bitwidth of variable \nv. initialization: set all vectors to (T,T)S,where Sis the number of variables.  meet and join operators:is \nthe meet and join operator for the forward and backward analysis respectively. Zero Bit Sections Analysis: \nSolve iteratively  Sin[n,v]=pE (n) (Sout[p,v]).Sout[n,v]=    {    (h,t) ifnisv=;isa+veonstant;and \n(h,t)represents'szerobitsetions (h+,t-)L(T,0)ehseifnisv=x\u00bb\u00bb;Sin[n,x]=(isa+veonstant (h-,t+)\\(0,T)ehseifnisv=x\u00ab\u00ab;Sin[n,x]=(isa+veonstantSin[n,x] \nehseifnisv=xSin[n,x]/Sin[n,y]ehseifnisv=xIySin[n,x]VSin[n,y]ehseifnisv=x&#38;y (0,0) ehseifndefinesvSin[n,v] \notherwise h,h, tt )and )and Dead Bit Sections Analysis: Solve iterativelySin[n,v]= { NSE(n,v)/Sout[n,v]ifnusesv \n(T,T) ehseifndefinesvSout[n,v] otherwiseSout[n,v]= sESu(n) (Sin[s,v]). Figure 3: Forward and backward \nbit sections analysis. intA; 32 bits shortD, E; 16 bits harB, C; 8 bits 1. E=... 2. D=El 3. D=D\u00bb\u00bb4 \n4. A=(E\u00ab\u00ab4)I xf thiswasE s lastuse. 5. useA 6. A=A\u00bb\u00bbl2 7. B=... 8. C=(Bx7f)l 9. lastuseofA 10. lastuseofBx8 \n11. lastuseofC 12. lastuseofD Zero Bit Sections 1. E: (0,0) 2. D: (0,0) 3. D: (4,0) 4. A: (12,0) 5. 6. \nA: (24,0) 7. B: (0,0) 8. C: (0,0) 9. 10. 11. 12. Dead Bit Sections 0. A:B:C:D:E; beforel 1. A:B:C:D 2. \nA:B:C 3. A:B:C:D(4,0) 4. A(12,0):B:C:D(4,0):E 5. A(12,0):B:C:D(4,0):E 6. A(24,0):B:C:D(4,0):E 7. A(24,0):C:D(4,0):E \n8. A(24,0):B(0,7):D(4,0):E 9. A:B(0,7):D(4,0):E 10. A:B:D(4,0):E 11. A:B:C:D(4,0):E 12. A:B:C:D:E 16 \nE 4 12 D 12 A 12 8 1 7 B 8 C Figure 4: Illustration of live range construction. Figure 5: Using registers \nwith packed variables. 1. E=... 2. D=El 3. D=D\u00bb\u00bb4 4. A=(E\u00ab\u00ab4)I xf ; thiswasE s lastuse. 5. useA 6. A=A\u00bb\u00bbl2 \n7. B=... 8. C=(Bx7f)l 9. lastuseofA 10. lastuseofBx8 11. lastuseofC 12. lastuseofD 1. Ro..15=... 2. R16..31=Ro..15 \nl 3. R16..27=R2o..31 R2o..31=R16..27 4. R4..19=Ro..15; Ro..3=xf 5. useRo..19 6. Ro..7=R12..19 7. Rs..15=... \nR16=R15 8. Rs..15=Rs..14 l 9. lastuseofRo..7 10. lastuseofR16 11. lastuseofRs..15 12. lastuseofR2o..31 \n1. Rlo..15=... 2. Rl16..31=Rlo..15 l 3. Rl16..27=Rl2o..31 4. R24..19=Rlo..15; R2o..3=xf 5. useR2o..19 \n6. R2o..7=R212..19 7. R2s..15=... 8. R216..23=R2s..14 l 9. lastuseofR2o..7 10. lastuseofR215 11. lastuseofR216..23 \n12. lastuseofRl16..27 Original code. Code using one register. Code using two registers.  3. VARIABLE \nPACKING = ITERATIVE CO-  ALESCING OF INTERFERING NODES In this section we present our variable packing \nalgorithm. Let us .rst see the impact of variable packing on the generated code. Fig. 5 shows the code \nresulting after packing all variables of Fig. 4 s example into one register R. The subscripts indicate \nthe bit sec\u00adtions within Rbeing referenced. It is clear that if bit section refer\u00adencing is supported, \na small number of registers can be used very effectively. Note that the shift operations of statements \n3, 4, and 6 are translated into intraregister bit section moves which move a se\u00adquence of bits from one \nposition to another. Also two additional intraregister moves, preceding statements 4 and 8, are required. \nThese moves are required because sometimes when a variable that is allocated to the register is being \nde.ned, a free contiguous reg\u00adister bit section of the appropriate size may not be available. This is \nbecause the free bits may be fragmented. In this case the values of live variables present in the register \nmust be shifted to combine the smaller free bit section fragments into one large contiguous bit section. \nThe algorithm we have developed sacri.ces some of the variable packing opportunities in favor of fast \nexecution time. For the pre\u00adceding example, although one register is suf.cient, our algorithms allocates \nA, B, and C to one register and D and E to another reg\u00adister. The resulting code based upon using two \nregisters is shown in the .gure. Since the variables are not packed as tightly, we .nd that there is \nno need to carry out the two intraregister moves for overcoming the problem of fragmentation of free \nbits. Interference graph Our approach to variable packing is to perform it as a prepass to global register \nallocation. The merit of this approach is that exist\u00ading register allocation algorithms can be used without \nany modi.\u00adcations once variable packing has been performed. In addition, we design the variable packing \nalgorithm to operate upon the live range interference graph which must be constructed any way to perform \nglobal register allocation. The nodes of an interference graph corre\u00adspond to the live ranges. Interference \nedges are introduced between node pairs representing overlapping live ranges. It is easy to see that \nfrom the perspective of the interference graph, variable packing can be performed through iterative coa\u00adlescing \nof interfering nodes. In each step a pair of interfering live ranges can be coalesced into one node if \nno place in the program is their collective width greater than the number of bits in the register. After \nvariable packing, register allocation is performed using the transformed interference graph. De.nition \n4. (Maximum Interference Width) Given a pair of live ranges lrland lr2,the maximum interference width \nof these live ranges, denoted by (lrl,lr2),is the maximum combined width of these live ranges across \nall program points where the two live ranges overlap. Let Width(lr,p)denote the width of live range lrat \nprogram point p. (lrl,lr2)is computed as follows: (lrl,lr2)=Width(lrl,p)+Width(lr2,p) iffVnstlrandlroverlapatn, \nl2Width(lrl,n)+Width(lr2,n): Width(lrl,p)+Width(lr2,p). It should be clear that (lrl,lr2)are coalesced \niff (lrl,lr2):IRI,where IRIis the number of bits in each register. We always assume that no variable \nhas width greater than IRI. The desired goal of coalescing can be set as achieving maxi\u00admal coalesing \nthat reduces the number of nodes in the interference graph to the mimimum possible that is achievable \nby any legal se\u00adquence of coalescing operations. However, the theorem we present next establishes that \nachieving maximal coalescing is NP-complete. In fact from our construction it can be seen that this result \nholds true even for straightline code. Theorem (Live Range Coalescing is NP-complete). Given a set of \nlive ranges L, a constant lILI.Does there exist a live range coalesing that reduces the number of live \nranges to lsuch that the width of no coalesced variable exceeds IRIat any program point? Proof. It is \ntrivial to see that live range coalescing problem is in NP as given a solution it is easy to verify that \nit is correct in polynomial time. By performing a reduction from the bin packing problem (see [7], page \n226) we can show that live range coalescing is NP-complete. The bin packing problem can be stated as \nfollows. Given a set of items ,a size s(u)for each uE,and a positive integer bin capacity . Is there \na partition of into disjoint sets l, 2,, Ksuch that the sum of sizes of the items in each set iis or \nless? An instance of bin packing problem can be transformed into an instance of live range coalescing \nproblem as fol\u00adlows. Corresponding to each item uE, we construct a live range of uniform width s(u). \nWe further assume that there is some program point where all live ranges fully overlap with each other. \nNow let IRI=and l=K. If we can .nd a live range coalescing that reduces the number of live ranges to \nlsuch that none of the coalesced live ranges has a width greater than IRI, wehaveessen\u00adtially solved \nthe corresponding instance of the bin pack\u00ading problem. The overall outcome of coalescing depends upon \nthe selection and order in which pairs of nodes are examined for coalescing. Given the above result we \nuse an iterative coalescing heuristic which picks a node from the graph, coalesces it with as many neighboring \nnodes as possible, and then repeats this process for all remaining nodes. Let us brie.y consider the \nruntime complexity of an itera\u00adtive coalescing algorithm. The coalescing must have been carried out in \na series of steps where in each step two nodes are coalesced. To determine whether two nodes, say lrland \nlr2, can be coalesced, we must check the condition (lrl,lr2):IRIby scanning the two live ranges across \nthe entire length of the program where the two live ranges overlap. The time complexity of this operation \nis (L),where Lis bounded by the number of statements in the program. The number of coalescing operations \nis bounded by the number of nodes in the interference graph. Thus the total time spent in coalescing \nis bounded by (xL). To avoid the expensive operation of scanning two live ranges to compute (lrl,lr2)at \nthe time of attempting coalescing, we explore the use of fast methods based upon the use of conservative \nestimates of (lrl,lr2). A conservative estimate can overes\u00adtimate but it must never underestimate it. \nLet us consider a simple and most obvious approximation. By scanning the en\u00adtire program exactly once, \nwe can precompute the maximum width of each live range lr, A (lr). Using this information, esti\u00admated \nmaximum interference width (lrl,lr2)can be com\u00adputed as follows: (lrl,lr2)=A (lrl)+A (lr2) (\"(lrl,lr2)). \nNote we do not need to scan the program to compute (lrl,lr2). While this method is simple and allows \nestimation of of two live ranges at the time of iter\u00adative coalescing in (1)time, it fails to handle \na common situation well. In Fig. 6 live ranges Aand are shown. It is clear that they can be allocated \nto a single 32 bit register. However, since A (A)=A ()=32and therefore (A,)= 64, we cannot coalesce them \nusing this simple approach. 16 16 MAX(A)=32  respectively at a program point corresponding to maximum \ninter\u00adference width of Aand (i.e., (A,)=Ab+a). For the above example, the edge label is (16,16). The \nimportant observa\u00adtion is that by looking at the edge label we can now determine that coalescing of Aand \nis possible because their combined width at any program point does not exceed 32 (because 16+16=32). \nThe edge labels are more formally de.ned below. De.nition 5. (Interference Graph Labels) Initially each \ninter\u00adference edge (A,)is labelled with (Ab,a)where Ab and aare the contributions of Aand to (A,) (i.e., \n(A,)=Ab+a). Subsequently, each edge (C,D)formed after coalescing, is labelled with (C,D)where Cand Dare \nthe contributions of C and Dto (C,D)(i.e., (C,D)=C+ D). When nodes are coalesced, labels for the edges \nemanating from the newly created node must be computed. It is during this process that some imprecision \nis introduced. We have developed a fast and highly accurate method for computing the edge labels. Next \nwe present this method in detail. Updating edge labels following coalescing If there was an edge between \na node Cand one or both of nodes Aand , then there will be an edge between Aand Cin the transformed graph. \nWe must determine the label (A,Cab)for this edge. Two cases that arise are handled as shown in Fig. 7. \nThe .rst case involves the situation in which Cwas connected by an edge to either Aor . In this case \nthe label of edge (A,C) will be same as the label on the edge (A,C)or (,C)as the case may be. Since Cinterferes \nonly with A(or ), after coalescing of Aand the maximum interference width between Aand Cis same as maximum \ninterference width between A(or )and C.It should be noted that no additional imprecision is introduced \nduring the generation of the label for edge (A,C). The second case considers the situation in which there \nis an edge between Cand both Aand . In this case additional impreci\u00adsion may be introduced during the \nestimation of (A,Cab)for edge (A,C)as this label is based upon a conservative estimate of (A,,C). Our \ngoal is to carry out this estimation quickly by avoiding examining the complete live ranges corresponding \nto nodes A, and C. In addition, we would like to obtain a label that is as precise as possible based \nupon the existing labels of the three nodes and edges between them. Three candidate estimates for (A,,C)denoted \nby A, B,and oin Fig. 7 are considered. Ais the estimate of the (Ba,Ab)=(16,16)  that (A,,C)is equal \nto any of the three computed val- Edge labelling. sum of widths of A, ,and Cat a point where maximum \ninter\u00adference width between and Ctakes place. At such a point, the  best estimates for the widths of \nA, and Care max(Ab A ,A), and Cbrespectively (i.e., A=max(Ab,A)++Cb). Simi\u00adlarly B( o) represents the \npoint at which maximum interference B width of Aand C(Aand ) takes place. Therefore values of B and \nocan be similarly computed. While it may not be the case MAX(B)=32 Node labelling. ues (i.e., A, Band \no) we can derive a conservative estimate 16 16 Figure 6: Node vs. edge labels. To address the above problem \nwith node labels we make use of edge labels. Each edge (A,)is labelled with a pair of values (Ab,a), \nsuch that Aband arepresent the widths of Aand of (A,,C)from these values. In particular, if we sort \nthe values of A, B,and o,the intermediate value intis a safe approximation for (A,,C). Therefore as shown \nin Fig. 7, we choose this value and depending upon whether intis A, Bor o, we accordingly compute (A,Cab).The \ntheo\u00adrem in Fig. 8 formally proves the correctness of this method. Case I. Node C has an edge to either \nAor . (Ac,Ca) C C ,Ca) Cab) C (Bc,Cb) C c,Cb) ,Cab) A B AB =(Ac(ABc, A B AB =(B(ABc (Ab,Ba) (Ab,Ba) \nCase II. Node Chas an edge to both Aand . A (Ac,Ca) C (Ab,Ba) B (Bc,Cb) AB C (ABc,Cab) ifEmin:Eint:Emax,st{ \nEmin=minimum(EA,EB,Eo) Emax=maximum(EA,EB,Eo) where, EA=max(Ab,A)++Cb EB=max(a,)+A+Ca Eo =max(Ca,Cb)+Ab+a \nthen(A,Cab)={(max(Ab,A)+,Cb)ifEint=EA (max(a,)+A,Ca)ifEint=EB (Ab+a,max(Ca,Cb))ifEint=Eo Figure 7: Updating \nlabels after coalescing A and B. From the intermediate value theorem it follows that a single co\u00adalescing \noperation takes (1)time. Each coalescing operation re\u00ad moves a node from the interference graph. Therefore \nthe number of (20,12) (20,12) coalescing operations is bounded by the number of nodes in the D DEA A \n interference graph. Hence the run time complexity of the coalesc\u00ading operations is bounded by (). Recall \nthat the slow algorithm had a complexity of (xL). Example Let us apply the coalescing operations using \nthe intermediate value theorem to the interference graph of the live ranges constructed for an example \nin Fig. 4. We assume that the registers are 32 bits wide for this example. From the live ranges constructed \nwe .rst build the .ve node interference graph shown in Fig. 9. While the nodes in the interference graph \ncan be coalesced in a number of ways, one such order is shown in Fig. 9. First we merge Dand . According \nto rules for handling Case I, the labels for all edges emanating from Dbecome labels of the corresponding \nedges emanating from D. In the next two steps nodes A, ,and Care coalesced during which Case II arises. \nTherefore, the labels on edges are updated using the intermediate value theorem giving (8,12) (8,12) \nE (1,8) (1,8)  the results shown in the .gure. Note that if the bitwidths of the variables are ignored, \nthe original interference graph requires 4 registers to color as the graph contains a clique of four \nnodes. In contrast a register allocator will need to use two colors to color the coalesced interference \ngraph. Thus the proposed coalescing algorithm reduces the register requirements for the interference \ngraph from 4 registers to 2 registers. The code Figure 9: Illustration of node coalescing. 20 16 20 3228 \n32 based upon usage of two registers was shown in Fig. 5. 16 Now let us conisder the result of application \nof simple coalesc-  ing approach which only maintains node labels. Assuming that the same pairs of nodes \nare considered for coalescing as were consid\u00adered during the application of algorithm based upon edge \nlabels  8888 8 in Fig. 9, we can perform at most two coalescing operations as shown in Fig. 10. Thus, \nin this case 3 registers would be required. Therefore using edge labels is superior to using node labels \nin this Figure 10: Node coalescing using node labels. example. Theorem (Intermediate Value Theorem). \n {min=minimum(A,B,o) ifmin:int:max,st,(A,,C)=intis safe. max =maximum(A,B,o) Proof. The proof is carried \nout in two parts. Lemma 1 shows that in general minis not a safe estimate for (A,,C) because mincan be \nless than (A,,C). Lemma 2 shows that if minis less than (A,,C), then values of both intand maxare greater \nthan (A,,C). From Lemma 1 and Lemma 2 it follows that intis the best safe estimate for (A,,C)from among \nthe three values, A, Band o. (Lemma 1) min (A,,C)can be true: Consider the construction of live ranges \nas shown in the .gure be\u00adlow. Note that d1in this construction which clearly con.rms that indeed minmaynot \nbeasafeestimate of (A,,C)as min=A (A,,C). A d > 1 C r(A,,C)=w+d-l  EA=max(w-l,w)+w+w=w EB=max(w+d,w)+w+w+d=w+d \nEo =max(w+d,w)+w-l+w+d=w+d-l =Emin=EA\u00ab r(A,,C). (Lemma 2) min (A,,C)=max \" int \"(A,,C): Let (A,,C)=A+B+o, \nwhere A, B,and oare contributions of A, and Cto (A,,C). By de.nition of it must be the case that: 0+\"+ \n\u00aeA\" A+o,and \u00aeAbaAB, 1+Ca \u00ae2+Cb \" B+o. Without any loss of generality, let us assume that min=A. Given \nthat min (A,,C)it follows that: A=max(Ab,A)++CbA+B+oor \u00aeABo,++Cb. 3++max(AbA) [A+B+omax(Ab,A)+3and [+Cb \n\" B+o2+Cb]\u00ae]\u00ae=Amax(Ab,A) =\u00aeAAb 4and \u00aeA. A [Ab+a \" A+0and [AAb]\u00ae=aB=\u00aemax(a,)B B]\u00ae46[A+Ca \" A+o1and [A \n=Ca=Jmax(Ca,Cb) ]\u00aeA]\u00aeo\u00aeo [max(a,)6and [A\"+]\u00aeB]\u00ae+CaAo1 =max(a,)+A+CaA+B+o =\u00aeB (A, ,C). [max(Ca,Cb)]\u00aeand \n[Ab+\"+0 oJaAB]\u00ae =max(Ca,Cb)+Ab+aA+B+o =\u00aeo (A, ,C). We have shown that if A (A,,C),then B (A,,C)and \no (A,,C). Given that Ais min, int=min(B,o)and max =max(B,o). Thus we conclude that: min (A,,C)=max \" \nint \"(A,,C). From Lemma 1 and Lemma 2 it follows that (A,,C)=intis the best safe estimate for (A,,C)from \namong the three values, A, Band o. Hence the proof of the intermediate value theorem is complete. Figure \n8: The intermediate value theorem. Priority based coalescing So far we have focussed on the fundamental \nissues of bitwidth aware register allocation (i.e., analysis for live range construction and variable \npacking using node coalescing). We have not ad\u00addressed the following issues: Is coalescing always good? \nIn what order should node coalescing be attempted? While coalescing can reduce the chromatic number of \na graph, this in not always the case. In some situations coalescing may in\u00adcrease the chromatic number \nof graph for the graph shown below the chromatic number is two before coalescing but it increases to \nthree after coalescing. A solution for preventing harmful coalescing was proposed by Briggs et al. [1]. \nThey observed that if the node created by coalescing of two nodes has fewer than kneighbors with degree \nof kor more, where kis the number of colors, the result\u00ading node will always be colorable. Thus, they \npropose restricting coalescing to situations where resulting nodes are guaranteed to be colorable. Figure \n11: Increase in chromatic number due to coalescing. The order in which node coalescings are attempted \nimpacts the shape of the .nal graph and thus the number of colors required to color the resulting graph. \nFor example, if we reconsider the exam\u00adple of Fig. 10 and merge nodes A and B as well as nodes C and \nD, the resulting graph contains three nodes which can be colored using two colors as opposed to three \ncolors that are required by resulting graph of Fig. 10. 2016 2816 2824 88 8 Figure 12: The impact of \nordering of coalescing operations. One approach that we propose to address the above problem is to assign \npriorities to all of the nodes. Node with the highest priority, say n, is picked and neighbors of nare \nconsidered for coalesc\u00ading in decreasing order of priority. When no more nodes can be coalesced with \nn, the next highest priority node is picked and the above process repeated. The priority of a live range \nlis computed as shown below. The greater the savings due to elimination of loads and stores, the higher \nis the priority. However, the savings are nor\u00admalized with respect to the the amount of register resources \nused. The register usage is based upon the duration and the number of bits that are occupied by the live \nrange. Hence, it is simply the area of the live range which can be obtained by summing together the number \nof bits occupied by the live range at all relevant program points. Priority(lr )e SaviR gist ngs(l) \nUsag(l ) = Loa s&#38;Sto sAvoi Liv Rang A a = Loa s&#38;Sto . it sAvoi (l,p) . Ip Our algorithm for \ncarrying out node coalescing followed by reg\u00adister allocation is summarized in Fig. 13. Following the \niterative node coalescing phase each set of coalesced variables is given a new name and the code is transformed \nto use this name. In ad\u00addition, intravariable moves are introduced to preserve program se\u00admantics. The \nresulting interference graph is then processed using a traditional coloring based register allocator. \n1. Construct interference graph. 2. Label edges with interference widths. 3. Construct prioritized \nnode list. 4. while node list =qdo 5. Get a node, say n, from prioritized node list. 6. for each node \nain n s adjacency list do 7. Attempt coalescing awith n. 8. if successful, update graph and prioritized \nlist. 9. endfor 10. endwhile 11. Replace each coalesced variable set with a new name. 12. Introduce \nintravariable moves. 13. Perform coloring based register allocation.  Figure 13: Algorithm summary. \n 4. EXPERIMENTAL RESULTS We evaluated the proposed technique using benchmarks taken from the Mediabench \n[12] (adpcm and g721), NetBench [14] (crc and dh), and Bitwise project at MIT [17] (SoftFloat, NewLife, \nMotionTest, Bubblesort and Histogram)as they are representative of a class of applications important \nfor the embedded domain. We also added an image processing applica\u00adtion (thres). We applied our technique \nto selected functions from these benchmarks that are large in size. We constructed the interference graphs \nfor the selected functions and measured the register requirements for fully coloring these graphs using \nthe following algorithms: (a) Bitwidth unaware algorithm which at any given time allows only a single \nvariable to reside in a register; (b) Naive coalescing (NC) algorithm that labels each node with its \ndeclared width and uses these labels to perform coalescing; and (c) Our coalescing (OC) algorithm that \nbuilds live ranges using bit section analysis and labels edges with maximum interference width information \nto drive coalescing. In all three cases the regis\u00adter requirements were computed by repeatedly applying \nChaitin s algorithm to .nd the minimum number of registers for which the graph could be fully colored. \nThe results of our experiments are given in Table 1. While the OC algorithm reduces register requirements \nby 10% to 50%, NC algorithm is nearly not as successful. By reducing the register re\u00adquirements by a \nfew registers, the quality of code can be expected to improve signi.cantly. This is particularly true \nfor the ARM proces\u00adsor with bit section referencing extensions [13] in context of which this research \nis being carried out as ARM has 16 only registers. Table 1: Register requirements.   Table4: Liveranges \nwithbitwidth 32bits. Benchmark Registers Used Function BU NC OC Live Range Widths (bits) adpcm decoder \n 15 15 13 of live Declared Max Size adpcm coder 18 18 15 ranges Size After BSA g721.update 15 12 10 \nadpcm.decoder g721.fmult 4 3 3 1 32 4 g721.quantize 6 5 5 2 32 1 thres.memo 6 6 4 adpcm.coder thres.coalesce \n 10 10 5 2 32 1 thres.homogen 11 11 6 1 32 8 thres.clip 6 6 4 1 32 3 SoftFloat.mul32To64 8 7 7 g721.update \n NewLife.main 7 7 4 7 16 16 MotionTest.main 6 6 5 1 32 3 Bubblesort.main 9 9 7 4 16 5 Histogram.main \n 7 7 6 1 16 1 crc.main 10 10 9 1 8 1 dh.encodelastquantum 7 7 4 g721.fmult 3 16 16 1 16 12 2 16 4 \ng721.quantize 3 16 16 1 16 12 Table 2: Bene.ts of coalescing. 2 16 4 Benchmark thres.memo Function \n Before After Number of Nodes  Number 2 32 10 adpcm decoder 17 15 1 32 2 adpcm coder 20 17 thres.coalesce \n g721.update 22 15 2 32 10 g721.fmult 8 7 5 32 8 g721.quantize 8 6 thres.homogen thres.memo 6 4 \n2 32 10 thres.coalesce 10 5 5 32 8 thres.homogen 12 7 1 32 2 thres.clip 7 5 thres.clip SoftFloat.mul32To64 \n 14 12 2 32 10 NewLife.main 8 5 1 32 8 MotionTest.main 9 7 SoftFloat.mul32To64 Bubblesort.main 15 \n11 2 32 1 Histogram.main 13 11 4 16 16 crc.main 12 11 NewLife.main dh.encodelastquantum 8 5 1 32 \n12 2 32 6 2 32 4 MotionTest.main 1 32 16 1 32 6 Table 3: Change in maximum clique size. 1 32 4  Bubblesort.main \n Benchmark Number of Nodes  3 32 16 Function Before After 6 32 10 adpcm decoder 15 13 Histogram.main \n adpcm coder 18 15 1 32 16 g721.fmult 4 3 1 32 12 g721.quantize 6 5 2 32 10 thres.memo 6 4 1 32 8 \nthres.coalesce 10 5 crc.main thres.homogen 11 6 1 64 56 thres.clip 6 4 1 32 8 NewLife.main 7 4 dh.encodelastquantum \n MotionTest.main 6 5 4 32 6 crc.main 10 9 dh.encodelastquantum 7 4                \n              To further understand the signi.cance of the two key steps of our algorithm, \nnamely live range construction based upon bit sec\u00adtion analysis and node coalescing, we examined the \ndata in greater detail. The results in Table 2 show the extent to which node coa\u00adlescing reduces the \nnumber of nodes in each interference graph. As we can see signi.cant amount of coalescing is observed \nto occur. The data in Table 4 shows the signi.cance of our live range con\u00adstruction algorithm. The declared \nwidths of live ranges as well as their reduced maximum widths after bit section analysis (BSA) are given. \nAs we can see, for many live ranges, the declared widths are much larger than their reduced maximum widths. \nThe reason why NC algorithm is nearly not as successful as our OC algorithm is made clear in part by \nthis data the declared bitwidths of variables are often much greater than their true bitwidths. Finally \nit should be noted that although coalescing does not nec\u00adessarily guarantee a reduction in register requirements, \nin most of the programs a signi.cant reduction was observed. We looked at the interference graphs to \nunderstand why this was the case. We found that in most of these programs there were signi.cantly large \ncliques present which accounted for most of the register require\u00adments. Moreover the maximum sized clique \nin the interference graph typically contained multiple subword data items. Thus, node coalescing resulted \nin a reduction in the size of the maximum sized clique and hence the register requirements. Table 3 shows \nthe re\u00adduction in the size of the largest cliques for the programs where the above observation holds. \nThe benchmarks which did not exhibit this behavior are omitted from the table. In the case of Soft-Float \nand Bubblesort there were no large cliques while in the case of g721.update and Histogram although large \ncliques were present, they were not reduced in size by node coalescing. 5. RELATED WORK Bit Section Analysis \nStephenson et al. [17] proposed bitwidth analysis to discover nar\u00adrow width data by performing value \nrange analysis. Once the com\u00adpiler has proven that certain data items do not require a complete word \nof memory, they are compressed to smaller size (e.g., word data may be compressed to half-word or byte \ndata). There are a number of important differences between bitwidth analysis and our analysis for live \nrange construction. First our analysis is aimed at narrowing the width of a variable at each program \npoint as much as possible since we can allocate varying number of register bits to the variable at different \nprogram points. Second, while our approach can eliminate a trailing bit section, value range analysis \ncan never do so. Our approach can eliminate a leading bit section of dead bits which contains non-zero \nvalues while value range analysis can only eliminate a leading bit section if it contains zero bits through \nout the program. Budiu et al. [2] propose a analysis for inferring the values of individual bits. This \nanalysis is much more expensive than our analysis as it must analyze each bit in the variable while our \napproach maintains summary information in form of three bit sections for each variable. Finally, the \nanalysis by Zhang et al. [9] is aimed at automatic discovery of packed variables, while this pa\u00adper is \naimed at carrying out analysis to facilitate variable packing. Memory coalescing and data compression \nDavidson and Jinturkar [4] were .rst to propose a compiler op\u00adtimization that exploits narrow width data. \nThey proposed mem\u00adory coalescing for improving the cache performance of a program. Zhang and Gupta [20] \nhave proposed techniques for compressing narrow width and pointer data for improving cache performance. \n However, both of these techniques were explored in context of gen\u00aderal purpose processors. Therefore \naggressive packing of scalar variables into registers was not studied. In contrast, the work we present \nin this paper is aimed at new class of embedded processors where ef.cient use of small number of registers \nis made possible by holding multiple values in a single register. The only work we are aware of that \ndeals with register allocation for processors that sup\u00adport bit section referencing is by Wagner and \nLeupers [18]. How\u00adever, their work exploits bit section referencing in context of vari\u00adables that already \ncontained packed data. They do not carry out any additional variable packing as described in this paper. \nSome multi\u00admedia instruction sets support long registers which can hold multi\u00adple words of data for carrying \nout SIMD operations [5, 11]. Com\u00adpiler techniques allocate array sections to these registers. In con\u00adtrast, \nour work is aimed at shrinking scalars to subword entities and packing them into registers which are \none word long. The scalar variables that we handle are ignored by superword techniques. Fi\u00adnally in context \nof embedded processors work has been done on dealing with irregular constraints on register allocation \n(e.g., [10]). However, our work is being done in context of ARM instruction set with bit referencing \nextensions where bit section packing is an important issue [13]. 6. CONCLUSIONS Multimedia and network \nprocessing applications make extensive use of subword data. Moreover embedded processors typically sup\u00adport \na small number of word sized registers. Instruction set sup\u00adport for bit section referencing provides \nus with an opportunity to make effective use of small number of registers by packing multi\u00adple subword \nsized variables into a single register, without incurring any additional penalty for accessing packed \nvariables. However, no techniques exist for either identifying variable bitwidth data or packing them \ninto registers. We presented the .rst algorithms to solve both of these problems. We presented ef.cient \nanalyses for constructing variable bitwidth live ranges and an ef.cient variable packing algorithm that \noperates on an enhanced interference graph. Our experiments show that the proposed techniques can reduce \nreg\u00adister requirements of embedded applications by 10% to 50%. 7. ACKNOWLEDGEMENTS This work is supported \nby DARPA award F29601-00-1-0183 and National Science Foundation grants CCR-0208756, CCR-0105535, CCR-0096122, \nand EIA-9806525 to the University of Arizona. 8. REFERENCES [1] P. Briggs, K.D. Cooper, and L. Torczon, \nImprovements to graph coloring register allocation, ACM Transactions on Programming Languages and Systems \n(TOPLAS), 16(3):428-455, May 1994. [2] M. Budiu, M. Sakr, K. Walker, and S.C. Goldstein, BitValue Inference: \nDetecting and Exploiting Narrow Width Computations, 6th European Conference on Parallel Computing (Euro-Par), \nAugust 2000. [3] G. J. Chaitin, Register allocation and spilling via graph coloring, SIGPLAN Symposium \non Compiler Construction, June 1982. [4] J. Davidson and S. Jinturkar, Memory access coalescing : a technique \nfor eliminating redundant memory accesses, ACM SIGPLAN Conference on Programming Language Design and \nImplementation (PLDI), pages 186 195, 1994. [5] R.J. Fisher and H.G. Dietz, Compiling for SIMD within \nRegister, 11th International Workshop on Languages and Compilers for Parallel Computing (LCPC), LNCS, \nSpringer Verlag, Chapel Hill, NC, August 1998. [6] J. Fridman, Data Alignment for Sub-Word Parallelism \nin DSP, IEEE Workshop on Signal Processing Systems (SiPS), pages 251-260, 1999. [7] M.R. Garey and D.S. \nJohnson, Computers and intractability: a guide to the theory of NP-completeness, 1979. [8] L. George \nand A.W. Appel, Iterated register coalescing, ACM Transactions on Programming Languages and Systems (TOPLAS), \n18(3):300-324, May 1996. [9] R. Gupta, E. Mehofer, and Y. Zhang, A Representation for Bit Section Based \nAnalysis and Optimization, International Conference on Compiler Construction (CC), LNCS 2304, Springer \nVerlag, pages 62-77, Grenoble, France, April 2002. [10] A. Koseki, H. Komatsu, and T. Nakatani, Preference-Directed \nGraph Coloring, ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI), pages \n33 44, June 2002. [11] S. Larsen and S. Amarasinghe, Exploiting Superword Level Parallelism with Multimedia \nInstruction Sets, ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI), pages \n145 156, Vancouver B.C., Canada, June 2000. [12] C. Lee, M. Potkonjak, and W.H. Mangione-Smith, Mediabench: \na tool for evaluating and synthesizing multimedia and communications systems, IEEE/ACM International \nSymposium on Microarchitecture (MICRO), Research Triangle Park, North Carolina, December 1997. [13] \nB. Li and R. Gupta, Bit section instruction set extension of ARM for embedded applications, International \nConference on Compilers, Architecture, and Synthesis of Embedded Systems (CASES), Grenoble, Oct. 2002. \n [14] G. Memik, B. Mangione-Smith, and W. Hu, NetBench: A Benchmarking Suite for Network Processors, \nIEEE International Conference Computer-Aided Design (ICCAD), Nov. 2001. [15] X. Nie, L. Gazsi, F. Engel, \nand G. Fettweis, A New Network Processor Architecture for High Speed Communications, IEEE Workshop on \nSignal Processing Systems (SiPS), pages 548-557, 1999. [16] D. Seal, Editor, ARM architecture reference \nmanual, Second Addition, Addison-Wesley. [17] M. Stephenson, J. Babb, and S. Amarasinghe, Bitwidth Analysis \nwith Application to Silicon Compilation, ACM SIGPLAN Conference on Programming Language Design and Implementation \n(PLDI), pages 108 120, Vancouver B.C., Canada, June 2000. [18] J. Wagner and R. Leupers, C Compiler Design \nfor an Industrial Network Processor, ACM SIGPLAN Workshop on Languages, Compilers, and Tools for Embedded \nSystems (LCTES), pages 155-164, June 2001. [19] T. Wolf and M. Franklin, Commbench -a telecommunications \nbenchmark for network processors, IEEE International Symposium on Performance Analysis of Systems and \nSoftware (ISPASS), April 2000. [20] Y. Zhang and R. Gupta, Data Compression Transformations for Dynamically \nAllocated Data Structures, International Conference on Compiler Construction (CC), LNCS 2304, Springer \nVerlag, pages 14-28, Grenoble, France, April 2002. \n\t\t\t", "proc_id": "604131", "abstract": "Multimedia and network processing applications make extensive use of subword data. Since registers are capable of holding a full data word, when a subword variable is assigned a register, only part of the register is used. New embedded processors have started supporting instruction sets that allow direct referencing of bit sections within registers and therefore multiple subword variables can be made to simultaneously reside in the same register without hindering accesses to these variables. However, a new register allocation algorithm is needed that is aware of the bitwidths of program variables and is capable of packing multiple subword variables into a single register. This paper presents one such algorithm.The algorithm we propose has two key steps. First, a combination of forward and backward data flow analyses are developed to determine the bitwidths of program variables throughout the program. This analysis is required because the declared bitwidths of variables are often larger than their true bitwidths and moreover the minimal bitwidths of a program variable can vary from one program point to another. Second, a novel interference graph representation is designed to enable support for a fast and highly accurate algorithm for packing of subword variables into a single register. Packing is carried out by a node coalescing phase that precedes the conventional graph coloring phase of register allocation. In contrast to traditional node coalescing, packing coalesces a set of interfering nodes. Our experiments show that our bitwidth aware register allocation algorithm reduces the register requirements by 10\\% to 50% over a traditional register allocation algorithm that assigns separate registers to simultaneously live subword variables.", "authors": [{"name": "Sriraman Tallam", "author_profile_id": "81100082535", "affiliation": "The University of Arizona, Tucson, AZ", "person_id": "P414188", "email_address": "", "orcid_id": ""}, {"name": "Rajiv Gupta", "author_profile_id": "81100027751", "affiliation": "The University of Arizona, Tucson, AZ", "person_id": "PP43126354", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/604131.604139", "year": "2003", "article_id": "604139", "conference": "POPL", "title": "Bitwidth aware global register allocation", "url": "http://dl.acm.org/citation.cfm?id=604139"}