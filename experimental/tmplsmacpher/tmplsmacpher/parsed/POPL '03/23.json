{"article_publication_date": "01-15-2003", "fulltext": "\n Interprocedural Compatibility Analysis for Static Object Preallocation Ovidiu Gheorghioiu, Alexandru \nS.alcianu, and Martin Rinard Laboratory for Computer Science Massachusetts Institute of Technology Cambridge, \nMA 02139 {ovy, salcianu, rinard}@lcs.mit.edu ABSTRACT We present an interprocedural and compositional \nalgorithm for .nding pairs of compatible allocation sites, which have the property that no object allocated \nat one site is live at the same time as any object allocated at the other site. If an allocation site \nis compatible with itself, it is said to be unitary: at most one object allocated at that site is live \nat any given point in the execution of the program. We use the results of the analysis to statically \npreallocate memory space for the objects allocated at unitary sites, thus simpli\u00adfying the computation \nof an upper bound on the amount of memory required to execute the program. We also use the analysis to \nenable objects allocated at several compatible al\u00adlocation sites to share the same preallocated memory. \nOur experimental results show that, for our set of Java bench\u00admark programs, 60% of the allocation sites \nare unitary and can be statically preallocated. Moreover, allowing compat\u00adible unitary allocation sites \nto share the same preallocated memory leads to a 95% reduction in the amount of memory preallocated for \nthese sites. Categories and Subject Descriptors D.3 [Software]: Programming Languages; D.3.4 [Programming \nLanguages]: Processors compiler, optimization, memory management (garbage collection) General Terms \nLanguages, Algorithms Keywords Static analysis, interprocedural analysis, memory prealloca\u00adtion This \nresearch was supported by DARPA Contract F33615-00-C-1692, NSF Grant CCR-0086154, NSF Grant CCR-0073513, \nNSF Grant CCR-0209075, and the Singapore-MIT Alliance. Permission to make digital or hard copies of all \nor part of this work for personal or classroom use is granted without fee provided that copies are not \nmade or distributed for pro.t or commercial advantage and that copies bear this notice and the full citation \non the .rst page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires \nprior speci.c permission and/or a fee. POPL 03, January 15 17, 2003, New Orleans, Louisiana, USA. Copyright \n2003 ACM 1-58113-628-5/03/0001 ...$5.00. 1. INTRODUCTION Modern object-oriented languages such as Java \npresent a clean and simple memory model: conceptually, all ob\u00adjects are allocated in a garbage-collected \nheap. While this abstraction simpli.es many aspects of the program devel\u00adopment, it can complicate the \ncalculation of an accurate upper bound on the amount of memory required to execute the program. Scenarios \nin which this upper bound is es\u00adpecially important include the development of programs for embedded systems \nwith hard limits on the amount of avail\u00adable memory and the estimation of scoped memory sizes for real-time \nthreads that allocate objects in sized scoped memories [9]. This paper presents a static program analysis \ndesigned to .nd pairs of compatible allocation sites; two sites are com\u00adpatible if no object allocated \nat one site may be live at the same time as any object allocated at the other site. If an al\u00adlocation \nsite is compatible with itself (we call such allocation sites unitary allocation sites), then at any \ntime during the execution of the program, there is at most one live object that was allocated at that \nsite. It is therefore possible to statically preallocate a .xed amount of space for that allo\u00adcation \nsite, then use that space to hold all objects allocated at that site. Any further space usage analyses \ncan then focus only on the non-unitary allocation sites. Our analysis uses techniques inspired from register \nallo\u00adcation [2, 6] to reduce the amount of memory required to hold objects allocated at unitary allocation \nsites. The basic approach is to build and color an incompatibility graph.The nodes in this graph are \nthe unitary allocation sites. There is an undirected edge between two nodes if the nodes are not compatible. \nThe analysis applies a coloring algorithm that assigns a minimal number of colors to the graph nodes \nsub\u00adject to the constraint that incompatible nodes have di.erent colors. This information enables the \ncompiler to statically preallocate a .xed amount of memory for each color. At each unitary allocation \nsite, the generated code bypasses the standard dynamic allocation mechanism and instead simply returns \na pointer to the start of the statically preallocated memory for that allocation site s color. The object \nis stored in this memory for the duration of its lifetime in the com\u00adputation. Our algorithm therefore \nenables objects allocated at compatible allocation sites to share the same memory. Results from our implemented \nanalysis show that, for our set of Java benchmark programs, our analysis is able to iden\u00adtify 60% of \nall allocation sites in the program as unitary al\u00adlocation sites. Furthermore, our incompatibility graph \ncol\u00adoring algorithm delivers a 95% reduction in the amount of memory required to store objects allocated \nat these unitary allocation sites. We attribute the high percentage of unitary allocation sites to speci.c \nobject usage patterns characteris\u00adtic of Java programs: many unitary allocation sites allocate exception, \nstring bu.er, or iterator objects. We identify two potential bene.ts of our analysis. First, it can be \nused to simplify a computation of the amount of memory required to execute a given program. We have im\u00adplemented \na memory requirements analysis that, when pos\u00adsible, computes a symbolic mathematical expression for \nthis amount of memory [16]. Our results from [16] show that pre\u00adceding the memory requirements analysis \nwith the analysis presented in this paper, then using the results to compute the memory requirements \nof unitary sites separately, can signi.cantly improve both the precision and the e.ciency of the subsequent \nmemory requirements analysis. The second potential bene.t is a reduction in the memory management overhead. \nBy enabling the compiler to convert heap alloca\u00adtion to static allocation, our analysis can reduce the \namount of time required to allocate and reclaim memory. This paper makes the following contributions: \n Object Liveness Analysis: It presents a composi\u00adtional and interprocedural object liveness analysis \nthat conservatively estimates the set of objects that are live at each program point.  Compatibility \nAnalysis: It presents a compositional and interprocedural analysis that .nds sets of compat\u00adible allocation \nsites. All objects allocated at sites in each such set can share the same statically preallocated memory. \nThisanalysisusesthe resultsofthe object liveness analysis.  Implementation: We implemented our analyses \nin the MIT Flex [3] compiler and used them to analyze a set of Java benchmark programs. Our results show \nthat our analyses are able to classify the majority of the allocation sites as unitary allocation sites, \nand that many such sites can share the same memory. We also implemented and evaluated a compiler optimization \nthat transforms each unitary allocation site to use pre\u00adallocated memory space instead of invoking the \nstan\u00addard memory allocator.  The rest of this paper is organized as follows. Section 2 presents the \nanalysis algorithm. Section 3 describes the im\u00adplementation and presents our experimental results. We \ndis\u00adcuss related work in Section 4 and conclude in Section 5. 2. ANALYSIS PRESENTATION Given a program \nP , the goal of the analysis is to detect pairs of compatible allocation sites from P , i.e., sites that \nhave the property that no object allocated at one site is live at the same time as any object allocated \nat the other site. Equivalently, the analysis identi.es all pairs of incompatible allocation sites, i.e., \npairs of sites such that an object allo\u00adcated at the .rst site and an object allocated at the second \nsite may both be live at the same time in some possible exe\u00adcution of P . An object is live if any of \nits .elds or methods is used in the future. It is easy to prove the following fact: Fact 1. Two allocation \nsites are incompatible if an ob\u00adject allocated at one site is live at the program point that corresponds \nto the other site. To identify the objects that are live at a program point, the analysis needs to track \nthe use of objects throughout the program. There are two complications. First, we have an abstraction \nproblem: the analysis must use a .nite abstrac\u00adtion to reason about the potentially unbounded number \nof objects that the program may create. Second, some parts of the program may read heap references created \nby other parts of the program. Using a full-.edged, .ow-sensitive pointer analysis would substantially \nincrease the time and space requirements of our analysis; a .ow-insensitive pointer analysis [18, 5] \nwould not provide su.cient precision since liveness is essentially a .ow-sensitive property. We address \nthese complications as follows: We use the object allocation site model [13]: all objects allocated \nby a given statement are modelled by an in\u00adside node1 associated with that statement s program label. \n The analysis tracks only the objects pointed to by local variables. Nodes whose address may be stored \ninto the heap are said to escape into the heap. The analysis conservatively assumes that such a node \nis not unitary (to ensure this, it sets the node to be incompatible with itself). Notice that, in a usual \nJava program, there are many objects that are typically manipulated only through local variables: exceptions, \niterators, string bu.ers, etc.2  Under these assumptions, a node that does not escape into the heap \nis live at a given program point if and only if a variable that is live at that program point refers \nto that node. Variable liveness is a well-studied data.ow analysis [2, 6] and we do not present it here. \nAs a quick reminder, a variable v is live at a program point if and only if there is a path through the \ncontrol .ow graph that starts at that program point, does not contain any de.nition of v and ends at \nan instruction that uses v. The analysis has to process the call instructions accu\u00adrately. For example, \nit needs to know the nodes returned from a call and the nodes that escape into the heap dur\u00ading the execution \nof an invoked method. Reanalyzing each method for each call instruction (which corresponds con\u00adceptually \nto inlining that method) would be ine.cient. In\u00adstead, we use parameter nodes to obtain a single context\u00adsensitive \nanalysis result for each method. The parameter nodes are placeholders for the nodes passed as actual \nargu\u00adments. When the analysis processes a call instruction, it replaces the parameter nodes with the \nnodes sent as argu\u00adments. Hence, the analysis is compositional: in the absence of recursion, it analyzes \neach method exactly once to extract a single analysis result.3 At each call site, it instantiates the \nresult for the calling context of that particular call site. 1We use the adjective inside to make the \ndistinction from the pa\u00adrameter nodes that we introduce later in the paper. 2It is possible to increase \nthe precision of this analyis by tracking one or more levels of heap references (similar to [8]). 3The \nanalysis may analyze recursive methods multiple times before it reaches a .xed point. static void main(String \nargs[]) { . INode inside nodes List l = createList(10); n lb filterList(l); . PNode parameter nodes \nn n . Node = INode . PNode general nodes Figure 1: Node Abstraction Figure 1 presents a summary of our \nnode abstraction. We use the following notation: INode denotes the set of all inside nodes, PNode denotes \nthe set of parameter nodes, and Node denotes the set of all nodes. When analyzing a method M, the analysis \nscope is the method M and all the methods that it transitively invokes. The inside nodes model the objects \nSystem.out.println(listToString(l)); } static List createList(int size) { 1: List list = new LinkedList(); \nfor(int i = 0; i < size; i++) { 2: Integer v = new Integer(i); list.add(v); } return list; } allocated \nin this scope. I n lb denotes the inside node associ\u00ad static void filterList(List l) { 3a: for(Iterator \nit = l.iterator(); it.hasNext();) { ated with the allocation site from label lb (the superscript I Integer \nv = (Integer) it.next(); I stands for inside ; it is not a free variable). represents n lb if(v.intValue() \n% 2 == 0) all objects allocated at label lb in the currently analyzed it.remove(); scope. The parameter \nnodes model the objects that M re-} ceives as arguments. The parameter node n Pi models the } object \nthat the currently analyzed method receives as its ith argument of object type.4 The analysis has two \nsteps, each one an analysis in itself. The .rst analysis computes the objects live at each alloca\u00adtion \nsite or call instruction.5 The second analysis uses the liveness information to compute the incompatibility \npairs. We formulate our analyses as systems of set inclusion con\u00adstraints and use a bottom-up, iterative \n.xed-point algorithm to compute the least (under set inclusion) solution of the constraints. For a given \nprogram, the number of nodes is bounded by the number of object allocation sites and the number of parameters. \nHence, as our constraints are mono\u00adtonic, all .xed point computations are guaranteed to termi\u00adnate. The \nrest of this section is organized as follows. Section 2.1 describes the execution of the analysis on \na small example. static String listToString(List l) { 4: StringBuffer buffer = new StringBuffer(); 3b: \nfor(Iterator it = l.iterator(); it.hasNext();) { Integer v = (Integer) it.next(); buffer.append(v).append(\" \n\"); } 5: return new String(buffer); } Figure 2: Example Code The analysis processes the methods in \na bottom-up fash\u00adion, starting from the leaves of the call graph. The library method LinkedList.add (not \nshown in Figure 2) causes its parameter node n P 2 (n P 1 is the this parameter) to escape into the \nheap (its address is stored in a list cell). createListSection 2.2 presents the program representation \nthat the I 2 calls add with n as argument; therefore, the analysis instan\u00adanalysis operates on. Section \n2.3 describes the object live- P 2I 2I 2 escapes. In filterList,tiates n with n and detects that n ness \nanalysis. In Section 2.4, we describe how to use the P 1 the parameter node n (the list) escapes into \nthe heap be\u00adobject liveness information to compute the incompatibility cause list.iterator() stores a \nreference to the underlying pairs. Section 2.5 discusses how to apply our techniques to list in the iterator \nthat it creates. multithreaded programs. I 4 In the listToString method, n is live over the call to I \n3 2.1 Example list.iterator() that allocates n : it is pointed to by the local variable buffer, which \nis live both before and after the Consider the Java code from Figure 2. The program cre- I 4 is incompatible \nwith n I 3I 4 iscall. Therefore, n .Because n ates a linked list that contains the integers from 0 to \n9, live at line 5, n I 4I 5 . n I 3 is also incompatible with n is not live removes from the list all \nelements that satisfy a speci.c con\u00addition (the even numbers in our case), then prints a string representation \nof the remaining list. The program contains six lines that allocate objects. The two Iterators from lines \n3a and 3b are allocated in library code, at the same alloca\u00adtion site. The other four lines allocate \nobjects directly by executing new instructions. For the sake of simplicity, we ig\u00adnore the other objects \nallocated in the library. In our exam\u00ad ple, we have .ve inside nodes. Node n I 1 represents the linked \n list allocated at line 1, node n I 2 represents the Integersal\u00ad located at line 2, etc. The iterators \nfrom lines 3a and 3b are both represented by the same node n I 3 (they are allocated at  the same site). \nFigure 3 presents the incompatibility graph for this example. Figure 3: Incompatibility graph for the \ncode from 4I.e., not primitive types such as int, char etc. Figure 2. Circles represent inside nodes; \na double 5The object liveness analysis is able to .nd the live nodes at any circle indicates that the \nnode escapes into the heap. program point; however, for e.ciency reasons, we produce an analysis result \nonly for the relevant statements. n I 3 and n I 5 are compatible unitary nodes. Figure 4: Instructions \nrelevant for the analysis. Name Format Informal semantics COPY v1 = v2 copy one local variable into \nanother NEW v = new C create one object of class C STORE v1.f = v2 create a heap reference RETURN return \nv normal return from a method THROW throw v exceptional return from a method CALL (vN ,vE ) = v1.mn (v2, \n... , vk) method invocation PHI v = f(v1,... ,vk) SSA f nodes in join points TYPESWITCH (v1,v2) = typeswitch \nv : C instanceof tests I 3 and n I 5 are still compatible. The parameter erencing), our intermediate \nrepresentation follows the Java P 1 (the list) is live at lines 4 and 3b (but not at 5). convention \nof allocating and initializing an exception ob- P 1 is incompatible with n 4 and n 3. ject, (e.g., a \nNullPointerException), then propagating the I 1 The analysis of main detects that l points to n (because \nexception to the appropriate catch block or throwing the 1). escapes into the heap, the analysis detects \nthat n 1 and discovers the incompatibility pairs I As the parameter of filterList exception out of the \nmethod if no such block exists. Notice createList returns n I escapes. that due to the semantics of \nthe Java programming lan\u00ad 1 When processing the call to listToString, the analysis in-guage, each instruction \nthat can throw an exception is also P Iwith n stantiates n a potential object allocation site. Moreover, \nthe exception 1 I 4). escapes into the heap and is not an unitary node; references to it can be stored \ninto the heap or passed as I I (n 1,n3) and (n1,n I I The analysis has already determined objects are \n.rst class objects: once an exception is caught, that n 1 we generate the last two incompatibility pairs \nfor purely ex-arguments of invoked methods. In practice, we apply an op\u00adpository purposes. timization \nso that each method contains a single allocation The graph coloring algorithm colors n I 3 and n I 5 \n with the site for each automatically inserted exception (for example, same color. This means that the \ntwo iterators and the String allocated by the program have the property that no two of them are live \nat the same time. Hence, the compiler can statically allocate all of these objects into the same memory \nspace. 2.2 Program Representation We work in the context of a static compiler that compiles the entire \ncode of the application before the application is deployed and executes. Our compiler provides full re.ec\u00adtive \naccess to classes and emulates the dynamic loading of classes precompiled into the executable. It does \nnot support the dynamic loading of classes unknown to the compiler at compile time. This approach is \nacceptable for our class of target applications, real time software for embedded devices, for which memory \nconsumption analysis is particularly im\u00adportant. The analyzed program consists of a set of methods m1,m2,... \n. Method, with a distinguished main method. Each method m is represented by its control .ow graph CFGm \n.The vertices of CFGm are the labels of the instruc\u00adtions composing m s body, while the edges represent \nthe .ow of control inside m. Each method has local variables v1,v2,...vl . Var , and parameters p1,... \n,pk . Var,where Var is the set of local variables and method parameters. Figure 4 contains the instructions \nthat are relevant for the analysis. We assume that the analyzed program has already been converted into \nthe Single Static Information (SSI) form [4], an extension of the Static Single Assignment (SSA) form \n[15] (we explain the di.erences later in this sec\u00adtion). Our intermediate representation models the creation \nand the propagation of exceptions explicitly. Each instruction that might generate an exception is preceded \nby a test. If an exceptional situation is detected (e.g., a null pointer deref-NullPointerException and \nArrayIndexOutOfBoundsException) that the method may generate but not catch. When the method detects such \nan exception, it jumps to that allo\u00adcation site, which allocates the exception object and then executes \nan exceptional return out of the method. To allow the inter-procedural propagation of exceptions, a CALL \ninstruction from label lb has two successors: succN (lb) for the normal termination of the method and \nsuccE (lb) for the case when an exception is thrown out of the invoked method. In both cases locally \ngenerated exceptions or excep\u00adtions thrown from an invoked method the control is passed to the appropriate \ncatch block, if any. This block is determined by a succession of instanceof tests. If no applicable block \nexists, the exception is propagated into the caller of the current method by a THROW instruction throw \nv . Unlike a throw instruction from the Java language, a THROW instruction from our intermediate representation \nalways terminates the execution of the current method. Note: we do not check for exceptions that are \nsubclasses of java.lang.Error. 6 This is not a signi.cant restriction: as we work in the context of a \nstatic compiler, where we know theentirecode and class hierarchy, most of these errors can\u00adnot be raised \nby a program that compiled successfully in our system, e.g. VirtualMachineError, NoSuchFieldError etc. \nIf the program raises any one of the rest of the errors, e.g., OutOfMemoryError, it aborts. In most of \nthe cases, this is the intended behavior. In particular, none of our bench\u00admarks catches this kind of \nexception. We next present the informal semantics of the instructions from Figure 4. A COPY instruction \nv1 = v2 copies the 6In the Java language, these exceptions correspond to severe errors in the virtual \nmachine that the program is not expected to handle. value of local variable v1 into local variable v2.A \nPHI in\u00adstruction v = f(v1,... ,vk) is an SSA fnode that appears in the join points of the control .ow \ngraph; it ensures that each use of a local variable has exactly one reaching de.\u00adnition. If the control \narrived in the PHI instruction on the ith incoming edge, vi is copied into v. A NEW instruction v = new \nC allocates a new object of class C and stores a reference to it in the local variable v. A CALL instruction \n(vN ,vE )= v1.mn (v2, ... , vk) calls the method named mn of the object pointed to by v1, with the arguments \nv1,... ,vk. 7 If the execution of the invoked method terminates with a RETURN instruction return v , \nthe address of the returned object is stored into vN and the control .ow goes to succN (lb), where lb \nis the label of the call instruction. Otherwise, i.e., if an exception was thrown out of the invoked \nmethod, the address of the exception object is stored into vE and the control .ow goes to succE (lb). \nA TYPESWITCH instruction (v1,v2) = typeswitch v : C corresponds to a Java instanceof test. It checks \nwhether the class of the object pointed to by v is a subclass of C. v is split into two variables: v1 \nis v s restriction on the true branch, while v2 is v s restriction on the false branch. Therefore, the \nobject pointed to by v1 is an instance of C, while the object pointed to by v2 is not. A TYPESWITCH instruction \nis a simple example of an SSI sigma node, (v1,v2)= s(v) , that the SSI form introduces to preserve the \n.ow sensitive information acquired in the test instruc\u00adtions. SSI thus allows the elegant construction \nof predicated data.ow analyses. Apart from this variable splitting , SSI is similar to the SSA form. \nIn particular, the SSI conversion seems to require linear time in practice [4]. Finally, a STORE instruction \nv1.f = v2 sets the .eld f of the object referenced by v1 to point to the object ref\u00aderenced by v2. The \nother instructions are irrelevant for our analysis. In particular, as we do not track heap references, \nthe analysis cannot gain any additional information by an\u00adalyzing the instructions that read references \nfrom memory. However, we do analyze the STORE instructions because we need to identify the objects that \nescape into the heap. We assume that we have a precomputed call graph: for each label lb that corresponds \nto a CALL instruction, callees(lb) is the set of methods that that call instruction may invoke. The analysis \nworks with any conservative ap\u00adproximation of the runtime call graph. Our implementa\u00adtion uses a simpli.ed \nversion of the Cartesian Product Al\u00adgorithm [1]. 2.3 Object Liveness Analysis Consider a method M, a \nlabel/program point lb inside M,and let live(lb) denote the set of inside and parameter nodes that are \nlive at lb. We conservatively consider that a node is live at lb i. it is pointed to by one of the variables \nthat are live at that point: . live(lb)= v live in lb P(v) where P(v)is the set of nodes to which v \nmay point. To interpret the results, we need to compute the set EG of inside nodes that escape into the \nheap during the execution 7For the sake of simplicity, in the presentation of the analysis we consider \nonly instance methods (in Java terms, non-static methods), i.e., with v1 as the this argument. The implementation \nhandles both instance methods and static methods. of the program. To be able to process the calls to \nM,we also compute the set of nodes that can be normally returned from M, RN (M), the set of exceptions \nthrown from M, RE(M), and the set of parameter nodes that may escape into the heap during the execution \nof M, E(M). More formally, the analysis computes the following mathematical objects: P : Var .P(Node) \nEG . INode RN ,RE : Method .P(Node) E : Method .P(PNode) We formulate the analysis as a set inclusion \nconstraint problem. Figure 5 presents the constraints generated for a method M .Method with k parameters \np1,p2,... ,pk.At the beginning of the method, pi points to the parameter node n P . A COPY instruction \nv1 = v2 sets v1 to point i to all nodes that v2 points to; accordingly, the analysis gen\u00aderates the \nconstraint P(v1)= P(v2).8 The case of a PHI instruction is similar. A NEW instruction from label lb, \nv = new C , makes v point to the inside node nlb I attached to that allocation site. The constraints \ngenerated for RETURN and THROW add more nodes to RN (M)and RE(M), re\u00adspectively. A STORE instruction \nv1.f = v2 , causes all the nodes pointed to by v2 to escape into the heap. Accordingly, the nodes from \nP(v2) are distributed between EG (the inside nodes) and E(M) (the parameter nodes). A TYPESWITCH instruction \n(v1,v2)= typeswitch v : C works as a type .lter: v1 points to those nodes from P(v) that may represent \nobjects of a type that is a subtype of C, while v2 points to those nodes from P(v)thatmay represent objects \nof a type that is not a subtype of C.In Figure 5, SubTypes(C) denotes the set of all subtypes (i.e., \nJava sub\u00adclasses) of C (including C). We can precisely determine the type type(nlbI ' )of aninside node \nn I by examining the NEW lb' instruction from label lb'. Therefore, we can precisely dis\u00adtribute the \ninside nodes between P(v1)and P(v2). Aswe do not know the exact types of the objects represented by the \nparameter nodes, we conservatively put these nodes in both sets.9 A CALL instruction (vN ,vE )= v1.mn \n(v2, ... , vk) sets vN to point to the nodes that may be returned from the invoked method(s). For each \npossible callee m .callees(lb), we include the nodes from RN (m)into P(vN ). Note that RN (m) is a parameterized \nresult. We therefore instantiate RN (m) before use by replacing each parameter node n P with i the nodes \nthat the corresponding argument vi points to, i.e., the nodes from P(vi). The case of vE is analogous. \nThe ex\u00adecution of the invoked method m may also cause some of the nodes passed as arguments to escape \ninto the heap. Accord\u00adingly, the analysis generates a constraint that instantiates the set E(m) and the \nuses the nodes from the resulting set E(m)(P(v1),... ,P(vk)) to update EG and E(M). Here is a more formal \nand general de.nition of the previ\u00adously mentioned instantiation operation: if S .Node is a PP set that \ncontains some of the parameter nodes n1 ,... ,n k (not necessarily all), and S1,... ,Sk .Node,then . \nS(S1,...Sk) = {n I .S}. P Si n.S i 8As we use the SSI form, this is the only de.nition of v1; therefore, \nwe do not lose any precision by using = instead of . . 9A better solution would be to consider the declared \ntype Cp of the corresponding parameter and check that Cp and C have at least one common subtype. Instruction \nat label lb in method M Generated constraints method entry P(pi)= {n P i }, .1 =i=k , where p1, ... , \npk are M s parameters. COPY: v1 = v2 P(v1)= P(v2) NEW: v = new C P(v)= {n I lb } STORE: v1.f = v2 E(M) \n.P(v2) nPNode, EG .P(v2) nINode RETURN: return v RN (M) .P(v) THROW: throw v RE (M) .P(v) CALL: (vN ,vE \n)= v1.mn (v2, ... , vk) P(vN )= . m.callees(lb ) RN (m)(P(v1),... ,P(vk)) P(vE)= . m.callees(lb ) RE \n(m)(P(v1),... ,P(vk)) let A= . m.callees(lb ) E(m)(P(v1),... ,P(vk))in E(M) .AnPNode, EG .AnINode PHI: \nv = f(v1,... ,vk) P(v)= .k i=1 P(vi) TYPESWITCH: (v1,v2)= typeswitch v : C P(v1)= {n I lb ' .P(v) |type(n \nI lb ' ) .SubTypes(C)}.{n P .P(v)} P(v2)= {n I lb ' .P(v) |type(n I lb ' ) .SubTypes(C)}.{n P .P(v)} \nSubTypes(C) denotes the set of subclasses of class C. Figure 5: Constraints for the object liveness \nanalysis. For each method M, we compute RN (M), RE (M), E(M) and P(v) for each variable v live in at \na relevant label. We also compute the set EG of inside nodes that escape into the heap. 2.4 Computing \nthe Incompatibility Pairs Once the computation of the object liveness information completes, the analysis \ncomputes the (global) set of pairs of incompatible allocation sites IncG .INode \u00d7INode. 10 The analysis \nuses this set of incompatible allocation sites to detect the unitary allocation sites and to construct \nthe compatibility classes. Figure 6 presents the constraints used to compute IncG. An allocation site \nfrom label lb is incompatible with all the allocation sites whose corresponding nodes are live at lb. \nHowever, as some of the nodes from live(lb)may be pa\u00ad rameter nodes, we cannot generate all incompatibility \npairs directly. Instead, for each method M, the analysis collects the incompatibility pairs involving \none parameter node into aset of parametric incompatibilities ParInc(M). It instan\u00ad tiates this set at \neach call to M, similar to the way it in\u00ad stantiates RN (M), RE(M)and E(M): . ParInc(M)(S1,... ,Sk)= \n(nP ,n).ParInc(M) Si \u00d7{n} i 10Recall that there is a bijection between the inside nodes and the allocation \nsites. (Si is the set of nodes that the ith argument sent to M might point to). Notice that some Si may \ncontain a param\u00adeter node from M s caller. However, at some point in the call graph, each incompatibility \npair will involve only inside nodes and will be passed to IncG. To simplify the equations from Figure \n6, for each method M, we compute the entire set of incompatibility pairs AllInc(M). After AllInc(M) is \ncomputed, the pairs that contain only inside nodes are put in the global set of in\u00adcompatibilities IncG; \nthe pair that contains a parameter node are put in ParInc(M). Our implementation of this algorithm performs \nthis separation on the .y , as soon as an incompatibility pair is generated, without the need for AllInc(M). \nIn the case of a CALL instruction, we have two kinds of incompatibility pairs. We have already mentioned \nthe .rst kind: the pairs obtained by instantiating ParInc(m),.m . callees(lb). In addition, each node \nthat is live over the call (i.e., before and after the call) is incompatible with all the nodes corresponding \nto the allocation sites from the invoked methods. To increase the precision, we treat the normal and \nInstruction at label lb in method M Generated constraints v = new C live(lb) \u00d7{n I lb }.AllInc(M) (vN \n,vE)= v1.mn (v2, ... , vk) ,'succN (lb) succE (lb) .m .callees(lb), ParInc(m)(P(v1),... ,P(vk)).AllInc(M) \n(live(lb) nlive(succN (lb))) \u00d7AN (m) .AllInc(M) (live(lb) nlive(succE (lb))) \u00d7AE(m) .AllInc(M) .M .Method, \nAllInc(M) n(INode \u00d7INode) .IncG AllInc(M) \\(INode \u00d7INode) .ParInc(M) Figure 6: Constraints for computing \nthe set of incompatibility pairs. Instruction at label lb in method M Condition Generated constraints \nv = new C lb r return n I lb .AN (M) lb r throw n I lb .AE(M) (vN ,vE )= v1.mn (v2, ... , vk) succN (lb) \nr return succN (lb) r throw succE (lb) r return succE (lb) r throw AN (m) .AN (M),.m .callees(lb) AN \n(m) .AE (M),.m .callees(lb) AE(m) .AN (M),.m .callees(lb) AE(m) .AE(M),.m .callees(lb) Figure 7: Constraints \nfor computing AN , AE . For each relevant instruction, if the condition from the second column is satis.ed, \nthe corresponding constraint from the third column is generated. the exceptional exit from an invoked \nmethod separately. Let AN (m) .INode be the set of inside nodes that represent the objects that may be \nallocated during a method execu\u00adtion that returns normally. Similarly, let AE(m) .INode be the set of \ninside nodes that represent the objects that may be allocated during an invocation of m that returns \nwith an exception. We describe later how to compute these sets; for the moment we suppose the analysis \ncomputes them just before it starts to generate the incompatibility pairs. Let succN (lb) be the successor \ncorresponding to the nor\u00admal return from the CALL instruction from label lb.The nodes from live(lb) nlive(succN \n(lb)) are incompatible with all nodes from AN (m). A similar relation holds for AE(m). Computation of \nAN (M), AE(M) Given a label lb from the code of some method M,we de\u00ad.ne the predicate lb r return tobetrue \ni. there is apath inCFGM from lb to a RETURN instruction (i.e., the instruction from label lb may be \nexecuted in an invo\u00adcation of M that returns normally). Analogously, we de\u00ad.ne lb r throw to be true \ni. there is a path from lb to a THROW instruction. Computing these predicates is an easy graph reachability \nproblem. For a method M, AN (M) contains each inside node nlb I that corresponds to a NEW instruction \nat label lb such that lb r return. In addi\u00adtion, for a CALL instruction from label lb in M s code, if \nsuccN (lb) r return, then we add all nodes from AN (m) into AN (M), for each possible callee m. Analogously, \nif succE(lb) r return, AE (m) .AN (M). The computation of AE(m) is similar. Figure 7 formally presents \nthe con\u00adstraints for computing the sets AN (M)and AE (M). 2.5 Multithreaded Applications So far, we \nhave presented the analysis in the context of a single-threaded application. For a multithreaded appli\u00adcation, \nthe analysis needs to examine all methods that are transitively called from the main method and from \nthe run() methods of the threads that may be started. In addition, all nodes that correspond to started \nthreads need to be marked as escaped nodes. The rest of the analysis is unchanged. In Java, each thread \nis represented by a thread object allocated in the heap. For an object to escape one thread to be accessed \nby another, it must be reachable from either the thread object or a static class variable (global variables \nare called static class variables in Java). In both cases, the analysis determines that the corresponding \nallocation site is not unitary. Therefore, all objects allocated at unitary allocation sites are local \nto the thread that created them and do not escape to other threads. Although we know that no two objects \nallocated by the same thread at the same unitary site are live at any given moment, we can have multiple \nlive objects allocated at this site by di.erent threads. Hence, for each group of compatible unitary \nsites, we need to allocate one memory slot per thread, instead of one per program. The compiler generates \ncode such that each time the pro\u00adgram starts a new thread, it preallocates memory space for all unitary \nallocation sites that may be executed by that thread. For each unitary allocation site, the compiler \ngen\u00aderates code that retrieves the current thread and uses the preallocated memory space for the unitary \nsite in the current thread. When a thread terminates its execution, it deallo\u00adcates its preallocated \nmemory space. As only thread-local objects used that space, this deallocation does not create dangling \nreferences. To bound the memory space occupied by the unitary allocation sites, we need to bound the \nnum\u00adber of threads that simultaneously execute in the program at any given time. 2.6 Optimization for \nSingle-Thread Programs In the previous sections, we consider a node that escapes into the heap to be \nincompatible with all other nodes, in\u00adcluding itself. This is equivalent to considering the node to be \nlive during the entire program. We can gain additional precision by considering that once a node escapes, \nit is live only for the rest of the program. This enhancement allows us to preallocate even objects that \nescape into the heap, if their allocation site executes at most once. This section presents the changes \nto our analysis that apply this idea. We no longer use the global set EG. Instead, for each label lb, \nE(lb) .Node denotes the set of nodes that the instruc\u00adtion at label lb may store a reference to into \nthe heap. This set is relevant only for labels that correspond to STOREs and CALLs; for a CALL, it represents \nthe nodes that escape during the execution of the invoked method. We extend the set of objects live at \nlabel lb (from method M) to include all objects that are escaped by instructions at labels lb from M \nthat can reach lb in CFGM : . . live(lb)= . E(lb ' ) v live in lb P(v) lb ' in M lb ' r lb We change \nthe constraints from Figure 5 as follows: for a STORE instruction v1.f = v2 , we generate only the constraint \nE(lb)= P(v2). For a CALL instruction (vN ,vE )= v1.mn (v2, ... , vk) , we generate the same constraints \nas before for P(vN )and P(vE), and the additional constraint . E(lb)= E(m)(P(v1),... ,P(vk)) m.callees(lb \n) The rules for STORE and CALL no longer generate any constraints for EG (unused now) and E(M). Instead, \nwe de.ne E(M)as . E(M)= E(lb) lb in M Now, E(M) .P(Node) denotes the set of all nodes not only parameter \nnodes as before, but also inside nodes that escape into the heap during M s execution. The rest of the \nanalysis is unchanged. The new de.nition of live(lb) ensures that if a node escapes into the heap at \nsome program point, it is incompatible with all nodes that are live at any future program point. Notice \nthat objects allocated at unitary sites are no longer guaranteed to be thread local, and we cannot apply \nthe preallocation opti\u00admization described at the end of Section 2.5. Therefore, we use this version of \nthe analysis only for single thread pro\u00adgrams.  3. EXPERIMENTAL RESULTS We have implemented our analysis, \nincluding the opti\u00admization from Section 2.6, in the MIT Flex compiler sys\u00adtem [3]. We have also implemented \nthe compiler transfor\u00admation for memory preallocation: our compiler generates executables with the property \nthat unitary sites use preal\u00ad located memory space instead of calling the memory alloca\u00ad tion primitive. \nThe memory for these sites is preallocated at the beginning of the program. Our implementation does not \ncurrently support multithreaded programs as described in Section 2.5. We measure the e.ectiveness of \nour analysis by using it to .nd unitary allocation sites in a set of Java programs. We obtained our \nresults on a Pentium 4 2.8Ghz system with 2GB of memory running RedHat Linux 7.3. We ran our compiler \nand analysis using Sun JDK 1.4.1 (hotspot, mixed mode); the compiler generates native executables that \nwe ran on the same machine. Table 1 presents a description of the programs in our benchmark suite. We \nanalyze programs from the SPECjvm98 benchmark suite11 and from the Java version of the Olden benchmark \nsuite [12, 11]. In addition, we analyze JLex, JavaCUP, and 205 raytrace. Table 2 presents several statistics \nthat indicate the size of each benchmark and the analysis time. The statistics refer to the user code \nplus all library methods called from the user code. As the data in Table 2 indicate, in general, the \ntime required to perform our analysis is of the same order of magnitude as the time required to build \nthe intermedi\u00ad ate representation of the program. The only exceptions are 202 jess and 213 javac. Table \n3 presents the number of total allocation sites and unitary allocation sites in each program. These \nresults show that our analysis is usually able to identify the majority of these sites as unitary sites: \nof the 14065 allocation sites in our benchmarks, our analysis is able to classify 8396 (60%) as unitary \nsites. For twelve of our twenty benchmarks, the analysis is able to recognize over 80% of the allocation \nsites as unitary. Table 3 also presents results for the allocation sites that allocate exceptions (i.e., \nany subclass of java.lang.Throwable), non-exceptions (the rest of the ob\u00ad jects), and java.lang.StringBu.ers \n(a special case of non\u00ad exceptions). For each category, we present the total number of allocation sites \nof that kind and the proportion of these sites that are unitary. The majority of the unitary allocation \nsites in our benchmarks allocate exception or string bu.er objects. Of the 9660 total exception allocation \nsites in our benchmarks, our analysis is able to recognize 6602 (68%) as unitary sites. For thirteen \nof our twenty benchmarks, the analysis is able to recognize over 90% of the exception allocation sites \nas unitary sites. Of the 1293 string bu.er allocation sites, our analysis is able to recognize 1190 (92%) \nas unitary sites. For eight benchmarks, the analysis is able to recognize over 95% of the string bu.er \nallocation sites as unitary sites. Table 4 presents the size of the statically preallocated memory area \nthat is used to store the objects created at unitary allocation sites. The second column of the table \npresents results for the case where each unitary allocation site has its own preallocated memory chunk. \nAs described in the introduction of the paper, we can decrease the pre\u00ad allocated memory size signi.cantly \nif we use a graph color\u00ad ing algorithm to allow compatible unitary allocation sites to share the same \npreallocated memory area. The third col\u00ad umn of Table 4 presents results for this case. Our compiler \noptimization always uses the graph coloring algorithm; we provide the second column for comparison purposes \nonly. 11With the exception of 227 mtrt, which is multithreaded.      Application Description SPECjvm98 \nbenchmark set 200 check Simple program; tests JVM features 201 compress File compression tool 202 jess \nExpert system shell 209 db Database application 213 javac JDK 1.0.2 Java compiler 222 mpegaudio Audio \n.le decompression tool 228 jack Java parser generator Java Olden benchmark set BH Barnes-Hut N-body solver \nBiSort Bitonic Sort Em3d Models the propagation of electromagnetic waves through three dimensional objects \nHealth Simulates a health-care system MST Computes the minimum spanning tree in a graph using Bentley \ns algorithm Perimeter Computes the perimeter of a region in a binary image represented by a quadtree \nPower Maximizes the economic e.ciency of a community of power consumers TSP Solves the traveling salesman \nproblem using a randomized algorithm TreeAdd Recursive depth-.rst traversal of atreetosum the nodevalues \nVoronoi Computes a Voronoi diagram for a random set of points Miscellaneous 205 raytrace Single thread \nraytracer (not an o.cial part of SPECjvm98) JLex Java lexer generator JavaCUP Java parser generator \nTable 1: Analyzed Applications Application Analyzed methods Bytecode instrs SSI IR size (instr.) SSI \nconversion time (s) Analysis time (s) 200 check 208 7962 10353 1.1 4.1 201 compress 314 8343 11869 1.2 \n7.4 202 jess 1048 31061 44746 5.3 101.2 209 db 394 12878 18162 2.7 12.3 213 javac 1681 52941 71050 8.2 \n1126.2 222 mpegaudio 511 18041 30884 5.2 15.9 228 jack 618 23864 37253 11.6 55.6 BH 169 6476 8690 1.4 \n3.6 BiSort 123 5157 6615 1.2 2.9 Em3d 142 5519 7497 0.9 3.1 Health 141 5803 7561 0.9 3.2 MST 139 5228 \n6874 1.2 3.0 Perimeter 144 5401 6904 1.2 2.7 Power 135 6039 7928 1.0 3.2 TSP 127 5601 6904 0.9 3.1 TreeAdd \n112 4814 6240 0.8 2.8 Voronoi 274 8072 10969 1.8 4.3 205 raytrace 498 14116 20875 4.2 23.0 JLex 482 22306 \n31354 4.0 12.3 JavaCUP 769 27977 41308 5.8 32.0 Table 2: Analyzed Code Size and Analysis Time Application \nAllocation sites Unitary sites Exceptions Non-exceptions StringBu.ers count % total unitary % total unitary \n% total unitary % 200 check 407 326 80% 273 92% 134 57% 44 97% 201 compress 489 155 32% 390 28% 99 44% \n38 97% 202 jess 1823 919 50% 1130 58% 693 38% 233 84% 209 db 736 354 48% 565 48% 171 49% 65 98% 213 javac \n2827 1086 38% 1863 47% 964 23% 195 89% 222 mpegaudio 825 390 47% 625 55% 200 24% 43 97% 228 jack 910 \n479 53% 612 54% 298 50% 135 99% BH 329 281 85% 243 98% 86 51% 18 94% BiSort 234 198 85% 177 97% 57 47% \n17 94% Em3d 276 235 85% 206 98% 70 50% 20 95% Health 276 227 82% 202 97% 74 42% 17 94% MST 257 216 85% \n194 97% 63 44% 16 93% Perimeter 239 200 84% 180 97% 59 45% 16 93% Power 262 213 81% 192 97% 70 39% 15 \n93% TSP 235 199 85% 176 97% 59 49% 17 94% TreeAdd 227 190 84% 170 96% 57 46% 15 93% Voronoi 448 387 86% \n349 98% 99 44% 28 96% 205 raytrace 753 318 42% 525 44% 228 39% 43 95% JLex 971 812 84% 645 99% 326 54% \n72 86% JavaCUP 1541 1211 79% 943 93% 598 56% 246 92% Total 14065 8396 60% 9660 68% 4405 41% 1293 92% \n Table 3: Unitary Site Analysis Results Application Preallocated memory size (bytes) Size reduction % \nnormal sharing 200 check 5516 196 96% 201 compress 2676 144 95% 202 jess 17000 840 96% 209 db 6028 252 \n96% 213 javac 18316 332 98% 222 mpegaudio 6452 104 98% 228 jack 8344 224 97% BH 4604 224 95% BiSort 3252 \n96 98% Em3d 3860 200 95% Health 3716 96 97% MST 3532 96 97% Perimeter 3280 96 98% Power 3540 196 94% \nTSP 3292 104 97% TreeAdd 3120 92 98% Voronoi 6368 192 97% 205 raytrace 5656 644 89% JLex 13996 1676 88% \nJavaCUP 20540 1180 94% Total 143088 6984 95% Table 4: Preallocated Memory Size The graph coloring algorithm \n.nds an approximation of the smallest number of colors such that no two incompatible allocation sites \nhave the same color. For each color, we pre\u00adallocate a memory area whose size is the maximum size of \nthe classes allocated at allocation sites with that color. Our implementation uses the DSATUR graph coloring \nheuris\u00adtic [10]. It is important to notice that the DSATUR heuris\u00adtic minimizes the numbers of colors, \nnot the .nal total size of the preallocated memory. However, this does not appear to have a signi.cant \nnegative e.ect on our results: as the numbers from Table 4 show, we are able to reduce the preal\u00adlocated \nmemory size by at least 88% in all cases; the average reduction is 95%. Theoretically, the preallocation \noptimization may allocate more memory than the original program: preallocating a memory area for a set \nof compatible allocation sites reserves that area for the entire lifetime of the program, even when no \nobject allocated at the attached set of compatible sites is reachable. An extreme case is represented \nby the mem\u00adory areas that we preallocate for allocation sites that the program never executes. However, \nas the data from Table 4 indicate, in practice, the amount of preallocated memory for each analyzed application \nis quite small. We compiled each benchmark with the memory prealloca\u00adtion optimization enabled. Each \noptimized executable .n\u00adished normally and produced the same result as the unopti\u00admized version. We executed \nthe SPECjvm98 and the Olden applications with their default workload. We ran JLex and JavaCUP on the \nlexer and parser .les from our compiler in\u00adfrastructure. We instrumented the allocation sites to mea\u00adsure \nhow many objects were allocated by the program and how many of these objects used the preallocated memory. \nApplication Total objects Preallocated objects count % 200 check 725 238 33% 201 compress 941 108 11% \n202 jess 7917932 3275 0% 209 db 3203535 142 0% 213 javac 5763881 335775 6% 222 mpegaudio 1189 7 1% 228 \njack 6857090 409939 6% BH 15115028 7257600 48% BiSort 131128 15 0% Em3d 16061 23 0% Health 1196846 681872 \n57% MST 2099256 1038 0% Perimeter 452953 10 0% Power 783439 12 0% TSP 49193 32778 67% TreeAdd 1048620 \n13 0% Voronoi 1431967 16399 1% 205 raytrace 6350085 4080258 64% JLex 1419852 12926 1% JavaCUP 100026 \n16517 17% Table 5: Preallocated Objects  Table 5 presents the results of our measurements. For .ve \nof our benchmarks, at least one third of the objects resided in the preallocated memory. There is no \ncorrelation between the static number of unitary sites and the dynamic number of objects allocated at \nthose sites. This is explained by the large di.erence in the number of times di.erent allocation sites \nare executed. In general, application-speci.c details tend to be the only factor in determining these \ndynamic numbers. For example, in JLex, 95% of the objects are it\u00aderators allocated at the same (non-unitary) \nallocation site; 213 javac and JavaCUP use many StringBu.ersthat wecan preallocate; both 205 raytrace \nand BH use many temporary objects to represent mathematical vectors, etc. 4. RELATED WORK This paper \npresents, to our knowledge, the .rst use of a pointer analysis to enable static object preallocation. \nOther researchers have used pointer and/or escape analyses to im\u00adprove the memory management of Java \nprograms [14, 20, 7], but these algorithms focus on allocating objects on the call stack. Researchers \nhave also developed algorithms that cor\u00adrelate the lifetimes of objects with the lifetimes of invoked \nmethods, then use this information to allocate objects in di.erent regions [19]. The goal is to eliminate \ngarbage col\u00adlection overhead by atomically deallocating all of the objects allocated in a given region \nwhen the corresponding function returns. Other researchers [17] require the programmer to provide annotations \n(via a rich type systems) that specify the region that each object is allocated into. Bogda and Hoelzle \n[8] use pointer analysis to eliminate unnecessary synchronizations in Java programs. In spite of the \ndi.erent goals, their pointer analysis has many technical similarities with our analysis. Both analyses \navoid maintain\u00ading precise information about objects that are placed too deep into the heap. Bogda and \nHoelzle s analysis is more   precise in that it can stack allocate objects reachable from a single \nlevel of heap references, while our analysis does not at\u00ad tempt to maintain precise points-to information \nfor objects reachable from the heap. On the other hand, our analysis is more precise in that it computes \nlive ranges of objects and treats exceptions with more precision. In particular, we found that our predicated \nanalysis of type switches (which takes the type of the referenced object into account) was necessary \nto give our analysis enough precision to statically preallocate exception objects. Our analysis has more \naggressive aims than escape anal\u00ad ysis. Escape analysis is typically used to infer that the life\u00ad times \nof all objects allocated at a speci.c allocation site are contained within the lifetime of either the \nmethod that allo\u00ad cates them or one of the methods that (transitively) invokes the allocating method. \nThe compiler can transform such an allocation site to allocate the object from the method stack frame \ninstead of the heap. Notice that the analysis does not provide any bound on the number of objects allocated \nat that allocation site: in the presence of recursion or loops, there may be an arbitrary number of live \nobjects from a sin\u00ad gle allocation site (and an arbitrary number of these objects allocated on the call \nstack). In contrast, our analysis iden\u00ad tify allocation sites that have the property that at most one \nobject is live at any given time. In addition, the stack allocation transformation may re\u00ad quire the \ncompiler to lift the corresponding object allocation site out of the method that originally contained \nit to one of the (transitive) callers of this original allocating method [20]. The object would then \nbe passed by reference down the call stack, incurring runtime overhead.12 The static prealloca\u00ad tion \noptimization enabled by our analysis does not su.er from this drawback. The compiler transforms the original \nallocation site to simply acquire a pointer to the statically allocated memory; there is no need to move \nthe allocation site into the callers of the original allocating method. Our combined liveness and incompatibility \nanalysis and use of graph coloring to minimize the amount of memory re\u00ad quired to store objects allocated \nat unitary allocation sites is similar in spirit to register allocation algorithms [6, Chapter 11]. However, \nregister allocation algorithms are concerned only with the liveness of the local variables, which can \nbe computed by a simple intraprocedural analysis. We found that obtaining useful liveness results for \ndynamically allo\u00ad cated objects is signi.cantly more di.cult. In particular, we found that we had to \nuse a predicated analysis and track the .ow of objects across procedure boundaries to identify signi.cant \namounts of unitary sites. 5. CONCLUSIONS We have presented an analysis designed to simplify the computation \nof an accurate upper bound on the amount of memory required to execute a program. This analy\u00adsis statically \npreallocates memory to store objects allocated at unitary allocation sites and enables objects allocated \nat compatible unitary allocation sites to share the same pre\u00adallocated memory. Our experimental results \nshow that, for our set of Java benchmark programs, 60% of the allocation sites are unitary and can be \nstatically preallocated. More\u00adover, allowing compatible unitary allocation sites to share 12A semantically \nequivalent alternative is to perform method inlining. However, inlining introduces its own set of overheads. \nthe same preallocated memory leads to a 95% reduction in the amount of memory required for these sites. \nBased on this set of results, we believe our analysis can automatically and e.ectively eliminate the \nneed to consider many object allocation sites when computing an accurate upper bound on the amount of \nmemory required to execute the program. We have also used the analysis to optimize the memory man\u00adagment. \n 6. ACKNOWLEDGEMENTS We would like to thank Wes Beebee and Scott C. Ananian for their useful advice on \nimplementing the preallocation optimization in the MIT Flex compiler system [3], and Vik\u00adtor Kuncak for \nproofreading early drafts of the paper. We also want to thank the anonymous referees for their valuable \ncomments. 7. REFERENCES [1] Ole Agesen. The cartesian product algorithm. In Proceedings of the 9th European \nConference on Object-Oriented Programming. Lecture Notes in Computer Science, 1995. [2] A.V. Aho,R. Sethi,and \nJ. Ullman. Compilers: Principles, Techniques, and Tools. Addison-Wesley, Reading, Mass., Reading, MA, \nsecond edition, 1986. [3] C. Scott Ananian. MIT FLEX compiler infrastructure for Java. http://www..ex-compiler.lcs.mit.edu. \n[4] C. Scott Ananian. Static single information form. Master s thesis, Laboratory for Computer Science, \nMassachusetts Institute of Technology, September 1999. [5] Lars Ole Andersen. Program Analysis and Specialization \nfor the C Programming Language.PhD thesis, DIKU, University of Copenhagen, May 1994. [6] Andrew Appel. \nModern Compiler Implementation in Java. Cambridge University Press, 1998. [7] B. Blanchet. Escape analysis \nfor object oriented languages. Application to Java. In Proceedings of the 14th Annual Conference on Object-Oriented \nProgramming Systems, Languages and Applications, Denver, CO, November 1999. [8] J. Bogda and U. Hoelzle. \nRemoving unnecessary synchronization in Java. In Proceedings of the 14th Annual Conference on Object-Oriented \nProgramming Systems, Languages and Applications,Denver, CO, November 1999. [9] Gregory Bollella, James \nGosling, Benjamin Brosgol, Peter Dibble, Steve Furr, and Mark Turnbull. The Real-Time Speci.cation for \nJava. Addison-Wesley, Reading, Mass., 2000.  [10] Daniel Br\u00b4elaz. New methods to color the vertices \nof a graph. Communications of the ACM, (22):251 256, 1979. [11] Brendon Cahoon and Kathryn S. McKinley. \nData .ow analysis for software prefetching linked data structures in Java. In Proceedings of the 10th \nInternational Conference on Parallel Architectures and Compilation Techniques, 2001. [12] Martin C. Carlisle \nand Anne Rogers. Software caching and computation migration in Olden. In Proceedings of the 5th ACM SIGPLAN \nSymposium on Principles and Practices of Parallel Programming, 1995. [13] David R. Chase, Mark Wegman, \nand F. Kenneth Zadeck. Analysis of pointers and structures. In Proceedings of the SIGPLAN 90 Conference \non Programming Language Design and Implementation, 1990. [14] J. Choi, M. Gupta, M. Serrano, V. Sreedhar, \nand S. Midki.. Escape analysis for Java. In Proceedings of the 14th Annual Conference on Object-Oriented \nProgramming Systems, Languages and Applications, Denver, CO, November 1999. [15] R. Cytron, J. Ferrante, \nB. Rosen, M. Wegman, and K. Zadeck. E.ciently computing static single assignment form and the control \ndependence graph. ACM Transactions on Programming Languages and Systems, 13(4):451 490, October 1991. \n [16] Ovidiu Gheorghioiu. Statically determining memory consumption of real-time Java threads. MEng thesis, \nMassachusetts Institute of Technology, 2002. [17] Dan Grossman, Greg Morrisett, Trevor Jim, Michael Hicks, \nYanling Wang, and James Cheney. Region-based memory management in Cyclone. In Proceedings of the SIGPLAN \n02 Conference on Programming Language Design and Implementation, 2002. [18] Bjarne Steensgaard. Points-to \nanalysis in almost linear time. In Proceedings of the 23rd Annual ACM Symposium on the Principles of \nProgramming Languages, St. Petersburg Beach, FL, January 1996. [19] M. Tofte and L. Birkedal. A region \ninference algorithm. ACM Transactions on Programming Languages and Systems, 20(4), July 1998. [20] J. \nWhaley and M. Rinard. Compositional pointer and escape analysis for Java programs. In Proceedings of \nthe 14th Annual Conference on Object-Oriented Programming Systems, Languages and Applications, Denver, \nCO, November 1999.  \n\t\t\t", "proc_id": "604131", "abstract": "We present an interprocedural and compositional algorithm for finding pairs of <i>compatible</i> allocation sites, which have the property that no object allocated at one site is live at the same time as any object allocated at the other site. If an allocation site is compatible with itself, it is said to be <i>unitary</i>: at most one object allocated at that site is live at any given point in the, execution of the program. We use the results of the analysis to statically preallocate memory space for the objects allocated at unitary sites, thus simplifying the computation of an upper bound on the amount of memory required to execute the program. We also use the analysis to enable objects allocated at several compatible allocation sites to share the same preallocated memory. Our experimental results show that, for our set of Java benchmark programs, 60% of the allocation sites are unitary and can be statically preallocated. Moreover, allowing compatible unitary allocation sites to share the same preallocated memory leads to a 95% reduction in the amount of memory preallocated for these sites.", "authors": [{"name": "Ovidiu Gheorghioiu", "author_profile_id": "81100164383", "affiliation": "Massachusetts Institute of Technology, Cambridge, MA", "person_id": "P414187", "email_address": "", "orcid_id": ""}, {"name": "Alexandru Salcianu", "author_profile_id": "81100441082", "affiliation": "Massachusetts Institute of Technology, Cambridge, MA", "person_id": "P335277", "email_address": "", "orcid_id": ""}, {"name": "Martin Rinard", "author_profile_id": "81100087275", "affiliation": "Massachusetts Institute of Technology, Cambridge, MA", "person_id": "P192534", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/604131.604154", "year": "2003", "article_id": "604154", "conference": "POPL", "title": "Interprocedural compatibility analysis for static object preallocation", "url": "http://dl.acm.org/citation.cfm?id=604154"}