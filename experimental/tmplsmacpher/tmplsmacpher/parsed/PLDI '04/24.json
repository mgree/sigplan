{"article_publication_date": "06-09-2004", "fulltext": "\n Balancing Register Allocation Across Threads for a Multithreaded Network Processor Xiaotong Zhuang \nGeorgia Institute of Technology College of Computing Atlanta, GA, 30332-0280 xt2000@cc.gatech.edu ABSTRACT \nModern network processors employ multi-threading to allow concurrency amongst multiple packet processing \ntasks. We studied the properties of applications running on the network processors and observed that \ntheir imbalanced register requirements across different threads at different program points could lead \nto poor performance. Many times application needs demand some threads to be more performance critical \nthan others and thus by controlling the register allocation across threads one could impact the performance \nof the threads and get the desired performance properties for concurrent threads. This prompts our work. \nOur register allocator aims to distribute available registers to different threads according to their \nneeds. The compiler analyzes the register needs of each thread both at the point of a context switch \nas well as internally. Compiler then designates some registers as shared and some as private to each \nthread. Shared registers are allocated across all threads explicitly by the compiler. Values that are \nlive across a context switch can not be kept in shared registers due to safety reasons; thus, only those \nlive ranges that are internal to the context switch can be safely allocated to shared registers. Spill \ncan cause a context switch. and thus, the problems of context switch and allocation are closely coupled \nand we propose a solution to this problem. The proposed interference graphs (GIG,BIG,IIG) distinguish \nvariables that must use a thread's private registers from those that can use shared registers. We first \nestimate the register requirement bounds, then reduce from the upper bound gradually to achieve a good \nregister balance among threads. To reduce the register needs, move insertions are inserted at program \npoints that split the live ranges or the nodes on the interference graph. We show that the lower bound \nis reachable via live range splitting and is adequate for our benchmark programs for simultaneously assigning \nthem on different threads. As our objective, the number of move instructions is minimized. Empirical \nresults show that the compiler is able to effectively control the register allocation across threads \nby maximizing the number of shared registers. Speed-up for performance critical threads ranges from 18 \nto 24% whereas degradation for performance of non-critical threads ranges only from 1 to 4%. Permission \nto make digital or hard copies of all or part of this work for personal or classroom use is granted without \nfee provided that copies are not made or distributed for profit or commercial advantage and that copies \nbear this notice and the full citation on the first page. To copy otherwise, or republish, to post on \nservers or to redistribute to lists, requires prior specific permission and/or a fee. PLDI 04, June 9 \n11, 2004, Washington, DC, USA. Copyright 2004 ACM 1-58113-807-5/04/0006...$5.00. Santosh Pande Georgia \nInstitute of Technology College of Computing Atlanta, GA, 30332-0280 santosh@cc.gatech.edu Categories \nand Subject Descriptors: D.3.4 [Programming Languages]: Processors Optimization, Code generation, Run-time \nenvironments. General Terms: Algorithms, Languages, Performance. Keywords: Network Processor, Register \nAllocation, Multithreaded Processor. 1. INTRODUCTION The dramatic growth in Internet traffic has motivated \na specialized category of embedded processors called Network Processors (NPs) with fast processing speed \nand specialized hardware support for network applications. Network processors are distinguished by their \nfast processing core and are programmed in a dedicated manner for catering to the specific needs of underlying \napplications. The compiler optimization for network processor is an emerging topic for research [3][4][5][19]. \nIn this paper, we attempt the register allocation problem for a multithreaded network processor IXP. \nThe IXP s network processor model can be applied to any network processor with shared CPU and register \nfile for multiple threads and with fast context switch to hide long latency operations such as memory \naccesses. Typically, network processor applications consist of multiple threads concurrently executing \nmultiple tasks of a network processing application. The tasks can be as simple as packet routing to complex \nones that process packet contents for viruses and malignant code etc. In contrast to general processors, \nthe tasks that execute on different threads of a network processor are bound to them at compilation time; \nin other words, no run time thread assignment takes place. Since low level operations originally done \nwith OS or hardware such as context switch are exposed to the programmer, the compiler has the knowledge \nof thread interactions which are predictable. It is obvious that different tasks have different complexities \nand also levels of desired performance. Some tasks may be more performance critical than others. Implementing \n(effecting) such performance needs across threads is currently impossible for any user. This is so since \ncompiler allocates a fixed number (32) of registers to each thread and does not undertake inter-thread \nanalysis to balance their overall register needs. It may be noted that performance of a thread is quite \nsensitive to register needs; even though the number of spills may be small for a larger number of registers, \neach spill is very expensive (latency of about 20 cycles). Our experience with Intel s IXP network process \nfamily, which largely follows this model, tells us that: 1) we can achieve register balancing among different \nthreads and 2) we can reduce spills through the safe use of shared registers which are not live across \ncontext switch instructions for individual thread 3) through the use of register sharing, overall, we \nmake more registers available to threads boosting their performance. Thus, overall by balancing register \nneeds across threads we can meet their performance requirements. These optimizations are necessary due \nto the disparity of register pressures across threads and across different regions of code in each thread. \nWe first discuss the network processor architectures to gain some understanding of the problem of balancing \nregister requirements across threads. 1.1 Network processors State-of-the-art network processors like \nIntel IXP1200/2400/2800, MMC nP series, IBM power NP etc.[21]. have programmable processing core that \ncan be coded for application needs. In contrast to traditional processors, network processors have their \nspecial properties. Speed vs Flexibility Network processors face the dilemma of offering both prompt \nprocessing of the network traffic and flexibility to the software programmers to meet the requirements \nof different applications. As the network speed continues to increase, the time to process each packet \nmust be shortened to avoid packet loss. For example, processing at OC-192 allows only 52ns for each packet \nand OC-768 leaves only 13ns for processing. The higher speed requires both shorter processing time for \neach packet and shorter time a packet can stay in the system (waiting time + processing time). To speedup \nthe critical paths for packet processing, normally a number of RISC processor cores are equipped to work \nin parallel. Although sometimes a co-processor (typically a general purpose processor) is added to handle \nother slow tasks, the packet processing core must be optimized for speed. Therefore, features such as \nexplicit multithreading, explicit and fast context switching (only pc is saved), direct memory access \n(without the complication of caches) are commonly seen in network processor designs. As memory operations \nare extremely time-consuming, solutions should focus on hiding the latency with tolerable hardware and \nsoftware complexity. With fast context switch, each processor core can hide latencies by context switching \nto other threads when accessing the peripherals. Even if caches are enabled, context switch to other \nthreads is generally a clever way to avoid the deviation in memory access time like in the MMC nP series. \nNetwork processors are also aimed to provide plethora of solutions for network applications, which were \noriginally implemented with dedicated hardware (not flexible) or general purpose processors (too slow). \nRecent research [1][2][22] has attempted complicated tasks such as content inspection, software routers, \nintrusion detection, etc. As more code base is added to network processors, writing in assembly can be \nerror-prone and time-assuming, which greatly hampers the fast prototyping and increases time to market. \nTo provide programmability, a High Level Language (HLL) compiler is sometimes provided, although typically \nwith limited language feature support. There are several on-going research efforts to build proper optimizing \ncompiler for network processors [3][4][5]. As mentioned earlier, non of the current compilers undertake \ninter-thread analysis forcing programmers to manage the register pressure across threads. Without any \nhelp from the compiler it is impossible for a user to hand-tune (multi-threaded) code. This mot Intel \nIXP Network Processor In this paper, we base our work on the Intel IXP network processor. Since its successful \ndesign has made it a very popular product in the network processor market, we generalize its features \nas a general model in section 2. Here, we present several prominent features of the IXP network processor, \nwhich prompt the thread register allocation problem (details in section 1.2). Figure 1 shows the block \ndiagram of the IXP1200 network processor. The chip has 6 micro-engines (processing units or PU) and 4 \nthreads share the same PU. The chip has connections to off\u00adchip SRAM, SDRAM, PCI bus etc. As shown in \nFigure 2.a, typically, each PU gets packets from its input queues, processes it and then writes to its \noutput queues, or the input queues of the next PU in the next pipeline stage. With pipeline processing, \ntypically, some PUs are in charge of getting packets from the input ports; some handle packet processing \nand some are for output ports. Our optimization focuses on the code on different threads of the same \nPU. Figure 2.b shows major components inside each PU. Some of the important features are as follows: \n1. Shared register file but typically non-overlapped partitions. Figure 2.b shows that the general purpose \nregister (GPR) file is shared by the 4 threads. Each thread has access to all registers; however without \noptimization, each thread is normally allocated non-overlapping part of the register file. The reason \nfor the register file partition is due to light\u00adweighted context switch as discussed below. 2. Non-preemptable \nthread execution. There is no operating system, no control present over the threads sharing the CPU. \nA thread gives up the CPU only when it blocks on I/O or other long latency operation or executes a context \nswitch (ctx_switch) instruction voluntarily1. 3. Light-weighted context switch. Context switch is cheap \n(only PC is saved), this is also the reason registers are normally allocated in a non-overlapped fashion \nfrom the register file. If a register is allocated to two threads, after context switch, the content \nin that register may be modified by the other thread. Since registers are neither automatically saved \nnor restored during a context switch such possibilities exist and this is where it becomes a compiler \nproblem to manage registers. 4. Cheap ALU, expensive memory access. No cache is available for memory \naccesses; at least 20 cycles are needed for each load/store instruction. Context switches are typically \nfollowed to hide the long latency of memory accesses. In contrast, all ALU instructions can be completed \nin 1 cycle. Large memory latency makes overall performance sensitive to spills even  1 ctx_switch instruction \ncan be inserted by the programmer to achieve fair sharing of the CPU. A Processing Unit--MicroEngine \n Thread 1 Thread 2 Thread 3 Thread 4 (b) Figure 2. IXP1200 threads and register file on a PU. The above \nfeatures of the IXP network processor are driven by design philosophy to simplify hardware so as to increase \nthe clock rate and execution speed. For instance, context switch is kept very simple and fast (1 cycle \nlatency). For this only program counter (pc) is saved but no registers are saved because it can cause \nlong delay in context switch which may offset the benefits of CPU sharing. On the other hand, since all \nthe hardware details are exposed, compiler can prudent decisions regarding register sharing etc. Next, \nwe propose the multi-threaded register allocation problem. 1.2 The Register Allocation Problem As mentioned \nabove, although the register file can be accessed by all threads, it has to be partitioned without overlap \nacross threads because no register is saved/restored during context switch. Here, we argue that some \nregisters can be safely shared by all threads through compiler analysis since thread switch is predictable. \nThe example in Figure 3 illustrates the problem and the possible ways to solve it. In Figure 3.a, the \ncode for two threads are shown. Assume all variables are dead after their last use in the code. In thread \n1, a code segment contains 12 instructions, including two context switch instructions ctx_switch gives \nup CPU voluntarily and a load causes context switch to wait for I/O operation. Any pair of the 3 variables \ninterferes with each other (co-live at some program points), so in Figure 3.b, they are assigned 3 different \nphysical registers. Notice that variable a is live across ctx_switch instruction, so it must be allocated \nto a physical register that is not used by any other thread, because when thread 1 is context switched \nat this point, other thread should not modify the physical register of variable a, which means only thread \n1 should use the register. On the contrary, variable b and c are only used between two context switch \ninstructions. In other word, when thread 1 is switched out of the CPU, both b and c must be dead. Therefore \nit is safe to reuse the physical registers allocated to b and c in other threads. Thread 2 has 4 instructions, \nwith two context switch instructions. d is only live between two context switch instructions, therefore \nd can share a physical registers with other threads. Simply, r2 is shared used for b in thread 1 and \nd in thread 2, because the code guarantees that when context is switched to thread 2, r2 contains a dead \nvalue (b) for thread 1. Similarly, when context is switched to thread 1, r2 contains a dead value (d) \nin thread 2. This example shows benefits of sharing registers and lowering total register requirements \nfrom four to three. We now show that through another technique (live range splitting) one can reduce \ntotal register requirement further. Three registers seem necessary for thread 1, however we notice that \nat any program point, only two variables are co-live. This prompts our technique of splitting one of \nthe variables and inserting a move instruction at certain point. This is demonstrated in Figure 3.c. \nIn instruction 6, r3 is replaced by r1, while from instruction 8 to 9, r3 is replaced by r2. Instruction \n10 copies r2 to r1, so in instruction 12, we have a consistent replacement (r3\u00c6r1). We have managed to \nreduce total register requirements down to two now. Thread 1 Thread 1 Thread 1 4. store 1. a= 2. ctx_switch \n3. if( )br L1 4. b= 5. =a+b 6. c= 7. br L2 L1: 8. c= 9. =a+c 10. b= L2: 11. =b+c 12. load 1. ctx_switch \n2. d= 3. =d+ Thread 2 1. r1= 2. ctx_switch 3. if( )br L1 4. r2= 5. =r1+r2 6. r3= 7. br L2 L1: 8. r3= \n9. =r1+r3 10. r2= L2: 11. =r2+r3 12. load 1. ctx_switch 2. r2= 3. =r2+ Thread 2 4. store 1. r1= 2. ctx_switch \n3. if( )br L1 4. r2= 5. =r1+r2 6. r1(r3)= 7. br L2 L1: 8. r2(r3)= 9. =r1+r2(r3) 10. r1(r3)=r2(r3)* 11. \nr2= L2: 12. =r2+r1(r3) 13. load (a) (b) (c) Figure 3. Example of register sharing and move insertion. \nThe above example illustrates the potential benefits of register sharing across threads and live range \nsplitting. To further justify the multi-threaded register allocation is important and a compiler solution \nis feasible, we list some properties of the programs that run on the networks to support this argument. \n1. For IXP1200, the hardware provides seemingly enough registers. 128 general purpose registers (GPRs) \ncan be used for each PU. However, for each thread, only 32 GPRs are available if no GPR is shared across \nthreads. Register sharing in IXP is a purely software solution, unlike some SMTs (Simultaneous Multi-threading) \nwhere it is hardware managed. Compiler designates and allocates a register either as a shared or private \none. 2. Since there is no operating system to manage threads, memory access, context switch etc. are \nall explicit and thus context switch is predictable at compile time. 3. As shown in our experiments, \ncontext switch instructions are typically less than 10% of the total instructions and many variables \nare not live across context switch instructions. 4. PUs are assigned with different tasks. Packets are \nprocessed in pipeline fashion--Figure 2.a. Currently, task assignment cannot be done automatically. Although \nin most cases, the same task is assigned to threads on the same microengine. This actually leads to low \nutilization of the CPU, because it is hard to chop tasks properly so that they all take roughly \u00bc of \nthe computation power of the PU. Therefore, we should assume tasks might be different for threads on \nthe same PU.  Item 1 indicates that the registers may not be sufficient on the network processor. Item \n2 and 3 support the feasibility of a compiler solution to optimize the register allocation. Finally, \nitem 4 prompts two kinds of problems, i.e. symmetric vs. asymmetric register allocation, which will be \ndefined in next section. This paper is organized as follows. Section 2 describes the system model and \nproblem formation, section 3 talks about the construction of the interference graphs, section 4 is the \noverall framework, section 5 proposes the algorithm to estimate bounds of register numbers, section 6 \nand 7 are for inter-thread and intra\u00adthread register allocation, section 8 mentions SRA problem briefly, \nsection 9 shows performance evaluation results and section 10 talks about related work and section 11 \nis the conclusion.  2. PRELIMINARIES System Model In this paper, we study a multithreaded network processor \nthat can run multiple threads on a single processing unit (PU i.e. micro-engine for IXP). The threads \non one PU share the computation power of the PU and register files etc. Formally, the model is as follows: \n1. There are totally Nreg registers that can be used by Nthd threads sharing a single PU. 2. Explicit \ncontext switch. A thread won t give up the CPU once it starts execution on it, until a context switch \ninstruction is met. Context switch can happen due to explicit instruction or long latency instructions \nlike a load or a store. 3. Context switch is very cheap (only pc is saved) and it is intended to hide \nlong latency operations. 4. Since network packets are mostly independent of each other so are threads. \nThe purpose of multithreading on the same PU is mainly for latency hiding and concurrency. When one thread \nis stalled due to I/O or other long latency operations, other thread can take the CPU. Therefore, code \non different threads are almost independent (Figure 2.a). Thread communication or synchronization rarely \nhappens, however, our current solutions still works under such circumstances. As a future work, knowledge \nabout thread communication or synchronization might be exploited to improve the register allocator. \n5. All registers are accessible by all threads, but the registers used by one thread at the point of \ncontext switch should not be used anywhere by other threads (later, we will define these registers as \nprivate registers), because this might cause unexpected modification to the registers and lead to unsafe \ncode. 6. Move instruction is much cheaper than spill. 7. Code on different threads of the same PU can \nbe different.  Problem Classification As mentioned in section 1.2, programs executing on different threads \ncan be identical. We call the register allocation problem under such circumstances Symmetric Register \nAllocation (SRA). On the contrary, Asymmetric Register Allocation (ARA) assumes different programs for \ndifferent threads. Mixing threads with different computation requirements can achieve better CPU utilization. \nSince SRA is a sub-problem of ARA, in this paper, we develop our approaches based on ARA. Notice that, \nalthough currently most real programs are for SRA, we are not intentionally complicating the problem, \nbecause our algorithms are equally necessary and important to SRA, as will be illustrated later, SRA \nonly reduces searching space during inter-thread register allocation, while all techniques in this paper \nare applicable to both problems. Our goal is develop general techniques that apply without undue restrictions. \nObjectives The number of total available registers is limited. Therefore, in a multithreaded network \nprocessor model, we aim to (for ARA) balance the register allocation among all threads, so that more \nregisters are allocated to the thread with higher register pressure and the register allocation is catered \nto the requirements of different threads in the system. Furthermore, designating a larger number of shared \nregisters can help all threads to internally adjust their register pressures without causing spills. \nIn case there are not enough registers available for all threads, we attempt to split the live ranges \ninside a thread by using move instructions. Also, our objective is to minimize the number of move instructions \ninserted. The results show move insertion is cheap and effective. Problem Formulation To formalize the \nproblem, we define several concepts. DEFINITIONS: PRi: Number of private registers for thread i, these \nare physical registers only (exclusively) used by thread i. SRi: Number of shared registers needed by \nthread i, these are physical registers used by thread i, but other thread may use them as well. Ri: Number \nof total physical registers needed by thread i, equals PRi +SRi SGR: Number of globally shared registers \nneeded, it is the maximum of shared register demands of each thread, since shared registers can be used \nby all threads, this is the maximum of all SRs. Nreg: Total number of physical registers available in \na PU. For a thread, PR is the number of physical registers that are exclusively allocated to it or the \nnumber of physical registers that can be live across context switch instructions, while SR is the number \nof allocated physical registers that are dead during context switch, which means they can be shared across \nthreads. For example, in Figure 3.b, for thread 1, PR1=1, SR1=2, for thread 2, PR2=0,SR2=1, therefore, \nSGR=2. The relationship and restrictions among these variables are illustrated as the following conditions: \n SGR = Max SR SR ( 1, 2...SRNthd )  .PRi + SGR = Nreg  PR + SR = R  i i ii For SRA, all PRi s and \nSRi s are equal. Given these restrictions, we need to assign registers in a way that the overall register \nneed is satisfied and spills are minimized. 3. CONSTRUCTION OF INTERFERENCE GRAPHS 3.1 Non-Switch Region \nDEFINITIONS: Non-Switch Region (NSR): A non-switch region is a maximal connected sub-graph of the CFG \nwithout any internal context switch instructions. It contains connected parts from several basic blocks. \nThe boundaries of the NSR are either context switch instructions or program entry/exit points. Context \nSwitch Boundary (CSB): The program point of the context switch instruction. A CSB separates the basic \nblock it resides, thus becomes the boundary of NSR(s). A NSR can be constructed by starting from an individual \ninstruction and grown it until all nearby instructions are context switch instruction or program entry/exit \npoints. To illustrate, Figure 4.a shows the CFG and NSR for a code segment from benchmark frag in the \nCommbench suite [15]. This code segment is from one of the functions to calculate the IP checksum. The \nCFG consists of 10 basic blocks. Noticeably, there are four context switch instructions, i.e. the read \ninstructions in BB3 and BB7, the explicit ctx_switch instructions in BB5 and BB6. The ctx_switch instructions \nare inserted by the programmer to avoid the monopoly of the CPU. Figure 4.b shows the NSRs. After terminating \nthe CFG at the points of context switch instructions (boundaries), we get 3 NSRs. The NSRs are bound \nby either program entry/exit points or context switch instructions (CSBs). We can assume all terminating \nare inside basic blocks, therefore some basic blocks are split, like BB5 is split into BB5.a in NSR2 \nand BB5.b in NSR1. Sometimes, two parts of a separated basic block still belong to the same NSR like \nthe BB7 in Figure 4. For the example in Figure 3, thread 1 has two NSRs, instruction 1 and 2 are in NSR1 \nand instructions 2 to 12 constitute NSR2. For thread 2, all instructions form one NSR. BB1 Sum=0 DEFINITIONS: \nNode: Live range of a virtual register or variable2 Boundary Node: Node that is live across the CSB, \nwhich may interfere with other boundary nodes. Internal Node: Node that is not live across CSB. Boundary \nInterference: If two boundary nodes are co-live across the same CSB, they are said to be boundary interfering \nwith each other. Internal Interference: If two nodes (internal or boundary nodes) interfere (co-live \nat a program point) within a NSR. Boundary Interference Graph (BIG): A graph consists of all boundary \nnodes and edges only representing boundary interference. Internal Interference Graph (IIG): For each \nNSR, we have an IIG, which only includes the internal nodes live within this NSR and their interference \nedges. Global Interference Graph (GIG): The global interference graph includes both boundary nodes and \ninternal nodes. An edge is added if any two nodes (internal or boundary) interfere with each other. \n return ~sum (a) NSR1 NSR3 BB5.b BB1 BB6.b ctx_switch ctx_switch BB6 ctx_switch If!(len) goto BB8 BB7 \nBB9 Sum=(sum&#38;0xFFFF) +(sum>>16) goto BB8 BB10 Read tmp2\u00c5[buf],1 Sum+=tmp2&#38;0xFFFF BB8 Goto BB2 \n BB7.a BB2 read BB7.b BB3.a Read Sum+= BB6.a read ctx_switch BB8 BB3.b read NSR2  (b) Figure 4. Program \nCFG and the constructed NSR.  3.2 Interference Graphs After building the NSR, we build the interference \ngraph, which will guide the register requirement estimation and register allocation. We need to distinguish \ntwo kinds of interferences and introduce some other definitions for the interference graph. The GIG of \nthe code for the example in Figure 4 is drawn in Figure 5. We assume both len and buf are live at the \nentry point as the length and the buffer pointer of the packet to be calculated. Also, we assume all \nvariables are dead after their last use in the code. From Figure 4.b, we can see both variable tmp1 and \ntmp2 are only live within an NSR, so they are internal nodes. Other variables are live across CSB boundaries. \nThey are boundary nodes. For memory read, since all data is first loaded into transfer registers3, the \ndestination register is not assumed to be live across the memory read i.e. the CSB. At BB1, sum, buf \nand len interfere with each other internally (they also interfere at CSB), thus, the 3 nodes form a clique \non the GIG. tmp1 interfere with sum, buf and len in BB3.b, but at the live point of tmp2 in BB7.b, both \nbuf and len are dead. Thus, sum, buf and len form a BIG; the IIG1 for NSR1 is empty; the IIG2 for NSR2 \nincludes only tmp1, the IIG3 for NSR3 includes only tmp2. Obviously, we have the following claims for \neach thread. Claim 1: To avoid spills, the GIG should be colored with R colors and the BIG should be \ncolored with PR colors. Each IIG, as a part of the GIG, should be colored with no more than R colors. \nClaim 2: Internal nodes on different IIGs are not connected i.e. they do not interfere with each other. \nboundary nodes internal nodes IIG1 IIG2 IIG3 Figure 5. Global interference graph for the example. Notice \nthat, NSRs and interference graphs can be constructed inter-procedurally. CFGs and NSRs of different \nfunctions are connected with edges linking function calls and return points. 2 Here, we assume each live \nrange represents one variable. 3 Transfer registers are special registers on IXP used to store data from/to \nthe memory, generally we can assume they are temporary registers dedicated for memory accesses but unavailable \nas a GPR. our model, although it is much cheaper than spill, we still need to keep the number of inserted \nmove instructions small.  4. OVERALL FRAMEWORK Similarly, we can estimate the MinR needed. MinR=RegPmax=Max(# \nof co-live registers at program points) Figure 6 shows our framework to perform the register allocation. \nOur first step is to build NSR and interference graphs, we then try to estimate the lower and upper bound \nof PR and R for each thread. Starting from the upper bound the inter-thread register allocator reduces \nthe overall register requirement gradually until it is within Nreg. During this process, when the inter-thread \nregister allocator intends to reduce PR or SR, it calls the intra-thread allocators for all threads. \nThe inter-thread allocator goes towards the direction of the smallest cost increase. The framework allows \nthe intra-thread register allocator to be built separately from the inter-thread register allocator. \n 5. REGISTER NUMBER ESTIMATION As the first step towards assigning registers to multiple threads, we \nneed to estimate the number of registers each thread needs based on the interference graph. The estimation \nhelps to guide the distribution of registers to threads at the beginning. Here, we are concerned with \nfinding the bounds for R and PR as defined below. We do not estimate bounds for SR, since the number \nof SR is always equal to R-PR. DEFINITIONS: MinPR, MaxPR: Minimal, maximal number of PR Lemma 1: Regardless \nof shared registers, MinPR can be made equal to RegPCSBmax by inserting move instructions. Proof: If \nwe are given private registers PR1, PR2 PR , Re gPCSB max and at a certain CSB, there are V1, V2 Vn \ntotally n variables live across, RegPCSBmax = n. Simply, insert n move instructions PR1=V1,PR2=V2, PRn=Vn \nbefore the CSB and n move instructions V1= PR1, V2= PR2, Vn=PRn after the CSB can make the code equivalent \nto the original and the number of private registers needed is no more than RegPCSBmax. However, in reality, \nmove instruction still costs 1 cycle in This lower bound is also achievable given enough move instructions. \nThe proof is similar to the one above. Upper Bound Estimation The upper bound gives a maximal number \nof registers required without any extra move instructions inserted. According to claim 1 in section 3.2, \nthe best estimation for MaxPR and MaxR is the minimal number of colors required to color BIG and GIG. \nHowever, for GIG the coloring problem is slightly different from the traditional graph coloring. The \nproblem is to find a coloring scheme for a thread which satisfies: 1. All boundary nodes are colored \nwith at most MaxPR colors 2. All nodes are MaxR colorable 3. Any two interfering nodes are colored \ndifferently  For the GIG in Figure 5, all boundary nodes can be minimally colored with 3 colors; thus \nMaxPR=3. And, all nodes can be minimally colored with 4 colors (there is one 4-node clique), so MaxR=4 \n. SR=1. Actually, there is a tradeoff between MaxPR and MaxR estimation. Reducing MaxPR may induce a \nlarger MaxR. To minimize MaxPR, we can first remove all internal nodes and color the BIG minimally, then \ninsert back the internal nodes and color the graph assuming all boundary nodes have fixed color. To find \nthe tighest (minimal) value of MaxR sufficient to color, we should ignore the condition 1 above, i.e. \nwe could assume that all nodes are indistinguishable and we could simply color the GIG as usual using \nany coloring allocator. Such a coloring would then minimize MaxR but may give a higher MaxPR.  Figure \n7. Estimate the maximal register requirements. We take an approach slightly different from the first \none, i.e. we minimize the MaxPR first. This approach is motivated by the fact that increase in PR causes \ndirect increase in total number of registers, while increase in SR only affects the total number of registers \nwhen this SR is the maximum among all threads (refer to the formula at the end of section 2). Based on \nclaim 2 mentioned in section 3.2, (i.e. IIGs are not connected with each other) we can color IIGs and \nBIG separately and then merge them together to keep a tight control on colorability. After merging, edges \nadded between BIG and IIGs may cause conflicts. For example, in Figure 5, when IIGs and BIG are colored \nseparately, variable sum may get the same color as tmp1, leading to color conflict when the edge between \nthem is added during the merge. A general algorithm to color the whole graph altogether may take much \nmore time, since the graph can be big (it includes all live ranges in the program. Some code in our experiment \ncontains hundreds of nodes). Our approach is similar to the fusion-based or region-based register allocation \n[23], except that our regions are chosen as the IIGs and BIG. The algorithm (Figure 7.a) first builds \nBIG and IIGs from the GIG and colors each of them independently. In other words, the BIG is colored with \ncolor number from 1 to PR, while each IIG is colored with color number from 1 up to R. Some IIGs may \nbe colored with less than R colors, but an IIG can be colored with at most R colors. The next step tries \nto merge each IIG with the BIG. The edges between IIG and BIG can cause problem if the two end nodes \nof an edge have the same color. Such edges are called Conflict Edges. The loop in Figure 7.a shows how \nto resolve all the conflict edges. We illustrate the procedure in Figure 7.b. Suppose boundary node s \nand internal node t is colored with the same color. If s s color can be changed to another color within \ncolor number 1 to PR or t s color can be changed to another color within color number 1 to R, then one \nof them can be changed to another color to remove this conflict edge. If that fails, we heuristically \ntry to change their neighbors colors to see if the two nodes can be recolored after that. After all these \nattempts fail, we have to increase R and t is re-colored with the new color. The algorithm gives MaxPR \nand MaxR finally. The complexity of the algorithm is SO(mini_ color(IIGi))+O(mini_color(BIG))+O(#Edge \nbetween BIG and IIGs). In contrast, the complexity to color the whole graph is O(mini_color(GIG)). This \nmeans the algorithm is also quite fast to try out a given coloring for a thread.  6. INTERTHREAD REGISTER \nALLOCATION 6.1 Our approach One of the difficulties in register allocation for multiple threads is that \nwe do not know exactly how many registers each thread needs. Trying all combinations to find out the \nbest register allocation will cause tremendous amount of compilation time and will be infeasible to build \ninto any practical system. Our approach is to first get an estimation (range) of how many registers are \nneeded by individual thread via the algorithm proposed in the previous section. From this starting point, \nwe use a greedy heuristic algorithm to approach a sub-optimal solution by reducing the total number of \nrequired physical registers gradually. The algorithm also encapsulates the intra-thread register allocator, \nso that it can be developed independently. 6.2 The Register Allocation Algorithm After getting the estimated \nupper bounds MaxPRi and MaxRi for each thread, Let SR = MaxR -MaxPR and PR = MaxPR . iiii i We can check \nwith the following condition: .PRi + ( 1, S2... SN ) reg Max S = N (**) thd i If this holds, we can assign \n( , SS ) as the SGR =Max S 1 2... Nthd number of globally shared registers and MaxPRi as the number of \nprivate registers for each thread to satisfy all register requirements. If the above condition (**) cannot \nhold good, the register requirement is too high. We must either reduce the PR(s) or SR(s) to satisfy \n(**). From (**), we can see, there are two ways to reduce the left side value. Either we can reduce one \nof the PRi, which will result in direct reduction of the left-side value. The other way is to reduce \nSRi, we should reduce the one(s) with the maximal value. In case multiple SRi s have the same maximal \nvalue, we should consider reducing one of the PRi if that costs less. The inter-thread register allocation \nalgorithm is shown in Figure 8. The algorithm first builds GIG and gets the estimations for each thread. \nIf the needed registers are enough (less than Nreg), the program simply allocates register and return. \nOtherwise, it enters a loop to gradually reduce the number of overall register requirement through a \ngreedy algorithm, i.e. every time we choose a direction that can achieve the minimal cost. To reduce \nthe register requirement (i.e. the left side of (**)) by 1, we have many choices. Either we can reduce \none of the PRs by 1 or reduce all the maximal SR(s) by 1 to cut down Max SR SR ( , ...SR ) . Every 12 \nNthd time we reduce PR of one thread, we check if it is larger than the lower bound. Also, the lower \nbound of Ri=PRi+SRi>=MinRi is verified when either PRi or SRi is reduced. INPUT: Nthd, Nreg, CFGs of \nall threads OUTPUT: all PRi and SRi, SGR, CFGs after register allocation /*Intra-thread register allocator, \nreturns move cost*/ Intra_thd_allocator(CFG, GIG, PR, SR); ALGORITHM: Inter_thd_reg_allocation 1. Build_GIG() \n 2. 3. Estimate_reg_requirement() 4. 5. While(sum(PRi)+max(SR1,SR2 SRNthd)>Nreg) 6. Foreach PRi>MinPRi \nand PRi+SRi>MinRi do 7. cost_PRi=Register allocation cost after reducing PRi by 1. 8. od 9. 10. max_SR=max(SR1, \nSR2 SRNthd) 11. cost_SR= Register allocation cost after reducing all SRs 12. that equal max_SR by 1, \nif all such SRs can 13. be reduced(by checking PR+SR>MinR) 14. Find the min one among cost_SR, cost_PR1,cost_PR2 \n 15. Choose the one with minimal cost, modify PRs and SRs. 16. Endw 17. 18. Actually modify the CFGs \nbased on new PRs and SRs 19. SGR= Max(SRi) 20. Return all PRi and SRi, SGR, all CFGs  Figure 8. Algorithm \nfor inter-thread register allocation. The function Intra_thd_allocator is an intra-thread register allocator. \nIt accepts the PR and SR, then tries to return an allocation using PR and SR number of registers. This \nfunction is called when we calculate register allocation cost for each thread and when we finally modify \nthe CFGs. It returns the allocation cost. Actually, the interference graph and coloring scheme given \nby the function Estimate_reg_requirement can be passed to the intra-thread register allocator as a starting \npoint. However, to provide more flexibility, we leave this to the implementation of Intra_thd_allocacor. \nThe complexity of our heuristic algorithm is O(Nreg*Nthd)* O(Intra_thd_allocator), which largely depends \non the complexity of the intra-thread register allocator. Our register allocation algorithm generates \nsatisfactory solution for all benchmark programs within almost negligible compilation time. 7. INTRATHREAD \nREGISTER ALLOCATION The intra-thread register allocator attempts to allocate up to PR number of physical \nregisters to boundary nodes and up to R=PR+SR physical registers to all nodes. 7.1 Move Insertion and \nLive Range Splitting Our intra-register allocation is based on live range splitting and move instruction \ninsertion. Live range splitting has been used in register allocation [11] to spill part of the live range \nto memory. In this paper, we attempt to split the live ranges by inserting move instructions to reduce \nthe chromatic number. Lemma 1 has shown that through live range splitting MinPR can be reached. Figure \n9 gives another example. In Figure 9.a, live ranges A B and C interfere with each other at three different \nCSB points. The lower bound lemma in section 3 gives MinPR=2, but the interference graph must be colored \nwith 3 colors, because A,B, and C form a clique. In Figure 9.b we split the live range of variable A \ninto A1 and A2 by inserting move instruction at the split point. The resulting interference graph can \nbe colored with 2 colors which is equal to MinPR. Notice that, this is also the way we reduce the number \nof registers required in the first example (Figure 3.c). In our intra-thread allocation algorithm, we \nfocus on live range splitting through move insertion because spill is too expensive on network processor \nand our experiments show MinPR (MinR) is much smaller than MaxPR (MaxR). This provides us room to reduce \nchromatic number towards the lower bound by inserting move instructions.  Node Color A \u00c6 1 B \u00c6 2 C \n\u00c6 3   Node Color A1 \u00c6 1 A2 \u00c6 2 B \u00c6 2 C \u00c6 1 B    (a) (b) Figure 9. Live range splitting via move \ninsertion. 7.2 Intra-thread Register Allocation Algorithm Our register allocator works incrementally, \ni.e. it records the context (interference graph with split nodes and the position of move instructions) \nof the last 2 invocations and modifies the context to satisfy the new PR and SR values. Notice that the \ninter\u00adthread allocation algorithm in Figure 8 calls Intra_thd_allocator multiple times. In each step, \neither it accepts the previous context and reduces PR or SR by 1 or it rejects the previous modification \nand starts from the previous to previous context and reduces PR or SR by 1. Incremental modification \ncan save time for otherwise repetitive work. Further, based on the records of the two contexts, we can \nassume that each time the allocator is invoked, it attempts to reduce either PR or SR by 1 from one of \nthe recorded contexts. We name these two kinds of invocation as Reduce-PR invocation and Reduce-SR invocation. \nReduce-PR Invocation In this type of invocation the allocator wants to reduce the PR by one from its \nlast invocation. In other words, the last accepted context can color all boundary nodes with PR colors \nand this invocation wants to color it with PR-1 colors. In this stage, we assume all move instructions \nare inserted near the CSB. With this assumption, we do not need to alter the colors of internal nodes. \nNormally, changing the color of both internal and boundary nodes might induce more move instructions \n(in this case we must split the live range to recolor an internal nodes) and increase the cost accordingly. \nLater, we will show some of the move instructions at the CSB can be eliminated by merging them with move \ninstructions inside the NSR. This actually relocates the move instructions from the CSB boundary. Before \nthe discussion of our algorithm, we first define Neighbor Color Number (NCN). Definition: Neighbor Color \nNumber (NCN): The number of colors used by the neighbors of a given node in a colored graph. INPUT: \nPR, SR OUTPUT: cost (number of inserted move instructions) Static context_pre, context_pre_pre 1. FUNCTION \nReduce_PR(context):cost 2. Begin 3. Foreach color c in PR do 4. Cost=0 5. Foreach node t in Set_color_node(c,BIG) \ndo 6. If NCN(t,BIG)<PR-1 then 7. Change t to another color c in PR other than c. 8. Cost+=min(Cut_if_conflict(t,c,c \n)) for all possible c 9. Else 10. Cost+=min(NSR_exclusion_cost(t,c,c )) for each 11. color c in PR \nother than c 12. Add newly split node with color c to Set_color_node(c,BIG) 13. if it is boundary node \n 14. Endif 15. od 16. Eliminate_unnecessary_move() 17. Record to min_cost if this cost is smaller \nand record the context. 18. od 19. Keep the minimal cost context and return min_cost 20. End 21. \nFUNCTION Reduce_SR(context):cost 22. Begin 23. Foreach color c in SR 24. Cost=0 25. Foreach NSRi \ncolor c is used do 26. Foreach internal node t in Set_color_node(c,IIGi) do 27. If NCN(t, GIG)<R-1 \nthen 28. Color t with a color other than c. 29. Else 30. Cost+=min(live_range_exclusion_cost(t,c,c \n)) 31. For each color c in R other than c 32. Add newly split node with color c to Set_color_node(c,IIGi) \n 33. Endif 34. od 35. od 36. Eliminate_unnecessary_move() 37. Record to min_cost if this cost is \nsmaller and record the context. 38. od 39. Keep the minimal cost context and return min_cost 40. End \n 41. FUNCTION Intra_thd_allocator(PR,SR):cost 42. Begin 43. According to the accepted context, pick \nstored either context_pre 44. or context_pre_pre => context. 45. If(PR is reduced) return Reduce_PR(context) \n 46. Else if (SR is reduced) return Reduce_SR(context) 47. Else return cost for the context //no change \n 48. End  Figure 10. Algorithm for intra-thread register allocation. The algorithm in Figure 10 uses \nfunction NCN(t,BIG) to get the neighbor color number of node t on the BIG. The algorithm also works in \na greedy manner. It tries each color c in PR colors and checks the cost to eliminate that color. Then, \nthe color with least elimination cost is selected to be eliminated and all needed move instructions are \ninserted. Function Set_color_node(c,BIG) returns the set of nodes on BIG with color c. We need to change \nevery node in this set to a different color in PR. Firstly, we check the NCN of t that has color c on \nthe BIG. If this number is less than PR-1 (which means there is at least one color available in PR not \nused by its neighbors), we can change t to another color. Since we have changed t s color on BIG and \nt may internally interfere with other internal nodes or boundary nodes (two boundary nodes can interfere \nonly inside NSR but not on the CSB), we need to check if there is a color conflict. The function Cut_if_conflict(t,c,c \n) attempts to insert move instructions to disconnect such edges. Figure 11 shows how the disconnection \nis done and the corresponding changes on the GIG. In Figure 11.a, s is originally colored with color \nc ; after node t is changed to color c from color c it conflicts with internal node s. We insert a move \nat the CSB, so live range t is split. The part of the live range t in NSR2 becomes t , and this part \ncan keep color c, so it does not conflict with s, while, on the BIG, t is changed to color c . Figure \n11.b shows the changes on the GIG. The edge between t and s gets eliminated after t splits from t. t \nkeeps the original color of t, so in the IIG, it is compatible with s, while on the BIG, the color of \nt is changed. In the algorithm, we try every candidate color for t and pick the one with minimal cost. \ninternal nodes boundary nodes in NSR2 Boundary node t t CSB NSR1 move  t s t Internal node s (a) \n(b) Figure 11. Node splitting to change the color of node t. If this step fails, i.e. NCN(t ,BIG)=PR-1, \nthe algorithm calls function NSR_exclusion_cost(t,c,c ) to get the cost of changing t to another color \nc and to exclude all the NSRs with conflict nodes. NSR_exclusion_cost looks at each NSR where t is live \nto see if there is any node with color c in it. If so, the NSR is excluded by splitting the live range \nof t in that NSR and by inserting move instructions. In our approach, the NSRs are split in whole, i.e. \neither the live range in that NSR is kept with color c (if no conflict) or the live range is split (after \nsplitting, t in that NSR keeps color c). internal nodes Boundary node t boundary nodes in NSR2 Internal \nnode s CSB Boundary node r move  t (a) (b) Figure 12. NSR exclusion to reduce PR. Figure 12 shows how \nNSR exclusion is done. Boundary node t cannot change to color c because the boundary node r and the internal \nnode s are using color c . The conflict NSRs are NSR2 and NSR3, where s and r are live. So, these two \nNSRs are excluded from the live range of the original boundary node t. On the GIG, we see t is split \nfrom t and t now can be colored with c . t keeps color c and it is still compatible with s and r. Notice \nthat, after splitting, the edge originally connected from r to t is connected to t . Therefore, the NCN \nof t is reduced and t can be recolored with c .    The algorithm tries each color other than c to \nrecolor t and finds the minimal value to finally color t. Also notice that, after this step, t is colored \nwith c and, if it is a boundary node, we should add t to Set_color_node(c,BIG) and we will color it with \nsome other color during the later iterations. Set_color_node(c,BIG) will not increase infinitely, since \nfurther splitting t will finally generate internal nodes. Reduce-SR Invocation To reduce SR, we check \nwith each color c in SR to see which one should be reduced with minimal cost. The cost is calculated \nby adding up costs in every NSR where this color is used. Also notice that in this step, all boundary \nnodes are assumed to have fixed colors so that the phase will not affect the PR number. The algorithm \ntries to recolor node with color c in a NSR to other color. If the node on the GIG has NCN less than \nR-1, we can just pick that color and color the node without any cost. Otherwise, live range splitting \nis needed. Live range splitting is illustrated in Figure 13. In Figure 13.a, the example has 3 basic \nblocks. Live range t is recolored with color c , however, live range s also uses color c . Our algorithm \nthen splits t at the boundary where the two live ranges overlap. After splitting, t can still use color \nc and t now changes to c . We assign the color with minimal cost to node t. After the splitting, node \nt is push into Set_color_node(c,IIGi), because now it bears color c. This process will finally stop. \nAfter each splitting, the live range with color c is reduced. Since the value R-1 = RegPmax (according \nto the lower bound estimation in section 5 and the algorithm in Figure 7), in the extreme case, each \nlive range is a single program point, there will be at most RegPmax nodes co-live and live range with \ncolor c can always be recolored. t s st move (a) t  t t t ss (b) Figure 13. Excluding a live range \nwithin NSR to reduce SR. Eliminate Unnecessary Moves During the attempt to reduce PR, we assume that \nall move instructions are inserted near the CSB boundary and during reduce SR, some move instructions \nare inserted inside the NSR. At this point, we can merge some of the internal move instructions with \nthose at the boundary. For two consecutive moves, the first move instruction to the live range is unnecessary \nif the color at the entrance to the first move is also acceptable in the region between the two move \ninstructions. We can safely eliminate the first move and this actually relaxes the restriction in Reduce_PR \nto bind moves to the CSB.   8. THE SRA PROBLEM For SRA problem (defined in section section 2), given \nthe PRs are equal and SRs are also equal. The restriction can be rewritten in a simple form:  N * PR \n+ SR = N thd reg Thus, the inter-thread register allocation algorithm can also be simplified. There are \nonly two possibilities to reduce the register requirements. Due to the shrunk solution space, for algorithm \nin Figure 8, we can actually traverse all the possible PRs and SRs to find the best solution. 9. EXPERIMENTAL \nRESULTS The evaluation of our algorithm is done with the Intel\u00adprovided simulation environment IXP1200 \nDeveloper Benchmark 2.01. The IXP1200 workbench supports cycle\u00adaccurate simulation for IXP microengines \nand other peripheries with high fidelity. In this section, we experiment with 11 benchmark programs and \nsome of their combinations to see the effectiveness of the register allocator. These benchmarks are collected \nfrom Commbench[15], Netbench[16], Intel provided example code and a packet scheduling algorithm from \n[18]. To evaluate our algorithm, the benchmark programs are rewritten in IXP C code (a subset of standard \nC) and a few of them are directly written in assembly (microcode). For those written in assembly code, \nwe restore the virtual registers so that our register allocator can work on the live ranges from scratch. \nOur pass builds the CFG and interference graph from the assembly code, after simple translation of the \nassembly directives. The assembly code is then passed to the assembler to generate machine code. The \nIXP assembly consists of only 40 RISC instructions which makes the translation easy. The assembler simply \nexits if too many registers are required. However, after our pass, the register requirements are always \nsatisfied, so the machine code can be generated properly. Table 1 shows the properties of the benchmark \nprograms. The code size is number of instructions after code generation. The cycle counts are measure \nas follows: for some programs like L2l3forward, it cannot run to a stop in finite time, since these programs \nall runs in a while loop to accept and process packets, the cycle counts are averaged number per iteration \nof the main loop. We list CTX instructions (context switch instructions, which includes load/store, voluntary \ncontext switch and other I/O operations that can cause context switch) each benchmark has. Roughly, about \n10% instructions are CTX instructions. The CTX instructions here do not include spill instructions, as \nwe have removed all spills and reconstructed original live ranges (we did this based on the source code \nand the annotations embedded in the generated assembly code by the Intel IXP compiler). The number of \nlive ranges (nodes on the GIG) is listed in the 5th column. These numbers come from the restored virtual \nregisters. Column 6 and 7 are maximal register pressures in the program (RegPmax) and maximal register \npressure at the CSBs (RegPCSBmax). These are the lower bound estimation for register requirements of \nthe threads. Column 8 and 9 are the upper bound estimation for R and PR based on the algorithm in Figure \n7. The 10th and 11th column give statistics for the numbers of NSRs and their average sizes. One observation \nis that normally larger NSR leads to bigger difference between the maximal and minimal value of P and \nPR. Because more internal nodes can exist in larger NSRs, the register pressure for GIG should exceed \nthe BIG with larger margin. Figure 14 evaluates our inter-thread register allocation algorithm for SRA. \nThe same evaluation for ARA is combined in Table 3. For each benchmark program, we show two relevant \nbars. The first bar is the number of registers allocated to the benchmark assuming only single thread \nis available. We use a Chaitin [9] style register allocator for comparison with our shared register allocator. \nThe second and third bars are the number of private registers and shared registers assigned with our \ninter-thread register allocation algorithm. The same benchmark is assumed to execute on four threads. \nThe algorithm continues until the cost returned is non-zero, which means we want to test how many PR \nand SRs are needed without any move instruction insertions with the inter-thread allocation algorithm. \nThe figure shows that the number of private registers allocated for the multi-threaded case is less than \nthe number of registers needed for standalone register allocation. This is not surprising because shared \nregisters can take care the higher register pressure inside the NSRs. If no shared registers are used \nand each thread runs the single-thread register allocator, many registers are wasted. Compared to the \ncase with multi-threaded register requirements i.e. 4*PR+SR, the average total register saving for all \nbenchmarks is 24%. In Table 2, we collect data for the extreme case with our register allocation algorithm, \ni.e. the maximal number of move instructions that will be inserted, if only the minimal number of registers \nis allocated. This means our algorithm must split many live ranges to reach the minimal number of registers. \nThe move insertion overhead in the extreme case is mostly within 10% of the total number of instructions \nfor the benchmarks. This cost is affordable compared to the overhead due to register spill if the register \nnumber is out of range with the single thread register allocation algorithm. Finally, Table 3 evaluates \nour register allocation algorithm for ARA with 3 scenarios. Notice that all tasks are periodic, independently \nsharing the CPU and execute forever. Thus, we measure the performance improvements of each thread in \nterms of the percentage reduction of cycles per iteration. The first scenario put two Md5 programs on \nthread 0 and 1, two fir2dim on thread 2 and 3. This can be a processing module between the receiving \nand sending module. Our data show the PR and SR assigned, the number of live ranges after the register \nallocation (#Live Ranges), context switch instruction number reduction and cycle change. The column of \n#CTX Reg Spill is the original code generated by the Intel compiler that allocates registers with spilling \nand without register sharing across threads (only allocate 32 registers for each thread). And, #CTX Reg \nSharing is the number with our allocator (actually no change compared with Table 1, because we avoid \nspills). The same is true for cycle count ( #Cycle Reg Spill and #Cycle Reg Sharing ). The fir2dim actually \nruns slower due to inserted moves. But this is profitable due to the big saving from Md5. Thus, the allocator \nis able to boost the performance critical thread (Md5) by slightly slowing down less performance critical \none (fir2dim). The second scenario consists of L2l3fwd receive and send on thread 0 and 1 and Md5 on \nthread 2 and 3. This can be a complete processing modules serving on one sending and one receiving port. \nThe results still show the spills are saved for Md5 with minor costs for moves on L2l3fwd threads. The \nlast scenario runs wraps receive and send on thread 0 and 1, fir2dim and frag on thread 2 and 3. The \nallocator balances register allocation to satisfy wraps thread. Due to a high register pressure, wraps \nreceive and send can run much slower (due to spills) if registers are not allocated properly. Our results \nshow that over 20% speedup is achieved for wraps, whereas only slight slowdown is incurred for the other \ntwo benchmarks, which is in accordance with our optimization objective of boosting performance critical \nthread. 10. RELATED WORK The multi-threaded architecture of our model (similar to IXP) differs from \ntraditional general purpose multi-threaded processors or SMT processors in that the number of threads \nand the code for each thread is known beforehand (at compile time). Further, the architecture exposes \ncontext switch to users (actually the programmer should handle everything except the save/restore of \nPC for each thread). This makes register sharing across threads difficult, since registers are not saved \nduring context switch. On the other hand, exposed architecture features allow the compiler to undertake \ninter-thread register allocation which is the subject of this paper. This problem is also different from \nthe traditional concepts of caller-save and callee-save registers, since registers cannot be saved to \nthe memory during context switch due to the high costs of memory operations. The only chance to use a \nregister that is also used (shared) by other threads is to guarantee the register is dead during the \ncontext switch. [24] studies live range analyses for context switches at the procedure boundary on Alpha \nmachines. Optimizations are conducted to minimize the number of registers that should be saved during \ncontext switches in [24] it is equivalent to reduce the number of callee-save registers. In contrast, \ncontext switches taking place on the network processor are more frequent (reaching basic block level) \nthus require analyses at finer granularities; it is not profitable to save/restore but allow small deadness \nat context switch point for a fine granularity allocation. [20] talks about the inter-task register allocation \nproblem for embedded systems with static OS and predetermined tasks. The goal of the paper is to minimize \nthe number of registers that should be saved during context switch. However, the assumption that tasks \nhave fixed priority and saving the variables to memory during context switch is not applicable in our \nmodel. Finally, their assumption that tasks can be preempted at any given program points except for critical \nsections is not true for our network processor model either. Therefore, the techniques proposed in their \nwork are not applicable to our problem. Table 1. Benchmark applications. A recent publication [19] \nstudies register allocation problem for single thread on the IXP network processor. The compiler is dedicated \nto the particular processor with consideration to many architecture details which involve irregularities \nbut they do not focus on an important IXP feature--multi-threading. The goal of our paper is to study \na generalized model for multi-threaded register allocation, so it can be extended to other network processors \nwith similar designs. We focus on the balancing of registers among different threads and the allocation \nof shared registers to meet the overall register demands across threads. We show that the problem is \nquite involved and provide a systematic solution to balance register requirements across threads by determining \nthe number of registers to be shared and by splitting live ranges within selected threads inserting moves \nminimally.  11. CONCLUSION In conclusion, our approach attempts to maximize sharing of registers across \nthreads to make more registers available to them reducing their spills. The values that are not live \nacross the context switch program points are held in shared registers. Maximizing shared registers in \nturn reduces the spill and context switches making it safer to keep more ranges in shared registers. \nWe approach this problem from zero-spill accounting only for mandatory load/stores and other context \nswitches and work out an approach to balance the registers across threads stabilizing the solution as \nabove. The results show that we are able to minimize register requirements in SRA setting and are able \nto improve the cycle counts substantially in the ARA setting for large benchmarks executing on different \nthreads. This means that it is viable to develop multi-threaded large applications on IXP effectively \nwith a good compiler support. The solution is able to speed-up performance critical threads by meeting \ntheir demands through maximal sharing of registers. 12. ACKNOWLEDGEMENT This work was supported in part \nby NSF grants CCR\u00ad0220262, CCR-0208953 and CCR-03263. 13. REFERENCES [1] Feliks J. Welfeld Network processing \nin content inspection applications, In Proceedings of International Symposium on Systems Synthesis, Sep. \n2001. [2] T.Spalink, S.Karlin, L.Peterson, Y.Gottlieb, Building a Robust Software-Based Router Using \nNetwork Processors, In Proceedings of ACM Symposium on Operating Systems Principles, Oct. 2001. [3] J. \nWagner and R. Leupers, \"C Compiler Design for an Industrial Network Processor,\" In Proceedings of ACM \nSIGPLAN Conference on Languages, Compiler, and Tools for Embedded Systems, Jun. 2001. [4] J.Kim, S.Jung, \nY.Park, Experience with a Retargetable Compiler for a Commercial Network Processor, In Proceedings of \nInternational Conference on Compilers, Architecture and Synthesis for Embedded Systems, Oct. 2002. [5] \nJ. Liu, T. Kong, and F. Chow. Effective compilation support for variable instruction set architecture \n, In Proceedings of the International Conference on Parallel Architectures and Compilation Techniques, \nSep. 2002. [6] T.H.Cormen, C.E.Leiserson, R.L.Rivest, Introduction to algorithms, MIT Press, 1989 [7] \nC.H.Papadimitriou, K.Steiglitz, Combinatorial optimization Algorithms and Complexity, Dover Publications \nINC, 1998. [8] A.V.Aho, R.Sethi, J.D.Ullman, Compilers Principles, Techniques and Tools, Addison-Wesley, \nReading, MA, 1986 [9] S.S.Muchnick, Advanced Compiler Design and Imple\u00admentation, Morgan Kaufman, 1997. \n[10] J.Ferrante, K.J.Ottenstein, J.D.Warren, The program dependence graph and its use in optimization \n, ACM Transactions on Programming Languages and Systems, Jul. 1987. [11] Fred C. Chow and John L. Hennessy, \nThe priority-based coloring approach to register allocation , ACM Transactions on Programming Languages \nand Systems, Oct. 1990. [12] L. George and A. Appel. \"Iterated Register Coalescing,\" ACM Transactions \non Programming Languages and Systems, May 1996. [13] G.J. Chaitin, Register allocation and spilling via \ngraph coloring , In Proceedings of International Conference on Compiler Construction, Jun. 1982. [14] \nG.J. Chaitin, M.A. Auslander, A.K. Chandra, J. Cocke, M.E. Hopkins, P. Markstein, Register allocation \nvia coloring , Computer Language. Vol.6, pp.47--57, Jan. 1981. [15] T. Wolf and M. Franklin, CommBench \n A Telecommunication Benchmark for Network Processors , In Proceedings of IEEE International Symposium \non Performance Analysis of Systems and Software, Apr. 2000. [16] G.Memik, W.H.Mangione-Smith, W. Hu., \nNetBench: A Benchmarking Suite for Network Processors , In Proceedings of the International Conference \non Computer Aided Design, Nov. 2001. [17] Rajiv Gupta, Mary Lou Soffa, and Tim Steele, Register allocation \nvia clique separators , In Proceedings of the ACM SIGPLAN Conference on Programming Language Design and \nImplementation, Jun. 1989. [18] Xiaotong Zhuang, Jian Liu, WRAPS Scheduling and Its Efficient Implementation \non Network Processors , In Proceedings of the 9th International Conference on High Performance Computing, \nDec. 2002. [19] L. George, M. Blume, Taming the IXP Network Processor , In Proceedings of the ACM SIGPLAN \nConference on Programming Language Design and Implementation, 2003. [20] V. Barthelmann, Inter-Task Register-Allocation \nfor Static Operating Systems , In Proceedings of ACM SIGPLAN Conference on Languages, Compiler, and Tools \nfor Embedded Systems, Jun. 2002. [21] http://www.cs.purdue.edu/np/npc.html [22] Timothy Sherwood, George \nVarghese, and Brad Calder, A Pipelined Memory Architecture for High Throughput Network Processors, In \nProceedings of the 30th Annual Intl. Symposium on Computer Architecture, Jun. 2003. [23] David Callahan, \nBrian Koblenz, Register allocation via hierarchical graph coloring, In Proceedings of the ACM SIGPLAN \nConference on Programming Language Design and Implementation, Jun. 1991. [24] Dirk Grunwald, Rich Neves, \nWhole-Program Optimization for Time and Space Efficient Threads, In Proceedings of International Conference \non Architectural Support for Programming Languages and Operating Systems, Oct. 1996.  \n\t\t\t", "proc_id": "996841", "abstract": "Modern network processors employ multi-threading to allow concurrency amongst multiple packet processing tasks. We studied the properties of applications running on the network processors and observed that their imbalanced register requirements across different threads at different program points could lead to poor performance. Many times application needs demand some threads to be more performance critical than others and thus by controlling the register allocation across threads one could impact the performance of the threads and get the desired performance properties for concurrent threads. This prompts our work.Our register allocator aims to distribute available registers to different threads according to their needs. The compiler analyzes the register needs of each thread both at the point of a context switch as well as internally. Compiler then designates some registers as shared and some as private to each thread. Shared registers are allocated across all threads explicitly by the compiler. Values that are live across a context switch can not be kept in shared registers due to safety reasons; thus, only those live ranges that are internal to the context switch can be safely allocated to shared registers. Spill can cause a context switch. and thus, the problems of context switch and allocation are closely coupled and we propose a solution to this problem. The proposed interference graphs (GIG,BIG,IIG) distinguish variables that must use a thread's private registers from those that can use shared registers. We first estimate the register requirement bounds, then reduce from the upper bound gradually to achieve a good register balance among threads. To reduce the register needs, move insertions are inserted at program points that split the live ranges or the nodes on the interference graph. We show that the lower bound is reachable via live range splitting and is adequate for our benchmark programs for simultaneously assigning them on different threads. As our objective, the number of move instructions is minimized.Empirical results show that the compiler is able to effectively control the register allocation across threads by maximizing the number of shared registers. Speed-up for performance critical threads ranges from 18 to 24% whereas degradation for performance of non-critical threads ranges only from 1 to 4%.", "authors": [{"name": "Xiaotong Zhuang", "author_profile_id": "81100621952", "affiliation": "Georgia Institute of Technology, Atlanta, GA", "person_id": "PP28017370", "email_address": "", "orcid_id": ""}, {"name": "Santosh Pande", "author_profile_id": "81409594751", "affiliation": "Georgia Institute of Technology, Atlanta, GA", "person_id": "PP17009986", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/996841.996876", "year": "2004", "article_id": "996876", "conference": "PLDI", "title": "Balancing register allocation across threads for a multithreaded network processor", "url": "http://dl.acm.org/citation.cfm?id=996876"}