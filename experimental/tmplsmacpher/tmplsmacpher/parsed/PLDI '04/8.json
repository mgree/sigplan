{"article_publication_date": "06-09-2004", "fulltext": "\n Cost Effective Dynamic Program Slicing* Xiangyu Zhang Rajiv Gupta Department of Computer Science The \nUniversity of Arizona Tucson, Arizona 85721 {xyzhang,gupta}@cs.arizona.edu ABSTRACT Although dynamic \nprogram slicing was .rst introduced to aid in user level debugging, applications aimed at improving software \nquality, reliability, security, and performance have since been iden\u00adti.ed as candidates for using dynamic \nslicing. However, the dy\u00adnamic dependence graph constructed to compute dynamic slices can easily cause \nslicing algorithms to run out of memory for realis\u00adtic program runs. In this paper we present the design \nand evaluation of a cost effective dynamic program slicing algorithm. This algo\u00adrithm is based upon a \ndynamic dependence graph representation that is highly compact and rapidly traversable. Thus, the graph \ncan be held in memory and dynamic slices can be quickly computed. A compact representation is derived \nby recognizing that all dy\u00adnamic dependences (data and control) need not be individually rep\u00adresented. \nWe identify sets of dynamic dependence edges between a pair of statements that can share a single representative \nedge. We further show that the dependence graph can be transformed in a manner that increases sharing \nand sharing can be performed even in the presence of aliasing. Experiments show that transformed dynamic \ndependence graphs explicitly represent only 6% of the dependence edges present in the full dynamic dependence \ngraph. When the full graph sizes range from 0.84 to 1.95 Gigabytes in size, our compacted graphs range \nfrom 20 to 210 Megabytes in size. Average slicing times for our algorithm range from 1.74 to 36.25 seconds \nacross several benchmarks from SPECInt2000/95. Categories and Subject Descriptors D.3.4 [Programming \nLanguages]: Processors Debuggers; D.2.5 [Software Engineering]: Testing and Debugging Debug\u00adging aids, \nTesting tools, Tracing General Terms Algorithms, Measurement, Performance Keywords dynamic dependence \ngraph, debugging, testing *Supported by grants from Intel, IBM, Microsoft, and NSF grants CCR-0324969, \nCCR-0220262, CCR-0208756, CCR-0105535, and EIA-0080123 to the Univ. of Arizona. Permission to make digital \nor hard copies of all or part of this work for personal or classroom use is granted without fee provided \nthat copies are not made or distributed for pro.t or commercial advantage and that copies bear this notice \nand the full citation on the .rst page. To copy otherwise, to republish, to post on servers or to redistribute \nto lists, requires prior speci.c permission and/or a fee. PLDI 04, June 9 11, 2004, Washington, DC, USA. \nCopyright 2004 ACM 1-58113-807-5/04/0006 ...$5.00.  1. INTRODUCTION The notion of program slicing was \n.rst proposed by Mark Weiser [25]. He introduced program slicing as a debugging aid and gave the .rst \nstatic slicing algorithm. Since then a great deal of research has been conducted on static slicing and \na survey of many of the proposed techniques and tools can be found in [23] and [12]. For C programs that \nmake extensive use of pointers, the highly con\u00adservative nature of static data dependency analysis leads \nto highly imprecise and considerably larger program slices. Since the main purpose of slicing is to identify \nthe subset of program statements that are of interest for a given application, conservatively computed \nlarge slices are clearly undesirable. Recognizing the need for accu\u00adrate slicing, Korel and Laski proposed \nthe idea of dynamic slicing [14]. The dependences that are exercised during a program exe\u00adcution are \nidenti.ed and a precise dynamic dependence graph is constructed. Dynamic program slices are constructed \nupon user s requests by traversing the dynamic dependence graph. It has been shown that the dynamic slices \ncan be considerably smaller than static slices [24, 12]. The importance of dynamic slicing extends well \nbeyond debug\u00adging of programs [2, 15]. Increasingly applications aimed at im\u00adproving software quality, \nreliability, security, and performance are being identi.ed as candidates for making automated use of \ndy\u00adnamic slicing. Examples of these applications include: detect\u00ading spyware that has been installed \non systems without the user s knowledge [13], carrying out dependence based software testing [8, 16], \nmeasuring module cohesion for purpose of code restructur\u00ading [11], and guiding the development of performance \nenhancing transformations based upon estimation of criticality of instructions [28] and identi.cation \nof instruction isomorphism [22]. The bene.t of using dynamic slicing is shown in Table 1. Each dynamic \nslice contains a subset of statements that are executed at least once. The number of unique statements \nthat are executed at least once (USE) and average dynamic slice size (SS) are given. SS contains 2.46 \nto 56.08 times fewer statements than USE. While the notion of dynamic slicing is very useful for the \nabove mentioned applications [2, 13, 8, 16, 11], an impediment to their widespread use in practice has \nbeen the high cost of computing them. Consider the cost data in Table 1. For program runs involv\u00ading \nexecution of 67 to 220 million statements, the sizes of dynamic dependence graphs required to carry out \ndynamic slicing take 0.84 to 1.95 Gigabytes of storage. Given the large sizes of these graphs, it is \nnot possible to keep them in memory. While for small program runs it may be possible to maintain dynamic \ndependences in mem\u00adory and use them in dynamic slicing, for realistic program runs this is not possible. \nTo address this problem recently we proposed the LP algorithm in [26] where the dynamic dependence graph \nis con\u00adstructed on-demand in response to dynamic slicing requests from Table 1: Cost of dynamic slicing. \nBenchmark Suite Statements Bene.t Costs Executed Unique Stat. Av. Slice USE/ Full Graph LP s Average \n(Millions) Exec. (USE) Size (SS) SS Size (MBs) Slicing Times (Min.) 300.twolf SPECInt2000 140.63 15,955 \n5,789 2.75 1,568.44 13.98 256.bzip2 SPECInt2000 67.19 1,420 25 56.08 1,296.14 9.19 255.vortex SPECInt2000 \n108.37 70,920 18,587 3.82 1,442.66 10.17 197.parser SPECInt2000 123.00 2,942 583 5.04 1,816.95 9.91 181.mcf \nSPECInt2000 118.57 2,179 858 2.54 1,535.84 12.32 164.gzip SPECInt2000 71.05 4,474 336 13.32 834.74 4.69 \n134.perl SPECInt95 220.08 21,984 3,588 6.13 1,954.40 25.21 130.li SPECInt95 124.91 10,215 2,849 3.59 \n1,745.72 11.28 126.gcc SPECInt95 131.24 151,420 26,436 5.73 1,534.37 12.04 099.go SPECInt95 138.15 49,577 \n20,158 2.46 1,707.36 10.67 the execution trace that is saved on disk. While this approach greatly reduces \nthe size of dynamic dependence graph held in mem\u00adory, the on-demand construction of the dynamic dependence \ngraph is quite slow since it requires repeated traversals of the trace stored on disk. To enable faster \ntraversal of the trace, we augmented the trace with summary information which allowed us to skip irrelevant \nparts of the trace during traversal. As shown by data in Table 1, we found that even after enabling faster \ntraversal, across the different benchmarks, on an average it took 4.69 to 25.21 minutes to com\u00adpute a \nsingle dynamic slice. A dynamic slicing algorithm would be cost effective if the dy\u00adnamic dependence \ngraphs could be compacted so that they are small enough to hold in memory and the design of the compacted \ngraphs is such that they can be rapidly traversed to compute dynamic slices. One approach proposed by \nresearchers sacri.ces the precision of dynamic data dependences to construct a dynamic dependence graph \nthat is greatly reduced in size [1]. However, recent work has shown that algorithms that sacri.ce precision \nin order to limit the graph size are ineffective as they can produce slices that are many times larger \nthan accurate slices [26, 19]. In this paper we present a practical dynamic slicing algorithm which is \nbased upon a novel representation of the dynamic depen\u00addence graph that is highly compact and rapidly \ntraversable. The contributions of this paper include the following key ideas on which the design of our \nalgorithm is based and the experimental evalua\u00adtion of the algorithm. Sharing a dependence edge across \nmultiple dynamic instances of a data dependence. In general, it is not merely suf.cient to re\u00admember \nwhether a pair of statements was involved in a dynamic (data or control) dependence. For computing dynamic \nslices it is also necessary to remember the speci.c execution instances of the statements that are involved \nin a dynamic dependence. We identify conditions under which we do not need to remember the execution \ninstances of statements involved in a dependence. Thus, a single representative edge can be shared across \nall dynamic instances of an exercised dependence. In particular, in these situations there is an one-to-one \ncorrespondence between all execution instances of a pair of statements involved in a dependence because \nthe statements involved are local to the same basic block. In presence of aliasing, multiple de.nitions \nof a variable may reach a use even if the def\u00adinitions and use are local to a basic block. We show that \nin such situations partial sharing is possible. Transformations for increasing sharing. It is possible \nto con\u00adstruct a transformed dynamic dependence graph in a manner that converts non-local dependence edges \ninto local dependence edges and therefore increases the amount of sharing. First we show that in some \nsituations non-local def-use dependence edges can be re\u00adplaced by local use-use edges. Second we show \nthat by performing path specialization we can convert non-local def-use dependence edges into local def-use \ndependence edges. To limit the increase in static code size due to path specialization, we apply this \ntransfor\u00admation selectively in a pro.le guided fashion. Third we show that in the presence of aliasing, \nthrough node specialization, full local sharing can be achieved. Experimental evaluation. Our experimental \nevaluation shows that once sharing of edges is achieved, the number of dependence edges is reduced to \nroughly 6% of total edges. When the full graph sizes range from 0.84 to 1.95 Gigabytes in size, our corresponding \ncom\u00adpacted graphs range from 20 to 210 Megabytes in size. Average slicing times for our algorithm range \nfrom 1.74 to 36.25 seconds across the benchmarks studied while average slicing times of the LP algorithm \nrange from 4.69 to 25.21 minutes. The remainder of the paper is organized as follows. Section 2 presents \nthe unoptimized dynamic slicing algorithm. In section 3 we identify the conditions that give rise to \nsharable dependence edges, develop transformations for increasing sharing, and present algorithm details. \nExperimental results are presented in section 4. Related work and conclusions are given in sections 5 \nand 6. 2. DYNAMIC SLICING USING FULL DY-NAMIC DEPENDENCE GRAPH We .rst begin by describing the dynamic \ndependence graph rep\u00adresentation used to capture the dynamic data and control depen\u00addences exercised \nduring program execution. Once we have de\u00adscribed this representation we will present a series of optimizations \nthat lead to a compact representation whose edges can be rapidly traversed during slicing. When the program \nbegins execution, the dynamic dependence graph (dyDG) merely consists of a .xed number of nodes corre\u00adsponding \nto the basic blocks in the program. As the program ex\u00adecutes, the graph is transformed by introducing \nedges for the dy\u00adnamically exercised control and data dependences. Since the same dependence may be exercised \nmany times, the edge is labeled with additional information to uniquely identify the execution instances \nof the statements which are involved in the dependence. Execution instances are identi.ed by generating \ntimestamps. A global times\u00adtamp value is maintained and each time a basic block is executed, it is assigned \na new timestamp value generated by incrementing the current timestamp value. Each execution of a statement \n(also every de.nition and use) in the basic block is uniquely identi.ed by this timestamp. Thus, dynamic \ndependences exercised can be captured by introducing the following dependence edges: Data dependence \nedge dmun(tm,tn) represents the use of a value during execution of statement un with timestamp value \ntn that was de.ned during the execution of statement dm with timestamp value tm. We will refer to the \nsubgraph of dyDG showing dynamic data dependence edges as the dyDDG. Control dependence edge cmdn(tm,tn) \nrepresents the execution of statement dn with timestamp value tn that was control dependent upon execution \nof conditional statement cm with timestamp value of tm. We refer to the subgraph of dyDG showing dynamic \ncontrol dependences as the dyCDG. Let us consider examples of dyDG s. We present two examples, one to \nillustrate dyDDG and the other to illustrate dyCDG. The control .ow graph in Fig. 1(a) represents a function \nthat contains static dependences due to de.nitions and uses of variable x. Let us assume that this function \nis invoked three times during execution and the paths taken in each of the executions and the timestamps \nassigned to the basic block executions are as shown. The corre\u00adsponding dyDDG contains nodes for the \nbasic blocks and edges for data dependences where the labels on the edges uniquely identify the dynamically \nexercised data dependences. Similarly the control .ow graph in Fig. 1(b) represents a function that is \ninvoked three times with execution histories as shown. The corresponding dy-CDG containing labeled control \ndependence edges is shown. 1 1 (10,10) = X X = (20,20) (30,30) (10,11)  (30,31) = X 3 X = (10,11) = \nX (30,31)  (10,12) (30,32)4 = X (21,22) Nodes Executed Timestamps 1 - 2 - 4 10 - 11 - 12 1 - 3 - 4 \n20 - 21 - 22 1 - 2 - 4 30 - 31 - 32 (a) CFG; Sample execution history; and dyDDG. 1 (10,11) (20,21) \n2 if P (30,31) (11,12) 3 (21,22) 4 if Q Nodes Executed Timestamps 1 - 2 - 3 - 5 10 - 11 - 12 - 13 \n1 - 2 - 4 - 5 20 - 21 - 22 - 23 1 - 3 - 5 30 - 31 - 32 5 (b) CFG; sample execution history; and dyCDG. \nFigure 1: Examples of dyDDG and dyCDG. For enabling program slicing, for each variable (data address) \nv, we remember the statement s and its corresponding timestamp ts that most recently assigned a value \nto v. The program slice for v can be computed by starting at execution instance s(ts) and travers\u00ading \nthe dyDG backwards following the relevant dynamic data and control dependence edges. Recall that if execution \ninstance s(ts) is dependent upon execution instance s (ts. ), an edge from s(ts) to s (ts. ) denoted \nby s . s(ts. ,ts) is present in the dyDG. All state\u00adments (including s) that are visited during this \ntraversal are added to the dynamic slice. This computation, denoted as Slice(s(ts)), is expressed below: \n[ Slice(s(ts)) = {s}. Slice(s (ts. )). .s.s(t . ,ts) s Let us consider the space and time costs of dynamic \nslicing brie.y. The space needed to hold dynamic dependence graph of a given program in general cannot \nbe statically bounded across all execu\u00adtions since as the program continues to execute, it continues \nto ex\u00adercise dependences which must be captured by the graph. Thus, it is clear that space is spent on \nlabeling the edges and the lists of labels continue to grow as the program executes. The time spent on \ndynamic slicing is simply the time spent on traversing the graph. In general it is clear that this time \nis not bounded across all executions because it depends on the number of dynamic dependence edges in \nthe graph which cannot be bounded across all program execu\u00adtions. However, it is also clear that the \ntime for traversing a single edge can be signi.cant as we must examine labels of all outgoing edges to \nlocate the relevant edge for traversal. Therefore in order to limit the space and time costs of dynamic \nslicing we must develop techniques for the following: To save space we must develop techniques that \nwill lead to the saving of fewer dynamic labels (i.e., timestamp pairs).  To save time we must develop \ntechniques that avoid search\u00ading through large number of dynamic labels to locate the rel\u00adevant dynamic \ndependence edge to follow.  In the next section we present a series of optimizations that reduce the \nnumber of labels that need to be stored in the dyDG. Once these optimizations are presented, it will \nalso become clear that after op\u00adtimization, the traversal of edges can be carried out in a much more \ntime ef.cient manner. 3. OPTIMIZED DYNAMIC DEPENDENCE GRAPH REPRESENTATION While in general we need to \nremember all dynamic instances of all dependences, next we show that all dynamic instances need not be \nremembered explicitly. We develop optimizations that have the effect of eliminating timestamp pairs. \nThese optimizations can be divided into the the following three categories: Infer -Static edge is introduced \nfor a dependence and the times\u00adtamp pairs corresponding to the dynamic instances of the de\u00adpendence are \ninferred and thus need not be explicitly remem\u00adbered. Transform -While some timestamp pairs cannot be \ninferred from the original static dependence graph, transformations can be applied to the static graph \nso that the timestamp pairs can be inferred from the transformed graph. Thus, these transfor\u00admations \nenable inferring of timestamp pairs. Redundant -There are situations in which different dependence edges \nare guaranteed to have identical timestamp pairs. Re\u00ad dundant copies of timestamp pairs can thus be discarded. \n  3.1 dyDDG Optimizations Given an execution instance of a use u(tu), during dynamic slic\u00ading, we need \nto .nd the corresponding execution instance of the relevant de.nition d(td). There are two steps to this \nprocess: (.nd\u00ading d) in general many different de.nitions may reach the use but we need to .nd the relevant \nde.nition for u(tu); and (.nding td) even if the relevant de.nition d is known we need to .nd the ex\u00adecution \ninstance of d, i.e. d(td), that computes the value used by u(tu). The following optimizations show how \nthe above determi\u00adnations can be made even in the absence of some timestamp pairs. 3.1.1 (OPT-1) Infer \n(OPT-1a) Infer Local Def-Use for Full Elimination. Consider a de.nition d and a use u that are local \nto the same ba\u00adsic block, d appears before u, and there is no de.nition between d and u that can ever \nprevent d from reaching u. In this case there is one-to-one correspondence between execution instances \nof d and u. Since d and u belong to the same basic block, the times\u00adtamps of corresponding instances \nare always the same, i.e. given a dynamic data dependence du(td,tu) it is always the case that td = tu. \nTherefore, given the use instance u(tu), the correspond\u00ading d is known statically and the corresponding \nexecution instance is simply d(tu). Thus we do not need to remember dynamic in\u00adstances individually \nit is enough to introduce a static edge from u to d. In the dynamic slicing algorithm based upon the \nfull dependence graph, we began with a set of nodes (basic blocks) and introduced all dependence edges \ndynamically. To take advantage of the above optimization we simply introduce the edge from u to d statically \nprior to program execution. No new information will be collected or added at runtime for the use u as \nthe edge from u to d does not need any timestamp labels. In other words all dynamic instances of def-use \nedge from u to d are statically replaced by a single shared representative edge. The impact of this optimization \nis illustrated using the dyDDG of Fig. 1(a). As shown in Fig. 2, basic block 1 contains a labeled local \ndef-use edge which is replaced by a static edge that need not be labeled by this optimization. We draw \nstatic edges as dashed edges to distinguish them from dynamic edges. 11 Figure 2: Effect of applying \nOPT-1a. (OPT-1b) Infer Local Def-Use for Partial Elimination. In the above optimization it was important \nthat certain subpath was free of de.nitions of the variable involved (say v) so that a de\u00adpendence edge \ninvolving v that is free of labels could be used. In programs with pointers, the presence of a de.nition \nof a may alias of v may prevent us from applying the optimization even though at runtime this de.nition \nmay rarely rede.ne v. To enable the appli\u00adcation of preceding optimization in presence of de.nitions \nof may aliases of v we proceed as follows. We introduce a static unlabeled edge from one de.nition to \nits potential use. If at runtime another may alias turns out to truly refer to v, additional dynamic \nedges labeled with timestamp pairs will be added. The effect of this opti\u00admization is that the timestamp \nlabels corresponding to the statically introduced data dependence are eliminated while the labels for \nthe dynamically introduced data dependence edge are not, i.e. labels have been partially eliminated. \nDuring traversal, .rst the labels on dynamic edges are examined to locate the relevant dependence. If \nthe relevant dependence is not found, then it must be the case that the dependence involved corre\u00adsponds \nto the static edge which can then be traversed. It should also be clear that greater bene.ts will result \nfrom this optimization if the edge being converted to an unlabeled edge is the more frequently exercised \ndependence edge. Thus, if pro.le data is available we can make use of it in applying this optimization. \nIn the example shown in Fig. 3 let us assume that *P is a may alias of X and *Q is a may alias of Y . \nFurther assume that the code fragment is executed twice resulting in the introduction of the following \nlabeled dynamic edges: between the uses of X and de.\u00adnitions of X and *P ; and between the uses of Y \nand the de.nitions of Y and *Q. We introduce the following static unlabeled edges: from the use of X \nto the de.nition of X (as in OPT-1a); and later the use of Y to the earlier use of Y (as in OPT-2b described \nlater). The dynamic edges introduced are: from the use of X to the def\u00adinition of *P ; and from the later \nuse of Y to the de.nition of *Q. Thus some, but not all, labels have been removed. Figure 3: Effect \nof applying OPT-1b.  3.1.2 (OPT-2) Transform (OPT-2a) Transform Local Def-Use for Full Elimination. \nWhile the above optimization was able to achieve partial elimina\u00adtion of labels, next we present an optimization \nthat can eliminate all of the labels present in situations with aliasing. Full elimina\u00adtion of labels \nis achieved through specialization. Given a use of variable v in a node (basic block) that is reachable \nby two distinct de.nitions (say d1 and d2) that may de.ne v, we create two copies of the node. One copy \nis used to exclusively represent dynamic dependences between d1 and the use of v while the other copy \nis used to represent only the dynamic dependences between d2 and use of v. Since in each copy of the \nnode the use of v is always data dependent upon the same de.nition point of v, we do not need to maintain \nthe timestamp labels on these edges. Consider the example shown in Fig. 4. One use of X is reached by \nthe de.nition of X in statement X = f(Y ) while the second use of X is reached by the de.nition of X \nin statement *P = g(Z).By making two copies of the basic block that contains the two de.ni\u00adtions and \nthe use, we are able to introduce static edges to represent both of the above dependences and thus the \nlabels corresponding to these edges are eliminated. Note that the dependence edges cor\u00ad Z = Y = (20,21) \n (20,21) (10,11) (10,11) X = f(Y) X = f(Y) *P = g(Z) *P = g(Z) (21,21)  Figure 5: Effect of applying \nOPT-2b. = X = X (11,11) Figure 4: Effect of applying OPT-2a. responding to the uses of Y and Z in the \nbasic block must also be replicated and appropriately labeled. In the above example, two copies of the \nnode were suf.cient to eliminate the local labels. In general, if uses of multiple variables have multiple \nde.nitions due to aliasing, we will require greater d cannot be killed along p prior to reaching u), \nwe can convert this non-local dynamic edge into a local dynamic edge du(td,tIf the list of labels is \nvery long, node replication may be justi.ed. for path p. While for the original edge du(td,tu) the values \nof td number of copies to be created to eliminate all of the local labels. ) u are not equal, for the \nmodi.ed edge du(td,tare equal. At runtime if the dependence between d and tu ) the values However, if \nthere are only few labels, partial elimination may be u d and tuof tpreferable to full elimination. Since \nthe above optimizations show that timestamp labels on lo\u00adcal dependences edges can be eliminated, we \nfurther develop op\u00adtimizations that convert non-local dependence edges into local de\u00adpendence edges. \nOnce non-local dependence edges have been con\u00adverted to local dependence edges, their labels can be eliminated \nusing the above optimizations. (OPT-2b) Transform Non-Local Def-Use to Local Use-Use. Consider two uses \nu1 and u2 such that u1 and u2 are local to the same basic block, u1 and u2 always refer to the same location \ndur\u00ading any execution of the basic block, and there is no de.nition be\u00adtween u1 and u2 that can cause \nthe uses to see different values. Now let us assume that a non-local de.nition d reaches the uses u1 \nand u2. In this case each time u1 and u2 are executed, two non\u00adlocal def-use edges du1(td,tu1 ) and du2(td,tu2 \n) are introduced. Let u1 appear before u2. We can replace the non-local def-use edge du2(td,tu2 ) by \na local use-use edge u1u2. The latter does not re\u00adquire a timestamp label because tu1 is always equal \nto tu2 . By re\u00adplacing a non-local def-use edge by a local use-use edge, labels on the edge are eliminated. \nDuring slicing an extra edge (the use-use edge) will be traversed. Moreover, use-use edges are treated \ndiffer\u00adently. In particular, a statement visited by traversing a use-use edge is not included in the \ndynamic slice. Using static analysis we can identify uses local to basic blocks which always share the \nsame reaching de.nition. Once having identi.ed these uses we statically introduce use-use edges from \nand u is established along path p, then that dependence would be represented by an unlabeled edge local \nto node for path p. However, if the dependence is established along some path other than p,it is represented \nusing a labeled non-local edge between bd and bu. The consequence of earlier optimizations was that the \ninitial graph that we start out with contains some statically introduced data de\u00adpendence edges. The \nconsequence of this optimization is that in\u00adstead of starting out with a graph that contains only basic \nblock nodes, we start out with a graph that contains additional nodes corresponding to paths that have \nbeen specialized. During execu\u00adtion we must detect when specialized paths are executed (we will present \nan algorithm to do so in section 3.3). This is necessary for construction of the dyDG due to the following \nreasons. The value of global timestamps must be incremented after the execution of code corresponding \nto a node in the graph. Thus, we no longer will increment the timestamp each time a basic block is executed \nbe\u00adcause nodes representing specialized paths contain multiple basic blocks. At runtime we must distinguish \nbetween executions of a block that correspond to its appearance in a specialized path from the rest of \nits executions so that when we introduce a dynamic data dependence edge in the graph we know which copy \nof the block to consider. 1 Nodes Executed Timestamps 1.2.4 10 1 - 3 - 4 20 - 21 - 22 1.2.4 30  uu \n1.2.4 1 edge any dynamic information corresponding to the later uses. The impact of this optimization \nis illustrated by further optimiz\u00ad = X(30,32) ing the dyDDG obtained by applying OPT-1a. As shown in \nFig. 5, 3 basic block 2 contains a two uses of X each having the same reach- X = uu edge ing de.nition \nfrom block 1. The labeled non-local def-use edge from the second use to the de.nition is replaced by \nan unlabeled static use-use edge by this optimization. We draw use-use edge us\u00ad 4 = X ing a dashed edge \nto indicate it is static and further indicate that it is a use-use edge. Figure 6: Effect of applying \nOPT-2c. (OPT-2c) Transform Non-Local Def-Use to Local Def-Use. Given non-local def-use edge du(td,tu) \nbetween basic blocks bd The impact of this optimization is illustrated by further optimiz\u00adand bu, by \ncreating a specialized node for the path (say p) that ing the optimized dyDDG from Fig. 5. As shown in \nFig. 6, if we when executed always establishes the def-use edge du(td,tu) (i.e., create a specialized \nnode for path along basic blocks 1, 2 and 4, many of the previously dynamic non-local def-use edges \nare con\u00adverted to dynamic local def-use edges within this path. The def-use edges established along this \npath can now be statically introduced within the statically created node representing this path. Thus, \nthe timestamp labels for these def-use edges are no longer required. Since block 2 can only be executed \nwhen path 1-2-4 is executed, we do not need to maintain a separate node for 2 once node for path 1-2-4 \nhas been created. However, the same is not true for blocks 1 and 4. Therefore we continue to maintain \nnodes representing them to capture dynamic dependences that are exercised when path 1-2-4 is not followed. \nAfter applying multiple optimizations to the dyDDG of Fig. 1(a), we have eliminated all but one of the \nlabels in the dyDDG. In fact this label can also be eliminated by creating another specialized node for \npath containing blocks 3 and 4. Finally it should be noted that the above optimization only elimi\u00adnates \nlabels corresponding to dependence instances exercised along the path for which a specialized node is \ncreated. Thus, greater ben\u00ade.ts will be derived if the path specialized is a frequently executed path. \nAs a result, selection of paths for specialization can be based upon pro.le data.  3.1.3 (OPT-3) Redundancy \n(OPT-3) Redundancy Across Non-Local Def-Use Edges. In all the optimizations considered so far we have \nidenti.ed and created situations in which the labels were guaranteed to have a pair of identical timestamps. \nNow we present an optimization which identi.es pairs of dynamic edges between different statements that \nare guaranteed to have identical labels in all executions. Thus, the statements can be clustered so that \nthey can share the same edge and thus a single copy of the list of labels. Given basic blocks bd and \nbu such that de.nitions d1 and d2 in bd have corresponding uses u1 and u2 in bu. If it is guaranteed \nthat along every path from bd to bu either both d1 and d2 will reach u1 and u2 or neither d1 nor d2 will \nreach u1 and u2, then the labels on the def-use edges d1u1 and d2u2 will always be identical. The example \nin Fig. 7 shows that the uses of Y and X always get their de.nitions from the same block and thus dependence \nedges for Y and X can share the labels. A shared edge between clusters of statements (shown by dashed \nboxes) is introduced by this optimization. Figure 7: Effect of applying OPT-3.  3.2 dyCDG Optimizations \nControl dependences are introduced at the granularity of basic blocks. Next we present the optimizations \nthat enable introduction of static unlabeled control dependence edges. 3.2.1 (OPT-4) Infer (OPT-4) Infer \nFixed Distance Unique Control Ancestor. Often basic blocks (nodes) in a control .ow graph have a unique \ncontrol ancestor. Whenever a node is control dependent upon a unique conditional predicate, the control \ndependence edge can be introduced statically. In addition, sometimes the difference in the timestamps \ncorresponding to a dynamic control dependence is a compile time constant. Thus, we can remember the difference \nvalue and avoid labeling the edge with a timestamp pair each time the dependence is exercised. In particular, \nfor a dynamic control dependence edge cd(tc,td) which satis.es the above conditions, tc + d = td because \ntimestamp is incremented by d whenever after the execution of the predicate when control transfers to \nthe depen\u00addent basic block. When this optimization is applied to the example from Fig. 1(b), the d values \nof edges from node 2 to node 1 and node 4 to node 2 are determined to be the value 1. Figure 8: Effect \nof applying OPT-4.  3.2.2 (OPT-5) Transform (OPT-5a) Transform Multiple Control Ancestors. If a node \nhas multiple control ancestors, we can replicate the node creating specialized copies for each of the \ncontrol ancestors. Static control dependence edges can now be introduced and their d values can be remembered. \nThe dynamic timestamp labels are no longer required. Continuing with the example from Fig. 8, the labeled \nedges corresponding to the two control ancestors of node 3 can be replaced by static edges after replicating \n3 as shown in Fig. 9. 1 1 d=1 3 d=1 d=1 4 d=0 5 Figure 9: Effect of applying OPT-5a. Specialization \nalso enables another optimization for control de\u00adpendences which is analogous to OPT-2b. Following specializa\u00adtion, \na node representing a path may contain multiple basic blocks that are control equivalent [9]. Instead \nof using separate non-local edges for two control equivalent blocks, we can replace the non\u00adlocal edge \nfor the second block by a local edge which points to the .rst block. (OPT-5b) Transform Varying Distance \nUnique Control Ances\u00adtor. In optimization OPT-4 we had shown how to handle the case when a node had a \nunique control ancestor which was at a constant distance from the node. It is possible that there are \nmultiple paths from the control ancestor to the control dependent node causing the former to be at varying \ndistances from the latter depending upon the path taken. In this case we can apply specialization to \ncreate copies of the dependent node such that each copy created is at a constant distance from the control \nancestor. In Fig. 10, node 4 is at distance 3 from node 1 along path 1.2.3.4 and at distance 2 from \nnode 1 along path 1.2.4. By specializing path 1.2.3.4 as shown in the .gure we are able to convert the \ncontrol dependence edge from 4 to 1 into a pair of control dependence edges that are each at constant \ndistances of 2 and 0.  3.2.3 (OPT-6) Redundant (OPT-6) Redundancy Across Non-Local Def-Use and Control \nDependence Edges. In OPT-3 we showed how two non-local data dependence edges can share common labels. \nThe same approach can be extended to allow a non-local control dependence edge to share labels with a \nnon-local data dependence edge as long as these edges connect the same pair of blocks. An example illustrating \nthis optimization is shown in Fig. 11.  3.3 Completeness of the Optimization Set In an unoptimized \ndyDG any dependence edge may have a long list of labels attached to it. To compact the graph we may wish \nto apply transformations that can eliminate this list of labels. Given this requirement, it is important \nthat we have available to us an opti\u00admization (or a series of optimizations) that can eliminate any list \nof labels. We consider a set of optimizations to be complete if for any given list of labels, we can \n.nd a sequence of optimizations in the optimization set that can be used to eliminate the list of labels. \nThe completeness property of the optimization set is important because it tells us that we have suf.cient \noptimizations and do not need to continue developing additional ones. In fact we can say that given an \noptimization set that is complete, it is possible to convert any labeled dyDG into one which has no timestamp \npair labels. [Theorem] (Completeness). The set of optimizations OPT-1 through OPT-6 is complete. [Proof] \nThere are two types of edges in the dyDG, data depen\u00addence and control dependence. Lets us consider each \nof the edge types and show that a list of labels associated with an edge can be eliminated using the \noptimizations described. Data dependence labels. (Local Edge) If the labels are associ\u00adated with an edge \nthat is local to a basic block the labels can be al\u00adways removed because either they can be inferred \nand hence OPT\u00ad1a is applicable or they can be entirely converted to labels that can be inferred by carrying \nout specialization using OPT-2a. (Non-local Edge) If the labels are associated with an edge that is non-local, \ni.e. it connects two different basic blocks, then it can always be con\u00adverted into a local edge by applying \npath specialization using OPT\u00ad2c. Once it has been converted to a local edge, its labels can always be \neliminated as described above. Thus, we conclude that labels as\u00adsociated with all data dependence edges \ncan be eliminated by using the optimizations we provide .......................................................(1) \nControl dependence labels. (Fixed Distance from Unique An\u00adcestor) If a node is at a .xed distance from \nits control ancestor, then the labels can be inferred and hence optimization OPT-4 is applica\u00adble. (Others) \nIf the node has multiple control ancestors and/or it is at a varying distance from its control ancestors, \nthen path special\u00adization using optimizations OPT-5a and OPT-5b can always be ap\u00adplied to convert the \nlabels into ones that can be inferred. Thus, we conclude that labels associated with all control dependence \nedges can be eliminated using the optimizations we provide.................(2) From (1) and (2) we conclude \nthat the optimization set we have developed is complete. . It is worth noting that in the above proof \nno reference was made to optimizations OPT-1b, OPT-2b, OPT-3, and OPT-6. These opti\u00admizations are not \nneeded for completeness. They are provided as cheaper alternatives to specialization in situation where \nthey may be found to be applicable. 3.4 dyDG Construction and Dynamic Slicing Static Component of dyDG. \nTo construct the static component of dyDG we need to perform the following analyses: (i) reaching de.\u00adnitions \nanalysis is carried out to compute def-use information. May alias information is needed to carry out \nthis analysis; (ii) reaching uses analysis is carried out to compute use-use information; (iii) simultaneous \nreachability analysis is carried out to identify situa\u00adtions in which a pair of non-local data dependence \nedges can share labels; and (iv) postdominator analysis is carried out to compute control dependences \n[9]; and (v) must reachability analysis is car\u00adried out to identify situations in which a pair of non-local \ndata and control dependence edges can share labels. Except for simultaneous reachability analysis all \nother analyses are standard. Therefore next we describe the details of the simul\u00adtaneous reachability \nanalysis. Given a pair of de.nitions d1 and d2 in block s, with corresponding uses u1 and u2 in block \nd, the edges d1u1 and d2u2 will share identical labels if and only if whenever s and then d are executed \neither both data dependences are exercised or neither of them are exercised. We consider the subgraph \nconsist\u00ading of s, d, and all nodes along paths from s to d this is similar to the way chops are computed \n[21]. We refer to set of nodes in this subgraph excluding s as reach(sd). KILLn is a two bit value where \nbits correspond to the two de.nitions; bit value of 1 indi\u00adcates that n does not kill the de.nition while \n0 indicates that n kills the de.nition. The following equations compute for each node in reach(sd) a \ndata .ow value which is .{11, 10, 01, 00}. T .n . succ(s) reach(sd),xn = {11} .n . reach(sd) - succ(s), \nS xn = {KILLp . x: x. xp}p.pred(n)nreach(sd) If the solution for node dis {11} (i.e., both de.nitions \nalways reach d)or {11,00} (either both de.nitions reach dor neither reaches d), then the two dependence \nedges will always have identical labels. On the other hand, if the solution contains 10 (01), then there \nis a path from s to d along which d1 (d2) reaches d but d2 (d1) does not reach d. This analysis does \nnot need to be carried out for every pair of de.nitions but rather for those that appear in the same \nbasic block and have corresponding uses in the same basic block. More\u00adover, transitivity can be used \nto further reduce the pairs considered (i.e, if (d1u1,d2u2) can share labels and (d2u2,d3u3) can share \nlabels, then so can (d1u1,d3u3)). Given the results of the above analyses, we have enough informa\u00adtion \navailable to construct the static component of dyDG. However, we observe that the static component of \ndyDG must be constructed once and then used repeatedly to capture dynamic dependence his\u00adtories of different \nprogram runs. In other words the optimizations we have developed must be applied to construct the static \ncompo\u00adnent. While many of the optimizations can be applied for every opportunity that exists, there is \na subset of optimizations that must be applied selectively. In particular all of the specialization based \noptimizations should be applied only if we expect that their ap\u00adplication will result in more compaction \nthan the graph expansion that is caused by specialization. Therefore we should apply these optimizations \nin a pro.le guided fashion. We specialized all Ball Larus paths [3] that were found to have a non-zero \nfrequency dur\u00ading a pro.ling run. This approach works well because nearly all of the optimizations requiring \nspecialization, are actually based upon path specialization. There are two optimizations that require \ndata dependence pro.les OPT-1b and OPT-2a. Our implementation does not make use of data dependence pro.les \nyet. Instead we ap\u00adplied OPT-1b such that data dependence edges created due to must aliases were given \npriority for partial elimination over edges due to may aliases. We do not apply OPT-2a because we do \nnot have an effective static heuristic for applying OPT-2a. Dynamic Component of dyDG. As the program \nexecutes, it sends a trace of one basic block at a time to an online algorithm which builds the dyDG. \nThis online algorithm must carry out two tasks. First it must buffer the basic block traces until it \nis determined which node in the static dyDG must be augmented with additional dynamic edges. This is \nnecessary because there may be multiple copies of a basic block due to specialization. Second it maintains \nthe timestamp value and uses it to create the labels corresponding to the dynamic edges. Consider the \nexample shown in Fig. 12. Let us assume that for the CFG shown the static graph constructed has nodes \nfor each of the basic blocks and another node for path 1245 is created due to specialization. When the \nprogram executes and generates a trace for block 1, we cannot at this time introduce dependence edges \nfor statements in 1 because we do not know where to introduce these edges in copy of statements of 1 \nin node 1 or node 1245. The trace must be buffered till it is clear that either the program has followed \npath 1245 or that it has taken some other path. To detect when it is the right time to introduce edges \nwe can construct the tree shown in Fig. 12(c). The online algorithm is initially at the root of the tree. \nDepending upon the basic block executed, the appropriate edge labeled with that block is traversed and \nthe trace is buffered. When we reach a leaf, it is time to process the buffered trace. The leaf is labeled \nwith the list of nodes in the dyDG from which the edges introduced will originate. For example if basic \nblocks 1, 2, 4, and 5 are executed the edges originate from node 1245 while if blocks 1, 2, 4, and 6 \nare executed the edges originate from nodes 1, 2, 4, and 6.  1 6 2 5 6 1245 3 4 2 1 2,4 53 2 4 3 1,3 \n5 6 4 5 1245 1,2,4,6 6 (a) CFG (b) Nodes (c) Find and update tree. Figure 12: Introducing dynamic edges. \nDynamic Slicing. During the computation of a dynamic slice, the dyDG is traversed backwards to identify \nthe statements that belong to the slice. The set of dependence edges Es going backwards from a statement \ns can be partitioned into subsets of edges Eus corre\u00adsponding to each use us and subset of edges Ecs \ncorresponding to SS all control ancestors of s. In other words, Es = Eus Ecs . .us Given an execution \ninstance of s, say s(ts), for each subset of edges corresponding to a dependence in Es (i.e., Eus or \nEcs ), we need to locate the speci.c edge s . sin Es that must be followed. Moreover, since the edge \ns . s may have been exercised many times, we must identify the speci.c dynamic instance of this edge \ns . s(ts. ,ts) that is involved in the dependence. l l l l (a) (b) (c) Figure 13: Traversing dependence \nedges. There are three situations that arise as shown in Fig. 13. Let us say we are considering a subset \nof edges Ed(s) from Es due to a dependence d involving s (i.e., Ed(s) corresponds to some Eus or Ecs \n). In the .rst case, Ed(s) contains a single static edge s . s which is thus not labeled dynamically \nwith timestamp pairs. The traversal is straightforward as there is only one choice and the timestamp \nts. in s . s(ts. ,ts) can be easily determined (ts. = ts for data dependences and ts. = ts - d for control \ndependences). In the second case there are multiple dynamic and thus labeled edges (say ssand ss). The \nlabels on these edges (l. and l) must be searched to locate the relevant edge and its instance s . s(ts.. \n,ts) or s . s(ts... ,ts). In the third case, there is a single unlabeled static edge s . s as well as \nmultiple labeled dynamic edges (say s . s and ss). The labels on ssand ss(i.e., l. and l) are .rst searched. \nIf we .nd the relevant dependence ss(ts.. ,ts) or ss(ts... ,ts), we are done; otherwise we select the \nstatic edge s . s and compute the value of timestamp ts. in s . s(ts. ,ts) as discussed in the .rst case. \nIt is worth noting that removal of explicit timestamps, as is car\u00adried out by the series of optimizations \nwe have developed, not only makes the dependence graph more compact, it also speeds up the traversal \nprocess as fewer timestamps are searched to locate the rel\u00adevant timestamp. The .rst and third cases \ncontain a static unlabeled edge and hence the search is reduced while the second case repre\u00adsents the \nsituation in which no reduction in search is achieved as all dynamic labels are saved and hence potentially \nsearched. We have described the key points of the traversal process. Now we summarize our dynamic slicing \nalgorithm. In order to enable computation of slices, for each variable v we maintain the triple <s, n, \nts > such that v was last de.ned by statement s in node n at time ts. The dynamic slice for v is computed \nas shown be\u00adlow. Notice the manipulation of timestamps for unlabeled edges and also note that if s . \ns is a uu-edge then s . is not added to the slice. The sharing of labels between different edges is not \nexplicitly re.ected in the algorithm below since it is an implementation detail which affects how the \ntimestamp labels on edges are accessed. In the algorithm below, sSlice(s(ts)) represents the set of statements \nthat belong to the dynamic slice of execution instance s(ts) and eSlice(E, ts) represents the subset \nof statements in the dynamic slice of execution instance s(ts) that are contributed by the traver\u00adsal \nof the subset of dynamic edges E from s(ts). [ Slice(v)= {s} sSlice(s(ts)) 8 SS > if Es = Eus Ecs =fthen \n> > .us >S S > > < eSlice(Eus ,ts) eSlice(Ecs ,ts) .us sSlice(s(ts)) = > else > > > > f > : endif 8 > \n if . labeled edge s. s(ts. ,ts) . Ethen >S > . > sSlice(s (ts)) {s } > > > > elseif . unlabeled edge \ns. s . Ethen > < case s sis : S eSlice(E, ts)= > du edge : sSlice(s (ts)) {s } > > > > uu edge : sSlice(s \n(ts)) >S > > cd edge : sSlice(s (ts - ds)) {s } > .s : endif  Using Shortcuts to Speed Up Traversal. \nFinally we present an optimization that is used to speed up the traversal of the dyDG by the above slicing \nalgorithm. As we have shown, our optimized al\u00adgorithm introduces some dependence edges statically while \nothers are introduced dynamically. It is possible that at some points in the dyDG multiple edges are \ntraversed in sequence that are all static edges. If this is the case, the contributions to the dynamic \nslice when these edges are traversed is also known statically and always the same. Therefore, to speed \nup traversal of these edges, we intro\u00adduce a shortcut edge that replaces the traversal of multiple static \nde\u00adpendence edges by the traversal of a single shortcut edge. The edge is labeled with the set of statements \nthat are skipped by the shortcut edge so that the dynamic slice can be appropriately updated when the \nshortcut edge is traversed. In the example shown in Fig. 14, corresponding to the sequence of two static \nedges S3 . S2 . S1, we introduce a shortcut edge S3 . S1 labeled with {S2}.  4. EXPERIMENTAL RESULTS \nWe have implemented the algorithm described using the Tri\u00admaran compiler infrastructure which handles \nprograms written in C. We make use of this infrastructure because we have implemented several other dynamic \nslicing algorithms using this system [26, 27]. The programs we use in our experiments include 6 programs \nfrom the SPECInt2000 suite (the .rst 6 programs in Table 1) and 4 programs from the SPECInt95 suite (the \nlast 4 programs in Ta\u00adble 1). The reason we could not run remaining SPECInt2000 programs is because they \ncannot be successfully compiled using the version of Trimaran being used. All experiments were performed \non a workstation with 2.4 GHz Pentium IV and 2 GB RAM. The goal of our experiments was to determine the \nspace and time costs of our dynamic slicing algorithm which we will refer to as the optimized algorithm \nOPT. We also compare the performance of OPT with the traditional slicing algorithm (FP) for short program \nruns and the best overall algorithm (LP) for long program runs ac\u00adcording to [26]. 4.1 Performance Evaluation \nof OPT Graph sizes. We measured the size of the full dynamic depen\u00addence graph and compared it against \nthe size of the optimized graph obtained after application of all the optimizations described in this \npaper. These graph sizes are shown in Table 2. As we can see the graphs sizes are reduced by factors \nranging from 7.46 to 93.40. As a result, while the full graph sizes range in size from 0.84 to 1.95 Gigabytes, \nthe optimized graphs range from 20 to 210 Megabytes in size. Table 2: dyDG size reduction. Program Graph \nSize (Megabytes) Ratio Before After Before/After 300.twolf 1568.44 210.21 7.46 256.bzip2 1296.14 50.48 \n25.68 255.vortex 1442.66 64.81 22.26 197.parser 1816.95 69.81 26.03 181.mcf 1535.84 170.29 9.02 164.gzip \n834.74 51.57 16.19 134.perl 1954.40 20.92 93.40 130.li 1745.72 96.50 18.09 126.gcc 1534.37 74.71 20.54 \n099.go 1707.36 131.24 13.01 The substantial reduction in the graph size is due to the fact that roughly \nonly 6% of the dynamic dependences are explicitly main\u00adtained after the proposed optimizations are applied. \nThe contri\u00adbutions of the various optimizations in reducing the graph size are shown in Fig. 15. Here \n100% corresponds to the full graph size and dyncorresponds to the size of the graph after application \nof all the optimizations. The other points in the bar graph show how the size reduces as different categories \nof optimizations are applied one by one. As we can see, OPT-1 is very effective as it reduces graph Relative \nsizes of dyCDG and dyDDG sizes to roughly 35% of the full graph size. However, the other op-100% timizations \nalso contribute signi.cantly as they together reduce the 90% graph size from 35% to 6% of the full graph \nsize. It is important 80% 70% to point out that the distribution obtained is dependent upon the or\u00ad 60% \nder in which the optimizations are applied since some cases can be 50% handled by multiple optimizations. \n40% 30% Effects of optimizations  20% 10% 100% 90% 0%80% OPT-1 70% OPT-2 60% OPT-3 OPT-4 50% OPT-5 \n100%40% OPT-6 30% DYN dyCDG dyDDG  80% 70% 20% 10% 60%0% 50% Figure 15: Effect of various optimizations \non dyDG size. 10% 0% We also observe that the majority of the savings comes from ap\u00adplying optimizations \nfor dynamic data dependence edges. This is because the dynamic control dependences represent a small \nfrac\u00ad tion of information contained in dyDG (see the .rst graph in Fig. 16). 90% This is not surprising \nbecause in our implementation control depen\u00ad 80% dence edges are introduced at basic block granularity \nwhile data 70% dependence edges have to be introduced at statement granularity. 60% The second and third \ngraphs in Fig. 16 separately show the reduc\u00ad 50% tions in the sizes of dyDDG and dyCDG due to the application \nof 40% optimizations. We further breakdown the contributions of the in-30% dividual optimizations. Note \nthat the second graph in Fig. 16 does 20% not include OPT-2a because it was never applied. 10% In recent \nwork, the SEQUITUR algorithm [20] has been effec-0% tively used to compress control .ow traces [18] and \naddress traces [5]. We also used the same algorithm to compress the labeling in\u00adformation in dyDGs. On \nan average, SEQUITUR compressed the dyDGs by a factor of 9.18 across the ten benchmarks we consid\u00adered \nand our approach compressed the dyDGs by a factor of 23.4. Therefore the approach we propose is much \nmore effective than Figure 16: dyDDG vs. dyCDG size reduction. executed. More importantly the slicing \ntimes are very promising. SEQUITUR. For 9 out of 10 benchmarks the average slicing time for 25 slices \ncomputed at the end of the run is below 18 seconds. The only ex-Execution times. The next step was to \nanalyze the slicing times of ception is 300.twolf for which average slicing time at the end our algorithm. \nTo carry out this study we computed multiple pro-of the program run is roughly 36 seconds. It is worth \nnoting that gram slices at various points during the execution of each program. our optimizations did \nnot reduce the graph size for this program as The reason why we computed multiple slices is because depend-much \nas many of the other benchmarks. Finally, at earlier points ing upon the memory address or variable chosen, \nthe slicing times during program runs the slicing times are even lower. can vary. The reason we carried \nout slicing at different points dur-We also performed the above experiment without making use of ing \nexecution is because we wanted to study the change in slicing the shortcut edges in the dyDG. The average \nslicing times at the times as the size of the dyDG grows. Moreover this scenario also end of the program \nrun with and without making use of shortcuts represents a realistic use of slicing applications often \ncompute are given in Table 3. In 8 out of 10 benchmarks, by making use of slices at different execution \npoints. shortcuts, the average slicing time is cut by more than half. Thus, The results of this study \nare presented in Fig. 17. In this graph this is an important optimization. each point corresponds to \naverage slicing time for 25 slices. For Finally let us consider the cost of generating the dyDGs so that \neach benchmark 25 new slices are computed after execution inter-dynamic slicing can be performed, Our \nimplementation performs val of 15 million statements these slices correspond to 25 distinct dyDG construction \nin two steps. First instrumented programs are memory references. Following each execution interval slices \nare run to collect execution traces (control .ow and data address traces). computed for memory addresses \nthat had been de.ned since the In the Trimaran environment, the execution of the program slows last execution \ninterval this was done to avoid repeated computa-down roughly by a factor of two when traces are generated. \nSecond tion of same slices during the experiment. As we can see, the in-execution traces are preprocessed \nto generate dyDGs. The prepro\u00adcrease in slicing times is linear with respect to number of statements \ncessing times are shown in Table 4. 300.twolf 400 256.bzip2 250 350 OPT LP FP     300 250 200 150 \n100 50 0 200 150 100 50 Cumulative slicingCumulative slicingCumulative slicingCumulative slicingCumulative \nslicingtime (min.) time (min.) time (min.) time (min.) time (min.) Cumulative slicingCumulative slicingCumulative \nslicingCumulative slicingCumulative slicingtime (min.) time (min.) time (min.) time (min.) time (min.) \n0 5 1015202530 0 5 10 15 20 25 30 Slicing queries Slicing queries 255.vortex 300 197.parser 300 250 \n250 200 200 150 150 100 100 50 50 Statements executed (millions) 0 0 5 1015202530 0 5 10 15 20 25 30 \nSlicing queries Slicing queries 181.mcf 350 164.gzip 140 OPT LP FP    OPT LP FP    300 250 200 \n150 100 50 120 100 80 60 40 20 Table 3: Bene.t of providing shortcuts. Program OPT slicing Times (seconds) \nRatio w/o shortcuts with shortcuts w/o / with 0 0 0 5 1015202530 0 5 1015202530 Slicing queries Slicing \nqueries 134.perl 300.twolf 68.01 36.25 1.88 700 130.li 350 OPT LP FP ** OoM    OPT LP FP    256.bzip2 \n6.14 2.10 2.92 255.vortex 5.57 1.92 2.90 197.parser 4.86 2.21 2.20 181.mcf 22.05 17.10 1.29 164.gzip \n4.54 1.74 2.61 134.perl 12.59 4.05 3.11 130.li 15.65 6.09 2.57 126.gcc 9.76 3.80 2.57 099.go 26.85 11.36 \n2.36 600 500 400 300 200 100 300 250 200 150 100 50 0 0 0 5 1015202530 0 5 10 15 20 25 30 Slicing queries \nSlicing queries 126.gcc 350 099.go 250 OPT LP FP    300 250 200 150 100 200 150 100 5050 Table 4: \nPreprocessing time for OPT. Program Preprocessing Program Preprocessing Time (Minutes) Time (Minutes) \n300.twolf 65.29 256.bzip2 38.36 255.vortex 44.46 197.parser 44.06 181.mcf 53.64 164.gzip 23.52 134.perl \n51.12 130.li 49.88 126.gcc 48.83 099.go 35.24  4.2 Comparison with Other Algorithms We also compare \nthe performance of OPT with the LP algorithm and the traditional FP algorithm. The LP algorithm was found \nto be the best overall in [26] as it does not run out of memory for reasonably long program runs. The \ntraditional FP algorithm runs out of memory for long program runs. However, in order to be able to successfully \nrun FP, we used a machine with 2 Gigabyte RAM which was suf.cient to accommodate the original dyDGs for \nall but one program run (134.perl). LP versus OPT. The cumulative slicing times for computing up to 25 \nslices at the end of the program run for the two algorithms are plotted in Fig. 18. As we can see, the \nLP algorithm is much slower than the proposed algorithm. Computing each new slice us\u00ading LP on an average \ntakes 4.69 to 25.21 minutes depending upon the benchmark while computing the same slice using our optimized \nalgorithm OPT takes 1.74 to 36.35 seconds. The LP algorithm spends a great deal of time traversing the \nexecution trace stored on disk during each slice computation. The point at which the curves intersect \nthe y-axis represents the preprocessing time for the pro\u00adposed algorithm this is the time for building \nthe dyDG while for the LP algorithm this is the time for preprocessing the execution trace to enable \nfaster traversal of the trace as described in [26]. The ex\u00ad 0 0 5 1015202530 0 5 10 15 20 25 30 Slicing \nqueries Slicing queries Figure 18: Comparison of OPT with LP and FP. act preprocessing times are given \nin Table 5. As we can see, while the preprocessing time of the proposed OPT algorithm is higher, the \ndifference is comparable to the time spent on computing a few slices using the LP algorithm. The memory \nneeded by the OPT and LP algorithms is given in Table 6. While the memory used by the OPT algorithm is \nthe size of reduced dyDG, the memory used by LP is the size of the dyDG subgraph corresponding to a slicing \nrequest. Since the latter varies with slicing requests, we report the largest dyDG subgraph size constructed \nin response to 25 distinct slicing requests. We note that in 5 out of 10 benchmarks the size of the largest \ndyDG subgraph built by LP is greater than the full reduced dyDG built by OPT. It is clear from this data \nthat on average, the memory needs of LP and OPT are comparable to each other. Therefore, based upon the \nabove results we can say that OPT is superior to LP because it is much faster than LP and at the same \ntime it uses roughly the same amount of memory as LP. FP versus OPT. As we know, FP runs out of memory \nfor reason\u00adably long program runs [26]. However, we wanted to see how the slicing times of the OPT algorithm \ncompared to that of the FP al\u00adgorithm in situations where the program run was short enough to enable \nthe entire (unoptimized) dynamic dependence graph to be kept in memory. We were able to successfully \nrun FP on a machine with 2 Gigabyte RAM for all programs except 134.perl. The slicing times of FP and \nOPT are compared in Table 7. We observe that OPT is faster than FP. This is because of the use of shortcut \nedges that speed up the traversal of the dyDG. The same optimization cannot be applied to FP because \nthe unoptimized dyDG only contains dynamic edges. We know that the difference between OPT slicing times \nand FP slicing times are caused by shortcuts be\u00adcause when we compare the slicing times of OPT without \nshortcuts (given earlier in Table 3) with slicing time of FP given below, they are quite close. Table \n5: Preprocessing time: LP vs. OPT. Program Preprocessing Time (Minutes) Ratio OPT LP LP/OPT 300.twolf \n65.29 14.54 0.22 256.bzip2 38.36 9.38 0.25 255.vortex 44.46 16.35 0.37 197.parser 44.06 16.23 0.37 181.mcf \n53.64 16.64 0.31 164.gzip 23.52 14.56 0.62 134.perl 51.12 17.18 0.34 130.li 49.88 19.23 0.39 126.gcc \n48.83 26.65 0.55 099.go 35.24 17.06 0.48 Table 6: dyDG graph sizes: LP vs. OPT. Program Graph Size (Megabytes) \nOPT LP (Max. of 25 slices) 300.twolf 210.21 296.06 256.bzip2 50.48 80.66 255.vortex 64.81 33.60 197.parser \n69.81 40.04 181.mcf 170.29 113.74 164.gzip 51.57 34.75 134.perl 20.92 53.62 130.li 96.50 105.45 126.gcc \n74.71 57.70 099.go 131.24 162.28 Table 7: Slicing times: FP vs. OPT. Program Slicing Times (seconds) \nFP OPT 300.twolf 65.99 36.25 256.bzip2 5.92 2.10 255.vortex 6.17 1.92 197.parser 5.28 2.21 181.mcf 21.71 \n17.10 164.gzip 4.83 1.74 134.perl - 4.05 130.li 17.86 6.09 126.gcc 11.03 3.80 099.go 29.79 11.36 Finally \nwe compare the preprocessing times of OPT and FP. We had expected that the preprocessing times for OPT \nwould be higher than FP because the OPT algorithm must spend some extra time for checking whether the \ntimestamp pairs of all exercised dependences should be added to the dyDG or not. However, our experiments \nshow otherwise. As the data in Table 8 shows, the preprocessing times for the FP algorithm are consistently \nhigher than those for OPT. The reason for this behavior is that the list of timestamp pairs that are \nassociated with a dependence edge often grow very large and thus resizing of the array in which these \nare stored must of\u00adten be performed. These memory reallocation operations take up signi.cant amount of \ntime in FP while in OPT this is not the case. Thus, the overall effect of this behavior is that the preprocessing \ntimes of OPT are lower than that of FP. Table 8: Preprocessing time: FP vs. OPT. Program Preprocessing \nTime (Minutes) Ratio OPT FP FP/OPT 300.twolf 65.29 99.62 1.53 256.bzip2 38.36 80.78 2.11 255.vortex 44.46 \n55.47 1.25 197.parser 44.06 67.57 1.53 181.mcf 53.64 71.17 1.33 164.gzip 23.52 31.66 1.35 134.perl 51.12 \n- - 130.li 49.88 74.86 1.50 126.gcc 48.83 52.70 1.08 099.go 35.24 42.17 1.20 Therefore, based upon the \nabove results we can say that OPT is superior to FP not only because it scales to longer program runs, \nbut also because it has lower preprocessing and slicing times. Combining idea behind LP with OPT. While \nthe above results indicate that OPT is superior to LP making the latter obsolete, the idea of demand-driven \nprocessing behind LP can be integrated into OPT to further increase the scalability of OPT as follows. \nWe continue to use the optimizations of OPT and build the optimized dyDG in memory until the graph in \nmemory reaches a predeter\u00admined size. At that point the current block of dynamic dependence information \nlabeling the graph is saved on disk freeing up the mem\u00adory used by these dynamic dependences. This process \nis applied repeatedly as execution continues. In other words, the new algo\u00adrithm can be designed such \nthat the graph in memory only contains a single block of dynamic dependences while all other blocks are \nheld on disk. During slicing, a new block is loaded into memory on demand while the old block is discarded \nto free up memory. Since this algorithm will use disk space to store compacted graphs, it is expected \nto scale to much longer runs than the OPT algorithm.  5. RELATED WORK Precise dynamic slicing algorithms \ncan be divided into two broad categories: backward computation algorithms [14, 1, 26] that com\u00adpute slices \non-demand through backward traversal of dynamic de\u00adpendence graph and forward computation algorithms \n[4, 17, 27] that perform forward precomputation of dynamic slices without constructing the dynamic dependence \ngraph. In this section we discuss the drawbacks of prior algorithms and show that the algo\u00adrithm proposed \nin this paper is a signi.cant improvement that can be used for a broad range of applications. When a \nbackward computation algorithm constructs the entire dynamic dependence graph prior to slicing (e.g., \nAlgorithm III in [1]), as our prior experience in [26] shows, for realistic program runs the graph is \ntoo big and thus we run out of memory while building the graph. When graph is compacted by making it \nim\u00adprecise (e.g., Algorithms I and II in [1]), as our prior experience in [26] shows, the slices computed \ncan be many times larger. An alternative that we proposed in [26] is the LP algorithm which con\u00adstructs \nthe dynamic dependence graph on demand and thus only a part of the graph is constructed reducing memory \nneeds. However, on demand construction of the dynamic dependence graph requires repeated traversals of \nthe execution trace which yields a slow slic\u00ading algorithm. We believe the key difference in our approach \nand the one used in [1] is the use of timestamps instead of node repli\u00adcation to represent dynamic instances \nof dependences. While we were able to develop a series of optimizations to replace explicit timestamps \nby implicit ones, it is unclear how similar .ne-grained optimizations can be developed for nodes. In \n[6] a technique for compressing a dynamic execution trace is presented. Execution in\u00adstances of statements \nare classi.ed as critical and non-critical. For non-critical nodes, only the latest execution instances \nare saved in the trace. The trace compression optimization is analogous to our OPT-1a optimization at \nthe dependence graph level. In general, the problem with forward computation algorithms is that exhaustive \nprecomputation of all dynamic slices at all program points produces large amounts of information which \nmust be stored on disk [4]. Exhaustively computing all slices, and then .nding de\u00adsired slices from the \nextremely large number of precomputed slices, also takes signi.cant amount of time thus yielding slow \nalgorithms [27]. Algorithm IV in [1] is also a forward computation algorithm in which slices are computed \nand annotated on the dyDG. The role of the dyDG is to provide access to the slices and to avoid saving \nof multiple identical slices sometimes generated during repeated executions of the same statements. The \nforward computation algo\u00adrithm in [17] sacri.ces precision to speedup dynamic slicing. We have recently \nproposed an algorithm that goes well beyond Algo\u00adrithm IV [1] in reducing memory reads. It uses reduced \nordered binary decision diagrams to store the dynamic slices in compact form [27]. Finally we would like \nto point out that backward computation algorithms are more generally useful than forward computation \nal\u00adgorithms. This is because, when dynamic slices are computed from dynamically constructed dependence \ngraphs, not only can we com\u00adpute dynamic slices, we can also identify the exercised dynamic dependences \nthat contribute to the dynamic slices. The identi.ca\u00adtion of dynamically exercised dependences is essential \nfor some ap\u00adplications such as carrying out dependence based software testing [8, 16], measuring module \ncohesion for purpose of code restructur\u00ading [11], and performance enhancement by identifying criticality \nof instructions [28] and presence of instruction isomorphism [22]. For other applications such as debugging \nof programs [2, 15] and detecting spyware that has been installed on systems without the users knowledge \n[13], while it may useful to identify the depen\u00addences, it is not necessary to do so. Thus the algorithm \nwe present in this paper is both cost effective and generally applicable. 6. CONCLUSION In conclusion \nwe can see that the OPT algorithm we have pro\u00adposed in this paper provides fast slicing times (1.74 to \n36.25 sec\u00adonds) and compact dynamic dependence graph representation (20 to 210 Megabytes) leading to \na space and time ef.cient algorithm for dynamic slicing. In contrast, the prior algorithms are either \nspace inef.cient (corresponding graph sizes for FP are 0.84 to 1.95 Gigabytes) or time inef.cient (corresponding \nslicing times for LP are 4.69 to 25.21 minutes) making them unattractive for use in prac\u00adtice. The development \nof a cost effective dynamic slicing algorithm is an important contribution as a wide range of applications \nthat re\u00adquire analysis of dynamic information are making use of dynamic slicing [2, 15, 13, 8, 16, 11, \n28, 22]. 7. REFERENCES [1] H. Agrawal and J. Horgan, Dynamic Program Slicing, ACM SIGPLAN Conference \non Programming Language Design and Implementation, pages 246-256, 1990. [2] H. Agrawal, R. DeMillo, and \nE. Spafford, Debugging with Dynamic Slicing and Backtracking, Software Practice and Experience, 23(6):589-616, \n1993. [3] T. Ball and J.R. Larus, Ef.cient Path Pro.ling, IEEE/ACM International Symposium on Microarchitecture, \n1996. [4] A. Beszedes, T. Gergely, Z.M. Szabo, J. Csirik, and T. Gyimothy, Dynamic Slicing Method for \nMaintenance of Large C Programs, 5th European Conference on Software Maintenance and Reengineering, pages \n105-113, March 2001. [5] T.M. Chilimbi, Ef.cient Representations and Abstractions for Quantifying and \nExploiting Data Reference Locality, ACM SIGPLAN Conference on Programming Language Design and Implementation, \npages 191-202, 2001. [6] D.M.D. Dhamdhere, K. Gururaja, and P.G. Ganu, A Compact Execution History for \nDynamic Slicing, Information Processing Letters, 85:(145-152), 2003. [7] E. Duesterwald, R. Gupta, and \nM.L. Soffa, Distributed Slicing and Partial Re-execution for Distributed Programs, 5th LCPC Workshop, \nLNCS 757 Springer, pages 497-511, August 1992. [8] E. Duesterwald, R. Gupta, and M.L. Soffa, Rigorous \nData Flow Testing through Output In.uences, 2nd Irvine Software Symposium, pages 131-145, UC Irvine, \nCA, March 1992. [9] J. Ferrante, K.J. Ottenstein, and J.D. Warren, The Program Dependence Graph and Its \nUse in Optimization, ACM Transactions on Programming Languages and Systems, 9(3):319-349, 1987. [10] \nR. Gupta and M.L. Soffa, Hybrid Slicing: An Approach for Re.ning Static Slices using Dynamic Information, \nACM SIGSOFT Symp. on the Foundations of Software Engineering, pages 29-40, 1995. [11] N. Gupta and P. \nRao, Program Execution Based Module Cohesion Measurement, 16th IEEE International Conf. on Automated \nSoftware Engineering, pages 144-153, San Diego, CA November 2001. [12] T. Hoffner, Evaluation and Comparison \nof Program Slicing Tools. Tech. Report, Dept. of Computer and Info. Science, Linkoping University, Sweden, \n1995. [13] S. Jha, Private communication, University of Wisconsin at Madison, Department of Computer \nScience, 2003. [14] B. Korel and J. Laski, Dynamic Program Slicing, Information Processing Letters, 29(3):155-163, \n1988. [15] B. Korel and J. Rilling, Application of Dynamic Slicing in Program Debugging, Automated and \nAlgorithmic Debugging, 1997. [16] M. Kamkar, Interprocedural Dynamic Slicing with Applications to Debugging \nand Testing, PhD Thesis, Linkoping University, 1993. [17] B. Korel and S. Yalamanchili. Forward Computation \nof Dynamic Program Slices, International Symposium on Software Testing and Analysis, August 1994. [18] \nJ.R. Larus, Whole Program Paths, ACM SIGPLAN Conference on Programming Language Design and Implementation, \npages 259-269, Atlanta, GA, May 1999. [19] M. Mock, D.C. Atkinson, C. Chambers, and S.J. Eggers, Improving \nProgram Slicing with Dynamic Points-to Data, ACM SIGSOFT 10th Symp. on the Foundations of Software Engineering, \n2002. [20] C.G. Nevil-Manning and I.H. Witten, Linear-time, Incremental Hierarchy Inference for Compression, \nData Compression Conference, Snowbird, Utah, IEEE Computer Society, pages 3-11, 1997. [21] T. Reps and \nG. Rosay, Precise Interprocedural Chopping, Third ACM SIGSOFT Symposium on the Foundations of Software \nEngineering, Washington, DC, pages 41-52, October 1995. [22] Y. Sazeides, Instruction-Isomorphism in \nProgram Execution, Value Prediction Workshop (held with ISCA), June 2003. [23] F. Tip, A Survey of Program \nSlicing Techniques, Journal of Programming Languages, 3(3):121-189, Sept. 1995. [24] G. Venkatesh, Experimental \nResults from Dynamic Slicing of C Programs, ACM Transactions on Programming Languages and Systems, 17(2):197-216, \n1995. [25] M. Weiser, Program Slicing, IEEE Transactions on Software Engineering, SE-10(4):352-357, 1982. \n[26] X. Zhang, R. Gupta, and Y. Zhang, Precise Dynamic Slicing Algorithms, IEEE/ACM International Conference \non Software Engineering, pages 319-329, Portland, Oregon, May 2003. [27] X. Zhang, R. Gupta, and Y. Zhang, \nEf.cient Forward Computation of Dynamic Slices Using Reduced Ordered Binary Decision Diagrams, IEEE/ACM \nInternational Conference on Software Engineering, Edinburgh, UK, May 2004. [28] C.B. Zilles and G. Sohi, \nUnderstanding the Backward Slices of Performance Degrading Instructions, ACM/IEEE 27th International \nSymposium on Computer Architecture, 2000.  \n\t\t\t", "proc_id": "996841", "abstract": "Although dynamic program slicing was first introduced to aid in user level debugging, applications aimed at improving software quality, reliability, security, and performance have since been identified as candidates for using dynamic slicing. However, the dynamic dependence graph constructed to compute dynamic slices can easily cause slicing algorithms to run out of memory for realistic program runs. In this paper we present the design and evaluation of a cost effective dynamic program slicing algorithm. This algorithm is based upon a dynamic dependence graph representation that is highly compact and rapidly traversable. Thus, the graph can be held in memory and dynamic slices can be quickly computed. A compact representation is derived by recognizing that all dynamic dependences (data and control) need not be individually represented. We identify sets of dynamic dependence edges between a pair of statements that can <i>share</i> a single representative edge. We further show that the dependence graph can be transformed in a manner that increases sharing and sharing can be performed even in the presence of aliasing. Experiments show that transformed dynamic dependence graphs explicitly represent only 6% of the dependence edges present in the full dynamic dependence graph. When the full graph sizes range from 0.84 to 1.95 Gigabytes in size, our compacted graphs range from 20 to 210 Megabytes in size. Average slicing times for our algorithm range from 1.74 to 36.25 seconds across several benchmarks from SPECInt2000/95.", "authors": [{"name": "Xiangyu Zhang", "author_profile_id": "81384614270", "affiliation": "University of Arizona, Tucson, AZ", "person_id": "P514156", "email_address": "", "orcid_id": ""}, {"name": "Rajiv Gupta", "author_profile_id": "81100027751", "affiliation": "University of Arizona, Tucson, AZ", "person_id": "PP43126354", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/996841.996855", "year": "2004", "article_id": "996855", "conference": "PLDI", "title": "Cost effective dynamic program slicing", "url": "http://dl.acm.org/citation.cfm?id=996855"}