{"article_publication_date": "06-09-2004", "fulltext": "\n Array Regrouping and Structure Splitting Using Whole-Program Reference Af.nity Yutao Zhong, Maksim \nOrlovich, Xipeng Shen, and Chen Ding Computer Science Department University of Rochester {ytzhong,orlovich,xshen,cding}@cs.rochester.edu \n ABSTRACT While the memory of most machines is organized as a hierarchy, program data are laid out in \na uniform address space. This paper de.nes a model of reference af.nity, which measures how close a group \nof data are accessed together in a reference trace. It proves that the model gives a hierarchical partition \nof program data. At the top is the set of all data with the weakest af.nity. At the bottom is each data \nelement with the strongest af.nity. Based on the the\u00adoretical model, the paper presents k-distance analysis, \na practical test for the hierarchical af.nity of source-level data. When used for array regrouping and \nstructure splitting, k-distance analysis con\u00adsistently outperforms data organizations given by the programmer, \ncompiler analysis, frequency pro.ling, statistical clustering, and all other methods we have tried. \nCategories and Subject Descriptors D.3.4 [Programming Languages]: Processors optimization, compilers \n General Terms Algorithms, Languages, Performance  Keywords Program locality, program transformation, \nreference af.nity, vol\u00adume distance, reuse signature, array regrouping, structure splitting 1. INTRODUCTION \nAll current PCs and workstations use cache blocks of at least 64 bytes, making the utilization an important \nproblem. If only one word is useful in each cache block, a cache miss will not serve as a prefetch for \nother useful data. Furthermore, the program would waste up to 93% of memory transfer bandwidth and 93% \nof cache space, causing even more memory access. Permission to make digital or hard copies of all or \npart of this work for personal or classroom use is granted without fee provided that copies are not made \nor distributed for pro.t or commercial advantage and that copies bear this notice and the full citation \non the .rst page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires \nprior speci.c permission and/or a fee. PLDI 04, June 9 11, 2004, Washington, DC, USA. Copyright 2004 \nACM 1-58113-807-5/04/0006 ...$5.00. To improve cache utilization we need to group related data into the \nsame cache block. The question is how to de.ne the relation. We believe that it should meet three requirements. \nFirst, it should be solely based on how data are accessed. For example in an ac\u00adcess sequence abab..ab \n, a and b are related and should be put in the same cache block, regardless how they are allocated and \nwhether they are linked by pointers. Second, the relation must give a unique partition of data. Consider \nfor example the access sequence abab..ab...bcbc..bc . Since data a and c are not related, b cannot relate \nto both of them because it cannot stay in two loca\u00adtions in memory. Finally, the relation should be a \nscale. Different memory levels have blocks of increasing sizes, from a cache block to a memory page. \nThe grouping of most related data into the smallest block should precede the grouping of next related \ndata into larger blocks. In summary, the relation should give a unique and hierarchical organization \nof all program data. In this paper, we de.ne such a relation we call reference af.nity, which measures \nhow close a group of data are accessed together in an execution. Unlike most other program analysis, \nwe measure the togetherness using the LRU stack distance, de.ned as the amount of data accessed between \ntwo memory references in an execution trace [28]. As a notion of locality, stack distance is bounded, \neven for long-running programs. The long stack distance often reveals long-range data access patterns \nthat may otherwise hide behind complex control .ows, indirect data access, or variations in cod\u00ading and \ndata allocation. We prove that the new de.nition gives a unique partition of program data for each distance \nk. When we decrease the value of k, the reference af.nity gives a hierarchical decomposition and .nds \ndata sub-groups with closer af.nity, much in the same way we sharpen the focus by reducing the radius \nof a circle. Two earlier studies de.ned a reuse signature as a histogram of the reuse distance of all \nprogram data access [17, 36]. They showed that the reuse signature has a consistent pattern across all \ndata in\u00adputs even for complex programs or regular programs after complex compiler optimizations. This \nsuggests that we can analyze the refer\u00adence af.nity of the whole program by looking at its reuse signatures \nfrom training runs. We present k-distance analysis, which simpli.es the require\u00adments of reference af.nity \ninto a set of necessary conditions about reuse signatures. The simpli.ed conditions can then be checked \nef.ciently for large, complex programs. The parameter k has an intuitive meaning elements in the same \ngroup are almost always used within a distance of k data elements. The analysis handles se\u00adquential programs \nwith arbitrarily complex control .ows, indirect data access, and dynamic memory allocation. The analysis \nuses multiple training runs to take into account the variation caused by program inputs. In addition, \nwe will use the analysis to .nd the worst reference af.nity and measure the range of the impact from \ndifferent data layouts. Although reference af.nity and k-distance analysis have strong properties, they \nare not optimal. We will formulate the problems in terms of partial and dynamic reference af.nity and \ndiscuss their complexity. Our analysis checks the necessary rather than the suf\u00ad.cient conditions of \nreference af.nity, so it may include data with false af.nity. We will show that the probability of error \nis small and can be further reduced by strengthening the conditions. The rest of the paper is organized \nas follows. Section 2 de\u00ad.nes the formal model of reference af.nity and proves its proper\u00adties. Section \n3 presents k-distance analysis and a comparison with a number of other methods. Section 4 describes the \ncompiler support for array regrouping and structure splitting. The last three sections present the experimental \nevaluation, related work, and conclusions.  2. MODEL OF REFERENCE AFFINITY This section .rst de.nes \nthree preliminary concepts and gives two examples of the reference af.nity model. Then it presents the \nformal de.nition and proves the properties including the unique and hierarchical partition of program \ndata. An address trace or reference string is a sequence of accesses to a set of data elements. If we \nassign a logical time to each access, the address trace is a vector indexed by the logical time. We use \nletters such as x,y,zto represent data elements, subscripted symbols such as ax,ax . to represent accesses \nto a particular data element x, and the array index T[ax]to represent the logical time of the access \nax on a trace T. The LRU stack distance between two accesses, ax and ay (T[ax] <T[ay]), in a trace T \nis the number of distinct data elements ac\u00adcessed in times T[ax],T[ax]+1,...,T[ay]- 1. We write it as \ndis(ax,ay).If T[ax]>T[ay], we let dis(ax,ay)=dis(ay,ax). If T[ax]= T[ay], dis(ax,ay)=0. The distance \nis the volume of data accessed between two points of a trace, so we also call it the volume distance. \nIn comparison, the time distance is the differ\u00adence between the logical time of two accesses. For example, \nthe volume distance between the accesses to aand cin the trace abbbc is 2, while the time distance is \n4. The volume distance is Euclidean. Given any three accesses in the time order, ax,ay, and az,wehave \ndis(ax,az)= dis(ax,ay)+dis(ay,az), because the cardinality of the union of two sets is no greater than \nthe sum of the cardinality of each set. Based on the volume distance, we de.ne a linked path in a trace. \nIt is parameterized by a distance bound k. There is a linked path from ax to ay (x = y) if and only if \nthere exist t accesses, ax1 , ax2 , ..., axt , such that (1) dis(ax,ax1 ) = k. dis(ax1 ,ax2 ) = k . ... \n. dis(axt ,ay) = k and (2) x1,x2,...,xt, x and y are different data elements. In other words, a linked \npath is a sequence of accesses to different data elements, and each link (between two consecutive members \nof the sequence) has a volume distance no greater than k. We call k the link length. We will later restrict \nx1,x2,...,xt to be members of some set S. If so, we say that there is a linked path from ax to ay with \nlink length kfor the set S. We now explain reference af.nity with two example address traces in Figure \n1. The ... represents accesses to data other than w, x, y, and z. In the .rst example, accesses to x, \ny,and z are in three sec\u00adtions. The three elements belong to the same af.nity group because they are \nalways accessed together. The consistency is important for data placement. For example, xand ware not \nalways used together, so putting them into the same cache block would waste cache space when only one \nof the two is accessed. The example shows that .nding this consistency is not trivial. The accesses to \nthe three data elements appear in different orders, with a different frequency, and mixed with accesses \nto other data. However, one property holds in all three sections the accesses to the three elements \nare connected by a linked path with the link length 2. xyz ... xwzzy ... yzvvvvvx ... (1) The affinity \ngroup {x,y,z} with link length k = 2 wxwxuyzyz ... zyzyvwxwx ... (2) The affinity group {w,x,y,z} at \nk=2 becomes two groups {w,x} and {y,z} at k=1 Figure 1: Examples of reference af.nity model and properties \nAs we will prove later, af.nity groups are parameterized by the link length k, and they form a partition \nof the program data for each k. The second example in Figure 1 shows that this group partition has a \nhierarchical structure. The af.nity group with the link length of2is {w,x,y,z}. If we reduce the link \nlength to 1, the two new groups will be {w,x} and {y,z}. The groups at a smaller link length are subsets \nof the groups at a greater link length. The hier\u00adarchical structure is useful in data placement because \nit can match different-size af.nity groups to a multi-level cache hierarchy. We now present the formal \nde.nition of the reference af.nity. DEFINITION 1. Strict Reference Af.nity. Given an address trace, a \nset G of data elements is a strict af.nity group (i.e. they have the reference af.nity) with the link \nlength kif and only if 1. for any x . G, all its accesses ax must have a linked path from ax to some \nay for each other member y . G, that is, there exist different elements x1,x2,...,xt . G such that dis(ax,ax1 \n)= k.dis(ax1 ,ax2 )= k.....dis(axt ,ay)= k 2. adding any other element to G will make Condition (1) \nim\u00adpossible to hold  The following theorem proves that strict af.nity groups are con\u00adsistent because \nthey form a partition of program data. In other words, each data element belongs to one and only one \naf.nity group. THEOREM 1. Given an address trace and a link length k, the af.nity groups de.ned by De.nition \n1 form a unique partition of program data. PROOF. We show that any element x of the program data be\u00adlongs \nto one and only one af.nity group at a link length k. For the one part, observe that Condition (1) in \nDe.nition 1 holds trivially when xis the only member of a group. Therefore any element must belong to \nsome af.nity group. We prove the only-one part by contradiction. Suppose x be\u00adlongs to G1 and G2 (G1 \n= G2). Then we can show that G3 = G1 . G2 satis.es Condition (1). For any two elements y,z . G3, if both \nbelong to G1 and G2, then Condition (1) holds. Without loss of generality, assume y . G1 .y. G2 and z \n.G2 .z. G1. Because y,x .G1,any ay, must have a linked path to an ax, that is, there exist y1,...,yt \n.G1 and an access ax such that dis(ay,ay1 )=k.....dis(ayt ,ax)= k. Similarly, there is a linked path \nfor this ax to an az because x,z .G2, that is, there exist z1,...,zm .G2 and an access az two sides of \nthe section is no greater than 2k|G|+1, where |G|is the number of elements in the af.nity group. PROOF. \nAccording to De.nition 1, for any y in G, there is a linked path from ax to some ay. Sort these accesses \nin time order. Let aw be the earliest and av be the latest in the trace. There is a such that dis(ax,az1 \n)=k.....dis(azm ,az)=k. If y1,...,yt .{z1,...,zm}, then there is a linked path from ay to some az. Suppose \ny1,...,yi-1 z1,...,zm}but yi .{= zp. Then we have a linked path from ay to ayi . Since yi = zp . G2, \nthere is a linked path from yi to z, that is, there exist lll z1,z2,...,zn .G2 such that dis(ay,ay1 \n)=k.....dis(ayi1 ,ayi ) l =k.dis(ayi ,a;).....dis(a,az)=k.Now yi belongs to zzn 1 G1 nG2, just like x. \nWe have come back to the same situation ex\u00adcept that the linked path from ay to ayi is shorter than the \npath from ay to ax. We repeat this process. If y1,...,yi-1 z1l,...,z l }, .{nthen we have a linked path \nfrom ay to az. Otherwise, there must be yj .{z1l,...,znl }for some j<i. The process cannot repeat for\u00adever \nbecause each step shortens the path from yto the access chosen next by this process. It must terminate \nin a .nite number of steps. We then have a linked path from ay to az in G3. Therefore, Con\u00addition (1) \nalways holds for G3. Since G1,G2 .G3, they are not the largest sets that satisfy Condition (1). Therefore, \nCondition (2) does not hold for G1 or G2. A contradiction. Therefore, xbelongs to only one af.nity group, \nand af.nity groups form a partition. For a .xed link length, the partition is unique. Suppose more than \none type of partition can result from De.nition 1, then some x belongs to G1 in one partition and G2 \nin another partition. As we have just shown, this is not possible because G3 = G1 .G2 satis.es Condition \n(1) and therefore neither G1 nor G2 is an af.nity group. As we just proved, reference af.nity is consistent \nbecause all members will always be accessed together (i.e. linked by some linked path with the link length \nk). The consistency means that packing data in an af.nity group will always improve cache utiliza\u00adtion. \nIn addition, the group partition is unique because each data element belongs to one and only one group \nfor a .xed k. Next we prove that the strict reference af.nity has a hierarchical structure an af.nity \ngroup with a shorter link length is a subset of an af.nity group with a greater link length. THEOREM \n2. Given an address trace and two distances k and kl (k<kl), the af.nity groups at k form a .ner partition \nof the af.nity groups at kl. PROOF. We show that any af.nity group at kis a subset of some af.nity group \nat kl.Let G be an af.nity group at k and Gl be the af.nity group at kl that overlaps with G (G nGl \u00d8). \n= Since any x,y .G are connected by a linked path with the link length k, they are connected by a linked \npath with the larger link length kl. According to the proof of Theorem 1, G .Gl is an af.nity group at \nkl. G must be a subset of Gl; otherwise Gl is not an af.nity group because it can be expanded while still \nguaranteeing Condition (1). Finally, we show that elements of the same af.nity group are always accessed \ntogether. When one element is accessed, all other elements will be accessed within a bounded volume distance. \nTHEOREM 3. Given an address trace with an af.nity group G at the link length k, any time an element xof \nGis accessed at ax, there exists a section of the trace that includes ax and at least one access to all \nother members of G. The volume distance between the linked path from aw to ax. Let the sequence be ax1 \n,ax2 ,...,axt . The volume distance from aw to ax is dis(aw,ax). It is no greater than dis(aw,ax1 )+dis(ax1 \n,ax2 )+...+dis(axt ,ax), which is (t+1)k =|G|k. The bound of the volume distance from ax to av is the \nsame. Considering that av is included in the section, the total volume distance is at most 2k|G|+1. The \nstrict af.nity requires that members of an af.nity group be always accessed together. Ding and Kennedy \nshowed that it gives the best data layout when no side effect (i.e. increased cache misses) is allowed \n[14]. On most machines, it is still pro.table to group data that are almost always accessed together \nbecause the side effect would not outweigh the bene.t. For programs with different behav\u00adior phases, \nit may be pro.table to exploit reference af.nity in each phase and change data layout between phases. \nWe call these exten\u00adsions the partial reference af.nity and the dynamic reference af.n\u00adity. Ding and \nKennedy showed that the optimal data layout in these two cases is machine dependent and .nding the optimal \nlayout is an NP-hard problem [14]. Next we present a method that measures the almost strict reference \naf.nity in complex programs. The link length kwill play a critical role as it did in this section.  \n3. WHOLE-PROGRAM AFFINITY We now study the reference af.nity among the source-level data. We speci.cally \ntarget data arrays and instances of structure .elds because they account for major portions of global \nand heap data in most programs. Since an array or a .eld represents a set of data, we need to extend \nthe af.nity de.nition. The af.nity exists among data sets if the sets have the same number of elements, \nand one element in one set has the reference af.nity with one and only one element in every other set. \nIn particular, reference af.nity exists for two arrays, Aand B, if the reference af.nity exists for A[i]and \nB[i]for all i. The af.nity exists for structure .elds, f1 and f2, if it exists for o.f1 and o.f2 for \nall instance o. Next, we introduce the reuse signature as the basis for whole-program af.nity analysis. \nWe then present k-distance analysis and a comparison with other methods. 3.1 Reuse signature The reuse \nsignature of a set of data is the histogram of the reuse distance of their accesses. Ding and Zhong showed \nthat the whole\u00adprogram reuse signature exhibits a consistent pattern across data in\u00adputs for large, complex \ninteger and .oating-point benchmarks [17]. The result suggests that we can use the reuse signature from \none pro.ling run to infer the reuse signatures in other executions. There\u00adfore, reuse signature allows \nnot just the whole-trace analysis but also the whole-program analysis of reference af.nity. We illustrate \nthe use of reuse signature through an example pro\u00adgram, Cheetah, a fully associative LRU cache simulator \nthat is part of the SimpleScalar 3.0 tool set. The main data structure is a splay tree, and each tree \nnode has a number of .elds, of which we con\u00adcern three in this example. Based on a pro.le from a simple \ninput, we draw the accesses to the three .elds on time-space graphs shown in Figure 2. Each access is \na point whose x-axis value is the log\u00adical time (in memory references) and y-axis the memory address. \nThe similarity of the graphs suggests that the two .elds rtwt (the sub-tree weight) and lft (left-child \npointer) have the reference af.n\u00adity because they seem to be always accessed together. The third  Figure \n2: The time-space graphs of the accesses to the three tree-node .elds .eld, addr, is accessed only occasionally \ntogether with the other two .elds. A manual inspection of the splay-tree algorithm con\u00ad.rms these conjectures. \nThe program uses the .rst two .elds to\u00adgether for the tree rotation at every step of a tree search, while \nit uses the third .eld only at the end of a tree search (the tree is in\u00addexed by time not the address). \nFigure 3 shows the reuse signature of the three .elds. Each is a histogram of the reuse distance for \nall accesses to a structure .eld. The reuse distance of an access is the volume distance between this \nand the previous access to the same data element. The x-axis is a sequence of bins representing different \nranges of a reuse distance. The bins may be of the same size (linear scale), exponential size (log scale \nas in this example), or their combinations. The y-axis gives the number of memory accesses whose reuse \ndistance falls into each bin. The .gure shows reuse signatures for reuse distances greater than 1024. \nThe short-distance reuses (that are not shown) account for about 70% of accesses to addr and over 85% \nto lft and rtwt. Figure 3: The reuse signatures of the three tree-node .elds We now compare the reuse \nsignature of rtwt and lft. The latter has more reuses at the .rst bin because it is repeatedly accessed \nduring initialization. However, the additional reuses have a short distance and do not affect reference \naf.nity. The number of reuses in the second and third bins also differ. This is because many reuses with \na distance close to 4K are separated by the arbitrary bin boundary. The total number of reuses with a \ndistance more than 2K is very similar. In the last two (larger) bins, rtwt and lft have the same number \nof accesses, but the number is different for addr. We use the reuse signature for af.nity analysis as \nfollows. We treat reuse signatures as vectors, remove the .rst few elements (reuse distances shorter \nthan 2048), and .nd af.nity groups by com\u00adparing the reuse signatures. The comparison method is key to \nthe af.nity analysis, as described next. Reuse signatures are measured through pro.ling, so a remaining \nquestion is whether other inputs to Cheetah have the same af.nity. As a cache simulator, the ac\u00adcess \npattern in Cheetah depends on the input. After checking a few other inputs, we found that the consistency \nremains: the time-space graph and the upper portion of the reuse signature remain similar between rtwt \nand lft but different from addr. In fact, we can con\u00adsider multiple training runs by combining their \nreuse signatures and therefore rule out false af.nity relations that appear in one input but not others. \nAs shown by this example, we may identify similar access patterns even if they are hidden behind branches, \nrecursive functions, and pointer indirections. We have converted the problem from checking reference \naf.nity in a trace, to checking patterns in time-space graphs, and .nally to checking the similarity \nof reuse signatures. The compression of the information is dramatic: from billions of memory accesses \nand trillions of time-space points to vectors of logM elements, where M is the size of program data (the \nmaximal volume distance). Next we inject rigor into the use of the reuse signature by formulating it \nin terms of the necessary conditions of the reference af.nity.  3.2 K-distance analysis To measure the \nreference af.nity, we check a necessary but not suf.cient condition on the reuse signature. We .rst derive \nthe nec\u00adessary condition for two data elements, then extend it to two or more data sets, and discuss \nthe improvements to the condition. K\u00addistance analysis checks an improved version of the necessary con\u00addition. \nThe derivation of the necessary condition is complex. The following description is terse for lack of \nspace. A reader may skip to Equation 1 for the formula used by the k-distance analysis. As a basic case, \nlet an af.nity group have two elements, x and y, always accessed within a link length k. For any access \na to one of the two elements, there must be an access a l to the other element, within the volume distance \nk. We call the smallest section of the trace that contains a and a l an af.nity window. We merge two \nwindows by taking their union whenever they overlap in time, so the resulting windows are disjoint. By \nnow all accesses to x and y are within one of the windows. We further divide these accesses into two \ngroups. An access is a local reuse if the previous access is inside the same window. Otherwise, it is \na remote reuse. We visualize a reuse distance by an edge connecting the two accesses in the trace. The \nedges are either local (inside a window) or remote (between two closest windows). We can now pick the \nthreshold for removing the short reuse dis\u00adtances. For reasons we do not elaborate, the necessary condition \nwould be too weak to be useful if we are not careful in setting the threshold. The threshold must be \nsuch that after the removal, the ending points of the remaining long distances (accesses to x and y) \nstill have the reference af.nity among themselves. The threshold, h, must meet two conditions. It must \nbe no less than 2k, so that we remove all local edges. In addition, no reuse distance should be close \nto h. To be exact, for an integer range w that is at least 2k in size and contains h inside the range, \nwe require that no reuse distance be within the range. The second condition ensures that the remote edges \nbetween two windows are either both below h or both over h. Then the ending points of the remaining reuse \ndistances have the reference af.nity among themselves. Since the threshold should be lower than the length \nof long reuse distances, the range w may not always exist. However, as Ding and Zhong observed in most \nprograms they tested, a reuse distance either stays constant or lengthens as the program input size increases \n[17]. In these pro\u00adgrams, we can always .nd the range by using a large enough input in the analysis. \n After removing short reuse distances, x and y have the same number of remaining long reuse distances. \nThey are paired. If there is a long reuse distance of one element between two windows, there is a long \nreuse distance of the other element between the same two windows. The maximal length difference between \neach pair is 2k. We do not include the proof, but the key observation in the proof is as follows. The \nending point of a long reuse distance is the .rst access of the element in the window, while the starting \npoint of the long distance is the last access of the element in the preceding win\u00addow. Since the maximal \ndifference between each pair is 2k,a nec\u00adessary condition for the reference af.nity relation between \nx and y is that after removing short reuse distances, the average length of their long reuse distances \nis no more than 2k. For two sets of data, X and Y , to have the reference af.nity, the necessary condition \nholds between the af.nity pairs. Therefore, ference between X and Y known as the Manhattan distance of \nthe two vectors. For two-dimensional vectors, it is the driving distance of a taxi going from one place \nto another in downtown New York City (no relation to the driving time, however). In addition, a reuse \ndistance does not include the exact time of the data access. It is possible that two elements are accessed \nin the same reuse distance, but one in the .rst half of the execution, and the other in the second half. \nAn improvement is to divide the execution trace into sub-parts and check the condition for each part. \nThe maximal difference between any two members of a g-element af.nity group is no more than 2gk. The \ncondition is recursive be\u00adcause knowing group members requires knowing the group size .rst. The recursive \nequation can be solved iteratively. For each data set X, we .nd all other sets whose average distance \ndiffers no more than bk and let b range from 1to 2g. The solution is the largest b such that exactly \nb -1data sets satisfy the condition. The process must terminate with the correct result. In practice, \nwe use a stricter condition to build a group incremen\u00adtally. Initially each data set is a group. Then \nwe traverse the groups and merge two groups if a member in one group and another mem\u00adber in the other \ngroup satisfy Equation 1. The process terminates if no more groups can be merged. We need to calculate \nthe distance difference between any two data sets in O(g 2)time. The iterative solutions takes at most \nO(g 2). The incremental solution takes lin\u00adear time if implemented using a work-list. k 693   a necessary \ncondition is that the average length of the long reuse distances from the two sets differ by no more \nthan 2k. In addition, we use a similar process to .nd the necessary condition for af.nity groups of more \nthan two elements. The average distance between any two elements in a g-element group is at most 2gk. \nThe same 0 condition holds for af.nity groups of g data sets. The condition is necessary, as shown by \nthe derivation. It is not suf.cient. Using the case of two-element groups again, the bound 2k is for \nthe worst case. In the best case, the difference between the average reuse distance is 0. An improvement \nis to check for the middle point, the bound of k. Another source of inaccuracy is that we do not check \nthe reuse distance of each access but only the average distance calculated from the total distance. It \nis possible that the total distance is the same but individual reuses differ by more than k in distance. \nAn improvement is to check the sum of each sub-set of memory accesses instead of the set of all accesses. \nThe dif.culty is to partition in the same way for accesses of the two data sets. We use the bins of the \nreuse signature and check the condition in each bin separately. Considering partition variations at the \nbin boundaries (as shown by the example in Figure 3), we apply the necessary condition to the sum of \nthe average of all bins rather than the average of each bin. The improved condition is as follows. Let \nthe reuse signature have B bins after removing short-distance addr lft rtwt inum rt Figure 4: Dendrogram \nfrom k-distance analysis for the .ve tree node .elds in Cheetah The reference af.nity forms a hierarchy \nfor different k values. Figure 4 shows the hierarchy in a dendrogram. The reference af.n\u00adity between \nlft and rtwt is strongest their accesses are within a distance of 19 data elements. Other two .elds, \ninum and rt, are used together within a distance of 55. These two groups are reused within k =69. The \nlast .eld, addr, has the least reference af.nity, not accessed with other .elds within hundreds of data. \n 3.3 Comparison with other methods This section discusses other locality analysis and shows the three \nunique features of the k-distance analysis the use of af.nity groups, the comparison based on reuse signatures, \nand the use of the con\u00adstant bound k. An experimental comparison follows in Section 5. Yi and Avgbe \nthe average reuse distance of the two data sets in the ith bin. bins. Let X and Y be the two sets of \ndata, and Avg Xi 3.3.1 Compiler analysis For programs written in regular loops and array access, a com\u00adpiler \ncan determine the exact access pattern and the best compu- B |Avg Xi -Avg Yi |=k \u00b7B (1) tation and \ndata organization. However, for programs with com\u00ad plex control .ows and indirect data access, a compiler \nmust make d = =1 The equation ensures that the average reuse distance per bin differs conservative assumptions \nto ensure the bene.t of a transformation. by no more than k. The left-hand side of the inequality is \nthe dif-While pro.ling analysis is more generally applicable, results from one pro.le may not re.ect \nthe access pattern in other executions. K-distance analysis alleviates this problem by relying on long \nreuse distances and the reuse signature, which often have consistent pat\u00adterns across data inputs. In \naddition, the analysis considers the ac\u00adcess pattern in multiple inputs (by merging reuse signatures) \nand further reduces the chance of a false positive. The negative con\u00adclusion from the analysis, i.e. \ntwo data do not have close reference af.nity, is always true for the program as a whole: an execution \nviolates the necessary condition in some program path.  3.3.2 Frequency pro.ling Often in programs some \ndata are more frequently used than oth\u00aders. Grouping the high frequency data often reduces the working \nset of a program. However, a question remains on how to place data that have the same access frequency, \nincluding the (larger) infre\u00adquently accessed data. Frequency is not the same as af.nity. Data having \nthe same frequency may not be accessed together at all. In comparison, the reference af.nity gives a \ndata partition based on the pattern of data reuse. It is important to measure distance by volume not \ntime. An execution may nibble small data bits or de\u00advour huge data sets in the same amount of time. Reuse \nsignature allows us to remove short-distance reuses as noises, regardless of their frequency or time \ndistance. An extension to frequency is the pair-wise af.nity the frequency that a pair of data are used \ntogether. The pair-wise af.nity forms a complete graph where each datum is a node and the pair-wise frequency \nis the edge weight. However, the reference af.nity is not transitive in a (pair-wise) graph. Consider \nthe access sequence abab..ab ... bcbc..bc: the pair-wise af.nity exists for a and b, for b and c, but \nnot for a and c. Hence the pair-wise af.nity is indeed pair wise and cannot guarantee the af.nity relation \nfor data groups with more than two elements. Furthermore, the criterion for two data accessed together \nis based on preselected cut-off radii. In comparison, k-distance analysis de.nes af.nity in data groups \nand measures the togetherness with a scale the data volume between accesses. 3.3.3 Statistical clustering \nSince the reuse signature is a vector, a natural impulse is to apply the sophisticated and readily available \nclustering techniques based on the well-grounded multivariate statistical theory. First proposed by MacQueen \nin 1967, k-means groups high-dimensional vectors by minimizing the within-group sum of distances to the \ngroup cen\u00adtroid [27]. It iteratively regroups points until a local minimum is reached [21]. It requires \nk, the number of groups, be part of the input. Af.nity analysis, however, cannot pre-determine the number \nof af.nity groups. An extension, x-means, .nds the best k using the Bayesian Information Criterion (BIC). \nIt compares different groups formed for different k s and chooses the one with the highest prob\u00adability \n[32]. In our earlier study, k-means and x-means proposed many can\u00addidates, and a few showed good improvements \n[42]. However, we could not explain the results in any sensible way because little con\u00adsistency existed \nbetween the best grouping of t clusters, t +1, and t - 1 clusters. To us, the groups seemed to come out \nrandomly, likely because we could not penetrate the two complex algorithms, each of which has a number \nof tunable parameters. For the same number of clusters, k-means produced entirely different groups than \nx-means did. The primary problem, as we learned, is that statistical clustering uses relative closeness. \nThe grouping of any two points is determined, not by their position but by the position of other points. \nFigure 5 shows two vector spaces each containing three points. The points X and Y have a .xed position, \nbut their grouping completely depends on the position of the third point Z. We can and we did al\u00adleviate \nthe problem by introducing anchor points into the space, but we also realized that the right measure \nis the absolute closeness, not the relative closeness. For example, the bene.t of grouping two data should \nbe determined by how they are accessed, not by other data, which may not even be used at all in the same \npart of the exe\u00adcution. By using the absolute closeness, we simpli.ed the problem and (happily) retired \nthe somewhat unwieldy statistical tools. Figure 5: An example of statistical clustering. The points \nX and Y have a .xed position, but their grouping completely depends on the position of the third point \nZ. 3.3.4 K%-distance K%-distance groups two reuse signatures X and Y (of length B) if the difference \np, shown below, is less than k%. The difference in each bin, |xi - yi|, can be in the number of reuses, \nthe sum of the reuse distance, or a combination of the two. .B |xi - yi| i=1 p<X,Y > = .B .B \u00d7 100% xi \n+ yi i=1 i=1 K%-distance is an improvement over statistical clustering. It mea\u00adsures the absolute closeness \nbetween reuse signatures. Compared to x-means or k-means, the result is tangible the difference in each \ngroup is no more than k%. The partitions are hierarchical. The groups resulted from a lower k must be \na .ner partition of those from a higher k. Despite being intuitive and hierarchical, k%-distance analysis \nhas a problem. It does not really measure the absolute closeness. For example, 1% of one thousand is \nmuch smaller than 1% of one million. While the .rst case indicates close access, the latter does not, \nat least not to the same degree. We need a measure that is not relative to the length of reuse distances, \nthe length of the execution, or the size of program data. The answer, as it turned out, is in our de.nition \nof the reference af.nity: the link length k. It is a constant. We describe k%-distance analysis as it \nwas in our study a half\u00adway solution. By comparing k-distance and k%-distance, we show the importance \nof the constant bound, which plays the central role in both the af.nity de.nition and the af.nity testing. \nThis connec\u00adtion is the most important discovery from this work. It unites the theory and the use of \nreference af.nity.   4. DATA REORGANIZATION Programs often have a large number of homogeneous data \nob\u00adjects such as molecules in a simulated space or nodes in a search tree. Each object has a set of attributes. \nIn Fortran 77 programs, at\u00adtributes of an object are stored separately in arrays. In C programs, the \nattributes are stored together in a structure. Neither scheme is sensitive to the access pattern of a \nprogram. A better way is to group attributes based on their reference af.nity. For arrays, the transfor\u00admation \nis array regrouping [14, 15]. For structures, it is structure splitting [9, 8, 34]. This section describes \nthe two transformations and their compiler support. 4.1 Array regrouping Figure 6 shows an example. Initially \nthe three coordinates of M molecules are stored in three arrays X, Y , and Z. They can be grouped into \na single array whose .rst dimension has 3 elements, equivalent to an array of M structures. Fortran 90 \nand C allows grouping arrays of different types by using structures. Ding and Kennedy showed that array \nregrouping can be automatically ap\u00adplied in Fortran programs by a compiler [15]. We use the same compiler \nsupport in this work. real*8 X(M), Y(M), Z(M) real*8 XYZ(3,M) (a) before grouping (b) after grouping \nFigure 6: Array regrouping example in Fortran 77  4.2 Structure splitting To reduce the programming \neffort and ensure the correctness of the transformed programs, we have built a compiler that handles \nstatic type-safe C programs. The compiler support is just enough to evaluate reference af.nity in experiments. \nIt is not our purpose in this paper to develop a general technique for structure splitting or to argue \nthat a general solution exists. In static type-safe C programs, a compiler knows the type of ev\u00adery memory \naccess. Given a program and a structure type targeted for splitting (we call it a split type), the compiler \nchanges the allo\u00adcation and the access of all objects of the split type (we call them split objects). \nIt .rst pre-allocates space in .eld arrays. A split ob\u00adject is allocated from available entries in .eld \narrays. A split object is no longer identi.ed by a pointer but by an index. Figure 7 shows an example \nwhere the three .elds of a structure type in Part (a) are split into two groups. Two .eld arrays are \npre-allocated in Part (b) as arrays of structures. The pointers to a split object are changed to integer \nindices in the new program. struct N fragm0 { int val; struct N {unsigned int left; int val; }; struct \nN* left; struct N fragm1 {struct N* right; unsigned int right; }; }; struct N fragm0 f0 store[RESERVED]; \n (a) before splitting struct N fragm1 f1 store[RESERVED]; (b) after splitting Figure 7: Structure splitting \nexample in C As object pointers become array indices, object access becomes array access (and structure \naccess if the array contains multiple .elds). Through type analysis, the compiler knows all the places \nwhere a split object is accessed and makes conversion in those and only those places. Taking the pointer \nof a .eld in a split object is allowed, which returns a pointer to an element in a .eld array. Three \nproblems immediately arise with this scheme. First, a split object can be local to a function and should \nnot permanently stay in .eld arrays. The compiler solves this problem by managing the upper-end of the \n.eld array as a stack and inserting allocation and free calls at the entry and the exit of the function. \nA .eld array is then shared by global and stack allocated objects in the same way as virtual memory is \nshared by heap and stack data, except that we manage a set of stacks for each split type. The second \nproblem happens when a split object is nested as a child inside a parent object. Three cases may happen. \nWhen the parent is split but the child is not, we support it by not splitting inside a .eld if it is \na structure. When the parent and the child both split, we convert the child .eld into a pointer, which \npoints to an independent object allocated when the parent object is allocated. We do not support the \nthird case when the child is split but the parent is not. Another major problem is the size of pre-allocation. \nThe con\u00adversion of pointers to indices actually enables a solution through dynamic re-allocation. During \nan execution when the pre-allocated space is full, an allocator doubles the .eld arrays by allocating \ntwice the space, copying the old array to the new space, changing the base of the .eld arrays, and reclaiming \nthe space from the old array. The object access is unimpeded after the re-allocation. To support the \narray re-allocation, the compiler can no longer let pointers be taken from a .eld of a split object. \nIn our test programs, pre-allocation is not an obstacle because the programmer speci.es a bounded size \nfor the main data structure, as done in all performance-critical pro\u00adgrams. For example, Cheetah sets \nthe maximal size of the splay tree by a compile-time constant. None of our programs needs re\u00adallocation \nat run time. We note that our scheme ignores a host of issues such as union types, non-local jumps, exceptions, \nand con\u00adcurrency because they do not arise in our test programs. One type of splitting is no splitting, \nwhere the compiler does not change the layout of object .elds, but it converts split objects from the \npointer form to the array form. In the evaluation section, we will measure the effect of reference af.nity \nusing the array allo\u00adcated version as the base case. We will also compare the (faster) performance of \narray version with that of the pointer version. Chilimbi et al. .rst used structure splitting to improve \ndata local\u00adity [9]. For type safe programs, they split an object into two parts, one storing frequently \naccessed .elds and the other storing the rest. A pointer is inserted into the .rst part to link to the \nsecond part. Rabbah and Palem split structured data in C programs by allocating objects in large chunks \nwhere structure .elds were stored in sep\u00adarate arrays [34]. Their method uses the address of the .rst \n.eld to identify the object and calculates the address of other .elds at run time. It uses point-to analysis \nand dynamic checks to ensure correctness. It avoids the space and time cost of additional pointers as \nin the method of Chilimbi et al., but run-time (access) checks are needed for correctness even for static \ntype-safe programs. In comparison, our array allocation relies on static type safety not run\u00adtime checking. \nIt does not support as many types of programs or as ef.cient dynamic allocation as the other two methods \ndo, but our method incurs least overhead when accessing the transformed data. Previous work did not explore \nall choices of structure splitting. Chilimbi split structure .elds into at most two parts [9]. Rabbah \nand Palem split structure .elds completely [34]. Our compiler permits arbitrary structure splitting, \nwhich is needed for using and evaluat\u00ading reference af.nity.  5. EVALUATION This section evaluates \nk-distance analysis and compares it with the alternative data layouts given by the programmer and four \nother methods described in Section 3.3. 5.1 Methodology Test programs. Table 1 lists a set of 9 programs. \nThe .rst four are array-based programs, including two from Spec95 suite and two dynamic programs originally \nfrom the Chaos group [12] for un\u00adstructured mesh and N-body simulation. The rest .ve programs use different \ntree structures including a splay tree for cache simulation, a quad-tree for image processing, and binary \nsearch trees for sort\u00ading and various other purposes. The cache simulator is part of the SimpleScalar \ntool set. The other four programs come from Olden benchmark set. We use different inputs for training \nand testing, as listed in the table. For a fair comparison, we only use one training input for each benchmark, \ndespite that k-distance analysis can use multiple training runs. We do not test more programs in part \nbecause our current com\u00adpiler cannot safely split structures in all Olden programs, but also because \nthe current set is statistically signi.cant. All programs have at least 3 .elds or arrays, .ve have 7 \nor more, and one program, Swim, has 14 arrays. A program with n arrays or structure .elds .n has an exponential \nnumber (at least 2n-2 and i=0 i(n-i) to be exact) of possible data layouts. The possible layouts is 4 \nfor n =3, 210 for n =7, and over 6 million for n =14. Therefore, the prob\u00adability for a single method \nto consistently pick the best layout for all programs among all tested choices and on all tested machines \nis effectively zero, unless the method is indeed the best. Platforms. The experiments use two machines, \na 1.3GHz IBM Power4 processor with the AIX compiler, and a 2GHz Intel Pen\u00adtium IV processor with the \nLinux gcc compiler. All testing pro\u00adgrams are compiled at the optimization level -O5 -qstrict with AIX \nand -O3 with Linux gcc respectively. The time is for complete ex\u00adecutions on unloaded processors. We \ntake the shortest time in 5 runs. The programs run 10% to 60% faster on IBM than on PC due to a faster \nprocessor and a better compiler. The only excep\u00adtion is TSP, which runs twice slower on IBM possibly \nbecause of the highly frequent .oating-point square-root operations (used by TSP to calculate the Euclidean \ndistance). We also tested the pro\u00adgrams on a 250 MHz MIPS R10K processor on SGI Origin2000 using the \nMIPSpro compiler and a 336 MHz UltraSparc processor using the Sun compiler at the highest optimization \nlevel. The qual\u00aditative results are similar. The best layout on IBM is also the best on SGI or Sun. The \nprograms run up to .ve times slower on SGI and many more times slower on Sun. The improvement is less \ndra\u00admatic because the memory problem is less severe on the two slower processors. We do not report SGI \nand Sun results for lack of space. Tools. We use the source-level instrumentation and a run-time mon\u00aditor \nto pro.le accesses for individual arrays and structure .elds [16, 40]. They are accurate and ef.cient. \nFor example in one execution of Cheetah, the monitor tracks the distinct access to 1.2 million data elements \nusing a hashtable of less than 18 thousand entries. We use a 99%-accurate analyzer for reuse-distance \nanalysis [17]. It measures reuse distance in long traces in effectively linear time and guarantees that \nthe measured distance is between 99% and 100% of the actual distance. The two tools give the reuse signature \nfor each array and structure .eld. The k-means and x-means tools come from Pelleg and Moore [32]. Compiler-based \narray regrouping is due to Ding and Kennedy [13, 15]. We implement k-distance, k%\u00addistance, and frequency \nmethods in MATLAB and array regrouping and structure splitting in our compilers as described in Section \n4. Analysis Time. The trace and af.nity analysis takes a time pro\u00adportional to the length of the execution \nand the number of data el\u00adements to be clustered. The pro.ling is more time consuming than the clustering. \nFor all the tested programs, the pro.ling time is less than ten minutes. The k-distance analysis on reuse \nsignatures takes less than one second.  5.2 Performance Comparison We compare nine methods. The results \nare shown in Table 2 for the PC and Table 3 for the IBM machine. All execution times are in seconds. \nThe last row is the arithmetic mean of the speedup. The .rst is the original layout coded by the programmer. \nThe next two are k-distance for k =256 and 64, followed by two k%\u00addistance methods for 1% and 0.1%. Then \nit is x-means. We do not include k-means because it cannot pre-determine the number of af.nity groups. \nThe seventh method uses the access frequency to divide data into groups. It clusters the smallest set \nthat accounts for at least 50% of all accesses into one group and stores the others in single arrays. \nThe eighth method uses the compiler implementation from Ding and Kennedy [13, 15]. An interesting use \nof k-distance analysis is to .nd the worst data layout by reversing the reference af.nity. It has no \npractical use other than showing the range of the effect from the data layout. The last column gives \nthe execution time of data layout obtained by this reverse k-distance(RK) method. With the parameter \n2048, it groups data whose average reuse dis\u00adtance is greater than 2K. For all methods, the same data \nlayout is tested on both machines. The k-distance method for k = 256 should be the best method. It groups \ndata that are almost always used within one to two kilo\u00adbytes of data access, which .ts comfortably in \nthe L1 cache of all machines we use. For large cache, k should be greater. We stick to a single value \nbecause all other methods use a single parameter. Indeed, the analysis picks the best data layout for \nall programs on the two machines. Swim has 14 arrays and over 6 million possi\u00adble choices, the analysis \nsingles out a layout, which outperforms all others by a wide margin. The af.nity hierarchy of Swim is \na very impressive, large tree, as shown by Figure 8. On the PC, the layout from k-distance is 4% faster \nthan the fastest alternative layout and 38% faster than the original layout. The improvement is smaller \non IBM, 8% faster than the original. The margin is indisputable no other known layout on both machines \ncomes within 90% perfor\u00admance of this seemingly singular choice by the analysis. For other programs, \n256-distance improves Tomcatv and Perime\u00adter by 25% and TSP by 20% on the PC. The average improvement \nis 12% on the PC and 4.5% on IBM. The improvement is more signi.cant for programs that have many arrays \nor .elds. The anal\u00adysis does not blindly transform a program. The structure in Bisort given by the programmer \nhas the best performance. The analysis recommends no change. 256-distance analysis runs slower in .ve \nplaces, TreeAdd against the original on the PC, Tomcatv against x-means on IBM, Perimeter against 64-distance \nand Moldyn against x-means and the reverse k\u00addistance on the PC. However, the loss is extremely small \n(no more than 1% or 0.004 seconds) and happens on only one machine. For TreeAdd against the original, \nit loses by 0.7% on the PC but wins by 13% on IBM. Except for them, 256-distance always picks the Table \n1: Benchmark characteristics Benchmark Source Description Main data structure Training input Testing \ninput Swim Spec95 shallow water equation 14 real arrays test(1282) ref(5122) Tomcatv Spec95 vectorized \nmesh generation 9 real arrays test(5132) ref(5132) Mesh Chaos Group mesh structure simulation 7 .oat \narrays 10k 10k MolDyn Chaos Group molecular dynamics simulation 3 double arrays 13500 molecules 62500 \nmolecules Cheetah SimpleScalar fully associative LRU cache simulator splay tree, 5 .elds jpeg encode \n21.8K image jpeg encode 940K image Bisort Olden forward &#38; backward integer sorting binary tree, 3 \n.elds 217 nodes 221 nodes Perimeter Olden perimeters of regions in images quad-tree, 7 .elds 12 levels \n12 levels TreeAdd Olden recursive sum of values in a balanced-tree binary tree, 3 .elds 218 nodes 222 \nnodes TSP Olden traveling salesman problem solver binary tree, 7 .elds 105 nodes 4 * 106 nodes Table \n2: Execution time (sec) on Intel Pentium IV Benchmark Orig K=256 K=64 K=1% K=0.1% X-means Freq=50% Static \nWorst Swim 52.34 37.90 53.15 46.99 53.15 39.28 45.84 45.37 38.36 Tomcatv 45.37 36.43 36.43 36.43 36.43 \n37.65 37.35 36.85 38.15 MolDyn 69.78 69.78 69.78 69.78 69.78 69.68 69.78 69.78 69.55 Mesh 4.31 4.25 4.31 \n4.25 4.31 5.29 5.69 4.31 15.80 Cheetah 263.96 263.64 263.64 263.64 263.64 293.23 306.76 compiler analysis \nnot applicable 330.93 Bisort 12.16 12.16 12.16 12.16 12.16 14.38 14.22 15.98 Perimeter 0.035 0.028 0.026 \n0.028 0.028 0.028 0.029 0.039 TreeAdd 0.262 0.264 0.264 0.264 0.264 0.264 0.264 0.272 TSP 17.79 14.86 \n14.86 16.95 14.92 16.91 14.86 17.19 Average Speedup 1.000 1.120 1.085 1.074 1.074 1.044 1.025 1.096 0.920 \n 16 14 12 10 8 6 4 2 0 Figure 8: Dendrogram from k-distance for Swim best data layout when competing \nagainst all 8 other methods for all 9 programs and 2 machines. It ties or wins in 97% of all contests. \nWhen measured by the average speed on the two machines, among the 72 alternative layouts, 256-distances \nloses to only x-means on MolDyn by less than one tenth of a percent. The layout by k-distance with k \n=64 is overly conservative because it does not exploit the af.nity at a distance greater than 64. It \npicks the same data layout as k = 256 for all but three programs, suggesting that the af.nity groups \nhave a short distance in the majority of the programs. The two k%-clustering methods give competitive \nresults, im\u00adproving average performance by 7% on the PC and 3% on IBM. The two k values of 1% and 0.1% \nyield the same layout in all but three programs; k =1% wins in two on the PC, but loses on two k (log \n2) on IBM. This veri.es that no single k% is the best, because k% does not have an absolute meaning and \nmay be too small in one program but too large in another. In contrast, the k in k-distance has an absolute \nmeaning the volume distance. X-means, using the relative closeness, performs worse than all methods using \nthe absolute closeness. On average, the performance is improved by 4% on the PC but slightly impaired \non IBM. X\u00admeans features bewilderingly different quality on the two machines. It gives the third best \ndata layout for Swim on the PC, but the same layout is the third slowest on IBM. The three .elds of Bisort \nhave al\u00admost identical reuse signatures. Based on the relative closeness, x\u00admeans stubbornly splits them \ninto two groups, losing performance by 17% on PC and 30% on IBM. The frequency splitting using a single \nparameter performs worse on average than all other methods, showing that grouping on fre\u00adquency is not \nas good as grouping on reuse signatures. The com\u00adpiler analysis is conservative. It causes no slowdown \nexcept for Swim on IBM but has less bene.t than k-distance analysis. It can\u00adnot yet analyze structure \naccess in C programs. Reference af.nity has a very different effect on the two machines for the .rst \ntwo Fortran programs. The improvement for Swim is 8% on IBM but 38% on the PC. The reason is the compiler. \nThe IBM compiler performs sophisticated loop transformations to im\u00adprove the spatial locality, while \nthe Gcc compiler does little, leaving ample room for improvement. The IBM compiler outperforms the static \ndata regrouping of Ding and Kennedy for Swim but not for Tomcatv [15]. In both cases, reference af.nity \noutperforms the best static compiler optimization. On IBM, the reverse k-distance gives the slowest running \ntime for all benchmarks except for Perimeter. The average loss is 12%. The results on the PC are mixed, \nlikely for factors other than the Table 3: Execution time (sec) on IBM Power 4 Benchmark Orig K=256 \nK=64 K=1% K=0.1% X-means Freq=50% Static Worst Swim 25.32 23.46 26.01 27.87 26.01 26.48 27.29 26.85 29.80 \nTomcatv 21.74 20.70 20.70 20.70 20.70 20.60 23.74 20.90 23.80 MolDyn 52.22 52.22 52.22 52.22 52.22 52.25 \n52.22 52.22 52.79 Mesh 3.29 3.26 3.29 3.26 3.29 3.32 3.42 3.29 5.09 Cheetah 195.08 190.60 190.60 190.60 \n190.60 204.45 208.65 compiler analysis not applicable 218.76 Bisort 8.07 8.07 8.07 8.07 8.07 10.44 10.25 \n11.16 Perimeter 0.025 0.021 0.022 0.021 0.021 0.021 0.024 0.024 TreeAdd 0.230 0.226 0.226 0.226 0.226 \n0.226 0.226 0.258 TSP 41.69 40.34 40.34 40.47 40.37 41.15 40.34 41.73 Average Speedup 1.000 1.045 1.026 \n1.026 1.032 0.995 0.958 0.996 0.883 data layout. However, it still gives the worst slowdown of 8% on \naverage. By comparing the best (k=256) and the worst (rk=2048) data layout, we observe that reference \naf.nity affects performance differently, from almost no effect in MolDyn on both machines to 74% for \nMesh on the PC and 36% for the same program on IBM. The average effect is 20% on the PC and 16% on the \nIBM machine. Table 4 shows the effect of converting the .ve C programs from using pointer-based data \nto using array allocation. On average, the array version improves the original pointer version by 30% \non the PC and 44% on the IBM machine. The only degration occurs for TSP on the PC. But k-distance splitting \neventually improves the performance by 10%. The bene.t of array allocation becomes more signi.cant when \na program deals with the inputs with a larger mem\u00adory working set. An example is Cheetah. The input to \nthe cache simulator is the access trace of the ijpeg encoding program from MediaBench. The original, \npointer-based version outperforms the best array version when simulating the encoding of an image line \nby line with a memory footprint of thousands of kilobytes. When we change the encoding option to interlaced \nGIF .les and enlarge the footprint to tens of megabytes, the encoding takes much longer, and the array \nversion runs consistently faster than the pointer version. The Cheetah results in this section are for \none of the larger inputs. We found a similar trend in other programs. The improvement from the reference \naf.nity becomes greater when a program uses a larger input and takes longer to run. Next we quantify \nthe improvement across all program inputs for one of our test programs. Miss-rate improvement across \nall inputs. The effect of data trans\u00adformation may change with program inputs. Our past work showed that \nTomcatv had a predictable miss rate across all inputs [41]. Us\u00ading that tool on a DEC Alpha machine, \nwe draw the miss rate of 96KB cache (the L2 on-chip cache size size of Alpha) for the origi\u00adnal and k-distance \nanalysis with k =256as two curves in Figure 9. The data input is measured by the data size in cache blocks. \nArray regrouping reduces the miss rate by little to over 5% depending on the input. The vertical bar \nmarks the data input used in our exper\u00adiments. The difference is about 0.7% in the absolute miss rate. \nOn the DEC machine, the corresponding speed improvement is 7.24%.  6. RELATED WORK This section discusses \nmainly the past work on data transforma\u00adtion. The discussion is more bibliographical than technical. \nThe reader should refer Section 3.3 for a technical comparison. Early compiler analysis identi.es groups \nof data that are used to\u00adgether in loop nests. Thabit used the concept of pair-wise af.nity he called \nreference proximity [38]. Wolf and Lam [29] and McKin\u00adley et al. [39] used reference groups. Thabit showed \nthat the op\u00adtimal data placement using the pair-wise af.nity is NP-hard [38]. Kennedy and Kremer gave \na general model that considered, among others, run-time data transformation. They showed that the problem \nis also NP-hard [24]. Ding and Kennedy used the results of Thabit and of Kennedy and Kremer to prove \nthe complexity of the par\u00adtial and dynamic reference af.nity [14]. To reduce false sharing in multi-treaded \nprograms, Anderson et al. [4] and Eggers and Jeremi\u00adassen [22] grouped data accessed by the same thread. \nAnderson et al. optimized a program for computation as well as data locality, but they did not combine \ndifferent arrays. Eggers and Jeremiassen combined multiple arrays for thread locality, but their scheme \nmay hurt cache locality if not all thread data are used at the same time. For improving the cache performance, \nDing and Kennedy grouped arrays that are always used together in a program [14]. They gave the optimal \narray layout for strict af.nity. They later grouped ar\u00adrays at multiple granularity [15]. An earlier \nversion of this work de.ned hierarchical reference af.nity and tested two programs us\u00ading x-means and \nk-means clustering [42]. Program pro.ling has long been used to measure the frequency of data access \n[25]. Seidl and Zorn grouped frequently accessed objects to improve virtual memory performance [35]. \nUsing pair\u00adwise af.nity, Calder et al. [7] and Chilimbi et al. [10] developed algorithms for hierarchical \ndata placement in dynamic memory al\u00ad Table 4: Execution time(sec) comparison between array-based and \npointer-based versions Benchmark Intel Pentium IV IBM Power 4 Pointer\u00adbased Array\u00adbased K=256 Pointer\u00adbased \nArray\u00adbased K=256 Cheetah 290.78 263.96 263.64 209.39 195.08 190.60 Bisort 13.79 12.16 12.16 9.57 8.07 \n8.07 Perimeter 0.039 0.035 0.028 0.033 0.025 0.021 TreeAdd 0.584 0.262 0.264 0.598 0.230 0.226 TSP 16.32 \n17.79 14.86 42.10 41.69 40.34 Average Speedup 1.000 1.300 1.388 1.000 1.438 1.509 location. The locality \nmodel of Calder et al. was an extension of the temporal relation graph of Gloy and Smith, who considered \nreuse distance in estimating the af.nity relation [19]. Chilimbi et al. split structure data in C and \nJava programs using pair-wise af.nity [9]. Chilimbi later improved structure splitting using the frequency \nof data sub-streams called hot-streams [8]. Hot-streams combines dy\u00adnamic af.nity with frequency but \ndoes not yet give whole-program reference af.nity. Access frequency and pair-wise af.nity do not distinguish \nthe time or the distance of data reuses. Petrank and Rawitz formalized this observation and proved a \nharsh bound: with only frequency or pair-wise information, no algorithm can guarantee a static data layout \nwithin a factor of k - 3 from the optimal solution, where k is proportional to the size of cache [33]. \nRabbah and Palem gave another method for structure splitting. It .nds opportunities for complete splitting \nby calculating the neigh\u00adbor af.nity probability without constructing an explicit af.nity graph [34]. \nThe probability shows the quality of a given layout but does not suggest the best reorganization. Their \nsplitting method is fully automatic as discussed in Section 4. Reference af.nity may change during a \ndynamic execution. Re\u00adsearchers have examined various methods for dynamic data reor\u00adganization [11, 13, \n20, 30, 31, 37]. Ding and Kennedy found that consecutive packing (.rst-touch data ordering) best exploits \nrefer\u00adence af.nity for programs with good temporal locality [13], an ob\u00adservation later con.rmed by Mellor-Crummey \net al. [30] and Strout et al [37]. Ding and Kennedy considered the time distance of data reuses and used \nthe information in group packing. They also gave a uni.ed model in which consecutive packing and group \npacking became special cases. In principle, the model of reference af.nity can be used at run time to \nanalyze sub-parts of an execution. How\u00adever, it must be very ef.cient to be cost effective. On-line af.nity \nanalysis is a subject of our on-going study. Matson et al. .rst used reuse distance to measure the perfor\u00admance \nof virtual memory systems [28]. The recent uses include those of Zhou et al. [43] and Jiang and Zhang \n[23], who studied .le caching and showed a signi.cant improvement for program, server, and database traces. \nAt least .ve compiler groups have used reuse distance to analyze program locality [3, 5, 15, 26, 40]. \nBeyls and D Hollander used per-reference distance pattern to annotate pro\u00adgrams with cache hints and \nimproved SPEC95 FP program perfor\u00admance by 7% on an Itanium processor [6]. Ding and Zhong ana\u00adlyzed large \ntraces by reducing the analysis cost to near linear time. They found that reuse-distance histograms change \nin predictable patterns in many programs [17]. Zhong et al. used this result to predict cache miss rates \nacross program inputs and cache con.gu\u00adrations [41]. Loop transformations have long been used to arrange \nstride-one access to maximize spatial reuse (see examples in [1, 18, 29] or a comprehensive text [2]). \nComputation reodering is preferable be\u00adcause it makes no (negative) impact in other parts of a program. \nStill, this paper shows that using reference af.nity, data transforma\u00adtion is bene.cial for programs \nwhere loop transformations or other types of static techniques are inadequate because of the complex \ncontrol .ow and indirect data access.  7. CONCLUSIONS Reference af.nity gives a unique and hierarchical \npartition of program data. The reference af.nity among the source-level data can be tested by k-distance \nanalysis. The result of k-distance analy\u00adsis has an intuitive meaning. The elements of an af.nity group \nmust be accessed within a volume distance of at most k. Experiments show that the new method uncovers \nrich af.nity relations among the data in complex programs. When used in array and structure reorganization, \nk-distance analysis outperforms all other methods with remarkable consistency. The close agreement between \ntheo\u00adretical properties and experimental observations suggests that ref\u00aderence af.nity is an effective \nway to bridge the gap between the memory hierarchy of a machine and the linear data layout of a pro\u00adgram. \n 8. ACKNOWLEDGMENTS Ken Kennedy gave the name reference af.nity [15]. The expla\u00adnation shown in Figure \n5 was due to Mitsu Ogihara. Kevin Stoodley pointed out the cause for the different effects of reference \naf.nity on the IBM and the PC machines. The work and its presentation bene.ted from discussions with \nTrishul Chilimbi, Guang Gao, and our colleagues at University of Rochester and Rice University as well \nas the anonymous reviewers of the LCPC 2003 workshop and the PLDI 2004 conference. This work is supported \nby the National Science Foundation (Contract No. CCR-0238176, CCR-0219848, and EIA-0080124), the Department \nof Energy (Contract No. DE\u00adFG02-02ER25525), and an equipment grant from IBM. 9. REFERENCES [1] W. Abu-Sufah, \nD. Kuck, and D. Lawrie. On the performance enhancement of paging systems through program analysis and \ntransformations. IEEE Transactions on Computers, C-30(5):341 356, May 1981. [2] R. Allen and K. Kennedy. \nOptimizing Compilers for Modern Architectures: A Dependence-based Approach. Morgan Kaufmann Publishers, \nOctober 2001. [3] G. Almasi, C. Cascaval, and D. Padua. Calculating stack distances ef.ciently. In Proceedings \nof the .rst ACM SIGPLAN Workshop on Memory System Performance, Berlin, Germany, June 2002. [4] J. Anderson, \nS. Amarasinghe, and M. Lam. Data and computation transformation for multiprocessors. In Proceedings of \nthe Fifth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, Santa Barbara, CA, \nJuly 1995. [5] K. Beyls and E. D Hollander. Reuse distance as a metric for cache behavior. In Proceedings \nof the IASTED Conference on Parallel and Distributed Computing and Systems, August 2001. [6] K. Beyls \nand E. D Hollander. Reuse distance-based cache hint selection. In Proceedings of the 8th International \nEuro-Par Conference, Paderborn, Germany, August 2002. [7] B. Calder, C. Krintz, S. John, and T. Austin. \nCache-conscious data placement. In Proceedings of the Eighth International Conference on Architectural \nSupport for Programming Languages and Operating Systems (ASPLOS-VIII), San Jose, Oct 1998. [8] T. M. \nChilimbi. Ef.cient representations and abstractions for quantifying and exploiting data reference locality. \nIn Proceedings of ACM SIGPLAN Conference on Programming Language Design and Implementation, Snowbird, \nUtah, June 2001. [9] T. M. Chilimbi, B. Davidson, and J. R. Larus. Cache-conscious structure de.nition. \nIn Proceedings of ACM SIGPLAN Conference on Programming Language Design and Implementation, Atlanta, \nGeorgia, May 1999. [10] T. M. Chilimbi, M. D. Hill, and J. R. Larus. Cache-conscious structure layout. \nIn Proceedings of ACM SIGPLAN Conference on Programming Language Design and Implementation, Atlanta, \nGeorgia, May 1999. [11] R. Das, D. Mavriplis, J. Saltz, S. Gupta, and R. Ponnusamy. The design and implementation \nof a parallel unstructured euler solver using software primitives. In Proceedings of the 30th Aerospace \nScience Meeting, Reno, Navada, January 1992. [12] R. Das, M. Uysal, J. Saltz, and Y.-S. Hwang. Communication \noptimizations for irregular scienti.c computations on distributed memory architectures. Journal of Parallel \nand Distributed Computing, 22(3):462 479, Sept. 1994. [13] C. Ding and K. Kennedy. Improving cache performance \nin dynamic applications through data and computation reorganization at run time. In Proceedings of the \nSIGPLAN 99 Conference on Programming Language Design and Implementation, Atlanta, GA, May 1999. [14] \nC. Ding and K. Kennedy. Inter-array data regrouping. In Proceedings of The 12th International Workshop \non Languages and Compilers for Parallel Computing, La Jolla, California, August 1999. [15] C. Ding and \nK. Kennedy. Improving effective bandwidth through compiler enhancement of global cache reuse. Journal \nof Parallel and Distributed Computing, 64(1), 2004. [16] C. Ding and Y. Zhong. Compiler-directed run-time \nmonitoring of program data access. In Proceedings of the .rst ACM SIGPLAN Workshop on Memory System Performance, \nBerlin, Germany, June 2002. [17] C. Ding and Y. Zhong. Predicting whole-program locality with reuse distance \nanalysis. In Proceedings of ACM SIGPLAN Conference on Programming Language Design and Implementation, \nSan Diego, CA, June 2003. [18] D. Gannon, W. Jalby, and K. Gallivan. Strategies for cache and local memory \nmanagement by global program transformations. In Proceedings of the First International Conference on \nSupercomputing. Springer-Verlag, Athens, Greece, June 1987. [19] N. Gloy and M. D. Smith. Procedure placement \nusing temporal-ordering information. ACM Transactions on Programming Languages and Systems, 21(5), September \n1999. [20] H. Han and C. W. Tseng. Locality optimizations for adaptive irregular scienti.c codes. Technical \nreport, Department of Computer Science, University of Maryland, College Park, 2000. [21] J. A. Hartigan. \nClustering Algorithms. John Wiley &#38; Sons, 1975. [22] T. E. Jeremiassen and S. J. Eggers. Reducing \nfalse sharing on shared memory multiprocessors through compile time data transformations. In Proceedings \nof the Fifth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, pages 179 188, \nSanta Barbara, CA, July 1995. [23] S. Jiang and X. Zhang. LIRS: an ef.cient low inter-reference recency \nset replacement to improve buffer cache performance. In Proceedings of ACM SIGMETRICS Conference on Measurement \nand Modeling of Computer Systems, Marina Del Rey, California, June 2002. [24] K. Kennedy and U. Kremer. \nAutomatic data layout for distributed memory machines. ACM Transactions on Programming Languages and \nSystems, 20(4), 1998. [25] D. Knuth. An empirical study of FORTRAN programs. Software Practice and Experience, \n1:105 133, 1971. [26] Z. Li, J. Gu, and G. Lee. An evaluation of the potential bene.ts of register allocation \nfor array references. In Workshop on Interaction between Compilers and Computer Architectures in conjuction \nwith the HPCA-2, San Jose, California, February 1996. [27] J. MacQueen. Some methods for classi.cation \nand analysis of multivariate observations. In Proceedings of 5th Berkeley Symposium on Mathematical Statisitics \nand Probability, pages 281 297, 1967. [28] R. L. Mattson, J. Gecsei, D. Slutz, and I. L. Traiger. Evaluation \ntechniques for storage hierarchies. IBM System Journal, 9(2):78 117, 1970. [29] K. S. McKinley, S. Carr, \nand C.-W. Tseng. Improving data locality with loop transformations. ACM Transactions on Programming Languages \nand Systems, 18(4):424 453, July 1996. [30] J. Mellor-Crummey, D. Whalley, and K. Kennedy. Improving \nmemory hierarchy performance for irregular applications. International Journal of Parallel Programming, \n29(3), June 2001. [31] N. Mitchell, L. Carter, and J. Ferrante. Localizing non-af.ne array references. \nIn Proceedings of International Conference on Parallel Architectures and Compilation Techniques, Newport \nBeach, California, October 1999. [32] D. Pelleg and A. Moore. X-means: Extending k-means with ef.cient \nestimaiton of the number of clusters. In Proceddings of the 17th International Conference on Machine \nLearning, pages 727 734, San Francisco, CA, 2000. [33] E. Petrank and D. Rawitz. The hardness of cache \nconscious data placement. In Proceedings of ACM Symposium on Principles of Programming Languages, Portland, \nOregon, January 2002. [34] R. M. Rabbah and K. V. Palem. Data remapping for design space optimization \nof embedded memory systems. ACM Transactions in Embedded Computing Systems, 2(2), 2003. [35] M. L. Seidl \nand B. G. Zorn. Segregating heap objects by reference behavior and lifetime. In Proceedings of the Eighth \nInternational Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-VIII), \nSan Jose, Oct 1998. [36] X. Shen, Y. Zhong, and C. Ding. Regression-based multi-model prediction of data \nreuse signature. In Proceedings of the 4th Annual Symposium of the Las Alamos Computer Science Institute, \nSante Fe, New Mexico, November 2003. [37] M. M. Strout, L. Carter, and J. Ferrante. Compile-time composition \nof run-time data and iteration reorderings. In Proceedings of ACM SIGPLAN Conference on Programming Language \nDesign and Implementation, San Diego, CA, June 2003. [38] K. O. Thabit. Cache Management by the Compiler. \nPhD thesis, Dept. of Computer Science, Rice University, 1981. [39] M. E. Wolf and M. Lam. A data locality \noptimizing algorithm. In Proceedings of the SIGPLAN 91 Conference on Programming Language Design and \nImplementation, Toronto, Canada, June 1991. [40] Y. Zhong, C. Ding, and K. Kennedy. Reuse distance analysis \nfor scienti.c programs. In Proceedings of Workshop on Languages, Compilers, and Run-time Systems for \nScalable Computers, Washington DC, March 2002. [41] Y. Zhong, S. G. Dropsho, and C. Ding. Miss rate preidiction \nacross all program inputs. In Proceedings of the 12th International Conference on Parallel Architectures \nand Compilation Techniques, New Orleans, Louisiana, September 2003. [42] Y. Zhong, X. Shen, and C. Ding. \nA hierarchical model of reference af.nity. In Proceedings of the 16th International Workshop on Languages \nand Compilers for Parallel Computing, College Station, Texas, October 2003. [43] Y. Zhou, P. M. Chen, \nand K. Li. The multi-queue replacement algorithm for second level buffer caches. In Proceedings of USENIX \nTechnical Conference, June 2001.  \n\t\t\t", "proc_id": "996841", "abstract": "While the memory of most machines is organized as a hierarchy, program data are laid out in a uniform address space. This paper defines a model of <i>reference affinity</i>, which measures how close a group of data are accessed together in a reference trace. It proves that the model gives a hierarchical partition of program data. At the top is the set of all data with the weakest affinity. At the bottom is each data element with the strongest affinity. Based on the theoretical model, the paper presents <i>k-distance analysis</i>, a practical test for the hierarchical affinity of source-level data. When used for array regrouping and structure splitting, <i>k</i>-distance analysis consistently outperforms data organizations given by the programmer, compiler analysis, frequency profiling, statistical clustering, and all other methods we have tried.", "authors": [{"name": "Yutao Zhong", "author_profile_id": "81100049521", "affiliation": "University of Rochester", "person_id": "PP14028644", "email_address": "", "orcid_id": ""}, {"name": "Maksim Orlovich", "author_profile_id": "81100361651", "affiliation": "University of Rochester", "person_id": "P677801", "email_address": "", "orcid_id": ""}, {"name": "Xipeng Shen", "author_profile_id": "81452603368", "affiliation": "University of Rochester", "person_id": "PP39038591", "email_address": "", "orcid_id": ""}, {"name": "Chen Ding", "author_profile_id": "81309499457", "affiliation": "University of Rochester", "person_id": "PP43124106", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/996841.996872", "year": "2004", "article_id": "996872", "conference": "PLDI", "title": "Array regrouping and structure splitting using whole-program reference affinity", "url": "http://dl.acm.org/citation.cfm?id=996872"}