{"article_publication_date": "06-09-2004", "fulltext": "\n Fast Searches for Effective Optimization Phase Sequences Prasad Kulkar ni1, Stephen Hines1, Jason Hiser2, \nDavid Whalley1, Jack Davidson2, Douglas Jones3 1Computer Science Dept., Florida State University, Tallahassee, \nFL 32306-4530; e-mail: whalley@cs.fsu.edu 2Computer Science Dept., University of Virginia, Charlottesville, \nVA 22904; e-mail: jwd@virginia.edu 3Electr ical and Computer Eng. Dept, University of Illinois, Urbana, \nIL 61801; e-mail: dl-jones@uiuc.edu ABSTRACT It has long been known that a .xed ordering of optimization \nphases will not produce the best code for every application. One approach for addressing this phase ordering \nproblem is to use an evolutionary algorithm to search for a speci.c sequence of phases for each module \nor function. While such searches have been shown to produce more ef.cient code, the approach can be extremely \nslow because the application is compiled and executed to evaluate each sequence s effectiveness. Consequently, \nevolu\u00adtionary or iterative compilation schemes have been promoted for compilation systems targeting embedded \napplications where longer compilation times may be tolerated in the .nal stage of development. In this \npaper we describe two complementary gen\u00aderal approaches for achieving faster searches for effective opti\u00admization \nsequences when using a genetic algorithm. The .rst approach reduces the search time by avoiding unnecessary \nexecu\u00adtions of the application when possible. Results indicate search time reductions of 65% on average, \noften reducing searches from hours to minutes. The second approach modi.es the search so fewer generations \nare required to achieve the same results. Mea\u00adsurements show that the average number of required generations \ndecreased by 68%. These improvements have the potential for making evolutionary compilation a viable \nchoice for tuning embedded applications. Categories and Subject Descriptors D.3.4 [Programming Languages]: \nProcessors - compilers, opti\u00admization D.4.7 [Operating Systems]: Organization and Design - real-time \nsystems and embedded systems. General Terms Measurement, Performance, Experimentation, Algorithms. \nKeywords Phase ordering, interactive compilation, genetic algorithms. Permission to make digital or hard \ncopies of all or part of this work for personal or classroom use is granted without fee provided that \ncopies are not made or distributed for pro.t or commercial advantage and that copies bear this notice \nand the full citation on the .rst page. To copy otherwise, or republish, to post on servers or to redistribute \nto lists, requires prior spe\u00adci.c permission and/or a fee. PLDI 04, June 9-11, 2004, Washington, DC, \nUSA. Copyright 2004 ACM 1-58113-807-5/04/0006...$5.00.  1. INTRODUCTION The phase ordering problem \nhas long been known to be a dif.cult dilemma for compiler writers [17, 19]. One sequence of optimiza\u00adtion \nphases is highly unlikely to be the most effective sequence for every application (or even for each function \nwithin a single appli\u00adcation) on a given machine. Whether or not a particular optimiza\u00adtion enables or \ndisables opportunities for subsequent optimiza\u00adtions is dif.cult to predict since it depends on the application \nbeing compiled, the previously applied optimizations, and the tar\u00adget architecture [19]. One approach \nto deal with this problem is to search for effec\u00adtive optimization phase sequences using genetic algorithms \n[5, 11]. When the .tness criteria for such searches involve dynamic measures (e.g., cycle counts or power \nconsumption), thousands of direct executions of an application may be required. The search time can be \nsigni.cant, often needing hours or days when .nding effective sequences for a single application, making \nit less attrac\u00adtive for developers. There are application areas where long compilation times are acceptable. \nFor example, long compilation times may be tolerated in application areas where the problem size is directly \nrelated to the execution time to solve the problem. In fact, the size of many computational chemistry \nand high-energy physics problems is limited by the elapsed time to reach a solution (typically a few \ndays or a week). Long compilation times may be acceptable if the resulting code allows larger problem \ninstances to be solved in the same amount of time. Evolutionary compilation systems have also been proposed \nfor compilation systems targeting embedded systems where meet\u00ading strict constraints on execution time, \ncode size, and power con\u00adsumption is paramount. Here long compilation times are accept\u00adable because in \nthe .nal stages of development an application is compiled and embedded in a product where millions of \nunits may be shipped. For embedded systems, the problem is further exacer\u00adbated because the software \ndevelopment environment is often dif\u00adferent from the target environment. Obtaining performance mea\u00adsures \non cross-platform development environments often requires simulation which can be orders of magnitude \nslower than native execution. Even when it is possible to use the target machine to gather performance \ndata directly, the embedded processor may be signi.cantly slower (slower clock rate, less memory, etc.) \nthan available general-purpose processors. We have found that search\u00ading for an effective optimization \nsequence can easily require hours or days even when using direct execution on a general-purpose processor. \nFor example, using a conventional genetic algorithm to search for effective optimization sequences for \nthe jpeg applica\u00adtion on an Ultra SPARC III processor required over 20 hours to complete. Thus, .nding \neffective sequences to tune an embedded application may result in an intolerably long search time. In \nthis paper we describe approaches for achieving faster searches for effective optimization sequences \nusing a genetic algo\u00adrithm. We performed our experiments using the VISTA (VPO Interactive System for \nTuning Applications) framework [20]. One feature of VISTA is that it can automatically obtain performance \nfeedback information which can be presented to the user and can be used to make phase ordering decisions \n[11]. We use this per\u00adformance information to drive the genetic algorithm searches for effective optimization \nsequences. The remainder of the paper is structured as follows. First, we review other aggressive compilation \ntechniques that have been used to tune applications. Second, we give an overview of the VISTA framework \nin which our experiments are performed. Third, we describe methods for reducing the overhead of the searches \nfor effective sequences. Fourth, we discuss techniques for .nding effective sequences in fewer generations. \nFifth, we show results that indicate the effectiveness of using our techniques to perform faster searches \nfor optimization sequences. Finally, we outline future work and present the conclusions of the paper. \n 2. RELATED WORK Prior work has used aggressive compilation techniques to improve performance. Superoptimizers \nhave been developed that use an exhaustive search for instruction selection [12] or to eliminate branches \n[7]. Selecting the best combination of optimizations by turning on or off optimization .ags, as opposed \nto varying the order of optimizations, has also been investigated [4]. Some systems perform transformations \nand use performance feedback information to tune applications. Iterative techniques using performance \nfeedback information after each compilation have been applied to determine good optimization parameters \n(e.g., blocking sizes) for speci.c programs or library routines [10, 18]. Another technique uses compile-time \nperformance estima\u00adtion [16]. All of these systems are limited in the set of optimiza\u00adtions they apply. \nSpeci.cations of code-improving transformations have been automatically analyzed to determine if one \ntype of transformation can enable or disable another [19]. This information can provide insight into \nhow to specify an effective optimization phase order\u00ading for a conventional optimizing compiler. A number \nof systems have been developed that use evolu\u00adtionary algorithms to improve compiler optimizations. A \nneural network has been used to tune static branch predictions [3]. Genetic algorithms have been used \nto better parallelize loop nests [13]. Another system used genetic algorithms to derive improved compiler \nheuristics for hyperblock formation, register allocation, and data prefetching [15]. A low-level compilation \nsystem devel\u00adoped at Rice University uses a genetic algorithm to reduce code size by .nding ef.cient \noptimization phase sequences [5, 6]. The Rice system uses a similar genetic algorithm as in VISTA for \n.nd\u00ading phase sequences. However, the Rice system is batch oriented instead of interactive and applies \nthe same optimization phase order for all of the functions within a .le. Some aspects of the approaches \ndescribed in our paper may be useful for obtaining faster searches in all of these systems.  3. THE \nVISTA FRAMEWORK This section provides a brief overview of the framework used for the experiments reported \nin this paper. A more detailed descrip\u00adtion of VISTA s architecture can be found in prior publications \n[20, 11]. Figure 1 illustrates the .ow of information in VISTA, which consists of a compiler and a viewer. \nThe programmer ini\u00adtially indicates a .le to be compiled and then speci.es requests through the viewer, \nwhich include sequences of optimization phases, manually speci.ed transformations, and queries. The compiler \nperforms the speci.ed actions and sends program repre\u00adsentation information back to the viewer. Each \ntime an optimiza\u00adtion sequence is selected for the function being tuned, the com\u00adpiler instruments the \ncode, produces assembly code, links and executes the program, and gets performance measures from the \nexecution. When the user chooses to terminate the session, VISTA writes the sequence of transformations \nto a .le so they can be reapplied at a later time, enabling future updates.  Figure 1: Interactive Code \nImprovement Process The compiler used in VISTA is based on VPO (Very Portable Optimizer), which is a \ncompiler back end that performs all of its optimizations on a single low-level representation called \nRTLs (register transfer lists) [1, 2]. Because VPO uses a single repre\u00adsentation, it can apply most analyses \nand optimization phases repeatedly and in an arbitrary order. This feature facilitates .nd\u00ading more effective \nsequences of optimization phases. Figure 2 shows a snapshot of the viewer with the history of a sequence \nof optimization phases displayed. Note that not only is the number of transformations associated with \neach optimization phase displayed, but also the improvements in instructions executed and code size are \nshown. This information allows a user to quickly gauge the progress that has been made in improving the \nfunction. The frequency of each basic block relative to the func\u00adtion is also shown in each block header \nline, which allows a user to identify the critical regions of a function. VISTA allows a user to specify \na set of distinct optimization phases and have the compiler attempt to .nd the best sequence for applying \nthese phases. Figure 3 shows the different options that we provide the user to control the search. The \nuser speci.es the sequence length, which is the total number of phases applied in each sequence. Our \nexperiments used the biased sampling search, which applies a genetic algorithm in an attempt to .nd the \nmost effective sequence within a limited amount of time since in many cases the search space is too large \nto evaluate all possible sequences [9]. A population is the set of solutions (sequences)  Figure 2: \nMain Window of VISTA Showing History of Optimization Phases that are under consideration. The number \nof generations indicates how many sets of populations are to be evaluated. The population size and the \nnumber of generations limits the total number of sequences evaluated. VISTA also allows the user to choose \ndynamic and static weight factors, where the relative improvement of each is used to determine the overall \n.tness. Figure 3: Selecting Options to Search for Possible Sequences Performing these searches is time \nconsuming, typically requiring tens of minutes for a single function, and hours or days for an entire \napplication even when using direct execution. Thus, VISTA provides a window showing the current search \nstatus. Fig\u00adure 4 shows a snapshot of the status of the search selected in Fig\u00adure 3. The percentage \nof sequences completed, the best sequence, and its effect on performance are given. The user can terminate \nthe search at any point and accept the best sequence found so far.  4. REDUCING THE SEARCH OVERHEAD \nPerforming a search for an effective optimization phase sequence can be quite expensive, perhaps requiring \nhours or days for an entire application even when using direct execution. One obvious bene.t for speeding \nup these searches is that the technique is more likely to be used. Another bene.t is that the search \ncan be made more aggressive, such as increasing the number of generations, in an attempt to produce a \nbetter tuned application. Figure 4: Window Showing the Search Status VISTA performs the following tasks \nto obtain dynamic per\u00adformance measurements for a single sequence. (1) The compiler applies the optimization \nphases in the order speci.ed by the sequence. (2) The generated code for the function is instrumented \nif required to obtain performance measurements and the assembly code for that function and the remaining \nassembly code for the functions in the current source .le are written to a .le. (3) The newly generated \nassembly .le is assembled. (4) The object .les comprising the entire program are linked together into \nan executable by a command supplied in a con.guration .le. (5) The program is executed using a command \nin a con.guration .le, which may involve direct execution or simulation. As a side effect of the execution, \nperformance measurements are produced. (6) The output of the execution is compared to the desired output \nto provide assurance that the new sequence did not cause the gen\u00aderated code to become invalid. Tasks \n2-6 often dominate the search time, which is probably due to these tasks requiring I/O and task 1 being \nperformed in memory. The following subsections describe methods to reduce the search overhead by inferring \nthe outcome of a sequence. Figure 5 illustrates the order in which the different methods are attempted. \nThe methods are ordered according to cost. Each method handles a superset of the sequences handled by \nthe methods applied before it, but the later methods are more expensive. Figure 5: Methods for Reducing \nSearch Overhead 4.1 Finding Redundant Attempted Sequences Sometimes the same optimization phase sequence \nis reattempted during the search. Consider Figure 6, where each optimization phase in a sequence is represented \nby a letter. The same sequence can be reattempted due to mutation not occurring on any of the phases \nin the sequence (e.g. sequence i remaining the same in Fig\u00adure 6). Likewise, a crossover operation or \nmutation changing some individual phases can produce a previously attempted sequence (e.g. sequence k \nmutates to be the same as sequence j before mutation in Figure 6). A hash table of attempted sequences \nalong with the performance result for each sequence is main\u00adtained. If a sequence is found to be previously \nattempted, then the evaluation of the sequence is not performed and the previous result is used. This \ntechnique of using a hash table to capture pre\u00adviously attempted solutions has been previously used to \nreduce search time [5, 15, 11]. before mutation after mutation ... ... seq i: d a e d c f seq i: d \na e d c f ... ... seq j: f a c b c d seq j: f a c a c d ... ... seq k: f e c b b d seq k: f a c \nb c d ... ... Figure 6: Example of Redundant Attempted Sequences  4.2 Finding Redundant Active Sequences \nA transformation is a sequence of changes to the program repre\u00adsentation, where the semantic behavior \nis preserved. A phase is a sequence of transformations caused by a single type of optimiza\u00adtion. Borrowing \nfrom biological terminology, an active optimiza\u00adtion phase (gene) is one that applies transformations, \nwhile a dor\u00admant optimization phase (gene) is one that has no effect. An opti\u00admization phase is dormant \nwhen the enabling conditions for the optimization to be applied are not satis.ed. As one would expect, \nonly a subset of the attempted phases in a sequence will typically be active. It is common that a dormant \nphase may be mutated to another dormant phase, but it would not affect the compilation. Figure 7 illustrates \nhow different attempted sequences can map to the same active sequence, where the bold boxes represent \nactive phases and the nonbold boxes represent dormant phases. A sec\u00adond hash table is used to record \nsequences where only the active phases are represented. attempted: seq i: seq j:  active: seq i: seq \nj: d e f Figure 7: Example of a Redundant Active Sequence  4.3 Detecting Identical Code Sometimes identical \ncode can be generated from different active sequences. Often different optimization phases can be applied \nand can have the same effect. Consider the two different ways that the pair of instructions in Figure \n8 can be merged together. Instruction selection symbolically merges the instructions and checks to see \nif the resulting instruction is legal. The same effect in this case can be produced by constant propagation \nfollowed by dead assignment elimination. We also found that performing some optimization phases in a \ndifferent order will have no effect on the .nal code that is generated. For instance, consider apply\u00ading \nbranch chaining before and after register allocation. Both branch chaining and register allocation will \nneither inhibit nor enable the other phase. original code segment r[2]=1; r[3]=r[4]+r[2]; original code \nsegment r[2]=1; r[3]=r[4]+r[2]; after instruction selection r[3]=r[4]+1; after constant propagation r[2]=1; \nr[3]=r[4]+1; after dead assignment elimination r[3]=r[4]+1; Figure 8: Different Optimizations Having \nthe Same Effect VISTA has to ef.ciently detect when different active sequences generate identical code \nto be able to reduce the search overhead. A search may result in thousands of unique function instances, \nwhich may be too large to store in memory and very expensive to access on disk. The key realization in \naddressing this issue was that while we need to detect when function instances are identical, we can \ntolerate occasionally treating different instances as being identical since the sequences within a population \nare sorted and the best sequence found by the genetic algorithm must be completely evaluated. Thus, we \ncalculate a CRC (cyclic redun\u00addancy code) checksum on the bytes of the RTLs and keep a hash table of \nthese checksums. CRCs are commonly used to check the validity of data transmitted over a network and \nhave an advantage over conventional checksums in that the order of the bytes of data does affect the \nresult [14]. If the checksum has been generated for a previous function instance, then we use the performance \nresults of that instance. We have veri.ed it is rare that we generate the same checksum for different \nfunction instances and that the best .tness value found is never affected in our experiments. 4.4 Detecting \nEquivalent Code Sometimes the code generated by different optimization sequences are equivalent, in regard \nto speed and size, but not iden\u00adtical. Consider two function instances that have the same sequence of \ninstruction types, but use different registers. This can occur since different optimization phases compete \nfor registers. For instance, consider the source code in Figure 9(a). Figures 9(b) and 9(c) show two \npossible translations given two different order\u00adings of optimization phases that consume registers. To \ndetect this situation, we identify the live ranges of all of the registers in the function and map each \nlive range to a distinct pseudo register. Equivalent function instances become identical after mapping, \nwhich is illustrated for the example in Figure 9(d). We compute the CRC checksum for the mapped function \ninstance and check in a separate hash table of CRC checksums to see if the mapped function had been previously \ngenerated. (a) Source Code sum += a[i]; for (i = 0; i < 1000; i++) sum = 0; L3 r[9]=4000+r[12]; r[12]=r[12]+LO[a]; \nr[10]=r[10]+r[8]; (b) Register Allocation before Code Motion r[10]=0; r[12]=HI[a]; r[1]=r[12]; r[8]=M[r[1]]; \nr[1]=r[1]+4; IC=r[1]?r[9]; PC=IC<0,L3; L3 Register Allocation (c) Code Motion before PC=IC<0,L3; IC=r[1]?r[9]; \nr[1]=r[1]+4; r[11]=r[11]+r[8]; r[8]=M[r[1]]; r[9]=4000+r[10]; r[1]=r[10]; r[10]=r[10]+LO[a]; r[10]=HI[a]; \nr[11]=0; r[32]=r[32]+r[36]; r[34]=r[34]+4; IC=r[34]?r[35]; PC=IC<0,L3; L3 r[36]=M[r[34]]; (d) After Mapping \nRegisters r[32]=0; r[33]=HI[a]; r[33]=r[33]+LO[a]; r[34]=r[33]; r[35]=4000+r[33]; Figure 9: Different \nFunctions with Equivalent Code On most machines there is a uniform access time for each register in the \nregister .le. Likewise, most statically scheduled processors do not generate stalls due to anti (write \nafter read) and output (write after write) dependences. However, these depen\u00addences could inhibit future \noptimizations. Thus, comparing regis\u00adter mapped functions to avoid executions in the search should only \nbe performed after all remaining optimizations (e.g. .lling delay slots) have been applied. Given that \nthese assumptions are true, if we .nd that the current mapped function is equivalent to a previ\u00adous mapped \ninstance of the function, then we can assume the two are equivalent and will produce the same result \nafter execution.  5. PRODUCING SIMILAR RESULTS IN FEWER GENERATIONS Another approach that can be used \nto reduce the search time for .nding effective optimization sequences is to produce the same results \nin fewer generations of the genetic algorithm. If this approach is feasible, then users can either specify \nfewer genera\u00adtions to be performed in their searches or they can stop the search sooner once the desired \nresults have been achieved. The follow\u00ading subsections describe the different techniques that we use \nto obtain effective sequences of optimization phases in fewer genera\u00adtions. All of these techniques identify \nphases that are likely to be active or dormant at a given point in the compilation process. 5.1 Using \nthe Batch Sequence The traditional or batch version of our compiler always attempts the same order of \noptimization phases for each function. We obtain the sequence of active phases (those phases that were \nable to apply one or more transformations) from the batch compilation of the function. We have used the \nlength of the active batch sequence to establish the length of the sequences attempted by the genetic \nalgorithm in previous experiments [11]. We propose to use the active batch sequence for the function \nas one of the sequences in the initial population. The premise is that if we initialize a sequence in \nthe population with optimization phases that are likely to be active, then this may allow the genetic \nalgorithm to converge faster on the best sequence it can .nd. This approach is similar to including in \nthe initial population the com\u00adpiler writer s manually speci.ed priority function when attempt\u00ading to \ntune a compiler heuristic [15]. 5.2 Prohibiting Speci.c Phases While many different optimization phases \ncan be speci.ed as can\u00addidate phases for the genetic algorithm, sometimes speci.c phases can never be \nactive for a given function. If the genetic algorithm only attempts phases that have an opportunity to \nbe active, then the algorithm may converge on the best sequence it can .nd in fewer attempts. There are \nseveral situations when speci.c opti\u00admizations should not be attempted. Loop optimization phases cannot \nbe active for a function that does not contain any loops. Register allocation in VPO cannot be active \nfor a function that does not contain any local variables or parameters. Branch opti\u00admizations and unreachable \ncode elimination cannot be active for a function that contains a single basic block. Detecting that a \nspe\u00adci.c set of optimization phases can never be active for a given function requires simple analysis \nthat only needs to be performed once at the beginning of the genetic algorithm. 5.3 Prohibiting Prior \nDormant Phases When compiling a function, we .nd certain optimization phases will be dormant given that \na speci.c pre.x of active phases has been performed. Given that the same pre.x of phases is attempted \nagain, there is no bene.t from attempting the same dormant phase in the same situation since it will \nremain dormant. To avoid repeating these dormant phases, we represent the active phases as nodes in a \ntree, where each child corresponds to the next phase in an active sequence. We also store at each node \nthe set of phases that were found to be dormant for that pre.x of active phases. Figure 10 shows an example \ntree where the bold portions repre\u00adsent active pre.xes and the nonbold boxes represent dormant phases \ngiven that pre.x. For instance, a and f are dormant phases for the pre.x bac. To prohibit applying a \nprior dormant phase, we force a phase to change during mutation until we .nd a phase that has either \nbeen active with the speci.ed pre.x or has not yet been attempted.  5.4 Prohibiting Unenabled Phases \nCertain optimization phases when performed cannot become active again until enabled. For instance, register \nallocation replaces references to variables in live ranges with registers. A live range is assigned to \na register when a register is available at that point in the coloring process. After the compiler applies \nreg\u00adister allocation, this optimization phase will not have an opportu\u00adnity to be active again until \nthe register pressure has changed. Unreachable code elimination and a variety of branch optimiza\u00adtions \nwill not affect the register pressure and thus will not enable register allocation. Figure 11 illustrates \nthat a speci.c phase, the nonbold box of the sequence on the right, will at times be unen\u00adabled and cannot \nbe active. Again the premise is that if the genetic algorithm concentrates on the phases that have an \nopportu\u00adnity to be active, then it will be able to apply more active phases in a sequence and converge \nto the best sequence it can .nd in fewer attempts. Note that determining which optimization phases can \nenable another phase requires careful consideration by the com\u00adpiler writer. c enables a b and d do not \nenable a ... a b c a ... ... a b d a ... Figure 11: Enabling Previously Applied Phases We implemented \nthis technique by forcing a phase to mutate if the same phase has already been performed and there are \nno intervening phases that can enable it. We realized that a speci.c phase can become unenabled after \nan attempted phase is found to be active or dormant. We .rst follow the tree of active pre.xes, which \nwas described in the previous subsection, to determine which phases are currently enabled. For example, \nconsider again Figure 10. Assume that b can be enabled by a, but cannot be enabled by c. Giv en the pre.x \nbac, we know that b cannot be active at this point since b was dormant after the pre.x ba and c cannot \nreenable it. After reaching a leaf of the tree we track which phases cannot be enabled by just examining \nthe subse\u00adquently attempted phases.  6. EXPERIMENTS This section describes the results of a set of \nexperiments to illus\u00adtrate the effectiveness of the previously described techniques for obtaining fast \nsearches for effective optimization phase sequences. We .rst perform experiments on a Ultra SPARC III \nprocessor so that the results could be obtained in a reasonable time. After ensuring ourselves that the \ntechniques were sound, we use these techniques when obtaining results for the Intel StrongARM SA-110 \nprocessor, which has a clock rate that is more than 5 times slower than the Ultra SPARC III. We used \na subset of the mibench benchmarks, which are C applications targeting speci.c areas of the embedded \nmarket [8]. We used one benchmark from each of the six categories of appli\u00adcations. When executing each \nof the benchmarks, we used the sample input data that was provided with the benchmark. Table 1 contains \ndescriptions of these programs. Category Program Description auto/industrial bitcount test bit manipulation \nabilities network dijkstra calculates shortest path between nodes using Dijkstra s algorithm telecomm \nfft performs fast fourier transform consumer jpeg image compression &#38; decompression security sha \nsecure hash algorithm of.ce stringsearch searches for words in phrases Table 1: MiBench Benchmarks Used \nin the Experiments Table 2 shows each of the candidate code-improving phases that we used in the experiments \nwhen compiling each function. In addition, register assignment, which is a compulsory phase that assigns \npseudo registers to hardware registers, has to be per\u00adformed. VISTA implicitly performs register assignment \nbefore the .rst code-improving phase in a sequence that requires it. After applying the last code-improving \nphase in a sequence, we perform another compulsory phase which inserts instructions at the entry and \nexit of the function to manage the activation record on the run-time stack. Finally, we also perform \nadditional code\u00adimproving phases afterwards, such as .lling delay slots. Our genetic algorithm search \nfor obtaining the baseline mea\u00adsurements was accomplished in the following manner. Unlike past studies \nusing genetic algorithms to generate better code [13, 5, 15], we perform a search on each function (a \ntotal of 106 func\u00adtions in our test suite), which requires longer compilations but results in better \noverall improvements [11]. In fact, most of the techniques we are evaluating would be much less effective \nif we searched for a single sequence to be applied on an entire applica\u00adtion. We set the sequence (chromosome) \nlength to be 1.25 times the number of active phases that were applied for the function by the batch compiler. \nWe felt this length was a reasonable limit and gives us an opportunity to apply more active phases than \nwhat the batch compiler could accomplish, which is much less than the number of phases attempted during \nthe batch compilation. The sequence lengths used in these experiments varied between 4 and 48 with an \naverage of 14.15. We set the population size (.xed number of sequences or chromosomes) to twenty and \neach of these initial sequences is randomly initialized with candidate opti\u00admization phases. We performed \n100 generations when searching for the best sequence for each function. We sort the sequences in Optimization \nPhase Description branch chaining Replaces a branch or jump target with the target of the last jump in \na jump chain. common subexpression elimination Eliminates fully redundant calculations, which also includes \nconstant and copy propagation. remove unreachable code Removes basic blocks that cannot be reached from \nthe entry block of the function. remove useless blocks Removes empty blocks from the control-.ow graph. \ndead assignment elimination Removes assignments when the assigned value is never used. block reordering \nRemoves a jump by reordering basic blocks when the target of the jump has only a single predecessor. \nminimize loop jumps Removes a jump associated with a loop by duplicating a portion of the loop. register \nallocation Replaces references to a variable within a speci.c live range with a register. loop transformations \nPerforms loop-invariant code motion, recurrence elimination, loop strength reduction, and induction variable \nelimination on each loop ordered by loop nesting level. Each of these transformations can also be individually \nselected by the user. merge basic blocks Merges two consecutive basic blocks a and b when a is only followed \nby b and b is only preceded by a. evaluation order determination Reorders RTLs in an attempt to use fewer \nregisters. strength reduction Replaces an expensive instruction with one or more cheaper ones. reverse \njumps Eliminates an unconditional jump by reversing a conditional branch when it branches over the jump. \ninstruction selection Combine instructions together and perform constant folding when the combined effect \nis a legal instruction. remove useless jumps Removes jumps and branches whose target is the following \nblock. Table 2: Candidate Optimization Phases in the Genetic Algorithm Experiments the population by \na .tness value calculated using 50% weight on speed and 50% weight on code size. The speed factor we \nused was the number of instructions executed since this was a measure that could be consistently obtained, \nit has been used in similar studies [5, 11], and allowed us to obtain baseline measurements within a \nreasonable period of time. We could obtain a more accu\u00adrate measure of speed by using a cycle-accurate \nsimulator. How\u00adever, the main point of our experiments was to evaluate the effec\u00adtiveness of techniques \nfor obtaining faster searches, which can be applied with any type of .tness evaluation criteria. At each \ngener\u00adation (time step) we remove the worst sequence and three others from the lower (poorer performing) \nhalf of the population chosen at random. Each of the removed sequences are replaced by ran\u00addomly selecting \na pair of the remaining sequences from the upper half of the population and performing a crossover (mating) \nopera\u00adtion to create a pair of new sequences. The crossover operation combines the lower half of one \nsequence with the upper half of the other sequence and vice versa to create two new sequences. Fifteen \nsequences are then changed (mutated) by considering each optimization phase (gene) in the sequence. Mutation \nof each phase in a sequence occurs with a probability of 10% and 5% for the lower and upper halves of \nthe population, respectively. When an optimization phase is mutated, it is randomly replaced with another \nphase. The four sequences subjected to crossover and the best performing sequence are not mutated. Finally, \nif we .nd identical sequences in the same population, then we replace the redundant sequences with ones \nthat are randomly generated. Figures 12, 13, and 14 show the percentage improvement that we obtained \nfor the SPARC when optimizing for speed only, size only, and 50% for each factor, respectively. Performance \nresults for the ARM, a widely used embedded processor, are presented later in this section. The baseline \nmeasures were obtained using the batch VPO compiler, which iteratively applies optimization phases until \nno more improvements can be obtained. This base\u00adline is much more aggressive than always using a .xed \nlength sequence of phases [11]. The average bene.ts shown in the .gure are slightly improved from previously \npublished results [11] since the searches now include additional optimization phases that were not previously \nexploited by the genetic algorithm. Note that the contribution of our paper is that the search for these \nbene.ts is more ef.cient, rather than the actual bene.ts obtained.  Figure 15 shows the average number \nof sequences whose executions were avoided for each benchmark using the methods described in Section \n4. These results do not include the functions in the benchmarks that were not executed when using the \nsample input data since these functions were evaluated on code size only and did not require execution \nof the application. Consider for now only the top bar for each benchmark, which represents the results \nwithout applying any of the techniques in Section 5. As men\u00adtioned previously, each method in Section \n4 is able to .nd a super\u00adset of the sequences handled by methods applied before it. On average 41.3% \nof the sequences were detected as redundantly attempted, 27.0% were caught as redundant active sequences, \n14.9% were discovered to produce identical code as generated by a previous sequence, and 1.0% were found \nto produce unique, but equivalent code. Thus, over 84% of the executions were avoided. We found that \nwe could avoid a higher percentage of the execu\u00adtions when tuning smaller functions since we used shorter \nsequence lengths that were established by the batch compilation due to fewer optimization phases being \nactive. A shorter sequence length results in more redundant sequences. For instance, the likelihood of \nmutation is less when there are fewer phases in a sequence to mutate. Also, identical or equivalent code \nis more likely when fewer phases could be applied. Figure 16 shows the relative search time required \nwhen applying the methods described in Section 4 to not applying these methods. The average search time \nrequired 0.35 of the time when no executions were avoided and 0.51 of the time when redundant attempted \nsequences were avoided. The average time required to evaluate each of the six benchmarks improved from \n5.57 hours to 2.27 hours. The reduction appears to be affected not only by the percentage of the avoided \nexecutions, but also by the size of the functions. The larger functions tended to have fewer avoided \nexecutions and also had longer compilations. While the average search time was signi.cantly reduced for \nthese experiments using direct execution on a SPARC processor, the savings would only increase when using \nsimulation since the executions of the appli\u00adcation would comprise a larger portion of the search time. \n Figures 17-21 show the average number of generations that were evaluated for each of the functions \nbefore .nding the best .t\u00adness value in the search. The baseline result is without using any of the techniques \ndescribed in Section 5. The other results indi\u00adcate the generation when the .rst sequence was found whose \nper\u00adformance equaled the best sequence found in the baseline search. To ensure a fair comparison, we \ndid not include the results for the functions when the best .tness value found was not identical to the \nbest .tness value in the baseline, which occurred on about 18% of the functions. This caused the baseline \nresults to vary slightly since the functions with different .tness values were not always the same when \napplying each of the techniques. About 11.3% of the functions had improved .tness values and about 6.6% \nof the functions had worse .tness values when all of the techniques were applied. On average the best \n.tness values improved by 0.24% (by 1.33% for only the differing functions). The maximum number of generations \nbefore .nding the best .t\u00adness value for any function was 91 out of a possible 100 when not applying \nany of the four techniques. The maximum was 56 when all four techniques were used. The techniques occasionally \ncaused the best .tness value to be found later, which we believe is due to the inherent randomness of \nusing a genetic algorithm. However, all of the techniques were bene.cial on average. Figure 17 shows \nthe effect of using the batch sequence in the initial population, which in general was quite bene.cial. \nWe found that this technique worked well for the smaller functions in the applications since it was often \nthe case that the batch compiler produced code that was as good as the code generated by the best sequence \nfound in the search. However, the smaller functions tended to converge on the best sequence in the search \nin fewer generations anyway since the sequence lengths were typically shorter. In fact, it is likely \nthat performing a search for an effec\u00adtive optimization sequence is in general less bene.cial for smaller \nfunctions since there is less interplay between phases. Using the batch sequence for the larger functions \noften resulted in .nding the best sequence in fewer generations even though the batch compiler typically \ndid not produce code that was as good as pro\u00adduced by the best sequence found in the baseline results. \nThus, simply initializing the population with one sequence containing phases that are likely to be active \nis quite bene.cial. The effect of prohibiting speci.c phases throughout the search was less bene.cial, \nas shown in Figure 18. Speci.c phases can only be safely prohibited when the function is relatively sim\u00adple \nand a speci.c condition (such as no loops, no variables, or no unconditional jumps) can be detected. \nSeveral applications, such as stringsearch, had no or very few functions that met these crite\u00adria. The \nsimpler functions also tended to converge faster to the best sequence found in the search since the sequence \nlength estab\u00adlished by the length of the batch compilation was typically shorter. Likewise, the simpler \nfunctions also have little impact on the size of the entire application and have little impact on speed \nwhen they are not frequently executed. Best Fitness Value When Prohibiting Speci.c Phases In contrast, \nprohibiting prior dormant and unenabled phases, which are depicted in Figures 19 and 20, had a more signi.cant \nimpact since these techniques could be applied to all functions. Without using these two techniques, \nit was often the case that many phases were reattempted when there was no opportunity for them to be \nactive. Applying all the techniques produced the best overall results, as shown in Figure 21. In fact, \nonly about 32% of the generations on average (from 25.74 to 8.24) were required to .nd the best sequence \nin the search as compared to the baseline. As expected, applying all of the techniques did not result \nin the sum of the ben\u00ade.ts of the individual techniques since some of the phases that were prohibited \nwould be caught by multiple techniques.  Consider again Figure 15, which depicts the number of avoided \nexecutions. The bottom bar for each benchmark shows the number of executions that are avoided when all \nof the tech\u00adniques described in Section 5 are applied. One can see that while the number of redundantly \nattempted sequences decrease, the number of sequences caught by the three other techniques increase. \nThe remaining redundantly attempted sequences were the sequences created by the crossover operation and \nthe best sequence in the population, which were not subject to mutation, and the redundant sequences \nwith only active phases. The average number of avoided executions decreases by about 10%, which means \na greater number of functions with unique code were generated. However, the decrease in avoided executions \nis much less than the average decrease in generations required to reach the best sequence found in the \nsearch, as shown in Figure 21. Figure 22 shows the relative time for .nding the best .tness value when \nall of the techniques in Section 5 were applied. The actual times are shown in minutes since .nding the \nbest sequence is accomplished in a fraction of the total generations performed in the search. Note the \nbaseline for .nding the best .tness value includes all of the methods described in Section 4 to avoid \nunnec\u00adessary executions. The best .tness value was found in 53.0% of the time on average as compared \nto the baseline.  After ensuring that the techniques we developed to improve the search time for effective \nsequences were sound, we obtained results on the Intel StrongARM SA-110 processor. Figures 23, 24, and \n25 show the percentage improvement when optimizing for speed only, size only, and 50% for each factor, \nrespectively. The average time required to obtain results for each of the benchmarks when optimizing \nfor both speed and size on the ARM required 12.67 hours. Using the average ratio shown in Figure 16, \nwe esti\u00admate it would have taken over 36.19 hours without applying the techniques in Section 4.  7. \nIMPLEMENTATION ISSUES During the process of this investigation, we encountered several implementation \nissues that made this work challenging. First, producing code that always generates the correct output \nfor differ\u00adent optimization phase sequences is dif.cult. Even implementing a conventional compiler that \nalways generates code that produces correct output when applying one prede.ned sequence of opti\u00admization \nphases is not an easy task. In contrast, generating code that always correctly executes for thousands \nof different optimiza\u00adtion phase sequences is a severe stress test. Ensuring that all sequences in the \nexperiments produced valid code required track\u00ading down many errors that had not yet been discovered \nin the VISTA system. Second, the techniques presented in Sections 5.2 and 5.4 required analysis and judgement \nby the compiler writer to determine when optimization phases will be enabled. We inserted sanity checks \nwhen running experiments without using these methods to ensure that our assertions concerning the enabling \nof optimization phases were accurate. We found several cases where our reasoning was faulty after inspecting \nthe situations uncovered by these sanity checks and we were able to correct our enabling assertions. \nThird, we sometimes found that dormant optimization phases did have unexpected side effects by changing \nthe analysis information, which could enable or disable a subsequent optimization phase. These side effects \ncan affect the results of the methods described in Sections 4.2, 5.3, and 5.4. We also inserted sanity \nchecks to ensure that different dormant phases did not cause different effects on subsequent phases. \nWe detected when these situations occurred, properly set the information about what analysis is required \nand invalidated by each optimization phase, and now rarely encounter these problems.   8. FUTURE WORK \nThere is much future research that can be accomplished on pro\u00adviding fast searches for effective optimization \nsequences. We have shown that detecting when a particular optimization phase will be dormant can result \nin fewer generations to converge on the best sequence in the search. We believe it is possible to estimate \nthe likelihood that a particular optimization phase will be active given the active phases that precede \nit by empirically collecting this information. This information could be exploited by adjust\u00ading the \nmutation operation to more likely mutate to phases that have a better chance of being active with the \ngoal of converging to a better .tness value in fewer generations. Another area of future work is to vary \nthe characteristics of the search. It would be interesting to see the effect on a search as one changes \naspects of genetic algorithm, such as the sequence length, population size, number of generations, etc. \nWe may .nd that certain search characteristics may be better for one class of functions, while other \ncharacteristics may be better for other func\u00adtions. In addition, it would be interesting to perform searches \ninvolving more compiler optimizations and benchmarks. Finally, the use of a cluster of processors can \nreduce the search time. Certainly different sequences within a population can be evaluated in parallel \n[15]. Likewise, functions within the same application can be evaluated independently. Even with the use \nof a cluster, the techniques we have presented in our paper would still be useful since they will further \nenhance the search time. In addition, not every developer has access to a cluster. 9. CONCLUSIONS There \nare several contributions that we have presented in this paper. First, we have shown there are effective \nmethods to reduce the search overhead for .nding effective optimization phase sequences by avoiding expensive \nexecutions or simulations. Detecting when a phase was active or dormant by instrumenting the compiler \nwas very useful since many sequences can be detected as redundant by memoizing the results of active \nphase sequences. We also discovered that the same code is often gener\u00adated by different sequences. We \ndemonstrated that using ef.cient mechanisms, such as a CRC checksum, to check for identical or equivalent \nfunctions can also signi.cantly reduce the number of required executions of an application. Second, we \nhave shown that on average the number of generations required to .nd the best sequence can be reduced \nby over two thirds. One simple, but effective technique is to insert the active sequence of phases from \nthe batch compilation as one of the sequences in the initial popu\u00adlation. We also found that we could \noften use analysis and empiri\u00adcal data to determine when phases could not be active. These techniques \nresult in faster convergence to more effective sequences, which can allow equally effective searches \nto be per\u00adformed with fewer generations of the genetic algorithm. An environment to tune the sequence \nof optimization phases for each function in an embedded application can be very bene.\u00adcial. However, \nthe overhead of performing searches for effective sequences using a genetic algorithm can be quite signi.cant \nand this problem is exacerbated when performance measurements for an application are obtained by simulation \nor on a slower embed\u00added processor. Many developers are willing to wait for tasks to run overnight to \nimprove a product, but are unwilling to wait longer. We have shown that the search overhead can be signi.\u00adcantly \nreduced, perhaps to a tolerable level, by using methods to avoid redundant executions and techniques \nto converge to the best sequence it can .nd in fewer generations. ACKNOWLEDGEMENTS Clark Coleman and \nthe anonymous reviewers provided helpful suggestions that improved the quality of the paper. This research \nwas supported in part by National Science Foundation grants EIA-0072043, ACI-0203956, CCR-0208892, ACI-0305144, \nand CCR-0312493. 10. REFERENCES [1] M. E. Benitez and J. W. Davidson, A Portable Global Opti\u00admizer and \nLinker, Proceedings of the SIGPLAN 88 Sym\u00adposium on Programming Language Design and Implemen\u00adtation, \npp. 329-338 (June 1988). [2] M. E. Benitez and J. W. Davidson, The Advantages of Machine-Dependent Global \nOptimization, Proceedings of the Conference on Programming Languages and Systems Architectures, pp. 105-124 \n(March 1994). [3] B. Calder, D. Grunwald, and D. Lindsay, Corpus-based Static Branch Prediction, Proceedings \nof the SIGPLAN 95 Conference on Programming Language Design and Imple\u00admentation, pp. 79-92 (June 1995). \n[4] K. Chow and Y. Wu, Feedback-Directed Selection and Characterization of Compiler Optimizations, Workshop \non Feedback-Directed Optimization, (November 1999). [5] K. Cooper, P. Schielke, and D. Subramanian, Optimizing \nfor Reduced Code Space Using Genetic Algorithms, ACM SIGPLAN Workshop on Languages, Compilers, and Tools \nfor Embedded Systems, pp. 1-9 (May 1999). [6] K. Cooper, D. Subramanian, and L. Torczon, Adaptive Optimizing \nCompilers for the 21st Century, Journal of Supercomputing 23(1) pp. 7-22 (). [7] T. Granlund and R. Kenner, \nEliminating Branches using a Superoptimizer and the GNU C Compiler, Proceedings of the SIGPLAN 92 Conference \non Programming Language Design and Implementation, pp. 341-352 (June 1992). [8] M. Guthaus, J. Ringenberg, \nD. Ernst, T. Austin, T. Mudge, and R. Brown, MiBench: A Free, Commercially Represen\u00adtative Embedded Benchmark \nSuite, IEEE Workshop on Workload Characterization, (December 2001). [9] J. Holland, Adaptation in Natural \nand Arti.cial Systems, Addison-Wesley (1989). [10] T. Kisuki, P. Knijnenburg, and M. O Boyle, Combined \nSelection of Tile Sizes and Unroll Factors Using Iterative Compilation, Proceedings of the 2000 International \nCon\u00adference on Parallel Architectures and Compilation Tech\u00adniques, pp. 237-248 (October 2000). [11] P. \nKulkarni, W. Zhao, H. Moon, K. Cho, D. Whalley, J. Davidson, M. Bailey, Y. Paek, and K. Gallivan, Finding \nEffective Optimization Phase Sequences, ACM SIGPLAN Conference on Languages, Compilers, and Tools for \nEmbedded Systems, pp. 12-23 (June 2003). [12] H. Massalin, Superoptimizer - A Look at the Smallest Pro\u00adgram, \nProceedings of the 2nd International Conference on Architectural Support for Programming Languages and \nOperating Systems, pp. 122-126 (October, 1987). [13] A. Nisbet, Genetic Algorithm Optimized Parallelization, \nWorkshop on Pro.le and Feedback Directed Compilation, (1998). [14] W. Peterson and D. Brown, Cyclic Codes \nfor Error Detec\u00adtion, Proceedings of the IRE 49 pp. 228-235 (January 1961). [15] M. Stephenson, S. Amarasinghe, \nM. Martin, and U. O Reilly, Meta Optimization: Improving Compiler Heuris\u00adtics with Machine Learning, \nACM SIGPLAN Conference on Programming Language Design and Implementation, pp. 77-90 (June 2003). [16] \nS. Triantafyllis, M. Vachharajani, N. Vachharajani, and D. August, Compiler Optimization Space-Exploration, \nACM SIGMICRO International Symposium on Code Generation and Optimization, (March 2003). [17] S. Vegdahl, \nPhase Coupling and Constant Generation in an Optimizing Microcode Compiler, International Symposium on \nMicroarchitecture, pp. 125-133 (1982). [18] R. Whaley, A. Petitet, and J. Dongarra, Automated Empiri\u00adcal \nOptimization of Software and the ATLAS Project, Par\u00adallel Computing 27(1) pp. 3-35 (2001). [19] D. Whit.eld \nand M. L. Soffa, An Approach for Exploring Code-Improving Transformations, ACM Transactions on Programming \nLanguages and Systems 19(6) pp. 1053-1084 (November 1997). [20] W. Zhao, B. Cai, D. Whalley, M. Bailey, \nR. van Engelen, X. Yuan, J. Hiser, J. Davidson, K. Gallivan, and D. Jones, VISTA: A System for Interactive \nCode Improvement, ACM SIGPLAN Conference on Languages, Compilers, and Tools for Embedded Systems, pp. \n155-164 (June 2002).  \n\t\t\t", "proc_id": "996841", "abstract": "It has long been known that a fixed ordering of optimization phases will not produce the best code for every application. One approach for addressing this phase ordering problem is to use an evolutionary algorithm to search for a specific sequence of phases for each module or function. While such searches have been shown to produce more efficient code, the approach can be extremely slow because the application is compiled and executed to evaluate each sequence's effectiveness. Consequently, evolutionary or iterative compilation schemes have been promoted for compilation systems targeting embedded applications where longer compilation times may be tolerated in the final stage of development. In this paper we describe two complementary general approaches for achieving faster searches for effective optimization sequences when using a genetic algorithm. The first approach reduces the search time by avoiding unnecessary executions of the application when possible. Results indicate search time reductions of 65% on average, often reducing searches from hours to minutes. The second approach modifies the search so fewer generations are required to achieve the same results. Measurements show that the average number of required generations decreased by 68%. These improvements have the potential for making evolutionary compilation a viable choice for tuning embedded applications.", "authors": [{"name": "Prasad Kulkarni", "author_profile_id": "81100633789", "affiliation": "Florida State University, Tallahassee, FL", "person_id": "PP18002476", "email_address": "", "orcid_id": ""}, {"name": "Stephen Hines", "author_profile_id": "81100100608", "affiliation": "Florida State University, Tallahassee, FL", "person_id": "PP14045005", "email_address": "", "orcid_id": ""}, {"name": "Jason Hiser", "author_profile_id": "81100623088", "affiliation": "University of Virginia, Charlottesville, VA", "person_id": "P542766", "email_address": "", "orcid_id": ""}, {"name": "David Whalley", "author_profile_id": "81100296923", "affiliation": "Florida State University, Tallahassee, FL", "person_id": "PP39036517", "email_address": "", "orcid_id": ""}, {"name": "Jack Davidson", "author_profile_id": "81100099215", "affiliation": "University of Virginia, Charlottesville, VA", "person_id": "P369474", "email_address": "", "orcid_id": ""}, {"name": "Douglas Jones", "author_profile_id": "81100556447", "affiliation": "University of Illinois, Urbana, IL", "person_id": "PP14193170", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/996841.996863", "year": "2004", "article_id": "996863", "conference": "PLDI", "title": "Fast searches for effective optimization phase sequences", "url": "http://dl.acm.org/citation.cfm?id=996863"}