{"article_publication_date": "06-09-2004", "fulltext": "\n Inducing Heuristics To Decide Whether To Schedule John Cavazos J. Eliot B. Moss Department of Computer \nScience Department of Computer Science University of Massachusetts, Amherst University of Massachusetts, \nAmherst Amherst, MA 01003-9264, USA Amherst, MA 01003-9264, USA cavazos@cs.umass.edu moss@cs.umass.edu \nCategories and Subject Descriptors D.3.4 [Programming languages]: Processors Compilers, Optimiza\u00adtion; \nI.2.6 [Arti.cial intelligence]: Learning Induction  General Terms Performance, Experimentation, Languages \n Keywords Compiler optimization, Machine learning, Supervised learning, In\u00adstruction scheduling, Java, \nJikes RVM  ABSTRACT Instruction scheduling is a compiler optimization that can improve program speed, \nsometimes by 10% or more but it can also be ex\u00adpensive. Furthermore, time spent optimizing is more important \nin a Java just-in-time (JIT) compiler than in a traditional one because a JIT compiles code at run time, \nadding to the running time of the pro\u00adgram. We found that, on any given block of code, instruction schedul\u00ading \noften does not produce signi.cant bene.t and sometimes degrades speed. Thus, we hoped that we could focus \nscheduling effort on those blocks that bene.t from it. Using supervised learning we induced heuristics \nto predict which blocks bene.t from scheduling. The induced function chooses, for each block, between \nlist scheduling and not scheduling the block at all. Using the induced function we obtained over 90% \nof the improve\u00adment of scheduling every block but with less than 25% of the schedul\u00ading effort. When \nused in combination with pro.le-based adaptive op\u00adtimization, the induced function remains effective \nbut gives a smaller reduction in scheduling effort. Deciding when to optimize, and which optimization(s) \nto apply, is an important open problem area in com\u00adpiler research. We show that supervised learning solves \none of these problems well. 1. INTRODUCTION It is common for compiler optimizations to bene.t certain \nprograms, while having little impact (or even a negative impact) on other pro\u00adgrams. For example, instruction \nscheduling is able to speed up certain Permission to make digital or hard copies of all or part of this \nwork for personal or classroom use is granted without fee provided that copies are not made or distributed \nfor pro.t or commercial advantage and that copies bear this notice and the full citation on the .rst \npage. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior \nspeci.c permission and/or a fee. PLDI 04, June 9 11, 2004, Washington, DC, USA. Copyright 2004 ACM 1-58113-807-5/04/0006 \n...$5.00. programs, sometimes by 10% or more [16]. Yet on other programs, applying instruction scheduling \nhas little impact (and in some rare cases, degrades performance). Reasons for this are that equivalent \nor\u00adderings happen to execute at the same speed, or because the block has only one legal order, etc. If \ninstruction scheduling was a cheap optimization to apply we would apply it to all blocks without regard \nto whether it bene.ts a par\u00adticular block. However, scheduling is a costly optimization, account\u00ading \nfor more than 10% of total compilation time in our optimizing JIT compiler. Because it is costly and \nbecause it is not bene.cial to many blocks (or even entire programs), we want to apply it selectively. \nWe would prefer to apply scheduling to those blocks that: 1) ac\u00adcount for a signi.cant part of a program \ns running time, and 2) will bene.t from applying scheduling to them. Determining the .rst property is \ndone through pro.ling and is a well-studied area of research [3, 19, 5]. On the other hand, deter\u00admining \nthe second property, before applying scheduling, has received relatively little attention. Also, to our \nknowledge this is the .rst ap\u00adplication of supervised learning to determine whether to apply an op\u00adtimization, \nin our case instruction scheduling. (Supervised learning is the induction of functions from labeled examples. \nIn this case, we de\u00advelop examples from a benchmark suite, where each example comes from a basic block. \nThe example gives the values for certain proper\u00adties of the block (called the features) and a label indicating \nwhether or not scheduling improves the block.) We present here a technique for building heuristics, which \nwe call .lters, that accurately predict which blocks will bene.t from schedul\u00ading. This allows us to \n.lter from scheduling the blocks that will not bene.t from this optimization. Since in practice a large \nfraction of blocks do not bene.t from instruction scheduling, and since the .lter is much cheaper to \napply than instruction scheduling itself, we signif\u00adicantly decrease the compiler s time spent scheduling \ninstructions. We show that inexpensive and static features can be successfully used to determine whether \nto schedule or not. 1.1 Instruction scheduling Consider the sequence of machine instructions that a Java \nJIT com\u00adpiler emits for a Java method. A number of permutations of this se\u00adquence may, when executed, \nproduce the same result as the original but different permutations may execute at different speeds. To \nim\u00adprove code speed, compilers often include an instruction scheduler, which chooses a semantically equivalent, \nbut one hopes faster, per\u00admutation. Permutations are semantically equivalent if all pairs of de\u00adpendent \ninstructions occur in the same order in both permutations. Two instructions are dependent if they access \nthe same data (regis\u00adter, memory, etc.) and at least one writes the data, or if at least one of the instructions \nis a branch. (With additional analysis (and some\u00adtimes insertion of compensation code), some schedulers \nsafely move instructions across a branch [9, 8]; we do not pursue that further here.) We consider list \nscheduling over basic blocks: sequences with one entry and one exit. List scheduling is a traditional \nand widely used instruction scheduling method [11]. We used the critical path schedul\u00ading (CPS) model \n[16] in implementing our list scheduler. List schedul\u00ading works by starting with an empty schedule and \nthen repeatedly ap\u00adpending ready instructions to it. An instruction I is ready if every instruction upon \nwhich I depends is already in the schedule. If there is more than one ready instruction, CPS chooses \nthe one that can start soonest. If there is a tie, CPS chooses the instruction that has the longest (weighted) \ncritical path to the end of the block, i.e., the path of dependent instructions that takes the longest \nto execute. While we offer this description of our scheduler for those who are interested, we note that \nour .ltering technique applies to any competent scheduler: in essence we are discriminating between those \nblocks that a sched\u00aduler can improve signi.cantly and those that it cannot, and this has more to do with \nthe block than with details of the scheduler, provided the scheduler is generally competent at making \nsome improvement if it is possible.  2. PROBLEM AND APPROACH We want to construct a .lter that with \nhigh effectiveness predicts whether scheduling a block will bene.t an application s running time. The \n.lter should be signi.cantly cheaper to apply than instruction scheduling; thus we restrict ourselves \nto using properties (features) of a block that are cheap to compute. To our knowledge, this is the .rst \ntime anyone has used static features to apply instruction schedul\u00ading selectively, so we had no good \nhand-coded heuristics to start from. We opted not to construct .lters by hand, but instead to try to \ninduce them automatically using supervised learning techniques. Developing and .ne-tuning .lters manually \nrequires experimenting with different features (i.e, combinations of features of the block). Fine-tuning \nheuristics to achieve suitable performance is therefore a tedious and time-consuming process. Machine \nlearning, if it works, is thus a desirable alternative to manual tuning. Our approach uses a technique \ncalled rule induction to induce a .l\u00adter that is based on the features of the block. Other researchers \nhave applied unsupervised learning, speci.cally genetic programming, to the problem of deriving compiler \nheuristics automatically, and argued for its superiority [18]. In contrast, we found that supervised \nlearn\u00ading is not only applicable to this problem, but preferable because (1) it is faster,1 simpler, \neasier to use with rule induction (giving more understandable heuristic functions), and easier to make \nwork (than unsupervised learning). We emphasize that while scheduling involves a heuristic to choose \nwhich instruction to schedule next, the learning of which we have considered elsewhere [15], our goal \nhere is to learn to choose between scheduling and not scheduling, not to induce the heuristic used by \nthe scheduler. In other words, the research here involves learning whether to schedule, while that previous \nresearch involved learning how to schedule. We now consider the features, the methodology for developing \nthe training instances, and the learning algorithm in more detail. 2.1 Features What properties of a \nblock might predict its scheduling improvement? This aspect of applying machine learning is more an art \nthan a step\u00adby-step procedure. It is the part of machine learning that least yields to having a procedure \nthat is guaranteed to produce a good result. It is not uncommon to need to iterate a number of times \nin developing features 1Our technique induces heuristics in seconds on one desktop com\u00adputer. Stephenson \net al. report taking days to induce heuristics on a cluster of 15 to 20 machines. Feature Type Meaning \nbbLen BB size Number of Instructions in the block Category Fraction of instructions that ... Branch Op \nkind are Branches Call Op kind are Calls Load Op kind are Loads Store Op kind are Stores Return Op kind \nare Returns Integer FU use use an Integer functional unit Float FU use use a Floating point functional \nunit System FU use use a System functional unit PEI Hazard are Potentially Excepting GC Hazard are Garbage \nCollection points TS Hazard are Thread Switch points Yield Hazard are Yield points Table 1: Features \nof a basic block. for a given problem. However, we believe it is much easier to develop features of a \nproblem than to come up with interesting combinations of features (i.e., heuristics) that are successful \nat solving a problem. One can imagine that certain properties of the block s dependence graph (DAG) might \npredict scheduling bene.t. However, building the DAG is an expensive phase that can sometimes dominate \nthe over\u00adall running time of the scheduling algorithm [16]. Since we require cheap-to-compute features, \nwe speci.cally choose not to use proper\u00adties of the DAG. Instead, we try the simplest kind of cheap-to-compute \nfeatures that we thought might be relevant. Computing these features requires a single pass over the \ninstructions in the block. We grouped the different kinds of instructions into 12 possibly overlapping \ncategories, where instructions in each category have sim\u00adilar scheduling properties. Rather than examining \nthe structure of the block, we consider just the fraction of instructions of each category that occur \nin the block (e.g., 30% loads, 22% .oating point, 5% yield points, etc.). We also supply the block size \n(number of instructions in the block). See Table 1 for a complete list of the features. Hazards are possible \nbut unusual branches, which disallow reordering. These features are as cheap to compute as we can imagine \nwhile offering some useful information. It turns out that they work well. We present all of the features \n(except block size) as ratios to the size of the block (i.e., fraction of instructions falling into a \ncategory, rather than the number of such instructions). This allows the learning algorithm to generalize \nover many different block sizes. We could have potentially re.ned our set of features by including more \nof different kinds, but what we have works well. We note that coming up with features for other optimizations \nmight be easy or hard, depending on the optimization. In this case, we were lucky in that a little domain \nknowledge allowed us to develop on our .rst attempt a set of features that produced highly-predictive \nheuristics. Through our experience with identifying features and using machine learning, we noticed some \nuseful and (possibly obvious) general principles. 1. Experiment with the simplest features .rst. In this \ncase, it would have been moot to develop additional features. 2. Normalize features and simplify them. \nPrefer categorical or boolean values over integral or continuous ones. Binning of continuous values can \nalso help the learning task: it simpli.es and also tends to enhance readability of the induced heuristic. \n 3. Examine any relevant hand-coded heuristics. This not only helps in identifying important features \nto use, but allows us to see the underlying structure of successful heuristics, which will give clues \nas to how the features should be represented and used.  4. Apply the simplest learning algorithm possible \nto start with. Obviously, a procedure that is easier to get working (i.e., re\u00adquire less tweaking) is \npreferable. It is possible that a smaller set of features would perform nearly as well. However, it is \ndoubtful that in this case reducing the feature set will make the .ltering process run faster, since \nwe calculate fea\u00adtures values in a simple linear pass over the basic block, incrementing counters corresponding \nto the categories pertaining to the instruction. Calculating features and evaluating the heuristic functions \ntypically consumed .5%-1% of compile time (a negligible fraction of total time) in our experiments, so \nwe did not explore this possibility. For other problems, it might be important to prune the feature set, \nespecially if the features are not as cheap to compute as these are. One .nal observation is that these \nfeatures are fairly generic, for the most part, and might be useful across a wide range of systems. The \nGC, TS, and Yield features are speci.c to Jikes RVM, but other sys\u00adtems may have similar barriers to \ncode reordering. If those barriers are plain in the code being scheduled, then a feature similar to ours \nwill probably be useful. 2.2 Learning Methodology Determining what features to use is an important (and \npossibly the most dif.cult) step in applying supervised learning to a problem. Once we determine the \nfeatures, we can generate positive and negative ex\u00adamples for training the supervised learning component. \nThese train\u00ading instances consist of a vector of feature values, plus a boolean clas\u00adsi.cation label, \ni.e., LS (Schedule) or NS (Don t Schedule), depending on whether or not the block bene.ts from scheduling. \nOur procedure is as follows. As the Java system compiles each Java method, it divides the method into \nblocks, which it presents to the instruction scheduler. We instrument the scheduler to print into a trace \n.le raw data for forming instances, consisting of the features of the block and an estimate of the block \ns cost (number of cycles) without scheduling, and an estimate of the block s cost with list scheduling \napplied. We obtain these estimates of block cost from a simpli.ed machine simulator. The simulator makes \na number of simplifying assumptions, partly for speed but also because it is hard to determine what the \nstate of the machine will be at run time at the moment the machine begins to execute a particular block \n(and that state may be different for different executions of the same block). The exact cycle estimate \nis not crucial; rather, the estimate needs only to give a good sense of the difference in timing between \ntwo versions of the same block. Note that on modern processors timing of small code fragments is not \nonly dif.cult, it is not clear that it is meaningful, because there may be tens, even hundreds, of instructions \nin .ight, with execution overlapped. It is not clear how one would further validate our simpli.ed simulator: \nthat our overall procedure produces good results is itself evidence of adequacy of the estimator.2 We \nlabel an instance with LS if the estimated time after list schedul\u00ading is more than t% less than before \nscheduling. We label an instance with NS if scheduling is not better (at all). We do not produce a training \ninstance if the bene.t lies between 0 and t%. We call t the threshold value. We .rst consider the case \nt =0 and discuss positive threshold values later. Typically we obtain thousands of instances for each \nprogram (one for each block in the program). Table 5(a) shows training set sizes for different threshold \nvalues for SPECjvm98. We apply a learning algorithm to the training instances; the output of the learning \nalgorithm is a heuristic function: given the features of the block, it indicates whether or not we should \nschedule the block. 2The estimator is also used by the list scheduler as it makes decisions, but that \nusage is irrelevant to our learning procedure. Program Description compress Java version of 129.compress \nfrom SPEC 95 jess Java expert system shell db Builds and operates on an in-memory database javac Java \nsource to bytecode compiler in JDK 1.0.2 mpegaudio Decodes an MPEG-3 audio .le raytrace A raytracer working \non a scene with a dinosaur jack A Java parser generator with lexical analysis Table 2: Characteristics \nof the SPECjvm98 benchmarks. It is important to note that the procedure above (including learning) occurs \nentirely of.ine. The .nal step involves installing the heuristic function in the com\u00adpiler and applying \nit online. Each block from each method that is compiled by the optimizing compiler is considered as a \npossible can\u00addidate for scheduling. We compute features for the block. The cost of computing the features \nis included in all of our actual timings. It is small relative to scheduling and to the rest of the cost \nof compiling a method. If the heuristic function says we should schedule a block, we do so. 2.3 Learning \nAlgorithm An important rule in applying machine learning successfully is to try the simplest learning \nmethodology that might solve the problem. We chose the supervised learning technique called rule set \ninduction, which has many advantages over other learning methodologies. The speci.c tool we use is Ripper \n[6]. It is easy and fast to tune Ripper s parameters (typically an impor\u00adtant part in obtaining the best \nresult). Ripper generates sets of if-then rules that are more expressive, more compact, and more human \nread\u00adable (hence good for compiler writers) than the output of other learn\u00ading techniques, such as neural \nnetworks and decision tree induction algorithms. We analyze one of the induced if-then rule sets (a .lter) \nin Section 4.6. 2.4 Benchmarks We examine 7 programs drawn from the SPECjvm98 suite [17] in our .rst \nset of experiments. We detail our chosen benchmarks in Table 2. We ran these benchmarks with the largest \ndata set size (called 100).  3. EVALUATION METHODOLOGY As is customary in evaluating a machine learning \ntechnique, our learn\u00ading methodology was leave-one-out cross-validation: given a set of n benchmark programs, \nin training for benchmark i we train (develop a heuristic) using the training set (the set of instances) \nfrom the n .1 other benchmarks, and we apply the heuristic to the test set (the set of instances from \nbenchmark i). This makes sense in our case for two reasons: 1. We envision developing and installing \nof the heuristic at the factory , and it will then be applied to code it has not seen before.3 3One could \nprovide tools to end users so that they could develop their own training sets and retrain. This would \nbe valuable only if they are likely to come up with a signi.cantly different function, which would have \nsigni.cantly different performance. If we train over a large enough set at the factory , then we presumably \ncover all the interesting behaviors of our compiler and a variety of blocks that present a full range \nof scheduling issues. Thus, it is not clear that user retraining would have much value. This is something \nwe could explore using additional experimental data, such as training on an in\u00addividual program and testing \non that same program, which gives a kind of upper bound on how much improvement you could get by retraining. \n2. While the end goal is to develop a single heuristic, it is impor\u00adtant that we test the overall procedure \nby developing heuristics many times and seeing how well they work. The leave-one-out cross-validation \nprocedure is a commonly used way to do this. Another way is repeatedly to choose about half the programs \nand use their data for training and the other half for testing. However, we want our heuristics to be \ndeveloped over a wide enough range of benchmarks that we are likely to see all the interesting behaviors, \nso leave-one-out may be more realistic in that sense. To evaluate a .lter on a benchmark, we consider \nthree kinds of re\u00adsults: classi.cation accuracy, scheduler running time, and application running time. \nClassi.cation accuracy refers to the accuracy of the induced .lter on correctly classifying a set of \nlabeled instances. Classi.cation accu\u00adracy tells us whether a .lter heuristic has the potential of being \nuseful, however, the real measure of success lies in whether applying the .lter can successfully reduce \nscheduling time while not adversely affecting the bene.t of scheduling to application running time. In \na few cases, using a .lter improved application running time over always applying the scheduler (this \noccurs when .lters inhibit scheduling that actually degrades performance). Scheduler running time refers \nto the impact on compile time, com\u00adparing against not scheduling at all, and against scheduling every \nblock. Since timings of our proposed system include the cost of com\u00adputing features and applying the \nheuristic function, this (at least indi\u00adrectly) substantiates our claim that the cost of applying the \nheuristic at run time is low. (We also supply measurements of those costs, in a separate section.) Application \nrunning time (i.e., without compile time), refers to mea\u00adsuring the change in execution time of the scheduled \ncode, comparing against not scheduling and against scheduling every block. This val\u00adidates not only the \nheuristic function but also our instance labeling procedure, and by implication the block timing simulator \nwe used to develop the labels. What we can verify with this is that we have not undermined the scheduler; \nthe scheduler can still improve some pro\u00adgrams a lot while having little impact on others. The goal is \nto achieve application running time close to the best of the .xed strategies, and compilation time substantially \nless than scheduling every block. 3.1 Experimental infrastructure We implemented our instruction schedulers \nin Jikes RVM, a Java vir\u00adtual machine with JIT compilers, provided by IBM Research [1]. Jikes RVM does \nnot have an interpreter: all bytecodes are compiled into na\u00adtive code before execution. The system has \ntwo bytecode compilers, a baseline compiler that essentially macro-expands each bytecode into machine \ncode, and an optimizing compiler. We optimized all methods at the highest optimization setting, and with \naggressive settings for inlining. We used the build con.gura\u00adtion called OptOpt with more aggressive \ninlining, which increases scheduling bene.t.4 As mentioned previously, we apply our technique to local \n(basic block) scheduling, not global scheduling. We have investigated su\u00adperblock scheduling in our compiler \nsetting, and with it one can get slight (1-2%) additional improvement over local scheduling. How\u00adever, \nsuperblock formation requires detailed pro.ling information and we did not want to require that. Also, \nit is in a way beside the point: we are not trying to build a better scheduler, but trying to decide \n4We set the maximum callee size to 30 bytecode instructions, the maximum inlining depth to 6, and the \nupper bound on the relative expansion of the caller due to inlining to be a factor of 7. whether to apply \nwhatever scheduler we have. We could apply our same procedure to the superblock case, and it might provide \nadditional evidence that we can induce heuristics that greatly reduce scheduling effort while preserving \nmost of the bene.t. Later we offer some comparison with compilation techniques that identify and optimize \nonly frequently executed (hot) methods. Our speci.c target architecture is the PowerPC. We ran our exper\u00adiments \non an Apple Macintosh system with two 533 MHz G4 pro\u00adcessors, model 7410. This is an aggressive superscalar \narchitecture and represents the current state of the art in processor implementa\u00adtions.5 For instruction \nscheduling, the 7410 implementation of the PowerPC is interestingly complex, having two dissimilar integer \nfunc\u00adtional units and one each of the following functional units: .oating point, branch, load/store, \nand system (handles special system instruc\u00adtions). It can issue one branch and two non-branch instructions \nper cycle, if a complicated set of conditions holds. Instructions take from one to many tens of cycles \nto execute. What value does static instruction scheduling have in the face of out-of-order execution, \netc.? We have done some investigation of older processors, which have less dynamic scheduling (reordering \nof execution in the hardware), and static scheduling does give bigger percent improvements on such architectures. \nHowever, we still see useful improvements for some programs on more recent machines. In a sense this \nsupports our methodology: if static scheduling helps a lot sometimes, but only in a minority of cases, \nthen it is more interesting to have a good way to choose when to apply it. All measurements are elapsed \n(wall clock) times. The system in\u00adfrastructure also measures elapsed time spent in the compiler, broken \ndown by phase and individual optimization. These measurements use the bus clock rate time counter and \nthus give sub-microsecond accu\u00adracy; this clock register is also cheap to read, so there is little over\u00adhead \nin collecting the information.6 The time to apply the .lter was included in the cost we attribute to \nscheduling.  4. EXPERIMENTAL RESULTS We aimed to answer the following questions: How ef.cient is schedul\u00ading \nusing .lter heuristics as compared to scheduling all blocks? How effective are the .lter heuristics in \nobtaining best application perfor\u00admance? We ask these questions .rst on the SPECjvm98 standard benchmark \nsuite and next on a suite that includes only benchmarks for which list scheduling made an impact of more \nthan 2% on their run\u00adning time. We then consider some additional questions, such as how much time does \nit take to apply our heuristic .lters in the compiler, and what happens if we apply our .lter in compilations \nthat optimize only hot methods. We address the .rst question by comparing the time spent schedul\u00ading. \nWe answer the second by comparing the running time of the application, with compilation time removed. \nTo accomplish the latter, we requested that the Java benchmark iterate 6 times. The .rst iter\u00adation will \ncause the program to be loaded, compiled, and scheduled according to the appropriate scheduling protocol. \nThe remaining 5 iterations should involve no compilation; we use the median of the 5 runs as our measure \nof application performance. 4.1 Classi.cation Accuracy Before presenting ef.ciency and effectiveness \nresults, we offer statis\u00adtics on the accuracy of the induced classi.ers (for threshold values 5The G5 \nprocessor is only just beginning to be available as of this writing, and was not available when we performed \nmost of the re\u00adsearch. In any case, it is at least as complex as the 7410. 6Applying the .ltering function \n(heuristic) is clearly cheap, but if the reviewers feel it to be important to do so, we can break that \ncost out and report it. t from 0 to 50). For each benchmark, we built a .lter with leave\u00adone-out cross-validation \nusing the set of benchmarks from which the particular benchmark in question came. The .lter chooses between \nlist scheduling and no scheduling. Table 3 shows the classi.cation errors rates of rules induced by Rip\u00adper \non SPECjvm98 benchmark program test sets generated during the cross-validation tests. We also include \nthe geometric mean of these error rates. These impress us as good error rates, and they are also fairly \nconsistent across the benchmarks. t % com\u00adpress jess ray\u00adtrace db javac mpeg\u00adaudio jack Geo. mean 0 6.7 \n7.7 11.0 6.3 8.3 7.5 7.6 7.86 5 6.5 8.3 9.0 5.9 8.9 6.9 7.0 7.53 10 5.8 7.5 7.1 5.4 8.6 5.9 6.1 6.62 \n15 5.7 6.7 6.5 5.5 6.9 5.8 5.4 6.04 20 1.5 2.0 2.8 1.1 4.1 2.3 1.7 2.22 25 0.9 1.2 2.5 0.7 3.1 1.8 1.3 \n1.63 30 0.8 1.0 1.8 0.3 2.2 1.2 1.0 1.17 35 0.3 0.5 1.3 0.3 1.5 1.5 1.1 0.92 40 0.5 0.3 0.4 0.1 0.9 0.6 \n0.2 0.43 45 0.2 0.0 0.2 0.1 0.2 0.2 0.0 0.14 50 0.0 0.0 0.2 0.0 0.1 0.1 0.0 0.06 Table 3: Classi.cation \nerror rates (percent misclassi.ed) for dif\u00adferent threshold values. 4.2 Simulated Execution Times Before \nlooking at execution times on an actual machine, we consider the quality of the induced .lters (compared \nwith always scheduling and never scheduling) in terms of the simulated running time of each benchmark. \nWe used the block simulator to predict (and therefore la\u00adbel) whether a block will bene.t from scheduling \nor not. Thus, we hoped that our .lters would perform well on a metric based on time reported by the block \nsimulator. Comparing our .lters with simu\u00adlated execution time helps us validate the learning methodology, \nand to separate validation of the learning methodology from validation of the block simulator s model \nof the actual machine. We calculate the weighted simulated running time of each block by multiplying \nthe block s simulated time by the number of times that block is executed (as reported by pro.ling information). \nWe obtain the simulated running time of the application by summing the weighted simulated running time \nof each block. More precisely, the perfor\u00admance measure for program P is: SIMp(P)=.(# Executions of b).(cycles \nfor b under scheduler p) bEP where b is a basic block and p is either using a .lter, always schedul\u00ading, \nor never scheduling. Table 4 shows predicted execution times as a ratio to predicted time of unscheduled \ncode. We see that the model predicts improvements at all thresholds. These improvements do not correspond \nexactly to our measured improvements, which is not sur\u00adprising given the simplicity of the basic block \ntime estimator. What the numbers con.rm is that the induced heuristic indeed improves the metric on which \nwe based its training instances. Thus, machine learn\u00ading worked . Whether we get improvement on the real \nmachine is concerned with how predictive the basic block simulator is of reality, at least in relative \nterms. 4.3 Ef.ciency and Effectiveness We now consider the quality of each induced .lter for threshold \nt =0, and then present results for the rest of the threshold values. Figure 1(a) shows the scheduling \ntime of the L/N .lters (chooses to schedule or not) relative to LS (always perform list scheduling). \nNS (no schedul\u00ading) is 0 since it does no scheduling work. We .nd that on average (geometric mean) L/N \ntakes 38% of the time of LS (i.e., is 2.5 times faster). These numbers are also fairly consistent across \nthe bench\u00admarks. Figure 1(b) shows the impact of L/N .lters and LS on application running time, presented \nrelative to NS (a value smaller than 1 is an improvement, greater than 1 a slow down). Here there is \nmore vari\u00adation across the benchmarks, with LS doing the best at .977 and L/N .lters doing well at .979. \nOf the bene.t LS obtains (2.3%), L/N ob\u00adtains 93% of it. Given the substantially lower cost of L/N to \nrun, it is preferable to running LS all the time. The results are fairly consistent across the benchmarks, \nthough some benchmarks improve more than others. Note that our features (and .lters) do not take into \naccount the importance of the blocks and therefore do not require pro.le infor\u00admation. Scheduling only \nimportant blocks based on pro.ling can do no better at improving application running time than just always \nscheduling (unless scheduling degrades performance). Thus, even if we used pro.le information to schedule \nonly the important blocks, we could still improve application running time over L/N only by a small amount. \n(Note: here we are talking only about skipping scheduling of cold blocks; skipping all optimized compilation \nof cold blocks is a different matter, which we address in Section 4.8.)  4.4 Filtering the Instances \nWhile the t =0 result is not bad, we suspected that we could improve the classi.cation error rates by \nincreasing t. (Of course for a value large enough, only the NS category would be left and the error rate \nwould be 0%!) More signi.cantly, we suspected that by eliminat\u00ading instances where the schedulers behaved \nsimilarly, and giving the learning algorithm only those points where the choice makes a signif\u00adicant \ndifference, we might improve scheduler effectiveness. We were less certain of the impact on ef.ciency, \nbut thought it might increase because the training sets would have fewer LS instances, but as many NS \ninstances. We reasoned that this would tend to induce functions that would prefer scheduling blocks less \noften. These speculations were borne out, as can be seen in Figures 2(a) and 2(b). Again, we performed \nthe experiment for the L/N protocol, varying t from 0 to 50 in increments of 5. Note that t =0 is the \nsame L/N from the previous graphs. Considering .rst the ef.ciency effects, the geometric mean shows a \nsteady improvement as t goes from 0 to 50, from 39% to 6% of the cost of LS. This is somewhat consistent \nacross the benchmarks, but there is de.nite variation. We were able to cut the scheduling effort in half, \nbut what happened to the effectiveness? First it degraded, but at t =20 it improved, offering 93% of \nthe bene.t of LS. Thereafter, it generally degrades. While the results seem sensitive to the exact value \nof t, the value 20 improves over straight L/N (t =0). At this value, scheduling is 4.3 times faster than \nLS. How does thresholding affect the size of the training sets? And how does it affect the classi.cation \nby the induced heuristics? We include two tables that offer some simple statistics that show what happens. \nTable 5(a) indicate how many instances (across all the benchmarks) have label LS at the given t value. \nThat number is constant for NS (at 37280, so we show statistics only for instances with the LS label), \nbut drops off steadily for LS as t increases. Table 5(b) show how many instances at run time (i.e., when \nthe optimizer processes the application s code) were classi.ed with that label by the induced heuristic. \nWe develop the use numbers separately Threshold Values 0% 5% 10% 15% 20% 25% 30% 35% 40% 45% 50% compress \n84.66 83.70 83.92 83.79 87.33 84.18 85.45 98.16 92.79 99.55 100.00 jess 92.53 92.09 95.76 92.64 95.51 \n97.09 97.69 96.42 98.46 99.98 100.00 Benchmark program raytrace db javac 93.56 88.53 96.63 92.81 88.53 \n96.08 84.60 88.53 96.92 86.38 88.54 97.55 89.38 92.61 97.07 92.31 90.24 97.79 89.68 90.34 97.77 99.48 \n92.74 97.94 99.78 99.94 98.06 99.96 100.00 98.85 99.95 100.00 100.00 mpegaudio 90.53 89.26 86.84 89.16 \n87.00 92.10 92.26 99.75 97.16 89.99 97.64 jack 97.20 97.16 97.35 97.54 97.62 98.39 98.97 99.59 99.43 \n99.93 99.89 Geometric mean 91.85 91.27 90.39 90.67 92.26 93.04 93.04 97.70 97.92 98.26 99.64 Table 4: \nPredicted execution times for different threshold values. (a) Scheduling Time Using No Thresholds (b) \nApplication Running Time Using No Thresholds Figure 1: Ef.ciency and Effectiveness Using Filters. for \neach benchmark s heuristic (using leave-one-out cross validation), applied to that benchmark s instances; \nthe table gives the sum across the benchmarks. The sum is the same for all t values (45453), but the \nnumber of NS instances increases, and the number of LS instances steadily decreases, as t increases. \nThis clearly explains the ef.ciency results. As the threshold increases, the induced rules predict more \nblocks not to bene.t from scheduling. This result further shows that effectiveness depends on the scheduling \nof a rather small minority of the methods, 7.4% of them for t =20. This is not surprising: in compiler \noptimization it is often true that an optimization has little effect on many if not most programs, but \nis crucial to improving a certain minority of them. Since our noise reduction technique worked well in \nthis case, we encourage others to explore its effectiveness in other settings. We suspect it may be helpful \nwhenever class labels are chosen on a best (a) (b)or better than basis that compares a predicted metric \n(such as sim\u00adulated cycle count of a block) under different treatments (scheduling Table 5: Training \nset sizes (a) and number of blocks classi.ed NS and not scheduling). vs. LS at run time (b) for each \nthreshold value. 4.5 Filtering Applied to Other Benchmarks point) computations. For this architecture, \ninstruction scheduling is While the above result is not bad, we suspected that we would have an important \noptimization for removing stalls caused by .oating point better results focusing only on benchmarks where \nscheduling is ben-instructions having long latencies. e.cial. We gathered a suite of programs that bene.t \nfrom scheduling Our reasoning for focusing on the following set of benchmarks is through an exploration \nof freely available Java programs on the Inter-as follows: If a program gains little bene.t from scheduling \nat all, net. Table 6 offers more details about these benchmarks. Note that this our .ltering technique \ncan reduce the compile time, but will have no suite of benchmarks consists solely of numerically intensive \n(.oating-substantial impact on the application running time. We could do a t% 0 5 10 15 20 25 30 35 40 \n45 50 Training set size LS LS% 8173 21.9 7976 21.4 7098 19.0 4930 13.2 2438 6.5 1443 3.9 912 2.4 565 \n1.5 316 0.8 192 0.5 49 0.1 Classi.cations at run time NS LS LS% 39389 6064 13.3 39256 6197 13.6 40250 \n5203 11.4 41065 4388 9.7 42046 3407 7.5 42557 2896 6.4 43154 2299 5.1 44061 1392 3.1 44851 602 1.3 45142 \n311 0.7 45293 160 0.4  (a) Scheduling Time Using Thresholds (b) Application Running Time Using Thresholds \nFigure 2: Ef.ciency and Effectiveness Using Filter Thresholds. Program Description linpack A numerically \nintensive .oating point kernel power Power pricing optimization problem solver bh Barnes and Hut N-body \nphysics algorithm voronoi Computes Voronoi diagram of a set of points re\u00ad cursively on the tree aes Tests \nvectors from the NIST encryption tests scimark A scienti.c and numerical computation Table 6: Characteristics \nof a set of benchmarks that bene.t from scheduling. poor job, or a good job, of choosing which blocks \nto schedule, and it won t matter because the scheduler just is not having much effect. On the other hand, \nif you consider a program that gets a lot of bene.t from scheduling, then we want to make sure that we \ndo not seriously undermine that bene.t. Focusing on benchmarks that gain scheduling bene.t allows us \nto determine this. Suppose, for the sake of argument, we included a large number of programs with little \n(but barely measurable) scheduling bene.t. And further suppose that we show that .ltering preserves that \nbene.t. We claim that is not at all as interesting or useful as showing that we preserve the bene.t gained \nby programs that bene.t a lot from scheduling. By focusing on this set of benchmarks we are trying to \nbe more critical, not less, of our technique. We expected that .ltering would achieve most of the bene.t \nof scheduling all blocks, while being much more ef.cient. This expecta\u00adtion was borne out, as can be \nseen in Figures 3(a) and 3(b).  4.6 A Sample Induced (Learned) Filter Some learning schemes produce \nexpressions that are dif.cult for hu\u00admans to understand, especially those based on numeric weights and \nthresholds such as neural networks and genetic algorithms. Rule sets are easier to comprehend and are \noften compact. It is also relatively easy to generate code from a rule set that will evaluate the learned \n.lter in a scheduler. Table 7 shows a rule set induced by training using examples drawn from 6 of 7 SPECjvm98 \nbenchmark programs. If the right hand side condition of any rule (except the last) is met, then we will \napply the scheduler on the block; otherwise the learned .lter predicts that scheduling will not bene.t \nthe block. The numbers in the .rst two columns give the number of correct and incorrect training examples \nmatching the condition of the rule. In this case we see that block size and several classes of instructions \n(call, system, load, and store) are the most important features, with the rest offering some .ne tuning. \nFor example, the .rst if-then rule predicts that it is bene.cial to schedule blocks consisting of 7 instruc\u00adtions \nor more, that have a small fraction of call and PEI instructions, but possibly a larger fraction of load \nand integer instructions. Note that for this training set a large percentage of blocks were predicted \nnot to bene.t from scheduling. As we will see shortly, determining the feature values and then eval\u00aduating \nrules like this sample one does not add very much to compila\u00adtion time, and typically takes an order \nof magnitude less time than actually scheduling the blocks selected for scheduling. Thus, while we might \nbe able to eliminate some of the features and retain most of the effectiveness of the .lter heuristics, \nthere was not much motivation in this case to do so. 4.7 The Cost of Evaluating Filters Table 8 gives \na breakdown of the compilation costs of our system, and statistics concerning the percentage of blocks \nand instructions sched\u00aduled. For the individual programs, we give a range of values, covering all threshold \nvalues (t =0 through 50). As t increases, s decreases, and while f remains nearly constant, f ls will \nincrease. We also give, for each threshold value, the geometric mean of each statistic across the six \nbenchmarks. Here are some interesting facts revealed in the table. First, the frac\u00adtion of blocks and \nof instructions scheduled steadily decreases with increasing t, dropping signi.cantly at t =30 and t \n=35. Second, the fraction of instructions scheduled, which tracks the relative cost of scheduling fairly \nwell, tends to be about twice as big as the frac\u00adtion of blocks scheduled, implying that the .lter tends \nto retain longer blocks. This makes sense in that longer blocks probably tend to ben\u00ade.t more from scheduling. \nThird, the cost of calculating the .lter, as a percentage of non-scheduling compilation time, is always \n1% or less. Fourth, there is not a lot of variation in these statistics across the benchmarks. Finally, \nwe always obtain substantial reduction in scheduling time compared with List (scheduling every block). \nProgram SB SI f ls f lc slc aes 0.2 14% 0.2 36% 8 20% 1.0% 4 12% bh 0.1 19% 1.0 41% 7 27% 0.3% 1 5% linpack \n0.4 14% 0.9 30% 9 17% 1.0% 5 11% power 0.3 20% 0.6 40% 4 27% 0.4% 1 9% voronoi 0.3 19% 0.9 39% 4 27% \n0.4% 1 9% scimark 0.6 16% 1.3 37% 8 22% 1.0% 5 13% gm, t =0 16.8% 36.3% 6.5% 0.7% 10.3% gm, t =5 16.1% \n36.9% 6.3% 0.7% 10.3% gm, t =10 12.7% 28.5% 8.8% 0.7% 7.6% gm, t =15 11.2% 23.3% 9.3% 0.6% 6.7% gm, t \n=20 8.3% 20.5% 9.8% 0.6% 6.2% gm, t =25 9.2% 21.0% 10.3% 0.7% 6.3% gm, t =30 5.6% 10.2% 15.1% 0.6% 4.1% \ngm, t =35 1.7% 3.8% 20.0% 0.6% 3.0% gm, t =40 1.3% 2.9% 20.9% 0.6% 2.9% gm, t =45 0.5% 1.8% 21.8% 0.6% \n2.7% gm, t =50 0.3% 0.7% 22.6% 0.6% 2.5% List 100% 100% 0% 0.0% 13 23% Table 8: Cost breakdowns: gm \n=geometric mean; SB =sched\u00aduled blocks; SI =scheduled instructions; f =time to evaluate features and \nheuristic function; s =time spent in scheduling; c =compile time excluding scheduling.  4.8 Filtering \nwith Adaptive Optimization The Jikes RVM system includes an adaptive compilation mechanism, which determines \nat run time which methods are executed frequently, and then optimizes those methods at progressively \nhigher levels of optimization [2]. This system typically achieves most of the bene.t of optimizing all \nmethods, but with much lower compilation cost. There are two obvious comparisons one might make between \nour .ltering technique and the Jikes RVM adaptive system: 1. The improvement offered by .ltering alone \nversus the improve\u00adment offered by the adaptive system alone. Even when schedul\u00ading every basic block, \nscheduling costs only 13 23% of com\u00adpile time, which gives an upper bound on the improvement we can obtain \nwith .ltering. Adaptive compilation reduces com\u00adpile time much more than this, so we do not even bother \nto report speci.c numbers. 2. The improvement offered by the adaptive system alone versus the improvement \noffered by the adaptive system plus our .lter\u00ading. We report this comparison below.  (a) Scheduling \nTime Using Thresholds (b) Application Running Time Using Thresholds Figure 3: Ef.ciency and Effectiveness \nUsing Filters On Other Benchmarks. ( 924/ 12) list ;bbLen>.7 icalls<.0.0857 iloads<.0.3793 ipeis<.0.1493 \niintegers<.0.6087 ( 661/ 8) list ;bbLen>.7 isystems<.0.0889 istores<.0.05 ( 452/ 23) list ;bbLen>.7 icalls<.0.1034 \nistores>.0.1778 ( 218/ 14) list ;bbLen>.7 isystems<.0.0606 iintegers<.0.4167 ( 272/ 19) list ;bbLen>.7 \nisystems<.0.0741 ibranches<.0.1111 ( 518/ 41) list ;bbLen>.7 isystems<.0.0606 igcpoints>.0.0714 ( 269/ \n52) list ;bbLen>.7 icalls<.0.119 istores<.0.0667 ( 74/ 3) list ;bbLen>.5 istores<.0.1613 iloads>.0.3 \n( 166/ 5) list ;bbLen>.7 icalls<.0.119 ibranches<.0.0476 ( 75/ 13) list ;bbLen>.5 istores<.0.12 iloads>.0.2083 \n( 51/ 14) list ;bbLen>.7 isystems<.0.0741 iloads>.0.3 ( 39/ 8) list ;bbLen>.5 istores<.0.1562 iloads>.0.3 \n( 33/ 7) list ;bbLen>.5 istores<.0.1562 iloads>.0.3 ( 25/ 3) list ;bbLen>.5 istores<.0.12 iloads>.0.2222 \n( 18/ 5) list ;bbLen>.5 istores<.0.1613 iloads>.0.2941 (27476/1946) orig ; iloads>.0.1538 iloads>.0.375 \nipeis<.0.2361 iloads<.0.3667 iintegers>.0.4091 iloads>.0.2222 iintegers>.0.3438 ipeis<.0.1765 igcpoints<.0.0833 \nibranches<.0.1 iintegers<.0.3667 iintegers<.0.2857 istores>.0.1237 iintegers>.0.3448 iyieldpoints>.0.0143 \nisystems<.0.0465 ipeis<.0.2 iintegers>.0.3529 igcpoints<.0.1818 iintegers>.0.3529 ipeis<.0.15 iintegers>.0.3889 \nistores>.0.1 iintegers>.0.3846 icalls>.0.0769 iloads<.0.5556 iloads>.0.3636 istores<.0.0435 ipeis<.0.1667 \ni.oats<.0 iloads<.0.625 ipeis>.0.093 ipeis>.0.1667 ipeis>.0.125 icalls<.0 ibranches>.0.1111 istores>.0.1111 \n Table 7: Induced Heuristic Generated By Ripper. First we need to explain a methodological point. The \nadaptive system is generally triggered according to time-driven sampling of program counter values, which \nmake it non-deterministic from run to run. To obtain deterministic results we proceeded as follows. First, \nwe per\u00adformed a number of adaptive runs with compilation logging turned on. This told us, for each run, \nthe methods optimized and their opti\u00admization levels. From the logs, we determined for each method the \nhighest optimization level achieved by that method in a majority of the runs of that benchmark. To obtain \ndeterministic runs similar to the adaptive system, we force optimization of each method to its major\u00adity \nlevel when the system .rst attempts to compile the method, and we prevent any further optimization as \nthe system runs. We call this the Pseudo-Adaptive system, and its compile time and execution time behavior \nis very similar to the adaptive system. Table 9 shows .ltering cost breakdowns similar to those we pre\u00adsented \nfor .ltering alone. We observe that in this case our .lters select a larger fraction of the instructions. \nThis may seem surprising, but is logical in that these benchmarks tend to spend much of their time in \n.oating point computations, and blocks with .oating point instruc\u00adtions are more likely to bene.t from \ncareful scheduling. These blocks may also be longer, some of them resulting from loop unrolling, etc. \n Program SB SI f ls f lc slc aes 0.0 26% 0.0 68% 4 13% 0.5% 4 16% bh 0.0 24% 0.0 57% 4 7% 0.6% 7 14% \nlinpack 0.0 21% 0.0 63% 3 4% 0.4% 8 13% power 5.3 38% 12.2 71% 4% 0.4% 10 15% voronoi 1.7 30% 5.4 59% \n5 10% 0.9% 8 18% scimark 1.3 23% 2.0 59% 4 7% 0.7% 5 16% gm, t =0 26.0% 60.8% 4.0% 0.6% 14.6% gm, t =5 \n24.7% 61.3% 3.9% 0.6% 15.0% gm, t =10 23.3% 58.3% 4.2% 0.6% 14.8% gm, t =15 17.4% 42.6% 4.2% 0.6% 13.4% \ngm, t =20 18.3% 47.4% 4.0% 0.5% 13.4% gm, t =25 14.6% 38.3% 4.7% 0.6% 12.7% gm, t =30 6.4% 14.1% 5.7% \n0.6% 9.6% gm, t =35 6.1% 11.9% 5.7% 0.5% 9.2% gm, t =40 3.9% 8.2% 6.2% 0.5% 8.7% gm, t =45 0.0% 0.0% \n5.7% 0.4% 7.3% gm, t =50 0.0% 0.0% 6.7% 0.5% 7.2% Table 12 presents analogous measurements for the pseudo-adaptive \nsystem. Here it is clear that instruction scheduling is not at all helpful for bh, linpack,or voronoi. \nFor the remaining benchmarks, scheduling helps, at least sometimes.  Table 9: Cost breakdowns for Pseudo-Adaptive \nruns: gm =ge\u00adometric mean; SB =scheduled blocks; SI =scheduled instruc\u00adtions; f =time to evaluate features \nand heuristic function; s =time spent in scheduling; c =compile time excluding schedul\u00ading. Ef.ciency: \nScheduling all blocks in pseudo-adaptive runs con\u00adsumed about 16% of non-scheduling compile time (geometric \nmean). Comparing with Table 9, we see that .ltering does not save as much on this population of blocks. \nPut another way, .ltering, to some sig\u00adni.cant extent, avoids scheduling blocks that turn out to be cold. \nThis is interesting, though it is not immediately clear how we can exploit it (since scheduling sees \ncode in its form after most optimizations, we cannot apply the .lters much earlier in the optimization \nprocess). Still, we might be able to save perhaps 5% of compilation costs, provided .ltering does not \nundermine effectiveness. Effectiveness: Table 10 shows the geometric mean execution time ratio compared \nwith no scheduling, for list scheduling and for .lter\u00ading with our various threshold values. It is notable \nthat instruction scheduling appears less effective on the blocks optimized in (pseudo) adaptive runs. \nWe still do well for t =20 and t =25, but in those cases we reduce compilation time by at most a few \npercent. We are forced to conclude that .ltering may not be worthwhile in this adaptive com\u00adpilation \nsetting.  4.9 Time Compiling versus Time Running It might at .rst seem reasonable to report comparisons \nof total exe\u00adcution time (compilation plus application execution) with and without .ltering, etc. We \nbelieve this does not make much sense for bench\u00admark programs, since we can make application execution \ntime arbi\u00adtrarily large compared with compilation simply by iterating the bench\u00admark more times. Put \nanother way: is there any typical ratio of compile time to running time? Still, we can determine a pay-back \nra\u00adtio for each benchmark, i.e., how much of the added compilation time does each iteration of the application \nrecover? Table 11 gives these pay-back numbers for List and .ltering at the different thresholds for \nour six benchmarks that bene.t from scheduling. Some numbers are reported as <0 , which means that the \noptimization actually hurt application performance in that case. We observe that compile time is generally \nmuch greater than application time for these benchmarks; the application times for powerand scimarkare \ncloser to the com\u00adpile times, hence their higher pay-back.  5. RELATED WORK Instruction scheduling is \na well-known problem with a developed lit\u00aderature. It is also known that optimal instruction scheduling \nfor com\u00adplex processors is NP-complete [10]. For brevity and focus we de\u00adscribe the works, being most \nclosely related to ours in that they con\u00adsider application of machine learning to compiler optimization \nprob\u00adlems. Calder et al. [4] used supervised learning techniques, namely de\u00adcision trees and neural networks, \nto induce static branch prediction List t =0 t =5 t =10 t =15 t =20 t =25 t =30 t =35 t =40 t =45 t \n=50 94.3 95.5 94.8 96.0 99.4 95.3 95.9 99.1 97.4 99.1 99.6 100.0 Table 10: Application execution time \nratio (versus no scheduling) for List and Pseudo-Adaptive runs, geometric mean across six bench\u00ad marks. \nFilter aes bh linpack power voronoi scimark LS 2.5 1.0 1.2 53 1.0 136 t =0 5.1 1.3 2.7 132 1.1 <0 t =5 \n5.1 1.6 2.7 118 1.0 <0 t =10 5.5 2.1 3.0 130 <0 299 t =15 5.1 3.1 0.4 156 1.5 175 t =20 7.2 0.7 3.7 26 \n1.5 502 t =25 6.0 4.7 3.7 164 1.0 207 t =30 8.5 4.7 0.6 142 0.2 <0 t =35 13.1 4.8 6.3 280 1.3 427 t =40 \n11.7 5.0 0.4 305 1.5 350 t =45 16.0 5.0 0.5 25 1.7 465 t =50 19.8 <0 0.5 3 1.8 15 NSA/NSC 3.5 1.4 2.9 \n54 1.7 617 Table 11: Percent of compile time recovered by each iteration of the application, and application \ntime as percentage of compile time for NS.  Filter aes bh linpack power voronoi scimark LS 4.1 <0 <0 \n<0 <0 1900 t =0 4.3 0.6 5.1 76 <0 250 t =5 4.6 <0 <0 <0 <0 1484 t =10 4.8 <0 <0 <0 <0 119 t =15 3.9 <0 \n<0 <0 <0 <0 t =20 5.4 <0 <0 64 <0 <0 t =25 5.0 <0 <0 29 <0 689 t =30 2.8 0.3 <0 20 <0 1108 t =35 0.0 \n<0 <0 41 <0 2542 t =40 <0 <0 <0 37 <0 8991 t =45 0.0 <0 <0 24 <0 68 t =50 0.0 <0 <0 46 <0 7365 NSA/NSC \n7.0 36.7 8.5 1113 20.1 1918 Table 12: Percent of Pseudo-Adaptive compile time recovered by each iteration \nof the application, and application time as percent\u00adage of compile time for NS.  heuristics. The prediction \nrates of their approach resulted in a miss rate of 20% as compared with the 25% miss rate obtained using \nthe best hand-crafted heuristics existing at the time. Our learning method\u00adologies are similar, but there \nare important differences. First, they began with a rich set of hand-crafted heuristics from which form \nfea\u00adtures. On the other hand, we had no pre-existing heuristics from which to draw features. Second, \ntheir technique made it inherently easy to determine a label for their training instances. The optimal \nchoice for predicting a branch was easily obtained by instrumenting their bench\u00admarks to observe each \nbranch s most likely direction. We obtained our labels using a simpli.ed model of our target processor, \nwhich is imprecise as previously mentioned. Because our measurements are imprecise, it is impossible \nto determine the optimal choice of whether to schedule or not to schedule. Monsifrot et al. [14] use \na classi.er based on decision tree learning to determine which loops to unroll. Like in Calder et al. \n[4], there were many hand-coded heuristics from which to draw features. In contrast to our approach and \nCalder s, they obtain labels by using tim\u00ading measurements from a real machine. For each loop, they measure \nthe effect of unrolling and not unrolling that particular loop. If the effect of unrolling is bene.cial \nabove some threshold, they create a positive training example pertaining to the loop. If unrolling the \nloop causes a degradation in performance, a negative training example is generated from the loop. A group \nof researchers at MIT used genetic algorithms to tune heuristic priority functions in three compiler \noptimizations [18]. They generated, at random, expressions for a priority function for a spe\u00adci.c compiler \noptimization, and formed an initial population for a ge\u00adnetic algorithm. They performed crossovers and \nmutations by mod\u00adifying the expressions with relational and/or real-valued functions of random expressions. \nThey derived priority functions for these tasks: hyperblock selection, spilling in register allocation, \nand data prefetch\u00ading. Their generated heuristics outperformed hand-crafted ones on an architectural \nsimulator. (However, simply by producing 399 heuris\u00adtics at random and choosing the best they were able \nto outperform the hand-crafted heuristics.) Iterating the genetic programming produced a signi.cantly \nbetter result only for the spilling priority function in register allocation, and it stabilized to the \nbest performing genomes in a few iterations. Unsupervised learning, such as genetic program\u00adming, has \ntwo advantages over our technique: in the learning process it uses measured rather than simulated execution \ntimes, and it does not require a timing simulator. However, unsupervised learning is typically more complex, \nand the resulting functions are often more opaque. Also, this genetic programming work took days of CPU \ntime to derive a heuristic, whereas our supervised learning procedure com\u00adpletes in seconds (once we \nhave developed the training instances). Cooper et al. [7] use genetic algorithms to solve the compilation \nphase ordering problem. They were concerned with .nding good compiler optimization sequences that reduced \ncode size. Unfortu\u00adnately, their technique is application-speci.c. That is, a genetic algo\u00adrithm has \nto retrain for each program to decide the best optimization sequence for that program. The genetic algorithm \nbuilds up chromo\u00adsomes pertaining to different sequences of optimizations and adapts these for each individual \nprogram. Mutations can involve adding new optimizations into the sequence or removing existing ones from \nthe sequence. Their technique was successful at reducing code size by as much as 40%. We previously reported \n[15] results on generating a priority func\u00adtion in instruction scheduling. Using supervised learning, \nwe gener\u00adated preference functions that determined the preferred instruction to schedule next from a \npair of instructions (in the LS algorithm). Our conclusion was that machine learning could .nd, automatically, \nquite competent priority functions for local instruction scheduling heuris\u00adtics. In later work [13, 12] \nwe had some success applying reinforce\u00adment learning to the same problem.  6. CONCLUSIONS Choosing when \nto apply potentially costly compiler optimizations is an important open problem. We consider here the \ncase of instruction scheduling, with the possible choices being a traditional list scheduler (LS) and \nno scheduling (NS). Since many blocks do not bene.t from scheduling, one can obtain most of the bene.t \nof scheduling by apply\u00ading it to a subset of the blocks. What we demonstrated here is that it is possible \nto induce a function that is competent at making this choice: we obtain almost all the bene.t of LS at \nless than 1/4 of the cost. On the way to this result we found that it helped to reduce noise: to remove \ntraining instances whose cost under different schedulers is within a chosen threshold value, i.e., not \ndifferent enough to provide a good signal on which to train. Interestingly, this instance .lter\u00ading improved \nboth the ef.ciency and the effectiveness of our induced function. Sometimes (perhaps only rarely) it \nis bene.cial to perform instruc\u00adtion scheduling in a JIT, depending on how long the program runs, etc. \nIf it is rarely worthwhile, that only emphasizes the need for our heuris\u00adtic to decide when to apply \nit. The general approach we took here should apply in other JIT situations. Of course all we have demon\u00adstrated \nrigorously is that it works for one Java compilation system. If the JIT is adaptive, applying optimization \nonly to hot methods, then .ltering is less effective and may not be worthwhile (instruction scheduling \nappears less helpful in general in the adaptive case, though it is not clear from our results why this \nis so). We found supervised learning to work excellently for this task. Thus, beyond achieving good performance \non the task, we obtain the additional bene.ts of a simple cheap learning algorithm that produces understandable \nheuristics. As with any machine learning technique, devising the appropriate features is critical. Choosing \nwhether to ap\u00adply an instruction scheduler turns out to require only simple, cheap\u00adto-compute features. \nMore complex compiler optimizations, such as redundancy elimination, almost certainly need more complex \nfeatures to use in deciding if the optimization is likely to be worthwhile, but we hope that this positive \nexperience will inspire success on harder problems. 7. ACKNOWLEDGMENTS We thank Amy McGovern for coding \nthe simulator and for prior work in this domain. We are especially grateful to IBM Research for sup\u00adplying \nthe Jikes RVM system to the research community. This mate\u00adrial is based upon work supported by the National \nScience Foundation under grant number CCR-0085792. Any opinions, .ndings, conclu\u00adsions, or recommendations \nexpressed in this material are those of the authors and do not necessarily re.ect the views of the NSF. \n 8. REFERENCES [1] B. Alpern, C. R. Attanasio, J. J. Barton, M. G. Burke, P.Cheng, J.-D. Choi, A. Cocchi, \nS. J. Fink, D. Grove, M. Hind, S. F. Hummel, D. Lieber, V. Litvinov, M. F. Mergen, T. Ngo, J. R. Russell, \nV. Sarkar, M. J. Serrano, J. C. Shepherd, S. E. Smith, V. C. Sreedhar, H. Srinivasan, and J. Whaley. \nThe Jalape no virtual machine. IBM Systems Journal, 39(1), Feb. 2000. [2] M. Arnold, S. Fink, D. Grove, \nM. Hind, and P. F. Sweeney. Adaptive optimization in the Jalape no JVM. In ACM SIGPLAN Conference on \nObject-Oriented Programming Systems, Languages, and Applications (OOPSLA 2000), Minneapolis, MN, Oct. \n2000. [3] M. Arnold and B. G. Ryder. A framework for reducing the cost of instrumented code. In SIGPLAN \nConference on Programming Language Design and Implementation, pages 168 179, 2001. [4] B. Calder, D. \nGrunwald, M. Jones, D. Lindsay, J. Martin, M. Mozer, and B. Zoren. Evidence-based static branch prediction \nusing machine learning. ACM Transactions on Programming Languages and Systems, 19(1):188 222, January \n1997. [5] C. Chekuri, R. Johnson, R. Motwani, B. Natarajan, B. Rau, and M.Schlansker. Pro.le-driven instruction \nlevel parallel scheduling with application to super blocks. In Proceedings of the 29th International \nSymposium on Microarchitecture, pages 58 67, 1992. [6] W. W. Cohen. Fast effective rule induction. In \nProceedings of the Twelfth International Conference on Machine Learning, Lake Tahoe, CA, Nov. 1995. \n[7] K. D. Cooper, P. J. Schielke, and D. Subramanian. Optimizing for reduced code space using genetic \nalgorithms. In Workshop on Languages, Compilers, and Tools for Embedded Systems, pages 1 9, 1999. [8] \nJ. R. Ellis. Bulldog: A Compiler for VLIW Architectures. PhD thesis, Yale, Feb. 1985. [9] J. A. Fisher. \nTrace scheduling: a technique for global microcode compaction. IEEE Transactions on Computers, 30:478 \n490, July 1981. [10] M. R. Garey and D. S. Johnson. Computers and Intractability: A Guide to the Theory \nof NP-Completeness. W.H. Freeman and Co., San Francisco, CA, 1979. [11] P. B. Gibbons and S. S. Muchnick. \nEf.cient instruction scheduling for a pipelined architecture. In Proceedings ACM SIGPLAN 86 Conference \non Programming Language Design and Implementation, pages 11 16, 1986. [12] A. McGovern, E. Moss, and \nA. G. Barto. Building a basic block instruction scheduler with reinforcement learning and rollouts. Machine \nLearning, 49(2/3):141 160, 2002. [13] A. McGovern and J. E. B. Moss. Scheduling straight-line code using \nreinforcement learning and rollouts. In S. Solla, editor, Advances in Neural Information Processing Systems \n11, Cambridge, MA, 1998. MIT Press. [14] A. Monsifrot and F. Bodin. A machine learning approach to automatic \nproduction of compiler heuristics. In Tenth International Conference on Arti.cial Intelligence: Methodology, \nSystems, Applications, AIMSA, pages 41 50, September 2002. [15] J. E. B. Moss, P. E. Utgoff, J. Cavazos, \nD. Precup, D. Stefanovi\u00b4c, C. Brodley, and D. Scheeff. Learning to schedule straight-line code. In Proceedings \nof Neural Information Processing Systems 1997 (NIPS*97), Denver CO, Dec. 1997. [16] S. S. Muchnick. Advanced \nCompiler Design &#38; Implementation. Morgan Kaufmann Publishers, Inc., San Francisco, CA, 1997. [17] \nStandard Performance Evaluation Corporation (SPEC), Fairfax, VA. SPEC JVM98 Benchmarks, 1998. [18] M. \nStephenson, S. Amarasinghe, M. Martin, and U.-M. O Reilly. Meta optimization: Improving compiler heuristics \nwith machine learning. In Proceedings of the ACM SIGPLAN 03 Conference on Programming Language Design \nand Implementation, San Diego, Ca, June 2003. Association of Computing Machinery. [19] C. Young and M. \nSmith. Better global scheduling using path pro.les. In Proceedings of the 28th International Symposium \non Microarchitecture, pages 199 206, Nov. 1995.  \n\t\t\t", "proc_id": "996841", "abstract": "Instruction scheduling is a compiler optimization that can improve program speed, sometimes by 10% or more, but it can also be expensive. Furthermore, time spent optimizing is more important in a Java just-in-time (JIT) compiler than in a traditional one because a JIT compiles code at run time, adding to the running time of the program. We found that, on any given block of code, instruction scheduling often does not produce significant benefit and sometimes degrades speed. Thus, we hoped that we could focus scheduling effort on those blocks that benefit from it.Using supervised learning we induced heuristics to predict which blocks benefit from scheduling. The induced function chooses, for each block, between list scheduling and not scheduling the block at all. Using the induced function we obtained over 90% of the improvement of scheduling every block but with less than 25% of the scheduling effort. When used in combination with profile-based adaptive optimization, the induced function remains effective but gives a smaller reduction in scheduling effort. Deciding when to optimize, and which optimization(s) to apply, is an important open problem area in compiler research. We show that supervised learning solves one of these problems well.", "authors": [{"name": "John Cavazos", "author_profile_id": "81100096445", "affiliation": "University of Massachusetts, Amherst, MA", "person_id": "PP39026940", "email_address": "", "orcid_id": ""}, {"name": "J. Eliot B. Moss", "author_profile_id": "81406593781", "affiliation": "University of Massachusetts, Amherst, MA", "person_id": "PP39023964", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/996841.996864", "year": "2004", "article_id": "996864", "conference": "PLDI", "title": "Inducing heuristics to decide whether to schedule", "url": "http://dl.acm.org/citation.cfm?id=996864"}