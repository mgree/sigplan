{"article_publication_date": "06-09-2004", "fulltext": "\n Precise and Ef.cient Static Array Bound Checking for Large Embedded C Programs Arnaud Venet Kestrel \nTechnology NASA Ames Research Center Moffett Field, CA 94035 arnaud@email.arc.nasa.gov ABSTRACT In this \npaper we describe the design and implementation of a static array-bound checker for a family of embedded \nprograms: the .ight control software of recent Mars mis\u00adsions. These codes are large (up to 280KLOC), \npointer intensive, heavily multithreaded and written in an object\u00adoriented style, which makes their analysis \nvery challenging. We designed a tool called C Global Surveyor (CGS) that can analyze the largest code \nin a couple of hours with a pre\u00adcision of 80%. The scalability and precision of the analyzer are achieved \nby using an incremental framework in which a pointer analysis and a numerical analysis of array indices \nmutually re.ne each other. CGS has been designed so that it can distribute the analysis over several \nprocessors in a cluster of machines. To the best of our knowledge this is the .rst distributed implementation \nof static analysis algo\u00adrithms. Throughout the paper we will discuss the scalability setbacks that we \nencountered during the construction of the tool and their impact on the initial design decisions. Categories \nand Subject Descriptors F.3.2 [Logics and Meanings of Programs]: Semantics of Programming Languages Program \nAnalysis General Terms Algorithms, Languages, Veri.cation  Keywords Abstract interpretation, program \nveri.cation, pointer anal\u00adysis, array-bound checking, di.erence-bound matrices 1. INTRODUCTION It is \nwell-known that runtime errors plague the develop\u00adment of large mission-critical software. In 1996, the \nexplo\u00adsion of Ariane 501 shortly after launch was due to an over\u00ad.ow in an arithmetic conversion. This \nfailure cost over $500 Permission to make digital or hard copies of all or part of this work for personal \nor classroom use is granted without fee provided that copies are not made or distributed for pro.t or \ncommercial advantage and that copies bear this notice and the full citation on the .rst page. To copy \notherwise, to republish, to post on servers or to redistribute to lists, requires prior speci.c permission \nand/or a fee. PLDI 04, June 9 11, 2004, Washington, DC, USA. Copyright 2004 ACM 1-58113-807-5/04/0006 \n...$5.00. Guillaume Brat Kestrel Technology NASA Ames Research Center Moffett Field, CA 94035 brat@email.arc.nasa.gov \nmillions to the European space program. Classical veri.\u00adcation techniques based on development process, \ncode re\u00adviewing and testing were unable to detect that defect. This over.ow could have been detected \nby employing static anal\u00adysis techniques which can automatically inspect the text of a program and check \nthe safety of all operations. As a matter of fact, the failure of Ariane 501 gave birth to a commer\u00adcial \nstatic analysis tool called PolySpace Ada Veri.er [22]. This tool can perform precise static analysis \nof large Ada programs (over 1 MLOC) and .nd runtime errors. In a pre\u00advious article [5], we have reported \nour experience with C Veri.er (the C version of Ada Veri.er) on real NASA soft\u00adware. Unfortunately, we \nfound that C Veri.er does not scale as well as its Ada counterpart. In short, we had to limit our analysis \nto code pieces of 20to 40KLOC and we obtained 20% of warnings after 8 to 12 hours of analysis. This level \nof performance was not enough to convince NASA software developers to adopt the technology. We analyzed \nthe reasons for these limitations and we de\u00adcided to address them by prototyping our own static analysis \ntool called C Global Surveyor (CGS). We believe that it is extremely hard to build a static analyzer \nthat works well for any C programs. The precision of a static analysis tool is measured in terms of the \npercentage of operations in the program that can be decided as safe (or unsafe). Precision is the main \nmetric for judging the quality of a static ana\u00adlyzer. Therefore, designing a static analyzer for any \ntype of C programs forces the tool implementer to make tradeo.s that sacri.ce scalability. We extensively \nexperienced with PolySpace C Veri.er on a variety of NASA programs and we observed that precision remained \nconsistently around 80%. However, there was a huge discrepancy between execution times, from a couple \nof hours to days. Our driving philos\u00adophy is that designing a tool for speci.c coding style and software \narchitecture allows us to make di.erent tradeo.s that optimize execution time for the software family \nwe tar\u00adget. Cousot et al. [3] used a similar approach to build a static analyzer that is specialized \nfor software developed by Airbus; it can analyze 75,000 lines of C code without producing any warnings. \nOur goal with CGS is not as ambitious. Whereas the software analyzed in [3] is safety-critical, single-threaded \nand uses a very restricted subset of C, we have to analyze programs that are multithreaded and use the \nfull power of pointer arithmetic. Our main purpose is to achieve a level of precision comparable to that \nof PolySpace C Veri.er with much lower execution times, since in our case this is the de\u00adcisive factor \nfor having the technology adopted by missions at NASA. C Global Surveyor checks for one type of runtime \nerrors: out-of-bounds array accesses. This is probably the most critical category of runtime errors because \nit silently corrupts the memory, causing nondeterministic behaviors during the mission. CGS is specialized \nfor the NASA soft\u00adware following the Mars Path Finder (MPF) legacy, which we call the MPF family. The \n.ight software for the Deep Space One mission (DS1) and the Mars Exploration Rover mission (MER) all \nbelong to the MPF family. The programs of the MPF family share a unique feature in the .eld of embedded \napplications: they are written in an object-oriented style. This means that these programs con\u00adtain a \nmyriad of small generic functions which are passed pointers to the segments of data on which they shall \noper\u00adate. This has two consequences on the structure of the an\u00adalyzer. First, context-sensitivity should \nbe enabled in order to distinguish between hundreds of calls to the same func\u00adtion. Second, interprocedural \npropagation should be very e.cient. All decisions made in the design of CGS originate from these two \nobservations. We do not claim that the ar\u00adchitecture of CGS represents the optimal solution to this problem. \nThe experiments showed that some of our choices were justi.ed and some others were questionable. This \npa\u00adper should be seen as the critical report of a practical expe\u00adrience in implementing a large scale \nstatic analyzer The paper is organized as follows. In Sect. 2 we intro\u00adduce the abstract interpretation \nframework underlying the architecture of CGS. In particular we de.ne the semantic model of the memory \nin which the symbolic information pro\u00adduced by the pointer analysis interacts with the numerical invariants \nproduced by the .ow-sensitive analysis of loops. Section 3 de.nes the abstract semantics of memory accesses \nand the generation of semantic equations that are used dur\u00ading the interprocedural propagation phase. \nIn Sect. 4 we describe the architecture of CGS and our implementation choices. Section 5 summarizes the \nexperimental results ob\u00adtained for the MPF and DS1 codes on a cluster of PC work\u00adstations. We give a \ncritical interpretation of these results with respect to the design decisions. We discuss related work \nin Sect. 6 and we end the paper with concluding re\u00admarks.  2. ABSTRACT INTERPRETATION FRAME-WORK Abstract \nInterpretation [7, 8, 10] is a theoretical frame\u00adwork for the systematic construction of provably correct \nstatic analyzers. Classically, the abstract interpretation of a program consists of attaching to each \nprogram point an ab\u00adstract memory con.guration that is a conservative approxi\u00admation of the actual memory \ncon.guration for all executions of the program that reach that point. This information can be automatically \ninferred by associating an abstract seman\u00adtic transformer to each basic operation of the program and \ncomputing the composition of these transformers along all possible executions paths in the control-.ow \ngraph. This is achieved in practice by constructing a system of abstract semantic equations that describes \nthe .ow of information in the program and by applying appropriate .xpoint algo\u00adrithms for computing the \nsolution of the system, usually with the help of widening/narrowing operators in order to ensure termination \nand/or rapid convergence. In our case we are interested in discovering all possible addresses that can \n.ow through each pointer variable in the program. Thus, we can check whether every memory read or write \noperation of the program occurs within the bounds of a memory block. We are not interested in check\u00adingwhether \napointer is NULL or contains an undetermined value. This is a di.erent problem that has to be treated \nwith a separate analysis. Therefore, in our abstract seman\u00adtic model the denotation of a pointer always \ncontains NULL and any undetermined value. We can nevertheless .ag an illegal memory access with certainty \nwhenever our analysis discovers an empty points-to set. The C language authorizes creating a pointer \nto an object inside a compound data structure, for example to the ele\u00adment of an array. This construct \nis heavily used in the MPF and DS1 codes, since data are organized in large structures which are modi.ed \nvia pointer references passed to generic functions. Therefore, our abstract memory model should represent \nreferences as a triple (a,p,s)where a is the ad\u00address of a memory block, p is an access path into the \nblock and s is the size of the block. An address is either the ad\u00address of a variable &#38;A, a constant \ncharacter string string\u00a3 that appears in the program at the program location e,or the dynamic allocation \nof a block malloc\u00a3 at the program lo\u00adcation e. Our model does not distinguish between instances of a \nmalloc in a loop simply because this situation never occurs in the class of programs that we are considering, \nal\u00adthough techniques exist that can cope with this problem [29, 26]. Without access path information \nit is impossible to per\u00adform any precise array bound checking. We could use the type information contained \nin the C program for represent\u00ading access paths symbolically. Unfortunately, the aggressive type casting \nmechanism of C combined to pointer arithmetic ruins this idea. Consider for example the following fragment \nof code: struct MsgHeader { int id; int length; }; struct Msg_X { struct MsgHeader header; Data_X data; \n}; /* Thread 1 */ struct Msg_X *msg = malloc (...); ... sendMsg (Thread_2, msg); ... /* Thread 2 */ struct \nMsgHeader *msg = readMsg (...); if (msg->id == ID_OF_X) { Data_X *data = (DataX *)(msg + 1); ... This \nis in essence how the message passing mechanism for thread communication is implemented in the MPF family. \nAll messages start with the same header which contains an id that uniquely determines the type of the \nmessage. The data are stored right after the header. The actual type of the data is only known after \nthe message id has been read, which explains this seemingly odd construction. This piece of code illustrates \nthe overall object-oriented design of the MPF family software. Messages are considered as objects and \nthis is nothing more than a manual encoding of virtual method dispatch. However, this makes the manipulation \nof symbolic access paths extremely di.cult since we must keep track of the actual layout of structure \ncomponents in memory in order to cope accurately with pointer arithmetic. Our solution to this problem \nconsists of choosing a uniform o.set-based representation of structure components instead of symbolic \naccess paths. A reference is then represented by atriple (a,o,s)where ois an o.set from the beginning \nof the block expressed in bytes. With this numerical model, type casting is no more an issue and becomes \ntransparent for the analysis. All architecture-dependent problems entailed by this representation like \nmemory alignment and padding are completely resolved by the C front-end. Therefore, there is no extra \ncomplexity in implementing this model. For ensuring computability we approximate a set {(ai,oi, si) | \ni . I} of memory references by an abstract memory reference ({ai | i. I},O,S)where O and S are the smallest \nintervals such that .i . I : oi . O &#38; si . S. This corre\u00adsponds to the notion of attribute-independent \nor cartesian approximation [9]. We can gain precision by considering the reduced product [8] between \nthe powerset lattice of addresses and the lattice of intervals. The size of memory blocks is known at \ncompile time for the address of a static memory block, i.e. an address of type &#38;A or string. Wedenoteby \nsz(a) the size of the block at address a.If a is the address of a dynamically allocated block we set \nsz(a)=[-8,+8]. The reduced product consists of re.ninig the expressiveness of each lattice by bringing \ninformation from the other one. In practice this is performed by applying a reduction opera\u00adtion s de.ned \nas follows: C C)) s(A,O,S)= {a . A| sz(a) . S},O,Sn. sz(a) a.A The e.ect of this operation is to remove \nspurious references and reduce the size range, which results in better accuracy. The reduction should \nalways be performed on an abstract memory reference before any operation is applied to it. In practice \nreduction turned out to be very important, because in many cases the numerical information was too coarse \nto represent the size precisely. An abstract memory con.guration is thereby a couple (E,H)where E is \nan abstract environment mapping each local pointer variable of a function to an abstract memory reference \nand each local integer variable to an interval, and H is an abstract heap. An abstract heap is a set \nof points-to relations ((a,O) ',O',S)) where a,a . (a' are addresses and O,O',S are intervals. Such a \nrelation expresses that there may be references in the block a within the range of o.sets O to the elements \nwithin the range of o.sets O' in the block ' a, the size of which lies in the interval S.Furthermore, \nwe impose that there are no two distinct points-to relations in H with the same addresses a and a'.We \nperform two ad\u00additional approximations that are crucial for the tractability of this model: 1. Abstract \nenvironments are .eld-insensitive, i.e. we do not distinguish between the values of .elds within a compound \nlocal variable. 2. Local variables that are address-taken (i.e. modi.ed through a pointer reference) \nare globalized, i.e. they are represented in the abstract heap H. This means  that we cannot distinguish \nthe value of such a variable between di.erent execution contexts. These approximations ensure that the \ndomain of an abstract environment E only contains variable names without any o.set information, and that \nany modi.cation of the value of avariable v in E may only be performed by an assignment operation in \nwhich v explicitly appears. Even though local structures and address-taken variables are quite common \nin the MPF family, they rarely concern variables that carry pointers. Thus the impact of these approximations \non the precision is low whereas they greatly simplify the design of the analyzer. As is, the classical \nabstract interpretation framework that assigns an abstract memory con.guration (E,H)ateach control point \nis not applicable to heavily multithreaded pro\u00adgrams, since this requires considering all possible thread \nin\u00adterleavings. A solution would be to use a .ow-insensitive analysis, which can obviously cover all \npossible thread inter\u00adleavings at a low cost. However, the array bound checking absolutely requires precise \nloop invariants, which cannot be obtained without .ow-sensitivity. Our solution consists of using a mixed \nframework similar to that of [26] in which the abstract environment E is computed in a .ow-sensitive \nway whereas the abstract memory heap H is constructed in a .ow-insensitive way. More precisely, let H \nbe an abstract heap that is a conser\u00advative approximation of all possible heaps that can be gen\u00aderated \nby the program at any point of any execution. We perform a .ow-sensitive analysis by keeping the second \ncom\u00adponent of the abstract memory con.gurations equal to H. In this case we do not have to consider any \nthread interleav\u00ading at all since the variables in the domain of the abstract environments are thread-local. \nWe simply analyze the pro\u00adgram as if it were sequential, the initial states being given by all the thread \nentry points. During the analysis memory reads are always performed on H and memory writes are never \ntaken into account. More precisely, if p=*q is a read operation that fetches a pointer from the heap, \nwe get the abstract memory reference (A,O,S) associated to q at this stage of the computation. The result \nof the read operation is the join of all memory references ({a'},O',S')for which ' there exists a points-to \nrelation ((a,O'') . (a,O',S')) in H such that a . A&#38; On O'' = \u00d8. At the end of the analysis, we consider \nall memory write operations of the program. For each such operation *p =q that may carry a pointer we \nretrieve the abstract memory reference (A,O,S) associated to p and the abstract memory reference (A',O',S') \nassociated to q at this point. For each a in A and each a in A', we generate a points-to relation ((a,O) \n',O',S')). . (aWe gather all the points-to relations generated this way to form a new abstract heap H'.The \nabstract heap H' satis.es two properties: 1. H' is a conservative .ow-insensitive approximation of all \nactual heaps of the program. 2. H' re.nes H, denoted by H' . H:  for all ((a,O1). (a',O1',S1)) in H', \nthere is a points-to relation ((a,O2) ' . (a,O2',S2)) such that O1 . O2,O1'. O2'&#38; S1 . S2. This provides \nus with a process for incrementally re.ning the abstract heap. We start with a coarse .ow-insensitive \napproximation of the heap H0 and we construct a decreas\u00ading sequence H0 . H1 . \u00b7\u00b7\u00b7 . Hn of abstract heaps \nwith respect to the re.nement order. We can use the pointwise extension of the narrowing of intervals \nto de.ne a narrow\u00ading operation over abstract heaps. We can then automate this process, using the narrowing \nto enforce stabilization. Automatic stabilization is not implemented in the current version of CGS, the \nuser must explicitly give the number of re.nement steps that shall be computed. To illustrate this mechanism, \nconsider for example a pro\u00adgram working on an array A of two pointers, a pointer vari\u00adable P and two \ninteger variables I and J, and made of three simple threads de.ned as follows: void task1() { void task2() \n{ void task3() { A[0] = &#38;I; P = A[0]; A[1] = &#38;J; }}} Imagine that we are provided with a conservative \n.eld-in\u00adsensitive approximation H0 of the memory graph as follows: .. . (&#38;A, [-8, +8], [8, 8]).(&#38;I, \n[-8, +8], [4, 4]), . .. .. (&#38;A, [-8, +8], [8, 8]).(&#38;J, [-8, +8], [4, 4]), H0 = . (&#38;P, [-8, \n+8], [4, 4]).(&#38;I, [-8, +8], [4, 4]), . .. .. (&#38;P, [-8, +8], [4, 4]).(&#38;J, [-8, +8], [4, 4]) \nassuming pointers and integers occupy four bytes in mem\u00adory on the architecture considered. After one \nstep of itera\u00adtion, the elements at indices 0and 1 of array A are entirely determined, however the value \nof P is computed from the points-to information contained in H0. Therefore we obtain (&#38;P, [0, 0], \n[4, 4]).(&#38;I, [-8, +8], [4, 4]), the following memory graph: . . H1 = . (&#38;A, [0, 0], [8, 8]).(&#38;I, \n[0, 0], [4, 4]), . . (&#38;A, [4, 4], [8, 8]).(&#38;J, [0, 0], [4, 4]), . . . .. .. .. (&#38;P, [0, \n0], [4, 4]).(&#38;J, [-8, +8], [4, 4]) Note that the o.set in the memory block &#38;P has been solved \nbecause the assignment P = A[0] writes its lefthand side at the o.set 0. After one more iteration step, \nthe assignment to P in task 2 can be precisely solved, since the memory layout of A has been completely \ndetermined at the previous iteration step. We .nally obtain: . . . (&#38;A, [0, 0], [8, 8]).(&#38;I, \n[0, 0], [4, 4]), . H2 = (&#38;A, [4, 4], [8, 8]).(&#38;J, [0, 0], [4, 4]), . . (&#38;P, [0, 0], [4, 4]).(&#38;I, \n[0, 0], [4, 4]) It now remains the problem of bootstrapping the itera\u00adtive process, i.e. obtaining the \n.rst approximation H0.We .rst used Steensgaard s analysis [24] enhanced with Das one-level .ow edges \noptimization [13]. However the result\u00ading abstract heap was too coarse, and there were spurious points-to \nrelations introduced at that stage that remained in all subsequent re.nement steps. One source of impreci\u00adsion \nwas due to the way message queues are allocated. The unique malloc call that creates a queue is nested \nwithin several function calls. Since in our memory model alloca\u00adtions can only be distinguished by the \nsyntactic location of the corresponding malloc, all message queues were merged, resulting in an unrecoverable \nloss of precision. Adding an option to CGS allowing to inline the corresponding functions solved this \nproblem. The idea is to treat isolated sources of imprecision manually in this way rather than complicating \nthe pointer analysis in order to cover all special cases. The drawback is that this kind of instrumentation \ncan only be done by a high-end user who perfectly knows the internals of the analysis and how to cope \nwith this kind of situation (see also [3] for a discussion of this issue). A substantial amount of the \nremaining spurious points-to relations was due to brutal uni.cation operations in Steens\u00adgaard s analysis \ncaused by pointers stored in global vari\u00adables. The solution consisted of extending Das analysis in order \nto be able to handle n-level .ow edges without sac\u00adri.cing e.ciency. We believe that scalable versions \nof An\u00addersen s analysis [2] could have been considered as well for the bootstrap [18]. We unfortunately \ndid not have the time to implement an inclusion-based analysis and compare the results. This ends the \npresentation of the abstract interpretation framework implemented in CGS. We now have to present the \ndetails of the abstract semantic equations.  3. ABSTRACT SEMANTICS The symbolic and numerical parts \nof an abstract memory reference are independent, which means that we can com\u00adpute these two pieces of \ninformation separately. We just need to perform a reduction operation s whenever there is a context change \n(function call) or an interaction with the abstract heap (memory read). The choice of performing a cartesian \napproximation for the abstract memory references was mainly motivated by this simplifying assumption \nin the abstract semantics. We generate two separate sets of semantic equations for each function in the \nprogram, one for the symbolic part in the form of inclusion constraints between points-to sets, the second \nas a system of numerical constraints between o.set and size variables. The resolution of these equations \nfollows the call graph by propagating call contexts made of points-to sets and intervals. The symbolic \nand numerical systems as\u00adsociated to a function f are solved separately for all possible call contexts \nof f (depending on whether context-sensitivity is enabled for this function or not). The resolution of \nthese two systems of equations is interleaved, interactions occur\u00adring whenever some information is retrieved \nfrom the en\u00advironment, i.e. by a memory read. In this case we have to combine the numerical and symbolic \ninformation in or\u00adder to query the memory graph H used at this step of the resolution. 3.1 Points-to \nInclusion Constraints Given a function f of the program, we associate a metava\u00adriable Ap to each local \nvariable p of f that may carry a pointer (either a pointer variable itself or a compound vari\u00adable with \npointer-valued .elds). These metavariables repre\u00adsent the .rst component of an abstract memory reference, \ni.e. a set of symbolic addresses. Following the model de.ned in [26] we associate an anchor metavariable \nA\u00a3 to each loca\u00adtion e of a memory read operation or a function call that may return a pointer. The metavariable \nA\u00a3 represents the set of addresses returned by the read operation or the function call. We similarly \nassign a special anchor metavariable Ax@f to each formal parameter x of f that may carry a pointer. This \nanchor denotes the points-to set of the argument passed to the function and is used during interprocedural \npropagation. Following Andersen s model [2] we use inclusion constraints of the form Ap .Aq to relate \nthe metavariables. The generation of inclusion constraints is quite straight\u00adforward. For all assignments \np=q, p=q+n (pointer arithmetic) or p = (T *)q (type cast), we generate a con\u00adstraint Ap . Aq . For all \nmemory read operation p=*q or function call p = f (...) at a location e in the program we generate a \nconstraint Ap . A\u00a3 and we record a semantic operation read(A\u00a3,Aq ) which is used during interprocedu\u00adral \npropagation for querying the abstract memory graph. A memory write operation *p =q is not assigned an \ninclu\u00adsion constraint, it is simply assigned a semantic operation write(Ap ,Aq ) which is used at the \nend of an analysis pass to generate a new abstract heap, as described in the previous section. Similarly \na return p statement is recorded sepa\u00adrately as return(Ap ) and is used for the construction of the transformers \nin the backward propagation phase described in Sect. 3.3. We must also add the constraints corresponding \nto the implicit binding relations between formal and actual parameters as follows: Ax . Ax@f , for all \nformal parameter x of f. The resolution of these constraints di.ers from Ander\u00adsen s algorithm [2] since \nread operations retrieve data from the abstract memory graph H and require some information about the \no.set at which the memory block is read. Our al\u00adgorithm consists of a local .xpoint iteration that computes \na set of symbolic addresses for each metavariable of f and launches the resolution of numerical constraints \non demand whenever a memory read is encountered. For e.ciency the resolution algorithm implemented in \nCGS .rst computes the directed acyclic graph of strongly connected components of the dependency graph \nof the system of inclusion constraints. The iterations are then performed locally on each strongly connected \ncomponent following a weak topological ordering of the metavariables [4]. 3.2 Numerical Constraints \nClassically, when building an abstract interpretation of numerical computations, the abstract semantic \nequations follow the program structure [12]. A loop statement in the body of a function will appear as \na recursive dependency in the equations. Solving the system precisely usually requires computing two \n.xpoint iterations, the .rst one with widen\u00ading the second with narrowing. These calculations should \nbe performed on the whole program, i.e. hundreds of thou\u00adsands lines of C, at each step of the heap re.nement \npro\u00adcess described in the previous section. In practice, we mea\u00adsured that at least .ve global iterations \nover the program are needed to achieve a good level of precision. It was unrealis\u00adtic to perform a full-strength \n.xpoint iteration at each step; it would severely impair the e.ciency of the analyzer. We decided to \n.rst compute a summary of each function of the program by using a relational numerical lattice as described \nin [11]. As for the points-to inclusion constraints, given a function f of the program, we associate \ntwo numerical metavariables Op and Sp to each local variable p of f that may carry a pointer. The metavariables \nOp and Sp represent respectively the o.set and size ranges of the abstract memory reference carried by \nthe variable. We also associate a metavariable In to each integer valued local variable n. Recall that \nlocal variables that are address-taken are globalized and never oc\u00adcur in an abstract environment. We \nalso attach two anchor metavariables O\u00a3 and S\u00a3 to each location e of a memory read/write operation or \na function call that may return a pointer. The metavariables O\u00a3 and S\u00a3 represent respectively the o.set \nand size ranges of the abstract memory reference returned by the operation at that point. We similarly \nat\u00adtach special anchors Ox@f and Sx@f (resp. Ix@f )to each pointer-valued (resp. integer-valued) formal \nparameter x of f.. We could also attach anchor metavariables I\u00a3 to each lo\u00adcation e of a memory read \noperation or a function call that returns an integer. CGS actually has command-line options to generate \nsuch anchors. The representation of integer val\u00adues in the abstract heap is identical to that of pointers, \ni.e. it consists of mapping a memory location (a,O,S) to an in\u00adterval [a,b]. Some extra care is required \nwhen reading an integer from the heap in order to ensure that the o.set of the read operation is aligned \nwith the o.set of the integer in the memory block, otherwise this would result into re\u00adturning a truncated \nvalue. Similarly we have to make sure that the sizes match, for example if we try to read a byte from \nthe location of an integer, otherwise the results would be inconsistent. We address these issues in a \nvery simple way: whenever we encounter a read operation of an integer of size s from the address a at \nthe o.set O ' and there is a mapping (a,O,S). [a,b] in the abstract heap, we re\u00adturn the interval [a,b] \nif and only if O and S are singletons and O = O ' ,S =[s,s]. We return [-8,+8]otherwise. Surprisingly \nenough, the experiments showed no noticeable gain in precision on the MPF family with this option of \nCGS enabled. Now we need to choose a relational abstract domain for representing relationships between \nthe numerical metavari\u00adables. Consider for example the following function which is representative of \nthe matrix computations performed in the programs of the MPF family: void equate (double *p, double *q, \nint n) { int i; for (i =0; i< n; i++) p[i] = q[i]; } In the abstract syntax tree of this function the \nbody of the loop is represented by the three following statements: a=p +i; b=q +i; c = *b; *a= c; The \nvariables a, b and c are internal names generated by the front-end. If we assume that the size of a double \nis 8 bytes, the exact loop invariant is given by . . Sa = Sp@equate . . 0 = Oa - Op@equate = 8 * In@equate \n- 8 . Sb = Sq@equate . . 0 = Ob - Oq@equate = 8 * In@equate - 8 where we have eliminated all metavariables \nassociated to local integer variables of the function, since they are just used for storing the result \nof intermediate computations. It immediately appears in this simple example that we need general linear \ninequalities in order to be precise. The only abstract domain that is expressive enough for representing \nthis kind of invariants is the lattice of convex polyhedra [12]. Unfortunately, because of the complexity \nof the underlying algorithms this lattice cannot be used for representing rela\u00adtionships between more \nthan 20variables in practice. The functions in the codes of the MPF family can be quite large and use \nmany pointers simultaneously. We found that in some functions more than 30pointers were active in the \nbody of a loop. Moreover, the abstract syntax tree rep\u00adresentation provided by the front-end introduces \nnumerous internal variables since all statements are broken down into a 3-address format. Some numerical \nrelational lattices have been developed recently that showed good promises of scalability [20, 21]. However \nthey are not expressive enough for representing the kind of linear inequalities in which we are interested. \nThey can only express linear inequalities between two variables and the coe.cients of these variables \nmay only be 1 or -1. Our solution consists of modifying the form of our numerical constraints by introducing \nadditional variables so that the overall expressiveness of a system of numerical constraints is kept \nconstant, whereas the class of numerical relations required to achieve this expressiveness is simpler. \nMore precisely, it appears that the main source of com\u00adplexity comes from the byte-based representation \nof o.sets. An array access p[i] is transformed into an arithmetic ex\u00adpression in which we multiply the \nindex by the size of an array element expressed in bytes. We extend the represen\u00adtation of a pointer \np by attaching additional metavariables d1(p),...,dk(p)and u1(p),...,uk(p)for a .xed k.A pair (di(p),ui(p)) \nrepresents an o.set expressed in a di.erent unit than the byte. di(p) is the relative o.set and ui(p)is \nthe base. The actual o.set in bytes denoted by this representa\u00adtion is given by the following formula: \nk Op + di(p) * ui(p) i=1 We call the representation Wp = (Op ,(d1(p),u1(p)),..., (dk(p),uk(p))) a sliding \nwindow.We call Op the base o.set. The associated sliding operation slide(Wp ,d,u) is de.ned as follows: \nslide(Wp ,d,u)= (Op + d1(p) * u1(p),(d2(p),u2(p)),..., (dk-1(p),uk-1(p)),(d,u)) The initial values of \nthe sliding window for metavariables associated to inputs of the function, i.e. the parameters and the \nreturn values of a memory read or a function call, are set to 0except for the base o.set and uk. The \nbase o.set is the one associated to the metavariable and uk is the size of the element pointed to by \nthe variable as it appears in the type inferred by the C front-end. The sliding operation is used for \nhandling a type cast op\u00aderation p =(T*)q. When analyzing this operation we .rst retrieve the range of \nuk(q) from the current system of in\u00adequalities. If it is a singleton and it is equal to the size t of \nT then Wp = Wq ,otherwise Wp = slide(Wq ,0,t). This way uk always represents the size of the element \ncurrently pointed-to by the variable. Whenever a pointer arithmetic operation p = q + n is analyzed, \nthe sliding window Wp is equated to Wq except for dk(p) for which the constraint dk(p)= dk(q)+ In is \ngenerated. Now if we analyze the function equate with sliding windows of size k =2 and the abstract numerical \ndomain of di.erence-bound matrices [20], we obtain the following system of constraints for the loop invariant: \n. . Sa = Sp@equate Sb = Sq@equate . . . Oa = Op@equate Ob = Oq@equate d1(a)= u1(a)=0 d1(b)= u1(b)=0 . \n. . 0 = d2(a) = In@equate - 10 = d2(b) = In@equate - 1 . u2(a)=8 u2(b)=8 We can express the exact loop \ninvariant with a less powerful abstract lattice and more variables. We chose the domain of di.erence-bound \nmatrices [20] (DBMs for short) for expressing numerical constraints be\u00adtween variables. In this domain \na constraint may only have the form x- y = c where c is an integer. The fundamen\u00adtal operation on a DBM \nis the normalization that re.nes constraints by repeated application of the following rule: . x- y = \nc . y- z = c ' . x- z = min(c+ c ' ,c '' ) x- z = c '' . Our choice was motivated by the observation \nthat DBMs have a su.cient expressiveness for our purpose and by the existence of an e.cient quadratic \nalgorithm devised by John\u00adson [6] for the normalization of sparse systems of constraints. We assumed \nindeed that the systems of constraints would be rather sparse, since it would be very unlikely to have \nall variables in a function related at the same time. Our .rst im\u00adplementation used Floyd-Warshall s \nalgorithm [6] for com\u00adputing the normalization operation. The execution times were catastrophic. A simple \nfunction independently manip\u00adulating 20pointer variables within a loop took more than 15 minutes to analyze. \nThe execution time did not change at all when we tried Johnson s algorithm. After a careful inspection \nof the results it appeared that the system of inequalities was always dense, i.e. all variables were \nrelated. Therefore the cubic worst case execution time was always attained. The reason was to be found \nin the way simple range constraints of the form a = x = b are represented. A DBM always contains a dummy \nzero variable Z which has the value 0. Range constraints are translated into constraints of the form \na = x- Z = b. Therefore all variables introduced in a DBM during the analysis become implicitly related \nas soon as a range constraint is involved, in other terms always. Thus completely independent variables \nbecome related from the moment they receive a constant (during initialization for example). This was \na surprising and disappointing result. Our response to this situation was to explicitly pack com\u00adputationally \ndependent variables together, so that the ana\u00adlyzer works on a collection of smaller DBMs. A similar \nsit\u00aduation has been independently reported in [3]. In that work the authors pack variables in small groups \nusing a syntac\u00adtic criterion (all variables that appear within a same state\u00adment). In our case, such \na simple criterion does not work. Pointer variables and loop counters can become related in a nontrivial \nway via the sliding window representation. We could not even use a dependency analysis because the ap\u00adplication \nof the slide operation depends on the range of uk which can only be known during the .xpoint iteration. \nAny dependency analysis performed beforehand would relate all variables of the sliding windows which \nwould still lead to a high workload. Our solution consisted of dynamically computing the de\u00adpendency \nrelation between metavariables during the execu\u00adtion of the analysis. We start with all metavariables \nbeing unrelated and we incrementally merge the DBMs whenever two of their variables become related by \nan operation of the program. We also merge the associated zero variables. We should also take care of \nimplicit dependencies, i.e. the in\u00advisible dependencies between variables which are modi.ed within a \nloop. If we do not consider these relations we lose all relations between array indices and loop counters \nfor ex\u00adample. Therefore we .rst perform a rapid analysis of every loop in order to check the variables \nthat can be modi.ed in the body and we explicitly relate them before analyzing the loop. We are then \nable to infer all invariants that can be expressed with our abstraction. The function that took 15 minutes \nwith the classic DBM domain could now be ana\u00adlyzed in about 10seconds. Thedomain of adaptiveDBMsthatwehaveconstructed \nin that way is an order of magnitude of complexity beyond the original one. Fortunately it can be simply \ndescribed as an instance of a co.bered domain [27, 28]. Co.bered domains were initially introduced to \nconstruct complex do\u00admains for pointer analysis. They enable the manipulation of dependent abstract domains, \ni.e. families of abstract do\u00admains indexed by the elements of a lattice. The domain of adaptive DBMs \nis exactly a co.bered domain: the indexing lattice is the set of all partitionings of the set of variables \nordered by the re.nement relation, and the abstract domain associated to one partitioning of the variables \nis the prod\u00aduct of the family of DBM domains based upon each set in the partitioning. We measured that \nthe average size of a partition of correlated variables was .ve elements. It would actually be an interesting \nexperiment to use convex polyhe\u00addra instead of DBMs in the co.bered domain, since .ve is a tractable \ndimension for polyhedra, and compare the gain in precision. 3.3 Interprocedural Propagation Function \npointers are widely used in embedded programs for e.ciency reasons. There are plenty of them in codes \nof the MPF family. We realized that a simple control-.ow anal\u00adysis based on Steensgaard s algorithm [24] \nwas su.cient to solve exactly almost all computed calls. As a matter of fact, recent experimental evaluations \nshowed that simple pointer analyses were su.cient to resolve computed calls in most applications [19]. \nWe perform this simple control-.ow anal\u00adysis at the bootstrap prior to launching the interprocedural \npropagation phases. Having all computed calls resolved at bootstrap makes the design of the interprocedural \npropaga\u00adtion algorithms tremendously simpler. In order to achieve e.ciency we break down the interprocedural \npropagation into two phases: 1. A backward propagation phase computes transformers relating the parameters \nof a function with its return value. These transformers are expressed using the do\u00admain of adaptive DBMs. \n 2. A forward propagation phase uses the transformers computed in the previous phase to propagate abstract \nmemory references and ranges using the lattice of in\u00adtervals.  The transformers computed during the \nbackward propaga\u00adtion phase are used during the forward propagation to solve a function call without \nhaving to analyze the body of the called function. The return operations are used at this moment to propagate \nthe constraints between the return value and the arguments of the call. A coarse version of the transformers \nare computed during the bootstrap in order to enable the .rst forward propagation phase. Using a classi\u00adcal \nresolution scheme would have implied iterating over in\u00adterprocedural cycles induced by the two-way dependencies \nbetween a caller and a callee (function parameters/return value), which is completely unrealistic for \nlarge programs. The interprocedural propagation phase of CGS can be context-sensitive. We implemented \ncall-site sensitivity, i.e. the invariants of a function are duplicated depending on the syntactic call \nsite. This level of context-sensitivity is su.\u00adcient for the MPF family, since it handles the common \nsitu\u00adation where a pointer to some part of a big structure (typi\u00adcally an array of double representing \na vector or a matrix) is transmitted to a mathematical function. Context sensitivity is not applied uniformly, \nbut only to functions which have a pointer in their signature, since this is the only situation where \nthe analysis is able to distinguish between di.erent call contexts. Context-sensitivity is extremely \nimportant for precision. Arrays of double, which are the main data struc\u00adtures manipulated by the MPF \nfamily codes, are usually transmitted together with an integer parameter containing the size of the array \nlike in the equate example above. Since the numerical call contexts computed by CGS only are made of \nintervals, they cannot express a relation between the size of the array and the integer parameter. The \nonly way to capture this information is to enumerate all call contexts. Hence, without context-sensitivity \nthe tool would be unable to perform any precise array bound checking on this large family of functions. \n 4. ARCHITECTURE OF CGS The algorithmic core of C Global Surveyor consists of 20,000 lines of C code. \nThe tool is architected around three main phases: 1. The build. This phase computes the points-to con\u00adstraints \nand the numerical inequalities for each func\u00adtion in the program. 2. The bootstrap. This phase performs \na .ow-insens\u00aditive pointer analysis and a coarse context-independent resolution of the numerical inequalities, \nin order to obtain a .rst approximation of all memory accesses. These results are used to construct the \ncall graph and an initial approximation of the heap. 3. The solve. This phase consists of performing \na for\u00adward or backward interprocedural propagation of nu\u00admerical invariants. The results obtained at \nthe end of this phase are used to compute a new abstract heap that re.nes the previous one. This phase \nshould be re\u00adpeated until a satisfactory level of precision has been attained.  There are two additional \nsatellite phases: The initialization. This phase is performed at the very beginning and collects general \ninformation about the program, like the table of global variables, the table of functions, etc. The array-bound \ncheck (abc). This phase can be executed at any time after the bootstrap and checks the safety of all \nmemory accesses from the results of the analysis available at this moment. The precision computed at \nthe end of this phase is the main criterion for deciding whether to continue re.ning the results or stop \nat this point. A very important decision in the initial design of a static analyzer is the choice of \nthe front-end. We chose the Edi\u00adson Design Group s C/C++ front-end [15], a commercial front-end which \nsupports a large variety of C dialects. More\u00adover, the Green Hills compiler [17], which is widely used \nat NASA especially for developing .ight software, is based on this front-end. This is a relevant factor \nwhen considering the application of the tool to other types of programs developed inside NASA. CGS has \nbeen designed from the beginning with a dis\u00adtributed model of computation in mind. Therefore, we tried \nto parallelize all phases for which this makes sense, i.e. the build and the re.nement, the nature of \nthe algorithms used in the bootstrap precluding any attempt of parallelization. We chose the Parallel \nVirtual Machine (PVM) for imple\u00admenting the distribution layer [16]. A major problem con\u00adsisted of storing \nthe artifacts of the analysis and transmitting them to the processes running on parallel. We decided \nto use a relational database for both the storage and the commu\u00adnication between processes of the artifacts, \nthe PVM com\u00admunication mechanism being merely used for sending com\u00admands to processes. We chose the PostgreSQL \n[25] database to work with CGS. The architecture of CGS is illustrated in Fig. 1. Note that each phase \nlaunches a master PVM pro\u00adcess that in turn launches slave processes. Slave processes operate on each \nC .le of the program for the initialization, the build and the array-bound check, whereas they operate \non functions in the solve phase. The bootstrap is the only sequential phase. It is not surprising to \nsay that the cost of communications is the major limiting factor in designing a distributed ap\u00adplication. \nCGS follows the same communication pattern for each job: all needed artifacts are retrieved from the \ndatabase at the beginning of the job, the results are stored in internal memory until the job completes, \nthen the results are writ\u00adten into the database. Two important algorithmic issues in designing the distribution \nof jobs in CGS are the gran\u00adularity (which jobs should be executed in parallel) and the scheduling (in \nwhich order jobs should be executed). The granularity of the build phase is the .le: one PVM process \nis launched for generating the semantic equations of each source .le. The scheduling of tasks in the \nbuild follows a metric calculated during the initialization phase which estimates the complexity of the \n.xpoint computation for each function of the program. Complex .les are executed in priority in order \nto prevent the computation from being blocked by a big job that has been scheduled at the end of the \nworklist. The function-level granularity gave poor results because the analysis time of a single function \nis so short that the database becomes overwhelmed by numerous concurrent accesses. The granularity of \nthe solve phase is the function: one PVM process is launched for computing the invariant of each function. \nThe scheduling follows a weak topological order\u00ading [4] given by the call graph in each way (forward/back\u00adward): \na function is added to the worklist whenever all its  Figure 3: Average analysis times (in seconds) \nper phase for DS1 predecessors have been analyzed. We have limited control on the granularity and scheduling \nof the solve phase because of it is entirely bound to the structure of the call graph. The choice of \nthe next function to schedule from the worklist turned out to be critical. In our .rst experiments we \nused simple heuristics that all led at some point to an almost se\u00adquential execution. Therefore, we should \n.nd a scheduling strategy that tries to maximize the parallelism. We chose a heuristic that consists \nof picking up the next function to schedule from the worklist that has the largest number of calls to \nfunctions which are not in the worklist yet. This heuristic is simple to compute and gives good results \nin terms of distribution.  5. EXPERIMENTAL RESULTS This section shows two types of performance measures \nfor CGS. First, we study the improvement of analysis times (for each phase) in function of the number \nof available CPUs. Note that all CPUs are identical (2.2 MHz with 1 GB of memory). Second, we show how \nthe precision evolves with each solve phase. We distinguish between forward and back\u00adward interprocedural \npropagation in the solve phases. All experiments are conducted using two NASA mission soft\u00adware systems, \ni.e., the .ight software of the Mars Path Finder missions (about 140KLOC) and the Deep Space One mission \n(about 280KLOC). Both are written in C and follow the same architectural and programming principles. \n 5.1 Analysis Time Measures Figure 2 and 3 show the results of the evolution of the av\u00aderage analysis \ntimes of each phase for MPF and DS1 when the number of available processors varies. We distinguish between \nsuccessive solve phases because the input data at each iteration are di.erent. Fig 4 gives a synthetic \nview of these times on a graph plot. These number are averages over   enables (next phase)  database \ncommunications PVM communications phase launching Figure 1: Architecture of C Global Surveyor several \nmeasurements. We sometimes noticed a signi.cant variation between trials which can be imputed to the \nnet\u00adwork load at that moment, since we do not have a dedicated cluster of machines for running our experiments. \nNote that the di.erences between execution times for the bootstrap phase are not relevant since this \nphase is purely sequential. The main conclusion is that contrarily to our expectations, parallelizing \nthe algorithms does not bring a substantial pay\u00ado.. Four CPUs seems to be the threshold beyond which \nthe communication cost counterbalances the parallelization bene.ts. The execution times consistently \ndecrease for all parallel phases except for the array-bound check. The expla\u00adnation is that each slave \nprocess for the array-bound check\u00ading performs very simple computations over a large amount of data (the \nnumerical invariants associated to all memory accesses in a C .le), hence the execution time is dominated \nby the I/O with the database. This phase should de.nitely be made sequential like the bootstrap.  5.2 \nPrecision Measures First, we study the precision in terms of ABC checks. All runs have been performed \nwith context-sensitivity en\u00adabled. We count the number of ABC checks performed and compute the percentage \nof these checks that are not warn\u00adings (i.e. array-bound checks that could have been decided by CGS), \nwhich provides us with a measure of the preci\u00adsion of the analysis. We display the results in Fig. 5. \nNote that we group the solve phases by pairs backward-forward, since a backward interprocedural propagation \nwhich com\u00adputes function transformers is of no use if there is no follow\u00ading forward propagation phase \nthat uses the transformers to analyze function calls more precisely. Two passes seem to be the optimal \ncon.guration. # solves MPF total checks warnings precision 1 37044 13248 64% 2 37044 9216 75% 3 37044 \n9216 75% # solves DS1 total checks warnings precision 1 72152 18878 74% 2 72152 15103 79% 3 72152 15103 \n79% Figure 5: Evolution of the precision after successive pairs of backward-forward solve phases for \nMPF and DS1 We also study the precision in terms of the number of points-to relations in the abstract \nheap computed for the program. As described in Sect. 2, each points-to relation carries three numerical \ninvariants representing the o.sets from the pointer and into the pointee, as well as the size of the \nmemory block being pointed to. In Fig. 6 we dis\u00adplay the evolution of the number of points-to relations \nafter successive pairs of backward-forward solve phases for MPF. We also show the number of imprecise \nnumerical invariants (i.e. intervals which have one of their bounds equal to \u00b18) for the pointer/pointee/size \ninformation respectively. The last column represents the number of alias relations with an imprecise \nnumerical invariant for either the pointer, or the pointee, or the size of the pointed memory block. \nSeconds Seconds 5000 4500 4000 3500 3000 2500 2000 1500 1000 500 0 10000 8000 6000 4000 2000 0 Analysis \ntimes for MPF. CPUs Analysis times for DS1.  CPUs Figure 4: Average analysis times per phase and total \ntime for MPF and DS1 # solves relations pointer pointee size any 1 306 23 71 72 111 2 306 23 48 51 90 \n3 306 23 47 43 89 Figure 6: Evolution of the points-to relations after successive pairs of backward-forward \nsolve phases for MPF We notice that the number of points-to relations is con\u00adstant. The major improvement \nconcerns the numerical in\u00advariants. Although the points-to table may seem very small compared to the \nsize of the code, the points-to relations recorded there are pervasively used throughout the code. During \nthe development of the tool we noticed that improv\u00ading the precision of few critical entries in this \ntable could resolve thousands of checks at once.  6. RELATED WORK There are two bodies of work that \nare directly related to our work. The .rst one is the commercial tool PolySpace C Veri.er [22]. At the \ntime of writing, the tool is available in three versions: C, C++ and Ada. We do not have any infor\u00admation \nabout the C++ version. The Ada version seems to scale quite well, even though we do not have any practical \nexperience with it. The C version however does not really scale. Our experiments, using PolySpace C Veri.er, \non MPF and DS1 showed that the tool could only process the code in chunks no bigger than 40KLOC. Still, \nPolySpace Veri.er was useful and found quite a few bugs (mainly uninitial\u00adized variables, out-of-bound \narray accesses, and over.ows). Unfortunately, it also produced a large amount of warnings which deters \ndevelopers. The second body of work precisely addresses the prob\u00adlem of generating too many warnings. \nIn [3] the authors describe a static analyzer (also based on abstract interpre\u00adtation) that can analyze \n75,000 lines of C code in a couple of hours with a high level of precision (11 false alarms on the code \nused for their experiment). Like for CGS, the au\u00adthors specialized their algorithms for a family of software \nwith the following characteristics: many global and static variables, no recursive functions nor gotos, \nand simple data structures. Furthermore, the authors mentioned than the alias information is trivial \nin the code they analyze. There are many analyses that can now scale to large pro\u00adgrams [24, 2, 1, 14, \n18], but none of those o.er the level of precision that can meet our requirements. For example, none \nof those analyses can track o.sets (in arrays or com\u00adplex data structures) with su.cient precision. Moreover, \nall these analyses have been designed for sequential programs. More precise analyses, such as those used \nin shape analysis [23], exist but they fail to scale to large programs. In fact, it is extremely di.cult \nto design an analysis that scales with high precision for any C program. However, as we demon\u00adstrate \nhere, high precision can be achieved on large programs that share the same basic structure. 7. CONCLUSION \nWe have shown in this paper that the array bound check\u00ading of large C programs can be performed with \na high level of precision (around 80%) in nearly the same time as compi\u00adlation. The key to achieve this \nresult is the specialization of the analysis towards a particular family of software. Most importantly, \nthis experience emphasizes the importance of specializing the algorithms (the domain of adaptive DBMs) \nand dismisses the use of general solutions (parallelization). This approach has a major drawback however: \ndeveloping a specialized static analyzer is a huge e.ort that requires an important expertise, which \nlimits the impact of these tech\u00adniques in the software industry. CGS is currently being applied to other \nkinds of NASA software. It has been recently run with success on several pieces of software operating \nin the International Space Sta\u00adtion. This is an interesting process that will give us informa\u00adtion on \nhow a specialized analyzer behaves on programs that do not belong to its primary scope. The .rst results \nshow noticeable variations in the precision. However, the scala\u00adbility of the tool remains remarkably \nintact and CGS is able to analyze small programs of 20KLOC in few minutes. 8. REFERENCES [1] A. Aiken \nand M. F\u00a8ahndrich. Program analysis using mixed term and set constraints. In Proceedings of 4th International \nStatic Analyses Symposium (SAS 97), 1997. [2] L. Andersen. Program Analysis and Specialization for the \nC Programming Language. PhD thesis, DIKU, University of Copenhagen, 1994. [3] B. Blanchet, P. Cousot, \nR. Cousot, J. Feret, L. Mauborgne, A. Min\u00b4e, D. Monniaux, and X. Rival. A static analyzer for large safety-critical \nsoftware. In Proceedings of the ACM SIGPLAN 2003 Conference on Programming Language Design and Implementation \n(PLDI 03), pages 196 207, San Diego, California, USA, June 7 14 2003. ACM Press. [4] F. Bourdoncle. \nE.cient chaotic iteration strategies with widenings. In Proceedings of the International Conference on \nFormal Methods in Programming and their Applications, volume 735 of Lecture Notes in Computer Science, \npages 128 141. Springer Verlag, 1993. [5] G. Brat and R. Klemm. Static analysis of the mars exploration \nrover .ight software. In Proceedings of the First International Space Mission Challenges for Information \nTechnology, pages 321 326, 2003. [6] T. Cormen, C. Leiserson, and R. Rivest. Introduction to Algorithms. \nThe MIT Press, 1990. [7] P. Cousot and R. Cousot. Abstract interpretation: a uni.ed lattice model for \nstatic analysis of programs by construction or approximation of .xpoints. In Proceedings of the 4th Symposium \non Principles of Programming Languages, pages 238 353, 1977. [8] P. Cousot and R. Cousot. Systematic \ndesign of program analysis frameworks. In Conference Record of the Sixth Annual ACM SIGPLAN-SIGACT Symposium \non Principles of Programming Languages, pages 269 282, San Antonio, Texas, 1979. ACM Press, New York, \nNY. [9] P. Cousot and R. Cousot. Abstract interpretation and application to logic programs. Journal of \nLogic Programming, 13(2 3):103 179, 1992. [10] P. Cousot and R. Cousot. Abstract interpretation frameworks. \nJournal of Logic and Computation, 2(4):511 547, 1992. [11] P. Cousot and R. Cousot. Modular static program \nanalysis, invited paper. In R. Horspool, editor, Proceedings of the Eleventh International Conference \non Compiler Construction (CC 2002), pages 159 178, Grenoble, France, April 6 14 2002. LNCS 2304, Springer, \nBerlin. [12] P. Cousot and N. Halbwachs. Automatic discovery of linear restraints among variables of \na program. In Conference Record of the Fifth Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming \nLanguages, pages 84 97, Tucson, Arizona, 1978. ACM Press, New York, NY. [13] M. Das. Uni.cation-based \npointer analysis with directional assignments. In Proceedings of the ACM SIGPLAN 00 conference on Programming \nlanguage design and implementation, pages 35 46. ACM Press, 2000. [14] M. Das, B. Liblit, M. F\u00a8ahndrich, \nand J. Rehof. Estimating the impact of scalable pointer analysis on optimization. In Proceedings of 8th \nInternational Static Analyses Symposium (SAS 01), pages 260 278, 2001. [15] Edison Design Group. http://www.edg.com. \n[16] A. Geist, A. Beguelin, J. Dongarra, W. Jiang, R. Manchek, and V. Sunderam. Pvm 3 User s Guide And \nReference Manual. MIT Press, 1994. [17] Green Hills Software. http://www.ghs.com. [18] N. Heintze and \nO. Tardieu. Ultra-fast aliasing analysis using CLA: A million lines of c code in a second. In SIGPLAN \nConference on Programming Language Design and Implementation, pages 254 263, 2001. [19] A. Milanova, \nA. Rountev, and B. G. Ryder. Precise and e.cient call graph construction for c programs with function \npointers. Journal of Automated Software Engineering, 2004. [20] A. Min\u00b4e. A new numerical abstract domain \nbased on di.erence-bound matrices. In Proceedings of the 2nd Symposium PADO 2001, volume LNCS 2053, pages \n155 172, 2001. [21] A. Min\u00b4e. The octagon abstract domain. In AST 2001 in WCRE 2001, IEEE, pages 310 \n319. IEEE CS Press, October 2001. [22] PolySpace Technologies. http://www.polyspace.com. [23] M. Sagiv, \nT. Reps, and R. Wilhelm. Parametric shape analysis using 3-valued logic. In Proceedings of Symposium \non Principles of Programming Languages, 1999. [24] B. Steensgaard. Points-to analysis in almost linear \ntime. In Proceedings of theACM Conference on Principles of Progamming Languages, 1996. [25] The PostgreSQL \nGlobal Development Group. http://www.postgresql.org. [26] A. Venet. A scalable nonuniform pointer analysis \nfor embedded programs. Submitted to publication. [27] A. Venet. Abstract co.bered domains: Application \nto the alias analysis of untyped programs. In Proceedings of SAS 96, volume 1145 of Lecture Notes in \nComputer Science, pages 266 382. Springer Verlag, 1996. [28] A. Venet. Automatic analysis of pointer \naliasing for untyped programs. Science of Computer Programming, 35(2):223 248, 1999. [29] A. Venet. Nonuniform \nalias analysis of recursive data structures and arrays. In Proceedings of the 9th International Symposium \non Static Analysis SAS 02, volume 2477 of Lecture Notes in Computer Science, pages 36 51. Springer, 2002. \n  \n\t\t\t", "proc_id": "996841", "abstract": "In this paper we describe the design and implementation of a static array-bound checker for a family of embedded programs: the flight control software of recent Mars missions. These codes are large (up to 280 KLOC), pointer intensive, heavily multithreaded and written in an object-oriented style, which makes their analysis very challenging. We designed a tool called C Global Surveyor (CGS) that can analyze the largest code in a couple of hours with a precision of 80%. The scalability and precision of the analyzer are achieved by using an incremental framework in which a pointer analysis and a numerical analysis of array indices mutually refine each other. CGS has been designed so that it can distribute the analysis over several processors in a cluster of machines. To the best of our knowledge this is the first distributed implementation of static analysis algorithms. Throughout the paper we will discuss the scalability setbacks that we encountered during the construction of the tool and their impact on the initial design decisions.", "authors": [{"name": "Arnaud Venet", "author_profile_id": "81100150806", "affiliation": "NASA Ames Research Center, Moffett Field, CA", "person_id": "P444757", "email_address": "", "orcid_id": ""}, {"name": "Guillaume Brat", "author_profile_id": "81100171456", "affiliation": "NASA Ames Research Center, Moffett Field, CA", "person_id": "P437059", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/996841.996869", "year": "2004", "article_id": "996869", "conference": "PLDI", "title": "Precise and efficient static array bound checking for large embedded C programs", "url": "http://dl.acm.org/citation.cfm?id=996869"}