{"article_publication_date": "06-09-2004", "fulltext": "\n Min-Cut Program Decomposition for Thread-Level Speculation* Troy A. Johnson, Rudolf Eigenmann, T. N. \nVijaykumar {troyj, eigenman, vijay}@ecn.purdue.edu School of Electrical and Computer Engineering Purdue \nUniversity West Lafayette, IN 47907 ABSTRACT With billion-transistor chips on the horizon, single-chip \nmul\u00adtiprocessors (CMPs) are likely to become commodity com\u00adponents. Speculative CMPs use hardware to \nenforce de\u00adpendence, allowing the compiler to improve performance by speculating on ambiguous dependences \nwithout abso\u00adlute guarantees of independence. The compiler is respon\u00adsible for decomposing a sequential \nprogram into specula\u00adtively parallel threads, while considering multiple perfor\u00admance overheads related \nto data dependence, load imbal\u00adance, and thread prediction. Although the decomposition problem lends \nitself to a min-cut-based approach, the over\u00adheads depend on the thread size, requiring the edge weights \nto be changed as the algorithm progresses. The changing weights make our approach di.erent from graph-theoretic \nsolutions to the general problem of task scheduling. One recent work uses a set of heuristics, each targeting \na speci.c overhead in isolation, and gives precedence to thread pre\u00addiction, without comparing the performance \nof the threads resulting from each heuristic. By contrast, our method uses a sequence of balanced min-cuts \nthat give equal considera\u00adtion to all the overheads, and adjusts the edge weights after every cut. This \nmethod achieves an (geometric) average speedup of 74% for .oating-point programs and 23% for in\u00adteger \nprograms on a four-processor chip, improving on the 52% and 13% achieved by the previous heuristics. \n Categories and Subject Descriptors D.3.4 [Programming Languages]: Processors compil\u00aders, optimization; \nC.1.4 [Processor Architectures]: Par\u00adallel Architectures; G.2.2 [Discrete Mathematics]: Graph Theory \ngraph algorithms * This material is based upon work supported in part by the National Science Foundation \nunder Grants No. 9703180, 9975275, 9986020, and 9974976. Any opinions, .ndings, and conclusions or recommendations \nexpressed in this material are those of the authors and do not necessarily re.ect the views of the National \nScience Foundation. Permission to make digital or hard copies of all or part of this work for personal \nor classroom use is granted without fee provided that copies are not made or distributed for pro.t or \ncommercial advantage and that copies bear this notice and the full citation on the .rst page. To copy \notherwise, to republish, to post on servers or to redistribute to lists, requires prior speci.c permission \nand/or a fee. PLDI 04, June 9 11, 2004, Washington, DC, USA. Copyright 2004 ACM 1-58113-807-5/04/0006 \n...$5.00. General Terms Algorithms, Performance  Keywords Thread-Level Speculation, Chip Multiprocessor, \nMin-Cut, Program Decomposition, Partitioning 1. INTRODUCTION Single-chip multiprocessors (CMPs) are likely \nto become commodity components within a few years, as the number of transistors per chip crosses the \none billion mark. CMPs may be operated as conventional multiprocessors, where explicit parallelism is \nexploited from concurrently running applica\u00adtions or parallel sections within a single application. It \nis di.cult, however, for programmers to parallelize applica\u00adtions manually. Compilers have been successful \nparalleliz\u00ading numerical applications; non-numerical codes confound compilers with dependences that are \nnot statically analyz\u00adable. To alleviate this problem, speculative CMPs [10, 26, 29, 30, 32, 36] have \nbeen proposed to exploit the parallelism implicit in an application s sequential instruction stream. \nBecause speculative CMPs use hardware to enforce depen\u00addence, the compiler can improve performance by \nspeculating on ambiguous dependences without absolute guarantees of independence. Speculative CMPs provide \nthe compiler with the same in\u00adterface as a standard, sequential processor while supporting the safe, \nsimultaneous execution of potentially dependent threads. The compiler may viewa speculative CMP as a \nmultiprocessor in which the simultaneous execution of de\u00adpendent threads results in performance degradation \nrather than incorrect execution, and can select threads to optimize run time. The compiler is responsible \nfor decomposing the control-.owgraph (CFG), and hence the sequential instruc\u00adtion stream, into these \nspeculatively parallel threads. The compiler creates speculative threads by inserting boundary marks \ninto the sequential instruction stream to tell the CMP where to speculate; that is, which code segments \nto try to ex\u00adecute in parallel with each other. The CMP uses prediction to select and execute a set of \nthreads while enforcing cor\u00adrectness, such that the program s output is consistent with that of its sequential \nexecution. To enforce correctness, the CMP employs data-dependence-tracking mechanisms, keeps uncertain \ndata in speculative storage, rolls back incorrect executions, and commits data to the memory system only \nwhen speculative threads succeed. Because thread decomposition is a critical factor in de\u00adtermining the \nperformance achieved by a speculatively\u00adthreaded program, the decomposition scheme used by the compiler \nis key to the success of the speculative approach. To perform thread decomposition, the compiler faces \nmul\u00adtiple performance overheads related to data dependence, load imbalance, thread size, and thread prediction. \nIde\u00adally, no data dependence should cross a thread boundary to avoid dependence-synchronization delays \nand dependence\u00adviolation rollbacks; thread sizes should be chosen to avoid load imbalance; a thread should \nbe large enough to amor\u00adtize its dispatch overhead, but small enough such that all of its speculative \ndata can be bu.ered; and thread se\u00adquences should be predictable to avoid misprediction roll\u00adbacks. Finding \noptimum program decompositions in general is NP-complete [25]. Because this problem requires partitioning \na program, it naturally lends itself to a min-cut-based approach. Cer\u00adtainly, others have used graph-theoretic \napproaches to solve compiler problems. Most relevant are static scheduling al\u00adgorithms [18] that map \nthreads from an explicitly paral\u00adlel program onto processors while minimizing interthread\u00adcommunication \noverhead and load imbalance. These algo\u00adrithms share a subset of the goals minimize dependences and \nload imbalance of this decomposition problem. Nev\u00adertheless, there is a fundamental di.erence between \nschedul\u00ading and our decomposition. In our case, all the overheads due to data dependence, thread-sequence \nmisprediction, and load imbalance depend on the size of the threads. Because an edge s weight represents \nthe run-time overhead incurred by cutting the edge, our weights depend on thread sizes and change as \nthe algorithm progresses and newer threads are made. In contrast, the weights in scheduling are .xed \nat the beginning and do not change. Decomposition us\u00ading .xed weights results in poor speculative-threaded \nper\u00adformance because the weights lack a relationship to over\u00adhead. Apart from the changing weights, most \nscheduling algorithms [18] make assumptions that are not applicable to our problem, such as unlimited \nprocessors, uniform thread run time, or constant communication cost (i.e., weights are the same for all \nedges). Some past approaches for decomposition have focused ex\u00adclusively on loops [17, 20, 27]. Unfortunately, \nnon-loop code sections are crucial for non-numerical programs, which are the primary target for speculative \nCMPs. To that end, [31] applies several di.erent heuristics to build threads from loops and non-loops. \nThe heuristics, however, do not con\u00adsider load imbalance and use limited dependence and predic\u00adtion information. \nThe heuristics conservatively attempt to make threads out of all loop bodies, and terminate threads at \nall but the smallest function calls, ignoring opportuni\u00adties for coarser parallelism. The heuristics \ntarget the over\u00adheads of data dependence, load imbalance, and thread pre\u00addiction separately, and give \nprecedence to thread prediction, without comparing the performance of the threads resulting from each \nisolated heuristic. In contrast to [31], this paper chooses the best-performing threads by giving equal \nconsid\u00aderation to all the overheads using a min-cut-based approach. Both techniques are back-end compiler \nalgorithms that do not modify the application s source code. We apply min-cut on the CFG of each procedure \nof an application. In any min-cut-based approach including ours, where overhead due to data dependence \nand thread pre\u00addiction can be represented as edge weights, load imbalance does not lend itself to being \nrepresented as edge weight. Consequently, we use balanced min-cut [35]. For balanc\u00ading, [35] modi.es \na min-cut to reduce the di.erence be\u00adtween the vertex sizes of the cut s two vertex sets. Our balancing \nis signi.cantly more sophisticated in that it re\u00adduces the overall run time of the threads resulting \nfrom the cut. We employ an abstract-execution-based scheme to es\u00adtimate run times of candidate thread \nsets. Combined with the previously-mentioned requirement that our edge weights change as newer cuts are \nmade, our approach performs a sequence of balanced min-cuts where the edge weights are adjusted after \neach balanced cut. Our main contributions are: We are the .rst to map the speculative-thread decom\u00adposition \nproblem onto a graph-theoretic framework. By using a sequence of balanced min-cuts and adjust\u00ading the \nedge weights after every cut, we give equal con\u00adsideration to the overheads of data dependence, thread \nprediction, and load imbalance.  We introduce a method for assigning edge weights such that the cost \nof cutting a control-.owedge models the data dependence and thread misprediction overhead cycles incurred \nby placing a thread boundary on the edge.  We present an abstract-execution-based scheme for comparing \nexecution times of candidate threads.  We have implemented the algorithm as part of a fully\u00adautomated, \npro.le-based compilation process that measures 17 C and Fortran SPEC CPU2000 programs. Our method achieves \nan (geometric) average speedup of 74% for .oating-point programs and 23% for integer programs, improving \non the 52% and 13% achieved by the approach in [31].  In Section 2 we explain the execution model of \na spec\u00adulative CMP, followed by a discussion of our algorithm in Section 3 and results in Section 4. \nAdditional related work is discussed in Section 5. 2. SPECULATIVE CMP EXECUTION MODEL We introduce an \nexecution model for speculative CMPs in terms of the execution overheads that are a.ected by the compiler \ns choice of thread boundaries. Our model is gener\u00adally applicable to the architectures mentioned in Section \n1. The primary di.erence among the architectures lies in the cache protocol they use for managing speculative \nstorage and detecting misspeculation. Di.erent cache protocols im\u00adpact performance, but do not change \nthe compiler s viewof the execution model [3, 8, 9, 11, 28]. Thread-level specula\u00adtion also has appeared \nin virtual machines [14]. The prob\u00adlem of partitioning a program into speculative threads arises with \nall of these architectures. Thread Execution. A thread dispatcher (in hardware) fetches threads from \nthe sequential instruction stream and dispatches them to processors. It uses prediction to de\u00adcide which \nthread to dispatch next. A thread s execution may be incorrect either because the prediction was wrong, \nresulting in a control-dependence violation, or because an interthread data dependence was violated. \nThe CMP de\u00adtects both types of violations and reacts by rolling back and restarting threads as necessary \n[7, 9]. The oldest thread in execution (w.r.t. sequential order) is always nonspecu\u00adlative, guaranteeing \nprogress, while all younger threads are speculative. A speculative thread keeps its uncertain data in \nspeculative storage until it becomes the nonspeculative thread and commits changes to memory. A formal \nexecu\u00adtion model can be found in Section 2 of [16]. Data Dependence Rollback Overhead. True depen\u00addences \nthat cross thread boundaries may lead to data\u00addependence violations and cause rollbacks, as in Figure \n1. A data-dependence violation is detected at the write reference to a memory location that was read \npreviously by a younger thread. The reader and all younger threads are rolled back as in Figure 1. The \nentire run time of the rolled-back threads is overhead. Only true memory dependences (read-after-write) \ncause violations. Anti (write-after-read) and output (write-after\u00adwrite) dependences are properly handled \nby bu.ering in the speculative storage. Furthermore, register dependences are speci.ed by the compiler, \nallowing the hardware to commu\u00adnicate register values from one thread to another as appro\u00adpriate [2]. \nCMP architectures also have evolved to learn and synchronize dynamically any frequently-encountered mem\u00adory \ndependences that impede parallel execution [19]. Control Dependence Rollback Overhead. Control de\u00adpendence \nrollbacks are caused by thread misprediction. A control dependence violation is detected when an older \nthread completes and its actual successor di.ers from the predicted successor. The overhead is the run \ntime of all rolled-back, younger threads as in Figure 1. Figure 1: Rollback due to a data or control \ndependence violation: Thread numbers indicate sequential order. Thread 1 detects a violation in thread \n2. Thread 2 and the younger thread 3 are rolled back, followed by the dispatch of new threads on P2 and \nP3. Threads 2b and 3b may or may not be the same as threads 2 and 3. Load Imbalance Overhead. Threads \nof unequal size can cause load imbalance, as in Figure 2. The imbalance stems from an architecture property: \nthreads are dispatched to the processors in a cyclic order and a processor does not receive a newthread \nuntil it has committed its current thread. Be\u00adcause threads commit in program order, later threads have \nto wait for previous threads to commit. A large thread pre\u00adceding (in program order) a small thread causes \nthe small thread to wait until the large thread commits, idling exe\u00adcution cycles. Maintaining a cyclic \ndispatch order allows the sequence of threads to be determined easily for rollback operations. Although \nthis order simpli.es the architectural design, it results in load imbalance. With more complicated hardware \nit is possible to avoid this overhead by dispatch\u00ading out-of-order or executing multiple threads per \nprocessor core. Such hardware, however, may impact the access speed of the memory hierarchy. We do not \nassume such hardware in this paper. Figure 2: Load imbalance: Strict cyclic dispatch order inherent \nin the architecture leads to load imbalance. General Thread Overhead. Although thread dispatch is e.cient, \nit remains a signi.cant overhead for small threads (i.e., less than twenty cycles). Decomposing a program \ninto large threads will reduce the signi.cance of this overhead; however, much larger threads (i.e., \nthousands of cycles) may over.owthe speculative storage because they will include more writes to memory. \nAn over.ow completely stalls spec\u00adulative execution, until the nonspeculative thread completes and allows \nthe next thread to become nonspeculative, free\u00ading speculative storage. Storage over.owoccurs only in \nvery large threads that access many distinct memory locations, and was not an issue in our benchmarks. \nTechniques exist to reduce the amount of speculative storage required by a program [8, 16].  3. COMPILER \nALGORITHM FOR THREAD DECOMPOSITION 3.1 Compiler s De.nition of a Thread The compiler creates threads \nby designating control-.ow graph (CFG) edges as thread boundaries. A thread begins at the .rst basic \nblock of a procedure or after any thread boundary. The set of basic blocks in a thread is de.ned as the \nset of blocks reachable from its starting block without crossing a thread boundary. More formally, for \na CFG G = {V, E}, where vertices represent basic blocks, the compiler determines boundary edges Eb . \nE such that: v . V begins a thread i. .e . Eb s.t. e =(u . V, v) or v is the initial block of a procedure. \n w . V is part of the thread beginning at v i. v . w in the graph {V, Eb },where Eb = E - Eb.  A CFG \nwith some cut edges is shown in Figure 3. Conve\u00adniently, each thread can be uniquely identi.ed by its \n.rst basic block. A basic block is part of one or more threads and the compiler back end replicates blocks \nas necessary to package code into threads. Function calls are special: they can either be included or \nexcluded from the calling thread. If included, all thread boundaries within the callee are ig\u00adnored at \nrun time; the hardware creates the illusion that the compiler had chosen Eb = \u00d8 for the CFG of the callee. \nThus, inclusion prevents thread boundaries within function calls from interrupting the calling thread, \nallowing the call\u00ading thread to contain more basic blocks at run time. If excluded, thread dispatch will \ncontinue with the threads in the callee. Figure 4 demonstrates inclusion versus exclu\u00adsion and the e.ect \non the number of run-time threads. This mechanism is our only means of making context-sensitive decisions \nfor function calls, since thread boundaries within a function body are the same no matter the location \nfrom which it was called. The decision to include or exclude a call is made statically at each call site \non the CFG. Hence, the entire function can execute as part of the calling thread, or execute as a .xed \nset of threads. Thread boundaries within a loop apply to all iterations of a loop. Figure 3: Relationship \nbetween basic blocks, threads, and edges: Eb = {e5,e6} while E = {e1,e2,e3,e4,e7,e8}.Basic b blocks A, \nE, and F begin new threads. Thread TA con\u00adtains blocks A, B, C, and D; TE contains blocks E, F, and G; \nand TF contains blocks F and G. Figure 4: Function call exclusion versus inclusion: f() is excluded, \nso the threads it contains are executed; g() is included, so the call and everything below it on the \ncall graph executes as a single thread. 3.2 Approach 3.2.1 Goal The goal of our algorithm is to maximize \nthe potential parallelism exposed to the underlying speculative CMP ar\u00adchitecture. The architecture exploits \nthe parallelism when it is actually present and ensures correctness when it is not. The compiler needs \nto attend carefully to each of the over\u00adheads introduced in Section 2 while designating CFG edges as \nthread boundaries. Edge weights are related to thread size and must change after each partitioning step. \nIn our compiler implementation, thread boundaries are chosen af\u00adter all other optimizations have been \napplied. 3.2.2 Min-Cut Decomposition We apply a sequence of balanced min-cuts, where cut edges represent \nthread boundaries, to split a sequential program into threads. We use the standard min-cut [5, 6] for \ncre\u00adating a cut, and an edge s weight represents the estimated cycles lost to misspeculation (both data \nand control .ow) if the edge were cut. Thus the cut-metric is misspeculation penalties, and min-cut minimizes \nthe penalties. By itself, however, the minimum cut does not ensure good parallelism; certainly in the \nextreme case, not cutting anything has zero penalty, but there would be no parallelism. To improve par\u00adallelism \nusing balancing, we modify the algorithm originally proposed in [35]. In [35], balanced min-cut was applied \nto circuit placement and routing. Each balanced min-cut works by performing an initial min-cut and then \nreducing a second metric, the balancing-metric, which in [35] is the di.erence of the sum of vertex sizes \nof the two partitions. The cost of the cut will increase or remain the same while the balancing\u00admetric \nis being reduced, because the initial min-cut is min\u00adimal. We make two important changes to this algorithm \nto use it for thread decomposition: 1. Our balancing-metric has two components: The .rst is the estimated \noverall run time and cycles lost to load imbalance assuming no misspeculation penalties. The second is \nthe cut-metric that represents misspeculation penalties. The balancing-metric is the sum of the two components. \nThus, our balancing-metric represents the overall run time spent by the threads for execution, load imbalance \nand misspeculation (i.e. performance). 2. Because our balancing-metric includes the cut-metric, our \nalgorithm terminates when it reaches a minimal overall run time. By contrast, the balancing-metric does \nnot include the cut-metric in [35]. That ap\u00adproach will continue considering more-balanced cuts even \nwhen the misspeculation penalty has increased to the point that it negates the performance gain from \nparallelism.  By using overall run time as the balancing-metric, our algo\u00adrithm gives equal consideration \nto all the overheads at every balanced min-cut step, allowing one overhead to be traded o. for another. \nBy beginning with a sequential program and decomposing, our algorithm tends to select threads that are, \non average, larger than the threads selected by [31], which builds threads bottom-up from basic blocks. \n 3.2.3 Differences from the Heuristic Approach There are several di.erences between our approach and \nthe approach in [31]. In [31], the overheads of prediction and data dependence are considered separately; \nthe prediction heuristic overrides the dependence heuristic; and there is no consideration of load imbalance. \nFigures 5 and 6 show some problems that arise when the overheads are considered separately and are not \ngiven equal importance.  3.3 The Algorithm 3.3.1 Input Our algorithm s input is an annotated CFG, \nwhere each vertex represents a basic block of the program. The annota\u00adtions include static information \nabout instructions per block. They also include dynamic information about branch fre\u00adquencies, average \ncycles per function call, and dependences.  Figure 5: Some di.erences between [31](left) and our al\u00adgorithm \n(right): On the left, the dependence heuristic suggests grouping basic blocks D and E into the same thread; \nhowever, the prediction heuristic overrides that decision assuming loops are highly predictable. Because \nthe loop iterates only twice, the cost of cutting the de\u00adpendence edge outweighs the gain from parallelizing \nthe loop. On the right, the loop and block E are kept within a single thread, avoiding inter-thread dependence. \nWe do not claim that our pro.ling methods, described in Section 3.6, are perfect. We showthat our algorithm \nim\u00adproves performance using reasonable pro.ling information, and would expect equivalent or better performance \nwith more accurate pro.ling.  3.3.2 Overall Scheme A high-level viewof the algorithm is shown in Figure \n7 and psuedocode for the algorithm is shown in Figure 8. The fol\u00adlowing subsections will cover subroutines \nof the algorithm in detail. We begin with the entire program as a single thread. Each step seeks to decompose \na procedure s CFG such that performance will improve. Initially, each thread includes (as in Section \n3.1) its called subroutines. The initial part of the algorithm prepares the graph and the main part it\u00aderatively \n(re)computes the weights representing overheads and applies balanced min-cuts. As mentioned above, the \n.rst min-cut step minimizes the dependence and prediction penalties, while the balancing extension to \nmin-cut reduces the overall run time. The M function represents the perfor\u00ad mance metric, where M(G)= \nR(G)+ Eb W(ei). R is the estimated run time including load imbalance, but excluding other penalties. \nW is the weight of an edge representing de\u00adpendence and prediction penalties, W(ei)= D(ei)+ P(ei). A \nnumber of balancing steps followthe initial cut. The algorithm tries moving each vertex bordering the \ncut to the other side, one at a time, and keeps the version with the best performance metric. After each \nvertex is moved, a newmin- Figure 6: Some di.erences between [31](left) and our algorithm (right): On \nthe left, the prediction heuristic causes threads to begin at all loop boundaries, even when there is \ncoarser parallelism. These boundaries cause load imbalance, which will negate the speedup from paralleliz\u00ading \nthe inner loop unless the inner loop is large. On the right, inner loop iterations become part of the \nouter-loop thread, avoiding load imbalance. cut is performed. This newmin-cut is necessary because the \nvertex move may cause the cut to cross an expensive edge. The authors of [35] showthat these additional \nmin-cuts do not increase the asymptotic complexity of the algorithm, provided that the number of vertex \nmoves is bounded by some constant. They also showhowto prevent min-cut from simply .nding the same cut \nagain and ignoring the move: the vertices on one side of the cut are temporarily collapsed into a supernode. \nFigure 9 shows the collapsing idea from [35]. Our balancing steps repeat while perfor\u00admance improves. \nFinally, our algorithm compares the best version to the performance of not cutting at all. If there is \nno improvement, decomposition of this thread stops. Other\u00adwise, the best cut is applied and the newly-created \nthreads are considered for decomposition in the same manner. A key feature of our algorithm is that it \nis able to handle loops and straight-line code in the same way. Loops may become part of a single thread, \neach iteration may become a thread, or an iteration may get split up. The algorithm sees large weights \nacross back edges of loops with cross\u00aditeration dependences, biasing min-cut toward more inde\u00adpendent \nloops with smaller weights. For nested loops that have identical edge weights, min-cut .rst encounters \nthe out\u00adermost back edge as part of its normal operation. Therefore, coarser parallelism is tried before \n.ner parallelism. Making both outer and inner loops into threads generally is avoided for perfectly nested \nloops because that causes load imbal\u00adance, which negatively impacts the performance metric. To\u00adgether, \nthese properties cause trade-o.s that are more ap\u00adpropriate for handling loops than employed by [31], \nas shown in Figure 6. Because our approach is a heuristic for an NP-complete problem, our algorithm may \nconverge on a local optimum. Although using simulated annealing or genetic algorithms may overcome this \nlimitation, such options would increase compilation time substantially. Instead, we use a simple perturbation \nin the .nal step, edge perturbation,to help free the algorithm from local optima. We stipulate that every \nedge in the graph should be con\u00adsidered for cutting at least once. Each edge that was not examined throughout \nthe sequence of balanced min-cuts is nowexamined exactly once to determine if cutting it im\u00adproves performance. \nThese edges are examined in reverse order, from the end of the procedure s CFG up to the entry point, \nsuch that any unexamined back edges of outer loops are encountered before inner loops. We choose to cut \nan edge if run time decreases. We consider each edge in iso\u00adlation, so this perturbation step is linear \nin edges and does not increase our algorithmic complexity. Figure 7: Our approach: The edge-weight assignment \nand the performance metric calculation make use of pro.le information. A di.erent metric could be sub\u00adstituted \nto accommodate di.erent architectures or im\u00adproved models. 3.3.3 PrepareGraph Our algorithm relies on \nthe standard max-.ow/min-cut algorithm [5, 6] to partition a set of vertices into subsets, such that \nthe sum of the weights of the edges joining the subsets is minimal. Min-cut is not appropriate for a \nCFG containing loops however, because we often want to cut only the back edge of a loop without cutting \nits body. Min-cut is forced to cut at least two edges to break the cycle [34]. Therefore we use a technique \ncalled vertex splitting,shown in Figure 10, that replaces one vertex with two, and di\u00advides the edges \nof the original vertex between them. The transformation converts the back edge to a forward edge, while \npreserving the number of edges and all information necessary to use min-cut for penalty minimization. \nOnly for .procedure P .Program G .PrepareGraph(P) thread set .{GetEntryBlock(G) } while (thread set \n= \u00d8) /* Determine penalties for current threads in G */ ComputeWeights(G) T .GetFirstItem(thread set) \nGT .{VT ,ET } /* Initial bipartition of VT */ cut .MinCut(GT ) /* Seek improvement */ do initial cut \n.cut .u .VT s.t. (u, v) .ET or (v, u) .ET , and u and v are in di.erent partitions G..temporarily collapse \nu with T all vertices in v s partition temp cut .MinCut(G.) T if M(G with temp cut) < M(G with cut) cut \n.temp cut while cut =initial cut /* Improvement? */ if M(G with cut) < M(G) G .Gwith cut  thread set \n.thread set new threads else thread set .thread set -T Edge Perturbation: Check all unexamined edges \nand cut if M(G with edge cut) < M(G) Figure 8: Thread decomposition algorithm: The goal is to .nd the \nset of boundary edges Eb with the best perfor\u00ad mance metric M(G)= R(G)+ W(ei). Eb purposes of estimating \nexecution time is it necessary to re\u00admember that a loop was there, as illustrated by the dashed edge. \n 3.3.4 ComputeWeights The dependence and prediction penalties are based on the surrounding thread sizes, \nso the edge weights must be recom\u00adputed as decomposition progresses to re.ect newknowledge about the \ncurrent set of threads. Each edge is assigned a weight that models the number of cycles that may be lost \nif a newthread were to begin at the sink of the edge. Weights are based on thread sizes, which change, \nand worst-case sce\u00adnarios for wasted cycles. All edges have an initial weight to which dependence and \nprediction penalties are added. The initial weight is .ve cycles, a typical thread dispatch over\u00adhead \nfor speculative CMPs. The dependence and prediction penalties are described in Sections 3.3.7 and 3.3.8, \nrespec\u00adtively. Thread sizes are computed as follows. 3.3.5 S: Thread Size S(T ) is the size of thread \nT in terms of number of cy\u00adcles. T may contain several possible control paths. Hence, the dynamic size \nof the thread depends on the actual path taken. Using pro.ling information, we compute an expected value \nfor the dynamic size of T using equation (1). We rep\u00adresent the probability that path k is taken by pk \nand the Figure 9: Balanced Min-Cut from [35]: 1 . Beginning with graph a, a min-cut is computed, resulting \nin a 30 to 10 split. 2 . The vertices that border the cut, of size 8 and 5, alternately are collapsed \nalong with all vertices on the other side of the cut, leading to graphs b and c,respec\u00adtively. 3Min-cuts \nare recomputed. At this point the  algorithm would choose c over b because it is more bal\u00adanced, and \nthen repeat Step .. 2Applications of this algorithm use a balancing threshold and stop when the cut is \nbalanced su.ciently. Note that the cut cost will remain the same or increase as balancing improves. \nsize of path k as sk. The probability of a path is the prod\u00aduct of the branch frequencies along the path. \nThe size of the path is computed as the sum of static instructions for basic blocks and dynamic cycles \nfor function calls included along the path, similar to estimating procedure execution time in [24]. For \nloops, the total run time is the loop body run time multiplied by the average number of iterations. Thread \nsize can be computed e.ciently by a depth-.rst traversal of the CFG, provided that loops are detected \nand handled properly. The expected thread size is the size of the initial block bb0 plus the size of \nits target paths. Equa\u00adtion (1) gives the computation for any number of targets. For each recursive step, \nthe target sizes are weighted by the probability of a particular branch being taken instead of the product \nof all branch frequencies along the path. That prod\u00aduct will emerge naturally from the recursion. When \na loop s back edge is encountered, the size of the subpath that was taken through the loop is multiplied \nby the average number of iterations minus one because the loop body has already been seen once (not shown \nin equation). Figure 11 shows the size calculation for a simple thread. paths targets S(T )= pksk = bb0 \n+ pmS(m) (1) k=1 m=1 Figure 11: Expected value of the size of a thread: There are four paths through \nthe thread with sizes of 22, 72, 102,and 152. Their probabilities, respectively, are 0.45, 0.15, 0.30,and \n0.10. The direct calculation gives 22 * 0.45 + 72*0.15+102*0.30+152*0.10 =66.5. The indirect, but more \ne.cient, recursive method calculates the same result as 4+6+0.75t1 +0.25(50 + t1),where t1 =6+0.6t2 +0.4(80 \n+ t2), and t2 =4+2.The values t1 and t2 are computed once and reused. 3.3.6 R and L: Run Time and Load \nImbalance The expected run time R(G) of a graph G represents the .rst component of the balancing-metric, \nused in the min\u00adcut balancing step. R(G) and the load imbalance L(G) are calculated by determining the \nrun time and load im\u00adbalance of each possible sequence of threads and performing a weighted sum based \non the probability of each sequence, as in (2), where pk is the probability of each sequence sk of threads, \nr(sk) is the run time of each sequence, and l(sk) is the load imbalance of each sequence. Enumerating \nall se\u00adquences would require exponential time. We approximate by considering up to n most likely sequences, \nwhere n is several hundred. The abstract execution algorithm in Fig\u00adure 12 is used to .nd r(sk)and l(sk) \nunder the assumption that no rollbacks occur. Since run time factors in load im\u00adbalance, L(G) is not \nexplicitly added to M(G), but we show howit could be computed here. Potential rollbacks are not considered \nbecause they are modeled by the edge weights. A subsequence of threads that constitute a path through \na loop has its expected run time multiplied by the average number of loop iterations and divided equally \nacross the processors. This computation is more e.cient than execut\u00ading all threads from all iterations, \nespecially in the case of nested loops, and produces nearly the same e.ect. Replacing the subsequence \nwith these num procs threads allows outer loops to be handled the same way. Time spent in an ex\u00adcluded \nfunction call (as per Section 3.1) is approximated by the performance metric that was computed for that \nfunction (computed on demand, thus enforcing a decomposition or\u00adder on procedures), or simply the pro.led \nexecution time for recursive calls. The excluded call is expanded to num procs threads, each the size \nof its performance metric, to repre\u00ad  prev =0; imbalance =0; for(i = 0; i < num procs; ++i) t[i] = 0; \n for(i = 0; i < num threads; ++i) if (loop detected) replace loop subsequence with num procs threads \neach of size (subsequence size * average iterations / num procs) and modify num threads appropriately \nif (excluded call detected) replace call with num procs threads each of size M(call) and modify num threads \nappropriately p= i mod num procs; if (t[p] < prev) imbalance += prev -t[p]; t[p] = prev; else prev \n= t[p]; t[p] += S(Ti); r(sk) = max(t[0], ..., t[num procs-1]); l(sk) = imbalance; Figure 12: Code for \nabstract execution of a thread se\u00adquence: The if-else statement enforces the dispatch con\u00addition. sent \nprocessors being occupied by threads within the call. This approximation allows us to avoid executing \nthe entire program interprocedurally. paths paths R(G)= pkr(sk), L(G)= pkl(sk) (2) k=1 k=1 3.3.7 D: Dependence \nPenalty Di, the dependence penalty due to cutting edge ei,is es\u00adtimated from dependence pro.le information. \nIf a thread boundary must be placed across a dependence, the proba\u00adbility of it causing a rollback becomes \ngreater the nearer it is placed to the sink.1 Cutting just prior to the sink of a lex\u00adically forward \ndependence places the sink early in a thread and does not allowsu.cient time for the source to execute. \nSimilarly, cutting just after the sink of a lexically backward dependence places the source late in a \nthread, which has the same e.ect. It is not su.cient to begin a thread at a block that is not a sink, \nas suggested by [1], because the following block could be the sink of many dependences with sources late \nin the previous thread. It is also possible for innocuous interthread dependences to exist if the source \nwill always execute before the sink. Therefore, our model for depen\u00addence penalties considers interthread \ndependence distances, instead of simply the number of interthread dependences. In our model, data-dependence \nedges do not exist as sep\u00adarate edges in the CFG. We assign the data-dependence penalty to the edges \nalong the appropriate paths of the CFG. An edge along a path from a source A toasink B has its weight \nincreased by the number of cycles that the sink would 1In this section, sink refers to a dependence sink, \nand not the sink of a .owgraph. need to be delayed in order to satisfy the dependence if that edge were \nmade a thread boundary, as in Figure 13. The de\u00adlay is due to the synchronization mechanism [19] mentioned \nin Section 2, and is useful for modeling the severity of the dependence. If the quantity X -Y is negative, \nthen no penalty is applied. The penalty value is a worst-case esti\u00admate, as the actual delay depends \non the thread start times, which are not known. B: Bneeds delayed by X - Y to satisfy the dependence. \nLexical backward dependences are handled similarly. 3.3.8 P: Prediction Penalties Pi, the thread prediction \npenalty due to cutting edge ei, is estimated from branch frequencies and thread sizes. The hardware must \npredict the successor of each thread. If the prediction is incorrect, then the chosen successor eventually \nwill be rolled back and cycles wasted. When thread bound\u00adaries are placed in regions of unpredictable \ncontrol-.ow, it is more di.cult for the hardware to correctly predict the next thread. To some extent, \nmin-cut naturally helps pre\u00addiction; because there are fewer edges to be cut at points of control-.owreconvergence, \nthe created threads do not have many successors. For assigning edge weights, we use the simple approximation \nthat a completely biased branch (0% or 100% taken) is perfectly predictable, while a completely unbiased \nbranch (50% taken) is totally unpredictable. In between, we assume that predictability is linear. Equation \n(3) allows us to compute an expected value for the number of cycles lost. This value is added to the \nweight of each edge of the branch. There are better ways of modelling the hardware thread predictor; \nalthough we look at each branch in isolation, we could consider control-.ow dependence in\u00adformation and \nuse conditional path probabilities. We leave such extensions as future work. P= thread size *(1 -2 *|0.5 \n-bfreq|) (3)  3.4 Algorithm Ef.ciency Balanced min-cut is bounded by the cost of a single balancing \nstep [35], which requires O(VE2)for the.ow computation, T(E(V + E)) to set prediction penalties, T(D(V \n+ E)) to set dependence penalties where D is the number of dependences between basic blocks, and O(nT \n) for abstract execution up to a .xed number n of most likely paths (several hundred). Repeating balanced \nmin-cut until there are T threads leads to a O(VE2T +D(V +E)T +nT 2) upper bound for decomposing a procedure. \nPerturbation at the end requires an additional O(nET ). In practice, T \u00abV and V ' 0.75E, so the algorithm \nhas a complexity of O(V 3) assuming D = O(V 2).  3.5 Compiler Implementation Our compiler is based on \nthe GNU C compiler and has been extended several times to add and improve support for speculative CMPs. \nThe f2c program allows the compiler to handle Fortran 77. Compilation involves two passes. The .rst pass \nperforms common optimizations and unrolls loops with small bodies. The output of the .rst pass is a CFG \nfor each source .le. Our algorithm reads this graph, annotates it with dynamic pro.ling data, inserts \nthread boundaries as described in Section 3.1, and outputs newCFGs that re.ect its decisions. GCC does \nnot normally keep the CFG of the entire program in memory at once, but we save the CFG for each source \n.le so that we have that information. The second pass of the compiler reads the newCFGs and gen\u00aderates \nMultiscalar code. Threads are placed in the binary according to the thread boundaries speci.ed on the \nCFG. The binary is executed on a simulator to determine per\u00adformance and generate the statistics discussed \nin Section 4. The binary could be executed on a real CMP, but currently there are no commercial speculative \nCMPs. Two issues arise with interprocedural operations. One caveat is that libc calls must be included \nas in Section 3.1, because the library code does not contain thread headers. However, these calls are \nshort and should be included for performance reasons anyway ([31] has the same restriction). The other \nproblem is that calls through pointers represent an unknown, variable number of cycles because a single \ncall site may invoke numerous di.erent functions with di.erent behaviors. The decision to include or \nexclude applies iden\u00adtically to all of these invocations. We force exclusion for two reasons. (i) Excluding \nthese calls generally gave bet\u00adter performance than including them. (ii) We observed that programs typically \ncall their own functions through pointers, instead of libc functions. While calls through pointers are \nnot found in Fortran 77 programs, they are present in some C benchmarks (notably gap, mesa, and ammp) \nand would be more common in C++ programs. There is no ideal static solution to handling these calls, \nand we do not assume the hardware supports conditional run-time inclusion. We note that [31] is far more \nrestrictive and excludes all non-library calls. 3.6 Obtaining Pro.le Information We use pro.le information \nfor dependencies and branch probabilities. We developed a source-code instrumentation tool to facilitate \nthe run-time detection of data dependences. The tool uses a method similar to [22], which gathers de\u00adpendences \nand dependence distances. The distances are important for handling loops. For example, a dependence crossing \nmore iterations than there are processors is im\u00adplicitly enforced by the execution model. Anti and out\u00adput \ndependences are irrelevant, as explained in Section 2. The instrumentation prevents register allocation \nso it sees all dependences. Source code instrumentation also provides branch frequencies, however the \npro.le may not cover all paths taken in the .nal run. For branches lacking this in\u00adformation, we use \nan estimate that 60% of forward branches are taken and 85% of backward branches are taken [12]. As is \ntypical for pro.le-guided optimizations, the input data for the pro.le run di.ers from the input data \nused to evaluate our techniques [33]. For all pro.le runs, we use the train data set, and for all .nal \nruns we used the ref data set.  4. PERFORMANCE ON SPEC CPU2000 We evaluated the performance of our \nalgorithm using the Multiscalar simulator, con.gured according to Table 1. Cur\u00adrently there are no commercial \nspeculative CMPs; however, Multiscalar shares many properties with proposed imple\u00admentations, and the \nmemory system parameters we use are similar to an IBM Power4 [13]. We execute beyond each pro\u00adgram s \nstartup code with a functional simulation, before per\u00adforming a detailed timing simulation for at least \n500 million instructions. Rather than using a .xed number of instruc\u00adtions for detailed simulation, we \nuse the same start and end points in the source code for all versions of a program. This methodology \nis important, because the instruction count of the di.erent versions may vary due to the number and loca\u00adtion \nof thread boundaries. To verify each benchmark passes the validation test, we complete the runs using \nthe func\u00adtional simulation. A typical simulation runs for one to three days. SPEC CPU2000 contains 19 \nbenchmarks written in C or Fortran 77, of which we use 17. Table 2 shows our bench\u00admarks. We could not \ncompile two C benchmarks, gcc and crafty, because our compiler is based on an old version of gcc. The \nrest are Fortran 90 codes that f2c cannot handle or contain C++. Table 1: Simulator Con.guration CPUs \n4 dual-issue, out-of-order L1 i-cache 64KB, 2-way, 2-cycle hit L1 d-cache 64KB, 2-way, 3-cycle hit, 32-byte \nblock, byte-level disambiguation Rollback Bu.er 64 entries Reorder Bu.er 32 entries Load/Store Queue \n32 entries Function Units 2Int, 2 FP,2Mem Branch Predictor path-based, 2 targets Thread Predictor path-based, \n4 targets Descriptor Cache 16KB, 2-way, 1-cycle hit Shared L2 2MB, 8-way, 64-byte block, 12-cycle hit \nand transfer L1/L2 Connect Snoopy split-transaction bus, 128-bit wide Memory Latency 120 cycles 4.1 \nPerformance and Overhead Analysis Table 2 shows the baseline instructions-per-cycle (IPC) of each benchmark \nrun as a single-thread on one processor, the speedup on four processors over the baseline using the techniques \nin [31], and our speedup over the baseline. Each processor is dual-issue out-of-order. Single-thread \nIPC is typically in the 0.50 to 1.50 range, though mcf is 0.23 and ammp is 0.12 due to poor cache behavior. \nIn FP2000, applu, mgrid, and swim achieve a speedup above 2.0 using [31], while the rest of the benchmarks \nremain below a speedup of 1.40. Of particular interest are the INT2000 benchmarks, of which most remain \nbelow a speedup of 1.30. Integer (i.e., non-numerical) programs are known to be more di.cult for compilers \nto parallelize than scienti.c programs, due to the lack of large, regular loops. Our results showsimilar \ntrends, but note that a speculative CMP is able to extract some parallelism from these programs. The \nlast two columns in the table showthat in general our approach creates longer threads compared to [31], \ncon.rming that longer threads achieve more parallelism and better performance. Single [31] s Our [31] \ns FP Thread Speedup Speedup Insns/ Insns/ IPC Thread Thread    ammp 0.12 1.02 1.04 14.2 15.5 applu \n1.00 2.14 2.20 36.7 46.2 art 0.48 1.93 2.14 15.7 19.3 equake 1.23 1.07 1.15 15.5 19.6 mesa 1.56 1.13 \n1.34 33.2 49.0 mgrid 1.11 2.13 2.15 113.9 115.4 sixtrack 1.17 1.15 1.23 44.3 52.9 swim 0.58 2.57 2.93 \n101.9 112.8 wupwise 1.22 1.36 2.49 26.6 1374.7 g. mean 1.52 1.74 SPEC Single [31] s Our [31] s Our INT \nThread Speedup Speedup Insns/ Insns/ 2000 IPC Thread Thread bzip2 1.37 1.08 1.12 12.7 16.6 gap 1.32 1.18 \n1.27 25.5 30.1 gzip 1.33 1.26 1.23 19.4 43.4 mcf 0.23 0.87 1.26 7.4 13.0 parser 1.14 0.98 1.02 10.3 11.3 \ntwolf 1.05 1.09 1.12 12.4 15.2 vortex 1.46 1.33 1.45 15.7 24.9 vpr 1.30 1.36 1.47 26.0 32.2 g. mean 1.13 \n1.23 Figure 14 shows the improvement gained by our decompo\u00adsition method over [31]. To isolate the e.ect \nof the chang\u00ading the edge weights as described in this paper, we show our method using changing weights \n(black bars) and us\u00ading .xed weights (white bars). We .rst discuss the black bars. Mesa and wupwise have \nthe best improvement. The INT2000 codes that showthe most improvement are gap, mcf, vortex, and vpr. \nThe average gain over [31] is 14.3% for .oating-point programs and 9.0% for integer programs. A more \ndetailed analysis showed that most of the improve\u00adment in FP2000 was from our algorithm using the depen\u00addence \npro.le to identify potentially parallel outer loops. While our approach gives equal consideration to \nall the over\u00adheads, [31] gives precedence to the thread prediction heuris\u00adtic which creates thread boundaries \nat all loops even when creating threads only from an outer loop would perform bet\u00adter. For wupwise, a \nsigni.cant outer loop was parallel, which lead to a large improvement. It is far more di.cult to de\u00advelop \nsource-code-level explanations for improvement in the INT2000 programs, due to more complex control-.ow. \nNev\u00adertheless, we observe a trend that thread dispatch overhead and load imbalance usually decrease, \nwhile dependence and prediction overhead usually remain about the same. We in\u00adfer there was potential \nfor larger, more-balanced threads, but that dependence and prediction problems are still di.\u00adcult to \navoid at a coarser level. We veri.ed our algorithm performs as well as manual thread selection for some \nparts of the smaller benchmarks. Figure 14 also shows the e.ects of .xing weights at the be\u00adginning, \nas would be done by related scheduling algorithms. This variant of the algorithm assumes an average thread \nsize and does not update the weights as decomposition proceeds. We note that the scheduling algorithms \nwere not designed for our problem and are shown here merely to isolate the e.ect of our strategy of changing \nthe edge weights. Our real point of comparison is [31] which directly targets our problem. Although it \nmight be possible to .nd useful .xed weights for some programs through much trial and error, we believe \nthese results show that recomputing weights is of paramount importance. Figures 15 and 16 showthe relative \nimportance of the dif\u00adferent overheads for each benchmark, the left bar (labeled as a belowthe X-axis) \nusing [31] and the right bar (labeled as b belowthe X-axis) using our algorithm. The 100% mark represents \nthe total amount of overhead for [31] for each benchmark. Comparing [31] to our approach, our overall \noverhead decreases even though some individual overhead increases. This behavior follows from the fact \nthat by giv\u00ading equal consideration to all the overheads, our algorithm trades-o. one overhead for another \nto reduce the overall overhead. For instance, misprediction overhead for applu and mesa in Figure 15 \nand for bzip2 and gzip in Figure 16 increases though their overall overhead decreases. Similarly load \nimbalance overhead for swim in Figure 15 and for parser and vortex in Figure 16 increases. We make a \nfewother ob\u00adservations. The poor cache behavior of ammp shows up as memory latency and dependence overhead. \nLoad imbalance decreases in applu, equake, mesa, mgrid, and especially wup\u00adwise. Dependence is the overhead \nthat typically is reduced for FP2000 benchmarks. Load imbalance and dispatch over\u00adhead typically is reduced \nfor INT2000 benchmarks. Overheads not directly related to speculation include memory latency and function-unit \nstalls. The overheads ap\u00adpear because both approaches are designed to target spec\u00adulation overheads, \nand leave these problems for other opti\u00admizations, such as high-level source transformations or in\u00adstruction \nscheduling within threads. Although memory la\u00adtency is not modeled in our algorithm, our decomposition \nchanges memory latency as a side e.ect due to changes in access locality. Two accesses to the same address \nseparated into di.erent threads, and hence di.erent caches with [31], may be combined into one larger \nthread, and hence the same cache, with our algorithm. There is no support for forc\u00ading particular threads \nonto certain processors. Our thread sizes are generally larger, as can be seen in Table 2, and larger \nthreads combine more nearby basic blocks which ex\u00adhibit high data locality.   4.2 Compilation Time \nWe record in Table 3 the time required to compile each benchmark on a 450 MHz Sparc using min-cut decomposi\u00adtion \n(MC) versus using the heuristics in [31]. Eight of the benchmarks .nished in less than one minute, while \n3-10 min\u00adutes was common for the rest. Sixtrack contains three ex\u00adtremely large procedures (daten, maincr,and \numlau6)that each have over 2000 basic blocks and require a long time for either method. Due to the nature \nof our algorithm, a large program with typically small functions will decompose faster than a smaller \nprogram with several very large functions.  5. RELATED WORK We have already discussed the relation of \nour work to scheduling. Our work di.ers from another recent compiler framework for speculative execution \n[1] in that we do not handle loops separately, address all overheads in an inte\u00adgrated fashion, and evaluate \nlarger applications using more realistic hardware assumptions. Other compilers for CMPs have focused \nexclusively on creating threads from loop it\u00aderations or function calls, while performing much of the \nwork manually [27]. Some compilers [29] are responsible for adding extra code to support speculation \nand synchro\u00adnization, depending on the level of architectural support, whereas we are concerned only \nwith thread boundary place\u00adment. Source-code transformations designed speci.cally to improve speculation, \nsuch as those applied manually in [20, 23], can be applied before performing thread selection. We assume \nthat all transformations have been applied prior to decomposition, although we did not modify any of \nthe source code for this paper. If source code is unavailable, it is still possible to make an existing \nbinary run on a speculative CMP by annotating the binary with thread boundaries [17]. The partitioning \ncan be done entirely in hardware, but suf\u00adfers from a lack of compile-time information and increased \nrun-time overhead [4]. Some compilers [15] consider both implicit and explicit (i.e., with speculation \ndisabled) paral\u00adlelism, if the target architecture supports both options [21]. Table 3: Mean and Maximum \nVertices and Edges and Elapsed Time for Heuristics versus Min-Cut FP2000 Lines [31](s) MC(s) Vavg Eavg \nVmax Emax ammp 13483 35 48 492 649 144 284.3 applu 4975 62 82 169 232 186 232.1 art 1270 32 44 137 200 \n10 9.3 equake 1513 27 35 220 295 22 24.0 mesa 58724 23 33 786 1166 394 511.2 mgrid 1270 30 38 72 99 11 \n2.6 sixtrack 89918 112 146 2996 4018 3542 4983.1 swim 907 30 38 73 88 9 1.2 wupwise 3353 36 48 300 443 \n33 16.9     INT2000 Lines [31](s) MC(s) Vavg Eavg Vmax Emax bzip2 4649 24 33 328 465 24 43.4 gap \n71363 43 62 1015 1554 412 607.2 gzip 8616 28 38 181 254 40 45.6 mcf 2412 27 39 83 128 12 13.1 parser \n11391 22 30 343 426 61 203.8 twolf 20459 61 87 451 710 283 377.6 vortex 67213 25 35 614 916 307 491.5 \nvpr 17729 24 32 337 431 76 214.2 6. CONCLUSIONS The issue of program decomposition for speculative exe\u00adcution \narises in all speculative CMPs. We have presented an algorithm for decomposing programs into threads \nto expose parallelism to a speculative CMP, while considering multi\u00adple performance overheads related \nto data dependence, load imbalance, and thread prediction. The key challenge is that the overheads depend \non the thread size, and change as de\u00adcomposition progresses. Our algorithm uses a sequence of balanced \nmin-cuts, which gives equal consideration to all the overheads, and adjusts the edge weights after every \ncut. We have compared our scheme with the previous heuristic method developed for the Multiscalar architecture. \nWhile the previous method uses each heuristic to target an in\u00addividual overhead in isolation and gives \nprecedence to the thread-prediction heuristic, our algorithm provides a more integrated solution by simultaneously \nconsidering all the overheads. Compared to other work, we have an e.ective, automated decomposition method \nfor programs instead of only loops. Our results showan average speedup of 48% across SPEC CPU2000, whereas \nthe multiple-heuristic ap\u00adproach yields an average of 32%. 7. REFERENCES [1] A. Bhowmik and M. Franklin. \nA General Compiler Framework for Speculative Multithreading. In Proceedings of the 14th ACM Symposium \non Parallel Algorithms and Architectures, August 2002. [2] S. E.Breach, T. N. Vijaykumar, and G.S.Sohi. \nThe Anatomy of the Register File in a Multiscalar Processor. In Proceedings of the 27th International \nSymposium on Microarchitecture, pages 181 190, November 1994. [3] M. Cintra, J. F. Mart\u00b4inez, and J. \nTorrellas. Architectural Support for Scalable Speculative Parallelization in Shared-Memory Multiprocessors. \nIn Proceedings of the 27th International Symposium on Computer Architecture,pages 13 24, June 2000. [4] \nL. Codrescu and D. S. Wills. On Dynamic Speculative Thread Partitioning and the MEM-Slicing Algorithm. \nJournal of Universal Computer Science, 6(10):908 914, 2000. [5] J. Edmonds and R. M. Karp. Theoretical \nImprovements in Algorithmic E.ciency for Network Flow Problems. Journal of the ACM, 19:248 264, 1972. \n[6] L. R.Ford Jr. and D.R.Fulkerson. Flows in Networks. Princeton University Press, 1962. [7] M. Franklin \nand G. S. Sohi. ARB: A Hardware Mechanism for Dynamic Reordering of Memory References. IEEE Transactions \non Computers, pages 552 571, May 1996. [8] M. J. Garzar\u00b4an et al. Tradeo.s in Bu.ering Memory State for \nThread-Level Speculation in Multiprocessors. In Proceedings of the 9th IEEE Symposium on High-Performance \nComputer Architecture, February 2003. [9] S. Gopal, T.N.Vijaykumar, J. E. Smith, and G.S.Sohi. Speculative \nVersioning Cache. In Proceedings of the 4th IEEE Symposium on High-Performance Computer Architecture, \npages 195 205, February 1998. [10] L. Hammond, B. A. Nayfeh, and K. Olukotun. A Single-Chip Multiprocessor. \nIEEE Computer, 30(9):79 85, 1997. [11] L. Hammond, M. Willey, and K. Olukotun. Data Speculation Support \nfor a Chip Multiprocessor. In Proceedings of the 8th International Conference on Architectural Support \nfor Programming Languages and Operating Systems, pages 58 69, October 1998. [12] J. Hennessy and D. Patterson. \nComputer Architecture: A Quantitative Approach, 3rd Edition. Morgan Kaufmann, New York, 2002. [13] IBM. \nIBM e-Server Power4 System Microarchitecture. Technical report, October 2001. [14] I. H. Kazi and D. \nJ. Lilja. JavaSpMT: A Speculative Thread Pipelining Parallelization Model for Java Programs. In Proceedings \nof the 14th International Parallel and Distributed Processing Symposium, pages 559 564, May 2000. [15] \nS. W. Kim and R. Eigenmann. The Structure of a Compiler for Explicit and Implicit Parallelism. In Proceedings \nof the Workshop on Languages and Compilers for Parallel Computing (LCPC), August 2001. [16] S. W. Kim \net al. Reference Idempotency Analysis: A Framework for Optimizing Speculative Execution. In Proceedings \nof the Symposium on Principles and Practice of Parallel Programming, volume 36, pages 2 11, 2001. [17] \nV. Krishnan and J. Torrellas. Hardware and Software Support for Speculative Execution of Sequential Binaries \non a Chip-Multiprocessor. In Proceedings of the International Conference on Supercomputing, pages 85 \n92, July 1998. [18] Y.-K. Kwok and I. Ahmad. Static Scheduling Algorithms for Allocating Directed Task \nGraphs to Multiprocessors. ACM Computing Surveys, 31(4):406 471, 1999. [19] A. Moshovos, S. E. Breach, \nT. N. Vijaykumar, and G. S. Sohi. Dynamic Speculation and Synchronization of Data Dependences. In Proceedings \nof the 24th International Symposium on Computer Architecture, pages 181 193, June 1997. [20] K. Olukotun, \nL. Hammond, and M. Willey. Improving the Performance of Speculatively Parallel Applications on the Hydra \nCMP. In Proceedings of the International Conference on Supercomputing, pages 21 30, June 1999. [21] C. \nL. Ooi et al. Multiplex: Unifying Conventional and Speculative Thread-Level Parallelism on a Chip Multiprocessor. \nIn Proceedings of the International Conference on Supercomputing, June 2001. [22] P. M. Petersen and \nD. A. Padua. Static and Dynamic Evaluation of Data Dependence Analysis. In Proceedings of the International \nConference on Supercomputing, pages 107 116, July 1993. [23] M. K. Prabhu and K. Olukotun. Using Thread-Level \nSpeculation to Simplify Manual Parallelization. In Proceedings of the Symposium on Principles and Practice \nof Parallel Programming, June 2003. [24] V. Sarkar. Determining Average Program Execution Times and their \nVariance. In Proceedings of the Conference on Programming Language Design and Implementation,pages 298 \n312, 1989. [25] V. Sarkar and J. Hennessy. Partitioning Parallel Programs for Macro-Data.ow. In Proceedings \nof the Conference on LISP and Functional Programming, pages 202 211, 1986. [26] G. Sohi, S. Breach, and \nT. N. Vijaykumar. Multiscalar Processors. In Proceedings of the 22nd International Symposium on Computer \nArchitecture, June 1995. [27] J. G. Ste.an. Hardware Support for Thread-Level Speculation. PhD thesis, \nCarnegie-Mellon University, April 2003. [28] J. G. Ste.an, C.B.Colohan, A.Zhai, and T.C.Mowry.A Scalable \nApproach to Thread-Level Speculation. In Proceedings of the 27th International Symposium on Computer \nArchitecture, pages 1 12, June 2000. [29] J. G. Ste.an and T. C. Mowry. The Potential for Using Thread-Level \nData Speculation to Facilitate Automatic Parallelization. In Proceedings of the 4th IEEE Symposium on \nHigh-Performance Computer Architecture, pages 2 13, February 1998. [30] J.-Y. Tsai and P.-C. Yew. The \nSuperthreaded Architecture: Thread Pipelining with Run-Time Data Dependence Checking and Control Speculation. \nIn Proceedings of the International Conference on Parallel Architecture and Compiler Techniques, pages \n35 46, October 1996. [31] T. N. Vijaykumar and G. S. Sohi. Task Selection for a Multiscalar Processor. \nIn Proceedings of the 31st International Symposium on Microarchitecture, December 1998. [32] E. Waingold \net al. Baring It All to Software: Raw Machines. IEEE Computer, 30(9):86 93, 1997. [33] D. W. Wall. Predicting \nProgram Behavior Using Real or Estimated Pro.les. In Proceedings of the Conference on Programming Language \nDesign and Implementation, volume 26, pages 59 70, June 1991. [34] D. B. West. Introduction to Graph \nTheory -Second Edition. Prentice-Hall, 2001. [35] H. H. Yang and D. F. Wong. E.cient Network Flow Based \nMin-Cut Balanced Partitioning. IEEE Transactions on Computer-Aided Design of Integrated Circuits and \nSystems, 15(12), December 1996. [36] Y. Zhang, L. Rauchwerger, and J. Torrellas. Hardware for Speculative \nRun-Time Parallelization in Distributed Shared-Memory Multiprocessors. In Proceedings of the 4th IEEE \nSymposium on High-Performance Computer Architecture, pages 162 173, 1998.  \n\t\t\t", "proc_id": "996841", "abstract": "With billion-transistor chips on the horizon, single-chip multiprocessors (CMPs) are likely to become commodity components. Speculative CMPs use hardware to enforce dependence, allowing the compiler to improve performance by speculating on ambiguous dependences without absolute guarantees of independence. The compiler is responsible for decomposing a sequential program into speculatively parallel threads, while considering multiple performance overheads related to data dependence, load imbalance, and thread prediction. Although the decomposition problem lends itself to a min-cut-based approach, the overheads depend on the thread size, requiring the edge weights to be changed as the algorithm progresses. The changing weights make our approach different from graph-theoretic solutions to the general problem of task scheduling. One recent work uses a set of heuristics, each targeting a specific overhead in isolation, and gives precedence to thread prediction, without comparing the performance of the threads resulting from each heuristic. By contrast, our method uses a sequence of balanced min-cuts that give equal consideration to all the overheads, and adjusts the edge weights after every cut. This method achieves an (geometric) average speedup of 74% for floating-point programs and 23% for integer programs on a four-processor chip, improving on the 52% and 13% achieved by the previous heuristics.", "authors": [{"name": "Troy A. Johnson", "author_profile_id": "81537782856", "affiliation": "Purdue University, West Lafayette, IN", "person_id": "P677808", "email_address": "", "orcid_id": ""}, {"name": "Rudolf Eigenmann", "author_profile_id": "81100505647", "affiliation": "Purdue University, West Lafayette, IN", "person_id": "PP39046092", "email_address": "", "orcid_id": ""}, {"name": "T. N. Vijaykumar", "author_profile_id": "81100528323", "affiliation": "Purdue University, West Lafayette, IN", "person_id": "P276818", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/996841.996851", "year": "2004", "article_id": "996851", "conference": "PLDI", "title": "Min-cut program decomposition for thread-level speculation", "url": "http://dl.acm.org/citation.cfm?id=996851"}