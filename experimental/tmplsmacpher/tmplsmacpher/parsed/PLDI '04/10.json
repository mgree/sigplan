{"article_publication_date": "06-09-2004", "fulltext": "\n Parametric Analysis for Adaptive Computation Of.oading Cheng Wang Department of Computer Science Purdue \nUniversity West Lafayette, IN 47907 wangc@cs.purdue.edu ABSTRACT Many programs can be invoked under \ndi.erent execution options, input parameters and data .les. Such di.erent ex\u00adecution contexts may lead \nto strikingly di.erent execution instances. The optimal code generation may be sensitive to the execution \ninstances. In this paper, we show how to use parametric program analysis to deal with this issue for \nthe optimization problem ofcomputation o.oading. Computation o.oading has been shown to be an e.ective \nway to improve performance and energy saving on mobile devices. Optimal program partitioning for computation \nof\u00ad.oading depends on the tradeo. between the computation workload and the communication cost. The computation \nworkload and communication requirement may change with di.erent execution instances. Optimal decisions \non program partitioning must be made at run time when su.cient in\u00adformation about workload and communication \nrequirement becomes available. Our cost analysis obtains program computation workload and communication \ncost expressed as functions of run-time parameters, and our parametric partitioning algorithm .nds the \noptimal program partitioning corresponding to di.er\u00adent ranges ofrun-time parameters. At run time, the \ntrans\u00adformed program self-schedules its tasks on either the mobile device or the server, based on the \noptimal program parti\u00adtioning that corresponds to the current values ofrun-time parameters. Experimental \nresults on an HP IPAQ handheld device show that di.erent run-time parameters can lead to quite di.erent \nprogram partitioning decisions. Categories and Subject Descriptors D.3.4 [Programming Languages]: Processors \nCompil\u00aders, Optimization, Code Generation General Terms Algorithms, Design, Experimentation, Measurement, \nPer\u00adformance Permission to make digital or hard copies of all or part of this work for personal or classroom \nuse is granted without fee provided that copies are not made or distributed for pro.t or commercial advantage \nand that copies bear this notice and the full citation on the .rst page. To copy otherwise, to republish, \nto post on servers or to redistribute to lists, requires prior speci.c permission and/or a fee. PLDI \n04, June 9 11, 2004, Washington, DC, USA. Copyright 2004 ACM 1-58113-807-5/04/0006 ...$5.00. Zhiyuan \nLi Department of Computer Science Purdue University West Lafayette, IN 47907 li@cs.purdue.edu Keywords \nProgram Partitioning, Program Analysis, Program Trans\u00adformation, Computation O.oading, Program Pro.ling, \nHand\u00adheld Devices, Adaptive Optimization, Distributed System 1. INTRODUCTION Many programs can be invoked \nunder di.erent execution options, input parameters and data .les. Such di.erent ex\u00adecution contexts may \nlead to strikingly di.erent execution instances. The optimal code generation may be sensitive to the \nexecution instances. In this paper, we show how to use parametric program analysis to deal with this \nissue for the optimization problem ofcomputation o.oading. Computation o.oading has been shown to be \nan e.ective way to improve performance and energy saving on mobile devices [10, 9, 11]. In a client-server \ndistributed computing environment, the e.ciency ofan application program can be improved by careful partitioning \nof the program between the server and the client. Optimal program partitioning depends on the tradeo. \nbetween the computation workload and the communication cost. 1.1 The Problem The computation workload \nand communication require\u00adment may change with di.erent execution instances. A program partitioning decision \nmay work well under certain options and workloads, but may work poorly under other options and workloads. \nCorrect decisions on program parti\u00adtioning must be made at run time when su.cient informa\u00adtion about \nworkload and communication requirement be\u00adcomes available. Figure 1 (a) shows an example from an audio \nencoding application. Function f repeatedly gets a frame of the audio sample data from a certain audio \ndevice and puts them into the input bu.er inbuf. It then calls certain encoder function g, which encodes \neach unit ofdata from inbuf and stores it in outbuf. At last, function f writes the encoded data from \noutbuf to the output device. Here g is a function pointer which, under di.erent run-time options, can \nmap to di.erent encoders. The program workload and communication requirement in the above example depend \non three pieces ofrun-time information: the number of input frames x, the bu.er size y and the amount \nofcomputation z,performed in g to en\u00adcode a unit ofdata. Let the computation workload in each iteration \nofthe inner most loop off equals 1. The com\u00adputation workload in function f will be 1 * y * 2 * x =2xy. \nThe computation workload in function g is xyz. Ifwe o.oad f() { for(j = 0; j < x; j++) { f1: for(i = \n0; i < y; i++) { get audio sample p; inbuf[i] = p; } g(); f2: for(i = 0; i < y; i++) { q = outbuf[i]; \nwrite q to output; } } } (a) (b) Figure 1: An Example function g to the server, the mobile device will \nhave only the computation workload from function f. However, we now need to send data inbuf from the \nclient to the server and send data outbuf from the server back to the client. Sup\u00adpose the data transfer \nstartup cost is 6 and communication cost to transfer a unit of data is 1. We get the total data communication \ncost (6 + 1 * y) * 2 * x =12x +2xy.Ifwe o.oad both functions f and g to the server, there will be no \ncomputation cost on the mobile device. However, we need to send data p from the client to the server \nand data q from the server back to the client, with the data communication cost (6+1) * y * 2 * x =14xy. \nTable 1 lists the computa\u00adtion workload and the communication cost under di.erent choices ofcomputation \no.oading. From this table, We see that di.erent values of x, y and z may change the optimal choice ofcomputation \no.oading. The condition for f to be o.oaded is: 14xy < xyz +2xy &#38;&#38; 14xy < 12x +4xy, i.e. 12 <z \n&#38;&#38; 5y< 6. The condition for g to be o.oaded is: 12x+4xy < xyz +2xy . 14xy < xyz +2xy, i.e. 12+2y< \nyz . 12 <z. Table 1: Cost for Di.erent Computation O.oading o.oad - g f,g computation workload xyz + \n2xy 2xy 0 communication cost 0 12x + 2xy 14xy total cost xyz + 2xy 12x + 4xy 14xy The problem to solve \nin this paper is how to .nd opti\u00admal program partitioning and formulate it in terms of the program execution \nparameters.  1.2 The Solution In our computation o.oading, we model the optimal pro\u00adgram partitioning \nproblem as a min-cut network .ow prob\u00adlem with run-time parameters. Our parametric partition\u00ading algorithm \n.nds the optimal program partitioning cor\u00adresponding to di.erent ranges ofrun-time parameters. We then \ntransform the program into a distributed program which, at run time self-schedules its computation tasks \non either the mobile device or the server. The task scheduling is based on the optimal program partitioning \nthat corresponds to the current values ofrun-time parameters. The program exam\u00adple in Figure 1 (a) is \ntransformed into the form shown in Figure 2. In order to derive the optimal partitioning, several issues \nmust be addressed. Table 2 lists the key issues and their so- In f():In the caller to f(): ... ... if \n((12 + 2*y< y*z) if ((12 < z) &#38;&#38; (5 * y < 6)) || (12 < z))call server_f(); call server_g();else \nelse call client_f(); call client_g(); ... ... Figure 2: Program Transformation Table 2: Main Contributions \nIssues Solutions Redundant data transfer data validity states Inexact data dependency memory abstraction \ninformation Cost depending on input parametric cost analysis parameters and parametric partitioning algorithm \n lutions. These are presented in the rest ofthe paper, which is organized as follows. We .rst present \nour program partition\u00ading model in section 2. Based on our program partitioning model, we present a parametric \ncost analysis for program partitioning in section 3 and give a parametric partitioning algorithm in section \n4. We discuss implementation issues in section 5 and report the experimental results in section 6. We \ndiscuss related work in section 7 and conclude in section 8.  2. PROGRAM PARTITIONING MODEL To explore \nopportunities for computation o.oading, we divide an ordinary program into tasks and assign each task \neither to the server or to the client (i.e. the mobile device), but not both. Server tasks and client \ntasks do not run si\u00admultaneously in our current model. Instead, they take turns to execute by following \nthe original program control .ow. The task scheduling and data transfers between the server and the client \nare implemented by message passing. At any moment, only one host (the active host) performs the computation. \nMeanwhile, the other host (the passive host) blocks its task execution and acts in accordance to the \nin\u00adcoming messages. To start a task on the passive host, the active host sends a message to the passive \nhost. Upon receiv\u00ading this message, the receiver becomes the active host and starts the execution ofthe \ncorresponding task. Meanwhile, the sender blocks its current task execution and becomes the passive host. \nThe active host can also send a message to the passive host indicating the termination ofits current \ntask, which will cause the receiver to resume its previously blocked task execution. 2.1 Task Control \nFlow Graph We next show how to divide an ordinary program into tasks, and represent the program execution \nby a task con\u00adtrol .ow graph (TCFG). The TCFG is the program control .ow graph (V, E) whose nodes represent \ntasks. Each edge e =(vi,vj ) . E denotes the fact that task vj may be ex\u00adecuted immediately after task \nvi under certain control .ow conditions. For a program (for example, a C program) composed of functions, \nit is tempting to simply de.ne the task as a func\u00adtion. Unfortunately, if functions are the only nodes \nin the TCFG, then the execution order among the function calls can not be speci.ed by the TCFG. Therefore, \nthe tasks must be de.ned at a .ner grain than functions. There exist di.erent ways to form the tasks, \nas long as the following de.nitions are met: De.nition 1: A task is a consecutive statement segment in \na function that starts with a statement called the task header, and ends with a task branch.There exist \nno task branches or task headers in the middle. De.nition 2: A task header is a statement oftwo kinds: \n1. The branch target ofa task branch 2. The statement immediately following a task branch  De.nition \n3: A task branch is a branch statement (includ\u00ading conditional and unconditional jumps, function calls \nand returns) between di.erent tasks. By de.nition, the entry statement ofeach function must be a task \nheader because it starts a statement segment in a function. Function calls and returns must be task branches \nbecause they jump between statement segments in di.erent functions. Other branch statements may be task \nbranches depending on whether or not they jump between di.erent tasks. Once the task headers are designated, \nit is easy to .nd the task header for each statement. We simply trace the control .ow backward from the \ngiven statement until reaching a task header. This header is the wanted. Furthermore, we can identify \neach task by its unique header. It is easy to see that we can make each basic block a task and each branch \nstatement a task branch. In this way, the TCFG is the conventional control .ow graph. However, we want \nthe tasks to be as large as possible in order to reduce the size ofthe TCFG. Hence, we use the following \niterative algorithm to build the TCFG: Algorithm 1: input: a program output: TCFG = (V, E) make the \nfirst statement of each function a task header; change = TRUE; while (change) { change = FALSE; for \n(each branch statement s) { let t be the branch target of s; let r be the statement following s; find \nthe task header hs of s find the task header ht of t if (hs != ht){ if (r is not a task header) { make \nr a task header; change = TRUE; } if (t is not a task header) { make t a task header; change = TRUE; \n} } } } V = {all task headers}; for (each branch statement s) { let t be the branch target of s; let \nr be the statement following s; find the task header hs of s find the task header ht of t if(hs != ht) \n{ add edge (hs, ht) to E; if (s is a conditional branch statement) { add edge (hs, r) to E; } } } For \nthe example program in Figure 1 (a), Algorithm 1 builds the TCFG shown in Figure 1 (b). The task assignment \ndecision is represented by a boolean value M(v)for each task v such that: . 1task v is assigned to the \nserver M(v)= 0task v is assigned to the client  2.2 Data Validity States In this section, we present \na new method to represent data communication requirement in the TCFG. In traditional approaches (see \n[12] for an example), whenever a task pro\u00adduces a piece ofdata which are consumed by another task, a \ncommunication cost is charged as long as these two tasks run on di.erent machines. Such an approach may \nexaggerate the communication cost when the TCFG is generated from an ordinary sequential program, because \nit overlooks the fact that the produced data may have consumers in more than one task. Take the TCFG \nin Figure 3 (a) as an example. Task A writes to data item d,task B and task C read data item d. Ifwe \npartition the program such that task Aruns on the server, task B and task C run on the client, we will \nhave two DU-chains from the server to the client: A . B and A . C. The traditional approach charges the \ncommunica\u00adtion cost for both DU-chains. However, if we take a closer look at this example, we see that \nonly the data transfer from A to B is necessary, because task C can share the received data with task \nB. AA server server Vco(A, d) = 0 Vci(B, d) = 1 client client Vco(B, d) = 1 Vci(C, d) = 1 B CB C control \nflow data flow partitioning control flow partitioning (a) (b) Figure 3: Example of Data Transfer To \novercome the above problem, we represent the data transfer requirement by data validity states. Each \nshared data item in the original program has two copies, one on the server and the other on the client. \nHence all the tasks assigned to the same host share the same data. We use four binary variables to indicate \nthe data validity states: Vsi(v, d): Is data item d valid on the server at the entry oftask v? Vso(v, \nd): Is data item d valid on the server at the exit oftask v? Vci(v, d): Is data item d valid on the client \nat the entry oftask v? Vco(v, d): Is data item d valid on the client at the exit oftask v? Each ofthese \nboolean variables has the value 1 ifdata item d is valid and value 0 ifinvalid. Figure 3 (b) shows an \nexample ofdata validity states. After the data item d is written on the server in task A,the data item \nd on the client becomes invalid (Vco(A,d)= 0). Before the data item d is read on the client in task B,the \ndata item d on the client must be made valid (Vci(B, d)=1). Therefore, the state of data item d on the \nclient changes from invalid to valid, which means that a data transfer is required for d from the server \nto the client on edge (A, B). From task B to task C there is no need to transfer d from the server to \nthe client, because d on the client remains valid (Vco(B, d)= 1, Vci(C, d) = 1). In this .gure, we only \nshow Vci and Vco, which determine the data transfers from the server to the client. Data transfers from \nthe client to the server are determined by Vsi and Vso, which always equal 1 in this example.  2.3 Memory \nAbstraction To transform an ordinary program into a distributed pro\u00adgram for computation o.oading, we \nmust honor all the data dependences under all possible program execution contexts. Unfortunately, there \noften exist control and data .ow infor\u00admation which can not be determined at compile time. We take an \napproach called memory abstraction to overcome this di.culty. At compile time, we abstract all the memory \n(including code and data) accessed at run time by a .nite set of typed abstract memory locations. Each \nrun-time memory address is represented by a unique abstract memory location, al\u00adthough an abstract memory \nlocation may represent multi\u00adple run-time memory addresses. The abstraction ofrun\u00adtime memory is a common \napproach used by pointer anal\u00adysis techniques [14] to obtain conservative but safe point\u00adto relations. \nThe type information is needed to maintain the correct data endians and address values during the data \ntransfers between the server and the client. We statically determine all the data transfers in terms \nof the abstract memory locations and insert message passing primitives for them. At run time, we perform \ndynamic bookkeeping to correctly map abstract memory locations to their physical memory. This mapping \nis used by message passing primi\u00adtives to determine the exact data memory locations. Figure 4 shows an \nexample ofmemory abstraction. Func\u00adtion f allocates a linked list with n elements and returns the list \nheader. Although the amount ofmemory used in this function depends on the function parameter n,we can \nstill abstract all the memory used in this function into six 1: struct list { 2: int index; 3: struct \nlist *next; 4: }; Abstract Memory Locations: 5: struct list *f(int n) { 6: inti; A1:f 7: struct list \n*p, *q = NULL; A2: n 8: for(i=0; i<n;i++){ A3:i 9: p = malloc(...); A4: p 10: p->index = i; A5: q 11: \np->next = q; A6: memory allocated 12: q=p; inline9 13: } 14: return q; 15: } Figure 4: Example of Memory \nAbstraction abstract memory locations A1to A6. Here, A6represents all the dynamically allocated memory \nin line 9. The dynamic bookkeeping mentioned above is performed by a registration mechanism based on \nthe registration table and the mapping table. Each host has a registration table, whose entries are indexed \nby the abstract memory location ID for lookup. Each entry in the registration table contains a list ofmemory \naddresses for that abstract memory location. The server also maintains a mapping table. Entries in the \nmapping table contain the mapping ofmemory addresses for the same data on the server and on the client. \nThe dynamic translation mechanism on the server trans\u00adlates the data representation between the two hosts. \nData on the server is .rst translated and then sent to the client, and data on the client is .rst sent \nto the server and then translated on the server. This mechanism translates not only the data endians \nbut also the address values contained in pointer variables, using the mapping table. To reduce run-time \noverhead due to registration, our reg\u00adistration mechanism works only on dynamically allocated data that \nare accessed by both hosts. For each dynamically allocated data item d, we use two variables for the \ndata ac\u00adcess states such that: . 1data item d is accessed on the server Ns(d)= 0data item d is not accessed \non the server . 1data item d is accessed on the client Nc(d)= 0data item d is not accessed on the client \n 2.4 Program Partitioning Constraints The task assignment is performed under three kinds of constraints: \nsemantic constraints, data validity state con\u00adstraints and data access state constraints. server A server \n A Vco(A, d) = 0 Vco(A, d) = 0 Vci(B, d) = 1 Vci(B, d) = 1 client client B B Vco(B, d) = 1 Vco(B, d) \n= 1 (a) possible write (b) partial write Figure 5: Example of Conservative Constraint The program semantics \nrequire the distributed program to have the identical external behavior as the original program that \nruns on the client only. Hence, all the I/O operations run on the client, as required by the following \nsemantic con\u00adstraint: Semantic Constraint: Iftask v performs I/O operations, then M(v)= 0. There are \nfour data validity state constraints: First, the local copy ofdata must be valid before each data read \n(Read Constraint). Second, after each data write, the copy of the data on the current host becomes valid \nand the copy on the opposite host becomes invalid (Write Constraint). Third, ifthe data is not written \nwithin a task, then the local copy ofthe data is valid at the exit ofthe task only ifit is valid at the \nentry ofthe task (Transitive Constraint). Last, ifthe data are possibly or partially written in a task, \nwe conservatively require the local copy ofthat data to be valid beforethe write(Conservative Constraint). \nThe conservative constraint is necessary because, in static analysis, we sometimes can only obtain the \npossible or par\u00adtial data write information. Figure 5 (a) shows an example ofpossible data write, where \npointer p may point to data item d. Under the write constraint, data item d on the client becomes valid \nafter the data write through pointer p (Vco(B,d) = 1). However, this will cause a problem if p does not \npoint to d. To overcome this problem, we need a conservative constraint to require data item d on the \nclient to be valid before the possible data write (Vci(B,d)=1). In this way, there will be no problem \nno matter ppoints to d or not. As a side e.ect, the conservative constraint will cause a (conservative) \ndata transfer from the server to the client in this case, because data item d on the client is invalid \naf\u00adter the data write on the server (Vco(A,d)= 0). Figure 5 (b) shows a similar example ofpartial data \nwrite. Here, we treat the whole array d as a data unit. Thus each write to an array element is a partial \nwrite. Formally, we can express the data validity state constraints as follows: Read Constraint: Iftask \nv has a (de.nite, possible or partial) upward exposed read ofdata item d,then M(v) . Vsi(v,d)and \u00acM(v) \n. Vci(v,d). Write Constraint: Ifdata item d is (de.nitely, possibly or partially) written in task v,then \nM(v)= Vso(v,d)and \u00acM(v)= Vco(v,d). Transitive Constraint: Ifdata item d is de.nitely not written in task \nv,then Vso(v,d) . Vsi(v,d)and Vco(v,d) . Vci(v,d). Conservative Constraint: Ifdata item d is possibly \nor partially written in task v,then M(v) . Vsi(v,d)and \u00acM(v) . Vci(v,d). The data access states depend \non the task assignments. Data are accessed on a host only ifcertain tasks that ac\u00adcess the data are assigned \nto this host. Hence we have the following data access state constraint: Data Access State Constraint: \nifdata item d is (def\u00adinitely, possibly or partially) accessed within task v,then M(v) . Ns(d)and \u00acM(v) \n. Nc(d).  3. PARAMETRIC COST ANALYSIS To .nd the optimal program partitioning for computation o.oading, \nwe perform cost analysis. Two issues are consid\u00adered. First, di.erent program partitioning decisions \nhave di.erent costs. Hence, we de.ne each cost in such a way that it is counted only under certain program \npartitioning conditions. Second, the value ofthe cost may change in dif\u00adferent program execution instances. \nHence, we represent the cost in terms ofthe program input parameter values. 3.1 Cost Factors Four kinds \nofcosts are identi.ed in our program parti\u00adtioning: Computation Cost Computation cost is the cost for \ntask execution. For each task v . V,if v is assigned to the server (M(v)= 1), there is a server computation \ncost cs(v). If v is assigned to the client (M(v) = 0), there is a client computation cost cc(v). The \ntotal computation cost can be expressed as: X M(v)cs(v)+ \u00acM(v)cc(v) (1) v.V Data Communication Cost Data \ncommunication cost is the cost for data transfer between the server and the client. On each edge e = \n(vi,vj) . E,ifdata item dis transfered from the client to the server on edge e (Vso(vi,d)=0, Vsi(vj ,d)=1), \nthere will be a client-to-server data communication cost ccsd(vi,vj ,d). Ifdata item d is transfered \nfrom the server to the client on edge e (Vco(vi,d)=0, Vci(vj ,d) = 1), there will be a server-to-client \ndata communication cost cscd(vi,vj,d). The total commu\u00adnication cost can be expressed as: X \u00acVso(vi,d)Vsi(vj,d)ccsd(vi,vj \n,d) (vi,vj ).E,d +\u00acVco(vj ,d)Vci(vi,d)cscd(vi,vj ,d) (2) Task Scheduling Cost Task scheduling cost is \nthe overhead due to task schedul\u00ading via remote procedure calls between the server and the client. On \neach edge e =(vi,vj ) . E,if vi is as\u00adsigned to the client (M(vi) = 0) and vj is assigned to the server \n(M(vj ) = 1), there will be a client-to\u00adserver task scheduling cost ccst(vi,vj ). If vi is assigned to \nthe server (M(vi) = 1) and vj is assigned to the client (M(vj ) = 0), there will be a server-to-client \ntask scheduling cost csct(vi,vj ). The total task scheduling cost can be expressed as: X \u00acM(vi)M(vj )ccst(vi,vj \n) (vi,vj ).E + \u00acM(vj )M(vi)csct(vi,vj ) (3) Data Registration Cost The data registration cost is the \noverhead due to the registration mechanism. For each dynamically allo\u00adcated data item d,if d is accessed \non both the server and the client, (Nc(d)=1,Ns(d) = 1), there will be a data registration cost ca(d). \nThe total data registra\u00adtion cost can be expressed as: X Nc(d)Ns(d)ca(d) (4) d  3.2 Cost Formula We \nnow derive the cost formula for cost analysis. For computation cost, we have: X cs(v)= ts(i) * r(i,v) \ni.v X cc(v)= tc(i) * r(i,v) i.v where tc(i)and ts(i) are the average time to execute in\u00adstruction i on \nthe client and on the server respectively, and r(i,v) is the execution count ofinstruction i in task \nv. For data communication cost, we have: ccsd(vi,vj ,d)= tscd(d) * r(vi,vj) cscd(vi,vj ,d)= tcsd(d) * \nr(vi,vj) where tcsd(d)and tscd(d) are the time to transfer data d from the client to the server and from \nthe server to the client respectively. r(vi,vj ) is the execution count ofthe control edge (vi,vj ). \ntcsd(d)and tscd(d) can be written as: tcsd(d)= tcsh + tcsu * s(d) tscd(d)= tsch + tscu * s(d) where tcsh \nand tsch are the data transfer startup time, tcsu and tscu are the unit data transfer time and s(d)is \nthe data size. For task scheduling cost, we have: ccst(vi,vj )= tcst * r(vi,vj) csct(vi,vj )= tsct * \nr(vi,vj) where tcst and tsct are the average time for client-to\u00adserver and server-to-client task scheduling \nrespectively, and r(vi,vj ) is the execution count ofcontrol edge (vi,vj). For data registration cost, \nwe have: ca(d)= ta * r(d) where ta is the average data registration time and r(d)is the execution count \nofthe statement that allocates data d. In the formula above, all the values in bold face are run\u00adtime \nvalues which should be expressed as functions of input parameters. The other values are constant values \nwhich are measured by experiments using synthesized benchmarks.  3.3 Program Flow Constraints Next, \nwe use program .ow constraints to obtain the exe\u00adcution count rand the dynamically allocated data size \ns.We expressed r and s as functions of input parameter vector h.. Worst case execution time (WCET) estimation \ntechniques [4] use program .ow constraints among execution counts r to get an upper-bound on the program \nexecution time. We extend their .ow constraints for both r and s: The execution count ofthe program \nentry node is 1.  For a dynamic allocation statement whose allocation size is expressed as a function \nS(h.), we know:  s= rS(h.) where s is the total size ofthe data allocated by this statement and r is \nthe execution count ofthe same statement. For a loop whose loop trip count is expressed as a function \nL(h.), we know: rb = riL(h.) where ri is the execution count ofthe loop header and rb is the execution \ncount ofthe loop body. For a branch statement whose branch condition is ex\u00adpressed as a function B(h.) \nsuch that the true branch is taken when B(h.) = 1 and the false branch is taken when B(h.)=0, we know: \nrt = riB(h.) rf = ri - rt where rt is the execution count ofthe true branch, rf is the execution count \nofthe false branch and ri is the execution count ofthe branch header. For each node n in the control \n.ow graph, we have: XX ri = ro where ri is the execution count ofan entry edge of node n and ro is execution \ncount ofan exit edge of node n. Symbolic analysis techniques [5] such as forward expres\u00adsion substitution \nand inductive variable recognition are used to .nd the dynamic data sizes, loop trip counts and branch \nconditions that are expressed as functions of input parame\u00adter vector h.. 3.4 User Annotations A program \nmay contain features which make it hard (if not impossible) to express r and s as functions of h..Ifthe \noptimal program partitioning depends on such unknown r and svalues, user annotations are required. We \ndo not limit user annotations to constant values, as was done in many WCET analysis techniques [4]. We \nallow user annotations expressed as functions of h., whose values vary with di.erent run time parameter \nvalues. There exist unknown variable values which do not a.ect the optimal program partitioning. This \ncommonly happens when the di.erent branches ofan IF statement have approxi\u00admately equal workload and \ndata access amount. In this case, the branches have little e.ect on the program partitioning decision, \neven though we do not know the exact execution count for each branch. This also happens in loops where \nthe ratio between the computation cost and the data commu\u00adnication cost is the same in each iteration \nofthe loop. In this case, the exact value ofloop trip count only a.ects the value oftotal execution cost, \nbut not the optimal program partitioning decision. Our parametric algorithm discussed in the next section \nhas an important advantage that it can be used to determine the necessary user annotations. We treat \neach unknown r or s as a dummy parameter in our parametric partitioning problem and then solve it in \nthe normal way. The dummy parameters which do not occur in the solution will not af\u00adfect the program \npartitioning decision. Hence, we only need annotations for the dummy parameters that appear in the solution. \n4. PARAMETRIC PARTITIONING ALGO-RITHM  4.1 The Problem Formulation The optimal program partitioning \nproblem can be ex\u00adpressed as: Problem 1: Assign binary values to variables M, Vsi, Vso, Vci, Vco, Ns \nand Nc subject to program partitioning con\u00adstraints in section 2.4 and minimize the sum ofthe cost in \n(1) -(4). We have the following theorem. Theorem 1: With given cost values cc, cs, ccsd, cscd, ccst, \ncsct and ca, problem 1 can be reduced to a single-source single-sink min-cut network .ow problem [2]. \nProof: First, we show how the overall reduction works. We build a min-cut network .ow problem (N, A, \nc, s, t), where N is the node set, A is the arc set, c is the capacity on each arc, s is the source node \nand t is the sink node. We represent the terms M, Vsi, Vso, \u00acVci, \u00acVco, Ns and \u00acNc by nodes in N. We \nsolve the min-cut network .ow problem to cut all the nodes into two sets, the source set S containing \ns and the sink set T containing t, such that the term represented by a node in S has value 1 and the \nterm represented by a node in T has value 0. The following two steps build the arc set and de.ne a capacity \non each arc such that the min-cut network .ow problem is equivalent to problem 1: 1. All the constraints \nin problem 1 can be normalized into the form X .Y , where X and Y are terms. Con\u00adstraint X = Y can be \nnormalized to X . Y and Y .X.Constraint \u00acX .\u00acY can be normalized to Y .X.We then represent X .Y by an \narc X .Y with in.nity capacity. Therefore if X is cut to S,then Y must also be cut to S. This is equivalent \nto the constraint X .Y . The constant values 1 and 0 in the constraints are represented by node s and \nnode t respectively. Thisisbecause s is always cut to S and t is always cut to T . 2. All the costs \nin problem 1 can be normalized into the form \u00acYXc,where X and Y are terms and c is the cost value. The \ncost Xc can be normalized as \u00ac0Xc, and \u00acXc as \u00acX1c. We then represent cost \u00acYXc by an arc X .Y with capacity \nc.Therefore, if X is cut to S and Y is cut to T , the cut value equals c.This is equivalent to the cost \n\u00acYXc.  After these steps, we can know that the minimum cut in the .ow network corresponds to an optimal \nsolution to Problem 1. The resulting min-cut network .ow problem is equivalent to problem 1. 0  4.2 \nA Parametric Algorithm With all the cost values expressed as functions of h.,the optimal program partitioning \nproblem can be expressed as the following parametric min-cut network .ow problem (h., N, A, c, s, t): \n Problem 2: X h min c(a,.) a=(ni,nj).A P (ni)-P (nj )=1 subject to: 0 =P (n) =1, .n .N P (s)=1,P (t)=0 \n(5) where h. is the parameter vector, N is thenodeset, A is h the arc set, c(a,.) is the capacity on \narc a expressed as a function of h., s is thesource nodeand t is the sink node. P (n) is a binary cut \nvariable for each node n. P (n)=1 represents the fact that n is cut to source set S and P (n)=0 represents \nthe fact that n is cut to sink set T . With the value of h. given, we can calculate c(a,h.) and, using \nmax-.ow/min-cut algorithms, we can .nd the mini\u00admum cut P . However, the value of h. is only available \nat run time. Performing the max-.ow/min-cut algorithm at run time introduces unacceptable run-time overhead, \nas the best known algorithms run in O(n 3) time complexity. We statically solve the parametric min-cut \nnetwork .ow prob\u00adlem and .nd the set Z ofpairs (P, H), such that P is the minimum cut ofproblem 2 for \nall parameter values h. .H. At run time, we can easily .nd the set Hthat contains the current value of \nhBased on H, the corresponding minimum .. cut P is determined. The following theorem is the foundation \nof our paramet\u00adric algorithm. It comes from the equivalence between the maximum .ow and the minimum cut \nofa .ow network [2]. Theorem 2: Acut P that satis.es constraint (5) is a min\u00adimum cut ofproblem 2 with \nparameter value h. ifand only ifthere exists a .ow f satisfying the following constraints: XX Flow: f(a)= \nf(a), a=(n,.).Aa=(.,n).A .n : n = s, n = t, n .N h Capacity:0 =f(a) =c(a,.), .a : a .A h Opt 1: f(a)= \nc(a,.), .a : a =(ni,nj) .A, P (ni) -P (nj)=1 Opt 2: f(a)=0, .a : a =(ni,nj ) .A, P (ni) -P (nj)= -1 (6) \n Proof: . If P is a minimum cut ofproblem 2 with parameter value h., from the equivalence between the \nmaximum .ow and the minimum cut ofa .ow network, there exists a max\u00adimum .ow f satisfying constraints \nof Flow and Capacity. Moreover, because the maximum .ow value of f is equiv\u00adalent to the minimum cut \nvalue of P , constraints of Opt1 and Opt2 must be satis.ed. Otherwise, the .ow value of f will be smaller \nthan the cut value of P . . Ifthere exists a .ow f satisfying (6), due to the con\u00adstraints of Opt1 and \nOpt2, it is easy to verify that the .ow value of f is equivalent to the cut value of P .Since a cut value \nis never less than a .ow value, P must be a minimum cut ofproblem 2 with parameter value h  .. From \ntheorem 2, we immediately have the following lemma. Lemma 1: For a given cut P satisfying constraint \n(5), the following set contains all the parameter values h. which make P a minimum cut: h {h. |.f : f, \nP , . satisfy constraints (6) } (7) Based on the discussions above, we devise the following iterative \nalgorithm to solve the parametric min-cut problem: Algorithm 2: Input: X -set ofparameter values h. Output: \nZ -set ofpairs (P, H) such that P is a minimum cut ofproblem 2 for all the parameter values h. .H. 1: \nZ= f 2: while (X = f) { 3: Choose a sample parameter value h. .X; 4: Find a minimum cut P for problem \n2 with parameter value h.; 5: With P , compute set Hde.ned by (7); 6: Add (P, H)to Z; 7: X = X-H; 8: \n} In the above, X and H are sets of h. expressed as con\u00adstraints on h.. A max-.ow/min-cut algorithm \ncan be used to .nd the minimum cut P in step 4. It is not hard to use polyhedral operations to perform \nstep 2, 3, 5 and 7, if hh c(a,.) are linear functions. For a nonlinear function c(a,.), we approximate \nit as a new parameter independent of h.. This approximation expands the set X, which may produce false \nmin-cut solutions. Since no parameter values at run time will correspond to the false solutions, the \nfalse solu\u00adtions cause no harm except incurring the time to check for nonexistent parameter ranges. As \nan example, we show how the algorithm works on the program in Figure 1. We divide the program into 5 \ntasks, I, f1, g, f2and O,where tasks I and O represent the tasks for input and output operations respectively. \nThe paramet\u00adric min-cut problem is shown in Figure 6 (a), where s and t are the source node and sink \nnode respectively. For sim\u00adplicity, we modify the original graph to eliminate the data validity state \nnodes and replace them with the equivalent data communication costs between tasks. The .ow network simpli.cation \nmethod discussed in later section can auto\u00admatically do that. For simplicity ofdiscussion, we ignore \nthe task scheduling costs and the registration costs. All the costsmarked onarcsexisting s are client \ncomputation costs and all the costs marked on arcs entering t are server compu\u00adtation costs. The other \ncosts are data communication costs between the tasks. Tasks I and O must run on the client, so they have \nzero client computation costs and in.nite server computation costs (denoted by *). In our parametric \nalgorithm, we .rst arbitrarily select pa\u00adrameter values and get, for example, x =1, y =6 and z =3. Then \nwe get the min-cut problem and its minimum cut P 1 in Figure 7 (b). With P 1, we .nd the parameter range \nR1: z = 12 and yz = 12+ 2y,for which P 1 is the minimum cut. In the second iteration, we arbitrarily \nselect param\u00adeter values in the remaining parameter range and get, for example x =1, y =6 and z = 6. \nThen we get the min-cut problem and its minimum cut P 2 in Figure 7 (a). With P 2, we .nd the parameter \nrange R2: 6 =5y and 12+2y =yz, minimum cut P1: (a) (b) Figure 6: Illustration of Parametric Algorithm \n(1) for which P 2 is the minimum cut. In the third iteration, we arbitrarily select parameter values \nin the remaining pa\u00adrameter range and get, for example x =1,y =1,z = 18. Then we get the min-cut problem \nand its minimum cut P 3 in Figure 7 (b). With P 3, we .nd the parameter range R3: 5y =6and 12 =z,for \nwhich P 3 is the minimum cut. (a) (b) Figure 7: Illustration of Parametric Algorithm (2) Thus, we obtain \nthree optimal partitioning decisions P 1, P 2and P 3 for three parameter ranges R1, R2and R3. In\u00adterestingly, \nalthough all the costs depend on the parameter x, the optimal program partitioning decisions do not depend \non x.  5. IMPLEMENTATION We implement our parametric analysis in GCC. We use a .ow and context insensitive \npoint-to analysis algorithm similar to [1] and apply symbolic analysis in the SSA form. Our symbolic \nanalysis works in a demand-driven way as in [13]. Only the symbols which occur in loop trip counts, dynamic \nallocation size expressions and branch conditions are backward substituted. We implement our parametric \nalgorithm with PolyLib [8], a library which performs polyhedral operations. PolyLib provides basic functions \nfor our parametric algorithm. The d 2 Chernikova algorithm used in PolyLib has O(n )com\u00adplexity, where \nn is the number ofconstraints and d is the polyhedron dimension. This is the theoretical best one can \nexpect for operations on polyhedron because the size of out\u00adput is ofthe same order. 5.1 Implicit Functions \nOur parametric algorithm does not restrict the computa\u00adtion and communication costs to be closed-form \nfunctions. Since all the parameter value sets in Algorithm 2 are ex\u00adpressed as constraints, we can also \nexpress cost functions in implicit form as constraints. Programs often contain many conditional expressions \nin branch and switch statements. For cost functions which depend on such conditional expressions, the \nfollowing lemma is useful for transforming a nonlinear function into implicit functions of linear constraints. \nLemma 2: Nonlinear function f(x, y)= x *y,where x . {0, 1}and y .[N, M], is equivalent to the following \nimplicit linear form: x *N = f(x, y) =x *M y -(1 -x) *M = f(x, y) =y -(1 -x) *N We can easily verify \nthat Lemma 2 is true for both values of x. Lemma 2 can be extended to function f(x1,x2, ..., xn,y) = \nx1 *x2 *... *xn *y,where xi .{0, 1}and y .[N, M]. We .rst transform it into: f1(x1,x2, ..., xn,y)= x1 \n*f2(x2,x3, ..., xn,y) f2(x2,x3, ..., xn,y)= x2 *f3(x3,x4, ..., xn,y) ... fn(xn,y)= xn *y We then use \nLemma 2 to transform each function fi into an implicit linear form. We can also extendLemma 2tofunction \nf(x, y)= x *y, where x .{0, 1, ..., n}and y .[N, M]. In this case, we .rst transform it into: f(x, y)= \nf1(x1,y)+ f2(x2,y)+ ... + fn(xn,y) f1(x1,y)= x1 *y f2(x1,y)= x2 *y ... fn(xn,y)= xn *y where xi .{0, \n1}. We thenuse Lemma2totransform each function fi into an implicit linear form.  5.2 Degeneracy Problem \nDue to the degeneracy problem in linear systems, the pa\u00adrameter ranges for di.erent optimal partitioning \ndecisions may overlap with each other. This makes our parametric partitioning solutions depend on the \nchoice ofminimum cuts in max-.ow/min-cut algorithms. For example, in Figure 8 (a), the small oval denotes \nthe parameter range for parti\u00adtioning P 1 and the large oval denotes the parameter range for partitioning \nP 2. Ifthe algorithm .nds the optimal parti\u00adtioning P 2 .rst, there will be only one optimal partitioning \ndecision P 2 with the larger parameter range. However, if the algorithm .nds partitioning P 1 .rst, there \nwill be two optimal partitioning decisions: P 1and P 2. In our implementation, in order to reduce the \nnumber of optimal partitioning decisions, we use a simple heuristic to  C (a) (b) Figure 8: Implementation \nIssues compare the parameter ranges between each pair ofoptimal partitioning decisions. 5.3 Path Sensitivity \nProblem In our current method, the assignment ofa task is .xed throughout the run-time execution, regardless \nthe control path which leads to its execution. Such path insensitivity makes our program partitioning \nmodel simpler. However, it may lose certain opportunities for computation o.oading. For example, in the \nTCFG shown in Figure 8 (b), task A and B write to data d and task C reads data d.Ifwe partition the program \nsuch that task A runs on the server and task B runs on the client, there will be a data transfer, either \nfrom A to C or from B to C, depending on the task assignment of C. It is better to assign task C in such \na way that C runs on the server ifthe program control .ow goes from A to C and runs on the client ifthe \nprogram control .ow goes from B to C. In this way, there will be no data transfer. In our implementation, \nwe alleviate our limitation ofpath insensitivity by inlining small functions based on heuristics to achieve \ncertain path sensitivity. In future work, we will consider a general context sensitive program partitioning \nmodel for computation o.oading. 5.4 Flow Network Simpli.cation The .ow network reduced from problem \n1 contains much redundancy due to the in.nity capacities. We simplify the .ow network based on the following \nheuristic before applying our parametric algorithm. Heuristic: For minimum cut network problem G =(N, \nA, c, s, t), two nodes ni .N and nj .N,can be merged into a single node without losing the optimal solution \nifthe following constraints are satis.ed: X c(ni,nj ) = c(nj ,nk) (nj ,nk).A,nk =ni X c(nj ,ni) = c(nk,nj) \n(nk,nj ).A,nk =ni It is easy to verify that, under above constraints, a cut on edge (vi,vj) is never \nbetter than a cut on all edges (vj ,vk), anda cut on edge (vj ,vi) is never better than a cut on all \nedges (vk,vj ). Therefore we can merge node vi and vj to a single node. We repeatedly check the above \nconstraints and merge the nodes until nothing can be merged.  6. EXPERIMENT The client used in our experiments \nis an HP IPAQ 3970 Pocket PC which has a 400MHZ Intel XScale processor. The server is a P4 2GHz Dell \nPrecision 340 desktop machine. Table 3: Test programs Program Name Description No. ofParameters No. ofSource \nLines rawcaudio ADPCM in Mediabench, Speech Compression 1 205 rawdaudio ADPCM in Mediabench, Speech Decompression \n1 178 encode G.721 in Mediabench, CCITT Voice Compression 4 1118 decode G.721 in Mediabench, CCITT Voice \nDecompression 4 1248 .t FFT in Mibench, Discrete Fast Fourier Transforms 3 332 susan susan in Mibench, \nPhoto Processing 12 2122 Table 4: Parametric Analysis Results Program No. ofTasks No. ofAnnotations \nNo. ofPartitioning Choices Analysis Time (s) rawcaudio 10 2 1 164 rawdaudio 10 2 1 185 encode 107 4 4 \n2247 decode 87 4 4 2159 .t 26 3 2 748 susan 95 13 3 3482 We run Linux on both machines. The wireless \nconnection is through a Lucent Orinoco (WaveLan) Golden 11Mbps PCMCIA card inserted into a PCMCIA expansion \npack for the IPAQ. Besides the program execution time, we also mea\u00adsure the program energy consumption. \nWe connect an HP 3459A high precision digital multimeter to measure the cur\u00adrent drawn by the handheld \ndevice during program execu\u00adtion. In order to get a reliable and accurate reading, we disconnect the \nbatteries from both IPAQ and the extension pack and we use an external 5V DC power supply instead. Further, \nwe make use ofthe built-in trigger mechanism in the multimeter. After the test program starts running, \nthe IPAQ triggers the meter to do the reading with a high fre\u00adquency. The trigger stops when the test \nprogram .nishes. According to our real measurement, the overhead associ\u00adated with the triggering interrupts \nis less than 0.5% and the readings are consistent over repeated runs. Figure 9: G.721 encode with di.erent \noptions Table 3 gives the information for the benchmark programs used in our experiments. Programs rawcaudio \nand rawdau\u00addio have only one option, which is the input .le name. We use the input data size as a parameter. \nPrograms encode and decode have three command options: the coding method, the audio data format and the \ninput .le name. These two bench\u00admarks for the G.721 standard use unbu.ered I/O functions, but real G.721 \napplications use bu.ers. We modify the code by using bu.ered I/O and set the bu.er size as a new pa\u00adrameter. \nAs we shall see in the experimental results, the bu.er size greatly a.ects the partitioning decision. \n Program .t has three command options: the sinusoid number, the sample number and a .ag for inverse .t. \nPro\u00adgram susan has 12 parameters, 10 ofwhich are command options. The other two are the photo sizes in \neach dimen\u00adsion (x dimension and y dimension). 6.1 Parametric Partitioning Analysis Result Table 4 gives \nour parametric partitioning analysis results. The simplest programs are rawcaudio and rawdaudio.Their \ncomputation cost increases with the input data size. How\u00adever, our parametric analysis .nds that the \nbest way of program execution is always to run the whole program lo\u00adcally, no matter what the input data \nsize is. This is because the communication cost for computation o.oading also in\u00adcreases with the input \ndata size. The tradeo. between com\u00adputation and communication cost remains invariant. For programs encode \nand decode, our parametric parti\u00adtioning algorithm .nds that the input data size does not af\u00adfect the \npartitioning decision. The di.erent values of other parameters generate 4 di.erent choices ofprogram \nparti\u00adtioning. For program .t, our analysis .nds that the sinusoid num\u00adber and the .ag for inverse .t \nhave no e.ect on partitioning decision. However, the sample number a.ects the partition\u00ading decision, \nand there exist two choices ofoptimal program partitioning decision. Program susan has many parameters \nwhich lead to 3 kinds ofoptimal program partitioning. 6.2 Experimental Results Figure 9 shows the performance \nresults of di.erent parti\u00adtioning decisions under di.erent coding methods and audio .le format compositions \nfor G.721 encode.Option -3 uses G.723 coding for 24kpbs bandwidth, option -4 uses G.721 coding for 32kbps \nbandwidth, and option -5 uses G.723 cod\u00ading for 40kbps bandwidth. Option -l works on linear PCM audio \ndata, option -a works on a-law audio data and option -u works on u-law audio data. All the data in this \n.gure are normalized such that the local program execution time is 1. We see that no single partitioning \ndecision always performs best under all command options. Each partitioning decision can be the best partitioning \nunder certain command options. This fact justi.es our parametric analysis for adaptive com\u00adputation o.oading. \n Figure 10 shows the performance results of di.erent pro\u00adgram partitioning decisions under di.erent \nI/O bu.er sizes for G.721 encode. An interesting point here is that the four choices ofprogram partitioning \nhave only two di.erent ef\u00adfects under di.erent bu.er sizes. Three of them (Partition\u00ading 1, 3 and 4) \nhave exactly the same e.ect under di.erent bu.er sizes. This is because we use one particular coding \nmethod (-4) and audio .le format (-l) to get the performance under di.erent bu.er sizes. Under this particular \ncombina\u00adtion ofthe coding method and the audio .le format, the program execution follows a particular \nexecution path, on which partitioning 1, 3 and 4 are exactly the same. We also see from this .gure that \nthe bu.er size greatly a.ects the partitioning decisions. Any .xed choice ofpartitioning may lead up \nto 60% performance decrease from the optimal choice. Figure 11 shows the performance results under di.erent \nsample numbers for .t. Again we see that no .xed parti\u00ad tioning decision remains the optimal for all \ntest cases.  We select 6 representative parameter values and show the performance results for susan \nin Figure 12. Option -s per\u00adforms photo smoothing, option -e recognizes edges in the photo and option \n-c recognizes corners in the photo. The horizontal axis also shows the input .le size. Notice that partitioning \n3 always performs the worst in these experi\u00adments. Accordingtoour analysis, however, it isthe best when \nthe input photo is smaller than hundreds ofpixels. Such an extreme case is rare in reality. Therefore, \nit is not used in the experiments. 6.3 Prediction Error Figure 13 shows the prediction errors ofdi.erent \nprogram partitioning decisions under di.erent command options for the program G.721 encode. All the data \nin the .gure are the ratios between the predicated costs and corresponding measured costs. We see that, \nalthough we can not guarantee the accuracy ofour cost analysis, in the experiments, the prediction errors \nare all within 10%.  For all the parameter values shown in Figures 9 to 12, our parametric analysis \ncorrectly determines the corresponding optimal choice ofpartitioning. The average performance improvement \nofcomputation o.oading over local program execution is about 37%, excluding the execution instances where \nthe whole program executes locally. Energy mea\u00adsurement results for our test programs show that the aver\u00adage \nelectrical current and voltage do not di.er much with di.erent program partitioning decisions, therefore \nthe total energy consumption improves roughly in proportion to the execution time.  7. RELATED WORK \nProgram pro.ling information has been used in many pro\u00adgram optimization techniques [7] [3]. Pro.ling \ninformation is only useful when the speci.c program behaviors vary lit\u00adtle with di.erent program execution \ninstances. Optimal pro\u00adgram partitioning for computation o.oading depends on the tradeo. between the \nprogram computation workload and communication cost, which often varies strikingly with dif\u00adferent program \ninputs. Rudenko, Reiher, Popek and Kuenning [11] experiment with computation o.oading at process level. \nThey show sig\u00adni.cant energy saving with computation o.oading on lap\u00adtops. Since they o.oad an entire \nprogram, there is no anal\u00adysis for program partitioning. Early computation o.oading techniques determine \npro\u00adgram partitioning with either static analysis [9] or program pro.ling information [10]. These techniques \ndo not adapt the program partitioning to di.erent program execution in\u00adstances. As we have seen in our \nexperiments, di.erent pro\u00adgram execution contexts may lead to quite di.erent optimal program partitioning \ndecisions. Worst case execution time (WCET) estimation techniques [4] can be viewed as a special case \nofparametric analysis. Instead ofestimating the program execution time for di.er\u00adent input parameters, \nWCET estimates the worst execution time for all possible program input parameter values. Feautrier [6] \npresents a parametric integer programming algorithm for array data .ow analysis. We use parametric analysis \nand parametric partitioning algorithm for adaptive optimization problem ofcomputation o.oading. 8. CONCLUSION \nThe experimental results in our work have shown that the optimal partitioning for computation o.oading \nmay vary with di.erent program execution instances. The perfor\u00admance penalty can be quite high ifa .xed \npartitioning deci\u00adsion is used for all execution instances. We have presented a parametric program analysis \nto transform the program such that the optimal partitioning decision can be made at run time according \nto the run-time parameter values. In future work, we hope to extend our parametric analysis for other \nprogram optimization problems. This would provide a pow\u00aderful approach to complement pro.le-guided schemes. \n 9. REFERENCES [1] L. O. Andersen. Program analysis and specialization for the C programming language. \nPhD thesis, DIKU, University of Copenhagen, 1994. [2] J. Bang-Jensen and G. Gutin. Graphs: theory, algorithms, \nand applications. Springer-Verlag, London, 2001. [3] P.Chang,S.A. Mahlke, W. Y. Chen,and W. mei W. Hwu. \nPro.le-guided automatic inline expansion for c programs. Software Practice and Experience, 22(5):349 \n369, 1992. [4] J. Engblom and A. Ermedahl. Modeling complex .ows for worst-case execution time analysis. \nIn Proc. of RTSS 00, 21st IEEE Real-Time Systems Symposium, 1998. [5] T. Fahringer. E.cient symbolic \nanalysis for parallelizing compilers and performance estimators. The Journal of Supercomputing, 12(3):227 \n252, 1994. [6] P. Feautrier. Parametric integer programming. Operationnelle/Operations Research, 22:243 \n268, 1988. [7] R. Gupta, D. A. Berson, and J. Z. Fang. Path pro.le guided partial redundancy elimination \nusing speculation. In Proc. of IEEE International Conference on Computer Languages, 1998. [8] P. Jansson \nand J. Jeuring. Polylib -a library of polytypic functions. In Informal Proceedings Workshop on Generic \nProgramming (WGP 98), 1998. [9] U. Kermer, J. Hicks, and J. M. Rehg. A compilation framework for power \nand energy management on mobile computers . In 14th International Workshop on Parallel Computing (LCPC \n01), August 2001. [10] Z. Li, C. Wang, and R. Xu. Computation o.oading to save energy on handheld devices: \nA partition scheme. In International Conference on Compiler, Architecture and Synthesis for Embeded Systems, \npages 238 246, November 2001. [11] A. Rudenko, P. Reiher, G. J. Popek, and G. H. Kuenning. Saving portable \ncomputer battery power through remote process execution. Mobile Computing and Communications Review, \n2(1):19 26, January 1998. [12] H. Topcuoglu, S. Hariri, and M.-Y. Wu. Task scheduling algorithms for \nheterogeneous processors. In Proc. of Eighth Heterogeneous Computing Workshop (HCW 99), 1999. [13] P. \nTu and D. Padua. Gated ssa-based demand-driven symbolic analysis for parallelizing compilers. In Proc. \nof the 1995 International conference on Supercomputing, 1995. [14] R. P. Wilson and M. S. Lam. E.cient \ncontext-sensitive pointer analysis for C programs. In Proc. of the ACM SIGPLAN 95 Conference on Programming \nLanguage Design and Implementation, 1995. \n\t\t\t", "proc_id": "996841", "abstract": "Many programs can be invoked under different execution options, input parameters and data files. Such different execution contexts may lead to strikingly different execution instances. The optimal code generation may be sensitive to the execution instances. In this paper, we show how to use parametric program analysis to deal with this issue for the optimization problem of computation offloading.Computation offloading has been shown to be an effective way to improve performance and energy saving on mobile devices. Optimal program partitioning for computation offloading depends on the tradeoff between the computation workload and the communication cost. The computation workload and communication requirement may change with different execution instances. Optimal decisions on program partitioning must be made at run time when sufficient information about workload and communication requirement becomes available.Our cost analysis obtains program computation workload and communication cost expressed as functions of run-time parameters, and our parametric partitioning algorithm finds the optimal program partitioning corresponding to different ranges of run-time parameters. At run time, the transformed program self-schedules its tasks on either the mobile device or the server, based on the optimal program partitioning that corresponds to the current values of run-time parameters. Experimental results on an HP IPAQ handheld device show that different run-time parameters can lead to quite different program partitioning decisions.", "authors": [{"name": "Cheng Wang", "author_profile_id": "81451596933", "affiliation": "Purdue University, West Lafayette, IN", "person_id": "PP14086730", "email_address": "", "orcid_id": ""}, {"name": "Zhiyuan Li", "author_profile_id": "81409597718", "affiliation": "Purdue University, West Lafayette, IN", "person_id": "PP39044657", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/996841.996857", "year": "2004", "article_id": "996857", "conference": "PLDI", "title": "Parametric analysis for adaptive computation offloading", "url": "http://dl.acm.org/citation.cfm?id=996857"}