{"article_publication_date": "06-09-2004", "fulltext": "\n A Cost-Driven Compilation Framework for Speculative Parallelization of Sequential Programs Zhao-Hui \nDu Intel China Research Center Intel China Ltd. Beijing, China zhao.hui.du@intel.com Chen Yang Intel \nChina Research Center Intel China Ltd. Beijing, China  chen.yang@intel.com Chu-Cheow Lim Programming \nSystems Lab Intel Corporation Santa Clara, California, USA  chu-cheow.lim@intel.com Qingyu Zhao Intel \nChina Research Center Intel China Ltd. Beijing, China  qingyu.zhao@intel.com Xiao-Feng Li Intel China \nResearch Center Intel China Ltd. Beijing, China  xiao.feng.li@intel.com Tin-Fook Ngai Programming Systems \nLab Intel Corporation Santa Clara, California, USA  tin-fook.ngai@intel.com ABSTRACT The emerging hardware \nsupport for thread-level speculation opens new opportunities to parallelize sequential programs beyond \nthe traditional limits. By speculating that many data dependences are unlikely during runtime, consecutive \niterations of a sequential loop can be executed speculatively in parallel. Runtime parallelism is obtained \nwhen the speculation is correct. To take full advantage of this new execution model, a program needs \nto be programmed or compiled in such a way that it exhibits high degree of speculative thread-level parallelism. \nWe propose a comprehensive cost-driven compilation framework to perform speculative parallelization. \nBased on a misspeculation cost model, the compiler aggressively transforms loops into optimal speculative \nparallel loops and selects only those loops whose speculative parallel execution is likely to improve \nprogram performance. The framework also supports and uses enabling techniques such as loop unrolling, \nsoftware value prediction and dependence profiling to expose more speculative parallelism. The proposed \nframework was implemented on the ORC compiler. Our evaluation showed that the cost-driven speculative \nparallelization was effective. Our compiler was able to generate good speculative parallel loops in ten \nSpec2000Int benchmarks, which currently achieve an average 8% speedup. We anticipate an average 15.6% \nspeedup when all enabling techniques are in place. Categories and Subject Descriptors D.3.4 [Programming \nLanguages]: Processors compilers, optimization, code-generation.  General Terms Languages, Design, \nAlgorithms, Performance.  Keywords Speculative multithreading, speculative parallel threading, thread\u00adlevel \nspeculation, speculative parallelization, cost-driven compilation, loop transformation. 1. INTRODUCTION \nHardware support for speculative multithreading has been extensively investigated as a way to speed up \nhard-to-parallelize sequential programs [3][4][10]. It is generally recognized that to take full advantage \nof the hardware thread-level speculation support, software support is required to identify and create \nlikely successful speculative threads and to avoid poor speculations. Main thread Speculative thread \ni-th iteration  spawn thread (i+1)-th iteration speculative results (i+2)-th iteration (i+1)-th iteration \nPermission to make digital or hard copies of all or part of this work for personal or classroom use is \ngranted without fee provided that copies are not made or distributed for profit or commercial advantage \nand that copies bear this notice and the full citation on the first page. To copy otherwise, or republish, \nto post on servers or to redistribute to lists, requires prior specific permission and/or a fee. PLDI \n04, June 9 11, 2004, Washington, DC, USA. Copyright 2004 ACM 1-58113-807-5/04/0006 $5.00.  Figure 1: \nAn SPT Execution Model We propose a comprehensive cost-driven compiler framework to speculatively parallelize \nloops of sequential programs. We developed a misspeculation cost model and used it to drive the speculative \nparallelization. The compiler aggressively looks for loop-level speculative execution opportunities, \nselects and transforms candidate loops into speculative parallel loops that likely can gain from the \nspeculative parallel threading (SPT) execution. The proposed framework also allows the use of enabling \ntechniques such as loop unrolling, value prediction and dependence profiling to expose more speculative \nparallelism. Figure 1 depicts a general speculative parallel threading (SPT) execution model. The main \nthread is the only non-speculative thread that can commit and change the program state. It executes the \nmain program. The threads share the same memory space but have their own context and execution states. \nDuring the parallel execution, the threads do not communicate. For the speculative threads, all speculative \nresults (including memory writes) are buffered and are not part of the program state. Suppose the main \nthread is executing the i-th iteration of a speculative parallel loop. When it executes an SPT fork instruction, \nit spawns a speculative thread to execute the next iteration (i.e., the (i+1)-th iteration) speculatively. \nTo start the speculative execution, the context of the main thread is copied to the speculative thread \nas its initial context. Besides, the start address of the next iteration where the speculative thread \nstarts execution is encoded in the SPT fork instruction and passed to the speculative thread. After the \nspeculative thread is created, the main thread continues its normal execution with the current iteration. \nWhen the main thread comes to the point where the speculative thread started execution (i.e., beginning \nof the next iteration), it will check the speculative execution results of the speculative thread and \nperform any of the following actions. If there is no dependence violation, the main thread commits the \nentire speculative results (i.e., the context of the speculative thread) at once. Otherwise, the main \nthread will commit those correct speculative results and for those incorrect results it re-executes the \ncorresponding misspeculated instructions. The main thread then resumes normal program execution when \nit catches up with the speculative thread (i.e., when there is no more speculative results.) When the \nmain thread executes an SPT kill instruction (usually at the exit of an SPT loop), it kills any running \nspeculative threads. temp_i=i; while (i<n) { while (i<n) { i=temp_i; cost0=0; temp_i=temp_i+1; for (j=0;j<i;j++) \n{ SPT_FORK(loop_id); cost0+=fabs(error[i][j]- p[j]); cost0=0; } for (j=0;j<i;j++) { cost+=cost0;  cost0+=fabs(error[i][j]-p[j]); \ni++; } } cost+=cost0; i=temp_i; } (a) Original loop SPT_KILL(loop_id); (b) SPT-transformed loop Figure \n2: An SPT Loop Transformation Example Figure 2 shows how a loop can be transformed into a speculative \nparallel loop or SPT loop. In the SPT loop (Figure 2(b)), the SPT_FORK statement represents the SPT fork \ninstruction. The code before the fork instruction is called the pre-fork region of the SPT loop. The \ncode after the fork instruction is called the post-fork region. The partitioning of a loop body into \npre-fork region and post-fork region has special meaning in the speculative parallelization. (Hereafter \nwe refer such a partition an SPT loop partition.) Because all statements in the pre-fork region are executed \nbefore the speculative thread is spawned, their results are visible to the speculative thread. Their \nuses in the speculative thread do not violate any data dependences and are always correct. On the contrary, \na statement in the post-fork region is executed after the speculative thread starts. If its result is \nsupposed to be used in the next iteration, the speculative thread may use the old value in its speculative \nexecution and violates the true data dependence. The corresponding speculative instruction and all its \ndependent instructions need to be re-executed. One key speculative parallelization transformation in \nour framework is code reordering. If a define statement in the post\u00adfork region can be moved into the \npre-fork region, all re\u00adexecutions due to its data dependence violation can be avoided. The example in \nFigure 2 illustrates this transformation. The increment of the induction variable i at the end of the \noriginal loop (Figure 2(a)) will cause many re-executions if it remains in the post-fork region. The \nSPT loop transformation moves it from the end of the loop body to the pre-fork region (Figure 2(b)) and \navoids the corresponding re-executions. Note that the temporary variable temp_i is introduced to allow \noverlapping of the life ranges of the old and new values of the variable i. We have implemented the proposed \ncompilation framework on the Open Research Compiler (ORC) [8] and used it to generate speculative parallel \nloops. ORC is an open-source C/C++/Fortran compiler targeting Intel s Itanium processor family (IPF) \nprocessors. The performance of ORC 2.1 is on par with the Intel IPF product compiler ecc 7.0 and 30% \nahead of gcc 3.1 on an Itanium system for the spec2000 integer benchmarks. We implemented our speculative \nparallelization framework primarily in the machine-independent scalar global optimization (WOPT) phase \nof ORC, right after its loop-nested optimization (LNO) phase. Most analyses and transformations were \ndone in ORC s SSA form [14]. The rest of this paper is organized as follows. Section 2 discusses related \nwork. In Section 3, we give an overview of our compilation framework. Section 4 describes the misspeculation \ncost model that we use to drive the speculative parallelization. Section 5 describes how we obtain the \noptimal loop partition that generates the least amount of re-executions for a particular loop candidate. \nSection 6 describes the final SPT loop selection and transformation. This framework was designed to support \na number of important techniques that enables speculative parallelization. Section 7 describes the enabling \ntechniques being supported and used. In Section 8, we present results to evaluate our framework and the \ngenerated SPT code. Section 9 concludes the paper.  2. RELATED WORK The Multiscalar project was the \nfirst comprehensive study of both hardware and software supports for speculative multithreading [3][12]. \nIn particular, Vijaykumar et. al. described the use of compiler techniques to partition a sequential \nprogram into tasks for the Multiscalar architecture [12]. They showed that good task selection is crucial \nto the performance achieved by the Multiscalar architecture, as it pertains to performance issues such \nas control flow/data speculation, register communication and load imbalance. Various compiler heuristics \nwere developed for task selection and register communication. Tsai et. al. described how basic techniques \nsuch as alias analysis, function inlining and data dependence analysis could be used to partition a program \ninto threads which are then pipelined in the superthread architecture [11]. For their experiments, the \nbenchmark programs were manually transformed at the source level. Zhai et.al. studied compilation techniques \nto improve thread-level speculation performance [13]. They focused on code scheduling and inter-thread \ncommunication optimization. They showed that the compiler could reduce value communication delay between \nthe threads significantly by forwarding scalar values to the speculative threads and by inserting synchronization \ninstructions into the threads. More recently, Chen et. al. described a dynamic parallelization system \nthat transformed sequential Java programs to run on a chip multiprocessor that supports speculative threads \n[1][2]. They used hardware profiling support to guide speculative thread decomposition. They estimated \nthe execution time of a speculative loop from the profiling data (such as time stamps collected during \nprofile runs) and based on the estimation selected which nested level of a loop to be run speculatively. \nWhile our speculative parallelization also does profiling and loop selection, our work provides a more \ngeneral speculative parallelization framework and applies more aggressive cost-directed loop transformation \nand selection at compilation time. 3. SPT COMPILATION FRAMEWORK This section gives an overview of our \ncost-driven compilation framework. There are two key elements in our speculative parallelization approach. \nFirst it is cost-driven. Because speculation does not always gain performance, it is essential to estimate \nand reduce the cost associated with misspeculation. We developed a misspeculation cost model to drive \nour speculative parallelization. The second element is aggressive but careful selection. We use a two-pass \ncompilation process to explore every speculative parallelization opportunity, obtain the best transformation \nand select only loops that are likely to deliver performance gain. 3.1 Cost-driven transformations The \nnotion of misspeculation cost is central to our speculative parallelization. Given a particular SPT loop \npartition, its misspeculation cost is defined as the expected amount of misspeculated computation that \nneeds to be re-executed within a speculative executed loop iteration. Figure 3 shows the core of our \ncompilation framework. The central service component is the misspeculation cost computation. We developed \na misspeculation cost model to estimate misspeculation cost of an SPT loop partition. The cost model \nis built using annotated control-flow and data dependence graphs. The following section will describe \nin detail the cost model construction and how it is used to compute misspeculation cost. Figure 3: Core \nof compilation framework The bottom component is the algorithm to find an optimal SPT loop partition \nfor a given loop. The purpose of this component is to determine if a loop can become a good SPT loop. \nIt searches for all possible SPT loop partitions of the loop, evaluate their misspeculation costs, and \ndetermine the best SPT loop partition that generates the least misspeculation cost. We will describe \nthis algorithm in detail in Section 5. 3.2 Two-pass compilation Figure 4: Overall framework -- Combination \nof core with compiler transformation techniques Figure 4 shows our overall framework with a two-pass \ncompilation process. The purpose of the two-pass compilation process is to select and transform only \nall good SPT loops in a program. In the first pass compilation, the initial loop selection selects all \nloops that meet simple selection criteria (such as the loop body size requirement) as SPT loop candidates. \nLoop preprocessing such as loop unrolling and privatization is then applied to transform the loops into \nbetter forms for speculative parallelization. For each loop candidate, the SPT compilation Annotated \nCFG Annotate DD graph framework core is invoked to determine its best SPT loop partition. The result \nof the first pass is a list of SPT loop candidates with their optimal SPT loop partition results. All \nloop transformations are performed in a tentative manner and do not alter the compiled program. No actual \nSPT code is generated. This pass allows us to measure the amount of speculative parallelism in all loop \ncandidates (including each nested level of a loop nest) and evaluate their potential benefits/costs. \nThe second pass takes the result list from the first pass and performs the final SPT loop selection. \nIt evaluates all loop candidates together (not individually as in the case in the first pass) and selects \nonly those good SPT loops. Then the selected loops are again preprocessed and partitioned. Finally the \ncompiler applies the SPT transformations to generate the SPT loop code. In Section 6, we will describe \nthe SPT loop selection and transformation in detail.  4. MISSPECULATION COST MODEL The misspeculation \ncost model is the central component in this cost-driven framework. For a given loop, we build a simplified \ncontrol-flow graph and a dependence graph annotated with dependence probabilities. A misspeculation cost \nmodel is constructed based on these graphs and is used to compute the misspeculation cost of a loop partition. \n4.1 Data-dependence graph A dependence graph is built for each loop body. In order to indicate the likelihood \nof a particular data dependence, each true data-dependence edge in the graph is annotated with a probability \nvalue. A probability value of p on an edge W.R means for every N writes at W, only pN reads will access \nthe same memory location at R during program execution. These data dependence probabilities form the \nbase reason for speculation. If we have a cross-iteration W.R edge with a low p value, we know that that \nit is unlikely for the write in the main thread to cause a misspeculated read in the speculative thread \nso that we could speculate it. A high p value on the other hand suggests a high probability of a misspeculated \nread when W is in the post-fork region. In such a case, to reduce the misspeculation cost, the write \nW should be moved to the pre-fork region. 4.2 Misspeculation cost computation This subsection describes \nthe details of the misspec ulation cost model and how it is used to compute misspeculation cost. Misspeculation \ncost is a useful metric to evaluate the potential performance gains of an SPT partition. It gives, for \na given loop partition, the expected amount of computation (typically in number of elementary operations) \nwithin a speculative loop iteration that needs to be re-executed. A higher cost means that the main thread \nneeds more time to re-execute the misspeculated instructions, and hence such a partition is less desirable. \nWe build a cost graph to represent the code re-execution dependency due to misspeculation. Suppose we \nhave an expression E, and it has a read R in a sub-expression. If R is misspeculated and has to be re-executed, \nthis will cause E to be re-executed as well. 4.2.1 Violation candidate The source of a cross-iteration \ntrue data-dependence edge is called the violation candidate 1 . A violation candidate will introduce \nsome misspeculation cost if it is not in the pre-fork region. On the other hand, if the violation candidate \nis in the pre\u00adfork region, it is executed sequentially before the speculative thread starts. It is not \npart of the parallel execution and the amount of parallelism is reduced. 4.2.2 Cost graph construction \nThe control-flow and data-dependence graphs are used to construct the cost graph. The cost graph is initialized \nwith the set of violation candidates and their cross-iteration dependence edges. Then nodes in the dependence \ngraph that can be reached by the dependence edges and their intra-iteration dependence edges are added \nto the cost graph recursively. To estimate the misspeculation cost more accurately, the head of a cost \ngraph edge is an operation, rather than a statement2. Except the initial set of cost graph nodes, each \nsubsequent node added to the cost graph represents an operation. Conceptually the initial set of cost \ngraph nodes represents the statements in the main thread that may cause misspeculation in the speculative \nthread. The subsequent portion of the graph represents the propagation effect of any misspeculation and \nre\u00adexecution within the speculative thread iteration. Each edge X.Y in the cost graph is annotated with \na probability p. This is the conditional probability that a re-execution at X will cause Y to be misspeculated \nand re-executed, given that X is misspeculated. 4.2.3 Computation of re-execution probability The misspeculation \ncost of a given partition is calculated based on an estimate of the re-execution probability of each \noperation in the speculative thread. This value is updated during the misspeculation cost computation. \nThe following is an algorithm to estimate the re-execution probability of each node in the cost graph. \nSteps 1 and 2 are preparation steps which are performed once for a given loop candidate. 1. We calculate \nthe violation probability of each violation candidate node in the cost graph. The violation probability \nis the probability that the result of the corresponding statement is modified within an iteration. For \na violation candidate, its violation probability means how often the main thread will reach it and modifies \nits results, thus initiating re-executions in the next but speculatively executed iteration. 2. Topologically \nsort all nodes in the cost graph. 3. For each node corresponding to a violation candidate not in the \npre-fork region, initialize its re-execution probability to be its violation ratio.  1 Each violation \ncandidate is an SSA statement in our representation. 2 In terms of actual implementation, each cost graph \nnode corresponds to an operation (Coderep) rather than a statement (Stmtrep) in ORC s SSA representation. \n4. The remaining nodes in the cost graph are visited in the topologically sorted order. We compute the \nprobability that a node c is re-executed due to re-execution of any of its predecessors as follows: a. \nInitialize x = 0; b. For each predecessor node p of c,  x = 1 - (1 - x)*(1 - r*v(p)), where v(p) is \nthe re-execution probability of node p, and r is the dependency probability on edge p->c (as described \nin Section 4.2.2). c. Set the re-execution probability of c to be x For a node dependent on multiple \npredecessors, this algorithm assumes that the re-execution probabilities of different nodes are independent \nof one another. So this algorithm only approximates the misspeculation cost. 4.2.4 Computation of misspeculation \ncost The misspeculation cost of the given SPT partition is computed as: .v(c)*Cost(c) c The summation \nis for all nodes c in the cost graph (excluding those for the violation candidates), v(c) is the re-execution \nprobability of node c and Cost(c) is the amount of computation in node c. 4.2.5 An example A C0.2 0.2 \nF D Figure 5: A dependence graph example Figure 5 shows a data dependence graph of a loop body. For clearer \nillustration, we assume there is no branch statement in the loop body. All solid lines represent intra-iteration \ndependences while the dashed lines represent cross-iteration true data dependences. The dependence probability \nfor the edges without annotation is 1. The cost graph is shown in Figure 6. D , E and F are the pseudo \nnodes for violation candidates D, E and F respectively. Let us consider an SPT partition in which only \nD is in the pre-fork region. D E F 0.1 0.2 A B 0.2 0.5 C F D E Figure 6: Cost graph for the dependence \ngraph in Figure 5. The re-execution probabilities of the pseudo nodes for the violation candidates are \ninitialized. v(D ) = 0 (since D is in pre-fork region) v(E ) = v(F ) = 1 (since there is no branch) The \nre-execution probability algorithm then calculates the re\u00adexecution probabilities of other nodes in topological \norder. v(A) = 1 (1 0.2*v(D )) = 0 v(B) = 1 (1 0.1*v(E )) = 0.1 v(C) = 1 (1 0.5*v(B))(1 0.2*v(F )) \n= 0.24 v(D) = v(F) = 0 v(E) = 0.24 Assuming all nodes have cost of one, the misspeculation cost of the \nSPT partition is 0.58.   5. OPTIMAL LOOP PARTITION As described in Section 3, one key algorithm in \nour cost-driven framework is to determine an optimal SPT loop partition for a given loop. In this section, \nwe describe how we formulate the problem and how we search for an optimal partition. We formulate the \noptimal loop partitioning problem as an optimization problem: To find a legal loop partition such that \nits misspeculation cost is minimum under the constraint that the pre\u00adfork region size is no more than \na given threshold. Legal partitions are those which maintain program correctness. Any partition that \nmaintains all forward intra-iteration dependence edges is a legal partition of the loop. A partition \nthat causes a forward intra-iteration edge to become backward is illegal3. The fork region size threshold \nis imposed to limit the amount of sequential execution, thus allowing some minimum degree of parallelism \nafter the speculative parallelization. A branch and bound search algorithm is used to find the optimal \npartition. Search efficiency is one of our major concerns. We reduce the search space by focusing on \nviolation candidates only. We construct a violation candidate dependence graph (VC-dep graph) to facilitate \nthe search. Furthermore, the search tree is pruned more effectively by observing that both the misspeculation \n3 This restriction can be relaxed when intra-thread speculation is allowed. cost and size of pre-fork \nregion are monotone functions of the set of statements in the pre-fork region. When additional statements \nare moved into the pre-fork region, the misspeculation cost will be reduced (compared to the partition \nprior to the move) and the size of the pre-fork region becomes larger. 5.1 Construction of VC-dep graph \nA violation-candidate dependence graph (VC-dep graph) is built from the data dependence graph. The node \nset of the VC-dep graph is the set of violation candidates. A node N in the VC-dep graph is a successor \nof another node S iff the corresponding violation candidate for N is directly or indirectly dependent \non the corresponding violation candidate for S in the data dependence graph. Only intra-iteration data \ndependences are considered. All nodes in the VC-dep graph are topologically sorted before the search \nstarts. That is for any two nodes u and v in the graph, if u is dependent on v, u must have a larger \ntopological order number than v.  5.2 The optimal search algorithm The search algorithm starts with \nan empty pre-fork region. In each step, we will add one node of the VC-dep graph into the pre\u00adfork region \nif all its predecessors have been added into the pre- A  C 0.1 0.2 E D E F Figure 7: VC-dep graph \nfor the dependence graph in Figure 5 Let us consider the dependence graph example in Figure 5 again. \nFigure 7 shows the corresponding VC-dep graph. There are three violation candidates D, E and F. E is \na successor of D in the data dependence graph.   Figure 8: Search space. Each node represents a pre\u00adfork \nregion which completely defines a partition. Figure 8 gives the search space formed by searching all \nthe different pre-fork regions. To avoid searching the pre-fork region {D, E, F} multiple times, a constraint \nis added. At each step, only the node with larger topological order number could be added into the pre-fork \nregion. 5.2.1 Pruning heuristics To speedup the search, we adopt two pruning heuristics: 1. When a pre-fork \nregion s size exceeds the given threshold, there is no need to search down the search tree. 2. Suppose \nthe current search node has a pre-fork region  R. Because we never move the nodes in current post\u00adfork \nregion that has smaller topological order number than those in R into the pre-fork region when searching \nthe offspring nodes of the current search node, we could estimate the lower bound cost for all offspring \nnodes in the search space. If this lower bound is already larger than the minimum misspeculation cost \nfound so far, there is no need to search successors of the current node.  Figure 9: Search space pruning. \nThe dash edge is pruned. Figure 9 illustrates the application of heuristic 1 applied to the search of \nthe search space in Figure 8. The search is pruned at node {D, E, F} because the size of its pre-fork \nregion already exceeds the threshold. The search algorithm still has exponential time complexity. To \navoid exceedingly long compilation time, loops with too many violation candidates are skipped. Our experiments \nshow that only a few loops in Spec2000Int have more than 30 violation candidates and got skipped due \nto this reason.   6. SPT LOOP SELECTION &#38; TRANSFORMATION After the first compilation pass, we obtain \nall optimal partitions and their associated optimal misspeculation costs for all loop candidates. In \nthe second compilation pass, our speculative parallelization examines all loop candidates together (such \nas all nesting levels of a loop nest) and select all those good SPT loops that are likely to deliver \nperformance gain and does the final SPT transformation to generate SPT code. This section describes the \nfinal SPT loop selection and transformation. 6.1 SPT loop selection criteria We select SPT loops based \non the following criteria: 1. Misspeculation cost. The optimal misspeculation cost of the loop must be \nsmaller than a predefined threshold which is a fraction of the loop body size. This attempts to limit \nthe potential performance loss due to misspeculation. 2. Pre-fork region size. The size of the pre-fork \nregion is less than a predefined threshold which is a fraction of the loop body size. This limits the \nsequential execution portion in the speculative parallel execution. The same threshold is used in the \nfirst pruning heuristic (Section 5.2.1). 3. Loop body size. Loop body size is a fundamental characteristic \nof an SPT loop. It shows how far ahead of the current execution a speculative thread starts the speculative \nexecution. It also indicates roughly the size of the speculative thread.  It should be larger than a \npredefined threshold. If the loop body size is too small, the performance gain of speculative parallelism \nwill not be enough to compensate for the overhead of forking a thread.  It should be smaller than a \nmachine-dependent threshold. Hardware resources can only support speculative execution of limited size. \n  4. Iteration count. A small iteration count (especially a number smaller than 2) means the next iteration \nis not likely to be executed and any speculative thread is likely to be killed. We avoid speculatively \nparallelizing such loops.  6.2 SPT loop transformation After the SPT loop selection, the original loop \nbodies of the selected loops are transformed into SPT loops using the optimal partition results obtained \nearlier. To simplify the transformation procedure, the CFG of original loop is duplicated with empty \nbasic blocks as the initial CFG of the pre-fork region. Then the statements are moved from the original \nloop body (which becomes the post-fork region) into the pre-fork region according to the optimal partition \nresult. After that, the pre-fork and post-fork regions are connected together with the SPT_FORK statement \ninserted between them. There are two complications in the SPT loop transformation. One is how to deal \nwith overlapped live ranges after code reordering. The other one is to deal with code motion of partial \nconditional statements. i_3=phi(i_1,i_2) (1) i_3=phi(i_1,i_2) (1) i_2=i_3+1; (3) foo(i_3); (2) SPT_FORK(loopid) \ni_2=i_3+1; (3) foo(i_3); (2) (a) Original loop (b) SPT loop partition Figure 10: Life-range overlap after \npartition After code reordering, the life ranges of different definitions of the same variable may be \noverlapped. For example, in Figure 10, suppose the partition result requires statements (1) and (3) to \nbe moved into the pre-fork region. After the code is moved, we find that the life range of i_2 overlaps \nwith the life range of i_3. This is not permitted in the SSA form in ORC. temp_var=i_1; start_of_loop: \ni_3=temp_var; temp_i_3=i_3; i_2=i_3+1; (3) temp_i_2=i_2; temp_var=i_2; SPT_FORK(loopid) i_3 =temp_i_3; \nfoo(i_3 ); (2) i_2 =temp_i_2; Figure 11: Temporary variable insertion to avoid the life\u00adrange overlap \nin Figure 10. (Code is in SSA form with phi-nodes not shown.) The above problem is solved by introducing \ntemporary variables to break the life ranges. Figure 11 shows the immediate results after the temporary \nvariables are inserted. The temporary variable temp_i_2 is used to avoid the life-range overlap within \nthe loop body while the temporary variable temp_var carries the cross\u00aditeration definition (3). After \nthe above code motion and temporary variable insertions, the code is immediately cleaned and optimized \nby applying SSA renaming, copy propagation and dead code elimination in ORC. When there is a branch statement \ninside the loop body and some code control-dependent on the branch statement is to be moved into the \npre-fork region, the branch statement needs to be copied into the pre-fork region as well. In Figure \n12, the partial conditional statement if (x<y) s++; is moved into the pre-fork region. The branch statement \nif (x<y) is replicated so that the transformed code maintains the correct control flow in both the pre-fork \nand post-fork regions. temp_cond=(x<y); if (temp_cond) { if (x<y) { s++; } s++; foo(); } SPT_FORK(loopid) \nif (temp_cond) { foo(); } Figure 12: Moving a partial conditional statement into the pre-fork region \n 7. SPT-ENABLING TECHNIQUES While the core SPT speculative parallelization attempts to identify and \ntransform the best SPT loops, other SPT-enabling techniques are needed to expose more speculative parallelism \nto the compiler. Our compilation framework can be easily extended to support and use some of these techniques. \nThis section describes three techniques which our framework has incorporated loop unrolling, software \nvalue prediction and data-dependence profiling. 7.1 Loop unrolling Our current SPT compilation is focused \non generating speculative threads for next iterations of a loop. It is therefore important that the loop \nbody size be sufficiently large to compensate for the overheads of the speculative thread execution. \nLoop unrolling is a useful technique to increase the body size of a loop. The SPT compilation does loop \nunrolling as follows. Before the Loop Nested Optimization (LNO) phase, based on the loop body size and \nthe minimum SPT loop body size requirement, the SPT compilation module decides if a loop should be unrolled \nand what the unroll factor will be. A loop-unrolling pragma is inserted into the loop s header. We modified \nLNO to recognize this pragma and perform the required loop unrolling (including outer loop unrolling). \nLoop unrolli ng is always enabled in all our experiments. Since ORC can only unroll DO loops in LNO, \nmany l oops are still not unrolled automatically in the current compiler. 7.2 Software value prediction \nWe have developed a software value predic tion (SVP) technique to predict critical values purely in software \nwithout any hardware support. Based on our misspeculation cost model, the compiler identifies critical \ndependences that cause unacceptably high misspeculation cost. The compiler then instruments the program \nto profile the value patterns of the corresponding variables. If the values are found to be predictable, \nand both the corresponding value-prediction overhead and the mis-prediction cost are acceptably low, \nthe compiler inserts the appropriate software value prediction code to generate the predicted values. \nIt also generates the software check and recovery code to detect and correct potential value mis-prediction \n[6]. while (x) { pred_x = x; foo(x); while (x) { ;x=bar(x) x; x = pred_ } 2; pred_x = x + id); SPT_FORK(loop \n) Original loop (a x); foo(x) x = bar( _x) { if (x != pred pred_x = x; } } Speculative parallel loop(b) \nFigure 13: Speculative parallelization with software value prediction  Figure 13 gives a softw are value \nprediction example. The compiler profiles the values for x. We assume that a pattern is identified whereby \nx is often incremented by 2 by bar(x). A check-and-recovery code is also inserted at the end of the loop \niteration. If the actual value of x is different from the predicted value during execution time, pred_x \nis corrected with the right value. In our cost-model, x=bar(x) is a violation candidate which cannot \nbe moved to the pre-fork region because of the legality constraints imposed by code reordering. SVP provides \na way to overcome this limitation so the loop can become an SPT loop. 7.3 Data-dependence profiling \nAs described in Section 4, data dependence probability is an essential basis for speculation. Data dependence \nprofiling is one important means to obtain reliable dependence probability information. Our current SPT \ncompilation includes a data-dependence profiling tool to complement the static type-based memory disambiguation \nanalysis in ORC. The profiling is done offline. The results are used during pass 1 compilation (together \nwith the reaching probability information of the control-flow graph) to annotate both intra-iteration \nand cross-iteration true data dependence edges. These edge probabilities are in turn used to annotate \nthe edges in the cost graph (Section 4.2.2). The goal is to better estimate the re-execution probabilities \nthat reflect the runtime behavior, and in the process, be able to parallelize more loops with more accurate \nand hopefully lower misspeculation costs. In order to use the data-dependence instrumentation and profile \nfeedback results, we only need to modify the annotation of data\u00addependence probabilities of the dependence \ngraph to take the profiling input. There was no change to the underlying cost computation module.  \n8. RESULTS AND LESSONS We evaluated our framework by generating SPT code for an SPT architecture [15] \nand running the generated code on a simulator. The simulated machine is a tightly-coupled multiprocessor \nwith one main core and one speculative core. Each processor core is an in-order Itanium2-like core. The \ncores have their own register files but share the memory/cache hierarchy. The main core always executes \nthe main thread. Speculative threads must run on the speculative core. Both processor pipelines, branch \npredictors and the cache hierarchy are simulated. The memory/cache hierarchy has the same configuration \nand latencies as the Intel s Itanium2 systems. Branch misprediction penalty is 5 cycles. The minimum \noverheads to fork and commit a speculative thread are 6 and 5 cycles respectively. To evaluate our speculative \nparallelization framework, we compiled 10 Spec2000Int benchmarks4 using our SPT compiler, ran the generated \ncode on the simulator and collected performance data. All compilation, both for the non-SPT base reference \ncode and for the different versions of SPT code, used O3 level optimization, profiled guided optimization \nand type\u00adbased alias analysis. The generated code was simulated using an in-house trimmed down input \nset that is derived from the SPEC 4 The remaining two Spec2000Int benchmarks (eon and perlbmk) were not \nevaluated because they failed to run on our simulator. Eon requires C++ library supports and perlbmk \nrequires additional system call supports. reference input set but runs about 5% as long while exhibiting \nsimilar program behavior. All simulation runs ran until program completion. Table 1 shows the performance \nof the non-SPT base reference code on a single core. The IPC (instruction per cycle) numbers shown exclude \nnop instructions. All speedup numbers reported in this section are based on comparing the execution time \nof the generated SPT code against the execution time of the non-SPT base reference code. Table 1: IPC \n(excluding nops) of the non-SPT base reference Program IPC Program IPC bzip2 1.69 mcf 0.44 crafty 1.49 \nparser 1.30 gap 1.30 twolf 1.05 gcc 1.33 vortex 0.56 gzip 1.77 vpr 1.22 Three sets of SPT code were \nevaluated. The first set of SPT code was generated by the basic compilation which used our cost model \nand all basic SPT optimizations (such as code reordering and loop unrolling). This basic compilation \nused only control flow edge profiling. The second set of SPT code was generated by the current best compilation \nwhich in addition to the basic compilation applied software value prediction (SVP) and data dependence \nprofiling feedback. The third set of SPT code was generated by the current best compilation plus manual \napplication of a few additional enabling techniques which were not yet implemented. These enabling techniques \ninclude while-loop unrolling, privatization and the export of global variables beyond their visible scopes. \nWe anticipate that the last set of SPT code is what our compiler can achieve when those enabling techniques \nare in place. We refer to this last set of code as the result of the anticipated best compilation. Their \nperformance results are reliable indicators of what can be practically achieved. Figure 14 summarizes \nour evaluation results. It shows that simple techniques such as loop unrolling and code reordering are \nnot enough to achieve good performance. The basic compilation achieves only 1% average speedup as compared \nto 8% in the best compilation and 15.6% in the anticipated best compilation. This also shows that only \ncontrol profiling and type-based alias analysis are not enough to identify speculative parallelism. Software \nvalue prediction is an important SPT-enabler because it both helps to reduce misspeculation cost and \nenables more code reordering. The anticipated compilation results are encouraging. It shows that our \nframework is able to perform aggressive speculative parallelization when more speculative parallelism \ngets exposed. Note that in this work, the current compiler does not introduce intra-thread speculation \nto exploit more inter-thread speculative parallelism. We now discuss the results of the current best \ncompilation in detail. Figure 15 shows the breakdown of loops with respect to whether the loops can \nbe SPT transformed and the reasons why they cannot be transformed under our framework. The fraction labelled \nas Valid Partition are the percentage of loops that satisfies the SPT loop selection criteria. For those \nthat cannot be transformed, only a few loops failed because of too many violation candidates. 35% of \nthe loops cannot be transformed because they have either too small iteration count or too large body \nsize. We note that 34% of the loops are not transformed because their loop bodies are too small. These \nloops are while loops. Our current compiler can only unroll DO loops to increase their body sizes. As \nindicated in our anticipated best compilation result, while-loop unrolling is one important enabling \ntechnique.  Figure 16 shows the runtime coverage of the loops and the number of speculative parallel \nloops. Runtime coverage refers to the percentage of total program execution cycles being spent in the \nSPT loops. Our current best compilation is able to generate SPT loops to cover 30% of the total program \nexecution cycles. Compared to the 68% maximum coverage of all loops with the same maximum loop size limit5, \nour compiler does a decent job, successfully realizing 40% of the potential opportunity. On average, \nonly 30 SPT loops were generated per benchmark. This indicates that a few hot loops were selected and \ntransformed. There are still ample opportunities this is not surprising with the One main goal of our \ncompiler framework is to be able to speculatively parallelize only loops with low misspeculation costs. \nFigure 18 shows the actual performance characteristic of the SPT loops generated by the current best \ncompilation. On average, the misspeculation ratio is only 3% while the speedup over the original loop \nis about 26%. anticipated best compilation results.  Figure 17 shows the average loop body size and \nthe general characteristic of the partitions of the SPT loops. On average, a speculative parallel loop \nexecutes about 400 instructions per plots, for each loop, the compiler-estimated misspeculation cost \nagainst its actual re-execution ratio. The re-execution ratio of a loop is the fraction of computation \nof a loop iteration that is re\u00adexecuted due to misspeculation. Figure 19 shows that the costs and re-execution \nratios are generally well-correlated, except that the estimated costs tend to be conservative and over-estimate \nthe iteration (i.e., the loop body size). The pre-fork region has about 5 The maximum loop size limit \nused in the experiments is 1000 re-execution ratio (as the data is more clustered near the y-axis.) Figure \n19 also shows some loops near to the x-axis. Our analysis shows that the discrepancies come from function-calls \ninside instructions. these loops, which will modify and use some global variables unknown to the caller \nloops. This points an area for improvement in the cost estimation. 9. FUTURE WORK AND CONCLUSIONS The \nemerging hardware support for thread-level speculation opens new opportunities to parallelize sequential \nprograms beyond the traditional limits. To take full advantage of this new execution model, a program \nneeds to be programmed or compiled in such a way that it exhibits a high degree of speculative thread\u00adlevel \nparallelism. We propose a comprehensive cost-driven compilation framework to perform speculative parallelization. \nBased on a misspeculation cost model, the compiler aggressively transforms loops into optimal speculative \nparallel loops and selects only those loops whose speculative parallel execution is likely to improve \nprogram performance. The framework also supports and uses enabling techniques such as loop unrolling, \nsoftware value prediction and dependence profiling to expose more speculative parallelism. The proposed \nframework was implemented on ORC compiler. Our evaluation showed that the cost-driven speculative parallelization \nwas effective. Our compiler was able to generate good speculative parallel loops in ten Spec2000Int benchmarks, \nwhich currently achieve an average 8% speedup. We anticipate an average 15.6% speedup when all enabling \ntechniques are in place. We noted in Figure 15 that there are loops which are not transformed because \neither their body size is too large, or their iteration count is too small. Such cases can be handled \nif we generalize our work to perform speculative parallelization for general code regions. For example, \na speculative thread may be forked for a section of the loop body within the same iteration, or for a \nsection of code after the loop body. 10. ACKNOWLEDGMENTS Our thanks to Jesse Fang for his encouragement \nand support of this work. 11. REFERENCES [1] Michael K. Chen and Kunle Olukotun. TEST: A tracer for extracting \nspeculative threads. Proceedings of 2003 Intl Symposium on Code Generation and Optimization, San Francisco, \nCA, Mar 2003. [2] Michael Chen and Kunle Olukotun. The Jrpm system for dynamically parallelizing Java \nprograms. Proceedings of the 30th Annual Symposium on Computer Architecture, Jun 2003. [3] M. Franklin. \nThe Multiscalar Architecture. PhD thesis. University of Wisconsin at Madison, 1993. [4] L. Hammond, B. \nHubbert, et al. The Stanford Hydra CMP. IEEE Micro, Volume.20 No.2, Mar. 2000, pp. 71--84. [5] Yuan-Shin \nHwang, Peng-Sheng Chen, Jenq Kuen Lee, Roy Dz-Ching Ju. Probabilistic points-to analysis. Lanaguages \nand Compilers for Parallel Computing, 14th Intl Workshop, LCPC 2001, Cumberland Falls, KY, Aug 2001, \npp. 290-305. [6] Xiao-Feng Li, Zhao-Hui Du, Qingyu Zhao, and Tin-Fook Ngai. Software value prediction \nfor speculative parallel threaded computations. The First Value-Prediction Workshop, San Diego, CA, June \n7, 2003, pp. 18-25. [7] Jin Lin, Tong Chen, Wei-Chung Hsu, Pen-Chung Yew, Roy Dz-Ching Ju, Tin-Fook Ngai \nand Sun Chan. A compiler framework for speculative analysis and optimizations. Proceedings of the ACM \nSigplan 2003 Conference on Programming Language Design and Implementation, San Diego, CA, Jun 2003, pp. \n289-299. [8] Open Research Compiler for Itanium Processor Family. http://ipf-orc.sourceforge.net/ [9] \nJ. G. Steffan, C. B. Colohan, A. Zhai, and T. C. Mowry. A scalable approach to thread-level speculation. \nProceedings of Intl Symp. On Computer Architecture 2000, pp. 1-24. [10] J. Tsai, J. Huang, C. Amlo, D. \nLilja, and P. Yew. The Superthreaded processor architecture. IEEE Transactions on Computers, Volume \n48, Number 9, September 1999, pp. 881-902. [11] Jenn-Yuan Tsai, Zhenzhen Jiang and Pen-Chung Yew. Compiler \ntechniques for the superthreaded architectures. Intl Journal of Parallel Programming, 27(1), 1999, pp \n1-19. [12] T. N. Vijaykumar. Compiling for the Multiscalar Architecture. PhD thesis. Computer Science \nDepartment, University of Wisconsin at Madison, Jan 1998. [13] A. Zhai, C.B. Colohan, J.G. Steffan and \nT. C. Mowry, Compiler optimization of scalar value communication between speculative threads. The Tenth \nInternational Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-X), \nSan Jose, CA, USA, Oct 7-9, 2002. [14] F. Chow, S. Chan, S. Liu, R. Lo, and M. Streich. Effective representation \nof aliases and indirect memory operations in SSA form. In Proc. of the Sixth Int l Conf. on Compiler \nConstruction, pages 253--267, April 1996. [15] Xiao-Feng Li, Zhao-Hui Du, Chen Yang, Chu-Cheow Lim, Qingyu \nZhao, William Chen and Tin-Fook Ngai. Speculative parallel threading architecture and compilation. Submitted \nto the 9th Asia-Pacific Computer Systems Architecture Conference, 2004.   \n\t\t\t", "proc_id": "996841", "abstract": "The emerging hardware support for thread-level speculation opens new opportunities to parallelize sequential programs beyond the traditional limits. By speculating that many data dependences are unlikely during runtime, consecutive iterations of a sequential loop can be executed speculatively in parallel. Runtime parallelism is obtained when the speculation is correct. To take full advantage of this new execution model, a program needs to be programmed or compiled in such a way that it exhibits high degree of speculative thread-level parallelism. We propose a comprehensive cost-driven compilation framework to perform speculative parallelization. Based on a misspeculation cost model, the compiler aggressively transforms loops into optimal speculative parallel loops and selects only those loops whose speculative parallel execution is likely to improve program performance. The framework also supports and uses enabling techniques such as loop unrolling, software value prediction and dependence profiling to expose more speculative parallelism. The proposed framework was implemented on the ORC compiler. Our evaluation showed that the cost-driven speculative parallelization was effective. Our compiler was able to generate good speculative parallel loops in ten Spec2000Int benchmarks, which currently achieve an average 8% speedup. We anticipate an average 15.6% speedup when all enabling techniques are in place.", "authors": [{"name": "Zhao-Hui Du", "author_profile_id": "81414607304", "affiliation": "Intel China Ltd., Beijing, China", "person_id": "P677811", "email_address": "", "orcid_id": ""}, {"name": "Chu-Cheow Lim", "author_profile_id": "81100398423", "affiliation": "Intel Corporation, Santa Clara, CA", "person_id": "PP14140956", "email_address": "", "orcid_id": ""}, {"name": "Xiao-Feng Li", "author_profile_id": "81388599257", "affiliation": "Intel China Ltd., Beijing, China", "person_id": "P677809", "email_address": "", "orcid_id": ""}, {"name": "Chen Yang", "author_profile_id": "81451593298", "affiliation": "Intel China Ltd., Beijing, China", "person_id": "PP95033643", "email_address": "", "orcid_id": ""}, {"name": "Qingyu Zhao", "author_profile_id": "81100535636", "affiliation": "Intel China Ltd., Beijing, China", "person_id": "P677804", "email_address": "", "orcid_id": ""}, {"name": "Tin-Fook Ngai", "author_profile_id": "81100430856", "affiliation": "Intel Corporation, Santa Clara, CA", "person_id": "P282900", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/996841.996852", "year": "2004", "article_id": "996852", "conference": "PLDI", "title": "A cost-driven compilation framework for speculative parallelization of sequential programs", "url": "http://dl.acm.org/citation.cfm?id=996852"}