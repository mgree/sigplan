{"article_publication_date": "06-04-2011", "fulltext": "\n A Case for an SC-Preserving Compiler DanielMarino AbhayendraSingh* ToddMillstein MadanlalMusuvathi \nSatishNarayanasamy* University ofCalifornia,LosAngeles * University ofMichigan,AnnArbor MicrosoftResearch,Redmond \nAbstract The most intuitive memory consistency model for shared-memory multi-threaded programming is \nsequential consistency (SC). How\u00adever, current concurrent programming languages support a re\u00adlaxed model, \nas such relaxations are deemed necessary for en\u00adabling important optimizations. This paper demonstrates \nthat an SC-preserving compiler, one that ensures that everySCbehavior of a compiler-generated binary \nis an SC behavior of the source pro\u00adgram, retains most of the performance bene.ts of an optimizing compiler. \nThe key observation is that a large class of optimizations crucial for performance are either already \nSC-preserving or can be modi.edtopreserveSC while retaining much oftheir effectiveness. An SC-preserving \ncompiler, obtained by restricting the optimiza\u00adtion phases in LLVM, a state-of-the-art C/C++ compiler, \nincurs an average slowdown of 3.8% and a maximum slowdown of 34% on a set of 30 programs from the SPLASH-2, \nPARSEC, and SPEC CINT2006benchmark suites. While the performance overhead of preserving SC in the com\u00adpiler \nis much less than previously assumed, it might still be un\u00adacceptable for certain applications. Webelieve \nthere are several av\u00adenuesforimprovingperformance withoutgiving upSC-preservation. Inthisvein,we observethatthe \noverhead ofourSC-preserving com\u00adpiler arises mainlyfromitsinabilityto aggressivelyperform a class of \noptimizations weidentify as eager-load optimizations.This class includescommon-subexpression elimination,constantpropagation, \nglobal value numbering, and common cases of loop-invariant code motion. We propose a notion of interference \nchecks in order to enable eager-load optimizations while preserving SC. Interference checks expose to \nthe compiler a commonly used hardware spec\u00adulation mechanism that can ef.ciently detect whether a particular \nvariablehas changedits value sincelast read. Categories and Subject Descriptors D.3.2 [Programming Lan\u00adguages]: \nLanguage Classi.cations Concurrent, distributed, and parallellanguages; D.3.4[Programming Languages]:Processors \nOptimization General Terms Languages,Performance Keywords memory consistency models, sequential consistency, \nSCpreservation,interference checks Permission to make digital or hard copies of all or part of this work \nfor personal or classroomuseisgranted withoutfeeprovided that copiesarenot madeordistributed forpro.tor \ncommercial advantage andthat copiesbearthis notice andthefull citation onthe .rstpage.Tocopy otherwise,torepublish,topostonservers \nortoredistribute tolists, requiresprior speci.cpermission and/or afee. PLDI 11, June4 8,2011,SanJose,California,USA. \nCopyright c &#38;#169; 2011ACM978-1-4503-0663-8/11/06. . .$10.00 1. Introduction Amemory consistency \nmodel(or simply memory model)de.nesthe semantics of a concurrentprogramming language by specifying the \norderin which memory operationsperformedby onethreadbecome visible to other threads in the program. The \nmost natural memory modelis sequential consistency(SC)[31].UnderSC,theindividual operations of the program \nappear to have executed in a global sequential order consistent with theper-threadprogram order.This \nsemantics matches theintuition of a concurrentprogram sbehavior as a set ofpossible threadinterleavings. \nItis commonlyacceptedthatprogramminglanguages must relax the SC semantics of programs in order to allow \neffective compiler optimizations. This paper challenges that assumption by demon\u00adstrating an optimizing \ncompilerthat retains most oftheperformance of the generated code while preserving the SC semantics. A \ncom\u00adpileris saidtobe SC-preserving ifeverySCbehavior of agenerated binaryisguaranteed tobe anSCbehavior \nof the sourceprogram. StartingfromLLVM[32], a state-of-the-artC/C++compiler, we obtain an SC-preserving \ncompiler by modifying each of the op\u00adtimization passes to conservatively disallow transformations that \nmight violate SC.1 Our experimental evaluation (Section 3) indi\u00adcates that the resulting SC-preserving \ncompiler incurs only 3.8% performance overheadon average over the originalLLVM compiler with all optimizations \nenabled on a set of 30 programs from the SPLASH-2[49],PARSEC[7], andSPECCINT2006(integer com\u00adponent ofSPECCPU2006[26]) \nbenchmark suites.Moreover, the maximum overheadincurredby anyofthesebenchmarksisjust over 34%. 1.1 AnOptimizingSC-PreservingCompiler \nTheempirical observation of thispaperisthatalargeclassof opti\u00admizationscrucialforperformance areeitheralreadySC-preserving \nor can be modi.ed topreserve SC while retaining much of their ef\u00adfectiveness. Several common optimizations, \nincluding procedure inlining, loop unrolling, and control-.ow simpli.cation, do not change the order \nof memory operations and are therefore natu\u00adrallySC-preserving.Other common optimizations, such as common \nsubexpression elimination (CSE) and loop-invariant code motion, can have the effect of reordering memory \noperations. However, these optimizations can still be performed on accesses to thread\u00adlocal variables \nand compiler-generated temporary variables. The analysis required to distinguish such variables is simple, \nmodular, and is already implemented by modern compilers such as LLVM. Furthermore, transformationsinvolving \na single shared variable are alsoSC-preserving under special cases(Section2). Consider the instance of \nCSE in Figure 1, where the compiler eliminates the subexpression X*2. By reusing the value of X read \nat L1 in L3, this transformation effectively reorders the second ac\u00adcess to X with the access to Y at \nL2. While invisible to sequential 1The SC-preserving version of LLVM is available at http://www.cs. ucla.edu/~todd/research/memmodels.html. \n Original Transformed Concurrent Context L1: t=X*2; L1: t=X*2; N1: X =1; L2: u =Y; . L2: u =Y; N2: Y \n=1; L3: v=X*2; M3: v=t; (a) (b) (c) Figure1:A compilertransformationfromprogram(a) into(b) that elim\u00adinates \nthe common subexpression X*2. In the presence of a concurrently runningthread(c) and aninitial statewhereall \nvariables arezero,(b) can observe a state u== 1 &#38;&#38; v == 0, which is not visible in(a).Lowercase \nvariablesdenotelocal temporaries,whileuppercasevariables arepotentially shared. programs, this reordering \ncanintroduce non-SCbehaviorsin a con\u00adcurrentprogram, as shown inFigure 1.However, an SC-preserving compiler \ncan stillperform thistransformation aslong as atleast one of X and Y is known to be thread-local. If \nX is thread-local, then its valuedoes not changebetween L1 and L3 and so thetransformation isSC-preserving.Onthe \notherhand,ifY isthread-localthen anySC execution ofthetransformedprogram canbe showntobe equivalent to \nanSC execution of the originalprogramin whichinstructions L1 to L3 execute withoutbeinginterleaved withinstructionsfrom \nother threads. By carefully enabling transformations only when they are SC-preserving, our compileris \nable to achieveperformance compa\u00adrable to a traditional optimizing compiler while retaining the strong \nSCsemantics.  1.2 ProvidingEnd-to-EndProgrammerGuarantees Providing end-to-end SC semantics to theprogrammer \nrequires ex\u00adecuting the output of an SC-preserving compiler on SC hardware. The empirical results in \nthis paper complement recent architecture research [8, 15, 23, 24, 28, 44] that demonstrates the feasibility \nof ef.cient SC hardware. The basic idea behind these proposals is to speculatively reorder memory operations \nand recover in the rarecasethatthesereorderings canbecome visibletootherproces\u00adsors.While such speculation \nsupport necessarilyincreaseshardware complexity, we hope that our work on an SC-preserving compiler increases \nthe incentives for building SC hardware, since in combi\u00adnation they enable end-to-end SC semantics for \nprogrammers at a reasonable cost. Eveninthe absence ofSChardware, thetechniquesdescribedin this paper \ncan be used to provide strong semantics to the program\u00admer.Forinstance, when compiling to x86hardware, \nwhich supports the relatively-strong total store order (TSO) memory model [40], a compiler that preserves \nTSO behavior (Section 7.2) provides TSO semantics at the programming language level. The result is a \nlanguage-level memory model that is stronger and simpler than the current memory-modelproposalsforC++[6,11] \nandJava[36].  1.3 SpeculativeOptimizationForSC-Preservation While the cost of anSC-preserving compileris \nmuch less thanpre\u00adviously assumed, one possible concern is that some applications might be unwilling \nto pay this cost, however small. We argue that one should exhaustpossible avenuesforimproving theperformance \nof SC-preservation, such as more sophisticated static and dynamic analyses, before exposing a relaxed \nprogram semantics to the pro\u00adgrammer. In this vein, we observe that a number ofdisabled optimizations \nresponsible for lostperformance in our SC-preserving compiler in\u00advolve an eager load.Forinstance, the \nelimination of the expression X*2 inFigure1canbe considered asperformingtheload of variable X eagerly \natline L1 insteadof at L3.Other eager-load optimizations include constantpropagation, copypropagation,partial-redundancy \nelimination, global value numbering, and common cases of loop- L1: t= X*2; L1: t=X*2; L2: u=Y; L2: u= \nY; . M3: v= t; L3: v = X*2; C3: if(X modified since L1) L3: v=X*2; (a) (b) Figure2:Performingcommon subexpression \nelimination whileguaranteeing SC.Theinterference check at C3 ensures that the value of X has not changed \nsince last read at L1. This allows the compiler to reuse the value of X*2 computedin L1 without violating \nSC. invariant code motion. Our experiments show that fully enabling these eager-load optimizations in \nour compiler reduces the maxi\u00admum slowdown of anybenchmark from34% to6.5%. To enable eager-load optimizations \nwithout violating SC, we propose the use ofcompiler-inserted interference checks todynami\u00adcallyensurethecorrectness \nofoptimizationsthat cannotbe statically validated asSC-preserving(Section4).Figure2demonstratesthis idea. \nThe .gure shows the code from Figure 1(a) and its transfor\u00admation with an interference check. For the \nCSE optimization to be sequentially valid, the compiler already ensures that the variable X is not modi.edbyinstructionsbetween \nL1 and L3.Theinterference check lifts this correctness requirement to concurrent programs by ensuring \nthat no other threadhas modi.ed X sincelast read at L1.If the check succeeds, theprogram can safely reuse \nthe earlier compu\u00adtation of X*2;ifnot, theprogram reverts to the unoptimized code. Our interference checks \nare inspired by a common hardware speculation mechanism[23] thatis used to safely strengthenhard\u00adware \nmemory models. This mechanism allows a processor to track cache-coherence messages to conservatively \ndetect when a partic\u00adular memory location may have been modi.ed by another proces\u00adsor.We observe that \nthis speculation mechanism canbe used todis\u00adcharge our interference checks ef.ciently. Wedescribe a simplein\u00adterfacefor \nexposing this capabilitytothe compiler,based ontheIta\u00adnium architecture s design of a similarfeature[29](Section5).We \nhave built a hardware simulator supporting our speculation mecha\u00adnism andhaveperformed a simulation study \non15parallelprograms from the SPLASH-2 and PARSEC benchmarks. By incorporating interference checks into \na single optimization pass, we reduce the average performance overhead of our SC-preserving compiler \non simulated TSO hardware from 3.4% to 2.2% and reduce the maxi\u00admum overheadfrom23% to17%(Section6). \n2. CompilerOptimizationsasMemory Reorderings In this section, we classify compiler optimizations based \non how they affect the memory reorderings of theprogram[2,47]. 2.1 SC-PreservingTransformations Informally, \nwe representthe(SC) behaviors of aprogram as a set of interleavings of the individual memory operations \nof program threads that respect theper-threadprogram order.A compiler trans\u00adformationisSC-preservingifeverybehavior \nofthetransformedpro\u00adgramis abehavior ofthe originalprogram.Notethatitis acceptable for a compiler transformation \nto reduce the set ofbehaviors. Transformations involving thread-local variables and compiler\u00adgenerated \ntemporaries are always SC-preserving. Furthermore, some transformations involving a single shared variable \nare SC\u00adpreserving[47].For example,if aprogramperformstwo consecu\u00adtive loads of the same variable, as \nin Figure 3(a), the compiler can remove the second load. This transformation preserves SC as any execution \nof the transformed program can be emulated by an in\u00adterleaving of the original program where no other \nthread executes a) redundantload: t=X; u=X; . t=X; u=t; b)forwardedload: X=t; u=X; . X=t; u=t; c)dead \nstore: X=t; X=u; . X=u; d) redundant store: t=X; X=t; . t=X;  Figure3:SC-preserving transformations \nu = X*X; for(...){ for(...){ L1: X = 1; L1: X = 1; ... ... L2: P = Q; . L2: P = Q; P = Q; . P = Q; L3: \nt = X; L3: t = 1; t = X*X; t = u; ... ... } } (a) (b) Figure4:Examplesofeager-load optimizationsincludeconstant/copyprop\u00adagation \n(a) and loop-invariant code motion (b). Both involve relaxing the L . L and S . L ordering constraints. \nbetween the two loads. On the other hand, this transformation re\u00adduces the set of behaviors, as the behavior \nin which the two loads seedifferent valuesis notpossible after the transformation. Similar reasoningcan \nshowthatthe othertransformations shown inFigure3 are alsoSC-preserving.Further, a compiler canperform \nthese transformations even when the two accesses on the left-hand sideinFigure3 are separatedbylocal \naccesses, sincethose accesses areinvisible to other threads.  2.2 OrderingRelaxations Optimizations \nthatare notSC-preserving change the order of mem\u00adory accessesperformedby one threadin a manner that canbecome \nvisible to other threads. We characterize these optimizations based on relaxations of the following ordering \nconstraints among loads and stores that theyinduce: L . L, S . L, S . S, and L . S. Consider the CSE \nexample in Figure 1(a). This optimization involves relaxing the L . L constraint between theloads at \nL2 and L3,moving thelattertobeperformed right afterthe .rstload of X at L1, and eliminatingit using the \ntransformationinFigure3(a).If the example contained a store,instead of aload, at L2, thenperforming CSE \nwould have involved an S . L relaxation. We classify an optimization as an eager load if it only involves \nL . L and S . L relaxations, as these optimizations involves performing a load earlierthanit wouldhavebeenperformedbeforethetransformation. \nAnother example of an eagerload optimizationis constant/copy propagation as shown in Figure 4(a). In \nthis example, the transfor\u00admationinvolves moving theload of X toimmediately after the store of X (whichrequires \nL . L and S . L relaxation with respect to the P and Q accesses) and then applying the transformation \nin Fig\u00adure 3(b). The loop-invariant code motion example in Figure 4(b) involves eagerly performing the \n(possibly unbounded number of) loads of X within the loop once before the loop. This also requires relaxing \nL . L and S . L ordering constraintsdue to the store and load to shared variables P and Q respectively. \nFigure 5 shows examples of optimizations that are not eager loads. The dead-store elimination example \nin Figure 5(a) involves relaxing the S . S and S . L constraints by delaying the .rst store and then \napplying the SC-preserving step of combining the adjacent stores as in Figure 3(c). Figure 5(b) shows \nan example of a redundant store elimination that involves eagerly performing the store of X by relaxing \nthe L . S and S . S ordering constraints and then applying the transformationinFigure3(d). X = 1; ; t \n= X; t = X; P = Q; . P = Q; P = Q; . P = Q; X = 2; X = 2; X = t; ; (a) (b) Figure5:(a)Dead store elimination \ninvolves relaxing the S . S and S . L constraints. (b) Redundant store elimination involves relaxing \nthe L . S and S . S constraints. 3. AnSC-PreservingModi.cationtoLLVM This section describes the design \nand implementation of our opti\u00admizing SC-preserving compiler on top of LLVM and evaluates the compiler \ns effectiveness in terms of performance of the generated code versus that of thebaselineLLVM compiler. \n3.1 Design Asdescribedintheprevious section, we can characterize eachcom\u00adpiler optimization s potential \nfor SC violations in terms of how it reorders memory accesses.InordertobuildourSC-preserving com\u00adpiler, \nwe examined each transformation pass performed by LLVM anddetermined whetherornotit couldpotentially \nreorderaccesses to shared memory.Wefurther classi.ed thesepassesbased on what types of accesses mightbe \nreordered. Perhaps surprisingly, many of LLVM s passes do not relax the order of memory operations at \nall and these SC-preserving passes canbeleft unmodi.ed.Thesepassesinclude sparse conditional con\u00adstant \npropagation, dead argument elimination, control-.ow graph simpli.cation, procedure inlining, scalar replication, \nallocation of function-local variables to virtual registers, correlated value propa\u00adgation,tail-callelimination, \narithmetic re-association,loop simpli.\u00adcation,loop rotation,loop unswitching, loop unrolling, unreachable \ncode elimination, virtual-to-physical register allocation, and stack slot coloring. OtherLLVMoptimizations \ncan relaxthe order ofmemory opera\u00adtions.Table1liststhese optimizationpasses and classi.es thekinds of \nrelaxations that arepossible in each.To ensure that the compiler wouldbeSC-preserving, wedisabled afewofthesepasses \nand mod\u00adi.edthe remainingpassesto avoidreordering accessestopotentially shared memory.  3.2 Implementation \nOur compiler does not perform any heavyweight and/or whole\u00adprogram analyses to establish whether or not \na location is shared (e.g., thread-escape analysis). Rather we use simple, conservative, local information \nto decide if a location is potentially shared. Dur\u00ading anearlyphase of compilation,LLVM convertsloadsand \nstores of non-escapingfunction-local variablesinto reads and writes of vir\u00adtual registers.Operations \non these virtual registers canbe freely re\u00adordered. In certain situations, structures that are passed \nby value to a function are accessed using load and store operations. Our com\u00adpiler recognizes these situations \nand allows these memory opera\u00adtions to be reordered in any sequentially valid manner. In addition, shared \nmemory operations maybe reordered withlocal operations. Thus, for instance, we can safely allow the instcombine \npass to transform t=X; t+=u; t+=X; into t=X \u00ab 1; t+=u; when both t and u arelocal variables. Incorporating \nour modi.cations to LLVM was a fairly natural and noninvasive change tothe compiler code.LLVM already \navoids reordering and removing loads and stores marked asbeing volatile. Therefore, in the IR optimization \npasses we were often able to use existing code written to handle volatiles in order to restrict optimizations \non other accesses to shared memory. The primary mechanism by which we avoided reordering during the x86 \ncode generation passes was by chaining memory operations to one      ShortName Description L . L \nL . S S . L S . S SCVersion LLVMIROptimization Passes x86CodeGeneration Passes seldag Builds the initial \ninstruction selection DAG. Performs some CSEduring construction. yes no no no modi.ed nodecombine Performs \nforms of CSE, constant folding, strength reduction, store-to\u00adload forwarding, and dead store elimination \non the selection DAG. Can reduce atomicity of certain operations; for instance a store of a 64-bit .oat \nthat can be done atomically on some architectures maybe changed to two 32-bit integer stores. Also, bit-masking \ncode may be recognized and changed to smaller operations without masking. This can have the effect of \nreordering a store with prior loads. yes yes no no modi.ed  another in program order in the instruction \nselection DAG. This indicatestothe scheduler andotherpassesthatthereis adependence from each memory operation \nto the next and prevents them from being reordered.  3.3 Example The exampleinFigure6helpsillustrate \nwhyanSC-preserving com\u00adpiler can stilloptimizeprograms effectively.The source code shown in part (a) \nof the .gure is a simplifed version of a performance\u00adintensive function in one of our benchmarks. The \nfunction calcu\u00adlates the distance between two n-dimensional points represented as(possibly shared) arraysof \n.oatingpointvalues.Inadditionto performing the .oating point operations that actually calculate the distance, \ndirectly translating thisfunction into x86 assembly would allocate space on the stack for the locally \ndeclared variables and performfouraddress calculationsduring eachiterationof theloop. Each address calculation \ninvolves an integer multiply and an inte\u00adger add operation ashintedby the commentsinFigure6(a).Our SC-preserving \ncompiler is able to perform a variety of important optimizations on this code: Sincethelocallydeclared \nvariables(including theparameters) do not escape thefunction, they canbe storedin registers rather than \non the stack.  CSEcanbe used to removetwo ofthe address calculations since they are redundant and onlyinvolvelocals. \n Loop-induction-variable strength reduction allows us to avoid the multiplication involved in the two \nremaining address calcu\u00adlations by replacing the loop variable representing the array in\u00addex withaloop \nvariable representing an address offsetthat starts at zero andisincrementedby4 eachiteration.  Usingloop-invariantcode \nmotion(and associativity of addition), we canincrement the array addressesdirectlyduring eachitera\u00adtionratherthanincrementing \nanoffset andlateraddingittothe base addresses.  The .nal result of applying the above SC-preserving \noptimizations isshowninpart(b) ofthe .gure(usingC syntaxratherthanx86 assembly).Thefully optimizing compiler \nthatdoes notpreserveSC is able to perform one additional optimization: it can use CSE to eliminatetheredundant \n.oatingpointloadsand subtractionineach iteration of theloop.The resulting codeis showninpart(c) of the \n.gure.  float Distance( float Distance( float Distance( float* x, float*y, int n){ float* x, float* \ny, int n){ float* x, float* y, int n){ float sum = 0; register float sum = 0; register float sum = 0; \nint i=0; register px = x; register px = x; register py = y; register py = y; for(i=0; i<n; i++){ register \nrn = n; register rn = n; sum += (x[i]-y[i]) *(x[i]-y[i]); for(; rn-->0; px+=4,py+=4){ for(; rn-->0; \npx+=4,py+=4){ // Note: x[i] is *(x+i*4) sum += (*px-*py) register t = (*px-*py); // and y[i] is *(y+i*4) \n*(*px-*py); sum += t*t; }}} return sqrt(sum); return sqrt(sum); return sqrt(sum); }}} (a) (b) (c) Figure6:Exampledemonstrating \nthe allowed optimizationsin anSC-preserving compiler.Thefunctionin(a) computes thedistancebetween two \nn-dimensional points x and y represented as arrays.AnSC-preserving compileris ableto safelyperform avariety \nof optimizations,leading tothe versionin(b).However,it cannot eliminate the common-subexpression *px \n-*py involving possibly-shared accesses to the array elements. A traditional optimizing compiler does \nnot havethisrestriction andisabletogeneratetheversionin(c). No optimization Na\u00efve SC-preserving SC-preserving \n 149 169.5 487 150.0% 120.0% 90.0% 60.0% 30.0% 0.0%  Figure7:Performance overheadincurredbyvarious compiler \ncon.gurations compared tothe stockLLVM compiler with -O3 optimization forSPECCINT2006 benchmarks. 3.4 \nEvaluation We evaluated our SC-preserving compiler on a variety of sequen\u00adtial and parallel benchmarks. \nEven though the topic of this paper only concerns multi-threadedprograms, weincluded the sequential benchmarks \nin our evaluation as optimizing compilers are tuned to perform well for these benchmarks. Our experimental \nresults indi\u00adcatethatthe vast majority ofthe optimizationsinLLVM responsible forgoodperformance areinfactSC-preserving. \nWeexecuted allprograms on anIntelXeon machine with eight cores, each of which supports two hardware threads \nand 6 GB of RAM. We evaluated each program under three compiler con.gu\u00adrations. The con.guration No optimization \nis the stock LLVM compiler with all optimizations disabled; Na\u00a8ive SC-preserving enables only those LLVM \npasses that are already SC-preserving, because they never reorder accesses to shared memory; and SC\u00adpreserving \nis our full SC-preserving compiler, which includes modi.ed versions of someLLVMpasses. Figure7shows the \nresultsfortheSPECCINT2006benchmarks. The .gure shows the performance overhead of each benchmark under \nthe three compiler con.gurations, normalized to the bench\u00admark s performance after being compiled with \nthe stock LLVM compiler and all optimizations enabled (-O3). With no optimiza\u00adtions, the benchmarks incur \nan average 140% slowdown. Re\u00adenablingjustthe optimizationsguaranteedtopreserveSC reduces this overhead \nall the way to 34%. Our full SC-preserving compiler reduces the average overhead to only 5.5%, with a \nmaximum over\u00adheadfor anybenchmark of28%. The results for parallel applications from the SPLASH-2 and \nPARSECbenchmark suites are showninFigure10fromSection6 (the last two compiler con.gurations shown in \nthe .gure pertain to the notion of interference checks that we introduce in the next section).The results \nagree withthose of the sequentialbenchmarks. Withoutoptimizationsthebenchmarksincur an average153% slow\u00addown. \nRe-enabling na\u00a8ively SC optimizations reduces the over\u00adheadto22%, and ourfullSC-preserving compilerincurs \nan average overhead of only 2.7%, with a maximum overhead for any bench\u00admark of34%. 4. SpeculationforSC-Preservation \nAs shown in Table 1, most of the optimization passes that reorder shared memory accesses only relax the \nL . L and S . L or\u00adderings. In other words, these optimizations have the potential to perform eager loads \nbut no other memory reorderings. In order to evaluate how important these eager-load optimizations are \nfor per\u00adformance, wefully enabled thefour(SC-violating) eager-loadIR optimization passes in our SC-preserving \ncompiler and re-ran our parallelbenchmarks.The Only-Eager-Loads con.gurationinFig\u00adure 10 illustrates \nthe results. The benchmark with the largest over\u00adhead in our SC-preserving compiler, facesim, rebounded \nfrom a 34% slowdown to only a 6.5% slowdown, and many other bench\u00admarks regained of all of theirlostperformance. \nThis experiment motivates our desire to speculatively perform eager-load optimizationsand thendynamically \nrecoverupon apos\u00adsible SC violation in order to preserve SC. This section describes how our compiler \nperforms such speculation via a notion of inter\u00ad  DOM DOM ORIG CONTINUE . rcvr: ORIG i.chk monitoredAccesses, \njump cont RECOVER rcvr cont: CONTINUE Figure 8: Introducing interference checks when performing eager-load \ntransformations in ORIG, a single-entry, single-exit region of code with no stores. Either or both of \nDOM and ORIG contain the de.nitions for monitoredAccesses for the eager loadsinvolved in the transformation. \nference checks, which conservatively determine whether a memory location s valuehasbeen modi.ed sinceit \nwaslast readby the cur\u00adrentthread.Firstwe specifytheinstruction setarchitecture(ISA)ex\u00adtensions in the \nhardware that support interference checks. Then we showhow a compiler can usethese newinstructionsto \nspeculatively perform eager-load optimizations, and we argue for the correctness of the approach. 4.1 \nISAExtensions Interference checks rely on three new instructions to be provided by the architecture: \nm.load (monitored load), m.store (moni\u00adtored store), and i.check (interference check). The m.load and \nm.store instructions behave as regular loads and stores but addi\u00adtionally instruct theprocessor to start \nmonitoringpossible writes to the memory location being accessed. We assume that theprocessor can monitor \nupto a maximum of N locations simultaneously.These instructions therefore take as an additional parameter \na tag from 0 to N - 1, which is used as an identi.er for the memory location being monitored. The i.check \ninstruction provides a mechanism to query the hardware as to whether or not writes could have occurred \nto a set of memory locations. It accepts an N -bit mask and a recovery branch target as aparameter.Theinstruction \nconditionallybranches to the recovery target based on whether or not writes may have occurred for any \nof the monitored memory addresses indicated by the mask. If the instruction does not branch, it is guaranteed \nthat no thread has written to any of the locations indicated by the mask since the instructions that \ninitiated their monitoring were executed. When usingan i.check inthe examplesbelow, we willlistthetags \nexplicitlyfor clarity rather than using abit mask. Note that our use of tags toidentify accesses, rather \nthan simply identifyingthem withthe addresstheyaccess, allowsthe compilerto safely use interference checks \nin the face of potential aliasing. The compiler may end up monitoring two accesses to the same location \nusing separate tags due to unknown aliasing. The hardware will correctly report interference between \nthe time when the monitored access for each tag was executed and the time of the i.check for that tag. \nThis design places the burden on the compiler to manage the resources available for monitoring. It must \nensure that when it reuses a tag, the access that was previously assigned to that tag no longer needs \ntobe monitored. 4.2 InterferenceCheckAlgorithm Figure8illustrateshow our compilerperforms eagerloadoptimiza\u00adtions \nwith interference checks. Informally, the algorithm works on codeinStaticSingleAssignmentform(SSA)inthefollowing \nsteps: 1. Find a contiguous, single-entry, single-exit block of code with\u00adout stores.Call thisblock ORIG. \n 2. Createabranch target atthe .rstinstructionafter ORIG.Call the followinginstructions, starting at \nthis new target, CONTINUE.  3. Make a copy of ORIG in its entirety and call it RECOVER. Note that, since \nwe are manipulating SSA code, all local and tempo\u00adrary values willbegiven a newSSA namein the copied \ncode. 4. Apply eager-load transformations in ORIG and call the resulting block of code ORIG . The transformations \nmay include any combination of thefollowing: (a) Eliminate a load and replace its uses with a value \nfrom a previous load or store to that address that dominates the currentload.Thisprior memory access \nmay or may notbein ORIG.Convert thisprevious memory access to an m.load or m.store ifitis notalready \none.If multiplede.nitions reach theload tobe removed, all of themhave tobe converted. (b) Hoist a load \nfrom ORIG to a position dominating all of its uses, potentially reordering with previous load and/or \nstore operations. Its new position may or may not be in ORIG. Convert thehoistedload to an m.load.  \n We ll call the code that dominated ORIG and may now contain monitored instructions DOM . Each access \nthat is converted to a monitored instruction must use adistinct tag, so the compileris limitedto at most \nN eager-load conversions in this step. 5. Perform any desired SC-preserving optimizations on the code \nremainingin ORIG . 6. Insert an i.check instruction after ORIG that checks forinter\u00adference on all accesses \nthat were marked as monitoredby step4 andbranches to the recovery code onfailure. 7. For all values \nthat arelive-out of ORIG, transform CONTINUE by inserting an SSA phi instruction at the beginning choosing \nthe appropriatevaluebased onwhethercode .owedfrom ORIG or RECOVER.Call the transformedblock CONTINUE \n.  4.3 ImplementationandExample We modi.edLLVM sglobal value numbering(GVN)pass to make use of interference \nchecks in order to allow more aggressive opti\u00admization whilemaintainingSC.TheGVNpassperforms a variety \nof eager-load optimizations,including CSE,partial redundancy elimi\u00adnation, and copy/constant propagation. \nDue to time limitations, we have notimplemented the algorithm on otherpasses. Figure9shows someLLVMIR \ncode that calculates X2 + Y + X2 , along with the transformations that takeplace onitduring theGVN pass \nin order to eliminate the redundant computation of . Virtual X2 registers, or temporaries, are pre.xed \nby the % symbol and are in SSAform.First,theGVNpass removes the secondloadof memory locationX (whichde.nes%5)and \nreplaces allofitsuses withthe .rst load of X. After this load elimination, we are left with the code \nin (b), whereitis clear that the secondmul instruction is unnecessary, soitis removed andits useis replaced \nwiththepreviously calculated valuein virtual register %2.The .nal code withtheload and multiply eliminatedisshownin(c).Figure9(d) \nshowshowour algorithm addsinterference checkstomakethistransformationSC-preserving.  4.4 Correctnessof \ntheAlgorithm We now arguethatour algorithmforinsertinginterference checksis SC-preserving. First consider \nthe case when the interference check fails.NeitherORIG nor ORIG contains any stores.Thus,the state of \nnon-local memory does not change during the execution of ORIG . As the code is in SSA form, all the effects \nof ORIG on local state become dead once the code switches to RECOVER, which is a copy of ORIG. Hence, \nother than needlessly executing ORIG , the transformedprogramhas the samebehavior as the originalprogram \nwhen theinterference checkfails. Now consider the case when the interference check succeeds. This means \nthat each monitored memory location is guaranteed to  // DOM %1 = m.load X, 0 %2 = mul %1, %1 // DOM \n%1 = load X %2 = mul %1, %3 = load Y %4 = add %2, //ORIG %5 = load X %6 = mul %5, //CONTINUE %7 = add \n%4, %1 %3 %5 %6 . // %1 %2 %3 %4 // %6 // %7 DOM = load X = mul %1, = load Y = add %2, ORIG = mul %1, \nCONTINUE = add %4, %1 %3 %1 %6 . // %1 %2 %3 %4 // // %7 DOM = load X = mul %1, = load Y = add %2, ORIG \nCONTINUE = add %4, %1 %3 %2 %3 = load Y %4 = add %2, %3 // ORIG i.check 0, rcvr jump cont // RECOVER \nrcvr: %5 = load X %6 = mul %5, %5 // CONTINUE cont: %merge = phi (orig, %2, rcvr, %6) %7 = add %4, %merge \n (a) (b) (c) (d) Figure9:GVN.rsttransformsprogram(a) into(b) by eliminatingthe availableload from X, \nthen notices that the result of the second multiplication has alreadybeen computed andperforms common \nsubexpression eliminationto arrive at(c).Thistransformationis notSC sinceit reordersthe secondload of \nX with theload of Y. be unmodi.ed from the start of monitoring through the execution of ORIG . The key \nproperty of our algorithm is that every memory locationinvolvedin an eagerloadis monitoredfromthepointwhere \nthe eagerload occurs until atleastthepoint at whichtheload would have occurredin the originalprogram(sinceitwouldhave \noccurred somewhere within ORIG). Thus the value loaded in the optimized codeisthe valuethat wouldhavebeen \nreadby the originalprogram, therebypreservingSC. 5. HardwareSupportforInterferenceChecks In this section \nwe describe hardware support for ef.ciently imple\u00admenting the m.load, m.store, and i.check instructionsdescribed \nin the previous section. The hardware changes required are simple and ef.cient, andthereforepractical.Infact,the \nnewinstructions we propose are similar to the data speculation support in the Itanium s ISA[29],which \nwasdesignedto enable speculative optimizationsin asinglethreadinthefaceofpossible memory aliasing.Ourdesign \nsafely supportsboth ourgoal(to ensure sequential consistency) as well asItanium s speculativeload optimizations.Our \nrequiredhard\u00adware support is simple: a structure to store N addresses(32in our implementation), each \nwith an associatedbitindicating whether the address waspossibly written. 5.1 HardwareDesign We propose \na hardware structure called the Speculative Memory Address Table (SMAT) which is similar to the Advanced \nLoad AddressTable(ALAT) usedinItaniumprocessors[29].SMATis a Content-Addressable-Memory(CAM).Ithas N \nentries, enabling the compiler to monitor interference on N addresses at any instant of time. Each entry \nin the SMAT contains an address .eld and an interferencebit. We collectively refer to m.load and m.store \ninstructions as monitor instructions. As described in the previous section, each monitor instruction \ncontains a tag between 0 and N - 1. When executing a monitor instruction, the hardware stores the address \naccessed by that instruction in the SMAT entry speci.ed by the tag, resets that entry s interference \nbit, starts to monitor writes to the address, and executes the memory operation requested by the instruction. \nA processor core can easily detect when another processor core wants to write an address by monitoring \ninvalidation coherence requests.Whenaprocessor corereceivesaninvalidationtoacache block, the interference \nbit of each SMAT entry holding an address from that block is set. The interference bit of an entry is \nalso set when a store to the associated address commits from the current processor.Whilethelatterbehavioris \nnotnecessary topreserveSC, itenablesItanium-style speculativeload optimizations[29]. The compiler generates \nan i.check instruction with an N \u00adbit mask to check for interference on a maximum of N different addresses. \nEach bit in the mask corresponds to an entry in the SMAT.Thehardware executesthei.check instructionbychecking \ntheinterferencebits oftheSMATentries speci.edinits mask.If any of the checked interference bitsis set, \nthehardware branches to the recovery code whose targetis speci.edin the i.check instruction. The hardware \nupdates the SMAT for a monitor instruction and executes i.check instructions only when they are ready \nto com\u00admit from a processor core s instruction window. This ensures that thehardwaredoes not updateSMAT \nentries speculatively while ex\u00adecuting instructions on an incorrect path taken due to branch mis\u00adprediction.The \nnextsection explains a subtletyinimplementing the monitorinstructionsin out-of-orderprocessors.  5.2 \nRelationToIn-WindowHardwareSpeculation Our approach of monitoring invalidation coherence requests to \nde\u00adtectinterference for a set of addresses is similar to what manypro\u00adcessors already implement for ef.ciently \nsupporting TSO at the hardwarelevel[23].TSOdoes notallow aloadtobe executedbefore another load in program \norder even if they are accessing different addresses.To achievegoodperformance,Gharachorloo et al.[23] \nproposed to speculatively execute loads out-of-order. However, in\u00adstructions are still committedin orderfrom \naFIFOqueue called the reorderbuffer(ROB).Therefore, todetect misspeculation thehard\u00adware simply needs \ntodetect when anotherprocessor triesto writeto an address that has been read by a load that is yet to \ncommit from the ROB. This is achieved by monitoring the address of invalida\u00adtion coherence requests from \nother processor cores. On detecting a misspeculation,thehardware .ushesthemisspeculatedload andits followinginstructionsfrom \nthepipeline and restarts execution.  Our proposed hardware design essentially extends the above hardware \nmechanism to detect interference for addresses of certain memory operations(speci.edby the compiler) \neven afterthey are committed from the ROB. This allows our compiler to eagerly executeloads andlater \ncheckforinterference atthe originallocation of the load in the source code. On a m.load, the monitoring \nneeds to start logically when the processor receives the value of the load. However, the SMAT entry is \nupdated only when the instruction is committed. In between the two events, when the load instruction \nis in .ightintheROB,werely onthemonitoringperformed aboveto provide the requiredsemantics of the i.check. \n 5.3 ConservativeInterferenceChecks While an implementation of interference checks must detect inter\u00adference \nwhenever it occurs, it is legal to signal interference when none actually exists. Such false positives \nare acceptable in our de\u00adsign because they simply result in execution of the unoptimized code, losing \nsome performance but maintaining SC. The ability to tolerate false positives allows us to avoid a number \nof potentially complexissues andkeep thehardware simple. First, our hardware monitors interference at \nthe cache block granularity as coherence invalidation messages operate at cache block level. This may \nresult in false positives when compared to a detector that monitors byte-level access. But the probability \nthat a cache block gets invalidated between a monitor instruction and an i.check is very low. Moreover, \nfrequent invalidations or false sharing of hot cache lines result in performance degradations and thus \ncan expected tobe rarein well-tuned applications. Second, we conservatively invalidate SMAT entries for \na cache blockthatgets evicteddueto capacityconstraints.Monitoringinter\u00adference for uncached blocks would \nrequire signi.cant system sup\u00adport(similarin complexityto unbounded transactional memory sys\u00adtems[17]),but \nwebelieveitis not necessary forperformance. Third,inISAslikex86 one memoryinstruction couldpotentially \naccess two or more cache lines, but our SMAT entry can monitor only one cacheblock address.To address \nthisproblem,if a monitor instruction accesses more than one cacheblock weimmediately set theinterferencebitfor \ntheSMAT entry that monitors the associated address, which could cause a future i.check tofailforcing \nexecu\u00adtion down an unoptimized path. Fortunately, such unaligned cache accesses are rare. Finally, a \ncontext switch may occur while multiple addresses are monitored in the hardware SMAT. Instead of virtualizing \nthis structure, we propose to set the interference bit in all SMAT en\u00adtries after a context switch.This \nmay causefuture i.check instruc\u00adtions from the same thread to fail unnecessarily when it is context switched \nback in, but we expect this overhead to be negligible as context switches are relatively rare when compared \ntothefrequency of memory accesses. 6. Results The experimental results relating to the performance of \nour SC\u00adpreserving compiler were discussed in Section 3.4. In this section we discuss additional experiments \nwhich evaluate the potential ef\u00adfectiveness of our interference checks. In addition, we compare the performance \nof our SC-preserving compilers to a fully optimizing compiler running on simulatedhardware that uses \naDRF0 memory model whichis more relaxed(allows morehardware reorderings) thanTSO.Thisgives a sense oftheperformanceburden \nofproviding a strong, end-to-end memory model acrosshardware and software. 6.1 CompilerCon.gurations \nAsdescribedinSection3.4, ourbaseline compileris the out-of-the\u00adboxLLVM compiler with all optimizations(-O3). \nFor our experi- Table 2: Baseline IPC for simulated DRF0 hardware running binaries from the stockLLVM \ncompiler.    Application Avg.IPC Application Avg.IPC blackscholes 1.94 bodytrack 1.61 .uidanimate \n1.28 swaptions 1.67 streamcluster 1.42 barnes 1.57 water(nsquared) 1.66 water(spatial) 1.66 cholesky \n1.78 fft 1.39 lu(contiguousblocks) 1.64 radix 0.99 ments on parallel benchmarks, we used the three compiler \ncon.g\u00adurationsdiscussedinthatsection( Nooptimization , Na\u00a8iveSC\u00adpreserving , and SC-preserving ), as \nwell as two additional con\u00ad.gurations. The Only Eager Loads con.guration includes all the optimizations \nfrom the SC-preserving compiler plus the unmodi\u00ad.ed(SC-violating) version of allIRpassesthatperformonly \neager load optimizations (GVN, instcombine, argpromotion, and jump\u00adthreading). This con.guration isintended \ntogive a sense of the op\u00adportunity for improvement available to optimizations based on our interference \ncheck technique and is only used for experiments on native hardware and not on simulated machines. Finally, \nthe SC\u00adpreserving+GVN w/ ICheck con.guration includes all of the op\u00adtimizations from the SC-preserving \ncompiler plus a modi.ed GVN pass that is made SC-preserving using our interference checks and recovery \ncode.Whenthis con.guration targets a simulated machine with appropriate support, it emits m.load, m.store, \nand i.check instructions. But when it targets native hardware, the con.guration emits m.load and m.store \ninstructions as regularloads and stores and emulates a never-failing i.check using alogical comparison \nof constant values followed by a conditional branch. Thus, when run\u00adning on the native machine, the overhead \ncaused by increased code size and the additionalbranchis captured,but the effect of actual or false con.icts \non monitored accesses is not. In a real implementa\u00adtion, however, we expect the i.check instruction to \nbe more ef.\u00adcient than abranch. 6.2 Benchmarks We evaluated the performance of the various compiler \ncon.gu\u00adrations on the PARSEC [7] and SPLASH-2 [49] parallel bench\u00admark suites. Table 2 lists the average \ninstructions executed per cy\u00adcle (IPC) for each of these benchmarks when compiled with the stock LLVM \ncompiler at -O3 optimization and run on our simu\u00adlated DRF0 hardware which implements weak consistency \nand is described below. All of these benchmarks are run to completion. For our experiments on actual \nhardware, we used the native in\u00adputforPARSECbenchmarks, whilefor the simulated machines we used the sim-medium \ninput set tokeep the simulation time reason\u00adable.(Since streamcluster was especially slow to simulate, \nwe used the sim-small input.) For SPLASH-2 applications, we used thedefaultinputsfor simulation.We modi.ed \ntheinputs toincrease the problem size for experiments on native hardware. We veri.ed the correct behavior \nof the benchmarks under all compiler con.g\u00adurations by using a self-testing option when available, or \nby com\u00adparing results with those produced when compiling the benchmark usinggcc.  6.3 ExperimentsonNativeHardware \nWeevaluatedallsix compiler con.gurations(includingthebaseline) on an Intel Xeon machine with eight cores \neach of which supports twohardware threads and 6GB ofRAM.Eachbenchmark was run .ve times for each compiler \ncon.guration and the execution time was measured.(Theresultsgivenhere areforCPUusertime,though theresultsfortotal \ntime elapsed were very similar.)Theoverheads given are relative to thebaseline,fully-optimizing compiler \nand are showninFigure10.Let s consider thebaseSC-preserving compiler  Table3:ProcessorCon.guration \nProcessor 4 coreCMP.Each core operating at2Ghz.  4instructions(maximum2loads or1 store) Fetch/Exec/Commit \nwidth per cyclein eachcore. StoreBuffer TSO:64entryFIFObuffer with8bytegranularity. DRF0, DRFx: 8 entry \nunordered coalescing buffer with 64 byte granularity. L1Cache 64KBper-core (private), 4-waysetassociative, \n64B block size, 1-cycle hit latency, write-back. L2Cache 1MBprivate, 4-way set associative, 64Bblock \nsize, 10-cycle hit latency.  .rst. Notice that for many of our benchmarks, restricting the com\u00adpiler \nto perform only SC-preserving optimizations has little or no effect. In fact, in some cases, disabling \nthese transformations ap\u00adpears to speed the code up, indicating that the compiler ought not tohaveperformed \ntheminthe .rstplace.Thereareseveralbench\u00admarks,however,for which theSC-preserving compilerincurs a no\u00adticeableperformancepenalty,34%in \nthe case of facesim.2 On av\u00aderage, we see a 2.7% slowdown. Consider now the compiler con\u00ad.guration which \nre-enables various eager load optimizations. Sev\u00aderal ofthe applications which suffered a signi.cant \nslowdown under theSC-preserving compiler regain much of thisperformancein this con.guration. Most notably, \nfacesim vastly improves to6.5% and bodytrack, streamcluster, and x264 recover all(or nearly all) of theirlostperformance. \nOn average, the compiler with eagerload relaxations enabled is as fast as the stock compiler, indicating \nthat ourtechnique of usinginterference checkstosafelyallow eagerload optimizations holds signi.cant promise. \nFinally, the rightmost bar in the graph shows the slowdown of the aggressive SC-preserving compiler that \nincludes the modi.ed GVN pass with interference checks.(Remember,wearerunning onanativemachineinthisset \nof experiments, so a never-failload check is emulated.)We see that this technique regains agoodportion \nof theperformancelostby the base SC-preserving compiler for facesim, reducing the overhead from 34% to \nunder 20%, with streamcluster and x264 showing a more modestimprovement.  6.4 ExperimentsonSimulatedMachines \nTo study the performance of interference checks in hardware, we used a cycle-accurate, executiondriven,Simics[35] \nbased x86 64 simulator calledFeS2[19].We simulatedTSOhardware with and without support for interference \nchecks and compared it to DRF0 hardware that supports weak consistency. The processor con.gu\u00adration that \nwe modelled is shown in Table 3. For the TSO simu\u00adlation, we modelled a FIFO store buffer that holds \npending stores and retires themin-order.We also modelled speculativeload execu\u00adtion support[23].The weakly \nconsistentDRF0 simulation allowed stores andloads to retire out-of-order. 2Additional pro.ling and investigation \nrevealed that the slowdown in facesim was largely caused by a commonly invoked 3x3 matrix multiply routine.TheSC-preserving \ncompiler was unable to eliminate the two redun\u00addant loads of each of the 18 shared, .oating point matrix \nentries involved in the calculation. This resulted in 36 additional load instructions for each matrix \nmultiplication performed by the SC-preserving version of facesim. OurGVNpasswithinterference checksisableto \nrelegatethe36 additional loadstotherecovery code, eliminating them onthefastpath.A straightfor\u00adward rewrite \nof the source code to .rst read the 18 shared values into local variables would have allowed the base \nSC-preserving compiler to generate thefully optimized code. Figure11 shows the results of our simulation \nstudy. When com\u00adpared to thefully optimizing compiler con.guration running on the simulated DRF0 machine, \nthe performance overhead of using our SC-preserving compiler on simulated TSO hardware is 3.4% on average. \nThis cost is reduced to 2.2% when the GVN pass with interference checks is used. For several programs \nthat incur sig\u00adni.cant overhead, such as bodytrack and facesim, our interfer\u00adence check optimizations \nreduce the overhead to almost zero. For streamcluster, the overhead is reducedfrom about 23% to17%. We \nalsofound that the frequency ofload-check failuresis, on aver\u00adage, only about one in ten million instructions. \nThis indicates that theperformance overheaddue tofalsepositives arisingfrom several hardware simpli.cationsdescribedinSection5.3is \nnegligible. 7. DiscussionandRelatedWork 7.1 Relationship toData-Race-FreeMemoryModels Today s mainstream \nconcurrent programming languages including C++ and Java use variants of the data-race-free memory model \nknown asDRF0[2,3].These memory models are rootedin the ob\u00adservation that good programming discipline \nrequires programmers to protect shared accesses to data with appropriate synchroniza\u00adtion(such aslocks).For \nsuchdata-race-freeprograms, these models guaranteeSC.Forprograms withdata races,however, these models \nguarantee either no semantics[11] or a weak and complex seman\u00adtics[36].The compiler and hardware are \nrestrictedin the optimiza\u00adtions allowed across synchronization accesses, but they can safely perform \nmost sequentially-valid optimizations on regular memory accesses. Our work is primarily motivated by \nthe need to provide under\u00adstandable semanticsto racyprograms[1] bothfordebugging ease of large software \nsystems that are likely to have data races and for guaranteeingsecuritypropertiesin safelanguages such \nasJava[36]. Inthe context of aDRF0-compliant compiler(such asLLVM), our interference checks can be seen \nas a specialized form of data race detection. Our approach allows the compiler to speculatively performthe \neager-loadoptimizations which wouldbe allowedinthe DRF0 model. However, rather than silently providing \nunde.ned or weak semantics upon a data race, our interference checks dynami\u00adcallydetectthe race and recoverin \nordertopreserveSC.Inthis way, data-race-free programs can be aggressively optimized and incur just the \nadditionalcost of the interference check itself and the rare falseinterference check that may occur(Section5.3).Furthermore, \neven racyprogramsbene.tfrom our approach,because theinterfer\u00adence check captures exactly the requirement \nnecessary tojustify an optimization sSC-preservation.Forinstance,data races on variables that are not \neagerly loaded, as well as data races on eagerly-loaded variables that occur outside of the scope of \nthat reordering, cannot violateSCpreservation and so need notbedetected. While this paper focuses on \neager-load optimizations, we hope to explore infuture work speculation mechanisms that enable other DRF0-compliant \noptimizations whilepreservingSC.  7.2 ATSO-PreservingCompiler ProvidingSC semanticstotheprogrammer requiresthattheoutput \nof an SC-preserving compiler be executed on SC hardware. While most hardware platforms today support \na relaxed memory model, popular platforms such as Intel s x86 and Sun s SPARC platforms provide a relatively-strong \nmemory model known as total store or-dering(TSO).Conceptually,TSOcanbe seen as a relaxation ofSC that \nallows S . L reorderings[2] and has aprecise understandable operational semantics[40]. Our approach to \ndeveloping an SC-preserving compiler can be naturally adapted to instead preserve TSO, thereby providing \nend\u00adto-end TSO semantics on existing TSO hardware. Transformations  No optimization Na\u00efve SC-preserving \n Only-Eager-Loads SC-preserving+GVN w/Icheck 373.1 480 298 132.1 95.5 89.1 173.1 200 116.5 89.7 154 \n236.5 75.7 153.1 Figure 10: Performance overhead incurred by the various compiler con.gurations compared \nto stock LLVM compiler with -O3 optimization running on native XeonhardwareforPARSECandSPLASH-2benchmarks. \n Figure11:Performance overhead ofSC-preserving compiler on simulatedTSOhardware with and without usinginterference \nchecks relative tofully optimizing SC-violating compiler onsimulatedDRF0hardware. that do not reorder \nmemory accesses and those that only reorder thread-local variables areTSO-preserving[48].In addition, \nall of theSC-preserving transformations showninFigure3 alsopreserve TSOexceptfor redundant store elimination[14]. \nIn the context of a DRF0-compliant compiler, the result of this approach would be a language-level memory \nmodel that provides SC semantics for data-race-free programs and TSO semantics for racyprograms.This \nvariation onDRF0is signi.cantly stronger and much simplerthanboththeC++0x[6,11] andJava[36] memory models. \nInrecentwork, .c\u00b4et al.describeaconcurrency extensionto Sev.ika smallC-likeprogramminglanguage thatprovides \nend-to-endTSO semantics[48].They modify anexisting compilerforthelanguage and mechanically prove that \nthe optimizations areTSO-preserving, therebyproviding an end-to-endguarantee when the resultingbina\u00adries \nare executed on x86hardware.Ourperformance measurements complement their work by indicating that a TSO-preserving \ncom\u00adpiler could be practical to use in a full-.edged programming lan\u00adguage.  7.3 DynamicDetectionofDataRacesandSCViolations \nOthershave arguedforthe use ofdynamic racedetectiontoimprove theDRF0 memory model[10,16,18],in order \ntohalt an execu\u00adtion when its semantics becomes unde.ned, analogous with Java s fail-stop approach to \npreventing array-bounds violations and null\u00adpointerdereferences.However,detectingdata races eitherincurs8x \nor moreperformance overheadin software[20] orincurs signi.cant hardware complexity[4,38,43] despite manyproposed \noptimiza\u00adtions to thebasic technique. Recently,Marino et al.[37] andLucia et al.[34] observed that itsuf.ces \ntodynamicallydetectSC violations rather than races, and that this canbedone much more ef.ciently.In their \napproaches, the compiler partitions a program into regions and may perform most sequentially valid optimizations \nwithin a region but cannot opti\u00admize across regions. Region boundaries are communicated to the hardware \nas memory fences, thereby also preventing hardware op\u00adtimizations across regions. Given these requirements, \nthe hardware can conservatively identify data races that potentially cause SC vi\u00adolationsbydetecting \ncon.icting accessesin concurrently executing regions, similar to the con.ictdetectionperformed by transactional \nmemory(TM) systems[27].  Our interference checks are a form of dynamic data-race detec\u00adtion that is \nsuf.cient to ensure SC-preservation of compiler trans\u00adformations. Such detection provides a weaker guarantee \nthan the above approaches, which additionally detect SC violations due to hardware reorderings. However, \nour approach has a number of ad\u00advantages.First,ourdetectionschemeis .ne-grained,requiring only data race \ndetectionfor variables that are involvedin a compiler op\u00adtimization and only during the dynamic lifetime \nof that optimiza\u00adtion s effect. Second, we can perform this detection with relatively minimal hardware \nsupport based on existing hardware speculation mechanisms[50], rather than requiring the complexity ofTM-style \ncon.ict detection. Finally, we show how to safely recover from in\u00adterferenceforcommon compiler optimizationsbased \non eagerloads, thereby allowing the execution to safely continue while maintaining SC.  7.4 OptimisticOptimizationviaHardwareSpeculation \nOurinterference checks areinspiredby a common hardware mech\u00adanismforenabling out-of-order executioninthepresenceof \nstrong memory models[23].This mechanism[50] allows a memoryload tobe executed out-of-order speculatively,before \nearlierinstructions have completed. Once those instructions have completed, the load need not be re-executed \nif the value has not changed in the mean\u00adwhile, and this can be conservatively detected by checking if \nthe associated cache line has been invalidated. We illustrate how this technique canbe adapted to the \ncompilerby viewing common com\u00adpiler optimizations asperformingeager(i.e., speculative) reads, and we \ndescribe a simple way for the hardware to expose this mecha\u00adnism to the compiler. Others have proposed \nhardware support for dynamically de\u00adtecting memory aliasing between local loads and stores in a sin\u00adgle \nthread and expose that feature to the compiler so that it can perform optimistic optimizations [22, 41]. \nThe Itanium processor implemented this feature using an Advanced Load Address Table (ALAT)to enable aggressiveloadoptimizations[29].Recently,Na\u00adgarajan \nand Gupta [39] extended Itanium s ALAT mechanism to detect memory aliasing with remote writes, enabling \nthe compiler to speculatively reorder memory operations across memory barri\u00aders. While our hardware mechanism \nto detect memory aliasing is similar to theseproposals, we apply it to solve a differentproblem: preservingSCin \ntheface ofcommon compiler transformations.  7.5 GuaranteeingEnd-to-EndSequentialConsistency Our approach \nensures that the compiler is SC-preserving but does notprevent thehardwarefrom exposing non-SCbehavior.We \ncould augment our compiler to address thisproblembyinserting memory fences to prevent hardware reorderings \nthat potentially violate SC. Such fences cause a signi.cant performance penalty, so there has been researchin \nminimizing the number offences required.Shasha andSnirproposed the delay sets algorithmfordeterminingthe \nset of fencestoinsert[45].Recentresearchhasfurther reducedthenumber offences requiredbyincorporating \nanalysesthatdetect which mem\u00adorylocations arepossibly accessedby multiple threads[30,46].Fi\u00adnally, recent \nwork describes a new hardware mechanism called a conditional fence [33], whichuses the results of a compiler \nanaly\u00adsis to dynamically decide whether a given fence in the instruction stream can be safely ignored \nwhile still ensuring SC. All of these approaches rely critically on whole-program analyses to obtain \nsuf\u00ad.cientprecision. A different way to ensure SC at the language level is by stati\u00adcally rejecting possibly \nracy programs. Several static type systems havebeenproposed thatprevent races(e.g.,[12,13,21]) and en\u00adsure \nstrongerproperties such asdeterminism[9].By rejectingpo\u00adtentially racyprograms,thesetype systems ensurethatallprogram \nexecutionshaveSC semantics.However,thesetype systems enforce arestrictedprogramming stylethatisnecessarily \nconservative.For example, many static type systems for race detection only account for lock-based synchronization \nand will reject race-free programs that use other synchronization mechanisms. Further, even correct programsthat \nemploy locks canberejectedduetoimpreciseinfor\u00admation aboutpointer aliasing.Moreprecisionin staticracedetection \ncanbe achieved through whole-program analysis[42]. Hammond et al.[25] proposedthetransactional coherency \nand consistency(TCC) memory model.Theprogrammer and the com\u00adpiler ensure that every instruction is part \nof some transaction. The runtime usestransactional memory[27] to ensure serializability of transactions, \nwhichin turnguaranteesSC at thelanguagelevel.The Bulkcompiler[5] andtheBulkSChardware[15]together alsoguar\u00adantee \nSC at the language level. The bulk compiler partitions a pro\u00adgraminto chunks and theBulkSChardwareemploysspeculation \nand recovery to ensure serializable execution of chunks. Con.icts are resolved through rollback and re-execution \nof chunks. These techniques obtain a strongguarantee ofSC,but atthe cost of signif\u00adicant hardware extensions \nthat are similar to transactional memory support. 8. Conclusions A memory model forms the foundation \nof shared-memory multi\u00adthreaded programming languages. This paper empirically demon\u00adstrates that the \nperformance incentive for relaxing the intuitive SC semanticsinthecompilerismuchlessthanpreviously assumed.In \nparticular, this paper describes how to engineer an SC-preserving compiler through simple modi.cations \nto LLVM, a state-of-the-art C/C++ compiler.For a wide range ofprograms from theSPLASH\u00ad2, PARSEC, and \nSPEC CINT2006 benchmark suites, our SC\u00adpreservingcompiler resultsin aperformance overhead of only3.8% \non average with a maximum of34% overhead. While the overheads, however small, might be unacceptable for \ncertain applications,thispaper arguesthat other avenuesforimprov\u00adingtheperformance ofSC-preservingcompilers \nshouldbe explored before resorting to relaxing the program semantics. We proposed a novel hardware-software \ncooperation mechanism in the form of interference checks, which enabled us to regain much of theperfor\u00admancelostdue \nto restrictionsimposed on compiler optimizations to preserveSC. 9. Acknowledgements The authors would \nlike to thank Sarita Adve, Vikram Adve, Hans Boehm,SebastianBurckhardt,MarkHill,SureshJagannathan,Milo \nMartin,ToddMytkowicz,andJaroslav Sev..c\u00b4ikforinsightfuldiscus\u00adsions regarding this work. We also like \nto thank the anonymous re\u00adviewersfor valuablefeedback on thispaper.This workis supported by the National \nScience Foundation under awards CNS-0725354, CNS-0905149, and CCF-0916770 as well as by the Defense Ad\u00advancedResearchProjectsAgency \nunder awardHR0011-09-1-0037. References [1] S. V. Adve and H.-J. Boehm. Memory models: A case for rethinking \nparallellanguages andhardware. Commun. ACM,53(8):90 101,2010. [2] S.V.AdveandK.Gharachorloo. Shared memory \nconsistency models: a tutorial. Computer,29(12):66 76, 1996. [3] S. V. Adve and M. D. Hill. Weak ordering \na new de.nition. In Proceedings of ISCA,pages2 14.ACM,1990. [4] S.V.Adve,M.D.Hill,B.P.Miller,andR.H.B.Netzer.Detectingdata \nraces on weak memory systems. In ISCA,pages234 243,1991. [5] W. Ahn, S. Qi, J.-W. Lee, M. Nicolaides, \nX. Fang, J. Torrellas, D. Wong, and S. Midkiff. BulkCompiler: High-performance sequen\u00adtial consistency \nthrough cooperative compiler and hardware support. In 42nd International Symposium on Microarchitecture, \n2009.  [6] M.Batty,S.Owens,S.Sarkar,P.Sewell,andT.Weber.Mathematizing C++ concurrency. In Proceedings \nof the 38th annual ACM SIGPLAN-SIGACT symposium on Principles of programming languages, POPL 11,pages55 \n66.ACM,2011. [7] C.Bienia,S.Kumar,J.P.Singh, andK.Li. ThePARSECbenchmark suite: Characterization and \narchitectural implications. In Proceedings of the 17th International Conference on Parallel Architectures \nand Compilation Techniques, October 2008. [8] C. Blundell, M. M. Martin, and T. F. Wenisch. InvisiFence: \nPerformance-transparent memory ordering in conventional multipro\u00adcessors. In Proceedings of the 36th \nannual International Symposium on Computer architecture,ISCA 09,pages233 244.ACM,2009. [9] R.Bocchino,V.Adve,D.Dig,S.Adve,S.Heumann,R.Komuravelli, \nJ. Overbey, P. Simmons, H. Sung, and M. Vakilian. A type and effect systemforDeterministic Parallel Java. \nIn OOPSLA,2009. [10] H. J. Boehm. Simple thread semantics require race detection. In FIT session at PLDI,2009. \n[11] H. J. Boehm and S. V. Adve. Foundations of the C++ concurrency memory model. In Proceedings of PLDI,pages68 \n78.ACM,2008. [12] C.Boyapati andM.Rinard.Aparameterized typesystemforrace-free Javaprograms. InProceedings \nof OOPSLA,pages56 69.ACMPress, 2001. [13] C. Boyapati, R. Lee, and M. Rinard. Ownership types for safe \npro\u00adgramming: Preventing data races and deadlocks. In Proceedings of OOPSLA,2002. [14] S. Burckhardt, \nM. Musuvathi, and V. Singh. Verifying local transfor\u00admations on relaxed memory models.In Compiler Construction, \nvolume 6011 of Lecture Notes in Computer Science, pages 104 123. Springer Berlin/Heidelberg, 2010. [15] \nL. Ceze, J. Tuck, P. Montesinos, and J. Torrellas. BulkSC: Bulk enforcement of sequential consistency. \nIn ISCA,pages278 289,2007. [16] L. Ceze, J. Devietti, B. Lucia, and S. Qadeer. The case for system supportforconcurrency \nexceptions. In USENIX HotPar,2009. [17] W.Chuang,S.Narayanasamy,G.Venkatesh,J.Sampson,M.V.Bies-brouck,G.Pokam,B.Calder, \nandO.Colavin. Unboundedpage-based transactional memory. International Conference on Architectural Sup\u00adport \nfor Programming Languages and Operating Systems,2006. [18] T.Elmas,S.Qadeer,andS.Tasiran.Goldilocks:Araceandtransaction\u00adawareJava \nruntime. In PLDI,pages245 255,2007. [19] FeS2.TheFeS2 simulator. URLhttp://fes2.cs.uiuc.edu/. [20] C. \nFlanagan and S. Freund. FastTrack: Ef.cient and precise dynamic racedetection. In Proceedings of PLDI,2009. \n[21] C.Flanagan andS.N.Freund. Type-based racedetectionforJava. In Proceedings of PLDI,pages219 232,2000. \n[22] D. M. Gallagher, W. Y. Chen, S. A. Mahlke, J. C. Gyllenhaal, and W.meiW.Hwu. Dynamicmemorydisambiguation \nusing thememory con.ictbuffer. In ASPLOS,pages183 193,1994. [23] K. Gharachorloo, A. Gupta, and J. Hennessy. \nTwo techniques to en\u00adhance theperformance ofmemory consistency models. In Proceedings of the 1991 International \nConference on Parallel Processing, volume1, pages355 364,1991. [24] C.Gniady andB.Falsa..Speculative \nsequential consistency withlittle custom storage. InIEEE PACT,pages179 188,2002. [25] L. Hammond, V. \nWong, M. K. Chen, B. D. Carlstrom, J. D. Davis, B.Hertzberg,M.K.Prabhu,H.Wijaya,C.Kozyrakis, andK.Olukotun. \nTransactionalmemory coherence and consistency. In ISCA,pages102 113,2004. [26] J. L. Henning. SPEC CPU2006 \nbenchmark descriptions. SIGARCH Computer Architecture News, 34:1 17, September 2006. ISSN 0163\u00ad5964. \n[27] M. Herlihy and J. E. B. Moss. Transactional memory: architectural support for lock-free data structures. \nIn Proceedings of ISCA, pages 289 300.ACM,1993. [28] M. D. Hill. Multiprocessors should support simple \nmemory\u00adconsistency models. IEEE Computer, 31:28 34, 1998. ISSN 0018\u00ad9162. [29] Itanium. InsidetheIntelItanium2processor. \nHewlett Packard Techni\u00adcal White Paper,2002. [30] A.Kamil,J.Su,andK.Yelick.Making sequential consistencypractical \nin Titanium. In Proceedings of the 2005 ACM/IEEE conference on Supercomputing, page15.IEEEComputerSociety,2005. \n[31] L. Lamport. How to make a multiprocessor computer that correctly executes multiprocess programs. \nIEEE Transactions on Computers, 100(28):690 691, 1979. [32] C.Lattner andV.Adve.LLVM:A compilationframeworkforlifelong \nprogram analysis&#38;transformation. InProceedings of the International Symposium on Code Generation \nand Optimization: Feedback-Directed and Runtime Optimization. IEEEComputerSociety, 2004. [33] C. Lin, \nV. Nagarajan, and R. Gupta. Ef.cient sequential consistency using conditional fences. In International \nConference on Parallel Architectres and Compilation Techniques,2010. [34] B. Lucia, L. Ceze, K. Strauss, \nS. Qadeer, and H. Boehm. Con.ict Exceptions:Providing simpleparallellanguage semantics withprecise hardware \nexceptions. In 37th Annual International Symposium on Computer Architecture,June2010. [35] S.Magnusson,M.Christensson,J.Eskilson,D.Forsgren,G.H\u00b0allberg, \nJ. H\u00a8ogberg, F. Larsson, A. Moestedt, and B. Werner. Simics: A full system simulationplatform. IEEE Computer,35(2):50 \n58, 2002. [36] J.Manson,W.Pugh,andS.V.Adve. Thejavamemory model. In Proceedings of POPL,pages378 391.ACM,2005. \n[37] D. Marino, A. Singh, T. Millstein, M. Musuvathi, and S. Narayanasamy. DRFx: A simple and ef.cient \nmemory model for concurrent programming languages. In PLDI 10,pages 351 362. ACM,2010. [38] A. Muzahid, \nD. Suarez, S. Qi, and J. Torrellas. SigRace: Signature\u00adbaseddata racedetection. In ISCA,2009. [39] V. \nNagarajan and R. Gupta. Speculative optimizations for parallel programs on multicores. In LCPC,pages323 \n337,2009. [40] S.Owens,S.Sarkar,andP.Sewell. Abetter x86 memory model: x86-TSO. In In TPHOLs 09: Conference \non Theorem Proving in Higher Order Logics, volume 5674 of LNCS,pages391 407.Springer,2009. [41] M.Postiff,D.Greene, \nandT.N.Mudge. Thestore-load addresstable and speculative registerpromotion. In MICRO,pages235 244,2000. \n[42] P. Pratikakis, J. S. Foster, and M. Hicks. LOCKSMITH: Context\u00adsensitive correlation analysis for \nrace detection. In Proceedings of PLDI,pages320 331,2006. [43] M.Prvulovic andJ.Torrelas. Reenact:Using \nthread-level speculation mechanismstodebugdata racesin multithreaded codes.In Proceedings of ISCA,SanDiego,CA,June2003. \n[44] P. Ranganathan, V. Pai, and S. Adve. Using speculative retirement andlargerinstruction windows to \nnarrow theperformancegapbetween memory consistency models. In SPAA 97,pages199 210,1997. [45] D. Shasha \nand M. Snir. Ef.cient and correct execution of parallel programs that share memory. ACM Transactions \non Programming Languages and Systems (TOPLAS),10(2):282 312, 1988. [46] Z.Sura,X.Fang,C.Wong,S.Midkiff,J.Lee,andD.Padua.Compiler \ntechniquesforhighperformancesequentially consistentjavaprograms. In Proceedings of PPoPP,pages2 13,2005. \n[47] J. .c\u00b4Sev.ik and D. Aspinall. On validity of program transformations in theJava memory model. In \nECOOP,pages27 51,2008. [48] J. .c\u00b4. Vafeiadis, F. Zappa Nardelli, S. Jagannathan, and Sev.ik, V P. Sewell. \nRelaxed-memory concurrency and veri.ed compilation. In Proceedings of the 38th annual ACM SIGPLAN-SIGACT \nsymposium on Principles of programming languages,POPL 11,pages43 54.ACM, 2011. [49] S. C. Woo, M. Ohara, \nE. Torrie, J. P. Singh, and A. Gupta. The SPLASH-2 programs: characterization and methodological consider\u00adations. \nIn ISCA,pages24 36,NewYork,NY,USA,1995.ACM. [50] K. Yeager. The MIPS R10000 superscalar microprocessor. \nMicro, IEEE,16(2):28 41, 2002. ISSN0272-1732.     \n\t\t\t", "proc_id": "1993498", "abstract": "<p>The most intuitive memory consistency model for shared-memory multi-threaded programming is <i>sequential consistency</i> (SC). However, current concurrent programming languages support a relaxed model, as such relaxations are deemed necessary for enabling important optimizations. This paper demonstrates that an SC-preserving compiler, one that ensures that every SC behavior of a compiler-generated binary is an SC behavior of the source program, retains most of the performance benefits of an optimizing compiler. The key observation is that a large class of optimizations crucial for performance are either already SC-preserving or can be modified to preserve SC while retaining much of their effectiveness. An SC-preserving compiler, obtained by restricting the optimization phases in LLVM, a state-of-the-art C/C++ compiler, incurs an average slowdown of 3.8% and a maximum slowdown of 34% on a set of 30 programs from the SPLASH-2, PARSEC, and SPEC CINT2006 benchmark suites.</p> <p>While the performance overhead of preserving SC in the compiler is much less than previously assumed, it might still be unacceptable for certain applications. We believe there are several avenues for improving performance without giving up SC-preservation. In this vein, we observe that the overhead of our SC-preserving compiler arises mainly from its inability to aggressively perform a class of optimizations we identify as <i>eager-load</i> optimizations. This class includes common-subexpression elimination, constant propagation, global value numbering, and common cases of loop-invariant code motion. We propose a notion of <i>interference checks</i> in order to enable eager-load optimizations while preserving SC. Interference checks expose to the compiler a commonly used hardware speculation mechanism that can efficiently detect whether a particular variable has changed its value since last read.</p>", "authors": [{"name": "Daniel Marino", "author_profile_id": "81410595225", "affiliation": "University of California, Los Angeles, Los Angeles, CA, USA", "person_id": "P2690539", "email_address": "dlmarino@cs.ucla.edu", "orcid_id": ""}, {"name": "Abhayendra Singh", "author_profile_id": "81464655647", "affiliation": "University of Michigan, Ann Arbor, Ann Arbor, MI, USA", "person_id": "P2690540", "email_address": "ansingh@eecs.umich.edu", "orcid_id": ""}, {"name": "Todd Millstein", "author_profile_id": "81100018064", "affiliation": "University of California, Los Angeles, Los Angeles, CA, USA", "person_id": "P2690541", "email_address": "todd@cs.ucla.edu", "orcid_id": ""}, {"name": "Madanlal Musuvathi", "author_profile_id": "81100333862", "affiliation": "Microsoft Research, Redmond, Redmond, WA, USA", "person_id": "P2690542", "email_address": "madanm@microsoft.com", "orcid_id": ""}, {"name": "Satish Narayanasamy", "author_profile_id": "81100556410", "affiliation": "University of Michigan, Ann Arbor, Ann Arbor, MI, USA", "person_id": "P2690543", "email_address": "nsatish@umich.edu", "orcid_id": ""}], "doi_number": "10.1145/1993498.1993522", "year": "2011", "article_id": "1993522", "conference": "PLDI", "title": "A case for an SC-preserving compiler", "url": "http://dl.acm.org/citation.cfm?id=1993522"}