{"article_publication_date": "06-04-2011", "fulltext": "\n Automatic CPU-GPU Communication Management and Optimization Thomas B. Jablin Prakash Prabhu James A. \nJablin Nick P. Johnson Stephen R. Beard David I. August Princeton University, Princeton, NJ Brown University, \nProvidence, RI {tjablin, pprabhu, npjohnso, sbeard, august}@cs.princeton.edu jjablin@cs.brown.edu Abstract \nThe performance bene.ts of GPU parallelism can be enormous, but unlocking this performance potential \nis challenging. The ap\u00adplicability and performance of GPU parallelizations is limited by the complexities \nof CPU-GPU communication. To address these communications problems, this paper presents the .rst fully \nauto\u00admatic system for managing and optimizing CPU-GPU communca\u00adtion. This system, called the CPU-GPU \nCommunication Man\u00adager (CGCM), consists of a run-time library and a set of com\u00adpiler transformations \nthat work together to manage and optimize CPU-GPU communication without depending on the strength of \nstatic compile-time analyses or on programmer-supplied annota\u00adtions. CGCM eases manual GPU parallelizations \nand improves the applicability and performance of automatic GPU parallelizations. For 24 programs, CGCM-enabled \nautomatic GPU parallelization yields a whole program geomean speedup of 5.36x over the best sequential \nCPU-only execution. Categories and Subject Descriptors D.1.3 [Programming Tech\u00adniques]: Concurrent Programming \nParallel Programming General Terms Algorithms, Experimentation, Performance Keywords GPU, communication, \nmanagement, optimization 1. Introduction Currently, even entry-level PCs are equipped with GPUs capable \nof hundreds of GFLOPS. Real applications, parallelized to take ad\u00advantage of GPUs, regularly achieve \nspeedups between 4x and 100x [8, 10, 20]. Unfortunately, parallelizing code for GPUs is dif.cult due \nto the typical CPU-GPU memory architecture. The GPU and CPU have separate memories, and each processing \nunit may ef.\u00adciently access only its own memory. When programs running on the CPU or GPU need data-structures \noutside their memory, they must explicitly copy data between the divided CPU and GPU memories. The process \nof copying data between these memories for correct execution is called Managing Communication. Generally, \nprogram\u00admers manage CPU-GPU communication with memcpy-style func\u00adtions. Manually managing CPU-GPU communication \nis tedious and error-prone. Aliasing pointers, variable sized arrays, jagged ar\u00adrays, global pointers, \nand subversive typecasting make it dif.cult for programmers to copy the right data between CPU and GPU \nPermission to make digital or hard copies of all or part of this work for personal or classroom use is \ngranted without fee provided that copies are not made or distributed for pro.t or commercial advantage \nand that copies bear this notice and the full citation on the .rst page. To copy otherwise, to republish, \nto post on servers or to redistribute to lists, requires prior speci.c permission and/or a fee. PLDI \n11, June 4 8, 2011, San Jose, California, USA. Copyright c &#38;#169; 2011 ACM 978-1-4503-0663-8/11/06. \n. . $10.00 memories. Unfortunately, not all communication management is ef.cient; cyclic communication \npatterns are frequently orders of magnitude slower than acyclic patterns [15]. Transforming cyclic communication \npatterns to acyclic patterns is called Optimizing Communication. Na\u00a8ively copying data to GPU memory, \nspawn\u00ading a GPU function, and copying the results back to CPU memory yields cyclic communication patterns. \nCopying data to the GPU in the preheader, spawning many GPU functions, and copying the result back to \nCPU memory in the loop exit yields an acyclic com\u00admunication pattern. Incorrect communication optimization \ncauses programs to access stale or inconsistent data. This paper presents CPU-GPU Communication Manager \n(CGCM), the .rst fully automatic system for managing and op\u00adtimizing CPU-GPU communication. Automatically \nmanaging and optimizing communication increases programmer ef.ciency and program correctness. It also \nimproves the applicability and perfor\u00admance of automatic GPU parallelization. CGCM manages and optimizes \ncommunication using two parts, a run-time library and a set of compiler passes. To manage com\u00admunication, \nCGCM s run-time library tracks GPU memory alloca\u00adtions and transfers data between the CPU memory and \nGPU mem\u00adory. The compiler uses the run-time library to manage and opti\u00admize CPU-GPU communication without \nstrong analysis. By rely\u00ading on the run-time library, the compiler postpones, until run-time, questions \nthat are dif.cult or impossible to answer statically. Three novel compiler passes for communication optimization \nleverage the CGCM run-time: map promotion, alloca promotion, and glue kernels. Map promotion transforms \ncyclic CPU-GPU communica\u00adtion patterns into acyclic communication patterns. Alloca promo\u00adtion and glue \nkernels improve the applicability of map promotion. The contributions of CGCM over prior work are: The \n.rst fully automatic CPU-GPU communication manage\u00adment system.  The .rst fully automatic CPU-GPU communication \noptimiza\u00adtion system.  Figure 1 shows a taxonomy of CPU-GPU communication man\u00adagement techniques. No \nprior work fully automates CPU-GPU communication, but several semi-automatic techniques can manage communication \nif programmers supply annotations [12, 24, 26]. Some of these communication management techniques are \nstrongly coupled with automatic parallelization systems [12, 24]; others are not [26]. None of the semi-automatic \ncommunication systems op\u00adtimize CPU-GPU communications. Some prior automatic paral\u00adlelization techniques \nrequire manual communication [3, 13, 25]. The earliest GPU parallelization systems feature manual paral\u00adlelization \nand manual communication [6, 11, 16]. These systems remain the most popular. CGCM enables fully-automatic \ncommu\u00adnication management for manual and automatic parallelizations. Communication management is also \na problem for distributed memory systems. Inspector-executor techniques automatically manage communication \nfor distributed memory systems [4, 14, 22] Figure 1. A taxonomy of related work showing automatic and \nmanual communication and parallelization as independent axes.  but have not been used for CPU-GPU systems. \nInspector-executor techniques can reduce the number of bytes transferred, but the overall communication \npattern remains cyclic. We have coupled CGCM with an automatic parallelizing com\u00adpiler to produce a fully \nautomatic GPU parallelization system. To compare a strong cyclic communication system against CGCM s \nacyclic communication, we adapted inspector-executor to GPUs. Across 24 programs, CGCM coupled with automatic \nparalleliza\u00adtion shows a geomean whole program speedup of 5.36x over se\u00adquential CPU-only execution versus \n0.92x for inspector-executor. This paper provides a detailed description of the design and im\u00adplementation \nof CGCM and presents an evaluation of the system. Section 2 describes background information about the \nchallenges of CPU-GPU communications and limitations of related work. Sec\u00adtion 3 describes the run-time \nsupport library. Section 4 explains how the compiler automatically inserts calls to the CGCM library. \nThe map promotion, alloca promotion, and glue kernel techniques for optimizing CPU-GPU communication \nappear in Section 5. Sec\u00adtion 6 interprets the performance results for 24 programs taken from the PARSEC \n[5], StreamIt [23], Rodinia [7], and PolyBench [17] benchmark suites. This section also compares CGCM \nto an ideal\u00adized inspector-executor system. Section 7 discusses related work, and Section 8 concludes. \n2. Motivation The divided CPU-GPU memories create communication manage\u00adment and optimization dif.culties \nthat motivate CGCM. Manag\u00ading communication means copying data between CPU and GPU memories to ensure \neach processing unit has the data it needs. Manual communication management is time-consuming and error\u00adprone, \nand semi-automatic communication management in prior work has limited applicability. Optimizing communication \nmeans transforming communication patterns to remove cyclic dependen\u00adcies. Optimizing communication is \nvital for program performance, but increases the dif.culty of communication management. Lack of optimized \ncommunication limits the performance of prior semi\u00adautomatic communication management frameworks. CGCM \nis the .rst fully-automatic communication management and optimization system. Listing 1: Manual explicit \nCPU-GPU memory management char *h h array[M] = { What so proudly we hailed at the twilight s last gleaming, \n, ... }; D global void kernel(unsigned i, char **d array); void foo(unsigned N) { /* Copy elements from \narray to the GPU */  char *h d array[M];  for(unsigned i = 0; i < M; ++i) {  size t size = strlen(h \nh array[i]) + 1;  cudaMalloc(h d array + i, size);  cudaMemcpy(h d array[i], h h array[i], size,  \ncudaMemcpyHostToDevice);  }  /* Copy array to the GPU */ char **d d array;  cudaMalloc(&#38;d d array, \nsizeof (h d array));  cudaMemcpy(d d array, h d array, sizeof (h d array),  cudaMemcpyHostToDevice); \n D for(unsigned i = 0; i < N; ++i)  kernel<<<30, 128>>>(i, d d array);  /* Free the array */ cudaFree(d \nd array); /* Copy the elements back, and free the GPU copies */ for(unsigned i = 0; i < M; ++i) {  \nsize t size = strlen(h h array[i]) + 1;  cudaMemcpy(h h array[i], h d array[i], size,  cudaMemcpyDeviceToHost); \n cudaFree(h d array[i]);  }} D Useful work Communication Kernel spawn Listing 2: Automatic implicit \nCPU-GPU memory management char *h h array[M] = { What so proudly we hailed at the twilight s last gleaming, \n, ... }; D global void kernel(unsigned i, char **d array); void foo(unsigned N) {D for(unsigned i = 0; \ni < N; ++i) { kernel<<<30, 128>>>(i, h h array); } } D Useful work Communication Kernel spawn 2.1 \nCommunication Management Communication management presents a major dif.culty for man\u00adual and automatic \nGPU parallelizations. Current GPU program\u00adming languages, such as CUDA and OpenCL, require manual com\u00admunication \nmanagement using primitive memcpy-style functions.  Framework Comm. Opti. Requires Annotations Applicability \nAcyclic Communication CPU-GPU Aliasing Pointers Irregular Accesses Weak Type Systems Pointer Arithmetic \nMax Indirection JCUDA [26] \u00d7 Yes . . . \u00d7 \u00d7 8 No Named Regions [12] \u00d7 Yes . . \u00d7 . \u00d7 1 No Af.ne [24] \u00d7 \nYes . \u00d7 \u00d7 . \u00d7 1 With Annotation Inspector-Executor [4, 14, 22] \u00d7 Yes \u00d7 \u00d7 . . \u00d7 1 No CGCM . No . . . . \n. 2 After Optimization Table 1. Comparison between communication systems Manually copying complex data-types \nfrom CPU memory to GPU memory is tedious and error-prone. Listing 1 shows how a CUDA programmer might \ncopy an array of strings to and from the GPU, allocating and freeing memory as necessary. Almost every \nline of code in the listing involves communication management and not useful computation. Furthermore, \nthe programmer must manage buffers and manipulate pointers. Buffer management and pointer manipulation \nare well-known sources of bugs. Automatic communication management avoids the dif.culties of buffer management \nand pointer manipulation, improving pro\u00adgram correctness and programmer ef.ciency. However, automati\u00adcally \nmanaging communication based on compile-time static anal\u00adysis is impossible for general C and C++ programs. \nIn C and C++, any argument to a GPU function could be cast to a pointer, and an opaque pointer could \npoint to the middle of data-structures of arbitrary size. Prior work restricts its applicability to avoid \nthese dif.culties [12, 24, 26]. Table 1 compares the applicability of prior communication management \ntechniques. Prior GPU techniques [12, 24, 26] re\u00adquire programmer annotations, do not handle the full \ngenerality of pointer arithmetic and aliasing, create cyclic CPU-GPU commu\u00adnication patterns by default, \nand do not optimize communication. Consequently, prior automatic communication management tech\u00adniques \nhave limited applicability and frequently yield poor perfor\u00admance. To gain acceptance in the GPU programming \ncommunity, automatic communication management techniques must overcome these limitations.  2.2 Communication \nOptimization Figure 2 shows execution schedules for three communication pat\u00adterns: a na\u00a8ive cyclic pattern, \nan inspector-executor pattern, and an acyclic pattern. In the na\u00a8ive schedule, data transfers to and \nfrom GPU memory create cyclic dependencies, forcing the CPU and GPU to wait for each other. GPU programmers \nunderstand cyclic dependencies dramatically increase execution time. Nevertheless, all prior automatic \nGPU communication systems generate cyclic communication [12, 24, 26]. Cyclic communication patterns pre\u00advent \nthese systems from ef.ciently parallelizing complex programs that launch many GPU functions. Inspector-executor \nsystems manage communication in clusters with distributed memory [4, 14, 22]. The inspector-executor \nap\u00adproach breaks loops into an inspector, a scheduler, and an ex\u00adecutor. The inspector simulates the \nloop to determine which ar\u00adray offsets the program reads or writes during each iteration. Af\u00adter the \ninspection, a scheduler assigns loop iterations to cluster nodes and transfers the appropriate data. \nExecutors on each clus\u00adter node compute loop iterations in parallel. The inspector-executor schedule \nin Figure 2 outperforms the na\u00a8ive communication sched\u00adule because the bene.t of communicating only the \nneeded array Figure 2. Execution schedules for na\u00a8ive cyclic, inspector\u00adexecutor, and acyclic communication \npatterns elements exceeds the cost of the inspection. However, in real pro\u00adgrams the cost of sequential \ninspection may exceed the bene.ts of parallel execution. Generally, inspector-executor is cyclic since \nit transfers only the needed bytes for a single iteration at a time. Acyclic inspector-executor variants \nrequire annotations or strong static analysis [18, 21]. Figure 2 shows the execution schedule for acyclic \nCPU-GPU communications. Removing cyclic communication avoids the la\u00adtency of back-and-forth communication \nand allows the CPU and GPU to work in parallel. The performance bene.t of acyclic com\u00admunication is signi.cant. \n 2.3 Overview CGCM avoids the limitations of prior work by employing a run\u00adtime support library and \nan optimizing compiler to automatically manage and optimize CPU-GPU communication, respectively. The \nrun-time library determines the size and shape of data-structures during execution. The compiler uses \nthe run-time library to man\u00adage memory without strong analysis and then optimizes commu\u00adnications to \nproduce acyclic patterns. CGCM has two restrictions:  Figure 3. High-level overview of CGCM CGCM does \nnot support pointers with three or more degrees of in\u00addirection, and it does not allow pointers to be \nstored in GPU func\u00adtions. Using CGCM, programmers can write the example code in Listing 1 as Listing \n2. Replacing explicit CPU-GPU communica\u00adtion with CGCM yields dramatically shorter, simpler, clearer \ncode and prevents several classes of programmer error. Figure 3 shows a high-level overview of CGCM s \ntransforma\u00adtion and run-time system. The run-time library provides mapping functions which translate \nCPU pointers to equivalent GPU point\u00aders (Section 3). The compiler inserts mapping functions to manage \nCPU-GPU communication (Section 4). Map promotion optimizes CPU-GPU communication by transferring data \nto the GPU early and keeping it there as long as possible (Section 5). Two enabling transformations, \nglue kernels and allocation promotion, improve map promotion s applicability. 3. Run-Time Library The \nCGCM run-time library enables automatic CPU-GPU commu\u00adnication management and optimization for programs \nwith complex patterns of memory allocation and unreliable typing. To accomplish this feat, the run-time \nlibrary correctly and ef.ciently determines which bytes to transfer. For correctness, the run-time library \ncopies data to the GPU at allocation unit granularity. A pointer s allocation unit comprises all bytes \nreachable from a pointer by valid pointer arithmetic. Using the concept of allocation units, the run-time \nli\u00adbrary can support the full semantics of pointer arithmetic without strong static analysis. A one-to-one \nmapping between allocation units in CPU memory and allocation units in GPU memory allows the run-time \nlibrary to translate pointers. 3.1 Tracking Allocation Units Unlike inspector-executor systems which \nmanage memory on a per-byte or per-word granularity, CGCM manages memory at the granularity of allocation \nunits. CGCM determines which bytes to transfer by .nding allocation information for opaque pointers to \nthe stack, heap, and globals. In C and C++, an allocation unit is a contiguous region of memory allocated \nas a single unit. Blocks of memory returned from malloc or calloc, local variables, and global variables \nare all examples of allocation units. All bytes in an array of structures are considered part of the \nsame allocation unit, but two structures de.ned consecutively are different allocation units. Transferring \nentire allocation units between CPU and GPU memories ensures that valid pointer arithmetic yields the \nsame re\u00adsults on the CPU and GPU, because the C99 programming stan\u00addard [1] stipulates that pointer arithmetic \noutside the bounds of a single allocation unit is unde.ned. Copying an allocation unit between CPU and \nGPU memories requires information about the allocation unit s base and size. The run-time library stores \nthe base and size of each allocation unit in a self-balancing binary tree map indexed by the base address \nof each allocation unit. To determine the base and size of a pointer s allocation unit, the run-time \nlibrary .nds the greatest key in the allocation map less than or equal to the pointer. Although allocation \ninformation for global variables is known at compile-time, stack and heap allocations change dynamically \nat run-time. The run-time library uses different techniques to track the allocation information for global, \nstack, and heap memory. To track global variables, the compiler inserts calls to the run\u00adtime library \ns declareGlobal function before main. Declar\u00ading addresses at run-time rather than at compile-time or \nlink\u00adtime avoids the problems caused by position independent code and address space layout randomization. \n To track heap allocations, the run-time library wraps around malloc, calloc, realloc, and free. These \nwrappers modify the allocation map to re.ect the dynamic state of the heap at run-time.  To track escaping \nstack variables, the compiler inserts calls to declareAlloca. The registration expires when the stack \nvariable leaves scope.   3.2 CPU-GPU Mapping Semantics Table 2 lists each function in the run-time \nlibrary and its arguments. The run-time library contains functions that translate between CPU and GPU \npointers. The three basic functions are map, release, and unmap. Each of these functions operates on \nopaque pointers to CPU memory. Mapping a pointer from CPU to GPU memory copies the cor\u00adresponding allocation \nunit to GPU memory, allocating memory if necessary. The run-time library employs reference counting to \ndeallocate GPU memory when necessary. Mapping a pointer from CPU to GPU memory increases the GPU allocation \nunit s reference count.  Unmapping a CPU pointer updates the CPU allocation unit with the corresponding \nGPU allocation unit. To avoid redun\u00addant communication, the run-time library will not copy data if the \nCPU allocation unit is already up-to-date. Since only a GPU function can modify GPU memory, unmap updates \neach alloca\u00adtion unit at most once after each GPU function invocation.  Releasing a CPU pointer decreases \nthe corresponding GPU allocation unit s reference count, freeing it if necessary.   Function prototype \nDescription map(ptr) Maps from host to device pointer, allocating and copying memory if necessary. Increases \nthe allocation unit s reference count. unmap(ptr) Maps to host memory if the allocation unit s epoch \nis not current. Updates the allocation unit s epoch. release(ptr) Decreases the reference count of the \nallocation unit. If the reference count is zero, frees resources. mapArray(ptr) Maps from host to device \npointer, allocating and copying memory if necessary. Increases the allocation unit s reference count. \nunmapArray(ptr) Maps to host memory if the allocation unit s epoch is not current. Updates the allocation \nunit s epoch. releaseArray(ptr) Decreases the reference count of the allocation unit. If the reference \ncount is zero, frees resources. declareAlloca(size) Allocates memory on the stack and registers it with \nthe run-time library. declareGlobal(name,ptr,size,isReadOnly) Registers a global with the run-time library. \n Table 2. CGCM s run-time library interface Listing 3: Listing 2 after the compiler inserts run-time \nfunctions (unoptimized CGCM). char *h h array[M] = { What so proudly we hailed at the twilight s last \ngleaming, , ... }; D global void kernel(unsigned i, char **d array); void foo(unsigned N) {D for(unsigned \ni=0; i < N; ++i) { char **d d array = mapArray(h h array);  kernel<<<30, 128>>>(i, d d array);  unmapArray(h \nh array);  releaseArray(h h array); }} D Useful work Communication Kernel spawn Each of the primary \nrun-time library functions has an array variant. The array variants of the run-time library functions \nhave the same semantics as their non-array counterparts but operate on doubly indirect pointers. The \narray mapping function translates each CPU memory pointer in the original array into a GPU memory pointer \nin a new array. It then maps the new array to GPU memory. Using run-time library calls, Listing 2 can \nbe rewritten as Listing 3.  3.3 Implementation The map, unmap, and release functions provide the basic \nfunc\u00adtionality of the run-time library. The array variations follow the same patterns as the scalar versions. \nAlgorithm 1 is the pseudo-code for the map function. Given a pointer to CPU memory, map returns the corresponding \npointer to GPU memory. The allocaInfoMap contains information about the pointer s allocation unit. If \nthe reference count of the allocation unit is non-zero, then the allocation unit is already on the GPU. \nAlgorithm 1: Pseudo-code for map Require: ptr is a CPU pointer Ensure: Returns an equivalent GPU pointer \ninfo . greatestLTE(allocInfoMap, ptr) if info.refCount = 0 then if \u00acinfo.isGlobal then info.devptr . \ncuMemAlloc(info.size) else info.devptr . cuModuleGetGlobal(info.name) cuMemcpyHtoD(info.devptr, info.base, \ninfo.size) info.refCount . info.refCount + 1 return info.devptr + (ptr - info.base) Algorithm 2: Pseudo-code \nfor unmap Require: ptr is a CPU pointer Ensure: Update ptr with GPU memory info . greatestLTE(allocInfoMap, \nptr) if info.epoch = globalEpoch .\u00acinfo.isReadOnly then cuMemcpyDtoH(base, info.devptr, info.size) info.epoch \n. globalEpoch When copying heap or stack allocation units to the GPU, map dynamically allocates GPU memory, \nbut global variables must be copied into their associated named regions. The map function calls cuModuleGetGlobal \nwith the global variable s name to get the variable s address in GPU memory. After increasing the reference \ncount, the function returns the equivalent pointer to GPU memory. The map function preserves aliasing \nrelations in GPU memory, since multiple calls to map for the same allocation unit yield point\u00aders to \na single corresponding GPU allocation unit. Aliases are com\u00admon in C and C++ code and alias analysis \nis undecidable. By han\u00addling pointer aliases in the run-time library, the compiler avoids static analysis, \nsimplifying implementation and improving applica\u00adbility.  Algorithm 3: Pseudo-code for release Require: \nptr is a CPU pointer Ensure: Release GPU resources when no longer used info . greatestLTE(allocInfoMap, \nptr) info.refCount . info.refCount - 1 if info.refCount = 0 .\u00acinfo.isGlobal then cuMemFree(info.devptr) \nThe pseudo-code for the unmap function is presented in Algo\u00adrithm 2. Given a pointer to CPU memory, unmap \nupdates CPU memory with the latest state of GPU memory. If the run-time li\u00adbrary has not updated the \nallocation unit since the last GPU func\u00adtion call and the allocation unit is not in read only memory, \nunmap copies the GPU s version of the allocation unit to CPU memory. To determine if the CPU allocation \nunit is up-to-date, unmap maintains an epoch count which increases every time the program launches a \nGPU function. It is suf.cient to update CPU memory from the GPU just once per epoch, since only GPU functions \nalter GPU memory. Algorithm 3 is the pseudo-code for the release function. Given a pointer to CPU memory, \nrelease decrements the GPU allocation s reference count and frees the allocation if the reference count \nreaches zero. The release function does not free global variables when their reference count reaches \nzero. Just as in CPU codes, it is not legal to free a global variable. 4. Communication Management CPU-GPU \ncommunication is a common source of errors for man\u00adual parallelization and limits the applicability of \nautomatic par\u00adallelization. A CGCM compiler pass uses the run-time library to automatically manage CPU-GPU \ncommunications. For each GPU function spawn, the compiler determines which values to transfer to the \nGPU using a liveness analysis. When copying values to the GPU, the compiler must differentiate between \nintegers and .oating point values, pointers, and indirect pointers. The C and C++ type systems are fundamentally \nunreliable, so the compiler uses simple type-inference instead. The communication management compiler \npass starts with se\u00adquential CPU codes calling parallel GPU codes without any CPU-GPU communication. \nAll global variables share a single common namespace with no distinction between GPU and CPU memory spaces. \nFor each GPU function, the compiler creates a list of live-in values. A value is live-in if it is passed \nto the GPU function directly or is a global variable used by the GPU. The C and C++ type systems are \ninsuf.cient to determine which live-in values are pointers or to determine the indirection level of a \npointer. The compiler ignores these types and instead infers type based on usage within the GPU function, \nignoring usage in CPU code. If a value .ows to the address operand of a load or store, potentially through \nadditions, casts, sign extensions, or other opera\u00adtions, the compiler labels the value a pointer. Similarly, \nif the result of a load operation .ows to another memory operation, the com\u00adpiler labels the pointer \noperand of the load a double pointer. Since types .ow through pointer arithmetic, the inference algorithm \nis .eld insensitive. Determining a value s type based on use allows the compiler to circumvent the problems \nof the C and C++ type systems. The compiler correctly determined unambiguous types for all of the live-in \nvalues to GPU functions in the 24 programs mea\u00adsured. For each live-in pointer to each GPU function, \nthe compiler transfers data to the GPU by inserting calls to map or mapArray. After the GPU function \ncall, the compiler inserts a call for each live-out pointer to unmap or unmapArray to transfer data back \nto Algorithm 4: Pseudo-code for map promotion forall region . Functions . Loops do forall candidate . \n.ndCandidates(region) do if \u00acpointsToChanges(candidate, region) then if \u00acmodOrRef(candidate, region) \nthen copy(above(region), candidate.map) copy(below(region), candidate.unmap) copy(below(region), candidate.release) \ndeleteAll(candidate.DtoH) Listing 4: Listing 3 after map promotion char *h h array[M] = { What so proudly \nwe hailed at the twilight s last gleaming, , ... }; D global void kernel(unsigned i, char **d array); \nvoid foo(unsigned N) { mapArray(h h array); D for(unsigned i = 0; i < N; ++i) {  char **d d array = \nmapArray(h h array);  kernel<<<30, 128>>>(i, d d array);  releaseArray(h h array);  } mapArray(h \nh array);  releaseArray(h h array);  } D Useful work Communication Kernel spawn the CPU. Finally, \nfor each live-in pointer, the compiler inserts a call to release or releaseArray to release GPU resources. \n5. Optimizing CPU-GPU Communication Optimizing CPU-GPU communication has a profound impact on program \nperformance. The overall optimization goal is to avoid cyclic communication. Cyclic communication causes \nthe CPU to wait for the GPU to transfer memory and the GPU to wait for the CPU to send more work. The \nmap promotion compiler pass manip\u00adulates calls to the run-time library to remove cyclic communication \npatterns. After map promotion, programs transfer memory to the GPU, then spawn many GPU functions. For \nmost of the program, Communication .ows one way, from CPU to GPU. The results of GPU computations return \nto CPU memory only when absolutely necessary. The alloca promotion and glue kernels compiler passes improve \nthe applicability of map promotion. 5.1 Map Promotion The overall goal of map promotion is to hoist \nrun-time library calls out of loop bodies and up the call graph. Algorithm 4 shows the pseudo-code for \nthe map promotion algorithm. First, the compiler scans the region for promotion candidates. A region \nis either a function or a loop body. Each promotion candidate captures all calls to the CGCM run-time \nlibrary featuring the same pointer. Map promotion attempts to prove that these pointers point to the \nsame allocation unit throughout the region, and that the allocation unit is not referenced or modi.ed \nin the region. If successful, map promotion hoists the mapping operations out of the target region. The \nspeci.c implementation varies slightly depending on whether the region is a loop or a function.  For \na loop, map promotion copies map calls before the loop, moves unmap after the loop, and copies release \ncalls after the loop. Map promotion copies the map calls rather than moving them since these calls provide \nCPU to GPU pointer translation. Copying release calls preserves the balance of map and release operations. \nInserting map calls before the loop may require copying some code from the loop body before the loop. \nFor a function, the compiler .nds all the function s parents in the call graph and inserts the necessary \ncalls before and after the call instructions in the parent functions. Some code from the original function \nmay be copied to its parent in order to calculate the pointer earlier. The compiler iterates to convergence \non the map promotion op\u00adtimization. In this way, map operations can gradually climb up the call graph. \nRecursive functions are not eligible for map promotion in the present implementation. CGCM optimizes \nListing 3 to Listing 4. Promoting the initial mapArray call above the loop causes the run-time library \nto trans\u00adfer h h array s allocation units to the GPU exactly once. The sub\u00adsequent calls to mapArray \ninside the loop do not cause additional communication since the GPU version of the allocation units is \nal\u00adready active. Moving the unmapArray call below the loop allows the run-time to avoid copying allocation \nunits back to CPU memory each iteration. The optimized code avoids all GPU to CPU commu\u00adnication inside \nthe loop. Spawning GPU functions from the CPU is the only remaining communication inside the loop. The \n.nal result is an acyclic communication pattern with information only .owing from CPU to GPU during the \nloop.  5.2 Alloca Promotion Map promotion cannot hoist a local variable above its parent func\u00adtion. \nAlloca promotion hoists local allocation up the call graph to improve map promotions applicability. Alloca \npromotion preallo\u00adcates local variables in their parents stack frames, allowing the map operations to \nclimb higher in the call graph. The alloca promo\u00adtion pass uses similar logic to map promotion, potentially \ncopying code from child to parent to calculate the size of the local variable earlier. Like map promotion, \nalloca promotion iterates to conver\u00adgence.  5.3 Glue Kernels Sometimes small CPU code regions between \ntwo GPU functions prevent map promotion. The performance of this code is inconse\u00adquential, but transforming \nit into a single-threaded GPU function obviates the need to copy the allocation units between GPU and \nCPU memories and allows the map operations to rise higher in the call graph. The glue kernel optimization \ndetects small regions of code that prevent map promotion using alias analysis and lowers this code to \nthe GPU. Interrelationships between communication optimization passes imply a speci.c compilation schedule. \nSince alloca promotion and glue kernels improve the applicability of map promotion, the com\u00adpiler schedules \nthese passes before map promotion. The glue ker\u00adnel pass can force some virtual registers into memory, \ncreating new opportunities for alloca promotion. Therefore, the glue kernel op\u00adtimization runs before \nalloca promotion, and map promotion runs last. 6. Evaluation CGCM is applicable to all 101 DOALL loops \nfound by a simple automatic DOALL parallelizer across a selection of 24 programs drawn from the PolyBench \n[17], Rodinia [7], StreamIt [23], and PARSEC [5] benchmark suites. The named region technique [12] and \ninspector-executor [4, 14, 22] were only applicable to 80. With\u00adout communication optimizations, many \nprograms show limited speedup or even dramatic slowdown with automatic communica\u00adtion management. By \noptimizing communication, CGCM enables a whole program geomean speedup of 5.36x over best sequential \nCPU-only execution. 6.1 Experimental Platform The performance baseline is an Intel Core 2 Quad clocked \nat 2.40 GHz with 4MB of L2 cache. The Core 2 Quad is also the host CPU for the GPU. All GPU parallelizations \nwere executed on an NVIDIA GeForce GTX 480 video card, a CUDA 2.0 device with 1,536 MB of global memory \nand clocked at 1.40 GHz. The GTX 480 has 15 streaming multiprocessors with 32 CUDA cores each, for a \ntotal of 480 cores. The CUDA driver version is 3.2 release candidate 2. The parallel GPU version is always \ncompared with the original single threaded C or C++ implementation running on the CPU, even when multithreaded \nCPU implementations are available. All .gures show whole program speedups, not kernel or loop speedups. \nAll program codes are compiled without any alterations. The sequential baseline compilations are performed \nby the clang compiler version 2.9 (trunk 118020) at optimization level three. The clang compiler produced \nSSE vectorized code for the sequential CPU-only compilation. The clang compiler at optimiza\u00adtion level \nthree does not use automatic parallelization techniques beyond simple vectorization. The nvcc compiler \nrelease 3.2, V0.2.1221 compiled all CUDA C and C++ programs using optimization level three. CGCM uses \nthe same performance .ags to manage and opti\u00admize communication for all programs. The optimizer runs \nthe same passes with the same parameters in the same order for every pro\u00adgram. A simple DOALL GPU parallelization \nsystem coupled with CGCM and an open source PTX backend [19] performed all auto\u00admatic parallelizations. \n 6.2 Benchmark Suites PolyBench [2, 9] is a suite composed of programs designed to evaluate implementations \nof the polyhedral model of DOALL par\u00adallelism in automatic parallelizing compilers. Prior work on au\u00adtomatic \nGPU parallelization reports impressive performance on kernel-type micro-benchmarks without communication \noptimiza\u00adtion. The jacobi-2d-imper, gemm, and seidel programs have been popular targets for evaluating \nautomatic GPU parallelization systems [3, 12]. The simple DOALL parallelizer found opportuni\u00adties in \nall of the PolyBench programs, and CGCM managed com\u00admunication for all GPU functions. Figure 4 shows \nperformance re\u00adsults for the entire PolyBench suite. The Rodinia suite consists of 12 programs with CPU \nand GPU implementations. The CPU implementations contain OpenMP pragmas, but CGCM and the DOALL parallelizer \nignore them. The simple DOALL parallelizer found opportunities in six of the 12 Rodinia programs and \nfrom one selected program from the PARSEC and StreamIt benchmark suites. CGCM managed com\u00admunications \nfor all functions generated by the DOALL parallelizer. The StreamIt benchmark suite features pairs of \napplications writ\u00adten in C and the StreamIt parallel programming language. PARSEC consists of OpenMP \nparallelized programs for shared memory sys\u00adtems. The eight applications from Rodinia, StreamIt, and \nPARSEC are larger and more realistic than the PolyBench programs.  6.3 Results Figure 4 shows whole \nprogram speedup over sequential CPU\u00adonly execution for inspector-executor, unoptimized CGCM, opti\u00adtial \nCPU-only execution.  Program Suite Limiting Factor Performance Study GPU Communication UnOpti. Opti. \nUnOpti. Opti. Total Kernels Applicability Study CGCM Inspector-Executor Named Regions Manual Prior Work \n adi PolyBench GPU 0.02% 100.00% 99.98% 0.00% 7 7 7 7 atax PolyBench Comm. 0.28% 0.28% 98.20 98.44% 3 \n3 3 3 bicg PolyBench Comm. 4.36% 4.46% 72.38% 74.15% 2 2 2 2 correlation PolyBench GPU 87.49% 87.39% \n10.17% 10.12% 5 5 5 5 covariance PolyBench GPU 77.12% 77.28% 18.61% 18.43% 4 4 4 4 doitgen PolyBench \nGPU 87.48% 87.52% 11.29% 11.20% 3 3 3 3 gemm PolyBench GPU 73.49% 73.76% 19.69% 19.49% 4 4 4 4 gemver \nPolyBench Comm. 4.06% 4.10% 88.21% 89.36% 5 5 5 5 gesummv PolyBench Comm. 6.17% 6.29% 86.17% 86.74% 2 \n2 2 2 gramschmidt PolyBench Comm. 1.82% 8.37% 98.18% 90.91% 3 3 3 3 jacobi-2d-imper PolyBench GPU 7.20% \n95.97% 92.82% 3.32% 3 3 3 3 seidel PolyBench Other 0.01% 0.01% 0.59% 0.59% 1 1 1 1 lu PolyBench GPU 0.41% \n88.05% 99.59% 7.02% 3 3 2 2 ludcmp PolyBench GPU 1.23% 87.38% 98.10% 4.13% 5 5 3 3 2mm PolyBench GPU \n75.53% 77.25% 17.96% 18.25% 7 7 7 7 3mm PolyBench GPU 78.75% 79.29% 17.86% 17.85% 10 10 10 10 cfd Rodinia \nGPU 4.65% 77.96% 85.90% 0.16% 9 9 3 3 [7] hotspot Rodinia GPU 2.78% 71.57% 92.60% 0.89% 2 2 1 1 [7] kmeans \nRodinia Other 0.65% 0.00% 10.84% 0.05% 2 2 2 2 [7] lud Rodinia GPU 3.77% 63.57% 91.56% 0.39% 6 6 1 1 \n[7] nw Rodinia Other 0.00% 2.44% 100.00% 24.19% 4 4 2 2 [7] srad Rodinia Other 0.00% 27.08% 100.00% 6.20% \n6 6 1 1 [7] fm StreamIt Other 0.00% 0.00% 0.00% 0.00% 4 4 4 4 blackscholes PARSEC Other 1.74% 3.23% 45.84% \n0.96% 1 1 0 0 Table 3. Summary of program characteristics including: program suite, limiting factor \nfor performance, the contributions of GPU and communication time to total execution time as a percentage, \nthe number of applicable kernels for the CGCM, Inspector-Executor, and Named Region management techniques, \nand a citation for prior manual parallelizations. mized CGCM, and a manual parallelization if one exists. \nThe .g-0.71x for unoptimized CGCM, and 5.36x for optimized CGCM. ure s y-axis starts at 0.25x, although \nsome programs have lower Taking the greater of 1.0x or the performance of each application speedups. \nTable 3 shows additional details for each program. The yields geomeans of 1.53x for inspector-executor, \n2.81x for unopti\u00adgeomean whole program speedups over sequential CPU only exe-mized CGCM, and 7.18x for \noptimized CGCM. cution across all 24 applications are 0.92x for inspector-executor,  Before optimization, \nmost programs show substantial slow\u00addown. The srad program has a slowdown of 4,437x and nw has a slowdown \nof 1,126x. By contrast, ludcmp s slowdown is only 4.54x. After optimization, most programs show performance \nim\u00adprovements and none have worse performance. However, several fail to surpass the CPU-only sequential \nversions. For comparison, we simulate an idealized inspector-executor system. The inspector\u00adexecutor \nsystem has an oracle for scheduling and transfers exactly one byte between CPU and GPU for each accessed \nallocation unit. A compiler creates the inspector from the original loop [4]. To mea\u00adsure performance \nignoring applicability constraints, the inspector\u00adexecutor simulation ignores its applicability guard. \nCGCM outper\u00adforms this idealized inspector-executor system. The disadvantages of sequential inspection \nand frequent synchronization were not overcome by transferring dramatically fewer bytes. Figure 4 shows \nperformance results for automatic paralleliza\u00adtion coupled with automatic communication management. Across \nall 24 applications, communication optimizations never reduce per\u00adformance. This is a surprising result \nsince the glue kernel optimiza\u00adtion has the potential to lower performance, and CGCM s imple\u00admentation \nlacks a performance guard. Communication optimiza\u00adtion improves the performance for .ve of the sixteen \nPolyBench programs and six of the eight other programs. For many PolyBench programs, the outermost loop \nexecutes on the GPU, so there are no loops left on the CPU for map promotion to target. Therefore, op\u00adtimization \nonly improve performance for six of the 16 PolyBench programs. Table 3 shows the number of GPU kernels \ncreated by the DOALL parallelizer. For each DOALL candidate, CGCM auto\u00admatically managed communication \ncorrectly without programmer intervention. Unlike CGCM, the parallelizer requires static alias analysis. \nIn practice, CGCM is more applicable than the simple DOALL transformation pass. The table also shows \nthe applicability of named regions [12] and inspector-executor management systems [4, 14, 22]. Af.ne \ncom\u00admunication management [24] has the same applicability as named regions, but a different implementation. \nThe named region and inspector-executor techniques require that each of the live-ins is a distinct named \nallocation unit. The named regions technique also requires induction-variable based array indexes. The \nnamed region and inspector-executor systems are applicable to 66 of 67 kernels in the PolyBench applications. \nHowever, they are applicable to only 14 of 34 kernels from the more complex non-PolyBench applica\u00adtions. \nAlthough inspector-executor and named region based tech\u00adniques have different applicability guards, they \nboth fail to transfer memory for the same set of kernels. Table 3 shows the GPU execution and communication \ntime as a percent of total execution time. The contributions of CPU exe\u00adcution and IO are not shown. \nThis data indicates the performance limiting factor for each program: either GPU execution, commu\u00adnication, \nor some other factor (CPU or IO). GPU execution time dominates total execution time for 13 programs, \nten from Poly\u00adbench and three from other applications. The simpler PolyBench programs are much more likely \nto be GPU performance bound than the other more complex programs. GPU-bound programs would bene.t from \nmore ef.cient parallelizations, perhaps using the poly\u00adhedral model. Communication limits the performance \nof .ve pro\u00adgrams, all from PolyBench. The only application where inspector\u00adexecutor outperforms CGCM, \ngramschmidt, falls in this category. Finally, six programs, one from PolyBench and .ve from else\u00adwhere, \nare neither communication nor GPU performance bound. Improving the performance of these applications \nwould require par\u00adallelizing more loops. Two of the applications that are neither GPU nor communication-bound, \nsrad and blackscholes, outperform sequential CPU-only execution. These applications have reached the \nlimit of Amdahl s Law for the current parallelization. The manual Rodinia parallelizations involved complex \nalgorith\u00admic improvements. For example, in hotspot the authors replace the original grid-based simulation \nwith a simulation based on the pyramid method. Surprisingly, the simple automatic parallelization coupled \nwith CGCM is competitive with expert programmers using algorithmic transformations. Table 3 explains \nwhy. Programmers tend to optimize a program s hottest loops, but ignore the second and third tier loops \nwhich become important once the hottest loops scale to thousands of threads. Automatic GPU parallelizations \nsub\u00adstitutes quantity for quality, pro.ting from Amdahl s Law. 7. Related Work Although there has been \nprior work on automatic paralleliza\u00adtion and semi-automatic communication management for GPUs, these \nimplementations have not addressed the problems of fully\u00adautomatic communication management and optimization. \nCUDA-lite [25] translates low-performance, na\u00a8ive CUDA func\u00adtions into high performance code by coalescing \nand exploiting GPU shared memory. However, the programmer must insert transfers to the GPU manually. \nC-to-CUDA for Af.ne Programs [3] and A mapping path for GPGPU [13] automatically transform programs similar \nto the PolyBench programs into high performance CUDA C using the polyhedral model. Like CUDA-lite, they \nrequire the programmer to manage memory. OpenMP to GPGPU [12] proposes an automatic compiler for the \nsource-to-source translation of OpenMP applications into CUDA C. Most programs do not have OpenMP annotations. \nFur\u00adthermore, these annotations are time consuming to add and not performance portable. Their system \nautomatically transfers named regions between CPU and GPU using two passes. The .rst pass copies all \nnamed annotated regions to the GPU for each GPU func\u00adtion, and the second cleanup pass removes all the \ncopies that are not live-in. The two passes acting together produce a communica\u00adtion pattern equivalent \nto unoptimized CGCM communication. JCUDA [26] uses the Java type system to automatically trans\u00adfer GPU \nfunction arguments between CPU and GPU memories. JCUDA requires an annotation indicating whether each \nparameter is live-in, live-out, or both. Java implements multidimensional ar\u00adrays as arrays of references. \nJCUDA uses type information to .atten these arrays to Fortran-style multidimensional arrays but does \nnot support recursive data-types. The PGI Fortran and C compiler [24] features a mode for semi\u00adautomatic \nparallelization for GPUs. Users target loops manually with a special keyword. The PGI compiler can automatically \ntrans\u00adfer named regions declared with C99 s restrict keyword to the GPU and back by determining the range \nof af.ne array indices. The restrict keyword marks a pointer as not aliasing with other pointers. By \ncontrast, CGCM is tolerant of aliasing and does not require programmer annotations. The PGI compiler \ncannot paral\u00adlelize loops containing general pointer arithmetic, while CGCM preserves the semantics of \npointer arithmetic. Unlike CGCM, the PGI compiler does not automatically optimize communication across \nGPU function invocations. However, programmers can use an optional annotation to promote communication \nout of loops. Incorrectly using this annotation will cause the program to access stale or inconsistent \ndata. Inspector-executor systems [18, 21] create specialized inspec\u00adtors to identify precise dependence \ninformation among loop iter\u00adations. Some inspector-executor systems achieve acyclic commu\u00adnication when \ndynamic dependence information is reusable. This condition is rare in practice. Salz et al. assume a \nprogram annota\u00adtion to prevent unsound reuse [21]. Rauchwerger et al. dynamically check relevant program \nstate to determine if dependence informa\u00adtion is reusable [18]. The dynamic check requires expensive \nse\u00adquential computation for each outermost loop iteration. If the check fails, the technique defaults \nto cyclic communication.  8. Conclusion CGCM is the .rst fully automatic system for managing and opti\u00admizing \nCPU-GPU communication. CPU-GPU communication is a crucial problem for manual and automatic parallelizations. \nManu\u00adally transferring complex data-types between CPU and GPU mem\u00adories is tedious and error-prone. Cyclic \ncommunication constrains the performance of automatic GPU parallelizations. By managing and optimizing \nCPU-GPU communication, CGCM eases manual GPU parallelizations and improves the performance and applica\u00adbility \nof automatic GPU parallelizations. CGCM has two parts, a run-time library and an optimizing com\u00adpiler. \nThe run-time library s semantics allow the compiler to man\u00adage and optimize CPU-GPU communication without \nprogrammer annotations or heroic static analysis. The compiler breaks cyclic communication patterns by \ntransferring data to the GPU early in the program and retrieving it only when necessary. CGCM out\u00adperforms \ninspector-executor systems on 24 programs and enables a whole program geomean speedup of 5.36x over best \nsequential CPU-only execution. Acknowledgments We thank the entire Liberty Research Group for their support \nand feedback during this work. We also thank Helge Rhodin for gen\u00aderously contributing his PTX backend. \nAdditionally, we thank the anonymous reviewers for their insightful comments. This material is based \non work supported by National Science Foundation Grants 0964328 and 1047879, and by United States Air \nForce Contract FA8650-09-C-7918. James A. Jablin is supported by a Department of Energy Of.ce of Science \nGraduate Fellowship (DOE SCGF). References [1] ISO/IEC 9899-1999 Programming Languages C, Second Edition, \n1999. [2] C. Ancourt and F. Irigoin. Scanning polyhedra with DO loops. In Proceedings of the Third ACM \nSIGPLAN Symposium on Principles and Practice of Parallel Programming (PPoPP), 1991. [3] M. M. Baskaran, \nJ. Ramanujam, and P. Sadayappan. Automatic C-to-CUDA code generation for af.ne programs. In Compiler \nConstruction (CC), 2010. [4] A. Basumallik and R. Eigenmann. Optimizing irregular shared\u00admemory applications \nfor distributed-memory systems. In Proceedings of the Eleventh ACM SIGPLAN Symposium on Principles and \nPractice of Parallel Programming (PPoPP), 2006. [5] C. Bienia, S. Kumar, J. P. Singh, and K. Li. The \nPARSEC benchmark suite: characterization and architectural implications. In Proceedings of the 17th International \nConference on Parallel Architectures and Compilation Techniques (PACT), 2008. [6] I. Buck, T. Foley, \nD. Horn, J. Sugerman, K. Fatahalian, M. Houston, and P. Hanrahan. Brook for GPUs: Stream computing on \ngraphics hardware. ACM Transactions on Graphics, 23, 2004. [7] S. Che, M. Boyer, J. Meng, D. Tarjan, \nJ. W. Sheaffer, S.-H. Lee, and K. Skadron. Rodinia: A benchmark suite for heterogeneous computing. 2009. \n[8] D. M. Dang, C. Christara, and K. Jackson. GPU pricing of exotic cross-currency interest rate derivatives \nwith a foreign exchange volatil\u00adity skew model. SSRN eLibrary, 2010. [9] P. Feautrier. Some ef.cient \nsolutions to the af.ne scheduling problem: I. one-dimensional time. International Journal of Parallel \nProgram\u00adming (IJPP), 1992. [10] D. R. Horn, M. Houston, and P. Hanrahan. Clawhmmer: A streaming HMMer-Search \nimplementation. Proceedings of the Conference on Supercomputing (SC), 2005. [11] Khronos Group. The OpenCL \nSpeci.cation, September 2010. [12] S. Lee, S.-J. Min, and R. Eigenmann. OpenMP to GPGPU: a compiler framework \nfor automatic translation and optimization. In Proceed\u00adings of the Fourteenth ACM SIGPLAN Symposium on \nPrinciples and Practice of Parallel Programming (PPoPP), 2009. [13] A. Leung, N. Vasilache, B. Meister, \nM. M. Baskaran, D. Wohlford, C. Bastoul, and R. Lethin. A mapping path for multi-GPGPU accel\u00aderated computers \nfrom a portable high level programming abstraction. In Proceedings of the 3rd Workshop on General-Purpose \nComputation on Graphics Processing Units (GPGPU), pages 51 61, 2010. [14] S.-J. Min and R. Eigenmann. \nOptimizing irregular shared-memory applications for clusters. In Proceedings of the 22nd Annual Interna\u00adtional \nConference on Supercomputing (SC). ACM, 2008. [15] NVIDIA Corporation. CUDA C Best Practices Guide 3.2, \n2010. [16] NVIDIA Corporation. NVIDIA CUDA Programming Guide 3.0, February 2010. [17] L.-N. Pouchet. \nPolyBench: The Polyhedral Benchmark suite. http://www-roc.inria.fr/ pouchet/software/polybench/download. \n[18] L. Rauchwerger, N. M. Amato, and D. A. Padua. A scalable method for run-time loop parallelization. \nInternational Journal of Parallel Programming (IJPP), 26:537 576, 1995. [19] H. Rhodin. LLVM PTX Backend. \nhttp://sourceforge.net/projects/llvmptxbackend. [20] S. Ryoo, C. I. Rodrigues, S. S. Baghsorkhi, S. S. \nStone, D. B. Kirk, and W.-m. W. Hwu. Optimization principles and application performance evaluation of \na multithreaded GPU using CUDA. In Proceedings of the Thirteenth ACM SIGPLAN Symposium on Principles \nand Practice of Parallel Programming (PPoPP), 2008. [21] J. Saltz, R. Mirchandaney, and R. Crowley. Run-time \nparallelization and scheduling of loops. IEEE Transactions on Computers, 40, 1991. [22] S. D. Sharma, \nR. Ponnusamy, B. Moon, Y.-S. Hwang, R. Das, and J. Saltz. Run-time and compile-time support for adaptive \nirregular problems. In Proceedings of the Conference on Supercomputing (SC). IEEE Computer Society Press, \n1994. [23] StreamIt benchmarks. http://compiler.lcs.mit.edu/streamit. [24] The Portland Group. PGI Fortran \n&#38; C Accelator Programming Model. White Paper, 2010. [25] S.-Z. Ueng, M. Lathara, S. S. Baghsorkhi, \nand W.-m. W. Hwu. CUDA-Lite: Reducing GPU Programming Complexity. In Proceeding of the 21st International \nWorkshop on Languages and Compilers for Parallel Computing (LCPC), 2008. [26] Y. Yan, M. Grossman, and \nV. Sarkar. JCUDA: A programmer-friendly interface for accelerating Java programs with CUDA. In Proceedings \nof the 15th International Euro-Par Conference on Parallel Processing. Springer-Verlag, 2009.    \n\t\t\t", "proc_id": "1993498", "abstract": "<p>The performance benefits of GPU parallelism can be enormous, but unlocking this performance potential is challenging. The applicability and performance of GPU parallelizations is limited by the complexities of CPU-GPU communication. To address these communications problems, this paper presents the first fully automatic system for managing and optimizing CPU-GPU communcation. This system, called the CPU-GPU Communication Manager (CGCM), consists of a run-time library and a set of compiler transformations that work together to manage and optimize CPU-GPU communication without depending on the strength of static compile-time analyses or on programmer-supplied annotations. CGCM eases manual GPU parallelizations and improves the applicability and performance of automatic GPU parallelizations. For 24 programs, CGCM-enabled automatic GPU parallelization yields a whole program geomean speedup of 5.36x over the best sequential CPU-only execution.</p>", "authors": [{"name": "Thomas B. Jablin", "author_profile_id": "81343495702", "affiliation": "Princeton University, Princeton, NJ, USA", "person_id": "P2690515", "email_address": "tjablin@cs.princeton.edu", "orcid_id": ""}, {"name": "Prakash Prabhu", "author_profile_id": "81464645003", "affiliation": "Princeton University, Princeton, NJ, USA", "person_id": "P2690516", "email_address": "pprabhu@cs.princeton.edu", "orcid_id": ""}, {"name": "James A. Jablin", "author_profile_id": "81485659242", "affiliation": "Brown University, Providence, RI, USA", "person_id": "P2690517", "email_address": "jjablin@cs.brown.edu", "orcid_id": ""}, {"name": "Nick P. Johnson", "author_profile_id": "81470644754", "affiliation": "Princeton University, Princeton, NJ, USA", "person_id": "P2690518", "email_address": "npjohnso@cs.princeton.edu", "orcid_id": ""}, {"name": "Stephen R. Beard", "author_profile_id": "81485645000", "affiliation": "Princeton University, Princeton, NJ, USA", "person_id": "P2690519", "email_address": "sbeard@cs.princeton.edu", "orcid_id": ""}, {"name": "David I. August", "author_profile_id": "81100388492", "affiliation": "Princeton University, Princeton, NJ, USA", "person_id": "P2690520", "email_address": "august@cs.princeton.edu", "orcid_id": ""}], "doi_number": "10.1145/1993498.1993516", "year": "2011", "article_id": "1993516", "conference": "PLDI", "title": "Automatic CPU-GPU communication management and optimization", "url": "http://dl.acm.org/citation.cfm?id=1993516"}