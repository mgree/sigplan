{"article_publication_date": "06-04-2011", "fulltext": "\n Veri.cation of Semantic Commutativity Conditions and Inverse Operations on Linked Data Structures Deokhwan \nKim Martin C. Rinard Massachusetts Institute of Technology {dkim,rinard}@csail.mit.edu Abstract We present \na new technique for verifying commutativity conditions, which are logical formulas that characterize \nwhen operations com\u00admute. Because our technique reasons with the abstract state of veri\u00ad.ed linked data \nstructure implementations, it can verify commuting operations that produce semantically equivalent (but \nnot necessar\u00adily identical) data structure states in different execution orders. We have used this technique \nto verify sound and complete commuta\u00adtivity conditions for all pairs of operations on a collection of \nlinked data structure implementations, including data structures that ex\u00adport a set interface (ListSet \nand HashSet) as well as data structures that export a map interface (AssociationList, HashTable, and \nAr\u00adrayList). This effort involved the speci.cation and veri.cation of 765 commutativity conditions. Many \nspeculative parallel systems need to undo the effects of speculatively executed operations. Inverse operations, \nwhich undo these effects, are often more ef.cient than alternate approaches (such as saving and restoring \ndata structure state). We present a new technique for verifying such inverse operations. We have speci.ed \nand veri.ed, for all of our linked data structure implementations, an inverse operation for every operation \nthat changes the data structure state. Together, the commutativity conditions and inverse operations \nprovide a key resource that language designers, developers of pro\u00adgram analysis systems, and implementors \nof software systems can draw on to build languages, program analyses, and systems with strong correctness \nguarantees. Categories and Subject Descriptors D.1.3 [Programming Tech\u00adniques]: Concurrent Programming; \nD.2.4 [Software Engineering]: Software/Program Veri.cation; F.3.1 [Logics and Meanings of Programs]: \nSpecifying and Verifying and Reasoning about Pro\u00adgrams General Terms Languages, Reliability, Veri.cation \nKeywords Commutativity Condition, Data Structure, Inverse Op\u00aderation, Veri.cation 1. Introduction Commuting \noperations on shared data structures (operations that produce the same result regardless of the order \nin which they execute) play a central role in many parallel computing systems: Parallelizing Compilers: \nIf a compiler can statically detect that all operations in a given computation commute, it can generate \nparallel code for that computation [41]. Permission to make digital or hard copies of all or part of \nthis work for personal or classroom use is granted without fee provided that copies are not made or distributed \nfor pro.t or commercial advantage and that copies bear this notice and the full citation on the .rst \npage. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior \nspeci.c permission and/or a fee. PLDI 11, June 4 8, 2011, San Jose, California, USA. Copyright &#38;#169; \n2011 ACM 978-1-4503-0663-8/11/06. . . $10.00 c Deterministic Parallel Languages: Including support for \ncommuting operations in deterministic parallel languages in\u00adcreases the expressive power of the language \nwhile preserving guaranteed deterministic parallel execution [5, 42].  Transaction Monitors: If a transaction \nmonitor can detect that operations within parallel transactions commute, it can use ef.cient locking \nalgorithms that allow commuting operations from different transactions to interleave [17, 49]. Because \nsuch locking algorithms place fewer constraints on the execution order, they increase the amount of exploitable \nparallelism.  Irregular Parallel Computations: Exploiting commuting op\u00aderations has been shown to be \ncritical for obtaining good paral\u00adlel performance in irregular parallel computations that manip\u00adulate \nlinked data structures [28 30]. The reason is essentially the same as for ef.cient transaction monitors \n it enables the use of ef.cient synchronization algorithms for atomic transac\u00adtions that execute multiple \n(potentially commuting) operations on shared objects. For similar reasons, exploiting commuting operations \nhas also been shown to be essential for obtaining good parallel performance for the SPEC benchmarks [7]. \n  Despite the importance of commuting operations, there has been relatively little research in automatically \nanalyzing or verifying the conditions under which operations commute. Indeed, the determin\u00adistic parallel \nlanguage, transaction monitor, and irregular parallel computation systems cited above all rely on the \ndeveloper to iden\u00adtify commuting operations, with no way to determine whether the operations do, in fact, \ncommute or not. A mistake in identifying commuting operations invalidates both the principles upon which \nthe systems operate and the correctness guarantees that they claim to provide. 1.1 Previous Research \nCommutativity analysis [41] uses static program analysis to .nd operations that produce identical concrete \nobject states in all exe\u00adcution orders. But this approach is inadequate for linked data struc\u00adtures \nconsider, for example, a linked list that implements a set interface. Operations that insert elements \ninto this data structure commute at the semantic level all insertion orders produce the same abstract \nset of elements. But they do not commute at the concrete implementation level different insertion orders \n(even though they produce the same set) produce different linked lists. Any commutativity analysis that \nreasons at the concrete implemen\u00adtation level (as opposed to the abstract semantic level) would there\u00adfore \nconservatively conclude that such operations do not commute. Another approach uses random interpretation \n[22] to detect commuting operations [1]. This technique explores control .ow paths from different execution \norders, with af.ne join operations combining states from different control .ow paths to avoid expo\u00adnential \nblowup in the number of analyzed states. Instead of directly comparing states from different execution \norders, the technique rea\u00adsons about the return values of the functions that the program uses to observe \nthe different states. Because effective af.ne join opera\u00adtions do not currently exist for linked data \nstructures, this approach does not detect commuting operations on linked data structures. Both of these \napproaches are designed only to .nd operations that commute in all possible object states and for all \npossible parameter values. But some operations commute only under certain conditions. Consider, for example, \nan operation that removes a key, value pair from a hash table and an operation that looks up a given \nkey in the hash table. These operations commute only if the two keys are different. Recognizing and exploiting \nsuch commutativity conditions is essential to obtaining good parallel performance for many irregular \ncomputations [28 30] these computations use the commutativity conditions to dynamically recognize and \nexploit commuting operations whose commutativity properties they cannot statically resolve.  1.2 Semantic \nCommutativity Analysis This paper presents a new approach for verifying the speci.c con\u00additions under \nwhich operations on linked data structures seman\u00adtically commute. Instead of reasoning directly about \nthe concrete data structure state, this approach builds on the availability of fully veri.ed linked data \nstructure implementations [51, 52] to reason at the higher semantic level of the (veri.ed) abstract data \nstructure state. The approach is therefore able to detect operations that com\u00admute at the semantic level \neven though they may produce different concrete data structure states. We have used this approach to \nverify both soundness (conceptually, if the conditions hold, the operations produce the same abstract \ndata structure state regardless of the or\u00adder in which they execute, see Section 4) and completeness \n(con\u00adceptually, if the conditions do not hold, then different execution or\u00adders produce different abstract \ndata structure states, see Section 4) of commutativity conditions. We have speci.ed and veri.ed sound \nand complete commuta\u00adtivity conditions for all pairs of operations from a variety of linked data structure \nimplementations: Sets: ListSet and HashSet both implement a set interface. List-Set uses a singly-linked \nlist; HashSet uses a hash table.  Maps: AssociationList, HashTable, and ArrayList all imple\u00adment a map \ninterface. AssociationList uses a singly-linked list of key, value pairs; HashTable implements a separately-chained \nhash table an array contains linked lists of key, value pairs with a hash function mapping keys to linked \nlists via the array. ArrayList maps integers to objects and is optimized for storing maps from a dense \nsubset of the integers starting at 0.  Accumulator: Accumulator maintains a value that clients can increase \nand read.  Altogether, we speci.ed and veri.ed 765 commutativity conditions (216 from ListSet and HashSet, \n294 from AssociationList and HashTable, 243 from ArrayList, and 12 from Accumulator). Because the implementations \nof all of these data structures have been veri.ed to correctly implement their speci.cations, the se\u00admantic \ncommutativity conditions and inverses are guaranteed to be valid for the concrete data structure implementations \nthat execute when the program runs. We emphasize, however, that our technique works with data structure \nspeci.cations, not data structure imple\u00admentations. In particular, it is capable of verifying semantic \ncom\u00admutativity conditions and inverses even in the absence of any im\u00adplementation at all (in this case, \nof course, the commutativity con\u00additions and inverses are sound only to the extent that the actual data \nstructure implementation, when provided, correctly implements its speci.cation). We note that the well-known \ndif.culty of reasoning about se\u00admantic properties of linked data structures [43] has limited the range \nof available results in this area. These veri.ed commutativity conditions therefore provide a solid foundation \nfor the use of these linked data structures in a range of parallel programs and systems.  1.3 Inverse \nOperations In one of our usage scenarios, the system uses the commutativity conditions to dynamically \ndetect speculatively executed operations that do not commute with previously executed operations [28 \n30]. In this case, the system must roll the data structure back to the abstract semantic state before \nthe operations executed, then continue from this restored state. Executing inverse operations that undo \nthe effect of executed operations can be substantially more ef.cient than alternate ap\u00adproaches (such \nas pessimistically saving the data structure state be\u00adfore operations execute, then restoring the state \nto roll back the ef\u00adfect of the operations). Note that even though the restored abstract semantic state \nis the same, the underlying concrete states may dif\u00adfer. For example, the inverse of an operation that \nremoves an ele\u00adment from a set implemented as a linked list inserts the removed element back into the \nlist. Even though the reinserted element may appear in a different position in the list, the restored \nabstract set is the same as the original set. We have developed an approach that is capable of verifying \nsemantic inverse operations. We have speci.ed inverses for all of the operations on our set of data structures \nthat update the data structure state. We have used our approach to verify that all of these inverses \nundo the effect of the corresponding operation to correctly restore the initial abstract state of the \nlinked data structure. We note that the need to undo the effects of executed opera\u00adtions occurs pervasively \nthroughout computer systems, from clas\u00adsical database transaction processing systems [21] to systems \nthat recover from security breaches [20, 27, 38, 45]. In addition to the speci.c motivating use described \nabove, the veri.ed inverse oper\u00adations may therefore .nd broader applicability in a variety of con\u00adtexts \nin which it is desirable to ef.ciently undo data structure state changes. 1.4 Jahob We use the Jahob \nprogram speci.cation and veri.cation system to specify and verify the data structure implementations, \ncommutativ\u00adity conditions, and inverse operations. Jahob enables developers to write higher-order logic \nspeci.cations for Java programs [51, 52]. It also enables developers to guide proofs of complex program \nprop\u00aderties by using the Jahob integrated proof language to resolve key choice points in these proofs \n[52]. Once these choice points have been resolved, Jahob uses integrated reasoning to invoke a variety \nof powerful reasoning systems (such as .rst-order provers [44, 48], SMT provers [10, 19], MONA [24], \nand the BAPA [31, 34] deci\u00adsion procedure) to discharge the resulting automatically generated veri.cation \nconditions [51, 52]. In our approach, a data structure implementor speci.es com\u00admutativity conditions \nfor pairs of data structure operations and/or inverses for individual operations that update the data \nstructure state. Our commutativity condition and inverse operation veri.ca\u00adtion system generates stylized \nJahob methods whose veri.cation establishes the validity of the corresponding commutativity condi\u00adtions \nand inverse operations. In our experience, Jahob is often able to verify these methods without further \nintervention from the data structure implementor. Speci.cally, for our set of linked data struc\u00adture \nimplementations, all but 57 of the 1530 automatically gener\u00adated commutativity testing methods and all \nof the eight automat\u00adically generated inverse testing methods verify as generated. If a method does not \nverify, the data structure implementor uses the Ja\u00adhob proof language [52] to appropriately guide the \nveri.cation of the method. For our set of linked data structure implementations, Jahob was able to verify \nthe remaining 57 commutativity testing methods after we augmented the methods with a total of 201 Jahob \nproof language commands (see Section 5).  1.5 Contributions This paper makes the following contributions: \n Semantic Commutativity Analysis: It presents a new commu\u00adtativity analysis technique that veri.es the \nsoundness and com\u00adpleteness of semantic commutativity conditions for linked data structures. Because \nthis analysis reasons about the abstract se\u00admantic state of the data structure (as opposed to the concrete \nim\u00adplementation state), it can verify semantic commutativity con\u00additions that are inherently beyond the \nreach of previously pro\u00adposed approaches. To the best of our knowledge, this analysis is the .rst to \nverify semantic commutativity conditions for linked data structures.  Semantic Commutativity Conditions: \nIt presents veri.ed sound and complete commutativity conditions for a variety of linked data structures. \nIn this paper all of these commutativity conditions are provided by the developer and veri.ed by our \nimplemented system. To the best of our knowledge, these are the .rst fully veri.ed semantic commutativity \nconditions for linked data structures.  Semantic Inverse Analysis: It presents a new analysis for ver\u00adifying \ninverse operations that undo the effect of previously exe\u00adcuted operations on linked data structures. \nBecause the analysis reasons about the abstract data structure state, it can verify se\u00admantic inverses \nthat correctly restore the abstract data structure state even though they may produce different concrete \nstates. To the best of our knowledge, this analysis is the .rst to verify semantic inverse operations \nfor linked data structures.  Semantic Inverse Operations: It presents veri.ed inverses for operations \nthat update the data structure state. Systems can use these operations to ef.ciently roll back speculatively \nexecuted data structure operations. To the best of our knowledge, these are the .rst fully veri.ed semantic \ninverse operations for linked data structures.  Experience: It discusses our experience using the Jahob \n[6, 51, 52] program speci.cation and veri.cation system to specify and verify commutativity conditions \nand inverse operations for a group of data structures including a ListSet and HashSet that implement \na set interface, an AssociationList, HashTable, and ArrayList that implement a map interface, and an \nAccumulator.  We emphasize that in this paper, we focus on the speci.cation and veri.cation of the commutativity \nconditions and inverse opera\u00adtions. We assume that any parallel system that uses these conditions will \nimplement some synchronization mechanism that ensures that the operations execute atomically. Such mechanisms \nare already implemented and available in many of the systems which we ex\u00adpect to be of interest [5, 7, \n17, 28, 29, 41, 42, 49]. 2. Example Figure 1 presents the Jahob interface for the HashSet class, which \nuses a separately-chained hash table [9] to implement a set inter\u00adface. The HashSet class, like all of \nour data structures, is writ\u00adten in Java augmented with speci.cations written in the Jahob higher-order \nlogic speci.cation language. The interface exports a collection of speci.ed operations. Each operation \nspeci.cation consists of a precondition (the requires clause), a postcondition (the ensures clause), \nand a modifies clause. These speci.cations completely capture the desired behavior of the data structure \n(with the exception of properties involving execution time and/or mem\u00adory consumption) [51, 52]. 2.1 \nAbstract State The interface uses the abstract state of the HashSet to specify the behavior of HashSet \noperations. This state consists of the set contents of objects in the HashSet, the size of this set, \nand the .ag init, which is true if the HashSet has been initialized (see lines 2, 3, and 4 of Figure \n1). The speci.cation for the add(v) operation, for example, uses this abstract state to specify that, \nif the HashSet is initialized and the parameter v is not null, it adds v to the set of objects in the \nHashSet (see lines 11-14 of Figure 1). 2.2 Concrete State and Abstraction Function When the program \nruns, the HashSet operations manipulate the concrete state of the HashSet. The concrete state consists \nof the array table, which contains pointers to linked lists of elements in the HashSet, and the int _size, \nwhich stores the size of the hash table (see lines 5 and 6 of Figure 1). An abstraction function in the \nform of Jahob invariants (not shown, but see the complete data structure speci.cations and im\u00adplementations \navailable in the technical report version of the pa\u00adper [26]) speci.es the relationship between the concrete \nand ab\u00adstract states [51, 52]. Like all of the data structures in this paper, we have used the Jahob \nsystem to verify that the HashSet correctly im\u00adplements its interface [51, 52]. This veri.cation, of \ncourse, includes the veri.cation of the abstraction function. 2.3 Commuting Operations Consider the \nadd(v1) and contains(v2) operations on a Hash-Set s. These operations commute if and only if v1 does \nnot equal v2 or v1 is already in s. Figure 2 presents the two methods that our system automatically generates \nto verify the soundness and completeness of this commutativity condition. The .rst method (contains_add_between_s_40, \nline 1 of Figure 2) veri.es soundness. The second method (contains_add_between_c_40, line 14 of Figure \n2) veri.es completeness. The methods are written in a subset of Java with Jahob annotations [52]. 2.4 \nVerifying Commutativity Condition Soundness The generated soundness testing method executes the add(v1) \nand contains(v2) operations in both execution orders on equivalent HashSets (HashSets with the same abstract \nstate). The method speci.cation checks that, if the commutativity condition is true, then both execution \norders produce the same return values and .nal abstract HashSet states. If Jahob veri.es the method (which \nit does in this case without developer assistance), it has veri.ed that if the commutativity condition \nholds, then the operations commute. The requires clause (lines 2 and 3 of Figure 2) ensures that the \nmethod starts with two HashSets (sa and sb) that have identical abstract states. The method .rst applies \nthe sa.contains(v1) and sa.add(v2) operations to one of the HashSets (sa). A Jahob assume command (line \n8 of Figure 2) instructs Jahob to assume the commutativity condition (v1 ~=v2|r1a). The method next executes \nthe two operations in the reverse order on the second HashSet sb (lines 10 and 11 of Figure 2). The assert \ncommand at the end of the method (line 12 of Figure 2) checks that the return values are the same in \nboth execution orders and that the two HashSets have the same abstract states at the end of the method. \nIn this example the commutativity condition works with the be\u00adtween state that is available after the \n.rst operation executes but before the second operation executes. A system would use such a between condition \njust before executing the add(v2) operation to dynamically check if this operation commutes with a previously \nex\u00adecuted contains(v1) operation. We also verify before conditions (which may be used to determine if \ntwo operations that have yet to execute will commute when they execute) and after conditions (which may \nbe used to trigger rollbacks when already executed op\u00aderations do not commute [28, 29]). 1 public class \nHashSet { 2 /*: public ghost specvar init :: \"bool\" = \"False\"; */ 3 /*: public ghost specvar contents \n:: \"obj set\" = \"{}\"; */ 4 /*: public specvar size :: \"int\"; */ 5 private Node[] table; 6 private int \n_size; 7 public HashSet() 8 /*: modifies \"init\", \"contents\", \"size\" 9 ensures \"init &#38; contents = \n{} &#38; size = 0\" */ { ... } 10 public boolean add(Object v) 11 /*: requires \"init &#38; v ~= null\" \n12 modifies \"contents\", \"size\" 13 ensures \"(v ~: old contents --> contents = old contents Un {v} &#38; \nsize = old size + 1 &#38; result) &#38; 14 (v : old contents --> contents = old contents &#38; size \n= old size &#38; ~result)\" */ { ... } 15 public boolean contains(Object v) 16 /*: requires \"init &#38; \nv ~= null\" 17 ensures \"result = (v : contents)\" */ { ... } 18 public boolean remove(Object v) 19 /*: \nrequires \"init &#38; v ~= null\" 20 modifies \"contents\", \"size\" 21 ensures \"(v : old contents --> contents \n= old contents -{v} &#38; size = old size -1 &#38; result) &#38; 22 (v ~: old contents --> contents \n= old contents &#38; size = old size &#38; ~result)\" */ { ... } 23 public int size() 24 /*: requires \n\"init\" 25 ensures \"result = size\" */ { ... } 26 } Figure 1. The Jahob HashSet Speci.cation 1 static \nvoid contains_add_between_s_40(HashSet sa, HashSet sb, Object v1, Object v2) 2 /*: requires \"sa ~= null \n&#38; sb ~= null &#38; sa ~= sb &#38; sa..init &#38; sb..init &#38; v1 ~= null &#38; v2 ~= null &#38; \n3 sa..contents = sb..contents &#38; sa..size = sb..size\" 4 modifies \"sa..contents\", \"sb..contents\", \n\"sa..size\", \"sb..size\" 5 ensures \"True\" */ 6 { 7 boolean r1a = sa.contains(v1); 8 /*: assume \"v1 ~= v2 \n| r1a\" */ 9 sa.add(v2); 10 sb.add(v2); 11 boolean r1b = sb.contains(v1); 12 /*: assert \"r1a = r1b &#38; \nsa..contents = sb..contents &#38; sa..size = sb..size\" */ 13 } 14 static void contains_add_between_c_40(HashSet \nsa, HashSet sb, Object v1, Object v2) 15 /*: requires \"sa ~= null &#38; sb ~= null &#38; sa ~= sb &#38; \nsa..init &#38; sb..init &#38; v1 ~= null &#38; v2 ~= null &#38; 16 sa..contents = sb..contents &#38; \nsa..size = sb..size\" 17 modifies \"sa..contents\", \"sb..contents\", \"sa..size\", \"sb..size\" 18 ensures \"True\" \n*/ 19 { 20 boolean r1a = sa.contains(v1); 21 /*: assume \"~(v1 ~= v2 | r1a)\" */ 22 sa.add(v2); 23 sb.add(v2); \n24 boolean r1b = sb.contains(v1); 25 /*: assert \"~(r1a = r1b &#38; sa..contents = sb..contents &#38; \nsa..size = sb..size)\" */ 26 } Figure 2. HashSet Commutativity Testing Methods for Between Commutativity \nCondition for contains(v1) and add(v2) 1 static void add_0(HashSet s, Object v) 2 /*: requires \"s ~= \nnull &#38; s..init &#38; v ~= null\" 3 modifies \"s..contents \", \"s..size\" 4 ensures \"True\" */ 5 { 6 boolean \nr = s.add(v); 7 if (r) { s.remove(v); } 8 /*: assert \"s..contents = s..(old contents) &#38; s..size \n= s..(old size)\" */ 9 } Figure 3. HashSet Inverse Operation Testing Method for add(v) 1 static void \nput_0(HashTable s, Object k, Object v) 2 /*: requires \"s ~= null &#38; s..init &#38; k ~= null &#38; \nv ~= null\" 3 modifies \"s..contents \", \"s..size\" 4 ensures \"True\" */ 5 { 6 Object r = s.put(k, v); 7 \nif (r != null) { s.put(k, r); } else { s.remove(k); } 8 /*: assert \"s..contents = s..(old contents) \n&#38; s..size = s..(old size)\" */ 9 } Figure 4. HashTable Inverse Operation Testing Method for put(k, \nv) 2.5 Verifying Commutativity Condition Completeness The contains_add_between_c_40 method, which checks \ncom\u00adpleteness, uses a similar pattern except it negates both the com\u00admutativity condition and the assertion \nat the end of the generated method. If Jahob veri.es the method (which it does in this case without developer \nassistance), it has veri.ed that if the commuta\u00adtivity condition does not hold, then the operations produce \ndifferent return values or different abstract data structure states when they execute in different orders. \n 2.6 Verifying Inverse Operations Figure 3 presents the generated inverse testing method for the HashSet \nadd(v) operation. This method .rst executes the add(v) operation, then the inverse if (r) { s.remove(v); \n}. The in\u00adverse must consider two cases: when v was in the set before the execution of add(v) (in which \ncase the inverse must not remove v) and when v was not in the set before the execution of add(v) (in \nwhich case the inverse must remove v). Note that the inverse uses the return value r to distinguish these \ntwo cases. The .nal assert command forces Jahob to prove that the .nal abstract state is the same as \nthe initial abstract state. Note that there is no requirement that the .nal concrete state must be the \nsame as the initial concrete state. Figure 4 presents the inverse testing method for a more complex operation, \nthe HashTable put(k, v) operation. If the initial state of the HashTable mapped k to a value, the inverse \nreinserts the mapping (the value to which the HashTable mapped k is available as the return value r from \nthe put(k, v) operation). If the initial state did not map k to a value, the inverse removes the mapping \nthat the add(k,v) inserted, leaving k unmapped as in the initial state. Both of the inverses in our example \nuse the return value from the operation to carry information from the initial state that the inverse \ncan then use to undo the effect of the operation. This approach works for all of our linked data structures. \nWe note that it is, in general, possible for operations to destroy information from the initial state \nthat the inverse needs to restore this initial state. In this case the operation needs to save information \nfrom the initial state so that the inverse can later use this saved information to restore the initial \nstate. 3. Commutativity and Inverse Testing Methods The commutativity testing method generator takes \nas input the data structure interface and, for each pair of data structure operations, developer-speci.ed \nbefore, between, and after commutativity con\u00additions. It produces as output the commutativity testing \nmethods. It then presents each method to the Jahob program veri.cation sys\u00adtem [51, 52]. If the method \nveri.es, the system has veri.ed the cor\u00adresponding commutativity condition. If it does not verify, either \nthe commutativity condition is not sound or complete or Jahob is not capable of verifying the soundness \nand completeness without ad\u00additional developer assistance. The developer then, as appropriate, either \nmodi.es the commutativity condition or augments the gen\u00aderated commutativity testing methods with additional \nproof com\u00admands written in the Jahob proof language [52]. 3.1 Completeness Commutativity Testing Template \nFigure 5 presents the template that the generator uses to produce the completeness commutativity testing \nmethod. The generation pro\u00adcess simply iterates over all commutativity testing conditions (and corresponding \npairs of operations in the data structure interface), .lling in the template parameters as appropriate. \nIn Figure 5 all template parameters appear in italic font. The name of the commutativity testing method \ncontains the names of the two operations, a .eld that speci.es whether the method tests a before, between, \nor after commutativity condition, the tag c (which identi.es the method as a completeness testing method), \nand a numerical identi.er id. The method takes as param\u00adeters two data structures (sa and sb) and the \nparameters of the two data structure operations. The requires clause ensures the data structures are \ndistinct but have identical abstract states. The generated method uses Jahob assume commands to instruct \nJahob to assume that the preconditions of the operations hold in the .rst execution order. If the preconditions \ndo not involve the state of the data structure (as in our example in Figure 2), the generator moves the \npreconditions up into the requires clause. The generator also uses an assume command to insert the nega\u00adtion \nof the commutativity condition (recall that the template is a completeness template and therefore includes \nthe negation of the condition) in the appropriate place in the generated method. The template identi.es \nthe insertion points for all three kinds of com\u00admutativity conditions (before, between, and after). A \ngenerated method, of course, contains a commutativity condition at only one of these points. The method \nnext contains the operations in the reverse order, with assume commands instructing Jahob to assume that \nthe pre\u00adconditions of the operations hold. Once again, if the preconditions do not depend on the data \nstructure state, the generator places them in the requires clause of the method, not before the operation \ninvocations as in the template. The method ends with the .nal assertion (which Jahob must prove) that \neither one of the corresponding return values or the .nal abstract states are different in the two different \nexecution orders. As is appropriate for a completeness testing method, this struc\u00adture forces Jahob to \nprove that if the operation preconditions and the negation of the commutativity condition holds in the \n.rst execution order, then either one of the operation preconditions is violated in the reverse execution \norder or the .nal assertion holds.  3.2 Soundness Commutativity Testing Template The soundness commutativity \ntesting template has the same basic structure as the completeness template, with the exception that 1) \nit inserts the commutativity testing condition (not its negation), 2) it omits the assume command for \nthe operation preconditions in the second execution order, and 3) the .nal assertion forces Jahob to \nprove that the return values and .nal abstract states are the same in both execution orders. As is appropriate \nfor a soundness testing method, this structure forces Jahob to prove that if the operation preconditions \nand com\u00admutativity condition holds in the .rst execution order, then the op\u00aderation preconditions hold \nin the reverse execution order and the return values and .nal abstract states are the same.  3.3 Inverse \nTesting Methods The inverse testing method generator takes as input the data struc\u00adture interface and \na developer-speci.ed set of inverse operation pairs. It produces as output the inverse testing methods \nand feeds each method to the Jahob program veri.cation system [52]. As for the commutativity testing \nmethods, the developer may, if necessary, augment the inverse testing methods with additional Jahob proof \ncommands. Figure 6 presents the template that the generator uses to produce the inverse testing methods. \nThe generation process simply iterates over all of the speci.ed inverses, .lling in the template parameters \n(in italic font) as appropriate. The .nal Jahob assert command requires Jahob to prove that the .nal \nabstract state (after the appli\u00adcation of the inverse operation) is the same as the initial abstract \nstate from the start of the method. Jahob must also prove the pre\u00adcondition of the inverse operation. \n4. Formal Treatment We assume a set s . S of concrete states and a corresponding set sS. S of abstract \nstates. We also assume that the data structure de.nes an abstraction function a : S . S . The commutativity \nand inverse testing methods work with log\u00adical formulas written in the higher-order logic Jahob speci.cation \nlanguage [52]. For our data structures, the speci.cations, commuta\u00adtivity conditions, commutativity testing \nmethods, and inverse test\u00ading methods require only .rst-order logic. Given an operation m(v) on a given \ndata structure, pre(m(v)) denotes the precondition of the method m from the data struc\u00adture speci.cation. \nThe precondition is a logical formula written in the Jahob speci.cation language [52]. It is expressed \nin the name space of the caller (i.e., with the formal parameter from the de.ni\u00ad 1 static void method1_method2_(before \n| between | after)_c_id 2 (sa decl, sb decl, argv1 decls, argv2 decls) 3 /*: requires \"sa ~= null &#38; \nsb ~= null &#38; sa ~= sb &#38; 4 sa abstract state = sb abstract state\" 5 modifies \"sa frame condition\", \n\"sb frame condition\" 6 ensures \"True\" */ 7 { 8 [/*: assume \"~(before commutativity condition)\" */] \n9 /*: assume \"method1 precondition\" */ 10 r1a type r1a = sa.method1(argv1); 11 [/*: assume \"~(between \ncommutativity condition)\" */] 12 /*: assume \"method2 precondition\" */ 13 r2a type r2a = sa.method2(argv2); \n 14 [/*: assume \"~(after commutativity condition)\" */] 15 /*: assume \"method2 precondition\" */ 16 r2b \ntype r2b = sb.method2(argv2); 17 /*: assume \"method1 precondition\" */ 18 r1b type r1b = sb.method1(argv1); \n 19 /*: assert \"~(r1a = r1b &#38; r2a = r2b &#38; 20 sa abstract state = sb abstract state)\" */ 21 } \nFigure 5. Template for Completeness Commutativity Testing Methods 1 static void method_id(s decl, argv \ndecls) 2 /*: requires \"s ~= null &#38; method precondition\" 3 modifies \"s frame condition\" 4 ensures \n\"True\" */ 5 { 6 r type r = s.method(argv); 7 execute inverse operation(); 8 /*: assert \"s abstract \nstate = s initial abstract state\" */ 9 } Figure 6. Template for Inverse Testing Methods tion of m replaced \nby the actual parameter v from the caller). We write a(s) |= pre(m(v)) if the precondition is true in \nthe abstract state a(s). ' We write (s,r) = s.m(v) if executing the operation m(v) in state s produces \nreturn value r and new state s'. Given a starting state s and two operations m1(v1) and m2(v2), we are \ninterested in the following states and return values (see Figure 7): (s(;2,r(;2) = s.m1(v1): the intermediate \nstate s(;2 and return value r(;2 that results from executing m1(v1) in the original state s.  (s1;@,r1;@) \n= s(;2.m2(v2): the .nal state s1;@ and return value r1;@ that results from executing m2(v2) in the intermedi\u00adate \nstate s(;2.  (s@;1,r@;1) = s.m2(v2): the intermediate state s@;1 and return value r@;1 that results \nfrom executing m2(v2) in the original state s.  (s2;(,r2;() = s@;1.m1(v1): the .nal state s2;( and return \nvalue r2;( that results from executing m1(v1) in the intermedi\u00adate state s@;1.  We are interested in \nstates and return values for the two op\u00aderations m1(v1) and m2(v2) executing in both execution orders \n(m1(v1) followed by m2(v2) and m2(v2) followed by m1(v1)). The subscripts in our notation are designed \nto identify both the execu\u00adtion order and (with a circle) the most recent operation that has executed. \nSo, for example, s(;2 denotes the state after the opera\u00ad a a Figure 7. Execution on Concrete States \nand Abstract States tion m1(v1) executes when the operation m1(v1) executes .rst and then m2(v2) executes. \nSimilarly, r2;( denotes the value that the operation m1(v1) returns when the operation m2(v2) executes \n.rst and then the operation m1(v1) executes and returns r2;(. 4.1 Commutativity Conditions A commutativity \ncondition f is a logical formula written in the Ja\u00adhob speci.cation language [52]. In general, the free \nvariables of f can include the arguments v1 and v2, the return values r(;2 and r1;@, and abstract speci.cation \nvariables that denote various ele\u00adments of the three abstract states a(s), a(s(;2), and a(s1;@). We write \n((s(;2,r(;2) = s.m1(v1); (s1;@,r1;@) = s(;2.m2(v2)) |= f if the commutativity condition f is satis.ed \nwhen the operations execute in the order m1(v1); m2(v2) (.rst m1(v1), then m2(v2)). We anticipate that \nstatic analyses will work with commutativity conditions that involve the abstract state. Systems that \ndynamically evaluate commutativity conditions, of course, must work with the concrete data structure \nstate, not the abstract data structure state. We therefore use the abstraction function to translate \ncommutativ\u00adity conditions over the abstract states into commutativity conditions over the concrete states. \nSection 5 presents examples that illustrate this translation. 4.1.1 Soundness and Completeness Conceptually, \na commutativity condition is sound if, whenever the commutativity condition and the preconditions of \nthe two opera\u00adtions are satis.ed in the .rst execution order, then 1) the precondi\u00adtions of the operations \nare satis.ed in the second execution order, 2) the operations return the same return values in both execution \nor\u00adders, and 3) the abstract .nal states are the same in both execution orders. We formalize this concept \nas follows. Given a commutativity condition f for two operations m1(v1) and m2(v2), the veri.cation of \nthe soundness commutativity testing method for these two operations establishes (by the construction \nof this method) the following property: Property 1 (soundness). If a(s) |= pre(m1(v1)) and a(s(;2) |= \npre(m2(v2)) and ((s(;2,r(;2) = s.m1(v1); (s1;@, r1;@) = s(;2.m2(v2)) | = f then a(s) |= pre(m2(v2) and \na(s@;1) |= pre(m1(v1)) and r(;2 = r2;( and r1;@ = r@;1 and a(s1;@)= a(s2;(). Conceptually, a commutativity \ncondition is complete if, when\u00adever the preconditions of the two operations are satis.ed in the .rst \nexecution order but the commutativity condition is not satis.ed, then either 1) the preconditions of \nthe operations are not satis.ed in the second execution order, 2) the return values are different in \nthe second execution order, or 3) the abstract .nal states are dif\u00adferent in the two execution orders. \nWe formalize this concept as follows. Given a commutativity condition f for two operations m1(v1) and \nm2(v2), the veri.cation of the completeness commutativity testing method for these two operations establishes \n(by the con\u00adstruction of this method) the following property: Property 2 (completeness). If a(s) |= pre(m1(v1)) \nand a(s(;2) |= pre(m2(v2)) and ((s(;2,r(;2) = s.m1(v1); (s1;@,r1;@) = s(;2.m2(v2)) |= ~f then a(s) |= \n~pre(m2(v2)) or a(s@;1) |= ~pre(m1(v1)) or r(;2 = r2;( or r1;@= r@;1 or a(s1;@)= a(s2;().  4.1.2 Kinds \nof Commutativity Conditions In general, a system or analysis may wish to test if a commutativity condition \nis satis.ed either 1) before either operation executes, 2) after the .rst operation executes but before \nthe second operation executes, and/or 3) after both operations execute. We therefore identify the following \nkinds of commutativity conditions: Before Condition: A commutativity condition f is a before condition \nif its free variables include at most the arguments v1 and v2 and elements of the initial abstract state \na(s). Because a before condition does not involve the return values r(;2 and r1;@, the intermediate state \na(s(;2), or the .nal state a(s1;@), it is possible to evaluate the condition before either of the operations \nexecutes.  Between Condition: A commutativity condition f is a between condition if its free variables \ninclude at most the arguments v1 and v2, the initial abstract state a(s), the .rst return value r(;2, \nand elements of the intermediate abstract state a(s(;2). Be\u00adcause the between condition does not involve \nthe second return value r1;@ or the .nal abstract state a(s1;@), it is possible to evaluate the condition \nafter the .rst operation m1(v1) executes but before the second operation m2(v2) executes. Note that if \nthe between condition references elements of the initial abstract state a(s), the system may need to \nsave corresponding values from this state before the .rst operation m1(v1) executes so that it can subsequently \nuse the saved values to evaluate the between condition after the .rst operation executes.  After Condition: \nAll commutativity conditions are after con\u00additions. Note that if an after condition references the .rst \nre\u00adturn value r(;2 or elements of the initial or intermediate abstract states a(s) or a(s(;2), the system \nmay need to save referenced elements of these states so that it can evaluate the after condi\u00adtion after \nboth operations execute.  In practice we expect developers to minimize references to ele\u00adments of previously \ncomputed initial or intermediate abstract states when they specify between and after conditions. One \nstrategy re\u00adplaces clauses in commutativity conditions that reference elements of the initial or intermediate \nabstract states with equivalent clauses that reference return values from executed operations. Before \ncon\u00additions for our set-based data structures, for example, often test whether a parameter v of one of \nthe operations is an element of the set in the initial state s. The return value of the operation often \nindicates whether the element was, in fact, an element of this ini\u00adtial set. The corresponding between \nand after conditions can then replace the clause that tests membership in the initial set with an equivalent \nclause that tests the return value. See Section 5 for spe\u00adci.c occurrences of this pattern in the commutativity \nconditions for our set of data structures. For completeness, some of the between conditions cannot help \nquerying the initial state (the state before the .rst operation ex\u00adecutes). For the same reason, some \nof the after conditions query the initial and/or between states. In practice, there are two ways to dynamically \ncheck such commutativity conditions: 1) perform the query before the operation executes and record the \nresult for the commutativity condition to check after the operation executes, or 2) drop the clause containing \nthe query from the commutativity con\u00addition and use the resulting simpler, conservative, but not complete \ncommutativity condition that does not reference the initial and/or between states. Because the commutativity \nconditions for our set of data struc\u00adtures are both sound and complete, the before, between, and after \nconditions are equivalent even if they reference different return val\u00adues or elements of different abstract \nstates.  4.2 Inverse Operations Conceptually, an operation is an inverse of an initial operation if \nwhenever the precondition of the initial operation is satis.ed, then 1) the precondition of the inverse \nis satis.ed after the initial operation executes and 2) executing the inverse after the initial operation \nexecutes restores the abstract state (but not necessarily the concrete state) back to what it was before \nthe initial operation executed. We formalize this concept as follows. Given an operation m1(v1) with \ninverse operation m2(v2), the veri.cation of the inverse testing method for these two operations establishes \n(by the construction of this method) the following prop\u00aderty: Property 3 (inverse). If a(s) |= pre(m1(v1)) \nthen a(s(;2) |= pre(m2(v2)) and a(s)= a(s1;@). 5. Experimental Results We next discuss the commutativity \nconditions, inverse operations, and veri.cation process for our set of data structures. We .rst dis\u00adcuss \nthe operations that each data structure exports. The source code for all of the data structures (including \nboth speci.cation and implementation) as well as the commutativity and inverse testing methods (which \ncontain all of the commutativity conditions and Jahob proof constructs required to enable Jahob to verify \nthe meth\u00adods) is available in the technical report version of the paper [26]. The Accumulator implements \na counter with two operations: increase(v): Adds the number v to the counter.  read(): Returns the \nvalue in the counter.  HashSet and ListSet implement a set of elements with the fol\u00adlowing operations. \nBecause they implement the same speci.cation, they have the same commutativity conditions. add(v): Adds \nthe element v to the set of elements in the data structure. Returns false if the element was already \npresent and true otherwise.  contains(v): Returns true if the element v is in the set and false otherwise. \n remove(v): Removes the element v from the set. Returns true if v was included in the set and false \notherwise.  size(): Returns the number of elements in the set.  HashTable and AssociationList implement \na map from keys to values with the following operations. Because they implement the same speci.cation, \nthey have the same commutativity conditions. containsKey(k): Returns true if there exists a value v \nfor the key k in the map.  get(k): Returns the value v for the key k, or null if k is not mapped.  \n put(k, v): Maps the key k to the value v. Returns the previous value for the key k, or null if k was \nnot mapped.  remove(k): Removes the mapping for the key k. Returns the value that the key k was mapped \nto, or null if the data structure did not have a mapping for the key k.  size(): Returns the number \nof key, value pairs in the data structure.  ArrayList implements a map from the integers to objects \nwith the following operations: add at(i, v): Pushes all objects with indices greater than or equal to \ni up one position to create an empty position at index i, then inserts the object v into that position. \n get(i): Returns the object at index i.  indexOf(v): Returns the index of the .rst occurrence of the \nobject v or -1 if the object v is not in the map.  lastIndexOf(v): Returns the index of the last occurrence \nof the object v or -1 if the object v is not in the map.  remove at(i): Removes the element at the speci.ed \nindex i, then slides all objects above i down one position to .ll the newly empty position at index i. \n set(i, v): Replaces the object at the index i with the ob\u00adject v. Returns the replaced object previously \nat index i.  size(): Returns the number of elements in the map.  5.1 Commutativity Conditions Tables \n1 through 7 present the commutativity conditions for se\u00adlected illustrative pairs of operations from \nour set of linked data structures. For space reasons, we do not present all 765 commuta\u00adtivity conditions \n(the complete set of conditions is available in the technical report version of the paper [26]). The \n.rst and second columns in each table identify the pair of operations. The third column presents the \ncommutativity conditions in terms of the arguments, return values, and abstract data structure states. \nThese commutativity conditions are suitable for static anal\u00adyses that reason about the commutativity \nconditions at the level of the abstract states. The fourth column translates any abstract state queries \n(typically set membership operations) into operations that can be invoked on the concrete data structure. \nThese commutativity conditions are suitable for dynamically checking the commutativity conditions when \nthe program runs. The commutativity conditions assume the operations operate on the same data structure \n(operations on different data structures triv\u00adially commute). s1 denotes the data structure state before \nthe .rst operation executes; s2 denotes the state of the same data structure after the .rst operation \nexecutes but before the second operation executes. Each commutativity condition in the table corresponds \nto the execution order in which the operation in the .rst column executes .rst followed by the operation \nin the second column. The before condition tables are symmetric (for a given pair of operations, the \ncommutativity conditions are the same for both exe\u00adcution orders). The between condition tables may be \nasymmetric if the commutativity condition references either the return value from the .rst operation \n(which is not available in the other execution or\u00adder) or depends on the intermediate data structure \nstate (which may be different in the other execution order). Similarly, the after tables may be asymmetric \nif the commutativity condition depends on the intermediate or .nal data structure states. In general, \nthe commutativity conditions take the form of a dis\u00adjunction of clauses. Dropping clauses produces conservative \nsound commutativity conditions that may be easier (for static analyses) or more ef.cient (for dynamic \ncheckers) to work with. Such com\u00ad Methods Before / Between / After Commutativity Condition s1.increase(v1) \ns2.increase(v2) true r2 = s2.read() v1 = 0 r1 = s1.read() s2.increase(v2) v2 = 0 r2 = s2.read() true \n Table 1. Before / Between / After Commutativity Conditions on Accumulator Methods Before Commutativity \nCondition s1.add(v1) s2.add(v2) true true r2 = s2.contains(v2) v1 = v2 . v1 . s1 v1 = v2 . s1.contains(v1) \n= true s2.remove(v2) v1 = v2 v1 = v2 r1 = s1.contains(v1) s2.add(v2) v1 = v2 . v1 . s1 v1 = v2 . s1.contains(v1) \n= true r2 = s2.contains(v2) true true s2.remove(v2) v1 = v2 . v1 . s1 v1 = v2 . s1.contains(v1) = false \ns1.remove(v1) s2.add(v2) v1 = v2 v1 = v2 r2 = s2.contains(v2) v1 = v2 . v1 . s1 v1 = v2 . s1.contains(v1) \n= false s2.remove(v2) true true Table 2. Before Commutativity Conditions on ListSet and HashSet Methods \nBetween Commutativity Condition s1.add(v1) s2.add(v2) true true r2 = s2.contains(v2) v1 = v2 . v1 . s1 \nv1 = v2 . s1.contains(v1) = true s2.remove(v2) v1 = v2 v1 = v2 r1 = s1.contains(v1) s2.add(v2) v1 = v2 \n. r1 = true v1 = v2 . r1 = true r2 = s2.contains(v2) true true s2.remove(v2) v1 = v2 . r1 = false v1 \n= v2 . r1 = false s1.remove(v1) s2.add(v2) v1 = v2 v1 = v2 r2 = s2.contains(v2) v1 = v2 . v1 . s1 v1 \n= v2 . s1.contains(v1) = false s2.remove(v2) true true Table 3. Between Commutativity Conditions on \nListSet and HashSet Methods Before Commutativity Condition r1 = s1.get(k1) r2 = s2.get(k2) true true \ns2.put(k2,v2) k1 = k2 . (k1, v2) . s1 k1 = k2 . s1.get(k1) = v2 s2.remove(k2) k1 = k2 . (k1, ) . s1 k1 \n= k2 . s1.containsKey(k1) = false s1.put(k1,v1) r2 = s2.get(k2) k1 = k2 . (k1, v1) . s1 k1 = k2 . s1.get(k1) \n= v1 s2.put(k2,v2) k1 = k2 . v1 = v2 k1 = k2 . v1 = v2 s2.remove(k2) k1 = k2 k1 = k2 s1.remove(k1) r2 \n= s2.get(k2) k1 = k2 . (k1, ) . s1 k1 = k2 . s1.containsKey(k1) = false s2.put(k2,v2) k1 = k2 k1 = k2 \ns2.remove(k2) true true Table 4. Before Commutativity Conditions on AssociationList and HashTable Methods \nAfter Commutativity Condition r1 = s1.get(k1) r2 = s2.get(k2) true true s2.put(k2,v2) k1 = k2 . r1 = \nv2 k1 = k2 . r1 = v2 s2.remove(k2) k1 = k2 . r1 = null k1 = k2 . r1 = null s1.put(k1,v1) r2 = s2.get(k2) \nk1 = k2 . (k1, v1) . s1 k1 = k2 . s1.get(k1) = v1 s2.put(k2,v2) k1 = k2 . v1 = v2 k1 = k2 . v1 = v2 s2.remove(k2) \nk1 = k2 k1 = k2 s1.remove(k1) r2 = s2.get(k2) k1 = k2 . (k1, ) . s1 k1 = k2 . s1.containsKey(k1) = false \ns2.put(k2,v2) k1 = k2 k1 = k2 s2.remove(k2) true true Table 5. After Commutativity Conditions on AssociationList \nand HashTable Methods Between Commutativity Condition s1.add at(i1,v1) s2.add at(i2,v2) (i1 < i2 = |s2| \n- 1 . s2[i2] = v2) . (i1 = i2 . v1 = v2) . (i1 > i2 . s2[i1 - 1] = v1) (i1 < i2 = s2.size() - 1 . s2.get(i2) \n= v2) . (i1 = i2 . v1 = v2) . (i1 > i2 . s2.get(i1 - 1) = v1) r2 = s2.indexOf(v2) \u00ac(.i : s2[i] = v2) \n. (.i < i1 : s2[i] = v2) . (\u00ac(.i < i1 : s2[i] = v2) . s2[i1] = v2 . s2[i1 + 1] = v2) s2.indexOf(v2) < \n0 . 0 = s2.indexOf(v2) < i1 . (s2.indexOf(v2) = i1 . s2.get(i1 + 1) = v2) s2.remove at(i2) (i1 < i2 < \n|s2| - 1 . s2[i2] = s2[i2 + 1]) . (|s2| - 2 = i1 = i2 . s2[i1 + 1] = v1) . (|s2| - 2 = i1 > i2 . s2[i1 \n+ 1] = v1) (i1 < i2 < s2.size() - 1 . s2.get(i2) = s2.get(i2 + 1)) . (s2.size() - 2 = i1 = i2 . s2.get(i1 \n+ 1) = v1) . (s2.size() - 2 = i1 > i2 . s2.get(i1 + 1) = v1) r1 = s1.indexOf(v1) s2.add at(i2,v2) (r1 \n< 0 . v1 = v2) . 0 = r1 < i2 . (r1 = i2 . v1 = v2) (r1 < 0 . v1 = v2) . 0 = r1 < i2 . (r1 = i2 . v1 = \nv2) r2 = s2.indexOf(v2) true true s2.remove at(i2) r1 < 0 . 0 = r1 < i2 . (r1 = i2 . i2 < |s2| - 1 . \ns2[i2 + 1] = v1) r1 < 0 . 0 = r1 < i2 . (r1 = i2 . i2 < s2.size() - 1 . s2.get(i2 + 1) = v1) s1.remove \nat(i1) s2.add at(i2,v2) (i1 < i2 . s2[i2 - 1] = v2) . (i1 = i2 . s1[i1] = v2) . (i1 > i2 . s2[i1 - 1] \n= s1[i1]) (i1 < i2 . s2.get(i2 - 1) = v2) . (i1 = i2 . s1.get(i1) = v2) . (i1 > i2 . s2.get(i1 - 1) = \ns1.get(i1)) r2 = s2.indexOf(v2) (\u00ac(.i : s2[i] = v2) . s1[i1] = v2) . (.i < i1 : s2[i] = v2) . (\u00ac(.i < \ni1 : s2[i] = v2) . s2[i1] = v2 . s1[i1] = v2 . i1 < |s2|) (s2.indexOf(v2) < 0 . s1.get(i1) = v2) . 0 \n= s2.indexOf(v2) < i1 . (s2.indexOf(v2) = i1 . s1.get(i1) = v2 . i1 < s2.size()) s2.remove at(i2) (i1 \n< i2 . s2[i2 - 1] = s2[i2]) . i1 = i2 . (i1 < i2 . s2.get(i2 - 1) = s2.get(i2)) . i1 = i2 . (s2.size() \n> i1 > i2 . s1.get(i1) = s2.get(i1)) (|s2| > i1 > i2 . s1[i1]= s2[i1]) Table 6. Between Commutativity \nConditions on ArrayList Methods After Commutativity Condition s1.add at(i1, v1) s2.add at(i2, v2) (i1 \n< i2 = |s3| - 2 . s3[i2 + 1] = v2) . (i1 = i2 . v1 = v2) . (i1 > i2 . s3[i1] = v1) (i1 < i2 = s3.size() \n- 2 . s3.get(i2 + 1) = v2) . (i1 = i2 . v1 = v2) . (i1 > i2 . s3.get(i1) = v1) r2 = s2.indexOf(v2) r2 \n< 0 . 0 = r2 < i1 . (r2 = i1 . s3[i + 1] = v2) r2 < 0 . 0 = r2 < i1 . (r2 = i1 . s3.get(i1 + 1) = v2) \ns2.remove at(i2) (i1 < i2 < |s3| . s2[i2] = s3[i2]) . (|s3| - 1 = i1 = i2 . s3[i1] = v1) . (|s3| - 1 \n= i1 > i2 . s3[i1] = v1) (i1 < i2 < s3.size() . s2.get(i2) = s3.get(i2)) . (s3.size() - 1 = i1 = i2 . \ns3.get(i1) = v1) . (s3.size() - 1 = i1 > i2 . s3.get(i1) = v1) r1 = s1.indexOf(v1) s2.add at(i2, v2) \n(r1 < 0 . v1 = v2) . 0 = r1 < i2 . (r1 = i2 . v1 = v2) (r1 < 0 . v1 = v2) . 0 = r1 < i2 . (r1 = i2 . \nv1 = v2) r2 = s2.indexOf(v2) true true s2.remove at(i2) r1 < 0 . 0 = r1 < i2 . (r1 = i2 . i2 < |s3| . \ns3[i2] = v1) r1 < 0 . 0 = r1 < i2 . (r1 = i2 . i2 < s3.size() . s3.get(i2) = v1) s1.remove at(i1) s2.add \nat(i2, v2) (i1 < i2 . s3[i2 - 1] = v2) . (i1 = i2 . s1[i1] = v2) . (i1 > i2 . s3[i1] = s1[i1]) (i1 < \ni2 . s3.get(i2 - 1) = v2) . (i1 = i2 . s1.get(i1) = v2) . (i1 > i2 . s3.get(i1) = s1.get(i1)) r2 = s2.indexOf(v2) \n(r2 < 0 . s1[i1] = v2) . 0 = r2 < i1 . (r2 = i1 . s1[i1] = v2 . i1 < |s3|) (r2 < 0 . s1.get(i1) = v2) \n. 0 = r2 < i1 . (r2 = i1 . s1.get(i1) = v2 . i1 < s3.size()) s2.remove at(i2) (i1 < i2 . s3[i2 - 1] = \ns2[i2]) . i1 = i2 . (i1 < i2 . s3.get(i2 - 1) = s2.get(i2)) . i1 = i2 . (s3.size() + 1 > i1 > i2 . s1.get(i1) \n= s3.get(i1 - 1)) (|s3| + 1 > i1 > i2 . s1[i1]= s3[i1 - 1]) Table 7. After Commutativity Conditions \non ArrayList mutativity conditions are, of course, no longer complete. If the dropped clauses usually \nhave no effect on the value of the commu\u00adtativity condition, the gain in ease of reasoning or ef.ciency \nmay be worth the loss of completeness. One particularly useful special case is when the commutativity \ncondition is true i.e., the operations commute regardless of the data structure state. For example, \nadd operations typically com\u00admute with other add operations, contains operations typically commute with \nother contains operations, and remove operations typically commute with other remove operations. Such \ncommuta\u00adtivity conditions are particularly easy to reason about at compile time since the compiler does \nnot need to reason about the parame\u00adter values or state to .nd commuting operations. In general, our \ndata structures implement the update operations that return values (add(v), remove(v), put(k, v), remove(k), \nremove_at(i), and set(i, v)). For example, the add(v) oper\u00adation from the ListSet and HashSet data structures \nreturns true if the elememt v was not already present in the abstract set, while the remove(v) operation \nreturns true if the element v was in the abstract set. We have veri.ed commutativity conditions for two \nvariants of these operations one in which the client records the return value (typically by assigning \nthe return value to a variable) and another in which the client discards the return value. The tables \nin this paper present the commutativity conditions only for the variants that dis\u00adcard the return value; \nthe complete tables available in the technical report version of the paper [26] present commutativity \nconditions for both variants. Because clients that record the return values ob\u00adserve more information \nabout the data structure, the commutativity conditions for these variants can be more complex. For example, \nthe between commutativity condition for the r1a = sa.add(v1), r2a = sa.add(v2) pair is (v1 ~= v2 | ~r1a) \n(i.e., either v1 and v2 are different or v1 was already in the set before the .rst operation executed), \nwhile the commutativity condition for the s.add(v1), s.add(v2) pair is simply true. For a data structure \nwith n operations, there are 3n2 commu\u00adtativity conditions a before, between, and after condition for \neach pair of operations. For our data structures we consider two versions of operations that update the \ndata structure one with a return value and one that discards the return value. So there are 2 operations \nfor Accumulator, 6 for HashSet and ListSet, 7 for HashTable and AssociationList, and 9 for ArrayList, \nfor a total of (3 * 22)+ 2 * (3 * 62)+ 2 * (3 * 72)+(3 * 92)= 765 commutativity conditions and 1530 generated \ncommutativity testing methods a soundness testing method and a completeness testing method for each \ncommutativity condition.  5.2 Veri.cation of the Commutativity Conditions For HashSet, ListSet, AssociationList, \nHashTable, and Accumula\u00adtor, all of the automatically generated commutativity testing meth\u00adods verify \nas generated. Table 8 presents the time required to ver\u00adify all of these automatically generated methods. \nThe veri.cation times are all quite reasonable less than four minutes for all data structures except \nArrayList. For ArrayList, 429 of the 486 methods verify as generated. The entry for ArrayList in Table \n8 indicates that Jahob spent 12m 18s attempting to verify all 486 automatically generated methods (with \nthe majority of this time spent waiting for the Jahob integrated reasoning systems to time out as they \ntry, but fail, to verify the 57 methods that require additional proof commands), 3m 04s verifying the \n429 methods that verify as generated, and 27s verifying the remaining 57 methods after the addition of \nthe required Jahob proof commands. In general, the commutativity conditions for ArrayList are sub\u00adstantially \nmore complicated than for other the data structures. We Data Structure Veri.cation Time Accumulator 0.8s \nAssociationList 1m 35s HashSet 44s HashTable 3m 20s ListSet 40s ArrayList 12m 18s (3m 04s, 27s) Table \n8. Commutativity Testing Method Veri.cation Times Proof Language Command Count note 128 assuming 51 pickWitness \n22 Total 201  Table 9. Additional Jahob Proof Language Commands for Re\u00admaining 57 ArrayList Commutativity \nTesting Methods attribute this complexity in part to the use of integer indexing and in part to the presence \nof operations (such as add_at and remove_at) that shift the indexing relationships across large regions \nof the data structure. The veri.cation of the remaining 57 ArrayList commutativity testing methods required \nthe addition of 128 note commands, 51 assuming commands, and 22 pickWitness commands (see Table 9). In \ngeneral, the note command allows the developer to specify an intermediate formula for Jahob to prove. \nJahob can then use this formula in subsequent proofs. In this way, the developer can identify a lemma \nstructure that helps Jahob .nd the proof. The assuming command allows the developer to prove formu\u00adlas \nof the form A =. B (by assuming A, then using A to prove B). We typically use the assuming command when \nJahob is unable to prove a goal B in one case A of the cases of the commutativity condition (or, when \nproving completeness, the negation of the com\u00admutativity condition). Providing a proof of A =. B enables \nJahob to verify the commutativity condition. The pickWitness command allows the developer to start with \nan existentially quanti.ed formula, name an element for which the formula holds, then remove the quanti.er \nand use the resulting for\u00admula in a subsequent proof. We typically use this command when the commutativity \ncondition (or its negation) contains an existential quanti.er and we need to use the commutativity condition \nto prove a goal. 5.3 Verifying the Remaining 57 Methods The 57 remaining methods fall naturally into \nfour categories. Each requires the proof language commands to manipulate either an existentially quanti.ed \nformula or the negation of such a formula. 12 of the 57 methods are a soundness testing method for a \ncom\u00adbination of either add at(i,v1) or remove at(i) with either indexOf(v2) or lastIndexOf(v2). The commutativity \ncondi\u00adtion is either a between or after condition. We discuss the between condition for add_at(i,v1) \nwith indexOf(v2). The other com\u00adbinations are similar. One of the cases of the commutativity con\u00addition \nstates that v2 is not present in the intermediate state of the map (so that indexOf(v2) returns -1). \nIn this case Jahob must prove that the element is also not present in the initial state before add_at(i,v1) \nexecutes (so that the return value of indexOf(v2) is -1 in both execution orders). In effect, Jahob must \nprove that if the element is not present in the intermediate state, it is also not present in the initial \nstate in other words, Jahob must prove that the The Z3 [10] and CVC3 [19] decision procedures were each \ngiven a 20\u00adsecond timeout. negation of one existentially quanti.ed formula implies the nega\u00adtion of \nanother existentially quanti.ed formula. To enable Jahob to prove this fact, we use an assuming command, \na pickWitness command, and several note commands to prove the contraposi\u00adtion (i.e., that if the element \nis present in the initial state, then it is also present in the intermediate state). A key step in the \nproof of the contraposition involves the identi.cation of the new position of v2 in the array after the \nadd_at(i,v1) shifts it over (Jahob can automatically prove the cases when it is not shifted). 8 of the \n57 methods are a soundness testing method for com\u00adbinations of remove_at(i) with indexOf(v). In these \nmethods Jahob must prove that the return value of indexOf(v) is the same in both execution orders. The \nproof involves a case analysis of the initial state of the ArrayList. In one of the cases, the initial \nstate contains two adjacent copies of v: one at location i and the other at location i+1. In this case, \nremove_at(i) removes the .rst occur\u00adrence of v, leaving the second occurrence of v in location i. In \nboth execution orders indexOf(v) returns i (but i references concep\u00adtually different versions of v in \nthe two execution orders). Jahob is unable to prove this fact without help. The addition of a note com\u00admand \nthat identi.es the case and the new position of the second v after the remove_at(v) operation executes \nenables Jahob to com\u00adplete the proof. The formula that identi.es the case is the negation of a complex \nexistentially quanti.ed formula. 20 of the 57 methods are a completeness testing method for various combinations \nof add_at(i,v), remove_at(i,v), and set(i,v). In these methods Jahob must prove that the two .nal abstract \nstates are different. In general, Jahob accomplishes such a proof by .nding an element that is present \nin one abstract state but not the other. In some cases, however, Jahob is unable to .nd such an element. \nThe addition of an assuming command (which identi.es the case) and note commands that identify the element \nand help Jahob prove which abstract state contains the element and which does not enables Jahob to complete \nthe case analysis. The relevant formula identifying the case is existentially quanti.ed. 17 of the 57 \nmethods are a completeness testing method for combinations of either add at(i,v1) or remove at(i) with \nei\u00adther indexOf(v2) or lastIndexOf(v2). Recall that the opera\u00adtion add_at(i,v1) shifts the region of \nthe map above i up to make space for v1 at index i. Similarly, remove_at(i) shifts the region of the \nmap above i down to .ll the hole left by the removed element. The veri.cation of the completeness testing \nmethod in\u00advolves a case analysis of the relative positions of the inserted or removed element and the \nelement v2 (whose index is returned by indexOf(v2) or lastIndexOf(v2)). In one of the cases Jahob is \nunable to reason successfully about these relative positions. The ad\u00addition of an assuming command (which \nidenti.es the case) and a note command that identi.es the precise position of v2 (this note command follows \nfrom the formula which identi.es the case) en\u00adables Jahob to complete the case analysis. Once again, \nthe formula identifying the relevant case is existentially quanti.ed.  5.4 Inverse Operations Table \n10 presents, for every operation that changes the data struc\u00adture s abstract state, the corresponding \ninverse operation that rolls back the effect of the .rst operation to restore the original abstract state. \ns1 and s2 denote the data structure states before and after the .rst operation executes, respectively. \nNote that some of the inverse operations use the return value from the .rst operation. Any system that \napplies such inverse operations must therefore store the return value from the .rst operation so that \nit can provide the return value to the corresponding inverse operation. All of the eight inverse test\u00ading \nmethods veri.ed as generated without the need for additional Jahob proof commands. Operation Inverse \nOperation Accumulator s1.increase(v) s2.increase(-v) ListSet HashSet r = s1.add(v) if r = true then s2.remove(v) \nr = s1.remove(v) if r = true then s2.add(v) AssociationList HashTable r = s1.put(k,v) if r = null then \ns2.put(k,r) else s2.remove(k) r = s1.remove(k) if r = null then s2.put(k,r) ArrayList s1.add at(i,v) \ns2.remove at(i) r = s1.remove at(i) s2.add at(i,r) r = s1.set(i,v) s2.set(i,r) Table 10. Inverse Operations \n 6. Related Work A general theme in this research is decoupling the veri.cation of data structure implementations \nfrom the analysis of data structure clients. The immediate goal of the research presented in this paper \nis to verify commutativity conditions and inverses for sophisticated linked data structures. Other systems \ncan then build on the avail\u00adability of these veri.ed conditions and inverses to more effectively analyze \nand transform clients that use the data structures. This approach is designed to encapsulate the complex \nreason\u00ading required to verify sophisticated data structure properties (such as commutativity and inverses) \nwithin a specialized analysis and veri.cation framework. This encapsulation then enables the devel\u00adopment \nof simpler but more scalable client analyses that can work at the higher level of the veri.ed properties \nrather than attempting to directly analyze the data structure implementations along with the client together \nin the same analysis framework. Decoupling data structure and client analyses is appropriate because \ndata structure implementations and clients have different analysis/veri.cation needs. With current technology, \nthe veri.ca\u00adtion of linked data structure implementations requires the use of sophisticated but unscalable \ntechniques. Such techniques are appro\u00adpriate in this context because of the tractable size of data structure \nimplementations, the abstraction boundary between data structure implementations and clients, and because \nthe cost of the result\u00ading ambitious data structure veri.cation efforts can be effectively amortized \nacross many uses of the veri.ed properties. Client anal\u00adyses, in contrast, face signi.cant scalability \nrequirements. Working with veri.ed data structure properties (instead of directly with data structure \nimplementations) can enable the development of simpler but still precise client analyses that can acceptably \nscale to analyze large client code bases. In previous research we have found that aspect-oriented tech\u00adniques \ncan help developers productively factor and modularize the desired client correctness properties, enabling \nclient analyses to work within an appropriately focused analysis scope [35]. In partic\u00adular, this approach \nsupports the expression and veri.cation of client correctness properties that involve interactions among \nmultiple data structures and invariants over them. Other examples of projects that are designed to exploit \nor enable decoupled data structure im\u00adplementation and client analyses include the Hob project [33, 35 \n37, 50], the Jahob project [6, 51, 52], research on ef.cient imple\u00admentations of multiple relations over \nobjects [23], and research on verifying properties of programs that use containers [13]. Typestate analyses \nare designed to scalably verify simpler properties involv\u00ading abstract object state changes [3, 11, 15, \n16, 18, 32, 37, 47]. We recently became aware of a project that reduces the veri.ca\u00adtion of commutativity \nconditions and inverses to solving automat\u00adically generated SAT problems [40]. The speci.cation language \nis an abstract imperative language (as opposed to logic speci.cations as in our research). To enable \nthe reduction to SAT, the speci.ca\u00adtion language does not include loops and recursion. It instead adds \nadditional constructs to provide an acceptably expressive speci.ca\u00adtion language. We have already surveyed \nrelated work in detecting and exploit\u00ading commuting operations (see Section 1). To use the commutativ\u00adity \nconditions in practice, systems must typically deploy some syn\u00adchronization mechanism to ensure that \noperations execute atomi\u00adcally. The speci.c synchronization mechanisms are orthogonal to the issues we \naddress in this paper (our goal is to verify the cor\u00adrectness of commutativity conditions and inverses, \nnot to design mechanisms that ensure that operations execute atomically). One synchronization approach \nis to simply use standard pes\u00adsimistic (such as mutual exclusion locks [2, 4, 21] or their extension \nto views of objects [12]) or optimistic (such as optimistic locks [21] or software transactional memory \n[46]) concurrency control mech\u00adanisms. It is also possible to deploy customized nonblocking syn\u00adchronization \nalgorithms that are tailored for the speci.c data struc\u00adture at hand [25, 39]. It is often possible to \nexploit the structure of the commutativ\u00adity conditions to develop optimized synchronization mechanisms. \nFor example, abstract locks, forward gatekeepers, and general gatekeepers are successively more general \nsynchronization mecha\u00adnisms, each of which is appropriate for a successively larger class of commutativity \nconditions [30]. Our sound and complete commuta\u00adtivity conditions typically take the form of a disjunction \nof clauses. Dropping clauses produces sound, simpler, but in general incom\u00adplete commutativity conditions. \nSimpler commutativity conditions are typically more ef.cient to check but expose less concurrency. It \nis possible to start with a sound and complete commutativity con\u00addition and generate a lattice of sound \ncommutativity conditions by dropping clauses (here the least upper bound is disjunction [30]). Which \ncommutativity condition is most appropriate for a given context depends on the interaction between the \ncommutativity con\u00addition checking overhead and the amount of concurrency that the commutativity condition \nexposes in that context. Commuting operations can also be used to simplify correctness proofs of parallel \nprograms [14]. The basic idea is to use commu\u00adtativity information to enable reduction obtaining larger-grained \natomic blocks by showing that .ner-grain statements adjacent in one thread commute with statements in \nother threads. The speci.c method uses a form of computation abstraction (replacing state\u00adments with \nstatements that have more behaviors) to enhance their ability to obtain statements that commute with \nother statements. While this may increase the possible behaviors of the program, the idea is to prove \nassertions at the end of the program. If these asser\u00adtions are valid under the extended set of behaviors \nof the abstracted program, they are also valid for the original program. The research presented in this \npaper uses a different form of ab\u00adstraction (data abstraction as opposed to computation abstraction) \nfor a different purpose (reasoning about the semantic equivalence of commuting and inverse operations \non linked data structures). Our results may, however, enhance the effectiveness of techniques that reason \nabout explicitly parallel programs they provide such reasoning techniques with useful commutativity \nand inverse infor\u00admation about operations that manipulate linked data structures. Bridge predicates enable \ndevelopers to specify equivalent states in parallel programs with operations that are intended to execute \natomically but whose execution is, in practice, interleaved [8]. These predicates can then be used to \nrecognize and discard false positives in the interleaved execution. In the absence of bridge pred\u00adicates, \nsuch false positives occur when the interleaved execution produces a concrete state that is unrealizable \nin the atomic exe\u00adcution but nevertheless semantically equivalent to some state that an atomic execution \nproduces. Bridge predicates enable the testing system to recognize the state equivalence and therefore \nthe accept\u00adability of the interleaved execution. 7. Conclusion Commuting operations, commutativity conditions, \nand inverse op\u00aderations play an important role in a broad range of current and envi\u00adsioned static reasoning \nsystems and parallel programs, languages, and systems. We have presented new techniques for verifying \nse\u00admantic commutativity conditions and inverse operations for linked data structures. Our results show \nthat these techniques can effec\u00adtively verify inverse operations and sound and complete commuta\u00adtivity \nconditions for a collection of challenging linked data struc\u00adtures. Our results therefore provide a useful \nfoundation that others can build on as they develop static reasoning systems and parallel programs, languages, \nand systems. In the longer term we envision the integration of the commu\u00adtativity condition and inverse \nveri.cation techniques presented in this paper into mature software development kits. Deploying these \ntechniques in this way would promote their wider use by developers within a familiar environment as well \nas their productive integration with other software development tools. Acknowledgments We would like \nto thank Karen Zee for explaining various aspects of Jahob in detail. We also acknowledge an earlier \ntechnical report version of this paper [26]. This work was supported in part by the National Science \nFoun\u00addation (Grants CCF-0811397, CCF-0905244, CCF-1036241 and IIS-0835652), the United States Department \nof Energy (Grant DE-SC0005288), and the Engineering Research Center of Excellence Program of Korea Ministry \nof Education, Science and Technology / National Research Foundation of Korea (Grant 2010-0001717). References \n[1] F. Aleen and N. Clark. Commutativity analysis for software paral\u00adlelization: Letting program transformations \nsee the big picture. In Proc. of the International Conference on Architectural Support for Programming \nLanguages and Operating Systems (ASPLOS), 2009. [2] T. E. Anderson. The performance of spin lock alternatives \nfor shared\u00admemory multiprocessors. IEEE Trans. Parall. Distrib. Syst., 1(1), 1990. [3] H. Attiya, R. \nGuerraoui, D. Hendler, P. Kuznetsov, M. M. Michael, and M. Vechev. Laws of order: Expensive synchronization \nin con\u00adcurrent algorithms cannot be eliminated. In Proc. of the ACM SIGACT-SIGPLAN Symposium on Principles \nof Programming Lan\u00adguages (POPL), 2011. [4] D. F. Bacon, R. Konuru, C. Murthy, and M. Serrano. Thin locks: \nFeath\u00aderweight synchronization for Java. In Proc. of the ACM SIGPLAN Conference on Programming Language \nDesign and Implementation (PLDI), 1998. [5] R. L. Bocchino Jr., V. S. Adve, D. Dig, S. V. Adve, S. Heumann, \nR. Komuravelli, J. Overbey, P. Simmons, H. Sung, and M. Vakilian. A type and effect system for Deterministic \nParallel Java. In Proc. of the ACM SIGPLAN Conference on Object-Oriented Programming, Systems, Languages \nand Applications (OOPSLA), 2009. [6] C. Bouillaguet, V. Kuncak, T. Wies, K. Zee, and M. C. Rinard. Using \n.rst-order theorem provers in the Jahob data structure veri.cation system. In Proc. of the International \nConference on Veri.cation, Model Checking, and Abstract Interpretation (VMCAI), 2007. [7] M. J. Bridges, \nN. Vachharajani, Y. Zhang, T. B. Jablin, and D. I. Au\u00adgust. Revisiting the sequential programming model \nfor the multicore era. IEEE Micro, 28(1), 2008. [8] J. Burnim, G. Necula, and K. Sen. Specifying and \nchecking semantic atomicity for multithreaded programs. In Proc. of the International Conference on Architectural \nSupport for Programming Languages and Operating Systems (ASPLOS), 2011. [9] T. H. Cormen, C. E. Leiserson, \nand R. L. Rivest. Introduction to Algorithms. The MIT Press and McGraw-Hill Book Company, 1989.  [10] \nL. de Moura and N. Bj\u00f8rner. Ef.cient E-matching for SMT solvers. In Proc. of the International Conference \non Automated Deduction (CADE), 2007. [11] R. DeLine and M. F\u00a8ahndrich. Enforcing high-level protocols \nin low\u00adlevel software. In Proc. of the ACM SIGPLAN Conference on Pro\u00adgramming Language Design and Implementation \n(PLDI), 2001. [12] B. Demsky and P. Lam. Views: Object-inspired concurrency control. In Proc. of the \nACM/IEEE International Conference on Software En\u00adgineering (ICSE), 2010. [13] I. Dillig, T. Dillig, and \nA. Aiken. Precise reasoning for programs using containers. In Proc. of the ACM SIGACT-SIGPLAN Symposium \non Principles of Programming Languages (POPL), 2011. [14] T. Elmas, S. Qadeer, and S. Tasiran. A calculus \nof atomic actions. In Proc. of the ACM SIGACT-SIGPLAN Symposium on Principles of Programming Languages \n(POPL), 2009. [15] M. F\u00a8ahndrich and R. DeLine. Adoption and focus: Practical linear types for imperative \nprogramming. In Proc. of the ACM SIGPLAN Conference on Programming Language Design and Implementation \n(PLDI), 2002. [16] M. F\u00a8ahndrich and K. R. M. Leino. Heap monotonic typestates. In Proc. of the International \nWorkshop on Aliasing, Con.nement and Ownership in Object-Oriented Programming (IWACO), 2003. [17] A. \nFekete, N. A. Lynch, M. Merritt, and W. E. Weihl. Commutativity\u00adbased locking for nested transactions. \nIn Proc. of the International Workshop on Persistent Object Systems (POS), 1989. [18] J. Field, D. Goyal, \nG. Ramalingam, and E. Yahav. Typestate veri.ca\u00adtion: Abstraction techniques and complexity results. In \nProc. of the International Static Analysis Symposium (SAS), 2003. [19] Y. Ge, C. Barrett, and C. Tinelli. \nSolving quanti.ed veri.cation condi\u00adtions using satis.ability modulo theories. In Proc. of the International \nConference on Automated Deduction (CADE), 2007. [20] A. Goel, K. Po, K. Farhadi, Z. Li, and E. de Lara. \nThe Taser intrusion recovery system. In Proc. of the ACM Symposium on Operating Systems Principles (SOSP), \n2005. [21] J. Gray and A. Reuter. Transaction Processing: Concepts and Tech\u00adniques. Morgan Kaufmann, \n1993. [22] S. Gulwani and G. C. Necula. Precise interprocedural analysis using random interpretation. \nIn Proc. of the ACM SIGACT-SIGPLAN Sym\u00adposium on Principles of Programming Languages (POPL), 2005. [23] \nP. Hawkins, A. Aiken, K. Fisher, M. Rinard, and M. Sagiv. Data representation synthesis. In Proc. of \nthe ACM SIGPLAN conference on Programming Language Design and Implementation (PLDI), 2011. [24] J. G. \nHenriksen, J. Jensen, M. J\u00f8rgensen, N. Klarlund, R. Paige, T. Rauhe, and A. Sandholm. Mona: Monadic second-order \nlogic in practice. In Proc. of the International Conference on Tools and Algorithms for the Construction \nand Analysis of Systems (TACAS), 1995. [25] M. Herlihy, V. Luchangco, and M. Moir. Obstruction-free synchro\u00adnization: \nDouble-ended queue as an example. In Proc. of the Interna\u00adtional Conference on Distributed Computing \nSystems (ICDCS), 2003. [26] D. Kim and M. C. Rinard. Veri.cation of semantic commutativity conditions \nand inverse operations on linked data structures. Technical Report MIT-CSAIL-TR-2010-056, Computer Science \nand Arti.cial Intelligence Laboratory, Massachusetts Institute of Technology, Dec. 2010. [27] T. Kim, \nX. Wang, N. Zeldovich, and M. F. Kaashoek. Intrusion recovery using selective re-execution. In Proc. \nof the Symposium on Operating Systems Design and Implementation (OSDI), 2010. [28] M. Kulkarni, K. Pingali, \nB. Walter, G. Ramanarayanan, K. Bala, and L. P. Chew. Optimistic parallelism requires abstractions. In \nProc. of the ACM SIGPLAN conference on Programming Language Design and Implementation (PLDI), 2007. [29] \nM. Kulkarni, D. Prountzos, D. Nguyen, and K. Pingali. De.ning and implementing commutativity conditions \nfor parallel execution. Technical Report TR-ECE-09-11, School of Electrical and Computer Engineering, \nPurdue University, Aug. 2009. [30] M. Kulkarni, D. Nguyen, D. Prountzos, X. Sui, and K. Pingali. Ex\u00adploiting \nthe commutativity lattice. In Proc. of the ACM SIGPLAN Conference on Programming Language Design and \nImplementation (PLDI), 2011. [31] V. Kuncak and M. Rinard. Towards ef.cient satis.ability checking for \nBoolean algebra with Presburger arithmetic. In Proc. of the Interna\u00adtional Conference on Automated Deduction \n(CADE), 2007. [32] V. Kuncak, P. Lam, and M. Rinard. Role analysis. In Proc. of the ACM SIGACT-SIGPLAN \nSymposium on Principles of Programming Languages (POPL), 2002. [33] V. Kuncak, P. Lam, K. Zee, and M. \nC. Rinard. Modular pluggable analyses for data structure consistency. IEEE Trans. Softw. Eng., 32 (12), \n2006. [34] V. Kuncak, H. H. Nguyen, and M. Rinard. Deciding Boolean algebra with Presburger arithmetic. \nJournal of Automated Reasoning, 36(3), 2006. [35] P. Lam, V. Kuncak, and M. Rinard. Crosscutting techniques \nin pro\u00adgram speci.cation and analysis. In Proc. of the International Confer\u00adence on Aspect-Oriented Software \nDevelopment (AOSD), 2005. [36] P. Lam, V. Kuncak, and M. Rinard. Hob: A tool for verifying data structure \nconsistency. In Proc. of the International Conference on Compiler Construction (CC), 2005. [37] P. Lam, \nV. Kuncak, and M. Rinard. Generalized typestate checking for data structure consistency. In Proc. of \nthe International Conference on Veri.cation, Model Checking, and Abstract Interpretation (VMCAI), 2005. \n[38] P. Mahajan, R. Kotla, C. C. Marshall, V. Ramasubramanian, T. L. Rodeheffer, D. B. Terry, and T. \nWobber. Effective and ef.cient com\u00adpromise recovery for weakly consistent replication. In Proc. of the \nACM European Conference on Computer Systems (EuroSys), 2009. [39] M. M. Michael and M. L. Scott. Simple, \nfast, and practical non\u00adblocking and blocking concurrent queue algorithms. In Proc. of the ACM Symposium \non Principles of Distributed Computing (PODC), 1996. [40] E. Moss. Personal communication, 2011. [41] \nM. C. Rinard and P. C. Diniz. Commutativity analysis: a new analysis technique for parallelizing compilers. \nACM Trans. Prog. Lang. Syst., 19(6), 1997. [42] M. C. Rinard and M. S. Lam. The design, implementation, \nand evaluation of Jade. ACM Trans. Prog. Lang. Syst., 20(3), 1998. [43] S. Sagiv, T. W. Reps, and R. \nWilhelm. Parametric shape analysis via 3-valued logic. ACM Trans. Prog. Lang. Syst., 24(3), 2002. [44] \nS. Schulz. E a brainiac theorem prover. AI Commun., 15(2 3), 2002. [45] F. Sha.que, K. Po, and A. Goel. \nCorrelating multi-session attacks via replay. In Proc. of the Workshop on Hot Topics in System Dependabil\u00adity \n(HotDep), 2006. [46] N. Shavit and D. Touitou. Software transactional memory. Distributed Computing, \n10(2), 1997. [47] R. E. Strom and S. Yemini. Typestate: A programming language concept for enhancing \nsoftware reliability. IEEE Trans. Softw. Eng., 12(1), 1986. [48] C. Weidenbach. Combining superposition, \nsorts and splitting. In A. Robinson and A. Voronkov, editors, Handbook of Automated Rea\u00adsoning, volume \n2, chapter 27, pages 1965 2013. The MIT Press, 2001. [49] W. E. Weihl. Commutativity-based concurrency \ncontrol for abstract data types. IEEE Trans. Comput., 37(12), 1988. [50] T. Wies, V. Kuncak, P. Lam, \nA. Podelski, and M. Rinard. Field con\u00adstraint analysis. In Proc. of the International Conference on Veri.ca\u00adtion, \nModel Checking, and Abstract Interpretation (VMCAI), 2006. [51] K. Zee, V. Kuncak, and M. C. Rinard. \nFull functional veri.cation of linked data structures. In Proc. of the ACM SIGPLAN Conference on Programming \nLanguage Design and Implementation (PLDI), 2008. [52] K. Zee, V. Kuncak, and M. C. Rinard. An integrated \nproof language for imperative programs. In Proc. of the ACM SIGPLAN Conference on Programming Language \nDesign and Implementation (PLDI), 2009.    \n\t\t\t", "proc_id": "1993498", "abstract": "<p>We present a new technique for verifying <i>commutativity conditions</i>, which are logical formulas that characterize when operations commute. Because our technique reasons with the abstract state of verified linked data structure implementations, it can verify commuting operations that produce semantically equivalent (but not necessarily identical) data structure states in different execution orders. We have used this technique to verify sound and complete commutativity conditions for all pairs of operations on a collection of linked data structure implementations, including data structures that export a set interface (ListSet and HashSet) as well as data structures that export a map interface (AssociationList, HashTable, and ArrayList). This effort involved the specification and verification of 765 commutativity conditions.</p> <p>Many speculative parallel systems need to undo the effects of speculatively executed operations. <i>Inverse operations</i>, which undo these effects, are often more efficient than alternate approaches (such as saving and restoring data structure state). We present a new technique for verifying such inverse operations. We have specified and verified, for all of our linked data structure implementations, an inverse operation for every operation that changes the data structure state.</p> <p>Together, the commutativity conditions and inverse operations provide a key resource that language designers, developers of program analysis systems, and implementors of software systems can draw on to build languages, program analyses, and systems with strong correctness guarantees.</p>", "authors": [{"name": "Deokhwan Kim", "author_profile_id": "81485643254", "affiliation": "Massachusetts Institute of Technology, Cambridge, MA, USA", "person_id": "P2690643", "email_address": "dkim@csail.mit.edu", "orcid_id": ""}, {"name": "Martin C. Rinard", "author_profile_id": "81100087275", "affiliation": "Massachusetts Institute of Technology, Cambridge, MA, USA", "person_id": "P2690644", "email_address": "rinard@csail.mit.edu", "orcid_id": ""}], "doi_number": "10.1145/1993498.1993561", "year": "2011", "article_id": "1993561", "conference": "PLDI", "title": "Verification of semantic commutativity conditions and inverse operations on linked data structures", "url": "http://dl.acm.org/citation.cfm?id=1993561"}