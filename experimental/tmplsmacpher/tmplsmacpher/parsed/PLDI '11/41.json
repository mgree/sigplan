{"article_publication_date": "06-04-2011", "fulltext": "\n ALTER: Exploiting Breakable Dependences for Parallelization Abhishek Udupa Kaushik Rajan William Thies \nUniversity of Pennsylvania Microsoft Research India Microsoft Research India audupa@seas.upenn.edu krajan@microsoft.com \nthies@microsoft.com Abstract For decades, compilers have relied on dependence analysis to deter\u00admine \nthe legality of their transformations. While this conservative approach has enabled many robust optimizations, \nwhen it comes to parallelization there are many opportunities that can only be ex\u00adploited by changing \nor re-ordering the dependences in the program. Thispaperpresents ALTER:asystemforidentifyingandenforc\u00ading \nparallelism that violates certain dependences while preserving overall program functionality. Based on \nprogrammer annotations, ALTER exploits new parallelism in loops by reordering iterations or allowing \nstale reads. ALTER can also infer which annotations are likely to bene.t the program by using a test-driven \nframework. Our evaluation of ALTER demonstrates that it uncovers paral\u00adlelism that is beyond the reach \nof existing static and dynamic tools. Across a selection of 12 performance-intensive loops, 9 of which \nhave loop-carried dependences, ALTER obtains an average speedup of 2.0x on 4 cores. Categories and Subject \nDescriptors D.3.3 [Programming Lan\u00adguages]: Language Constructs and Features Concurrent program\u00adming \nstructures; D.3.4 [Programming Languages]: Processors Compilers, Optimization, Run-time environments \nGeneral Terms Performance, Languages, Experimentation 1. Introduction Throughout the long history of \nresearch on automatic paralleliza\u00adtion, much of the progress has been driven by the compiler s ability \nto accurately detect the dependences between different parts of the program. Such dependence analysis \nhas evolved greatly over the years, encompassing representations such as direction vectors, dis\u00adtance \nvectors, and af.ne dependences [12] as well as techniques such as array data.ow analysis [26], interprocedural \ndependence analysis [9] and constraint-based dependence analysis [32]. The precision of dependence analysis \nhas also depended on improve\u00adments in alias analysis, shape analysis, and escape analysis, which each \nhave their own rich history. Despite the tremendous progress in dependence analysis, in practice it remains \ncommonplace for an incomplete understand\u00ading of the program s dependences to prohibit seemingly simple \ntransformations, such as parallelization of DOALL loops. There are three fundamental limitations that \nprevent any dependence analysis from fully solving the problem of automatic parallelization. First, Permission \nto make digital or hard copies of all or part of this work for personal or classroom use is granted without \nfee provided that copies are not made or distributed for pro.t or commercial advantage and that copies \nbear this notice and the full citation on the .rst page. To copy otherwise, to republish, to post on \nservers or to redistribute to lists, requires prior speci.c permission and/or a fee. PLDI 11, June 4 \n8, 2011, San Jose, California, USA. Copyright c &#38;#169; 2011 ACM 978-1-4503-0663-8/11/06. . . $10.00 \n as the general case of dependence analysis is undecidable [36], any static analysis must be conservative \nin over-approximating the pro\u00adgram dependences, thereby under-approximating the opportunities for parallelization. \nA related problem is that of induction variable analysis: certain dependences can be soundly eliminated \nvia con\u00adversion to a closed-form function of the loop index, but complex induction variables (such as \niterators through a linked list) are dif\u00ad.cult to detect automatically. Second, even if dependences are \nprecisely identi.ed (e.g., us\u00ading dynamic dependence analysis [43], program annotations [45], or speculative \nparallelization [11, 15, 21, 27, 33, 39, 44]), there re\u00admain many programs in which memory dependences \nare accidental artifacts of the implementation and should not inhibit paralleliza\u00adtion. For example, \nit has been shown that in many performance\u00adintensive loops, the only serializing dependences are due \nto calls to the memory allocator, which can be freely reordered without im\u00adpacting correctness [7, 41]. \nSimilar dependences are due to benign references to uninitialized data structures, maintenance or output \nof unordered debugging information, and other patterns [41]. Only by allowing the compiler to reorder \nor ignore such dependences is it possible to extract parallelism from many programs. Third, there are \nmany cases in which broken data dependences do change the course of a program s execution, but the algorithm \nemployed is robust to the changes and arrives at an acceptable out\u00adput anyway. A simple example is the \nchain of dependences implied by successive calls to a random number generator, which has the potential \nto serialize many loops. However, in algorithms such as simulated annealing or monte-carlo simulation, \nany ordering of the calls is permissible so long as each result is random [7, 41]. A more subtle example \nis that of successive relaxation algorithms, in which the solution is iteratively improved in a monotonic \nfashion and bro\u00adken dependences (such as stale reads) may enable parallelism at the expense of a slight \nincrease in convergence time. In this paper, we make three contributions towards enabling par\u00adallelization \nof programs that are beyond the reach of dependence analysis. First, we propose a new execution model, \nStaleReads, which enables iterations of a loop to execute in parallel by allowing stale reads from a \nconsistent snapshot of the global memory. This model enforces a well-known guarantee called snapshot \nisolation that is frequently employed in the database community as a permis\u00adsive and high-performance \npolicy for transactional commit. How\u00adever, while other concepts from databases have impacted program\u00adming \nlanguages via transactional memory, we are unaware of any exploration or implementation of snapshot isolation \nas a general\u00adpurpose mechanism for loop parallelization. In addition to the basic model, we also offer \nsupport for reductions, in which commutative and associative updates to speci.c variables are merged \nupon com\u00adpletion of each iteration. We demonstrate that snapshot isolation exposes useful parallelism \nin several algorithms, in particular for convergence algorithms that are robust to stale reads (as described \nin the previous paragraph). while (CheckConvergence(AMatrix, XVector, BVector, dimA) == 0) { tripCount++; \n[StaleReads] for(inti=0;i< dimA; i++) { sum=0; // scalarProduct reads all of XVector sum = scalarProduct(AMatrix[i], \nXVector); sum -= AMatrix[i][i] * XVector[i]; // write to XVector[i] XVector[i] = (BVector[i] -sum) / \nAMatrix[i][i]; } } Figure 1. Iterative algorithm to solve a system of linear equations of the form Ax=b. \nThe for loop is annotated by ALTER to indicate that the iterations are likely to tolerate stale reads. \nParallelization using the ALTER compiler and runtime system gives a speedup of 1.70x on 4 cores (for \na sparse matrix A with 40,000 elements). Our second contribution is a general framework, ALTER, for specifying \ndependences that do not matter and leveraging that in\u00adformation in a parallel runtime system. Using ALTER, \nthe program\u00admer annotates each loop with one of two permissive execution poli\u00adcies. In addition to StaleReads,ALTER \nsupports OutOfOrder execution of the loop iterations; this corresponds to the database notion of con.ict \nserializability and may break the original depen\u00addences so long as execution is consistent with some \nserial order\u00ading of the iterations. Annotations are also used to declare reduc\u00adtion variables, as described \nabove. Given the annotations, ALTER implements the desired parallelism using a deterministic fork-join \nmodel. Each loop iteration (or chunk of iterations) is treated as a transaction, executing in an isolated \nprocess and committing with a different policy depending on the model of parallelism. The im\u00adplementation \nof ALTER relies on a novel multi-process memory allocator as well as custom collections classes that \nenable iterators over linked data structures to be recognized as induction variables. Our third contribution \nis a practical methodology that leverages ALTER to discover and exploit parallelism. As the ALTER annota\u00adtions \nmay change the local behavior of a program, it is intractable to infer or verify them using a sound static \nanalysis. However, to assist programmers in exploring the space of potential parallelizations, we propose \na test-driven framework that evaluates each possible annotation on each loop in the program and conveys \nto the pro\u00adgrammer the set of annotations that preserve the program output across a test suite. This \ndynamic analysis, while unsound, can be used to infer likely annotations that are subsequently validated \nby the programmer. The test-driven framework bene.ts greatly from ALTER s deterministic execution model, \nas each test needs to be executed only once. Using this end-to-end methodology, we ap\u00adply ALTER to assist \nwith parallelization of a set of loops, drawn from Berkeley s parallel dwarfs as well as the STAMP suite. \nAL-TER .nds parallelismthat isundiscoverable by prior techniques and accelerates performance-intensive \nloops by an average of 2.0x on four cores. The rest of this paper is organized as follows. We start with \na motivating example for the violation of program dependences dur\u00ading loop parallelization (Section 2). \nWe proceed to describe the ALTER annotation language (Section 3), the implementation of the ALTER compiler \nand runtime system (Section 4), the annotation inference algorithm (Section 5) and the usage scenarios \nin which ALTER could be applied (Section 6). We then present our experi\u00admental evaluation (Section 7) \nbefore wrapping up with related work (Section 8) and conclusions (Section 9). // convergence check simplified \nfor presentation while (delta > threshold) { delta = 0.0; [OutOfOrder + Reduction(delta, +)] or [StaleReads \n+ Reduction(delta, +)] for (i = 0; i < npoints; i++) { index = common_findNearestPoint(feature[i], nfeatures, \nclusters, nclusters); // If membership changes, increase delta // by 1. membership[i] cannot be // changed \nin other iterations if (membership[i] != index) { delta += 1.0; } // Assign the membership to object \ni membership[i] = index; // Update new cluster centers : // sum of objects located within *new_centers_len[index] \n= *new_centers_len[index]) + 1; for (j = 0; j < nfeatures; j++) { new_centers[index][j] = new_centers[index][j] \n+ feature[i][j]; } } } Figure 2. K-means clustering algorithm from STAMP. ALTER suggests the OutOfOrder \nand StaleReads annotations along the for loop, in combination with an additive reduction on the vari\u00adable \ndelta (which is used to determine the termination of the overall algorithm). Parallelizing this algorithm \nusing StaleReads gives a speedup of 1.71x on 4 cores. 2. Motivating Examples This section illustrates \ntwo examples where breaking dependences enables parallel execution while preserving the high-level function\u00adality. \nThe .rst example (see Figure 1) is a numerical algorithm to solve a system of linear equations of the \nform Ax = b, where A is an n \u00d7 n input matrix, b is an input vector of n elements and x is the solution \nvector of n unknown elements. This algorithm often forms the kernel for solving partial differential \nequations. The algorithm has two loops, an outer while loop that checks for convergence and an inner \nfor loop that re-calculates each element of XVector based on the values of the other elements. The inner \nloop has a tight dependence chain as the XVector element written to in one iteration is read in every \nsubsequent iteration. Thus, the only possible way to parallelize this loop is to violate sequential semantics \nby not enforcing some of the true dependences. Based on the results of test cases, ALTER suggests that \nthe in\u00adner loop can be parallelized under the StaleReads model. While some of the values read from XVector \nwill be stale, the algorithm has an outer while loop which checks for convergence and prevents these \nbroken dependences from affecting the overall correctness. This alternate version of the algorithm has \nbeen shown in the lit\u00aderature to have the same convergence properties as the sequential version [31]. \nIn fact, this algorithm belongs to a class of algorithms, commonly referred to as algebraic path problems \n[40], that can tol\u00aderate some stale reads. This class includes many popular algorithms such as the Bellman \nFord shortest path algorithm, iterative mono\u00adtonic data-.ow analyses, Kleene s algorithm (for deciding \nwhether a regular language is accepted by a .nite state automaton) and sten\u00adcil computations. Note that \nit is possible that embracing stale reads can increase the number of (outer loop) iterations needed to \nconverge. For this benchmark, we observe that this increase is quite small in practice. This is expected \nas typically the size of XVector is large (tens of thousands of elements) while an iteration will read \na small number of stale values (at most (N -1)\u00d7chunkFactor values on an N-way multicore, where chunkFactor \nis the number of iterations executed per transaction as detailed later). Using ALTER, this parallelization \ngives a speedup of 1.70x with 4 cores for a problem size of 40,000. The second example (see Figure 2) \nshows the main loop of the K-means clustering algorithm. ALTER suggests that the loop can be parallelized \nwith either OutOfOrder or StaleReads in combi\u00adnation with an additive reduction on the variable delta. \nIn this case, OutOfOrder and StaleReads are equivalent with respect to the program s execution, because \nevery read of a shared variable (new centers and new centers len) is followed by a write to the same \nlocation. Thus, even under the StaleReads model there will not be any stale reads in the execution trace, \nbecause con\u00ad.icts in the write sets would cause such concurrent iterations to con.ict (this is clari.ed \nin the next section). Nonetheless, annotat\u00ading the loop with StaleReads can lead to higher performance \nthan OutOfOrder, as it is not necessary to track or compare the read sets within ALTER. 3. Alter Annotation \nLanguage ALTER provides an annotation language for specifying the paral\u00adlelism in loops (see Figure 3). \nFor all annotated loops, ALTER treats each iteration as a transaction that executes atomically and in \niso\u00adlation. The conditions under which an iteration commits its results are governed by the annotation \nA, which contains two components: a parallelism policy P and, optionally, a set of reductions R. The \nparallelism policy can be one of two types: 1. OutOfOrder speci.es that ALTER may reorder the loop itera\u00adtions, \nso long as the execution is equivalent to some serial order\u00ading of the iterations. This corresponds to \nthe database notion of con.ict serializability. OutOfOrder preserves the semantics of the original program \nif the iterations are commutative. 2. StaleReads is more permissive than OutOfOrder: in addi\u00adtion to \nreordering iterations, the values read in a given iteration may be stale. All stale reads are drawn from \na consistent but possibly obsolete snapshot of the global memory. The degree of staleness is bounded: \nthe memory state seen by iteration i, which writes to locations W , is no older than the state commit\u00adted \nby the last iteration to write to any location in W . (The only exception is that of reduction variables, \nwhich do not count as part of W .)  This model corresponds to the database notion of snapshot iso\u00adlation; \ntwo concurrent iterations can commit so long as their write sets do not overlap. Snapshot isolation is \na popular al\u00adternative to con.ict serializability in the database community, as it leads to better performance. \nIt is adopted by several ma\u00adjor database management systems, including InterBase, Oracle, PostgreSQL \nand Microsoft SQL Server (2005 onward). How\u00adever, to date there has been no programming language or run\u00adtime \nsupport for snapshot isolation as a mechanism for loop parallelization. The second part of an ALTER annotation \nis an optional set of reductions. Each reduction is speci.ed by the name of a program variable, as well \nas an operation that is commutative and associa\u00adtive. The annotation asserts that every access to the \nvariable within the loop represents an update using the corresponding operation. For example, if the \noperation is +, then the variable may appear only in statements of the form var+=value. Note that this \nalso pro\u00adhibits any reads of var elsewhere in the loop. ALTER guarantees that upon completion of the \nloop, each re\u00adduction variable has the value corresponding to a serial execution A := (P, R) P := OutOfOrder \n| StaleReads R := E | R; R | (var, O) O := + |\u00d7| max | min |.|. Figure 3. ALTER annotation language \n of the operations that updated it. Note that under the OutOfOrder execution model, annotating reductions \nwill not impact the .nal values assigned to reduction variables, as OutOfOrder guaran\u00adtees that all variables \n(not just reduction variables) are updated in a manner that is consistent with a serial execution. However, \nanno\u00adtating reductions can nonetheless improve the performance under OutOfOrder, as iterations that were \npreviously serialized by a dependence on a reduction variable can now execute in parallel. In the case \nof StaleReads execution, annotating reduction vari\u00adables can impact the .nal values of those variables \n(but not any other variables) as well as impacting performance. In addition to the parameters speci.ed \nin the annotation lan\u00adguage, ALTER also allows the user to specify a chunk factor (cf), which dictates \nthe granularity of the execution. While our descrip\u00adtion above deals in terms of individual iterations, \nin practice it is bene.cial to group multiple iterations together for improving the ef.ciency of OutOfOrder \nand StaleReads execution. The se\u00admantics can be understood in terms of chunking: the original loop L \nis refactored into a nested loop L' in which each iteration of L' represents cf iterations of L. The \nchunk factor can be designated on a per-loop basis, or globally for the entire program. 4. Alter Compiler \nand Runtime Given a sequential program and a loop to parallelize, the ALTER compiler produces a concurrent \nprogram with embedded calls to the ALTER runtime library. The concurrent program is parameter\u00adized by \nsome additional inputs that indicate the semantics to be enforced for each loop. If the user provides \nannotations for a loop, then these are used to generate the parameters. Otherwise, likely an\u00adnotations \ncan be inferred by testing their conformance to a test suite (see Section 5). Section 4.2 describes the \nparameters governing the execution and the assignments needed to exploit the parallelism al\u00adlowed by \nthe annotations. A salient feature of the ALTER frame\u00adwork is that the output of the compilation process \nis a deterministic parallel program. Details of how this is achieved are provided in Section 4.3. 4.1 \nProgram Transformation Program transformations are implemented as phases in the Mi\u00adcrosoft Phoenix compiler. \nDeterministic Fork Join Model The ALTER compiler transforms the program to use a process\u00adbased fork-join \nmodel of computation. The model provides ef\u00ad.cient isolation between concurrently executing loop iterations \nthrough copy-on-write page protection. Figure 4 shows what the transformed program looks like. The shaded \nvertices and edges in the .gure represent the original sequential control .ow. The high-level functionality \nof the transformation is to: 1. Create N processes that have identical stack and heap states (apart from \nALTER s internal variables), corresponding to the program state on entry to the loop. These N private \nmemories are copy-on-write (COW) mappings of a committed memory state. Thus, just before the loop, there \nare N +1 versions of memory. In the .gure, 0 and @ describe the implementation of this functionality \nusing Win32 system calls.  Figure 4. ALTER s fork-join model for executing loop iterations concurrently \nand in isolation. 2. Distribute iterations to processes and orchestrate a lock-step execution where processes \nrepeatedly: @ pickup iterations to execute. 0 execute them concurrently in isolation on their own private \nmemory while tracking read and write sets (details on instrumentation below). 0 wait for all processes \nto complete execution and then depending on validation against execution policy either commit writes \nto committed memory state or mark the iteration for re-execution next time. Runtime barriers are used \nto ensure that processes commit changes one after another in deterministic order (ascending order of \nchild pid s). @ re-synchronize process private memory with the committed memory state once all processes \nhave committed. These steps are repeated until the loop execution is complete. To keep things simple, \nFigure 4 depicts the program transfor\u00admation without chunking. The actual transformation introduces an \nadditional inner loop such that each process executes a consecutive chunk of iterations between joins. \nMemory Management In addition to the above transformation, all calls to memory allo\u00adcator methods are \nreplaced with calls to the ALTER-allocator. This transformation is essential to ensure that in a multi-process \nsetting, objects can be directly copied between processes without overwrit\u00ading live values. The ALTER-allocator \nis designed to be safe and ef.cient under a multi-process execution environment. It ensures safety by \nguaranteeing that no two concurrent processes are allo\u00adcated the same virtual address. We do not describe \nthe design in detail here except to say that it uses techniques similar to ones used by HOARD [5]. Where \nHOARD is optimized to be ef.cient in a multi-threaded environment, the ALTER-allocator is designed to \nbe ef.cient in a multi-process environment. For example, the allocator is optimized to minimally use \ninter-process semaphores and mutual exclusion primitives. Such inter-process communication primitives \nare typically much more expensive than locking primitives that can be used in a multi-threaded setting. \n ALTER Collection Classes While simple induction variables can be identi.ed via static anal\u00adysis, induction \nvariables of loops that iterate over elements of a heap data structure will not be detected by most compilers. \nTo en\u00adable parallelization of such loops ALTER exposes a library of stan\u00addard data structures that are \ncommonly iterated over. When a user wants to parallelize a loop that iterates over such a data structure, \nshe could replace the data-structure with an equivalent ALTER col\u00adlection class and then use any of the \nALTER annotations. ALTER internally manages the state associated with collection classes in a process-safe \nmanner. Note that ALTER collections can also safely be used in a sequential program. Instrumenting Reads \nand Writes The read-write instrumentation performed by the compiler is fairly standard; we brie.y describe \nit here for the sake of complete\u00adness. Accesses to both heap memory locations as well as to local stack \nvariables are instrumented by the compiler by adding calls to runtime methods InstrumentRead and InstrumentWrite. \nHeap accesses are instrumented at allocation granularity. For in\u00adstance if a statement of the form ObjPtr->Field \n= Value; is encountered, then the object referenced by ObjPtr as a whole is instrumented, assuming that \nthe ObjPtr refers to an allocation. The compiler performs the following optimizations to avoid unnec\u00adessary \ninstrumentation calls:  No instrumentation is inserted for a memory access if the object to be instrumented \nalready has a dominating instrumentation.  If a local variable is de.ned before its .rst use, i.e., \nit is de.ned afresh in each iteration of the loop, then it is not instrumented, unless it is also live-out \non loop exit.  If the loop to be parallelized contains function calls, then ac\u00adcesses to local variables \nin the function are not instrumented.  If the memory access to be instrumented matches a pattern of \nan array indexed by an induction variable, i.e., a loop invariant base pointer indexed by an induction \nvariable, then we instrument the entire range once rather than instrumenting each individual array access. \n At runtime, the library stores the addresses of the instrumented blocks in a (local) hash set as well \nas a (global) array. The hash set allows quick elimination of duplicates, while the global array allows \nother processes to check for con.icts against their respective read-and write-sets.  4.2 Runtime Parameters \nAs mentioned previously the ALTER compiler translates the se\u00adquential program into a concurrent program \nwith some additional con.guration parameters. Four different parameters are exposed, combinations of \nwhich can be used to enforce various formal guar\u00adantees for the loop. 1. The ConflictPolicy con.guration \nparameter selects one among four different de.nitions of con.ict, which are applied to all memory locations \nnot corresponding to reduction variables. These four policies, FULL, WAW, RAW and NONE form a partial \norder with respect to the conditions under which they allow processes to commit. These terms have the \nnatural intuitive meaning. FULL allows a process to commit only if neither its read nor its write set \noverlaps with the write set of any of the concurrent processes that committed before it. WAW allows a \nprocess to commit only if its write set does not overlap with the write set of any of the concurrent \nprocesses committed before it. RAW allows a process to commit only if its read set does not overlap with \nthe write set of any of the concurrent processes committed before it. NONE does not check for any con.icts \nand allows all processes to commit. 2. The CommitOrderPolicy de.nes whether the iterations of the loop \nshould commit in program order (InOrder)or are allowed to commit OutOfOrder. Note that even under OutOfOrder, \niterations commit out of program order only if there are con.icts. 3. ReductionPolicy takes a set of \n(var, op) pairs and applies reduction as follows. Let Sc(x) denote the latest value of x in the committed \nmemory. Let oldSt(x) and newSt(x) denote the values of x in the private memory of transaction t, at the \nstart and end of its execution. ALTER modi.es the state de\u00adpending on the operation in consideration \nas follows. (1) if op is idempotent, that is op . (., ., max, min), then the commit\u00adted memory state \nis updated as Sc(x):= Sc(x) op newSt(x).  (2) if op =+ then the committed state is updated as Sc(x):= \nSc(x)+ newSt(x) - oldSt(x). \u00d7 is handled similarly. Our framework currently only supports these 6 operations. \nThe li\u00adbrary also has partial support for programmer-de.ned reduction operations but this is not fully \ntested and is not exposed as yet. 4. Finally, ChunkFactor takes a integer that de.nes the chunk factor \nto be used. We omit this parameter in discussions below and refer to it only when relevant. Parameters \nRespecting ALTER Annotations The following theorems assert that the ALTER annotations, which represent \nconstraints on the parallelism in loops, can be enforced via certain selections of the runtime parameters. \nTheorem 4.1. Executing a loop under ALTER with: ConflictPolicy = RAW, CommitOrderPolicy = OutOfOrder, \nReductionPolicy =R respects the annotation (OutOfOrder, R). Proof Sketch The OutOfOrder annotation speci.es \nthat loop iterations are treated as transactions that commit subject to con.ict serializability. ALTER \nstarts concurrent iterations on a consistent memory snapshot and provides isolation for iterations by \nexecuting each in its own address space. The RAW con.ict policy guarantees that a transaction t with \nlocation L in its read set will not commit if a concurrent transaction that committed before t has L \nin its write set, which is the criterion required for con.ict serializability. Reordering of iterations \nis enabled by the OutOfOrder ordering policy. Theorem 4.2. Executing a loop under ALTER with: ConflictPolicy \n= WAW, CommitOrderPolicy = OutOfOrder, ReductionPolicy =R respects the annotation (StaleReads, R). Proof \nSketch The StaleReads annotation speci.es that loop it\u00aderations are treated as transactions that commit \nsubject to snapshot isolation. ALTER starts concurrent iterations on a consistent mem\u00adory state and provides \nisolation for iterations by executing each in its own address space. The WAW con.ict policy guarantees \nthat a transaction t with location L in its write set will not commit if a concurrent transaction that \ncommitted before t has L in its write set, which is the criterion required for snapshot isolation. Parameters \nRespecting Other Semantics While our focus in this paper is to explore loop semantics that de\u00adpart from \nordinary execution models, ALTER can also be used to implement well-known models such as safe speculative \nparallelism and DOALL parallelism. This is asserted by the following theo\u00adrems: Theorem 4.3. Executing \na loop under ALTER with: ConflictPolicy = RAW, CommitOrderPolicy = InOrder, ReductionPolicy = NONE offers \nsafe speculative parallelism (equivalent to sequential seman\u00adtics). Proof Sketch The iterations commit \nInOrder while respecting all RAW dependences, which implies that none of the dependences in the original \nprogram are broken. Theorem 4.4. Executing a loop under ALTER with: ConflictPolicy = *, CommitOrderPolicy \n= *, ReductionPolicy =R offers DOALL parallelism with a reduction R. Proof Sketch As DOALL parallelism \napplies when there are no dependences between iterations, the ConflictPolicy and CommitOrderPolicy are \nnot relevant. Nonetheless, reductions can be supported using the ALTER framework. While other combinations \nof the ALTER parameters also lead to sensible execution models, we did not .nd an analogue for them in \npractice or application for them in example programs. We leave potential investigation of these models \nfor future work.  4.3 Determinism A salient feature of ALTER is that given a deterministic se\u00adquential \nprogram as input ALTER produces a deterministic par\u00adallel program. That is, every time the generated \nexecutable is run with the same program input and the same values for number of processes N, the chunk \nfactor cf and con.gura\u00adtion parameters (ConflictPolicy, CommitOrderPolicy, ReductionPolicy) it produces \nthe same output so long as the program terminates. If the program crashes or does not terminate this \nalso happens deterministically. The runtime avoids races by enforcing runtime barriers. Determinism follows \nfrom the following facts: (1) all concur\u00adrent processes have independent memory spaces, (2) no process \nis allowed to execute join() until all processes have completed their work, (3) processes commit their \nchanges to the committed memory state one after another in deterministic order, (4) updates from the \ncommitted memory state to private memory spaces occur only after all processes have committed, and (5) \nthe same con.icts are detected for every execution with the same inputs. Determinism is a desirable property \nfor a framework like ALTER that uses a dy\u00adnamic annotation inference engine with test suite conformance \nas the validation criterion. Determinism is desirable because the cor\u00adrectness of each test can be determined \nin a single execution. 5. Annotation Inference In addition to implementing parallelism consistent with \nthe pro\u00adgrammer s annotations, ALTER can also be used to infer a set of annotations that are likely valid \nfor a given loop. Because this in\u00adference algorithm is unsound, it is important to use it carefully. \nWe discuss speci.c application scenarios in the next section. The annotation inference works via a simple \ntest-driven frame\u00adwork. Given a program, ALTER creates many different versions, each containing a single \nannotation on a single loop. Together, these versions exhaustively enumerate the ways in which one could \nadd a single annotation to the program (details below). Then, ALTER runs each of these programs on every \ninput in the test suite. Be\u00adcause the ALTER runtime is deterministic, only one execution of each test \nis needed. Those versions matching the output of the un\u00admodi.ed sequential version (according to a program-speci.c \ncor\u00adrectness criterion) are presented to the user as annotations that are likely valid. To enumerate \nthe candidate annotations, our current implemen\u00adtation works as follows. ALTER systematically explores \nvalues for P and R (see Figure 3) by .xing the chunk factor at 16 and evalu\u00adating all valid candidates \nfor the other runtime parameters. For each reduction R, a bounded search over reduction variables and \noper\u00adators is performed. The search is restricted to (i) apply one of six reduction operators (+, \u00d7, \nmax, min, ., .), and (ii) apply the same ttime (ssecs) 14 12 10 10 8 6  16k-512 4 16k-1024 22 0 1 \n2 4 816 chunk factor Figure 5. Performance impact of the chunk factor, for various in\u00adput sizes (numbers \nof points and clusters) on the K-means bench\u00admark. reduction operator (or lack thereof) to all stack \nvariables1. Also a search for a valid reduction is performed only if none of the anno\u00adtations of the \nform (P, E) are valid. After ascertaining valid annotations, an iterative doubling algo\u00adrithm is used \nto the .nd an appropriate chunk factor. Starting from a candidate value of 1 the chunk factor is iteratively \ndoubled until a performance degradation is seen over two successive increments. The candidate that led \nto the best performance is then chosen as the chunk factor. Our results indicate that the chunk factor \nis de\u00adpendent only on the loop structure and not on the inputs. Figure 5 shows how performance varies \nwith increasing chunk factors for different inputs of K-means. As can be seen the best performing chunk \nfactor for all 4 inputs remains the same. Similar behavior is observed for other benchmarks. In addition \nto reporting the set of annotations that lead to valid executions, ALTER also gives hints as to the cause \nof failure for an\u00adnotations that are unsuccessful. For each annotation, the reported outcome is one of \nthe following: success, failure . (crash, time\u00adout, high con.icts, output mismatch). Success represents \nthe case where the execution produces a valid output. Failure cases can be classi.ed into cases where \nno output is produced (crash, timeout) and cases where an output is produced but it is either incorrect \n(output mismatch) or is unlikely to lead to performance improve\u00adments (high con.icts). A timeout is .agged \nif the execution takes more than 10 times the sequential execution time. An execution is .agged as having \nhigh con.icts if more than 50% of the attempted commits fail. Both of these behaviors are correlated \nwith perfor\u00admance degradation and hence we deem them as failures. 6. Usage Scenarios To explore concrete \napplications of ALTER in real-world contexts, we consider three scenarios manual parallelization, assisted \npar\u00adallelization, and automatic parallelization. While our primary target is assisted parallelization, \nunder certain circumstances ALTER can also make sense for purely manual or automatic parallelization. \nWe summarize these scenarios in Table 1 and expand on them below. Assisted parallelization In this scenario, \na user is attempting to parallelize a code base for which she has partial but incomplete knowledge. For \nexample, perhaps the user has authored part of the system, but relies on third-party libraries as well. \nIn such a situa\u00adtion, ALTER serves a natural role in assisting the user to parallelize 1 This is suf.cient \nfor our current benchmarks. It should be fairly straight\u00adforward to extend the search strategy to explore \ndifferent reductions for different variables. Because reduction variables are not allowed to interact, \neither with each other or with the program variables, each variable can be tested independently without \nleading to a combinatorial explosion. Manual Parallelization Assisted Parallelization Automatic Parallelization \nUser has: deep understanding of code some familiarity with code little or no familiarity with code Annotations \nare: written by hand inferred but checked by hand inferred and used as-is ALTER serves as: 1) high-level \nparallelism library 2) tool for rapid prototyping 1) set of hints to investigate further 2) upper bound \nfor discoverable parallelism 1) temporary substitute for human parallelizer 2) unsound parallelizer for \ntolerant environments Table 1. Usage scenarios for ALTER. While ALTER is intended primarily for assisted \nparallelization, it could also .nd application in purely manual or automatic parallelization. the code. \nBy evaluating various loop annotations via executions on test suites, ALTER can suggest which models \nof parallelism are likely to be valid and bene.cial for each loop in the program. The user can investigate \nthese suggestions to determine whether or not they are sound before integrating them into source base. \nIn addi\u00adtion, ALTER s suggestions could serve as a useful upper bound for certain kinds of parallelism \nin the program. If ALTER does not .nd likely parallelism in any loop, then perhaps a new algorithm will \nbe needed to leverage multicore hardware. Manual parallelization This scenario applies to cases in which \nthe user of ALTER is authoring a new parallel program or is already equipped with a deep understanding \nof an existing code base. In this case, the user can think of ALTER as providing an API for ex\u00adploiting \nknown parallelism in the program. All of the annotations are written by the user herself. The ALTER runtime \nsystem could be shipped as part of a product, or applied only for internal prototyping in order to quickly \nexplore the speedups offered by various trans\u00adformations. In the latter case, the most bene.cial transformations \nmay be re-written manually (without the ALTER runtime system) to further customize and optimize the performance. \nAutomatic parallelization In the .nal scenario (which remains largely hypothetical), ALTER is applied \nas an autonomous paral\u00adlelization engine. Perhaps this could make sense for obscure legacy codes, in \nwhich it is unduly expensive for new developers to fully understand the intricacies of the implementation. \nALTER could be used to infer parallel annotations by evaluating them for correctness across a large and \nrigorous test suite. In the case of legacy codes, test suites often provide the only arbiter as to whether \nchanges in the environment have violated implicit assumptions of the code. Just as many human intuitions \nabout very complex systems can only be veri.ed via testing, there might be cases where it is not unreasonable \nto utilize testing as the sole correctness criterion for a compiler-directed transformation. Nonetheless, \nto make sense in a production environment, ad\u00additional .exibility is likely needed to embrace ALTER as \nan auto\u00admatic parallelization engine. For example, if parallel speedups are urgently needed to proceed \nwith whole-system prototyping (e.g., to evaluate the user experience of a new gaming engine), then perhaps \nALTER could be applied temporarily until a human developer has a chance to step in and laboriously verify \nthe changes. Alternately, there may be end-user applications where speed is more important than correctness. \nFor example, one may desire a low-latency pre\u00adview of multimedia content before the of.cial decoder has \n.nished rendering it, even if the preview has a chance of being wrong. Or, on a mobile platform, if the \nbattery is low one may want to quickly execute some application (with some chance of failure) rather \nthan initiating a slower execution that is guaranteed to timeout due to a dead battery. 7. Experimental \nEvaluation We evaluate ALTER with algorithms from Berkeley s dwarfs [4] and sequential versions of the \nSTAMP [28] benchmarks (see Ta\u00adble 2). A dwarf represents an algorithmic method that captures a commonly \nused pattern of computation. We utilize all dwarfs for which we could .nd a good representative algorithm \nand all STAMP benchmarks that we could get to compile on Windows2. Table 2 describes the benchmarks \nused. Our transformations target a single loop in each benchmark. For many benchmarks the main computation \nhappens in a single loop nest; we report results for the nesting level that leads to the most parallelism. \nFor benchmarks containing multiple important loops, the targeted loop is indicated in the description \ncolumn of the table. For each benchmark, we use test inputs to infer the annotations and then use a larger \ninput (shown in bold) to measure performance. All performance measurements are conducted on an 8-core \nIntel Xeon machine (2 \u00d7 quad core at 2.4GHz) with a 64-bit Windows Server 2008 operating system. 7.1 \nResults of Annotation Inference We utilized the annotation inference algorithm to infer all anno\u00adtations \nused in our evaluation. Correctness of the program out\u00adput was evaluated on a case-by-case basis. For \n4 of the bench\u00admarks (Labyrinth, Genome, GSdense, GSsparse) assertions in the code validate the output. \nFor the remaining programs, we utilized a program-speci.c output validation script, which often made \nap\u00adproximate comparisons between .oating-point values. The results from the inference algorithm are reported \nin Ta\u00adble 3. In addition to the models exposed by our language we also check if the program can be parallelized \nby some existing auto\u00adparallelization techniques. We add a check in join() to see if the loop has any \nloop-carried dependences. The results of this check are shown in column dep. We also check to see if \nthe loop is amenable to thread level speculation (TLS). The outcome of this check could be one among \n(success, failure . (crash, timeout, high con.icts)). Interestingly, we .nd that in all cases a single \ntest is suf.\u00adcient to identify incorrect annotations. We .nd that when TLS, OutOfOrder or StaleReads \nfail, they fail either due to time\u00adouts or due to a large number of con.icts. The only exception is Ag\u00adgloClust, \nwhere the application crashes under OutOfOrder and TLS. In these two cases the machine runs out of memory \n(due to very large read sets). We .nd that an incorrect reduction leads ei\u00adther to an invalid output \nor a timeout. An interesting case is the + reduction for SG3D. The convergence check in this algorithm \nlooks for max.i(errori) < threshold so a max reduction works. A + leads to a check of the form .i(errori) \n< threshold, which also produces a valid output but convergence takes much longer. Other reductions lead \nto a deviation of \u00b10.01% from sequential output in some entries. Overall we .nd that all but one benchmark \n(Labyrinth) can be parallelized with ALTER. Both SSCA2 and Genome, which are known to be amenable to \nOutOfOrder [28], also lead to a correct execution under StaleReads. This is because all variables that \nare read in the loop are also written to. Hence it is suf.cient to check for WAW con.icts alone and no \nread instrumentation is required. Though it is known that K-means can be parallelized with OutOfOrder \nwe .nd that it leads to a very slow execution. We .nd that the only practical execution model for K-means \nis to use a combination of StaleReads and + reduction. 2 Compiling STAMP benchmarks on Windows requires \na substantial man\u00adual effort. We succeeded in compiling 4 benchmarks out of the 8 bench\u00admarks in the \nsuite; we present full results for these 4 cases. BENCHMARK DESCRIPTION LOOP WGT INPUT SIZE (inference; \nbenchmarking) Genome (STAMP) The genome sequencing algorithm from STAMP is described in detail in [28]. \nWe parallelize the .rst step (remove duplicate sequences). 89% 4M segments; 16M segments SSCA2 (STAMP) \nThis benchmark includes a set of graph kernels (details in STAMP [28]). We focus on the second loop in \nkernel 1. 76%* 18; 19; 20 (problem scale) K-means (STAMP) The K-means algorithm is a popular clustering \nalgorithm (we use the implementation from STAMP [28]). The main loop in the algorithm recomputes the \nassociation of points to clusters until convergence (see Figure 2). 89% 16K pts, 512 clusts; 16K pts, \n1024 clusts; 64K pts, 512 clusts; 64K pts, 1024 clusts Labyrinth (STAMP) uses ALTERVector This algorithm \ndoes an ef.cient routing through a mesh (details in STAMP [28]). An ALTERVector is used here. 99% 128 \n\u00d7 128 \u00d7 3 grid, 128 paths; 256 \u00d7 256 \u00d7 5 grid, 256 paths AggloClust (Branch and bound) uses ALTERList \nThe agglomerative clustering algorithm utilizes a special tree (k-d tree) to ef.ciently bound nearest \nneighbor searches in a multi-dimensional space. Our implementation is adapted (C++ instead of Java) from \nLonestar [22]. We simplify the original imple\u00admentation by not updating the bounding boxes on k-d tree \nnode deletions. We focus on the main loop of the program ([22] has the pseudo-code). As the loop iterates \nover a list we replace the sequential list with an ALTERList. 89% 100K pts; 1M pts GSdense (Dense linear \nalgebra) We use the Gauss-Seidel iterative method [31] for solving a system of equations (refer Figure \n1). Depending on whether the A matrix is sparse or dense it uses two different representations of the \nmatrix. As noted before, violating some true dependences still preserves the overall functionality and \nprovides the same convergence guarantees. 100% 10000 \u00d7 10000; 20000 \u00d7 20000 GSsparse (Sparse linear algebra) \n100% 20000 \u00d7 20000; 40000 \u00d7 40000 Floyd (Dynamic programming) The Floyd-Warshall algorithm uses a triply \nnested loop (on k, i, and j) within which it repeatedly applies the relaxation path[i][j]:= min(path[i][j],path[i][k]+ \npath[k][j]). Though the loop has a tight dependence chain, it turns out that even if some true dependences \nare violated, all possible paths between each pair of vertices are still evaluated [40]. 100% 1000 nodes; \n2000 nodes SG3D (Structured grids) The algorithm uses a 27-point three-dimensional stencil computation \nto solve a partial differential equation [13]. A triply-nested inner loop iterates over points in 3D \nspace, updating their value and tracking the maximum change (error) that occurs at any point. An outer \nloop tests for convergence by checking if the error is less than a given threshold. While the stencil \ncomputations can tolerate stale reads, the update of the error value must not violate any dependences, \nor the execution could terminate incorrectly. 96% 64 \u00d7 64 \u00d7 64; 128 \u00d7 128 \u00d7 128 BarnesHut (N-body methods) \nuses ALTERList The Barnes-Hut algorithm uses a quad-tree data structure to solve the N-body prob\u00adlem. \nWe use the implementation from Olden [10]. We parallelize the main loop that iterates over a list by \ntransforming it to use an ALTERList. 99.6% 4096 particles; 8192 particles FFT (Spectral methods) We utilize \nthe two-dimensional iterative FFT solver from [1]. 100%* 1024, 2048 HMM (Graphical models) We use the \nHidden Markov Model solver from [1]. 100% 512, 1024 * FFT has two identical loops each taking 50% of \nthe execution time. The annotations inferred and speedups obtained apply to both these loops. SSCA2 has \nan initial random input generation step. We do not include the time taken for random input generation. \n Table 2. Benchmarks used for evaluation. Eight benchmarks are drawn from Berkeley s dwarfs; the algorithm \nname is subtitled with the dwarf that it represents. Four benchmarks are drawn from STAMP. The LOOP WGT \n(loop weight) column indicates the fraction of the program s runtime that is spent in the loop targeted \nby ALTER. Benchmark Dep TLS OutOrd Stale Reduction Genome SSCA2 K-means Labyrinth AggloClust GSdense \nGSsparse Floyd SG3D BarnesHut FFT HMM Yes Yes Yes Yes Yes Yes Yes Yes Yes No No No success timeout h.c. \nh.c. crash timeout timeout timeout h.c. success success success success success h.c. h.c. crash timeout \ntimeout timeout h.c. success success success success success h.c. h.c. success success success success \nh.c. success success success N/A N/A + N/A N/A N/A N/A N/A max/+ N/A N/A N/A  Benchmark cf Transaction \nCount RW Set / Trans. Retry Rate Genome-StaleReads Genome-OutOfOrder Genome-TLS SSCA2-StaleReads SSCA2-OutOfOrder \nK-means-1024 K-means-512 AggloClust GSdense GSsparse Floyd SG3D 4096 4096 4096 64 64 4 4 64 32 32 256 \n4 32768 32768 32768 16384 16384 65552 81940 46608 2720 12580 24576 23560 16 89 90 277 6340 136 136 28 \n62 32 428 208 0.2% 0.2% 0.16% 3.5% 3.5% 3.4% 6.3% 3.6% 0 0 0 0 Table 3. Results of annotation inference. \nThe table shows whether there is a dependence carried by the loop (Dep) as well as the re-Table 4. Instrumentation \ndetails for representative benchmarks. sults for thread-level speculation (TLS), OutOfOrder execution \nThe columns show the chunk factor (cf), the number of chunks (OutOrd), and StaleReads (Stale). High con.ict \nresults are ab-(transactions) executed (Transaction Count), the average size of breviated as h.c. Reductions \nare shown where applicable. the read and write set, in words, per transaction (RW Set / Trans.), and \nthe average rate at which iterations fail to commit (Retry rate). 5 4  3 2 Speedup 1 0 2468 Processors \nProcessors Figure 6. Genome. Figure 7. SSCA2. 2.5 2 Speedup 1.5 1 0.5 0 2468 Processors Figure 8. \nK-means. The plot compares results with manual paral\u00adlelization on two different inputs. Four other benchmarks \n(GSdense, GSsparse, Floyd and SG3D) are tolerant to stale reads. Among these, SSG3D needs a combina\u00adtion \nof StaleReads and reduction for parallelization while the other three do not need reduction. Only three \nbenchmarks have no loop carried dependences and (of those with dependences) only one is amenable to ef.cient \nspeculation.   7.2 Performance Results The results for all annotations that lead to a valid execution \nduring testing are shown in Figures 6 through 13. Each .gure shows the speedup over the original sequential \nexecution (without ALTER) for the loop nest being parallelized. A maximum speedup of up to 4.5X is observed \n(ignoring triv\u00adial benchmarks with no loop carried dependences) with 8 cores. For the two benchmarks \n(Genome and SSCA2) that are amenable to multiple annotations, we .nd that StaleReads leads to much better \nperformance than OutOfOrder. This is because enforcing StaleReads does not need read instrumentation. \nWe report the size of the read and write sets per transaction as well as retry rates in Table 4. As can \nbe seen by comparing the amount of instrumen\u00adtation for OutOfOrder and StaleReads for a given bench\u00admark, \nthere is a much larger number of reads (instrumented by OutOfOrder) than writes (instrumented by both \nOutOfOrder and StaleReads) within a transaction3. Further, we .nd that for Processors Figure 9. Dense \nand sparse linear algebra. The plot compares results with manual parallelization for both dense and sparse \ncases. Genome TLS performs nearly as well as OutOfOrder but not as well as StaleReads. We .nd that the \nspeedup obtained for K-means depends on the number of clusters to be formed. The larger the number of \nclusters to be formed, the fewer the con.icts. As can be seen from Figure 8, when the number of clusters \ndecreases from 1024 to 512 the speedup comes down from 2.8X to 1.7X. So long as the probability that \ntwo points map to the same cluster is low, the algorithm should see a speedup. Further note that while \nthe speedup varies, the best parallelism policy or chunk factor does not change with input. As seen before, \nboth inference and evaluation inputs perform best at the same chunk factor. Benchmarks GSdense, GSsparse, \nand Floyd when executed un\u00adder StaleReads lead to no con.icts. This is because while these loops have \nmany RAW dependences there are no loop-carried WAW dependences. We also .nd that breaking RAW dependences \nhardly increases the number of iterations needed to converge (GS\u00addense increases from 16 to 17, while \nGSsparse increases from 20 to 21). Unfortunately, both GSdense and GSsparse are memory bound and hence \ndo not scale well beyond 4 cores. Like the above bench\u00ad 3 Though STAMP benchmarks come along with instrumentation \nhints, we ignore them and use our own static analysis to embed instrumentation in the program. Because \nthis analysis is automatic and does not depend on any special knowledge of the program, the overheads \nobserved are likely to generalize to other programs. 3 2 2.5 1.5  Floyd    Speedup 2 1.5 Speedup \n1 1 0.5 0.5 0 0 2468  Processors Processors Figure 10. Floyd Warshall algorithm. Figure 11. SG3D: 27 \npoint 3D stencil, with alternate reductions. 62   AggloClust  5 1.5 Speedup 1 0.5 0 2468  Processors \nFigure 12. Agglomerative Clustering. marks SG3D leads to no con.icts under StaleReads with reduc\u00adtion. \nAmong max and + reduction we .nd that using + degrades performance as it leads to a signi.cant increase \nin the number of iterations to converge (1670 to 2752). Among the three benchmarks that have no loop \ncarried depen\u00addences, ALTER provides reasonable speedups for HMM and bar\u00adnesHut but slows down FFT. The \nslowdown on FFT is due to high instrumentation overhead. FFT uses a complex data type, which results \nin many copy constructors that are instrumented by ALTER. This effect could be avoided by a more precise \nalias analysis or via conversion of complex types to primitive types.  7.3 Comparison with Manual Parallelization \nFinally, we manually parallelize two of the benchmarks in order to provide a realistic parallel baseline \nfor ALTER. We manually implement a multi-threaded version of Gauss-Seidel that mimics the runtime behavior \nof StaleReads by maintaining multiple copies of XVector that are synchronized in exactly the same way \nas a chunked execution under ALTER. We also parallelize K-means using threads and .ne-grained locking. \nAs shown in Figure 9, ALTER performs comparably to manual parallelization on Gauss-Seidel. However, on \nK-means, it performs 20-47% slower than manual parallelization (considering all tests between 4 and 8 \ncores) as shown in Figure 8. This slowdown is due to the overhead of the ALTER runtime system as it explores \npar\u00adallelism via optimistic, coarse-grained execution rather than pes\u00adsimistic .ne-grained locking. \n4 3 2 Speedup 1 0 Processors Figure 13. BarnesHut, FFT, and HMM. 8. Related Work Parallelizing compilers \nCompilers rely on static analysis to iden\u00adtify parts of the program that can be parallelized. Often they \ntarget simple forms of loop parallelism like DOALL and pipeline parallel loops or loops with simple reductions \n[17]. Parallelizing compilers are typically restricted by the accuracy of static analyses such as data \ndependence analysis and alias analysis [20]. Richer forms of parallelism can be identi.ed via commutativity \nanalysis [2, 38] that detects if two program tasks commute with each other or not. The analysis needs \nto be able to prove the equivalence of the .nal mem\u00adory state with either order of execution of the tasks \nbefore applying a parallelizing transformation. One of the contributions of ALTER is to provide a test-driven \nframework to identify whether iterations of a loop are likely to commute with each other. The classic \ncompiler transformation of privatization is similar to StaleReads in that it enables loop iterations \nto operate on local copies of global variables [26, 34]. However, there is a key difference between privatization \nand StaleReads: privatization does not allow loop iterations to communicate via the privatized variables. \nThe key aspect of StaleReads is that loop iterations do communicate, but the values read might be stale, \nre.ecting the update of an earlier iteration rather than the most recent one. Implicitly parallel programming \nRather than providing ex\u00adplicit directives as to the allowable parallelism, as in OpenMP or ALTER, an \nalternative approach is to utilize an implicitly par\u00adallel programming model that enables the compiler \nto automati\u00adcally identify and exploit the parallelism [19]. Of the many efforts in this space, some \nhave also proposed an annotation language to prevent spurious dependences from inhibiting parallelization. \nBridges et al. advocate using annotations for commutative func\u00adtions and non-deterministic branches to \nenable parallelization of legacy code [7]; their commutative annotation plays a similar role to our OutOfOrder \nannotation in allowing certain dependences to be reordered. In the Paralax infrastructure [45], the programmer \nannotates functions, function arguments, and variables with extra information regarding dependences; \nfor example, a kill annota\u00adtion indicates that a given variable is no longer live. The system can infer \nlikely annotations using a dynamic analysis. However, there is a key difference between this inference \nalgorithm and ours: the Paralax system aims to infer annotations that precisely describe the sequential \nexecution, but could not be identi.ed without testing. In contrast, our testing procedure identi.es opportunities \nto violate the sequential semantics while maintaining end-to-end functionality. Parallel runtime systems \nSystems such as OpenMP allow the programmer to indicate parallelism via a set of annotations, in a manner \nanalogous to ALTER. However, the parallelism offered by ALTER can not be implemented using OpenMP directives. \nWhile OpenMP supports DOALL loops and privatized variables, ALTER provides isolated and potentially stale \nreads of state that is commu\u00adnicated between loop iterations. ALTER is also very different from OpenMP \nin that (i) ALTER supports various notions of con.icts and provides runtime support to roll back and \nretry iterations, (ii) it ef.ciently and automatically handles full state replication with\u00adout requiring \nannotations for shared and private variables, and (iii) ALTER is deterministic: for a given thread count \nand chunk fac\u00adtor, ALTER always produces the same output for a given input, while parallelization with \nOpenMP may admit non-deterministic data races. STAPL [3] is a runtime library that provides parallel \nalgorithms and containers for collection classes; it is a superset of the stan\u00addard template library \n(STL) in C++. STAPL overlaps our goal of providing customized collection classes that are suitable for \nparal\u00adlelization. However, this is only a small piece of our overall system. We utilize the custom collection \nclasses to enable a new execution model and a deterministic parallel runtime for discovering hidden parallelism \nvia testing. We note that there are other parallelization frameworks that use a process-based model to \nensure isolation. These include (1) the behavior oriented parallelization framework [15] that uses process\u00adbased \nisolation for speculative parallelization, and (2) the Grace runtime system [6] that forks off threads \nas processes in an attempt to avoid common atomicity related concurrency errors. Test-driven parallelization \nClosely related to our work is the QuickStep system for parallelizing programs with statistical ac\u00adcuracy \ntests [29]. QuickStep searches for DOALL parallelism by annotating loops with OpenMP directives and evaluating \nif the resulting parallelization is acceptably accurate for the inputs in the test suite. Quickstep also \nexplores synchronization, replica\u00adtion, and caching transformations that could improve accuracy or performance, \nand provides metrics and tools to help programmers .nd an acceptable parallelization. Both systems share \nthe philos\u00adophy of violating dependences to enable new parallelism detec\u00adtion. One difference is in the \nrun-time system: while QuickStep utilizes OpenMP, we propose a new runtime system that enables the StaleReads \nexecution model and other bene.ts (see above). A second difference is that ALTER is designed to provide \ndeter\u00administic execution and freedom from data races, while QuickStep is designed to generate potentially \nnondeterministic parallel programs thatmay contain dataraces. As aconsequence, ALTER usesasingle execution \nto verify correctness on a given input, while QuickStep performs multiple test executions on the same \ninput to determine if a candidate parallelization respects the accuracy bounds with ac\u00adceptable probability. \n Pro.le-driven and speculative parallelization Pro.le driven parallelization tools augment compiler driven \ntransformations by using pro.le inputs to identify DoAll and pipeline parallelism in programs [30, 41, \n43]. Speculative parallelization [11, 15, 27, 33, 35, 44] is a related form of parallelization that can \nbe enabled by pro.le-driven tools. Execution of speculatively parallelized pro\u00adgrams typically requires \nsupport for thread level speculation either in hardware [21, 39] or in software [11, 15, 27, 35, 42]. \nTo the best of our knowledge, all existing speculative and pro.le-driven tools that exploit inter-iteration \nparallelism in loops are restricted to guarantee sequential semantics for the loop. By contrast, ALTER \nexposes and exploits alternative semantics, including the ability to permit (bounded) stale reads for \ncertain values while nonetheless preserving overall functionality. In this way, the StaleReads ex\u00adecution \nmodel is fundamentally different from speculation. While frameworks such as behavior oriented parallelism \n[15] may have similar goals at the abstract level, thus far (in their concrete form) they have utilized \nspeculation that respects program dependences. Transactional memory systems Software transactions have \nbeen proposed as an alternative to traditional lock-based program\u00adming. A transactional memory system \n[18, 25] is needed to sup\u00adport concurrent execution of software transactions. Transactional memory systems \ntry to provide con.ict serializability as the cor\u00adrectness criterion, however the exact semantics provided \nby most STMs is captured better by a correctness guarantee referred to as opacity [16]. Recently, researchers \nhave also explored snap\u00adshot isolation as a correctness criterion in STMs [14, 37]; how\u00adever, the experiments \nto date have utilized a transactional pro\u00adgramming model where the programmer thinks in terms of explicit \ntransactions. ALTER targets existing software and helps to identify whether loops can tolerate stale \nreads and reordering of iterations. Language constructs for expressing parallelism Many re\u00adsearchers \nhave investigated new programming language constructs for expressing parallelism. Most of these constructs \nintroduce richer semantics that cannot be detected by existing paralleliza\u00adtion tools. The Galois programming \nsystem introduces constructs to iterate over sets (optimistic iterators) and allows programmers to specify \nconditions under which methods commute [23, 24]. With these constructs a programmer can write loops whose \niterations can execute optimistically in parallel and commit out of order as long as there are no commutativity \nviolations. The revisions program\u00adming model [8] exposes language constructs through which a pro\u00adgrammer \ncan assign special isolation types to variables and specify merge functions for them. These merge functions \nare applied at the end of a parallel section of computation. ALTER s contribution is in determining if \nexisting sequential programs are amenable to richer forms of parallelism, without intervention on the \npart of the programmer. It also proposes the StaleReads execution model, which to our knowledge has not \nbeen used as a general approach for loop parallelization. 9. Conclusions Despite decades of research \non automatic parallelization, the re\u00adsults achievable by a compiler are rarely comparable to that of \na human. We believe that one of the primary limitations holding com\u00adpilers back is their conservative \ntreatment of memory dependences. Though humans can and do re-arrange dependences which are often artifacts \nof the implementation, or non-essential elements of the algorithm compilers, for the most part, do not. \nIn this paper, we take a .rst step towards bridging this gap by proposing ALTER: a system that violates \ncertain memory depen\u00addences in order to expose and exploit parallelism in loops. We em\u00adphasize that ALTER \nis not intended to completely replace a human; as its inferences are unsound, we still rely on human \njudgement to drivetheparallelizationprocess.However, ALTER couldgreatlyas\u00adsist the developer by providing \na new set of parallelism primitives as well as suggestions regarding their most effective use. Our evaluation \nindicates that the parallelism exploited by AL-TER is beyond the reach of existing static and dynamic \ntools. In particular, for one third of our benchmarks, the use of snapshot isolation allowing stale \nreads within loops enables parallel ex\u00adecution even when prior techniques such as speculative parallelism \nand out-of-order commit do not. Acknowledgments We are very grateful to Sriram Rajamani, R. Govindarajan, \nMah\u00admut Kandemir, and the reviewers for their thoughtful feedback. References [1] The Parallel Dwarfs \nProject. http://paralleldwarfs. codeplex.com. [2] F. Aleen and N. Clark. Commutativity Analysis for Software \nParal\u00adlelization: Letting Program Transformations See the Big Picture. In ASPLOS, 2009. [3] P. An, A. \nJula, S. Rus, S. Saunders, T. Smith, G. Tanase, N. Thomas, N. Amato, and L. Rauchwerger. STAPL: An Adaptive, \nGeneric Paral\u00adlel C++ Library. LNCS, 2624:195 210, 2003. [4] K. Asanovic, R. Bodik, J. Demmel, T. Keaveny, \nK. Keutzer, J. Kubi\u00adatowicz, N. Morgan, D. Patterson, K. Sen, J. Wawrzynek, D. Wessel, and K. Yelick. \nA view of the parallel computing landscape. CACM, 52(10), 2009. [5] E. Berger, K. McKinley, R. Blumofe, \nand P. Wilson. Hoard: A Scalable Memory Allocator for Multithreaded Applications. In ASPLOS, 2000. [6] \nE. Berger, T. Yang, T. Liu, and G. Novark. Grace: Safe Multithreaded Programming for C/C++. In OOPSLA, \n2009. [7] M. Bridges, N. Vachharajani, Y. Zhang, T. Jablin, and D. August. Re\u00advisiting the sequential \nprogramming model for multi-core. In MICRO, 2007. [8] S. Burckhardt, A. Baldassion, and D. Leijen. Concurrent \nProgram\u00adming with Revisions and Isolation Types. In OOPSLA, 2010. [9] M. Burke and R. Cytron. Interprocedural \ndependence analysis and parallelization. SIGPLAN Notices, 2004. [10] M. Carlisle. Olden: Parallelizing \nPrograms with Dynamic Data Struc\u00adtures on Distributed-Memory Machines. PhD thesis, Princeton Univer\u00adsity, \nJune 1996. [11] T. Chen, F. Min, V. Nagarajan, and R. Gupta. Copy or Discard Execution Model for Speculative \nParallelization on Multicores. In MICRO, 2008. [12] A. Darte, Y. Robert, and F. Vivien. Scheduling and \nAutomatic Paral\u00adlelization. Birkhauser Boston, 1st edition, 2000. [13] K. Datta. Auto-tuning Stencil \nCodes for Cache-Based Multicore Plat\u00adforms. PhD thesis, UC Berkeley, December 2009. [14] R. Dias, J. \nSeco, and J. Lourenco. Snapshot isolation anomalies detection in software transactional memory. In INForum, \n2010. [15] C. Ding, X. Shen, K. Kelsey, C. Tice, R. Huang, and C. Zhang. Software Behavior Oriented Parallelization. \nIn PLDI, 2007. [16] R. Guerraoui and M. Kapalka. On the Correctness of Transactional Memory. In PPoPP, \n2008. [17] M. Hall, J. Anderson, S. Amarasinghe, B. Murphy, S. Liao, E. Bugnion, and M. Lam. Maximizing \nMultiprocessor Performance with the SUIF Compiler. IEEE Computer, 29:84 89, 1996. [18] M. Herlihy and \nJ. E. B. Moss. Transactional Memory: Architectural Support for Lock-free Data Structures. In ISCA, 1993. \n[19] W. Hwu, S. Ryoo, S. Ueng, J. Kelm, I. Gelado, S. Stone, R. Kidd, S. Baghsorkhi, A. Mahesri, S. Tsao, \nN. Navarro, S. Lumetta, M. Frank, and S. Patel. Implicitly parallel programming models for thousand\u00adcore \nmicroprocessors. In DAC, 2007. [20] K. Kennedy and J. Allen. Optimizing Compilers for Modern Architec\u00adtures: \nA Dependence-Based Approach. Morgan Kaufmann Publishers Inc., 2002. [21] V. Krishnan and J. Torrellas. \nA Chip-Multiprocessor Architecture with Speculative Multithreading. IEEE Trans. on Computers, 48(9), \n1999. [22] M. Kulkarni, M. Burtscher, C. Cascaval, and K. Pingali. Lonestar: A Suite of Parallel Irregular \nPrograms. In ISPASS, 2009. [23] M. Kulkarni, K. Pingali, G. Ramanarayanan, B. Walter, K. Bala, and L. \nP. Chew. Optimistic Parallelism Bene.ts from Data Partitioning. In ASPLOS, 2008. [24] M. Kulkarni, K. \nPingali, B. Walter, G. Ramanarayanan, K. Bala, and L. P. Chew. Optimistic Parallelism Requires Abstractions. \nIn PLDI, 2007. [25] J. Larus and R. Rajwar. Transactional Memory (Synthesis Lectures on Computer Architecture). \nMorgan &#38; Claypool Publishers, 2007. [26] D. Maydan, S. Amarasinghe, and M. Lam. Array data-.ow analysis \nand its use in array privatization. In POPL, 1993. [27] M. Mehrara, J. Hao, P. Hsu, and S. Mahlke. Parallelizing \nSequential applications on Commodity Hardware using a Low-cost Software Transactional Memory. In PLDI, \n2009. [28] C. Minh, J. Chung, C. Kozyrakis, and K. Olukotun. STAMP: Stanford Transactional Applications \nfor Multi-Processing. In IISWC, 2008. [29] S. Misailovic, D. Kim, and M. Rinard. Parallelizing Sequential \nPro\u00adgrams With Statistical Accuracy Tests. Technical Report MIT-CSAIL\u00adTR-2010-038, MIT, Aug 2010. [30] \nS. Moon and M. W. Hall. Evaluation of Predicated Array Data-.ow Analysis for Automatic Parallelization. \nIn PPoPP, 1999. [31] W. Press, S. Teukolsky, W. Vetterling, and B. Flannery. Numerical Recipes in C. \nCambridge University Press, 2nd edition, 1992. [32] W. Pugh and D. Wonnacott. Constraint-based array \ndependence anal\u00adysis. ACM TOPLAS., 1998. [33] E. Raman, N. Vachharajani, R. Rangan, and D. August. SPICE: \nSpeculative Parallel Iteration Chunk Execution. In CGO, 2008. [34] L. Rauchwerger and D. Padua. The Privatizing \nDOALL Test: A Run-Time Technique for DOALL Loop Identi.cation and Array Privatiza\u00adtion. In ICS, 1994. \n[35] L. Rauchwerger and D. Padua. The LRPD Test: Speculative Run-Time Parallelization of Loops with Privatization \nand Reduction Paralleliza\u00adtion. In PLDI, 1995. [36] T. Reps. Undecidability of context-sensitive data-independence \nanal\u00adysis. ACM TOPLAS, 2000. [37] T. Riegel, C. Fetzer, and P. Felber. Snapshot isolation for software \ntransactional memory. In TRANSACT, 2006. [38] M. Rinard and P. Diniz. Commutativity Analysis: A New Analysis \nTechnique for Parallelizing Compilers. ACM TOPLAS, 19(6), 1997. [39] G. Sohi, S. Breach, and T. N. Vijaykumar. \nMultiscalar Processors. In ISCA, 1995. [40] R. Tarjan. A Uni.ed Approach to Path Problems. J. ACM, 28(3), \n1981. [41] W. Thies, V. Chandrasekhar, and S. Amarasinghe. A Practical Ap\u00adproach to Exploiting Coarse-Grained \nPipeline Parallelism in C Pro\u00adgrams. In MICRO, 2007. [42] C. Tian, M. Feng, and R. Gupta. Supporting \nSpeculative Paralleliza\u00adtion in the Presence of Dynamic Data Structures. In PLDI, 2010. [43] G. Tournavitis, \nZ. Wang, B. Franke, and M. O Boyle. Towards a Holistic Approach to Auto-parallelization: Integrating \nPro.le-driven Parallelism Detection and Machine-learning Based Mapping. PLDI, 2009. [44] N. Vachharajani, \nR. Rangan, E. Raman, M. Bridges, G. Ottoni, and D. August. Speculative Decoupled Software Pipelining. \nIn PACT, 2007. [45] H. Vandierendonck, S. Rul, and K. Bosschere. The Paralax infrastruc\u00adture: automatic \nparallelization with a helping hand. In PACT, 2010.   \n\t\t\t", "proc_id": "1993498", "abstract": "<p>For decades, compilers have relied on dependence analysis to determine the legality of their transformations. While this conservative approach has enabled many robust optimizations, when it comes to parallelization there are many opportunities that can only be exploited by changing or re-ordering the dependences in the program.</p> <p>This paper presents Alter: a system for identifying and enforcing parallelism that violates certain dependences while preserving overall program functionality. Based on programmer annotations, Alter exploits new parallelism in loops by reordering iterations or allowing stale reads. Alter can also infer which annotations are likely to benefit the program by using a test-driven framework.</p> <p>Our evaluation of Alter demonstrates that it uncovers parallelism that is beyond the reach of existing static and dynamic tools. Across a selection of 12 performance-intensive loops, 9 of which have loop-carried dependences, Alter obtains an average speedup of 2.0x on 4 cores.</p>", "authors": [{"name": "Abhishek Udupa", "author_profile_id": "81435608003", "affiliation": "University of Pennsylvania, Philadelphia, PA, USA", "person_id": "P2690627", "email_address": "audupa@seas.upenn.edu", "orcid_id": ""}, {"name": "Kaushik Rajan", "author_profile_id": "81100661664", "affiliation": "Microsoft Research India, Bangalore, India", "person_id": "P2690628", "email_address": "krajan@microsoft.com", "orcid_id": ""}, {"name": "William Thies", "author_profile_id": "81100276340", "affiliation": "Microsoft Research India, Bangalore, India", "person_id": "P2690629", "email_address": "thies@microsoft.com", "orcid_id": ""}], "doi_number": "10.1145/1993498.1993555", "year": "2011", "article_id": "1993555", "conference": "PLDI", "title": "ALTER: exploiting breakable dependences for parallelization", "url": "http://dl.acm.org/citation.cfm?id=1993555"}