{"article_publication_date": "06-04-2011", "fulltext": "\n Evaluating Value-Graph Translation Validation for LLVM Jean-Baptiste Tristan Paul Govereau Greg Morrisett \nHarvard University {tristan,govereau,greg}@seas.harvard.edu Abstract Translation validators are static \nanalyzers that attempt to verify that program transformations preserve semantics. Normalizing trans\u00adlation \nvalidators do so by trying to match the value-graphs of an original function and its transformed counterpart. \nIn this paper, we present the design of such a validator for LLVM s intra-procedural optimizations, a \ndesign that does not require any instrumentation of the optimizer, nor any rewriting of the source code \nto compile, and needs to run only once to validate a pipeline of optimizations. We present the results \nof our preliminary experiments on a set of bench\u00admarks that include GCC, a perl interpreter, SQLite3, \nand other C programs. Categories and Subject Descriptors F.3.2 [Logics and Mean\u00adings of Programs]: Semantics \nof Programming Languages -Al\u00adgebraic approaches to semantics; F.3.1 [Logics and Meanings of Programs]: \nSpecifying and Verifying and Reasoning about Pro\u00adgrams -Mechanical Veri.cation; F.3.3 [Logics and Meanings \nof Programs]: Studies of Program Constructs -Program and recursion schemes General Terms Algorithms, \nlanguages, reliability, theory, veri.\u00adcation Keywords Translation validation, symbolic evaluation, LLVM, \noptimization 1. Introduction Translation validation is a static analysis that, given two programs, tries \nto verify that they have the same semantics [13]. It can be used to ensure that program transformations \ndo not introduce semantic discrepancies, or to improve testing and debugging. Previous exper\u00adiments have \nsuccessfully applied a range of translation validation techniques to a variety of real-world compilers. \nNecula [11] exper\u00adimented on GCC 2.7; Zuck et al. [4] experimented on the Tru64 compiler; Rival [14] \nexperimented on unoptimized GCC 3.0; and Kanade et al. [8] experimented on GCC 4. Given all these results, \ncan we effectively validate a produc\u00adtion optimizer? For a production optimizer, we chose LLVM. To be \neffective, we believe our validator must satisfy the following crite\u00adria. First, we do not want to instrument \nthe optimizer. LLVM has a large collection of program transformations that are updated and improved at \na frantic pace. To be effective, we want to treat the Permission to make digital or hard copies of all \nor part of this work for personal or classroom use is granted without fee provided that copies are not \nmade or distributed for pro.t or commercial advantage and that copies bear this notice and the full citation \non the .rst page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires \nprior speci.c permission and/or a fee. PLDI 11, June 4 8, 2011, San Jose, California, USA. Copyright \nc &#38;#169; 2011 ACM 978-1-4503-0663-8/11/06. . . $10.00 optimizer as a black box . Second, we do not \nwant to modify the source code of the input programs. This means that we handle the output of the optimizer \nwhen run on the input C programs as is. Third, we want to run only one pass of validation for the whole \noptimization pipeline. This is important for ef.ciency, and also be\u00adcause the boundaries between different \noptimizations are not al\u00adways .rm. Finally, it is crucial that the validator can scale to large functions. \nThe experiments of Kanade et al. are the most impres\u00adsive of all, with an exceptionally low rate of false \nalarms (note the validator requires heavy instrumentation of the compiler). However the authors admit \nthat their approach does not scale beyond a few hundred instructions. In our experiments, we routinely \nhave to deal with functions having several thousand instructions. The most important issue is the instrumentation. \nTo our knowl\u00adedge, two previous works have proposed solutions that do not re\u00adquire instrumentation of \nthe optimizer. Necula evaluated the ef\u00adfectiveness of a translation validator for GCC 2.7 with common\u00adsubexpression \nelimination, register allocation, scheduling, and loop inversion. The validator is simulation-based: \nit veri.es that a simulation-relation holds between the two programs. Since the compiler is not instrumented, \nthis simulation relation is inferred by collecting constraints. The experimental validation shows that \nthis approach scales, as the validator handles the compilation of programs such as GCC or Linux. However, \nadding other optimiza\u00adtions such as loop-unswitch or loop-deletion is likely to break the collection \nof constraints. On the other hand, Tate et al. [16] recently proposed a system for translation validation. \nThey compute a value-graph for an in\u00adput function and its optimized counterpart. They then augment the \nterms of the graphs by adding equivalent terms through a process known as equality saturation, resulting \nin a data structure similar to the E-graphs of congruence closure. If, after saturation, the two graphs \nare the same, they can safely conclude that the two pro\u00adgrams they represent are equivalent. However, \nequality saturation was originally designed for other purposes, namely the search for better optimizations. \nFor translation validation, it is unnecessary to saturate the value-graph, and generally more ef.cient \nand scalable to simply normalize the graph by picking an orientation to the equa\u00adtions that agrees with \nwhat a typical compiler will do (e.g. 1+1 is replaced by 2). Their preliminary experimental evaluation \nfor the Soot optimizer on the JVM shows that the approach is effective and can lead to an acceptable \nrate of false alarms. However, it is unclear how well this approach would work for the more challenging \nopti\u00admizations available in LLVM, such as global-value-numbering with alias analysis or sparse-conditional \nconstant propagation. We believe that a normalizing value-graph translation validator would have both \nthe simplicity of the saturation validator proposed by Tate et al., and the scalability of Necula s constraint-based \nap\u00adproach. In this paper we set out to evaluate how well such a de\u00adsign works. We therefore present the \ndesign and implementation of such a validator along with experimental results for a number of benchmarks \nincluding SQLite [2] and programs drawn from spec marks [5], including GCC and a perl interpreter. The \nopti\u00admizations we consider include global-value numbering with alias analysis, sparse-conditional constant \npropagation, aggressive dead\u00adcode elimination, loop invariant code motion, loop unswitching, loop deletion, \ninstruction combining, and dead-store elimination. We have also experimented, on a smaller suite of hand-written \npro\u00adgrams and optimizations, that our tool could handle without further modi.cation various .avors of \nscheduling (list, trace, etc.) and loop fusion and .ssion.  While Tate et al. tried a saturation approach \non JVM code and found the approach to be effective, we consider here a normaliza\u00adtion approach in the \ncontext of C code. C code is challenging to validate because of the lack of strong typing and the relatively \nlow\u00adlevel nature of the code when compared to the JVM. However, our results show that a normalizing value-graph \ntranslation validator can effectively validate the most challenging intra-procedural opti\u00admizations of \nthe LLVM compiler. Another signi.cant contribution of our work is that we provide a detailed analysis \nof the effective\u00adness of the validator with respect to the different optimizations. Fi\u00adnally, we discuss \na number of more complicated approaches that we tried but ultimately found less successful than our relatively \nsimple architecture. The remainder of the paper is organized as follows. Section 2 presents the design \nof our tool. Section 3 details the construction of the value-graph using examples. Section 4 reviews \nthe normal\u00adization rules that we use and addresses their effectiveness through examples. Section 5 presents \nthe results of our experiments. We discuss related work in Section 6 and conclude in Section 7.  2. \nNormalizing translation validation Our validation tool is called LLVM-MD. At a high-level, LLVM-MD is \nan optimizer: it takes as input an LLVM assembly .le and outputs an LLVM assembly .le. The difference \nbetween our tool and the usual LLVM optimizer is that our tool certi.es that the semantics of the program \nis preserved. LLVM-MD has two com\u00adponents, the usual off-the-shelf LLVM optimizer, and a translation \nvalidator. The validator takes two inputs: the assembly code of a function before and after it has been \ntransformed by the optimizer. The validator outputs a boolean: true if it can prove the assem\u00adbly codes \nhave the same semantics, and false otherwise. Assuming the correctness of our validator, a semantics-preserving \nLLVM op\u00adtimizer can be constructed as follows (opt is the command-line LLVM optimizer, validate is our \ntranslation validator): function llvm-md(var input) { output = opt -options input for each function f \nin input { extract f from input as fi and output as fo if (!validate fi fo) { replace fo by fi in output \n} } return output } For now, our validator works on each function independently, hence the limitation \nto intra-procedural optimizations. We believe that standard techniques can be used to validate programs \nin the pres\u00adence of function inlining, but have not yet implemented these. Rather, we concentrate on \nthe workhorse intra-procedural opti\u00admizations of LLVM. At a high level, our tool works as follows. First, \nit compiles each of the two functions into a value-graph that represents the data dependencies of the \nfunctions. Such a value-graph can be thought of as a data.ow program, or as a generalization of the result \nof sym\u00adbolic evaluation. Then, each graph is normalized by rewriting using rules that mirror the rewritings \nthat may be applied by the off-the\u00adshelf optimizer. For instance, it will rewrite the 3-node sub-graph \nrepresenting the expression 2+3 into a single node representing the value 5, as this corresponds to constant \nfolding. Finally, we com\u00adpare the resulting value-graphs. If they are syntactically equivalent, the validator \nreturns true. To make comparison ef.cient, the value\u00adgraphs are hash-consed (from now on, we will say \nreduced ). In addition, we construct a single graph for both functions to allow sharing between the two \n(conceptually distinct) graphs. Therefore, in the best case when semantics has been preserved the com\u00adparison \nof the two functions has complexity O(1). The best-case complexity is important because we expect most \noptimizations to be semantics-preserving. The LLVM-MD validation process is depicted in .gure 1. First, \neach function is converted into Monadic Gated SSA form [6, 10, 18]. The goal of this representation is \nto make the assembly in\u00adstructions referentially transparent: all of the information required to compute \nthe value of an instruction is contained within the in\u00adstruction. More importantly, referential transparency \nallows us to substitute sub-graphs with equivalent sub-graphs without worrying about computational effects. \nComputing Monadic Gated SSA form is done in two steps: 1. Make side-effects explicit in the syntax by \ninterpreting assem\u00adbly instructions as monadic commands. For example, a load instruction will have a \nextra parameter representing the mem\u00adory state. 2. Compute Gated SSA form, which extends f-nodes with \ncon\u00additions, insert \u00b5-nodes at loop headers, and .-nodes at loop exits[18].  Once we have the Monadic \nGated SSA form, we compute a shared value-graph by replacing each variable with its de.nition, being \ncareful to maximize sharing within the graph. Finally, we apply normalization rules and maximize sharing \nuntil the value of the two functions merge into a single node, or we cannot perform any more normalization. \nIt is important to note that the precision of the semantics\u00adpreservation property depends on the precision \nof the monadic form. If the monadic representation does not model arithmetic over\u00ad.ow or exceptions, \nthen a successful validation does not guarantee anything about those effects. At present, we model memory \nstate, including the local stack frame and the heap. We do not model runtime errors or non-termination, \nalthough our approach can be extended to include them. Hence, a successful run of our validator implies \nthat if the input function terminates and does not produce a runtime error, then the output function \nhas the same semantics. Our tool does not yet offer formal guarantees for non-terminating or semantically \nunde.ned programs.  3. Validation by Example 3.1 Basic Blocks We begin by explaining how the validation \nprocess works for basic blocks. Considering basic blocks is interesting because it allows us to focus \non the monadic representation and the construction of the value-graph, leaving for later the tricky problem \nof placing gates in f-and .-nodes. Our validator uses LLVM assembly language as input. LLVM is a portable \nSSA-form assembly language with an in.nite number of registers. Because we start with SSA-form, producing \nthe value\u00adgraph consists of replacing variables by their de.nitions while maximizing sharing among graph \nnodes. For example, consider the Gating    Hash-consed symbolic analysis Normalization Merging + \n Gating query Are the state s pointers for both LLVM .les equal? Figure 1: LLVM M.D. from a bird s eye \nview following basic block, B1: B1: x1 =3+3 x2 = a * x1 x3 = x2 + x2 and its optimized counterpart, B2: \nB2: y1 = a * 6 y2 = y1 << 1 Replacing variables x1, x2,and y1 by their de.nition, we obtain the value-graph \npresented below. The dashed arrows are not part of the value graph, they are only meant to point out \nwhich parts of the graph correspond to which program variables. Note that both blocks have been represented \nwithin one value graph, and the node for the variable a has been shared. x3 y2  + \u00ab **1 +a6 3 Suppose \nthat we want to show that the variables x3 and y2 will hold the same value. Once we have the shared value \ngraph in hand, we simply need to check if x3 and y2 are represented by subgraphs rooted at the same graph \nnode. In the value graph above, x3 and y2 are not represented by the same subgraph, so we cannot conclude \nthey are equivalent. However, we can now apply normalization rules to the graph. First, we can apply \na constant folding rule to reduce the subgraph 3+3 to a single node 6. The resulting graph is shown below \n(we have maximized sharing in the graph). y2x3 + \u00ab *1 6a We have managed to make the value graph smaller, \nbut we still can\u00adnot conclude that the two variables are equivalent. So, we continue to normalize the \ngraph. A second rewrite rule allows us to replace x + x with x \u00ab 1 for any x. In our setting, this rule \nis only appro\u00adpriate if the optimizer would prefer the shift instruction to addition (which LLVM does). \nAfter replacing addition with left shift, and maximizing sharing, x3 and y2 point to the same node and \nwe can conclude that the two blocks are equivalent. Side Effects. The translation we have described up \nto this point would not be correct in the presence of side effects. Consider the following basic block. \np1 = alloc 1 p2 = alloc 1 store x, p1 store y, p2 z = load p1 If we naively apply our translation, then \nthe graph corresponding to z would be: z . load (alloc 1), which does not capture the complete computation \nfor register z. In order to make sure that we do not lose track of side effects, we use abstract state \nvariables to capture the dependencies between instructions. A simple transla\u00adtion gives the following \nsequence of instructions for this block: p1,m1 = alloc 1,m0 p2,m2 = alloc 1,m1 m3 = store x, p1,m2 m4 \n= store y, p2,m3 z, m5 = load p1,m4 Here, the current memory state is represented by the m registers. \nEach instruction requires and produces a memory register in ad\u00addition to its usual parameters. This extra \nregister enforces a depen\u00addency between, for instance, the load instruction and the preceding store instructions. \nThis translation is the same as we would get if we interpreted the assembly instructions as a sequence \nof monadic commands in a simple state monad[10]. Using these monadic in\u00adstructions, we can apply our \ntransformation and produce a value graph that captures all of the relevant information for each register. \nThe rewriting rules in our system are able to take into account aliasing information to relax the strict \nordering of instructions im\u00adposed by the monadic transformation. In our setting (LLVM), we know that \npointers returned by alloc never alias with each other.  Using this information, we are able to replace \nm4 with m3 in the load instruction for z. Then, because we have a load from a mem\u00adory consisting of a \nstore to the same pointer, we can simplify the load to x. Using the state variables, we can validate \nthat a function not only computes the same value as another function, but also affects the heap in the \nsame way. The same technique can be applied to different kinds of side effects, such as arithmetic over.ow, \ndivision by zero, and non-termination. Thus far we have only modeled memory side-effects in our implementation. \nHence, we only prove semantics preservation for terminating programs that do not raise runtime errors. \nHowever, our structure allows us to easily extend our implementation to a more accurate model, though \ndoing so may make it harder to validate optimizations.  3.2 Extended Basic Blocks These ideas can be \ngeneralized to extended basic blocks as long as f-nodes are referentially transparent. We ensure this \nthrough the use of gated f-nodes. Consider the following program, which uses a normal f-node as you would \n.nd in an SSA-form assembly program. entry : c = a<b cbr c, true, false true : x1 = x0 + x0 (True branch) \nbr join false : x2 = x0 * x0 (False branch) br join join : x3 = f(x1,x2) (Join point) Rewriting these \ninstructions as is would not be correct. For in\u00adstance, replacing the condition a<b by a = b would result \nin the same value-graph, and we could not distinguish these two different programs. However, if the f-node \nis extended to include the conditions for taking each branch, then we can distinguish the two programs. \nIn our example, the last instruction would become x3 = f(b, x1,x2), which means x3 is x1 if b is true,and \nx2 other\u00adwise. In order to handle real C programs, the actual syntax of f-nodes has to be a bit more \ncomplex. In general, a f-node is composed of a set of possible branches, one for each control-.ow edge \nthat enters the f-node. Each branch has a set of conditions, all of which must be true for the branch \nto be taken.1 .. . c11 ...c1n . v1 . .. f \u00b7\u00b7\u00b7 .. .. ck1 ...ckm . vk Given this more general syntax, the \nnotation f(c, x, y) is simply shorthand for f(c . x, !c . y). Gated f nodes come along with a set of \nnormalization rules that we present in the next section. When generating gated f-nodes, it is important \nthat the conditions for each branch are mutually exclusive with the other branches. This way, we are \nfree to apply normalization rules to f-nodes (such as reordering conditions and branches) without worrying \nabout changing the semantics. It is also worth noting that if the values coming from various paths are \nequivalent, then they will be shared in the value graph. This makes it possible to validate optimizations \nbased on global\u00ad 1 f-nodes with several branches and many conditions are very common in C programs. For \nexample, an if-statement with a condition that uses short-cut boolean operators, can produce complex \nf-nodes. value numbering that is aware of equivalences between de.nitions from distinct paths.  3.3 \nLoops In order to generalize our technique to loops, we must come up with a way to place gates within \nlooping control .ow (including breaks, continues, and returns from within a loop). Also, we need a way \nto represent values constructed within loops in a referentially transparent way. Happily, Gated SSA form \nis ideally suited to our purpose. Gated SSA uses two constructs to represent loops in a referen\u00adtially \ntransparent fashion. The .rst, \u00b5,is usedto de.ne variables that are modi.ed within a loop. Each \u00b5 is \nplaced at a loop header and holds the initial value of the variable on entry to the loop and a value \nfor successive iterations. The \u00b5-node is equivalent to a non\u00adgated f node from classical SSA. The second, \n., isusedtorefer to loop-de.ned variables from outside their de.ning loops. The .\u00adnode carries the variable \nbeing referred to along with the condition required to reach the . from the variable de.nition. Figure \n2a shows a simple while loop in SSA form. The Gated SSA form of the same loop is shown in .gure 2b. The \nf-nodes in the loop header have been replaced with \u00b5-nodes, and the access to the xp register from outside \nto loop is transformed into an .-node that carries the condition required to exit the loop and arrive \nat this de.nition. With Gated SSA form, recursively de.ned variables must con\u00adtain a \u00b5-node. The recursion \ncan be thought of as a cycle in the value graph, and all cycles are dominated by a \u00b5-node. The value \ngraph corresponding to our previous example is presented in .gure 2c. Intuitively, the cycle in the value-graph \ncan be thought of as generating a stream of values. The \u00b5-nodes start by selecting the initial value \nfrom the arrow marked with an i . Successive values are generated from the cyclic graph structure attached \nto the other arrow. This \u00b5-node produces the values c, c +1, c +2,... The . receives a stream of values \nand a stream of conditions. When the stream of conditions goes from true to false,the . selects the corresponding \nvalue in the value stream. Generally, we can think of \u00b5 and . behaving according to the following formulas: \n\u00b5(a, n)= a : \u00b5(n[a/x],n) .(0 : b, x : v)= .(b, v) .(1 : b, x : v)= x Of course, for our purposes, we \ndo not need to evaluate these formulas, we simply need an adequate, symbolic representation for the registers \nx, xp and bp. A more formal semantics should probably borrow ideas from data.ow programming languages \nsuch as those studied by Tate et. al.[16]. 4. Normalization Once a graph is constructed for two functions, \nif the functions values are not already equivalent, we begin to normalize the graph. We normalize value \ngraphs using a set of rewrite rules. We apply the rules to each graph node individually. When no more \nrules can be applied, we maximize sharing within the graph and then reapply our rules. When no more sharing \nor rules can be applied, the process terminates. Our rewrite rules come in two basic types: general simpli.ca\u00adtion \nrules and optimization-speci.c rules. The general simpli.ca\u00adtion rules reduce the number of graph nodes \nby removing unneces\u00adsary structure. We say general because these rules only depend on the graph representation, \nreplacing graph structures with smaller, simpler graph structures. The optimization-speci.c rules rewrite \ngraphs in a way that mirrors the effects of speci.c optimizations.  .  x0 = cx0 = c < loop : loop \n: xp = f(x0,xk) xp = \u00b5(x0,xk) \u00b5 b = xp <n b = xp <n cbr b, loop1, exit cbr b, loop1, exit  1 loop1 \n: loop1 : (c) Value graph representation xk = xp +1 xk = xp +1 br loop br loop exit : exit : x = xp \nx = .(b, xp) (a) SSA while loop (b) GSSA while loop Figure 2: Representation of while loops These rules \ndo not always make the graph smaller or simpler, and one often needs to have speci.c optimizations in \nmind when adding them to the system. General Simpli.cation Rules. The notation a . b means that we match \ngraphs with structure a and replace them with b.The .rst four general ruless simplify boolean expressions: \na = a . true (1) a (2) = a . false a = true . a (3) a (4) = false . a These last two rules only apply \nif the comparison is performed at the boolean type. In fact, all LLVM operations, and hence our graph \nnodes, are typed. The types of operators are important and uninteresting: we do not discuss types in \nthis paper. There are two general rules for removing unnecessary f-nodes. f {..., truei . t,...}. t (5) \n{} fci . t. t (6) The .rst rule replaces a f-node with one of its branches if all of its conditions are \nsatis.ed for that branch. We write xi for a set of terms indexed by i.Inthe .rst rule, we have a set \nof true values. Note that the conditions for each branch are mutually exclusive with the other branches, \nso only one branch can have conditions which are all true. The second rule removes the f-node if all \nof the branches contain the same value. A special case of this rule is a f\u00adnode with only one branch \nindicating that there is only one possible path to the f-node, as happens with branch elimination. The \nf rules are required to validate sparse conditional constant propagation (SCCP) and global value numbering \n(GVN). The fol\u00adlowing example can be optimized by both: if (c) {a =1; b= 1;d =a;} else {a =2; b= 2;d \n=1;} if (a == b){x =d;} else {x = 0;} return x; Applying global-value numbering followed by sparse conditional \nconstant propagation transforms this program to return 1.In\u00addeed, in each of the branches of the .rst \nif-statement, a is equal to b.Since a == b is always true, the condition of the second if-statement is \nconstant, and sparse conditional constant propaga\u00adtion can propagate the left de.nition of x. The above \nprogram and return 1 have the same normalized value graph, computed as fol\u00adThere are also general rules \nfor simplifying .-and \u00b5-nodes. The .rst rule allows us to remove loops that never execute. lows: x .f(f(c, \n1, 2) == f(c, 1, 2),f(c, 1, 1), 0) .f(true,f(c, 1, 1), 0) .f(c, 1, 1) .1 by (1) by (5) by (6) .(false,\u00b5(x, \ny)) . x (7) This rule rewrites to the initial value of the \u00b5-node before the loop is entered, namely \nx. This rule is needed to validate loop-deletion, a form of dead code elimination. In addition, there \nare two rules for loop invariants. The .rst says that if we have a constant \u00b5-node, then the corresponding \n.-node can be removed: .(c, \u00b5(x, x)) . x (8) .(c, y . \u00b5(x, y)) . x (9) In rule (8), the \u00b5-node has an \ninitial value of x, which must be de.ned outside of the loop, and therefore cannot vary within the loop. \nSince, x does not vary within the loop the \u00b5-node does not vary, and the loop structure can be removed. \nRule (9), expresses the same condition, but the second term in the \u00b5-node is again the same \u00b5-node (we \nuse the notation y . \u00b5(x, y) to represent this self-reference). These rules are necessary to validate \nloop invariant code motion. As an example, consider the following program: x= a+3;c =3; for (i =0; i< \nn; i++) {x= a+ c;} return x; In this program, variable x is de.ned within a loop, but it is invari\u00adant. \nMoreover, variable c is a constant. Applying global constant propagation, followed by loop-invariant \ncode motion and loop dele\u00adtion transforms the program to return (a + 3). The value graph for x is computed \nas follows: in . \u00b5(0,in +1) x . .(in <n,\u00b5(a +3,a +3)) . a + c (by 8) Note that the global copy propagation \nis taken care of automati\u00adcally by our representation, and we can apply our rule (8) imme\u00addiately. The \nother nodes of the graph (in) are eliminated since they are no longer needed.  Optimization-speci.cRules. \nIn addition to the general rules, we also have a number of rewrite rules that are derived from the semantics \nof LLVM. For example, we have a family of laws for simplifying constant expressions, such as: add 32 \n. 5 mul 32 . 6 sub 32 . 1 Currently we have rules for simplifying constant expressions over integers, \nbut not .oating point or vector types. There are also rules for rewriting instructions such as: add aa \n. shl a 1 mul a 4 . shl a 2 These last two rules are included in our validator because we know the LLVM \ns optimizer prefers the shift left instruction. While preferring shift left may be obvious, there are \nsome less obvious rules such as: add x (-k) . sub xk gt 10 a . lt a 10 lt ab . le a (sub b 1) While these \ntransformations may not be optimizations, we believe LLVM makes them to give the instructions a more \nregular structure. Finally, we also have rules that make use of aliasing information to simplify memory \naccesses. For example, we have the following two rules for simplifying loads from memory: load(p, store(x, \nq, m)) . load(p, m) (10) load(p, store(x, p, m)) . x (11) when p and q do not alias. Our validator can \nuse the result of a may alias analysis. For now, we only use simple non-aliasing rules: two pointers \nthat originate from two distinct stack allocations may not alias; two pointers forged using getelemptr \nwith different parameters may not alias, etc.  4.1 Ef.ciency At this point is natural to wonder why \nwe did not simply de.ne a normal form for expressions and rewrite our graphs to this normal form. This \nis a good option, and we have experimented with this strategy using external SMT provers to .nd equivalences \nbetween expressions. However, one of our goals is to build a practical tool which optimizes the best \ncase performance of the validator (we expect most optimizations to be correct). Using our strategy of \nperforming rewrites motivated by the optimizer, we are often able to validate functions with tens of \nthousands of instructions (resulting in value graphs with hundreds of thousands of nodes) with only a \nfew dozen rewritings. That is, we strive to make the amount of work done by the validator proportional \nto the number of transformations performed by the optimizer. To this end, the rewrite rules derived from \nLLVM semantics are designed to mirror the kinds of rewritings that are done by the LLVM optimization \npipeline. In practice, it is much more ef\u00ad.cient to transform the value graphs in the same way the optimizer \ntransforms the assembly code: if we know that LLVM will prefer shl a 1 to a + a, then we will rewrite \na + a but not the other way around. As another, more extreme, example, consider the following C code, \nand two possible optimizations: SCCP and GVN. a=x<y; b=x<y; if (a) { if (a== b) {c= 1;} else {c =2;} \n+ ld st 42 . st m < \u00b5 \u00b5 + st 1 n f 01 al == al f  ... 2 1 Figure 3: A shared value-graph } else \n{c = 1;} return c; If the optimization pipeline is setup to apply SCCP .rst, then a may be replaced by \ntrue. In this case, GVN can not recognize that a and b are equal, and the inner condition will not be \nsimpli.ed. However, if GVN is applied .rst, then the inner condition can be simpli.ed, and SCCP will \npropagate the value of c, leading to the program that simply returns 1. The problem of how to order optimizations \nis well-known, and an optimization pipeline may be reordered to achieve better results. If the optimization \npipeline is con.gured to use GVN before SCCP, then, for ef.ciency, our simpli.cation should be setup \nto simplify at join points before we substitute the value of a.  4.2 Extended Example We now present \nan example where all these laws interplay to produce the normalized value-graph. Consider the C code \nbelow: int f(int n, int m) { int * t= NULL; int * t1 = alloca(sizeof(int)); int * t2 = alloca(sizeof(int)); \nint x, y, z=0; *t1=1; *t2=m; t = t1; for (int i =0; i< n;++i) { if(i % 3){ x =1; z= x<< y;y =x; } else \n{ x =2; y= 2; } if(x==y) t=t1; else t= t2; } *t =42; return *t2 + *t2; } First, note that this function \nreturns m + m. Indeed, x is always equal to y after the execution of the .rst conditional statement in \nthe for loop. Therefore, the second conditional statement always executes the left branch and assigns \nt1 to t. This is actually a loop invariant, and, since t1 is assigned to t before the loop, t1 is always \nequal to t. t1 and t2 are pointers to two distinct regions of memory and cannot alias. Writing through \nt1 does not affect what is pointed to by t2, namely m. The function therefore returns m + m.Since the \nloop terminates, an optimizer may replace the body of this function with m \u00ab 1, using a blend of global-value \nnumbering with alias analysis, sparse-conditional constant propagation and loop deletion.  Our value-graph \nconstruction and normalization produces the value-graph corresponding to m \u00ab 1 for this example. The \ninitial value-graph is presented in .gure 3. Some details of the graph have been elided for clarity. \nWe represent load, store,and alloca nodes with ld, st,and al respectively. To make the graph easier to \nread, we also used dashed lines for edges that go to pointer values. Below is a potential normalization \nscenario. The arguments of the == node are shared; it is rewritten to true.  Thegateofthe f node is \ntrue; its predecessor, \u00b5, is modi.ed to point to the target of the true branch of the f node rather than \nto f itself.  The arguments of this \u00b5 node are shared; its predecessor, .,is modi.ed to point to the \ntarget of \u00b5 instead of \u00b5 itself.  The condition of the . node is terminating, and its value acyclic; \nIts predecessor, st42, is modi.ed to point to the target of . instead of . itself.  It is now obvious \nthat the load and its following store use distinct memory regions; the load can jump over the store. \n The load and its new store use the same memory region; the load is therefore replaced by the stored \nvalue, m.  The arguments of the + node are shared; The + node is rewrit\u00adten into a left shift.   \n5. Experimental evaluation In our experimental evaluation, we aim to answer three different questions. \n1. How effective is the tool for a decent pipeline of optimization? 2. How effective is the tool optimization-by-optimization? \n 3. What is the effect of normalization?  Our measure of effectiveness is simple: the fewer false alarms \nour tool produces, the more optimized the code will be. Put another way, assuming the optimizer is always \ncorrect, what is the cost of validation? In our experiments, we consider any alarm to be a false alarm. \n 5.1 The big picture We have tested our prototype validator on the pure C programs of the SPECCPU 2006[5] \nbenchmark (The xalancbmk benchmark is missing because the LLVM bitcode linker fails on this large program). \nIn addition, we also tested our validator on the SQLite embedded database[2]. Each program was .rst compiled \nwith clang version 2.8[1], and then processed with the mem2reg pass of the LLVM compiler to place f-nodes. \nThese assembly .les make up the unoptimized inputs to our validation tool. Each unoptimized input was \noptimized with LLVM, and the result compared to the unoptimized input by our validation tool. Test suite \ninformation. Table 1 lists the benchmark programs with the size of the LLVM-assembly code .le, number \nof lines of assembly, and the number of functions in the program. Our research SQLite 5.6M 136K 1363 \nbzip2 904K 23K 104 gcc 63M 1.48M 5745 h264ref 7.3M 190K 610 hmmer 3.3M 90K 644 lbm 161K 5K 19 libquantum \n337K 9K 115 mcf 149K 3K 24 milc 1.2M 32K 237 perlbench 15M 399K 1998 sjeng 1.5M 39K 166 sphinx 1.7M 44K \n391 Table 1: Test suite information prototype does not analyze functions with irreducible control .ow \ngraphs. This restriction comes from the front-end s computation of Gated SSA form. It is well known how \nto compute Gated SSA form for any control-.ow graph[18]. However, we have not extended our front-end \nto handle irreducible control .ow graphs. We do not believe the few irreducible functions in our benchmarks \nwill pose any new problems, since neither the Gated SSA form nor our graph representation would need \nto be modi.ed. Pipeline information. For our experiment, we used a pipeline consisting of: ADCE (advanced \ndead code elimination), followed by  GVN (global value numbering),  SCCP (sparse-condition constant \npropagation),  LICM (loop invariant code motion),  LD (loop deletion),  LU (loop unswitching),  DSE \n(dead store elimination).  The optimizations we chose are meant to cover, as much as possi\u00adble, the \nintra-procedural optimizations available in LLVM. We do not include constant propagation and constant \nfolding because both of these are subsumed by sparse-conditional constant propagation (SCCP). Similarly, \ndead-code and dead-instruction elimination are subsumed by aggressive dead-code elimination (ADCE). We \ndid not include reassociate and instcombine because we haven t ad\u00addressed those yet. One reason these \nhaven t been our priority is that they are conceptually simple to validate but require many rules. For \neach benchmark, we ran all of the optimizations, and then attempted to validate the .nal result. Since \nwe used the SQLite benchmark to engineer our rules, it is not surprising that, overall, that benchmark \nis very close to 90%. The rules chosen by studying SQLite are also very effective across the other benchmarks. \nWe do not do quite as well for the perlbench and gcc benchmarks. Cur\u00adrently, our tool only uses the basic \nrewrite rules we have described here. Also we do not handle global constants or .oating point ex\u00adpressions. \nPipeline Results. Overall, with the small number of rules we have described in this paper, we can validate \n80% of the per-function optimizations. The results per-benchmark are shown in .gure 4. For our measurements, \nwe counted the number of functions for which we could validate all of the optimizations performed on \nthe function: even though we may validate many optimizations, if even one optimization fails to validate \nwe count the entire function as failed. We found that this conservative approach, while rejecting  \nFigure 4: Validation results for optimization pipeline more optimizations, leads to a simpler design \nfor our validated optimizer which rejects or accepts whole functions at a time. The validation time for \nGCC is 19m19s, perl 2m56s, and SQLite 55s.  5.2 Testing Individual Optimizations The charts in Figure \n5 summarize the results of validating functions for different, single optimizations. The height of each \nbar indicates the total number of functions transformed for a given benchmark and optimization. The bar \nis split showing the number of validated (below) and unvalidated (above) functions. The total number \nof functions is lower in these charts because we do not count functions that are not transformed by the \noptimization. It is clear from the charts that GVN with alias analysis is the most challenging optimization \nfor our tool. It is also the most important as it performs many more transformations than the other optimizations. \nIn the next section we will study the effectiveness of rewrite rules on our system.  5.3 Rewrite Rules \nFigure 6 shows the effect of different rewrite rules for the GVN optimization with our benchmarks. The \ntotal height of each bar shows the percentage of functions we validated for each benchmark after the \nGVN optimization. The bars are divided to show how the results improve as we add rewrite rules to the \nsystem. We start with no rewrite rules, then we add rules and measure the improvement. The bars correspond \nto adding rules as follows: 1. no rules 2. f simpli.cation 3. constant folding 4. load/store simpli.cation \n 5. . simpli.cation 6. commuting rules  We have already described the .rst .ve rules. The last set \nof rules tries to rearrange the graph nodes to enable the former rules. For example, we have a rule that \ntries to push down .-nodes to get them close to the matching \u00b5-nodes. We can see from this chart that \ndifferent benchmarks are ef\u00adfected differently by the different rules. For example, SQLite is not improved \nby adding rules for constant folding or f simpli.cation. However, load/store simpli.cation has an effect. \nThis is probably because SQLite has been carefully tuned by hand and does not have many opportunities \nfor constant folding or branch elimination. The lbm benchmark, on the other hand, bene.ts quite a lot \nfrom f sim\u00adpli.cation. Figure 6: GVN From the data we have, it seems that our technique is able to suc\u00adcessfully \nvalidate approximately 50% of GVN optimizations with no rewrite rules at all. This makes intuitive sense \nbecause our sym\u00adbolic evaluation hides many of the syntactic details of the programs, and the transformations \nperformed by many optimizations are, in the end, minor syntactic changes. By adding rewrite rules we \ncan dramatically improve our results. Up to this point, we have avoided adding special purpose rules. \nFor instance, we could improve our results by adding rules that allow us to reason about speci.c C library \nfunctions. For example, the rule: x = atoi(p); y = atoi(q); =. y = atoi(q); x = atoi(p); can be added \nbecause atoi does not modify memory. Another example is: memset(p, x, l1); =. y = x y = load(getelemptr(p, \nl2)) l2<l1 which enables more aggressive constant propagation. Both of these rules seem to be used by \nthe LLVM optimizer, but we have not added them to our validator at this time. However, adding these sorts \nof rules is fairly easy, and in a realistic setting many such rules would likely be desired. Figure 7 \nshows similar results for loop-invariant code motion (LICM). The baseline LICM, with no rewrite rules, \nis approxi\u00admately 75-80%. If we add in all of our rewrite rules, we only im\u00adprove very slightly. In theory, \nwe should be able to completely val\u00adidate LICM with no rules. However, again, LLVM uses speci.c knowledge \nof certain C library functions. For example, in the fol\u00adlowing loop: for (int i = 0; i < strlen(p); i++) \nf(p[i]); the call to strlen is known (by LLVM) to be constant. Therefore, LLVM will lift the call to \nstrlen out of the loop: int tmp = strlen(p); for (int i = 0; i < tmp; i++) f(p[i]); Our tool does not \nhave any rules for speci.c functions, and there\u00adfore we do not validate this transformation. The reason \nwhy we sometimes get a small improvement in LICM is because very oc\u00adcasionally a rewriting like the one \nabove corresponds to one of our general rules. Finally, .gure 8 shows the effect of rewrite rules on \nsparse\u00adconditional constant propagation. For this optimization, we used four con.gurations: 1. no rules \n  Figure 5: Validator results for individual optimizations Figure 7: LICM 2. constant folding 3. f \nsimpli.cation 4. all rules  As expected, with no rules the results are very poor. However, if we add \nrules for constant folding, we see an immediate improvement, as expected. If we also add rules for reducing \nf-nodes, bzip2 immediately goes to 100%, even though these rules ave no effect on SQLite. However, additional \nrules do improve SQLite, but not the other benchmarks.  5.4 Discussion While implementing our prototype, \nwe were surprised to .nd that essentially all of the technical dif.culties lie in the complex f\u00adnodes. \nIn an earlier version of this work we focused on structured code, and the (binary) f-nodes did not present \nany real dif.culties. However, once we moved away from the structured-code restriction Figure 8: SCCP \nwe encountered more problems. First, although the algorithms are known, computing the gates for arbitrary \ncontrol .ow is a fairly involved task. Also, since the gates are dependent on the paths in the CFG, and \ngeneral C code does not have a simple structured control .ow, optimizations will often change the gating \nconditions even if the control .ow is not changed. Another important aspect of the implementation is \nthe technique for maximizing sharing within the graph. The rewrite rules do a good job of exposing equivalent \nleaf nodes in the graphs. However, in order to achieve good results, it is important to .nd equivalent \ncycles in the graphs and merge them. Again, matching complex f-nodes seems to be the dif.cult part. To \nmatch cycles, we .nd pairs of \u00b5-nodes in the graph, and trace along their paths in par\u00adallel trying to \nbuild up a unifying substitution for the graph nodes involved. For f-nodes we sort the branches and conditions \nand per\u00adform a syntactic equality check. This technique is very simple, and ef.cient because it only \nneeds to query and update a small portion of the graph.  We also experimented with a Hopcroft partitioning \nalgorithm[7]. Rather than a simple syntactic matching, our partitioning algorithm uses a prolog-style \nbacktracking uni.cation algorithm to .nd con\u00adgruences between f-nodes. Surprisingly, the partitioning \nalgorithm with backtracking does not perform better than the simple uni.ca\u00adtion algorithm. Both algorithms \ngive us roughly the same percent\u00adage of validation. Our implementation uses the simple algorithm by default, \nand when this fails it falls back to the slower, partitioning algorithm. Interestingly, this strategy \nperforms slightly better than either technique alone, but not signi.cantly better. Matching expressions \nwith complex f-nodes seems well within the reach of any SMT prover. Our preliminary experiments with \nZ3 suggest that it can easily handle the sort of equivalences we need to show. However, this seems like \na very heavy-weight tool. One question in our minds is whether or not there is an effective tech\u00adnique \nsomewhere in the middle: more sophisticated than syntactic matching, but short of a full SMT prover. \n  6. Related work How do those results compare with the work of Necula and Tate et al.? The validator \nof Necula validates part of GCC 2.7 and the ex\u00adperiments show the results of compiling, and validating, \nGCC 2.91. It is important to note that the experiments were run on a Pentium Pro machine running at 400 \nMHz. Four optimizations were con\u00adsidered. Common-subexpression elimination, with a rate of false alarms \nof roughly 5% and roughly 7 minutes running time. Loop unrolling with a rate of false alarms of 6.3% \nand roughly 17 min\u00adutes running time. Register allocation with a rate of false alarms of 0.1% and around \n10 minutes running time. Finally, instruction scheduling with a rate of false alarms of 0.01% and around \n9 min\u00adutes running time. Unfortunately, the only optimization that we can compare to is CSE as we do \nnot handle loop unrolling, and reg\u00adister allocation is part of the LLVM backend. In theory, we could \nhandle scheduling (with as good results) but LLVM does not have this optimization. For CSE, our results \nare not as good as Necula s. However, we are dealing with a more complex optimization: global value numbering \nwith partial redundancy elimination and alias in\u00adformation, libc knowledge, and some constant folding. \nThe running times are also similar, but on different hardware. It is therefore un\u00adclear whether our validator \ndoes better or worse. The validator of Tate et al. validates the Soot research compiler which compiles \nJava code to the JVM. On SpecJVM they report an impressive rate of alarms of only 2%. However, the version \nof the Soot optimizer they validate uses more basic optimizations than LLVM, and does not include, for \ninstance, GVN. Given that our results are mostly directed by GVN with alias analysis, it makes comparisons \ndif.cult. Moreover, they do not explain whether the number they report takes into account all the functions \nor only the ones that were actually optimized. The validator of Kanade et al., even though heavily instru\u00admented, \nis also interesting. They validate GCC 4.1.0 and report no false alarms for CSE, LICM, and copy propagation. \nTo our knowl\u00adedge, this experiment has the best results. However, it is unclear whether their approach \ncan scale. The authors say that their ap\u00adproach is limited to functions with several hundred RTL instruc\u00adtions \nand a few hundred transformations. As explained in the in\u00adtroduction, functions with more than a thousand \ninstructions are common in our setting. There is a wide array of choices for value-graph representations \nof programs. Weise et al. [19] have a nice summary of the various value-graphs. Ours is close to the \nrepresentation that results from the hash-consed symbolic analysis of a gated SSA graph [6, 18]. 7. \nConclusion In conclusion, we believe that normalizing value-graph translation validation of industrial-strengh \ncompilers without instrumentation is feasible. The design relies on well established algorithms and is \nsimple enough to implement. We have been able, in roughly 3 man-months, to build a tool that can validate \nthe optimizations of a decent LLVM pipeline on challenging benchmarks, with a reason\u00adable rate of false \nalarms. Better yet, we know that many of the false alarms that we witness now require the addition of \nnormalization rules but no signi.cant changes in the design. For instance, insider knowledge of libc \nfunctions, .oating-points constant folding and folding of global variables are sources of false alarms \nthat can be dealt with by adding normalization rules. There is also room for improvement of the runtime \nperformance of the tool. There are still a few dif.cult challenges ahead of us, the most important of \nwhich is inter-procedural optimizations. With LLVM, even -O1 makes use of such optimizations and, even \nthough it is clear that simulation-based translation validation can handle inter\u00adprocedural optimizations \n[12], we do not yet know how to precisely generalize normalizing translation. We remark that, in the \ncase of safety-critical code that respects standard code practices [3], as can be produced by tools like \nSimulink [15], the absence of recursive functions allows us to inline every function (which is reasonable \nwith hash-consing). Preliminary experiments indicate that we are able to validate very effectively inter-procedural \noptimizations in such a restricted case. Advanced loop transformations are also important, and we believe \nthat this problem may not be as hard as it may seem at .rst. Previous work [9, 17] has shown that it \ncan be surprisingly easy to validate advanced loop optimizations such as software pipelining with modulo \nvariable expansion if we reason at the value-graph level.  Acknowledgments We would like to thank Vikram \nAdve for his early interest and en\u00adthusiasm, and Sorin Lerner for discussing this project and exchang\u00ading \nideas.  References [1] LLVM 2.8. http://llvm.org. [2] SQLite 3. http://www.sqlite.org. [3] The Motor \nIndustry Software Reliability Association. Guidelines for the use of the c language in critical systems. \nhttp://www.misra.org.uk, 2004. [4] Clark W. Barrett, Yi Fang, Benjamin Goldberg, Ying Hu, Amir Pnueli, \nand Lenore Zuck. TVOC: A translation validator for optimizing compilers. In Computer Aided Veri.cation, \nvolume 3576 of Lecture Notes in Computer Science, pages 291 295. Springer, 2005. [5] SPEC CPU2006. http://www.spec.org/cpu2006/. \n[6] Paul Havlak. Construction of thinned gated single-assignment form. In Proc. 6rd Workshop on Programming \nLanguages and Compilers for Parallel Computing, pages 477 499. Springer Verlag, 1993. [7] John Hopcroft. \nAn n log n algorithm for minimizing states of a .nite automaton. In The Theory of Machines and Computations, \n1971. [8] Aditya Kanade, Amitabha Sanyal, and Uday Khedker. A PVS based framework for validating compiler \noptimizations. In 4th Software Engineering and Formal Methods, pages 108 117. IEEE Computer Society, \n2006. [9] Xavier Leroy. Formal certi.cation of a compiler back-end, or: pro\u00adgramming a compiler with \na proof assistant. In 33rd symposium Prin\u00adciples of Programming Languages, pages 42 54. ACM Press, 2006. \n[10] E. Moggi. Computational lambda-calculus and monads. In 4th Logic in computer science, pages 14 23. \nIEEE, 1989.  [11] George C. Necula. Translation validation for an optimizing compiler. In Programming \nLanguage Design and Implementation 2000, pages 83 95. ACM Press, 2000. [12] Amir Pnueli and Anna Zaks. \nValidation of interprocedural optimiza\u00adtion. In Proc. Workshop Compiler Optimization Meets Compiler Veri\u00ad.cation \n(COCV 2008), Electronic Notes in Theoretical Computer Sci\u00adence. Elsevier, 2008. [13] Amir Pnueli, Michael \nSiegel, and Eli Singerman. Translation valida\u00adtion. In Tools and Algorithms for Construction and Analysis \nof Sys\u00adtems, TACAS 98, volume 1384 of Lecture Notes in Computer Science, pages 151 166. Springer, 1998. \n[14] Xavier Rival. Symbolic transfer function-based approaches to certi\u00ad.ed compilation. In 31st symposium \nPrinciples of Programming Lan\u00adguages, pages 1 13. ACM Press, 2004. [15] Simulink. http://mathworks.com. \n[16] Ross Tate, Michael Stepp, Zachary Tatlock, and Sorin Lerner. Equality saturation: A new approach \nto optimization. In 36th Principles of Programming Languages, pages 264 276. ACM, 2009. [17] Jean-Baptiste \nTristan and Xavier Leroy. A simple, veri.ed valida\u00adtor for software pipelining. In 37th Principles of \nProgramming Lan\u00adguages, pages 83 92. ACM Press, 2010. [18] Peng Tu and David Padua. Ef.cient building \nand placing of gating functions. In Programming Language Design and Implementation, pages 47 55, 1995. \n[19] Daniel Weise, Roger F. Crew, Michael D. Ernst, and Bjarne Steens\u00adgaard. Value dependence graphs: \nRepresentation without taxation. In 21st Principles of Programming Languages, pages 297 310, 1994.  \n \n\t\t\t", "proc_id": "1993498", "abstract": "<p>Translation validators are static analyzers that attempt to verify that program transformations preserve semantics. Normalizing translation validators do so by trying to match the value-graphs of an original function and its transformed counterpart. In this paper, we present the design of such a validator for LLVM's intra-procedural optimizations, a design that does not require any instrumentation of the optimizer, nor any rewriting of the source code to compile, and needs to run only once to validate a pipeline of optimizations. We present the results of our preliminary experiments on a set of benchmarks that include GCC, a perl interpreter, SQLite3, and other C programs.</p>", "authors": [{"name": "Jean-Baptiste Tristan", "author_profile_id": "81342514111", "affiliation": "Harvard University, Cambridge, MA, USA", "person_id": "P2690566", "email_address": "tristan@seas.harvard.edu", "orcid_id": ""}, {"name": "Paul Govereau", "author_profile_id": "81100380486", "affiliation": "Harvard University, Cambridge, MA, USA", "person_id": "P2690567", "email_address": "govereau@seas.harvard.edu", "orcid_id": ""}, {"name": "Greg Morrisett", "author_profile_id": "81339518683", "affiliation": "Harvard University, Cambridge, MA, USA", "person_id": "P2690568", "email_address": "greg@seas.harvard.edu", "orcid_id": ""}], "doi_number": "10.1145/1993498.1993533", "year": "2011", "article_id": "1993533", "conference": "PLDI", "title": "Evaluating value-graph translation validation for LLVM", "url": "http://dl.acm.org/citation.cfm?id=1993533"}