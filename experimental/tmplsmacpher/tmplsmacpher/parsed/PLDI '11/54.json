{"article_publication_date": "06-04-2011", "fulltext": "\n Synchronization via Scheduling TechniquesForEf.ciently Managing Shared State MicahJBest Shane Mottishaw \n* Andrew Brownsword University of British Columbia Craig Mustard Mark Roth Electronic Arts, Inc mjbest@cs.ubc.ca \nAlexandra Fedorova brownsword@ea.com Simon Fraser University {smottish, cam14, mroth, fedorova}@cs.sfu.ca \nAbstract Shared state access con.icts are one of the greatest sources of er\u00adror for .ne grained parallelism \nin any domain. Notoriously hard to debug, these con.icts reduce reliability and increase develop\u00adment \ntime. The standard task graph model dictates that tasks with potential con.icting accesses to shared \nstate must be linked by a dependency, even if there is no explicit logical ordering on their execution. \nIn cases where it is dif.cult to understand if such im\u00adplicit dependencies exist, the programmer often \ncreates more de\u00adpendencies than needed, which results in constrained graphs with large monolithic tasks \nand limited parallelism. We propose a new technique, Synchronization via Scheduling (SvS), that uses \nthe results of static and dynamic code analysis to manage potential shared state con.icts by exposing \nthe data accesses of each task to the scheduler. We present an in-depth performance analysisofSvSviaexamples \nfrom videogames, our target domain, and show that SvS performs well in comparison to software transactional \nmemory (TM) and .ne grained mutexes. Categories and Subject Descriptors D.1.3 [Concurrent Pro-gramming]:Parallel \nprogramming; D.3.4[Processors]: Compil\u00aders; D.3.4[Processors]: Run-time environments General Terms Design, \nLanguages, Measurement, Performance, Reliability Keywords parallel programming, shared state management, \nSyn\u00adchronization via Scheduling, Dynamic Reachability Analysis 1. Introduction Shared state access con.icts \nare the cause of majority of errors in parallel programming. Race conditions and corruption of shared \ndata are common. Thesebugs can be notoriously dif.cult to track down as theyoften manifest rarely, depending \non the state of not * Partially supported by the Natural Sciences and Engineering Research Council of \nCanada (NSERC) Permission to make digital or hard copies of all or part of this work for personal or \nclassroom use is granted without fee provided that copies are not made or distributed for pro.t or commercial \nadvantage and that copies bear this notice and the full citation on the .rst page.To copyotherwise, to \nrepublish, to post on servers or to redistribute to lists, requires prior speci.c permission and/or a \nfee. PLDI 11, June 4 8, 2011, San Jose, California, USA. Copyright c &#38;#169; 2011ACM 978-1-4503-0663-8/11/06... \n$10.00 justone,butseveral threadsofexecution. Unfortunatelymostexist\u00ading frameworks don tprovide mechanisms \nto automatically protect shared state. Runtime systems such as the one driving Cilk [12], OpenMP [4] \nand the venerable pthreads are largely concerned with dispatching code for execution. Systems such as \nIntel s TBB [6] do provide a vast array of synchronization primitives, including a number of distinct \ntypes of mutexes, for the programmer to con\u00adstruct their own state protection. Unfortunately, theycontain \nvery little in the way of support for orchestrating these schemes. Soft\u00adwareTransactional Memory(STM) \n[25] does address this problem and as STM is the closest in effect to our proposed technique, we will \ndiscuss it further in the context of our experiments in Section 3. Very few domains have been as profoundly \naffected by the multicore revolution as video games. The complexity and high level of interaction of \nthe systems which often rivals operating systems, a nearly inexhaustible demand for better performance, \nlarge programming teams and tight deadlines makes it an ideal testbed for parallel techniques. The volume \nof this software and its commercial appeal would make the problems worth solving even if they were unique \nto the domain, but any technique developed have applicationin manyother domains.For these reasonswehave \nchosen to let the needs of this domain drive our research. Our assumptions and choice of experiments \nre.ect this. Programmers faced with the dif.culties in managing the data accesses of a large collection \nof tasks tend to leave many tasks monolithic, comprised sometimes of thousands of lines of codes. The \nembarrassingly parallelkernels, those without complex state interaction, will generally be dispatched \nin patterns similar to parallel for. This has lead to the structure of the current gen\u00aderationof videogames. \nOften theworkof an entire subsystem or one its major components will be assigned to a single processing \ncontext, co-scheduled with other components that are guaranteed to be con.ict free. Interspersed betweentheexecutionof \nthese groups will be the execution of the embarrassingly parallel sections and a number of explicit synchronization \npoints. This structure is com\u00admon in the industry [1]. The lack of parallel width in manyphases of execution \nleaves resources idle and this approach will not scale as the number of cores increase. The parallel \nstructure of the program is often represented in the standard task graph model where tasks that have \nnot had a dependencydeclared between them can be scheduled concurrently. There are cases where ensuring \nordering of tasks with dependencies is necessary for correctness. However, there remain many cases where \nthere is not an explicit logical ordering between the tasks and a dependency is declared to prevent two \ntasks that touch the same data from running concurrently. This serializing of tasks is necessary even \nif the two tasks touch the same data rarely.  Whenaprogramisexecutedandtasksare serialized unnecessar\u00adily \nbecause of unneeded dependencies, parallelism is reduced and performance can suffer. This work proposes \na mechanism to cor\u00adrect this de.ciency. Our model only requires that the programmer state explicit inter-task \ndependencies, i.e., those that are required by the program logic. In cases where tasks that are not explicitly \ndependent on each other may touch the same data, our system auto\u00admatically inserts a new implicit dependencybetween \nthem, which will prevent these tasks from running concurrently and corrupting shared state.Weuseconventional \nstatic analysistodeterminewhen implicit dependencies are necessary. During runtime, when we can determine \nmore precisely what data the task will actually access, we detect and remove any overly constraining \nimplicit dependen\u00adcies to accomplish this we propose new dynamic analysis tech\u00adniques. Making the shared \nstate access patterns of each applicable task available to the scheduler we are able to safely schedule \ntasks with potential con.icts.We call this technique Synchronization via Scheduling(SvS). ThegeneralconceptofSvSisnotoverly \ncomplicated.Thechief dif.culty lies in utilizing the information provided by the static analysis and \nre.nement without adding muchoverhead.To achieve the highly desirable performance of 60 frame-per-second \n(FPS), a frame must be constructed in just over 16ms. For optimal paral\u00adlelization manytasks will have \nruntimes in the tens or hundreds of microseconds. This strict timebudget does not allow for muchex\u00adtra \ncomputation. One of the major contributions of this work is the description of algorithms to achieve \nthis high speed organization and to demonstrate that theywork in practical contexts. SvS differs from \noptimistic techniques such as STM in that the work of evaluating the admissibility of a task is done \nprior to its execution. Though some of the mechanisms used to realize SvS are similar to those in some \nSTM implementations this difference means that there are no expensive rollbacks and there is a much smaller \nrequirement forextra bookkeeping during state access. Ad\u00additionally, STM requires programmers to de.ne \natomic transac\u00adtions while SvS is an automatic technique completely handled by the compiler and runtime \ncomponents.We will showa comparison between SvS and STM in section 3. The rest of the paper is organized \nas follows. In Section2 we discuss the model, algorithms and implementation of SvS. Experi\u00admental results, \ndrawn from several applications in our domain, are presentedin Section3.In section4 we will discuss relatedwork \nandin Section5 we will conclude andgivea short discussionof our future work. 2. SvS Model and Implementation \n2.1 Motivation and Overview Task graph models areastandard pattern for structuring parallelism in programs \n[20].Aprevailing problem with this modelis the lack of automatic shared state management between tasks. \nConsider an exampleofskeletal character animation.Typically,multiple anima\u00adtions are applied to the bones \nof a single character to produce real\u00adistic looking motion [2].Forexample, to producea character that \niswalkingand limpingwemay blendthe walking and limping animations.The mathematical operations performedby \nthese two routines are commutative, so there is no explicit ordering between them. However, it is unsafe \nto execute these animation routines in concurrent tasks, because they may touch the same bones of the \nsame character. In this case there exists a special type of depen\u00addencybetween tasks, which we term implicit. \nAn implicit depen\u00addency exists when there is no logical ordering between the tasks imposedby data or \ncontrol dependencies,but the tasks may access the same shared state and so it is unsafe to run them concurrently. \nWithout automatic shared state management, programmers must manage shared stateby insertingexplicit dependencies \nwhere implicit dependencies exist. Protecting shared state via explicit de\u00adpendencies has two problems. \nFirst, this is prone to programmer error, especially when considering dynamic, pointer-based mem\u00adory \naccesses. Second, this unnecessarily constrains parallelism in cases where the tasks may incur con.icting \naccesses of the shared state,butdo not actually perform them at runtime. We address the issue of shared \nstate management in task graph models by introducing a new technique called Synchronization via Scheduling \n(SvS). SvS provides automatic shared state manage\u00admentby combining staticand dynamic analysisto determineiftwo \ntasks can potentially access shared state. The result of static anal\u00adysis is a task graph with dependencies \nthat guarantee the protec\u00adtion of shared state. Dynamic analysis then utilizes run-time infor\u00admation \nto potentially remove unnecessary dependencies between tasks, allowing for increased parallelism. In \nthis way, SvS deter\u00admines the set of possible memory accesses a task makes before it isexecutedand schedules \ntaskssuchthatnotwo tasks concurrently access the same memory. In this section, we will provide the de\u00adtails \nbehind the model and implementation of SvS. In section 2.2 we outlinethe frameworkforSvS, followedbyabrief \ndiscussionof relevant background informationinsections2.3and2.4. Startingin section 2.5, we will go into \ndetail on the model and implementation of SvS.  2.2 Framework Figure1 shows the four main components \nthat comprise the SvS framework: an SvS compatible language, static analysis, dynamic analysis, and the \ntask scheduler. An SvS compatible language al\u00adlows a programmer to group blocks of code into units called \ntasks. Programmers canprovidea logical ordering between tasksbutdo not need to manage shared state between \nthem. Beyond providing a task-graph abstraction, an SvS compatible language must be type safe and disallow \npointer arithmetic.We describe our prototypical implementation of an SvS compatible language in section \n2.4. At compile time, a program written in an SvS compatible lan\u00adguageis passed througha static analysis \nphase that generates infor\u00admation pertaining to symbols (linguistic abstractions for memory accesses) \nand task dependencies. This information de.nes a static task graph which provides an initial scheduling \nof tasks that ensures the protection of shared state. Information from static analysis is stored for \nuse during dynamic analysis. We purposely visualizethe static analysisin .gure1asa black box because \nSvS is indifferent to the implementation of static analysis. Static analysis techniques that produce \na correct list of task dependencies and a list of all symbols that a task may access are suitable for \nuse within the SvS framework. Static analysis in SvS is formally de.ned as task dependency analysis in \nsection 2.5; this section also explains how task dependencyanalysis can be im\u00adplemented usingexisting \ntechniques. The classic limitingfactorof static analysis when applied to parallelization is that it is \nlimited to compile time information. As a result, it often is forced to create dependencies that are \npotentially unnecessary, thus restricting par\u00adallelism. This problem can be alleviated using dynamic \nanalysis. ThroughouttheexecutionofanSvS program, dynamic analysis maintains dynamic reachability information \n(i.e. potential memory accesses) for symbols accessed by tasks. As tasks are considered for scheduling, \nthis information is used to generate and compare read/write sets of tasks in order to remove anyimplicit \ndependen\u00adcies that were deemed necessary by the static analysis, but were found to be non-existent when \ndynamic reachability information\n\t\t\t", "proc_id": "1993498", "abstract": "<p>Shared state access conflicts are one of the greatest sources of error for fine grained parallelism in any domain. Notoriously hard to debug, these conflicts reduce reliability and increase development time. The standard task graph model dictates that tasks with potential conflicting accesses to shared state must be linked by a dependency, even if there is no explicit logical ordering on their execution. In cases where it is difficult to understand if such <i>implicit</i> dependencies exist, the programmer often creates more dependencies than needed, which results in constrained graphs with large monolithic tasks and limited parallelism.</p> <p>We propose a new technique, Synchronization via Scheduling (SvS), that uses the results of static and dynamic code analysis to manage potential shared state conflicts by exposing the data accesses of each task to the scheduler. We present an in-depth performance analysis of SvS via examples from video games, our target domain, and show that SvS performs well in comparison to software transactional memory (TM) and fine grained mutexes.</p>", "authors": [{"name": "Micah J. Best", "author_profile_id": "81442594977", "affiliation": "University of British Columbia, Vancouver, Canada", "person_id": "P2690674", "email_address": "mjbest@cs.ubc.ca", "orcid_id": ""}, {"name": "Shane Mottishaw", "author_profile_id": "81442619824", "affiliation": "Simon Fraser University, Vancouver, Canada", "person_id": "P2690675", "email_address": "smottish@cs.sfu.ca", "orcid_id": ""}, {"name": "Craig Mustard", "author_profile_id": "81442615535", "affiliation": "Simon Fraser University, Vancouver, Canada", "person_id": "P2690676", "email_address": "cam14@cs.sfu.ca", "orcid_id": ""}, {"name": "Mark Roth", "author_profile_id": "81328490063", "affiliation": "Simon Fraser University, Vancouver, Canada", "person_id": "P2690677", "email_address": "mroth@cs.sfu.ca", "orcid_id": ""}, {"name": "Alexandra Fedorova", "author_profile_id": "81100440291", "affiliation": "Simon Fraser University, Vancouver, Canada", "person_id": "P2690678", "email_address": "fedorova@cs.sfu.ca", "orcid_id": ""}, {"name": "Andrew Brownsword", "author_profile_id": "81442594727", "affiliation": "Electronic Arts, Inc, Vancouver, Canada", "person_id": "P2690679", "email_address": "brownsword@ea.com", "orcid_id": ""}], "doi_number": "10.1145/1993498.1993573", "year": "2011", "article_id": "1993573", "conference": "PLDI", "title": "Synchronization via scheduling: techniques for efficiently managing shared state", "url": "http://dl.acm.org/citation.cfm?id=1993573"}