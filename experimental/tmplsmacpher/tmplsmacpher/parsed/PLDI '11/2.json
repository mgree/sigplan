{"article_publication_date": "06-04-2011", "fulltext": "\n Parallelism Orchestration using DoPE: the Degree of Parallelism Executive Arun Raman Hanjun Kim Taewook \nOh Jae W. Lee David I. August Princeton University Parakinetics Inc. Princeton, NJ Princeton, NJ {rarun, \nhanjunk, twoh, august}@princeton.edu leejw@parakinetics.com Abstract In writing parallel programs, programmers \nexpose parallelism and optimize it to meet a particular performance goal on a single plat\u00adform under \nan assumed set of workload characteristics. In the .eld, changing workload characteristics, new parallel \nplatforms, and deployments with different performance goals make the pro\u00adgrammer s development-time choices \nsuboptimal. To address this problem, this paper presents the Degree of Parallelism Executive (DoPE), \nan API and run-time system that separates the concern of exposing parallelism from that of optimizing \nit. Using the DoPE API, the application developer expresses parallelism options. Dur\u00ading program execution, \nDoPE s run-time system uses this informa\u00adtion to dynamically optimize the parallelism options in response \nto the facts on the ground. We easily port several emerging parallel applications to DoPE s API and demonstrate \nthe DoPE run-time system s effectiveness in dynamically optimizing the parallelism for a variety of performance \ngoals. Categories and Subject Descriptors D.1.3 [Programming Tech\u00adniques]: Concurrent Programming Parallel \nProgramming; D.3.4 [Programming Languages]: Processors Run-time environments GeneralTerms Design, Languages, \nPerformance Keywords parallelization, parallelism, dynamic, run-time, schedul\u00ading, task, loop-level, \nnested, loop nest, pipeline, parametric, opti\u00admization 1. Introduction As multicore processors become \nubiquitous, application develop\u00aders and compilers must extract thread level parallelism (TLP) in order \nto exploit the execution resources afforded by the hardware. Parallelism of multiple types may exist \nin an application, such as task parallelism, data parallelism, and pipeline parallelism. Much progress \nhas been made in methodologies and systems to extract parallelism, even from seemingly sequential code \n[6, 7, 24, 25, 34, 40]. Tools such as POSIX threads (Pthreads) [33], Intel Thread- Permission to make \ndigital or hard copies of all or part of this work for personal or classroom use is granted without fee \nprovided that copies are not made or distributed for pro.t or commercial advantage and that copies bear \nthis notice and the full citation on the .rst page. To copy otherwise, to republish, to post on servers \nor to redistribute to lists, requires prior speci.c permission and/or a fee. PLDI 11, June 4 8, 2011, \nSan Jose, California, USA. Copyright c &#38;#169; 2011 ACM 978-1-4503-0663-8/11/06. . . $10.00. ing Building \nBlocks (TBB) [26], Cilk [5], OpenMP [22], and Ga\u00adlois [14] allow application developers to express TLP. \nMany applications have parallelism in multiple loops in a loop nest. Each loop may be parallelized by \nexploiting different types of parallelism to varying extents by allocating a different number of parallel \nresources (hardware threads) to each loop. The type and extent of each loop parallelization is called \nthe degree of paral\u00adlelism (DoP). Simultaneously parallelizing multiple loops can pro\u00advide both scalability \nand latency-throughput bene.ts. Unfortunately, determining the right degree of parallelism for even a \nsingle loop, let alone multiple loops in a nest, is a compli\u00adcated task. Application developers or parallel \nrun-times typically .x the degree of parallelism of each loop statically at development\u00adtime or run-time. \nThis is often suboptimal in the face of several run\u00adtime sources of performance variability that could \npotentially result in leaving hardware resources idle or over-subscribed. The applica\u00adtion workload characteristics \nmay vary as in the case of web ser\u00advices such as search and video. The parallel platform characteris\u00adtics \n(number of cores, memory bandwidth, etc.) may vary [18, 30]. Furthermore, the performance goals may not \nbe .xed and could be some complex time-varying functions of energy, throughput, etc. Together, these \nthree sources of variability workload characteris\u00adtics, platform characteristics, and performance goals \nare referred to as the execution environment of an application. To solve the above problem, the application \ndeveloper could statically produce multiple versions of code and dynamically se\u00adlect a version that best \n.ts each execution environment. Unfortu\u00adnately, the number of scenarios resulting from a myriad of plat\u00adforms, \nworkloads, and performance goals is so large as to preclude the possibility of incorporating all of them \ninto statically-compiled codes. To address the limitations of the static approach, run-time systems have \nbeen proposed to map parallel applications to their execution environments. Low-level substrates enable \nparallel li\u00adbrary composition but do not leverage application-speci.c run-time information [15, 23]. \nPrior work that performs adaptation by mon\u00aditoring application features has typically focused on a speci.c \ntype of parallelism (such as pipeline parallelism or task parallelism) and .xed dynamic adaptation mechanisms \nthat are tightly coupled to the target parallelism type [4, 5, 26, 29, 30, 35, 38]. More impor\u00adtantly, \nin all known prior work, the adaptation mechanisms are re\u00adstricted to a single loop in a loop nest. This \npaper proposes DoPE, a novel API and run-time system that enables the separation of the concern of developing \na func\u00adtionally correct parallel program from the concern of optimizing the parallelism for different \nexecution environments. The separa\u00adtion of concerns enables a mechanism developer to specify mech\u00adanisms \nthat encode the logic to adapt an application s parallelism  Library Multiple Performance Goals Parallelism \nin Loop Nest Multiple Parallelism Types Application Feature Monitoring Multiple Optimization Mechanisms \nPthreads [33] \u00d7 . . \u00d7 \u00d7 Intel TBB [26] \u00d7 \u00d7 . \u00d7 \u00d7 FDP [29] \u00d7 \u00d7 \u00d7 . \u00d7 DoPE [This paper] . . . . . Table1. \nComparison of various software-only parallelization libraries for general-purpose applications con.guration \nto meet the performance goals that are set by the ad\u00administrator. Table 1 highlights DoPE s advantages \nover other paral\u00adlelism management libraries. With DoPE, the application developer can expose all the \nparallelism in an application and express it in a uni.ed manner just once. Then, a run-time system adapts \nthe ap\u00adplication s parallelism con.guration by monitoring key application features and responding to \nchanges in the application s execution environment. Speci.cally, the run-time system automatically and \ncontinuously determines: which tasks to execute in parallel (e.g. what are the stages of a pipeline) \n how many hardware threads to use (e.g. how many threads to allocate to each stage of the pipeline) \n how to schedule tasks on to hardware threads (e.g. on which hardware thread should each stage be placed \nto maximize lo\u00adcality of communication)  We ported several emerging parallel applications to use the \nDoPE interface. Different performance goals included response time minimization, throughput maximization, \nand throughput max\u00adimization under power constraint. DoPE automatically adapted the application to meet \nthe goals, without necessitating a change in the application code by the developer. To adapt the parallelism, \nit used new mechanisms proposed in this paper and also mechanisms proposed in prior work [29, 38], demonstrating \nthe robustness of DoPE s interface and the ability for a mechanism developer to im\u00adplement better mechanisms \nin the future in a non-disruptive way. On a 24-core Intel Xeon machine, DoPE improved the re\u00adsponse time \ncharacteristics of four web service type applications to dominate the characteristics of the best static \nparallelizations. The throughputs of two batch-oriented applications were improved by 136% (geomean) \nover their original implementations. For one ap\u00adplication (an image search engine), three different goals \ninvolving response time, throughput, and power were independently speci\u00ad.ed. DoPE automatically determined \na stable and well performing parallelism con.guration operating point in all cases. The primary contributions \nof this work are: An API that separates the concern of correct speci.cation of an application s parallelism \nfrom the concern of optimization of the application s execution in a variety of environments  A smart \nrun-time system that enables the interface and monitors application execution to dynamically adapt the \nparallelism in program loop nests by means of suitable mechanisms in order to meet speci.ed performance \ngoals  The rest of this paper is organized as follows. The need for dynamic adaptation and separation \nof concerns is .rst motivated, followed by a description of DoPE s interfaces for the application developer, \nmechanism developer, and administrator. Various mech\u00adanisms that were implemented are described, followed \nby an eval\u00aduation of the DoPE system and a discussion of related work. 2. Motivation A parallel application \ns execution environment consists of the ap\u00adplication workload characteristics, platform characteristics, \nand performance goals. Variability in any of these parameters can ne\u00adcessitate dynamic adaptation of \nparallelism in order to meet the speci.ed performance goal. The following video transcoding ex\u00adample \nconcretely demonstrates the impact of workload character\u00adistic variability. Example: Video Transcoding \nVideo sharing websites such as YouTube, Google Video, and Dailymotion transcode user submit\u00adted videos \non their servers. Figure 1 shows the parallelism in video transcoding using x264, an implementation of \nthe popular H.264 standard [39]. Each video may be transcoded in parallel with oth\u00aders. Furthermore, \na single video may itself be transcoded in par\u00adallel by exploiting parallelism across the frames in the \nvideo in a pipelined fashion. <DoPouter, DoPinner > represents the type of parallelism of, and number \nof threads assigned to, the outer (inter\u00advideo) and inner (intra-video) loops in the loop nest. Examples \nof types of parallelism are DOALL and pipeline (PIPE) [1]. Statically .xing the DoP assigned to each \nloop may not be optimal for a given performance goal in all execution environments. To demonstrate this, \nwe measured throughput and execution time on a 24-core ma\u00adchine with Intel Xeon X7460 processors. User \nrequests were simu\u00adlated by a task queueing thread with arrivals distributed according to a Poisson distribution. \nThe average system load factor is de.ned as the average arrival rate of tasks (videos to be transcoded) \ndivided by the maximum throughput sustainable by the system. Figure 2(a) shows that exploiting intra-video \nparallelism pro\u00advides much lower per-video transcoding execution time than when only the outer loop is \nparallelized. Texec is improved up to a maxi\u00admum of 6.3\u00d7 on the evaluation platform. This speedup is \nachieved when 8 threads are used to transcode each video. Figure 2(b), how\u00adever, shows the dependency \nof throughput on the application load. At heavy load (load factor 0.9 and above), turning on intra-video \nparallelism actually degrades throughput. This is due to the inef\u00ad.ciency of parallel execution (a speedup \nof only about 6\u00d7 on 8 threads at load factor 1.0) caused by overheads such as thread cre\u00adation, communication, \nand synchronization. This experiment shows that the usual static choices of paral\u00adlelism con.guration \nare not ideal across all load factors for both execution time and throughput. In other words, there is \na trade\u00adoff between the two. This tradeoff impacts end user response time which is the primary performance \nmetric of service oriented ap\u00adplications. Equation 1 is helpful to understand the impact of the execution \ntime/throughput tradeoff on response time. The time to transcode a video is the execution time, Texec. \nThe number of videos transcoded per second is the throughput of the system, Figure1. Two-level loop \nnest in video transcoding: Across videos submitted for transcoding and across frames within each video \n 501 400.8 300.6 200.4   Execution Time (secs) 100.2 0 Normalized load on system Normalized load \non system Normalized load on system (a) Execution Time(b) Throughput(c) Response Time Figure2. Variation \nof (a) execution time and (b) throughput with load factor and parallelism con.guration in a video transcoding \napplication on a 24-core Intel Xeon machine; (c) impact of throughput and execution time on end user \nresponse time; an oracle achieves the best response time characteristic by continuously varying DoP with \nload (ideal parallelism con.guration for each load factor is shown) Throughput. The number of outstanding \nrequests in the system s work queue is the instantaneous load on the system, q(t). q(t) Tresponse (t)= \nTexec(DoP)+ (1) Throughput(DoP) The response time of a user request, Tresponse, is the time in\u00adterval \nfrom the instant the video was submitted for transcoding (at time t) to the instant the transcoded video \nis output. Tresponse has two components: wait time in the work queue until the re\u00adquest reaches the head \nof the queue, and execution time, Texec. At light to moderate load, the average arrival rate is lower \nthan the system throughput. Consequently, the wait time will tend to zero, and Tresponse will be determined \nby Texec. Assuming reasonably ef.cient intra-video parallelism, increasing the DoP extent of the inner \nloop reduces Texec and in turn Tresponse. In other words, in this region of operation, <DoPouter, DoPinner \n> must be op\u00adtimized for execution time (DoPinner = (8, PIPE)). At heavy load, Tresponse is dominated \nby the wait time in the work queue which is determined by the system throughput. In this region of op\u00aderation, \nDoPinner must be set to a value that optimizes throughput (DoPinner = (1, SEQ)). Figure 2(c) presents \nexperimental valida\u00adtion of the described response time characteristic. The same .gure also shows that \na mere turn inner parallelism on/off approach is suboptimal; an oracle that can predict load and change \nDoP contin\u00aduously achieves signi.cantly better response time. In addition to workload characteristics, \nplatform characteris\u00adtics including number of hardware contexts, memory space, etc. may vary. Further, \nthe same system (application+platform) may be called upon to maximize system utility with a variety of \nper\u00adformance goals involving energy, throughput, etc. If the applica\u00adtion developer were tasked with \nmatching application code to the variety of dynamic execution environments that might arise, there would \nbe a combinatorial explosion in the number of versions of application code. Each version would remain \nad hoc as the appli\u00adcation is deployed on newer platforms and is used in the context of different performance \ngoals. Separation of Concerns To address the explosion of developer effort and code versions, the task \nof expressing application paral\u00adlelism must be separated from the task of adapting that parallelism to \nspeci.c execution environments. Further, an administrator must be able to specify the current performance \ngoal, and the application must adapt itself to meet the goal. Such a separation of concerns would enable: \n application developers to focus on functional correctness of the parallel application  administrators \nto specify arbitrary performance goals involving performance, power, etc.  mechanism developers to implement \nparallelism adaptation mechanisms that meet the speci.ed performance goals Table 1 evaluates existing \nchoices for enabling such a separation of concerns. Using Pthreads, a developer speci.es a concrete, \nun\u00adchanging parallelism con.guration, or codes an ad hoc adaptation mechanism for every new execution \nenvironment. Intel TBB [26] and similar libraries [5, 22] support task parallelism for indepen\u00addent tasks \nand their schedulers optimize only for throughput. Feed\u00adback Directed Pipelining (FDP) implements an \nadaptation mecha\u00adnism tied to throughput maximization for a single loop in the appli\u00adcation [29]. In \nsummary, these libraries support only a single per\u00adformance goal, and closely couple the goal with a \nspeci.c mecha\u00adnism to adapt parallelism in order to meet the goal. DoPE enables an application developer \nto express common paradigms of nested parallelism in a uni.ed fashion. DoPE en\u00adables an administrator \nto specify different performance goals for the same application. DoPE enables a mechanism developer to \nim\u00adplement multiple mechanisms that recon.gure application paral\u00adlelism to meet a speci.ed performance \ngoal. The application devel\u00adoper needs to write the application just once, and the application executes \nrobustly across multiple scenarios of use, platforms, and workloads.  3. DoPEfortheApplicationDeveloper \n 3.1 DoPEAPI DoPE presents a task-oriented interface to the application devel\u00adoper. A task consists of \na template function that abstracts the con\u00adtrol for creating dynamic instances of each task, function \nobjects (functors) that encapsulate the task s functionality and expose ap\u00adplication level information, \nand a descriptor that describes the par\u00adallelism structure of the task. Figure 3 de.nes the Task type \nand the types from which it is composed. 1 Task = {control: TaskExecutor, function: Functor, 2 load: \nLoadCB, desc: TaskDescriptor, 3 init: InitCB, .ni: FiniCB} 4 TaskDescriptor = {type: TaskType, pd: ParDescriptor[]} \n5 TaskType = SEQ |PAR 6 ParDescriptor = {tasks: Task[]} 7 TaskStatus = EXECUTING |SUSPENDED |FINISHED \n Figure3. DoPE type de.nitions TaskExecutor DoPE provides the control .ow abstraction shown in Figure \n4(a). Loop exit is determined by status (line 7 in Figure 4(a)). The abstraction is templated on the \nFunctor type that encapsulates a task s functionality.  TaskStatus Task::begin() Signal DoPE that the \nCPU intensive part of the task has begun; DoPE returns task status TaskStatus Task::end() Signal DoPE \nthat the CPU intensive part of the task has ended; DoPE returns task status TaskStatus Task::wait() Wait \nuntil child tasks complete; DoPE returns status of master child task DoPE* DoPE::create(ParDescriptor* \npd) Launch parallel application described by speci.ed parallelism descriptor un\u00adder the DoPE run-time \nsystem void DoPE::destroy(DoPE* dope) Finalize and destroy the DoPE run-time system; wait for registered \ntasks to end Table2. DoPE API 1 template<Functor> 1 class Functor{ 2 void TaskExecutor(Functor 2 ... \n//Capturelocalvariables 3 Function){ 3 4 ... 4 ... //Constructor 5 while(true) { 5 6 ... 6 TaskStatus \noperator()(){ 7 TaskStatus status = 7 ... //Taskfunction body 8 Function(); 8 return taskstatus; 9 ... \n9 }  10 } 10 }; 11 } 11 (a) Control .ow abstraction (b) Functor for task functionality Figure4. Separation \nof task s control and functionality in DoPE Functor The developer must implement a functor that encap\u00adsulates \nthe desired functionality of a task. The functor binds the local variables of the original method containing \nthe parallelized loop as member .elds (line 2 in Figure 4(b)). At run-time, a task could be either executing, \nsuspended, or .nished. The functor must return the status of the task after each instance of the task \n(line 8 in Figure 4(b)). In particular, when a loop exit branch is to be taken, the functor must return \nFINISHED; otherwise, the functor must re\u00adturn EXECUTING. Combined with the control .ow abstraction in \nFigure 4(a), the control .ow structure of the original loop is dupli\u00adcated. The functor can also return \nSUSPENDED its discussion is deferred until Section 3.2. LoadCB Section 2 described the importance of \napplication fea\u00adtures such as workload to determine the optimal parallelism con.g\u00aduration for a given \nperformance goal. To capture the workload on each task, the developer implements a callback functor that \nwhen invoked returns the current load on the task. InitCB and FiniCB To restart parallel execution from \na glob\u00adally consistent program state after DoPE recon.gures parallelism, 1 class TranscodeFunctor{ 1 \nclass TranscodeLoadCB{ 2 //Capture localvariables 2 //Capture localvariables 3 Queue*&#38; inq; 3 Queue*&#38; \ninq; 4 Queue*&#38; outq; 4 Queue*&#38; outq; 5 ... //Constructor 5 ... //Constructor 6 TaskStatus operator()(){ \n6 .oat operator()(){ 7 Video* input, *output; 7 //Return occupancy 8 *input =inq.deque(); 8 return inq.size(); \n9 output = transcode(input); 9 } 10 outq.enqueue(*output); 10 }; 11 return EXECUTING; 11 12 } 12 13 }; \n13 (a) Functionality (b) Workload DoPE requires the programmer to implement the InitCB (Fini-CB) functor \nthat is invoked exactly once before (after) the task is executed. TaskDescriptor A task can be sequential \n(SEQ) or parallel (PAR). A parallel task s functionality can be executed by one or more threads. In other \nwords, the Functor() method (lines 6 9 in Figure 4(b)) can be invoked concurrently by multiple threads. \nTo enable description of nested parallelism, a task can specify one or more parallelism descriptors (ParDescriptor). \nSpecifying more than one descriptor exposes a choice to DoPE which at run\u00adtime chooses the optimal parallelism \ncon.guration (described by the corresponding ParDescriptor). ParDescriptor A parallelism descriptor is \nde.ned recur\u00adsively in terms of Tasks. A ParDescriptor is an array of one or more tasks that execute \nin parallel and potentially interact with each other (line 6 in Figure 3). 1 void Transcode(){ 2 Q* inq, \noutq; 3 Video* input, *output; 4 while(true){ 5 *input = inq.deque(); 6 output = transcode(input); 7 \noutq.enqueue(*output); 8 } 9 } Figure5. Outer loop in x264 video transcoding Puttingitalltogether Figure \n5 shows the outer loop code in x264 video transcoding. Figure 6 shows the transformation of the loop \nby instantiation of the DoPE types discussed above. In Figure 6(a), du\u00adplicated code from the original \nloop in Figure 5 is shown in bold. Referring to Figure 1, the outer loop task can itself be executed \n 1 TaskDescriptor 1 void Transcode(){ 2 *readTD(SEQ, NULL), 2 Queue* inq, *outq; 3 *transformTD(PAR, \nNULL), 3 Task* task 4 *writeTD(SEQ, NULL); 4 (TranscodeFunctor(inq, outq), 5 ...//Create tasks 5 TranscodeLoadCB(inq, \noutq), 6 //using descriptors 6 outerTD); 7 ParDescriptor 7 //TaskExecutor<OuterLoopFunctor> 8 *innerPD({readTask, \n8 //is usedautomatically by DoPE 9 transformTask, 9 } 10 writeTask}); 10 11 TaskDescriptor 11 12 *outerTD(PAR, \n{innerPD}); 12 13 13 (c) Descriptor (d) Task Figure6. Task de.nition using DoPE in a pipeline parallel \nfashion. Figure 6(c) shows the de.nition of the outer loop task descriptor in terms of the inner loop \nparal\u00adlelism descriptor. Note that the process of de.ning the functors is mechanical it can be simpli.ed \nwith compiler support. 3.2 Using theAPI:AVideoTranscodingExample A developer uses the types in Figure \n3 and associated methods in Table 2 to enhance a parallel application using DoPE. Figure 7 describes \nthe port of a Pthreads based parallelization (column 1) of the video transcoding example from before \nto the DoPE API (column 2). Code that is common between the Pthreads version and the DoPE version is \nshown in bold. Step 1: Parallelism Description In the Pthreads parallelization, lines 4 7 create NUM \nOUTER threads that execute the Transcode method. In the Transcode method, a thread dequeues work items \n(videos) from the work queue (line 14), transcodes them (lines 15 25), and enqueues the transcoded items \nto the output queue (line 26). Each video transcoding can itself be done in par\u00adallel in a pipelined \nfashion. For this, the Transcode method spawns NUM INNER threads to execute the pipeline. One thread \neach executes Read and Write, and one or more threads execute Transform. A common practice is to set \nboth NUM OUTER and NUM INNER statically based on pro.le information [21]. Section 2 already presented \nthe shortcomings of this approach to operate optimally, an application must dynamically change its parallelism \ncon.guration as the execution environment changes. In the DoPE parallelization, the application s parallelism \nis de\u00adscribed in a modular and bottom-up fashion. Line 4 gets the task de.nition of the outer loop by \ninvoking Transcode getTask. To encode nested parallelism, the Transcode getTask method speci.es that \nTranscode can be executed in parallel using the parallelism descriptor pd (lines 12 17 in Transcode getTask). \n    Line 5 in transcodeVideos creates a parallelism descriptor for the outer loop. Step2:ParallelismRegistration \nLine 6 in transcodeVideos registers the parallelism descriptor for execution by DoPE by in\u00advoking DoPE::create. \nLine 7 waits for the parallel phase of the application to .nish before freeing up execution resources \nby in\u00advoking DoPE::destroy. Step 3: Application Monitoring Each task marks the begin and end of its CPU \nintensive section by invoking Task::begin and Task::end, respectively. DoPE records application features \nsuch as task execution time in between invocations of these methods. To monitor per-task workload, the \ndeveloper implements LoadCB for each task to indicate the current workload on the task. The callback \nreturns the current occupancy of the work queue in the case of the outer task (line 26), and the input \nqueue occupancies in the case of Transform (line 62) and Write (line 75). The callbacks are registered \nduring task creation time. Step 4: Task Execution Control If a task returns EXECUTING, DoPE continues \nthe execution of the loop. If a task returns FINISH-ED, DoPE waits for other tasks that are at the same \nlevel in the loop nest to also return FINISHED. A task can explicitly wait on its children by invoking \nTask::wait. Exactly one task in each par\u00adallelized loop is assigned the role of the master task (the \n.rst task in the array of tasks registered in the parallelism descriptor). In the running example, the \ntask corresponding to Transcode is the master task for the outer loop and the task corresponding to Read \nis the master task for the inner loop. Invoking Task::wait on task (line 17) returns the status of the \nmaster child task. Step5:TaskYieldingforRecon.guration By default, DoPE re\u00adturns EXECUTING when either \nTask::begin or Task::end (a) Parallelization using POSIX threads (b) Parallelization using DoPE (1) \nRun-time initialization 1 #inelude <ptlread.l> 1 #inelude <dope> 2 void transeodeVideos() { 2 void transeodeVideos() \n{ 3  Q* inq, *outq; 3  Q\" inq, \"outq; 4  ptlread t tlreads[NUM OUTER]; 4  Task* outerTask . Transeode \ngetTask(inq, outq); 5  for (i . 0 ; i < NUM OUTER ; i++) { 5  ParDeseriptor* outerPD . new ParDeseriptor({outerTask}); \n 6   ptlread ereate(tlreads[i], attr, Transeode, 6  DoPE* dope . DoPE::ereate(outerPD); 7    \n       new ArgT(inq, outq)); 7  DoPE::destroy(dope); IIfWaitfforftasksftoffinish 8  } 8 } 9 \n} 9 (2) Transeoding of an individual video elip 10 void* Transeode(void* arg) { 10 elass TranseodeFunetor \n{ 10 Task* Transeode getTask(Q* inq, Q* outq) { 11  Q* inq . (ArgT*)arg->inq; 11  Task* task; IIJhisffunctor'sftask \n11  TranseodeFunetor* fune . new TranseodeFunetor(inq,outq); 12  Q* outq . (ArgT*)arg->outq; 12  ... \nIICaptureflocalfvariables 12  ParDeseriptor* pd . new ParDeseriptor 13  for(;;) { 13  ... IIConstructor \n13   ({Read getTask(fune->q1), 14   *input . inq->dequeue(); 14  TaskStatus operator()() { 14  \n  Transform getTask(fune->q1, fune->q2), 15   . IIinitializefqlfandfq2 15   \"input = inq->dequeue(); \n15   Write getTask(fune->q2)}); 16   ptlread t tlreads[NUM INNER]; 16   . IIInitialize ql and \nq2 16  IIfNotefhierarchicalfdescriptionfoffparallelismf 17   ptlread ereate(tlreads[0], attr, Read, \n17   status . task->wait(); 17  TaskDeseriptor* td . new TaskDeseriptor(PAR, {pd}); 18      \n    new ArgR(input, q1)); 18   if (status .. SUSPENDED) 18  Task* task . new Task(fune, new TranseodeLoadCB(inq), \n19   for (i . 1 ; i < NUM INNER - 1 ; i++) { 19    return SUSPENDED; 19             \n td, NULL, NULL); 20    ptlread ereate(tlreads[i], attr, 20   outq->enqueue(\"output); 20  fune->task \n. task; 21       Transform, new ArgTr(q1, q2)); 21   return EXECUTING; 21  return task; 22 \n  } 22  } 22 } 23   ptlread ereate(tlreads[NUM INNER-1], 23  friend Task* Transeode getTask(...); \n23 elass TranseodeLoadCB { 24     attr, Write, new ArgW(q2, output)); 24 }; 24  Q* inq; 25fffffffff.fIIf \noinfthreads 25 25  TranseodeLoadCB(Q* inq) : inq(inq) {} 26   outq->enqueue(*output); 26 26  double \noperator()() {return inq.size();} 27  } 27 27 }; 28 }  28 28  Figure7. Comparison of parallelization \nusing POSIX threads and DoPE continued on next page (3) Stages of pipeline to transeode an individual \nvideo elip 29 void* Read(void* arg) { 29 elass ReadFunetor { 29 Task* Read getTask(Q* q1) { 30 . IIGetjinputjandjqlj \nromjarg 30  Task* task; IIThisj unctor'sjtask 30  ReadFunetor* fune . new ReadFunetor(q1); 31 for(;;) \n{ 31  ... IICapturejlocaljvariables 31  TaskDeseriptor* td . new TaskDeseriptor(SEQ, NULL); 32  frame \n. readFrame(*input); 32  ... IIConstructor 32  Task* task . new Task(fune, NULL, td, NULL, 33  if \n(frame .. NULL) break; 33  TaskStatus operator()() { 33             new ReadFiniCB(q1)); \n34   q1->enqueue(frame); 34   status . task->begin(); 34  fune->task . task; 35 } 35   if (status \n.. SUSPENDED) 35  return task; 36 q1->enqueue(NULL); 36    return SUSPENDED; 36 } 37 } 37   frame \n= readFrame(*input); 37 38 38   if (frame == NULL) 38 elass ReadFiniCB { 39 39    return FINISHED; \n39  Q* q1; 40 40   task->end();  40  ReadFiniCB(Q* q1) : q1(q1) {} 41 41   ql->enqueue(frame); \n41  void operator()() {ql->enqueue(NULL);}; 42 42   return EXECUTING; 42 }; 43 43  }  43 44 44 \n friend Task* Read getTask(...);    44 45 45 }; 45 46 void* Transform(void* arg) { 46 elass TransformFunetor \n{ 46 Task* Transform getTask(Q* q1, Q* q2) { 47  .jIIGetjqlj romjarg 47  Task* task;jIIThisj unctor'sjtask \n47  TransformFunetor* fune . new TransformFunetor(q2); 48  for(;;) { 48  ... IICapturejlocaljvariables \n48  TaskDeseriptor* td . new TaskDeseriptor(PAR, NULL); 49   frame . q1->dequeue(); 49  ... IIConstructor \n49  Task* task . new Task(fune, new TransformLoadCB(q1), 50   if (frame .. NULL) break; 50  TaskStatus \noperator()() { 50         td, NULL, new TransformFiniCB(q2)); 51   frame . eneodeFrame(frame); \n51   frame = ql->dequeue(); 51  fune->task . task; 52   q2->enqueue(frame); 52   if (frame == \nnull) 52  return task; 53  } 53 return FINISHED; 53 } 54  q2->enqueue(NULL); 54   status . task->begin(); \n54 elass TransformFiniCB { 55 } 55   frame = encodeFrame(frame); 55  Q* q2; 56 56   status . task->end(); \n56  TransformFiniCB(Q* q2) : q2(q2) {} 57 57   q2->enqueue(frame); 57  void operator()() {q2->enqueue(NULL);} \n58 58   return EXECUTING; 58 }; 59 59  } 59 elass TransformLoadCB { 60 60  friend Task* Transform \ngetTask(...); 60  Q* q1; 61 61 }; 61  TransformLoadCB(Q* q1) : q1(q1) {} 62 62 62  double operator()() \n{return q1.size();} 63 63 63 }; 64 void* Write(void* arg) { 64 elass WriteFunetor { 64 Task* Write getTask(Q* \nq2) { 65  . IIGetjq2jandjoutputj romjarg 65  Task* task; IIThisj unctor'sjtask 65  WriteFunetor* fune \n. new WriteFunetor(q1); 66  for(;;) { 66  ... IICapturejlocaljvariables 66  TaskDeseriptor* td . new \nTaskDeseriptor(SEQ, NULL); 67   frame . dequeue(q2); 67  ... IIConstructorj 67  Task* task . new \nTask(fune, new WriteLoadCB(q2), td, 68   if (frame .. NULL) break; 68  TaskStatus operator()() { 68 \n             NULL, NULL); 69   writeFrame(output, frame); 69   frame = q2->dequeue(); \n69  fune->task . task; 70  } 70   if (frame == null) 70  return task; 71 } 71    return FINISHED; \n71 } 72 72   status . task->begin(); 72 elass WriteLoadCB { 73 73   writeFrame(output, frame); 73 \n Q* q2; 74 74   status . task->end(); 74  WriteLoadCB(Q* q2) : q2(q2) {} 75 75   return EXECUTING; \n75  double operator()() {return q2.size();} 76 76  } 76 }; 77 77  friend Task* Write getTask(...); \n77 78 78 }; 78 Figure7. Comparison of parallelization using POSIX threads and DoPE is invoked. When \nDoPE decides to recon.gure, it returns SUSPEN-DED. The application should check this condition (lines \n35 36 in ReadFunctor), and then enter a globally consistent state prior to recon.guration. The FiniCB \ncallbacks are used for this pur\u00adpose. In this particular example, Read noti.es Transform (via the ReadFiniCB \ncallback) which in turn noti.es Write (via the TransformFiniCB callback). The noti.cations are by means \nof enqueuing a sentinel NULL token to the in-queue of the next task. Note by comparing the Pthreads (lines \n36 and 54) and DoPE versions (lines 41 and 57) that the developer was able to reuse the thread termination \nmechanism from the Pthreads parallelization to implement the FiniCBs. InitCB callbacks are used symmet\u00adrically \nfor ensuring consistency before the parallel region is re\u00adentered after recon.guration. The video transcoding \nexample does not require any InitCB callbacks to be de.ned. 3.3 Summary In the Pthreads based parallelization, \nthe developer is forced to im\u00adplement a concrete, unchanging con.guration of parallelism. In the DoPE \nbased parallelization, the developer declares the parallelism structure of the program, while deliberately \nnot specifying the ex\u00adact parallelism con.guration. This underspeci.cation allows DoPE to adapt parallelism \nto determine the optimal con.guration at run\u00adtime. While the API has been described for use by a developer, \na parallelizing compiler could also target the API in the same way as it targets Pthreads.  4. DoPEfortheAdministrator \nThe administrator speci.es a performance goal that includes an ob\u00adjective and a set of resource constraints \nunder which the objective must be met. Examples of performance goals are minimize re\u00adsponse time and \nmaximize throughput under a peak power con\u00adstraint . The administrator may also invent more complex perfor\u00admance \ngoals such as minimizing the energy-delay product [9], or minimizing electricity bills while meeting \nminimum performance requirements [19]. DoPE aims to meet the performance goals by dynamically adapting \nthe con.guration of program parallelism.  A mechanism is an optimization routine that takes an objective \nfunction such as response time or throughput, a set of constraints including number of hardware threads \nand power consumption, and determines the optimal parallelism con.guration. The admin\u00adistrator provides \nvalues to a mechanism s constraints. An example speci.cation by the administrator to a mechanism that \nmaximizes throughput could be 24 threads, 600 Watts thereby instructing the mechanism to optimize under \nthose constraints. In the absence of a suitable mechanism, the administrator can play the role of a mechanism \ndeveloper and add a new mechanism to the library. 5. DoPEfortheMechanismDeveloper Figure 8 shows the \nDoPE system architecture. The DoPE-Executive is responsible for directing the interactions between the \nvarious system components. DoPE maintains a Thread Pool with as many threads as constrained by the performance \ngoals. DoPE uses mech\u00adanisms to adapt parallelism in order to meet the speci.ed goals. There are two \nmain information .ows when an application is launched. First, the application registers its parallelism \ndescriptors (expressed by the application developer). Second, the administra\u00adtor speci.es the performance \ngoals. The DoPE run-time system then starts application execution. During execution, it monitors and \nadapts the parallelism con.guration to meet those goals. Referring to Figure 8, DoPE monitors both the \napplication (A) and platform (B). Section 3.2 already described the methods that enable DoPE to monitor \napplication features such as task execu\u00adtion time and task load. DoPE uses per thread timers (updated \nus\u00ading calls to clock gettime) to obtain task execution time. To enable DoPE to monitor platform features \nsuch as number of hard\u00adware contexts, power, temperature, etc., the mechanism developer registers a feature \nwith an associated callback that DoPE can invoke to get a current value of the feature. Figure 9 shows \nthe registration API. For example, the developer could register SystemPower with a callback that queries \nthe power distribution unit to obtain the current system power draw [2].  Figure 8. Interactions of \nthree agents around DoPE. The appli\u00adcation developer describes parallelism using DoPE just once. The \nmechanism developer implements mechanisms to transform the parallelism con.guration. The administrator \nsets the constraint pa\u00adrameter values of the mechanism. (A) and (B) represent continuous monitoring of \napplication and platform features. (1) (5) denote the sequence of events that occurs when parallelism \nrecon.guration is triggered. 1 //Application features 2 double DoPE::getExecTime(Task* task); 3 double \nDoPE::getLoad(Task* task); 4 //Platformfeatures 5 void DoPE::registerCB(string feature, Functor* getValueOfFeatureCB); \n6 void* DoPE::getValue(string feature); Figure9. DoPE Mechanism Developer API The primary role of the \nmechanism developer is to implement the logic to adapt a parallelism con.guration to meet a performance \ngoal by using the information obtained via monitoring. For this, DoPE exposes the query API shown in \nFigure 9 to the mecha\u00adnism developer. Figure 10 shows a mechanism that can enable a Maximize Throughput \nwith N threads performance goal. Every mechanism must implement the reconfigureParallelism method. The \nmethod s arguments are the descriptor of the current parallelism con.guration and the maximum number \nof threads that can be used to construct a new con.guration. The new con.gura\u00adtion is returned to the \ncaller (DoPE-Executive). 1 ParDescriptor* Mechanism::recon.gureParallelism 2 (ParDescriptor* pd, int \nnthreads){ 3 .oat total time = 0.0; 4 //1. Compute totaltime 5 foreach (Task* task: pd.tasks) { 6 total \ntime += DoPE::getExecTime(task); 7 } 8 //2. Assign DoP proportionalto execution time; 9 //recurse if \nneeded 10 foreach (Task* task: pd.tasks) { 11 task.dop = nthreads * (DoPE::getExecTime(task)/total time); \n12 ParDescriptor* innerPD = task.pd; 13 if (innerPD) { 14 task.pd = recon.gureParallelism(innerPD, task.dop); \n15 } 16 } 17 ... //3. Construct new con.guration -Omitted 18 return newPD; 19 } Figure10. Mechanism \nto maximize throughput Assigns DoP to each task proportional to task s execution time The intuition encoded \nby the mechanism in Figure 10 is that tasks that take longer to execute should be assigned more re\u00adsources. \nIn step 1, the mechanism computes total execution time (lines 4 7) so that each task s execution time \ncan be normal\u00adized. In step 2, the mechanism assigns a DoP that is propor\u00adtional to the normalized execution \ntime of each task (line 11). reconfigureParallelism is recursively invoked to assign DoPs to the inner \nloops in the loop nest. For each loop, a new con\u00ad.guration is constructed with the new task descriptors \nand returned to the parent descriptor. For brevity, this last step is omitted.  6. DoPEOperationWalk-through \nOnce a mechanism is selected, DoPE uses it to recon.gure paral\u00adlelism. The Executive triggers a parallelism \nrecon.guration in re\u00adsponse to changes in the execution environment such as increase in workload. When \nrecon.guration is triggered, the following se\u00adquence of events occurs (refer to Figure 8): 1. The Mechanism \ndetermines the optimal parallelism con.gura\u00adtion, which it conveys to the Executive. 2. The Executive \nreturns SUSPENDED to invocations of Task::begin and Task::end in order to convey to the application DoPE \ns intent of recon.guration.   3. In response, the application and DoPE steer execution into a suspended \nstate by invoking the FiniCB callbacks of all the tasks. 4. The Executive then schedules a new set of \ntasks for execution by the Thread Pool the task set is de.ned by the new paral\u00adlelism con.guration speci.ed \nby the Mechanism. 5. The Thread Pool executes the new tasks on the Platform.  7. PerformanceGoalsandMechanismsTested \nOne advantage of the separation of concerns enabled by the DoPE interface is that a mechanism developer \ncan implement new mecha\u00adnisms and add them to the library in order to better support existing performance \ngoals or to enable new ones, without changing the ap\u00adplication code. The separation of concerns also \nenables reuse of mechanisms across many parallel applications. This separation al\u00adlowed us to implement \nand test three different goals of system use, with multiple mechanisms to achieve them. For each performance \ngoal, thereis abestmechanism thatDoPE usesbydefault.In other words, ahuman need not select aparticular \nmechanism to usefrom among many. Multiple mechanisms are described for each perfor\u00admance goal in order \nto demonstrate the power of DoPE s API. Ta\u00adble 3 lists the implemented mechanisms and the number of lines \nof code for implementing each. Two of the mechanisms are proposed in prior work for a .xed goal-mechanism \ncombination. Mechanism WQT-H WQ-Linear TBF FDP [29] SEDA [38] TPC 28 9 89 94 30 154 Table3. Lines of \ncode to implement tested mechanisms 7.1 Goal: MinResponsetimewithN threads For systems serving online \napplications, the system utility is often maximized by minimizing the average response time experienced \nby the users, thereby maximizing user satisfaction. In the video transcoding example of Section 2, the \nprogrammer used an obser\u00advation to minimize response time: If load on the system is light, a con.guration \nthat minimizes execution time is better, whereas if load is heavy, a con.guration that maximizes throughput \nis better. This observation informs the following mechanisms: Mechanism: Work Queue Threshold with Hysteresis \n(WQT-H) WQT-H captures the notion of latency mode and throughput mode in the form of a 2-state machine \nthat transitions from one state to the other based on occupancy of the work queue. Initially, WQT-H is \nin the SEQ state in which it returns a DoP extent of 1 (sequential execution) to each task. When the \noccupancy of the work queue remains under a threshold T for more than No. consecutive tasks, WQT-H transitions \nto the PAR state in which it returns a DoP extent of Mmax (DoP extent above which parallel ef.ciency \ndrops below 0.5) to each task. WQT-H stays in the PAR state until the work queue threshold increases \nabove T and stays like that for more than Non tasks. The hysteresis allows the system to infer a load \npattern and avoid toggling states frequently. The hysteresis lengths (Non and No. ) can be weighted in \nfavor of one state over another. For example, one extreme could be to switch to the P AR state only under \nthe lightest of loads (No. \u00bb Non ). Mechanism: Work Queue Linear(WQ-Linear) A more grace\u00adful degradation \nof response time with increasing load may be achieved by varying the DoP extent continuously in the range \n[Mmin ,Mmax ], rather than just toggling between two DoP extent values. WQ-Linear assigns a DoP extent \naccording to Equation 2. DoPextent = max(Mmin,Mmax -k \u00d7 WQo) (2) WQo is the instantaneous work queue \noccupancy. k is the rate of DoP extent reduction (k> 0). k is set according to Equation 3. Mmax -Mmin \nk = (3) Qmax Qmax in Equation 3 is derived from the maximum response time degradation acceptable to the \nend user and is set by the system ad\u00administrator taking into account the service level agreement (SLA), \nif any. The degradation is with respect to the minimum response time achievable by the system at a load \nfactor of 1.0. The thresh\u00adold value T in the WQT-H mechanism is obtained similarly by a back-calculation \nfrom the acceptable response time degradation. A variant of WQ-Linear could be a mechanism that incorporates \nthe hysteresis component of WQT-H into WQ-Linear.  7.2 Goal: MaxThroughput withN threads Many applications \ncan be classi.ed as throughput-oriented batch applications. The overall application throughput is limited \nby the throughput of the slowest parallel task. By observing the in-queue occupancies of each task and \ntask execution time, throughput lim\u00aditing tasks can be identi.ed and resources can be allocated accord\u00adingly. \nThis informs the following mechanisms: Mechanism: Throughput Balance with Fusion (TBF) TBF records a \nmoving average of the throughput (inverse of execu\u00adtion time) of each task. When recon.guration is triggered, \nTBF assigns each task a DoP extent that is inversely proportional to the average throughput of the task. \nIf the imbalance in the through\u00adputs of different tasks is greater than a threshold (set to 0.5), TBF \nfuses the parallel tasks to create a bigger parallel task. The ratio\u00adnale for fusion is that if a parallel \nloop execution is heavily unbal\u00adanced, then it might be better to avoid the inef.ciency of pipeline parallelism. \nOur current implementation requires the application developer to implement and register the desired fused \ntask via the TaskDescriptor API that allows expression of choice of ParDescriptors (see lines 4 and 6 \nin Figure 3). Creating fused tasks is easy and systematic: Unidirectional inter-task communi\u00adcation should \nbe changed to method argument communication via the stack. Some of the applications that we studied already \nhad pre\u00adexisting code for fusing tasks in the original Pthreads-parallelized source code. These were \noriginally included to improve sequential execution in case of cache locality issues. Once registered, \nDoPE will automatically spawn the fused task if task fusion is triggered by the mechanism. Other mechanisms \nfor throughput maximization that we tested are: Mechanism: Feedback Directed Pipelining (FDP) FDP uses \ntask execution times to inform a hill climbing algorithm to identify parallelism con.gurations with better \nthroughput [29]. Mechanism: Stage Event-Driven Architecture (SEDA) SEDA assigns a DoP extent proportional \nto load on a task [38].  7.3 Goal: MaxThroughput withN threads,PWatts Mechanism: Throughput Power Controller(TPC) \nThe admin\u00adistrator might want to maximize application performance under a system level constraint such \nas power consumption. DoPE enables the administrator to specify a power target, and uses a closed-loop \ncontroller to maximize throughput while maintaining power con\u00adsumption at the speci.ed target. The controller \ninitializes each task with a DoP extent equal to 1. It then identi.es the task with the least throughput \nand increments the DoP extent of the task if through\u00adput improves and the power budget is not exceeded. \nIf the power budget is exceeded, the controller tries alternative parallelism con\u00ad.gurations with the \nsame DoP extent as the con.guration prior to power overshoot. The controller tries both new con.gurations \nTable 4. Applications enhanced using DoPE. Columns 3 7 indicate the effort required to port the original \nPthreads based parallel code to the DoPE interface. Where applicable, column 6 indicates the number of \nlines of code in tasks created by fusing other tasks. Column 8 indicates the number of loop nesting levels \nin each application that were exposed for this study. Where applicable, the last column indicates the \nminimum DoP extent of the inner loop at which the execution time of a transaction is improved.  Application \nDescription Added Lines of Code Modi.ed Deleted Fused Total Number of Loop Nesting Inner DoPmin extent \nfor Levels speedup x264 Transcoding of yuv4mpeg videos [3] 72 10 8 - 39617 2 2 swaptions Option pricing \nvia Monte Carlo simulations [3] 85 11 8 - 1428 2 2 bzip Data compression of SPEC ref input [6, 28] 63 \n10 8 - 4652 2 4 gimp Image editing using oilify plugin [10] 35 12 4 - 1989 2 2 ferret Image search engine \n[3, 17] 97 15 22 59 14781 1 - dedup Deduplication of PARSEC native input [3] 124 10 16 113 7546 1 - \nand con.gurations from recorded history in order to determine the con.guration with best throughput. \nThe controller monitors power and throughput continuously in order to trigger recon.guration if needed. \n 8. Evaluation Table 4 provides a brief description of the applications that have been enhanced using \nDoPE. All are computationally intensive par\u00adallel applications. 8.1 TheDoPEInterface Columns 3 7 in Table \n4 are indicative of the effort required to port existing Pthreads based parallel applications to the \nproposed DoPE API. The nature of the changes has already been illustrated in Section 3. The number of \nadditional lines of code written by the application developer could be signi.cantly reduced with compiler \nsupport for functor creation and variable capture in C++ and task fusion. 8.2 TheDoPERun-timeSystem \nThe DoPE run-time system is implemented as a user-land shared library built on top of Pthreads. The performance \noverhead (com\u00adpared to the Pthreads parallelizations) of run-time monitoring of workload and platform \ncharacteristics is less than 1%, even for monitoring each and every instance of all the parallel tasks. \nWhile we have explored more combinations, for all but one benchmark in Table 4, we present results on \none performance goal. For one benchmark an image search engine (ferret) we present re\u00adsults on all the \ntested performance goals. All improvements reported are over the baseline Pthreads based parallelizations. \nAll evaluations were done natively on an Intel Xeon X7460 machine composed of 4 sockets, each with a \n6-core Intel Core Architecture 64-bit processor running at 2.66GHz. The total number of cores (and hardware \ncontexts) is 24. The system is equipped with 24GB of RAM and runs the 2.6.31-20-server Linux kernel. \nApplications were compiled using gcc 4.4.1 with the -O3 optimization .ag. Reported numbers are average \nvalues over three runs. In the case of applications with online server behavior, the arrival of tasks \nwas simulated using a task queuing thread that enqueues tasks to a work queue according to a Poisson \ndistribution. The average arrival rate determines the load factor on the system. A load factor of 1.0 \ncorresponds to an average arrival rate equal to the maximum throughput sustainable by the system. The \nmaximum throughput is determined as N/T where N is the number of tasks and T is the time taken by the \nsystem to execute the tasks in parallel (but executing each task itself sequentially). To determine the \nmaximum throughput for each application, N was set to 500. 8.2.1 Goal: MinResponsetimewithN threads The \napplications studied for this goal are video transcoding, op\u00adtion pricing, data compression, image editing, \nand image search. All applications studied for this goal have online service behavior. Minimizing response \ntime is most interesting in the case of appli\u00adcations with nested loops due to the potential latency-throughput \ntradeoff described in Section 2. The outermost loop in all cases it\u00aderates over user transactions. The \namount of parallelism available in this loop nesting level varies with the load on the servers. Figure \n11 shows the performance of the WQT-H and WQ-Linear mechanisms compared to the static <DoPouter , DoPinner> \ncon.gurations of DoP = <(24, DOALL), (1, SEQ)> and DoP = <(N/Mmax , DOALL), (Mmax , PIPE | DOALL)>. Here, \nMmax refers to the extent of DoPinner above which parallel ef\u00ad.ciency drops below 0.5. Interestingly, \nWQT-H outperforms both static mechanisms at certain load factors. For example, consider the response \ntimes at load factor 0.8 in Figure 11(b). Analysis of the work queue oc\u00adcupancy and DoP assignment to \ntasks reveals that even though the load factor is on average equal to 0.8, there are periods of heavier \nand lighter load. DoPE s dynamic adaptation of the DoP between DoP = <(24, DOALL), (1, SEQ)> and DoP \n= <(N/Mmax , DOALL), (Mmax , PIPE | DOALL)> results in an average DoP somewhere in between the two, and \nthis aver\u00adage DoP is better than either for minimizing response time. This provides experimental validation \nof the intuitive rationale behind WQ-Linear, which provides the best response time characteristic across \nthe load factor range. In the case of data compression (Fig\u00adure 11(c)), the minimum extent of DoPinner \nat which speedup is obtained over sequential execution is 4 (see Table 4). This results in two problems \nfor WQ-Linear. First, WQ-Linear may give unhelp\u00adful con.gurations such as <(8, DOALL), (3, PIPE)>. Second, \nthe number of con.gurations at WQ-Linear s disposal is too few to provide any improvement over WQT-H. \nFigure 12 shows the response time characteristic of ferret. The .gure shows the static distribution of \nthreads to each pipeline stage. For example, (<1, 6, 6, 6, 6, 1>, PIPE) indicates a single loop parallelized \nin a pipelined fashion with 6 threads allocated to each parallel stage and 1 thread allocated to each \nsequential stage. Oversubscribing the hardware resources by allocating 24 threads to each parallel task \nresults in much improved response time compared to a static even distribution of the 24 hardware threads. \nDoPE achieves a much better characteristic by allocating threads proportional to load on each task. \n 8.2.2 Goal: MaxThroughput withN threads For batch processing applications, a desirable performance goal \nis throughput maximization. DoPE uses the mechanisms described in Section 7.2 to improve the throughput \nof an image search engine and a .le deduplication application.  12 Power (Watts) Response Time (secs) \n Response Time (secs)   Normalized load on system Normalized load on system (a) Video transcoding(b) \nOption pricing 30 50  Normalized load on system Normalized load on system (c) Data compression(d) Image \nediting Figure11. Response time variation with load using Static, WQT-H, and WQ-Linear mechanisms Normalized \nload on system Time (seconds) Figure12. ferret Response TimeFigure13. ferret Throughput 70 600 50 500 \n800 700 Throughput (Queries/s) 60 40 Apps. ferret dedup 30 1.00\u00d7 1.00\u00d7 Pthreads Baseline OS 2.12\u00d7 \n0.89\u00d7 20 10 DoPE 0 Time (seconds) Figure14. ferret Power-Throughput Figure 15. Throughput improvement \nover static even thread distribution Table 15 shows the throughput improvements for ferret and dedup \nusing different mechanisms. Pthreads-Baseline is the orig\u00adinal Pthreads parallelization with a static \neven distribution of avail\u00adable hardware threads across all the parallel tasks after assigning a single \nthread to each sequential task. (This is a common prac\u00adtice [21].) The Pthreads-OS number shows the performance \nwhen each parallel task is initialized with a thread pool containing as many threads as the number of \navailable hardware threads in the platform, and the operating-system s scheduler is called upon to do \nload balancing. The remaining numbers represent the performance of the DoPEd applications using the mechanisms \ndescribed in Sec\u00adtion 7.2. DoPE-TB is the same as DoPE-TBF but with task fusion turned off, in order \nto demonstrate the bene.t of task fusion. DoPE-TBF outperforms all other mechanisms. OS scheduling causes \nmore context-switching, cache pollution, and memory con\u00adsumption. In the case of dedup, these overheads \nresult in virtu\u00adally no improvement over the baseline. The overheads may become prominent even in the \ncase of ferret on a machine with a larger number of cores. In addition, this mechanism is still a static \nscheme that cannot adapt to run-time events such as more cores becoming available to the application. \nEach task in SEDA resizes its thread pool locally without coordinating resource allocation with other \ntasks. By contrast, both FDP and TBF have a global view of re\u00adsource allocation and are able to redistribute \nthe hardware threads according to the throughput of each task. Additionally, FDP and TBF are able to \neither fuse or combine tasks in the event of very uneven load across stages. Compared to FDP which simulates \ntask fusion via time-multiplexed execution of tasks on the same thread, TBF has the additional bene.t \nof avoiding the overheads of for\u00adwarding data between tasks by enabling the developer to explicitly expose \nthe appropriate fused task. Figure 13 shows the dynamic throughput characteristic of ferret. DoPE searches \nthe parallelism con.guration space be\u00adfore stabilizing on the one with the maximum throughput under the \nconstraint of 24 hardware threads. 8.2.3 Goal: MaxThroughput withN threads,PWatts Figure 14 shows the \noperation of DoPE s power-throughput con\u00adtroller (TPC) on ferret. For a peak power target speci.ed by \nthe administrator, DoPE .rst ramps up the DoP extent until the power budget is fully used. DoPE then \nexplores different parallelism con\u00ad.gurations and stabilizes on the one with the best throughput with\u00adout \nexceeding the power budget. Note that 90% of peak total power corresponds to 60% of peak power in the \ndynamic CPU range (all cores idle to all cores active). DoPE achieves the maximum throughput possible \nat this setting. Fine-grained per core power control can result in a wider dynamic range and greater \npower sav\u00adings [27]. Full system power was measured at the maximum sam\u00adpling rate (13 samples per minute) \nsupported by the power distribu\u00adtion unit (AP7892 [2]). This limited the speed with which the con\u00adtroller \nresponds to .uctuations in power consumption. Newer chips have power monitoring units with higher sampling \nrates and clock gating per core. They could be used to design faster and higher performance controllers \nfor throttling power and parallelism. The transient in the Stable region of the .gure shows how constant \nmon\u00aditoring enables DoPE to respond to system events.  9. RelatedWork Parallelization Libraries Several \ninterfaces and associated run\u00adtime systems have been proposed to adapt parallel program exe\u00adcution to \nrun-time variability [4, 8, 9, 13, 20, 26, 29, 35, 37, 38]. However, each interface is tied to a speci.c \nperformance goal, spe\u00adci.c mechanism of adaptation, or a speci.c application/platform domain. OpenMP \n[22], Cilk [5], and Intel TBB [26] support task parallelism for independent tasks and their schedulers \noptimize only for throughput. DoPE enables the developer to express par\u00adallelism in loop nests involving \ninteracting tasks, and enables ad\u00administrators to specify different performance goals. Navarro et al. \ndeveloped an analytical model for pipeline parallelism to charac\u00adterize performance and ef.ciency of \npipeline parallel implemen\u00adtations [21]. Suleman et al. proposed Feedback Directed Pipelin\u00ading (FDP) \n[29]. Moreno et al. proposed a technique similar to FDP called Dynamic Pipeline Mapping (DPM) [20]. We \nimplemented FDP as a throughput maximization mechanism. Domain-speci.c Programming Models Traditionally, \nmultiple levels of parallelism across tasks and within each task has been in\u00advestigated in the database \nresearch community for SQL queries [7, 12, 31, 32]. DoPE extends these works by providing dynamic adap\u00adtation \nto general-purpose applications that typically involve other forms of parallelism like pipeline parallelism, \ntask parallelism, etc. DoPE also allows the administrator to specify different per\u00adformance goals, and \noptimizes accordingly. For network service codes, programming models such as the Stage Event-Driven Ar\u00adchitecture \n(SEDA) [38] and Aspen [35] have been proposed. We implemented the SEDA controller as a throughput maximization \nmechanism. Compared to these models, DoPE is applicable to pro\u00adgrams with loop nests, and supports multiple \nperformance goals. The mechanisms proposed for different performance goals in the context of DoPE could \nform a valuable part of the respective run\u00adtime schedulers of SEDA and Aspen. Blagojevic et al. propose \nuser-level schedulers that dynamically rightsize the loop nest\u00ading level and degree of parallelism on \na Cell Broadband Engine system [4]. Unlike DoPE, they exploit only one form of intra-task parallelism \nloop-level DOALL parallelism. Auto-tuning Wang et al. use machine learning to predict the best number \nof threads for a given program on a particular hardware platform [37]. They apply their technique on \nprograms with a single loop. Luk et al. use a dynamic compilation approach and curve .tting to .nd the \noptimal distribution of work between a CPU and GPU [16]. Hall and Martonosi propose to increase or decrease \nthreads allocated to compiler parallelized DOALL loops at run-time as the measured speedup exceeds or \nfalls short of the expected speedup [11]. The ADAPT dynamic optimizer applies loop optimizations at run-time \nto create new variants of code [36]. Some of these sophisticated machine learning techniques could be \nused to improve DoPE s mechanisms. 10. Conclusion Parallel applications must execute robustly across \na variety of exe\u00adcution environments arising out of variability in workload charac\u00adteristics, platform \ncharacteristics, and performance goals. For this, a separation of concerns of parallel application development, \nits optimization, and use, is required. The Degree of Parallelism Ex\u00adecutive (DoPE) enables such a separation. \nUsing DoPE, the appli\u00adcation developer can specify all of the potential parallelism in loop nests just \nonce; the mechanism developer can implement mecha\u00adnisms for parallelism adaptation; and the administrator \ncan select a suitable mechanism that implements a performance goal of system use. As a result of DoPE, \nthey can be con.dent that the speci.ed performance goals are met in a variety of application execution \nen\u00advironments.   Acknowledgments We thank the entire Liberty Research Group for their support and feedback \nduring this work. We thank Alexander Wauck for help with the data compression application. We also thank \nthe anonymous reviewers for their valuable feedback. This material is based on work supported by National \nScience Foundation Grants 0964328 and 1047879, and by United States Air Force Contract FA8650-09-C-7918. \nArun Raman is supported by an Intel Founda\u00adtion Ph.D. Fellowship.  References [1] R. Allen and K. Kennedy. \nOptimizing compilers for modern architec\u00adtures: A dependence-based approach. Morgan Kaufmann Publishers \nInc., 2002. [2] APC metered rack PDU user s guide. http://www.apc.com. [3] C. Bienia, S. Kumar, J. P. \nSingh, and K. Li. The PARSEC benchmark suite: characterization and architectural implications. In Proceedings \nof the Seventeenth International Conference on Parallel Architecture andCompilation Techniques(PACT), \n2008. [4] F. Blagojevic, D. S. Nikolopoulos, A. Stamatakis, C. D. Antonopou\u00adlos, and M. Curtis-Maury. \nRuntime scheduling of dynamic parallelism on accelerator-based multi-core systems. Parallel Computing, \n2007. [5] R. D. Blumofe, C. F. Joerg, B. C. Kuszmaul, C. E. Leiserson, K. H. Randall, and Y. Zhou. Cilk: \nAn ef.cient multithreaded runtime system. In Proceedings of the 5th ACM SIGPLAN Symposium on Principles \nandPractice ofParallelProgramming(PPoPP), 1995. [6] M. Bridges, N. Vachharajani, Y. Zhang, T. Jablin, \nand D. August. Revisiting the sequential programming model for multi-core. In Proceedings of the40thAnnualIEEE/ACMInternationalSymposium \nonMicroarchitecture(MICRO), 2007. [7] C. B. Colohan, A. Ailamaki, J. G. Steffan, and T. C. Mowry. Opti\u00admistic \nintra-transaction parallelism on chip multiprocessors. In Pro\u00adceedings of the 31st International Conference \non Very Large Data Bases(VLDB), 2005. [8] M. Curtis-Maury, J. Dzierwa, C. D. Antonopoulos, and D. S. \nNikolopoulos. Online power-performance adaptation of multi\u00adthreaded programs using hardware event-based \nprediction. In Pro\u00adceedings of the 20th International Conference on Supercomputing (ICS), 2006. [9] Y. \nDing, M. Kandemir, P. Raghavan, and M. J. Irwin. Adapting application execution in CMPs using helper \nthreads. Journal of Parallel andDistributedComputing, 2009. [10] GNU Image Manipulation Program. http://www.gimp.org. \n[11] M. W. Hall and M. Martonosi. Adaptive parallelism in compiler\u00adparallelized code. In Proceedings \nof the 2nd SUIF Compiler Work\u00adshop, 1997. [12] N. Hardavellas, I. Pandis, R. Johnson, N. Mancheril, A. \nAilamaki, and B. Falsa.. Database servers on chip multiprocessors: Limitations and opportunities. In \nProceedings of the Third Biennial Conference on Innovative DataSystems Research(CIDR), 2007.  [13] W. \nKo, M. N. Yankelevsky, D. S. Nikolopoulos, and C. D. Poly\u00adchronopoulos. Effective cross-platform, multilevel \nparallelism via dy\u00adnamic adaptive execution. In Proceedings of the International Paral\u00adlel andDistributedProcessingSymposium(IPDPS), \n2002. [14] M. Kulkarni, K. Pingali, B. Walter, G. Ramanarayanan, K. Bala, and L. P. Chew. Optimistic \nparallelism requires abstractions. In Proceedings oftheACMSIGPLAN2007Conference onProgramming LanguageDesign \nandImplementation(PLDI), 2007. [15] R. Liu, K. Klues, S. Bird, S. Hofmeyr, K. Asanovi, and J. Kubiatow\u00adicz. \nTessellation: Space-time partitioning in a manycore client OS. In Proceedings of the First USENIX Workshop \non Hot Topics in Paral-lelism(HotPar), 2009. [16] C.-K. Luk, S. Hong, and H. Kim. Qilin: Exploiting parallelism \non het\u00aderogeneous multiprocessors with adaptive mapping. In Proceedings of the42ndAnnualIEEE/ACMInternational \nSymposium onMicroar\u00adchitecture(MICRO), 2009. [17] Q. Lv, W. Josephson, Z. Wang, M. Charikar, and K. Li. \nFerret: A toolkit for content-based similarity search of feature-rich data. ACM SIGOPSOperating Systems \nReview, 2006. [18] J. Mars, N. Vachharajani, M. L. Soffa, and R. Hundt. Contention aware execution: Online \ncontention detection and response. In Pro\u00adceedings of the Eighth Annual International Symposium on Code \nGeneration andOptimization(CGO), 2010. [19] D. Meisner, B. T. Gold, and T. F. Wenisch. PowerNap: Eliminating \nserver idle power. In Proceedings of the Fourteenth International Symposium on Architectural Support \nfor Programming Languages andOperating Systems(ASPLOS), 2009. [20] A. Moreno, E. C\u00b4esar, A. Guevara, \nJ. Sorribes, T. Margalef, and E. Luque. Dynamic Pipeline Mapping (DPM). In Proceedings of theInternationalEuro-ParConference \nonParallelProcessing(Euro-Par), 2008. [21] A. Navarro, R. Asenjo, S. Tabik, and C. Cascaval. Analytical \nmod\u00adeling of pipeline parallelism. In Proceedings of the Eighteenth Inter\u00adnational Conference on Parallel \nArchitecture and Compilation Tech\u00adniques(PACT), 2009. [22] The OpenMP API speci.cation for parallel programming. \nhttp://www.openmp.org. [23] H. Pan, B. Hindman, and K. Asanovi\u00b4c. Composing parallel software ef.ciently \nwith Lithe. In Proceedings of the ACM SIGPLAN 2010 Conference on Programming Language Design and Implementation \n(PLDI), 2010. [24] M. K. Prabhu and K. Olukotun. Exposing speculative thread paral\u00adlelism in SPEC2000. \nIn Proceedings ofthe10thACMSIGPLANSym\u00adposium onPrinciples andPractice ofParallelProgramming(PPoPP), 2005. \n[25] A. Raman, H. Kim, T. R. Mason, T. B. Jablin, and D. I. August. Spec\u00adulative parallelization using \nsoftware multi-threaded transactions. In Proceedings of the Fifteenth International Symposium on Architec\u00adtural \nSupport for Programming Languages and Operating Systems (ASPLOS), 2010. [26] J. Reinders. Intel ThreadingBuilding \nBlocks. O Reilly &#38; Associates, Inc., Sebastopol, CA, USA, 2007. [27] G. Semeraro, G. Magklis, R. \nBalasubramonian, D. H. Albonesi, S. Dwarkadas, and M. L. Scott. Energy-ef.cient processor design using \nmultiple clock domains with dynamic voltage and frequency scaling. In Proceedings of the Eighth International \nSymposium on High-Performance Computer Architecture(HPCA), 2002. [28] Standard Performance Evaluation \nCorporation (SPEC). http://www.spec.org. [29] M. A. Suleman, M. K. Qureshi, Khubaib, and Y. N. Patt. \nFeedback\u00addirected pipeline parallelism. In Proceedings of the Nineteenth Inter\u00adnational Conference on \nParallel Architecture and Compilation Tech\u00adniques(PACT), 2010. [30] M. A. Suleman, M. K. Qureshi, and \nY. N. Patt. Feedback-driven threading: Power-ef.cient and high-performance execution of multi\u00adthreaded \nworkloads on CMPs. In Proceedings of the Thirteenth International Symposium on Architectural Support \nfor Programming Languages andOperating Systems(ASPLOS), 2008. [31] Sybase adaptive server. http://sybooks.sybase.com/nav/base.do. \n[32] J. Tellez and B. Dageville. Method for computing the degree of parallelism in a multi-user environment. \nUnited States Patent No. 6,820,262. Oracle International Corporation, 2004. [33] The IEEE and The Open \nGroup. TheOpenGroupBaseSpeci.cations Issue6IEEEStd1003.1,2004Edition. 2004. [34] C. Tian, M. Feng, V. \nNagarajan, and R. Gupta. Copy or discard execution model for speculative parallelization on multicores. \nIn Proceedings of the41stAnnualIEEE/ACMInternationalSymposium onMicroarchitecture(MICRO), 2008. [35] \nG. Upadhyaya, V. S. Pai, and S. P. Midkiff. Expressing and exploiting concurrency in networked applications \nwith Aspen. In Proceedings of the12thACMSIGPLANSymposium onPrinciples andPractice of Parallel Programming(PPoPP), \n2007. [36] M. J. Voss and R. Eigenmann. ADAPT: Automated De-Coupled Adaptive Program Transformation. \nIn Proceedings of the 28th In\u00adternationalConference onParallelProcessing(ICPP), 1999. [37] Z. Wang and \nM. F. O Boyle. Mapping parallelism to multi-cores: A machine learning based approach. In Proceedings \nof the 14th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming(PPoPP), 2009. [38] \nM. Welsh, D. Culler, and E. Brewer. SEDA: An architecture for well-conditioned, scalable internet services. \nACMSIGOPSOperating Systems Review, 2001. [39] T. Wiegand, G. J. Sullivan, G. Bjontegaard, and A. Luthra. \nOverview of the H.264/AVC video coding standard. IEEE Transactions on Circuits andSystemsforVideoTechnology, \n2003. [40] H. Zhong, M. Mehrara, S. Lieberman, and S. Mahlke. Uncovering hidden loop level parallelism \nin sequential applications. In Proceed\u00adings ofthe14thInternationalSymposium onHigh-PerformanceCom\u00adputer \nArchitecture(HPCA), 2008.  \n\t\t\t", "proc_id": "1993498", "abstract": "<p>In writing parallel programs, programmers expose parallelism and optimize it to meet a particular performance goal on a single platform under an assumed set of workload characteristics. In the field, changing workload characteristics, new parallel platforms, and deployments with different performance goals make the programmer's development-time choices suboptimal. To address this problem, this paper presents the Degree of Parallelism Executive (DoPE), an API and run-time system that separates the concern of exposing parallelism from that of optimizing it. Using the DoPE API, the application developer expresses parallelism options. During program execution, DoPE's run-time system uses this information to dynamically optimize the parallelism options in response to the facts on the ground. We easily port several emerging parallel applications to DoPE's API and demonstrate the DoPE run-time system's effectiveness in dynamically optimizing the parallelism for a variety of performance goals.</p>", "authors": [{"name": "Arun Raman", "author_profile_id": "81100001392", "affiliation": "Princeton University, Princeton, NJ, USA", "person_id": "P2690464", "email_address": "rarun@princeton.edu", "orcid_id": ""}, {"name": "Hanjun Kim", "author_profile_id": "81479653600", "affiliation": "Princeton University, Princeton, NJ, USA", "person_id": "P2690465", "email_address": "hanjunk@princeton.edu", "orcid_id": ""}, {"name": "Taewook Oh", "author_profile_id": "81381590681", "affiliation": "Princeton University, Princeton, NJ, USA", "person_id": "P2690466", "email_address": "twoh@princeton.edu", "orcid_id": ""}, {"name": "Jae W. Lee", "author_profile_id": "81479656211", "affiliation": "Parakinetics Inc., Princeton, NJ, USA", "person_id": "P2690467", "email_address": "leejw@parakinetics.com", "orcid_id": ""}, {"name": "David I. August", "author_profile_id": "81100388492", "affiliation": "Princeton University, Princeton, NJ, USA", "person_id": "P2690468", "email_address": "august@princeton.edu", "orcid_id": ""}], "doi_number": "10.1145/1993498.1993502", "year": "2011", "article_id": "1993502", "conference": "PLDI", "title": "Parallelism orchestration using DoPE: the degree of parallelism executive", "url": "http://dl.acm.org/citation.cfm?id=1993502"}