{"article_publication_date": "06-04-2011", "fulltext": "\n Commutative Set: A Language Extension for Implicit Parallel Programming Prakash Prabhu Soumyadeep Ghosh \nYun Zhang Nick P. Johnson David I. August Princeton University Princeton, NJ {pprabhu, soumyade, yunzhang, \nnpjohnso, august}@princeton.edu Abstract Sequential programming models express a total program order, \nof which a partial order must be respected. This inhibits paralleliz\u00ading tools from extracting scalable \nperformance. Programmer writ\u00adten semantic commutativity assertions provide a natural way of relaxing \nthis partial order, thereby exposing parallelism implic\u00aditly in a program. Existing implicit parallel \nprogramming mod\u00adels based on semantic commutativity either require additional pro\u00adgramming extensions, \nor have limited expressiveness. This paper presents a generalized semantic commutativity based programming \nextension, called Commutative Set (COMMSET), and associated compiler technology that enables multiple \nforms of parallelism. COMMSET expressions are syntactically succinct and enable the programmer to specify \ncommutativity relations between groups of arbitrary structured code blocks. Using only this construct, \nserializ\u00ading constraints that inhibit parallelization can be relaxed, indepen\u00addent of any particular \nparallelization strategy or concurrency con\u00adtrol mechanism. COMMSET enables well performing paralleliza\u00adtions \nin cases where they were inapplicable or non-performing be\u00adfore. By extending eight sequential programs \nwith only 8 annota\u00adtions per program on average, COMMSET and the associated com\u00adpiler technology produced \na geomean speedup of 5.7x on eight cores compared to 1.5x for the best non-COMMSET parallelization. Categories \nand Subject Descriptors D.1.3 [Programming Tech\u00adniques]: Concurrent Programming Parallel Programming; \nD.3.4 [Programming Languages]: Processors Compilers, Optimization General Terms Languages, Performance, \nDesign, Experimenta\u00adtion Keywords Implicit parallelism, semantic commutativity, pro\u00adgramming model, automatic \nparallelization, static analysis 1. Introduction The dominant parallel programming models for multicores \ntoday are explicit [8, 10, 33]. These models require programmers to ex\u00adpend enormous effort reasoning \nabout complex thread interleav- Permission to make digital or hard copies of all or part of this work \nfor personal or classroom use is granted without fee provided that copies are not made or distributed \nfor pro.t or commercial advantage and that copies bear this notice and the full citation on the .rst \npage. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior \nspeci.c permission and/or a fee. PLDI 11, June 4 8, 2011, San Jose, California, USA. Copyright &#38;#169; \n2011 ACM 978-1-4503-0663-8/11/06. . . $10.00 ings, low-level concurrency control mechanisms, and parallel \npro\u00adgramming pitfalls such as races, deadlocks, and livelocks. Despite this effort, manual concurrency \ncontrol and a .xed choice of paral\u00adlelization strategy often result in parallel programs with poor per\u00adformance \nportability. Consequently, parallel programs often have to be extensively modi.ed when the underlying \nparallel substrates evolve, thus breaking abstraction boundaries between software and hardware. Recent \nadvances in automatic thread extraction [26, 30, 34] pro\u00advide a promising alternative. They avoid pitfalls \nassociated with explicit parallel programming models, but retain the ease of rea\u00adsoning of a sequential \nprogramming model. However, sequential languages express a total order, of which a partial order of program \nexecution must be respected by parallelizing tools. This prohibits some execution orders that are often \npermitted by high-level al\u00adgorithm speci.cations. As a result, automatic parallelizing tools, overly \nconstrained by the need to respect the sequential semantics of programs written in languages like C/C++, \nare unable to extract scalable performance. Implicit parallel programming models (IPP) [4, 13, 15, 31] \nof\u00adfer the best of both approaches. In such models, programmers im\u00adplicitly expose parallelism inherent \nin their program without the explicit use of low-level parallelization constructs. An interesting subclass \nof models within this space includes those that are based on top of sequential programming models [15]. \nIn such models, programmer insights about high-level semantic properties of the program are expressed \nvia the use of extensions to the sequential model. These language extensions are then exploited by transfor\u00admation \ntools to automatically synthesize a correct parallel program. This approach not only frees the programmer \nfrom the burden of having to worry about the low-level details related to paralleliza\u00adtion, but also \npromotes retargetability of such programs when pre\u00adsented with newer parallel substrates. Recent work \nhas shown the importance of programmer speci\u00ad.ed semantic commutativity assertions in exposing parallelism \nim\u00adplicitly in code [5, 7, 18, 27, 30]. The programmer relaxes the or\u00adder of execution of certain functions \nthat read and modify muta\u00adble state, by specifying that they legally commute with each other, despite \nviolating existing partial orders. Parallelization tools ex\u00adploit this relaxation to extract performance \nby permitting behaviors prohibited under a sequential programming model. However, ex\u00adisting solutions \nbased on semantic commutativity either have lim\u00adited expressiveness or require programmers to use additional \nparal\u00adlelism constructs. This paper proposes an implicit parallel program\u00adming model based on semantic \ncommutativity, called Commutative Set (COMMSET), that generalizes existing semantic commutativity  IPP \nSystem Concept Speci.c Parallel Implementation Expressiveness of Commutativity Speci.cation Requires \nParallelism Forms Supported Concurrency Control Mechanism Parallelization Driver Optimistic or Speculative \nParallelism1 Predication Commuting Blocks Group Commutativity Additional Extensions Task1 Pipelined Data \nInterface Client Jade [27] Galois [18] DPJ [5] Paralax [30] VELOCITY [7] COMMSET \u00d7 . \u00d7 \u00d7 \u00d7 . \u00d7 \u00d7 \u00d7 \u00d7 \n\u00d7 . \u00d7 \u00d7 \u00d7 \u00d7 \u00d7 . \u00d7 \u00d7 \u00d7 \u00d7 \u00d7 . Yes Yes Yes No No No . \u00d7 . \u00d7 \u00d7 \u00d7 . \u00d7 \u00d7 . . . \u00d7 . . \u00d7 \u00d7 . Automatic Manual \nManual Automatic Automatic Automatic Runtime Runtime Programmer Compiler Compiler Compiler \u00d7 . \u00d7 \u00d7 . \n\u00d7 Table 1: Comparison between COMMSET and other parallel models based on semantic commutativity constructs \nand enables multiple forms of parallelism from the same speci.cation. Table 1 compares COMMSET with existing \nparallel program\u00adming models based on semantic commutativity. The main advan\u00adtages of COMMSET over existing \napproaches are: (a) COMMSET s commutativity construct is more general than others. Prior ap\u00adproaches \nallow commutativity assertions only on interface declara\u00adtions. However, commutativity can be a property \nof client code as well as code behind a library interface. COMMSET allows the com\u00admutativity assertions \nbetween arbitrary structured code blocks in client code as well as on interfaces, much like the synchronized \nkeyword in Java. It also allows commutativity to be predicated on variables in a client s program state, \nrather than just function ar\u00adguments as in earlier approaches. (b) COMMSET speci.cations be\u00adtween a group \nof functions are syntactically succinct, having lin\u00adear speci.cation complexity rather than quadratic \nas required by existing approaches. (c) COMMSET presents an implicit parallel programming solution that \nenables both pipeline and data paral\u00adlelism without requiring any additional parallelism constructs. \nEx\u00adisting approaches use parallelism constructs that tightly couple par\u00adallelization strategy with concrete \nprogram semantics, in contra\u00advention of the principle of separation of concerns. In contrast, using only \nCOMMSET primitives in our model, parallelism can be implicitly speci.ed at a semantic level and is independent \nof a spe\u00adci.c form or concurrency control mechanism. The contributions of this work are: 1. The design, \nsyntax, and semantics of COMMSET, a novel pro\u00adgramming extension that generalizes, in a syntactically \nsuccinct form, various existing notions of semantic commutativity. 2. An end-to-end implementation of \nCOMMSET within a paral\u00adlelizing compiler that includes the front-end, static analysis to enhance the \nprogram dependence graph with commutativity properties, passes to enable data and pipeline parallelizations, \nand automatic concurrency control. 3. A demonstration of COMMSET s applicability in expressing implicit \nparallelism by extending eight real world sequential programs with commutativity assertions, and an evaluation \nof the performance on real hardware.  The features of COMMSET are .rst motivated by a running example. \nA description of its syntax and semantics follows. An implementation of COMMSET within a parallelizing \ncompiler is explained step by step, followed by an evaluation and a discussion of related work. 2. Motivating \nExample Figure 1 shows a code snippet from a sequential implementation of md5sum (plus highlighted pragma \ndirectives introduced for COMMSET that are discussed later). The main loop iterates through a set of \ninput .les, computing and printing a message digest for each .le. Each iteration opens a .le using a \ncall to fopen, then calls the mdfile function which, in turn, reads the .le s contents via calls to fread \nand then computes the digest. The main loop prints the digest to the console and closes the .le by calling \nfclose on the .le pointer. Although it is clear that digests of individual .les can be safely computed \nout of order, a parallelizing tool cannot infer this automatically without knowing the client speci.c \nsemantics of I/O calls due to its externally visible side effects. However, the loop can be parallelized \nif the commuting behaviors of fopen, fread, fclose, and print digest on distinct .les are conveyed to \nthe parallelizing tool. One way to specify commuting behavior is at the interface dec\u00adlarations of .le \noperations. Galois [18] extracts optimistic paral\u00adlelism by exploiting semantic commutativity assertions \nspeci.ed between pairs of library methods at their interface declarations. These assertions can optionally \nbe predicated on their arguments. To indicate the commuting behaviors of the calls on distinct .les, \none would ideally like to predicate commutativity on the .lename. Since only fopen takes in the .lename \nas an argument, this is not possible. Another approach is to predicate commutativity on the .le pointer \nfp that is returned by fopen. Apart from the fact that expensive runtime checks are required to validate \nthe assertions be\u00adfore executing the I/O calls (which are now on the critical path), this approach may \nprevent certain valid commuting orders due to recycling of .le pointers. Operations on two distinct .les \nat differ\u00adent points in time that happen to use the same .le pointer value are now not allowed to commute. \nThis solution is also not valid for all clients. Consider the following sequence of calls by a client \nthat writes to a .le (fp1) and subsequently reads from it (fp2) in the next iteration: fwrite(fp1), fclose(fp1), \nfopen(fp2), fread(fp2). Here, even though fp1 and fp2 may have different runtime values, they still may \nbe pointing to the same .le. Com\u00admuting fopen(fp2), fread(fp2) with fclose(fp1) may cause a read from \nthe .le before the write .le stream has been completed. Approaches that annotate data (.le pointer fp \nin this case) to im\u00adplicitly assert commutativity between all pairs of operations on that .le pointer \n[27], run into the same problem. Allowing predication on the client s program state can solve the above \nproblem for md5sum. Since the input .les naturally map to different values of the induction variable, \npredicating commutativ\u00adity on the induction variable (in the client) solves the problems as\u00adsociated \nwith interface based predication. First, no legal commuting behavior is prohibited since induction variables \nare de.nitely dif\u00adferent on each iteration. Second, runtime checks for commutativity assertions are avoided \nsince the compiler can use static analysis to symbolically interpret predicates that are functions of \nthe induction variable to prove commutativity on separate iterations. In order to continue using commutativity \nspeci.cations on func\u00adtion declarations while still predicating on variables in client state, programmers \neither have to change existing interfaces or create new wrapper functions to take in those variables \nas arguments. Changing the interface breaks modularity since other clients which do not want commutative \nsemantics are now forced to pass in ad\u00ad 1 The commutativity speci.cations languages of Galois, Paralax, \nVELOC-ITY and COMMSET are conceptually amenable to task and speculative par\u00adallelism  Figure 1: Sequential \nversion of md5sum extended with COMMSET ditional dummy arguments to prevent commuting behaviors. Cre\u00adating \nwrapper functions involves additional programmer effort, es\u00adpecially while replicating functions along \nentire call paths. In the running example, the mdfile interface has to be changed to take in the induction \nvariable as an argument, to allow for predicated commutativity of fread calls with other .le operations \n(fopen and fclose) in the main loop. The additional programmer effort in creating wrappers can be avoided \nby allowing structured code blocks enclosing the call sites to commute with each other. This is easily \nachieved in md5sum by enclosing the call sites of fopen, fread, print digest, and fclose within anonymous \ncommutative code blocks. Commuta\u00adtivity between multiple anonymous code blocks can be speci.ed easily \nby adding the code blocks to a named set, at the begin\u00adning of their lexical scope. Grouping commutative \ncode blocks or functions into a set, as presented here, has linear speci.cation complexity. In contrast, \nexisting approaches [5, 18] require speci\u00adfying commutativity between pairs of functions individually \nlead\u00ading to quadratic speci.cation complexity. The modularity problem (mentioned above) can be solved \nby allowing optionally commut\u00ading code blocks and exporting the option at the interface, without changing \ninterface arguments. Clients can then enable the commu\u00adtativity of the code blocks if it is in line with \ntheir intended seman\u00adtics, otherwise default sequential behavior is preserved. In the case of md5sum, \nthe fread call can be made a part of an optionally commuting block. The option is exported at the interface \ndeclara- Figure 2: PDG for md5sum with COMMSET extensions Figure 3: Timeline for md5sum Parallelizations \ntion of mdfile and is enabled by the main loop, while other clients that require sequential semantics \ncan ignore the option. A commuting code block gives the programmer the .exibility to choose the extent \nto which partial orders can be relaxed in a program. This, in turn, determines the amount of freedom \na par\u00adallelizing tool has, to extract parallelism. For instance, enclosing fread calls inside mdfile \nwithin a commuting block gives more freedom to a parallelizing tool to extract performance, as opposed \nto the outer call to mdfile. Depending on the intended semantics, the programmer can choose the right \ngranularity for the commuting blocks which a parallelizing system should automatically guarantee to be \natomic. Commuting blocks allow for a declarative speci.ca\u00adtion of concurrency which a parallelizing tool \ncan exploit to au\u00adtomatically select the concurrency mechanism that performs best for a given application. \nAutomatic concurrency control has the ad\u00advantage of not requiring invasive changes to application code \nwhen newer mechanisms become available on a particular hardware sub\u00adstrate.  Parallelizing tools should \nbe able to leverage the partial orders speci.ed via commutativity assertions without requiring the pro\u00adgrammer \nto specify parallelization strategies. Existing runtime ap\u00adproaches require the use of additional programming \nextensions that couple parallelization strategies to program semantics. Galois [18] requires the use \nof set iterators that constrain parallelization to data parallelism. Jade [27] requires the use of task \nparallel constructs. DPJ [5] uses explicitly parallel constructs for task and data paral\u00adlelism. COMMSET \ndoes not require such additional extensions. For instance, in md5sum, a programmer requiring deterministic \noutput for the digests, should be able to express the intended semantics without being concerned about \nparallelization. The implementation should be able to automatically change to the best parallelization \nstrategy given the new semantics. Returning to md5sum, specify\u00ading that print digest commutes with the \nother I/O operations, but not with itself constrains output to be deterministic. Given the new semantics, \nthe compiler automatically switches from a better performing data parallel execution of the loop to a \nslightly less per\u00adforming (in this case) pipelined execution. In the data parallel ex\u00adecution, each iteration \nof the loop executes in parallel with other iterations. In a pipeline execution, an iteration is split \ninto stages, with the message digests computed in parallel in earlier stages of the pipeline being communicated \nto a sequential stage that prints the digest in order to the console. Parallelization within the compiler \nis based on the Program De\u00adpendence Graph (PDG) structure [12]. Figure 2 shows the simpli\u00ad.ed PDG for \nsequential md5sum. Each labeled code block is rep\u00adresented by a node in the PDG and a directed edge from \na node n1 to n2 indicates that n2 is dependent on n1. Parallelizing trans\u00adforms (e.g. DOALL [16] and \nPS-DSWP [26]) partition the PDG and schedule nodes onto different threads, with dependences span\u00adning \nthreads automatically respected by insertion of communica\u00adtion and/or synchronization operations. With \nthe original PDG, DOALL and PS-DSWP cannot be directly applied due to a cycle with loop carried dependences: \nB . D . H . I . B and self loops around each node in the cycle. The COMMSET exten\u00adsions help the compiler \nto relax these parallelism-inhibiting depen\u00addences, thereby enabling wider application of existing parallelizing \ntransforms. Figure 3 shows three schedules with different performance char\u00adacteristics for md5sum execution. \nThe .rst corresponds to sequen\u00adtial execution and the other two parallel schedules are enabled by COMMSET. \nEach of these schedules correspond to three different semantics speci.ed by the programmer. The sequential \nexecution corresponds to the in-order execution of all I/O operations, as im\u00adplied by the unannotated \nprogram. The PS-DSWP schedule cor\u00adresponds to parallel computation of message digests overlapped with \nthe sequential in-order execution of print digest calls. Fi\u00adnally, the DOALL schedule corresponds to \nout-of-order execution of digest computation as well print digests. Every COMMSET block in both the DOALL \nand PS-DSWP schedules is synchronized by the use of locks (provided by libc), while PS-DSWP has addi\u00adtional \ncommunication operations. The DOALL schedule achieves a speedup of 7.6x on eight threads over sequential \nexecution while PS-DSWP schedule gives a speedup of 5.8x. The PS-DSWP sched\u00adule is the result of one \nless COMMSET annotation than the DOALL schedule. The timeline in Figure 3 illustrates the impact of the \nse\u00admantic choices a programmer makes on the freedom provided to a parallelizing tool to enable well performing \nparallelizations. In essence, the COMMSET model allows a programmer to concen\u00adtrate on high-level program \nsemantics while leaving the task of determining the best parallelization strategy and synchronization \nmechanism to the compiler. In doing so, it opens up an parallel per\u00adformance optimization space which \ncan be systematically explored by automatic tools. 3. Syntax and Semantics This section describes the \nsemantics of various COMMSET features and the syntax of the COMMSET primitives. 3.1 CommSet Semantics \nSelf and Group Commutative Sets. The simplest form of se\u00admantic commutativity is a function commuting \nwith itself. A Self COMMSET is de.ned as a singleton set with a code block that is self-commutative. \nAn instantiation of this COMMSET allows for reordering dynamic invocation sequences of the code block. \nA straightforward extension of self-commutativity that allows com\u00admutativity between pairs of functions \nhas quadratic speci.cation complexity. Grouping a set of commuting functions under a name can reduce \nthe speci.cation burden. However, it needs to account for the case when a function commutes with other \nfunctions, but not with itself. For instance, the insert() method in a STL vector implementation does \nnot commute with itself, but commutes with search() on different arguments. A Group COMMSET is a set \nof code blocks where pairs of blocks commute with each other, but each block does not commute with itself. \nThese two concepts to\u00adgether achieve the goal of completeness and conciseness of com\u00admutativity speci.cation. \nDomain of Concurrency. A COMMSET aggregates a set of code blocks that read and modify shared program \nstate. The members are executed concurrently in a larger parallelization scope, with atom\u00adicity of each \nmember of the COMMSET guaranteed by automatic insertion of appropriate synchronization primitives. In \nthis sense, COMMSET plays the role of a concurrency domain, with updates to the shared mutable state \nbeing done in arbitrary order. However, the execution orders of members of a COMMSET with respect to \nthe rest of code (sequential or other COMMSETs) are determined by .ow dependences present in sequential \ncode. Non-transitivity and Multiple Memberships. Semantic commu\u00adtativity is intransitive. Given three \nfunctions f, g, and h where two pairs (f, g), (f, h) semantically commute, the commuting behavior of \n(g, h) cannot be automatically inferred without knowing the se\u00admantics of state shared exclusively between \ng and h. Allowing code blocks to be members of multiple COMMSETs enables expression of either behavior. \nProgrammers may create a single COMMSET with three members or create two COMMSETs with two members each, \ndepending on the intended semantics. Two code blocks com\u00admute if they are both members of at least one \nCOMMSET . Commutative Blocks and Context Sensitivity. In many programs that access generic libraries, \ncommutativity depends on the client code s context. The COMMSET construct is .exible enough to al\u00adlow \nfor commutativity assertions at either interface level or in client code. It also allows arbitrary structured \ncode blocks to commute with other COMMSET members, which can either be functions or structured code blocks \nthemselves. Functions belonging to a mod\u00adule can export optional commuting behaviors of code blocks in \ntheir body by using named block arguments at their interface, with\u00adout requiring any code refactoring. \nThe client code can choose to enable the commuting behavior of the named code blocks at its call site, \nbased on its context. For instance, the mdfile function in Fig\u00adure 1 that has a code block containing \ncalls to fread may expose the commuting behavior of this block at its interface via the use of a named \nblock argument READB. A client which does not care about the order of fread calls can add READB to a \nCOMMSET op\u00adtionally at its call site, while clients requiring sequential order can ignore the named block \nargument. Optional COMMSET speci.ca\u00adtions do not require any changes to the existing interface arguments \nor its implementation.  Predicated Commutative Set. The COMMSET primitive can be predicated on either \ninterface arguments or on variables in a client s program state. A predicate is a C expression associated \nwith the COMMSET and evaluates to a Boolean value when given arguments corresponding to any two members \nof the COMMSET . The pred\u00adicated expression is expected to be pure, i.e. it should return the same value \nwhen invoked with the same arguments. A pure pred\u00adicate expression always gives a deterministic answer \nfor deciding commutativity relations between two functions. The two members commute if the predicate \nevaluates to true when its arguments are bound to values of actual arguments supplied at the point where \nthe members are invoked in sequential code. Orthogonality to Parallelism Form. Semantic commutativity \nre\u00adlations expressed using COMMSET are independent of the speci.c form of parallelism (data/pipeline/task) \nthat are exploited by the associated compiler technology. Once COMMSET annotations are added to sections \nof a sequential program, the same program is amenable to different styles of parallelization. In other \nwords, a single COMMSET application can express commutativity relations between (static) lexical blocks \nof code and dynamic instances of a single code block. The former implicitly enables pipeline paral\u00adlelism \nwhile the latter enables data and pipeline parallelism. Well-de.ned CommSet members. The structure of \nCOMMSET members has to obey certain conditions to ensure well-de.ned se\u00admantics in a parallel setting, \nespecially when used in C/C++ pro\u00adgrams that allow for various unstructured and non-local control .ows. \nThe conditions for ensuring well-de.ned commutative se\u00admantics between members of a COMMSET are: (a) \nThe control .ow constructs within each code block member should be local or structured, i.e. the block \ndoes not include operations like longjmp, setjmp, etc. Statements like break and continue should have \ntheir respective parent structures within the commutative block. (b) There should not be a transitive \ncall from one code block to an\u00adother in the same COMMSET . Removing such call paths between COMMSET members \nnot only avoids the ambiguity in commuta\u00adtivity relation de.ned between a caller and a callee, but also \nsim\u00adpli.es reasoning about deadlock freedom in the parallelized code. Both these conditions are checked \nby the compiler. Well-formedness of CommSets. The concept of well-de.nedness can be extended from code \nblocks in a single COMMSET to mul\u00adtiple COMMSETs by de.ning a COMMSET graph as follows: A COMMSET graph \nis de.ned as a graph where there is a unique node for each COMMSET in the program, and there exists an \nedge from a node S1 to another node S2, if there is a transitive call in the program from a member in \nCOMMSET S1 to a member in COMMSET S2. A set of COMMSETs is de.ned to be well-formed if each COMMSET has \nwell-de.ned members and there is no cycle in the COMMSET graph. Our parallelization system guarantees \ndead\u00adlock freedom in the parallelized program if the only form of paral\u00adlelism in the input program is \nimplicit and is expressed through a well-formed set of COMMSETs. This guarantee holds when either Figure \n4: COMMSET Syntax pipeline or data parallelism is extracted and for both pessimistic and optimistic synchronization \nmechanisms (see Section 4.6).  3.2 CommSet Syntax The COMMSET extensions are expressed using pragma \ndirectives in the sequential program. pragma directives were chosen because a program with well-de.ned \nsequential semantics is obtained when they are elided. Programs with COMMSET annotations can also be \ncompiled without any change by a standard C/C++ compiler that does not understand COMMSET semantics. \nGlobal Declarations. The name of a COMMSET is indicative of its type. By default, the SELF keyword refers \nto a Self COMM-SET, while COMMSETs with other names are Group COMMSETs. To allow for predication of Self \nCOMMSETs, explicit type decla\u00adration can be used. The COMMSETDECL primitive allows for dec\u00adlaration of \nCOMMSETs with arbitrary names at global scope. The COMMSETPREDICATE primitive is used to associate a \npredicate with a COMMSET and is declared at global scope. The primitive takes as arguments: (a) the name \nof the COMMSET that is pred\u00adicated, (b) a pair of parameter lists, and (c) a C expression that represents \nthe predicate. Each parameter list represents the subset of program state that decides the commuting \nbehavior of a pair of COMMSET members, when they are executed in two different par\u00adallel execution contexts. \nThe parameters in the lists are bound to either a commutative function s arguments, or to variables in \nthe client s program state that are live at the beginning of a structured commutative code block. The \nC expression computes a Boolean value using the variables in the parameter list and returns true if a \npair of COMMSET members commute when invoked with the appropriate arguments. By default, COMMSET members \nare auto\u00admatically synchronized when their source code is available to the parallelizing compiler. A \nprogrammer can optionally specify that a COMMSET does not need compiler inserted synchronization us\u00ading \nCOMMSETNOSYNC. The primitive is applied to COMMSETs whose members belong to a thread-safe library which \nhas been sep\u00adarately compiled and whose source is unavailable. Instance Declarations and CommSet List. \nA code block can be declared a member of a list of COMMSETs by using the COMM-SET directive. Such instance \ndeclarations can be applied either at a function interface, or at any point in a loop or a function for \nadding an arbitrary structured code block (a compound statement in C/C++) to a COMMSET. Both compound \nstatements and func\u00adtions are treated in the same way as far as reasoning about com\u00admutativity is concerned. \nIn the case of predicated COMMSETs in the COMMSET list, the actual arguments for the COMMSETPRED-ICATE \nare supplied at the instance declaration. For function mem\u00adbers of a COMMSET, the actual arguments are \na list of parameter declarations, while for compound statements, the actual arguments are a set of variables \nwith primitive type that have a well-de.ned value at the beginning of the compound statement. Optionally \ncom\u00admuting compound statements can be given a name by enclosing the statements within COMMSETNAMEDBLOCK \ndirective. A function containing such a named block can expose the commuting option to client code using \nCOMMSETNAMEDARG at its interface declara\u00adtion. The client code that invokes the function can enable the \ncom\u00admuting behavior of the named block by adding it to a COMMSET using the COMMSETNAMEDARGADD directive \nat its call site.  3.3 Example Figure 1 shows the implicitly parallel program obtained by ex\u00adtending \nmd5sum with COMMSET primitives. The code blocks B, H, I enclosing the .le operations are added to a Group \nCOMM-SET FSET using annotations 5, 7, and 8. Each code block is also added to its own Self COMMSET. FSET \nis predicated on the loop induction variable s value, using a COMMSETPREDICATE expres\u00adsion (3) to indicate \nthat each of the .le operations commute with each other on separate iterations. The block containing \nfread call is named READB (10) and exported by mdfile using the COMM-SETNAMEDARG directive at its interface \n(9). The client code adds the named block to its own Self set (declared as SSET in 2) using the COMMSETNAMEDARGADD \ndirective at 6. SSET is predicated on the outer loop induction variable to prevent commuting across inner \nloop invocations (4). A deterministic output can be obtained by omitting SELF from annotation 7. 4. Commutative \nSet Implementation We built an end-to-end implementation of COMMSET within a par\u00adallelizing compiler. \nThe compiler is an extension of the clang/L-LVM framework [19]. Figure 5 shows the parallelization work.ow. \nThe parallelization focuses on hot loops in the program identi.ed via runtime pro.ling. The PDG for the \nhottest loop is constructed over the LLVM IR, with each node representing an instruction in the IR. The \nmemory .ow dependences in the PDG that inhibit par\u00adallelization are displayed at source level to the \nprogrammer, who inserts COMMSET primitives and presents the program back to the compiler. The subsequent \ncompiler passes analyze and transform this program to generate different versions of parallelized code. \n4.1 Frontend The COMMSET parser in the frontend parses and checks the syntax of all COMMSET directives, \nand synthesizes a C function for ev\u00adery COMMSETPREDICATE. The predicate function computes the value of \nthe C expression speci.ed in the directive. The argument types for the function are automatically inferred \nby binding the parameters in COMMSETPREDICATE to the COMMSET instances. Type mismatch errors between \narguments of different COMMSET instances are also detected. Commutative blocks are checked for enclosing \nnon-local control .ow by a top-down traversal of the ab\u00adstract syntax tree (AST) starting at the node \ncorresponding to the particular commutative block. Finally, global COMMSET meta-data is annotated at \nthe module level, while COMMSET instance data is annotated on individual compound statement or function \nAST nodes. This meta-data is automatically conveyed to the backend during the lowering phase. Figure \n5: COMMSET Parallelization Work.ow 4.2 CommSet Metadata Manager In the backend, the COMMSET meta-data \nis an abstraction over the low-level IR constructs, instead of the AST nodes. The COMM-SET Metadata Manager \nprocesses and maintains a meta-data store for all COMMSET instances and declarations, and answers queries \nposed by subsequent compiler passes. The .rst pass of the manager canonicalizes each commutative compound \nstatement, now a struc\u00adtured region (set of basic blocks) within the control .ow graph, by extracting \nthe region into its own function. Nested commuta\u00adtive regions are extracted correctly by a post-order \ntraversal on the control .ow graph (CFG). The extraction process ensures that argu\u00adments speci.ed at \na COMMSET instance declaration are parameters to the newly created function. At the end of this pass, \nall the mem\u00adbers of a COMMSET are functions. Call sites enabling optionally commutative named code blocks \nare inlined to clone the call path from the enabling function call to the COMMSETNAMEDBLOCK declaration. \nA robust implementation can avoid potential code ex\u00adplosion by automatically extending the interface \nsignature to take in additional arguments for optional commuting blocks. Next, each COMMSET is checked \nfor well-formedness using reachability and cycle detection algorithms on the call graph and the COMMSET \ngraphrespectively.The COMMSETPREDICATE functionsaretested for purity by inspection of its body.  4.3 \nPDG Builder The PDG builder constructs the PDG over the LLVM IR instruc\u00adtions for the target loop using \nwell-known algorithms [12]. A loop carried dependence detector module annotates dependence edges  Algorithm \n1: CommSetDepAnalysis let n1 = src(e); let n2 = dst(e); 3 if typeOf(n1) = Call . typeOf(n2)= Call then \n4 continue 5 end 6 let Fn(n1)= f(x1,...,xn) and Fn(n2)= g(y1,...,yn);; 7 let Sin = CommSets(f) n CommSets(g); \n8 foreach Cs . Sin do 9 if not Predicated(Cs) then 10 Annotate(e, P DG, uco); 11 end 12 else 13 let fp \n= P redicateF n(Cs); 14 let args1 = CommSetArgs(Cs,f); 15 let args2 = CommSetArgs(Cs,g); 16 let fargs \n= F ormalArgs(fp); 17 for i =0 to | args1 - 1 | do 18 let x1 = args1(i) ; let x2 = args2(i); 19 let y1 \n= fargs(2 * i) ; let y2 = fargs(2 * i + 1); 20 Assert(x1 = y1); Assert(x2 = y2); 21 end 22 if LoopCarried(e) \nthen 23 Assert(i1 induction = i2); // variable; 24 r = SymInterpret(Body(fp), true); 25 if (r = true) \nand (Dom(n2, n1)) then 26 Annotate(e, P DG, uco); 27 end 28 else if (r = true) then 29 Annotate(e, P \nDG, ico); 30 end 31 end 32 else 33 r = SymInterpret(Body(fp), true); 34 if (r = true) then 35 Annotate(e, \nP DG, uco); 36 end 37 end 38 end 39 end 40 end as being loop carried whenever the source and/or destination \nnodes read and update shared memory state.  4.4 CommSet Dependence Analyzer The COMMSET Dependence Analyzer \n(Algorithm 1) uses the COMMSET metadata to annotate memory dependence edges as being either unconditionally \ncommutative (uco) or inter-iteration commutative (ico). Figure 2 shows the PDG edges for md5sum an\u00adnotated \nwith commutativity properties along with the correspond\u00ading source annotations. For every memory dependence \nedge in the PDG, if there exists an unpredicated COMMSET of which both the source and destination s target \nfunctions are members, the edge is annotated as uco (Lines 9-11). For a predicated COMMSET, the actual \narguments of the target functions at their call sites are bound to corresponding formal parameters of \nthe COMMSETPREDICATE function (Lines 17-19). The body of the predicate function is then symbolically \ninterpreted to prove that it always returns true, given the inequality assertions about induction variable \nvalues on sepa\u00adrate iterations (Lines 21-22). If the interpreter returns true for the current pair of \nCOMMSET instances, the edge is annotated with a commutativity property as follows: A loop carried dependence \nis annotated as uco if the destination node of the PDG edge dominates the source node in the CFG (Lines \n23-34), otherwise it is annotated as ico (Lines 26-27). An intra-iteration dependence edge is always \nannotated as uco if the predicate is proven to be true (Lines 32-34). Once the commutative annotations \nare added to the PDG, the PDG builder is invoked again to identify strongly connected components (SCC) \n[16]. The directed acyclic graph of SCCs (DAG-SCC) thus obtained forms the basis of DSWP family of algorithms \n[24]. 4.5 Parallelizing Transforms The next step runs the DOALL and PS-DSWP parallelizing trans\u00adforms, \nwhich automatically partition the PDG onto multiple threads for extracting maximal data and pipelined \nparallelism respectively. For all the parallelizing transforms, the ico edges are treated as intra-iteration \ndependence edges, while uco edges are treated as non-existent edges in the PDG. The DOALL transform tests \nthe PDG for absence of inter-iteration dependencies, and statically schedules a set of iterations to \nrun in parallel on multiple threads. The DSWP family of transforms partition the DAG-SCC into a sequence \nof pipeline stages, using pro.le data to obtain a bal\u00adanced pipeline. The DSWP algorithm [25] only generates \nsequen\u00adtial stages, while the PS-DSWP algorithm [26] can replicate a stage with no loop carried SCCs \nto run in parallel on multiple threads. Dependences between stages are communicated via lock\u00adfree queues \nin software. Together, the uco and ico annotations on the PDG enable DOALL, DSWP, and PS-DSWP transforms \nwhen previously they were not applicable. Currently, the compiler gener\u00adates one of each (DSWP, PS-DSWP, \nand DOALL) schedule when\u00adever applicable, with a corresponding performance estimate. A pro\u00adduction quality \ncompiler would typically use heuristics to select the optimal across all parallelization schemes.  4.6 \nCommSet Synchronization Engine This step automatically inserts synchronization primitives to ensure atomicity \nof COMMSET members with respect to each other, taking multiple COMMSET memberships into account. The \ncompiler gen\u00aderates a separate parallel version for every synchronization method used. Currently three \nsynchronization modes are supported: op\u00adtimistic (via Intel s transactional memory (TM) runtime [33]), \npessimistic (mutex and spin locks) and lib (well known thread safe libraries or programmer speci.ed synchronization \nsafety for a COMMSET). Initially, the algorithm assigns a unique rank to each COMMSET which determines \nthe global order of lock acquires and releases. The next step determines the set of potential synchro\u00adnization \nmechanisms that apply to a COMMSET. Synchronization primitives are inserted for each member of a COMMSET \nby taking into account the other COMMSETs it is a part of. In the case of TM, a new version of the member \nwrapped around transactional constructs is generated. For the lock based synchronizations, lock acquires \nand releases are inserted according to the assigned global rank order. The global ordering along with \nthe acyclic communica\u00adtion primitives that use the lock free queues preserve the invariants required \nto ensure deadlock freedom [20]. 5. Evaluation The COMMSET programmingmodelwasevaluatedonasetofeight \nprograms shown in Table 2. The programs were selected from a repository sourced from a variety of benchmark \nsuites. Seventeen randomly selected programs with potential parallelism-inhibiting memory .ow dependencies \nwere examined. Out of these, programs whose hottest loops did not require any semantic changes for par\u00adallelization \nwere omitted. For the remaining programs, COMMSET primitives were applied to relax certain execution \norders after a careful examination of the intended program semantics. Apart from evaluating parallelization \nschemes enabled by COMMSET primi\u00adtives in these programs, alternative parallelization schemes without \nusing COMMSET primitives were also evaluated whenever applica\u00adble. Figure 6 shows the speedup of the \nparallelized programs run\u00adning on a 1.6GHz Intel Xeon 64-bit dual-socket quad core machine with 8GB RAM \nrunning Linux 2.6.24.  Program Origin Main loop Exec. Lines of Code COMMSET Attributes Parallelizing \nTransforms Best Speedup Best Scheme Time # COMMSET Annotations Total SLOC md5sum Open Src [2] main 100% \n10 399 PC, C, S&#38;G DOALL, PS-DSWP 7.6x DOALL + Lib 456.hmmer SPEC2006 [14] main loop serial 99% 9 \n20658 PC, C&#38;I, S&#38;G DOALL, PS-DSWP 5.8x DOALL + Spin geti MineBench [23] FindSomeETIs 98% 11 889 \nPI&#38;PC, C&#38;I, S&#38;G DOALL, PS-DSWP 3.6x PS-DSWP + Lib ECLAT MineBench [23] newApriori 97% 11 \n3271 PC, C&#38;I, S&#38;G DOALL, DSWP 7.5x DOALL + Mutex em3d Olden [9] initialize graph 97% 8 464 I, \nS&#38;G DSWP, PS-DSWP 5.8x PS-DSWP + Lib potrace Open Src [29] main 100% 10 8292 PC, C, S&#38;G DOALL, \nPS-DSWP 5.5x DOALL + Lib kmeans STAMP [22] work 99% 1 516 C, S DOALL, PS-DSWP 5.2x PS-DSWP url NetBench \n[21] main 100% 2 629 I, S DOALL, PS-DSWP 7.7x DOALL + Spin Table 2: Sequential Programs evaluated, their \norigin, execution time spent in target loop, number of COMMSET annotations over sequential code, total \nnumber of lines of source code, COMMSET features applied (PI: Predication at Interface, PC: Predication \nat Client, C: Commuting Blocks, I: Interface Commutativity, S: Self Commutativity, G: Group Commutativity), \nParallelizing Transforms, Best Speedup Obtained on eight threads, and corresponding Parallelization Scheme \nwith synchronization mechanism (Mutex: Mutex locks, Spin: Spin locks, Lib: Thread-safe Libraries). (a) \nmd5sum (b) 456.hmmer (c) geti (d) ECLAT (e) em3d (f) potrace (g) kmeans (h) url (i) Geomean   Figure \n6: Performance of DOALL and PS-DSWP schemes using COMMSET extensions. Parallelization schemes in each \ngraph s legend are sorted in decreasing order of speedup on eight threads, from top to bottom. The DSWP \n+ [...] notation indicates the DSWP technique with stage details within [...] (where S denotes a sequential \nstage and DOALL denotes a parallel stage). Schemes with Comm-pre.x were enabled only by the use of COMMSET \n. For each program, the best Non-COMMSET parallelization scheme, obtained by ignoring the COMMSET extensions \nis also shown. In some cases, this was sequential execution. 5.1 456.hmmer: Biological Sequence Analysis \n456.hmmer performs biosequence analysis using Hidden Markov Models. Every iteration of the main loop \ngenerates a new protein sequence via calls to a random number generator (RNG). It then computes a score \nfor the sequence using a dynamically allocated matrix data structure, which is used to update a histogram \nstruc\u00adture. Finally the matrix is deallocated at the end of the iteration. By applying COMMSET annotations \nat three sites, all loop car\u00adried dependences were broken: (a) The RNG was added to a SELF COMMSET, since \nany permutation of a random number sequence still preserves the properties of the distribution. (b) The \nhistogram update operation was also marked self commuting, as it performs an abstract SUM operation even \nthough the low-level statements involve .oating point additions and subtractions. (c) The matrix allocation \nand deallocation functions were marked as commuting with themselves on separate iterations. Overall, \nthe DOALL par\u00adallelization using spin locks performs best for eight threads, with a program speedup of \nabout 5.82x. A spin lock works better than mutex since it does not suffer from sleep/wakeup overheads \nin the midst of highly contended operations on the RNG seed variable. The three stage PS-DSWP pipeline, \ngives a speedup of 5.3x (doing better than the mutex and TM versions of DOALL) by moving the RNG to a \nsequential stage, off the critical path. 5.2 GETI: Greedy Error Tolerant Itemsets GETI is a C++ data \nmining program that determines a set of fre\u00adquent items that are bought together frequently in customer \ntransac\u00adtions (itemsets). Itemsets are implemented as Bitmap objects, with items acting as keys. Items \nare queried and inserted into the Bitmap by calls to SetBit() and GetBit(). Each itemset is inserted \ninto an STL vector and then printed to the console. By adding COMM-SET annotations at three sites, the \nmain loop was completely par\u00adallelizable with DOALL and PS-DSWP: (a) Itemset constructors and destructors \nare added to a COMMSET and allowed to com\u00admute on separate iterations. (b) SetBit() and GetBit() interfaces \nwere put in a COMMSET predicated on the input key values, to al\u00adlow for insertions of multiple items \nto occur out of order. (c) The code block with vector::push back() and prints was context sensitively \nmarked as self commutative in client code. The correct\u00adness of this application follows from the set \nsemantics associated with the output. The inter-iteration commutativity properties for constructor/destructor \npairs enabled a well performing three-stage PS-DSWP schedule. Transactions were not applicable due to \nuse of external libraries and I/O. Although DOALL schemes initially did better than PS-DSWP, the effects \nof buffering output indirectly via lock-free queues for PS-DSWP and the increasing number of acquire/release \noperations for DOALL led to a better performing schedule for PS-DSWP on eight threads. PS-DSWP achieved \na lim\u00adited speedup of 3.6x due to the sequential time taken for console prints but maintained deterministic \nbehavior of the program. 5.3 ECLAT: Association Rule Mining ECLAT is a C++ program that computes a list \nof frequent item\u00adsets using a vertical database. The main loop updates objects of two classes Itemset \nand Lists<Itemset*>. Both are internally implemented as lists, the former as a client de.ned class, and \nthe latter as an instantiation of a generic class. Insertions into Itemset have to preserve the sequential \norder, since the Itemset intersec\u00adtion code depends on a deterministic pre.x. Insertions into the Lists<Itemset*> \ncan be done out of order, due to set seman\u00adtics attached with the output. COMMSET extensions were applied \nat four sites: (a) Database read calls (that mutate shared .le de\u00adscriptors internally) were marked as \nself commutative. (b) Inser\u00adtions into Lists<Itemset*> are context-sensitively tagged as self commuting \ninside the loop. Note that it would be incorrect to tag Itemset insertions as self-commuting as it would \nbreak the in\u00adtersection code. (c) Object construction and destruction operations were marked as commuting \non separate iterations. (d) Methods be\u00adlonging to Stats class that computes statistics were added to \na un\u00adpredicated Group COMMSET. A speedup of 7.4x with DOALL was obtained, despite pessimistic synchronization, \ndue to a larger frac\u00adtion of time spent in computation outside critical sections. Transac\u00adtions are not \napplicable due to use of I/O operations. The PS-DSWP transform, using all the COMMSET properties generates \na schedule (not shown) similar to DOALL. The next best schedule is from DSWP, that does not leverage \nCOMMSET properties on database read. The resulting DAG-SCC has a single SCC corresponding to the entire \ninner for loop, preventing stage replication. 5.4 em3d: Electro-magnetic Wave propagation em3d simulates \nelectromagnetic wave propagation using a bipartite graph. The outer loop of the graph construction iterates \nthrough a linked list of nodes in a partition, while the inner loop uses a RNG to select a new neighbor \nfor the current node. Allowing the RNG routine to execute out of order enabled PS-DSWP. The pro\u00adgram \nuses a common RNG library, with routines for returning ran\u00addom numbers of different data types, all of \nwhich update a shared seed variable. All these routines were added to a common Group COMMSET and also \nto their own SELF COMMSET . COMMSET speci.cations to indicate commutativity between the RNG routines \nrequired only eight annotations, while specifying pair-wise com\u00admutativity would have required 16 annotations. \nSince the loop does a linked list traversal, DOALL was not applicable. Without commu\u00adtativity, DSWP extracts \na two-stage pipeline at the outer loop level, yielding a speedup of 1.2x. The PS-DSWP scheme enabled \nby COMMSET directives achieves a speedup of 5.9x on eight threads. A linear speedup was not obtained \ndue to the short execution time of the original instructions in the main loop, which made the over\u00adhead \nof inter-thread communication slightly more pronounced. 5.5 potrace: Bitmap tracing potrace vectorizes \na set of bitmaps into smooth, scalable images. The code pattern is similar to md5sum, with an additional \noption of writing multiple output images into a single .le. In the code section with the option enabled, \nthe SELF COMMSET annotation was omitted on .le output calls to ensure sequential output semantics. The \nDOALL parallelization yielded a speedup of 5.5x, peaking at 7 threads, after which I/O costs dominate \nthe runtime. For the PS-DSWP parallelization, the sequentiality of image writes limited speedup to 2.2x \non eight threads.  5.6 kmeans: K means clustering algorithm kmeans clusters high dimensional objects \ninto similar featured groups. The main loop computes the nearest cluster center for each object and updates \nthe center s features using the current object. The updates to a cluster center can be re-ordered, with \neach such or\u00adder resulting in a different but valid cluster assignment. Adding the code block that performs \nthe update to a SELF COMMSET breaks the only loop carried dependence in the loop. The DOALL scheme with \npessimistic synchronization showed promising speedup until .ve threads (4x), beyond which frequent cache \nmisses due to failed lock/unlock operations resulted in performance degradation. Trans\u00adactions (not shown) \ndo not help either, with speedup limited to 2.7x on eight threads. The three-stage PS-DSWP scheme was \nbest per\u00adforming beyond six threads, showing an almost linear performance increase by executing the cluster \nupdate operation in a third se\u00adquential stage. It achieved a speedup of 5.2x on eight threads. This highlights \nthe performance gains achieved by moving highly con\u00adtended dependence cycles onto a sequential stage, \nan important insight behind the DSWP family of transforms.  5.7 URL: url based switching The main loop \nin the program switches a set of incoming pack\u00adets based on its URL and logs some of the packet s .elds \ninto a .le. The underlying protocol semantics allows out-of-order packet switching. Adding the function \nto dequeue a packet from the packet pool and the logging function to SELF COMMSETs broke all the loop \ncarried .ow dependences. No synchronization was necessary for the logging function while locks were automatically \ninserted to synchronize multiple calls to the packet dequeuing function. A two stage PS-DSWP pipeline \nwas also formed by ignoring the SELF COMMSET annotation on the packet dequeue function. The DOALL parallelization \n(7.7x speedup on eight threads) outper\u00adforms the PS-DSWP version (3.7x on eight threads) because of low \nlock contention on the dequeue function and the overlapped parallel execution of the packet matching \ncomputation.  5.8 Discussion The application of COMMSET achieved a geomean speedup of 5.7x on eight \nthreads for the programs listed in Table 2, while the ge\u00adomean speedup for Non-COMMSET parallelizations \nis 1.49x (Fig\u00adure 6i). For four out of the eight programs, the main loop was not parallelizable at all \nwithout the use of COMMSET primitives. With the application of COMMSET, DOALL parallelization per\u00adforms \nbetter than PS-DSWP on 5 benchmarks, although PS-DSWP has the advantage of preserving deterministic output \nin two of them. For two of the remaining programs, PS-DSWP yields bet\u00adter speedup since its sequential \nlast stage performs better than con\u00adcurrently executing COMMSET blocks in the high lock contention scenarios. \nDOALL was not applicable for em3d, due to pointer chasing code. In terms of programmer effort, an average \nof 8 lines of COMMSET annotations were added to each program to enable the various parallelization schemes. \nPredication based on the client state, as a function of the induction variable enabled well perform\u00ading \nparallelizations without the need for runtime checks. The use of commuting blocks avoided the need for \nmajor code refactoring. The applicability of COMMSET compared favorably to the appli\u00adcability of other \ncompiler based techniques like Paralax and VE-LOCITY. VELOCITY and Paralax cannot be used to parallelize \nfour benchmarks: geti, eclat, md5sum and potrace since they do not support predicated commutativity. \nFor 456.hmmer, VELOC-ITY would require a modi.cation of 45 lines of code (addition of 43 lines and removal \nof 2 lines) in addition to the commutativity annotations. COMMSET did not require those changes due to \nthe use of named commuting blocks. 6. Related Work Semantic Commutativity based Parallelizing Systems. \nJade [27] supports object-level commuting assertions to specify commuta\u00adtivity between every pair of \noperations on an object. Additionally, Jade exploits programmer written read/write speci.cations for \nex\u00adploiting task and pipeline parallelism. The COMMSET solution re\u00adlies on static analysis to avoid read/write \nspeci.cations and runtime pro.les to select loops for parallelization. Galois [18], a runtime system \nfor optimistic parallelism, leverages commutativity asser\u00adtions on method interfaces. It requires programmers \nto use special set abstractions with non-standard semantics to enable data paral\u00adlelism. The COMMSET \ncompiler currently does not implement run\u00adtime checking of COMMSETPREDICATEs required for optimistic \nparallelism. However, the COMMSET model is able to extract both data and pipelined parallelism without \nrequiring any additional pro\u00adgramming extensions. DPJ [5], an explicitly parallel extension of Java uses \ncommutativity annotations at function interfaces to over\u00adride restrictions placed by the type and effect \nsystem of Java. Sev\u00aderal researchers have also applied commutativity properties for se\u00admantic concurrency \ncontrol in explicitly parallel settings [10, 17]. Paralax [30] and VELOCITY [6, 7] exploit self-commutativity \nat the interface level to enable pipelined parallelization. VELOCITY also provides special semantics \nfor commutativity between pairs of memory allocation routines for use in speculative parallelization. \nCompared to these approaches, the COMMSET language extension provides richer commutativity expressions. \nExtending the compiler with a speculative system to support all COMMSET features at run\u00adtime is part \nof future work. Table 1 summarizes the relationship between COMMSET and the above programming models. \nImplicit Parallel Programming. OpenMP [3] extensions re\u00adquire programmers to explicitly specify parallelization \nstrategy and concurrency control using additional primitives ( #pragma omp for , #pragma omp task , critical \n, barrier , etc.). In the COMMSET model, the choice of parallelization strategy and concurrency control \nis left to the compiler. This not only frees the programmer from having to worry about low-level parallelization \ndetails, but also promotes performance portability. Implicit par\u00adallelism in functional languages have \nbeen studied recently [13]. IPOT [31] exploits semantic annotations on data to enable par\u00adallelization. \nHwu et al. [15] also propose annotations on data to enable implicit parallelism. COMMSET extensions are \napplied to code rather than data, and some annotations like reduction pro\u00adposed in IPOT can be easily \nintegrated with COMMSET. Compiler Parallelization. Research on parallelizing FORTRAN loops with regular \nmemory accesses [11, 16] in the past has been complemented by more recent work on irregular programs. \nThe container-aware [32] compiler transformations parallelize loops with repeated patterns of collections \nusage. Existing versions of these transforms preserve sequential semantics. The COMMSET compiler can \nbe extended to support these and other parallelizing transforms without changes to the language extension. \nCommutativity Analysis. Rinard et al. [28] proposed a static analysis that determines if commuting two \nmethod calls preserves concrete memory state. Aleen et al. [1] apply random interpre\u00adtation to probabilistically \ndetermine function calls that commute subject to preservation of sequential I/O semantics. Programmer \nwritten commutativity assertions are more general since they allow multiple legal outcomes and give a \nprogrammer more .exibility to express intended semantics within a sequential setting. 7. Conclusion This \npaper presented an implicit parallel programming solution based on a uni.ed, syntactically succinct and \ngeneralized seman\u00adtic commutativity construct called COMMSET. The model provides programmers the .exibility \nto specify commutativity relations be\u00adtween arbitrary structured blocks of code and does not require \nthe use of any additional parallel constructs. Parallelism exposed im\u00adplicitly using COMMSET is independent \nof any particular paral\u00adlelization strategy or concurrency control mechanism. A complete end-to-end implementation \nof COMMSET exists in a parallelizing compiler. Evaluation of eight real world programs indicates that \nthe use of COMMSET extensions and the associated compiler tech\u00adnology enables scalable parallelization \nof programs hitherto not amenable to automatic parallelization. This demonstrates the ef\u00adfectiveness \nof the COMMSET construct in exposing different paral\u00adlelization opportunities to the compiler. Acknowledgments \nWe thank the entire Liberty Research Group for their support and feedback during this work. We also thank \nthe anonymous reviewers for their insightful comments. This material is based on work sup\u00adported by National \nScience Foundation Grants 1047879, 0964328, and 0627650, and United States Air Force Contract FA8650-09-C\u00ad7918. \n References [1] F. Aleen and N. Clark. Commutativity analysis for software parallelization: Letting \nprogram transformations see the big picture. In Proceedings of the 14th International Conference on Architectural \nSupport for Programming Languages and Operating Systems (ASPLOS), 2009. [2] Apple Open Source. md5sum: \nMessage Digest 5 computation. http://www.opensource.apple.com/darwinsource/. [3] E. Ayguad\u00b4e, N. Copty, \nA. Duran, J. Hoe.inger, Y. Lin, F. Mas\u00adsaioli, X. Teruel, P. Unnikrishnan, and G. Zhang. The design of \nOpenMP tasks. IEEE Transactions on Parallel and Dis\u00adtributed Systems, 2009. [4] G. E. Blelloch and J. \nGreiner. A provable time and space ef.cient implementation of NESL. In Proceedings of the First ACM SIGPLAN \nInternational Conference on Functional Programming (ICFP), 1996. [5] R. L. Bocchino, Jr., V. S. Adve, \nD. Dig, S. V. Adve, S. Heumann, R. Komuravelli, J. Overbey, P. Simmons, H. Sung, and M. Vakilian. A type \nand effect system for Deter\u00administic Parallel Java. In Proceedings of the 24th ACM SIG-PLAN Conference \non Object Oriented Programming Systems, Languages, and Applications (OOPSLA), 2009. [6] M. Bridges, N. \nVachharajani, Y. Zhang, T. Jablin, and D. Au\u00adgust. Revisiting the sequential programming model for multi\u00adcore. \nIn Proceedings of the 40th Annual IEEE/ACM Interna\u00adtional Symposium on Microarchitecture (MICRO), 2007. \n[7] M. J. Bridges. The VELOCITY compiler: Extracting ef.cient multicore execution from legacy sequential \ncodes. PhD thesis, 2008. [8] D. R. Butenhof. Programming with POSIX threads. Addison-Wesley Longman Publishing \nCo., Inc., 1997. [9] M. C. Carlisle. Olden: Parallelizing programs with dynamic data structures on distributed-memory \nmachines. PhD thesis, 1996. [10] B. D. Carlstrom, A. McDonald, M. Carbin, C. Kozyrakis, and K. Olukotun. \nTransactional collection classes. In Proceedings of the 12th ACM SIGPLAN Symposium on Principles and \nPractice of Parallel Programming (PPoPP), 2007. [11] R. Eigenmann, J. Hoe.inger, Z. Li, and D. A. Padua. \nEx\u00adperience in the automatic parallelization of four Perfect\u00adbenchmark programs. In Proceedings of the \nFourth Inter\u00adnational Workshop on Languages and Compilers for Parallel Computing (LCPC), 1992. [12] J. \nFerrante, K. J. Ottenstein, and J. D. Warren. The program dependence graph and its use in optimization. \nACM Trans. Program. Lang. Syst., 9(3), 1987. [13] T. Harris and S. Singh. Feedback directed implicit \nparallelism. In Proceedings of the 12th ACM SIGPLAN International Con\u00adference on Functional Programming \n(ICFP), 2007. [14] J. L. Henning. SPEC CPU2006 benchmark descriptions. SIGARCH Comput. Archit. News, \n2006. [15] W.-m. Hwu, S. Ryoo, S.-Z. Ueng, J. Kelm, I. Gelado, S. Stone, R. Kidd, S. Baghsorkhi, A. Mahesri, \nS. Tsao, N. Navarro, S. Lumetta, M. Frank, and S. Patel. Implicitly parallel pro\u00adgramming models for \nthousand-core microprocessors. In Pro\u00adceedings of the 44th annual Design Automation Conference (DAC), \n2007. [16] K. Kennedy and J. R. Allen. Optimizing Compilers for Mod\u00adern Architectures: a Dependence-based \nApproach. Morgan Kaufmann Publishers Inc., 2002. [17] E. Koskinen, M. Parkinson, and M. Herlihy. Coarse-grained \ntransactions. In Proceedings of the 37th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Program\u00adming \nLanguages (POPL), 2010. [18] M. Kulkarni, K. Pingali, B. Walter, G. Ramanarayanan, K. Bala, and L. P. \nChew. Optimistic parallelism requires ab\u00adstractions. In Proceedings of the 2007 ACM SIGPLAN Confer\u00adence \non Programming Language Design and Implementation (PLDI). [19] C. Lattner and V. Adve. LLVM: A compilation \nframework for lifelong program analysis and transformation. In Proceedings of 2nd International Symposium \non Code Generation and Optimization (CGO), 2004. [20] R. Leino, P. M\u00a8 Deadlock-free channels uller, and \nJ. Smans. and locks. In Proceedings of the 19th European Symposium on Programming (ESOP), 2010. [21] \nG. Memik, W. H. Mangione-Smith, and W. Hu. NetBench: a benchmarking suite for network processors. In \nProceedings of the 2001 IEEE/ACM International Conference on Computer-Aided Design (ICCAD), 2001. [22] \nC. C. Minh, J. Chung, C. Kozyrakis, and K. Olukotun. STAMP: Stanford Transactional Applications for Multi-Processing. \nIn IEEE International Symposium on Workload Characterization (IISWC), 2008. [23] R. Narayanan, B. Ozisikyilmaz, \nJ. Zambreno, G. Memik, and A. Choudhary. MineBench: A benchmark suite for data min\u00ading workloads. In \nIEEE International Symposium on Work\u00adload Characterization (IIWSC), 2006. [24] G. Ottoni. Global Instruction \nScheduling for Multi-Threaded Architectures. PhD thesis, 2008. [25] G. Ottoni, R. Rangan, A. Stoler, \nand D. I. August. Automatic thread extraction with decoupled software pipelining. Pro\u00adceedings of the \n38th annual IEEE/ACM International Sympo\u00adsium on Microarchitecture (MICRO), 2005. [26] E. Raman, G. Ottoni, \nA. Raman, M. J. Bridges, and D. I. Au\u00adgust. Parallel-stage decoupled software pipelining. In Pro\u00adceedings \nof the 6th annual IEEE/ACM International Sympo\u00adsium on Code Generation and Optimization (CGO), 2008. \n[27] M. C. Rinard. The design, implementation and evaluation of Jade, a portable, implicitly parallel \nprogramming language. PhD thesis, 1994. [28] M. C. Rinard and P. Diniz. Commutativity analysis: A new \nanalysis framework for parallelizing compilers. In Proceed\u00adings of the ACM SIGPLAN 1996 Conference on \nProgramming Language Design and Implementation (PLDI). [29] P. Selinger. potrace: Transforming bitmaps \ninto vector graph\u00adics. http://potrace.sourceforge.net. [30] H. Vandierendonck, S. Rul, and K. De Bosschere. \nThe Paralax infrastructure: Automatic parallelization with a helping hand. In Proceedings of the 19th \nInternational Conference on Paral\u00adlel Architectures and Compilation Techniques (PACT), 2010. [31] C. \nvon Praun, L. Ceze, and C. Cas\u00b8caval. Implicit parallelism with ordered transactions. In Proceedings \nof the 12th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming (PPoPP), 2007. [32] \nP. Wu and D. A. Padua. Beyond arrays -a container-centric approach for parallelization of real-world \nsymbolic applica\u00adtions. In Proceedings of the 11th International Workshop on Languages and Compilers \nfor Parallel Computing (LCPC), 1999. [33] R. M. Yoo, Y. Ni, A. Welc, B. Saha, A.-R. Adl-Tabatabai, and \nH.-H. S. Lee. Kicking the tires of software transactional memory: Why the going gets tough. In Proceedings \nof the Twentieth Annual Symposium on Parallelism in Algorithms and Architectures (SPAA), 2008. [34] H. \nZhong, M. Mehrara, S. Lieberman, and S. Mahlke. Un\u00adcovering hidden loop level parallelism in sequential \napplica\u00adtions. In Proceedings of 14th International Conference on High-Performance Computer Architecture \n(HPCA), 2008.   \n\t\t\t", "proc_id": "1993498", "abstract": "<p>Sequential programming models express a total program order, of which a partial order must be respected. This inhibits parallelizing tools from extracting scalable performance. Programmer written semantic commutativity assertions provide a natural way of relaxing this partial order, thereby exposing parallelism implicitly in a program. Existing implicit parallel programming models based on semantic commutativity either require additional programming extensions, or have limited expressiveness. This paper presents a generalized semantic commutativity based programming extension, called Commutative Set (COMMSET), and associated compiler technology that enables multiple forms of parallelism. COMMSET expressions are syntactically succinct and enable the programmer to specify commutativity relations between groups of arbitrary structured code blocks. Using only this construct, serializing constraints that inhibit parallelization can be relaxed, independent of any particular parallelization strategy or concurrency control mechanism. COMMSET enables well performing parallelizations in cases where they were inapplicable or non-performing before. By extending eight sequential programs with only 8 annotations per program on average, COMMSET and the associated compiler technology produced a geomean speedup of 5.7x on eight cores compared to 1.5x for the best non-COMMSET parallelization.</p>", "authors": [{"name": "Prakash Prabhu", "author_profile_id": "81464645003", "affiliation": "Princeton University, Princeton, NJ, USA", "person_id": "P2690447", "email_address": "pprabhu@cs.princeton.edu", "orcid_id": ""}, {"name": "Soumyadeep Ghosh", "author_profile_id": "81485642205", "affiliation": "Princeton University, Princeton, NJ, USA", "person_id": "P2690448", "email_address": "soumyade@cs.princeton.edu", "orcid_id": ""}, {"name": "Yun Zhang", "author_profile_id": "81350571220", "affiliation": "Princeton University, Princeton, NJ, USA", "person_id": "P2690449", "email_address": "yunzhang@cs.princeton.edu", "orcid_id": ""}, {"name": "Nick P. Johnson", "author_profile_id": "81470644754", "affiliation": "Princeton University, Princeton, NJ, USA", "person_id": "P2690450", "email_address": "npjohnso@cs.princeton.edu", "orcid_id": ""}, {"name": "David I. August", "author_profile_id": "81100388492", "affiliation": "Princeton University, Princeton, NJ, USA", "person_id": "P2690451", "email_address": "august@cs.princeton.edu", "orcid_id": ""}], "doi_number": "10.1145/1993498.1993500", "year": "2011", "article_id": "1993500", "conference": "PLDI", "title": "Commutative set: a language extension for implicit parallel programming", "url": "http://dl.acm.org/citation.cfm?id=1993500"}