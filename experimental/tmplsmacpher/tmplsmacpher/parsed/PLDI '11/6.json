{"article_publication_date": "06-04-2011", "fulltext": "\n Generalized Just-In-Time Trace Compilation using a Parallel Task Farm in a Dynamic Binary Translator \n Igor B\u00f6hm TobiasJ.K. EdlervonKoch StephenKyle Bj\u00f6rn Franke NigelTopham Institute for Computing Systems \nArchitecture School of Informatics, University of Edinburgh InformaticsForum,10 Crichton Street, Edinburgh, \nEH8 9AB, United Kingdom I.Bohm@sms.ed.ac.uk, T.J.K.Edler-Von-Koch@sms.ed.ac.uk, S.C.Kyle@sms.ed.ac.uk, \nbfranke@inf.ed.ac.uk, npt@inf.ed.ac.uk Abstract Dynamic BinaryTranslation(DBT)is thekey technology behind \ncross-platform virtualization and allows software compiled for one Instruction Set Architecture (I SA)to \nbe executed on a processor supporting a different I SA. Under the hood, DBT is typically im\u00adplemented \nusing Just-In-Time (J IT)compilation of frequently ex\u00adecuted program regions, also called traces. The \nmain challenge is translating frequently executed program regions as fast as possi\u00adble into highly ef.cient \nnative code. As time for JIT compilation adds to the overall execution time, the JIT compiler is often \nde\u00adcoupled and operates in a separate thread independent from the main simulationloopto reducetheoverheadofJIT \ncompilation.In this paper we present two innovative contributions. The .rst con\u00adtribution is a generalized \ntrace compilation approach that consid\u00aders all frequently executed paths in a program for JIT compilation, \nas opposed to previous approaches where trace compilation is re\u00adstrictedtopaths through loops.The second \ncontribution reducesJ IT compilation cost by compiling several hot traces in a concurrent task farm. \nAltogether we combine generalized light-weight trac\u00ading, large translation units, parallel JIT compilation \nand dynamic work scheduling to ensure timely and ef.cient processing of hot traces. We have evaluated \nour industry-strength, L LVM-based par\u00adallel DBT implementing the A RCompact ISA against three bench\u00admark \nsuites(EEMBC,BIOPERF andSPEC CPU2006)and demon\u00adstrate speedups of up to 2.08 on a standard quad-core \nIntel Xeon machine. Across short-and long-running benchmarks our scheme isrobustandnever resultsinaslowdown.Infact,usingfour \nproces\u00adsors totalexecution time canbe reducedby onaverage 11.5%over state-of-the-art decoupled, parallel \n(or asynchronous)JIT compila\u00adtion. Categories and Subject Descriptors D.3.4[Programming Lan\u00adguages]: \nProcessors Incremental Compilers General Terms Design, experimentation, measurement, perfor\u00admance Keywords \nDynamic binary translation, just-in-time compilation, parallelization, taskfarm, dynamicwork scheduling \nPermission to make digital or hard copies of all or part of this work for personal or classroom use is \ngranted without fee provided that copies are not made or distributed for pro.t or commercial advantage \nand that copies bear this notice and the full citation on the .rst page.To copyotherwise, to republish, \nto post on servers or to redistribute to lists, requires prior speci.c permission and/or a fee. PLDI \n11, June 4 8, 2011, San Jose, California, USA. Copyright c &#38;#169; 2011ACM 978-1-4503-0663-8/11/06... \n$10.00 1. Introduction DBT is a widely used technology that makes it possible to run code compiled for \na target platform on a host platform with a different ISA. With DBT, machine instructions of a program \nfor the target platform are translated to machine instructions for the host plat\u00adform during the execution \nof the program. Among the main uses of DBT are cross-platform virtualization for the migration of legacy \napplications to different hardware platforms (e.g. D EC FX!32[9], Apple Rosetta andIBMPOWERVMLX86[32] \nboth based onTran\u00adsitive s QuickTransit, or HP ARIES [44]) and the provision of vir\u00adtual platforms for \nconvenient software development for embedded systems (e.g. Virtual Prototype by Synopsys). Other current \nand emerging usesofDBT include,but are not limited to, generation of cycle-accurate architecture simulators \n[6, 12], dynamic instru\u00admentation [17], program analysis, cache modeling, and workload characterization \n[23], software security [42], and transparent soft\u00adware support for heterogeneous embedded platforms \n[11]. Ef.cient DBT heavily relies on Just-in-Time (JIT)compilation for the translation of target machine \ninstructions to host machine in\u00adstructions. AlthoughJ IT compiled code generally runs muchfaster than \ninterpreted code, JIT compilation incurs an additional over\u00adhead.For this reason, only the most frequentlyexecuted \ncode re\u00adgions are translatedto nativecode whereas less frequentlyexecuted code is still interpreted. \nUsing a single-threaded execution model, the interpreter pauses until the JIT compiler has translated \nits as\u00adsigned code block and the generated native code is executed di\u00adrectly. However, it has been noted \nearlier [1, 16, 20] that program execution does not need to be paused to permit compilation, as a JIT \ncompiler can operate in a separate thread while the program executes concurrently. This decoupled or \nasynchronous execution of theJIT compiler increases complexityof theDBT,butisvery effective in hiding \nthe compilation latency especially if the JIT compiler can run on a separate processor. Our main contribution \nis to demonstrate how to effectively reduce dynamic compilation overhead and speedup execution by doing \nparallel JIT compilation, exploiting the broad proliferation of multi-core processors. The key idea is \nto detect independent, large translation unitsinexecution traces and to farm out work to multiple, concurrent \nJIT compilation workers. To ensure that the latest and most frequently executed code traces are compiled \n.rst, we apply a priority queue based dynamic work scheduling strategy where the most recent, hottest \ntraces are given highest priority. We have integrated this novel, concurrent JIT compilation methodology \ninto our L LVM-based state-of-the-art ARCSIM [19, 38] DBT implementing the A RCompact ISA and evaluatedits \nper\u00adformance using three benchmark suites: E EMBC, BIOPERF and SPEC CPU2006.We demonstrate that our parallel \napproach yields an average speedup of 1.17 across all 61 benchmarks and up to 2.08 for individual benchmarks \n over decoupled JIT compila\u00adtion using only a single compilation worker thread on a standard quad-core \nIntel Xeon host platform. At the same time our scheme is robust and never resultsina slowdowneven forvery \nshort-and long-running applications.  1.1 Trace-based JIT Compilation Before we take a more detailed \nlook at our approach to general\u00adizationofJ IT trace compilation usinga parallel taskfarm, we pro\u00advide \na comparison of state-of-the-art trace-based JIT compilation approaches to highlight ourkeyconcepts (see \nFigure 1). Trace-based JIT compilation systems start executing in inter\u00adpreted mode until a special structure \n(e.g. loop header, method en\u00adtry) is detected. This causes the system to switch from interpreted modetotracemode.In \ntracemodeexecutedpaths withinthatpar\u00adticular structure are recorded to form a trace data structure. Once \nthe traced code region s execution count reaches a threshold, the recorded trace is compiled just-in-time \nand control .ow is diverted from the interpreter to the faster, native code. In general, trace\u00adbased \nJIT compilation approaches can be categorized based on the following criteria: Trace Boundaries -One \npopular [14 16, 26, 27, 29, 37] ap\u00adproachistoonly consider paths within natural loops( 1@ 3 @ 2@ in Figure \n1). However,there are also paths outside of loops (e.g. paths crossing methodor function boundaries,IRQhandlers,ir\u00adreducible \nloops) which are executed frequently enough to jus\u00adtify their compilationto machine code.Inthisworkwe \npropose a generalizedapproach to tracing based on traced control-.ow\u00adgraphs(CFG)ofbasicblocks,thatcan \nstartandend almostany\u00adwhereinthe program( 4 @ in Figure 1). As a result we can dis\u00adcover more hot traces \nand capture larger regions of frequently executed code than just loop bodies. JITCompilation Strategy \n-Some state-of-the-artJITtrace com\u00adpilers [26, 27, 37] are sequential and wait for JIT compilation ofa \ntraceto .nish before continuingexecution( 1 @ in Figure 1). An early approach to parallel JIT compilation \n[14] proposed to split trace compilation into multiple, parallel pipeline stages operatingat instruction \ngranularity( 2 @ in Figure 1). While the compiler pipeline can in principle be parallelized, this ap\u00adproach \nis fundamentally .awed as execution is stopped until compilation of a complete trace is .nished. This \nresults in con\u00adsistent slowdowns over all benchmarks in [14]. Synchroniza\u00adtion of up to 19 pipeline stages \nfor every compiled instruction adds signi.cantly to the overall compilation time while no new translation \nunits are discovered. The H OTSPOT JVM [29] and [16] implement a concurrent JIT compilation strategy \nby run\u00adning the JIT compiler in one separate helper thread whilst the master thread continuesto interpret \ncode( 3 @ in Figure 1).We buildonthis approachandextendittorunseveral JITcompilers ina parallel taskfarm \nwhilstexecution continues.Wedo thisto furtherhideJ ITcompilation costand switchtonativeexecution of hot \ntraceseven sooner and for longer( 4 @ in Figure 1) than previous approaches. The objective of our trace-based \nJIT compilation strategy 4 @ out\u00adlined in Figure 1 is translating frequently executed program re\u00adgions \ninto native codefaster when compared to e.g. 1@ 3 @ 2@ [14 16, 26, 27, 29, 37]. Figure1 also illustrates \nthat task parallel JIT compilation 4 @ reduces the time until native code execution starts in comparison \nto current approaches 1@ 3 @ 2@. Our generalized approach to tracing enables us to start tracing right \nfrom the .rst instruction, avoiding a period of interpretation until speci.c structures (e.g. loop headers) \nare encountered. Con-Figure 1. Comparisonof trace-basedJ IT compilation approaches: @1sequential approach \n[15, 26, 27, 37], 2 @ parallel compilation pipeline [14], 3 @ decoupled J IT compilation using one thread \n[16, 29]. Finally, 4 @ depicts our parallelJIT compilation taskfarm. sequently we can .nd more opportunities \nfor J IT compilation (i.e. hot traces) earlier. Being able to discover more hot traces that can be compiled \nindependently,wetaketheideaof decoupledJITcom\u00adpilationa step further, and proposea truly parallel and \nscalableJIT compilation scheme based on the parallel taskfarm design pattern.  1.2 Motivating Example \nConsider the full-system simulationofa LinuxOScon.gured and builtfortheARC 700 processorfamily(RISC ISA).Ona \nstandard quad-core Intel Xeon machine we simulate the complete boot-up sequence,the automatedexecutionofasetof \ncommands simulating interactiveuserinputatthe console,followedbythefull shut-down sequence. This example \nincludes rare events such as boot-up and shut-down comparable to the initialization phase in an application, \nFigure 2. Full-system Linux simulation benchmark -@ simulation time in seconds, rate in MIPS, and 2 \n comparison of 1@ interpreted vs. natively executed instructions in %, between decoupled only and decoupled \nparallel JIT compilation using three JIT compilation worker threads. Histogram @ demonstrates how often \nmore than one hot trace per trace interval is found. 3 but it also includes very frequent events occurring \nafter the boot\u00adup sequence during interactive user mode. In a full-system OS simulation there are frequent \ncalls to interrupt service routines that must be simulated. Our generalized tracing approach can easily \nidentify frequent traces including interrupt service routines that would otherwisebe missedifwewould \nrestrict tracingtoloopor function boundaries. Our dynamic binary translator speedsupthe simulationbyiden\u00adtifying \nand translating hot traces to native x86 code during simu\u00adlation using the sequence of steps illustrated \nin Figure 3. As sim\u00adulation runs in parallel to J IT compilation we continue to discover and dispatch \nmorehot tracestotheJIT compiler.Infact,itispos\u00adsible that JIT compilation of the previous trace has not \nyet been completedbythetimenewtracesarediscovered.Inthiscase,work is distributedover severalJIT compilation \nthreads.To ensure that the most pro.table traces are compiled .rst, we have implemented a dynamic work \nscheduling strategy that dynamically prioritizes compilation tasks according to their heat and recency. \nForthe purposeofthismotivatingexamplewe comparea simu\u00adlation using only one decoupledJIT compiler thread \n(current state\u00adof-the-art) withasimulation using multiple decoupledJ ITcompiler threads in parallel (see \n4@ of Figure2 we @ in Figure 1). In Chart 1compare the overall simulation time in seconds and the simulation \nrateinMIPSforboth approaches.Ournewapproachusingthreede\u00adcoupledJITcompilersin parallel completesthepreviously \noutlined sample application 21 seconds earlier. This results in an improve\u00adment of 38% and, thus, achieves \na speedup of 1.6 when compared to using only one decoupled JIT compiler. The overall simulation rate \n(in MIPS)improves from 62 to 112 MIPS by using our new approach. As several J IT compilers work on hot \ntraces in parallel, nativetranslations areavailable much earlier than usingasingle de\u00adcoupled JIT compiler \n(see 3 @ in Figure 1), leading to a substantial increase from 77% to 92% of natively executed target \ninstructions (see Chart 2 @ of Figure 2). The obvious question to ask is where does the speedup come \nfrom? Histogram 3 @ in Figure2showshow oftena certainamount of frequentlyexecuted tracesis found per \ntrace interval. Thefact that 65% of the time there are at least two or more hot traces dis\u00adcovered per \ninterval clearly demonstrates the bene.ts of having more than one JIT compiler available on today s multicore \nma\u00adchines. Even if only one hot trace per interval is discovered, the JIT compilation of the previous \nhot trace might not have .nished. Having severalJIT compilers that can already startworking on the newly \ndiscovered hot traces before others have .nished helps to ef\u00adfectively hide most of the JIT compilation \nlatency(see Box 1 @ in Figure 5).  1.3 Contributions Among the contributions of this paper are: 1. The \nintroduction of a light-weight and generalized JIT trace compilation approach considering frequently \nexecuted paths (i.e. hot traces) that can start and end almost anywhere in a program and are not restricted \nto loop or function boundaries, 2. the introduction of an innovative parallel task farming strategy for \ntruly concurrentJIT compilationof hot traces, 3. the development of dynamic work scheduling and adaptive \nhotspot threshold selection approaches that give priority to the most recent, hottest traces in order \nto reduce time spent in in\u00adterpreted simulation mode, and 4. an extensive evaluation of our LLVM-based \nDBT targeting the ARCompact ISA using three full benchmark suites, EEMBC, BIOPERF andSPEC CPU2006.  \n 1.4 Overview The remainder of this paper is structured as follows. In section2 we give an overview \nof the LLVM-based ARCSIM dynamic binary translator. This is followed by a presentation of our tracing \nap\u00adproach and parallelJIT compilation schemein section3.We then explain the evaluation methodology and \nshow our empirical results in section4. Relatedworkis discussedin section5and, .nally, we summarize and \nconclude in section 6. 2. Background In our work we extended our DBT ARCSIM, a target adaptable simulator \nwith extensive support of the A RCompact ISA. It is a full-system simulator,implementing the processor,its \nmemory sub\u00adsystem (including MMU), and suf.cient interrupt-driven periph\u00aderals to simulate the boot-up \nand interactive operation of a com\u00adplete Linux-based system. The DBT has a very fast and highly\u00adoptimized \ninterpreted simulation mode [38].Byusingatrace-based JITcompiler to translate frequently interpreted \ninstructions into na\u00adtive code, our DBT is capable of simulating applications at speeds approachingorevenexceeding \nthatofa siliconA SIP whilstfaith\u00adfully modeling the processor s architectural state [38]. TheJIT compileris \nbased on release 2.7of theLLVM compiler infrastructure [22] and executes in a concurrent thread to the \nmain interpretation loop, reducing the overhead caused by J IT compila\u00adtion [16, 29]. The LLVM JIT compilation \nengine implementation provides a mature and competitive JIT compilation environment Figure 3. Trace-based \nJIT DBT .ow showing main simulation loopatthetop, priority queue schedulinginthe middle,and parallel \ntaskfarm at the bottom.  used in manymajor products by companies such as Apple, Adobe, Nvidia, and \nSony, to name just a few [36]. State-of-the-art trace-based J IT compilers typically use natural loops \nas boundaries for traces, resulting in relatively small units of translation [14 16,26,27,29,37].OurD \nBTdeliberately considers larger traces by allowing traces to start and end almost anywhere in a program. \nThis approach is comparable to the techniques eval\u00aduated in [3, 19] and has the bene.t of providing greater \nscope for optimizationstotheJ IT compilerasit can operateonlarger traces. Furthermore ourD BTincludesvarious \nsoftware caches[3,38] (e.g. decode cache to avoid re-decoding recently decoded instructions, translation \ncache to improve lookup times for locations of native code) to improve simulation speed. 1@ 3interpreted \nbasic blocks @ during one trace interval. Figure 4. Incremental @ 2@ trace construction from sequence \nof 4 3. Methodology Our approach to parallel and decoupled JIT compilation in a dy\u00adnamic binary translator \nsigni.cantly speeds up execution whilst maintaining full architectural observability of the target architec\u00adture. \nWe use a light-weight approach to discover and construct traces eligible forJ IT compilation.Atrace re.ects \nthe control .ow exhibitedbythe source programat run-time.Newly discoveredhot traces are then translated \nusing multiple decoupled JIT compiler threads in parallel. Decoupling the simulation (i.e. interpretation) \nloop from JIT compilation prevents anyslowdown incurred by JIT compilation as the simulation continues \nwhile hot traces are be\u00ading translated [16, 34]. By adding more JIT compilation threads, working on independent \nhot traces,JIT compilation timeis further reduced resulting in increased simulation speed as native code \nbe\u00adcomes available earlier. In the following sections we outline our trace generation and hotspot detection \ntechniqueandexplainwhythis technique discov\u00aders more relevant hot traces for JIT compilation than previous \nap\u00adproaches [14 16, 26, 27, 29, 37].We also discuss the decoupling and parallelization of JIT compilation \nand demonstrate the impor\u00adtance of scheduling hot traces for JIT compilation. The key idea behind our \nscheduling scheme is to consider heat and recency of traces in order to prioritize hot traces that are \nbeing simulated right now and toJIT compile them .rst. 3.1 Trace Construction and Hotspot Detection \nInterpreted simulation time is partitioned into trace intervals (see Figures3,4and5) whose length is \ndetermined by a user-de.ned number of interpreted instructions. After each trace interval, the hottest \nrecorded traces are dispatched to a priority queue for JIT compilation before the simulationloop continues. \nDecoupled from this simulation loop,JITcompilationworkers dequeue and compile the dispatched traces. \nThe heat of a trace is de.ned as the sum of the execution frequencies of its constituent basic blocks. \nOurDBTmust maintain an accurate memory model (see Section 2)to preservefull architectural observability. \nThis means that traces generated during a trace interval are separated at page boundaries, where a page \ncan contain up to 8 KB of target instructions. For  2hot traces in parallel. Box @ showshow several \nconcurrentJIT compilation threads caneffectively hide mostof theJ IT compilation latency Figure 5. Concurrent \nJ IT compilation of hot traces -Box @ demonstrates how we exploit task parallelism by JIT compiling independent \n1 by overlapping compilation of hot traces. each page a trace recording interpreted basic blocks contained \nin that page is maintained (see Figure 5). Tracing is light-weight because we only record basic block \nentry points (i.e. memory addresses) as nodes, and pairs of source and target entry points as edgesintheper-pageCFG \ntraces. Figure4showsthe incremental trace construction for interpreted basic blocks within one page during \none trace interval and highlights our trace boundary or unit of compilation, namely a trace within a \npage of target memory instructions. Typically JIT compilation schemes [14 16, 26, 27, 29, 37] are based \non compilation units reaching threshold frequencies of exe\u00adcution to determine when to trigger compilation. \nOur approach is based around the concept of trace intervals during which traces are recorded and execution \nfrequencies are maintained. At the end of a trace interval we analyze generated traces for each page \nthat has been touched during that interval and dispatchfrequently executed traces to a trace translation \npriority queue (see 1@ 3 @ 2@ in Figure 3).We decided to use an interval based scheme because we found \nit generates more uniformly sized compilation units, resulting in better and more predictable load balance \nbetween compilation and execution. Our trace intervals are somewhat related to the concept of bounded \nhistorybuffersinDYNAMO [3]but imposefewer re\u00adstrictions on their use.  3.2 Parallel JIT Compilation \nOur main contribution is the demonstration, through practical im\u00adplementation, of the effectiveness of \na parallel multi-threaded J IT compilation taskfarm based on release 2.7of theL LVM [22] com\u00adpiler infrastructure.By \nparallelizingJITcompilationweeffectively hide JIT compilation latencyand exploit the available parallelism \nexposedby our generalized trace construction scheme. A concurrent trace translation priority queue acts \nas the main interface between the simulation loop and multipleJITcompilation workers (see 3@ in Figure \n3). Each JIT compilation worker @ and 4thread dequeues a trace and generates a corresponding LLVM in\u00adtermediate \nrepresentation (I R). Subsequently, a sequence of stan\u00addard LLVM optimization passes is applied to optimize \nthe gen\u00aderated LLVM-IR. We use LLVM s ExecutionEngine to JIT- Figure 6. Trace translation state variable \ntransitions. compile and link the generated LLVM-IR code. Each JIT compila\u00adtion worker thread receives \nits own private ExecutionEngine instance upon thread creation. In our parallel trace-based JIT compiler \nthe interpreter uses a translation state variable to mark unseen traces forJITcompilation and to determine \nif a native translation for a trace already exists. While traces are JIT compiled, the interpreter continues \nto execute the program. As soon as JIT compilation of a trace is .nished, the JIT compilation worker \nimmediately modi.es its translation state variable such that the interpreter is noti.ed about its availability. \nThus, a trace can be in one of the following three states: UNTRANSLATED -When a trace is seen for the \n.rst time, its state is set to untranslated by the interpreter.  IN TRANSLATION -When a trace is dispatched \nfor JIT compi\u00adlation the interpreter changes its state from untranslated to in translation.  TRANSLATED-WhenaJITcompilationworkeris \n.nished with the translation of a trace it changes its state from in translation to translated.   \nFigure 7. Speedups using the BIOPERF benchmark suite comparing (a) interpreted-only simulation, (b) simulation \nusing a decoupled J IT compiler, and (c) simulation using our novel, parallel JIT compiler with three \nJIT compilation worker threads including dynamic work scheduling. This particular use of a state variable \nindicating the translation state of a trace allows for lock-free synchronization between J IT compilation \nthreads and the simulation loop (see Figure 6). This approach is somewhat similar to the compiled state \nvariable con\u00adcept implemented in [16].  3.3 Dynamic Work Scheduling In any kind of JIT compilation environment \nit is paramount to translate hot code regions asfast as possible into highly ef.cient native code. Thus \nhaving discovered multiple traces across several trace intervals we would like to JIT compile the most \nrecently and frequently executed traces .rst. To ef.ciently implement a dynamic work scheduling scheme \nbased on both recencyand frequencyof interpreted traces, we have chosena priority queue as an abstract \ndata type usinga binary heap as the backbone data-structure.We chosea binary heap becauseof its worst \ncase complexity of O(log(n)) for inserts and removals. Our sorting criteria insert the most frequently \nexecuted trace from the most recent trace interval at the front of the priority queue.  3.4 Adaptive \nHotspot Threshold Selection It is possible that our trace-selection and dispatch system can pro\u00adduce \nmore tasks than the JIT workers can reasonably handle. The aforementioned dynamic work scheduling mitigates \nthis problem byensuring that the hottest and most recent traces are serviced .rst, leaving relatively \ncolder and older tasks waiting until the important work has been completed. However, it would also be \nbene.cial to reduce the number of tasks actually being dispatched to the trace translationqueueintheeventofanoverloadedJ \nITcompilationtask farm. In order to avoid large amounts of waiting trace translation tasks, we implemented \nan adaptive hotspot threshold scheme. Ini\u00adtially, the hotspot threshold is set to a constant value based \non the number of JIT workers available as the number of workers in\u00adcreases, the threshold can be set \nmore aggressively. This threshold is then adjusted based on the priority queue s current length, where \na longer queue raises the threshold at which new potential traces are considered to be hot enough. The \nthreshold can either be tied directly to the length of the queue, or a certain queue length can trigger \nan increase in the hotspot threshold. 4. Empirical Evaluation Wehaveevaluated our parallel trace-based \njust-in-time compilation and dynamicwork scheduling approach across more than60 indus\u00adtry standard benchmarks, \nincluding BIOPERF, SPEC, EEMBC, and COREMARK,fromvarious domains.Inthissectionwe describeour experimental \nsetup andmethodology before we present and discuss our results. 4.1 Evaluation Methodology We have evaluated \nour parallel trace-based JIT compilation ap\u00adproach using theBIOPERF benchmark suite that comprisesa com\u00adprehensive \nset of computationally-intensive life science applica\u00adtions [2]. It is well suited for evaluating our \nparallel trace-based JIT compiler as it exhibits many different potential hotspots per application for \nmost of its benchmarks. We also used the indus\u00adtry standard EEMBC 1.1, and COREMARK [35] embedded bench\u00admark \nsuites. These benchmarks represent small and relatively short running applications with complex algorithmickernels.Anevalua\u00adtionusingSPEC \nCPU2006 benchmarks[18]is includedastheyare widely used and considered to be representative of a broad \nspec\u00adtrum of application domains. The BIOPERF benchmarks were run with class-A input data\u00adsetsavailablefrom \ntheBIOPERF web site. TheEEMBC 1.1 bench\u00admarks were run for the default number of iterations andCORE-MARK \nwas run for 1000 iterations.For practical reasons we used the largest possible data set for eachof theSPEC \nCPU 2006 bench\u00admarks such that simulation time does not become excessive. Our main focus has been on \nsimulation speedupbyreducing the overall simulation time. Therefore we have measured the elapsed real \ntime between invocation and termination of our simulator using the UNIX time command. We used the average \nelapsed wall clock time across 10 runs for each benchmark and con.guration (i.e. interpreted-only, decoupled, \ndecoupled parallel) in order to calculate speedups. Additionally, we provide error bars for each benchmark \nresult denoting the standard deviation to show how much variation there is between different program \nruns. We use a strong and competitive baseline for our comparisons, namely a decoupled trace-based JIT \ncompiler using one asyn\u00adchronous thread for J IT compilation [16]. Relative to that base\u00adline we plot \nthe speedups achieved by our parallel trace-based JIT compiler using three asynchronous JIT compilation \nthreads. Fur\u00adFigure 8. Speedups using SPEC CPU 2006, EEMBC and COREMARK benchmark suites comparing (a) \ninterpreted-only simulation, (b) simulation using a decoupled J IT compiler, and (c) simulation using \nour novel, parallel JIT compiler with three JIT compilation worker threads including dynamic work scheduling. \n  thermore, we also present speedups (i.e. slowdowns) relative to our baseline when using interpreted \nsimulation only(i.e. disabling trace-basedJ IT compilation). All measurements were performed on a standard \nx86 DELL TM POWEREDGE TM quad-core outlinedinTable1under conditionsof low system load.Toevaluate the \nscalabilityof our approach, when adding more cores, we performed additional measurements on a parallel \nsymmetric multiprocessing machinewith162.6GHzAMD OpteronTM(AMD64e) processors running Scienti.c Linux \n5.0 (see Section 4.4).  4.2 Summary of Key Results Our novel parallel trace-based JIT compilation approach \nis always faster than the baseline decoupled JIT compiler and achieves an average speedup of 1.38 equivalent \nto an average execution time reduction of 22.8%for the BIOPERF benchmark suite. This corre\u00adsponds directly \nto an average increase of 14.7%in the number of natively executed instructions compared to the baseline. \nFor some benchmarks (e.g. blastp, clustalw) our pro\u00adposed scheme is more than twice asfastasthe baseline.Thiscanbe \nexplainedby thefact that 59%of the time we .nd more than one hot trace per trace interval for blastp. \nFrom this it follows that blastp exhibits a large amount of task parallelism (see Box 2 @in Figure 5). \nClustalw mainly bene.ts from hiding compilation latencyby using severalJIT compilationworker threads \n(seeBox 1@ in Figure 5). Shorter running BIOPERF benchmarks perform particularly well with our scheme \n(e.g. tcoffee, hmmsearch, clustalw) because more JIT compilation workers can deliver translations much \nquicker as they can split the workload (i.e. hide compila\u00adtion latency). Especially for hmmsearch where \nthe baseline de\u00adcoupled JIT compiler performs worse than the interpreted-only version, our scheme can \nsigni.cantly boost execution speed and reduce overall simulation time by 34.4%. Even for very long run\u00adning \nBIOPERF benchmarks (e.g. fasta-ssearch, promlk, hmmer-hmmpfam, ce), where JIT compilation time typically \nrepresents only a small fraction of the overall execution time, our scheme achieves a reduction of execution \ntimes of up to 6.8%.  4.3 Worst-Case Scenarios The BIOPERF benchmark suite is well suited to show the \nef.\u00adcacyof our parallel trace-basedJIT compiler. Additionally we also demonstrate itsfavorable impact \non benchmarks where wewould notexpecttosee signi.cantspeedupsfromourtechnique -socalled worst-case scenarios. \nFor this analysis we have considered short running embed\u00added benchmarks (EEMBC, COREMARK)containing few \napplica\u00adtion hotspots (i.e. algorithmickernels). Someof these benchmarks are so short that interpreted \nonly execution takes less than two sec\u00adonds, leaving very little scope for improvement by a JIT compiler. \nAt the other end of the scale are very long running and CPU in\u00adtensive benchmarks(SPEC CPU 2006)whereJITcompilation \ntime contributes only a marginal fraction to the overall execution time. Across the SPEC CPU 2006 benchmarks \nour parallel trace\u00adbased JIT compiler is never slower than the baseline and achieves an average speedup \nof 1.15, corresponding to an average increase of 4.2%in the number of natively executed instructions. \nThe best speedups are achieved for gcc (2.04x), xalancbmk (1.47x),  @ 2@ 5 @ 6additionalJ IT compilationworkers \non speedup.Top right cumulative histogram @ shows%of benchmarks bene.ting fromgiven number Figure 9. \nScalability charts for selected benchmarks from BIOPERF 1@ and SPEC CPU 2006 4@ demonstrating the effect \nof 3ofJIT compilation threads. TM TM Vendor &#38; Model DELL POWEREDGE 1950 NumberCPUs 4(quad-core) ProcessorType \nIntel cTM processor E5430 &#38;#169;XeonClock/FSB Frequency 2.66/1.33G Hz L1-Cache 32K Instruction/Data \ncaches L2-Cache 12MB Operating System Scienti.c Linux 5.5 (64-bit) Table 1. Simulation Host Con.guration. \npovray (1.2x), and perlbench (1.18x).For perlbench and gcc the number of natively executed instructions \nimproves by 19.5% and 38.0%, respectively,when using our parallel trace-based JIT compiler. The gcc benchmark \ngreatly bene.ts from our ap\u00adproach as it runs a compiler with manyoptimization .ags enabled resulting \nin a multitude of application hotspots representing com\u00adpilation phases. Xalancbmk is one of the shorter \nrunning SPEC CPU bench\u00admarks performing XML transformations. Due to its short run\u00adtime and abundance \nof application hotspots the tracing overhead causes the baseline decoupled JIT compiler to be slower \nthan the interpreted-only version. Our parallel trace-based JIT can easily recover this overhead resulting \nin a speedup of 1.47 when com\u00adpared to the baseline, and a speedup of 1.21 when compared to interpreted-only \nsimulation. Povray represents a long running benchmark where we achieve a speedup of 1.2. This is again \ndue to an abundance of application hotspots across the runtime of the povray benchmark. For the EEMBC \nand COREMARK benchmark suites our ap\u00adproach achieves an average speedup of 1.12 over the baseline, and \nan average improvement of 3.8% in the number of natively exe\u00adcuted instructions. Small embedded benchmarks \ndo not often ben\u00ade.t from task parallelism because they rarely exhibit more than one hot trace per trace \ninterval. The speedups are mostly due to thefact thatJIT compilationworkers can already startworking \non newly discovered traces while previous traces are being translated by other workers. Also tracing \nmust be light-weight, causing only very little overhead, to enable speedups for small benchmarks like \nEEMBC andCOREMARK. For all benchmarks performing Fast Fourier Transforms (i.e. aifftr01, aiifft01, fft00) \nspeedups ranging from 1.46 to 1.7 are achieved by our parallel trace-based JIT compiler. The bit manipulation(bitmnp01)and \nin.nite impulse response .l\u00adter(iirflt01) benchmarks also show speedups of 1.57 and 1.46 using our scheme. \nThreeEEMBC benchmarks(cacheb01, puwmod01, ospf)yield very short runtimes using interpreted\u00adonly mode \n(i.e. below one second) causing a small slowdown of the baseline decoupledJIT compiler and our parallelJIT \ncompiler. This is entirely due to the fact that for very short benchmarks a JIT compiler has almost no \nchance to speed up execution, actually causing a slight slowdown due to the overheads caused by tracing \nandJIT compiler thread creation.  4.4 Available Parallelism and Scalability According to Amdahl s law \nwe expected only marginal improve\u00adments with increasing numbersofJIT compilationworker threads, soitis \nremarkablehow well some benchmarks scale(see Figure9) ona16-core system.For blastp 1 @ fromBIOPERFthe \nmaximum speedup of 3.1 is reached with 14 JIT compilation workers. The gcc 45 @ and perlbench @ benchmarks \nfrom S PEC CPU reach their maximum speedupof2.6and1.5with10and11JITcompila\u00adtion workers, respectively. \nNot all benchmarks show bene.ts from adding moreJIT compilation threads.For bzip2 6 @ from SPEC CPU the \npeak speedup is reached with 3 JIT compilation threads, thus adding more threads does not improve execution \ntime.  JIT compilation worker threads. As shown in the scalability charts in Figure 9, the peak speedup \nis reached with different numbers of JIT compilation worker threads. So what is the maximum number of \nJIT compilation threads such that 100% of all benchmarks show speedup, i.e. how far does it scale? The \ncumulative histogram 3 @ in Figure9answers this question by depicting the number of benchmarks (in %) \nthat show an improvement by adding more JIT compilation threads. From the histogram we can see all benchmarks \nshow speedups with3JIT compilationworker threads and 50%of all benchmarks bene.t from9 or moreJIT compilation \nthreads.  4.5 Dynamic Work Scheduling We have also evaluated the impact of our novel dynamic work scheduling \nschemein comparisontoasimpler approach that sched\u00adules traces basedon their orderof creationforJITcompilation.For \nBIOPERF,we measuredanaverageimprovementof8.9%,whichis equivalenttoanaverage speedupof 1.13.FortheSPEC \nCPU 2006 benchmarks, the average improvement and speedup are 3.5% and 1.04, respectively.  4.6 Compiled \nTraces and Trace Translation Queue Length over Time The key performance indicator of our parallel trace \ncompilation scheme is the reduction of overall simulation time. In this section we give a more detailed \noverview of two additional performance indicators, namely (1) the number of compiled traces per time \ninterval (i.e. rate at which work is completed), and (2) the resulting trace translation queue lengths \nover the runtime of three selected benchmarks (see Figure 10).We chose one benchmark from each benchmark \nsuite exhibiting a variety of application hotspots over time, and used a rather aggressive threshold \nfor hot trace selection to highlight some of the main advantages of using more than one JIT compilation \nthread. Acomparison of the number of compiled traces and trace trans\u00adlation queue lengths over time indicates \nthat our parallel JIT com\u00adpilation task farm is able to translate traces signi.cantly faster than the \ndecoupled scheme which relies on a single JIT compi\u00adlation thread. On average, three parallel JIT compilation \nthreads can translate 2.1, 3.1 and 1.5 times as many traces per time in\u00adterval than the decoupled JIT \ncompiler for blastp, gcc and matrix01, respectively. Consequently, the average trace transla\u00adtion queue \nlength is 43%, 52% and 51% shorter, and the maximum observed queue sizes are 33%, 37%, and 9% shorter \nfor the same three benchmarks, respectively. At the same time, the queue length grows at a noticeably \nslower rate. Using a very aggressive initial hotspot threshold, the amount of hot traces identi.ed per \ntrace interval and the resulting translation queue lengths quickly exceed the number of available JIT \ntransla\u00adtionworkers,even fora short benchmark such as matrix01. This demonstrates the need for our dynamic \nwork scheduling and adap\u00adtive hotspot threshold selection schemes. Dynamic work schedul\u00ading ensures that \nthe most recent and hottest traces are scheduled for translation .rst, whereas adapting the hotspot threshold \nbased on the translation queue length aims at improving the utilisation of available resources.  4.7 \nMemory Overhead of Parallel JIT Compilation The use of multiple LLVM JIT compiler threads incurs a memory \noverhead.However,wefoundthisoverheadtobe modestcompared to the baseline. Internal memory consumption \n(i.e. memory which isnot applicationdataortargetbinarycode)neverexceeds150MB for anyof the benchmarks. \n5. Related Work 5.1 Dynamic Binary Translation Dynamic translation techniques are used to overcome the \nlack of .exibility inherent in statically-compiled simulators. The MIMIC simulator [24] simulatesI BMSYSTEM/370instructions \non theIBM RT PC and translates groups of target basic blocks into host in\u00adstructions. S HADE [10] and \nEMBRA [21] use DBT with transla\u00adtion caching techniques in order to increase simulation speeds. The Ultra-fastInstruction \nSet Simulator [45] improves the performance of statically-compiled simulation by using low-level binary \ntrans\u00adlation techniques to take full advantage of the host architecture. Just-In-Time Cache Compiled \nSimulation (J IT-CCS)[28] exe\u00adcutes and caches pre-compiled instruction-operation functions for each \nfunction fetched. The Instruction Set Compiled Simulation (IC-CS)simulator [31] was designed to be a \nhigh performance and .exible functional simulator.To achieve this the time-consuming instruction decode \nprocess is performed during the compile stage, whilst interpretationis enabledat simulation time.TheSIMICS[31] \nfull system simulator translates the target machine-code instruc\u00adtions into an intermediate format before \ninterpretation. During sim\u00adulation the intermediate instructions are processed by the inter\u00adpreter which \ncalls the corresponding service routines. QEMU [5] isafast simulator using an original dynamic translator. \nEach target instruction is divided into a simple sequence of micro-operations, the set of micro-operations \nhaving been pre-compiled of.ine into an object .le. During simulation the code generator accesses the \nobject .le and concatenates micro-operations to form a host func\u00adtion that emulates the target instructions \nwithin a block. More re\u00adcent approaches to JIT DBT ISS are presented in [7, 19, 30, 38]. Apart from different \ntarget platforms these approaches differ in the granularity of translation units (basic blocks vs. pages \nor CFG re\u00adgions) and theirJIT code generation target language(ANSI-C vs. LLVM IR). Parallel simulation \nof multi-core target platforms on multi-core host platforms has been demonstrated in [21, 25, 40]. These \nap\u00adproaches, however, are mainly concerned with the mapping of tar\u00adget to host processors and do not \nconsider the parallelization of the embeddedJ IT compiler. 5.2 Trace-based JIT Optimization/Compilation \nTracing is a well established technique for dynamic pro.le guided optimization of native binaries. Bala \net al. [3] introduced tracing as a method for runtime optimization of native program binaries in theirDYNAMO \nsystem. Theyused backward branch targets as can\u00addidatesforthe startofa trace,butdidnot attemptto capture \ntraces of loops. Zaleski et al. [43] used DYNAMO-like tracing in order to achieveinlining, indirect jump \nelimination, and other optimizations forJava. Their primarygoalwastobuildan interpreterthat could be \nextended to a tracing VM. Whaley [41] uses partial method compilation to reduce the granularity of compilation \nto the sub-method level. His system uses pro.le information to detect never or rarely executed parts \nof a method and to ignore them during compilation. If such a part gets executed later, execution continues \nin the interpreter. Compilation still starts at the beginning of a method. Similarly, Suganuma et al. \n[33] propose region-based compilation to overcome the limitations of method-based compilation. They use \nheuristics and pro.les to identifyandeliminaterarelyexecuted sectionsofcode,butrelyon expensive runtime \ncode instrumentation for trace identi.cation. Gal et al. [13, 15] proposed an approach tobuilding dynamic \ncompilersin whichnoC FGisever constructed,andno sourcecode level compilation units such as methods are \nused. Instead, they use runtime pro.ling to detect frequently executed cyclic code paths in the program. \nThe compiler then records and generates code from dynamically recorded code traces along these paths. \nIt assembles these traces dynamically into a trace tree, a tree-like data-structure, that covers frequentlyexecuted \n(and thus compilationworthy)code paths through hot code regions.Trace trees suffer from the problem of \ncode explosion when many control-.ow paths are present in a loop, causing them to grow to very large \nsizes due to excessive tail duplicationas outlinedin[4].Tosolvethisproblem Bebenitaetal. [4] propose \nto use trace-regions as a data-structure for tracing in their implementation of Hotpath, a trace-based \nJava JIT compiler inthe MaxineVM.Trace-regions areanextensionto trace treesas they can include join nodes \ninstead of using tail duplication. Lo\u00adcations where trace recording can be enabled, so called anchors, \nare determined statically during byte-code veri.cation, and trace regions are restricted to method boundaries. \nIn contrast, our ap\u00adproach does not rely on statically determined anchors for tracing and trace regions \nare not con.ned to method boundaries.  5.3 Parallel JIT Compilation JIT compilation has a long history \n[1] dating back to the 1960s. The possibility of reducing the overhead of dynamic compilation by decoupling \ntheJIT compiler from themain simulation loop and executing it in a separate thread has been suggested \nby several researchers, e.g. [16, 20, 39].  Parallel JIT compilation is not an entirely new concept. \nSome approaches[8,14]haveattemptedtoexploit pipeline parallelism in the JIT compiler. However, pipelining \nof the JIT compiler has sig\u00adni.cant drawbacks. First, compiler stages are typically not well bal\u00adanced \nand the overall throughput is limited by the slowest pipeline stage this is often the front-end or IR \ngeneration stage. Second, unlike method based compilers, trace-based JIT compilers operate on relatively \nsmall translation units in order to reduce the compila\u00adtion overhead to a bare minimum [15]. Small translation \nunits and long compilation pipelines, however, increase the relative synchro\u00adnization costs between pipeline \nstages and, again, limit the achiev\u00adable compiler throughput. Third, compilation pipelines are static \nand do not scale with the available task parallelism in inherently independent translation units. Most \nrelevant to our work is the approach presented in [30]. This paper pioneered the concept of concurrent \nJIT compilation workerstospeedupDBT,butsuffersfromanumberof.aws.First, rather than taking a trace-based \ncompilation approach entire pages are translated this is unnecessarily wasteful in a time-critical JIT \nenvironment. Second, there are no provisions for a dynamic work scheduling scheme that prioritizes compilation \nof hot traces this may defer compilation of critical traces and lower overall ef.ciency. Third, JIT compilers \nreside in separate processes on remote machines this signi.cantly increases the communication overhead \nand limits scalability. This last point is critical, as results shownin[30]arebasedsolelyonCPU timeofthemain \nsimulation process rather than the more relevant wall clock time that includes CPU time, I/O time and \ncommunication channel delay. 6. Summary and Conclusions In this paper we have presented a generalized \ntrace construction scheme enabling parallel JIT compilation based on a task farm design pattern. By combining \nparallel JIT compilation with light\u00adweight tracing, large translation units and dynamic work schedul\u00ading, \nwe not only minimize and hideJIT compilationoverhead,but fully exploit the available hardware parallelism \nin standard multi\u00adcore desktop PCs. Across three full benchmark suites comprising non-trivial and long-running \napplications fromvarious domains we achieve an average reduction in total execution time of 11.5% andupto51.9% \nforfour processors.Ourinnovative, parallelJIT schemeisrobustandnever resultsinaslowdown.Giventhatonlya \nsmall fractionoftheoverallexecutiontimeisspentonJITcompila\u00adtion, and the majority of time is spent executing \nnatively-compiled code, these results are more than remarkable. While primarily developed for DBT, the \nconcept of concurrent JIT compilation may also be exploited elsewhere to effectively reduce dynamic compilation \noverheads and speedup execution. Prime examples are JIT-compiled Java Virtual Machines (J VM) or JavaScript \nengines. In our future work, we plan to extend our DBT for the simulation of multi-core architectures, \nand explore alternative dynamic work scheduling strategies. References [1] J.Aycock. A brief history \nof just-in-time. ACM Comput. Surv., 35: 97 113, June 2003. [2] D. Bader,Y. Li,T. Li, andV. Sachdeva. \nBioperf:a benchmark suite toevaluate high-performance computer architecture on bioinformatics applications. \nIn Proceedings of the IEEE International Workload Characterization Symposium, IISWC 05, pages 163 173, \n2005. [3] V. Bala, E. Duesterwald, and S. Banerjia. Dynamo: a transparent dy\u00adnamic optimization system. \nIn Proceedings of the ACM SIGPLAN 2000 Conference on Programming Language Design and Implemen\u00adtation, \nPLDI 00, pages 1 12,NewYork,NY, USA, 2000.ACM. [4] M. Bebenita, M. Chang, G. Wagner, A. Gal, C. Wimmer, \nand M. Franz. Trace-based compilation in execution environments with\u00adout interpreters. In Proceedings \nof the 8th International Conference on the PrinciplesandPracticeofProgramminginJava, PPPJ 10, pages 59 \n68,NewYork,NY, USA, 2010.ACM. [5]F. Bellard. QEMU,afastand portable dynamic translator.In Proceed\u00adingsofthe \nUSENIX AnnualTechnical Conference,ATEC 05, page 41, Berkeley, CA, USA, 2005. USENIX Association. [6] \nI. B\u00f6hm, B. Franke, and N. Topham. Cycle-accurate performance modelling in an ultra-fast just-in-time \ndynamic binary translation in\u00adstruction set simulator. In Proceedings of the International Conference \non Embedded Computer Systems, IC-SAMOS 10, pages1 10, 2010. [7]F. Brandner,A. Fellnhofer,A. Krall,andD.Riegler.Fastand \naccurate simulation using the llvm compiler framework. In Proceedings of the 1stWorkshop on Rapid Simulation \nandPerformance Evaluation: Methods andTools, RAPIDO 09, pages 1 6, 2009. [8] S. Campanoni, G. Agosta, \nand S. C. Reghizzi. A parallel dynamic compiler for CIL bytecode. SIGPLAN Not., 43:11 20, April 2008. \n[9] A. Chernoff, M. Herdeg, R. Hookway, C. Reeve, N. Rubin, T. Tye, S. BharadwajYadavalli,andJ.Yates. \nFX!32:a pro.le-directed binary translator. Micro, IEEE, 18(2):56 64, 1998. [10]B. Cmelik andD.Keppel. \nShade:afast instruction-set simulator for execution pro.ling. In Proceedings of the 1994ACM SIGMETRICS \nConference on Measurement and Modeling of Computer Systems, SIGMETRICS 94, pages 128 137,NewYork,NY,USA, \n1994.ACM. [11] A. Cohen and E. Rohou. Processor virtualization and split compilation for heterogeneous \nmulticore embedded systems. In Proceedings of the 47th DesignAutomation Conference,DAC 10, pages 102 \n107, New York,NY, USA, 2010.ACM. [12] S. Farfeleder, A. Krall, and N. Horspool. Ultra fast cycle-accurate \ncompiled emulation of inorder pipelined architectures. J. Syst. Archit., 53:501 510, August 2007. [13] \nA. Gal, C. W. Probst, and M. Franz. HotpathVM: an effective JIT compiler for resource-constrained devices. \nIn Proceedings of the 2nd International Conference on Virtual Execution Environments, VEE 06, pages 144 \n153,NewYork,NY, USA, 2006.ACM. [14] A. Gal, M. Bebenita, M. Chang, and M. Franz. Making the compi\u00adlation \npipeline explicit: Dynamic compilation using trace tree se\u00adrialization. Technical Report 07-12, University \nof California, Irvine, 2007. [15] A. Gal, B. Eich, M. Shaver, D. Anderson, D. Mandelin, M. R. Haghighat, \nB. Kaplan, G. Hoare, B. Zbarsky, J. Orendorff, J. Ru\u00adderman, E. W. Smith, R. Reitmaier, M. Bebenita, \nM. Chang, and M. Franz. Trace-based just-in-time type specialization for dynamic languages. In Proceedingsofthe2009ACM \nSIGPLAN Conferenceon Programming LanguageDesign and Implementation,PLDI 09, pages 465 478,NewYork,NY, \nUSA, 2009.ACM. [16] J. Ha, M. Haghighat, S. Cong, and K. McKinley. A concurrent trace-based just-in-time \ncompiler for single-threaded JavaScript. In Proceedings of the Workshop on Parallel Execution of Sequential \nPrograms on Multicore Architectures, PESPMA 09, 2009. [17] M. Hauswirth,P.F. Sweeney, A. Diwan, and M. \nHind. Vertical pro\u00ad.ling: understanding the behavior of object-priented applications. In Proceedingsofthe19thannualACM \nSIGPLANConferenceonObject\u00adoriented Programming, Systems, Languages, and Applications, OOP-SLA 04, pages \n251 269,NewYork,NY, USA, 2004.ACM. [18] J. L. Henning. SPEC CPU2006 benchmark descriptions. SIGARCH Comput. \nArchit. News, 34:1 17, September 2006. [19] D. Jones and N. Topham. High speed CPU simulation using LTU \ndynamic binary translation. In HighPerformance Embedded Archi\u00adtectures and Compilers, volume 5409 of \nLecture Notes in Computer Science, pages 50 64. Springer Berlin Heidelberg, 2009. [20] C. J. Krintz, \nD. Grove, V. Sarkar, and B. Calder. Reducing the overhead of dynamic compilation. Software: Practice \nand Experience, 31(8):717 738, July 2001.  [21] R. Lantz. Fast functional simulation with parallel Embra. \nIn Pro\u00adceedingsofthe4th AnnualWorkshopon Modeling,Benchmarkingand Simulation, MOBS 08, 2008. [22] C. \nLattner andV. Adve. LLVM:Acompilation framework for lifelong program analysis &#38; transformation. In \nProceedings of the Interna\u00adtional Symposium on Code Generation and Optimization, CGO 04, page 75,Washington, \nDC, USA, 2004. IEEE Computer Society. [23] C.-K.Luk,R.Cohn,R.Muth,H.Patil,A. Klauser,G.Lowney,S.Wal\u00adlace,V. \nJ. Reddi, and K. Hazelwood. PIN:building customized pro\u00adgram analysis tools with dynamic instrumentation. \nIn Proceedings of the 2005ACM SIGPLAN Conference onProgramming Language De\u00adsign and Implementation, PLDI \n05, pages 190 200, NewYork, NY, USA, 2005.ACM. [24] C. May. MIMIC: a fast System/370 simulator. In Papers \nof the Symposium on Interpreters and Interpretive Techniques, SIGPLAN 87, pages 1 13,NewYork,NY, USA, \n1987.ACM. [25] J. Miller,H. Kasture,G.Kurian,C. Gruenwald,N. Beckmann,C. Ce\u00adlio,J. Eastep, andA.Agarwal. \nGraphite:Adistributed parallel sim\u00adulator for multicores. In Proceedings of the IEEE 16th International \nSymposium on HighPerformance Computer Architecture, HPCA 10, pages 1 12, 2010. [26] Mozilla Foundation. \nTamarin project, 17 November 2010. URL http://www.mozilla.org/projects/tamarin/. [27] Mozilla Foundation. \nTracemonkey, 17 November 2010. URL https://wiki.mozilla.org/JavaScript: TraceMonkey. [28] A. Nohl, G. \nBraun, O. Schliebusch, R. Leupers, H. Meyr, and A. Hoff\u00admann.Auniversal technique forfast and .exible \ninstruction-set archi\u00adtecture simulation. In Proceedingsofthe 39th annual DesignAutoma\u00adtion Conference,DAC \n02, pages 22 27, NewYork, NY, USA, 2002. ACM. [29] Oracle Corporation. HotSpot VM, 17 November 2010. \nURL http://java.sun.com/performance/reference/ whitepapers/. [30] W. Qin, J. D Errico, and X. Zhu. A \nmultiprocessing approach to accelerate retargetable and portable dynamic-compiled instruction-set simulation. \nIn Proceedings of the 4th International Conference on Hardware/Software Codesign and System Synthesis, \nCODES+ISSS 06, pages 193 198,NewYork,NY, USA, 2006.ACM. [31] M. Reshadi,P. Mishra, and N. Dutt. Instruction \nset compiled simu\u00adlation:a technique forfast and .exible instruction set simulation. In Proceedingsof \nthe 40th annual DesignAutomation Conference,DAC 03, pages 758 763,NewYork,NY, USA, 2003.ACM. [32] E. \nStahl and M. Anand. A comparison of PowerVM and x86-based virtualization performance.Technical Report \nWP101574, IBMTech\u00addocs WhitePapers, 2010. [33]T.Suganuma,T.Yasue,andT. Nakatani.A region-based compilation \ntechnique for dynamic compilers. ACMTrans. Program. Lang. Syst., 28:134 174, January 2006. [34]V.Tan. \nAsynchronous just-in-time compilation. United StatesPatent Application,WO/2007/055883, 2007. [35] The \nEmbedded Microprocessor Benchmark Consortium. EEMBC benchmark suite, 12 August 2009. URL http://www.eembc. \norg. [36] The LLVM Compiler Infrastructure. Users of LLVM JIT compilation engine, 17 November 2010. URL \nhttp://llvm.org/Users. html. [37] TheWebKit Open Source Project. SquirrelFish, 17 November 2010. URL \nhttp://trac.webkit.org/wiki/SquirrelFish. [38]N.TophamandD.Jones.HighspeedCPU simulationusingJITbinary \ntranslation. In Proceedings of the Annual Workshop on Modelling, Benchmarking and Simulation, MOBS 07, \n2007. [39]P. Unnikrishnan, M. Kandemir, andF. Li. Reducing dynamic com\u00adpilation overhead by overlapping \ncompilation and execution. In Pro\u00adceedings of the 2006 Asia and SouthPaci.c DesignAutomation Con\u00adference, \nASP-DAC 06, pages 929 934, Piscataway, NJ, USA, 2006. IEEE Press. [40] K.Wang,Y. Zhang, H.Wang, and X. \nShen. Parallelization of IBM Mambo system simulator in functional modes. SIGOPS Oper. Syst. Rev., 42:71 \n76, January 2008. [41] J. Whaley. Partial method compilation using dynamic pro.le in\u00adformation. In Proceedings \nof the 16th ACM SIGPLAN Conference on Object-oriented Programming, Systems, Languages, and Appli\u00adcations, \nOOPSLA 01, pages 166 179, NewYork, NY, USA, 2001. ACM. [42] B. Yee, D. Sehr, G. Dardyk, J. Chen, R. Muth, \nT. Ormandy, S. Okasaka, N. Narula, and N. Fullagar. Native Client: A sandbox for portable, untrusted \nx86 native code. In 30th IEEE Symposium on Security and Privacy, pages 79 93, May 2009. [43] M. Zaleski, \nA. D. Brown, and K. Stoodley. YETI: a gradually exten\u00adsible trace interpreter. In Proceedings of the \n3rdInternational Confer\u00adence onVirtual ExecutionEnvironments, VEE 07, pages 83 93, New York,NY, USA, \n2007.ACM. [44]C.ZhengandC. Thompson.PA-RISCto IA-64: transparentexecution, no recompilation. Computer, \n33(3):47 52, Mar. 2000. [45] J. Zhu and D. D. Gajski. A retargetable, ultra-fast instruction set simulator. \nIn Proceedings of the Conference on Design,Automation andTestin Europe,DATE 99,NewYork,NY, USA, 1999.ACM. \n   \n\t\t\t", "proc_id": "1993498", "abstract": "<p>Dynamic Binary Translation (DBT) is the key technology behind cross-platform virtualization and allows software compiled for one Instruction Set Architecture (ISA) to be executed on a processor supporting a different ISA. Under the hood, DBT is typically implemented using Just-In-Time (JIT) compilation of frequently executed program regions, also called <i>traces</i>. The main challenge is translating frequently executed program regions as fast as possible into highly efficient native code. As time for JIT compilation adds to the overall execution time, the JIT compiler is often decoupled and operates in a separate thread independent from the main simulation loop to reduce the overhead of JIT compilation. In this paper we present <i>two</i> innovative contributions. The first contribution is a <i>generalized</i> trace compilation approach that considers all frequently executed paths in a program for JIT compilation, as opposed to previous approaches where trace compilation is restricted to paths through loops. The second contribution reduces JIT compilation cost by compiling several hot traces in a concurrent task farm. Altogether we combine generalized light-weight tracing, large translation units, parallel JIT compilation and dynamic work scheduling to ensure timely and efficient processing of hot traces. We have evaluated our industry-strength, LLVM-based parallel DBT implementing the ARCompact ISA against three benchmark suites (EEMBC, BioPerf and SPEC CPU2006) and demonstrate speedups of up to 2.08 on a standard quad-core Intel Xeon machine. Across short- and long-running benchmarks our scheme is robust and never results in a slowdown. In fact, using four processors total execution time can be reduced by on average 11.5% over state-of-the-art decoupled, parallel (or <i>asynchronous</i>) JIT compilation.</p>", "authors": [{"name": "Igor B&#246;hm", "author_profile_id": "81460656882", "affiliation": "University of Edinburgh, Edinburgh, United Kingdom", "person_id": "P2690485", "email_address": "I.Bohm@sms.ed.ac.uk", "orcid_id": ""}, {"name": "Tobias J.K. Edler von Koch", "author_profile_id": "81460657516", "affiliation": "University of Edinburgh, Edinburgh, United Kingdom", "person_id": "P2690486", "email_address": "T.J.K.Edler-Von-Koch@sms.ed.ac.uk", "orcid_id": ""}, {"name": "Stephen C. Kyle", "author_profile_id": "81485651740", "affiliation": "University of Edinburgh, Edinburgh, United Kingdom", "person_id": "P2690487", "email_address": "S.C.Kyle@sms.ed.ac.uk", "orcid_id": ""}, {"name": "Bj&#246;rn Franke", "author_profile_id": "81100183298", "affiliation": "University of Edinburgh, Edinburgh, United Kingdom", "person_id": "P2690488", "email_address": "bfranke@inf.ed.ac.uk", "orcid_id": ""}, {"name": "Nigel Topham", "author_profile_id": "81339532550", "affiliation": "University of Edinburgh, Edinburgh, United Kingdom", "person_id": "P2690489", "email_address": "npt@inf.ed.ac.uk", "orcid_id": ""}], "doi_number": "10.1145/1993498.1993508", "year": "2011", "article_id": "1993508", "conference": "PLDI", "title": "Generalized just-in-time trace compilation using a parallel task farm in a dynamic binary translator", "url": "http://dl.acm.org/citation.cfm?id=1993508"}