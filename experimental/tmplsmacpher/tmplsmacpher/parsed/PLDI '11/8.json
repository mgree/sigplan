{"article_publication_date": "06-04-2011", "fulltext": "\n An SSA-based Algorithm for Optimal Speculative Code Motion under an Execution Pro.le Hucheng Zhou Wenguang \nChen Fred Chow Tsinghua University Tsinghua University ICube Technology Corp. zhou-hc07@mails.tsinghua.edu.cn \ncwg@tsinghua.edu.cn frdchow@gmail.com Abstract To derive maximum optimization bene.ts from partial redundancy \nelimination (PRE), it is necessary to go beyond its safety constraint. Algorithms for optimal speculative \ncode motion have been devel\u00adoped based on the application of minimum cut to .ow networks formed out of \nthe control .ow graph. These previous techniques did not take advantage of the SSA form, which is a popular \nprogram representation widely used in modern-day compilers. We have de\u00adveloped the MC-SSAPRE algorithm \nthat enables an SSA-based compiler to take full advantage of SSA to perform optimal specula\u00adtive code \nmotion ef.ciently when an execution pro.le is available. Our work shows that it is possible to form .ow \nnetworks out of SSA graphs, and the min-cut technique can be applied equally well on these .ow networks \nto .nd the optimal code placement. We provide proofs of the correctness and computational and lifetime \noptimality of MC-SSAPRE. We analyze its time complexity to show its ef.\u00adciency advantage. We have implemented \nMC-SSAPRE in the open\u00adsourced Path64 compiler. Our experimental data based on the full SPEC CPU2006 Benchmark \nSuite show that MC-SSAPRE can fur\u00adther improve program performance over traditional SSAPRE, and that \nour sparse approach to the problem does result in smaller prob\u00adlem sizes. Categories and Subject Descriptors \nD.3.4 [Programming Lan\u00adguage]: Processors Compilers, Optimization General Terms Algorithms, Design, Experimentation, \nMeasure\u00adment, Performance, Theory Keywords Code motion, Flow network, Minimum cut, Partial re\u00addundancy \nelimination, Pro.ling, Speculation, Static single assign\u00adment 1. Introduction Partial redundancy elimination \n(PRE) is a powerful and indispens\u00adable optimization in modern optimizing compilers. It recognizes computations \nthat are redundant in some or all paths of execution and eliminate them. PRE has been modeled as a code \nplacement problem, in which the best set of insertion points for the computa\u00adtion is to be determined \n[19]. The insertions render some original computations to be fully redundant, so they can be trivially \ndeleted. Permission to make digital or hard copies of all or part of this work for personal or classroom \nuse is granted without fee provided that copies are not made or distributed for pro.t or commercial advantage \nand that copies bear this notice and the full citation on the .rst page. To copy otherwise, to republish, \nto post on servers or to redistribute to lists, requires prior speci.c permission and/or a fee. PLDI \n11, June 4 8, 2011, San Jose, California, USA. Copyright c &#38;#169; 2011 ACM 978-1-4503-0663-8/11/06. \n. . $10.00 Since global common subexpression elimination and loop-invariant code motion are special \ncases of PRE, they are subsumed by PRE. Knoop et al. developed the code placement strategy called lazy \ncode motion (LCM) that guarantees that the code placement per\u00adformed is both computationally and lifetime \noptimal [15][16]. A code placement is computationally optimal if the number of com\u00adputations cannot be \nfurther reduced by safe code motion [13]. A code placement is lifetime optimal if it is computationally \noptimal and the lifetimes of the temporaries introduced for storing the com\u00adputed values are minimized. \nIn safe code motion, a computation is not allowed to be inserted at a point in the program where the \ncomputation is not down\u00adsafe (i.e. fully anticipated). This safety attribute guarantees that the transformed \ncode will never run slower than the original program, but it limits the extents to which PRE can speed \nup the program. Speculation refers to inserting the computation at points in the program where the computation \nis not down-safe. It suppresses redundancy in some path at the expense of another path along which it \nis computed but unused. Speculation may or may not be bene.cial to program performance, because it depends \non which execution paths are taken more frequently. But when execution pro.le data are available, it \nis possible to perform speculative PRE to further improve the run-time performance for program executions \nthat match the given pro.le. Xue and Cai developed an algorithm that performs computationally optimal \nspeculative PRE based on an execution pro.le [3]. They later improved their algorithm to make it also \nlifetime-optimal [22]. LCM is based on a bit-vector formulation of the PRE problem and on the iterative \nsolution of the data .ow equations. Chow et al. developed the SSAPRE algorithm that adapts LCM to SSA \nform [5][14]. SSA is a popular program representation used in modern optimizing compilers [6]. Optimizations \nbased on SSA all take advantage of the sparse representation of SSA that also encodes use-def information. \nData .ow attributes can be propagated through SSA form in a smaller number of steps and in linear time, \ncompared to O(n 2) in iterative data .ow analysis. The lower computational complexities in SSA-based \nalgorithms is also attributed to their operating on one candidate at a time, instead of an entire population \nencoded in bit vector data structures. Another advantage of an SSA-based algorithm is that we do not \nhave to implement the local and global versions of the optimization separately, as SSA\u00adbased algorithms \nare insensitive to basic block boundaries. Thus, a compiler that uses SSA as its program representation \nwill always prefer to use SSA-based optimization algorithms. Like LCM, Xue and Cai s computational and \nlifetime optimal algorithm for speculative PRE, called MC-PRE, uses bit-vector\u00adbased data .ow analysis. \nNo SSA-based algorithm for optimal speculative code motion exists today. In this paper, we present the \nalgorithm we developed, called MC-SSAPRE, that performs opti\u00admal speculative code motion in the SSA framework. \nThe contribu\u00adtions of this paper are as follows: First, we show that a .ow network can be formed out \nof a SSA graph to solve for a minimum cut. We prove that the code placement according to the minimum \ncut is both computationally and lifetime optimal. A compiler that uses SSA as its program representation \ncan now continue to use the SSA form to perform optimal speculative pro.le-based PRE, thus reaping all \nthe bene.ts of SSA-based algorithms outlined above. Second, MC-SSAPRE computes the optimal code placement \nmore ef.ciently compared with other min-cut approaches based on the control .ow graph and non-SSA program \nrepresentation. Among the many reasons for the improved ef.ciency are the smaller sizes of SSA graphs \nand their resulting narrower solution space. Third, MC-SSAPRE requires only node frequencies to do its \njob, whereas MC-PRE requires edge frequencies. Thus, MC-SSAPRE exerts less demand on instrumentation \nand the pro.le data collection process, which is an important consideration in just-in-time compilers. \nUsing edge frequencies would not have been as straightforward as node frequencies for MC-SSAPRE, be\u00adcause \neach edge in the SSA graph can correspond to multiple edges in the control .ow graph. Lastly, we provide \nexperimental data that con.rm that MC-SSAPRE can further improve program performance over tradi\u00adtional \nPRE. We also provide statistical data about the problem size distribution to con.rm the advantage of \nour sparse approach to the problem. The rest of this paper is organized as follows: Section two discusses \nprior work in speculative code motion. Section three presents the MC-SSAPRE algorithm, proves its correctness \nand op\u00adtimality and analyzes its time complexity. Section four compares MC-SSAPRE with MC-PRE. Section \n.ve presents experimental results collected using our implementation of MC-SSAPRE. Sec\u00adtion six concludes \nthe paper and points out promising areas for fur\u00adther investigation.  2. Prior Related Work There are \nthree main classes of techniques to overcome the conser\u00advativeness of safe PRE: control speculation, \ndata speculation and control .ow restructuring. Control speculation refers to the introduction of a computation \nto a program path that does not need to perform that computation. The term speculation by itself refers \nto control speculation, since this is the most common form of speculation. Data speculation makes the \nassumption that certain values have not changed, so that more redundancies can be eliminated among the \ncomputations that use those values. Lin et al incorporated data speculation into the SSAPRE framework \n[17]. The technique requires hardware support in the form of special check instructions inserted in place \nof the eliminated computations so that, in the event the speculation fails, recovery code will be executed \nto correct the computation. Since such hardware support is not common, we only address control speculation \nin this paper. The most fundamental example of control .ow restructuring is the restructuring of a while \nloop to a do-while loop, as shown in Figure 1. Because a while loop may iterate zero times, moving a \ncomputation out of the loop may make the program run slower or alter the behavior of the program if the \nhoisted computation generates exceptions. By introducing an entry test that duplicates the termination \ncondition, the main loop can be converted to a do\u00adwhile loop, which iterates at least one time. This \nloop restructuring has been performed by the earliest optimizing compilers. Though it causes a small \nincrease in code size, it makes it unnecessary to incur speculation while performing loop-invariant code \nmotion. Figure 1. Traditional control .ow restructuring of while loops Bodik et al. applied more general \ncontrol .ow restructuring so as to enable more partial redundancies to be eliminated without speculation \n[2]. To avoid code size explosion, their control .ow restructuring is only applied conservatively in \na framework that also involves speculation with pro.le data. Speculation improves performance only when \nthe path that is burdened with more computations is executed less frequently than the path where the \ncomputations are avoided. Computations that can cause runtime exceptions or faults cannot be speculated, \nas they would alter the external behavior of the program. Murphy et al. proposed techniques that enable \nsuch computations to be more freely speculated under speci.c situations in the program [20]. They implemented \ntheir techniques in the SSAPRE framework. In the absence of pro.le data, speculative PRE can be conserva\u00adtively \nperformed by restricting it to loops. Such speculative loop\u00adinvariant code motion is generally pro.table \nbecause the bene.t of suppressing a computation inside the loop outweighs the cost of computing it once \nat the loop header. Lo et al. has implemented this conservative form of speculative PRE in the SSAPRE \nframework [18]. When execution pro.le data are available, it is possible to tailor the use of speculation \nto maximize run-time performance for exe\u00adcutions of the program that match the pro.le. Gupta et al. [9]and \nLo et al. [18] introduced different algorithms for performing PRE guided by pro.le data. But because \ntheir algorithms do not com\u00adpare frequencies in the global scope, they are not computationally optimal. \nHorspool and Ho [10] were the .rst to relate this problem to network .ow and proposed the use of a min-cut \nalgorithm, but their formulation of the algorithm did not always produce optimal solutions. The algorithm \ndeveloped in Bodik s Ph.D. thesis [1] also involves the technique of solving the min-cut in a .ow network. \nBe\u00adcause his framework includes control .ow restructuring under his Code-Motion-Preventing (CMP) Region \nabstraction in addition to speculation, the computational optimality of his algorithm cannot be clearly \nde.ned. To avoid the computational intensiveness of min-cut algorithms, Horspool et al. developed a heuristics-based \nalgorithm to perform speculative PRE [11]. Their algorithm, called Isothermal Specula\u00adtive PRE (ISPRE), \nrelies only on bit-vector-based data .ow anal\u00adyses that are faster and simpler than those used in PRE, \nmaking it suitable for deployment in just-in-time compilers. But in this pro\u00adcess, they sacri.ced the \noptimality of their results. Cai and Xue were the .rst to provide an algorithm for specu\u00adlative PRE \nthat is provably computationally optimal under an edge pro.le [3]. The key to their algorithm lies in \nthe removal of the non-essential edges from the control .ow graph (CFG) so that the problem of .nding \nthe optimal code placement is reduced to one of .nding a minimum cut in the reduced .ow graph. They subse\u00adquently \nenhanced their algorithm to produce a code placement that is also lifetime optimal [22]. In this paper, \nMC-PRE will refer to the enhanced version of their algorithm. Scholz et al. came up with their own formulation \nfor performing computationally optimal speculative code motion [21]. Though their algorithm is driven \nby local data .ow attributes, it does not involve any global data .ow analysis. Their .nal solution also \ninvolves .nding a minimum cut in the .ow graph formed out of the CFG. They regard the entry and exit \nof each basic block as two separate nodes, so they only need node frequencies for their solution. Finding \nthe min-cut corresponds to minimizing the objective function formed out of totaling the costs of the \nlocal transformations in the basic blocks. Their formulation also enables their algorithm to consider \nplacements that minimize the number of static occurrences. Their objective function can be chosen to \noptimize for speed or code size or a trade-off between the two. Despite the greater adaptability of the \nalgorithm of Scholz et al., we consider MC-PRE to be more practical for the following rea\u00adsons. First, \nScholz s .ow graphs are substantially larger not only because each basic block yields two nodes, but \nalso because they do not use global data .ow properties to remove edges that will not contribute to the \n.nal solution. Scholz et al. did employ a different technique that uses their correctness constraints \nto merge multiple adjacent nodes into one. Second, because the objective function in Scholz s algorithm \nrequires summing over many basic blocks, .nd\u00ading their min-cut is more computationally intensive. Third, \nScholz s algorithm does not optimize the lifetimes of the temporaries intro\u00adduced for storing computation \nresults for re-use. This issue is espe\u00adcially important when optimizing only for code size, because spec\u00adulative \ncode hoisting to reduce the number of static occurrences often increases the lifetimes of the temporaries. \nSuch opportuni\u00adties are also much more widespread than the opportunities for re\u00addundancy elimination. \nThe increased register pressure can cause indiscriminate register spilling that negates the effect of \ncode size reduction. The experimental data given in their paper [21] only con\u00adtain statistics collected \nduring their optimization phase. There is no measured data to show how the timing or code size of the \n.nal ex\u00adecutables improve. In this paper, we present a new algorithm, MC-SSAPRE, for performing computational \nand lifetime optimal speculative PRE based on applying min-cut to .ow networks formed out of SSA graphs. \n 3. The MC-SSAPRE Algorithm As in all literature related to PRE, we present our algorithm by focusing \non the optimization of an expression a+b that appears in the program. The PRE phase in the compiler will \napply the algorithm to each lexically identi.ed expression1 in the program one after another. Figure \n2 shows the running example we use to illustrate our al\u00adgorithm. Each rectangle denotes a basic block, \nwith its execution frequency below the block number to the left of each block. As shown in the example, \nthe program input to MC-SSAPRE is al\u00adready in SSA form. 1 Two expressions are lexically identi.ed to \nbe the same expression if they apply the same operation to the same variables or constants before being \nrepresented in SSA form. B1 B2a1+b1 20 50 B3 70 B5 B6 B7 10 10 B4 50 a1+b1 exit  3.1 The Algorithm \nWe .rst give an overview of how SSAPRE computes safe opti\u00admal PRE. Then we describe how we modify SSAPRE \nto make it perform optimal speculative PRE for an execution pro.le. The main focus is to determine the \nbest positions to perform insertions. Once the insertions have been performed, the computations ren\u00addered \nfully redundant can be trivially deleted. The overall code mo\u00adtion can be regarded as insertions followed \nby deletions. 3.1.1 SSAPRE Overview As described by Kennedy et al., the SSAPRE algorithm consists of \nsix steps [14]. The .rst two steps, F-Insertion and Rename, applies the SSA construction algorithm to \neach occurrence of a+b in the program. The F-Insertion step inserts F s at the dominance frontiers of \nall the occurrences of a+b.These F s capture all the possible insertion points for the purpose of PRE. \nThe Rename step assigns versions to the occurrences of a+b such that two occurrences with the same version \nwill compute to the same value. The renaming is done via a pre-order traversal of the dominator tree. \nA renaming stack is maintained for a+b during the traversal such that the top of the stack gives the \ncurrent version of the expression. The resulting SSA form for a+b is a representation of redun\u00addancy \nrelationships among the occurrences of a+b in the program. A less abstract view is to think of it as \nthe SSA representation of the hypothetical temporary h that will be introduced to store its com\u00adputed \nvalues for re-uses in the optimized output. The graph inher\u00adent in this expression SSA form is called \nthe factored redundancy graph (FRG). In this paper, the term SSA graph will refer exclu\u00adsively to the \nFRG, though it could also be used to refer to the graph inherent in the SSA form for variables. We use \nthe term real occur\u00adrences to refer to the occurrences of a+b that exist before the SSA construction. \nThus, F s and F operands are not real occurrences. One characteristic of FRG that is absent from the \nordinary SSA form for variables is that a F operand can be ., which means that there is no computation \nof a+b available at the incoming edge corresponding to that F operand. In addition, each edge in the \nFRG B1 B2h1 50 20 B3 70 B4 B5 B7h350 10 50 B8 B9 B10 60 5 5 B11 B13 B12 exit 50 510 B14 450 B15 B16 \nB17 exit 300 100 50 B18 400 exit Figure 3. After SSA construction where h refers to a+b that ends at \na F operand is marked with a has real use .ag. This .ag is set to true if the F operand is de.ned by \na real occurrence of a+b or if the path leading to its de.nition by a F crosses a real occurrence of \na+b. The third and fourth steps of SSAPRE, DownSafety and Will-BeAvail, perform data .ow analysis on \nthe SSA graph to solve for the F s where insertions are to be performed for safe PRE. After the fourth \nstep, the insertion points for a+b have been completely de\u00adtermined. For each F operand, it sets the \npredicate insert to denote whether insertion will be performed there. A F operand s insert is set to \ntrue if and only if the following hold: the F s will be avail attribute is true;and  the operand is \n.;or has real use is false for the operand and the operand is de.ned by a F whose will be avail attribute \nis false.  The .fth step of SSAPRE, Finalize, determines the form of the optimized code by introducing \nthe real temporary t for storing the computed values of a+b. For each occurrence of a+b, it determines \nif its computation should be deleted and replaced by a use of t,and if not, if its computed value should \nbe saved to t for re-use later. The uses of t are linked to their defs and extraneous f s for t are removed \nso that t is put in its minimal SSA form. The sixth and last step of SSAPRE, CodeMotion, effects the \nPRE of a+b by updating the native SSA representation of the program.  3.1.2 MC-SSAPRE Preview MC-SSAPRE \nmakes the same assumption as in SSAPRE that all critical edges in the CFG have been removed by inserting \nempty basic blocks at such edges2. SSAPRE s code placement for a+b is guaranteed to be bene.cial regardless \nof the execution pro.le. MC-SSAPRE will compute a different set of insertion points for a+b to effect \ncode motion that goes beyond that of SSAPRE so as to minimize the number of dynamic computations according \nto the execution pro.le. From this point of view, MC-SSAPRE 2 A critical edge is one where both its head \nand tail blocks have multiple successors and predecessors respectively. 1. F-Insertion Same as in SSAPRE. \n 2. Rename Same as in SSAPRE; in addition, mark use nodes dominated by its own non-F SSA versions as \nrg excluded. 3. Data Flow Solve for full availability and partial anticipa\u00adbility 4. Graph Reduction \n Form the reduced SSA graph by includ\u00ading only the following:  F nodes that are not fully available \nand also partially anticipated, and  their use nodes that are not marked rg excluded,and  the def-use \nedges between the above two types of nodes.  5. Single Source Create an arti.cial source node for \nthe reduced SSA graph; for at each . F operand in the SSA graph, create an edge that leads from the source \nnode to the F node, annotating its frequency with the frequency of the predecessor basic block corresponding \nto that . F operand. 6. Single Sink Create an arti.cial sink node for the reduced SSA graph; for each \nreal occurrence in the reduced SSA graph, create an edge that leads from the real occurrence node to \nthe sink, annotating its frequency as in.nity. 7. Min-Cut Find the optimal insertion points as a minimum \ncut on the formed single-source single-sink graph. Pick later cuts in case of ties. 8. WillBeAvail \nCompute the insert and will be avail at\u00adtributes for each F based on the determined insertion points. \n 9. Finalize Same as in SSAPRE. 10. CodeMotion Same as in SSAPRE.  Figure 4. Steps of the MC-SSAPRE \nalgorithm only needs to differ from SSAPRE in how the insertion points are computed. In fact, we have \ndesigned MC-SSAPRE so that it differs from SSAPRE only in the latter s steps 3 and 4, apart from making \nstep 2 compute an additional attribute. MC-SSAPRE consists of the ten steps summarized in Figure 4. We \nhave replaced SSAPRE s steps 3 and 4 by MC-SSAPRE s steps 3 to 8. Steps 9 and 10 of MC-SSAPRE are the \nsame as SSAPRE s steps 5 and 6. 3.1.3 MC-SSAPRE Steps 1 and 2 Figure 3 shows our running example after \nthe .rst two steps. The constructed SSA graph is identical to that from SSAPRE. As in SSAPRE, the F nodes \nin this SSA graph capture all the possible insertion points for performing optimal speculative code motion. \nBecause in step 4, we want to exclude occurrences at points in the graph where the computation is fully \navailable, we use the pre\u00adorder traversal of the dominator tree during step 2 to easily iden\u00adtify occurrences \nmade fully redundant by single real occurrences. This requires us to push initially encountered real \noccurrences on the expression stack, regardless of whether it de.nes a new version or not. Any subsequent \nencounter of an occurrence of the same operands versions as the real occurrence on the top of the expres\u00adsion \nstack will cause that occurrence to be marked with the attribute rg excluded, which means excluded from \nreduced graph . Such rg excluded occurrences do not need to be pushed on the stack. In Figure 3, h2 at \nB9 and h5 at B18 are marked rg excluded after step 2. 3.1.4 MC-SSAPRE Step 3 Next, we want to identify \nnodes and edges in the SSA graph that will never be useful insertion points even under speculative code \n h1 avail=0 pant=1 excluded Figure 5. SSA graph after data .ow analysis and excluding useless nodes \nand edges (steps 3 and 4 of MC-SSAPRE) motion. For this purpose, we solve for the following two data \n.ow attributes: Full availability Any insertion at a point in the graph where the computation is fully \navailable would be a redundant com\u00adputation, so such insertions can be precluded.  Partial anticipability \n Any insertion at a point in the graph where the computation is not partially anticipated would be a \nuseless computation, so such insertions can be precluded.  Each of the above two data .ow attributes \ncan be ef.ciently computed via the typical one-pass data .ow propagation algorithm for SSA graphs [14]. \n 3.1.5 MC-SSAPRE Step 4 After computing the above two data .ow attributes in step 3, step 4 starts with \nan empty graph and construct the reduced SSA graph as describedinstep4ofFigure 4 using the data .ow attributes \ncomputed in step 3. Figure 5 shows the reduced SSA graph of our running example after steps 3 and 4. \nNote the excluded edges. For our running examples, none of the F s can be excluded. In MC-SSAPRE, the \nexecution pro.le only needs to provide node frequencies. But min-cut on the .ow graph relies on edge \nfrequencies. We now describe how to derive the edge frequencies in the reduced SSA graph from node frequencies \nin the CFG based on how we perform insertions on edges in the SSA graph. In our reduced SSA graph, there \nare only two types of edges, which we refer to as follows: Type 1 edge an edge that leads from F to \nF operands. Type 2 edge an edge that leads from F to a real occurrence. In Figure 5, the edge ending \nat B8 and the two edges ending at B14 are type 1 edges. In performing an insertion at a type 1 edge, \nthe absence of critical edges allows us to insert at the exit of the predecessor block corresponding \nto the F operand. For this reason, we annotate each type 1 edge with the node frequency of the predecessor \nblock that corresponds to the F operand. In our example, the edge ending at B8 is annotated with the \nnode frequency of B5, which is 10; the edge from B8 to B14 is annotated with the node frequency of B11, \nwhich is 50; the backward edge from B18 to B14 is annotated with the node frequency of B18, which is \n400. Figure 6. Completed .ow network (EFG) after adding the arti.cial source and sink, performing min-cut \nand computing will be avail (step 5, 6, 7 and 8 of MC-SSAPRE) In Figure 5, the edges ending at B6 and \nB15 are type 2 edges. A cut at a type 2 edge does not require an insertion. Instead, we can just postpone \nthe computation until the real occurrence, at which it will be computed in place. For this reason, we \nannotate a type 2 edge with the node frequency of the block of the real occurrence. In our example, the \nedge from B3 to B6 is annotated with the frequency of B6, which is 10; the edge from B14 to B15 is annotated \nwith the frequency of B15, which is 300. 3.1.6 MC-SSAPRE Step 5 Using the edge insertion model, the \nedges in the CFG leading to . F operands represent the earliest useful insertion points for specu\u00adlative \ncode motion. As we ll prove in Section 3.2, any insertion be\u00adfore them would not be more computationally \noptimal and would increase the live range of the temporary. Step 5 introduces an arti\u00ad.cial source node \nas the common head of these edges. Since each of these edges from the source node ends at a . F operand, \nthey are type 1 edges. As a result, each of them is annotated with the node frequency of the corresponding \npredecessor block of its . F operand. 3.1.7 MC-SSAPRE Step 6 The sinks of the .ow network are represented \nby all the real occur\u00adrences in the reduced SSA graph. To complete the .ow network, step 6 introduces \nan arti.cial sink node and adds edges that lead to it from all the real occurrences in the graph. These \nedges are marked with the largest possible frequency (in.nite in our illustra\u00adtion) to prevent any of \nthem from becoming part of the min-cut. As a result, the minimum cut will always split the .ow network \nsuch that all the real occurrences are located downstream from the cut. This in turn guarantees that \nthe computation will have been performed before reaching any of the real occurrences. Figure 6 shows \nthe single-source single-sink .ow network formed out of our running example. We call it the essential \n.ow graph (EFG), since it is the basis for performing the minimum cut. 3.1.8 MC-SSAPRE Step 7 Minimum \ncut is de.ned as a cut that separates the source from the sink such that the sum of the weights across \nthe cut edges is minimized. Step 7 performs this task in the .ow network. To ensure procedure Reset \nwill be avail(g) will be avail(g) . false for each f . set of F s in the SSA graph with operand . such \nthat g = def(.) if (not has real use(.) and not insert(.) Reset will be avail(f) end Reset will be avail \nprocedure Compute will be avail() for each f . set of F s in the SSA graph will be avail(f) . true for \neach f . set of F s in the SSA graph if (will be avail(f) and . an operand . of f that is . and not insert(.)) \nReset will be avail(f) end Compute will be avail Figure 7. MC-SSAPRE s algorithm used in step 8 to compute \nwill be avail that we .nd the unique solution that minimizes the lifetime of the temporary, we use the \nReverse Labeling Procedure of Ford and Fulkerson [7]. Referring to our running example, there are actually \ntwo possible min-cuts: {(source, B3), (source, B18)} or {(B3,B8), (B3, B6), (source, B18)}3. The min-cut \nalgorithm we apply will pick the second one, shown in Figure 6, since it yields a smaller live range \nfor the temporary.  3.1.9 MC-SSAPRE Step 8 This step computes the insert attribute for each F operand \nand the will be avail attribute for each F. The purpose of this step is to enable our implementation \nto use the algorithms in Step 5 of SSAPRE without modi.cation. An implementation may choose to skip this \nstep, in which case the Finalize step will need to be modi.ed to take the min-cut result as input. In \nSSAPRE, the will be avail attribute tells whether the com\u00adputation will be fully available at each F \nafter PRE. This will then be used to compute the insert attribute. In MC-SSAPRE, we can easily determine \nthe insert attribute from the result of the min-cut by setting a F operand s insert to true if a cut \nis at its incom\u00ading edge. The will be avail attribute is computed by forward prop\u00adagation in the SSA \ngraph based on the settings of the insert at\u00adtribute. Figure 7 gives the pseudo-code for Compute will \nbe avail. The forward propagation is performed in the recursive procedure Reset will be avail. The .nal \nvalues for will be avail in our run\u00adning example is shown via the wba annotations in Figure 6.  3.1.10 \nMC-SSAPRE Steps 9 and 10 By computing the insert and will be avail attributes in step 8, the algorithms \nin SSAPRE s steps 5 and 6 can be re-used for these last two steps. Figure 8 shows the .nal output after \napplying MC-SSAPRE to our running example.  3.2 Correctness and Optimality We now cover the theoretical \naspects of the MC-SSAPRE algo\u00adrithm. We refer to the expression being optimized as X and begin by stating \nMC-SSAPRE s job. 3 The basic blocks are not part of the SSA graph structure; we use the block numbers \nin our discussion purely for easy reference to the various parts of the SSA graph. B1 50 a1+b1 B2 20 \n 3.2.1 Problem Statement De.nition 1 Original computation points are the points in the input program \nwhere X s computations take place. MC-SSAPRE will .nd a new set of computation points for X that satis.es \nall of the following criteria: 1. Correctness The new set of computation points for X makes X fully \navailable at its original computation points so they can be deleted. 2. Computational optimality The \nset of new computation points minimizes the dynamic number of computations of X under the given execution \npro.le. 3. Lifetime optimality Subject to the computational optimality, the life range of the temporary \nintroduced to store X is mini\u00admized.  3.2.2 Proof of Correctness We call a partial redundancy strictly \npartial if it is not redundant along some path. Strictly partial redundancies can only be elimi\u00adnated \nby performing insertions. Observe that the each occurrence of X at their original compu\u00adtation points \ncan be quali.ed by exactly one of the following: fully redundant,  strictly partially redundant (SPR), \nor  non-redundant.  LEMMA 1. The fully redundant occurrences of X at the original computation points \ncan be excluded from consideration in deter\u00admining the new computation points without affecting the correct\u00adness \nand optimality requirements for MC-SSAPRE s output. Proof The fully redundant occurrences of X are made \nfully re\u00addundant by the SPR and non-redundant occurrences in the input program. In the output program, \nX will continue to be available at the SPR and non-redundant original computation points. Thus, X will \ncontinue to be fully redundant at the fully redundant original computation points. MC-SSAPRE s step \n2 starts looking for those occurrences made fully redundant by single real occurrences and marks them \nrg excluded. The remaining fully redundant occurrences are iden\u00adti.ed based on the fully available attribute \ncomputed in step 3 and excludedinstep4. Next, we use Lemma 3 from Kennedy et al. [14] and re-word it \nas the following: LEMMA 2. The factored redundancy graph (FRG) constructed in steps 1 and 2 captures \nall redundancies in X in the program. Any edge in the FRG represents a redundancy relationship be\u00adtween \ntwo occurrences of X. Between any two mutually redundant occurrences of X, there must be a path in the \nFRG between them. An occurrence with no predecessor in the FRG is non-redundant. For such occurrences, \nthe best strategy is to compute in place,since inserting earlier would only lengthen the temporary s \nlive range un\u00adnecessarily. Based on this, we can also conclude that the best set of .nal computation \npoints will always include these non-redundant occurrences. In Figure 3, h1 in B1 and h3 in B4 are non-redundant \noccurrences. Thus, for the rest of our proofs, we only need to attend to the SPR occurrences. We refer \nto the new computation points found by MC-SSAPRE for X as insertion points.Weuse I to re\u00adfer to any set \nof insertion points found by MC-SSAPRE and Icorr to refer to all the I s that satisfy the correctness \ncriterion. Icorr designates the complement of Icorr. De.nition 2 Useless insertion points are the points \nin the input program where X is either fully available or not partially antici\u00adpated. This was explained \nearlier in 3.1.4. We can infer that any part of the CFG that cannot reach the FRG, including the part \ndownstream from the FRG, are useless insertion points because X is not par\u00adtially anticipated. Thus, \nall useful insertion points lie in the part of the CFG covered by the FRG plus the region of the CFG \npreceding it starting from the entry point. We now prove that the best insertion points can be located \nbased on the FRG. For this, we need the following two lemmas that enable us to model insertions on the \nFRG instead of the CFG: LEMMA 3. The best insertion point on an edge in the FRG that ends at a F operand \n(the type 1 edge of Section 3.1.5)isatthe exit of the predecessor block P corresponding to that F operand. \nProof Because of the absence of critical edges, block P must not have any successor other than the block \ncontaining the F. Thus, each traversal along that edge in the FRG will visit block P exactly once. The \nexit of block P is also the latest insertion point, resulting in the shortest life range for the temporary. \nLEMMA 4. The best insertion point on an edge from A to B in the FRG where B is a real occurrence (the \ntype 2 edge of Section 3.1.5) is not to insert at any edge, but to compute X in place at B. Proof If \nA is a real occurrence, any insertion on the edge is unnec\u00adessary. Thus, A must be a F. Since A dominates \nB, the only way for B to be executed more often than A is for B to be part of a loop inside the mini-CFG \nfrom A to B. Such a loop is not possible due to the absence of any F from A to B. Thus, it is best to \ncompute X as late as possible at its original point in B, which yields the shortest life range for the \ntemporary if the value needs to be saved. We have now proven that all useful insertion points can be \nlocated on the reduced FRG, formed at the end of step 4, plus the region of the CFG preceding it starting \nfrom the entry point. LEMMA 5. In the reduced FRG formed at the end of step 4, I . Icorr if and only \nif there exists a path from a . F operand to any SPR occurrences with no insertion of X. Proof Assume \nwe are to solve for the full availability attribute after performing the insertions represented by I. \nWe would start by ini\u00adtializing all F s to fully available except any F with any . operand that has no \ninsertion will be initialized to not fully available, which represent the boundary condition for the \nforward propagation. In the subsequent forward propagation, since X is partially antici\u00adpated at all \nthe F s, some F that has an SPR occurrence among its uses will be made not fully available. I .Icorr \nbecause X is not fully available at some SPR occurrence. For the converse, suppose I .Icorr. That means \nsome SPR occurrence is de.ned by a F that is not fully available. Such a F must either have a . F operand \nthat has no insertion, or has an operand de.ned by another F that is not fully available. By induction, \nat least one F in the reduced FRG has a . F operand that has no insertion. Lemma 5 implies that for each \nI .Icorr, there is at least one insertion along any path that leads from a . F operand to a SPR occurrence. \nAlong each such path, we want to perform one and only one insertion since that is suf.cient to achieve \nthe correctness criteria. We refer to such I s as belonging to Icut .Icorr.We view the . F operands as \nsources, the SPR occurrences as sinks, and each insertion as a cut along a path. Next we make Lemma 3 \napplicable to insertions at . F operands by performing step 5, which introduces an arti.cial source node \nwith edges that lead from it to all the . F operands. The implication is that any potential insertion \npoint in the region of the CFG preceding the FRG does not need to be considered, because it will not \nbe as good as insertion at the exits of the prede\u00adcessor blocks corresponding to the . F operands that \nit will reach. With the arti.cial source node, the reduced FRG also becomes one connected .ow network. \nIn order to apply the classical min-cut algorithm, we perform step 6 to introduce a single arti.cial \nsink. By annotating edges leading from the multiple SPR occurrences to the single sink with in.nite weight, \nwe ensure insertion will not be performed on these arti.cial edges. THEOREM 6. MC-SSAPRE is correct. \nProof This is the result of combining Lemmas 1 to 5, and the fact that the set of insertion points found \nby MC-SSAPRE satis.es I .Icut .Icorr. 3.2.3 Proof of Computational Optimality We use Icomp to refer \nto the subset of Icorr in which the dynamic number of computations of X is at the smallest possible value. \nTHEOREM 7. MC-SSAPRE is computationally optimal. Proof In the EFG formed at the end of step 6, the weight \nof each edge represents the additional number of times X would be computed during program execution if \nthere is an insertion of X at that edge. In step 7, by .nding the minimum cut, the total number of computations \nof X arising from the insertions across all the edges of the cut is minimized. Thus, the insertions at \nand only at all the edges of the minimum cut is the computationally optimal set of insertions. 3.2.4 \nProof of Lifetime Optimality We now show that MC-SSAPRE yields minimum live range for the temporary t \nintroduced to store X subject to the computational optimality criterion. We .rst prove the correctness \nof step 8: LEMMA 8. Given a set of insertions of X at F operands, the WillBeAvail algorithm of Figure \n7 causes a F to be marked will be avail if and only if X is fully available at the F. Proof The algorithm \nto compute will be avail showninFigure 7 computes the fully available attribute via forward propagation \nin the FRG. The algorithm is correct since it applies the standard depth-.rst search propagation technique \non SSA graphs. THEOREM 9. MC-SSAPRE is lifetime optimal. Proof There are two factors in MC-SSAPRE that \ncontribute to life\u00adtime optimality. The .rst factor is that the insertions performed need to be as late \nas possible on the FRG. The fact that the min\u00adimum cut may not be unique means that among the candidates \nof Icomp, we need to .nd Iopt in which the cut along each path is as close to the sink as possible. This \nhas been achieved by applying the Reverse Labeling Procedure of Ford and Fulkerson [7] while performing \nmin-cut in step 7. The second factor is for ensuring that the temporary t for storing X is not unnecessarily \nstored into or loaded. Lemma 8 assures that we feed the correct information to the Finalize step of Step \n9. Since steps 9 and 10 are the same as SSAPRE s steps 5 and 6, we cite the lifetime optimality of SSAPRE \nestablished by Kennedy et al. [14] to claim this second factor for MC-SSAPRE.  3.3 Time Complexity \nUsing the same analysis as Kennedy et al. [14], we can establish the complexities of steps 1, 2, 3, 8, \n9 and 10 as being linear with respect to the sum of the number of nodes and number of edges in the FRG. \nThe same linearity applies to step 4. Steps 5 and 6 can be done simultaneously in one pass throught the \nnodes of the FRG, so is also linear. That leaves step 7, .nding the minimum cut, as being the only step \nnot linear in complexity. Chekuri et al. studied the performance of several minimum cut algorithms and \nfound that they are all of polynomial time v complexities [4]. The algorithm we use is O(V 2 E),where \nV and E are the number of nodes and edges in the ERG respectively. Since the EFG is a subset of the FRG, \nand the FRG is usually much smaller than the CFG, MC-SSAPRE is much less affected by the polynomial time \ncomplexity of the min-cut step.  4. Comparison with MC-PRE Xue and Cai s MC-PRE algorithm [22] .nds \nthe optimal insertion points for pro.le-driven speculative code motion by determining the minimum cut \nin the .ow network formed by removing non\u00adessential edges from the CFG. Our work on MC-SSAPRE has shown \nthat this min-cut approach can be applied equally well in the FRG context to yield similarly optimal \nresults. But there are many intrinsic differences between the two algorithms. MC-PRE eliminates only \nstrictly global redundancies. A sep\u00adarate technique like local common subexpression elimination has to \nbe applied to each basic block to cover locally redundant com\u00adputations. In contrast, MC-SSAPRE covers \nboth global and local redundancies uniformly. MC-PRE uses classical bit-vector-based data .ow analysis \nto solve for full availability and partial anticipability. Thus, it can solve for all the expressions \nin the program at the same time. MS-SSAPRE solves for these data .ow attributes one expression at a time, \nbut its representation is sparse and the propagation algorithm is linear. Both approaches handle one \nexpression at a time in the remaining parts of their algorithms. MC-PRE s .ow networks are always based \non the CFG4.All the nodes in each reduced .ow graph that have no predecessor have to be connected to \nthe single source node. The edges coming out of the single source node cannot be insertion points, and \nthey 4 Some basic blocks in MC-PRE s CFG have to be split to allow the top part to function as a source \nand the bottom part to function as a sink. are annotated with in.nite frequency. For MC-SSAPRE, the \n.ow graphs are formed from the SSA graphs. They are sparse, because the number of nodes depends only \non how many times the expres\u00adsion occurs and the F nodes instantiated by them. MC-SSAPRE has fewer edges \nemanating from the single source node, because only F nodes with . operands need to be connected to the \nsingle source. These edges coming out of the single source can themselves be insertion points. The reduced \nsearch space in MC-SSAPRE s .ow networks enables its min-cut solution to incur much less over\u00adhead relative \nto MC-PRE. MC-SSAPRE s running time for each expression depends more on the problem size and less on \nthe size of the program. Since the min-cut parts of both algorithms work on one expression at a time, \n.ow graph size is the overriding factor affecting ef.ciency. Since MC-PRE uses edge placement for performing \ninsertions, it uses edge frequencies to model the costs of insertions. Though MC-SSAPRE also models insertions \nas edge placements, it only inserts at the ends of the predecessor blocks. This requires all crit\u00adical \nedges to have been removed by inserting empty blocks at such edges. In Section 3.1.5, we have described \nhow the frequencies of the edges of the .ow network are determined based on the blocks where the computation \nwill be inserted. This in effect enables us to derive edge frequencies in the SSA graph from node frequencies \nin the CFG. In pro.ling, collecting node frequencies incurs less run\u00adtime overhead than edge frequencies. \nTo ensure that a computation that has no redundancy is not unnecessarily stored to its temporary, and \nto ensure that any in\u00adserted computation post-dominated by its .rst use is not computed earlier than \nneeded, MC-PRE conducts a third data .ow analy\u00adsis phase to identify such isolated computations to prevent \nun\u00adnecessary code motion. The latter situation corresponds somewhat to MC-SSAPRE s type 2 edge discussed \nin Section 3.1.5.MC-SSAPRE s WillBeAvail and Finalize steps can be regarded as cor\u00adresponding to MC-PRE \ns live range analysis for the temporary. But MC-SSAPRE s .nal output is in SSA form. MC-PRE would not \nmeet the need of a compiler that uses SSA as its program represen\u00adtation unless it incurs the additional \ncost to construct SSA from its output. 5. Experimental Results We have implemented MC-SSAPRE in the \nopen-sourced Path64 compiler [8]. This compiler is a descendant of the SGI MIPSpro compiler in which \nthe SSAPRE algorithm was originally developed [5][14]. This allows us to make use of its SSAPRE infrastructure \nin our implementation of MC-SSAPRE. The current compiler is targeted to x86/x86-64, which is the most \nwidely used processor in servers, desktops and notebook computers nowadays. We implemented MC-SSAPRE \nas we have described in Section 3.1, replacing steps 3 and 4 of SSAPRE by steps 3 to 8 of MC-SSAPRE. \nThe compiler always restructures while loops in the way depicted in Figure 1 so that loop-invariant code \nmotion can occur without involving speculation in these loops. We .rst provide experimental data based \non benchmark running times. Then we provide statistical data on the sizes of the EFGs formed by MC-SSAPRE \nduring the compilation of the same bench\u00admarks. 5.1 Run-time Performance Improvements Our benchmarking \nmachine is a Linux system based on the Intel CoreTM i7-970 Processor running at 2.67GHz with 8MB on-chip \ncache. The operating system is Ubuntu 9.10 and the system has 6GB on-board memory. We use the popular \nSPEC CPU2006 Benchmark Suite [12] in performing our experiments. The SPEC benchmarks facilitate feedback-directed \noptimizations (FDO) by providing training input Table 1. CINT2006 execution times and speedup ratios \nof MC-SSAPRE relative to SSAPRE and SSAPREsp Benchmark A. SSAPRE B. SSAPREsp C. MC-SSAPRE A-C A B-C \nB perlbench 455 sec 452 sec 442 sec 2.86% 2.21% bzip2 599 sec 598 sec 583 sec 2.67% 2.51% gcc 406 sec \n402 sec 397 sec 2.22% 1.24% mcf 414 sec 415 sec 409 sec 1.21% 1.45% gobmk 539 sec 540 sec 526 sec 2.41% \n2.59% hmmer 528 sec 526 sec 521 sec 1.33% 0.95% sjeng 600 sec 601 sec 593 sec 1.17% 1.33% libquantum \n602 sec 610 sec 565 sec 6.15% 7.38% h264ref 1696 sec 1720 sec 1682 sec 0.83% 2.21% omnetpp 538 sec 525 \nsec 516 sec 4.09% 1.71% astar 611 sec 621 sec 609 sec 0.33% 1.93% xalancbmk 1472 sec 1489 sec 1467 sec \n0.34% 1.48% Average 2.13% 2.25% data that are separate from the input data for the reference runs. Compilation \nwith FDO uses the execution pro.les generated by the training runs. We compare the benchmark running \ntimes, measured in sec\u00adonds, between PRE performed via SSAPRE and MC-SSAPRE. All benchmarks were compiled \nat -O3 and with FDO. The original SS-APRE implementation in our compiler does not use pro.le data even \nwhen the compilation is with FDO. It by default performs speculation for invariant computations inside \nloops. In our experi\u00adment, we perform the following three separate compiles and runs: A. SSAPRE PRE \nvia SSAPRE with speculation turned off, with no use of pro.le data B. SSAPREsp PRE via SSAPRE with loop-based \nspeculation, with no use of pro.le data C. MC-SSAPRE PRE via MC-SSAPRE (with speculation), with use \nof pro.le data All other optimization phases in the three compiles are unchanged. The SPEC CPU2006 Benchmark \nSuite is divided into the CINT2006 suite, made up of 12 integer benchmarks, and the CFP2006 suite, made \nup of 17 .oating-point benchmarks. We show the running times of the full set of CINT2006 benchmarks in \nTa\u00adble 1, and the running times of the full set of CFP2006 benchmarks in Table 2. The running times displayed \nin the second to fourth columns of the tables correspond to A, B and C described above. Each running \ntime shown is the average of three runs. We found very little variation among the three runs of each \nbenchmark, which means any observed difference in performance is real and repro\u00adducible. The .fth column \nof each table gives the speedup of MC-SSAPRE over safe SSAPRE. For CINT2006, the speedup ranges from \n0.33% to 6.15%, with an average of 2.13%. For CFP2006, the speedup ranges from 0.87% to 8.78%, with an \naverage of 2.76%. The last column of each table gives the speedup of MC-SSAPRE over SSAPRE with loop-based \nspeculation. For CINT2006, this speedup ranges from 0.95% to 7.38%, with an average of 2.25%. For CFP2006, \nthis speedup ranges from 0.29% to 8.61%, with an average of 1.96%. The data show that the loop-based \nspeculation in the original compiler is not always bene.cial relative to safe SSAPRE. But it is more \nbene.cial in the .oating-point benchmarks than the integer benchmarks, since .oating-point benchmarks \nin general are more loop-oriented than integer benchmarks. This explains why, out of the four averages, \nMC-SSAPRE gives the highest average speedup when compared with safe SSAPRE in CFP2006, and the lowest \nTable 2. CFP2006 execution times and speedup ratios of MC-SSAPRE relative to SSAPRE and SSAPREsp Benchmark \nA. SSAPRE B. SSAPREsp C. MC-SSAPRE A-C A B-C B bwaves 386 sec 382 sec 375 sec 2.85% 1.83% gamess 1034 \nsec 1030 sec 997 sec 3.58% 3.20% milc 450 sec 452 sec 443 sec 1.56% 1.99% zeusmp 407 sec 403 sec 400 \nsec 1.72% 0.74% gromacs 578 sec 576 sec 570 sec 1.38% 1.04% cactusADM 547 sec 546 sec 499 sec 8.78% 8.61% \nleslie3d 598 sec 588 sec 584 sec 2.34% 0.68% namd 506 sec 505 sec 500 sec 1.19% 0.99% dealII 476 sec \n465 sec 459 sec 3.57% 1.29% soplex 654 sec 641 sec 616 sec 5.81% 3.90% povray 324 sec 326 sec 315 sec \n2.78% 3.37% calculix 825 sec 823 sec 811 sec 1.70% 1.46% GemsFDTD 505 sec 502 sec 493 sec 2.38% 1.79% \ntonto 552 sec 541 sec 535 sec 3.08% 1.11% lbm 344 sec 342 sec 341 sec 0.87% 0.29% wrf 593 sec 588 sec \n586 sec 1.18% 0.34% sphinx3 701 sec 691 sec 686 sec 2.14% 0.72% Average 2.76% 1.96% average speedup \nwhen compared with SSAPRE with loop-based speculation in CFP2006. In general, the usefulness of FDO depends \non how well the training runs correlate with the reference runs. Any non-correlation could be the cause \nof the smaller improvements of MC-SSAPRE observed in some of the benchmarks. Figure 9 and Figure 10 display \nthe data of Table 1 and Table 2 respectively in bar chart form after normalizing the running times under \nsafe SSAPRE to 1. These running time data verify that the optimal speculative code motion performed by \nMC-SSAPRE is effective in improving program performance beyond traditional SSAPRE. The positive across-the-board \nimprovements are a good indication of the optimality of MC-SSAPRE. 5.2 Sizes of EFGs To verify the advantages \nof our sparse approach to speculative code motion, we collected data on the sizes of the EFGs formed \nby MC-SSAPRE during the course of compiling the 29 SPEC CPU2006 benchmarks. We exclude empty EFGs in \nour statistics, and measure graph sizes according to the number of nodes in the graph. The results are \nshown in the bar chart of Figure 11, where each bar gives the number of EFGs of the size(s) given in \nthe horizontal axis. The most noteworthy observation is that 50% of the EFGs have only 4 nodes. A non-empty \nEFG cannot be smaller than 4 nodes because there must be at least the arti.cial source node, the arti.cial \nsink node, a F node and a strictly partially redundant real occurrence node for it to require a minimum \ncut. As the sizes of EFGs increase, their numbers taper off very quickly. Among the 183,152 EFGs formed \nin the compilation of the 29 benchmarks, there are only 24 EFGs larger than 300 nodes, and the largest \nEFG has 805 nodes. Figure 11 also plots the cumulative percentages of EFGs with sizes less than the given \nnumber of nodes. We found that 86.5% of the EFGs are less than or equal to 10 nodes, 99.0% of the EFGs \nare less than or equal to 50 nodes, and 99.67% of the EFGs are less than or equal to 100 nodes. This \nveri.es that our SSA-based approach does result in mostly small and trivial EFGs, which helps to ensure \nthat the polynomial time complexity of the min-cut step would only have very limited impact on the optimization \nef.ciency.  Figure 9. Performance comparison between SSAPRE, SSAPREsp and MC-SSAPRE based on the CINT2006 \nbenchmarks   6. Conclusion and Further Work Speculative code motion under an execution pro.le can \nbe mod\u00adeled as a .ow network problem. Previous works have solved this problem optimally by applying min-cut \nto .ow networks formed out of the control .ow graphs of the programs. In this paper, we have shown that \nthe min-cut technique can be applied equally well to .ow networks formed out of SSA graphs. Since SSA \nis a sparse program representation, this SSA-based approach to solve for op\u00adtimal speculative code motion \nsubstantially reduces the problem size, thus enabling the optimization to be performed much more ef.ciently. \nThe techniques described in this paper enables a com\u00adpiler that uses SSA as its internal program representation \nto per\u00adform optimal speculative code motion with very low compile-time overhead. This in turn opens the \nway for this optimization to be deployed in just-in-time compilers, where it can put the collected dynamic \npro.le data to good use and with low re-compilation time penalty. This paper has only addressed computational \nand lifetime op\u00adtimality in speculative code motion. Performing speculative code motion has already bene.ted \ncode size by making it unnecessary for the compiler to do traditional control .ow restructuring of the \nkind depicted in Figure 1. There is potential for using speculative code motion to further decrease code \nsize, as shown by the work of Scholz et al. [21]. It may also be possible to use our framework to optimize \nother program properties sensitive to code positioning, like power consumption. We are currently investigating \nthese other possibilities for our SSA-based min-cut technique, and will report on any interesting .nding. \n Acknowledgments We would like to thank the anonymous reviewers for their construc\u00adtive feedback regarding \nthe contents of this paper. We would also like to thank the PathScale team, especially Christopher Bergstr\u00a8om, \nfor their support of the compiler during the experimental phase of this project. This work was funded \nby the National Core Scien\u00adti.c and Technological Development Project of China under Grant 2009ZX01036-001-002. \n References [1] R. Bodik. Path-Sensitive Value-Flow Optimizations of Programs.PhD thesis, University \nof Pittsburgh, 1999. [2] R. Bodik, R. Gupta, and M. L. Soffa. Complete removal of redundant expressions. \nIn Proceedings of the ACM SIGPLAN 98 Conference on Programming Language Design and Implementation, pages \n1 14, 1998. [3] Q. Cai and J. Xue. Optimal and ef.cient speculation-based partial redundancy elimination. \nIn Proceedings of the 2th annual IEEE/ACM international symposium on Code generation and optimization, \npages 91 102, 2003. [4] C. S. Chekuri, A. V. Goldberg, D. R. Karger, M. S. Levine, and C. Stein. Experimental \nstudy of minimum cut algorithms. In Pro\u00adceedings of the Eighth Annual ACM-SIAM Symposium on Discrete \nAl\u00adgorithms (SODA), pages 324 333, 1997. [5] F.Chow,S.Chan,R.Kennedy,S.Liu,R.Lo,andP.Tu. Anew algorithm \nfor partial redundancy elimination based on ssa form. In Proceedings of the ACM SIGPLAN 97 Conference \non Programming Language Design and Implementation, pages 273 286, 1997. [6] R. Cytron, J. Ferrante, B. \nK. Rosen, M. N. Wegman, and F. K. Zadeck. Ef.ciently computing static single assignment form and the \ncontrol dependence graph. ACM Trans. Program. Lang. Syst., 13(4):451 490, 1991. ISSN 0164-0925. doi: \nhttp://doi.acm.org/10.1145/115372. 115320. [7] L. Ford and D. Fulkerson. Flows in Networks. Princeton \nUniversity Press, 1962. [8] git://github.org/path64/compiler.git. [9] R. Gupta, D. A. Berson, and J. \nZ. Fang. Path pro.le guided partial redundancy elimination using speculation, 1997. [10] R. N. Horspool \nand H. Ho. Partial redundancy elimination driven by a cost-bene.t analysis, 1997. [11] R. N. Horspool, \nD. J. Pereira, and B. Scholz. Fast pro.le-based partial redundancy elimination. In Proceedings of the \n7th Joint Modular Languages Conference, pages 362 376, September 2006. [12] http://www.spec.org/cpu2006. \n[13] K. Kennedy. Safety of code motion. International Journal of Com\u00adputer Mathematics, 3(2 and 3):117 \n130, 1972. [14] R. Kennedy, S. Chan, S. Liu, R. Lo, P. Tu, and F. Chow. Partial redundancy elimination \nin ssa form. ACM Trans. Program. Lang. Syst., 21(3):627 676, 1999. [15] J. Knoop, O. R\u00a8uthing, and B. \nSteffen. Lazy code motion. In Proceed\u00adings of the ACM SIGPLAN 92 Conference on Programming Language Design \nand Implementation, pages 224 234, 1992. [16] J. Knoop, O. R\u00a8uthing, and B. Steffen. Optimal code motion: \ntheory and practice. ACM Trans. Program. Lang. Syst., 16(4):1117 1155, 1994. [17] J. Lin, T. Chen, W. \nchung Hsu, and P. chung Yew. A compiler framework for speculative analysis and optimizations. In Proceedings \nof the ACM SIGPLAN 03 Conference on Programming Language Design and Implementation, pages 289 299, 2003. \n[18] R. Lo, F. Chow, R. Kennedy, S. Liu, and P. Tu. Register promotion by sparse partial redundancy elimination \nof loads and stores. In Proceedings of the ACM SIGPLAN 98 Conference on Programming Language Design and \nImplementation, pages 26 37, 1998. [19] E. Morel and C. Renvoise. Global optimization by suppression \nof partial redundancies. Communications of the ACM, 22(2):96 103, 1979. [20] B. Murphy, V. Menon, F. \nSchneider, T. Shpeisman, and A. Adl-Tabatabai. Fault-safe code motion for type-safe languages. In Pro\u00adceedings \nof the 6th annual IEEE/ACM international symposium on Code generation and optimization, pages 144 154, \n2008. [21] B. Scholz, N. Horspool, and J. Knoop. Optimizing for space and time usage with speculative \npartial redundancy elimination. In Proceed\u00adings of the 2004 ACM SIGPLAN/SIGBED Conference on Languages, \nCompilers, and Tools for Embedded Systems, pages 221 230, 2004. [22] J. Xue and Q. Cai. A lifetime optimal \nalgorithm for speculative pre. ACM Transactions on Architecture and Code Optimization, 3(2):115 155, \n2006.  \n\t\t\t", "proc_id": "1993498", "abstract": "<p>To derive maximum optimization benefits from partial redundancy elimination (PRE),it is necessary to go beyond its safety constraint. Algorithms for optimal speculative code motion have been developed based on the application of minimum cut to flow networks formed out of the control flow graph. These previous techniques did not take advantage of the SSA form, which is a popular program representation widely used in modern-day compilers. We have developed the MC-SSAPRE algorithm that enables an SSA-based compiler to take full advantage of SSA to perform optimal speculative code motion efficiently when an execution profile is available. Our work shows that it is possible to form flow networks out of SSA graphs, and the min-cut technique can be applied equally well on these flow networks to find the optimal code placement. We provide proofs of the correctness and computational and lifetime optimality of MC-SSAPRE. We analyze its time complexity to show its efficiency advantage. We have implemented MC-SSAPRE in the open-sourced Path64 compiler. Our experimental data based on the full SPEC CPU2006 Benchmark Suite show that MC-SSAPRE can further improve program performance over traditional SSAPRE, and that our sparse approach to the problem does result in smaller problem sizes.</p>", "authors": [{"name": "Hucheng Zhou", "author_profile_id": "81435599460", "affiliation": "Tsinghua University, Beijing, China", "person_id": "P2690495", "email_address": "zhou-hc07@mails.tsinghua.edu.cn", "orcid_id": ""}, {"name": "Wenguang Chen", "author_profile_id": "81100108264", "affiliation": "Tsinghua University, Beijing, China", "person_id": "P2690496", "email_address": "cwg@tsinghua.edu.cn", "orcid_id": ""}, {"name": "Fred Chow", "author_profile_id": "81100327963", "affiliation": "ICube Technology Corp., Beijing, China", "person_id": "P2690497", "email_address": "frdchow@gmail.com", "orcid_id": ""}], "doi_number": "10.1145/1993498.1993510", "year": "2011", "article_id": "1993510", "conference": "PLDI", "title": "An SSA-based algorithm for optimal speculative code motion under an execution profile", "url": "http://dl.acm.org/citation.cfm?id=1993510"}