{"article_publication_date": "06-04-2011", "fulltext": "\n Isolating and Understanding Concurrency Errors Using Reconstructed Execution Fragments Brandon Lucia \nBenjamin P. Wood Luis Ceze University of Washington, Department of Computer Science and Engineering {blucia0a,bpw,luisceze}@cs.washington.edu \nhttp://sampa.cs.washington.edu Abstract In this paper we propose Recon, a new general approach to concur\u00adrency \ndebugging. Recon goes beyond just detecting bugs, it also presents to the programmer short fragments \nof buggy execution schedules that illustrate how and why bugs happened. These frag\u00adments, called reconstructions, \nare inferred from inter-thread com\u00admunication surrounding the root cause of a bug and signi.cantly simplify \nthe process of understanding bugs. The key idea in Recon is to monitor executions and build graphs that \nencode inter-thread communication with enough context infor\u00admation to build reconstructions. Recon leverages \nreconstructions built from multiple application executions and uses machine learn\u00ading to identify which \nones illustrate the root cause of a bug. Recon s approach is general because it does not rely on heuristics \nspeci.c to any type of bug, application, or programming model. Therefore, it is able to deal with single-and \nmultiple-variable concurrency bugs regardless of their type (e.g., atomicity violation, ordering, etc). \nTo make graph collection ef.cient, Recon employs selective monitor\u00ading and allows metadata information \nto be imprecise without com\u00adpromising accuracy. With these optimizations, Recon s graph col\u00adlection imposes \noverheads typically between 5x and 20x for both C/C++ and Java programs, with overheads as low as 13% \nin our ex\u00adperiments. We evaluate Recon with buggy applications, and show it produces reconstructions \nthat include all code points involved in bugs causes, and presents them in an accurate order. We include \na case study of understanding and .xing a previously unresolved bug to showcase Recon s effectiveness. \nCategories and Subject Descriptors D.1.3 [Programming Tech\u00adniques]: Concurrent Programming; D.2.5 [Software \nEngineering]: Debugging Aids General Terms Algorithms, Reliability Keywords concurrency, statistical \ndebugging, multithreading 1. Introduction Concurrency bugs are a major stumbling block to writing multi\u00adthreaded \nprograms. Even expert programmers puzzle over compli\u00adcated behaviors resulting from the unexpected interaction \nof opera- Permission to make digital or hard copies of all or part of this work for personal or classroom \nuse is granted without fee provided that copies are not made or distributed for pro.t or commercial advantage \nand that copies bear this notice and the full citation on the .rst page. To copy otherwise, to republish, \nto post on servers or to redistribute to lists, requires prior speci.c permission and/or a fee. PLDI \n11, June 4 8, 2011, San Jose, California, USA. Copyright c &#38;#169; 2011 ACM 978-1-4503-0663-8/11/06. \n. . $10.00 tions in different threads. Developers face errors such as data races, atomicity violations, \ndeadlocks, and ordering errors. Concurrency bugs are particularly dif.cult to diagnose and .x for two \nmain reasons. First, developers must reason about the in\u00adteractions of many pieces of code executing \nin multiple threads. Observing one thread s buggy behavior is often insuf.cient for un\u00adderstanding the \ncause of the bug, which may lie in another thread. Second, non-determinism in multi-threaded execution \ncomplicates the process of interpreting buggy executions. The exact behavior of the application may vary, \neven from one buggy run to the next, making it very dif.cult to pin-point the root cause of the bug. \nPrior work [16] showed that the challenges of concurrency have led de\u00advelopers to give up on some bugs \nentirely, or to apply incorrect or incomplete stop-gap solutions. As a result, errors remain in the wild, \npotentially leading to surprising software failures. There is a large body of work addressing concurrency \nbugs. A signi.cant fraction of prior work focuses on dynamically detecting data races [6, 25], atomicity \nviolations [8, 17], and locking or shar\u00ading discipline violations [1, 26, 33]. Some recent work on testing \nin\u00advestigated new ways of systematically exploring executions [5, 21] and replaying buggy executions \n[24, 31]. Prior work has had con\u00adsiderable success, but there still remains much to be addressed. First, \nmany prior approaches to detect bugs report too little in\u00adformation to understand bugs: a single communication \nevent [18, 27, 32] or the thread preemptions from buggy runs [5, 21]. How\u00adever, concurrency bugs are \ncomplex and involve code points dis\u00adtributed over a code base and in multiple threads, requiring more \ninformation to be fully understood. To understand such bugs, de\u00advelopers bene.t from seeing a portion \nof the execution illustrat\u00ading the actual code interleaving that led to buggy behavior. Second, replay-based \napproaches [24, 31] often report too much information effectively, the entire execution schedule. Replay \nmakes bugs re\u00adproducible, but programmers must sift through an entire execution trace to comprehend bugs. \nFinally, many techniques are tailored to a speci.c class of concurrency errors [17, 23], limiting their \nappli\u00adcability. It is infeasible to anticipate every possible error scenario, and design a tool targeting \neach. Hence, generality is crucial. We propose Recon, a new approach to concurrency debugging based on \nreconstructions of buggy executions. Reconstructions are short, focused fragments of the interleaving \nschedule surrounding a program event, such as shared-memory communication. Figure 1 illustrates what \na reconstruction is and how a reconstruction relates to an execution. Observe that not all program events \nare included in the reconstruction. Instead, a reconstruction contains a concise summary of program behavior \nsurrounding an event that is likely to be related to a concurrency error. Reconstructions are based on \ncommunication graphs that encode information about the ordering of communication events. Based on this \nordering, reconstructions show the interleaving that caused buggy behavior, rather than just  Figure \n1. Recon reconstructs fragments of program execution. some of the code points involved. Reconstructions \nare general,as they make no assumptions about the nature of bugs i.e., Recon does not look for speci.c \npatterns. Figure 2 shows an overview of Recon s basic operation. The process begins when a programmer \nobserves a bug or receives a bug report. The programmer then derives a test case designed to trigger \nthe bug, and runs the test multiple times under Recon. Recon collects a communication graph from each \nexecution, and the programmer or test environment labels each graph as buggy or nonbuggy, depending on \nthe outcome of the test. Recon then builds reconstructions from edges in buggy graphs; for each one, \nRecon computes statistical features to quantify the likelihood that they are related to the bug. Recon \nuses the features to compute a rank for each reconstruction, and presents them to the user in rank order. \nWith Recon, we make several contributions: We propose the concept of reconstructing fragments of multi\u00adthreaded \nexecutions and develop an algorithm that builds recon\u00adstructions from communication graphs.  We propose \na set of features to describe reconstructions and use statistical techniques to identify reconstructions \nthat illustrate the root cause of bugs.  We develop optimization techniques to build communication graphs \nef.ciently.  We implement Recon for both C/C++ and Java. Our evaluation uses bugs from the literature, \nincluding several large applica\u00adtions, and shows that Recon precisely identi.es bugs and their corresponding \nreconstructions. We include a case study of using Recon to understand and .x an unresolved bug.  The \nrest of this paper is organized as follows. Section 2 dis\u00adcusses concurrency bugs in general and provides \nan overview of the role of communication graphs for debugging. Section 3 discusses our approach to reconstructing \nexecution fragments. Section 4 de\u00adscribes how we use machine learning techniques to identify bugs. Section \n5 describes our implementation of Recon for C/C++ and Java, and several enabling optimizations. Section \n6 describes our experiments, which show that Recon reconstructs buggy executions precisely and ef.ciently. \nSection 7 discusses related work and Sec\u00adtion 8 concludes. Figure 2. Overview of Recon s operation. \n2. Background 2.1 Concurrency Bugs Data races occur when two different threads access the same mem\u00adory \nlocation, at least one access is a write, and the accesses are not ordered by synchronization. Ordering \nviolations involve two or more memory accesses from multiple threads that happen in an unexpected order, \ndue to absent or incorrect synchronization. Atomicity violations result from a lack of constraints on \nthe interleaving of operations in a program. Figure 3(a) illustrates an atomicity violation bug that \nwas found in weblech, a multi\u00adthreaded web crawler. The program uses a shared queue that was implemented \nincorrectly. The check of the queue s size on line 168 should be atomic with the dequeue operation performed \non line 189 to ensure that there is always an item to dequeue, but the programmer has not implemented \nthis atomicity constraint. Figure 3(b) shows an execution trace manifesting the bug. In this trace, the \nsize() and dequeue() calls in Thread 2 inter\u00adleave between the size() and dequeue() calls in Thread 1. \nSince the queue is emptied by Thread 2, Thread 1 s call to dequeue() returns null, which is stored in \nthe local variable item. Thread 1 later crashes with a NullPointerException when trying to invoke the \ngetD() method on this null item.  2.2 Communication Graph Debugging Prior work [18, 27, 32] has observed \nthat communication graphs are useful for detecting concurrency errors. A communication graph is a representation \nof a program execution that captures inter\u00adthread data-.ow. A node in a communication graph represents \na memory instruction at some program point. An edge between two nodes, the source and sink, indicates \nthat the sink instruction read or overwrote data written by the source instruction. Furthermore, the \nsource and sink instructions executed in different threads. The main idea behind concurrency debugging \nwith communi\u00adcation graphs is that a problematic communication event charac\u00adterizes the error s behavior, \nand is represented by an edge in the graph. Identifying the problematic communication event illustrates \nthe bug s behavior to a developer, aiding in debugging. An ap\u00adproach used in prior work for identifying \nproblematic communi\u00adcation is to focus on communication events that tend to occur often in buggy program \nruns, and infrequently or never in correct pro\u00adgram runs [17, 32]. This technique is often useful, but \ninsuf.cient in general. To understand why, refer back to the example in Figure 3(b). The problematic \ncommunication event is Thread 1 s read of the queue s qsize .eld on line 115; it reads a value written \nby Thread 2 at line 117 rather than the same value Thread 1 read from qsize, at line 133. Looking at \nthis communication alone is insuf.cient to .nd the bug; the involved instructions communicate in both \nbuggy and nonbuggy executions. This dif.culty in identify\u00ading problematic communication graph edges was \nthe motivation for developing context-aware communication graphs in Bugaboo [18]. Context-Aware Communication \nGraphs. Bugaboo .rst demon\u00adstrated that communication graphs are insuf.cient for general con\u00adcurrency \nbug detection. Bugaboo addresses this inadequacy by adding communication context to communication graphs. \nCommu\u00adnication context is a short (e.g., 5 entries) history of context events that is maintained by each \nthread in an execution. Context events are Local Reads, Local Writes, Remote Reads, and Remote Writes. \nA thread records a Local event in its context when it executes a sharing read or write operation. A thread \nrecords a Remote event in its context when another thread reads or writes a memory loca\u00adtion that it \nhas accessed recently. Note that context events record only the type of a memory operation (e.g., Remote \nRead ), not the code or data involved.  class Queue { ... Queue(){ 46: items = ...; 47: qsize = 0; \n} synchronized dequeue(){ 115: if (qsize == 0) return null; 117: qsize--; 118: return items[...]; \n} synchronized size(){ 133: return qsize; } } Implicit assumption: q.size() != 0 class Spider { ... \npublic void run(){ 167: while (...) {  168: if (q.size() == 0) { 170: continue; } ... 189: item \n= q.dequeue();  ... 195: x = item.getD(); } } Bug: another thread dequeues last queue entry here; \nthis thread dequeues null; NullPtrException at line 195. (a) Program Should be atomic  Figure 3. Example \nshowing (a) a buggy program, (b) a bug-triggering execution trace, and (c) the context-aware communication \ngraph produced by the trace. Nodes represent the execution of operations in a speci.c context. Edges \nrepresent communication between nodes. Note that we only include events in nodes contexts that appear \nin our abbreviated trace for presentation purposes. Communication context is the basis for context-aware \ncommu\u00adnication graphs. In a context-aware graph a node is a pair (I,C) representing the execution of \na static instruction, I, in communica\u00adtion context, C, instead of just an instruction. Edges in a context\u00adaware \ngraph represent communication between dynamic instruc\u00adtion instances. In a context-aware graph, there \nmay be more than one node per static instruction, but the size of the graph is bounded by a function \nof the context size, as described in prior work [18]. Figure 3(c) shows a context-aware communication \ngraph pro\u00adduced by the execution trace in Figure 3(b). The numbered cir\u00adcles map events in the trace \nto their corresponding edge in the graph. In the context-aware graph, the sink node of edge 4 occurs \nonly in buggy executions. The most recent (left-most) two events in this node s context are the Remote \nWrite that corresponds to Thread 2 s write at line 117, and the Remote Read that corresponds to Thread \n2 s read at line 133. This context re.ects the buggy in\u00adterleaving of Thread 1 s read of qsize at line \n133, and its read of qsize at line 115. Hence, edge 4, which includes this unique context-aware node, \noccurs in buggy executions graphs, and not correct executions graphs. Context-aware graphs provide a \nway to identify buggy communication in such complex bugs. Debugging with communication graphs is one \nof the key mo\u00adtivating ideas behind Recon. However, rather than just isolating graph edges likely to \nrepresent problematic communication, as most prior work has done, Recon reconstructs temporal sequences \nof communication events and, using machine learning, infers which sequences most likely illustrate a \nbug s cause. 3. Reconstructing Execution Fragments The goal of Recon is to simplify debugging by presenting \nthe pro\u00adgrammer with a short, focused, temporally ordered reconstruction of the events that were responsible \nfor buggy behavior. Our tech\u00adnique for reconstructing execution fragments is based on a special\u00adized \nversion of context-aware communication graphs [18]. 3.1 Timestamped Communication Graphs We specialize \nthe context-aware communication graph abstraction to encode ordering between non-communicating nodes. \nWe do so by adding a timestamp to each node, indicating when the node s instruction executed in the node \ns context. We call this new graph abstraction the timestamped communication graph; referring to them \nas just graphs hereafter. To summarize the structure of the graphs used by Recon: A node is a pair (I,C) \nrepresenting the execution of static in\u00adstruction I in communication context C. Each node is labeled \nwith a timestamp, T , representing the global time when instruc\u00adtion I last executed in context C.  \nAn edge is a pair of nodes (u, v) representing communication from the instruction instance represented \nby u to the instruction instance represented by v.  Graph Construction. We collect graphs by keeping \na last-writer record for each memory location, storing: (1) the thread that last wrote the location; \n(2) the instruction address and context of that write; and (3) a timestamp for the access. When a memory \nlocation is accessed by a different thread than the thread that last wrote the location, an edge is added \nto the graph. The edge s source node is populated with the instruction address, context, and timestamp \nfrom the location s last-writer record. The edge s sink node is populated with the instruction address, \ncontext, and timestamp of the operation being performed. To limit the size of the graph, we record only \nthe most recent pair of timestamps for an edge, i.e., the timestamp is not used to identify a node, only \nthe instruction and context are. When adding a communication edge to the graph, if the edge already exists, \nonly the timestamps are updated. By overwriting timestamps, we lose some ordering information, so we \ncall our extension a lossy times\u00adtamp. Figure 4(a) shows an example of a timestamped communica\u00ad  Figure \n4. A timestamped communication graph (a) and reconstruction (b). The graph corresponds to Figure 3(c). \ntion graph. The graph is similar to the one in Figure 3, except that each node now has a timestamp indicating \nwhen it last occurred.  3.2 Reconstructions A reconstruction is a schedule of communicating memory opera\u00adtions \nthat occurred during a short fragment of an execution. In this section, we describe the process of building \na reconstruction around a single, arbitrary communication event (i.e., graph edge). Section 4 describes \nhow we identify the reconstructions most likely related to bugs, and Section 5.3 details the entire debugging \nprocess. 3.2.1 Building Reconstructions from Graphs Recon builds reconstructions starting from an edge \nin a graph. In addition to the instructions represented by the source and sink nodes of the edge, a reconstruction \nshould include the memory operations that executed in a short window prior to the source node, in the \ntime between the source and the sink nodes, and in a short window following the sink. These regions of \nthe execution are called the pre.x, body, and suf.x of the reconstruction, respectively, and are selected \nfrom the graph for a single execution according to nodes timestamps. The size of the window of nodes \nconsidered in computing the pre.x and suf.x is arbitrary. With a larger window, there is a greater chance \nthat unrelated nodes are included in a reconstruction. With a smaller window, fewer unrelated nodes are \nlikely to end up in a reconstruction, but we risk excluding events related to the bug that occur far \naway from the communication event. A reasonable window size heuristic is to use the length of the communication \ncontext of a node. Using the context length, we include nodes that were in.uenced by, or in.uenced the \ncontext of the sink or source. Formal De.nition of Reconstruction A reconstruction is a tu\u00adple (e, p, \nb, s). The reconstruction is built around an edge, e, with source node, esrc and sink node, esink. The \npre.x, p, is a set of consecutive nodes immediately preceding esrc in timestamp order. The body, b, is \nthe set of all nodes between esrc and esink in times\u00adtamp order. The suf.x s is a set of consecutive \nnodes immediately following esink in timestamp order. The cardinalities of the pre.x and suf.x are bounded \nby .xed constants. The cardinality of the body is bounded by a threshold function described in Section \n5.3. 3.2.2 Simpler Debugging Using Reconstructions Figure 4(b) shows the reconstruction Recon produces \nfrom one of the edges in the graph in Figure 4(a). This reconstruction illustrates the buggy interleaving \nof queue operations shown in Figure 3(b). It includes all the code points involved in the bug, and presents \nthem in the order that leads to buggy behavior. In buggy runs, the read of the queue s size on line 133 \nand the dequeue on line 115 are interleaved by the dequeuing decrement of qsize at line 117. This buggy \ninterleaving is clear in the reconstruction: line 133 s node is in the body and line 115 s node is in \nthe suf.x. The interleaving dequeue operation at line 117 is the sink node, ordered between the body \nand the suf.x. This example illustrates a key contribution of reconstructions. Looking at the buggy edge \nbetween line 117 s node and line 115 s node does not explicitly indicate the bug it suggests the in\u00advolvement \nof the queue, but not the atomicity violation involving line 133. Instead, the reconstruction built around \nthe nonbuggy edge from line 47 s node and line 117 s node illustrates the bug, showing all three involved \ncode points and the buggy execution order.  3.3 Aggregate Reconstructions We have described how to \nbuild a reconstruction for a single edge from a single execution. We can aggregate reconstructions from \na set of runs to see how frequently code points occur in a region of a reconstruction across executions. \nThis information allows us to de.ne our con.dence that a code point belongs in a region. Starting from \na set of graphs, we compute a reconstruction for each edge in each execution s graph individually. We \nthen aggregate the reconstructions of each edge across the executions by computing the union of each \nof their pre.xes, the union of each of their bodies, and the union of each of their suf.xes, producing \nthe aggregate pre.x, body, and suf.x. A node may occur in multiple different regions in an aggregate \nreconstruction, if, for instance, in half of executions it appeared in the pre.x, and in half it appeared \nin the body. Nodes in the same region in an aggregate reconstruction are unordered with one another, \nbut are ordered with the source and the sink of the edge in the reconstruction, and with nodes in other \nregions. Nodes within a region are unordered because timestamps are not comparable across executions. \nWhen aggregating reconstructions, we associate a con.dence value with each node in a region. The con.dence \nvalue is equal to the fraction of executions in which that node appeared in that region. The con.dence \nvalue of a node in a region represents the likelihood that a node occurs in that region. In Section 4, \nwe discuss using con.dence values to identify reconstructions likely related to buggy behavior. Figure \n5 shows an example of the aggregation process. Part (a) shows reconstructions produced from 4 different \nexecutions, and  describes these features in detail and veri.es their ef.cacy empiri- Run 1 Run 2 Run \n3 Run 4 cally using a feature importance metric from machine learning [14]. 4.1.1 Buggy Frequency Ratio \n(B) Pre.x Body Suf.x Intuition. A reconstruction s Buggy Frequency Ratio, or B value, describes the \ncorrelation between the frequency of the com\u00ad munication event from which the reconstruction was built, \nand the occurrence of buggy behavior. The motivation for this feature is that we are interested in events \nin an execution that occur often in buggy program runs, but rarely, or never, in nonbuggy runs. De.nition. \nFor each aggregate reconstruction, assume #Runsb buggy runs and #Runsn nonbuggy runs. Assume the reconstruc\u00ad \n(a)Reconstructions From Multiple Runs Figure 5. (a) Reconstructions of many runs and (b) the resulting \naggregate reconstruction with con.dence values. part (b) shows the aggregate reconstruction produced \nfrom these 4 reconstructions. In this example, node A appears in the pre.x of half of the reconstructions, \nand appears in the body in half of the reconstructions. The pre.x and body of the aggregate reconstruc\u00adtion \ntherefore both include node A, and assign it a con.dence value of 50%. Node C appears in the body of \nall of the individual recon\u00adstructions, so it appears in the body of the aggregate reconstruction with \na con.dence value of 100%. 4. Debugging with Reconstructions There are four steps to debugging a program \nwith Recon: 1. The program is run under Recon several times, yielding a set of timestamped communication \ngraphs labeled buggy or non\u00adbuggy (Section 2.1). 2. Next, Recon must decide which edges are worth using \nas the ba\u00adsis for a reconstruction. Recon selects edges from each buggy graph based on how strongly correlated \nthey are with the oc\u00adcurrence of buggy behavior. Section 4.1.1 describes this corre\u00adlation. 3. For each \nselected edge, Recon builds and aggregates recon\u00adstructions (Sections 3.2 and 3.3). 4. Finally, Recon \nranks the reconstructions by how likely they are to illustrate the bug, as determined by a set of features \ncomputed from the aggregated reconstructions and the nonbuggy graphs.  We now discuss the features we \ndeveloped, and how we use them to produce a reconstruction s rank. 4.1 Features of Reconstructions A \nkey design concern is that features are general. A feature that targets one bug type or pattern is not \nas useful. If we choose features that are not general, we bias our search toward some bugs and miss others \nentirely. For example, serializability analysis of memory access interleavings has been used to detect \natomicity violations [17, 23]. However, it does not detect ordering bugs or any multi-variable bugs. \nOur features should capture as much information as necessary to discriminate between reconstructions \nof buggy fragments of an execution and reconstructions of nonbuggy fragments. We use three features: \nBuggy Frequency Ratio focuses on the correlation be\u00adtween communication events and buggy behavior; Context \nVari\u00adation Ratio focuses on variations in communication context that correlate with buggy behavior; and \nReconstruction Consistency fo\u00adcuses on the consistency with which sequences of code points occur in reconstructions \nfrom buggy executions. The rest of this section tion s edge occurred in EdgeF reqb buggy runs and in \nEdgeF reqn nonbuggy runs. The fraction of buggy runs in which the edge oc\u00adcurred is: EdgeF reqb Fracb \n= #Runsb The fraction of nonbuggy runs in which the edge occurred is: EdgeF reqn Fracn = #Runsn We de.ne \nB as follows: Fracb B = Fracn If a reconstruction s edge occurs in many buggy runs and few nonbuggy runs, \nB is large. Conversely, if the edge occurs often in nonbuggy runs and rarely in buggy runs, B is small. \nIf the edge never occurs in a nonbuggy run, but occurs in buggy runs, then it is very likely related \nto the bug. However, in such a case, Fracn is 0, and unless we handle this case specially, B is unde.ned. \nIn this corner case, we give Fracn a value that is smaller than the value produced if the edge occurs \nin one nonbuggy run (by assigning Fracn = 1 ). This yields large B #Runsn+1 values for these important \nedges.  4.1.2 Context Variation Ratio (C) Intuition. The Context Variation Ratio (C) quanti.es how \nvari\u00adation of contexts of communicating code points correlates with buggy execution. We can determine \nthe pair of communicating code points in the edge around which a reconstruction is built, since a node \nis identi.ed by an instruction and context. From that, we can determine all edges involving that pair \nof code points, regardless of context; we then compute the set of all contexts in which the pair communicated. \nIn a program that has frequent, varied communica\u00adtion, there are many contexts in this set; in a program \nwith little or less-varied communication, the set is small. We consider a re\u00adconstruction suspicious \nif the pair of code points forming the edge around which the reconstruction was built execute in a substantially \ndifferent number of contexts in buggy runs than in correct runs. De.nition. For a reconstruction built \naround an edge between two code points, we de.ne #Ctxb to be the number of contexts in which the code \npoints communicated in buggy runs, and #Ctxn to be the number in nonbuggy runs. C is the ratio of the \nabsolute difference of #Ctxb and #Ctxn to the total number of contexts for the pair of code points in \nnonbuggy and buggy runs. We de.ne C as follows: |#Ctxb - #Ctxn| C = #Ctxb +#Ctxn Large C values indicate \na disparity in communication behavior between buggy and nonbuggy runs. Hence, a reconstruction with a \nlarge C value more likely illustrates the communication pattern that led to buggy behavior.  4.1.3 \nReconstruction Consistency (R) ReliefF Rank Program BRC Intuition. Reconstruction Consistency (R) is \nthe average con.\u00ad dence value over all code points in an aggregate reconstruction. R is useful because \ncode points that consistently occur in recon\u00adstructions of buggy executions are likely related to the \ncause of the bug. As described in Section 3.2, each node in an aggregate re\u00adconstruction has an associated \ncon.dence value that represents the frequency with which it occurs at a certain point in that reconstruc\u00adtion. \nIn an aggregate reconstruction produced from a set of buggy runs, a node with a high con.dence value \noccurs consistently in apache 0.99 0.91 0.16 mysql 0.20 0.59 0.76 pbzip2 0.26 0.28 0.28 aget 0.84 0.91 \n0.16 Table 1. ReliefF rank of features for C/C++ programs. apart. As our results in Section 6.2 con.rm, \nranking using all three the same region of reconstructions from those buggy runs. Such nodes operations \nare therefore likely to be related to the buggy features isolates the reconstruction of the bug in pbzip2. \nbehavior in those runs. Reconstructions containing many high con-Our ReliefF feature analysis emphasizes \ntwo properties of our .dence nodes re.ect a correlation between the co-occurrence of technique: (1) our \nfeatures precisely classify buggy reconstructions those nodes code points in the order shown by the reconstruction, \nto identify bugs; and (2) considered together, our features are more and the occurrence of buggy behavior. \npowerful than each individually. De.nition. We compute R for a reconstruction as the average con.dence \nvalue over all its nodes. Formally, for a reconstruction with pre.x region P , body B, and suf.x S and \nwhere V (n, r) is the con.dence value of node n in region r, we de.ne R as follows: 5. Implementation \nWe implemented two versions of Recon: one for C/C++, using Pin [19], and one for Java, using RoadRunner \n[7]. The implemen\u00adtation has three parts: (1) tracking communication; (2) collecting graphs; and (3) \ngenerating and ranking reconstructions. P V (p, P )+ P V (b, B)+ P p.Pb.Bs.S R = V (s, S) 5.1 Tracking \nCommunication |P | + |B| + |S| Nodes in a reconstruction with a large R value tend to occur in the reconstructed \norder when buggy behavior occurs. Such recon\u00adstructions are therefore more likely to represent problematic \ninter\u00adleavings, and to be useful for debugging. 4.2 Using Features to Find Bugs By construction, large \nvalues for B, C,or R indicate that a recon\u00adstruction is likely to be buggy. Therefore, we give each reconstruc\u00adtion \na score equal to the product of all non-zero features values. We rank reconstructions, with highest scoring \nreconstruction .rst. Empirical validation of B, C, and R. We now quantitatively jus\u00adtify our features \nusing real buggy code (we describe our experimen\u00adtal setup in Section 6). We assessed the discriminatory \npower of our features using Weka s [9] ReliefF [14] feature selection function. The magnitude of a feature \ns ReliefF value is greater if the dis\u00adtance between points of different classes (e.g.buggy or non-buggy) \nis greater along that feature s dimension, on average. The magni\u00adtude of a feature s ReliefF value corresponds \nto how well it dis\u00adcriminates between classes. Table 1 shows ReliefF values for bugs in several C/C++ \nappli\u00adcations. All features ReliefF values are non-zero, meaning they are useful for classi.cation, and \nmany have ReliefF values close to 1.0. The relative importance of features varies by program. For apache, \nB and R are the most useful. C is less important, indicating there is a similar amount of context variation \nin buggy and nonbuggy runs. Figure 6 illustrates the relative importance of the features graphically, \nwith pair-wise feature plots. Figure 6(a) shows that when viewed along the axes of highest ReliefF, there \nis clear segregation of buggy and nonbuggy reconstructions. In the plot, buggy points tend to the upper \nright, meaning they have larger feature values than nonbuggy points. Figures 6(b) and (c) reiterate the \nclass segregation along the B and R axes, and illustrate the less clear division along the C axis. pbzip2 \ns ReliefF values are smaller than other applications values. The disparity indicates that in each dimension, \npbzip2 s buggy and nonbuggy points tend to be nearer to one another than in other applications. Hence, \nranking by a single feature is inade\u00adquate to isolate bugs precisely. However, in the three-dimensional \nspace of all features, buggy and nonbuggy reconstructions are far To track communication we maintain \na metadata table. This table maps each memory location to an entry containing its last-writer record, \nand a list of threads that have read from the location since its last write that we call the sharers \nlist. Each thread has a commu\u00adnication context. A thread s context is a shift register of events, as \ndescribed in Section 3. We use a 5-entry context. When a thread writes to a memory location, it updates \nthe loca\u00adtion s last-writer record with its thread ID, the instruction address of the write, its current \ncontext, and the current timestamp. If the writing thread is different from the last writer, it does \nthree things: (1) update its context with a local write event; (2) update the con\u00adtext of each thread \nin the sharers list with a remote write event; and (3) clear the sharers list.  When a thread reads \na location, it looks up the last writer thread in the last-writer record. If the reading thread is different \nfrom the last writer, it does three things: (1) update its context with a local read; (2) update the \nlast writer thread s context with a remote read; and (3) add the reading thread to the memory location \ns sharers list. For C/C++, we implement the metadata table as a .xed size hash table of 32 million entries. \nTo .nd a memory location s metadata, we index with the address modulo the table size. We use a lossy \ncollision resolution policy: on a hash collision, an access may read or overwrite another colliding location \ns metadata. We ignore stack accesses, as they are rarely involved in communication. For Java, we use \nRoadRunner s shadow memory to implement a distributed metadata table. Its size scales with allocated \nmemory and it does not suffer from collisions. Unique identi.ers of memory access instructions in the \nbytecode replace instruction addresses. Contexts are stored as integers, using bit .elds. We instrument \naccesses to .elds and arrays, but not local variables. 5.2 Timestamped Communication Graphs Each thread \nmaintains its own partial communication graph. Par\u00adtitioning the communication graph over all threads \nmakes adding an edge a thread-local operation, which is critical for performance. When a thread tries \nto add an edge, it .rst searches the graph for the edge. If the edge is already in the graph, the thread \noverwrites the existing timestamps with the timestamps of the edge being added. If not, a new edge is \ncreated. When a thread ends, it merges its par\u00adtial graph into a global graph. Once all partial graphs \nare merged into the global graph, it is written to a .le.  1 1 Nonbuggy Buggy 0.8 0.8 0.6 0.6 Reconstruction \nConsistency Context Variation Ratio Context Variation Ratio 0.4 0.2 0.4 0.2 0 0 0 0 0.2 0.4 0.6 0.8 \n1 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 Buggy Run Frequency Ratio Buggy Run Frequency Ratio Reconstruction \nConsistency (a) B vs. R (b) B vs. C (c) R vs. C Figure 6. Pair-wise plots of features for apache showing \nthe top 2000 ranked reconstructions. Buggy reconstructions points are circled. For C/C++, we use the \nRDTSC x86 instruction to track times\u00adtamps. We maintain communication graphs as a chaining hash ta\u00adble. \nSeparately for the source and sink node, the hash function sums the entries in each node s context. Each \nnode s sum is then XORed with the node s instruction address. The result of the computation for the source \nnode is then XORed with the result of the compu\u00adtation for the sink, producing the hash key. For Java, \nwe generate timestamps from the system time and implement communication graphs as adjacency lists.  \n5.3 Generating and Ranking Reconstructions We generate and rank reconstructions with the following process. \nWe separately load sets of buggy and nonbuggy graphs into mem\u00adory and create a list of nodes ordered \nby timestamp for each buggy run. At this point, we compute C and B for each edge in the set of buggy \ngraphs. We then rank these edges by their B values. Next, we generate reconstructions for the top 2000 \nedges ranked by B, using the algorithm described in Section 3.2. To limit the size of the reconstructions \nproduced, we limit the number of code points in each region. To do so, we threshold by con.dence value, \nexclud\u00ading from a reconstruction any node that has a con.dence value less than half the region s maximum \ncon.dence value. After computing reconstructions, we compute their R values and their ranks, and output \nthem in rank order.  5.4 Optimizing Graph Collection We use two optimizations to reduce overheads: (1) \nwe reduce the number of instructions for which analysis is required and (2) we permit an instrumentation \ndata race to avoid locking overheads. 5.4.1 Selectively Tracking Memory Operations The simplest way of \nreducing graph collection overhead is mon\u00aditoring fewer memory operations. We develop two optimizations \nto do so. They can lead to lost or spurious edges, but our results (Section 6) show that Recon s accuracy \nis unaffected. First Read Only. Repeated reads to a memory location by the same thread are likely redundant. \nWe therefore develop the .rst\u00adread optimization: threads perform analysis only on their .rst read to \neach location after a remote write to that location. Skipping updates on subsequent reads is analogous \nto performing analysis only on cache read misses. Due to the frequency and temporal locality of reads, \nthis optimization eliminates many updates. This optimization is lossy. If a thread repeatedly reads the \nresult of a write, only its .rst read is reported. If subsequent reads are per\u00adformed by different code \nor in different contexts they will not cause edges to be added to the graph. Additionally, context events \ncorre\u00adsponding to ignored reads are not published to threads contexts, which may result in fewer distinct \ncontexts and edges. First Write Only. Repeated writes by the same thread are often redundant or non-communicating. \nUnder the .rst-write optimiza\u00adtion, a thread only updates the last-writer table and sharers list on a \nwrite to a memory location x when it is not the last thread to write x. This optimization is noisy. If \na thread that is not the last writer of x writes to x and does not update x s metadata on subsequent \nwrites, another thread s read of x may see outdated metadata and add a spurious edge with incorrect context \ninformation to the graph.  5.4.2 Intentional Instrumentation Races On every memory access, threads check \nthe last writer of the location they are accessing to determine what analysis operations must be performed, \nas described in Section 5.1. To ensure threads observe consistent metadata, they acquire a lock on each \naccess. We observe, however, that due to temporal locality these checks are often performed by the location \ns last writer. In such situations, reading all metadata is unnecessary. The cost of acquiring the lock \njust to check the last writer outweighs the cost of the check itself. To mitigate this cost, we can perform \nthe check without holding the lock, which we call the racy-lookups optimization. If, based on the check, \na thread determines it must perform further analysis or update the metadata, it acquires the lock. Only \nthe check to determine the location s last writer races. In principle, data races can lead to unde.ned \nbehavior [4] or memory inconsistency [20]. In practice, there are only two incon\u00adsistent outcomes of \nthis optimization. The .rst is that the last writer performs a check that indicates it is not the last \nwriter. In this case, the checking thread last wrote the metadata. On x86 the thread will correctly read \nits own write, making this situation impossi\u00adble. In Java, our metadata writes are well ordered, and \nthe read involved in the check is ordered with its metadata update. As a re\u00adsult, the checking thread \ncan only ever correctly read that it was the last writer. The other inconsistency is when a check indicates \nto a thread that it is the last writer when it is not. This situation is pos\u00adsible in x86 and Java. On \nx86, the check reports that the checking thread is the last writer, so it does no analysis. Because the \ncheck was not synchronized, however, another thread s update to the last\u00adwriter .eld may have been performed, \nbut not yet made visible to all threads. In this case, the checking thread should have seen the update, \nand added an edge, but did not. This situation can also arise because our instrumentation is not atomic \nwith program accesses. In practice, it has little impact on our analysis. Furthermore, the statistical \nnature of Recon is robust to noise, so such omissions do not impact Recon s bug detection capability. \n Category Program Version Bug Type C/C++ Bug Kernel logandswp circlist textre.ow jsstrlen n/a n/a Mozilla \n0.9 Mozilla 0.9 Atomicity Violation Atomicity Violation Multi-Variable Atomicity Violation Multi-Variable \nAtomicity Violation Full App. apache mysql pbzip2 aget httpd 2.0.48 mysqld 4.0.12 pbzip2 0.9.1 aget 0.4 \nAtomicity Violation Atomicity Violation Ordering Violation Multi-Variable Atomicity Violation Java Bug \nKernel stringbuffer vector JDK 1.6 JDK 1.4 Multi-Variable Atomicity Violation Multi-Variable Atomicity \nViolation Full App. weblech weblech 0.0.3 Atomicity Violation Table 2. The buggy programs we used to \nevaluate Recon 6. Evaluation There are several components to our evaluation. We show that our ranking \ntechnique is effective at .nding bugs and that the recon\u00adstructions Recon produces are useful and precise. \nWe show that Recon requires few program runs in order to be effective. We de\u00adscribe a case study of our \nexperience .xing a previously unresolved bug. Finally, we show that with our optimizations Recon s over\u00adheads \nare similar to other analysis tools, and overall data collection time is short. 6.1 Experimental Setup \nWe evaluated Recon s ability to detect concurrency bugs us\u00ading the buggy programs described in Table \n2. We used a set of full applications, as well as several bug kernels. Our bug ker\u00adnels are shorter programs \nwith bugs extracted from the litera\u00adture (stringbuffer, vector, circlist, logandswp), and buggy sections \nof code extracted from full versions of the Mozilla project (textreflow, jsstrlen). Our benchmarks encompass \nmany bug types observed in the wild [16] including ordering bugs and single-and multiple-variable atomicity \nbugs. We ran each ap\u00adplication in Recon with all optimizations. Our test script used ex\u00adternal symptoms \nsuch as crashes or corrupt output to label graphs. We evaluated Recon s runtime and memory overhead, \ncom\u00adpared to uninstrumented execution. We used the PARSEC bench\u00admark suite [2] with its simlarge input \nfor our C/C++ implemen\u00adtation, and for Java we used 6 applications from the DaCapo bench\u00admark suite [3], \nwith default inputs, and all the Java Grande bench\u00admarks [28], with size A inputs. We ran PARSEC and \nJava Grande with 8 threads; we let the DaCapo benchmarks self-con.gure based on the number of processors \nand did not instrument the DaCapo harness. We also ran 4 additional full applications, each with 8 threads: \nmysql, a database server, tested using the sysbench OLTP benchmark with the default table size (10,000) \nfor the performance measurements and table size 100 for debugging; apache, a web server, tested using \nApacheBench; aget, a download accelerator, tested fetching a large web .le; and pbzip2, a compression \ntool, tested compressing a 100MB text .le. For performance measure\u00adments, we ran the uninstrumented version \nand Recon, with the .rst\u00adread, .rst-write, and racy-lookups optimizations. We also ran three less-optimized \ncon.gurations to understand the impact of each op\u00adtimization: Base analyzes all memory accesses; FR uses \njust the .rst-read optimization; FR/W adds the .rst-write optimiza\u00adtion. We ran all experiments on an \n8-core 2.8GHz Intel Xeon with 16GB of memory and Linux 2.6.24. The Java tool used the Open-JDK 64-bit \nServer VM 1.6.0 with a 16GB max heap. We report results averaged over 10 runs of each experiment.  6.2 \nHow Effectively Does Recon Find Bugs? We produced reconstructions using graphs collected from 25 buggy \nruns and 25 non-buggy runs. We ranked the reconstructions as de\u00adscribed in Section 4.2. We examined the \nhighest-ranked reconstruc\u00adtion that illustrated the bug and analyzed the key properties of that reconstruction. \nTable 3 summarizes our .ndings. False Positives. The most important result in Table 3 is that for all \napplications, the top-ranked reconstruction revealed the bug, as shown in Column 2. This result demonstrates \nthat our ranking technique effectively directs programmer attention to buggy code with no distracting \nfalse positives. This result also corroborates the results from Section 4.2, showing that our features \nprecisely isolate buggy reconstructions. Unrelated Code in Reconstructions. Columns 3 and 4 in Table \n3 show the number of relevant and irrelevant code points that were included in the bug s reconstruction. \nWe consider a code point related if it performs a memory access that reads or writes a corrupt or inconsistent \nvalue, or if it is control-or data-dependent on the buggy code. In most cases, virtually all code in \nthe reconstruction is relevant to the bug. However, some reconstructions include code points unrelated \nto their bug. In aget, the two irrelevant code points are in straight-line code sequences with related \ncode points, at a distance of less than .ve lines. Such nearby but irrelevant code is not likely to confuse \na programmer. In mysql s case, .ve out of seven unrelated code points are in straight-line sequences \nwith relevant code. The remaining two in mysql s reconstruction, and all .ve in apache s reconstruction, \nwere not in straight-line code with relevant points. Instead, they were in another function that was \nthe caller or a callee of a func\u00adtion containing relevant code. Developers debugging programs are likely \nto understand such caller-callee relationships, suggesting that these code points will not be too problematic. \nweblech had several irrelevant code points in its reconstruc\u00adtion (28). The reason for their inclusion \nis that the bug usually oc\u00adcurs at the start of the execution. At this point, constructors have only \njust initialized data at a variety of code points in the program, resulting in many edges being added \nbetween initialization code and other code. The initialization code is easy to identify, especially with \nprogram knowledge. These code points clutter the reconstruc\u00adtion, but the bug is reported accurately. \nReconstruction Order Accuracy. Column 5 shows whether or not the code points in the reconstruction were \nshown in the order leading to buggy behavior. Code points appear in an order that leads to buggy behavior \nin all cases. In logandswp, the last code point in the buggy execution order appears in both the pre.x \nand suf.x of the reconstruction, because the code point is in a loop, however, the buggy interleaving \nis clear.  Sensitivity Collect Rank # Code Pts. In Code Pts To # Buggy Time Program of Bug Rel. Irr. \nOrder? Missing w/ 5 w/ 15 (h:m:s) logandswp circlist textre.ow jsstrlen 1 1 1 1 6 3 8 7 1 3 0 0 Yes \nYes Yes Yes 0 0 0 0 1 1 1 1 1 1 1 1 apache mysql pbzip2 aget 1 1 1 1 5 8 11 4 5 7 0 2 Yes Yes Yes Yes \n0 0 1 0 1 34 2 8 1 9 1 1 0:27:32 0:07:08 1:51:56 0:59:41 stringbuffer vector 1 1 6 6 0 0 Yes Yes 0 0 \n1 1 1 1 weblech 1 6 28 Yes 0 4 1 0:13:36 Table 3. Properties of reconstructions. Column 1 is the rank \nof the bug s reconstruction. Columns 2 and 3 show the number of relevant and irrelevant code points in \nthe bug s reconstruction. Column 4 shows whether the bug s reconstruction was in order. Column 5 shows \nthe number of relevant code points missing from the reconstruction. Columns 6 and 7 show the rank of \nthe bug s reconstructions using only 5 and 15 buggy graphs. Column 8 shows the graph collection time. \nMissing Code Points. Column 6 shows the number of code points directly involved in the bug that were \nomitted from the reconstruc\u00adtion. Only one case lacked any involved code points: the code points in pbzip2 \ns reconstruction all relate to establishing the corrupted state condition required for a crash to occur. \nThe actual crashing access is not included. Sensitivity to Number of Buggy Runs. Columns 7 and 8 illustrate \nRecon s sensitivity to the number of buggy runs used. Column 7 shows the rank of the bug s reconstruction \nusing 25 nonbuggy runs and 5 buggy runs. Column 8 shows the rank using 25 nonbuggy and 15 buggy runs. \nEven with very few buggy runs, Recon gives a high rank to reconstructions of the bug. Using fewer buggy \nruns does not impact precision substantially, except for mysql. Excluding mysql, Recon ranked the bug \ns reconstruction 8th or better with just 5 buggy runs, and .rst with 15 buggy runs. For mysql, using \nfewer runs caused Recon to rank some nonbuggy reconstructions above the bug s 33 with 5 buggy runs, \nand 8 with 15 buggy runs. As shown in Column 2, Recon always ranked the bug s reconstruction .rst with \n25 buggy runs. These results show that with very few buggy runs, Recon can .nd bugs with high precision. \nIn cases where a small number of buggy runs is insuf.cient, adding more runs increases Recon s precision. \nGraph Collection Time. Column 9 shows that the total time re\u00adquired to collect 25 buggy and 25 nonbuggy \ngraphs is not pro\u00adhibitively long. In our experiments, all applications took under two hours; apache, \nmysql, and weblech all took under 30 min\u00adutes. These data show that Recon is not only effective at detecting \nreal bugs in these full applications, but also reasonably fast. In Sec\u00adtion 6.4 we characterize the overheads \nour technique imposes over uninstrumented execution.  6.3 Case Study: Debugging an Unresolved Bug The \nweblech bug is open and unresolved in the program s bug repository. While the bug has been discussed \npreviously [12], we were unaware of any details of the bug prior to this case study. We used Recon to \n.nd the problem, and we were able to write a .x using Recon s output and our limited program knowledge. \nWe began with a bug report describing intermittent non-termi\u00adnation. Using the input from the report, \nwe were able to reproduce the bug in about 1 in 15 runs. We then ran the application repeat\u00adedly and \nwatched the output to identify the hang. We noticed that, consistently, at least one thread crashed on \na null pointer derefer\u00adence during hanging runs. We collected 25 buggy and 25 nonbuggy runs, identifying \nbugginess by watching for unhandled exceptions. We then produced reconstructions from these runs. The \n.rst reconstruction reported was related mostly to object constructors, but also included evidence of \nseveral accesses to a shared queue data structure, as well as a suspicious while loop termination condition \ninvolving the queue s size. The body of the reconstruction contained the initialization of and accesses \nto the size of the queue. The sink of the reconstruction s edge was an access to the queue data structure \nin the dequeue method. In the suf.x of the reconstruction was another call to the queue s dequeue method. \nAs we described in Section 2.1, such an interleaving of a dequeue call between an access to the queue \ns size and a subsequent dequeue call violates the atomicity of the pair of operations. The atomicity \nviolation leads to a thread crashing early due to the NullPointerException we observed. Crashing prevents \nthe thread from correctly updating the variable for the while loop to read. The crash is therefore also \nresponsible for the program s non\u00adtermination, as described in the bug report. We .xed the bug by extending \na synchronized block including the queue size check and the dequeue. With our .x, we didn t see the buggy \nbehavior in several hundred runs we conclude that we .xed the bug based on the information provided \nby Recon.  6.4 Performance In Table 4, we report runtimes relative to uninstrumented execu\u00adtion for \nRecon and the three less-optimized con.gurations. In the best case, Recon imposes slowdowns as low as \n34% for C/C++ (pbzip2) and 13% for Java (weblech). Slowdown for full applications never exceeds 24x, \neven during an industrial strength test of a commercial database (mysql). For PARSEC, we saw slowdowns \nranging from 5.5x to 28x, showing that Recon performs well on applications with a variety of sharing \npatterns. We saw comparable results for DaCapo: overheads of Recon ranged from 5.6x to 17.3x. Interestingly, \noverheads tended to be more severe for applica\u00adtions that perform infrequent sharing, than those that \nshare often. For example, dedup, which uses shared queues, and avrora, which exhibits a high-degree of \n.ne-grained sharing [3], both had fairly low overheads, around 6x. In contrast, swaptions has in\u00adfrequent \nsynchronization [2] and threads in lusearch interact very little [3] both suffered higher overheads, \naround 18x. This trend is further illuminated by the Java Grande benchmarks; these are primarily data-parallel \nscienti.c computations that perform lit\u00adtle sharing [28]; their average overhead is 75x. Nonetheless, \nRecon  Slowdown (x) Slowdown (x) Name Recon FR/W FR Base Name Recon FR/W FR Base Apps. weblech pbzip2 \naget apache mysql 1.1 1.3 1.9 5.4 23.9 1.2 1.3 1.9 31.7 102.1 1.2 1.3 1.9 31.7 127.2 1.1 1.5 1.9 177.1 \n129.9 PARSEC dedup canneal freqmine .uidanimate streamcluster blackscholes ferret bodytrack facesim swaption \nx264 vips 5.5 6.8 8.8 9.8 10.1 14.4 14.6 14.9 15.8 17.9 18.9 28.8 5.8 6.8 52.6 9.9 10.1 17.8 70.9 116.2 \n18.8 96.0 218.4 230.6 5.8 6.5 56.6 10.1 10.3 18.0 73.1 120.8 19.2 100.6 236.8 257.6 13.8 14.9 223.8 9.8 \n10.1 40.9 537.3 595.5 29.2 383.7 697.4 996.8 DaCapo pmd avrora tomcat xalan luindex lusearch 5.6 5.6 \n6.9 7.1 8.2 17.3 5.8 7.9 6.2 7.0 9.1 18.1 6.0 10.3 6.7 7.5 14.3 22.7 6.3 27.8 9.1 10.8 20.6 22.6 Java \nGrande 74.9 85.1 88.4 563.7 Table 4. Performance of Recon and less-optimized con.gurations relative \nto uninstrumented execution. is ef.cient in applications with high-frequency sharing and for all the \nmainstream applications we tested. Effectiveness of Optimizations. Comparing FR with Base and Recon with \nFR/W in Table 4, we see that the .rst-read and racy-lookup optimizations, respectively, signi.cantly \nimprove performance. Comparing FR/W and FR , we see that the .rst\u00adwrite optimization has less signi.cant \neffect in general likely because writes are less common than reads but for mysql and lusearch, the \n.rst-write optimization is clearly important. The data show that our optimizations are essential to Recon \ns ef\u00ad.ciency. For many applications, our optimizations reduce Recon s slowdown by orders of magnitude. \napache is one such applica\u00adtion: without optimizations, apache s slowdown is 177x, making full-scale \ntests nearly impossible due to timeouts and unhandled de\u00adlay conditions in the code. Optimizations reduce \nthis to just 5.4x, enabling Recon to be used with real bug-triggering inputs. In our experiments, we \nused PARSEC s simlarge inputs to make experimenting with unoptimized con.gurations feasible, but there \nis no need to scale inputs for use with Recon. We also exper\u00adimented with PARSEC s native input, using \nRecon with all op\u00adtimizations. Experiments .nished quickly, and we saw slowdowns nearly identical to \nthe simlarge input. The optimizations have less impact on our Java implemen\u00adtation, but still account \nfor signi.cant speedups (e.g., avrora, luindex). For most Java benchmarks, the racy-lookup optimiza\u00adtion \nhad little effect. Java uses several techniques that signi.cantly reduce the cost of acquiring locks \n[13]. It is likely that the racy\u00adlookup optimization is less bene.cial than in C/C++ because the cost \nof locking is lower in Java to begin with. Memory Overhead. The C/C++ Recon implementation uses a .xed-size \n4GB metadata table, dominating memory overheads in our experiments. Graphs are small in comparison. The \ntable is large enough that the impact of hash collisions was negligible. In a memory-constrained setting, \na smaller table could be used at the expense of decreased precision due to hash collisions. In Java, \neach .eld and array element is shadowed by a metadata location: mem\u00adory overhead scales roughly linearly \nwith the program s footprint. Peak overheads in the optimized version ranged from 2.5x to 16x. 7. Related \nWork Prior work has explored a variety of atomicity violation detection approaches. AVIO [17] is an invariant-based \napproach that infers from a set of training runs which unserializable interleavings are al\u00adlowed in correct \nexecutions. In subsequent runs, AVIO reports any unserializable interleavings not present in the invariant \nset as possi\u00adble atomicity violations. AVIO focuses on single-variable atomic\u00adity violations. SVD [30] \nattempts to infer atomic sections based on data and control .ow and determines whether an execution is \nse\u00adrializable with respect to those sections. Velodrome [8] is a sound, precise atomicity checker that \nreports an error if an program exe\u00adcution with explicit atomic blocks is not con.ict-serializable. Recent \nwork introduced general approaches to detecting con\u00adcurrency errors. Bugaboo [18] .rst proposed the use \nof context\u00adaware communication graphs for debugging general concurrency errors. DefUse [27] employs a \nsimilar communication-based strat\u00adegy, by .nding communication invariant violations related to er\u00adrors. \nNeither approach provides programmers with information about the actual interleaving schedule at the \nroot cause of the bug. Context-aware graphs, and DefUse invariants only isolate a sin\u00adgle communication \nevent related to a bug. One of Recon s main contributions is that it provides information about the execution \nschedule encoded in a reconstruction. Additionally, Bugaboo and DefUse rank anomalies along a single \naxis of suspiciousness. In contrast, Recon ranks reconstructions along several axes, using the features \ndescribed in Section 4. Finally, Recon s performance is orders of magnitude better than the performance \nof Bugaboo s software implementation. DefUse does not show overheads for a standard benchmark suite, \nso direct comparison is impossible, but Recon s performance on reactive applications is comparable. Interleaving \nConstrained MultiProcessor [32] (PSets) proposes architecture support for dynamic bug avoidance. PSets \nuses test runs to determine happens-before invariants on memory operations and tries to enforce them \nby manipulating the schedule. This tech\u00adnique handles several types of single-variable bugs. Falcon [23], \nuses a library of bug patterns to identify potential concurrency bugs; it watches a stream of memory \noperations to single addresses and searches for a matching pattern. Falcon s technique is mostly applicable \nto single-variable concurrency bugs. Hammer, et al. pro\u00adpose pattern-based dynamic analysis for detecting \nbugs related to atomicity properties of accesses to sets of variables [10]. Recon is set apart from prior \nwork by the fact that reconstruc\u00adtions are general, handling a variety of single-and multi-variable errors \nwithout relying on characteristics of speci.c bug types. Fur\u00adthermore, reconstructions illustrate bugs \nmore clearly, showing a focused portion of the interleaving schedule near the bug s cause. Inter-thread \ncommunication invariants have also been used to specify correct communication behavior at the function \nlevel and check that a program execution conforms to this speci.cation [29]. In contrast, Recon focuses \non communication at the instruction granularity and helps programmers understand unexpected behav\u00adior, \nrather than helping them enforce expected invariants. Another way of dealing with concurrency bugs is \nexplorative testing to expose buggy executions. CHESS [21] explores exe\u00adcutions by interposing on synchronization \noperations; its goal is to expose buggy executions during testing. Burckhardt et al. pro\u00adpose a scheduling \ntechnique that probabilistically exposes bugs [5]. CTrigger [22] is a heuristic to expose atomicity bugs. \nThe goal of these techniques is not to detect or explain bugs but to expose buggy executions. In contrast, \nRecon helps programmers understand bugs by reconstructing the interleavings that likely led to bugs symp\u00adtoms. \nExplorative testing is complementary to our approach.  Liblit et al. s Cooperative Bug Isolation (CBI) \n[15] uses sam\u00adpling techniques to collect information from deployed applications. Based on sets of labeled \npassing or failing runs, CBI .nds code points related to failures. CCI [11] extends these sampling tech\u00adniques \nto concurrency bug patterns. In contrast, Recon does not require deployment-scale data to detect bugs \nef.ciently. It provides more information about buggy interleavings, and doesn t rely on bug-speci.c patterns. \n8. Conclusion In this paper we introduced Recon, a general approach to isolat\u00ading and understanding concurrency \nbugs. Recon works by recon\u00adstructing fragments of buggy executions that are likely related to a bug. \nRecon provides suf.cient, yet succinct information to help programmers understand the causes of their \nbugs. Reconstructions show the schedule of execution that led to the bug, clearly exposing its root cause. \nReconstructions are built by observing multiple executions of a program and collecting times\u00adtamped communication \ngraphs that encode information about the ordering of inter-thread communication events. We developed \na simple machine-learning approach to identify buggy reconstruc\u00adtions. We proposed three bug-independent \nfeatures of reconstruc\u00adtions that together precisely isolate reconstructions of buggy ex\u00adecutions. In \norder to provide ef.cient collection of timestamped graphs, we used several techniques that signi.cantly \nreduce run\u00adtime overheads. We implemented Recon for C/C++ and Java and evaluated it using large software. \nOur results show Recon recon\u00adstructs buggy executions with virtually no false positives, and that collecting \nthe data comprising reconstructions takes just minutes. Acknowledgments We thank the anonymous reviewers \nand the Sampa group for their helpful feedback. Thanks to Joseph Devietti, Laura Ef.nger-Dean, Colin \nGordon, Dan Grossman, and Karin Strauss for their invalu\u00adable feedback on the manuscript. Special thanks \ngo to Zachary Rait and Julian Knutsen for early work on the Recon infrastructure. This work was supported \nin part by an IBM PhD fellowship, an ARCS Foundation Fellowship, a Microsoft Research Faculty Fellowship, \nNSF grant CCF-0811405, and gifts from Intel. References [1] Z. Anderson, D. Gay, R. Ennals, and E. Brewer. \nSharC: Checking Data Sharing Strategies for Multithreaded C. In PLDI, 2008. [2] C. Bienia, S. Kumar, \nJ. P. Singh, and K. Li. The parsec benchmark suite: Characterization and architectural implications. \nIn Proceedings of the 17th International Conference on Parallel Architectures and Compilation Techniques, \nOctober 2008. [3] S. M. Blackburn et al. The DaCapo Benchmarks: Java Benchmarking Development and Analysis. \nIn OOPSLA, 2006. [4] H.-J. Boehm and S. V. Adve. Foundations of the C++ Concurrency Memory Model. In \nPLDI, 2008. [5] S. Burckhardt, P. Kothari, M. Musuvathi, and S. Nagarakatte. A Ran\u00addomized Scheduler \nwith Probabilistic Guarantees of Finding Bugs. In ASPLOS, 2010. [6] C. Flanagan and S. N. Freund. FastTrack: \nEf.cient and Precise Dy\u00adnamic Race Detection. In PLDI, 2009. [7] C. Flanagan and S. N. Freund. The RoadRunner \nDynamic Analysis Framework for Concurrent Programs. In PASTE, 2010. [8] C. Flanagan, S. N. Freund, and \nJ. Yi. Velodrome: A Sound and Complete Dynamic Atomicity Checker for Multithreaded Programs. In PLDI, \n2008. [9] M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reutemann, and I. H. Witten. The WEKA Data \nMining Software: An Update. SIGKDD Explorations, 2009. [10] C. Hammer, J. Dolby, M. Vaziri, and F. Tip. \nDynamic Detection of Atomic-Set-Serializability Violations. In ICSE, 2008. [11] G. Jin, A. Thakur, B. \nLiblit, and S. Lu. Instrumentation and Sampling Strategies for Cooperative Concurrency Bug Isolation. \nIn OOPSLA, 2010. [12] P. Joshi and K. Sen. Predictive Typestate Checking of Multithreaded Java Programs. \nIn ASE, 2008. [13] K. Kawachiya, A. Koseki, and T. Onodera. Lock Reservation: Java Locks Can Mostly Do \nWithout Atomic Operations. In OOPSLA, 2002. [14] I. Kononenko. Estimating Attributes: Analysis and Extensions \nof RELIEF. In European Conference on Machine Learning, 1994. [15] B. Liblit. Cooperative Bug Isolation, \nvolume 4440 of Lecture Notes in Computer Science. Springer, 2007. [16] S. Lu, S. Park, E. Seo, and Y. \nZhou. Learning from Mistakes -A Com\u00adprehensive Study on Real World Concurrency Bug Characteristics. In \nASPLOS, 2008. [17] S. Lu, J. Tucek, F. Qin, and Y. Zhou. AVIO: Detecting Atomicity Violations via Access \nInterleaving Invariants. In ASPLOS, 2006. [18] B. Lucia and L. Ceze. Finding Concurrency Bugs with Context-Aware \nCommunication Graphs. In MICRO, 2009. [19] C.-K. Luk et al. Pin: Building Customized Program Analysis \nTools with Dynamic Instrumentation. In PLDI, 2005. [20] J. Manson, W. Pugh, and S. V. Adve. The Java \nMemory Model. In POPL, 2005. [21] M. Musuvathi, S. Qadeer, T. Ball, G. Basler, P. A. Nainar, and I. Neamtiu. \nFinding and Reproducing Heisenbugs in Concurrent Pro\u00adgrams. In OSDI, 2008. [22] S. Park, S. Lu, and Y. \nZhou. CTrigger: Exposing Atomicity Violation Bugs from Their Hiding Places. In ASPLOS, 2009. [23] S. \nPark, R. W. Vuduc, and M. J. Harrold. Falcon: Fault Localization in Concurrent Programs. In ICSE, 2010. \n[24] S. Park, Y. Zhou, W. Xiong, Z. Yin, R. Kaushik, K. H. Lee, and S. Lu. PRES: Probabilistic Replay \nwith Execution Sketching on Multiprocessors. In SOSP, 2009. [25] M. Ronsee and K. D. Bosschere. RecPlay: \nA Fully Integrated Practical Record/Replay System. ToCS, 1999. [26] S. Savage, M. Burrows, G. Nelson, \nP. Sobalvarro, and T. Anderson. Eraser: A Dynamic Data Race Detector for Multi-Threaded Programs. ToCS, \n1997. [27] Y. Shi, S. Park, Z. Yin, S. Lu, Y. Zhou, W. Chen, and W. Zheng. Do I Use the Wrong De.nition?: \nDeFuse: De.nition-Use Invariants for Detecting Concurrency and Sequential Bugs. In OOPSLA, 2010. [28] \nL. A. Smith, J. M. Bull, and J. Obdrz\u00b4alek. A parallel Java Grande benchmark suite. In Supercomputing, \n2001. [29] B. P. Wood, A. Sampson, L. Ceze, and D. Grossman. Composable Speci.cations for Structured \nShared-Memory Communication. In OOPSLA, 2010. [30] M. Xu, R. Bod\u00b4ik, and M. D. Hill. A Serializability \nViolation Detector for Shared-Memory Server Programs. In PLDI, June 2005. [31] M. Xu, M. D. Hill, and \nR. Bodik. A Regulated Transitive Reduction (RTR) for Longer Memory Race Recording. In ASPLOS, 2006. [32] \nJ. Yu and S. Narayanasamy. A Case for an Interleaving Constrained Shared-Memory Multi-Processor. In ISCA, \n2009. [33] P. Zhou, R. Teodorescu, and Y. Zhou. HARD: Hardware-Assisted Lockset-based Race Detection. \nIn HPCA, 2007.     \n\t\t\t", "proc_id": "1993498", "abstract": "<p>In this paper we propose Recon, a new general approach to concurrency debugging. Recon goes beyond just detecting bugs, it also presents to the programmer short fragments of buggy execution schedules that illustrate how and why bugs happened. These fragments, called <i>reconstructions</i>, are inferred from inter-thread communication surrounding the root cause of a bug and significantly simplify the process of <i>understanding</i> bugs.</p> <p>The key idea in Recon is to monitor executions and build graphs that encode inter-thread communication with enough context information to build reconstructions. Recon leverages reconstructions built from multiple application executions and uses machine learning to identify which ones illustrate the root cause of a bug. Recon's approach is <i>general</i> because it does not rely on heuristics specific to any type of bug, application, or programming model. Therefore, it is able to deal with single- and multiple-variable concurrency bugs regardless of their type (<i>e.g.</i>, atomicity violation, ordering, etc). To make graph collection efficient, Recon employs selective monitoring and allows metadata information to be imprecise without compromising accuracy. With these optimizations, Recon's graph collection imposes overheads typically between 5x and 20x for both C/C++ and Java programs, with overheads as low as 13% in our experiments. We evaluate Recon with buggy applications, and show it produces reconstructions that include all code points involved in bugs' causes, and presents them in an accurate order. We include a case study of understanding and fixing a previously unresolved bug to showcase Recon's effectiveness.</p>", "authors": [{"name": "Brandon Lucia", "author_profile_id": "81384609259", "affiliation": "University of Washington, Seattle, WA, USA", "person_id": "P2690594", "email_address": "blucia0a@cs.washington.edu", "orcid_id": ""}, {"name": "Benjamin P. Wood", "author_profile_id": "81470647599", "affiliation": "University of Washington, Seattle, WA, USA", "person_id": "P2690595", "email_address": "bpw@cs.washington.edu", "orcid_id": ""}, {"name": "Luis Ceze", "author_profile_id": "81100112680", "affiliation": "University of Washington, Seattle, WA, USA", "person_id": "P2690596", "email_address": "luisceze@cs.washington.edu", "orcid_id": ""}], "doi_number": "10.1145/1993498.1993543", "year": "2011", "article_id": "1993543", "conference": "PLDI", "title": "Isolating and understanding concurrency errors using reconstructed execution fragments", "url": "http://dl.acm.org/citation.cfm?id=1993543"}