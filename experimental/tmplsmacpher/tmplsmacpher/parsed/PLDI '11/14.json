{"article_publication_date": "06-04-2011", "fulltext": "\n EnerJ: Approximate Data Types for Safe and General Low-Power Computation Adrian Sampson Werner Dietl \nEmily Fortuna Danushen Gnanapragasam Luis Ceze Dan Grossman University of Washington, Department of Computer \nScience &#38; Engineering http://sampa.cs.washington.edu/ Abstract Energy is increasingly a .rst-order \nconcern in computer systems. Exploiting energy-accuracy trade-offs is an attractive choice in applications \nthat can tolerate inaccuracies. Recent work has explored exposing this trade-off in programming models. \nA key challenge, though, is how to isolate parts of the program that must be precise from those that \ncan be approximated so that a program functions correctly even as quality of service degrades. We propose \nusing type quali.ers to declare data that may be subject to approximate computation. Using these types, \nthe system automatically maps approximate variables to low-power storage, uses low-power operations, \nand even applies more energy-ef.cient algorithms provided by the programmer. In addition, the system \ncan statically guarantee isolation of the precise program component from the approximate component. This \nallows a programmer to control explicitly how information .ows from approximate data to precise data. \nImportantly, employing static analysis eliminates the need for dynamic checks, further improving energy \nsavings. As a proof of concept, we develop EnerJ, an extension to Java that adds approximate data types. \nWe also propose a hardware architec\u00adture that offers explicit approximate storage and computation. We \nport several applications to EnerJ and show that our extensions are expressive and effective; a small \nnumber of annotations lead to sig\u00adni.cant potential energy savings (10% 50%) at very little accuracy \ncost. Categories and Subject Descriptors C.0 [Computer Systems Or\u00adganization]: General Hardware/software \ninterfaces; D.3.3 [Pro\u00adgramming Languages]: Language Constructs and Features Data types and structures \nGeneral Terms Languages, Performance, Design Keywords Accuracy-aware computing, power-aware computing, \nenergy, soft errors, critical data 1. Introduction Energy consumption is an increasing concern in many \ncomputer systems. Battery life is a .rst-order constraint in mobile systems, and power/cooling costs \nlargely dominate the cost of equipment Permission to make digital or hard copies of all or part of this \nwork for personal or classroom use is granted without fee provided that copies are not made or distributed \nfor pro.t or commercial advantage and that copies bear this notice and the full citation on the .rst \npage. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior \nspeci.c permission and/or a fee. PLDI 11, June 4 8, 2011, San Jose, California, USA. Copyright c &#38;#169; \n2011 ACM 978-1-4503-0663-8/11/06. . . $10.00 in data-centers. More fundamentally, current trends point \ntoward a utilization wall, in which the amount of active die area is limited by how much power can be \nfed to a chip. Much of the focus in reducing energy consumption has been on low-power architectures, \nperformance/power trade-offs, and re\u00adsource management. While those techniques are effective and can \nbe applied without software knowledge, exposing energy considera\u00adtions at the programming language level \ncan enable a whole new set of energy optimizations. This work is a step in that direction. Recent research \nhas begun to explore energy-accuracy trade-offs in general-purpose programs. A key observation is that \nsystems spend a signi.cant amount of energy guaranteeing correctness. Consequently, a system can save \nenergy by exposing faults to the application. Many studies have shown that a variety of applications \nare resilient to hardware and software errors during execution [1, 8, 9, 19, 21 23, 25, 31, 35]. Importantly, \nthese studies universally show that applications have portions that are more resilient and portions that \nare critical and must be protected from error. For example, an image renderer can tolerate errors in \nthe pixel data it outputs a small number of erroneous pixels may be acceptable or even undetectable. \nHowever, an error in a jump table could lead to a crash, and even small errors in the image .le format \nmight make the output unreadable. While approximate computation can save a signi.cant amount of energy, \ndistinguishing between the critical and non-critical portions of a program is dif.cult. Prior proposals \nhave used annotations on code blocks (e.g., [9]) and data allocation sites (e.g., [23]). These annotations, \nhowever, do not offer any guarantee that the fundamental operation of the program is not compromised. \nIn other words, these annotations are either unsafe and may lead to unacceptable program behavior or \nneed dynamic checks that end up consuming energy. We need a way to allow programmers to compose programs \nfrom approximate and precise components safely. Moreover, we need to guarantee safety statically to avoid \nspending energy checking properties at runtime. The key insight in this paper is the application of type-based \ninformation-.ow tracking [32] ideas to address these problems. Contributions. This paper proposes a model \nfor approximate pro\u00adgramming that is both safe and general. We use a type system that isolates the precise \nportion of the program from the approximate portion. The programmer must explicitly delineate .ow from \nap\u00adproximate data to precise data. The model is thus safe in that it guarantees precise computation unless \ngiven explicit programmer permission. Safety is statically enforced and no dynamic checks are required, \nminimizing the overheads imposed by the language. We present EnerJ, a language for principled approximate \ncomput\u00ading. EnerJ extends Java with type quali.ers that distinguish between approximate and precise data \ntypes. Data annotated with the ap\u00adproximate quali.er can be stored approximately and computations involving \nit can be performed approximately. EnerJ also provides endorsements, which are programmer-speci.ed points \nat which approximate-to-precise data .ow may occur. The language supports programming constructs for \nalgorithmic approximation, in which the programmer produces different implementations of functionality \nfor approximate and precise data. We formalize a core of EnerJ and prove a non-interference property \nin the absence of endorsements.  Our programming model is general in that it uni.es approximate data \nstorage, approximate computation, and approximate algorithms. Programmers use a single abstraction to \napply all three forms of approximation. The model is also high-level and portable: the implementation \n(compiler, runtime system, hardware) is entirely responsible for choosing the energy-saving mechanisms \nto employ and when to do so, guaranteeing correctness for precise data and best effort for the rest. \nWhile EnerJ is designed to support general approximation strategies and therefore ensure full portability \nand backward\u00adcompatibility, we demonstrate its effectiveness using a proposed approximation-aware architecture \nwith approximate memory and imprecise functional units. We have ported several applications to EnerJ \nto demonstrate that a small amount of annotation can allow a program to save a large amount of energy \nwhile not compromising quality of service signi.cantly. Outline. We .rst detail the EnerJ language extensions \nin Section 2. Section 3 formalizes a core of the language, allowing us to prove a non-interference property. \nThe full formalism and proof are presented in an accompanying technical report [33]. Next, Section 4 \ndescribes hypothetical hardware for executing EnerJ programs. While other execution substrates are possible, \nthis proposed model provides a basis for the evaluation in Sections 5 and 6; there, we assess EnerJ s \nexpressiveness and potential energy savings. The type checker and simulation infrastructure used in our \nevaluation are available at http://sampa.cs.washington.edu/. Section 7 presents related work and Section \n8 concludes. 2. A Type System for Approximate Computation This section describes EnerJ s extensions to \nJava, which are based on a system of type quali.ers. We .rst describe the quali.ers themselves. We next \nexplain how programmers precisely control when approximate data can affect precise state. We describe \nthe implementation of approximate operations using overloading. We then discuss conditional statements \nand the prevention of implicit .ows. Finally, we describe the type system s extension to object\u00adoriented \nprogramming constructs and its interaction with Java arrays. EnerJ implements these language constructs \nas backwards\u00adcompatible additions to Java extended with type annotations [11]. Table 1 summarizes our \nextensions and their concrete syntax. 2.1 Type Annotations Every value in the program has an approximate \nor precise type. The programmer annotates types with the @Approx and @Precise quali.ers. Precise types \nare the default, so typically only @Approx is made explicit. It is illegal to assign an approximate-typed \nvalue into a precise-typed variable. Intuitively, this prevents direct .ow of data from approximate to \nprecise variables. For instance, the following assignment is illegal: @Approx int a = ...; int p; // \nprecise by default p = a; // illegal Approximate-to-precise data .ow is clearly undesirable, but it seems \nnatural to allow .ow in the opposite direction. For primitive Java types, we allow precise-to-approximate \ndata .ow via subtyping. Speci.cally, we make each precise primitive Java type a subtype of its approximate \ncounterpart. This choice permits, for instance, the assignment a = p; in the above example. For Java \ns reference (class) types, this subtyping relationship is unsound. The quali.er of a reference can in.uence \nthe quali.ers of its .elds (see Section 2.5), so subtyping on mutable references is unsound for standard \nreasons. We .nd that this limitation is not cumbersome in practice. We also introduce a @Top quali.er \nto denote the common supertype of @Approx and @Precise types. Semantics of approximation. EnerJ takes \nan all-or-nothing ap\u00adproach to approximation. Precise values carry traditional guarantees of correctness; \napproximate values have no guarantees. A more complex system could provide multiple levels of approximation \nand guaranteed error bounds, but we .nd that this simple system is suf.ciently expressive for a wide \nrange of applications. The language achieves generality by leaving approximation pat\u00adterns unspeci.ed, \nbut programmers can informally expect approx\u00adimate data to be mostly correct and adhere to normal execution \nsemantics except for occasional errors. The degree of precision is an orthogonal concern; a separate \nsystem could tune the frequency and intensity of errors in approximate data. 2.2 Endorsement Fully isolating \napproximate and precise parts of a program would likely not be very useful. Eventually a program needs \nto store data, transmit it, or present it to the programmer at which point the program should begin behaving \nprecisely. As a general pattern, programs we examined frequently had a phase of fault-tolerant computation \nfollowed by a phase of fault-sensitive reduction or output. For instance, one application consists of \na resilient image manipulation phase followed by a critical checksum over the result (see Section 6.3). \nIt is essential that data be occasionally allowed to break the strict separation enforced by the type \nsystem. We require the programmer to control explicitly when approxi\u00admate data can affect precise state. \nTo this end, we borrow the concept (and term) of endorsement from past work on information-.ow con\u00adtrol \n[2]. An explicit static function endorse allows the programmer to use approximate data as if it were \nprecise. The function acts as a cast from any approximate type to its precise equivalent. Endorse\u00adments \nmay have implicit runtime effects; they might, for example, copy values from approximate to precise memory. \nThe previous example can be made legal with an endorsement: @Approx int a = ...; int p; // precise by \ndefault p= endorse(a); // legal By inserting an endorsement, the programmer certi.es that the ap\u00adproximate \ndata is handled intelligently and will not cause undesired results in the precise part of the program. \n 2.3 Approximate Operations The type system thus far provides a mechanism for approximating storage. \nClearly, variables with approximate type may be located in unreliable memory modules. However, approximate \ncomputation requires additional features. We introduce approximate computation by overloading operators \nand methods based on the type quali.ers. For instance, our language provides two signatures for the + \noperator on integers: one taking two precise integers and producing a precise integer and the other taking \ntwo approximate integers and producing an approximate integer. The latter may compute its result approximately \nand thus may run on low-power hardware. Programmers can extend this concept by overloading methods with \nquali.ed parameter types. Bidirectional typing. The above approach occasionally applies precise operations \nwhere approximate operations would suf.ce.  Construct Purpose Section @Approx, @Precise, @Top Type annotations: \nqualify any type in the program. (Default is @Precise.) 2.1 endorse(e) Cast an approximate value to its \nprecise equivalent. 2.2 @Approximable Class annotation: allow a class to have both precise and approximate \ninstances. 2.5 @Context Type annotation: in approximable class de.nitions, the precision of the type \ndepends on the 2.5.1 precision of the enclosing object. APPROX Method naming convention: this implementation \nof the method may be invoked when the 2.5.2 receiver has approximate type. Table 1. Summary of EnerJ \ns language extensions. Consider the expression a=b+c where a is approximate but b and c are precise. \nOverloading selects precise addition even though the result will only be used approximately. It is possible \nto force an approximate operation by upcasting either operand to an approximate type, but we provide \na slight optimization that avoids the need for additional annotation. EnerJ implements an extremely simple \nform of bidirectional type checking [7] that applies approximate arithmetic operators when the result \ntype is approximate: on the right-hand side of assignment operators and in method arguments. We .nd that \nthis small optimization makes it simpler to write approximate arithmetic expressions that include precise \ndata.  2.4 Control Flow To provide the desired property that information never .ows from approximate \nto precise data, we must disallow implicit .ows that occur via control .ow. For example, the following \nprogram violates the desired isolation property: @Approx int val = ...; boolean .ag; // precise if (val \n== 5) { .ag = true; } else { .ag = false; } Even though .ag is precise and no endorsement is present, \nits value is affected by the approximate variable val. EnerJ avoids this situation by prohibiting approximate \nvalues in conditions that affect control .ow (such as if and while statements). In the above example, \nval == 5 has approximate type because the approximate version of == must be used. Our language disallows \nthis expression in the condition, though the programmer can work around this restriction using if(endorse(val \n== 5)). This restriction is conservative: it prohibits approximate con\u00additions even when the result can \naffect only approximate data. A more sophisticated approach would allow only approximate values to be \nproduced in statements conditioned on approximate data. We .nd that our simpler approach is suf.cient; \nendorsements allow the programmer to work around the restriction when needed.  2.5 Objects EnerJ s type \nquali.ers are not limited to primitive types. Classes also support approximation. Clients of an approximable \nclass can create precise and approximate instances of the class. The author of the class de.nes the meaning \nof approximation for the class. Approximable classes are distinguished by the @Approximable class annotation. \nSuch a class exhibits quali.er polymorphism [14]: types within the class de.nition may depend on the \nquali.er of the instance. Note that precise class types are not subtypes of their approxi\u00admate counterparts, \nas is the case with primitive types (Section 2.1); such a subtyping relationship would be fundamentally \nunsound for the standard reasons related to mutable references. 2.5.1 Contextual Data Types The @Context \nquali.er is available in de.nitions of non-static members of approximable classes. The meaning of the \nquali.er depends on the precision of the instance of the enclosing class. (In terms of quali.er polymorphism, \n@Context refers to the class quali.er parameter, which is determined by the quali.er placed on the instance.) \nConsider the following class de.nition: @Approximable class IntPair { @Context int x; @Context int y; \n@Approx int numAdditions = 0; void addToBoth(@Context int amount) { x += amount; y += amount; numAdditions++; \n}}If a is an approximate instance of IntPair, then the .elds a.x, a.y, and a.numAdditions are all of \napproximate integer type. However, if p is a precise instance of the class, then p.x and p.y are precise \nbut p.numAdditions is still approximate. Furthermore, the argument to the invocation p.addToBoth() must \nbe precise; the argument to a.addToBoth() may be approximate.  2.5.2 Algorithmic Approximation Approximable \nclasses may also specialize method de.nitions based on their quali.er. That is, the programmer can write \ntwo implemen\u00adtations: one to be called when the receiver has precise type and another that can be called \nwhen the receiver is approximate. Con\u00adsider the following implementations of a mean calculation over \na list of .oats: @Approximable class FloatSet { @Context .oat[] nums = ...; .oat mean() { .oat total \n= 0.0f; for (int i = 0; i < nums.length; ++i) total += nums[i]; return total / nums.length; } @Approx \n.oat mean APPROX() { @Approx .oat total = 0.0f; for (int i = 0; i < nums.length; i += 2) total += nums[i]; \nreturn 2 * total / nums.length; }} EnerJ uses a naming convention, consisting of the APPROX suf.x, to \ndistinguish methods overloaded on precision. The .rst implementation of mean is called when the receiver \nis precise. The second implementation calculates an approximation of the mean: it averages only half \nthe numbers in the set. This implementation will be used for the invocation s.mean() where s is an approximate \ninstance of FloatSet. Note that the compiler automatically decides which implementation of the method \nto invoke depending on the receiver type; the same invocation is used in either case. It is the programmer \ns responsibility to ensure that the two im\u00adplementations are similar enough that they can be safely substituted. \nThis is important for backwards compatibility (a plain Java com\u00adpiler will ignore the naming convention \nand always use the precise  Prg ::= Cls, C, e Cls ::= class Cid extends C { fd md } C ::= Cid | Object \nP ::= int | float q ::= precise | approx | top | context | lost T ::= q C | q P fd ::= T f; md ::= T \nm(T pid) q { e } x ::= pid | this e ::= null | L | x | new q C() | e.f | e0.f:=e1 | e0.m(e) | (q C) e \n| e0 . e1 | if(e0) {e1} else {e2} f .eld identi.er pid parameter identi.er m method identi.er Cid class \nidenti.er Figure 1. The syntax of the FEnerJ programming language. The symbol A denotes a sequence of \nelements A. version) and best effort (the implementation may use the precise version if energy is not \nconstrained). This facility makes it simple to couple algorithmic approximation with data approximation \na single annotation makes an instance use both approximate data (via @Context) and approximate code (via \noverloading).  2.6 Arrays The programmer can declare arrays with approximate element types, but the \narray s length is always kept precise for memory safety. We .nd that programs often use large arrays \nof approximate primitive elements; in this case, the elements themselves are all approximated and only \nthe length requires precise guarantees. EnerJ prohibits approximate integers from being used as array \nsubscripts. That is, in the expression a[i], the value i must be precise. This makes it easier for the \nprogrammer to prevent out-of-bounds errors due to approximation. 3. Formal Semantics To study the formal \nsemantics of EnerJ, we de.ne the minimal lan\u00adguage FEnerJ. The language is based on Featherweight Java \n[16] and adds precision quali.ers and state. The formal language omits En-erJ s endorsements and thus \ncan guarantee isolation of approximate and precise program components. This isolation property suggests \nthat, in the absence of endorsement, approximate data in an EnerJ program cannot affect precise state. \nThe accompanying technical report [33] formalizes this language and proves type soundness as well as \na non-interference property that demonstrates the desired isolation of approximate and precise data. \n 3.1 Programming Language Figure 1 presents the syntax of FEnerJ. Programs consist of a sequence of classes, \na main class, and a main expression. Execution is modeled by instantiating the main class and then evaluating \nthe main expression. A class de.nition consists of a name, the name of the superclass, and .eld and method \nde.nitions. The @Approximable annotation is not modeled in FEnerJ; all classes in the formal language \ncan have approximate and precise instances and this has @Context type. The annotation is required only \nin order to provide backward\u00adcompatibility with Java so that this in a non-approximable class has @Precise \ntype. We use C to range over class names and P for the names of primitive types. We de.ne the precision \nquali.ers q as discussed in Section 2.1, but with the additional quali.er lost; this quali.er is used \nto express situations when context information is not expressible (i.e., lost). Types T include quali.ers. \nField declarations consist of the .eld type and name. Method declarations consist of the return type, \nmethod name, a sequence of parameter types and identi.ers, the method precision, and the method body. \nWe use the method precision quali.er to denote overloading of the method based on the precision of the \nreceiver as introduced in Section 2.5.2. Variables are either a parameter identi.er or the special variable \nthis, signifying the current object. The language has the following expressions: the null literal, liter\u00adals \nof the primitive types, reads of local variables, instantiation, .eld reads and writes, method calls, \ncasts, binary primitive operations, and conditionals. For brevity, the discussion below presents only \nthe rules for .eld reads, .eld writes, and conditionals. Subtyping. Subtyping is de.ned using an ordering \nof the precision quali.ers and subclassing. The following rules de.ne the ordering of precision quali.ers: \n' ordering of precision quali.ers q <:q q q =top q <:q lost q <:q top q <:q q Recall that top quali.es \nthe common supertype of precise and approx types. Every quali.er other than top is below lost; every \nquali.er is below top; and the relation is re.exive. Note that the precise and approx quali.ers are not \nrelated. Subclassing is the re.exive and transitive closure of the relation induced by the class declarations. \nSubtyping takes both ordering of precision quali.ers and subclassing into account. For primitive types, \nwe additionally have that a precise type is a subtype of the approximate type as described in Section \n2.1. Context adaptation. We use context adaptation to replace the context quali.er when it appears in \na .eld access or method invocation. Here the left-hand side of c denotes the quali.er of the receiver \nexpression; the right-hand side is the precision quali.er of the .eld or in the method signature. ' '' \nq c q= q combining two precision quali.ers ' q=context . (q .{approx, precise, context}) q c q' = q q'=context \n. (q .{top, lost}) q'= context ' '' q c q= lost q c q= q Note that context adapts to lost when the left-hand-side \nquali.er is top because the appropriate quali.er cannot be determined. We additionally de.ne c to take \na type as the right-hand side; this adapts the precision quali.er of the type. We de.ne partial look-up \nfunctions FType and MSig that deter\u00admine the .eld type and method signature for a given .eld/method in \nan access or invocation. Note that these use the adaptation rules described above. Type rules. The static \ntype environment sG maps local variables to their declared types. Given a static environment, expressions \nare typed as follows: sG f e : T expression typing sG f e0 : qC FType(qC , f ) = T sG f e0.f : T sG f \ne0 : qC FType(qC , f ) = T lost ./T sG f e1 : T sG f e0.f := e1 : T sG f e0 : precise P sG f e1 : T \nsG f e2 : T  sG f if(e0) {e1} else {e2} : T A .eld read determines the type of the receiver expression \nand then uses FType to determine the adapted type of the .eld. A .eld write similarly determines the \nadapted type of the .eld and checks that the right-hand side has an appropriate type. In addition, we \nensure that the adaptation of the declared .eld type did not lose precision information. Notice that \nwe can read a .eld with lost precision information, but that it would be unsound to allow the update \nof such a .eld. Finally, for the conditional expression, we ensure that the condi\u00adtion is of a precise \nprimitive type and that there is a common type T that can be assigned to both subexpressions.  3.2 Operational \nSemantics The runtime system of FEnerJ models the heap h as a mapping from addresses . to objects, where \nobjects are a pair of the runtime type T and the .eld values v of the object. The runtime environment \nrG maps local variables x to values v. The runtime system of FEnerJ de.nes a standard big-step opera\u00adtional \nsemantics: Figure 2. Hardware model assumed in our system. Shaded areas indicate components that support \napproximation. Registers and the data cache have SRAM storage cells that can be made approximate by decreasing \nsupply voltage. Functional units support approxima\u00adtion via supply voltage reduction. Floating point \nfunctional units also support approximation via smaller mantissas. Main memory (DRAM) supports approximation \nby reducing refresh rate. The non-interference property of FEnerJ guarantees that approx\u00adimate computations \ndo not in.uence precise values. Speci.cally, changing approximate values in the heap or runtime environment \ndoes not change the precise parts of the heap or the result of the computation. More formally, we show: \n . ... r G f h ' h, e . f Prg OK .f h, rG : sG big-step operational semantics ' rG f h, e h ,v  use \nspecial functional units that perform approximate operations. sG f e : T ,v rG f h, e h ' ,v ~ rG f \nh, e0 h ' ,.0 h ' (.0.f ) =v =.h ' = h ' h ~ rG ~r G = h . = . . v ~ = v h ' ,v rG f h, e0.f f h, r G \n: sG rG f h, e0 h0,.0 rG f h0, e1 h1,v ' For the proof of this property we introduced a checked operational \nh1[.0.f := v] = h ' rG f h, e0.f := e1 h ,v rG f h, e0 h0, (q, rL) rL=0 ' rG f h0, e1 h ,v ' rG f h, \nif(e0) {e1} else {e2} h ,v ' rG f h, e0 h0, (q, 0) rG f h0, e2 h ,v ' rG f h, if(e0) {e1} else {e2} h \n,v These rules re.ect precise execution with conventional precision guarantees. To model computation \non an execution substrate that supports approximation, the following rule could be introduced: r'' ~ \n' ~ G f h, e h ,v h = hv = v rG f h, e h ' ,v We use ~ = to denote an equality that disregards approximate \nvalues for comparing heaps and values with identical types. The rule permits any approximate value in \nthe heap to be replaced with any other value of the same type and any expression producing a value of \nan approximate type to produce any other value of that type instead. This rule re.ects EnerJ s lack of \nguarantees for approximate values.  3.3 Properties We prove two properties about FEnerJ: type soundness \nand non\u00adinterference. The full proofs are listed in the accompanying technical report [33]. The usual \ntype soundness property expresses that, for a well\u00adtyped program and corresponding static and runtime \nenvironments, we know that (1) the runtime environment after evaluating the expression is still well \nformed, and (2) a static type that can be assigned to the expression can also be assigned to the value \nthat is  the result of evaluating the expression. Formally: f Prg OK .f h, rG : sG semantics that ensures \nin every evaluation step that the precise and approximate parts are separated. We can then show that \nthe evaluation of a well-typed expression always passes the checked semantics of the programming language. \n4. Execution Model While an EnerJ program distinguishes abstractly between approxi\u00admate and precise data, \nit does not de.ne the particular approximation strategies that are applied to the program. (In fact, \none valid execu\u00adtion is to ignore all annotations and execute the code as plain Java.) An approximation-aware \nexecution substrate is needed to take advan\u00adtage of EnerJ s annotations. We choose to examine approximation \nat the architecture level. Alternatively, a runtime system on top of commodity hardware can also offer \napproximate execution features (e.g., lower .oating point precision, elision of memory operations, etc.). \nAlso, note that algorithmic approximation (see Section 2.5) is independent of the execution substrate. \nThis section describes our hardware model, the ISA extensions used for approximation, and how the extensions \nenable energy savings. 4.1 Approximation-Aware ISA Extensions We want to leverage both approximate storage \nand approximate op\u00aderations. Our hardware model offers approximate storage in the form of unreliable \nregisters, data caches, and main memory. Approximate and precise registers are distinguished based on \nthe register number. Approximate data stored in memory is distinguished from precise data based on address; \nregions of physical memory are marked as approximate and, when accessed, are stored in approximate portions \nof the data cache. For approximate operations, we assume speci.c instructions for approximate integer \nALU operations as well as ap\u00adproximate .oating point operations. Approximate instructions can f h ' , \nrG : sG sG f e : T =. rG f h, e h ' ,vh ' , rG(this) f v : T Figure 2 summarizes our assumed hardware \nmodel. An instruction stream may have a mix of approximate and pre- The proof is by rule induction over \nthe operational semantics; in cise instructions. Precise instructions have the same guarantees as separate \nlemmas we formalize that the context adaptation operation instructions in today s ISAs. Note that an \napproximate instruction c is sound. is simply a hint to the architecture that it may apply a variety \nof energy-saving approximations when executing the given instruction. The particular approximations employed \nby a given architecture are not exposed to the program; a processor supporting no approxima\u00adtions just \nexecutes approximate instructions precisely and saves no energy. An approximation-aware ISA thus allows \na single binary to bene.t from new approximations as they are implemented in future microarchitectures. \n Layout of approximate data. Our hardware model supports ap\u00adproximate memory data at a cache line granularity, \nin which soft\u00adware can con.gure any line as approximate. This can be supported by having a bit per line \nin each page that indicates whether the cor\u00adresponding line is approximate. Based on that bit, a cache \ncontroller determines the supply voltage of a line (lower for approximate lines), and the refresh rate \nfor regions of DRAM. This bitmap needs to be kept precise. With a typical cache line size of 64 bytes, \nthis is less than 0.2% overhead. Note that both selective supply voltage for caches [13] and selective \nrefresh rate for DRAM [15] are hardware techniques that have been proposed in the past. Setting approximation \non a cache line basis requires the runtime system to segregate approximate and precise data in different \ncache lines. We propose the following simple technique for laying out objects with both approximate and \nprecise .elds. First, lay out the precise portion of the object (including the vtable pointer) contiguously. \nEach cache line containing at least one precise .eld is marked as precise. Then, lay out the approximate \n.elds after the end of the precise data. Some of this data may be placed in a precise line (that is, \na line containing some precise data already); in this case, the approximate data stays precise and saves \nno memory energy. (Note that wasting space in the precise line in order to place the data in an approximate \nline would use more memory and thus more energy.) The remaining approximate .elds that do not .t in the \nlast precise line can be placed in approximate lines. Fields in superclasses may not be reordered in \nsubclasses. Thus, a subclass of a class with approximate data may waste space in an approximate line \nin order to place precise .elds of the subclass in a precise line. While we simulate the artifacts of \nthis layout scheme for our evaluation, a .ner granularity of approximate memory storage would mitigate \nor eliminate the resulting loss of approximation. More sophisticated layout algorithms could also improve \nenergy savings; this is a target for compile-time optimization. Note that even if an approximate .eld \nends up stored in precise memory, it will still be loaded into approximate registers and be subject to \napproximate operations and algorithms. The layout problem is much simpler for arrays of approximate primitive \ntypes. The .rst line, which contains the length and type information, must be precise, with all remaining \nlines approximate.  4.2 Hardware Techniques for Saving Energy There are many strategies for saving energy \nwith approximate storage and data operations. This section discusses some of the techniques explored \nin prior research. We assume these techniques in our simulations, which we describe later. The techniques \nare summarized in Table 2. Voltage scaling in logic circuits. Aggressive voltage scaling can result in \nover 30% energy reduction with ~ 1% error rate [10] and 22% reduction with ~ 0.01% error rate. Recent \nwork [9, 17] proposed to expose the errors to applications that can tolerate it and saw similar results. \nIn our model, we assume aggressive voltage scaling for the processor units executing approximate instructions, \nincluding integer and .oating-point operations. As for an error model, the choices are single bit .ip, \nlast value, and random value. We consider all three but our evaluation mainly depicts the random\u00advalue \nassumption, which is the most realistic. Mild Medium Aggressive DRAM refresh: per-second bit 10-9 10-5 \n10-3 .ip probability Memory power saved 17% 22% 24% SRAM read upset probability 10-16.7 10-7.4 10-3 SRAM \nwrite failure probability 10-5.59 10-4.94 10-3 Supply power saved 70% 80% 90%* float mantissa bits 168 \n4 double mantissa bits 3216 8 Energy saved per operation 32% 78% 85%* Arithmetic timing error proba-10-6 \n10-4 10-2 bility Energy saved per operation 12%* 22% 30% Table 2. Approximation strategies simulated \nin our evaluation. Numbers marked with * are educated guesses by the authors; the others are taken from \nthe sources described in Section 4.2. Note that all values for the Medium level are taken from the literature. \nWidth reduction in .oating point operations. A direct approach to approximate arithmetic operations on \n.oating point values is to ignore part of the mantissa in the operands. As observed in [34], many applications \ndo not need the full mantissa. According to their model, a .oating-point multiplier using 8-bit mantissas \nuses 78% less energy per operation than a full 24-bit multiplier. DRAM refresh rate. Reducing the refresh \nrate of dynamic RAM leads to potential data decay but can substantially reduce power consumption with \na low error rate. As proposed by Liu et al. [23], an approximation-aware DRAM system might reduce the \nrefresh rate on lines containing approximate data. As in that work, we assume that reducing the refresh \nrate to 1 Hz reduces power by about 20%. In a study performed by Bhalodia [4], a DRAM cell not refreshed \nfor 10 seconds experiences a failure with per-bit probability approximately 10-5. We conservatively assume \nthis error rate for the reduced refresh rate of 1 Hz. SRAM supply voltage. Registers and data caches \nconsist of static RAM (SRAM) cells. Reducing the supply voltage to SRAM cells lowers the leakage current \nof the cells but decreases the data in\u00adtegrity [13]. As examined by Kumar [18], these errors are domi\u00adnated \nby read upsets and write failures, which occur when a bit is read or written. A read upset occurs when \nthe stored bit is .ipped while it is read; a write failure occurs when the wrong bit is written. Reducing \nSRAM supply voltage by 80% results in read upset and write failure probabilities of 10-7.4 and 10-4.94 \nrespectively. Soft failures, bit .ips in stored data due to cosmic rays and other events, are comparatively \nrare and depend less on the supply voltage. Section 5.4 describes the model we use to combine these various \npotential energy savings into an overall CPU/memory system en\u00adergy reduction. To put the potential energy \nsavings in perspective, according to recent studies [12, 24], the CPU and memory together account for \nwell over 50% of the overall system power in servers as well as notebooks. In a smartphone, CPU and memory \naccount for about 20% and the radio typically close to 50% of the overall power [6]. 5. Implementation \nWe implement EnerJ as an extension to the Java programming language based on the pluggable type mechanism \nproposed by Papi et al. [28]. EnerJ is implemented using the Checker Framework1 1 http://types.cs.washington.edu/checker-framework/ \n  infrastructure, which builds on the JSR 3082 extension to Java s annotation facility. JSR 308 permits \nannotations on any explicit type in the program. The EnerJ type checker extends the rules from Section \n3 to all of Java, including arrays and generics. We also implement a simulation infrastructure that emulates \nan approximate computing architecture as described in Section 4. 3 5.1 Type Checker EnerJ provides the \ntype quali.ers listed in Table 1 @Approx, @Precise, @Top, and @Context as JSR 308 type annotations. The \ndefault type quali.er for unannotated types is @Precise, meaning that any Java program may be compiled \nas an EnerJ program with no change in semantics. The programmer can add approximations to the program \nincrementally. While reference types may be annotated as @Approx, this only affects the meaning of @Context \nannotations in the class de.nition and method binding on the receiver. Our implementation never approximates \npointers. 5.2 Simulator To evaluate our system, we implement a compiler and runtime system that executes \nEnerJ code as if it were running on an approximation-aware architecture as described in Section 4. We \ninstrument method calls, object creation and destruction, arithmetic operators, and memory accesses to \ncollect statistics and inject faults. The runtime system is implemented as a Java library and is in\u00advoked \nby the instrumentation calls. It records memory-footprint and arithmetic-operation statistics while simultaneously \ninjecting transient faults to emulate approximate execution. To avoid spurious errors due to approximation, \nour simulated approximate functional units never raise divide-by-zero exceptions. Approximate .oating-point \ndivision by zero returns the NaN value; approximate integer divide-by-zero returns zero. 5.3 Approximations \nOur simulator implements the approximation strategies described in Section 4.2. Table 2 summarizes the \napproximations used, their associated error probabilities, and their estimated energy savings. Floating-point \nbit-width reduction is performed when executing Java s arithmetic operators on operands that are approximate \n.oat and double values. SRAM read upsets and write failures are simu\u00adlated by .ipping each bit read or \nwritten with a constant probability. For DRAM refresh reduction, every bit also has an independent probability \nof inversion; here, the probability is proportional to the amount of time since the last access to the \nbit. For the purposes of our evaluation, we distinguish SRAM and DRAM data using the following rough \napproximation: data on the heap is considered to be stored in DRAM; stack data is considered SRAM. Future \nevaluations not constrained by the abstraction of the JVM could explore a more nuanced model.  5.4 Energy \nModel To summarize the effectiveness of EnerJ s energy-saving properties, we estimate the potential overall \nsavings of the processor/memory system when executing each benchmark approximately. To do so, we consider \na simpli.ed model with three components to the system s energy consumption: instruction execution, SRAM \nstorage (registers and cache), and DRAM storage. Our model omits overheads of implementing or switching \nto approximate hardware. For example, we do not model any latency in scaling the voltage on the logic \nunits. 2 http://types.cs.washington.edu/jsr308/ 3 The EnerJ type checker and simulator are available \nfrom our website: http://sampa.cs.washington.edu/sampa/EnerJ For this reason, our results can be considered \noptimistic; future work should model approximate hardware in more detail. To estimate the savings for \ninstruction execution, we assign abstract energy units to arithmetic operations. Integer operations take \n37 units and .oating point operations take 40 units; of each of these, 22 units are consumed by the instruction \nfetch and decode stage and may not be reduced by approximation strategies. These estimations are based \non three studies of architectural power consumption [5, 20, 27]. We calculate energy savings in instruction \nexecution by scaling the non-fetch, non-decode component of integer and .oating-point instructions. We \nassume that SRAM storage and instructions that access it account for approximately 35% of the microarchitecture \ns power consumption; instruction execution logic consumes the remainder. To compute the total CPU power \nsavings, then, we scale the savings from SRAM storage by 0.35 and the instruction power savings, described \nabove, by 0.65. Finally, we add the savings from DRAM storage to get an energy number for the entire \nprocessor/memory system. For this, we consider a server-like setting, where DRAM accounts for 45% of \nthe power and CPU 55% [12]. Note that in a mobile setting, memory consumes only 25% of power so power \nsavings in the CPU will be more important [6]. 6. Results We evaluate EnerJ by annotating a variety of \nexisting Java programs. Table 3 describes the applications we used; they have been selected to be relevant \nin both mobile and server settings. Applications. We evaluate the FPU-heavy kernels of the SciMark2 benchmark \nsuite to re.ect scienti.c workloads.4 ZXing is a bar code reader library targeted for mobile devices \nbased on the Android op\u00aderating system.5 Our workload decodes QR Code two-dimensional bar code images. \njMonkeyEngine is a 2D and 3D game engine for both desktop and mobile environments.6 We run a workload \nthat consists of many 3D triangle intersection problems, an algorithm frequently used for collision detection \nin games. ImageJ is an image-manipulation program; our workload exe\u00adcutes a .ood .ll operation.7 This \nworkload was selected as repre\u00adsentative of error-resilient algorithms with primarily integer rather \nthan .oating point data. Because the code already includes exten\u00adsive safety precautions such as bounds \nchecking, our annotation for ImageJ is extremely aggressive: even pixel coordinates are marked as approximate. \nRaytracer is a simple 3D renderer; our workload executes ray plane intersection on a simple scene.8 Annotation \napproach. We annotated each application manually. While many possible annotations exist for a given program, \nwe attempted to strike a balance between reliability and energy savings. As a rule, however, we attempted \nto annotate the programs in a way that never causes them to crash (or throw an unhandled exception); \nit is important to show that EnerJ allows programmers to write approximate programs that never fail catastrophically. \nIn our experiments, each benchmark produces an output on every run. This is in contrast to approximation \ntechniques that do not attempt to prevent crashes [22, 23, 35]. Naturally, we focused our effort on code \nwhere most of the time is spent. 4 SciMark2: http://math.nist.gov/scimark2/ 5 ZXing: http://code.google.com/p/zxing/ \n 6 jMonkeyEngine: http://www.jmonkeyengine.com/ 7 ImageJ: http://rsbweb.nih.gov/ij/ 8 Raytracer: http://www.planet-source-code.com/vb/scripts/ \n ShowCode.asp?txtCodeId=5590&#38;lngWId=2  Lines Proportion Total Annotated Endorse- Application Description \nError metric of code FP decls. decls. ments FFT Mean entry difference 168 38.2% 85 33% 2 SOR MonteCarlo \nSparseMatMult Scienti.c kernels from the SciMark2 benchmark Mean entry difference Normalized difference \nMean normalized difference 36 59 38 55.2% 22.9% 39.7% 28 15 29 25% 20% 14% 0 1 0 LU Mean entry difference \n283 31.4% 150 23% 3 ZXing Smartphone bar code decoder 1 if incorrect, 0 if correct 26171 1.7% 11506 4% \n247 jMonkeyEngine Mobile/desktop game engine Fraction of correct decisions 5962 44.3% 2104 19% 63 normalized \nto 0.5 ImageJ Raster image manipulation Mean pixel difference 156 0.0% 118 34% 18 Raytracer 3D image \nrenderer Mean pixel difference 174 68.4% 92 33% 10 Table 3. Applications used in our evaluation, application-speci.c \nmetrics for quality of service, and metrics of annotation density. Proportion FP indicates the percentage \nof dynamic arithmetic instructions observed that were .oating-point (as opposed to integer) operations. \nDRAM SRAM Integer FPDRAM storage Integer operations 100% normalized total energy 0% B 1 2 3 B 1 2 3 \nB 1 2 3 B 1 2 3 B 1 2 3 B 1 2 3 B 1 2 3 B 1 2 3 B 1 2 3 SRAM storage FP operations 1.0  80% fraction \napproximate 0.8 60% 0.6 40% 0.4 20% 0.2 0.0 Figure 3. Proportion of approximate storage and computation \nin each benchmark. For storage (SRAM and DRAM) measurements, the bars show the fraction of byte-seconds \nused in storing approxi\u00admate data. For functional unit operations, we show the fraction of dynamic operations \nthat were executed approximately. Three of the authors ported the applications used in our eval\u00aduation. \nIn every case, we were unfamiliar with the codebase be\u00adforehand, so our annotations did not depend on \nextensive domain knowledge. The annotations were not labor intensive. QoS metrics. For each application, \nwe measure the degradation in output quality of approximate executions with respect to the precise executions. \nTo do so, we de.ne application-speci.c quality of service (QoS) metrics. De.ning our own ad-hoc QoS metrics \nis necessary to compare output degradation across applications. A number of similar studies of application-level \ntolerance to transient faults have also taken this approach [3, 8, 19, 21, 25, 35]. The third column \nin Table 3 shows our metric for each application. Output error ranges from 0 (indicating output identical \nto the precise version) to 1 (indicating completely meaningless output). For applications that produce \nlists of numbers (e.g., SparseMatMult s output matrix), we compute the error as the mean entry-wise difference \nbetween the pristine output and the degraded output. Each numerical difference is limited by 1, so if \nan entry in the output is NaN, that entry contributes an error of 1. For benchmarks where the output \nis not numeric (i.e., ZXing, which outputs a string), the error is 0 when the output is correct and 1 \notherwise. 6.1 Energy Savings Figure 3 divides the execution of each benchmark into DRAM storage, SRAM \nstorage, integer operations, and FP operations and Figure 4. Estimated CPU/memory system energy consumed \nfor each benchmark. The bar labeled B represents the baseline value: the energy consumption for the program \nrunning without approximation. The numbered bars correspond to the Mild, Medium, and Aggressive con.gurations \nin Table 2. shows what fraction of each was approximated. For many of the FP-centric applications we \nsimulated, including the jMonkeyEngine and Raytracer as well as most of the SciMark applications, nearly \nall of the .oating point operations were approximate. This re.ects the inherent imprecision of FP representations; \nmany FP-dominated algorithms are inherently resilient to rounding effects. The same applications typically \nexhibit very little or no approximate integer operations. The frequency of loop induction variable increments \nand other precise control-.ow code limits our ability to approximate integer computation. ImageJ is the \nonly exception with a signi.cant fraction of integer approximation; this is because it uses integers \nto represent pixel values, which are amenable to approximation. DRAM and SRAM approximation is measured \nin byte-seconds. The data shows that both storage types are frequently used in approximate mode. Many \napplications have DRAM approximation rates of 80% or higher; it is common to store large data structures \n(often arrays) that can tolerate approximation. MonteCarlo and jMonkeyEngine, in contrast, have very \nlittle approximate DRAM data; this is because both applications keep their principal data in local variables \n(i.e., on the stack). The results depicted assume approximation at the granularity of a 64-byte cache \nline. As Section 4.1 discusses, this reduces the number of object .elds that can be stored approximately. \nThe impact of this constraint on our results is small, in part because much of the approximate data is \nin large arrays. Finer-grain approximate memory could yield a higher proportion of approximate storage. \n Mild Medium Aggressive 1.0 0.8 0.6 0.4 0.2 amount of random pixel noise increases with the aggressiveness \nof approximation. Under the Mild con.guration, it is dif.cult to distinguish the approximated image from \nthe precise one. We also measured the relative impact of various approximation strategies by running \nour benchmark suite with each optimization enabled in isolation. DRAM errors have a nearly negligible \nimpact on application output; .oating-point bit width reduction similarly results in at most 12% QoS \nloss in the Aggressive con.guration. SRAM write errors are much more detrimental to output quality than \nread upsets. Functional unit voltage reduction had the greatest impact on correctness. We considered \nthree possibilities for error output error modes in functional units: the output has a single bit .ip; \nthe last value computed is returned; or a random value is returned. The former two models resulted in \nsigni.cantly less QoS loss than the random-value model (25% vs. 40%). However, we consider the random-value \nmodel to be the most realistic, so we use it for the results shown in Figure 5. Figure 5. Output error \nfor three different levels of approximation varied together. Each bar represents the mean error over \n20 runs. To give a sense of the energy savings afforded by our proposed approximation strategies, we \ntranslate the rates of approximation de\u00adpicted above into an estimated energy consumption. Figure 4 shows \nthe estimated energy consumption for each benchmark running on approximate hardware relative to fully \nprecise execution. The energy calculation is based on the model described in Section 5.4. These simulations \napply all of the approximation strategies described in Section 4.2 simultaneously at their three levels \nof aggressiveness. As expected, the total energy saved increases both with the amount of approximation \nin the application (depicted in Figure 3) and with the aggressiveness of approximation used. Overall, \nwe observe energy savings from 9% (SOR in the Mild con.guration) to 48% (Raytracer in the Aggressive \ncon.guration). The three levels of approximation do not vary greatly in the amount of energy saved the \nthree con.gurations yield average energy savings of 19%, 24%, and 26% respectively. The majority of the \nenergy savings come from the transition from zero approximation to mild approximation. As discussed in \nthe next section, the least aggressive con.guration results in very small losses in output .delity across \nall applications studied. The .fth column of Table 3 shows the proportion of .oating point arithmetic \nin each application. In general, applications with principally integer computation (e.g., ZXing and ImageJ) \nexhibit less opportunity for approximation than do .oating-point applica\u00adtions (e.g., Raytracer). Not \nonly do .oating-point instructions offer more energy savings potential in our model, but applications \nthat use them are typically resilient to their inherent imprecision.  6.2 Quality-of-Service Tradeoff \nFigure 5 presents the sensitivity of each annotated application to the full suite of approximations explored. \nThis quality-of-service reduction is the tradeoff for the energy savings shown in Figure 4. While most \napplications show negligible error for the Mild level of approximation, applications sensitivity to error \nvaries greatly for the Medium and Aggressive con.gurations. Notably, MonteCarlo, SparseMatMult, ImageJ, \nand Raytracer exhibit very little output degradation under any con.guration whereas FFT and SOR lose \nsigni.cant output .delity even under the Medium con.guration. This variation suggests that an approximate \nexecution substrate for EnerJ could bene.t from tuning to the characteristics of each application, either \nof.ine via pro.ling or online via continuous QoS measurement as in Green [3]. However, even the conservative \nMild con.guration offers signi.cant energy savings. Qualitatively, the approximated applications exhibit \ngradual degradation of perceptible output quality. For instance, Raytracer always outputs an image resembling \nits precise output, but the  6.3 Annotation Effort Table 3 lists the number of quali.ers and endorsements \nused in our annotations. Only a fraction of the types in each program must be annotated: at most 34% \nof the possible annotation sites are used. Note that most of the applications are short programs implementing \na single algorithm (the table shows the lines of code in each program). Our largest application, ZXing, \nhas about 26,000 lines of code and only 4% of its declarations are annotated. These rates suggest that \nthe principal data amenable to approximation is concentrated in a small portion of the code, even though \napproximate data typically dominates the program s dynamic behavior. Endorsements are also rare, even \nthough our system requires one for every approximate condition value. The outlier is ZXing, which exhibits \na higher number of endorsements due to its frequency of approximate conditions. This is because ZXing \ns control .ow frequently depends on whether a particular pixel is black. Qualitatively, we found EnerJ \ns annotations easy to insert. The programmer can typically select a small set of data to approximate \nand then, guided by type checking errors, ascertain associated data that must also be marked as approximate. \nThe requirements that conditions and array indices be precise helped quickly distinguish data that was \nlikely to be sensitive to error. In some cases, such as jMonkeyEngine and Raytracer, annotation was so \nstraightforward that it could have been largely automated: for certain methods, every .oat declaration \nwas replaced indiscriminately with an @Approx .oat declaration. Classes that closely represent data are \nperfect candidates for @Approximable annotations. For instance, ZXing contains BitArray and BitMatrix \nclasses that are thin wrappers over binary data. It is useful to have approximate bit matrices in some \nsettings (e.g., during image processing) but precise matrices in other settings (e.g., in checksum calculation). \nSimilarly, the jMonkeyEngine benchmark uses a Vector3f class for much of its computation, which we marked \nas approximable. In this setting, approximate vector declarations (@Approx Vector3f v) are syntactically \nidentical to approximate primitive-value declarations (@Approx int i). We found that the @Context annotation \nhelped us to approach program annotation incrementally. A commonly-used class that is a target for approximation \ncan be marked with @Context members instead of @Approx members. This way, all the clients of the class \ncontinue to see precise members and no additional annotation on them is immediately necessary. The programmer \ncan then update the clients individually to use the approximate version of the class rather than addressing \nthe whole program at once. An opportunity for algorithmic approximation also arose in ZXing. The BitArray \napproximable class contains a method isRange that takes two indices and determines whether all the bits \nbetween the two indices are set. We implemented an approximate version of the method that checks only \nsome of the bits in the range by skipping some loop iterations. We believe that application domain experts \nwould use algorithmic approximation more frequently.  In one case, we found it convenient to introduce \na slight change to increase the fault tolerance of code dealing with approximate data. ZXing has a principally \n.oating-point phase that performs an image perspective transform. If the transform tried to access a \ncoordinate outside of the image bounds, ZXing would catch the ArrayIndexOutOfBoundsException and print \na message saying that the image transform failed. We modi.ed the algorithm to silently return a white \npixel in this case. The result was that the image transform became more resilient to transient faults \nin the transformation coordinates. We marked these coordinates as approximate and then endorsed them \nat the point they are used as array indices. In no case, however, does an application as we annotated \nit do more computation than the pristine version. 7. Related Work Space constraints preclude a discussion \nof the vast body of compiler or hybrid hardware/software work to improve energy ef.ciency. Instead, we \nfocus on work we are aware of that exploits approximate computing to improve energy. Many studies have \nshown that a variety of applications have a high tolerance to transient faults [8, 19, 21, 22, 25, 35]. \nHowever, certain parts of programs are typically more fault-tolerant than others. Our work exploits this \nproperty by allowing the programmer to distinguish critical from non-critical computation. Our work at \nthe language level was in.uenced by previous work on techniques for trading off correctness for power \nsavings. Flikker [23] proposes a programming model for reducing the DRAM refresh rate on certain heap \ndata via low-level program annotations. Besides being limited to heap storage, Flikker does not provide \nany safety guarantees. Relax [9] is an architecture that exposes timing faults to software as opposed \nto providing error recovery automatically in hardware; its goal is to improve error tolerance with lower \npower by exploiting portions of code that are tolerant to error. While Relax focuses on error recovery \nand hardware design simplicity, EnerJ emphasizes energy-ef.ciency over error detectability and supports \na wider range of power-saving approximations. Moreover, Relax explores a code-centric approach, in which \nblocks of code are marked for failure and recovery while EnerJ employs data-centric type annotations. \nWork by Rinard et al. proposes approximate code transforma\u00adtions in the compiler [1, 25, 31]. Relatedly, \nEnerJ s support for algorithmic approximation, the ability to write an approximate im\u00adplementation and \na precise implementation of the same functionality, bears some similarity to the Green programming model \n[3]. How\u00ad ever, Green primarily concerns itself with online monitoring of application QoS; EnerJ s guarantees \nare entirely static. Overall, En-erJ s type system makes approximation-based approaches to energy savings \ngeneral (it supports approximate operations, storage, and algorithms) and safe (it provides static isolation \nguarantees). Using types to achieve .ne-grained isolation of program com\u00adponents is inspired by work \non information .ow types for secure programming [26, 32]. That body of work also in.uenced the en\u00addorsement \nconstruct for explicitly violating non-interference. Work by Perry et al. also focuses on static veri.cation \nof fault tolerance [29, 30]. That work focuses on detection and recovery rather than exposing transient \nfaults. 8. Conclusion Approximate computing is a very promising way of saving energy in large classes \nof applications running on a wide range of systems, from embedded systems to mobile phones to servers. \nWe propose to use a type system based on information-.ow tracking ideas: vari\u00adables and objects can be \ndeclared as approximate or precise; approx\u00adimate data can be processed more cheaply and less reliably; \nand we can statically prove that approximate data does not unexpectedly affect the precise state of a \nprogram. Our type system provides a general way of using approximation: we can use approximate stor\u00adage \nby mapping data to cheaper memory, cache, and registers; we can use approximate operations by generating \ncode with cheaper, approximate instructions; and we can use method overloading and class parameterization \nto enable algorithmic approximation. We implement our type system on top of Java and experiment with \nseveral applications, from scienti.c computing to image pro\u00adcessing to games. Our results show that annotations \nare easy to insert: only a fraction of declarations must be annotated and en\u00addorsements are rare. Once \na program is annotated for approximation, the runtime system or architecture can choose several approximate \nexecution techniques. Our hardware-based model shows potential energy savings in the 10% to 50% range. \nAcknowledgments We would like to thank the anonymous reviewers for their valuable comments. We also thank \nthe members of the Sampa group for their feedback on the manuscript. This work was supported in part \nby NSF grant CCF-1016495, NSF CAREER REU grant CCF-0846004, an NSF Graduate Fellowship and a Microsoft \nResearch Faculty Fellowship. References [1] A. Agarwal, M. Rinard, S. Sidiroglou, S. Misailovic, and \nH. Hoff\u00admann. Using code perforation to improve performance, reduce energy consumption, and respond to \nfailures. Technical report, MIT, 2009. [2] A. Askarov and A. C. Myers. A semantic framework for declassi.cation \nand endorsement. In ESOP, 2010. [3] W. Baek and T. M. Chilimbi. Green: a framework for supporting energy-conscious \nprogramming using controlled approximation. In PLDI, 2010. [4] V. Bhalodia. SCALE DRAM subsystem power \nanalysis. Master s thesis, MIT, 2005. [5] D. Brooks, V. Tiwari, and M. Martonosi. Wattch: a framework \nfor architectural-level power analysis and optimizations. In ISCA, 2000. [6] A. Carroll and G. Heiser. \nAn analysis of power consumption in a smartphone. In USENIX, 2010. [7] A. Chlipala, L. Petersen, and \nR. Harper. Strict bidirectional type checking. In TLDI, 2005. [8] M. de Kruijf and K. Sankaralingam. \nExploring the synergy of emerging workloads and silicon reliability trends. In SELSE, 2009. [9] M. de \nKruijf, S. Nomura, and K. Sankaralingam. Relax: an architectural framework for software recovery of hardware \nfaults. In ISCA, 2010. [10] D. Ernst, N. S. Kim, S. Das, S. Pant, R. Rao, T. Pham, C. Ziesler, D. Blaauw, \nT. Austin, K. Flautner, and T. Mudge. Razor: a low-power pipeline based on circuit-level timing speculation. \nIn MICRO, 2003. [11] M. D. Ernst. Type Annotations speci.cation (JSR 308). http: //types.cs.washington.edu/jsr308/, \n2008. [12] X. Fan, W.-D. Weber, and L. A. Barroso. Power provisioning for a warehouse-sized computer. \nIn ISCA, 2007. [13] K. Flautner, N. S. Kim, S. Martin, D. Blaauw, and T. Mudge. Drowsy caches: simple \ntechniques for reducing leakage power. In ISCA, 2002. [14] J. S. Foster, M. F\u00a8 ahndrich, and A. Aiken. \nA theory of type quali.ers. In PLDI, 1999. [15] M. Ghosh and H.-H. S. Lee. Smart refresh: An enhanced \nmemory controller design for reducing energy in conventional and 3D die\u00adstacked DRAMs. In MICRO, 2007. \n [16] A. Igarashi, B. C. Pierce, and P. Wadler. Featherweight Java: a minimal core calculus for Java \nand GJ. TOPLAS, 23(3), 2001. [17] A. Kahng, S. Kang, R. Kumar, and J. Sartori. Designing a processor \nfrom the ground up to allow voltage/reliability tradeoffs. In HPCA, 2010. [18] A. Kumar. SRAM Leakage-Power \nOptimization Framework: a System Level Approach. PhD thesis, University of California at Berkeley, 2008. \n[19] L. Leem, H. Cho, J. Bau, Q. A. Jacobson, and S. Mitra. ERSA: error resilient system architecture \nfor probabilistic applications. In DATE, 2010. [20] S. Li, J. H. Ahn, R. Strong, J. Brockman, D. Tullsen, \nand N. Jouppi. McPAT: An integrated power, area, and timing modeling framework for multicore and manycore \narchitectures. In MICRO, 2009. [21] X. Li and D. Yeung. Exploiting soft computing for increased fault \ntolerance. In ASGI, 2006. [22] X. Li and D. Yeung. Application-level correctness and its impact on fault \ntolerance. In HPCA, 2007. [23] S. Liu, K. Pattabiraman, T. Moscibroda, and B. G. Zorn. Flikker: Saving \nrefresh-power in mobile devices through critical data partitioning. In ASPLOS, 2011. [24] A. Mahesri \nand V. Vardhan. Power consumption breakdown on a modern laptop. In PACS, 2004. [25] S. Misailovic, S. \nSidiroglou, H. Hoffman, and M. Rinard. Quality of service pro.ling. In ICSE, 2010. [26] A. C. Myers. \nJFlow: practical mostly-static information .ow control. In POPL, 1999. [27] K. Natarajan, H. Hanson, \nS. W. Keckler, C. R. Moore, and D. Burger. Microprocessor pipeline energy analysis. In ISLPED, 2003. \n[28] M. M. Papi, M. Ali, T. L. Correa Jr., J. H. Perkins, and M. D. Ernst. Practical pluggable types \nfor Java. In ISSTA, 2008. [29] F. Perry and D. Walker. Reasoning about control .ow in the presence of \ntransient faults. In SAS, 2008. [30] F. Perry, L. Mackey, G. A. Reis, J. Ligatti, D. I. August, and D. \nWalker. Fault-tolerant typed assembly language. In PLDI, 2007. [31] M. Rinard, H. Hoffmann, S. Misailovic, \nand S. Sidiroglou. Patterns and statistical analysis for understanding reduced resource computing. In \nOnward!, 2010. [32] A. Sabelfeld and A. C. Myers. Language-based information-.ow security. IEEE Journal \non Selected Areas in Communications, special issue on Formal Methods for Security, 21(1), 2003. [33] \nA. Sampson, W. Dietl, E. Fortuna, D. Gnanapragasam, L. Ceze, and D. Grossman. EnerJ: Approximate Data \nTypes for Safe and General Low-Power Computation Full Proofs. Technical Report UW-CSE\u00ad10-12-01, University \nof Washington, 2011. [34] J. Y. F. Tong, D. Nagle, and R. A. Rutenbar. Reducing power by optimizing the \nnecessary precision/range of .oating-point arithmetic. IEEE Trans. VLSI Syst., 8(3), 2000. [35] V. Wong \nand M. Horowitz. Soft error resilience of probabilistic inference applications. In SELSE, 2006.   \n \n\t\t\t", "proc_id": "1993498", "abstract": "<p>Energy is increasingly a first-order concern in computer systems. Exploiting energy-accuracy trade-offs is an attractive choice in applications that can tolerate inaccuracies. Recent work has explored exposing this trade-off in programming models. A key challenge, though, is how to <i>isolate parts of the program that must be precise from those that can be approximated</i> so that a program functions correctly even as quality of service degrades.</p> <p>We propose using type qualifiers to declare data that may be subject to approximate computation. Using these types, the system automatically maps approximate variables to low-power storage, uses low-power operations, and even applies more energy-efficient algorithms provided by the programmer. In addition, the system can statically guarantee isolation of the precise program component from the approximate component. This allows a programmer to control explicitly how information flows from approximate data to precise data. Importantly, employing static analysis eliminates the need for dynamic checks, further improving energy savings. As a proof of concept, we develop EnerJ, an extension to Java that adds approximate data types. We also propose a hardware architecture that offers explicit approximate storage and computation. We port several applications to EnerJ and show that our extensions are expressive and effective; a small number of annotations lead to significant potential energy savings (10%-50%) at very little accuracy cost.</p>", "authors": [{"name": "Adrian Sampson", "author_profile_id": "81470649493", "affiliation": "University of Washington, Seattle, WA, USA", "person_id": "P2690524", "email_address": "asampson@cs.washington.edu", "orcid_id": ""}, {"name": "Werner Dietl", "author_profile_id": "81100178705", "affiliation": "University of Washington, Seattle, WA, USA", "person_id": "P2690525", "email_address": "wmdietl@cs.washington.edu", "orcid_id": ""}, {"name": "Emily Fortuna", "author_profile_id": "81472645262", "affiliation": "University of Washington, Seattle, WA, USA", "person_id": "P2690526", "email_address": "fortuna@cs.washington.edu", "orcid_id": ""}, {"name": "Danushen Gnanapragasam", "author_profile_id": "81485657733", "affiliation": "University of Washington, Seattle, WA, USA", "person_id": "P2690527", "email_address": "nanabyte@cs.washington.edu", "orcid_id": ""}, {"name": "Luis Ceze", "author_profile_id": "81100112680", "affiliation": "University of Washington, Seattle, WA, USA", "person_id": "P2690528", "email_address": "luisceze@cs.washington.edu", "orcid_id": ""}, {"name": "Dan Grossman", "author_profile_id": "81405594870", "affiliation": "University of Washington, Seattle, WA, USA", "person_id": "P2690529", "email_address": "djg@cs.washington.edu", "orcid_id": ""}], "doi_number": "10.1145/1993498.1993518", "year": "2011", "article_id": "1993518", "conference": "PLDI", "title": "EnerJ: approximate data types for safe and general low-power computation", "url": "http://dl.acm.org/citation.cfm?id=1993518"}