{"article_publication_date": "06-04-2011", "fulltext": "\n kb-Anonymity: A Model for Anonymized Behavior-Preserving Test and Debugging Data Aditya Budi, David \nLo, Lingxiao Jiang, and Lucia School of Information Systems, Singapore Management University {adityabudi, \ndavidlo, lxjiang, lucia.2009}@smu.edu.sg Abstract It is often very expensive and practically infeasible \nto generate test cases that can exercise all possible program states in a program. This is especially \ntrue for a medium or large industrial system. In practice, industrial clients of the system often have \na set of input data collected either before the system is built or after the deploy\u00adment of a previous \nversion of the system. Such data are highly valu\u00adable as they represent the operations that matter in \na client s daily business and may be used to extensively test the system. How\u00adever, such data often carries \nsensitive information and cannot be released to third-party development houses. For example, a health\u00adcare \nprovider may have a set of patient records that are strictly con.dential and cannot be used by any third \nparty. Simply mask\u00ading sensitive values alone may not be suf.cient, as the correlation among .elds in \nthe data can reveal the masked information. Also, masked data may exhibit different behavior in the system \nand be\u00adcome less useful than the original data for testing and debugging. For the purpose of releasing \nprivate data for testing and debug\u00adging, this paper proposes the kb-anonymity model, which combines the \nk-anonymity model commonly used in the data mining and database areas with the concept of program behavior \npreservation. Like k-anonymity, kb-anonymity replaces some information in the original data to ensure \nprivacy preservation so that the replaced data can be released to third-party developers. Unlike k-anonymity, \nkb\u00adanonymity ensures that the replaced data exhibits the same kind of program behavior exhibited by the \noriginal data so that the replaced data may still be useful for the purposes of testing and debugging. \nWe also provide a concrete version of the model under three par\u00adticular con.gurations and have successfully \napplied our prototype implementation to three open source programs, demonstrating the utility and scalability \nof our prototype. Categories and Subject Descriptors D.2.5 [Testing and Debug\u00adging]: Symbolic Execution/Testing \ntools; H.2.8 [Database Appli\u00adcations]: Data Mining; K.4.1 [Public Policy Issues]: Privacy General Terms \nAlgorithms, Experimentation, Reliability, Secu\u00adrity Keywords k-anonymity, behavior preservation, privacy \npreserva\u00adtion, third-party testing and debugging, symbolic execution Permission to make digital or hard \ncopies of all or part of this work for personal or classroom use is granted without fee provided that \ncopies are not made or distributed for pro.t or commercial advantage and that copies bear this notice \nand the full citation on the .rst page. To copy otherwise, to republish, to post on servers or to redistribute \nto lists, requires prior speci.c permission and/or a fee. PLDI 11, June 4 8, 2011, San Jose, California, \nUSA. Copyright c &#38;#169; 2011 ACM 978-1-4503-0663-8/11/06. . . $10.00 1. Introduction It is common \nfor companies in healthcare, banking, and other in\u00addustries to employ third-party software houses to \ndevelop software systems for their day-to-day business. These software systems are used to manage and \nprocess various datasets containing person\u00adspeci.c information. Building perfect software for these industries \nin the .rst attempt is hard; extensive testing and debugging are needed to detect and .x bugs. From a \nsoftware developer s point of view, real data that would be processed by the software system in the .eld \nmay contain more relevant test cases and rare cases that are bug-revealing. Being able to use such real \ndata can be very helpful for testing and debugging. systems for their daily business, and helping developers \n(e.g.,pro\u00adviding real data for testing) aligns with their interest. However, much of the data is sensitive \nor con.dential. Such data cannot be released to third parties without proper anonymization or desensiti\u00adzation. \nFor example, data in a hospital could contain the list of dis\u00adeases each patient has; data in a bank \ncould contain all transaction records of a client. Sending these datasets directly to third-party software \ndevelopers could pose security risks and privacy concerns. Even though legal mechanisms, such as non-disclosure \nagreements (NDAs), can be applied to protect the data from leaking further, it can be very costly for \nthe data owners to recover from any damage caused by violations of NDAs. It would be much safer to avoid \nun\u00adnecessary releases of sensitive or con.dential data in the .rst place. One solution for protecting \nprivacy is to mask away (i.e.,re\u00adplace, or remove) identi.ers in all data points that can be used to \n(uniquely) identify an individual, such as names and full addresses, before releasing the data to a third \nparty. However, due to data spar\u00adsity, a quasi-identi.er,which is aset of .elds that can uniquely identify \na data point, may still exist in the dataset. For example, Golle and Sweeney found that about 63%-87% \nof the U.S. popu\u00adlation can be uniquely identi.ed by gender, 5-digit US ZIP code, and full date of birth \n[17, 37]. Simply masking away all possible quasi-identi.ers would result in much less useful data, and \nthus a more sophisticated masking scheme is needed. Another solution is to mask away sensitive or con.dential \nin\u00adformation in all data points so that no actual private information is seen even though each data point \nuniquely identi.es an individual. The main issue of this solution for the purposes of testing and de\u00adbugging \nis that particular sensitive information can be important for triggering and thus testing a particular \nfunctionality in a software system. For example, in a healthcare application, there might be code implementing \na special process for patients with lung cancer. Thus, we cannot simply mask away all possible sensitive \ninforma\u00adtion; a more sophisticated masking scheme is needed. In this work, we propose a model for anonymizing \nprivacy data that addresses the following challenges:  The resulting dataset should not leak individual-identifying \nin\u00adformation. The resulting dataset should still preserve the utility of the original dataset for the \npurposes of testing and debugging.  We address these challenges by performing selective data value replacement \nin the original dataset at a data owner s site. The value replacement will generate a new dataset satisfying \nthe following requirements. Each data point in the new dataset can still be used as a test case but cannot \nidentify any individual in the original dataset. Also, the behavior of a program on a new data point \nshould be the same as the behavior of the program on some original data point. This would allow failures \nexhibited by the original dataset to be exhibited by the new dataset, or the test coverage achieved by \nthe original dataset to be achieved by the new dataset. More speci.cally, our model and its implementation \nare mainly a combination and extension of two ideas: the k-anonymity model from the data mining and database \nresearch communities [7, 32, 38] that can provide guidance on choosing data .elds to mask, and concolic \nexecution [16, 36] that can guide the generation of new test cases based on known ones and make sure \nthe new test cases satisfy certain properties. Merging the two ideas in various con.gurations enables \nus to achieve both privacy and program behavior preservation in various degrees. We call our model kb\u00adanonymity1 \nto highlight that it can preserve behavior and satisfy requirements similar to k-anonymity. Note that \nanother important difference between k-anonymity and kb-anonymity is that the new data points generated \nby kb\u00adanonymity may not correctly re.ect the original data points, as fake values may be introduced and \nsome original data points may be lost. Certain statistics of the original data (e.g., the geographical \ndistribution of all persons contained in the original dataset or the percentage of persons having cancer) \ncan thus be distorted, render\u00ading the new dataset unsuitable for purposes (e.g., data mining and epidemiological \nstudies) other than testing and debugging. How\u00adever, we believe that maintaining statistics of the original \ndataset is not necessary for testing and debugging, as long as the new dataset can exhibit the same kinds \nof behavior as the original dataset. We present our privacy and program behavior preservation model in \nSection 3 and 4. We have built a prototype of the model on top of Java Path.nder (JPF) [39], JFuzz (an \nexten\u00adsion of JPF) [18], and an approximation algorithm for creating k-anonymized datasets [7]. The main \ncontributions of this work are as follows: 1. We propose a new problem of privacy preservation for testing \nand debugging. 2. We propose a new model for preserving both privacy and be\u00adhavior when generating and \nreleasing data for testing and de\u00adbugging, and analyze various con.gurations of the model. 3. We propose \nseveral algorithms to implement various con.gura\u00adtions of the model. 4. We empirically evaluate our \nsolution on several sliced real pro\u00adgrams and show the feasibility of our model and implementa\u00adtion in \ngenerating useful anonymized test and debugging data.  The outline of this paper is as follows. Section \n2 summarizes necessary concepts and de.nitions related to k-anonymity and pro\u00adgram behavior. We present \nour model and its privacy and behavior preservation properties in Section 3, and describe various con.gu\u00adrations \nof our model in Section 4. Section 5 elaborates our empiri\u00adcal evaluation. We discuss some further considerations \nand threats to validity in Section 6. Section 7 describes related work. Finally, we conclude with future \nwork in Section 8. 1 b stands for behavior  2. Preliminaries In this section, we introduce some concepts \nand de.nitions relevant  2.1 On Privacy Preservation DEFINITION 2.1 (Data). Each dataset is a set of \ndata points;each data point in a dataset is a tuple t of the same number of .elds: t = (f1,f2,...,fn). \nThe value of each .eld is from a domain speci.c to the .eld. We use t[i] to denote the value of the ith \n.eld in a tuple t, and t[i1,...,ij ] to denote the sequence of values from the ith 1 ,...,ithj .elds. \nGiven a dataset D, D[i] denotes the set of values from the ith .eld of all tuples in D, which can also \nbe viewed as a tuple when DEFINITION 2.2 (Raw Data). A raw dataset is a set of raw tuples; each raw \ntuple is a tuple whose .elds may contain person-speci.c values. Releasing raw tuples to any third party \nmay violate the privacy requirements of the data owner. EXAMPLE 1. Consider a raw dataset containing \nfour patient records, each of which has seven .elds (NID means national iden\u00adti.cation number which is \nabbreviated to 3 digits here): Name Bob Tom Bob Sue Name Patient Patient Patient Patient NID 254 284 \n893 283 NID * * * * Age 53 53 37 34 Address Doctor Disease Clementi Dr. Joe Cancer Clementi Dr. Joe Cancer \nJurong Dr. Anne Hypertension Jurong Dr. Jill Flu Some .elds in the raw dataset (e.g., NID) uniquely \nidentify an individual, and thus are referred to as identi.ers.Some sets of .elds can also uniquely identify \nan individual when used together (e.g., the set of .elds {name, age, gender,and address} in this example); \nthey are referred to as quasi-identi.ers.Some other .elds are usually not used to identify an individual, \nbut provide information about an individual (e.g., Disease); they are referred to as sensitive .elds. \nDEFINITION 2.3 ((In-)distinguishable Tuples). For two tuples t1 and t2, t1 is indistinguishable from \nt2 if for every identi.er or quasi-identi.er .eld f, t1[f]= t2[f]. We cannot release the raw dataset \nas it is, since each tuple can uniquely identify an individual through either the identi.ers or the quasi-identi.ers, \nleaking the patient s privacy. We transform the raw dataset into an anonymized dataset by replacing the \nvalues of some .elds in the raw tuples with some generic values or masking them away with asterisks. \nHere is an anonymized dataset for Ex\u00adample 1, with which it is impossible to pinpoint a person having \na particular disease when the receivers of the anonymized dataset have no other knowledge about the raw \ndataset. Notice that the val\u00adues of the identi.er .eld (NID) and the quasi-identi.er .elds (name, age, \ngender,and address) of each patient are equal to those of some other patient. Age 53 53 30 39 30 39 Gender \nMale Male Male Female Gender Male Male Address Doctor Disease Clementi Dr. Joe Cancer A well-known privacy \nprotection model, k-anonymity [32, 38], DEFINITION 2.4 (k-Anonymity). A dataset K is said to satisfy \nk\u00adanonymity if each tuple in K is indistinguishable from at least k 1 other tuples in K.  1: void processAPatient \n(int patient_id, int disease_id, int age) { 2: switch ( disease_id ) { 3: case 1: // Cancer 4: if (age \n>= 60) { 5: if (patient_id <= 1000) // VIP 6: treatment(\"Premium intensive cancer\"); 7: else 8: treatment(\"Intensive \ncancer\" ); 9: } else { 10: if (patient_id <= 1000) 11: treatment(\"Premium standard cancer\"); 12: else \n13: treatment(\"Standard cancer\"); 14: } 15: ... Figure 1. An example to illustrate path conditions The \nabove example dataset is obviously 2-anonymized with respect to the .rst .ve .elds. If we also consider \nDoctor as part of the quasi-identi.er, only the .rst two anonymized tuples satisfy 2-anonymity. Without \nloss of generality, this paper considers the set of all .elds in a dataset as one quasi-identi.er for \nsimpler exposition, unless some .elds are explicitly identi.ed as sensitive. Thus, we can simply denote \ntwo indistinguishable tuples t1 and t2 as t1=t2. DEFINITION 2.5 (Value Replacement Function). Given a \ndataset R with tuples having n .elds, a function F : R . RU ,where RU denotes the set of all possible \ntuples with n .elds, is a value re\u00adplacement function for R,if . t.R, . i.{1,...,n}, F (t)[i] is The \nnumber of value replacements induced by F for R is n I( F (t)[i] =t[i]),where I(\u00b7) is an indicator function, \nwhich t.Ri=1 returns 1 if its parameter is true, and 0 otherwise. DEFINITION 2.6 (Minimal k-Anonymity). \nGiven a raw dataset R and a k-anonymized dataset K, K is said to be minimal k\u00adanonymized if there exists \na value replacement function from R to K that requires the fewest number of value replacements among \nall possible replacement functions from R to any k-anonymized dataset. Constructing minimal k-anonymized \ndatasets for a given raw dataset has been proved to be NP-complete [7], but there exist algorithms that \ncan approximate minimal solutions in quadratic time [7].  2.2 On Program Behavior For software testing \nand debugging, we are also concerned with concepts related to program states, program execution paths, \npro\u00adgram inputs and outputs, etc. DEFINITION 2.7 (Program State). A program state corresponds to the \ncomplete status of a program at a point at runtime, which can include the values of all program variables, \nthe location of the program counter, the environment on which the program relies (e.g., the .le system \nstatus), etc. An execution of a program can be viewed as a sequence of program state transitions. DEFINITION \n2.8 (Path Condition). A path condition for an exe\u00adcution through a control .ow path in a program is the \nconjunction of all the conditionals along the path. Example. Consider the program shown in Figure 1. \nFor the input arguments: patient id==10, disease id==1,and age==65,the path condition is: disease id==1 \n. age;60 . patient id.1000. PROPERTY 1. For each program execution, there is only one path condition. \nTwo executions following the same control .ow paths have equivalent path conditions, although the executions \nmay fol\u00adlow different sequences of program state transitions. Last but not least, software testing and \ndebugging is often con\u00adcerned with reproducible bug reports and test cases, which requires the concept \nof equivalent program behavior. There exist various de.nitions for this concept. In this paper, we de.ne \nequivalence based mainly on path conditions: DEFINITION 2.9 (Behavior Equivalence). Two program execu\u00adtions \nare said to have equivalent behavior if their corresponding path conditions are equivalent (i.e., they \nfollow the same control .ow paths).  3. Privacy and Behavior Preservation Model In this section, we \naim to establish a formal privacy preservation model, kb-anonymity, that can guide the construction and \nevalu\u00adation of algorithms and tools that generate datasets suitable to be released to third parties for \ntesting and debugging purposes. 3.1 Objectives There are two dimensions that we need to consider in this \nwork: Privacy Preservation. We assume that the raw data points contain person-speci.c information, and \nthe objective of our model is to ensure that the identity of every person whose information is contained \nin the raw dataset cannot be revealed in the released dataset. Behavior Preservation. The other objective \nof our model is to en\u00ad sure that the program behavior exhibited by the raw dataset can be reproduced \nby the released dataset to a certain extent without compromising privacy. How program behavior is de.ned \ncan vary in different con.gurations of the model. The above two goals are inversely related. The more \nprivacy is desired, the more values in the raw dataset may need replacement, and the harder it is to \npreserve program behavior. Similarly, the more behavior preservation is desired, the harder it is to \npreserve privacy. In the worst case, for a particular level of behavior preser\u00advation, it may not be \npossible to preserve any privacy. We elaborate on our privacy and behavior model in the following subsections. \n 3.2 Privacy Preservation k-anonymity [32, 38], summarized in Section 2, is a well-known privacy protection \nmodel that requires every released data point to be indistinguishable from at least k 1 other released \ndata points with respect to all identi.er and quasi-identi.er .elds. Simply conforming to the k-anonymity \nmodel does not .t our needs because of the following issues: The k-anonymity model allows duplicated \ndata points in the dataset to be released, which may not be meaningful for testing and debugging due \nto redundant test cases;  The k-anonymity model often implies that certain values in raw tuples would \nbe replaced with a more generic value (e.g., replacing California, USA with USA)or an asterisk (*,which \nmeans Don t tell ). This may lead to unusable test cases since testing and debugging often require concrete \ndata values.2  2 A concrete data value refers to any value in the domain of program inputs. In the above \nexample, if the program accepts any location as an input, then both California, USA and USA are concrete, \nbut * is not; if the program requires a state name as part of its inputs, then only California, USA is \nconcrete.    R1: All values in a released dataset are concrete so that each data point can be directly \nused to execute programs; R2: All released tuples are distinguishable from each other so as to reduce \nredundant test cases; R3: For each raw tuple and its corresponding released tuple gen\u00aderated by applying \nthe value replacement function on the raw tuple, there exist at least k 1 other raw tuples that are mapped \nby the same function to a tuple indistinguishable from this re\u00adleased tuple. Intuitively, kb-anonymity \naims to provide a similar guarantee as k-anonymity (R3), but avoid generic values (R1) and dupli\u00adcate \ndata points (R2). However, note that kb-anonymity is differ\u00adent from k-anonymity. k-anonymity considers \nk indistinguishable tuples within the released dataset while kb-anonymity considers k indistinguishable \ntuples with respect to replacements of the raw dataset. If the number of raw tuples is m, the number \nof the re\u00adleased tuples will be at most Lm/kJ, while k-anonymity will still have m tuples. The following \nTheorem 1 states that the R2 and R3 requirements can be easily achieved by applying k-anonymity and suppressing \nindistinguishable tuples. 3.2.1 R2+ R3 = k-Anonymity Modulo Uniqueness LEMMA 1. Given a raw dataset R \nand its k-anonymized version K, K satis.es R3. PROOF. Let the value replacement function used to generate \nthe tuples in K from the tuples in R be F : R . K. According ' to the de.nition of k-anonymity, for each \nt1 in K,there exist at least k 1 tuples in K, t2',...,t'k, such that .i.{2,...,k},t'1 = ' ti.Since F \nmaps each raw tuple in R to at most one tuple in K, there must exist at least k raw tuples t1,...,tk \nsuch that .i.{1,...,k},F (ti)= t'i'. By transitivity of indistinguishability, .i.{1,...,k},F (ti)= t1. \nTHEOREM 1. Given a raw dataset R and its k-anonymized version K, construct a new dataset K' as follows: \nfor each group of indistinguishable tuples in K, add exactly one tuple from the group into K'. Then, \nK' satis.es R2 and R3. PROOF. i) K' trivially satis.es R2 due to the removal of indis\u00adtinguishable tuples. \nii) For each tuple t' in K',there must exist at ' least k tuples in K that are indistinguishable from \nt' (otherwise, twould not appear in K'); thus R3 is true based on the same reason\u00ading as Lemma 1. Theorem \n1 provides us a base to build a dataset satisfying R2 R3. The other issue is to replace certain values \nand asterisks in K' to satisfy R1 while maintaining R2 and R3. Depending on which values to replace and \nhow to perform replacement, we have various choices corresponding to various levels of privacy preservation. \nThe following subsections introduce two options in our model. 3.2.2 No Field Repeat This subsection \nproposes the no field repeat option for replac\u00ading values from raw data points. Its intuition is to ensure \nevery value in the released dataset has not appeared in any raw data point. For example, if age==30 appears \nin the raw dataset, 30 would not be used for the age .eld in the released dataset. DEFINITION 3.1 (No \nField Repeat Option). Given a raw dataset R, a released version X of R satis.es the no field repeat option \nif X satis.es the following: i) X satis.es R1 R3; ii) . i. {1,...,n}, . t.R, . t'.X, t[i]= t'[i]. In \nsome cases, no field repeat is impossible. For example, when both male and female have appeared in the \ngender .eld in the raw dataset, we may have to use one of two values (unless the program could handle \na more generic gender value, such as unknown). Thus, we also need a less restrictive option. 3.2.3 No \nTuple Repeat This subsection proposes the no tuple repeat option. This op\u00ad tion ensures that every released \ntuple is distinguishable from every raw tuple, but allows them to share some .eld values. DEFINITION \n3.2 (No Tuple Repeat Option). Given a raw dataset R, a released version X of R satis.es the no tuple \nrepeat op\u00adtion if X satis.es the following: i) X satis.es R1 R3; ii) . t.R, ' . t'.X, t = t. Note that \nthe no field repeat option subsumes the no tuple repeat option, meaning that a released dataset satisfying \nno field repeat also satis.es no tuple repeat. In summary, we have the following four levels of privacy \npreser\u00advation: (i) None which imposes no restriction on released tuples, (ii) Standard k-Anonymity modulo \nuniqueness, (iii) No Tuple Repeat,and (iv) No Field Repeat.  3.3 Behavior Preservation The second \nobjective of our kb-anonymity model is to ensure the following: R4: For each released tuple b and each \nraw tuple t that is mapped to b, b and t must exhibit the same behavior when run on the subject program. \nAs mentioned in Section 2, various de.nitions of program be\u00adhavior and equivalence exist and affect our \nmodel that is mainly following four levels of behavior equivalence in this paper: None, Same Path, Same \nPath with Input Restrictions,and Same Program States. The lowest level, none, isusedasa baseline that \nallows arbi\u00adtrary program behavior to be exhibited by the released dataset and provides no guarantee \non behavior preservation. The second level, same path, requires each released data point to follow the \nsame execution path as the path followed by the raw data point mapped to it: R4-1: A released tuple b \nis path preserving for a raw tuple t mapped to it if b and t follow the same execution path in subject \nprograms. If every raw data point could be mapped to a released data point, the resulting dataset would \nhave the same path coverage (which is a commonly used test suf.ciency criterion) as the raw dataset. \nThe third level, same path with input restrictions, aims to consider more program behaviors beyond execution \npaths, such as particular input values. This is useful for cases when two program runs have different \nobservable effects even if they follow the same path. For example, the following Java function, which \naccepts the original bank account balance (orig)and the with\u00addrawal (amt), and returns the .nal balance, \nhas a functional error when a negative amt is fed to it: double reduceBalance(double orig, double amt) \n{ return orig -amt; }  The error can be observed (e.g., by an auditor) if the raw dataset contains the \ninput (5.0, -2.0). If the released dataset only contains (5.0, 2.0), the error cannot be observed even \nthough the same path is executed. Thus, the third level introduces additional restrictions on pro\u00adgram \ninputs, aiming to preserve more program behaviors. The re\u00adstrictions may be expressed in terms of various \nconstraints (e.g., amt==-2.0 for the above example). In this paper, we consider only one type of input \nrestrictions: some (arbitrary) .elds of the released data points shall preserve their original values. \nWe refer to this as input preservation. The framework allows future exten\u00adsions including the consideration \nof constraints provided by users and expressed as general .rst-order predicates. However, this paper \nutilizes a minimal k-anonymization algorithm to decide the input preservation constraints (cf. Section \n4.4) without the need for user\u00adspeci.ed constraints. Even though the minimal k-anonymization algorithm \ncannot guarantee to retain all desired values (e.g., 2.0 in the above example), we still have the following \nbene.ts: (1) the total number of replaced values in the released dataset can be mini\u00admized so that more \nprogram behaviors may hopefully be preserved; (2) the need for user-speci.ed constraints is avoided so \nthat there is no risk of leaking privacy data by incorrectly speci.ed constraints. R4-2: A released tuple \nb is path and input preserving for a raw tuple t if b is path preserving for t and it satis.es the given \ninput constraints. The fourth level, same program states,is an extreme level used as a reference point \nwhere the sequence of program states exhibited by each release tuple and the raw tuple mapped to it should \nbe the same. The following subsection discusses the various combinations of privacy and behavior preservation \nlevels in our model. 3.4 Combining Privacy and Behavior Preservation High Privacy Preservation No Field \nRepeat No Tuple Repeat Standard k-Anonymity None N/I N/I N/I N/I N/I None N/I Same N/I N/I Same Path \nN/I Same High Low    Path with Input Program Behavior Restrictions States Preservation Figure 2. \nPrivacy vs. behavior preservation Some combinations marked with X are impossible to achieve. Many others \nmarked with N/I are not interesting for the purpose of preserving privacy and behavior. Those marked \nwith a tick are of interest and possible to achieve. The following paragraphs describe the combinations \nin more details.3 PROPERTY 2. The privacy preservation level none is not interest\u00ading. PROPERTY 3. Standard \nk-anonymity alone is not interesting since it cannot satisfy R1 required by our kb-anonymity model. PROPERTY \n4. The behavior preservation level none is not inter\u00adesting for the purposes of software testing and \ndebugging. PROPERTY 5. It is impossible to pair same program states with any privacy preservation level \n(except none). EXPLANATION: Since program states include input values to a program, same program states \nimplies the same input values. Thus, the released tuple must be the same as a raw tuple, which is only \nallowed when the raw dataset is already k-anonymized or when there is no desire for privacy. PROPERTY \n6. It is impossible to pair same path with input restrictions with no field repeat. EXPLANATION: In this \npaper, we consider input restrictions to be constraints that require some (arbitrary) input .elds to \nhave the same values as found in raw tuples. Thus, by de.nition, datasets complying with same path with \ninput restrictions cannot also conform to no field repeat. With the combination of k-anonymity and behavior \npreserva\u00adtion, we have the following property stated in Theorem 2. This property can be used to ef.ciently \ncheck whether a dataset can be kb-anonymized (cf. Section 4.1). THEOREM 2. For each behavior exhibited \nby a released tuple sat\u00adisfying R1 R4, there must exist at least k raw tuples that exhibit the same behavior. \nPROOF. The requirement R1 ensures that a released tuple can be used to run a program, as all raw tuples \ncan. R3 ensures that at least k raw tuples map to each released tuple. R4 ensures that a raw tuple only \nmaps to a released tuple with the same behavior. Thus, it must be the case that for each released tuple, \nthere are at least k raw tuples that map to it and they all have the same behavior.4 In a nutshell, \nour kb-anonymity model requires R1 R4 alto\u00adgether, and there are three interesting con.gurations: (same \npath, no field repeat)  (same path, no tuple repeat)  (same path with input restriction, no tuple\u00adrepeat) \n  As shorthand notations, we refer to these as P-F, P-T,and I-T, respectively. The next section presents \nour realization of these con.gurations. In cases when some raw data points cannot be mapped to a released \ntuple, we simply output error messages.   4. Model Realization In this section, we describe the algorithms \nand tools used to realized the three con.gurations of our kb-anonymity model: P-T, P-F, and I-T. The \noverall framework of our realization is illustrated in Figure 3. 3 We do not consider cases where the \nraw dataset satis.es k-anonymity. These uninteresting cases happen in situations, such as when k=1, or \nwhen there is no identi.er or quasi-identi.er. 4 R2 is unnecessary for proving Theorem 2, but it eliminates \nindistinguish\u00adable tuples and thus may help to save the cost of testing and debugging.     4.1 Overall \nFramework All three con.gurations require path preservation, which means we need to collect the execution \npaths of all raw tuples. The Program Execution module takes raw tuples and executes a program with each \nof the tuples, then it collects the path conditions exercised by each execution. We assume that each \ntuple is processed by the program independently; we do not consider any dependency among program states \nor path conditions that may be introduced by multiple runs of the program with different tuples. Theoretically, \ntwo executions having the same path condition follow the same execution path. Thus, this module can group \nraw tuples based on the equivalence of their path conditions. At the end of this step, groups of size \nless than k are discarded due to Theorem 2. For each of the groups left, the k-Anonymization module may \nreplace some .eld values with asterisks and make sure that each tuple is indistinguishable from at least \nk 1 other tuples in the group. Then, it outputs a set of unique tuples a la Theorem 1. Next, the Constraint \nGeneration module takes the set of unique tuples from the k-Anonymization module and the path conditions \nfor every tuple associated with the unique tuple. Various constraints are then generated for each of \nthe unique tuple according to each of the three con.gurations. Last, the Constraint Solver takes the \nconstraints for each of the unique tuple and tries to generate one new tuple satisfying the constraints. \nIf the solver .nds a satisfying tuple, this tuple will be part of the released dataset. When the solver \ncannot .nd a satisfying tuple, our framework simply outputs error messages. Theorems 3, 4, and 5 state \nthat the datesets outputted by the algorithms satisfy kb-anonymity (R1 R4) and can be released for software \ntesting and debugging. Figure 3. Overall framework of our model realization The overall parameterized \nAlgorithm 1 realizes the three con\u00ad.gurations. At Lines 3 4, we obtain the path condition for every raw \ntuple. At Lines 2 7, we group the raw tuples into different buckets based on their path conditions. At \nLines 8 11, we remove buckets of size less than k because of Theorem 2. In an extreme case when all buckets \nare of size less than k, the released dataset will be empty. The remaining buckets are those that require \nvalue replacement satisfying the given con.guration. Then, Lines 14 and 15 run a k-anonymization algorithm \nfor every bucket and generate the anonymized version a la Theorem 1, and Lines 16 21 store the k-anonymized \nbuckets that satisfy certain conditions. Note that for con.guration I-T, we require Line 13 to invoke \nthe approximate minimal k-anonymization, and only store an anonymized tuple if it contains some concrete \nvalues. We can simply discard a tuple if it contains no concrete value (Lines 17 20) since it means no \nvalues in the corresponding raw tuples can remain and the I-T con.gura\u00adtion cannot be satis.ed for these \ntuples. Then, for each bucket that can be k-anonymized (Line 22), Lines 23 29 construct suf.ciently strong \nconstraints for the given con.guration, feed the conjunction Algorithm 1 A Realization of kb-Anonymity \nInput: R:Raw dataset k: Level of anonymization P : A subject program O: Con.guration option: P-T, P-F,or \nI-T. Output: RI: Anonymized dataset for release 1: RI .\u00d8 2: PCBuckets .\u00d8, which groups tuples based \non path conditions 3: For each t in R 4: Execute P with t and collect the path condition pc 5: If PCBuckets \ndoes not contains pc 6: PCBuckets . PCBuckets .{(pc, \u00d8)} 7: PCBuckets[pc] . PCBuckets[pc] . t 8: For \neach Bucket = (pc, B). PCBuckets 9: If |B| <k 10: PCBuckets . PCBuckets -{Bucket} 11: Output Error: unsatisfiable \ncase and continue 12: A .\u00d8, holding intermediate k-anonymized datasets 13: For each (pc, B). PCBuckets \n14: Invoke a k-anonymization algorithm on B, 15: and get its result BI with no duplicates a la Theorem \n1 16: For each tuple bI . BI 17: If O = I-T 18: If |bI| <=1 or no .eld in bI contain concrete values \n19: Output Error: unsatisfiable case 20: continue I 21: A . A (b,pc,B 22: For each (bI,pc, B). A // Construct \nconstraints for various con.gurations 23: If O = P-F 24: S . Invoke Algorithm 2 on R 25: Else If O = \nP-T 26: S . Invoke Algorithm 3 on B 27: Else If O = I-T 28: S . Invoke Algorithm 4 on (B, bI) 29: Else \nOutput Error: unimplemented option and continue 30: S . Conjunction of S and pc 31: Invoke a constraint \nsolver on S, and get its result r 32: If r is not an error 33: RI . RI .{r} RI 34: Return of the constructed \nconstraints and the path condition for this bucket (Line 30) into a constraint solver (Line 31), and \nadd a valid solution from the solver into the resulting dataset (Lines 32 33). We .nally output the dataset \nfor release (Line 34). We have implemented a prototype of this framework. This pro\u00ad totype uses Java \nPathFinder (JPF) [39] and jFuzz [18], one of JPF s extensions that supports the combination of concrete \nand symbolic executions of Java programs, to emulate executions of our subject programs and collect path \nconditions. We rely on JPF s internal canonical representation of path conditions and use string compar\u00ad \n have implemented an approximate algorithm that can construct k\u00adanonymized datasets [7]. Also, we have \nextended JPF to allow ma\u00adnipulation of the collected path conditions and generation of the desired constraints. \nFinally, we utilize the constraint solver used in JPF Choco [1] to generate new tuples from the constraints. \nFor now, we only handle constraints with integers and real numbers. In the future, with the coming support \nfor string constraints in JPF [2], we hope our framework can handle programs with strings easily. The \nfollowing subsections describe more details and properties for each of the three con.gurations in our \nmodel.  4.2 Same Path, No Field Repeat (P-F) To realize this con.guration option, we perform Algorithm \n2 within  Algorithm 2 Generation of Constraints for P-F Input: T : A set of (raw) tuples Output: S: \nA (conjunctive) set of constraints for P-F For each .eld i Construct its constraint variable vi For each \nt . T S . S .{vi = t[i]}Return S The realization of this con.guration basically generates new data values \ndifferent from all of the original ones (via Line 24 in Algorithm 1) to ensure no violation of privacy, \nand ensure preserva\u00ad is invoked on the whole raw dataset to ensure no field repeat for all tuples, which \nof course imposes more constraints and makes it harder for the constraint solver to .nd a tuple. Alternatively, \nwe could relax the no field repeat requirement and invoke Algorithm 2 on B instead (Line 24) without \ncompromising privacy. Note that there is no particular input restrictions or minimal anonymization requirements \nin this con.guration. It is not neces\u00adsary to run minimal k-anonymization algorithms (Line 14). We just \nneed to ensure that there are at least k tuples in a bucket B having the same path (Lines 8 11) via Theorem \n2 and simply treat all tu\u00adples in B as one equivalence partition in its anonymized version. Doing so \nmay only produce one new tuple for each B, but help to speed up the anonymization process. Also, it is \nnot necessary to re\u00admove duplicated, intermediate tuples (Line 15), although doing so may help to prevent \nredundant operations at Lines 22 33 and make the control .ows for different con.guration options simpler. \nTHEOREM 3. An output dataset from Algorithm 1 along with Al\u00adgorithm 2 satis.es kb-anonymity (R1 R4). \nProof Sketch: Each dataset generated at Line 14 satis.es R2 and R3 according to Theorem 1; Each tuple \ngenerated at Line 31 is obviously path-preserving (via Line 30) and satis.es R4 and R1 (all values concrete). \nAlso, the constraints imposed by P-F, P-T,or I-T ensure no tuples from the raw dataset will appear at \nLine 31. Thus, the resulting dataset R ' at Line 34 satis.es kb-anonymity. D 4.3 Same Path, No Tuple \nRepeat (P-T) To realize this con.guration, we perform Algorithm 3 within Al\u00adgorithm 1. Similar to the \nrealization for P-F, P-T generates new data values and tuples to ensure privacy, although its privacy \nre\u00adquirement is weaker than that of P-F. Some data values from the raw dataset may remain even though \nthere are no tuples in the in\u00adtersection between the raw and the released datasets. In particular, Algorithm \n3 in this paper only considers changing the .rst .eld of all tuples to satisfy no tuple repeat. Alternatively, \nwe could choose a random .eld of all tuples or some random .elds from each Also similar to P-F, it is \nnot necessary to run minimal k\u00adanonymization algorithms (Line 14), as there is no need to satisfy any \ninput constraint or maximize the number of released tuples. THEOREM 4. An output dataset from Algorithm \n1 along with Al\u00adgorithm 3 satis.es kb-anonymity. Proof Sketch: Similar to that of Theorem 3. D  Algorithm \n3 Generation of Constraints for P-T Input: T : A set of (raw) tuples Output: S: A (conjunctive) set of \nconstraints for P-T For the .rst .eld in T Construct its constraint variable v1 For .rst .eld in each \nt in T S . S .{v1 = t[1]}Return S 4.4 Same Path &#38; Some Input, No Tuple Repeat (I-T) To realize this \ncon.guration, we perform Algorithm 4 within Algo\u00adrithm 1. Different from the previous realizations, we \nneed to ensure that some arbitrary one or a few raw input values are preserved. To do this, we need to \nrun a minimal k-anonymization algorithm at Line 14 in Algorithm 1 to decide the minimal number of .eld \nval\u00adues that need to be masked away to realize k-anonymity. By doing this, we can retain the maximal \nnumber of raw concrete values in released tuples (i.e., input preservation) so as to preserve more pro\u00adgram \nbehaviors. The process has been proven to be NP-complete, thus we run a variant of k-anonymization [7] \nto achieve approx\u00adimate results. After applying the k-anonymization algorithm, we could simply output \nan error message (Lines 17 20) for tuples for can be preserved).  Algorithm 4 Generation of Constraints \nfor I-T Input: T : A set of (raw) tuples b: A tuple with generic values or * Output: S: A (conjunctive) \nset of constraints for I-T // The If-Else ensures No Tuple Repeat If b contains no generic values or \n* S . Invoke Algorithm 3 on T i . 1 Else For the .rst .eld i in b containing a generic value or * Construct \nits constraint variable vi For each t in T S . S .{vi = t[i]} // The following helps to ensure some .elds \nmaintain their values For each .eld j in b containing a concrete value c and j = i Construct its constraint \nvariable vj S . S .{vj = b[j]} Return S THEOREM 5. An output dataset from Algorithm 1 along with Al\u00adgorithm \n4 satis.es kb-anonymity. Proof Sketch: Similar to that of Theorem 3. D   5. Empirical Evaluation We \nevaluate the capability and scalability of the realization of our kb-anonymity model with three sample \nprograms: OpenHos\u00adpital [4], iTrust [3], and PDManager [5]. All experiments are per\u00adformed on an Intel \nXeon server with a 2.53GHz quad-core E5540 CPU and 24 GiB of RAM running 64-bit Windows Server 2008 R2 \nStandard. Algorithm 1 is implemented in Visual C#.Net, while Algorithms 2, 3 and 4 are implemented in \nJava within jFuzz [18]. 5.1 OpenHospital OpenHospital is an open source hospital management system. \nIt provides various functionalities including managing patient records, pregnancy management, disease \ninformation, drug con\u00adtrol, pharmacy management, etc. All patient records are stored in a backend database. \nWe convert a part of the program into an integer program that reads inputs from a .le, as our current \nimplementa\u00adtion based on jFuzz does not handle string constraints or database queries.  The part that \nwe convert, denoted as OHc, is comprised of three Java classes which validate a patient record before \nstoring to database. It validates private information of a patient, including .rst name, last name, age, \ngender, address, city, number of siblings, telephone number, birth date, blood type, mother s name, mother \ns deceased status, father s name, father s deceased status, insurance status, and whether parents live \ntogether. Many of the input .elds are of string type. We then manually convert them into integers based \non their value domains and change corresponding string operations into integer operations. Even though \nthe program is not big, it demonstrates that our prototype implementation is capable of anonymizing data \nfor test\u00ading and debugging, for a large set of inputs. The following example shows sample output for \na given set of raw tuples under the P-T con.guration. Example. We randomly create synthetic tuples as \na raw dataset and run our tool on OHc with this set. Table 1 shows the raw tuples as well as the results \nof our tool using P-T con.guration for k =2. There is an error message for the .fth tuple as it could \nnot satisfy our kb-anonymity requirements. This tuple should not then be released to third-party developers. \nNo Raw Data Point Released Tuple  Figure 4. OHc runtime: per tuple (left) &#38; all tuples (right) \n 5.2 iTrust iTrust is an open source medical application that enables patients to maintain their medical \nhistory, records, and communications with their doctors. All patient records are also stored in a back\u00adend \ndatabase. Similar to OpenHospital, we convert a part of the program into an integer program that reads \ninput from a .le. The part that we convert, denoted as iT rustc, is comprised of ten Java classes that \nvalidate the insurance record of a patient. It involves private and sensitive information including: \n1) .rst name, last name, email, address, city, state, postal code, insurance number, credit card type, \ncredit card number, and telephone number of the patient, 2) name, address, city, state, and postal code \nof the insurance company, and 3) name and telephone number of an emergency contact person. Example. Table \n2 shows a sample output from our tool for a set of synthetic integer tuples under the P-F con.guration \nand k =2. No Raw Data Point Released Tuple Table 2. Sample raw tuples and the corresponding output after \nanonymization for iTrust Scalability Evaluation. Similar to OHc, we stress test our tool with various \nnumbers of raw tuples, i.e., 2,000, 4,000, 6,000, 8,000 and 10,000, and evaluate the runtime for P-T \nand P-F con.gurations for k set to 2,and I-T con.gurations for k set to 2 and 5.The performance results \nshown in Figure 5 are similar to those of OHc. For iTrust, for each con.guration and number of raw tuples, \nall raw tuples were successfully anonymized. Figure 5. ITc runtime: per tuple (left) &#38; all tuples \n(right)  5.3 PDManager PDManager is an open source insurance agent management system that provides features \nto manage clients, contracts, and commis-Table 1. Sample raw tuples and the output tuples after anonymiza\u00adtion \nfor OpenHospital. The integers could be mapped to real values. For example, the 3rd .eld directly maps \nto age; The 10th .eld in the tuples corresponds to blood types: 0 means type O, 1 means A, 2 means B, \n3 means AB, and negative numbers mean unknown. Scalability Evaluation. The computational complexities \nof k-anon\u00adymization algorithms and constraint generation and solving are po\u00adtentially exponential to \nthe number of raw tuples. To investigate the scalability of our proposed approach, we stress test our \ntool by increasing the number of raw tuples to be anonymized. We ex\u00adperiment with 2,000, 4,000, 6,000, \n8,000, and 10,000 tuples, and I-T con.guration (see Section 4) which can take quadratic time. Furthermore, \nalmost all raw tuples were successfully anonymized. In our experiments, at most two tuples out of the \nthousands of raw tuples failed to be anonymized. This occurred when using I-T con\u00ad.guration with k set \nto 5, while the other con.gurations were only unable to anonymize at most one tuple.  sions. Similar \nto OpenHospital and iTrust, we also convert a part of the program into an integer program that reads \ninput from a .le. The part that we convert, denoted as PDMc, is comprised of eight Java classes that \nvalidate the record of an insurance client of an agent. PDMc takes in 13 input .elds representing insurance \ns name, agent s name, as well as the .rst name, surname, gender, address, city, state, postal code, telephone \nnumber, fax, mobile number, and email of each client. Example. Table 3 shows sample output of our tool \nfor a set of synthetic integer tuples under the I-T con.guration with k =2. Note that the 8th .elds of \nthe tuples are unchanged. No Raw Data Point Released Tuple Table 3. Sample raw tuples and the output \ntuples for PDManager Scalability Evaluation. Similar to OHc and iT rustc,we stress test our tool with \nvarious numbers of raw tuples and evaluate the runtime for P-T and P-F con.gurations for k set to 2,and \nI-T con.gurations for k set to 2 and 5. The results plotted in Figure 6 are similar to those of OHc and \niT rustc. In our experiments, at most 8 tuples failed to be anonymized for I-T and k =5,at most 4 for \nother con.gurations, and only 1 failure for all con.gurations with 2,000 raw tuples. Figure 6. PDMc \nruntime: per tuple (left) &#38; all tuples (right)  6. Discussion Other privacy preservation models. \nIn this paper, we build our kb-anonymity model mainly on top of k-anonymity. There are vari\u00adous other \nmodels proposed in the literature, such as l-diversity [25], m-invariance [41], and t-closeness [22]. \nWe leave the possible ex\u00adtensions to cover other privacy preservation models as future work. Handling \nmore complex programs. In this work, our implemen\u00adtation only handles programs involving integers and \nreal numbers, and cannot solve non-linear or string constraints. In the future, with the advances of \nsymbolic execution, in particular JPF [2], we hope to be able to handle programs with strings and evaluate \nour proto\u00adtype implementation on larger, more complex programs. Also, this paper assumes that each tuple \nis processed indepen\u00addently. Privacy-related programs, such as healthcare management systems, often deal \nwith individuals and thus only take one tuple as its input, and program states or path conditions for \none tuple are not dependent on others. However, there are indeed real-world appli\u00adcations that use multiple \nindividuals records together (e.g.,when analyzing a patient s family medical history), and our current \nap\u00adproach will treat those records as one combined tuple. If a piece of code involves multiple tuples \nin a batch mode (e.g., when iterat\u00ading over a set of patients and printing out each record), it is possible \nand interesting future work to explore automated program slicing techniques to extract the essential \ncode that deals with one patient only, and then investigate the applicability of our model. Attacks. \nWe have not considered attack models beyond the natural mappings that come with value replacement functions. \nWe have assumed attackers can only link a released tuple back to a raw tuple through the inverse of a \nvalue replacement function. Based on Theorem 2, reversing the function would give no less than k raw \ntuples and would not breach kb-anonymity. However, it is worthwhile to discuss other kinds of attacks. \nk-anonymity is proposed to address the linking attack [32, 38] against released datasets that only remove \nidenti.er .elds. The linking attack works as follows: Some publicly available data (e.g., the voter list \nof a particular state) contains real values for the identi.ers (e.g., a person s name) and the quasi-identi.ers \nthat are also contained in a released dataset; then, the values of the quasi\u00adidenti.ers in the released \ndataset and the publicly available data may be uniquely matched to recover the values of the identi.ers \nand the sensitive .elds of each person. k-anonymity addresses this attack by ensuring that at least k \nreleased tuples are indistinguish\u00adable from each other. kb-anonymity also addresses the attack by ensuring \nthat each released tuple is mapped to at least k raw tuples. Other attacks against k-anonymity, such \nas unsorted matching at\u00adtack, complementary release attack,and temporal attack, can also be addressed \nby minor modi.cations of how k-anonymity is ap\u00adplied [32, 38]. kb-anonymity can address these attacks \nin a similar way by treating all .elds together as one quasi-identi.er. k-anonymity, however, cannot \naddress other attacks, such as ho\u00admogeneity attack due to the lack of diversity among the values of sensitive \n.elds [25]. Consider the example 2-anonymized dataset in Section 2. One can identify that both Bob and \nTom have cancer just by knowing that their age is 53 and their records are in the dataset. As an advantage, \nour model can address this issue when applied with the no field repeat con.guration. In this con.guration, \nthe disease would be replaced by another value that does not ex\u00adist in the dataset. There are also other \nattacks against k-anonymity discussed in the literature [22, 41]. We will investigate the suscep\u00adtibility \nof our model to those attacks in future work. In addition, program versioning may also lead to a privacy \ncon\u00adcern: our current kb-anonymity model may generate different re\u00adleased datasets for various versions \nof the same program and at\u00adtackers may link these versions together to increase the probability of identifying \nan individual in the raw dataset. Similarly, program back doors may even be a bigger concern. Also, data \nowners may be tricked into applying our model to programs that violate our assumptions (e.g., a hacker \nmay construct a program whose execu\u00adtion is dependent on not only the current input tuple, but also pre\u00advious \nexecutions with different tuples), then the released data may be used to infer original data. These potential \nattacks mean that kb\u00adanonymity model needs to be enhanced or applied with appropriate policies to disable \nundesirable information linkage among various data sources. We leave this as future work. Data Distortion. \nAs noted in Section 1, kb-anonymity may gener\u00adate values that do not correctly re.ect raw data values. \nAlso, many raw tuples may be suppressed into one released tuple (a la Algo\u00adrithm 1), causing loss of \ninformation. For example, there may be only two released tuples for the example in Section 2, as shown \nin Table 4, and some sensitive but useful information (e.g.,the exis\u00adtence of hypertension) is lost. \n         Name NID Age Address Doctor Disease Joel 999 54 Bishan Dr. Joe Cancer Bar 888 32 Female \nChangi Dr. Anne Flu Table 4. Sample kb-anonymized tuples that lose information. Our algorithms may be \nadapted to remedy the issues for certain cases: Change Line 31 in Algorithm 1 to produce more than one \ntuple for each path condition so that the values in the released tuples may contain any desired information. \nHowever, whether generating additional released tuples is cost-effective or robust against known or unknown \nattacks is still a question for future investigation. As a result, kb-anonymized datasets are only suitable \nfor the purposes of testing and debugging, where preserving statistics of raw datasets is not a concern. \nThreats to Validity. To address threats to construct validity, we have evaluated our model and its realization \nboth qualitatively (by some examples) and quantitatively (by the stress tests for scalabil\u00adity). To reduce \nthreats to external validity on the generalizability of our results, we have evaluated our approach on \nthree different programs. There may still be threats to internal validity (e.g.,se\u00adlection bias) as we \nchoose and slice the three sample programs by ourselves in a similar way. To reduce the threats further, \nit is possi\u00adble to include more subject programs and realistic datasets into our evaluations , which \nwe leave as future work. In particular, we plan anonymize their data and obtain their feedback. In the \nfuture, we hope that there will be more interest in this area and a comprehen\u00adsive benchmark could be \nbuilt to evaluate similar methodologies and tools.  7. Related Work In this section, we describe some \nrelated threads of work on privacy preservation, symbolic execution, testing &#38; debugging, and legal \nissues in software engineering. Privacy Preservation. Motivated by many threats to privacy and issues \nraised from identity thefts, there have been various studies on privacy preservation. Samarati and Sweeney \npropose the concept of k-anonymity [32,38]. There have been various studies on applying and extending \ntheir work [6, 7, 10]. Aggarwal et al. [7] prove that .nding the minimum number of value substitutions \nto ensure k-anonymity is an NP-complete problem; they thus propose an approximation algorithm to ensure \napproximate k-anonymity in a dataset. We leverage their algorithm in the realization of our kb\u00adanonymity \nmodel (cf. Section 4). There are also other models for privacy preservation, including l-diversity by \nMachanavajjhala et al. [25], m-invariance by Xiao and Tao [41], and t-closeness by Li et al. [22]. In \nthis study, we only focus on k-anonymity as the model for privacy preservation. Information Flow Security. \nIn the areas of programming language and security research, many studies use variants of taint analysis \nand information .ow for protecting sensitive information [24, 26, 31]. There are also studies on removing \nsensitive information from program execution records, such as core dumps, stack frames, and pro.les [11, \n40]. Some recent studies focus on monitoring the .ow of sensitive information in and across applications, \nsuch as TaintDroid [14] and TaintEraser [45], so as to prevent misuse of users private information. Our \napproach is different from all these techniques that we do not allow sensitive information going into \na program in the .rst place, while they detect, block, and remove sensitive information from within programs. \nHowever, information .ow and taint anal\u00adysis may be helpful to scale up our approach to identify parts \nof program and program inputs that involve sensitive information. Symbolic Execution. Symbolic execution \n[21] is a program anal\u00adysis technique that tracks variable values symbolically. A common use of symbolic \nexecution is to compute and manipulate path con\u00additions associated with an execution, and when combined \nwith con\u00adstraint solvers, to guide program analysis, testing, etc. It has been widely used for various \nsoftware engineering tasks, including test\u00ading, debugging, program analysis, and model checking [9, 20, \n29, 34, 42]. Recent studies combine symbolic execution with concrete execution to improve its utility \nand scalability [12, 16, 36]. For Java PathFinder (JPF) [39], there is also a symbolic execu\u00adtion extension \n[8]. Jayaraman et al. [18] provide a concolic white\u00adbox fuzzer, jFuzz, on top of JPF that allows both \nsymbolic and concrete executions. We use jFuzz to collect the path condition corresponding to each test \ncase, which is similar to many existing studies. However, we utilize path conditions for a quite different \npurpose, which is to guide test data anonymization. Testing &#38; Debugging. There are numerous studies \non testing and debugging. Many test case generation techniques are based on sym\u00adbolic and concrete executions \n[12, 15, 36]. Test case prioritization and selection have received much research interest [30, 33]. This \nis also the case with automated fault localization [19, 23, 35, 43, 44]. In this work we propose a new \nline of research on data anonymiza\u00adtion for testing and debugging. This complements the other re\u00adsearch \ndirections on testing and debugging, although it shares some fundamental techniques with them. There \nmight be further interest\u00ading research issues raised by combining anonymization problems with test case \nprioritization, selection, and even fault localization. We leave this research direction as future work. \nLegal Issues in Software Engineering. Recently, there have been studies investigating legal issues in \nsoftware engineering [13, 27, 28]. Metayer et al. [27] propose a set of methods and tools to de.ne and \nestablish software liability. Di Penta et al. [28] investigate the evolution of software licensing. Cleland-Huang \net al. [13] propose a machine learning approach for tracing regulatory code to require\u00adments. Technically, \nthese studies are all concerned with software itself, while our study is concerned more about the data \nthat soft\u00adware executes with. However, our study on privacy issues could potentially impact legal aspects \nin software engineering.  8. Conclusion and Future Work In this paper, we propose a new problem of privacy \npreserving test\u00ading and debugging. We address the problem of lack of test cases on a developer s side \nby allowing sensitive yet available test cases to be shipped from software users and data owners to third-party \nsoft\u00adware vendors through anonymization. Anonymization by naively masking away identi.ers and sensitive \ninformation would not work well due to the issues with quasi-identi.ers and ineffectiveness of masked \ndata for testing and debugging. Our approach combines the concept of privacy preservation and program \nbehavior preservation in some interesting ways, and provides guidance on replacing pri\u00advate data values. \nWe build a framework on the top of k-anonymity and concolic execution and implement several con.gurations. \nOur empirical evaluations on three sliced real programs show the utility of our prototype on providing \neffective anonymization for testing and debugging purposes. Our approach would help users to con\u00advey \nmore testing and debugging information to software vendors without disclosing private information. In \nthe future, we plan to address other privacy preservation criteria aside from k-anonymity, incorporate \nfurther progress on symbolic execution that is able to handle strings and more complex data structures \nin programs, and carry out larger case studies.   Acknowledgements We would like to thank for valuable \nfeedback from the anonymous reviewers and our shepherd Michael Burke. We also thank Julia Lawall and \nZhendong Su for their useful comments. Their insight\u00adful advice helped to improve our paper.  References \n[1] Choco solver. http://www.emn.fr/z-info/choco-solver/. [2] Fujitsu develops technology to enhance \ncomprehensive testing of java programs. http://www.fujitsu.com/global/news/pr/ archives/month/2010/20100112-02.html. \n[3] iTrust. http://sourceforge.net/projects/itrust/. [4] Open hospital. http://sourceforge.net/projects/angal/. \n[5] PDmanager. http://sourceforge.net/projects/pdmanager/. [6] G. Aggarwal, T. Feder, K. Kenthapadi, \nS. Khuller, R. Panigrahy, D. Thomas, and A. Zhu. Achieving anonymity via clustering. In PODS, pages 153 \n162, 2006. [7] G. Aggarwal, T. Feder, K. Kenthapadi, R. Motwani, R. Panigrahy, D. Thomas, and A. Zhu. \nApproximation algorithms for k-anonymity. In Int. Conf. on Data Theory, 2005. [8] S. Anand, C. Pasareanu, \nand W. Visser. JPF-SE: A symbolic execution extenion to Java PathFinder. In TACAS, 2007. [9] S.Artzi, \nJ.Dolby, F.Tip,and M. Pistoia. Directed test generation for effective fault localization. In ISSTA, pages \n49 60, 2010. [10] L. Backstrom, C. Dwork, and J. Kleinberg. Wherefore art thou r3579x? Anonymized social \nnetworks, hidden patterns, and structural steganography. In WWW, pages 181 190, 2007. [11] P. Broadwell, \nM. Harren, and N. Sastry. Scrash: A system for gener\u00adating secure crash information. In 12th USENIX Security \nSymposium, pages 273 284, 2003. [12] C. Cadar, D. Dunbar, and D. R. Engler. KLEE: Unassisted and au\u00adtomatic \ngeneration of high-coverage tests for complex systems pro\u00adgrams. In OSDI, pages 209 224, 2008. [13] J. \nCleland-Huang, A. Czauderna, M. Gibiec, and J. Emenecker. A machine learning approach for tracing regulatory \ncodes to product speci.c requirements. In ICSE, pages 155 164, 2010. [14] W. Enck, P. Gilbert, B. gon \nChun, L. P. Cox, J. Jung, P. McDaniel, and A. N. Sheth. TaintDroid: An information-.ow tracking system \nfor realtime privacy monitoring on smartphones. In OSDI, 2010. [15] P. Godefroid, N. Klarlund, and K. \nSen. DART: Directed automated random testing. In PLDI, pages 213 223. ACM, 2005. [16] P. Godefroid, M. \nY. Levin, and D. A. Molnar. Automated whitebox fuzz testing. In NDSS, 2008. [17] P. Golle. Revisiting \nthe uniqueness of simple demographics in the US population. In 5th ACM Workshop on Privacy in Electronic \nSociety (WPES), pages 77 80, 2006. [18] K. Jayaraman, D. Harvison, V. Ganesh, and A. Kiezun. jFuzz: A \nconcolic tester for NASA Java. In NASA Formal Methods Workshop, 2009. [19] D. Jeffrey, N. Gupta, and \nR. Gupta. Fault localization using value replacement. In ISSTA, pages 167 178, 2008. [20] S. Khurshid, \nC. S. P.as.areanu, and W. Visser. Generalized symbolic execution for model checking and testing. In TACAS, \npages 553 568, 2003. [21] J. C. King. Symbolic execution and program testing. Commun. ACM, 19(7):385 \n394, 1976. [22] N. Li, T. Li, and S. Venkatasubramanian. t-closeness: Privacy beyond k-anonymity and \nl-diversity. In Int. Conf. Data Eng., 2007. [23] B. Liblit, A. Aiken, A. X. Zheng, and M. I. Jordan. \nBug isolation via remote program sampling. In PLDI, pages 141 154, June 2003. [24] V. B. Livshits, A. \nV. Nori, S. K. Rajamani, and A. Banerjee. Merlin: Speci.cation inference for explicit information .ow \nproblems. In PLDI, pages 75 86, 2009. [25] A. Machanavajjhala, D. Kifer, J. Gehrke, and M. Venkitasubrama\u00adniam. \nl-diversity: Privacy beyond k-anonymity. ACM Trans. Knowl. Discov. Data, 1(1), 2007. [26] S. McCamant \nand M. D. Ernst. Quantitative information .ow as network .ow capacity. In PLDI, pages 193 205, 2008. \n[27] D. L. M\u00b4etayer, M. Maarek, V. V. T. Tong, E. Mazza, M.-L. Potet, N. Craipeau, S. Fr\u00b4enot, and R. \nHardouin. Liability in software en\u00adgineering: Overview of the LISE approach and illustration on a case \nstudy. In ICSE, pages 135 144, 2010. [28] M. D. Penta, D. M. German, Y.-G. Gu\u00b4eh\u00b4eneuc, and G. Antoniol. \nAn exploratory study of the evolution of software licensing. In ICSE, pages 145 154, 2010. [29] S. Person, \nM. B. Dwyer, S. G. Elbaum, and C. S. Pasareanu. Differ\u00adential symbolic execution. In FSE, pages 226 237, \n2008. [30] G. Rothermel, R. H. Untch, C. Chu, and M. J. Harrold. Prioritizing test cases for regression \ntesting. In IEEE Trans. Software Eng., pages 929 948, 2001. [31] A. Sabelfeld and A. C. Myers. Language-based \ninformation-.ow se\u00adcurity. IEEE Journal on Selected Areas in Communications, 21(1):5 19, 2003. [32] P. \nSamarati. Protecting respondents identities in microdata release. In IEEE Transactions on Knowledge and \nData Engineering, 2001. [33] R. A. Santelices, P. K. Chittimalli, T. Apiwattanapong, A. Orso, and M. \nJ. Harrold. Test-suite augmentation for evolving software. In ASE, pages 218 227, 2008. [34] R. A. Santelices \nand M. Harrold. Exploiting program dependencies for scalable multiple-path symbolic execution. In ISSTA, \npages 195 206, 2010. [35] R. A. Santelices, J. A. Jones, Y. Yu, and M. J. Harrold. Lightweight fault-localization \nusing multiple coverage types. In ICSE, pages 56 66, 2009. [36] K. Sen, D. Marinov, and G. Agha. CUTE: \nA concolic unit testing engine for C. In FSE, pages 263 272, 2005. [37] L. Sweeney. Uniqueness of simple \ndemographics in the U.S. popu\u00adlation. Technical Report LIDAP-WP4, Carnegie Mellon University, School \nof Computer Science, Data Privacy Laboratory, 2000. [38] L. Sweeney. k-anonymity: A model for protecting \nprivacy. Interna\u00adtional Journal of Uncertainty, Fuzziness and Knowledge-Based Sys\u00adtems, 10:557 570, 2002. \n[39] W. Visser and P. Mehlitz. Model checking programs with Java PathFinder. In SPIN, http://babelfish.arc.nasa.gov/trac/ \njpf, 2005. [40] R. Wang, X. Wang, and Z. Li. Panalyst: Privacy-aware remote error analysis on commodity \nsoftware. In 17th USENIX Security Sympo\u00adsium, pages 291 306, 2008. [41] X. Xiao and Y. Tao. m-invariance: \nTowards privacy preserving re\u00adpublication of dynamic datasets. In SIGMOD, pages 689 700, 2007. [42] T. \nXie, D. Marinov, W. Schulte, and D. Notkin. Symstra: A framework for generating object-oriented unit \ntests using symbolic execution. In TACAS, pages 365 381, 2005. [43] A. Zeller. Isolating cause-effect \nchains from computer programs. In FSE, pages 1 10, 2002. [44] X. Zhang, N. Gupta, and R. Gupta. Locating \nfaults through automated predicate switching. In ICSE, pages 272 281, 2006. [45] D. Zhu, J. Jungy, D. \nSong, T. Kohnoz, and D. Wetherall. TaintEraser: Protecting sensitive data leaks using application-level \ntaint tracking. ACM SIGOPS Operating Systems Review, 45(1), 2011.  \n\t\t\t", "proc_id": "1993498", "abstract": "<p>It is often very expensive and practically infeasible to generate test cases that can exercise all possible program states in a program. This is especially true for a medium or large industrial system. In practice, industrial clients of the system often have a set of input data collected either before the system is built or after the deployment of a previous version of the system. Such data are highly valuable as they represent the operations that matter in a client's daily business and may be used to extensively test the system. However, such data often carries sensitive information and cannot be released to third-party development houses. For example, a healthcare provider may have a set of patient records that are strictly confidential and cannot be used by any third party. Simply masking sensitive values alone may not be sufficient, as the correlation among fields in the data can reveal the masked information. Also, masked data may exhibit different behavior in the system and become less useful than the original data for testing and debugging.</p> <p>For the purpose of releasing private data for testing and debugging, this paper proposes the <i>kb</i>-anonymity model, which combines the <i>k</i>-anonymity model commonly used in the data mining and database areas with the concept of program behavior preservation. Like <i>k</i>-anonymity, <i>kb</i>-anonymity replaces some information in the original data to ensure privacy preservation so that the replaced data can be released to third-party developers. Unlike <i>k</i>-anonymity, <i>kb</i>-anonymity ensures that the replaced data exhibits the same kind of program behavior exhibited by the original data so that the replaced data may still be useful for the purposes of testing and debugging. We also provide a concrete version of the model under three particular configurations and have successfully applied our prototype implementation to three open source programs, demonstrating the utility and scalability of our prototype.</p>", "authors": [{"name": "Aditya Budi", "author_profile_id": "81474694606", "affiliation": "Singapore Management University, Singapore, Singapore", "person_id": "P2690616", "email_address": "adityabudi@smu.edu.sg", "orcid_id": ""}, {"name": "David Lo", "author_profile_id": "81452603381", "affiliation": "Singapore Management University, Singapore, Singapore", "person_id": "P2690617", "email_address": "davidlo@smu.edu.sg", "orcid_id": ""}, {"name": "Lingxiao Jiang", "author_profile_id": "81330492573", "affiliation": "Singapore Management University, Singapore, Singapore", "person_id": "P2690618", "email_address": "lxjiang@smu.edu.sg", "orcid_id": ""}, {"name": "Lucia", "author_profile_id": "81474693917", "affiliation": "Singapore Management University, Singapore, Singapore", "person_id": "P2690619", "email_address": "lucia.2009@smu.edu.sg", "orcid_id": ""}], "doi_number": "10.1145/1993498.1993551", "year": "2011", "article_id": "1993551", "conference": "PLDI", "title": "<i>kb</i>-anonymity: a model for anonymized behaviour-preserving test and debugging data", "url": "http://dl.acm.org/citation.cfm?id=1993551"}