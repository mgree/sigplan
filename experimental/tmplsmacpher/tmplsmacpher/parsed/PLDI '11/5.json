{"article_publication_date": "06-04-2011", "fulltext": "\n Synthesis of Loop-free Programs Sumit Gulwani Susmit Jha * Ashish Tiwari Microsoft Research, Redmond \nUniversity of California, Berkeley SRI International, Menlo Park sumitg@microsoft.com jha@eecs.berkeley.edu \ntiwari@csl.sri.com Ramarathnam Venkatesan Microsoft Research, Redmond venkie@microsoft.com Abstract \nWe consider the problem of synthesizing loop-free programs that implement a desired functionality using \ncomponents from a given library. Speci.cations of the desired functionality and the library components \nare provided as logical relations between their respec\u00adtive input and output variables. The library components \ncan be used at most once, and hence the library is required to contain a reason\u00adable overapproximation \nof the multiset of the components required. We solve the above component-based synthesis problem using \na constraint-based approach that involves .rst generating a synthesis constraint, and then solving the \nconstraint. The synthesis constraint is a .rst-order .. logic formula whose size is quadratic in the \nnumber of components. We present a novel algorithm for solving such constraints. Our algorithm is based \non counterexample guided iterative synthesis paradigm and uses off-the-shelf SMT solvers. We present \nexperimental results that show that our tool Brahma can ef.ciently synthesize highly nontrivial 10-20 \nline loop-free bitvector programs. These programs represent a state space of ap\u00adproximately 2010 programs, \nand are beyond the reach of the other tools based on sketching and superoptimization. Categories and \nSubject Descriptors D.1.2 [Programming Tech\u00adniques]: Automatic Programming; I.2.2 [Arti.cial Intelligence]: \nProgram Synthesis General Terms Algorithms, Theory, Veri.cation Keywords Program Synthesis, Component-based \nSynthesis, SMT 1. Introduction Composition has played a key role in enabling con.gurable and scalable \ndesign and development of ef.cient hardware as well as software systems. Hardware developers .nd it useful \nto design spe\u00adcialized hardware using some base components, such as adders and * Work done while visiting \nSRI International. Research supported in part by NSF grants CNS-0720721, CSR-EHCS\u00ad0834810 and CSR-0917398. \nPermission to make digital or hard copies of all or part of this work for personal or classroom use is \ngranted without fee provided that copies are not made or distributed for pro.t or commercial advantage \nand that copies bear this notice and the full citation on the .rst page. To copy otherwise, to republish, \nto post on servers or to redistribute to lists, requires prior speci.c permission and/or a fee. PLDI \n11, June 4 8, 2011, San Jose, California, USA. Copyright c &#38;#169; 2011 ACM 978-1-4503-0663-8/11/06. \n. . $10.00 multiplexers, rather than having to design everything using univer\u00adsal gates at bit-level. \nSimilarly, software developers prefer to use li\u00adbrary features and frameworks. Composition has also played \na key role in enabling scalable veri.cation of systems that have been de\u00adsigned in a modular fashion. \nThis involves verifying speci.cations of the base/constituent components in isolation, and then assum\u00ading \nthese speci.cations to verify speci.cation of the higher-level system made up of these components. In \nthis paper, we push the above-mentioned applications of com\u00adposition to another dimension, that of synthesis. \nWe focus on the component-based synthesis problem where the goal is to build a system by composing base \ncomponents. Automating component\u00adbased synthesis is attractive for many reasons. First, the designed \nsystem is correct by construction, which obviates the need for ver\u00adi.cation. Second, the designed system \ncan be guaranteed to be op\u00adtimal in terms of using the fewest possible number of components. Third, automation \nimproves developer s productivity, since .nding the exact set of components and their correct composition \ncan be a daunting task, especially when the base component library is huge. We present a synthesis procedure \nthat takes as input a speci.\u00adcation for the desired program and speci.cations of the base com\u00adponents, \nand synthesizes a circuit-style composition a straight\u00adline program using these base components. Our \nprocedure is semi-automated as it relies on the user to provide one additional piece of information: \nan over-approximation of the number of times each base component is used in the desired program. Our \nsyn\u00adthesis procedure views the base component library as a multiset whose elements are treated as resources \nthat can be used at most once. Hence, our procedure can be seen as solving a resource\u00adbounded component-based \nsynthesis problem. Our procedure is also restricted to discovering only straight-line programs. Note \nthat loop-free control-.ow can be encoded as a straight-line program by providing the ite (if-then-else) \noperator as a base component, but synthesizing loops is left for future work. The reliance on the user \nfor making copies of the base components and for discovering loops is indicative that the true success \nof program synthesis may lie in an interactive process that combines higher-level insights of humans \nwith computational capabilities of computers. While our focus on synthesizing loop-free programs may \nap\u00adpear narrow, it is nevertheless a signi.cant goal in itself [12]. Tech\u00ad niques for synthesizing loop-free \ncode will be needed when syn\u00adthesizing more complex code patterns. Synthesis of loops requires techniques \nthat are mostly orthogonal to the techniques for syn\u00adthesizing straight-line code, and hence it is fruitful \nto study them separately [13]. Loop-free program synthesis has several indepen\u00ad dent applications too \nthat have been pursued in different com\u00admunities recently; such as, optimizing the core of many compute \nintensive loops (superoptimizers) [28], synthesizing API call se\u00ad quence (e.g., Jungloid [26]), bitvector \nalgorithms, geometry con\u00ad structions [14, 18], text-editing [24, 38] and table-manipulation programs \n[16]. Finally, despite its appearance, loop-free program synthesis is challenging as the search space \nof loop-free programs is still huge. While we foresee several applications of component-based pro\u00adgram \nsynthesis, most of the examples in this paper relate to dis\u00adcovering intricate bitvector programs, which \ncombine arithmetic and bitwise operations. Bitvector programs can be quite unintu\u00aditive and extremely \ndif.cult for average, or sometimes even expert, programmers to discover methodically. Consider, for example, \nthe problem of turning-off the rightmost 1-bit in a bitvector x. This can be achieved by computing x&#38;(x \n- 1), which involves composing the bitwise &#38; operator and the arithmetic subtraction operator in \nan unintuitive manner. In fact, the upcoming 4th volume of the classic series art of computer programming \nby Knuth has a special chapter on bitwise tricks and techniques [23]. In this paper, we demonstrate how \nto semi-automate the discovery of small, but intricate, bitvec\u00adtor programs using the currently available \nformal veri.cation tech\u00adnology. This semi-automation can be exposed to users in at least two different \nways. First, software development environments can provide this capability to help programmers write \ncorrect and ef.\u00adcient code. Alternatively, compilers can use the synthesis procedure to optimize implementations \nor make them more secure. Superop\u00adtimizers, for example, perform automatic translation of a given se\u00adquence \nof instructions into an optimal sequence of instructions for performing aggressive peephole optimizations \n[5] or binary transla\u00ad tion [6]. Rather than achieve ef.ciency, the goal of such translation can also \nbe to reduce vulnerability in software. For example, any piece of code that computes the average of two \nnumbers, x and y, by evaluating (x+y)/2 is inherently .awed and vulnerable since it can over.ow. However, \nusing some bitwise tricks, the average can be computed without over.owing (e.g., (x|y) - ((x . y) >> \n1)). Compilers can replace vulnerable snippets of code by the discov\u00adered equivalent secure code. The \nnumber of straight-line programs that can be constructed using a given set of base library components \nis exponential in the number of base components. Rather than performing a naive exponential search for \n.nding the correct program, our synthesis algorithm relegates all exponential reasoning to tools that \nhave been engineered for ef.ciently performing exponential search, namely the Satis.ability (SAT) and \nSatis.ability Modulo Theory (SMT) solvers1. SMT solvers use intelligent backtracking and learning to \novercome the complexity barrier. SMT solvers can be used to verify that a given (loop-free) system meets \na given speci.cation. In this paper, we show how to use the same SMT solving technology to synthesize \nsystems that meet a given speci.cation. Existing synthesis techniques based on superoptimizers [11, 22, \n28] and sketching [32, 33] can also be used to solve the component\u00ad based synthesis problem. Superoptimizers \nexplicitly perform an ex\u00adponential search. Sketching solves a more general program synthe\u00adsis problem, \nand is not designed for solving the component-based synthesis problem. When they are used to solve the \ncomponent\u00adbased synthesis problem, both superoptimizers and sketching were empirically found to not scale. \nIn contrast, our technique leaves the inherent exponential nature of the problem to the underlying SMT \nsolver, whose engineering advances over the years have made them 1 SMT solving is an extension of SAT \nsolving technology to work with theory facts, rather than just propositional facts. In fact, there is \na SMT solving competition that is now held every year, and it has stimulated improvement in solver implementations \n[1]. effective at solving the usually-not-so-hard instances that arise in practice, and hence end up \nnot requiring exponential reasoning. Our assumption that we are given (by developers) logical spec\u00adi.cations \nof the desired program may appear to be unrealistic. It is not. The logical speci.cation can be given \nin the form of an (unopti\u00admized) program, which are easy to write. In fact, we did exactly this in our \nexperiments. In many domains, writing such logical speci\u00ad.cations is much less time-consuming than discovering \noptimized straight-line code. Our synthesis algorithm is based on a constraint-based ap\u00adproach that involves \nreducing the synthesis problem to that of solv\u00ading a constraint. This involves the two key steps of constraint \ngen\u00aderation and constraint solving. In the constraint generation phase, the synthesis problem is encoded \nas a constraint, referred to as synthesis constraint. Our synthesis constraint has two interesting aspects. \n The synthesis constraint is a .rst-order logic formula. The synthesis problem can be viewed as a generalization \nof the veri.cation problem. It is well known that veri.cation of a straight-line program can be reduced \nto proving validity of a .rst-order logic formula, and hence the synthesis problem can be reduced to \n.nding satis.ability of a second-order logic formula. But, the non-trivial aspect of our encoding is \nthat it generates a .rst-order logic formula. This is signi.cant because off-the-shelf constraint solvers \ncannot effectively solve second\u00adorder formulas.  The size of the synthesis constraint is quadratic in \nthe number of components. One way to generate a .rst-order logic constraint would be to use the constraint \ngeneration methodology used inside the sketching technique, which is also a constraint-based technique. \nHowever, the size of the constraint generated by the sketching technique could potentially be exponential \nin the number of components. In contrast, our encoding yields a constraint that is guaranteed quadratic \nin the number of components.  In the constraint solving phase, we use a re.ned form of the classic \ncounterexample guided iterative synthesis technique [10, 30] built on top of off-the-shelf SMT solvers. \nThe synthesis con\u00ad straint obtained from our encoding is an .. formula, which can\u00adnot be effectively \nsolved using off-the-shelf SMT solvers directly. The counterexample guided iterative synthesis technique \ninvolves choosing some initial set of test values for the (.) universally quan\u00adti.ed variables and then \nsolving for the (.) existentially quanti.ed variables in the resulting constraint using SMT solvers. \nIf the solu\u00adtion for the existentially quanti.ed variables works for all choices of universally quanti.ed \nvariables, then a solution has been found. Else, a counterexample is discovered and the process is repeated \nafter adding the counterexample to the set of test values for the uni\u00adversally quanti.ed variables. We \nuse this method, suitably adapted to handle de.nitions correctly, to solve our synthesis constraint. \nWe have implemented our constraint generation and constraint solving technique in a tool called Brahma. \nWe have applied Brahma to several different examples. Majority of the examples come from the domain of \nbitvector program synthesis using a set of compo\u00adnents that implement basic bitvector operations. These \nprograms typically involve unintuitive composition of the bitvector opera\u00adtions, and are quite challenging \nto synthesize manually. Brahma is able to synthesize (equivalent variants of) a variety of bitvec\u00adtor \nprograms picked up from a classic book [37] in time ranging from 1.0 to 2778.7 seconds. In contrast, \nthe Sketch and AHA tools, based respectively on sketching and super-optimization, time-out on 9 and 12 \nof the 25 examples respectively where timeout was set to 3600 seconds. Sketch is slower by an average \nfactor of 20 on the remaining examples. Contributions and Organization We de.ne the problem of component-based \nsynthesis using a set of base components (Section 3).  We present an encoding that reduces the synthesis \nproblem to that of .nding a satisfying assignment to a .rst-order logic constraint with quanti.er alternation, \nwhose size is at most quadratic in the number of base components. (Section 5).  We present a novel technique \nfor solving .rst-order logic con\u00adstraints with quanti.er alternation using off-the-shelf SMT solvers \n(Section 6).  We apply our constraint generation and solving technique to a set of benchmark examples. \nWe also experimentally compare our technique with other existing techniques, namely sketching and superoptimization, \nthat can be used to synthesize bitvector programs (Section 7). Tools based on other techniques either \nperform order of magnitude slower or timeout and fail to yield a solution.  2. Running Example First, \nwe introduce a small example to give a high-level overview of our technique. We also use this example \nas a running example to illustrate several details of our technique in following sections. Consider the \ntask of designing a bitvector program that masks off the right-most signi.cant 1-bit in an input bitvector. \nMore formally, the bitvector program takes as input one bitvector I and outputs a bitvector O such that \nO is obtained from I by setting the right-most signi.cant 1-bit in I to 0. For example, the bitvector \nprogram should transform the bitvector 01100 into 01000. A simple method to accomplish this would be \nto iterate over the input bitvector starting from the rightmost end until a 1 bit is found and then set \nit to 0. However, this algorithm is worst-case linear in the number of bits in the input bitvector. Furthermore, \nit uses undesirable branching code inside a loop. There is a non-intuitive, but elegant, way to achieving \nthe de\u00adsired functionality in constant time by using a tricky composition of the standard subtraction \noperator and the bitwise logical &#38; oper\u00adator, which are supported by almost every architecture. The \ndesired functionality can be achieved using the following composition: I &#38;(I - 1) The reason why \nwe can do this seemingly worst-case linear task in unit time using the subtraction operator and the logical \nbitwise-and operator is because the hardware implementations of these opera\u00adtors manipulate the constituent \nbits of the bitvectors in parallel in constant time. One way to discover the above tricky composition \nwould be ex\u00adhaustive enumeration. Let f1 denote a unary component that imple\u00adments the subtract-one operation, \nand let f2 denote a binary com\u00adponent that implements a binary bitwise-and operation. Suppose we knew \nthat the desired functionality can be achieved by some unknown composition of these two components f1 \nand f2. We can then simply enumerate all different ways of composing a unary op\u00aderator and a binary operator, \nand then verify which one of them meets the functional speci.cation with the help of an SMT solver (using \nthe process described in Section 4). Figure 1 shows the six different straight-line-programs that can \nbe obtained from compo\u00adsition of one unary and one binary operator. Of these the programs shown in 1(e) \nand 1(f) provide the desired functionality. There is a major problem with this explicit enumeration approach; \nit is too expensive. In fact, superoptimizers [28] do such an exhaustive enu\u00ad meration, and hence fail \nto scale beyond composition of 4 compo\u00adnents. In contrast, our technique encodes (instead of explicitly \nenu\u00admerating) the space of all (six) possible straight-line programs for composing the two operations \nf1 and f2 using a logical formula .wfp . The formula .wfp uses (.ve) integer variables, each corre\u00adsponding \nto an input or output of some component. Intuitively, the integer variable corresponding to the output \nof some component denotes the line number at which the component is positioned. The integer variable \ncorresponding to an input of some component de\u00adnotes the line number where the actual parameter corresponding \nto that input is de.ned. The formula .wfp is such that the satis\u00adfying assignments to the integer variables \nhave a one-to-one cor\u00adrespondence with the different straight-line programs that can be obtained from \ncomposition of these operators. In conjunction with some other constraints that encode the functional \nspeci.cations of the base component programs and the desired program, our tech\u00adnique generates a formula \nthat we refer to as the synthesis con\u00adstraint. A satisfying assignment to the integer variables that \nsatis\u00ad.es the synthesis constraint corresponds to the desired straight-line program. The synthesis constraint \nis a .rst-order logic constraint with quanti.er alternation, and is not amenable to solving directly \nusing off-the-shelf constraint solvers. One of the key technical con\u00adtributions of the paper is an algorithm \nto .nd satisfying assignments to such synthesis constraints by using an off-the-shelf SMT solver. Even \nthough there is no provable polynomial time guarantee as\u00adsociated with our technique, there is a crucial \ndifference between the exponential exhaustive enumeration technique and our tech\u00adnique based on synthesis-constraint \ngeneration and solving. The number of variables in the synthesis constraint is linear in the number of \ncomponents and the size of the synthesis constraint is quadratic in the number of components. The winning \nadvantage comes from the fact that we ride upon the recent engineering ad\u00advances made in SMT solving \ntechnology to solve a constraint with a linear number of unknowns as opposed to explicitly performing \nan exhaustive enumeration over an exponential search space. 3. Problem De.nition The goal of this paper \nis to synthesize a program by using a given set of base software components. The program as well as the \nbase components are speci.ed using their functional description. This description is given in the form \nof a logical formula that relates the inputs and the outputs. For simplicity of presentation, we assume \nthat all components have exactly one output. We also assume that all inputs and the out\u00adput have the \nsame type. These restrictions can be easily removed. Formally, the synthesis problem requires the user \nto provide: A speci.cation (I, O, ffspec (f I,O)) of the program, which in\u00adcludes a tuple of input variables \nIfand an output variable O. an expression fspec (fI and O that I,O) over the variables fspeci.es the \ndesired input-output relationship. A set of speci.cations {(Ifi,Oi,fi(fIi,Oi))| i =1,...,N}, called a \nlibrary, where fi(fIi,Oi) is a speci.cation for base component fi. All variables Ifi,Oi are assumed distinct. \n The goal of the synthesis problem is to discover a program f impl that correctly implements the speci.cation \nfspec using only the components provided in the library. The program f impl is essentially a straight-line \nprogram that takes as input Ifand uses the set {O1,...,ON } as temporary variables in the following form: \nf impl(If): Op1 := fp1 (Vfp1 ); ... ; OpN := fpN (VfpN ); return OpN ;  ffimpl (I): 1 O2 := f2(I, I); \n2 O1 := f1(O2); return O1; ffimpl (I): 1 O2 := f2(I, I); 2 O1 := f1(I); return O1; ffimpl (I): 1 O1 := \nf1(I); 2 O2 := f2(I, I); return O2; ffimpl (I): 1 O1 := f1(I); 2 O2 := f2(O1, O1); return O2; ffimpl \n(I): 1 O1 := f1(I); 2 O2 := f2(O1, I); return O2; ffimpl (I): 1 O1 := f1(I); 2 O2 := f2(I, O1); return \nO2; lI1 = 1 lO1 = 2 lI2 = 0 lO2 = 1 lI.2 = 0 lI1 = 0 lO1 = 2 lI2 = 0 lO2 = 1 lI.2 = 0 lI1 = 0 lO1 = 1 \nlI2 = 0 lO2 = 2 lI.2 = 0 lI1 = 0 lO1 = 1 lI2 = 1 lO2 = 2 lI.2 = 1 lI1 = 0 lO1 = 1 lI2 = 1 lO2 = 2 lI.2 \n= 0 lI1 = 0 lO1 = 1 lI2 = 0 lO2 = 2 lI.2 = 1 (a) (b) (c) (d) (e) (f) Figure 1. The .rst row shows six \ndifferent ways of composing a unary component f1 and a binary component f2 to synthesize a straight-line \nprogram ffimpl with one input I. Second row shows an integer encoding of the corresponding program using \nlocation variables. where each variable in Vfpi is either an input variable from If, or a temporary \nvariable Opj such that j<i,  p1,...,pN is a permutation of 1,...,N, and  the following correctness \ncriteria holds:  .f:fp1 Vp1 ,Op1 ) .\u00b7 \u00b7\u00b7. fpN (VfpN ,OpN )I,O1,...,ON (f . fspec (I, OfpN ) (1) The \nlast formula above is called the veri.cation constraint. It states the correctness criterion for the \noutput program: for all inputs f I, if OpN is the output of the implementation on If, then OpN should \nalso be the output of the speci.cation on If; that is, the implementation should imply the speci.cation. \nWe note that the implementation above is using all components from the library. We can assume this without \nany loss of generality. Even when there is a correct implementation using fewer compo\u00adnents, that implementation \ncan always be extended to an implemen\u00adtation that uses all components by adding dead code. Dead code \ncan be easily identi.ed and removed in a post-processing step. We also note that the implementation above \nis using each base component only once. If there is an implementation using multiple copies of the same \nbase component, we assume that the user pro\u00advides multiple copies explicitly in the library (discussed \nfurther in Section 7.3). Such a restriction of using each base component only once is interesting in \ntwo regards: It can be used to enforce ef.cient or minimal designs. This restriction also prunes down \nthe search space of possible designs making the problem .nite and tractable. Informally, the synthesis \nproblem seeks to come up with an implementation using only the base components in the given library \n that implies the given speci.cation. EXAMPLE 1 (Problem De.nition). The problem de.nition for the running \nexample in Sec. 2 can be stated as: The formal speci.cation of the desired program to be synthe\u00adsized \nis given by the following relationship fspec between the input bitvector I and the output bitvector O. \nWe use b to denote the total number of bits in the bitvectors, and I[j] to denote the bit at jth position \nin bitvector I, when viewed as an array of bits. bb 66 fspec (I,O) := I[t]=1 . I[j]=0. t=1j=t+1 . .. \n6 .O[t]=0 . [O[j]= I[j].. j=t The number of base components in the library is N =2. One of them is a \nunary component f1 that implements the subtract-one operation, and its formal speci.cation is given by \nthe following relationship f1 between its input parameter I1 and output O1. f1(I1,O1) := O1 =(I1 - 1) \nThe other component is a binary component that implements the bitwise-and operation, and its formal speci.cation \nis given by the following relationship f2 between its input parameters I2,I2 and output O2. f2(I2,I2,O2) \n:= O2 =(I2 &#38; I2) 4. Revisiting Veri.cation Constraint Before we describe our approach for solving \nthe synthesis prob\u00ad lem consisting of the synthesis constraint generation phase and the constraint solving \nphase we will perform two steps in this section to support the transition to these two phases. First, \nwe will rewrite the veri.cation constraint in Eq. 1 so that it resembles the synthesis constraint. Second, \nwe discuss solving of the veri.cation constraint, which is a small part of the process of solving the \nsyn\u00ad thesis constraint. Consider the veri.cation constraint in Eq. 1. We can replace each atomic fact \nfpi (Vfpi ,Opi ) in the antecedent by fpi (Ifpi ,Opi ). Ifpi = Vfpi . We can also replace the fact fspec \n(I, OfpN ) in the consequent by fspec (f= OpN I,O) provided we add O in the antecedent. Hence, the veri.cation \nconstraint can be rewritten as: .ff I, O, If1,.., In,O1,..,ON : N 6 (O = OpN ) . Ii,Oi) . f= . I,O) \n(fi(fIi Vfi)fspec (f i=1 We now split the antecedent in the above formula into two parts flib and fconn \n. We also group together the formal inputs and outputs of the base components into two sets P and R to \nrewrite the above veri.cation constraint as: .f(f(f(2) I, O, P, R :(flib (P, R) . fconn I, O, P, R)) \n. fspec I,O) where NN 66 flib := ( fi(Ifi,Oi)),fconn := (O = OpN ) . ( Ifi = Vfi), i=1 i=1 Here P \nand R denote the union of all formal inputs (Parameters) and formal outputs (Return variables) of the \ncomponents: N N f P := Ii R := {Oi} = {O1,...,ON } i=1 i=1 Note that flib represents the speci.cations \nof the base compo\u00ad nents, and fconn represents the interconnections that includes the mappings from \nformals to actuals and from the return variable of some component to the output of the program. Observe \nthat fconn is a conjunction of equalities between a variable in P .{O} and a variable in R . If. The \nconnectivity constraint fconn determines:  the order in which base components occur in the program. \n the value of each input parameter of each base component.  EXAMPLE 2 (Veri.cation Constraint). The \nveri.cation constraint for the program in Figure 1(e) when regarded as a solution to the running example \nformally described in Example 1 is the following formula. .I, O, I1,I2,I2,O1,O2 (flib . fconn . fspec \n) where flib := f1(I1,O1) . f2(I2,I2,O2) and fconn := I1 = I . I2 = O1 . I2 = I . O = O2 and f1,f2,fspec \nare as de.ned in Example 1. We now brie.y discuss the process of solving the veri.cation constraint, \nwhich is a universally quanti.ed formula. The complex\u00adity of deciding the validity of the formula in \nEq. 2 depends on the expression language used for de.ning fspec and fi s. If this ex\u00adpression language \nis a subset of the language that can be handled by Satis.ability Modulo Theory (SMT) solvers, then we \ncan use off-the-shelf SMT solvers to decide the formula in Eq. 2 and thus solve the veri.cation problem. \nSpeci.cally, we can check validity of a (universal) formula by asking an SMT solver for checking sat\u00adis.ability \nof the negation of that formula. 5. Synthesis Constraint In this section, we show how to reduce the problem \nof straight-line\u00adprogram synthesis to that of .nding a satisfying assignment to a .rst order logic constraint. \nGiven a library of base components, and a speci.cation for the desired program, we show how to generate \na formula that encodes the existence of a program that is constructed using the base components and that \nmeets the given speci.cation. Consider the veri.cation constraint in Eq. 2. We are given fspec and flib \nas part of the synthesis problem. However, we do not know the interconnections fconn between the inputs \nand outputs of the base components. Hence, the synthesis problem is equivalent to solving the following \nconstraint: : .f .fconn I, O, P, R : (f(f (flib (P, R) . fconn I,O , P, R)) . fspec I,O) where we have \na second-order existential quanti.er over the set of all possible interconnections. In the remaining \npart of this section, we show how to convert the second-order existential quanti.er into a .rst-order \nexistential quanti.er. The basic idea is to introduce new .rst-order integer\u00advalued variables, referred \nto as location variables, whose values decide the interconnections between the various components. To \ndescribe a program, we have to determine which component goes on which location (line-number), and from \nwhich location (line\u00adnumber or program input) does it get its input arguments. This information can be \ndescribed by a set of location variables L L := {lx | x . P . R} that contains one new variable lx for \neach variable x in P . R with the following interpretation associated with each of these variables. \nIf x is the output variable Oi of component fi, then lx repre\u00adsents the line in the program where the \ncomponent fi is posi\u00adtioned.  If x is the jth input parameter of component fi, then lx repre\u00adsents the \nlocation from where component fi gets its jth input .  A location above refers to either a line of the \nprogram, or to some program input. To represent different possible locations, we use integers in the \nset {0,..,M - 1}, where M is the sum of the number N of components in the library and the number |If| \nof program inputs, i.e., M = N + |If|, with the following interpretation. The jth input is identi.ed \nwith the location j - 1.  The jth line or the assignment statement in the program is identi.ed with \nthe location j + |If|- 1.   EXAMPLE 3 (Location Variables). For our running example for\u00admally described \nin Example 1, the set L of location variables con\u00adsists of 5 integer variables. L = {lO1 ,lO2 ,lI1 ,lI2 \n,lI}. The vari\u00ad 2 ables lO1 and lO2 denote the location at which the components f1 and f2 are positioned \nrespectively. The variable lI1 denotes the lo\u00adcation of the de.nition of the input to the unary component \nf1. The variables lI2 and lIdenote the locations of the de.nitions of the 2 .rst and the second input \nrespectively of the binary component f2. Since there are two components and one input, we have N =2 and \nM =3. The variables lO1 ,lO2 take values from the set {1, 2}, while the variables lI1 ,lI2 ,lItake values \nfrom the set {0, 1, 2}. 2 The synthesis constraint, which uses the location variables L, is given in \nEq. 4 in Section 5.3. We next discuss the key constituents of the synthesis constraint. For notational \nconvenience (for the discussion below), we also de.ne lx for the global inputs Ifand output O. We de.ne \nlO to be equal to M - 1, denoting that the output O of the program is de.ned on the last line of the \nprogram. For the jth input x to the program, we de.ne lx to be j-1, which is the integer location that \nwe associated with the jth program input. 5.1 Encoding Well-formed Programs: .wfp We noted above that \nevery straight-line program can be encoded by assigning appropriate values from the set {0,..,M - 1} \nto vari\u00adables in L. On the other hand, any possible assignment to variables in L from the set {0,..,M \n- 1} does not necessarily correspond to a well-formed straight-line program. We require the variables \nin L to satisfy certain constraints to guarantee that they de.ne well\u00adformed programs. The following \ntwo constraints guarantee this. Consistency Constraint : Every line in the program has at most one component. \nIn our encoding, lOi encodes the line number where component fi is positioned. Hence for different i, \nlOi should be different. Thus we get the following consistency constraint. 6 .cons := (lx= ly) x,y.R,x[=y \n Acyclicity Constraint : In a well-formed program, every variable is initialized before it is used. In \nour encoding, component fi is positioned at location lOi and its inputs are coming from locations {lx \n| x . Ifi}. Thus, we get the following acyclicity constraint. N 66 .acyc := ( lx <ly) i=1 x.I i,y=Oi \nThe acyclicity constraint says that, for every component, if x is an input of that component and y is \nan output of that component, then the location lx where the input is de.ned, should be earlier than the \nlocation ly where the component is positioned and its output is de.ned. We now de.ne .wfp (L) to be \nfollowing constraint that encodes the interpretation of the location variables lx along with the consis\u00adtency \nand acyclicity constraints. 66 .wfp (L) := (0 = lx = M - 1) . (|If|= lx = M - 1) . x.P x.R  .cons (L) \n. .acyc (L) quanti.er to get the following synthesis constraint. We note that if the location variables \nL satisfy .wfp , then L de\u00ad.nes a well-formed straight-line program in static single assign\u00adment (SSA) \nform [7], whose assignments make calls to the compo\u00ad nents in the library. Speci.cally, the function \nLval2Prog returns the program corresponding to a given valuation L as follows: in the ith line of Lval2Prog(L), \nwe have the assignment Oj := fj (Os(1),..,Os(t)) if lOj = i, lIk = ls(k) for k =1,..,t, where j t is \nthe arity of component fj , and (Ij 1,..,Ijt) is the tuple of input variables Ifj of fj . Our encoding \nhas the following natural property. THEOREM 1. Let L be the set of all valuations of L that satisfy the \nwell-formedness constraint .wfp . Let . be the set of all straight\u00adline programs in SSA form that take \ninput Ifand contain the N assignments, Oi := f(Vfi), such that every variable is de.ned before it is \nused. Then, the mapping Lval2Prog goes from L to . and it is bijective. EXAMPLE 4 (Well-formedness Constraint). \nFor our running ex\u00adample formally described in Example 1, the constraint .wfp is: 66 .wfp := .cons . \n.acyc . (0 = lx = 2) . (1 = lx = 2) x.P x.R where .cons := (lO1 = lO2 ) and .acyc := (lI1 <lO1 ) . (lI2 \n<lO2 ) . (lI<lO2 ) 2 {fff Here P = I1,I2,I2} and R = {O1,O2}. There are 6 solutions for lI1 ,lI2 ,lI,lO1 \n,lO2 that satisfy the constraint .wfp . Each of 2 these solutions correspond to a syntactically distinct \nand well\u00adformed straight-line program obtained by composition of unary component f1 and binary component \nf2. These 6 solutions and the corresponding straight-line-programs are shown in Figure 1.  5.2 Encoding \nData.ow in Programs: .conn Given an interconnection among components speci.ed by values of location variables \nL, we can relate the input/output variables of the components and the program by the following connectivity \nconstraint: 6 .conn := (lx = ly . x = y) x,y.P.R.I.{O} The constraint .conn will play the role of fconn \nlater.  5.3 Putting it all together We are now ready to present the (.rst-order) synthesis constraint \nthat encodes the synthesis problem. We showed how the set of all valid programs can be described by valuations \nof the location variables L. Hence, the synthesis problem reduces to .nding a value for the variables \nL such that (1) this valuation corresponds to a well-formed program and (2) the corresponding well-formed \nprogram is correct, as described by the veri.cation constraint (Eq. 2). In other words, we get the following \nsynthesis constraint:   I, O, P, R :.L :(.wfp (L) ..f (f(f flib (P, R) . .conn I, O, P, R,L) . fspec \nI,O)) (3) We merge the (temporary) variables P and R and call it the set T . We rewrite the formula in \nEq. 3 by pulling out the universal .L.f I, O, T : .wfp (L) . (flib (T ) . .conn I, O, T, L) . fspec \nI,O)) (4) (f(f EXAMPLE 5 (Synthesis Constraint). Of the 6 solutions to the lo\u00adcation variables L described \nin Example 4, there are 2 solutions that satisfy the entire synthesis constraint. These two solutions \nare shown in Figure 1(e) and Figure 1(f). The following theorem states that the synthesis constraint \nin Eq. 4 is quadratic in size and it exactly encodes our synthesis problem. Hence, solving the synthesis \nproblem is equivalent to solving the synthesis constraint. The proof of the theorem follows from the \nde.nition of Lval2Prog, Theorem 1, and the de.nitions of the veri.cation and synthesis constraints. THEOREM \n2 (Synthesis Constraint). Let (fspec ,flib ) be the given speci.cations. Let . be the corresponding synthesis \nconstraint, de.ned in Eq. 4, that is derived from the given speci.cations. The size of . is O(n + m 2) \nwhere n is the size of (fspec ,flib ) and m is the number of base components in the library. Furthermore, \n. is valid if and only if there is a straight-line program that implements the speci.cation fspec using \nonly the components in flib . PROOF: The number of variables in L is O(m) and hence the size of . is \nseen to be O(n + m 2). (.): Suppose . is valid. This implies that there exists a value for L, say L0, \nsuch that .wfp (L0) holds and the formula .f I, O, P, R : flib (P, R) . .conn I, O, P, R,L0) . fspec \nI,O) is valid. (f(fSince .wfp (L0) holds, we can use Theorem 1 to get a Program Lval2Prog(L), call it \nP . Now, the de.nition of Lval2Prog and the constraint .conn together guarantee that the connec\u00adtivity \nconstraint fconn de.ned by P and the connectivity con\u00adstraint .conn (L0) are equivalent. Since we know \n.f I, O, P, R : flib . .conn . fspec is valid, it follows that the formula .f I, O, P, R : flib . fconn \n. fspec is also valid. This shows that the veri.cation constraint for correctness of P is valid. (.): \nSuppose there is a straight-line program, say P , that cor\u00adrectly implements the given speci.cation fspec \nusing only the components in flib . Given a program P , we can immediately de.ne values for the location \nvariables L such that fconn is equiv\u00adalent to .conn (L). Since the program P is assumed to be well\u00adformed, \nthis valuation of L will satisfy .wfp . Furthermore, since P is correct, the veri.cation constraint is \nvalid. Replacing fconn in the veri.cation constraint by .conn shows that the synthesis constraint is \nalso valid. D 6. Synthesis Constraint Solving In this section, we show how to solve the synthesis constraint \n(Eq. 4 in Section 5.3). In particular, we show how to .nd an assignment to the decision variables L that \nwould witness the validity of the synthesis constraint. Our procedure is built over the standard counterexample-guided \niterative re.nement paradigm [10, 30]. It solves .L.If: f(L, If) by iteratively .nding values for L that \nwork for more and more values of If. Let S denote a .nite set of valuations of If. In each iteration, \nour procedure .rst .nds a valuation for L that works for the choices in S. It then tests if the discovered \nvaluation for L works for all If. If it does not, then a valuation of Iffor which it does not work is \nadded to the set S and the process is repeated. The Procedure ExAllSolver shown in Figure 2 is a high-level \ndescription of our solver for the synthesis constraint. It alternates ExAllSolver(.wfp , flib , .conn \n, fspec ): .L.f 1 // I, O, T : .wfp . (flib . .conn . fspec ) // is a synthesis constraint 2 // Output: \nsynthesis failed or values for L f 3 S := {If0} // I0 is an arbitrary input 4 while (1) {5 model := \nT-SAT(.L, O1,...,On,T1,...,Tn : .wfp (L). Ii.S(flib (Ti) . .conn (Ifi,Oi,Ti,L) .fspec (Ifi,Oi))); if \n(model = .) { 6 currL := model|L } else { 7 return(\"synthesis failed\"); } 8 model T-SAT(.f(f := I, \nO, T :.conn I, O, T, currL). I,O)); if (model = .) { flib (T ) .\u00acfspec (f f 9 I1 := model|; S := S.{If1}; \nI } else {10 return(currL); }11 } Figure 2. Counterexample guided .. solver for solving the syn\u00adthesis \nconstraint. Note that Line 5 and Line 8 use different formu\u00adlas. If successful, the procedure outputs \nvalues for L that can be used to extract the desired straight-line program (Theorem 1).  between .nite \nsynthesis .nding a valuation for L that works some .nite choices of If and veri.cation checking that \nthe valuation works for all If and, in each iteration, the two steps learn from each other. The new \nvalue for L is always guided by a set of inputs on which the previous choice for L failed. A technical \nissue worth pointing out in Procedure ExAllSolver is that it handles variables O, T differently from \nthe variables If even though they are all universally quanti.ed in the synthesis constraint. Intuitively, \nthis is because O, T are not independent variables, but their values depend on Ifand L. Speci.cally, \nProce\u00addure ExAllSolver uses the following two different variants of the synthesis constraint in its two \nsteps. The formula (Fver ) is same as the synthesis constraint, while the formula (Fsyn ) is a weaker \nversion of the synthesis constraint. (Fver ) I, O, T :(.wfp . (flib . fspec )) .L .f. .conn (Fsyn ) \n.L .If.O, T :(.wfp . (flib . .conn . fspec )) The two phases of the procedure can now be described as \nfollows. Finite Synthesis (Lines 5-7): In this step, we synthesize a design that works for .nitely many \ninputs. Speci.cally, the procedure .nds values for L that work for all the inputs in S (Line 5,6). If \nno such values are found, we terminate and declare that no design could be found (Line 7). Line 5 is \neffectively solving for Formula (Fsyn ), which is different from the synthesis constraint. Veri.cation \n(Lines 8-10): In this step, we verify if the synthesized design that we know works for the inputs in \nS also works for all inputs. Speci.cally, if the generated value currL for L work for all inputs, then \nwe terminate with success. If not, then we .nd an input If1 on which it does not work and add If1 to \nS (Line 9). Line 8 is verifying Formula (Fver ), which is the synthesis constraint. The function T-SAT \nchecks for satis.ability modulo theory of an existentially quanti.ed formula. If the formula is satis.able, \nthen  it returns a model, i.e., values for the existential variables that make the formula true. Note \nthat the function T-SAT is essentially a call to the SMT solver. We need to argue that Procedure ExAllSolver \nalways returns the correct answer on termination. This is stated in Theorem 3. But, before that, we need \na lemma that relates the two formula (Fsyn ) and (Fver ). Under the assumption that the implementations \nfi s of the base components in the library are all terminating, we can prove that (Fver ) logically implies \n(Fsyn ). LEMMA 1. Suppose the implementation of each base component fi in the library is terminating. \nThen, (Fver ) logically implies (Fsyn ). PROOF: Suppose (Fver ) holds. Let L0 be the values of L that \nshow validity of (Fver ). We need to prove that (Fsyn ) also holds. We will show that the values L0 will \nalso make the formula (Fsyn ) valid. Let Ifbe an arbitrary input. We need to show that there are values \nfor P, R and O such that flib (P,O) . .conn I, O, P, R,L0) holds. Since .wfp (L0) is true, it follows \n(f from Theorem 1 that there is a well-formed program P . Since all components in the library are assumed \nto be terminating, the program P on input Ifwill compute at least one value for each variable in the \nprogram. These values will make the formula flib (P,O) . .conn (fD I, O, P, R,L0) true. The following \ntheorem states the correctness of our constraint solving procedure, and its proof follows from Lemma \n1. THEOREM 3. Suppose that Procedure ExAllSolver is called with the input .wfp (L), flib (T ), .conn \n(f(f I, O, T, L), and fspec I,O), where T := (P . R). Then, (a) If the procedure terminates with answer \nsynthesis success\u00adful, then the synthesis constraint is valid. (b) If the procedure terminates with \nanswer synthesis failed, then the synthesis constraint is not valid.  PROOF: Proof of Part (a): First, \nsince currL is (a part of) a model for the formula in Line 6, the value of currL in the pro\u00adgram always \nsatis.es the constraint .wfp (L). Second, the proce\u00addure returns synthesis successful only when the constraint \n.I, O, T f: flib ..conn .\u00acfspec is unsatis.able. This means that the veri.cation constraint, .I, O, T \nf: flib . .conn . fspec , is valid. This completes the proof of Part (a). Proof of Part (b): The procedure \nreturns synthesis failed only when the constraint .L, O1,...,On,T1,...,Tn : .wfp (L). Pi.S (flib (Ti)..conn \n(Ifi,Oi,Ti,L).fspec (Ifi,Oi)) is unsat\u00adis.able. By Lemma 1, this implies that the veri.cation constraint \nis unsatis.able. D Now we have all the components synthesis constraint genera\u00adtion (Eq. 4), synthesis \nconstraint solving (Figure 2), and the map\u00ad ping from values of L to programs (Lval2Prog) to describe \nour overall approach. Our complete synthesis procedure is described in Figure 3, and its correctness \nfollows from the correctness of the three steps, namely Theorem 1, Theorem 2 and Theorem 3. 7. Experimental \nResults In this section, we present an experimental evaluation of our syn\u00adthesis technique. We also experimentally \ncompare our technique with other existing techniques. Benchmarks We selected 25 examples, numbered P1-P25, \nfrom the book Hacker s Delight, commonly referred to as the Bible of P22(x) : Compute Parity P1(x) : \nTurn-off rightmost 1 bit. This is the running ex\u00adample in the paper. 1 o1:=bvsub (x,1) 2 res:=bvand (x,o1) \nP2(x) : Test whether an un\u00adsigned integer is of the form 2n-1 1 o1:=bvadd (x,1) 2 res:=bvand (x,o1) P3(x) \n: Isolate the right\u00admost 1-bit 1 o1:=bvneg (x) 2 res:=bvand (x,o1) P4(x) : Form a mask that identi.es \nthe rightmost 1 bit and trailing 0s 1 o1:=bvsub (x,1) 2 res:=bvxor (x,o1) P5(x) : Right propagate rightmost \n1-bit 1 o1:=bvsub (x,1) 2 res:=bvor (x,o1) P6(x) : Turn on the right\u00admost 0-bit in a word 1 o1:=bvadd \n(x,1) 2 res:=bvor (x,o1) P7(x) : Isolate the right\u00admost 0-bit 1 o1:=bvnot (x) 2 o2:=bvadd (x,1) 3 res:=bvand \n(o1,o2) P8(x) : Form a mask that identi.es the trailing 0 s 1 o1:=bvsub (x,1) 2 o2:=bvnot (x) 3 res:=bvand \n(o1,o2) P9(x) : Absolute Value Function 1 o1:=bvshr (x,31) 2 o2:=bvxor (x,o1) 3 res:=bvsub (o2,o1) P10(x, \ny) : Test if nlz(x) == nlz(y) where nlz is number of leading zeroes 1 o1:=bvand (x,y) 2 o2:=bvxor (x,y) \n3 res:=bvule (o2,o1) P11(x, y) : Test if nlz(x) < nlz(y) where nlz is number of leading zeroes 1 o1:=bvnot \n(y) 2 o2:=bvand (x,o1) 3 res:=bvugt (o2,y) P12(x, y) : Test if nlz(x) <= nlz(y) where nlz is number of \nleading zeroes 1 o1:=bvnot (y) 2 o2:=bvand (x,o1) 3 res:=bvule (o2,y) P13(x) : Sign Function 1 o1:=bvshr \n(x,31) 2 o2:=bvneg (x) 3 o3:=bvshr (o2,31) 4 res:=bvor (o1,o3) P14 (x, y) : Floor of aver\u00adage of two \nintegers without over-.owing 1 o1:=bvand (x,y) 2 o2:=bvxor (x,y) 3 o3:=bvshr (o2,1) 4 res:=bvadd (o1,o3) \nP15 (x, y) : Ceil of aver\u00adage of two integers without over-.owing 1 o1:=bvor (x,y) 2 o2:=bvxor (x,y) \n3 o3:=bvshr (o2,1) 4 res:=bvsub (o1,o3) P16 (x, y) : Compute max of two integers 1 o1:=bvxor (x,y) 2 \no2:=bvneg (bvuge (x,y)) 3 o3:=bvand (o1,o2) 4 res:=bvxor (o3,y) P17(x) : Turn-off the right\u00admost contiguous \nstring of 1 bits 1 o1:=bvsub (x,1) 2 o2:=bvor (x,o1) 3 o3:=bvadd (o2,1) 4 res:=bvand (o3,x) P18(x) : \nDetermine if an in\u00adteger is a power of 2 or not 1 o1:=bvsub (x,1) 2 o2:=bvand (o1,x) 3 o3:=bvredor (x) \n4 o4:=bvredor (o2) 5 o5:=!(o4) 6 res:=(o5 &#38;&#38; o3) P19(x, m, k) : Exchanging 2 .elds A and B of \nthe same register x where m is mask which identi.es .eld B and k is number of bits from end of A to start \nof B 1 o1:=bvshr (x,k) 2 o2:=bvxor (x,o1) 3 o3:=bvand (o2,m) 4 o4:=bvshl (o3,k) 5 o5:=bvxor (o4,o3) 6 \nres:=bvxor (o5,x) P20(x) : Next higher un\u00adsigned number with same number of 1 bits 1 o1:=bvneg (x) 2 \no2:=bvand (x,o1) 3 o3:=bvadd (x,o2) 4 o4:=bvxor (x,o2) 5 o5:=bvshr (o4,2) 6 o6:=bvdiv (o5,o2) 7 res:=bvor \n(o6,o3) P21(x, a, b, c) : Cycling through 3 values a,b,c 1 o1:=bvneg (bveq (x,c)) 2 o2:=bvxor (a,c) 3 \no3:=bvneg (bveq (x,a)) 4 o4:=bvxor (b,c) 5 o5:=bvand (o1,o2) 6 o6:=bvand (o3,o4) 7 o7:=bvxor (o5,o6) \n8 res:=bvxor (o7,c) 1 o1:=bvshr (x,1) 2 o2:=bvxor (o1,x) 3 o3:=bvshr (o2,2) 4 o4:=bvxor (o2,o3) 5 o5:=bvand \n(o4,0x11111111) 6 o6:=bvmul (o5,0x11111111) 7 o7:=bvshr (o6,28) 8 res:=bvand (o7,0x1) P23(x) : Counting \nnumber of bits 1 o1:=bvshr (x,1) 2 o2:=bvand (o1,0x55555555) 3 o3:=bvsub (x,o2) 4 o4:=bvand (o3,0x33333333) \n5 o5:=bvshr (o3,2) 6 o6:=bvand (o3,0x33333333) 7 o7:=bvadd (o4,o6) 8 o8:=bvshr (o7,4) 9 o9:=bvadd (o8,o7) \n10 res:=bvand (o9,0x0F0F0F0F) P24(x) : Round up to the next highest power of 2 1 o1:=bvsub (x,1) 2 o2:=bvshr \n(o1,1) 3 o3:=bvor (o1,o2) 4 o4:=bvshr (o3,2) 5 o5:=bvor (o3,o4) 6 o6:=bvshr (o5,4) 7 o7:=bvor (o5,o6) \n8 o8:=bvshr (o7,8) 9 o9:=bvor (o7,o8) 10 o10:=bvshr (o9,16) 11 o11:=bvor (o9,o10) 12 res:=bvadd (o10,1) \nP25(x, y) : Compute higher order half of product of x and y 1 o1:=bvand (x,0xFFFF) 2 o2:=bvshr (x,16) \n3 o3:=bvand (y,0xFFFF) 4 o4:=bvshr (y,16) 5 o5:=bvmul (o1,o3) 6 o6:=bvmul (o2,o3) 7 o7:=bvmul (o1,o4) \n8 o8:=bvmul (o2,o4) 9 o9:=bvshr (o5,16) 10 o10:=bvadd (o6,o9) 11 o11:=bvand (o10,0xFFFF) 12 o12:=bvshr \n(o10,16) 13 o13:=bvadd (o7,o11) 14 o14:=bvshr (o13,16) 15 o15:=bvadd (o14,o12) 16 res:=bvadd (o15,o8) \n Figure 4. Benchmark Examples. The functions used in the examples have the usual semantics de.ned in \nSMTLIB QF BV logic [2]. CompositionSynthesis(fspec , {fi | i =1,...,N}): // Input: fspec : component \nspecification // // {fi | i = 1, . . . , N}: library specification Output: Failure/Program implementing \nfspec 1 Let .L.fI, O, P, R : .wfp . (flib . .conn . fspec ) be the synthesis constraint. 2 L := ExAllSolver(.wfp \n, flib , .conn , fspec ); 3 if (L = \"synthesis failed\") return(Lval2Prog(L)); { 4 5 } else {return(\"synthesis \n} failed\")}; Figure 3. Algorithm for the component-based synthesis problem. Q2(x): Compute x 31 Q1(a, \nb, c): Evaluate 1 o1:=x * x a * h2 + b * h + c 2 o2:=o1 * o1 1 o1:=a*h 3 o3:=x * o2 2 o2:=o1+b 4 o4:=o2 \n* o3 3 o3:=o2*h 5 o5:=o4 * o2 4 res:=o3+c 6 o6:=o5 * o4 7 res:=o6 * o4 Figure 5. Representative Arithmetic \nBenchmark Examples. The arithmetic functions used in the examples have the usual semantics. bit twiddling \nhacks [37]. We picked 2 non-bitvector benchmarks Q1-Q2. The bitvector examples are described in Figure \n4. The examples, P1-P25, are numbered in increasing order of complexity: P1 is a 2-line program and P25 \nis a 16-line program. Example Q1-Q2 in Figure 5 involve arithmetic. For each example, we provided the \nspeci.cation of the desired circuit by specifying the functional relationship between the inputs and \noutput of the circuit. We also provided the set of base components (in the form of their functional speci.cations) \nused in these examples. We chose 25 bitvector programs because they use ingenious little programming \ntricks that can sometimes stall programmers for hours or days and their correctness is not at all obvious \nuntil explained or fathomed [37]. Furthermore, it allows us to use existing tools based on superoptimization \n[4, 28] and sketching [32, 33] as a baseline for experimentally evaluating our work. We chose the 2 additional \nbenchmarks to illustrate the need for SMT solvers. Some programs in our benchmarks require us to also \ndiscover constants, such as 0xffff, that may occur in the program. Our syn\u00adthesis framework can be easily \nextended to discovering such con\u00adstants by introducing a generic base component fc that simply out\u00adputs \nan arbitrary constant c. Then, in the .nal synthesis constraint (Equation 4), we existentially quantify \nover c too. Implementation and Experimental Setup We implemented our technique in a tool called Brahma. \nIt uses Yices 1.0.21 [3] as the un\u00ad derlying SMT solver, which supports reasoning for quanti.er-free \nbitvectors and rational linear arithmetic. For bitvector examples, we synthesized programs to work on \nbit-vectors of size 32 bits. We used the optimization of synthesizing for increasingly large bit\u00advector \nlengths until veri.cation succeeded on bitvectors of size 32 bits. We ran our experiments on 8x Intel(R) \nXeon(R) CPU 1.86GHz with 4GB of RAM. Brahma was able to synthesize the desired pro\u00adgrams for each of \nthe benchmark examples. We now present various statistics below. Benchmark Brahma Sketch ratio AHA Id \n#lines Iter. runtime runtime Sketch/ time(sec) sec sec Brahma [#cand] 1 2 3 4 5 6 7 P1 2 2 3.2 69.8 22 \n0.1[1] P2 2 3 3.6 28.9 8 0.1[1] P3 2 3 1.4 91.8 63 0.1[1] P4 2 2 3.3 68.4 21 0.1[1] P5 2 3 2.2 67.9 31 \n0.1[1] P6 2 2 2.4 87.0 36 0.1[1] P7 3 2 1.0 69.6 68 1.7[9] P8 3 2 1.4 70.0 51 1.4[9] P9 3 2 5.8 85.1 \n15 6.5[5] P10 3 14 76.1 timeout NA 10.4[1] P11 3 7 57.1 timeout NA 9.3[1] P12 3 9 67.8 timeout NA 9.5[1] \nP13 4 4 6.2 193.7 31 timeout P14 4 4 59.6 935.3 16 timeout P15 4 8 118.9 726.5 6 timeout P16 4 5 62.3 \n820.8 13 timeout P17 4 6 78.1 626.1 8 108.6[9] P18 6 5 45.9 117.2 2 timeout P19 6 5 34.7 472.8 14 timeout \nP20 7 6 108.4 timeout NA timeout P21 8 5 28.3 timeout NA timeout P22 8 8 279.0 timeout NA timeout P23 \n10 8 1668.0 timeout NA timeout P24 12 9 224.9 timeout NA timeout P25 16 11 2778.7 timeout NA timeout \nQ1 4 2 0.2 timeout NA Q2 7 4 295.8 timeout NA  Table 1. Comparing our tool Brahma with Sketch and AHA. \nTime\u00adout was 1 hour. NA denotes not applicable. The table shows the runtime for Brahma (Col. 4), Sketch \n(Col. 5) and AHA (Col. 7) on 25 benchmarks sorted by lines of code (Col. 2). We also report the number \nof iterations needed by Brahma (Col. 3), ratio of run\u00adtimes of Brahma and Sketch (Col. 6) and the number \nof candidate solutions found by AHA (within brackets in Col. 7). 7.1 Performance of Synthesis Algorithm \nTable 1 reports some interesting statistics about the synthesis algo\u00adrithm (presented in Fig. 3). The \ntotal time taken by the algorithm (col. 4) on the various examples varies between 1.0 to 2778.7 sec\u00adonds. \nWe also report the number of iterations taken by the loop (col. 3) inside our constraint solving algorithm \nin Fig. 2 while per\u00adforming the re.ned counterexample guided iterative synthesis. The small number of \nthese iterations (which varies between 2 to 14) illustrates the effectiveness of our technique in using \ncounterexam\u00adples for iterative synthesis. There has been a huge investment in building formal reasoning \ntechnology for performing veri.cation of hardware or software sys\u00adtems. In this paper, we show that this \nveri.cation technology can be lifted to perform synthesis. In that context, the number of iterations \nrequired by our technique points out the extra factor of computa\u00adtional resources required to go from \nveri.cation to synthesis. The largest example in our experimental evaluation took over 45 min\u00adutes but \nit involved only 11 iterations. Hence, the largest SAT prob\u00adlem solved during synthesis is roughly 11 \ntimes the size of the SAT problem for veri.cation. Any improvement in satis.ability solvers for veri.cation \nwould also directly increase the scalability of our technique. Ratio Normalized No. of Runtime of Runtime \nConstraint Size Comps. Brahma Sketch Sketch/Brahma Brahma Sketch 123 456 2 0.11 0.27 2.45 1 1 3 0.14 \n0.83 5.93 1.52 5.00 4 0.20 2.09 10.45 1.91 19.85 5 0.25 6.78 27.12 2.36 48.01 6 0.36 19.69 54.70 3.18 \n129.26 7 0.33 164.80 499.39 3.76 242.04 Table 2. Comparing Brahma and Sketch on running example by increasing \nthe number of components. Constraint size is normal\u00adized with respect to the size for 2 components. \n 7.2 Comparison with Sketch and AHA We experimentally compared the implementation of our synthe\u00adsis technique \nBrahma with two other existing tools for synthesis, namely Sketch and AHA. Sketch The Sketch tool [32, \n33] takes as input a sketch a pro\u00adgram with holes and synthesizes programs by correctly .lling the \nholes. Hence, for comparing Sketch with Brahma, we expressed the component based design problem as a \nsketch. There are many dif\u00adferent ways of achieving this, and after consultation with the Sketch team, \nwe picked one encoding that produces a sketch that has size linear in the size of the component based \nsynthesis problem. We note that implicit in the sketch is an upper bound on how many times a component \ncan be used. The Sketch tool can not solve the unbounded component based synthesis problem. We used version \nv1.3.0 of Sketch for comparison with our technique. At a high level, the approach used in Sketch for \nsynthesis is similar to the approach used by Brahma both generate constraints in the .rst step and then \nuse off-the-shelf solvers to solve these constraints in the second step. However, there are fundamental \ndifferences in the constraints generated by the two techniques as well as the algorithms used for solving \nthe constraints. Sketch internally generates Boolean constraints, which are solved using Boolean satis.ability \nsolvers. Brahma generates for\u00admulas in a richer logic, which are solved using Satis.ability Mod\u00adulo Theory \n(SMT) solvers. As a result, Sketch performs poorly when there is reasoning in a theory, such as linear \narithmetic, in\u00advolved. Sketch times out on the arithmetic examples, Q1 and Q2, whereas Brahma easily \nsynthesizes them. (The description of Q1-Q2 may appear to involve nonlinear expressions, but nonlinear \nreasoning can be eliminated easily and SMT solvers can be used). Since Sketch is a generic tool, and \nnot tailored for component based synthesis, it uses a suitably general translation of the syn\u00adthesis \nproblem into a Boolean constraint. Consequently, while the size of the (SMT) constraints generated by \nBrahma is provably quadratic in the number of components, experimental evidence in\u00addicates that the size \nof the constraints generated by Sketch is either exponential or a high degree polynomial in the number \nof holes or components. To highlight this difference, we used Brahma and Sketch to synthesize the running \nexample, but we successively in\u00adcreased the number of components in the library. The time taken by Sketch, \nreported in Col. 3 of Table 2, appears to scale expo\u00ad nentially, whereas the time taken by Brahma, reported \nin Col. 2 of Table 2, appears to scale non-exponentially as the number of com\u00ad ponents in the library \nincreases from 2 to 7. The ratio of Sketch runtime to Brahma runtime, shown in col. 4 of Table 2, increases \nfrom 2 to nearly 500. In Table 2, we also show the normalized size of the constraints generated by both \ntechniques against the number of components (Col. 5 and Col. 6 for Brahma and Sketch respec\u00adtively). \nWe normalize the size of the constraints with respect to the Benchmark P1 35 18 1.94 P2 11 16 0.69 P3 \n98 57 1.72 P4 58 31 1.87 P5 59 45 1.31 P6 78 32 2.43 P7 03 11 0.27 P8 78 66 1.18 P9 14 08 1.75 P10 48 \nNA NA P11 29 NA NA P12 29 NA NA P13 12 16 0.75 P14 69 38 1.82 P15 108 56 1.93 P16 77 41 1.88 P17 109 \n78 1.40 P18 72 47 1.53 P19 64 52 1.23 P20 96 NA NA P21 42 NA NA P22 127 NA NA P23 103 NA NA P24 62 NA \nNA P25 184 NA NA Veri.cation Runtime(ms) Brahma Sketch Ratio  Table 3. Comparing the veri.cation times \nof Brahma and Sketch. Timeout was 1 hour. For the similar veri.cation step, Sketch is slower only by \nan average factor of 1.4 (maximum factor is 2.43) on all examples. NA denotes that Sketch timeouts on \nthat example and hence there is no veri.cation time. For the algorithmically-different synthesis step, \nas shown in Table 1, Sketch was slower by a factor of 20 on examples on which it terminates so, even \nif we normalize for use of different constraint solvers, sketch continues to be an order-of-magnitude \nslower. constraint size for 2 components. This ensures a fair comparison of the rate of increase in constraint \nsize with increase in number of components for the two tools irrespective of the absolute size of the \ngenerated constraints which may depend on optimizations and pre\u00adprocessing. Clearly, for both tools, \nthe runtime is correlated with the size of the constraint, but constraints generated by Brahma are much \nmore succinct and scale better than Sketch. The total runtime of Sketch on the bitvector examples is \npre\u00adsented in col. 5 in Table 1. Sketch times out on 6 examples and is slower by an average factor of \nover 20 on other examples (col. 6). One might speculate that the runtime gains of Brahma over Sketch \narise because Brahma uses a different SMT solver. However, this is not true, since Brahma and Sketch \nare experimentally observed to take comparable time for performing the veri.cation step; see Table 3. \nIt follows that the differences are entirely due to the algo\u00ad rithmic improvements in Brahma. AHA The \nAHA tool [4] is a superoptimizer, endorsed by our benchmark book [37] as A Hacker s Assistant. It is \nbased on an idea by Henry Massalin [28], and was made widely available by Granlund and Kenner as the \nGNU superoptimizer [11]. For exper\u00ad imental comparison, we provided the set of base components as a set \nof library functions. AHA enumerates all possible composi\u00adtion of these functions to generate candidate \nprograms (in a way described in Figure 1), but it tests the correctness of the candidate programs only \non some inputs, and often outputs a number of po\u00adtential solutions. The solutions produced by AHA must \nbe veri.ed in order to select the right solution. Table 1 lists the total number of solutions generated \nby AHA (col. 7 within [brackets]) and the total time (col. 7) taken for generation and veri.cation of \nthese so\u00adlutions. AHA times out on 12 examples. The better performance of Brahma is explained by the \nfact that Brahma does not perform an exhaustive enumeration of the exponential state space, but re\u00adlies \non a non-trivial strategy of candidate selection and elimination though SMT solving. Thus, we exploit \nthe engineering advances in the underlying SMT solving technology for an ef.cient search.  7.3 Choice \nof Multi-set of Base Components We now discuss the strategy that we used for choosing the multi\u00adset of \nbase components for our benchmark examples. Picking the multi-set of base components is the only step \nin our approach that currently requires human guidance. In our experiments, we started with a common \nmulti-set of base components, referred to as the standard library, for all bench\u00admarks. The standard \nlibrary included 12 components, one each for performing standard operations, such as bitwise-and, bitwise-or, \nbitwise-not, add-one, bitwise-xor, shift-right, comparison, add, and subtract operations. The standard \nlibrary was suf.cient for synthe\u00adsizing the .rst 17 benchmark examples. For other examples, the library \nwas augmented with a set of new components suggested by the user. We call this set the extended library. \n(Giving user the option to extend libraries can facilitate hierarchical synthesis the user can specify \na synthesized program as a new component in the extended library.) For the above-mentioned incremental \ndesign technique to be successful, it is pertinent that the synthesis engine not only synthe\u00adsize correct \ndesigns quickly but also report infeasibility of the syn\u00adthesis problem quickly. In our experiments, \nwe noted that Brahma reports infeasibility of design rather quickly. More speci.cally, when the standard \nlibrary was insuf.cient, Brahma terminated in less than 100 seconds on almost all examples. Hence, reliance \non human guidance can be reduced using a strategy where compo\u00adnents are successively added to the library \nuntil synthesis is suc\u00adcessful. Regarding the issue of synthesis of optimal designs designs that use \nthe minimal number of components we observed that in experiments, we always got minimal designs. However, \nthis is not a guarantee. Minimality can, however, be ensured by iteratively removing each component as \nlong as a design exists. 8. Related Work The component-based synthesis problem was recently indepen\u00addently \nformulated for the case when the components were .nite\u00adstate machines with outputs (transducers) [25]. \nOur formulation allows for high-level components whose speci.cations are given using logical formulas \nin rich theories. More recently, linear-time programs were synthesized from speci.cation automata recogniz\u00ading \nthe input/output relation [15]. Counterexample Guided Inductive Synthesis Inductive synthe\u00adsis refers \nto the process of generating a system from input-output examples. This process involves using each new \ninput-output ex\u00adample to re.ne the hypothesis about the system until convergence is reached. Inductive \nsynthesis had its origin in the pioneering work by Gold on language learning [10] and by Shapiro on algorith\u00ad \nmic debugging and its application to automated program construc\u00adtion [30]. The inductive approach [9, \n29] for synthesizing a pro\u00ad gram involves debugging the program with respect to positive and negative \nexamples until the correct program is synthesized. The negative examples can be counterexamples discovered \nwhile try\u00ading to prove a program s correctness. Counterexamples have been used in incremental synthesis \nof programs [17, 33] and switching logic for hybrid systems [20]. We have recently extended the ideas \ndescribed in this paper to provide an alternative to writing formal speci.cations for synthe\u00adsis [19], \nwherein logical speci.cation of the desired program is re\u00ad placed by an input-output oracle. This is \nespecially important for the application of software deobfuscation. Quite interestingly, this also obviates \nthe need for having a veri.er, albeit at the cost of introducing potential unsoundness in the process. \nIn contrast, the approach in this paper makes use of a veri.er and we show how to transform a veri.er \ninto a synthesizer. This theme of transforming a veri.er into a synthesizer is also present in recent \nwork [34, 36], where the focus is on synthesizing code fragements along with in\u00adductive invariants as \n.rst-order instantiations of user-provided tem\u00adplates. In contrast, our focus in this paper is restricted \nto straight\u00adline code fragments, but with the bene.t of requiring the user to only provide a multi-set \nof required components. Automated API Composition The Jungloid mining tool [26] syn\u00ad thesizes code-fragments \n(over a given set of API methods anno\u00adtated with their type signatures) given a simple query that describes \nthe desired code in terms of input and output types. We push this work forward to synthesizing code-fragments \nthat meet a functional speci.cation as opposed to simply type speci.cations. Typing con\u00adstraints can \nalso be easily incorporated in our synthesis constraints. DIPACS [21] compiler incorporates an AI planner \nto replace a call of a programmer-de.ned abstract algorithm with a sequence of library calls. It uses \nprogrammer-compiler interaction to prune undesirable compositions. DIPACS requires the library (or applica\u00adtion) \nprogrammer to specify behavior of the library procedures (or, desired effect of the abstract algorithm) \nusing high-level abstrac\u00adtions, such as predicates sorted and permutation. Furthermore, it then needs \naxioms for these predicates. This is similar to some early work on automatic program synthesis [27, 35], \nwhere a theorem prover was used instead of an AI planner. This early work was later extended [8] to perform \nschema-guided deductive synthesis. Our approach does not use abstract predicates and axioms and relies \non the predicates provided by the SMT solver. Our approach can only solve synthesis problems whose formalization \ncan be done in SMT-supported theories, whereas the work on deductive synthesis uses more general-purpose \ndeductive engines, but at the price of requiring axiomatization and performing incomplete reasoning. \nIn our approach, the SMT solver reasons about the implicit theories using decision procedures. Sketching \nSketching [31 33] relies on the developer to come up with the algorithmic insight and uses the sketch \ncompiler to .ll in missing details using counterexample guided inductive synthesis. In contrast, our \ntool seeks to discover algorithmic insights, albeit at cost of being more suited for a special class \nof programs. We chose bitvector programs as our main application domain since coming up with algorithmic \ninsight is the hard part here. Super-optimizers Superoptimization is the task of .nding an op\u00adtimal code \nsequence for a straight-line target sequence of instruc\u00adtions, and it is used in optimizing performance-critical \ninner loops. One approach to superoptimization has been to simply enumerate sequences of increasing length \nor cost, testing each for equality with the target speci.cation [28]. Another approach has been to constrain \nthe search space to a set of equality-preserving trans\u00adformations expressed by the system designer [22] \nand then select the one with the lowest cost. Recent work has used superoptimiza\u00adtion [5, 6] to automatically \ngenerate general purpose peephole op\u00ad timizers by optimizing a small set of instructions in the code. \nIn these approaches, the exhaustive state space search is quite expen\u00adsive making them amenable to only \ndiscovering optimal instruc\u00adtions of length four or less in reasonable amount of time. Use of satis.ability \nsolving for synthesis SAT solvers have been used for synthesis previously: Massalin [28] used them for \nveri.\u00ad cation of candidate synthesized programs and Sketching [33] used them to implement the inductive \nprogram synthesis technique. We use SMT solving to implement our algorithm for solving synthesis constraints. \nThis makes our synthesis approach more ef.cient as well as more general. 9. Conclusion and Future Work \nProgram synthesis has the potential to revolutionize the process of system development. Up until recently, \nautomating synthesis of non-trivial programs has mostly been impractical. However, huge engineering advances \nin logical reasoning have signi.cantly changed the landscape. These advances have enabled veri.cation \nof large systems, and in this paper, we show that they can also be used to synthesize 10-20 line programs \nwith some help from the user and using computational resources that are within one order of magnitude \nof the resources required for veri.cation. We have applied our component-based synthesis methodology \nto synthesis of bit-vector circuits. However, our solution applies more generally to other naturally \nresource-constrained domains, such as loop-free assembly of physical components (e.g., FPGA circuits, \nor even some physical/chemical/biological systems). In such situations, the user naturally starts out \nwith resource con\u00adstraints that provide a good over-approximation of the multi-set of available components. \nWe also foresee generalizations of our formulation of the component-based synthesis problem. This includes \nsynthesizing programs with richer control structure, such as loops and recursion, and synthesizing from \npartial speci.cations. There is also potential for using richer theories, and limited .rst-order reasoning \nthat is supported by modern SMT solvers, to synthesize from components whose speci.cations use richer \nlogical formulas. A followup prob\u00adlem worth investigating is to approximate some functionality using \na given multi-set of components. We believe that our paper lays down the foundation for investigating \nsuch a line of work. Acknowledgment We thank Viktor Kuncak, Rishabh Singh and other members of the Sketch \nteam, and the anonymous reviewers for their help and valuable feedback. References [1] Satis.ability \nmodulo theories competition (smt-comp). http://www.smtcomp.org/2009/index.shtml. [2] SMTLIB: Satis.ability \nmodulo theories lib. http://smtlib.org. [3] Yices: An SMT solver. http://yices.csl.sri.com. [4] The AHA! \n(A Hacker s Assistant) Superoptimizer, 2008. Down\u00adload: http://www.hackersdelight.org/aha.zip, Documenta\u00adtion: \nhttp://www.hackersdelight.org/aha/aha.pdf. [5] S. Bansal and A. Aiken. Automatic generation of peephole \nsuperopti\u00admizers. In ASPLOS, 2006. [6] S. Bansal and A. Aiken. Binary translation using peephole superopti\u00admizers. \nIn OSDI, 2008. [7] R. Cytron, J. Ferrante, B. K. Rosen, M. N. Wegman, and F. K. Zadeck. An ef.cient method \nof computing static single assignment form. In POPL, 1989. [8] B. Fischer and J. Schumann. Autobayes: \nA system for generating data analysis programs from statistical models. J. Funct. Program., 13(3):483 \n508, 2003. [9] P. Flener and L. Popelmnsky. On the use of inductive reasoning in program synthesis: Prejudice \nand prospects. In LOBSTR. 1994. [10] E. M. Gold. Language identi.cation in the limit. Information and \nControl, 10(5):447 474, 1967. [11] T. Granlund and R. Kenner. Eliminating branches using a superopti\u00admizer \nand In PLDI, 1992. [12] S. Gulwani. Dimensions in program synthesis. In PPDP, pages 13 24. ACM, 2010. \n[13] S. Gulwani. Automating string processing in spreadsheets using input\u00adoutput examples. In POPL, 2011. \n[14] S. Gulwani, V. Korthikanti, and A. Tiwari. Synthesizing geometry constructions. In PLDI, 2011. [15] \nJ. Hamza, B. Jobstmann, and V. Kuncak. Synthesis for regular speci\u00ad.cations over unbounded domains. In \nFMCAD, 2010. [16] B. Harris and S. Gulwani. Spreadsheet table transformations from examples. In PLDI, \n2011. [17] S. Itzhaky, S. Gulwani, N. Immerman, and M. Sagiv. A simple inductive synthesis methodology \nand its applications. In OOPSLA, pages 36 46, 2010. [18] R. N. Jackiw and W. F. Finzer. The geometer \ns sketchpad: program\u00adming by geometry. In Watch what I do: programming by demonstra\u00adtion, pages 293 307. \nMIT Press, 1993. [19] S. Jha, S. Gulwani, S. Seshia, and A. Tiwari. Oracle-guided component-based program \nsynthesis. In ICSE, pages 215 224, 2010. [20] S. Jha, S. Gulwani, S. Seshia, and A. Tiwari. Synthesizing \nswitching logic for safety and dwell-time requirements. In Proc. 1st ACM/IEEE Intl. Conf. on Cyber-Physical \nSystems, ICCPS, pages 22 31, 2010. [21] T. A. Johnson and R. Eigenmann. Context-sensitive domain\u00adindependent \nalgorithm composition and selection. In PLDI, 2006. [22] R. Joshi, G. Nelson, and K. H. Randall. Denali: \nA goal-directed superoptimizer. In PLDI, 2002. [23] D. E. Knuth. The art of computer programming. http:// \nwww-cs-faculty.stanford.edu/~knuth/taocp.html. [24] T. A. Lau, P. Domingos, and D. S. Weld. Version space \nalgebra and its application to programming by demonstration. In ICML, 2000. [25] Y. Lustig and M. Vardi. \nSynthesis from component libraries. In Proc. FoSSaCS, pages 395 409, 2009. [26] D. Mandelin, L. Xu, R. \nBod\u00b4ik, and D. Kimelman. Jungloid mining: helping to navigate the API jungle. In PLDI, pages 48 61, 2005. \n[27] Z. Manna and R. Waldinger. A deductive approach to program syn\u00adthesis. ACM TOPLAS, 2(1):90 121, \n1980. [28] H. Massalin. Superoptimizer -a look at the smallest program. In ASPLOS, pages 122 126, 1987. \n[29] S. Muggleton, editor. Inductive Logic Programming, volume 38 of The APIC Series. Academic Press, \n1992. [30] E. Y. Shapiro. Algorithmic Program DeBugging. MIT Press, Cam\u00adbridge, MA, USA, 1983. [31] A. \nSolar-Lezama, G. Arnold, L. Tancau, R. Bod\u00b4ik, V. A. Saraswat, and S. A. Seshia. Sketching stencils. \nIn PLDI, pages 167 178, 2007. [32] A. Solar-Lezama, R. Rabbah, R. Bod\u00b4ik, and K. Ebcioglu. Program\u00adming \nby sketching for bit-streaming programs. In PLDI, 2005. [33] A. Solar-Lezama, L. Tancau, R. Bod\u00b4ik, S. \nSeshia, and V. Saraswat. Combinatorial sketching for .nite programs. In ASPLOS, 2006. [34] S. Srivastava, \nS. Gulwani, and J. S. Foster. From program veri.cation to program synthesis. In POPL, pages 313 326, \n2010. [35] M. Stickel, R. Waldinger, M. Lowry, T. Pressburger, and I. Under\u00adwood. Deductive composition \nof astro. software from subroutine li\u00adbraries. In CADE, 94. [36] A. Taly, S. Gulwani, and A. Tiwari. \nSynthesizing switching logic using constraint solving. In VMCAI, pages 305 319. Springer, 2009. [37] \nH. S. Warren. Hacker s Delight. Addison-Wesley, 02. [38] I. H. Witten and D. Mo. TELS: learning text \nediting tasks from examples. In Watch what I do: programming by demonstration, pages 293 307. MIT Press, \nCambridge, MA, USA, 1993.  \n\t\t\t", "proc_id": "1993498", "abstract": "<p>We consider the problem of synthesizing loop-free programs that implement a desired functionality using components from a given library. Specifications of the desired functionality and the library components are provided as logical relations between their respective input and output variables. The library components can be used at most once, and hence the library is required to contain a reasonable overapproximation of the multiset of the components required.</p> <p>We solve the above component-based synthesis problem using a constraint-based approach that involves first generating a synthesis constraint, and then solving the constraint. The synthesis constraint is a first-order &#8707;&#8704; logic formula whose size is quadratic in the number of components. We present a novel algorithm for solving such constraints. Our algorithm is based on counterexample guided iterative synthesis paradigm and uses off-the-shelf SMT solvers.</p> <p>We present experimental results that show that our tool Brahma can efficiently synthesize highly nontrivial 10-20 line loop-free bitvector programs. These programs represent a state space of approximately 20<sup>10</sup> programs, and are beyond the reach of the other tools based on sketching and superoptimization.</p>", "authors": [{"name": "Sumit Gulwani", "author_profile_id": "81100315615", "affiliation": "Microsoft Research, Redmond, Washington, USA", "person_id": "P2690480", "email_address": "sumitg@microsoft.com", "orcid_id": ""}, {"name": "Susmit Jha", "author_profile_id": "81438592259", "affiliation": "University of California, Berkeley, California, USA", "person_id": "P2690481", "email_address": "jha@eecs.berkeley.edu", "orcid_id": ""}, {"name": "Ashish Tiwari", "author_profile_id": "81100398928", "affiliation": "SRI International, Menlo Park, USA", "person_id": "P2690482", "email_address": "tiwari@csl.sri.com", "orcid_id": ""}, {"name": "Ramarathnam Venkatesan", "author_profile_id": "81100057891", "affiliation": "Microsoft Research, Redmond, Washington, USA", "person_id": "P2690483", "email_address": "venkie@microsoft.com", "orcid_id": ""}], "doi_number": "10.1145/1993498.1993506", "year": "2011", "article_id": "1993506", "conference": "PLDI", "title": "Synthesis of loop-free programs", "url": "http://dl.acm.org/citation.cfm?id=1993506"}