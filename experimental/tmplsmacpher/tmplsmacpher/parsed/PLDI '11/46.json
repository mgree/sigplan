{"article_publication_date": "06-04-2011", "fulltext": "\n Exploiting the Commutativity Lattice * Milind Kulkarni School of Electrical and Computer Engineering \nPurdue University milind@purdue.edu Abstract Speculative execution is a promising approach for exploiting \npar\u00adallelism in many programs, but it requires ef.cient schemes for detecting con.icts between concurrently \nexecuting threads. Prior work has argued that checking semantic commutativity of method invocations is \nthe right way to detect con.icts for complex data structures such as kd-trees. Several ad hoc ways of \nchecking com\u00admutativity have been proposed in the literature, but there is no sys\u00adtematic approach for \nproducing implementations. In this paper, we describe a novel framework for reasoning about commutativity \nconditions: the commutativity lattice. We show how commutativity speci.cations from this lattice can \nbe systematically implemented in one of three different schemes: ab\u00adstract locking, forward gatekeeping \nand general gatekeeping. We also discuss a disciplined approach to exploiting the lattice to .nd different \nimplementations that trade off precision in con.ict de\u00adtection for performance. Finally, we show that \nour novel con.ict detection schemes are practical and can deliver speedup on three real-world applications. \nCategories and Subject Descriptors D.1.3 [Programming Tech\u00adniques]: Concurrent Programming Parallel Programming; \nD.3.3 [Programming Languages]: Language Constructs and Features Concurrent Programming Structures General \nTerms Languages Keywords Commutativity lattice, transactions, optimistic paral\u00adlelism 1. Introduction \nSpeculative execution of high-level language programs has emerged as an important concept in multicore \nprogramming [12, 19, 22]. A central concern in implementing such systems is detecting con.icts accurately \nand with low overhead. These two goals are usually contradictory. For example, a single global lock on \nall of shared\u00admemory will detect con.icts with low overhead, but it prevents par\u00ad * This work is supported \nin part by NSF grants 0923907, 0833162, 0719966, and 0702353 and by grants from IBM, NEC and Intel. Permission \nto make digital or hard copies of all or part of this work for personal or classroom use is granted without \nfee provided that copies are not made or distributed for pro.t or commercial advantage and that copies \nbear this notice and the full citation on the .rst page. To copy otherwise, to republish, to post on \nservers or to redistribute to lists, requires prior speci.c permission and/or a fee. PLDI 11, June 4 \n8, 2011, San Jose, California, USA. Copyright c &#38;#169; 2011 ACM 978-1-4503-0663-8/11/06. . . $10.00 \nDonald Nguyen1, Dimitrios Prountzos1 , Xin Sui1, Keshav Pingali1,2 1Department of Computer Science, \n2Institute for Computational Engineering and Sciences The University of Texas at Austin {ddn, dprountz, \nxinsui, pingali}@cs.utexas.edu allel execution of speculative computations. Transactional memory (TM) \nis a better approach that tracks the set of locations (or ob\u00adjects) read and written by each transaction \nand reports con.icts if a location written by one transaction is read or written by an\u00adother concurrently \nexecuting transaction [12]. This results in more .ne-grained con.ict detection than the .rst scheme and \npermits much more concurrency but at the expense of potentially higher overhead, especially if the TM \nis implemented in software. Though the TM approach works well for many programs, it has been pointed \nout in the literature [10, 18, 25] that con.ict detection at the memory level can be overly conservative \nfor programs that use complex abstract data types (ADTs). Consider the disjoint-set union-.nd data structure. \nTwo transactions might both invoke the .nd method, and these invocations may result in modi.cations to \nthe concrete (internal) representation of the data structure because of path compression, causing memory-level \ncon.icts1. However, from the ADT perspective, .nd operations do not modify the dis\u00adjoint sets; they are \nread-only operations that commute with each other and do not con.ict. Intuitively, there may be several \nconcrete states of the data structure that represent the same abstract state; if the client of the ADT \nonly cares about the abstract state, memory\u00adlevel con.ict detection can be overly conservative, reducing \ncon\u00adcurrency. This problem can also arise with other ADTs such as sets and kd-trees, as we discuss later \nin the paper. Although issues surrounding the behavior of commutativity based con.ict detection have \nbeen explored in prior work [10, 15, 25], no work has discussed how such semantic con.ict checks should \nbe implemented. It is straightforward to see how memory\u00adlevel con.ict checking can be performed for a \ncomplex data structure, but it is far less apparent how to implement a correct and ef.cient con.ict detection \nscheme based on the semantics of union-.nd. Worse, every abstract data type potentially requires a different \ncon.ict-detection scheme. Prior work has relied on ad hoc implementations for the speci.c data structures \nthey focus on [4, 10, 18, 20], but what is missing is a systematic way of gen\u00aderating correct con.ict \ndetectors. Complicating matters further is that for a given abstract data type, there may be multiple \ncorrect con.ict detection schemes that nevertheless provide different amount of parallelism at different \noverheads. For example, Ni et al. present a con.ict checker for sets based on read/write locks on keys \ninserted into the set [20]. In contrast, Herlihy and Koskinen propose a con.ict checker for sets based \non exclusive locks on keys [10]. Given two con.ict checkers, how do we know which one to use? Which one \nwill expose more parallelism? Which one will have higher overhead? Intuitively, a more complex scheme \nmay be able to expose more parallelism at 1 Ironically, a simpler ADT implementation for union-.nd that \ndoes not per\u00adform path compression would let the two invocations proceed concurrently! the cost of additional \nrun-time overhead; is this always the case? Can we produce multiple con.ict checkers for a data structure \nin a disciplined way? This paper addresses these issues through a novel way of rea\u00adsoning about commutativity \nin abstract data types: a lattice of com\u00admutativity speci.cations. A commutativity speci.cation is a \nset of predicates associated with pairs of methods in a data structure; a pair of methods commute if \nthe predicate associated with them evaluates to true. There are many possible commutativity spec\u00adi.cations \nfor a data structure, and Section 2 shows that they can be arranged into a commutativity lattice, ordered \nby the amount of parallelism they permit. Section 3 shows how commutativity speci.cations can be sys\u00ad \ntematically turned into commutativity checking implementations. We present three novel constructions: \none that generates abstract lock-based schemes and two that generate novel con.ict detection schemes \ncalled gatekeepers. Section 4 shows that different commu\u00ad tativity checking implementations for a data \nstructure are actually implementations of different speci.cations from the commutativ\u00adity lattice, and \ndescribes a disciplined way of producing different implementations for a given data structure. Section \n5 explores the tradeoff between performance and precision, and shows that our systematically constructed \ncon.ict detectors can deliver scalable performance. Section 6 concludes and presents related work. Contributions \nThis paper makes the following contributions: 1. We give a de.nition of commutativity conditions and \nspeci.\u00adcations, and describe a lattice-theoretic approach to reasoning about various commutativity speci.cations \nfor a data structure. 2. We present three approaches for implementing commutativ\u00adity speci.cations, \nabstract locking, forward gatekeeping and general gatekeeping. We develop algorithms for systematically \nconstructing commutativity checkers of each type given a spec\u00adi.cation. We discuss the expressivity of \neach scheme, and their overheads. 3. We show for three real-world applications that our commuta\u00adtivity \nschemes are practical. We quantify the potential tradeoffs between the parallelism exposed and the overhead \nincurred by different schemes. We also show, paradoxically, that for some benchmarks the higher-parallelism \nscheme in fact has lower overhead.  2. Commutativity We begin in Section 2.1 by brie.y explaining how \ncommutativity of method invocations can be used to check serializability. Section 2.2 next de.nes commutativity \nconditions, which are predicates a run\u00adtime system can use to guarantee serializability. Section 2.3 \nde\u00ad scribes the language used to specify commutativity conditions. We then present a lattice-theoretic \nview of commutativity speci.cations in Section 2.4. Section 2.5 provides example speci.cations for a \nfew data structures, illustrating the potential complexity of com\u00admutativity properties. 2.1 Serializability \nthrough commutativity Our goal is to ensure that two transactions, A and B, that execute concurrently \nmaintain transactional semantics. In particular, we want to ensure that A and B are serializable: that \nthe parallel execution of A and B produce the same results as if A and B were executed sequentially in \nsome order2. We consider data structures that provide a set of methods m .M. The data structure s state, \ns, consists of any informa\u00ad 2 The following discussion will assume that there are only two transactions \nexecuting, and that the transactions access a single data structure; the ap\u00adproach to guaranteeing serializability \ngeneralizes straightforwardly. tion needed to capture the abstract behavior of the structure. For example, \na set data structure s abstract state is the set of elements contained in the data structure. In general, \nmany concrete represen\u00adtations of a data structure have the same abstract state. For example, a set represented \nas a linked list and one represented as a hash set will have different concrete states, but, if they \ncontain the same elements, would have the same abstract state. The program consists of a series of method \ninvocations per\u00adformed by transactions on the data structure. Transaction T in\u00advoking a method m with \nargument v in state s will be written (m T (v))s. We will omit arguments, states and transaction labels \nunless necessary for clarity. Each of these method invocations is atomic: when a method m is invoked, \nit appears to execute instantaneously at some point be\u00adtween when m was invoked and when it returned. \nMore formally, we consider linearizable data structures [13]. Note that this means we need only consider \ninterleavings of transactions at the granular\u00adity of method invocations [15]. A history is a series of \nmethod invocations performed by one or more transactions, starting from a particular state. The history \nrecords method invocations as well as their return values3. Note that because the invocations are atomic, \nthe return values are available immediately after the method is invoked. Histories will be written as: \n(m1(v1),m2(v2),...,mk(vk))s, indicating a series of method invocations starting from state s. We can \nalso discuss sub-histories, which are subsequences of larger histories; as these sub-histories also have \nsome initial state, they can be considered histories in their own right. Two histories H and H' are equivalent, \ndenoted H = H', if their initial states are the same, they contain of the same method invocations (albeit \npotentially in a different order), each invocation returns the same result, and the states at the end \nof the histories are the same. Note that if we consider two equivalent sub-histories h and h', where \nh is a sub-history of some larger history H, replacing h with h' in H produces an equivalent history \nH'. We can de.ne commutativity in terms of history equivalence: De.nition 1. Two invocations m1(v1) \nand m2(v2) commute with respect to a state s if and only if (m1(v1),m2(v2))s = (m2(v2),m1(v1))s. In other \nwords, two method invocations commute with respect to a given state if they are invoked consecutively \nstarting in that state, and interchanging them produces an equivalent history4. We can use the commutativity \nof method invocations performed by A and B to check if their execution is serializable. Intuitively, \nif every method invocation in A commutes with every method in\u00advocation in B, then A and B are serializable, \nand B can be seen as executing before A. This type of serializability can be thought of as an extension \nof d-serializability [21] from non-interfering method invocations to commuting method invocations. As \nthis approach has been used by several prior works ([10, 15, 18]), we present only an informal inductive \nargument here. Assume that the .rst invocation performed by transaction A is (m1(v1)A)s1 , producing \nsome new state s'. When transaction B later performs its last invocation (m2(v2)B)s2 , a run-time check \nmust be performed. If s2 is s' (meaning each transaction has in\u00advoked one method), it suf.ces to show \nthat m1(v1)A and m2(v2)B commute with respect to s1. This check allows the methods to be 3 Because return \nvalues are determined by method arguments and state, we do not explicitly represent the return values \nin the history. 4 Prior work has distinguished between left-moving and right-moving com\u00admutativity, which \nconstrain in which directions method invocations can be moved [15]. In this paper, we consider both-moving \ncommutativity, where invocations can be moved in either direction. B interchanged in the history, producing \na history where m2 (v2) A executes before m1 (v1). In most cases, however, s2 will be separated from \ns1 by some intervening invocations by both A and B. It is apparent that we can AB move m1 (v1) and m2 \n(v2) towards each other by proving that the invocations commute with the intervening methods and swapping \ntheir positions in the history (hence pushing all later invocations B from A after m2 (v2), and all earlier \ninvocations from B before AA m1 (v1)). At this point, let s1 ' be the state m1 (v1) in which is B actually \ninvoked, while m2 (v2) is invoked in the next state, s2' . If AB we can then prove that m1 (v1) and m2 \n(v2) commute with respect to s1' , we can swap the two invocations and produce a serial history where \nall invocations from B occur before any invocations from A.  2.2 Commutativity conditions It is apparent \nfrom the previous discussion that, if we could .nd predicates .m1;m2 (s, v1,v2), for all pairs of methods \nm1 and m2, that were true for histories (m1(v1),m2(v2))s if and only if the two method invocations commuted \nin that history, we could check this formula as part of a run-time system that veri.ed that two concurrently \nexecuting transactions were serializable. Furthermore, such predicates clearly exist (even if they are \nnot easily expressible in a particular logic, or, for method invocations that don t commute, they are \nmerely false). Unfortunately, in a realistic run-time system, method invo\u00adcations from two transactions \nwill not appear back to back in a program s execution history. Instead, transaction A might in\u00advoke (m1(v1))s1 \nand, much later, transaction B might invoke (m2(v2))s2 . What we desire is a commutativity condition: \na predi\u00adcate over method invocations that could occur in any two states that nevertheless lets us prove \nthe commutativity of method invocations when they appear back-to-back and must be swapped. To make the \nde.nition of commutativity conditions more clear, we .rst de.ne a more re.ned equivalence properties \nfor histories: De.nition 2. Two histories H and H ' are C-EQUIVALENT, de\u00adnoted H =C H ', iff H can be \ntransformed into H ' by replacing sub-histories of the form (m1(v1),m2(v2))s with equivalent sub\u00adhistories \nof the form (m2(v2),m1(v1))s. In other words, two histories are C-EQUIVALENT if one can be transformed \ninto the other by swapping commuting method invocations. We can now de.ne commutativity conditions, which \nare over two method invocations that occur in two non-consecutive states in some history H: De.nition \n3. A commutativity condition, f, is a predicate on two method invocations (m1(v1))s1 and (m2(v2))s2 in \nan execution history H that is true only if, for all histories H ' =C H that con\u00adtain a sub-history h \n= (m1(v1),m2(v2))s1 , m1(v1) and m2(v2) commute with respect to s ' . In other words, a commutativity \ncondition is a predicate f that, when true in one history, means that in any C-EQUIVALENT history where \nthe two method invocations happen back to back, the invo\u00adcations commute. We say that a predicate is \na valid commutativity condition if it satis.es the requirements of De.nition 3. In practice, these conditions \ncan be expressed as a particular for\u00admula for a pair of methods, parameterized on the arguments, return \nvalue and states of the method invocations. Such commutativity formulae are thus written fm1;m2 (s1,v1,r1,s2,v2,r2) \n(where r1 and r2 are the return values of (m1(v1))s1 and (m2(v2))s2 , re\u00adspectively, and can be read \ninformally as: (m1(v1))s1 /r1 commutes with (m2(v2))s2 /r2 if f If, for each pair of method invocations \n(m1(v1))s1 and (m2(v2))s2 invoked by concurrently executing transactions, the run-time sys\u00adtem ensures \nthat fm1;m2 (s1,v1,r1,s2,v2,r2) is true, then the current execution is serializable (a proof of this \nclaim is given in Appendix A). If one of these commutativity checks does not succeed, then the run-time \ncan preserve serializability by undoing one transac\u00adtion and re-executing it later when it will not be \nconcurrent with the con.icting transaction. The implementation of commutativity checks is the subject \nof Section 3. Discussion Commutativity conditions are somewhat complex, and producing a valid commutativity \ncondition for a pair of methods may seem daunting. While this paper does not address the construction \nand correctness of commutativity conditions (the veri.cation problem, though not the construction problem, \nis addressed by techniques such as [14]), we discuss some interesting properties of commuta\u00ad tivity conditions \nthat can inform their speci.cation and veri.cation. First, most commutativity conditions are expressible \nsolely in terms of the arguments and return values of an invocation. While the return values are dependent \non the states the invocations occur in, return values have a distinguished place in the de.nition of \ncom\u00admutativity (speci.cally, return values must remain the same when two commuting method invocations \nare swapped). This means that such a predicate on method invocations in one history is true if and only \nif it is true in all C-EQUIVALENT histories, as the arguments and return values are invariant up to commutative \nswaps. Thus it suf.ces to consider the effects of two method invocations appear\u00ading back-to-back when \ncreating such commutativity conditions. As we will see in Section 2.5, some commutativity conditions \nare based on more than just arguments and return values. Instead, they also make use of relations over \nthe abstract state of the data structure. It is less obvious how to reason about valid commutativ\u00adity \nconditions in such cases. One approach is to consider the rela\u00adtions over abstract state to be merely \nauxiliary, hidden, return val\u00adues of the method invocations. As such, commuting methods must preserve \nthese hidden return values as well as the actual return val\u00adues. This is complicated by the complex conditions \nwe see in some data structures, where the state relations are over abstract state as well as information \nfrom future method invocations. In such cases, determining that a commutativity condition is valid requires \na more substantial proof; we leave this investigation to future work. Note that a particular commutativity \ncondition need not allow the run-time to detect that two transactions can execute in parallel, even if \na different commutativity condition would. If the commuta\u00adtivity condition for two method invocations \nis simply false, the run\u00adtime system will determine that the two transactions cannot execute concurrently \nand will delay the execution of one transaction until the other completes. This reduces parallelism by \npreventing concur\u00adrent execution, but preserves serializability. The implications of the mismatch between \nschedules that can be proved serializable given certain commutativity conditions and those that could \nbe proved serializable given better commutativity conditions is explored in Section 2.4. 2.3 Commutativity \nspeci.cations A commutativity speci.cation is a set of logical formulae that rep\u00adresent commutativity \nconditions for each pair of methods in a data structure. The logic, L1, of these formulae is given in \nFigure 1. The vocabulary of the logic includes the arguments and return values of method invocations. \nThe speci.cation can also use arbitrary func\u00adtions over the arguments as well as the abstract states \nin which the methods are invoked. The logic allows boolean connectives, equal\u00adity and arithmetic, but \nno quanti.ers. In Section 3, we present some S V F := := |:= s1 | s2 v1 | v2 | r1 | r2 Z | B f(S, V, \nV, ...) Abstract states Arguments, return values, constants (S \u00d7 V \u00d7 V \u00d7 \u00b7 \u00b7 \u00b7 ) . Z . B O P C := ||:= \n:= + | - | * | / | < | > . | . = V | F P O P | (C) | \u00acC | C O C Arithmetic connectives Boolean connectives \nEquality Primitive formula Formula fm1;m2 (s1, v1, r1, s2, v2, r2) = C Figure 1. L1: Logic to express \ncommutativity conditions (1) (add(a))s1 /r1 commutes with (add(b))s2 /r2 if a= b . (r1 = false . r2 = \nfalse) (2) (add(a))s1 /r1 commutes with if (remove(b))s2 /r2 a = b . (r1 = false . r2 = false) (3) (add(a))s1 \n/r1 commutes with if (contains(b))s2 /r2 a = b . r1 = false (4) (remove(a))s1 /r1 commutes with if (remove(b))s2 \n/r2 a = b . (r1 = false . r2 = false) (5) (remove(a))s1 /r1 commutes with if (contains(b))s2 /r2 a = \nb . r1 = false (6) (contains(a))s1 /r1 commutes with (contains(b))s2 /r2 Figure 2. Commutativity speci.cation \nfor sets. restricted logics that are subsets of L1, and show that if a speci.ca\u00adtion is expressible in \nthose restricted logics we can derive ef.cient commutativity checking implementations. An example commutativity \nspeci.cation for a set is given in Figure 25. Note that add and remove return a boolean that indicates \nwhether the invocation modi.ed the set. Each condition is relatively straightforward: methods commute \nwith each other if (i) neither modi.es the set or (ii) their arguments are different. For example, condition \n(1) states that add(x) commutes with add(y) if x y = or neither invocation of add modi.ed the set (as \nindicated by their returning false). For linearizable data structures, the relevant state of the data \nstructure is the abstract state; the concrete implementation of the data structure is not relevant to \ncommutativity. For example, a set data structure implemented concretely using a linked list has the same \ncommutativity properties as a set data structure implemented using a red-black tree. If a commutativity \nspeci.cation does refer to the concrete (i.e., memory-level) state of a data structure, then it will \nonly be valid for particular implementations of the ADT.  2.4 A Commutativity Lattice While Figure 2 \npresents a single commutativity speci.cation for sets, it is by no means the only one. Recall that a \ncommutativity condition is a predicate that satis.es De.nition 3. However, as discussed in Section 2.2, \nfor each pair of methods in an ADT, there may be multiple valid commutativity conditions (for example, \nfalse is always a valid commutativity condition). Despite there being multiple valid conditions for a \npair of meth\u00adods, there is only one precise condition, f *, which is true if and only if the conditions \nof De.nition 3 hold. Note that this means that all commutativity conditions fm1;m2 . f * . The com\u00ad m1;m2 \nmutativity speci.cation given in Figure 2 is precise, meaning that all the formulae capture precise commutativity \nconditions. 5 The commutativity formulae presented in this paper are symmetric: fm1;m2 is the same as \nfm2;m1 . A full speci.cation also includes these symmetric conditions, which have been excluded for brevity. \n(1) (add(a))s1 /r1 commutes with if (add(b))s2 /r2 a = b (2) (add(a))s1 /r1 commutes with if (remove(b))s2 \n/r2 a = b (3) (add(a))s1 /r1 commutes with if (contains(b))s2 /r2 a = b (4) (remove(a))s1 /r1 commutes \nwith if (remove(b))s2 /r2 a = b (5) (remove(a))s1 /r1 commutes with if (contains(b))s2 /r2 a = b (6) \n(contains(a))s1 /r1 commutes with (contains(b))s2 /r2 Figure 3. A strengthened commutativity speci.cation \nfor sets. If we consider the set of all valid commutativity conditions for a pair of methods, S, we \ncan build a partially ordered set (poset), P =(S, -) based on logical implication, such that f1 . f2 \niff f1 . f2. This poset has a natural least element, . = false, which is obviously less than all the \nother elements and is also trivially a valid commutativity condition. More interestingly, f * is the \ngreatest element (T) of the poset. This follows directly from the de.nitions of commutativity conditions \nand f * . We can then de.ne two binary operators, U (least upper bound, or join) and n (greatest lower \nbound, or meet) for every pair of conditions in S, as follows: a n b . a . b a U b . a . b The structure \n(S, U, n, T, .) forms a bounded lattice. Note that a . b . (a U b = b) . (a n b = a). Note that a particular \ncommutativity condition (in particular, f *) may not be expressible in a given logic L. We can create \na restriction of the set of valid commutativity conditions to those expressible in L, SL. Provided that \nL is closed under disjunction (as L1 is), we can build a lattice over SL with its T as the weakest commutativity \ncondition expressible in L. While the lattice has been de.ned only for a pair of methods of an ADT, it \ncan be readily extended to create a bounded lattice of commutativity speci.cations, with elements of \nthe lattice repre\u00adsenting speci.cations that are distinct up to logical equivalence. For two commutativity \nspeci.cations F1 and F2, F1 . F2 if and only if, for each pair of methods m1,m2, where f1 is the commutativ\u00adity \ncondition for m1 and m2 from F1 and f2 is the corresponding condition from F2, f1 . f2. We can de.ne \nU and n in a simi\u00adlar manner (joining or meeting corresponding pairs of commutativ\u00adity conditions from \nthe two speci.cations). . is the speci.cation where all conditions are false, and T is the speci.cation \nwhere all conditions are precise: F * . Moving up and down the commutativity lattice Different points \nin the commutativity speci.cation lattice have dif\u00adferent properties; as you move down in the lattice, \nthe commuta\u00adtivity conditions become stronger, and hence less likely to prove that two methods commute \n(and . does not allow any methods to commute). Recall that two transactions are allowed to proceed in \nparallel if we can prove that the method invocations from those transactions commute. As a consequence, \nstronger commutativity conditions permit fewer transactions to proceed in parallel, poten\u00adtially reducing \nthe amount of parallelism an application exhibits. For example, consider the speci.cation for sets, F \n', given in Figure 3, which is derived by dropping some of the clauses from F *, the precise speci.cation \nof Figure 2. It is obvious that the new speci.cation is lower in the commutativity lattice than the precise \nspeci.cation. If two transactions A and B, each execute add(x) on a set that already contains x, the \nadds commute according to F *, but not according to F '. A con.ict detection scheme based on the former \nspeci.cation will allow A and B to execute in parallel, while one based on the latter will not, reducing \nparallelism. De.nitions: dist(a, b) is an algorithm de.ned-distance metric such that nearest(a) returns \nthe nearest point according to dist. (1) (2) (nearest(a))s1 /r1 (nearest(a))s1 /r1 commutes with commutes \nwith if (nearest(b))s2 /r2 (add(b))s2 /r2 r2 = false . dist(a, b) > dist(a, r1) (3) (nearest(a))s1 /r1 \ncommutes with if (remove(b))s2 /r2 (a = b . r1 = b) . r2 = false (4) (remove(a))s1 /r1 commutes with \nif (remove(b))s2 /r2 a = b . (r1 = false . r2 = false) (5) (remove(a))s1 /r1 commutes with if (add(b))s2 \n/r2 a = b . (r1 = false . r2 = false) (6) (add(a))s1 /r1 commutes with if (add(b))s2 /r2 a = b . (r1 \n= false . r2 = false) Figure 4. Commutativity speci.cation for kd-trees. The choice of which commutativity \nspeci.cation from the lat\u00adtice to use as the basis for a con.ict detector clearly has implica\u00adtions for \nparallelism. However, as we will see in Section 3, different commutativity speci.cations have properties \nthat allow them to be implemented using different approaches. Hence, different choices of speci.cations \nalso affect the overhead of a con.ict detector. In fact, it may be worth accepting less parallelism in \nexchange for lower overhead. Although the precise commutativity speci.cation will expose the most parallelism, \ncon.ict detectors based on less precise schemes may achieve low enough overhead to deliver su\u00adperior \nperformance. Sections 4 and 5 investigate this tradeoff.  2.5 Example speci.cations In this section, \nwe provide the commutativity speci.cations for two complex data structures, the kd-tree and the union-.nd \nstructure, that have particularly interesting commutativity properties. As we will see in the next section, \nthe complexity of these speci.cations constrains their implementation choices. Kd-tree Kd-trees are used \nto .nd the nearest point to a given point among a set of points in space. They operate by recursively \npartitioning a high-dimensional space along one dimension at a time. The abstract state of a kd-tree \nis the set of points in the tree (in this respect, kd-trees are similar to sets) as well as a relation \ndist over pairs of points that captures the distance (usually Euclidean) between two points. The basic \noperations of a kd-tree are add(a), which adds point a to the kd-tree, and remove(a), which removes point \na from the kd-tree. The query nearest(a) returns the nearest point to a. One implementation of a kd-tree \nis as follows. Points are stored only in the leaf nodes, and each interior node records the location \nof its splitting plane. To accelerate nearest calls, each node also stores the bounding box of all the \npoints contained in the subtree below it. When a node is added, the tree is walked to .nd the appropriate \nleaf node and the point is added there. The bounding box of each node from the root to this leaf node \nis then updated to account for the new point. Removing a node functions similarly. When nearest is invoked, \nwe use the splitting planes and the bounding boxes to determine which subtrees could potentially contain \na nearby point. These bounding boxes allow us to avoid traversing some subtrees of the kd-tree, allowing \nnearest queries to be executed in expected time that is logarithmic in the number of points. The commutativity \nconditions for kd-trees are given in Figure 4. The commutativity conditions (4 6), are similar to those \nfor sets. The conditions dealing with nearest however are more complex. Recall that the nearest(a) returns \nthe point closest to a according to a distance metric dist. As nearest is a read-only operation, it \nDe.nitions: rep(s, x)= return value of .nd(x) invoked in state s rank(s, x)= the rank of x in state s \n(as de.ned in union-by-rank). j rep(s, x) if rank(rep(s, x)) < rank(rep(s, y)) loser(s, x, y)= rep(s, \ny) otherwise (1) (union(a, b))s1 commutes with if (union(c, d))s2 rep(s1, c) = loser(s1, a, b) . rep(s1, \nd) = loser(s1, a, b) (2) (union(a, b))s1 commutes with if (.nd(c))s2 /r2 rep(s1, c) = loser(s1, a, b) \n(3) (union(a, b))s1 commutes with if (create(c))s2 /r2 false (4) (5) (.nd(a))s1 /r1 (.nd(a))s1 /r1 commutes \nwith commutes with if (.nd(b))s2 /r2 (create(b))s2 /r2 false (6) (create(a))s1 /r1 commutes with if (create(b))s2 \n/r2 false Figure 5. Commutativity speci.cation for union-.nd structures. clearly commutes with itself, \nas stated in condition (1). Condition (3) states that nearest commutes with remove as long as remove \ndoes not remove either the argument or return value of nearest. Condition (2) states that nearest(a) \ncommutes with add(b) as long as add(b) does not modify state, or b is not closer to the a than the return \nvalue of nearest(a) is. Union-.nd The abstract state of a union-.nd data structure is the set of disjoint \nsets. Each set also has an associated representative el\u00adement, and a rank that determines how two sets \nshould be merged. Union-.nd data structures support two operations on disjoint sets: union(a, b), which \nmerges the subset containing a with that of b, and .nd(a), which determines which subset a belongs to, \nreturning its representative element. The most ef.cient union-.nd implemen\u00adtation is a disjoint-set forest. \nEach subset is represented as a tree rooted at the set s representative element, which is used to identify \nthe entire subset. Each element in the subset points to some other element as its parent. To .nd which \nsubset an element belongs to, we start at the element and traverse parent pointers until we reach the \nroot, and then return the representative element. To merge two subsets, the representative element of \nthe set with larger rank is made the parent of the representative element of the other. For asymptotic \nef.ciency, it is necessary to use path compres\u00adsion. When invoking .nd, the parent pointer of every element \ntra\u00adversed to .nd the representative element is updated to point to that representative element, .attening \nthat portion of the tree. An in\u00adteresting side effect of path compression is that .nd operations up\u00addate \nthe data structure. While the semantic state of the data struc\u00adture does not change (invoking .nd on \nan element does not change the membership of any subset), the concrete state of the data struc\u00adture can \nchange due to path compression. The union-.nd data structure has the most complex commuta\u00adtivity conditions. \nAs expected, .nd operations commute with each other (see condition (4)). More complex are the conditions \ndeal\u00ading with union. We use three helper functions that return proper\u00adties of the abstract state: rep(s, \na), which returns a s representative node; rank(s, x), which returns the rank of the subset containing \nx and loser(s, a, b) which .nds the representative nodes of a and b in state s and returns the one with \nlowest rank or rep(s, b) if the ranks are equal. Condition (1) states that two unions commute as long \nas the second doesn t operate on the loser from the .rst. Note that this is de.ned by evaluating rep \non the arguments to the sec\u00adond union in the state that the .rst union executed in. Similarly, condition \n(2) states that union commutes with .nd provided that .nd would not have returned the loser of the union \nhad it executed in s1. The dependence of commutativity on evaluating functions in an earlier state using \ninformation from a later method invocation in\u00adtroduces subtle dif.culties, as we discuss in the following \nsection. For simplicity, conditions (3), (5) and (6) state that create does not commute with anything \n(more precise conditions are possible, but they are quite complex). 3. Implementing commutativity speci.cations \nAs discussed in the previous section, for a given ADT there is a range of commutativity speci.cations \nto choose from, and differ\u00adent choices can expose different amounts of parallelism. However, what is \nnot clear is which speci.cation to choose. This choice is in\u00ad.uenced by the implementation of the commutativity \nspeci.cation: the code that actually detect con.icts between concurrently execut\u00ading transactions according \nto the commutativity speci.cations of the data structures they access. Most prior work that has discussed \nusing high-level con.ict de\u00adtection has provided ad hoc con.ict detection schemes for a few chosen data \nstructures [10, 18, 20]. However, previous work has left unanswered whether there are systematic approaches \nto build\u00ading ef.cient, correct con.ict detectors. In this section, we show that con.ict detectors can \nbe built systematically (albeit not automati\u00adcally) given a commutativity speci.cation. We present three \nsuch techniques, one based on abstract locks and two based on logging, that we call gatekeepers. We also \ndiscuss how the properties of a commutativity speci.cation constrain its implementation and relate these \nconstraints to the lattice-theoretic view of commutativity. 3.1 Soundness and Completeness In general, \nthere are many ways to implement a commutativity speci.cation. For simple data structures like sets, \nabstract locks are one popular approach [10, 20], but it is not clear that they can implement the commutativity \nspeci.cations for more complex data structures like kd-trees and union-.nd data structures. In fact, \nwe show in Section 3.2 that abstract locks are too coarse to implement commutativity conditions for these \ndata structures. To formalize the notions of correctness and coarseness, we introduce the following de.nitions. \nDe.nition 4. A commutativity checker is SOUND with respect to a commutativity speci.cation F if it allows \ntwo concurrent method invocations, m1 and m2, to execute without detecting a con.ict only if the methods \ncommute according to the F. De.nition 5. A commutativity checker is COMPLETE with respect to a commutativity \nspecifcation F if it permits every pair of methods invocations m1 and m2 that commute according to the \nF to execute without con.ict. A sound and complete implementation of a commutativity spec\u00adi.cation therefore \nguarantees safe parallel execution and will per\u00admit the maximum amount of parallelism allowed by the \nspeci.\u00adcation. It is interesting to note that if a commutativity checker is sound but not complete with \nrespect to a speci.cation F, it must be sound and complete with respect to some speci.cation F ' such \nthat F ' . F (in other words, the checker is sound and complete with respect to some speci.cation lower \nin the commutativity lat\u00adtice). Similarly, if a commutativity checker is sound with respect to a speci.cation \nF, it is sound with respect to all other speci.cations F ' such that F . F ' .  3.2 Abstract Locking \nSchemes The .rst commutativity checking scheme we discuss is abstract locking. An abstract lock is a \nlock with a number of modes. When attempting to acquire a lock in a particular mode, the acquisition \nsucceeds if no other entity holds the lock in an incompatible mode. The compatibility of modes is determined \nby a lock s compatibility matrix. These abstract locks are a generalization of database mode locks [8] \nand subsume the abstract locks used in [20]. V1 := v1 | r1 Arguments, return values of m1 V2 := v2 | \nr2 Arguments, return values of m2 P := (V1 = V2) | P . P Conjunctive formula C := P | true | false Formula \nfm1;m2 (s1,v1,r1,s2,v2,r2)= C Figure 6. L2: Logic for SIMPLE commutativity conditions (1) (increment(a))s1 \ncommutes with (increment(b))s2 (2) (increment(a))s1 commutes with (read())s2 /r2 if false  (3) (read())s1 \n/r1 commutes with (read())s2 /r2  Figure 7. Commutativity speci.cation for accumulators. An abstract \nlocking scheme for a data structure operates as fol\u00adlows: the data structure associates an abstract lock \nwith each of its data members (de.ned as any arguments or return values to meth\u00adods of the data structure). \nThe data structure also has an abstract lock which represents whole data structure. When a transaction \nin\u00advokes a method, the abstract locks on its arguments and on the data structure may be acquired in some \nmode, and when the method returns, the abstract lock on the return value may be acquired in some mode. \nIf an abstract lock cannot be acquired, it must be held in a con.icting mode by another transaction. \nThe abstract locking scheme triggers a con.ict, and a compensating action must be taken (either the current \ntransaction or the con.icting transaction must be aborted). All abstract locks are released when a transaction \nends6. To create a locking scheme for a particular data structure, we must choose which locks are acquired, \nin which modes, and the particular mode compatibility matrix for the abstract locks. Abstract locks are \nwell-suited for providing con.ict checking for collections [4, 20] and other basic data structures [10]. \nHowever, in prior work, programmers had to carefully consider the semantics of a data structure to build \nad hoc abstract locking schemes. Two key questions were left unanswered: (i) can abstract locks provide \na sound and complete implementation of a commutativity speci\u00ad.cation? and (ii) can an abstract lock-based \ncon.ict detector be systematically constructed? Constructing abstract locking schemes We now present \nan algorithm for constructing sound and complete abstract locking schemes for commutativity speci.cations \nwhose conditions satisfy the following property: De.nition 6. A commutativity condition fm1;m2 is SIMPLE \nif one of three conditions holds: (i) fm1;m2 = false (the methods do not commute); (ii) fm1;m2 = true \n(the methods always commute); or (iii) fm1;m2 is a conjunction of clauses of the form a = b where every \na is an argument or return value of m1 and every b is an argument or return value of m2. More formally, \na commutativity speci.cation is SIMPLE if all of its conditions are expressed in the language L2, given \nin Figure 6. To produce an abstract locking scheme, we must (i) de.ne the abstract locks, and their modes; \n(ii) determine which abstract locks should be acquired when a method is invoked, and in which mode; and \n(iii) construct the compatibility matrix between the various abstract lock modes. We provide a systematic \nprocedure for accomplishing these steps. As a running example, we will explain how the procedure applies \nto an accumulator data structure, using the commutativity speci.cation given in Figure 7. 6 One can imagine \na more liberal abstract locking scheme that allows simple predicates to be evaluated before acquiring \na lock. We leave the precise de.nition and investigation of such schemes to future work. inc:ds inc:x \nread:ds read:ret vv v inc:ds \u00d7 vvv v inc:x vv v read:ds \u00d7 vvv v read:ret (a) Compatibility matrix for \naccumulator abstract locks inc:ds read:ds v inc:ds \u00d7 v read:ds \u00d7 (b) Reduced compatibility matrix Figure \n8. Compatibility matrices for accumulator abstract locks 1. De.ne the abstract locks and their modes: \nAn abstract lock is associated with each data member and with the data structure itself. Each lock supports \nseveral modes: one mode per method to represent the method s access to the data structure as a whole, \nand one mode for each argument and return value of a method. For example, the accumulator data structure \nassociates ab\u00adstract locks with the data structure as a whole, as well as ev\u00adery object that can be passed \nas an argument or returned by a method (these object locks can be allocated as needed, rather than being \ncreated ahead of time). Each lock supports four modes: increment(x) requires two modes, one for its argument \n(called inc:x) and one for the data structure (called inc:ds); (read())ss /rs requires two modes, one \nfor its return value (called read:ret) and one for the data structure (called read:ds). 2. De.ne which \nabstract locks should be acquired when a method is invoked: Any time a method is invoked it acquires \nthe data structure lock in its mode, as well as locks on all its arguments in their appropriate modes. \n For example, when increment is invoked, it acquires the data structure lock in inc:ds mode and the lock \non its argument in inc:x mode. Similarly, when read is invoked, it acquires the data structure lock in \nread:ds mode and the lock on its return value in read:ret mode. 3. Construct the compatibility matrix: \nTo this point, our de.ni\u00adtions of abstract locks and modes did not refer to the commu\u00adtativity speci.cation. \nThe speci.cation is used to determine the compatibility between various abstract lock modes.  To derive \nthe mode compatibility matrix, we consider each pair of methods m1 and m2 and their commutativity condition, \nfm1;m2 , and applying the following three rules to determine the compatibility of the methods modes: \n1. If fm1;m2 = false, modes m1:ds and m2:ds are incompati\u00adble, but all other modes are left unde.ned. \nHence, inc:ds is incompatible with read:ds. 2. For each conjunct x = y in fm1;m2 , where x is an argument \nor return value of m1 and y is an argument or return value of m2, mode m1:x is incompatible with mode \nm2:y. 3. Any pair of lock modes whose compatibility is left unde\u00ad.ned by rules 1 and 2 are assumed to \nbe compatible.  The compatibility matrix thus generated for the accumula\u00adtor is shown in Figure 8(a). \nNote that this compatibility ma\u00ad trix, as well as the rules of abstract locks acquisition, prevents reads \nfrom being executed if another transaction has executed increment, while transactions can simultaneously \nexecute read. Given the full compatibility matrix, we note that if a lock mode is compatible with all \nother lock modes, acquiring it is super.u\u00adous. We can eliminate any such lock modes and remove the cor\u00adresponding \nlock acquisitions from the relevant methods. This op\u00adtimization yields the reduced compatibility matrix \nin Figure 8(b). The reduced abstract locking scheme acquires the data structure lock in inc:ds mode whenever \nincrement is called, allowing other calls to increment to proceed without con.ict, and read equiva\u00adlently \nacquires the lock in read:ds mode. While it is possible to produce a sound abstract locking scheme for \nany commutativity speci.cation (a single, global exclusive lock constitutes a sound abstract locking \nscheme), not all speci.cations have a complete abstract locking implementation. For example, the kd-tree \nspeci.cation in Figure 4 relies on evaluating a function (to .nd the distance between two points) to \ndetermine whether nearest commutes with add, which cannot be captured by abstract locks. In fact, we \ncan show the following: Theorem 1. Given a commutativity speci.cation F, a sound and complete abstract \nlocking scheme exists iff F is SIMPLE. D The proof of Theorem 1 is given in Appendix B. This theorem \nmeans that while the strengthened commutativity speci.cation for sets in Figure 3 can be implemented \nusing an abstract locking scheme, the precise speci.cation of Figure 2 cannot. 3.3 Gatekeeping In this \nsection, we present a new con.ict detection paradigm called gatekeeping.A gatekeeper is a special object \nassociated with a par\u00adticular data structure whose role is to ensure that methods invoked by concurrently \nexecuting transactions on a data structure respect the speci.ed commutativity conditions. At a high level, \ngatekeep\u00aders operate as follows. When an transaction A invokes a method m on a data structure, the gatekeeper \nintercepts the invocation and determines if the method invocation commutes with all other ac\u00adtive method \ninvocations (i.e., methods invoked by all transactions B which have not yet ended). If the invocation \ncommutes with all active method invocations, it is allowed to proceed and the trans\u00adaction receives the \nresult. If the invocation does not commute, then the gatekeeper triggers a con.ict. To determine if a \nmethod m1 commutes with an active method m2, the gatekeeper is allowed to evaluate predicates on the \nargu\u00adments and return values of m1 and m2. Additionally, the gate\u00adkeeper can invoke methods on the data \nstructure it protects. The ability to evaluate arbitrary predicates and methods makes the gate\u00adkeeping \napproach strictly more expressive than abstract locking. Because a gatekeeper interacts with a data structure \nonly by in\u00advoking methods on it, the data structure is effectively a black box. This means that the gatekeeper \nis agnostic to the actual implemen\u00adtation details of the data structure, and a gatekeeper constructed \nto protect one abstract data type (e.g., a gatekeeper which protects sets) can protect all implementations \nof that abstract data type. However, the gatekeeper s behavior must appear atomic. In other words, the \nentire sequence of events: (i) intercepting a method invo\u00adcation, (ii) checking commutativity; (iii) \nexecuting the method on the data structure, and (iv) returning the result to the invoking trans\u00adaction, \nshould appear atomic. There are two types of gate-keepers. The .rst type, forward gatekeepers, can only \nimplement a restricted set of commutativity speci.cations, while the second, general gate\u00adkeepers, can \nimplement more general commutativity conditions. 3.3.1 Forward gatekeepers A forward gatekeeper operates \nby building up information about method invocations as they are invoked, storing that information in \nlogs associated with the invocation, and using those logs to ver\u00adify that every invocation commutes with \nall preceding invocations from other transactions. Consider the operation of a forward gate\u00adkeeper for \nkd-trees: Whenever nearest(x) is invoked, returning r, the gatekeeper must both ensure that nearest commutes \nwith preceding invo\u00adcations and also record enough information to ensure that later method invocations \ncan check their commutativity with nearest(x). It does this by storing the tuple (x, dist(x, r)) in a \nlog associated S V V1 F1 := := |:= := s1 | s2 v1 | v2 | r1 | r2 Z | B v1 | r1 f(s1, V1, V1, ...) Abstract \nstates Arguments, return values, constants Arguments, return values of m1 Function of s1 and m1 information \nF2 := f(s2, V, V, ...) Function of s2 and any value O P C := ||:= := + | - | * | / | < | > . | . = V \n| F1 | F2 P O P | (C) | \u00acC | C O C Arithmetic connectives Boolean connectives Equality Primitive formula \nFormula fm1;m2 (s1, v1, r1, s2, v2, r2) = C Figure 9. L3: Logic for ONLINE-CHECKABLE conditions with \nthe invocation. When a transaction later invokes add(y), the information from this log is used to evaluate \ndist(x, r) < dist(x, y) to check commutativity. The point y is then stored in a log asso\u00adciated with \nthe invocation add(y) to facilitate later commutativity checks. As transactions commit or abort, the \nlogs associated with their invocations are cleared. Constructing forward gatekeepers Forward gatekeepers \ncan be constructed for commutativity speci.\u00adcations whose conditions are ONLINE-CHECKABLE: De.nition \n7. fm1;m2 (v1,s1,r1,v2,s2,r2) is ONLINE-CHECKABLE if any function of v2, s2 or r2 in fm1;m2 is not a \nfunction of s1. In other words, a condition is ONLINE-CHECKABLE if there is no function that must be \ninvoked in the state of the .rst method invocation using information from the second method invocation. \nMore formally, an ONLINE-CHECKABLE condition is one that can be expressed using the logic L3, given in \nFigure 9. Note that L3 is very similar to L1 (Figure 1), with the exception that functions that deal \nwith abstract state s1 cannot also use v2 or r2 as arguments. This restriction means that conditions \n(1 2) of the union-.nd spec\u00adi.cation in Figure 5 are not ONLINE-CHECKABLE. Before describing forward \ngatekeepers, let us de.ne the set Cm of primitive functions associated with each method, m. For each \ncommutativity condition fm1;m2 , collect any functions from F1 (which only operate over the arguments, \nreturn value and abstract state of m1) that appear in the formula. Place these into Cm1 . Note that in \ngeneral, for a given method m its associated set Cm will have functions from every commutativity condition \nthat m participates in. Given the function sets constructed as above, the operation of a forward gatekeeper \nproceeds as follows: 1. When a gatekeeper sees a method invocation m(v), it evaluates every function \nin Cm and records the result in a result log Lm(v). It also records the return value of m(v) in Lm(v). \nNote that this result log is keyed by both the method and its arguments.  2. The gatekeeper records \nm(v) as an active invocation (i.e.,a method invoked by a currently executing transaction).  3. The gatekeeper \nchecks the commutativity of m(v) by evaluat\u00ading, for every active method invocation ma(va) made by other \ntransactions, the predicate fma;m. This can be done ef.ciently using results stored in Lma(va) as well \nas the arguments and re\u00adturn values of m(v). If any such commutativity condition eval\u00aduates to false, \nm(v) cannot proceed, and one of the con.icting transactions must be aborted.  Crucially,becauseallthecommutativityconditionsare \nONLINE- CHECKABLE, Lma(va) contains all the information necessary from ma(va) for evaluating fma;m. \nThus, the gatekeeper can determine commutativity simply by recording information about method invocations \nas they happen, rather than well after they execute. 4. When a transaction completes, the gatekeeper \nremoves the re\u00adsult sets L associated with the methods invoked by that transac\u00adtion, and removes the \nmethods from the active invocation list. Because the forward gatekeeper explicitly evaluates fm1;m2 \nwhen determining the commutativity of m1 and m2, it is obviously sound and complete. Furthermore, it \nis apparent that if the online\u00adcheckability condition is violated, there will be a function f that uses \ns1 as well as arguments or return values from m2. This function cannot be evaluated when m1 is executed, \nand hence the forward gatekeeper will be unable to record information in its logs to evaluate fm1;m2 \n. 3.3.2 General gatekeepers General gatekeepers are con.ict detection schemes that can capture any commutativity \nspeci.cation expressible in L1 (Figure 1). For conditions which are not ONLINE-CHECKABLE, the gatekeeper \nwill perform a rollback by executing undo methods to evaluate the commutativity condition in the appropriate \nstate and then restore the data structure state by re-executing the forward actions. This means that \nthe gatekeeper must keep a log of all actions in case they need to be rolled back to check commutativity. \nNote that this entire process must appear atomic. General gatekeepers can be thought of as forward gatekeepers \nthat perform state rollback to evaluate any commutativity conditions that are not ONLINE-CHECKABLE. A \ngeneral gatekeeper for union-.nd: We now describe the design of a general gatekeeper that fully captures \nthe commutativity conditions of Figure 5 for the operations union and .nd. The union-.nd gatekeeper maintains \ntwo logs: .nd-reps which stores all the active elements that have been returned by a call to .nd as representatives \nof some set and loser-rep, which records the result of evaluating loser(a, b) whenever union(a, b) is \ninvoked. For each invocation, union(a, b), the gatekeeper .rst computes loser(a, b). If the loser has \nbeen recorded in the .nd-reps log as the return value of an active .nd, then a con.ict is detected. Additionally, \nif either of these representatives has been recorded in the loser-rep log as the loser representative \ninvolved in a previous union, then a con.ict is detected. The gatekeeper subsequently performs the call \nto union and updates loser-rep appropriately. In the case of an invocation .nd(a), the gatekeeper .rst \nexecutes the actual call on the data structure, getting the result ra. A subtle, but important point \nis that if .nd(a) had executed earlier, before some active invocation of union, it may have returned \na different result. This is captured in condition (2) of Figure 5 as commutativ\u00ad ity dependent on the \nresults of calling .nd in a different state. To ensure that commutativity holds, the gatekeeper undoes \nthe effects of all potentially interfering calls to union, and re-executes .nd(a) to ensure it still \nreturns ra. If so, invoking .nd(a) does not violate commutativity. If not, a con.ict is detected. The \ngatekeeper then restores the state of the data structure by re-invoking the unions. Finally, ra is stored \nin .nd-reps to aid in con.ict checking.  3.4 Discussion of expressivity and overheads The three con.ict \ndetection schemes we present, abstract locking, forward gatekeeping and general gatekeeping, form a hierarchy, \nranked by expressivity and overhead. The lowest-overhead scheme is abstract locking, as it merely requires \nacquiring a small number of locks per method invocation. Forward gatekeeping incurs more overhead as \nit can require a substantial amount of logging. General gatekeepers have the highest overhead, as they \nmay perform roll\u00adbacks to evaluate complex commutativity conditions. The hierarchy of expressivity is \nidentical: abstract locking schemes can be gener\u00adated for SIMPLE speci.cations, while forward gatekeepers \ncan be generated for more complex ONLINE-CHECKABLE speci.cations and general gatekeepers can be generated \nfor all speci.cations. Given this hierarchy of performance and precision, a natural question becomes: \nwhich scheme should we use for a particular data structure? A particular speci.cation can be implemented \nusing one particular scheme; can we produce different con.ict detectors that use different schemes? The \nnext section discusses how to leverage the commutativity lattice to create different commutativity implementations \nin a disciplined manner. 4. Exploiting the commutativity lattice 4.1 The precision/performance tradeoff \nUp to this point, we have focused on producing sound and complete implementations of commutativity speci.cations \nusing a variety of con.ict detection schemes. However, for a given application, a given con.ict detection \nscheme for a data structure may have unacceptably high overheads or enable too little parallelism. Prior \nwork on high-level con.ict detection has implicitly ac\u00adcepted this point. For example, in [10] the con.ict \ndetection scheme for a set explicitly does not permit concurrent adds when the argu\u00adments are identical, \neven though, as discussed in Section 2, do\u00ad ing so may allow more parallelism. Similar tradeoffs are \nmade in [4, 18, 20]. In other words, incomplete con.ict detection schemes are used in order to sacri.ce \nparallelism for overhead. Unfortu\u00adnately, it is dif.cult to directly produce incomplete yet sound com\u00admutativity \nimplementations or to safely modify existing sound im\u00adplementations to create other implementations. \nTo tackle this problem, we turn to the commutativity lattice of Section 2.4. Rather than change the con.ict \ndetection implemen\u00ad tation directly, we can choose a less precise speci.cation from the commutativity \nlattice that can be implemented in a more ef.cient manner. Recall that if a commutativity checker is \nsound with re\u00adspect to this less precise speci.cation, it must be sound with respect to the original \nspeci.cation. As a simple example of this, consider using the speci.cation . to build a commutativity \nchecker for any data structure. This spec\u00adi.cation is SIMPLE and trivially correct. The abstract locking \nsyn\u00adthesis algorithm will produce the correct implementation: a single, global lock that prevents transactions \nfrom accessing the data struc\u00adture concurrently. This example also illustrates the precision/per\u00adformance \ntradeoff, as the con.ict checker has very low overhead but admits no parallelism. A more compelling example \nis to consider the set speci.cations from Figures 2 and 3. While the former speci.cation is not SIMPLE \nand must therefore be implemented using a forward gatekeeper, the latter is, and can be implemented using \nthe abstract locking con\u00adstruction algorithm (which will use read/write locks on the method arguments). \nWe can move still further down the commutativity lat\u00adtice by allowing contains to commute with contains \nonly if the arguments differ; this new SIMPLE speci.cation will induce an ab\u00adstract locking scheme that \nuses cheaper exclusive locks on method arguments. Crucially, it is far easier to reason about the relation\u00adships \nbetween speci.cations in the commutativity lattice than it is to determine whether an abstract locking \nscheme constitutes a sound but incomplete re.nement of a forward gatekeeper. While we can generate different \ncon.ict detection schemes for a given data structure in a disciplined manner using the commutativ\u00adity \nlattice, it is unclear how to select the appropriate scheme. This is because the overhead and parallelism \nof a particular scheme is de\u00adpendent on application characteristics. In fact, as we demonstrate in Section \n5, in certain cases the higher parallelism schemes based on gatekeeping have lower overhead than less \nprecise schemes!  4.2 Disciplined lock coarsening The commutativity lattice approach also allows us \nto reason in a systematic way about other con.ict detection optimizations. One such optimization is lock \ncoarsening [17], where a data structure such as a graph is partitioned, and locks are acquired on entire \npar\u00adtitions when any object in the partition is accessed. While this opti\u00admization seems complex, it \ncan be formulated in a straightforward manner in terms of the commutativity lattice. We begin with a \nset data structure that has been logically par\u00adtitioned: every element that may appear in the set is \nassigned a partition. A helper method, part, returns the partition an object is assigned to. We can then \nproduce a stronger commutativity speci\u00ad.cation where every clause in the speci.cation of Figure 3 of \nthe form a = b is replaced by part(a)= part(b). This new speci.ca\u00adtion is very similar to a SIMPLE speci.cation, \nexcept that it deals in partitions rather than individual data elements. The commutativity lattice lets \nus immediately see that this partition-based scheme is stronger than the original scheme, as part(a)= \npart(b) . a = b. It can be implemented in an analogous manner. Rather than acquir\u00ading locks on arguments \nand return values, an implementation of this new speci.cation will acquire locks on partitions. Partition-based \nspeci.cations can be created from any SIMPLE speci.cation, allowing us to automatically derive partition-based \ncon.ict checkers for many data structures. Note that these con.ict checkers again trade off parallelism \nfor overhead (intuitively, the parallelism becomes bounded by the number of partitions). Sec\u00adtion 5 discusses \nthis issue in more detail and quanti.es the tradeoff. 4.3 Reasoning about transactional memory The precision \nof transactional memory [9, 12, 19] can be reasoned about in terms of the commutativity lattice. For \na particular con\u00adcrete instantiation of an ADT, we can de.ne a commutativity spec\u00adi.cation, FC , that \nrefers to the memory-level state of the data struc\u00adture. By referencing memory, FC can be based on establishing \ncon\u00adcrete commutativity rather than semantic commutativity. Two meth\u00adods will commute according to this \nspeci.cation if executing them in either order produces the same concrete state. Because concrete commutativity \nimplies semantic commutativity, it is apparent that FC can be found in the commutativity lattice, but \nthat FC F * . If two transactions can execute in parallel according to a TM, they must leave the data \nstructure in the same concrete state re\u00adgardless of the order in which they execute, effectively enforcing \nconcrete commutativity. As TM constitutes a sound (though not necessarily complete) implementation of \nFC , we can see that a sound and complete implementation of the precise speci.cation will always expose \nat least as much parallelism as TM (although potentially with higher overhead). 5. Experimental Evaluation \nIn this section, we present several experiments. First, we discuss a microbenchmark based on sets that \nallows us to explore the ef\u00adfects of moving between the various set commutativity speci.ca\u00adtions (and \nhence implementations) discussed in this paper. Then we present three case studies of real-world applications. \nThe .rst, pre.ow push, shows the effects of switching between systemati\u00adcally constructed con.ict detection \nschemes in a realistic appli\u00adcation. The second, agglomerative clustering, shows that forward gatekeepers \ncan deliver scalable performance. The third, Boruvka s algorithm, shows the same for general gatekeepers. \nFor each case study, we estimate the two parameters: the aver\u00adage parallelism and the overhead of con.ict \ndetection. This allows us to study the potential tradeoff between parallelism and overhead demonstrated \nby implementations based on different points in the commutativity lattice. These results are shown in \nTable 1. Application Variant Path length Parallelism Overhead Pre.ow-push part ex ml 2789217 51978 47558 \n25.69 1894.88 2072.52 1.14 1.80 5.62 Boruvka uf-ml uf-gk 3678 3681 271.89 271.67 2.5 1.31 Clustering \nkd-ml kd-gk 2209 123 115.88 2018.15 58.76 2.32 Table 1. Critical path lengths, average parallelism and \noverheads for different applications and con.ict detection schemes. We used ParaMeter [16] to estimate \nthe average amount of par\u00ad allelism in an application when using a particular con.ict detec\u00adtion scheme. \nWe parallelized our benchmarks using the Galois sys\u00adtem [18]7. We estimate the overhead of con.ict detection \nto be the ratio between the run time of the parallelized application running on a single thread and the \nruntime of the original sequential version. As a baseline, we also implemented our benchmarks using a \nsoftware transactional memory, DSTM2 (version 2.1b) [11]. For each benchmark we used boosted objects \n[10] wherever possible (for example, the worklist), except for our target data structures, which used \nobject-based con.ict detection. Boosting allowed us to isolate any performance or parallelism differences \nto only the data structures under study. Each benchmark also used the best available contention manager \n(determined empirically). Parallelism metrics were estimated by tracking memory-level read/write con.icts \nin ParaMeter. Our performance evaluations were performed on a Nehalem server running Ubuntu Linux version \n8.06. The system consists of two 2.93 GHz quad-core processors. Each core has two 32KB L1 caches and \na uni.ed 256 KB L2 cache. Each processor has a shared 8 MB L3 cache, and the system contains 24 GB of \nmain memory. The applications and the Galois system are written in Java and run on the Sun HotSpot 64-bit \nvirtual machine, version 1.6.0. Set microbenchmark To study the effects of using different con\u00ad.ict detection \nschemes of varying overheads and permittivity, we built a microbenchmark based on a set. In the microbenchmark, \na number of threads concurrently query and update a global set. Each thread picks an arbitrary object \nfrom a shared pool and, at random, inserts it to the set (with add) or checks if it is already there \n(with contains). The benchmark performs 1 million operations, using one of two inputs: in the .rst, all \nexamined objects are distinct, and in the second, objects are in 10 equivalence classes. We consider \nfour con.ict detection schemes generated from speci.cations drawn from the set s commutativity lattice: \n(i) a global lock generated from the . speci.cation; (ii) exclusive ab\u00adstract locks placed on set elements \ngenerated from the speci.cation discussed in Section 4; (iii) r/w abstract locks generated from the speci.cation \nin Figure 3; and (iv) a forward gatekeeper generated from the precise speci.cation of Figure 2. Table \n2 shows the results of running the microbenchmark on 4 threads. For both inputs, the global lock provides \npoor perfor\u00admance: the low single thread overhead is outweighed by the non\u00adexistent parallelism (and \nresulting high abort rate). We note that the advantage of the gatekeeping and read/write lock schemes \nover exclusive locks arises when multiple transactions access the same key in the set. As this does not \noccur in the distinct-element in\u00adput, all variants have no aborts and the low-overhead exclusive lock \nsolution performs best. In contrast, in the repeated-element input, the advantage of allowing multiple \nthreads to access the same el\u00adement (when invoking contains, or add on an element already in the set) \nmanifests, and gatekeeping performs the best, followed by 7 While our evaluation uses the Galois system, \nthe techniques given for con\u00adstructing con.ict detectors and using the commutativity lattice are general. \n(a) Distinct (b) Repeats Abort Time Abort Time Program Ratio % (sec) Ratio % (sec) Global Lock 48.68 \n4.644 44.07 3.935 Abs. Lock (Ex.) 0 1.097 1.53 1.538 Abs. Lock (RW) 0 1.365 0.09 0.818 Gatekeeper 0 1.191 \n0 0.697 Table 2. 4-thread results for set microbenchmark. (a) 1 million distinct elements. (b) 1 million \nelements in 10 equivalence classes.  the read/write scheme. Gatekeeping has a lower abort ratio because \nnon-mutating adds can proceed in parallel. Pre.ow-push We used pre.ow-push [7] to investigate the perfor\u00ad \nmance of different con.ict detection schemes for a single bench\u00admark. Pre.ow push .nds the maximum .ow \nof a network by push\u00ading excess .ow from the source towards the A worklist is initialized with the source \nnode. In each iteration, a node with excess .ow is removed from the worklist. It is relabeled and .ow \nis pushed to neighboring nodes along edges that have remaining capacity. This increases the excess .ow \nat the destination node, decreases the ex\u00adcess .ow at the source, and decrements the capacity of the \nedge. Any nodes that gained excess .ow are added back to the worklist. The algorithm terminates when \nno more .ow can be pushed. This algorithm is implemented in terms of a graph-like ADT, which supports \nrelabel, pushFlow and getNeighbors. The methods relabel and pushFlow do not commute with other methods \nthat access the same nodes, while getNeighbors commutes with itself. All the conditions in this speci.cation \nare SIMPLE, and can be implemented with read/write abstract locks on nodes and edges. Note that this \ncon.ict detection strategy is identical to the con.ict detection performed by a transactional memory. \nWe also investigate two schemes based on strengthened spec\u00adi.cations from lower in the commutativity \nlattice. The .rst does not allow getNeighbors to commute with methods that access the same nodes; in \neffect, this converts read/write locks on nodes to ex\u00adclusive locks. The second uses the partition-based \nscheme derived in Section 4.2, using 32 partitions. Table 1 shows parallelism and overhead data for the \nthree versions of pre.ow push: memory level locking (ml), exclusive locking (ex) and partition-locking \n(part). As input, we use a GENRMF challenge input from [1]. We see the expected behavior: implementations \nbased on spec\u00adi.cations from higher in the commutativity lattice expose more parallelism. However, the \nperformance results in Figure 10 show that run-time is inversely correlated with the amount of parallelism: \nin this application, moving down in the commutativity lattice pro\u00adduces lower parallelism, but lower \noverhead con.ict checkers. This is especially interesting in the case of the partition-based con.ict \ndetector, as the critical-path length is a factor of 50 longer than the precise scheme. However, parallelism \nbeyond the number of pro\u00adcessors available for execution is super.uous. Since the average parallelism \nof partition-based con.ict detection is well above the number of processors, trading parallelism for \noverhead is effective. Agglomerative Clustering To study the behavior of forward gate\u00adkeeping, we implement \nan algorithm called agglomerative cluster\u00ading that uses the kd-tree data structure (described in Section \n2.5). The input to agglomerative clustering is (i) a set of points and (ii)  One approach to building \nthe dendrogram was proposed by Wal\u00adter et al. [24]. We .rst build a kd-tree that contains all the points. \nAt each step of the algorithm, an arbitrary point p is chosen, and its nearest neighbor n is determined \nby querying the kd-tree. We then .nd the nearest neighbor of n; if that is p, the two points are clus\u00adtered \ntogether by removing them from the kd-tree and inserting a new point representing that cluster. The algorithm \nterminates when there is a single cluster; by convention, the point at in.nity is the closest point if \nthe data set contains a single point. Table 1 shows the critical path length and average parallelism \non an input of 100,000 randomly generated points for a forward\u00adgatekeeping implementation (kd-gk) and \nthe baseline implementa\u00adtion (kd-ml). The gatekeeping implementation exposes signi.cantly more parallelism \nthan the baseline; although the same amount of work is done, the critical path of kd-ml is an order of \nmagnitude larger than that of kd-gk. This is because the operations on the kd-tree manipulate the bounding \nboxes of interior kd-tree nodes, resulting in memory-level con.icts even when the operations com\u00admute. \nWe do not investigate an abstract locking implementation be\u00adcause there is no straightforward SIMPLE \nspeci.cation that does not merely prevent add and nearest from executing concurrently. Figure 11 shows \nthe performance of the gatekeeping version of agglomerative clustering as well as the transactional memory \nbaseline, using an input of 500,000 randomly generated points. The gatekeeping con.ict detector is built \nusing the construction procedure of Section 3.3.1. Interestingly, despite the precision of the commutativity \nspeci.cation implemented by the forward gate\u00adkeeper (and the resulting high parallelism), it has lower \noverhead and better scalability than the memory-level con.ict checker. This is because the gatekeeper \nneed only track semantic information about the data structure, rather than each memory access. Clearly, \nforward gatekeeping, despite its complexity, is a viable approach to con.ict detection for complex commutativity \nspeci.cations. Boruvka s algorithm Finally, to study the behavior of general gatekeeping, we evaluated \nan algorithm built around the union-.nd data structure (see Section 2.5), Boruvka s algorithm. Boruvka \ns algorithm builds minimal spanning trees (MST s) of undirected graphs. Initially, each node of the graph \nis in a component by itself. At each step, the algorithm (i) picks an arbitrary component, (ii) determines \nthe lightest weight edge e connecting that component to another component, (iii) merges the two components, \nand (iv) adds e to the MST. The algorithm terminates when the entire graph is in one component. At each \npoint during the execution of the algorithm, nodes in different components are in disjoint sets. When \ntwo components are merged, the corresponding sets of nodes are as well. Therefore, a union-.nd data structure \ncan be used in step (ii) to .nd whether or not the end points of an edge are in the same component. Table \n1 shows the parallelism and overhead of Boruvka s algo\u00ad rithm when run on a randomly generated 1000 \u00d7 \n1000 mesh using both a general gatekeeper (uf-gk), built using our construction, and our baseline TM \n(uf-ml). Curiously, we see that general gatekeep- Figure 12. Boruvka s algorithm speedup (serial time \n3.7 sec.) ing does not offer a parallelism bene.t over memory level lock\u00ading. Recall that although stronger \ncommutativity speci.cations will typically permit less parallelism than speci.cations from higher in \nthe lattice, this is entirely dependent on application behavior. In the case of union-.nd, additional \nparallelism is exposed by a gate\u00adkeeper if multiple interfering .nds are performed (as path com\u00adpression \nprevents a memory-level approach from performing them concurrently). However, Boruvka s does not perform \nsuch .nds, obviating the gatekeeper s parallelism advantage. Interestingly, however, the general gatekeeper, \ndespite its com\u00adplexity, has fairly low overhead (~ 31%), and far less than a TM, because it need only \ntrack semantic changes to the data structure (an especially large advantage considering all of the reads \nand writes involved in path compression). This advantage is further pressed when scaling up, as Figure \n12 shows that the general gate\u00ad keeper provides better performance than the baseline. It is apparent \nthat, for some applications, even a con.ict detection scheme as in\u00advolved as general gatekeeping can \nprovide acceptable performance. Putting it all together The results for our example applications demonstrate \nthat differ\u00adent con.ict detection schemes can provide vastly different per\u00adformance. To understand the \ntradeoffs between different schemes, consider a simple model in terms of the following three factors: \n T = sequential runtime of algorithm, od = overhead of con.ict detection scheme d, and  ad = average \nparallelism enabled by d.  T * od is the single-threaded runtime while performing con.ict detection, \nand T * od/ad is the best-case parallel runtime, assum\u00ading perfect load-balance. Note that ad and od \ncorrespond to the parallelism and overhead columns, respectively, in Table 1. Both a and o are application \nspeci.c. However, this model can help illuminate the choice of con.ict detection. Consider two con.ict \ndetection schemes l and h for which ol <oh and al <ah (i.e., scheme l has less overhead but also permits \nless parallelism). The lower overhead, lower parallelism con.ict detection scheme l offers better performance \nif ol/al <oh/ah (i.e., lower over\u00adhead makes up for lower parallelism). This explains why exclu\u00adsive \nlocking outperforms read/write locking in pre.ow push.  Most current systems have only a small number \nof processors, so the actual runtime of an algorithm on a machine with p processors may be bounded above \nby T * od/p. If al \u00bb p, we should use the lower overhead, lower parallelism scheme l, since the relative \namount of parallelism exposed by different con.ict detection schemes ceases to be the dominant factor \nin performance. This explains why partition locking for pre.ow push is the best-performing con.ict detection \nscheme.  If ah >al but oh <ol then the higher parallelism scheme actually has lower overhead. Our model \nsuggests that in such situations, we should prefer the higher parallelism scheme. This situation arises \nwhen considering the gatekeeping implementa\u00adtions of kd-trees and union-.nd.  In short, the best con.ict \nchecker for an ADT can depend on the characteristics of both the application and the parallel machine; \na single choice of con.ict detector is likely not ideal for every application. However, measuring simple \nparameters can guide the selection of con.ict checker. While the construction techniques of Section \n3 are intended to permit library writers to develop commutativity checkers for their ADTs, the commutativity \nlattice provides a useful interface be\u00adtween the library writer and the programmers who use the ADTs. \nThe library writer can produce a range of sound checkers for a given ADT, using the construction techniques \nof Section 3 and the strengthening techniques of Section 4. The lattice then allows the library writer \nto rank the checkers according to expressiveness. The user can then make an informed decision to move \nbetween imple\u00admentations if his/her application demands more parallelism or less overhead. Alluringly, \nthe ability to rank checkers by permittivity can allow an automated system to adaptively and dynamically \nse\u00adlect from these implementations as run-time needs change, given observations of parallelism and overhead, \nthough we leave the de\u00adsign and development of such a system to future work. 6. Related work and conclusions \nRelated work Commutativity conditions can be seen as a highly generalized form of predicate locks as \nused in databases [6], where locks on arbitrary predicates are used to determine when transac\u00adtions can \nsuccessfully execute in parallel. Database research has been the source of a considerable number of techniques \nto exploit the high level properties of abstract data types, including commuta\u00adtivity, for concurrency \ncontrol [2, 23, 25]. That work laid the foun\u00ad dation for subsequent work that focused on exploiting data \nstruc\u00adture semantics in speculative parallelization and synchronization systems on shared memory systems \n[5, 10, 18, 20]. Ni et al. addressed the shortcomings of memory-level locking in transactional memory \nsystems by introducing open nesting, which uses abstract locks to synchronize access to data structures \nby ex\u00adploiting their semantics [20]. However, the work focuses mainly on the mechanism of integrating \nabstract locking with a transactional memory, rather than on how to use abstract locks in data structures \nin a disciplined manner. In fact, the authors note that improper use of open nesting could lead to deadlock \nsituations. Kulkarni et al. described an approach to checking commutativ\u00adity [18]. This technique applies \nto ONLINE-CHECKABLE speci.\u00adcations, but requires logging entire method invocations, and hence is not \nas ef.cient as forward gatekeeping, which can bene.t from common sub-clauses in commutativity conditions. \nHerlihy and Koskinen proved that commutativity information informs a disciplined, safe approach for implementing \nopen nesting [10]. The key difference between their work and our work is that the former focused on how \ncommuting methods could be used to en\u00adforce serializability, while we focus on implementation approaches \nfor con.ict detection. Rather than using ad hoc con.ict detectors, we provide systematic approaches for \nconstructing commutativity checkers, each with well-de.ned expressive power. Furthermore, we provide \na disciplined approach to systematically generating the simpler abstract locking implementations used \nin prior work. Recently, Demsky and Lam proposed using object-level views to synthesize high-level concurrency \ncontrol [5]. Views are similar in spirit to our SIMPLE speci.cations, but allow for more complex interactions. \nAs a result, their lock-generation algorithm is greedy, rather than precise. An interesting avenue of \nfuture research is to determine how a view speci.cation is related to a commutativity speci.cation, and \nwhether the former can be derived from the latter. Other recent work from Burnim et al. discusses verifying \nthat method invocations on a data structure that are meant to be atomic indeed are [3]. Their notion \nof atomicity is semantic: two methods are atomic if they preserve certain semantic bridge predicates \nthat refer to the data structure s semantic state, rather than its concrete state. This results in a \nnotion of atomicity that is similar to semantic commutativity. Their atomicity checks are performed at \ncompile time by performing a limited search through a space of possible parallel executions. Unlike in \nour work, their aim is to show that a particular pair of method invocations (with particular arguments) \nare atomic; they do not consider commutativity conditions, which constrain the kinds of method invocations \nthat will appear atomic. Kim and Rinard look at verifying commutativity conditions [14]. Given a de.nition \nof the abstract state of a data structure, how methods use that abstract state, and commutativity conditions \nbe\u00adtween methods, Kim and Rinard s technique can verify that the commutativity conditions are, indeed, \ncorrect. This technique is complementary to the goals of our paper: we have not considered the correctness \nof commutativity conditions, instead relying on external techniques such as theirs to ensure that they \nare valid. Future work and conclusions There are a number of promising directions for future research. \nFirst, for data structures such as union-.nd, determining appropriate commutativity conditions is dif.cult; \ncan we generate commutativity conditions given a data structure speci.cation? Second, there are interesting \navenues of implementation research: although gatekeepers are expressive, they can be quite inef.cient. \nAre there more ef.cient con.ict detection schemes that do not overly sacri.ce expressibility? In Section \n1, we posed three questions: (i) how should seman\u00ad tic con.ict checks be implemented? (ii) can we trade \noff con.ict checking precision for performance? and (iii) which con.ict de\u00adtection scheme should be used? \nThrough our commutativity lat\u00adtice framework, we showed that con.ict detectors can be reasoned about \nin terms of a lattice of commutativity speci.cations. We showed how different points in the lattice can \nbe implemented sys\u00adtematically by three different con.ict detection schemes. We also showed how the lattice \ncan be exploited to derive lower parallelis\u00adm/lower overhead detectors by strengthening commutativity \nspec\u00adi.cations. Finally we studied the tradeoffs between precision and performance and showed that our \nsystematically generated con.ict detectors can provide acceptable performance. Acknowledgements We would \nlike to thank Roman Manevich for his insights and comments regarding the presentation of this paper. \nWe would also like to thank the anonymous reviewers for their helpful feedback. References [1] Synthetic \nmaximum .ow families. http://www.avglab.com/ andrew/CATS/maxflow_synthetic.htm. [2] A. Bondavalli, N. \nDe Francesco, D. Latella, and G. Vaglini. Shared abstract data type: an algebraic methodology for their \nspeci.cation. In MFDBS 89, 1989. [3] J. Burnim, G. Necula, and K. Sen. Specifying and checking semantic \natomicity for multithreaded programs. In ASPLOS 11: Proceedings of the 16th International Conference \non Architectural Support for Programming Languages and Operating Systems, 2011. [4] B. D. Carlstrom, \nA. McDonald, C. Kozyrakis, and K. Olukotun. Trans\u00adactional collection classes. In PPoPP, 2007. [5] B. \nDemsky and P. Lam. Views: object-inspired concurrency control. In ICSE, 2010. [6] K. P. Eswaran, J. N. \nGray, R. A. Lorie, and I. L. Traiger. The notions of consistency and predicate locks in a database system. \nCommun. ACM, 19(11):624 633, 1976. [7] A. V. Goldberg and R. E. Tarjan. A new approach to the maximum\u00ad.ow \nproblem. J. ACM, 35(4):921 940, 1988. [8] J. Gray and A. Reuter. Transaction Processing: Concepts and \nTech\u00adniques. Morgan Kaufmann Publishers Inc., 1992.  [9] T. Harris and K. Fraser. Language support for \nlightweight transactions. In OOPSLA. Oct 2003. [10] M. Herlihy and E. Koskinen. Transactional boosting: \na methodology for highly-concurrent transactional objects. In PPoPP, 2008. [11] M. Herlihy, V. Luchangco, \nand M. Moir. A .exible framework for implementing software transactional memory. In OOPSLA, 2006. [12] \nM. Herlihy and J. E. B. Moss. Transactional memory: architectural support for lock-free data structures. \nIn ISCA, 1993. [13] M. P. Herlihy and J. M. Wing. Linearizability: a correctness condition for concurrent \nobjects. TOPLAS, 12(3):463 492, 1990. [14] D. Kim and M. Rinard. Veri.cation of semantic commutativity \ncondi\u00adtions and inverse operations on linked data structures. In PLDI, 2011. [15] E. Koskinen, M. Parkinson, \nand M. Herlihy. Coarse-grained transac\u00adtions. In POPL, 2010. [16] M. Kulkarni, M. Burtscher, R. Inkulu, \nK. Pingali, and C. Casc\u00b8aval. How much parallelism is there in irregular applications? In PPoPP, 2009. \n[17] M. Kulkarni, K. Pingali, G. Ramanarayanan, B. Walter, K. Bala, and L. P. Chew. Optimistic parallelism \nbene.ts from data partitioning. In ASPLOS, 2008. [18] M. Kulkarni, K. Pingali, B. Walter, G. Ramanarayanan, \nK. Bala, and L. P. Chew. Optimistic parallelism requires abstractions. In PLDI, 2007. [19] J. Larus and \nR. Rajwar. Transactional Memory (Synthesis Lectures on Computer Architecture). Morgan &#38; Claypool \nPublishers, 2007. [20] Y. Ni, V. Menon, A.-R. Adl-Tabatabai, A. L. Hosking, R. Hudson, J. E. B. Moss, \nB. Saha, and T. Shpeisman. Open nesting in software transactional memory. In PPoPP, 2007. [21] C. H. \nPapadimitriou. The serializability of concurrent database up\u00addates. J. ACM, 26(4):631 653, 1979. [22] \nL. Rauchwerger and D. Padua. The LRPD test: Speculative run-time parallelization of loops with privatization \nand reduction paralleliza\u00adtion. In PLDI, 1995. [23] P. M. Schwarz and A. Z. Spector. Synchronizing shared \nabstract types. ACM Trans. Comput. Syst., 2(3):223 250, 1984. [24] B. Walter, K. Bala, M. Kulkarni, and \nK. Pingali. Fast agglomerative clustering for rendering. In Interactive Ray Tracing, 2008. [25] W. Weihl. \nCommutativity-based concurrency control for abstract data types. IEEE Transactions on Computers, 37(12), \n1988. A. Proof of serializability Here we prove the serializability claim made in Section 2.1. The theorem \nis stated in terms of an execution history consisting of two transactions A and B, which each start at \nthe beginning of the his\u00adtory and .nish at the end of the history (i.e., the transactions fully overlap \nand are coextensive with the history itself). The theorem can easily be extended to longer histories, \nwith multiple transac\u00adtions that only partially overlap. The theorem proceeds by showing how to construct \na serial schedule when commutativity conditions have been shown true for all pairs of method invocations. \nWe begin by showing that if we can show a commutativity condition to be true in one history H, then in \nany C-EQUIVALENT history H ' there exists some commutativity condition over the same method invocations \nthat is true. This allows us to claim that even as we rearrange invocations while constructing a serial \nschedule, we can assume that the commutativity conditions are true. Lemma 1. If the following three conditions \nhold: History H contains the invocations (m1(v1))s1 and (m2(v2))s2  There exists a C-EQUIVALENT history \nH ' with invocations  (m1(v1))s3 and (m2(v2))s4 fm1;m2 (s1,v1,r1,s2,v2,r2) is a commutativity condition \nin H then there exists a commutativity condition f ' (s3,v1,r1, m1;m2 s4,v2,r2) in H ' such that fm1;m2 \n(s1,v1,r1,s2,v2,r2) . f ' (s3,v1,r1,s4,v2,r2). m1;m2 Proof. We begin by assuming that H and H ' exist \naccording to the conditions of the lemma, and that fm1;m2 (s1,v1,r1,s2,v2,r2) is a commutativity condition. \nThere clearly exists a predicate f ' (s3,v1,r1,s4,v2,r2) that is true if and only if fm1;m2 m1;m2 is \ntrue (though it may not be expressible in a logic such as L1). All that remains is to show that this \npredicate is, indeed, a commuta\u00adtivity condition (in other words, that when it is true, the conditions \nof De.nition 3 are satis.ed). We begin by constructing the set of histories that are C-EQUIVALENT to \nH and in which m1(v1) and m2(v2) appear con\u00adsecutively: '' '' S(H)= {H |H =C H . . sub-history h of H \n'' = (m1(v1),m2(v2))s} We have assumed that f ' is true, and hence so is fm1;m2 . m1;m2 It follows \nfrom the de.nition of commutativity conditions that if fm1;m2 (s1,v1,r1,s2,v2,r2) holds, then all sub-histories \nof the form (m1(v1),m2(v2))s found in the histories of S(H) are equiv\u00adalent to their swapped sub-histories, \n(m2(v2),m1(v1))s. Because the C-EQUIVALENCE relation forms an equivalence class, the set S(H ' )= S(H). \nThus, f ' is a valid commutativity condition m1;m2 in H ' . We can now prove the following theorem: \n Theorem 2. If, in an execution history H, consisting of method in\u00advocations from two transactions A \nand B, for each pair of method A B invocations (ma (va))sa (returning ra) and (mb (vb))sb (return\u00ading \nrb), the commutativity condition fma;mb (sa,va,ra,sb,vb,rb) holds, then there is an equivalent history \nH ' where all the invoca\u00adtions of B appear before all the invocations of A. Proof. The proof proceeds \nby induction on the length of the sub\u00adhistory, h, starting with the .rst method invoked by A in H, A \n m1 (v1), and ending with the last method invoked by B in H, B m2 (v2). We can disregard invocations \nfrom B that occur before A m1 (v1), as they already appear before all invocations from A, and B likewise \nfor invocations from A that occur after m2 (v2). If there is no such sub-history, then H is already serialized. \nAB Base case: When a sub-history h = (m1 (v1),m 2 (v2))s, we can directly apply the commutativity condition \nfm1;m2 to produce a BA C-EQUIVALENT history h ' = (m2 (v2),m 1 (v1))s. Inductive hypothesis: When a sub-history \nh contains n or fewer invocations from A and m or fewer invocations from B, we can construct a sub-history \nh ' =C h that has all invocations from B before all invocations from A. Inductive step: We will look \nat two cases: one where there are n +1 invocations from A and m invocations from B in h, and a second \nwhere there are n invocations from A and m+1 invocations from B in h. Case 1: Consider the sub-history \nof h, i, that is the suf.x of h starting with the last invocation from A in h: A BB i = (mn+1(vn+1),m \nk (vk),...,m m(vm))s i has at most m invocations from B and 1 invocation from A, and hence the inductive \nhypothesis applies. We can construct i ' =C i where all the invocations from B appear .rst. Substi\u00adtuting \ni ' for i in h gives us a C-EQUIVALENT history h ' .  The commutativity conditions provided by the conditions \nof the theorem only apply to h, not to h '. By appealing to Lemma 1, we know there are commutativity \nconditions between the pairs of methods in h ' that are all true. Note that h ' has at least one method \ninvocation from A at the very end. We can now consider the sub-history j of h ' that excludes that last \nmethod invocation. This new sub-history has n invocations from A and m invocations from B, allowing us \nto apply the inductive hypothesis again, constructing an equivalent j ' in which all the invocations \nfrom A are at the end. Substituting j ' for j in h ' gives us a new sub-history h '' that is equivalent \nto h but has all the invocations from B occurring before any invocations from A, proving this case. Case \n2: This case proceeds analogously to Case 1. We have thus shown that there exists a sub-history h ' that \nis C-EQUIVALENT to sub-history h for sub-histories of any length. Substituting h ' for h in the complete \nhistory H produces a C-EQUIVALENT history H ' that has all invocations from A after all invocations from \nB, and hence is serialized. B. Proof of Theorem 1 In this section, we provide the proof of Theorem 1 \n(that a sound a complete abstract locking scheme exists for a speci.cation iff that speci.cation is SIMPLE). \nThe proof proceeds in two parts. First, we show that the construction algorithm presented in Section \n3.2 produces a sound and complete implementation of a SIMPLE com\u00admutativity speci.cation. Then, we show \nthat if a speci.cation is not SIMPLE, there does not exist a sound and complete abstract locking scheme \nfor it.   B.1 Soundness and completeness Lemma 2. The abstract locking construction presented in the \npa\u00adper produces an abstract locking scheme that is sound and com\u00adplete with respect to a SIMPLE commutativity \nspeci.cation. Proof. We will use the contrapositive de.nitions of soundness and completeness in this \nlemma, as described below. Soundness. For an abstract locking scheme to be a sound im\u00adplementation of \na commutativity speci.cation, if two method invocations are allowed to proceed by the scheme (i.e., both \ncan successfully acquire their abstract locks), then the methods must commute according to the speci.cation. \nBy contraposi\u00adtive, if two method invocations con.ict according to the speci\u00ad.cation, then one or more \nabstract lock acquires performed by the scheme must fail, triggering a con.ict.  Completeness. For an \nabstract locking scheme to be complete, we must show that if two method invocations commute accord\u00ading \nto the speci.cation, the abstract locking scheme will not de\u00adtect a con.ict. By contrapositive, if the \nabstract locking scheme detects a con.ict between two method invocations (because one cannot acquire \nits abstract locks), then the methods must con\u00ad.ict according to the speci.cation.  Because each method \nacquires its abstract locks in separate modes, it suf.ces to address the soundness and completeness of \neach commutativity predicate in isolation. For a commutativity speci.cation to be SIMPLE, all commutativity \npredicates must sat\u00adisfy one of three conditions. We address the three cases separately. Case 1: fm1;m2 \n= false. In this case, the m1 and m2 always con\u00ad.ict. We see that our lock compatibility table requires \nthat the lock modes on the data structure lock used by the two methods interfere, hence preventing the \nsecond method from success\u00adfully acquiring the data structure lock. Thus the implementa\u00adtion is sound. \nCompleteness is trivial, as the methods always con.ict. Case 2: fm1;m2 = true. In this case, m1 and \nm2 never con\u00ad.ict. For soundness, this case is uninteresting; regardless of the behavior of the abstract \nlocking scheme, the implementation will be sound. Completeness requires that the abstract locking scheme \nallow m1 and m2 to be successfully invoked by concur\u00adrently executing transactions. Because every method \nacquires locks in its own modes, and lock modes are compatible by de\u00adfault, we see that our abstract \nlocking scheme lets m1 and m2 proceed. Case 3: fm1;m2 = ....(x = y)..... To show soundness, consider \nthe negation of this condition: a disjunction of equalities of the form x = y. If any of the equality \nclauses are true, our abstract locking scheme triggers a con.ict. The equality of arguments means that \nthe two method invocations will attempt to acquire locks on the same data item, and our compatibility \nmatrix construction guarantees that the modes in which that lock will be acquired are incompatible. To \nshow completeness, we note that if our scheme detects a con.ict, then two method invocations must have \nacquired the same argument lock in con.icting modes. By our construction, this can only happen if the \ncommutativity condition contains a clause of the form x = y. Hence, the con.ict condition contains the \nclause x = y, and the two methods con.ict according to the speci.cation. Because the abstract locking \nscheme instantiated by our al\u00adgorithm for a SIMPLE commutativity speci.cation is sound and complete with \nrespect to each commutativity predicate, the over\u00adall scheme is sound and complete. B.2 Expressivity \nof abstract locking schemes Lemma 2 shows that if a commutativity speci.cation is SIMPLE, we can produce \na sound and complete abstract locking scheme for it by applying our construction algorithm. However, \nit is unclear whether sound and complete abstract locking schemes exist for non-SIMPLE commutativity \nspeci.cations. The following lemma shows that they do not. Lemma 3. If a commutativity speci.cation is \nnot SIMPLE, then there does not exist a sound and complete abstract locking scheme that implements it. \nProof. We proceed by contradiction. Assume that there is a non-SIMPLE commutativity speci.cation, F, \nwith a sound and com\u00adplete abstract locking implementation. From this abstract locking scheme, construct \nF ' as follows: For each pair of methods m1 and m2, consider at the compatibility sub-matrix induced \nby the locks acquired when invoking each method. If two argument locks, a and b are incompatible, the \ncommutativity condition fm1;m2 contains the conjunct a = b. If data structure locks acquired by the methods \nare incompatible, fm1;m2 is simply false. Note that F ' is clearly SIMPLE, and the abstract locking scheme \nis a sound and complete implementation of F ' by construction (speci.cally, applying the abstract locking \nconstruction algorithm to F ' will produce the original abstract locking scheme). As the locking scheme \nis also sound and complete with respect to F, F and F ' must be logically equivalent, contradicting the \nassumption that F is non-SIMPLE. Theorem 1 follows directly from Lemmas 2 and 3.  \n\t\t\t", "proc_id": "1993498", "abstract": "<p>Speculative execution is a promising approach for exploiting parallelism in many programs, but it requires efficient schemes for detecting conflicts between concurrently executing threads. Prior work has argued that checking semantic commutativity of method invocations is the right way to detect conflicts for complex data structures such as kd-trees. Several ad hoc ways of checking commutativity have been proposed in the literature, but there is no systematic approach for producing implementations.</p> <p>In this paper, we describe a novel framework for reasoning about commutativity conditions: the commutativity lattice. We show how commutativity specifications from this lattice can be systematically implemented in one of three different schemes: abstract locking, forward gatekeeping and general gatekeeping. We also discuss a disciplined approach to exploiting the lattice to find different implementations that trade off precision in conflict detection for performance. Finally, we show that our novel conflict detection schemes are practical and can deliver speedup on three real-world applications.</p>", "authors": [{"name": "Milind Kulkarni", "author_profile_id": "81331496893", "affiliation": "Purdue University, West Lafayette, IN, USA", "person_id": "P2690645", "email_address": "milind@purdue.edu", "orcid_id": ""}, {"name": "Donald Nguyen", "author_profile_id": "81435601909", "affiliation": "The University of Texas at Austin, Austin, TX, USA", "person_id": "P2690646", "email_address": "ddn@cs.utexas.edu", "orcid_id": ""}, {"name": "Dimitrios Prountzos", "author_profile_id": "81388601660", "affiliation": "The University of Texas at Austin, Austin, TX, USA", "person_id": "P2690647", "email_address": "dprountz@cs.utexas.edu", "orcid_id": ""}, {"name": "Xin Sui", "author_profile_id": "81542944756", "affiliation": "The University of Texas at Austin, Austin, TX, USA", "person_id": "P2690648", "email_address": "xinsui@cs.utexas.edu", "orcid_id": ""}, {"name": "Keshav Pingali", "author_profile_id": "81100554731", "affiliation": "The University of Texas at Austin, Austin, TX, USA", "person_id": "P2690649", "email_address": "pingali@cs.utexas.edu", "orcid_id": ""}], "doi_number": "10.1145/1993498.1993562", "year": "2011", "article_id": "1993562", "conference": "PLDI", "title": "Exploiting the commutativity lattice", "url": "http://dl.acm.org/citation.cfm?id=1993562"}