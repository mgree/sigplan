{"article_publication_date": "06-04-2011", "fulltext": "\n Understanding POWER Multiprocessors Susmit Sarkar1 Peter Sewell1 Jade Alglave2,3 Luc Maranget3 Derek \nWilliams4 1234 University of Cambridge Oxford University INRIA IBM Austin Abstract Exploiting today s \nmultiprocessors requires high\u00adperformance and correct concurrent systems code (op\u00adtimising compilers, \nlanguage runtimes, OS kernels, etc.), which in turn requires a good understanding of the observable processor \nbehaviour that can be relied on. Unfortunately this critical hardware/software interface is not at all \nclear for several current multiprocessors. In this paper we characterise the behaviour of IBM POWER multiprocessors, \nwhich have a subtle and highly relaxed memory model (ARM multiprocessors have a very similar architecture \nin this respect). We have conducted ex\u00adtensive experiments on several generations of processors: POWER \nG5, 5, 6, and 7. Based on these, on published de\u00adtails of the microarchitectures, and on discussions \nwith IBM sta., we give an abstract-machine semantics that abstracts from most of the implementation detail \nbut explains the be\u00adhaviour of a range of subtle examples. Our semantics is ex\u00adplained in prose but de.ned \nin rigorous machine-processed mathematics; we also con.rm that it captures the observ\u00adable processor \nbehaviour, or the architectural intent, for our examples with an executable checker. While not o.cially \nsanctioned by the vendor, we believe that this model gives a reasonable basis for reasoning about current \nPOWER mul\u00adtiprocessors. Our work should bring new clarity to concurrent systems programming for these \narchitectures, and is a necessary precondition for any analysis or veri.cation. It should also inform \nthe design of languages such as C and C++, where the language memory model is constrained by what can \nbe e.ciently compiled to such multiprocessors. Categories and Subject Descriptors C.1.2 [Multiple Data \nStream Architectures (Multiprocessors)]: Parallel pro\u00adcessors; D.1.3 [Concurrent Programming]: Parallel \npro\u00adgramming; F.3.1 [Specifying and Verifying and Reasoning about Programs] General Terms Documentation, \nLanguages, Reliability, Standardization, Theory, Veri.cation Keywords Relaxed Memory Models, Semantics \n1. Introduction Power multiprocessors (including the IBM POWER 5, 6, and 7, and various PowerPC implementations) \nhave for Permission to make digital or hard copies of all or part of this work for personal or classroom \nuse is granted without fee provided that copies are not made or distributed for pro.t or commercial advantage \nand that copies bear this notice and the full citation on the .rst page. To copy otherwise, to republish, \nto post on servers or to redistribute to lists, requires prior speci.c permission and/or a fee. PLDI \n11, June 4 8, 2011, San Jose, California, USA. Copyright c &#38;#169; 2011 ACM 978-1-4503-0663-8/11/06. \n. . $10.00 many years had aggressive implementations, providing high performance but exposing a very \nrelaxed memory model, one that requires careful use of dependencies and memory barriers to enforce ordering \nin concurrent code. A priori, one might expect the behaviour of a multiprocessor to be su.\u00adciently well-de.ned \nby the vendor architecture documenta\u00adtion, here the Power ISA v2.06 speci.cation [Pow09]. For the sequential \nbehaviour of instructions, that is very often true. For concurrent code, however, the observable behaviour \nof Power multiprocessors is extremely subtle, as we shall see, and the guarantees given by the vendor \nspeci.cation are not always clear. We therefore set out to discover the ac\u00adtual processor behaviour and \nto de.ne a rigorous and usable semantics, as a foundation for future system building and research. The \nprogrammer-observable relaxed-memory behaviour of these multiprocessors emerges as a whole-system prop\u00aderty \nfrom a complex microarchitecture [SKT+05, LSF+07, KSSF10]. This can change signi.cantly between generations, \ne.g. from POWER 6 to POWER 7, but includes: cores that perform out-of-order and speculative execution, \nwith many shadow registers; hierarchical store bu.ering, with some bu.ering shared between threads of \na symmetric multi\u00adthreading (SMT) core, and with multiple levels of cache; store bu.ering partitioned \nby address; and a cache protocol with many cache-line states and a complex interconnection topology, \nand in which cache-line invalidate messages are bu.ered. The implementation of coherent memory and of \nthe memory barriers involves many of these, working to\u00adgether. To make a useful model, it is essential \nto abstract from as much as possible of this complexity, both to make it simple enough to be comprehensible \nand because the de\u00adtailed hardware designs are proprietary (the published lit\u00aderature does not describe \nthe microarchitecture in enough detail to con.dently predict all the observable behaviour). Of course, \nthe model also has to be sound, allowing all be\u00adhaviour that the hardware actually exhibits, and su.ciently \nstrong, capturing any guarantees provided by the hardware that systems programmers rely on. It does not \nhave to be tight: it may be desirable to make a loose speci.cation, per\u00admitting some behaviour that current \nhardware does not ex\u00adhibit, but which programmers do not rely on the absence of, for simplicity or to \nadmit di.erent implementations in future. The model does not have to correspond in detail to the internal \nstructure of the hardware: we are capturing the external behaviour of reasonable implementations, not \nthe implementations themselves. But it should have a clear ab\u00adstraction relationship to implementation \nmicroarchitecture, so that the model truly explains the behaviour of examples. To develop our model, \nand to establish con.dence that it is sound, we have conducted extensive experiments, run\u00adning several \nthousand tests, both hand-written and auto\u00admatically generated, on several generations of processors, \nfor up to 1011 iterations each. We present some simple tests in \u00a72, to introduce the relaxed behaviour \nallowed by Power processors, and some more subtle examples in \u00a76, with repre\u00adsentative experimental data \nin \u00a77. To ensure that our model explains the behaviour of tests in a way that faithfully ab\u00adstracts from \nthe actual hardware, using appropriate con\u00adcepts, we depend on extensive discussions with IBM sta.. To \nvalidate the model against experiment, we built a checker, based on code automatically generated from \nthe mathemati\u00adcal de.nition, to calculate the allowed outcomes of tests (\u00a78); this con.rms that the model \ngives the correct results for all tests we describe and for a systematically generated family of around \n300 others. Relaxed memory models are typically expressed either in an axiomatic or an operational style. \nHere we adopt the latter, de.ning an abstract machine in \u00a73and \u00a74. We expect that this will be more intuitive \nthan typical axiomatic models, as it has a straightforward notion of global time (in traces of abstract \nmachine transitions), and the abstraction from the actual hardware is more direct. More particularly, \nto explain some of the examples, it seems to be necessary to model out-of-order and speculative reads \nexplicitly, which is easier to do in an abstract-machine setting. This work is an exercise in making \na model that is as simple as possible but no simpler: the model is considerably more complex than some \n(e.g. for TSO processors such as Sparc and x86), but does capture the processor behaviour or architectural \nintent for a range of subtle examples. Moreover, while the de.nition is mathematically rigorous, it can \nbe explained in only a few pages of prose, so it should be accessible to the expert systems programmers \n(of concurrency libraries, language runtimes, optimising compilers, etc.) who have to be concerned with \nthese issues. We end with discussion of related work (\u00a79) and a brief summary of future directions (\u00a710), \nreturning at last to the vendor architecture. 2. Simple Examples We begin with an informal introduction \nto Power multipro\u00adcessor behaviour by example, introducing some key concepts but leaving explanation \nin terms of the model to later. 2.1 Relaxed behaviour In the absence of memory barriers or dependencies, \nPower multiprocessors exhibit a very relaxed memory model, as shown by their behaviour for the following \nfour classic tests. SB: Store Bu.ering Here two threads write to shared\u00admemory locations and then each \nreads from the other loca\u00adtion an idiom at the heart of Dekker s mutual-exclusion algorithm, for example. \nIn pseudocode: Thread 0 Thread 1 x=1 r1=y y=1 r2=x Initial shared state: x=0 and y=0 Allowed .nal state: \nr1=0 and r2=0 In the speci.ed execution both threads read the value from the initial state (in later \nexamples, this is zero unless oth\u00aderwise stated). To eliminate any ambiguity about exactly what machine \ninstructions are executed, either from source\u00adlanguage semantics or compilation concerns, we take the \nde.nitive version of our examples to be in PowerPC as\u00adsembly (available online [SSA+11]), rather than \npseudocode. The assembly code is not easy to read, however, so here we present examples as diagrams of \nthe memory read and write events involved in the execution speci.ed by the initial and .nal state constraints. \nIn this example, the pseudocode r1 and r2 represent machine registers, so accesses to those are not memory \nevents; with the .nal state as speci.ed, the only conceivable execution has two writes, labelled a and \nc, and two reads, labelled b and d,withvaluesasbelow.They are related by program order po (later we elide \nimplied po edges), and the fact that the two reads both read from the initial state (0) is indicated \nby the incoming reads-from (rf) edges (from writes to reads that read from them); the dots indicate the \ninitial-state writes. Thread 0 Thread 1 a: W[x]=1 c: W[y]=1 po po b: R[y]=0 d: R[x]=0 rf rf Test SB \n: Allowed This example illustrates the key relaxation allowed in Sparc or x86 TSO models [Spa92, SSO+10]. \nThe next three show some ways in which Power gives a weaker model. MP: Message passing Here Thread 0 \nwrites data x and then sets a .ag y, while Thread 1 reads y from that .ag write andthenreads x. On Power \nthat read is not guaranteeed to see the Thread 0 write of x; it might instead read from before that write, \ndespite the chain of po and rf edges: Thread 0 Thread 1 a: W[x]=1 c: R[y]=1 po po b: W[y]=1 rf d: R[x]=0 \nTest MP : Allowed In real code, the read c of y might be in a loop, repeated until the value read is \n1. Here, to simplify experimental testing, we do not have a loop but instead consider only executions \nin which the value read is 1, expressed with a constraint on the .nal register values in the test source. \nWRC: Write-to-Read Causality Here Thread 0 com\u00admunicates to Thread 1 by writing x=1.Thread1reads that, \nand then later (in program order) sends a message to Thread 2 by writing into y. Having read that write \nof y at Thread 2, the question is whether a program-order\u00adsubsequent read of x at Thread 2 is guaranteed \nto see the value written by the Thread 0 write, or might read from before that, as shown, again despite \nthe rf and po chain. On Power that is possible. Thread 0 Thread 1 Thread 2 a: W[x]=1 b: R[x]=1 d: R[y]=1 \nrf po po c: W[y]=1 rf e: R[x]=0 Test WRC : Allowed IRIW: Independent Reads of Independent Writes Here \ntwo threads (0 and 2) write to distinct locations while two others (1 and 3) each read from both locations. \nIn the speci.ed allowed execution, they see the two writes in di.erent orders (Thread 1 s .rst read sees \nthe write to x but the program-order-subsequent read does not see the write of y, whereas Thread 3 sees \nthe write to y but not that to x). Thread 0 Thread 1 Thread 2 Thread 3 a: W[x]=1 b: R[x]=1 d: W[y]=1 \n e: R[y]=1 rf rf po po rf c: R[y]=0 rf f: R[x]=0 Test IRIW : Allowed  Coherence Despite all the above, \none does get a guaran\u00adtee of coherence: in any execution, for each location, there is a single linear \norder (co) of all writes (by any processor) to that location, which must be respected by all threads. \nThe four cases below illustrate this: a pair of reads by a thread cannot read contrary to the coherence \norder (CoRR1); the coherence order must respect program order for a pair of writes by a thread (CoWW); \na read cannot read from a write that is coherence-hidden by another write program-order\u00adpreceding the \nread (CoWR), and a write cannot coherence\u00adorder-precede a write that a program-order-preceding read read \nfrom. We can now clarify the before in the MP and WRC discussion above, which was with respect to the \nco\u00adherence order for x. Thread 0 Thread 1 Thread 0 a: W[x]=1 rf c: R[x]=0 b: W[x]=2 Test CoRR1 : Forbidden \nTest CoWW : Forbidden Thread 0 Thread 1 Thread 0 Thread 1 rf a: W[x]=1 c: W[x]=2 a: R[x]=2 c: W[x]=2 \npo po b: R[x]=2 b: W[x]=1 Test CoWR : Forbidden Test CoRW : Forbidden  2.2 Enforcing ordering The \nPower ISA provides several ways to enforce stronger ordering. Here we deal with the sync (heavyweight \nsync, or hwsync) and lwsync (lightweight sync) barrier instructions, and with dependencies and the isync \ninstruction, leaving load-reserve/store-conditional pairs and eieio to future work. Regaining sequential \nconsistency (SC) using sync If one adds a sync between every program-order pair of instructions (creating \ntests SB+syncs, MP+syncs, WRC+syncs, and IRIW+syncs), then all the non-SC results above are forbidden, \ne.g. Thread 0 Thread 1 a: W[x]=1 c: R[y]=1 sync sync b: W[y]=1 rf d: R[x]=0 Test MP+syncs : Forbidden \nUsing dependencies Barriers can incur a signi.cant runtime cost, and in some cases enough ordering is \nguaran\u00adteed simply by the existence of a dependency from a memory read to another memory access. There \nare various kinds: There is an address dependency (addr) from a read to a program-order-later memory \nread or write if there is a data .ow path from the read, through registers and arithmetic/logical operations \n(but not through other memory accesses), to the address of the second read or write.  There is a data \ndependency (data) from a read to a memory write if there is such a path to the value written. Address \nand data dependencies behave similarly.  There is a control dependency (ctrl) from a read to a memory \nwrite if there is such a data.ow path to the test of a conditional branch that is a program-order\u00adpredecessor \nof the write. We also refer to control depen\u00addencies from a read to a read, but ordering of the reads \nin that case is not respected in general.  There is a control+isync dependency (ctrlisync)froma read \nto another memory read if there is such a data.ow path from the .rst read to the test of a conditional \nbranch that program-order-precedes an isync instruction before the second read.  Sometimes one can \nuse dependencies that are naturally present in an algorithm, but it can be desirable to introduce one \narti.cially, for its ordering properties, e.g. by XOR ing a value with itself and adding that to an address \ncalculation. Dependencies alone are usually not enough. For exam\u00adple, adding dependencies between read/read \nand read/write pairs, giving tests WRC+data+addr (with a data depen\u00addency on Thread 1 and an address \ndependency on Thread 2), and IRIW+addrs (with address dependencies on Threads 1 and 3), leaves the non-SC \nbehaviour allowed. One cannot add dependencies to SB, as that only has write/read pairs, and one can \nonly add a dependency to the read/read side of MP, leaving the writes unconstrained and the non-SC be\u00adhaviour \nstill allowed. In combination with a barrier, however, dependencies can be very useful. For example, \nMP+sync+addr is SC: Thread 0 Thread 1 a: W[x]=1 c: R[y]=1 sync addr b: W[y]=1 rf d: R[x]=0 Test MP+sync+addr \n: Forbidden Here the barrier keeps the writes in order, as seen by any thread, and the address dependency \nkeeps the reads in order. Contrary to what one might expect, the combination of a thread-local reads-from \nedge and a dependency does not guarantee ordering of a write-write pair, as seen by another thread; the \ntwo writes can propagate in either order (here [x]=z initially): Thread 0 Thread 1 a: W[x]=y d: R[y]=1 \n rf addr c: W[y]=1 Test MP+nondep+sync : Allowed Control dependencies, observable speculative reads, \nand isync Recall that control dependencies (without isync) are only respected from reads to writes, not \nfrom reads to reads. If one replaces the address dependency in MP+sync+addr by a data.ow path to a conditional \nbranch before the second read (giving the test named MP+sync+ctrl below), that does not ensure that the \nreads on Thread 1 bind their values in program order. Thread 0 Thread 1 a: W[x]=1 c: R[y]=1 sync ctrl \n b: W[y]=1 rf d: R[x]=0 Test MP+sync+ctrl : Allowed  Adding an isync instruction between the branch \nand the second read (giving test MP+sync+ctrlisync) su.ces. The fact that data/address dependencies to \nboth reads and writes are respected while control dependencies are only respected to writes is important \nin the design of C++0x low-level atomics [BA08, BOS+11], where release/consume atomics let one take advantage \nof data dependencies without requiring barriers (and limiting optimisation) to ensure that all source-language \ncontrol dependencies are respected. Cumulativity For WRC it su.ces to have a sync on Thread 1 with a \ndependency on Thread 2; the non-SC behaviour is then forbidden: Thread 0 Thread 1 Thread 2 a: W[x]=1 \n c: W[y]=1 rf e: R[x]=0 Test WRC+sync+addr : Forbidden This illustrates what we call A-cumulativity of \nPower barri\u00aders: a chain of edges before the barrier that is respected. In this case Thread 1 reads from \nthe Thread 0 write before (in program order) executing a sync, and then Thread 1 writes to another location; \nany other thread (here 2) is guaranteed to see the Thread 0 write before the Thread 1 write. How\u00adever, \nswapping the sync and dependency, e.g. with just an rf and data edge between writes a and c, does not \nguarantee ordering of those two writes as seen by another thread: Thread 0 Thread 1 Thread 2 a: W[x]=1 \n b: R[x]=1 d: R[y]=1 rf data sync c: W[y]=1 e: R[x]=0 rf Test WRC+data+sync : Allowed In contrast to \nthat WRC+data+sync, a chain of reads\u00adfrom edges and dependencies after a sync does ensure that ordering \nbetween a write before the sync and a write after the sync is respected, as below. Here the reads e and \nf of z and x cannot see the writes a and d out of order. We call this a B-cumulativity property. Thread \n0 Thread 1 Thread 2 a: W[x]=1 c: R[y]=1 e: R[z]=1 sync data addr b: W[y]=1 d: W[z]=1 f: R[x]=0 rf Test \nISA2+sync+data+addr : Forbidden Using lwsync The lwsync barrier is broadly similar to sync, including \ncumulativity properties, except that does not order store/load pairs and it is cheaper to execute; it \nsuf\u00ad.ces to guarantee SC behaviour in MP+lwsyncs (MP with lwsync in each thread), WRC+lwsync+addr (WRC \nwith lwsync on Thread 1 and an address dependency on Thread 2), and ISA2+lwsync+data+addr, while SB+lwsyncs \nand IRIW+lwsyncs are still allowed. We return later to other di.erences between sync and lwsync. 3. The \nModel Design We describe the high-level design of our model in this sec\u00adtion, giving the details in the \nnext. We build our model as a composition of a set of (hardware) threads and a single storage subsystem, \nsynchronising on various messages:  Read-request/read-response pairs are tightly coupled, while the \nothers are single unidirectional messages. There is no bu.ering between the two parts. Coherence-by-.at \nOur storage subsystem abstracts completely from the processor implementation store\u00adbu.ering and cache \nhierarchy, and from the cache protocol: our model has no explicit memory, either of the system as a whole, \nor of any cache or store queue (the fact that one can abstract from all these is itself interesting). \nInstead, we work in terms of the write events that a read can read from. Our storage subsystem maintains, \nfor each address, the cur\u00adrent constraint on the coherence order among the writes it has seen to that \naddress, as a strict partial order (transitive but irre.exive). For example, suppose the storage subsystem \nhas seen four writes, w0, w1, w2 and w3, all to the same ad\u00address. It might have built up the coherence \nconstraint on the left below, with w0 known to be before w1, w2 and w3,and w1 known to be before w2, \nbut with as-yet-undetermined relationships between w1 and w3, and between w2 and w3. w0 w0 w1 w1  w2 \nw3 w2 w3 The storage subsystem also records the list of writes that it has propagated to each thread: \nthose sent in response to read-requests, those done by the thread itself, and those propagated to that \nthread in the process of propagating a barrier to that thread. These are interleaved with records of \nbarriers propagated to that thread. Note that this is a storage-subsystem-model concept: the writes propagated \nto a thread have not necessarily been sent to the thread model in a read-response. Now, given a read \nrequest by a thread tid, what writes could be sent in response? From the state on the left above, if \nthe writes propagated to thread tid are just [w1], perhaps because tid has read from w1, then: it cannot \nbe sent w0,as w0 is coherence-before the w1 write that (because it is in the writes-propagated list) \nit might have read from;  it could re-read from w1, leaving the coherence constraint unchanged;  it \ncould be sent w2, again leaving the coherence constraint unchanged, in which case w2 must be appended \nto the events propagated to tid;or  it could be sent w3, again appending this to the events propagated \nto tid, which moreover entails committing to w3 being coherence-after w1,asinthecoherencecon\u00adstraint \non the right above. Note that this still leaves the relative order of w2 and w3 unconstrained, so another \n  thread could be sent w2 then w3 or (in a di.erent run) the other way around (or indeed just one, \nor neither). In the model this behaviour is split up into separate storage\u00adsubsystem transitions: there \nis one rule for making a new co\u00adherence commitment between two hitherto-unrelated writes to the same \naddress, one rule for propagating a write to a new thread (which can only .re after su.cient coherence \ncommitments have been made), and one rule for returning a read value to a thread in response to a read-request. \nThe last always returns the most recent write (to the read address) in the list of events propagated \nto the reading thread, which therefore serves essentially as a per-thread memory (though it records more \ninformation than just an array of bytes). We adopt these separate transitions (in what we term a partial \ncoherence commitment style) to make it easy to relate model transitions to actual hardware implementation \nevents: co\u00adherence commitments correspond to writes .owing through join points in a hierarchical-store-bu.er \nimplementation. Out-of-order and Speculative Execution As we shall see in \u00a76, many of the observable \nsubtleties of Power be\u00adhaviour arise from the fact that the threads can perform read instructions out-of-order \nand speculatively, subject to an instruction being restarted if a con.icting write comes in before the \ninstruction is committed, or being aborted if a branch turns out to be mispredicted. However, writes \nare not sent to the storage subsystem before their instructions are committed, and we do not see observable \nvalue speculation. Accordingly, our thread model permits very liberal out-of\u00adorder execution, with unbounded \nspeculative execution past as-yet-unresolved branches and past some barriers, while our storage subsystem \nmodel need not be concerned with spec\u00adulation, retry, or aborts. On the other hand, the storage subsystem \nmaintains the current coherence constraint, as above, while the thread model does not need to have ac\u00adcess \nto this; the thread model plays its part in maintaining coherence by issuing requests in reasonable orders, \nand in aborting/retrying as necessary. Foreachthreadwehaveatreeofthe committed and in-.ight instruction \ninstances. Newly fetched instructions become in-.ight, and later, subject to appropriate precondi\u00adtions, \ncan be committed. For example, below we show a set of instruction instances {i1,...,i13} with the program-order\u00adsuccessor \nrelation among them. Three of those ({i1,i3,i4}, boxed) have been committed; the remainder are in-.ight. \n i12 i13 Instruction instances i5 and i9 are branches for which the thread has fetched multiple possible \nsuccessors; here just two, but for a branch with a computed address it might fetch many possible successors. \nNote that the committed instances are not necessarily contiguous: here i3 and i4 have been committed \neven though i2 has not, which can only happen if they are su.ciently independent. When a branch is committed \nthen any un-taken alternative paths are discarded, and instructions that follow (in program order) an \nuncommitted branch cannot be committed until that branch is, so the tree must be linear before any committed \n(boxed) instructions. In implementations, reads are retried when cache-line invalidates are processed. \nIn the model, to abstract from exactly when this happens (and from whatever tracking the core does of \nwhich instructions must be retried when it does), we adopt a late invalidate semantics, retrying appropriate \nreads (and their dependencies) when a read or write is committed. For example, consider two reads r1 \nand r2 in program order that have been satis.ed from two di.erent writes to the same address, with r2 \nsatis.ed .rst (out-of-order), from w1,and r1 satis.ed later from the coherence-later w2.When r1 is committed, \nr2 must be restarted, otherwise there would be a coherence violation. (This relies on the fact that writes \nare never provided by the storage subsystem out of coherence order; the thread model does not need to \nrecord the coherence order explicitly.) Dependencies are dealt with entirely by the thread model, in \nterms of the registers read and written by each instruction instance (the register footprints of instructions \nare known statically). Memory reads cannot be satis.ed until their addresses are determined (though perhaps \nstill subject to change on retry), and memory writes cannot be committed until their addresses and values \nare fully deter\u00admined. We do not model register renaming and shadow regis\u00adters explicitly, but our out-of-order \nexecution model includes their e.ect, as register reads take values from program-order\u00adpreceding register \nwrites. Barriers (sync and lwsync) and cumulativity-by-.at The semantics of barriers involves both parts \nof the model, as follows. When the storage subsystem receives a barrier request, it records the barrier \nas propagated to its own thread, marking a point in the sequence of writes that have been propagated \nto that thread. Those writes are the Group A writes for this barrier. When all the Group A writes (or \nsome coherence\u00adsuccessors thereof) of a barrier have been propagated to an\u00adother thread, the storage \nsubsystem can record that fact also, propagating the barrier to that thread (thereby mark\u00ading a point \nin the sequence of writes that have been prop\u00adagated to that thread). A write cannot be propagated to \na thread tid until all relevant barriers are propagated to tid, where the relevant barriers are those \nthat were propagated to the writing thread before the write itself. In turn (by the above), that means \nthat the Group A writes of those barri\u00aders (or some coherence successors) must already have been propagated \nto tid. This models the e.ect of cumulativity while abstracting from the details of how it is implemented. \nMoreover, a sync barrier can be acknowledged back to the originating thread when all of its Group A writes \nhave been propagated to all threads. In the thread model, barriers constrain the commit order. For example, \nno memory load or store instruction can be committed until all previous sync barriers are committed and \nacknowledged; and sync and lwsync barriers cannot be committed until all previous memory reads and writes \nhave been. Moreover, memory reads cannot be satis.ed until previous sync barriers are committed and acknowledged. \nThere are various possible modelling choices here which should not make any observable di.erence the \nabove corresponds to a moderately aggressive implementation. 4. The Model in Detail We now detail the \ninterface between the storage subsystem and thread models, and the states and transitions of each. The \ntransitions are described in \u00a74.3 and \u00a74.5 in terms of their precondition, their e.ect on the relevant \nstate, and the messages sent or received. Transitions are atomic, and synchronise as shown in Fig. 1; \nmessages are not bu.ered. This is a prose description of our mathematical de.nitions, available on-line \n[SSA+11]. 4.1 The Storage Subsystem/Thread Interface The storage subsystem and threads exchange messages: \n a write request (or write) w speci.es the writing thread tid, unique request id eid, address a,and value \nv.  a read request speci.es the originating thread tid,request id eid, and address a.  a read response \nspeci.es the originating thread tid,re\u00adquest id eid, and a write w (itself specifying the thread tid! \nthat did the write, its id eid!, address a,and value v). This is sent when the value read is bound. \n a barrier request speci.es the originating thread tid, request id eid, and barrier type b (sync or lwsync). \n a barrier ack speci.es the originating thread tid and request id eid (a barrier ack is only sent for \nsync barriers, after the barrier is propagated to all threads.  4.2 Storage Subsystem States A storage \nsubsystem state s has the following components. s.threads is the set of thread ids that exist in the \nsystem.  s.writes seen is the set of all writes that the storage subsystem has seen.  s.coherence is \nthe current constraint on the coherence order, among the writes that the storage subsystem has seen. \nIt is a binary relation: s.coherence contains the pair (w1, w2) if the storage subsystem has committed \nto write w1 being before write w2 in the coherence order. This relation grows over time, with new pairs \nbeing added, as the storage subsystem makes additional commitments. For each address, s.coherence is \na strict partial order over the write requests seen to that address. It does not relate writes to di.erent \naddresses, or relate any write that has not been seen by the storage subsystem to any write.  s.events \npropagated to gives, for each thread, a list of:  1. all writes done by that thread itself, 2. all \nwrites by other threads that have been propagated to this thread, and 3. all barriers that have been \npropagated to this thread. We refer to those writes as the writes that have been propagated to that thread. \nThe Group A writes for a  barrier are all the writes that have been propagated to the barrier s thread \nbefore the barrier is. s.unacknowledged sync requests is the set of sync bar\u00adrier requests that the storage \nsubsystem has not yet ac\u00adknowedged. An initial state for the storage subsystem has the set of thread \nids that exist in the system, exactly one write for each memory address, all of which have been propagated \nto all threads (this ensures that they will be coherence-before any other write to that address), an \nempty coherence order, and no unacknowledged sync requests.  4.3 Storage Subsystem Transitions Accept \nwrite request A write request by a thread tid can always be accepted. Action: 1. add the new write to \ns.writes seen, to record the new write as seen by the storage subsystem; 2. append the new write to \ns.events propagated to (tid),to record the new write as propagated to its own thread; and 3. update \ns.coherence to note that the new write is coherence-after all writes (to the same address) that have \npreviously propagated to this thread.  Partial coherence commitment The storage subsys\u00adtem can internally \ncommit to a more constrained coherence order for a particular address, adding an arbitrary edge (be\u00adtween \na pair of writes to that address that have been seen already that are not yet related by coherence) to \ns.coherence, together with any edges implied by transitivity, if there is no cycle in the union of the \nresulting coherence order and the set of all pairs of writes (w1,w2), to any address, for which w1 and \nw2 are separated by a barrier in the list of events propagated to the thread of w2. Action: Add the new \nedges to s.coherence. Propagate write to another thread The storage sub\u00adsystem can propagate a write \nw (by thread tid)thatithas seen to another thread tid!,if: 1. the write has not yet been propagated to \ntid!; 2. w is coherence-after any write to the same address that has already been propagated to tid!;and \n 3. all barriers that were propagated to tid before w (in s.events propagated to (tid)) have already \nbeen propa\u00adgated to tid!.  Action: append w to s.events propagated to (tid!). Send a read response \nto a thread The storage subsys\u00adtem can accept a read-request by a thread tid at any time, and reply with \nthe most recent write w to the same address that has been propagated to tid. The request and response \nare tightly coupled into one atomic transition. Action: send a read-response message containing w to \ntid. Accept barrier request A barrier request from a thread tid can always be accepted. Action: 1. append \nit to s.events propagated to (tid),torecordthe barrier as propagated to its own thread (and thereby .x \nthe set of Group A writes for this barrier); and 2. (for sync) add it to s.unacknowledged sync requests. \n  Propagate barrier to another thread The storage subsystem can propagate a barrier it has seen to another \nthread if: 1. the barrier has not yet been propagated to that thread; and 2. for each Group A write, \nthat write (or some coherence successor) has already been propagated to that thread  Action: append \nthe barrier to s.events propagated to (tid). Acknowledge sync barrier A sync barrier b can be ac\u00adknowledged \nif it has been propagated to all threads. Action: 1. send a barrier-ack message to the originating thread; \nand 2. remove b from s.unacknowledged sync requests.   Storage Subsystem Rule Message(s) Thread Rule \nAccept write request write request Commit in-.ight instruction Partial coherence commitment Propagate \nwrite to another thread Send a read response to a thread read request/read response Satisfy memory read \nfrom storage subsystem Satisfy memory read by forwarding an in-.ight write Accept barrier request barrier \nrequest Commit in-.ight instruction Propagate barrier to another thread Acknowledge sync barrier barrier \nack Accept sync barrier acknowledgement Fetch instruction Register read from previous register write \nRegister read from initial register state Internal computation step Figure 1. Storage Subsystem and \nThread Synchronisation 4.4 Thread States The state t of a single hardware thread consists of: its thread \nid.  the initial values for all registers, t.initial register state.  aset t.committed instructions \nof committed instruction instances. All their operations have been executed and they are not subject \nto restart or abort.  aset t.in .ight instructions of in-.ight instruction in\u00adstances. These have been \nfetched and some of the asso\u00adciated instruction-semantics micro-operations may have been executed. However, \nnone of the associated writes or barriers have been sent to the storage subsystem, and any in-.ight instruction \nis subject to being aborted (together with all of its dependents).  aset t.unacknowledged syncs of sync \nbarriers that have not been acknowledged by the storage subsystem.  An initial state for a thread has \nno committed or in-.ight instructions and no unacknowledged sync barriers. Each instruction instance \ni consists of a unique id, a rep\u00adresentation of the current state of its instruction seman\u00adtics, the \nnames of its input and output registers, the set of writes that it has read from, the instruction address, \nthe program-order-previous instruction instance id, and any value constraint required to reach this instruction \ninstance from the previous instance. The instruction semantics ex\u00adecutes in steps, doing internal computation, \nregister reads and writes, memory reads, and, .nally, memory writes or barriers.  4.5 Thread Transitions \nFetch instruction An instruction inst can be fetched, fol\u00adlowing its program-order predecessor iprev \nand from address a,if 1. a is a possible next fetch address for iprev;and 2. inst is the instruction \nof the program at a.  The possible next fetch addresses allow speculation past calculated jumps and \nconditional branches; they are de.ned as: 1. for a non-branch/jump instruction, the successor instruc\u00adtion \naddress; 2. for a jump to a constant address, that address; 3. for a jump to an address which is not \nyet fully determined (i.e., where there is an uncommitted instruction with a data.ow path to the address), \nany address; and  4. for a conditional branch, the possible addresses for a jump together with the successor. \n Action: construct an initialized instruction instance and add it to the set of in-.ight instructions. \nThis is an internal action of the thread, not involving the storage subsystem, as we assume a .xed program \nrather than modelling fetches with reads; we do not model self-modifying code. Commit in-.ight instruction \nAn in-.ight instruction can be committed if: 1. its instruction semantics has no pending reads (memory \nor register) or internal computation (data or address arithmetic); 2. all instructions with a data.ow \ndependency to this in\u00adstruction (instructions with register outputs feeding to this instruction s register \ninputs) are committed; 3. all program-order-previous branches are committed; 4. if a memory load or \nstore is involved, all program-order\u00adprevious instructions which might access its address (i.e., which \nhave an as-yet-undetermined address or which have a determined address which equals that one) are committed; \n 5. if a memory load or store is involved, or this instruction is a sync, lwsync, or isync, then  (a) \nall previous sync, lwsync and isync instructions are committed, and (b) there is no unacknowledged sync \nbarrier by this thread;  6. if a sync or lwsync instruction, all previous memory access instructions \nare committed; 7. if an isync, then all program-order-previous instructions which access memory have \ntheir addresses fully deter\u00admined, where by fully determined we mean that all in\u00adstructions that are \nthe source of incoming data.ow de\u00adpendencies to the relevant address are committed and any internal address \ncomputation is done.  Action: note that the instruction is now committed, and: 1. if a write instruction, \nrestart any in-.ight memory reads (and their data.ow dependents) that have read from the same address, \nbut from a di.erent write (and where the read could not have been by forwarding an in-.ight write); \n2. if a read instruction, .nd all in-.ight program-order successors that have read from a di.erent write \nto the same address, or which follow a lwsync barrier program\u00adorder after this instruction, and restart \nthem and their data.ow dependents;   3. if this is a branch, abort any untaken speculative paths of \nexecution, i.e., any instruction instances that are not reachable by the branch taken; and 4. send any \nwrite requests or barrier requests as required by the instruction semantics.  Accept sync barrier acknowledgement \nA sync bar\u00adrier acknowledgement can always be accepted (there will always be a committed sync whose barrier \nhas a match\u00ading eid). Action: remove the corresponding barrier from t.unacknowledged syncs. Satisfy memory \nread from storage subsystem A pending read request in the instruction semantics of an in\u00ad.ight instruction \ncan be satis.ed by making a read-request and getting a read-response containing a write from the storage \nsubsystem if: 1. the address to read is determined (i.e., any other reads with a data.ow path to the \naddress have been satis.ed, though not necessarily committed, and any arithmetic on such a path completed); \n 2. all program-order-previous syncs are committed and ac\u00adknowledged; and 3. all program-order-previous \nisyncs are committed.  Action: 1. update the internal state of the reading instruction; and 2. note \nthat the write has been read from by that instruc\u00adtion.  The remaining transitions are all thread-internal \nsteps. Satisfy memory read by forwarding an in-.ight write directly to reading instruction A pending \nmemory write w from an in-.ight (uncommitted) instruction can be forwarded directly to a read of an instruction \ni if 1. w is an uncommitted write to the same address that is program-order before the read, and there \nis no program\u00adorder-intervening memory write that might be to the same address; 2. all i s program-order-previous \nsyncs are committed and acknowledged; and 3. all i s program-order-previous isyncs are committed.  \nAction: as in the satisfy memory read from storage subsystem rule above. Register read from previous \nregister write A register read can read from a program-order-previous register write if the latter is \nthe last write to the same register program-order before it. Action: update the internal state of the \nin-.ight reading instruction. Register read from initial register state A register read can read from \nthe initial register state if there is no write to the same register program-order before it. Action: \nupdate the internal state of the in-.ight reading instruction. Internal computation step An in-.ight \ninstruction can perform an internal computation step if its semantics has a pending internal transition, \ne.g. for an arithmetic operation. Action: update the internal state of the in-.ight instruction.  4.6 \nFinal states The .nal states are those with no transitions. It should be the case that for all such, \nall instruction instances are committed. 5. Explaining the simple examples The abstract machine explains \nthe allowed and forbidden behaviour for all the simple tests we saw before. For example, in outline: \nMP The Thread 0 write-requests for x and y could be in-order or not, but either way, because they are \nto di.erent addresses, they can be propagated to Thread 1 in either order. Moreover, even if they are \npropagated in program order, the Thread 1 read of x can be satis.ed .rst (seeing the initial state), \nthen the read of y, and they could be committed in either order. MP+sync+ctrl (control dependency) Here \nthe sync keeps the propagation of the writes to Thread 1 in order, but the Thread 1 read of x can be \nsatis.ed speculatively, before the conditional branch of the control dependency is resolved and before \nthe program-order-preceding Thread 1 read of y is satis.ed; then the two reads can be committed in program \norder. MP+sync+ctrlisync (isync) Adding isync between the conditional branch and the Thread 1 read of \nx prevents that read being satis.ed until the isync is committed, which cannot happen until the program-order-previous \nbranch is committed, which cannot happen until the .rst read is satis.ed and committed. WRC+sync+addr \n(A-cumulativity) The Thread 0 write-request for a:W[x]=1 must be made, and the write propagated to Thread \n1, for b to read 1 from it. Thread 1 then makes a barrier request for its sync, and that is prop\u00adagated \nto Thread 1 after a (so the write a is in the Group A set for this barrier), before making the write-request \nfor c:W[y]=1. That write must be propagated to Thread 2 for d to read from it, but before that is possible \nthe sync must be propagated to Thread 2, and before that is possible a must be propagated to Thread 2. \nMeanwhile, the dependency on Thread 2 means that the address of the read e is not known, and so e cannot \nbe satis.ed, until read d has been satis.ed (from c). As that cannot be until after a is propagated to \nThread 2, read e must read 1 from a,not 0 from the initial state. WRC+data+sync Here, in contrast, while \nthe Thread 0/Thread 1 reads-from relationship and the Thread 1 dependency ensure that the write-requests \nfor a:W[x]=1 and c:W[y]=1 are made in that order, and the Thread 2 sync keeps its reads in order, the \norder that the writes are propagated to Thread 2 is unconstrained. ISA2 (B-cumulativity) In the ISA2+sync+data+addr \nB-cumulativity example, the Thread 0 write requests and barrier request must reach the storage subsystem \nin program order, so Group A for the sync is {a} and the sync is propagated to Thread 0 before the b \nwrite request reaches the storage subsystem. For c to read from b, the latter must have been propagated \nto Thread 1, which requires the sync to be propagated to Thread 1, which in turn requires the Group A \nwrite a to have been propagated to Thread 1. Now, the Thread 1 dependency means that d cannot be committed \nbefore the read c is satis.ed (and committed), and hence d must be after the sync is propagated to Thread \n1. Finally, for e to read from d, the latter must have been propagated to Thread 2, for which the sync \nmust be propagated to Thread 2, and hence the Group A write a propagated to Thread 2. The Thread 2 dependency \nmeans that f cannot be satis.ed until e is, so it must read from a, not from the initial state. The \nsame result and reasoning hold for the lwsync variant of this test (note that the reasoning did not involve \nsync acks or any memory reads program-order-after the sync). IRIW+syncs Here the two syncs (on Threads \n1 and 3) have the corresponding writes (a and d) in their Group A sets, and hence those writes must be \npropagated to all threads before the respective syncs are acknowledged, which must happen before the \nprogram-order-subsequent reads c and f can be be satis.ed. But for those to read 0,from coherence-predecessors \nof a and d, the latter must not have been propagated to all threads (in particular, they must not have \nbeen propagated to Threads 3 and 1 respectively). In other words, for this to happen there would have \nto be a cycle in abstract-machine execution time: a: W[x]=1 propagated to last thread d: W[y]=1 propagated \nto last thread Thread 1 sync acknowledgement Thread 3 sync acknowledgement Thread 1 c: R[y]=0 is satis.ed \nThread 3 d: R[x]=0 is satis.ed  With lwsyncs instead of syncs, the behaviour is allowed, because lwsyncs \ndo not have an analogous acknowledgement when their Group A writes have been propagated to all threads, \nand memory reads do not wait for previous lwsyncs to reach that point. 6. Not-so-simple examples We now \ndiscuss some more subtle behaviours, explaining each in terms of our model. Write forwarding In the PPOCA \nvariant of MP below, f is address-dependent on e, which reads from the write d, which is control-dependent \non c.One mightexpectthat chain to prevent read f binding its value (with the satisfy memory read from \nstorage subsystem rule) before c does, but in fact in some implementations f can bind out-of-order, as \nshown the write d can be forwarded directly to e within the thread, before the write is committed to \nthe storage subsystem. The satisfy memory read by forwarding an in-.ight write rule models this. Replacing \nthe control dependency with a data dependency (test PPOAA, not shown) removes that possibility, forbidding \nthe given result on current hardware, as far as our experimental results show, and in our model. The \ncurrent architecture text [Pow09] leaves the PPOAA outcome unspeci.ed, but we anticipate that future \nversions will explicitly forbid it. Thread 0 Thread 1 Thread 0 Thread 1 a: W[z]=1 c: R[y]=1 a: W[z]=1 \nc: R[y]=2 sync ctrl sync addr   b: W[y]=1 d: W[x]=1 b: W[y]=2 rf d: R[x]=0 rf po e: R[x]=1 rf e: \nR[x]=0 addr addr rf f: R[z]=0 rf f: R[z]=0 Test PPOCA : Allowed Test RSW : Allowed Aggressively out-of-order \nreads In the reads-from\u00ad same-writes (RSW) variant of MP above, the two reads of x, d and e, happen \nto read from the same write (the initial state). In this case, despite the fact that d and e are reading \nfrom the same address, the e/f pair can satisfy their reads out-of-order, before the c/d pair, permitting \nthe outcome shown. The address of e is known, so it can be satis.ed early, while the address of d is \nnot known until its address dependency on c is resolved. In contrast, in an execution in which d and \ne read from di.erent writes to x (test RDW, not shown), with another write to x by another thread), that \nis forbidden in the model, the commit of the .rst read (d) would force a restart of the second (e), \ntogether with its dependencies (including f), if e had initially read from a di.erent write to d. In \nactual implementations the restart might be earlier, when an invalidate is processed, but will have the \nsame observable e.ect. Coherence and lwsync: blw-w-006 This example shows that one cannot assume that \nthe transitive closure of lwsync and coherence edges guarantees ordering of write pairs, which is a challenge \nfor over-simpli.ed models. In our abstract machine, the fact that the storage subsystem com\u00admits to b \nbeing before c in the coherence order has no e.ect on the order in which writes a and d propagate to \nThread 2. Thread 1 does not read from either Thread 0 write, so they need not be sent to Thread 1, so \nno cumulativity is in play. Thread 0 Thread 1 Thread 2 a: W[x]=1 c: W[y]=2 e: R[z]=1 lwsync addr b: W[y]=1 \nd: W[z]=1 rf f: R[x]=0 Test blw-w-006 : Allowed In some implementations, and in the model, replacing \nboth lwsyncs by syncs (bsync-w-006) forbids this behaviour. In the model, it would require a cycle in \nabstract-machine execution time, from the point at which a propagates to its last thread, to the Thread \n0 sync ack, to the b write accept, to c propagating to Thread 0, to c propagating to its last thread, \nto the Thread 1 sync ack, to the d write accept, to d propagating to Thread 2, to e being satis.ed, to \nf being satis.ed, to a propagating to Thread 2, to a propagating to its last thread. The current architecture \ntext again leaves this unspeci.ed, but one would expect that adding sync everywhere (or, in this case, \nan address dependency between two reads) should regain SC. Coherence and lwsync: 2+2W and R01 The 2+2W+lwsyncs \nexample below is a case where the interac\u00adtion of coherence and lwsyncs does forbid some behaviour. Without \nthe lwsyncs (2+2W), the given execution is al\u00adlowed. With them, the writes must be committed in program \norder, but after one partial coherence commitment (say d be\u00adfore a)isdone, theother (b before c)isnolongerpermitted. \n(As this test has only writes, it may be helpful to note that the coherence order edges here could be \nobserved either by reading the .nal state or with additional threads reading x twice and y twice. Testing \nboth versions gives the same re\u00adsult.) This example is a challenge for axiomatic models with a view order \nper thread, as something is needed to break the symmetry. The given behaviour is also forbidden for the \nversion with syncs (2+2W+syncs). Thread 0 Thread 1 Thread 0 Thread 1 a: W[x]=1 c: W[y]=2 a: W[x]=1 c: \nW[y]=1 lwsync synclwsync  b: W[y]=2 d: W[x]=2 b: W[y]=1 rf d: R[x]=0 Test 2+2W+lwsyncs : Forbidden \nTest R01 : Allowed  The R01 test on the right above is a related case where we have not observed the \ngiven allowed behaviour in practice, but it is not currently forbidden by the architecture, and our model \npermits it. In the model, the writes can all reach the storage subsystem, the b/c partial coherence commitment \nbe made, c be propagated to Thread 0, the sync be committed and acknowledged, and d be satis.ed, all \nbefore a and the lwsync propagate to Thread 1. LB and (no) thin-air reads This LB dual of the SB example \nis another case where we have not observed the given allowed behaviour in practice, but it is clearly \narchi\u00adtecturally intended, so programmers should assume that fu\u00adture processors might permit it, and \nour model does. Adding data or address dependencies (e.g. in LB+datas) should for\u00adbid the given behaviour \n(the data dependency case could involve out-of-thin-air reads), but experimental testing here is vacuous, \nas LB itself is not observed. Thread 0 Thread 1 a: R[x]=1 c: R[y]=1 po po b: W[y]=1 d: W[x]=1 Test LB \n: Allowed Register shadowing Adir et al. [AAS03] give another variant of LB (which we call LB+rs, for \nRegister Shadow\u00ading), with a dependency on Thread 1 but re-using the same register on Thread 0, to demonstrate \nthe observability of shadow registers. That is also allowed in our model but not observable in our tests \n unsurprisingly, given that we have not observed LB itself. However, the following variant of MP does \nexhibit observable register shadowing: the two uses of r3 on Thread 1 do not prevent the second read \nbeing satis\u00ad.ed out-of-order, if the reads are into shadow registers. The reuse of a register is not \nrepresented in our diagrams, so for this example we have to give the underlying PowerPC assembly code. \na: W[x]=1 c: R[y]=1 sync po  b: W[y]=1 rf d: R[x]=0 Test MP+sync+rs : Allowed Thread 0 Thread 1 li r1,1 \nstw r1,0(r2) sync li r3,1 stw r3,0(r4) lwz r3,0(r2) mr r1,r3 lwz r3,0(r4) Initial state: 0:r2=x . 0:r4=y \n. 1:r2=y . 1:r4=x Allowed: 1:r1=1 . 1:r3=0 7. Experiments on hardware The use of small litmus-test programs \nfor discussing the be\u00adhaviour of relaxed memory models is well-established, but most previous work (with \nnotable exceptions) does not em\u00adpirically investigate how tests behave on actual hardware. We use our \nlitmus tool [AMSS11] to run tests on machines with various Power processors: Power G5 (aka PowerPC 970MP, \nbased on a POWER4 core), POWER 5, POWER 6, and POWER 7. The tool takes tests in PowerPC assem\u00adbly and \nruns them in a test harness designed to stress the processor, to increase the likelihood of interesting \nresults. This is black-box testing, and one cannot make any de.nite conclusions from the absence of some \nobservation, but our experience is that the tool is rather discriminating, identify\u00ading many issues with \nprevious models (and [AMSS10] report the discovery of a processor erratum using it). Our work is also \nunusual in the range and number of tests used. For this paper we have established a library based on \ntests from the literature [Pow09, BA08, AAS03, ARM08], new hand-written tests (e.g. the PPOCA, PPOAA, \nRSW, RDW, and 2+2W in \u00a76, and many others), and systematic variations of several tests (SB, MP, WRC, \nIRIW, ISA2, LB, and two others, RWC and WWC) with all possible combi\u00adnations of barriers or dependencies; \nwe call this the VAR3 family, of 314 tests. We ran all of these on Power G5, 6, and 7. In addition, we \nuse the diy tool [AMSS10] to sys\u00adtematically generate several thousand interesting tests with cycles \nof edges (dependencies, reads-from, coherence, etc.) of increasing size, and tested some of these. As \nan impor\u00adtant style point, we use tests with constraints on the .nal values (and hence on the values \nread) rather than loops, to make them easily testable. We give an excerpt of our exper\u00adimental results \nbelow, to give the .avour; more are avail\u00adable online [SSA+11]. For example, PPOCA was observable on \nPOWER G5 (1.0k/3.1G), not observable on POWER 6, and then observable again on POWER 7 consistent with \nthe less aggressively out-of-order microarchitecture of POWER 6. Test Model POWER 6 POWER 7 WRC Allow \nok 970k/12G ok 23M/ 93G WRC+data+addr Allow ok 562k/12G ok 94k/ 93G WRC+syncs Forbid ok 0/16G ok 0/110G \nWRC+sync+addr Forbid ok 0/16G ok 0/110G WRC+lwsync+addr Forbid ok 0/16G ok 0/110G WRC+data+sync Allow \nok 150k/12G ok 56k/ 94G PPOCA Allow unseen 0/39G ok 62k/141G PPOAA Forbid ok 0/39G ok 0/157G LB Allow \nunseen 0/31G unseen 0/176G  The interplay between manual testing, systematic test\u00ading, and discussion \nwith IBM sta. has been essential to the development of our model. For example: the PPOCA/PPOAA behaviour \nwas discovered in manual test\u00ading, leading us to conjecture that it should be explained by write-forwarding, \nwhich was later con.rmed by discussion; the blw-w-006 test, found in systematic testing, highlighted \ndi.culties with coherence and lwsync in an earlier model; and the role of coherence and sync acknowledgements \nin the current implementations arose from discussion. 8. Executing the model The intricacy of relaxed \nmemory models (and the number of tests we consider) make it essential also to have tool sup\u00adport for \nexploring the model, to automatically calculate the outcomes that the model permits for a litmus test, \nand to compare them against those observed in practice. To ease model development, and also to retain \ncon.dence in the tool, its kernel should be automatically generated from the model de.nition, not hand-coded. \nOur abstract machine is de.ned in Lem, a new lightweight language for machine\u00adformalised mathematical \nde.nitions, of types, functions and inductive relations [OBZNS]. From this we generate HOL4 prover code \n(and thence an automatically typeset version of the machine) and executable OCaml code, using a .nite \nset library, for the precondition and action of each transition rule. We also formalised a symbolic operational \nsemantics for the tiny fragment of the instruction set needed for our tests. Using those, we build an \nexhaustive memoised search proce\u00addure that .nds all possible abstract-machine executions for litmus tests. \nThis has con.rmed that the model has the expected behaviour for the 41 tests we mention by name in this \npaper, for the rest of the VAR3 family of 314 systematic tests, Thread 0 Thread 1  and for various other \ntests. In most cases the model exactly matches the Power7 experimental results, with the exception of \na few where it includes the experimental outcomes but is intentionally looser; this applies to 60 tests \nout of our batch of 333. Speci.cally: the model allows instructions to commit out of program order, which \npermits the LB and LB+rs test outcomes (not observed in practice); the model also allows an isync to \ncommit even in the presence of previously uncommitted memory accesses, whereas the speci.ed outcomes \nof tests such as WRC with an lwsync and isync have not been observed; and the R01 test outcome is not \nobserved. In all these cases the model follows the architectural intent, as con.rmed with IBM sta.. Our \nexperimental results also con.rm that Power G5 and 6 are strictly stronger than Power 7 (though in di.erent \nways): we have not seen any test outcome on those which is not also observable on Power 7. The model \nis thus also sound for those, to the best of our knowledge. 9. Related Work There has been extensive \nprevious work on relaxed mem\u00adory models. We focus on models for the major current pro\u00adcessor families \nthat do not have sequentially consistent be\u00adhaviour: Sparc, x86, Itanium, ARM, and Power. Early work \nby Collier [Col92] developed models based on empirical test\u00ading for the multiprocessors of the day. For \nSparc, the ven\u00addor documentation has a clear Total Store Ordering (TSO) model [SFC91, Spa92]. It also \nintroduces PSO and RMO models, but these are not used in practice. For x86, the vendor intentions were \nuntil recently quite unclear, as was the behaviour of processor implementations. The work by Sarkar, \nOwens, et al. [SSZN+09, OSS09, SSO+10] suggests that for normal user-or system-code they are also TSO. \nTheir work is in a similar spirit to our own, with a mech\u00adanised semantics that is tested against empirical \nobserva\u00adtion. Itanium provides a much weaker model than TSO, but one which is more precisely de.ned by \nthe vendor than x86 [Int02]; it has also been formalised in TLA [JLM+03] and in higher-order logic [YGLS03]. \nFor Power, there have been several previous models, but none are satisfactory for reasoning about realistic \nconcurrent code. In part this is because the architecture has changed over time: the lwsync barrier has \nbeen added, and barri\u00aders are now cumulative. Corella, Stone and Barton [CSB93] gave an early axiomatic \nmodel for PowerPC, but, as Adir et al. note [AAS03], this model is .awed (it permits the non-SC .nal \nstate of the MP+syncs example we show in \u00a72). Stone and Fitzgerald later gave a prose description of \nPowerPC memory order, largely in terms of the microarchitecture of the time [SF95]. Gharachorloo [Gha95] \ngives a variety of models for di.erent architectures in a general framework, but the model for the PowerPC \nis described as approxi\u00admate ; it is apparently based on Corella et al. [CSB93] and on May et al. [MSSW94]. \nAdve and Gharachorloo [AG96] make clear that PowerPC is very relaxed, but do not dis\u00adcuss the intricacies \nof dependency-induced ordering, or the more modern barriers. Adir, Attiya, and Shurek give a de\u00adtailed \naxiomatic model [AAS03], in terms of a view order for each thread. The model was developed through an \niterative process of successive re.nements, numerous discussions with the PowerPC architects, and analysis \nof examples and coun\u00adterexamples , and its consequences for a number of litmus tests (some of which we \nuse here) are described in detail. These facts inspire some con.dence, but it is not easy to understand \nthe force of the axioms, and it describes non\u00adcumulative barriers, following the pre-PPC 1.09 PowerPC \narchitecture; current processors appear to be quite di.er\u00adent. More recently, Chong and Ishtiaq give \na preliminary model for ARM [CI08], which has a very similar architected memory model to Power. In our \ninitial work in this area [AFI+09], we gave an axiomatic model based on a reading of the Power ISA 2.05 \nand ARM ARM speci.cations, with experimental results for a few tests (described as work in progress); \nthis seems to be correct for some aspects but to give an unusably weak semantics to barriers. More recently, \nwe gave a rather di.erent axiomatic model [AMSS10], further developed in Alglave s the\u00adsis [Alg10] as \nan instance of a general framework; it models the non-multi-copy-atomic nature of Power (with examples \nsuch as IRIW+addrs correctly allowed) in a simple global\u00adtime setting. The axiomatic model is sound with \nrespect to our experimental tests, and on that basis can be used for reasoning, but it is weaker than \nthe observed behaviour or architectural intent for some important examples. More\u00adover, it was based principally \non black-box testing and its relationship to the actual processor implementations is less clear than \nthat for the operational model we present here, which is more .rmly grounded on microarchitectural and \nar\u00adchitectural discussion. In more detail, the axiomatic model is weaker than one might want for lwsync \nand for cumulativity: it allows MP+lwsync+addr and ISA2+sync+data+addr, which are not observed and which \nare intended to be ar\u00adchitecturally forbidden. It also forbids R01, which is not ob\u00adserved but architecturally \nintended to be allowed, and which is allowed by the model given here. The two models are thus incomparable. \nWe mention also Lea s JSR-133 Cookbook for Compiler Writers [Lea], which gives informal (and approximate) \nmod\u00adels for several multiprocessors, and which highlights the need for clear models. 10. Conclusion To \nsummarise our contribution, we have characterised the actual behaviour of Power multiprocessors, by example \nand by giving a semantic model. Our examples include new tests illustrating several previously undescribed \nphenomena, together with variations of classic tests and a large suite of automatically generated tests; \nwe have experimentally investigated their behaviour on a range of processors. Our model is: rigorous \n(in machine-typechecked mathematics); experimentally validated; accessible (in an abstract machine style, \nand detailed here in a few pages of prose); usable (as witnessed by the explanations of examples); supported \nby atool, for calculating the possible outcomes of tests; and su.cient to explain the subtle behaviour \nexposed by our examples and testing. It is a new abstraction, maintaining coherence and cumulativity \nproperties by .at but modelling out-of-order and speculative execution explicitly. The model should provide \na good intuition for developers of concurrent systems code for Power multiprocessors, e.g. of concurrency \nlibraries, language runtimes, OS kernels, and optimising compilers. Moreover, as the ARM architecture \nmemory model is very similar, it may well be applicable (with minor adaptation) to ARM. The model also \nopens up many directions for future re\u00adsearch in veri.cation theory and tools. For example, it is now \npossible to state results about the correct compilation of the C++0x concurrency primitives to Power \nprocessors, and to consider barrier-and dependency-aware optimisa\u00adtions in that context. We have focussed \nhere primarily on the actual behaviour of implementations, but there is also work required to identify \nthe guarantees that programmers actually rely on, which may be somewhat weaker some of our more exotic \nexamples are not natural use-cases, to the best of our knowledge. There is also future work required \nto broaden the scope of the model, which here covers only cacheable memory without mixed-size accesses. \n We described our model principally in its own terms and in terms of the observed behaviour, without \ngoing into de\u00adtails of the relationship between the model and the underly\u00ading microarchitecture, or with \nthe vendor architecture spec\u00adi.cation [Pow09]; this remains future work. A central notion of the memory \nmodel text in the latter is that of when a memory read or write by one thread is performed with re\u00adspect \nto another, which has a hypothetical (or subjunctive) de.nition, e.g. for loads: A load by a processor \nP1 is per\u00adformed with respect to a processor P2 when the value to be returned can no longer be changed \nby a store by P2 ,where that P2 store may not even be present in the program un\u00adder consideration (again, \nARM is similar). This de.nition made perfect sense in the original white-box setting [DSB86], where the \ninternal structure of the system was known and one can imagine the hypothetical store by P2 appearing \nat some internal interface, but in the black-box setting of a commercial multiprocessor, it is hard or \nimpossible to make it precise, especially with examples such as PPOCA. Our abstract-machine model may \nprovide a stepping stone to\u00adwards improved architectural de.nitions, perhaps via new axiomatic characterisations. \nAcknowledgements We acknowledge funding from EP-SRC grants EP/F036345, EP/H005633, and EP/H027351, from \nANR project ParSec (ANR-06-SETIN-010), and from INRIA associated team MM. References [AAS03] A. Adir, \nH. Attiya, and G. Shurek. Information-.ow models for shared memory with an application to the PowerPC \narchitecture. IEEE Trans. Parallel Distrib. Syst., 14(5):502 515, 2003. [AFI+09] J. Alglave, A. Fox, \nS. Ishtiaq, M. O. Myreen, S. Sarkar, P. Sewell, and F. Zappa Nardelli. The se\u00admantics of Power and ARM \nmultiprocessor machine code. In Proc. DAMP 2009, January 2009. [AG96] S. V. Adve and K. Gharachorloo. \nShared memory consistency models: A tutorial. IEEE Computer, 29(12):66 76, 1996. [Alg10] Jade Alglave. \nA Shared Memory Poetics. PhD thesis, Universit\u00b4e Paris 7 Denis Diderot, November 2010. [AMSS10] J. Alglave, \nL. Maranget, S. Sarkar, and P. Sewell. Fences in weak memory models. In Proc. CAV, 2010. [AMSS11] J. \nAlglave, L. Maranget, S. Sarkar, and P. Sewell. Litmus: Running tests against hardware. In Proc. TACAS, \n2011. [ARM08] ARM. ARM Barrier Litmus Tests and Cookbook, October 2008. PRD03-GENC-007826 2.0. [BA08] \nH.-J. Boehm and S. Adve. Foundations of the C++ concurrency memory model. In Proc. PLDI, 2008. [BOS+11] \nM. Batty, S. Owens, S. Sarkar, P. Sewell, and T. Weber. Mathematizing C++ concurrency. In Proc. POPL, \n2011. [CI08] N. Chong and S. Ishtiaq. Reasoning about the ARM weakly consistent memory model. In MSPC, \n2008. [Col92] W.W. Collier. Reasoning about parallel architectures. Prentice-Hall, Inc., 1992. [CSB93] \nF. Corella, J. M. Stone, and C. M. Barton. A formal speci.cation of the PowerPC shared memory archi\u00adtecture. \nTechnical Report RC18638, IBM, 1993. [DSB86] M. Dubois, C. Scheurich, and F. Briggs. Memory access bu.ering \nin multiprocessors. In ISCA, 1986. [Gha95] K. Gharachorloo. Memory consistency models for shared-memory \nmultiprocessors. WRL Research Re\u00adport, 95(9), 1995. [Int02] Intel. A formal speci.cation of Intel Itanium \nproces\u00adsor family memory ordering, 2002. developer.intel. com/design/itanium/downloads/251429.htm. [JLM+03] \nR. Joshi, L. Lamport, J. Matthews, S. Tasiran, M. Tuttle, and Y. Yu. Checking cache-coherence protocols \nwith TLA+. Form. Methods Syst. Des., 22:125 131, March 2003. [KSSF10] R. Kalla, B. Sinharoy, W. J. Starke, \nand M. Floyd. Power7: IBM s next-generation server processor. IEEE Micro, 30:7 15, March 2010. [Lea] \nD. Lea. The JSR-133 cookbook for compiler writers. http://gee.cs.oswego.edu/dl/jmm/cookbook.html. [LSF+07] \nH.Q.Le,W.J.Starke, J.S.Fields,F.P.O Connell, D. Q. Nguyen, B. J. Ronchetti, W. Sauer, E. M. Schwarz, \nand M. T. Vaden. IBM POWER6 microar\u00adchitecture. IBM Journal of Research and Develop\u00adment, 51(6):639 662, \n2007. [MSSW94] C. May, E. Silha, R. Simpson, and H. Warren, edi\u00adtors. The PowerPC architecture: a speci.cation \nfor a new family of RISC processors. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, 1994. [OBZNS] \nS. Owens, P. B\u00a8 ohm, F. Zappa Nardelli, and P. Sewell. Lightweight tools for heavyweight semantics. Sub\u00admitted \nfor publication http://www.cl.cam.ac.uk/ ~so294/lem/. [OSS09] S. Owens, S. Sarkar, and P. Sewell. A better \nx86 memory model: x86-TSO. In Proc. TPHOLs, pages 391 407, 2009. [Pow09] Power ISATM Version 2.06. IBM, \n2009. [SF95] J. M. Stone and R. P. Fitzgerald. Storage in the PowerPC. IEEE Micro, 15:50 58, April 1995. \n[SFC91] P. S. Sindhu, J.-M. Frailong, and M. Cekleov. Formal speci.cation of memory models. In Scalable \nShared Memory Multiprocessors, pages 25 42. Kluwer, 1991. [SKT+05] B. Sinharoy, R. N. Kalla, J. M. Tendler, \nR. J. Eicke\u00admeyer, and J. B. Joyner. POWER5 system microar\u00adchitecture. IBM Journal of Research and Develop\u00adment, \n49(4-5):505 522, 2005. [Spa92] The SPARC Architecture Manual, V. 8.SPARC International, Inc., 1992. Revision \nSAV080SI9308. http://www.sparc.org/standards/V8.pdf. [SSA+11] S. Sarkar, P. Sewell, J. Alglave, L. Maranget, \nand D. Williams. Understanding POWER mul\u00adtiprocessors. www.cl.cam.ac.uk/users/pes20/ ppc-supplemental, \n2011. [SSO+10] P. Sewell, S. Sarkar, S. Owens, F. Zappa Nardelli, and M. O. Myreen. x86-TSO: A rigorous \nand usable programmer s model for x86 multiprocessors. Com\u00admunications of the ACM, 53(7):89 97, July \n2010. [SSZN+09] S. Sarkar, P. Sewell, F. Zappa Nardelli, S. Owens, T. Ridge, T. Braibant, M. Myreen, \nand J. Alglave. The semantics of x86-CC multiprocessor machine code. In Proc. POPL 2009, January 2009. \n[YGLS03] Y. Yang, G. Gopalakrishnan, G. Lindstrom, and K. Slind. Analyzing the Intel Itanium memory or\u00addering \nrules using logic programming and SAT. In Proc. CHARME, LNCS 2860, 2003.   \n\t\t\t", "proc_id": "1993498", "abstract": "<p>Exploiting today's multiprocessors requires high-performance and correct concurrent systems code (optimising compilers, language runtimes, OS kernels, etc.), which in turn requires a good understanding of the observable processor behaviour that can be relied on. Unfortunately this critical hardware/software interface is not at all clear for several current multiprocessors.</p> <p>In this paper we characterise the behaviour of IBM POWER multiprocessors, which have a subtle and highly relaxed memory model (ARM multiprocessors have a very similar architecture in this respect). We have conducted extensive experiments on several generations of processors: POWER G5, 5, 6, and 7. Based on these, on published details of the microarchitectures, and on discussions with IBM staff, we give an abstract-machine semantics that abstracts from most of the implementation detail but explains the behaviour of a range of subtle examples. Our semantics is explained in prose but defined in rigorous machine-processed mathematics; we also confirm that it captures the observable processor behaviour, or the architectural intent, for our examples with an executable checker. While not officially sanctioned by the vendor, we believe that this model gives a reasonable basis for reasoning about current POWER multiprocessors.</p> <p>Our work should bring new clarity to concurrent systems programming for these architectures, and is a necessary precondition for any analysis or verification. It should also inform the design of languages such as C and C++, where the language memory model is constrained by what can be efficiently compiled to such multiprocessors.</p>", "authors": [{"name": "Susmit Sarkar", "author_profile_id": "81392603911", "affiliation": "University of Cambridge, Cambridge, United Kingdom", "person_id": "P2690531", "email_address": "Susmit.Sarkar@cl.cam.ac.uk", "orcid_id": ""}, {"name": "Peter Sewell", "author_profile_id": "81100511814", "affiliation": "University of Cambridge, Cambridge, United Kingdom", "person_id": "P2690532", "email_address": "Peter.Sewell@cl.cam.ac.uk", "orcid_id": ""}, {"name": "Jade Alglave", "author_profile_id": "81392617420", "affiliation": "Oxford University, Oxford, United Kingdom", "person_id": "P2690533", "email_address": "Jade.Alglave@comlab.ox.ac.uk", "orcid_id": ""}, {"name": "Luc Maranget", "author_profile_id": "81100574739", "affiliation": "INRIA, Rocquencourt, France", "person_id": "P2690534", "email_address": "Luc.Maranget@inria.fr", "orcid_id": ""}, {"name": "Derek Williams", "author_profile_id": "81485642519", "affiliation": "IBM Austin, Austin, TX, USA", "person_id": "P2690535", "email_address": "striker@us.ibm.com", "orcid_id": ""}], "doi_number": "10.1145/1993498.1993520", "year": "2011", "article_id": "1993520", "conference": "PLDI", "title": "Understanding POWER multiprocessors", "url": "http://dl.acm.org/citation.cfm?id=1993520"}