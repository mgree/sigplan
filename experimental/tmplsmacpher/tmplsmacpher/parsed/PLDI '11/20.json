{"article_publication_date": "06-04-2011", "fulltext": "\n Mostly-Automated Veri.cation of Low-Level Programs in Computational Separation Logic Adam Chlipala \nHarvard University Cambridge, MA, USA adamc@cs.harvard.edu Abstract Several recent projects have shown \nthe feasibility of verifying low\u00adlevel systems software. Veri.cations based on automated theorem\u00adproving \nhave omitted reasoning about .rst-class code pointers, which is critical for tasks like certifying implementations \nof threads and processes. Conversely, veri.cations that deal with .rst-class code pointers have featured \nlong, complex, manual proofs. In this paper, we introduce the Bedrock framework, which supports mostly-automated \nproofs about programs with the full range of features needed to implement, e.g., language runtime systems. \nThe heart of our approach is in mostly-automated discharge of veri.cation conditions inspired by separation \nlogic. Our take on separation logic is computational, in the sense that function speci\u00ad.cations are usually \nwritten in terms of reference implementations in a purely functional language. Logical quanti.ers are \nthe most challenging feature for most automated veri.ers; by relying on functional programs (written \nin the expressive language of the Coq proof assistant), we are able to avoid quanti.ers almost entirely. \nThis leads to some dramatic improvements compared to both past work in classical veri.cation, which we \ncompare against with im\u00adplementations of data structures like binary search trees and hash tables; and \npast work in veri.ed programming with code pointers, which we compare against with examples like function \nmemoiza\u00adtion and a cooperative threading library. Categories and Subject Descriptors F.3.1 [Logics and \nmeanings of programs]: Mechanical veri.cation; D.2.4 [Software Engineer\u00ading]: Correctness proofs, formal \nmethods General Terms Languages, Veri.cation Keywords interactive proof assistants, separation logic, \nlow-level programming languages, functional programming 1. Introduction The desirability of verifying \nsystems infrastructure software has long been recognized. If our operating systems and runtime sys\u00adtems \nare not correct, then we can hope for little guarantee about the behavior of our applications. Thus, \nthe pay-off of formal cor\u00adrectness veri.cation of systems software is large, but the human Permission \nto make digital or hard copies of all or part of this work for personal or classroom use is granted without \nfee provided that copies are not made or distributed for pro.t or commercial advantage and that copies \nbear this notice and the full citation on the .rst page. To copy otherwise, to republish, to post on \nservers or to redistribute to lists, requires prior speci.c permission and/or a fee. PLDI 11, June 4 \n8, 2011, San Jose, California, USA. Copyright c &#38;#169; 2011 ACM 978-1-4503-0663-8/11/06. . . $10.00 \n cost of veri.cation has been considered to be so great as to offset the bene.t. Several recent projects \nhave given reason to reconsider that position. The L4.veri.ed project [18] has .nished a complete functional \ncorrectness veri.cation for a realistic operating system microkernel, based on a quite non-trivial amount \nof proof code for the Isabelle/HOL proof assistant [27]. The Verve project [30] has completed a mostly-automated \nBoogie [1] veri.cation of the core of an operating system which is designed to serve as a kind of run\u00adtime \nsystem for code written in C#. The productivity advantage of mostly-automated veri.cation over manual \ntactic-based proving seems clear. Unfortunately, the well-known automated veri.cation systems restrict \nspeci.cations to .rst-order logic, which creates a dramatic restriction on which speci.cations may be \nexpressed compactly, or even expressed at all. For instance, it seems unlikely that these .rst-order \nsystems will ever be able to handle a proof of type safety for a non-trivial programming language, but \nexactly that kind of proof is required to complete the Verve project. It is possible to use different \ntools for different parts of a veri.cation, but the programmer s task is certainly made simpler by sticking \nto a single environment. This choice should also help to minimize the trusted code base, which seems \nquite relevant, given the security consequences of operating system bugs. There has been a signi.cant \namount of work on low-level veri\u00ad.cation systems that are higher-order in at least two distinct senses: \nthey allow higher-order logic in speci.cations, and they allow rea\u00adsoning about code pointers as data. \nFor instance, the Certi.ed As\u00adsembly Programming project has suggested several such program logics, including \nXCAP [24] and SCAP [13]. These systems have been used to complete complex veri.cations of threading libraries \nand other examples involving .rst-class code pointers. Unfortu\u00adnately, the Coq [8] proofs involved are \nvery long, complex, de\u00adtailed, and brittle to changes of speci.cation. In this paper, we introduce Bedrock, \na Coq library in which we have attempted to reconcile the above concerns. In particular, Bedrock is: \n Low-level: The framework supports veri.cation of programs that, for performance reasons or otherwise, \ncannot tolerate any abstraction beyond that associated with assembly language.  Foundational: The output \nof a Bedrock veri.cation is a theo\u00adrem whose statement depends only on the predicates chosen for the \nkey speci.cations and on the operational semantics of some machine language. That is, there is no need \nto trust that the ver\u00adi.cation framework is bug-free; rather, one need only trust the usual Coq proof-checker \nand the formalization of the machine language semantics.  Higher-order: Bedrock facilitates quite pleasant \nreasoning about code pointers as data.   Computational: Many useful functions are speci.ed most ef\u00adfectively \nby comparing with reference implementations in a pure functional language. Bedrock supports that model, \nbacked by the full expressive power of Coq s usual programming lan\u00adguage.  Structured: Bedrock is an \nextensible programming language: any client program may add new control .ow constructs by providing their \n(sound) proof rules. For example, adding high\u00adlevel syntax for your own calling convention or exception \nhan\u00addling construct is relatively straightforward and does not require tweaking the core library code. \nUnfortunately, for reasons of space, we will not discuss this aspect of Bedrock more in the present paper. \n Mostly-automated: Tactics (proof procedures) automate veri\u00ad.cation condition generation (in a form \ninspired by separation logic [28]) and most of the process of discharging those condi\u00adtions. Manual proof \neffort is generally con.ned to, .rst, annota\u00adtions specifying which simpli.cation rules for abstract \nimpure predicates should be applied at which program points; and, sec\u00adond, hint lemmas about mathematical \nobjects like sets, maps, and lists. Crucially, neither kind of manual work deals explic\u00aditly with program \nsyntax, memories, or program states.  The next section of the paper introduces Bedrock with the com\u00adplete \ncode for two simple examples: swapping two values in mem\u00adory and incrementing the integer value in every \nnode of a linked list. After walking through the examples, we explain how the rea\u00adsoning we applied may \nbe generalized into a mechanical procedure for reducing veri.cation conditions in separation logic to \nsimpler conditions that deal only with normal mathematical objects. This forms the basis of a very effective \nveri.cation framework. To back up that claim, we perform two main sorts of comparison. First, we compare \nagainst some examples implemented with Jahob [32], a representative of recent developments in mostly\u00adautomated \nfunctional correctness veri.cation of data structure im\u00adplementations. While Jahob deals with high-level \nJava programs, in Bedrock we must implement a malloc and free library before get\u00adting started, and all \nprograms must contain explicit memory man\u00adagement code for both the heap and the stack. Despite that \nhand\u00adicap, our annotation burden is comparable to that documented for Jahob, and we even achieve much \nmore compact proofs for some examples, including hash tables, thanks to the computational spec\u00adi.cation \napproach we adopt. Second, we implement a set of interesting programs that use code pointers in ways \nbeyond usual function calling conventions. These examples cannot even be speci.ed, let alone veri.ed, \nin clas\u00adsical veri.ers. In Bedrock, each of our higher-order examples re\u00adquires only about a page of \nannotation, compared to thousands of lines in related work. The examples are an abstract memoization \nmodule, operating on imperative functions that compute pure func\u00adtions over machine words, using arbitrary \npersistent local state; destructive concatenation of linked lists, written in continuation\u00adpassing style \nusing explicit closures, which is the largest example from the .rst XCAP paper [24]; and a simple cooperative \nthread\u00ading library, similar to an earlier XCAP implementation [25, 26]. For the examples where we compare \nagainst past XCAP implementa\u00adtions, we reduce annotation overhead by a factor of about 100, and we also \npass the qualitative threshold of giving mostly-automated proofs that often adapt automatically to speci.cation \nchanges. The complete code for the framework and the examples is available online at: 2. Bedrock by \nExample The C programming language is often described as a macro as\u00adsembly language. Bedrock .ts that \ndescription taken literally. The Bedrock Coq library is parametrized over a machine language with some \nconcept of a basic block. In this paper, we deal with the framework instantiated to a simple idealized \nmachine language. The language is idealized in that it has in.nite-sized words and an in.nite memory. \nIn other respects, it is a realistic machine lan\u00adguage, with a .nite supply of global registers, a memory \nthat can be modeled as an in.nite array of words, and so on. Further details of this language will not \nbe critical for what follows, and our code examples will provide further demonstrations. We expect that \nthe Bedrock framework is applicable both to more realistic machine languages and, for slightly higher-level \nprograms that do not need to manipulate machine contexts, to common compiler intermediate languages. \nBedrock generalizes the XCAP program logic [24], in the sense that, in place of deduction rules for particular \ninstructions of a .xed machine language, Bedrock can be thought of as having a single rule that operates \na basic block at a time. The rule (and the other deductive parts of Bedrock) is parametrized over a standard \nopera\u00adtional semantics for the machine language in use. Every basic block is assigned a logical precondition, \nand the basic block rule requires that, if execution enters a block in a state satisfying its precondition, \nthen execution must proceed safely and reach a jump to some block whose precondition is then satis.ed. \nIn our idealized language, ex\u00adecution only goes wrong when a jump is made to a label that does not exist \nin the (unveri.ed) program. Therefore, the block precon\u00additions are the main component of program correctness \ntheorems. XCAP is based on an assertion logic (the language in which preconditions are written) whose \ndesign involves some subtleties which we will mostly ignore in this presentation, referring the reader \nto the .rst XCAP paper [24] for further details. It is a rea\u00adsonable approximation to say that the assertion \nlanguage is mostly a standard second-order logic, in the sense that quanti.cation is allowed over both \nnormal mathematical objects and over speci.\u00adcations themselves, but not over some more exotic domains \nlike functions over speci.cations. So far, this summary describes a sub\u00adlanguage of the logic built into \nCoq. All of the differences stem from an unusual connective of the form i@@p, which says that there is \na basic block at program counter i whose precondition is implied by speci.cation p. This connective is \nthe source of support for reasoning about sophisticated uses of code pointers, based on the possibility \nfor quanti.ed variables to appear inside p. We may even use second-order variables that stand for other \nspeci.cations. Prior work with XCAP has involved coding assembly programs directly. Bedrock makes the \nprogrammer s job easier by supporting a structured programming notation, such that programs look much \nlike C code where atomic statements are literal assembly instruc\u00adtions. The details of Bedrock s low-level \ndeductive system do not appear in veri.cations based on structured programming. Rather, we apply the \nstandard technique of veri.cation condition genera\u00adtion, so that the conditions to be proved do not refer \nto program syntax. As is usual, veri.cation condition generation depends on invariant annotations in \nthe program code. Structured syntax and its associated proofs are automatically compiled to proofs about \nnormal machine code programs. The rest of the details of Bedrock veri.cation are best intro\u00adduced through \nexamples that demonstrate the core of our proof methodology, a procedure for simplifying veri.cation \nconditions that are based on separation logic [28]. Figure 1 provides a quick informal reference for \nthe programming and speci.cation features that we will use. http://adam.chlipala.net/bedrock/  Program \nsyntax Ri Register $[E] Memory dereference L<-E Assignment command C;; C Command sequencing Goto E Computed \njump [p]While(E) {C } Loop (with invariant) Use [lemmaName] Proof hint Program states st st#Ri Project \nvalue of register st.[E] Project value stored at memory address st[L <-E] Update based on assignment \ncommand Assertions p st ~> p State predicate (st bound in p) p/\\ p Conjunction Ex x. p Existential quanti.cation \nExx :T.p Existential with type annotation i@@ A Assertion about precondition of a code pointer ![P ]st \nSeparation logic assertion Separation logic assertions P [<p >] Lift normal assertion u ==> v Pointer \nu points to value v P*P Separating conjunction Ex x. P Existential quanti.cation Exx :T.P Existential \nwith type annotation ![x ] Second-order speci.cation variable !{ f v1 ... vn } Abstract predicate Figure \n1. Bedrock syntax reference Definition swap := bmodule {{ bfunction \"swap\" [st ~> Ex fr : hprop, Exa \n:nat,Exb:nat, ![ st#R0 ==> a * st#R1 ==> b * ![fr] ] st /\\ st#Rret @@ (st ~> ![ st#R1 ==> a * st#R0 ==> \nb * ![fr] ] st ) ] { R2 <-$[R0];; $[R0] <-$[R1];; $[R1] <-R2;; Goto Rret } }}. Theorem swapOk : moduleOk \nswap. structured; sep. Qed. Figure 2. A Bedrock function implementing pointer swapping 2.1 Swapping \nthe Values at Two Memory Locations Figure 2 gives the complete code to implement and verify a Bedrock \nfunction for swapping the values at two memory ad\u00addresses. This code is processed by the normal, unmodi.ed \nCoq interpreter, thanks to the use of Coq s syntax extension mechanism (or macro system ). We start by \nde.ning a code module with the bmodule keyword. Inside the module is a single function \"swap\", introduced \nwith the bfunction keyword. After the function name appears the function precondition, which we will \nturn to shortly. First, we note that the function body here is a list of assembly in\u00adstructions. The \nfunction inputs are in registers R0 and R1, and the .rst three instructions use register R2 as a temporary \nin swapping the contents of the memory cells pointed to by the inputs. The no\u00adtation <-is for assignment, \nwhile $[E] stands for the memory cell pointed to by expression E. The last instruction returns from the \nfunction by jumping to the return pointer stored in register Rret. The function precondition may appear \ndaunting at .rst. It uses a few standard concepts with perhaps unusual ASCII syntax, along with a few \nless usual concepts. First, the notation st ~> p is a special lambda form that triggers the use of a \nspecial parsing non\u00adterminal for p, so that p is parsed as an XCAP-style assertion. The variable st is \nbound in p; it is the function argument, standing for a machine state. The body of swap s precondition \nbegins with three existential quanti.ers, written Ex. The simplest two of the three bound vari\u00adables \nare a and b, which stand for the initial contents of the mem\u00adory cells pointed to by R0 and R1, respectively. \nIn this idealized machine language, memory cells contain natural numbers, so we annotate a and b with \nthe Coq type nat. We will return shortly to the remaining variable fr. After the quanti.ers, we have \nan assertion of the form ![ P ] st, where P is an assertion of separation logic that we are requiring \nmust hold in machine state st. Within these brackets, we may write u ==> v to assert that the memory \ncell at address u holds word v. The separating conjunction P*Q, which has lower parsing precedence than \n==>, asserts that the machine memory may be broken into two disjoint pieces, such that P satis.es one \nand Q the other. In our example, we calculate two memory cell addresses for points-to facts using the \n# operator, which projects a particular register value out of a machine state. A third assertion is added \nto this memory precondition with the separating conjunction: we see the variable fr used with the syntax \n![fr]. Inside a separation logic assertion, the ![ ] notation indicates a speci.cation variable. That \nis, in this example, we are quantifying over a memory speci.cation fr and then using it to describe one \npart of the initial memory. Concretely, fr has type hprop, the type of predicates over partial heaps. \nThe name fr is meant to be suggestive of frame condition, a condition that describes all parts of memory \nthat are irrelevant to the present function. Standard separation logic contains the frame rule for the \nstatement partial correctness judgment {P }s{Q}: {P }s{Q} {P * R}s{Q * R}  That is, when a statement \nsatis.es a particular precondition and postcondition pair, the statement is also correct with respect \nto the conjunction of an arbitrary speci.cation R to both precondition and postcondition. R stands for \nsome additional part of the memory that s will not be allowed to touch. It is more natural to describe \nassembly programs with preconditions alone, especially to facili\u00adtate unusual control patterns that do \nnot .t the stack-based function convention that normal separation logic assumes. One consequence is that \nthe usual frame rule is inapplicable. Instead, we may repre\u00adsent frame conditions explicitly with second-order \nquanti.cation, as we do in this example. To give us the usual .exibility of the frame rule for function \ncalls, we must refer to the frame condition in one more place. This is part of a use of the i@@p connective \nthat we introduced earlier. We state that register Rret points to a code block that is safe to jump to \nif a particular condition holds. We have a nested use of the ~> notation, this time de.ning a speci.cation \nover a new state st , which effectively stands for the state upon returning from the function, while \nst stands for the state upon calling the function. The return-time invariant is identical to the initial \nseparation logic assertion, except that we have swapped the values found at the two distinguished memory \nlocations, as one would expect from this function s informal speci.cation. Crucially, the same frame \nvariable appears in the two snapshots of memory. Since we impose no further conditions on fr, it will \nbe impossible to prove that any memory slice satis.es fr, with the sole exception of the memory on entry \nto the function. Therefore, if the function manages to satisfy the condition attached to the return pointer, \nall memory but the two distinguished cells will have been preserved. One further subtle point in this \nspeci.cation style deserves some explanation. Most classical veri.cation tools support ghost variables, \nwhich are additional program variables used solely for speci.cation and veri.cation. In more standard \nspeci.cations for our swap example, we would probably see ghost variables used in place of existential \nquanti.cation for the initial values a and b of the two distinguished memory cells. In classical .rst-order \ntools, ghost variables increase expressiveness beyond simple existential quanti.cation, but only because \nfunction return pointers are not made explicit. Ghost variables provide the ability to share variables \nbetween function preconditions and postconditions. When we are able to talk about return pointers in \na .rst-class way, as we have in this example, we need only preconditions, and so there is no further \nneed for sharing of variables between specs. Once the function has a speci.cation, we can prove that \nthe speci.cation is met. A Coq Theorem command begins our proof of that fact. The proof is given with \na Coq proof script, a program in a domain-speci.c language for proof search. In this case, our script \nsays that the proof proceeds in two steps. First, we call the Bedrock tactic structured to reduce the \ntheorem to the truth of a set of automatically-generated veri.cation conditions. We use the semicolon \noperator to chain on a second tactic, which should be run on every veri.cation condition. This second \ntactic is the Bedrock tactic sep, a generic simpli.er for conditions involving separation logic. In our \nparticular example, there is one condition to prove, which amounts to showing the following implication, \nwhere we use the notation st[i] to denote the effect of executing instruction i in state st: State 1: \nst Pred.1: R0==>a*R1==>b* ![fr] State 2: st[R2 <-R0][$[R0] <-$[R1]][$[R1] <-R2] Pred.2: R1==>a*R0==>b* \n![fr] We want to show that the truth of Predicate 1 in State 1 implies the truth of Predicate 2 in State \n2. A good .rst step is to inline the state information into the two predicates, so that both are expressed \nin terms of the same variable st. We write st.[E] for the value found at memory cell E in state st. Pred. \n1: st#R0 ==> a * st#R1 ==> b * ![fr] Changes: R0 <-st.[st#R1]; R1 <-st.[st#R0] Pred. 2: st#R1 ==> a * \nst#R0 ==> b * ![fr] Next, we can use Predicate 1 to simplify the list of changes. Any read st.[u] may \nbe replaced by v, whenever Predicate 1 contains a conjunct u ==> v. Our new implication is: Pred. 1: \nst#R0 ==> a * st#R1 ==> b * ![fr] Changes: R0 <-b; R1 <-a Pred. 2: st#R1 ==> a * st#R0 ==> b * ![fr] \n To .nish the proof, we want to eliminate the need to take into account the set of memory changes. Our \nstrategy to do so is to execute the changes symbolically in Predicate 1. That is, for each write of value \nv to memory at address u, we .nd a fact u ==> v in Predicate 1 and replace v with v. That algorithm reduces \nthe implication to: Pred. 1: st#R0 ==> b * st#R1 ==> a * ![fr] Pred. 2: st#R1 ==> a * st#R0 ==> b * ![fr] \nThis implication is almost trivially true, modulo the fact of * s commutativity. Applying such facts \nmanually can be quite a hassle. Our separation logic tactic takes commutativity and associativity into \naccount and automatically .nishes proofs like this by cancel\u00adlation. That is, we iterate through .nding \na conjunct that appears on both sides of the implication and crossing it out. If the veri\u00ad.cation has \nbeen set up properly, cancellation eventually gives us an implication between identical formulas, which \nwe can dispatch trivially. We have completed our .rst Bedrock veri.cation, and we would like now to stress \none key property of our approach that distin\u00adguishes it from the mostly-automated veri.cation methods \nembod\u00adied in tools like ESC [14], Boogie [1], and Jahob [32]. Solvers for boolean satis.ability (SAT) \nand satis.ability modulo theories (SMT) have become increasingly practical in program veri.cation, and \nmore and more projects work by reducing veri.cation condi\u00adtions to domains that such solvers understand. \nThe solver works by a mechanical process with too many steps for humans to follow closely. Completeness \nguarantees for decision procedures some\u00adtimes make this loss of simplicity acceptable. Unfortunately, \nthe constraint-solver approach to veri.cation tends to lead to relatively inexpressive speci.cation languages, \nas these solvers do not sup\u00adport techniques like local universal quanti.cation over speci.cation variables. \nBedrock relies on a very different approach. Users of SMT\u00adbased veri.cation tools often describe separation \nlogic as too hard to automate, but we think of that statement as only true in the context of normal SMT \nsolvers. A simple syntactic algorithm can be very effective at discharging separation logic implications. \nThe informal procedure we just demonstrated for swap scales up to much more interesting veri.cations, \nas we will demonstrate with our next example and in our further case studies. When the proof state is \nset up properly beforehand, our sep tactic reduces separation implications to facts about normal math\u00adematical \nobjects like numbers, sets, maps, and lists. These sim\u00adpler facts are usually straightforward to discharge \nwith traditional solvers, ideally those giving completeness guarantees. The key win is that the reduction \nremoves all need to reason about machine states or memories. We manage to handle that part of the reason\u00ading \nin a quanti.er-free way, avoiding one of the biggest headaches for SMT solvers. The syntactic simpli.cation \nprocedure is much simpler for a programmer to keep track of than the usual SMT\u00adbased alternatives, which \nhave to do with predicting the action of quanti.er-instantiation triggers or adding manual instantiation \nan\u00adnotations. At this point, the reader may be willing to believe that such a syntactic approach works \nfor trivial examples like swap, while maintaining skepticism that we can scale to more complex ex\u00adamples \nlike typical imperative data structures. Our next example shows how to accomplish that scaling, based \non computational ab\u00adstract predicates and modest use of unfolding hints that simplify uses of those predicates. \n 2.2 Incrementing All of a Linked List Figure 3 shows our next example, which walks a singly-linked list \nof words, incrementing the value of every word. A list node is a pair of adjacent bytes, the .rst storing \nthe data value and the second storing the next pointer, which is 0 in the .nal node of a list. The de.nition \nof the function linc introduces some structured programming constructs. We have a standard while loop, \nwhich, as usual, must be prefaced by a loop invariant (here placed inside square brackets). For this \nexample, the loop invariant is the same as the function precondition, so we assigned that shared condition \nthe name lincS. There are also a few Use statements here; we will explain shortly how they are used to \nguide automated proving. The speci.cation lincS follows the basic form of our previous example s precondition. \nThe variables a and b have been replaced Definition lincS : state -> PropX pc state := st ~> Ex fr, \nEx ls, ![ !{llist ls st#R0} * ![fr] ] st /\\ st#Rret @@ (st ~> ![ !{llist (map S ls) st#R0} * ![fr] ] \nst ). Definition linkedList := bmodule {{ bfunction \"linc\" [lincS] { [lincS] While (R0 != 0) { Use [llist_nonempty_fwd];; \nUse [llist_nonempty_bwd];; $[R0] <-$[R0] + 1;; R0 <-$[R0+1] };; Use [llist_empty_fwd];; Use [llist_empty_bwd];; \nGoto Rret } }}. Theorem linkedListOk : moduleOk linkedList. structured; sep. Qed. Figure 3. A Bedrock \nfunction to increment all of a linked list by ls, which is a normal, purely-functional Coq list. This \nlist serves as a functional model of the imperative list that is to be manipulated, and the action of \nthe linc function can be modeled with a purely functional reference implementation. In particular, the \npre-state contains the assertion !{llist ls st#R0}, where the notation !{ } denotes the use of an abstract \npredicate. In this case, we are requiring that a linked list is present in memory, rooted at st#R0 and \ncontaining data elements matching those in ls. We model the action of the function in our choice of alternate \narguments to llist in the post state. We write !{llist (map S ls) st#R0}, applying the usual higher-order \nfunction map to replace every element of ls with the result of applying S (the increment-by-one function) \nto it. We should emphasize that, while we try to aid intuition about the computational speci.cation approach \nby writing that it relies on purely functional reference implementations, we are not lit\u00aderally writing \na library in both functional and imperative versions and proving equivalence between them. Non-trivial \nfunctional pro\u00adgrams may still deserve quite non-trivial veri.cations of their own, as ample research \non the subject can attest to. Instead, we rely only locally on functional programs as alternatives to \ntraditional mathematical notation. For example, the speci.cation discussed in the previous paragraph \nis structured mostly as normal second-order logic, with one localized use of a functional program that \napplies map to express how the function should mutate a list. Contrast this with an approach that might \nbe used with SMT solvers, where a list is modeled as an array, and we might charac\u00adterize linc s behavior \nlike this in terms of pre-and post-versions a and a' of an array: ' .i. a[i]= a[i]+1 By using a quanti.er, \nwe forfeit completeness guarantees, as most .rst-order theories with quanti.ers are undecidable. With \nthe com\u00adputational approach in Coq, we usually avoid the need to consider decidability questions. We \nknow that any Coq term has a single well-de.ned answer which may be determined by normalization under \nappropriate conditions. As Coq executes terms automatically throughout its proof infrastructure, a well-chosen \ncomputational abstraction can give us many proof steps for free. Some veri.cation tools such as Jahob \n[32] allow the use of sets and set theory in speci.cations, which removes some need for quanti.ers. However, \nuniversally-quanti.ed invariants still appear in most data structure implementations. In all of our case \nstudies so far, there have been only four functions that we veri.ed with invariants that use quanti.ers \nin any way besides strings of exis\u00adtential quanti.ers at the beginnings of preconditions, as seen in \nour examples so far. All of our data structure invariants stick to this restricted use of quanti.ers, \ntoo. This pattern is much closer to the well-understood ghost variables of classical veri.cation than \nto the more involved use of quanti.ers that causes trouble for SMT solvers. Quanti.ers are hard to reason \nabout, and our encoding ap\u00adproach allows us to delegate almost all reasoning work to the simple syntactic \nprocedure that we began describing with the last example. In our proof of correctness for the linkedList \nmodule, the structured tactic will hand us three veri.cation conditions: we must show that the function \nprecondition implies the loop invariant (which is trivial for this example), that the loop body preserves \nthe loop invariant when we assume that the loop test succeeded, and that the loop invariant implies the \npostcondition when we assume that the loop test failed. We focus on the .rst of the non-trivial implications, \ndealing with the effect of going once through the loop. Skipping to the second stage of the procedure \nwe used for the last example, we have the following implication goal, which includes a pure fact implied \nby the success of the loop test. Since we are not returning from the function immediately, the existential \nvariables of the post\u00adstate are up to us to instantiate. In effect, the loop body contains an assignment \nfor each of these ghost variables, and we will choose the righthand side of each such assignment as we \ncomplete the proof. We will write the new variable values as ls and fr until we determine what we want \nthem to be; these are uni.cation variables, part of Coq s standard proof search support. Pure: st#R0 \n<> 0 Pred. 1: !{llist st#R0 ls} * ![fr] Changes: R0 <-st.[st#R0]+1 Pred. 2: !{llist (st.[st#R0+1]) ls \n} * ![fr ] We would like to simplify the memory projection from the changes list, but clearly our previous \nsimple procedure is inade\u00adquate. That procedure involves consulting the set of points-to facts in Predicate \n1, and we have no such facts here. However, given our knowledge of how linked lists are represented, \nwe know that the list must not be empty because R0 is nonzero, so Predicate 1 is equiv\u00adalent to some \npredicate with a points-to fact for R0. To make this more formal, we should start with the formal de.nition \nof the ab\u00adstract predicate llist. We use Coq s standard facilities for recur\u00adsive de.nitions and pattern-matching, \nalong with a new separation assertion notation [<p >], which lifts the pure predicate p into an impure \npredicate that asserts the truth of p and applies only to empty heaps. Fixpoint llist (ls : list nat) \n(hd : nat) : sprop := match ls with |nil=>[<hd=0 >] |x::ls => Exu,[< hd<>0>] * hd ==> x * (hd+1) ==> \nu * !{llist ls u} end. This de.nition says: An empty list is represented by an empty memory and a head \npointer with value 0. A nonempty list is rep\u00adresented based on a local existentially-quanti.ed ghost \nvariable u, standing for the next pointer. We assert that the head pointer is nonzero, that it points \nto the head x of ls, that the next memory cell contains u, and that u is the root of a linked list representing \nthe tail ls of ls. From this de.nition, it is clear intuitively that we can materi\u00adalize a points-to \nfact for the beginning of a nonempty list. The following unfolding lemma makes that fact explicit, in \nterms of a separation logic implication operator ===>, which uses three = characters, in contrast to \nthe points-to operator which uses two. Theorem llist_nonempty_fwd : forall ls hd, hd <> 0 -> llist ls \nhd ===> Ex x, Ex ls , Ex u, [<ls=x:: ls >]* hd==>x *(hd+1) ==> u * !{llist ls u}. destruct ls; sepLemma. \nQed. The theorem is proved trivially, using destruct to ask for a case analysis on the list ls, and then \ncalling a variant of our sepa\u00adration logic simpli.er to do the rest of the work. We are now ready to \nlearn the purpose of the Use statements in the program code. The .rst such statement references our new \nunfolding lemma. By including this annotation statement, we are asking the separation simpli.er to use \nour new lemma to replace an instance of the ===> lefthand side with the corresponding righthand side. \nCoq uses uni.\u00adcation to discover the values of the lemma variables ls and hd, and Coq uses its extensible \nproof hint mechanism to discharge any extra hypotheses. In this example, that hypothesis is hd<> 0, which \nis discharged using the pure fact from the proof state. Coq introduces new variables to stand for the \nvalues x, ls , and u that we assert to exist. This brings our state to: Pure: st#R0 <> 0 /\\ ls = x :: \nls Pred. 1: st#R0 ==> x * (st#R0+1) ==> u * !{llist ls u} * ![fr] Changes: R0 <-st.[st#R0]+1 Pred. \n2: !{llist ls (st.[st#R0+1])} * ![fr ] Now that a points-to fact for R0 is exposed, we can simplify our \nexplicit memory accesses. Pure: st#R0 <> 0 /\\ ls = x :: ls Pred. 1: st#R0 ==> x * (st#R0+1) ==> u * \n!{llist ls u} * ![fr] Changes: R0 <-x+1 Pred. 2: !{llist ls u} * ![fr ] It is now easy to run symbolic \nevaluation of the memory changes in Predicate 1. Pure: st#R0 <> 0 /\\ ls = x :: ls Pred. 1: st#R0 ==> \nx+1 * (st#R0+1) ==> u * !{llist ls u} * ![fr] Pred. 2: !{llist ls u} * ![fr ] At this point, cancellation \nwill allow us to .nish this case of the proof, in a subtle method of automatic application of a frame \nrule. The two abstract predicates may be canceled if we set uni.cation variable ls to ls . That leaves \nthe frame condition uni.cation variable fr for us to determine. By setting it equal to all of Predicate \n1 that remains, we can .nish the case by re.exivity. The part of Predicate 1 that has been absorbed into \nfr constitutes the parameter R to the standard frame rule, and it has been determined using a very generic \nuni.cation technique that applies just as well to non-standard control structures that need not follow \na usual function call convention. The loop invariant is a conjunction, and we have .nished with the .rst \nconjunct. The second conjunct is an assertion about the conditions under which it is safe to jump to \nthe function return pointer. In particular, we need to show: Pred. 1: st#Rret @@ (st ~> ![ !{llist (map \nS ls) st#R0} * ![fr] ] st ) Pred. 2: st#Rret @@ (st ~> ![ !{llist (map S ls ) st#R0} * ![fr ] ] st ) \n It turns out that one @@ assertion about a code pointer implies another about the same pointer if their \npredicate operands imply each other in the reverse order. We apply that rule and also substi\u00adtute the \nvalues of ls and fr that we learned in proving the last conjunct. Pred. 1: !{llist (map S ls ) u} * st#R0 \n==> x+1 * (st#R0+1) ==> u * ![fr] Pred. 2: !{llist (map S (x :: ls )) st#R0} * ![fr] Coq automatically \nuses computation to replace mapS(x :: ls ) by S x :: map S ls . To make further progress, we would like \nto unfold the abstract predicate in Predicate 2, similarly to the way we did for Predicate 1 in an earlier \nstep. It is easy to prove another lemma that is like llist nonempty fwd but runs the implication in the \nother direction. This is exactly the lemma that we suggest with the second Use statement within the loop \nbody. Based on that hint, our sep tactic will .rst introduce new uni.cation variables x , ls , and u \n, standing for the existential quanti.ers in the theorem statement. Then, the pure goals st#R0 <> 0 and \nmap S (x :: ls ) = x :: ls are queued for solving by normal mathematical means. By stating the .rst \nof these facts outside of the separation implication in the theorem statement, we ask that it be proved \nbefore proceeding, and this is easily done, based on the .rst pure hypothesis in the proof state. The \nsecond pure fact is queued to be reconsidered after we .nish the impure part of the proof. Hopefully \nthe uni.cation variables will have been determined by then, simplifying the job of our pure solvers, \nmany of which are unable to handle uni.cation variables. Before we can get there, we must .nish with \nthis new modi.ed proof state based on the lemma statement: Pred. 1: !{llist (map S ls ) u} * st#R0 ==> \nx+1 * (st#R0+1) ==> u * ![fr] Pred. 2: st#R0 ==> x * (st#R0+1) ==> u * !{llist ls u } * ![fr] The canceler \n.nishes this proof by unifying x with x+1, u with u, and ls with map S ls . Notice that this is very \nsim\u00adple, eager syntactic uni.cation that occurs as we try to cross pred\u00adicates off from both sides of \nthe implication. We need none of the complexity of E-graph matching as pioneered in solvers like Sim\u00adplify \n[10]. The particular uni.cation that we discover leaves the earlier queued fact map S (x :: ls ) = x \n:: ls solv\u00adable trivially by computational normalization, and we have .nished proving the correctness \nof this veri.cation condition. The .nal veri.cation condition has a similar but simpler proof, based \non analogous unfolding lemmas for empty lists that are in\u00advoked in the last two Use statements of the \nprogram. Each of our unfolding lemmas has a trivial one-line proof. The Use statement also supports partial \ninstantiation of a lemma s quanti.ers, when uni.cation is not suf.cient to discover instantiations correctly; \nand there is a further form to allow references to the current machine state in computing the instantiations. \nWe .nd this kind of quanti\u00ad.er instantiation to be easier to keep track of than in the case of universally-quanti.ed \nprogram invariants, as the quanti.er reason\u00ading may be kept local and is also completed solely by uni.cation \nin a majority of cases. 3. A Simpli.cation Procedure for Separation Assertions Our walk-throughs of examples \nhave demonstrated the basic .ve\u00adstep procedure that we use in discharging all Bedrock veri.cation conditions. \nThe procedure is easily formalized as an algorithm, broken into discrete steps that are reasonably easy \nfor humans to keep track of. Here is the procedure, including three extra steps that are not speci.c \nto separation logic. 1. Standard .rst-order logic simpli.cation: Before beginning, it is useful to put \nstandard Coq simpli.cation to work. For in\u00adstance, our last example involved proving an implication be\u00adtween \ntwo conjunctions. This can be reduced to separate proofs of the two conclusion conjuncts, in each case \nassuming the truth of both hypothesis conjuncts. This stage also involves using each hypothesis disjunction \nto split the proof into two cases, re\u00adplacing each existentially-quanti.ed hypothesis with a version \nof its body that refers to a freshly-introduced variable, and so on. 2. Forward unfolding of abstract \npredicates: We apply the hints suggested with Use statements, along with some hints that have been registered \nglobally, so that they should be used wherever they apply. The separation logic hypothesis ( Predicate \n1 in our examples) is easily normalized to an iterated separating conjunction of impure facts. We walk \nthrough the conjuncts, looking for a hint proving an implication whose lefthand side uni.es with the \ncurrent conjunct. When we .nd a match, we .rst check that the premises of the lemma can be proved im\u00admediately, \nby calling all solvers that have been registered as hints. If this process succeeds, we introduce new \nvariables for any existential quanti.ers in the implication conclusion, new pure hypotheses for any pure \nconclusions, and replace the orig\u00adinal conjunct with the new impure conclusions from the lemma. This \nprocess is iterated until no more hints match. 3. Simpli.cation of memory accesses: For every points-to \nfact u ==> v in the separation hypothesis (referring to state st), we replace every occurrence of the \nmemory access st.[u] with v, anywhere the former appears in the proof state. 4. Symbolic execution of \nmemory writes: For every write of value v to address u implied by the current basic block s straightline \ninstructions, .nd a fact u ==> v in the separa\u00adtion hypothesis and replace v with v. 5. Backward unfolding \nof abstract predicates: This phase is very similar to the forward unfolding phase, but with consid\u00aderation \nof hints that apply to the separation conclusion ( Predi\u00adcate 2 in our examples). We replace the conclusion \nof a lemma with its premises. New existential variables are instantiated as uni.cation variables, whose \nvalues should be determined later through syntactic uni.cation. Rather than adding pure premises to our \nproof state, we queue them as proof obligations to return to later. These obligations often contain some \nof our new uni.\u00adcation variables. With proper foresight in Use annotations, the cancellation step will \ndetermine the values of almost all of these uni.cation variables, so that, when we return to the obligations, \nthey are in forms not so different from what SMT solvers are designed to handle. 6. More aggressive \n.rst-order logic simpli.cation: Here we re\u00adpeat all the simpli.cations of the .rst step, with one addition: \nWhen the goal begins with an existential quanti.er, we gener\u00adate a fresh uni.cation variable, substitute \nit for the quanti.er s variable, and make this substitution result the new goal. This quickly reduces \nour goal to a set of goals, some using separa\u00adtion logic and some using more standard mathematical theories. \nWe will try to .nish with the separation goals before proceed\u00ading, in hopes of determining most uni.cation \nvariable values. 7. Cancellation: We iterate through all conjuncts in the separation hypothesis, trying \nto .nd a uni.able conjunct in the conclusion.  Every matching pair is eliminated. By re.exivity of implication, \nwe are .nished when the full hypothesis and conclusion may be uni.ed. 8. Proof of remaining pure facts: \nIf cancellation worked prop\u00aderly, we should now be faced only with a collection of pure goals that deal \nonly with standard mathematical theories. These can be discharged by various solvers that are registered \nas hints. For instance, we have used congruence closure, a solver for linear arithmetic, brute force \nsearch via Prolog-style logic pro\u00adgramming, and other techniques. We also allow the programmer to register \nrewrite rules, de\u00adsigned to simplify proof states by simple syntactic replacement of one pattern by another. \nWe apply these rules wherever possible, be\u00adtween every pair of steps above. The whole process is implemented \nin Coq s Ltac language [9], which is a Turing-complete domain-speci.c language for proof search. By construction, \nevery Ltac program generates a proof term to explain why it concluded that a fact is true. These proof \nterms use a relatively simple, generic logical language that relies on a small number of axioms. Thus, \nit is possible to implement a small, trust\u00adworthy standalone checker for these proof terms. To trust \na proof, one need only trust the checker, not the more heuristic process by which proof terms are found. \nTherefore, since we implement our procedure as the Ltac program sep, we arrive at correctness by construction: \nwhenever the procedure succeeds, it has made only valid deductions. The sep procedure is the workhorse \nfor Bedrock veri.cation, but it is important that programmers can build on it with further problem-speci.c \nLtac code. For instance, most reasoning about code pointers must be done through Ltac, rather than through \ntra\u00additional solvers, which are unable to cope with the second-order quanti.cation that is usually involved. \nEach variety of control trans\u00adfer generally demands its own modest piece of Ltac code. For in\u00adstance, \nour last example program involved reasoning about the way the @@ operator is used to model return pointers. \nOur case studies use about 10 lines of Ltac code to guide the automatic application of the appropriate \nreasoning principle. Other constructs like computed function calls, exception handling, and so on will \ngenerally require similar up-front investments. Once this support code is written, no extra work is required \nin the veri.cation of individual programs. Though sep is designed to automate most of the proof process, \nwe often break it into its individual steps while verifying a program, so that we can watch to make sure \nthat each step goes the way we expect. Compared to SMT solvers and related techniques, we think it is \nan advantage of our high-level approach that humans can keep track of the steps of the algorithm and \nglean useful information from watching its progress. That is, at each stage of the proof, the human user \nis shown intermediate states that look much like those given for the examples in Section 2. Even the \nfull sep tactic is not an all-or-nothing solver; when it fails to prove a goal, it returns the unproved \nobligations in the same human-understandable format, with the possibility for exploratory proving to \ndetermine what went wrong. For each domain of uses of code pointers, we usually write a single automation \ntactic, but we also break the proof process into a series of calls to simpler named tactics. On approaching \na new veri.cation condition, we step through these sub-stages to get a sense for which Use annotations \nand hints will be needed. Any step may be traced in more detail, using Coq s usual proof debugging commands. \nWhen the user .gures out that a particular Use annotation is useful, a tactic may be called to add it \nfrom within the exploratory proving process; these hints can later be migrated into the program source, \nonce all of the cases work. At that point, we switch to the mostly-automated approach. We want to stress \nthat these lower-level tactics are still more like states in a small, .xed .nite state machine than like \nthe more common, very manual Coq proof scripts. Also, in switching to mostly-automated proofs, we often \n.nd that our proofs continue working even after making modest changes to a program s speci.cation. The \nhuman prover need only .nd the key reasons why a program is correct, and then the automation can take \ncare of the details. 4. Evaluation The Bedrock framework implementation consists of about 5000 lines \nof Coq code. We have also implemented a suite of case study programs, chosen to exercise both traditional \n.rst-order reasoning and the more unusual higher-order reasoning about code pointers. None of the case \nstudies require manual proof about speci.c pro\u00adgram states, despite the fact that we are always proving \nfull func\u00adtional correctness of our libraries, not just more shallow properties like memory safety. Figure \n4 gives some statistics about the case study programs. We explain the column meanings in left-to-right \norder: First, we give each module s total lines-of-code count (including both implementation and proof), \nfollowed by its number of structured assembly functions and the number of lines of code devoted to these \nfunctions, minus proof-related annotations. After that, we have, respectively, lines-of-code counts for \ndata structure representation invariants, pure lemmas (which do not deal with memories or machine states), \nimpure lemmas (which are phrased as implications in separation logic), invariants included in programs \n(e.g., loop invariants), Use annotations, proof hints (which guide automatic solving of pure goals), \nand the main correctness proof scripts for modules. In the cases of impure and pure lemmas, we differentiate \nbetween the lines devoted to theorem statements and the lines devoted to their proofs (with the latter \ninside parentheses). The last column of the table shows the time needed to compile each module, proofs \nincluded, on a 3.16 GHz Intel Core 2 Duo processor running Linux. Among the examples with the most code, \ntwo take about a half and a quarter hour, with the rest .nishing in 6 minutes or less. To suggest a fair \ncomparison with other ver\u00adi.cation tools, we note that our case studies spend over half their time in \ngeneration and checking of proof terms, which is a kind of bonus on which most other tools spend no time. \nBedrock is also implemented on top of the normal Coq engine, which uses naive interpretation to execute \nproof search programs; we would expect to see a constant-factor speedup of at least several times with \na cus\u00adtom implementation in a general-purpose language. Ideally, a fu\u00adture Coq version will include an \nimproved optimizing engine that brings these bene.ts to our existing code. Even as the framework stands \ntoday, we .nd that the programmer experience is quite rea\u00adsonable, with just a few seconds wait time \nrequired when focusing on and stepping through the proof of a single veri.cation condi\u00adtion. As is usual, \nthe different conditions may be tackled in paral\u00adlel; though Coq does not support this kind of parallelism \nyet, we expect that simple optimizations in Coq to enable multi-core exe\u00adcution would also bring large \nspeed-ups. To give a more qualitative account of the veri.cation experi\u00adence, we turn to descriptions \nof the case studies. First, we have the Malloc module, a heap memory manage\u00adment library that all of \nthe other modules rely on. Its interface is independent of the implementation strategy; for now, we are \nusing a simple unsorted free list with no coalescing. This data structure is an interesting take on the \nfamiliar linked list: we have variable\u00adsized free list nodes that must store both their own sizes and \ntheir next pointers, rather than uniformly-sized list nodes that point to data allocated elsewhere. Verifying \nMalloc requires proving im\u00adpure lemmas covering maneuvers like splitting a free list block into one piece \nto return to client code and another to keep in the list, based on the size of the memory chunk requested \nby the client. The remaining case studies fall into two categories. First, we compare against recent \ndevelopments in data structure veri.cation by implementing our own versions of .ve case studies used \nwith the Jahob tool [32]. Second, we demonstrate Bedrock s support for reasoning about code pointers, \nvia examples mostly inspired by past work with the XCAP logic [24]. 4.1 Data Structures Our data structure \nexamples can be split into three groups, based on the mathematical domain used to model the data structures. \nEach of our implementations provides the same functionality as the public methods in a particular example \nfrom a recent paper on Jahob [32]. We prove full functional correctness of each method, showing that \nthe imperative code realizes the same behavior as with the obvious implementation of each operation via \na small functional program. Our job is inherently harder than in Java-speci.c Jahob, since every module \nis at the assembly level of abstraction and must do explicit management of heap and stack memory, but \nour results show that the effective burden is not so great. First, we have arrays, modeled using functional \nlists. One mod\u00adule develops the program-independent aspects of this theory, in\u00adcluding impure lemmas \ncovering different ways of isolating cells within arrays. For instance, here is a theorem for unfolding \nan ar\u00adray in a way that exposes a points-to fact for the cell at index n. Theorem array_mid_fwd : forall \nn a ls, n < length ls -> array ls a ===> !{array (firstn n ls) a} * (a+n) ==> nth n ls 0 * !{array (skipn \n(n+1) ls) (a+n+1)}.  Notice the key use of the computational approach we have men\u00adtioned a few times \nalready. Usual mostly-automated program veri\u00ad.cation deals with array cell isolation mostly through universally\u00adquanti.ed \ninvariants. Instead, we take advantage of the rich possi\u00adbilities for computation with lists in Coq, \ndescribing a three-way split of an array in terms of recursive functions for keeping only the .rst n \nelements of a list, extracting the nth element, or drop\u00adping the .rst n elements, respectively. A few \nrewriting hints about the interactions of these recursive functions enable very effective proof automation \nabout array operations, after we add a few Use statements that suggest lemmas like array mid fwd. We, \nin ef\u00adfect, reduce quanti.er-heavy reasoning about arrays to quanti.er\u00adfree reasoning about functional \nlists. The ArrayList module ap\u00adplies this approach in the implementation of 19 common operations on an \nabstract datatype of growable arrays. As, compared to linked data structures, arrays offer relatively \nlittle opportunity for simple computational abstraction, our Bedrock ArrayList involves about 40% more \nannotation than the Jahob version. Next, we have a theory of sets, modeled as Coq values of type nat \n-> bool. That is, a set is a mathematical function from ma\u00adchine words (natural numbers) to booleans, \ntelling us whether each word belongs to the set. We also de.ne a common interface for assembly implementations \nof imperative .nite sets, providing op\u00aderations for membership checking and addition and removal from \na set. We give two implementations of this interface, with unsorted singly-linked lists and with binary \nsearch trees. The latter module additionally provides functions for extracting the minimum or max\u00adimum \nelement from a nonempty set. The computational approach helps us give dramatically simpler binary search \ntree invariants than in the Jahob code, where even the basic data structure invariants include 10 universal \nquanti.ers. We rely instead on the ability to .lter sets by computable predicates; for instance, in considering \nan internal node of the tree, to come up with the set that its left child Module Total #Funcs. Impl. \nData Pure (pfs) Imp. (pfs) Inv. Use Hints Main Build (min) Malloc 267 3 71 8 3(1) 50(14) 16 24 2 1 5 \nTheory of arrays/lists 111 - - 5 2(2) 35(32) - - 7 - <1 ArrayList 771 19 272 3 79(37) 29(15) 168 83 20 \n1 35 Theory of sets 159 - - - 61(43) - 15 - 7 - <1 SinglyLinkedList 157 4 54 6 - 14(6) 20 14 2 6 2 BinarySearchTree \n355 6 127 10 - 32(7) 43 40 2 28 13 Theory of maps 69 - - - 7(23) - 20 - 1 - <1 AssociationList 200 6 \n85 6 - 12(6) 28 27 2 1 3 Hashtable 374 6 90 6 36(13) 38(13) 56 39 18 1 4 Memoize 191 2 45 7 21(6) 6(3) \n36 5 5 18 2 AppendCPS 155 2 56 5 - 9(3) 44 12 - 1 4 ThreadLib 225 4 67 9 6(2) 22(7) 34 20 1 7 6 Figure \n4. Statistics on case studies, mostly in terms of number of lines of code of each kind represents, we \n.lter the original set by a function that keeps only those elements less than the current data value. \nFinally, we de.ne a similar theory of maps, modeled with the Coq type nat -> option nat, where an option \nnat is either None or Some n, for number n. Our two .nite map implementa\u00adtions use unsorted singly-linked \nlists and hash tables. Unlike its Ja\u00adhob counterpart, our hash table implementation uses the linked list \nimplementation, treated abstractly through its interface, within hash buckets. The computational approach \nagain brings some signi.cant simpli.cations for hash tables. A crucial function is only , which restricts \na map to just those keys that hash to a particular value, implemented using the natural number equality \ntest eq nat dec. Definition only (m : map) (hmax n : nat) : map := fun k => if eq_nat_dec (hash hmax \nk) n then m k else None. We use only to state impure lemmas like the following, which isolates a particular \nbucket within a hash table. Theorem htableOk_mid_fwd : forall n m hmax curHash len a, n < len -> htableOk \nm hmax curHash len a ===> Ex a , !{htableOk m hmax curHash n a} * (a+n) ==> a * !{alist (only m hmax \n(curHash+n)) a } * !{htableOk m hmax (curHash+n+1) (len-n-1) (a+n+1)}.  Without focusing on the details, \nwe point out that a single case of the hash table representation predicate htableOk is expanded to two \nother uses that together skip one bucket, where the missing bucket is described in terms of a points-to \nfact to a pointer a . This pointer itself represents an association list, which we express using the \nabstract predicate alist exported by the AssociationList module. We say that the association list represents \nthe restriction of the overall map to just those keys that belong in the bucket we have isolated. Summing \nup all of the different kinds of proof and annotation from Figure 4, we arrive at a total of about 200 \nfor Hashtable. The Jahob version uses over 250 lines of annotation just within the methods used to implement \nthe remove-from-map operation. The bulk of these annotations are qualitatively different from those required \nin Bedrock, as they involve explicitly nested proofs based on the quanti.er structure of invariants. \nOur removal function is self-contained, including one 5-line invariant and 5 Use statements. The remaining \nlines are normal executable code. Our data structure case studies deal only with keys and values that \nare uninterpreted machine words. However, since we are work\u00ading with low-level programs, machine words \nare suf.cient as a rep\u00adresentation of arbitrary data structures. These words may be treated as pointers \nby client code, and we may write invariants about the layout of such nested data structures solely in \nterms of the func\u00adtional model of a set or map, avoiding any coupling to the details of the data structure \nimplementation. It is still true that our implemen\u00adtations of sets and maps may not have the intended \nsemantics when used to store nested data structures via pointers, since our .xed comparison and hashing \noperations compare pointers and ignore further structure. Variations on the case studies from this section \nshould support the use of user-provided comparison and hashing functions, when accompanied by proofs \nof key facts about them (e.g., a comparator implements a total order in terms of some func\u00adtional model \nof the nested data structure).  4.2 Code Pointers as Data Our remaining case studies go beyond the domain \nof most previous mostly-automated veri.cation tools, where most assertions about callable code pointers \nmay not even be expressed, let alone veri.ed. Here we are still proving full functional correctness, \nthough it is often less obvious what precisely that means for libraries that work with code pointers. \nWe choose our speci.cations to formalize the intuitive notion of the abstraction that the library is \nmeant to provide. For instance, our threads library will include a yield function whose speci.cation \nguarantees that, when the yielding thread is rescheduled, its private memory has not been changed. Our \n.rst example is a generic function memoizer, backed by hash tables, which are themselves backed by association \nlists, cre\u00adating a non-trivial tower of uses of abstract datatypes. Any function may be memoized into \nvalues of a particular abstract datatype of memo tables. Each function is associated with a pure speci.cation, \na mathematical relation between input and output words. Addition\u00adally, a memoized function may use local \nstate, represented by a completely unconstrained separation logic predicate, which might stand for, e.g., \na local cache data structure. Memo tables use hash tables internally, along with an extra invariant that \nrefers only to the functional map that models the hash table contents. We require that every mapping \nis consistent with the pure speci.cation. This allows us to expose the memo table as a function with \nalmost the same speci.cation as the original. (We must expand the speci.ca\u00adtion to require that the memo \ntable is loaded in memory before the call and guarantee it is loaded afterward.) Our .nal two case studies \nare based on examples from papers about the XCAP program logic. First, we reimplemented and rever\u00adi.ed \nthe main example from the original XCAP paper [24]. This is an implementation of in-place concatenation \nof two linked lists, written in a form that might be output by an ML compiler. Rather than following \na C-style calling convention, the append func\u00adtion s return pointer is represented as an explicit closure, \nwhich pairs a function pointer with an environment argument that the function expects to be passed when \ncalled. The old version of this example was coded in an assembly language even more idealized than the \none we use in this paper, where malloc and free are in\u00adstructions, rather than library functions that \nmust be implemented. Nonetheless, despite this disadvantage, our Bedrock version is sub\u00adstantially simpler. \nThe XCAP version involves about 1500 lines of very manual proof. Our new version relies on just three \nof the four unfolding lemmas used for lists in Section 2.2, 12 Use state\u00adments, and a one-line main proof. \nOur invariants correspond to code that already appeared explicitly as basic block preconditions in the \nXCAP version, so it seems fair to say that we have reduced the proof burden by two orders of magnitude. \n We also veri.ed a cooperative threads library, similar to one im\u00adplemented in XCAP [25, 26]. That is, \nthe library relies on threads to yield to other threads, in place of the more common interrupt\u00adbased \napproach of preemptive threading. Here, the setting of past work is more challenging, as they veri.ed \nreal x86 code, but the basic approach is similar. We implement a fair round-robin thread scheduler, supporting \ndynamic thread creation, yielding, and exit\u00ading. There is a shared global invariant, characterizing the \npart of memory that threads may use to communicate with each other. Each blocked thread also has its \nown local invariant, which is in\u00addexed by the thread s saved stack pointer. Here is an example spec\u00adi.cation \nthat uses the abstract data type of thread schedulers; in particular, this is the precondition for the \nyield function. Definition yieldS := st ~> Ex fr, Ex ginv, Ex invs, Ex root, susp ginv (fun sp => sep \n([< sp = st#Rsp >] * ![fr])%Sep) st#Rret /\\ codesOk ginv invs /\\ ![ !{mallocHeap 0} * st#Rsp ==> root \n * !{threads invs root} * ![ginv] * ![fr] ] st. We see explicit quanti.cation over a global invariant \nginv. The susp predicate characterizes when a code pointer (the return pointer, in this invariant) is \nsafe to suspend, in terms of the global and local invariants. The local invariant we choose here combines \nthe frame condition with the requirement that the stack pointer upon resumption equals the original. \nThe codesOk invariant as\u00adserts similar properties for all of the suspended threads, as repre\u00adsented by \nthe functional model invs. Both susp and codesOk are implemented using second-order quanti.cation. The \nprecondition ends with a more normal separation logic formula, requiring a valid malloc heap, a pointer \nto the scheduler data structure in the .rst lo\u00adcal stack slot (found at an address based on stack pointer \nRsp), the scheduler data structure itself, the global invariant, and the frame condition. The old XCAP \nimplementation used about 8000 lines of code just for the proofs of correctness for three basic blocks \nthat imple\u00adment key operations on saved thread contexts [26]. A comparable amount of additional complexity \nwas associated with the thread scheduler and its data structure. The complete Bedrock implemen\u00adtation, \nincluding assembly code and veri.cation, totals just 250 lines. We again see a reduction of about two \norders of magnitude in the amount of program-state-speci.c proof. Of course, counting lines of annotation \nand proof gives only an approximation of the human cost of verifying a program. We would rather know \nthe time spent by a programmer in coming up with these lines, since some shorter proofs may be harder \nto .nd in practice. Unfortunately, we have not found much data of this kind in the literature. We can \ngive one piece of anecdotal evidence for the effectiveness of the Bedrock approach: The authors of the \nXCAP cooperative thread scheduling library mention that they spent six person-months completing it [25]. \nThis .gure includes time spent building and tweaking the veri.cation framework, but we hope it still \ngives a good sense of the scale of this sort of effort. In contrast, starting from no prior code dealing \nwith threads, we coded and veri.ed our threads library in the course of two days of work. 5. Related \nWork We have already discussed the CAP project [15] extensively. That line of work has been very successful \nat designing program log\u00adics suitable for foundational veri.cation of low-level code, lead\u00ading to successful \ncase studies in dynamic thread creation [11], em\u00adbedded code pointers [24], garbage collection [21], \nself-modifying code [3], and hardware interrupts [12]. The Bedrock project takes these results as its \nbase and adds infrastructure for higher-level coding and automated veri.cation, demonstrating that foundational \nguarantees can be had, for code that manipulates code pointers as data, without giving up the signature \nbene.ts of classical veri.ca\u00adtion tools. Classical veri.cation tools like ESC [14], Boogie [1], and Ja\u00adhob \n[31, 32] have been shown to be effective in a variety of veri.\u00adcation tasks that can be completed using \n.rst-order solvers (or addi\u00adtionally, in the case of Jahob, using an integrated proof language for a \nhigher-order logic). Hawblitzel and collaborators have done auto\u00admated classical veri.cation of garbage \ncollectors [16] and the core of an operating system kernel [30] with Boogie. Bedrock aims to take this \nstyle of highly productive veri.cation and add three main bene.ts. First, we apply the computational \napproach to speci.ca\u00adtion to reduce the need for logical quanti.ers and simplify proofs. Second, we support \ncallable code pointers with polymorphic spec\u00adi.cations. Third, we reduce the trusted code base to a foundational \nlevel, where only the standard Coq proof checker must be trusted to believe a veri.cation. Foundational \nveri.cation also opens the door for integration with proofs that would be hard to write in a classical \nveri.er but are tractable in general-purpose proof assistants. For ex\u00adample, the Verve project [30] aims \nto integrate a soundness proof for a typed assembly language; with such a proof and a Bedrock veri.cation \nboth carried out in Coq, the combined system can have a simple foundational correctness theorem. Separation \nlogic has been applied successfully in program anal\u00adysis tools, including Smallfoot [2]. Follow-on work \non abductive inference [4] infers procedure speci.cations that can be used to prove memory safety, without \nrequiring whole-program analysis. Alternative shape analysis techniques have been applied automati\u00adcally \nin tools like TVLA [29] and XISA [5]. Compared to Bedrock, these tools have applied to languages at higher \nlevels of abstraction than assembly, they have not been veri.ed formally, and they do not support reasoning \nabout code pointers. All of these features con\u00adtribute to some desirable properties of these other tools: \nsuperior performance and greater applicability to mainstream programming languages. McCreight s Coq tactics \nfor separation logic [20] address simi\u00adlar concerns to the tactic support presented in this paper, raising \nthe level of abstraction in proof about imperative low-level programs. This alternate veri.cation framework \nsatis.es all of our desiderata from the introduction, with the exception that it is not mostly\u00adautomated. \nProofs using McCreight s tactics still involve a signi.\u00adcant number of manual proof steps, applying such \noperations as ex\u00adplicit rearrangement of separating conjunctions using associativity and commutativity. \nFor example, a useful comparison comes from a simple in-place linked list reversal function implemented \nwith both systems. McCreight s proof includes about 80 atomic tactic calls, while the Bedrock proof includes \nabout 10 atomic tactic calls and 4 Use statements. About half of McCreight s atomic tactic calls men\u00adtion \nvariable or hypothesis names that are bound within the proof; none of the tactics in the Bedrock proof \ndo. The computational approach to data structure speci.cation has been adopted by many projects working \nwithin higher-order logics. Mehta and Nipkow [22] used computational speci.cation without separation \nlogic, and the combination with separation logic has ap\u00adpeared in work on XCAP [24], Ynot [23], CFML \n[6], and Veri-Fast [17]. The Ynot library [7] for Coq supports automated veri.cation of monadically impure \nHaskell-like programs annotated with speci.\u00adcations, using separation logic. Every Coq program is also \na valid Ynot program, so implicit memory management is baked into the system. Bedrock is designed to \napply to languages that are more realistic for low-level infrastructure. While the Ynot language and \nits veri.cation rules are speci.ed with Coq axioms, Bedrock sup\u00adports languages de.ned foundationally \nwith operational semantics. Bedrock has a few more high-level advantages over Ynot: it ap\u00adplies frame \nrules automatically, supports the inlining of quanti.er instantiation hints (Use statements) in programs, \nand deals with the compilation of more convenient, higher-level code into lower-level formats. The VeriFast \n[17] veri.er has been under development in paral\u00adlel with Ynot and Bedrock and involves many similar \ndesign deci\u00adsions. Program veri.cation is mostly-automatic, based on computa\u00adtional separation logic \nspeci.cations with support for higher-order reasoning. VeriFast is a standalone tool, with no formal \nproof of correctness, which enables better performance and easier integra\u00adtion of convenience features \nfor programmers. VeriFast also targets C and Java rather than assembly-level programs. Lack of integra\u00adtion \nwith a traditional proof assistant may make it harder to carry out some veri.cations that rely on reasoning \noutside the domain of the standard automation. The Bedrock approach usually reduces goal facts about \nprogram behavior into sets of quanti.er-free facts about normal mathemati\u00adcal objects like numbers, sets, \nmaps, lists, and trees. So far, we have proved many of these facts with some amount of manual proof, \nsince this total burden is not signi.cant compared to the costs of reasoning concretely about data structures \nand pointer aliasing. We might also try taking advantage of recent progress on complete de\u00adcision procedures \n[19] for very rich theories that encompass much of our pure proof obligations. Any such decision procedure \nshould be relatively straightforward to integrate with Coq, if it can be made to generate proof witnesses. \n6. Conclusion We have introduced Bedrock, a framework for implementation and veri.cation of low-level \nprograms in Coq. Bedrock s key produc\u00adtivity features center on automatic proof of invariants expressed \nwith the concepts of separation logic. Unlike other systems pro\u00adviding similar levels of automation, \nBedrock supports reasoning about .rst-class code pointers in terms of second-order variables that stand \nfor unknown invariants. This facility is critical for han\u00addling of runtime and operating systems. To \nthe best of our knowl\u00adedge, ours is the .rst veri.ed thread library with less than a few thousand lines \nof proof. Our implementation .ts well within that mark, with a total of 250 lines, including program \ncode and proofs. The framework is parametrized over machine languages and their operational semantics. \nTo date, we have only experimented with Bedrock instantiated to an idealized language with in.nite\u00adwidth \nwords. We expect that the approach will continue to work well for the machine languages of today s common \nmicroarchitec\u00adtures, though future work is needed to be sure. Effective decision procedures for bitvector \narithmetic are likely to be the main new requirement. Bedrock follows a very computational approach \nto separation logic, where most imperative functions are speci.ed in terms of purely-functional reference \nimplementations. This allows us to replace much formal reasoning with execution of programs in the speci.cation \nlanguage. Quanti.ed invariants are one of the greatest challenges for automated veri.cation, and the \ncomputational ap\u00adproach allows us to replace almost all uses of quanti.ers with calls to recursive functions \nin a functional language. Most program veri.cation today is done by direct reduction to domains with \neffective decision procedures. Even complete pro\u00adcedures for complex theories like (restricted forms \nof) transitive closure can have poor enough performance that the human veri.er must do extra work to \nguide the procedures. With Bedrock, we fol\u00adlow an alternate approach, where the heart of veri.cation \ncondition proving is a symbolic procedure for separation logic simpli.cation. This procedure has a few \nsimple steps that humans can follow with\u00adout much trouble, and our Coq library provides good support \nfor stepping through the procedure and animating the moves it makes. Understanding this process requires \nno reasoning about semantic theories, just about the syntactic form of separation assertions. When a \nprogram is annotated with some relatively simple in\u00advariants and invocations of simpli.cation lemmas \nfor abstract pred\u00adicates, the simpli.cation procedure reduces program correctness to the truth of quanti.er-free \nfacts that refer only to very basic math\u00adematical theories. These facts are much easier to reason about \nthan the original program, and we can hope that most will eventually be automatable with a few decision \nprocedures. Thus, Bedrock helps programmers do the hard parts of veri.cation with the familiar activity \nof programming and solve the remaining problems with traditional mathematical methods. Acknowledgments \nWe would like to thank Gregory Malecha and Ryan Wisnesky for helpful comments on drafts of this paper. \nThis work was funded within the DHOSA project through AFOSR MURI grant FA9550\u00ad09-01-0539. References \n[1] Mike Barnett, Bor-Yuh Evan Chang, Robert Deline, Bart Jacobs, and K. Rustan M. Leino. Boogie: A modular \nreusable veri.er for object\u00adoriented programs. In Proc. FMCO, 2006. [2] Josh Berdine, Cristiano Calcagno, \nand Peter W. O Hearn. Smallfoot: Modular automatic assertion checking with separation logic. In Proc. \nFMCO, 2005. [3] Hongxu Cai, Zhong Shao, and Alexander Vaynberg. Certi.ed self\u00admodifying code. In Proc. \nPLDI, 2007. [4] Cristiano Calcagno, Dino Distefano, Peter O Hearn, and Hongseok Yang. Compositional shape \nanalysis by means of bi-abduction. In Proc. POPL, 2009. [5] Bor-Yuh Evan Chang and Xavier Rival. Relational \ninductive shape analysis. In Proc. POPL, 2008. [6] Arthur Chargu\u00b4eraud. Program veri.cation through characteristic \nfor\u00admulae. In Proc. ICFP, 2010. [7] Adam Chlipala, Gregory Malecha, Greg Morrisett, Avraham Shinnar, \nand Ryan Wisnesky. Effective interactive proofs for higher-order imperative programs. In Proc. ICFP, \n2009. [8] Coq Development Team. The Coq proof assistant reference manual, version 8.3. 2010. [9] David \nDelahaye. A tactic language for the system Coq. In Proc. LPAR, 2000. [10] David Detlefs, Greg Nelson, \nand James B. Saxe. Simplify: a theorem prover for program checking. J. ACM, 52(3):365 473, 2005. [11] \nXinyu Feng and Zhong Shao. Modular veri.cation of concurrent assembly code with dynamic thread creation \nand termination. In Proc. ICFP, 2005. [12] Xinyu Feng, Zhong Shao, Yuan Dong, and Yu Guo. Certifying \nlow\u00adlevel programs with hardware interrupts and preemptive threads. In Proc. PLDI, 2008. [13] Xinyu Feng, \nZhong Shao, Alexander Vaynberg, Sen Xiang, and Zhaozhong Ni. Modular veri.cation of assembly code with \nstack\u00adbased control abstractions. In Proc. PLDI, 2006. [14] Cormac Flanagan, K. Rustan M. Leino, Mark \nLillibridge, Greg Nel\u00adson, James B. Saxe, and Raymie Stata. Extended static checking for Java. In Proc. \nPLDI, 2002. [15] Nadeem Abdul Hamid and Zhong Shao. Interfacing Hoare logic and type systems for foundational \nproof-carrying code. In Proc. TPHOLs, 2004. [16] Chris Hawblitzel and Erez Petrank. Automated veri.cation \nof practi\u00adcal garbage collectors. In Proc. POPL, 2009. [17] Bart Jacobs, Jan Smans, and Frank Piessens. \nA quick tour of the VeriFast program veri.er. In Proc. APLAS, 2010. [18] Gerwin Klein, Kevin Elphinstone, \nGernot Heiser, June Andronick, David Cock, Philip Derrin, Dhammika Elkaduwe, Kai Engelhardt, Rafal Kolanski, \nMichael Norrish, Thomas Sewell, Harvey Tuch, and Simon Winwood. seL4: Formal veri.cation of an OS kernel. \nIn Proc. SOSP, 2009. [19] Viktor Kuncak, Ruzica Piskac, Philippe Suter, and Thomas Wies. Building a calculus \nof data structures (invited paper). In Proc. VMCAI, 2010. [20] Andrew McCreight. Practical tactics for \nseparation logic. In Proc. TPHOLs, 2009. [21] Andrew McCreight, Zhong Shao, Chunxiao Lin, and Long Li. \nA gen\u00aderal framework for certifying garbage collectors and their mutators. In Proc. PLDI, 2007. [22] \nFarhad Mehta and Tobias Nipkow. Proving pointer programs in higher-order logic. In Proc. CADE, 2003. \n[23] Aleksandar Nanevski, Greg Morrisett, Avraham Shinnar, Paul Gov\u00adereau, and Lars Birkedal. Ynot: Reasoning \nwith the awkward squad. In Proc. ICFP, 2008. [24] Zhaozhong Ni and Zhong Shao. Certi.ed assembly programming \nwith embedded code pointers. In Proc. POPL, 2006. [25] Zhaozhong Ni, Dachuan Yu, and Zhong Shao. Modular \nveri.cation of machine-level thread implementation. Technical report, 2006. [26] Zhaozhong Ni, Dachuan \nYu, and Zhong Shao. Using XCAP to cer\u00adtify realistic system code: Machine context management. In Proc. \nTPHOLs, 2007. [27] Lawrence C. Paulson. Isabelle: A generic theorem prover. Journal of Automated Reasoning, \n5, 1994. [28] John C. Reynolds. Separation logic: A logic for shared mutable data structures. In Proc. \nLICS, 2002. [29] Mooly Sagiv, Thomas Reps, and Reinhard Wilhelm. Parametric shape analysis via 3-valued \nlogic. TOPLAS, 24, 2002. [30] Jean Yang and Chris Hawblitzel. Safe to the last instruction: auto\u00admated \nveri.cation of a type-safe operating system. In Proc. PLDI, 2010. [31] Karen Zee, Viktor Kuncak, and \nMartin Rinard. Full functional veri.\u00adcation of linked data structures. In Proc. PLDI, 2008. [32] Karen \nZee, Viktor Kuncak, and Martin Rinard. An integrated proof language for imperative programs. In Proc. \nPLDI, 2009.  \n\t\t\t", "proc_id": "1993498", "abstract": "<p>Several recent projects have shown the feasibility of verifying low-level systems software. Verifications based on automated theorem-proving have omitted reasoning about <i>first-class code pointers</i>, which is critical for tasks like certifying implementations of threads and processes. Conversely, verifications that deal with first-class code pointers have featured long, complex, manual proofs. In this paper, we introduce the Bedrock framework, which supports mostly-automated proofs about programs with the full range of features needed to implement, e.g., language runtime systems.</p> <p>The heart of our approach is in mostly-automated discharge of verification conditions inspired by separation logic. Our take on separation logic is <i>computational</i>, in the sense that function specifications are usually written in terms of <i>reference implementations in a purely functional language</i>. Logical quantifiers are the most challenging feature for most automated verifiers; by relying on functional programs (written in the expressive language of the Coq proof assistant), we are able to avoid quantifiers almost entirely. This leads to some dramatic improvements compared to both past work in classical verification, which we compare against with implementations of data structures like binary search trees and hash tables; and past work in verified programming with code pointers, which we compare against with examples like function memoization and a cooperative threading library.</p>", "authors": [{"name": "Adam Chlipala", "author_profile_id": "81100341086", "affiliation": "Harvard University, Cambridge, MA, USA", "person_id": "P2690550", "email_address": "adamc@cs.harvard.edu", "orcid_id": ""}], "doi_number": "10.1145/1993498.1993526", "year": "2011", "article_id": "1993526", "conference": "PLDI", "title": "Mostly-automated verification of low-level programs in computational separation logic", "url": "http://dl.acm.org/citation.cfm?id=1993526"}