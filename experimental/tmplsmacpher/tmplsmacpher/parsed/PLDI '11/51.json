{"article_publication_date": "06-04-2011", "fulltext": "\n Taming the Wildcards: Combining De.nition-and Use-Site Variance John Altidor Department of Computer \nScience University of Massachusetts, Amherst Amherst, MA 01003, USA jaltidor@cs.umass.edu Shan Shan \nHuang LogicBlox Inc. Two Midtown Plaza Atlanta, GA 30309, USA ssh@logicblox.com Yannis Smaragdakis Department \nof Computer Science, University of Massachusetts, Amherst, MA 01003, USA and Department of Informatics, \nUniversity of Athens, 15784, Greece  yannis@cs.umass.edu smaragd@di.uoa.gr Abstract Variance allows \nthe safe integration of parametric and subtype polymorphism. Two .avors of variance, de.nition-site versus \nuse\u00adsite variance, have been studied and have had their merits hotly debated. De.nition-site variance \n(as in Scala and C#) o.ers simple type-instantiation rules, but causes fractured de.nitions of naturally \ninvariant classes; Use-site variance (as in Java) o.ers simplicity in class de.nitions, yet complex type-instantiation \nrules that elude most programmers. We present a unifying framework for reasoning about variance. Our \nframework is quite simple and entirely denotational, that is, it evokes directly the de.nition of variance \nwith a small core calculus that does not depend on speci.c type systems. This general frame\u00adwork can \nhave multiple applications to combine the best of both worlds: for instance, it can be used to add use-site \nvariance anno\u00adtations to the Scala type system. We show one such application in detail: we extend the \nJava type system with a mechanism that mod\u00adularly infers the de.nition-site variance of type parameters, \nwhile allowing use-site variance annotations on any type-instantiation. Applying our technique to six \nJava generic libraries (including the Java core library) shows that 20-58% (depending on the library) \nof generic de.nitions are inferred to have single-variance; 8-63% of method signatures can be relaxed \nthrough this inference, and up to 91% of existing wildcard annotations are unnecessary and can be elided. \nCategories and Subject Descriptors D.3.1 [Programming Lan\u00adguages]: Formal De.nitions and Theory; D.3.3 \n[Programming Languages]: Language Constructs and Features Polymorphism,Data types and structures,Classes \nand objects General Terms Design, Languages Keywords variance, de.nition-site variance, use-site variance, \nwildcards, language extensions Permission to make digital or hard copies of all or part of this work \nfor personal or classroom use is granted without fee provided that copies are not made or distributed \nfor pro.t or commercial advantage and that copies bear this notice and the full citation on the .rst \npage. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior \nspeci.c permission and/or a fee. PLDI 11, June 4 8, 2011, San Jose, California, USA. Copyright &#38;#169; \n2011 ACM 978-1-4503-0663-8/11/06. . . $10.00 1. Introduction Genericity is one of the most signi.cant \nprogramming language advances of the past 40 years. Variance mechanisms are the key\u00adstone of safe genericity \nin modern programming languages, as they attempt to reconcile the two fundamental forms of genericity: \npara\u00admetric and subtype polymorphism. Concretely, variance mecha\u00adnisms aim to answer the question under \nwhat conditions for type expressions Exp1 and Exp2 is C<Exp1> a subtype of C<Exp2>? The conventional \nanswer to this question has been de.nition\u00adsite variance: the de.nition of generic class C<X> determines \nits variance [2, 9, 13]. Depending on how the type parameter X is used, C can have one of four .avors \nof variance: it can be covariant, meaning that C<S> is a subtype of C<T> if S is a subtype of T; it can \nbe contravariant, meaning that C<S> is a subtype of C<T> if T is a subtype of S; it can be bivariant, \nmeaning that C<S> is always a subtype of C<T>; or it can be invariant, meaning that C<S> and C<T> are \nnever subtype-related for di.erent types T and S. An alternative approach has been introduced in the \npast decade: Use-site variance [17, 22] interprets the de.nition of a generic class C<X> as the introduction \nof 4 distinct types in the type system: the covariant, contravariant, bivariant, and invariant part of \nC<X>. Each of these types only supports the methods compatible with each variance designation. For instance, \nthe covariant part of C<T> would only support methods that would be safe to call on an object of type \nC<S> when S is a subtype of T. In this way, the use-site of a generic class declares what it can accept, \nwith a vocabulary such as C\u00adof-any-subtype-of-T (concretely written C<? extends T> in Java), and the \ntype system checks that the use is indeed valid. Use-site variance is a truly elegant idea. Producing \nautomati\u00adcally all di.erent variance .avors from a single class de.nition is an approach of hard-to-dispute \n.exibility. The idea was quickly in\u00adtegrated in Java in the form of wildcards and it is widely used in \nstandard Java libraries. Despite the conceptual elegance, however, the practical deployment of wildcards \nhas been less than entirely successful. Among opponents, wildcards has become a virtual synonym for a \nlanguage design mess. (E.g., Josh Bloch s presen\u00adtation at Javapolis 2008 emphasized We simply cannot \na.ord an\u00adother wildcards [4].) The reason is that use-site variance results in conceptual complexity, \nrequires anticipation of generality at all usage points, and postpones the detection of overly restrictive \ntype signatures until their use. For these pragmatic reasons, newer languages, such as Scala [20] and \nC# [15], have returned to and re.ned the tried-and-true ap\u00ad proach of de.nition-site variance. De.nition-site \nvariance is hardly free of usability problems, however. For a class that is not purely covariant or contravariant, \nthe only way to achieve full genericity is by introducing specialized interfaces that correspond to the \nclass s co-, contra-, and bivariant parts. Consequently, users have to re\u00admember the names of these interfaces, \nlibrary designers must an\u00adticipate genericity, and a combinatorial explosion in the number of introduced \ninterfaces is possible. (E.g., for a type Triple<X,Y,Z>, we may need an interface for each of the 33 \n= 27 possible access combinations, such as covariant with respect to X, contravariant with respect to \nY and Z . The number is 33 and not 43 only because bivariance is not allowed as an explicit annotation.) \n In this paper, we introduce an approach that mitigates the ten\u00adsion between use-site and de.nition-site \nvariance. We present a core calculus, VarLang, for reasoning about de.nition-site variance in the presence \nof use-site variance annotations (wildcards). The calculus is very general and its soundness is based \ndirectly on the de.nition of variance, and not on any speci.c type system features. This calculus can \nbe used as the basis for variance reasoning in any type system, for either inference or checking. For \ninstance, our approach can be directly employed for extending Scala with use\u00adsite variance annotations, \nor for extending Java with de.nition-site variance. We explore one such application in detail: we de.ne \na type sys\u00adtem that adds inference of de.nition-site variance to Java. That is, we infer variance automatically \nfor classes that are purely co\u00advariant, purely contravariant, or purely bivariant with respect to a type \nparameter. Our solution is fully compatible with existing Java syntax and only changes the type system \nto make it more per\u00admissive in cases of purely variant types, so that exhaustive use\u00adsite annotation \nis unnecessary. For instance, with our type sys\u00adtem, the Apache Commons-Collections programmer would \nnot need to write Iterator<? extends Map.Entry<? extends K,V>> because Iterator and Map.Entry are inferred \nto be purely co\u00advariant with respect to their (.rst) parameter, hence the type Iterator<Map.Entry<K,V>> \nis exactly as general. Illustration of approach. The variance of a class with respect to its type parameters \nis constrained by the variance of the positions these type parameters occur in. For instance, an argument \ntype posi\u00adtion is contravariant, while a return type position is covariant. How\u00adever, in the presence \nof recursive type constraints and wildcards, no past technique reasons in a general way about the variance \nof a type expression in a certain position. For instance, past techniques would not infer anything other \nthan invariance for classes C and D: class C<X> { X foo (C<? super X> csx) { ... } void bar (D<? extends \nX> dsx) { ... } } class D<Y> { void baz (C<Y> cx) { ... } } Our approach is based on assigning a variance \nto every type expression, and de.ning an operator, . (pronounced transform ), used to compose variances. \nIn our calculus, inferring the most general variance for the above type de.nitions reduces to .nding \nthe maximal solution for a constraint system over the standard variance lattice (* is top, o is bottom, \n+ and - are unordered, with a join of * and a meet of o). If c stands for the (most general) variance \nof the de.nition of C<X> with respect to type parameter X, and d stands for the variance of D<Y> with \nrespect to Y, the constraints (simpli.ed) are: c [ + c [ -. (-U c) c [ -. (+ U d) d [ -. c Consider the \n.rst of these constraints. Its intuitive meaning is that the variance of class C (with respect to X) \nhas to be at most covari\u00adance, +, (because X occurs as a return type of foo). Similarly, for the third \nconstraint, the variance of C has to be at most the variance of type expression D<? extends X> transformed \nby the variance, -, of the (contravariant) position where the type expression occurs. The variance of \ntype expression D<? extends X> itself is the variance of type D joined with the variance of the type \nannotation, +. We will see the full rules and de.nition of ., as well as prove their soundness, later, \nbut for this example it su.ces to know that -.+= -, -.- =+, -.* = *, and -.o = o. It is easy to see with \nmere enumeration of the possibilities that the most general solution has c =+ and d = -. Thus, by formulating \nand solving these constraints, we correctly infer the most general variance: class C is covariant with \nrespect to X, and class D is contravariant with respect to Y. We note that the interaction of wildcards \nand type recursion is non-trivial. For instance, removing the ? super from the type of argument csx would \nmake both C and D be invariant. Contributions. Our paper makes several contributions: We present a general \napproach for reasoning about variance. The approach consists of a core calculus, whose soundness is proved \nby direct appeal to the de.nition of variance, i.e., inde\u00adpendently of any speci.c type system. To our \nknowledge, this is the .rst approach to allow general reasoning about de.nition\u00adsite variance in the \npresence of either use-site variance annota\u00adtions or type recursion.  We apply the approach in the context \nof Java to produce a (con\u00adservative) inference algorithm for computing the de.nition-site variance of \nclasses and interfaces. Unlike past work (e.g., [19]), our inference technique is modular: the variance \nof a generic class is purely determined by its own interface de.nition (and that of the types it uses), \nnot by the code that uses the class. Un\u00adder our updated type system, many uses of variance annotations \nbecome redundant. All previously legal Java programs remain legal, although some newly type-correct programs \nmight have been previously rejected.  We conduct a large study to show how use-site and de.nition\u00adsite \nvariance coexist fruitfully, even in code that was written with only use-site variance in mind. Past \nliterature [12, 17] has largely assumed that single-variant type parameters are seldom used, thus promoting \nuse-site variance for .exibility. We show that this is not the case over all libraries (including the \nstan\u00addard library: all packages in java.*) we .nd that 32% of the generic classes and interfaces have \nsingle-variance type param\u00adeters. This is strong evidence for the need to combine de.nition\u00adand use-site \nvariance, since they both occur commonly. Addi\u00adtionally, 13% of all signatures of methods using generics \nare found to use too-restricted types. (Importantly, this analysis is without concern for what the method \nactually does i.e., treating the body as a black box, which can change in the fu\u00adture.) Furthermore, \nour inference algorithm obviates the need for many uses of wildcards: 37% of existing wildcard uses in \nthese libraries are rendered unnecessary.  2. Type-Checking Variance The study of variance has a long \nhistory [2, 6 9, 13, 16, 17, 23]. It aims to provide safe conditions for subtyping between di.erent instantiations \nof the same generic class or interface. Consider the following example: class List<X> { void set(int \ni, X x) { ... } X get(int i) { ... } }  If we naively assume that List<S> <: List<T> for any S <: T \n(where <: is the subtyping relation), the following ill-typed code could pass compile-time type-checking, \nand result in runtime errors: List<Integer> intList = new List<Integer>(); List<Number> numList = intList; \nnumList.set(0, new Float(0.0f)); // Writing Float into intList! The key to safe parametric subtyping \nlies with the concept of the variance of type parameter X in the de.nition of C<X>. If X only appears \ncovariantly, then it is always safe to assume that C<S> <: C<T> if S <: T. If X only appears contravariantly, \nit is always safe to assume that C<S> <: C<T> if T <: S. If X appears both co\u00adand contravariantly, C \nis invariant: C<S> <: C<T> only if S = T. This notion of variance is called de.nition-site variance. \n2.1 De.nition-site Variance. Languages supporting de.nition-site variance [15, 20] typically re\u00ad quire \neach type parameter to be declared with a variance annota\u00adtion. For instance, Scala [20] requires the \nannotation + for covari\u00adant type parameters, -for contravariant type parameters, and in\u00advariance is the \ndefault. A well-established set of rules can then be used to verify that the use of the type parameter \nin the generic1 is consistent with the annotation. We provide an overview of these rules here, as they \nform the basis of both use-site variance and our inference. Each typing position in a generic s signature \nhas an associated variance. For instance, method return and exception types, super\u00adtypes, and upper bounds \nof type parameters are covariant positions; method argument types and type parameter lower bounds are \ncon\u00adtravariant positions; .eld types are both co-and contravariant oc\u00adcurrences, inducing invariance. \nType checking the declared vari\u00adance annotation of a type parameter requires determining the vari\u00adance \nof the positions the type parameter occurs in. The variance of all such positions should be at most the \ndeclared variance of the type parameter. Consider the following templates of Scala classes, where vX \n, vY , and vZ stand for variance annotations. abstract class RList[vX X] { def get(i:Int):X } abstract \nclass WList[vY Y] { def set(i:Int, y:Y):Unit } abstract class IList[vZ Z] { def setAndGet(i:Int, z:Z):Z \n} The variance vX is the declared de.nition-site variance for type variable X of the Scala class RList. \nIf vX = +, the RList class type checks because X does not occur in a contravariant position. If vY = \n+, the WList class does not type check because Y occurs in a contravariant position (second argument \ntype in set method) but vY = + implies Y should only occur in a covariant position. IList type checks \nonly if vZ = o because Z occurs in a covariant and a contravariant position. Intuitively, RList is a \nread-only list: it only supports retrieving objects. Retrieving objects of type T can be safely thought \nof as re\u00adtrieving objects of any supertype of T. Thus, a read-only list of Ts (RList[T]) can always be \nsafely thought of as a read-only list of some supertype of Ts(RList[S], where T <: S). This is the exact \nde.nition of covariant subtyping. Thus, RList is covariant in X.2 Similarly, WList is a write-only list, \nand is intuitively contravariant. Its de.nition supports this intuition: Objects of type T can be writ\u00adten \nto a write-only list of Ts and to a write-only list of WList[S], 1 We refer to all generic types (e.g., \nclasses, traits, interfaces) uniformly as generics . 2 We use interchangeably the wordings C is v-variant \nin X , C is v\u00advariant/has variance v with respect to X , and X s variance in C is v . For brevity, if \nit is clear from the context, we do not specify which type parameter a de.nition-site variance is with \nrespect to. where T <: S, because objects of type T are also objects of type S. Hence, a WList[S] can \nbe safely thought of as a WList[T], if T <: S. The variance of type variables is transformed by the variance \nof the context the variables appear in. Covariant positions preserve the variance of types that appear \nin them, whereas contravariant positions reverse the variance of the types that appear in them. The reverse \nof covariance is contravariance, and vice versa. The re\u00adverse of invariance is itself. Thus, we can consider \nthe occurrence of a type parameter to be initially covariant. For instance, consider again the Scala \nclasses above. In RList, X only appears as the re\u00adturn type of a method, which preserves the initial \ncovariance of X, so RList is covariant in X. In WList, Y appears in a contravariant po\u00adsition, which \nreverses its initial covariance, to contravariance. Thus, WList is contravariant. When a type parameter \nis used to instantiate a generic, its vari\u00adance is further transformed by the declared de.nition-site \nvariance of that generic. For example: class SourceList[+Z] { def copyTo(to:WList[Z]):Unit } Suppose \nthe declared de.nition-site variance of WList (with re\u00adspect to its single parameter) is contravariance. \nIn WList[Z], the initial covariance of Z is transformed by the de.nition-site variance of WList (contravariance). \nIt is then transformed again by the con\u00adtravariant method argument position. As a result, Z appears covari\u00adantly \nin this context, and SourceList is covariant in Z, as declared. Any variance transformed by invariance \nbecomes invariance. Thus, if Z was used to parameterize an invariant generic, its appearance would have \nbeen invariant. In Section 3.1 we generalize and for\u00ad malize this notion of transforming variance. We \nhave so far neglected to discuss bivariance: C<X> is bivariant implies that C<S> <: C<T> for any types \nS and T. Declaring a bivari\u00adant type parameter is not supported by the widely used de.nition\u00adsite variant \nlanguages. At .rst this seems to not be entirely sur\u00adprising. For a type parameter to be bivariant, it \nmust only appear bivariantly in a generic. This means either it does not appear at all, or it appears \nonly as the type argument to instantiate other bivariant generics. If a type parameter does not appear \nin a generic s signa\u00adture at all, then it is useless to parameterize over it; if it is only used to instantiate \nother bivariant generics, it could just as well be re\u00adplaced by any arbitrary type, since, by de.nition, \na bivariant generic does not care what type it is instantiated with. Nevertheless, this ar\u00adgument ignores \ntype recursion. As we discuss in Section 3.3 and in our experimental .ndings, several interesting interface \nde.nitions are inherently bivariant. Finally, instead of declaring the de.nition-site variance of a type \nparameter and checking it for consistency, it is tempting to infer the most general such variance from \nthe de.nition of a generic. This becomes hard in the presence of type recursion and supporting it in \nfull generality is one of the contributions of our work.  2.2 Use-site Variance. An alternative approach \nto variance is use-site variance [7, 17, 23]. Instead of declaring the variance of X at its de.nition \nsite, generics are assumed to be invariant in their type parameters. However, a type-instantiation of \nC<X> can be made co-, contra-, or bivariant using variance annotations. For instance, using the Java \nwildcard syntax, C<? extends T> is a covariant instantiation of C, representing a type C-of\u00adsome-subtype-of-T \n. C<? extends T> is a supertype of all type\u00adinstantiations C<S>, or C<? extends S>, where S <: T. In \nexchange for such liberal subtyping rules, type C<? extends T> can only ac\u00adcess those methods and .elds \nof C in which X appears covariantly. In determining this, use-site variance applies the same set of rules \nused in de.nition-site variance, with the additional condition that the upper bound of a wildcard is \nconsidered a covariant position, and the lower bound of a wildcard a contravariant position.  For example, \nList<? extends T>, only has access to method X get(int i) , but not method void set(int i, X x) . (More \nprecisely, method set can only be called with null for its second argument. We elide such .ne distinctions \nin this section.) Similarly, C<? super T> is the contravariant version of C, and is a supertype of any \nC<S> and C<? super S>, where T <: S. Of course, C<? super T> has access only to methods and .elds in \nwhich X appears contravariantly or not at all. Use-site variance also allows the representation of the \nbivariant version of a generic. In Java, this is accomplished through the unbounded wildcard: C<?>. Using \nthis notation, C<S> <: C<?>, for any S. The bivariant type, however, only has access to methods and .elds \nin which the type parameter does not appear at all. In de.nition-site variance, these methods and .elds \nwould have to be factored out into a non-generic class.  2.3 A Comparison. Both approaches to variance \nhave their merits and shortcomings. De.nition-site variance enjoys a certain degree of conceptual sim\u00adplicity: \nthe generic type instantiation rules and subtyping relation\u00adships are clear. However, the class or interface \ndesigner must pay for such simplicity by splitting the de.nitions of data types into co-, contra, and \nbivariant versions. This can be an unnatural ex\u00adercise. For example, the data structures library for \nScala contains immutable (covariant) and mutable (invariant) versions of almost every data type and this \nis not even a complete factoring of the variants, since it does not include contravariant (write-only) \nver\u00adsions of the data types. The situation gets even more complex when a generic has more than one type \nparameter. In general, a generic with n type param\u00adeters needs 3n (or 4n if bivariance is allowed as \nan explicit anno\u00adtation) interfaces to represent a complete variant factoring of its methods. Arguably, \nin practice, this is often not necessary. Use-site variance, on the other hand, allows users of a generic \nto create co-, contra-, and bivariant versions of the generic on the .y. This .exibility allows class \nor interface designers to implement their data types in whatever way is natural. However, the users of \nthese generics must pay the price, by carefully considering the correct use-site variance annotations, \nso that the type can be as general as possible. This might not seem very di.cult for a simple instantiation \nsuch as List<? extends Number>. However, type signatures can very quickly become complicated. For instance, \nthe following method signature is part of the Apache Commons-Collections Library: Iterator<? extends \nMap.Entry<? extends K,V>> createEntrySetIterator( Iterator<? extends Map.Entry<? extends K,V>>)  2.4 \nGeneralizing the Design Space. Our goal is to combine the positive aspects of use-site and de.nition-site \nvariance, while mitigating their shortcomings. The key is to have a uniform and general treatment of \nde.nition and use-site variance in the same type system. This creates opportuni\u00adties for interesting \nlanguage designs. For instance: A language can combine explicit de.nition-and use-site variance annotations \nand perform type checking to ensure their soundness. For instance, Scala or C# can integrate wildcards \nin their syntax and type reasoning. This will give programmers the opportunity to choose not to split \nthe de.nition of a type just to allow more general handling in clients. If, for instance, a List is supposed \nto support both reading and writing of data, then its interface can be de.ned to include both kinds of \nmethods, and not split into two types. The methods that use List can still be made fully general, as \nlong as they specify use-site annotations. Generally, allowing both kinds of variance in a single language \nensures modularity: parts of the code can be made fully general regardless of how other code is de.ned. \nThis reduces the need for anticipation and lowers the burden of up-front library design. Similarly, Java \ncan integrate explicit de.nition-site variance annotations for purely variant types. This will reduce \nthe need for use-site annotation and the risk of too-restricted types. A language can combine use-site \nvariance annotations with in\u00adference of de.nition-site variance (for purely variant types). This is the \napproach that we implement and explore in later sec\u00adtions. Consider the above example of the long signatures \nin the two type-instantiations of Iterator. Our approach can infer that Iterator is covariant, and Map.Entry \nis covariant in its .rst type parameter without having to change the de.nition of either generic. Thus, \nthe following signature in our system has exactly the same generality without any wildcards: Iterator<Map.Entry<K,V>> \ncreateEntrySetIterator(Iterator<Map.Entry<K,V>>) Furthermore, specifying the most general types proves \nto be challenging for even the most seasoned Java programmers: (at least) 8% of the types in method signatures \nof the Java core library (java.*) are overly speci.c. We will discuss the details of our .ndings in Section \n5.2. 3. Reasoning about Variance In order to meet our goal of a general, uni.ed framework (for both checking \nand inference of both use-site and de.nition-site variance) we need to solve three di.erent problems. \nThe .rst is that of composing variances, the second deals with the integration of use-site annotations \nin de.nition-site reasoning, and the third concerns the handling of recursive types. 3.1 Variance Composition \nIn earlier variance formalisms, reasoning about nested types, such as A<B<X>>, has been hard. Igarashi \nand Viroli pioneered the treat\u00adment of variant types as unions of sets of instances. Regarding nested \ntypes, they note (Section 3.3 of [17]): We could explain more complicated cases that involve nested types \nbut it would get harder to think of the set of instances denoted by such types. The .rst observation \nof our work is that it is quite easy to rea\u00adson about nested types, not as sets of instances but as variance \ncomposition. That is, given two generic types A<X> and B<X>, if the (de.nition-site) variances of A and \nB (with respect to their type parameters) are known, then we can compute the variance of type A<B<X>>.3 \nThis composition property generalizes to arbitrarily complex-nested type expressions. The basis of the \ncomputation of composed variances is the transform operator, ., de.ned in Fig\u00adure 1. The relation v1 \n. v2 = v3 intuitively denotes the following: If the variance of a type variable X in type expression \nE is v2 and the de.nition-site variance of the type parameter of a class C is v1,4 then the variance \nof X in type expression C<E> is v3. The behavior of the transform operator is simple: invariance transforms \neverything into invariance, bivariance transforms every\u00adthing into bivariance, covariance transforming \na variance leaves it 3 This relies on a natural extension of the de.nition of variance, to include the \nconcept of a variance of an arbitrary type expression with respect to a type variable. E.g., type expression \nE is covariant in X i. T1 <: T2 =. E[T1/X] <: E[T2/X]. (These brackets denote substitution of a type \nfor a type variable and should not be confused with the Scala bracket notation for generics, which we \nshall avoid except in pure-Scala examples.) 4 For simplicity, we often refer to generics with a single \ntype parameter. For multiple type parameters the same reasoning applies to the parameter in the appropriate \nposition.  De.nition of variance transformation: . + . +=+ -. += - *. += * o . += o + .- = - -.- =+ \n*.- = * o .- = o + .* = * -.* = * *.* = * o .* = o + . o = o -. o = o *. o = * o . o = o  Figure 1. \nVariance transform operator. the same, and contravariance reverses it. (The reverse of bivari\u00adance is \nitself, the reverse of invariance is itself.) To sample why the de.nition of the transform operator makes \nsense, let us con\u00adsider some of its cases. (The rest are covered exhaustively in our proof of soundness.) \n Case + .- = -: This means that type expression C<E> is con\u00adtravariant with respect to type variable \nX when generic C is covari\u00adant in its type parameter and type expression E is contravariant in X. This \nis true because, for any T1, T2:  T1 <: T2 =. (by contravariance of E) E[T2/X] <: E[T1/X] =. (by covariance \nof C) C<E[T2/X]> <: C<E[T1/X]> =. C<E>[T2/X] <: C<E>[T1/X] Hence, C<E> is contravariant with respect \nto X. Case *.v = *: This means that type expression C<E> is bivariant with respect to type variable X \nwhen generic C is bivariant in its type parameter, regardless of the variance of type expression E (even \ninvariance). This is true because: for any types S and T =. (by bivariance of C) C<E[S/X]> <: C<E[T/X]> \n=. C<E>[S/X] <: C<E>[T/X] Hence, C<E> is bivariant with respect to X. As can be seen by inspection of \nall cases in Figure 1, operator . is associative. The operator would also be commutative, except for \nthe case *. o = * * o = o .*. This is a design choice, however. With the types-as-sets approach that \nwe follow in our formalization, operator . would be safe to de.ne as a commutative operator, by changing \nthe case o.* to return *. To see this, consider the meaning of o .*. When generic C is invariant with \nrespect to its type parameter X and type expression E is bivariant in X, should type expression C<E> \nbe bivariant or invariant with respect to X? The answer depends on what we mean by invariance . We de.ned \ninvariance earlier as C<S > <: C<T> only if S = T . Is the type equality S = T syntactic or semantic? \n(I.e., does type equality signify type identity or equivalence, as can be established in the type system?) \nIf type equality is taken to be syntactic, then the only sound choice is o .* = o: C<E>[S/X] <: C<E>[T/X] \n=. C<E[S/X]> <: C<E[T/X]> =. (by invariance of C) E[S/X] = E[T/X] =. (assuming X occurs in E) S = T Hence, \nC<E> is invariant with respect to X. If, however, the de.ni\u00adtion of invariance allows for type equivalence \ninstead of syntactic equality, then it is safe to have o .* = *: By the bivariance of E, E[S/X] <: E[T/X] \nand E[T/X] <: E[S/X]. Hence, E[S/X] is equivalent to E[T/X] and consequently C<E>[S / X] can be shown \nequivalent to C<E>[T / X] (assuming a natural extensionality axiom in the type system). We chose the \nconservative de.nition, o .* = o, in Figure 1 to match that used in our implementation of a de.nition-site \nvariance inference algorithm for Java, discussed later. Since, in our appli\u00adcation, bivariance is often \ninferred (not stated by the programmer) and since Java does not naturally have a notion of semantic type \nequivalence, we opted to avoid the possible complications both for the user and for interfacing with \nother parts of the language. Similar reasoning to the transform operator is performed in Scala to check \nde.nition-site variance annotations. Section 4.5 of [20] de.nes the variance position of a type parameter \nin a type or template and states Let the opposite of covariance be contravariance, and the opposite of \ninvariance be itself. It also states a number of rules de.ning the variance of the various type positions \nsuch as The variance position of a method parameter is the opposite of the variance position of the enclosing \nparameter clause. The . operator is a generalization of the reasoning stated in that section; it adds \nthe notion of bivariance and how the variance of a context transforms the variance of a type actual in \ngeneral instead of de.ning the variance of a position for language speci.c constructs.  3.2 Integration \nof Use-Site Variance The second new element of our work is the integration of use-site annotations in \nthe reasoning about de.nition-site variance. Earlier work such as [17] showed how to reason about use-site \nvariance for Java. Emir et al [13] formalized de.nition-site variance as it occurs in of C#.5 However, \nno earlier work explained how to formally reason about variance in a context including both de.nition-site \nand use-site variance. For example, suppose Scala is extended with support for use-site variance, v is \na variance annotation (+, -, or o), and the following are syntactically legal Scala classes. abstract \nclass C[vX] { def set(arg1:X):Unit } abstract class D[+X] { def compare(arg2:C[+X]):Unit } Section 2.1 \ngave an overview of how declared de.nition-site variance annotations are type checked in Scala. Since \nclass C only contains the set method, it type checks with v = -because X only appears contravariantly \nin the type signature of the set method. However, type checking class D with the compare method requires \nreasoning about the variance of X in the argument type expression C[+X]. In our uni.ed framework, a use-site \nannotation corresponds to a join operation in the standard variance lattice (Figure 2). That is, if generic \nC<X> has de.nition-site variance v1 with respect to X, then the type expression C[v2X] has variance v1 \nU v2 with respect to X. Figure 2. Usual variance lattice. Intuitively, this rule makes sense: When applying \nuse-site vari\u00adance annotations, it is as if we are removing from the de.nition of the generic the elements \nthat are incompatible with the use-site variance. For instance, when taking the covariant version, C[+X], \nof our Scala class C, above, we can only access the members that 5 Their calculus is an extension of \nC# minor [18].  use type parameter X covariantly e.g., method set would be inac\u00adcessible. Hence, if \nclass C is naturally contravariant in X (meaning that X only occurs contravariantly in the body of C), \nthen C[+X] is a type that cannot access any member of C that uses X. Thus, C[+X] is bivariant in X: the \nvalue of the type parameter cannot be used. This is precisely what our lattice join approach yields: \n+ U- = *. As a result, any declared de.nition-site variance for class D would be legal. To see more rigorously \nwhy the lattice-join approach is correct, let us consider the above case formally. (Other cases are covered \nexhaustively in our proof of soundness.) Given a contravariant generic C, why is it safe to infer that \nC<+X> (C[+X] in Scala syntax) is bivariant in X? We start from the Igarashi and Viroli approach to variance: \nAll types are in a lattice with subtyping as its partial order and the meaning of C<+T> is T' <: T C<T \n>. This de.nition yields the standard variance theorems T <: T'. C<+T> <: C<+T > and C<T> <: C<+T>. Consider \nthen the bottom element of the type lattice. (This lattice depends on the type system of the language \nand is not to be confused with the simple variance lattice of Figure 2.) We have: . <: T . (by .rst theorem \nabove) C<+.> <: C<+T> (1) But also, for any type T : . <: T'. (C contravariant) C<T > <: C<.> (2) Therefore: \nC<+T> = (by variance def) C<T > <: (by (2), above) T' <: T C<.> <: (by second theorem above) C<+.> (3) \nHence, from (1) and (3), all C<+T> are always subtype-related, i.e., have type C<*>. A Note on Scala: \nTo enable interoperability between Scala and Java, Scala represents Java wildcard types as existential \ntypes. For example, a Java Iterator<?> could be written as Iterator[T] forSome { type T } or more compactly \nas Iterator[ ]. Sim\u00adilarly, the type Java Iterator<? extends Comparator> maps to the Scala type Iterator[ \n<: Comparator>], and the type Java Iterator<? super Comparator> maps to the Scala type Iterator[ >: Comparator>]. \nHowever, Scala variance reasoning with existential types is too conservative because it just assumes \nthat the use-site variance annotation overrides the de.nition-site variance instead of reasoning about \nhow they both interact. For ex\u00adample, consider the Scala traits below. trait GenType[+Y] { def get(i:Int):Y \n} trait Wild[-X] { def add(elem:X):Unit // flagged as error but actually safe def compare(w:GenType[_ \n>: X]):Unit } The Scala compiler .ags an error because it assumes the vari\u00adance of X in GenType[ >: X] \nis contravariance. This contravari\u00adance occurrence is then negated (transformed by contravariance) to \ncovariance because it occurs in an argument (contravariant) posi\u00adtion. Because the Scala compiler assumes \nX occurs in a covariant position in compare s argument type but the de.nition-site of X in trait Wild \nis contravariance, Scala .ags this occurrence as an error. However, it is safe to assume that the variance \nof X in GenType[ >: X] is bivariance. Because GenType is covariant in its type parameter, the contravariant \nversion of GenType essentially provides no mem\u00adbers of GenType that contain GenType s type parameter \nin their type signature. Our joining of the de.nition-site and use-site variances takes advantage of \nthis reasoning enabling more safe code to type check.  3.3 Recursive Variances The third novel element \nof our approach consists of reasoning about recursive type de.nitions. This is particularly important \nfor infer\u00adring (instead of just checking) de.nition-site variance. With type recursion, the unknown variance \nbecomes recursively de.ned and it is not easy to compute the most general solution. Furthermore, type \nrecursion makes the case of bivariance quite interesting. In con\u00adtrast to non-recursive types, recursive \ntypes can be bivariant even when their type parameter is used. For instance the following type is safely \nbivariant: interface I<X> { I<X> foo (I<X> i); } To appreciate the interesting complexities of reasoning \nabout type recursion, we discuss some cases next. Recursive Variance Type 1: The following interface \ndemon\u00adstrates a most simple form of recursive variance: interface C1<X> { C1<X> foo1 (); } The variance \nof C1 depends on how X appears in its signature. The only appearance of X is in the return type of foo1, \na covariant position, as the argument to C1. Thus, the variance of X in this appearance is its initial \ncovariance, transformed by the variance of C1 the very variance we are trying to infer! This type of \nrecursive variance essentially says that the variance of C1 is the variance of C1, and thus can be satis.ed \nby any of the four variances: covariance, contravariance, invariance, or bivariance. Without any more \nappearances of X, the most liberal form of variance for C1 is bivariance. If X does appear in other typing \npositions, however, the variance of its declaring generic is completely determined by the variance of \nthese appearances: interface C2<X> extends C1<X> { void bar2 (X x); } interface C3<X> extends C1<X> { \nX bar3 (); } The de.nition-site variance of C2 is constrained by the variance of C1, as well as X s appearance \nas a method argument type a contravariant appearance. Since C1 s variance is completely un\u00adconstrained, \nC2 is simply contravariant. Similarly, C3 is only con\u00adstrained by X s appearance as a method return type \na covariant appearance and is thus covariant, as well. The above pattern will be common in all our recursive \nvariances. Without any constraints other than the recursive one, a generic is most generally bivariant. \nWhen other constraints are factored in, however, the real variance of C1 can be understood informally \nas can be either co-or contravariant . Recursive Variance Type 2: The next example shows a similar, but \nmuch more restrictive form of recursive variance: interface D1<X> { void foo1 (D1<X> dx); } The variance \nof D1 is again recursively dependent on itself, only this time X appears in D1<X> which is a method argument. \nIf a recursive variance did not impose any restrictions in a covariant position, why would it be any \ndi.erent in a contravariant position? Interestingly, the contravariance means that the variance of D1 \nis the variance of D1 transformed by the contravariance. This means the variance of D1 must be the reverse \nof itself! The only two variances that can satisfy such a condition are bi\u00adand invariance. Again, without \nany other uses of X, D1<X> is most generally bivariant.  However, if X does appear either co-or contravariantly \nin com\u00adbination with this type of recursive variance, the resulting variance can only be invariance: \ninterface D2<X> extends D1<X> { void bar2 (X x); } interface D3<X> extends D1<X> { X bar3 (); } In the \nabove example, X appears contravariantly in D2, as the argument of bar2. At the same time, the variance \nof X must be the opposite of itself, as constrained by the recursive variance in supertype D1. This is \nequivalent to X appearing covariantly, as well. Thus, the only reasonable variance for D2 is invariance. \nA similar reasoning results in the invariance of D3. Thus, recursive variance of this type can be understood \ninfor\u00admally as cannot be either co-or contravariant when other con\u00adstraints are taken into account. Recursive \nVariance Type 3: The following example shows yet a third kind of recursive variance: interface E1<X> \n{ E1<E1<X>> foo1 (); } The variance of E1 is the same as X s variance in E1<E1<X>>. That is, the initial \ncovariance of X, transformed by the variance of E1 twice. This type of recursive variance can, again, \nlike the previous two, be satis.ed by either in-or bivariance. However, the key insight is that, no matter \nwhether E1 is contra-or covariant, any variance transformed by E1 twice (or any even number of times, \nfor that matter) is always preserved. This is obvious if E1 is covariant. If E1 is contravariant, being \ntransformed by E1 twice means a variance is reversed, and then reversed again, which still yields a preserved \nvariance. Thus, unless E1 is bi-or invariant, X in E1<E1<X>> is always a covariant appearance. Thus, \nwhen other appearances of X interact with this form of recursive variance, its informal meaning becomes \ncannot be con\u00adtravariant . In other words, when this recursive variance is part of the constraints of \na type, the type can be bivariant, covariant, or invariant. The following examples demonstrate this: \ninterface E2<X> extends E1<X> { void bar2 (X x); } interface E3<X> extends E1<X> { X bar3 (); } X appears \ncontravariantly in E2, eliminating bivariance and co\u00advariance as an option for E2. However, X also appears \nin E1<E1<X>> through subtyping, which means it cannot be contravariant. Thus, E2 is invariant. In E3, \nX appears covariantly, and X in E1<E1<X>> can still be covariant. Thus, E3 can safely be covariant. Recursive \nVariance Type 4: Our last example of recursive vari\u00adance is also twice constrained by itself. But this \ntime, it is further transformed by a contravariance: interface F1<X> { int foo1(F1<F1<X>> x); } The variance \nof F1 is the same as X s variance in F1<F1<X>>, then transformed by the contravariant position of the \nmethod ar\u00adgument type. That is, X s initial covariance, transformed twice by the variance of F1, then \nreversed. Like all the other recursive vari\u00adances, bi-and invariance are options. However, since the \ntwice\u00adtransformation by any variance preserves the initial covariance of X in F1<F1<X>>, the transformation \nby the contravariance produces a contravariance. Thus, if F1 cannot be bivariant, it must be con\u00adtravariant \n(or invariant). In other words, along with other constraints, F1 has the informal meaning: cannot be \ncovariant . For instance: interface F2<X> extends F1<X> { void bar2 (X x); } interface F3<X> extends \nF1<X> { X bar3 (); } In F2, X appears contravariantly as a method argument. Com\u00adbined with the recursive \nvariance through subtyping F1<X>, F2 can be contravariant. In F3, however, X appears covariantly. With \nbi\u00advariance and contravariance no longer an option, the only variance satisfying both this covariant \nappearance and the recursive variance of F1<F1<X>> is invariance. Thus, F3 is invariant in X. Handling \nRecursive Variance. The above list of recursive vari\u00adances is not exhaustive, although it is representative \nof most obvi\u00adous cases. It should be clear that handling recursive variances in their full generality \nis hard and requires some form of search. The reason our approach can handle recursive variance well \nis that all reasoning is based on constraint solving over the standard variance lattice. Constraints \nare simple inequalities ( below on the lattice) and can capture type recursion by having the same constant \nor vari\u00adable (in the case of type inference) multiple times, both on the left and the right hand side \nof an inequality. A Note on Scala: Scala s reasoning about recursive variances is limited because it \ndoes not have the notion of bivariance; it does not allow the most general types to be speci.ed. Consider \nthe three following traits. trait C1[vX X] { def foo:C1[X] } trait C2[vY Y] extends C1[Y] { def bar(arg:Y):Unit \n} trait C3[vZ Z] extends C1[Z] { def baz:Z } Because trait C1 has type 1 recursive variance, if Scala \nsupported bivariant annotations, it would be safe to set the de.nition-site vari\u00adances as follows: vX \n= *, vY = -, and vZ =+. Since Scala does not support bivariant annotations, no assignments allow both \ntrait C2 to be contravariant and trait C3 to be covariant. For example, setting vX =+ implies attempting \nto compile trait C2 will generate an error because Scala infers Y occurs covariantly in the base type \nexpres\u00adsion occurring in C2[-Y] extends C1[Y] ; since Y is declared to be contravariant, Y should not \noccur in a covariant position in the de.nition of C2. Below are the only three assignments allowed by \nthe Scala compiler. vX = - vY = - vZ = o vX =+ vY = o vZ =+ vX = o vY = o vZ = o 4. Putting It All Together: \nA Core Language and Calculus We combine all the techniques of the previous section into a uni.ed framework \nfor reasoning about variance. We introduce a core lan\u00adguage, VarLang, for describing the various components \nof a class that a.ect its variance. Reasoning is then performed at the level of this core language, by \ntranslating it to a set of constraints. 4.1 Syntax A sentence S in VarLang is a sequence (treated as \na set) of modules, the syntax of which is given in Figure 3. M . Module ::= module C<X> { Tv } T . Type \n::= X | C<vT> v . Variance ::=+ |-|*| o C . ModuleNames is a set of module names X . VariableNames is \na set of variable names Figure 3. Syntax of VarLang Note that variance annotations, v,(+/-/*/o) can appear \nin two places: at the top level of a module, as a su.x, and at the type level, as a pre.x. Informally, \na v at the top level means that the corresponding type appears covariantly/contravariantly/invariantly \n(i.e., in a covariant/contravariant/invariant position). A v on a type means that the type parameter \nis quali.ed with the corresponding use-site variance annotation, or no annotation (for invariance). For \ninstance, consider the VarLang sentence:  module C<X> { X+, C<-X>-, void+, D<+X>-} module D<Y> { void+, \nC<oY>-} This corresponds to the example from the Introduction. That is, the informal meaning of the VarLang \nsentence is that: In the de.nition of class C<X>, X appears covariantly; C<? super X> appears contravariantly; \nvoid appears covariantly; D<? extends X> appears contravariantly.  In the de.nition of class D<Y>, void \nappears covariantly; C<Y> appears contravariantly.   4.2 VarLang Translation Our reasoning approach \nconsists of translating a VarLang sentence S into a set of constraints over the standard variance lattice \n(Fig\u00adure 2). The constraints are below -inequalities and contain vari\u00ad ables of the form var(X, T) and \nvar(X, C), pronounced variance of type variable X in type expression T and (de.nition-site) vari\u00adance \nof type variable X in generic C . The constraints are then solved to compute variances, depending on \nthe typing problem at hand (checking or inference). The following rules produce the con\u00adstraints. (Note \nthat some of the constraints are vacuous, since they establish an upper bound of *, but they are included \nso that the rules cover all syntactic elements of VarLang and the translation from a VarLang sentence \nto a set of constraints is obvious.) var(X, C) [ vi . var(X, Ti), .i, where module C<X> { Tv }. S (1) \nvar(X, C<>) [* (2) var(X, Y) [*, where X * Y (3) var(X, X) [ + (4) var(X, C<vT>) [ (vi U var(Y, C)) . \nvar(X, Ti), .i, (5) where Y is the i-th type variable in the de.nition of C. Rule 1 speci.es that for \neach type Ti in module C, the variance of the type variable X in C must be below the variance of X in \nTi transformed by vi, the variance of the position that Ti appears in. This corresponds to the traditional \nreasoning about de.nition site variance from Section 2.1. Rules 2 and 3 specify that the X can have any \nvariance in a type expression for which it does not occur in. Rule 4 constrains the initial variance \nof a type variable to be at most covariance. Rule 5 is the most interesting. It integrates our reasoning \nabout how to compose variances for complex expressions (using the transform operator, as described in \nSection 3.1) and how to factor in use-site variance annotations (using a join in the variance lattice, \nas described in Section 3.2). Note that the rules use our transform operator in two di.erent ways: to \ncombine the variance of a position with the variance of a type, and to compose variances. We prove the \nsoundness of the above rules denotationally that is, by direct appeal to the original de.nition and axioms \nof use-site variance [17]. The proof can be found in the extended version of this paper [1, Appendix \nI]. Example. We can now revisit in more detail the example from the Introduction, containing both recursive \nvariance and wildcards: class C<X> { X foo (C<? super X> csx) { ... } void bar (D<? extends X> dsx) { \n... } } class D<Y> { void baz (C<Y> cx) { ... } } As we saw, the corresponding VarLang sentence is: module \nC<X> { X+, C<-X>-, void+, D<+X>-} module D<Y> { void+, C<oY>-} The generated constraints (without duplicates) \nare: var(X, C) [ + . var(X, X) (rule 1) var(X, X) [ + (rule 4) var(X, C) [-. var(X, C<-X>) (rule 1) var(X, \nC<-X>) [ (-U var(X, C)) . var(X, X) (rule 5) var(X, C) [ + . var(X, void) (rule 1) var(X, void) [* (rule \n2) var(X, C) [-. var(X, D<+X>) (rule 1) var(X, D<+X>) [ (+ U var(Y, D)) . var(X, X) (rule 5) var(Y, D) \n[ + . var(Y, void) (rule 1) var(Y, void) [* (rule 2) var(Y, D) [-. var(Y, C<oY>) (rule 1) var(Y, C<oY>) \n[ (o U var(X, C)) . var(Y, Y) (rule 5) var(Y, Y) [ + (rule 3) 4.3 Revisiting Recursive Type Variances \nArmed with our understanding of variance requirements as sym\u00adbolic constraints on a lattice, it is quite \neasy to revisit practical ex\u00adamples and understand them quickly. For instance, what we called type 2 \nrecursive variance in Section 3.3 is just an instance of a re\u00ad cursive constraint c [-. c, where c is \nsome variable of the form var(X, C). This is a case of a type that recursively (i.e., inside its own \nde.nition) occurs in a contravariant position. (Of course, the recursion will not always be that obvious: \nit may only become ap\u00adparent after other constraints are simpli.ed and merged.) It is easy to see from \nthe properties of the transform operator that the only solutions of this constraint are o and *; i.e., \ncannot be either co\u00ador contravariant as we described in Section 3.3. If c =+, then the constraint generated \nby type 2 recursive variance would be vio\u00adlated, since c =+ [-. c = -. += -. Similar reasoning shows \nc cannot be - and satisfy the constraint.  4.4 Constraint Solving Checking if a variance satis.es a \nconstraint system (i.e., the con\u00adstraints generated for a VarLang module) corresponds to checking de.nition-site \nvariance annotations in type de.nitions that can con\u00adtain use-site variance annotations. Analogously, \ninferring the most general de.nition-site variances allowed by a type de.nition cor\u00adresponds to computing \nthe most general variances that satisfy the constraint system representing the type de.nition. The trivial \nand least general solution that satis.es a constraint system is assigning the de.nition-site variance \nof all type parameters to be invariance. Assigning invariance to all type parameters is guaranteed to \nbe a so\u00adlution, since invariance is the bottom element, which must be below every upper bound imposed \nby the constraints. Stopping with this solution would not take advantage of the subtyping relationships \nal\u00adlowed by type de.nitions. Fortunately, the most general solution is always unique and can be computed \ne.ciently by .xed-point com\u00adputation running in polynomial time of the program size (number of constraints \ngenerated). The only operators in constraint systems are the binary opera\u00adtors U and .. Both of these \nare monotone, as can be seen with the variance lattice and Figure 1. Every constraint system has a unique \nmaximal solution because there is guaranteed to be at least one solution (assign every type pa\u00adrameter \ninvariance) and solutions to constraint systems are closed under point-wise U; we get a maximal solution \nby joining all of the greatest variances that satisfy each constraint. Because opera\u00adtors U and . are \nmonotone, we can compute the maximal solution e.ciently with .xed point iteration halting when the greatest \n.xed point of the equations has been reached. We demonstrate this al\u00adgorithm below by applying it to \nthe example Java classes C and D from Section 4.2.  First, because we are only interested in inferring \nde.nition-site variance, we only care about computing the most general variances for terms of the form \nvar(X, C) but not var(X, T). We can expand var(X, T) terms with their upper bounds containing only unknowns \nof the form var(X, C) Consider the constraint generated from foo s argument type: var(X, C) [-. var(X, \nC<-X>). Because we are computing a maximal solution and because of the monotonicity of U and ., we can \nreplace var(X, C<-X>) and var(X, X) by their upper bounds, rewriting the constraint as: var(X, X) var(X, \nC) [-. (-U var(X, C)) . + var(X, C<-X>) Lastly, we can ignore type expressions that do not mention a \ntype parameter because they impose no real upper bound; their upper bound is the top element (e.g. var(X, \nvoid) [*). This leads to the following constraints generated for the example two Java classes: foo return \ntype =. var(X, C) [ + . + var(X, X) foo arg type =. var(X, C) [-. (var(X, C) U-)  var(X, C<-X>) bar \narg type =. var(X, C) [-. (var(Y, D) U +)  var(X, D<+X>) baz arg type =. var(Y, D) [-. (var(X, C) U \no) var(Y, C<oY>) Letting c denote var(X, C) and d denote var(Y, D), the above constraints correspond \nto the four constraints presented in the In\u00adtroduction. For each expanded constraint r [ l in a constraint \nsystem, r is a var(X, C) term and l is an expression where the only unknowns are var(X, C) terms. The \ngreatest .xed-point of a constraint system is solved for by, .rst, assigning every var(X, C) term to \nbe * (top). Each constraint r [ l is then transformed to r . l n r, since r need not increase from the \nvalue it was lowered to by other assignments. The last step is to iterate through the assignments for \neach constraint until the var(X, C) terms no longer change, which results in computing the greatest .xed-point. \nFinally, computing the greatest .xed-point runs in at most O(2n) iterations, where n is the number of \nconstraint inequalities, since for each r . l n r, r can decrease at most 2 times to invariance (bottom) \nfrom initially being bivariance (top). 5. An Application: De.nition-Site Variance Inference for Java \nTo showcase the potential of our uni.ed treatment of use-site and de.nition-site variance, we implemented \na mapping from Java to VarLang, used it to produce a (de.nition-site) variance inference algorithm, and \nevaluated its impact on large Java libraries with generics (including the standard library). 5.1 Applications \nOur mapping from Java to VarLang is straightforward: We produce a VarLang module de.nition for each Java \nclass or interface, and all Java type expressions are mapped one-to-one on VarLang type expressions with \nthe same name. The module de.nitions contain variance constraints that correspond to the well-understood \nvari\u00adance of di.erent positions (as discussed in Section 2): return types are a covariant position, argument \ntypes are a contravariant posi\u00adtion, types of non-.nal .elds are both covariant and contravariant positions, \nsupertypes are a covariant position. Our mapping is conservative: although we handle the entire Java \nlanguage, we translate some tricky aspects of the language into an invariance constraint, potentially \nmaking our algorithm infer less general variances than is occasionally possible. For instance, we do \nnot try to infer the most general variance induced by polymorphic methods: if a class type parameter \nappears at all in the upper bound of a type parameter of a polymorphic method, we consider this to be \nan instance of an invariant position. Another source of conservatism is that we ignore the potential \nfor more general typing through reasoning about member visibility (i.e., private/protected access control). \nMember visibility, in combination with conditions on self\u00adreference in type signatures, can be used to \nestablish that some .elds or methods cannot be accessed from outside a class/package. Nevertheless, our \nmapping does not try to reason about such cases to produce less restrictive variance constraints. The \nreason for our conservatism is that we do not want to reason about language speci.c constructs that are \northogonal to the denotational meaning of variance and would make our soundness proof be of the same \ncomplexity as in e.g., TameFJ [7]. We prefer to use only positions of unquestionable variance at the \nexpense of slightly worse numbers (which still fully validate the potential of our approach). We used \nthis mapping to implement a de.nition-site variance in\u00adference algorithm. That is, we took regular Java \ncode, written with no concept of de.nition-site variance in mind, and inferred how many generics are \npurely covariant/contravariant/bivariant. Infer\u00adring pure variance for a generic has several practical \nimplications: One can use our algorithm to replace the Java type system with a more liberal one that \ninfers de.nition-site variance and allows subtyping based on the inferred variances. Such a type system \nwould accept all current legal Java programs, yet allow programs that are currently not allowed to type-check, \nwithout violating soundness. This would mean that wildcards can be omitted in many cases, freeing the \nprogrammer from the burden of always specifying tedious types in order to get generality. For instance, \nif a generic C is found to be covariant, then any occurrence of C<? extends T> is unnecessary. (We report \nsuch instances as unnecessary wildcards in our measurements.) Furthermore, any occurrence of C<T> or \nC<? super T> will be immediately considered equivalent to C<? extends T> or C<?>, respectively, by the \ntype system, resulting in more general code. (We report such instances as over-speci.ed methods in our \nmeasurements.)  One can use our algorithm as a programmer s assistant in the context of an IDE or as \nan o.-line tool, to o.er suggestions for more general types that are, however, still sound. For instance, \nfor a covariant generic, C, every occurrence of type C<T> can be replaced by C<? extends T> to gain more \ngenerality without any potential for more errors. Just running our algorithm once over a code body will \nreveal multiple points where a programmer missed an opportunity to specify a more general type. The programmer \ncan then determine whether the speci.city was intentional (e.g., in anticipation that the referenced \ngeneric will later be augmented with more methods) or accidental.  In practice, our implementation (in \nScala) of the optimized constraint solving algorithm described in Section 4.4 takes less than 3 minutes \n(on a 3.2GHz Intel Core i3 machine w/ 4GB RAM) to analyze the generics of the entire Java standard library. \nAlmost all  Library # Type defs # Generic defs invar. Type De.nitions variant cov. contrav. biv. Recursive \nvariances Unnecess. wildcards Over-specif. methods java.* classes interfaces total 5550 1710 7260 99 \n44 143 69% 43% 61% 31% 57% 39% 20% 41% 27% 17% 25% 20% 4% 0% 3% 10% 22% 14% 12% 12% 12% 7% 16% 8% JScience \nclasses interfaces total 70 51 121 25 11 36 76% 55% 69% 24% 45% 31% 0% 0% 0% 0% 9% 3% 24% 36% 28% 76% \n7% 53% 90% 100% 91% 19% 11% 19% Apache Collec. classes interfaces total 226 23 249 187 22 209 66% 55% \n65% 34% 45% 35% 14% 32% 16% 21% 18% 21% 2% 0% 1% 4% 24% 6% 63% 20% 62% 17% 0% 17% Guava classes interfaces \ntotal 204 35 239 101 26 127 90% 46% 81% 10% 54% 19% 9% 38% 15% 1% 19% 5% 0% 4% 1% 8% 18% 10% 51% 38% \n50% 18% 5% 17% GNU Trove classes interfaces total 25 8 33 8 4 12 62% 0% 42% 38% 100% 58% 12% 25% 17% \n25% 100% 50% 0% 0% 0% 33% 0% 20% 0% 0% 0% 62% 0% 62% JPaul classes interfaces total 77 9 86 65 9 74 75% \n67% 74% 25% 33% 26% 12% 11% 12% 12% 33% 15% 5% 0% 4% 18% 9% 17% 86% 0% 86% 23% 0% 23% Total classes interfaces \ntotal 6152 1836 7988 485 116 601 73% 47% 68% 27% 53% 32% 13% 32% 17% 14% 24% 16% 3% 4% 3% 11% 18% 13% \n42% 19% 39% 13% 15% 13% Figure 4. Class/Interface Inference Results Library # Type Parameters inv. Type \nParameters variant total cov. contrav. biv. java.* classes interfaces total 128 54 182 67% 46% 61% 33% \n54% 39% 16% 33% 21% 14% 20% 16% 3% 0% 2% JScience classes interfaces total 29 14 43 79% 64% 74% 21% 36% \n26% 0% 0% 0% 0% 7% 2% 21% 29% 23% Apache Collec. classes interfaces total 254 33 287 72% 64% 71% 28% \n36% 29% 11% 24% 13% 16% 12% 15% 1% 0% 1% Guava classes interfaces total 150 39 189 93% 54% 85% 7% 46% \n15% 7% 26% 11% 1% 18% 4% 0% 3% 1% GNU Trove classes interfaces total 9 6 15 67% 0% 40% 33% 100% 60% 11% \n17% 13% 22% 83% 47% 0% 0% 0% JPaul classes interfaces total 93 11 104 76% 64% 75% 24% 36% 25% 10% 9% \n10% 9% 27% 11% 5% 0% 5% Total classes interfaces total 663 157 820 77% 53% 72% 23% 47% 28% 10% 24% 13% \n10% 20% 12% 3% 3% 3% Figure 5. Type Parameter Inference Results of the time is spent on loading, parsing, \nand processing .les, with under 30sec constraint solving time. Finally, we need to emphasize that our \ninference algorithm is modular. Not only does it reason entirely at the interface level (does not inspect \nmethod bodies), but also the variance of a generic de\u00adpends only on its own de.nition and the de.nition \nof types it (tran\u00adsitively) references, and not on types that reference it. This is the same modularity \nguarantee as with standard separate compilation. If we were to allow our algorithm to generate constraints \nafter in\u00adspecting method bodies, we would get improved numbers (since, for instance, an invariant type \nmay be passed as a parameter, but only its covariance-safe methods may be used e.g., a list argu\u00adment \nmay only be used for reading). Nevertheless, analyzing the bodies of methods would have a cost in modularity: \nthe analysis would still not depend on clients of a method, but it would need to examine subtypes, to \nanalyze all the possible overriding methods. This is yet another way in which our numbers are conservative \nand only compute a lower bound of the possible impact of integrating de.nition-site variance inference \nin Java.  5.2 Analysis of Impact To measure the impact of our approach, we ran our inference al\u00adgorithm \nover 6 Java libraries, the largest of which is the core Java library from Sun s JDK 1.6, i.e., classes \nand interfaces in the pack\u00adages of java.*. The other libraries are JScience [10], a Java library for \nscienti.c computing; Guava [5], a superset of the Google collec\u00ad tions library; GNU Trove [14]; Apache \nCommons-Collection [3]; and JPaul [21], a library supporting program analysis. The results of our experiment \nappear in Figure 4. Together, these libraries de.ne 7,988 classes and interfaces, out of which 601 are \ngenerics. The .ve invar./variant/cov./contrav./biv/ columns show the percentage of classes and interfaces \nthat are inferred by our algorithm to be invariant versus variant, for all three .avors of variance. \nThe statistics are collapsed per-class: An invariant class is invariant in all of its type parameters, \nwhereas a variant class is variant in at least one of its type parameters. Hence, a class can be counted \nas, e.g., both covariant and contravariant, if it is covariant in one type parameter and contravariant \nin another. The variant column, however, counts the class only once. Statistics per type parameter are \nin included in Figure 5. As can be seen, 32% of classes or interfaces are variant in at least one type \nparameter. (Our Total row treats all libraries as if they were one, i.e., sums individual numbers before \naveraging. This means that the Total is in.uenced more by larger libraries, especially for metrics that \napply to all uses of generics, which may also occur in non-generic code.) This means that about 1/3 of \nthe generics de.ned should be allowed to enjoy general variant subtyping without users having to annotate \nthem with wildcards. The next column shows how many generics have a recursive variance constraint. One \ncan see that these numbers are usually low, especially considering that they include direct self-recursion \n(e.g., a trivial constraint var(X, C) [ + . var(X, C)). The last two columns illustrate the burden of \ndefault invariant subtyping in Java, and the bene.ts of our approach. Unnecessary Wildcards shows the \npercentage of wildcards in method signa\u00adtures that are unnecessary in our system, based on the inferred \nde.nition-site variance their generics. For instance, given that our technique infers interface java.util.Iterator<E> \nto be covariant, all instantiations of Iterator<? extends T>, for any T, are unnec\u00adessary. This number \nshows that, using our technique, 37% of the current wildcard annotations can be eliminated without sacri.cing \neither type safety or the generality of types! The Over-speci.ed Method column lists the percentage of \nmethod arguments that are overly speci.c in the Java type sys\u00adtem, based on the inferred de.nition-site \nvariance of their gener\u00adics. For instance, given that the inferred de.nition-site variance of Iterator<E> \nis covariant, specifying a method argument with type Iterator<T>, instead of Iterator<? extends T>, is \noverly spe\u00adci.c, since the Java type system would preclude safe invocations of this method with arguments \nof type Iterator-of-some-subtype\u00adof-T. Note again that this percentage is purely based on the inferred \nde.nition-site variance of the arguments types, not on analysis of the arguments uses in the bodies of \nmethods. We .nd that 13% of methods are over-speci.ed. This means that 13% of the methods could be used \nin a much more liberal, yet still type-safe fashion. It is also interesting that this number is derived \nfrom libraries and not from client code. We expect that the number of over-speci.ed methods would be \nmuch higher in client code, since programmers would be less familiar with wildcards and less con.dent \nabout the operations supported by variant versions of a type. Backward Compatibility and Discussion. \nAs discussed earlier, our variance inference algorithm can be used to replace the Java type system with \na more liberal one, or can be used to o.er sugges\u00adtions to programmers in the context of an IDE. Replacing \nthe Java type system with a type system that infers de.nition-site variance is tempting, but would require \na pragmatic language design decision, since there is a cost in backward compatibility: in some cases \nthe programmer may have relied on types being rejected by Java, even though these types can never cause \na dynamic type error. We found one such instance in our experiments. In the reference implementation \nfor JSR 275 (Measures and Units) [11], included with the JScience library [10], a group of 11 classes \nand interfaces are collectively bivariant in a type parameter, Q extends Quantity. In the de.nition of \nUnit<Q extends Quantity>, for example, the type parameter Q appears nowhere other than as the type argument \nto Unit<Q>. Closer inspection of the code shows that Quantity is extended by 43 di.erent subinterfaces, \nsuch as Acceleration, Mass, Torque, Volume, etc. It appears that the authors of the library are actually \nrelying on the invariant subtyping of Java generics, to ensure, e.g., that Unit<Acceleration> is never \nused as Unit<Mass>. Of course, full variance inference is only one option in the design space. Any combination \nof inference and explicitly stated variance annotations, or just adding explicit de.nition-site variance \nto Java, are strictly easier applications from a typing standpoint. The ultimate choice is left with \nthe language designer, yet the potential revealed by our experiments is signi.cant. 6. Conclusions In \nthis work we showed that the need for the .exibility of use-site variance is somewhat overrated, and \nthe rigidity of de.nition-site variance is unwarranted. We introduce the .rst uni.ed framework for reasoning \nabout variance, allowing type system designs that combine the bene.ts of both de.nition-and use-site \nvariance. Thus, our approach resolves questions that are central in the design of any language involving \nparametric polymorphism and subtyping. Our work is the .rst to fully study how de.nition-site variance \ninteracts with use-site variance annotations and type recursion. Our calcu\u00adlus allows to reason about \ncomplex constraints independently of speci.c type systems on variances involved in recursive class def\u00adinitions. \nAs a speci.c application, we introduce a type system for Java that modularly infers de.nition-site variance, \nwhile allowing use-site variance annotations where necessary. We show that com\u00adbining de.nition-site \nand use-site variance allows us to infer more general types than either approach alone. Our Java implementation \ndemonstrates the bene.ts and practicality of our approach, and our algorithm can also be used in other \ncontexts, such as to aid a pro\u00adgrammer via an IDE plugin to choose types that generalize method signatures \nas much as possible. Acknowledgments We would like to thank Jens Palsberg (especially for valuable in\u00adsights \non the constraint solving algorithm) as well as Christoph Reichenbach and several anonymous reviewers \nfor their sugges\u00adtions that helped improve the paper. This work was funded by the National Science Foundation \nunder grants CCF-0917774, CCF\u00ad0934631, and IIP-0838747 to the University of Massachusetts. References \n[1] J. Altidor, S. S. Huang, and Y. Smaragdakis. Taming the wildcards: Combining de.nition-and use-site \nvariance (ex\u00adtended version). http://www.cs.umass.edu/ yannis/ variance-extended2011.pdf. [2] P. America \nand F. van der Linden. A parallel object-oriented language with inheritance and subtyping. In OOPSLA/ECOOP \n90: Proc. of the European Conf. on object-oriented programming on Object-oriented programming systems, \nlanguages, and applications, 1990. [3] Apache Software Foundation. Apache commons-collections library. \nhttp://larvalabs.com/collections/. Version 4.01.  [4] J. Bloch. The closures controversy. http://www.javac.info/ \nbloch-closures-controversy.ppt. Accessed Nov. 2010. [5] K. Boumillion and J. Levy. Guava: Google core \nlibraries for Java 1.5+. http://code.google.com/p/guava-libraries/. Ac\u00adcessed Nov. 2010. [6] G. Bracha \nand D. Griswold. Strongtalk: typechecking smalltalk in a production environment. In OOPSLA 93: Proc. \nof the Conf. on Object-oriented programming systems, languages, and applications, 1993. [7] N. Cameron, \nS. Drossopoulou, and E. Ernst. A model for Java with wildcards. In ECOOP 08: Proc. of the 22nd European \nConf. on Object-Oriented Programming, 2008. [8] R. Cartwright and J. Guy L. Steele. Compatible genericity \nwith run\u00adtime types for the Java programming language. In OOPSLA 98: Proc. of the 13th ACM SIGPLAN Conf. \non Object-oriented programming, systems, languages, and applications, 1998. [9] W. Cook. A proposal for \nmaking ei.el type-safe. In ECOOP 89: Proc. of the 3rd European Conf. on Object-Oriented Programming, \n1989. [10] J.-M. Dautelle et al. Jscience. http://jscience.org/. Accessed Nov. 2010. [11] J.-M. Dautelle \nand W. Keil. Jsr-275: Measures and units. http: //www.jcp.org/en/jsr/detail?id=275. Accessed Nov. 2010. \n[12] M. Day, R. Gruber, B. Liskov, and A. C. Myers. Subtypes vs. where clauses: constraining parametric \npolymorphism. In OOPSLA 95: Proc. of the tenth annual Conf. on Object-oriented programming sys\u00adtems, \nlanguages, and applications, 1995. [13] B. Emir, A. Kennedy, C. Russo, and D. Yu. Variance and generalized \nconstraints for C# generics. In ECOOP 06: Proc. of the European Conf. on Object-Oriented Programming, \n2006. [14] E. Friedman and R. Eden. Gnu Trove: High-performance collections library for Java. http://trove4j.sourceforge.net/. \nVersion 2.1.0. [15] A. Hejlsberg, S. Wiltamuth, and P. Golde. C# Language Speci.cation. Addison-Wesley \nLongman Publishing Co., Inc., Boston, MA, 2003. [16] S. S. Huang, D. Zook, and Y. Smaragdakis. cJ: Enhancing \nJava with safe type conditions. In Proc. of the 6th Intl. Conf. on Aspect-Oriented Software Development, \n2007. [17] A. Igarashi and M. Viroli. Variant parametric types: A .exible subtyping scheme for generics. \nACM Trans. Program. Lang. Syst., 28(5):795 847, 2006. [18] A. Kennedy and D. Syme. Transposing f to c#: \nexpressivity of para\u00admetric polymorphism in an object-oriented language: Research arti\u00adcles. Concurr. \nComput. : Pract. Exper., 16:707 733, 2004. [19] A. Kiezun, M. Ernst, F. Tip, and R. Fuhrer. Refactoring \nfor param\u00adeterizing Java classes. In ICSE 07: Proc. of the 29th Intl. Conf. on Software Engineering, \n2007. [20] M. Odersky. The Scala Language Speci.cation v 2.8. 2010. [21] A. Salcianu. Java program analysis \nutilities library. http://jpaul. sourceforge.net/. Version 2.5.1. [22] K. K. Thorup and M. Torgersen. \nUnifying genericity: Combining the bene.ts of virtual types and parameterized classes. In ECOOP 99: Proc. \nof the European Conf. on Object-Oriented Programming, 1999. [23] M. Torgersen, C. P. Hansen, E. Ernst, \nP. von der Ahe, G. Bracha, and N. Gafter. Adding wildcards to the Java programming language. In SAC 04: \nProc. of the 2004 Symposium on Applied Computing, 2004.    \n\t\t\t", "proc_id": "1993498", "abstract": "<p>Variance allows the safe integration of parametric and subtype polymorphism. Two flavors of variance, definition-site versus use-site variance, have been studied and have had their merits hotly debated. Definition-site variance (as in Scala and C#) offers simple type-instantiation rules, but causes fractured definitions of naturally invariant classes; Use-site variance (as in Java) offers simplicity in class definitions, yet complex type-instantiation rules that elude most programmers.</p> <p>We present a unifying framework for reasoning about variance. Our framework is quite simple and entirely denotational, that is, it evokes directly the definition of variance with a small core calculus that does not depend on specific type systems. This general framework can have multiple applications to combine the best of both worlds: for instance, it can be used to add use-site variance annotations to the Scala type system. We show one such application in detail: we extend the Java type system with a mechanism that modularly infers the definition-site variance of type parameters, while allowing use-site variance annotations on any type-instantiation.</p> <p>Applying our technique to six Java generic libraries (including the Java core library) shows that 20-58 (depending on the library) of generic definitions are inferred to have single-variance; 8-63% of method signatures can be relaxed through this inference, and up to 91% of existing wildcard annotations are unnecessary and can be elided.</p>", "authors": [{"name": "John Altidor", "author_profile_id": "81548008760", "affiliation": "University of Massachusetts, Amherst, Amherst, MA, USA", "person_id": "P2690664", "email_address": "jaltidor@cs.umass.edu", "orcid_id": ""}, {"name": "Shan Shan Huang", "author_profile_id": "81100080995", "affiliation": "LogicBlox Inc., Atlanta, GA, USA", "person_id": "P2690665", "email_address": "ssh@logicblox.com", "orcid_id": ""}, {"name": "Yannis Smaragdakis", "author_profile_id": "81100614708", "affiliation": "University of Massachusetts, Amherst, Amherst, MA, USA", "person_id": "P2690666", "email_address": "yannis@cs.umass.edu", "orcid_id": ""}], "doi_number": "10.1145/1993498.1993569", "year": "2011", "article_id": "1993569", "conference": "PLDI", "title": "Taming the wildcards: combining definition- and use-site variance", "url": "http://dl.acm.org/citation.cfm?id=1993569"}