{"article_publication_date": "06-04-2011", "fulltext": "\n Data Representation Synthesis * Peter Hawkins Alex Aiken Kathleen Fisher Computer Science Department, \nStanford Computer Science Department, Stanford Computer Science Department, Tufts University University \nUniversity hawkinsp@cs.stanford.edu aiken@cs.stanford.edu k.sher@eecs.tufts.edu Martin Rinard MIT Computer \nScience and Arti.cial Intelligence Laboratory rinard@csail.mit.edu Abstract We consider the problem \nof specifying combinations of data struc\u00adtures with complex sharing in a manner that is both declarative \nand results in provably correct code. In our approach, abstract data types are speci.ed using relational \nalgebra and functional depen\u00addencies. We describe a language of decompositions that permit the user to \nspecify different concrete representations for relations, and show that operations on concrete representations \nsoundly imple\u00adment their relational speci.cation. It is easy to incorporate data representations synthesized \nby our compiler into existing systems, leading to code that is simpler, correct by construction, and \ncompa\u00adrable in performance to the code it replaces. Categories and Subject Descriptors D.3.3 [Programming \nLan\u00adguages]: Language Constructs and Features Abstract data types, Data types and structures; E.2 [Data \nStorage Representations] General Terms Languages, Design, Algorithms, Performance, Veri.cation Keywords \nSynthesis, Composite Data Structures 1. Introduction One of the .rst things a programmer must do when \nimplementing a system is commit to particular choices of data structures. For example, consider a simple \noperating system process scheduler. Each process has an ID pid,a state (running or sleeping), and a variety \nof statistics such as the cpu time consumed. Since we need to .nd and update processes by ID, we store \nprocesses in a hash table indexed by pid; as we also need to enumerate processes in each state, we simultaneously \nmaintain a linked list of running processes and a separate list of sleeping processes. * This work was \nsupported by NSF grants CCF-0702681 and CNS-050955. Kathleen was at AT&#38;T Labs Research when this \nresearch was done. Permission to make digital or hard copies of all or part of this work for personal \nor classroom use is granted without fee provided that copies are not made or distributed for pro.t or \ncommercial advantage and that copies bear this notice and the full citation on the .rst page. To copy \notherwise, to republish, to post on servers or to redistribute to lists, requires prior speci.c permission \nand/or a fee. PLDI 11, June 4 8, 2011, San Jose, California, USA. Copyright c &#38;#169; 2011 ACM 978-1-4503-0663-8/11/06. \n. . $10.00 Mooly Sagiv Tel-Aviv University msagiv@post.tau.ac.li ns, pid, state, cpu ns, pid . state, \ncpu class process_relation { void insert(...); bool query(...); void update(...); void remove(...); \n }; Low-Level Implementation Figure 1. Schematic of Data Representation Synthesis. Whatever our choice \nof data structures, it has a pervasive in.u\u00adence on the subsequent code, and as requirements evolve it \nis dif.\u00adcult and tedious to change the data structures to match. For exam\u00adple, suppose we add virtualization \nsupport by allowing processes with the same pid number to exist in different namespaces ns, to\u00adgether \nwith the ability to enumerate processes in a namespace. Ex\u00adtending the existing data structures to support \nthe new requirement may require many changes throughout the code. Furthermore, invariants on multiple, \noverlapping data structures that represent different views of the same data are hard to state, dif.cult \nto enforce, and easy to get wrong. For the scheduler, we require that each process appears in both the \nhash-table indexed by process ID and exactly one of the running or sleeping lists. Such invariants must \nbe enforced by every piece of code that manipulates the scheduler s data structures. It is easy to forget \na case, say by failing to add a process to the appropriate list when it changes state or by failing to \ndelete a hash table entry when a process terminates. Invariants of this nature require deep knowledge \nabout the heap s structure, and are dif.cult to enforce through existing static analysis or veri.cation \ntechniques. We propose a new method termed data representation synthesis, depicted in Figure 1. In our \napproach, a data structure client writes code that describes and manipulates data at a high-level as \nrela\u00adtions; a data structure designer then provides decompositions which describe how those relations \nshould be represented in memory as a combination of primitive data structures. Our compiler RELC takes \na relation and its decomposition and synthesizes ef.cient and cor\u00adrect low-level code that implements \nthe relational interface. Synthesis allows programmers to describe and manipulate data at a high level \nas relations, while giving control of how relations are represented physically in memory. By abstracting \ndata from its representation, programmers no longer prematurely commit to a particular representation \nof data. If programmers want to change or extend their choice of data structures, they need only change \nthe de\u00adcomposition; the code that uses the relation need not change at all. Synthesized representations \nare correct by construction; so long as the programmer conforms to the relational speci.cation, invariants \non the synthesized data structures are automatically maintained.  We build on our previous work [12], \nwhich introduced the idea of synthesizing shared low-level data structures from a high-level relational \ndescription. Our theoretical framework is substantially simpler and more .exible. In particular, we can \nhandle destructive updates to relations while still preserving all relation invariants. We have also \nimplemented a compiler for relational speci.cations and an autotuner that .nds the best decomposition \nfor a relation auto\u00admatically. Finally, we have integrated synthesized data representa\u00adtions into existing \nC++ code as a proof of concept. Each section of this paper highlights a contribution of our work: We \ndescribe a new scheme for synthesizing ef.cient low-level data representations from abstract relational \ndescriptions of data (Section 2). We describe a relational interface that abstracts data from its concrete \nrepresentation.  The decomposition language (Section 3) speci.es how rela\u00ad tions should be mapped to \nlow-level physical implementations, which are assembled from a library of primitive data struc\u00adtures. \nThe decomposition language provides a new way to spec\u00adify high-level heap invariants that are dif.cult \nor impossible to express using standard data abstraction or heap-analysis tech\u00adniques. We describe adequacy \nconditions that ensure a decom\u00adposition faithfully represents a relation.  We synthesize ef.cient implementations \nof queries and updates to relations, tailored to the speci.ed decomposition (Section 4). Key to our approach \nis a query planner that chooses an ef.cient strategy for each query or update. We show queries and updates \nare sound, that is, each query or update implementation faith\u00adfully implements its relational speci.cation. \n A programmer may not know the best decomposition for a particular relation. We describe an autotuner \n(Section 5), which given a relational speci.cation and a performance metric .nds the best decomposition \nup to a user-speci.ed bound.  The compiler RELC (Section 6) takes as input a relation and its decomposition, \nand generates C++ code implementing the re\u00adlation, which is easily incorporated into existing systems. \nWe show different decompositions lead to very different perfor\u00admance characteristics. We incorporate \nsynthesis into three real systems, namely a web server, a network accounting daemon and a map viewer, \nin each case leading to code that is simpler, correct by construction, and comparable in performance. \n 2. Relational Abstraction We .rst introduce the relation abstraction via which data structure clients \nmanipulate synthesized data representations. Representing and manipulating data as relations is familiar \nfrom databases, and our interface is largely standard. We use relations to abstract a pro\u00adgram s data \nfrom its representation. Describing particular represen\u00adtations is the task of the decomposition language \nof Section 3. A relational speci.cation is a set of column names C and func\u00adtional dependencies .. In \nthe scheduler example from Section 1 a natural way to model the processes is as a relation with columns \n{ns, pid, state, cpu}, where the values of state are drawn from the set {S, R}, representing sleeping \nand running processes re\u00adspectively, and the other columns have integer values. Not every relation represents \na valid set of processes; all meaningful sets of processes satisfy a functional dependency ns, pid . \nstate, cpu, which allows at most one state or cpu value for any given process. To formally de.ne relational \nspeci.cations, we need to .x notation for values, tuples, and relations: Values, Tuples, Relations We \nassume a set of untyped values v drawn from a universe V that includes the integers (Z . V). A tuple \nt = (c1: v1,c2: v2,... ) maps a set of columns {c1,c2,... }to values drawn from V. We write dom t for \nthe columns of t.A tuple t is a valuation for a set of columns C if dom t = C.A relation r is a set of \ntuples {t1,t2,... } over identical columns C. We write t(c) for the value of column c in tuple t. We \nwrite t . s if the tuple t extends tuple s, that is t(c)= s(c) for all c in dom s. We say tuple t matches \ntuple s, written t ~ s, if the tuples are equal on all common columns. Tuple t matches a relation r, \nwritten t ~ r, if t matches every tuple in r. We write s < t for the merge of tuples s and t, taking \nvalues from t wherever the two disagree on a column s value. For example, the scheduler might represent \nthree processes as the relation: rs = {(ns:1, pid:1, state: S, cpu:7) , (ns:1, pid:2, state: R, cpu:4) \n, (1) (ns:2, pid:1, state: S, cpu:5)} Functional Dependencies A relation r has a functional depen\u00addency \n(FD) C1 . C2 if any pair of tuples in r that are equal on columns C1 are also equal on columns C2. We \nwrite r |= fd . if the set of FDs . hold on relation r. If a FD C1 . C2 is a conse\u00adquence of FDs . we \nwrite . hfd C1 . C2. Sound and complete inference rules for functional dependencies are well-known. Relational \nAlgebra We use the standard notation of relational al\u00adgebra. Union (.), intersection (n), set difference \n(\\), and symmet\u00adric difference (8) have their usual meanings. The operator pC r projects relation r onto \na set of columns C, and r1 W r2 is the natural join of relation r1 and relation r2. Relational Operations \nWe provide .ve operations for creating and manipulating relations. Here we represent relations as ML-like \nreferences to a set of tuples; ref x denotes creating a new reference to x, !r fetches the current value \nof r and r . v sets the current value of r to v: empty () = ref \u00d8 insert rt = r . !r .{t} remove rs = \nr . !r \\{t . !r | t . s} update rsu = r .{if t . s then t < u else t | t . !r} query rsC = pC {t . !r \n| t . s} Informally, empty () creates a new empty relation. The opera\u00adtion insert rt inserts tuple t \ninto relation r, remove rs removes tuples matching tuple s from relation r, and update rsu applies the \nupdates in tuple u to each tuple matching s in relation r. Finally query rsC returns the columns C of \nall tuples in r matching tuple s. The tuples s and u given as arguments to the remove, update and query \noperations may be partial tuples, that is, they need not contain every column of relation r. Extending \nthe query operator to handle comparisons other than equality or to support ordering is straightforward; \nhowever, for clarity of exposition we restrict our\u00adselves to queries based on equalities. For the scheduler \nexample, we call empty () to obtain an empty relation r. To insert a new running process into r, we invoke: \ninsert r (ns:7, pid: 42, state: R, cpu:0) The operation query r (state: R){ns, pid} returns the namespace \nand ID of each running process in r, whereas query r (ns:7, pid: 42){state, cpu}  (a) (b)  ns state \npid ns, pid pid ns, pid 11, 2 cpu cpu:7 cpu:4 cpu:5 Figure 2. Data representation for a process scheduler: \n(a) a de\u00adcomposition, (b) an instance of that decomposition. Solid edges represent hash tables, dotted \nedges represent vectors, and dashed edges represent doubly-linked lists. returns the state and cpu of \nprocess 42 in namespace 7. By invoking update r (ns:7, pid: 42)(state: S) we can mark process 42 as sleeping, \nand .nally by calling remove r (ns:7, pid: 42) we can remove the process from the relation. The RELC \ncompiler emits C++ classes that implement the rela\u00adtional interface, which client code can then call. \nFor the scheduler relation example the compiler generates the class: class scheduler_relation { void \ninsert(tuple_cpu_ns_pid_state const &#38;r); void remove(tuple_ns_pid const &#38;pattern); void update(tuple_ns_pid \nconst &#38;pattern, tuple_cpu_state const &#38;changes); void query(tuple_state const &#38;input, iterator_state__ns_pid \n&#38;output); ... }; Each method of the class instantiates a relational operation. We could generate \ninstantiations of each operation for all possible kinds of tuples passed as arguments, however in practice \nwe allow the programmer to specify the needed instantiations. 3. Decompositions and Decomposition Instances \nDecompositions describe how to represent relations as a combi\u00adnation of primitive data structures. Our \ngoal is to prove that the low-level representation of a relation faithfully implements its high\u00adlevel \nspeci.cation. In this section, we develop the technical ma\u00adchinery to reason about the correspondence \nbetween relations and their decompositions. A decomposition is a static description of the structure \nof data, akin to a type. Its run-time (dynamic) counterpart is the decomposi\u00adtion instance, which describes \nthe representation of a particular re\u00adlation using the decomposition. We de.ne an abstraction function \nthat computes the relation represented by a given decomposition instance, and well-formedness criteria \nthat check that a decompo\u00adsition instance is a well-formed instance of a particular decompo\u00adsition. Finally, \nwe de.ne adequacy conditions which are suf.cient conditions for a decomposition to faithfully represent \na relation. 3.1 Decompositions A decomposition is a rooted, directed acyclic graph that describes how \nto represent a relational speci.cation. The subgraph rooted at each node of the decomposition describes \nhow to represent part of . p ::= C | C -. v | p 1 Wp 2 decomposition primitives d ::= let v: C1 WC2 = \np in d | v decompositions . ::= dlist | htable | vector | \u00b7\u00b7\u00b7 data structures Figure 3. The decomposition \nlanguage. p ::= t |{t . vte ,... }| p1 Wp2 instance primitives d ::= let {vt = p, . . . } in d | vo) \ninstances Figure 4. Decomposition instances. the original relation; each edge of the decomposition describes \na way of breaking up a relation into a set of smaller relations. We use the scheduler example to explain \nthe features of the de\u00adcomposition language. Figure 2(a) shows one possible decomposi\u00ad tion for the scheduler \nrelation. Informally, this decomposition reads as follows. From the root (node x), we can follow the \nleft-hand edge, which uses a hash table to map each value n of the ns .eld to a sub-relation (node y) \nwith the {pid, cpu} values for n. From one such sub-relation, the outgoing edge of node y maps a pid \n(using another hashtable) to a sub-relation consisting of a single tuple with one column, the corresponding \ncpu time. The state .eld is not rep\u00adresented on the left-hand path. Alternatively, from the root we can \nfollow the right-hand edge, which maps a process state (running or sleeping) to a sub-relation of the \n{ns, pid, cpu} values of the processes in that state. Each such sub-relation (rooted at node z) maps \na {ns, pid} pair to the corresponding cpu time. While the left path from x to w is implemented using \na hash table of hash tables, the right path is a vector with two entries, one pointing to a list of running \nprocesses, the other to a list of sleeping processes. Because node w is shared, there is only one physical \ncopy of each cpu value, shared by the two access paths. A decomposition instance, or instance for short, \nis a rooted, di\u00adrected acyclic graph representing a particular relation. Each node of a decomposition \ncorresponds to a set of nodes in an instance of that decomposition. Figure 2(b) shows an instance of \nthe decom\u00ad position representing the relation rs de.ned in Equation (1). The structure of an instance \ncorresponds to a low-level memory state; nodes are objects in memory and edges are data structures navi\u00adgating \nbetween objects. Note, for example, node zostate: S) has two outgoing edges, one for each sleeping process; \nthe dashed edge in\u00addicates that the collection of sleeping processes is implemented as a doubly-linked \nlist. To reason formally about decompositions and decomposition instances we encode graphs in a let-binding \nnotation, using the language shown in Figure 3 for decompositions and Figure 4 for instances. We stress \nthat this notation is isomorphic to the graph notation and only exists to aid formal reasoning. The decomposition \nof Figure 2(a) written in let-notation is: let w: {ns, pid, state} W {cpu} = {cpu} in htable let y: {ns} \nW {pid, cpu} = {pid} ---. w in dlist let z: {state} W {ns, pid, cpu} = {ns, pid} --. w in (2) let x: \n\u00d8 W {ns, pid, cpu, state} = htablevector ({ns} ---. y) W ({state} ---. z) in x In a decomposition a let-binding \nlet v: BWC = p in d allows us to share instances of the sub-relation v with decomposition p between multiple \nparts of a decomposition d . Let-bound variables must be distinct (to avoid name con.icts) and in let \nv: BWC = p in d , variable v must appear in d (to ensure the decomposition graph is connected). Each \ndecomposition variable is annotated with a type, consisting of a pair of column sets BWC; every instance \nof variable v in a decomposition instance has a distinct valuation of columns B, and each such instance \nrepresents a relation with columns C. Figure 2(b) written in the let-notation of instances is:  letwons:1,pid:1,state: \nS) = (cpu:7) , wons:1,pid:2,state: R) = (cpu:4) , wons:2,pid:1,state: S) = (cpu:5)in letyons:1) = {(pid:1) \n. wons:1,pid:1,state: S), (pid:2) . wons:1,pid:2,state: R)}, yons:2) = {(pid:1) . wons:2,pid:1,state: \nS)}in letzostate: S) = {(ns:1, pid:1) . wons:1,pid:1,state: S), (ns:2, pid:1) . wons:2,pid:1,state: \nS)}, zostate: R) = {(ns:1, pid:2) . wons:1,pid:2,state: R)}in letxo) = {(ns:1) . yons:1), (ns:2) . \nyons:2)} W {(state: S) . zostate: S), (state: R) . zostate: R)}in xo) Each let-binding in the instance \nparallels a binding of v: BWC in the decomposition; the instance binds a set of variable instances {vt,vte \n,... }, each for different valuations of columns B. For ex\u00adample, decomposition node z has two different \ninstances zostate: S)and zostate: R), one for each state value in the relation. We now describe the three \ndecomposition primitives and their corresponding decomposition instance primitives. A unit C represents \na single tuple t with columns C. Unit decompositions in diagrams are nodes labeled with columns C. For \nexample, in Figure 2(a) node w is a unit decomposition containing a single cpu value. . A map C -. v \nrepresents a relation as a mapping {t . vte ,... } from a set of columns C, called key columns, to a \nset of residual relations rte , one for each valuation t of the key columns. Each residual relation rte \nis in turn represented by the decomposition v. The data structure used to implement the map is ., which \ncan be any data structure that implements a key\u00advalue associative map interface. In the example . is \none of dlist (an unordered doubly-linked list of key-value pairs), htable (a hash table), or vector (an \narray mapping keys to values). The set of data structures is extensible; any data structure implementing \na common interface may be used. The choice of . only affects the computational complexity of operations \non a data structure; where the complexity is irrelevant we omit . and simply write C . v. In diagrams \nwe depict map decompositions as edges labeled with the set of columns C. For example, in Figure 2(a) \nthe edge from y to w labeled pid indicates that for each instance of vertex y in a decomposition instance \nthere is a data structure that maps each value of pid to a different residual relation, represented using \nthe decomposition rooted at w.  A join p 1 Wp 2 represents a relation as the natural join of two different \nsub-relations r1 and r2, where p 1 describes how to decompose r1 and p 2 describes how to decompose r2. \nIn diagrams, join decompositions exist wherever multiple map edges exit the same node. For example, in \nFigure 2(a) node x has two outgoing map edges and hence is the join of two map decompositions.   3.2 \nAbstraction Function The abstraction functions a(d, G) and a(p, G) map instances d and instance primitives \np, respectively, to the relation they represent. Argument G is an environment that maps instance variables \nto (WFUNIT) .t . T. dom t = C dom t = Ct ~ a(vte , G) G,vte |=G,v (WFMAP) G,t |=G,C G, {t . vte }t.T \n|=G,C . v G,p1 |=G,p 1 G,p2 |=G,p 2 r1 = a(p1, G) r2 = a(p2, G) (WFVAR) pdom r2 r1 = pdom r1 r2 G, G(vt) \n|=G, G(v) (WFJOIN) G,p1 Wp2 |=G,p 1 Wp 2 G,vt |=G,v .t . T. dom t = B G .{vt . pt}t.T ,d |=G .{v . p \n},d (WFLET) G, let {vt = pt}t.T in d |=G, let v: BWC = p in d Figure 5. Well-formed instances: G,d \n|=G,d and G,p |=G,p de.nitions. We write \u00b7 to denote the initial empty environment. a(t, G) = {t}  a({t \n. vte }t.T , G) = {t} Wa(vte , G) t.T a(p1 Wp2, G) = a(p1, G) Wa(p2, G) a(let {vt = pt}t.T in d, G) = \na(d, G .{vt . pt | t . T }) a(vt, G) = {a (G(vt), G)} 3.3 Well-formed Decomposition Instances Next we \nintroduce a well-formedness invariant ensuring that the structure of an instance d corresponds to that \nof a decomposition d . We say that a decomposition instance d is a well-formed instance of a decomposition \nd if \u00b7,d |= \u00b7,d follows from the rules given in Figure 5. The .rst argument to the judgment is an environment \nG mapping instance variables to de.nitions; similarly the third argument G is an environment mapping \ndecomposition variables to de.nitions. Rule (WFUNIT) checks that a unit node is a tuple with the correct \ncolumns. Rule (WFMAP) checks that each key tuple t has the correct columns, that t matches all tuples \nin the associated residual relation, and that variable instance vte is well-formed. Rule (WFJOIN) checks \nthat we do not have dangling tuples on one side of a join without a matching tuple on the other side. \nRule (WFLET) introduces variables into environments G and G and checks variable instantiations have the \ncorrect columns. Finally rule (WFVAR) checks the de.nition of a variable is well-formed.  3.4 Adequacy \nof Decompositions Not every relation can be represented by every decomposition. In general a decomposition \ncan only represent relations with speci.c columns satisfying certain functional dependencies. For example \nthe decomposition d in Figure 2(a) cannot represent the relation r' = {(ns:1, pid:2, state: S, cpu: 42) \n, (ns:1, pid:2, state: R, cpu: 34)}, since for each pair of ns and pid values the decomposition d can \nonly represent a single value for the state and cpu .elds. However r' does not correspond to a meaningful \nset of processes the rela\u00adtional speci.cation in Section 2 requires that all well-formed sets of processes \nsatisfy the functional dependency ns, pid . state, cpu, which allows at most one state or cpu value for \nany given process. We say that a decomposition d is adequate for relations with columns C satisfying \nFDs . if \u00b7; \u00d8ha,. d ; C follows from the rules in Figure 6. The .rst argument to the judgement is an \nenvironment S that maps a variable v bound in the context to a pair BWC, where  (AVAR) (v: \u00d8 W C) . \nS S; \u00d8 ha,. v; C (AUNIT) A = \u00d8 . hfd A . C S; A ha,. C; C (v: A W D) . S . hfd B . C . AA . B . C (AMAP) \nS; B ha,. C . v; C . D . hfd A . (B n C) . B 8 C S; A ha,. p 1; B S; A ha,. p 2; C (AJOIN) S; A ha,. \np 1 Wp 2; B . C S; B ha,. p ; C S,v: BWC; A ha,. d; D (ALET) S; A ha,. let v: BWC = p in d ; D Figure \n6. Adequacy rules: S; A ha,. d ; B and S; A ha,. p ; B B is the set of columns bound on any path to node \nv from the root of the decomposition, and C is the set of columns bound within the subgraph rooted at \nv. The second argument A is a set of columns .xed by the context. If a decomposition d is adequate, then \nit can represent every possible relation with columns C satisfying FDs .: Lemma 1 (Soundness of Adequacy). \nIf \u00b7; \u00d8ha,. d ; C then for each relation r with columns C such that r |= fd . there is some d such that \n\u00b7,d |= \u00b7,d and a(d, \u00b7)= r. The adequacy rules enforce several properties, most of which are boundary \nconditions. Rule (AVAR) ensures the root vertex has exactly one instance (since \u00d8 has only one valuation). \nRules (AU-NIT) and (AMAP) record the columns they contain, and the top\u00adlevel rule (AVAR) then ensures \nthe decomposition represents all columns of the relation. Rule (AUNIT) also ensures that unit de\u00adcompositions \nare not part of the graph root. Since a unit decompo\u00adsition represents exactly one tuple, a unit decomposition \nat the root (A = \u00d8) would prevent us from representing the empty relation. Rule (AMAP) is the most involved \nand consequential rule. Sharing occurs when the same variable is the target of two or more maps (see \nthe uses of variable w in (2) for an example). Rule (AMAP) checks in two steps that decomposition instances \nare shared only when the corresponding relations are equal. First, note that B . C are columns bound \nfrom the root to v, and the functional dependency B . C . A guarantees there is a unique valuation of \nA per valuation of B . C. Second, the requirement that A . B . C guarantees that A includes all the columns \nbound on all paths reaching v (since this same requirement is also applied to other map edges that share \nv). Because B . C . A, and A includes any other key columns used in other maps reaching v, the sub-relation \nreached via any of these alternative paths is the same. To split a relation into two parts using a join \ndecomposition, rule (AJOIN) requires a functional dependency that ensures that we can match tuples from \neach side without anomalies, such as missing or spurious tuples; recall 8 denotes symmetric difference. \nFinally rule (ALET) introduces variable typings from let bindings into the variable binding environment \nS. 4. Querying and Updating Decomposed Relations In Section 3 we introduced decompositions, which describe \nhow to represent a relation in memory as a collection of data structures. In this section we show how \nto compile the relational operations described in Section 2 into code tailored to a particular decompo\u00ad \nsition. There are two basic kinds of relational operation, namely queries and mutations. Since we use \nqueries when implementing mutations, we describe queries .rst. q ::= qunit | qscan(q) | qlookup(q) | \nqlr(q, lr) | qjoin(q1,q2, lr) lr ::= left | right Figure 7. Query Plan Operators 4.1 Queries and Query \nPlans Recall that the query operation retrieves data from a relation; given a relation r, a tuple t, \nand a set of columns C, a query returns the projection onto columns C of the tuples of r that match tuple \nt. We implement queries in two stages: query planning, which at\u00adtempts to .nd the most ef.cient execution \nplan q for a query, and query execution, which evaluates a particular query plan over a de\u00adcomposition \ninstance. This approach is well-known in the database literature, although our formulation is novel. \nIn the RELC compiler, query planning is performed at compile time; the compiler generates specialized \ncode to evaluate the cho\u00adsen plan q with no run-time planning or evaluation overhead. The compiler is \nfree to use any method it likes to chose a query plan, as long as the resulting query satis.es the query \nvalidity criteria described in Section 4.2. We describe the query planner implemen\u00ad tation of the RELC \ncompiler in Section 4.3. As a motivating example, suppose we want to .nd the set of pid values of processes \nthat match the tuple (ns:7, state: R) using the decomposition of Figure 2. That is, we want to .nd the \nrunning processes in namespace 7. One possible strategy would be to look up (state: R) on the right-hand \nside, and then to iterate over all ns, pid pairs associated with the state, checking to see whether they \nare in the correct namespace. Another strategy would be to look up namespace 7 on the left-hand side, \nand to iterate over the set of pid values associated with the namespace. For each pid we then check to \nsee whether the ns and pid pair is in the set of processes associated with (state: R) on the right-hand \nside. Each strategy has a different computational complexity; the query planner enumerates the alternatives \nand chooses the best strategy. We describe the semantics of query execution using a function dqexec qdt \nwhich takes a query plan q, a decomposition instance d, an input tuple t, and evaluates the plan over \nthe decomposition, and produces a set of tuples in the denotation of d that match tuple t. We do not \nimplement dqexec directly; instead the compiler emits instances of dqexec specialized to particular queries \nq. A query plan is a tree of query plan operators, shown in Fig\u00adure 7. The query plan tree is superimposed \non a decomposition and rooted at the decomposition s root. A query plan prescribes an or\u00addered sequence \nof nodes and edges of the decomposition instance to visit. There are .ve query plan operators: Unit The \nqunit operator returns the unique tuple represented by a unit decomposition instance if that tuple matches \nt. It returns the empty set otherwise. Scan The operator qscan(q) invokes operator q for each child node \nvs where s matches t. Recall a map primitive is a mapping from a set of key columns C to a set of child \nnodes {vt}t.T . Since operator qscan iterates over the contents of a map data structure, it typically \ntakes time linear in the number of entries. Lookup The qlookup(q) operator looks up a particular set \nof key values in a map decomposition; each of the key columns of the map must be bound in the tuple t \ngiven as input to the operator. Query operator q is invoked on the resulting sub\u00addecomposition, if any. \nThe complexity of the qlookup depends on the particular choice of data structure .. In general, we expect \nqlookup to have better time complexity than qscan.  Left/Right The qlr(q, lr) operator performs query \nq on either the left-hand or right-hand side of a join speci.ed by the argument lr. The other side of \nthe join is ignored. Join The qjoin(q1,q2, lr) operator performs a join across both sides of a join decomposition. \nThe computational complexity of the join may depend on the order of evaluation. If lr is the value left, \nthen .rst query q1 is executed on the left side of the join decomposition, then query q2 is executed \non the right side of the join for each tuple returned by tuple q1; the result of the join operator is \nthe natural join of the two subqueries. If lr is the value right, the two queries are executed in the \nopposite order. In the scheduler example, the query query r (ns:7, pid: 42){cpu} returns the cpu values \nassociated with the process with pid 42 in namespace 7. One possible query plan is: qcpu = qlr(qlookup(qlookup(qunit)), \nleft). To perform query qcpu on an instance d we evaluate dqexec qcpu d (ns:7, pid: 42) which .rst looks \nup namespace 7 in the data structure correspond\u00ading to decomposition edge from x to y, returning an instance \nof node y. We lookup pid 42 in the data structure corresponding to the edge from y to w, obtaining an \ninstance of node w. We then use the qunit operator to retrieve the cpu value associated with node w. \nRecall our motivating example, namely the query query r (ns:7, state: R){pid} that returns the set of \nrunning processes in namespace 7. Two plans that implement the query are q1 = qjoin qlookup(qscan(qunit)), \nqlookup(qlookup(qunit)), left q2 = qlr qlookup(qscan(qunit)), right . Plan q1 .rst enumerates the set \nof processes with ns 7 (the left\u00adhand side of the join), and then checks whether each process is associated \nwith the running state (the right-hand side of the join). Plan q2 iterates over all processes in the \nrunning state, checking to see whether they are in the appropriate namespace. An important property of \nthe query operators is that they all re\u00adquire only constant space; there is no need to construct intermediate \ndata structures to execute a query. Having a predictable space over\u00adhead for queries ensures that query \nexecution does not need to al\u00adlocate memory. Constant-space queries can also be a disadvantage; for example, \nthe current restrictions would not allow a hash-join strategy for implementing the join operator, nor \nis it possible to per\u00adform duplicate-elimination. It would be straightforward to extend the query language \nwith non-constant-space operators.  4.2 Query Validity Not every query plan is a correct strategy for \nevaluating a query. We must check three properties: .rst that queries produce all of the columns requested \nas output, second that when performing a lookup we already have all of the necessary key columns, and \nthird that enough columns are computed on each side of a join so that tuples from each side can be accurately \nmatched with one another. Figure 8 gives inference rules for a validity judgment that is a suf.cient \ncondition for query plan correctness. We say a query plan is valid, written G , d, A hq,. q, B if q \ncorrectly answers queries over decomposition d , where A is the set of columns bound in the input tuple \npattern t and B is the set of columns bound in the output tuples; G is an environment that maps variables \nin the decomposition to their de.nitions, whereas . is a set of FDs. (QSCAN) (QUNIT) G, G(v), (A . C) \nhq,. q, B G , C, A hq,. qunit,C G ,C . v, A hq,. qscan(q),B . C C . A G, G(v),A hq,. q, B (QLOOKUP) \nG ,C . v, A hq,. qlookup(q),B . C G,p 1,A hq,. q1,B1 G,p 2,A . B1 hq,. q2,B2 . hfd A . B1 . B2 . hfd \nA . B2 . B1 (QJOIN) G ,p 1 Wp 2,A hq,. qjoin(q1,q2, left),B1 . B2 or G ,p 2 Wp 1,A hq,. qjoin(q2,q1, \nright),B1 . B2 G ,p 1,A hq,. q, B (QLR) G ,p 1 Wp 2,A hq,. qlr(q, left),B or G ,p 2 Wp 1,A hq,. qlr(q, \nright),B (QVAR) (QLET) G, G(v),A hq,. q, B G .{v . p }, d, A hq,. q, B G , v, A hq,. q, B G , let v: \n\u00b7\u00b7\u00b7 = p in q,. q, B d, A h Figure 8. Validity of query plans: G , d, A hq,. q, B Rule (QUNIT) states \nthat querying a unit decomposition binds its .elds. Rule (QSCAN) states when scanning over a map decom\u00adposition \nwe bind the keys of the map both as input to the sub-query and in the output. Rule (QLOOKUP) is similar, \nhowever lookups require that the key columns already be bound in the input. Rule (QJOIN) requires that \neach subquery of a join must bind enough columns so that we can match the results of the two subqueries \nwithout any ambiguity. As a special case, rule (QLR) allows ar\u00adbitrary queries that only inspect one \nside of a join. Finally rules (QVAR) and (QLET) handle introduction and elimination of de\u00adcomposition \nvariables into the variable binding environment G . Lemma 2 (Decomposition Query Soundness). Suppose \nwe have d , C, ., d, and r such that decomposition d is adequate (i.e., \u00b7; \u00d8ha,. d; C), instance d is \nwell-formed (i.e., \u00b7,d |= \u00b7,d ), and d represents a relation r (i.e., a(d, \u00b7)= r) satisfying the FDs \n. (i.e., . |= fd r). If a query plan q is valid for input tuples with columns A and produces columns \nB (i.e., \u00b7, d, A hq,. q, B), then for any tuple s with dom s = A we have pB (dqexec qds)= pB {t . r | \nt . s}.  4.3 Query Planner To pick good implementations for each query, the compiler uses a query planner \nthat .nds the query plan with the lowest cost as measured by a heuristic cost estimation function. The \nquery planner enumerates the set of valid query plans for a particular decomposition d, input columns \nB, and output columns C, and it returns the plan with the lowest cost. It is straightforward to enumerate \nquery plans, although there may be exponentially many possible plans for a query. The RELC compiler uses \na simple query cost estimator EG that has performed well in our experiments. Many extensions to our cost \nmodel are possible, inspired by the large literature on database query planning. For every edge from \nnode v1 to node v2 in a decomposition d we require a count c(v1,v2) of the expected number of instances \nof the edge outgoing from any given instance of node v1. The count can be provided by the user, or recorded \nas part of a pro.ling run. Each data structure . must provide a function m.(n) that estimates the number \nof memory accesses to lookup a key in a data structure . containing n elements. For  (a) (b)  ns, \npid 1, 2  Figure 9. Example of insertion and removal. Inserting the tuple t = (ns:2, pid:1, state: S, \ncpu:5) into instance (a) produces instance (b); conversely removing tuple t from (b) produces (a). Differences \nbetween the instances are shown using dashed lines. a binary tree we might set mbtree(n) = log2 n, whereas \nfor a linked list we might set mdlist(n)= n. Let G be the environment mapping each let-bound variable \nin d to its de.nition. We compute EG (q, v, d ), where v is the decomposition root: EG (qunit, v, C)=1 \n. EG (qscan(q),v1,C -. v2)= c(v1,v2) \u00d7 EG (q, v2, G(v2)) . (qlookup(q),v1,C -. v2)= EG m.(c(v1,v2)) \n\u00d7 EG (q, v2, G(v2)) EG (qjoin(q1,q2, ), v, p 1 Wp 2)= EG (q1, v, p 1)+ EG (q2, v, p 2) EG (qlr(q, left), \nv, p 1 Wp 2)= EG (q, v, p 1) EG (qlr(q, right), v, p 1 Wp 2)= EG (q, v, p 2) The cost estimate for joins \nis optimistic since it assumes that queries on each side of the join need only be performed once each, \nwhereas in general one side of a join is executed once for each tuple yielded by the other side. We could \nextend the heuristic to estimate how many tuples are returned by a query, however this has not proved \nnecessary so far.  4.4 Mutation: Empty and Insert Operations Next we turn our attention to compiling \nthe empty and insert operations. The empty operation is implemented using a function dempty d which creates \nan empty instance of a decomposition d . The insert operation is implemented by a function dinsert dtd, \nwhich inserts a tuple t into a decomposition instance d. To create an empty instance of a decomposition, \nthe dempty operation simply creates a single instance of the root node of the decomposition graph; since \nthe relation does not contain any tuples, we do not need to create instances of any map edges. The adequacy \nconditions for decompositions ensure that the root node does not contain any unit decompositions, so \nit is always possible to represent the empty relation. To insert a tuple t into an instance d of a decomposition \nd , for each node v:BWC in the decomposition we need to .nd or create an instance vs where s = pB t in \nthe decomposition instance. For each edge in the decomposition we also need to .nd or create an instance \nof the edge connecting the corresponding pair of node instances. We perform insertion over the nodes \nof a decomposition in topologically-sorted order. For each node v we locate the existing node instance \nvs corresponding to tuple t, if any. If no such vs exists, we create one, inserting vs into any data \nstructures that link it to its ancestors. For example, suppose we want to insert the tuple t = (ns:2, \npid:1, state: S, cpu:5) (a) (b)  ns state ns state pid ns, pid pid ns, pid cpu cpu   Figure 10. Two \ncuts of a decomposition: (a) the cut for columns {ns, pid}, and (b) the cut for columns {state}. into \nthe decomposition instance shown in Figure 9(a). We need to .nd or create the node instances xo), yons:2), \nzostate: S), and wons:2,pid:1,state: S,cpu:5). We consider each in topologically\u00adsorted order. Node xo) \nis the root of the decomposition instance, so we know its location already. Next we lookup the tuple \n(ns:2) in the instance of the map from x to y associated with xo); no such tuple exists so we create \na new node yons:2) and insert it into the map. Similarly we look up the tuple (state: S) in the instance \nof the map from x to z associated with node xo) to discover the ex\u00adisting node instance zostate: S). \nFinally, we have a choice; we can either look up tuple (pid:1) in the map from y to w or look up the \ntuple (ns:2, pid:1) in the map from z to w; in either case we .nd that no such tuple exists, hence we \nmust create a new instance of vertex w and insert it into both maps. If tuple t was a duplicate of a \ntuple already present in the relation then vertex w would have already been present and we would not \nneed to do any further work.  4.5 Mutation: Removal and Update Operations We next consider the remove \nand update operations. We imple\u00adment remove using a function dremove dsd, which removes tu\u00adples matching \ntuple s from an instance d of decomposition d . The operation works by removing any nodes and edges from \nd that form part of the representation of tuples that only match s. To implement removal and update we \nneed the notion of a cut of a decomposition. Given a tuple t with domain C, a cut of a decomposition \nd is a partition (X, Y ) of the nodes of d into nodes yA . Y that can only be part of the representation \nof tuples matching t, that is, . hfd A . C, and nodes xB . X that may form part of the representation \nof tuples that do not match t, that is . hfd B -C. Figure 10 shows two possible cuts of the scheduler \ndecomposition for different sets of columns C. Edges in a decomposition cut (X, Y ) may point from X \nto Y but not vice-versa. This result follows from the adequacy judgment, which ensures that the columns \nbound in the child of a map edge must functionally determine the columns bound in its parent. The adequacy \njudgement also guarantees that the cut for a particular decomposition d and set of columns C always exists \nand is unique. To remove tuples matching a tuple t using a cut (X, Y ), we simply break any edges crossing \nthe cut. That is, we remove any references from data structures linking instances of nodes in X to instances \nof nodes in Y that form part of the representation of tuples that match t. Once all such references are \nremoved, the instances of nodes in Y are unreachable from the root of the decomposition instance and \ncan be deallocated. We can also clean up any map nodes in X that are now devoid of children. For example, \nsuppose we want to remove all tuples matching the tuple t = (ns:2, pid:1) from the decomposition instance \nshown in Figure 9(b). Tuple t has the domain C = {ns, pid}; Figure 10(a) shows the corresponding decomposition \ncut. Nodes x, y, and z lie above the cut; an instance of node x is always present in every possible relation, \ninstances of node y are speci.c to a particular namespace but not to any particular process id, and instances \nof node z are speci.c to a particular process state but not to any particular process. Node w lies below \nthe cut; each instance of node w forms part of exactly one valuation for the columns C. To perform removal, \nwe break any instances of the edges from instances of nodes y and z to instances of node w which match \ntuple t; these edges are drawn as dashed lines in Figure 9(b). Once the dashed edges/nodes in Figure \n9 are removed, we have the option to deallocate the map at node y2 as well. Our implementation deallocates \nempty maps to minimize space consumption.  To .nd the edge instances to break we can reuse the query \nplanner. Any query that takes columns C as input and visits each of the edges we want to cut will work. \nOne such plan is qjoin qlookup(qlookup(qunit)), qlookup(qlookup(qunit)), left . For some data structures, \nsuch as intrusive doubly-linked lists, we can remove edges given the destination node alone. If the edge \nfrom z to w uses such a data structure we could use the cheaper plan: qlr qlookup(qlookup(qunit)), left \n. We implement update using a function dupdate dsud, which updates tuples matching s using values from \nu in an instance d of decomposition d . Semantically, updates are a removal followed by an insertion. \nIn the implementation we can reuse the nodes and edges discarded in the removal in the subsequent insertion \ni.e., we can perform the update in place. We provide only the common case for updates of tuples t match\u00ading \na tuple pattern s, namely when s is a key for the relation and u does not alter columns appearing in \ns. Non-key patterns or key\u00admodifying tuple updates may merge multiple tuples, and hence re\u00adquire the \nimplementation to merge nodes. Our restriction guaran\u00adtees no merging can occur. We can reuse nodes and \nedges below the cut, and any changes in u that apply to nodes below the cut can be performed in-place. \n 4.6 Soundness of Relational Operations Next we show that the operations on decompositions faithfully \nimplement the corresponding relational speci.cations. We show that sequences of relational operations \non graph decompositions are sound with respect to their logical counterparts (Theorem 5) by induction \nusing initialization and preservation lemmas. Lemma 3 (Decomposition Initialization). For any decomposition \n d, if d = dempty d then \u00b7,d |= \u00b7,d and a(d, \u00b7)= \u00d8. Lemma 4 (Decomposition Preservation). For all d , \n., t, d, C, and r where decomposition d is adequate (\u00b7; \u00d8ha,. d ; C), decom\u00ad position instance d is \nwell-formed (\u00b7,d |= \u00b7,d), and d represents relation r (a(d, \u00b7)= r) satisfying FDs . (. |= fd r), we have: \n(a) If dom t = C, d ' = dinsert dtd , and . |= fd r .{t} then \u00b7,d ' |= \u00b7,d and a(d ' , \u00b7)= r .{t}. (b) \nIf dom t . C and d ' = dsd then \u00b7,d ' = \u00b7,d and dremove | '' ' a(d ' , \u00b7)= r where r = r \\{t . r | t \n. s} and r |= fd .. (c) Suppose s is a key for r (. hfd dom s . dom C), the domains of s and u do not \nintersect (dom s n dom u = \u00d8), and we have d '' = dupdate = {if t . s then t < u else t | t . dsud and \nr r}. If r ' |= fd . then \u00b7,d ' |= \u00b7,d and a(d ' , \u00b7)= r ' . Theorem 5 (Decomposition Soundness). Let \nC be a set of columns, . a set of FDs, and d a decomposition adequate for C and .. Sup\u00adpose a sequence \nof insert, update and remove operators starting from the empty relation produce a relation r, and that \neach oper\u00adation satis.es the conditions of Lemma 4. Then the correspond\u00ad ing sequence of dinsert, dupdate, \nand dremove operators given dempty d as input produce d such that \u00b7,d |= \u00b7,d and a(d, \u00b7)= r. 5. Autotuner \nThus far we have concentrated on the problem of compiling rela\u00adtional operations for a particular decomposition \nof a relation. How\u00adever, a programmer may not know, or may not want to invest time in .nding the best \npossible decomposition for a relation. We have therefore constructed an autotuner that, given a program \nwritten to the relational interface, attempts to infer the best possible decom\u00adposition for that program. \nThe autotuner takes as input a benchmark program that pro\u00adduces as output a cost value (e.g., execution \ntime), together with the name of a relation to optimize. The autotuner then exhaustively constructs all \ndecompositions for that relation up to a given bound on the number of edges, recompiles and runs the \nbenchmark pro\u00adgram for each decomposition, and returns a list of decompositions sorted by increasing \ncost. We do not make any assumptions about the cost metric any value of interest such as execution time \nor memory consumption may be used. 6. Experiments We have implemented a compiler, named RELC, that takes \nas input a relational speci.cation and a decomposition, and emits C++ code implementing the relation. \nWe evaluate our compiler using micro\u00adbenchmarks and three real-world systems. The micro-benchmarks (Section \n6.1) show that different decompositions have dramatically different performance characteristics. Since \nour compiler generates C++ code, it is easy to incorporate synthesized data representations into existing \nsystems. We apply synthesis to three existing systems (Section 6.2), namely a web server, a network .ow \naccounting dae\u00ad mon, and a map viewer, and show that synthesis leads to code that is simultaneously simpler, \ncorrect by construction, and comparable in performance to the code it replaces. We chose C++ because \nit allows low-level control over memory\u00adlayout, has a powerful template system, and has widely-used li\u00adbraries \nof data structures from which we can draw. Data structure primitives are implemented as C++ template \nclasses that implement a common associative container API. The set of data structures can easily be extended \nby writing additional templates and providing the compiler some basic information about the data structure \ns ca\u00adpabilities. We have implemented a library of data structures that wrap code from the C++ Standard \nTemplate Library and the Boost Library [4], namely both non-intrusive and intrusive doubly-linked lists \n(std::list, boost::intrusive::list), non-intrusive and intrusive binary trees (std::map, boost::intrusive::set), \nhash\u00adtables (boost::unordered map), and vectors (std::vector). Since the C++ compiler expands templates, \nthe time and space overheads introduced by the wrappers is minimal. 6.1 Microbenchmarks We implemented \na selection of small benchmarks: a benchmark based on our running example of a process scheduler, a cache \nbenchmark based on the real systems discussed in the next section, and a graph benchmark. For space reasons, \nwe focus just on the graph benchmark. The graph benchmark reads in a directed weighted graph from a text \n.le and measures the times to construct the edge re\u00adlation, to perform forwards and backwards depth-.rst \nsearches over the whole graph, and to remove each edge one-by-one. We represent the edges of a directed \ngraph as a relation edges with columns {src, dst, weight} and a functional dependency src, dst . weight. \nWe represent the set of the graph nodes as a re\u00adlation nodes consisting of a single id column. The RELC \ncompiler emits a C++ module that implements classes nodes::relation and edges::relation with methods \ncorresponding to each rela\u00adFigure 11. Elapsed times for directed graph benchmarks for de\u00adcompositions \nup to size 4 with identical input. For each decom\u00adposition we show the times to traverse the graph forwards \n(F), to traverse both forwards and backwards (F+B), and to traverse for\u00adwards, backwards and delete each \nedge (F+B+D). We elide 68 de\u00adcompositions which did not .nish a benchmark within 8 seconds.  tional \noperation. A typical client of the relational interface is the algorithm to perform a depth-.rst search: \nedges::relation graph_edges; nodes::relation visited; // Code to populate graph_edges elided. stack<int> \nstk; stk.push(v0); while (!stk.empty()) { int v = stk.top(); stk.pop(); if (!visited.query(nodes::tuple_id(v))) \n{ visited.insert(nodes::tuple_id(v)); edges::query_iterator_src__dst_weight it; graph_edges.query(edges::tuple_src(v), \nit); while (!it.finished()) { stk.push(it.output.f_dst()); it.next(); } } } The code makes use of the \nstandard STL stack class in addition to an instance of the nodes relation visited and an instance of \nthe edges relation graph edges. To demonstrate the tradeoffs involved in the choice of decompo\u00adsition, \nwe used the autotuner framework to evaluate three variants of the graph benchmark under different decompositions. \nWe used a single input graph representing the road network of the northwest\u00adern USA, containing 1207945 \nnodes and 2840208 edges. We used three variants of the graph benchmark: a forward depth-.rst search (DFS); \na forward DFS and a backward DFS; and .nally a forward DFS, a backward DFS, and deletion of all edges \none at a time. We measured the elapsed time for each benchmark variant for the 84 decompositions that \ncontain at most 4 map edges (as generated by the autotuner). Timing results for decompositions that completed \nwithin an 8 second time limit are shown in Figure 11. Decompositions that (1) (5) (9) src src dst src \ndst dst src weight weight weight weight dst src dst  are isomorphic up to the choice of data structures \nfor the map edges are counted as a single decomposition; only the best timing result is shown for each \nset of isomorphic decompositions. There are 68 decompositions not shown that did not complete any of \nthe benchmarks within the time limit. Since the autotuner exhaustively enumerates all possible decompositions, \nnaturally only a few of the resulting decompositions are suitable for the access patterns of this particular \nbenchmark; for example, a decomposition that indexes edges by their weights performs poorly. Figure 12 \nshows three representative decompositions from those shown in Figure 11 with different performance characteristics. \nDe\u00ad composition 1 is the most ef.cient for forward traversal, however it performs terribly for backward \ntraversal since it takes quadratic time to compute predecessors. Decompositions 5 and 9 are slightly \nless ef.cient for forward traversal, but are also ef.cient for back\u00adward traversal, differing only in \nthe sharing of objects between the two halves of the decomposition. The node sharing in decomposi\u00adtion \n5 is advantageous for all benchmarks since it requires fewer memory allocations and allows more ef.cient \nimplementations of insertion and removal; in particular because the lists are intrusive the compiler \ncan .nd node w using either path and remove it from both paths without requiring any additional lookups. \n 6.2 Data Representation Synthesis in Existing Systems To demonstrate the practicality of our approach, \nwe took three existing open-source systems thttpd, Ipcap, and ZTopo and re\u00adplaced core data structures \nwith relations synthesized by RELC. All are publicly-available programs with real-world users. The thttpd \nweb server is a small and ef.cient web server imple\u00admented in C. We reimplemented the module of thttpd \nthat caches the results of the mmap() system call. When thttpd receives a re\u00adquest for a .le, it checks \nthe cache to see whether the same .le has previously been mapped into memory. If a cache entry exists, \nit reuses the existing mapping; otherwise it creates a new mapping. If the cache is full then the code \ntraverses through the mappings re\u00admoving those older than a certain threshold. Other researchers have \nused thttpd s cache module as a program analysis target [21]. The IpCap daemon is a TCP/IP network .ow \naccounting system implemented in C. IpCap runs on a network gateway, and counts the number of bytes incoming \nand outgoing from hosts on the local network, producing a list of network .ows for accounting purposes. \nFor each network packet, the daemon looks up the .ow in a table, and either creates a new entry or increments \nthe byte counts for an existing entry. The daemon periodically iterates over the collection of .ows and \noutputs the accumulated .ow statistics to a log .le; .ows that have been written to disk are removed \nfrom memory. We replaced the core packet statistics data structures with relations implemented using \nRELC. ZTopo is a topographic map viewer implemented in C++. A map consists of millions of small image \ntiles, retrieved using HTTP over the internet and reassembled into a seamless image. To mini\u00admize network \ntraf.c, the viewer maintains memory and disk caches Table 1. Non-comment lines of code for existing system \nexperi\u00adments. For each system, we report the size of entire original system and just the source module \nwe altered, together with the size of the altered source module and the mapping .le when using synthesis. \n Original Synthesis System Everything Module Decomposition Module thttpd Ipcap ZTopo 7050 2138 5113 \n402 899 1083 42 55 39 239 794 1048 of recently viewed map tiles. When retrieving a tile, ZTopo .rst \nat\u00adtempts to locate it in memory, then on disk, and as a last resort over the network. The tile cache \nwas originally implemented as a hash table, together with a series of linked lists of tiles for each \nstate to enable cache eviction. We replaced the tile cache data structure with a relation implemented \nusing RELC. Table 1 shows non-comment lines of code for each test-case. In each case the synthesized \ncode is comparable to or shorter than the original code in size. Both the thttpd and ipcap benchmarks \norig\u00adinally used open-coded C data structures, accounting for a large fraction of the decrease in line \ncount. ZTopo originally used C++ STL and Boost data structures, so the synthesized abstraction does not \ngreatly alter the line count. The ZTopo benchmark originally contained a series of fairly subtle dynamic \nassertions that veri.ed that the two different representations of a tile s state were in agree\u00adment; \nin the synthesized version the compiler automatically guar\u00adantees these invariants, so the assertions \nwere removed. For each system, the relational and non-relational versions had equivalent performance. \nIf the choice of data representation is good enough, data structure manipulations are not the limiting \nfactor for these particular systems. The assumption that the implementations are good enough is important, \nhowever; the auto-tuner considered plausible data representations that would have resulted in signi.\u00adcant \nslow-downs, but found alternatives where the data manipula\u00adtion was no longer the bottleneck. For example \nwe used the auto\u00adtuner on the Ipcap benchmark to generate all decompositions up to size 4; Figure 13 \nshows the elapsed time for each decomposi\u00ad tion on an identical random distribution of input packets. \nThe best decomposition is a binary-tree mapping local hosts to hash-tables of foreign hosts, which performs \napproximately 5\u00d7 faster than the decomposition ranked 18th, in which the data structures are identi\u00adcal \nbut local and foreign hosts are transposed. For this input distri\u00adbution the best decomposition performs \nidentically to the original hand-coded implementation to within the margin of measurement error. Our \nexperiments show that different choices of decomposition lead to signi.cant changes in performance (Section \n6.1), and that the best performance is comparable to existing hand-written im\u00adplementations (Section \n6.2). The resulting code is concise (Sec\u00ad tions 6.1 and 6.2), and the soundness of the compiler (Theorem \n5) guarantees that the resulting data structures are correct by construc\u00adtion. 7. Discussion and Related \nWork We build on our previous work [12], which introduced the idea of synthesizing shared low-level data \nstructures from a high-level re\u00adlational description. We decompose relations directly using graphs, rather \nthan .rst decomposing relations into trees that are then fused into graphs. As a consequence our theoretical \nframework is much simpler. We describe a complete query planning implementation, and we show how to reuse \nthe query planning infrastructure to per\u00adform ef.cient destructive updates using graph cuts. We present \na compiler that can synthesize ef.cient C++ implementations of rela-tional operations; previous work \nonly described a proof-of-concept simulator. We present an autotuner that automatically infers the best \ndecomposition for a relation. Finally, using three real examples we show that synthesis leads to code \nthat is simpler, guaranteed to be correct, and comparable in performance to the code it replaces. Relational \nRepresentations Many authors propose adding rela\u00adtions to both general-and special-purpose programming \nlanguages (e.g., [3, 22, 23, 26, 30]). We focus on the orthogonal problem of specifying and implementing \nthe underlying representations for re\u00adlational data. Relational representations are well-known from the \ndatabase community; however, databases typically treat the rela\u00adtions as a black box. Many extensions \nof our system are possible, motivated by the extensive database literature. Data models such as E/R diagrams \nand UML rely heavily on relations. One application of our technique is to close the gap between modeling \nlanguages and implementations. The autotuner framework has a similar goal to AutoAdmin [6]. AutoAdmin \ntakes a set of tables, together with a distribution of input queries, and identi.es a set of indices \nthat are predicted to produce the best overall performance under the query optimizer s cost model. The \ndetails differ because our decomposition and query languages are unlike those of a conventional database. \nSynthesizing Data Representations The problem of automatic data structure selection was explored in SETL \n[5, 24, 27] and has also been pursued for Java collection implementations [28]. The SETL representation \nsublanguage [9] maps abstract SETL set and map objects to implementations, although the details are quite \ndif\u00adferent from our work. Unlike SETL, we handle relations of arbi\u00adtrary arity, using functional dependencies \nto enforce complex shar\u00ading invariants. In SETL, set representations are dynamically em\u00adbedded into carrier \nsets under the control of the runtime system, while by contrast our compiler synthesizes low-level representa\u00adtions \nfor a speci.c decomposition with no runtime overhead. Previous work has proposed using a programming \nmodel based on relations which is implemented in the backend using container data structures [8, 29]. \nA novel aspect of our approach is that our relations can have speci.ed restrictions (speci.cally, functional \nde\u00adpendencies) which enable a much wider range of possible imple\u00admentations, including complex patterns \nof sharing. We also present the .rst formal results, including the notion of adequate decompo\u00adsitions \nand a proof that operations on adequate decompositions are sound with respect to their relational speci.cations. \nUnlike previ\u00adous work, we propose a dynamic autotuner that can automatically synthesize the best decomposition \nfor a particular relation, and we present our experience with a full implementation of these tech\u00adniques \nin practice.  Synthesizing specialized data representations has previously been considered in other \ndomains. Ahmed et al. [1, 15] proposed transforming dense matrix computations into implementations tai\u00adlored \nto speci.c sparse representations as a technique for handling the proliferation of complicated sparse \nrepresentations. Synthesis Versus Veri.cation Approaches A key advantage of data representation synthesis \nover hand-written implementations is the synthesized operations are correct by construction, subject \nto the correctness of the compiler. We assume the existence of a library of data structures; the data \nstructures themselves can be proved correct using existing techniques [31]. Our system provides a modular \nway to assemble individually correct data structures into a complete and correct representation of a \nprogram s data. The Hob system uses abstract sets of objects to specify and ver\u00adify end-to-end properties \nof systems using multiple data structures that share objects [17, 19]. Monotonic typestates enable aliased \nob\u00ad jects to monotonically change their typestates in the presence of sharing without violating type \nsafety [11]. Researchers have de\u00ad veloped systems that have mechanically veri.ed data structures (for \nexample, hash tables) that implement binary relational inter\u00adfaces [7, 31, 32]. The relation implementation \npresented in this pa\u00ad per is more general (it can implement relations of arbitrary arity) and solves \nproblems orthogonal to those addressed in previous re\u00adsearch. Specifying And Inferring Shared Representations \nThe decom\u00adposition language provides a functional description of the heap that separates the problem \nof modeling individual data structures from the problem of modeling the heap as a whole. Unlike pre\u00advious \nwork, decompositions allow us to state and reason about complex sharing invariants that are dif.cult \nto state and impossi\u00adble to verify using previous techniques. Previous work investigated modular reasoning \nabout data structures shared between different modules [13]. Graph types [14] extend tree-structured \ntypes with extra pointers that are functionally determined by the structure of the tree backbone, but \ncannot reason about overlapping structures. Separation logic allows elegant speci.cations of disjoint \ndata struc\u00adtures [25], and mechanisms have been added to separation logic to express some types of sharing \n[2, 10]. Some static analysis al\u00ad gorithms infer some sharing between data structures in low level code \n[16, 18, 20, 21]; however verifying overlapping shared data structures in general remains an open problem \nfor such approaches. The combination of relations and functional dependencies allows us to reason about \nsharing that is beyond current static analysis techniques. 8. Conclusion We have presented a system for \nspecifying and operating on data at a high level as relations while correctly compiling those relations \ninto a composition of low-level data structures. Most unusual is our ability to express, and prove correct, \nthe use of complex sharing in the low-level representation. We show using three real-world systems that \ndata representation synthesis leads to code that is simpler, correct by construction, and comparable \nin performance to existing hand-written code. Acknowledgments The fourth author thanks Viktor Kuncak, \nPatrick Lam, Darko Mari\u00adnov, Alex S. alcianu, and Karen Zee for discussions in 2001 2 on programming \nwith relations, with the relations implemented by automatically-generated linked data structures. The \nsecond author thanks Daniel S. Wilkerson and Simon F. Goldsmith for discussions in 2005 on Wilkerson \ns language proposal Orth, which includes re\u00adlational speci.cations of data structures, the generation \nof functions for querying and maintaining them, and was further envisioned by Goldsmith to perform automatic \ndata structure selection via pro.l\u00ading. References [1] Nawaaz Ahmed, Nikolay Mateev, Keshav Pingali, \nand Paul Stodghill. A framework for sparse matrix code synthesis from high-level spec\u00adi.cations. In Supercomputing, \npage 58. IEEE Computer Society, November 2000. doi: 10.1109/SC.2000.10033. [2] Josh Berdine, Cristiano \nCalgano, Byron Cook, Dino Distefano, Pe\u00adter W. O Hearn, Thomas Wies, and Hongseok Yang. Shape analy\u00adsis \nfor composite data structures. In CAV, volume 4590 of LNCS, pages 178 192. Springer Berlin / Heidelberg, \n2007. doi: 10.1007/ 978-3-540-73368-3 22. [3] Gavin Bierman and Alisdair Wren. First-class relationships \nin an object-oriented language. In ECOOP, volume 3586 of LNCS, pages 262 286. Springer Berlin / Heidelberg, \n2005. doi: 10.1007/11531142 12. [4] Boost. Boost C++ libraries, 2010. URL http://www.boost.org/. [5] \nJiazhen Cai and Robert A. Paige. Look ma, no hashing, and no arrays neither . In POPL, pages 143 154, \nNew York, NY, USA, 1991. ACM. ISBN 0-89791-419-8. doi: 10.1145/99583.99605. [6] Surajit Chaudhuri and \nVivek R. Narasayya. An ef.cient cost-driven index selection tool for Microsoft SQL Server. In VLDB, pages \n146 155, San Francisco, CA, USA, 1997. Morgan Kaufmann Publishers. ISBN 1-55860-470-7. [7] Adam Chlipala, \nGregory Malecha, Greg Morrisett, Avraham Shinnar, and Ryan Wisnesky. Effective interactive proofs for \nhigher-order im\u00adperative programs. In ICFP, pages 79 90, New York, NY, USA, 2009. ACM. ISBN 978-1-60558-332-7. \ndoi: 10.1145/1596550.1596565. [8] Donald Cohen and Neil Campbell. Automating relational operations on \ndata structures. IEEE Software, 10(3):53 60, May 1993. ISSN 0740-7459. doi: 10.1109/52.210604. [9] Robert \nB. K. Dewar, Arthur Grand, Ssu-Cheng Liu, Jacob T. Schwartz, and Edmond Schonberg. Programming by re.nement, \nas exempli.ed by the SETL representation sublanguage. ACM Trans. Program. Lang. Syst., 1(1):27 49, January \n1979. ISSN 0164-0925. doi: 10.1145/ 357062.357064. [10] Dino Distefano and Matthew J. Parkinson. jStar: \ntowards practical veri.cation for Java. In OOPSLA, pages 213 226, New York, NY, USA, 2008. ACM. ISBN \n978-1-60558-215-3. doi: 10.1145/1449764. 1449782. [11] Manuel F\u00a8ahndrich and K. Rustan M. Leino. Heap \nmonotonic type\u00adstates. In International Workshop on Alias Con.nement and Owner\u00adship, July 2003. [12] \nPeter Hawkins, Alex Aiken, Kathleen Fisher, Martin Rinard, and Mooly Sagiv. Data structure fusion. In \nAPLAS, volume 6461 of LNCS, pages 204 221. Springer Berlin / Heidelberg, 2010. doi: 10.1007/978-3-642-17164-2 \n15. [13] Uri Juhasz, Noam Rinetzk, Arnd Poetzsch-Heffter, Mooly Sagiv, and Eran Yahav. Modular veri.cation \nwith shared abstractions. In In\u00adternational Workshop on Foundations of Object-Oriented Languages (FOOL), \n2009. [14] Nils Klarlund and Michael I. Schwartzbach. Graph types. In POPL, pages 196 205, New York, \nNY, USA, 1993. ACM. ISBN 0-89791\u00ad560-7. doi: 10.1145/158511.158628. [15] Vladimir Kotlyar, Keshav Pingali, \nand Paul Stodghill. A relational approach to the compilation of sparse matrix programs. In Euro\u00adPar 97 \nParallel Processing, volume 1300 of LNCS, pages 318 327. Springer Berlin / Heidelberg, 1997. doi: 10.1007/BFb0002751. \n[16] J\u00a8org Kreiker, Helmut Seidl, and Vesal Vojdani. Shape analysis of low-level C with overlapping structures. \nIn VMCAI, volume 5044 of LNCS, pages 214 230. Springer Berlin / Heidelberg, 2010. doi: 10.1007/978-3-642-11319-2 \n17. [17] Victor Kuncak, Patrick Lam, Karen Zee, and Martin Rinard. Modular pluggable analyses for data \nstructure consistency. IEEE Transactions on Software Engineering, 32(12):988 1005, 2006. ISSN 0098-5589. \ndoi: 10.1109/TSE.2006.125.  [18] Viktor Kuncak, Patrick Lam, and Martin Rinard. Role analysis. In POPL, \npages 17 32, New York, NY, USA, 2002. ACM. ISBN 1\u00ad58113-450-9. doi: 10.1145/503272.503276. [19] Patrick \nLam, Viktor Kuncak, and Martin Rinard. Generalized typestate checking for data structure consistency. \nIn VMCAI, volume 3385 of LNCS, pages 430 447. Springer Berlin / Heidelberg, 2005. doi: 10.1007/978-3-540-30579-8 \n28. [20] Oukseh Lee, Hongseok Yang, and Rasmus Petersen. Program analysis for overlaid data structures. \nIn CAV, LNCS, 2011. To appear. [21] Bill McCloskey, Thomas Reps, and Mooly Sagiv. Statically inferring \ncomplex heap, array, and numeric invariants. In Static Analysis, volume 6337 of LNCS, pages 71 99. Springer \nBerlin / Heidelberg, 2011. doi: 10.1007/978-3-642-15769-1 6. [22] Eric Meijer, Brian Beckman, and Gavin \nBierman. LINQ: Reconciling objects, relations and XML in the .NET framework. In SIGMOD, pages 706 706, \nNew York, NY, USA, 2006. ACM. ISBN 1-59593\u00ad434-0. doi: 10.1145/1142473.1142552. [23] Christopher Olston, \nBenjamin Reed, Utkarsh Srivastava, Ravi Ku\u00admar, and Andrew Tomkins. Pig Latin: a not-so-foreign language \nfor data processing. In SIGMOD, pages 1099 1110, New York, NY, USA, 2008. ACM. ISBN 978-1-60558-102-6. \ndoi: 10.1145/1376616. 1376726. [24] Robert Paige and Fritz Henglein. Mechanical translation of set theo\u00adretic \nproblem speci.cations into ef.cient RAM code a case study. Journal of Symbolic Computation, 4(2):207 \n232, 1987. ISSN 0747\u00ad7171. doi: 10.1016/S0747-7171(87)80066-4. [25] John C. Reynolds. Separation logic: \nA logic for shared mutable data structures. In LICS, pages 55 74, 2002. doi: 10.1109/LICS.2002. 1029817. \nInvited paper. [26] Tom Rothamel and Yanhong A. Liu. Ef.cient implementation of tuple pattern based retrieval. \nIn PEPM, pages 81 90, New York, NY, USA, 2007. ACM. ISBN 978-1-59593-620-2. doi: 10.1145/1244381. 1244394. \n[27] Edmond Schonberg, Jacob T. Schwartz, and Micha Sharir. Automatic data structure selection in SETL. \nIn POPL, pages 197 210, New York, NY, USA, 1979. ACM. doi: 10.1145/567752.567771. [28] Ohad Shacham, \nMartin Vechev, and Eran Yahav. Chameleon: adaptive selection of collections. In PLDI, pages 408 418, \nNew York, NY, USA, 2009. ACM. ISBN 978-1-60558-392-1. doi: 10.1145/1542476. 1542522. [29] Yannis Smaragdakis \nand Don Batory. DiSTiL: a transformation library for data structures. In Conference on Domain-Speci.c \nLanguages (DSL 97), pages 257 271. USENIX, October 1997. [30] Mandana Vaziri, Frank Tip, Stephen Fink, \nand Julian Dolby. Declar\u00adative object identity using relation types. In ECOOP, volume 4609 of LNCS, pages \n54 78. Springer Berlin / Heidelberg, 2007. doi: 10.1007/978-3-540-73589-2 4. [31] Karen Zee, Viktor Kuncak, \nand Martin Rinard. Full functional ver\u00adi.cation of linked data structures. In PLDI, pages 349 361, New \nYork, NY, USA, 2008. ACM. ISBN 978-1-59593-860-2. doi: 10. 1145/1375581.1375624. [32] Karen Zee, Viktor \nKuncak, and Martin Rinard. An integrated proof language for imperative programs. In PLDI, pages 338 351, \nNew York, NY, USA, 2009. ACM. ISBN 978-1-60558-392-1. doi: 10. 1145/1542476.1542514.   \n\t\t\t", "proc_id": "1993498", "abstract": "<p>We consider the problem of specifying combinations of data structures with complex sharing in a manner that is both declarative and results in provably correct code. In our approach, abstract data types are specified using relational algebra and functional dependencies. We describe a language of decompositions that permit the user to specify different concrete representations for relations, and show that operations on concrete representations soundly implement their relational specification. It is easy to incorporate data representations synthesized by our compiler into existing systems, leading to code that is simpler, correct by construction, and comparable in performance to the code it replaces.</p>", "authors": [{"name": "Peter Hawkins", "author_profile_id": "81331494100", "affiliation": "Stanford University, Stanford, CA, USA", "person_id": "P2690472", "email_address": "hawkinsp@cs.stanford.edu", "orcid_id": ""}, {"name": "Alex Aiken", "author_profile_id": "81100399954", "affiliation": "Stanford University, Stanford, CA, USA", "person_id": "P2690473", "email_address": "aiken@cs.stanford.edu", "orcid_id": ""}, {"name": "Kathleen Fisher", "author_profile_id": "81331492634", "affiliation": "Tufts University, Medford, MA, USA", "person_id": "P2690474", "email_address": "kfisher@eecs.tufts.edu", "orcid_id": ""}, {"name": "Martin Rinard", "author_profile_id": "81100087275", "affiliation": "Massachusetts Institute of Technology, Cambridge, MA, USA", "person_id": "P2690475", "email_address": "rinard@csail.mit.edu", "orcid_id": ""}, {"name": "Mooly Sagiv", "author_profile_id": "81100150928", "affiliation": "Tel-Aviv University, Tel-Aviv, Israel", "person_id": "P2690476", "email_address": "msagiv@post.tau.ac.il", "orcid_id": ""}], "doi_number": "10.1145/1993498.1993504", "year": "2011", "article_id": "1993504", "conference": "PLDI", "title": "Data representation synthesis", "url": "http://dl.acm.org/citation.cfm?id=1993504"}