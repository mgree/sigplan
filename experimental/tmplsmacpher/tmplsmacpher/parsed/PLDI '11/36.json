{"article_publication_date": "06-04-2011", "fulltext": "\n LL(*): The Foundation of the ANTLR Parser Generator Terence Parr Kathleen Fisher * University of San \nFrancisco Tufts University parrt@cs.usfca.edu k.sher@eecs.tufts.edu Abstract Despite the power of Parser \nExpression Grammars (PEGs) and GLR, parsing is not a solved problem. Adding nonde\u00adterminism (parser speculation) \nto traditional LL and LR parsers can lead to unexpected parse-time behavior and in\u00adtroduces practical \nissues with error handling, single-step de\u00adbugging, and side-e.ecting embedded grammar actions. This \npaper introduces the LL(*) parsing strategy and an asso\u00adciated grammar analysis algorithm that constructs \nLL(*) parsing decisions from ANTLR grammars. At parse-time, decisions gracefully throttle up from conventional \n.xed k = 1 lookahead to arbitrary lookahead and, .nally, fail over to backtracking depending on the complexity \nof the parsing de\u00adcision and the input symbols. LL(*) parsing strength reaches into the context-sensitive \nlanguages, in some cases beyond what GLR and PEGs can express. By statically removing as much speculation \nas possible, LL(*) provides the expressiv\u00adity of PEGs while retaining LL s good error handling and unrestricted \ngrammar actions. Widespread use of ANTLR (over 70,000 downloads/year) shows that it is e.ective for a \nwide variety of applications. Categories and Subject Descriptors F.4.2 Grammars and Other Rewriting Systems \n[Parsing]; D.3.1 Formal Lan\u00adguages [syntax]; D.3.4 Processors [Parsing] General Terms Algorithms, Languages, \nTheory Keywords nondeterministic parsing, backtracking, context\u00adsensitive parsing, semantic predicates, \nsyntactic predicates, deterministic .nite automata, augmented transition net\u00adworks, subset construction, \nmemoization, PEG, GLR 1. Introduction Parsing is not a solved problem, despite its importance and long \nhistory of academic study. Because it is tedious and error-prone to write parsers by hand, researchers \nhave spent decades studying how to generate e.cient parsers from high\u00adlevel grammars. Despite this e.ort, \nparser generators still su.er from problems of expressiveness and usability. When parsing theory was \noriginally developed, machine resources were scarce, and so parser e.ciency was the paramount concern. \nIn that era, it made sense to force programmers to contort their grammars to .t the con\u00adstraints of LALR(1) \nor LL(1) parser generators. In con\u00ad * Author worked at AT&#38;T Labs Research when work was done. Permission \nto make digital or hard copies of all or part of this work for personal or classroom use is granted without \nfee provided that copies are not made or distributed for pro.t or commercial advantage and that copies \nbear this notice and the full citation on the .rst page. To copy otherwise, to republish, to post on \nservers or to redistribute to lists, requires prior speci.c permission and/or a fee. PLDI 11, June 4 \n8, 2011, San Jose, California, USA. Copyright c &#38;#169; 2011 ACM 978-1-4503-0663-8/11/06. . . $10.00 \ntrast, modern computers are so fast that programmer ef\u00ad.ciency is now more important. In response to \nthis de\u00advelopment, researchers have developed more powerful, but more costly, nondeterministic parsing \nstrategies following both the bottom-up approach (LR-style parsing) and the top-down approach (LL-style \nparsing). In the bottom-up world, Generalized LR (GLR) [19] parsers parse in linear to cubic time, depending \non how closely the grammar conforms to classic LR. GLR essentially forks new subparsers to pursue all \npossible actions ema\u00adnating from nondeterministic LR states, terminating any subparsers that lead to \ninvalid parses. The result is a parse forest with all possible interpretations of the input. Elkhound \n[12] is a very e.cient GLR implementation that achieves yacc-like parsing speeds when grammars are LALR(1). \nPro\u00adgrammers unfamiliar with LALR parsing theory, though, can easily get nonlinear GLR parsers. Since \nGLR parser generators do not issue LR con.ict warnings, programmers can unwittingly specify non-LALR \ngrammars that lead to parsers with poor performance. In the top-down world, Ford introduced Packrat parsers \nand the associated Parser Expression Grammars (PEGs) [6, 7]. PEGs preclude only the use of left-recursive \ngrammar rules. Packrat parsers are backtracking parsers that attempt the alternative productions in the \norder speci.ed. The .rst production that matches at an input position wins. Pack\u00adrat parsers are linear \nrather than exponential because they memoize partial results, ensuring input states will never be parsed \nby the same production more than once. The Rats! [8] PEG-based tool vigorously optimizes away memoization \nevents to improve speed and reduce the memory footprint. A signi.cant advantage of both GLR and PEG parser \ngenerators is that they accept any grammar that conforms to their meta-language (with the exception that \nPEGs cannot be left-recursive). Programmers no longer have to wade through reams of con.ict messages. \nDespite this advantage, neither GLR nor PEG parsers are completely satisfactory, for a number of reasons. \nFirst, GLR and PEG parsers do not always do what was intended. GLR silently accepts ambiguous grammars, \nthose that match the same input in multiple ways, forcing programmers to detect ambiguities dynamically. \nPEGs have no concept of a grammar con.ict because they always choose the .rst interpretation, which can \nlead to unexpected or inconvenient behavior. For example, the second production of PEG rule A . a|ab \n(meaning A matches either a or ab ) will never be used. Input ab never matches the second alternative \nsince the .rst symbol, a, matches the .rst alternative. In a large grammar, such hazards are not always \nobvious and even experienced developers can miss them without exhaustive testing. Second, debugging nondeterministic \nparsers can be very di.cult. With bottom-up parsing, the state usually repre\u00adsents multiple locations \nwithin the grammar, making it dif\u00ad.cult for programmers to predict what will happen next. Top-down parsers \nare easier to understand because there is a one-to-one mapping from LL grammar elements to parser operations. \nFurther, recursive-descent LL implementations allow programmers to use standard source-level debuggers \nto step through parsers and embedded actions, facilitating understanding. In contrast, there is no easy-to-read \ncounter\u00adpart for the state machines derived from LR grammars. This advantage is weakened signi.cantly, \nhowever, for backtrack\u00ading recursive-descent packrat parsers. Nested backtracking is very di.cult to \nfollow!  Third, generating high-quality error messages in nonde\u00adterministic parsers is di.cult but very \nimportant to com\u00admercial developers. GLR parsers pursue all possible paths emanating from LR states with \ncon.icts. If no parse at\u00adtempt succeeds, the parser has to .gure out or guess the parse intended by the \nuser to emit a meaningful message. (One way is to pick the failed parse that shifted the most tokens.) \nPackrat parsers have the same issue since they are always speculating. In fact, they cannot recover from \nsyntax errors because they cannot detect errors until they have seen the entire input. Reducing uncertainty \nduring the parse is the key to recovering well from erroneous input. Determin\u00adistic LL and LR parsers \nhandle erroneous input much better than packrat and GLR parsers. Finally, nondeterministic parsing strategies \ncannot easily support arbitrary, embedded grammar actions, which are useful for manipulating symbol tables, \nconstructing data structures, etc. Speculating parsers cannot execute side\u00ade.ecting actions like print \nstatements, since the speculated action may never really take place. Even side-e.ect free actions such \nas those that compute rule return values can be awkward in GLR parsers [12]. For example, since the parser \ncan match the same rule in multiple ways, it might have to execute multiple competing actions. (Should \nit merge all results somehow or just pick one?) GLR and PEG tools address this issue by either disallowing \nactions, disallowing arbitrary actions, or relying on the programmer to avoid side-e.ects in actions \nthat could be executed speculatively. 1.1 ANTLR This paper describes version 3.3 of the ANTLR parser \ngen\u00aderator and its underlying top-down parsing strategy, called LL(*), that address these de.ciencies. \nThe input to ANTLR is a context-free grammar augmented with syntactic [17] and semantic predicates and \nembedded actions. Syntactic pred\u00adicates allow arbitrary lookahead, while semantic predicates allow the \nstate constructed up to the point of a predicate to direct the parse. Syntactic predicates are given \nas a gram\u00admar fragment that must match the following input. Semantic predicates are given as arbitrary \nBoolean-valued code in the host language of the parser. Actions are written in the host\u00adlanguage of the \nparser and have access to the current state. As with PEGs, ANTLR requires programmers to avoid left\u00adrecursive \ngrammar rules. The contributions of this paper are 1) the top-down pars\u00ading strategy LL(*) and 2) the \nassociated static grammar analysis algorithm that constructs LL(*) parsing decisions from ANTLR grammars. \nThe key idea behind LL(*) parsers is to use regular-expressions rather than a .xed constant or backtracking \nwith a full parser to do lookahead. The analy\u00adsis tries to construct a deterministic .nite automaton \n(DFA) for each nonterminal in the grammar to distinguish between alternative productions. If the analysis \ncannot .nd a suitable DFA for a nonterminal, it fails over to backtracking. As a re\u00adsult, LL(*) parsers \ngracefully throttle up from conventional .xed k = 1 lookahead to arbitrary lookahead and, .nally, fail \nover to backtracking depending on the complexity of the parsing decision. Even within the same parsing \ndecision, the parser decides on a strategy dynamically according to the input sequence. Just because \na decision might have to scan arbitrarily far ahead or backtrack does not mean that it will at parse-time \nfor every input sequence. In practice, LL(*) parsers only look one or two tokens ahead on average despite \nneeding to backtrack occasionally (Section 6). LL(*) parsers are LL parsers with supercharged decision \nengines. This design gives ANTLR the advantages of deterministic top-down parsing without the downsides \nof frequent specu\u00adlation. In particular, ANTLR accepts all but left-recursive context-free grammars so, \nas with PEG parsing, program\u00admers do not have to spend a lot of time contorting their grammars to .t \nthe parsing strategy. The lack of left re\u00adcursion is a disadvantage, though, because some very com\u00admon \nconstructs, such as arithmetic expressions, are awkward without left recursion. The next major release \nof ANTLR will allow rules with immediate left-recursion (self-referential rules) while still supporting \nthe usual tree construction operators and arbi\u00adtrary actions. Following a technique similar to Hansen \n[9], our prototype replaces left-recursion with a predicated loop that compares the precedence of the \nprevious and the next operator. For example, here is an expression rule that is LR but not LL because \nof the left recursion: e :e * e |e + e | INT ; Supporting such a grammar is a matter of identifying the \noperators and rewriting it to the equivalent predicated LL(*) grammar: e : e_[0] ; e_[int p] // pass \nin precedence of operator(s) to match : INT ( {p <= 2}? * e_[3] | {p <= 1}? + e_[2] )* ; The mechanism \nis su.ciently general to support su.x, pre.x, binary, and ternary operators. Operator precedence follows \nthe order of the alternatives, highest to lowest. Unlike GLR or PEGs, ANTLR can statically iden\u00adtify \nsome grammar ambiguities and dead productions. ANTLR generates top-down, recursive-descent, mostly non\u00adspeculating \nparsers, which means it supports source-level debugging, produces high-quality error messages, and al\u00adlows \nprogrammers to embed arbitrary actions. A survey of 89 ANTLR grammars [1] available from sourceforge.net \nand code.google.com reveals that 75% of them had em\u00adbedded actions, counting conservatively, which reveals \nthat such actions are a useful feature in the ANTLR community. Widespread use shows that LL(*) .ts within \nthe pro\u00adgrammer comfort zone and is e.ective for a wide vari\u00adety of language applications. ANTLR 3.x \nhas been down\u00adloaded 41,364 (binary jar .le) + 62,086 (integrated into ANTLRworks) + 31,126 (source code) \n= 134,576 times ac\u00adcording to Google Analytics (unique downloads January 9, 2008 -October 28, 2010). \nProjects using ANTLR include Google App Engine (Python), IBM Tivoli Identity Man\u00adager, BEA/Oracle WebLogic, \nYahoo! Query Language, Ap\u00adple XCode IDE, Apple Keynote, Oracle SQL Developer IDE, Sun/Oracle JavaFX language, \nand NetBeans IDE.  Figure 1. LL(*) lookahead DFA for rule s. Notation sn => i means predict the ith \nalternative. This paper is organized as follows. We .rst introduce ANTLR grammars by example (Section \n2). Next we for\u00admally de.ne predicated grammars and a special subclass called predicated LL-regular grammars \n(Section 3). We then describe LL(*) parsers (Section 4), which implement pars\u00ading decisions for predicated \nLL-regular grammars. Next, we give an algorithm that builds lookahead DFA from ANTLR grammars (Section \n5). Finally, we support our claims regard\u00ading LL(*) e.ciency and reduced speculation (Section 6). 2. \nIntroduction to LL(*) In this section, we give an intuition for LL(*) parsing by ex\u00adplaining how it works \nfor two ANTLR grammar fragments constructed to illustrate the algorithm. Consider nontermi\u00adnal s, which \nuses the (omitted) nonterminal expr to match arithmetic expressions. s :ID | ID = expr | unsigned * int \nID | unsigned * ID ID ; Nonterminal s matches an identi.er (ID), an ID followed by an equal sign and \nthen an expression, zero or more occurrences of the literal unsigned followed by the literal int followed \nby an ID, or zero or more occurrences of unsigned followed by two IDs. ANTLR grammars use yacc-like syntax \nwith extended BNF (EBNF) operators such as Kleene star (*) and token literals in single quotes. When \napplied to this grammar fragment, ANTLR s grammar analysis yields the LL(*) lookahead DFA in Fig\u00adure \n1. At the decision point for s, ANTLR runs this DFA on the input until it reaches an accepting state, \nwhere it selects the alternative for s predicted by the accepting state. Even though we need arbitrary \nlookahead to distinguish between the 3rd and 4th alternatives, the lookahead DFA uses the minimum lookahead \nper input sequence. Upon int from input int x, the DFA immediately predicts the third alternative (k \n= 1). Upon T (an ID) from Tx, the DFA needs to see the k = 2 token to distinguish alternatives 1, 2, \nand 4. It is only upon unsigned that the DFA needs to Figure 2. LL(*) parsing decision DFA for rule \nt using mixed k = 3 lookahead and backtracking scan arbitrarily ahead, looking for a symbol (int or ID) \nthat distinguishes between alternatives 3 and 4. The lookahead language for s is regular, so we can match \nit with a DFA. With recursive rules, however, we usually .nd that the lookahead language is context-free \nrather than regular. In this case, ANTLR fails over to backtracking if the programmer has requested this \nfeature by adding syntactic predicates. As a convenience, option backtrack=true auto\u00admatically inserts \nsyntactic predicates into every production, which we call PEG mode because it mimics the behavior of \nPEG parsers. However, before resorting to backtracking, ANTLR s analysis algorithm builds a DFA that \nadds a few extra states that allow it avoid backtracking for many input cases. In the following rule \nt, both alternatives can start with an arbitrary number of -negation symbols; the second alternative \ndoes so using recursive rule expr. options {backtrack=true;} // auto-insert syntactic preds t : - * ID \n| expr ; ; expr: INT | - expr ; Figure 2 shows the lookahead DFA that ANTLR constructs for this input. \nFor clarity, syntactic predicate edges are writ\u00adten in English. This DFA can immediately choose the ap\u00adpropriate \nalternative upon either input x or 1; by looking at just the .rst symbol. Upon -symbols, the DFA matches \na few -before failing over to backtracking. The number of times ANTLR unwinds the recursive rule before \nbacktrack\u00ading is controlled by an internal constant m, which we set to 1 for this example. Despite the \npossibility of backtracking, the decision will not backtrack in practice unless the input starts with \n-- , an unlikely expression pre.x. LL parsers need more lookahead in general than LR parsers do because \nLL parsers must predict which produc\u00adtion will succeed whereas LR parsers make decisions af\u00adter examining \nentire productions. For example, both gram\u00admars shown so far are LR(1) but not LL(k) for any .xed k, \ndemonstrating that LL(*) s arbitrary lookahead signi.\u00adcantly increases LL s recognition strength. In \nfact, there is no strict ordering between LR(k) and LL(*). Nonterminal a in the following grammar is \nLL(*) but not LR(k) for any k. a : b A+ X // token vocabulary = {A, X, Y}| c A+ Y ; b:; c:; The LPG (2.0.20) \nLALR(k) parser generator [3] demon\u00adstrates this by reporting a con.ict even at k = 10000:  t.g:8:1:8:1:51:51: \nWarning: Grammar is not LALR(10000) \u00adit contains 1 reduce/reduce conflicts. Lookahead depth k = 100000 \nexposes the exponential space requirements, forcing LPG to core dump after 1.2 minutes on the OS X box \nused in Section 6. In contrast, ANTLR quickly creates the following cyclic DFA that distinguishes between \na s productions. (0.7 seconds to analyze and generate the LL(*) parser and DFA.) 3. Predicated Grammars \nTo describe LL(*) parsing precisely, we need to .rst formally de.ne the predicated grammars from which \nthey are derived. A predicated grammar G =(N, T, P, S, ., M) has elements: N is the set of nonterminals \n(rule names)  T is the set of terminals (tokens)  P is the set of productions  S . N is the start \nsymbol  . is a set of side-e.ect-free semantic predicates  M is a set of actions (mutators)  Predicated \ngrammars are written using the notation shown in Figure 3. Productions are numbered to express precedence \nas a means to resolve ambiguities. The .rst pro\u00adduction form represents a standard context-free grammar \nrule. The second denotes a production gated by a syntactic predicate: symbol A expands to ai only if \nthe current input also matches the syntax described by A'i. Syntactic pred\u00adicates enable arbitrary, programmer-speci.ed, \ncontext-free lookahead. The third form denotes a production gated by a semantic predicate: symbol A expands \nto ai only if the predicate pi holds for the state constructed so far. The .nal form denotes an action: \napplying such a rule updates the state according to mutator \u00b5i. The derivation rules in Figure 4 de.ne \nthe meaning of a predicated grammar. To support semantic predicates and mutators, the rules reference \nstate S, which abstracts user state during parsing. To support syntactic predicates, the rules reference \nwr, which denotes the input remaining to be . matched. The judgment form (S,a) . (S',\u00df), may be read: \nIn machine state S, grammar sequence a reduces in one step to modi.ed state S' and grammar sequence \u00df \nwhile emitting b trace .. The judgment (S,a)= ..* (S',\u00df) denotes repeated applications of the one-step \nreduction rule, accumulating all actions in the process. We omit . when it is irrelevant to the discussion. \nThese reduction rules specify a leftmost derivation. A production with a semantic predicate pi can .re \nonly if pi is true of the current state S. A production with syntactic predicate A'i can .re only if \nthe string derived from A'i in the current state is a pre.x of the remaining input, written w -wr. Actions \nthat occur during the attempt to parse A'i are executed speculatively. They are undone whether or not \nA'i matches. Finally, an action production uses the speci.ed mutator \u00b5i to update the state. Formally, \nthe language generated by grammar sequence a is L(S,a)= {w | (S,a) . * (S',w)} and the language of A \n. N Nonterminal a . T Terminal X . (N . T ) Grammar symbol a, \u00df, d . X * Sequence of grammar symbols \nu, x, y, w . T * Sequence of terminals wr . T * Remaining input terminals E Empty string p . . Predicate \nin host language \u00b5 .M Action in host language . . (N . . .M) Reduction label . . = .1...n Sequence of \nreduction labels Production Rules: ith A . ai context-free production of A ith A . (A'i)=> ai production \npredicated on syntax A'i A .{pi}? ai ith production predicated on semantics A .{\u00b5i} ith production with \nmutator Figure 3. Predicated Grammar Notation A . aA .{\u00b5} Prod Action (S, uAd) . (S, uad) \u00b5 (S, uAd) \n. (\u00b5(S), ud) (S,A'i) . * (S',w) pi(S) w -wr A .{pi}? ai A . (A'i)=>ai Sem Syn pi A' (S, uAd)= . (S, uaid) \ni (S, uAd)= . (S, uaid) b (S,a)= .. (S,a'), (S,a')= .. * (S,\u00df) Closure b .. (S,a)= .* (S,\u00df) Figure 4. \nPredicated Grammar Leftmost Derivation Rules grammar G is L(G)= {w | (E, S) . * (S,w)}. Theoretically, \nthe language class of L(G) is recursively enumerable because each mutator could be a Turing machine. \nIn practice, gram\u00admar writers do not use this generality, and so we consider the language class to be \nthe context-sensitive languages in\u00adstead. The class is context-sensitive rather than context-free because \npredicates can check both the left and right context. This formalism has various syntactic restrictions \nnot present in actual ANTLR input, for example, forcing predi\u00adcates to the left-edge of rules and forcing \nmutators into their own rules. We can make these restrictions without loss of generality because any \ngrammar in the general form can be translated into this more restricted form [1]. One of the key concepts \nbehind parsing is the language matched by a production at a particular point in the parse. Definition \n1. C(a)= {w | (E, S) . * (S, uad) . * (S', uw)}is the continuation language for production a. Finally, \ngrammar position a . \u00df means after a but before \u00df during generation or parsing. 3.1 Resolving ambiguity \nAn ambiguous grammar is one in which the same string may be recognized in multiple ways. The rules in \nFigure 4 do not preclude ambiguity. However, for a practical parser, we want each input to correspond \nto a unique parse. To that end, ANTLR uses the order of the productions in the grammar to resolve ambiguities, \nwith con.icts resolved in favor of the rule with the lowest production number. Programmers are instructed \nto make semantic predicates mutually exclusive for all potentially ambiguous input sequences, making \nsuch productions unambiguous. However, that condition cannot be enforced because predicates are written \nin a Turing\u00adcomplete language. If the programmer fails to satisfy this condition, ANTLR uses production \norder to resolve the ambiguity. This policy matches what is done in PEGs [6, 8] and is useful for concisely \nrepresenting precedence.  3.2 Predicated LL-regular grammars There is one .nal concept that is helpful \nin understanding the LL(*) parsing framework, namely, the notion of a pred\u00adicated LL-regular grammar. \nIn previous work, Jarzabek and Krawczyk [10] and Nijholt [15] de.ne LL-regular grammars to be a particular \nsubset of the non-left-recursive, unambigu\u00adous CFGs. In this work, we extend the notion of LL-regular \ngrammars to predicated LL-regular grammars for which we will construct e.cient LL(*) parsers. We require \nthat the input grammar be non-left-recursive; we use rule ordering to ensure that the grammar is unambiguous. \nLL-regular grammars di.er from LL(k) grammars in that, for any given nonterminal, parsers can use the \nentire re\u00admaining input to di.erentiate the alternative productions rather than just k symbols. LL-regular \ngrammars require the existence of a regular partition of the set of all terminal sequences for each nonterminal \nA. Each block of the parti\u00adtion corresponds to exactly one possible production for A. An LL-regular parser \ndetermines to which regular set the remaining input belongs and selects the corresponding pro\u00adduction. \nFormally, Definition 2. Let R =(R1,R2,...,Rn) be a partition of T * into n nonempty, disjoint sets Ri. \nIf each block Ri is regular, R is a regular partition. If x, y . Ri, we write x = y (mod R). Definition \n3. G is predicated LL-regular if, for any two alternative productions of every nonterminal A expanding \nto ai and aj , there exists regular partition R such that (E, S) . * (S,wiAdi) . (S,wiaidi) . * (Si,wix) \n(1) (E, S) . * (S,wj Adj ) . (S,wj aj dj ) . * (Sj ,wj y) (2) x = y (mod R) (3) always imply that ai \n= aj and Si = Sj . 1 4. LL(*) Parsers Existing parsers for LL-regular grammars, proposed by Ni\u00adjholt \n[15] and Poplawski [18], are linear but often impractical because they cannot parse in.nite streams such \nas socket protocols and interactive interpreters. In the .rst of two passes, these parsers must read \nthe input from right to left. Instead, we propose a simpler left-to-right, one-pass strat\u00adegy called \nLL(*) that grafts lookahead DFA onto LL parsers. A lookahead DFA matches regular partition R associated \nwith a speci.c nonterminal and has an accept state for each Ri. At a decision point, LL(*) parsers expand \nproduction i if Ri matches the remaining input. In the worst case, an LL(*) parser examines the remaining \ninput at each sym\u00adbol as it consumes the input. As a result, LL(*) parsers are O(n 2), but in practice, \nthey typically examine one or two tokens (Section 6). As with previous parsing strategies, an 1 To be \nstrictly correct, this de.nition technically corresponds to Strong LL-regular, rather than LL-regular \nas Nijholt [15] points out. Strong LL parsers ignore left context when making decisions. api p -pi(S) \np -. fi . q (S, p, aw) . (S, q, w) pi (S, p, w) . (S,fi,w) (S,fi,w) Accept, predict production i Figure \n5. Lookahead DFA Con.guration Change Rules LL(*) parser exists for every LL-regular grammar. Unlike previous \nwork, LL(*) parsers can take as input a predicated LL-regular grammar; they handle predicates by inserting \nspecial edges into the lookahead DFA that correspond to the predicates. Definition 4. Lookahead DFA are \nDFA augmented with predicates and accept states that yield predicted produc\u00adtion numbers. Formally, given \npredicated grammar G = (N, T, P, S, ., M), DFA M =(S, Q, S, .,D0,F ) where: S is the system state inherited \nfrom surrounding parser  Q is the set of states  S= T . . is the edge alphabet  . is the transition \nfunction mapping Q \u00d7 S . Q  D0 . Q is the start state  F = {f1,f2,...,fn} is the set of .nal states, \nwith one fi . Q per regular partition block Ri (production i)  A transition in . from state p to state \nq on symbol a . S has a the form p . q. There can be at most one such transition. p Predicate transitions, \nwritten p . fi, must target a .nal state. There can be multiple predicate transitions emanating from \np but each must have a unique predicate p. The instantaneous con.guration c of theDFA is (S, p, wr) where \nS is the system state and p is the current state; the initial con.guration is (S,D0,wr). The notation \nc . c ' means the DFA changes from con.guration c to c ' using the rules in Figure 5. As with predicated \ngrammars, the rules do not forbid ambiguous DFA paths arising from predicated transitions. In practice, \nANTLR tests edges in order to resolve ambiguities. For e.ciency, lookahead DFA match lookahead sets rather \nthan continuation languages. Given R =({ac * }, {bd * }), for example, there is no point in looking beyond \nthe .rst symbol. The lookahead sets are ({a}, {b}). Definition 5. Given partition R distinguishing n \nalterna\u00adtive productions, the lookahead set for production i is the minimal-pre.x set of Ri that still \nuniquely predicts i: LAi = { w | ww ' . Ri,w /. LAj for j = i and no strict pre.x of w has the same property} \n4.1 Erasing syntactic predicates To avoid a separate recognition mechanism for syntactic predicates, \nwe reduce syntactic predicates to semantic pred\u00adicates that launch speculative parses. To erase syntac\u00adtic \npredicate (Ai' )=>, we replace it with semantic predi\u00adcate {synpred(A ' i)}?. Function synpred returns \ntrue if A ' i matches the current input; otherwise it returns false. To sup\u00adport PEG not predicates, \nwe can .ip the result of calling function synpred, as suggested by Ford [7].  4.2 Programmer-speci.ed \nsemantic predicates Most grammars do not need semantic predicates, but when they do, the predicates are \nusually small expressions that query a symbol table, check context (e.g., does this token start in column \n1? ), or turn on language constructs (e.g., should the language allow GCC C extensions? ). They do not \nrepresent a signi.cant cost to the programmer. For example, the ANTLR C grammar has just one predicate \nthat tests whether or not an identi.er is a type name:  type id : {isTypeName(next input symbol)}? ID \n;  4.3 Arbitrary actions in predicate grammars Formal predicated grammars fork new states S during spec\u00adulation. \nIn practice, duplicating system state is not feasible. Consequently, ANTLR deactivates mutators during \nspecu\u00adlation by default, preventing actions from launching mis\u00adsiles speculatively. However, some semantic \npredicates rely on changes made by mutators, such as the symbol table manipulations required to parse \nC. Avoiding speculation whenever possible attenuates this issue, but still leaves a semantic hazard. \nTo address this issue, ANTLR supports a special kind of action, enclosed in double brackets {{...}}, \nthat executes even during speculation. ANTLR requires the programmer to verify that these actions are \neither side-e.ect free or undoable. It is also the responsibility of the program\u00admer to insert actions \nthat undo side-e.ects. Luckily, symbol table manipulation actions, the most common {{...}} ac\u00adtions, \nusually get undone automatically. For example, a rule for a code block typically pushes a symbol scope \nbut then pops it on exit. The pop e.ectively undoes the side-e.ects that occur during code block.  4.4 \nError reporting and arbitrary lookahead LL(k) parsers report prediction errors by indicating the k\u00adsequence \nthat failed to predict a production of the current nonterminal. For example, given A . ab | ac and input \nae, an LL(k) parser might report no viable production upon ae. Because LL(*) prediction DFA can scan \narbitrarily far ahead, printing the entire erroneous lookahead sequence is impractical. Instead, LL(*) \nparsers should report an error at the speci.c token that led to a lookahead DFA error state. Given A \n. a +b | a + c and input aaaaad, the parser should report that it could not predict a production due \nto d. Reporting an error at the current symbol (the .rst a) would be confusing. For prediction errors \nwhen backtracking across multiple productions, the parser should report errors at the deepest symbol \nreached by a failed speculative parse. ANTLR 3.3 does not always conform to this ideal; we plan to address \nthis in a future version. 5. LL(*) Grammar Analysis For LL(*), analyzing a grammar means .nding a lookahead \nDFA for each parsing decision, i.e., for each nonterminal in the grammar with multiple productions. In \nour discussions, we use A as the nonterminal in question and ai for i . 1..n as the corresponding collection \nof right-hand sides. Our goal is to .nd for each A a regular partition R, represented by a DFA, that \ndistinguishes between productions. To succeed, A must be LL-regular: partition block Ri must contain \nevery sentence in C(ai), the continuation language of ai, and the Ri must be disjoint. The DFA tests \nthe remaining input for membership in each Ri; matching Ri predicts alternative i. For e.ciency, the \nDFA matches lookahead sets instead of partition blocks. It is important to point out that we are not \nparsing with the DFA, only predicting which production the parser should expand. The continuation language \nC(ai) is often context- Figure 6. ATN for G with P ={S . Ac | Ad, A . aA | b} free, not regular, but \nexperience shows there is usually an ap\u00adproximating regular language that distinguishes between the ai. \nFor example, consider rule A . [ A ] | id that matches balanced brackets around an identi.er, i.e., the \ncontext-free language {[nid ]n}. Approximating the C(ai) sets with regu\u00adlar expressions gives a partition \nthat satis.es the LL-regular condition: R = {{[ * id ] * }, {id}}. In fact, the .rst input sym\u00adbol is \nsu.cient to predict alternatives: LA = {{[}, {id}}. The decision is LL(1). Not all grammars are LL-regular \nand so our algorithm may fail to .nd a partition for A. Worse, Poplawski [18] showed that the LL-regular \ncondition is undecidable so we must use heuristics to prevent nontermination, sometimes forcing the algorithm \nto give up before .nding R even when A is LL-regular. In such cases, we fall back on other strategies, \ndiscussed in Sections 5.3 and 5.4, rather than failing to create a DFA. The LL(*) analysis algorithm \nstarts by converting the in\u00adput grammar to an equivalent augmented transition network (ATN) [20]. It \nthen computes lookahead DFA by simulating the actions of the ATN in a process that mimics how the well-known \nsubset construction algorithm computes a DFA that simulates the actions of an NFA. 5.1 Augmented transition \nnetworks Given predicated grammar G =(N, T, P, S, ., M), the cor\u00adresponding ATN MG =(Q, S, ., E, F ) \nhas elements: Q is the set of states  S is the edge alphabet N . T . . .M  . is the transition relation \nmapping Q \u00d7 (S . E) . Q  E = {pA | A . N} is the set of submachine entry states  F = {p ' | A . N} \nis the set of submachine .nal states  A We describe how to compute Q and . shortly. ATNs resemble the \nsyntax diagrams used to document programming languages, with an ATN submachine for each nonterminal. \nFor example, Figure 6 gives the ATN for a A simple grammar. Nonterminal edges p -. p ' are like function \ncalls. They transfer control of the ATN to A s submachine, pushing return state p ' onto a state stack \nso it can continue from p ' after reaching the stop state for A s submachine. To get an ATN from a grammar, \nwe create a submachine for each nonterminal A as shown in Figure 7. Start state pA targets pA,i created \nfrom the left edge of ai. The last state created from ai targets pA' . The language matched by the ATN \nis the same as the language of the original grammar. Grammar analysis is like an inter-procedural .ow \nanalysis that statically traces an ATN-like graph representation of a program, discovering all nodes \nreachable from a top-level call site. The unique con.guration of a program for .ow purposes is a graph \nnode and the call stack used to reach  Input Grammar Element Resulting ATN Transitions A . ai pA E-. \npA,i E-. ai E-. p ' A A . {pi}? ai pA E-. pA,i pi-. ai E-. p ' A A . {\u00b5i} pA E-. pA,i \u00b5i-. p ' A A . \nE pA E-. pA,i E-. p ' A ai = X1X2 . . . Xm for Xj . N . T, j = 1..m p0 X1- . p1 X2- . . . . Xm--. pm \n Figure 7. Predicated Grammar to ATN transformation that node. Depending on the type of analysis, it \nmight also track some semantic context such as the parameters from the top-level call site. Similarly, \ngrammar analysis statically traces paths through the ATN reachable from the call site of production ai, \nwhich is the left edge state pA,i. The terminal edges collected along a path emanating from pA,i represent \na lookahead se\u00adquence. Analysis continues until each lookahead sequence is unique to a particular alternative. \nAnalysis also needs to track any semantic predicate pi from the left edge of ai in case it is needed \nto resolve ambiguities. Consequently, an ATN con.guration is a tuple (p, i, ., p) with ATN state p, predicted \nproduction i, ATN call stack ., and optional predicate p. We will use the notation c.p, c.i, c.., and \nc.p to denote projecting the state, alternative, stack, and predicate from con.guration c, respectively. \nAnalysis ignores machine storage S because it is unknown at analysis time.  5.2 Modi.ed subset construction \nalgorithm For grammar analysis purposes, we modify subset construc\u00adtion to process ATN not NFA con.gurations. \nEach DFA state D represents the set of possible con.gurations the ATN could be in after matching a pre.x \nof the remaining input starting from state pA,i. Key modi.cations include: The closure operation simulates \nthe push and pop of ATN nonterminal invocations.  If all the con.gurations in a newly discovered state \npre\u00addict the same alternative, the analysis does not add the state to the work list; no more lookahead \nis necessary.  To resolve ambiguities, the algorithm adds predicate transitions to .nal states if appropriate \npredicates exist.  The structure of the algorithm mirrors that of subset con\u00adstruction. It begins by \ncreating DFA start state D0 and adding it to a work list. Until no work remains, the algo\u00adrithm adds \nnew DFA states computed by the move and clo\u00adsure functions, which simulate the transitions of the ATN. \nWe assume that the ATN corresponding to our input gram\u00admar G, MG =(QM ,N . T . . .M, .M ,EM ,FM ), and \nthe nonterminal A that we are analyzing are in scope for all the operations of the algorithm. Function \ncreateDFA, shown in Algorithm 8, is the entry point: calling createDFA(pA) constructs the lookahead DFA \nfor A. To create start state D0, the algorithm adds con.g\u00aduration (pA,i, i, [],pi) for each production \nA . pi ai and con.guration (pA,i, i, [], -) for each production A . ai; the symbol - denotes the absence \nof a predicate. The core of Alg. 8: createDFA(ATN State pA) returns DFA work := []; . := {}; D0 := {}; \nF := {fi | fi := new DFA state, 1 . . . numAlts(A)}; Q := F ; E foreach pA -do . pA,i . .M Epi if pA \n-- . pA,i . p then p := pi else p := -; D0 += closure(D0, (pA,i, i, [],p)); end work += D0; Q += D0; \nDF A := DF A(-, Q, T . ., .,D0,F ); foreach D . work do a TD := {a | p1 -. p2 . .M and (p1, -, -, -) \n. D}; foreach a . TD do mv := move(D, a); S D ' := closure(D, c); c.mv if D ' ./Q then resolve(D ' ); \nswitch findP redictedAlts(D ' ) do case Singleton list of j: fj := D ' ; otherwise work += D ' ; endsw \n Q += D ' ; end a . += D -. D ' ; end foreach c . D such that wasResolved(c) do c.p . += D - . fc.i; \nend work -= D; end return DFA; createDFA is a combined move-closure operation that cre\u00adates new DFA states \nby .nding the set of ATN states directly reachable upon each input terminal symbol a . T : a move(D, \na)= {(q, i, ., p) | p -. q, (p, i, ., p) . D} and then adding the closure of those con.gurations. Once \nthe algorithm identi.es a new state D ' , it invokes resolve to check for and resolve ambiguities. If \nall of the con.gurations in D ' predict the same alternative j, then D ' is marked as fj , the accept \nstate for alternative j, and D ' is not added to the work list: once the algorithm can uniquely identify \nwhich production to predict, there is no point in examining more of the input. This optimization is how \nthe algorithm con\u00adstructs DFA that match the minimum lookahead sets LAj instead of the entire remaining \ninput. Next, the algorithm adds an edge from D to D ' on terminal a. Finally, for each con.guration c \n. D with a predicate that resolves an am\u00adbiguity, createDFA adds a transition predicated on c.p from \nD to the .nal state for alternative c.i. The test wasResolved checks whether the resolve step marked \ncon.guration c as having been resolved by a predicate. Closure The LL(*) closure operation, shown in \nAlgo\u00adrithm 9, is more complex than the closure function from NFA subset construction because of the ATN \nstack. Never\u00adtheless, the intuition is the same. When called on a con.g\u00aduration c, closure recursively \n.nds all ATN states reachable from c s state by traversing all ATN edges that are not ter\u00adminals, i.e., \npredicates, nonterminals, and mutators.  Alg. 9: closure(DFA State D, c =(p, i, ., p)) returns set closure \nif c . D.busy then return {}; else D.busy += c; closure := {c}; if p = p ' (i.e., p is stop state) then \n A ' . ' if . = p then closure += closure(D, (p ' , i, . ' ,p)); S else closure += closure(D, (p2, i, \n[],p)); A . p2 : p1-.p2..M end foreach transition t emanating from ATN state p do switch t do A case \np -. p ' : depth := number of occurrences of p ' in .; if depth =1 then D.recursiveAlts += i; if |D.recursiveAlts| \n> 1 then throw LikelyNonLLRegularException; end if depth = m,(i.e., the max recursion depth) then mark \nD to have recursion over.ow; return closure; end closure += closure(D, (pA, i, p ' ., p)); p ' \u00b5E case \np --- . q, p . q, or p . q transition: closure += closure(D, (q, i, ., p)); endsw end return closure; \nThe call closure(D, c) takes the DFA state D to which c belongs as an additional argument. The function \nstarts by adding the argument con.guration to a busy list to avoid redundant computation and in.nite \nloops. To simulate A ATN nonterminal transition p -. p ' , closure duplicates con.guration c, pushing \nreturn state p ' onto its stack. At submachine stop state pA' , closure duplicates c, popping p ' from \nits stack. If closure reaches pA with an empty stack, we have no information statically about which rule \ninvoked A. (This situation only happens when there is a path from pA,i to p ' with no terminal edges.) \nIn this case we have A A to assume any production p1 -. p2 in the input grammar might have invoked A, \nand so closure must chase all such states p2. If closure detects recursive nonterminal invocations (sub\u00admachines \ndirectly or indirectly invoking themselves) in more than one alternative, it terminates DFA construction \nfor A by throwing an exception; Section 5.4 describes our fall back strategy. If closure detects recursion \ndeeper than internal constant m, closure marks the state parameter D as having over.owed. In this case, \nDFA construction for A continues but closure no longer pursues paths derived from c s state and stack. \nWe discuss this situation more in Section 5.3. DFA State Equivalence The analysis algorithm relies on \nthe notion of equivalence for DFA states: Definition 6. Two DFA states are equivalent, D = D ' , if their \ncon.guration sets are equivalent. Two ATN con.gura\u00adtions are equivalent, c = c ' , if the p, i, and p \ncomponents are equal and their stacks are equivalent. Two stacks are equiv\u00adalent, .1 = .2, if they are \nequal, if at least one is empty, or if one is a su.x of the other. This de.nition of stack equivalence \nre.ects ATN con\u00adtext information when closure encounters a submachine stop state. Because analysis searches \nthe ATN for all possible lookahead sequences, an empty stack is like a wildcard. Any transition p1 -Acould \nhave invoked A, so analysis must . p2 include the closure of every such p2. Consequently, a clo\u00adsure \nset can have two con.gurations c and c ' with the same ATN state but with c.. = E and c ' .. = E. This \ncan only happen when closure reaches a nonterminal s submachine stop state with an empty stack and by \nchasing states fol\u00adlowing references to that nonterminal, closure reenters that submachine. For example, \nconsider the DFA for S in gram\u00admar S . a|E, A . SS. Start state construction computes closure at positions \nS . . a and S . . E then S . E ., S s stop state. The state stack is empty so closure chases states following \nreferences to S, such as position A . S .S. Finally, closure reenters S, this time with a nonempty stack. \nEquivalence of .1 = .1.2 where .1,.2 = E degenerates to the previous case of .1 = E. Given con.gurations \n(p, ,.1, ) and (p, ,.1.2, ) in D, closure reaches p following the same sequence of most recent submachine \ninvocations, .1. Once .1 pops o., closure has con.gurations (p, , [], ) and (p, ,.2, ). Resolve One of \nbene.ts of static analysis is that it can sometimes detect and warn users about ambiguous nonter\u00adminals. \nAfter closure .nishes, the resolve function (Algo\u00adrithm 10) looks for con.icting con.gurations in its \nargu\u00adment state D. Such con.gurations indicate that the ATN can match the same input with more than one \nproduction. Definition 7. If DFA state D contains con.gurations c = ' (p, i, .i,pi) and c =(p, j, .j \n,pj ) such that i = j and .i = .j , then D is an ambiguous DFA state and c and c ' are con.icting con.gurations. \nThe set of all alternative numbers that belong to a con.icting con.guration of D is the con.ict set of \nD. For example, the ATN for subrule (a|a) in A . (a|a) b merges back together and so analysis reaches \nthe same state from both alternatives with the same (empty) stack context: D0 = {(p2, 1), (p5, 2)}, \nwhere we abbreviate (p2, 1, [], -) as (p2, 1) for clarity. D1 = {(p3, 1), (p4, 1), (p6, 2), (p4, 2)}, \nreachable with symbol a, has con.icting con.gurations (p4, 1) and (p4, 2). No further lookahead will \nresolve the ambiguity because ab is in the continuation language of both alternatives. If resolve detects \nan ambiguity, it calls resolveWithPred\u00adicate (Algorithm 11) to see if the con.icting con.gura\u00adtions have \npredicates that can resolve the ambiguity. For example, a predicated version of the previous grammar, \nA . ({p1}? a |{p2}? a) b, yields ATN:  Alg. 10: resolve(DFA State D) conflicts := the con.ict set of \nD; if |conflicts| =0 and not over.owed(D) then return; if resolveW ithP reds(D, conflicts) then return; \nresolve by removing all c from D such that c.i . conflicts and c.i = min(conflicts); if over.owed(D) \nthen report recursion over.ow; else report grammar ambiguity; Alg. 11: resolveWithPreds(DFA State D, \nset conflicts) returns boolean pconfigs := []; // con.g with predicate for alt i foreach i . conflicts \ndo pconfigs[i] := pick any representative ( ,i, ,p) . D; end if |pconfigs| < |conflicts| then return \nfalse; foreach c . pconfigs do mark c as wasResolved; return true; D1 has con.icting con.gurations as \nbefore, but now predi\u00adcates can resolve the issue at runtime with DFA D0 -a . D1, p1p2 D1 -. f1, D1 -. \nf2. If resolve found predicates, it returns without emitting a warning, leaving createDFA to incorporate \nthe predicates into the DFA. Without predicates, there is no way to re\u00adsolve the issue at runtime, so \nresolve statically removes the ambiguity by giving precedence to A s lowest con.ict\u00ading alternative by \nremoving con.gurations associated with higher-numbered con.icting alternatives. For example, in the unpredicated \ngrammar for A above, the resulting DFA a is D0 -. f1 because the analysis resolves con.icts by remov\u00ading \ncon.gurations not associated with highest precedence production 1, leaving {(p3, 1), (p4, 1)}. If closure \ntripped the recursion over.ow alarm, resolve may not see con.icting con.gurations in D, but D might still \npredict more than one alternative because the analysis terminated early to avoid nontermination. The \nalgorithm can use predicates to resolve the potential ambiguity at runtime, if they exist. If not, the \nalgorithm again resolves in favor of the lowest alternative number and issues a warning to the user. \n 5.3 Avoiding analysis intractability Because the LL-regular condition is undecidable, we expect a potential \nin.nite loop somewhere in any lookahead DFA construction algorithm. Recursive rules are the source of \nnontermination. Given con.guration c =(p, i, .) (we omit p from con.gurations for brevity in the following) \nthe closure A of c at transition p -. p ' includes (pA, i, p ' .). If closure reaches p again, it will \ninclude (pA, i, p ' p ' .). Ultimately, closure will pump the recursive rule forever, leading to stack \nexplosion. For example, for the ATN with recursive nonterminal A given in Figure 6, the DFA start state \nD0 for S . Ac | Ad is: D0 = {(p1, 1, []), (pA, 1,p2), (p7, 1,p2), (p10, 1,p2), (p4, 2, []), (pA, 2,p5), \n(p7, 2,p5), (p10, 2,p5)} Function move(D0,a) reaches (p8, 1,p2) to create D1 via (p7, 1,p2) in D0. The \nclosure of (p8, 1,p2) traverses the implied E edge to pA, adding three new con.gurations: (pA, 1,p9p2), \n(p7, 1,p9p2), (p10, 1,p9p2). D1 has the same con.guration as D0 for p7 but with a larger stack. The con\u00ad \nm .guration stacks grow forever as p9 p2 for recursion depth m, yielding an ever larger DFA path: D0 \n-a-a-a . D1 . ... . Dm. There are two solutions to this problem. Either we for\u00adget all but the top m \nstates on the stack (as Bermudez and Schimpf [2] do with LAR(m)) or simply avoid computing closure on \ncon.gurations with m recursive invocations to any particular submachine start state. We choose the lat\u00adter \nbecause it guarantees a strict superset of LL(k) (when m = k). We do not have to worry about an approximation \nintroducing invalid sequences in common between the alter\u00adnatives. In contrast, Bermudez and Schimpf \ngive a family of LALR(1) grammars for which there is no .xed m that gives a valid LAR(m) parser for every \ngrammar in the family. Hard-limiting recursion depth is not a serious restriction in practice. Programmers \nare likely to use (regular) grammar S . a * bc | a * bd instead of the version in Figure 6. 5.4 Aborting \nDFA construction As a heuristic, we terminate DFA construction for nonter\u00adminal A upon discovering recursion \nin more than one alter\u00adnative. (Analysis for S in Figure 6 would actually terminate before triggering \nrecursion over.ow.) Such decisions are ex\u00adtremely unlikely to have exact regular partitions and, since \nour algorithm does not approximate lookahead, there is no point in pursuing the DFA in vain. ANTLR s \nimplementa\u00adtion falls back on LL(1) lookahead for A, with backtracking or other predicates if resolve \ndetects recursion in more than one alternative.  5.5 Hoisting Predicates This algorithm and the formal \npredicated grammar seman\u00adtics in Section 3 require that predicates appear at production left edges. This \nrestriction is cumbersome in practice and can force users to duplicate predicates. The full algorithm \nin ANTLR automatically discovers and hoists all predicates visible to a decision even from productions \nfurther down the derivation chain. (See [1] for full algorithm and discus\u00adsion.) ANTLR s analysis also \nhandles EBNF operators in the right-hand side of productions, e.g. A . a * b by adding cycles to the \nATN. 6. Empirical Results This paper makes a number of claims about the suitabil\u00adity and e.ciency of \nour analysis algorithm and the LL(*) parsing strategy. In this section, we support these claims with \nmeasurements obtained by running ANTLR 3.3 on six large, real-world grammars, described in Figure 12, \nand pro\u00ad.ling the resulting parsers on large sample input sentences, described in Figure 13. We include \ntwo PEG-derived gram\u00admars to show ANTLR can generate valid parsers from PEGs. (Readers can examine the \nnon-commercial grammars, sam\u00adple input, test methodology, and raw analysis results [1].) 6.1 Static \ngrammar analysis We claim that our grammar analysis algorithm statically op\u00adtimizes away backtracking \nfrom the vast majority of parsing decisions and does so in a reasonable amount of time. Table 1 summarizes \nANTLR s analysis of the six grammars. ANTLR processes each of them in a few seconds except for the 8,231 \nline-SQL grammar, which takes 13.1 seconds. Anal\u00adysis times include input grammar parsing, analysis (with \nEBNF construct processing and predicate hoisting [1]), and parser generation. (As with the exponentially-complex \nclas\u00adsic subset construction algorithm, ANTLR s analysis can hit Java1.5: Native ANTLR grammar, uses \nPEG mode.  RatsC, RatsJava: Rats! grammars, manually converted to ANTLR syntax using PEG mode while \npreserving the essential structure. We removed left-recursion, which Rats! supports but ANTLR does not. \nVB.NET, TSQL, C#: Commercial grammars for Mi\u00adcrosoft languages provided by Temporal Wave, LLC. Figure \n12. Benchmark ANTLR grammars RatsJavaParser.java: Parser generated by ANTLR for RatsJava grammar; tests \nboth Java parsers. pre javaParser.c: Preprocessed ANTLR-generated parser for Java1.5 grammar; tests RatsC \nparser. LinqToSqlSamples.cs: Microsoft sample code; tests C#; big.sql: Collected Microsoft SQL samples; \ntests TSQL. Northwind.vb: Microsoft sample code; tests VB.NET. Figure 13. Benchmark input sentence Grammar \nLines n Fixed Cyclic Backtrack Runtime Java1.5 1,022 170 150 1 20 (11.8%) 3.1s RatsC 1,174 143 111 0 \n32 (22.4%) 2.8s RatsJava 763 87 73 6 8 (9.2%) 3s VB.NET 3,505 348 332 0 16 (4.6%) 6.75s TSQL 8,241 1,120 \n1,053 10 57 (5.1%) 13.1s C# 3,476 217 189 2 26 (12%) 6.3s Table 1. Grammar decision characteristics. \nLines is the size of the grammar, n is the number of parsing decisions in the grammar, Fixed is the number \nof pure LL(k) decisions, Cyclic is the number of cyclic DFA falling between LL(k) s acyclic DFA and DFA \nwith syntactic predicate edges, Back\u00adtrack is the number of decisions that potentially backtrack, and \nthe runtime is the time to process a grammar and gen\u00aderate an LL(*) parser. Tests performed under OS \nX with 2x3.2Ghz Quad-Core Intel Xeon with 14G RAM. a land-mine in rare cases; ANTLR provides a means \nto iso\u00adlate the o.ending decisions and manually set their lookahead parameters. None of these grammars \nhits land-mines.) All the grammars use backtracking to some extent, pro\u00adviding evidence that it is not \nworth contorting a large gram\u00admar to make it LL(k). The .rst three grammars use PEG mode, in which ANTLR \nautomatically puts a syntactic pred\u00adicate on the left edge of every production. Unlike a PEG parser, \nhowever, ANTLR can statically avoid backtracking in many cases. For example, ANTLR strips away syntactic \npredicates from all but 11.8% of the decisions in Java1.5. As expected, the RatsC grammar has the highest \nratio of back\u00adtracking decisions at 22.4% because C variable and function declarations and de.nitions \nlook the same from the left edge. The author of the three commercial grammars manually\u00adspeci.ed syntactic \npredicates, which reduced lookahead re\u00adquirements. In 4 out of 6 grammars, ANTLR was able to construct \ncyclic DFA to avoid backtracking. Table 1 shows that the vast majority of decisions in the sample grammars \nare .xed LL(k) for some k. Table 2 reports the number of decisions per lookahead depth, showing that \nmost decisions are in fact LL(1). The table also shows that Grammar LL(k) LL(1) Look ahea d depth k 1 \n2 3 4 5 6 Java1.5 88.24% 74.71% 127 20 2 1 RatsC 77.62% 72.03% 103 7 1 RatsJava 83.91% 73.56% 64 8 1 \nVB.NET 95.40% 88.79% 309 18 4 1 TSQL 94.02% 83.48% 935 78 11 14 9 6 C# 87.10% 78.34% 170 19 Table 2. \nFixed lookahead decision characteristics Grammar Input parse-n avg back. max lines time kk k Java1.5 \n12,416 78ms 111 1.09 3.95 114 RatsC 37,019 771ms 131 1.88 5.87 7,968 RatsJava 12,416 412ms 78 1.85 5.95 \n1,313 VB.NET 4,649 351ms 166 1.07 3.25 12 TSQL 794 13ms 309 1.08 2.63 20 C# 3,807 524ms 146 1.04 1.60 \n9 Table 3. Parser decision lookahead depth. n is the number of decision points covered while parsing. \navg k is the sum of all decision event lookahead depths divided by the number of decision events. back. \nk is the average speculation depth for backtracking decision events only. max k is the deepest lookahead \ndepth encountered during the parse. LL(*) parsers were run with Java 1.6.0 under OS X with 2x3.2Ghz Quad-Core \nIntel Xeon with 14G RAM. VB.NET time includes preprocessing. All times include lexing. Times re.ect prior \non-the-.y compiler warm-up. Parse times for last 3 include tree construction. ANTLR is able to statically \ndetermine k almost all the time, even though this problem is undecidable in general.  6.2 Parser runtime \npro.ling At runtime, we claim that LL(*) parsing decisions use only a few tokens of lookahead on average \nand parse with reason\u00adable speed. Table 3 shows that for each sample input .le, the average lookahead \ndepth per decision event is roughly one token, with PEG-mode parsers requiring almost two. The average \nparsing speed is 71,035 lines / second for the .rst three grammars and 27,196 for the other grammars, \nwhich have tree-building overhead. The average lookahead depth for just the backtracking decisions is \nless than six tokens, highlighting the fact that although backtracking can scan far ahead, usually it \ndoes not. For example, a parser might need to backtrack to match a C type speci.er, but most of the time \nsuch speci.ers look like int or int * and so they can be identi.ed quickly. The grammars not derived \nfrom PEGs have much smaller maxi\u00admum lookahead depths, indicating the authors did some re\u00adstructuring \nto take advantage of LL(k) e.ciency. The RatsC grammar, in contrast, backtracks across an entire function \n(looking ahead 7,968 tokens in one decision event) instead of looking ahead just enough to distinguish \ndeclarations from de.nitions, i.e., between int f(); and int f() {...}. Although we derived two sample \ngrammars from Rats!, we did not compare parser execution times nor memory uti\u00adlization as one might expect. \nBoth ANTLR and Rats! are practical parser generators with parsing speed and memory footprints suitable \nto most applications. Rats! is also scan\u00adnerless, unlike ANTLR, which makes memoization caches hard to \ncompare.  Grammar Can Did decision Back- Back. back. back. events track rate Java1.5 19 16 462,975 2.36% \n45.22% RatsC 30 24 1,343,176 16.85% 65.27% RatsJava 8 7 628,340 14.07% 74.68% VB.NET 6 3 109,257 0.46% \n20.84% TSQL 29 19 17,394 3.38% 27.01% C# 24 19 141,055 3.68% 40.22% Table 4. Parser decision backtracking \nbehavior. Can back. is the number of decisions that potentially backtrack. Did back. is the number of \nthose that did for the sample input. Backtrack is the percentage of decision events that backtracked. \nBack. rate is the likelihood that a potentially backtracking decision actually backtracks when triggered. \nANTLR statically removes backtracking from most parser decisions as shown in Table 1. At runtime, ANTLR \nback\u00adtracks even less than predicted by static analysis. For exam\u00adple, statically we .nd an average of \n10.9% of the decisions backtrack for our sample grammars but Table 4 shows that the generated parsers \nbacktrack in only 6.8% of the de\u00adcision events on average. The non-PEG-derived grammars backtrack only \nabout 2.5% of the time. This is partly be\u00adcause some of the backtracking decisions manage uncommon grammar \nconstructs. For example, there are 1,120 decisions of any kind in the TSQL grammar but the sample input \nexercises only 309 of them. Most importantly, just because a decision can backtrack, does not mean it \nwill. The last column in Table 4 shows that the potentially backtracking decisions (PBDs) only back\u00adtrack \nabout half the time on average across the sample grammars. The commercial VB.NET and TSQL grammars yield \nPBDs that backtrack in only about 30% of the decision events. Some PBDs never trigger backtracking events. \nSub\u00adtracting the .rst two columns in Table 4 gives the number of PBDs that avoid backtracking altogether. \nWe should point out that without memoization, back\u00adtracking parsers are exponentially complex in the \nworst case. This matters for grammars that do a lot of nested back\u00adtracking. For example, the RatsC grammar \nappears not to terminate if we turn o. ANTLR memoization support. In constrast, the VB.NET and C# parsers \nare .ne without it. Packrat parsing [6] achieve linear parsing results at the cost of the increased memory \nfor the memoization cache. In the worst case, we need to squirrel away O(|N|* n) decision event results \n(one for each nonterminal decision at each in\u00adput position). The less we backtrack, the smaller the cache \nsince ANTLR only memoizes while speculating. We did not speci.cally measure the performance of the lookahead \nDFA portion of ANTLR parsers because the over\u00adall performance of such parsers is more than adequate, \na claim supported by the performance numbers provided in this section and the experience of developers \nin the real world. For example, Twitter parses 1 billion queries/day with an ANTLR-generated parser. \nThe speed of the looka\u00adhead DFA likely does not contribute signi.cantly to the over\u00adall parse time because \nthe average lookahead depth for any decision, backtracking or not, is only one or two tokens (col\u00adumn \navg k in Table 3). ANTLR v3 s LL(*) parsers are about 2.5x faster than ANTLR v2 parsers for the same \ngrammar. For example, parsing the 565k lines of the Java 1.3 library takes 4.9 sec\u00adonds with ANTLR v2 \ns Java grammar. The same grammar converted to v3 syntax takes only 1.9 seconds. (Neither test warmed \nup the on-the-.y compiler.) The v2 version needed to backtrack but v3 s more powerful LL(*) made it unneces\u00adsary \nto backtrack. The Java1.5 grammar from this section, written speci.cally for v3 and a larger language \nthan Java 1.3, backtracks occasionally and took 3 seconds. Unfortu\u00adnately, it is di.cult to draw conclusions \nfrom this speci.cally about LL(*) parsing. The runtime and lexer capabilities of v3 and v2 are so di.erent \nthat we cannot be sure where the speed increase lies. 7. Related work Many parsing techniques exist, \nbut currently the two domi\u00adnant strategies are Tomita s bottom-up GLR [19] and Ford s top-down packrat \nparsing [6], commonly referred to by its associated meta-language PEG [7]. Both are nondetermin\u00adistic \nin that parsers can use some form of speculation to make decisions. LL(*) is an optimization of packrat \npars\u00ading just as GLR is an optimization of Earley s algorithm [5]. The closer a grammar conforms to the \nunderlying LL or LR strategy, the more e.cient the parser in time and space. LL(*) ranges from O(n) to \nO(n 2) whereas GLR ranges from O(n) to O(n 3). Surprisingly, the O(n 2) potential comes from cyclic lookahead \nDFA not backtracking (assuming we mem\u00adoize). ANTLR generates LL(*) parsers that are linear in practice \nand that greatly reduce speculation, reducing mem\u00adoization overhead over pure packrat parsers. GLR and \nPEG tend be scannerless, which is necessary if a tool needs to support grammar composition. Composition \nmeans that programmers can easily integrate one language into another or create new grammars by modifying \nand composing pieces from existing grammars. For example, the Rats! Jeannie grammar elegantly composes \nall of C and Java. The ideas behind LL(*) are rooted in the 1970s. LL(*) parsers without predicated lookahead \nDFA edges imple\u00adment LL-regular grammars, which were introduced by Jarz\u00adabek and Krawczyk [10] and Nijholt \n[15]. Nijholt [15] and Poplawski [18] gave linear two-pass LL-regular parsing strategies that had to \nparse from right-to-left in their .rst pass, requiring .nite input streams (i.e., not sockets or inter\u00adactive \nstreams). They did not consider semantic predicates. Milton and Fischer [13] introduced semantic predicates \nto LL(1) grammars but only allowed one semantic predi\u00adcate per production to direct the parse and required \nthe user to specify the lookahead set under which the predicates should be evaluated. Parr and Quong \n[17] introduced syntac\u00adtic predicates and semantic predicate hoisting, the notion of incorporating semantic \npredicates from other nonterminals into parsing decisions. They did not provide a formal pred\u00adicated \ngrammar speci.cation or an algorithm to discover visible predicates. In this paper, we give a formal \nspeci\u00ad.cation and demonstrate limited predicate discovery dur\u00ading DFA construction. Grimm supports restricted \nsemantic predicates in his PEG-based Rats! [8] and arbitrary actions but relies on programmers to avoid \nside-e.ects that cannot be undone. Recently, Jim et al added semantic predicates to transducers capable \nof handling all CFGs with Yakker [11]. ANTLR 1.x introduced syntactic predicates as a manually\u00adcontrolled \nbacktracking mechanism. The ability to specify production precedence came as a welcome, but unantici\u00adpated \nside-e.ect of the implementation. Ford formalized the notion of ordered alternatives with PEGs. Backtracking \nin versions of ANTLR prior to 3.x su.ered from exponential time complexity without memoization. Ford \nalso solved this problem by introducing packrat parsers. ANTLR 3.x users can turn on memoization with \nan option.  Previous attempts to extend parsing strength added .xed k> 1 lookahead to LL and LALR parsers. \nSince k-lookahead sets have space O(|T |k), the challenge was to compress sets to a practical size. Charles \n[3] represented LALR(k) lookahead sets as minimal acyclic DFA with practical space costs on average. \nParr [16] used a lossy compression called linear approximate lookahead that reduced space to O(|T |\u00d7 \nk) and, in practice, worked for almost all decisions. (ANTLR 2.x used this compression.) The key di.erence \nbetween these techniques and LL(*) is that LL(*) lookahead DFA can be cyclic, thus, signi.cantly extending \nparsing strength with arbitrary lookahead. LL-regular grammars are the analog of LR-regular gram\u00admars \n[4]. Bermudez and Schimpf [2] provided a parsing strat\u00adegy for LR-regular grammars called LAR(m). Parameter \nm is a stack governor, similar to ours, that prevents analysis algorithm nontermination. LAR(m) builds \nan LR(0) ma\u00adchine and grafts on lookahead DFA to handle nondetermin\u00adistic states. Finding a regular partition \nfor every LL-regular or LR-regular parsing decision is undecidable. Analysis al\u00adgorithms for LL(*) and \nLAR(m) that terminate construct a valid DFA for a subset of the LL-regular or LR-regular gram\u00admar decisions, \nrespectively. In the natural language commu\u00adnity, Nederhof [14] uses DFA to approximate entire CFGs, \npresumably to get quicker but less accurate language mem\u00adbership checks. Nederhof inlines rule invocations \nto a speci.c depth, m, e.ectively mimicking the constant from LAR(m). 8. Conclusion LL(*) parsers are \nas expressive as PEGs and beyond because of semantic predicates. While GLR accepts left-recursive grammars, \nit cannot recognize context-sensitive languages as LL(*) and other predicated parsers can. Unlike PEGs \nor GLR, LL(*) parsers enable arbitrary action execution and provide good support for debugging and error \nhandling. The LL(*) analysis algorithm constructs cyclic lookahead DFA to handle non-LL(k) constructs \nand then fails over to back\u00adtracking via syntactic predicates when it fails to .nd a suit\u00adable DFA. Experiments \nreveal that ANTLR generates e.\u00adcient parsers, eliminating almost all backtracking. ANTLR is widely used \nin practice, indicating LL(*) hits a sweet spot in the parsing spectrum. 9. Acknowledgements We would \nlike to thank the anonymous reviewers for their detailed comments and to thank Sriram Srinivasan for \ndis\u00adcussing LL(*) parsing and grammar analysis at length. References [1] Appendix. http://antlr.org/papers/LL-star/index.html. \n[2] Bermudez, M. E., and Schimpf, K. M. Practical arbitrary lookahead LR parsing. Journal of Computer \nand System Sciences 41, 2 (1990), 230 250. [3] Charles, P. A Practical Method for Constructing E.cient \nLALR(K) Parsers with Automatic Error Recovery. PhD thesis, New York University, New York, NY, USA, 1991. \n[4] Cohen, R., and Culik, K. LR-Regular grammars an extension of LR(k) grammars. In SWAT 71: Proceedings \nof the 12th Annual Symposium on Switching and Automata Theory (swat 1971) (Washington, DC, USA, 1971), \nIEEE Computer Society, pp. 153 165. [5] Earley, J. An e.cient context-free parsing algorithm. Communications \nof the ACM 13, 2 (1970), 94 102. [6] Ford, B. Packrat Parsing: Simple, powerful, lazy, linear time. In \nProceedings of annual ACM SIGPLAN Interna\u00adtional Conference on Functional Programming (2002), ACM Press, \npp. 36 47. [7] Ford, B. Parsing Expression Grammars: A recognition\u00adbased syntactic foundation. In POPL \n04: Proceedings of the 37th annual ACM SIGPLAN-SIGACT symposium on Principles of Programming Languages \n(2004), ACM Press, pp. 111 122. [8] Grimm, R. Better extensibility through modular syntax. In PLDI 06: \nProceedings of annual ACM SIGPLAN Confer\u00adence on Programming Language Design and Implementation (2006), \nACM Press, pp. 38 51. [9] Hanson, D. R. Compact recursive-descent parsing of expressions. Software Practice \nand Experience 15 (December 1985), 1205 1212. [10] Jarzabek, S., and Krawczyk, T. LL-Regular grammars. \nInformation Processing Letters 4, 2 (1975), 31 37. [11] Jim, T., Mandelbaum, Y., and Walker, D. Semantics \nand algorithms for data-dependent grammars. In POPL 10: Proceedings of the 37th annual ACM SIGPLAN-SIGACT \nsymposium on Principles of Programming Languages (New York, NY, USA, 2010), ACM, pp. 417 430. [12] McPeak, \nS., and Necula, G. C. Elkhound: A fast, practical GLR parser generator. In Compiler Construction (2004), \npp. 73 88. [13] Milton, D. R., and Fischer, C. N. LL(k) parsing for attributed grammars. In International \nConference on Automata, Languages, and Programming (1979), pp. 422 430. [14] Nederhof, M.-J. Practical \nexperiments with regular approximation of context-free languages. Computational Linguistics 26, 1 (2000), \n17 44. [15] Nijholt, A. On the parsing of LL-Regular grammars. In Mathematical Foundations of Computer \nScience 1976 (Heidelberg, 1976), A. Mazurkiewicz, Ed., vol. 45 of Lecture Notes in Computer Science, \nSpringer Verlag, pp. 446 452. [16] Parr, T. J. Obtaining practical variants of LL(k) and LR(k) for k> \n1 by splitting the atomic k-tuple. PhD thesis, Purdue University, West Lafayette, IN, USA, 1993. [17] \nParr, T. J., and Quong, R. W. Adding Semantic and Syntactic Predicates to LL(k) pred-LL(k). In Proceedings \nof the International Conference on Compiler Construction; Edinburgh, Scotland (April 1994). [18] Poplawski, \nD. A. On LL-Regular grammars. Journal of Computer and System Sciences 18, 3 (1979), 218 227. [19] Tomita, \nM. E.cient Parsing for Natural Language. Kluwer Academic Publishers, 1986. [20] Woods, W. A. Transition \nnetwork grammars for natural language analysis. Communications of the ACM 13, 10 (1970), 591 606.  \n  \n\t\t\t", "proc_id": "1993498", "abstract": "<p>Despite the power of Parser Expression Grammars (PEGs) and GLR, parsing is not a solved problem. Adding nondeterminism (parser speculation) to traditional <i>LL</i> and <i>LR</i> parsers can lead to unexpected parse-time behavior and introduces practical issues with error handling, single-step debugging, and side-effecting embedded grammar actions. This paper introduces the <i>LL</i>(*) parsing strategy and an associated grammar analysis algorithm that constructs <i>LL</i>(*) parsing decisions from ANTLR grammars. At parse-time, decisions gracefully throttle up from conventional fixed k&gt;=1 lookahead to arbitrary lookahead and, finally, fail over to backtracking depending on the complexity of the parsing decision and the input symbols. <i>LL</i>(*) parsing strength reaches into the context-sensitive languages, in some cases beyond what GLR and PEGs can express. By statically removing as much speculation as possible, <i>LL</i>(*) provides the expressivity of PEGs while retaining <i>LL</i>'s good error handling and unrestricted grammar actions. Widespread use of ANTLR (over 70,000 downloads/year) shows that it is effective for a wide variety of applications.</p>", "authors": [{"name": "Terence Parr", "author_profile_id": "81322502685", "affiliation": "University of San Francisco, San Francisco, CA, USA", "person_id": "P2690611", "email_address": "parrt@cs.usfca.edu", "orcid_id": ""}, {"name": "Kathleen Fisher", "author_profile_id": "81331492634", "affiliation": "Tufts University, Boston, MA, USA", "person_id": "P2690612", "email_address": "kfisher@eecs.tufts.edu", "orcid_id": ""}], "doi_number": "10.1145/1993498.1993548", "year": "2011", "article_id": "1993548", "conference": "PLDI", "title": "LL(*): the foundation of the ANTLR parser generator", "url": "http://dl.acm.org/citation.cfm?id=1993548"}