{"article_publication_date": "06-04-2011", "fulltext": "\n Automatic Compilation of MATLAB Programs for Synergistic Execution on Heterogeneous Processors Ashwin \nPrasad Jayvant Anantpur R. Govindarajan Supercomputer Education and Research Centre, Indian Institute \nof Science, Bangalore, India ashwin@hpc.serc.iisc.ernet.in jayvant@hpc.serc.iisc.ernet.in govind@serc.iisc.ernet.in \nAbstract MATLAB is an array language, initially popular for rapid pro\u00adtotyping, but is now being increasingly \nused to develop produc\u00adtion code for numerical and scienti.c applications. Typical MAT-LAB programs have \nabundant data parallelism. These programs also have control .ow dominated scalar regions that have an \nim\u00adpact on the program s execution time. Today s computer systems have tremendous computing power in \nthe form of traditional CPU cores and throughput oriented accelerators such as graphics pro\u00adcessing units(GPUs). \nThus, an approach that maps the control .ow dominated regions to the CPU and the data parallel regions \nto the GPU can signi.cantly improve program performance. In this paper, we present the design and implementation \nof MEGHA, a compiler that automatically compiles MATLAB pro\u00adgrams to enable synergistic execution on \nheterogeneous proces\u00adsors. Our solution is fully automated and does not require program\u00admer input for \nidentifying data parallel regions. We propose a set of compiler optimizations tailored for MATLAB. Our \ncompiler iden\u00adti.es data parallel regions of the program and composes them into kernels. The problem \nof combining statements into kernels is for\u00admulated as a constrained graph clustering problem. Heuristics \nare presented to map identi.ed kernels to either the CPU or GPU so that kernel execution on the CPU and \nthe GPU happens synergisti\u00adcally and the amount of data transfer needed is minimized. In order to ensure \nrequired data movement for dependencies across basic blocks, we propose a data .ow analysis and edge \nsplitting strat\u00adegy. Thus our compiler automatically handles composition of ker\u00adnels, mapping of kernels \nto CPU and GPU, scheduling and insertion of required data transfer. The proposed compiler was implemented \nand experimental evaluation using a set of MATLAB benchmarks shows that our approach achieves a geometric \nmean speedup of 19.8X for data parallel benchmarks over native execution of MAT-LAB. Categories and Subject \nDescriptors D.3.2 [Programming Lan\u00adguages]: Language Classi.cations Very high-level languages; D.3.4 \n[Programming Languages]: Processors Compilers General Terms Algorithms, Languages, Performance Permission \nto make digital or hard copies of all or part of this work for personal or classroom use is granted without \nfee provided that copies are not made or distributed for pro.t or commercial advantage and that copies \nbear this notice and the full citation on the .rst page. To copy otherwise, to republish, to post on \nservers or to redistribute to lists, requires prior speci.c permission and/or a fee. PLDI 11, June 4 \n8, 2011, San Jose, California, USA. Copyright c &#38;#169; 2011 ACM 978-1-4503-0663-8/11/06. . . $10.00 \n 1. Introduction Recent advances in the design of Graphics Processing Units (GPUs) [3][20][23] have signi.cantly \nincreased the compute ca\u00adpabilities of these processors. Today s desktops and servers, in addition to \nhaving multiple CPU cores, have graphics processing units with hundreds of SIMD cores. GPUs, due to their \nability to accelerate data-parallel applications, are well suited for numerical and scienti.c computation. \nHowever, programming in these plat\u00adforms remains a challenge as the programmer needs to manually coordinate \ntasks across the CPU cores and the GPU and manage data transfer. This requires him to have a detailed \nunderstanding of the machine architecture. MATLAB [21], an array language, is popular for implementing \nnumerical and scienti.c applications. MATLAB programs natu\u00adrally express data-level parallelism and are \ntherefore ideal candi\u00addates for acceleration using throughput-oriented processors like GPUs. Frequently, \nMATLAB programs are hand translated into compiled languages like C to improve performance. Manually translating \nMATLAB programs to use available accelerators re\u00adquires the programmer to manage details such as data \ntransfer and scheduling. This makes the translation a painstaking and er\u00adror prone process. Therefore, \na tool to automatically transform MATLAB programs so that accelerators are used to speedup the data-parallel \nparts of the program and the CPU is used to ef.ciently execute the rest of the program would be very \nuseful. In this paper, we present MEGHA1, a compiler that automati\u00adcally compiles MATLAB programs for \nexecution on machines that contain both CPU and GPU cores. Prior work on compiling MAT-LAB has focussed \non compiling it to languages such as FORTRAN [10] or C [17]. Just-in-time compilation techniques for \nexecution of MATLAB programs on CPUs were implemented in MaJIC [2] and McVM [8]. However, these efforts \non automatic translation are limited to CPUs and do not consider GPUs. Currently, Jacket [14] and GPUmat \n[12] provide the user with the ability to run MATLAB code on GPUs. These require users to specify arrays \nwhose oper\u00adations are to be performed on the GPU by declaring them to be of a special type. Further, \nthe user needs to identify which operations are well suited for acceleration on the GPU, and is required \nto man\u00adually insert calls to move the data to and from the GPU memory. Our compiler automatically identi.es \nregions of MATLAB pro\u00adgrams that can be executed ef.ciently on the GPU and tries to schedule computations \non the CPU and GPU in parallel to fur\u00adther increase performance. There are several associated challenges. \nFirst, the CPU and GPU operate on separate address spaces requir\u00ading explicit memory transfer commands \nfor transferring data from 1 MATLAB Execution on GPU based Heterogeneous Architectures. Megha also means \ncloud. and to the GPU memory. Second, sets of statements (kernels) in the input program that can be \nrun together on the GPU need to be identi.ed. This is required to minimize data transfers between host \nand device memory and also to reduce kernel call overheads. Third, program tasks 2 need to be mapped \nto either GPU cores or CPU cores and scheduled so that the program execution time is minimized. Required \ndata transfers also need to be taken into ac\u00adcount while mapping and scheduling tasks. Lastly, ef.cient \ncode needs to be generated for each task depending on which processor it is assigned to. The following \nare the main contributions of this work. We present a compiler framework, MEGHA, that translates MATLAB \nprograms for synergistic parallel execution on ma\u00adchines with GPU and CPU cores.  We formulate the identi.cation \nof data parallel regions of a MATLAB program as a constrained graph clustering problem and propose an \nef.cient heuristic algorithm to solve the prob\u00adlem.  We propose an ef.cient heuristic algorithm to map \nkernels to either the CPU or GPU and schedule them so that memory transfer requirements are minimized. \nTechniques for generating ef.cient code for kernels mapped to either the CPU or GPU are also described. \n Experimental evaluation on a set of MATLAB benchmarks shows a geometric mean speedup of 19.8X for data \nparal\u00adlel benchmarks over native MATLAB execution. Compared to manually tuned GPUmat versions of the \nbenchmarks, our ap\u00adproach yields speedups up to 10X.  To the best of our knowledge, MEGHA is the .rst \neffort to automat\u00adically compile MATLAB programs for parallel execution across CPU and GPU cores. The \nrest of this paper is organized as fol\u00adlows: Section 2 brie.y reviews the necessary background. Section \n3 provides an overview of the compiler and brie.y describes var\u00adious stages in the frontend. Section \n4 describes the details of our proposed compilation methodology. In Section 5, we present our experimental \nmethodology and report performance results. Section 6 discusses related work and Section 7 concludes. \n2. Background 2.1 NVIDIA GPUs and CUDA This section describes the architecture of NVIDIA GPUs in gen\u00aderal. \nWhile models might differ in the amount of resources avail\u00adable, the basic architecture stays roughly \nthe same [22][20]. The architecture of NVIDIA GPUs is hierarchical. They consist of a set streaming multiprocessors \n(SMs) each of which consists of sev\u00aderal scalar units (SUs). For example, the 8800 series GPUs have 16 \nSMs each having 8 SUs. The basic schedulable entity for GPUs is a warp which currently consists of 32 \ncontiguous threads. SMs have support for the time interleaved execution of several warps. They periodically \nswitch between warps to hide latency. Within an SM, all SUs execute instructions in lock-step. Additionally, \nany di\u00advergence in the execution path of threads within a warp results in a performance penalty. Each \nSM has a partitioned register .le and a shared memory which is accessible by all SUs in the SM. The shared \nmemory is similar to a software managed cache or a scratch pad memory. The 8800 series GPUs have register \nbanks containing 8192 32-bit registers and 16KB of shared memory. All SMs are connected to the device \nmemory by a very wide bus. This bus is 256-512 bits wide for 8800 series GPUs depending 2 Statements \nthat can only run on the CPU and kernels (which can be run on CPU or GPU) are collectively called tasks. \n on the model. The memory bus is capable of providing very high bandwidth, provided all threads in a \nwarp access consecutive mem\u00adory locations. Such accesses are said to be coalesced. Essentially the address \naccessed by thread n of a warp must be of the form BaseAddress + n. Programs to be run on NVIDIA GPUs \nare typically written us\u00ading the CUDA(Compute Uni.ed Device Architecture) program\u00adming language which \nis an extension of C++ [22]. The CUDA pro\u00adgramming model provides a higher level view of the GPU archi\u00adtecture \nto application programmers. From the programmer s point of view, threads are divided into thread blocks, \nand several thread blocks form a grid. A GPU kernel call, which is executed on the GPU, in general consists \nof multiple thread blocks organized as a grid. Each thread block is executed on a single SM and can con\u00adtain \nup to 512 threads. However, a single SM can execute multiple thread blocks simultaneously depending on \nthe register and shared memory requirement per block. Kernel calls are asynchronous in the sense that \ncontrol is returned to the calling CPU thread before the kernel execution on the GPU completes. Data \ntransfers between GPU and CPU memory must be initiated by the CPU thread. 2.2 Supported MATLAB Subset \nMATLAB is a dynamically typed array based programming lan\u00adguage which is very popular for developing \nscienti.c and numer\u00adical applications. As MATLAB has grown into a diverse and vast language over the \nyears, the compiler described in this paper sup\u00adports only a representative subset of MATLAB. A brief \ndescription of the subset of MATLAB supported and a set of other assumptions made by our compiler implementation \nare presented below. 1. MATLAB supports variables of several primitive types like logical, int, real, \ncomplex and string. It is also possible to construct arrays of these types with any number of dimensions. \nCurrently, our compiler does not support variables of type complex and string. Arrays are also restricted \nto a maximum of three dimensions. 2. MATLAB supports indexing with multi-dimensional arrays. However, \nour implementation currently only supports indexing with single dimensional arrays. 3. In MATLAB, it \nis possible to change the size of arrays by as\u00adsigning to elements past their end. We currently do not \nsupport indexing past the end of arrays. Further, in this paper, we re\u00adfer to assignments to arrays through \nindexed expressions (Eg: a(i)) as indexed assignments or partial assignments. 4. We assume that the \nMATLAB program to be compiled is a sin\u00adgle script without any calls to user-de.ned functions. Support \nfor user de.ned functions can be added by extending the fron\u00adtend of the compiler. Also, anonymous functions \nand function handles are not currently supported. 5. In general, types and shapes (array sizes) of MATLAB \nvariables are not known until run-time. The compiler currently relies on a simple data .ow analysis to \nextract sizes and types of program variables. It also relies on programmer input when types cannot be \ndetermined. We intend to extend our type system to support symbolic type inferencing in future [16]. \nUltimately, we envi\u00adsion that the techniques described in this paper will be used in both compile time \nand run-time systems.  3. Compiler Overview A high level overview of the proposed compiler is shown \nin Fig\u00adure 1. The input to the compiler is a MATLAB script and the output is a combination of C++ and \nCUDA code. This section brie.y de\u00adscribes the frontend of the compiler. A detailed explanation of the \nstages in the backend is given in Section 4.  Figure 1: Compiler Overview 3.1 Code Simpli.cation The \ncompiler .rst simpli.es the input source code by breaking complex expressions into simpler ones. The \noutput of this pass is a single operator intermediate form; i.e., each statement in the resulting IR \nhas at most one operator on the right hand side of assignments. Expressions are broken into single operator \nform by introducing temporaries. For example, 1: x=(a+b)-c; is converted to 1: tempVar1 = a + b; 2: x \n= tempVar1 -c; Array indexing is also treated as an operator. First each indexing expression is simpli.ed \nif needed. Then a simpli.ed array reference is emitted. For example, 1: x = a(2*i); is converted to 1: \ntempVar1 = 2*i; 2: x = a(tempVar1); However, it is not always possible to completely simplify indexing \nexpressions. This is the case when indexing expressions contain magic ends (e.g., a(2:end) refers to \nall elements of a starting from the second element) and magic colons (e.g., a(:) refers to all elements \nof a). These are handled using a set of semantics preserving transformations described below.  3.2 Semantics \nPreserving Transformations Transformation of the MATLAB code is necessary to simplify subsequent analysis, \nenable conversion to single assignment form and ultimately enable CUDA code generation. Hence, our compiler \nmakes the following changes to the simpli.ed IR before converting it to SSA form. MATLAB supports context-sensitive \nindexing of arrays. Users can specify all elements along a dimension by simply using a : in the index \ncorresponding to that dimension. For example, to set all elements of a vector a to 42, one could write: \na( : ) = 42 One can also use the keyword end inside an array index expression in MATLAB to refer to the \nlast element of the array along that dimension. For example, if a is a vector, a(4:end) = 42; assigns \n42 to all elements of a starting from element 4. The compiler removes these context-sensitive operators \nby replac\u00ading them by the indices they imply : e.g., a(:) is replaced by a(1:length(a)) and a(2:end) \nis replaced by a(2: length(a)). Some transformations are also needed on for-loops in user code to preserve \nMATLAB semantics. In MATLAB, assignments to the loop index variable or to the loop bounds inside the \nloop body do not have any effect on the iteration count of the loop. For example, the loop in the following \nprogram runs for ten iterations. 1: n=10; 2: for i=1:n 3: % .... 4: i=i+10; 5: endfor Our compiler preserves \nthese semantics by renaming the loop index (and bounds). A statement to copy the new loop index into \nthe old loop index is inserted at the head of the loop. Therefore, the above code would be transformed \nto the following equivalent code. 1: n=10; 2: for tempIndex=1:n 3: i = tempIndex; 4: % .... 5: i=i+10; \n6: endfor 3.3 Static Single Assignment Construction Since static single assignment form (SSA) is a well \nestablished IR for performing optimizations, we choose it as our intermediate form. The SSA form is constructed \nusing the method described by Cytron et al [9]. However, since the variables in the program are possibly \narrays, it is important to optimize the insertion of f nodes. One optimization we make is that we insert \nf nodes for a variable at a point only if it is live at that point. Further, assignments to array variables \nthrough indexed expressions are not renamed; in other words, values are assigned in-place. However, all \ntotal assignments are renamed. 3.4 Type and Shape Inference Since MATLAB is a dynamically typed language, \ntype and shape information needs to be inferred before a MATLAB script can be compiled to a statically \ntyped language like C. Our de.nition of the type of a variable is based on the de.nition used by De Rose \net al [10]. The type of a variable is a tuple consisting of the following elements 1. Intrinsic Type \nwhich can be boolean, integer or real, 2. Shape which can be scalar, vector, matrix or 3D-array, and \n 3. Size which indicates the size of each dimension of the variable.  Our compiler currently tries \nto infer base types of variables and shapes of arrays based on program constants and the semantics of \nbuilt-in MATLAB functions and operators by performing a forward data .ow analysis that is similar to \nwhat was used in FALCON [10] and MaJIC [2]. It is also possible for the user to specify types of variables \nin the input MATLAB script via annotations. The compiler does not currently implement symbolic type inference \nand symbolic type equality checking [16]. We leave this for future work. 4. Backend The backend of the \ncompiler performs kernel identi.cation, map\u00adping, scheduling and data transfer insertion. Kernel identi.cation \nidenti.es sets of statements, called kernels3, which can be treated as a single entity by the mapping \nand scheduling phase. It also trans\u00adforms kernels into loop nests from which the code generator can generate \nef.cient lower level code. The mapping and scheduling phase assigns a processor to each identi.ed kernel \nand also decides when it should be executed. Kernel identi.cation and mapping are performed per basic \nblock. Global transfer insertion then performs a global data .ow analysis and inserts the required inter \nbasic block transfers. 4.1 Kernel Identi.cation The purpose of the kernel identi.cation process is to \nidentify the entities on which mapping and scheduling decisions can be made. We call these entities kernels \nor tasks4. They are sets of IR state\u00adments that, when combined, can be ef.ciently run on the GPU. The \nkernel identi.cation process consists of the following steps. 1. Identify IR statements that are data \nparallel as GPU friendly statements. 2. Kernel Composition identi.es sets of statements that can be \ngrouped together in a common kernel. 3. Scalarization and Index Array elimination eliminate arrays that \nare not needed because of the way statements were grouped together. 4. Surrounding Loop Insertion transforms \nkernels into loop nests consisting of parallel loops.  Our compiler currently tags statements that perform \nelement\u00adwise operations or other data parallel operations like matrix multi\u00adplication as GPU friendly \nstatements. It should be noted that this step just identi.es statements that could run on the GPU and \ndoes not make any decisions on what should actually be run on the GPU. 1: A =(B + C) + (A + C); 2: C \n=A * C; (a) MATLAB Statements 1: tempVar0 = B + C; 2: tempVar1 = A + C; 3: A 1 = tempVar0 + tempVar1; \n4: tempVar2 = A1 * C; 5: C 1 = tempVar2; (b) IR Representation Figure 2: A simple MATLAB program and \nits IR representation. A, B and C are 100x100 matrices. Once GPU friendly statements have been identi.ed, \nthe kernel composition step decides the granularity at which mapping deci\u00adsions are to be made. It does \nthis by grouping IR statements to\u00adgether. It is inef.cient to make these decisions at the level of IR \nstatements as there may be too many of them. Kernel composition reduces the number of entities that need \nto be mapped and could also lead to a signi.cant decrease in the memory requirement of the compiled program. \nTo understand why, consider the example in Figure 2(a). Code simpli.cation generates a considerable num\u00adber \nof temporaries that are used only once after they are de.ned as can be seen in Figure 2(b). If mapping \ndecisions are made per IR statement, the de.nition and use of these temporaries could be 3 We use kernel \nto refer to either the smallest unit of data-parallel work in a set of IR statements or a set of GPU \nfriendly IR statements. Which is meant will be clear from context. 4 The term kernel or task is used \ndepending on whether they are mapped to the GPU or CPU respectively. We use both terms interchangeably \nwhen the mapping is not known. mapped to different processors necessitating the storage of these temporaries, \nwhich may be arrays, in memory. In the example in Figure 2(b), three temporaries, each of which is a \nmatrix have been generated by the simpli.cation process. In the worst case, when all these temporaries \nneed to be stored in memory, the memory re\u00adquirement of the program increases signi.cantly. However, \nif state\u00adments 1, 2 and 3 are grouped into the same kernel, it is possible to replace the arrays tempVar0 \nand tempVar1 with scalars. When this kernel is rewritten as a loop nest, these variables are only live \nwithin one iteration of the loop nest. Hence they can be replaced by scalars as in the following code. \n1: for i=1:100 2: for j=1:100 3: tempVar0_s = B(i, j) + C(i, j); 4: tempVar1_s = A(i, j) + C(i, j); 5: \nA_1(i, j) = tempVar0_s + tempVar1_s; 6: endfor 7: endfor The above problem is even more exaggerated in \nreal benchmarks. To give a speci.c example, in the benchmark fdtd [15], we observed that worst case memory \nrequirement increase can be as high as 30X. Combining statements into kernels is also important because \nthe mapping and scheduling phase would then be able to solve a scheduling problem with a fewer number \nof nodes. We refer to this process of reducing arrays to scalar variables as Scalarization. However, \nthe newly created scalars are likely to be saved in registers thereby increasing the register utilization. \nTwo statements can be combined into a kernel only if the fol\u00adlowing conditions hold for the statements \nto be combined. 1. Both statements perform element-wise operations. 2. Both statements have identical \niteration spaces. (By iteration space of a statement, we mean the iteration space of the parallel loops \nsurrounding the statement.) 3. Combining the statements will not result in cyclic dependencies among \nkernels. 4. The loops surrounding the kernel created by combining the two statements should be parallel5. \n  In the example in Figure 2(b), it is legal to group the .rst three statements, in the simpli.ed IR, \ninto the same kernel. However, it is illegal to put statement 4 into the same group as matrix multiplica\u00adtion \nis not an element-wise operation and requires elements at other points in the iteration space to have \nbeen computed. Thus, combin\u00ading the addition and the multiplication statements introduces cross iteration \ndependencies and therefore the surrounding loops of the resulting group are no longer parallel. Even \nthough condition 1 is subsumed by condition 4, it is speci.ed separately as it is easier to check for \nviolations of condition 1. Condition 4 is similar to the condition for legal loop fusion [18] but is \nstricter. For loop fusion to be legal, it is only required that directions of dependencies between statements \nin the loops being fused are not reversed because of the fusion. However, for kernel composition to be \nlegal, we require that the surrounding loops still be parallel loops after the statements are combined \nas our objective is to map identi.ed kernels to the GPU. Combining statements into kernels has many bene.ts. \nCom\u00adpiler generated temporary arrays and user arrays can be converted to scalars. Locality between the \ncombined statements can be ex\u00adploited. Lastly, combining statements into kernels also results in a reduction \nin the CUDA kernel invocation overhead at run-time. However, forming larger kernels can also increase \nregister usage and the memory footprints of kernels. These factors are extremely 5 Parallel loops are \nloops whose iterations can all be run in parallel. important if the kernel is to be run on the GPU. \nLarger kernels may also require more variables to be transferred to and from the GPU. This may lead to \ndecreased opportunities for overlapping computa\u00adtion and data transfer. Thus there are trade offs in \nkernel composi\u00adtion. We model the kernel composition problem as a constrained clustering problem modelling \nthe costs and bene.ts described above. It is loosely related to the parameterized model for loop fusion \n[24]. However, compared to the model for loop fusion, our formulation additionally models memory utilization \nof kernels. Register utilization is also more accurately modelled. The details of the construction and \na heuristic algorithm to compute a solution are described in the following sub-sections. 4.1.1 Graph \nFormulation The problem of composing statements into kernels is that of clus\u00adtering the nodes of an augmented \ndata .ow graph G =(V, E) into clusters. A cluster is a set of nodes of the graph G. The graph G is de.ned \nas follows. 1. Each node n . V represents an IR statement in the basic block under consideration. The \nstatement represented by n . V is denoted by stm(n). 2. The set of edges, E has two types of edges. \n Dependence edges are directed edges between nodes whose statements are data dependent. An edge (n1,n2) \nimplies that n2 must run after n1. All types of dependences (true, anti and output) are represented uniformly. \n A Fusion preventing edge connects a pair of nodes that can\u00adnot be merged. For example, such an edge \nwould exist be\u00adtween the vertices for statements 3 and 4 in Figure 2(b). There are also fusion preventing \nedges between nodes rep\u00adresenting GPU friendly statements and others representing non GPU friendly statements. \n  The kernel composition problem is that of clustering the set of nodes V into a set of clusters {C1,C2, \n...Cn}. All the Ci s are disjoint and their union is V . An edge e =(n1,n2) is said to be in a cluster \nCi if n1 and n2 are in Ci. To accurately model register utilization, we consider the origi\u00adnal register \nusage of each statement in the basic block and the in\u00adcrease in register usage due to scalarization of \narrays. First, the reg\u00adister utilization of each statement stm(n), for each n . V , denoted by regn, \nis estimated as the number of scalars used in stm(n). Sec\u00adondly, we need to differentiate between variables \nthat can be scalar\u00adized (scalarizable variables) and those that cannot be scalarized by combining two \nstatements. To model the increase in register utiliza\u00adtion when two statements are combined, we de.ne \nthe weight rege on each edge e =(n1,n2) . E. The value of this quantity is the number of variables that \ncan be scalarized by combining stm(n1) and stm(n2). For example, it is 1 for the edge between the nodes \nfor statements 1 and 3 in the above example since tempVar0 can be eliminated by combining these statements. \nNow, when a cluster is formed, the register utilization of the kernel represented by the cluster is the \nsum of the reg values of all nodes and edges in the cluster. Therefore, to limit the register utilization \nof each cluster, we introduce the following constraint for each cluster Ci. XX rege + regn = Rmax (1) \ne.Ci n.Ci To model memory utilization, the distinction between scalariz\u00adable variables and non-scalarizable \nvariables is needed again. For scalarizable variables, such as tempVar0, the memory utilization becomes \nzero when its de.ning node and use node are combined (statements 1 and 3 in this case). However, for \nnon-scalarizable variables, the memory usage is the sum of the sizes of all such variables used by statements \nin the cluster. To model the memory utilization due to scalarizable variables, we use weights on both \nedges and nodes. For a node n, we de\u00ad.ne memn as the sum of sizes of all scalarizable variables in stm(n). \nTherefore memn, for the node representing statement 1 is size(tempV ar0). For an edge e between n1 and \nn2, if there is a variable scal var that can be scalarized by combining stm(n1) and stm(n2), we de.ne \nmeme = -2.size(scal var). Therefore, the memory utilization of a scalarizable variable reduces to zero \nif both its de.nition and use are in the same cluster as the weights on the nodes are cancelled out by \nthe weight of the edge between them. The total memory usage of the kernel represented by the cluster \nCi, due to scalarizable variables is therefore given by: XX memn + meme = Mscal,i (2) n.Ci e.Ci For \nnon-scalarizable variables, we use a bit vector to model the memory utilization. For each node n . V \n, and each non\u00adscalarizable variable var, we de.ne usedn(var) to be 1 if n uses var. For example, the \nnode m, representing statement 1 in the above example, has usedm(B)=1 and usedm(C)=1.A variable var is \nused in a cluster if any of the nodes in the cluster has used(var) as 1. The memory utilization of a \ncluster due to non-scalarizable variables is the sum of sizes of all such variables used in the cluster. \nThe memory utilization due to non-scalarizable variables is given by: X_ ( usedn(var)).size(var)= Mnonscal,i \n(3) var.V ars n.Ci where V ars is the set of all non-scalarizable variables in the pro\u00adgram. To limit \nthe total memory utilization of each kernel, we intro\u00adduce the following constraint. Mscal,i + Mnonscal,i \n= Mmax (4) Additionally, for all fusion preventing edges e . E, e must not be in any cluster Ci. Also, \nwhen nodes are grouped together as kernels, the graph must remain acyclic. For each edge e =(n1,n2) . \nE, benefite, quanti.es the bene.t of combining the statements stm(n1) and stm(n2) into the same kernel. \nThe bene.t of collapsing an edge is de.ned as benefite = aMem(n1,n2)+ \u00dfLoc(n1,n2)+ . where, Mem(n1,n2) \nis the sum of sizes of array variables that can be eliminated by combining stm(n1) and stm(n2), Loc(n1,n2) \nquanti.es the locality between the two statements. It is currently estimated as the number of pairs of \noverlapping array references in the two statements. If two statements being considered for com\u00adposition \nhave a common array reference, then this can be read into shared memory and reused. The term Loc(n1,n2) \nis meant to model this. . represents the reduction in the kernel call overhead. a and \u00df are parameters. \nThe objective of kernel composition is to maximize the total benefit: XX benefit = benefite Ci e.Ci \nsubject to the above constraints. The above problem can be shown to be NP-hard by a reduction from the \nloop fusion problem de.ned by Singhai and McKinley [24]. We therefore use a heuristic algo\u00adrithm to solve \nthe kernel composition problem. 4.1.2 Kernel Composition Heuristic Our heuristic kernel clustering method \nuses a k-level look ahead to merge nodes of the graph G into clusters. It computes the bene.ts and memory \nrequirement for collapsing all possible legal sets of k edges starting at node n by exhaustively searching \nthrough all possibilities. (By collapsing an edge, we mean combining its source and destination nodes). \nIt then collapses the set of k edges that results in the maximum bene.t. For small values of k(k =3 or \nk =4), this achieves reasonably good results without a signi.cant increase in compile time. The details \nof the clustering algorithm are listed in Algorithm 1. The function legalKDepthMerges returns multiple \nsets of (up to) k edges that can be legally collapsed. This function makes sure that clusters that have \nfusion preventing edges are not formed and that no cycles are formed in the graph. It also ensures that \nthe constraints listed earlier are not violated. The function maxBenefit returns the set of edges with \nthe maximum bene.t and collapseEdges collapses all edges in a set of edges and adds all combined nodes \nto Ci. Algorithm 1 Kernel Composition Heuristic 1: procedure KernelComposition(G =(V,E)) 2: nodes . \nV 3: clusters . f 4: while nodes = f do 5: n . node in nodes that uses minimum memory 6: Ci . n 7: nodes \n. nodes -{n} 8: while true do 9: T . legalKDepthMerges(Ci,k) 10: if T= f then 11: bestSet . maxBenefit(T \n) 12: collapseEdges(Ci, bestSet) 13: for all e =(n1,n2) . bestSet do 14: nodes . nodes -{n1} 15: nodes \n. nodes -{n2} 16: end for 17: else 18: break 19: end if 20: end while 21: clusters . clusters . Ci 22: \nend while 23: end procedure  4.1.3 Identifying In and Out Variables Once kernels have been identi.ed, \nwe need to compute the set of variables that are required for the execution of the kernel and the set \nof variables that are modi.ed by the kernel and are needed by future computations. To do this we .rst \nperform a live-variable analysis [1] on the SSA IR. A variable is an in-variable of a given kernel if \nit is live on entry to the kernel and is accessed (either read or partially written) within the kernel. \nWe de.ne a variable as an out-variable if the variable is modi.ed inside the kernel and is live at the \nexit of the kernel. Data transfer from CPU memory or GPU memory may be required for in-variables and \nout-variables depending on the processors kernels are ultimately mapped to.  4.1.4 Array Elimination \nAs discussed in Section 4.1, arrays that become unnecessary after kernel composition need to be eliminated. \nReplacing these arrays with scalar variables not only reduces memory requirements but also eliminates \nmemory accesses that are expensive, especially if they are serviced out of GPU device memory. To remove \narray variables, we use the two transformations, scalarization and index array elimination, described \nbelow. Scalarization After in-variables and out-variables of each kernel have been iden\u00adti.ed it is \npossible to replace an array that is neither an in-variable nor an out-variable by a scalar as shown \nin the earlier example. This is possible because it is guaranteed that no subsequent state\u00adment depends \non writes to the array within this kernel and that this kernel does not depend on the prior value of \nthis array. We refer to this process as scalarization. Index Array Elimination In MATLAB programs, arrays \n(especially those de.ned using the colon operator) are frequently de.ned and used either for index\u00ading \nor in element-wise computations. It is possible to replace some references to these arrays with scalars \nthat are computed inside the kernel in which the original array was used. Any array repre\u00adsentable as \na function f : Z+ . Z+ that maps the index of an array element to the element value and used to index \none or more dimen\u00adsions of another array is called an index array. A total reference to such an array \nin a kernel can be replaced by a scalar that takes the value f(i) in the ith iteration of the surrounding \nloop. Consider the following example, in which a subset of the 3D array Ey is copied into tempVar1. 1: \ntempVar0 = Nz + 1; 2: tempVar1 = 2:tempVar0; 3: tempVar2 = Ey(:, :, tempVar1); Here, tempVar1 is an index \narray with the function f given by f(i) = 2+(i - 1). It can be eliminated by replacing it with a scalar \nwhile adding surrounding loops as shown below (details in Section 4.1.5). 1: tempVar0 = Nz + 1; 2: fori=1:Sx \n3: forj=1:Sy 4: fork=1:Sz 5: indexVar1 = 2 + (k-1); 6: tempVar2(i, j, k) = Ey(i, j, indexVar1); 7: end \n8: end 9: end Sx,Sy and Sz represent the size of the iteration space of the origi\u00adnal assignment in the \n.rst, second and third dimension respectively. We refer to this transformation as index array elimination. \nCurrently, our compiler identi.es arrays de.ned using the colon operator as index arrays. However, arrays \nde.ned using functions like linspace 6 and in user loops could also be identi.ed as index arrays. We \nintend to implement these extensions in the future. 4.1.5 Surrounding Loop Insertion Before code can \nbe generated, identi.ed kernels need to be trans\u00adformed into a form more suitable for code generation. \nThe compiler achieves this by adding loops around kernels and introducing the necessary indexing. For \nkernels that contain element-wise operations, the required number of surrounding loops are inserted and \narray references are changed as described next. Consider the example in Section 4.1.4. Introducing indexing \nis simple for originally unindexed references like tempVar2. The indices are just the loop variables \nof the surrounding loops. However, the case for indexed references is slightly more complicated. The \nn th non-scalar index in the indexed reference is transformed according to the following rules. 6 linspace \nis a MATLAB function that generates linearly spaced vectors. linspace(x, y, n) generates n linearly spaced \nnumbers from x to y. For example, linspace(1, 9, 3) returns the array [1 5 9]. 1. If it is a magic colon, \nreplace it by in that is the loop variable of the n th surrounding loop. For example, the .rst and second \nin\u00addexing expressions in the reference to Ey in the above example are changed to i and j. 2. If the \nindexer is an index array, replace the array by a scalar variable initialized with the value f(in), where \nf is the function described in Section 4.1.4. This rule is used to change the third indexing expression \nin the reference to Ey from tempVar2 to indexVar1. 3. Otherwise, if the n th non-scalar indexer is x, \nthen replace it by x(in).  Once surrounding loops are inserted, the body of the loop nest represents \nexactly the CUDA code that would need to be generated if the kernel under consideration gets mapped to \nthe GPU. Kernels that contain more complex operators such as matrix multiplication are treated as special \ncases when they are trans\u00adformed into loop nests. The compiler also keeps the original high\u00adlevel IR \nas this is useful during code generation (for example to map a matrix multiplication kernel to a BLAS \nlibrary call).  4.2 Parallel Loop Reordering After the steps described in the previous section have \nbeen per\u00adformed, the IR contains a set of kernels and their surrounding par\u00adallel loops. The in-variables \nand out-variables of each kernel are also known. When these kernels are ultimately executed, the way \nin which the iteration space of the surrounding loops is traversed can have a signi.cant impact on performance \nirrespective of whether the kernel is mapped to the CPU or the GPU. Further, the iteration space may \nneed to be traversed differently depending on whether the kernel is mapped to the CPU or the GPU. Since \nthe surrounding loops are parallel, the iteration space of these loops can be traversed in any order. \nThe loop reordering transform decides how the itera\u00adtion space must be traversed to maximize performance. \nFor a kernel that is executed on the CPU, locality in the inner most loop is important from a cache performance \nviewpoint. For example, consider the code listed in Section 4.1.4. There are three surrounding loops \nwith indices i, j and k. Therefore, the two indexed references to tempVar2 and Ey will access consecutive \nmemory locations for consecutive values7of j. This implies that having the j loop as the innermost loop \nwill maximize locality. GPU memory optimizations are however very different from memory optimizations \nimplemented by the CPU. When all threads in a warp access a contiguous region of memory, the GPU is able \nto coalesce [22] these multiple memory requests into a single memory request thereby saving memory bandwidth \nand reducing the overall latency of the memory access. If a kernel is assigned to the GPU, the code generator \nmaps the index of the outer most loop to the x dimension of the thread block. The GPU groups neighboring \nthreads in the x dimension into the same warp. Therefore, in the GPU case, the loop with the maximum \nlocality needs to be the outermost loop (which in the above example is the j loop). At this stage, since \nit is not known which processor, CPU or GPU, a kernel is mapped to, the compiler computes loop orders \nfor both of them. Also, both versions are needed during the pro\u00ad.ling stage. Currently, a simple heuristic \nis used to compute the loop ordering. The number of consecutive memory references in successive iterations \nof each loop (with all other loop indices stay\u00ading constant) is computed. The loops are then sorted based \non this number. The loop that causes accesses to consecutive memory lo\u00ad 7 Currently, in our implementation, \nthree dimensional arrays are stored as stacked row major matrices in the spirit of MATLAB. Therefore, \ntempVar2(i, j, k) and tempVar2(i, j+1, k) are in succes\u00adsive memory locations. cations for the most \nreferences in the kernel is treated as the loop with the maximum locality. For the CPU loop order, the \nloops are sorted in increasing or\u00adder of locality. (The .rst loop in the sorted sequence is the outer\u00admost \nloop and the last loop is the inner most loop.) For the example above, the j loop has the most locality \nsince both indexed refer\u00adences access successive memory locations for successive values of j. The other \nloops have no locality. Therefore, the j loop becomes the innermost loop when the above kernel is executed \non the CPU. For the GPU, the loop with the maximum locality becomes the outer-most loop since this enables \ncoalescing for the greatest number of accesses. If a particular kernel is mapped to the GPU, .nal code \ngeneration maps the iteration number in the outermost loop in the GPU order to the thread ID in the X \ndirection. It generates thread blocks of size (32, 1, 1). Since CUDA supports only 2 dimensional grids, \nthe iteration space of the next two loops (if they exist) is .attened into the block ID in the y direction. \nIn the above example, j becomes the outer most loop and the i and k loop get .attened into a single loop \nas shown below. 1: tempVar0 = Nz + 1; 2: for tidx = 1:Sy 3: for tidy = 1:Sx*Sz 4: j=tidx; 5: i = floor(tidy/Sz); \n6: k=tidy%Sz; 7: indexVar1 = 2 + (k-1); 8: tempVar2(i, j, k) = Ey(i, j, indexVar1); 9: end 10: end It \nis possible that changing the layout of certain arrays depend\u00ading on the surrounding loops may improve \nperformance by either improving locality on CPUs or increasing the number of coalesced accesses on the \nGPU. We leave this problem for future work. 4.3 Mapping and Scheduling Once kernels have been identi.ed \nby the preceding steps of the compiler, each basic block can be viewed as a directed acyclic graph (DAG) \nwhose nodes represent kernels. Edges represent data dependencies. It is now required to assign a processor \nand a start time to each node in the DAG such that the total execution time of the basic block under \nconsideration is minimized. This problem is similar to the traditional resource constrained task/instruction \nscheduling problem except that some tasks can be scheduled either on the CPU or GPU. Therefore, when \ndependent tasks are mapped to different types of processors, required data transfers also need to be \ninserted. This makes the above problem harder than the scheduling problem. It is therefore NP-hard. Thus, \nwe propose a heuristic approach for the integrated mapping and scheduling problem. The compiler uses \na variant of list scheduling to assign nodes to processors. The heuristic is based on the observation \nthat a node whose performance is very good on one processor and very bad on the other needs to be given \nmaximum chance to be assigned to the processor on which it performs best. We de.ne skew(n) where n is \na node in the DAG as \u00ab TGP U (n) TCP U (n) skew(n)= max ,TCPU (n) TGP U (n) where TGP U and TCP U are \nfunctions that give the execution time of n on the GPU and CPU respectively. To obtain estimates for \nthe execution time for each node, we currently use pro.ling informa\u00adtion. The algorithm maintains resource \nvectors for the GPU, CPU and the bus. In every iteration, it gets the node from the ready queue with \nthe highest skew and tries to schedule it so that the .nish time is minimized. While scheduling a node, \ntransfers required to Figure 3: Motivating Example for Edge Splitting. Processor name in brackets indicates \nmapping. The control .ow graph shows split edges as dotted lines and the location of A at each program \npoint.  start the node need to be considered. The algorithm computes the earliest time at which the \nnode can start on the GPU and the CPU considering the data transfers that need to be performed so that \nall required data is available. The .nish time of the node is computed for both the CPU and the GPU and \nthe node is then scheduled on the processor on which it has the lower .nish time. The details are given \nin Algorithm 2. In the algorithm, readyQueue is a priority queue that is priori\u00adtized based on skew. \nThe function computeMinimumStartT ime computes the minimum time at which a node can start on a given \nprocessor considering the data transfers that are needed due to prior processor assignments and the resource \navailability. The functions scheduleOnGP U and scheduleOnCP U schedule a node onto the GPU and CPU respectively. \nThey also insert and schedule the required data transfers and update the resource vectors. Algorithm \n2 Processor Assignment 1: procedure Assignment (DataFlowGraph G =(V, E)) 2: readyQueue . f 3: .v . V \nwith no in edges : readyQueue.insert(v) 4: while readyQueue = f do 5: v . readyQueue.remove() 6: startGP \nU . computeMinimumStartT ime(v, GP U) 7: startCP U . computeMinimumStartT ime(v, CP U) 8: finishGP U \n. startGP U + TGP U (v) 9: finishCPU . startCPU + TCP U (v) 10: if finishGP U = finishCPU then 11: scheduleOnGP \nU(v, startGP U ) 12: else 13: scheduleOnCP U(v, startCP U ) 14: end if 15: V = V - v 16: for all v1 . \nV with all preds scheduled do 17: readyQueue.insert(v1) 18: end for 19: end while 20: end procedure \n 4.4 Global Data Transfer Insertion All the previously described optimizations are performed per basic \nblock. The mapping heuristic assumes that data which is read before it is written in a basic block is \navailable on both the GPU and CPU. However, where the data actually is when the basic block begins executing \ndepends on the program path that was taken to get to the basic block. Consider the code segment in Figure \n3. When execution reaches statement 4, matrix A may either be in the CPU memory or GPU memory depending \non the path the program took to get to statement 4. If the path taken was 1.2.4, then A is in CPU memory \nsince statement 2 runs on the CPU. However, if the path taken to reach statement 4 was 1.2.4.4, then \nA is in GPU memory since statement 4 runs on the GPU. Additionally, statement 4 assumes that A is available \non the GPU. To remove this dependence of a variable s location on the pro\u00adgram path taken, we use a \ntwo step process. We .rst use a data .ow analysis to compute the locations of all program variables at \nthe entry and exit of each basic block in the program. Then, we split control .ow edges whose source \nand destination have variables in different locations. 4.4.1 Variable Location Analysis The aim of the \ndata .ow analysis is to compute the locations of all program variables at the beginning and end of each \nbasic block. The location of a variable is an element from the lattice L, de.ned over the set T = {Unknown, \nCP UAndGP U, GP U, CP U} with the partial order -and the join operator . where, . = Unknown, T = CPU \n Unknown -CP UAndGP U -GP U -CPU The reason CPU dominates GP U in the lattice is that the CPU is the \ncoordinating processor. At a program point where a variable is on the GPU along one reaching path, and \non the CPU on an\u00adother, it is probably better to move the variable to the CPU. Also, CP UAndGP U is dominated \nby both CPU and GP U to avoid extra transfers. Consider the case when GP U is dominated by CP UandGP \nU. Then, at a program point where a variable is on the GPU on one reaching path, and on both along another, \na trans\u00adfer would have been needed for the GPU path to make the variable available on both. It is faster \nto just say that the variable is no longer available on the CPU. Similar reasoning applies for the CPU \ncase. If V ars is the set of all variables in the program and B is the set of all basic blocks, for each \nbasic block b . B and each v . V ars, the following values are de.ned. 1. in(v, b) .T is the location \nof v at the start of b. 2. out(v, b) .T is the location of v at the exit of b. 3. asmp(v, b) .T is \nthe mapping of all statements in b that read v before it is written in b. It is . if v is not accessed \nin b. 4. grnt(v, b) .T indicates where v will be after the execution of  b. If v is not used in b it \nis de.ned to be .. The data .ow analysis is de.ned by the following equations. j asmp(v, b) if asmp(v, \nb)= .;in(v, b)= V out(v, b') Otherwise. b'.preds(b) (5) j grnt(v, b) if grnt(v, b)= .; out(v, b)= (6) \nin(v, b) Otherwise. A standard forward data .ow analysis algorithm [1] is used to .nd a .xed point for \nin(v, b) and out(v, b) using the data .ow equations de.ned in equations 5 and 6. When a .xed point is \nreached for in and out, the values of in(v, b) and out(v, b) signify where v needs to be at the start \nof basic block b and where v will be at the end of basic block b. The motivating example has three basic \nblocks as shown in Figure 3. The values of in and out for the variable A are: in(A, b1)= Unknown; out(A, \nb1)= CPU; in(A, b2)= GP U; out(A, b2)= GP U ; in(A, b3)= CPU and out(A, b3)= CPU. 4.4.2 Edge Splitting \nOnce the data .ow analysis is performed, the location of every variable at the entry and exit of every \nbasic block is known. When a variable changes locations across a control .ow edge, a transfer needs to \nbe inserted. More precisely, a transfer for a variable v needs to be inserted on a control .ow edge e \n=(b1,b2), when out(v, b1)= in(v, b2). MEGHA does not have the ability to explicitly split a control \n.ow edge as it emits structured C++ code8. We therefore split edges by dynamically determining which \ncontrol .ow edge was taken to reach the current basic block. This is done by assigning ID numbers to \neach basic block and inserting code to set the value of a variable, prevBasicBlock, to the ID at the \nend of the basic block. Then, at the entry of basic blocks, prevBasicBlock can be checked to determine \nwhich control .ow edge was taken and the required transfers can be performed. For the example in Figure \n3, the edges (b1,b2) and (b2,b3) need to be split. This results in the following code. 1: n=100; 2: A \n= rand(n, n); 3: _prevBasicBlock = 1; 4: for i=1:n 5: if (_prevBasicBlock == 1) 6: transferToGPU(A); \n7: A=A*A; 8: _prevBasicBlock = 2; 9: end 10: if (_prevBasicBlock == 2) 11: transferToCPU(A); 12: print(A); \n  4.5 Code Generation Currently our code generator generates C++ code for parts of the input program \nmapped to the CPU and CUDA kernels for all kernels mapped to the GPU. For kernels that are assigned to \nthe CPU, the loop nests are generated with the computed CPU loop order. For a kernel mapped to the GPU, \neach thread of the generated CUDA kernel performs the computation for a single iteration of the surrounding \nloop nest. Assignment of threads to points in the iteration space is performed as described in Section \n4.2. For the example in Section 4.2, the following CUDA kernel would be generated (Some details related \nto indexing have been simpli.ed. Sizes of Ey and tempVar2 need to be passed to the kernel to correctly \nindex them on line 13). 1: __global__ void 2: gpuKernel(float *Ey, float *tempVar2, 3: int Sx, int Sy, \nint Sz) 4: { 5: int tidx = (blockIdx.x*blockDim.x)+threadIdx.x; 6: int tidy = (blockIdx.y*blockDim.y)+threadIdx.y; \n7: j=tidx; 8: i = floor(tidy/Sz); 9: k=tidy%Sz; 10: if (i>Sx || j>Sy || k>Sz) 11: return; 12: indexVar1 \n= 2 + (k-1); 13: tempVar2(i, j, k) = Ey(i, j, indexVar1); 14:} The above kernel would be called as follows. \n1: dim3 grid(ceil(Sx/32), Sy*Sz, 1); 2: dim3 block(32, 1, 1); 3: gpuKernel<<< grid, block >>>(Ey, tempVar2, \nSx, Sy, Sz); 8 This decision was made so that the optimizations of the C++ compiler would be more effective. \n We chose to generate GPU kernels in this form since it is suitable for further automatic optimization \n[29]. For kernels that perform matrix multiplication, the code generator emits calls to the matrix multiply \nin CUBLAS rather than trying to generate an optimized matrix multiply kernel. This is possible because \na matrix multiply kernel cannot be composed with other kernels (Section 4.1). To perform memory transfers, \nthe code generator uses the blocking cudaMemcpy routine. Therefore, the generated code does not overlap \nmemory transfers with computation. However, it is possible to implement a code generation scheme that \nachieves this. Such a scheme would further improve the performance of the generated code. We leave this \nto future work. 5. Experimental Evaluation We implemented MEGHA using the GNU Octave [11] system and \nused the benchmarks listed in Table 1 to evaluate it. These bench\u00admarks are from several previous projects9 \nwith the exception of bscholes, which was developed in-house. For each benchmark the table shows the \nnumber of tasks identi.ed by our compiler, e.g. clos has a total of 37 tasks out of which 27 are GPU \nfriendly kernels and 10 are CPU only tasks. The number of kernels can be larger than the number of MATLAB \nstatements as the compiler frontend generates many IR statements for each statement in the MATLAB code. \nWe had to make minor modi.cations to capr as our compiler does not support auto-growing arrays. We modi.ed \nthe loop counts to increase the amount of computation in clos and fiff. As our compiler does not support \nuser functions, we manually inlined calls to user-de.ned functions in all these bench\u00admarks. The code \ngenerated by our compiler was compiled using nvcc (version 2.3) with the optimization level -O3. The \ngenerated executables were run on a machine with an Intel Xeon 2.83GHz quad-core processor10 and a GeForce \n8800 GTS 512 graphics pro\u00adcessor. We also ran these on a machine with the same CPU but with a Tesla S1070 \n[20]. Both machines run Linux with kernel version 2.6.26. We used the time system utility to measure \nthe runtimes of the executables generated by MEGHA and used the MATLAB function clock to measure the \nruntime for MATLAB programs run in the MATLAB environment. For each benchmark, we report the runtime \nin the MATLAB environment, execution time of CPU only C++ code generated by our compiler and the execution \ntime of C++ plus CUDA code generated by our compiler and the speedups achieved by these two versions \nover MATLAB execution. Each benchmark was run multiple times. The reported runtime is the average execution \ntime per run. Also, all experiments were performed in single precision since the GeForce 8800 does not \nhave support for double precision arithmetic. Table 2 shows the runtimes for benchmarks with data parallel \nregions. In benchmark bscholes we get a speedup of 191X over MATLAB for large problem sizes. In this \nbenchmark our compiler was able to identify the entire computation as one kernel and execute it on the \nGPU. MATLAB performs better than our CPU only code in bscholes as it handles full array operations better \nthan our C++ code. Speedups for fdtd are higher on Tesla (63X-94X) as memory accesses are coalesced better \non the Tesla than on the 8800. We did not report runtimes for the benchmark clos with C++ code as it \nuses matrix multiplication and our C++ code generator at present generates na\u00a8ive code for matrix multiplication. \nHowever, these routines can easily be replaced by 9 http://www.ece.northwestern.edu/cpdc/pjoisha/MAT2C/ \n10 Although our experimental system contained two quad-core Xeon pro\u00adcessors (8 cores), in our experiments, \nwe use only a single core as the present compiler schedules tasks to a single GPU and a single CPU core. \nThe compiler can easily be extended to use multiple CPU cores.  Benchmark Size Execution Time (in Seconds) \nSpeedup over MATLAB MATLAB CPU CPU+8800 CPU+Tesla CPU CPU+8800 CPU+Tesla bscholes 51200 Options, 100 \nIters 102400 Options, 500 Iters 204800 Options, 500 Iters 0.9 8.9 17.2 1.99 19.7 39.5 0.03 0.07 0.1 0.02 \n0.05 0.09 0.45 0.45 0.43 30 127 172 45 178 191 clos 1024 vertices 2048 vertices 4096 vertices 0.5 4.1 \n30.8 --- 0.18 1.13 mem-exc 0.11 0.73 5.82 --- 2.8 3.6 - 4.5 5.6 5.3 fdtd 512 x 512 x 15 512 x 512 x 32 \n1024 x 512 x 32 1024 x 1024 x 32 187 408 806 1623 13.5 30.5 62.7 121.6 10.1 22.69 mem-exc mem-exc 2.94 \n5.48 9.8 17.2 13.8 13.4 12.8 13.3 18.5 18.0 -- 63.6 74.4 82.2 94.4 nb1d 4096 bodies 8192 bodies 16384 \nbodies 14.8 50.1 513 7.6 29.2 323 5.4 13.7 119 5.4 15 128 1.95 1.7 1.6 2.7 3.6 4.3 2.7 3.3 4.0 nb3d 512 \nbodies 1024 bodies 1536 bodies 42.3 224.8 508 12.4 47.5 101.5 4.82 17.8 38.6 3.64 16.7 37.15 3.4 4.7 \n5.0 8.8 12.6 13.2 11.6 13.5 13.7 Table 2: Runtimes and speedups for data parallel benchmarks Benchmark \nDescription # of Lines Tasks bscholes(dp) Stock Option Pricing 50 35/7 capr Line Capacitance 60 69/23 \nclos(dp) Transitive Closure 30 27/10 crni Heat Equation Solver 60 55/14 dich Laplace Equation Solver \n55 68/15 edit Edit Distance 60 35/15 fdtd(dp) EM Field Computation 70 74/12 fiff Wave Equation Solver \n40 45/11 nb1d(dp) 1D N-Body Simulation 80 63/18 nb3d(dp) 3D N-Body Simulation 75 69/26  Benchmark Size \nExecution Time (in Seconds) MATLAB CPU CPU+8800 capr 256 x 512 500 x 1000 1000 x 2000 9.15 17.3 34.7 \n1.43 2.65 5.37 1.43 2.66 5.38 crni 2000 x 1000 4000 x 2000 8000 x 4000 16.05 64.1 254.8 6.33 24.7 104 \n6.39 23 103 dich 300 x 300 2000 x 2000 4000 x 4000 1.24 55.7 249.2 0.3 8.01 31.95 0.3 8.15 31.89 edit \n512 4096 8192 0.35 22.1 88.1 0.11 0.45 1.47 0.11 0.45 1.48 .ff 450 x 450 2000 x 2000 4096 x 4096 0.51 \n2.04 8.17 0.22 0.41 2.5 0.21 0.41 2.51 Table 1: Description of the benchmark suite. (dp) indicates bench\u00admarks \nwith data parallel regions. Number of kernels/number of CPU only statements are reported under the column \nTasks. calls to optimized BLAS libraries. The GPU versions for clos, which use the CUBLAS library, achieve \na speedup11 of 2.8X-5.6X on 8800 and Tesla. For nb1d and nb3d the GPU versions achieve speedups of 2.7X-4.0X \nand 8.8X-13.7X respectively. Lastly, even the CPU only version gives considerable speedups of 1.6X-13.4X \nfor fdtd, nb1d and nb3d on a single CPU core. Table 3 shows the runtimes for benchmarks with no or very \nfew data parallel regions. The performance bene.ts over MATLAB are mainly due to compiling the MATLAB \ncode to C++. Even here, we observe an improvement in execution time by factors of 2X-10X. Table 4 compares \nperformance of code generated by our com\u00adpiler with that of GPUmat. GPUmat requires the user to explicitly \nmark variables as GPU variables before an operation on them can be performed on the GPU. The user has \nto manually identify data parallel regions and insert code to move data to the GPU only in these regions. \nIncorrect use of GPU variables can have an adverse impact on performance. We used some information from \nthe code generated by our compiler to mark the GPU variables. GPUmat per\u00adforms considerably better than \nMATLAB. However, because our compiler can automatically identify GPU kernels and reduce the amount of \ndata transfer by intelligent kernel composition, we were able to get better performance (1.5X-10X) than \nGPUmat. 11 Benchmarks clos (with 4096 vertices) and fdtd (problem size of 1024x512x32 or 1024x1024x32) \ncould not be run on the 8800 as the kernel memory requirement exceeded the available memory (512MB) in \nthe GPU. Table 3: Runtimes for non data parallel benchmarks Benchmark Size Execution Time (in Seconds) \nMATLAB GPUmat CPU+Tesla bscholes 102400, 500 8.9 7.2 0.08 clos 2048 4.1 1.13 0.73 fdtd 512 x 512 x 15 \n187 30 2.94 nb1d 4096 14.8 12.03 5.44 nb3d 1024 224.8 172 16.67 Table 4: Comparison with GPUmat Our \ncompiler reorders loops to improve cache locality on CPUs and coalesced accesses on GPUs. Table 5 shows \nthe performance gains obtained by reordering loop nests. The benchmark fdtd has 3 dimensional arrays \nand bene.ts a lot on both the CPU and GPU with this optimization. Loop reordering improved the performance \nof fdtd CPU+GPU code by a factor of 16 and CPU only code by a factor of 5.1. Whereas for nb3d this optimization \nresults in an improvement of 3X for CPU+GPU code and 1.05X for CPU only code. Benchmark Execution Time \n(in Seconds) MATLAB CPU Only CPU + GPU No Opt Opt No Opt Opt fdtd 187 82.05 15.82 48.14 2.94 nb3d 224.8 \n50.1 47.5 47.6 16.7 Table 5: Performance gains with parallel loop reordering. Problem sizes are 512 \nx 512 x 15(fdtd) and 1024(nb3d). 6. Related Work Several previous projects have studied the problem of \ncompiling MATLAB. The .rst of these was the FALCON project that com\u00adpiled MATLAB programs to FORTRAN \n[10]. Most of our fron\u00adtend is based on work done in the FALCON project. MaJIC [2] is a just-in-time \nMATLAB compiler that compiled and executed MAT-LAB programs with a mixture of compile time and run-time \ntech\u00adniques. The MAGICA project [16] focussed on statically inferring symbolic types for variables in \na MATLAB program and establish\u00ading relationships between shapes of various variables. Mat2C [17] is a \nMATLAB to C compiler based on MAGICA. MATCH [13] is a virtual machine that executes MATLAB programs in \nparallel by mimicking superscalar processors. The possibility of using ex\u00adpensive compile time analysis \nfor generating high performance li\u00adbraries from MATLAB routines was studied by Chauhan et al [7]. More \nrecently, McVM [8], a virtual machine for the execution of MATLAB programs, that performs specialization \nbased on types has been developed. However, none of these projects studied auto\u00admatically compiling MATLAB \nto machines with distributed mem\u00adory or heterogeneous processors. Jacket [14] and GPUMat [12] are systems \nthat enable users to execute parts of MATLAB programs on GPUs. However, these require users to specify \nwhat is to be run on the GPU by annotating the source code. Arrays whose operations are to be performed \non the GPU are declared with a special type. This is similar to the concept of data parallel arrays in \nAccelerator [25]. Similarly, Khoury et al extend Octave to target the Cell BE processor [19]. However, \ntheir system still requires users to cast matrices to a speci.c type so that operations on these matrices \ncan be performed on the SPEs. It also uses a virtual machine based approach to perform matrix operations \non the SPEs as opposed to generating custom code. Several previous works have studied problems related \nto com\u00adpilation for GPUs. Techniques for optimizing na\u00a8ive CUDA ker\u00adnels were studied by Yang et al [29]. \nThe use of the polyhedral model to optimize code for accelerator architectures was studied by Baskaran \net al [4][5]. These techniques are complementary to those discussed in this paper as they can be used \nto optimize ker\u00adnels and low level code generated by our compiler. Previous works have also explored \ncompiling programs in other high level languages to GPUs. For example, compilation of stream programs \nto GPUs and heterogeneous machines was studied by Udupa et al [26][27]. To the best of our knowledge, \nour paper is the .rst to study the problem of automatically compiling MATLAB programs for execution on \nsystems with heterogeneous processors. 7. Conclusions and Future Work In this paper, we presented a compiler \nframework, MEGHA, for the automatic compilation of MATLAB programs for execution on systems with heterogeneous \nprocessors. We presented a technique for identifying and composing kernels in the input MATLAB pro\u00adgram \nwhile taking several costs and bene.ts into account. An ef.\u00adcient heuristic for automatically mapping \nand scheduling identi.ed kernels on CPU and GPU has also been proposed. The heuristic takes into account \ndata transfer while mapping kernels to proces\u00adsors. Then, a data .ow analysis based edge splitting strategy \nto han\u00addle dependencies across basic blocks was proposed. The proposed framework was implemented and \ninitial performance results indi\u00adcate that our approach is promising with performance gains up to 191X \nover native MATLAB execution being obtained. There are also several interesting directions for future \nwork. One interesting direction is to implement the techniques described in this paper as a mixture of \nstatic and dynamic optimizations. Also, in this paper, we did not consider GPU memory constraints while \nmapping and scheduling kernels. This needs to be considered for larger programs to be correctly run. \nOptimizing the generated kernels using shared memory and performing other optimizations on them also \nneeds to be studied. Acknowledgments We thank R. Manikantan and other members of the Lab for High Performance \nComputing, IISc for several helpful discussions re\u00adgarding the work described in this paper. We would \nalso like to thank the anonymous reviewers for several valuable suggestions that helped improve our paper. \nWe would also like to acknowledge funding received from Microsoft and NVIDIA. References [1] A. V. Aho, \nRavi Sethi, J. D. Ullman, M. S. Lam. Compilers: Principles, Techniques, &#38; Tools. Pearson Education, \n2009. [2] G. Almasi, D. Padua. MaJIC: Compiling MATLAB for Speed and Responsiveness. In the ACM SIGPLAN \n2002 Conference on Programming Language Design and Implementation (PLDI 02). [3] ATI Technologies, http://ati.amd.com/products/index.html \n[4] M. Baskaran, U. Bondhugula, S. Krishnamoorthy, J. Ramanujam, A. Rountev, P. Sadayappan. A Compiler \nFramework for Optimization of Af.ne Loop Nests for GPGPUs. In the 22nd Annual International Conference \non Supercomputing (ICS 08). [5] M. Baskaran, U. Bondhugula, S. Krishnamoorthy, J. Ramanujam, A. Rountev, \nP. Sadayappan. Automatic Data Movement and Computation Mapping for Multi-level Parallel Architectures \nwith Explicitly Managed Memories. In the 13th ACM SIGPLAN Symposium on Principles and Practice of Parallel \nProgramming (PPoPP 08). [6] U. Bondhugula, A. Hartono, J. Ramanujam, P. Sadayappan. A Practical Automatic \nPolyhedral Parallelizer and Locality Optimizer. In the 2008 ACM SIGPLAN conference on Programming language \ndesign and implementation (PLDI 08). [7] A. Chauhan, C. McCosh, K. Kennedy, and R. Hanson. Automatic \nType-Driven Library Generation for Telescoping Languages. In the 2003 ACM/IEEE Conference on Supercomputing \n(SC 03). [8] M. Chevalier-Boisvert, L. Hendren, C. Verbrugge. Optimizing MAT-LAB Through Just-In-Time \nSpecialization. In the 2010 International Conference on Compiler Construction (CC 10). [9] R. Cytron, \nJ. Ferrante, B. K. Rosen, M. N. Wegman, F. K. Zadeck. Ef.ciently Computing Static Single Assignment Form \nand the Control Dependence Graph. In the ACM Transactions on Programming Languages and Systems, 13(4):451 \n490, Oct. 1991. [10] L. De Rose, D. Padua. Techniques for the translation of MATLAB programs into Fortran \n90. In the ACM Transactions on Programming Languages and Systems, 21(2):286 323, Mar. 1999. [11] J. W. \nEaton. GNU Octave Manual, Network Theory Limited, 2002. [12] GPUMat Home Page. http://gp-you.org/ [13] \nM. Haldar et. al. MATCH Virtual Machine: An Adaptive Run-Time System to Execute MATLAB in Parallel. In \nthe 2000 International Conference on Parallel Processing (ICPP 00). [14] Jacket Home Page. http://www.accelereyes.com/ \n[15] P. Joisha, P. Banerjee. Static Array Storage Optimization in MATLAB. In the ACM SIGPLAN 2003 conference \non Programming language design and implementation (PLDI 03). [16] P. Joisha, P. Banerjee. An Algebraic \nArray Shape Inference System for MATLAB. ACM Transactions on Programming Languages and Systems, 28(5):848 \n907, September 2006. [17] P. Joisha, P. Banerjee. A Translator System for the MATLAB Language, Research \nArticles on Software Practices and Experience 07. [18] K. Kennedy, K. S. McKinley. Maximizing Loop Parallelism \nand Improving Data Locality via Loop Fusion and Distribution. In the 6th International Workshop on Languages \nand Compilers for Parallel Computing (LCPC 93). [19] R. Khoury, B. Burgstaller, B. Scholz, Accelerating \nthe Execution of Matrix Languages on the Cell Broadband Engine Architecture. IEEE Transactions on Parallel \nand Distributed Systems, 22(1):7 21, Jan. 2011. [20] E. Lindholm, J. Nickolls, S. Oberman, J. Montrym. \nNVIDIA Tesla: A Uni.ed Graphics and Computing Architecture. IEEE Micro, March 2008. [21] Mathworks Home \nPage. http://www.mathworks.com/ [22] NVIDIA Corp, NVIDIA CUDA: Compute Uni.ed Device Architec\u00adture: Programming \nGuide, Version 3.0, 2010. [23] NVIDIA Corp, Fermi Home Page, http://www.nvidia.com/object/fermi architecture.html \n [24] S. K. Singhai, K. S. Mckinley. A Parametrized Loop Fusion Algorithm for Improving Parallelism and \nCache Locality, Computer Journal, 1997. [25] D. Tarditi, S. Puri, J. Oglesby. Accelerator: Using Data \nParallelism to Program GPUs for General-Purpose Uses. In the 12th International Conference on Architectural \nSupport for Programming Languages and Operating Systems (ASPLOS-XII). [26] A. Udupa, R. Govindarajan, \nM. J. Thazhuthaveetil. Software Pipelined Execution of Stream Programs on GPUs. In the 7th annual IEEE/ACM \nInternational Symposium on Code Generation and Optimization (CGO 09). [27] A. Udupa, R. Govindarajan, \nM. J. Thazhuthaveetil. Synergistic Execution of Stream Programs on Multicores with Accelerators. In the \n2009 ACM SIGPLAN/SIGBED Conference on Languages, Compilers, and Tools for Embedded Systems (LCTES 09). \n[28] V. Volkov, J. W. Demmel. Benchmarking GPUs to Tune Dense Linear Algebra. In the 2008 ACM/IEEE Conference \non Supercomputing (SC 08). [29] Y. Yang, P. Xiang, J. Kong, H. Zhou. A GPGPU Compiler for Memory Optimization \nand Parallelism Management. In the 2010 ACM SIGPLAN conference on Programming Language Design and Implementation \n(PLDI 10).   \n\t\t\t", "proc_id": "1993498", "abstract": "<p>MATLAB is an array language, initially popular for rapid prototyping, but is now being increasingly used to develop production code for numerical and scientific applications. Typical MATLAB programs have abundant data parallelism. These programs also have control flow dominated scalar regions that have an impact on the program's execution time. Today's computer systems have tremendous computing power in the form of traditional CPU cores and throughput oriented accelerators such as graphics processing units(GPUs). Thus, an approach that maps the control flow dominated regions to the CPU and the data parallel regions to the GPU can significantly improve program performance.</p> <p>In this paper, we present the design and implementation of MEGHA, a compiler that automatically compiles MATLAB programs to enable synergistic execution on heterogeneous processors. Our solution is fully automated and does not require programmer input for identifying data parallel regions. We propose a set of compiler optimizations tailored for MATLAB. Our compiler identifies data parallel regions of the program and composes them into kernels. The problem of combining statements into kernels is formulated as a constrained graph clustering problem. Heuristics are presented to map identified kernels to either the CPU or GPU so that kernel execution on the CPU and the GPU happens synergistically and the amount of data transfer needed is minimized. In order to ensure required data movement for dependencies across basic blocks, we propose a data flow analysis and edge splitting strategy. Thus our compiler automatically handles composition of kernels, mapping of kernels to CPU and GPU, scheduling and insertion of required data transfer. The proposed compiler was implemented and experimental evaluation using a set of MATLAB benchmarks shows that our approach achieves a geometric mean speedup of 19.8X for data parallel benchmarks over native execution of MATLAB.</p>", "authors": [{"name": "Ashwin Prasad", "author_profile_id": "81485640981", "affiliation": "Indian Institute of Science, Bangalore, India", "person_id": "P2690521", "email_address": "ashwin@hpc.serc.iisc.ernet.in", "orcid_id": ""}, {"name": "Jayvant Anantpur", "author_profile_id": "81485656317", "affiliation": "Indian Institute of Science, Bangalore, India", "person_id": "P2690522", "email_address": "jayvant@hpc.serc.iisc.ernet.in", "orcid_id": ""}, {"name": "R. Govindarajan", "author_profile_id": "81453614211", "affiliation": "Indian Institute of Science, Bangalore, India", "person_id": "P2690523", "email_address": "govind@serc.iisc.ernet.in", "orcid_id": ""}], "doi_number": "10.1145/1993498.1993517", "year": "2011", "article_id": "1993517", "conference": "PLDI", "title": "Automatic compilation of MATLAB programs for synergistic execution on heterogeneous processors", "url": "http://dl.acm.org/citation.cfm?id=1993517"}