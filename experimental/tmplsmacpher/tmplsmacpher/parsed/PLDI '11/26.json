{"article_publication_date": "06-04-2011", "fulltext": "\n Safe Optimisations for Shared-Memory Concurrent Programs Jaroslav Sev..c\u00b4ik University ofCambridge,TheMathWorks \n jarin.sevcik@gmail.com Abstract Current proposals for concurrent shared-memory languages, in\u00adcluding \nC++ and C, provide sequential consistency only for pro\u00adgrams without data races(the DRFguarantee). While \ntheimplica\u00adtions of such a contract for hardware optimisations are relatively well-understood, the correctness \nof compiler optimisations under the DRF guarantee is less clear, and experience with Java shows that \nthis areais error-prone. Inthispaper wegive a rigorous study of optimisationsthatin\u00advolveboth reordering \nand elimination of memory reads and writes, covering manypracticallyimportant optimisations.We .rstde.ne \npowerful classes of transformations semantically, in a language\u00adindependent trace semantics. We prove \nthat any composition of thesetransformationsissound with respecttotheDRFguarantee, and moreover that \ntheyprovide basic securityguarantees(no thin\u00adair reads) even for programs with data races. To give a \nconcrete example, we apply our semantic results to a simpleimperativelan\u00adguage andprovethat several syntactictransformations \naresafefor that language. We also discuss some surprising limitations of the DRFguarantee. Categories \nand Subject Descriptors D.3.3[LanguageConstructs and Features]: Concurrent programming structures; D.3.4 \n[Pro\u00adcessors]: Optimization; F.3.2 [Semantics of Programming Lan\u00adguages] General Terms Languages,Reliability,Theory,Veri.cation \nKeywords Relaxed Memory Models, Semantics, Compiler Opti\u00admizations 1. Introduction Standardcompiler optimisation, \nsuch as common expression elimi\u00adnation, violate sequentially consistent semanticsfor multi-threaded programs.For \nexample, observe that thefollowingprogram cannot printvalue 1 in any ofitsinterleavings. initiallyrequestReady \n= responseReady = data = 0 Thread1 Thread2 data := 1 if (requestReady==1) { requestReady := 1 data := \n2 if (responseReady==1) responseReady := 1 print data } Permission to make digital or hard copies of \nall or part of this work for personal or classroomuseisgranted withoutfeeprovided that copiesarenot madeordistributed \nforpro.tor commercial advantage andthat copiesbearthis notice andthefull citation onthe .rstpage.Tocopy \notherwise,torepublish,topostonservers ortoredistribute tolists, requiresprior speci.cpermission and/or \nafee. PLDI 11, June4 8,2011,SanJose,California,USA. Copyright c &#38;#169; 2011ACM978-1-4503-0663-8/11/06. \n. .$10.00 However, an optimising compiler propagates the constant 1 from the data:=1 statement and replaces1 \nprint data with print 1. While such an optimisation would be correct for sequential pro\u00adgrams, itis not \nsafe for thisprogram in the sequentially consistent semanticsbecause unlikethe originalprogram,the optimisedpro\u00adgram \ncan output 1. Sucha resultis worrisomebecause mosttheoreticians andprac\u00adtitionersassumeasequentially \nconsistentsemantics.Indeed, some researchers believe that compilers should perform only those op\u00adtimisations \nthat do not break the sequentially consistent semantics (see \u00a77 for more details). In contrast, designers \nof common lan\u00adguages and hardware do permit and implement aggressive optimi\u00adsations that can exhibit \nsurprising behaviours for multi-threaded programs, but they provide mechanisms for recovering a sequen\u00adtially \nconsistent semantics.On multi-processors,programmers can constrain optimisations using memoryfenceinstructions, \nwhich of\u00adten have intricate semantics [3, 11, 20] and high run-time costs, usuallyin the order of tens \ntohundreds of cycles. In higher-level programming languages, the recent trend is to guaranteeinterleavedsemanticsforprograms \nwithoutdata races[6, 16]; we will call such a speci.cation the DRF guarantee [1]. As programming without \ndata races is considered good practice, the DRFguaranteeis safefor well-engineeredprograms while validat\u00adingmost \ncompilersbecausethepossibly unintended effects of com\u00admon optimisations cannot be observed without data \nraces. How\u00adever, to our knowledge, there is little rigorous evidence for this claim.We emphasise that \ntheDRFguaranteeleaves thebehaviours ofprograms with races unspeci.ed,possibly leading to executions wherevaluesappear \nout-of-thin-air [16].Thisisunacceptablefor languages that aim to give basic security guarantees for arbitrary \nprograms, e.g.,Java applets. Contribution. WeprovetheDRFguarantee and theabsence of out-of-thin-air values \nfor a large class of compiler optimisations. In more detail, we design a novel trace-semantic characterisation \nof thread-local program transformations that is suitable for rea\u00adsoning about validity of common optimisations \nof concurrent data race free programs. Using the characterisation, we establish that the transformations \ncannot introduce behaviours for race free pro\u00adgrams andpreventout-of-thin-air valuesfor arbitraryprograms.We \ndemonstrate thepower of the semantic techniqueby applyingit on several syntactic transformations, such \nas reorderings of indepen\u00addent statements, and eliminations of redundant memory accesses in the sameblock.The \nmain advantage ofthe semantic approachis itsindependence from syntax:it allows using the sameproof tech\u00adniques \nfor different languages, such as intermediate languages in compilers. We believe that our semantic transformations \nare gen\u00aderal enough to capture most thread-local optimisations performed by realistic compilers.We also \nshow that,perhaps surprisingly, re\u00ad 1For example, the gcc compiler version 4.1.2 on the x86 architecture \nperforms this optimisation.  dundant read introduction invalidates some optimisations that are otherwise \nsafefor theDRF-guarantee. Approach. We view programs as sets of traces of its individ\u00adual threads(\u00a73); \noptimisations are modelled as relations on these tracesets(\u00a74). Our main result shows that given a .nite \nchain of programs, where the .rst program is data race free and the opti\u00admisation relation relates adjacent \nprograms in the chain, the set of behaviours of the last program is a subset of the set of behaviours \nofthe .rstprogram(\u00a75), where the behaviours are sequences of externally observable actions(input or output) \nof allinterleavings of theprogram. To show the absence of out-of-thin-air values we observe that all \nour semantic transformationspreserve animportantproperty:if a thread writes or outputs a value then it \nmust have read the value before.Usingthisproperty, weprove our out-of-thin-airguarantee: if a program \ndoes not contain constant c explicitly and there is no way to build c (for example because c is an integer \nand the program does not contain any arithmetic), then no transformation of theprogram can read, write \nor output value c (\u00a75). We have carried most of our work on semantic level, but we show how to apply \nour work by de.ning a simple imperative lan\u00adguage withsynchronisationprimitivestogether with several \nsimple syntactic transformations, andproving them safe(\u00a76). 2. Traces,DRF andTransformations In our examples, \nwe use a simpleC-like language. By convention, variables with the name beginning with r are local(registers), \nthe remaining variables reside in distinct shared-memory locations. Similarly toJava[10,16] orC++0x[4 \n6], somelocations canbe designatedby theprogrammer as volatile (atomics in C++0x). We make the syntax \nand semantics formal in \u00a76. Intuitively, volatile locations areintendedfor synchronisation between threads \nandfor thepurposes of theDRFguarantee,data races on volatilelocations do not count as data races, i.e., \na program is data race free if it cannot perform two con.icting accesses to the same non-volatile location \nat the same time.Wegive a formal de.nition of data race freedom in \u00a73. Technically, the set of volatile \nlocations should be part of a program. In our examples, all locations are non-volatile, unless stated \notherwise. We assume that all locations are zero\u00adinitialised. We represent programs as sets of memory \naction traces where the trace is a sequence of memory actions of a single thread. We havethefollowingmemory \nactions: R[l=v]is a readfromlocation l with value v; W[l=v] a write to l with value v, L[m] lock of monitor \nm;U[m]an unlock of m;X(v)an external action(input or output) with value v; S(e)is a thread start action \nwith entry point e, where the entrypointis a threadidenti.er. 2.1 TransformationsbyExample We consider \nfour classes ofprogram transformations andillustrate them on simple examples. We emphasise that the syntactic \nexam\u00adples cover only small range of all the transformations allowed by the semantics. Trace preserving \ntransformations. Since our trace semantics only includes shared-memory optimisations, many otherwise \nnon\u00adtrivial optimisations, such as loop unrolling or inlining, are iden\u00adtity optimisations in the trace \nsemantics because they do not af\u00adfect memory accesses. Interestingly, the trace semantics does not directly \ncapture syntactic dependencies. For example, the code snippets r:=x; if (r==0) y:=1 else y:=1 and r:=x; \ny:=1 have the same sets oftraces the set of all sequences of a read of x followedby a write of 1to y. \nThread 0 Thread 1 Thread 0 Thread 1 r1:=y r1:=y x:=2 y:=1 x:=1 print r1:=x r2:=x r1 y:=1 x:=1 print r1 \nr1:=x r2:=r1 print r2 print r2 (original) (transformed) Figure1. Elimination example. Elimination. TheexampleinFig.1 \nshows an eliminationof an overwritten write to x in the .rst thread and an elimination of a re\u00addundantreadfrom \nx inthe secondthread.Notethatsuch atransfor\u00admationis not safein a sequentially consistent semantics: \nunlike the original program, the transformed program can output 1 followed by 0, assuming that all memory \nlocations are zero-initialised.This does notviolatetheDRFguarantee sincetheprogram containsdata races \non x and y.In the absence ofdata races, we show that elimi\u00adnations cannotintroduce newbehaviours(\u00a75). \nEliminations are easy to describe on traces: intuitively, a pro\u00adgram is an elimination of another program \nif for each trace of the transformed program there is a trace in the original program such that we can \nobtain the transformed trace by eliminating some re\u00addundant actions from the original trace. For example, \nconsider the trace t =[S(1),R[y=1],X(1),R[x=0],X(0)] of Thread 1 of the transformed program from Fig. \n1 and observe that we can obtain t from trace [S(1),R[y=1],X(1),R[x=0],R[x=0],X(0)] of the original program \nby removing the redundant read of x. We con\u00adsider the read redundant because there is an earlier read \nfrom the same location of the same value. We give the precise de.nition of variouskinds of redundant \nactions and of the semantic elimination in \u00a74. The semantic elimination transformation is general enough \nto cover optimisations that eliminate memory accesses based on data-.ow analyses, i.e., common subexpression \nelimination, con\u00adstantpropagation, or evenloop-invarianthoistingif combined with loop unrolling. Reordering. \nIn the example in Fig. 2, we show a reordering of a read from y with a later write to x. Again, this \ntransforma\u00adtion is not safe in the interleaved semantics because the original program cannot print 1, \nas opposed to the transformed program (assuming zero-initialised memory). For semantic reordering we \nonly require each trace of the transformedprogram tobe apermu\u00adtation of some trace of the original program \nwith certain restric\u00adtions, such as preventing reordering of two con.icting accesses to the same memory \nlocation. Note that this de.nition can allow re\u00adordering of actions that are control dependent or falsely \ndata de\u00adpendent. Using our notion of reordering, the code snippet r:=x; if (r==1) {y:=1;z:=1}else {z:=1;y:=1} \nis a reordering of y:=1;z:=1;r:=x because any trace of the latter is a permutation of a trace of theformer.Wegive \naprecisede.nition of the seman\u00adtic reordering in \u00a74 and show its safety for DRF programs in \u00a75. The semantic \nreordering transformation covers code motion trans\u00adformations, which are typically employed inloop optimisations. \nIntroduction. It is known that write introduction (sometimes called write speculation)generally violates \nthe DRFguarantee be\u00adcause theintroduced write mightbe seenby adifferent thread even though there was \nnodata racein the originalprogram[6].Itisless clear whether a redundant read introduction can violate \nthe DRF guarantee in practice, especially in the seemingly harmless case where the program never uses \nthe value obtained from the intro\u00adduced read. Unsurprisingly, if we introduce irrelevant reads and execute \non a sequentially consistent architecture, the reads cannot  Thread0 Thread1 r2:=y r1:=x x:=1 y:=r1 \n print r2 Thread0 Thread1 x:=1 r1:=x r2:=y y:=r1 print r2 (original) (transformed) Figure2. Reordering \nexample. r1 := y r2 := x lock m lock m lock m lock m x := 1 y := 1 x := 1 y := 1 print y print x print \ny print x unlock m unlock m unlock m unlock m (a)original (b)withintroduced reads r1 := y r2 := x lock \nm lock m x:= 1 y:= 1 print r1 print r2 unlock m unlock m (c)after read elimination Can theprogramprint \ntwo zeros? Figure3. Irrelevant readintroduction. changethebehaviours.However,ifcombinedwithotherwiseDRF\u00adfriendly \noptimisations, we can obtain non-sequentially consistent behaviourfromprograms thataredata racefree.For \nexample, note that the .rst program from Fig. 3 cannot print two zeros, but if an optimiserinsertsirrelevant \nreads and then reuses theintroduced reads to eliminate other reads, asillustratedby theprograms(b) and(c)inthe \n.gure,theresultingprogramcanprinttwozeroseven on a sequentially consistent architecture. Onemight .ndboth \ntheoptimisationsfromFig.3dubious,but compilers(including gcc)do introduce reads when hoisting reads from \na loop. Although we have not seen the redundant read elim\u00adination across synchronisation in any compiler \nyet, this optimisa\u00adtion has been proposed and implemented in gcc for the upcoming C++0x implementation \n[12]. In any case, this case of redundant read eliminationis safein theDRFguarantee(\u00a74). 3. TraceSemantics \nWe begin the technical development with setting up our intuitive trace semantics rigorously. Actions, \nTraces and Interleavings. Our traces are sequences of memory operations. In addition to the standard \nread, write, thread start and synchronisation operations, the traces also include external I/O operations, \nsuch as printing, because ultimately we wish to reason about observable I/Obehaviours. The thread start \naction is always the .rst action of a thread. Its purpose is toprovide a connection between theidentityof \na thread and its entry point. To simplify the discussion, we create threads staticallyand we use threadidenti.ers \nas entrypoints. We will use the following terminology to refer to classes of actions: a memory access \nto location l is a read or a write to l; a volatile memory access (resp. read, write) is a memory access \n(resp. read, write)to a volatile location; a normal memory access (resp.read, write)is an access(resp. \nread,write)toanon-volatile location; an acquire action is either a lock or a volatile read; a release \naction is an unlock or a volatile write; a synchronisation actionis an acquire or release action. To \nwork with sequences of actions we use the following nota\u00ad ' '' tion: t+ t is a concatenation of lists \nt and t ; we write t = t if t is pre.x of t ' , i.e., if there is s such that t+ s = t ' . Trace t ''' \n' is a strict pre.x of t (t<t ), if t = t and t t = ; |t| denotes the length of the sequence t; ti is \ni-th element of the list t, in\u00addices are 0-based;[a . t.P(a)]standsfor thelist of allactionsin list t \nthat satisfycondition P,infunctional languages, thisis often written as filter Pt; we generalise the \n.lter notation to a map\u00ad.lter notation;the expression [f(a)| a . t.P(a)]denotes thelist [a . t.P(a)] \nwith each element transformed by function f, in functionallanguages, one would writethis as map f (filter \nPt); t|S is a sublist of t that contains all elementswithindicesfrom S; for example, [a,b,c,d]|{1,3} \nis [b,d]; dom(t)is the set of all in\u00addices to t: dom(t)= {0,...,|t|- 1}; ldom(t)is the list of all indices \nto t in theincreasing order: ldom(t)=[0,...,|t|- 1]. Programs are represented as sets of traces, calledtracesets.The \ntracesin a tracesetdo not have tobe complete; their execution can .nishat anypoint.We modelthisbyassumingthat \nthe set oftraces ofaprogramispre.x-closed,i.e.,for traceset T, t = t ' and t ' . T implies t . T.We also \nrequire the tracesets tobe well locked,i.e., for each t . T and monitor m, the number of unlocks of m \nin t is not greater than the number of locks of m in t. All traces in a traceset must be properly started \nmeaning that if a trace is not emptyits .rst actionmustbeastart action. For example, the traceset of \nthe .rst program in Fig. 2 is the pre.x closure ofthefollowing set(V is the set of values): {[S(0),R[x=v],W[y=v]]| \nv . V} .{[S(1),R[y=v],W[x=1],X(v)]| v . V}. We should note that this notion of traceset is rather weak \nas it does not enforce determinism or receptiveness. For instance, the set of traces {[S(0)],[S(0),R[x=1]],[S(0),W[y=1]]} \nis a valid traceset.Havingnon-determinismis usefulto modelunderspeci.ed features oflanguages, such as \ntheloose evaluation orderinC/C++. Interleavings and Executions. Interleavings are sequences of thread-identi.er \nactionpairs.For apair p = (.,a), we write A(p) to refer to the action a, and T (p)for..Asequence ofsuchpairsis \nan interleaving.Given aninterleavingI,the trace of .inI isthe se\u00adquence of actions of thread . in I,i.e., \n[A(p)| p . I.T (p)= .]. In the text, we often omit the projection A(-)and write Ii is a read instead \nof A(Ii)is a read .We mayalso omit theinterleav\u00adingI and write iis a read ifI is obviousfrom the context. \nInterleaving is an execution of traceset T that respects mutual exclusion, its reads see the values of \nmost recent writes and the traces ofitsthreadarein T.Formally,interleavingI is aninterleav\u00ading of traceset \nT if for all thread identi.ers ., the trace of . is in T,threadidenti.ers correspond to entry-points,i.e., \nA(Ii)=S(.) implies T (Ii)= . for all i and ., and A(Ii) =L[m]implies that for each thread . T (Ii)wehave \n= |{j| j<i.T (Ij)= ..A(Ij)=L[m]}| = |{j| j<i.T (Ij)= ..A(Ij)=U[m]}|. We say that r . dom(I)(i)sees write \nw if A(Ir)=R[x=v], A(Iw)=W[x=v]for some x, v and w< r, andfor allisuch that w<i<r the action Ii is not \na write to l,(ii)sees default value if Ir is a read of the default value from l (typically0)and there \nis no write i<r to location l in I,(iii)sees the most recent write if r sees the default value or it \nsees some write w or r is not a read. Interleaving I is sequentially consistent if all j . dom(I)see \nthe most recent write in I. Sequentially consistent interleavings of T are called executions of T.  \nOrders on Actions. The sequencing of actions in interleavings imposes a totalorder on the execution of \nactions.In reality, actions are oftenperformed concurrently.We will model thisby construct\u00ading apartialhappens-before \norder[13],which relatesactions only ifthey are orderedby theprogram code orby synchronisation. First,wede.netheprogram \nordertorelateactionsof thesame thread in interleaving I, i.e., =I = {(i,j) | 0 = i = j< po |I|.T (Ii)= \nT (Ij)}. We say that i synchronises-with j, written i<I j,if i<j < |I| and A(Ii), A(Ij)are a release-acquirepair, \nsw where actions a and bare a release-acquire pairifa is an unlockof monitor m and bis alockof m, or \na is a writeto a volatilelocation land bis a readof l.The happens-before order of I isthetransitive closure \nofprogram order and synchronizes-with. Note that i =I j implies i = j, and i synchronises-with j po implies \ni <j. Hence, i =I jimplies i = j. As any subset of a hb total order is antisymmetric and =I hb is transitive \nand re.exive by construction, thehappens-before orderis apartial order. A matching is a function that \nrelates the actions in two traces or interleavings. Formally, matching between lists I and I ' is a partial \ninjective function f from dom(I) to dom(I ' ) such that Ii = If' (i) for all i . dom(f) . The matching \nf is complete if dom(f) = dom(I). We use matchings to relate actions in a trace(resp.interleaving) of \na transformedprogram to a trace(resp. interleaving) of the originalprogram. Data Race Freedom. Two actions \nare con.icting if they access the same non-volatile location and at least one of them is a write. Aninterleavinghas \nadata race ifitcontains two adjacent con.ict\u00ading actions from different threads. A traceset is data race \nfree if none of its executions has a data race. Equivalently, one could de\u00ad.nedata racefreedom usingthehappens-before \nrelation:aprogram is data race free if in all its executions, all the pairs of con.icting actions are \nordered by the happens-before order of the execution [6,21]. The common way of ensuring data race freedom \nis protecting every shared-memory location with alock.Then there cannot be a data race because there \nmust be an unlock-lock pair of actions on the same monitorbetween any two accesses tothe samelocationin \nany execution.Alternatively, we can use volatilelocations to make a program data race free. For example, \nif we mark the locations requestReady and responseReady inthe .rstprogramin \u00a71 as volatile, theprogrambecomesdata \nracefree. 4. SemanticTransformations We nowde.ne eliminations and reorderings semantically. Eliminations. \nTo de.ne semantic read eliminations, we intro\u00adduce wildcard traces. The wildcard traces are generalisations \nof ordinary traces, where each each element of a wildcard traceis ei\u00adther an action or a wildcard read \nR[x=*]. We use the wildcards to express independence of the trace s validity on the value that thewildcardreads \nmightread.Wesaythata(normal) trace t is an instance of a wildcard trace t ' , if we can obtain t by replac\u00ading \nall wildcardsin t ' with some concrete values.A wildcard trace belongs-to traceset T ifT contains allinstances \nof the trace. Similarly, we de.ne wildcard interleavings to be interleavings with someordinary actions \nreplacedby wildcard reads.Weobtain an instance of a wildcard interleaving by replacing each wildcard \nreadby a read ofthe samelocation withthe value ofthe most recent write to the same location, or with \nthe default value if there is no earlier write to the same location. As opposed to trace instances, the \ninstance of an interleaving is unique. We say that a wildcard interleaving belongs-to T iffor each thread \n., the(wildcard) trace of . belongs-to T. For example,let T be the traceset of thefollowingprogram y:=1; \n r2:=y; r1:=x; x:=1; print r1; and observe that the wildcard traces [S(0),W[y=1],R[x=*]]and [S(1),R[y=*],W[x=1]] \nbelong-to T. In contrast, the wildcard trace [S(0),W[y=1],R[x=*],X(1)]does notbelong-toT because some \nofitsinstances, e.g., [S(0),W[y=1],R[x=2],X(1)], are not in T. Our de.nition of eliminations on traces \nconsiders pairs of pos\u00adsibly non-adjacent memory accesses and identi.es the conditions on theintervening \nactions that enable elimination of one of the ac\u00adcesses.Moreover, thede.nition allows removal ofirrelevant(wild\u00adcard) \nreadsand certainactionsfromtheend of thetrace.The last\u00adaction eliminations are usefulfor reordering. \nDe.nition1. We say that there is a release-acquirepairbetween i and jin tracetifthere arer and a such \nthat i<r< a<j, tr is a release and ta is an acquire.Given trace t, we say that i . dom(t) is 1. redundant \nread after read ifti = tj =R[l=v]for some v, non\u00advolatile l and j<i, and there is no release-acquire \npair or write to l between jand i, 2. redundant read after write if ti = R[l=v], tj = W[l=v]for some \nv, non-volatile l and j<i, and there is no release\u00adacquirepair or write to l between jand i, 3. irrelevant \nread if ti is a wildcard non-volatile read, 4. redundant write after read if ti = W[l=v], tj = R[l=v]for \nsome v, non-volatile l and j<i, and there is no release\u00adacquirepair or other access to l between jand \ni, 5. overwritten write if ti = W[l=v], tj = W[l=v ' ]for some v, v ' , non-volatile l and j<i, and \nthere is no release-acquire pair or other access to l between jand i, 6. redundant last write if ti \nis a normal write and there is no later release action and nolater memory accesstothe samelocation, \n7. redundant release if ti is a release and there are no later syn\u00adchronisation or external actions, \n 8. redundant external action if ti is an external action and there are no later synchronisation or external \nactions.  Anindexiiseliminable intifisatis.es one ofthe conditions above. Given traces t and t ' , the \ntrace t ' is an elimination of t if there is S . dom(t) such that t ' = t|S and all i . dom(t)\\ S are \neliminable in t.A traceset T ' is an elimination of a set of traces T if each trace t ' . T ' is an elimination \nof some wildcard trace that belongs-to T. For example,in the wildcard trace [S(0),W[x=1],R[y=*],R[x=1], \nX(1),L[m],W[x=2],W[x=1],U[m]], theindices 2, 3, and 6are eliminable; soits elimination couldbethe trace \n[S(0),W[x=1],X(1),L[m],W[x=1],U[m]]. For an example of eliminations on tracesets, observe that all traces \nof the traceset of theprogram x:=1; print 1; lock m; x:=1; unlock m; are eliminations of some tracesbelonging-to \nthe traceset of x:=1; r1:=y; r2:=x; print r2; if (r2!=0) {lock m; x:=2; x:= r2; unlock m;} so theformer \ntracesetis an elimination of thelatter. Reordering. The reordering transformation allows changing or\u00adder \nofexecution of memory actions.However, not allpermutations of actions preserve behaviours. We say that \na is reorderable with b if either(i) a is a non-volatile memory access, and b is a non\u00adcon.icting non-volatile \nmemory access, or an acquire action, or an external action; or (ii) b is a non-volatile memory access, \nand a is a non-con.icting non-volatile memory access, or a release, or an external action.Thefollowingtable \nsummarisesthepermissible re\u00adorderingin a more readableform(cf.DougLea s cookbook [14]).  b = a = W[x=vx]1 \nR[x=vy]1 Acq Rel Ext W[y=vy]1 x = y x = y . \u00d7 . R[y=vy]1 x = y . . \u00d7 . Acquire \u00d7 \u00d7 \u00d7 \u00d7 \u00d7 Release . . \n\u00d7 \u00d7 \u00d7 External . . \u00d7 \u00d7 \u00d7 Note that reorderabilityis not symmetric as we can reorder a write with a later \nacquire, but not the opposite. The only reason for the asymmetry is the so-called roach motel reordering \n[16], i.e., moving non-volatile memory accesses into synchronized blocks (for example, see rules R-RL,R-UWinFig.11). \n' '' AtracesetT is a reorderingof atraceset T ifeachtrace t inT is apermutation ofsometrace tfromT.Moreover,thepermutation \nhas to satisfy two conditions: (i) it may only swap reorderable actions, (ii) if we apply the permutation \nto any pre.x of t ' , i.e., if weleave outfrom t all the actions that are notin thepre.x, then the resulting \ntracebelongs to T. In the rest of this section we will make this de.nition precise and then we apply \nthe de.nition to a simple example. Given trace t, a bijection f : dom(t) . dom(t)is a reordering function \nfor t if for all i <j we have that f(j) <f(i) implies that tj is reorderable with ti. It might seem that \nti should reorderable with tj and notthe opposite,but reorderingfunctiontransformstracesin the opposite \nway:from traces of the transformedprogram to traces of the originalprogram. Before lifting the notion \nof reordering to traces and tracesets, we de.ne a de-permutation of a pre.x of a given trace t using \nfunction f. Formally, for n =|t| and bijection f on dom(t), the de-permutation of t oflength n,denotedby \nf. (t),is the trace <n -1 tf-1 (i) | i . ldom(t).f(i)<n . The de-permutation of t, written f.(t),is thede-permutation \nof t oflength |t|. Note f is a complete matching between t and f.(t). Now we lift the notion of reordering \nto tracesets: Given a set of traces T ' ''' and trace t , function f : dom(t ) . dom(t )de-permutes t \nto T if f is a reordering function for t ' and for any n =|t ' | we have f.' <n(t). T.A set of traces \nT is a reordering of a set of traces T '' ' iffor each t . T thereis afunction thatde-permutes t into \nT. We willdemonstrate the application ofthe reorderingde.nition onthe examplefromFig.2.Thetracesets oftheprogram \nontheleft is thepre.x closure of the set T ={[S(0),R[x=v],W[y=v]]| v . N}. {[S(1),R[y=v],W[x=1],X(v)]| \nv . N}. The traceset of the transformedprogramis thepre.x closure of T ' ={[S(0),R[x=v],W[y=v]]| v . \nN}. {[S(1),W[x=1],R[y=v],X(v)]| v . N}. Ideally, we wouldliketo showthatT ' is a reorderingof T,i.e.,that \nfor any trace t ' . T ' there is permutation function de-permuting t ' to T. However, this is not the \ncase, because none of the two 1Locations x and y are not volatile. permutations ofthe trace [S(0),W[x=1]] \nbelongs toT.Therefore, traceset T ' cannotbe a reordering of T. This is where the eliminations become \nuseful: we can obtain the trace [S(0),W[x=1]] by eliminating the irrelevant readfrom y from the wildcard \ntrace [S(0),R[y=*],W[x=1]], which belongs\u00adto T.Moreprecisely,let T = T . [S(0),W[x=1]] and note that \nT is an elimination of T. It remains to check that T ' is a reordering of T .Let usillustrate this on \nt ' =[S(0),W[x=1],R[y=1],X(1)]. Let n .' f = {(0,0) ,(1,2) ,(2,1) ,(3,3)} t = f<n(t ) n ' n The meaning \nof tis the following: for n = |t |, tis obtained '' n from t by applying the function f so that t = tf(i) \nor, equiva\u00ad i ' n '' f-1 (i) lently, t = ti for all i . dom(t ). If n< |t |, then we use the transformation \nonly on thepre.x of t ' oflength n.Forillustra\u00ad tion, see Figure 4. Since tn . T for any n =|t ' | =4, \nwe satisfy thede.nition of reordering and f reorders T to t ' .Similarly,for all '' ' other traces t \nfrom T thereis afunction thatde-permutes t to T . We shouldnotethat most ofthecomplexityhereis required \nonly to cover the roach-motel reorderings,i.e., reordering with synchro\u00adnisation. In the absence of roach-motel \nreordering, we could dis\u00adpense withthede-permutations ofpre.xes. 5. Safety ofTransformations Here we \nsketch the main idea of our safety proof. The full details canbefoundinthe author sPhDthesis[21].We establishthatboth \nthe elimination and reordering transformations have the following properties: (i) any execution of the \ntransformed traceset has the samebehaviour as some execution ofthe originaltraceset,provided thatthe \noriginalprogram wasdata racefree;(ii)thetransformations preservedata racefreedom;(iii) thetransformations \ncannotintro\u00adduce values out-of-thin-air. Toprove(i)and(ii), wetake an arbitrary execution ofthe trans\u00adformedprogram \nandconstruct an execution ofthe originalprogram that has the same behaviour. For both the elimination \nand the re\u00adorderingtransformations, wedecomposethe execution ofthetrans\u00adformed program into traces for \neach thread, use the de.nitions of transformed tracesets from the previous section to obtain untrans\u00adformed \ntraces of the original traceset and then we compose the un\u00adtransformed tracesbackinto an untransformedinterleaving \nso that the order of the external and synchronisation actions is preserved. Then we prove that either \nthe constructed interleaving is an exe\u00adcution of the original traceset or there must have been a data \nrace. Moreover, weshowthatthetransformedprogramisdata racefree as the happens-before order of the constructed \nexecution between two actions on the same variable implies happens-before ordering inthe execution of \nthetransformedprogram.We establish(iii) by showingthatthetransformations cannotintroduce origins of values \nin tracesets, where a trace is an origin for v if there is a trace that contains a write of v or an output \nof v withoutanypreceding read of v.The rest of this sectiondescribes the mainproofideasin more detail. \nElimination. We can prove sequential consistency for untrans\u00adformed execution of an eliminated traceset \ndirectly because the untransformation of elimination embeds the happens-before order on each location. \nThe main technical dif.culty in the proof lies in showing that the extra actions introduced by the untransformation \nof eliminationdo notbreak sequential consistency. Here we show thatgiven adata racefreetracesetT,its \nelimina\u00adtion T ' , and an execution of T ' , we can untransform the execution so that the untransformation \nis an execution of T with the same behaviour as the execution of T ' . The .rst step is the de.nition \nof the untransformationbylifting thede.nition of eliminations(De.\u00ad  (a)n =4 (b)n =3 (c)n =2 (d)n =1 \n(e)n =0 Figure4. Example of reordering traces. nition1) tointerleavings.A na\u00a8ivede.nition would require \nthat the eliminatedinterleavingisjust a sublist with some eliminable ac\u00adtions left out. However, this \ndoes not guarantee sequential consis\u00adtency for volatile locations in the untransformed interleaving, \nbe\u00adcause the untransformation mightintroduce a volatile write action. Instead we will allow the untransformation \nto swap some actions while preserving the program order and the order of synchronisa\u00adtion and external \nactions. Moreover, all the release and external actions introduced by the untransformation must be ordered \nafter the release and external actions from the interleaving of the trans\u00adformedprogram. The precise \nde.nition follows. An index i is eliminable in an interleaving I if the corresponding index in the trace \nof T (Ii), i.e., the index |{j | j<i .T (Ii)= T (Ij)}|, is eliminable in the trace of T (Ii)in I. Function \nf is an unelimination function from interleaving I ' to wildcard interleaving I if f is a complete matching \nbetween I ' and I such that(i) if i <j . dom(I ' )and '' ' T (Ii)= T (Ij) then f(i) <f(j), (ii) if i \n<j . dom(I ) and A(Ii' ), A(Ij' ) are synchronisation or external actions then f(i)<f(j),(iii)if i . \nrng(f), j. dom(I)\\ rng(f)and A(Ii), A(Ij)are synchronisation or external actions, then i<j,(iv) if i \n. dom(I)\\ rng(f), then iis eliminablein I. Lemma 1. Let traceset T ' be an elimination of traceset T \nand I ' an interleaving of T ' . Then there is a wildcard interleaving I belonging-to T and an unelimination \nfunction f from I ' to I. We construct the function f and the uneliminated interleaving in three steps: \nwe decompose the interleaving I ' into individual threads, then we obtain uneliminated traces for each \nthread, and .nally we interleave the uneliminated traces of the threads so that we preserve the order \nof synchronisation and external actions from I ' while ordering allintroduced synchronisation and external \nactions after the synchronisation and external actionsfrom I ' . For example, consider theprogram(v is \nvolatile) v:=1; r1:=x; y:=1; r2:=v;print r2; By our de.nition of elimination on tracesets, we can eliminate \nthe lastrelease v:=1 inthe .rstthreadand theirrelevant read r1:=x in the second thread: y:=1; r2:=v;print \nr2; Consider thefollowing execution of theprogram: I ' =[(0,S(0)) ,(1,S(1)) ,(0,W[y=1]) ,(1,R[v=0]) \n,(1,X(0))] Figure5 shows onepossible construction of unelimination I of I ' . The unelimination function \nis a composition of the functions fI' , fe and fI. For example, the unelimination function maps 2 to \n6, i.e., it moves the second action of I ' to the last position in I. Notethatwe cannotjustinsertthe \neliminated actionsbackinto I ' toget the unelimination because we would have toinsert the write W[v=1] \nbetween the start of thread 0 and the write to y and this wouldbreak sequential consistencyfor the read \nof v. Uneliminations have an important property: any unelimination of an executionis also an executionifthe \neliminated execution con\u00adtained at most onedata race.Moreprecisely,let ushave adata race ' '' free traceset \nT, its elimination T , an execution I of T , a wild- Figure5. Unelimination construction. card interleaving \nI belonging-to T and an unelimination function from I ' to I, andlet all strictpre.xes of I ' bedata \nracefree.Then the instance of I is an execution of T. Moreover, uneliminations preservedata races.Theseproperties \nare not obvious. We refer the reader to [21]for afullproof.Consequently, eliminationpreserves data race \nfreedom: suppose that an elimination of a data race free traceset was notdata racefree.Then wetakethe \nshortest execution I of the eliminated traceset with a data race. By the properties of unelimination, \nthe uneliminated interleaving of I is an execution with adata race.This contradictsdata racefreedom of \nthe original traceset.Thus, we conclude: Theorem 1. Let traceset T ' be an elimination of a data free \ntraceset T. Then T ' is data race free and any execution of T ' has thesamebehaviour assomeexecutionof \nT. Reordering. Sincethe reorderinguntransformationdoes notpre\u00adservehappens-before orderingeneral,wecannot \nusethesamedi\u00adrectproof we used for elimination.Instead, weprove the safetyby induction on the size of \nthe interleaving of the transformed pro\u00adgram. Just like with eliminations, we lift the notion of reordering \nto interleavings we de.ne an unordering function describing how to permute the actions in the transformed \ninterleaving to get an interleaving of the original program. We require that whenever restricting an \nunordering on aninterleavingto actions of one thread yields a reorderingfunction on traces, asde.nedin\u00a74. \nGiven traceset T and interleaving I ' , we say that complete matching f : dom(I ' ) . dom(I ' ) is an \nunordering from I ' to T if we have: (i) if i <j . dom(I ' ), T (Ii' )= T (Ij' ) and A(Ij' ),A(Ii' ) \nare not reorderable, then f(i) <f(j), (ii) if ' '' i<j . dom(I )and A(Ii),A(Ij)are synchronisation or \nexternal actions, then f(i) <f(j),(iii)for each thread ., the permutation f restricted to actions of \n. de-permutes the trace of . in I ' into T. Using a similar construction to unelimination, unordering \nal\u00adways exists. Unlike in the elimination safety proof, we establish the safetybyinduction onthe size \nofthe execution ofthe reordered traceset. To do that, we observe that restricting a reordering func\u00adtionforanexecutiontothepre.xof \ntheexecution withoutthelast elementyields a valid reordering function. This allows us toprove by induction \non the size of I ' that for any unordering function f from an execution I ' to a data race free traceset \nT, the interleav\u00ading I = f.(I ' )is an execution of T.The technical details of this proofcanbefoundin \n[21].TheDRFguaranteedirectlyfollows.  Theorem2. Suppose thattracesetT ' is a reorderingofadata race \nfree traceset T. Then any execution of T ' has the same behaviour as some execution of T.Moreover, T \n' isdata race free. The data race freedom of T ' follows from reordering function being order-re.ecting \nfor happens-before order restricted to any individual memorylocation. Out-of-thin-air. So far we have \nseen that the transformations provide anintuitive semanticsforprograms withoutdata races.But what happens \nif there are data races in a program? Is anything possible? This would be unacceptable for languages \nthat aim to give security guarantees for arbitrary programs, such as Java with sand-boxing.Foranillustration \nofundesirablebehaviours, consider theprogram Initially,x = y =0. r2:=y; r1:=x; x:=r2; y:=r1; print \nr2; Sincetheprogramdoes notcontain value 42nor anyarithmeticthat could createit, no transformation oftheprogram \nshould output 42. Although our transformations do not give sequential consis\u00adtencyforprograms withdata \nraces,wecan showthatout-of-thin-air behaviours, such as the one above, are impossible. More speci.\u00adcally, \nwe will establish that for each output action of some value from an execution of a transformed program \nthere is a statement in the originalprogram that musthave created that value. In alan\u00adguage without arithmetic, \nsuch as the one introduced in \u00a76, this might mean that if a transformed program outputs value v, then \nv must be a default value, or the original program must have con\u00adtained v in its program text as a constant. \nIn a language with dy\u00adnamic object allocation, we might use similar technique to show that if a program \ncannot allocate objects of a certain class in any thread, then in no transformed program the reference \nto such an object appears out-of-thin-air. The guarantee is based on a simple observation: Let v be a \nvalue thatisdifferentfrom thedefault values.Ifaprogram without arithmeticdoes not contain v as a constant \nin the source code then in each trace, each write of the value v and each external action with the value \nv mustbeprecededby a read of the value v. Formally, we say that trace t is an origin for value v if there \nis i . dom(t)such that ti is a write of v or an external action with the value v, and thereis no j<isuch \nthat tj is a read of the value v.Later,in \u00a76.1, we show an application of this semanticproperty for a \nsimplelanguage. Lemma 2. Let traceset T ' be a reordering or an elimination of traceset T and suppose \nthat no tracein T is an originfor v.Let us assume that nolocationhas a singleton type with value v.Then \nno tracein T ' is an origin for v. Finally, observe thatifT does not contain an originfor a value, no \nexecution of T can output that value: Lemma 3. Suppose that v is a value, that is not a default value \nfor any type, T is a traceset, and no t in T is an origin for v.Then there is no execution of T that \ncontains a read, write or external action with the value v. 6. ConnectingSyntaxandSemantics Sofar wehave \nreferredtoanintuitive understandingofthe relation\u00adshipbetweenprograms andtraces.Toillustratethe transformations \non a concrete syntax, we de.ne a simple concurrent language and several simple but illustrative syntactic \nprogram transformations. Then we show that the syntactic transformations correspond to the semantic transformations \nandthus satisfytheDRFandout-of-thin\u00adair guarantees. It is easy to add more language features, such as \npointers, rich expression language and functions, without funda\u00admental changes to the proofs because \nthese features do not have anymemory side-effects. The syntax ofourlanguageisgiveninFig.6.Thegrammar \nuses distinctidenti.ersforthread-localregister names, rangedoverbyr or r1,r2 in examples, natural numbers \nranged over by i, location names(also called variables)ranged overbyl,in examples x,y,z, monitor names \nranged overbym,in examples m1,m2. We use a labellised small-step semantics to de.ne the meaning of programs \nin the language introduced. A thread-local con.gura\u00adtion is a triple (.,s,C), where monitor state . is \na function that maps monitor namestothe nestingleveloflocks,localstate smaps register namesto values,i.e.,naturalnumbers, \nandC is a codefrag\u00adment, which is either S or L or P from the syntax in Fig. 6. The onlypurpose ofthe \nmonitor stateis toprevent threadsfromissuing more unlocks thanlocks on each monitor. a The small-steprelation \n(.,s,C)-. (. ' ,s ' ,C ' ) takes a code fragment C in state ., s to a code fragment C ' and states . \n' , s ' while issuing shared-memory action a. The action a may be empty, denoted by t.Fig.7 contains \naninductivede.nitionof the small step relation.WeusethetermVal(s,E)for the value of the expression E \ninthe environment s,i.e.,Val(s,i)= ifor anyvalue i . N,Val(s,r)= s(r)for register name r,Val(s,r1==r2)(resp. \nVal(s,r1!=r2))istt ifVal(s,r1)= Val(s,r2)(resp.Val(s,r1)= Val(s,r2)) or ff otherwise. We write f[a . \nb] for a function update,i.e., f[a . b](a)= band f[a . b](x)= f(x)for x = a. t We will write (.,s,C) \n=.(. ' ,s ' ,C ' ) for a sequence n t of n transitions, as de.ned in Fig. 8; notation (.,s,C) =. (. ' \n,s ' ,C ' ) stands for a .nite number of transition, i.e., it is a t shorthand for .n. (.,s,C) =.(. ' \n,s ' ,C ' ). A con.guration n (.,s,C) may issue trace t, written (.,s,C). t, if there are . ' , s '' \n' t and C such that (.,s,C) =.(. ' ,s ' ,C ).The meaning of a code fragment C in thread-local state ., \ns is the set of all traces that it may issue, i.e., [ C] .,s = {t |(.,s,C). t} The meaning ofprogram \nP, written [ P] , is the set of traces [ P] .0 ,s0 where .0 maps allmonitor namesto 0andsmaps alllocationsto \n0.Observe that [ P] is a traceset. 6.1 Transformations:FromSyntaxtoSemantics Our basic template for \nlocal transformation t is given in Fig. 9. Observethatthe rulesfortcannotperform anytransformationsyet, \nt i.e.,for any P, wehave P P (byinduction on the structure of t P)and if P P ' then P = P ' (byinduction \non the derivation t of P P ' ).To allow someinterestingtransformations, we need to add some additionalbase \nrules. Elimination. In Fig. 10, we give the additional base rules for e our elimination transformation.Technically, \nthe relationisde\u00ad.ned inductively using the rules from Fig. 10 in addition to the te rulesfromFig.9 with \nreplacedby .The elimination trans\u00adformation removes redundant(shared-memory) reads and writes. The term \nfv(S)stands for all shared-memory locations contained in S. Statement S is sync-free if it does not contain \nany lock or unlock statements or accesses to volatile locations. Rule E-RAR (resp. E-RAW)removes redundant \nreadifthe value of thelocation isknownfrom aprevious read (resp. write). Rule E-WARremoves a write that \nfollows a read of the same value in the same location. Overwritten writes canbe eliminatedbyrule E-WBW.Rule \nE-IR removesirrelevantreads whichcannotaffecttherestoftheprogram because their valueis thrown away. \n ri ::= r | i T ::= ri == ri | ri != ri S ::= l := r; | r := l; | r := ri; | lock m; | unlock m; | skip; \n| print r; | {L} | if (T) S else S | while (T) S L ::= S | SL P ::= L|| L|| ... || L Figure6. Asimple \nconcurrent language syntax. t (.,s,r:=ri;) -.(.,s[r . Val(s,ri)],skip;) (REGS) W[x=s(r)] (.,s,x:=r;) \n- -----. (.,s,skip;) (WRITE) R[x=v] (.,s,r:=x;) ----. (.,s[r . v],skip;) where v . t(x)(READ) L[m] \n(.,s,lock m;) - -. (.[m . .(m)+1],s,skip;) (LOCK) U[m] (.,s,unlock m;) ---. (.[m . .(m)- 1],s,skip;) \nwhere .(m)> 0(ULK) t (.,s,unlock m;) -.(.,s,skip;) where .(m)=0 (E-ULK) X(s(r)) (.,s,print r;) - ---. \n(.,s,skip;) (EXT) t (.,s,if (T) S1 else S2) -.(.,s,S1) ifVal(s,T)= tt (COND-T) t (.,s,if (T) S1 else \nS2) -.(.,s,S2) ifVal(s,T)= ff (COND-F) t (.,s,while (T) S) -.(.,s,S;while (T) S) ifVal(s,T)= tt (LOOP-T) \nt (.,s,while (T) S) -.(.,s,skip;) ifVal(s,T)= ff (LOOP-F) t (.,s,skip;L) -.(.,s,L) (SEQ) t (.,s,{skip;}) \n-.(.,s,skip;) (BLOCK) S(i) (.,s,L0 || ... || Ln) --. (.,s,Li) where 0 = i = n (PAR) aa (. ' . . ' (.,s,S) \n-. ,s ' ,S ' ) .,s,L -,s ' ,L ' (EV-SEQ)(EV-BLOCK) (.,s,S L)-a (.' ,s',S' L)(.,s,{L})-a (.' ,s' ,{L' \n}) .. Figure7. Small-stepTraceSemantics. t (. '' ,s '' '' (. '' ,s '' '' a (. '' (.,s,C) -. ,C ) ,C ) \n=. ,s ' ,C ) (TR-ID) an (TR-SEQT) [] ' ) (.,s,C) =.(.' ,s',C (.,s,C) =.(.,s,C) n+1 0 aa (. '' '' (. \n'' '' ' (.,s,C) -. ,s '' ,C ) a = t ,s '' ,C ) =.(. ' ,s ' ,C ) a::an (TR-SEQA) (.,s,C) =.(.' ,s',C') \nn+1 Figure8. Multi-stepTraceSemantics. t tt L ' S ' L ' LS11 L22 (T-ID) (T-BLOCK) (T-SEQ) tt t SS {L}{L' \n} S1 L2 S1 ' L2 ' t S ' t S ' S11 S22 (T-IF) if (T) then S1 else S2 t if (T) then S1 ' else S2 ' t S \n' t S ' S .i .{0,...,n}.Sii (T-WHILE) (T-PAR) while (T) S t while (T) S' S0 || ... || Snt S0 ' || ... \n|| Sn ' Figure9. Transformation template. x not volatile r1,r2,x/. fv(S) S sync-free x not volatile \nr1,r2,x/. fv(S) S sync-free (E-RAR) (E-RAW) r1:=x; S; r2:=x e r1:=x; S; r2:=r1 x:=r1; S; r2:=x e x:=r1; \nS; r2:=r1 x not volatile r,x/. fv(S) S sync-free x not volatile r1,r2,x/. fv(S) S sync-free (E-WAR) (E-WBW) \nr:=x; S; x:=r e r:=x; S; x:=r1; S; x:=r2 e S; x:=r2 x not volatile (E-IR) r:=x; r:=i e r:=i Figure10. \nAdditional rulesfor syntactic elimination. Our de.nition of syntactic eliminations (Figure 10) does not \nincludelastaction eliminationsbecause theyare not composablein the sense that trace t1 ' being a(last \naction)elimination of t1 and t2 ' beingan elimination oft2 does notnecessarilyimplythat t1 ' + t2 ' is \nan elimination oft1 + t2.To recover compositionality, we willcall index i properly eliminable in a wildcard \ntrace t if i is a redundant readafter read, or a redundant readafter write, or anirrelevant read, or \na redundant write after reador an overwritten write.Giventraces t and t ' , the trace t ' is a proper \nelimination of t if there is S such that t ' = t|S and all i . dom(t)\\ S areproperly eliminablein t. \nWedenote theproper eliminationbyt .e t ' . Thefollowinglemma clari.esthe relationshipbetween the syn\u00adtactic \nelimination and the semantic eliminationfrom \u00a74. Lemma4. Let C be a code fragment and C e C ' .Thenfor \nany monitor states .,. ' , register states s,s ' and trace t ' wehave: If (.,s,C ' ). t ' then there \nis a wildcard trace t such that t .e t ' and for any instance t of t wehave (.,s,C). t . ' If (.,s,C \n' ) =t .(. ' ,s ' ,skip;) then there is a wildcard trace t such that t .e t ' and for any instance t \nof t we have t (.,s,C) =.(. ' ,s ' ,skip;). As a consequence, if P e P ' for some programs P and P ' \n, then [ P ' ] is an elimination of [ P] . Combining this observation with theresultsforthesemantic elimination(Theorem1)gives \nus a compositionalDRFguaranteefor syntactic elimination: Theorem 3. Suppose that P e P ' and [ P] is \ndata race free. Then [ P ' ] is data race free, and any execution of [ P ' ] has the samebehaviour assomeexecutionof \n[ P] . Reordering. Fig.11 shows additional rulesfor reordering.Sim\u00adilarlyto the elimination transformations, \nthe relation r isde.ned inductively using the rules from Fig. 11 and relabelled rules from Fig.9.Wecapturereordering \nofindependent non-volatilememory accessesin rulesR-RW,R-RR,R-WRandR-WW.RulesR-WL, R-RL,R-UW andR-UR allow \nmoving non-volatile memory ac\u00adcessesinside synchronisedblocks.NotethattheR-RR,R-WRand R-WW rules allowlimited \nreordering of accesses to volatileloca\u00adtions with non-volatile accesses. The relationshipbetween the \nsyntactic and semantic reordering is more involved in our framework because syntactic reordering corresponds \nto semantic eliminationfollowedbysemantic reorder\u00ading;for anillustration, seethe examplein\u00a74.Thefollowinglemma \nformalisesthe relationshipbetweenthe syntactic reorderingandthe trace semantics. Lemma 5. Assume that \nC r C ' . Then for each . and s there is apre.x closed set of traces T satisfying these conditions:(i)the \nset of traces [ C] .,s is a subset of T,(ii)each trace from T is an elimination of some wildcard trace \nthatbelongs-to [ C] .,s,(iii)for ' '' each trace t , if (.,s,C ). t holds then there is a function that \nde-permutes t ' into T,(iv)for each trace t ' , if there are . ' and s ' ' such that (.,s,C ' ) =t .(. \n' ,s ' ,skip;) then thereis afunction f f.(t ' ) thatde-permutes t ' into T and (.,s,C) =.(. ' ,s ' ,skip;). \nTogether withthe semanticDRFguaranteesfrom \u00a75, we obtain the compositionalDRFguaranteefor syntactic reordering. \nTheorem 4. Suppose that P r P ' and [ P] is data race free. Then [ P ' ] is data race free, and any execution \nof [ P ' ] has the samebehaviour assomeexecutionof [ P] . Out-of-thin-air. To establish the out-of-thin-air-guarantee \nfor our simple language, we .rst observe that if a program does not contain constant c then theprogramis \nnot an originfor c: Lemma 6. Let v be a value such that v is not a default value for anylocation,i.e., \nv =0.Let P be aprogram withoutanystatement of the form r:=v, where r is a register name. Then no trace \nin the traceset of P is an originfor the value v. This observation allows us to use our semantic observations \nfrom \u00a75 and state the syntactic counterpart of the out-of-thin-air guarantee. Theorem 5. Suppose that \nc is a constant different from 0, and P a program that does not contain a statement of the form r := \nc, where r is a register. Let P ' be a program obtained from P by any composition of syntactic reorderings \nor eliminations. Then P ' cannot output c. 7. RelatedWork Theexistingresearchon compiler optimisationsforshared-memory \nconcurrency concentrates mainly on maintaining sequential con\u00adsistency for all programs, starting with \nthe foundational work of Shasha andSnir [24], wheretheytake a sequentiallyconsistent ex\u00adecution of a \nstraight line program and describe a set of reordering constraints that preserve sequential consistency. \nBuilding on these foundations,papers [15,17,26]describe wholeprogram analyses that determine allowable \nreorderings in multi-threaded programs and compilersthatpreservebehaviours of arbitraryprograms.The emphasis \nof that line of work is different from ours: while they design a restrictedcompilerthatguarantees(anillusion \nof)sequen\u00adtial consistency for all programs, we show that commonly used program transformations maintain \nanillusion of sequential consis\u00adtency for correctly synchronised programs. We are not aware of anysuch \nworkin the context of compilers. Correctness of optimisationsis closelyrelatedto weak memory modelsforprogramming \nlanguages andhardware.Infact,validity of common optimisations was the main motivation for designing the \nJava Memory Model [16]. Despite this, Java does not allow several common optimisations[9,23], and eventhe \nreferenceim\u00adplementation of Java Virtual Machine does not conform with the speci.cation[22].The situationdoes \nnot seemtobe muchbetter  r1 = r2 x not volatile x = yy not volatile (R-RR) (R-WW) r1:=x; r2:=y; r r2:=y; \nr1:=x; x:=r1; y:=r2; r y:=r2; x:=r1; r1 = r2 x = yx or y not volatile r1 = r2 x = y x,y not volatile \n(R-WR) (R-RW) x:=r1; r2:=y; r r2:=y; x:=r1; r1:=x; y:=r2; r y:=r2; r1:=x;; x not volatile x not volatile \n(R-WL) (R-RL) x:=r; lock m; r lock m; x:=r; r:=x; lock m; r lock m; r:=x; x not volatile x not volatile \n(R-UW) (R-UR) unlock m;x:=r; r x:=r;unlock m; unlock m;r:=x; r r:=x;unlock m; r1 = r2 x not volatile \nx not volatile (R-XR) (R-XW) rr print r1;r2:=x; . r2:=x;print r1; print r1;x:=r2; . x:=r2;print r1; Figure11. \nAdditional rulesfor syntactic reordering. inMicrosoftCommonLanguageInfrastructure(CLI):the of.cial speci.cation \nis in informal prose without any clear guarantees for programmers.Inpractice,theCLIvirtual machine seemstobe \nvery conservative, and it appears that it conforms to a much stronger unof.cial model [18]. There have \nbeen several other suggestions for weak memory models for high-level languages, but none of these addressed \nvalidity of compiler transformations[9,19].Hard\u00adware memory models or hardware-inspired memory models, \nsuch as [2, 7, 8], do not seem to be suitable for higher languages be\u00adcause they are either too prohibitive \nand do not allow reordering of reads with later independent writes/reads at all, or they rely on syntactic \nnotions of dependency for reordering, which can be of\u00adtenremovedby optimising compilers.Wehopethatourwork \nwill serve as abasisfor aformal weak memory modelde.ned in terms ofpermissible transformations. 8. Conclusion \nWe have proved that two large classes of compiler optimisations, elimination and reordering, are safe \nin the DRF guarantee, i.e., they cannot introduce new behaviours fordata racefreeprograms. Since our \ntransformations preserve data race freedom, arbitrary composition of the transformationsis also safe. \n As reasoning about multi-threading is notoriously error-prone, we mechanised the de.nitions and dif.cult \nparts of the semantic elimination safetyproof(such asLemma1) in theIsabelle/HOL proof assistant. The \nmechanisation has about ten thousand lines of proof script. The current development snapshot is available \nat http://www.cl.cam.ac.uk/~ js861/transafety/mm-traces. In our work, we assume that the transformed \nprogram always runs sequentially consistently. This might seem tobe a severe lim\u00aditation because few \nmodern processors guarantee sequential con\u00adsistency. However, it is well-understood how to ensure the \nDRF guarantee on hardware and all our transformations preserve data race freedom, so we can safely assume \nsequential consistency in hardware. Unfortunately, we cannot easily recover the out-of-thin\u00adairguaranteewithoutdeeperunderstanding \nof theprocessormem\u00adory models. Therefore, we started investigating extensions of our techniquestocoverhardwarememory \nmodels.Our .rst resultsare encouraging we canexplaintheSunTSO memory model[25], used by most SPARC processors \nwith our semantic transforma\u00adtions.Webelievethatthere similar results canbe achievedfor other processor \nmemory models. Acknowledgments I would like to thank David Aspinall for his guidance and Pe\u00adter Sewell \nfor many useful comments on earlier drafts of the paper. I acknowledge funding from EPSRC grants EP/F036345, \nEP/H005633. References [1] S. V. Adve and M. D. Hill. Weak ordering a new de.nition. In ISCA 90,pages2 \n14,1990. [2] Sarita V. Adve and Kourosh Gharachorloo. Shared memory consis\u00adtency models:A tutorial. Computer,29(12):66 \n76, 1996. [3] J.Alglave,A.Fox,S.Ishtiaq,M.O.Myreen,S.Sarkar,P.Sewell,and F.ZappaNardelli. Thesemantics \nofPower andARM multiprocessor machine code. In Proc.DAMP2009,January2009. [4] M.Batty,S.Owens,S.Sarkar,P.Sewell,andT.Weber.Mathematizing \nC++ concurrency. In Proc.POPL,2011. [5] P.Becker, editor. Programming Languages C++.FinalCommittee Draft. \n2010. ISO/IECJTC1SC22WG21N3092. [6] Hans-J. Boehm and Sarita V. Adve. Foundations of the C++ concur\u00adrency \nmemory model. In PLDI 08, pages 68 78, New York, NY, USA,2008.ACM. [7] G\u00b4erardBoudol andGustavoPetri. \nRelaxed memory models: anoper\u00adational approach. In POPL 09,pages392 403,2009. [8] S.Burckhardt,M.Musuvathi, \nandV.Singh. Verifying local transfor\u00admations on relaxed memory models. In CC 10,pages104 123,2010. [9] \nP. Cenciarelli, A. Knapp, and E. Sibilio. The Java memory model: Operationally, denotationally, axiomatically. \nIn16th ESOP,2007. [10] J. Gosling, B. Joy, G. Steele, and G. Bracha. Java(TM) Language Speci.cation, \nThe(3rd Edition)(Java Series). Addison-Wesley Pro\u00adfessional, July2005. [11] Intel.Aformalspeci.cationofIntelItaniumprocessorfamily \nmemory ordering, 2002. Available from http://www.intel.com/design/ itanium/downloads/251429.htm. [12] \nP.Joisha,R.Schreiber,P.Banerjee,H.-J.Boehm,andD.Chakrabarti. Atechniqueforthe effective andautomatic \nreuse of classical compiler optimizations on multithreaded code. TechnicalReportHPL-2010-81, HPLaboratories, \n2010. To appear inPOPL2011. [13] Leslie Lamport. Time, clocks, and the ordering of events in a dis\u00adtributed \nsystem. Commun.ACM,21(7):558 565, 1978. [14] DougLea.TheJSR-133 cookbookforcompilerwriters,2008. http: \n//g.oswego.edu/dl/jmm/cookbook.html. [15] Jaejin Lee, David A. Padua, and Samuel P. Midkiff. Basic compiler \nalgorithmsforparallelprograms.In PPOPP,pages1 12.ACM,1999.  [16] J.Manson,W.Pugh,andS.Adve.TheJavamemorymodel.In \nPOPL 05,pages378 391,NewYork,NY,USA,2005.ACMPress. [17] SamuelP.Midkiff andDavidA.Padua. Issuesinthe \noptimization of parallel programs. In ICPP(2), pages 105 113. Pennsylvania State University Press,1990. \n[18] Vance Morrison. Understand the impact of low-lock techniques in multithreaded apps. MSDNMagazine,Oct2005. \n[19] V.Saraswat,R.Jagadeesan,M.Michael, andC.vonPraun. Atheory of memory models. In PPoPP 07.ACM,Mar2007. \n[20] S. Sarkar, P. Sewell, F. Zappa Nardelli, S. Owens, T. Ridge, T. Braibant, M. Myreen, and J. Alglave. \nThe semantics of x86-CC multiprocessor machine code. In POPL 09,January2009. [21] J. .c\u00b4PhD Sev.ik. Program \nTransformations in Weak Memory Models. thesis, University of Edinburgh, Laboratory for Foundations of \nCom\u00adputerScience, 2008. [22] J. .c\u00b4The Sun Hotspot JVM does not conform with the Java Sev.ik. memory \nmodel. Technical Report EDI-INF-RR-1252, School of In\u00adformatics,University ofEdinburgh,2008. [23] J. \n.c\u00b4 Sev.ik andD.Aspinall. On validity ofprogramtransformationsin theJava memory model. In ECOOP 08,pages27 \n51,2008. [24] Dennis Shasha and Marc Snir. Ef.cient and correct execution of parallel programs that share \nmemory. ACM Trans. Program. Lang. Syst.,10(2):282 312, 1988. [25] SparcInternational.Sparcarchitecturemanual,version9,2000.Avail\u00adable \nfrom http://developers.sun.com/solaris/articles/ sparcv9.html. [26] Z. Sura, X. Fang, C. Wong, S. Midkiff, \nJ. Lee, and D. Padua. Com\u00adpilertechniquesforhighperformance sequentiallyconsistentJavapro\u00adgrams. InPPoPP \n05,pages2 13,NewYork,NY,USA,2005.ACM.   \n\t\t\t", "proc_id": "1993498", "abstract": "<p>Current proposals for concurrent shared-memory languages, including C++ and C, provide sequential consistency only for programs without data races (the DRF guarantee). While the implications of such a contract for hardware optimisations are relatively well-understood, the correctness of compiler optimisations under the DRF guarantee is less clear, and experience with Java shows that this area is error-prone.</p> <p>In this paper we give a rigorous study of optimisations that involve both reordering and elimination of memory reads and writes, covering many practically important optimisations. We first define powerful classes of transformations semantically, in a language-independent trace semantics. We prove that any composition of these transformations is sound with respect to the DRF guarantee, and moreover that they provide basic security guarantees (no thin-air reads) even for programs with data races. To give a concrete example, we apply our semantic results to a simple imperative language and prove that several syntactic transformations are safe for that language. We also discuss some surprising limitations of the DRF guarantee.</p>", "authors": [{"name": "Jaroslav &#352;ev&#269;&#237;k", "author_profile_id": "81384598170", "affiliation": "University of Cambridge, The MathWorks, Cambridge, United Kingdom", "person_id": "P2690569", "email_address": "jarin.sevcik@gmail.com", "orcid_id": ""}], "doi_number": "10.1145/1993498.1993534", "year": "2011", "article_id": "1993534", "conference": "PLDI", "title": "Safe optimisations for shared-memory concurrent programs", "url": "http://dl.acm.org/citation.cfm?id=1993534"}