{"article_publication_date": "06-04-2011", "fulltext": "\n Finding and Understanding Bugs in C Compilers Xuejun Yang Yang Chen Eric Eide John Regehr University \nof Utah, School of Computing { jxyang, chenyang, eeide, regehr }@cs.utah.edu Abstract Compilers should \nbe correct. To improve the quality of C compilers, we created Csmith, a randomized test-case generation \ntool, and spent three years using it to .nd compiler bugs. During this period we reported more than 325 \npreviously unknown bugs to compiler developers. Every compiler we tested was found to crash and also \nto silently generate wrong code when presented with valid input. In this paper we present our compiler-testing \ntool and the results of our bug-hunting study. Our .rst contribution is to advance the state of the art \nin compiler testing. Unlike previous tools, Csmith generates programs that cover a large subset of C \nwhile avoiding the unde.ned and unspeci.ed behaviors that would destroy its ability to automatically \n.nd wrong-code bugs. Our second contribution is a collection of qualitative and quantitative results \nabout the bugs we have found in open-source C compilers. Categories and Subject Descriptors D.2.5 [Software \nEngineer\u00ading]: Testing and Debugging testing tools; D.3.2 [Programming Languages]: Language Classi.cations \nC; D.3.4 [Programming Languages]: Processors compilers General Terms Languages, Reliability Keywords \ncompiler testing, compiler defect, automated testing, random testing, random program generation 1. Introduction \nThe theory of compilation is well developed, and there are compiler frameworks in which many optimizations \nhave been proved correct. Nevertheless, the practical art of compiler construction involves a morass \nof trade-offs between compilation speed, code quality, code debuggability, compiler modularity, compiler \nretargetability, and other goals. It should be no surprise that optimizing compilers like all complex \nsoftware systems contain bugs. Miscompilations often happen because optimization safety checks are inadequate, \nstatic analyses are unsound, or transfor\u00admations are .awed. These bugs are out of reach for current and \nfuture automated program-veri.cation tools because the speci.ca\u00adtions that need to be checked were never \nwritten down in a precise way, if they were written down at all. Where veri.cation is imprac\u00adtical, however, \nother methods for improving compiler quality can succeed. This paper reports our experience in using \ntesting to make C compilers better. Permission to make digital or hard copies of all or part of this \nwork for personal or classroom use is granted without fee provided that copies are not made or distributed \nfor pro.t or commercial advantage and that copies bear this notice and the full citation on the .rst \npage. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior \nspeci.c permission and/or a fee. PLDI 11, June 4 8, 2011, San Jose, California, USA. Copyright c &#38;#169; \n2011 ACM 978-1-4503-0663-8/11/06. . . $10.00 1 int foo (void) { 2 signed char x = 1; 3 unsigned char \ny = 255; 4 return x > y; 5 } Figure 1. We found a bug in the version of GCC that shipped with Ubuntu \nLinux 8.04.1 for x86. At all optimization levels it compiles this function to return 1; the correct result \nis 0. The Ubuntu compiler was heavily patched; the base version of GCC did not have this bug. We created \nCsmith, a randomized test-case generator that sup\u00adports compiler bug-hunting using differential testing. \nCsmith gen\u00aderates a C program; a test harness then compiles the program us\u00ading several compilers, runs \nthe executables, and compares the out\u00adputs. Although this compiler-testing approach has been used be\u00adfore \n[6, 16, 23], Csmith s test-generation techniques substantially advance the state of the art by generating \nrandom programs that are expressive containing complex code using many C language features while also \nensuring that every generated program has a single interpretation. To have a unique interpretation, a \nprogram must not execute any of the 191 kinds of unde.ned behavior, nor depend on any of the 52 kinds \nof unspeci.ed behavior, that are described in the C99 standard. For the past three years, we have used \nCsmith to discover bugs in C compilers. Our results are perhaps surprising in their extent: to date, \nwe have found and reported more than 325 bugs in mainstream C compilers including GCC, LLVM, and commercial \ntools. Figure 1 shows a representative example. Every compiler that we have tested, including several \nthat are routinely used to compile safety-critical embedded systems, has been crashed and also shown \nto silently miscompile valid inputs. As measured by the responses to our bug reports, the defects discovered \nby Csmith are important. Most of the bugs we have reported against GCC and LLVM have been .xed. Twenty-.ve \nof our reported GCC bugs have been classi.ed as P1, the maximum, release-blocking priority for GCC defects. \nOur results suggest that .xed test suites the main way that compilers are tested are an inadequate mechanism \nfor quality control. We claim that Csmith is an effective bug-.nding tool in part because it generates \ntests that explore atypical combinations of C language features. Atypical code is not unimportant code, \nhow\u00adever; it is simply underrepresented in .xed compiler test suites. Developers who stray outside the \nwell-tested paths that represent a compiler s comfort zone for example by writing kernel code or embedded \nsystems code, using esoteric compiler options, or au\u00adtomatically generating code can encounter bugs quite \nfrequently. This is a signi.cant problem for complex systems. Wolfe [30], talk\u00ad ing about independent \nsoftware vendors (ISVs) says: An ISV with a complex code can work around correctness, turn off the optimizer \nin one or two .les, and usually they have to do that for any of the compilers they use (emphasis ours). \nAs another example, the front page of the Web site for GMP, the GNU Multiple Precision Arith\u00admetic Library, \nstates, Most problems with compiling GMP these days are due to problems not in GMP, but with the compiler. \n Improving the correctness of C compilers is a worthy goal: C code is part of the trusted computing \nbase for almost every modern computer system including mission-critical .nancial servers and life\u00adcritical \npacemaker .rmware. Large-scale source-code veri.cation efforts such as the seL4 OS kernel [12] and Airbus \ns veri.cation of .y-by-wire software [24] can be undermined by an incorrect C compiler. The need for \ncorrect compilers is ampli.ed because operating systems are almost always written in C and because C \nis used as a portable assembly language. It is targeted by code generators from a wide variety of high-level \nlanguages including Matlab/Simulink, which is used to generate code for industrial control systems. Despite \nrecent advances in compiler veri.cation, testing is still needed. First, a veri.ed compiler is only as \ngood as its speci.cation of the source and target language semantics, and these speci.cations are themselves \ncomplex and error-prone. Second, formal veri.cation seldom provides end-to-end guarantees: details such \nas parsers, libraries, and .le I/O usually remain in the trusted computing base. This second point is \nillustrated by our experience in testing CompCert [14], a veri.ed C compiler. Using Csmith, we found \npreviously unknown bugs in unproved parts of CompCert bugs that cause this compiler to silently produce \nincorrect code. Our goal was to discover serious, previously unknown bugs: in mainstream C compilers \nlike GCC and LLVM;  that manifest when compiling core language constructs such as arithmetic, arrays, \nloops, and function calls;  targeting ubiquitous architectures such as x86 and x86-64; and  using mundane \noptimization .ags such as O and O2.  This paper reports our experience in achieving this goal. Our .rst \ncontribution is to advance the state of the art in compiler test-case generation, .nding as far as we \nknow many more previously unknown compiler bugs than any similar effort has found. Our second contribution \nis to qualitatively and quantitatively characterize the bugs found by Csmith: What do they look like? \nIn what parts of the compilers are they primarily found? How are they distributed across a range of compiler \nversions? 2. Csmith Csmith began as a fork of Randprog [27], an existing random C program generator \nabout 1,600 lines long. In earlier work, we extended and adapted Randprog to .nd bugs in C compilers \ntranslation of accesses to volatile-quali.ed objects [6], resulting in a 7,000-line program. Our previous \npaper showed that in many cases, these bugs could be worked around by turning volatile-object accesses \ninto calls to helper functions. The key observation was this: while the rules regarding the addition, \nelimination, and reordering of accesses to volatile objects are not at all like the rules governing ordinary \nvariable accesses in C, they are almost identical to the rules governing function calls. For some test \nprograms generated by Randprog, our rewriting procedure was insuf.cient to correct a defect that we had \nfound in the C compiler. Our hypothesis was that this was always due to reg\u00adular compiler bugs not related \nto the volatile quali.er. To investigate these compiler defects, we shifted our research emphasis toward \nlooking for generic wrong-code bugs. We turned Randprog into Csmith, a 40,000-line C++ program for randomly \ngenerating C pro\u00adgrams. Compared to Randprog, Csmith can generate C programs that utilize a much wider \nrange of C features including complex control .ow and data structures such as pointers, arrays, and structs. \nMost of Csmith s complexity arises from the requirement that it Figure 2. Finding bugs in three compilers \nusing randomized differ\u00adential testing interleave static analysis with code generation in order to produce \nmeaningful test cases, as described below. 2.1 Randomized Differential Testing using Csmith Random testing \n[9], also called fuzzing [17], is a black-box testing method in which test inputs are generated randomly. \nRandomized differential testing [16] has the advantage that no oracle for test results is needed. It \nexploits the idea that if one has multiple, deter\u00administic implementations of the same speci.cation, \nall implementa\u00adtions must produce the same result from the same valid input. When two implementations \nproduce different outputs, one of them must be faulty. Given three or more implementations, a tester \ncan use voting to heuristically determine which implementations are wrong. Figure 2 shows how we use \nthese ideas to .nd compiler bugs.  2.2 Design Goals Csmith has two main design goals. First and most \nimportant, every generated program must be well formed and have a single meaning according to the C standard. \nThe meaning of a C program is the sequence of side effects it performs. The principal side effect of \na Csmith-generated program is to print a value summarizing the com\u00adputation performed by the program.1 \nThis value is a checksum of the program s non-pointer global variables at the end of the program s execution. \nThus, if changing the compiler or compiler options causes the checksum emitted by a Csmith-generated \nprogram to change, a compiler bug has been found. The C99 language [11] has 191 unde.ned behaviors e.g., \ndereferencing a null pointer or over.owing a signed integer that destroy the meaning of a program. It \nalso has 52 unspeci.ed behaviors e.g., the order of evaluation of arguments to a function where a compiler \nmay choose from a set of options with no requirement that the choice be made consistently. Programs emitted \nby Csmith must avoid all of these behaviors or, in certain cases such as argument-evaluation order, be \nindependent of the choices that will be made by the compiler. Many unde.ned and unspeci.ed behaviors \ncan be avoided structurally by generating programs in such a way that problems never arise. However, \na number of important unde.ned and unspeci.ed behaviors are not easy to avoid in a structural fashion. \nIn these cases, Csmith solves the problem using static analysis and by adding run-time checks to the \ngenerated code. Section 2.4 describes the hazards that Csmith must avoid and its strategies for avoiding \nthem. Csmith s second design goal is to maximize expressiveness subject to constraints imposed by the \n.rst goal. An expressive generator supports many language features and combinations of features. Our \nhypothesis is that expressiveness is correlated with bug-.nding power. 1 Accesses to volatile objects \nare also side effects as described in the C standard. We do not discuss these secondary side effects \nof Csmith\u00adgenerated programs further in this paper.  Csmith creates programs with the following features: \n function de.nitions, and global and local variable de.nitions  most kinds of C expressions and statements \n control .ow: if/else, function calls, for loops, return, break, continue, goto  signed and unsigned \nintegers of all standard widths  arithmetic, logical, and bitwise operations on integers  structs: \nnested, and with bit-.elds  arrays of and pointers to all supported types, including pointers and arrays \n the const and volatile type quali.ers, including at different levels of indirection for pointer-typed \nvariables  The most important language features not currently supported by Csmith are strings, dynamic \nmemory allocation, .oating-point types, unions, recursion, and function pointers. We plan to add some \nof these features to future versions of our tool.  2.3 Randomly Generating Programs The shape of a program \ngenerated by Csmith is governed by a grammar for a subset of C. A program is a collection of type, variable, \nand function de.nitions; a function body is a block; a block contains a list of declarations and a list \nof statements; and a statement is an expression, control-.ow construct (e.g., if, return, goto, or for), \nassignment, or block. Assignments are modeled as statements not expressions which re.ects the most common \nidiom for assignments in C code. We leverage our grammar to produce other idiomatic code as well: in \nparticular, we include a statement kind that represents a loop iterating over an array. The grammar is \nimplemented by a collection of hand-coded C++ classes. Csmith maintains a global environment that holds \ntop-level de.nitions: i.e., types, global variables, and functions. The global environment is extended \nas new entities are de.ned during program generation. To hold information relevant to the current program\u00adgeneration \npoint, Csmith also maintains a local environment with three primary kinds of information. First, the \nlocal environment describes the current call chain, supporting context-sensitive pointer analysis. Second, \nit contains effect information describing objects that may have been read or written since (1) the start \nof the current function, (2) the start of the current statement, and (3) the previous sequence point.2 \nThird, the local environment carries points-to facts about all in-scope pointers. These elements and \ntheir roles in program generation are further described in Section 2.4. Csmith begins by randomly creating \na collection of struct type declarations. For each, it randomly decides on a number of members and the \ntype of each member. The type of a member may be a (possibly quali.ed) integral type, a bit-.eld, or \na previously generated struct type. After the preliminary step of producing type de.nitions, Csmith begins \nto generate C program code. Csmith generates a program top-down, starting from a single function called \nby main. Each step of the program generator involves the following sub-steps: 1. Csmith randomly selects \nan allowable production from its gram\u00admar for the current program point. To make the choice, it consults \n2 As explained in Section 3.8 of the C FAQ [25], A sequence point is a point in time at which the dust \nhas settled and all side effects which have been seen so far are guaranteed to be complete. The sequence \npoints listed in the C standard are at the end of the evaluation of a full expression (a full expression \nis an expression statement, or any other expression which is not a subexpression within any larger expression); \nat the ||, &#38;&#38;, ?:, and comma operators; and at a function call (after the evaluation of all the \narguments, and just before the actual call). a probability table and a .lter function speci.c to the \ncurrent point: there is a table/.lter pair for statements, another for ex\u00adpressions, and so on. The table \nassigns a probability to each of the alternatives, where the sum of the probabilities is one. After choosing \na production from the table, Csmith executes the .lter, which decides if the choice is acceptable in \nthe current con\u00adtext. Filters enforce basic semantic restrictions (e.g., continue can only appear within \na loop), user-controllable limits (e.g., maximum statement depth and number of functions), and other \nuser-controllable options. If the .lter rejects the selected pro\u00adduction, Csmith simply loops back, making \nselections from the table until the .lter succeeds. 2. If the selected production requires a target e.g., \na variable or function then the generator randomly selects an appropriate target or de.nes a new one. \nIn essence, Csmith dynamically constructs a probability table for the potential targets and in\u00adcludes \nan option to create a new target. Function and variable de.nitions are thus created on demand at the \ntime that Csmith decides to refer to them. 3. If the selected production allows the generator to select \na type, Csmith randomly chooses one. Depending on the current context, the choice may be restricted (e.g., \nwhile generating the operands of an integral-typed expression) or unrestricted (e.g., while generating \nthe types of parameters to a new function). Random choices are guided by the grammar, probability tables, \nand .lters as already described. 4. If the selected production is nonterminal, the generator recurses. \nIt calls a function to generate the program fragment that corre\u00adsponds to the nonterminal production. \nMore generally, Csmith recurses for each nonterminal element of the current production: e.g., for each \nsubcomponent of a compound statement, or for each parameter in a function call. 5. Csmith executes a \ncollection of data.ow transfer functions. It passes the points-to facts from the local environment to \nthe transfer functions, which produce a new set of points-to facts. Csmith updates the local environment \nwith these facts. 6. Csmith executes a collection of safety checks. If the checks succeed, the new code \nfragment is committed to the generated program. Otherwise, the fragment is dropped and any changes to \nthe local environment are rolled back.  When Csmith creates a call to a new function one whose body \ndoes not yet exist generation of the current function is suspended until the new function is .nished. \nThus, when the top-level function has been completely generated, Csmith is .nished. At that point it \npretty-prints all of the randomly generated de.nitions in an appropriate order: types, globals, prototypes, \nand functions. Finally, Csmith outputs a main function. The main function calls the top\u00adlevel randomly \ngenerated function, computes a checksum of the non-pointer global variables, prints the checksum, and \nexits.  2.4 Safety Mechanisms Table 1 lists the mechanisms that Csmith uses to avoid generating C programs \nthat execute unde.ned behaviors or depend on unspeci.ed behaviors. This section provides additional detail \nabout the hazards that Csmith must avoid and its strategies for avoiding them. Integer safety More and \nmore, compilers are aggressively ex\u00adploiting the unde.ned nature of integer behaviors such as signed \nover.ow and shift-past-bitwidth. For example, recent versions of Intel CC, GCC, and LLVM evaluate (x+1)>x \nto 1 while also eval\u00aduating (INT_MAX+1) to INT_MIN. In another example, discovered by the authors of \nGoogle s Native Client software [3], routine refac\u00ad toring of C code caused the expression 1<<32 to be \nevaluated on a Table 1. Summary of Csmith s strategies for avoiding unde.ned and unspeci.ed behaviors. \nWhen both a code-generation-time and code-execution-time solution are listed, Csmith uses both.  Problem \nCode-Generation-Time Solution Code-Execution-Time Solution use without initialization quali.er mismatch \nin.nite recursion signed integer over.ow OOB array access unspeci.ed eval. order of function arguments \nR/W and W/W con.icts betw. sequence points access to out-of-scope stack variable null pointer dereference \nexplicit initializers, avoid jumping over initializers static analysis disallow recursion bounded loop \nvars bounded loop vars effect analysis effect analysis pointer analysis pointer analysis  safe math \nwrappers force index in bounds  null pointer checks platform with 32-bit integers. The compiler exploited \nthis unde.ned behavior to turn a sandboxing safety check into a nop. To keep Csmith-generated programs \nfrom executing integer unde.ned behaviors, we implemented a family of wrapper functions for arithmetic \noperators whose (promoted) operands might over.ow. This was not dif.cult, but had a few tricky aspects. \nFor example, the C99 standard does not explicitly identify the evaluation of INT_MIN%-1 as being an unde.ned \nbehavior, but most compilers treat it as such. The C99 standard also has very restrictive semantics for \nsigned left-shift: it is illegal (for implementations using 2 s complement integers) to shift a 1-bit \ninto or past the sign bit. Thus, evaluating 1<<31 destroys the meaning of a C99 program on a platform \nwith 32-bit ints. Several safe math libraries for C that we examined themselves ex\u00adecute operations \nwith unde.ned behavior while performing checks. Apparently, avoiding such behavior is indeed a tricky \nbusiness. Type safety The aspect of C s type system that required the most care was quali.er safety: \nensuring that const and volatile quali.ers attached to pointers at various levels of indirection are \nnot removed by implicit casts. Accessing a const-or volatile-quali.ed object through a non-quali.ed pointer \nresults in unde.ned behavior. Pointer safety Null-pointer dereferences are easy to avoid using dynamic \nchecks. There is, on the other hand, no portable run-time method for detecting references to a function-scoped \nvariable whose lifetime has ended. (Hacks involving the stack pointer are not robust under inlining.) \nAlthough there are obvious ways to structurally avoid this problem, such as using a type system to ensure \nthat a pointer to a function-scoped variable never outlives the function, we judged this kind of strategy \nto be too restrictive. Instead, Csmith freely permits pointers to local variables to escape (e.g., into \nglobal variables) but uses a whole-program pointer analysis to ensure that such pointers are not dereferenced \nor used in comparisons once they become invalid. Csmith s pointer analysis is .ow sensitive, .eld sensitive, \ncontext sensitive, path insensitive, and array-element insensitive. A points-to fact is an explicit set \nof locations that may be referenced, and may include two special elements: the null pointer and the invalid \n(out\u00adof-scope) pointer. Points-to sets containing a single element serve as must-alias facts unless the \npointed-to object is an array element. Because Csmith does not generate programs that use the heap, assigning \nnames to storage locations is trivial. Effect safety The C99 standard states that [t]he order of evalua\u00adtion \nof the function designator, the actual arguments, and subexpres\u00adsions within the actual arguments is \nunspeci.ed. Also, unde.ned behavior occurs if [b]etween two sequence points, an object is modi.ed more \nthan once, or is modi.ed and the prior value is read other than to determine the value to be stored. \nTo avoid these problems, Csmith uses its pointer analysis to perform a conservative interprocedural analysis \nand determine the effect of every expression, statement, and function that it generates. An effect consists \nof two sets: locations that may be read and locations that may be written. Csmith ensures that no location \nis both read and written, or written more than once, between any pair of sequence points. As a special \ncase, in an assignment, a location can be read on the RHS and also written on the LHS. Effects are computed, \nand effect safety guaranteed, incrementally. At each sequence point, Csmith resets the current effect \n(i.e., may\u00adread and may-write sets). As fragments of code are generated, Csmith tests if the new code \nhas a read/write or write/write con.ict with the current effect. If a con.ict is detected, the new code \nis thrown away and the process restarts. For example, if Csmith is generating an expression p+func() \nand it happens that func may modify p, the call to func is discarded and a new subexpression is generated. \nIf there is no con.ict, the read and write sets are updated and the process continues. Probabilistic \nprogress is guaranteed: by design, Csmith always has a non-zero chance of generating code that introduces \nno new con.icts, such as a constant expression. Array safety Csmith uses several methods to ensure that \narray indices are in bounds. First, it generates index variables that are modi.ed only in the increment \nparts of for loops and whose values never exceed the bounds of the arrays being indexed. Second, variables \nwith arbitrary value are forced to be in bounds using the modulo operator. Finally, as needed, Csmith \nemits explicit checks against array lengths. Initializer safety A C program must not use an uninitialized \nfunction-scoped variable. For the most part, initializer safety is easy to ensure structurally by initializing \nvariables close to where they are declared. Gotos introduce the possibility that initializers may be \njumped over; Csmith solves this by forbidding gotos from spanning initialization code.  2.5 Ef.cient \nGlobal Safety Csmith never commits to a code fragment unless it has been shown to be safe. However, loops \nand function calls threaten to invalidate previously validated code. For example, consider the following \ncode, in which Csmith has just added the loop back-edge at line 7. 1 int i; 2 int *p = &#38;i; 3 while \n(...) { 4 *p = 3; 5 ... 6 p = 0; 7 } The assignment through p at line 4 was safe when it was generated. \nHowever, the newly added line 7 makes line 4 unsafe, due to the back-edge carrying a null-valued p. One \nsolution to this problem is to be conservative: run the whole\u00adprogram data.ow analysis before committing \nany new statement to the program. This is not ef.cient. We therefore restrict the analysis to local scope \nexcept when function calls and loops are involved. For a function call, the callee is re-analyzed at \neach call site immediately. Csmith uses a different strategy for loops. This is because so many statements \nare inside loops, and the extra calls to the data.ow analysis add substantial overhead to the code generator. \nCsmith s strategy is to optimistically generate code that is locally safe. Local safety includes running \na single step of the data.ow engine (which reaches a sound result when generating code not inside any \nloop).  The global .xpoint analysis is run when a loop is closed by adding its back-edge. If Csmith \n.nds that the program contains unsafe statements, it deletes code starting from the tail of the loop \nuntil the program becomes globally safe. This strategy is about three times faster than pessimistically \nrunning the global data.ow analysis before adding every piece of code.  2.6 Design Trade-offs Allow \nimplementation-de.ned behavior An ideally portable test program would be strictly conforming to the C \nlanguage standard. This means that the program s output would be independent of all unspeci.ed and unspeci.ed \nbehaviors and, in addition, be indepen\u00addent of any implementation-de.ned behavior. C99 has 114 kinds \nof implementation-de.ned behavior, and they have pervasive impact on the behavior of real C programs. \nFor example, the result of per\u00adforming a bitwise operation on a signed integer is implementation\u00adde.ned, \nand operands to arithmetic operations are implicitly cast to int (which has implementation-de.ned width) \nbefore performing the operation. We believe it is impossible to generate realistically ex\u00adpressive C \ncode that retains a single interpretation across all possible choices of implementation-de.ned behaviors. \nPrograms generated by Csmith do not generate the same output across compilers that differ in (1) the \nwidth and representation of integers, (2) behavior when casting to a signed integer type when the value \ncannot be represented in an object of the target type, and (3) the results of bitwise operations on \nsigned integers. In practice there is not much diversity in how C implementations de.ne these behaviors. \nFor mainstream desktop and embedded targets, there are roughly three equivalence classes of compiler \ntargets: those where int is 32 bits and long is 64 bits (e.g., x86-64), those where int and long are \n32 bits (e.g., x86, ARM, and PowerPC), and those where int is 16 bits and long is 32 bits (e.g., MSP430 \nand AVR). Using Csmith, we can perform differential testing within an equivalence class but not across \nclasses. No ground truth Csmith s programs are not self-checking: we are unable to predict their outputs \nwithout running them. This is not a problem when we use Csmith for randomized differential testing. We \nhave never seen an interesting split vote where randomized differential testing of a collection of C \ncompilers fails to produce a clear consensus answer, nor have we seen any cases in which a majority of \ntested compilers produces the same incorrect result. (We would catch the problem by hand as part of verifying \nthe failure-inducing program.) In fact, we have not seen even two unrelated compilers produce the same \nincorrect output for a Csmith\u00adgenerated test case. It therefore seems unlikely that all compilers under \ntest would produce the same incorrect output for a test case. Of course, if that did happen we would \nnot detect that problem; this is an inherent limitation of differential testing without an oracle. In \nsummary, despite the fact that Knight and Leveson [13] found a substantial number of correlated errors \nin an experiment on N\u00adversion programming, Csmith has yielded no evidence of correlated failures among \nunrelated C compilers. Our hypothesis is that the observed lack of correlation stems from the fact that \nmost compiler bugs are in passes that operate on an intermediate representation and there is substantial \ndiversity among IRs. No guarantee of termination It is not dif.cult to generate random programs that \nalways terminate. However, we judged that this would limit Csmith s expressiveness too much: for example, \nit would force loops to be highly structured. Additionally, always-terminating tests cannot .nd compiler \nbugs that wrongfully terminate a non\u00adterminating program. (We have found bugs of this kind.) About 10% \nof the programs generated by Csmith are (apparently) non\u00adterminating. In practice, during testing, they \nare easy to deal with using timeouts. Target middle-end bugs Commercial test suites for C compil\u00aders \n[1, 19, 20] are primarily aimed at checking standards confor\u00ad mance. Csmith, on the other hand, is mainly \nintended to .nd bugs in the parts of a compiler that perform transformations on an interme\u00addiate representation \nthe so-called middle end of a compiler. As a result, we have found large numbers of middle-end bugs missed \nby existing testing techniques (Section 3.6). At the same time, Csmith is rather poor at .nding gaps \nin standards conformance. For example, it makes no attempt to test a compiler s handling of trigraphs, \nlong identi.er names, or variadic functions. Targeting the middle end has several aspects. First, all \ngenerated programs pass the lexer, parser, and typechecker. Second, we per\u00adformed substantial manual \ntuning of the 80 probabilities that govern Csmith s random choices. Our goal was to make the generated \npro\u00adgrams look right to contain a balanced mix of arithmetic and bitwise operations, of references to \nscalars and aggregates, of loops and straight-line code, of single-level and multi-level indirections, \nand so on. Third, Csmith speci.cally generates idiomatic code (e.g., loops that access all elements of \nan array) to stress-test parts of the compiler we believe to be error-prone. Fourth, we designed Csmith \nwith an eye toward generating programs that exercise the constructs of a compiler s intermediate representation, \nand we decided to avoid generating source-level diversity that is unlikely to improve the coverage of \na compiler s intermediate representations. For exam\u00adple, since additional levels of parentheses around \nexpressions are stripped away early in the compilation process, we do not generate them, nor do we generate \nall of C s syntactic loop forms since they are typically all lowered to the same IR constructs. Finally, \nCsmith was designed to be fast enough that it can generate programs that are a few tens of thousands \nof lines long in a few seconds. Large programs are preferred because (empirically see Section 3.3) they \n.nd more bugs. In summary, many aspects of Csmith s design and implementation were informed by our understanding \nof how modern compilers work and how they break. 3. Results We conducted .ve experiments using Csmith, \nour random program generator. This section summarizes our .ndings. Our .rst experiment was uncontrolled \nand unstructured: over a three-year period, we opportunistically found and reported bugs in a variety \nof C compilers. We found bugs in all the compilers we tested hundreds of defects, many classi.ed as high-priority \nbugs. (\u00a73.1) In the second experiment, we compiled and ran one million random programs using several \nyears worth of versions of GCC and LLVM, to understand how their robustness is evolving over time. As \nmeasured by our tests over the programs that Csmith produces, the quality of both compilers is generally \nimproving. (\u00a73.2) Third, we evaluated Csmith s bug-.nding power as a function of the size of the generated \nC programs. The largest number of bugs is found at a surprisingly large program size: about 81 KB. (\u00a73.3) \nFourth, we compared Csmith s bug-.nding power to that of four previous random C program generators. Over \na week, Csmith was able to .nd signi.cantly more distinct compiler crash errors than previous program \ngenerators could. (\u00a73.4) Finally, we investigated the effect of testing random programs on branch, function, \nand line coverage of the GCC and LLVM source code. We found that these metrics did not signi.cantly improve \nwhen we added randomly generated programs to the compilers existing test suites. Nevertheless, as shown \nby our other results, Csmith-generated programs allowed us to discover bugs that are missed by the compilers \nstandard test suites. (\u00a73.5) We conclude the presentation of results by analyzing some of the bugs we \nfound in GCC and LLVM. (\u00a73.6, \u00a73.7)  GCC LLVM Crash 2 10 Wrong code 2 9 Total 4 19  Table 2. Crash \nand wrong-code bugs found by Csmith that manifest when compiler optimizations are disabled (i.e., when \nthe O0 command-line option is used) 3.1 Opportunistic Bug Finding We reported bugs to 11 different C \ncompiler development teams. Five of these compilers (GCC, LLVM, CIL, TCC, and Open64) were open source \nand .ve were commercial products. The eleventh, CompCert, is publicly available but not open source. \nWhat kinds of bugs are there? It is useful to distinguish between errors whose symptoms manifest at compile \ntime and those that only manifest when the compiler s output is executed. Compile\u00adtime bugs that we see \ninclude assertion violations or other internal compiler errors; involuntary compiler termination due \nto memory\u00adsafety problems; and cases in which the compiler exhausts the RAM or CPU time allocated to \nit. We say that a compile-time crash error has occurred whenever the compiler process exits with a status \nother than zero or fails to produce executable output. Errors that manifest at run time include the computation \nof a wrong result; a crash or other abnormal termination of the generated code; termination of a program \nthat should have executed forever; and non-termination of a program that should have terminated. We refer \nto these run-time problems as wrong-code errors.A silent wrong-code error is one that occurs in a program \nthat was produced without any sort of warning from the compiler; i.e., the compiler silently miscompiled \nthe test program. Experience with commercial compilers There exist many more commercial C compilers than \nwe could easily test. The ones we chose to study are fairly popular and were produced by what we believe \nare some of the strongest C compiler development teams. Csmith found wrong-code errors and crash errors \nin each of these tools within a few hours of testing. Because we are not paying customers, and because \nour .ndings represent potential bad publicity, we did not receive a warm response from any commercial \ncompiler vendor. Thus, for the most part, we simply tested these compilers until we found a few crash \nerrors and a few wrong-code errors, reported them, and moved on. Experience with open-source compilers \nFor several reasons, the bulk of our testing effort went towards GCC and LLVM. First and most important, \ncompiler testing is inherently interactive: we require feedback from the development team in the form \nof bug .xes. Bugs that occur with high probability can mask tricky, one-in-a\u00admillion bugs; thus, testing \nproceeds most smoothly when we can help developers rapidly destroy the easy bugs. Both the GCC and LLVM \nteams were responsive to our bug reports. The LLVM team in particular .xed bugs quickly, often within \na few hours and usually within a week. The second reason we prefer dealing with open\u00adsource compilers \nis that their development process is transparent: we can watch the mailing lists, participate in discussions, \nand see .xes as they are committed. Third, we want to help harden the open-source development tools that \nwe and many others use daily. So far we have reported 79 GCC bugs and 202 LLVM bugs the latter .gure \nrepresents about 2% of all LLVM bug reports. Most of our reported bugs have been .xed, and twenty-.ve \nof the GCC bugs were marked by developers as P1: the maximum, release-blocking priority for a bug. To \ndate, we have reported 325 in total across all tested compilers (GCC, LLVM, and others). An error that \noccurs at the lowest level of optimization is pernicious because it defeats the conventional wisdom that \ncompiler bugs can be avoided by turning off the optimizer. Table 2 counts these kinds of bugs, causing \nboth crash and wrong-code errors, that we found using Csmith. Testing CompCert CompCert [14] is a veri.ed, \noptimizing com\u00ad piler for a large subset of C; it targets PowerPC, ARM, and x86. We put signi.cant effort \ninto testing this compiler. The .rst silent wrong-code error that we found in CompCert was due to a miscompilation \nof this function: 1 int bar (unsigned x) { 2 return -1 <= (1 &#38;&#38; x); 3 } CompCert 1.6 for PowerPC \ngenerates code returning 0, but the proper result is 1 because the comparison is signed. This bug and \n.ve others like it were in CompCert s unveri.ed front-end code. Partly in response to these bug reports, \nthe main CompCert developer expanded the veri.ed portion of CompCert to include C s integer promotions \nand other tricky implicit casts. The second CompCert problem we found was illustrated by two bugs that \nresulted in generation of code like this: stwu r1, -44432(r1) Here, a large PowerPC stack frame is being \nallocated. The problem is that the 16-bit displacement .eld is over.owed. CompCert s PPC semantics failed \nto specify a constraint on the width of this immediate value, on the assumption that the assembler would \ncatch out-of-range values. In fact, this is what happened. We also found a handful of crash errors in \nCompCert. The striking thing about our CompCert results is that the middle\u00adend bugs we found in all other \ncompilers are absent. As of early 2011, the under-development version of CompCert is the only compiler \nwe have tested for which Csmith cannot .nd wrong-code errors. This is not for lack of trying: we have \ndevoted about six CPU-years to the task. The apparent unbreakability of CompCert supports a strong argument \nthat developing compiler optimizations within a proof framework, where safety checks are explicit and \nmachine-checked, has tangible bene.ts for compiler users.  3.2 Quantitative Comparison of GCC and LLVM \nVersions Figure 3 shows the results of an experiment in which we com\u00ad piled and ran 1,000,000 randomly \ngenerated programs using LLVM 1.9 2.8, GCC 3.[0 4].0, and GCC 4.[0 5].0. Every pro\u00adgram was compiled \nat O0, O1, O2, Os, and O3. A test case was considered valid if every compiler terminated (successfully \nor otherwise) within .ve minutes and if every compiled random program terminated (correctly or otherwise) \nwithin .ve seconds. All compilers targeted x86. Running these tests took about 1.5 weeks on 20 machines \nin the Utah Emulab testbed [28]. Each machine had one quad-core Intel Xeon E5530 processor running at \n2.4 GHz. Compile-time failures The top row of graphs in Figure 3 shows the observed rate of crash errors. \n(Note that the y-axes of these graphs are logarithmic.) These graphs also indicate the number of crash \nbugs that were .xed in response to our bug reports. Both compilers became at least three orders of magnitude \nless crashy over the range of versions covered in this experiment. The GCC results appear to tell a nice \nstory: the 3.x release series increases in quality, the 4.0.0 release regresses because it represents \na major change to GCC s internals, and then quality again starts to improve. The middle row of graphs \nin Figure 3 shows the number of distinct assertion failures in LLVM and the number of distinct internal \ncompiler errors in GCC induced by our tests. These are the numbers of code locations in LLVM and GCC \nat which an internal  Figure 3. Distinct crash errors found, and rates of crash and wrong-code errors, \nfrom recent LLVM and GCC versions consistency check failed. These graphs conservatively estimate the \nbugs, and compiler writers can reduce it to zero by eliminating number of distinct failures in these \ncompilers, since we encountered error messages and always returning a success status code to the many \nsegmentation faults caused by use of free memory, null-pointer operating system. The number of distinct \ncrashes, on the other hand, dereferences, and similar problems. We did not include these faults suffers \nfrom the drawback that it depends on the quantity and style in our graphed results due to the dif.culty \nof mapping crashes back of assertions in the compiler under test. Although GCC has more to distinct causes. \ntotal assertions than LLVM, LLVM has a higher density: about one It is not clear which of these two metrics \nof crashiness is assertion per 100 lines of code, compared to one in 250 for GCC. preferable. The rate \nof crashes is easy to game: we can make it arbitrarily high by biasing Csmith to generate code triggering \nknown Run-time failures The bottom pair of graphs in Figure 3 shows the rate of wrong-code errors in \nour experiment. Unfortunately, we Figure 4. Number of distinct crash errors found in 24 hours of testing \nwith Csmith-generated programs in a given size range  can only report the rate of errors, and not the \nnumber of bugs causing them, because we do not know how to automatically map failing tests back to the \nbugs that cause them. These graphs also indicate the number of wrong-code bugs that were .xed in response \nto our bug reports.  3.3 Bug-Finding Performance as a Function of Test-Case Size There are many ways \nin which a random test-case generator might be tuned for particular goals, e.g., to focus on certain \nkinds of compiler defects. We performed an experiment to answer this question: given the goal of .nding \nmany defects quickly, should one con.gure Csmith to generate small programs or large ones? Other factors \nbeing equal, small test cases are preferable because they are closer to being reportable to compiler \ndevelopers. Using the same compilers and optimization options that we used for the experiments in Section \n3.2, we ran our testing process multiple times. For each run we selected a size range for test inputs, \ncon.gured Csmith to generate programs in that range,3 executed the test process for 24 hours, and counted \nthe distinct crash errors found. We repeated this for various ranges of test-input sizes. Figure 4 shows \nthat the rate of crash-error detection varies signi.cantly as a function of the sizes of the test programs \nproduced by Csmith. The greatest number of distinct crash errors is found by programs containing 8 K \n16 K tokens: these programs averaged 81 KB before preprocessing. The con.dence intervals are at 95% and \nwere computed based on .ve repetitions. We hypothesize that larger test cases expose more compiler errors \nfor two reasons. First, throughput is increased because compiler start\u00adup costs are better amortized. \nSecond, the combinatorial explosion of feature interactions within a single large test case works in \nCsmith s favor. The decrease in bug-.nding power at the largest sizes appears to come from algorithms \nin Csmith and in the compilers that have superlinear running time. 3.4 Bug-Finding Performance Compared \nto Other Tools To evaluate Csmith s ability to .nd bugs, we compared it to four other random program \ngenerators: the two versions of Randprog described in Section 2 and two others described in Section 5. \nWe ran each generator in its default con.guration on one of .ve identical 3 Although we can tune Csmith \nto prefer generating larger or smaller output, it lacks the ability to construct a test case of a speci.c \nsize on demand. We ran this experiment by precomputing seeds to Csmith s random-number generator that \ncause it to generate programs of the sizes we desired. Figure 5. Comparison of the ability of .ve random \nprogram gener\u00adators to .nd distinct crash errors Line Coverage Function Coverage Branch Coverage GCC \nmake check-c make check-c &#38; random % change absolute change 75.13% 75.58% +0.45% +1,482 82.23% 82.41% \n+0.13% +33 46.26% 47.11% +0.85% +4,471 Clang make test make test &#38; random % change absolute change \n74.54% 74.69% +0.15% +655 72.90% 72.95% +0.05% +74 59.22% 59.48% +0.26% +926 Table 3. Augmenting the \nGCC and LLVM test suites with 10,000 randomly generated programs did not improve code coverage much and \notherwise-idle machines, using one CPU on each host. Each generator repeatedly produced programs that \nwe compiled and tested using the same compilers and optimization options that were used for the experiments \nin Section 3.2. Figure 5 plots the cumulative number of distinct crash errors found by these program \ngenerators during the one-week test. Csmith signi.cantly outperforms the other tools. 3.5 Code Coverage \nBecause we .nd many bugs, we hypothesized that randomly gener\u00adated programs exercise large parts of the \ncompilers that were not cov\u00adered by existing test suites. To test this, we enabled code-coverage monitoring \nin GCC and LLVM. We then used each compiler to build its own test suite, and also to build its test suite \nplus 10,000 Csmith-generated programs. Table 3 shows that the incremental coverage due to Csmith is so \nsmall as to be a negative result. Our best guess is that these metrics are too shallow to capture Csmith \ns effects, and that we would generate useful additional coverage in terms of deeper metrics such as path \nor value coverage.  3.6 Where Are the Bugs? Table 4 characterizes the GCC and LLVM bugs we found by \ncompiler part. Tables 5 and 6 show the ten buggiest .les in LLVM and GCC as measured by our experiment \nin Section 3.1. Most of the bugs we found in GCC were in the middle end: the machine\u00adindependent optimizers. \nLLVM is a younger compiler and our testing shook out some front-end and back-end bugs that would probably \nnot be present in a more mature software base.  GCC LLVM Front end 0 10 Middle end 49 75 Back end 17 \n74 Unclassi.ed 13 43 Total 79 202  Table 4. Distribution of bugs across compiler stages. A bug is unclassi.ed \neither because it has not yet been .xed or the developer who .xed the bug did not indicate what .les \nwere changed. C File Name Purpose Wrong-Code Bugs Crash Bugs fold-const constant folding 3 6 combine \ninstruction combining 1 5 tree-ssa-pre partial redundancy elim. 0 4 tree-vrp variable range propagation \n0 4 tree-ssa-dce dead code elimination 0 3 tree-ssa-reassoc arithmetic expr. reassociation 0 2 reload1 \nregister reloading 1 1 tree-ssa-loop\u00ad loop iteration counting 1 1 niter dse dead store elimination 2 \n0 tree-scalar\u00ad scalar evolution 2 0 evolution Other (15 .les) n/a 19 24 Total (25 .les) n/a 29 50 Table \n5. Top ten buggy .les in GCC C++ File Name Purpose Wrong-Code Bugs Crash Bugs Instruction-Combining mid-level \ninstruction combining 9 6 SimpleRegister-Coalescing register coalescing 1 10 DAGCombiner instruction \ncombining 5 3 LoopUnswitch loop unswitching 1 4 LICM loop invariant code motion 0 5 LoopStrength-Reduce \nloop strength reduction 1 3 FastISel fast instruction selection 1 3 llvm-convert GCC-LLVM IR conversion \n0 4 ExprConstant constant folding 2 2 JumpThreading jump threading 0 4 Other (72 .les) n/a 46 92 Total \n(82 .les) n/a 66 136 Table 6. Top ten buggy .les in LLVM  3.7 Examples of Wrong-Code Bugs This section \ncharacterizes a few of the bugs that were revealed by miscompilation of programs generated by Csmith. \nThese bugs .t into a simple model in which optimizations are structured like this: analysis if (safety \ncheck) { transformation } An optimization can fail to be semantics-preserving if the analysis is wrong, \nif the safety check is insuf.ciently conservative, or if the transformation is incorrect. The most common \nroot cause for bugs that we found was an incorrect safety check. GCC Bug #1: wrong safety check4 If x \nis variable and c1 and c2 are constants, the expression (x/c1)!=c2 can be pro.tably rewritten as (x-(c1*c2))>(c1-1), \nusing unsigned arithmetic to avoid problems with negative values. Prior to performing the transformation, \nexpressions such as c1*c2 and (c1*c2)+(c1-1) are checked for over.ow. If over.ow occurs, further simpli.cations \ncan be made; for example, (x/1000000000)!=10always evaluates to 0 when x is a 32-bit integer. GCC falsely \ndetected over.ow for some choices of constants. In the failure-inducing test case that we discovered, \n(x/-1)!=1 was folded to 0. This expression should evaluate to 1 for many values of x, such as 0. GCC \nBug #2: wrong transformation5 In C, if an argument of type unsigned char is passed to a function with \na parameter of type int, the values seen inside the function should be in the range 0..255. We found \na case in which a version of GCC inlined this kind of function call and then sign-extended the argument \nrather than zero-extending it, causing the function to see negative values of the parameter when the \nfunction was called with arguments in the range 128..255. GCC Bug #3: wrong analysis6 We found a bug \nthat caused GCC to miscompile this code: 1 static int g[1]; 2 static int *p = &#38;g[0]; 3 static int \n*q = &#38;g[0]; 4 5 int foo (void) { 6 g[0] = 1; 7 *p = 0; 8 *p = *q; 9 return g[0]; 10 } The generated \ncode returned 1 instead of 0. The problem oc\u00ad curred when the compiler failed to recognize that pand \nqare aliases; this happened because q was mistakenly identi.ed as a read-only memory location, which \nis de.ned not to alias a mutable location. The wrong not-alias fact caused the store in line 7 to be \nmarked as dead so that a subsequent dead-store elimination pass removed it. GCC Bug #4: wrong analysis7 \nA version of GCC miscompiled this function: 1 int x = 4; 2 int y; 3 4 void foo (void) { 5 for (y = 1; \ny < 8; y += 7) { 6 int *p = &#38;y; 7 *p = x; 8 } 9 } When foo returns, y should be 11. A loop-optimization \npass determined that a temporary variable representing *p was invariant with value x+7 and hoisted it \nin front of the loop, while retaining a data.ow fact indicating that x+7==y+7, a relationship that no \nlonger held after code motion. This incorrect fact lead GCC to generate code leaving 8 in y, instead \nof 11. 4 http://gcc.gnu.org/bugzilla/show_bug.cgi?id=42721 5 http://gcc.gnu.org/bugzilla/show_bug.cgi?id=43438 \n6 http://gcc.gnu.org/bugzilla/show_bug.cgi?id=42952 7 http://gcc.gnu.org/bugzilla/show_bug.cgi?id=43360 \n  LLVM Bug #1: wrong safety check8 (x==c1)||(x<c2) can be simpli.ed to x<c2 when c1 and c2 are constants \nand c1<c2. An LLVM version incorrectly transformed (x==0)||(x<-3) to x<-3. LLVM did a comparison between \n0 and -3 in the safety check for this optimization, but performed an unsigned comparison rather than \na signed one, leading it to incorrectly determine that the transformation was safe. LLVM Bug #2: wrong \nsafety check9 (x|c1)==c2 evaluates to 0 if c1 and c2are constants and (c1&#38; c2)!=0. In other words, \nif any bit that is set in c1 is unset in c2, the original expression cannot be true. A version of LLVM \ncontained a logic error in the safety check for this optimization, wrongly replacing this kind of expression \nwith 0 even when c1 was not a constant. LLVM Bug #3: wrong safety check10 Narrowing is a strength\u00adreduction \noptimization that can be applied to loads when only part of an object is needed, or to stores where only \npart of an object is modi.ed. For example, at the level of the abstract machine this code loads and stores \nan unsigned int: 1 unsigned y; 2 3 void bar (void) { 4 y |= 255; 5 } Optimizing compilers for x86 may \ntranslate bar into the following code, which loads nothing and stores a single byte: bar: movb $-1, y \nret We found a case in which LLVM attempted to perform an analogous narrowing operation, but a logic \nerror caused the safety check to succeed even when a different store modi.ed the object prior to the \nstore that was the target of the narrowing transformation. LLVM Bug #4: wrong analysis11 This code should \nprint 5 : 1 void foo (void) { 2 int x; 3 for (x = 0; x < 5; x++) {  4 if (x) continue; 5 if (x) break; \n6 } 7 printf(\"%d\", x); 8 } LLVM s scalar evolution analysis computes properties of loop induction variables, \nincluding the maximum number of iterations. Line 5 of the program above caused this analysis to mistakenly \nconclude that x was 1 after the loop executed. 4. Discussion Are we .nding bugs that matter? One might \nsuspect that random testing .nds bugs that do not matter in practice. Undoubtedly this happens sometimes, \nbut in a number of instances we have direct con.rmation that Csmith is .nding bugs that matter, because \nbugs that we have found and reported have been independently rediscovered and re-reported by application \ndevelopers. By a very conservative estimate counting only the times that a compiler 8 http://llvm.org/bugs/show_bug.cgi?id=2844 \n9 http://llvm.org/bugs/show_bug.cgi?id=7750 10 http://llvm.org/bugs/show_bug.cgi?id=7833 11 http://llvm.org/bugs/show_bug.cgi?id=7845 \n developer explicitly labeled a wrong-code bug report as a duplicate of one of ours this has happened \neight times: four times for GCC and four for LLVM. We also have indirect con.rmation that our bugs matter. \nThe developers of open-source compilers .xed almost all of the bugs that we reported, and the GCC development \nteam marked 25 of our bugs as P1: the maximum, release-blocking priority. Creating reportable bugs Reporting \ncompiler crash bugs is easy, but reporting wrong-code bugs is harder. Compiler developers will (rightfully) \nignore a wrong-code bug report that is based on a large random program. Rather, a bug report must be \naccompanied by com\u00adpelling evidence that a bug exists; in most cases the best evidence is a small test \ncase that is obviously miscompiled. Delta debug\u00adging [31] automates test-case reduction, but all existing \nvariants that are intended for reducing C programs such as hierarchical delta debugging [18] and Wilkerson \ns implementation [29] introduce unde.ned behavior. The resulting programs are small but useless. To avoid \nunde.ned behavior during reduction, we rely on compiler warnings, dynamic checkers, and manual test-case \nreduction. There is substantial room for improvement. The relationship between testing and veri.cation \nAs our Comp-Cert results make plain, veri.cation does not obviate testing, but rather complements it. \nTesting can provide end-to-end evidence that numerous paths through a system work properly. Veri.cation, \non the other hand, typically focuses on a narrow slice of a stack of tools, and the parts outside the \nslice remain in the trusted computing base. There does not yet appear to be a nuanced understanding of \nthe kinds of testing, and the amount of testing effort, that are rendered unnecessary by artifacts like \nCompCert [14] and seL4 [12]. Toward realistic, correct compilers Compilers must support rapid development \nto cope with new optimizations, new source languages, and new target architectures. Generated code often \nneeds to be resource-ef.cient to support application developers goals. Finally, compilers should generate \ncorrect code. Meeting even two of these goals is challenging, and it is not clear how to meet all three \nin a single tool. There seem to be four paths forward. Compiler veri.cation. Although it is dif.cult \nto imagine a veri.ed compiler for C++0x, due to the immense complexity of the draft standard, CompCert \nis an existence proof that a veri.ed, optimizing C compiler is within reach. However, the burden of veri.cation \nis signi.cant. CompCert still lacks a number of useful C features and few mainstream compiler developers \nhave the formal veri.cation skills that are needed to add new language features and optimization passes. \nOn the other hand, projects such as XCERT [26] may dramatically lower the bar for working on veri.ed \ncompilation. Compiler simplicity. For non-bottleneck applications, compiler optimization adds little \nend-user value. It would seem possible to take a simple compiler such as TCC [2], which does not optimize \nacross statement boundaries, and validate it through code inspec\u00adtions, heavy use, and other techniques. \nAt present, however, TCC is much buggier than more heavily-used compilers such as GCC and LLVM. Compiler \ntesting. We hypothesize that it is possible to gain high con.dence in a complex compiler like GCC by \nchoosing a .xed con.guration, disabling optimization passes whose effects are signi.cantly non-local, \nand performing just enough testing. A test plan would be suf.cient if all code paths through the compiler \nthat are used to compile an application of interest had been tested. Clearly, a sophisticated way to \nabstract over paths is needed. Equivalence checking. If equivalence checkers for machine code [7] could \nscale to large programs, veri.ed compilers would be largely unnecessary because one compiler s output \ncould be proved equivalent to another s. Although these tools are not likely to scale up to multi-megabyte \napplications anytime soon, it should be possible to automatically partition applications into smaller \nparts so that equivalence checking can be done piecewise.  Future work Augmenting Csmith with white-box \ntesting tech\u00adniques, where the structure of the tested system is taken into account in a .rst-class way, \nwould be productive. This will be dif.cult for several reasons. First, we anticipate substantial challenges \nin inte\u00adgrating the necessary constraint-solving machinery with Csmith s existing logic for generating \nvalid C programs. It is possible that we will need to start over, next time engineering a version of \nCsmith in which all constraints are explicit and declarative, rather than being buried in a small mountain \nof C++. Second, the inverse problems that must be solved to generate an input become prohibitively dif\u00ad.cult \nwhen inputs pass through a parser, particularly if the parser contains hash tables. Godefroid et al. \n[8] showed a way to solve this problem by integrating a constraint solver with a grammar for the language \nbeing generated. However, due to its non-local pointer and effect analyses, the validity decision problem \nfor programs in the subset of C that Csmith generates is far harder than the question of whether a program \ncan be generated by the JavaScript grammar used by Godefroid et al. 5. Related Work Compilers have been \ntested using randomized methods for nearly 50 years. Boujarwah and Saleh [4] gave a good survey in 1997. \nIn 1962, Sauder [22] tested the correctness of COBOL compilers by placing random variables in programs \ndata sections. In 1970, Hanford [10] used a PL/1 grammar to drive the generation of random programs. \nThe grammar was extensible and was augmented by syntax generators that could be used, for example, to \nensure that variables were declared before being used. In 1972, Purdom [21] used a syntax-directed method \nto generate test sentences for a parser. He gave an ef.cient algorithm for generating short sentences \nfrom a context-free grammar such that each production of the grammar was used at least once, and he tested \nLR(1) parsers using this technique. Burgess and Saidi [5] designed an automatic generator of test cases \nfor FORTRAN compilers. The tests were designed to be self\u00adchecking and to contain features that optimizing \ncompilers were known to exploit. In order to predict test cases results, the code generator restricted \nassignment statements to be executed only once during the execution of the sub-program or main program. \nThese tests found four bugs in two FORTRAN 77 compilers. In 1998, McKeeman [16] coined the term differential \ntesting. His work resulted in DDT, a family of program generators that conform to the C standard at various \nlevels, from level 1 (random characters) to level 7 (generated code is model conforming, incor\u00adporating \nsome high-level structure). DDT is more expressive than Csmith (DDT is capable of generating all legal \nC programs) and it was used to .nd numerous bugs in C compilers. To our knowledge, McKeeman s paper contains \nthe .rst acknowledgment that it is im\u00adportant to avoid unde.ned behavior in generated C programs used \nfor compiler testing. However, DDT avoided only a small subset of all unde.ned behaviors, and only then \nduring test-case reduc\u00adtion, not during normal testing. Thus, it is not a suitable basis for automatic \nbug-.nding. Lindig [15] used randomly generated C programs to .nd several compiler bugs related to calling \nconventions. His tool, called Quest, was specially targeted: rather than generating code with control \n.ow and arithmetic, Quest generates code that creates complex data structures, loads them with constant \nvalues, and passes them to a function where assertions check the received values. Because its tests are \nself-checking, Quest is not based on differential testing. Self-checking tests are convenient, but the \ndrawback is that Quest is far less expressive than Csmith. Lindig used Quest to test GCC, LCC, ICC, and \na few other compilers and found 13 bugs. Sheridan [23] also used a random generator to .nd bugs in C \ncompilers. A script rotated through a list of constants of the principal arithmetic types, producing \na source .le that applied various operators to pairs of constants. This tool found two bugs in GCC, one \nbug in SUSE Linux s version of GCC, and .ve bugs in CodeSourcery s version of GCC for ARM. Sheridan s \ntool produces self-checking tests. However, it is less expressive than Csmith and it fails to avoid unde.ned \nbehavior such as signed over.ow. Zhao et al. [32] created an automated program generator for testing \nan embedded C++ compiler. Their tool allows a general test requirement, such as which optimization to \ntest, to be speci.ed in a script. The generator constructs a program template based on the test requirement \nand uses it to drive further code generation. Zhao et al. used GCC as the reference to check the compiler \nunder test. They reported greatly improved statement coverage in the tested modules and found several \nnew compiler bugs. 6. Conclusion Using randomized differential testing, we found and reported hun\u00addreds \nof previously unknown bugs in widely used C compilers, both commercial and open source. Many of the bugs \nwe found cause a compiler to emit incorrect code without any warning. Most of our re\u00adported defects have \nbeen .xed, meaning that compiler implementers found them important enough to track down, and 25 of the \nbugs we reported against GCC were classi.ed as release-blocking. All of this evidence suggests that there \nis substantial room for improvement in the state of the art for compiler quality assurance. To create \na random program generator with high bug-.nding power, the key problem we solved was the expressive generation \nof C programs that are free of unde.ned behavior and independent of unspeci.ed behavior. Csmith, our \nprogram generator, uses both static analysis and dynamic checks to avoid these hazards. The return on \ninvestment from random testing is good. Our rough estimate including faculty, staff, and student salaries, \nmachines purchased, and university overhead is that each of the more than 325 bugs we reported cost less \nthan $1,000 to .nd. The incremental cost of a new bug that we .nd today is much lower. Software Csmith \nis open source and available for download at http://embed.cs.utah.edu/csmith/. Acknowledgments The authors \nwould like to thank Bruce Childers, David Coppit, Chucky Ellison, Robby Findler, David Gay, Casey Klein, \nGerwin Klein, Chris Lattner, Sorin Lerner, Xavier Leroy, Bill McKeeman, Diego Novillo, Alastair Reid, \nJulian Seward, Zach Tatlock, our shepherd Atanas Rountev, and the anonymous reviewers for their invaluable \nfeedback on drafts of this paper. We also thank Hans Boehm, Xavier Leroy, Michael Norrish, Bryan Turner, \nand the GCC and LLVM development teams for their technical assistance in various aspects of our work. \nThis research was primarily supported by an award from DARPA s Computer Science Study Group. References \n[1] ACE Associated Computer Experts. SuperTest C/C++ compiler test and validation suite. http://www.ace.nl/compiler/supertest. \nhtml. [2] F. Bellard. TCC: Tiny C compiler, ver. 0.9.25, May 2009. http: //bellard.org/tcc/. [3] C. L. \nBif.e. Unde.ned behavior in Google NaCl, Jan. 2010. http:// code.google.com/p/nativeclient/issues/detail?id=245. \n [4] A. S. Boujarwah and K. Saleh. Compiler test case generation methods: a survey and assessment. Information \nand Software Technology, 39(9):617 625, 1997. [5] C. J. Burgess and M. Saidi. The automatic generation \nof test cases for optimizing Fortran compilers. Information and Software Technology, 38(2):111 119, 1996. \n[6] E. Eide and J. Regehr. Volatiles are miscompiled, and what to do about it. In Proc. EMSOFT, pages \n255 264, Oct. 2008. [7] X. Feng and A. J. Hu. Cutpoints for formal equivalence veri.cation of embedded \nsoftware. In Proc. EMSOFT, pages 307 316, Sept. 2005. [8] P. Godefroid, A. Kie. zun, and M. Y. Levin. \nGrammar-based whitebox fuzzing. In Proc. PLDI, pages 206 215, June 2008. [9] R. Hamlet. Random testing. \nIn J. Marciniak, editor, Encyclopedia of Software Engineering. Wiley, second edition, 2001. [10] K. V. \nHanford. Automatic generation of test cases. IBM Systems Journal, 9(4):242 257, Dec. 1970. [11] International \nOrganization for Standardization. ISO/IEC 9899:TC2: Programming Languages C, May 2005. http://www.open-std. \norg/jtc1/sc22/wg14/www/docs/n1124.pdf. [12] G. Klein et al. seL4: Formal veri.cation of an OS kernel. \nIn Proc. SOSP, pages 207 220, Oct. 2009. [13] J. C. Knight and N. G. Leveson. An experimental evaluation \nof the assumption of independence in multiversion programming. IEEE Trans. Software Eng., 12(1):96 109, \nJan. 1986. [14] X. Leroy. Formal veri.cation of a realistic compiler. Commun. ACM, 52(7):107 115, July \n2009. [15] C. Lindig. Random testing of C calling conventions. In Proc. AADEBUG, pages 3 12, Sept. 2005. \n[16] W. M. McKeeman. Differential testing for software. Digital Technical Journal, 10(1):100 107, Dec. \n1998. [17] B. P. Miller, L. Fredriksen, and B. So. An empirical study of the reliability of UNIX utilities. \nCommun. ACM, 33(12):32 44, Dec. 1990. [18] G. Misherghi and Z. Su. HDD: Hierarchical delta debugging. \nIn Proc. ICSE, pages 142 151, May 2006. [19] Perennial, Inc. ACVS ANSI/ISO/FIPS-160 C validation suite, \nver. 4.5, Jan. 1998. http://www.peren.com/pages/acvs_set.htm. [20] Plum Hall, Inc. The Plum Hall validation \nsuite for C. http: //www.plumhall.com/stec.html. [21] P. Purdom. A sentence generator for testing parsers. \nBIT Numerical Mathematics, 12(3):366 375, 1972. [22] R. L. Sauder. A general test data generator for \nCOBOL. In AFIPS Joint Computer Conferences, pages 317 323, May 1962. [23] F. Sheridan. Practical testing \nof a C99 compiler using output compar\u00adison. Software Practice and Experience, 37(14):1475 1488, Nov. \n2007. [24] J. Souyris, V. Wiels, D. Delmas, and H. Delseny. Formal veri.cation of avionics software products. \nIn Proc. FM, pages 532 546, Nov. 2009. [25] S. Summit. comp.lang.c frequently asked questions. http://c-faq. \ncom/. [26] Z. Tatlock and S. Lerner. Bringing extensibility to veri.ed compilers. In Proc. PLDI, pages \n111 121, June 2010. [27] B. Turner. Random Program Generator, Jan. 2007. http://sites. google.com/site/brturn2/randomcprogramgenerator. \n[28] B. White et al. An integrated experimental environment for distributed systems and networks. In \nProc. OSDI, pages 255 270, Dec. 2002. [29] D. S. Wilkerson. Delta ver. 2006.08.03, Aug. 2006. http://delta. \ntigris.org/. [30] M. Wolfe. How compilers and tools differ for embedded systems. In Proc. CASES, Sept. \n2005. Keynote address. http://www.pgroup. com/lit/articles/pgi_article_cases.pdf. [31] A. Zeller and \nR. Hildebrandt. Simplifying and isolating failure\u00adinducing input. IEEE Trans. Software Eng., 28(2):183 \n200, Feb. 2002. [32] C. Zhao et al. Automated test program generation for an industrial optimizing compiler. \nIn Proc. ICSE Workshop on Automation of Software Test, pages 36 43, May 2009.   \n\t\t\t", "proc_id": "1993498", "abstract": "<p>Compilers should be correct. To improve the quality of C compilers, we created Csmith, a randomized test-case generation tool, and spent three years using it to find compiler bugs. During this period we reported more than 325 previously unknown bugs to compiler developers. Every compiler we tested was found to crash and also to silently generate wrong code when presented with valid input. In this paper we present our compiler-testing tool and the results of our bug-hunting study. Our first contribution is to advance the state of the art in compiler testing. Unlike previous tools, Csmith generates programs that cover a large subset of C while avoiding the undefined and unspecified behaviors that would destroy its ability to automatically find wrong-code bugs. Our second contribution is a collection of qualitative and quantitative results about the bugs we have found in open-source C compilers.</p>", "authors": [{"name": "Xuejun Yang", "author_profile_id": "81435601120", "affiliation": "University of Utah, Salt Lake City, UT, USA", "person_id": "P2690562", "email_address": "jxyang@cs.utah.edu", "orcid_id": ""}, {"name": "Yang Chen", "author_profile_id": "81444601555", "affiliation": "University of Utah, Salt Lake City, UT, USA", "person_id": "P2690563", "email_address": "chenyang@cs.utah.edu", "orcid_id": ""}, {"name": "Eric Eide", "author_profile_id": "81341490043", "affiliation": "University of Utah, Salt Lake City, UT, USA", "person_id": "P2690564", "email_address": "eeide@cs.utah.edu", "orcid_id": ""}, {"name": "John Regehr", "author_profile_id": "81100459621", "affiliation": "University of Utah, Salt Lake City, UT, USA", "person_id": "P2690565", "email_address": "regehr@cs.utah.edu", "orcid_id": ""}], "doi_number": "10.1145/1993498.1993532", "year": "2011", "article_id": "1993532", "conference": "PLDI", "title": "Finding and understanding bugs in C compilers", "url": "http://dl.acm.org/citation.cfm?id=1993532"}