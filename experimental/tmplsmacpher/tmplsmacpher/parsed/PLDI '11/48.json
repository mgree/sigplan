{"article_publication_date": "06-04-2011", "fulltext": "\n Precise and Compact Modular Procedure Summaries for Heap Manipulating Programs * Isil Dillig Thomas \nDillig Alex Aiken Department of Computer Science Stanford University Department of Computer Science Stanford \nUniversity Department of Computer Science Stanford University isil@cs.stanford.edu tdillig@cs.stanford.edu \naiken@cs.stanford.edu Mooly Sagiv Department of Computer Science Tel Aviv University msagiv@post.tau.ac.il \n Abstract We present a strictly bottom-up, summary-based, and precise heap analysis targeted for program \nveri.cation that performs strong up\u00addates to heap locations at call sites. We .rst present a theory of \nheap decompositions that forms the basis of our approach; we then de\u00adscribe a full analysis algorithm \nthat is fully symbolic and ef.cient. We demonstrate the precision and scalability of our approach for \nveri.cation of real C and C++ programs. Categories and Subject Descriptors D.2.4 [Software Engineer\u00ading]: \nSoftware/Program Veri.cation General Terms Languages, Veri.cation, Experimentation 1. Introduction It \nis well-known that precise static reasoning about the heap is a key requirement for successful veri.cation \nof real-world software. In standard imperative languages, such as Java, C, and C++, much of the interesting \ncomputation happens as values .ow in and out of the heap, making it crucial to use a precise, context-and \n.ow-sensitive heap analysis in program veri.cation tools. Flow-sensitivity, in par\u00adticular, enables strong \nupdates. Informally, when analyzing an as\u00adsignment a := b, a strong update replaces the analysis informa\u00adtion \nfor a with the analysis information for b. This natural rule is unsound if a is a summary location, meaning \na may represent more than one concrete location. In previous work there is an apparent tension between \nscalability and precision in heap analysis: * This work was supported by grants from NSF and DARPA (CCF\u00ad0430378, \nCCF-0702681). Permission to make digital or hard copies of all or part of this work for personal or classroom \nuse is granted without fee provided that copies are not made or distributed for pro.t or commercial advantage \nand that copies bear this notice and the full citation on the .rst page. To copy otherwise, to republish, \nto post on servers or to redistribute to lists, requires prior speci.c permission and/or a fee. PLDI \n11, June 4 8, 2011, San Jose, California, USA. Copyright c &#38;#169; 2011 ACM 978-1-4503-0663-8/11/06. \n. . $10.00 For scalability, it is desirable to analyze the program in pieces, for example, one function \nat a time. Many of the most scalable analyses in the literature are modular [1, 2].  For precision, \na large body of empirical evidence shows it is necessary to perform strong updates wherever possible \n[3, 4].  It is not obvious, however, how to perform strong updates in a modular heap analysis. Consider \na function h(x, y){e}. When ana\u00adlyzing h in isolation, we do not know how many, or which, locations x \nand y may point to at a call site of h it may be many (if either x or y is a summary location), two, \nor even one (if x and y are aliases). Without this information, we cannot safely apply strong updates \nto x and y in e. Thus, while there is a large body of existing work on .ow-and context-sensitive heap \nanalysis, most algorithms for this purpose either perform a whole-program analysis or perform strong \nupdates under very restrictive conditions. In this paper, we present a modular, strictly bottom-up, .ow\u00adand \ncontext-sensitive heap analysis that uses summaries to apply strong updates to heap locations at call \nsites. As corraborated by our experiments, strong updates are crucial for the level of preci\u00adsion required \nfor successful veri.cation. Furthermore, we are inter\u00adested in a modular, summary-based analysis because \nit offers the following key advantages over a whole program analysis: Reuse of analysis results: A major \nproblem with whole\u00adprogram analysis is that results for a particular program com\u00adponent cannot be reused, \nsince functions are analyzed in a par\u00adticular context. For instance, adding a single caller to a library \nmay require complete re-analysis of the entire library. In con\u00adtrast, modular analyses allow complete \nreuse of analysis results because procedure summaries are valid in any context.  Analysis scalability: \nFunction summaries express a function s behavior in terms of its input/output interface, abstracting \naway its internal details. We show experimentally that our function summaries do not grow with program \nsize; thus, an implemen\u00adtation strategy that analyzes a single function at a time, requir\u00ading only one \nfunction and its callee s summaries to be in mem\u00adory, should scale to arbitrarily large programs.  Parallelizability: \nIn modular analysis, any two functions that do not have a caller/callee relationship can be analyzed \nin paral\u00adlel. Thus, such analyses naturally exploit multi-core machines.   Figure 1. Summary associated \nwith function f To illustrate our approach, consider the following simple func\u00adtion f along with its \nthree callers g1, g2, and g3: void f(int** a, int** b, int* p, int* q) { *a = p; *b = q; **a = 3; **b \n= 4; } void g1() { void g2() { void g3() { int** a, int** b; int** a, int** b; int** a, int** b; a = \nnew int*; a = new int*; a = new int*; b=newint*; b=newint*; b=a; intp=0,q=0; intp=0; intp=0,q=0; f(a, \nb, &#38;p, &#38;q); f(a, b, &#38;p, &#38;p); f(a, b, &#38;p, &#38;q); assert(p == 3); } assert(p == 4); \n} assert(p == 0); } Here, although the body of f is conditional-and loop-free, the value of *p after \nthe call to f may be either 3, 4, or remain its initial value. In particular, in contexts where p and \nq are aliases (e.g., g2), *p is set to 4; in contexts where neither a and b nor p and q are aliases (e.g., \ng1), *p is set to 3, and in contexts where a and b are aliases but p and q are not (e.g., g3), the value \nof *p is unchanged after a call to f. Furthermore, to discharge the assertions in g1, g2, and g3, we \nneed to perform strong updates to all the memory locations. To give the reader a .avor of our technique, \nthe function sum\u00admary of f computed by our analysis is shown in Figure 1, which shows the points-to graph \non exit from f (i.e., the heap when f returns). Here, points-to edges between locations are quali.ed \nby constraints, indicating the condition under which this points-to re\u00adlation holds. The meaning of a \nconstraint such as *p = *q is that the location pointed to by p and the location pointed to by q are \nthe same, i.e., p and q are aliases. Observe that Figure 1 encodes all possible updates to *p precisely: \nIn particular, this summary indi\u00adcates that *p has value 3 under constraint *a = *b .*p= *q (i.e., neither \na and b nor p and q are aliases); *p has value 4 if p and q are aliases, and *p retains its initial value \n(**p) otherwise. There are three main insights underlying our approach: First, we observe that a heap \nabstraction H at any call site of f can be overapproximated as the .nite union of some structurally distinct \nskeletal points-to graphs H 1,... H m where each ab\u00adstract location points-to at most one location. This \nobservation yields a naive, but sound, way of performing summary-based analysis where the heap state \nafter a call to function f is condi\u00adtioned upon the skeletal graph at the call site.  Second, we symbolically \nencode all possible skeletal heaps on entry to f in a single symbolic heap where points-to edges are \nquali.ed by constraints. This insight allows us to obtain a single, polymorphic heap summary valid at \nany call site.  Third, we observe that using summaries to apply strong updates at call sites requires \na negation operation on constraints. Since these constraints may be approximations, simultaneous reason\u00ading \nabout may and must information on points-to relations is necessary for applying strong updates when safe. \nTo solve this dif.culty, we use bracketing constraints [4].  The .rst insight, developed in Section \n2, forms the basic frame\u00adwork for reasoning about the correctness and precision of our ap\u00adproach. The \nsecond and third insights, exploited in Section 4, yield a symbolic and ef.cient encoding of the basic \napproach. To sum\u00admarize, this paper makes the following contributions: We develop a theory of abstract \nheap decompositions that elu\u00adcidates the basic principle underlying modular heap analyses. This theory \nshows that a summary-based analysis must lose ex\u00adtra precision over a non-summary based analysis in some \ncir\u00adcumstances and also sheds light on the correctness of earlier work on modular alias analyses, such \nas [5 7].  We present a full algorithm for performing modular heap analy\u00adsis in a symbolic and ef.cient \nway. While our algorithm builds on the work of [7] in predicating summaries on aliasing pat\u00adterns, our \napproach is much more precise and is capable of per\u00adforming strong updates to heap locations at call \nsites.  We demonstrate experimentally that our approach is both scal\u00adable and precise for verifying \nproperties about real C and C++ applications up to 100,000 lines of code.  2. Foundations of Modular \nHeap Analysis As mentioned in Section 1, our goal is to analyze a function f independently of its callers \nand generate a summary valid in any context. The main dif.culty for such an analysis is that f s heap \nfragment (the portion of the program s heap reachable through f s arguments and global variables on entry \nto f) is unknown and may be arbitrarily complex, but a modular analysis must model this unknown heap \nfragment in a conservative way. Our technique models f s heap fragment using abstractions H1,...,Hk such \nthat (i) in each Hi, every location points to ex\u00adactly one location variable representing the unknown \npoints-to tar\u00adgets of that location on function entry, (ii) each Hi represents a distinct aliasing pattern \nthat may arise in some calling context, and (iii) the heap fragment reachable in f at any call site is \noverapprox\u00adimated by combining a subset of the heaps in H1,...,Hk. As the above discussion illustrates, \nour approach requires rep\u00adresenting the heap abstraction at any call site as the .nite union of heap \nabstractions where each pointer location has exactly one tar\u00adget. We observe that every known modular \nheap analysis, including ours, has this this one-target property. In principle, one could allow the unknown \nlocations in a function s initial heap fragment to point to 2, 3, or any number of other unknown heap \nlocations, but it is unclear how to pick the number or take advantage of the potential extra precision. \nIn this section, we present canonical decompositions, through which the heap is decomposed into a set \nof heaps with the one\u00adtarget property, and structural decompositions, which coalesce iso\u00admorphic canonical \nheaps. We then show how these decompositions can be used for summary-based heap analysis. 2.1 Preliminaries \nWe describe the basic ideas on a standard may points-to graph, which we usually call a heap for brevity. \nA labeled node A rep\u00adresents one or more concrete memory locations .(A). DEFINITION 1. (Summary Location) \nAn abstract location that may represent multiple concrete locations is a summary location (e.g., modeling \nelements in an array/list). An abstract location rep\u00adresenting exactly one concrete location is a non-summary \nlocation. For any two distinct abstract locations A and AI, we require .(A) n .(AI)= \u00d8, and that |.(A)| \n=1 if A is a non-summary node. An edge (A, B) in the points-to graph denotes a partial function .(A,B) \nfrom pointer locations in .(A) to locations in Figure 2. A heap H and its canonical decomposition H1,...,H4 \n  .(B), with the requirement that for every pointer location l . .(A) there is exactly one node B such \nthat .(A,B)(l) is de.ned (i.e., each pointer location has a unique target in a concrete heap). Finally, \neach possible choice of . and compatible edge functions .(A,B) for each edge (A, B) maps a points-to \ngraph H to one concrete heap. We write .(H) for the set of all such possible concrete heaps for the points-to \ngraph H. We also write H1 ; H2 if .(H1) . .(H2), and H1 U H2 for the heap that is the union of all nodes \nand edges in H1 and H2. We de.ne a semantic judgment H |= S : HI as: I III H |= S : H..h . .(H). .h. \n.(H). eval(h, S)= h where eval(h, S) is the result of executing code fragment S starting with concrete \nheap h. Now, we write H fa S : HI to indicate that, given a points-to graph H and a program fragment \nS, HI is the new heap obtained after analyzing S using pointer analysis a. The pointer analysis a is \nsound if for all program fragments S: H fa S : HI . H |= S : HI  2.2 Canonical Decomposition In this \nsection, we describe how to express a points-to graph H as the union of a set of points-to graphs H1,...,Hk \nwhere in each Hi, every abstract location points to at most one location. DEFINITION 2. (Canonical points-to \ngraph) We say a points-to graph is canonical if every abstract memory location has an edge to at most \none abstract memory location. DEFINITION 3. (Canonical decomposition) The canonical de\u00adcomposition of \nheap H is obtained by applying these steps in order: 1. If a summary node A points to multiple locations \nT1,...,Tk, replace T1,...,Tk with a single summary node T such that any edge to/from any Ti is replaced \nwith an edge to/from T . 2. Let B be a location with multiple edges to T1,...,Tk. Split the heap into \nH1,...,Hk where in each Hi, B has exactly one edge to Ti, and recursively apply this rule to each Hi. \n LEMMA 1. Let H1,...,Hk be the canonical decomposition of H. Then (H1 U ... U Hk) ; H. PROOF 1. Let \nHI be the heap obtained from step 1 of De.ni\u00adtion 3. To show HI ; H we must show .(HI) . .(H). Let h \n. .(H) and let .H by the corresponding mapping. We choose .HI (T )= .H (T1) . ... . .H (Tk) and .HI (X)= \n.H (X) other- I wise, and construct the edge mappings .(HA,B) from .(HA,B) analo\u00adgously. Thus, h . .(HI) \nand we have .(HI) . .(H). In step 2, observe that any location B with multiple edges to T1,...,Tk must \nbe a non-summary location. Hence, the only concrete location rep\u00adresented by B must point to exactly \none Ti in any execution. Thus, in this step, (H1 U ... U Hk)= HI ; H. D EXAMPLE 1. Figure 2 shows a heap \nH with only non-summary locations. The canonical decomposition of H is H1,H2,H3,H4, representing four \ndifferent concrete heaps encoded by H. Figure 3. A heap H and its canonical decomposition H1 Figure \n4. Two isomorphic canonical heaps and their skeleton EXAMPLE 2. Figure 3 shows another heap H with summary \nnode A (indicated by double circles) and its canonical decomposition H1. Heap H1 is obtained from H by \ncollapsing locations C and D into a summary location CD. Observe that we cannot split H into two heaps \nH1 and H2 where A points to C in H1 and to D in H2: Such a decomposition would incorrectly state that \nall elements in A must point to the same location, whereas H allows distinct concrete elements in A to \npoint to distinct locations. COROLLARY 1. If H has no summary nodes with multiple edges, then its canonical \ndecomposition is exact, i.e., Hi = H. 1=i=k PROOF 2. This follows immediately from the proof of Lemma \n1. D LEMMA 2. Consider a sound pointer analysis a and a heap H with canonical decomposition H1,...,Hk \nsuch that: H1 fa S : H1 I ... Hk fa S : Hk I Then, H |= S : H1 I U ... U HkI . PROOF 3. This follows \ndirectly from Lemma 1. D According to this lemma, we can conservatively analyze a pro\u00adgram fragment S \nby .rst decomposing a heap H into canonical heaps H1,...,Hk, then analyzing S using each initial heap \nHi, and .nally combining the resulting heaps H1I ,...,HkI . Recall that in a modular heap analysis, we \nrequire each node in a function f s initial heap abstraction to have the single-target property. Corollary \n1 implies that if a call site of f has no summary nodes with multiple targets, then this assumption results \nin no loss of information, because we can use multiple distinct heaps for f that, in combination, are \nan exact representation of the call site s heap. However, if a summary location has multiple targets \nand there is aliasing involving that summary node, as illustrated in Figure 3, the modular analysis may \nstrictly overapproximate the heap after a call to f. In this case, the requirement that f s initial heap \nhave the single-target property means that f can only represent the call\u00adsite s heap (shown on the left \nof Figure 3) by an overapproximating heap that merges the target nodes (shown on the right of Figure \n3).  2.3 Structural Decomposition Consider the result of analyzing a program fragment S starting with \ninitial canonical heaps H1 and H2 shown in Figure 4. Here, nodes labeled X and Y represent memory locations \nof x and y, which are the only variables in scope in S. Since the only difference between H1 and H2 is \nthe label of the node pointed to by x and y, the heaps H1 I and H2 I obtained after analyzing S will \nbe identical except  Figure 5. Structural decomposition of heap H from Figure 2 for the label of a \nsingle node. Thus, S can be analyzed only once starting with heap H in Figure 4, and H1 I and H2 I can \nbe obtained from the resulting heap by renaming . to loc1 and loc2 respectively. The rest of this section \nmakes this discussion precise. DEFINITION 4. (Skeleton) Given a set of nodes N, let .N (H) be the heap \nobtained by erasing the labels of all nodes in H except for those in N. Now .N de.nes an equivalence \nrelation H =N HI if .N (H)= .N (HI). We select one heap in each equivalence class of =N as the class \nunique skeleton. Note that nodes of skeletons are labeled label erasure is only used to determine equivalence \nclass membership. EXAMPLE 3. In Figure 4, H1 and H2 have the same skeleton H . In other words, if heaps \nH1,...,Hk have the same aliasing patterns with respect to a set of root locations N, then H is a unique \npoints-to graph which represents their common aliasing structure. Skeletons are useful because, if N \nrepresents formals and globals in a function f, all possible aliasing patterns at call sites of f can \nbe expressed using a .nite number of skeletons. DEFINITION 5. (.) Let H be a heap and let H be its skeleton \nw.r.t. nodes N. The mapping .H, maps every node label in H to the H label of the corresponding node in \nH and any other node to itself. DEFINITION 6. (Structural Decomposition) Given heap H and nodes N, the \nstructural decomposition of H w.r.t. N is a set of heaps D such that for every Hi in the canonical decomposition \nof H, the sketelon H of Hi w.r.t. N is in D. Observe that the cardinality of the structural decomposition \nof H is never larger than the cardinality of H s canonical decomposition. DEFINITION 7. (Instances of \nskeleton) Let H be a skeleton in the structural decomposition of H. The instances of H , written IH (H \n), are the canonical heaps of H with skeleton H . EXAMPLE 4. Consider heap H from Figure 2 and the root \nset {A, B}. The structural decomposition H1,H2 of H is shown in Figure 5. Observe that canonical heaps \nH1 and H4 from Figure 2 have the same skeleton H 1, and H2 and H3 have skeleton H 2. Thus, IH (H 1)= \n{H1,H4} and IH (H 2)= {H2,H3}. Also: . =[.1 . C, .2 . D]. =[.1 . D, .2, . C] H1,H1 H4,H1 .H2,H2 .H3,H2 \n =[. . D] =[. . C] LEMMA 3. Consider program fragment S and nodes N represent\u00ading variables in scope \nat S. Let HN be the heap fragment reach\u00adable through N before analyzing S and let H 1,..., H m be the \nstructural decomposition of HN w.r.t. N. If H I H I H1 fa S : 1 ... Hm fa S : m HI and if N is the heap \nde.ned as: H I = (H iI) N 1=i=mHij.IHN (H i) .Hij,Hi H I then HN |= S : N . PROOF 4. First, by De.nitions \n4 and 2, we have: 1=i=m Hij .IHN ( Hi) Hij ; HN Second, using Lemma 2, this implies:  HI HN |= S : \n1=i=mHij .IHN (H i) ij(*) where Hij fa ij . From De.nition 7, since Hij and S : HI Hi are equivalent \nup to renaming, then HI Hi I are also equivalent up ij and to this renaming, given by . . Together with \n(*), this implies Hij ,Hi HI HN |= S : 1=i=mHij .IHN (H i) .Hij ,H i ( i). D In other words, the heap \nde.ned as H I in Lemma 3 gives us a N sound abstraction of the heap after analyzing program fragment \nS. Furthermore, N is precise in the sense de.ned below: HI LEMMA 4. Let H I be the heap de.ned in Lemma \n3, and let N H1,...,Hk be the canonical decomposition of the heap fragment reachable from N before analyzing \nS. If Hj fa Hj I , then: H I HI = N 1=j=kj PROOF 5. This follows from Corollary 1 and De.nition 6. D \nThe following corollary states a stronger precision result: H I COROLLARY 2. Let HN and N be the heap \nabstractions from HI Lemma 3, and let HN fa S : N . If HN does not contain summary locations with multiple \npoints-to targets, then II HN . HN PROOF 6. This follows from Lemma 4 and Corollary 1.  2.4 From Decompositions \nto Modular Heap Analysis We now show how the ideas described so far yield a basic modular heap analysis. \nIn the remainder of this section, we assume there is a .xed bound on the total number of memory locations \nused by a program analysis. (In practice, this is achieved by, e.g., collapsing recursive .elds of data \nstructures to a single summary location.) LEMMA 5. Consider a function f, and let N denote the abstract \nmemory locations associated with the formals and globals of f. Then, there is a .nite set Q of skeletons \nsuch that the structural decomposition D w.r.t. N of the heap fragment reachable from N in any of f s \ncall sites satis.es D . Q. PROOF 7. Recall that in any canonical heap, every location has exactly one \ntarget. Second, observe that when there is bound b on the total number of locations in any heap, any \ncanonical heap must have at most b locations. Thus, using a .xed set of nodes, we can only construct \na .nite set Q of structurally distinct graphs. D Since there are a bounded number of skeletons that can \narise in any context, this suggests the following strategy for computing a complete summary of function \nf: Let N be the set of root locations (i.e., formals and globals) on entry to f, and let H 1,..., H k \nbe the set of all skeletons that can be constructed from root set N. We analyze f s body for each initial \nskeleton H i, obtaining a new heap H I i. Now, let C be a call site of f and let R be the subset of the \nskeletons H 1,..., H k that occur in the structural decomposition of  heap H in context C. Then, following \nLemma 3, the heap fragment after the call to f can be obtained as: . (H iI) Hi.RHij .IH (H i)) Hij ,Hi \nThis strategy yields a fully context-sensitive analysis because f s body is analyzed for any possible \nentry aliasing pattern H i, and at a given call site C, we only use the resulting heap H i if H i is \npart of the structural decomposition of the heap at C. Furthermore, as indicated by Corollary 2, this \nstrategy is as precise as analyzing the inlined body of the function if there are no summary locations \nwith multiple points-to targets at this call site; otherwise, the precision guarantee is stated in Lemma \n4.  2.5 Discussion While the decompositions described here are useful for understand\u00ading the principle \nunderlying modular heap analyses, the naive algo\u00adrithm sketched in Section 2.4 is completely impractical \nfor two rea\u00adsons: First, since the number of skeletons may be exponential in the number of abstract locations \nreachable through arguments, such an algorithm requires analyzing a function body exponentially many \ntimes. Second, although two initial skeletons may be different, the resulting heaps after analyzing the \nfunction body may still be iden\u00adtical. In the rest of this paper, we describe a symbolic encoding of \nthe basic algorithm that does not analyze a function more than once unless cycles are present in the \ncallgraph (see Section 4). Then, in Section 5, we show how to identify only those initial skeletons that \nmay affect the heap abstraction after the function call. 3. Language To formalize our symbolic algorithm \nfor modular heap analysis, we use the following typed, call-by-value imperative language: Program P := \nF + Function F := de.ne f (a1 : t1,...,an : tn)= S; Statement S := v1 .*v2 |* v1 . v2 | v . alloc.(t \n) | f.(v1,...,vk) | assert(v1 = v2) | let. v : t in S end | S1; S2 | choose(S1,S2) Type t := int | ptr(t \n) A program P consists of one or more (possibly recursive) func\u00adtions F . Statements in this language \nare loads, stores, memory al\u00adlocations, function calls, assertions, let bindings, sequencing, and the \nchoose statement, which non-deterministically executes either S1 or S2 (i.e., a simpli.ed conditional). \nAllocations, function calls, and let bindings are labeled with globally unique program points .. Since \nthis language is standard, we omit its operational seman\u00adtics and highlight a few important assumptions: \nExecution starts at the .rst function de.ned, and an assertion failure aborts execution. Also, all bindings \nin the concrete store have initial value nil. 4. Modular &#38; Symbolic Heap Analysis In this section, \nwe formally describe our symbolic algorithm for modular heap analysis. In Section 4.1, we .rst decribe \nthe abstract domain used in our analysis. Section 4.2 formally de.nes function summaries, and Section \n4.3 presents a full algorithm for summary\u00adbased heap analysis for the language de.ned in Section 3. \n4.1 Abstract Domain Abstract locations p represent a set of concrete locations: Abstract Locations p \n:= a | l Location Variables a := .i |* a Location Constants l := loc.P| nil An abstract location p in \nfunction f is either a location variable a or a location constant l. Location constants represent stack \nor heap allocations in f and its transitive callees as well as nil. In con\u00adtrast, location variables \nrepresent the unknown memory locations reachable from f s arguments at call sites, similar to access \npaths in [5]. Informally, location variables correspond to the node labels of a skeleton from Section \n2.3. Recall from Section 2 that, in any canonical points-to graph, every abstract memory location points \nto at most one other abstract memory location; hence, location vari\u00adable *.i describes the unknown, but \nunique, points-to target of f s i th argument in some canonical heap at a call site of f. Abstract environment \nE maps program variables v to abstract locations p, and abstract store S maps each abstract location \np to an abstract value set . of (abstract location, constraint) pairs: (p,f) Abstract value set . := \n2 The abstract store de.nes the edges of the points-to graph from Section 2. A mapping from abstract \nlocation p to abstract value set {(p1,f1),..., (pk,fk)} in S indicates that the heap abstraction contains \na points-to edge from node labeled p to nodes labeled p1,...,pk. Observe that, unlike the simple may \npoints-to graph we considered in Section 2, points-to edges in the abstract store are quali.ed by constraints, \nwhich we utilize to symbolically encode all possible skeletons in one symbolic heap (see Section 4.3). \nConstraints in our abstract domain are de.ned as follows: f := (.may,.must) . := T | F | .1 . .2 | .1 \n. .2 |\u00ac. | t1 = t2 Here, f is a bracketing constraint (.may,.must) as in [4], represent\u00ading the condition \nunder which a property may and must hold. Recall from Section 1 that the simultaneous use of may and \nmust informa\u00adtion is necessary for applying strong updates whenever safe. In par\u00adticular, updates to \nheap locations require negation (see Section 4.3). Since the negation of an overapproximation is an underapproxima\u00adtion, \nthe use of bracketing constraints allows a sound negation op\u00aderation, de.ned as \u00ac(.may,.must) = (\u00ac.must, \n\u00ac.may). Conjunction and disjunction are de.ned on these constraints as expected: II II (.may,.must) * \n(.may,.must) = (.may *.may,.must *.must) where * . {., .}. In this paper, any constraint f is a bracketing \nconstraint unless stated otherwise. To make this clear, any time we do not use a bracketing constraint, \nwe use the letter . instead of f. Furthermore, if the may and must conditions of a bracketing constraint \nare the same, we write a single constraint instead of a pair. Finally, for a bracketing constraint f \n= (.may,.must), we de.ne ifl = .may and LfJ = .must. In the de.nition of constraint ., T and F represent \nthe boolean constants true and false, and a term t is de.ned as: Term t := v | drf(t) | alloc(.p) | nil \nHere, v represents a variable, drf is an uninterpreted function, and alloc is an invertible uninterpreted \nfunction applied to a vector of constants .p. Thus, constraints . belong to the theory of equality with \nuninterpreted functions. Our analysis requires converting be\u00adtween abstract locations and terms in the \nconstraint language; we therefore de.ne a lift operation, written p, for this purpose: . .i = .i *a = \ndrf(a) nil = nil locP= alloc(.p) Observe that a location constant loc. is converted to a term alloc(.p), \nwhich effectively behaves as a constant in the constraint language: Since alloc is an invertible function, \nalloc(.p)= alloc(.pI) exactly when . = .I. A location variable . is converted to a constraint vari\u00adable \nof the same name, and the location variable *. is represented by the term drf(.) which represents the \nunknown points-to target  Figure 6. A symbolic heap representing two skeletons Figure 7. The abstract \nstore in f s summary of . on function entry. We write lift-1(t) to denote the conversion of a term to \nan abstract location. EXAMPLE 5. In Figure 6, a symbolic heap H represents two skele\u00adtons H 1 and H 2. \nIn H, the constraint drf(.1)= drf(.2) describes contexts where the .rst and second arguments are aliases. \nOb\u00adserve that, at call sites where the .rst and second arguments alias, drf(.1)= drf(.2) instantiates \nto true and drf(.1)= drf(.2) is false; thus at this call site, H instantiates exactly to H 2. Similarly, \nif the .rst and second arguments do not alias, H instantiates to H 1.  4.2 Function Summaries A summary \n. for a function f is a pair .= (f, S) where f is a constraint describing the precondition for f to succeed \n(i.e., not abort), and S is a symbolic heap representing the heap abstraction after f returns. More speci.cally, \nlet H 1,..., H k be the set of all skeletons for any possible call site of f, and let H i f S : H i I \nwhere S is the body of f. Then, the abstract store Ssymbolically encodes that in contexts where the constraints \nin S are satis.ed by initial heap H i, the resulting heap is H iI . Observe that a summary can also be \nviewed as the Hoare triple {f} f {S}. Thus, the computation of a summary for f is equivalent to the inference \nof sound pre-and post-conditions for f. EXAMPLE 6. Consider the function: define f(a1 : ptr(ptr(int)), \na2 : ptr(ptr(int))) = 1 : *a1 . alloc1 (int); 2 : *a2 . alloc2 (int); 3 : let t1 : ptr(int) in t1 . *a1 \n; *t1 . 7 end; 4 : let t2 : ptr(int) in t2 . *a2 ; *t2 . 8 end; 5 : let t3 : ptr(int) in t3 . *a1 ; 6 \n: let t4 : int in t4 . *t3 ; asse rt(t4 == 7) end; 7 : end The summary for f is ( drf(.1)= drf(.2), \nS ) where S is shown in Figure 7. The pre-condition drf(.1)= drf(.2) indicates that the assertion fails \nin those contexts where arguments of f are aliases. Also, in symbolic heap S, the abstract location reached \nby deref\u00aderencing a1 (whose location is .1) is either loc1, corresponding to the allocation at line 1, \nor loc2, associated with the allocation at line 2, depending on whether a1 and a2 are aliases. A global \nsummary environment G is a mapping from each function f in the program to a summary .f. E =[a1 . .1,...,ak \n. .k] . ai . dom(A).  S(ai) ..k=i(*ak, ( j<k *ai = *aj ) .*ai = *ak) A f init heap(a1,...,ak): E, S \nFigure 8. Local Heap Initialization  4.3 The Analysis We now present the full algorithm for the language \nof Section 3. Section 4.3.1 describes the symbolic initialization of the local heap to account for all \npossible aliasing relations on function entry. Sec\u00adtion 4.3.2 gives abstract transformers for all statements \nexcept func\u00adtion calls, which is described in Section 4.3.3. Finally, Section 4.3.4 describes the generation \nof function summaries. 4.3.1 Local Heap Initialization To analyze a function f independent of its callers, \nwe initialize f s abstract store to account for all possible relevant aliasing relation\u00adships at function \nentry. To perform this local heap initialization, we utilize an alias partition environment Awith the \nsignature a . 2a . This environment maps each location variable a to an ordered set of location variables, \ncalled a s alias partition set. If aI . A(a), then f s summary may differ in contexts where a and aI \nare aliases. Since aliasing is a symmetric property, any alias partition environ\u00adment A has the property \naI . A(a) . a . A(aI). Any location aliases itself, and so A is also re.exive: a . A(a). A correct alias \npartition environment A can be trivially computed by stipulating that aI . A(a) if a and aI have the \nsame type. We discuss how to compute a more precise alias partition environment Ain Section 5. A key \ncomponent of the modular analysis is the init heap rule in Figure 8. Given formal parameters a1,...,ak \nto function f, this rule initializes the abstract environment and store on entry to f. The environment \nE is initialized by binding a location variable .i to each argument ai. The initialization of the abstract \nstore S, however, is more involved because we need to account for all possible entry aliasing relationships \npermitted by A. Intuitively, if A indicates that a1 and a2 may alias on function entry, we need to analyze \nf s body with two skeletal heaps, one where a1 and a2 point to the same location, and one where a1 and \na2 point to different locations. To encode this symbolically, one obvious solution is to introduce three \nlocation variables, *a1, *a2, and *a12 such that a1 (resp. a2) points to *a1 (resp. *a2) if they do not \nalias (i.e., under constraint drf(a1)= drf(a2)) and point to a common location named *a12 if they alias \n(i.e., under drf(a1)= drf(a2)). While this encoding correctly describes both skeletal heaps, it unfortunately \nintroduces an exponential number of locations, one for each subset of entry alias relations in A. To \navoid this exponential blow-up, we impose a total order on abstract locations such that if ai and aj \nare aliases, they both point to a common location *ak such that ak is the least element in the alias \npartition class of ai and aj . Thus, in the init heap rule of Figure 8, ai points to *ak where k = i \nunder constraint: (*ai = *aj ) .*ai = *ak j<k This condition ensures that ai points to a location named \n*ak only if it does not alias any other location aj . A(ai) with j<k. EXAMPLE 7. Consider the function \nde.ned in Example 6. Suppose the alias partition environment Acontains the following mappings: .1 .{.1,.2},.2 \n.{.1,.2}, *.1 . {*.1}, *.2 . {*.2}, ** .1 . {** .1}, ** .2 . {** .2}  Figure 9. The initial heap abstraction \nfor function from Example 6 Figure 9 shows the initial heap abstraction using Aand the order\u00ading .1 <.2. \nSince Aincludes .1 and .2 in the same alias partition set, .2 points to *.1 under drf(.1)= drf(.2) and \nto *.2 under its negation. But .1 only points to *.1 since .2 <.1. The following lemma states that the \ninitial heap abstraction correctly accounts for all entry aliasing relations permitted by A: LEMMA 6. \nLet ai and aj be two abstract locations such that aj . A(ai). The initial local heap abstraction S constructed \nin Figure 8 encodes that ai and aj point to distinct locations exactly in those contexts where they do \nnot alias. PROOF 8. Without loss of generality, assume i<j. . Suppose ai and aj are not aliases in a \ncontext C, but Sencodes they may point to the same location *ak in context C. Let f and fI be the constraints \nunder which ai and aj point to ak respectively. By construction, k = i, and f implies drf(ai)= drf(ak) \nand fI implies drf(aj )= drf(ak). Thus, we have drf(ai)= drf(aj ), contradicting the fact that ai and \naj do not alias in C. . Suppose ai and aj are aliases in context C, but S allows ai and aj to point \nto distinct locations *ak and *am. Let f and fI be the constraints under which ai points to *ak and aj \npoints to *am respectively. Case (i): Suppose k<m. Then, by construction, f implies drf(ai)= drf(ak), \nand fI implies drf(aj )= drf(ak). Hence, we have drf(aj )= drf(ai), contradicting the assumption that \nai and aj are aliases in C. Case (ii): k>m. Then, fI implies drf(aj )= drf(am), and f implies drf(ai)= \ndrf(am), again contradicting the fact that ai and aj are aliases in C. D  LEMMA 7. For each alias partition \nset of size n, the init heap rule adds n(n + 1)/2 points-to edges. As Lemma 7 states, this construction \nintroduces a quadratic number of edges in the size of each alias partition set to represent all possible \nskeletal heaps. Furthermore, the number of abstract locations in the initial symbolic heap is no larger \nthan the maximum number of abstract locations in any individual skeleton.  4.3.2 Abstract Transformers \nfor Basic Statements In this section, we describe the abstract transformers for all state\u00adments except \nfunction calls, which is the topic of Section 4.3.3. Statement transformers are given as inference rules \nof the form E, S, G,f f S : SI,fI which states that under abstract environment E, store S, summary environment \nG, and precondition f, statement S produces a new abstract store SI and a new precondition fI of the \ncurrent function. The operation S(.) looks up the value of each pi in .:  S({(p1,f1),..., (pk,fk)})= \nS(pi) . fi 1=i=k where S(pi) . fi is a shorthand de.ned as follows: . . f = {(pj ,fj . f)|(pj ,fj ) . \n.} E(v1)= p1 E(v2)= p2 S(p2)= . SI = S[p1 . S(.)](1) E, S, G,f f v1 .*v2 : SI,f E(v1)= p1 E(v2)= p2 S(p1)= \n.1 S(p2)= .2 SI = S[pi . ((.2 . fi) . (S(pi) .\u00acfi)) | (pi,fi) . .1] (2) E, S, G,f f*v1 . v2 : SI,f E(v)= \np SI = S[p .{(loc.,T )}, loc. .{(nil,T )}](3) E, S, G,f f v . alloc.(t): SI,f E(v1)= p E(v2)= pI S(p)= \n{..., (pi,fi),...} S(pI))= {..., (pj I ,fI j ),...} fI =(pi = pI . fi . fI ) i,jj j (4) E, S, G,f f \nassert(v1 = v2): S,f . fI EI = E[v . loc.] SI = S[*loc. .{(nil,T )}] EI , SI , G,f f S : SII,fI (5) E, \nS, G,f f let. v : t in S end : SII\\loc.,fI E, S, G,f f S1 : SI,fI E, SI , G,fI f S2 : SII,fII (6) E, \nS, G,f f S1; S2 : SII,fII E, S, G,f f S1 : S1,f1 E, S, G,f f S2 : S2,f2 (7) E, S, G,f f choose (S1,S2): \nS1 S2,f1 . f2 Figure 10. Abstract Transformers for Basic Statements In Figure 10, rules (1) and (2) \ngive the transformers for loads and stores respectively. The rule for loads is self-explanatory; thus, \nwe focus on the store rule. In the third hypothesis of rule (2), each pi represents a location that v1 \npoints to under constraint fi, and .2 is the value set for v2. Since the write to pi happens under constraint \nfi, the new value of pi in SI is .2 under constraint fi and retains its old value set S(pi) under \u00acfi. \nObserve that if fi is true, this rule performs a standard strong update to pi. On the other hand, if \nv1 points to pi under some entry alias assumption, then there is a strong update to pi exactly in those \ncalling contexts where this alias assumption holds. EXAMPLE 8. Figure 11 shows the relevant portion of \nthe heap abstraction before and after the store at line 2 in Example 6. Rule (3) processes allocations \nby introducing a new location loc. and initializing its value in the store to nil. Rule (4) analyzes \nan assertion by computing the condition fI for the assertion to hold such that if fI can be proven valid \nin a calling context, then this assertion must hold at that call site. In rule (4), fI is computed as \n S, I f map loc(*. : t): II S, I f map loc(. : int): IS, I f map loc(. : ptr(t)) : II II = I[*a . S(I(a))] \nII = I[*a . S(I(a))] II f map loc(** a : t): III S, I f map loc(*a : int): II S, I f map loc(*a : ptr(t)) \n: III S, [.1 .{(E(v1),T )}] f map loc(.1): I1 ... S, Ik-1[.k .{(E(vk),T )}] f map loc(.k): Ik E, S f \nmap args(v1 : t1,...,vk : tk): Ik Figure 12. Rules for computing instantiation environment I .I . = {(loc.:: \nP,T )} .PI (. . .kI) I,. f inst loc(nil): {(nil,T )} I,. f inst loc(loc): . .PI . = {(loc , (T,F ))} \n.PI (. . .kI) I,. f inst loc(a): I(a) I,. f inst loc(loc): . Figure 13. Rules for instantiating locations \nthe disjunction of all pairwise equalities of the elements in the two abstract value sets associated \nwith v1 and v2, i.e., a case analysis of their possible values. Rule (5) describes the abstract semantics \nof let statements by binding variable v to a new location loc. in E. Rule (6) for sequencing is standard, \nand rule (7) gives the semantics of the choose construct, which computes the join of two abstract stores \nS1 and S2. To de.ne a join operation on abstract stores, we .rst de.ne domain extension: DEFINITION 8. \n(Domain Extension) Let p be any binding in ab\u00adstract store SI and let (pi,fi) be any element of SI(p). \nWe say an abstract store SII = S .SI is a domain extention of S with respect to SI if the following condition \nholds: 1. If p . dom(S) . (pi,fI i) . S(p), then (pi,fI i) . S .SI (p). 2. Otherwise, (pi, false) . \nS .SI (p)  DEFINITION 9. (Join) Let S1 I = S1 .S2 and let S2 I = S2 .S1 . If (pI , (.1 ,.11(p) and (pI \n, (.2 ,.22(p), then: maymust)) . SI maymust)) . SI I 121 2 (p, (.may . .may,.must)) . (S1 U S2)(p)must \n. .  4.3.3 Instantiation of Summaries The most involved statement transformer is the one for function \ncalls, which we describe in this subsection. Figure 15 gives the complete transformer for function calls, \nmaking use of the helper rules de.ned in Figures 12-14, which we discuss in order. Given the actuals \nv1,...,vk for a call to function f, Figure 12 computes the instantiation environment I with signature \na . . for this call site. This environment I, which serves as the symbolic equivalent of the mapping \n. from Section 2, maps location vari\u00adables used in f to their corresponding locations in the current \n(call\u00ading) function. However, since Iis symbolic, it produces an abstract value set {(p1,f1),..., (pk,fk)} \nfor each a such that a instanti\u00adates to pi in some canonical heap under constraint fi. Figure 13 describes \nthe rules for instantiating any location p used in the summary. If p is a location variable, we use environment \nI to look up its instantiation. On the other hand, if p is a location constant allocated in callee f \n, we need to rename this constant to distinguish allocations made at different call sites for full context\u00ad \nsensitivity. In general, we rename the location constant loc.PI by prepending to .pI the program point \n. associated with the call site. I,. f inst loc(lift-1(t1)) : {(p1,f1),..., (pk,fk)} I,. f inst loc(lift-1(t2)) \n: {(pI ,fI ),..., (pI ,fI )} ))11mm f =(k = pi . fi) fI =(kI = pI . fI )(k, kI fresh) ijjj I,. f inst.(t1 \n= t2): k = kI,f . fI b .{T,F } I,. f inst.(.): .1,f2 I,. f inst.(b): b, T I,. f inst.(\u00ac.): \u00ac.1,f2 I,. \nf inst.(.1): ., f I,. f inst.(.2): .I,fI (* . {., .}) I,. f inst.(.1 *.2): .*.I,f . fI I,. f inst.(.may): \n.I ,fI I,. f inst.(.must): .I maymay must,fmust I .II .II  may = iQE(.kk. (.may I . fmayI ))l must \n= lQE(.kk. (.must I . fmustI ))J I,. f instf((.may,.must)): (fII ,fII maymust) Figure 14. Rules for instantiating \nconstraints However, in the presence of recursion, we need to avoid creating an unbounded number of location \nconstants; thus, in Figure 13, we check if this allocation is created on a cyclic path in the callgraph \nby testing whether the current program point . is already in .pI. In the latter case, we do not create \na new location constant but weaken the bracketing constraint associated with loc.PI to (T,F ), which \nhas the effect of ensuring that stores into this location only apply weak updates [4], meaning that loc.PI \nbehaves as a summary location. In addition to instantiating locations, we must also instanti\u00adate the \nassociated constraints, which is described in Figure 14. In the last rule of this .gure, instf instantiates \na bracketing con\u00adstraint, making use of inst. to map the constituent may and must conditions. The inst. \nrule derives judgments of the form I,. f inst.(.): .I,f, where .I preserves the structure of . by substi\u00adtuting \neach term t in . with a temporary variable k and f constrains the values of k. The .rst rule in Figure \n14 for instantiating a leaf t1 = t2 is the most interesting one: Here, we convert t1 and t2 to their \ncor\u00adresponding memory locations using the lift-1 operation from Sec\u00adtion 4.1 and instantiate the corresponding \nlocations using inst loc to obtain abstract value sets .1 and .2. We then introduce two tem\u00adporary variables \nk and kI representing .1 and .2 respectively, and introduce constraints f and fI, stipulating the equality \nbetween k and .1 and between kI and .2. Observe that in the last rule of Fig\u00adure 14, these temporary \nvariables k and kI are removed using a QE procedure to eliminate existentially quanti.ed variables. Figure \n15 makes use of all the afore-mentioned rules to instan\u00adtiate the summary of function f at a given call \nsite .. In the last rule of this .gure, we .rst look up f s summary (ff , Sf ) in the global summary \nenvironment G. The precondition ff is instantiated to fI f using instf. Observe that if fI f is valid, \nthen the potential assertion failure in f is discharged at this call site; otherwise, fI f is conjoined \nwith the precondition f of the current function. Next, we compose the partial heap Sf , representing \nthe heap fragment reachable in f after the call, with the existing heap S be\u00adfore the function call. \nThe compose partial heap rule used in com\u00adpose heap instantiates an entry p . . in f s summary. Observe \nthat if location p in f s summary instantiates to location pi in the current function under fi, existing \nvalues of pi are only preserved under \u00acfi. Hence, if fi is true, this rule applies a strong update to \npi. On the other hand, if p instantiates to pi under some entry alias condition, then this rule represents \na strong update to pi only in those contexts where the entry aliasing condition holds. EXAMPLE 9. Consider \na call to function f of Example 6: define g(a1 : ptr(ptr(int))) = f3 (a1 , a1 )  I,. f inst loc(p1)= \n.1,... inst loc(pk)= .k I,. f inst theta({(p1,f1),..., (pk,fk)}): (.i . fi) 1=i=k I,. f inst loc(p)= \n.s I,. f inst theta(.)= .t SI = S[pi . (.t . fi) . (S(pi) .\u00acfi) | (pi,fi) . .s] S, I,. f compose partial \nheap(p, .): SI Sf = [(p1 . .1),..., (pk . .k)] S, I,. f compose partial heap(p1,.1): S1 ... Sk-1, I,. \nf compose partial heap(pk,.k): Sk S, I,. f compose heap(Sf ): Sk G(f)= (ff , Sf )E, S f map args(v1,...,vk): \nI I,. f instf(ff ): fI f S, I,. f compose heap(Sf ): SI E, S, G,f f f.(v1,...,vk): SI,f . fI f Figure \n15. Summary Instantiation rules Before the call to f, g s local heap is depicted as: Recall from Example \n6 that f s precondition is drf(.1)= drf(.2), which instantiates to drf(.1)= drf(.1) . false at this call \nsite, indicating that the assertion is guaranteed to fail. The store in f s summary from Figure 7 is \ninstantiated at the call site to: Composing initial heap (*) with the instantiated heap (**), we obtain \nthe .nal heap after the function call: Observe that the resulting abstract heap is as precise as analyzing \nthe inlined body of f.  4.3.4 Summary Generation and Fixed-point Computation We now conclude this section \nby describing function summary generation, given in Figure 16. Before analyzing the body of f, the local \nabstract heap S is initialized as described in Section 4.3.1. Next, f s body is analyzed using the abstract \ntransformers from Section 4.3.2 and 4.3.3, which yields a store SI and a precondition fI. According to \nthe last hypothesis in Figure 16, the summary (ff , Sf ) is sound if Sf overapproximates S\\{.1,...,.k} \nand ff implies fI. Here, S1 S2 is de.ned as: DEFINITION 10. ( ) Let SI 1 = S1.S2 and SI 2 = S2.S1 . We \nsay S1 S2 if for every p . dom(SI 1) and for every pI such that (pI , (.1 ,.11(p), (pI , (.2 ,.22(p), \nwe have: maymust)) . SI maymust)) . SI 122 1 .may . .may . .must . .must While the rule in Figure 16 \nveri.es that (ff , Sf ) is a sound sum\u00admary, it does not give an algorithmic way of computing it. In \nthe presence of recursion, we perform a least .xed-point computation where all entries in Gare initially \n. (i.e., any location points to any other location under false), and a new summary for f is obtained \nby computing the join of f s new and old summaries: (S1,f1)U(S2,f2) = (S1 U S2,f1 . f2) A f init heap(a1,...,ak): \nE, S E, S, G, true f S : fI , SI G f f : (ff , Sf ) ff . fI Sf ; (SI\\{.1,...,.k}) G, A f de.ne f(a1 : \nt1,...,ak : tk)= S : (ff , Sf ) Figure 17. Heaps from Lemma 8 This strategy ensures that the analysis \nis monotonic by construc\u00adtion. Furthermore, since the analysis creates a .nite number of ab\u00adstract locations \nand the constraints are over a .nite vocabulary of predicates, this .xed-point computation is guaranteed \nto converge. In fact, for an acyclic callgraph, each function is analyzed only once if a reverse topological \norder is used. 5. Computing Alias Partition Sets In the previous section, we assumed the existence of \nan alias parti\u00adtion environment A that is used to query whether aliasing between locations a and aI may \naffect analysis results. One simple way to compute such an environment is to require that aI . A(a) if \na and aI have the same type (at least in a type-safe language). Fortunately, it is possible to compute \na much more precise alias partition envi\u00adronment because many aliasing relations at a call site of f \ndo not affect the state of the heap after a call to f. The following lemma elucidates when we can safely \nignore potential aliasing between two locations in a code fragment S. LEMMA 8. Let H1 and H2 be the canonical \nheap fragments shown in Figure 17, and let S be a program fragment such that: There is either no store \nto A and no store to B, or  There is a store to only A that is not followed by a load from B, or  There \nare only stores to both A and B, but the store to A must happen after the store to B  Let H1 f S : H1 \nI and H2 f S : H2I , and let O be a partial order such that O(B) -O(A) if there must be a write to A \nafter a write to B in S. Let H2 II be the graph obtained by replacing G s targets with E s targets in \nH2 I if .H2,H1 (A)=.H2,H1 (B) and (HII O(B) -O(A). Then, H1 I =.H2,H1 2 ). PROOF 9. (sketch) There are \nthree cases: (i) If there is no store to A or B, then in H2I , E and G still point to F and H, both of \nwhich are equivalent to D in H1I . Thus, H1 I =.H2,H1 (H2I ). (ii) There is only a store to A, not followed \nby a load from B: In H1I , C will point to some set of new locations T1,...,Tk. In H2I , E must also \npoint to T1I,...,T k I such that Ti =.H2,H1 (Ti I) and G must point to H. First, the result of any load \nfrom B (i.e., H) can be correctly renamed to D, as the read happens before the store to A. Second,  \nLiteSQL OpenSSH Inkscape widget lib. Digikam Lines 16,030 22,615 37,211 128,318 Strong updates at instantiation \nRunning time (1 CPU) 4.5 min 3.9 min 7.2 min 45.1 min Running time (8 CPUs) 1.6 min 1.8 min 2.3 min 8.7 \nmin Memory use 430 MB 230 MB 195 MB 400 MB Error reports 7 6 7 37 False positives 2 1 3 9 Weak updates \nat instantiation Running time (1 CPU) 7.1 min 4.8 min 8.1 min 60.0 min Running time (8 CPUs) 4 min 3.6 \nmin 2.5 min 10.1 min Memory use 410 MB 250 MB 200 MB 355 MB Error reports 312 209 730 1140 False positives \n307 204 726 1112 Figure 18. Comparison of strong/weak updates at call sites  HII 2 is obtained by removing \nthe edge from G to H and adding edges from G to each Ti I. Thus, .H2,H1 (G)=.H2,H1 (E)= C and G and E \ns targets are renamed to T1,...,Tk. (iii) Similar to (ii). This lemma shows the principle that can be \nused to reduce the number of entries in A: Assuming we can impose an order on the sequence of updates \nto memory locations and assuming we instantiate summary edges in this order, then the initial heap abstraction \nonly needs to account for aliasing between a1 and a2 if there is a store to a1 followed by a load from \na2, which is necessary because updates through a1 to a location may now affect locations that are reachable \nthrough a2. On the other hand, if there is no load after a store and the updates to memory locations \ncan be ordered, it is possible to .x up the summary at the call site by respecting the order of updates \nduring instantiation. To allow such an optimization in the analysis described in Sec\u00adtion 4, we impose \na partial order -on points-to relations such that (p1 . .1) -(p2 . .2) indicates that p1 must be assigned \nto .1 before p2 is assigned to .2. Then, to respect the order of up\u00addates in the callee when instantiating \nthe summary, we ensure that if pi . .i -pj . .j , the compose partial heap rule is invoked on pi . .i \nbefore pj . .j in the compose heap rule of Figure 15. Thus, assuming we modify the analysis from Section \n4 as de\u00adscribed above, we can compute a better alias partition environment A by performing a least .xed-point \ncomputation over the current function f. In particular, A(a) is initialized to {a} for each loca\u00adtion \nvariable a. Then, if the analysis detects a store to a followed by a load from aI of the same type, then \naI . A(a) and a . A(aI). Similarly, if there is a store s1 to a and a store s2 to aI (of the same type) \nsuch that there is no happens-before relation between s1 and s2, then aI . A(a) and a . A(aI). 6. Experiments \nWe have implemented the technique described in this paper in our COMPASS program veri.cation framework \nfor analyzing C and C++ applications. Our implementation extends the algorithm de\u00adscribed in this paper \nin two ways: First, our analysis is fully (i.e., interprocedurally) path-sensitive and uses the algorithm \nof [8] for this purpose. Second, our implementation improves over the anal\u00adysis presented here by employing \nthe technique described in [4], which uses indexed locations to reason precisely about contents of arrays \nand containers. Hence, the algorithm we implemented is sig\u00adni.cantly more precise than a standard may \npoints-to analysis. Figure 18 summarizes the results of our .rst experiment, which involves verifying \nmemory safety properties (buffer overruns, null dereferences, casting errors, and access to deleted memory) \nin four real C and C++ applications ranging from 16,030 to 128,318 lines. The .rst part of the table, \nlabeled Strong Updates at Instantia-tion , reports the results obtained by using the modular heap anal\u00adysis \ndescribed in this paper. Observe that the proposed technique is both scalable, memory-ef.cient, and precise. \nFirst, the running times on 8 CPU s range from 1.6 minutes to 8.7 minutes, and in\u00adcrease roughly linearly \nwith the size of the application. Further\u00admore, observe that the modular analysis takes advantage of \nmulti\u00adple CPUs to signi.cantly reduce its wall-clock running time. Sec\u00adond, the maximum memory used by \nany process does not exceed 430 MB, and, most importantly, the memory usage is not corre\u00adlated with the \napplication size. Figure 19 sheds some light on the scalability of the analysis: This .gure plots the \nmaximum call stack depth against summary size, computed as the number of points-to edges weighted according \nto the size of the edge constraints plus the size of the precondition. In this .gure, observe that summary \nsize does not increase with depth of the callstack, con.rming our hypothesis that summaries are useful \nfor exploiting information lo\u00adcality and therefore enable analyses to scale. hostname chroot rmdir su \nmv Lines 304 371 483 1047 1151 Modular analysis running time 0.53s 0.75s 1.54s 2.3s 2.55s Whole program \nrunning time 3.1s 6.3s 21.6s 45.9s 30.7s Figure 20. Comparison of modular and whole program analysis \n Figure 18 also illustrates that performing strong updates at call sites is crucial for the precision \nrequired by veri.cation tools. Observe that the analysis using strong updates at instantiation sites \nis very precise, reporting only a handful of false positives on all the applications. In contrast, if \nwe use only weak updates when applying summaries, the number of false positives ranges from 200 to 1000, \ncon.rming that the application of strong updates interprocedurally is a key requirement for successful \nveri.cation. In a second set of experiments on smaller benchmarks, we com\u00adpare the running times of our \nveri.cation tool using the modular analysis described here with the running times of the same tool us\u00ading \na whole-program analysis. Figure 20 shows a comparison of the analysis running times of the modular and \nwhole program anal\u00adysis on .ve Unix Coreutils applications. As shown in this .gure, the whole program \nanalysis, which did not report any errors, takes ~50 seconds on a program with only 1000 lines, whereas \nthe modular analysis, which also did not report any errors, analyzes the same program in 2.3 seconds. \nFurthermore, observe that the running time of the whole program analysis increases much more quickly \nin the size of the application than that of the modular analysis. In a .nal set of experiments, we plot \nthe size of the alias parti\u00adtion set vs. the frequency of this set size for the benchmarks from Figure \n18. The solid (red) line shows the size of the alias partition sets obtained by assuming aI . A(a) if \na and aI have compati\u00adble types. In contrast, the dashed (green) line shows the size of the alias partition \nsets obtained as described in Section 5. Observe that these optimizations signi.cantly reduce the size \nof alias partition sets and substantially improve running time. In particular, without these optimizations, \nthe benchmarks take an average of 2.7 times longer.  7. Related Work Compositional Alias Analysis Modular \nalias analysis of a proce\u00addure performed by starting with unknown values to all parameters was also explored \nin [6] and then in Relevant Context Inference (RCI) [7]. The technique presented in [6] computes a new \npar\u00adtial transfer function as new aliasing patterns are encountered at call sites and requires reanalysis \nof functions. In contrast, the tech\u00adnique in [7] is purely bottom-up, and uses equality and disequal\u00adity \nqueries to generate summary transfer functions. Our approach is similar to [7] in that we perform a strictly \nbottom-up analysis where the unknown points-to target of an argument is represented using one location \nvariable and summary facts are predicated upon possible aliasing patterns at function entry. In contrast \nto our tech\u00adnique, RCI is only able to perform strong updates in very special cases intraprocedurally, \nand cannot perform strong updates at call sites. In fact, the summary computation described in [7] is \nonly sound under the assumption that no points-to relations are killed by summary application. In contrast, \nsummaries generated by our analysis are used to perform strong updates at call sites, and for the recursion-free \nfragment of the language from Section 3, applying a summary is as precise as analyzing the inlined body \nof the function. The compositional pointer analysis algorithms given in [9, 10] assume there is no aliasing \non function entry and analyze the function body under this assumption. However, since summaries computed \nin this way may be unsound, the summary is corrected using a fairly involved .xed-point computation at \ncall sites. This approach is also much less precise than our technique because it only performs strong \nupdates in a very limited number of situations. Compositional Shape Analysis Recently, there has also \nbeen in\u00adterest in compositional shape analysis using separation logic [11, 12]. Both of these works use \nbi-abduction to compute pre-and post-conditions on the shapes of recursive data structures. How\u00adever, \nneither of these works guarantee precision. While this paper does not address computing summaries about \nshapes of recursive data structures, our technique can handle deep sharing and allows disjunctive facts. \nGeneral Modular Analysis Frameworks Theoretical foundations for modular program analysis are explored \nin [13], [14], and [15]. The work in [16] provides a framework for computing precise and concise summaries \nfor IFDS [17] and IDE [18] data.ow problems. This framework is mainly specialized for typestate properties \nand relies on global points-to information. While it may be possible to apply this framework to obtain \nsome form of modular heap analysis in principle, it is unclear how to do so, and the authors of [16] \nlist this application as a future research direction. References [1] Aiken, A., Bugrara, S., Dillig, \nI., Dillig, T., Hackett, B., Hawkins, P.: An overview of the saturn project. In: PASTE, ACM (2007) 43 \n48 [2] Bush, W., Pincus, J., Sielaff, D.: A static analyzer for .nding dynamic programming errors. Software: \nPractice and Experience 30(7) (2000) 775 802 [3] Reps, T.W., Sagiv, S., Wilhelm, R.: Static program analysis \nvia 3\u00advalued logic. In: CAV. Volume 3114., Springer (2004) 15 30 [4] Dillig, I., Dillig, T., Aiken, A.: \nFluid updates: Beyond strong vs. weak updates. In: ESOP. (2010) [5] Landi, W., Ryder, B.G.: A safe approximate \nalgorithm for interproce\u00addural aliasing. SIGPLAN Not. 27(7) (1992) 235 248 [6] Wilson, R.P., Lam, M.S.: \nEf.cient context-sensitive pointer analysis for c programs. In: PLDI. (1995) [7] Chatterjee, R., Ryder, \nB., Landi, W.: Relevant context inference. In: POPL, ACM (1999) 133 146 [8] Dillig, I., Dillig, T., Aiken, \nA.: Sound, complete and scalable path\u00adsensitive analysis. In: PLDI, ACM (2008) 270 280 [9] Whaley, J., \nRinard, M.: Compositional pointer and escape analysis for Java programs. In: OOPSLA, ACM (1999) 187 206 \n[10] Salcinau, A.: Pointer Analysis for Java Programs: Novel Techniques and Applications. PhD thesis, \nMIT (2006) [11] Calcagno, C., Distefano, D., O Hearn, P., Yang, H.: Compositional shape analysis by means \nof bi-abduction. POPL (2009) 289 300 [12] Gulavani, B., Chakraborty, S., Ramalingam, G., Nori, A.: Bottom-up \nshape analysis. SAS (2009) 188 204 [13] Cousot, P., Cousot, R.: Modular static program analysis. In: \nCC. (2002) 159 178 [14] Gulwani, S., Tiwari, A.: Computing procedure summaries for inter\u00adprocedural analysis. \nESOP (2007) 253 267 [15] Pnueli, M.: Two approaches to interprocedural data .ow analysis. Program Flow \nAnalysis: Theory and Applications (1981) 189 234 [16] Yorsh, G., Yahav, E., Chandra, S.: Generating precise \nand concise procedure summaries. POPL 43(1) (2008) 221 234 [17] Reps, T.W., Horwitz, S., Sagiv, S.: Precise \ninterprocedural data.ow analysis via graph reachability. In: POPL. (1995) 49 61 [18] Sagiv, S., Reps, \nT.W., Horwitz, S.: Precise interprocedural data.ow analysis with applications to constant propagation. \nTheor. Comput. Sci. 167(1&#38;2) (1996) 131 170   \n\t\t\t", "proc_id": "1993498", "abstract": "<p>We present a strictly bottom-up, summary-based, and precise heap analysis targeted for program verification that performs strong updates to heap locations at call sites. We first present a theory of heap decompositions that forms the basis of our approach; we then describe a full analysis algorithm that is fully symbolic and efficient. We demonstrate the precision and scalability of our approach for verification of real C and C++ programs.</p>", "authors": [{"name": "Isil Dillig", "author_profile_id": "81331491247", "affiliation": "Stanford University, Stanford, CA, USA", "person_id": "P2690653", "email_address": "isil@stanford.edu", "orcid_id": ""}, {"name": "Thomas Dillig", "author_profile_id": "81331491149", "affiliation": "Stanford University, Stanford, CA, USA", "person_id": "P2690654", "email_address": "tdillig@stanford.edu", "orcid_id": ""}, {"name": "Alex Aiken", "author_profile_id": "81100399954", "affiliation": "Stanford University, Stanford, CA, USA", "person_id": "P2690655", "email_address": "aiken@cs.stanford.edu", "orcid_id": ""}, {"name": "Mooly Sagiv", "author_profile_id": "81100150928", "affiliation": "Tel Aviv University, Tel Aviv, Israel", "person_id": "P2690656", "email_address": "msagiv@post.tau.ac.il", "orcid_id": ""}], "doi_number": "10.1145/1993498.1993565", "year": "2011", "article_id": "1993565", "conference": "PLDI", "title": "Precise and compact modular procedure summaries for heap manipulating programs", "url": "http://dl.acm.org/citation.cfm?id=1993565"}