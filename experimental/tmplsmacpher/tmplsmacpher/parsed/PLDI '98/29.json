{"article_publication_date": "05-01-1998", "fulltext": "\n The Design and Implementation of a Certifying Compiler George C. Necula Peter Lee School of Computer \nScience Carnegie Mellon University Pittsburgh, Pennsylvania 15213-3891 {necula,petel}@cs.cmu.ecJu Abstract \nThis paper presents the design and implementation of a com- piler that translates programs written in \na type-safe subset of the C programming language into highly optimized DEC Alpha assembly language programs, \nand a certifier that au-tomatically checks the type safety and memory safety of any assembly language \nprogram produced by the compiler. The result of the certifier is either a formal proof of type safety \nor a counterexample pointing to a potential violation of the type system by the target program. The ensemble \nof the compiler and the certifier is called a certi,fying compiler. Several advantages of certifying \ncompilation over previ-ous approaches can be claimed. The notion of a certify- ing compiler is significantly \neasier to employ than a formal compiler verification, in part because it is generally easier to verify \nthe correctness of the result of a computation than to prove the correctness of the computation itself. \nAlso, the approach can be applied even to highly optimizing compil-ers, as demonstrated by the fact that \nour compiler generates target code, for a range of realistic C programs, which is competitive with both \nthe cc and gee compilers with all op- timizations enabled. The certifier also drastically improves the \neffectiveness of compiler testing because, for each test case, it statically signals compilation errors \nthat might oth-erwise require many executions to detect. Finally, this ap-proach is a practical way to \nproduce the safety proofs for a Proof-Carrying Code system, and thus may be useful in a system for safe \nmobile code. 1 Introduction The question of compiler correctness is as old as the first compiler implementations. \nIn a paper published in 1963, This research was sponsored in part by the Advanced Research Projects Agency \nCSTO under the title The Fox Project: Advanced Languages for Systems Software, ARPA Order No. C533, issued \nby ESC/ENS under Contract No. F19628-95-C-0050. The views and conclusions contained in this document \nare those of the authors and should not be interpreted as representing the official policies, either \nexpressed or implied, of the Advanced Research Projects Agency or the U.S. Government. 0 1998 ACM 0.89791-987-4198/0008...S6.00 \n John McCarthy refers to this problem as one of the most interesting and useful goals for the mathematical \nscience of computation (McCarthy 1963). However, despite a large body of work in the area (Dybjer 1986; \nGuttman, Ramsdell, and Wand 1995; Moore 1989; Morris 1973; Oliva, Ramsdell, and Wand 1995; Thatcher, \nWagner, and Wright 1980; Young 1989), we still lack the technology to prove automatically the correctness \nof an optimizing compiler. Even manual proofs are rare, and they tend to verify only the algorithms rather \nthan the implementations. Plus, the correctness proofs need to be redone after even the slightest modification \nor improve- ment to the compiler. Proving compiler correctness is just a means towards the actual goal \nof ensuring that only correct output is ever pro-duced by the compiler. In this paper we propose a poten- \ntially more practical approach to the same goal. Instead of verifying the compiler once and for all, \nwe check aspects of the correctness of every individual compilation. This will not ensure that the compiler \nis bug-free, but it will signal most incorrect compiler outputs as soon as they are pro-duced. To reduce \nthe complexity of the checking process, we do not try to check full equivalence of the source and target \nprograms, but instead we verify only that the target program has certain key properties that can be verified \nusing a small amount of information about the source program. We present in this paper the design and \nimplementa-tion of Touchstone, an optimizing compiler that translates a strongly typed programming language \n(essentially a type- safe subset of C) into DEC Alpha assembly language, and a certifier that checks \nthe type safety of any assembly lan-guage program produced by the compiler. The result of the certifier \nis either a formal proof of type safety or a counterex- ample pointing to a potential violation of the \ntype system by the assembly-language target program. We refer to the ensemble of the compiler and the \ncertifier as a certifying compiler. Our approach provides several advantages: l This method is significantly \neasier to employ than a formal verification of the compiler, even if the formal verification is restricted \nto proving that only type-safe code is emitted. This is because it is easier in general to verify the \ncorrectness of the result of a computation than to prove the correctness of the computation it-self. \nFurthermore, with this approach, most compiler revisions and improvements do not require any change to \nthe certifier. l This method can be applied to optimizing compilers, because the design of the certifier \ndoes not restrict the optimizations that the compiler is allowed to perform. Our optimizing compiler \ngenerates code that, for many programs, matches or is within 15% of the performance of both gee and cc \nwith all optimizations enabled, the difference being due mostly to several optimizations that we have \nnot yet implemented. Also, we have suc-cessfully tested the certifier on hand-optimized sssem-bly language. \n Type Specification Annotated Code ProofKounterexample Figure 1: Overview of the Touchstone certifying \ncompiler. l The presence of the certifier drastically improves the effectiveness of compiler testing \nbecause, for each test case, it statically signals compilation errors that might otherwise require many \nexecutions to detect. Even though this approach does not ensure full compiler cor-rectness, in our experience \nthe vast majority of com-piler bugs lead the compiler to generate unsafe target programs for at least \none of the test cases. l This method is applicable to the compilation of any type-safe language, as well \nas for certifying other prop-erties of the target programs beyond type safety. Also, a significant benefit \nof our design is that it requires rel-atively few modifications to the traditional compiler design, and \nhence it should be possible to adapt exist-ing compilers to this technique. l This is a practical method \nfor producing, in an auto- matic manner, the safety proofs for a Proof-Carrying Code (Necula 1997; Necula \nand Lee 1996) system for type safety. By attaching the type-safety proof emit-ted by the certifier to \nthe assembly language program, we enable a circumspect software system to easily ver-ify (by checking \nthat the attached proof is valid and applies to the given target program) that the program is type safe \nand memory safe. Thus, a certifying com-piler can be at the base of a system for safe execution of untrusted \nmobile code. This paper is organized as follows. In Section 2 we give a high-level overview of the certifying \ncompiler that we have implemented, and we compare it with related systems. Then WC present some details \nof the source language compiled by our prototype compiler. We continue with the implementa- tion details \nof the compilation and the certification phases. We discuss the certification phase first (Section 4) \nbecause its design is of independent interest and because it sets up the requirements for the compiler \nsubsystem, which is dis- cussed in Section 5. Of all of the optimizations, we focus on array bounds-checking \nelimination and we show what addi-tional output the compiler must produce so that the certifier can check \nthe memory safety of the optimized code (Sec-tion 5.1). We conclude with experimental results on a range \nof realistic C programs (Section 6). The experiments show that the cost of generating and checking the \nsafety proofs is low, and also that we are indeed certifying a true optimizing compiler whose output \ncode performance approaches that of both cc and gee. 2 Overview of the Touchstone Certifying Compiler \nAt a high-level, the certifying compiler is, as shown in Fig- ure 1, a pipeline composed of a compiler \nand a certifier. The compiler is a traditional compiler adapted to produce type specifications and code \nannotations in addition to the as-sembly language target program. Determining whether the target programs \nare type safe and memory safe is not an easy matter, due to the fact that the compiler performs a wide \nrange of global optimizations. For example, the compiler performs global register allocation (with spilling \nand coa-lescing), and so a register might be used to store values of different types within a single \ncode block. Also, the compiler aggressively analyzes and removes array-bounds checks, thus making it \nnontrivial to deduce that the target code is mem- ory safe. (The full range of optimizations performed \nby our compiler is described in Section 5.) The purpose of the code annotations is to make it pos-sible \nfor a simple certifier to understand enough of the code to verify its type safety and memory safety, \ndespite the opti- mizations. Owing to the design of the certifier, the required annotations are limited \nto loop invariants that declare the types of the live registers at the beginning of a loop body. The \ntype specifications declare the type of argument and result registers for every function in the code. \nThe type specifications are thus the vehicle for propagating source level information to the certification \nstage and to allow the certifier to verify that the target program retains at least the typing characteristics \nof the source program, if not full equivalence. The certifier subsystem is itself a pipeline composed \nof three subsystems: the verification condition generator (re-ferred to as VCGen), the prover and the \nproof checker, as shown in Figure 2. The VCGen scans the annotated as-sembly language program and, using \nthe type specifications and the code annotations, produces a safety predicate for each function in the \ncode, such that the safety predicate has a proof if and only if the assembly language program is memory-safe \nand type-safe according to the typing specifica-tion. Due to the code annotations and typing specifications, \nthe VCGen can be performed on a function-at-a-time basis and can be implemented as an efficient single \npass through the program. Following the VCGen phase, the safety predicate is sub- mitted to a prover \nfor first-order predicate logic that pro-duces a formal proof of the predicate. Finally, the safety predicate \nand its proof are given to a very simple proof checker that verifies that we actually have a valid proof \nof the required safety predicate, and therefore the compiler output is memory safe and type safe. An \nimportant characteristic of our system is that it has a small safety-critical infrastructure. That is, \nthe code that is relied upon to guarantee that no unsafe target programs escape unnoticed includes only \nthe VCGen and the proof checker. Neither the compiler nor the prover need to be correct in order to be \nguaranteed to detect incorrect com-piler output. This is a significant advantage, since the VC- VCGen \n Figure 2: The structure of the certifier. Gen and proof checker are significantly simpler than the compiler \nand the prover. Our confidence in VCGen and the proof checker is further enhanced by the fact that they \nare borrowed unchanged from our Proof-Carrying Code sys- tem, (Necula and Lee 1996) which has been in \nuse since September 1996. 3 The Source Language Our current prototype implementation of a certifying \ncom- piler is for a strongly typed language, essentially a type-safe subset of the C programming language. \nUnlike C, all array subscripting operations are implicitly guarded by bounds- checking conditionals. \nAlso, in order to simplify the elimi- nation of bounds-checking, an array is represented as a pair of \nvalues representing the base address and the array length. The length operator refers to the length component, \nwhile the subscripting operation refers to the base address com- ponent. This arrangement is compatible \nwith the common programming practice of passing the array length value to- gether with the base address. \nMultidimensional arrays have a length component for every dimension. In addition to safe arrays, our \ncompiler supports Java-style exceptions and exception handling (mostly for a cleaner treatment of array \nsubscript errors), dynamic allocation of data structures in the heap, booleans as a separate type, and \nmost of the arithmetic expression constructs of the C programming language. Common language features \nthat are currently missing are: recursive data structures, func-tion pointers, floating point numbers, \nand allocation of data structures on the stack. Of these, only the function pointers are expected to \npose some difficulties because they are not currently supported by the certification subsystem. And of \ncourse, we do not implement casts, the address-of operator, pointer arithmetic and explicit memory deallocation \nbecause they are not safe in general (though perhaps safely restricted versions of these operators might \nbe added in the future). Finally, the implementation of the language assumes the use of an automatic \ngarbage collector. 4 Design Details of the Certifier The design of the certifier establishes the required \ncode an-notations and type specifications that the compiler must produce. There are other important aspects \nof the certi-fier s design, as well. Although we shall discuss only type and memory safety here, the \ncertifier is general enough to be used for certifying other properties, and for handling safety properties \nin other languages. For a more concrete presentation of the certification pro-cess we introduce a simple \nexample program and the corre-sponding compiler output. The program in Figure 3a com- putes the sum of \nall elements of an integer array. Our com-piler (which compiles one function at a time) compiles this \nprogram into the annotated code shown in Figures 3b and the typing specification shown in 3c. Note that \nthe source-level array argument is represented in the target program as int main(int af.1) { int i, 8 \n= 0; for(i=O;i<length(a) ;i++) { 8 += aCi1; 1 return s; #a0 -base address, al -array length main : \nmov zero, v0 #a=0 mov zero, t0 #i=O Ll: ANN-NV (v0 : int A t0 >= O,{tO,tl,vO}) sub1 tO,al,tl #i -length \n(a) t1 ,L2 be slladdl tO,aO,tl ttl=a0+4*tO add1 to,1 ,to #i++ Id1 t1,0(t1> #a Cil add1 t1,vo,vo br Ll \nL2: ret (b) main : (Pre = a0 : array(int, al) A al 2 1, Post = vo : int) (4 Figure 3: An example source \nprogram (a) and the corre-sponding compiler output, consisting of the annotated code (b) and the typing \nspecification (c). two values, namely the base address in register a0 and the array length in register \nal. The return value is returned in register vo, following the standard DEC Alpha calling con-vention. \nNote also that our compiler is successful in remov- ing the bounds-checking operations in this example. \nThe syntax and meaning of the loop invariant code annotation appearing at label Ll in Figure 3b and the \ntyping specifica-tions from Figure 3c are described in the next section. 4.1 The VCGen The VCGen is a \nverification condition generator for the DEC Alpha annotated assembly language. Traditionally, verifica-tion \ncondition generation is implemented as a backward pass through the code. However, we choose a different \nimplemen-tation technique that uses a forward symbolic-evaluation pass through the code. VCGen operates \non a per-function basis and performs three main operations. Firstly, it ensures that the code satisfies \ncertain simple syntactic conditions (e.g., that all branch targets are within the code bound-aries and \nthat only recognized instructions occur). Secondly, VCGen evaluates the code symbolically and whenever \nit en-counters a memory operation it emits a verification condi-tion (VC) that states under what conditions \nthe memory operation is considered safe. For example, in the case of a read operation from address a, \nthe condition saferd(a) is emitted. For a write operation the condition safewr(a, e) VOrS 2 ::= rm. 1 \nri (i=O,...,31) Expr e::= zlnlel+ezlel-es) se1 (el, ez) I upd (el, es, es) Types T ::= int 1 boo1 I array(T,e) \nPred P ::= true ( PI A P2 I PI > P2 I Vx.P, I el = e2 1 el # e2 1 el 1 e2 I el < e-2 e: T I saferd (el) \n1 safeur (e2,es) Inv L ::= ANN-INV(P, ($1,. . . ,xk}) Spec 0 ::= f : (Pre = PI, Post = Pz) Figure 4: \nThe syntax of the safety predicates. is emitted (e denotes the value being written). The meaning of the \npredicates saferd and safewr is defined at the level of the prover (described in Section 4.2) to allow \nfor a greater flexibility in choosing the desired flavor of memory safety. VCGen must operate on a per-function \nbasis and ana-lyze code with loops without having to iterate through the loop body multiple times. To \naccomplish this, we require that each function in the code have a typing specification in the form of \na precondition and a postcondition, and that each loop have an invariant annotation, which is a predicate \nthat must hold every time around the loop. As these speci- fications and annotations come with the code \nand cannot be trusted in general, the third function of VCGen is to ensure that they exist and are valid. \nTo ensure that all loop invari- ants are present, VCGen verifies that ewh backward-branch target is associated \nwith an invariant annotation. In order to set the stage for a more detailed discussion of VCGen, we proceed \nwith the introduction of the required notation. The symbolic evaluator operates with the syn-tactic entities \nshown in Figure 4. Among the variables we have the 32 physical DEC Alpha registers (ri, i = 0,. . . ,31) \nand the memory pseudo-register rm. The latter is used to denote the contents of the memory during execution. \nThe contents of a memory address a is written as sel(r,, a) and the effect of updating the memory at \naddress a with the ex-pression e is modeled by the assignment rm. t upd(r,, a, e). We write CS and Temp \nto refer to the callee-save and tempo- rary machine registers, as defined by the DEC Alpha calling convention. \nThe language of predicates contains the first-order pred-icate logic constructors, the memory-safety \npredicates and the typing predicate. Among the types we consider here only the integers, booleans and \none-dimensional arrays. Note that the array type encodes not only the element type but also the array \nlength, which is guaranteed to be at least one. The type of pointers to elements of type 7 is expressed \nas array(r, 1). The only code annotations that we need for the purpose of this paper are the loop invariant \nannotations. Each such annotation contains an invariant predicate and a set of reg- isters that are modified \nin the loop body (see the invariant at label Ll in Figure 3b). For a simpler presentation we show the \ncode annotations as part of the code although in practice they are stored in the data segment. The typing \nspecification of a function is a pair of a pre- condition and a postcondition. The precondition is essen-tially \na description of the calling convention and it declares the type of each argument register used by a \nfunction. The We only show here the syntax that is required for the examples. In practice, a more comprehensive \nlanguage of expressions and pred-icates is used. Vao. Val . Vr, . (a0 : array(int,al) A al > 1) > (0 \n: int A 0 > 0) A (vto.vtl .vvo. (vg : int A to 2 0) 3 (to -al 2 0 > vg : int) A (to -al < 0 > (saferd(ao \n+ 4 x to) A to+l>OA vg + sel(rm, a0 + 4 X to) : int))) Figure 6: The safety predicate for the annotated \ncode of Figure 3b. postconclition is a similar declaration of the types of the re-sult registers (VO \nand rm according to the standard calling convention on the DEC Alpha). A function returning no result \nhas the postcondition true. The specifications are easily derived from the type of the function (see \nFigure 3~). Intuitively, the precondition is a predicate that can be as-sumed to be true when analyzing \nthe body of the function, while the postcondition is a predicate that must be made true by the body of \nthe function. VCGen is defined as a symbolic evaluator whose result is a predicate (the verification \ncondition) that is provable only if the program is safe with respect to the typing specifica-tion. Let \nC : Label --+ Spec be the type specification for the entire program, represented as a map from function \nlabels to their typing specifications. We assume that the target program is an array II of instructions \nand code annotations. The state of the symbolic evaluator consists of the current index i in the target \nprogram II, the register state p and a list C of the loop invariants encountered on the path from the \nstart of the function. (Recall that VCGen translates one function at a time.) The register state is a \nmapping from register names to expressions p E VarState = Vars -+ Expr. We write p[ri +-e] to denote \nassigning of e to ri and we write p(e) to denote the expression obtained after substitut-ing the register \nnames with their values in p. We extend the substitution notation to predicates. The loop invariant mapping \nL: maps the indices of loop invariants to the regis- ter states at the beginning of the corresponding \nloop body. These states are used to verify the set of changed registers in a loop. The core of VCGen \nis the symbolic evaluator, which can be described as a function SEn,~,~~,p~~t (i, p, C) with seven parameters: \nthe annotated program II, the type specifica-tion C, the initial register state and the postcondition \nof the current function (po and Post), and the current values of the instruction index i, the register \nstate p and the loop state c. To compute the safety predicate of a function f with precondition Pre and \npostcondition Post, we first initialize the registers with new variables IO,. . . , ~32 (for the machine \nregisters 10, . . . , r31 and the memory pseudo-register r,,,). If po is the resulting initial register \nstate, then the safety predicate is given by the formula: SPf = vxo . . . m.po(Pre) 3 SJ%I,~,~,,,P~~~(~, \nPO, [I) To simplify the notation we omit the subscripts on the SE function from now on. The symbolic \nevaluation function is defined formally as a recursive function in Figure 5, and described informally \nin SE(i + 1, p[rd t p(rr + rz)], c) (p(r,) = 0 > SE(i + n + 1, p, L)) A (pb) # 0 3 SE@ + ~,P,C)) saferd \n(p(r,) + n) A SE(i + 1, p[rd t p(sel(r,, rs + n))], c) SafeWr (p(rd)+n,p(r,))) A SE(i + 1, p[rm t p(W(r,, \nrd + n, r5)>], L) p(Pre) A Vyl . . . yk.p (Post) > SE(i + l,p ,L)  p(Post) A checkEq (p, PO, CS) p(P) \nA &#38;/I . . . yk.p (P) > SE(i + l,p , C[i t p ]) p(P) A checkEq (p, Li, Re,gs-S) if Iii = add1 ri,rz,rd \nif Iii = beqr,,n I&#38;+,,+1 = ANN-INV if n < 0 if IIi=ldlrd,nr, if Iii =Stlrs,7&#38; rd if Iii = jsr \ng and C, = (Pre, Post) * I yk}) t scramble(p, !femp) if g)b;. , if Iii = ANN-INV P, s and i $! Dam(L) \n(f ,bl,.**,Yk)) t scramble(p, s) if Iii = ANN-INV P, s and i E Eom(L) scramble(p, (21,. . . ,zk}) = (p[zl \nc yr,. . . ,5k + yk], {yl,. . . , yk}) , yi are new variables checW(P,po>s) = A P(Z) = P&#38;C) xEs \nFigure 5: The definition of the symbolic evaluator function predicate which is true if the program is \nsafe. the rest of this section. For arithmetic operations the evalu- ator updates the symbolic register \nstate and continues with the next instruction. In the case of a conditional branch both branches are \nevaluated, each with the appropriate as-sumption about the outcome of the conditional. Note the use of \nimplication to include control flow information in the resulting verification condition, so that the \nverification con-dition can be proved without further reference to the code. A backward branch is verified \nto point to an invariant in-struction. This is a simple way to verify that all loops have at least one \ninvariant and to ensure the termination of the symbolic evaluator. For a memory operation, the appropri- \nate safety predicate is emitted, in addition to updating the register state. When dealing with a function \ncall the evaluator must ensure that the precondition part of the typing specification for the target \nfunction is established prior to the call. As is the case with the memory operations, VCGen does not \nitself verify the precondition, but instead it emits the ap-propriate verification conditions so that \nthe precondition is verified by the prover when proving the verification condi-tion. The symbolic evaluator \nassumes conservatively that all temporary registers are changed during a function invo-cation. The unknown \neffect of the function call on the tem-porary registers is expressed in the symbolic evaluator with the \nhelp of the scramble operation, defined at the bottom of Figure 5. To process the code following the \nfunction call, the symbolic evaluator uses the register state produced by scramble and assumes that the \npostcondition component of the typing specification is met upon return. Note how quantification on the \nnew values of the temporary registers is used to ensure that they are new from the logical point of view. \nWhen the symbolic evaluator encounters the return in-struction, it emits verification conditions that \nare provable only if the current function s postcondition is satisfied and if all the callee-save registers \nare preserved since the begin- ning of the function. The latter condition is encoded as a conjunction \nof equalities between the values of registers on function entry (as encoded by the register state PO) \nand their values on function exit. A loop invariant annotation is dealt with in a manner SEn~,~,,,p,,~~(i, \np, C). Th e result of symbolic evaluation is a safety e : array(r, 1) Oli i-cl saf erd(e + 4 x i) e \n: arrav(r, 1) O<i i<l  Figure 7: Proof rules for proving the safety of array accesses. Currently only \nbase types can occur in arrays, thus the size of an array entry is four bytes. similar to a function \ncall or a return instruction, depending on whether this is the first time it is encountered. When a loop \ninvariant is encountered for the first time, the symbolic evaluator verifies that the invariant is established \nbefore the loop is started. Then the symbolic evaluator simulates an arbitrary iteration through the \nloop, and for that purpose creates new register values for those registers that are de-clared to be modified \nby the loop body. In order to process the loop body, the symbolic evaluator uses the new values of registers \nand assumes that the invariant holds in this new state before the execution of the loop body. A loop \ninvari-ant that is encountered for the second time marks the end of the arbitrary iteration that was \ninitiated at the first occur-rence of the invariant. At this time, the evaluator requires that the invariant \nbe established and that only registers that were declared to be modified by the loop body have actually \nbeen modified. We conclude the presentation of the VCGen by showing in Figure 6 the safety predicate \nthat it produces for the program of Figure 3. 4.2 The Prover and the Proof Checker To prove the safety \npredicates produced by VCGen we need a theorem prover for first-order logic. Many of the existing theorem \nprovers (Boyer and Moore 1979; Detlefs 1996; Gor- don 1985; Owre, Rushby, and Shankar 1992) can be used \nfor this purpose, although they do not produce proofs that can be checked independently. That is not \nan impediment as long as we agree to rely on the correctness of the prover, and to give up the possibility \nof using the certifying compiler as a front end to Proof-Carrying Code systems. However, we feel that \nthese are important properties, and thus, to re-tain them we have implemented a theorem prover that emits \nproofs. The theorem prover is based on the Nelson-Oppen architecture for cooperating decision procedures \n(Nelson and Oppen 1979), also implemented in the Stanford Pascal Ver- ifier (D.C. Luckham 1979) and the \nExtended Static Check-ing (Detlefs 1996) systems. Theorem provers are traditionally viewed as logically- \nincomplete systems that require human intervention in many instances. In our system, however, the theorem \nprover is guaranteed to be able to prove the safety predicates automat-ically because these predicates \nare implicitly proved by the compiler itself during compilation. For example, during bounds-checking \nelimination, the compiler eliminates those bounds-checking conditionals that it can prove to be always \ntrue. Later, during certifica-tion, the corresponding array operation prompts the sym-bolic evaluator \nto emit a predicate that captures exactly the arithmetic facts that were proved by the compiler. Thus, \nit is enough for the theorem prover to be as good at prov-ing arithmetic facts as the compiler is. This \nis usually the case in practice, as theorem provers are much more powerful than the typical compiler \nanalysis of arithmetic. Beyond the predicate calculus and simple linear arith-metic, the theorem prover \nmust also be able to interpret the typing and the memory-safety predicates that occur in the symbolic \nevaluator s output. This can be done in most theorem provers by specifying a collection of inference \nrules. Two such rules are shown in Figure 7. The first rule says that it is safe to read an element of \nan array if its index is within the array boundaries, and the second rule says that the result of this \nread operation has the type of the array elements. By using these rules plus the usual predicate cal-culus \nrules, the reader can verify informally that the safety predicate shown in Figure 6 is indeed valid, \nand therefore the assembly language program of Figure 3b is memory safe. The role of the proof checker \nis to verify that every step in the proof is valid and also that the proof proves the re-quired safety \npredicate and not another one. We use the proof checker of the Proof-Carrying Code system, which rep-resents \nproofs in a language based on LF (Harper, Honsell, and Plotkin 1993), a simple typed &#38;calculus. There \nare sev- eral engineering advantages of using LF to represent proofs, perhaps the most fundamental being \nthat proof checking can be accomplished simply by type checking of LF terms. We encode a proof as an \nLF expression and the safety predi-cate as an LF type. Then LF type-checking is enough to validate the \nproof. (The fact that this approach is sound is established in (Harper, Honsell, and Plotkin 1993). We \nhave made some modifications that are described and proved to be sound in (Necula and Lee 1997).) Another \nadvantage of this arrangement is that the LF type checker is independent of the particular logic, and \nthus we are able to reuse its implementation for checking proofs in many logics, including the memory-safety \nand type-safety logic presented here. Also LF and LF type checking are simple, which leads to a small \nand fast implementation of the proof checker. 5 The Optimizing Compiler The compiler component of our \nsystem is not very different from a traditional compiler for C. The differences can be classified ss \ndue to changes in the language semantics and due to changes in the requirements on the output. The former \nclass includes the enforcement of the array bounds, as mentioned before. The latter class includes the \nmechanisms for emitting the code annotations and type specifications. A common task in producing both \nthe loop invariants and the type specifications is the conversion of variable type declarations to typing \npredicates involving machine regis-ters. This is done in two stages. The first stage happens in the compiler \nfront-end and consists of generating a predicate t : T for every source-level variable v of type T, where \nt is the intermediate language temporary variable corresponding to v. Because we have chosen the type \ncomponents of pred- icates to be similar to the source-level types, this stage is very simple. The second \nstage is done after register alloca-tion and consists of replacing the temporaries occurring in predicates \nwith the register names chosen for them by the allocator.  The procedure described above is all that \nis necessary for producing the type specifications. For loop invariants, we have to emit typing predicates \nfor the variables that are live at the beginning of the loop body, and we also have to compute the set \nof registers that are changed in the loop body. This is done by a separate pass over the output pro-gram. \nOne of the goals of the compiler implementation is to show that even the output of an optimizing compiler \ncan be certified for type-safety. The main optimizations that we have implemented are: array bounds-checking \nelimination, constant propagation with algebraic reductions, dead-code elimination, common-subexpression \nelimination, loop invari- ant hoisting, in-register global variables, induction variable elimination, \nand global register allocation. Most of the implementation effort was directed towards array bounds-checking \nelimination both because bounds-checking is our most significant handicap with respect to the C compil-ers \ncompiling the same programs, and because it is noto- riously difficult to verify the memory safety of \nassembly language programs whose bounds-checking code was elim-inated. Our results in this area are a \nmajor advantage over TIL (Tarditi, Morrisett, Cheng, Stone, Harper, and Lee 1996) and Java (Gosling, \nJoy, and Steele 1996) bytecode verification. The type-safety aspect of the certification is always in-sensitive \nto most optimizations that a compiler might per-form, including all of the above. This is not true for \nthe memory-safety aspect of the certification. The most obvi-ous complication for memory safety is generated \nby array bounds-checking elimination. The only other optimization implemented in our compiler that complicates \nthe certifica- tion of memory-safety is the induction variable elimination in the instance when it replaces \nthe array indexing with a running pointer inside the array. We discuss here only the array bounds-checking \nelimination. 5.1 Array Bounds-Checking Elimination The array bounds-checking elimination is implemented \nin our compiler as an instance of the more general condi-tional elimination, that is, the elimination \nof the condition- als whose boolean expression can be statically proved to be always true or always false. \nThe proof is attempted using a simple decision procedure for linear arithmetic based on computing loop \nresidues (Shostak 1981). The conditional elimination analysis is implemented as a pass through the intermediate \nrepresentation. When a bounds-checking conditional is encountered, its boolean ex- pression is converted \nto the form z -y + c 2 0, where z and 6 Experimental Results y are arbitrary expressions (usually variables) \nand c is a con- stant. This form is submitted to the loop residue decision procedure that returns a value \nsaying that, in the current state, the boolean is always true, or always false, or that its value cannot \nbe determined statically. In the first two cases the conditional is replaced with the code of the appro- \npriate branch, otherwise the boolean expression is recorded in the decision procedure s state and the \ntrue branch is considered recursively. When the true branch is finished, the boolean is retracted and \nits negation is asserted instead for processing the false branch. Because all conditionals involved in \narray bounds-checking are of the form z > 0 or z < y, and because the loop residue is complete for this \nfragment of arithmetic, our compiler is able, in practice, to eliminate almost all bounds checks. There \nare two situations when the above analysis does not succeed in eliminating bounds-checks. One is when \nthe information required for the proof is external to the current function. This happens, for example, \nin the function int subht aC1, int i) {return akl;} because there is no way to verify statically that \ni is a valid index for a. This situation would not occur if the function were inlined at the call site. \nTo cover for the lack of interprocedural analysis, we have extended the language to allow the programmer \nto write simple function preconditions consisting of boolean expres-sions involving the formal parameters. \nFor example, to elim- inate the bounds check in the above function the program- mer can write: int sub(int \na[] , int i) PRECONDITION (0 <= i &#38;&#38; i < length(a)) { return a[il; )  The function preconditions \nare assumed true when analyzing the function but are checked at the call site. The precondi-tions are \na convenient way to hoist the bounds checks out of the function to the call site, where there might be \nmore information for eliminating them. In our experiments, these checks are in most cases eliminated \nby the same conditional elimination phase that eliminates the array bounds checks. Another situation \nwhen the conditional elimination anal- ysis presented above might fail to eliminate bounds-checks is \ninside loops like the one in Figure 3a. In that example, the upper bound of the index is given by the \nloop termi-nation conditional, while the lower bound is implicit. It can be seen from the loop invariant \nin that example that the compiler discovers a lower bound (to 2 0) and emits it as part of the invariant. \nTo deal with such situations the compiler first discovers monotone variables. A variable v is monotone \nif, on all paths through the loop body, it is in- cremented by expressions that are either all positive \nor all negative. To detect monotone variables, the compiler first collects a set of increments for each \nvariable, and then using the same loop-residue decision procedure verifies the sign of the set elements. \nFor a monotone variable with only positive increments, the compiler generates a loop invariant stating \nthat the value of the variable is always greater or equal than the value of the same variable on loop \nentry. This is how the conjunct to > 0 appeared in the invariant annotation of Figure 3b. We have two \npurposes in reporting the results of our exper-iments with the Touchstone certifying compiler. First, \nwe wish to support the claim that we are applying the certifi-cation technique to an optimizing compiler. \nAnd second, we wish to show that the costs of certification are reasonably low. For the first purpose, \nwe compare the running times of several benchmarks compiled by Touchstone against the running times of \nthe same programs compiled with the GNU gee compiler and the vendor-supplied compiler (DEC cc) with all \noptimizations enabled. For the second purpose, we measure the size of proofs and also the time consumed \nfor VC generation, theorem proving, and proof checking. We compare these with the code size and the compilation \ntime respectively. Our benchmark programs depend only on those language features that are currently implemented \nin the certifying compiler (this ruled out floating-point benchmarks, for ex-ample) with a bias towards \nprograms for which array-bounds checking elimination could make a significant difference in the running \ntime. We furthermore preferred programs that might be useful as native-code components in a safe mobile \ncode system, in order to evaluate the certifying compiler as a front-end to a system for safe execution \nof Proof-Carrying Code. These considerations led us to eight benchmarks. Three of them, blur, sharpen, \nand edge are bidimensional convo-lutions used as image processing filters in the xv program. qsort is \nan implementation of the quicksort algorithm for an array of integers. simplex is the linear programming \nal-gorithm implemented for rational numbers. kmp (an imple- mentation of the KMP search algorithm) and \nunpack (one of the gzip decompression algorithms and the core of the Unix utility with the same name) \nwere chosen as examples of cases where array bounds-checking elimination is not ef-fective. The bcopy \nprogram is an implementation of string copy for non-overlapping strings. It is worth noting that some \nof these C programs are fairly realistic in both size and complexity, and none required anything more \nthan mi-nor syntactic modifications to conform to our safe C dialect. The main changes involved replacing \nthe use of pointer arith-metic with array indexing. All results are the average of at least 1000 runs \non a DEC Alpha 21064 running at 175MHz. Figure 8 shows the effect of optimizations on the running time \nof the benchmark programs for the GNU gee compiler, the DEC cc compiler, and the certifying compiler. \nThe C compilers were invoked with all optimizations enabled (-64). The running times are reported as \nspeedups over the run-ning time of the unoptimized code as compiled with gee -00. The last set of bars \nin Figure 8 is the geometric mean of the speedups for each compiler. On the average, the cer-tifying \ncompiler performs slightly better than gee (by about 10%) and not quite as well as cc (the difference \nbeing about 12%). The programs for which the certifying compiler is not quite as good as the C compilers \nare kmp and unpack, due to the bounds checks that cannot be eliminated, and bcopy, because of the lack \nof loop-unrolling in the certifying compiler. In addition to array bounds-checking elimination, the inter-procedural \nregister allocation and the common-subexpression elimination played a major role in making the quality \nof code generated by Touchstone comparable to that produced by the other C compilers. In our experiments, \nthe C compilers compile the pro-grams unsafely (that is, without any bounds checking), while Touchstone \nhas the handicap of having to implement (and Figure 8: The effect of optimizations in the certifying \ncompiler, expressed as the ratio between the running time of the optimized code to the rumring time of \nthe same code compiled with GNU gee -00 . For comparison, we also show for each benchmark the effect \nof the optimizations in the GNU C compiler ( GNU gee -04 ) and the vendor C compiler (UDEC cc -04 ). \nThe last column is the geometric mean over all the benchmarks. Figure 9: Comparison of the compilation \ntime for Touchstone and the GNU gee and DEC cc compilers with all optimizations enabled. The times in \nthe table are shown in milliseconds. On the average, Touchstone is 20% slower than gee and 72% slower \nthan cc. Note that the compilation time does not include VC generation, proof generation or proof checking. \nFigure 10: Comparison of the target code sizes for programs compiled with Touchstone and the GNU gee \nand DEC cc compilers with all optimizations enabled. The sizes in the table are shown in bytes of machine \ncode. On the average, Touchstone is within 5% of the sizes of code emitted by the C compilers. 718 2774 \n17-m 11810 1234 5838 250 1122 Is? 242 272 82s 132 tla 38 102 204 1216 644 24aa 576 2422 64 224 Figure \n11: The relative sizes (in bytes) of proofs, invariants and the machine code. then hopefully remove) \nthe array-bounds checks. The array- bounds checking elimination described in Section 5.1 is able to eliminate \nmost, of those checks whose proof is local to the current function, but is ineffective when the elimination \nrequires global information. This weakness is a problem in all of our benchmarks except for blur, edge \nand bcopy. To substitute for the required global information in these cases, we have added simple one-line \nfunction preconditions to sharpen, qsort and simplex. With the preconditions, our compiler succeeds in \neliminating all bounds-checking op-erations in all but the kmp and unpack benchmarks. What makes these \ntwo benchmarks special is that array indices are computed based on the contents of some auxiliary data \nstructures. The formal safety argument for these array op-erations involves the proof of complicated \nglobal program invariants, and thus it is probably not reasonable to expect a compiler to be able to \neliminate these bounds checks. Even though the preconditions are added to programs only for the benefit \nof the bounds-checking elimination in our compiler, we do not feel that this gives us an unfair ad-vantage \nover the C compilers. To the contrary, the precon- ditions enable more extensive bounds-checking elimination \nand thus make the job of the certifier more difficult. The formal proof of redundancy for the bounds-checks \nthat are eliminated based on preconditions and global information is larger and more complicated than \nfor the locally-provable checks. Our experiments show that the additional bounds-checking elimination \nthat is enabled by the preconditions leads, on the average, to a 7% reduction in code size, a 12% reduction \nof the running time and a 12% increase of the proof sizes. Due to the fact that Touchstone is an early \nprototype, the compilation time is significantly larger than that of the C compilers used in the performance \ncomparisons. Figure 9 shows the compilation times (not including the time for VC generation, proof generation \nor proof checking) of Touch- stone and of the C compilers (with ail optimizations enabled) for our set \nof benchmarks. On the average, Touchstone is 20% slower than GNU gee and 72% slower than DEC cc. Figure \n10 shows the comparison of the machine-code sizes % E I= Figure 12: The distribution of time spent for \nthe compilation and certification of several benchmarks. The data in the table is expressed in milliseconds. \nof programs compiled with Touchstone and the C compil- ers. Unlike the compilation times, the sizes of \nmachine code emitted by Touchstone are within 5% of that emitted by the C compilers. Note, however, that \nthere is no fundamental reason why a certifying compiler should emit code that is larger than that emitted \nby a traditional compiler. With re-spect to the compilation time, the certifying compiler must incur \nthe extra cost of emitting the loop invariants and type specifications. This cost, however, should negligible \nwith respect, to the rest of the compilation effort. Hoping to have convinced the reader that we are \nindeed certifying optimized assembly language, we now move to the presentation of the costs of certification. \nFor this purpose, we have measured the proof size and the time required for VC generation, theorem proving \nand proof checking, for the benchmarks discussed above. Figure 11 shows the sizes of the safety proofs \nand the annotations as compared to the sizes of the machine code for each benchmark. The annotations \nare only 30% of the size of the code, on the average. The average ratio of proof size to code size is \n2.5, which is consistent with our observa-tions in experiments with PCC using hand-written assembly language. \nWhile this factor seems large, one must consider that the proofs are not currently compressed. Preliminary \nmeasurements show that general-purpose compression algo-rithms can decrease the size of proofs by a factor \nof two. However, larger reductions are likely to be obtained by fist optimizing the proof representations \nand then employing a compression algorithm. Further discussion about proof op-timizations is given in \nSection 8. Figure 12 displays graphically the distribution of time spent for compilation and certification. \nOn the average, 72% of the time is spent compiling, 22% is used for the-orem proving and the rest of \n6% is split evenly between VC generation and proof checking. Based on these results we make two observations. \nFirst, the cost of certification is only about a third of the cost of compilation, meaning that it is \nreasonable to use the certifier throughout the life of the compiler, and not just during compiler development. \nSec-ond, not only are VCGen and the proof checker much sim-pler than the compiler and the theorem prover, \nbut they are also much faster. Hence, this safety-critical infrastructure is both small and fast. This \nis important in situations when the certifying compiler is used to produce Proof-Carrying Code, because \nthe system receiving the code needs to trust and run only the VCGen and the proof checker. 7 Related \nWork The idea of checking individual compilations instead of ver- ifying the compiler also appears in \nthe work of Cimatti et al. (Cimatti et al. 1997), though in the much simpler in-stance of a non-optimizing \ncompiler from an expression lan-guage without loops or function calls to an RTL-like lan-guage. On the \nother hand they have the more ambitious goal of verifying full equivalence of the source expression and \nthe target program. The compilation approach presented here resembles in many respects the compilation \nstrategy of the TIL (Tarditi, Morrisett, Cheng, Stone, Harper, and Lee 1996) compiler for Standard ML, \nwhich uses a typed intermediate language that can be easily type-checked to achieve an independent validation \nof optimizations. However, the TIL type-system does not guarantee memory safety in the presence of cer-tain \noptimizations such as array bounds-checking elimina-tion, and furthermore, it cannot be used after the \nregister allocation phase when some variables (registers) are reused to hold values of different types \nin the body of the same function. For this reason, types are dropped in TIL be-fore the register allocation \nphase and thus, no type-checking is possible at the level of the compiler output. The prob-lems related \nto register allocation are solved by Morrisett et al. (Morrisett, Walker, and Crary 1998) by choosing \na more expressive type system, but the issue of memory-safety in the presence of optimizations such as \narray bounds-checking elimination still remains a problem. The purpose and the design of our certifying \ncompiler are also related to the Java (Gosling, Joy, and Steele 1996) compiler and bytecode verifier \n(Lindholm and Yellin 1997) systems. The similarity is that both systems produce code that is annotated \nfor the purpose of enabling a certification system (the bytecode verifier, in the Java case) to verify \nthe type safety. The difference is that our certifier has a more flexible annotation language that permits \nthe verification of arbitrarily optimized assembly language while necessitat-ing fewer annotations. The \nbytecode verifier only works on a specially designed bytecode intermediate language where typing annotations \nare contained in the instruction codes themselves. Furthermore, the Java bytecode verifier pre-vents \nthe compiler from doing several important optimiza-tions, such as array bounds-checking elimination and \nglobal register allocation, since these checks are built in to the def- inition of the byte codes. 8 \nDiscussion and Future Work The approach to a certifying compiler presented in this paper is inspired \nby our work on Proof-Carrying Code (PCC) (Necula 1997; Necula and Lee 1996), and in fact reuses the VCGen, \nthe theorem prover, and the proof checker components of our implementation of PCC. If in-tegration with \nPCC and the generality and simplicity of the certifier were not important to us, we could have chosen \nfrom several alternate implementation approaches. One alternative is suggested by the fact that the verifi- \ncation conditions emitted by VCGen can be proved auto- matically. Thus, one can incorporate parts of \nthe prover in VCGen and prove the VCs as they are encountered, with- out actually generating a safety \npredicate and maybe not even a proof that can be checked. This might be particu- larly practical when \narray bounds are not verified and thus only a small part of the prover is used. The Java bytecode verifier \n(Lindholm and Yellin 1997) can be viewed as taking this approach, as can the type-checker in the typed \nassembly language of Morrisett, et al. (Morrisett, Walker, and Crary 1998). Another variation of the \nmethod presented here is to attempt to certify the output of an off-the-shelf compiler, which does not \nproduce annotations or type specifications. We suspect that this can be achieved by interposing a loop \ninvariant inference phase before VCGen. For the source lan- guage presented here, and for a compiler \nthat does not per- form aggressive global optimizations, it should be possible in principle to discover \nthe typing invariants completely au- tomatically. Our current experimental results show that the proofs \nare about 2.5 times larger than the code. Some preliminary experiments show that standard compression \ntechniques re-duce the proof sizes by a factor of 2. We believe, however, that the biggest gains in reduction \nof size will be obtained by designing and implementing optimizations in the repre-sentation of proofs. \nAlready, our current LF representation provides a simple approach to type reconstruction that al-lows \nsome type information to be elided (Necula and Lee 1997). We are currently exploring more aggressive \ntech-niques that involve finding common subterms (essentially a kind of common subexpression elimination). \nA manual inspection of the proofs gives some indication that such an approach should yield good reductions, \nthough much further work is necessary to measure the effects. In addition to the general notion of certifying \ncompila-tion, we believe that we have discovered a simple correct-ness criterion for both register allocation \nwith spilling and for instruction scheduling. Bugs in these compiler optimiza-tions are notoriously difficult \nto find because they lead to subtle errors in the output that tend to surface as sporadic program failures, \nusually many instructions past the actual erroneous instruction. Furthermore, the low-level nature of \nthe output and the fact that such errors most likely occur in large programs, makes the visual inspection \nof the output quite tedious. We have observed that the result of our symbolic evalu-ator is insensitive \nto global register allocation with spilling and to global code scheduling. During the development of \nour compiler, this has meant that we could verify each run of these transformations simply by comparing \nthe safety pred- icates computed before and after the transformation. To see an example of this, consider \nthe annotated code of Figure 3b. In this code, the register tl is used to hold values of different types \nin the body of the loop. One can now observe that even if the independent uses of rl are renamed, the \nsafety predicate does not change, up to the renaming of bound variables in the predicate. A similar experiment \nshows the same phenomenon in the case of instruction scheduling. To preserve this invariance property \neven in the presence of register spilling, the symbolic evaluator must be extended to interpret a portion \nof the stack frame as an extension of the register file and thus consider the read/write operations to \nthe stack frame as moves from/to these pseudo-registers. To simplify the symbolic evaluator, only memory \nreferences to addresses that are computed as immediate offsets from the dedicated stack pointer (or frame \npointer) register are intercepted; all other memory references must be proved to be within heap-allocated \narrays. We strongly suspect that this observation can form the basis for a general correctness criterion \nfor global register allocation and instruction scheduling, and one that would be useful for any compiler, \nnot only certifying compilers. However, we do not yet have a formal proof of this claim. We hope to make \na more formal statement and proof of this correctness criterion in future work. Conclusion This paper \npresents the design and implementation of a cer- tifying compiler composed of a traditional optimizing \ncom-piler for a typed language and a certifier that automatically produces a proof of type safety for \neach assembly language program resulting from the compilation. The main benefit of such a system over \na traditional compiler is that the cer-tifier acts as an effective referee for the correctness of each \ncompilation, thus simplifying compiler testing and develop- ment. Only rare compilation errors that do \nnot break the type-safety of the target program are not detected by a cer- tifying compiler. During the \ndevelopment of the certifying compiler we have encountered only one such error, as op- posed to a large \nnumber of errors that were caught early by the certifier. The certifier reduced the effort required for \nthe development of an optimizing compiler whose per-formance rivals that of production compilers, to \nonly three man-months. A second important benefit of a certifying compiler is that it can serve as an \nautomatic front-end to a system that uses Proof-Carrying Code to enable the safe execution of untrusted \nmobile code. The main contribution of this research is the design of a certifier that does not restrict \nthe optimizations that the compiler can perform, while requiring only a small amount of information from \nthe compiler. As an indirect result, we have identified that the symbolic evaluation technique that is \nat the base of the certifier leads to a simple but effec-tive correctness criterion for low-level optimizations \nsuch as register allocation and code scheduling. Acknowledgments The authors would like to thank Gary \nLindstrom and Trevor Jim for the helpful comments and suggestions on earlier drafts of this paper. We \nalso thank the anonymous refer-ees for many suggestions on how to improve this paper. References Boyer, \nR. and J. S. Moore (1979). A Computational Logic. Academic Press. Cimatti, A. et al. (1997, June). A \nprovably correct em-bedded verifier for the certification of safety critical software. In Computer Aided \nVerification. 9th Zntema- tionai Conference. Proceedings, pp. 202-213. Springer-Verlag. D.C. Luckham, \ne. (1979, March). Stanford Pascal veri-fier user manual. Technical Report STAN-CS-79-731, Dept. of Computer \nScience, Stanford Univ. Detlefs, D. (1996). An overview of the Extended Static Checking system. In Proceedings \nof the First Formal Methods in Software Practice Workshop. Dybjer, P. (1986). Using domain algebras to \nprove the correctness of a compiler. Lecture Notes in Computer Science (182). Gordon, M. (1985, July). \nHOL: A machine oriented for-mulation of higher-order logic. Technical Report 85, University of Cambridge, \nComputer Laboratory. Gosling, J., B. Joy, and G. L. Steele (1996). The Java Language Specification. The \nJava Series. Reading, MA, USA: Addison-Wesley. Guttman, J. D., J. D. Ramsdell, and M. Wand (1995). VLISP: \na verified implementation of Scheme. Lisp and Symbolic Computation (8), 5-32. Harper, R., F. Honsell, \nand G. Plotkin (1993, January). A framework for defining logics. Journal of the Asso- ciation for Computing \nMachinery 40(l), 143-184. Lindholm, T. and F. Yellin (1997, January). The Java Virtual Machine Specificntion. \nThe Java Series. Read- ing, MA, USA: Addison-Wesley. McCarthy, J. (1963). Towards a mathematical theory \nof computation. In C. M. Popplewell (Ed.), Proceedings of the International Congress on Information Proceas-ing, \npp. 21-28. North-Holland. Moore, J. S. (1989). A mechanically verified language im-plementation. Journal \nof Automated Reasoning (5), 461-492. Morris, F. L. (1973). Advice on structuring compilers and proving \nthem correct. In Proceedings of the First ACM Symposium on Principles of Programming Languages, pp. 144-152. \nMorrisett, G., D. Walker, and K. Crary (1998, January). From System F to typed assembly language. In \nThe 25th Annual ACM Symposium on Principles of Pro-gramming Languages. ACM. To appear. Necula, G. C. \n(1997, January). Proof-carrying code. In The 24th Annual ACM Symposium on Principles of Programming Languages, \npp. 106-119. ACM. Necula, G. C. and P. Lee (1996, October). Safe kernel ex-tensions without run-time \nchecking. In Second Sympo- sium on Operating Systems Design and Zmplementa-tions, pp. 229-243. Usenix. \nNecula, G. C. and P. Lee (1997, October). Efficient rep-resentation and validation of logical proofs. \nTechnical Report CMU-CS-97-172, Computer Science Depart-ment, Carnegie Mellon University. Nelson, G. \nand D. Oppen (1979, October). Simplification by cooperating decision procedures. ACM Tkawac-tions on \nProgramming Languages and Systems l(2), 245-257. Oliva, D. P., J. D. Ramsdell, and M. Wand (1995). The \nVLISP verified PreScheme compiler. Lisp and Sym-bolic Computation (8), 111-182. Owre, S., J. M. Rushby, \nand N. Shankar (1992, June). PVS: A prototype verification system. In D. Kapur (Ed.), 11th International \nConference on Automated Deduction (CADE), Volume 607 of Lecture Notes in Artificial Intelligence, Saratoga, \nNY, pp. 748-752. Springer-Verlag. Shostak, R. (1981, October). Deciding linear inequal-ities by computing \nloop residues. Journal of the ACM 28(4), 769-779. Tarditi, D., J. G. Morrisett, P. Cheng, C. Stone, R. \nHarper, and P. Lee (1996, May). TIL: A type-directed optimizing compiler for ML. In PLDZ 96 Con-ference \non Programming Language Design and Imple- mentation, pp. 181-192. Thatcher, J. W., E. G. Wagner, and \nJ. B. Wright (1980). More on advice on structuring compilers and proving them correct. LNCS 94: Proceedings \nof a Workshop on Semantics-Directed Compiler Generation (94), 165- 188. Young, W. D. (1989). A mechanically \nverified code gen- erator. Journal of Automated Reasoning (5), 493-518.   \n\t\t\t", "proc_id": "277650", "abstract": "This paper presents the design and implementation of a compiler that translates programs written in a type-safe subset of the C programming language into highly optimized DEC Alpha assembly language programs, and a <i>certifier</i> that automatically checks the type safety and memory safety of any assembly language program produced by the compiler. The result of the certifier is either a formal proof of type safety or a counterexample pointing to a potential violation of the type system by the target program. The ensemble of the compiler and the certifier is called a <i>certifying compiler</i>.Several advantages of certifying compilation over previous approaches can be claimed. The notion of a certifying compiler is significantly easier to employ than a formal compiler verification, in part because it is generally easier to verify the correctness of the result of a computation than to prove the correctness of the computation itself. Also, the approach can be applied even to highly optimizing compilers, as demonstrated by the fact that our compiler generates target code, for a range of realistic C programs, which is competitive with both the cc and gcc compilers with all optimizations enabled. The certifier also drastically improves the effectiveness of compiler testing because, for each test case, it statically signals compilation errors that might otherwise require many executions to detect. Finally, this approach is a practical way to produce the safety proofs for a Proof-Carrying Code system, and thus may be useful in a system for safe mobile code.", "authors": [{"name": "George C. Necula", "author_profile_id": "81100295630", "affiliation": "School of Computer Science, Carnegie Mellon University, Pittsburgh, Pennsylvania", "person_id": "PP14109324", "email_address": "", "orcid_id": ""}, {"name": "Peter Lee", "author_profile_id": "81100384353", "affiliation": "School of Computer Science, Carnegie Mellon University, Pittsburgh, Pennsylvania", "person_id": "PP39040384", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/277650.277752", "year": "1998", "article_id": "277752", "conference": "PLDI", "title": "The design and implementation of a certifying compiler", "url": "http://dl.acm.org/citation.cfm?id=277752"}