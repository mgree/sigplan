{"article_publication_date": "05-01-1998", "fulltext": "\n Optimizing direct threaded code by selective inlining Ian Piumarta and Fabio Riccardi INRIA Roquencourt, \nB.P. 105, 78153 Le Chesnay Cedex, France email: ian.piumarta@inria.fr fabio.riccardi(Dinria.fr Abstract \nAchieving good performance in bytecoded language inter-preters is difficult without sacrificing both \nsimplicity and portability. This is due to the complexity of dynamic trans-lation ( just-in-time compilation \n) of bytecodes into native code, which is the mechanism employed universally by high- performance interpreters. \nWe demonstrate that a few simple techniques make it possible to create highly-portable dynamic translators \nthat can attain as much as 70% the performance of optimized C for certain numerical computations. nanslators \nbased on such techniques can offer respectable performance without sacrificing either the simplicity \nor portability of much slower pure bytecode interpreters. Keywords: bytecode interpretation, threaded \ncode, inlin- ing, dynamic translation, just-in-time compilation. 1 Introduction Bytecoded languages \nsuch as Smalltalk [Go183], Cam1 [Ler97] and Java [Arn96, Lin97] offer significant engineer-ing advantages \nover more conventional languages: higher levels of abstraction, dynamic execution environments with incremental \ndebugging and code modification, compact rep-resentation of executable code, and (in most cases) platform \nindependence. The success of Java is due largely to its promise of plat- form independence and compactness \nof code. The com-pactness of bytecodes has important advantages for net-work computing where code must \ndownloaded on-demand for execution on an arbitrary platform and operating sys-tem while keeping bandwidth \nrequirements to a minimum. The disadvantage is that bytecode interpreters typically of-fer lower performance \nthan compiled code, and can consume significantly more resources. Most modern virtual machines perform \nsome degree of dynamic translation to improve program performance jDeu84). Such techniques significantly \nincrease the com-plexity of the virtual machine, which must be tailored for each hardware architecture \nin much the same way as a con- B 1998 ACM 0-89791.987498/0006...$5.00 ventional compiler s back-end. \nThis increases development costs (requiring specific knowledge about the target archi-tecture and the \ntime for writing specific code), and reduces reliability (by introducing more code to debug and support). \nSome of these languages (Cam1 for example) also have more traditional compilers that produce high-performance \nnative code, but this defeats the advantages that comk with platform independence and compactness. We \npropose a novel dynamic retranslation technique that can be applied to a certain class of virtual machines. \nThis technique delivers high performance, up to 70% that of opti- mized C. It is easy to retrofit to \nexisting virtual machines, and requires almost no effort to port to a new architecture. This paper continues \nas follows. The next section gives a brief survey of bytecode interpretation mechanisms, pro-viding a \ncontext for the remainder of the paper. Our novel dynamic retranslation technique is explained in Section \n3. Section 4 presents the results of applying the technique to two interpreters: the small RISC-like \ninterpreter that in-spired this work, and a production virtual machine for Objective Cam]. The final \ntwo sections contrast our tech-nique with related work and present some final conclusions. 2 Background \nInterpreter performance can depend heavily on the repre-sentation chosen for executable code, and the \nmechanism used to dispatch opcodes. This section describes some of the common techniques. 2.1 Pure bytecode \ninterpreters The inner loop of a pure bytecode interpreter is very simple: fetch the next bytecode and \ndispatch to the implementation using a switch statement. Figure 1 shows a typical pure bytecode interpreter \nloop, and an array of bytecodes that calculate 3 + 4 (we will use this as a running example). The interpreter \nis an infinite loop containing a switch statement to dispatch successive bytecodes. Each case in the \nbody of the switch implements one bytecode, and passes control to the next bytecode by breaking out of \nthe switch to pass control back to the start of the infinite loop. Assuming the compiler optimizes the \njump chains from the breaks through the implicit jump at the end of the for body back to its beginning, \nthe overheads associated with this approach are as follows: l increment the instructionPointer; l fetch \nthe next bytecode from memory; compiled code: unsigned char codo[] = 1 . . . , bytecode-push3, bytscode-push4. \nbytecode-add. . . . 1:  bytecode implementations: unsigned char *instructionPointer p code -1; for (;;I \ni unsigned char bytecode = +++instructioaPointer; switch (bytecods) < /* . . . */ case bytecoda-push3: \n*++stackPointe+ = 3; break; case bytecode-push4: *++stackPointer = 4; break; case bytecoda-add: --stsckPointsr; \nl atackPointar += stackPointer[l] ; break; /a . . . */ 1 1 Figure 1: Pure bytecode interpreter. l a \nredundant range check on the argument to switch; l fetch the address of the destination case label from \na table; l jump to that address; and then at the end of each bytecode: l jump back to the start of the \nfor body to fetch the next bytecode. Eleven machine instructions must be executed on the Pow-erPC to \nperform the push3 bytecode. Nine of these instruc- tions are dedicated to the dispatch mechanism, including \ntwo memory references and two jumps (among the most ex-pensive instructions on modern architectures). \nPure bytecoded interpreters are easy to write and under- stand, and are highly portable -but rather slow. \nIn the case where most bytecodes perform simple operations (as in the push3 example) the majority of \nexecution time is wasted in performing the dispatch. 2.2 Threaded code interpreters Threaded code was \npopularized by the Forth programming language [Moo70]. There are various kinds of threaded code, the \nmost efficient of which is generally direct threading [Ert98]. Bytecodes are simply integers: dispatch \ninvolves fetch-ing the next opcode (bytecode), looking up the address of the associated implementation \n(either in an explicit table, or implicitly using switch) and then transferring control to that address. \nDirect threaded code improves performance by eliminating this table lookup: executable code is repre- \nsented as a sequence of opcode implementation addresses, and dispatch involves fetching the next opcode \n(implemen-tation address) and jumping directly to that address. An additional optimization eliminates \nthe centralized dis-patch. Instead of returning to a central dispatch loop, each direct threaded opcode \ns implementation ends with the code compiled code: void *cods[] = ( . . . . &#38;topcods_push3, kkopcodevpush4, \ntlopcode-add, . . . 1; opcode implementatione: /+ dispatch next instruction */ #define NF.XT() goto \n*+++instructionPointar void **instructionPointer = code -1; /+ start execution: dispatch first opcode \n*/ NEXT0 i /* opcode implementations... */ opcode-push3: *++atackPointer = 3; NEXTO; opcode-push4: l \n++stackPointer = 4; NEW); opcode-add: --stackPointer; **tackPointar += atackPointor[il; NEXT0 ; /+ . \n. . */ Figure 2: Direct threaded code. required to dispatch the next opcode. The direct threaded version \nof our 3 + 4 example is shown in Figure 2. Execution begins by fetching the address of the first op-code \ns implementation from the compiled code and then jumping to that address. Each opcode performs its own \nwork, and then dispatches to the next opcode implied by the compiled code. (Hence the name: control flow \nthreads its way through the opcodes in the order implied by the compiled code, without ever returning \nto a central dispatch loop.) The overheads associated with threaded code are much lower than those associated \nwith a pure bytecode inter-preter. For each opcode executed, the only additional over-head is dispatching \nto the next opcode: l increment the instructionPointer; l fetch the next opcode address from memory; \nl jump to that address. Five machine instructions are required to implement push3 on the PowerPC. Three \nof these are associated with opcode dispatch, with only one memory reference and one jump. We have saved \nsix instructions over the pure bytecode approach. Most importantly we have saved one memory reference \nand one jump instruction (both of which axe ex- pensive).  2.3 Dynamic translation to threaded code \nThe benefits of direct threaded code can easily be obtained in a bytecoded language by translating the \nbytecodes into direct threaded code before execution. This is illustrated in Tlw threaded code examples \nare written using the first-class la-bels provided by GNU C. The expression void *addr = C&#38;label \nas-signs the address (of type void + ) of the statement attached to the given label to addr. Control \ncan be transferred to this location using a goto that dereferences the address: goto *ad&#38; . Note \nthat gee s first-class labels are not required to implement these techniques: the same effects can be \nachieved with a couple of macros containing a few lines of a*m. translation table: variable, add, store \nvariable can be translated as a single void *opcodesCI ; /* . . . */ opcodesIbytscode_pu~bU = kkopcodswpush3; \nopcodes[bytecode_push4] = kkopcode-puah4; opcodeaCbytmcode_addI = kkopcode-add; /* . . . */ dynamic \ntranslator: unsigned char *bytecodePointer = first.Bytacods; void **opcodaPointor Q translatedCodeForFunction; \nwhile (moreBytecodeaToTranslate) ropcodePointer++ = opcodes[*bytecodePointer+tl; Figure 3: Dynamic translation \nof bytecodes into thre- aded code. Figure 3. The translation loop reads each bytecode, looks up the \naddress of its implementation in a table, and then writes this address into the direct threaded code. \nThe only complication is that most bytecode sets have extension bytes. These provide additional information \nthat cannot be encoded within the bytecode itself: branch offsets, indices into literal tables or environments, \nand so on. These extension bytes are normally placed inline in the translated threaded code by the translator, \nimmediately after the thre- aded opcode corresponding to the bytecode. Translation to threaded code permits \nother kinds of op- timization. Smalltalk, for example, contains a range of six bytecodes for pushing \nan implicit integer constant (between -1 and +2) onto the stack. The translator loop could easily translate \nthese as a single pushLiteral opcode followed by the constant to be pushed as an inline operand. The \nsame treatment can be applied to other kinds of literal quantity, relative branch offsets, and so on. \nAnother possibility is partial decoding , where the translator loop examines an overloaded bytecode \nat translation time, and translates it into one of several threaded opcodes. The translator loop must \nbe aware of the kind of operand that it is copying. A relative offset, for example, might require modification \nor scaling during the translation loop. It is possible to make an approximate evaluation of this approach \nin a realistic system. Squeak [Ing97] is a portable pure bytecode implementation of Smalltalk-80; it \nper-forms numerical computations at approximately 3.7% the speed of optimized C. BrouHaHa [Mir87] is \na portable Small-talk virtual machine that is very similar to the Squeak VM, except that it dynamically \ntranslates bytecodes into direct threaded code for execution [MirSl]. BrouHaHa performs the same numerical \ncomputations at about 15% the speed of optimized C. Both implementations have been carefully hand-tuned \nfor performance; the essential difference between them is the use of dynamic translation to direct threaded \ncode in BrouHaHa. 2.4 Optimizing common bytecode sequences Bytecodes can typically only represent 256 \noperations. Thre-aded opcodes can represent many more, since they are en- coded as pointers. Translating \nbytecodes into threaded code therefore gives us the opportunity to make arbitrary trans-formations on \nthe executable code. One such transformation is to detect common sequences of bytecodes and translate \nthem as a single threaded macro opcode; this macro op-code performs the work of the entire sequence of \noriginal bytecodes. For example, the bytecodes push literal, push add-literal-to-variable opcode in the \ntranslated threaded code. Such optimizations are effective because they avoid the overhead of the multiple \ndispatches that are implied by the original bytecodes (but elided within the macro opcode). A single \nmacro opcode that is translated from a sequence of N original bytecodes avoids N -1 opcode dispatches \nat execution time. This technique is particularly important in cases where the bytecodes are simple (as \nin the 3 + 4 example), when the implementation of each bytecode can be as short as as single register-register \nmachine instruction. The cost of threading can often be significantly larger than the cost of useful \nexecution. If three instructions must be executed to dispatch to the next opcode then the overhead for \nthis threading for 3 + 4 is 75% (four useful instructions exe-cuted and 12 instructions for dispatching \nthe threaded op-codes). This overhead drops to 43% when the operation is optimized into a single macro \nopcode (four useful instruc-tions and 3 instructions for threading). Dispatching to opcode implementations \nat non-contig-uous addresses also undermines code locality, causing un-necessary processor pipeline stalls \nand inefficient utilization of the instruction cache and TLBs. Combining common seq-uences of bytecodes \ninto a single macro opcode considerably reduces these effects. The compiler will also have a chance to \nmake inter-bytecode optimizations (within the implementa- tion of the single macro opcode) that are impossible \nto make between the implementations of the individual bytecodes. Determining an appropriate set of common \nbytecode seq-uences is not difficult. The virtual machine can be instru- mented to record execution traces, \nand a simple offline anal-ysis will reveal the likely candidates. The corresponding pat-tern matching \nand macro opcode implementations can then be incorporated manually into the VM. For example, such analysis \nhas been applied to an earlier version of the Objec- tive Cam1 bytecode set, resulting in a new set of \nbytecodes that includes several macro-style operations. 2.5 Problems with static optimization The most \nsignificant problem with this static approach is that the number of possible permutations of even the \nshortest common sequences of consecutive bytecodes is pro- hibitive. For example, Smalltalk provides \n4 bytecodes to push the most popular integer constants (minus one through two), and bytecodes to load \nand store 32 temporary and 256 receiver variables. Manually optimizing the possible per-mutations for \nincrementing and decrementing a variable by a small constant would require the translator to implement \nexplicitly 2304 special cases. This is clearly unreasonable. The problem is made more acute since different \nappli-cations running on the same virtual machine will favor dif-ferent sequences of bytecodes. Statically \nchasing a single optimal set of common sequences is therefore impossible. Our technique focuses on making \nthis choice at runtime, which allows the set of common sequences to be optimal for the particular application \nbeing run. Instruction counting is not a very accurate way to estimate the savings, since the instructions \nthat we avoid are some of the most expensive to execute. dynamic-opcode-push3-pushrl-add: *++stackPointer \n= 3; *++stackPointer = 4; stackPointsr--; rata&#38;Pointer += stackPointterC11; goto **++instructionPointar; \n Figure 4: Equivalent macro opcode for push3, push4, add. int nfibscint n) : nfibs(n -2) + nfibs(n -1) \n+ 1; > Figure 5: Benchmark function in C. 3 Dynamically rewriting opcode sequences We generate implementations \nfor common bytecode sequen-ces dynamically. These implementations are available as new macro opcodes, \nwhere a single such macro opcode re-places the several threaded opcodes generated from the orig- inal \ncommon bytecode sequence. These dynamically gen-erated macro opcodes are executed in precisely the same \nmanner as the interpreter s predefined opcodes; the original execution mechanism (direct threading) requires \nno modifi- cation at all. The transformation can be performed either during bytecode-to-threaded code \ntranslation, or as a sepa- rate pass over already threaded code. Figure 4 shows the equivalent C for \na dynamically gen-erated threaded opcode for the sequence of three bytecodes needed to evaluate the 3 \n+ 4 example. The translator concatenates the compiled C implemen- tations for several intrinsic threaded \nopcodes, each one cor- responding to a bytecode in the sequence being optimized. Since this involves \nrelocating code, it is only safe to perform this concatenation for threaded opcodes whose implementa- \ntion is position independent. In general there are three cases to consider when concatenating opcode \nimplementations: l A threaded opcode cannot be inlined if its implemen-tation contains a call to a C \nfunction, where the desti- nation address is relative to the processor s PC. Such destination addresses \nwould be invalidated as they are copied to form the new macro opcode s implementa-tion. l Any threaded \nopcode that changes the flow of control through the threaded code must only appear at the end of a translated \nsequence. This is because different paths through the sequence might consume different numbers of inline \narguments. l Any threaded opcode that is a branch destination can only appear at the beginning of a macro \nopcode, since incorporating it into the middle of a macro opcode would delete the branch destination \nin the final thre-aded code. The above can be simplified to the following rule: we only consider basic \nblocks for inlining, where a basic block begins with a jump destination and ends with either a jump Figure \n6: Threaded code for nf ibs benchmark, before inlining. nfibs: push l-1 ; rl saved during call move m2 \nrl ; if brg < 2) jsa l-0 rl a =cont POP rl ; restore rl return 91 ; rwJrn 1 cone : more t-0 l-1 ; &#38;I* \narg -> rl sub t1 ro ; call nfibs(ar,yl) Cdl 0 wfibs *\"SIP ra rl ; nfibs(arg-1) -> rl, arg -> r0 sub 1)2 \nl-0 ; call nfiba(aeg-2) call a anfibs add r1 ro ; nfibs(arg-2) + 1 -> r0 add t1 ro ; nfibscarg-1) + nfibs(arg-2) \n+ 1 -> r0 POP X9 ; rsstore rl rsturn r0 ; return nfibm(arg-1) + nfibs(a rg-2) + 1 start: mmw it32 1.0 \n; call nfibo(32) call 0 =nfibs print r0 ; print result halt ; atop destination or a change of control \nflow. For inlining pur-poses, opcodes that contain a C function call are considered to be single-opcode \nbasic blocks. (This restriction can be relaxed if the target architecture and/or the compiler used to \nbuild the VM uses absolute addresses for function call destinations.) Our technique was designed for \n(and works best with) fine-grained opcodes, where the implementations are short (typically a few machine \ninstructions) and therefore the cost of opcode dispatch dominates. The next section presents an example \nin such a context. 3.1 Simple example We will illustrate our technique by applying it to a simple RISC-like,, \nvirtual machine executing the nfibs func-tion, as shown in Figure 5.s Our example interpreter implements \na register-based ex-ecution model. It has a handful of registers for performing arithmetic, and a stack \nthat is used for saving return ad-dresses and the contents of clobbered registers during sub-routine \ncalls. The direct threaded code has two kinds of in- line operand: instruction pointer-relative offsets \nfor branch destinations, and absolute addresses for function call desti-nations. The interpreter translates \nbytecodes into threaded code in two passes. It makes a first pass over the bytecodes, expanding them \ninto threaded opcodes with no inlining, ex-actly as explained in Section 2.3. Figure 6 shows a symbolic \nlisting of the nf ibs function, implemented for our example interpreter s opcode set, after this initial \ntranslation into threaded code. Bytecode operands are placed inline in the threaded code during translation. \nFor example, the offset for the jge op-code and the call destinations are placed directly in the opcode \nstream, immediately after the associated opcode. These are represented as the pseudo-operand % in the \nfig- This doubly-recursive function has the interesting property that its result is the number of function \ncalls required to calculate the result. nfiba: Xl -b { pu&#38; rl, EIOWS2 rl, jga dl rl 0, <thr> ) =cont \n%2 + < pop rl, return 11, <thr> ) cent: %3 + { move r0 rl, nub ll r0, call 0, <thr> 1 =nfibs %4 + { swap \nr0 rl, sub t2 r0, call 0, <thr> ) =nfiba 16 --+ t add rl r0, add 11 r0, pop rl, return r0, <thr> ) Figure \n7: Threaded code for nfibs benchmark, af-ter inlining. The implementations of the new macro opcodes are \nshown on the right. ure, and appear on a separate line in the code prefixed with 2 After this initial \ntranslation to threaded code, a second pass performs inlining on the threaded code: basic blocks are \nidentified, used to dynamically generate new threaded macro opcodes, and the corresponding original sequences \nof threaded opcodes are replaced with single macro opcodes. The rewriting of the threaded code can be \nperformed in-situ, since optimizing an opcode sequence will always result in a shorter sequence of optimized \ncode; there is no possibility of overwriting an opcode that has not yet been considered for inlining. \nFigure 7 shows the code for the nfibs function after in-lining has taken place. The function has been \nreduced to five threaded macro opcodes (shown as Xl through 35 )) each replacing a basic block in the \noriginal code. The imple-mentation of each new macro opcode is the concatenation of the implementations \nof the opcodes that it replaces. These new implementations are written in a separate area of mem- ory \ncalled the macro cache. Five such implementations are required for nf ibs, and are shown within curly \nbraces in the figure. Each one ends with a copy of the implementation of the pseudo-opcode <thr>, which \nis the threading operation to dispatch the next opcode. Inline arguments are copied verbatim, except \nfor cant (a jump offset) which is adjusted appropriately by the transla- tor. (These inline arguments \nare used by the macro opcode implementations at the points marked with 0 in the figure.) To help with \nthe identification of basic blocks, we classify our threaded opcodes into four classes, as follows: INLINE \n-the opcode s implementation can be inlined into a macro opcode without restriction (the srith-metic \nopcodes belong to this class); PROTECT - the implementation contains a C function call and therefore \ncannot be inlined (the print opcode belongs to this class); FINAL -the opcode changes the flow of control \nand therefore defines the end of a basic block (e.g. the call opcode); RELATIVE -the opcode changes the \nflow of control and therefore defines the end of a basic block (e.g. the conditional branch jge). The \nonly difference between FINAL and RELATIVE is the way in which the opcode s inline operand is treated. \nIn the first case the operand is absolute, and can be copied directly into the final translated code. \nIn the second csse the operand is relative to the current threaded program counter, and so must be adjusted \nappropriately in the final translated code. Figure 8 shows the translator code that initializes the threaded \nopcode table, along with representative implemen-tations of several of our threaded opcodes (each of \nthe four classes of threaded opcode is represented). M*fin* PUSH(I) (*++sp = (long)(X)) Ildafia* POP0 \n(rap--) #define GET0 ((Iong)(t++ip)) /* read inlins operand */ Idafine NEXT0 goto **++ip /+ dispatch \nnaxt opcoda */ tdafine PRGTECT (O&#38;J) /* never expanded */ #define INLINE (l<(O) /* axpand.d */ \n#define FINAL (l<(l) /* expanded, enda a basic block */ #define RELATIVE (1~2) /* arpanded, ends a basic \nblock, offa*t follows +/ #define OP(NANE, NARGS, FLAGS) \\ caaa WANE: \\ I\\ info[op].nargs = NARGS; info \nCop1.flags = FLAGS; \\ info[opJ.addr = kkltart-llONAI(E; \\ info[op].end = Lkend-WNANE; \\ info[op].six. \n= (int)kkend-WNANE -(int)kkstart-**NAa; \\ if (!initialIP) break; \\ start-ttNAl4E: /+ opcode body */ \n#define END(NAME) \\ end-#tNANE: NEXT0 \\ 1 /* initialize rather than execute (see macro 'OP') */ initialIP \n= 0; for (int op = FIRST-OPCODE; OP <= LAST-OPCOW ++Op) switch (0;) < OP(add-l_rO, 0, INLINE) I ro += \n1; END(add-l-r0); ) OP(mul-rl_rO, 0, INLINE) { ro *= rl; FND(mul-rl-r0); ) DP(jga-rO-rl, 1, RgLATIVE) \ni register long offsot = GETO; if b-0 >= rl) in +-offs*t: END(jge-rO_ri);') OP(cal1. 1. FINAL) < register \nlong dest = GETO; PUHE(ip); = (void *t)deat -1; OP(raturn-r0, 0, FINAL) i ip -(void **)POP(); END(raturn-ro) \n: ) OP(print-r0, 0, PRGTECT) ( printf(\"Xld\\n;'; i0); gND(printJ0); 1 default: fprintf(stdarr, \"panic: \nop Xd is undefined!\\n\", 0~); abort(); 1 Figure 8: Opcode table initialization. The translator s inlining \nloop is shown in Figure 9. It is not as complex as it might first appear. code is a pointer to the translated \nthreaded code, which is rewritten in-situ. in and out are indices into code pointing to the next opcode \nto be copied (or inlined) and the location to which it will be copied, respectively (in >= out at all \ntimes). The loop considers each in opcode for inlining: the inlin- ing loop is entered only if both the \ncurrent opcode and the opcode following it can be inlined. If this is not the csse, the opcode at in \nis copied (along with any inline arguments) directly to out. nextMacro is a pointer to the next unused \nlocation in the macro cache. The inlining loop first writes this address to out (it represents the opcode \nfor the macro opcode implementation that is about to be generated), and then copies the compiled implementations \nof opcodes from in into the macro cache. The inlined threaded opcodes are not copied, although any inline \narguments that are encountered are copied directly to out. The inlining loop continues until it copies \nan opcode that either explicitly ends a basic block or cannot be inlined, or until the next opcode is \na branch destination (implying the int in = 0, out = 0; while (thisOp = codeCin1) i int nextIn = in + \n1 + infoCthisOpl.narga; long nsrtOp = codaCnaxtIn1; relocationa [id] = out; if (info[thisOp] .flaga == \nINLINE CC info[nertOpl .flags != PROTECT kk !destination[nertIn]) { /t CAN INLINE: create new macro opcode \nat nextMacro C/ void *ep = nextMacro; codelout++] = (long)sp; /* new macro opcode */ while (info[thisOpl.flags \n!= PROTECT) < icopy(info[thisOpl.addr, sp, info[thisOp) .sizs); BP += infoCthisOp1 .aize; ++in; /* skip \nopcode */ if (info[thisOpl .flags == RELATIVE) < patcbList[patchIndex++] = out; /+ locn of offset */ \ncode[out++I = in + 1 + code[inl; /* original destn */ ++in; ) else c for (int i = info[thisOp] .nargs; \ni > 0; --i) code [out++] = code [in++] ; 1 if (info[thisOpl .flaga == FINAL I I info[thisOpl .flags \n== RELATIVE i 1 destination[in]) break; /* end of basic block t/ thiaOp = code [in] : 1 /L copy threading \noperation t/ icopy(info[thr] .addr, p, info[tbr] .aize); ep += infoCthr3. size; nextltacro = ap; ) else \n( I* CAN T INLINE: copy opcode and inline arguments t/ code[out++I = (long)info[thisLlpl .addr; ++in; \n/t skip opcode */ if (info[thisOpl .flags == RELATIVE) { patchlist [patchIndex++] = out; code[out++] \n= in + 1 + code[inl; ++in; ) else I /* copy literal arguments */ for (int i = infoCthiaOp1 .nargs; i \n> 0; --i) code [out++] = code [in++] ; > I 1 Figure 9: Dynamic translator loop. end the current basic \nblock). The translator then appends the implementation of the pseudo-opcode thr, which is the threading \noperation itself. The nextMacro location is then updated ready for the next inlining operation. The translator \nloop uses an array of flags destina-tion,, that identifies branch destinations within the thre-aded code. \nThis array is easily constructed during the trans- lator s first pass, as bytecodes are expanded into \nnon-inlined threaded code. The loop also creates two arrays, reloca-tions and patchlist, that are used \nto recalculate relative branch offsets.4 The inlining loop concatenates opcode implementations using \nthe icopy function, shown in Figure 10. This function is similar to bcopy except that it also synchronizes \nthe pro- cessor s instruction and data caches to ensure that the new macro opcode s implementation is \nexecutable. It contains the only line of platform-dependent code in our interpreter. 4TIle branch destination \nidentification and relative offset recalcu-lation are not shown here. These can be seen in the full source \ncode for the example interpreter, which is available online. (See Section 6.) static inline void icopy(void \n*source, void *dent., size-t size) t bcopy(aource, dent, size) ; while (size > 0) ( Xii definod(PPC) \nam ( dcbst 0,X0; qnnc; icbi 0,X0; isync :: r (p)); telif def ined(--spare) asm ( flush X0; atbar :: \n 9 (p) 1 ; Oelif def inad{--i386) /* no-op */ telif definedc...) . . . tendif dest += 4: size -= 4; \nFigure 10: The icopy function, containing the single line of platform-dependent code.  3.2 Saving space \nTranslating multiple copies of the same opcode sequences would waste space. We therefore keep a cache \nof dynami- cally generated macro opcodes, keyed on a hash value com-puted from the incoming (unoptimized) \nopcodes during translation. In the case of a cache hit we reuse the exist-ing macro opcode in the translated \ncode, and immediately reclaim the macro cache space occupied by the newly trans-lated version. In the \ncase of a cache miss, the newly gen-erated macro opcode is used in the translated code and the hash table \nupdated to include this opcode. This ensures that we never have more than one macro opcode corresponding \nto a given sequence of unoptimized opcodes. 4 Experimental results We are particularly interested in \nthe performance benefits when dynamic inlining is applied to interpreters with fine-grain instruction \nsets. Nevertheless, we were also curious to see how the technique would perform when applied to an interpreter \nhaving a more coarse-grained bytecode set. We took measurements in both of these contexts, using our \nown RISC-like interpreter and the widely-used (but less suited) interpreter for the Objective Cam1 language. \n 4.1 Fine-grained opcodes Our RISC-like interpreter has an opcode set similar to that presented in Section \n3.1. It can be configured (at compile time) to use bytecodes, direct threaded code, or direct thre-aded \ncode with dynamically-generated macro opcodes. The performance of two benchmarks was measured using this \nin- terpreter: the function-call intensive Fibonacci benchmark presented earlier (nfibs), and a memory \nintensive, function call free, prime number generator (sieve). Table 1 shows the number of seconds required \nto execute these benchmarks on several architectures (133MHz Pen-tium, SparcStation 20, and 200MHz PowerPC \n603ev). The figures shown are for a simple bytecode interpreter, the same interpreter performing translation \ninto direct threaded code, then direct threaded code with dynamic inlining of common opcode sequences, \nand finally the benchmark written in C and compiled with the same optimization options (-02) as our interpreter. \nThe final column shows the performance of the inlined threaded code compared to optimized C. nf ibs \n machine bytecode threaded inlined C inlined/C Pentium 63.2 37.1 22.3 11.1 49.8% spare 93.6 51.4 24.9 \n18.1 72.7% PowerPC 40.6 20.3 10.4 6.0 57.7% sieve machine bytecode threaded inlined C Mined/C Pentium \n25.1 17.6 13.2 4.6 34.8% spare 41.5 23.9 15.1 4.4 29.1% PowerPC 24.0 12.8 8.7 2.4 27.6% Table 1: nfibs \nand sieve benchmark results for the three architectures tested. The final column shows the speed of the \ninlined threaded code relative to op- timized C. %C nfibs . . . . . . . . . . . . . . . . . . . . . . \n. . . . . . . . . . . . . . . . Pentium spare PowerPC %C sieve . . . . . . . . . . . . . . . . . . . \n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40 20 0 Pentium \nspare PowerPC El bytecode m direct threaded H inlined Figure 11: Benchmark performance relative to optimized \nC. nf ibs spends much of its time performing arithmetic be-tween registers. Memory operations are performed \nonly dur-ing function call and return. Our interpreter allocates the first few VM registers in physical \nmachine registers whenever possible. The opcodes that perform arithmetic are therefore typically compiled \ninto a single machine instruction on the Spare and PowerPC. These two architectures show a marked improvement \nin per- formance when common sequences are inlined into single macro opcodes, due to the significantly \nreduced ratio of op- code dispatch to real work. The effect is less pronounced on the Pentium, which \nhas so few machine registers that all the VM registers must be kept in memory. Each arith-metic opcode \ncompiles into several Pentium instructions, and therefore the ratio of dispatch overhead to real work \nis lower than for the RISC architectures. We observe a marked improvement (approximately a fac- tor of \ntwo) between successive versions of the interpreter for nf ibs. sieve shows a less pronounced improvement \nbecause it spends the majority of its time performing memory opera-tions. The contribution of opcode \ndispatch to the overall execution time is therefore smaller than with nf ibs. It is also interesting \nto observe the performance of each version of the interpreter relative to that of optimized C. Figure \n11 shows that nfibs gains approximately 14% the speed of optimized C when moving from a bytecoded rep-resentation \nto threaded code. The gain when moving from threaded to inlined threaded code is more dependent on the \narchitecture: approximately 20% for the Pentium, and 38% for the Spare. The gains for sieve are both \nsmaller and less dependent on the architecture: approximately 9% at each step, for all three architectures. \n 4.2 Objective Cam1 We also applied our technique to the Objective Cam1 byte- code interpreter, in order \nto obtain realistic measurements of its performance and overheads in a less favorable environ-ment. Objective \nCam1 was chosen because the design and im- plementation of the interpreter s core is clean and simple, \nand so understanding it before making the required modifi-cations did not present a significant challenge. \nFurthermore it is a fully-fledged system that includes a bytecode com-piler, a benchmark suite, and some \nlarge applications. This made it easier to collect meaningful statistics. The interpreter is also equipped \nwith a mechanism to bulk-translate the bytecodes into threaded code at startup (on those platforms that \nsupport it). We needed only to extend this initial translation phase to perform the analysis of opcode \nsequences, generate macro opcode implementa-tions, and rewrite the threaded code in-situ to use these \ndynamically-generated macro opcodes. Implementing our technique for the Cam1 virtual machine took one \nday. There were only two small details that required careful attention. The first was the presence of \nthe SWITCH opcode. This performs a multi-way branch, and is followed in the threaded code by an inline \ntable mapping values onto branch offsets. We added a special case to our translator loop to handle this \nopcode. The second was the existence of a handful of opcodes that consume two inline arguments (a literal \nand a rela-tive offset). We introduced a new opcode class RELATIVE2 for these, which differs from RELATIVE \nonly by copying an additional inline literal argument before the offset in the translator loop. Our translation \nalgorithm was identical in all other re-spects to the one presented in Section 3. We ran the standard \nObjective Cam1 benchmark suite with our modified VM (see Table 2). The VM was in-strumented to gather \nstatistics relating to execution speed, It uses gee s first-class labels to do this portably. eftP://ftP.inria.fr/INRIA/Project~/cri~tal/Xavi~r.Leroy/ \nbenchmarka/objcaml.tar.ge I Pentium m Figure 12: Objective-Cam1 benchmark results for the three relative \nto the original (non-inlining) interpreter. Asterisks checking disabled. term processing, function calls \nfib integer arithmetic, function calls (1 arg) genlex lexing, parsing, symbolic processing kb term processing, \nfunction calls, functionals qsort integer arrays, loops sieve integer arithmetic, list processing, functionals \nsoli puzzle solving, arrays, loops take integer arithmetic, function calls (3 args, curried) taku integer \narithmetic, function calls (3 args, tuplified) Table 2: Objective Cam1 benchmarks. memory usage, and \nthe characteristics of dynamically gen-erated macro opcodes. Figure 12 shows the performance of the benchmarks \nafter inlining, relative to the original performance without inlin-ing. It is important to note that \nthe Objective Caml byte-code set has already been optimized statically, as described in Section 2.4 [Ler98]. \nAny further improvements are there- fore due mainly to the elimination of dispatch overhead in common \nsequences that are particular to each application. Virtual machines whose bytecode sets have not been \nstat-ically optimized in this way would benefit more from our technique. Figure 12 shows that the majority \nof benchmarks benefit from a significant performance advantage after inlining. In most cases the inlined \nversion runs more than 50% faster than the original, with two of the benchmarks running twice as fast \nas the original non-inlined version. It is clear that the improvements are related to the pro- cessor \narchitecture. This is probably due to differences in the cost of the threading operation. On the Spare, \nfor ex-ample, avoiding the pipeline stalls associated with threading seems to make a significant difference. \nFigure 13 shows the final size of the macro cache for each benchmark on the Spare, plotted as a factor \nof the size of the original (unoptimized) code. The final macro cache sizes vary slightly for each architecture, \nsince they depend spare I---I PowerPC architectures tested. The vertical axis shows the performance \nindicate versions of the benchmarks compiled with array bounds 1 10 100 1000 original code size (kbytes) \nFigure 13: Macro cache size (diamonds) and opti- mized threaded code size (crosses), plotted as a factor \nof the original code size. on the size of the bytecode implementations. However, the shape is the same \nin each case. The average ratios of original bytecode size to the macro cache size show that the cost \nis between three and four times the size of the original code on the Spare. (The ratio is almost identical \nfor the PowerPC, and slightly smaller for the Pentium.) We observe that this ratio decreases gradually \nas the original code size increases. This is to be expected, since larger bodies of code will tend to \nreuse macro opcodes rather than generating new ones. We tested this by translating the bytecoded version \nof the Objective Cam1 compiler: 421,532 bytes of original code generated 941,008 bytes of macro op-code \nimplementation on the Spare. This is approximately 2.2 times the size of the original code, and is shown \nas the rightmost point in the graph. Inlined threaded code is always smaller than the original code from \nwhich is generated. Figure 13 also shows the final optimized code size for each benchmark. We observe \nthat 298 the ratio is independent of the size of the benchmark. This is also to be expected, since the \nreduction in size is dependent on the average number of opcodes in a common sequence and the density \nof the corresponding macro opcodes in the final code. These depend mainly on the characteristics of the \nlanguage and its opcode set. Some systems have a long-lived object memory, and gen- erate new executable \ncode at runtime. A realistic implemen-tation for such systems would recycle the macro cache space, and \npossibly use profiling to optimize only popular areas of the program. For example, the 6804OLC emulator \nfound on Macintosh systems performs dynamic translation of 68040 into PowerPC code; it normally requires \nonly 250Kb of cache in which the most commonly used translated code sequences are stored [Tho95]. A similar \n(fixed) cache size is effective in the BrouHaHa Smalltalk system [Mir97]. Translation speed is also an \nimportant factor. To mea-sure this we ran the Object Cam1 bytecode compiler (a much larger program than \nany of the benchmarks) with our modi-fied interpreter. The 105,383 opcodes of the Objective Cam1 compiler \nare translated in 0.22 seconds on the Spare, a rate of 480,000 opcodes per second. The inlining interpreter \nex-ecutes the compiler at a rate of 2.4 million opcodes per sec-ond. Translation is therefore approximately \nfive times slower than execution. 5 Related work BrouHaHa and Objective Cam1 have both demonstrated the \nbenefits of creating specialized macro opcodes that perform the work of a sequence of common opcodes. \nIn Objective Cam1 this led to a new bytecode set. In BrouHaHa the standard Smalltalk-bytecodes are translated \ninto thre-aded code for execution; the detection of a limited number of pre-determined common bytecode \nsequences is performed during translation, and a specialized opcode is substituted in the executable \ncode. Our contribution is the extension of this technique to dynamically analyze and generate imple-mentations \nfor new macro opcodes at runtime. Several systems use concatenation of pre-compiled seq-uences of code \nat runtime [Aus96, NoegG], but in a com-pletely different context. Their precompiled code sequences are \ngeneric templates that can be parameterized at run-time with particular constant values. A template-based \napproach is also used in some com-mercial Smalltalk virtual machines that perform dynamic compilation \nto native code [Mir97]. However, this technique is complex and requires a significant effort to implement \nthe templates for a new architecture. An interesting system for portable dynamic code gener- ation is \nvcode [Eng95], an architecture-neutral runtime as-sembler. It generates code that approaches the performance \nof C on some architectures. Its main disadvantage is that retrofitting it to an existing virtual machine \nrequires a signif- icant amount of effort -certainly more than the single day that was required to implement \nour technique in a produc- tion virtual machine. (Our simple nfibs benchmark runs about 40% faster using \nvcode, compared to our RISC-like inlined threaded code virtual machine.) Since translation is performed \nonly mce for each opcode, the break-even point is passed in any program that executes more than six times \ntbe number of opcodes that it contains. 6 Conclusions This work was inspired by the need to create an \ninterpreter with a very fine-grain RISC-like opcode set, that is both general (not tied to any particular \nhigh-level language) and amenable to traditional compiler optimizations. The cost of opcode dispatch \nis more significant in such a context, compared to more abstract interpreters whose bytecodes are carefully \nmatched to the language semantics. The expected benefits of our technique are related to the average \nsemantic content of a bytecode. We would expect languages such as Tel and Per], which have relatively \nhigh-level opcodes, to benefit less from macroization. Interpreters with a more RISC-like opcode set \nwill benefit more -since the cost of dispatch is more significant when compared to the cost of executing \nthe body of each bytecode. The Objective Cam1 bytecode set is positioned between these two extremes, \ncontaining both simple and complex opcodes. Vcode has better performance than our technique be-cause \nits instruction set matches very closely the underlying architecture. It can exert very fine control \nover the code that is generated, such as performing some degree of reordering for better instruction \nscheduling. We believe that similar re-sults can be achieved with our RISC-like inlining threaded code \ninterpreter, but in a more portable manner. Out technique is limited mainly by the inability of the compiler \nto perform the inter-opcode optimizations that are possible when a static analysis is performed and new \nmacro opcodes implemented manually in the interpreter. We be- lieve that these limitations are less important \nwhen using a very fine-grain opcode set, corresponding more closely to a traditional RISC architecture. \nMost opcodes will be imple- mented as a single machine instruction, and new opportu-nities for inter-opcode \noptimization will be available to the translator s code generator. Our technique is portable, simple \nto implement, and or-thogonal to the implementation of the virtual machine s op-codes. In reducing the \noverhead of opcode dispatch, it helps to bring the performance of fine-grained bytecodes to the same \nlevel as that of more abstract, language-dependent op-code sets. The complete source code for the simple \ninlining dy-namic threaded-code translator that was used to generate the benchmark results, and our modified \nversion of Objec- tive Cam1 1.05, are available online.g 7 Acknowledgements The authors would like to \nthank Xavier Leroy, John Mal-oney, Eliot Miranda, Dave Ungar, Mario Wolczko and the anonymous referees, \nfor making helpful comments on a draft of this paper. References [Am961 K. Arnold and J. Gosling, The \nJava Progmmming Language, Addison Wesley, 1996. ISBN o-201-63455-4 [Aus Joel Auslander, Matthai Philipose, \nCraig Chambers, Susan J. Eggem and Brian Bershad, Fast, Eflectiue Dynamic Compilation, Proc. PLDI 96, \npublished as ACM SIGPLAN Notices 31(5). Significant overheads are associated with the techuique used \nto check for stack overflow and pending signals in Objective Can&#38; but a discussion of these are beyond \nthe scope if this paper. http://wv-sor.inria.fr/rupiumarta/pldi98/ speed (seconds) space (bytes) Pentium \nspar-c PowerP C Spare benchmark original inlined on oinal inlined I orioinal inlined original inlined \ncache boyer 2.0 1.81 (111%) 2.3 1.50 (154%) 1.4 1.19 (113%) 13800 8324 42012 fib 2.0 1.44 (140%) 4.0 \n2.47 (163%) 1.6 1.12 (139%) 5288 3320 20160 genlex 1.0 0.93 (110%) 1.1 0.84 (127%) 0.7 0.59 (118%) 45696 \n26856 156892 kb 10.3 8.15 (126%) 16.9 7.71 (219%) 6.3 5.36 (118%) 20968 13048 75868 qsort 5.8 3.95 (146%) \n9.5 5.39 (175%) 4.1 2.98 (137%) 6676 3932 26416 qsort* 4.8 3.04 (158%) 8.0 4.26 (188%) 3.3 2.27 (147%) \n6532 3884 25280 sieve 3.0 2.79 (107%) 2.5 2.22 (110%) 1.9 1.86 (100%) 5200 3312 20124 soli 3.1 2.18 (144%) \n5.1 2.98 (170%) 2.1 1.50 (142%) 6644 3952 25516 soli* 2.4 1.38 (172%) 4.0 2.00 (202%) 1.6 0.93 (168%) \n6544 3908 24548 take 2.8 1.91 (144%) 5.0 3.26 (152%) 2.1 1.47 (142%) 4784 3012 18652 taku 4.9 3.20 (152%) \n7.0 4.14 (170%) 3.2 2.33 (139%) 4812 3036 18296 Figure 4: Raw results for the Objective-Cam1 benchmarks. \n [Deu84] L. Peter Deutsch and Alan M. Schil%nan, Eficient Appendix Implementation of the Smalltalk-System, \nProc. POPL 84. Figure 14 shows the raw results for the Objective Cam1 [Eng95] Dawson R. Engler, VCODE: \nA Retargetable, Extensible, benchmarks. The execution speed (in seconds) is shown Very Fast Dynamic Code \nGenemtion System, Proc. PLD1 96, for all three architectures, for both the original interpreter published \nas ACM SIGPLAN Notices 31(5). and the inlined interpreter. The inlined interpreter speed is [Ert98] \nM. Anton Ertl, A Portable Forth Engine. shown both as an absolute figure and as a percentage rela-http://vvm.complang.tuvien.ac.at/forth/ \ntive to the original interpreter s speed. threaded-code.html The final three columns show the sizes of \nthe original [Go1831 Adele Goldberg and David Robson, Smalltalk-80: The threaded code, the threaded \ncode after inlining, and the fmal Language and its Implementation, Addison-Wesley, 1983. size of the \nmacro cache for the Spare only. All are measured ISBN O-201-11371-6 in bytes. [Ing97] Dan Ingalls, Ted \nKaehler, John Maloney, Scott Wallace and Alan Kay, Back to the Future: the Story of Squeak, a Usable \nSmalltalk Written in Itself, Proc. OOPSLA 97, October 5-9 1997, Atlanta, Georgia. [Ler97] Xavier Leroy, \nThe Objective Cam1 system release 1.05, INRIA, 1997. [Ler98] Xavier Leroy, private communication. [Lin97] \nTim Lindholm and Frank Yellin, The Java Virtual Machine Specification, Addison-Wesley, 1997. ISBN 0-201-63452-X \n[Mir87] Eliot Miranda, BrvuHaHa -A Portable Smalltalk Interpreter, Proc. OOPSLA 87, published as ACM \nSIGPLAN Notices 22(12). [MirSl] Eliot Miranda, Portable Fast Direct Threaded Code, posted to camp. compilers. \nhttp://cuivw.unige.ch/0SG/people/jvitek/Compilsrs/ Year91/msg00215.html [Mir97] Eliot Miranda, private \ncommunication. [Moo701 Charles H. Moore and Geoffrey C. Leach, FORTH -A Language for Intemctive Computing, \nTechnical Report, Mohasco Industries, Inc., 1970. http://ww.dnai.com/~jfoxlF7OPOST.ZIP [Noe96] Fran cois \nNofl, Luke Hornof, Charles Consel and Julia L. Lawall, Automatic, Template-Based Run-Time Specialization: \nImplementation and Experimental Study, Proc. ICCL 98. http://uuw.irisa.ir/compose/papexs/rt-bench.ps.gz \n [Tho95] Tom Thompson, Building the Better Virtual CPU, Byte Magazine, August 1995.   \n\t\t\t", "proc_id": "277650", "abstract": "Achieving good performance in bytecoded language interpreters is difficult without sacrificing both simplicity and portability. This is due to the complexity of dynamic translation (\"just-in-time compilation\") of bytecodes into native code, which is the mechanism employed universally by high-performance interpreters.We demonstrate that a few simple techniques make it possible to create highly-portable dynamic translators that can attain as much as 70% the performance of optimized C for certain numerical computations. Translators based on such techniques can offer respectable performance without sacrificing either the simplicity or portability of much slower \"pure\" bytecode interpreters.", "authors": [{"name": "Ian Piumarta", "author_profile_id": "81100232407", "affiliation": "INRIA Roquencourt, B.P. 105, 78153 Le Chesnay Cedex, France", "person_id": "PP39033659", "email_address": "", "orcid_id": ""}, {"name": "Fabio Riccardi", "author_profile_id": "81100335589", "affiliation": "INRIA Roquencourt, B.P. 105, 78153 Le Chesnay Cedex, France", "person_id": "PP40026174", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/277650.277743", "year": "1998", "article_id": "277743", "conference": "PLDI", "title": "Optimizing direct threaded code by selective inlining", "url": "http://dl.acm.org/citation.cfm?id=277743"}