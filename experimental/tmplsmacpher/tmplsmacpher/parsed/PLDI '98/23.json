{"article_publication_date": "05-01-1998", "fulltext": "\n Garbage Collection and Local Variable Qpe-Precision and Liveness in JavaTM Virtual Machines Ole Agesen \nand David Detlefs J. Eliot B. Moss Sun Microsystems Laboratories Department of Computer Science 2 Elizabeth \nDrive University of Massachusetts Chelmsford, MA 01824, USA Amherst, MA 01003-4610, USA ole.agesen@sun.com, \ndavid.detlefs@sun.com moss@cs.umass.edu Abstract. Full precision in garbage collection implies retaining \nonly those heap allocated objects that will actually be used in the fu- ture, Since full precision is \nnot computable in general, garbage col- lectors use safe (i.e., conservative) approximations such as \nreach- ability from a set of root references. Ambiguous roots collectors (commonly called conservative \n) can be overly conservative be-cause they overestimate the root set, and thereby retain unexpect- edly \nlarge amounts of garbage. We consider two more precise col- lection schemes for Java virtual machines \n(JVMs). One uses a type analysis to obtain a type-precise root set (only those variables that contain \nreferences); the other adds a live variable analysis to reduce the root set to only the live reference \nvariables. Even with the Java programming language s strong typing, it turns out that the JVM specification \nhas a feature that makes type-precise root sets difficult to compute. We explain the problem and ways \nin which it can be solved. Our experimental results include measurements of the costs of the type and \nliveness analyses at load time, of the incremental ben- efits at run time of the liveness analysis over \nthe type- analysis alone, and of various map sixes and counts. We find that the liveness anal- ysis often \nproduces little or no improvement in heap size, some- times modest improvements, and occasionally the \nimprovement is dramatic. While further study is in order, we conclude that the main benefit of the liveness \nanalysis is preventing bad surprises. 1 Introduction The goal of garbage collection (gc) is to reclaim \nmemory allocated to objects that will not be used again. Knowing exactly which ob- jects a program will \nnot access again is equivalent to the halting problem, and is thus not computable. In response, researchers \nand implementers have devised gc algorithms spanning a wide range of precision. We are concerned with \ngc algorithms that retain objects Sun, Sun Microsystems, the Sun Logo, Java, and HotJava are trademarh \nor regis- tered trademarks of Sun Microsystems, Inc.. in the United States and other countries. Volaoo, \nVolanoChat, and VolanoMark are trademarks of Volano LLC. v Q 1996 ACM 0-69791-967-4/96/0006...$5.00 \nif and only if they are reachable from a set of roof references. Such roots include local and global \nvariables. We focus on precision of that part of the root set resulting from local variables, which we \nterm the local variable root set, or l-roots for short. At the imple- mentation level, local variables \nare stored in slots in stack frames. Since our setting is the JavaTM programming language, there will \nbe multiple stacks, one for each thread, but many of the same no- tions apply to single-threaded languages \nor those requiring closures rather than stacks. There are at least four degrees of precision one might \napply in calculating l-roots: 1. Beat every local variable as an l-mot, without regard to type. This \nis ambiguous roots collection [Boehm &#38; Weiser, 1988; Boehm &#38; Shao, 1993; Bartlett, 1988; Bartlett, \n1989; Appel &#38; Hanson, 19881, also commonly called conservative gc. 2. Use type information to obtain \ntype-pnxise roots, i.e., only those l-roots whose variable is of reference type. This has also been called \naccurate , precise , and aggressive (in contrast with conservative ). 3. Extend type-precision by adding \nliveness information from an intra-procedurallive variable analysis; we call this live-precise collection. \n 4. More refined liveness analyses, such as inter-procedural anal-ysis.  One can also analyze liveness \nof heap objects, and fields of heap objects; this is often called compile-time gc. Note, though, that \nwe are concerned with which mot references will be used again, not with which objects will be. We do \nassume that the pointer- containing fields of heap objects can be determined precisely. A requirement \nof precise collection is that one must somehow provide the collector with information about the locations \nof ref- erences. This may introduce both performance overhead and ex- tra implementation complexity. \nWe can provide type-precision in-formation via tags that make stack frames and their contents self- describing. \nTags may be supported by hardware, but more com- monly require the generation of extra instructions to \ncheck, mask, and insert tags. If tagging is not used, then the system must asso- ciate information with \neach stack frame to indicate the l-roots of the frame; we call such a data structure a stack map. The \nterm conservative has been applied (ambiguously) to ambiguous treatment of heap contents a$ well as ambiguous \ndetermination of roots. Here we are concerned with how to generate stack maps for code presented as bytecodes \nused in the Java Virtual Machine (JVM) [Lindholm &#38; Yellin, 19961, hereafter referred to as Java bytecode, \nand thus to implement type-precise collection. We are also con- cerned with the impact of refining type-precision \nto live-precision. We describe the problems in generating stack maps for Java byte- code and how to solve \nthe problems. We have implemented both type-precise and live-precise collection in the same system, and \nof- fer direct comparison of the resulting heap sizes during the execu- tion of a suite of benchmark \nprograms. We have also measured the cost of generating both type-precise and live-precise stack maps, \nwhich is of greater relevance in Java virtual machines than many other systems since Java bytecode is \nloaded (and stack maps are generated) at run time.  2 Related work Work most closely related to our \ntopics falls into three categories: tagless garbage collection, compile-time analyses for garbage col- \nlection, and experimental results related to precision of garbage col- lection. Tagless collection goes \nback at least to Branquart and Lewi s [Branquart &#38; Lewi, 19711 and Wodon s [Wodon, 19711 collectors \nfor Algold and Britton s [Britton, 19751 for Pascal. Branquart and Lewi s collector is notable in that \nit updated tables at run time as stack slot contents changed. More recently Appel [Appel, 19891 and Goldberg \n[Goldberg, 19911 considered tagless collection for Standard ML, which is complicated by the presence \nof polymor- phic functions, where the types of their arguments vary from call to call. In a followup \npaper [Goldberg &#38; Gloger, 19921, Goldberg and Gloger presented a system that uses type unifications \nto derive types at gc time. If the collector is unable to determine a type for any given variable or \nfield, then that variable or field will not be accessed in the future, and can safely be ignored by the \ncollector. Baker discussed the general idea of using Hindley-Milner unifica-tion in this way a bit earlier \n[Baker, 19901. A little later, Fradet [Fradet, 19941 extended this sort of collector to include a certain \nkind of liveness information, based on the intuitive idea that if a polymorphic function does not depend \non a type parameter, then it could not actually use any dataitems of that type. A simple example of this \nis length on lists, which does not examine the list elements, but only counts how many there are. We \nobserve that Fradet s scheme can in some cases determine that object fields are dead, and that it relies \non Hindley-Milner style polymorphism, somewhat different from the type system of the Java programming \nlanguage. Aditya, ef al. compared, in the context of the polymorphic func-tional language Id, the cost \nof type-reconstruction-based collection and conservative collection [Aditya et al., 19941, and found \nthat run-time type reconstruction can have a significant impact. Tolmach [Tolmach, 19941 and Tarditi, \ner al. [Tarditi et al., 19961 describe schemes that represent the polymorphic type parameters more explicitly, \npotentially at run time, but frequently optimized away. The Tarditi, et al., object and stack information \nis similar to ours, and they use liveness information at gc points (but do not re- port any experience \nwith it). They also handle callee-save registers, which require traversing the stack to find callers \nregister informa- tion in order to type a register saved by a callee; we also encoun- tered that issue \nin implementing the scheme laid out by Diwan, et al. [Diwan et al., 19921. We need not do that in a bytecode \ninter- pretation implementation of the JVM, but if we produced optimized native code, the issue would \narise. Note that many of these schemes are concerned not only with eliminating reference/non-reference \ntags in the stack, but also with eliminating type tags in heap objects. In object-oriented languages \nsimilar to the Java progr amming language, objects carry full type information to support run-time type \ndiscrimination operations. The availability of full type information makes it possible to iden- tify \nreference 6elds in heap objects, and thus we are concerned only with reference/non-reference distinctions \nfor roots. The Java programming language, as it currently stands, does not have parametric polymorphism, \nthough there is considerable dis-cussion of possible techniques for adding parameter&#38;d types and \nclasses. If these were implemented with shared code bodies, then some of the same stack map generation \nissues would arise as do with Standard ML polymorphic functions [Agesen et uf., 19971. In the area of \nprocedural and object-oriented languages, Diwan, et al., described a scheme for building stack maps for \nModula- 3 [Diwan et al., 19921, which deals with reconstructing pointers to heap objects from offsets \nand other optimized representations, which come about at least in part from the ability to pass object \nfields by reference in calls. They also considered how stack maps might be compressed to save space. \nA related topic is ensuring that compiler optimizations will not effectively hide live pointers from \na collector, and has been considered by Boehm and Chase (at least), separately and together [Chase, 1988; \nBoehm, 1991; Boehm &#38; Chase, 1992; Boehm, 19961. The relatively simple and highly constrained model \nof references presented by the JVM avoids the optimization-induced problems these other works address, \nsuch as interior and derived pointers. However, once one considers gener- ating native code from Java \nbytecode, the optimization issues may arise. In other work, Boehm and Shao considered how to con- struct \na useful conservative approximation of object type informa- tion at run time for a conservative collector \npoehm, 19931. Finally, Thomas, with Jones, built routines for tracing stack frames, moving from an interpretive \nto a compiled model for stack maps [Thomas, 1993; Thomas &#38;Jones, 1994; Thomas, 19951. Clearly the \nnotion of tagless collection is now fairly well devel- oped; we certainly do not claim that building \nstack maps for Java bytecode is a novel idea or even that the difticulties peculiar to this context require \ndeep new approaches. Turning to compile-time analyses for garbage collection, there has been much work \ndone on such analyses for functional and applicative languages [Barth, 1977; Bruynooghe, 1987; Chase \net al., 1990; Deutsch, 1990; Foster &#38; Winsborough, 1991; Hamil- ton &#38; Jones, 1991; Hamilton, \n1993; Hamilton, 1995; Heder- man, 1988; Hicks, 1993; Hudak, 1986; Hudak, 1987; Hughes, 1992; Inoue et \nal., 1988; Jensen &#38; Mogensen, 1990; Jones &#38; le Metayer, 1989; Jones &#38;White, 1991; Jones &#38; \nlyas, 1993; Jones, 1995; Mohnen, 1995; Mulkers, 1993; Mulkers et al., 1994; Wadler, 19841. There are \ntwo important ways in which that work does not carry over to our situation. The most obvious difference \nis that the Java programming language is not functional, so the patterns of al- location, mutation, and \nheap use in general might be quite different. A more subtle difference is that most of the work on compile-time \ngc is focused on showing (statically) that certain objects are not reachable and can be reused or collected \nimmediately. We are con- cerned only with whether references in local variables will be used again, which \nis a weaker property. Some schemes are more similar to our liveness analysis. For ex- ample, Appel described \na continuation-passing style compiler for Standard ML [Appel, 19921, which effectively removed dead vari- \nables from closures. This resulted in there being more closures (one for each set of live variables), \nprompting some to call for closure combination to save on closure allocation, but Appel has noted that \nthis would risk retaining more allocated heap objects because of dead variables. Shao and Appel devised \nan arguably better scheme, based on control and data flow analyses, that shares closures heavily but \nstill guarantees that dead variables are unreachable [Shao &#38; Ap- pel, 19941. Thomas s compiler-generated \ntracing routines [Thomas, 1993; Thomas &#38; Jones, 1994; Thomas, 19951 take liveness into ac- count \nfor closures, and a given closure may be traced more than once, with different livenesses for the variables, \nto trace all live ref- erences. Again, we do not claim that the idea of using liveness information is \nnew. However, we found no reports of its use for procedural or object-oriented languages (other than \nan indication that Chase has built a collector similar to ours for a JVM [Chase, 19971). On the other \nhand, it has likely been done before but simply not reported. Most significantly, we have found no previous \nmeasurement of the impact of live variable analysis, only anecdotal discussions in the context of functional \nlanguages, which leads us to the topic of ex- perimental results. Overall, the improvements obtained \nwith compile-time gc for functional languages have been minor. For example, Jones [Jones, 19951 obtained \nan 8% reduction in bytes allocated for Haskell, re-ducing overall execution time by 4.5%. Likewise, Wentworth \nfound that conservative gc generally did well [Wentworth, 19901. On the other hand, he made a telling \nobservation: sometimes conservatism makes a big difference.2 Similarly, it appears that the primary ben-efit \nof liveness analyses is in reducing the likelihood of surpris- ing space retention. Evaluations of conservative \ngc have been in terms of the incidence of non-reference values looking like refer- ences and thus causing \ngarbage to be retained. We note that such evaluations overlook the storage that can be reclaimed by omitting \ndead variables from the root set; that is, type-precision and live- precision constitute two separate \nprecision improvements over am- biguous roots gc. Our contributions: From this overview of related work, \nwe con- clude that our primary contribution lies in reporting measurements of the impact of liveness \nanalysis for a procedural object-oriented language. We previously reported in more detail on the difticul- \nties in producing stack maps for Java bytecode [Agesen &#38; Detlefs, 19971, and summarize that work \nhere, extending it with the liveness analysis. 3 Stack maps and gc points The contents of a stack slot \ncan change during the execution of a Java method. Slots, with the exception of those occupied by ar-guments \nto the method, start uninitialized. Thus a simple way in which slot types can change is from uninitialized \nto containing a value of a particular type. However, a Java compiler is permitted to (and indeed existing \nones do) store source variables with disjoint live ranges in the same slot. Thus a slot can contain values \nof differ- 2Ropooents of conservative gc argue that cases where it performed poorly for Wentworth can \nbe largely prevented by avoiding allocation in regions of virtual mem- ory whose addresses look like \nnumeric values in we by the program at hand. ent types at different points in the execution of a method; \nsome of those types may be reference types and others may be non-reference types. At this juncture, we \nstress that we are concerned with processing Java bytecode, loaded at run time. Thus we are concerned \nnot so much with the Java pmgmmming language specification [Gosling et al., 19961 as with the Java virtual \nmachine specification [Lind- holm &#38; Yellin, 19961. (The instruction set also has been described separately \nby Gosling [Gosling, 19951.) Java bytecode must pass well-formedness tests performed by a run-time bytecode \nverifier, we assume that we deal only with such well-formed Java bytecode methods. Some of the relevant \nverified properties are: A type can be calculated for each local variable and stack tem- porary at each \ninstruction of a method, using a straightforward data flow analysis over the lattice of object types, \naugmented with some non-object types. This implies that types may de- pend on program point, but nor \non the path by which the pro- gram point is reached. We call this the Gosling property be- low, since \nit was first stated explicitly by Gosling [Gosling, 19951.3 The types of the operands for each instruction \nwill satisfy the instruction s type constraints. In particular, integer, floating point, and reference \nhandling instructions are distinct and type checked. The simple static data flow analysis suffices to \nshow that no uninitialized variable is ever used. Following Diwan, et al. [Diwan et al., 19921, we do \nnot generate stack maps for every instruction. Rather, we restrict the VM im-plementation so that collection \ncan occur only at certain gc points. These must include the allocation bytecodes. In the absence of a \nwhole-program style inter-procedural analysis, which would be dif- ficult in the face of the JVM s dynamic \nloading capabilities, one must also include calls as possible gc points. Finally, to insure that one \ncan stop each thread if another thread initiates collection, each loop should contain a gc point. This \nis easily accomplished by making backward branches gc points. Beyond that, the choice of gc points is \nan engineering decision. Discussion of techniques to advance threads to gc points falls outside the scope \nof this paper. Bytecode verification uses a full type lattice, but for stack map generation we need to \nknow only whether a slot contains a reference or non-reference, etc., and not the specific type. We do, \nhowever, need to track program counter (pc) values, as will be described in Section 4. We may thus use \nthis lattice: top/conflict val ref uninitPC bottom/untouched To start the data flow analysis for a method, \nwe set all variable values to bottom, except for the entry point to the method, where we 3As we will \nsee, the Gosling property does not CJway~ hold; the complete story is more complex. set arguments to \nval or refaccording to their incoming type, and re- maining variables to uninit. Any of the well-known \ndata flow analy-sis computation algorithms will suffice; we used a simple work-list algorithm.  4 The \njsr problem Unfortunately, we have not yet told the complete story. The JVM specification explicitly \nallows one exception to the Gosling prop-erty. Ihe Java bytecode instruction set includes a pair of operations \ncalled jsr and ret. The jsr instruction jumps to an address specified in the instruction and pushes a \nreturn address value on the operand stack4 of the current method. The ret instruction speci-6es a local \nvariable that must contain a return address, and jumps to that return address. The intended use of these \nbytecodes is in the implementation of the try { body } finally { handler } construct of the Java programming \nlanguaie, in which harder is executed no matter how body is exited. Ihe bandlerwould be trans- lated \nas a jsr subroutine: a mini-method within the method. Ev- ery instruction that exits body, such as return \nor throw state-ments or falling off the end , would be preceded in the translation by a j sr to the handler \nsubroutine, which would store the pushed return address in a local variable, perform the work of handler, \nthen perform a ret. Although a jsr subroutine resembles a real method, there is a crucial difference: \nit executes in the same stack frame as its containing method and has access to all the local variables \nof the method. The JVM specification for verification of jsr subroutines contains an explicit exception \nto the Gosling property [Lindholm &#38; Yellin, 1996, p. 1361: the bytecode veritier permits any local \nvariable v that is neither read nor written in a jsr subroutine to retain its type across a j sr to that \nsubroutine. This seemingly reasonable weakening of the Gosling property causes serious difficulty for \nprecise garbage collection. Consider a case in which there are two jsrs to the same jsr subroutine. At \none j sr, local variable v is being used to hold an integer, and at the other, it holds a reference. \nShould a garbage collection occur while a thread is in the jsr subroutine, a simple program-counter based \nstack map scheme cannot determine if v contains a reference, since the stack layout is now path dependent. \nSimply disallowing garbage collections for the duration of the jsr subroutine is not an option since \ntry-f inally handlers can perform arbitrary computation, including calling methods that may execute indefinitely \nand allocate an unbounded number of objects. 5 Possible solutions There are several possible solutions \nto the jsr problem. An obvious one is to change the JVM specification to remove the Gosling prop- erty \nexception for jsr subroutines, which would simplify bytecode verification as well as stack map generation. \nHowever, removing the exception might increase the size of some stack frames, and For the present discussion, \nit is unnecessary to distinguish behveen stack frame slots holding operand stack values and stack frame \nslots holding local variables: we think of them as two separate sets of local variables, one being addressed \nfrom the stack frame. base, the other being addressed from the top of stack pointer. in any case it would \nbe politically difficult to make changes to the speciEcation that would invalidate any existing code. \nA second class of solutions rewrites the bytecodes to eliminate any violation of the Gosling property. \nOne specific technique is to replicate jsr subroutines that are called from sites with different stack \nmaps, so that each replica is called only from sites with the same stack map. Since jsr routine calls \ncan be nested, this can re- sult in exponential code duplication. While the occurrence of the jsr problem \nis rare for many programs, even one case of exponen- tial expansion might be fatal. Also, we anticipate \nthat exception handling features will be used more in the future than they are now, as programmers become \nmore accustomed to them. Rather than du- plicating code, we chose to split variables; we describe this \nin more detail below. A third class of solutions is to allow the Gosling property viola- tion, and add \nadditional information to stack maps so that one can determine the nesting of jsr subroutine calls in \nprogress, and com- bine stack map information through the j sr call chain. One of us is pursuing this \napproach, which has the advantages of not affecting the VM specification, of not requiring any bytecode \nrewriting, and of not imposing any normal case overhead in method execution. It is more complex, and \nmay require slightly more work at collection time (probably not enough to matter), and slightly larger \nand more complex stack maps (again, probably not significant in practice).  6 Bytecode rewriting to \nsplit conflicting variables Our first step was to refine the lattice used in the data flow analysis that \ncomputes stack maps to record not only that a conflict occurs but also the kind of conflict. Essentially, \nwe used the power set lattice, adding cases for ref-nonref, ref-knit, vahninit, etc. This is easy to \nimplement using bit vector operations. We need this more detailed information because we resolve conflicts \nbetween references and uninitialized values (ref-uninit conflicts) differently from conflicts between \nreferences and non- reference values (ref-nonref conflicts): ref-uninit conflicts are eliminated by prepending \ncode to the start of the method to initialize the variables to null. ref-nonref conflicts are eliminated \nby splitting the variables. top (ref-nonref-uninit) conflicts are resolved by a combination of the above \ntwo actions: we first introduce initializations to null, eliminating the uninit conflicts, and then split \nto eliminate the ref-nonref conflicts? Note that the initialization to null will be associated with the \npart of the split variable that has a ref type, and not with the part that has a non-reftype. We extend \nthe stack map computation for a method m as fol- lows. We alternate data flow analysis and conflict elimination, \niter-ating until all conflicts have been eliminated. During the data flow analysis, a variable vars ZHnit \nholds a set of reference-containing variables requiring initialization. It is initially empty. The data \nflow analysis treats variables in the set as holding initialized reference values at the start of the \nmethod. Each iteration initializes a vari- able varstiSplit to the empty set of variables. This set will \nhold We ignore vnl-&#38;nit conflicts since they are irrelevant to garbage collection. variables that \nwere found to hold a ref-nonref conflict at a point when they were used. (Such a conflict can happen \nonly when the Gosling property is violated, i.e., through the type merging that jsr subroutines can induce.) \nThe stack map computation then proceeds as described previ- ously, except in its handling of conflict \nvalues. A use of a vari- able whose value in the data flow analysis is the ref-knit conflict value causes \nthe variable to be added to varsToInit. A use of a variable holding the ref-nonref conflict value adds \nthe variable to varsZbSplit. If a use is of a value having both kinds of conflicts, we add the variable \nto varsToInit only.6 At the end of an iteration, varsToSplit is checked. If it is non- empty, then each \nvariable in it is split. To split local variable n, we increase the number of local variables allocated \nin stack frames for method m by one; let nn be the number of the newly allocated local variable. We then \nexamine the bytecodes for method m, modifying them so that instructions that use the original variable \nn to hold ref- erences are unchanged, but non-reference uses are changed to use variable nn instead? \nIt is a happy property of the Java bytecode instruction set that instructions have sufficient type information \nen-coded in theiropcodes to determine locally whether a given instruc- tion uses a local variable as \na reference, making the rewriting fairly simp1e.s There is one more complication. Rewriting instructions \ncan cause instruction positions and lengths to change, so code must be relocated, branch offsets updated, \netc., a process we will not de- scribe in detail. If any uses of conflict variables are detected, at \nleast some are repaired by this variable-splitting process or by addition to the vardblnit set. Ihe next \niteration of the loop may still find some conflicts in the rewritten code (perhaps a variable has both \nref-uninit and ref-nonref conflicts), causing another iteration, or it will detect no conflicts and successfully \ngenerate the stack mapse9 The rewriting may fail, in the following ways. Allocating new local variables \ncould exceed the limit on the number of locals in a method imposed by the bytecode instruction set. Widening \ninstruc- tions could conceivably cause a method to exceed the maximum method size. In such cases, the \nVM would have to somehow indi- cate an error akin to a verification error. Such programs would be exceedingly \nunlikely to occur in practice. The performance of the by&#38;ode rewriting part of the process is not \na crucial issue since with the most commonly used compiler, javac, very few methods need rewriting. In \nthe benchmark suite we used for the comprehensive measurements we describe in more de- tail later, we \nsplit only six variables (five in one program, one in %te in the game we realized that it is also correct, \nand probably slightly better, to add the viable only to varsToSpZif. A later iteration will add it to \nvarsTolnir, but only if a ref-uninit conflict remains after splitting the variable. The other choice, \nwhere reference uses of n are changed to use nn and non- reference uses are unchanged, is equivalent. \nThere is one exception to this property: the astore instruction is usually used to pop a reference (an \naddress, hence the prefix letter a) from the operand stack and store it in a local variable, but it may \nalso be used to do the same with return addresses pushed on the stack by j ST instructions. Fortunately, \nthe data flow analysis already maintains sufficient state to determine whether the opemnd stack top at \nthe point of the astore is such a return addre~, so this complication is easily circumvented. Again. \nlate in the game, we realized that varsZbSplit can only be non-empty after the first iteration of the \noverall process. So we need perform the data flow analysis no more than two times. Furthermore, if we \nare willing possibly to over-estimate ref- uninit conllicts by allowing variables to be added to both \nvnrsZXplit and vad dnir, we need only perform the data flow analysis once. Doing so might significantly \nreduce time needed to generate stack maps for methods that require rewriting. another). Ref-uninit conflicts \nwere somewhat more common. An average of 20 variables per program required initialization. Ihese were \nconcentrated in two programs that used the Java Abstract Wm- dows Toolkit. On the other hand, the performance \nof the first iteration of the analysis, the only iteration required by most methods, is of some interest \nsince it will be performed for any dynamically loaded code before that code is executed. Further below \nwe report some mea- sures of the cost of the data flow analysis. 7 Live variable analysis The additional \nlive variable analysis is straightforward, and requires only a two-element lattice. Since liveness is \na backwards flow prop- erty, we unfortunately cannot compute liveness by augmenting the forward flow \ntype analysis lattice. We observe, though, that the live- ness analysis may eliminate some conflicts \nat gc points. However, by the Gosling property, except at j sr instructions and in jsr sub- routines, \nref-uninit and ref-nonref conflicts indicate variables that must be dead, so such items should be dropped \nfrom stack maps anyway. Still, the liveness analysis will identify dead references. Since the JVM instruction \nset is stack-oriented, data movement operations such as the assignment x = y ; present themselves as \npushes and pops. Our live variable analysis propagates liveness information through local variables and \nstack temporaries. This means that in the data flow analysis, the live/dead values for differ- ent variables \nare coupled, so the height of the lattice is the number of variables; i.e., variables cannot be analyzed \nseparately from one another. It is unlikely that one would see worst case iteration of the algorithm \nin practice, though. On the run-time side, what should the collector do with dead reference variables? \nObviously it should ignore any dead reference for tracing purposes. Less obviously, if a program is run \nin the presence of a debugger, the collector has three options. It can trace dead references (so they \ncan still be examined in the debugger for as long as possible); it can set dead references to null (so \nthat the debugger will not try to follow a reference that may be invalid after collection); or it can \ntreat dead references as weak references, retaining their value if and only if the referent objects are \notherwise reachable. It also appears that omitting dead references when collecting can expose program \nbugs. For example, suppose object x refers to ob- ject y, object x has a finalizer that uses object y, \nand the program s last reference to n becomes dead while the program is still using y. If a gc occurs \nat this point, y may be accessed concurrently by the linalizer and by the main program code, a form of \nconcurrency that may surprise the programmer. Ihis does not appear to violate the Java programming language \nspecifications, so we consider it to be legal. We also observe that many optimizing transformations can \nexpose (or hide) bugs, so our liveness analysis is far from unique in this regard. While it is conceimble \nthat one might generate stack maps at gc time, it is prob- lematic because implementations of the data \nflow analyses will tend to allocate heap storage, which is not generally possible during gc. It is also \npossible to pre-analyze code and insert gc stack maps into class files as additional attributes, but \nthis is possi- ble only for local trusted class files. This example was supplied by an anonymous reviewer. \n 7.1 Why liveness analysis? Why might it be important to include a live variable analysis? One school \nof thought is that the dead variable heap object retention problem can be fixed simply by having programmers \ninsert assign- ments of null at the right places. Ihere are several problems with this view. First, it \nintroduces an overhead all the time to address a situation that occurs relatively rarely (gc). It is \nmore efficient to have the gc treat the slots as containing null than it is to set them to null. Second, \nwhy should programmers have to waste their time even thinking about an issue like this when an automated \ntool can address it? It is not as if programmers are likely to wunt dead ob- jects retained, and thus \nperhaps desire control over this behavior. Third, even if programmers insert assignments of null, an \noptimiz- ing compiler might remove them, since they are assignments to dead variables!12 Finally, and \nwe think this is the nail in the coffin, there are cases where it is virtually impossible for the programmer \nto do the assignment at the critical moment. A good example of this is a method call such as v . m (x) \n, where x is the last live reference to some sizeable object subgraph, variable x is dead after the call, \nand method m also reaches a point where it no longer uses x. This is a particularly disturbing possibility, \nsince a call has indefinite du- ration and may be deep in the stack, thus retaining garbage for quite \na long time. One of us ran into a concrete example when working on a theorem-proving system in Modula-3. \nRewriting the essential part in the Java programming language, the pertinent code was: boo1 proveTheorem(InputStream \nis) { Sexp sx = Sexp.read(is); Pred p = Pred.sxToPred(sx); return refute (Pred.not (p) ) ; 1 The salient \nfeature of this code is that the Sexp form was used only because there was a convenient library available \nto read expressions in LISP S-expression form. The S-expression form was immedi- ately converted to a \npredicate form and discarded. In realistic situ- ations, sx might refer to a megabyte or more of S-expression \ndata that is dead, across a long running call to refute. Even more in- teresting, the actual code was \nwritten in a more functional style, and the dead variable was actually a compiler temporary! This made \nthe problem quite difficult to discover and remedy. Furthermore, when the original functional form was \nrewritten to the form shown above, and then modified by explicitly assigning null to sx after its last \nuse, the problem still persisted. We speculate that this was because the Modula-3 collector was an ambiguous \nroots collector, and an- other copy of the sx pointer, which had been passed in the call to Pred, was \nlying in the stack frame for refute, or some place even further towards the top of the stack. Our fix \nwas to null out the entire sx structure after building the pred form. Our reason for including this story \nis to make it clear how diffi- cult it cm be to locate and resolve problems of unexpected storage retention. \n 8 Experiments In this section we give experimental data obtained on a 296 Mhz Ul- tra SPARC with 512 \nMbytes of memory, running Solaris 2.6. The *David Chase brought this to our attention; he mentions the \npossibility in passing in his dissertation [Chase, 19871. programs we measured are a collection of benchmarks \nunder con- sideration for a SPEC suite to measure Java platforms.13 We had to exclude four of the programs \nbecause thread-library issues pre- vented us from running them correctly; we excluded two more be- cause \nthey allocated too little storage to be interesting in this study. Finally, we added ellisgc, a GC stress \ntest program, that John Ellis sent to us. While it bears some relationship to gcbench, they react differently \nunder liveness analysis, so we felt it useful to include both. The VM we used is based on the Javasoft \nJDK VM, modified to (among other things) support generation and use of our stack maps. Note that since \nwe are comparing the amount of reachable heap data as we vary the stack root set, the actual gc technique \nis irrelevant (it happened to be mark-sweep). 8.1 Cost of type and liveness analysis We measured the \nelapsed time used by stack map generation and liveness analysis while running our benchmark suite on \nan oth- erwise idle workstation. For purposes of comparison, we also measured total time, time consumed \nby class loading, and time for bytecode verification (running the VM in a mode where all classes are \nverified). Table 1 displays these measurements. The Stack map/loading column divides stack map computation \nby class loading time, and the Verification/Stack map column ex-presses verification time as a multiple \nof stack map computation time. Ihe Average row gives geometric means for the columns containing ratios. \nJust as verification can be done once (off-line) for local trusted classes, one could similarly compute \nstack map information off-line for such classes, speeding up program startup accordingly. On the other \nhand, for classes obtained over a network, possibly from untrusted sites, verification is necessary, \nand as our numbers show, usually dominates stack map computation time by an order of mag- nitude. We \nalso measured how much liveness analysis increased the cost of stack map generation. The increase was \nquite uniform, be-tween 54% and 58% over all the benchmarks. A further breakdown showed that the forward \nanalysis and backward analysis were quite similar in cost. However, the liveness analysis is able to \nreuse data structures created for the type analysis (basic blocks, etc.), thus de- creasing its incremental \ncost. 8.2 Stack map size measurements We present a range of stack map size and related statistics in \nTa- bles 2 and 3. In these and subsequent measurements, we have added runs of three more programs. The \nspreaukheet program is a prototype financial calculation engine, obtained via private com-munication. \nThe hotjuva run is part of a morning s exploration with the HotJava web browser. Both of these programs \nhave elab- orate graphical user interfaces. The volano run gives the behav- ior of the server-side program \nin Volano LLC s VolanoMark 1.0 benchmark, over several invocations of a client-side program pm- vided \nin the benchmark that imposes a workload on the server. This benchmark is intended to estimate the performance \nof the real %ose benchmarks selectedby SPEC (if any) may have different versions and/or Workloads, so \nour results cannot necessarily be compared meaningfully with any SPEC re.3ult.x Our purpose was only \nto compare different gc algor&#38;ms on a se4 of programs. not to compare platforms differing in any \nother way. Benchmark Total run time Loading Stack map Stack map/ Verification Verification/ (msec) (msec) \nLoading (msec) Stack map (se4 compress 82.0 269.6 172.0 0.63 1838.6 10.6 I iess 9.9 128.2 lW? 1.01 1147.8 \n1 8.8 _._- _--.I i iinoack I 89.9 1 991 I 918, I n-98-._ -I 711.5 II 7.2 ,,,A I.- newmst Oh?,v.-I, 912 \nI, n95 I 821.0 1 32.2 ,a.--._-8.9 raytrace 21.5 114.0 1 118.8 1 1.04 1 1107.8 I ----.-, 9.3 cst 17.9 \nllA1 1 1115 I 0.99 I 729.2 1 6.4 1 db II _-.-7.6 1 1u1.8 1I lU1.3 1 U.YY 1 030.0 , L&#38;I si-_ 12.3 \nI 108.4 I 106.0 I 0.97 I 632.2 1 anaoram 1 -.-I 97.2 i 94.8 1 0.97 I 732.7 1 7.7 I 1.8 ----e---I I I \n1697 801 gcbench 1 2.8 1 98.4 1 96.0 1 0.97 1 . -..- I -.-iavac I 7.4 I 143.5 I 1717_. *.- I 119 I 4531.7 \n_.-. 26.4 _I w3 I n93 I 723.2 7.3 deltablue 23.4 105.2 ,v._ -.r-, mpegaudio 91.6 107.2 146.9 1.37 1479.7 \n_ .._.. 10.0 jack 21.4 156.6 175.0 1.11 2102.4 12.0 t sgp 350.4 98.0 93.2 0.95 614.3 6.5 ellisgc 8.8 \n40.2 62.3 1.54 936.3 15.0 Average Table 1: Comparison of stack map computation with class loading and \nbytecode verification VolanoChat chat server on a given Java platform, and is available at the mark phase \nof the live-precise collector and determined the http://www.volano.com/mark.html. number of words of \nobjects it marked. We did a sweep only when In Table 2, the Code size column shows the number of byte- \nthe allocation area was exhausted. codes in all methods executed in the run. The GC points column For \neach precision we can construct a function giving the heap gives the total number of gc points, bytecode \ninstructions for which size over time, where time is measured in words allocated and is stackmaps were \ncomputed, for the methods executed in the run. The sampled every 1OOK words. We connect the points of \neach func- Code bytes/gc point column gives the ratio of these two numbers, tion and compute the integral \nunder the function s curve, which an estimate of the interval between bytecode instructions requiring \ngives us the space-time product of the run. We report total space- stack maps. The Slots column shows \nthe sum of the number of time products, and the ratio of those products, for the two levels of local \nvariable and operand stack slots in use at all gc points, and the precision in Table 4. We also report \nfor each benchmark the (geo- average number for each gc point. The Refs column shows how metric) mean \nof the ratios of the heap sizes at each sampled point many of these slots contained references, and the \nLive columns during the run. Finally we report geometric means of each column shows how many of these \nwere live. The last two columns show of the table, i.e., across all benchmarks.14 We present some sample \nthe fraction of slots that contained references, and the fraction of curves showing reachable data with \nand without liveness analysis reference slots that were live. The Average row gives geometric in Figure \n1. The ellisgc run shows the most dramatic improvement means for the rows representing ratios. Table \n3 presents the same from liveness analysis of the programs we ran. The volano run is information, averaged \nover methods instead of gc points. Again, more typical. (Each hump corresponds to the response of the \nthe Average row gives geometric means for the rows representing server-side program to one invocation \nof the client-side simulated ratios. Roughly speaking, a little more than half of all slots are ref- \nload.) erences, and approximately 314 of these are live. In the particular Overall, liveness information \nreduces the time-space product by system in which we did these experiments, stackmaps consumed an average \nof 11%. This result is skewed by the ellisgc program. an average of 57% as much space as the bytecode \nitself. However, That program is somewhat contrived and is intended to challenge the representation uses \nno compression, so we believe this overhead garbage collectors. However, it does not intentionally include \ndead could be substantially reduced. variables. Still, we include averages omitting ellisgc and see that \nthe time-space product improves by an average of 3.6%. We note that it is not necessarily reasonable \nto reject ellisgc from the results, 8.3 Run-time heap size measurements since dead variable space retention \nis likely to be an occurrence that To measure the impact of liveness analysis, we ran the suite of is \nusually not too bad, but occasionally terrible. It is interesting to benchmark programs on our modified \nJVM. This system uses our see that almost every program we ran shows a measurable differ- stack maps \nto trace stack frames in either a type-precise or live- ence, so some degree of dead variable space retention \nappears to be precise manner. In fact, both levels of precision are available in the common. same system, \nso we compared them directly, as follows. A separate point is that this is a non-generational collector. \nIn a After every 1OOK words of allocation, we invoked the mark 14We use geometric rather than arithmetic \nmeans since they are more suitable for phase of the type-precise collector and determined the number \nof comparing ratios. The geometric mean of n item is the nth root of their product, or, words of objects \nmarked. We then reset the mark bits and invoked equivalently, the anti-logarithm of the arithmetic mean \nof their logarithms. Benchmark Code size GC Code bytes/ Slots I Refs I Live Refsl (bytes) points gc point \n# 1 per pt 1 # I perpt I # 1 perpt Slots 0.528 0.785 0.564 0.720 0.505 0.789 0.518 0.791 0.535 0.771 \n0.431 0.774 0.522 0.784 0.484 0.758 0.519 0.790 0.511 0.781 0.596 0.702 0.546 0.780 0.511 0.830 0.552 \n0.746 0.508 0.795 0.522 0.667 quantum 357035 61166 5.8 419349 6.86 240788 3.94 166998 2.73 0.574 0.694 \nhotjava 399746 67248 5.9 1005937 14.96 597734 8.89 520071 7.73 0.594 0.870 volano 97797 7136 13.7 42357 \n5.94 24241 3.40 18098 2.54 0.572 0.747 Average 1 I 7.9 I 1 7.48 1 I 3.97 I I 3.04 0.530 m Table 2: Measured \nstack map size and related statistics 1 Benchmark 1 Methods I Bytes/method 1 GC points/method I Slots/method \n1 Refs/method 1 Live refs/method 1 I cnm rerr I 1li9 I javac 1124 141.68 16.24 128.74 76.68 53.86 deltablue \n404 225.22 14.35 98.19 53.66 41.88 mpegaudio 535 139.97 18.01 123.81 63.24 52.47 jack 759 179.67 18.77 \n126.40 69.79 52.10--.-_ tsgp I 337 83.07 15.49 113.79 &#38;8k 45.99 ellisgc 182 52.99 8.99 51.76 27.04 \n18.03 quantum 5843 61.10 10.47 71.77 41.21 28.58 hotjava 4669 85.62 14.40 215.45 128.02 111.39 volano \n596 164.09 11.97 71.07 40.67 30.37 Average 117.16 14.91 111.63 59.14 45.28 Table 3: Measured stack map \nsize reported per method compress jess linpack newmst raytrace cst db si anagram gcbench iavac .-_---_I- \nnpegaudio nrk --or ellisgc sureaclsheet hotj ava volano Average Average, without ellisgc 61128242 868824 \n3355585 78055 37817490 12504270 7978578 22599024 27918894 1 226114414 ] 9186308 II 3741663 76096 9716810_. \n----- 710108 289690805 154266857 929502587 39938934 11778354 9858598 Time space with liveness w kd) \n57363160 0.9384 0.9330 825482 0.9501 0.9472 3292842 0.9813 0.9497 73266 37508324 12258149 0.9803 0.9685 \n7882272 0.9879 0.9736 20103152 0.8896 0.8896 27712322 0.9926 0.9858 224068:i99 0.9910 0.9888 9007787 \n0.9806 0.9759 3426734 71294 9419524_ .-_--. 686164 61671602 0.2129 I 0.0904 154255015 929484329 38008136 \n10487525 9504470  Table 4: Heap sizes with and without liveness generational collector the benefit of \nliveness analysis may turn out to be greater, since one might reduce the volume of tenured garbage, which \ntakes longer to collect.  9 Conclusions We found that adding a live variable analysis to a type-precise \ngarbage collector, so as to increase its precision, reduced heap size (time-space product) by an average \nof 11% for a suite of bench- mark programs, with most programs showing some difference and a few showing \nmore dramatic differences. Liveness analysis ap- pears to offer minimal benefit to many programs, but \nit is important in reducing the possibility of surprisingly large volumes of retained garbage. Preliminary \nmeasurements indicate that the cost of gener- ating live-precise stack maps for Java bytecode is about \n50% greater than the cost of generating only type-precise stack maps. We also described some technical \ndifficulties in generating stack maps for Java bytecode so as to accomplish type-precise collection, \nand in- dicated several solution approaches. Acknowledgments. The greatly simplifying realization that \nref and noltlt?fuses of variables can be distinguished based only on the opcode of the using instruction \ncame about in a discussion with Boris Beylin, Ross Knippel, and Bob Wilson. John Rose ex- plained to \nus the behavior of the javac compiler when translating the try-finally construct. Hans Boehm, David Chase, \nand Richard Jones were particularly helpful in referring us to related work. Eliot Moss s participation \nin this work is supported in part by gifts from Sun Microsystems Laboratories and grants from the National \nScience Foundation. References (Aditya et a!., 19941 Shail Aditya, Christine Flood, and James Hicks. \nGarbage collection for strongly-typed languages using run-time type reconstruction. In [LFP, 19941, pp. \n12-23. [Agesen &#38; Detlefs, 19971 Ole Agesen and David Detlefs. Finding references in Java stacks. \nTech. Rep. SMGB97-67, Sue Microsystems Laboratories, Chelmsford, MA, USA, Oct. 1997. Presented at the \nOOPSLA 97 workshop on garbage collection. [Agesen etal., 19971 Ole Agesen, Stephen Freuad, and John C. \nMitchell. Adding type parameterization to Java. In Proceedings of the ACM SIGPLAN Conference on Object-Oriented \nPmgramming Systems, languages aruiApp1ication.s (OOPSLA-97) (New York, Oct.5-9 1997), vol. 32, 10 of \nACM SIGPLAN Notices, ACM Press, pp. 49-65. (Appel, 19891 Andrew W. Appe]. Runtime tags aren t necessary. \nLisp and Symbolic Computation 2 (1989), 153-162. [Appel, 19921 Andrew W. Appel. Compiling with Continuations. \nCambridge University Press, 1992, ch. 16, pp. 205-214. [Appel &#38; Hanson, 19881 Andrew W. Appel and \nDavid R. Hanson. Copying garbage collection in the presence of ambiguous references. Tech. Rep. CS-TR-162-88, \nPrinceton University, 1988. [Baker, 19901 Henry G. Baker. Unify and conquer (garbage, updating, aliasing, \n. . .) in functional languages. In Conference Record of the 1990 ACM Symposium on L&#38;p and Functional \nProgramming (Nice, Prance, June 1990), ACM Press, pp. 218-226. [Barth, 19771 Jeffrey M. Barth. Shifting \ngarbage collection overhead to compile time. Comman ications of the ACM 20.7 (July 1977). 513-518. 0.5 \ntype-preliae- 0.45 live-pra&#38; -. 0.4 g 0.35 ioX 4 0.2 Pa 0.15 0.1 0.05 0 0 0.5 I 1.5 2 25 tme (mrllion \nlime (minion words) o I 2 3 4 5 6 7 8 9 IO words) Figure 1: Sample curves showing reachable data over \ntime [Bartlett, 19881 Joel F. Bartlett. Compacting garbagecollection with [Chase et al., 19901 David \nR. Chase, Wegman, and Zadeck Analysis of ambiguous roots. Tech. Rep. 88i2, DEC Western Research Laboratory, \npointers and structures. ACM SIGPIAN Notices 25.6 (1990). Palo Alto, CA, Feb. 1988. Also in Lisp Pointers \n1,6 (April-June 1988), [Deutsch, 19901 A. Deutsch. On determining lifetime and aliasing of pp. 2-12. \ndynamically allocated data in higher-order functional specifications. In [Bartlett, 19891 Joel F. Bartlett. \nMostly-Copying garbage collection picks Conference Record of the Seventeenth Annual ACMSymposium on up \ngenerations and C++. Technical note, DEC Western Research Principles of Programmiing Lunguages (San Francisco, \nCA, Jan. 1990), Laboratory, Palo Alto, CA, Oct. 1989. Sources available in ACM SIGPLAN Notices, ACM Press, \npp. 157 - 168. ftp://gatekeeper.dec.com/pub/DEC/CCgc. [Diwan et al., 19921 Amer Diwan, J. Eliot B. Moss, \nand Richard L. [Boehm, 19911 Hans-Juergen Boehm. Simple GC-safe compilation. In Hudson. Compiler support \nfor garbage collection in a statically typed OOPSLA/ECOOP 91 Workshop on Garbage Collection in language. \nIn Proceedings of SIGPLAN 92 Conference on Programming Object-Oriented Systems (Oct. 1991), Paul R. Wilson \nand Barry Hayes, Languages Design and Implementation (San Francisco, CA, June Ed.% 1992), vol. 27 of \nACM SlGPLANNotices, ACM Press, pp. 273-282. [Boehm, 19931 Hans-Juergen Boehm. Space efficient conservative \n[Foster &#38; Winsborough, 19911 Ian Foster and William Winsborough. garbage collection. In Proceedings \nof SIGPL4N 93 Conference on Copy avoidance through compile-time analysis and local reuse. In Programming \nLanguages Design and Implementation (Albuquerque, Proceedings of International Logic Programming Sympsium \n(1991), New Mexico, June 1993), vol. 28(6) of ACM SIGPLAN Notices, ACM pp. 455-469. Press, pp. 197-206. \n[Fradet, 19941 Pascal Fradet. Collecting more garbage. In [LFP, 19941, [Boehm, 19961 Hans-Juergen Boehm. \nSimple garbage-collector safety. In pp. 24-33. [PLDI, 19961, pp. 89-98. [Goldberg, 19911 Benjamin Goldberg. \nTag-free garbage collection for [Boehm &#38; Chase, 19921 Hans-Juergen Boehm and David R. Chase. A strongly \ntyped programming languages. ACM SIGPLAN Notices 26,6 proposal for garbage-collector-safe C compilation. \nJournal of C (1991), 165-176. Language Translation (1992), 126-141. [Goldberg &#38; Gloger, 19921 Benjamin \nGoldberg and Michael Gloger. [Boehm &#38; Shao, 19931 Hans-Juergen Boehm and Zhong Shao. Inferring Polymorphic \ntype reconstruction for garbage collection without tags. In type maps during garbage collection. In OOPSWECOOP \n93 Conference Record of the 1992 ACM Symposium on Lisp and WanGshop on Garbage Collection in Object-Oriented \nSystems (Oct. Functional Programming (San Francisco, CA, June 1992), ACM Press, 1993), Eliot Moss, Paul \nR. Wilson, and Benjamin Zorn, Eds. pp. 53-65. [Boehm % Weiser, 19881 Hans-Juergen Boehm and Mark Weiser. \n[Gosling, 199.51 James Gosling. Java intermediate bytecodes. In Garbage collection in an uncooperative \nenvironment. Sofhare Practice Proceedings of the ACM SIGPLAN Workshop on Intermediate and Experience \nl&#38;9 (1988), 807-820. Representations (IR 9.5) (Jan. 1995), pp. 111-l 18. published as ACM [Branquart \n&#38; Lewi, 19711 l? Branquart and J. Lewi. A scheme of storage SIGPLAN Notices 30(3), March 1995. allocation \nand garbage collection for Algol-68. In [Peck, 19711, [Gosling et al., 19961 James Gosling, Bill Joy, \nand Guy Steele. The Java pp. 198-238. hguage Specification. Addison-Wesley, 1996. [Britton, 19751 Dianne \nEllen B&#38;ton. Heap storage management for the [Hamilton, 19931 G. W. Hamilton. Compile-Time Optimisation \nof Storeprogramming language Pascal. Master s thesis, University of Arizona, Usage in Lazy Funtional \nPrograms. PhD thesis, University of Stirling, 1975. 1993. [Bruynooghe, 19871 Maurice Bruynooghe. Compile-time \ngarbage collection or How to transform programs in an assignment-free [Hamilton, 19951 G. W. Hamilton. \nCompile-time garbage collection for language into code with assignments. In Program specification and \nlazy functional languages. In Proceedings of International Workshop on transfotmation. Ihe IFIP TC2/WG \n2.1 Working Conference, Bad Tok Memory Management (Dept. of Computer Science, Keele University, Germany, \nL. G. L. T. Meertens, Ed. North-Holland, Amsterdam, April Sept. 1995), Henry Baker, Ed., Lecture Notes \nin Computer Science, 15-17,1986 1987, pp. 113-129. Springer-Verlag. [Chase, 19971 David Chase, Nov. \n1997. Personal communication. [Hamilton &#38; Jones, 19911 G. W. Hamilton and Simon B. Jones. Compile-time \ngarbage collection by necessity analysis. In [Peyton [Chase, 19871 David R. Chase. Garbage collection \nand other Jones et al., 19911, pp. 6670. optimizations. Tech. rep., Rice University, Aug. 1987. [Hederman, \n19881 Lucy Hederman. Compile-time garbage collection [Chase, 19881 David R. Chase. Safety considerations \nfor storage using reference count analysis. Master s thesis, Rice University, Aug. allocation optimizations. \nACM SIGPL4N Notices 23,7 (1988), I-IO. 1988. Also Rice University Technical Report TR88-75 but, according \nto Rice University s technical report list, this report is no longer available for distribution. [Hicks, \n19931 James Hicks. Experiences with compiler-directed storage reclamation. In Record of the 1993 Conference \non Functional Programming and Computer Architecturn motorola Cambridge Research Center, June 1993), R. \nJohn M. Hughes, Ed., vol. 523 of Lecture Notes in Computer Science, Springer-Verlag. [Hudak, 19861 Paul \nR. Hudak. A semantic model of reference counting and its abstraction (detailed summary). In Conference \nRecord of the I986 A CM Symposium on Lkp and Functional Programming (Cambridge, MA, Aug. 1986). ACM SIGPLAN \nNotices, ACM Press, pp. 351-363. [Hudak, 19871 Paul R. Hudak A semantic model of reference counting and \nits abstraction. In Absrract Interpmtation of Declarative Languages, Samson Abramsky and Chris Hankin, \nEds. Ellis Horward, 1987, pp. 45-62. [Hughes, 19921 Simon Hughes. Compile-time garbage collection for \nhigher-order functional languages. Journal of Log&#38; and Computution 2,4 (Aug. 1992), 483-509. Special \nIssue on Abstract Interpretation. [Inoue et al., 19881 Katsuro Inoue, Hiroyuki Seki, and Hikaru Yagi. \nAnalysis of functional programs to detect run-time garbage cells. ACM Tmnsactions on Programming Languages \nand Systems IO, 4 (Oct. 1988), 555-578. [Jensen &#38; Mogensen, 19901 Thomas P. Jensen and Torben Mogensen. \nA backwards analysis for compile-time garbage collection. In ESOP 9U 3ni European Symposium on Programming, \nCopenhagen, Denmark, May 1990. (Lecture Notes in Computer Science, vol. 432) (1990), Neil D. Jones, Ed., \nSpringer-Verlag, pp. 227-239. [Jones, 19951 Simon B. Jones. An experiment in compile time garbage collection. \nTech. Rep. 84, Programming Methodology Group, Giiteborg University and Chalmers University of Technology, \nJan. 1995. [Jones &#38; le M&#38;ayer, 19891 Simon B. Jones and D. le M&#38;ayer. Compile-time garbage \ncollection by sharing analysis. In Reconi of the 1989 Conference on Functional Programming and Computer \nArchitecture (Imperial College, London, Aug. 1989), ACM Press, pp. 54-74. [Jones &#38; Tyas, 19931 Simon \nB. Jones and Andrew S. Tyas. The implementer s dilemma: A mathematical model of compile-time garbage \ncollection. In Sixth Annual GIapgow Workshop on Functional Pmgmmming (1993), Workshops in Computer Science, \nSpringer-Verlag, pp. 139-144. [Jones &#38; White, 19911 Simon B. Jones and M. White.. Is compile time \ngarbage collection worth the effort. In [Peyton Jones etal., 19911, pp. 172176. [LFP, 19941 Conference \nRecord of the 1994 ACM Symposium on Lisp and Functional Programming (June 1994), ACM Press. [Lindholm \n&#38; Yellin, 19961 lim Lindholm and Frank Yellin. The Java Virtual Machine Specification. Addison-Wesley, \n1996. [Mohnen, 19951 Markus Mohnen. Efficient compile-time garbage collection for arbitrary data structures. \nTech. Rep. 95-08, University of Aachen, May 1995. Also in Seventh International Symposium on Programming \nLanguages, Implementations, Logics and Programs, PLILP95. [Mulkers, 19931 Anne Mulkers. Live Data Structures \nin Lagic Pmgramx No. 675 in Lecture Notes in Computer Science. Springer-Verlag, 1993. [Mulkers eta&#38; \n19941 Anne Mulkers, William Winsborough, and Maurice Bruyncqhe. Live-structure analysis for Prolog. ACM \nTransactions on Programming Languages and Systems 16,2 (Mar. 1994). [Peck, 19711 J. E. L Peck, Ed. Algol-68 \nimplementation. North-Holland, Amsterdam, 1971. [Peyton Jones et al., 19911 Simon L. Peyton Jones, G. \nHutton, and C. K. HOIS, Eds. ThiniAnnual Gkzvgow Workshop on Functional Pmgrtunming (1991), Spinger-Verlag. \n[PLDI, 19961 Pmceedings of SIGPIAN 96 Conference on Programming Languages Design and Implementation (1996), \nACM SIGPLAN Notices, ACM Press. [Shao &#38; Appel, 19941 Zhong Shao and Andrew W. Appel. Space-efficient \nclosure representations. In [LFP, 19941, pp. 150-161. [Tarditi et al., 19961 David Tarditi, Greg Morrisett, \nPerry Cheng, Chris Stone, Robert Harper, and Peter Lee. TIL: A type-directed optimizing compiler for \nML. In [PLDI, 19961. [Thomas, 19951 Stephen Thomas. Garbage collection in shared-environment closure \nreducers: Space-efficient depth first copying using a tailored approach. Information Processing Ltters \n56,l (Oct. 1995), l-7. [Thomas, 19931 Stephen P. Thomas. The Pragmatics of Closure Reduction. PhD thesis, \nThe Computing Laboratory, University of Kent at Canterbury, Oct. 1993. [Thomas &#38; Jones, 19941 Stephen \nP. Thomas and Richard E. Jones. Garbage collection for shared environment closure reducers. Tech. rep., \nUniversity of Kent and University of Nottingham, Dec. 1994. [Tolmach, 19941 Andrew Tolmach. Tag-free \ngarbage collection using explicit type parameters. In Proceedings of SIGPIAN 94 Conference on Programming \nLanguages Design and Implementation (Orlando, Florida, June 1994), vol. 29 of ACM SIGPL4N Notices, ACM \nPress, pp. l-11. Also Lisp Pointers VIII 3, July-September 1994. [wadler, 19841 Philip L. Wadler. Listlessness \nis batter than laziness: Lazy evaluation and garbage collection at compile time. In Conference Record \nof the 1984 ACM Symposium on tip a&#38; Functional Programming (Austin, Texas, Aug. 1984), Guy L. Steele, \nEd., ACM Press, pp. 45-52. [wentworth, 19901 E. P. Wentworth. Pitfalls of conservative garbage collection. \nSoftware Practice andExperience 20,7 (1990), 719-727. [Wodon, 19711 P. L. Wodon. Methods of garbage collection \nfor AlgoI-68. In [Peck, 19711, pp. 245-262.  \n\t\t\t", "proc_id": "277650", "abstract": "Full precision in garbage collection implies retaining only those heap allocated objects that will actually be used in the future. Since full precision is not computable in general, garbage collectors use safe (i.e., conservative) approximations such as reachability from a set of root references. Ambiguous roots collectors (commonly called \"conservative\") can be overly conservative because they overestimate the root set, and thereby retain unexpectedly large amounts of garbage. We consider two more precise collection schemes for Java virtual machines (JVMs). One uses a type analysis to obtain a <i>type-precise</i> root set (only those variables that contain references); the other adds a live variable analysis to reduce the root set to only the live reference variables. Even with the Java programming language's strong typing, it turns out that the JVM specification has a feature that makes type-precise root sets difficult to compute. We explain the problem and ways in which it can be solved.Our experimental results include measurements of the costs of the type and liveness analyses at load time, of the incremental benefits at run time of the liveness analysis over the type analysis alone, and of various map sizes and counts. We find that the liveness analysis often produces little or no improvement in heap size, sometimes modest improvements, and occasionally the improvement is dramatic. While further study is in order, we conclude that the main benefit of the liveness analysis is preventing bad surprises.", "authors": [{"name": "Ole Agesen", "author_profile_id": "81100428004", "affiliation": "Sun Microsystems Laboratories, 2 Elizabeth Drive, Chelmsford, MA", "person_id": "PP18001641", "email_address": "", "orcid_id": ""}, {"name": "David Detlefs", "author_profile_id": "81100499495", "affiliation": "Sun Microsystems Laboratories, 2 Elizabeth Drive, Chelmsford, MA", "person_id": "P61233", "email_address": "", "orcid_id": ""}, {"name": "J. Eliot Moss", "author_profile_id": "81406593781", "affiliation": "Department of Computer Science, University of Massachusetts, Amherst, MA", "person_id": "PP39076229", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/277650.277738", "year": "1998", "article_id": "277738", "conference": "PLDI", "title": "Garbage collection and local variable type-precision and liveness in Java virtual machines", "url": "http://dl.acm.org/citation.cfm?id=277738"}