{"article_publication_date": "05-01-1998", "fulltext": "\n Fast, Effective Code Generation in a Just-In-Time Java Compiler Ali-Reza Ad-Tabataba?, Michal Cierniak, \nGuei-Yuan Lueh, Vishesh M. Parikh, James M. Stichnoth Intel Corporation 2200 Mission College Blvd. Santa \nClara, CA 95052 ABSTRACT A Just-In-Time (JIT) Java compiler produces native code from Java byte code \ninstructions during program execution. As such, compilation speed is more important in a Java JIT compiler \nthan in a traditional compiler, requiring optimization algorithms to be lightweight and effective. We \npresent the structure of a Java JIT compiler for the Intel Architecture, describe the lightweight implementation \nof JIT compiler optimizations (e.g., common subexpression elimination, register allocation, and elimination \nof array bounds checking), and evaluate the performance benefits and tradeoffs of the optimizations. \nThis JIT compiler has been shipped with version 2.5 of Intel s VTune for Java product. 1. INTRODUCTION \nIhe Java programming language [ 10) introduces new challenges to the compiler writer, because of the \nJust-In-Time (JIT) na-ture of the compilation model. A static compiler converts Java source code into \na verifiably secure and compact architecture- neutral distribution format, called Java by&#38; codes. \nA Java Vir- tual Machine (JVM) interprets the byte code instructions at run time. To improve runtime \nperformance, a JIT compiler converts byte codes into native code at run time. Although offline compilation \nof byte codes into native code is possible, it cannot always be performed, because all Java class files \nare not guaranteed to be available at the start of program execution. Therefore, a byte code compiler \nneeds to be prepared to execute dynamically at run time, hence the term JIT com- t Author s current affiliation \nis Oracle Corporation, 500 Oracle Parkway, Redwood Shores, CA 94065. All third party trademarks, tradenames, \nand other brands are the property of their respective owners. Psrmie&#38;n fc make digital or hard copier \nof all or part of this work for parsonal or clansroom use is granted without fee provided that copies \nare not made or distributed for profit or commercial sdvan- tape and that copiee bear this notice and \nthe full citation on the fir81 pat~a. To copy othenvi~s. to rapubliah, 10 p-1 on wwven w 10 radi8tributs \nto limts. requires prior specific pwmis4on and/or a fee. SlGPfAN 98 Montraal, Canada Q 1998 ACM 0.89791.987-4/96/0006...(6.00 \npiler. For this reason, overall program execution time now includes JIT compilation time, in contrast \nto the traditional methodology of performance measurement, in which compila-tion time is ignored. As \na result, it is extremely important for the compiler optimizations to be lightweight and effective. It \nis also important for the Java JIT compiler to interact with other parts of the system, such as the garbage \ncollector and perform- ance analysis tools (e.g., Intel s VTune [ 131 tool). In this paper, we present \nthe design and implementation of a production Java JIT compiler for the Intel IA32 architecture [11,12]. \nWe describe our approach, called lazy code selection, for quickly generating good quality IA32 code. \nThe key to the lazy code selection approach is that it generates native IA32 instructions directly from \nthe byte codes, in a single pass. Other than a control-flow graph used for register allocation, the JIT \ndoes not generate an explicit intermediate representation. Rather, it uses the byte codes themselves \nto represent expres- sions and maintains additional structures that are managed on- the-fly. This is \nin contrast to other Java JIT implementations which transform byte codes to an explicit intermediate \nrepre-sentation [21,16]. We describe our lightweight implementations of several standard compiler optimizations-lightweight \nin terms of both execution time and auxiliary data structures. We use several benchmark programs to show \nthe impact of the op- timizations on overall runtime performance. The JIT that we describe in this paper \ninterfaces with the Micro- soft JVM from SDK 1.5.1 [20] and is currently being shipped with version 2.5 \nof the Intel VTune for Java product [ 131, an application profiling tool for Java. The performance of \nthe Intel JIT is comparable to that of the Microsoft m, the running times of several benchmarks, measured \nin seconds, are summarized in the table below (full results for the benchmarks are presented in Section \n4). MSJIT 1 IntelJIT 1 MSJIT 1 IntelJIT The rest of this paper is organized as follows. In Section 2, \nwe describe the details of the code generator and of our optimiza- tion algorithms. In Section 3, we \ndescribe our technique for tracking the location of object references, so that the code gen- erator can \ncompute the root set of references at garbage collec- tion sites. In Section 4, we present measurements \nof the effec- tiveness of the JIT s optimizations. Finally, in Section 5 we present our conclusions. \n2. CODE GENERATION DETAILS Figure 1 shows the five major phases of the Intel JIT. The pre- pass phase \nperforms a linear-time traversal of the byte codes to collect information needed for the global register \nallocation and lazy code selection phases, and for implementing garbage col- lection support. The global \nregister allocation phase assigns physical registers to local variables. The code generation phase generates \nIA32 instructions using the lazy code selection algo-rithm described in Section 2.2 and performs several \noptimiza- tions: common subexpression elimination, array bounds check elimination, peephole optimizations, \nand frame pointer elimina- tion. The code emission phase copies the generated code and data sections \nto their final locations in memory. The patching phase fries up relocations in the emitted code and data \nsections; for instance, offsets of forward branches, addresses of code la- bels in switch table entries, \nand the address of switch tables in the read-only data section. With the exception of the global register \nallocation phase, all phases are linear in time and space. 4 I Code emission I 1 Code and data patching \nFigure 1: Compiler passes. 2.1 The Prepass Phase The prepass phase builds a control-flow graph, and \ncollects three pieces of information: (1) the depth of the Java operand stack at the entry of each basic \nblock; (2) the static reference count of each local variable; (3) the Java operand stack locations con-taining \nreferences at each point where garbage collection may occur; and (4) a list of those variables that alternately \nhold refer- ence and non-reference values at different points in the method. The stack depth information \nis needed by the code selector to initialize the locations of operands on the Java operand stack at the \nbeginning of a basic block. The static reference count in- formation is needed by the global register \nallocator to assign priorities to variables. The information collected for garbage collection allows \nthe JIT to compute the root set of live objects reachable from stack frame locations and from registers. \nVari-ables that hold reference and non-reference values are treated in a special way by the garbage collector. \nWe discuss the details of garbage collection in Section 3. 2.2 Lazy Code Selection The lazy code selection \nalgorithm is a single pass code selection algorithm. It emits assembled native instructions directly \ninto a temporary code buffer that is later copied by the code emission phase. lbe code selector also \nuses a temporary data buffer to assemble read-only constant data, such as floating-point con-stants and \nswitch tables. The goal of the lazy code selection algorithm is twofold: (1) to keep intermediate values \n(i.e., Java operand stack values) in scratch registers, and (2) to reduce register pressure and take \nadvantage of the IA32 addressing modes by folding loads of immediate operands and accesses to memory \noperands into the compute instructions that use them. Lazy code selection achieves these goals by propagating \ninformation about source operands via an auxiliary data structure called the mimic stack. Ihe mimic stack \nsimulates the Java runtime operand stack at JIT time: for each byte code s selected instruction sequence, \nthe source operands of the instruction sequence are popped from the mimic stack and the result operand \nof the instruction sequence is pushed onto the mimic stack. Instruction operands are modeled in a C++ \nclass hierarchy (Fig- ure 2); the base of this hierarchy is the Operand class. There are four main types \nof operands: (1) register operands (Register), which are values in physical integer registers and directly \nad-dressable by most integer compute instructions, (2) immediate operands (Immediate), which are constant \nvalues that can be folded into the immediate fields of integer compute instructions, (3) memory operands \n(Memory), which are values in memory that can be folded into floating-point or integer compute in-structions \nusing one of L432 s memory addressing modes, and (4) floating point operands (FP), which are values \non top of the IA32 floating-point register stack. Memory operands are further classified according to \nthe kind of data being accessed: (1) ob- ject field references (Field), which use the offset addressing \nmode (base register plus constant offset), (2) array elements (Array), which use the indexed addressing \nmode (base register plus scaled index register), (3) static class variables that are not declared as \nfinal (Static), which use the absolute addressing mode, (4) floating point constants and static class \nvariables that are declared as final (Constant), which also use the absolute addressing mode (IA32 floating-point \ninstructions do not have an immediate form), and (5) stack frame locations (Stack), which use the offset \naddressing with either the stack or frame pointer register as the base. Stack frame locations are used \nfor spilling and for those local variables that are not allocated a register. The JIT eliminates the \nframe pointer in most cases so that most Stack operands use the stack pointer register as the base register. \nFrame pointer elimination frees up an additional register for use by the global register allocator, and \nreduces the number of instructions executed in a method s Prolog.  I I I I I Field St&#38; stack co-t \nhY Figure 2: Opaand class hierarchy. To select code for a byte code B that pops source values from the \nJava operand stack, the code selector first pops the corre- sponding source operands from the mimic stack, \nand then tries to fold the source operands into the compute instruction selected for B. If the attempt \nis successful, then the folded compute in- struction is selected. Otherwise, if an operand 0 cannot be \nfolded into the compute instruction, the code selector selects an instruction that loads 0 into a scratch \nregister R, and then gener- ates a compute instruction that uses R as the source operand. The result \nof the compute instruction (which often is a register) is pushed onto the mimic stack to make it available \nfor folding into subsequent instructions. The register manager, which we discuss in Section 2.4.1, handles \nallocation of scratch registers. Ihe floating-point registers of IA32 are organized as a stack ill]; \na floating-point compute instruction pops one operand from the register stack (the other operand can \nbe a memory op- erand or another location on the register stack) and pushes its result onto the register \nstack. This maps perfectly to the Java Virtual Machine s stack-based architecture: whenever an FP operand \nis popped from the mimic stack, this operand must rep- resent the top of the floating-point register \nstack. The only complication is that the code selector needs to keep track of the floating-point register \nstack depth and generate spill code if overflow occurs. The floating-point register stack has only 8 \nregisters but we have found that none of our applications cause floating-point register stack overflow. \nAt a call site, the code selector generates spills for those oper- ands on the mimic stack that are live \nacross the call site, since the calling conventions consider the FP stack to be caller-saved. Only those \noperands that may be killed by the call need to be saved; that is, mimic stack operands that are of type \nField, Ar- ray, Static, FP, and caller-saved Register; operands of type Im- mediate, Constant, Stack, \nand callee-saved Register do not need to be spilled. One problem for the code generator is that the Java \noperand stack can be non-empty at the entry or exit of a basic block. This condition occurs mainly because \nof conditional expressions (i.e., question mark colon expressions such as a>b?a :b). The problem is that \nthe code generator must guarantee that the oper- ands on the mimic stack are the same at the merge point \nof two paths. To guarantee that mimic stack operands are the same at the merge point of several paths, \nall values that remain on the mimic stack at the end of a basic block are spilled to canonical spill \nlocations in the stack frame. Similarly, if the Java operand stack depth is non-zero at a label (i.e., \nbranch target), then for each Java operand stack location that contains a value, the corre-sponding mimic \nstack location is initialized to its canonical spill location. This is the reason the prepass phase computes \nthe Java operand stack depth. The lazy code selector performs several simple optimizations during code \nselection. First, if one of the operands of a compute instruction is an Immediate or Constant operand, \nthen the code selector attempts to perform strength reduction (for multiply, divide, and mod operators) \nor constant folding on the compute instruction. Second, the code selector detects compare followed by \nbranch byte code sequences so that it can generate the corm- sponding IA32 compare and branch instruction \nsequence. Third, the code selector performs redundant load-after-store elimina-tion by tracking values \nloaded into registers; this optimization replaces the use of a memory operand with the use of a register \nthat already contains the memory value. Although the design of the lazy code selector is tailored to \ntake advantage of IA32 s CISC architecture, this style of code selec- tion can also benefit a RISC architecture. \nComputation instruc-tions in RISC architectures cannot operate directly on memory operands; thus the \ncode selector needs to propagate only register and immediate operands. The benefits for a RISC architecture \nare that the code selector will eliminate unnecessary moves from registers assigned to local variables \nand unnecessary loads of immediate operands. Since only register and immediate oper-ands are propagated, \nan implementation for a RISC architecture may not require an operand class hierarchy.  2.3 Common Subexpression \nElimination Traditional common subexpression elimination (CSE) algo-rithms, which are based on data flow \nanalysis [7, 151 and value numbering [7, 21, are expensive in both time and space, and we prefer to avoid \nthem in a JIT compiler. We have developed a fast, lightweight CSE algorithm that focuses on common subex-pressions \nwithin extended basic blocks. Our CSE algorithm uses the Java byte codes themselves as a compact representation \nof expressions. Consider the expression x+y . Assuming x and y are local variables 1 and 2, respec- tively, \nthe expression s byte code sequence is [iload-1, iload-2, iadd]. Because the byte codes of the iload-1, \niload-2, and iadd instructions are Oxlb, Oxlc, and 0x60, respectively, the value 0x1 blc60 represents \nme expression x+y . Note that the value Oxlblc60 appears as a subsequence in the byte code instruction \nstream; as such, the expression can be represented using the pair <o$set,length>, which we call an expression \ntag. To detect whether the tags afietl,n> and <off- set2,n> represent the same syntactic expression, \nwe simply compare the subsequences of length n starting at o$ketl and oJj%et2. Because the maximum size \nof the stream is 2 [17], an expression tag can be represented concisely using a single word (16 bits \nfor the offset and 16 bits for the length). To detect common subexpressions, the code selector tracks \nthe expression values held in the scratch registers by annotating each scratch register R with the tag \nof the expression that R contains. Before selecting code for a byte code B, the code se- lector looks \nahead in the stream to see whether the expression starting from B matches one already associated with \na scratch register; if so, it pushes the register onto the mimic stack and skips over the common subexpression \nin the byte code stream. The registers are checked in decreasing order of subsequence length to match \nthe largest sized expression. To keep the com- pilation time linear, expression lengths are limited to \n16 byte codes, an empirically sufficient limit. If a match is not found, the code selector selects an \ninstruction sequence for the byte code B and updates the expression tag of the register R containing \nthe result of the instruction sequence. The expression tags are ini- tialized to hold no values at the \nbeginning of basic blocks that have branch labels. There are two ways that the availability of an expression \nE held in a register R is killed: 1. By instructions that modify the value of R. If register R is a caller-saved \nregister, then a call site kills the availability of E in R; in this case, the expression tag of R is \nupdated to indicate that R contains no value. The availability of E in R is also killed when the register \nmanager allocates R for reuse by the code selector; in this case, the code selector updates the expression \ntag of R to indicate that it contains a new expression (or no value if R was used to hold a tempo- rary \nvalue). 2. By assignments or method calls that (potentially) modify a value loaded by E. E can contain \nloads of variables, array elements, and object fields, which can be modified by method calls and by assignments. \nAt a method call, the code selector kills the availability of all expressions that contain loads of array \nelements or object fields. At an as- signment, the code selector kills the availability of all ex-pressions \nthat load (or may load) the assigned variable, ob-ject field, or array element.  The information about \nthe set of variables and object fields loaded by an expression is held in kill sets; there is one kill \nset associated with each physical register managed by the register manager. Each variable has a unique \nindex, and each object field has a unique constant pool index. This allows a kill set to be maintained \nas a bit vector with the fiit few bits dedicated to variable indices and the rest of the bits dedicated \nto object field indices. Whenever an object field assignment byte code (i.e., a putfield byte code) assigns \na new value to an object field with index I, the code selector kills the availability of a register R \nif the I th object field index is set in R s kill set bit vector. The code selector performs similar \nbookkeeping for assignments to variables. To save memory space and compilation time, the size of each \nkill set bit vector is limited to 256 bits; the code selector gives up on CSE opportunities for indices \nthat fall out- side of this limit. The code selector takes a more conservative approach tn killing expressions \nthat load array elements. Rather than performing expensive alias analysis, the JIT takes advantage of \nthe Java feature that there is no aliasing between arrays with different element types. Each register \nR has a fixed-size bit vector that contains the set of array element types loaded by the expression held \nin R. When the code selector encounters an assignment to an array element of type T (e.g., an assignment \nto an integer element), it kills all registers containing expressions that load array elements of type \nT. This bit vector, in conjunction with an additional bit flag that indicates whether an expression has \nany object field references, is used by the code selector to detect expressions that are killed by method \ncalls. Traditional CSE approaches generate temporaries to hold the values of common subexpressions, which \nmay cause high reg- ister pressure and introduce more spill code. Our CSE approach does not increase \nregister pressure because the availability of E in R is killed immediately once R is used by the code \nselector. Our CSE approach has some limitations. First, it cannot re-associate expressions; for example, \nx+y and Ly+x are treated as distinct expressions because they have different sub-sequences. Second, the \napproach cannot detect expressions that are syntactically different but have the same value; for instance, \nXq ; x+y and w+y . Third, the approach can only repre- sent expressions that are contiguous in the byte \ncode stream (no bubble/gap is allowed). 2.4 Register Allocation The IA32 architecture includes only \n7 general-purpose integer registers that can be used for register allocation. By convention, the 7 registers \nare partitioned into 3 caller-saved scratch regis- ters (eax, ecx, and edx) and 4 callee-saved registers \n(ebx, ebp, esi, and edi). The Intel JIT uses the 3 caller-saved reg- isters for local register allocation \nand the 4 callee-saved registers for global register allocation. 2.4.1 Local Register Allocation The \nlocal register allocator, or register manager, allocates the registers that the lazy code selector uses \nfor expression evalua-tion. When the code selector requires a scratch register (i.e., a register to hold \na temporary expression value), it requests one from the register manager. If there are multiple registers \navail- able, the register manager returns the register that was least re- cently allocated (i.e., a circular \nallocation strategy). This simple heuristic benefits the CSE optimization, described in Section 2.3, \nby trying to equalize the lifetimes of common subexpres-sions within scratch registers (i.e., afair policy). \nHowever, if the code selector requests a register but none are currently available, the register manager \nfinds the least recently allocated register that can be used, generates code to spill the register to \nthe stack frame, and returns that register. To find the least recently allo-cated register, the register \nmanager searches the operands on the mimic stack, starting from the bottom-most operand; in this manner, \nthe register manager spills the register with the most distant use in the past. After producing the instruction \nsequence for evaluating an expression, the scratch registers used in the source operands of the instruction \nsequence are given back to the register manager. 2.4.2 Global Register Allocation The global register \nallocator allocates the 4 callee-saved regis- ters to local variables within a single method (i.e., no \ninterpro- cedural register allocation). Global register allocation has been an active area of research \nresulting in several effective algo-rithms [6, 3, 4, 8, 5, 22, 9, 19, 181. A JIT compiler, however, introduces \na new challenge-how to balance the cost of running the potentially expensive register allocation algorithm \nagainst the expected performance gains. The key is to use an algorithm that is both fast and effective. \nThe Intel JIT provides two different register allocation algo-rithms. The fist algorithm is extremely \nsimple and cheap to execute: the register allocator allocates the 4 callee-saved regis- ters to the 4 \nvariables with the highest static reference counts. The complexity of the algorithm is O(B), where B \nis the number of byte codes in the method. This simple allocation scheme is limited, however, because \nit does not allow two variables with non-overlapping live ranges to share the same register. Thus fewer \nvariables may be allocated registers, and register save/restore costs may increase. The second register \nallocation algorithm is a priority-based scheme similar to the one described by Chow [S], but with two \ndifferences: our scheme does not use an interference graph (which is expensive in terms of time and space) \nand does not perform live range splitting. Priority-based register allocation is effective in allocating \nregisters to the most important variables in a function and can easily take into account call costs [18]. \n(An alternative linear-time approach that does not consider priorities is described in [23].) Our algorithm \nis as follows: For each vari- able u, ordered by priority, the allocator performs a backwards depth-first \nsearch through the flow graph, starting the search at all basic blocks in which u is used (representing \na possible end of the live range), and terminating the search at a basic block in which u is defined \n(representing the start of the live range). This depth-first search visits the set of basic blocks covering \nthe en- tire live range of u, and keeps track of the callee-saved registers that are unavailable (i.e., \nalready allocated) in these blocks. If there is a register R available in all of these basic blocks, \nthen the allocator assigns R to u and marks R as unavailable in the basic blocks comprising the live \nrange of u. The complexity of the algorithm is O(B+NV), where B is the number of byte codes in the method, \nN is the number of basic blocks, and V is the number of local variables considered for register allocation. \nCall cost is an important issue in register allocation [18]. If cal- lee-save cost is not taken into \naccount (i.e., the cost to save and restore a callee-saved register at the prolog and epilog of a func- \ntion), the register allocator may assign a register with high cal- lee-save cost to a variable with low \nspill cost. Both global reg- ister allocation algorithms implemented in the Intel JIT assign registers \nto variables only if the spill costs are greater than the callee-save cost. In our priority-based approach, \nthe first live range that is assigned a given callee-saved register pays the cal- lee-save cost; the \nregister allocator does not include the callee- costs in the cost benefit analysis of subsequent live \nranges that are assigned the same register. After the global register allocation is completed, there \nare typi- cally some basic blocks in which not all registers can be allo- cated; the code selector can \nuse these leftover registers to reduce spill code. Before generating code for a basic block b, the code \nselector first notifies the register manager of the set of callee- saved registers that are available \n(i.e., not allocated to any vari- able) in b so that the register manager can add these registers to \nits pool of scratch registers available inside b. This has three benefits: First, the register manager \ngenerates less spill code. Second, the callee-saved registers are used as spill locations for operands \nthat are live across call sites, thereby reducing the number of stack frame accesses around call sites. \nThird, more common subexpressions are found because registers containing expression values are live longer. \n 2.5 Array Bounds Check Elimination The Java language specifies that all array accesses are checked \nat run time; an attempt to use an index that is out of bounds causes an exception (ArrayIndexOutOf BoundsExcep-tion) \nto be thrown. The JIT can eliminate bounds checks if it can prove that the index is always within the \ncorrect range, or if it can prove that an earlier check will throw an exception. The Intel JIT uses a \nsimple mechanism to eliminate bounds checks of indices that are constant. For each Java operand stack \nlocation and variable that contains a reference to an array A, the code generator keeps track of the \nmaximum constant bound for which no bounds check is needed for A. This information is updated when a \nbounds check is generated for a constant index or when an array of constant size is created. For example, \nif the code selector has already generated bounds checking for A[ l], then a subsequent access to A[51 \ndoes not require bounds checking. In addition, when the array is created (using the ne- warray byte code), \nthe JlT can use the creation size to elimi- nate bounds checking on subsequent array accesses. This ap-proach \nis especially effective during array initialization (e.g., A[O] = A[l] = . . . = A]91 = 1). This algorithm \nis limited in two ways. First, it is applied only locally to each extended basic block, and not globally. \nSecond, only constant operands are used; more bounds checks could be eliminated if symbolic information \nwere used as well. 2.6 Out-of-Line Exception Throws An array reference in Java must include code to \ncheck whether the subscript is within the bounds of the array. The array length is usually stored within \nthe array object, allowing the JIT to inline bounds-checking code. Assuming that the address of the array \nis in eax and the subscript is in ecx, the naive code se- quence for array bounds checking is as follows. \ncmp [eax + offset(length)], ecx ja OK ; fold two tests into one! . . . ; throw an exception OK: ; access \nthe array element The code for throwing an exception is infrequently executed (i.e., cold code) and the \nabove implementation has two perform- ance problems for the IA32 architecture: 1. Static branch prediction \non Pentium@ Pm and Pentium II processors predicts forward conditional branches not to be taken. 2. The \nexception-throwing portion of the code is likely to be loaded into the instruction cache even though \nit is unlikely to be executed.  The Intel JIT produces code optimized for the common case of the subscript \nbeing within array bounds, with the code after the no tOK label appearing at the end of the method: cmp \ndword ptr [eax+of f set (length) ] , ecx jbe notOK . . . ; access the array element . . . ; cold code \nat end of method s code space notOK: . . . ; throw an exception  2.7 Example Figure 3 shows a code sequence \nfrom the MPEG Player pro-gram, a Java applet for viewing MPEG files. We use this exam- ple to illustrate \nthe lazy code selection algorithm, common su-bexpression elimination, out-of-line exception throwing, \nand register allocation. The first column shows a Java byte code sequence; each byte code is numbered \nwith its index in the byte of the expression formed by the first two byte codes. Notice that code stream. \nThe second column shows the native code gener- the code selector delays generating code for byte codes \n490 ated by the Intel JlT. This column also shows the point at which (getfield #44), 493 (lload IS), \nand 495 (Ui), until it en- the code selector generates instructions; for instance, the code counters \nbyte code 496 (aaload). The values of 12i and selector generates the first native instruction when it \nencounters getfield #44 are kept in ecx and eax, respectively, and the thirdbyte code. 1 : third column \nshows the expression tags the expression tags of these registers are updated ac IA32 native code eax \necx edx 483: aload- 484: getfield #34 487: getfield #45 mov ecx, [ebp+O4h] [483,4] --- 490: getfield \n#44 mov edx, [ecx+l9Oh] [483,4] [483,7] 493: lload la 495: l2i 496: aaload mov eax, [edx+l8h] mov ecx, \n[esp+l28h] !iiRJfiy7:;,, cmp [eax+04h], ecx jbe -throw 497: getfield #16 mov edx, [eax+ecx*4+08h] 500: \nl2i 501: istore 5 mov eax, [edx+04h] [483,18] [493,3] [483,14] mov [esp+l60h], eax 503: aload- 504: \ngetfield #34 507: getfield #45 510: getfield #44 513: lload 18 515: l2i 516: aaload 517: getfield #49 \n 520: i2l mov ecx, [edx+OCh] mov edx, ecx sar edx, 1 Fh 521: lstore 20 mov [esp+l24h], edx mov [esp+l20h], \necx Figure 3: Example of lazy code selection and several optimizations in the Mel JIT, excerpted from \nthe MPEG Player code. held in the scratch registers at each point during code selection. (<493,3> and \n<483,10>). As the value of byte code 496 (aa- In this example, global register allocation has assigned \nthe ebp load) is loaded into edx, the expression tags <483,10> of register to variable 0 (variables 5, \n18, and 20 are in memory). eax and <493,3> of ecx are combined to form the expression The ebp register \nis typically the frame pointer, but for the tag <483,14> of edx. When the lazy code selection finishes \nmethod shown in this example, the Intel JIT eliminates the generating code for byte code 501 (istore \n5) and scans the frame pointer allowing the global register allocator to allocate next byte code (aload-at \nindex .503), three expressions, ebp; accesses to variables 5 and 20 are based off the stack <483,18>, \n<493,3> and <483,14>, are being held in the three pointer (esp). Note that the code for throwing the \nexception scratch registers. The selector attempts to match a CSE by ArrayIndexOutOfBoundsException is \nmoved out of searching the expression tags in the order of <483,18>, line (the code that throws the exception \nis not shown). <483,14>, and <493,3>. The expression d03,14> matches <483,14> (i.e., a CSE is detected), \nand the code selector pushes The second column illustrates the laziness of our code selection edx onto \nthe mimic stack. Then the selector skips byte codes approach. For instance, the code generation of the \nbyte code at 503 to 516, and continues generating code from byte code 517 index 484 (getfield #34) is \ndelayed until the byte code at (getf ield #49). index 487 (getfield #45) and the code generation of the \nbyte code at index 490 (getfield #44) is delayed until the  3. GARBAGE COLLECTION SUPPORT byte code \nat index 496 (aaload) . In both cases, the code se- Java is a garbage collected (GC) language, moving \nthe burden of lector delays generating code for loading the object field until memory management from \nthe programmer to the system (i.e., the field s value is needed. the JVM). When the program runs low \non heap space, the gar- At the time when the code selector generates the fast native bage collector determines \nthe set of objects that the program instruction (mov ecx, [ebp+04h] ), it annotates ecx with may still \naccess-the live objects-and frees the space used by the expression tag <483,4> to indicate that ecx holds \nthe value dead objects. The garbage collector computes the set of live objects by starting with the set \nof references in global variables, in registers, and on the runtime stack (the root set), and locating \nall the references that can be reached from the root set by trav- ersing the graph of reachable objects. \nSome GC algorithms move live objects to a new place in memory [14]. The Intel JIT is designed to work \nwith moving as well as non-moving GC. Computing the root set requires the cooperation of the JIT, be- \ncause only the JIT is capable of precisely locating references held by local variables and by temporaries \n(which are assigned to either stack locations or registers); the JVM keeps track of which classes have \nbeen loaded and thus can determine the set of global variables containing references, without any support \nfrom the JIT. Moreover, the JIT is capable of performing analy- sis to identify only those stack locations \nand registers containing live references. The JIT keeps track of Java operand stack locations that contain \nreferences by computing a type bit vector for each GC site dur- ing the prepass. The type bit vector \nmarks those stack locations that contain live references at the GC site. Finding the set of variables \n(as opposed to operand stack loca- tions) containing references, however, is not trivial for the JIT, \nbecause of ambiguous types : the same variable may hold ref- erence and non-reference values at different \ntimes during the execution of the method. At a GC site, the JIT must distinguish between variables that \ncontain references and those that do not contain references; that is, the JIT must precisely enumerate \nthe complete set of variables containing valid references. In our development, we have experimented with \ndifferent strate-gies for detecting variables containing references: . For each variable that is ever \nused as a reference, keep an extra bit in the method s stack frame. The JIT generates code that dynamically \nupdates the bit on every write to one of these variables, and that initializes the bit in the method \ns prolog. (The bit is needed for tracking ambigu- ously-typed variables, as well as for tracking whether \na ref- erence variable is initialized.) The JIT uses these tags at CC time to decide which variables \nhold references. This approach incurs an overhead of an extra memory reference for every store to one \nof these variables, as well as initiali- zation overhead upon method entry. . We can refine this approach \nby using global data flow analysis to statically analyze ambiguously-typed variables, and record the \ninformation for each GC site. Liveness and SDK 1.5.1 Go 14.62 15.33 11.14 Jpeg 12.03 12.81 11.84 Lisp \n39.73 44.10 40.52 type analysis (which our priority-based global register allo- cator provides for free) \nallows us to determine all GC sites at which a variable (a) is initialized, (b) is live, and (c) contains \na reference. However, it is not possible in general to analyze all variables, because the specification \nof the JVM allows the same variable to hold either a reference or a primitive type value at the same \nbyte code instruction [ 17, Section 4.9.61. This ambiguity is due entirely to the j sr instruction. (The \nprogram is not allowed to reference such a variable at that point, but if it contains a reference, it \nmust still be enumerated at a GC site.) These kinds of variables require other techniques, such as the \ndynamic tagging ap-proach described above, to maintain reference information (another solution involves \nvariable splitting [l]). The dynamic approach has a higher runtime cost but a lower JIT-time cost than \nthe static approach, so it is unclear which approach is preferable. However, experiments with our bench- \nmarks show that the incidence of ambiguously-typed variables is small and the choice of a solution to \nthe enumeration problem will likely have a negligible impact on performance.  4. EXPERIMENTS We used \nseveral Java benchmark programs to test the effective- ness of individual optimizations described in \nthis paper, as well as combinations of the optimizations. Five of the benchmarks (Backprop from Spec92, \nand Compress, Go, JPEG, and Lisp from Spec95) were originally C programs, hand-translated into Java. \nOne benchmark, Java Cup, is a parser originally written in Java. We measured the wall-clock time of each \napplication, in seconds, averaged over several runs; the tests were measured on a 233MHz Pentium II with \n64 MB of RAM, running Windows NT4.0, Service Pack 3. Ihe Intel JIT is integrated with the Mi- crosoft \ns JVM in both SDK 1.5.1 and SDK 2.0. The table below compares the running times of Microsoft s JIT, Intel \ns JIT with all optimizations disabled, Intel s JIT with all optimizations enabled (using the simple global \nregister allocator described in Section 2.4.2), and Intel s JIT with all optimizations enabled (using \nthe priority-based register allocator described in Section 2.4.2). The optimizations include array bounds \ncheck elimination, common subexpression elimination, out-of-line exception throwing, and global register \nallocation. This table shows that the Intel JIT s performance is comparable to that of the Microsoft \nJIT. Note that both the Microsoft JIT and the JVM differ between SDK 1.5.1 and SDK 2.0, but essentially \nthe same Intel JIT is used with both SDKs. SDK 2.0 (a) Backprop 11.33 9.35 11.66 8.05 8.06 11.75 6.61 \n8.04 7.12 7.12 40.84 14.42 18.72 18.23 17.97 (c) Java Cup (4 Go (4 Jm Figure 4: Effect of optim.iz,ations \nunder SDK 1.5.1. Figures 4 and 5 show the performance improvement from the various optimizations. AU \nvalues are given as percent im-provements over the no-optimization case. Each figure is di- vided into \nthree categories (separated by dark lines): the effect of a single optimization, the effect of all but \none optimization, and the effect of all optimizations. When register allocation is considered, we use \n(S) and (P) to denote the simple and priority-baaed methods, respectively. Figures 4(a) and 5(a) show \nthe performance of Backprop. The key feature here is that register allocation is the most important optimization, \nand that the other optimizations have little effect on the overall performance. Both register allocation \nalgorithms perform roughly equally well. Note that register allocation accounts for a huge gain in overall \nperformance. Figures 4(b) and S(b) show the performance of Compress. While CSE and out-of-line exception \nthrowing each contribute to the overall performance, once again it is register allocation that is the \nmost important optimization. Figures 4(c) and 5(c) show the performance of Java Cup. Note that the performance \ndifferences are fairly small, and recall from the table above that the total execution time is only around \n1 second. Jn this example, it appears that the costs of executing the CSE and register allocation algorithms \n(the priority-based method in particular) do not make up for the resulting performance gains in this \napplication. Figures 4(d) and 5(d) show the performance of Go. Here, bounds check elimination and CSE \nare ineffective, while register allocation is important. In addition, out-of-line (b) Compress (a) B \nackprop  (c) Java Cup (4 Go  (d Jpeg 07 Lisp Figure 5: Effect of optimizations under SDK 2.0. exception \nthrowing is even more important, due to the amount priority-based algorithm assigns more variables to \nfewer of array bounds checking that must be performed. registers in at least two frequently-executed \nmethods. Figures 4(e) and 5(e) show the performance of JPEG. As Figure 6 shows the separation of the \nJIT time and execution time before, register allocation is an important optimization. In of each program \nunder SDK 2.0 (the results for SDK 1.5.1 are addition, CSE and out-of-line exception throwing produce \nsimilar). The CSE optimization is more expensive than other noticeable improvements, while bounds check \nelimination optimizations in terms of the JIT time. One noticeable aspect makes virtuallv no difference. \nfrom the figure is that the JIT time of non-optimization is more than the JIT time of turning on bounds \ncheck elimination. The Figure 4(f) and 5(f) show the performance of Lisp. We note that reasoning is that \nour bounds check elimination is fast and avoids bounds check elimination and out-of-line execption throwing \ngenerating the internal data structures for the eliminated bounds have little effect, whereas CSE costs \nmore to execute than it checking instructions that would be needed otherwise. For com- gains in runtime \nperformance. Particularly interesting is the fact putation-extensive programs (Backprop and Lisp), the \nJIT time that priority-based register allocation is noticeably more is negligible relative to the running \ntime. For those programs, effective than simple register allocation; the reason is that the the JIT compiler \ncan afford to apply aggressive global optimiza- tions; e.g., bounds check elimination, code hosting, \ninlining, and (b) Compress (a) B ackprop (c) Java Cup  63 Jpeg (0 Lisp Figure 6: JIT time and execution \ntime under SDK 2.0. code scheduling. However, for short-running applications like Java Cup, where JIT \ntime dominates the total execution, fast and effective code generation and optimizations are critical. \n 5. CONCLUDING REMARKS In this paper, we have presented the design and implementation of a Just-In-Tie \nJava compiler tailored for the Intel Architec-ture. We have described lazy code selection, the basic \ncode gen- eration strategy used in this JIT. We have shown lazy code se- lection to be a lightweight, \neffective way to fold Java stack op- erands and instructions into addressing modes of IA32 instruc-tions, \nthus reducing register pressure and allowing more inter- mediate values to be kept in scratch registers. \nLazy code selec- tion is fast (i.e., linear-time), and generates IA32 code directly from Java byte codes. \nWe have also described lightweight implementations of several standard optimizations, including common \nsubexpression elimi- nation, priority-based global register allocation, and array bounds check elimination. \nOur optimizations use memory spar- ingly because they do not use an explicit intermediate represen-tation; \nall optimizations operate directly on the byte codes plus additional data structures that are managed \non the fly. Using several benchmark programs, we have shown our optimizations to have various degrees \nof effectiveness, ranging from small (e.g., CSE) to large (e.g., register allocation). The Intel JIT \nserves as a framework for the design and evaluation of these and other lightweight optimizations for \nJava programs. A Just-In-Tie compiler is a critical component of a high-performance Java Virtual Machine \nimplementation. To achieve high performance, not only must a JIT compiler generate high quality code, \nbut it must also be fast because it executes as part of the Java application runtime. The complexity \nconstraints on a JIT compiler are therefore much stricter than a traditional static compiler+ JIT compiler \nmust sometimes trade off the quality of the generated code to achieve better compilation time and smaller \nmemory space requirements. The Intel JIT is represen- tative of this tradeoff: it achieves high performance \nby being fast and by generating good quality IA32 code.  REFERENCES VI 0. Agesen and D. Detlefs. Finding \nReferences in Java Stacks. Presented at the OOPSLA 97 Workshop on Gar- bage Collection and Memory Management, \nAtlanta, Octo-ber 1997. PI A. V. Aho, R. Sethi, and J. Ullman. Compilers: Principles, Techniques, and \nTools. Addison-Wesley, Reading, MA, second edition, 1986. [31 D. Bernstein, D. Q. Goldin, M.C. Golumbic, \nH. Krawczyk, Y. Mansour, I. Nahshon, and R.Y. Pinter. Spill code mini- mization techniques for optimizing \ncompilers. In Proceed- ings of the ACM SIGPLAN 89 Conference on Program- ming Language Design and Implementation, \npages 258- 263. ACM, July 1989. 141 P. Briggs, K.D. Cooper, K. Kennedy, and L. Torczon. Coloring heuristics \nfor register allocation. In Proceedings of the ACM SIGPUN 89 Conference on Programming Language Design \nand Implementation, pages 275-284. ACM, July 1989. I51 D. Callanhan and B. Koblenz. Register allocation \nvia hier- archical graph coloring. In Proceedings of the ACM SIGPL4N 91 Conference on Programming Language \nDe-sign and Implementation, pages 192-203. ACM, June 1991. 161 G. J. Chaitin, M. A. Auslander, A. K. \nChandra, J. Cocke, M. E. Hopkins, and P. W. Ma&#38;stein. Register allocation via coloring. Computer \nLanguages, 6~47-57, January 1981. [71 F. Chow. A Portable, Machine-Independent Global Opti-mizer-Design \nand Measurements. PhD thesis, Stanford University, 1984. PI F. C. Chow and J. L. Hennessy. A priority-based \ncoloring approach to register allocation. ACM Trunsactions on Pro-gramming Languages and Systems, 12501-535, \nOct. 1990. 191 D. W. Goodwin and K. D. Wilken. Optimal and Near- Optimal Global Register Allocation Using \nO-l Integer Pro- gramming. Software-Practice and Experience, 26:930-965, Aug. 1996. [lo] J. Gosling, \nB. Joy and G. Steele. The Java Language Specification. Addison-Wesley, 1996. [ll] Intel Corp. Intel Architecture \nSoftware Developer s Man-ual, order number 243192. 1997. [ 121 Intel Corp. Pentium Pro Family Developer \ns Manual, order number 000900-001. 1996. [13] Intel Corp. V Ame: Visual Tuning Environment. Available \nat httv://develouer.intel.corn/desien/Derftool/vtune [14] R. Jones and R. Lins. Garbage Collection. \nJohn Wiley &#38; Sons, 1996. [15] J. Knoop, 0. Ruthing, and B. Steffen. Lazy code motion. In Proceedings \nof the ACM SIGPLAN 92 Conference on Programming Language Design and Implementation, pages 224-234. ACM, \nJune 1992. [16] A. Krall and R. Grafl. CACAO-A 64-bit Java VM Just-in-Time Compiler. In Proceedings of \nthe ACM PPoPP97 Workshop on Java for Science and Engineering Computa-tion. [17] T. Lindholm and F. Yellin \nThe Java Virtual Machine Specification. Addison-Wesley, 1996. [18] G. Lueh and T. Gross. Call-cost directed \nregister allocation. In Proceedings of the ACM SIGPUN 97 Conference on Programming Language Design and \nImplementation, pages 296-307. ACM, June 1997. [19] G. Lueh, T. Gross, and A. Adl-Tabatabai. Global register \nallocation based on graph fusion. In Proceedings of the 96 Workshop on Languages and Compilers for Parallel \nCom-puting, pages 246-265. Aug. 1996. Springer-Verlag. [20] Microsoft b-q Corp. MS SDK 1.5.1. Available \nat [21] Microsoft Available Corp. at MS SDK 1.5.1 JIT Structure. htto://www.microsoft.com/iava/sdk/l5I/ \n vendor/vmOl5.htm [22] C. Norris and L. L. Pollock. Register allocation over the program dependence \ngraph. In Proceedings of the ACM SIGPLAN 94 Conference on Programming Language De-sign and Implementation, \npages 266-277. ACM, June 1994. [23] M. Poletto, D.R. Engler and M.P. Kaashoek. tee: A System for Fast, \nFlexible, and High-Level Dynamic Code Genera- tion. In Proceedings of the ACM SIGPLAN 97 Conference on \nProgramming Language Design and Implementation, pages 109-121. ACM, June 1997. \n\t\t\t", "proc_id": "277650", "abstract": "A \"Just-In-Time\" (JIT) Java compiler produces native code from Java byte code instructions during program execution. As such, compilation speed is more important in a Java JIT compiler than in a traditional compiler, requiring optimization algorithms to be lightweight and effective. We present the structure of a Java JIT compiler for the Intel Architecture, describe the lightweight implementation of JIT compiler optimizations (e.g., common subexpression elimination, register allocation, and elimination of array bounds checking), and evaluate the performance benefits and tradeoffs of the optimizations. This JIT compiler has been shipped with version 2.5 of Intel's VTune for Java product.", "authors": [{"name": "Ali-Reza Adl-Tabatabai", "author_profile_id": "81100032153", "affiliation": "Oracle Parkway, Redwood Shores, CA and Intel Corporation, 2200 Mission College Blvd., Santa Clara, CA", "person_id": "PP14023844", "email_address": "", "orcid_id": ""}, {"name": "Micha&#322; Cierniak", "author_profile_id": "81100326251", "affiliation": "Intel Corporation, 2200 Mission College Blvd., Santa Clara, CA", "person_id": "P196161", "email_address": "", "orcid_id": ""}, {"name": "Guei-Yuan Lueh", "author_profile_id": "81331498383", "affiliation": "Intel Corporation, 2200 Mission College Blvd., Santa Clara, CA", "person_id": "PP33032555", "email_address": "", "orcid_id": ""}, {"name": "Vishesh M. Parikh", "author_profile_id": "81100298860", "affiliation": "Intel Corporation, 2200 Mission College Blvd., Santa Clara, CA", "person_id": "P291734", "email_address": "", "orcid_id": ""}, {"name": "James M. Stichnoth", "author_profile_id": "81100435976", "affiliation": "Intel Corporation, 2200 Mission College Blvd., Santa Clara, CA", "person_id": "P133606", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/277650.277740", "year": "1998", "article_id": "277740", "conference": "PLDI", "title": "Fast, effective code generation in a just-in-time Java compiler", "url": "http://dl.acm.org/citation.cfm?id=277740"}