{"article_publication_date": "05-01-1998", "fulltext": "\n Improving Data-flow Analysis with Path Profiles* Glenn Ammons James R. Larust ammons@cs.wisc.edu larus@cs.wisc.edu \nDepartment of Computer Sciences University of Wisconsin-Madison 1210 West Dayton St. Madison, Abstract \n Data-flow analysis computes its solutions over the paths in a control-flow graph. These paths-whether \nfeasible or in-feasible, heavily or rarely executed-contribute equally to a solution. However, programs \nexecute only a small fraction of their potential paths and, moreover, programs execution time and cost \nis concentrated in a far smaller subset of hot paths. This paper describes a new approach to analyzing \nand optimizing programs, which improves the precision of data flow analysis along hot paths. Our technique \nidentifies and duplicates hot paths, creating a hot path graph in which these paths are isolated. After \nflow analysis, the graph is reduced to eliminate unnecessary duplicates of un- profitable paths. In experiments \non SPEC95 benchmarks, path qualification identified 2-112 times more non-local con-stants (weighted dynamically) \nthan the Wegman-Zadek con-ditional constant algorithm, which translated into l-7% more dynamic instructions \nwith constant results. Introduction Data-flow analysis computes its solutions over the paths in a control-flow \ngraph. The well-known, meet-over-all-paths formulation produces safe, precise solutions for general data-flow \nproblems. All paths-whether feasible or infeasible, heavily or rarely executed-contribute equally to \na solution. This egalitarian approach, unfortunately, is at odds with the realities of program behavior. \nEven moderately large programs execute only a few tens of thousands of paths (out of a universe of billions \nof acyclic paths) and, moreover, programs execution time and cost is concentrated in a far smaller subset \nof hot paths [BL96, ABL97]. This paper presents a new data-flow analysis technique that attempts to compute \nmore precise solutions along the hot paths in a program. Improved analysis along these paths This research \nsupported by: NSF NY1 Award CCR-9357779, with support from Sun Microsystems and Intel, and NSF Grant \nMIP-9625558. On sabbatical at Microsoft Research. Permission 10 make digital or hard copies of all or \npan of this wcrk for personal or classroom use is granted without ka provided that copies are not made \nor distributed for profit or ccmmwcial advan-tage and that copies bear this notice and the full citation \non the first page. To copy otherwise, 10 republish, 10 pcsf on sawen w 10 redistributa 10 lists, requires \nprior specific psrmiasion and/w a fw. SIGPLAN 98 Montrasl, Canada @ 1998 ACM 0-89791~987.4/98/0006...$5.00 \n WI 53706 can aid a compiler in optimizing these heavily executed por-tions of a program. Path-qualified \ndata-flow analysis con-sists of the following steps: 1. Identify hot paths by profiling a program. We \nuse a Ba&#38;Larus path profile [BL96] to determine how often acyclic paths in a program execute. 2. \nIdentify and isolate the hot paths in the program s control-flow graph (CFG). This step produces a new \nCFG in which each hot path is duplicated. Since a hot path is separated from other paths, data-flow facts \nalong the path do not merge with facts from other, overlapping paths. Moreover, as programs do not exe-cute \nmany hot paths, this hot-path graph (HPG) is not much larger than the original graph. 3. Perform data-flow \nanalysis on the HPG. The solutions found by this technique are conservative in the hot path graph-not \nin the original control-flow graph. 4. Reduce the graph to preserve only valuable solutions. The HPG \nduplicates code for paths whose solutions did not improve. Extra code both increases the cost of subsequent \ncompiler analyses and adversely affects a processor s instruction cache and branch predictor. Reduction \nuses results from the data-flow analysis and frequencies from the path profile to decide which paths \nto preserve in the TI&#38;K~ hot-path graph (THPG). 5. Translate the original path profile into a path \npro-file for the rHPG, so profiling information is avail-able for subsequent analyses and optimizations. \nBall-Lams path profiles are determined by a set of recording edges, which start and end paths. The algorithm \nthat produces an HPG also identifies recording edges in the HPG, which allows interpretation of the original \npath profile as a path profile of the HPG. The reduction step properly maintains these recording edges. \n  The technique can be applied to any data-flow prob-lem, although this paper focuses on constant propagation. \nIn experiments on SPEC95 benchmarks, path qualification identified 2-112 times more non-local constants \n(weighted dynamically) than the Wegman-Zadek conditional constant algorithm, which translated into l-7% \nmore dynamic in-structions with constant results. Moreover, the technique is practical. With the exception \nof the go benchmark, the hot- path graphs were 3-32% larger and the reduced hot-path graphs were only \nl-7% larger than the original CFG. On go, the hot-path graphs were 184% larger and the reduced hot-path \ngraphs 70% larger. 1.1 Qualified Flow Analysis Our implementation is based on Holley and Rosen s qual-ified \nflow analysis technique [HR~I]. A qualified data-flow problem is a conventional data-flow problem together \nwith a deterministic finite automaton, A, whose transitions are labelled by the edges of the control-flow \ngraph, G. A en-codes additional information about the program-in path-qualified analysis, it recognizes \nhot paths. Data-flow analy-sis answers questions of the form What can be said about the data-flow value \nat vertex v? Qualified data-flow analy-sis answers questions of the form What can be said about the data-flow \nvalue at vertex v given that A is in state q? Holley and Rosen used qualified data-flow analysis to identify \ninfeasible paths and exclude them from analysis. They created an automaton in which infeasible paths \nended in a failure state. The best solution at v, given that A is not in the failure state, is the meet \nover the non-failure states of A. For path qualification, we use the Aho-Corasick al-gorithm [Aho94] \nto construct an automaton that recognizes hot paths in a path profile. Holley and Rosen describe two \ntechniques for solving qualified problems, data-flow tracing and conted tupling. This paper uses data-flow \ntracing, which constructs a new graph GA whose vertices encode the vertex from G and the state from A. \nThe qualified problem is then solved as a con- ventional data-flow problem over GA-qualified solutions \nin G have become true solutions in GA. GA is, of course, our hot-path graph. The qualified solution is \nnever lower in the lattice than the unqualified solution. To see why, consider the solution at vertex \nv of G. If P is the set of all paths from routine entry to v and I, is the data-flow value from path \np E P, then the meet-over-all-paths solution 1, at v is given by 1, = A 1,. PEP l It describes how to \nreduce the hot-path graph, by elim- inating paths that prove unnecessary or unprofitable. l It shows \nhow to preserve path-profiling information through the CFG transformations. l It applies path qualification \nto constant propagation and demonstrates a significant improvement over the widely-used Wegman-Zadek \ntechnique. 1.3 Outline of the Paper This paper is structured as follows. Section 2 sketches the theoretical \ngroundwork and formalizes path profiles. Sec-tion 3 describes the automaton that recognizes the hot path \nin the profile. Section 4 shows how data-flow tracing con-structs a new control-flow graph with duplicated \nhot paths. Section 5 shows how to reduce the traced graph. Section 6 presents the results of our experiments \non SPEC95 bench-marks. Section 7 discusses related work. 2 Preliminaries This section states definitions \nand theorems used in the rest of this paper. 2.1 Data Flow Problems We begin with standard definitions \nof data-flow problems and their solutions. Definition 1 A monotonic data-flow problem D is a tuple (L, \nA, F, G,r, l,, M) where: l L is a complete semilattice with meet operation A. l F is a set of monotonic \nfunctions from L to L. Now partition P by the state of A. If Q is the set of states of A, Pp C_ P is \nthe set of paths in P that drive A to state q E Q. It is clear that A 1, = A A 1,. PEP nEQPEP, Or, omitting \nthe outer meet on the right hand side and converting the equality to an inequality, for all q E Q PEP \nPEP, The inequality is not strict, so the qualified solution is not necessarily sharper than the meet-over-all-paths \nsolu-tion. However, when it is sharper, it is doubly beneficial to find this increased precision in heavily \nexecuted code. 1.2 Contributions This paper makes four contributions: l It shows how path profiles can \nimprove the precision of data-flow analysis through guided code duplication. l G = (V, E) is a control-flow \ngraph with entry vertex r. l 1, E L is the data-pow fact associated with I-. l M : E + F maps the edges \nof G to functions in F. M can be extended to map every path p = [eo, el, . . . , ek] in G to a function \nf : L + L: f = M(p) = M(ek) 0 M(ek-1) 0 . . . 0 M(eo) The next three definitions come from Holley and \nRosen p~8i]. Definition 2 A solution I of D is a map I : V + L such that, for any path p from r to a \nvertex: u, I(u) < (M(p))&#38;). Definition 3 A fixpoint J of D is a map J : V -+ L such that J(r) 5 \nI, and, if e is an edge from vertex u to vertex v, J(v) 5 (M(e))(J(u))- Definition 4 A good solution \nI of D is a solution of D such that, for any fipoint J, J(u) < I(u) for all u E V. 2.2 Traced Data Flow \nProblems As mentioned in Section 1, a qualified data-flow problem is qualified with respect to a finite \nautomaton. This paper only considers qualification automata: Definition 5 If G = (V, E) is a control-flow \ngraph, a qual- ification automaton is a complete, deterministic finite au-tomaton with transitions labelled \nby elements of E. Given a data-flow problem D over G and a qualification automaton A, a traced data-flow \nproblem separates solu-tions by states of A, as well as by vertices of G. More formally: Definition 6 \nLet D = (L, A, F, G,r,&#38;, M) be a data-flow problem over G and A be a qualification automaton. Then, \nthe traced data-flow problem DA is a data flow problem (L,A,F,GA,TA,hA,MA) where L, A, and F are as in \nD. GA = (VA, EA) is a control-jldw graph with entry ver- tex TA = (r,q6). For v E V and q E Q, (v,q> \nE VA if there exists a path p from I-to v that drives A from its start state qC to q. In addition, ((~0, \ngo), (VI, ql)) E EA if there exists an edge (VO, vl) E E and a transition in A from qo to ql on (vo,vl). \n= 1,. 1PA MA : EA + F is defined by MA(((vo,QO),(VI,~~))) = M((vo,vI)) where (vo,vl) E EA. DA is a data-flow \nproblem that can be solved by conven-tional means. Holley and Rosen prove the following theo-rem [HlUl, \nTheorem 4.21: Theorem 1 If IA is a good solution of DA, then the solu- tion I of D given by I(v) = A{~A((%Q)) \n: b,d E h, IL = v}, for all v E V, is a good solution of D. The meet in the theorem may lose information. \nAs men- tioned above, since 1,~ at (v,q) need only meet over paths to v that also drive the finite state \nmachine to q, IA((v,q)) can be strictly more precise than I(v), even if I is the meet- over-all-paths \nsolution. An algorithm for producing DA from D is given in Sec- tion 4.  2.3 Path Profiles A path profile \ncounts the number of times that a program traverses acyclic paths in a routine s CFG. Path profiles, \n along with a low-overhead algorithm to obtain them, are described in papers by Ball and Larus [BL96] \nand by Am- mons, Ball, and Lams [ABL97]. The acyclic paths recorded in a profile start and end at recording \nedges. The set of recording edges, R, is, at a minimum: edges from the entry vertex, edges into the exit \nvertex, and retreating edges. Thus, removing the recording edges turns G into an acyclic graph. Additional \nedges may also be designated recording edges.   Entry I clA i=O n=i 0I t Exit Figure 1: Our running \nexample. Dashed lines indicate edges along which the Ball-Larus path profiling algorithm records the \ncurrent path. Without path qualification, only the as-signments of constants are constant instructions. \nDefinition 7 Given a control-flow graph G and a set of recording edges R, a Ball-Laxus path is a placeholder \n(e) followed by a path in G from the target WJ of some TO E R to the target VI of some T1 E R, which \ncontains no recording edges besides v-1. The set of all such paths is denoted by PBL. Given a Ball-Lam \npath pBL = [a, eo, el, . . . , ek] and a path p in G, we say that pBL is a subpath of p if [eo,el,. . \n. , ek] occurs as a subpath of p that immediately follows an edge in R. The l is a reminder that Ball-Larus \npaths start with a recording edge. More than one recording edge may target a vertex, as for a doubly-nested \nloop in which both loops share a header. For Ball-Lams paths that start at such a node, the reminder \ndoes not specify which recording edge started a path. As a running example, we will use the problem of \nfinding instructions with constant results in the program in Figure 1. Without path qualification, the \nonly constant instructions are the assignment statements in vertices A, C, D, F, and G. The dashed edges \nin Figure 1 designate recording edges. In this example, b,B, C,E,F,H,B] is a Ball-Lams path, while [o, \nC, E, F, H, I] is not a Ball-Larus path. Definition 8 Given a multiset of paths P through a control- \nflow graph G and a set of recording edges R, a path profile is a multiset Ppp of Ball-Larus paths such \nthat pBL E PBL occurs in PPP with multiplicity equal to the number of times PBL occurs as a subpath of \npaths in P. Consider the example in Figure 1. Writing the paths as lists of vertices, if P were: ( 70 \ntimes) [Entry, A, B, C, E, F,H,I,Exit] . (0) CJ%D) x Figure 2: A path profile for the example. (5 \ntimes) [Entry, A, B, D, E, F, H] . [B,D, E, G, HI6 .[B, D, E, F, H, I, Exit] (25 times) [Entry, A, B, \nD, E, F, H] . [B, D, E, G, HI3 .[B, D, E, F, H, I, Exit] the path profile is shown in Figure 2. 3 Creating \nthe Automaton This section describes an algorithm to construct a deter-ministic finite automaton that \nrecognizes hot paths. The algorithm is an application of the Aho-Corasick algorithm for matching keywords \nin a string [Aho94]. In our case, the keywords are hot Ball-Larus paths. The constructed DFA is used \nas the qualification automaton for data-flow tracing. The Aho-Corasick algorithm begins by constructing \na retrieval tree (also known as a trie) from a set of keywords. A retrieval tree is a tree with edges \nlabelled by letters from the alphabet, which satisfies two properties. First, each path from the root \nof the tree to a node corresponds to a prefix of a keyword from the set. Second, every prefix of every \nkeyword has a unique path from the root that is labelled by letters of the prefix. Given a set of keywords, \nconstructing its retrieval tree takes time proportional to the sum of the lengths of the keywords. In \nour case, the alphabet is edges in a CFG and key-words are hot paths. Assuming that all paths in Figure \n2 are hot, Figure 3 shows the retrieval tree. Our algorithm for constructing the retrieval tree consists \nof the following steps: Identify the hot paths. In our experiments, we selected the minimal set of paths \nthat executed a fixed fraction CA (e.g., 97%) of the dynamic instructions in a training run. Hot paths \nwere selected by considering each path, ordered by the number of instructions executed along the path \n(length times frequency), and marking paths hot until CA of the dynamic instructions were covered. Trim \nthe final recording edge from each hot path. The constructed automaton will recognize these trimmed paths. \nTrimming paths ensures that the automaton returns to the same state after any recording edge. Construct \nthe retrieval tree for the set of trimmed hot paths. Note that only one edge in the retrieval tree is \nlabelled by 0. In general, we make this definition: Definition 9 q. is the target of the retrieval tree \nedge la- belled by 0. 16 0 Figure 3: A retrieval tree for the path profile in our example. In Figure \n3, q. = qo. In Aho-Corasick, pattern matching steps through an in- put string while making transitions \nbetween vertices of the retrieval tree. At each step, if an edge from the current ver-tex in the tree \nis labelled by the next letter in the string, that edge is followed. If a leaf of the tree is reached, \na match has been found. If no edge from the current vertex is labelled by the next letter (a), the current \nvertex (q) is reset by consulting a failure function, h(q, a). The failure function avoids rescanning \nthe input string, by resuming scanning in the retrieval tree state correspond-ing to the longest keyword \nprefix that could lead to a match. If this prefix is nonempty, it must consist of a proper suffix of \nthe match that just failed followed by a. A Ball-Larus path p starts with a l representing a record- \ning edge and ends with another recording edge. No other edges in p are recording edges, by definition. \nThus, no paths start with a substring from the middle of another path, so the failure function always \nresets the automaton. The fol-lowing theorem shows that the failure function becomes triv- ial. Theorem \n2 Say q,, is the retrieval tree vertm representing the keyword prejix u. For any Aho-Corasick rewgnizer \npro-duced from a set of trimmed Ball-Larus paths, h(q,, u) = qeif a is not a recording edge and h(q,, \na) = q. if a is a record- ing edge. Proof: Suppose v is the longest proper su#ix of u that is also a \nprefix of some tm mmed path in the profile. v must start with a l , which represents a recording edge. \nBut no proper sufi of u contains a recording edge, so Iv1 = 0. If a is not a recording edge, it cannot \nbegin a Ball-Larms path and h(qU, u) = qt. If a is a recording edge, then it is equivalent to l and so \nh(qU, a) = q.. Since the failure function is trivial, our implementation only stores retrieval tree edges, \nwhich greatly reduces its size. If the automaton is in state q and sees the input a, the next state is \nfound by checking: l If there exists a retrieval tree edge from q labelled by a, the next state is the \ntarget of the edge. l If a is a recording edge, the next state is q.. l Otherwise, the next state is \nqC. 4 Building the HPG This section explains how a hot path graph (HPG) is con- structed. The algorithm \nboth produces a graph for data flow analysis and also identifies recording edges in that graph, SO that \npath profiling information can be carried over to later stages of compilation. Section 4.1 applies Holley \nand Rosen s data-flow tracing algorithm to the original graph and the path qualification automaton from \nSection 3. The output of the tracing algo-rithm is a hot path graph without recording edges--GA from \nDefinition 6. In this HPG, every path from entry represents both a path in the original CFG and a path \nin the automa-ton. Moreover, two paths from entry end at the same vertex iff the corresponding paths \nin the CFG end at the same ver- tex and the corresponding paths in the automaton end in the same state. \nThus, data-flow solutions over the HPG do not merge values from paths that reach different automaton \nstates. Section 4.2 explains how our algorithm also identifies the recording edges in the HPG, so that \nthe path profile infor- mation can be correctly interpreted in the modified CFG. Holley and Rosen discuss \ntwo qualification methods, one of which is data-flow tracing. The other method is context tupling. Section \n4.3 explains why we use data-flow tracing instead of context tupling for path qualification. 4.1 Tracing \nthe HPG Figure 4 presents Holley and Rosen s algorithm for data-flow tracing, extended to identify recording \nedges (discussed in the next subsection). The algorithm is a worklist algorithm that finds all pairs \nof CFG vertices and automaton states reachable from the entry of the CFG (T) and the initial state of \nthe automaton (qc). The vertices of the HPG are these pairs. Initially, the worklist holds (r,qr). Each \niteration of the While loop removes a pair (v,q) from the worklist. The algorithm iterates over each \npair (v ,q ) reachable in one step from (v, q). If (v , q ) is not in the HPG, it is added to the HPG \nand the worklist. In any case, an edge is added from (v,q) to (v ,q ). The algorithm terminates when \nthe worklist is exhausted, at which point all possible pairs have been added to the HPG. The constructed \nHPG fits the definition of GA in Defi- nition 6. The following theorem, together with Theorem 1, justifies \nperforming data-flow analysis on the HPG. Theorem 3 When the algorithm in Figure 4 completes, (v,q) E \nVA iff there exists a path p in G from r to v that drives A from its start state qC to q. G = (V, E) \nis a control-flow graph. A is a qualification automaton. Q is the set of states of A. qe is the start \nstate of A. T is the set of transitions in A. R C E is the set of recording edges. IV% a worklist of \npairs (v,q), where v E V and q E Q. GA = (VA,EA) is the hot path graph. RA 2 EA is the new set of recording \nedges. 2 +-~b-,q.)l R::te W +-(r,n.)While W # 0 (v, d + Take(W)ForeachEdge (v,v ) E E (q, (v, v ), \nq ) E T (it is unique) If (V',(f) e VA VA + VA u (V',(f) putw, (v', n')) EA t EA U {((v,q), (v ,q ))} \nIf (v,v ) E R RA + RA u {((%q), (V , i))) Figure 4: An algorithm for data-flow tracing. The original \nHolley-Rosen algorithm has been extended to mark record-ing edges in the traced graph. Proof: The only-if \ndirection of both requirements is ob- vious. The %f direction follows by induction on the length of the \npaths p. For paths of length 0, both requirements are trivially true. If i) holds for all paths up to \nsome length n and ii) holds for all edges reachable along such paths, then all final nodes of such paths \nmust have been added to the worklist at some point in the algorithm. After these nodes are processed, \nthe requirements hold for all paths up to length n+ 1. Figure 5 shows the example after data-flow tracing. \nThe automaton is in state q. at shaded vertices and state qa at vertices filled with diagonal lines. \nOnly these vertices are targeted by multiple edges, as qI and qo are the only states in the automaton \nreached by multiple transitions. The original graph had no constant results other than simple assignments, \nbut the graph in Figure 5 has several new constant results: a + b is always 6 at H14, 5 at H12 and H15, \nand 4 at H13, i + + is 1 at H14 and H15, and n is always 1 at 117. Unfortunately, although the original \nflow graph in Fig- ure 1 is reducible, the rHPG in Figure 5 is not. For example, the edge (HE, BO) is \na retreating edge but not a backedge in a natural loop since BO does not dominate He. Because of this \nproblem, tracing should only be used with data-flow solvers that can handle irreducible graphs.  4.2 \nIdentifying Recording Edges in the HPG ii) ((%no), (us, VI) (vl,ql)) E E and E EA iff a transition there \nexists an edge in A from qa to q1 on The algorithm ((vo,qa), (v~,ql)) in Figure a recording 4 makes an \nedge iff (ve,vr) HPG edge is a record- (VO,Wl). ing edge in the original graph. The next two lemmas show \n Figure 5: The control-flow graph of our example, traced with respect to the automaton in Figure 3. Dashed \nlines indicate edges along which the Ball-Larus path profiling algorithm records the current path. The \nautomaton is in state qC at shaded vertices and state qo at vertices filled with diagonal lines. Original \nPath Traced Path .,A,B,C,E,F,H,I,Exit .,AO, Bl, C3,E6,FlO, H14,17, Exit01 [*,A,B,D,E,F>H,B] [*,B,D,E,G,H,Bll \n,B,D,E,F,H,I,Exit ] [.,AO,Bl,D4,E7,Fll,H15,BO] [e, BO, D2, E5, G9, H13, BO] ., BO, D2,E5,F8, H12,116, \nE&#38;O] Figure 6: For each Ball-Larus path in the original path pro- file, the corresponding Ball-Larus \npath through Figure 5. that these edges suffice to interpret the path profile from the original graph \nin the HPG. Lemma 1 Let p be a path from entry to edt in G and pA the corresponding path in the HPG. \nThen, a Ball-Lam path begins at the kth edge inp $a Ball-Larus path begins at the kth edge in pa. Proof: \nBy the construction in Figure 4, if the kth edge in p is a recording edge, the kth edge in pA is a recording \nedge, and vice versa. Lemma 2 Given a Ball-Larus path [o,vo,vl,. . . ,vk] in G, the corresponding HPG \ncontains exactly one Ball-Larus path [ ,(VO,~O),(vl,ql),...,(vk,~k)]. Proof: By Lemma 1, some such Ball-Larus \npath exists in the HPG. All automaton transitions on recording edges target q.. Thus, qo = q.. By requirement \nii) of Theorem 3 and the determinism of A, the rest of the path is determined. Lemma 1 implies that there \nis a one-to-one and onto map between any path profile of G and a path profile of the HPG. Lemma 2 implies \nthat this map is unique and gives a way to construct the map: given a path from the original path profile \nthat begins at v, start the path for the HPG profile at (v, 4,). The rest of the path can be laid out \neasily. Figure 6 shows the Ball-Larus path through Figure 5 for each path in Figure 2. It is instructive \nto consider graphs in which the lemmas would fail. Lemma 1 would fail if the edge (H15,BO) in Figure \n5, which is not a retreating edge, were not a record- ing edge. This is simply because (H, B) in Figure \n1 is a recording edge, so the original path profile does not track correlations across it. Lemma 2 would \nfail if recording edges targeted more than one B vertex. This could happen if, for example, tracing \nwere allowed to unroll loops. 4.3 Context Tupling In their original paper on qualified data-flow problems, \nHol-ley and Rosen presented two techniques for solving qualified problems [HR81]. Their first technique, \ndata-flow tracing, has already been discussed. Their other technique is context tupling. While data-flow \ntracing solves a conventional data-flow problem over an expanded graph, context tupling solves a tupled \ndata-flow problem over the original graph. Intu-itively, data-flow tracing tracks the state of A in the \ncontrol- flow graph, while context tupling tracks the state of A in the lattice of values. We chose data-flow \ntracing over context tupling for three reasons: l Data flow tracing is easier to understand and explain. \nl We envision that path profiles will be used through- out a sequence of compiler analyses and optimizations. \nWith data-flow tracing, each analysis sees the results of previous passes through a modified control-flow \ngraph. It is not obvious how to pass this information with con-text tupling. l Holley and Rosen did not \nfind context tupling to be any more efficient than data-flow tracing. Its main advantage is that context \ntupling does not produce irreducible CFGs, so elimination solvers can be used with context tupling. 5 \nReducing the Traced CFG This section describes our algorithm for identifying and eliminating vertices \nin the HPG that have either coarse data- flow results or low execution frequencies. Figure 7 illustrates \nthe need for this step. This chart shows the cumulative distribution of the number of execu- tions of \ninstructions with constant results by basic block. Constants that can be found solely through analysis \nwithin a basic block are excluded. For example, just 11 vertices account for virtually all nonlocal constants \nin compress. At the other extreme, 10,000 vertices are necessary to cover the constants in go. Tracing \nadds about 100 vertices to compress and over 93,000 vertices to go. Even if all vertices that contribute \nconstants were included among the dupli-cated vertices, most new vertices would contribute only an insignificant \nimprovement in the solution. The reduction algorithm eliminates many of these useless vertices. {Entree), \n{AO), 0% Bl), {CE, C3), W, D41, {Ee, E5, E6, E7}, {FE, F8,FlO, Fll}, {GE, G9}, Number of blocks Figure \n7: The distribution of dynamic executions of constant instructions by basic block in selected SPEC95 \nbenchmarks. In this paper, we describe the reduction algorithm in terms of constant propagation. However, \nthe algorithm is not restricted to constant propagation. All that is necessary is a way to assign a benefit \nto duplicating a vertex in the HPG. The reduction algorithm is a heuristic algorithm with the following \nsteps: 1. Identify the hot vertices. First,, the vertices are ordered by the number of dynamic constants \nthey execute, as computed from the path profile. In our experiments, we chose a fixed fraction CR of \nthe (nonlocal) dynamic constants as a goal. Vertices are added to the set of hot vertices until CR is \nreached. In our running example, Hl2 weighs 30, H13 weighs 100, H14 weighs 140, H15 weighs 60, and 117 \nweighs 70. All the other vertices have weight 0. For the sake of the example, suppose cR is chosen such \nthat H13 and H14 are the only hot vertices. 2. For each vertex v in the original graph, partition the \nvertices (v, q) in the HPG into sets of vertices that are compatible. This is the heuristic step of the \nalgorithm. Call the partition II. At this stage, two vertices are compatible if neither vertex is hot \nor, if one or both is hot, lowering both solutions to the meet of their lattice values does not destroy \nany constants in a hot vertex. Compatibility is not an equivalence relation (it is not transitive), so \nII cannot be found by looking for equiva- lence classes. Instead, II is formed greedily by throw-ing \nin vertices one at a time. As each vertex (v,q) is thrown in, it merges with the first S E IX for which \nadding (v,q) to S does not destroy constants in a hot vertex. If there is no such S, (v,q) starts a new \nset. Our implementation tries to keep hot vertices to-gether by considering the vertices in descending \norder by weight. In the example, since H13 and H14 are the only hot vertices, II is @,;,, H15), Q-I13), \n(H141, (kI16, II7), xi 3. Use the standard DFA minimization algorithm [Gri73] to produce a partition \nII which respects the data-flow solutions. The complexidty of this algorithm is O(7LlogTZ). Why is this \nalgorithm applicable? The HPG can be thought of as a finite automaton with edges labelled by the edges \nof the original graph. The elements of II can be thought of as equivalence classes of final states of \nan automaton that recognizes several different kinds of tokens. The only way to lower the solution over \na set S E II is to cause some new path p from entry to reach a vertex in S. Viewing p as a string, that \nwould mean that p was not recognized as a token of type S before minimization but is recognized as such \na token after minimization. This cannot happen. In our example, the minimized partition II is -Pntveh \n{AOh WXI, @lb {ce), {C3), {D2), {D4), 0% E71, {E5), @6), {FE, F8, Fll), VW, {GE), {G% @,H12,H15), {H13), \n{H14}, {IE, 116,117}, {ExitO} 4. Replace the vertices in each set in II with a rep-resentative and produce \na new set of recording edges. If SO,&#38; E II have representatives SO and 51, respectively, an edge \n(SO, ~1) exists iff an edge ((v~,qo), (vI,~I)) exists in the HPG, where (vo,qo) E SOand (vl,ql) E Sl. \nIf ((vo,qo),(vl,ql)) isarecording edge, (SO, ~1) is a recording edge. This is well-defined: for (vo,no),(vo,d) \nE So ad (vl,ql),(vl,d) E Sl, ((vo,qo),(v1,ql)) is a recording edge iff (vo,v~) is a recording edge in \nthe original graph and the same for ((vo, q&#38;), (~1, q:)), so ((vo, qo), (VI, 41)) is a recording \nedge iff ((~0, qb), (VI, q{)) is a recording edge. Figure 8 shows the reduced hot path graph for our \nrunning example. 6 Experimental Results This section presents measurements of the benefits and costs \nof using path-qualified flow analysis for constant propaga-tion. We implemented the analysis as two new \npasses in the SUIF compiler [WFW+]. The fist pass, PP, instrumented a C program for path profiling. The \nother pass, PW, used a path profile to perform path-qualified constant propagation. The first step was \nto produce a path profile for each routine in the program. In this stage, SUIF compiled a C program into \nits low-SUIF intermediate form. The PP pass instrumented this intermediate code for path profiling. We \ndid not run SUIF s optimization passes. The SUIF-to-C converter transformed PP s output into C code, \nwhich was compiled by GCC into an instrumented program. When run, this program produced a path profile. \nThe next step was to optimize programs. The program was again compiled by SUIF. This time, the SUIF code \nwas I Proeram I Nodes I Paths I Hot Paths I ComDile Time I Anal. Time I r vortex I 21190 I 1729 I 152 \nI 1042 I 163 1 Table 1: General information about the benchmarks. Nodes is the total number of CFG nodes \nin the original program. Paths is the number of Ball-Larus paths executed in the training run. Hot Paths \nis the number of paths needed to cover 97% of a training run s dynamic instructions. Compile Time is \nthe total compile time (seconds) without constant propagation. Anal. Time is the total time (seconds) \nrequired for constant propagation with CA= 0. t Exit Figure 8: The control-flow graph after reduction. \nState numbers have been dropped from all merged vertices. fed to PW together with the previously obtained \npath pro-file. PW used the path profile to construct a hot path graph, discover constants, produce a \nreduced hot path graph, and finally generate optimized code. The output of PW was con- verted to C code, \nwhich was compiled by GCC (-02) into an optimized executable. As SUIF did not directly generate assembly \nor machine code, our evaluations are in terms of the SUIF intermediate code. In this paper, by instruction \nwe always mean SUIF instructions, not machine instructions. The constant propagator in PW uses Wegman \nand Zadek s Conditional Constant algorithm [WZSl]. This al-gorithm is a worklist algorithm that symbolically \nexecutes a routine, starting at its entry node and propagating values only across the legs of branches \nthat can execute, given the current assignment of values to variables. Our implementa-tion is conservative, \nas it does not track pointers or constants manipulated through pointers or structures, sssumes that calls \nand assignments through pointers write to all aliased variables, and initially sets all variables to \n1. Since we ran the constant propagator immediately after SUIF s front end, the constant propagator saw \ncode that was very close to the original C. We ran PP and PW on seven of the C SPEC95 bench-marks on \na Sun UltraSPARC SMP. In all cases, we used an input data set from the SPEC train data to produce the \npath profile that drove the flow analysis. A different and larger input from the ref data set produced \na path profile used to evaluate the effectiveness of the constant propaga-tor. The path profile of the \nreference input did not affect the optimization; it was only used to compute the dynamic number of constants \ndiscovered by the propagator. Path-qualified analysis becomes more expensive as the number of hot paths \nincreases. On the other hand, consid-ering more paths can improve a solution, as it increases the portion \nof the program s execution covered by an analyzed path. To quantify this tradeoff, we ran the path-qualified \nanalysis several times, varying path coverage-the CA pa- rameter in Section 3. That is, the analysis \nwas first run on the minimum set of paths that covered three quarters of the program s execution, then \non paths that covered seven eighths of the execution, and so forth. The other parameter in our analysis \nis CR, the benefit cutoff for the graph reduction algorithm. In the experi-ments, we set CR to .95, so \nreduction preserved approxi-mately 95% of the nontrivial constants discovered by con-stant propagation. \nThis value was arrived at empirically. Table 1 lists basic information about the benchmarks. Most of \nthe analysis time for per1 was spent in two huge routines, yylex and eval, for which the non-linear running \ntime of constant propagation became a problem. 6.1 Benefit of Path Qualification Figure 9 shows that \nthe number of executed instructions with statically constant results increased as the hot path coverage \nincreased. At full coverage (CA = l), the improve- ment ranged from 7% for m88ksim and vortex to 0.6% \nfor perl. In all benchmarks, most of the benefit of path qual-ification was attained before full coverage \nwas reached-typically somewhere above 90% coverage. ijpeg attained most of its benefit at CA = 0.75 (the \nlowest non-zero value tested) but all benchmarks saw virtually all of their bene-fit by cA = 0.97. In \ntwo cases, the improvement degraded slightly at high coverage, because of heuristics in the reduc- tion \nalgorithm. These results confirm earlier path profiling vortex sped up by 1.9% while m88ksim sped up \nby 9.8%. Similarly, the largest slowdown was in Ii, yet per1 had the smallest increase in constant instructions. \nFigure 9: Increase in instructions with constant results (weighted dynamically) versus the level of path \ncoverage. The baseline is the number of constants at CA = 0 (Wegman-Zadek). Table 2: Effects of path-qualified \nconstant propagation on run- ning time. Base is the running time in seconds of the program after Wegman-Zadek \nconstant propagation. Optimized is the running time in seconds of the program after path-qualified con-stant \npropagation with cA set to 0.97 and CR set to 0.95. All running times are on the ref data set. Speedup \nreports the improvement in running time. measurements, which show that a small kernel of hot paths dominate \na program s execution. 6.1.1 Running Time We measured each benchmark s execution time after con-stant \npropagation with CA = 8.97 and CR = 9.95 and compared this time against the program s time with only \nWegman-Zadek constant propagation (CA = 0). The runs were on the ref data set. The best time from three \nruns is reported. The relationship between constants and program speed is not clear. The three benchmarks \nwith the largest number of newly discovered constants sped up, while the other bench-marks slowed down. \nHowever, the change in execution time was not proportional to the increase in the number of con- stants. \nFor example, the speedup for go was almost equal to that of m88ksim, but go only showed a 4% increase \nin constant instructions while m88ksim showed a 7% increase. (Keep in mind that the increase in constant \ninstructions is a dynamic measure.) Also, m88ksim and vortex had ap-proximately the same increase in \nconstant instructions, but Furthermore, running times do not seem to relate easily to the increase in \nprogram size. For example, go had a good speedup, but its size increased by the largest amount (Figure \n11). To be fair, this experiment did not control for several significant factors. First, the IMPACT group \ns work on su- perblock scheduling [mWHMC+93] found that tail dupli-cation, like that done to isolate \nhot paths, can expose large amounts of instruction-level parallelism. Thus, running time improvements \nare not necessarily due to improvements in constant propagation. Second, we rely on GCC to perform all \noptimizations beyond constant propagation, but GCC may produce poor code for the irreducible graphs produced \nduring tracing. Third, because a node can have at most one fall-through predecessor, tracing can introduce \nextra jumps. For example, E can fall through to F in Figure 1, but it is im- possible for both E5 and \nE to fall through to F in Figure 8. PW could use the path profile to place these jumps more intelligently \nor to further duplicate code to avoid jumps al-together, but our implementation does neither. Fourth \nand last, our experiments did not measure the effect on the in- struction cache or branch predictor. \n  6.2 Classifying Constants This section examines the constants discovered by path qual- ification. \nThe Venn diagram in Figure 13 classifies dynamic instructions based on the type of analysis required \nto iden- tify them as either constant or dynamic. The categories are: Local These instructions can be \ndetermined to be constant with local analysis-that is, by scanning their enclos-ing basic block. In Figure \n1, assignments to a and b are local constants. Iterative These instructions can be determined to be con- \nstant by Wegman-Zadek iterative analysis. We found these constants by running the constant propagator \nwith CA = 0. MOP These instructions are found to be constant by a meet-over-all-paths solution. Constant \npropagation is not a distributive problem, so an iterative solution may be less precise than a meet-over-all-paths \nsolution. We cannot measure this category directly. Qualified These instructions are found to be constant \nby the path-qualified analysis. This set does not contain MOP, nor does MOP contain Qualified. The inter- \nsection of the two sets includes Iterative and pos-sibly other constants. Because we cannot measure MOP, \nwe cannot measure the intersection precisely. The Identical and Variable sets below attempt to approximate \nthis intersection. Identical These instructions include all Iterative instruc-tions, plus instructions \nnot found by Wegman-Zadek for which path-qualified analysis finds the same con-stant value everywhere \nthey are duplicated. These constants would also be found by meet-over-all-paths. Variable These instructions \nare found to be constant by the qualified analysis, but have different values at dif- ferent sites in \nthe reduced graph. For example, in Fig- ure 8, the value of a + b is 6 at H14 and is 4 at H13. Unknowable \nOnly duplication will reveal these constants. Meet-over-all-paths will not find these constants. 3 Unknowable \nInstructions in this category either are not constant or cannot be identified as constant because of \nother limitations of the analyses. Our analyses do not track pointers, values stored in memory, or the \nresults of calls. Therefore, instructions that consume these values will never be found constant. We \nestimated this set by counting the number of values produced within a basic block, yet found equal to \n1. Figure 10 divides instructions (dynamically weighted) into these categories. Most instructions in \neach benchmark fall in the Unknowable or Local categories. Path qualifi-cation does not affect these \ncategories. The other part of Figure 10 focuses on the instructions targeted by constant propagation \nalgorithms. Our technique found many (2-122) times more knowable and nonlocally constant instructions. \nInterestingly, most instructions found constant by qualified analysis were neither Identical nor Variable. \nThese instructions had one constant value at one or more sites and were also unknown at one or more sites. \nThe exceptions are vortex and go, both of which contained a significant, but small, number of Variable \nconstants. Other techniques, which do not duplicate paths, will not find these constants. Although the \ndirect improvement from our technique is large, the instructions it finds constant still make up a small \npercentage of all dynamic instructions. This further explains why we did not see speedups for most of \nthe benchmarks. In the above discussion, we assumed that the MOP is not attainable for constant propagation. \nThis is true for the non-distributive Wegman-Zadek formulation. Recently, Bodfk and Anik published a \ndistributive formulation of con- stant propagation [BA98]. It would be interesting to com-pare path-qualified \nanalysis against this formulation. 6.3 Cost of Path Qualification This section examines the cost of \npath qualification. 6.3.1 Cost of Duplication Figure 11 shows that CFG size only increased significantly \nfor go and that the reduction algorithm successfully con-trolled the increase in CFG size. The cost of \ndata-flow analysis is proportional to the CFG s size before reduction. For go, the maximum increase was \n722%, and for the other programs the maximum increase was 80%. However, Figure 9 showed that 100% coverage \nof-fers little benefit. 97% coverage achieves almost all of the benefit, and limits CFG growth to 184% \nfor go and 32% for the other programs. The CFG s size after reduction is an indirect measure of the spatial \nlocality of the constants found. Our experiments show that this locality is high-with CR = 0.95, only \ngo grew by more than 10% at any level of coverage. go grew by 77% at full coverage, but, again, full \ncoverage is unnecessary: at CA = 0.97, its increase was 70%. The cost of subsequent Figure 13: A Venn \ndiagram classifying a program s dynamic instructions. analysis and the running time of the program may \ndegrade as the CFG grows, but these increases seem manageable. Why was go exceptional? Table 1 shows \nthat go exe-cuted many more paths than other programs and also re-quired more paths to reach high coverage \nlevels. Further experiments are necessary to see whether go s distribution is atypical or not. 6.3.2 \nAnalysis Time Path-qualified data-flow analysis increases analysis time, both by adding three new steps-building \nthe qualifica-tion automaton, tracing, and reduction-and by running the data-flow solver on larger graphs. \nFigure 12 shows the relative increase in SJdySiS time as cA iS increased. Once again, go was exceptional. \nFor the other benchmarks, the increase was less than 61% at almost full coverage. Figure 9 shows that \nmost of the benefit is gained before full cover-age, so these increases are reasonable. For go, analysis \ntime increased sixfold at CA = 0.97. The observed analysis time seems to grow a bit faster than linearly \nwith the size of the hot path graph. 7 Related Work Feasible path analysis attempts to identify and eliminate \nin-feasible paths. Holley and Rosen introduced qualified data-flow analysis to separate known infeasible \npaths from the remaining paths, some of which might be feasible m81]. Goldberg et al. applied theorem \nproving techniques to iden- tify infeasible paths in testing a program s path cover-age [GWZ94]. Bodik \net al. used a weaker (but less expen- sive) decision technique to determine if all paths between a definition \nand use were infeasible, and therefore the def-use pair actually did not exist [BGS97b]. Our work differs \nfrom these, as we focus on directly improving the precision of Benchmark Benchmark (a) Local and unknowable \n(b) Other categories Figure 10: Fraction of dynamic instructions that fall into categories in Figure \n13. The qualified analysis was done at full coverage (CA = 1). program analysis along a subset of important \npaths, rather than improving analysis everywhere by eliminating spurious paths. However, the two techniques \nare certainly comple-mentary, as our technique would work well in a CFG from which infeasible paths were \neliminated. Paths have long been used in program analysis and opti- mization. Fisher s trace scheduling \ntechnique heavily opti-mized the hot paths (called traces) in a CFG [Fis81]. I&#38;e scheduling did not \nduplicate paths, instead it introduced fixup code along control flow edges into or out of the mid-dle \nof a trace. More recently, Hwu et al. eliminated this fixup code by duplicating paths to form superblocks, \nwhich is a collection of traces without control flow into the middle of a trace [mWHMC+93]. Our approach \ndiffers from both techniques. First, it is a technique for improving program analysis, not a technique \nfor optimization and instruction scheduling. Second, although it duplicates paths, like su-perblocks, \nits duplication is guided by path profiles. Finally, both scheduling techniques attempted to maximize \nthe size of traces. This work evaluates the improvement from dupli-cation, and eliminates duplicated \nblocks that provide little or no improvement. Mueller and Whalley used an ad-hoc framework and code duplication \nto eliminate certain partially redundant branches [MW95]. Mueller and Whalley s code duplica-tion algorithm \ncan be seen as a qualification algorithm in which states in the qualification automaton encode in-formation \nabout the direction of the partially redundant branches. Bodfk et al. used a limited form of interproce- \ndural analysis to detect redundant branches along interpro- cedural paths [BGS97a]. This work differs \nby incorporating paths into a more precise and general framework, by us-ing paths to derive more precise \ndata-flow analyses, and by using path frequencies to overcome the costs of exploiting increased precision \n(code duplication). Bodfk et al. presented an algorithm for complete partial redundancy elimination using \nboth code motion and code duplication [BGS98]. Their technique also used profiles (ei-ther edge or path) \nto drive code duplication. Our paper is not directly comparable with their paper, as their paper used \nduplication to carry out an optimization while our pa-per uses duplication to improve analysis. However, \nthere is a difference in philosophy between the two papers. They first analyzed the original control \nflow graph to identify ver-tices for which duplication would enable better code motion. Using a profile, \ntheir algorithm decides which of these can-didates should be duplicated. Our work takes the other tack: \na profile guides an initial round of duplication. Anal-ysis of the duplicated flow graph, together with \nthe profile, identifies blocks that should not have been duplicated. By contrast, their approach starts \nand performs analysis over a smaller graph. Our approach, however, can find solutions not found by a \nmeet-over-all-paths analysis. Ftamalingam combined data-flow analysis with program frequency information \nby associating probabilities with dataflow values and developing a data-flow framework for combining \nthese pairs of values @am96]. Our goal differs. Instead of incorporating frequencies into the meet-over-all-paths \nframework, we use frequency information to improve analysis precision in heavily executed code. 8 Conclusion \nThis paper describes a new approach to analyzing and op- timizing programs. Our technique starts with \na path profile that identifies the hot paths that incur most of the program s cost. This information \nprovides the basis for a hot path graph, in which hot paths are isolated in order to compute data-flow \nvalues more precisely. After analysis, the hot path graph is reduced to eliminate unnecessary or unprofitable \npaths. We applied this technique to constant propagation Path coverage 0 (. . ,. , .. , 0.7 0.8 0.9 \nPath cover*ge 1.0 (a) Before reduction (go) (b) After minimization (go) -m88ksim --+-compress -&#38;8ksim \n-compress -ll -8 -+ kg -UP% -per1-perl -voflex -vorm 018 0:9 Path eovenge Path coverage (c) Before reduction \n(other benchmarks) (d) After minimization (other benchmarks) Figure 11: Increase in the number of CFG \nnodes before and after reduction versus the level of path coverage. The baseline is the unoptimized program. \nThe scale of the y-axis differs in each graph. The graph on the left (i.e., before reduction) is approximately \nan order of magnitude larger than the graph after reduction. Also, the scale for go is about an order \nof magnitude larger than the other graphs. and obtained significant improvement over the widely-used \nAcknowledgements Wegman-Zadek technique, without a large increase in pro- gram size. Thomas Reps spotted \na problem with an earlier formula- tion of our interpretation of path profiles of the hot path graph. \nTom Ball, Rajiv Gupta, Rastislav Bodfk, and the anonymous reviewers provided many helpful suggestions \nand comments on this work. This technique is applicable to other data-flow problems, References as well. \nAside from its simplicity, its primary advantage is that it improves the precision of an analysis (by \nexcluding [ABL97] Glenn Ammons, Thomas Ball, and James R. Laws. Exploiting hardware performance counters \nwith flow the effect of infeasible or infrequently executed paths) in the and context sensitive profiling. \nIn Proceedings of the heavily executed portions of a program, where the benefits SIGPLAN 97 Conference \non Programming Lan-are largest. guage Design and Implementation, June 1997. -m88ksim -compnss -KO -84-6 \n-We3 -F-1 -YOrteX 0 0.7 0.8 0.9 1.1 Path coverage (b) Other benchmarks Figure 12: Time required for \nqualified flow analysis versus path coverage (CA). The baseline is the time required at CA = 0. [Aho94] \n[BASS] Alfred V. Aho. Algorithms for finding patterns in strings. In J. van Leeuwen, editor, Handbook \nof Theoretical Computer Science, volume A, chapter 5, pages 255-300. MIT Press, 1994. Rastislav Bodlk \nand Sadun Anik. Path-sensitive value-flow analysis. In Proceedings of the SIGPLAN 98 Symposium on Principles \nof Programming Lan-guages (POPL), January 1998. [mWHMC+931 Wen mei W. Hwu, Scott A. Mahlke, William Y. \n Chen, Pohua P. Chang, Nancy J. Warter, Roger A. Bringmann, Roland G. Ouellette, Richard E. Hank, Tokuso \nKiyohara, Grant E. Haab, John G. Helm, and Daniel M. Lavery. The superblock: An effec-tive technique \nfor VLIW and superscalar compila-tion. The Journal of Supercomputing, 7(1-2):229-248, May 1993. [BGS97a] \nRastislav Bodik, Interprocedural Rajiv Gupta, and Mary Lou Soffa. conditional branch elimination. In \n[Ram961 G. Ramalingam. Proceedings of Data flow the SIGPLAN frequency analysis. 96 Conference In on Proceedings \nProgramming of the SIGPLAN Language Design 97 Conjerence and Implementa- on Programming Language tion, \npages 267-277, May Design 1996. and Implementa- [BGS97b] tion (PLDI), pages 146-158, June 1997. Rastislav \nBodik, Rajiv Gupta, and Mary Lou Soffa. Refining data flow information using infeasible paths. [WFW+] \nRobert Wilson, derson, P. Wilson, Robert S. French, Christopher S. Saman P. Amarasinghe, Jennifer M. \nAn-Steve W. K. Tjiang, Shih-Wei Liao, Chau- In Fifth ACM SIGSOFT Symposium on Founda- Wen Tseng, Mary \nW. Hall, Monica S. Lam, and tions of Software Engineering and Sixth European John L. Hennessy. An overview \nof the SUIF com- [BGS98] Software Engineering Conference, September 1997. Rastislav Bodik, Rajiv Gupta, \nand Mary Lou S&#38;a. Complete removal of redundant computations. In Proceedings of the SIGPLAN 98 Conference \non Programming Language Design and Implementa-tion (PLDI), June 1998. To appear. [WZ91] piler system. \nPublished on the World Wide Web at http://suif.stanford.edu/suif/suifl/suif-overview/suif.html. Mark \nN. Wegman and F. Kenneth Zadeck. Constant propagation with conditional branches. ACM !&#38;a~-actions \non Programming Languages and Systems, 13(2):181-210, April 1991. (BL96j T. Ball and J. R. Laws. Proceedings \nof MICRO Efficient path profiling. In 96, pages 46-57, December 1996. [FisSl] Joseph A. Fisher. Trace \nscheduling: global microcode compaction. IEEE A technique Transactions for on Computers, C-30(7):478-490, \nJuly 1981. [Gri73] David Gries. Acta Injonatica, Describing an algorithm 2:97-109, 1973. by hopcroft. \n[GWZ94] Allen Goldberg, T. C. Wang, and David Zimmer-man. Applications of feasible path analysis to pro-gram \ntesting. In International Symposium on Sojt-zuar.e Testing and Analysis. ACM SIGSOFT, August 1994. [HRSl] \nL. Howard Halley and Barry K. Rosen. Qualified flow problems. IEEE Transactions on Software data En- \ngineering, SE-7(1):60-78, January 1981. [MW98] Frank Mueller and David B. Whalley. conditional branches \nby code replication. Avoiding In Pro- ceedings of the SIGPLAN 95 Conference on Pro- gramming Language \nDesign and Implementation (PLDI), pages 56-66, June 1995.   \n\t\t\t", "proc_id": "277650", "abstract": "Data-flow analysis computes its solutions over the paths in a control-flow graph. These paths---whether feasible or infeasible, heavily or rarely executed---contribute equally to a solution. However, programs execute only a small fraction of their potential paths and, moreover, programs' execution time and cost is concentrated in a far smaller subset of <i>hot paths</i>.This paper describes a new approach to analyzing and optimizing programs, which improves the precision of data flow analysis along hot paths. Our technique identifies and duplicates hot paths, creating a <i>hot path graph</i> in which these paths are isolated. After flow analysis, the graph is reduced to eliminate unnecessary duplicates of unprofitable paths. In experiments on SPEC95 benchmarks, path qualification identified 2--112 times more non-local constants (weighted dynamically) than the Wegman-Zadek conditional constant algorithm, which translated into 1--7% more dynamic instructions with constant results.", "authors": [{"name": "Glenn Ammons", "author_profile_id": "81546156556", "affiliation": "Department of Computer Sciences, University of Wisconsin-Madison, 1210 West Dayton St., Madison, WI", "person_id": "PP39040589", "email_address": "", "orcid_id": ""}, {"name": "James R. Larus", "author_profile_id": "81100277326", "affiliation": "Department of Computer Sciences, University of Wisconsin-Madison, 1210 West Dayton St., Madison, WI", "person_id": "P132790", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/277650.277665", "year": "1998", "article_id": "277665", "conference": "PLDI", "title": "Improving data-flow analysis with path profiles", "url": "http://dl.acm.org/citation.cfm?id=277665"}