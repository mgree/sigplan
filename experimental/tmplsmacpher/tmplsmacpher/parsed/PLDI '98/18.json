{"article_publication_date": "05-01-1998", "fulltext": "\n The Implementation of the Cilk-5 Multithreaded Language Matte0 Frigo Charles E. Leiserson Keith H. \nRandall MIT Laboratory for Computer Science 545 Technology Square Cambridge, Massachusetts 02139 {athena,cel,randall}@lcs.mit.edu \nAbstract The fifth release of the multithreaded language Cilk uses a provably good work-stealing scheduling \nalgorithm similar to the first system, but the language has been completely re-designed and the runtime \nsystem completely reengineered. The efficiency of the new implementation was aided by a clear strategy \nthat arose from a theoretical analysis of the scheduling algorithm: concentrate on minimizing overheads \nthat contribute to the work, even at the expense of overheads that contribute to the critical path. Although \nit may seem counterintuitive to move overheads onto the critical path, this work-first principle has \nled to a portable Cilk-5 im-plementation in which the typical cost of spawning a parallel thread is only \nbetween 2 and 6 times the cost of a C function call on a variety of contemporary machines. Many Cilk \npro-grams run on one processor with virtually no degradation compared to equivalent C programs. This \npaper describes how the work-first principle was exploited in the design of Cilk-5 s compiler and its \nruntime system. In particular, we present Cilk-5 s novel two-clone compilation strategy and its Dijkstra-like \nmutual-exclusion protocol for implementing the ready deque in the work-stealing scheduler. Keywords Critical \npath, multithreading, parallel computing, program-ming language, runtime system, work. 1 Introduction \n Cilk is a multithreaded language for parallel programming that generalizes the semantics of C by introducing \nlinguistic constructs for parallel control. The original Cilk-1 release [3, 4, 181 featured a provably \nefficient, randomized, work-stealing scheduler [3, 51, but the language was clumsy, because parallelism \nwas exposed by hand using explicit continuation passing. The Cilk language implemented by This research \nwas supported in part by the Defense Advanced Research Projects Agency (DARPA) under Grant N00014-94-1-0985. \nComputing facilities were provided by the MIT Xolas Project, thanks to a generous equipment donation \nfrom Sun Microsystems. Permission tc make digital or hard copies of all or pan of thii wcrk for pereonsl \nor classroom US* is grantad without 1~ provided that copies are not made M distributed lot profit 01 \ncommercial advan- taps and that copies bear thie notiw and tha full citation on ti fit paw. To copyotherwise, \nto republish. tc poston l rwnn OT tc redistribute to lists, requires prior spadfic pwniwicn end/w a fu. \nSIGPLAN SE Montreal, Canada 8 1998 ACM 0-89791~987-4/98/0006...$5,00 our latest Cilk-5 release [8] still \nuses a theoretically efficient scheduler, but the language has been simplified considerably. It employs \ncall/return semantics for parallelism and features a linguistically simple inlet mechanism for nondeterminis- \ntic control. Cilk-5 is designed to run efficiently on contem- porary symmetric multiprocessors (SMP s), \nwhich feature hardware support for shared memory. We have coded many applications in Cilk, including \nthe *Socrates and Cilkchess chess-playing programs which have won prizes in interna-tional competitions. \n The philosophy behind Cilk development has been to make the Cilk language a true parallel extension \nof C, both semantically and with respect to performance. On a paral- lel computer, Cilk control constructs \nallow the program to execute in parallel. If the Cilk keywords for parallel control are elided from a \nCilk program, however, a syntactically and semantically correct C program results, which we call the \nC elision (or more generally, the serial elision) of the Cilk program. Cilk is a faithful extension of \nC, because the C elision of a Cilk program is a correct implementation of the semantics of the program. \nMoreover, on one processor, a parallel Cilk program scales down to run nearly as fast as its C elision. \nUnlike in Cilk-1, where the Cilk scheduler was an identifi- able piece of code, in Cilk-5 both the compiler \nand runtime system bear the responsibility for scheduling. To obtain ef-ficiency, we have, of course, \nattempted to reduce scheduling overheads. Some overheads have a larger impact on execu-tion time than \nothers, however. A theoretical understanding of Cilk s scheduling algorithm [3, 51 has allowed us to \niden- tify and optimize the common cases. According to this ab-stract theory, the performance of a Cilk \ncomputation can be characterized by two quantities: its work, which is the to-tal time needed to execute \nthe computation serially, and its critical-path length, which is its execution time on an in- finite \nnumber of processors. (Cilk provides instrumentation that allows a user to measure these two quantities.) \nWithin Cilk s scheduler, we can identify a given cost as contribut- ing to either work overhead or critical-path \noverhead. Much of the efficiency of Cilk derives from the following principle, which we shall justify \nin Section 3. The work-first principle: Minimize the schedul- ing overhead borne by the world of a computation. \nSpecifically, move overheads out of the work and onto the critical path. The work-first principle played \nan important role during the design of earlier Cilk systems, but Cilk-5 exploits the prin-ciple more \nextensively. The work-first principle inspired a two-clone strategy for compiling Cilk programs. Our \ncilk2c compiler [23] is a type-checking, source-to-source translator that transforms a Cilk source into \na C postsource which makes calls to Cilk s runtime library. The C postsource is then run through the \ngee compiler to produce object code. The cilk2c compiler produces two clones of every Cilk procedure-a \nfast clone and a slow clone. The fast clone, which is identical in most respects to the C elision of \nthe Cilk program, executes in the common case where serial semantics suffice. The slow clone is executed \nin the infrequent case that parallel seman-tics and its concomitant bookkeeping are required. All com-munication \ndue to scheduling occurs in the slow clone and contributes to critical-path overhead, but not to work \nover-head. The work-first principle also inspired a Dijkstra-like [ll], shared-memory, mutual-exclusion \nprotocol as part of the runtime load-balancing scheduler. Cilk s scheduler uses a work-stealing algorithm \nin which idle processors, called thieves, steal threads from busy processors, called vic-tims. Cilk s \nscheduler guarantees that the cost of steal-ing contributes only to critical-path overhead, and not to \nwork overhead. Nevertheless, it is hard to avoid the mutual- exclusion costs incurred by a potential \nvictim, which con-tribute to work. To minimize work overhead, instead of using locking, Cilk s runtime \nsystem uses a Dijkstra-like protocol, which we call the THE protocol, to manage the runtime deque of \nready threads in the work-stealing algorithm. An added advantage of the THE protocol is that it allows \nan exception to be signaled to a working processor with no ad- ditional work overhead, a feature used \nin Cilk s abort mech-anism. The remainder of this paper is organized as follows. Sec-tion 2 overviews \nthe basic features of the Cilk language. Sec-tion 3 justifies the work-first principle. Section 4 describes \nhow the two-clone strategy is implemented, and Section 5 presents the THE protocol. Section 6 gives empirical \nevi-dence that the Cilk-5 scheduler is efficient. Finally, Section 7 presents related work and offers \nsome conclusions. 2 The Cilk language This section presents a brief overview of the Cilk extensions to \nC as supported by Cilk-5. (For a complete description, consult the Cilk-5 manual [8].) The key features \nof the lan- guage are the specification of parallelism and synchroniza-tion, through the spawn and sync \nkeywords, and the speci-fication of nondeterminism, using inlet and abort. #include tstdlib.h> #include \n<stdio.h> #include <cilk.h> cilk int fib (int n) c if (n(2) return n; else c int x, y; x = spawn fib \n(n-1); y = spawn fib (n-2); sync; return (x+y) ; lt 1 cilk int main (int argc, char *argv[l) { int n. \nresult; n = atoi(argvC11); result. = spawn fib(n); sync ; printf ( Result: %d\\n , result) ; return 0: \n Figure 1: A simple Cilk program to compute the nth Fibonacci number in parallel (using a very bad algorithm). \nThe basic Cilk language can be understood from an exam- ple. Figure 1 shows a Cilk program that computes \nthe nth Fibonacci number. Observe that the program would be an ordinary C program if the three keywords \ncilk, spawn, and sync are elided. The keyword cilk identifies fib as a Cilk procedure, which is the parallel \nanalog to a C function. Parallelism is created when the keyword spawn precedes the invocation of a procedure. \nThe semantics of a spawn differs from a C function call only in that the parent can continue to ex-ecute \nin parallel with the child, instead of waiting for the child to complete as is done in C. Cilk s scheduler \ntakes the responsibility of scheduling the spawned procedures on the processors of the parallel computer. \nA Cilk procedure cannot safely use the values returned by its children until it executes a sync statement. \nThe sync statement is a local barrier, not a global one as, for ex-ample, is used in message-passing \nprogramming. In the Fi-bonacci example, a sync statement is required before the statement return (x+y> \nto avoid the anomaly that would occur if x and y are summed before they are computed. In addition to \nexplicit synchronization provided by the sync statement, every Cilk procedure syncs implicitly before \nit returns, thus ensuring that all of its children terminate be-fore it does. Ordinarily, when a spawned \nprocedure returns, the re-turned value is simply stored into a variable in its parent s frame: This program \nUSES an inefficient algoritlnn which runs in exponen- tial time. Although logarithmic-time methods are \nknown [9, p. 8501, this program nevertheless provides a good didactic example. cilk int fib (int n) c \nint x = 0; inlet void summsr (int result) x += result; return; 1 if (nt2) return n; else c summer(spawn \nfib (n-l)); summsr(spawn fib (n-2)); sync; return (x); lt > Figure 2: Using an inlet to compute the \nnth Fibonnaci number. x = spawn foo (y) ; Occasionally, one would like to incorporate the returned value \ninto the parent s frame in a more complex way. Cilk provides an inlet feature for this purpose, which \nwas in-spired in part by the inlet feature of TAM [lo]. An inlet is essentially a C function internal \nto a Cilk pro-cedure. In the normal syntax of Cilk, the spawning of a procedure must occur as a separate \nstatement and not in an expression. An exception is made to this rule if the spawn is performed as an \nargument to an inlet call. In this case, the procedure is spawned, and when it returns, the inlet is \ninvoked. In the meantime, control of the parent procedure proceeds to the statement following the inlet \ncall. In princi- ple, inlets can take multiple spawned arguments, but Cilk-5 has the restriction that \nexactly one argument to an inlet may be spawned and that this argument must be the first argument. If \nnecessary, this restriction is easy to program around. Figure 2 illustrates how the f ib0 function might \nbe coded using inlets. The inlet summer 0 is defined to take a returned value result and add it to the \nvariable x in the frame of the procedure that does the spawning. All the variables of f ib() are available \nwithin summer 0, since it is an internal function of fib().2 No lock is required around the accesses \nto x by summer, because Cilk provides atomicity implicitly. The concern is that the two updates might \noccur in parallel, and if atomic- ity is not imposed, an update might be lost. Cilk provides implicit \natomicity among the threads of a procedure in-stance, where a thread is a maximal sequence of instruc- \ntions ending with a spawn, sync, or return (either explicit or implicit) statement. An inlet is precluded \nfrom containing spawn and sync statements, and thus it operates atomically as a single thread. Implicit \natomicity simplifies reasoning aThe C elision of a Cilk progranl with inlets is not ANSI C, because ANSI \nC does not support internal C functions. Cilk is based on Gnu C technology, however, which does provide \ntbis support. about concurrency and nondeterminism without requiring locking, declaration of critical \nregions, and the like. Cilk provides syntactic sugar to produce certain commonly used inlets implicitly. \nFor example, the statement x += spawn fib(n-1) conceptually generates an inlet similar to the one in \nFigure 2. Sometimes, a procedure spawns off parallel work which it later discovers is unnecessary. This \nspeculative work can be aborted in Cilk using the abort primitive inside an in-let. A common use of abort \noccurs during a parallel search, where many possibilities are searched in parallel. As soon as a solution \nis found by one of the searches, one wishes to abort any currently executing searches as soon as possible \nso as not to waste processor resources. The abort statement, when executed inside an inlet, causes all \nof the already-spawned children of the procedure to terminate. We considered using futures [19] with \nimplicit synchro-nization, as well as synchronizing on specific variables, in-stead of using the simple \nspawn and sync statements. We realized from the work-first principle, however, that differ-ent synchronization \nmechanisms could have an impact only on the critical-path of a computation, and so this issue was of \nsecondary concern. Consequently, we opted for imple-mentation simplicity. Also, in systems that support \nre-laxed memory-consistency models, the explicit sync state-ment can be used to ensure that all side-effects \nfrom previ-ously spawned subprocedures have occurred. In addition to the control synchronization provided \nby sync, Cilk programmers can use explicit locking to syn-chronize accesses to data, providing mutual \nexclusion and atomicity. Data synchronization is an overhead borne on the work, however, and although \nwe have striven to min-imize these overheads, fine-grain locking on contemporary processors is expensive. \nWe are currently investigating how to incorporate atomicity into the Cilk language so that pro-tocol \nissues involved in locking can be avoided at the user level. To aid in the debugging of Cilk programs \nthat use locks, we have been developing a tool called the Nonde-terminator [7, 131, which detects common \nsynchronization bugs called data races. 3 The work-first principle This section justifies the work-first \nprinciple stated in Sec- tion 1 by showing that it follows from three assumptions. First, we assume that \nCilk s scheduler operates in practice according to the theoretical analysis presented in [3, 51. Sec- \nond, we assume that in the common case, ample parallel slackness [28] exists, that is, the average parallelism \nof a Cilk program exceeds the number of processors on which we run it by a sufficient margin. Third, \nwe assume (as is indeed the case) that every Cilk program has a C elision against which its one-processor \nperformance can be measured. The theoretical analysis presented in [3, 51 cites two funda- mental lower \nbounds as to how fast a Cilk program can run. Let us denote by Tp the execution time of a given computa-tion \non P processors. Then, the work of the computation is Tl and its critical-path length is Tm. For a computation \nwith Tl work, the lower bound Tp > Tl/P must hold, because at most P units of work can be executed in \na single step. In addition, the lower bound Tp > Too must hold, since a finite number of processors cannot \nexecute faster than an infinite number. Cilk s randomized work-stealing scheduler [3, 51 executes a Cilk \ncomputation on P processors in expected time TP = TI/P + O(T,) , (1) assuming an ideal parallel computer. \nThis equation resem-bles Brent s theorem [6, 151 and is optimal to within a constant factor, since Tl/P \nand T, are both lower bounds. We call the first term on the right-hand side of Equation (1) the world \nterm and the second term the critical-path term. Importantly, all communication costs due to Cilk s scheduler \nare borne by the critical-path term, as are most of the other scheduling costs. To make these overheads \nexplicit, we de-fine the critical-path overhead to be the smallest constant co3 such that TP <TI/P+c,T, \n. (2) The second assumption needed to justify the work-first principle focuses on the common-case regime \nin which a parallel program operates. Define the average parallelism as Tis = Tl/T,, which corresponds \nto the maximum pos-sible speedup that the application can obtain. Define also the parallel slackness \n[28] to be the ratio F/P. The as-sumption of parallel slackness is that PIP >> coo, which means that \nthe number P of processors is much smaller than the average parallelism F. Under this assumption, it \nfollows that TIIP >> cooToo, and hence from Inequality (2) that TP z Tl/P, and we obtain linear speedup. \nThe critical-path overhead cm has little effect on performance when suffi- cient slackness exists, although \nit does determines how much slackness must exist to ensure linear speedup. Whether substantial slackness \nexists in common applica-tions is a matter of opinion and empiricism, but we suggest that slackness is \nthe common case. The expressiveness of Cilk makes it easy to code applications with large amounts of \nparallelism. For modest-sized problems, many applica-tions exhibit an average parallelism of over 200, \nyielding sub-stantial slackness on contemporary SMP s. Even on Sandia National Laboratory s Intel Paragon, \nwhich contains 1824 nodes, the *Socrates chess program (coded in Cilk-1) ran in its linear-speedup regime \nduring the 1995 ICCA World Computer Chess Championship (where it placed second in a field of 24). Section \n6 describes a dozen other diverse applications which were run on an b-processor SMP with This abstract \nmodel of execution time @ores real-life details, such as memory-hierarchy effects, but is nonetheless \nquite accurate [a]. considerable parallel slackness. The parallelisim of these ap- plications increases \nwith problem size, thereby ensuring they will run well on large machines. The third assumption behind \nthe work-first principle is that every Cilk program has a C elision against which its one-processor performance \ncan be measured. Let us denote by TS the running time of the C elision. Then, we defme the work overhead \nby cl = Tl/Ts. Incorporating critical-path and work overheads into Inequality (2) yields TP 5 cJ s/P \n+ c,Too (3) = CITSIP , since we assume parallel slackness. We can now restate the work-first principle \nprecisely. h&#38;n-imize cl, even at the expense of a larger c,, because cl has a more direct impact \non performance. Adopting the work-first principle may adversely affect the ability of an application \nto scale up, however, if the critical-path overhead coo is too large. But, as we shall see in Section \n6, critical-path over-head is reasonably small in Cilkd, and many applications can be coded with large \namounts of parallelism. The work-first principle pervades the Cilk-5 implementa-tion. The work-stealing \nscheduler guarantees that with high probability, only O(PT,) steal (migration) attempts occur (that is, \nO(T,) on average per processor), all costs for which are borne on the critical path. Consequently, the \nscheduler for Cilkd postpones as much of the scheduling cost as pos- sible to when work is being stolen, \nthereby removing it as a contributor to work overhead. This strategy of amortiz- ing costs against steal \nattempts permeates virtually every decision made in the design of the scheduler. 4 Cilk s compilation \nstrategy This section describes how our cilk2c compiler generates C postsource from a Cilk program. As \ndictated by the work-first principle, our compiler and scheduler are designed to reduce the work overhead \nas much as possible. Our strategy is to generate two clones of each procedure--a fast clone and a slow \nclone. The fast clone operates much as does the C elision and has little support for parallelism. The \nslow clone has full support for parallelism, along with its concomitant overhead. We first describe the \nCilk scheduling algorithm. Then, we describe how the compiler translates the Cilk lan-guage constructs \ninto code for the fast and slow clones of each procedure. Lastly, we describe how the runtime sys-tem \nlinks together the actions of the fast and slow clones to produce a complete Cilk implementation. As \nin lazy task creation [24], in Cilk-5 each proces-sor, called a worker, maintains a ready deque (doubly-ended \nqueue) of ready procedures (technically, procedure instances). Each deque has two ends, a head and a \ntail, from which procedures can be added or removed. A worker operates locally on the tail of its own \ndeque, treating it much 1 int fib (int n) 2 3 { fib-frame *f; frame pointer 4 f = alloc(sizeof(*f)); \nallocate frame 5 f->sig = fib-sig; initialixe frame 6 if (n<2) I 7 free(f, sizeof(* free frame 8 return \nn; 9 > 10 else { 11 int x. y; 12 f->entry = 1; save PC 13 f->n = n; save live vars 14 *T = f; store fmme \npointer 15 push0 ; push frame 16 x = fib h-1); do C call 17 if (pop(x) == FAILURE) POP frame 18 return \n0; frame stolen 19 . . . second spawn 20 sync is free! 21 k(f, sizeof(* free frame 22 return (x+y); 23 \n) 24 1 Figure 3: The fast clone generated by cilk2c for the fib proce-dure from Figure 1. The code for \nthe second spawn is omitted. The functions allot and free are inlined calls to the runtime system s fast \nmemory allocator. The signature f ib-sig contains a description of the fib procedure, including a pointer \nto the slow clone. The push and pop calls are operations on the scheduling deque and are described in \ndetail in Section 5. as C treats its call stack, pushing and popping spawned acti-vation frames. When \na worker runs out of work, it becomes a thief and attempts to steal a procedure another worker, called \nits victim. The thief steals the procedure from the head of the victim s deque, the opposite end from \nwhich the victim is working. When a procedure is spawned, the fast clone runs. When-ever a thief steals \na procedure, however, the procedure is converted to a slow clone. The Cilk scheduler guarantees that \nthe number of steals is small when sufficient slackness exists, and so we expect the fast clones to be \nexecuted most of the time. Thus, the work-first principle reduces to mini-mizing costs in the fast clone, \nwhich contribute more heavily to work overhead. Minimizing costs in the slow clone, al-though a desirable \ngoal, is less important, since these costs contribute less heavily to work overhead and more to critical- \npath overhead. We minimize the costs of the fast clone by exploiting the structure of the Cilk scheduler. \nBecause we convert a pro- cedure to its slow clone when it is stolen, we maintain the invariant that \na fast clone has never been stolen. Further-more, none of the descendants of a fast clone have been stolen \neither, since the strategy of stealing from the heads of ready deques guarantees that parents are stolen \nbefore their children. As we shall see, this simple fact allows many optimizations to be performed in \nthe fast clone. We now describe how our cilk2c compiler generates post-source C code for the fib procedure \nfrom Figure 1. An ex-ample of the postsource for the fast clone of fib is given in Figure 3. The generated \nC code has the same general structure as the C elision, with a few additional statements. In lines 4-5, \nan activation fMcme is allocated for fib and initialized. The Cilk runtime system uses activation frames \nto represent procedure instances. Using techniques similar to [16, 171, our inlined allocator typically \ntakes only a few cycles. The frame is initialized in line 5 by storing a pointer to a static structure, \ncalled a signature, describing fib. The first spawn in fib is translated into lines 12-18. In lines 12-13, \nthe state of the fib procedure is saved into the activation frame. The saved state includes the program \ncounter, encoded as an entry number, and all live, dirty vari-ables. Then, the frame is pushed on the \nruntime deque in lines 14-15.4 Next, we call the fib routine as we would in C. Because the spawn statement \nitself compiles directly to its C elision, the postsource can exploit the optimization capabilities of \nthe C compiler, including its ability to pass arguments and receive return values in registers rather \nthan in memory. After fib returns, lines 17-18 check to see whether the parent procedure has been stolen. \nIf it has, we return im-mediately with a dummy value. Since all of the ancestors have been stolen as \nwell, the C stack quickly unwinds and control is returned to the runtime system.6 The protocol to check \nwhether the parent procedure has been stolen is quite subtle-we postpone discussion of its implementation \nto Section 5. If the parent procedure has not been stolen, it continues to execute at line 19, performing \nthe second spawn, which is not shown. In the fast clone, all sync statements compile to no-ops. Because \na fast clone never has any children when it is exe- cuting, we know at compile time that all previously \nspawned procedures have completed. Thus, no operations are re-quired for a sync statement, as it always \nsucceeds. For exarn- ple, line 20 in Figure 3, the translation of the sync statement is just the empty \nstatement. Finally, in lines 21-22, fib deal-locates the activation frame and returns the computed result \nto its parent procedure. The slow clone is similar to the fast clone except that it provides support \nfor parallel execution. When a proce- dure is stolen, control has been suspended between two of the procedure \ns threads, that is, at a spawn or sync point. When the slow clone is resumed, it uses a goto statement \nto restore the program counter, and then it restores local variable state from the activation frame. \nA spawn statement is translated in the slow clone just as in the fast clone. For a sync statement, cilk2c \ninserts a call to the runtime system, which checks to see whether the procedure has any spawned children \nthat have not returned. Although the parallel book- 41f the shared memory is not sequentially consistent, \na memory fence must be inserted between lines 14 and 15 to ensure that the surrounding writes are executed \nin the proper order. The setjmp/longjmp facility of C could have been used as well, but our unwinding \nstrategy is simpler. keeping in a slow clone is substantial, it contributes little to work overhead, \nsince slow clones are rarely executed. The separation between fast clones and slow clones also allows \nUS to compile inlets and abort statements efficiently in the fast clone. An inlet call compiles as efficiently \nas an ordinary spawn. For example, the code for the inlet call from Figure 2 compiles similarly to the \nfollowing Cilk code: tmp = spawn fibh-1); summer(tmp) ;  Implicit inlet calls, such as x += spawn f \nib(n-l), compile directly to their C elisions. An abort statement compiles to a no-op just as a sync \nstatement does, because while it is executing, a fast clone has no children to abort. The runtime system \nprovides the glue between the fast and slow clones that makes the whole system work. It includes protocols \nfor stealing procedures, returning values between processors, executing inlets, aborting computation \nsubtrees, and the like. All of the costs of these protocols can be amor- tized against the critical path, \nso their overhead does not significantly affect the running time when sufficient parallel slackness exists. \nThe portion of the stealing protocol exe-cuted by the worker contributes to work overhead, however, thereby \nwarranting a careful implementation. We discuss this protocol in detail in Section 5. The work overhead \nof a spawn in Cilk-5 is only a few reads and writes in the fast clone-3 reads and 5 writes for the fib \nexample. We will experimentally quantify the work overhead in Section 6. Some work overheads still remain \nin our im-plementation, however, including the allocation and freeing of activation frames, saving state \nbefore a spawn, pushing and popping of the frame on the deque, and checking if a procedure has been stolen. \nA portion of this work overhead is due to the fact that Cilk-5 is duplicating the work the C compiler \nperforms, but as Section 6 shows, this overhead is small. Although a production Cilk compiler might be \nable eliminate this unnecessary work, it would likely compromise portability. In Cilk-4, the precursor \nto Cilk-5, we took the work-first principle to the extreme. Cilk-4 performed stack-based al-location \nof activation frames, since the work overhead of stack allocation is smaller than the overhead of heap \nahoca-tion. Because of the cactus stack 1251 semantics of the Cilk stack,6 however, Cilk-4 had to manage \nthe virtual-memory map on each processor explicitly, as was done in [27]. The work overhead in Cilk-4 \nfor frame allocation was little more than that of incrementing the stack pointer, but whenever the stack \npointer overflowed a page, an expensive user-level interrupt ensued, during which Cilk-4 would modify \nthe memory map. Unfortunately, the operating-system mech-anisms supporting these operations were too \nslow and un-predictable, and the possibility of a page fault in critical sec- 6Suppose a procedure A \nspawns two children B and C. The two children can reference objects in A s activation frame, but B and \nC do not see each other s frame. tions led to complicated protocols. Even though these over-heads could \nbe charged to the critical-path term, in practice, they became so large that the critical-path term contributed \nsignificantly to the running time, thereby violating the as-sumption of parallel slackness. A one-processor \nexecution of a program was indeed fast, but insufficient slackness some-times resulted in poor parallel \nperformance. In Cilk-5, we simplified the allocation of activation frames by simply using a heap. In \nthe common case, a frame is allocated by removing it from a free list. Deallocation is performed by inserting \nthe frame into the free list. No user-level management of virtual memory is required, except for the \ninitial setup of shared memory. Heap allocation con-tributes only slightly more than stack allocation \nto the work overhead, but it saves substantially on the critical path term. On the downside, heap allocation \ncan potentially waste more memory than stack allocation due to fragmentation. For a careful analysis \nof the relative merits of stack and heap based allocation that supports heap allocation, see the paper \nby Appel and Shao [l]. For an equally careful analysis that supports stack allocation, see [22]. Thus, \nalthough the work-first principle gives a general un-derstanding of where overheads should be borne, \nour expe-rience with Cilk-4 showed that large enough critical-path overheads can tip the scales to the \npoint where the assump- tions underlying the principle no longer hold. We believe that Cilk-5 work overhead \nis nearly as low as possible, given our goal of generating portable C output from our compiler. Other \nresearchers have been able to reduce overheads even more, however, at the expense of portability. For \nexample, lazy threads (141 obtains efficiency at the expense of im-plementing its own calling conventions, \nstack layouts, etc. Although we could in principle incorporate such machine-dependent techniques into \nour compiler, we feel that Cilk-5 strikes a good balance between performance and portability. We also \nfeel that the current overheads are sufficiently low that other problems, notably minimizing overheads \nfor data synchronization, deserve more attention. 5 implemention of work-stealing In this section, we \ndescribe Cilk-5 s work-stealing mecha-nism, which is based on a Dijkstra-like [ll], shared-memory, mutual-exclusion \nprotocol called the THE protocol. In accordance with the work-first principle, this protocol has been \ndesigned to minimize work overhead. For example, on a 167-megahertz UltraSPARC I, the fib program with \nthe THE protocol runs about 25% faster than with hardware locking primitives. We first present a simplified \nversion of the protocol. Then, we discuss the actual implementation, which allows exceptions to be signaled \nwith no additional overhead. Although the runtime system requires some effort to port between architectures, \nthe compiler requires oo changes whatsoever for differ-ent platform. Several straightforward mechanisms \nmight be considered to implement a work-stealing protocol. For example, a thief might interrupt a worker \nand demand attention from this victim. This strategy presents problems for two reasons. First, the mechanisms \nfor signaling interrupts are slow, and although an interrupt would be borne on the critical path, its \nlarge cost could threaten the assumption of parallel slack-ness. Second, the worker would necessarily \nincur some over- head on the work term to ensure that it could be safely interrupted in a critical section. \nAs an alternative to send-ing interrupts, thieves could post steal requests, and workers could periodically \npoll for them. Once again, however, a cost accrues to the work overhead, this time for polling. Tech-niques \nare known that can limit the overhead of polling [12], but they require the support of a sophisticated \ncompiler. The work-first principle suggests that it is reasonable to put substantial effort into minimizing \nwork overhead in the work-stealing protocol. Since Cilk-5 is designed for shared- memory machines, we \nchose to implement work-stealing through shared-memory, rather than with message-passing, as might otherwise \nbe appropriate for a distributed-memory implementation. In our implementation, both victim and thief \noperate directly through shared memory on the victim s ready deque. The crucial issue is how to resolve \nthe race con- dition that arises when a thief tries to steal the same frame that its victim is attempting \nto pop. One simple solution is to add a lock to the deque using relatively heavyweight hardware primitives \nlike Compare-And-Swap or Test-And-Set. Whenever a thief or worker wishes to remove a frame from the deque, \nit first grabs the lock. This solution has the same fundamental problem as the interrupt and polling \nmechanisms just described, however. Whenever a worker pops a frame, it pays the heavy price to grab a \nlock, which contributes to work overhead. Consequently, we adopted a solution that employs Di-jkstra \ns protocol for mutual exclusion [ll], which assumes only that reads and writes are atomic. Because our \nproto-col uses three atomic shared variables T, H, and E, we call it the THE protocol. The key idea is \nthat actions by the worker on the tail of the queue contribute to work overhead, while actions by thieves \non the head of the queue contribute only to critical-path overhead. Therefore, in accordance with the \nwork-first principle, we attempt to move costs from the worker to the thief. To arbitrate among different \nthieves attempting to steal from the same victim, we use a hard- ware lock, since this overhead can be \namortized against the critical path. To resolve conflicts between a worker and the sole thief holding \nthe lock, however, we use a lightweight Dijkstra-like protocol which contributes minimally to work overhead. \nA worker resorts to a heavyweight hardware lock only when it encounters an actual conflict with a thief, \nin which case we can charge the overhead that the victim incurs to the critical path. In the rest of \nthis section, we describe the THE protocol Worker/Victim Thief 1 push0 I 1 steal0 C 2 T++; 2 lock(L) \n; 3 1 3 li++; 4 pop0 I 4 5 if (H > T) H-m. { 5 T--; 6 u&#38;k(L) ; 6 if (ii > T) I 7 return FAILURE; \n7 T++ ; 8 > 8 lock(L) ; 9 unlock(L) ; 9 T--; 10 return SUCCESS; 10 if (ii > T) i 11 1 11 T++; 12 unlock(L) \n; 13 return FAILURE; 14 > 15 unlock(L) ; 16 It 17 return SUCCESS; 18 1 Figure 4: Pseudocode of a simplified \nversion of the THE protocol. The left part of the figure shows the actions performed by the victim, and \nthe right part shows the actions of the thief. None of the actions besides reads and writes are assumed \nto be atomic. For example, T--; can be implemented as tmp = T; tmp = trap -I; T = trap;. in detail. We \nfirst present a simplified protocol that uses only two shared variables T and H designating the tail \nand the head of the deque, respectively. Later, we extend the protocol with a third variable E that allows \nexceptions to be signaled to a worker. The exception mechanism is used to implement Cilk s abort statement. \nInterestingly, this exten-sion does not introduce any additional work overhead. The pseudocode of the \nsimplified THE protocol is shown in Figure 4. Assume that shared memory is sequentially consistent [2O].s \nThe code assumes that the ready deque is implemented as an array of frames. The head and tail of the \ndeque are determined by two indices T and II, which axe stored in shared memory and are visible to all \nprocessors. The index T points to the first unused element in the array, and H points to the first frame \non the deque. Indices grow from the head towards the tail so that under normal con-ditions, we have T \n1 H. Moreover, each deque has a lock L implemented with atomic hardware primitives or with OS calls. \nThe worker uses the deque as a stack. (See Section 4.) Before a spawn, it pushes a frame onto the tail \nof the deque. After a spawn, it pops the frame, unless the frame has been stolen. A thief attempts to \nsteal the frame at the head of the deque. Only one thief at the time may steal from the deque, since \na thief grabs L as its first action. As can be seen from the code, the worker alters T but not H, whereas \nthe thief only increments H and does not alter T. The only possible interaction between a thief and its \nvic- sIf the shared memory is not sequentially consistent, a memory fence must be inserted between lines \n6 and 6 of the worker/victim code and between lines 3 and 4 of the thief code to ensure that these instructions \nare executed in the proper order. Thief H=T Victim (a) (b) Cc) Figure 5: The three cases of the ready \ndeque in the simplified THE protocol. A shaded entry indicates the presence of a frame at a certain position \nin the deque. The head and the tail are marked by T and H. tim occurs when the thief is incrementing \nH while the vic-tim is decrementing T. Consequently, it is always safe for a worker to append a new frame \nat the end of the deque (push) without worrying about the actions of the thief. For a pop operations, \nthere are three cases, which are shown in Figure 5. In case (a), the thief and the victim can both get \na frame from the deque. In case (b), the deque contains only one frame. If the victim decrements T without \ninterference from thieves, it gets the frame. Similarly, a thief can steal the frame as long as its victim \nis not trying to obtain it. If both the thief and the victim try to grab the frame, however, the protocol \nguarantees that at least one of them discovers that H > T. If the thief discovers that H > T, it restores \nH to its original value and retreats. If the victim discovers that H > T, it restores T to its original \nvalue and restarts the protocol after having acquired L. With L acquired, no thief can steal from this \ndeque so the victim can pop the frame without interference (if the frame is still there). Finally, in \ncase (c) the deque is empty. If a thief tries to steal, it will always fail. If the victim tries to pop, \nthe attempt fails and control returns to the Cilk runtime system. The protocol cannot deadlock, because \neach process holds only one lock at a time. We now argue that the THE protocol contributes little to \nthe work overhead. Pushing a frame involves no overhead beyond updating T. In the common case where a \nworker can succesfully pop a frame, the pop protocol performs only 6 operations-2 memory loads, 1 memory \nstore, 1 decre- ment, 1 comparison, and 1 (predictable) conditional branch. Moreover, in the common case \nwhere no thief operates on the deque, both H and T can be cached exclusively by the worker. The expensive \noperation of a worker grabbing the lock L occurs only when a thief is simultaneously trying to steal \nthe frame being popped. Since the number of steal attempts depends on T,, not on TI, the relatively heavy \ncost of a victim grabbing L can be considered as part of the critical-path overhead coo and does not \ninfluence the work overhead cl. We ran some experiments to determine the relative per-formance of the \nTHE protocol versus the straightforward protocol in which pop just locks the deque before accessing it. \nOn a 167-megahertz UltraSPARC I, the THE protocol is about 25yo faster than the simple locking protocol. \nThis machine s memory model requires that a memory fence in- struction (membar) be inserted between lines \n5 and 6 of the pop pseudocode. We tried to quantify the performance im-pact of the membar instruction, \nbut in all our experiments the execution times of the code with and without membar are about the same. \nOn a 200-megahertz Pentium Pro run-ning Linux and gee 2.7.1, the THE protocol is only about 5% faster \nthan the locking protocol. On this processor, the THE protocol spends about half of its time in the memory \nfence. Because it replaces locks with memory synchronization, the THE protocol is more nonblocking than \na straightfor- ward locking protocol. Consequently, the THE protocol is less prone to problems that arise \nwhen spin locks are used extensively. For example, even if a worker is suspended by the operating system \nduring the execution of pop, the infrequency of locking in the THE protocol means that a thief can usually \ncomplete a steal operation on the worker s deque. Recent work by Arora et al. [2] has shown that a completely \nnonblocking work-stealing scheduler can be im- plemented. Using these ideas, Lisiecki and Medina [21] \nhave modified the Cilk-5 scheduler to make it completely non-blocking. Their experience is that the THE \nprotocol greatly simplifies a nonblocking implementation. The simplified THE protocol can be extended \nto support the signaling of exceptions to a worker. In Figure 4, the index H plays two roles: it marks \nthe head of the deque, and it marks the point that the worker cannot cross when it pops. These places \nin the deque need not be the same. In the full THE protocol, we separate the two functions of H into \ntwo variables: H, which now only marks the head of the deque, and E, which marks the point that the victim \ncannot cross. Whenever E > T, some exceptional condition has occurred, which includes the frame being \nstolen, but it can also be used for other exceptions. For example, setting E = 00 causes the worker to \ndiscover the exception at its next pop. In the new protocol, E replaces H in line 6 of the worker/victim. \nMoreover, lines 7-15 of the worker/victim are replaced by a call to an exception handler to determine \nthe type of exception (stolen frame or otherwise) and the proper action to perform. The thief code is \nalso modified. Before trying to Program fib blockedmul notempmul atraasen *cilkaort tqueens tknapaack \n1U *&#38;&#38;sky heat fft Barnes-But Sile 35 1024 1024 1024 4,100,000 22 30 2048 BCSSTK32 4096 x 512 \n220 2 O Tl 12.77 29.9 29.7 20.2 5.4 150. 75.8 155.8 1427. 62.3 4.3 124. TC.3 0.0005 0.0044 0.015 0.58 \n0.0049 0.0015 0.0014 0.42 3.4 0.16 0.0020 0.15 P 25540 6730 1970 35 1108 96898 54143 370 420 384 2145 \n853 c1 3.63 1.05 1.05 1.01 1.21 0.99 1.03 1.02 1.25 1.08 0.93 1.02 Ts 1.60 4.3 3.9 3.54 0.90 18.8 9.5 \n20.3 208. 9.4 0.77 16.5 3-g 7:o 7.6 5.7 6.0 8.0 8.0 7.7 6.9 6.6 5.6 7.5 Ts/Ts 2.2 6.6 7.2 5.6 5.0 8.0 \n7.7 7.5 5.5 6.1 6.0 7.4 Figure 6: The performance of example Cilk programs. Times are in seconds and \nare accurate to within about 10%. The serial programs are C elisions of the Cilk programs, except for \nthose programs that are starred (*), where the parallel program implements a different algorithm than \nthe serial program. Programs labeled by a dagger (t) are nondeterministic, and thus, the running time \non one processor is not the same as the work performed by the computation. For these programs, the value \nfor Ti indicates the actual work of the computation on 8 processors, and not the running time on one \nprocessor. steal, the thief increments E. If there is nothing to steal, the thief restores E to the original \nvalue. Otherwise, the thief steals frame H and increments H. From the point of view of a worker, the \ncommon case is the same as in the simplified protocol: it compares two pointers (E and T rather than \nH and T). The exception mechanism is used to implement abort. When a Cilk procedure executes an abort \ninstruction, the runtime system serially walks the tree of outstanding descen-dants of that procedure. \nIt marks the descendants as aborted and signals an abort exception on any processor working on a descendant. \nAt its next pop, an aborted procedure will discover the exception, notice that it has been aborted, and \nreturn immediately. It is conceivable that a procedure could run for a long time without executing a \npop and discovering that it has been aborted. We made the design decision to accept the possibility of \nthis unlikely scenario, figuring that more cycles were likely to be lost in work overhead if we abandoned \nthe THE protocol for a mechanism that solves this minor problem. Benchmarks In this section, we evaluate \nthe performance of Cilk-5. We show that on 12 applications, the work overhead cl is close to 1, which \nindicates that the Cilk-5 implementation exploits the work-first principle effectively. We then present \na break- down of Cilk s work overhead cl on four machines. Finally, we present experiments showing that \nthe critical-path over-head cm is reasonably small as well. Figure 6 shows a table of performance measurements \ntaken for 12 Cilk programs on a Sun Enterprise 5000 SMP with 8 167-megahertz UltraSPARC processors, each \nwith 512 kilo- bytes of L2 cache, 16 kilobytes each of Ll data and instruc- tion caches, running Solaris \n2.5. We compiled our programs with gee 2.7.2 at optimization level -03. For a full descrip-tion of these \nprograms, see the Cilk 5.1 manual [8]. The table shows the work of each Cilk program TI, the critical \npath T,, and the two derived quantities 7 and cr. The ta-ble also lists the running time T8 on 8 processors, \nand the speedup Tl/Ts relative to the one-processor execution time, and speedup Ts/TB relative to the \nserial execution time. For the 12 programs, the average parallelism F is in most cases quite large relative \nto the number of processors on a typical SMP. These measurements validate our assumption of parallel \nslackness, which implies that the work term dom-inates in Inequality (4). For instance, on 1024 x 1024 \nmatri-ces, notempmul runs with an average parallelism of 1970- yielding adequate parallel slackness for \nup to several hun-dred processors. For even larger machines, one normally would not run such a small \nproblem. For notempmul, as well as the other 11 applications, the average parallelism grows with problem \nsize, and thus sufficient parallel slackness is likely to exist even for much larger machines, as long \nas the problem sizes are scaled appropriately. The work overhead cl is only a few percent larger than \n1 for most programs, which shows that our design of Cilk-5 faithfully implements the work-first principle. \nThe two cases where the work overhead is larger (cilksort and cholesky) are due to the fact that we had \nto change the serial algo-rithm to obtain a parallel algorithm, and thus the compar- ison is not against \nthe C elision. For example, the serial C algorithm for sorting is an in-place quicksort, but the par-allel \nalgorithm cilksort requires an additional temporary array which adds overhead beyond the overhead of \nCilk it-self. Similarly, our parallel Cholesky factorization uses a quadtree representation of the sparse \nmatrix, which induces more work than the linked-list representation used in the serial C algorithm. Finally, \nthe work overhead for fib is large, because fib does essentially no work besides spawn-ing procedures. \nThus, the overhead cl = 3.63 for fib gives a good estimate of the cost of a Cilk spawn versus a traditional \nC function call. With such a small overhead for spawning, one can understand why for most of the other \napplications, which perform significant work for each spawn, the overhead of Cilk-5 s scheduling is barely \nnoticeable compared to the 10% noise in our measurements. 466huh Alpha 21164 2OOMI-h PalUum Ro 167 \nMHz Ultra SPARC I 195 MHz MIPS RlOOOO 0r .1 2 3 4 i 6 7 ovaheads Figure 7: Breakdown of overheads \nfor fib running on one pro-cessor on various architectures. The overheads are normalized to the running \ntime of the serial C elision. The three overheads are for saving the state of a procedure before a spawn, \nthe allocation of activation frames for procedures, and the THE protocol. Ab-solute times are given for \nthe per-spawn running time of the C elision. We now present a breakdown of Cilk s serial overhead cl \ninto its components. Because scheduling overheads are small for most programs, we perform our analysis \nwith the fib program from Figure 1. This program is unusually sensi-tive to scheduling overheads, because \nit contains little actual computation. We give a breakdown of the serial overhead into three components: \nthe overhead of saving state before spawning, the overhead of allocating activation frames, and the overhead \nof the THE protocol. Figure 7 shows the breakdown of Cilk s serial overhead for fib on four machines. \nOur methodology for obtaining these numbers is as follows. First, we take the serial C fib program and \ntime its execution. Then, we individually add in the code that generates each of the overheads and time \nthe execution of the resulting program. We attribute the additional time required by the modified program \nto the scheduling code we added. In order to verify our numbers, we timed the fib code with all of the \nCilk overheads added (the code shown in Figure 3), and compared the resulting time to the sum of the \nindividual overheads. In all cases, the two times differed by less than 10%. Overheads vary across architectures, \nbut the overhead of Cilk is typically only a few times the C running time on this spawn-intensive program. \nOverheads on the Alpha machine are particularly large, because its native C function calls are fast compared \nto the other architectures. The state-saving costs are small for fib, because all four architectures \nhave write buffers that can hide the latency of the writes required. We also attempted to measure the \ncritical-path over-head coo. We used the synthetic knary benchmark [4] to synthesize computations artificially \nwith a wide range of work and critical-path lengths. Figure 8 shows the outcome from many such experiments. \nThe figure plots the measured * . --...I 0.1 1 Normalized Machine Size Figure 8: Normalized speedup \ncurve for Cilk-5. The horizontal axis is the number P of processors and the vertical axis is the speedup \nTl/Tp, but, each data point has been normalized by di- viding by Tl /Tm. The graph also shows the speedup \npredicted by the formula Tp = Tl/P + Too. speedup TllTp for each run against the machine size P for that \nrun. In order to plot different computations on the same graph, we normalized the machine size and the \nspeedup by dividing these values by the average parallelism F = Tl/T,, as was done in [4]. For each run, \nthe horizontal position of the plotted datum is the inverse of the slackness P/F, and thus, the normalized \nmachine size is 1.0 when the number of processors is equal to the average parallelism. The vertical position \nof the plotted datum is (Tl/Tp)/F = T,/Tp, which measures the fraction of maximum obtainable speedup. \nAs can be seen in the chart, for almost all runs of this bench-mark, we observed TP 5 Tl/P + l.OT,. (One \nexceptional data point satisfies TP z Tl/P + l.O5T,.) Thus, although the work-first principle caused \nus to move overheads to the critical path, the ability of Cilk applications to scale up was not significantly \ncompromised. 7 Conclusion We conclude this paper by examining some related work. Mohr et al. [24] introduced \nlazy task creation in their im-plementation of the Mul-T language. Lazy task creation is similar in many \nways to our lazy scheduling techniques. Mohr et al. report a work overhead of around 2 when com-paring \nwith serial T, the Scheme dialect on which Mul-T is based. Our research confirms the intuition behind \ntheir methods and shows that work overheads of close to 1 are achievable. The Cid language [26] is like \nCilk in that it uses C as a base language and has a simple preprocessing compiler to convert parallel \nCid constructs to C. Cid is designed to work in a distributed memory environment, and so it employs latency-hiding \nmechanisms which Cilk-5 could avoid. (We are working on a distributed version of Cilk, however.) Both \nCilk and Cid recognize the attractiveness of basing a parallel language on C so as to leverage C compiler \ntechnology for high-performance codes. Cilk is a faithful extension of C, however, supporting the simplifying \nnotion of a C elision and allowing Cilk to exploit the C compiler technology more readily. TAM [IO] and \nLazy Threads [14] also analyze many of the same overhead issues in a more general, nonstrict lan-guage \nsetting, where the individual performances of a whole host of mechanisms are required for applications \nto obtain good overall performance. In contrast, Cilk s multithreaded language provides an execution \nmodel based on work and critical-path length that allows us to focus our implemen-tation efforts by using \nthe work-first principle. Using this principle as a guide, we have concentrated our optimizing effort \non the common-case protocol code to develop an effi- cient and portable implementation of the Cilk language. \nAcknowledgments We gratefully thank all those who have contributed to Cilk development, including Bobby \nBlumofe, Ien Cheng, Don Dailey, Mingdong Feng, Chris Joerg, Bradley Kuszmaul, Phil Lisiecki, Albert0 \nMedina, Rob Miller, Aske Plaat, Bin Song, Andy Stark, Volker Strumpen, and Yuli Zhou. Many thanks to \nall our users who have provided us with feedback and suggestions for improvements. Martin Rinard suggested \nthe term work-first. References Andrew W. Appel and Zhong Shao. Empirical and analytic study of stack \nversus heap cost for languages with closures. Journal of Functional Programming, 6(1):47-74, 1996. PI \nPI Nimar S. Arora, Robert, D. Blumofe, and C. Greg Plaxton. Thread scheduling for multiprogrammed multiprocessors. \nIn Proceedings of the Tenth Annual ACM Symposium on Par-allel Aloon thms and Architectures (SPAA). Puerto \nVallarta, Mexico; June 1998. To appear. Robert D. Blumofe. Executing Multithreaded Programs Ej-ficiently. \nPhD thesis, Department of Electrical Engineering and Computer Science, Massachusetts Institute of Techno-logy, \nSeptember 1995. [31 Robert D. Blumofe, Christopher F. Joerg, Bradley C. Kuss- maul, Charles E. Leiserson, \nKeith H. Randall, and Yuli Zhou. Cilk: An efficient multithreaded runtime system. Journal of Parallel \nand Distributed Computing, 37(1):55-69, August 1996. I41 Robert D. Blumofe and Charles E. Leiserson. \nScheduling multithreaded computations by work stealing. In Pmceed-ings of the 35th Annual Symposium on \nFoundations of Com- puter Science (FOCS), pages 356-368, Santa Fe, New Mex-ico, November 1994. [51 Richard \nP. Brent. The parallel evaluation of general arith-metic expressions. Journal of the ACM, 21(2):201-206, \nApril 1974. Guang-Ien Cheng, Mingdong Feng, Charles E. Leiserson, Keith H. Randall, and Andrew F. Stark. \nDetecting data races in Cilk programs that use locks. In Proceedings of the Tenth Annual ACM Symposium \non Parallel Algorithms and PI Architectures (SPAA), Puerto Vallarta, Mexico, June 1998. To appear. Cilk-5.1 \n(Beta 1) Reference Manual. Available on the Inter- net fromhttp://theory.lcs.mit.edu/ cilk. PI Thomas \nH. Cormen, Charles E. Leiserson, and Ronald L. Rive&#38; Introduction to Algorithms. The MIT Press, Cam-bridge, \nMassachusetts, 1990. PI David E. Culler, Anurag Sah, Klaus Erik Schauser, Thorsten von Eicken, and John \nWawrzynek. Fine-grain parallelism with minimal hardware support: A compiler-controlled threaded abstract \nmachine. In Proceedings of the Fourth International Conference on Architectural Support for Pro-gmmming \nLanguages and Opemting Systems (ASPLOS), pages 164-175, Santa Clara, California, April 1991. Pll PO1 \nE. W. Dijkstra. Solution of a problem in concurrent pro-gramming control. Communications of the ACM, \n8(9):569, September 1965. Marc Feeley. Polling efficiently on stock hardware. In Pro- 1121 ceedings of \nthe 1993 ACM SIGPLAN Conference on Func-tional Programming and Computer Architecture, pages 179-187, \nCopenhagen, Denmark, June 1993. Mingdong Feng and Charles E. Leiserson. Efficient detection of determinacy \nraces in Cilk programs. In Proceedings of the Ninth Annual ACM Symposium on Parallel Algorithms and Architectures \n(SPAA), pages l-11, Newport, Rhode Island, June 1997. P31 S. C. Goldstein, K. E. Schauser, and D. E. \nCuller. Lazy threads: Implementing a fast parallel call. Journal of Paml- lel and Distributed Computing, \n37(1):5-20, August 1996. P41 R. L. Graham. Bounds on multiprocessing timing anoma-lies. SIAM Journal \non Apvlied Mathematics, 17(2):416-429, D51 .c March 1969. __ Dirk Grunwald. Heaps o stacks: Time and \nspace efficient threads without operating system support. Technical Report CU-CS-750-94, University of \nColorado, November 1994. 1161 Dirk Grunwald and Richard Neves. Whole-program opti-mization for time and \nspace efficient threads. In Proceedings of the Seventh International Conference on Architectural Support \nfor Programming Languages and Opemting Systems (ASPLOS), pages 50-59, Cambridge, Massachusetts, Octo-ber \n1996. PI Christopher F. Joerg. The Cilk System for Parallel Multi-threaded Computing. PhD thesis, Department \nof Electrical Engineering and Computer Science, Massachusetts Institute of Technology, January 1996. \nP91 Robert H. Halstead Jr. Multilisp: A language for concurrent symbolic computation. ACM 7Wzsactions \non Progmmming Languages and Systems, 7(4):501-538, October 1985. WI Leslie Lamport. How to make a multiprocessor \ncomputer that correctly executes multiprocess programs. IEEE Darts-actions on Computers, C-28(9):690-691, \nSeptember 1979. PI 1171 Phillip Lisiecki and Albert0 Medina. Personal communica- tion. P21 James S. Miller \nand Guillermo J. Rozas. Garbage collection is fast, but a stack is faster. Technical Report Memo 1462, \nMIT Artificial Intelligence Laboratory, Cambridge, MA, 1994. Robert C. Miller. A type-checking preprocessor \nfor Cilk 2, a multithreaded C language. Master s thesis, Department of Electrical Engineering and Computer \nScience, Massachusetts Institute of Technology, May 1995. [231 Eric Mohr, David A. Kranz, and Robert \nH. Halstead, Jr. Lazy task creation: A technique for increasing the granular- ity of parallel programs. \nIEEE Dunsactions on Pamllel and Distributed Systems, 2(3):264-280, July 1991. [251 Joel Moses. The function \nof FUNCTION in LISP or whv the FUNARG problem should be called the environment prob-lem. Technical Report \nmemo AI-199, MIT Artificial Intelli-gence Laboratory, June 1970. 1241 [ZS] Rishiyur Sivaswami Nikhil. \nParallel Symbolic Computing in Cid. In Prvc. Wkehp. on Parallel Symbolic Computing, Beaune, I%ance, Springer-Verlag \nLNCS 1068, pages 217- 242, October 1995. [27] Per Stenstriim. VLSI support for a cactus stack oriented \nmemory organization. Proceedings of the Twenty-First An-nual Hawaii International Conference on System \nSciences, volume 1, pages 211-220, January 1988. [28] Leslie G. Valiant. A bridging model for parallel \ncomputation. Communications of the ACM, 33(8):103-111, August 1990. \n\t\t\t", "proc_id": "277650", "abstract": "The fifth release of the multithreaded language Cilk uses a provably good \"work-stealing\" scheduling algorithm similar to the first system, but the language has been completely redesigned and the runtime system completely reengineered. The efficiency of the new implementation was aided by a clear strategy that arose from a theoretical analysis of the scheduling algorithm: concentrate on minimizing overheads that contribute to the work, even at the expense of overheads that contribute to the critical path. Although it may seem counterintuitive to move overheads onto the critical path, this \"work-first\" principle has led to a portable Cilk-5 implementation in which the typical cost of spawning a parallel thread is only between 2 and 6 times the cost of a C function call on a variety of contemporary machines. Many Cilk programs run on one processor with virtually no degradation compared to equivalent C programs. This paper describes how the work-first principle was exploited in the design of Cilk-5's compiler and its runtime system. In particular, we present Cilk-5's novel \"two-clone\" compilation strategy and its Dijkstra-like mutual-exclusion protocol for implementing the ready deque in the work-stealing scheduler.", "authors": [{"name": "Matteo Frigo", "author_profile_id": "81100040135", "affiliation": "MIT Laboratory for Computer Science, 545 Technology Square, Cambridge, Massachusetts", "person_id": "P194388", "email_address": "", "orcid_id": ""}, {"name": "Charles E. Leiserson", "author_profile_id": "81331497630", "affiliation": "MIT Laboratory for Computer Science, 545 Technology Square, Cambridge, Massachusetts", "person_id": "PP39067312", "email_address": "", "orcid_id": ""}, {"name": "Keith H. Randall", "author_profile_id": "81100063247", "affiliation": "MIT Laboratory for Computer Science, 545 Technology Square, Cambridge, Massachusetts", "person_id": "P159129", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/277650.277725", "year": "1998", "article_id": "277725", "conference": "PLDI", "title": "The implementation of the Cilk-5 multithreaded language", "url": "http://dl.acm.org/citation.cfm?id=277725"}