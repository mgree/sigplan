{"article_publication_date": "05-01-1998", "fulltext": "\n Automatically Closing Open Reactive Programs Christopher Colby Patrice Godefroid Lalita Jategaonkar \nJagadeesan Loyola University Chicago Bell Laboratories Bell Laboratories Dept. of Math. and Comp. Sciences \nLucent Technologies Lucent Technologies 6525 N. Sheridan Rd. 1000 E. Warrenville Road 1000 E. Warrenville \nRoad Chicago, IL 60626 Naperville, IL 60566 Naperville, IL 60566 colby@math.luc.edu godQbell-labs.com \nlalita@bell-labs.com Abstract We study in this paper the problem of analyzing implemen-tations of open \nsystems -systems in which only some of the components are present. We present an algorithm for auto- \nmatically closing an open concurrent reactive system with its most general environment, i.e., the environment \nthat can provide any input at any time to the system. The result is a nondeterministic closed (i.e., \nself-executable) system which can exhibit all the possible reactive behaviors of the original open system. \nThese behaviors can then be analyzed using VeriSoft, an existing tool for systematically exploring the \nstate spaces of closed systems composed of multiple (pos-sibly nondeterministic) processes executing \narbitrary code. We have implemented the techniques introduced in this pa-per in a prototype tool for \nautomatically closing open pro-grams written in the C programming language. We dis- cuss preliminary \nexperimental results obtained with a large telephone-switching software application developed at Lu- \ncent Technologies. 1 Introduction Systematic state-space exploration, as such or elaborated into temporal-logic \nmodel-checking (e.g., [CES86, QSSl]), is attracting growing attention for checking the correctness of \nconcurrent reactive systems. State-space exploration tools for software systems have traditionally been \nrestricted to the exploration of the state space of an abstract description of the system, specified \nin a modeling language (e.g., [HohU, CPS93, McM93]). VeriSoft [God971 is a recent tool which ex-tends \nthe scope of systematic state-space exploration to con- current systems in which processes execute arbitrary \ncode written in general-purpose programming languages such as C or C++. VeriSoft explores the state space \nof a concurrent system by actually executing the code for the components of the system and by controlling \ntheir synchronizations. It thus eliminates the need for modeling the system, an often time-consuming \nand error-prone task, and combines aspects of debugging and replay tools for concurrent systems with \nPermkkn 10 make digital or hard copies of all OT part of this wwh for personal or clsswoom usa is granted \nwithout fea provided that copies era not made or distributed for profit or commercial advan-tage and \nthat ccpks boar this notice and the full citation on tha 6-t page. To copy otherwise. 10 republish. lc \np-1 cn servws 01 to redistribute lc Iis*. raquirw prior mpesific permismlcn and/or a fae. SIGPIAN 96 \nMontreal. Canada 0 1998 ACM 0-69791~9674/96/0006...$5,00 the sort of state-space exploration associated \nwith model checking. Systematic state-space exploration requires the system being analyzed to be closed, \ni.e., self-executable. This im-plies that, given an open system, an executable representa-tion of the \nenvironment in which the system operates need be available for closing the system. Providing manually \na simple but faithful representation of the environment of an open system is often a difficult task. \nIn order to analyze the implementation of a large open system with VeriSoft, this task can become tedious \nand impractical because the interface between the implementation of a system and its environment can \nbe complex. We address in this paper the problem of automaticaZEy closing the implementation of an open \nsystem. Given an open system, a model of its environment -whether generated manually or automatically \n-might be either too restrictive, in which case it may cause the ver-ification to miss some possible \nerroneous behaviors of the system, or too general, in which case it will increase the size of the state \nspace and may result in unrealistic erroneous behaviors. This paper describes an algorithm for closing \na system with its most general environment. In this way, it is certain that the verification will not \nmiss any erroneous behaviors due to an insufficiently general environment. In practice, the intended \nuse of this approach is that a de-veloper provides manually an implementation for a partial model of \nthe environment, in order to capture more precisely certain areas of interest, and then applies our algorithm \nto close the remainder of the system. At first blush, such an algorithm might seem easy to achieve: Given \nan open system S, add a new component Es to S whose behavior includes all possible sequences of inputs \nand outputs of S. However, this naive approach gen-erates a closed system whose state space is typically \nso large that it renders any analysis intractable: for instance, Es is infinitely branching whenever \nthe set of inputs is infinite. Instead, our algorithm uses a static analysis of the open sys- tem to \neliminate its interface. In fact, our transformation preserves, or may even reduce, the static degree \nof branching of the original code. Our work is inspired by challenges encountered when ap- plying VeriSoft \nto analyze software applications in Lucent Technologies 5ESS telephone switching system. Many ap-plications \nwithin the 5ESS software are composed of concur- rent reactive components of considerable size and complex- \nity, and cannot be suitably analyzed using traditional test-ing tools. On the other hand, it is non-trivial \nto close such multi-process applications in order to analyze them using automatic state-space exploration \ntechniques. For example, manually closing a 5ESS application can be tantamount to simulating millions \nof lines of code implementing the rest of the 5ESS software. We have implemented the techniques introduced \nin this paper in a prototype tool for automatically closing open programs written in the C programming \nlanguage. We have applied our tool to a complex 5ESS software application, and are in the process of \nanalyzing the resulting closed system using VeriSoft. This paper is organized as follows. In the next \nsection, we give some background about the verification of concurrent reactive systems, and describe \nthe underlying framework adopted in this work. Then we discuss in Section 3 basic issues involved in \nclosing open concurrent reactive systems. Next, we present our algorithm for closing open programs, and \nillustrate it with two examples. The correctness of the algorithm is established in Section 5, where \nits precision is also discussed. We then remark on our practical experi- ence in applying this work to \ntelecommunication software. Finally, we conclude with a comparison between program closing and related \nwork, and with suggestions on possible improvements. Background We recall in this section the framework \nintroduced in [God971 We consider a closed concurrent system S composed of a fi- nite set P of processes \nand a finite set 0 of communication objects. Each process P E P executes a sequence of op-erations that \nis described in a sequential program written in a full-fledged programming language such as C. Such se-quential \nprograms are deterministic: every execution of the program on the same data performs the same sequence \nof op- erations. We assume that processes only communicate with each other by performing operations on \ncommunication ob-jects. A communication object 0 E 0 is defined by a pair (V, OP), where V is the set \nof aJl possible values for the ob- ject (its domain), and OP is the set of operations that can be performed \non the object. Examples of communication objects are shared variables, semaphores, and FIFO buffers. \nAt any time, at most one operation can be performed on a given communication object (operations on a \nsame commu- nication object are mutually exclusive). For the purpose of this work, we also assume that \nthe enabledness of any op- eration on any communication object depends exclusively on the sequence of \noperations that has been performed on the object in the history of the system, and not on values that \nare possibly stored or passed through the object, or passed as argument to the operation. Operations \non com-munication objects axe called visible operations, while other operations are by default called \ninvisible. The execution of an operation is said to be blocking if it cannot be completed. We assume \nthat only executions of visible operations may be blocking. At any time, the concurrent system is said \nto be in a state. The system is said to be in a global state when the next operation to be executed by \nevery process in the sys-tem is a visible operation. We assume that every process in the system always \neventually attempts to execute a visible operation.] This implies that initially, after the creation \nof VeriSoft (see forthcoming description) reports a divergence all the processes of the system, the system \nmay reach a first and unique global state so, called the initial global state of the system. We define \na process transition, or transition for short, as one visible operation followed by a finite sequence \nof invisible operations performed by a single process and ending just before a visible operation. Let \nI denote the set of all transitions of the system. A transition is said to be disabled in a global state \ns when the execution of its visible operation is blocking in s. Oth-erwise, the transition is said to \nbe enabled in s. A transition t that is enabled in a global state s can be ezecuted from s. Because the \nnumber of invisible operations in a transi- tion is finite, the execution of an enabled transition always \nterminates. When the execution of t from s is completed, the system reaches a global state s , called \nthe successor of s by t. We write s 5 s to mean that the execution of the transition t leads from the \nglobal state s to the global state s , while s $- s means that the execution of the finite sequence M \nof transitions leads from s to 9 . If s 4 s , s is said to be reachable from s. We now define a formal \nsemantics for the concurrent sys-tems that satisfy our assumptions. A concurrent system as defined here \nis a closed system: from its initial global state, it can evolve and change its state by executing enabled \ntran-sitions. Therefore, a natural way to describe the possible behaviors of such a system is to consider \nits set of reachable global states and the transitions that are possible between these. Formally, the \njoint gZoba1 behavior of all processes in a concurrent system can be represented by a transition system \nAs = (S, A, SO) such that S is the set of global states of the system, A G S x S is the transition relation \ndefined such that (s,s ) E A iff 3 E 7 : s 5 s , and SO is the initial global state of the system. An \nelement of A corresponds to the execution of a single transition t E 7 of the system. The elements of \nA will be referred to as global transitions. It is natural to restrict As to its global states and transitions \nthat are reachable from so, because the other global states and transitions play no role in the behavior \nof the system. In what follows, a state in As denotes a global state that is reachable from SO. As is \ncalled the global state space of the system. Because we consider here closed concurrent systems, the \nenvironment of one process is formed by the other processes in the system. This implies that, in the \ncase of a single Lopen reactive system, the environment in which this sys-tem operates has to be represented, \npossibly using other processes. In practice, a complete representation of such an environment may not \nbe available, or may be quite com-plex. It is then convenient to use a model, i.e., a simplified representation, \nof the environment to simulate its external behavior. For this purpose, we introduce a special oper-ation \nVS-toss to express a valuable feature of modeling languages: nondeterminism. This operation takes as \nargu- ment a positive integer n, and returns an integer in [0, n]. The operation is nondeterministic: \nthe execution of a transi- tion starting with VS-toss(n) may yield up to n+ 1 different successor states, \ncorresponding to different values returned by VSfoss. In contrast with what was defined in [God97], when \na process does not attempt to execute any visible operation for mire than a given (user-specified) amount \nof time. Operations 011 objects (and IEIUX transitions) are deterministic: the execution of a transition \nt in a state 8 leads to a unique swceswr state. we consider VS-toss as an invisible operation in this \npaper, in order to simplify the following presentation and terminol- ogy. In [God97], it is shown that \ndeadlocks and assertion vio-lations can be detected by exploring only the global states of a concurrent \nsystem as defined above. Deadlocks are states where the execution of the next operation of every process \nin the system is blocking. Assertions can be specified by the user with the special operation VSassert \n. This operation can be inserted in the code of any process, and is consid- ered visible. It takes as \nits argument a boolean expression that can test and compare the value of variables local to the process. \nWhen VS_assert(expression) is executed, the expression is evaluated. If the expression evaluates to false, \nthe assertion is said to be violated. VeriSoft [God971 is a tool for systematically exploring the state \nspace of a concurrent system as defined above. In a nutshell, every process of the concurrent system \nto be analyzed is mapped to a UNIX process. The execution of the system processes is controlled by an \nexternal process, called the scheduler. The scheduler observes the visible and VSfoss operations performed \nby processes inside the sys-tem, and can suspend their execution. By resuming the execution of (the next \nvisible operation of) one selected sys-tem process in a global state, the scheduler can explore one transition \nbetween two global states in the state space of the concurrent system. By reinitializing the system, \nthe scheduler can explore alternative paths in the state space. Because states of processes can be complex \n(due to point- ers, dynamic memory allocation, large data structures of various shapes, recursion, etc.), \nVeriSoft does not attempt to compute any representation for the reachable states of the system being \nanalyzed, and hence performs a system- atic state-space exploration without storing any intermedi- ate \nstates in memory. It is shown in [God971 that the key to make this approach tractable is to use a new \nsearch al-gorithm built upon existing state-space pruning techniques known as partial-order methods [God96]. \nFor finite acyclic state spaces, this search algorithm is guaranteed to termi-nate and can be used for \ndetecting deadlocks and assertion violations without incurring the risk of any incompleteness in the \nverification results. In practice, VeriSoft can be used for systematically and efficiently testing the \ncorrectness of any concurrent system, whether or not its state space is acyclic. Indeed, it can always \nguarantee, from a given ini-tial state, complete coverage of the state space up to some depth. Closing \nOpen Systems Systematic state-space exploration requires the system be-ing analyzed to be closed. Given \nan open system, an exe-cutable representation of its environment need be provided for closing the system. \nIn this section, we define more pre-cisely what closing an open system means, and discuss several approaches \nto the problem. Let us now consider an open concurrent system S: S can interact with its environment \nvia an interface composed of a set IS of inputs and a set OS of outputs. Let V; denote the set of possible \ninput values that can be provided by the environment to the system S via input i in Is, and let V, denote \nthe set of possible output values that can be pro- duced by S to its environment via output o in OS. \nClosing such an open system means combining the system with an executable representation for its environment. \nThe result of the combination is a self-contained executable system. Creating an executable representation \nfor the environ-ment of an open system can be a tedious task, because (1) the interface between the system \nand its environment may be complex and (2) many possible data values may be pro- vided by the environment. \nIn order to facilitate this task, it would be useful to have an algorithm for generating auto-matically \nan executable representation of the most general environment in which an open system can operate. Pre-cisely, \nwe define the most general environment Es of a sys- tem S as the environment that nondeterministically \nprovides any value v; in Vi whenever the system S takes an input i in IS, and that can take any output \no in OS produced by the system. By construction, the combination of the system S with its most general \nenvironment ES makes it possible to observe all the possible visible behaviors that S can exhibit. Note \nthat, by definition, there are no dependencies between the input and output values provided and accepted \nby ES, respectively. Generating automatically ES from S would address prob- lem (1) above, but not problem \n(2): the set of input values provided by ES can be very large, even infinite, which would yield an intractable \nstate-space search. A better approach would be to partition the sets of possible input values pro-vided \nby Es into equivalence classes such that values in the same equivalence class would imply exactly the \nsame behavior of S. Of course, partitioning each data domain vi, i E Is, into such equivalence classes \nis a hard problem, and undecidable in general. Computing simple conservative approximations for these \nequivalence classes is possible but would require a sophisticated, and hence more expensive, static analysis \nof the code describing the system. We will come back to this option in Section 7. In this work, we investigate \nan alternative approach: In-stead of generating an executable representation of the most general environment \nES of an open system S, we propose to eliminate its interface altogether. Specifically, we present in \nthe next section an algorithm that transforms an open sys-tem S into a closed (nondeterministic) system \nS such that all data values in S x Es that may depend on the behavior of ES are eliminated in S , and \nall control-flow choices in S x ES that may depend on these data values are replaced by non-deterministic \nchoices in S . The reactive behavior of S x ES and S , as well as their effect on data values that do \nnot depend on ES, are closely related: Every execution of S x Es corresponds to an execu-tion of S that \nexhibits the same sequence of visible operations and that preserves all data values that do not depend \non ES. All deadlocks and all assertion violations in Asx~s that evaluate only expressions whose value \ndoes not depend on Es are preserved in ASI. Our algorithm performs a static analysis of the source code \nexecuted by the processes P in P of the open system S. Specifically, we assume that each process P in \nP executes a sequence of operations, that is described in a sequential pm-gram written in a full-fledged \nprogramming language such as C. We also assume that such a program can be decomposed into a finite collection \nof procedures pj which may call each other, and includes a unique top-level procedure. Because open systems \nare composed of processes, and because programs describing processes are composed of pro- cedures, we \nmap the notions of inputs and outputs from the system level to the procedure level as follows. Let 1j \nand Oj denote the input and output sets of procedure pj, respec-tively. When a procedure pj produces \nilll output o in Oj that is taken as input i in Ik by another procedure pk, we will write o = i. By construction, \nwe have: IS = (Uj Ij) \\ (Uj Oj) and OS C Uj Oj. Examples of input values of a procedure pj include values \npassed as argument to pj when it is being called, and point- ers to values used in pj but set outside \nof pj. In the next section, we present an algorithm for trans-forming each procedure pj individually. \nThen, we prove that these local transformations preserve together the visible behavior of each of the \nprocesses P in P and of the system S itself. 4 The Algorithm We assume we are working with an imperative \nprogram-ming language that meets the following general description. A program consists of a sequence \nof statements of the follow- ing four types: assignment statements which assign values to memory locations \ncalled variables, conditional statements (if-then-else, switch-case, while, for), procedure call state-ments, \nand termination statements (return, exit). The pro-gramming language also provides basic atomic data \nstruc-tures (e.g., integer, real, boolean), and constructor and se-lector operations (e.g., records, \narrays, pointers) for creating and manipulating data structures built up from the basic data structures. \nVisible operations are procedure calls of a specific kind. Throughout this paper, we use the term variable \nto refer to a memory location in which a value may be stored. Exam-ples of variables are identifiers \n(program variables), pointers, array elements, mutable record fields, and so forth. A vari- able is thus \na semantic object rather than a syntactic one. Our motivation for this is several-fold: increased generality, \nindependence of the framework from the choice of source language, and increased abstraction in the correctness \nspec-ification of our algorithm. Every procedure pj described in a program satisfying the above assumptions \ncan be represented by a control-flow graph Gj = (Nj, Aj), where l the set of nodes Nj is the set of statements \nthat appear in the program describing pj; l the set of ucs Aj C Nj x Nj is such that (n, n ) E Aj iff \nthe program statement corresponding to n may be the next one to be executed by pj after the execution \nof the program statement corresponding to n. Each arc (n, n ) in Aj is labeled with a boolean expression \non variables occurring in the statement that specifies when the program statement corresponding to n \nis executed af-ter the program statement corresponding to n. For every node n in Nj , the boolean expressions \nthat label arcs from n are mutually exclusive, and their disjunction is a tautology. Note that we assume \nthat the control flow describing the se-quencing of operations performed by a process is completely specified \nby the procedures pj; interruptions and other pre-emptive schemes that may violate this assumption are \nnot considered here for the sake of simplicity. Let n be a node of the control-flow graph of some proce- \ndure pj implementing an open system S. We say a variable v is used in node n if the value of v may be \nrequired during some execution of the statement corresponding to n (i.e., the value in the memory location \ncorresponding to v may be read). Similarly, we say a variable v is defined in node n if the value of \nv may be modified during the execution of the statement corresponding to n (i.e., a value may be written \nin the memory location corresponding to v). Let V(n) denote the set of variables used in node n. For \nsimplicity, we assume that every execution of an assignment statement defines the value of exactly one \nvariable. Conditional and termination statements are assumed not to define any variables. Procedure calls \nare modeled as follows. We assume that each argument of a procedure call is a variable. When the procedure \ncall is executed, a new fresh variable is created for each argument and is initialized (defined) with \nthe value of the corresponding variable passed as argument. Fresh variables are assumed not to escape \ntheir scope: they can-not be used by the calling procedure. The execution of the called procedure can \nthen start with the execution of its start node. By definition, we assume that start nodes do not use \nnor define any variables. After the execution of a termina- tion statement in the called procedure, the \nexecution of the calling procedure is resumed at the (unique) node following the corresponding procedure \ncall. We assume that termi-nation statements in the top-level procedure of any process is always blocking. \n(Therefore, the number of processes is always constant.) In addition to the above somewhat standard assump-tions, \nwe assume that, for each input i in Ij, it is possible to determine whether i is also in Is. This means \nthat it is possible to determine statically which input values of a pro- cedure pj may be provided by \nthe environment Es, including indirectly via other procedures. For simplicity, we assume the environment \nEs is not allowed to define variables that have been previously defined by the system S. For every procedure \npj, we also compute a define-use graph q = (Nj , q) , where l the set of nodes Nj is the set of statements \nthat appear in the program describing pj; l the set of arcs x 5 Nj x Nj is such that (n, n ) is in Ai \nimplies that the program statement corresponding to n uses the value of a variable v defined by the program \nstatement corresponding to n; the arc (n,n ) is then labeled with v. Furthermore, if a node n defines \na variable v and a node n uses variable v, and if there is a control-flow path from n to n in Gj along \nwhich v is not defined, then there is an arc (n, n ) in z labeled with v. Techniques for computing define-use \ndependencies are dis-cussed, e.g., in [ASUSS, FOW87, MR90], and require a may- alias analysis (e.g., \n[CWZSO, Lan91, Deu94, Ruf95j). We now turn to the presentation of our algorithm for closing an open procedure \npj. This algorithm is presented in Figure 1. It takes as input both the control-flow graph Gj and the \ndefine-use graph Gj of the procedure pj. The algorithm generates a new control-flow graph Gi by trans- \nforming Gj using information extracted from ?$. From Gi, 1. Input: the control-flow graph Gj and define-use \ngraph q of procedure pj. 2. Analysis of q to compute VI(n) for each node n:  . Let NnS denote the \nset of nodes in Nj that usa the value of a variable defined by the environment Es. . Compute the set \nNI of nodes in Nj that are reachable from a node in NED by a (possibly empty) sequence of define-use \n arcs in q. l For each node n in N,, let VJ(~) d enote the set of variables used in n that are defined \nby Es or that are labeling an arc leading to n from n E NI in q. l For each node n not in N,, we have \nVI(~) = 0. 3. Mark the nodes of Gj according to the following rules: l mark the start node; l mark each \nnode corresponding to a termination statement; e mark each node corresponding to a call to a procedure \nof the system; . mark each node n corresponding to an assignment or conditional statement such that rr \nis not in N,. 4. Generate the control-flow graph G . = (Ni, AI) as follows. For each node rr of Gj marked \nin Sftep 3, do the following: . addntoNi; . for each arc a = (n, n ) E Aj, let succ(a) denote the set \nof marked nodes of Gj that are reachable from n by a sequence aw of control-flow arcs in Gj passing through \nunmarked nodes exclusively and starting with arc a; - if Isucc(o)I = 0, do nothing; -  if Isucc(a)l \n= 1, add an arc in Ai from n to the node in SUCC(~) and labeled with the boolean expression labeling \narc a; - if Is~cc(a)l > 1, create a new node n corresponding to a conditional statement testing the value \nof VS-toss(lsucc(a)l -1) ; add an arc in A: from n to n and labeled with the boolean expression labeling \narc a; then, fir every node nh E aucc(a), 0 5 i 5 (I s~cc(a)I -l), add an arc in Ai from n to nk and \nlabeled with a boolean expression that is satisfied iff the value returned by the call to VSfoss performed \nin n returns 5. 5. Perform the following final modiflcetions to G>: . remove the parameters of pj that \nare defined by Es; l for each node n in Gi corresponding to a procedure call to procedure pl, remove \neach argument of the procedure call whose corresponding parameter has been removed by Point 1 of Step \n6 when transforming GI. 0. Output: the control-flow graph Gi of procedure pi. Figure 1: Algorithm transforming \nGj into Gi it is then easy to construct a new procedure pi that has Gi for control-flow graph. Let us \ndetail the different steps performed by the algo-rithm. Step 2 determines which program statements of \npj may use (possibly indirectly via other variables) a value de-fined by the environment Es. This information \nis computed from the define-use graph of pj. Step 3 of the algorithm selects the program statements of \npj that will be preserved in $. These include all the procedure calls (which, by def- inition, include \nall the visible operations) and termination statements, as well as the assignment and conditional state-ments \nthat do not use any value provided by ES. Then, Step 4 constructs from the control-flow graph Gj a new \ncontrol-flow graph Gi which simulates, using the nondeter-ministic VS-toss operation, all the possible \neffects of the values provided by Es on the control-flow of pj. Step 5 completes the transformation by \nremoving references to pa- rameters of procedures that may be used to transmit values of variables defined \nby Es. Note that variables defined by Es from the point of view of a procedure include variables v defined \nin other procedure calls during the executions of nodes n corresponding to assignment statements such \nthat VI(~) # 0, or of nodes n corresponding to procedure calls such that v E VI(~). Therefore, the existence \nof a single node n corresponding to a procedure call to pj such that v E VI(~) is sufficient to make \nPoint 1 of Step 5 remove the parameter of pj corresponding to v. The overall time complexity of the above \nalgorithm is es- sentially linear in the size of Gj and q since the transforma- tion can be performed \nby a single traversal of both graphs. Note that Step 4 of the algorithm eliminates cyclic paths that \ntraverse exclusively unmarked nodes. Divergences due to such paths are therefore not preserved in Gg. \nFigures 2 and 3 illustrate the result of applying our al-gorithm on two different open procedures, p \nand q. The graphs on the left are the control-flow graphs G, and G, of the procedures. In each case, \nthe procedure takes as input a value provided by the environment ES and stored in a variable named z. \nThe graphs on the right are the closed control-flow graphs Gb and G.b that the algorithm gener-ates from \nG, and G,, respectively. Note that Gk and Gb are equivalent; although p and q are functionally distinct, \nthe algorithm transforms each of them to the same closed program. In the case of procedure p, the resulting \nclosed program is a strict upper approximation of p combined with its most general environment ES. For \nno values of z can G, send a mixture of even and odd values, but for cer-tain combinations of VS-toss \nresults, Gb can. In the case of procedure q, however, the resulting closed program is equiv- alent to \nq combined with its most general environment ES. In this case, q sends the ten least-significant bits \nof z, and so the set of executions induced by the set of all input values x is equivalent to the set \nof executions induced by the set of all VS-toss results. SW 0proc P(X) proc PO TIW y=n%Z TIW ti  f--Y \nI cnt=0; T TN0 -++-]?%!%y-iL--] cnt=cnt+l; +----J --I Figure 2: A simple example of transformation: \noriginal GP (left) and transformed Gb (right) SkWt SbXt Prm q(x) PC so Lr TIN TNe (CnKlO) y&#38;%2: \nTIUe + (Y=O) Tm TfW send( even ,cnt); S&#38;( Cdd .~l); send( edd ,atl); TfUe -is TrUe X=X/?.: TI-W \nTNe cnt=cnt+1; St+ =I I- Figure 3: Another example of transformation: original G, (left) and transformed \nGi (right) The correctness of the algorithm is established in the next section. 5 Correctness and Precision \nLet a store s be a function from variables (memory locations) to values. Let an execution u = so 2 91 \n2 92.. .&#38;-I 3 sm of a closed system S x Es be the sequence of stores si the system goes through while \nexecuting the sequence nr . . . nm of nodes. A variable v is said to be functionally dependent on an \ninput value vi provided by the environment Es after the execution of a sequence of nodes ni . . . n,,, \nof system S if there exist some executions c and u of S x Es executing the sequence of nodes nr . . . \nn, such that cr and a differ only by the value ui provided by Es sometime before or during the execution \nand the value of v at the end of u is different from the value of v at the end of a . Let Vr[nl . . . \nn,] denote the set of variables functionally dependent on some value provided by the environment after \nthe execution of n1 . ..n.. For instance, in the following simple procedure, proc p(x); { a=x%2 ; b=a+l \n; c=b: variables a, b, and c are functionally dependent on Es at the end of the procedure if the value \nof variable x is provided by the environment. In contrast, in the following procedure, proc p(x); { a=O; \nif (x> then b=a-1; else b=a+l; c=b: none of the variables a, b, and c are functionally dependent on \nthe environment at the end of the procedure, even if the value of variable x is provided by the environment. \nIndeed, given any control-flow path leading to the end of the pro-cedure (there are two such paths in \nthis example), all the executions of the procedure following this path will yield the same final values \nfor variables a, b, and c. In other words, the environment has no influence on the set of final values \nobtained after executing this path. In practice, computing such sets V~[nl . . . n,] is prob- lematic \nsince they are defined with respect to the execu-tions of the system to be analyzed, whose dynamic behavior \nis unknown. Therefore, our algorithm rather exploits the sets VI(~), which are computed for each node \nof each proce- dure in Step 2 of the algorithm by analyzing the define-use graph of that procedure. For \neach node n, the set VI(~) is an . . upper approxlmatron of the set of variables that are used in node \nn and functionally dependent of the environment after the execution of any sequence of nodes ending just \nbefore n. Formally, we have the following. Lemma 1 Let VI(~) denote the set of variables computed by \nStep 2 of the algorithm of Figure 1 for each node n in the control-flow graph Gj of a procedure pj. Then, \nfor any exe- cution u = so 3 91 3 92. . . s,,-1 + sm. of the closed sys-tem S x Es, we have (Vr[nl . \n. . ,,I:] n V(n,)) C_ Vi(nm). Proof: The proof is by induction on the length of execu- tions. For an \nexecution of length zero, i.e., when the system S is in its initial global state SO, the first node nr \nto be executed can only be the start node of the top-level proce-dure of some process in the system. \nSince by definition start nodes do not use any variables, we have V(nl) = 0, and the lemma trivially \nholds. Let us now prove that, if the lemma holds for executions of length smaller or equal to m, then \nit also holds for exe-cutions of length m + 1. Consider an execution SO 2 si 2 7% .Q...sm-1 , Sm sm+r \nof S x ES, and a variable v in Vr[nl... nm]lnV(n,+l). Let us show that v is in V (n,+l). If the value \nof v in nm+r is directly defined by Es some-time before the execution of node n,+r, nm+r is in the set \nNzs of nodes in Nj that uses the value of a variable defined by the environment Es (Point 1 of Step 2). \nHence, nm+i is in Nr (Point 2 of Step 2). Moreover, since v E V(nnt+l), v is in Vr(n,+*) (Point 3 of \nStep 2). Otherwise, let ni be the last node preceding n,+i in o and cr where v is defined. Hence, since \nthe environment is not allowed to define variables previously defined by the system, the value of v does \nnot change between si and sm+l. Since v E Vr[nl . . . nm], there exist some executions c and u of S x \nEs executing the sequence of nodes nr . . . n, such that cr and u differ only by some input value provided \nby Es sometime before or during the computation and the value of v at the end of B is different from \nthe value of v at the end of u . Since the value of v is the same in si and sm, there also exists executions \nof ni . . . ni that differ only by the same input value provided by ES sometime before or during the \ncomputation and that lead to different values of v after the execution of ni. Therefore, we have v E \nVr[nl . . . nil. Moreover, since v is defined in ni, this means that there exists some other variable \nv functionally dependent on Es after 711 . . . ni-r that is used in ni to define v. In other words, we \nhave V E Vr[nl . . . ni-l]CIV(ni). By applying the inductive hypothesis to execution SO 3 sr 2 92,. . \n2 si, we have V E Vr(ni). Consider the case where ni corresponds to the execution of a statement in another \nprocedure call. If n; corresponds to au assignment statement, since Vr(ni) # 0, II is considered as being \ndefined by ES from the point of view of the procedure of nm+r. Else ni corresponds to a higher-level \nprocedure call (fresh variables created by lower-level procedure calls are assumed not to escape their \nscope), and v is defined in ni with the value of V E VI (ni), then v is again considered as being defined \nby Es from the point of view of the procedure of %7%+1. Consider the case where ni and n,+r are nodes \ncorre-sponding to the execution of statements of the same proce-dure call pj. In this case, we know that \nni is an assignment statement defining variable v (since fresh variables created and defined by procedure \ncalls are assumed not to escape their scope). Since V E Vr(ni), Vr(ni) # 0, and hence ni E NI (Point \n3 and 4 of Step 2). Furthermore, there is an arc labeled with v from ni to nm+r in the define-use graph \nq of procedure pj (see definition of define-use graph). Con-sequently, 21 E fi(nm+i) (Point 3 of Step \n2). n Note that Step 2 of the algorithm simply gives one way to compute approximations of the sets vI[nl \n, . . n,,--11 O V(nm) using standard notions (i.e., define-use graph) for which algorithms already exist \nin the literature. Of course, other algorithms could be used, provided that they can be proved to compute \nsets VI(n) satisfying the previous lemma. As we will see in Theorem 6, the following definition precisely \ndefines the set of variables of the system S whose values are preserved by our algorithm. Definition \n2 Let c = so 2 si 2 s~...s,,-1 2 sm be an execution of the closed system S x ES. Then, Vs(ni . . . n,) \nis defined inductively as follows. 0 T/s(c)=I. Vs(?Ll... nm) is defined as: -if nnt corresponds to an \nassignment statement that defines variable v and fi(n,) = 0, Vs(nl . . . 72,) = Vs(nl . . . n,-1) U (71); \n- if n,,, corresponds to an assignment statement that defines variable v and Vr(n,) # 0, Vs(n1 . ..n.)=Vs(nl...n,-l)\\(v); \n- if n, corresponds to a procedure call and A de- notes the set of fresh variables corresponding to parameters \nthat are not removed in Point 2 of step 5 of the algorithm, vs(n1... n,) = Vs(nl . . . n,,pl) u {A}; \n- otherwise, Vs (nl . ..n.) = Vs(nl. ..nm-1). n Intuitively, Vs (ni . . . n,) reflects the accumulation \nof im- precisions due to the successive approximations of the sets Vr[nl... ni] n V(ni) by Vr(ni), for \nall 1 2 i 5 m. We have the following. Theorem 3 Let u = SO 2 s1 3 ~2.. .s,+.l 3 sm be an execution of \nthe closed system S x ES. Then, Vr[nl . . . n,]n VS((nl.. .n,) =0. Proof: Follows from Lemma 1 by an \ninduction on the length of executions. n When, for any execution cr = SO 1! sr 2 82.. . sm-1 2 sm of \nthe closed system S x Es, every variable defined in s, is either in Vr[ni . . . n,] or in Ks(ni . . . \nn,,,), the approxima-tions due to the sets Vr(ni), 1 5 i < m, are optimal. We will come back to this \npoint when we will discuss the precision of our algorithm at the end of this section. A property similar \nto Lemma 1 holds for VS. Lemma4 Let u = so 2 s1 1 sz...s,,+1 n4 s,,, be an execution of the closed system \nS x ES. Then, we have (V(n,) \\ Q(h)) C Vs(n1 . . . h-1). Proof: Let IJ be a variable in V(n,) that is \nnot in Vr(n,). Let us show that u E Vs(nl . . . nm-1). Since v $Z Vr(n,), we know v is not defined by \nEs. Hence, let mi denote the last node in ni . . . n, where v has been defined. Consider the case where \nni corresponds to the execution of a statement in another procedure call. If ni corresponds to an assignment \nstatement, then Vr(ni) = 0, otherwise r~ would be consid- ered as defined by the environment from the \npoint of view of the procedure of n,. Else ni corresponds to a higher-level procedure call (fresh variables \ncreated by lower-level proce-dure calls are assumed not to escape their scope), and u is a besh variable \ncorresponding to a parameter not removed in Point 2 of Step 5 of the algorithm, since v would be oth- \nerwise considered as defined by the environment from the point of view of the procedure of n,. Therefore, \nby Defini- tion 2, u was added to Vs (ni . . . n;) in ni. Consider now the other case where ni and nm \nare nodes corresponding to the execution of statements in the same procedure call. Then, ni corresponds \nto an assignment statement (since fresh vari- ables created and defined by procedure calls are assumed \nnot to escape their scope), and there is a define-use arc from ni to nm. labeled with v (see definition \nof define-use graph); this implies that Vr(ni) = 0, since otherwise, by Point 3 of Step 2 of the algorithm, \nVr(nm) would contain v. Therefore, by Definition 2, v was added to Vs(nl . . . ni) in ni. Since ni is \nthe last node in ni . . . n,,, where v has been de- fined during the execution of ni . . . n,, v is in \nI!.s(ni . . . n,). w We now prove that the system S obtained after the transformation is closed. Lemma \n5 For any node n in the control-flour graph Gi of any procedure p[i generated by the algorithm of Figure \n1, we have Vr(n ) = 0. Proof: Each node n of a control-flow graph G$ either (1) is a conditional statement \ntesting the value returned after a call to VS-toss, introduced in Step 4 of the algorithm, or (2) corresponds \nto a marked node n of the original control-flow graph Gj. For nodes of type (l), the lemma trivially \nholds. Let us now consider nodes of type (2). By Step 3 of the algorithm, we know that the corresponding \nnode n of Gj is associated with either (2.1) a start node, (2.2) a termination statement, (2.3) a procedure \ncall, or (2.4) an assignment or conditional statement for which VI(n) = 0. Since start nodes and nodes \ncorresponding terminal statements do not use any variables, we have V(n) = 0 and hence VI(n) = 0, i.e., \nthe lemma holds for subcases (2.1) and (2.2). By Step 5 of the algorithm, references to variables in \nVI(n) are eliminated. Therefore, the lemma holds for subcases (2.3) and (2.4). n Since we have proved \nthat the system S obtained after the transformation performed by our algorithm is closed and hence self-executable, \nwe can now consider its executions and relate them precisely to the executions of S in conjunc- tion \nwith ES. Specifically, the following theorem establishes a formal correspondence between computations \nof both systems. Let ?r denote a finite sequence of nodes non1 . . . n&#38; executed by a single process \nsuch that no is a node marked in Step 3 of the algorithm, and, for all 1 5 j 5 1, nj is unmarked. A computation \nu = so 2 sr 2 92.. . s,,+i 2 s,,, of a closed system is then defined as the finite sequence of stores \nsi the system goes through while executing the sequences ni of nodes. In what follows, all the nodes \nin Gg that are added by Point 1 of Step 4 of the algorithm are considered as marked in Gi, while all \nthe other nodes of Gj are considered as unmarked. This implies that there is a one-to-one corre-spondence \nbetween the marked nodes n and n of Gj and Gi respectively. We write n = n to denote this correspon-dence. \nBy construction (see algorithm), the statements asso-ciated with corresponding marked nodes are either \nidentical or are both procedure calls that may differ only by their arguments. We write 2) E s to denote \na variable that is defined in store s. Let s(w) denote the value of variable w defined in store 9. Let \n7r denote the first node no of the sequence of nodes ?r = nonI.. . nk. We write 7r4 E 7r y to mean that \nX: = 7r p and that these nodes are executed by cor-responding processes P and P of S and S respectively. \nLet nezt(s) denote the set of marked nodes next to be exe- cuted from a store s in a computation. Since \nthere is exactly one such node per process (which may possibly be blocking), the size of nezt(s) is constant \nand equal to the number of processes in the system. We write nezt(si) E nezt(s:) to mean that tin E next(si) \n: 3n E nezt(si) : n E n and Vn E next(si) : 3n E neiCt(Si) : n' Z 12. We are now ready to state the main \ntheorem that defines the correctness of our algorithm. Theorem 6 Let S be an open system that satisfies \nall the assumptions previously defined and implemented by a set of procedures pj. Let S be the system \nimplemented by the set of procedures pi obtained by transforming each procedure pj using the algorithm \nof Figure 1. Then, for every computa- tion CT = SO 3 s1 2 ~2.. . sm-l zG sm of S x Es, there 4 4 , exists \na computation u = sb + si -+ sa . . . s;-1 ,-s:, of S such that, VO 5 i < m, the three following properties \nare satisfied: 1. if i > 0, 7rF E 7r p; 8. neCit(S;) I nect(si); and 3. vv E VS(7rl . . .Ti) 1Si(V) = \nS:(V). Proof: The proof is by induction on the length of com-putations. For computations of length zero, \nthe closed sys- tems S x Es and S are in their initial global states SO and sb respectively, and Property \n1 is vacuously true. The set nezt(s0) contains the start node of the top-level procedure executed by \neach process P in P of the system S. Since start nodes are preserved by the algorithm (Point 1 of Step \n3), we have nezt(so) z nezt(sb). Since initially, we assume no variables have been defined by the system \nyet, we have Vs(e) = 0, and Property 3 of the theorem trivially holds. Let us consider a computation \nCT,,, of S x Es of length m and the corresponding computation a; of S obtained by the induction hypothesis. \nLet us show that, for any pos-sible computation cm+1 of length m + 1 extending c,,, by a sequence Am+1 \n= nOnI.. . 7&#38;k of nodes, there exists a se- quence x1+1 = r&#38;n:. . . n:, of nodes extending a&#38; \nto a computation &#38;+I such that &#38;+I G IT IO??%+I,next(s,+l) z next(&#38;+,) and Vu E Vs(?r~. . \n.~,,,+l) : s,+l(v) = sk+1(v). Let P denote the process of S which executes the se-quence of nodes non1 \n. . . nk. Let P be the process of S corresponding to P. Since no is a node of some procedure pj that \nis marked by Step 3 of the algorithm, let nb be the corresponding node of G$ such that no = nb. Since \nurn+1 is a computation, no is not blocking in s,,, . Since no E nezt(s,), it follows from the inductive \nhypothesis that nb E next(&#38;). If no is not an operation on a communication object, then neither is \nnb and hence nb is also not blocking. Otherwise, we know, by assumption, that the enabledness of any \nop-eration on any communication object depends exclusively on the sequence of operations that has been \nperformed on the object in the history of the system, and not on the val- ues that are possibly stored \nor passed through the object, or passed as an argument to the operation. Since all op-erations on communication \nobjects are procedure calls, the corresponding nodes are marked in the calling procedures and in their \ntransformed versions. Therefore, from the in-ductive hypothesis, it is easy to show that the projections \nof (Jo and u; onto nodes corresponding to operations on any given communication object are identical. \nSince no = nb and no is not blocking in sm, nb is not blocking in s ,. Thus, IO we have 7rL+1 g 7~n&#38;+1. \nBy definition of a computation, we know that the next node to be executed by P from sm+l is a marked \nnode, let us denote it by nk+l. Hence, we have nezt(s,+l) = (nezt(s,) \\ {no}) U {nk+l}. Let n;+l be the \nmarked node for S corre,sponding to the, node yk+l. In order to prove :k;; ;~~;);m~$ =Ce=$sn.) \\ {no)) \nu {ni+d =d hence 111 m+l), we have to show that there is an execution of process P from &#38; that leads \nto n;+, without traversing any marked nodes other than nb. Two cases are possible: either no and nk+] \ncorrespond to the executions of statements of the same procedure call pj, or they do not. We consider \nthese two cases successively. If no and nk+l are nodes of procedure pj, then nb and nL+1 are nodes of \nprocedure pi. This also means that the marked node no corresponds to either a start node, or an assignment \nstatement, or a conditional statement, and that Vr(no) = 0. Therefore, there is exactly one arc a from \nno in Gj whose label evaluates to true in sm. This arc leads to node nl. Let succ(a) denote the set of \nmarked nodes of Gj that axe reachable from no by a sequence aw of control-flow arcs in Gj passing through \nunmarked nodes exclusively and starting with arc a. Since nk+l E succ(a), Isucc(a)I > 1. Therefore, by \nPoint 2 of Step 4 of the algo-rithm, there is an arc a from nb in Gi that is labeled with the boolean \nexpression labeling arc a. Since Vr(no) = 0, we know by Lemma 4 that V(no) E VS(X~. . . n,). There-fore, \nby applying Property 3 of the inductive hypothesis, we have VW E VS(~FI . ..n.) : So = s&#38;(v), and \nthus Vv E V(n0) : s,,, (IJ) = S;(U). By definition of a control-flow graph, all variables occurring in \nthe boolean expression labeling arc a are in V(no). Thus, if the boolean expression labeling arc a evaluates \nto true in s,,, , then the same boolean expression, which labels arc a , also evaluates to true in sk. \nSince this boolean expression labels arc a in G$ , the execu- tion of node nb from sk leads to the successor \nnode of arc a in Gi, let us call it n: . If Isucc(a)I = 1, by Point 2.2 of Step 4 of the algo-rithm, \nn: is the node corresponding to n&#38;l, i.e., n;+l, which concludes the proof of this case. Otherwise, \nwe have Isucc(a)I > 1. By Point 2.3 of Step 4 of the algorithm, node n: corresponds to a test on the \nvalue returned by a call to the nondeterministic function VS-toss, and there is an arc from n; to nL+l \nin Gg. Since boolean expressions testing the value returned by a call to VS-toss introduced in Step 4 \naI- ways evaluate to true in any store, by taking K*+I = r&#38;n\\, we define an execution of process \nP from &#38; that leads to 4+1. We now consider the case where no and nk+l correspond to the executions \nof statements of two different procedure calls pj and pt. (Note that, in case of a recursive procedure \ncall, pt executes the same code as pj.) This implies that nb ad nk+1 also correspond to the executions \nof statements of two different procedure calls pi and pi. Two cases are possible: no corresponds either \nto a procedure call, or to a termination statement. If no is a procedure call, nk+r is the start node \nof the procedure pt being called. Since proce-dure calls and start nodes are marked nodes, the execution \nof nb leads directly to nk+l, which completes the proof of Property 2 for this case. Otherwise, no is \na termination statement. Since no is not blocking in sm, pj is not the top-level procedure call of this \nprocess, and procedure pt is the procedure that called pj. Thus nr must be the successor node in Gl of \nthe node of Gl that called pj (by construction, every node corresponding to a procedure call has always \nex-actly one successor node in a control-flow graph). Let a be the arc from this node to nr in GI. The \nrest of the proof for this case is identical to the case analysis on Isucc(o)I done in the previous paragraph. \nThis concludes the proof of Property 2. We now prove Property 3. Recall that the environment cannot define \nany variables previously defined by the system and hence any variables in Vs((nr . . . w,,,). The key \nobserva- tion to prove Property 3 is then that, by Definition 2, for any node n that does not correspond \nto an assignment state-ment with VI(n) = 0, or does not correspond to a procedure call with a nonempty \nset A of fresh variables correspond-ing to parameters not removed in Point 2 of step 5 of the algorithm, \nwe have Vs(n~. . . A~) _> Vs(nl . . . ?r,no). Let s and s denote the stores reached by processes P and \nP after the execution of nodes no and nb from stores s,,, and s&#38;, respectively. If node ne satisfies \nthe conditions stated in the previous paragraph, we immediately have Vu E vs (m . . . 7rmno) : s(v) = \ns (v). If the marked node no corresponds to an assignment statement that defines a variable ve, we know \nno $ NI (Point 4 of Step 3 of the algorithm), and Vl(no) = 0. By Lemma 4, this means that V(no) C VS(AI \n. . . n,). There-fore, by applying Property 3 of the inductive hypothesis, we have Vv E Y.s(wr . ..r.,,) \n: S,,,(V) = s:(v), and thus vv E V(n0) : sm(v) = &#38;(v). This implies that no and r$, performs identical \nstore transformations. Therefore, UJ is defined both in s and s with the same value. Moreover, by Definition \n2, V.s(?rr . . . IT,,,~o) = Vs(nl . . . T~)u{vo}. We thus have Vv E V,(wr . . . 7rmn0) : s(v) = s (v). \nConsider the case where the marked node nc, corresponds to a procedure call with a nonempty set A of \nfresh variables corresponding to parameters not removed in Point 2 of step 5 of the algorithin. Let A \nbe the set of variables whose val- ues are passed as argument via such parameters, and hence copied into \nsome fresh variable in A. Hence, we have A E V(no) and A n R(no) = 0. By Lemma 4, we know (V(no) \\ Vr(n0)) \nE Vs(7r1...7rm). Therefore, A s Vs(nl . . .rr,,,). Therefore, by applying Property 3 of the inductive \nhypoth-esis, we have VII E Vs(z1.. . ?F~) : s,,,(v) = s:,(v), and thus VIJ E A : sm(v) = s&#38;(v). This \nimplies that Vv E A : s(v) = s (v). By Definition 2, vS(?rr . . .~,,,no) = Vs(nl,. .n,) u {A}. We thus \nhave again Vu E Vs(nl . ..r~,,,no) : s(v) = s (v). By repeating the same argument for the remaining nodes \nin X~+I (if any), the proof of Property 3 is complete. n Intuitively, the previous theorem states that, \nfor every com-putation u of S x Es, the projection of u (including call stacks of processes) onto the \nset of variables that do not use variables defined by or functionally dependent on Es is pre- served \nby the transformation performed by our algorithm. Note that the original correctness criterion used in \nThe- orem 6 combines aspects of both reactive and functional program semantics: Points 1 and 2 of the \ntheorem estab-lishes a simulation relation between S x Es and S , while Point 3 of the theorem establishes \na functional equivalence for a subset of the values computed by S x Es and S . From Theorem 6, it is \nthen easy to show that deadlocks and assertion violations that do not test expressions involv-ing values \nprovided by the environment Es are preserved by the transformation performed by the algorithm. Precisely, \nrecall that assertions are visible operations, and hence pro- cedure calls from the point of view of \nour algorithm. To be consistent with our previous assumption that only variables can be passed as arguments \nto procedure calls, we assume every assertion in S has exactly one argument, which is a variable whose \nvalue determines whether or not the asser-tion is violated. We say that an assertion that corresponds \nto some (by construction, marked) node n in S is preserved in S if the variable passed as argument in \nn is not elimi-nated by Point 2 of Step 5 of the algorithm of Figure 1 in the corresponding node n in \nS . We then have the following. Theorem 7 Let S and S be defined as in Theorem 6. Let AsXE= denote the \nstate space of the closed system S x Es obtained by combining S with its most general environment Es, \nand let Asi denote the state space of the closed system S . Then, all the deadlocks in AsXEd we in As,. \nMoreover, for all the assertions in procedures pj preserved in pi, if there exists a global state in \nAsx~s where such un assertion is violated, then there exists u global state in As, where the same assertion \nis violated. Proof: Consider a deadlock s in Asx~s. By definition, a deadlock is a reachable global state \nin &#38;x&#38; where ail the processes are blocked. Let u be a computation of S x Es that leads to the \ndeadlock. By applying Theorem 6, we know that there exists a computation u of S that leads to a global \nstate s such that nezt(s) s nezt(s ). Since the exe- cutions of all the nodes in nezt(s) are blocking, \nall the nodes in nezt(s) attempt to perform a visible operation. Since the enabledness of any operation \non any communication object depends exclusively on the sequence of operations that has been performed \non the object in the history of the system, and since this history is the same in u and u for all commu-nication \nobjects by Property 1 of Theorem 6, the executions of all the nodes in nezt(s ) are also blocking, and \ns is a deadlock in As,. Let us now consider the case of assertion violations. Let s be a global state \ns in A.s XE= where an assertion is violated. This means that there is a node n in nest(s) corresponding \nto this assertion which tests the value of a variable v. Hence, v E V(n). Since the assertion is violated, \nthe value of II in s is false . Let u be a computation of S x Es that leads to s. By applying Theorem \n6, we know that there exists a computation u executing ~1 . , .x,,, of S that leads to a global state \ns such that nezt(s) E nezt(s ). Let n be the node in nezt(s ) such that n s n . Since the assertion is \npreserved in S , we know that VI(n) = 0, and that n also tests the value of variable w. By Lemma4, VI(~) \n= 0 implies that V(n) s Vs(nl . . . ?r,). Since v 6 V(n), by Property 3 of Theorem 6, we conclude that \ns(v) = s (v). Hence, the assertion is also violated in s in Ag. W Note that the transformation from S \nto S eliminates some program statements from S, yet it must preserve all possi- ble behaviors of S as \nspecified above. One must be careful to ensure that when the transformation removes program statements \nfrom S that may lead to run-time errors, these errors cannot generate extra executions in S (that S would \nthus not preserve). Therefore, we make the following obser-vations about the run-time errors in the original \nsystem S. If the semantics of the source language specifies the behav-ior of a particular run-time error, \nthat behavior needs to be preserved in S if it may lead to additional executions in S. Otherwise, for \nall errors whose behavior is not specified by the source-language semantics, but rather left to the psrtic- \nular implementation of the language, S does not need to preserve any particular choice of behavior. In \nother words, the correctness of the transformation is defined relative to the source language itself \nrather than a particular imple-mentation of the source language. For example, C does not specify the \nbehavior of run-time errors such as array-out- of-bounds, and so the transformation algorithm for C pro- \ngrams may tieely remove array references when appropriate. In contrast, an array-out-of-bounds error \nin an ML program throws an exception, and so S needs to preserve any bounds check whose exception may \nlead to additional executions in S. An optimal translation of an open system S is a closed system S&#38;,, \nsuch that the properties in Theorem 6 addition- ally hold in the other direction. In other words, for \nevery computation of S&#38; there exists a computation in S x Es that satisfies the three properties \nin the theorem. For ex-ample, consider Figure 3 and consider a system S that com-prises only the procedure \nq(x). Then the algorithm performs an optimal translation as explained in the text that accom-panies that \nexample. In contrast, the translation in Figure 2 is not optimal. Of course, it is not possible in general \nto achieve an op- timal translation. There are several sources of conservative approximation in our algorithm. \nInterprocedural issues: The algorithm assumes that it is known which nodes in a control-flow graph of \nproce- dure p use the value of a variable defined by the envi- ronment Es. The source of this information \nmay be manual, in the form of a specification, or automatic, in the form of an interprocedural analysis \non top of our intraprocedural analysis. If manual, then it is only possible to achieve an optimal specification \nif the en-tire open system S is known in advance; otherwise, it will be necessary to assume conservatively \nthat any in- put variables or variables whose addresses escape are defined by the environment. If automatic, \nthen the in- terprocedural analysis will necessarily be approximate in general due to well-known sources \nof imprecision such as escaping variables, the call-context problem, and so forth. Dataflow analysis: \nThere are the standard imprecisions associated with a classic dataflow analysis. For in-stance, any may-alias \nanalysis may generate spurious dependencies. Furthermore, composing define-use arcs is imprecise. For \nexample, the code a=x+l; b=a-x; will report incorrectly that b is dependent upon x. Lemma 1 covers this \nsource of imprecision. Finite variance: In a given run of a system, some execu- tions of a given node \nmay be functionally dependent on ES and others not. Our algorithm conservatively re-moves such nodes. \nIn brief, any dataflow analysis must be of finite variance and thus may suffer this kind of imprecision. \nOur analysis is monovariant, and the cor-responding imprecision is summarized in Theorem 3. Temporal \nindependence: In a given run of a system, there may be multiple executions of a given conditional node \nthat are all functionally dependent on Es in precisely the same way. For instance, consider Figure 2. \nThe open program encounters the conditional test 10 times per call to p, but goes down the same branch \neach time. The particular branch is dependent on Es. The trans- lated closed program performs 10 VSfoss \noperations rather than a single one before the loop. In this case, hoisting the conditional test y=O \noutside the loop in p would have eliminated this imprecision. In general, however, it is unavoidable. \nNote that the above discussion concerns only the precision of our algorithm with respect to the possible \nsets of com-putations satisfying Theorem 6. One can also discuss the optimality of the branching structure \nof the generated pro-gram. For instance, sequences of VSfoss that result in the same sequences of marked \nnodes are redundant, and could thus be eliminated. This line of thought will not be discussed further \nhere. 6 Applications The motivation behind this work is the desire to analyze au-tomatically very large \nconcurrent reactive systems for which reliability is critical. One such system is Lucent Technolo-gies \n5ESS telephone switching system [MS85], which pro-vides telecommunications services for land-line and \nwireless networks. The 5ESS software consists of thousands of in- teracting concurrent reactive processes, \nand is comprised of millions of lines of code, mostly written in the C program- ming language. The software \nis continually evolving as new features are added. The sheer size, complexity, and changing nature of \nthe code renders it extremely difficult to understand the possible interactions between the processes \nin the switch. Such inter- actions are often extremely hard to reproduce and analyze in the existing \ntesting environments available in the 5ESS development organization. These include an on-line simula-tion \nenvironment, which can execute a complete version of the 5ESS software, and testing labs where the 5ESS \nsoftware can be executed on actual 5ESS hardware switches. A de- veloper using either of these testing \nenvironments must first set a significant number of data configurations before being able to run a test. \nNon-determinism among concurrent pro-cesses is implicitly resolved by the execution environment. Therefore, \nit can be difficult to reproduce a specific scenario. Recently, we have started investigating in collaboration \nwith a group of 5ESS developers how the techniques in-troduced in this paper combined with VeriSoft could \nbe used for providing a new lightweight testing and reverse-engineering platform for reactive properties \nof 5ESS code. As a case study, we considered a large multi-process 5ESS application that is responsible \nfor providing call processing features -such as originations, terminations, location regis-tration, hand \nover, roaming, and ca,ll forwarding -for spe-cific wireless systems. This call processing software describes \nabout, 10 main families of concurrent reactive processes. The code describing each family of processes \nranges from approx-imately 30,000 to several hundred thousand lines of C code. In order to be able to \nexecute this code for analyzing its dynamic behavior without using the existing heavyweight execution \nenvironments, we first needed a way to make the application %t,and-alone . Therefore, we implemented \nthe algorithm described in this paper in a prototype tool for automatically closing open programs written \nin the C pro- gramming language. We manually developed software stubs for providing a small number of \ninputs corresponding to ba- sic external events we wanted to control in order to trig-ger, and observe \nafterwards, interactions between concur-rent processes of the application. The remainder of the sys- \ntem was closed automatically using our tool. At the time of this writing, VeriSoft is currently being \nused to analyze the dynamic behavior of the closed application. Note that com-pletely closing this application \nby hand is clearly impractical because it would require developing and maintaining code for simulating \na substantial portion of the entire 5ESS switch software and databases. 7 Related Work and Conclusions \nThe core of our algorithm for closing an open system is a dataflow analysis of C or C-like procedures. \nThe dataflow analysis is phrased as a graph-reachability problem. This is a common approach to dataflow \nproblems that our al-gorithm shares with many others, such as [Cal88, CK88, Kou77], to name but a few. \nIn particular, we use stan-dard techniques for computing define-use dependencies, such as [ASUSS, FOW87, \nMR90]. These techniques rely on a (conservative) solution to the aliasing problem. Examples of alias \nanalyses include [CWZSO, Deu94, Lan91, Ruf95, SRW96], to name a few of many. Using graph-reachability-based \ndataflow analyses to drive transformations of imperative programs is a well established technique. For \ninstance, [HR92] describes transformations based on the Program Dependence Graph [FOW87, KKL+81]. Perhaps \nthe most common such transformation is program slicing, originally introduced by Weiser in [WeiSl] and \nlater much investigated and extended (e.g., [HRB88, Tip951). The input to a typical slicing tool is a \nprogram, a point, p within the program, and an identifier 3. The output is a possibly reduced program \nthat preserves the trace of values bound to 2 at p. Note that a correct (albeit useless) slicing algo-rithm \nis the identity transformation. Our transformation is different in that it mvst eliminate some parts \nof the original program -namely, the parts that depend on values sup-plied by the environment. Our transformation \nmay seem similar to a simultaneous slice on each identifier at, each program point that does not depend \non a value supplied by the environment the environment, where those identifiers are pre-computed by a \nforward analysis. However, such a slice would be too large. For instance, consider the exam-ple programs \nin Figures 2 and 3. In each case, the trace of cnt values depends on the conditional test involving y; \nhence, the above slicing procedure would not eliminate the the nodes involving y, and the resulting program \nwould re-main open. To close the program, we must eliminate all nodes that depend on a value provided \nby the environment. Therefore, in contrast to slicing, we require only inclusion rather than equivalence \nof executions, and we are forced to introduce VSfoss in order to achieve in a single transforma- tion \nboth inclusion of executions and complete elimination of nodes dependent, upon the environment. Combining \nan open system with its most general envi-ronment is related to the idea of hiding a set of visible actions \nof a process [Hoa85, Mi189] in a process calculus. The orig-inality and technical contributions of our \nwork are to apply this idea to full-fledged programming languages, instead of simple transition systems. \nSystematically exploring the state space of the imple-mentation of a concurrent system written in a programming \nlanguage such as C, rather than constructing and/or analyz-ing an approximate model of that implementation, \nis a new approach to concurrent program analysis. This work con-tinues the line of research set forth \nin [God97]. A comple-mentary approach to analyzing such systems is to use static analysis, such as abstract \ninterpretation [CC77]. Most work has involved analyzing the communication patterns that oc-cur in a system \n(Tay83, LC91, MR93, Co195, Cri95, Ven97]. A model checker could analyze the results of such static anal-yses \nin order to prove the absence of certain specific types of errors. In contrast, our approach is based \non dynamic observation of a system. This opens up the possibility of detecting a wider range of behaviors \nthat may have been abstracted away by a static analysis. In this paper, we have shown that this kind \nof dynamic analysis cm be used even on open systems by first applying a static analysis -not to construct \na model of the system, but rather to transform it to a closed form. Our algorithm is a first solution \nto the general problem of closing an open system. There is evidence to suggest, that this algorithm can \nbe applied to large pieces of code. The algorithm also has the significant practical benefit that it \ncan close any open system completely automatically. It is also applicable to open sequential systems, \ni.e., systems comprising only a single process. The experience that we have gained in developing this \ntransformation algorithm has shed some light on possible ways to improve the precision of the result,. \nConsider, for instance, a resource-management system that receives (via its open interface) 32-bit integers \nrepresenting amounts of time requested from the resource, but whose visible behav-ior only depends on \nwhich of a small set, of ranges each re- quest falls into. Our transformation would completely elim-inate \nthe open interface in order to avoid the intractability that arises from the systematic exploration of \nall possible inputs. However, one could hope for a static analysis that would determine the appropriate \npartitioning of the input domain, and, if it is small enough, simplify the interface in-stead of eliminating \nit,. Consider further that the original system contains a control path along which are two con-ditional \ntests that both depend on an input, say the time request, but always evaluate to the same value. The \ncur-rent algorithm inserts a VSfoss operation at both points. A static analysis that could detect this \nproperty could cut, the possible branching of the state-space exploration in half. Both of these examples \nme special cases of the general prob- lem of symbolically analyzing the behavior of an imperative program \nwith respect to unknown values (the open inter-face). We are investigating the applicability of an existing \nsymbolic analysis [Co1961 to this problem. References (ASUBG] A. Aho, R. Sethi, and J. UIlman. Compilers: \nPrinci- [CalSS] [CC771 [CES86] [CKSS] [Co1951 [Co1961 [CPSSS] [CriSB] (CWZ90] [Deu94] [FOW87] [God961 \n[God971 [Hoa85] [Ho1911 pies, Techniques and Tools. Addison-Wesley, 1986. D. Callahan. The program summary \ngraph and flow- sensitive interprocedural data flow analysis. In Pm-ceedings of the ACM 88 Conference \non Programming Lanugage Design and Implementation, pages 47-56, Atlanta, GE, USA, June 1988. ACM Press. \nP. Cousot and R. Cousot. Abstract interpretation: A unified lattice model for static analysis of programs \nby construction of approximation of fixed points. In Proceedings of the 4th ACM Symposium on Principles \nof Programming Languages, Los Angeles, pages 238-252, New York, NY, 1977. ACM. E.M. Clarke, E.A. Emerson, \nand A.P. Sistla. Auto-matic verification of finite-state concurrent systems using temporal logic specifications. \nACM Trans- actions on Programming Languages and Systems, 8(2):244-263, January 1986. K. D. Cooper and \nK. Kennedy. Interprocedural side-effect analysis in linear time. In David S. Wise, edi-tor, Proceedings \nof Ihe SZGPLAN 88 Conference on Programming Lanugage Design and Implementation enGTLA8NAy&#38; gr 57-66, \nAtlanta, GE, USA, C. Colby. Analyzing the communication topology of concurrent programs. In Proceedings \nof the Sym-posium on Partial Evaluation and Semantics-Based Program Manipulation, pages 202-213, New \nYork, NY, USA, June 1995. ACM Press. C. Colby. Semantics-based Program Analysis via Symbolic Composition \nof Tmnsfer Relations. PhD thesis, Carnegie Mellon University, August 1996. R. Cleaveland, J. Parrow, \nand B. Steffen. The con-currency workbench: A semantics based tool for the verification of concurrent \nsystems. ACM Transactions on Programming Languages and Systems, 1(15):36-72, 1993. R. Cridlig. Semantic \nanalysis of shared-memory con-current languages using abstract model-checking. In Proceedings of the \nSymposium on Partial Evaluation and Semantics-Based Program Manipulation, pages 214-225, New York, NY, \nUSA, June 1995. ACM Press. D.R. Chase, M. Wegman, and F.K. Zadeck. Analysis of pointers and structures. \nIn Proceedings of Confer-ence on Programming Language Design and Zmple-mentation, pages 296-310, June \n1990. A. Deutsch. Interprocedural May-Alias analysis for pointers: Beyond k-limiting. SZGPLAN Notices, \n29(6):23&#38;241, June 1994. Proceedings of the ACM SZGPLAN 94 Conference on Programming Language Design \nand Implementation. J. Ferrante, K. J. Ottenstein, and J. D. Warren. The program dependence graph and \nits use in optimisa-tion. ACM Transactions on Pmgmmming Languages and Systems, 9(3):319-349, July 1987. \n P. Godefroid. Partial-Order Methods for the Ver-ification of Concurrent Systems -An Approach to the \nState-Explosion Problem, volume 1032 of Lecture Notes in Computer Science. Springer-Verlag, January 1996. \n P. Godefroid. Model Checking for Programming Lan-guages using VeriSoft. In Proceedings of the 24th ACM \nSymposium on Principles of Programming Lan-guages, pages 174-186, Paris, January 1997. C. A. R. Hoare. \nCommunicaling Sequential Processes. Prentice-Hall, 1985. G. J. Holemann. Design and Validation of Computer \nProtocols. Prentice Hall, 1991.  [HR92] [HRB88] [KKL+81] [Kou77] [LanSl] [LCSl] [McM93] [Mi189] [MR90] \n[MR93] [MS851 [QS811 [Ruf95] (SRW96] Pay831 [Tip951 [Ven97] [WeiSl] S. Horwitz and T. Reps. The use of \nprogram depen-dence graphs in software engineering. In Proceedings of the 14th International Conference \non Software En-gineering, pages 392-411, May 1992. S. Horwitz, T. Reps, and D. BinkIey. Interprocedural \nslicing using dependence graphs. In David S. Wise, editor, Proceedings of the ACM 88 Conference on Programming \nLanguage Design and Implementation, pages 35-46, Atlanta, GE, USA, June 1988. ACM Press. D. J. Kuck, \nR. H. Kuhn, B. Leasure, D. A. Padua, and M. Wolfe. Dependence graphs and compiler opti-mizations. In \nProceedings of the 8th ACM Symposium on Principles of Programming Languages, pages 207- 218, January \n1981. L. T. Kou. On live-dead analysis for global data flow problems. Journal of the ACM, 23(3):473-483, \nJuly 1977. W. Landi. Interprocedural aliasing in the presence of pointers. PhD thesis, Rutgers University, \n1991. D. L. Long and L. A. Clarke. Data flow analysis of concurrent systems that use the rendezvous model \nof synchronization. In Proceedings of ACM Symposium on Testing, Analysis, and verification (TAVd), pages \n21-35, Vancouver, October 1991. K. L. McMillan. Symbolic Model Checking. Kluwer Academic Publishers, \n1993. R. Milner. Communication and Concurrency. Pren-tice Hall, 1989. T. J. Marlowe and B. G. Ryder. \nProperties of data flow frameworks. Acla Znformatica, 28:121-163, 1990. S. P. Masticola and B. G. Ryder. \nNon-concurrency analysis. In Proceedings of Fourth ACM SZGPLAN Symposium on Principles EI Practice of \nPamllel pm-gmmming, pages 129-138, San Diego, May 1993. K.E. Martersteck and A.E. Spencer. Introduction \nto the 5ESS(TM) switching system. ATdT Technical Journal, 64(6 part 2):1305-1314, July-August 1985. J.P. \nQuielle and J. Sifakis. Specification and verifica- tion of concurrent systems in CESAR. In Proc. 5th \nZnt l Symp. on Programming, volume 137 of Lecture Notes in Computer Science, pages 337-351. Springer-Verlag, \n1981. E. Ruf. Context-insensitive alias analysis reconsid-ered. In Proceedings of Conference on Progmmming \nLanguage Design and Implementation, New York, NY, USA, June 1995. ACM Press. M. Sagiv, T. Reps, and \nR. Wilhelm. Solving shape-analysis problems in languages with destructive up-dating. In Proceedings of \nthe 29rd ACM Sympo-sium on Principles of Programming Languages, pages 16-31, St. Petersburg, Florida, \nJanuary 1996. ACM Press. R. N. Taylor. A general-purpose algorithm for ana-lyzing concurrent programs. \nComm&#38;entions of the ACM, pages 362-376, May 1983. F. Tip. Generic techniques for source-level debugging \nand dynamic program slicing. In Proceedings of !l P- SOFT 95. volume 915 of Lecture Notes in Computer \nScience. Springer-Verlag, 1995. A. Venet. Abstract interpretation of the n-calculus. In Mads Dam. editor. \nAnalvsis and Verification of Multiple-Ageni Lang&#38;ages (&#38;oceedings oj the Fifth LOMAPS Workshop), \nvolume 1192 of Lecture Notes in Computer Science, pages 51-75. Springer-Verlag, 1997. M. Weiser. Program \nslicing. In Proceedings of the 5th International Conference on Software Engineer-ing, March 1981.  NOTES \n \n\t\t\t", "proc_id": "277650", "abstract": "We study in this paper the problem of analyzing implementations of open systems --- systems in which only some of the components are present. We present an algorithm for automatically closing an open concurrent reactive system with its most general environment, i.e., the environment that can provide any input at any time to the system. The result is a nondeterministic closed (i.e., self-executable) system which can exhibit all the possible reactive behaviors of the original open system. These behaviors can then be analyzed using VeriSoft, an existing tool for systematically exploring the state spaces of closed systems composed of multiple (possibly nondeterministic) processes executing arbitrary code. We have implemented the techniques introduced in this paper in a prototype tool for automatically closing open programs written in the C programming language. We discuss preliminary experimental results obtained with a large telephone-switching software application developed at Lucent Technologies.", "authors": [{"name": "Christopher Colby", "author_profile_id": "81100165190", "affiliation": "Loyola University Chicago, Dept. of Math. and Comp. Sciences, 6525 N. Sheridan Rd., Chicago, IL", "person_id": "P47547", "email_address": "", "orcid_id": ""}, {"name": "Patrice Godefroid", "author_profile_id": "81100504535", "affiliation": "Bell Laboratories, Lucent Technologies, 1000 E. Warrenville Road, Naperville, IL", "person_id": "PP40027996", "email_address": "", "orcid_id": ""}, {"name": "Lalita Jategaonkar Jagadeesan", "author_profile_id": "81100214350", "affiliation": "Bell Laboratories, Lucent Technologies, 1000 E. Warrenville Road, Naperville, IL", "person_id": "P168203", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/277650.277754", "year": "1998", "article_id": "277754", "conference": "PLDI", "title": "Automatically closing open reactive programs", "url": "http://dl.acm.org/citation.cfm?id=277754"}