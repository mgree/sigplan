{"article_publication_date": "05-01-1998", "fulltext": "\n Proper Tail Recursion and Space Efficiency William D Clinger Northeastern University will@ccs.neu.edu \nAbstract The IEEE/ANSI standard for Scheme requires implementa-tions to be properly tail recursive. This \nensures that portable code can rely upon the space efficiency of continuation-pass- ing style and other \nidioms. On its face, proper tail recursion concerns the efficiency of procedure calls that occur within \na tail context. When examined closely, proper tail recur-sion also depends upon the fact that garbage \ncollection can be asymptotically more space-efficient than Algol-like stack allocation. Proper tail recursion \nis not the same as ad hoc tail call optimization in stack-based languages. Proper tail recursion often \nprecludes stack allocation of variables, but yields a well-defined asymptotic space complexity that can \nbe relied upon by portable programs. This paper offers a formal and implementation-indepen- dent definition \nof proper tail recursion for Scheme. It also shows how an entire family of reference implementations \ncan be used to characterize related safe-for-space properties, and proves the asymptotic inequalities \nthat hold between them. Introduction Tail recursion is a phrase that has been used to refer to var- ious \nsyntactic notions, to particular techniques for imple-menting syntactic tail recursion, and to the space \nefficiency of those techniques. Syntactically, a call is a tail call if it appears within a function \nbody that can reduce to the call; this is formalized in Section 2. Since the complete call graph is seldom \navailable, a tail call is often said to be tail recur--siwe regardless of whether it occurs within a \ncycle in the call graph. Scheme, Standard ML, and several other mostly-function-al languages rely heavily \non the efficiency of tail recursion. Common idioms, notably continuation-passing style (CPS), would quickly \nrun out of stack space if tail calls were to consume space. To ensure that portable code can rely upon \nthese idioms, the IEEE standard for Scheme [IEESl] says Implementations of Scheme are required to be \nproperly tail-recursive [Ste78]. This allows the a ,996 ACM 0-69791~967-4/96/0006...55.00 E ..-..- (quote \nc> constants I variable references L lambda expressions (if EO El E2) conditional expressions I (set! \nI Eo) assignments (Eo El . ..I procedure calls L ..-..- (lambda (11 . . . > E) TRUE 1 FALSE 1 NUM:z \n1 SYM:I v$E&#38;3;. .) I * * * Location Identifier Figure 1: Internal syntax of Core Scheme. execution \nof an iterative computation in constant space, even if the iterative computation is de- scribed by a \nsyntactically recursive procedure. The standard s citation refers to a technical report that uses CPS-conversion \nto explain what proper tail recursion meant in the context of the first Scheme compiler [Ste78]. That \nexplanation is formally precise, but it is not entirely clear how it applies to an implementation that \nuses a different al-gorithm for CPS-conversion or does not use CPS-conversion at all. Most attempts to \ncharacterize proper tail recursion in a truly implementation-independent way have been more informal \n[Ste78]: Intuitively, function calls do not push control stack ; instead, it is argument evaluation which \npushes control stack. Using a style of definition proposed by Morrisett and Harper [MH97], this paper \ndefines a set of asymptotic space complexity classes that characterize proper tail recursion and several \nrelated safe-for-space complexity properties. Al-though these complexity classes are defined in terms \nof spe- cific reference implementations, they can be used without depending upon the details or even \nthe existence of imple- mentation-dependent data structures such as stacks or heaps. This provides a \nsolid foundation for reasoning about the asymptotic space complexity of Scheme programs, and also provides \nimplementors with a formal basis for determin-ing whether potential optimizations are safe with respect \nto proper tail recursion. Figure 2: Static frequency of tail calls. These numbers were lFH95. CH941. \nThe self-tail calls shown for Scheme include all iecogn ize self&#38;l calls as a special case. See also \nSection 14. 2 Tail Calls Figure 1 shows an internal syntax for the core of Scheme. The external syntax \nof full Scheme can be converted into this internal syntax by expanding macros and by replacing vector, \nstring, and list constants by references to constant storage. Deflnition 1 The tail expressions of a \nprogram written in Core Scheme are defined inductively as follows. 1. The body of a lambda expression \nis a tail expression. 2. If (if Eo El Ez) is a tail expression, then both El and E2 are tail expressions. \n 3. Nothing else is a tail expression.  [KCR98] extends this definition to the syntax of full Scheme. \nDefinition 2 A tail call is a tail expression that is a proce- dure call. Figure 2 shows that tail calls \nare much more common than the special case of self-tail calls, in which a procedure calls itself tail \nrecursively. 3 The Essence of Proper Tail Recursion The essence of proper tail recursion is that a procedure \ncan return by performing a tail call to any procedure, including itself. This is a kind of dual to the \ninformal characterization quoted in Section 1. A tail call does not cause an immediate return, but passes \nthe responsibility for returning from the procedure that performs the tail call to the procedure it is \ncalling. In other words, the activation of a procedure extends from the time that it is called to the \ntime that it performs either a return or a tail call. This is fundamentally different from the traditional \nview of procedure calls, in which the activation of a procedure encompasses the activations of all procedures \nthat it calls. 4 An Example Figure 3 shows a procedure definition that contains three tail calls, of \nwhich the last is a self-tail call. Given a predicate, a binary tree, and a failure continuation of no \narguments, find-leftmost searches for the leftmost leaf that satisfies the predicate. If such a leaf \nis found, then it is returned normally. Otherwise the procedure returns by performing a tail call to \nthe failure continuation or to itself. obtained by instrumenting two compilers: ICC and Twobit tail calls \nto known closures, because Twobit has no reason to (define (find-leftmost predicate? tree fail) (if (leaf? \ntree) (if (predicate? tree) tree ; return (fail)) ; tail call (let ((continuation (lambda 0 (find-leftmost \n; tail call predicate? (right-child tree) fail>>>> (find-leftmost predicate? ; tail call (left-child \ntree) continuation>>>> Figure 3: An example with three tail calls. Although find-leftmost uses an explicit \nfailure continu-ation, it is not a pure example of continuation-passing style, because its fourth line \nreturns tree to the implicit continu-ation. Returning is equivalent to performing an implicit tail call \nto the implicit continuation. In Scheme, it is perfectly feasible to write large programs in which no \nprocedure ever returns, and all calls are tail call~.~ This is pure continuation-passing style. Proper \ntail recursion guarantees that implementations will use only a bounded amount of storage to implement \nall of the calls that are performed by a program written in this style. To make this precise, we need \na proper model of space consumption. This model should allow us to reason about the space needed to run \na program, independent of imple- mentation. For this to be tractable, the space model should take the \nform of an asymptotic upper bound on the space consumed by an implementation. Proper tail recursion constrains \nbut does not determine the space model. To obtain a complete model, we must also model the space consumed \nby variables and data, and spec- ify the roots that a garbage collector would use to determine whether \na variable or datum is reachable. Garbage collec-tion then completes the model. With a reasonable garbage \ncollector, the asymptotic space required for variables and data is O(N), where N is the largest number \nof words oc-cupied by reachable variables and data at any point in the program [App92]. For example, \na Scheme programmer can tell that the space required by find-leftmost is independent of the num- Some \ncompilers do this routinely, using CPS Scheme as their tsr-get language. ber of right edges in the tree, \nand is proportional to the maximal number of left edges that occur within any directed path from the \nroot of the tree to a leaf. If every left child is a leaf, then find-leftmost runs in constant space, \nno matter how large the tree. 5 Retention versus Deletion A deletion strategy reclaims storage at statically \ndetermined points in the program, whereas a retention strategy retains storage until it is no longer \nneeded, as determined by dy- namic means such as garbage collection [Fis72]. Algol-like stack allocation \nis the most important deletion strategy. By allowing the lifetime of a variable or value to extend beyond \nthe lifetime of the block in which it was declared or created, retention strategies support more flexible \nprogram-ming styles. It is less well-known that retention strategies can also reclaim storage sooner \nthan a deletion strategy, and can have better asymptotic space efficiency. This is crucial for proper \ntail recursion. Deletion strategies interfere with proper tail recursion [Ste78, Cha88, App92, ASwS96]. \nFor the example of Section 4, allocating continuation on a stack would require O(n) space instead of \nO(1) space, even when every left child is a leaf. Nevertheless many compilers for Scheme, Standard ML, \nand similar languages employ optimizations that allocate some variables on a stack [Ste78, KKsR+86, HanSO, \nSF96, Ser97, Sis97]. Some researchers have gone so far as to sug- gest that a static deletion strategy \ncould be used to replace dynamic garbage collection altogether, for some programs at least [TT94, AFL95, \nSis97]. In Scheme, stack allocation and other deletion strategies can be used only when they do not destroy \nthe property of proper tail recursion. This is not always easy to determine [Cha88, SF96]. It has not \nbeen made easier by the informal- ity with which proper tail recursion has been defined. It is fairly \neasy to define proper tail recursion formally for a particular implementation, but an implementation-independent \nformalization cannot refer to implementation-dependent structures such as a stack. Standard ML of New \nJersey, for example, allocates continuation frames on a heap, and does not distinguish them from closures \nthat are allo-cated for explicit lambda expressions [App92]. Algol-like im-plementations use a single \nstack to represent three distinct kinds of runtime structure: continuations, environments, and the store. \nThe distinction between storage allocated for continuations, environments, and the store is not always \nclear even to the implementor. 6 Forinal Definitions The largest number of machine words that a (possibly \nnon-deterministic) implementation X might consume when run-ning a program P on an input D can be described \nby a (de- terministic) function SX such that Sx(P, D) E R U {oo}, where R stands for the real numbers. \nI will refer to SX as the space consumption function of X. To compare the asymptotic behavior of such \nfunctions, I will use big-0 no-tation as defined below. For real-valued functions of the natural numbers, \nthis definition is equivalent to the usual definition as given in [Knu73, CLRSO]. Definition 3 (asymptotic \ncomplexity, O(f)) Zf A is any set, and f : A --t RU {co}, then the asymptotic (upper bound) complexity \nclass off is O(f), which is defined Figure 4: Syntax of configurations. Cotiguration ::= (v, a) I (E, \nP, K:, 4 I b,P,K,d v E Value ::= c UNSPECIFIED I UNDEFLNED PRIMOP$ ESCAPE:(a$) CLOSURE:(+, p) I(, ::= \nhalt select:(El, &#38;, p, n) I assign:(I, p, K) I push:((E,. ..), h.. .),T,p,d I call:((E,. . .), K) \nP E Identifier 3 Location I7 E Location 2 Value ?r E Permutation as the set of all functions g : A -+ \nW U {oo} for which there exist real constants cl and co such that cl > 0 and Vu E A . g(a) 5 cl f (a) \n+ co Each space consumption function Sx induces an asymp- totic space complexity class O(Sx). Sections \n7 through 10 describe a family of reference implementations that formal-ize several important models \nof space efficiency. Their space consumption functions Sstack, &#38;ail, Sevlis, and S,fs are de- fined \nin Section 12. Their induced complexity classes are related by the proper inclusions O(Ssfs) C O(SevId \nC O(Stail> C O(Sstack) Definition 4 (conventional space leaks) An implemen- tation has a conventional \nspace leak iff its space consumption is not in O(Set&#38;). An implementation has no conventional space \nleaks iff its space consumption is in O(Ssta&#38;. Definition 5 (properly tail recursive) An implementa- \ntion is properly tail recursive iff its space consumption is in O(St&#38;. An implementation has a stack-like \nspace leak iff it hers no conventional space leaks but is not properly tail recursive. An implementation \nwith a stack-like space leak is also known aa improperly tail recursive. Definition 6 (evlis tail recursive, \nsafe for space) An implementation is evlis tail recursive [WangO, Que96] iff its space wnsumption is \nin O(Sevlis). An implementation is safe for space complexity in the sense of Appel [App92] i# its space \nconsumption is in 0(&#38;f,). Among these properties of an implementation, proper tail recursion is \nespecially important because it is easy to construct programs that should run in small constant space \nbut are likely to require at least linear space in implemen- tations that are not properly tail recursive. \n Reduction rules: ((quote C),P,K,U) + (1, Pl %a) + if I E Dom p, p(l) E Dom o, and o@(l)) #UNDEFINED \n(CLOSURE:(Q, L, p), p, K, u[a I+ urwmcwmD]) if a does not occur within L, p, K, u ((if Eo El Ed,p,tc,a) \n+ (Eo,p,select:(&#38;, E~,P,K),u)(bet! I Eo),p,lE,u) + (Eo, p, asskn:(I, p, K), 0) (Wo El *. .),p,K,u) \n+ (G,p,push:((E:,. .A 0,~,~,4,4 if (Eb,Ei,...) = reverse(?r- (Eo, El,. . .)) Continuation rules: b, 0, \nhalt,4 + b>(J) b, P , halt, 0) + (v, { 1, halt, 0) (v,p , select:(El, E2, p, K), 0)  + if v # FALSE \n b, P , asskn:(I, P, K), 4 --f (~NsPEcIFIED,~, K,&#38;(I)-v]) (~~,P ,push:((E:,E~,...),(v:,...),A,p,KE),u) \n+ (E;,p,push:((E;,...),(v~,v:,...),a,p,KE),u) (~:,,p ,push:(O,(~:,...),~,P,K),u) --f (vo,p,call:((vl,. \n. .),K),u) if (vo,vl,...) =7r(wh,w: ,...) (CLOSURE:@, L,p),p , call:((vl,. . . ,v,), IE),U) + (E,P ,K,Q \n)ifL= (lambda (11 . ..> E) andpI, . . . . fin do not occur within E, p, K, u andp =p[Il,..., Intip ,..., \nfin] andu =u[/&#38; ,..., &#38;-WI ,..., V,J Garbage collection rule: (~,P,K.,U1o,...I--1~ ,...]) --t \n(%P,IE,4 if {/I,. . . } is nonempty and p, . . . do not occur within v, p, K, u Figure 5: Properly tail \nrecursive semantics. 7 Proper Tail Recursion (Gail) To expose its space requirements, while retaining \nthe sim-plicity needed for proofs, the properly tail recursive ref-erence implementation is expressed \nas a small-step oper-ational semantics of the kind known as a CEKS machine [NN92, MFH%]. The asymptotic \nspace required by a refer- ence implementation must be at least as great as the space required by any \nreasonable implementation of Scheme, in-cluding pure interpreters, so this implementation is reso-lutely \nnon-clever. It is also nondeterministic, to reflect ram-pant underspecification in the definition of \nScheme. A configuration of this semantics, as shown in Figure 4, is either a final configuration consisting \nof a value and a store, or an intermediate configuration consisting of an expression or value, an environment, \na continuation, and a store. An intermediate configuration represents the state of an abstract machine \nwith three registers-accumulator, en-vironment (static link), continuation (dynamic link)-and a garbage-collected \nheap. When the first component of a configuration is an expression, the accumulator acts as a program \ncounter. When the first component is a value, the continuation register acts as the program counter. \nThe ini-tial configurations are described in Section 11, which also defines the observable answer represented \nby a final config-uration. The core transition rules are shown in Figure 5. These core rules must be \nsupplemented by additional rules, mainly for primitive procedures, which are not specified in this pa-per. \nThe six reduction rules say that: l A quoted datum evaluates to the datum. l An identifier evaluates \nto its R-value; if I e Dom p, p(l) $Z Dom u, or u(p(1)) = UNDEFINED, then the transition rule cannot \nbe applied, and the computation will be stucL l A lambda expression evaluates to a closure. A bug in \nthe design of Scheme requires that a location (Y be allocated to tag the closure [Ram94]. l A conditional \nexpression is evaluated by evaluating the test with a continuation that will select one of the al- ternatives. \nl The right-hand side of an assignment is evaluated with a continuation that will store its R-value into \nthe L-value of the left-hand side. l A procedure call is evaluated by nondeterministically choosing a \npermutation ?r of its operator and operand expressions; the first of the permuted expressions is then \nevaluated with a continuation that will evaluate the others and then perform the call. Other sources \nof nondeterminism include the choice of loca- tions to allocate when a closure is called, and the choice \nof whether and when to use the garbage collection rule. The conceptual difference between proper tail \nrecursion and ad hoc tail call optimization permeates this semantics. Proper tail recursion affects every \nrule shown in Figure 5 except for the first three reduction rules and the first two continuation rules. \nIn a properly tail recursive implementation, continua-tions are created to evaluate subexpressions, not \nto ca]] a procedure [Ste78]. A procedure call is just a goto that changes the environment register. Notice \nthat every call is a goto, not just tail calls. Hence the last continuation rule, which shows how closures \nare called, does not create a new continuation. In particular, a procedure call does not create or pass \na new return address, push the environment p , or allocate a stack frame. The continuations that are \ntagged by select, assign, and push include an environment, which is restored when the continuation is \ninvoked. This allows the test part of a conditional, the right hand side of an assignment, and an operator \nor operand expression to destroy the environment by performing a procedure call. The garbage collection \nrule allows unreachable storage to be recycled.2 If there exists a nonempty set of locations that are \nnot reachable via the active store from the loca- tions mentioned by V, p, and IE, then those locations \nmay be removed from the active store and become available for future steps of the computation. The garbage \ncollection rule allows but does not require garbage collection. To prevent improper tail recursion from \nbeing masked by uncollected garbage, we must require the garbage collection rule to be used sufficiently \noften. In Sec- tion 12 we will require the garbage collection rule to be used whenever garbage remains \nto be collected. In a real imple- mentation the garbage collector would run much less often, but would \nuse no more than some fixed constant R times the space required when collecting after every computation \nstep ([App92], Section 12.4). Usually R 5 3. 8 Improper Tail Recursion (Zgc, GtacJ An improperly tail \nrecursive reference implementation Zgc is obtained by replacing the last continuation rule in Figure \n5 by (CLOSURE:((Y, L,p), p', call:((vr,. . . ,21,), IE),O) -+ (E,p ,return:(p ,~),u ) if L = (lambda \n(11 . . . I,,) E) mdP1, . . . . ,&#38; do not occur in E, p, K, u andp =p[li ,..., I,,c,Pi ,..., Pn] \nandu =upr ,..., /3,,eur ,..., u,J and by adding the continuation rule By creating a continuation for \nevery procedure call, these rules waste space for no reason [Ste78]. They would look a lot less silly \nif they implemented a deletion strategy for local variables. In Scheme, however, a deletion strategy \ncan create dangling pointers. As mentioned in Section 4, most Scheme compilers nonetheless use a deletion \nstrategy as an optimization for variables whose lifetimes can be statically bounded. To see what would \nhappen if such optimizations were pursued without regard for their effect on tail recursion, be the semantics \nobtained by replacing the last let %tack continuation rule in Figure 5 by (CLOSURE:(CY, L,p), p , call:((m,. \n. . ,?I,), K),u) + (E, p , return:(A, p , IC), a ) if L = (lambda (11 . . . 1,) El andPI, . . . . /3,, \ndo not occur in E, p, K, u dp\"=p[Il,..., Ine-tpl,..., /3n] andu =upi ,..., p,,e,i ,..., V,,] and AC {Pl,...rPn} \n No sequence of reduction steps allocates more than one storage location, so oo garbage collection rule \nis needed for configurations whose first cornpooerlt is ao expression. (v, p, return:(A, p , IE), a ) \n(CEO El . . .),P,K,U) --f b,P',~,4 -+ (E~,p,push:((E:,...),O,w,~ ,K),u) if no p E A occurs within v, \np , K, u if (Eh, Ei,. . .) = reverse(rr-l(Eo, El,. . .)) and u = u ] (Dom u \\ A) and p = p 1 (Dom p n \n(FV(E:) u . . .)) The nondeterministic choice of A subsumes any static anal-ysis for an optimization \nthat allocates variables on a stack, provided the optimization does not create dangling point-ers, and \ndoes not extend the lifetime of a garbage variable beyond that of Algol-like stack allocation. For the \nAlgol-like subset of Scheme, it is always possible to choose A = {PI,... ,&#38;}, which results in the \nspace re-quired by Algol-like stack allocation of variables. This choice of A always consumes the most \nspace, so it determines the space consumption S s,,&#38; on such programs. Hence Sstad also characterizes \nthe space consumed by Algol-like imple-mentations. 9 Evlis Tail Recursion (Ze,lis) Although &#38;ail \nuses less space than Zgc or &#38;tad, it too wastes space needlessly. For example, it is not necessary \nto preserve the environment across the evaluation of the last subexpression to be evaluated during the \nevaluation of a procedure call. Even a pure interpreter can take advan-tage of this technique, which \nis known as e&#38;s tail recursion [Wan80, Que96]. Let { } denote an empty environment, and let Gvlis \nbe the semantics obtained from &#38;ail by replacing the first continuation rule for push by the two \nrules I I (~~o,P,PUS~:((E:,E~,E~ ,... ),(v:,...),x,P,K),~ --f (E;,p,push:((E~,E~,...),(vb,v:,...),~,p,K),u) \n(~:,~ ,push:((E~),(v:,...),~,~,~),u) + (E~,p,push:(O,(v;,v;,...),?r,O,K),u) 10 Safe for Space Complexity \n(Z&#38;,. Z&#38;J In the implementations that have been described so far, a lambda expression is closed \nover all variables that are in scope, regardless of whether those variables actually occur free within \nthe lambda expression. This is typical of in- terpretive and Algol-like implementations, but it sometimes \nforces programmers to write awkward code to avoid space leaks. Compiled implementations often close over \nonly the free variables, which improves the space complexity of some programs. Let Zfree be the semantics \nobtained from Z&#38;l by replac- ing the reduction rule for lambda expressions by (L,P,h4 -+ (CLOSURE:(CY, \nL,p ),p, K, U[(Y i--k UNSPECIFIED]) if cx does not occur within L, K, p, u and p = p 1 (Dom p n FV(L)) \nLet Zsfs be the semantics obtained from &#38;J by replacing the reduction rule for lambda expressions \nas above, and by replacing the last three reduction rules and the next-to-last continuation rule by ((if \nEO EI Ed,p,~,u) + (Eo,p,select:(E1,Ez,p , ),u) if p = p 1 (Dom p fl (FV(&#38;) U FV(E2))) ((set! I Eo),p,~,u) \n+ (Eo,p,assign:(l,p ,~),O) ifp =p 1 {I}  (vb,~',~ush:((E:,E;,...),(v;,...),~,~,~E),o) --$ (E;,p,push:((E;,...),(v~,v;,...),~,p',KE),u) \nif p = p 1 (Dom p II (FV(Eh) U . . .)) The asymptotic space efficiency of Zsfs represents safe-for-space \ncomplexity in the sense defined by Appel [AppSP, ASSG], whereas the asymptotic space efficiency of Z&#38;e \nrep-resents a weaker but still useful sense of safe-for-space. 11 Equivalence of Implementations This \nsection proves that all of the reference implementations compute the same answers. Since some of the \nimplemen-tations use more space than others, this result requires a countably infinite set of locations \nLocation = {cri ( i 2 0). Lemma 7 If C is a configuration of &#38;,twkt and C is the configuration of \nZgc obtained by replacing all continuations of the form return:(A,p,K) by return:(p,K), and C +* (71, \nu), then C +* (v, u). Proof: By induction on the number of computation steps. The locations that occur \nwithin C are a subset of those that occur within C, so all of the nondeterministic choices that are made \nin the original computation can also be made by the computation in Zgc. cl The next two lemmas are proved \nsimilarly. Lemma 8 If C is a configuration of Z&#38;, and C is the con- figuration of I&#38;l obtained \nby replacing all continuations of the form return:(p,~) by K, and C -+* (w,u), then C +* (v,u). Lemma \n9 If C +* (~,a) by the rules of &#38;I, then C +* (v, u) by the rules of Ze,lis. Definition 10 (initial \nconfigurations) An initial config-uration is a configuration of the form (E,po, halt, 00) such that l \nZf cy occurs in E, po, or UO, then (Y E Dom 60. l If an identifier Z occurs free in E, then Z E Dom po. \nl Zf (Y occurs in E, then a $! Ran po. Definition 11 (answers) The observable answer represented by a \nfinal configuration (v, a) is the possibly infinite sequence of output tokens answer(v,u) where answer(v, \na) ans(v, u, EOF) itnS(TRUE, U,S) #t, ans(s, u,EOF) anS(FALSE, u, s) #f, ans(s, u,EOF) ans(NUM:z, (T, \ns) z, ans(s, (r,ECF) ans(SYM:~, 6, S) I,ans(s, u,EOF) anS(VEC:(oe, . . .),a, S) #(,~S((U(cu),...),U,S) \nanS(ESCAPE:(o, K), 0, S) #<PROC>, an+, u,EOF) ans(CLOSURE:(cy, L, p), u, s) #<PROC>, ans(s, u,EOF) am((), \nu, s) = 1, ans(s, u, EOF) am((z1o,m,. . .),u,s) = =s(vo,u, ((211,. . $5)) ans(EOF,store,s) = ;End of \noutput Lemma 12 If  wstack 1 l X is l&#38;j1 and Y is Zfree, or Wgc) 0 X is Ze,ljs and Y is Z&#38;f,, \nor l X is Ii&#38;,, and Y is qfs, and C +* (v,u) by the rules of X, then there exist v and o such that \nanswer(v, u) = answer(v , 0 ) and C +* (~ ,a ) by the rules of Y. Proof: At each step of the computation, \nthe analogous rule of Y can be used with the same nondeterministic choices, be- cause the only differences \nbetween two corresponding con- figurations are that some of the environments that occur in the configuration \nof Y have been restricted to the free vari- ables of the expression(s) that will be evaluated using that \nenvironment. The final configurations differ only in the en- vironment components of closures and of \ncontinuations that have been captured as part of an escape procedure; these differences are not observable. \n0 Definition 13 (a-convertible) Configurations Cl and CZ are cr-convertible, written Cl g CZ, iff there \nexists a one-to- one function R : Location + Location such that CZ is the configuration obtained from \nCl by renaming its locations according to R. The following lemma shows that a-convertible configura- \ntions are semantically equivalent. Lemma 14 Zf C g C , and C +* (~,a), then there exist v and U such \nthat answer(v,c) = answer(v ,o ) and C +* (v ,u ). Sketch of proof: None of the rules treat any location \nspecially. This must be verified for the primitive operations as well, whose rules are omitted from this \npaper. 0 The next few definitions and lemmas establish the ex-istence of two canonical forms for computations: \nwithout garbage collection, and with maximal garbage collection. Definition 15 (gc-transform of a configuration) \nIf C is a configuration, then its gc-transform is D defined as follows. l If C is a final configuration \n(~,a), then D = (v,o ) where (v, { }, halt, 0 ) is the gc-transform of b,O,halt,d. l If the garbage collection \nrule does not apply to C, and C is not a final configuration, then D = C. l Otherwise D is the unique \nconfiguration such that C --t D by the garbage collection rule, and the garbage wl-k&#38;ion rule does \nnot apply to D. Lemma 16 Zf C + C , and D and D are the gc-transforms of C and c , then D = D , OT D \n--f D , OT there exists a configuration D (unique up to a-convertibility) such that D+D +D . Definition \n17 (gc-transform of a computation) Zf co + Cl + *. . is a computation, and for each i the gc- transform \nof Ci is Di, then the gc-transform of the wmputa- tion is the computation obtained from the sequence \nDo, D1, . . , as follows. If Di = Di+l, then D~+I is erased from the se- quence. If Di + Di+l, then D; \nand D;+l remain adjacent. If Di + D + Di+l, then D is inserted into the sequence between Di and D+l. \n o(stail) o(sevlis) o(sfree) I> w%fd Figure 6: A hierarchy of space complexity classes. Theorem 18 \n(canonical forms) If Co + Cl --t *-a is any computation, then its gc-transform is (pointwise) cr-convertible \nto the gc-transform of a computation that never uses the garbage collection rule. Theorem 19 If an answer \ncan be computed from some ini- tial configuration by the rules of&#38;f,, then it can be computed from \nthat configuration by the rules of Z&#38;.k. Proof: Given a computation in Z&#38;s, Theorem 18 can be \nused to lift it to a structurally similar computation that does not use the garbage collection rule. \nThis gc-free computa-tion can be transformed into a computation in &#38;ail by using p instead of p at \neach use of a rule for which Zsfs differs from ail. The resulting computation can then be transformed \nz,into a computation in Zgc by adding return continuations at each use of the call rule, and by inserting \nuses of the re-turn rule into the computation. The resulting computation can be transformed into a computation \nin &#38;tack by adding an empty set to each return continuation. 0 Corollary 20 All of the reference \nimplementations wm- pute the same answers. 12 Space Consumption This section gives a formal definition \nof the space required by a configuration, and defines functions S&#38;l, Sgc, S&#38;a&#38;, Se,,l+, Sfree, \nand Ssfs that characterize the space required to run a program on an input as the worst case among all \nexecution sequences that have a certain property. These functions induce the hierarchy of space complexity \nclasses that is shown in Figure 6. To define the space consumed by an implementation, I will take inputs \nto be expressions, and programs to be ex-pressions that evaluate to a procedure of one argument. The \nconstants of a program P must not share storage with the constants of an input D, nor may P or D share \nstorage with the standard library. The easiest way to ensure this is to forbid vector, string, and list \nconstants. This entails no loss of generality, because all such constants can be replaced by references \nto global variables that are initialized using the standard library procedures that allocate new vectors, \nstrings, and lists. Let po and 00 be some fixed initial environment and ini- tial store that contain \nScheme s standard procedures, as de- scribed in Section 6 of [IEESl]. Let Program and Input both denote \nthe set of Core Scheme expressions that contain no locations, and whose free variables are bound in ~0. \nspace((v, u)) space((E,p,K,u)) space((v, p, K, a)) space(u) spaCe(TRUE) = SpXe(FALSE) = space(SYM:I) \nspace(vEc:(crg, . . . , on--l)) SpXe(NUM:z)  space(cLosuRE:(cu, L,p)) space(halt) space(select:(Ei, \nEz, p, K)) space(assign:(I, p, K)) space(push:((&#38;,. . . , J-L), (VI,. . . ,vn),7r,p,~)) space(call:((vi,...,v,),K)) \nspace(return:(p, K)) = = = = = = = = = = = = = = space(return:(A, p, K)) = space(v) + space(u) [Dam \npi +space(K) +space(u) space(v) + (Dom pi + space(K) + space(u) C (1 + space(u(cr))) QEU 1 1 + n 1 \n+ log, z if z is an exact positive integer [IEESl] l+lDompl 1 1 + [Dam pi + space(K) 1 + [Dam pi + space(&#38;) \nl+m+n+lDompl+space(K) l+m+space(~) 1 + [Dam pi + space(&#38;) 1 + [Dam pi + space(K) Figure 7: The space \nconsumed by a configuration (using flat environments). The space consumed by a configuration is defined \nin Fig- ure 7. This definition corresponds to the use of flat (copied) environments, because the purpose \nof this definition is to es- tablish upper bounds, not lower bounds. For a comparison of flat environments \nwith linked (shared) environments, see Section 13. Definition 21 A space-efficient computation in Zx \nis a fi- nite OT countably infinite sequence of wnjigurations {Ci}ier such that . C0 is an initial configuration. \nb If the sequence is finite, then it ends with a final con-figuration C, . . For each i E I, if i > 0 \nthen C.&#38;-I + Ci by the rules of zx. . If the garbage collection rule is applicable to Ci, then Ci \n+ Ci+1 by the garbage collection rule. This definition eliminates incomplete or stuck computations from \nconsideration. A stuck computation represents gram error or, in the case of Zeta&#38;, a stack allocation \ncreates a dangling pointer. Definition 22 (supremum) Zf R C Sz, then the mum sup R is the least upper \nbound of R, OT 00 if is no upper bound in 8. a pro- that supre-there Definition 23 (space consumption \nSx) The space con- sumption function of Zx is Sx : Program x Input + 32 U {w} defined by sxU ,D) = PI \n+ sup{sup{space(Ci) ( i E I} I {Ci}i,I is a space-eficient computation in 2x, with Co = (W D),po,halt,uo)} \n where (PI is the number of nodes in the abstract syntax tree of P. Theorem 24 FOT all P E Program and \nD E Input, l stail(P,D) I sgc(P,D) 5 Sstak(P,D) l ssfs(P, D) 5 sevlis(P, D) 5 sttil(P, D) l ssfs(p, D) \nI sfree(p, D) I sttil (p, D) ProoE To prove Sx(P, D) 5 Sy(P, D), consider an arbi- trary space-efficient \ncomputatron in 1,. This computation is pointwise o-convertible to the gc-transform of a compu- tation \nthat does not use the garbage collection rule. This gc-free computation can be lifted to Zy as in the \nproof of Theorem 19. The gc-transform of this lifted computation is a space-efficient computation in \nZY that is equivalent to the original computation in 2x, and consumes at least as much space. cl Theorem \n25 All set inclusions shown in Figure 6 are pro-per, and O(Sev&#38; and O(Sf,,,) are inwmparab6e. Proof: \nTo show O(Sy) g O(Sx), it suffices to give an example of a program P such that AN. Sy(P, (quote NJ) $! \nO(XN. Sx(P, (quote N))) For readability, I will write each program in full Scheme as a procedure definition. \nExcept for the program that distin-guishes Stail from Sgc, these programs consume quadratic space in \none implementation but only linear in the other. (In Scheme the linear cause of unlimited with fixed \nprecision 1-0 show 0(&#38;t&#38;) programs are actually O(Nlog, N) be-precision arithmetic, but would \nbe O(N) arithmetic.) $i o(sgc): (define (f n) (let ((v (make-vector n>>> (if (zero? n) 0 (f (-II 1))))) \n To show O(Sgc) g O(St,jl): (define (f n> (if (zero? n) 0 (f (-n 1)))) TO show O(Sttil) !Z O(Sevd~ 0(&#38;e) \nSZ O(sevd, Wfme) Sz O&#38;f,): (define (f n> (define (g) (begin (f (-n 1)) (lambda 0 n))) (let ((v (make-vector \nII))) (if (zero? n> 0 ((g)>))> To show W%l) Gz O(Sf*tXA WG,lis) It O(Sfr,,), WWli,) 52 O(Ssfs): (define \n(f n> (let ((v (make-vector n) )) (if (zero? n) 0 ((lambda 0 (begin (f (-n 1)) n>>)>>> u 13 Fiat versus \nLinked Environments A definition of space consumption that corresponds to linked environments can be \nobtained by counting each binding (of an identifier I to a location o) only once per configuration, regardless \nof how many environments contain that binding. Recall that p is a finite function, so it can be viewed \nas a subset of Identifier x Location. Let graph(p) be that set of ordered pairs. Figure 8 then defines \nthe space consumed by a configuration using linked environments. The space consumed by an implementation \nZX that uses linked environments can then be defined in terms of the space consumption function Ux that \ndiffers from Sx by us-ing Figure 8 instead of Figure 7 in Definition 23. It is easy to see that analogues \nof Theorems 24 and 25 hold for linked environments, and that Ux 5 S X for each implementation TX* In \nthis sense, linked environments are more space-efficient than flat environments. Unfortunately, Z&#38;e, \nand Zsfs can- not always use linked environments; in general, they require flat environments. Hence Ufm, \nand Vaf, have no practical meaning. The asymptotic relationships that hold between and Sef, therefore \nbecome a question of some practical interest. u tail, uevlis~ Sfreer Theorem 26 Both O(Utail) and O(Ue,&#38; \nare inwmpara- ble with both O(Sfree) and O(S,.,). Proof: Appel [App92] has given examples that show o( \nevlis) SZ o(Sfree) Since Uevlis C UtGl and Appel s examples Ssfs = Sfree, establish half of the theorem. \nThe other half of the theorem is established by using an example to show O(S,f,) g O(Ut,i1). For each \nnatural number k, let Ee,k be (let ((x0 n)> (define (loop i thunks) (if (zero? i) ((list-ref thunks (random \n(length thunks)))) (loop (-i 1) (cons (lambda 0 (list i x0 xl . . . xk)) thuas)))) (loop n JO>> For \neach j > 0 let Ej,k E (let ((xi (-n j>>> Ej-1.k). Let pk 2 (define (f n) &#38;,k). Then XIV. Uf&#38;l(PN, \n(quote N)) E O(NlOg, N) and would be linear with fixed precision arithmetic, but AN. S,f,(PN, (quote \nNJ) E O(N2) 0 This theorem reveals an important difference between the formulation of space complexity \nin Section 6 of this pa- per and the formulation used by Appel in [AppSP, AS96]. Appel s formulation \nallows the constants of Definition 3 to be chosen separately for each program. This effectively ig-nores \nthe extra space that is consumed by flat environments and closures, as well as the space consumed by \na large class of compiler optimizations such as inlining and loop unrolling. As formulated here, the \nspace safety properties allow any bounded increase in space due to inlining, loop unrolling, shared closures, \nand similar techniques, but the bound must be independent of the source program. Appel s formulation \nhas led to a perception that flat closures (over free variables only) are asymptotically more space-efficient \nthan linked closures (that close over all vari- ables in scope); as formulated here, however, the asymptotic \nspace complexities of flat and linked closures are incompa-rable. This is consistent with the intuition \nof implemen- tors who have argued that it does not make sense to im-pose stringent safe for space complexity \nrequirements with-out also bounding the increases in space consumption that result from a compiler s \ntransformation of the program. 14 Sanity Check The idea of defining proper tail recursion as a space \ncomplex- ity class is new, so it is intensionally not the same as informal notions of proper tail recursion. \nExtensionally, however, it should coincide with the consensus of the Scheme and Stan- dard ML programming \ncommunities concerning which im-plementations are properly tail recursive. To the best of my knowledge, \nmost implementations of Scheme and Standard ML are properly tail recursive by the formal definition of \nthis paper. Some implementations are not, for one of the following reasons: space leaks caused by bugs. \nspace leaks caused by conservative garbage collection (Boe93J. space leaks caused by over-aggressive \nstack allocation or other optimizations. a target architecture that makes proper tail recursion difficult, \nhence slow, so the implementors have deliber- ately sacrificed proper tail recursion for speed and/or \ncompatibility. space(CLOSURE:(Q, L, p)) space(balt) space(select:(E1, Ez,p, K)) bindings(cLosURE:(cr, \nL,p)) bindings(halt) bindings(select:(Ei, Ez, p, 6)) bindings(assign:(I, p, tc)) bindings(push:((Ei,. \n. . , Em), (VI,. . . ,~),r,p, K)) bindings(call:((vi,. . . ,nm), K)) bindings(return:(p, K)) bindings(return:(A, \np, K)) = = = = = = = = = = = = = = space(v) + space(a) ]graph(p) U bindings(n)] + space(K) + space(u) \n space(v) + ]graph(p) U bindings(K)] + space(K) + space(a) 1 1 1 + space(K) gwhb) 0 graph(p) U bindings(K) \ngraph(p) U bindings(K) graph(p) U bindings(K) bindings(K) graph(p) U bindings(K) graph(p) U bindings(K) \n Figure 8: The space consumed by a configuration (using linked environments). For example, Larceny v0.29 \n[CHSB] sometimes retains a temporary for too long within a stack frame. As with conservative garbage \ncollection, few users would notice this bug, but it causes rare failures of proper tail recursion. On \nmost target architectures, for languages like Scheme that require garbage collection anyway, proper tail \nrecursion is considerably faster than improper tail recursion. Unfortu-nately, the standard calling conventions \nof some machines3 make proper tail recursion difficult or inefficient, so the im- plementor must choose \nbetween l improper tail recursion, or l proper tail recursion using a nonstandard and possibly slower \ncalling convention. This dilemma arises most often when the target architecture is ANSI C. As explained \nby the Bigloo user s manual [Ser97]: Bigloo produces C files. C code uses the C stack, so some programs \ncan t be properly tail recur-sive. Nevertheless all simple tail recursions are compiled without stack \nconsumption. Thus Bigloo and similar implementations fail with contin-uation-passing style and with the \nfind-leftmost example of Section 4, but most tail calls to known procedures con-sume no space. Implementations \nof this kind often assume that the global variable defined by a top-level procedure def-inition is never \nassigned, which increases the percentage of calls to known procedures beyond that shown in the last column \nof Figure 2. 3Examples include the SPARC and PowerPC. Their standard call-ing conventions pass some arguments \nin a stack frame created by the caller. If the callee performs a tail call to a procedure whose argu-ments \nrequire more space than wes allocated in the caller s frame, then the callee must either allocate a new \nframe, which causes im-proper tail recursion, or must increase the size of its caller s frame, which \nis impossible on the PowerPC and slow on the SPARC. Both architectures perform very well in properly \ntail recursive impletnen-tations that use slightly nonstandard calling conventions. One of the standard \ntechniques for generating properly tail recursive C code is to allocate stack frames for all calls, but \nto perform periodic garbage collection of stack frames as well as heap nodes [BakSB]. A definition of \nproper tail re-cursion that is based on asymptotic space complexity allows this technique. To my knowledge, \nno other formal defini-tions do. 15 Previous Work Scheme was invented by Steele and Sussman for the purpose \nof understanding Hewitt s actor model, which made heavy use of continuation-passing style and was among \nthe first attempts to formalize proper tail recursion [SS75, Hew77]. Proper tail recursion was then popularized \nby Steele, Suss-man, and others [SS75, WF78, SS76, Ste76, Ste77, Ste78, SS78b, SS78a, ASwSSG]. Proper \ntail recursion is one of the safety properties con-sidered by Chase in his analysis of optimizations \nthat can in- crease the asymptotic space complexity of programs [Cha88]. The close connection between \nproper tail recursion, garbage collection, and asymptotic space complexity was pointed out by Appel in \nChapter 12 of [App92]. Most discussions of proper tail recursion have been infor- mal, but there have \nbeen several formal definitions within the context of a particular implementation [Ste78, Cli84, App92, \nFSDF93, Ram97]. My definition is essentially the same as a definition pro-posed by Morrisett and Harper \n[MH97]. My treatment of garbage collection is closely related to that of Morrisett, Felleisen, and Harper \n[MFH95]. 16 Future Work The reference implementations described here can be related to the denotational \nsemantics of Scheme by proving that every answer that is computed by the denotational semantics is computed \nby the reference implementations. 183 It should be possible to prove that implementations such as MacScheme \nand VLISP are properly tail recursive [Cli84, Lig90, GW95]. These proofs should be much easier than proofs \nof their correctness, and might not be much more difficult than the proofs in Sections 11 and 12. It \nshould also be possible to formulate and to prove the soundness of a formal system for reasoning about \nthe space complexity of programs written in Scheme, Standard ML, and similar languages. 17 Conclusions \nThe space efficiency that is required of properly tail recursive implementations can be stated in a precise \nand implemen-tation-independent manner. Other kinds of safe-for-space properties can be defined in similar \nfashion. The space complexity classes that are defined in this paper justify formal, implementation-independent \nreasoning about the space required by programs. Three of the space complexity classes defined in this \npaper (Sstad, Stail, and Ssfs) correspond to the models that working programmers already use to reason \nabout the space required by Algol-like, Scheme, and Standard ML programs respectively. These complexity \nclasses can be used to classify space leaks created by some optimizations. Algol-like stack alloca-tion \ncan itself be viewed as an optimization that usually de-livers a small constant improvement over the \nspace required by a garbage collector, while risking asymptotic increases in the space required to run \nsome programs. Acknowledgements This paper owes much to electronic communications with Henry Baker, Hans \nBoehm, Olivier Danvy, Matthias Fellei-sen, Greg Morrisett, Kent, Pitman, John Ramsdell, and Jef- frey \nMark Siskind, but the mistakes are mine. References [AFL951 A. Aiken, M. F&#38;ndrich, and R. Levien. \nBet-ter static memory management: Improving region-based analysis of higher-order languages. In 1995 \nConference on Programming Language Design and Implementation, pages 174-185, San Diego, California, June \n1995. [App921 Andrew W. Appel. Compiling with Continua-tions. Cambridge University Press, 1992. (AS961 \nAndrew W. Appel and Zhong Shao. An empiri- cal and analytic study of stack vs. heap cost for languages \nwith closures. Journal of Functional Programming, 6(1):47-74, 1996. [ASwS96] Hal Abelson and Gerald Jay \nSussman with Julie Sussman. Structure and Interpretation of Computer Programs. MIT Press, Cambridge, \nMA, second edition, 1996. [Bak95] Henry Baker. Cons should not cons its arguments, part, II: Cheney on \nthe M.T.A. ACM SZGPLAN Notices, 30(9):17-20, Septem-ber 1995. [Boe93] Hans-Juergen Boehm. Space-efficient \nconserva-tive garbage collection. In PFOC. 1993 ACM Conference on Programming Language Design and Implementation, \npages 197-206, 1993. [CH94] [Cha88] [Cli84] [CLRSO] [FH95] [Fis72] [FSDF93] [GW95] [HanSO] [Hew771 [IEESl] \n[KCR98] [KKsR+86] William D. Clinger and Lars Thomas Hansen. Lambda, the ultimate label, or a simple \nop-timizing compiler for Scheme. In Proc. 1994 ACM Symposium on Lisp and Functional PFO-gramming, pages \n128-139, 1994. David R. Chase. Safety considerations for stor- age allocation optimizations. In PFOC. \n1988 ACM Conference on Programming Language Design and Implementation, pages l-10, 1988. William Clinger. \nThe Scheme 311 compiler: an exercise in denotational semantics. In PFOC. 1984 ACM Symposium on Lisp and \nFunctional Programming, pages 356-364, August 1984. Thomas H. Cormen, Charles E. Leiserson, and Ronald \nL. Rive&#38; Introduction to Algorithms. MIT Press/McGraw-Hill, 1990. Christopher W. Fraser and David \nR. Hanson. A Retargetable C Compiler: Design and Imple- mentation. Benjamin/Cummings, 1995. Michael J. \nFischer. Lambda calculus schemata. In Proceedings of the ACM Conference on Prov- ing Assertions about \nPrograms, pages 104-109. ACM SIGPLAN, SIGPLAN Notices, Vol. 7, No 1 and SIGACT News, No 14, January 1972. \nCormac Flanagan, Amr Sabry, Bruce F. Duba, and Matthias Felleisen. The essence of com-piling with continuations. \nIn PFOC. 1993 ACM Conference on Programming Language Design and Implementation, pages 237-247, 1993. \nJoshua D. Guttman and Mitchell Wand (ed-itors). VLZSP: A Verified Implementation of Scheme. Kluwer, Boston, \n1995. Originally pub-lished as a special double issue of the journal Lisp and Symbolic Computation (Volume \n8, Is- sue l/2). Chris Hanson. Efficient stack allocation for tail- recursive languages. In PFOC. 1990 \nACM Sym-posium on Lisp and Functional Programming, 1990. Carl Hewitt. Viewing control structures as pat- \nterns of passing messages. Artificial Intelli-gence, 8:323-364, 1977. IEEE Computer Society, New York. \nIEEE Standard for the Scheme Programming Lan-guage, IEEE standard 1178-1990 edition, 1991. Richard Kelsey, \nWilliam Clinger, and Jonathan Rees (eds.). Revised5 report on the algorithmic language Scheme. ftp://ftp.nj.nec.com/pub/kelsey/r5rs.ps, \nFebruary 1998. David A. Kranz, Richard Kelsey, Jonathan A. Rees, Paul Hudak, James Philbin, and Nor-man \nI. Adams. Orbit: An optimizing compiler for Scheme. In Proceedings SIGPLAN 86 Sym- posium on Compiler \nConstruction, 1986. SIG-PLAN Notices 21(7), July, 1986, 219-223. [Knu73] [LigSO] [MFH95] [MH97] [NN92] \n[QueW [Ram941 Donald A. Knuth. The Art of Programming, Volume I /Fundamental Algorithms. Addison-Wesley, \nBoston, second edition, 1973. Lightship Software. MacScheme Manual and Software. The Scientific Press, \n1990. Greg Morrisett, Matthias Felleisen, and Robert Harper. Abstract models of memory manage-ment. In \nPFOC. 1995 ACM Conference on Func- tional Programming and Computer Architec-ture, pages 66-77, 1995. \nG. Morrisett and R. Harper. Semantics of mem- ory management for polymorphic languages. In A. D. Gordon \nand A. M. Pitts, editors, Higher Order Operational Techniques in Semantics, Publications of the Newton \nInstitute, pages 175-226. Cambridge University Press, 1997. Hanne Riis Nielson and Flemming Nielson. \nSe-mantics with Applications: a Formal Introduc-tion. John Wiley &#38; Sons, 1992. Christian Queinnec. \nLisp in Small Pieces. Cam- bridge University Press, 1996. John D. Ramsdell. Scheme: the next genera-tion. \nACM Lisp Pointers, 7(4):13-14, Ott-Dee 1994. [R-am971John D. Ramsdell. The tail recursive SECD [Ser97] \n[SF961 [Sis97] [SS75] [SS76] [SS78a] [SS78b] machine. To appear, 1998. Manuel Serrano. Bigloo user s \nmanual. Part of the Bigloo 1.9 distribution available via http://cuiwuw.unige.ch/gerrano/bigloo.html, \n1997. Manuel Serrano and Marc Feeley. Storage use analysis and its applications. In PFOC. 1996 A CM International \nConference on Functional Programming, pages 50-61, 1996. Jeffrey Mark Siskind. Stalin -a STAtic Lan-guage \nImplementatioN. Software available via http://www.neci.nj.nec.com/homepages/qobi/, 1997. Gerald J. Sussman \nand Guy L. Steele, Jr. Scheme: An interpreter for extended lambda calculus. Artificial Intelligence Memo \n349, Mas- sachusetts Institute of Technology, Cambridge, MA, December 1975. Guy L. Steele, Jr. and Gerald \nJay Sussman. Lambda: The ultimate imperative. Artificial Intelligence Memo 353, Massachusetts Institute \nof Technology, Cambridge, MA, March 1976. Guy L. Steele, Jr. and Gerald Jay Sussman. The art of the interpreter \nor, the modularity complex (parts zero, one and two). Artificial Intelligence Memo 453, Massachusetts \nInstitute of Technology, Cambridge, MA, May 1978. Guy L. Steele, Jr. and Gerald Jay Sussman. The revised \nreport on SCHEME. Artificial Intel-ligence Memo 452, Massachusetts Institute of Technology, Cambridge, \nMA, January 1978. [Ste76] [Ste77] [Ste78] [TT94] [WanSO] [~~78] Guy L. Steele, Jr. Lambda: The ultimate \ndeclarative. Artificial Intelligence Memo 379, Massachusetts Institute of Technology, Cam-bridge, MA, \nOctober 1976. Guy Lewis Steele, Jr. Debunking the expensive procedure call myth. In ACM National Con-ference, \npages 153-162, Seattle, October 1977. Revised as MIT AI Memo 443, October 1977. Guy L. Steele, Jr. Rabbit: \nA compiler for Scheme. Technical Report 474, Massachusetts Institute of Technology, Cambridge, MA, May \n1978. Mads Tofte and Jean-Pierre Talpin. Implemen-tation of the typed call-by-value X-calculus us-ing \na stack of regions. In Proceedings Zlst An-nual ACM Symposium on Principles of PFO-gramming Languages, \npages 188201, 1994. Mitchell Wand. Continuation-based program transformation strategies. Journal of the \nA CM, 27:164-180, 1980. Mitchell Wand and Daniel P. Friedman. Com-piling lambda expressions using continuations \nand factorizations. Journal of Computer Lan-guages, 3:241-263, 1978.  \n\t\t\t", "proc_id": "277650", "abstract": "The IEEE/ANSI standard for Scheme requires implementations to be <i>properly tail recursive</i>. This ensures that portable code can rely upon the space efficiency of continuation-passing style and other idioms. On its face, proper tail recursion concerns the efficiency of procedure calls that occur within a tail context. When examined closely, proper tail recursion also depends upon the fact that garbage collection can be asymptotically more space-efficient than Algol-like stack allocation.Proper tail recursion is not the same as ad hoc tail call optimization in stack-based languages. Proper tail recursion often precludes stack allocation of variables, but yields a well-defined asymptotic space complexity that can be relied upon by portable programs.This paper offers a formal and implementation-independent definition of proper tail recursion for Scheme. It also shows how an entire family of reference implementations can be used to characterize related safe-for-space properties, and proves the asymptotic inequalities that hold between them.", "authors": [{"name": "William D. Clinger", "author_profile_id": "81100543143", "affiliation": "Northeastern University", "person_id": "P298778", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/277650.277719", "year": "1998", "article_id": "277719", "conference": "PLDI", "title": "Proper tail recursion and space efficiency", "url": "http://dl.acm.org/citation.cfm?id=277719"}