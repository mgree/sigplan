{"article_publication_date": "05-01-1998", "fulltext": "\n Run-time Code Generation and Modal-ML * Philip W ickline Peter Lee Frank Pfenning School of Computer \nScience Carnegie Mellon University Pittsburgh, Pennsylvania 15213-3891 {philipw,petel,fp}@cs.cmu.edu \nAbstract This paper presents a typed programming language and compiler for run-time code generation. \nThe language, called ML , extends ML with modal operators in the style of the Mini-ML: language of Davies \nand Pfenning. ML allows programmers to use types to specify precisely the stages of computation in a \nprogram. The types also guide the com-piler in generating target code that exploits the staging in-formation \nthrough the use of run-time code generation. The target machine is currently a version of the Categorical \nAb-stract Machine, called the CCAM, which we have extended with facilities for run-time code generation. \nThis approach allows the programmer to express the staging that he wants directly to the compiler. It \nalso pro- vides a typed framework in which to verify the correctness of his staging intentions, and to \ndiscuss his staging decisions with other programmers. Finally, it supports in a natural way multiple \nstages of run-time specialization, so that dy-namically generated code can be used in the generation \nof yet further specialized code. This paper presents an overview of the language, with several examples \nof programs that illustrate key concepts and programming techniques. Then, it discusses the CCAM and \nthe compilation of MLo programs into CCAM code. Fi-nally, the results of some experiments are shown, \nto demon- strate the benefits of this style of run-time code generation for some applications. Introduction \n A well-known technique for improving the performance of a computer program is to separate its computations \ninto dis- *This research was supported in part by the National Science Foundation under grant #CCR-9619832, \nand by Defense Advanced Research Projects Agency IT0 under the title The Fox Project: Ad-vanced Language \nTechnology for Systems Software, DARPA Order No. C533, issued by ESC/EN.S under Contract No. F19628-95-C-0050. \nThe views and conclusions contained in this document are those of the authors and should not be interpreted \nas representing the official policies, either expressed or implied, of the National Science Foun-dation, \nthe Defense Advanced Research Projects Agency or the U.S. Government. CI 1996 ACM 0-69791~9674/96/0006...55.00 \n tinct stages. If this is done carefully, the results of early com-putations can be exploited by later \ncomputations in a way that leads to faster execution (Pike, Locanthi, and Reiser 1985; Jarring and Scherlis \n1986; Massalin and Pu 1989). In recent years, a number of researchers have shown that com-pilers, if \nmade knowledgeable about staged computation, can compile staged programs into target code that performs \nrun-time code generation (Keppel, Eggers, and Henry 1993; Consel and NoZl 1996; Engler, Hsieh, and Kaashoek \n1996; Auslander, Philipose, Chambers, Eggers, and Bershad 1996; Lee and Leone 1996; Grant, Mock, Philipose, \nChambers, and Eggers 1997), sometimes reaping substantial perfor-mance gains in the process. The basic \nadvantage of run-time code generation is that it allows low-level code optimizations that are difficult \nto express as source-to-source transforma-tions, such as register allocation and instruction selection, \nto take advantage of values that are not known until run time. Dynamic information also permits some \noptimizations such as loop unrolling and array-bounds checking removal to be pursued more aggressively \nthan is safe at compile time. In order to make use of run-time code generation, a com- piler must first \nunderstand how the program s computations are staged. Determining this staging information is not a simple \nmatter, however. While automatic binding-time analyses have been used by partial evaluators and some \ncom- pilers, we are interested here in developing a programming language that supports a principled and \nsystematic method for describing computational stages and checking their con-sistency. For this purpose, \nwe have developed ML=, an ex- tension of the Mini-ML: language of Davies and Pfenning (1996) to nearly \nall core ML constructs. This language pro-vides modal operators that allow a programmer to specify precisely, \nthrough types, the stages of computation in a pro- gram. Besides providing the programmer with full control \nover when and where run-time code generation occurs, the overall implementation is relatively simple, \nsince the com-plexity of a sophisticated automatic analysis is replaced by an ML-style type checker. \nFurthermore, by taking a princi- pled approach to program staging, we can work in a frame- work that \nfacilitates formal development, including precise definitions of the language and its implementation. \nIn order to demonstrate these advantages, we have im-plemented a prototype compiler for our language. \nThe com-piler generates code for a version of the Categorical Abstract Machine (Cousineau, Curien, and \nMauny 1987), called the CCAM, which is extended with a facility for emitting fresh code at run time. \nAn interesting technical issue arises in the compilation process. MLo programs, which may describe programs \nwhich specialize in any number of stages, must be compiled to a machine which can directly encode only \none level of specialization. We have chosen to restrict the CCAM in this way in order to avoid the code-size \nblowup that can occur when generating machine instructions that generate machine instructions. Resolving \nthis mismatch be-tween the n-level language and the P-level machine is one of the chief technical contributions \nof this paper. Our com-pilation technique is formalized and presented carefully in the hope that the \nideas embodied in it may be useful in the development of other multi-staged run-time code generating \nsystems, regardless of the source languages they use. We begin the paper by explaining some of the motive \ntions for staged computation, and follow this in Section 3 with a brief introduction to the basic constructs \nof ML . -TJ + r3. While this scheme has the advantage that all Fabius programs are also legal ML programs, \nit is also quite limited in its ability to express staging decisions. We claim that MLo can be used as \na clear and expressive notation for staged computation. Drawing on previous work on the language Mini-ML: \n(Davies and Pfenning 1996), and on the interpretation of this language for run-time code getl- eration \ndescribed in Wickline, Lee, Pfenning, and Davies (1998), we present an implementation of a prototype \ncom-piler for a version of the ML language that uses modal op-erators to specify precisely the stages \nof a program s com-putation. We believe that using the modal source language has the following advantages: \nThen, in Section 4, we give a series of programming exam-ples, to show what it is like to write staged \nprograms in our language. These examples are chosen to illustrate dif-ferent aspects of staged computation, \nincluding multi-stage specialization. Section 5 presents the CCAM, our abstract machine for run-time \ncode generation. This is followed in Section 6 by a thorough description of how MLo programs are compiled \ninto CCAM code. Section 7 presents reduc-tion counts on the CCAM for staged and unstaged versions of \nsome of the example programs in Section 4. We end with a brief discussion of related work. Program Staging \n The temporal separation of the computations of a program, or staging, is usually done manually. A notable \nexcep-tion to this is partial evaluation (Jones, Gomard, and Ses- toft 1993), in which the staging of \nprograms is performed semi-automatically, according to a programmer-supplied in-dication of which program \ninputs will be available in the first stage of computation. This information is used to synthesize a \ngenerating eztension that will generate specialized code for the late stages of the computation when \ngiven the first-stage inputs. Both the Tempo system (Consel and Noel 1996) and DyC (Grant, Mock, Philipose, \nChambers, and Eggers 1997) use forms of binding-time analysis for the automatic staging of C programs. \nWhile much work on partial evaluation has focused on programs with two stages, more recent work has extended \nthe partial evaluation framework to account for multiple computation stages (Gliick and Jorgensen 1995). \nMore common is the use of a programming notation or annotation scheme to support programmer-specified \nstag-ing. The backquote and antiquote notation of Lisp macros, for example, provides an intuitive though \nhighly error-prone approach to staged computation. A particularly nasty error is the inadvertent capture \nof variables, leading to the evalu- ation of variables in incorrect contexts. This has motivated relatively \ncomplex hygienic macro systems (Kohlbecker, Friedman, Felleisen, and Duba 1986). Problems involving infinite \nexpansion of macros are also quite common in prsc- tice. More recently, this notion of backquote/antiquote \nhas been introduced as an extension to the C programming lan-guage, in the language C (Engler, Hsieh, \nand Kaashoek 1996). While C has an advantage over Lisp in providing a degree of static type checking, \nthe fundamental problems of variable capture and infinite expansion remain. The Fabius system (Lee and \nLeone 1996) uses another approach to programmer-specified staging. In Fabius, the staging of ML programs \nis specified by currying the argu-ments to functions-a function of type rr + r2 + rs, when applied to \nan argument of type ri, causes the dynamic gen- eration of optimized code for the resulting function \nof type The programmer is able to express the staging that he wants to the compiler directly, rather \nthan through a heavyweight (and, in practice, unpredictable) analysis. The programmer is given a framework \nwhich allows him to verify the correctness of his staging intentions. A staging error becomes a type \nerror which can be an- alyzed and fixed, rather than simply resulting in a slow or incorrect program. \nFurthermore, this framework is useful for conceptualizing and discussing staging with other programmers \nthrough typing specifications. This approach is complementary to the use of auto-matic staging through \nbinding-time analysis. A com-piler is free to augment the staging requirements from a hand-staged program \nusing other means. The language naturally handles situations in which more than two stages are desired, \nsuch as Fabius-style multi-stage specialization (Leone and Lee 1998). This arises, for example, when \ndynamically generated code is used to compute values that are used in the special- ization of yet more \ncode. 3 Modal ML We briefI introduce the language MLO, our extension of Mini-ML, (Davies and Pfenning \n1996). While we present only the most basic and important operators of ML here because of space considerations, \nthe compilation technique described in Section 6 extends easily to all core ML con-structs. Indeed, our \nprototype compiler for MLo handles almost all of core ML. 3.1 Syntax ML arises from the simply-typed \nX-calculus by adding a new type constructor 0. Except for the addition of a new primitive operator, lift, \nit is related to the modal logic S4 by an extension of the Curry-Howard isomorphism, where Or means r \nis necessarily true . In our context, we think of q lr as the type of genera- tors for code of type r. \nGenerators are created with the construct, code M, where M is any ML expression. For example, l-code \n(Xz.t) : ql(cr + a) is a generator which, when invoked, generates code for the identity function. Fig-ure \n1 presents the syntax of MLo which renames the box and let box constructs in (Davies and Pfenning 1996) \nto code and let cogen, respectively. Note that there are two kinds of variables: value variables bound \nby X (denoted by x) and code variables bound by let cogen (denoted by u). We use a to range over either \ntype of variable. Type Variables (Y, p, 7 TYPIC ::= a17+u107 Terms LyN ::= ;Jffz.$ 1 MN 1 u 1 code M \n1 let cogen IL = M in N end Contexts P ::= *1r,2:71r,u:7 Figure 1: Basic ML0 Syntax To invoke a generator, \none might expect a correspond- ing eval construct of type (Oa) -+ cr. Such a function is in fact definable \nin MLO, but is not a part of the ba-sis of the language. Instead we have a binding construct let cogen \n21= M in N end which expects a code gener-ator M of type Or and binds a code variable u. However, even \nevaluation of let cogen u = M in N end will not immediately generate code. Instead, the generation of \nspe- cialized code for M is deferred until the code variable I is encountered during evaluation. For \nexample, I- Ax.let cogen u = x in u end : (ma) + Q! is the function eval mentioned above which invokes \na code generator and then evaluates that code. Generation of code is postponed as long as possible so \nthat the context into which the code is emitted can be used for optimizations. For example, the following \nis a higher- order function which takes generators for two functions and creates a generator for their \ncomposition. The result may be significantly more efllcient than generating 6rst and then composing the \nresulting functions. Note that this function returns a generator, but does not call the given generators \nor emit code itself. 4 Programming with ML0 In order to give a feeling for what it is like to write ML \nprograms, we present several examples. 4.1 Computing the Value of Polynomials To start with a simple \nexample, consider the following ML function which evaluates a polynomial for a given base. For this function, \nthe polynomial aa + arz + aax + . . . a,x is represented as the list [ao, al, aa, . . . , a,]. type poly \n-int list; val polyl = [2,4,0,23333; (* val evalPoly : int * poly -> int *I fun evalPoly (x, nil) = 0 \nI evalPoly (x, a: :p) = a + (x * evalPoly (x, p)> ; If this function were called many times with the \nsame polynomial but different bases, it might be profitable to specialize it to the particular polynomial, \nin et&#38;t synthesiz-ing an ML function that directly computes the polynomial rather than interpreting \nits list representation. We can a~-complish this is by transforming the code as follows. fun specPoly \n(nil) = (fn x => 0) I specPoly (a: :p) = let val polyp = zpecPoly p in fn x => a + (x * polyp x) end \n val polyltarget * specPoly polyi; I- Xf.Xg.let cogen f = f in let cogen g = g in code Xz.f (g (%)) end \nend : cl@ + -y) + q (a + p) + q (a + -f) Readers familiar with Davies and Pfenning s Mini-ML: will notice \nthat we have added a special operator called lift , which obeys the rule that lift M has type q lr if \nM has type r. lift M evaluates M and returns a generator which just quotes the resulting value. In contrast \nto the code construct, this prohibits all optimizations during code gen- eration. As noted in Davies \nand Pfenning (1996), lift is definable in Mini-ML: for base types, but its general form has no basis \nin the underlying modal logic. Here we show that it nonetheless has a reasonable and useful operational \ninterpretation in the context of run-time code generation. 3.2 Typing Rules The typing judgment A; P \nl- M : 7 uses two contexts: a code context A in which code variables are declared, and an ordinary context \nT declaring value variables. The typing rules in Figure 2 are the familiar ones for the X-calculus plus \nthe rules for let cogen, code and lift. The critical restriction which guarantees proper staging is that \nonly code variables (which occur in A) are permit-ted to occur free in generators (underneath the code \ncon-structor), but no value variables -this is enforced by the empty value context in the premise of \nthe code rule. The let cogen rule expresses that if we have a value which is a code generator (and therefore \nof type Or), we can bind a code variable u of type r which may be included in other code generators. \nWhile polyltarget is an improvement over the more general evalpoly, it is far from the fully specialized \nresult we would like. Without support from the compiler, com-mon source-level optimizations are not performed, \nsuch as unfolding of applications. Furthermore, code-level optimiza-tions cannot take advantage of the \nstaging, for example in instruction selection and register allocation, as described by Lee and Leone \n(1996). Therefore we rewrite specpoly as the MLo function compPoly. (* val compPoly : poly -> [int -> \nintl *I fun compPoly (nil) = code (fn x => 0) I compPoly (a: :p> = let cogen f -compPoly p cogen a = \nlift a in code (fn x => a + (x * f I)) end val codeGenerator * compPoly polyl; val mlPolyFun = eval codeGenerator; \nHere the code operator marks the introduction of a code gen- erator, and [r] is concrete syntax for Or. \nThus the compPoly function takes a list of code generators for integers and trans- forms it into a code \ngenerator for a function that computes the value of the polynomial for a particular base. 4.2 Libraries \nOne possibility afforded by ML0 is to install staged versions of library routines, so that client applications \ncan benefit from dynamic specialization of the library code. Consider, for example, placing the comppoly \nfunction in a library. Then, suppose we have a client program: x:rinF A;Fl-M:r+a A;Fl-N:T A;.t-M:r A;Ft-Z:T \nA;FtMN:c A;I kcodeM:Or A;(I ,z:T)kM:a u:rinA A;I l-M:r A;rt-M:Or (A,u:?-);~FN:(T A;l- kXx.M:r-+a n;rt-u:: \nA;rt-liftM:Or A;I l-letcogenu=MinNend:u Figure 2: Typing rules for ML0 (* val client : t.1 -> Ct2 -> \nt31 *I fun client x = . . . code (fn y => . . . compPoly (makePoly y) , , .) . . . where makePoly : t2 \n-> poly. Even though the client program does not have access to the source code of the compPoIy library \nroutine, it is still able to benefit from the fact that it will perform run-time code generation on the \npolynomial computed by makePoly. This example also illustrates one way that multi-stage specialization \ncan be achieved in our system. Note that the client program takes the argument x and generates code for \na function of type t2 -> t3, and that it is this dynamically generated code that invokes the compPoly \nfunction. Hence, dynamically generated code can compute values which in turn are used to generate yet \nmore code. This kind of multi- stage specialization is difficult to achieve in standard partial evaluation, \nbut is supported naturally in our framework. 4.3 Packet Filters A packet filter is a procedure invoked \nby an operating system kernel to select network packets for delivery to a user-level process. To avoid \nthe overhead of a context switch on every packet, a packet filter must be kernel resident. But kernel \nresidence has a distinct disadvantage: it can be difficult for a user-level process to specify precisely \nthe types of packets it wishes to receive, because packet selection criteria are dif- ferent for each \napplication and can be quite complicated. A commonly adopted solution to this problem is to allow user-level \nprocesses to install a program that implements a selec- tion predicate into the kernel s address space \n(McCanne and Jacobson 1993). In order to ensure that the selection predi-cate will not corrupt internal \nkernel structures, the predicate must be expressed in a safe programming language. Un-fortunately, this \napproach has a substantial overhead, since the safe programming language is typically implemented by \na simple (and therefore easy-to-trust) interpreter. As demonstrated by several researchers, run-time \ncode generation can eliminate the overhead of interpretation by specializing the interpreter to each \npacket filter program as it is installed. This has the effect of compiling each packet fil-ter (Massalin \nand Pu 1989; Engler, Wallach, and Kaashoek 1995; Lee and Leone 1996; Sirer, Savage, Pardyak, DeFouw, \nand Bershad 1996). To demonstrate this idea in our lan-guage, consider the following excerpt of the implementa-tion \nof a simple interpreter for the BSD packet filter lan-guage (McCanne and Jacobson 1993) in ML. (* val \nevalpf : instruction array * * int array * * int * int * int -> int * Return 1 to select packet, 0 to \nreject, * -1 if error *)  fun evalpf (filter. pkt, A, X, pc) = if pc > length filter then I else case \nsub (filter, pc) of RET-A => A I RET-K(k) => k I LD-IND(i) => let val k = X + i in if k > length pkt \nthen 1 else evalpf (filter, pkt, sub(pkt,k), x, pc+l) end , . . The interpreter is given by a simple \nfunction called evalpf, which is parameterized by the filter program, a network packet, and variables \nthat encode the machine state. The machine state includes an accumulator, a scratch register, and program \ncounter. In order to stage this function, it is straightforward to transform the code so that the packet \nfilter program and pro- gram counter are early values, and the packet, accumula-tor, and scratch register \nare late. Then, the computations that depend only on the late values can be generated dynam-ically by \nenclosing them in code constructors. At first glance it might seem odd that the interpreter can be specialized \nnot only to the packet filter program but also the program counter. This is possible since (Berkeley-style) \npacket filters have no loops and thus always terminate. Thus, specializ-ing the interpreter on the program \ncounter has the effect of unrolling the interpreter over all of the instructions in the given packet \nfilter program. (* val bevalpf : * (instruction array * int.) -> * [int * int * int array -> intl *I \nfun bevalpf (filter, pc) = if pc > length filter then (fn _ => -1) else case sub (filter, PC> of RET-A \n=> code (fn (A,X,pkt) => A) I RET-K(k) => let cogen k = lift k in code (fn _ => k ) end 1 LD-IND(i) => \nlet cogen ev = bevalpf (filter, pc+l) cogen i = lift i in code (fn (A,X.pkt) => let val k = X + i in \nif k >= length pkt then -1 else ev (sub(pkt,k), X, pkt) end) . . ,  When applied to a filter program \nand program counter, the result of bevalpf is the CCAM code of a function that takes a machine state \nand packet, and computes the result of the packet filter on that packet and state. Later, in Section \n7, we show that the improvement in execution time for a typical BPF packet filter is substantial. 4.4 \nMemoizing ML0 Programs Since specializing programs at run time typically involves additional expense, \na central assumption of this approach is that the specialized code generated will often be used many \ntimes. This happens naturally in some programs. If, for example, a program specializes a section of code \nand then immediately, in the same scope in the code, uses that specialized code many times, it is easy \nto bind the generated code to a variable and use that variable, thereby avoiding regeneration of the \ncode. In other situations we must work harder to get this sort of memoizing behavior. Consider the following \nspecializing function to compute the value of an integer raised to the power of e. (* val codePower : \nint -> [int -> intl *) fun codePower a = if e = 0 then code (fn _ => I> else let cogen p = codePower \n(e -I> in code (fn b => b * (p b)) end If this function is used to compute powers in two or more sections \nof the same program, it is possible that the same code will be generated and regenerated many times, \nmaking the resulting program slaver rather than faster. We must carefully arrange to have generated programs \nsaved for fu-ture use in situations where we they are likely to be needed again. Fortunately, we can \nbind up this functionality with the function itself. We assume an implementation of tables and table \nspecCode with the types given below. (* speccode : (int -> int) table get : a table * int -> a option \nadd : a table * (int * a) -> unit *>  (* memoPower : int -> int -> int *) fun memoPower e = case lookup \n(speccode, e) of NONE => let cogen p = codePower e val p = p in add (speccode, (e, p >> ; p end I SOME \np => p; This function simply embeds the codePower function within a wrapper that checks a table to determine \nwhether or not a particular specialized version of the function exists. If it does, then it is returned, \nwithout need for further work. Otherwise, codePower is called, and a new function is gen- erated, stored \nin the table, and returned. While memoPower saves generated code, so that it will benefit from past computations \non the same exponent, it does nothing to speed up the computation for two different exponents, even though \nthey may share subcomputations. memoPower goes even further than memoPower1. It saves the result of each \ninternal call to the power function in a table, genExts, of generatin&#38; extensions. Then if it is \ncalled to compute, for instance, n and then m34 it will not have to do any additional work to make a \ngenerating extension for the second call. (* speccode : tint -> int) table genExts : Cint -> int.1 table \nget : a table * int -> a option add : a table * (int * a) -> unit *) (* memoPower : int -> int -> int \n*> fun memoPower e = (case lookup (speccode, e) of NONE => let cogen p = mPower e val p = p in add (speccode, \n(a, p )); p end I SOME p => p) (* mPower : int -> [int -> intl *I and mPower e = (case lookup (genExts, \ne> of NONE => let val p = bPower e in add (genExts, (e, p>> ; p end I SOME p => p) (* mPower : int -> \n[int -> intl *> and bPower e = if e = 0 then code (fn _ => 1) else let cogen p = mPower (a -1) in code \n(fn b => b * (p b)) end; While specifying memoization behavior by hand in this fashion may be tedious \nin some cases, it does allow the pro- grammer to control carefully what and how memoization will occur. \nFurthermore, generic memoization routines can accommodate most common memoization needs. 5 The CCAM In \nthis section we present the CCAM, an ad hoc extension of the CAM (Cousineau, Curien, and Mauny 1987) \nwhich provides facilities for run-time code generation and is the target of the compiler detailed in \nthe next section. For reasons explained below, we would like to model the form of run-time code generation \nprovided by the Fabius compiler (Lee and Leone 1996), which does not manipulate source terms at run time. \nThe design of an abstract machine for this kind of code generation must strike a delicate bal-ance. On \none hand, we want to abstract away from the de- tails of individual architectures as much as possible. \nOn the other hand, a realistic compiler must directly generate ma-chine code at run time, which by its \nvery nature depends on the specific machine architecture. The CCAM therefore has instructions to emit \ninstructions directly into a code block, but we carefully limit the power of such emit instructions so \nthey can serve as a generic model from which compilation schemes for individual machine architectures \ncan be derived. 5.1 Run-Time Code Generation Many techniques have been used to build systems which dy-namically \nspecialize programs at run time. The most niiive technique is to perform, at run time, source level substitu-tion \non the program to be specialized and then call a full compiler. Obviously, this technique is extremely \nexpensive. Less expensive schemes have been developed for run-time specialization, notably template filling, \nwhich is used by the Tempo system (Consel and No61 1996) and the Synthesis kernel (Massalin and Pu 1989). \nTemplate filling compiles selected parts of the program to sequences of machine code with holes , that \nis, sections whose instructions need to be filled in at run time. These templates may then be copied \nand have their holes plugged to produce specialized machine code. Template filling is typically much \nmore efficient than calling a full compiler, because source-level terms are never manipulated at run \ntime, and most of the compilation (all of the template that is not a hole) can be computed ahead of time. \nHowever, template filling is inflexible about the kinds of values which may fill its holes and the kinds \nof optimiza- tions that may be performed at run time. We intend our abstract machine to model run-time \ncode generation. In particular, there are three important features that we believe allow substantial \nflexibility of specialization at relatively low cost. The machine should not manipulate source-level \nterms at run time. Instead machine language programs should be synthesized directly from machine language \nprograms. The machine should encode terms to be specialized directly into the instruction stream, for \nexample in the form of immediate operands to instructions. This is in contrast to systems which copy \ntemplates and fill in holes at run time. Instruction stream encoding enables a great deal of flexibility \nin the kinds of specialization that can be performed at run time. The machine must allow dynamic staging \nof code, i.e., the number of stages of program specialization may de- pend on some value that will not \nbe known until run time. This is necessary to fully exploit the specializa- tion opportunities in many \nsituations. For example, many programs have a top-level loop which waits to receive input in some form, \nand then takes appropri-ate action. In contrast, conventional off-line partial evaluation will fail to \nserve such a program well be-cause even multi-level partial evaluation has no way to specialize on each \nof the variably many inputs.  5.2 The CAM The CAM (Cousineau, Curien, and Mauny 1987) is a simple abstract \nmachine which uses categorical combinators as its instructions. The CAM allows a great deal of flexibility \nin manipulating environments, which are represented as nested pairs of values. This flexibility makes \nthe CAM an especially good choice as a base for our code-generating abstract ma-chine. For the purposes \nof this paper, the reader need not have knowledge of categorical logic. We use the CAM only as a simple, \nstack-based machine. 5.2.1 Instructions The syntactic definitions of various CAM components fol-low. \nInstructions i ::= id 1 fst ] snd ] push ] swap ] cons I app ] e I Cur(P) Values e,f ::= ij I Ie f PI \nI (6 f) Programs ..-, ;z Stacks 5 III .le::S We will also use . to represent the empty program, with \nthe understanding that (.; P) = (P; -) = P. The values of the CAM consist of the empty tuple, 0; closures, \ncreated when Cur instructions execute; and pairs, which also serve to represent the environment. The \nenvi- ronment consisting of values fi, . . . , f,, is represented as the value ((0, fi), . . . , fn). \n 5.2.2 Transitions A state of the CAM consists of a stack paired with an in-struction sequence, (S, P). \nWe think of the top element e of the stack (e :: S) as the current environment in which the program P \nis to be executed. It is characteristic of the CAM that the value returned by an instruction is also \nplaced on top of the stack. The upper part of Figure 3 describes the transitions of the CAM. We write \n(S, P) a (S , P ) if the stack 5 and program P match the first two columns of Fig- ure 3 while 5 and \nP match the last two. w* denotes the reflexive transitive closure of the s relation. The instruction \nid leaves the stack unchanged. f st and snd project the first and second elements of pairs found on top \nof the stack, respectively. A variable which accesses the current environment (on top of the stack) is \ntherefore com-piled to a sequence of fst instructions followed by a snd instruction. The instructions \npush, swap, and cons manip-ulate the stack, and taken as a trio constitute the pairing mechanism of the \nmachine. If push; PI; swap; P2; cons is run with the environment e on top of the stack, then push will \nsave e by making another copy on the stack. PI will then consume the first copy of e, replacing it with \nits return value fi. The swap instruction will restore the second copy of e to the top of the stack, \nsaving fr in the process. P2 will then consume e and replace it with its return value fi. Fi-nally, cons \nwill pair the top two elements of the stack, fr and f2, making the pair (fi, f2) the return value for \nthe whole program. To emphasize the pairing operation of these instructions, we will often write ( for \npush, , for swap, and ) for cons, and drop the separating semicolons. Thus push; 9; swap; P2; cons may \nbe written as (PI ,P2). Cur(P ) creates a closure [e : p] from the current envi-ronment e on top of the \nstack and P . The corresponding app instruction expects a pair consisting of a closure [e : P ] and its \nargument f on top of the stack. It applies the func- tion by evaluating its body P in the environment \ne extended by f, represented by (e, f).  5.3 An Abstract Machine for Run-time Code Generation The CCAM \nhas been designed as an extension of the CAM with the goals listed in Section 5.1 in mind. The primary \nnovelty of the CCAM is the emit(i) instruction. It repre-sents the series of instructions required on \na real computer to produce the instruction i in a specialized program. As will be made more clear below, \nthe CCAM encodes a generating extension as a series of emit(i) instructions. As an example of this form \nof code generation, consider the instruction emit(add). If this instruction were compiled to real machine \ninstructions it might be represented by three instructions, one which contained the lower 16 bits of \nthe add instruction in an immediate load low instruction, one which contained the upper 16 bits, and \nfinally one to write the assembled instruction to memory. A more sophisticated specialization system \nmight compile emit (add) to a series of instructions which tests the values of the operands of the add \ninstruction at specialization time (if they are available) and eliminate the instruction altogether if \neither one is 0. 5.3.1 Nested Code Emission Multi-staged programs are a potential problem for our ab-stract \nmachine. It is clearly possible, in MLO, to write pro-grams containing expressions such as code (. +. \ncode M * . a) in which there are nested code generators. If we encode gen- erating extensions with emit(i) \ninstructions, must such pro- grams give rise to instructions of the form emit (emit(i) ) ? If so, then \na chain of n generating extensions could lead to n nested emits. Observe, however, that on a machine \nwith fixed-length instructions, there is a limited amount of space available for immediate operands, \nand so if instructions to be emitted are embedded in instructions in the instruction stream, it will \ntake at least two instructions to represent one emitted in-struction. Furthermore it could take 2 instructions \nto rep- z resent emit(emit(. . . emit(i) ..a)). For this reason, nested emits are not allowed on the \nCCAM, and our compilation scheme therefore needs to take special steps in order to allow multi-level \nspecialization. Note that for a similar reason, emitting a Cur(P) instruc-tion would be unrealistic and \nis therefore also disallowed.  5.3.2 Instructions The CCAM has the usual seven instructions associated \nwith the CAM, and five more for code generation. The new syn-tactic definitions of the CCAM follow. Simple \nInst i ::= id 1 fst 1 snd 1 push 1 swap I cons I app 1 e I lift 1 arena I merge I call Composite Inst \nValues Code Blocks I e, f B ::= ::= i 1 emit(i) ) Cur(P) (et f 1 I [e : PI I B I 0 Programs Stacks fJ \n::=..-IIS Y};....I .le:: S Instructions are now broken into simple and composite instructions. This \ndivision is to enforce the restriction we note above on the arguments of the emit instruction. The CCAM \nhas a new kind of value, code blocks, not available in the CAM. A code block is a dynamically created \nsequence of instructions. emit instructions, when executed, append their arguments to the end of code \nblocks. On a real computer, a code block would be represented by a pointer to the end of a dynamically \ncreated segment of machine code. When a new instruction is added to the sequence, the pointer is incremented \nappropriately. All of the new instructions of the CCAM manipulate code blocks.  5.3.3 Transitions The \nCCAM is a conservative extension of the CAM in that it has all of the same values, instructions, and \ntransitions that the CAM has. Figure 3 describes the transitions of the CCAM. There is one new transition \nfor each new instruction: arena creates an empty code block on top of the stack, emit(i), emits instruction \ni into the current code block at the head of the current environment, lift directly inserts a quoted \nvalue into the current code block, merge inserts a function into the code block, and call invokes the \ncurrent code block by inserting it into the instruction stream. This use of these instructions will be \nexplained in more detail in Section 6.2. 6 Compilation In this section we describe our technique for \ncompiling MLD terms into CCAM code. While the compilation of normal ML terms to the CAM is straightforward, \nour task is con- siderably more difficult. The principal difficulty comes from the fact that ML programs \ncan specify multiple rounds of dynamic code generation so that, for example, the results computed by \ndynamically generated code can be used to specialize yet more code. This feature of ML0 is problematic \nbecause the CCAM does not have any facility for emitting emit instructions. Thus, a primary technical \ncontribution of this paper is the strategy for compiling multi-stage pro-grams down to code appropriate \nfor a two-level machine. It should be noted that this scheme is applicable to lan-guages and machines \nbeyond those considered in this paper. Therefore we provide a detailed explanation of the compila- tion \nscheme and its development in the hope that this may serve as a road map to designing similar compilation \nschemes for other multi-level languages. We begin with a high-level overview of our compilation strategy \nin Section 6.1 before diving into the rather technical details of compilation to the CCAM in Section \n6.2. Some readers, particularly those un-familiar with the CAM, may prefer to skip Section 6.2 in favor \nof a more careful reading of Section 6.1. 6.1 Strategy As a fnst attempt at a compilation strategy, \nif we would normally compile the term M to the series of instructions i1; iz; . *. ; i,, we can compile \nthe term code M to the instruc- tions emit( emit(i2); . . . ; emit(&#38;) to produce a run-time code \ngenerator for M. While this is the basic idea of our compilation technique, this simple strategy neglects \ntwo im- portant details. First, it does not take account of the possi- bility of free code variables \nin an expression and the special- ization opportunities presented by those variables. Second, it does \nnot explain how to deal with code generators which are nested within code generators. In order to benefit \nfrom run-time specialization, the code generators need to make use of values that are computed at run \ntime. In our system, this new information is supplied through the free code variables in a code expression. \nThese variables become bound to code generators which can be called to substitute code into the containing \nexpression. To make this more clear, consider the following ML0 term: let cogen u = code 0 in code xx.2~ \n* x end In Section 6.2 we describe in some detail the compilation of ML0 terms into CCAM code. Here, \nwe try to avoid the rather intricate details of the CCAM, and instead simply underline all sub-terms \nwhich would be compiled into series of emit instructions. For the current example, then, we obtain the \nfollowing: let cogen u = code 0 in code Xx.u *x end Note that the code variable u in the expression code \nXx.u *x is not underlined, that is, it will not be translated into emit instructions. Instead, when code \nXx.u *x is activated as a code generator, it will in turn activate the code generator for code Q bound \nto u. Therefore the whole expression will end up emitting Xx.0 * x. Although we do not address in this \npaper exactly what kinds of low-level optimizations will be performed by our emit instructions, it is \neasy to imagine that specializing code for Xx.u*x could test to see if the value being substi-tuted for \nu is a zero, and if so specialize to Xx.0. This and other similar optimizations are performed, for example, \nby the Fabius compiler (Lee and Leone 1996). We now consider the problem of nested code generators. Ndively, \nif M compiles to ii; is;. . + ; i,, then code M is com- piled into emit(&#38;); emit(i *. ; emit(&#38;) \n Stack S Program id; P fst;P snd; P f;P push;P swap; P cons: P Stack s e :: S ;;;; e :: e :: s f :: \ne :: S (f,e) :: S [e : P ] :: S (e, f) :: S S {L.fik ; i)) :: S (e, {P:j e}) :: T (e, y ; Cur(P )}) :: \nS e :: Proof-am ; P P P P P P P ; P P P P P P :P Figure 3: Transitions of the CCAM and therefore code \ncode M becomes emit(emit(ii));emit(emit(i2)); . . ..emit(emit(i.)) Of course, as discussed in Section \n5.3.1, such instructions are not allowed on the CCAM. Our solution is to avoid re- peated specialization \nof the inner code generator by lifting it unchanged into the specialized code of the outer code gener- \nator. For example, instead of treating code (a.. code M - . a) as the three-level term code (. - . code \nE * . a), we can compile it as the two-level term let cogen u = lift (code &#38;) in code (. . .w * *) \nend Of course, this is not entirely satisfactory, since M may con-tain variables which are bound in the \nenclosing code expres- sion. For example, if we transform the expression code (let cogen U = N in code \nU + M end) to let cogenu = lift (code u +M) in code (let cogen u = N in&#38;@ end as we did above, then \nu is unbound in code u +M. For this reason, we need to arrange for the code generz which is lifted into \nthe enclosing generator to use the environment that is active where it was originally located, not where \nit is lifted into the environment. It is not possible to represent this operation in our source language, \nbut it is quite easy to do so on the CCAM, since no distinction is made between en-vironments and other \ndata values. When translating to the CCAM, a nested code generator is compiled into a function which \naccepts as its argument an environment, with which it replaces its current environment. This function \nis then inserted into the specialized code via the lift operator. Finally, code is emitted that applies \nthe lifted function to the environment that will be available when the specialized code is run. The net \nresult of our compilation scheme is that it yields run-time code generators which do not incrementally \nspe-cialize code, even if that code is nested under several code constructs. Instead code is generated \nin a lazy fashion, only when it is needed and only when all of the code variables in it are bound. The \nprimary advantage of this, as we have noted, is that nested emit instructions are not necessary, thereby \navoiding possible exponential code blowup. Another advantage is that if intermediate stages of code generators \nin a multiply nested code generator will not be used more than once, this strategy will be faster than \nan incrementally specializing strategy, without sacrificing the quality of the specialized code which \nis the end product. On the other hand, if the intermediate stages will be used repeatedly, an incremental \nstrategy may be preferable. 6.2 Details of the Compilation Scheme We use variable contexts R and A to \nkeep track of the posi- tion of bound variables in the run-time environment, so that correct code can \nbe generated for variable references. They are formed as follows. Variable Contexts R,A ::= () ) (n,a) \n We will define a pair of functions, [i%f]o and [MIA, from ML0 terms to CCAM programs. The first, [M]n, \ncompiles an ML term M with free variables in R into normal CCAM code. The second, [Ml;, translates M \nwith free variables either in R or A into a generating extension for M, special-izing it to the variables \ncontained in A. 6.2.1 Preliminaries In the compilation rules, we will apply contexts to vari-ables as \nif they were functions. This is intended to rep-resent the CCAM code necessary to select the argument \nvariable from the context. For example, if R is the context (,..((A,a,),a,-~),... , ao) then Q(a,) = \nf st; . . . ; f st; snd. We also need a context extension operation A: fl. If A and R are variable contexts, \nand R = (*a * ((0, ai), as), . . . , a,) then we write AIR to represent the extended context of form \n(...((A,al),a2),...,a,) For brevity and clarity, we will often use underlining to denote the emission \nof code, so that emit(i) is written as i. This also applies to our convention of applying contexts as \nif thev were functions: for examnle. if R is the context (. . . ((A, a,), an-I), ... , ad) then a(;,) \n= fst; ~9 + ; fst; snd. We now describe the comnilation of ML0 terms into CCAM code. ML is a a fulllfeatured \ndialect of ML that includes datatypes, pattern-matching, and references. Since space does not permit \nus to describe the compilation of all of the features of MLb, we will focus on the core features that \npertain most directly to staged computation. 6.2.2 Compiling core ML terms The compilation function \n[M]o is used to compile ML0 terms M which are not enclosed in a code construct in the context SZ. At \nrun time, the term M will be evaluated in an environment which matches R, supplying values for its value \nvariables and code generators for its code variables. For the base ML constructs, compilation is done \nin ex-actly the same way as described for the CAM (Cousineau, Curien, and Mauny 1987). The fundamental \ninvariant to keep in mind is that if the value of M in environment e is f, then on the CAM    (e :: \ns , [M]n; P) ==s* (f :: s ) P). In other words, the instructions for M consume the current environment \ne on the top of the stack and replace it by the value of M. Value variable references are compiled to \nselections from the environment. [x]cl = O(x) An abstraction is directly compiled to a Cur instruction \nwhich will immediately create the appropriate closure value when executed. [Ax.MBn = Cur([Ml(o,,)) On \nthe CAM, application of functions is achieved via the app instruction. This instruction expects that \nthe top of the stack will be a pair of a closure and its argument Assuming [e : P ] is the value of M \nand f is the value of N, the evaluation of [MN]* proceeds as follows: (e :: S, (Pfln s KNln); w; PI a \nWin , KWn); app; P) -i* ge :: ;]: e :: s, s Win); w; PI ===+ (e :: [e : P ] :: S, [No); app; P) a* (f \n:: [e : P ] :: s ); ==+ (([ P ] f) : i papFp, ==?  ((e:, )) :: >, :  PI;&#38;) Here it helps to remember \nthat ( is push, , is swap and ) is cons. 6.2.3 Compiling code generators While let cogen u = M in N \nend is treated specially when typing ML0 terms, operationally it introduces nothing new. It binds the \nresult of evaluating M to u in the environment. The following sequence accomplishes precisely this. [let \ncogen u = M in N endlo = ([Mjjn); [N](n,u) Our plan is to compile code expressions as described in Section \n6.1. Since our code generators must be supplied with a code block into which to emit code, we enclose \neach generating extension in a Cur instruction so that when it is evaluated it will form a closure which \ncan be bound into the environment. We will need to arrange for these closures to be applied to code blocks. \nThe compilation function for generating extensions, [MBA, employs two contexts, A to hold the free code \nvariables to which the term should be specialized, and R to keep track of the local binders under which \nwe have descended. The rule, then, for compiling code expressions is surprisingly simple: [code Mjn = \nCur([M]t) R contains all of the variables free in M, and in particular the code variables to which we \nwant to specialize M. The rule for compiling code variables must select out of the environment the code \ngenerator to which the variable is bound, create a new code block (accomplished with the arena instruction), \nand apply the code generator to that new code block. Finally, we need to jump (using the call instruction) \nto the code that is deposited in the code block by the generator. [u]o = (R(u) ,arena); app; call For \nreferences to (non-code) value variables, function ap-plications, and let cogen expressions, the compilation \nfunc-tion for code generators looks just as one would expect, given the strategy described above. = a(x) \n= (lWP~,Knlli %; app [let cogen u = M in N end]; = t[M1:,; [N]&#38;,,) These rules are exactly the same \nas their normal compilation counterparts in [Ml*, except that each instruction added by them is emitted. \nTo illustrate how code is generated, con-sider the reductions that compiled code for an application MN \nwill take when supplied with an environment e that supplies code generators for the free code variables \nin M and N, and which has a code block as its rightmost value. ((e, {P l) :: s, (UMBk,KW~); amp) =b- \n((e, {P ; 0) :: s, UMl~LKNli$&#38; P) a* ((e, if ; U%l) :: ST JNBk); am PI =+ ((e, {J ; (Ev .)) :: S, \nIV%);% P) ==F* ((e,{P ;(P.w,PN}) :: s, );bi3) a ((e,{P ; (PM ,pN)}) :: s, -azp) * - ((e, {P ; (PM ,PN); \nam>) :: S, P) Here, PM is the program emitted by [MBA and PN is the program emitted by [WA. A technical \nproblem arises when we compile functions in code generators. Since the argument of emit must be a simple \ninstruction and therefore not a Cur, we cannot simply compile code Xx.M as emit(Cur([M]h)). For this \nreason the CCAM has the special instruction merge which allows one segment of emitted code to be inserted \ninto another as the argument of a Cur. This also means that we must be able handle multiple code blocks, \nand be sure that we will emit code to the appropriate one at the appropriate time. To compile an abstraction \ninside a code generator, we need to initialize a new code block, emit the body of the function to that \ncode block, and the merge it back into the original code block. [Xx.M~~ = ((f st, arena); [M]g ); snd, \nid); merge Some code variables in code generators are bound to code generators which should be called \nin order to emit their code into the current block, effectively performing code substitu- tion. However, \nnot all code variables encountered will be bound yet. For example, in the expression let cogen u = code \n(4 + 5) in code (let cogen u = code 2 in 3 + u t u end) end the term code (let cogen U = code 2 in ~+u \n+u end) (call it M) contains to a reference to u which will have a code generator bound in the environment \nwhen M is activated as a code generator. However, u , which is also referenced in M will not have an \nentry in the environment until the code of M is generated and run. Therefore we need to take careful \naccount of which variables are available and when. The compilation function for code generators, [MI: \nemploys two variable contexts to do this: one (A) to hold the position of the variables which have bindings \navailable at generation time, and to which we can therefore specialize the code; and the other (a) to \nhold the position of the rest of the variables. Thus, compiling a code variable depends on in which context \nthe variable is bound. run;l = { yyzswPJ~ if u is in R f st , f st; AT%), snd); app; snd) if u is in \nA The first case, when u is in a, corresponds to compiling a code variable that does not yet have a binding \nin the envi- ronment (u in the example above). It emits into the current block code which will activate \nthe generating extension to which the variable will become bound. Therefore, it looks just like case \nfor code variables in the normal compiler, ex-cept that each of its instructions is wrapped with an emit. \nThe second case handles code variables which already have code generators available. These are activated \nand the code that they generate is added to the current code block. Recall from Section 6.1 that we want \nto compile nested code generators into functions which we simply lift into the code block, without specialization. \nThis function accepts as its argument an environment, and the lifted function is applied to the current \nenvironment. All of this makes for a complex rule for code M. [code Ml; = ((f st , ( 0; Cur(f st; Cur([M]It \n*)), snd); lift; snd)L idi; app To understand this complicated rule, first observe that the program (P, \nid); app will be emitted into the current code block, where P is the program that is emitted by the code \n(fst, ( 0; Cur(fst; Cur([M];f )),snd); lift; snd). This will have the effect of explicitly applying the \nfunction produced by P when it is run to the current environment, since id leaves the environment untouched. \nP creates a closure containing the code for a generator for M and lifts that closure into the code block. \nIn detail, P first saves a copy of the current environment without the current code block on the stack. \n((e, {p )) :: S, (fst, ( 0; Cur(f st; Cur([MJt )) , snd); lift; snd); P) ((es ]) :: e :: S, ( 0; Cur(f \nst; Cur([Ml$$,))), snd); lift; snd); P) Next, an empty environment is created, and the closure is created \nin that empty environment (the environment that the code generator will need will be supplied at the \ntime the generator is activated). ((e, {P}) :: e :: S, ( 0; Cur(fst; Cur([M# ())), snd); lift; snd); \nP) a* (0 :: (e, {P }) :: e :: S, Cur(fst; Cur([M]fi *)) ,snd); lift; snd); P) -*- ([() : fst;Cur([M~~ \n*)] :: (e, {P }) :: e :: S, t snd); lift; snd); P) Now the closure and the code block are paired and \nthe clo- sure is lifted into the block, and the resulting block is paired with the saved environment. \n([() : fst; Cur([Mja )] :: (e, {P }) :: e :: S, , snd); lift; snd); P) =+-* (([() : fst; Cur([M~; , )], \n{P }) :: e :: S, lift; snd); P) ===+* ({P ; [() : fst; Cur([Mj~ *)]} :: e :: s, PI Thus, running [Ml: \nwill deposit (I[() : f st; Cur([M]; )], id); am in the current code block, which will, when run in an \nenvi- ronment e create the closure [e : Cur([M]t )], which is a code generator. The compilation functions \nare summarized in Figure 4. Many optimizations are possible, but left out of the figure in the interests \nof brevity. 7 Implementation We have implemented a prototype ML0 compiler. The Ian-guage includes most \nof core ML, including datatypes, ref-erence cells, and arrays. All of the programs presented in this \npaper are working programs compiled by our compiler. The compiler generates code for the CCAM extended \nwith support for efficient handling of conditionals, recursion, and various base types. In addition, \nwe have built a CCAM simulator on which to run the output of our compiler. While CCAM instructions are \nrather abstract compared to native machine code, we can still observe the benefits of specialization \nby counting reduction steps in the CCAM machine. Figure 1 contains a list of reduction counts for executions \nof the CCAM on some of the programs in this paper. As indicated in the table, the non-specializing packet \nfil-ter interpreter, evalpf, takes 9163 reductions on our CCAM simulator to recognize each telnet packet \npresented to it. On the other hand the specializing interpreter, bevalpf, only takes 1104 reduction steps \nafter paying an initial cost of 11984 reductions to generate a specialized version of the in- terpreter \nfor the telnet packet filter. We also obtain a significant speedup from the specializ- ing version of \nthe polynomial calculator, compPoly. In fact, the specialized version somewhat suspiciously pays for \nitself after only one application. However, part of the improve-ment in performance of compPoly over \nevalPoly comes from the fact that compPoly is impeded less by the low quality of our current pattern \nmatching compiler. ikln  ETn n bh [code M]ri llift M]n [let cogenu= 'PI:: [Ax. M]l$, @WI rr4t-i \n [code M]; ilift M]: [let cogent= MinN end]n M inNend]:: = fG) = Cu4Wh,=~) = (Win p INIn); am = (Q(u) \n,arena); app; call = cdWIl~) = [Ml*; Cur(lift) = Wfln); P%w = n(z) = ((fst ,arena); [Ml&#38;,,.; snd, \nid); merge = $ #-&#38;%%; --app if u is in R (fst,(f st; ATu&#38;d); app; snd) if u is in A  = 1W+izszd;ww&#38;U \n= ((fst,( ();Cur(fst; Cur([M]~'\")),snd);lift;snd)&#38;$japp = [MIA; ((fst,arena);J,j&#38;;snd,id);merge \n= jPfl$; lWl&#38;,u, Figure 4: Compilation rules 1 Computation Reductions) [ evalpf on first telnet packet \n9163 1 evalpf on nth telnet packet bevalpf on 6rst telnet packet bevalpf on nth telnet packet evalPoly \n(47, polyl) specPoly polyl polyiTarget 47 compPoly polyl eval codeGenerator mlPolvFun 47 Table 1: Reduction \nsteps on the CCAM for various functions in the text Related Work The style of run-time specialization \ndescribed in Section 5.1 is inspired by the Fabius system (Lee and Leone 1996). Fabius compiles a pure, \nfirst-order subset of ML into n&#38; tive MIPS code. It chooses curried functions as the sites for specialization, \ncompiling them into generating extensions parameterized by their early arguments. Tempo (Consel and Noel \n1996) is a compiler which ex-tends several techniques from traditional partial evaluation in order to \nstage C programs semi-automatically. Program-mers can supply an initial division of a program into stages \nvia auxiliary data files which are used to guide a binding- time analysis. Programs are then compiled \ninto templates which can be specialized either at run time or in a pre-run time specialization phase. \nThe DyC system (Auslan-der, Philipose, Chambers, Eggers, and Bershad 1996; Grant, Mock, Philipose, Chambers, \nand Eggers 1997) provides a relatively sophisticated and declarative annotation scheme for specifying \nthe initial division of inputs, but then shares several characteristics with Tempo, including its use \nof an extended binding-time analysis and use of template filling for run-time specialization. Though \nwe have chosen to compile ML programs to perform run-time code generation in the style of Fabius, it \nis easy to see that we could also view code expressions as templates, and their free code variables as \nthe holes to be filled. Instruction-stream encoding and template filling each have their advantages as \ntechniques for run-time code gener- ation. It is conceivable that a sophisticated compiler might have \nboth techniques at its disposal and use the most ap-propriate one based upon the requirements of the \nprogram. Engler, Hsieh, and Kaashoek (1996) describe C, an ex- tension of the C language which has facilities \nfor creat-ing run-time specializing programs. C has an annotation scheme in the style of Lisp which allows \nprogrammers to annotate their programs to stage them the way they desire. The compiler for C, called \ntee, provides several diierent back ends, depending on the style of run-time specialization desired. \nThese styles range from a heavyweight invocation of a full C compiler back-end at run time, to a lighter-weight \nsingle-pass translation to an abstract machine code. Davies (1996) describes the language X0, which is \nbased on linear-time temporal logic. It allows the manipulation of code with free variables and can thereby \nforce inlining, which is not expressible in MLO. However, an eval operator is not expressible in this \nlanguagepartial evaluation can proceed only by a sequence of global program transformations. Taha and \nSheard (1997) attempt to combine the benefits of ML and X0 in the language MetaML. The language can express \ninlining in much the same way as X0, while still allowing an eval operator. However, MetaML is not known \nto have a logical basis as the other two languages. Furthermore, their operational semantics is quite \ndifferent from the CCAM and does not address light-weight run-time code generation. 9 Conclusion We have \ndesigned the language MLO, and implemented a compiler that translates compiles ML0 code expressions into \nefficient run-time code generators. The compiler targets the CCAM, an extension of the CAM carefully \ndesigned to em-ulate the style of run-time code generation first provided by the Fabius compiler. 234 \nIn our early experience with the ML language and our compiler, we have been able to express precisely \nthe stag-ing of computations necessary to take best advantage of the run-time code generation facilities \nof the CCAM. This ex-perience is an early indication that a language that provides explicit control over \nstaging decisions can be a practical way to improve the performance of programs. Furthermore, we have \nfound that the typed framework of Mini-ML: provides, in addition to an expressive method for specifyin \nf staged computation, a basis for formalization of both ML and its compiler. We believe that this framework \nextends to other languages and compilation schemes, and thus provides a ba- sis for reasoning about staged \ncomputation in general. Acknowledgments We would like to thank Rowan Davies, Olivier Danvy, and the anonymous \nreviewers for their comments. References UW-CSE97-03-03, Department of Computer Science, University of \nWashington. Jarring, U. and W. L. Scherlis (1986, 21-24 January). Compilers and staging transformations. \nIn Confer-ence Record of POPL 86: The thth ACM SIGPLAN-SIGACT Symposium on Principles of Programming \nLanguages, pp. 86-96. Jones, N. D., C. K. Gomard, and P. Sestoft (1993). Par-tial Evaluation and Automatic \nProgram Generation. Prentice-Hail. Keppel, D., S. J. Eggers, and R. R. Henry (1993, Novem- ber). Evaluating \nruntime-compiled value-specific op-timizations. Technical Report 93-11-02, Department of Computer Science \nand Engineering, University of Washington. Kohlbecker, E., D. P. Friedman, M. Felleisen, and B. Duba \n(1986). Hygienic macro expansion. In Pro-ceedings of the 1986 ACM Conference on Lisp and Functional Programming, \npp. 151-159. Auslander, J., M. Philipose, C. Chambers, S. Eggers, and B. Bershad (1996, May). Fast, \neffective dynamic com-pilation. In Proceedings of the ACM SIGPLAN 96 Conference on Programming Language \nDesign and Im- plementation, Philadelphia, Pennsylvania. Consel, C. and F. No61 (1996, 21-24 January). \nA general approach for run-time specialization and its applica-tion to C. In Conference Record of POPL \n96: The 23 d ACM SIGPLAN-SIGACT Symposium on Prin-ciples of Programming Languages, pp. 145-156. Cousineau, \nG., P.-L. Curien, and M. Mauny (1987). The categorical abstract machine. Science of Computer Programming, \n173-202. Davies, R. (1996, 27-30 July). A temporal-logic approach to binding-time analysis. In Proceedings, \n1 lth Annual IEEE Symposium on Logic in Computer Science, New Brunswick, New Jersey, pp. 184-195. IEEE \nComputer Society Press. Davies, R. and F. Pfenning (1996, 21-24 January). A modal analysis of staged \ncomputation. In Conference Record of POPL 96: The 231d ACM SIGPLAN-SIGACT Symposium on Principles of \nProgramming Languages, pp. 258-270. Engler, D. R., W. C. Hsieh, and M. F. Kaashoek (1996, 21-24 January). \nC: A language for high-level, efficient, and machine-independent dynamic code generation. In Conference \nRecord of POPL 96: The 23 d ACM SIGPLAN-SIGACT Symposium on Principles of Pro- gramming Languages, pp. \n131-144. Engler, D. R., D. Wallach, and M. F. Kaashoek (1995, March). Efficient, safe, application-specific \nmessage process-ing. Technical Memorandum MIT/LCS/TM533, MIT Laboratory for Computer Science. Gliick, \nR. and J. Jorgensen (1995). Efficient multi-level generating extensions. In Programming Languages, Implementations, \nLogics and Programs (PLILP 95), volume 1181 of Lecture Notes in Computer Science. Springer-Verlag. Grant, \nB., M. Mock, M. Philipose, C. Chambers, and S. J. Eggers (1997). DyC: An expressive annotation-directed \ndynamic compiler for C. Technical Report Lee, P. and M. Leone (1996, May). Optimizing ML with run-time \ncode generation. In Proceedings of the ACM SIGPLAN 96 Conference on Programming Language Design and Implementation, \nPhiladelphia, Pennsylva-nia, pp. 137-148. Leone, M. and P. Lee (1998). Dynamic specialization in the \nFabius system. ACM Computing Surveys 1998 Symposium on Partial Evaluation. To appear. Massalin, H. and \nC. Pu (1989, December). Threads and input/output in the Synthesis kernel. In Proceedings of the 12th \nACM Symposium on Operating Systems Principles, pp. 191-201. McCanne, S. and V. Jacobson (1993, January). \nThe BSD packet filter: A new architecture for user-level packet capture. In The Winter 1993 USENIX Conference, \npp. 259-269. USENIX Association. Pike, R., B. Locanthi, and J. Reiser (1985, February). Hardware/software \ntrade-offs for bitmap graphics on the Blit. Software -Practice and Experience 15(2), 131-151. Sirer, \nE. G., S. Savage, P. Pardyak, G. P. DeFouw, and B. N. Bershad (1996, February). Writing an operating \nsystem with Modula-3. In The Inaugural Workshop on Compiler Support for Systems Software, pp. 134-140. \nTaha, W. and T. Sheard (1997). Multi-stage programming with explicit annotations. In Partial Evaluation \nand Semantics-Based Program Manipulation, Amsterdam, The Netherlands, June 1997, pp. 203-217. Wickline, \nP., P. Lee, F. Pfenning, and R. Davies (1998). Modal types as staging specifications for run-time code \ngeneration. ACM Computing Surveys 1998 Sympo-sium on Partial Evaluation. To appear. \n\t\t\t", "proc_id": "277650", "abstract": "This paper presents a typed programming language and compiler for run-time code generation. The language, called ML', extends ML with modal operators in the style of the Mini-ML'<inf><i>e</i></inf> language of Davies and Pfenning. ML' allows programmers to use types to specify precisely the stages of computation in a program. The types also guide the compiler in generating target code that exploits the staging information through the use of run-time code generation. The target machine is currently a version of the Categorical Abstract Machine, called the CCAM, which we have extended with facilities for run-time code generation.This approach allows the programmer to express the staging that he wants directly to the compiler. It also provides a typed framework in which to verify the correctness of his staging intentions, and to discuss his staging decisions with other programmers. Finally, it supports in a natural way multiple stages of run-time specialization, so that dynamically generated code can be used in the generation of yet further specialized code.This paper presents an overview of the language, with several examples of programs that illustrate key concepts and programming techniques. Then, it discusses the CCAM and the compilation of ML' programs into CCAM code. Finally, the results of some experiments are shown, to demonstrate the benefits of this style of run-time code generation for some applications.", "authors": [{"name": "Philip Wickline", "author_profile_id": "81100482913", "affiliation": "School of Computer Science, Carnegie Mellon University, Pittsburgh, Pennsylvania", "person_id": "P226559", "email_address": "", "orcid_id": ""}, {"name": "Peter Lee", "author_profile_id": "81100384353", "affiliation": "School of Computer Science, Carnegie Mellon University, Pittsburgh, Pennsylvania", "person_id": "PP39040384", "email_address": "", "orcid_id": ""}, {"name": "Frank Pfenning", "author_profile_id": "81100157780", "affiliation": "School of Computer Science, Carnegie Mellon University, Pittsburgh, Pennsylvania", "person_id": "PP39030152", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/277650.277727", "year": "1998", "article_id": "277727", "conference": "PLDI", "title": "Run-time code generation and modal-ML", "url": "http://dl.acm.org/citation.cfm?id=277727"}