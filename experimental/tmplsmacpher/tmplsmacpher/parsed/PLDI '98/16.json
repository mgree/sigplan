{"article_publication_date": "05-01-1998", "fulltext": "\n Using Integer Sets for Data-Parallel Program Analysis and Optimization* Vikram Adve John Mellor-Crummey \nDepartment of Computer Science Rice University {advejohnmc}@cs.rice.edu Abstract In this paper, we describe \nour experience with using an ab- stract integer-set framework to develop the Rice dHPF com-piler, a compiler \nfor High Performance Fortran. We present simple, yet general formulations of the major computation partitioning \nand communication analysis tasks as well as a number of important optimizations in terms of abstract \nop-erations on sets of integer tuples. This approach has made it possible to implement a comprehensive \ncollection of ad- vanced optimizations in dHPF, and to do so in the context of a more general computation \npartitioning model than pre-vious compilers. One potential limitation of the approach is that the underlying \nclass of integer set problems is fun- damentally unable to represent HPF data distributions on a symbolic \nnumber of processors. We describe how we ex- tend the approach to compile codes for a symbolic number \nof processors, without requiring any changes to the set formu- lations for the above optimizations. We \nshow experimen-tally that the set representation is not a dominant factor in compile times on both small \nand large codes. Finally, we present preliminary performance measurements to show that the generated \ncode achieves good speedups for a few bench- marks. Overall, we believe we are the tist to demonstrate \nby implementation experience that it is practical to build a compiler for HPF using a general and powerful \ninteger-set framework. 1 Introduction Data-parallel languages such as High-Performance Fortran (HPF)[20] \naim to provide a simple, portable, abstract pro-gramming model applicable to a wide variety of parallel \nsys-tems. To achieve wide acceptance, a data-parallel language requires paraIIeIizing compilers that \ncan provide consistently high performance for a broad class of applications. Cur-rent commercial compilers \nfor HPF [8, 12, 141 typically yield *This work has been supported in part by DARPA Contract DABT63-92-C-0038, \nthe Texas Advanced Technology Program Grant TATP 003604-017, an NSF Research Instrumentation Award CDA-9617383, \nand sponsored by DARPA and Rome Laboratory, Air Force Materiel Command, USAF, under agreement number \nF30602-96-1-0159. The U.S. Government is authorized to reproduce and distribute reprints for Governmental \npurposes notwithstanding any copyright annotation thereon. The views and conclusions contained herein \nare those of the authors and should not be interpreted as representing the official policies or endorsements, \neither expressed or implied, of DARPA and Rome Laboratory or the U.S. Government. P*mission lo maks digital \nor h.rd copi.. of .II or P.n of chi. work for P.r.on.1 or cl...room U.. i. pr.nt.d without 1.. provid.d \nthat COPi.. .r. not mad. or di8tribut.d for profit or cammsrcial .dv.n. Wl* *t-d that co~i.. b..r thi. \nnotic. .nd th. tull cit.tiwr .n th. tint p.0.. 70 COPY othw*ri... lo r.pub1i.h. lo pw 0 ..vv.~. 0r to \n*.di**ribut. 10 li.1.. roquir.. prior .p.cific permi..ion and/or . f... bIGPlAN 98 Montrd. C.n.d. Q 1998 \nACM 0-89791~987-4/98/0008...$6.00 widely varying performance for different applications, even among %egular \ndata-parallel applications on message pass- ing systems. (Regular applications are those with statically \nanalyzable data access patterns and, usually, well-balanced computational costs.) Most research and commercial \ndata-parallel compilers to date [22, 7, 21, 29, 16, 11, 12, 14, 8, 13, 32, 331 (including the Rice Fortran \n77D compiler) perform communication analy-sis and code generation by considering specific combinations \nof the form of references, data layouts and computation par-titionings. Such case-based analysis has \nbeen the principal implementation technique for data-parallel compilation sys-tems because it provides \na practical and conceptually simple strategy for developing compilers, and it can be relied on to yield \nexcellent performance for common cases. For example, even though Hiranandani et al. [16] and Koelbel \n[21] both formulated computation partitionings and communication analysis in terms of abtract integer-set \noperations (assum-ing the owner-computes rule [26]), they used case-based ap-proaches in their implementations \nof the Fortran D and Kali compilers. The case-based approach, however, provides poor performance for \ncases that have not been explicitly consid-ered. Perhaps more significantly, such case-based compilers \nrequire a relatively high development cost for each new opti-mization because the analysis and code generation \nfor each case is handled separately. This makes it difficult to achieve wide coverage with optimizations \nto offer consistently high performance. A few researchers have used analysis techniques based on linear \ninequalities, which enable a more general and flexible approach than case-based analysis for implementing \ndata parallel languages [3, 6, 21. This work has focused on com- puting local communication and iteration \nsets, and perform- ing code generation from these sets using Fourier-Motzkin elimination (FME) [4]. Amarasinghe \nand Lam [2] also de- scribe how inequalities and FME can support array dataflow analysis and a few specific \ncommunication optimizations. A limitation of the linear inequality representations used by these groups \nis that they cannot represent general non-convex sets. These representations therefore preclude op-timizations \nsuch as coalescing communication for arbitrary affine references [2], non-local index-set splitting [21], \nor the use of a general computation partitioning model such as that used in our work. Recently, Pugh \net al. have reported important progress in developing efficient algorithms for manipulating and gen- \nerating code from general non-convex sets [25, 181. This provides a potentially important new tool for \ndeveloping parallelizing compilers, but leaves open two key questions: Is it practical to use these techniques \nfor real programs? And what challenges must be addressed in formulating and implementing the key analysis \nand optimizations problems using such an approach? In this paper, we describe our experience with using \na general integer set representation to develop the Rice dHPF compiler, a compiler for High Performance \nFortran. In par- ticular, the compiler is based on an abstract equational framework that expresses data \nparallel program analyses and optimizations in terms of operations on symbolic in-teger sets. Using this \nframework, we have devised and im- plemented simple, concise, yet general, equational formula-tions of \nthe major partitioning and communication analyses as well as a number of important optimizations. Because \nof the simplicity of these formulations, it has been possible to implement a comprehensive collection \nof advanced opti-mizations in dHPF. In most cases, our implementations of the optimizations are more \ngeneral than in previous compil-ers. Furthermore, it has been possible to support all these analyses \nand optimizations in the context of a more gen-eral computation partitioning (CP) model than that used \nin previous compilers. datak : the index set of an array of rank k, k 2 0 bopk : the iteration space \nof a loop nest of depth k, k > 0 prock: the processor index space in a processor array of rank k, k > \n1 Layout: pro% -+ datak: {[p] -+ H : array element +&#38; E datak is allocated to processor p E proc,} \nRefMap: 1OOpk + data,: {a+ H : array element a E datak is referenced jn iteration i-E joopk} CPMap: proc \n + 1oOpk : {[tl] + u : t tS a ement instance i E loopk is assigned to processor p E pro&#38;} Figure \n1: Primitive sets and mappings for compiling data-parallel programs. The specific contributions of this \nwork are as follows. First, we describe the formulation and implementation of key data-parallel program \nanalyses and several ontimiza-tions using integer-set-equations . These include: - l static loop partitioning \nand communication analysis for a general computation partitioning (CP) model; l communication vectorization \nfor arbitrary regular com-munication patterns; b message coalescing for arbitrary affine references to \nan array; l a powerful class of loop-splitting transformations that support several optimizations, including \noverlapping communication with computation ruithin a single loop- nest, and reducing the overhead of \naccessing buffered non-local data; and . a combined compile-time/run-time algorithm to re-duce explicit \ndata copies for a message. All of these analyses and optimizations have been imple-mented in the dHPF \ncompiler. Of these, the CP model, the loop-splitting based optimizations, the analysis for buffering \nnon-local data, and message coalescing are all more general than in previous compilers. Second, a fundamental \nlimitation of a general integer-set based representation is that set constraints containing products \nof integer variables yield problems that are unde-cidable [17]. The primary source of such symbolic product \nterms is the HPF distribute directive with unknown pro-cessor counts or block sizes. We describe a natural \nexten-sion to our framework based on a virtual processor model that supports these symbolic terms without \nrequiring any changes to the set formulations for the above optimizations. A key component of this extension \nis an integer-set algo-rithm and associated code-generation strategy that elimi-nates or reduces nmtime \nchecks by restricting the compu-tation and communication to the active virtual processors. This extension \nmakes it possible to compile HPF programs for an unspecified number of processors, and we present ex-perimental \nresults to show that there is little or no difference in compile-time for a symbolic than for a constant \nnumber of processors, even on fairly large and complex codes such as the NAS SP application benchmark. \nFinally, we show experimentally that the set represen-tation is not a dominant factor in compile times \non both small and large codes. We also present preliminary results to show that the set-based implementation \nof the optimiza- tions is practical and achieves good speedups for realistic kernels and benchmarks. \nThe next section describes our integer-set based equa-tional framework. Section 3 describes the formulation \nof analyses and optimizations. Section 4 describes the frame- work extension for a symbolic number of \nprocessors. Sec-tion 5 describes a few key implementation considerations. Section 6 examines the compile-time \ncost of using this ap-proach. Section 7 presents preliminary results on the perfor- mance of benchmark \ncodes. Section 8 reviews related work. Section 9 states our conclusions. 2 A Set Framework for Data-Parallel \nCompilation An integer Ic-turle is a point in 2 ; a tuple space of rank k is a subset of 2 . Any compiler \nfor a data-parallel language based on data distributions, such as HPF, operates primar-ily on three types \nof tuple spaces, and the three pairwise mappings between these tuple spaces ([2, 15, 211). These are \nshown in figure 1. Scalar quantities such as a data set for a scalar, or the iteration set for a statement \nnot enclosed in any loop are handled uniformly as tuples of rank zero. Hereafter, the terms %rray and \niterations of a state- ment imply scalars and outermost statements as well. The sets loop and proc and \nthe mappings Layout and RefMap are computable directly from the source program and form the primary inputs \nfor further analyses. Figure 2 illustrates these primitive sets and mappings for an HPF code fragment. \nLayout is computed from Align, which rep-resents the alignment of an array with a template, and Dist, \nwhich represents the distribution of a template on a physi- cal processor array. The iteration set, loop, \nfollows directly from the loop bounds. The ON-HOME CP notation and con-struction of CPMap are described \nin the following section. To implement analysis and optimization in dHPF as op- erations on these symbolic \nsets and mappings, we require an integer set package that supports operations including intersection, \nunion, difference, domain, range, composition, and projection. For this purpose, we use the Omega Library \ndeveloped by Pugh et al at the University of Maryland [17]. Omega supports representation and manipulation \nof (po-tentially non-convex) integer sets described by Presburger formulae, using algorithms based on \nFourier-Motzkin Elimi-nation (FME) [25]. This approach has both advantages and We use names with lower-case \ninitial letters for tuple sets and upper-case letters for mappings respectively. real A(O:99,100),B(100,100) \nprocessors P(4) template T(100,lOO) align A(Q) with T(i+l,j) align B(Q) with T(*,i) distribute t(*,block) \nonto P read(*), N doi= l,N doj=2,N+l ! ONHOME B(j-1,i) A(ij) = B(j-1,i) enddo enddo symbolic pl-OC Align* \nAligna Distr Layout* Layoutn loop CPRef CPMap N = UP1 : 1 5 P 5 41 = {[a1,a1] + [t1,tz] : t - = {[bl \nbz] --t [t t ] t:=bl) = {[t&#38;]--t &#38;i5;+1<12 = Dist,i o Aligni = {[p]+ [a~,a2]:maz(25p+ 0 2 a1 \n2 99) = Dist, o Align; = {b] + [bl, bz] : mao(25p + 1,1) 1 5 b2 5 100) = {[11,/2] : 1 2 I1 2 N A 2 5 \nI2 5 a1 + 1 A tz = az} 5 25(p+l)A O_<p<3} 1,l) 5 a2 5 min(25p+ 25,100) A 5 bl < min(25p + 25,100) A N+ \n1) = ([II, [2] + [bl, bz] : b2 = 11 A bl = I2 -1) = Layoutn o CPRef- nrkngeloop = {[p] -+ [11,12]: 1 \n5 l1 5 min(N,lOO) A mas(2,25p+ 2)s 12 5 min(N+l,101,25p+26)} Figure 2: Construction of primitive sets \nand mappings for an example program. Al&#38;in, Alignn, and Distr also include constraints for the array \nand template ranges, but these have been omitted here for brevity. (See Appendix A for notation.) disadvantages \ncompared to simpler set representations such as extended Rectangular Sections [16] or Data Access De-scriptors \n[5]. In Section 4, we discuss the tradeoffs that arise and describe how we accommodate one of the key \nlimitations of FME. 3 Optimizations using the Integer Set Framework In this section, we describe analysis \nand optimizations using the integer set framework described in the previous section. 3.1 Computation \nPartitioning Model A computation partitioning (CP) for a statement specifies which processor(s) must \nexecute each dynamic instance of the statement. The compiler must support a flexible class of computation \npartitionings to enable choice of a partitioning well-suited to an application s needs. Research and \ncommercial HPF compilers primarily use the owner-computes rule [26] to assign CPs to statements. This \nrule specifies that a computation is executed by the owner of the value being computed. This rule, as \nwell as other variants used in some compilers (e.g., decHPF [14] and SUIF [2]) can be expressed in terms \nof the processor(s) that own a particular set of data elements. In particular, for a statement enclosed \nin a loop nest with index vector i, and for some variable A, the CP ONHOME{A(~(~))}, speci-fies that \nthe dynamic instance of the statement in iteration i will be executed by the processor(s) that own the \narray element(s) A(f(i)). This set of processors is uniquely spec-ified by subscript vector f(d and the \nlayout of array A at that point in the execution of the program. The SUIF com-piler [2] further restricts \nall statements in a loop to have the same computation partition. The dHPF compiler supports a more general \nCP model in which a CP for a statement can be specified as the owner of one or more arbitrary data references, \nand each statement in a program may have its own CP. A statement s CP is specified by a union of one \nor more ON-HOME terms: CP : U~=:ONHOME{A,(~~(~))}. This implicit representation of a computation partitioning \nin dHPF supports arbitrary index expressions or any set of values in each index position in f,(i). We \nconvert the implicit CP form into an explicit integer tuple mapping, CPMap. This is possible when each \nsub- script expression in fJ(i) is an affine expression of the index variables, i, with known constant \ncoefficients, or is a strided range specifiable by a triplet 2b:ub:step with known constant step. The \noverall mapping is a union of mappings for the individual ONHOME terms: j=n CPMap = U (LayoutAj oRefr \n) n loop range j=l (The operator o is defined in Appendix A.) Here, the map- ping for a single term \nONHOME{Aj(fj(i))} is a composition of the layout and reference mappings, restricted in range to the loop \nindex space. CPMap explicitly specifies the pro-cessor assignment for the instance of a statement in \nloop iteration i. There are two key challenges in using such a general CP model: (a) generating efficient \ncode to implement a chosen static loop partitioning, and (b) deriving symbolic data and processor sets \nfor communication. We discuss code genera- tion to support loop partitioning briefly below; computing \ncommunication sets is discussed in the next section. The compiler generates a partitioned single program \nmul-tiple data (SPMD) node program as specified by the CP for each statement. Briefly, the compiler uses \na hierarchi- cal code generation strategy that considers each scope (i.e., loop, conditional, or procedure) \nin isolation [l]. For effi-ciency, code generation proceeds in a depth first traversal over the tree \nof scopes in a procedure. For each scope, we compute a vector of sets, CPMap(m), one for each statement \ngroup in the scope. (A statement group is a sequence of con- secutive statements with identical CPs. \n{rn} is a singleton set representing the processor index vector for the fixed pro-cessor myid.) We then \nuse Kelly, Pugh, and Rosser s algo-rithm for multiple-mappings code generation [18] (defined in Appendix \nB) to compute loop bounds and guards that enu-merate the SPMD iteration space for each statement group \nin the scope. To avoid adding the same guards at multiple levels, we provide the iteration set of the \nimmediately enclos-ing scope statement as the known parameter to Codegen, because those constraints will \nbe enforced when generat-ing code for the outer scope. In Section 5, we describe the techniques we use \nin the implementation to minimize code replication during CP code generation. Inputs: sets of read and \nwrite references in a single logical communication event RefMap, : map representing reference r, \\dr \nE Refs,,,d U Refswrite CPMap, : computation partitioning map for reference r LayoutA : layout of the \ncommon referenced array, denoted A v, : loop-level of innermost loop enclosing communication for r, \nafter vectorieation; J1 . . . Jv, are index variables of enclosing loops Ref.&#38;d, Refs,,it, : m : \nprocessor index vector for the representative processor m or myid Algorithm: CPMapy = CPMap, nronge{ljl,. \n. . ,jn] : j, = JI A . . . j, = &#38;, A -00 < jv,sl < 03 A . . .} (1)  DataAccessedt = U CPMapy o RefMap,, \n(hereafter, t E {read,write}) (2) rERefst NLDataAccessedt = off-processor array elements a referenced \nby processor p (3) {rp1+ k&#38;l: -1 DataAccessedt -LayoutA if t = read = DataAccessedt I-I LayoutA \nif t = write i nlDataSett(m) = NLDataAccessedt((d)  NLCommMapt(m) = {[e] + w : off-processor elements \nreferenced by proc. m and owned by proc. e } (4)  = LayoutA n,,,p,nlDataSett(m)) (5) LocalCommMapt(m) \n= {[p] --t w : array elements owned by proc. m to be communicated with proc. p } = DataAccessedt nrange \nLayoutA({E*}) (6) SendCommMap(m) = LocaICommMap,,,~(m) U NLCommMap,,it,(m) (7) RecvCommMap(m) = NLCommMap,,,d(m) \nU LocalCommMap,,;t,(m) Figure 3: Equations for computing communication sets 3.2 Implementing Explicit \nCommunication For message-passing systems, data-parallel compilers must compute the data to be exchanged \nbetween processors, and generate code to pack, communicate, unpack, and reference the non-local data. \nMessage vectorization is a fundamental optimization for such systems which reduces communication frequency \nby hoisting communication for a reference out of one or more enclosing loops. To vectorize communication, \nthe compiler must compute the set of data to send between each pair of processors; such communication \nsets depend on the reference, layout, and computation partitioning. Mes-sage coalescing combines messages \nfor multiple references to eliminate redundant communication and further reduce the number of messages. \nCoalescing requires unioning com-munication sets and can require sophisticated techniques to enable efficient \naccess to buffered non-local data. Early phases in dHPF identify potentially non-local ref-erences that \nmight access off-processor data, compute where to place communication for each reference (using depen- \ndence and optionally dataflow analysis to vectorize commu- nication), and which sets of references can \nhave their com- munication coalesced. dHPF assumes that all data is com- municated to and from its owner(s) \nonly, as defined by the data layout directives. A read reference is non-local if the location is not \nowned by the processor executing the read. A write reference is non-local if the location is owned by \none or more processors besides the processor executing the write. (These definitions are equivalent if \ndata is not replicated.) of communication, we compute the communication sets for each logical communication \nevent using the inputs and set equations shown in Figure 3. The goal of these equations is to compute \ntwo maps, SendCommMap(m) and RecvCom-mMap(m), for the representative processor m = myid. The maps respectively \nspecify the data that processor m must send to each other processor p and the data that processor m must \nreceive from each oth-&#38; processor p. The key operations are as follows. Steps2 and 2 compute the \ntwo maps DataAccessedt, t E {read, write}, which spec-ify the entire set of data accessed by each processor \np_, via all read and write references in all iterations of the loops out of which communication has been \nvectorized. Then (step 3), the non-local data accessed by read references is the dif- ference of DataAccessedt \nand the local data owned by each processor, LayoutA. The non-local data accessed by write references \nis the intersection of DataAccessedt and the data owned by any other processors. Note that the read and \nwrite equations are equivalent for the common case that each ar- ray element is owned by a single processor, \ni.e., where the data layout is not replicated.2 We convert this map to a set nlDataSett (m) specifying \nthe non-local data accessed by the fixed processor m. We then compute two maps describing the non-local \nand local data (w.r.t. to the fixed processor m) that must be communicated with each other processor \np (steps 4,5). Re-stricting the range of Layout.4 to the non-local data set, nIDataSett(m), gives the \nnon-local data referenced by pro- We refer to the sequence of messages required for a set of In this \ncase, in fact, we can skip step (3) and simply use coalesced references as a single logical communication \nevent. NLDotaAccessed* = DataAccessedt, because steps (4) and (5) will restrict communication to non-local \ndata. During code generation, Given sets of coalesced references and the placement level we ensure that \na processor does not communicate with itself. cessor m and owned by each other processor p. Restricting \nthe range of DataAccessedt to the local sect,ion owned by m gives the local data owned by m and accessed \nby each other processor p Finally, from LocalCommMapt and NLCommMapt, t E {read, umite}, we compute the \ndata to send to and receive from each processor (steps 6,7). Later, we generate code from these maps \nto pack and unpack the data at the sending and receiving ends. The equations presented here unify the \nhandling of both communication vectorization and coalescing for arbitrary references and communication \npatterns. This abstract for-mulation of static communication analysis has greatly sim-plified the core \nof the dHPF compiler and enabled efficient handling of general classes of computation partitionings and \naffine references. 3.3 Recognizing in-place communication Common MPI implementations permit data to \nbe sent or received in-place (avoiding an explicit data copy) when the address range of the data is contiguous. \nTo increase the likelihood that communication can be performed in-place, we develop a combined compile-time/run-time \nalgorithm for recognizing contiguous data based on the capability of gen- erating code from integer sets. \nFORTRAN arrays are stored in column-major order. Ac-cordingly, a rectangular communication set C for \ndata in an array A with n dimensions represents contiguous data if there exists a k such that for the \ndimensions 1 5 i < k, the set spans the full range of array dimension i, along di-mension k the set has \na contiguous index range, and in the low-order dimensions k + 1 2 j 2 n, the set contains a single index \nvalue. Let A represent the local index set of the array, and define SC,> to be the projection (i.e., \nrange) of set S in dimension i, 1 5 i 5 rank(S). Then the above condition can be formalized as: 3k s.t. \n1 5 k 5 n A /\\j::(C<i> = A<;>) A IsConvex(C<k>) A /\\:f~+,lsSingleton(C<+) To permit runtime evaluation \nwhen necessary, we reduce each of the tests to a satisfiability question for which we can synthesize \nan equivalent conditional to be tested at run time (if it cannot be proved at compile-time). The predicate \nIsConvex reduces to testing if the set ConvexHuIl(S) -S is empty. The predicate IsSingleton also reduces \nto a satisfiability test, but we omit the details here. To avoid evaluating O(n ) predicates at compile-time, \nwe use a single scan of the dimensions (leftmost first) to find the first dimension k for which C<k> \n# &#38;k>, and check the predicates for k . . . n. If in-place communication can-not be proven at compile \ntime, we synthesize code from the unproven predicates to repeat this scan and check at run-time, when \nit can be done precisely by evaluating at most n + 2 predicates. In general, this test can be performed \nmuch faster than packing a buffer of modest size. This ap-proach, based on explicit integer sets, enables \nus to exploit in-place communication for arbitrary communication sets, independent of data layouts and \ncommunication patterns. There are currently two limitations of our implementa-tion of this analysis in \nthe dHPF compiler. First, we apply the compile-time test for in-place communication only to communication \nsets with only a single conjunct. Our com-piler support can be generalized straightforwardly to handle \ndisjoint disjunctions as well when the satisfiability condi-tions on all conjuncts are mutually exclusive. \nSecond, the code generation for runtime evaluation of these predicates is currently incomplete. 3.4 \nLoop Splitting Loop splitting (or non-local index set splitting) is a power- ful but complex transformation \nthat has been proposed to ameliorate two types of communication overhead: the cost of referencing buffered \nnon-local data, and the latency of com- munication [21]. Both techniques involve splitting a loop to \nseparate the iterations that access only local data from those that may access non-local data. First, \nbuffer access costs arise when local and non-local data are stored sepa-rately, and the correct location \nmust be chosen with a run- time check on each reference. After splitting local and non-local iterations, \nno checks are needed for references in the local iterations. Second, the latency of communication can \nbe (partly) hidden by splitting because communication re-quired for non-local iterations can be overlapped \nwith com-putation of the local iterations. The only implementation of this transformation we know of \nis in Kali [21], where the authors used set equations to ex- plain the optimization but used case-based \nanalysis to derive the iteration sets for special cases limited to one-dimensional distributions. This \napproach is only practical for a small number of special cases. We extend the equations in [21] to apply \nto arbitrary sets of references, and any CP in our CP model. We first describe loop-splitting for communication \noverlap, because it sub-sumes splitting for buffer access. We apply loop-splitting to any perfect loop-nest \n(not necessarily innermost) containing at least one partitioned loop and having no dependences that prevent \niteration reordering. Since, in dHPF, write refer-ences may be non-local, we split the set of iterations \nof such a loop nest into four sections: those that access only local data (localIters), and those that \nonly read, only write, or read and write non-local data (nlROIters, nlWOIters and nlRWIters respectively). \nThese sets are computed as shown in Fig-ure 4(a) for a loop-nest containing one or more potentially non-local \nreferences. (The equations shown are applied sep-arately to each statement group in the loop nest.) The \nkey steps are to compute IocalDataAccessed, (analogous to computing nIDataSett in Figure 3), and then \nlocahters, which are the iterations in which reference r accesses local data. The desired four sets are \nthen computed by grouping locaRters, by read and write references. We schedule the communication and \ncomputation for this loop nest in the sequence shown in Figure 4(b). When NLRW is non-empty, we can overlap \neither read or write latency, but not both; a simple heuristic could be used to choose between the two. \nThe sequence in the figure overlaps read latency with NLWOIters and LocaRters. When NLRW is empty, however, \nthe latency for writes as well as reads can be overlapped with LocalIters by placing the SEND for non-local \nwrites immediately after NLWOIters. This form of splitting subsumes splitting for non-local buffer access. \nReferences in the local iterations do not need buffer-access checks, and a reference r in a non-local \nloop section (e.g., NLROIters) also does not need such checks if nlROIters C nlIters, = cpIterSet -locaRters,, \nbecause the reference will access only non-local data in these iterations. Code generation for loop splitting \nsubsumes the opera-tion of partitioning the loop by reducing the loop bounds, since each of the four \nloop sections is a subset of cpIterSet for Inputs: CPMap : Common CP map for each statement in given \nstatement group SG Refsread, Refswrite : non-local read and write references in SG RefMap, : map representing \nreference r, Vr E Refsread U Refswrite Layout.4 : layout of the common referenced array, denoted A : \nprocessor index vectors as in Figure 3 %P?i?iA Algorithm: cpIterSet = CPMad{ml) dataAccessed, = RefMap,(cpIterSet) \n dataaccessed, n LayoutA ({rnA}) if t = read localDataAccessed, = datahccessed, -LayoutA(~{&#38;}) if \nt = write locaRters, = Ref,- (localDataAccessed,) nlReadIters = cpIterSet -n IocalIters, SEND data for \nnon-local reads 1ERefsre.d n execute NLWOIters execute LocalItersnlWriteIters = cpIterSet -locaRters, \nRECV data for non-local reads rERefswrite execute NLROIters U NLRWIters locaRters = cpIterSet n n locaRters, \nSEND data for non-local writes RECV data for non-local writes nlRWIters = nlReadIters n nlWriteIters \nnlROIters = nlReadIters -nlWriteIters nlWOIters = nlWriteIters -nlReadIters  (a) Computing local/nonlocal \niteration sets (b) Scheduling loop iterations Figure 4: Loop splitting to overlap communication and computation. \nthe statement group. The code generation is performed as part of the hierarchical code generation framework \nfor com-putation partitioning described briefly in Section 3.1 [l]. 4 Extensions for Symbolic Distribution \nParameters A variety of set representations can be used to implement the set framework, with a wide range \nof expressiveness and efficiency. We observe that the primary benefit of using the integer set approach \nis that it enables simple but rigorous formulations of the key data-parallel optimization problems. This \nbenefit is somewhat independent of the generality of the underlying set representation. The Omega library \nprovides a powerful integer set representation based on FME, and the library has been invaluable in developing \nand prototyping the set-based formulations of the optimizations. Neverthe-less, FME has two potential \ndisadvantages which we address as follows. First, algorithms based on FME have poor worst-case performance. \nA goal of our research has been to determine whether or not this general approach is practical for com-mercial \nHPF compilers. There is some previous evidence that poor cases may be unlikely to occur in practice [25]. \nAlso, changing the formulation of a problem can be ex-tremely effective in avoiding complex sets, and \nhas improved running times by more than an order of magnitude in some cases. In practice, we have not \nfound the cost of the set framework to be a problem so far, as shown in Section 6. Nevertheless, if the \napproach still proves impractical, we can use one of two alternatives (without sacrificing the benefits \nof the simple equational formulations). We can use a simpler set representation such as Data Access Descriptors \n[5], or we can use competitive algorithms that limit the time spent on any single optimization or code \ngeneration problem. Both approaches would fall back on runtime techniques (such as inspector-executor \n[27]) which are required in any case for irregular problems. A second, fundamental limitation of Omega \nis that it does not permit symbolic coefficients in affine constraints, because multiplication of integer \nvariables renders the un-derlying class of integer sets undecidable [lo]. Symbolic mul-tiplication is \nrequired to represent a symbolic stride, k, for example, {[i] : 1 5 i < N A 3a s.t. i = ka + 1). In compil- \ning an HPF program, symbolic strides arise for any type of HPF distribution when the number of processors \nis unknown at compile time, for the cyclic(k) distribution with unknown k, and for loops with unknown \nstrides. We have extended our framework to accommodate the limitation on symbolic number of processors \nand k, as described below. Loops with unknown strides are not supported by our framework, and would have \nto fall back on more expensive run-time tech-niques such as a finite-state-machine approach for comput-ing \ncommunication and iteration sets (for example, [19]). 4.1 An Optimized Virtual Processor Model To circumvent \nthe limitation on symbolic data distribution parameters, we use a standard technique that avoids rep-resenting \nthese parameters explicitly within the set frame- work, and instead incorporates them directly during \ncode generation [2, 131. We retine this technique to provide signif- Inputs: Same as in Figure 3 Algorithm: \nbusyVPSett = Domain(CPMap,), (hereafter, t E {read,urrite}) U rERefs: allNLDataSett = NLDataAccessedt(busyVPSett) \nVpsThatOwnNLDatat = Layouti:(allNLDataSett) VpsThatAccessNLDatat = Domain(NLDataAccessedt) activeSendVPSet \n= vpsThatOwnNLData,,,d U vpsThatAccessNLData,,it, activeRecvVPSet = vpsThatAccessNLData,..d U vpsThatOwnNLData,,it, \nreal A(l:lOO) processors PA(Pl,P2) template T( 100,100) align A(ij) with T(ij) distribute t(cyclic,cyclic) \n. . . do i = PIVOT+l, 100 do j = PIVOT+l, 100 ! ONHOME { A(i,j) } A(i,j) = . . . + A(PIVOTj) enddo enddo \nonto (a) Equations for computing vpArray PA loop CPMap RefMaPread RefMaPet, bUsyVPSet,,,d NLDataAccessed,,,d \nactiveSendVPset = = = = = = = = the active virtual processors {[v1,212] : 1 5 Wlr~2 < 1001 {[i,j] : PIVOT \n+ 1 2 i,j 2 100) {[v~,v~]+[i,j]:i=v~hj=v~hPIVOT<v~,v~~100) {[iAl + WVOT,jlI @(no non-local writes) {[v1,v2] \n: PIVOT < v1,v2 < 100) {[vlr v2] + [PIVOT,vz] : PIVOT {[vI,v~] : VI = PIVOT A PIVOT < < vl,v2 vl,v2 < \n5 100) 100) (b) Gauss parallel loop in HPF activeRecvVPset = busyVPSet,,,d (c) Active virtual processors \nin Gauss loop Figure 5: Active virtual processors for computing, sending and receiving icantly simpler \ncode for block distributions (by far the most common in practice). We also add an additional optimiza-tion \nto to eliminate or reduce the runtime overhead in the resulting code. Gupta et al. [13] apply a similar \nstrategy but their approach was based on detailed analysis of spe- cific cases. Instead, we describe \na general integer-set-based algorithm to perform this optimization. The VP model and optimization are \nas follows. To avoid representing the distribution explicitly, we re- place each physical processor array \nin our equations by a vir- tual processor (VP) array corresponding to using template indices (i.e., ignoring \nthe distribute directive) in dimensions where the block size or number of processors is unknown, but \nusing one virtual processor index per physical processor index in all other dimensions. (We do not require \ntemplate sizes to be known constants.) We make this replacement simply by constructing the Layout mapping \nas a map from VP indices to data elements. All the analyses described in the previous sections then operate \nunchanged on this virtual processor domain. During code generation for each specific problem (e.g., generating \na partitioned loop), we add extra enclosing loops that enumerate the VPs that are owned by Lhe physical \nprocessor myid. Also, when generating code for communication, we must aggregate the messages to all the \nVPs belonging to the same physical processor. These code generation steps are described in Section 4.2. \nThe refinement we use for bZock distributions is as fol- lows. Consider a template dimension of extent \nN that is block-distributed on a processor array dimension of extent P, and let B = [N/P] be the block \nsize. The precise representation of this distribution is {[t] -+ b] : Bp + 1 < t 2 Bp + B A 1 < t < N \nA 1 5 p 5 P}. In this case, only the product term Bp is not directly representable. We compute a distribution \nonto virtual processors as follows: {[t]-)[v]:ultiu+B-1AlltlNAlSuIN). Intuitively, this assumes that \nthe virtual processor v owns template elements [u, 2, + B -11. Each physical processor p is mapped to \na unique virtual processor u = BP+ 1. However, any other value of w represents a fictitious virtual processor \nnot mapped to any physical processor. The special physical processor id m used in Figures 3 and 4 is \nreplaced by its vir- tual processorid w,,,, = Bm; + 1. Therefore, the generated SPMD code is automatically \nparameterized by urn instead -of m. There are two key advantages in using this refinement for the 6Zock \ndistribution, and both are due to the property that there is a single virtual processor per physical \nproces-sor. First, this property implies that no additional virtual processor loops are required. In \nfact, h always represents a true physical processor (myid), and therefore a block distri-bution does \nnot require any additional changes to the com-putational loops in the generated SPMD code (even with \ntransformations such as non-local index set splitting). Sec-ond, the above property implies that the \ncommunication sets capture the entire section that must be communicated to each physical processor (for \neach block dimension). This enables some optimizations and simplifies code generation. For example, if \nall array dimensions are block-distributed, the equations in Section 3.3 can be applied to the commu-nication \nset to determine whether in-place communication is feasible. For code generation, the only additional \nstep required is to ensure that communication is not attempted with a fictitious virtual processor. This \nstep is described in Section 4.2. do v = vlb, vub do firstVP = vlb, min(vub, vlb+P) myVLB = ((activeSendLB-tLB)/P)*P \n+ m + tLB ! pack data for v pid = (firstVP-tlb) % P + tlb if (myVLB .It. activeSendLB) myVLB = myVLB \n+ P ! send data to v do v = firstVP, vp2, P do firstVP = vlb, min(vub, vlb+P) enddo ! pack data for v \npid = (firstVP-tlb) % P + tlb enddo do myVP = myVLB, activeSendUB, P ! send data to pid do vl = firstVP, \nvub, P enddo ! pack data for vl enddo enddo ! send data to pid enddo (a) Initial SEND code from (b) \nSeparating physical and virtual (c) Adding active virtual processor loop Domain(SendDataMap(m)) processors \n(final SEND code) Figure 6: Code generation for SEND with optimized virtual processor model For cyclic \nand cyclic(k) distributions, there are multiple virtual processors for each physical processor. However, \nnot all virtual processors owned by a physical processor are nec- essarily active in any particular operation \n(a partitioned computation, sending data, or receiving data). An addi-tional optimization step can be \nused to eliminate or reduce runtime overhead by restricting the virtual processor loop to the set of \nVPs owned by processor myid that are actu-ally active. Since such a set for a single physical processor \nwould not be directly representable, in general, we do this in two steps. We ftrst use the integer-set \nequations shown in Figure 5(a) to compute the set of active VPs across all processors for the problems \nof interest. Then, we generate a loop nest from this set and explicitly rewrite the loop bounds to restrict \nit to the active VPs owned by processor myid. First, the set of active VPs for any partitioned computa-tion, \ndenoted busyVPSet, is simply the domain of CPMap. (As in Figure 4, this must be applied for each statement \ngroup in a loop nest.) Second, for each logical commu-nication event, the active VPs that must send or \nreceive data can be computed directly from NLDataAccessedt, the map from processors to non-local data \nreferenced by each processor (t E {read, write}). This map is already com-puted for communication generation \n(Figure 3). The map is first used to compute the sets of all virtual processors that own non-local data \nand all those that access non-local data (vpsThatOwnNLDatat and VpsThatAccessNLDatat). These in turn \ndirectly provide the virtual processors that must be active in sending or receiving data. The results \nof these equations are illustrated for the Gaus- sian Elimination example in Figure 5(b,c), where the \nrefer- ence to the pivot row, A(PZVOT, j), requires off-processor data. The busyVPSet reflects that only \nVPs correspond-ing to the lower, right portion of the matrix A are active. activeSendVPSet and activeRecvVPSet \nindicate that only VPs owning elements in the pivot row (PIVOT) must send any data, but all VPs active \nin the computation (busyVPSet) must receive non-local data. In practice, we can generate code so that \nonly one VP per physical processor will receive each such element. 4.2 Code Generation using Virtual \nProcessors Given the active virtual processor sets computed above, a few conceptually simple steps are \nrequired to generate final SPMD code. For a computational loop nest, if the CPs of alI statement groups \nare described via block-distributed arrays, no addi-tional steps are required as explained earlier. If \nany array dimension used in a given CP has a cyclic or cyclic(k) dis-tribution, we must enclose the loop \nnest with one extra loop for each such dimension. We generate these loops as follows. We first generate \na loop nest to enumerate the elements of the set busyVPSet computed from the CP. For each loop in the \nnest, we then directly modify its bounds and stride to restrict the loop to the active VPs owned by processor \nm. For example, for the i dimension in Figure 5, the final VP loop would have 16 = (PIVOT/P) + P + m \n+ 1 + (m < PIVOT%P)?P : 0, ub = 100, and step = Pl. If non-local index-set splitting is applied to the \nloop nest, the same vir-tual processor loop nest would have to be wrapped around each loop nest section. \nFor communication code, we describe the steps required for the sending side; the receiving code is similar. \nThe steps are illustrated in Figure 6, assuming a 1D processor array with P processors, a template with \nlower bound tLB, and a cyclic distribution, First, we generate a loop nest to enumer- ate Domain(SendDataMap(m)), \nviz., the virtual processors to which processor m must send data, and insert code to pack data for u \nby enumerating Range(SendDataMap(m)) (part (a) of the figure). Then, we rewrite this loop to sep- arately \nenumerate the physical processors, and the virtual processors for each physical processor (part (b)). \nFinally, we enumerate the active sending processors owned by m, by applying the procedure described above \n(for busyVPSet) to the set activeSendVPSet. This loop nest is wrapped around the inner virtual processor \n(packing) loop, yielding the final send code in part (c). (The bZock distribution case is again much \nsimpler because the two inner virtual processor loops are not required.) More generally, if the original \nloop-nest of part (a) had r loops in a (possibly imperfect) loop nest, we generate r physical processor \nloops with the same nesting structure. If k of these loops correspond to cyclic data dimensions, we generate \n2k virtual processor loops in a single perfect loop nest enclosing the packing code. Finally, we insert \none copy of this virtual processor loop nest into each innermost physical processor loop. The extensions \ndescribed above allow us to use a very general set representation without unduly restricting the use \nof symbolic quantities. The additional complexity in-troduced by these techniques is largely encapsulated \nin the implementation of the framework, and (we believe) are out- weighed by the analysis capabilities \nand flexibility the inte- ger set framework provides. 5 Implementation Issues ln the course of implementing \nthe set framework in the dHPF compiler, several interesting technical issues arose that are worth noting. \nHandling multiple processor arrays in HPF. An HPF pro-gram may declare multiple processor arrays with \ndifferent ranks or extents. It is not meaningful to combine sets of tuples from different processor arrays \nin a single operation. It is also not possible to convert all processor index tuples into a common (e.g., \nlinear) domain because the conversion would require a symbolic multiplication if some of the pro- cessor \narray sizes were unknown. For example, the union operation used to compute CPMap in Section 3.1 cannot \nbe performed directly when the arrays Ak are distributed on different processor arrays. The implications \nof this are as follows. First, the CPMap or any set or mapping derived there-from is stored as a list \nof the sets or mappings, representing an implicit union or intersection of the list members. (Note that \narbitrary CPs on the same processor domain can still be unioned explicitly into a single mapping.) The \nrange of such a mapping list can be converted back into a sin-gle set without any loss of precision (e.g., \nCPMap({m}), Range(SendDataMap) or Range(RecvDataMap) in Figure 3). The domain of such a mapping, however, \ncannot be com-bined into a single set, e.g., Domain(SendDataMap) spec-ifying the set of processors to \nwhom data should be sent. We account for such cases explicitly during code generation from such a set \nat the cost of some runtime overhead. We expect it to be rare in practice to have a CP from multiple \ndifferent processor arrays. Second, in Figure 3, the difference and intersection op-erations in Equation \n3 cannot be performed directly since the domains of DataAccessedt and LayoutA may represent different \nprocessor arrays. Instead, we convert the maps to sets of data for the fixed processor myid (represented \nby the singleton sets {nzcp} and {By} in the two domains), and then compute nlDataSett(m) directly as \nfollows: (3) nlDataSett(m) = DataAccessedt({sp}) -Layout*({E*}), t = read DataAccessedt({a,,}) n LayoutA(l{r&#38;}), \nt = write { Finally, NLDataAccessedMap, is also required in Figure 5 but in this case it is required \nas a map. In the rare case that we have CP terms from multiple processor arrays, we cannot compute the \nbusyVPSet explicitly and so cannot optimize the runtime checks. Minimizing intermediate set sizes. In \nour experience, the two primary causes of high compile-time in the set frame-work are set operations \nthat produce a large number of dis- junctions, and code generation operations from sets that cause excessive \ncode replication. (The latter is discussed be- low.) Disjunctions are expensive because the cost of simpli- \nfying sets after a sequence of high-level operations increases quadratically with the number of disjunctive \nterms. For this reason, where possible, we avoid computing large disjunc-tions in intermediate sets.3 \nThe equations of both Figures 3 and 4 are specifically formulated for this purpose. In Fig-ures 3, we \nuse Equation (3) to combine the DataAccessed, maps for all reads and all writes, rather than applying \nequa-tions (4)-(7) for individual references and performing a union We are grateful to Bill Pugh for \nseveral suggestions on how to control set complexity in practice. of potentially many terms at the end. \nThe DataAccessed, maps are inherently much simpler and therefore it is easier to eliminate excess disjunctive \nterms. In Figure 4, we first compute the intersection of local iterations, n, localIters,, and use that \nto derive nlReadIters and nlWriteIters. The more intuitive formulation we had originally used was to \ncompute the non-local iterations for each reference and then take unions over all read and all write \nreferences. In state-ment groups with many references, this sometimes produced intermediate sets with \nmany more disjunctive terms. Limiting Code Replication. We take two approaches to limit code replication. \nWe factor loop invariant constraints from iteration spaces before code generation and we care-fully limit \nguard lifting. We discuss these issues in turn below. CPs with multiple disjunctive terms arise fairly \nfrequently in practice, particularly for IF and DO statements which get the union of the CPs of statements \ncontrol dependent on them. When a statement has a disjunctive CP, the MM-CODEGEN algorithm first computes \ndisjoint disjunctive form and then generates separate code for each of the resulting terms. When the \ndisjunction arised from a loop-invariant condition, this code replication is unnecessary. To mini-mize \ncode replication, we factor each of the iteration spaces in a compound statement into two parts, just \nbefore code generation: (1) the set of iteration space constraints that relate to the bounds of the iteration \nspace, and (2) a set of other constraints that are loop-invariant and unrelated to these bounds. We then \ngenerate code separately for these two pieces. The latter provides a sequence of one or more disjoint \nIF statements which we fold together and then in-sert a single copy of the code for the iteration space \nwithin. As an additional optimization, we project away all enclos-ing loop variables from other constraints \nbefore generating guard code (since our CPs enforce these at outer loop levels). In generating code for \na non-convex iteration space, guard overhead is reduced by lifting guards out of loops, but this in turn \ncan cause significant code duplication. The MMCODE-GEN function enables the user to specify how many \nlevels guards should be lifted out when generating code (see Ap- pendix A). We generally lift guards \n1 level when generating a code template for a (not necessarily innermost) perfect loop nest. However, \nto avoid excessive code replication we don t lift guards out of loops that contain communication , or \nout of fully replicated loops (such as time-step loops). Also we disallow code replication at the procedure \nscope, where MM-CODEGEN would otherwise attempt to duplicate and regroup statements to exploit partial \noverlap in any guard conditions for the statements. 6 Compiler Performance In this section, we assess \nthe performance of the dHPF com-piler in terms of the detailed costs of applying the set-based compilation \ntechniques described in previous sections. The benchmarks we use are TOMCATV-a SPEC92 benchmark that \nperforms mesh generation, and sp-a serial version of one of the NAS parallel application benchmarks. \nBoth pro-grams perform stencil-based computations on multidimen-sional arrays. The version of TOMCATV \nwe studied is simply the For-tran 77 code for the SPEC92 benchmark with HPF directives to specify a (BLOCK, \n*) distribution of the arrays over a 1D processor grid. With our directives added, this benchmark Breakdown \nof comoi lation time I L --- r application SP-4 SP-sym =i T-sym 7 --total compilation wall-clock time \n1145s 1073s 28s interprocedural analysis m -ix% ---ix%-module compilation 97.9% 97.8% 97.1% partitioning \ncomputation 14.5% 11.3% 16.1% loop splitting 6.4% 2.0% 3.7% loop bounds reduction 5.6% 6.7% 6.1% communication \ngeneration 31.4% 34.6% 28.1% loops to compute msg sizes 12.9% 13.4% 7.0% loops over comm partners 12.8% \n14.1% 10.4% check if msg is contiguous 1.3% 1.9% 2.6% check if msg is rect section 1.2% 1.4% 1.4% opt \nof generated code 28.1% 28.9% 21.3% mult mappings code generation 26.4 7iizZ% -iirK Table 1: Breakdown \nof dHPF compilation time. has 228 lines and a single procedure. Compared to TOM-CATV, the SP application \nbenchmark is more than an order of magnitude larger and the computation is considerably more complex. \nSP has much larger and non-uniform loop nests, procedure calls within parallel loops, and makes lib-eral \nuse of privatizable arrays whereas TOMCATV uses only privatizable scalars. We developed an HPF version \nof SP us- ing minimal modifications to a serial Fortran 77 code from the NPB2.bserial release4. We specified \nblock distributions in the y and z spatial dimensions of the program s 3D and 4D arrays. Our modified \nversion of the source is 3502 lines, compared to the 3382 lines in the NPB2.3-serial release. The application \nhas 30 procedures. We used a version of dHPF compiled with -02 optimiza-tion and measured compile times \non a 250MHz UltraSparc workstation. We used Rational Software s QuantifyTM utility to obtain an execution \nprofile. For TOMCATV, the number of processors was left unspecified at compile-time. For SP, we considered \ntwo variants: SP-4 used a fixed 2x2 proces-sor array, while sp-sym left the total number unspecified, \nusing a 2 x (number_of -processors()/2) processor array. For both benchmarks, the compiler exploited \nall of the op- timizations described in previous sections, including non-owner-computes computation partitionings \nfor some state-ments to reduce the number and frequency of communica- tion operations. Table 1 shows \nthe wall-clock time in seconds for com-piling each of these benchmark versions and a breakdown of total \nexecution time spent in key phases of dHPF, in-cluding each of the major integer-set optimizations. (The \n&#38;ml optimization of generated code does not use integer-set-based operations.) The nesting of phases \nis shown by indentation in the fist column. The numbers do not sum to 100% because percentages shown \nfor indented phases are merely refinements of their enclosing phase. Although the benchmarks are quite \ndifferent in size and complexity, the breakdown of compilation time for them is remarkably con-sistent. \nNone of the phases is especially dominant in compile time, although SP has a somewhat high cost for communica- \ntion generation because of a huge number number of com- munications (even after a high degree of coalescing), \nsome with complex patterns. Based on these results, we would anticipate that other programs would have \na similar cost breakdown although the distribution of compilation effort would differ depending on the \nnumber and complexity of loops in a routine, the number of communication events, The NAS benchmarks are \navailable from http://science.nas.nasa.govtSoftuarelNPB. and the shape of the communication sets. At \nthe bottom of the table, we note the time spent in Kelly, Pugh and Rosser s multiple mappings code genera-tion \noperation which we use to synthesize loops that enu-merate iteration or data sets. This algorithm accounts \nfor virtually all of the cost of the set framework. Most of this time is spent in simplifying Presburger \nformulae represent-ing integer sets and mappings. These numbers show that the integer-set framework is \nnot a dominant cost in compile-time, even for fairly large and complex codes such as SP. Finally, the \ntable shows that there is no significant addi-tional cost to compiling for a symbolic number of processors \nvs. a known (fixed) number. sp-sym is in fact faster than SP-4 because the compiler performs more aggressive \nloop-splitting in the latter, which leads to more complex sets and correspondingly higher compile-time \ncost. The absolute compile-times for SP are somewhat high but (we believe) acceptable for a research \ncompiler where efficiency in the implementation has not been a primary goal. There are opportunities \nfor significant improvements. Approximately 30% of compilation time is spent on gener- ating custom inline \ncode for counting, packing and unpack- ing buffers. While such custom code can be important for complex \ncommunication patterns, for the vast majority of simple patterns such as a shift communications, invoking \na run-time library operation would not be more expensive, and would largely eliminate the communication \ngeneration cost. We also spend nearly 30% in a post-pass optimizing the SPMD code we generate, which \nwe believe can be largely eliminated through an algorithmic improvement. 7 Preliminary Results The purpose \nof this section is primarily to demonstrate that our compiler implementation using an integer set framework \nis practical and effective enough to achieve good speedups for realistic kernels and benchmarks. Many \nof the specific optimizations we have described in this paper are not new, although few if any compilers \nhave implemented more than a few of these in a general way. Evaluating the impact of par- ticular optimizations \non applications is not directly relevant to this paper but is a subject of our current work. We present \nresults on the overall performance of three benchmark codes compiled with dHPF. The codes are TOM- CATV, \nERLEBACHER-a larger 3D compact differencing code, and JACOBI-a simple 4-point stencil kernel with a conver- \ngence loop. These results are preliminary and a more exten-sive performance evaluation with larger codes \n(including the NAS benchmarks) and comparisons with other compilers is currently under way. We distributed \nthe data arrays (BLOCK,*) in TOMCATV, (*, * ,BLclCK) in ERLEBACHER, and (BLOCK,BLOCK) in JA-COBI. The \nnumber of processors was left unspecified in each case. For JACOBI, we used a 2D processor array of shape \n(2, number-of -processors () /2), while the other benchmarks used 1D processor arrays. The Fortran77+MPl \ncode gener- ated by dHPF was compiled using the IBM xlf compiler at optimization level -02. It was executed \non an IBM SP-2 us- ing the IBM MPI library with the user-level communication layer. Figure 7 shows the \nresulting speedups for the three codes. Speedups for JACOBI and the small problem sizes for ER-LEBACHER \nand TOMCATV are relative to the original serial version of each benchmark. The larger problem sizes for \nERLEBACHER and TOMCATV exceed the available physical memory on smaller numbers of processors. For this \nreason, -B-Sire -256x256~256: T(4) -5.218 e Size I 128x128~128: T(1) -5.46s IC - a* , I .* (a) Speedups \nfor TOMCATV (b) Speedups for EFUEBACHER (c) Speedups for JACOBI Figure 7: Speedups for benchmarks on \nan IBM SP2. speedups of the large problem size for TOMCATV and ER-LEBACHER are computed relative to the \n4-processor speedup as 4 *734)/T(p). For TOMCATV, the compiler provides mod-erate speedups for a small \nproblem size of 514x514. For the larger problem, we see that the scaling improves. The re-duced scalability \non smaller problems is primarily because of two global maxloc reductions within a relatively small main \ncomputational loop. (The compiler recognizes and imple-ments these reductions extremely efficiently [23].) \nThe com-piler selected non-owner computes computation partitions for two array assignment statements, \nand generated differ-ent partitionings for different statements in several of the loops. The scalar efficiency \nof our generated code is quite good. For TOMCATV, our generated code actually achieves a speedup of 1.08% \non one processor over the original, per-haps because of a reduction in cache conflicts that is a side \neffect of our changes to data layout. Measurements of TOMCATV showed some interesting ben-efits from \nloop-splitting for efficient buffer access. In TOM-CATV, data cannot be received in-place with the (BLOCK, \n*) distribution, and for such stencil computations compilers have traditionally unpacked the received \ndata into overlap areas. However, the loop-splitting into local and non-local iterations enabled us to \nreference data directly from the re-ceive buffer without unpacking. For the smaller problem size, this \nactually proved faster than when using overlap areas [24]. For the larger problem size, the two versions \nperformed approximately as well. Loop-splitting is essential to achieving such good performance without \noverlap areas. This result is significant because overlap areas only apply to nearest-neighbor communication \npatterns whereas loop splitting is applicable to arbitrary patterns. For ERLEBACHER, several factors \nlimit the speedup: a pipelined communication pattern with numerous relatively small messages, a broadcast \ncommunication of a large panel of data, a large reduction of a 3D to a 2D array, and a substantial load \nimbalance in computing boundary condi-tions. Nevertheless, the principal computation phases are efficiently \nparallelized by dHPF, and the compiler is able to achieve fairly good scaling in performance for the \nlarger problem size. In addition to static computation partition-ing, message vectorization, and simple \ninstances of message coalescing, the compiler again applied loop splitting for both communication-computation \noverlap and efficient buffer ac-cess. Loop-splitting permitted the code to reference non-local data directly \nfrom receive buffers for the large broad-cast messages, where overlap areas were not useful. For JACOBI, \nthe speedup scales linearly as should be ex- pected for this simple, regular stencil code. The optimiza-tions \napplied here included static loop partitioning identical to the owner-computes rule, using in-place send \nand receive operations along one of the two dimensions, and recognizing the reduction for the convergence \nloop. Currently, we are experimenting with the NAS appli-cation benchmarks, LU, BT, and SP (as described \non the previous section). We have successfully compiled versions of these benchmarks with dHPF, and are \ncurrently experiment-ing with advanced optimizations that leverage our integer set framework. Optimizations \nwe have designed and imple- mented in dHPF for handling such real-world codes include interprocedural \ncomputation partitioning, effective compu-tation partitioning for privatizable arrays (which uses partial \nreplication of computation to reduce communication), and communication-sensitive loop distribution. Preliminary \nre-sults have been encouraging and we have measured better than of 85% efficiency on 4 processors for \nBT, as well as good scaling behavior up to 16 processors. We expect to use these codes to compare the \nperformance of dHPF with other selected compilers, and to evaluate the benefit of the specific optimizations \nimplemented in dHPF. 8 Related Work As explained in the Introduction, most research and com-mercial data-parallel \ncompilers to date use case-based ap-proaches for implementing basic communication and itera- tion set \nanalysis [22, 7, 21, 29, 11, 12, 14, 8, 13, 31, 32, 331 This is a fundamentally different approach from \nthat taken in this paper, and its strengths and weaknesses have been discussed in Section 1. Previously, \nHiranandani et al. [16] and Koelbel [21] both formulated the basic computation partitionings and com-munication \nanalysis problems in terms of abstract integer-set operations, but their formulations assumed the owner-computes \nrule. Our formulations apply to a much more general partitioning model, and support more general com-munication \ncoalescing, non-local index set splitting, and in- place communication analysis. Furthermore, they used \ncase- based approaches in their implementations of the Kali and Fortran D compilers. In contrast, we \npresent an actual and fairly general implementation in terms of integer set opera- tions, address a fundamental \nlimitation in the representation of symbol&#38;, and demonstrate experimentally that such an implementation \nis practical for large, real codes. There is also a large body of work on techniques to enu- merate \ncommunication sets and iteration sets in the presence of cyclic(lc) distributions (e.g., [13, 9, 19, \n29, 321). These techniques would provide more efficient support for cyclic(k) distributions but would \nbe much less efficient if used in the general form for block and cyclic distributions. Previous work \non using linear inequalities and Fourier- Motzkin Elimination [28] for code generation shares our goal \nof improving the generality, the level of abstraction, and the quality of code in data-parallel compilers. \nThree groups (Ancourt et al. [3], the Paradigm compiler group [30] and SUIF [2]) applied these techniques \nto support code genera- tion for communication and iteration sets described by lin- ear inequalities. \nThe latter work also describes how basic communication analyses and optimizations such as message coalescing \nand redundant communication elimination can be performed using linear inequalities. However, since these \ngroups did not use a representation capable of representing general non-convex sets, it precluded performing \npotentially important optimizations such as precise communication co- alescing for arbitrary affine references, \nsplitting iterations sets into local and non-local components, or the use of a more general computation \npartitioning model. All of these are possible in dHPF, and in fact have been used for many or all of \nour benchmarks. Furthermore, all of our optimizations fully support our general computation partitioning \nmodel. Our virtual processor approach for capturing distribu- tions on symbolic numbers of processors \nis similar to the approach used by Gupta et al. [13] for computing commu- nication sets and that used \nby SUIF [2] for cyclic distribu- tions. As discussed in Section 4, compared to SUIF, the key improvement \nin our work and the work of Gupta et al. is to compute active processor sets and use these to eliminate \nor reduce runtime checks in the generated code. The pri- mary difference in our algorithm compared to \nGupta et al. is that they compute the active VP sets by detailed analysis of specific cases whereas we \ndescribe a simpler, more general algorithm expressed as integer set equations. Perhaps the most important \ndistinction between our work and previous data parallel compilers is that (we believe) we are the fist \nto demonstrate by implementation experience that it is practical to build a complete compiler for High \nPer-formance Fortran using a general and powerful integer-set framework. Conclusions We have described \nan integer-set-based approach for analy- sis and code generation for data-parallel programs. We have \nused this framework as the basis for implementing very gen-eral versions of a comprehensive collection \nof data-parallel optimizations in the dHPF compiler. It is particularly signif-icant that the framework \nhas been able to support such com- prehensive optimizations despite the fact that dHPF sup-ports a more \ngeneral computation partitioning model than has been implemented previously. Furthermore, the com-piler \nhas relatively modest compile times, and in particular, the analysis and code generation operations using \nthe set framework account for only about 25?% of the overall execu-tion time of the compiler for the \nlarger examples we studied. Overall, our integer-set-based framework provides a sim- ple, uniform, and \npowerful foundation for analysis, opti-mization and code generation of data-parallel programs. In dHPF, \nour implementation preserves much of the simplicity and expressiveness of the integer set framework while \naccom-modating performance tuning to minimize the nmtime over-head for important cases. Preliminary measurements \nshow that our set-based analyses and optimizations enable us to achieve good performance on small codes. \nIn our current work with larger programs, we have found that our uniform approach to analysis and optimization \nhas facilitated rapid development of several sophisticated new optimizations that appear important for \nachieving high performance on these more complex codes. Acknowledgments Ajay Sethi collaborated in the \ndevelopment and implemen- tation of early versions of the equational framework for com-munication sets. \nGuohua Jin provided invaluable assistance in obtaining the results for the NAS benchmark BT. The performance \nresults we present in this paper would not have been possible without the efforts of individuals who \ncon-tributed to the design and implementation of the dHPF com-piler including Arun Chauhan, Chen Ding, \nKathi Fletcher, Robert Fowler, Guohua Jin, Coffin McCurdy, Mike Paleczny, Dejan Mircevski, Nenad Nedeljkovic, \nMonika Mevencamp, Bo Lu, and Ajay Sethi. Ken Kennedy provided invaluable guidance and leadership to the \ndHPF project. We are grateful to Bill Pugh and Evan Rosser who pro- vided valuable advice and technical \nassistance in using the Omega library effectively. Sarita Adve, Robert Fowler and Ken Kennedy provided \nvaluable comments on earlier drafts of this paper. References V. Adve and J. Mellor-Crummey. Advanced \ncode generation for High Performance Fortran. In Languages, Compilation Techniques and Run Time Systems \nfor Scalable Parallel Sys-tems, Lecture Notes in Computer Science Series. Springer-Verlag. (to appear). \n PI PI S. Amarasinghe and M. Lam. Communication optimization and code generation for distributed memory \nmachines. In Proceedings of the SIGPLAN 93 Conference on Program-ming Language Design and Implementation, \nAlbuquerque, NM, June 1993. C. Ancourt, F. Coelho, F. Irigoin, and R. Keryell. A linear algebra framework \nfor static HPF code distribution. In Pro-ceedings of the Fourth Workshop on Compilers for Parallel Computers, \nDelft, The Netherlands, Dec. 1993. [31 C. Ancourt and F. Irigoin. Scanningpolyhedrawith do loops. In \nProceedings of the Third ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, Wiliiams-burg, \nVA, Apr. 1991. [41 V. Balasundaram. A mechanism for keeping useful internal information in parallel programming \ntools: The data access descriptor. Journal of Parallel and Distributed Computing, 9(2):154-170, June \n1990. [51 P. Banerjee, J. Chandy, M. Gupta, E. Hodges, J. Holm, PI A. Lain, D. Palermo. S. Ramaswamv. \nand E. Su. The Paradigm compiler for distributed-memory multicomputers. IEEE Computer, 28(10):37-47, \nOct. 1995. S. Benkner, B. Chapman, and H. Zima. Vienna Fortran[71 90. In Proceedings of the 1992 Scalable \nHigh Performance Computing Conference, Williamsburg, VA, Apr. 1992. Z. Bozkus, L. Meadows, S. Nakamoto, \nV. Schuster, and PI M. Young. Compiling High Performance Fortran. In Proceed-ings of the Seventh SIAM \nConference on Parallel Processing for Scientific Computing, pages 704-709, San Francisco, CA, Feb. 1995. \nS. Chatterjee, J. Gilbert, Fl. Schreiber, and S. Teng. Op- PI timal evaluation of array expressions \non massively parallel machines. Technical Report CSL-92-11, Xerox Corporation, Dec. 1992. D. C. Cooper. \nTheorem proving in arithmetic without mul- 1101 tiplication. In Machine Intelligence, volume 7, pages \n91-99, New York, 1972. American Elsevier. M. Gupta and P. Banerjee. A methodology for high-level IllI \nsynthesis of communication for multicomputers. In Proceed- ings of the 1992 ACM International Conference \non Super- computing, Washington, DC, July 1992. M. Gupta, S. Midkiff, E. Schonberg, V. Seshadri, D. Shields, \n I121 K. Wang, W. Ching, and T. Ngo. An HPF compiler for the IBM SP2. In Proceedings of Supercomputing \n95, San Diego, CA, Dec. 1995. S. K. S. Gupta, S. D. Kaushik, C. Huang, and P. Sadayap-pan. Compiling \narray expressions for efficient execution on distributed-memory machines. Journal of Parallel and Dis-tributed \nComputing, 32(2):155-172, Feb. 1996. P31 J. Harris, J. Bircsak, M. IX. Bolduc, J. A. Diewald, I. Gale, \n iI41 N. Johnson, S. Lee, C. A. Nelson, and C. Offner. Compil-ing High Performance Fortran for distributed-memory \nsys-tems. Digital Technical Journal of Digital Equipment Corp., 7(3):5-23, Fall 1995. S. Hiranandani, \nK. Kennedy, and C.-W. Tseng. Com-piler support for machine-independent parallel program-ming in Fortran \nD. In J. Saltz and P. Mehrotra, editors, Languages, Compilers, and Run-Time Environments for Distributed \nMemory Machines. North-Holland, Amsterdam, The Netherlands, 1992. 1151 S. Hiranandani, K. Kennedy, and \nC.-W. Tseng. Preliminary experiences with the Fortran D compiler. In Proceedings of Supercomputing 93, \nPortland, OR, Nov. 1993. I161 W. Kelly, V. Maslov, W. Pugh, E. Rosser, T. Shpeisman, and iI71 D. Wonnacott. \nThe Omega Library Interface Guide. Tech-nical report, Dept. of Computer Science, Univ. of Maryland, College \nPark, Apr. 1996. W. Kelly, W. Pugh, and E. Rosser. Code generation for multiple mappings. In Frontiers \n95: The 5th Symposium on the Frontiers of Massively Parallel Computation, McLean, VA, Feb. 1995. P81 \n[19] K. Kennedy, N. Nedeljkovit, and A. Sethi. A linear-time al-gorithm for computing the memory access \nsequence in data-parallel programs. In Proceedings of the Fifth ACM SIG-PLAN Symposium on Principles \nand Practice of Parallel Programming, Santa Barbara, CA, July 1995. C. Koelbel, D. Loveman, R. Schreiber, \nG. Steele, Jr., and PO1 M. Zosel. The High Performance Fortran Handbook. The MIT Press, Cambridge, MA, \n1994. C. Koelbel and P. Mehrotra. Compiling global name-space parallel loops for distributed execution. \nIEEE Transac-tions on Parallel and Distributed Systems, 2(4):440-451, Oct. 1991. [ a J. Li and M. Chen. \nCompiling communication-efficient pro-grams for massively parallel machines. IEEE Transactions on Parallel \nand Distributed Systems, 2(3):361-376, July 1991. WI B. Lu and J. Mellor-Crummey. Compiler optimization \nof implicit reductions for distributed memory multiprocessors. In Proceedings of the 12th International \nParallel Processing Symposium, Orlando, FL, Mar. 1998. To appear. [231 [241 J. Mellor-Crummey and V. \nAdve. Simplifying control flow in compiler-generated parallel code. International Journal of Parallel \nProgramming, 1998. (to appear). A previous ver-sion of this paper was presented at the 1997 Workshop \non Languages and Compilers for Parallel Computing. [251 W. Pugh. A practical algorithm for exact array \ndependence analysis. Communications of the ACM, 35(8):102-114, Aug. 1992. @I A. Rogers and K. Pingali. \nProcess decomposition through locality of reference. In Proceedings of the SIGPLAN 89 Conference on Programming \nLanguage Design and Imple-mentation, Portland, OR, June 1989. J. Saltz, K. Crowley, R. Mirchandaney, \nand H. Berryman. Run-time scheduling and execution of loops on message pass-ing machines. Journal of \nParallel and Distributed Comput-ing, 8(4):303-312, Apr. 1990. I271 [=I A. Schrijver. Theory of Linear \nand Integer Programming. John Wiley and Sons, Chichester, Great Britain, 1986. J. Stichnoth, D. O Hallaron, \nand T. Gross. Generatingcom-munication for array statements: Design, implementation, and evaluation. \nJournal of Parallel and Distributed Com-puting, 21(1):150-159, Apr. 1994. WI E. Su, A. Lain, S. Ramaswamy, \nD. J. Palermo, E. W. H. IV, and P. Banerjee. Advanced compilation techniques in the PARADIGM compiler \nfor distributed-memory multicomput-ers. In Proceedings of the 1995 ACM International Conjer-ence on Supercomputing, \nBarcelona, Spain, July 1995. 1301 C.-W. Tseng. An Optimizing Fortran D Compiler for MIMD Distributed-Memory \nMachines. PhD thesis, Dept. of Com-puter Science, Rice University, Jan. 1993. 1311 [321 K. van Reeuwijk, \nW. Denissen, H. Sips, and E. Paalvast. An implementation framework for hpf distributed arrays on message-passing \nparallel computer systems. IEEE Trans-actions on Parallel and Distributed Systems, 7(8):897-914, Sept. \n1996. [331 H. Zima, H.-J. Bast, and M. Gerndt. SUPERB: A tool for semi-automatic MIMD/SIMD parallelization. \nParallel Com-puting, 6:1-18, 1988. Appendix A. Definitions of integer set operations Following the notation \nused by the Omega library[l7], we use the following sets and maps to define some of the less common operations \nused in the paper: Sl = {[il...in]:fsl(il...in)} s2 = {[il . im] : jsa(i1 . . im)} Rl = {[il...i,]+ lil...j,]:j~l(il...i~,jl...j~)} \nR2 = {[il..,im]+~~...jP]:  Some of the key operations that we use in our equational framework are defined \nas follows: Composition of two maps : RlOR-2 E [il in] -+ bl .j,] : 3a1 a, a.t. 1 fm(il . ..i., al.. \nam) A fR2(%. arn,jl.. .j,) > Composition of a map with a set : Rl(S1) f LI.. j,] : 301~. a, 8.t. -i \n jRl(%... a,,jl .j,) A fs~(co a,) > Restrict Range : Rl 52 E [il in] -+ bl .j,] : n range 1 jRl(il \n.i,, jl.. . jm) A fsz(jl.. .j,) > Codegen(S1.. SW 1 Known) : Sl Sv specify the iteration spaces for \nZI statements. Known is a rank 0 set of constraints on global variables in Sl Sv that are guaranteed \ntrue. CodeGen synthesizes a code fragment to enumerate the tuples in Sl Sv in lexicographic order, where \nthe same tuple in different sets is ordered as: (1 E Sj) 4 (I E Sk), j < k.  \n\t\t\t", "proc_id": "277650", "abstract": "In this paper, we describe our experience with using an abstract integer-set framework to develop the Rice dHPF compiler, a compiler for High Performance Fortran. We present simple, yet general formulations of the major computation partitioning and communication analysis tasks as well as a number of important optimizations in terms of abstract operations on sets of integer tuples. This approach has made it possible to implement a comprehensive collection of advanced optimizations in dHPF, and to do so in the context of a more general computation partitioning model than previous compilers. One potential limitation of the approach is that the underlying class of integer set problems is fundamentally unable to represent HPF data distributions on a symbolic number of processors. We describe how we extend the approach to compile codes for a symbolic number of processors, without requiring any changes to the set formulations for the above optimizations. We show experimentally that the set representation is not a dominant factor in compile times on both small and large codes. Finally, we present preliminary performance measurements to show that the generated code achieves good speedups for a few benchmarks. Overall, we believe we are the first to demonstrate by implementation experience that it is practical to build a compiler for HPF using a general and powerful integer-set framework.", "authors": [{"name": "Vikram Adve", "author_profile_id": "81100524180", "affiliation": "Department of Computer Science, Rice University", "person_id": "P291197", "email_address": "", "orcid_id": ""}, {"name": "John Mellor-Crummey", "author_profile_id": "81100196441", "affiliation": "Department of Computer Science, Rice University", "person_id": "PP14078179", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/277650.277721", "year": "1998", "article_id": "277721", "conference": "PLDI", "title": "Using integer sets for data-parallel program analysis and optimization", "url": "http://dl.acm.org/citation.cfm?id=277721"}