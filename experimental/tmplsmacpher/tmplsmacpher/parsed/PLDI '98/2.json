{"article_publication_date": "05-01-1998", "fulltext": "\n Register Promotion by Sparse Partial Redundancy Elimination of Loads and Stores Raymond Lo Fred Chow \nRobert Kennedy Shin-Ming Liu Peng Tul loQsgi.com Silicon Graphics Computer Systems 2011 N. Shoreline \nBlvd. Mountain View, CA 94043 Abstract An algorithm for register promotion is presented based on the \nobservation that the circumstances for promoting a memory location s value to register coincide with \nsituations where the program exhibits partial redundancy between ac-cesses to the memory location. The \nrecent SSAPRE al-gorithm for eliminating partial redundancy using a sparse SSA representation forms the \nfoundation for the present al-gorithm to eliminate redundancy among memory accesses, enabling us to achieve \nboth computational and live range op- timality in our register promotion results. We discuss how to effect \nspeculative code motion in the SSAPRE framework. We present two different algorithms for performing specu-lative \ncode motion: the conservative speculation algorithm used in the absence of profile data, and the the \nprofile-driven speculation algorithm used when profile data are available. We define the static single \nuse (SSU) form and develop the dual of the SSAPRE algorithm, called SSUPRE, to perform the partial redundancy \nelimination of stores. We provide measurement data on the SPECint95 benchmark suite to demonstrate the \neffectiveness of our register promotion ap-proach in removing loads and stores. We also study the rel- \native performance of the different speculative code motion strategies when applied to scalar loads and \nstores. 1 introduction Register allocation is among the most important func-tions performed by an optimizing \ncompiler. Prior to reg-ister allocation, it is necessary to identify the data items in the program that \nare candidates for register alloca-tion. To represent register allocation candidates, compil-ers commonly \nuse an unlimited number of pseudo-registers [CACt81, TWL+Sl]. Pseudo-registers are also called sym-bolic \nregisters or virtual registers, to distinguish them from real or physical registers. Pseudo-registers \nhave no alias, and the process of assigning them to real registers involves only renaming them. Thus, \nusing pseudo-registers simplifies the register allocator s job. Present address: Intel Corporation, 2200 \nMission College Blvd., Santa Clara, CA 95052. Permission to make digital or hard oopiaw of all or pan \n01 thii work for psrsonsl or claewoom we is granted without 1~ provided that copies arm not made or dieributed \nfor profit or commwcial advan- tage and that copies bear this notice and the iull citotion on the firat \npage. To copy otherwise, to republish. to p(wt cm swvem or to redietributa to lists, rsquira prior spacific \npermission and/w a fae SIGPLAN 98 Montraal. Canada 0 1668 ACM 0.89791-9874/98/0006...$6.00 Optimization \nphases generate pseudo-registers to hold the values of computations that can be reused later, like common \nsubexpressions and loop-invariant expressions. Variables declared with the register attribute in the \nC pro- gramming language, together with local variables deter-mined by the compiler to have no alias, \ncan be directly repre-sented as pseudo-registers. All remaining register allocation candidates have to \nbe assigned pseudo-registers through the process of register promotion [CL97]. Register promotion identifies \nsections of code in which it is safe to place the value of a data object in a pseudo-register. Register \npro-motion is regarded as an optimization because instructions generated to access a data object in register \nare more effi-cient than if it is not in a register. If later register allocation cannot find a real \nregister to map to a pseudo-register, it can either spill the pseudo-register to memory or re-materialize \nit, depending on the nature of the data object. This paper addresses the problem of register promotion \nwithin a procedure. We assume that earlier alias analysis has already identified the points of aliasing \nin the program, and that these aliases are accurately characterized in the static single assignment (SSA) \nrepresentation of the pro-gram [CCLs96]. The register promotion phase inserts ef-ficient code that sets \nup data objects in pseudo-registers, and rewrites the program code to operate on them. The pseudo-registers \nintroduced by register promotion are main- tained in valid SSA form. Our targets for register promo-tion \ninclude scalar variables, indirectly accessed memory lo-cations and program constants. Since program \nconstants can only be referenced but not stored into, they represent only a subset of the larger problem \nof register promotion for memory-resident objects. For convenience, we choose to exclude them from discussion \nfor the rest of this paper, even though our solution does apply to them.3 Register promotion is relevant \nonly to objects that need to be memory-resident in some part of the program. Global variables are targets \nfor register promotion because they re-side in memory between procedures. Aliased variables need to reside \nin memory at the points of aliasing. Before register promotion, we can regard the register promotion \ncandidates as being memory-resident throughout the program. As a re- For a program variable that has \nan allocated home location, spilling its pseudo-register to the home location produces more ef-ficient \ncode than spilling to a new temporary location. Spilling-to-home and re-materialization can be regarded \naa the reverse of register promotion [Bri92]. In applying register promotion to program constants, the \nprocess of materializing a program constant in a register corresponds to the loading of a variable from \nmemory to a register. our algorithm for the PRE of stores. In Section 7, we pro- 2 t (redundant) vide \nmeasurement data to demonstrate the effectiveness of Xt Xt x (redundant) (a) loads of x (b) stores to \nx Figure 1: Duality between load and store redundancies sult, there is a load operation associated with \neach of their uses, and there is a store operation associated with each as- signment to them. Hence register \npromotion can be modeled as two separate problems: partial redundancy elimination of loads, followed \nby partial redundancy elimination of stores. Partial redundancy elimination (PRE) is a powerful op-timization \nconcept first developed by Morel and Renvoise [MR79]. By performing data flow analysis on a computa-tion, \nit determines where in the program to insert the com-putation. These insertions in turn cause partially \nredundant computations to become fully redundant, and therefore safe to delete. Knoop et al. came up \nwith a different PRE algo-rithm called lazy code motion that improves on Morel and Renvoise s results \n[KRS92, DS93, KRS94a]. The result of lazy code motion is optimal: the number of computations cannot be \nfurther reduced by safe code motion, and the life- times of the pseudo-registers introduced are minimized. \nOur team at Silicon Graphics has recently developed a new algorithm to perform PRE based on SSA form, \ncalled SSAPRE [CCKS97]. SSAPRE achieves the same optimal result as lazy code motion. Applying SSAPRE \nto loads thus has the effects of moving them backwards with respect to the control flow while inserting \nthem as late as possible. The development of SSAPRE was motivated by the fact that traditional data flow \nanalysis based on bit vectors does not interface well with the SSA form of program represen-tation. In \ncontrast, the SSAPRE algorithm takes advantage of the SSA representation and intrinsically produces its \nop- timized output in SSA form. It does not use bit vectors and instead works on one candidate at a time, \nusing the built-in use-def edges in SSA to propagate data flow informa-tion. The SSAPRE algorithm exhibits \nthe same attributes of sparseness inherent in other SSA-based optimization al-gorithms. The entire program \nis maintained in valid SSA form as SSAPRE iterates through the PRE candidates. For the sake of recognizing \nredundancy, loads behave like ordinary expressions because the later occurrences are the ones to be deleted. \nFor stores, the reverse is true: the earlier stores are the ones to be deleted, as is evident in the \nexam-ples of Figure l(a) and (b). The PRE of stores problem, also called partial dead store elimination, \ncan thus be treated as the dual of the PRE of loads problem. Performing PRE of stores thus has the effects \nof moving stores forward while inserting them as early as possible. By combining the ef-fects of the \nPRE of loads and stores, our register promotion approach results in optimal placements of loads and stores \nwhile minimizing the live ranges of the pseudo-registers. The rest of this paper is organized as follows. \nSection 2 surveys previous works related to register promotion and partial dead store elimination. Section \n3 gives an overall perspective of our register promotion approach. Section 4 discusses our algorithm \nfor the PRE of loads. Section 5 dis- cusses how speculative code motion can be incorporated into the \nSSAPRE framework, and presents two different strate-gies for performing speculation, depending on whether \npro-file data are available. In Section 6, we develop and present the techniques presented in removing \nloads and stores, and to study the relative performance of the different specula-tive code motion strategies \nwhen applied to scalar loads and stores. We conclude in Section 8. 2 Related Work Different approaches \nhave been used in the past to perform register promotion. Chow and Hennessy use data flow anal- ysis \nto identify the live ranges where a register allocation candidate can be safely promoted [CH90]. Because \ntheir global register allocation is performed relatively early, at the end of global optimization, they \ndo not require a sep- arate register promotion phase. Instead, their register pro-motion is integrated \ninto the global register allocator, and profitable placement of loads and stores is performed only if \na candidate is assigned to a real register. In optimizing the placement of loads and stores, they use \na simplified and symbolic version of PRE that makes use of the fact that the blocks that make up each \nlive range must be contiguous. Cooper and Lu use an approach that is entirely loop-based [CL97]. By scanning \nthe contents of the blocks com-prising each loop, they identify candidates that can be safely promoted \nto register in the full extent of the loop. The load to a pseudo-register is generated at the entry to \nthe outer-most loop where the candidate is promotable. The store, if needed, is generated at the exit \nof the same loop. Their algorithm handles both scalar variables and pointer-based memory accesses where \nthe base is loop-invariant. Their approach is all-or-nothing, in the sense that if only one part of a \nloop contains an aliased reference, the candidate will not be promoted for the entire loop. They do not \nhan-dle straight-line code, relying instead on the PRE phase to achieve the effects of promotion outside \nloops, but it is not clear if their PRE phase can handle stores appropriately. Dhamdhere was first to \nrecognize that register promo-tion can be modeled as a problem of code placement for loads and stores, \nthereby benefiting from the established results of PRE [Dha88, DhaSO]. His Load-Store Insertion Algorithm \n(LSIA) is an adaptation of Morel and Renvoise s PRE algorithm for load and store placement optimization. \nLSIA solves for the placements of both loads and stores at the same time. The PRE of stores in the context \nof register promo-tion can be viewed as another approach to partial dead store elimination (PDE), for \nwhich numerous algorithms have been described. Chow applied the dual of Morel and Renvoise s PRE algorithm \nto the optimization of store state- ments [Cho83]. After solution of the data flow equations in bit vector \nform, an insertion pass identifies the latest in-sertion point for each store statement taking into account \nany possible modification of the right hand side expres-sion. The algorithm by Knoop et al. is also PRE-based, \nbut they separate it into an elimination step and a sink-ing step, and iterate them exhaustively so as \nto cover sec-ond order effects [KRS94b]. Their algorithm is thus more expensive than straight PRE. To \nadditionally cover faint code elimination,4 they use slotwise solution of the data flow equations [DRZ92]. \nThe PRE-based approaches to PDE do not modify the control flow structure of the program, but this limits \nthe partial dead stores that can be removed. Non-PRE-based A store is faint if it is dead or becomes \ndead after some other dead stores have been deleted. xx-r (a) original code (b) speculative load/store \ninserted Figure 2: Speculative insertion of load and store PDE algorithms can remove additional partial \ndead stores by modifying the control flow. In the revival transformation [FKCX94], a partially dead statement \nis detached from its original place in the program and reattached at a later point at which it is minimally \ndead. In cases where movement of a single store is not possible, the transformation will move a superstructure \nthat includes other statements and branches. However, the coverage of the revival transformation is lim- \nited because it cannot be applied across loop boundaries. The algorithm as presented also does not consider \nsitua-tions that require multiple reattachment points to remove a partial dead store. A PDE approach \nusing slicing transformations was re-cently proposed by Bo&#38;k and Gupta [BG97]. Instead of moving \npartially dead statements, they take the approach of predicating them. The predication embeds the par-tially \ndead statement in a control flow structure, determined through program slicing, such that the statement \nis executed only if the result of the statement is eventually used. A sep- arate branch deletion phase \nrestructures and simplifies the flow graph. Their algorithm works on one partially dead statement at \na time. Since the size of the code may grow after the PDE of each statement, complete PDE may take exponential \ntime, and results in massive code restructuring. The vastly different code shape can cause additional \nvaria-tion in program performance. Another PDE algorithm described by Gupta et al. [GBF97a] uses predication \nto enable code sinking in re-moving partial dead stores. The technique uses path pro-filing information \nto target only statements in frequently executed paths. A cost-benefit data flow analysis technique determines \nthe profitability of sinking, taking into account the frequencies of each path considered. The same approach \nis used in [GBF97b] to speculatively hoist computations in PRE. Decisions to speculate are made locally \nat individual merge or split points based on the affected paths. Acyclic and cyclic code are treated \nby different versions of the algo- rithm. 3 Overview of Approach In our PRE-based approach to register \npromotion, we apply the PRE of loads first, followed by the PRE of stores. This is different from Dhamdhere \ns LSIA, which solves for the placements of both loads and stores at the same time. Our ordering is based \non the fact that the PRE of loads is not affected by the results of the PRE of stores, but the PRE of \nloads creates more opportunities for the PRE of stores by deleting loads that would otherwise have blocked \nthe movement of stores. Decoupling the treatments of loads and stores also allows us to use an algorithm \nessentially un-J\\ x+-r f0 rtrtl f rtx t xtr (a) original code (b) x promoted to register Figure 3: Load \nand store inserted on unpromoted path changed from the base PRE algorithm. Our approach is SSA-based \nand worklist-driven. We cannot benefit from the parallelism inherent in bit vector operations, but we \nmake up for that by doing data flow analysis on the sparse SSA representation, which takes fewer steps. \nHandling one can- didate at a time allows easier, more intuitive and flexible implementation. When there \nare fewer candidates to work on, our approach will finish earlier, whereas a bit-vector-based approach \nalways requires some material fixed cost. Our approach is thus more cost-effective, because the num-ber \nof candidates for register promotion in a procedure often shows wide variation. An advantage of SSAPRE \nrelated to the optimization of loads is that, given our SSA program representation that en-codes alias \ninformation using virtual variables [CCL+96], it is easy to perform additional context-sensitive alias \nanalyses during SSAPRE s Rename step to expose more redundancy among loads that have potential aliases. \nIn situations where there is a chain of aliasing stores, our sparse approach can stop after identifying \nthe ftrst aliasing store. In contrast, traditional bit-vector-based approaches would have to ana-lyze \nthe sequence completely in order to initialize the bit vectors for data flow analyses. Hence, in programs \nwith many aliased loads and stores, SSAPRE is often faster than traditional bit-vector-based PRE. A further \nadvantage of using the SSAPRE framework is that, given an existing implementation of SSAPRE for general \nexpressions, only a small effort is needed to obtain coverage for the PRE of indirect loads and scalar \nloads. In our case, most of the additional implementation effort was spent in implementing the PRE of \nstores. There is one important difference between PRE-based and non-PRE-based register promotion approaches. \nPRE by its nature does not perform speculative code motion, but in the area of register promotion, it \nis sometimes beneficial to insert loads and stores speculatively. In Figure 2(a), it is highly desirable \nto promote variable x to register for the duration of the loop, but the branch in the loop that con-tains \nthe accesses to x may not be executed, and promoting x to register, as shown in Figure 2(b), is speculative. \nIn this respect, non-PRE-based promotion approaches have an ad- vantage over PRE-based approaches. On \nthe other hand, it is not always good to insert loads and stores speculatively. In the slightly different \nexample of Figure 3(a), the call f() contains aliases to variable 2. Promoting z to register re-quires \nstoring it before the call and reloading it after the call (Figure 3(b)). If the path containing the \ncall is executed more frequently than the path containing the increment to x, promoting z to register \nwill degrade the performance of the loop. We have developed new techniques to control spec-ulation in \nthe SSAPRE framework. l-t2 *P + *P *P +-  \\/ \\/i 1 *p (redundant) *p (partially redundant) Figure 4: \nRedundant loads after stores To perform the PRE of stores, we develop the dual of the SSAPRE algorithm \ncalled SSUPRE. Because we treat PDE in the context of register promotion, we do not take into account \nthe right hand side of the store. We view each store statement z t (expr) as if it is made up of the \nsequence: r t (expr) Xtl-  PDE is then applied purely to the store x t r. This allows greater movement \nof the store, because it is not blocked by any modification to (expr), while simplifying our algo- rithm. \nWe also do not need to be concerned with second order effects, because doing the earlier PRE of loads \neffec- tively removes most of the loads that can block the move- ment of stores. Before we perform register \npromotion, we invoke the standard SSA-based dead store elimination al- gorithm [CFRS91], which deletes \nall dead or faint stores. We cannot eliminate stores that become partially dead af- ter some other partially \ndead stores have been removed; if desired, they can still be eliminated by iterating the PRE of stores \nphase. 4 Load Placement Optimization Before register promotion, there is a load associated with each \nreference to the variable. Applying PRE to loads re- moves redundancy among the loads and introduces \npseudo- registers to hold the values of redundant loads to be reused. The same holds for indirect load \noperations in the pro- gram. The SSAPRE algorithm can be applied to both types of loads without much \nmodification. We now give a brief overview of the SSAPRE algorithm. The reader is referred to [CCKt97] \nfor a full discussion of SSAPRE. SSAPRE performs PRE on one program computation at a time. For a given \nprogram computation, E, SSAPRE consists of six separate steps. The first two steps, (I) Cp-Insertion \nand (2) Rename, construct the SSA form for the hypothetical temporary h that represents the value of \nE. The next two steps, (3) DownSafety and (4) WillBeAvail, perform sparse computation of global data \nflow attributes based on the SSA graph for h. The fifth step, (5) Finalize, determines points in the \nprogram to insert computations of E, marks computations that need to be saved and compu-tations that \nare redundant, and determines the use-def re-lationship among SSA versions of the real temporary t that \nwill hold the value of E. The last step, (6) CodeMotion, transforms the code to form the optimized output. \nIn our SSA representation [CCL+96], indirect loads are in the form of expression trees, while direct \nloads are leaves in the expression trees. SSAPRE processes the operations in an expression tree bottom-up. \nIf two occurrences of an indirect load, *(al + bl) and *(a~ + bs), have partial redun-dancy between them, \nthe two address expressions (al + b,) and (~2 + bz) must also have partial redundancy between them. Because \nof the bottom-up processing order, by the time SSAPRE works on the indirect loads, the address ex-pressions \nmust have been converted to temporaries tl and tz. i 1 rtr+l cx+l c xtr i 1 (a) original code (b) after \nPRE of loads Figure 5: Load placement via store-load interaction Hence, SSAPRE only needs to handle \nindirect loads whose bases are (or have been converted to) leaves. A store of the form z t (expr) can \nbe regarded as being made up of the sequence: r t (expr) xi-r Because the pseudo-register r contains \nthe current value of I, any subsequent occurrences of the load x can reuse the value from r, and thus \ncan be regarded as redundant. The same observations apply to indirect stores, replacing x by *p. Figure \n4 gives examples of loads made redundant by stores. The implication of this store-load interaction is \nthat we have to take into account the occurrences of the stores when we perform the PRE of loads. During \nPRE on the loads of 2, x t is called a left occurrence. The @-Insertion step will also insert Cp s at \nthe iterated dominance frontiers of left occurrences. In the Rename step, a left occurrence is always \ngiven a new h-version, because a store is a definition. Any subsequent load renamed to the same h-version \nis redundant with respect to the store. In the CodeMotion step, if a left occurrence is marked save, \nthe corresponding store statement will be split into two statements: h t (exw) x + (ew-) * x + tl The \nplacement of the new store x t tl will be optimized by the PRE of stores performed after the PRE of loads. \nThe importance of store-load interaction is illustrated by Figure 5. Ordinarily, we cannot move the load \nof z out of the loop because x s value is changed inside the loop. Recog-nizing z t as a left occurrence \nexposes partial redundancy in the load of x. PRE in turn moves the load of x to the loop header. The \nstore to x will be moved to the loop exit when we perform PRE of stores later (see Section 6). In performing \nSSAPRE for direct loads, the Q-Insertion and Rename steps can be streamlined by taking advantage of the \nvariable being already in SSA form. We can just map the 4 s and SSA versions of the variable to the a \ns and h-versions of its load operation. 5 Speculative Code Motion In its basic framework, PRE does not \nallow the insertion of any computation at a point in the program where the com-putation is not down-safe \n(i.e., anticipated). This is neces- sary to ensure the safety of the code placement. Specula-tion corresponds \nto inserting computations during SSAPRE at Q s where the computation is not down-safe. We can function \nHas-i-@-opnd(X, F) if (X is I) return true if (X not defined by @) return false if (X = F) return false \n if (visited(X)) return false visited(X) t true for each operand opnd of X do if (Has-l-Q-opnd(opnd, \nF)) return true return false end Has-l-@-opnd function Can-speculate(F) for each 0 X in the loop do visited(X) \nt false for each back-edge operand opnd of F do if (Has-l-@-opnd(opnd, F)) return false return true end \nCanspeculate Figure 6: Algorithm for Can-speculate accomplish this effect by selectively marking non-down-safe \n@ s as downsafe in the DownSafety step of SSAPRE. In the extreme case, we can mark all a s as downsafe \nin the DownSafety step of SSAPRE. We refer to the resulting code motion as full speculation. A necessary \ncondition for speculative code motion in gen- eral is that the operation moved must not cause any un-maskable \nexception. Direct loads can usually be speculated. However, indirect loads from unknown pointer values \nneed to be excluded, unless the hardware can tolerate them. Speculation may or may not be beneficial \nto program performance, depending on which execution paths are taken more frequently. Thus, it is best \nto base speculation deci-sions on the profile data of the program being compiled. In the absence of profile \ndata, there are situations where it is often desirable to speculatively insert loads and stores, as we \nhave discussed with resoect to Figure 2. In this section, we present two different speculative code motion \nstrategies depending on whether profile data are available. 5.1 Conservative Speculation The conservative \nspeculation strategy is used when profile data are not available. Under this situation, we restrict spec-ulative \ncode motion to moving loop-invariant computations out of single-entry loops. We base our analysis on \nthe Cp located at the start of the loop body. We perform speculative insertion at the loop header only \nif no other insertion inside the loop is needed to make the commutation fullv available at the start \nof the 100~ body. The algorithm for &#38;is analysis is given by function Cansoeculate shown in Figure \n6. For the @ F. we iden- tify the @ operands that correspond to the back edges of the loop, and call \nfunction Has-l-@-opnd for each of these Cp operands. Since I represents potential insertion points, Has-J--a-opnd \nreturning false will indicate that the com-putation is available at that back edge without requiring \nany insertion other than the one at the operand of F that corresponds to the loop entry. If the checks \nsucceed, we mark F as downsafe. Figure 7 shows the results of ap- plying this algorithm to the program \nof Figure 2(a). The @(hs,1) \\ h + < 1 x t [hz t] X t2 t tz J 7  @(hl, hz) Q(h, t2) + exit exit (a) \nSSA graph for h (b) resulting program @ for hl marked as down-safe Figure 7: Speculative load placement \nexample algorithm marks the @ corresponding to hl in Figure 7(a) as down-safe. Figure 7(b) shows the \nprogram after the load has been speculatively moved out of the loop. Subsequent application of speculative \ncode motion in the PRE of stores will move the store out of the loop to yield the result shown in Figure \n2(b). 5.2 Profile-driven Speculation Without knowledge of execution frequency, any speculation can potentially \nhurt performance. But when execution pro-file data are provided, it is possible to tailor the use of \nspec- ulation to maximize run-time performance for executions of the program that match the given profile. \nThe optimum code placement lies somewhere between no speculation and full speculation. Code placement \nwith no speculation cor-responds to the results obtained by traditional PRE. Code placement with full \nspeculation corresponds to the results of SSAPRE if all @ s are marked downsafe in the DownSafety step. \nThe problem of determining an optimum placement of loads and stores can be expressed as instances of \nthe in- teger programming problem, but we know of no practical algorithm for solving it. Instead of aiming \nfor the opti-mal solution, we settle on a practical, versatile and easy-to-implement solution that never \nperforms worse than no speculation, subject to accuracy of the profile data. In our approach, the granularity \nfor deciding whether to speculate is each connected component of the SSA graph formed by SSAPRE. For \neach connected component, we either perform full speculation, or do not speculate at all. Though this \nlimits the possible placements that we can con- sider, it enables us to avoid the complexity of finding \nand evaluating the remaining placement possibilities. When the connected components are small, we usually \nget better re-sults, since we miss fewer placement possibilities. The con-nected components for expressions \nare generally quite small. Our profile-driven speculative code motion algorithm works by comparing the \nperformance with and without spec-ulation using the basic block frequency data provided by the execution \nprofile. The overall technique is an exten-sion of the SSAPRE algorithm. Pseudo-code is given in Figure \n8. Procedure SSAPRE-with-profile gives the overall phase structure for profile-driven SSAPRE. We start \nby per- forming regular SSAPRE with no speculation. Before the CodeMotion step, we call function Speculatingo, \nwhich de-termines whether there is any connected component in the procedure Computespeculationsavings \nfor each connected component C in SSA graph sauings[C] t 0 for each real occurrence R in SSA graph if \n(Reload(R) = false and R is defined by a Cp F) sauings[Connected-component(F)] += freq(R) for each @ \noccurrence F in SSA graph for each operand opnd of F if (Insert(opnd) = true) { if (opnd is defined by \na @) savings[Connected-component(F)] += freq(R) > else if (opnd is I) sauings[ Connected-component (F)] \n-= freq( optad) end Computespeculationsavings function Speculating(,) has-speculation t false remove \nQ s that are not partially available or partially anticipated identify connected components in SSA graph \nComputespecuJation*avingsc,) for each connected component C in SSA graph if (sauings[C] > 0) { mark all \nQ s in C downsafe has-speculation t true 1 return has-speculation end Speculating procedure SSAPRE-with-profile \n@-Insertion Step Rename Step DownSafetv Step WiJJBeAvajJ Step Finalize Step if (Speculating()) { WiJJBeAvaiJ \nStep Finalize Steo 1 CodeMotion Step end SSAPRE-with-profile Figure 8: Profile-driven SSAPRE Algorithm \n computation being processed that warrants full speculation. If SpecuJating() returns true, it will have \nmarked the rele-vant @ s as down-safe. Thus, it is necessary to re-apply the WiJJBeAvaiJ and Finalize \nsteps, which yield the new code placement result with speculation. The last step, CodeMo-tion, works \nthe same way regardless of whether speculation is performed or not. Speculating() is responsible for \ndetermining if full spec-ulation should be applied to each connected component in the SSA graph. First \nit prunes the SSA graph by remov-ing @ s where the computation is not partially available or not partially \nanticipated. @ s where the computation is not partially available are never best insertion points because \nsome later insertion points yield the same redundancy and are better from the point of view of the temporary \ns live range. Insertions made at Cp s where the computation is not partially anticipated are always useless \nbecause they do not make possible any deletion. After removing these ip s and deleting all references \nto them, Speculating(,) partitions the SSA graph into its connected components. Next, procedure Compute-speculation-savings \ndetermines whether specula-tion can reduce the dynamic counts of the computation on a per-connected-component \nbasis, using the results of SSAPRE with no speculation as the baseline. Given a connected component of \nan SSA graph where the computation is partially available throughout, it is straight- forward to predict \nthe code placement that corresponds to full speculation. Since we regard alI Q s in the component as \ndown-safe, the WiJJBeAvaiJ step will find that can-be-avail holds for all of them. The purpose of the \ncomputation of Later in the WiUBeAvaiJ step is only for live range opti- mality, and does not affect \ncomputational optimality. If we ignore the Later property, the Finalize step will decide to insert at \nall the I operands of the Cp s. In addition, the insertions will render fully redundant any real occurrence \nwithin the connected component whose h-version is defined by a. By predicting the code placement for \nfull speculation, Compute-speculation-savings can compute the benefits of performing full speculation \nfor individual connected com-ponents and store them in the array savings, indexed by the connected components. \nsavings is the sum of the dynamic counts of the real occurrences deleted and any non-speculative SSAPRE \ninsertions suppressed due to full speculation, minus the dynamic counts for the new insertions made by \nfulI speculation. Procedure Com-pute-speculation-savings iterates through all real and Q! oc- currences \nin the SSA graph. Whenever the algorithm en-counters a non-deleted real occurrence that is defined by \na a , it increases savings for the connected component by the execution frequency of the real occurrence. \nDeletions made by SSAPRE are passed over because those deletions would have been done with or without \nthe speculation part. Other real occurrences are not affected by full speculation. For each @ occurrence, \nprocedure Compute-speculation-savings iterates through its operands. If the @ operand is marked insert \nand is defined by another 9, the algorithm also in- creases savings for the connected component by the \nexecu-tion frequency of the Cp operand because full speculation wilI hoist such insertions to earlier \n@ s. If the Cp operand is I, the algorithm decreases savings for the connected compo-nent by the execution \nfrequency for the Q, operand because under full speculation, insertions will be performed at these Cp \noperands. After procedure Compute-speculation-savings returns, SpecuJating() iterates through the list \nof connected com-ponents. If the tallied result in savings is positive for a connected component, it \nmeans speculative insertion is prof- itable, and SpecuJating() will effect full speculation by mark- \ning all a s in the connected component as down-safe. With our profile-driven speculation algorithm, specula-tion \nis performed only in those connected components where it is beneficial. In contrast to the technique \nby Gupta et al. [GBF97a], our decisions are made globally based on a per-connected-component basis in \nthe SSA graph. In the example of Figure 3, our algorithm will promote x to a reg- ister if the execution \ncount of the path containing the call is lower than that of the path containing the store. In the ab- \nsence of profile data, our conservative speculation algorithm of Section 5.1 will not trigger the speculative \ncode motion in this example because it requires insertion in the body of the loop. 6 Store Placement \nOptimization In this section, we develop our algorithm to perform PRE of stores (or PDE). We first present \nthe underlying prin-ciples behind our sparse approach to PRE. We then relate and contrast the characteristics \nbetween loads and stores to establish the duality between load redundancy and store re-dundancy. Given \nthis duality, we then describe our SSUPRE (a) before factoring (b) after factoring + redundancy edge \n-control flow edge C is either an expression or a load Figure 9: Factoring of redundancy edges algorithm \nthat. performs PRE of stores. 6.1 Foundation of Sparse PRE Suppose we are working on a computation C, \nwhich per-forms an operation to yield a value. Let us focus on the occurrence Cl with respect to which \nother occurrences are redundant, and assume there is no modification of the value computed by Cl in the \nprogram.5 Any occurrence of C in the region of the control flow graph dominated by CI is fully redundant \nwith respect to Cl; an occurrence of C outside this region may be partially redundant with respect to \nCl. The earliest possible strictly partially redundant occurrences of C with respect to Cl are in the \ndominance frontier of 9. Dominance frontiers are also the places where 4 operators are required in minimal \nSSA form [CFR+91], intuitively in-dicating that there are common threads between PRE and properties of \nSSA form. Our sparse approach to PRE, as exemplified by SSAPRE, relies on a representation that can directly \nexpose partial redundancy; such a representation can be derived as follows. Suppose an occurrence C2 \nis partially redundant with respect to Cl. We represent this redundancy by a di- rected edge from Cz \nto Cl. In general, if the computation C occurs many times throughout the program, there will be many \nsuch edges. The relation represented by these edges is many-to-many, because an occurrence can be redundant \nwith respect to multiple occurrences. We factor these redundancy edges by introducing a @ operator at \ncontrol flow merge points in the program. The effect of this factoring is to remove the many- to-many \nrelationships, and convert them to many-to-one so that each occurrence can only be redundant with respect \nto a single occurrence, which may be a @ occurrence. In the factored form, each edge represents full \nredundancy because the head of each edge must dominate the tail of the edge after the factoring. Strict \npartial redundancy is exposed whenever there is a missing incoming edge to a @, i.e., a I @ operand (Figure \n9). Having ident,ified this sparse graph representation that can expose partial redundancy, we need a \nmethod to build the representation. Because the representation shares many of the characteristics of \nSSA form, the method to build this sparse graph closely parallels the standard SSA construc-tion algorithm.6 \nThe Q-Insertion step inserts @ s at the it- We use subscript here purely to identify individual occurrences; \nthey are not SSA versions. SSA form is actually a kind of factored use-def representation; discussion \nof this can be found in [Wo196]. erated dominance frontiers of each computation to serve as anchor points \nfor placement analysis. In the Rename step, we assign SSA versions to occurrences according to the val- \nues they compute. The resulting SSA versions encode the redundancy edges of the sparse graph as follows: \nif an oc-currence has the same SSA version as an earlier occurrence, it is redundant with respect to \nthat earlier occurrence. In our sparse approach to PRE, the next two steps, DownSafety and WtiBeAvail, \nperform data flow analysis on the sparse graph. The results enable the next step, Fi-nalize, to pinpoint \nthe locations in the program to insert the computation. These insertions make partially redundant OC-currences \nbecome fully redundant, which are marked. At this point, the form of the optimized output has been deter- \nmined. The final step, CodeMotion, transforms the code to form the optimized program. 6.2 Duality between \nLoads and Stores For register promotion, we assign a unique pseudo-register r for each memory location \ninvolved in load and store place- ment optimization. For indirect loads and stores, we assign a unique \npseudo-register for each lexically identical address expression. The discussion in this section applies \nto both direct and indirect loads and stores, though we use direct loads and stores as examples in the \ndiscussion. A load of the form r t z is fully (partially) redundant if the load is fully (partially) \navailable. Thus, given two oc-currences of the loads, the later occurrence is the redundant occurrence. \nOn the other hand, a store, of the form z t r, is fully (partially) redundant if the store is fully (partially) \nanticipated. Given two occurrences of the stores, the ear-lier occurrence is the redundant occurrence \n(Figure 1). As a result, redundancy edges for loads point backwards with re-spect to the control flow, \nwhile redundancy edges for stores point forward. The availability and anticipation of a load is killed \nwhen the memory location is modified. On the other hand, the availability and anticipation of a store \nis killed when the memory location is used; the movement of an available store is blocked additionally \nby an aliased store. A load of z is fully redundant with respect to an ear-lier load of 2 only if the \nearlier load occurs at a place that dominates the current load, because this situation implies the earlier \nload must be executed before control flow reaches the current load. Since redundancy edges are factored \nacross control flow merge points, the targets of the new edges al- ways dominate their sources. All the \nedges now represent full load redundancies, and partia1 load redundancies are ex- posed when there are \nI operands in the factoring operator a. A store to I is fully redundant with respect to a later store \nto x onIy if the later store occurs at a place that post-dominates the current store, because this implies \nthe later store must eventually be executed after the current store. Since redundancy edges are factored \nacross control flow split points, the targets of the new edges always post-dominates their sources. All \nthe edges now represent full store redun-dancies, and partial store redundancies are exposed when there \nare I operands in the factoring operator A. In per- forming PRE, we move loads backward with respect \nto the control flow and insert them as late as possible to minimize r s lifetime; for stores, however, \nwe move them forward and insert them as early as possible to minimize r s lifetime. We define static \nsingle use (SSU) form to be the dual of SSA form; in SSU form each use of a variable establishes a new \nversion (we say the load uses the version), and every store reaches exactly one load. Just as the SSA \nfactoring Table 1: Duality between load and store redundancies operator Q, is regarded as a definition \nof the corresponding variable and always defines a new version, the SSU factoring operator A is regarded \nas a use of its variable and always establishes (uses) a new version. Each use post-dominates all the \nstores of its version. Just as SSA form serves as the framework for the SSAPRE algorithm, SSU form serves \nas the framework for our algorithm for eliminating partial redundancy among stores, which we call SSUPRE. \nWe an- notate SSU versions using superscripts. Table 1 summarizes our discussion on the duality be-tween \nload and store redundancies. 6.3 SSUPRE Algorithm Having established the duality between load and store \nre-dundancies, we are now ready to give the algorithm for our sparse approach to the PRE of stores. For \na general store statement, of the form z t (expr), we view it as if it is made up of the sequence: r \nt (expr) x+-r  PRE is only applied to the store E t r, where z is a direct or indirect store and r is \na pseudo-register. For maximum effectiveness, the PRE of stores should be performed after the PRE of \nloads, because the PRE of loads will convert many loads into register references so they would not block \nthe movement of the stores, as shown in Figure 2. Our SSUPRE algorithm for the PRE of stores is tran- \nscribed and dualized from the SSAPRE algorithm, except that it cannot exploit the SSA representation \nof the input in the same way as SSAPRE. As a result, it is less efficient than SSAPRE on a program represented \nin SSA form. To achieve the same efficiency as SSAPRE, it would have been necessary for the input program \nto be represented in SSU form. Such a representation of the input is not practical because it would benefit \nonly this particular optimization, so SSUPRE constructs the required parts of SSU form on demand. Like \nSSAPRE, the SSUPRE algorithm is made up of six steps, and is applicable to both direct and indirect stores. \nIt works by constructing the graph of factored redundancy edges of the stores being optimized, called \nthe SSU graph. The first two steps, A-Insertion and Rename, work on all stores in the program at the \nsame time while conducting a pass through the entire program. The remaining steps can be applied to each \nstore placement optimization candidate one at a time. 6.3.1 The A-Insertion Step The purpose of A is \nto expose the potential insertion points for the store being optimized. There are two different scenar-ios \nfor A s to be placed. First, A s have to be placed at the iterated post-dominance frontiers of each store \nin the pro-gram. Second, h s also have to be placed when a killed store reaches a split point; since \nstores are killed by loads, this means A s have to be placed at the iterated post-dominance frontiers \nof each load (including aliased load) of the memory location. In Figure 10(b), the A at the bottom of \nthe loop body is inserted due to its being a post-dominance frontier of x t inside the loop, and the \nA at the split in the loop body is inserted due to its being a post-dominance frontier of the use of \nx in one branch of the split. A insertion is per- formed in one pass through the entire program for all \nstores that are PRE candidates. 6.3.2 The Rename Step This step assigns SSU versions to all the stores. \nEach use is assigned a new SSU version, which is applied to the stores that reach it. Each A is assigned \na new SSU version because we regard each A as a use. The result of renaming is such that any control \nflow path that includes two different ver-sions must cross an (aliased) use of the memory location or \na A. Renaming is performed by conducting a preorder traver-sal of the post-dominator tree, beginning \nat the exit points of the program. We maintain a renaming stack for every store that we are optimizing. \nWhen we come across an aliased load (use) or A, we generate a new SSU version and push it onto the stack. \nWhen we come to a store, we assign it the SSU version at the top of its stack and also push it onto the \nstack. Entries on the stacks are popped as we back up the blocks containing the uses that generate them. \nThe operands of h s are renamed at the entries of their cor-responding successor blocks. The operand \nis assigned the SSU version at the top of its stack if the top of its stack is a store or a A; otherwise, \nit is assigned 1. To recognize that local variables are dead at exits, we assume there is a virtual store \nto each local variable at each exit of the program unit. Since these virtual stores are first occurrences \nin the preorder traversal of the post-dominator tree, they are assigned unique SSU versions. Any stores \nfurther down in the post-dominator tree that are assigned the same SSU versions are redundant and will \nbe deleted. 6.3.3 The UpSafety Step One criterion required for PRE to insert a store is that the store \nbe up-safe (i.e., available) at the point of insertion. This step computes up-safety for the A s by forward \npropa-gation along the edges of the SSU graph. A A is up-safe if, in each backward control flow path \nleading to the procedure entry, another store is encountered before reaching the pro- cedure entry, an \naliased load or an aliased store. The propa- gation algorithm is an exact transposition of the DownSafety \nalgorithm in SSAPRE. To perform speculative store placement, we apply the strategies discussed in Section \n5. Figure 11 shows an exam- ple where a store is speculatively moved out of the loop. 6.3.4 The WillBeAm \nStep The WillBeAnt step predicts whether the store will be an-ticipated at each A following insertions \nfor PRE. The al-gorithm again is an exact transposition of the WillBeAvail algorithm in SSAPRE. It consists \nof two backward propaga-tion passes performed sequentially. The first pass computes the can-be-ant predicate \nfor each A. The second pass works within the region of can-be-ant A s and compute earlier. A wba = 1 \nXt (a) original code (b) after A insertion (c) after WillBeAnt (d) final result *p is an aliased use \nof x Figure rtx \\ r t (expr) / :,c 1 X+r (a) original code (b) speculative store inserted Figure 11: \nSpeculative insertion of store false value of earlier implies that the insertion of store can-not be \nhoisted earlier without introducing unnecessary store redundancy. At the end of the second pass, will-be-ant \nfor a A is given by: will-be-ant = can-be-ant A -earlier. Figure 10(c) shows the values of upsafe (us), \ncan-be-ant (cba), earlier and will-be-ant (wba) for the example at each A. The predicate insert indicates \nwhether we wiII perform insertion at a A operand. insert holds for a A operand if and only if the following \nhold: l The A satisfies will-be-ant; and . the operand is I or hasreal-def is faise for the operand and \nthe operand is used by a A that does not satisfy will-be-ant; i.e., the store is not anticipated at the \nA operand. 6.3.5 The Finalize Step The Finalize step in SSUPRE is simpler than the corre-sponding step \nin SSAPRE because placement optimization of stores does not require the introduction of temporaries. \nThis step only identifies the stores that will be fully redun-dant after taking into account the insertions \nthat wiII be performed. This is done in a preorder traversal of the post- dominator tree of the program. \nRenaming stacks are not required, because SSU versions have already been assigned. For each store being \noptimized, we update and use an array Ant-use (cf. AvaiLdef in SSAPRE) indexed by SSU version to identify \nstores that are fully redundant. 10: Sparse PRE of stores 6.3.6 The CodeMotion Step This last step performs \nthe insertion and deletion of stores to reflect the results of the store placement optimization. The \nstores inserted always use the pseudo-register as their right hand side, and are of either of the following \nforms depending on whether the store is direct or indirect: Xtr or *ptr It is necessary to make sure \nthat the value of the pseudo-register r is current at the inserted store. This implies that we need to \ncheck if the definitions of r track the definitions of the redundant stores. To do this, we follow the \nuse-def edges in SSA form to get to ah the definitions of x that reach the point of store insertion. \nIf the right hand side of a definition is r, the store is simply deleted. If the right hand side is not \nr, we change it to r, thereby removing the store, which must have been marked redundant x t (expr) * \nr In cases where the inserted store necessary to insert a load on the by the Finalize step: t (expr) \nis speculative, it may be path where the store is not available, so that the pseudo-register wiII have \nthe right value at the inserted store. In the example of Figure 11, the load r c x is inserted at the \nhead of the loop for this reason. One of our requirements is that the program be main- tained in valid \nSSA form. This implies introducing d s at iterated dominance frontiers and assigning the correct SSA \nversions for r and x.s The current version for r can easily be found by following the use-def edges of \nx. For x, we as- sign a new SSA version in each store inserted. Uses of x reached by this inserted store \nin turn need to be updated to the new version, and can be conveniently handled if def-use chains are \nalso maintained. Instead of maintaining def-use chains, we find it more expedient to perform this task \nfor aII affected variables by adding a post-pass to SSUPRE. The post-pass is essentially the renaming \nstep in the SSA con-struction algorithm, except that rename stacks only need to be maintained for the \naffected variables. The right hand side is a pseudo-register if the store was a left oc-currence that \neffected redundancy elimination in the load placement phase. *In the case of indirect stores, the virtual \nvariables have to be maintained in correct SSA versions; see [CCL+96]. PRE of PRE of PRE of PRE of benchmark \nloads off loads on ratio benchmark stores off stores on ratio A B B/A _ 099.go 23498625 23210822 0.988 \n124.m88ksim 9174980 9138470 0.996 132.ijpeg 258354279 227019081 0.879 134.perl 357417768 293678627 0.822 \n147.vortex 558680056 395136113 0.707 geom. mean 0.744 Table 2: Dynamic counts of all loads executed in \nSPECint95 7 Measurements We have implemented the register promotion techniques de-scribed in this paper \nin the Silicon Graphics MlPSpro Com-pilers Release 7.2. In this section, we study the effectiveness of \nour techniques by compiling the SPECint95 benchmark suite and measuring the resulting dynamic load and \nstore counts when the benchmarks are executed using the train-ing input data. The benchmarks were compiled \nat the -02 optimization level with no inlining. Only intra-procedural alias analysis was applied. The \nmeasurement data are gath- ered by simulating the compiled program after register pro-motion, but before \ncode generation and register allocation. In the simulation, each access to a pseudo-register is not counted \nas load or store. This is equivalent to assuming that the underlying machine has an infinite number of \nregisters; the assumption allows us to measure the effects of register promotion without confounding \neffects such as spilling per-formed by the register allocator. Our measurement data are organized into \ntwo sets. In the first set, we measure the overall effectiveness of the SSAPRE and SSUPRE approaches \nin removing scalar and indirect loads and stores in the programs. In the second set, we study the relative \nperformance of the different specula-tive code motion strategies presented in Section 5. 7.1 Overall \nPerformance Since our register promotion is made up of the PRE of loads and the PRE of stores, we present \nour data for loads and stores in separate tables. Tables 2 and 3 show the ef-fects of performing PRE \nof loads and stores respectively on the SPECint95 benchmarks, without speculative code motion. The data \nshown include both scalar and indirect loads/stores. Column A in the tables shows the number of loads/stores \nexecuted in the benchmark programs if register promotion is turned off. Even though register promotion \nis disabled, non-aliased local variables and compiler-generated temporaries are still assigned pseudo-registers \nby other parts of the compiler, so the baselines shown in column A repre- sent quite respectable code \nquality. Column B shows the effects on the number of executed loads and stores when the PRE of loads \nand the PRE of stores are enabled. According to Table 2, the PRE of loads reduces the dy- namic load \ncounts by an average of 25.6%. In contrast, Ta-ble 3 shows the PRE of stores is able to reduce the dynamic \nstore counts by only an average of 1.2%. There are a number of reasons. First, earlier optimization phases \nhave applied the SSA-based dead store elimination algorithm [CFR+91], which efficiently removes all faint \nand dead stores; the only opportunities left are those exposed by the removal of loads geom. mean 1 j \n0.988 1 , Table 3: Dynamic counts of all stores executed in SPECint95 or those due to strictly partial \nstore redundancy. The side ef- fect of earlier loop normalization also moves invariant stores to the \nend of loops [LLC96]. Second, for aliased variables, there are usually aliased uses around aliased stores, \nand these uses block movement of the stores. Third, apart from aliased local variables, the other candidates \nfor the PRE of stores are global variables, and they tend to exhibit few store redundancies. Our PRE \nof stores is performed after the PRE of loads. If the PRE of stores is performed when the PRE of loads \nis turned off, the resulting dynamic store counts are identical to those in column A, indicating that \nremoving loads is crucial to the removal of stores. 7.2 Speculative Code Motion Strategies In this section, \nwe study the relative performance of the different speculative code motion strategies we presented in \nSection 5. Although speculative code motion is applicable to any computation, we concern ourselves only \nwith loads and stores in this paper. In our target architecture, the MIPS RlOOOO, indirect loads and \nindirect stores cannot be speculated. Thus, we exclude register promotion data for indirect loads and \nstores from this section, and present data only for scalar loads and stores. Tables 4 and 5 show the \neffects of different speculative code motion strategies on the executed scalar load and store counts \nrespectively in the SPECint95 benchmarks. Our baseline is column A, which shows the scalar load and store \ncounts without register promotion. Column B shows the same data when PRE of loads and stores are applied \nwith-out speculation. Column C shows the result of applying the conservative speculative code motion \nstrategy we pre-sented in Section 5.1. Column D shows the result when we apply the profile-driven speculative \ncode motion strategy de-scribed in 5.2 guided by execution profile data. Column E is provided for comparison \npurposes only; it shows the results if we perform full speculation, as defined in Section 5, in the PRE \nof loads and stores. Only column D in the tables uses profile data. According to the results given by \ncolumn C, D and E, full speculation yields the worst performance of the three differ-ent speculative \ncode motion strategies. This supports our conviction that it is worth the effort to avoid overdoing spec-ulation. \nFull speculation yields improvement over no spec-ulation only with loads in 126.gcc and stores in 134.perl. \nWhen profile data are unavailable, our conservative specu-lation strategy yields mixed results compared \nwith no spec- ulation, as indicated by comparing columns B and C. There is no effect on a number of the \nbenchmarks. Where there is a change, the effect is biased towards the negative side. Our Table 4: Dynamic \ncounts of scalar loads executed in SPECint95 due to the three different speculation strategies geom. \nmean 11 ( 0.958 u 1 0.962 1 1 0.952 1 1 0.987 Table 5: Dynamic counts of scalar stores executed in SPECint95 \ndue to the three different speculation strategies conservative speculative code motion strategy can increase \nthe executed operation count if some operations inserted in loop headers would not have been executed \nin the absence of speculation. In the case of the SPECint95 benchmark suite, this situation seems to \narise more often than not. When profile data are available, our profile-driven spec-ulative code motion \nstrategy consistently yields the best re-sults. This outcome is indicated in column D, which shows the \nbest number for each benchmark among all the columns in Tables 4 and 5. The data show that our profile-driven \nspeculation strategy is successful in making use of execution frequency information in avoiding over-speculation \nby spec- ulating only in cases where it is certain of improvement. In programs with complicated control \nflow like 126.gcc, our profile-driven speculation yields greater improvements. Since inlining augments \ncontrol flow, we can expect even better results if the benchmarks are compiled with inlining. Overall, \nprofile-driven speculation contributes to 2% further reduction in dynamic load counts and 0.5% further \nreduction in dynamic store counts. When profile-driven speculation is applied to other types of operations \nthat can be speculated, we can expect similar reduction in dynamic counts for those operations. Given \nthe current trend toward hardware sup-port for more types of instructions that can be speculated, we \ncan expect our profile-driven speculation algorithm to play a greater role in improving program performance \nfor newer generations of processors. Conclusion In this paper, we have presented a pragmatic approach \nto register promotion by modeling the optimization as two sep- arate problems: the PRE of loads and the \nPRE of stores. Both of these problems can be solved through a sparse ap-proach to PRE. Since the PRE \nof loads uses the same algo- 36 rithm as the PRE of expressions, it can be integrated into an existing \nimplementation of SSAPRE with minimal effort and little impact on compilation time. The PRE of stores \nuses a different algorithm, SSUPRE, which is the dual of SSAPRE, and is performed after the PRE of loads, \ntaking advantage of the loads having been converted into pseudo-register ref- erences so that there are \nfewer barriers to the movement of stores. SSUPRE is not as efficient as SSAPRE because the SSA form of \nits input does not directly facilitate the construction of SSU form. Since register promotion is rele- \nvant only to aliased variables and global variables, the num-ber of candidates in each program unit is \nusually not large. Therefore a sparse, per-variable approach to the problem is justified. In contrast, \na bit-vector-based approach takes ad- vantage of the parallelism inherent in bit vector operations, but \nincurs some larger fixed cost in initializing and operat-ing on the bit vectors over the entire control \nflow graph. As a result, we have seen very little degradation in compilation time with our sparse approach \ncompared to a bit-vector-based implementation that precedes this work. We have presented techniques to \neffect speculative code motion in our sparse PRE framework. In particular, the profile-driven speculation \ntechnique enables us to use pro- file data to control the amount of speculation. These tech-niques could \nnot be efficiently applied in a bit-vector-based approach to PRE. Our measurement data on the SPECint95 \nbenchmark suite demonstrate that substantial reduction in load counts is possible by applying our techniques \nto aliased variables, global variables and indirectly accessed memory locations. There is not a large \nreduction in store counts due to earlier optimizations in the compiler. Our study of the different speculative \ncode motion strategies shows that the profile-driven speculative code motion algorithm is most promising \nin improving program performance over what can be achieved by partial redundancy elimination. References \n[BG97] [Bri92] [CAC+sl] [CCK+97] [CCL+961 [CFR+Sl] [CH90] [Cho83] [CL971 [DhaSS] [DhaSO] [DRZ92] R. Bodik \nand R. Gupta. Partial dead code elimi- nation using slicing transformations. In Proceed- ings of the \nACM SIGPLAN 97 Conference on Programming Language Design and Implemen-tation, pages 159-170, June 1997. \nP. Briggs. Rematerialization. In Proceedings of the ACM SIGPLAN 92 Conference on Pro-gramming Language \nDesign and Implementa-tion, pages 311-321, June 1992. G. Chaitin, M. Auslander, A. Chandra, J. Cocke, \nM. Hopkins, and P. Markstein. Regis-ter allocation via coloring. Computer Languages, 6:47-57, January \n1981. F. Chow, S. Chan, R. Kennedy, S. Liu, R. Lo, and P. Tu. A new algorithm for partial redun-dancy \nelimination based on SSA form. In Pro-ceedings of the ACM SIGPLAN 97 Conference on Programming Language \nDesign and Imple-mentation, pages 273-286, June 1997. F. Chow, S. Chan, S. Liu, R. Lo, and M. Streich. \nEffective representation of aliases and indirect memory operations in SSA form. In Proceedings of the \nSixth International Conference on Com-piler Construction, pages 253-267, April 1996. R. Cytron, J. Ferrante, \nB. Rosen, M. Wegman, and F. Zadeck. Efficiently computing static sin-gle assignment form and the control \ndependence graph. ACM Trans. on Programming Languages and Systems, 13(4):451-490, October 1991. F. Chow \nand J. Hennessy. The priority-based coloring approach to register allocation. ACM Trans. on Programming \nLanguages and Systems, 12(4):501-536, October 1990. F. Chow. A portable machine-independent global optimizer \n-design and measurements. Technical Report 83-254 (PhD Thesis), Com-puter Systems Laboratory, Stanford \nUniversity, December 1983. K. Cooper and J. Lu. Register promotion in C programs. In Proceedings of the \nA CM SIGPLAN 97 Conference on Programming Language De-sign and Implementation, pages 308-319, June 1997. \nD. Dhamdhere. Register assignment using code placement techniques. Journal of Computer Languages, 13(2):75-93, \n1988. D. Dhamdhere. A usually linear algorithm for register assignment using edge placement of load and \nstore instructions. Journal of Computer Languages, 15(2):83-94, 1990. D. Dhamdhere, B. Rosen, and K. \nZadeck. How to analyze large programs efficiently and infor-matively. In Proceedings of the ACM SIGPLAN \n92 Conference on Programming Language De-sign and Implementation, pages 212-223, June 1992. [DS93] [FKCX94] \n[GBF97a] [GBF97b] [KRS92] [KRS94a] [KRS94b] [LLC96] [MR79] [TWL+Sl] [ Wo196] K. Drechsler and M. Stadel. \nA variation of knoop, tithing and steffen s lazy code motion. SIGPLAN Notices, 28(5):29-38, May 1993. \nL. Feigen, D. Klappholz, R. Casazza, and X. Xue. The revival transformation. In Confer-ence Record of \nthe Twenty First ACM Sympo-sium on Principles of Programming Languages, pages 147-158, January 1994. \nR. Gupta, D. Berson, and J. Fang. Path profile guided partial dead code elimination using pred- ication. \nIn Proceedings of the Fifth International Conference pilation T1997. on Parallel echniques, p Architectures \nages 102-113, and Com-November R. Gupta, D. Berson, and J. Fang. Resource- sensitive profile-directed \ndata flow analysis for code optimization. In Proceedings of the 30th Annual International Symposium on \nMicroarchi-tecrure, pages 358-368, December 1997. J. Knoop, 0. Riithing, and B. Steffen. Lazy code motion. \nIn Proceedings of the ACM SIGPLAN 92 Conference on Programming Language De-sign and Implementation, pages \n224-234, June 1992. J. Knoop, 0. Riithing, and B. Steffen. Opti-mal code motion: Theory and practice. \nACM Trans. on Programming Languages and Systems, 16(4):1117-1155, October 1994. J. Knoop, 0. Riithing, \nand B. Steffen. Par-tial dead code elimination. In Proceedings of the ACM SIGPLAN 94 Conference on Pro-gramming \nLanguage Design and Zmplementa-tion, pages 147-158, June 1994. S. Liu, R. Lo, and F. Chow. Loop induction \nvari-able canonicalization in paraIIeIizing compilers. In Proceedings of the Fourth International Con-ference \non Parallel Architectures and Compila-tion Techniques, pages 228-237, October 1996. E. Morel and C. Renvoise. \nGlobal optimization by suppression of partial redundancies. Comm. ACM, 22(2):96-103, February 1979. S. \nTjiang, M. Wolf, M. Lam, K. Pieper, and J. Hennessy. Integrating scalar optimization and parallelization. \nIn Proc. 4th International Work-shop on Languages and Compilers for Parallel Computing, pages 137-151, \nAugust 1991. M. Wolfe. High Performance Compilers For Parallel Computing. Addison Wesley, 1996.  \n\t\t\t", "proc_id": "277650", "abstract": "An algorithm for register promotion is presented based on the observation that the circumstances for promoting a memory location's value to register coincide with situations where the program exhibits partial redundancy between accesses to the memory location. The recent SSAPRE algorithm for eliminating partial redundancy using a sparse SSA representation forms the foundation for the present algorithm to eliminate redundancy among memory accesses, enabling us to achieve both computational and live range optimality in our register promotion results. We discuss how to effect speculative code motion in the SSAPRE framework. We present two different algorithms for performing speculative code motion: the <i>conservative</i> speculation algorithm used in the absence of profile data, and the the <i>profile-driven</i> speculation algorithm used when profile data are available. We define the static single use (SSU) form and develop the dual of the SSAPRE algorithm, called SSUPRE, to perform the partial redundancy elimination of stores. We provide measurement data on the SPECint95 benchmark suite to demonstrate the effectiveness of our register promotion approach in removing loads and stores. We also study the relative performance of the different speculative code motion strategies when applied to scalar loads and stores.", "authors": [{"name": "Raymond Lo", "author_profile_id": "81100147119", "affiliation": "Silicon Graphics Computer Systems, 2011 N. Shoreline Blvd., Mountain View, CA", "person_id": "PP37024390", "email_address": "", "orcid_id": ""}, {"name": "Fred Chow", "author_profile_id": "81100327963", "affiliation": "Silicon Graphics Computer Systems, 2011 N. Shoreline Blvd., Mountain View, CA", "person_id": "P86530", "email_address": "", "orcid_id": ""}, {"name": "Robert Kennedy", "author_profile_id": "81100450533", "affiliation": "Silicon Graphics Computer Systems, 2011 N. Shoreline Blvd., Mountain View, CA", "person_id": "PP31082148", "email_address": "", "orcid_id": ""}, {"name": "Shin-Ming Liu", "author_profile_id": "81100526198", "affiliation": "Silicon Graphics Computer Systems, 2011 N. Shoreline Blvd., Mountain View, CA", "person_id": "P263984", "email_address": "", "orcid_id": ""}, {"name": "Peng Tu", "author_profile_id": "81100439481", "affiliation": "Silicon Graphics Computer Systems, 2011 N. Shoreline Blvd., Mountain View, CA", "person_id": "PP15032280", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/277650.277659", "year": "1998", "article_id": "277659", "conference": "PLDI", "title": "Register promotion by sparse partial redundancy elimination of loads and stores", "url": "http://dl.acm.org/citation.cfm?id=277659"}