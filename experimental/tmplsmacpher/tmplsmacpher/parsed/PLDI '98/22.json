{"article_publication_date": "05-01-1998", "fulltext": "\n Thin Locks: Featherweight Synchronization for Java David F. Bacon Ravi Konuru Chet Murthy Mauricio Serrano \nIBM T.J. Watson Research Center Abstract Language-supported synchronization is a source of serious performance \nproblems in many Java programs. Even single-threaded applications may spend up to half their time per-forming \nuseless synchronization due to the thread-safe na-ture of the Java libraries. We solve this performance \nprob-lem with a new algorithm that allows lock and unlock oper-ations to be performed with only a few \nmachine instructions in the most common cases. Our locks only require a partial word per object, and \nwere implemented without increasing object size. We present measurements from our implemen-tation in \nthe JDK 1.1.2 for AIX, demonstrating speedups of up to a factor of 5 in micro-benchmarks and up to a \nfactor of 1.7 in real programs. 1 Introduction Monitors [5] are a language-level construct for providing \nmu-tually exclusive access to shared data structures in a multi- threaded environment. However, the overhead \nrequired by the necessary locking has generally restricted their use to relatively heavy-weight objects. \nRecently, their incorporation in Java [3] has led to re-newed interest in monitors, because of both their \nprevalence and their associated performance problems. Java uses mon- itor semantics derived from Mesa \n[ll]. In Java, the methods of an object may be declared synchronized meaning that the object must be \nlocked for the duration of the method s ex-ecution. Since Java is an explicitly multi-threaded language, \nde-signers of general-purpose class libraries must make their classes thread-safe. For instance, the \nmost commonly used public methods of standard utility classes like Vector and Hashtable are synchronized. \nWhen these classes are used by single-threaded programs, or used locally within a thread, there is substantial \nperformance degradation in t%e absence of any trve concurrency. We have measured slowdowns due to synchronization \nof up to a factor of two in both compiled and interpreted Java programs. One way to speed up synchronization \nis to dedicate a Q 1988 ACM 0.89791~9874/98/0006...16.00 portion of each object as a lock. Unfortunately, \nJava s de-sign inherently allows any object to be synchronizable, even those without synchronized methods. \nTherefore, adding one or more synchronization words to each object is an unac-ceptable space-time tradeoff. \nThe current Sun JDK favors space over time. Monitors are kept outside of the objects to avoid the space \ncost, and are looked up in a monitor cache. Unfortunately this is not only inefficient, it does not scale \nbecause the monitor cache itself must be locked during lookups to prevent race condi-tions with concurrent \nmodifiers. In addition, if large num-bers of synchronized objects are created, the space overhead of \nthe monitor structures may be considerable. In this paper we describe thin locks, an implementation of \nmonitors in IBM s version of the JDK 1.1.2 for AIX. Our implementation has the following desirable characteristics: \nSpeed In the absence of contention both initial locking and nested locking are very fast -only a few \nmachine in-structions. In the presence of contention performance is still better than in the JDK. Compactness \nOnly 24 bits in each object are used for lock- ing, but object size is not increased due to other space \ncompression techniques. Scalability Use of global locks and of synchronization in-structions that must \nbe broadcast over the global bus are kept to an absolute minimum, allowing efficient execution on large \nmultiprocessors. Simplicity The scope of changes required in the JVM is small -thin locks are implemented \nas a veneer over the existing heavy-weight locking facilities. Maintainability The thin lock code is \nfully portable, as-suming only the existence of a compare-and-swap op-eration. While hand-coded assembly \nlanguage routines are required for maximum performance, the amount of platform-specific assembly language \ncode is small and localized into two functions in a single file. The goal was a locking algorithm with \nvery low overhead for single-threaded programs, but also with excellent per-formance in the presence \nof multithreading and contention. These parameters are appropriate to a Java server or to a client that \nis running windowing or network code that is likely to involve multiple threads of control. 1.1 Outline \nSection 2 describes the locking algorithm in detail. Section 3 presents measurements from our implementation \nand com-pares them against two other JDK-based implementations. Section 4 discusses related work, and \nSection 5 presents con-clusions and directions for future work. 2 Locking Algorithm In order to properly \noptimize Java s locking performance, one must know what the most common cases are. Implicit in our design \nis the assumption that the order of frequency of different locking scenarios is as follows, with each \nsce-nario about an order of magnitude less common than the one preceding it: I. locking an unlocked object. \n2. locking an object already locked by the current thread a small number of times (shallowly nested locking). \n 3. locking an object already locked by the current thread many times (deeply nested locking). 4. attempting \nto lock an object already locked by another thread, for which no other threads are waiting. 5. attempting \nto lock an object already locked by another thread, for which other threads are already waiting.  We \nprovide detailed measurements supporting our as-sumptions in Section 3.2. The measurements show that \nfor our benchmarks, a median of 80% of all lock operations are on unlocked objects, and that nesting \nis very shallow. 2.1 Software Environment We assume that there is a pre-existing heavy-weight system \nin place to support the full range of Java synchronization se-mantics, including queuing of unsatisfied \nlock requests, and the wait, notify, and notifyAll operations. Such a system will represent a monitor \nas a multi-word structure which in-cludes space for a thread pointer, a nested lock count, and the necessary \nqueues. We refer to such multi-word lock objects as fat locks.  2.2 Hardware Support Almost all modern \nhigh-performance microprocessors are designed so that they can be used in multi-processor sys-tems. Therefore, \nthey provide user-mode instructions for performing synchronization -either compare-and-swap (in-troduced \nin the IBM 370 and provided on Intel 80486 and higher processors) or load-and-reserve and store-conditional \ninstructions (as on the PowerPC, Alpha, and Spare) which allow various atomic primitives to be synthesized \nfrom short instruction sequences. We will assume only the existence of a compare-and-swap operation (either \nas a primitive instruction or as a synthe- sized operation). On older systems without user-level atomic \nprimitives, some other mechanism for achieving atomicity will be required. Our implementation is binary-compatible \nwith both PowerPC machines and with older POWER ar-chitecture machines that do not have load-and-reserve. \nIn the latter case, we use a system call to a compare-and-swap operation that is implemented by the kernel. \nThe compare-and-swap operation takes three inputs: an address, an old value, and a new value. If the \ncontents of the (a) Object layout showing lock word (b) Lock word structure for thin lock  (c) Unlocked \n(d) Locked once by thread A (e) Locked twice by thread A  Figure 1: Thin Locks address is equal to \nthe old value, the new value is stored at the address and the operation returns true; if the contents \nof the address are not equal to the old value, storage remains unchanged and the operation returns false. \nThe compare-and-swap operation is atomic. 2.3 Monitor Implementation In the Java run-time system upon \nwhich we based our imple- mentation (IBM s AIX port of the 1.1.2 JDK), each object consists of a three-word \nheader followed by data. In order to implement thin locks, we reserve 24 bits in the header of each object \nas shown in Figure l(a). We were able to obtain 24 free bits using various encoding techniques for the \nother values that are typically stored in the header. Allocating an extra word per object was deemed \nunacceptable both be-cause of the additional space overhead and because there is already a substantial \nbody of native code with dependencies on the object size. The 8 bits that share the word with the lock \nfield are either constant or subject to change only when an object is moved, and since the garbage collector \nis not concurrent we can treat those 8 bits a.s constant values. The structure of the 24-bit lock field \nhas been very care-fully engineered to allow the most common locking and un-locking operations to be \nperformed with the minimum num-ber of machine instructions. The lock field represents either a thin lock \nor a reference to a fat lock: the first bit (the monitor shape bit) is 0 if the lock is thin and 1 if \nit is fat. Thin locks are used for objects that are not subject to contention, do not have wait, notify, \nor notifyAll operations performed upon them, and are not locked to an excessive nesting depth (in our \nimplementation, we define excessive as 257). The vast majority of all objects meet this criterion; those \nthat do not have their locks implemented as fat locks. (a) Lock word stmchm for inflated lock I I (b) \nLocked once by thread B I I(c) Unlocked Figure 2: Mated Locks Once an object s lock is inflated, it remains \ninflated for the lifetime of the object. This discipline prevents thrash-ing between the thin and fat \nstates. It also considerably simplifies the implementation. The structure of a thin lock is shown in \nFigure l(b). The monitor shape bit is 0. The remaining 23 bits are divided between a thread identifier \n(15 bits) and a nested lock count (8 bits). If the thread identifier is 0, the object is unlocked (and \nthe count field must also be 0). If the thread identifier is non-zero, it is an index into a table we \nmaintain which maps thread indices to thread pointers. When the object is locked, the count field represents \nthe number of locks minus one. The structure of an intlated lock is shown in Figure 2(a). The monitor \nshape bit is one, and the remaining 23 bits of the lock field contain the index of a fat lock. We maintain \nthe table which maps inflated monitor indices to fat locks. Figure 2(b) shows an inflated lock with \nthe fat lock to which the lock index M refers. The fat lock contains a thread identifier for the lock \nowner (in this case, thread B), a count of the number of locks (not the number of locks minus one, as \nin a thin lock), the necessary queues, and other fields. The locking algorithm is greatly simplified \nby adopting a discipline in which the lock field of an object, while it is owned by a particular thread, \nis never modified by any other thread. This has major performance implications: once a thread has locked \nan object, all subsequent operations (in-cluding the unlock) can be performed with loads and stores, \ninstead of with atomic primitives. 2.3.1 Locking without Contention We will now explain the operation \nof the locking algorithm by taking an object through a series of locking operations under varying conditions. \nInitially, the object is unlocked, so the entire lock field is 0, as shown in Figure l(c). Thread A wishes \nto lock the object. Assuming that the object is unlocked (since this is the most common case), thread \nA performs a compare-and-swap operation on the word containing the lock field. The old value supplied \nto the compare-and-swap is that shown in Figure l(c), which is constructed by loading the lock word and \nmasking out the high 24 bits. The new value is a lock field containing a monitor shape bit of 0, a thread \nindex corresponding to thread A, and a count of 0, as shown in Figure l(d). The new value is constructed \nby taking the bitwise or of the old value and the thread index, shifted 16 bits to the left. The thread \nindex of the currently running thread is stored in the execution environment structure, and can be accessed \nwith a single load instruction. The thread index is stored pre-shifted by 16 bits, so that the locking \ncode does not have to perform an extra ALU operation. If the compare-and-swap succeeds, then the object \nwas not already locked by another thread (and no concurrently locking thread obtained the lock). By convention, \nthe count field is the number of locks minus one, so the compare-and- swap operation has already properly \nset the count, and the lock operation is complete. 2.3.2 Unlocking without Contention At some later \ntime, thread A unlocks the object. Since the most common case for unlocking is that the current thread \nowns the lock and has locked the object once, we construct an old value as in Figure l(d) and a new value \nas in Figure l(c). However, instead of performing a compare-and-swap, we simply check that the value \nof the lock word is equal to our old value, and if so, store our new value into the lock word. Unlocking \ndoes not require a compare-and-swap because of our discipline that once a thread owns a lock, no other \nthread may modify the lock word. Locking is a stable prop-erty: if thread A owns the lock, the value \nwill not be stale; if thread A does not own the lock, it does not matter if the value read is stale, \nbecause any possible stale value will show that thread A does not own the lock.  2.3.3 Nested Locking \nand Unlocking Now assume that thread A once again locks the object, and then attempts to lock it a second \ntime. It will begin by performing the compare-and-swap operation, which will fall because the object \nis already locked (by thread A itself). The locking routine will then check for the next most likely \ncase: nested locking by the owning thread. In particular, the locking routine will check that the mon- \nitor shape bit is 0, that the thread index is A s thread index, and that the count field is less than \n255 (8 bits, all ones). The lock word layout has been designed so that this check can be implemented \nby taking the pre-shifted thread index of A, taking its bitwise exclusive-or with the contents of the \nlock word, and checking that the resulting quantity is less than 255 shifted left by 8 bits (which happens \nto fit into a 16-bit unsigned immediate field on most RISC architec-tures). If the check succeeds, the \ncount field is incremented by adding 256 to the lock word. The updated value, shown in Figure l(e), is \nwritten to memory using a simple store instruction, following the same argument that we applied to using \nstore instructions for unlocking. When thread A unlocks the object, an analogous proce-dure is followed \nfor decrementing the lock count. In the event that the nested lock count overflows, we inflate the lock. \nLock inflation is described more fully below. 2.3.4 Locking with Contention Assume now that thread A \nhas the object locked once, and thread B attempts to lock the object. Thread B wilI first attempt to \nlock the object with a compare-and-swap, which will fail. It wilI then check whether it has already locked \nthe object, and this test will fail. Therefore, the object is locked by another thread (thread A, in \nfact). Thread B therefore needs to force a transition on the object from a thin lock (monitor shape bit \nequal to 0) to an inflated lock (monitor shape bit equal to 1). However, our locking discipline is that \nthe lock field is only modified by the owning thread. Therefore, thread B enters a spin-locking loop \non the ob- ject. Once thread A unlocks the object, thread B obtains the lock. Thread B creates a fat \nlock, assigns a monitor index to the newly created monitor object, and changes the lock field to contain \na monitor shape bit of 1 and the new monitor index. Figure 2(b) shows the resulting lock struc-ture. \nThe monitor index M is an indirect reference to the fat lock, via the vector that maps monitor indices \nto monitor pointers. Finally, when thread B unlocks the object, it remains in the inhated state, as shown \nin Figure 2(c). Subsequent attempts to lock the object will use the fat lock, and if there is contention \nthe fat lock discipline wiII handle the necessary queuing. While spin-locking in general is undesirable, \nwe deem it to be acceptable because we are assuming a locality of contention principle: if there is contention \nfor an object once, there is likely to be contention for it again in the future. Therefore, we will only \npay spin-locking costs once and those costs will be amortized over the lifetime of the object. In general, \nthis works well. The only pathological case occurs when an object is locked by one thread and not released \nfor a long time, during which time other threads are spinning on the object. Standard back-off techniques \n[l] for reducing the cost of spin-locking can be applied to solve this problem. 3 Measurements In this \nsection we evaluate our implementation of thin locks in the JDK 1.1.2 for IBM s AIX operating system \non the PowerPC. We compare our implementation to a straight-forward port of Sun s JDK 1.1.1 to AIX (using \nthe POSIX threads package to support locking) and to IBM s 1.1.2 ver- sion of the JDK for AIX, which \ncontains significant monitor optimizations. We refer to the three versions as ThinLock , JDKlll , and \nIBM112 , respectively. The IBM112 implementation assumes that most appli-cations wilI have a small number \nof heavily used locks. It therefore pre-allocates a small number (32) of hot locks . The system begins \nby using the default fat locks, slightly modified to record locking frequency. When a fat lock is de- \ntected to be Lhot , a pointer to the hot lock is placed in the header of the object. Because a full 32-bit \npointer is used, the displaced header information is moved into the hot lock structure. One bit in the \nheader word indicates whether the word is a hot lock pointer or regular header data. The hot lock scheme \nallows the overhead of the monitor cache to be by-passed in the most common cases, However, as we wiII \nsee, it suffers when large numbers of locks are used, which happens more often than one might expect. \nAll performance measurements represent the median of 10 sample runs. Time measured is elapsed time on \nan un- loaded IBM RS/6000 43T workstation, containing a 120 MHz PowerPC 604 microprocessor with 128 MB \nof RAM memory. The 604 has 16KB 4-way associative split caches, 64 entry 2-way associative split TLB \ns, a 512KB direct-mapped physically addressed level two cache, and a 32 byte reservation gram size for \nthe load-and-reserve instructions. The 604 is a fairly aggressive superscalar processor for its generation. \nIt is capable of dispatching up to four instruc-tions per cycle, including two ALU operations. It also \nhas a 512 entry branch history table and performs speculative execution of instructions beyond unresolved \nbranches. These processor characteristics play a significant part in our low-level design and affect \nthe trade-offs we made in the hand-tuned assembly language code, as we will describe in more detail below. \n3.1 Macro-Benchmarks Table 1 summarizes the macro-benchmarks we used for our performance measurements. \nThese macro-benchmarks are real programs, and can therefore be expected to give some indication of the \ntype of speed-ups that could be obtained in practice. To give a sense of the scale of the benchmarks, \nthe size in bytes of both the application and the library bytecode files is given. Library bytecode size \nis for alI classes transi- tively reachable from the application bytecodes; alI code in the java and \nsun hierarchy is considered library code. To give an overall characterization of synchronization behavior, \nwe measured the total number of objects cre-ated, the number of objects that were synchronized, and the \ntotal number of synchronization operations. The num-ber of synchronized objects is generally less than \na tenth of the total number of objects created. The average num-ber of synchronizations per synchronized \nobject shows that re-synchronization is quite common; the median number of synchronizations per synchronized \nobject is 22.7. The benchmarks suffer from two disadvantages: they are alI single-threaded programs, \nand they are predominantly language processing tools. While it may initially seem nonsensical to use \nsingle-threaded benchmarks to measure speed-ups gained from a locking implementation, such benchmarks \nactually illustrate the point of our work. Thin locks are designed to be highly efficient when there \nis no sharing or when despite sharing there is no actual contention. By evaluating thin locks on single-threaded \nbenchmarks, we demonstrate that they are able to remove the performance tax that Java levies on single-threaded \napplications as the price of using a multi-threaded language. 3.2 Characterization of Locking In Section \n2 we made some assumptions about the relative frequency of various types of locking operations, upon \nwhich key aspects of the thin lock design were based. However, these assumptions should also be validated \nexperimentally. Figure 3 shows the frequency of locking operations by nesting depth. Because the benchmark \nprograms were single- threaded, the contention scenarios were not measured. The measurements do show \nthat locking unlocked objects is indeed far more common than any other case: at least 45% of locks obtained \nby any of the benchmark applications were for unlocked objects; the median is 80%. Program trans javac \n;A I inhe ran I javalex jax javacup NdR.eua uy=. ~yyv Ha~h.Tav;r ________-crema inNet, Description (source) \nHigh Performance Java Compiler (IBM) Java source to bytecode compiler (Sun) C~nmir T.ihmrv 1 fl IOhiecSnace1 \n --__-__--_I--J _.---J ---=--- Java Obiect Reauest Broker 0.5 (Freie U.) Java grammar parser (Sun) Java \nto C translator (K.B. Sriram) Java Obfuscator 1.0 (E. Jokioiij I .Tnv;l t,n C translator (U. Arizona) \n/ Lexical An alvzer* for Java (E. Berkl I .T;lva Scanner---__ Generator (J?B. S&#38;am) _-._ :tmrt.nr \nnf Pslrccr. (I; Hwlsnn\\ I Java Cons v_yIuv- __ _ II--I \\-. --------, I NetRexx to Java translator 1.0 \n(IBM) --.-I---- -lJ. ----_ __ ._-__ ---=---, I .Java Obfuscator ----.-~.~~~ ~~ f K.B. Sriram I Java Obfuscator \n(H.P. van VIiet) ___ -. ToolKit I .Java _ New-al-----Network-~.~~ ~~ ~~~~~ (W. Gander) I Tavrr rlnr11mf.nt \nvf=nl=mt.nr s11n -.-U-I Y___-__-o -__-_I ----- Java disassembler (Sun) Java decompiler H.P. van Vliet \nJava source to bytecode compiler (M. Odersky) Java decompiler, demo version (WingSoft) Lib Objects Sync \nd S yncs S yncs/ Size Size Objects S.Obj. 124751 159747 486215 49313 1 (173911 17.7 0 298436 345687 24735 \n1 856666 1 34.6 1 12182 159747 4258177 150175 12975639 86.4 APP I 59431 159747 39138 888390 22.7 52961 \n159747 31 621 20.0 23743 166472 70796 1611558 22.8 10105 159758 12243 90573 7.4 1 24154 1 161229 1 625039 \n1 119179 , 1651763 13.9 1 25058 1 159747 1 43392 1 10333 1 1975481 1 191.2 1 1 19182 ( 160963 1 24615 \n1 4629 1 19960283 [ 4312.0 I 30.569 I 160963 I 221093 I 23676 1 330100 1 13.94 -____ ----.----. ~~ I \nI 1 136535 1 298436 1 2258960 1 139253 1 1918352 1 13.8 .08 i 12305 1 30.2 1 16821 160827 247723 7281 \n212148 29.1 26008 161071 84532 10228 275155 26.9 8825 160827 1083688 234 23369 99.9 I I 0 I XI.5285 \nI 879254 I 107510 I 2175567 I 20.2 i -__----..-.--. ~~~ ~~ ii 266198 824681 61951 917038 14.8 J --437793 \n61064 807000 13.2 139800 161096 334824 448 12030 26.9 79260 162650 2577899 633145 3647296 5.8 Table \n1: Macro-Benchmarks Nesting of locks in general is very shallow: none of the benchmarks obtained any \nlocks nested more than four deep. These measurements tell us is that in most cases only a few bits need \nto be allocated for the lock nesting count. Our use of 8 bits for the lock count is highly conservative; \n2 or 3 bits is probably sufficient. Note that locking behavior can vary between different releases of \nJava. While the overall pattern remained the same, we saw significant individual differences between \nthe 1.0 and 1.1 releases of the JDK.  3.3 Micro-Benchmark Results While micro-benchmarks do not, give \nan accurate por-trayal of the types of performance improvements that can be expected in practice, they \nare very useful for gaining in-sight into how different implementations behave in various parts of the \ndesign space. Table 2 summarizes our micro-benchmarks. Each bench- mark runs a tight. loop for a specified \nnumber of iterations; inside the loop an integer variable is incremented. The benchmarks differ in what \noccurs between the outer loop and the inner variable update. For instance, the NoSync benchmark does \nnothing at aII between the loop and the update. It therefore measures the cost of bytecode interpre-tation \nof the loop. The Sync benchmark is a loop containing a synchronized0 block, which in turn contains an \ninteger increment. state-ment. The object that is the argument of the synchronized0 block is unlocked, \nso the Sync benchmark measures the cost of initial locking using the monitorenter and monitorexit bytecodes. \nNestedSync is Iike Sync, except that the object is locked outside of the loop, so that it measures the \ncost of nested locking (at level 1). MultiSync is Iike Sync, but it synchronizes n objects on every iteration. \nIt is designed to simulate the effects of vari- ous working sets of locks, where n is the size of the \nworking set. The results are shown in Figure 4. For initial locking (Sync), our thin lock implementation \nis 3.7 times faster than the Sun JDK 1.1.1 ported to AIX (JDKlll), and 1.8 times faster than the IBM \n1.1.2 version of the JDK with hot locks (lBM112). This is unsurprising, since the locking overhead for \nthin locks in the most, common case is only 17 instruc- tions, whereas the other implementations are \nfollowing sev-eral levels of indirection into the fat lock structure, and are performing a system caII \nto acquire the lock. In addition, the JDKlll implementation is looking up the fat lock in the monitor \ncache, which must itself be locked. For nested locking (NestedSync), the performance advan-tage of thin \nlocks is significantly reduced compared to the IBM112 hot lock implementation, since in that implementa-tion \nnested locking of a hot lock essentially involves following a pointer, comparing a thread identifier, \nand incrementing a memory location. The vanilla JDKlll is still much slower, because it must perform \na lookup in the monitor cache and then execute a system caII to obtain the thread identifier. The Multisyncbenchmark \ndemonstrates the Achilles heel of the hot lock approach: when the number of hot locks ex-ceeds 32, the \nIBM112 implementation slows down consider-ably. Surprisingly, the JDKlll implementation also slows down \nas the number of locked objects increases. This is due to the fact that the monitor cache thrashes its \nfree list when the working set of monitors exceeds the size of the monitor cache. The Call, CallSync, \nand NestedCallSync benchmarks are the analogues of the tist three micro-benchmarks, except that they \ncall synchronized methods instead of executing a synchronized0 block. still large, but slightly involved \nin performing Finally, the Threads of which runs a tight same object. Unlike Speedups achieved by thin \nlocks are lower because of the extra overhead method invocations. benchmark spawns n threads, each loop \nof synchronized0 blocks on the the other micro-benchmarks, which Lock Operations (%) 0% 10% 2042 30% \n40% w?4 w% 70% 80% 90% IOU% tmns Jamb javaparser Jobr toba Javalsx n Fourth I= Whlrd llSSCOfld HFiM \nq javacup HashJava crsms JaNst Javadoc lavw mocha wlngdls Figure 3: Depth of lock nesting by benchmark. \nMost lock operations are performed on objects that are not locked (they are the First lock on the object). \nOf the remaining lock operations, the vast majority are Second locks. 6000 -6000 E .E I-4000 Program \nDescription NoSync No locking -reference benchmark Sync Initial lock with a synchronized0 statement NestedSync \nNested lock with a synchronized0 statement MultiSync n Like Sync, but synchronizes n objects every iteration \nCdl Calls a non-synchronized method -reference benchmark CallSync Calls a synchronized method to obtain \nan initial lock NestedCallSync Calls a synchronized method to obtain a nested lock Threads n Initial \nlocking performed concurrently by n competing threads Table 2: Micro-Benchmarks Figure 4: Performance \nof locking mechanisms on various micro-benchmark tests. measured performance when the lock was in the \nuninfIated ?hin state, the Threads benchmark will cause the locks in question to be inflated when run \nwith our thin lock imple-mentation. The Threads benchmark shows the real advantage of the hot lock approach \nof the IBM112 implementation: when there is significant contention for a small number of objects, hot \nlocks are almost twice as fast as the JDKlll. However, as before, the performance of hot locks suffers \nsignificantly when the working set size increases. Thin locks achieve some performance improvement over \nthe monitor cache approach of the JDKlll, since instead of locking the monitor cache and performing a \ntable lookup, the fat lock pointer is simply obtained by shifting the moni- tor index to the right and \nindexing into the vector that maps indices to pointers. Note that the thin lock implementation is the \nonly one that scales linearly for both the MultiSync and the Threads benchmarks.   3.4 Macro-Benchmark \nResults While we have obtained speedups of more than a factor of five on some micro-benchmarks, real \napplications do not usually consist of tight loops performing synchronization op-erations. Figure 5 shows \nthe results of running the macro-benchmarks of Table 1. Thin locks sped up the benchmark programs by \na median of 1.22 and a maximum of 1.7 over the JDKlll implementation. The IBM112 implementation only \nachieved a median speedup of 1.04, due to the fact that a significant number of applications were actually \nslowed down. We believe this to be due to the frequent use of a large (more than 32) working set of synchronized \nobjects. In fact, some of the benchmarks are in effect tight loops performing synchronized operations. \nThe javalex bench-mark performs 3.4 million method calls, of which 2.4 mil- lion are synchronized. Almost \none million calls are to the synchronized elementAt method of the Vector class. The javalex benchmark \nwas sped up by 6.6 seconds. From Fig-ure 4 we can predict 2.7 seconds of speedup per 1 million synchronized \nmethod invocations, or 6.5 seconds of speedup for 2.4 million synchronized method calIs. Another interesting \nexample is jax, which was sped up by 66 seconds. Jax made almost 19 million calls to the get method of \nBitSet (two orders of magnitude more than for any other method). The get method is not synchronized; \nhowever, it executes a synchronized0 block after checking for some error conditions. From Figure 4 we \npredict 3.5 seconds of speedup for every 1 million synchronized block executions, or 66.5 seconds. 3.5 \nTradeoffs At the beginning of this section, we mentioned that low-level hardware characteristics significantly \ninfluenced the imple-mentation of the hand-tuned assembly language code that implements locking and unlocking. \nWe will now explore these issues in detail. Figure 6 shows the performance of a number of vari-ations \nof our thin lock implementation on selected micro-benchmarks, using the IBM112 as a reference for compari-son \npurposes. The MixedSync benchmark is a cross between Sync and NestedSync -it performs three nested locks \nof the same object on every iteration. The NOP case represents the speed of light -the very best that \nany implementation could achieve within the framework of the existing system. These measurements were \nobtained by removing all instructions related to synchroniza- tion from the assembly language version \nof the interpreter loop. The only overhead for synchronization in the NOP case is the extra bytecodes \nthat are executed (this overhead is significant -it amounts to more than a factor of two!). NOP results \ncould not be collected for the CallSync and Threads cases because the Java VM was unable to initialize \nitself properly. The Inline case represents our best implementation of thin locks, regardless of portability \nand maintenance con-cerns. The assembly language code for locking and un-locking is inlined into each \nrelevant bytecode implemen-tation, and specialized if possible. For the Sync bench-mark, the time increases \nby 20ms relative to the NOP case. In fact, three quarters of that time is due to the syn- thesized compare-and-swap \noperation (using the load-and-reserve and store-conditional instructions lwarx and stwcx). lnlined, specialized \nassembly code is not well suited to long-term code maintenance. We therefore experimented with using \na single lock and unlock routine (the FnCall case), and calling the routines from the bytecode implemen-tations. \nThis change resulted in a surprisingly small degra-dation in performance, presumably due to pre-fetching. \n 3.5.1 Architectural Variations The next problem we faced was more significant. AIX runs on PowerPC uniprocessors, \nPowerPC multiprocessors, and old IBM POWER and POWER2 uniprocessor machines. The POWER and POWER2 architectures \ndo not have user-level synchronization instructions, so the compare-and-swap must be performed by calling \na kernel routine. On the other hand, on a PowerPC multiprocessor locking and unlocking must be followed \nby the issuance of isync and sync instructions, respectively, which ensure that if another processor \nlocks the object that it will observe a consistent state. These instructions are not needed on a uniprocessor, \nbut as the measurements of the MP Sync case in Figure 6 show, adding those instructions resulted in a \n25ms slowdown of the Sync benchmark. These architectural variations raise a problem: to gain maximum \nperformance, we need to perform different lock and unlock operations depending on the type of hardware. \nUnfortunately, building those differences into the Java vir-tual machine would have resulted in an unacceptably \ncom-plex change to the JDK source code. We also considered using a dynamically linked library, but the \ncost of calling an AIX dynamically linked library function is significantly higher than calling a local \nfunction. However, because of the availability of surplus super-scalar parallelism, we were able to solve \nthis problem by dynamicaly testing the architecture type on every lo&#38; and unlock operation, as shown \nin the case labeled ThinLock -our final implementation, which we used for the other bench- mark results \nin this paper. Dynamic testing of the CPU type only slowed down the Sync benchmark by an additional 3ms, \nabout the same slowdown as from adding the function call. Finally, to demonstrate the advantages of our \ndiscipline in which only the owner of the lock is allowed to modify the lock field, we modified the code \nto perform the unlock oper-ation using a compare-and-swap instead of a load followed by a store. As seen \nin the UnlkC&#38;S case, the cost of the additional atomic operation is significant. Speedup 0 0.2 \n0.4 0.6 0.6 1 1.2 1.4 1.6 1.6 tram3 javac jacorb javaparser javalex ii r javacup r d NetRexx Espresso \nHashJava crema jaNet javadoc Javap mocha wingdls Figure 5: Relative performance of locking mechanisms \non various macro-benchmarks. Figure 6: Effect of various performance Related Work Krall and Probst [7] \nimplemented monitors for the CACAO Java JIT compiler. They argue that because object size is at a premium, \nmonitors should be kept externally in a hash table. However, our 24-bit thin locks have been used in \na Java implementation with only two words of overhead per object. The overhead can not be further reduced \nwithout converting the class pointer to a class index, which would have unacceptable performance implications \nin most envi-ronments. Therefore, our 24-bit dedicated monitors do not increase object size. On the other \nhand, the dedicated monitors greatly re-duce the number of instructions required to obtain a lock as \nwell eliminating the needs to synchronize the monitor cache. CACAO is implemented with user-level threads, \nso mutual exclusion on the monitor cache is obtained by setting a flag to disable pre-emption. However, \nour thin lock approach could be adapted to user-level threading and would still re-quire substantially \nfewer instructions and fewer memory ac-cesses than a monitor cache approach. Krall and Probst also rely \non the assumption that con-secutive accesses of the same bucket in the monitor cache hash table will \nusually be for the same monitor. Therefore, when an object is unlocked they leave the monitor installed \nin the cache with a count of zero. Their fast path for mon-itor entry assumes that the monitor is already \ninstalled in the cache. This approach is similar to that used by the IBM112 hot locks implementation; \nas our measurements showed, some applications have a sufficiently large working set of locks that they \nwill thrash such a cache, and will pay a substantial performance penalty. tradeoffs on selected micro-benchmarks. \n4.1 General Locking Research The MCS locks of [12] are similar to thin locks in that they only require \na single atomic operation to lock an object in the most common case. However, MCS locks also require \nan atomic operation to release a lock, whereas we can release a lock with a much less expensive load-store \nsequence. MCS locks are designed for maximal efficiency on mul-tiprocessor systems with significant amounts \nof contention, whereas thin locks are designed for maximal efficiency on uniprocessor systems or on multiprovessor \nsystems with rel-atively small amounts of contention. There is a significant body of work on how to achieve \nmutual exclusion with only atomic read and write opera-tions [2, 9, 10, 13, 141. These solutions were \nrendered ob-solete by the introduction of instructions that performed compound atomic operations such \nas exchange, test-and-set, and compare-and-swap [6]. Such operations were later generalized to the Fetch-and-Q \nprimitive for multiproces-sors [8], and were particularly popular on machines with butterfly-type interconnects \nbecause concurrent Fetch-and-@ operations to the same location could be combined in the network [4]. \nMany microprocessors have not had compound atomic operations until relatively recently because mutual \nexclusion was generally considered to be the province of operating systems and parallel processors. In \n1987, Lamport stated that if the concurrent processes are being time-shared on a single processor, then \nmutual exclusion is easily achieved by inhibiting hardware interrupts at crucial times [lo]. He is implicitly \nassuming that mutual exclusion is only being used in the operating system, or that the overhead of an \noperating system trap is acceptable on every lock and unlock operation from user-level code. Both Anderson \n[l] and Mellor-Crummey and Scott [la] 267 provide thorough discussions of synchronization algorithms \nfor multiprocessors and include comparative performance measurements. Conclusions We have presented \nthin locks, a method for implementing monitors in the Java programming language using a partial word \nof storage per object. Thin locks are implemented as a veneer on top of an existing heavy-weight locking \nsubsystem. We have implemented our technique in the Sun JDK, and have shown that it yields significant \nspeedups. For micro-benchmarks, thin locks are as much as five times faster than the original JDK implementation. \nFor real programs, thin locks achieve a median speedup of 1.22, and a maximum speedup of 1.7. Thin locks \ndo not increase the size of an object, and because fat locks are only created under contention, thin \nlocks also result in a significant savings in space when there are large numbers of synchronized objects. \nThe efficiency of our technique is due to careful engineer-ing to allow the most common cases to be executed \nwith a minimal number of machine instructions, and a design which obviates the need for atomic operations \nwhen unlocking or when acquiring nested locks. Acknowledgements Thanks to Tamiya Onodera for his advice \non the implemen- tation, Rob Strom for his help in validating the algorithm, Mark Wegman for suggesting \na key improvement, Kevin Stoodley for his help with optimizations for the Pentium, and Alex Dupuy and \nPeter Sweeney for their comments on earlier drafts of this paper. References [l] ANDERSON, T. E. The \nperformance of spin lock al-ternatives for shared memory multiprocessors. IEEE Transactions on Parallel \nand Distributed Systems 1, 1 (Jan. 1990), 6-16. [2] DIJKSTRA, E. W. Solution of a problem in concurrent \nprogramming and control. Commun. ACM 8, 9 (Sept. 1965), 569. [3] GOSLING, J., JOY, B., AND STEELE, G. \nThe Java Language Specification. Addison-Wesley, Reading, Mas-sachussetts, 1996. [4] GOTTLIEB, A., LUBACHEVSKY, \nB. D., AND RUDOLPH, L. Basic techniques for the efficient coordination of very large numbers of cooperating \nsequential proces-sors. ACM Trans. Program. Lang. Syst. 5, 2 (Apr. 1983), 164-189. [5] HOARE, C. A. R. \nMonitors: An operating system structuring concept. Commun. ACM 17, 10 (Oct. 1974), 549-557. [6] IBM CORPORATION. \nIBM 370 Principles of Operation. [7] KRALL, A., AND PROBST, M. Monitors and exceptions: How to implement \nJava efficiently. In ACM Work-shop on Java for High-Performance Network Comput-ing (1998). [8] KRUSKAL, \nC. P., RUDOLPH, L., AND SNIR, M. Ef-ficient synchronization on multiprocessors with shared memory. In \nProceedings of the Fifth Annual ACM Sym-posium on Principles of Distributed Computing (19X6), pp. 218-228. \n[9] LAMPORT, L. The mutual exclusion problem. J. ACM 33, 2 (Apr. 1986), 313-348. [lo] LAMPORT, L. A fast \nmutual exclusion algorithm. A CM Trans. Comput. Syst. 5, 1 (Feb. 1987), l-11. [II] LAMPSON, B. W., AND \nREDELL, D. D. Experience with processes and monitors in Mesa. Commun. ACM 23, 2 (1980), 105-117. [12] \nMELLOR-CRUMMEY, J. M., AND SCOTT, M. L. Algo-rithms for scalable synchronization on shared-memory multiprocessors. \nACM Trans. Comput. Syst. 9, 1 (Feb. 1991), l-20. [13] PETERSON, G. L. A new solution to Lamport s con-current \nprogramming problem using small shared vari-ables. ACM Trans. Program. Lang. Syst. 5, 1 (Jan. 1983), \n56-65. [14] RAYNAL, M. Algorithms for Mutual Exclusion. MIT Press Series in Scientific Computation. MIT \nPress, Cambridge, Massachussetts, 1986. Translated from the French by D. Beeson.  \n\t\t\t", "proc_id": "277650", "abstract": "Language-supported synchronization is a source of serious performance problems in many Java programs. Even single-threaded applications may spend up to half their time performing useless synchronization due to the thread-safe nature of the Java libraries. We solve this performance problem with a new algorithm that allows lock and unlock operations to be performed with only a few machine instructions in the most common cases. Our locks only require a partial word per object, and were implemented without increasing object size. We present measurements from our implementation in the JDK 1.1.2 for AIX, demonstrating speedups of up to a factor of 5 in micro-benchmarks and up to a factor of 1.7 in real programs.", "authors": [{"name": "David F. Bacon", "author_profile_id": "81100628167", "affiliation": "IBM T.J. Watson Research Center", "person_id": "P60470", "email_address": "", "orcid_id": ""}, {"name": "Ravi Konuru", "author_profile_id": "81436593342", "affiliation": "IBM T.J. Watson Research Center", "person_id": "PP14142235", "email_address": "", "orcid_id": ""}, {"name": "Chet Murthy", "author_profile_id": "81100580771", "affiliation": "IBM T.J. Watson Research Center", "person_id": "PP31088112", "email_address": "", "orcid_id": ""}, {"name": "Mauricio Serrano", "author_profile_id": "81339527190", "affiliation": "IBM T.J. Watson Research Center", "person_id": "PP43131988", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/277650.277734", "year": "1998", "article_id": "277734", "conference": "PLDI", "title": "Thin locks: featherweight synchronization for Java", "url": "http://dl.acm.org/citation.cfm?id=277734"}