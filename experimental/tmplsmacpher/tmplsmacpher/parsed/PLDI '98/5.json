{"article_publication_date": "05-01-1998", "fulltext": "\n Simplification of Array Access Patterns for Compiler Optimizations * Yunheung Paek$ Jay Hoeflingert \nDavid Paduat $ New Jersey Institute of Technology paekQcis.njit.edu t University of Illinois at Urbana-Champaign \n{hoef ling ,padua}Buiuc . edu Abstract Existing array region representation techniques are sen- sitive \nto the complexity of array subscripts. In general, these techniques are very accurate and efficient for \nsim- ple subscript expressions, but lose accuracy or require potentially expensive algorithms for complex \nsubscripts. We found that in scientific applications, many access patterns are simple even when the subscript \nexpressions are complex. In this work, we present a new, general array access representation and define \noperations for it. This allows us to aggregate and simplify the rep-resentation enough that precise region \noperations may be applied to enable compiler optimizations. Our ex-periments show that these techniques \nhold promise for speeding up applications. 1 Introduction The array is one of the most important data \nstructures in imperative programs. Particularly in numerical ap-plications, almost all computation is \nperformed on ar-rays. Therefore, the identification of the array elements accessed within a program section \n(e.g., a loop or a sub- routine) by a particular reference (we call this array access analysis) is crucial \nto programming and compiler optimizations. Figure 1 shows a reference to an m-dimensional ar-ray X with \nsubscript function (81 (i), - - -, am(i)) defined on the set of indices i = (ir , ia,. . . , id) within \nthe pro-gram section P . In array access analysis, the set of all elements of X accessed during the execution \nof sec- tion P (we call this the acceaa region of X in P) must *This work is supported in part by Army \ncontract DABT63-95-C-0097; Army contract N66001-97-C-8532; NSF contract MIP-9619351; and a Partnership \nAward from IBM. This work is not necessarily representative of the positions of the Army or the Government. \nIndicee are all basic induction variables of loops surrounding the reference SIGPIAN 96 Montmal. Canada \n1996 ACM 0-89791~S67-W36/0006  Figure 1: Access to array X with indices ik, 1 5 k < d, in the program \nScCtiOn P such that 15 5 ik 5 Uk. be determined. For this purpose, when doing array ac-cess analysis \nfor the references to an array, a compiler would first calculate a Reference Descriptor (RD) for each, \nwhich extends the array reference with range con-straints. For instance, given that il: ranges from lk \nto uk in p (for k = Figure 1 is X(81(i), lk < ik Next, the compiler section and store 1,2, * - -, d), \nthe RD for the access in .92(i), e-s, am(i)) subject to 5 Uk, for k = 1,2, * * , d. would summarize all \nthe RDs in the their union in some standard repre- sentation [9, 15, 191. Simple accesses can be summa-rized \nwith simple representations without losing preci-sion in access analysis. For example, in Figure 2, the \nre- gion accessed by reference b(ir , iz) can be represented by b(O:n5:l,O:n5:1) using the traditional \ntriplet notation. In general, more powerful representations are required to accurately represent other \naccesses with more complex subscripts, such as the references to array a in Figure 2. program PROG real \na[0 : n,], b[O : nb, 0 : q,] . . . for iI = 0 to nb with step 1 do b(il,iI) = .a. for is = 0 to TZ.I) \nwith step 1 do b(il, iz) = a(8 * iI + ia) + a(&#38; + 8 * ia + I) end for ia = 0 to il with step 1 do \na(ir * (il + 1)/2+is) = .-. end end  Figure 2: Code example where the section P is the ir-loop Conventional \nwisdom holds that complex array sub-scripts result in complex access patterns. However, we believe that \ncomplex subscripts often form simple ac- cess patterns. This belief is based on informal obser-vations \nof many programs. For instance, although the subscript function for a in the is-loop seems complex, we \ncan see in Figure 3 that the actual access pattern is simple. In many cases, the simplicity of the real \naccess is hidden inside the subscript expressions, making it dif- ficult to discover. Sometimes originally \nsimple subscript expressions are converted to complex ones during com-piler transformations, such as \ninduction variable sub-stitution, value propagation [3], and subroutine inlin-ing [18,13], although the \noriginal access patterns remain intact. Previous techniques sometimes fail to recognize these simple \npatterns and, as a result, lose accuracy in their access analysis. When an m-dimensional array is allocated \nin mem-ory, it is linearized and laid out in either row-major or column-major order, depending on the \nlanguage being used. Thus, we view accessing an array as traversing a linear memory space. Figure 3 shows \nhow the two ar-rays a and b are mapped to memory and accessed within the &#38;-loop of Figure 2. 0 for \n+ 8 * + 1 0000000000000000000000 for a(il * (il+ 1)/2 + i5) index ir independently iterates through \nits entire range of values with the value of iz fixed, the range of the ac-cess within memory has length \n8%. Similarly, when iz iterates alone, the access range has length tit,. The memory traversal due to \nthe independent iter-ation of each index is characterized by its strides and spans, as can be seen from \nFigure 3. We pair the stride and span produced by a single index for an array ref-erence to describe \nthe access pattern created by that index. Given a set of indices (ix, iz, . . . , id), the collec-tion \nof the stride/span pairs for all indices represents the entire access pattern for array A, denoted by \nwhere 6;, and oik are the stride and span due to index in,, and -r is the base offset, the offset of \nthe first ac-cess from the beginning of the array. We call this form a Linear Memory Access Descriptor \n(LMAD) and each stride/span pair a dimension of the descriptor. Also, we call index ir, which is associated \nwith the kth di-mension (d;k,~ih), the kth dimension index of the descriptor. Using the LMAD form, we \ncan summarize the four accesses in Figure 3 with a l-dimensional descriptor jtJnb-k2 b( ir,ir), and 2dimensional \ndescriptors nb(nb+a)+O for A SlshbInb+O, &#38;, ~,s,,b+1 and kib:>i+l, . +0 for a(8*ir+is), a(il+8*&#38;+1) \nand a(il*(dz1+1;;2+is), re- Definition 1 On the assumption that two LMADs A and A represent the access \nregions R and R , respec-tively, 1. A u A represents the aggregated LMAD of the two access regions, that \nis, R U R. . Figure 3: Access patterns in the il-loop in Figure 2 for nb = 1= 3: gray boxes represent \nthe array elements accessed, and arrows with black heads and white heads keep track of the access driven \nby indices ir and iz (or is), respectively. 1.1 Access Descriptor Exploiting Regularity of Mem-ory Accesses \nFor the most part, the simplicity of accesses is exposed via a regularity of access structure within \nlimited sec-tions of a program. By regularity we mean that the same access patterns are repeated in the \ntraversal of memory. The patterns are characterized by two factors: the stride and the span. The stride \nrecords the distance traveled in the mem-ory space when an index is incremented. For example, for array \nb of Figure 3, it can be seen that the incre-menting of index il causes movement through memory in strides \nof size nb $ 2, which actually corresponds to accessing the main diagonal of b. In the two references \nto a within the iz-loop, index ir causes a stride of 8 in the first and 1 in the second, while index \niz causes a stride of 1 in the first and 8 in the second. The span records the total distance traveled \nin the memory space due to a single index with the other in-dices fixed (that is, the difference between \nthe last offset and the first offset of the array elements accessed). Con-sider the reference a(8 * il \n+ is). Notice that when the 2. If R is a subregion of R (that is, R. Ca), then we write A cA. 3. Let \nA = &#38; f.2,>,, ;::. ~~d + 7. Suppose A is built  by eliminating the kth dimension (J~,uL) from A, \nis, A 61,-*,b-1 ,~k+l-*Jd that cl ,..., ak-l,mk+l ,..., rrd + r. We call A the k-subLMAD of A. Notice \nthat when A (with access region 77. ) is the k-subLMAD of A (with access region R), then I?. CR. 1.2 \nAnalysis of Subscripting Patterns The development of the access region notation was orig- inally motivated \nby the project [24, 251 to retarget the Polaris compiler [4] at distributed memory multiproces-sors. \nIn that project, the triplet notation used by Po-laris for array access analysis prevented us from generat-ing \nefficient code for our target multiprocessors because subscript expressions that could not be represented \nac-curately in triplet notation limited our compiler opti-mizations. The eventual success in that project \nwas due in part to our use of the LMAD notation. Section 6 presents a few results on the improvements \ndue to use of the LMAD. To develop a new access region notation, we needed to better understand the actual \naccess patterns in real applications. For this, we chose fourteen Fortran pro-grams, including codes \nfrom the SPEC85fp and Perfect Figure 4: Percentage of non-triplet-representable access summaries versus \ntotal number of access summaries aidm LlC2.3 b&#38;la avrosm flc.52 mag ocean (Perrece.) ! (spec95+p]~(Ncsn)l \nFigure 5: Percentage of access summaries which are not provably-monotonic versus total number of access \nsummaries benchmarks, and one from a set of production codes obtained from the National Center for Supercomputing \nApplications (NCSA) to study. After applying inter-procedural value propagation, induction variable sub-stitution, \nand forward substitution within these codes, we summarized each array reference to each of its en-closing \ndo loops, counted how many of these summaries would not be representable in triplet notation, and plot- \nted their percentage with respect to the total number of summaries in Figure 4. For this analysis, we \ndivided the array accesses that could not be represented by triplet notation into the following five \ncategories: subscripted-subscripts : accesses due to references with subscripted-subscript expressions; \nnon-affine : accesses due to references with non-affine subscript expressions; triangular afflne : accesses \ndue to references within a triangular loop; coupled-subscripts : accesses due to references with coupled-subscript \nexpressions [20]; multiple index afflne : accesses due to references con-taming multiple indices in a \nsubscript position. In this classification, each category excludes those above it. For instance, a reference \nwith a subscripted-subscript inside a triangular loop would be counted as subscripted- subscript, and \nnot triangular affine. In order for the difference between the last offset and the first offset (the \nspan) to represent the true dis-tance moved for a dimension, the subscript function must cause movement \nto be consistently in the same di- rection. Such a function is called monotonic [3], which will be formally \ndefined in Section 4. This implies that the LMAD can be accurate only when the subscripting functions \nare monotonic. Thus, to see how often the LMAD can be accurate in reality, we determined the percentage \nof array accesses that were provably mono-tonic at compile time. By their nature, all categories of references \nexcept subscripted-subscripts and non-&#38;Tine are monotonic. But we checked all non-affine references \nin our set of test codes and, unexpectedly, all of those accesses were prov- ably monotonic. Only the \nsubscripted-subscripts were not provably monotonic at compile time. This data is presented in Figure \n5. From these results, we learned that most subscript functions encountered in the programs we tested \nare monotonic. This indicates the general tendency that for the iteration of a single index in a program \nsection, an array is accessed in one direction either from a low to a high address in memory or vice \nversa. We also conclude that use of the LMAD makes possible an improvement in the accuracy of the representation \nover that obtained with triplet notation. The purpose of this paper is to show how the LMAD is used to \nanalyze and simplify array access patterns in a program for more efficient and accurate compiler optimizations. \n 1.3 Organization of this Paper Section 2 discusses previous work done on array access analysis and representation. \nSection 3 discusses sev-eral properties of LMADs and classifies access patterns that allow simplification \nof LMADs. Section 4 shows how to build a LMAD from a RD. Section 5 describes basic principles and methods \nto handle the access pat- terns classified in Section 3. Section 6 shows examples encountered in our \nexperiments where LMADs were ad-vantageous to our optimizing compiler, along with some performance results. \nWe briefly discuss the impact of this work on our current compiler project in Section 7, and present \nour conclusions in Section 8. 2 Array Access Analysis Techniques Work on representing array access regions \nhas followed three major approaches: triplet-notation-based, reference-list-based, and linear constraint-based. \n2.1 Triplet-notation-based techniques Triplet notation is an array region representation for a set of \ninteger values that start at a lower bound and proceed to an upper bound via a stride for each array \ndimension declared by the programmer. For instance, the notation for a J-dimensional array A is given \nby A(lbl:ubl:rl, lba:uba:aa, lb3:Ub3:83) where lbr: is the kth lower bound, uba is the kth up-per bound, \nand 86 is the kth stride. Triplet notation is simple, yet practical. Typically, the region opera-tions \n(e.g., union, subtraction, and intersection) defined on the notation can be implemented with fast linear \nal-gorithms. The study of Shen et al [28] indicated that most real-world access patterns in scientific \nprograms are representable by triplet notation. This is, in fact, the reason why many researchers, including \nseveral in our own research group at Illinois, have used the nota-tion to implement their compiler techniques \n(including array privatization, dependence analysis, and message generation [3, 7, 16, 311). However, \nas discussed in Section 1.2, the limited ex-pressive power of the triplet notation often hinders anal-ysis \nin some important cases. To alleviate this problem, researchers at Rice University [6] have devised several \nvariants of regular section descriptors @SDS), with op-erations deS.ned on a lattice. RSDs are able to \nexpress single array elements, complete rows and columns, and diagonals. Restricted RSDs [15] were devised \nto han-dle coupled subscripts, and then Bounded RSDs were devised to further improve the accuracy with \nsymbolic bound information. Researchers at the University of Minnesota have used Guarded Array Regions \n[14], which are equivalent to Bounded RSDs with an additional predicate (guard). More information can \nbe added to the guard to sharpen the accuracy in a given situation. 2.2 Reference-list based techniques \nLi and Yew proposed a reference-list based representa- tion, called an atom image [19], which captures \nthe co-efficients of the loop indices and the loop bounds of each surrounding loop. Burke and Cytron \nrepresented array references in one-dimensional form by linearization [5]. These reference-list based \ntechniques lose no precision for any array access because they rely on making a list of each individual \narray reference in a program section. They are meant to capture program information, but not summarize \nit. 2.3 Linear-constraint based techniques In linear constraint-based techniques, the set of linear \nconstraints is constructed from the subscripting func-tion, loop bounds, and other information found \nin the program, similar to our calculation of the RD. The fol-lowing is an example: 11 5 Xl 5 Ul la 5 \nXa 5 ua 13 5 Xc1 +Xa 5 U3 14 < 21 -Xa 5 U4 With these techniques, array accesses can be expressed as \nconvex regions in a geometrical space. The linear constraint-based techniques that were first proposed \nby Triolet, et al [30] have been widely used as an alter-native way to summarize array accesses. In particular, \nthese techniques have been used for dependence anal-ysis. When a potential dependence between two array \nreferences is being tested, the linear inequalities associ-ated with the two references are aggregated \nto form a linear system and the feasibility of the system is tested using Fourier-Motzkin elimination \n[ll] techniques. The Omega Test [26] is an example of a dependence test built in this way. The PIPS project \nat Kcole des Mines de Paris [9] has added an indicator of the accuracy of the representation, referred \nto as MUST/MAY, to the rep-resentation itself. The notion of MUST/MAY approxi-mations helps a compiler \nto determine when a result is accurate or inaccurate. Linear constraint-based techniques are generally \ncon-sidered more precise than triplet notation in handling access patterns with non-rectangular expressions \n(see Figure 4). However, they also have several critical draw-backs. First, the Fourier-Motzkin linear \nsystem solver requires worst-case exponential time algorithms [2]. Bal- asundaram and Kennedy [l] proposed \na simplified form of linear constraint representation, called simple aec-tions, that eliminates the need \nfor such expensive al-gorithms, but at the cost of accuracy. Although simple sections can represent many \nof the commonly occur-ring forms, such as a whole array, a single row/column, a diagonal, or a triangular \nsection, they are limited in that they cannot express the whole range of con-straints found in programs. \nSecond, Fourier-Motzkin is limited to affine expressions. To overcome this limi-tation, Pugh and Wonnacott \n[27] have developed tech-niques for replacing non-affine terms occurring in array subscripts with uninterpretedfunction \nsymbols, but this does not handle all situations involving non-affine terms. Third, Fourier-Motzkin requires \nthat the linear inequal-ities form a convex hull, forcing a loss of accuracy when regions must be altered \nto maintain the convex form. Work on the SUIF system at Stanford uses a repre- sentation [21] very similar \nto that of PIPS. SUIF uses a set of region operations for systems of linear inequal-ities and special \nalgorithms for maintaining the convex shape of the regions during the analysis. To recap, the linear \nconstraint-based representation improves accuracy over triplet notation, but still loses accuracy and \nrequires potentially expensive algorithms for many complex subscripting expressions (the frequen- cies \nof which are shown in Figure 4). The performance of a compiler based on linear constraints would be im- \nproved if those expressions could be simplified. 3 The Similarity of Array Access Patterns In addition \nto the property of regularity discussed in Section 1, different accesses in the same program sec-tion \noften have a similarity of pattern because multiple references to an array within the section are generally \naccessed using the same indices and similar subscript expressions. Figure 3 shows that the accesses repre-sented \nby descriptors &#38; , + 0 and A: :, + 1 describe exactly the same memory access pattern but with differ-ent \nbase offsets. This example helps demonstrate that the order of the stride/span pairs on the LMAD does \nnot affect the access pattern represented by it. We call such similar access patterns isomorphic. Definition \n2 Let A and A be two LMADs. If A has the same stride/span pairs as A, regardless of their or-der, then \nA and A are isomorphic, denoted by A/ /A , meaning the access regions described have the same shape and \nstructure. Definition 3 Two LMADa A and A are said to be equivalent, denoted by A E A , if they represent \nthe 3ame access region. If two LMADs are isomorphic and have the same base offset, they are equivalent. \nFor instance, since the two descriptors di$, + 0 and di*i, + 1 are isomorphic, they would be equ&#38;lent \nif we could show that 1= 0. To illustrate another type of $r$larity, consider the . . 1 whose accesses \nare denoted by dashed lines in the figure, represents the access region produced by a(i+j+l). Notice \nthat another descriptor A:;$,, + 1, whose accesses are denoted by the lower solid lines, represents the \nsame region as the original descriptor. Even a l-dimensional descriptor can represent the same access, \nsuch as dir + 1, whose accesses are denoted by the upper solid lines. loop m Frgure 6. A descnptor ds;, \n+ , real a(0 : 71) . . . for j = 0 to 2 with step 1 do for i = 0 to 9 with step 3 do a(i+j+l)=---end \nend Figure 6: Equivalent regions for the accesses to array a, which cover the region from a(l) to 412). \nAll three descriptors in Figure 6 are equivalent. From these examples, we learn that an access region \ncan be represented by numerous LMADs consisting of different stride/span pairs. In principle, we can \nshow that the same access region can be represented with an infmite number of equivalent LMADs by Theorem \n1, which will be used in Section 5. Theorem 1 A LMAD A~~:::$~ +r can be expanded to form other equivalent \ndescriptors by adding a dimension (a*, 0) in any position, such as d~l~~.$:.~~:~~ + T, where 6 can be \nany integer. PROOF: A dimension describes the movement from a lower bound to an upper bound with a stride. \nThe span is defined as the difference between the upper bound and the lower bound. If the span is zero, \nthe upper bound and lower bound are the same, thus describing no movement at all. A dimension involving \nno movement can have any stride, and still neither adds nor subtracts elements to/from a given access \nregion. n As we have mentioned, the complexity of subscript expressions sometimes prevents the representation \ntech-niques described in Section 2 from accurately represent-ing an access. However, if the access patterns \nare repre- sented in a sufficiently general form, they often can be aggregated and simplified into forms \nwhich are repre-sentable in the notation of choice, by using techniques such as those discussed in this \npaper. For example, a direct translation to triplet notation of the access in the loop of Figure 6 would \nnot be possible because the ac-cess involves multiple index subscripts (see Figure 4). However, using \nour algorithms, we can show that the original pattern is equivalent to an access with dir + 1; therefore, \nwe can use the triplet notation a(l:12:1) to accurately represent the access. Our work is based on the \nobservation that typical scientific programs have several common forms of access patterns produced by \nsubscript expressions, regardless of their complexity. We call these forms coalesceable, interleaved, \nand contiguous. These are useful for our purposes because, whenever one of these forms appears, the original \naccess pattern can be transformed into a simpler one. Coalesceable Accesses Given an array access represented \nwith a LMAD A, we call the access coalesceable if it also can be represented with another LMAD A equivalent \nto A, but with fewer dimensions than A. One typical example of a coalesce- able access is one that moves \nwith a small stride due to one index and due to a different index, strides over the accesses of the first \nstride to the very next element in the sequence. For instance, the reference a(&#38; *(&#38;+1)/l+&#38;) \nin Figure 2 accesses ir + 1 consecutive elements with stride 1 for every iteration of the ir-loop, then \njumps over those elements with stride ir + 1 to the next ele-ment. In a case like this. we can show that \nthe number of dimensions can be reduced by one. We can show that + o) E (dr+s+ll+,b + 0), a using the \nalgorithm presented in Figure 10. Interleaved Accesses We call array accesses interleavedwhen their dimensions \nhave the same strides and they are offset from each other by a fixed distance which divides one of their \nstrides. An example is shown in Figure 7, where we can see that the access patterns for arrays x and \ny have this property. We can see that the union of the three access regions for x is equal to the whole \nregion from x(1) to x(u+4) with stride 2, which can be represented by a single descriptor Xi-i+4 + 1. \nThis implies (Xi-l + 1) u (XL + 1+ 2) u (XL + 1+ 4) E (XL+4 + 1). Contiguous Accesses We call LMADs contiguous \nwhen the access patterns they represent are similar, can be fit together to cover a portion of the array \nwithout a break, and can be ex-pressed in a single LMAD. Figure 8 illustrates four ac-cess patterns for \narray a. From this example, we can clearly see the similarity between those accesses; and we see that \nwhen viewed from outside the outermost real ~(0 : n), 1/(0 : m) . . . for i = 1 to u with step 6 do \ntemp = temp + S(i) *y(i) +r(i + 2) * ?/(i + 2) +o(i + 4) * ar(i + 4) end Figure 7: Code example of the \ndot product of two vectors x and y with stride 2, and the illustration of the access patterns for r in \nmemory, represented by three descriptors Xt-,+l, X:-,+1+2, and X:-,+1+4, respectively loop, they fit \ntogether to form an unbroken stream of access. This results in (&#38;6 + 1) u (&#38;&#38; + 2) lJ (A::;, \n+ 3) u (AL + 4) = (A:9 + 1) which indicates that the union of the four access regions can be represented \nwith a single LMAD. real a(0 : n) . . . for i = 0 to 3 with step 1 do a(5*i+1)= ... for k = 1 to 2 with \nstep 1 do a(k+5*i+l)=-.-end for j = 1 to 4 with step 1 do a(5*i+j+2)=a(S*i+4)+... end end Figure 8: \nSimilar access patterns with four descriptors AT,+&#38; d$r1+2, di:$+3, aad &#38;+4 Generating Linear \nMemory Access Descriptors Let Z denote the set of all integers and Z the set of all m-tuples of integers. \nWe assume here, for simplic-ity of explanation, that all subscripts of an array start at 0, and all loops \nare normalized with a stride of 1. For example, the array X in Figure 1 is declared as X(O:nl,O:nz,. \n- * ,O:n,). We define the orroy space of X, Z $, as the set of m-tuples: z; = {(rl,rz, ,r,)EZmlOIrkInk,lIkIm}. \nBecause vector r = (rr,rs, *.a ,rm) represents the ad-dress in the array space of X, to access the actual \ndata stored in memory it must be mapped to a single integer that is the offset from the beginning of \nthe array. We define this mapping, denoted by FX : ZT + Z, as m Fx(r) = Fx(rl,ra,*-.,r,) = Crk *XI, II=1 \n where, provided X is allocated in column-major order, x1 = 1 and &#38; = Xk-1 . (n&#38;r + 1) for k \n# 1. If the array is allocated in row-major order, X, = 1 and &#38; = Xk+l . (nktl i-1) for k # m. Applying \nthe definition of Fx to the subscript function in Figure 1, we have a linearized form: Fx(s(i)) = sr \n(i)Xr + az(i)Xs + . . + + a,(i)X, (1) where we assume that s(i) = (al(i), az(i), * * *, am(i)). Now, \nwe show how we could use Equation 1 to calcu- late the dimensions (stride/span pairs) and base offset \nof the accesses made by an array reference X(s(i)). Let %th denote the difference in Fx(s(i)) made by \nreplacing ik with ik $ h, where h is a positive integer. For gen- eral subscript functions, it can be \nrepresented by the difference operator A, defined by Fx(s(il,...,&#38;+h,... rid)) -FX(S(Z ~, ,ik, ,id)) \nh which is used to define a monotonic subscript function mentioned in Section 1. Definition 4 Let s(i) \nbe a subscript function defined Oni = (il,i2, ,id). For all the values of index ik in the runge between \nlk and Uk, we soy the function s(i) is nondecreasing for the index ik. if @,, 2 0; otherwise, we say \ns(i) is nonincreasing. The subscript function s(i) is monotonic for index ik if s(i) is either nonde- \nCreaSing Or nOninCreaSing for kk. Suppose s(i) is monotonic for all indices in i. Then, the stride 6i, \ncaused by the iteration of index ik E i can be obtained directly from &#38; as follows: where h is set \nto 1, as it is in finite calculus [12]. As stated in Section 1, the span oil. is the distance moved in \nthe memory region accessed during the iteration of ik from #?k to uk. If the function is monotonic, then, \nby finite calculus, this can be calculated by subtracting the function values of the two end-points of \nthe intervals. Thus, the span is where h is again 1. The sign of 8: denotes the direc-tion of the movement \nthrough memory due to i in an array access. In our formulas, however, we ignore the able. For example, \nthe access for a(&#38; * (ir + 1)/2 + is) sign by taking the absolute value, l$l, for the stride 6;, \nin Figure 3 is represented by the descriptor and changing the base offset because, as we stated in Section \n1, the access pattern (or access region) is char- acterized by the stride size of the movement, not the \ndirection. To show this with an example, consider Fig- ure 9. Here, we have two accesses to array a, \nwhere 6:, in the first is -1 (causing movement to the left with stride 1, starting from a(4)) and in \nthe second is 1 (causing movement to the right with stride 1, starting from o(2)). However, despite the \ndifferent access structures, we see that the access region (represented with gray boxes) for both accesses \nis actually identical. for ia = 0 to 6 do for il = 0 to 2 do a(+ + 4 * iz + 4) = . . . a(il+4*&#38;+2)=... \nend end Figure 9: Access to array a through two references and their access patterns in memory: solid \nlines denote the access for a(-il+4&#38;+4) and dashed lines the access for a(i1+4&#38;+2): arrows with \nwhite heads and black heads keep track of the access driven by indices il and ia, respectively. As can \nbe seen in Figure 9, the direction of access only adds to the complexity in access analysis. Thus, we \nremove the extra complexity by normalizing LMADs to a form in which alI the directions of access are \npositive. This normalization requires us to change the base offset of the access to the lower bound of \nthe access reglon. For example, in Figure 9, the original base of the access for a(-&#38; + 4 * iz + \n4) was a(4); but, during normalization, the base must be changed to a(2). For the normalized form, using \nEquations 1 and 2, we calculate the base offset of the access made by the reference X(s(i)) as follows: \nwhere ck = uk : 6: < 0 for 1 2 k 5 d. lk : 8: 2 0 Note that r is the minimum value of Fx(s(i)). The LMADs \nfor the accesses in Figure 9 produced by our formulas would both be A$, + 2. Thus, we can prove that \nthe two references access the same array region. 5 Linear Memory Access Descriptor Manipulation In this \nsection, we discuss the techniques that identify the regularity and similarity of the three categories \nof access patterns discussed earlier and that use the char- acteristics to simplify or to aggregate their \ndescriptors. 5.1 Coalesceable Accesses Given a single LMAD, the algorithm in Figure 10 deter-mines if \nthe access represented by the LMAD is coalesce- which has two dimensions: (1,ir) due to index is and \n(il + 1, v) due to il. These pairs are coalesce-able because 6is(= 1) divides &#38;,(= ir + l), and 6i, \n= ui, + 6;s. Thus, we eliminate (ir + 1, w) from the descriptor and update the original span uis(= ir) \nwith v + il. Since the new span contains is, we replace the index with its upper bound nb, resulting \nin the l-dimensional descriptor This eliminates one dimension of the LMAD, thereby simplifying the representation \nof the original access pat- tern. In a similar way, we can show that the access for the reference b(ir,is) \ndue to the two indices ir and is in Figure 2 can be represented by the l-dimensional de-scriptor A&#38;,+ \n)~- + 0. Algorithm Coalesce Input 1: dimension index set z = {il , iz . . *, id} with constraints II, \n< ik <..rk for k = 1,2, * * * ,d Input 2: LMAD A = A$l$~z ,..:~$ + r Note: Here f [j t x] means to substitute \nx for j in function f Algorithm: while an unselected index pair (i , ik) from z remains do Select two \ndimensions (6ij ,cij ) and (Ji, #vi, ) from A; if ij or ik appears in the other dimensions of A then \ncontinue; fl if 6ij divides 6;, and 6ik 5 Qij + 6ij then if ik appears in Qij then if 3i, lk 5 i < Uk, \nsuch that bi, < 4ij [ik t C] -Uij [;k t i + l] then continue; tl aij = oij [ik t Uk]; fl Uij t Uij + \n4ik; Eliminate (a;,, ci,, ) from A; z z -{ik}; t fi od end Figure 10: An algorithm that detects coalesceable \naccesses from a LMAD by comparing its stride/span pairs. Algorithm coalesce does O(da) dimension compar-isons \nfor a d-dimensional LMAD in the worst case.   5.2 Interleaved Accesses In Figure 6, we can find that \nthe access from a(l) to a(12) with stride 1 (represented by .A:, + 1) comprises two separate interleaved \naccesses with stride 2 (repre- sented by d:s + 1 and d&#38; + 2) or three accesses with stride 3 (represented \nby d:+l, di+2, and di+3). The-orem 2 shows how simplification can be applied to ag-gregate multiple regions \nwith an interleaved structure; it gives us the flexibility to convert a single LMAD to its n-interleaved \ndescriptors, or vice versa. To determine whether n 8iven d-dimensional descrip-Using LMADs, a compiler \ncould automatically identify tors are n-interleaved with respect to a given stride 6, the interleaved \npatterns from the loops, as discussed we would first sort the dimensions of each by stride, an above, \nand choose an arbitrary n to transform the orig- O(ndlogd) process, then use a linear-time bucket sort \ninal loop with n-interleaved accesses. For instance, we to order the descriptors by base offsets, checking \nthat can transform the 3-interleaved accesses in Figure 7 to in sorted order the offsets differ by the \nsame J/n. The the Cinterleaved accesses for deeper unrolling because complexity of this process is dominated \nby O(nd log d). we can easily show, for example, Theorem 2 = 61~6a~~~~Jd For a dimension Let A A,,,, \n=,..., cd $7. (6k,uk) in A, where 6k and ak-are invariant, and a chosen n, 1 2 n 5 1 + 2, there always \nezists a set of ._ n descriptor8 {Aj 11 5 j 5 n} = { d~~~:~:::.'~~d +T + (j- l)ak} where 6; = nA and \na; = 1-1 a:, such that A= IJ Aj. We say the n d ercrip&#38;s are the n- l<j<n interleaved descriptors \nof A. PROOF : To prove this, we will show that both A C A and A c A . In order to focus on the k-th dimension, \nwithout losing any generality, we will fix the values of all indices except the k-th. The set of elements \nwithin A which result have a specific starting point, TF, de-termined by the values we choose for the \nother indices. First, to prove A C A, we will show that an arbitrary element of Aj, for any j, is always \nan element of A. By definition, the m th element of Aj has the offset: (m -l)&#38; $ TF + (j -l)Cfk for \n1 2 m 2 2 + 1 Which We Cm CXpreSS 88 ((m -1) + j -1)6k + TF. This form indicates that the m th element \nof Aj cor-responds to the (m -1)n + jth element of A. This is a valid element of A because, from the \nassumption, we can show l<(m -l)n+j<z+l since 1 5 m 5 $ + 1 = 1-J + 1. Next, to showk A C A , we st:t \nfrom the mth ele-ment of A, which has the offset: (m -1)6k + Tp for 1 5 m 5 2 + 1. Given two integers \nm and y such that x = 1-1 and y=(m-l)modn,wehavem-l=nz+y.Then,the offset of the mth element of A is transformed \nas follows: (n= + Y)dk + 7F = Zdk + 7F + Y6k = X6: + TF + Ysk from which we see that the element corresponds \nto the x + lth element of descriptor A,+I. We can verify that Av+l is one of the n descriptors Aj and \nthat it contains the z + lth element since, from the definition of z, y and m, we can show the bounds \non x and y are l~z+l<~+l,andl~y+l<n. Together, these results show that any element of A is also an element \nof A , and vice versa, which implies A= U Aj. n l-Cj<n Interleaved accesses are common in unrolled loops, \nsuch as those represented with the 3-interleaved descrip-tors of Xisl+, + 1 shown in Figure 7. Different \nproces-sors cause different unrolling depths to be advantageous. (XL+4 + 1) s (XL + 1) u (XL + 1+ 2) \nu (Xi-, + 1 + 4) U (X:-I + 1 + 8), (assuming, for simplicity, that u -1 is a multiple of 8) and, from \nthe 4interleaved LMADs for both arrays and y, it is straightforward to generate the new code for i = \n1 to u with step 8 do temp = temp + z(i) *y(i) + o(i + 2) * y(i + 2)f x(i t 4) * y(i + 4) + x(i t 6) \n* Y(; + 3) end The property of interleaving also can be used to ag- gregate multiple contiguous accesses \nto a single one, such as those represented in Figure 8 by the two LMADs At&#38;+2 and dfs+4. The descriptor \nA:&#38;$ $2 consists of its 2-interleaved descriptors dy$,,+2 and dt;20+3. Us- 2 5 ing Theorem 1, dig+4 \ncan be converted to .A,:,,+4 to match the dimensions of the other descriptors. Now, we apply Theorem \n2 to show that this expanded de-scriptor and the other two descriptors together form 3-interleaved descriptors \nof dff,+2. In our distributed memor; multiprocessor code gen- eration project, mentioned in Section 1.2, \nthe notion of interleaving has been useful to perform three region op-erations (aggregation, intersection \nand subtraction) on the LMAD notations, as will be briefly discussed in Sec- tion 6, and to determine \nsubregions [24]. For instance, in Figure 8, df5 + 4, which represents a subregion of the region represented \nby d:;i55+ 3, is in fact one of the Cinterleaved descriptors of d3i15 + 3. 5.3 Contiguous Accesses Con$guous \naccesses can be formally defined as follows: Definition 5 Given r F r , let A = d~ J.$ ;ll. $d+~ 6; ,..., \n6; ,..., 6, and A = d The LMADs are con- u;,...,u;,...,LT; +7 . tiguous, denoted by A w A , if there \nexist dimensions (up, 6,) and (ub, a&#38;), satisfying the conditions 1. Ap II A:, 2. a, = a:, .?. 6, \ndivide8 T --r ,  4. T -7 < u: + S:, and 5. the pth dimension index of A and the qth dimen-sion index \nof A do not appear in the expressions for the stride/span pairs of A, or Ai  where Ap and Ai are the \np-subLMAD of A and the q- subLMAD of A , respectively. Then, (cPp, 6,) and (a:, 6;) are the bridge dimensions \nfor the relation A w A . In Figure 8, Ai&#38; + 2 and &#38;;$, + 3 are contiguous by Definition 5. Also, \nin Figure 3, we can show that the two descriptors d&#38; + 0 and dkf,2r + I are contiguous as long as \n1 5 nb + 1 holds. Theorem 3 Let A and A be the contiguous descrip-tors defined in Definition 5, and let \nt = 7 -7 . Then, PROOF: Once the conditions of Definition 5 are met and we have identified dimensions \np from A and q from A , we know that all strides from A and A are the same, and that A is shifted to \nthe right from A . Con-dition 3 means that A is shifted an integral number of qth dimension strides from \nthe start of A . Condition 4 means that the shift leaves no gap between the end of dimension q and the \nstart of A. If T-T < u:+6:, then the start of dimension p would overlap the end of dimen- sion q, while \n7 - T = U: + cI~ would mean that A starts immediately at the end of dimension q. If a, + t > o:, this \nserves to extend the span of dimension q. Other-wise, dimensionp would be completely inside the span \nof dimension q and would add nothing to dimension q. By Condition 5, no other dimension uses the pth \nand qth di- mension indices associated with the bridge dimensions, so forming a combined dimension has \nno implications for the other dimensions. n Notice that it follows immediately from Theorem 3 that if \ntwo LMADs are found to be contiguous as above, and or + t 5 crh, then A C A . Theorem 3 can be used to \naggregate contiguous ac-cesses to represent their access patterns with a single LMAD. For instance, A&#38;+2 \nand d$$,+3 in Figure 8 are aggregated to a single descriptor by showing d $, + 2 u d$, + 3 E A:;;, + \n2. Similarly, the two descriptors /I:<,, +0 and .A>f,s4+1 in Figure 3 can be aggregated to produce d>f+l,PQ+O \nunder the constraint 1 < n&#38;l. To determine whether two descriptors are contigu-ous, we would first \nsort the dimensions of each by stride, an O(d log d) process, then in one pass check that at least d \n-1 dimensions match and keep track of which indices are used in which other dimensions. If only one dimension \nof each differs by the span, we check Con-ditions 3, 4, and 5. If all dimensions match, then one more \npass needs to be made, checking Conditions 3, 4, and 5 for each pair. This process is dominated by the \nO(d log d) process. In Section 5.2, we showed how to use the interleaved structure to aggregate &#38;s \n+ 4 and A!$, + 2. In fact, we can do the same with Theorem 3. For this, we first must obtain a 2dimensional \ndescriptor di:1)6 + 4, ex-panded from dfs + 4 using Theorem 1 because the orig- inal l-dimensional LMAD \ncannot be directly applied to Definition 5. Since 6 can be any number by definition, we set 6* = 1 to \nmatch the stride of its counterpart pair (1,l). Thereafter, it is again straightforward to show A 61 \n&#38;l + 2 w A;;:, + 4 (= A:;:, + 2). Theorem 4 Let A, A and A be LMADs. Assume A w A and A w A , where \nthe bridge dimension in A for both relations is the same. Then (A U A ) w A and A w (A U A ). PROOF: \nLet (6,~) in A and (#,I#) in A be the bridge dimensions for A w A , and (#,o ) in A and (6 ,&#38; ) in \nA be those for A w A . Note that, by assumption, the bridge dimensions in A for both relations are the \nsame (b ,u ). We will first prove A w A , where we refer to AUA as A. If A C A , then, trivially, A = \nA w A . If A @ A , it is obviously true that Conditions 1, 2, and 5 in Definition 5 must hold between \n.k and A because of the fact that, by Theorem 3, x has the same dimen-sions as A except the bridge dimension \nfor AwA , and 6=6 =6 . For Conditions 3 and 4, let 7, 7 , and 7 be the base offsets of A, A , and A , \nrespectively, and let tl = 1-r - 7 ) and t2 = 1~ - ~ 1. We must consider six cases, corresponding to \nthe ordering possibilities of 71 7 1 and 7 . In the proof, a 4 denotes conditions that follow trivially \nfrom assumptions or simple alge-braic manipulation. For instance, in Case 1 below, the ,/next to Condition \n3 denotes that it is trivial to prove b divides tl + tz since tl and t2 are multiples of 6. 1. T<T <T \nA w A implies : t1 <r+6 A w A implies : tz < u + d A u A = A$+tI + T, Thus : 6 divides tl + tz (Cond \n3) d tl+tl<u +tl+6 (Cond4) ,/ 2. T < T 5 T A W A implies : t1 Su+b A w A implies : it2 < 0 + 6 A U A \n= A;,+tl + 7, Thus : 6 divides tl -t2 (Cond 3) J tl -t2 5 u + tl + 6 (Cond 4) 4 3. T < T < r A W Af \nimplies : t1 5 67 + 6 A w A implies : t2 5 u + 6 A U A = A:+,, + T , Thus : 6 divides t2 (Cond 3) J \ntz<u+tl+J (Cond4) 4 4. 7 5 7 < 7 A w A implies : t1 5 CT + 6 A w A implies : t2 < u + 6 A p A implies \n: u + t1 > u  A U A = A:+,, + T , Thus : 6 divides t2 (Cond 3) ,/ tz<uftl+6 (Cond4) 4 5 . T <T<T AwA \nimplies: hIu+6 A w A implies : ta 5 0 + 6 A u A = A$+,, + r, Thus: 6 divides t2 -tl (Cond 3) d t2 -tl \n2 u + 6 (Cond 4) 4 6.T <T <T AwA implies: t1 5 u + 6 A w A implies : t2 5 at, + d A u A = A;,,, + r , \nThus : 6 divides t2 (Cond 3) 4 t2 5 u + 6 (Cond 4) J From the above proof, we showed A U A and A are \ncontiguous. Proving A w A U A uses identical logic, so we will omit it. n Notice that, given three descriptors \nA, A , and A , A w A and A w A do not necessarily imply A w A . This is because the relation w is not \ntransitive. To il- lustrate this, consider Figure 8. Ai, + 1 and A:;$, + 3 are not contiguous, although \ndfs + 1 w d$ + 2 and d;&#38; + 2 w A; :, + 3. Therefore, the question is given a se t of pairs of contiguous \nLMADs, how does one de- termine the order of aggregation to obtain the optimal result? To this question, \nTheorem 4 responds that the result of a computation does not depend on the or-der in which aggregations \nare applied as long as they meet the constraint on the bridge dimensions given in Theorem 4. One form \nof this theorem is illustrated in Figure 11, where three LMADs A( ), A( ), and A@) and their initial \ncontiguous relations, A( 1 w AI21 and A(') w At3), are given in a contiguous relation graph. Whether \nA( 1 and At21 are aggregated first or A( ) and At31 are aggregated first, we can always obtain the same \nresult A( 23l if the bridge dimensions of Af21 for both relations are the same. This is the case with \nthe ac-cess descriptors in Figure 8 which can be aggregated lb in any order to produce d,;,,+l, which \ncan be further simplified to dis+l using coalescing. Figure 11: Example of contiguous relation graphs: \nin the graph, a node A( ) denotes a LMAD, and an edge the rc-letion w. A( il represents the aggregated \nregion of those represented by A( ) and A(j); that is, A( j) s A( ) U A(j). 5.4 Other operations In \naddition to the operations described in this paper, we have devised LMAD algorithms for several other \nregion operations [24]: union, intersection, and subtraction. In the cases where these operations cannot \nbe carried out precisely, the resulting descriptor can be marked to be an over- or under-estimation of \nthe actual locations. 5.5 Experimental results showing the degree of sim- plification In an attempt to \ndetermine how much simplification can be achieved by the techniques of coalescing and contigu- ous aggregation, \nwe measured the simplification pro-duced by coalescing and contiguous aggregation for our set of test \ncodes. We computed all LMADs interproce-durally without applying any simplification techniques. We counted \nthe total number of LMADs produced at all loop headers and CALL statements, and totaled the number of \ndimensions used for all LMADs. Then, we again computed all LMADs interprocedu-rally, but applied both \ncoalescing and contiguous ag-gregation iteratively during the process until no more simplification was \npossible, and recorded the number of LMADS and dimensions used in them. We chose to show the reduction \nin the total number of dimensions as a measure to indicate the amount of simplification performed because \nit captures both the reduction in the number of LMADs (through aggrega-tion) and the reduction of the \nnumber of dimensions (through coalescing). The results, presented in Fig-ure 12, show that a significant \namount of simplification can be achieved in most cases. 6 Applications of LMADs for Compiler Techniques \nTypically, array privatization and dependence analy-sis [3, 9, 14, 291 are based on array region operations. \nThus, their accuracy is heavily dependent on array ac-cess analysis. According to our experiments with \nPo-laris, current techniques are limited in some cases by the complexity of subscripts. To illustrate \nthis, consider the loop in Figure 13. To parallelize the I-loop in the ex-ample, it is necessary to determine \nthat array Y can be privatized. That is, it must be shown that within each iteration of the outermost \nloop, every element of array Y is always written before read. The difficulty here is that the subscript \nexpressions for array Y are non-affie, and the accesses are made by multiple indices, J and K. Due to \nthese complications, existing array privatization techniques [21, 311 cannot identify the exact access \nre- gion for Y; as a consequence, they could not privatize Y and thus would fail to parallelize the loop. \nWe found that the LMAD is often effective to overcome these lim- itations. In the loop of Figure 13, \nfor example, the two write accesses for array Y are represented with descriptors from which it is clear \nthat they are contiguous (in fact, also interleaved). Therefore, they can be aggregatedinto an exact \nregion represented with as illustrated in Figure 13, which in turn can be coa-lesced into the equivalent \nform y$+2-3 + 1. Despite the complex subscript expressions in the ex-ample, using the LMAD representation \nhelps reveal that the write region for Y is as simple as one equivalently represented by Y (1: 2 + -2: \n1) in triplet notation. In our experiments, this simplification allowed existing ar-ray privatization \ntechniques to calculate the exact write region of array Y and, similarly, the read region due to two \nreferences, Y (K+2**J) and Y(K+2**J+2**(N+l)-I), in the loop. As a result, we can prove that the read \nre-gion is covered by the write region (in fact, they are the same), and thereby eliminate dependence \nby declaring Y as private to the loop. The LMAD representation is useful not only for ar-ray privatization, \nbut also for other techniques depend-ing on array access analysis, including dependence anal-ysis and \nthe generation of communication primitives such as Send/Receive or Put/Get. For instance, in our code \ntransformation [25] for multiprocessors with phys-ically distributed memory, we used the LMAD to gen-erate \nPut/Get primitives of the general form put/get~c~l,:u,:8.~,Y~I,:u~:s,~,p~. The put transfers the elements \nof z (from r(Z,) to z(u2) with stride 8 ) in local memory to remote address y(l,) with stride sy in destination \nprocessor p. The get works the same way except that the source and destination of data movement are reversed. \nMost communication primitives supported in existing languages or machines [8, Figure 12: Percentage reduction \nin total number of LMAD dimensions by coalescing and contiguous aggregation dol=l,M doJ=O,N ig2TbJ,;i \n= ..: enddo doJ=O,N do K - 0 2* J-1 . . . Z-Y i<+2 J) e;i;oY IK+2 J+2 (N+i)-1) erz! Figure 13: Code \nexample similar to code found in FFT applications, such as tfft2 10, 22, 231 require triplet notation \nfor fast vector copy-ing between distributed memories. Without simplifica-tion of access patterns, we \ncould not generate efficient Put/Get primitives for codes with complex subscript expressions, such as \nthose shown in Figures 2 and 13. But, by showing that the actual access patterns are just simple consecutive \nmemory accesses, we significantly re-duced communication overhead in our target code. In [24], we presented \nthe experimental evidence that the LMAD can be useful to simplify various access pat- terns with complex \nsubscripts and, thereby, facilitate the application of compiler techniques. In the exper-iment, as should \nbe expected, the effectiveness of the LMAD is roughly proportional to the percentage of the complex accesses \nshown in Figure 4. processors 2 4 8 16 32 64 mdg 1.2 1.3 1.4 1.6 2.6 3.0 trfd 1.1 1.0 1.2 1.5 1.6 1.5 \ntfftz 1.6 2.1 3.2 4.7 7.1 7.4 Table 1: Speed increase factor due to the LMAD on the Cray T3D for processors \nbetween 2 and 64. Table 1 shows the results of experiments with three of the programs used to compute \nthe frequencies in Fig- ure 4. The entries in the table represent the ratio of speedup improvement produced \nby using the LMAD compared to the speedup produced by Polaris using techniques supported by triplet notation. \nFor instance, we were able to improve the original speedup of trf d by about 50% on 16 processors (leading \nto a speed improve- ment factor of 1.5) when we simplified the array accesses with our techniques before \napplying Polaris analysis. Not surprisingly, using the LMAD produced no addi-tional speedup on the programs \nwith simple subscript-ing patterns, such as snim and tomcatv. 7 The Contribution of This Work We have \nextended the work described here so that it works inter-procedurally and devised parallelization and \narray privatization techniques based on the LMAD. The implementation of our techniques is only partially \ncom-plete, but preliminary tests and hand analysis indicate that the techniques will be able to automatically \ndo in- terprocedural parallelization and privatization of loop nests within the code tfft2 from the SPEC95fp \nbench-marks, despite its usage of non-affine subscript expres-sions like X(I+K+I*P**(L-I)), in which \nI, K, and L are indices of the surrounding loops. A more detailed de-scription of these techniques and \ntheir implementations will be presented in a forthcoming PhD thesis [17]. We believe that our techniques \nwill subsume the ex-isting Polaris intra-procedural parallelization and priva- tization techniques because \nof the increased precision of the representation. Furthermore, we believe that we will be able to parallelize \ninter-procedural loop nests because the techniques work across procedure boundaries. 8 Conclusion A significant \nportion of the array subscript expressions encountered in the benchmark programs we used are too complex \nto be precisely representable in triplet notation, yet quite often still lead to simple access patterns. \nWe have shown that a more general array access represen- tation, based on the stride and span produced \nby each loop index, can accurately represent the accesses. Furthermore, with the application of polynomial-time \nalgorithms, we can aggregate and simplify the rep- resentation of accesses that exhibit common patterns, \nsuch as coalesceable, interleaved, and contiguous ac-cesses. The result is often an access pattern that \nis simple enough to allow the application of efficient region operations analysis, eration. obtained \nniques. promise used in optimizing compilers for dependence array privatization, and communication gen-We \npresented the performance improvements on our target machines by applying these tech- Results indicate \nthat these techniques hold real for optimizing programs. We believe that the notion of strides and spans, \nand the simplification techniques based on them, should be useful to other compiler studies that need \naccurate intra-and inter-procedural array access analysis, re-gardless of the representation used for \ntheir array ac-cesses. References PI V. Balasundaram and K. Kennedy. A Technique for Summarizing Data \nAccess and its Use in Parallelism Enhancing aansformations. Proceedings of the SIG-PLAN Conference on \nProgramming Language Design and ImplemenWion, June 1989. U. Banerjee. Dependence Analysis. Kluwer Academic \nPublishers, Norwell, MA, 1997. PI W. Blume. Symbolic Analysis Techniques for Effective Automatic Parallelizotion. \nPhD thesis, Univ. of Illi-nois at Urbana-Champaign, Dept. of Computer Science, June 1995. [31 W. Blume, \nR. Doallo, R. Eigenmann, J. Grout, J. Hoe- flinger, T. Lawrence, J. Lee, D. Padua, Y. Paek, W. Pot- tenger, \nL. Rauchwerger, and P. Tu. Parallel Program-ming with Polqris. IEEE Computer, 29(12):78-82, De-cember \n1996. 141 [51 M. Burke and R. Cytron. Interprocedural Dependence Analysis and Parallelieation. Proceedings \nof the SIG-PLAN Symposium on Compiler Construction, pages 162-175, July 1986. D. Callahan and K. Kennedy. \nAnalysis of Interproce-dural Side Effects in a Parallel Programming Environ-ment. Journal of Parallel \nand Distributed Computing, 5:617-550, 1988. PI [71 S. Chatterjee, J. Gilbert, F. Long, R. Schreiber, \nand S. Teng. Generating Local Address and Communication Sets for Data-ParallelPrograms. Journal of Parallel \nand Distributed Computing, 26(1):72-84, April 1995. Cray Research Inc. SHMEM Technical Note for For-&#38;an, \n1994. PI B. Creusillet and F. Irigoin. Exact vs. Approximate Array Region Analyses. In Lecture Notea \nin Computer Science. Springer Verlag, New York, New York, August 1996. PI [lOI D. Culler, A. Dusseau, \nS. Goldstein, A. Krishnamurthy, S. Lumetta, T. Eicken, and K. Yelick. Parallel Pro-gramming in Split-C. \nProceedings of Supercomputing 99, pages 262-273, November 1993. PI G. Danteig and B.Eavcs. Fourier-Motekin \nElimination and its Dual. Journal of Combinatorial Theory, pages 288-297,1973. P21R. Graham, D. Knuth, \nand 0. Patashnik. Concrete Mathematics: A Foundation for Computer Science. Addison-Wesley Pub. Co., New \nYork, 1989. J. Grout. Inline Expansion for the Polaris Research Compiler. Master s thesis, Univ. of Illinois \nat Urbana-Champaign, Dept. of Computer Science, May 1996. I141 J. Gu, Z. Li, and G. Lee. Symbolic Array \nDataflow Analysis for Array Privatization and Program Paral-lelization. Proceedings of Supercomputing \n95, Decem- ber 1995. 1131 [15] P. Havlak. Interprocedurul Symbolic Analysis. PhD the-sis, Rice University, \nMay 1994. [16] S. Hiranandani, K. Kennedy, and C. Tseng. Evaluat-ing Compiler Optimizations for Fortran \nD. Journal of Parallel and Distributed Computing, pages 27-45, 1994. [17] J. Hoeflinger. PhD thesis, \nUniv. of Illinois at Urbana-Champaign, Dept. of Computer Science, forthcoming. [18] C. Huson. An In-line \nSubroutine Expander for Parafrase. Master s thesis, Univ. of Illinois at Urbana- Champaign, Dept. of \nComputer Science, [19] Z. Li and P. Yew. Efficient Interprocedural Program Parallelization and Restructuring. \nof the SIGPLAN Symposium on Parallel Ezperience with Applications, Languages July 1988. PI Z. Li, P. \nYew, and C. Zhu. An Efficient dence Analysis for Parallelizing Compilers. May 1982. Analysis for Proceedings \nProgramming: and Systems, Data Depen-IEEE Z kna- action on Parallel and Distributed Systems, 1(1):26-34, \nJanuary 1990. D. Maydan, S. Amarasinghe, and M. Lam. Array Data-Flow Analysis and its Use in Array Privatization. \nPro-ceedings of ACM SIGPLAN Sympoaium on Principles of Programming Langugea, January 1993. 1211 WI Message \nPassing Interface Forum. MPI-2: Extension8 to the Message-Passing Interface, January 12, 1996. J. Nielocha, \nR. Harrison, and R. Littlefield. Global Ar-rays: A Portable Shared-Memory Programming Model for Distributed \nMemory Computers. Proceedings of Su- percomputing 94, pages 340-349, November 1994. [23l Y. Paek. Automatic \nParallelization for Distributed Memory Machines Baaed on Access Region Analysis. PhD thesis, Univ. of \nIllinois at Urbana-Champaign, Dept. of Computer Science, April 1997. 1241 P51 Y. Pack and D. Padua. Experimental \nStudy of Com-piler Techniques for NUMA Machines. IEEE Interna-tional Parallel Proceaaing Symposium U \nSymposium on Parallel and Distributed Processing, April 1998. W. Pugh. A Practical Algorithm for Exact \nArray Dependence Analysis. Communications of the ACM, 35(8), August 1992. PI W. Pugh and D. Wonnacott. \nNonlinear Array Depen-dence Analysis. Technical Report 123, Univ of Maryland at College Park, November \n1994. [271 1281 Z. Shen, Z. Li, and P. Yew. An Empirical Study of For- tran Programs for Parallelizing \nCompilers. IEEE Z kana-action on Parallel and Distributed Syatema, 1(3):350-364, July 1990. P91 P. Tang. \nExact Side Effects for Interprocedural De-pendence Analysis. Communications of the ACM, 35(8):102-114, \nAugust 1992. [30] R. Xolet, F. Irigoin, and P. Feautrier. Direct Paral-lelization of Call Statements. \nProceedinga of the SIG-PLAN Symposium on Compiler Construction, pages 176-185,1986. 1311 P. Tu. Automatic \nArray Privatization and Demand-Driven Symbolic Analysis. PhD thesis, Univ. of Illinois at Urbana-Champaign, \nDept. of Computer Science, May 1995. \n\t\t\t", "proc_id": "277650", "abstract": "Existing array region representation techniques are sensitive to the complexity of array subscripts. In general, these techniques are very accurate and efficient for simple subscript expressions, but lose accuracy or require potentially expensive algorithms for complex subscripts. We found that in scientific applications, many access patterns are simple even when the subscript expressions are complex. In this work, we present a new, general array access representation and define operations for it. This allows us to aggregate and simplify the representation enough that precise region operations may be applied to enable compiler optimizations. Our experiments show that these techniques hold promise for speeding up applications.", "authors": [{"name": "Yunheung Paek", "author_profile_id": "81100373014", "affiliation": "New Jersey Institute of Technology", "person_id": "PP14132257", "email_address": "", "orcid_id": ""}, {"name": "Jay Hoeflinger", "author_profile_id": "81100599322", "affiliation": "University of Illinois, Urbana-Champaign", "person_id": "P135779", "email_address": "", "orcid_id": ""}, {"name": "David Padua", "author_profile_id": "81452612804", "affiliation": "University of Illinois, Urbana-Champaign", "person_id": "P63208", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/277650.277664", "year": "1998", "article_id": "277664", "conference": "PLDI", "title": "Simplification of array access patterns for compiler optimizations", "url": "http://dl.acm.org/citation.cfm?id=277664"}