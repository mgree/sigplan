{"article_publication_date": "05-01-1998", "fulltext": "\n Scalable Cross-Module Optimization Andrew Ayers* Stuart de Jongt John PeytonJ Richard Schoolers Hewlett-Packard \nMassachusetts Language Laboratory 300 Apollo Drive Chelmsford, MA 01824 Abstract Large applications \nare typically partitioned into sepa-rately compiled modules. Large performance gains in these applications \nare available by optimizing across module boundaries. One barrier to applying cross-module optimization \n(CMO) to large applications is the potentially enormous amount of time and space con-sumed by the optimization \nprocess. We describe a framework for scalable CM0 that pro-vides large gains in performance on applications \nthat contain millions of lines of code. Two major techniques are described. First, careful management \nof in-memory data structures results in sub-linear memory occupancy when compared to the number of lines \nof code being op- timized. Second, profile data is used to focus optimiza- tion effort on the performance-critical \nportions of appli- cations. We also present practical issues that arise in deploying this framework in \na production environment. These issues include debuggability and compatibility with existing development \ntools, such as m&#38;e. Our framework is deployed in Hewlett-Packard s (HP) UNIX compiler products and \nspeeds up shipped independent software vendors applications by as much as 71%. Introduction Most modern \nprogramming languages support a con- cept of separate compilation in which a single program is broken \nup into modules that can be fed individually to a compiler. This separation allows for better manage-ment \nof the program source and allows for faster recom- * ayersOincert.com tsdejongOmicrosoft.com ~jpeytonOallaire.com \n\u00a7schoolerQincert.com pilation of the program. The reduction in compilation time results from programmers \nonly needing to recom- pile the modules they are working on when building an application. After being \nbuilt once, other modules are simply linked into the system along with the changed modules. Modularization \nprovides benefits for source manage- ment and build time, but can limit the level of applica- tion performance \nachieved by the compiler optimizer [ 11. For instance, module barriers hide information about aliasing \neffects on routine arguments and global vari-ables. More directly, compiling only within module boundaries \nlimits the compiler s ability to inline a rou- tine from one module into another. Eliminating these boundaries \nin all or just a small part of a program can lead to substantial speedups in benchmarks and appli- cations. \nWhile the performance of many applications is greatly enhanced by cross-module optimization (CMO), various \ndrawbacks have prevented wide-scale use by ap- plication developers. The biggest hurdles in deploying \nCM0 are the increases in build time and memory con-sumption of the compiler. With CMO, a change in one \nmodule potentially requires recompilation of all mod-ules in the CM0 set, leading to increased build \ntime. Furthermore, optimizers often keep all the information they use during compilation in memory. Although \nthe trend is for development and build machines to con-tain larger and larger amounts of memory, there \nstill comes a point in compiling a large application when the optimization information needed for all \nthe mod-ules the programmer wants to optimize together will not fit within available memory. To overcome \nthese obstacles in deploying CM0 we have developed a framework that limits optimizer mem-ory consumption \nby storing inactive optimizer data in a repository, and that limits build time by selectively optimizing \nonly the parts of applications that are heav- ily used during their runs. This framework has been deployed \nin HP production compilers and is used to __________________________________ SPECint 95 _________._._ \n_.__... 2.40 ........._... [SV Apps .._.__ . . . . . . . . . . ____ ._..............-.-----. . . . _ \n 2*20 . . . . . . . . . . . _ _....._....___..................... WCMO .---________________.._.............~.... \n2.00 . . . . . . . . . . . . . . . . . . . ..~. . . . . . . . . . . . . . ____., .eo . . . . . . . . \n. . . . ..__............................... n PBO . . . . .._.___.__ . . . . . . . . . ..-...... . . \n. . , ,60 . . .._.-. _-.._... &#38;MO+PBO . . . . . . . . . . . . _ __ . . . . . . . . . . . . . . . \n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ____.._.____ ___._ \n. . . . . . . . . . . . . . . . ..-.- . . . . . , .40 .-...-.._ f%, d go m88k gee camp Ii ijpag per1 \nvortex Mcadl McadP Mcadt* Figure 1: Relative speedup of aggressively optimized SPECint95 and ISV application \nprograms with respect to run times at the default optimization level (+02), as measured on a 180 MHz \nHP-UX PA8000 workstation. McadS s speedup is relative to optimization only within basic block boundaries \n(+Ol). ship highly optimized versions of our vendors applica-tions. One such program, containing five \nmillion lines of code, runs 71% faster when compiled with both cross-module and profile-based optimization \nthan when com-piled with default intraprocedural optimization. In section 2 we present performance results \nachieved using this framework. Section 3 provides a brief de-scription of the HP-UX compiler infrastructure. \nIn sec-tion 4 we describe the not-all-in-memory (NAIM) model of compilation. In section 5 we describe \nthe selective op- timization framework. Section 6 delves into some of the practical issues encountered \nwhen developing and work- ing within our compilation framework, including debug-gability and compatibility \nwith standard build tools, such as make [5]. Section 7 compares our framework to other related compilation \nframeworks such as the Con- vex Application Compiler [12] and Parascope [3]. In section 8 we discuss \nthe history of our framework and its future. Section 9 closes the paper with a summary of our results. \nPerformance Results The aim of CM0 is to eliminate module boundaries as barriers to interprocedural analysis \nand optimization. We have found that the combination of profile based optimization (PBO) and CM0 can \nlead to substantial performance improvements in applications consisting of more than one module [I]. \nFigure 1 shows the impact that CM0 and PBO can have on benchmark and application performance. The speedups \nshown in the figure are relative to the applica- tion run time when compiled with a default level of \nop- timization (-0 or +02). Even at this default level the compiler aggressively optimizes the code [lo, \n41, per- forming a full complement of global optimizations, such as register allocation, scheduling, \nand strength reduc-tion. However, the optimizations performed are strictly intraprocedural. In the figure \nwe present data measured at three more aggressive optimization levels: PBO (+02 +P); CM0 (+04); and the \ncombination of CM0 and PBO (+04 +P). When optimizing with just PBO, the compiler uses block and edge \nexecution profiles in various ways: optimizing the layout of basic blocks, improving prof-itability estimates \nfor code motion, and improving the cost model for register allocation. The linker also uses profile data \nto cluster frequently-used routines together in the final program image [13,15]. With just CMO, the compiler \nperforms a limited amount of interprocedu- ral analysis across all the modules being optimized and inlines \ncalls irrespective of module boundaries. When both CM0 and PBO are used together the compiler uses call \nprofiles to improve the inlining heuristics [l]. In particular, the optimizer will attempt to aggressively \ninline at hot call sites: call sites with high profile counts. We gathered data for eleven programs: \nthe eight SPEC95 integer benchmarks, plus three multi-million line mechanical computer-aided design (MCAD) \nappli-cations from three different independent software ven-dors (ISVs). All of the SPEC95 integer benchmarks \nare in C, and the largest, 126.gcc, is approximately 120K lines of code. Mcadl is approximately 5M lines \nof C code; Mcad.2, 3M lines of C code, 3M lines of FOR- TRAN, and 0.5M lines of C++ code; and McadS, \n9M lines of C++ code. Profile data for the SPEC bench-marks was obtained using the SPEC-supplied train-ing \ndata sets, and the benchmark runs used the ref-erence data sets. The ISV applications were trained and \nbenchmarked on the same data sets. As previously noted [l], the best performance is ob- tained by a combination \nof PBO and CMO. Not surpris- ingly, there is considerable variability in the speedups, though all programs \nbenefit to some degree. What Figure 2: Schematic picture of the HP-UX compiler infrastructure. we find \ninteresting here is that speedups seen in the ISV applications are among the better results reported, \nindicating that the style of optimization may have greater leverage in large applications than in rather \nsmall benchmark programs. For reasons further explained in section 5, we were unable to compile the large \nMCAD applications with just CMO. Furthermore, the baseline data for McadS was taken at optimization level \n+Ol (optimize only within basic blocks), and so the speedup shown is not directly comparable to the other \nbenchmark results. Compiler Infrastructure The HP-UX compiler [9] breaks the compilation process up \ninto a series of pipelined components (see Figure 2), where each component communicates with the other \ncomponents through a common intermediate language (IL). The first stage of the pipeline consists of the \ncom- piler frontends, which convert source code into the IL. At the default optimization level (+02) \nthe IL is passed directly to the code generator and low level optimizer (LLO). At higher levels of optimization \n(+03 or +04) It might be argued that the ISV speedups we show might be artificially inflated in comparison \nto the SPEC results because the same data set was used in profile and benchmark runs on the ISV codes \nbut not on the SPEC codes. However, informal exper-iments we conducted on SPEC using the reference data \nto gen-erate profiles did not provide significant additional performance. We believe the change in profile \ngeneration methodology for ISV codes had no real impact on the results. the IL is first routed through \nthe high level optimizer (HLO) and then passes to the code generator and LLO. HLO optimizes code through \na series of transfor- mation phases. Each transformation phase defines a set of analyses and transformations \nthat HLO exe-cutes on either a single routine or on multiple routines. HLO s transformations include \nboth intra-and inter-procedural optimizations, including inlining, cloning, dead code elimination, constant \npropagation, mem-ory disambiguation, redundant branch elimination, and locality and schedule-enhancing \nloop transformations. The LLO is a sophisticated and mature intraprocedural optimizer [lo, 41, handling \nall optimizations that require detailed knowledge of the machine architecture, such as register allocation \nand scheduling. During compilation the user may specify that the compiler and linker should insert instrumentation \ncode into the program to collect profile data (+I). The cur-rent technology inserts counting probes into \neach in- traprocedural branch and each call. When this specially instrumented program is run, a profile \ndatabase is gen- erated (or added to, if data from an earlier run already exists). To enable PBO, this \ndatabase is made available to the compiler. The compiler correlates profile infor-mation from the database \nwith current program struc-tures, and uses the data to improve various heuristics in HLO, LLO, and the \nHP-UX linker [1, 13, 151. When compiling in CM0 mode, the frontends dump the IL directly to object files \nthat correspond to the source modules being compiled. When the linker en-counters these IL objects, it \nsends them to the opti-mizer and code-generator for further processing before creating the executable. \nIn CM0 mode the linker sends multiple IL objects to HLO in a single compilation. Large programs are often \nwritten in more than one source language (for instance, Mcad2 is a mixture of C, C++, and FORTRAN). Because \nHLO works at the IL level, it can freely optimize mixed-language applica-tions. In fact, HLO does not \nneed to know the source language of a module. 4 Not-All-In-Memory (NAIM) As we developed HLO we quickly \nrealized that the mem- ory consumption of the compiler severely limited our ability to aggressively compile \nlarge programs. The ini- tial release of HLO (circa 1994, HP-UX 9.05) used a representation that occupied \nupwards of 1.7K bytes per source line. This base representation included a fairly skeletal IR for each \nroutine s code, a complete symbol table for each module, and global data structures such as the program \nsymbol table and call graph. The 1.7K per line of code base representation mem-ory cost in the original \nHLO made it painful to compile a benchmark like SPECint92 s 085.gcc (about 60K lines of code) on a typical \nworkstation. To handle larger codes we developed a variety of techniques for both reducing the base representation \nmemory footprint and also for offloading information to free up process memory. Col-lectively we refer \nto these techniques as the not-all-in- memory (NAIM) model for large program optimization. This section \ndescribes the essence of our approach. 4.1 HLO Data Structures HLO s major data structures (see Figure \n3) fall into three classes: global, transitory, and derived. Global data structures, such as the call \ngraph and program-wide symbol table, are always resident in memory. Transitory data structures hold information \nrelated to specific modules, such as the individual module sym-bol tables and the internal representation \n(IR) for code. These data structures can be located either in or out of virtual memory at any given time, \nand when in memory, may exist in either expanded or relocatable form. De-rived data structures represent \nthe results of compiler analyses (for instance, data flow arcs, interval trees, or 2A particular intraprocedural \noptimization phase might tem-porarily require a great deal more memory per line of source. But, since \nsuch phases operate on one routine at a time, this memory can be freed and reused over and over again. \nThus, the over-all contribution of phase memory requirements to the compiler s memory requirement scales \n(linearly, one hopes) with the size of the largest routine in the program. Thus, intraprocedural mem-ory \nrequirements are not a dominant factor in compiling large programs, since large programs do not seem \nto contain larger routines. induction variable annotations). Early on in the devel- opment of HLO, we \nadopted a discipline whereby de-rived data was always recomputed from scratch rather than kept incrementally \nup to date. Because of this, derived data can be freely discarded when no longer needed, and later recomputed \non demand. To facilitate transitory object relocation, we also adopted a discipline for inter-object \nreferences. Concep-tually, all objects in HLO are part of a tree structure, with the global objects at \nthe root. Less permanent objects can freely refer to more permanent objects by the more permanent object \ns address. For instance, an instruction operand in the IR can point to a variable in the module symbol \ntable, or to a node in the call graph. Downward pointers are allowed only in special handle objects (indicated \nby the wedge-shaped boxes in Figure 3). The handle tracks the status of the more transitory object, so \nthat if a reference is made to a re- locatable object, the appropriate action can be taken. Sideways \npointers (say, from one module s symbol table to another s) are not allowed. Such pointers can usually \nbe simulated by walking up to the the common ancestor in the tree and then back down. 4.2 Limiting Memory \nConsumption The intuition behind NAIM is that only the data needed to perform a particular task is resident \nin memory. All other transitory data is compacted and potentially kept in an off-line disk repository. \nConceptually, HLO loads into memory only those objects it needs to carry out a particular optimization. \nAfter the optimization is per- formed, HLO unloads unneeded data to a repository. The process that manages \nthe movement of data in and out of the repository is called the loader. As Figure 4 illustrates, NAIM \nachieves sub-linear growth in mem-ory consumption as more modules are compiled under CMO. 4.2.1 HLO Object \nRelocation To support unloading, transitory objects have both relo- catable and expanded forms. In expanded \nform, objects freely refer to other objects using addresses, allowing for efficient data structure traversal \nat the cost of somewhat larger objects. Relocatable objects, on the other hand, are designed for compact \naddress-independent representation. To handle inter-object references we adopted a technique from the \nworld of object-oriented databases: references between relocatable objects use persistent identifiers \n(PID). When a pool of inter-related objects is loaded, all inter-object references within the pool are \nconverted from PIDs to virtual addresses in a process called eager Global Objects CY*O*aYPI*CI*a** Transitory \nObjects ExDanded Rdutlne  Figure 3: Organization of HLO-created objects. Global objects are always \nmemory resident. Transitory objects like routines have both expanded and compact relocatable in-memory \nrepresentations. The relocatable form can be offloaded to disk to free up memory. Derived objects represent \nthe results of compiler analyses and can be freely deleted and recreated. 800 Overall Compiler g 500 \n8 400 = 2 300 HLO - 0 200 5 100 2 0 0 1 ,ooo,ooo 2,000,000 3,000,000 4,000,000 5,000,000 I SC Lines of \nCode Compiled with Cross-Module Optimization Figure 4: The increase in both compiler and HLO memory \nusage as more lines of the Mcadl application are compiled in CM0 mode. The growth rate differences between \nthe overall compiler and HLO memory consumption are caused by the increase in the number of inlines that \nare made as more code is processed. LLO s memory requirements increase quadratically as the sizes of \nthe routines it processes are increased. The small arrow on the the left side of the x-axis shows the \nrelative size of Mcadl to the largest of the SPECint95 benchmarks, 126.gcc. swizzling [ll, 191. Conversely, \nwhen a pool is unloaded all references are swizzled back to PIDs. 4.2.2 Compaction To convert object \npools to and from their relocatable forms, the loader calls on the compaction and uncom-paction drivers, \nrespectively. In addition to swizzling the pointers, the compaction algorithm shrinks the size of the \ndata being processed. Compaction is realized through the careful layout of objects in the pool and the \nremoval of unneeded fields from the objects. Objects are placed in the relocatable pool in a stack form, \nminimizing the number of inter- object connections that need to be maintained. For in- stance, an encoded \nbasic block is immediately followed by the encoded instructions it contains, and these in turn are each \nfollowed by their operands. Expanded objects also contain redundant pointers which can be omitted from \ntheir compacted form. As an example, doubly-linked lists can be encoded as singly linked lists, which \nin turn can be encoded with no pointer fields using the stack representation. Other fields in the objects \ncan simply be ignored because they point to derived data. For example, data flow and loop dependence \ndata are temporary attributes attached to IR objects. As mentioned earlier, these tem- porary attributes \nare recomputed by HLO when needed, so the fields that contain these attributes are not needed when the \nobject itself is inactive. Since these fields typ- ically take up about 2/3 of an object, significant \nspace can be saved by not representing these fields in com-pacted objects. A side benefit of compaction \nis garbage collection. To enable a more efficient implementation of mem-ory allocation, HLO s memory \nsystem does not sup-port explicit deallocation of individual variable-sized objects. Compaction works \nby traversing the set of objects reachable from some root object. Thus, after compacting the reachable \nobjects in a pool of memory, HLO simply returns the pool s memory to its free list for reallocation, \nthereby reclaiming the space occupied by any remaining unreachable objects.  4.3 Reducing Compile Time \nEffects of NAIM While the NAIM model allows the compiler to process large amounts of code, it can also \nresult in a consid- erable amount of compile time overhead. To diminish the compile time effects of loading \nand unloading the following three techniques are used. First, HLO uses arena-based dynamic memory allo-cation. \nThis allows HLO to group objects that are opti- mized together into a relatively dense set of pages. \nFor example, all objects that make up a single IR routine are grouped together. We have found that explicitly \nmanaging locality in this fashion results in faster com-pilation times, especially as the optimizer s \nmemory use approaches the amount of physical memory available on the machine. 250 I NAM off t IR Compaction \nST Compaction Offloading t to disk 0 10 20 30 40 HLO Optimization Time (min) Figure 5: HLO compile time \nversus memory usage when compiling 126.gcc. This graph shows the effect differ-ent memory usage optimizations \nhave on compile time compared to how much memory they save. This graph factors out LLO s fixed contribution \nto compile time and memory usage. Second, HLO only uses NAIM functionality when necessary. This means \nit is turned off when compil-ing smaller programs. To implement this, the compiler uses a series of memory \nthresholds that are tied to the amount of physical memory on the machine executing the optimizer process. \nWhen hit, these thresholds turn on more and more of the NAIM functionality. For in- stance, when the \nprocess starts getting too large, com-paction of IR routines is turned on. At another thresh-old, compaction \nof symbol tables is turned on. Finally, for compilations that have memory requirements larger than the \navailable memory, HLO unloads object pools into disk repositories. With thresholding, our frame-work \nfor compiling massive amounts of code imposes little or no overhead on compilations that fit into the \nmemory constraints of their development environments. Third, HLO s unloader works lazily, maintaining \na cache of the most recently used expanded object pools. This diminishes the affect our NAIM functionality \nhas on compile time once it is turned on. Cache sizes are based dynamically on the memory resources of \nthe ma-chine. Cache fullness is based on the number of ex-panded pools resident in memory. When the optimizer \nrequests the loader to unload a pool, that pool is marked unload pending and placed in the cache. If \nthe cache is full, the loader unloads the least recently touched pools. The result is that the loader \noften only needs to change the state of a cached pool when the optimizer requests the data in that pool, \nas opposed to actually doing the work of loading. The threshold and caching schemes are transparent to \nclients of the loader. Clients simply request that all unneeded pools are unloaded from memory. Whether \nor not the objects actually get compacted and unloaded is determined internally by the loader. Despite \nthis, the caching scheme is more effective if the optimizer takes caching into account when scheduling \nthe optimization of code. For example, HLO s inliner tries to carefully schedule inlines so that cross-module \ninlines from the same pair of modules are processed one after another. Figure 5 indicates the time-space \ntradeoffs that are available using NAIM. For instance, the compiler can choose to occupy about 240 MB \nand run through HLO s optimizations in about 18 minutes. The compiler can reduce memory requirements \nfor HLO s optimizations to about 100 MB while increasing compile time to about 22 minutes (spending the \nextra 4 minutes compacting and uncompacting the IR). At the extreme, HLO can be squeezed into 25 MB or \nso, at the cost of a 50% increase in compile time. It s important to note that the machine used to gather \nthis data had enough available physical memory to comfortably hold all the data in each com- pile. Thus, \nFigure 5 does not show the additional time cost that would be incurred if the compile process was forced \nto swap pages. For large programs, this cost will exceed the overhead imposed by compaction and offloading. \nFurthermore, the HP-UX 9.x and 10.x op- erating systems impose a hard limit of about 1 GB on the virtual \nheap space available to a running process, so even if available physical memory is not a constraint, \navailable virtual memory eventually limits the scale of optimizations that keep all data resident in \nmemory. 4.4 Is NAIM Necessary? One might argue that RAM sizes are growing at such a pace that in the \nnear future that managing the com-piler s memory consumption will be unnecessary. We believe this will \nnot be true. Naively keeping every-thing in memory would consume gigabytes of memory for large programs, \nand programs are getting larger. Additionally, memory system implementations increas-ingly reward memory \naccess locality, which our tech-niques enhance. 5 Selectivity Despite our efforts to make compiling large \namounts of code efficient, compiling more code requires more time and memory. To avoid spending time \nand space com- piling code that is not critical to the performance of the application, profile data is \nused to guide the optimizer in deciding where to spend the majority of its time. This use of profile \ndata is termed selectivity. A coarse-grained selective process determines which source code modules should \nbe compiled with CM0 and PBO. First, the user specifies a selection percentage. Using the profile data, \nthe compiler orders all the call sites within the program by call frequency, and then retains only the \nselected percentage of sites. The com-piler then identifies the modules containing the callers and callees \nof the selected sites. These modules are compiled with CM0 and PBO. The remaining modules bypass HLO \nentirely, and are optimized at the default optimization level using PBO. With coarse-grained selectivity \nthe compiler deter-mines which modules are compiled with CM0 and PBO, but it does not know which functions \nwithin those modules are the important ones. Fine-grained selectiv-ity is used to determine which functions \nshould be con- sidered for optimizations such as inlining, and which ones should be passed through the \ninterprocedural op-timizer with little or no optimization. One complication is that information about \nroutines not selected for optimization can influence the opti-mization of selected routines. For example, \ninformation about global or module private variable usage can only be determined if all routines that \ncan access a variable are examined, not just the performance-critical ones. HLO addresses this by reading \nin all of the code and data within the set of modules compiled in CM0 mode. As the code and data are \nread in, a minimum amount of analysis and transformation is done to ensure that all information available \nabout data accesses is known. After this, routines not selected for optimization are left unloaded until \nsent to LLO for further processing. Leaving the non-critical routines in an unloaded state throughout \nthe remainder of HLO s run prevents them from further consuming compile time and memory re-sources. In \nFigure 6 we show the impact of selectivity on the compile-time and run-time behavior of Mcadl. The x-axis \nindicates the fraction of the application s code com- piled with both CM0 and PBO, as measured by both \nselectivity cutoff and lines of code. The remaining por-tion of the application s code is optimized only \nwith PBO. Note that Mcadl is large enough that optimizing with PBO alone requires about 200 minutes of \ncom-pile time. As more and more of the code is compiled under CM0 and PBO, the compilation time increases \nfrom 200 minutes to around 900 minutes. Thanks to NAIM, the compile time scales fairly linearly as more \ncode is compiled under CM0 and PBO. The second curve shows the run time benefit obtained from the dif- \nferent levels of investment in CMO. Peak performance is reached when roughly 20% of the code is compiled, \nin-dicating that about 80% of the code has no appreciable effect on performance. Thus, by choosing the \nappropri- ate selectivity parameter, the user can obtain the full benefit of CM0 while limiting compile \ntime to around 300 minutes.3 In large CM0 compilations without profile informa-tion, the compiler is \nunable to determine where to fo- cus its efforts. In such non-PBO cases, our heuristics drive the compiler \nto thoroughly optimize all routines. In comparison with selective methods, this thorough optimization \nof all code causes large increases in com-pile time and compiler memory usage. These problems are further \nexacerbated by the fact that certain inter-nal algorithms do not scale well when faced with large amounts \nof potentially important code. Hence, we have never been able to compile all of Mcadl without the help \nof profile data. Our best attempts exhaust the heap after allocating roughly 1GB of memory and af-ter \nrunning for about 40 hours. While we believe it is possible to re-engineer the heuristics and algorithms \nto substantially reduce the time and memory required for a pure CM0 compilation, doing so seems pointless \nin the light of the overwhelming benefits PBO selectivity provides at both compile time and run time. \n 5.1 Is Automatic Selectivity Necessary? It is possible for the user to manually specify the performance-sensitive \nportion of an application instead of relying on potentially expensive automatic tech-niques. Our experience \nis that, in practice, the per-formance kernel is not known and changes over time with the application \nand workload. Many ISVs compete primarily on functionality and not on performance, so maintaining in-depth \nunderstanding of application per-formance is often a low priority for applications devel- opers. 6 Practical \nConsiderations 6.1 Build Environment Compatibility Most large systems are built using make [5] or similar \ntools that assume a program is composed of a collection of object modules derived from source modules. \nMany commercial applications are delivered and built on sev- eral platforms, so compatibility with standards \nand ex- isting build tools is very important. Using a persistent database is problematic in this environment, \nsince typ- ically special commands must be invoked to access and SThe factor of 1.33 speedupshown here \nreflects the speedup of CM0 and PBO over PBO alone. An additional speedup factor of 1.25 is obtained \nby from PBO over default optimization, for a total speedup factor of about 1.7 (66 minutes versus 39), \nas is shown in Figure 1. Selectivity Parameter (Percent of call sites selected) +g- 1000 55 I .5 E \n800 50 if Q i= Q)z 600 45 ;;; s e 400 ; 8 = 2 200 40 * 2 s! 0 o( 35 .= 0 0 1 ,ooopoo 2,000,000 3,000,000 \n4,000,000 5,000,000 Lines of Code Optimized by Selective Cross-Module Optimization Figure 6: These two \ncurves show how compile time and run time of Mcadl vary as more and more of the application is compiled \nwith CM0 and PBO (+04 +P). Code not compiled with CM0 and PBO is compiled at the default optimization \nlevel with PBO (+02 +P). Results were measured on a 180 MHz HP PA8000 workstation with 512Mb of memory. \nmaintain the database. Our system works with exist-ing processes by maintaining all persistent information \n(save for profile data) in object files, and rebuilding program-wide information at optimization time. \nThe disadvantage is that no persistent program library is available to minimize re-compilation [2]. \n6.2 Selectivity The profile data used by the HP-UX compilers to select the modules to optimize and call \nsites to inline is based on data collected from running an instrumented version of the application on \na fixed set of training inputs. It is possible that the training sets will not exercise parts of the \napplications that are important to some users. Those untrained parts of the application might not be \noptimized adequately if selectivity is used. We have found that designing a good training set for gathering \nprofile data is a continuous process. As the application changes and the application is used in new ways, \nthe training set needs to be updated. Our system does allow old profile data to be used with new code, \nbut as the new code base diverges from the old, the benefits obtained with stale profiles will diminish \nover time in a manner reminiscent of the diminishing benefit of stale receiver class profiles described \nby Grove [6]. Another issue involving the dynamic nature of se-lectivity and our memory consumption optimizations \nis reproducibility. Good compiler diagnostics on what the compiler is optimizing are essential when deploying \nse-lectivity. Also, the compiler must behave in exactly the same way when compiling the same piece of \ncode, us- ing the same profile data, on a machine with the same memory configuration from run to run. \nFor example, to enhance stability, we had to rewrite those parts of HLO that sorted or hashed on virtual \naddresses. This makes it possible to reproduce compiler problems, as well as yielding consistent performance \nresults. Anything else is confusing to compiler developers, performance ana-lysts, and end users.  \n6.3 Optimizer-Induced Behavior Changes Run-time behavior differences that appear only when large-scale \ninterprocedural optimizations are deployed are particularly difficult to diagnose. The technique that \nwe find most effective is divide and conquer, re-ducing things to simpler and simpler instances until \nthe problem becomes evident. We often work our way along two dimensions: both reducing the amount of \ncode ex- posed to the optimizer, and reducing the number of optimizations performed on the code. Both \nof these reductions can in principle be auto- mated. Binary search is an effective technique to elimi- \nnate irrelevant optimizer actions first in bulk, and then in smaller units. Pure binary search on the \nmodules has limited applicability, because often several modules will need to be optimized together to \ndemonstrate the problem. On one occasion we found a bug that re-quired eight modules to be compiled under \nCMO. Usu-ally, however, problems reduce to two or three required modules. Once the amount of code involved \nhas been minimized, the second stage of isolation usually involves pinpointing a particular optimizer \ntransformation. To do this we have implemented controllable operation lim- its on transformations such \nas inlining so we can employ binary search to identify the inline that makes the dif- ference between \na failing and a working program [18]. Even if that operation is itself correct, its identity can sharply \nnarrow the subsequent search space. Typically we focus on making the optimizer correct for legal source \nprograms. Trying to maintain incor-rect program behavior across the optimization levels severely limits \nthe optimizer s freedom to optimize le-gal programs. Unfortunately, the larger the program, the more \nlikely it is to contain some non-standards-conforming constructs, and some of these, such as mis- matched \ninterfaces, only show up with interprocedural optimization. Uninitialized local variables are a com- \nmon source of problems because inlining often makes significant changes to the stack frame layout and \ndy-namic stack behavior. Many programs (one notable ex-ample being the SPEC95 benchmark 099.go) take \nliber- ties with global storage as well, indexing off the end of global or module-static arrays, tripping \nup optimizers that reorder global variables or remove unused global variables. Inlining can also derail \nvarious clever coding tricks. For instance, the alloca package supplied with the SPEC95 benchmark 126.gcc \nrelies on the fact that a callee s local variable will either be at a consistently higher or lower address \nthan a caller s local variable (depending on the direction of stack growth). Unfor-tunately, when the \ncallee is inlined into the caller, this property may no longer hold, causing perplexing run-time failures. \nWe strongly advocate the use of interface checkers and memory error diagnostic tools to make programs \nmore robust in the face of large-scale optimization. 6.4 Source Availability Unfortunately, large programs \nlike Mcadl, Mcad2, and Mcad3 are hard to come by. We would like to have re- ported data for other large \nprograms, but had to work with what was available. Bringing the first few large ISV programs up under \naggressive optimization requires intensive effort and works best if the compiler team has access to the \nsource code, build process, test suites, and application developers. Finding CM0 bugs often required \nhelp from development engineers who under- stood the code. They were able to point us to the source of \nproblems based on which regression tests were fail-ing. Working directly at the ISV site can be a big \nhelp in speeding up the process. Getting all this access may require months of careful negotiation with \nthe vendor, as they often regard their source code as a major asset. 7 Related Work While there have \nbeen many papers describing the ben- efits of interprocedural optimization and whole program analysis, \nthere are few that directly confront the issue of deploying these technologies in production compila- \ntion systems. Systems such as the original Convex Ap- plication Compiler [12] and the ParaScope Program- \nming Environment [3] require their users to develop within special environments. These environments use \ndatabases to keep track of interprocedural analysis in- formation and interprocedural dependencies, using \nthe latter to determine whether a particular module needs recompilation [2]. Other systems allow for \nmore limited CM0 capabilities by allowing users to compile multiple modules at once, but without providing \na means to se- lectively limit the amount of code being optimized or providing ways for unneeded data \nto be stored outside of virtual memory. Our scheme is similar to Convex s current Applica-tion Compiler \nin that both systems use a temporary repository to store data not in use by the compiler [IS]. However, \nthere are two significant differences in the way the application compiler manages data. First the appli- \ncation compiler s offloaded representation for the sym- bol table and IR is substantially different from \nthe in- core representation, so that loading and unloading re-quire costly translation steps, increasing \ncompile times. Second, the application compiler maintains detailed in-terprocedural analysis information \nin the repository in a monolithic form, so all the data from the repository must be read in to access \na particular piece of data, thus limiting scalability. The advantage of our scheme is that the offloaded \nrepresentation can be directly mapped to the loaded representation. Since loading requires no rebuilding \nof the symbol table and IR information, it is very fast. Always having the program-wide data in memory \nis also an advantage, because it is readily acces-sible without loading it from a repository. With our \nap-proach we have been able to successfully compile much larger applications. Self-contained compilation \nsystems and development environments offer some advantages over our frame-work, however. These environments \ncan easily track changes in the source, giving users immediate feedback about their program constructs \nand possible problems. Having a database also allows the compiler to store program-wide data about the \napplication that can be used to improve compile time. Having dependency in-formation stored away in a \npersistent repository allows the compiler to know exactly what modules need to be rebuilt when a change \nis made, because information about optimization dependencies can be stored in the repository [2]. In \naddition to fully functional CM0 frameworks, many linkers also incorporate the ability to do whole program \ntype optimizations. The linker s ability to ag- gressively optimize is limited, however, because of link \ntime considerations and the fact that the information passed to the linker is at a very low level. However, \nworking at such a low level reduces or eliminates the need to represent bulky symbol table information, \nand hence simplifies the memory management problem con-siderably [17, 71. Though our framework supports \ninterprocedural op-timization, we have found that its main benefit is in enabling profile-based cross-module \ninlining. In this we agree with Richardson and Ganapathi [14], who found that inlining gave much more \nsignificant performance improvements than did classical interprocedural opti-mizations. However, we do \nnot feel that our lack of results with interprocedural optimization should taken as an indication that \ninterprocedural optimization is of little use in speeding up scalar programs. We believe that there are \ngains to be had from interprocedural optimization, but they are highly dependent on con-text, specifically \nthe surrounding compiler technology and the hardware implementation. 8 Past and Future Work Our optimizer \ns framework for CM0 and PBO has been developed over a number of years. CM0 and PBO were first introduced \nin the HP-UX 9.0 release. At that time all objects within the optimizer were left in expanded form. HLO \nrequired about 1.7KB of memory per line of code, and compile time issues prevented us from compil- ing \napplications much larger than 60,000 lines of code. With HP-UX 10.01 compaction of IR routines was in- \ntroduced. This brought memory consumption down to about 0.9KB per line of code. Finally with later ver-sions \nof our HP-UX 10.20 compilers, our NAIM and coarse-grained selectivity framework were introduced and tuned \nto work on applications containing millions of lines of code. We see our work as a preliminary foray \ninto the world of large-scale optimization. Fine-grained selectivity can be further improved. The optimizer \nitself can be par- allelized so the work of loading and unloading code to and from memory can be done \nconcurrently with the optimization process. Improving compiler diagnostics and the debuggability of optimized \ncode are areas that continually need work. Higher levels of performance can be reached by exploiting \nwhole program informa-tion obtained from the linker. In addition to the strategy where code is either \nop-timized or not optimized depending on whether it is part of the selected critical code, one can take \na more multi-layered approach. For example, the most critical code can be compiled using CMO, while code \nthat is executed little or not at all may not be optimized at all. Code that falls somewhere in between \ncan be opti- mized more or less aggressively, depending on the fre- quency of execution as compared to \nother code within the same application. Using this strategy, it is possible to maximize the benefit of \nthe optimizer to suit different applications. Another area for improvement is in eliminating un-necessary \nrecompilation. Current techniques for doing this require persistent databases to hold program-wide information \nabout dependencies [2]. To be effective in HP s market, we would have to accomplish this with-out imposing \nconstraints on our users development en-vironments. On the downside, the increasing prevalence of shared \nlibraries poses a significant challenge to large-scale static optimization, since related program units \nmay only be associated when the program is actually run. 9 Summary We have presented practical techniques \nfor cross-module and profile-based optimization of large applica- tions which can yield substantial speedups \nin our ISV applications. This framework has been deployed in HP production compilers and is used to ship \nhighly opti-mized versions of our vendors applications. Mcodl, containing five million lines of code, \nruns 71% faster when compiled with both CM0 and PBO compared to only intra-routine optimizations. This \nis achieved after compiling about 20% of the code and 5% of the call sites in CM0 mode. Our framework \nis based on two fundamental obser-vations. First, there is substantial locality in the op-timization \nprocess, so we do not need to maintain all the program representation in memory at any given time. For \nthis we implement a not-all-in-memory model which carefully manages the in-core storage devoted to optimization-related \ndata. Second, programs typically spend the majority of their runtime in very small frac-tions of their \ncode, so one should focus optimization effort where it makes the most difference. To accom-plish this \nwe have developed a selective optimization framework that uses run-time profile data to guide op- timization \neffort. We have also touched on usability issues when de-ploying CM0 and PBO techniques in production \nenvi-ronments. Compatibility with existing build processes is one important property. Reliability is \nalso an impor- tant issue, and we described techniques for isolating and diagnosing optimization and \nsource code defects. 10 Acknowledgments This work would not have been possible without the contributions \nmade by the members of the HLO project and other HP language projects, especially Carl Burch, Bob Gottlieb, \nRajiv Kumar, Sridhar Ramakrishnan, and Jonathan Springer. Manuel Benitez and Lacky Shah developed and \nimplemented coarse-grained selec-tivity. Special thanks to Anne Holler for her work with Mcad3, to Adam \nMatusiak for his work on Mcad.2, and to Frank Deming and Manuel Benitez for their work on Mcadl. When \nwrestling an elephant it s good to have a few friends along. References [l] A. Ayers, R. Gottlieb, and \nR. Schooler. Aggressive inlining. ACM SIGPLAN Conference on Program- ming Language Design and Implementation \n(PLDl 97)) 134-145. [2] M. Burke and L. Torczon. Interprocedural opti-mization: eliminating unnecessary \nrecompilation. ACM Transactions on Programming Languages and Systems (TOPLAS) 15(3), July 1993,367-399. \n[3] K. Cooper, M. Hall, R. Hood, K. Kennedy, K. McKinley, J. Mellor-Crummey, L. Torczon and S. Warren. \nThe ParaScope parallel programming en-vironment. Proceedings of the IEEE 81(2), Febru-ary 1993. [4] D. \nDunn and W. Hsu. Instruction scheduling for the HP PA-8000. Micro 29. [5] S. Feldman. Make -a computer \nprogram for main- taining computer programs. Software Practice and Experience 9, 1979, 255-265. [6] D. \nGrove, J. Dean, C. Garrett, and C. Chambers. Profile-guided receiver class prediction. OOPSLA 1995, 108-123. \n[7] D. Goodwin. Interprocedural dataflow analysis in an executable optimizer. ACM SIGPLAN Confer-ence \non Programming Language Design and Imple- mentation (PLDI 97), 122-133. [8] M. Hall. Managing Interprocedural \nOptimization, Ph.D. Thesis, Rice University, Department of Computer Science, May 1991. [9] HP Optimization \nTechnology White Paper. http://www.hp.com/wsg/ssa/fortran/optimiz.html. [lo] A. Holler. Compiler optimizations \nfor the PA-8000. COMPCON 1997 Digest of Papers, February 1997. [ll] M. Loomis. Object Databases, The \nEssentials Addison-Wesley, Reading, Massachusetts, 1995. [12] J. Loeliger and R. Metzger. Developing \nan inter-procedural optimizing compiler. ACM SIGPLAN Notices 29(d), April 1994, 41-48. [13] K. Pettis \nand R. Hansen. Profile guided code posi- tioning. ACM SIGPLAN Conference on Program- ming Language Design \nand Implementation (PLDI 90)) 16-27. [14] S. Richardson and M. Ganapathi. Interprocedural optimization: \nexperimental results. Software Prac-tice and Experience 19(2), February 1989, 149-169. [15] S. Speer, \nR. Kumar, and C. Partridge. Improving UNIX kernel performance using profile based opti- mization. In \nUSENIX 1994 Proceedings. [16] S. Stroud, private communication. [17] D. Wall. Global register allocation \nat link time. Proceedings 86 Symposium on Compiler Constr-uc-tion, Palo Alto, California, 1986. 181 D. \nWhalley. Automatic isolation of compiler er-rors. ACM Transactions on Programming Lan-guages and Systems \n15(5), September 1994, 1648- 1659. 191 S. Zdonik and D. Maier. Readings in Object-Oriented Database Systems, \nMorgan Kaufmann Publishers, Inc, 1990.  \n\t\t\t", "proc_id": "277650", "abstract": "Large applications are typically partitioned into separately compiled modules. Large performance gains in these applications are available by optimizing across module boundaries. One barrier to applying crossmodule optimization (CMO) to large applications is the potentially enormous amount of time and space consumed by the optimization process.We describe a framework for scalable CMO that provides large gains in performance on applications that contain millions of lines of code. Two major techniques are described. First, careful management of in-memory data structures results in sub-linear memory occupancy when compared to the number of lines of code being optimized. Second, profile data is used to focus optimization effort on the performance-critical portions of applications. We also present practical issues that arise in deploying this framework in a production environment. These issues include debuggability and compatibility with existing development tools, such as <i>make</i>. Our framework is deployed in Hewlett-Packard's (HP) UNIX compiler products and speeds up shipped independent software vendors' applications by as much as 71%.", "authors": [{"name": "Andrew Ayers", "author_profile_id": "81100199268", "affiliation": "Hewlett-Packard Massachusetts Language Laboratory, 300 Apollo Drive, Chelmsford, MA", "person_id": "P17687", "email_address": "", "orcid_id": ""}, {"name": "Stuart de Jong", "author_profile_id": "81100307758", "affiliation": "Hewlett-Packard Massachusetts Language Laboratory, 300 Apollo Drive, Chelmsford, MA", "person_id": "PP31036521", "email_address": "", "orcid_id": ""}, {"name": "John Peyton", "author_profile_id": "81100493350", "affiliation": "Hewlett-Packard Massachusetts Language Laboratory, 300 Apollo Drive, Chelmsford, MA", "person_id": "PP31092382", "email_address": "", "orcid_id": ""}, {"name": "Richard Schooler", "author_profile_id": "81100550569", "affiliation": "Hewlett-Packard Massachusetts Language Laboratory, 300 Apollo Drive, Chelmsford, MA", "person_id": "P242939", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/277650.277745", "year": "1998", "article_id": "277745", "conference": "PLDI", "title": "Scalable cross-module optimization", "url": "http://dl.acm.org/citation.cfm?id=277745"}