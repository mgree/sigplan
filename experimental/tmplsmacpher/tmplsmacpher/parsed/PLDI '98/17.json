{"article_publication_date": "05-01-1998", "fulltext": "\n Communication Optimization5 for Parallel C Programs * Yingchun Zhu and Laurie J. Hendren School of Computer \nScience McGill University Montreal, Quebec, CANADA H3A 2A7 {ying,hendren}Qcs.mcgill.ca Abstract This \npaper presents algorithms for reducing the com-munication overhead for parallel C programs that use dynamically-allocated \ndata structures. The framework consists of an analysis phase called possible-placement analysis, and \na transformation phase called communi-cation selection. The fundamental idea of possible-placement analy-sis \nis to find all possible points for insertion of remote memory operations. Remote reads are propagated \nup-wards, whereas remote writes are propagated down-wards. Based on the results of the possible-placement \nanalysis, the communication selection transformation selects the best place for inserting the communica-tion, \nand determines if pipelining or blocking of com- munication should be performed. The framework has been \nimplemented in the EARTH-McCAT optimizing/parallelizing C compiler, and experimental results are presented \nfor five pointer- intensive benchmarks running on the EARTH-MANNA distributed-memory parallel architecture. \nThese exper- iments show that the communication optimization can provide performance improvements of \nup to 16% over the unoptimized benchmarks. Introduction In programming for distributed-memory parallel \nproces-sors, one important aspect is to minimize communica-tion overhead. As distributed-memory processors \nincur a significant penalty for accessing memory that is re- mote not local to the processor), programmers \nand/or compi I ers often try to maximize locality. However, there remain many situations where remote \naccesses cannot be made local. In some cases the programmer imple-ments a naive algorithm that does not \nfully exploit lo-cality, and in other cases applications have inherent ir-regularities that require a \nsignificant number of remote memory accesses. The focus of this paper is on reducing the overhead for \nremote memory accesses. In particular, we focus on reducing the communication overhead for irregular \nprograms that use dynamically-allocated pointer data This work supported, in part, by NSERC and FCAR. \nPwmisaion to make digital or hard copka of all or pan of this work for pwsonal 01 olassroom US. h granted \nwithout ia* provided that copies l . not mada w dhtributed for profit 0) commercial advan-11gc and thet \ncopias bear thb notla and the lull citation on tha first page. To cop otherwisb. to rapublbh, to ..rv.rs \no, to podon rediatributa to liste. require prior specific pwmission and/or . 1-e. SIQPLAN 98 Montreal. \nCan.da 8 is98 ACM o-89791-987-4/98/000...(6.00  structures. Our framework includes: (1) code move- ment \nto issue remote reads earlier and writes later, (2) code transformations to replace repeated/redundant \nre- mote accesses with one access, and (3) transformations to block or pipeline a group of remote requests \ntogether. Our framework consists of an analysis phase that deter- mines where it is safe to move communication! \nand a transformation phase that selects the best location and type of communication primitives. To test \nour approach we implemented the techniques in the EARTH-McCAT C compiler [12], and we exper- imented \nwith a collection of pointer-based benchmarks that were written in EARTH-C, a high-level parallel di-alect \nof C. The main goal of the EARTH-C project is to provide a high-level parallel language that exposes \ncoarse-grain parallelism and data locality to the pro-grammer, but hides all the details of communication \nand thread generation. Thus, it is the EARTH-McCAT compiler s job to insert the appropriate communication \nprimitives and to generate appropriate threads and syn- chronization. This paper focuses on how to optimize \nthe placement and type of communication. In order to present relatively simple and efficient al-gorithms \nwe take advantage of the structured nature of the McCAT SIMPLE representation [19], and the pres- ence \nof accurate read/write sets for both stack and heap- directed pointers 8, 111. However, the general ideas \nshould be applica b le to other compilers that support pointer analysis and read/write sets for indirect \nrefer-ences. We present experimental measurements for the EARTH-MANNA distributed-memory parallel architec-ture \n[13], and compare the performance of benchmarks with and without communication optimization. The remainder \nof the paper is organized as follows. Section 2 gives the essential background to the EARTH- C language \nand the EARTH-McCAT compiler. Section 3 provides some small examples to motivate the com-munication optimizations, \nand Section 4 presents the analysis itself. In Section 5 we provide experimental results, and illustrative \nexamples., for our set of bench- mark programs. Finally, in Section 6 we discuss some related work, and \nin Section 7 we give conclusions and some suggestions on further work. 2 Background The EARTH-McCAT compiler \nhas been designed to ac- cept a high-level parallel C language called EARTH-C, and to produce a low-level \nthreaded-C program that can be executed on the EARTH-MANNA parallel ar-chitecture. In this section we \nprovide an overview of the important points about the language and compiler WI. 2.1 The EARTH-C Language \nThe EARTH-C language has been designed with simple extensions to C. These extensions can be used: to \nex-press parallelism via parallel statement sequences and a general type of forall loop; to express concurrent \naccess via shared variables; and to express data locality via data declarations of local pointers. Any \nsequential C program is a valid EARTH-C program, and the compiler automatically produces a correct low-level \nthreaded pro-gram. Usually the programmer uses the EARTH-C con-structs to expose coarse-grain parallelism, \nand to add some information about data locality. The compiler per-forms analysis to infer additional \nlocality[22], to expose fine-grain parallelism via data dependence analysis, and to reduce communication \n(the topic of this paper). Two sample list processing functions, written in EARTH-C, are given in Figure \n1. In both cases the functions take a pointer to a list head, and a pointer to a node x, and return the \nnumber of times x occurs in the list. Figure l(a) uses a forall loop to indicate that all iterations \nof the loop body may be performed in paral- lel. Since a f oral1 loop must not have any loop-carried \ndependences on ordinary variables, we have used the shared variable count to accumulate the counts. Shared \nvariables must always be accessed via atomic functions and in this case we have used the built-in functions \nwriteto, addto and valueof. Figure l(b) presents an alternative solution usin recursion. In this example \nwe use a parallel sequence Tdenoted using (^ . . . -I), to indicate that the call to equalnode and the \nrecursive call to countlec can be performed in parallel. int count(node *head, node *x) { shared int \ncount; node *p; nriteto(&#38;count,O); forall(p=head; p != BULL; p=p->next) if (equal-node(p,x)(POWlVER-OF(p)) \naddto(&#38;count,l); return(valueof(&#38;count)); 1 int equal-nodecnode local *p, node *q) C returncp->ve.lue \n==q->value); 1 (a) iterative solution int count-reccnode *head, node *x); { node *next; int cl, ~2; if \n(head != NULL) { {-cl = equal-node(head,x)QOWPEll-OF(x); c2 = count-recchead->next ,x1; 1 return(cl+c2); \n> else return(O); 1 int equal-nodebode *p, node local *q) { returncp->value== q->value); 1 (b) recursive \nsolution Figure 1: Example functions written in EARTH-C As the target architecture for EARTH is a distributed-memory \nmachine, the distinction between local memory accesses and remote memory accesses is very important. \nLocal memory accesses are expressed in the generated lower-level threaded-C program as or- dinary C variables \nthat are handled efficiently, and they may be assigned to registers or stored in the local data cache. \nHowever, remote memory references must be re- solved by calls to the underlying EARTH runtime sys-tem. \nThus, for remote memory accesses, there is the ad- ditional cost of the call to the appropriate EARTH \nprim-itive operation, plus the cost of accessing the communi- cation network. In compiling the EARTH-C \nlanguage the compiler can assume that all direct (non-pointer) references to parameters and locally-scoped \nvariables are local references. In contrast, unless further infor-mation is known, the compiler must \nassume that all indirect (via a pointer) references and all references to global variables are remote. \nIn our program examples we underline all remote references. Explicit local pointer declarations, and \nlocal-ity analysis can be used to provide extra infor-mation to the compiler. In Figure l(b) the call \nto equalnode(head,x) is specified to occur at the OWNER-OF(x). This means that within the body of equalnode, \nthe second parameter can be assumed to be a local pointer. This locality can be exposed either through \nexplicit local declarations, or automatically via locality analysis [22].  2.2 Memory Model An important \npoint about the EARTH-C language is that parallel computations expressed via parallel state-ment sequences \nor forall loops, may not interfere ex-cept on explicit shared variables. It is the program-mer s responsibility \nto ensure this non-interference for explicit parallel constructs. The compiler ensures this non-interference \nfor all parallelizing transformations. Shared variables are explicitly declared using the key- word shared, \nand they may be any type (scalars, point- ers, arrays or structs). Shared variables must be ac-cessed \nvia atomic operations. The compiler is allowed to reorder shared variable references within a thread, \nas long as data dependences are maintained. Thus, shared variables are handled with kind of weak consis- \ntency model where each thread sees its own writes in an order that obeys data dependences. However, since \nin- dependent writes can be reordered, other threads may see those writes in a different order from which \nthey appeared in the original program. In EARTH-C pro-grams, shared variables are seldom used for large \ndata structures, and are most often used for shared counters, or for shared structure headers. Any variable \nthat is not shared is called an ordinary variable and most memory accesses in EARTH-C pro-grams are made \nvia ordinary variables. The program given in Figure l(b) shows one case of parallel compu-tations on \nlarge dynamic data structures. Since the sub- computations are only reading the data structure they do \nnot interfere, thus we do not need shared variables. Another typical example of non-interfering parallelism \noccurs when two or more recursive calls can be done in parallel on independent pieces of a large data \nstructure. Execution of an EARTH-C program can lead to many concurrent threads. The memory model interacts \nwith concurrency as follows: l A local memory reference in some thread T must access memory that is local \nto the processor ex-ecuting T, and no other concurrent thread may interfere with this memory location \n(i.e. if one thread writes to the location, no other concurrent thread can read or write the location). \nLocal mem-ory references are inexpensive and they can be al-located in registers or in a local cache. \nThey arise from references to locally-scoped variables, param-eters, and accesses via local pointers. \n . A shared remote memory reference in some thread T accesses memory that may be on another pro-cessor. \nOther threads may concurrently access the same memory location, but all accesses to this location must \nbe done via explicit shared mem-ory operations, which guarantee atomicity. Shared memory references are \nthe most expensive sort of reference, and they are typically used sparingly. l An ordinary remote memory \nreference in some thread T accesses memory that may be on an-other processor. No other concurrent thread \nT may interfere with this memory location (i.e. if one thread writes to the location, no other con-current \nthread can read or write the location). Ordinary remote memory references are compiled to low-level EARTH \nprimitives, and are consider-ably more expensive than local memory references. They arise from accesses \nvia global variables and non-local pointers. This paper focuses on the communication optimiza-tion of \nordinary remote memory references that are made via pointers. For example, the remote memory references \nunderlined in Figure 1 would be potential tar-gets for our communication optimization. 2.3 The EARTH-McCAT \nC Compiler This paper builds upon the existing EARTH-McCAT C compiler. The overall structure of the compiler \nis given in Figure 2. The compiler is split into three phases. Phase I contains our standard transformations \nand anal-yses. The methods presented in this paper are found in Phase II. We use the results of the analyses \nfrom Phase I in order to transform the SIMPLE program representa-tion into a semantically-equivalent \nprogram. Phase III takes the transformed SIMPLE program from Phase II, generates threads, and produces \nthe target threaded-C code that can be run on the EARTH-MANNA architec-ture. 3 Motivating Examples In \nthis section we present two small program examples that motivate the types of communication optimizations \nthat we consider. These examples also illustrate the var-ious tradeoffs that need to be considered when \napplying the communication optimizations. Let us first consider the simple function distance given in \nFigure 3(a). This function takes a pointer to a structure Point, and returns the distance of the pointfrom \nthe origin. As no special locality information is given for the parameter p, the compiler must assume \nthat each indirect reference via p is potentially a re-mote operation. Each program example shows remote \noperations underlined, and in Figure 3(a), it is clear EAFTH-C I ,.......A. A.... .,A.,,, A., .. ,, , \n..$ Slmpllty .... Dl..lICE I I IrT L I Goto-Ellmtnatton B] local Fwctlon tnllntng Points-toAnalyet \nHeap Analysis If/WSetAnalysis Transformations) Army Dependence Tester SIMPLE-C . . . . ..AA... ..,..... \nA ............. . Locality Anatydr i PHASE II Communication Analysis i (Locality and Communication Enhancement) \nt THREADED-C Figure 2: Overall structure of the compiler that there are four remote reads via p. In \norder to ap-ply the communication optimizations, each program is first simplified so that each statement \nhas at most one remote read or write. Figure 3(b) shows the simplified version of distance. One goal \nof communication optimization is to move the reads earlier in the program. For example, the four remote \nreads in Figure 3(b) can be moved to the begin-ning of the function. Moving remote reads earlier has \nseveral advantages. First, because the remote opera-tions are split-phase, by issuing the remote read \nas early as possible, one allows communication to overlap with the computation following the read. Secondly, \nmoving remote reads can also expose some opportunities for dis-covering redundant communication. Figure \n3(c) shows that two remote reads are all that are necessary. Fi-nally, by moving remote reads together, \ncommunication may be pipelined or blocked. Figure 3(d) shows one such transformation for blocking. In \nthis case the en-tire structure is moved to a local struct (bconunl) by one remote operation (blkmov), \nand then local reads are made with respect to the local struct. The choice of whether to use blocked \noperations depends on two factors: (1) the relative cost of pipelined scalar reads vs. block reads, and \n(2) the extra overhead of reading spurious fields of a struct (i.e. the fields required may not be contiguous \nin memory). Figure 4 illustrates an example where both remote reads and remote writes are optimized. \nNote that the two remote reads are moved earlier in the program, whereas the two remote writes are moved \nlater in the program. In the case of remote reads, moving the oper-ations earlier in the program has \ntwo advantages. First, it allows overlapping communication with computation, and further it exposes opportunities \nfor pipelined and blocked communication. Thus, it is always a good idea to move remote reads earlier. \nHowever, in the case of remote writes there are two conflicting goals. Moving remote writes earlier may \nimprove the overlap of com-munication and computation, but moving remote writes double distance(Point \n*p) { double dist-p; dist-p = sqrt((p->x * p-)x) + (p->y * p->y)); returncdist-p) ; - 1 (a) original \nC code double distance(Point *p) ( double dist-p; double tempi, temp2, . . . , temp7; tempi = p-)x ; \ntemp2 = p-)x ; temp3 = templ * temp2; - temp4 = p->y ; temp5 = p->y ; tempd = temp( * tempS; - temp7 \n5 temp3 + temp6; dist-p = sqrt(temp7) ; return(dist,p); ) (b) SIMPLE C code double distance(Point *p) \n{ double dist-p; double temp3, temp6, temp7; double cosuni, comm2; comml = p->x ; comm2 = p->y ; temp3 \n= conunl * comml; tempd = comml * comml; temp7 = temp3 + temp6; dist-p = sqrt(temp7) ; return(dist,p); \n 1 (c) Collecting and Moving Reads double distance(Point *p) { double dist-p; double temp3, temp6, temp7; \nPoint bcomml ; blkmov(p,lbcomml,sizeof(Point.)) ; temp3 = bcomm1.x * bcomm1.x; temp6 = bcomm1.y * bcomm1.y; \ntemp7 = tempt + temp6; dist-pl + sqrt(temp7); return(temp7) ; 1 (d) Blocking Reads Figure 3: Optimizing \nremote reads later may expose opportunities for blocking. Figure 4(c) shows the remote writes moved later, \nand this is a good idea if the blocking transformation in Figure 4(d) is per- formed. However, it is \na bad idea if blocking should not be performed. Thus, we can see that there is a fine bal- ance between \nthe placement of remote write operations and the blocking transformations. The two examples presented \nin this section show two simple applications of the communication optimiza-tions. In general, one needs \nmore sophisticated analyses that determine when it is safe to move communication, where to place the \ncommunication, and where to apply blocking. 4 Communication Optimizations In this section, we describe \nthe algorithm for commu-nication optimization. It consists of an analysis phase followed by a transformation \nphase. The analysis phase, called possible-placement analysis, collects the set of re- mote reads and \nwrites that can possibly be placed at each program point. The transformation phase, called communication \nselection, picks the best location for int scale-point(Point *p. double k) { p-)x = scale( p-)x ,k); \n = scale( p-)y ,k); >-p->u (a) original C code int scale-point(Point *p, double k) { double templ, temp?, \ntemp3, temp4; templ = p-h ; temp2 -scale(t.empl,k); p->x = temp2; ten\\p3 = p->y ; temp4 = sx(temp3,k) \n; p )y = temp4; ) (b) SIMPLE C code int scale-point(Point *p, double k) { double templ. templ; double \ncomml. comm2; comml = p-)x ; comm2 = p->y ; temp2 = scale(conmnl,k); temp4 = scale(connn2,k); p->x = \ntemp2; p-)Y = templ; 1- (c) Collecting and Moving Reads/Writes int scale-point(Point *p, double k) { \ndouble temp2, temp4; Point bcomml ; blkmov(p,kbcomml,sizeof(Point)) ; temp2 = scale(bcomml.x,k); = scale(bcomml.y,k)* \nbtll%v~v(lrbconunI,p,sizeof(Pdint)) ; 1 (d) Blocking Reads/Writes Figure 4: Optimizing remote reads/writes \neach remote read/write and performs the appropriate transformation, applying blocking when applicable. \nThe algorithm operates on SIMPLE, a compositional intermediate representation for C programs [19 . SIM-PLE \nprograms are composed of basic statements l assign-ments and function calls , statement sequences, con-ditionals \n(if and switch , and loops (while, do, and I for). Each basic statement has a unique label, and can have \nat most one remote operation (remote read or remote write). Basic statements that involve remote op- \nerations are called remote basic statements, those with- out remote operations are called local basic \nstatements. There is no irregular flow of control since SIMPLE pro-grams have been automatically structured \nusing goto-elimination [9]. Thus, we give our analysis rules in a structured form. The algorithm makes \nuse of advanced side-effect analysis. Each basic and compound statement is dec- orated with the set of \nlocations read/written. In addi-tion, statements involving pointers to the heap are dec- orated with \nheap read/write sets that have been com-puted using connection analysis [lo, 111. 4.1 Possible-Placement \nAnalysis Possible-placement analysis collects sets of remote com-munication expressions. Each remote \ncommunication expression is a 4-tuple (p, f, n, Dlist) where p is a pointer variable, f is the field \nof a struct, n is an es-timated frequency, and Dlist is a set of basic state-ment labels. For convenience \nwe sometimes write these tuples combining the first two components, so that (p, f, n, Dlist) is sometimes \nwritten as (p+f, n, Dlist). Possible-placement analysis computes two sets Re-moteReads and RemoteWrites, \ndefined as follows: RemoteReads( the set of remote reads that may be safely placed just before statement \nS; RemoteWrites( th e set of remote writes that may be safely placed just after statement S. These \nsets are collected using structured analyses. RemoteReads are propagated via a backwards analy-sis, whereas \nRemoteWrites are propagated via a for-wards analysis. Each analysis is completed with a sin- gle traversal \nof the structured SIMPLE representation, no iteration is required. The structured analysis rules are \ngiven in Figures 5 and 6. The first rule in Figure 5 shows the driving rule, col-IectCommSet, that selects \nthe appropriate rule depend-ing on the kind of the statement. The simplest rule, for basic statements, \nis collectCommSetBasic. A basic remote statement of the form S; : lhs = p--+f generates the RemoteRead \ntuple (p+f, 1, Si). A statement of the form Si : p+f = rhs generates a RemoteWrite tuple of the form \n(p+f, 1, Si). The last two rules in Figure 5, collectCommReadsSeq and collectCommWritesSeq, demonstrate \nhow the tuples are propagated through statement sequences. Let us first focus on collectComReadsSeq, \nthe rule for propa-gating RemoteReads earlier in the program. In this case RemoteRead tuples are propagated \nfrom S,, to Si. At each step i, the RemoteRead set has already been calcu- lated for the program point \njust before Si (currcomm-ReadSet . Statement Si-i is analyzed to find the set of Remote A eads tuples \nthat it generates (predCommRead-Set), and then all tuples from currCommReadSet that are not killed by \nSi-i are added to predCommReadSet, giving the set of RemoteReads valid at the program point just before \nSi-1. Determining the kill set is actually the tricky part of the algorithm and requires relatively de-tailed \nside-effect analysis. It is obvious that if Si writes to pointer p (either directly or via an alias), \nthen p is no longer pointing to the same structure, and all tu-ples with p as the first item must be \nkilled. However, one must also kill all tuples of the form (p-f, *, *) if 5 ; writes to p+f via an alias. \nNote that a direct write via p+f should not be killed because in the best case we want to replace all \nreads and writes via pointer p with accesses to a local structure. We use our connec-tion analysis with \nanchor handles to distinguish between direct reads and aliased reads [ 1 I]. The rule for propagating \nRemoteWrites forwards, col-IectCommWritesSeq, is similar to the previous function. In this case the statement \nsequence is processed from Si to S,,. The other crucial difference is that tuples of the form (p+f, *, \n*) must be killed if Si reads or writes p+f via an alias. To demonstrate the backward propagation of \nRe-moteReads, consider the example program in Figure 7. This program traverses a list pointed to by p, \nlooking for a point that is within epsilon distance to the point pointed to by t. The last such point \nis pointed to by close. After the loop the differences between the x and y fields oft and close are computed. \nThe backward propagation proceeds as follows. First, consider the outer statement sequence Sl; ,,,; S8. \nThe analysis proceeds starting with S8 which has no remote accesses, and therefore generates the empty \nset. Statement S7 generates (t+y, 1, S7), statement 5 6 generates (close+y, 1, S6), and so on until state- \nment S3 where the set {(t+y, 1, S7), (close+y, I, S6), (t+~, 1, S4), (close+, 1, S3)) has been calculated. \nFigure 6 gives the rules for handling loops and con-ditionals. In the case of if and switch statements \neach alternative is analyzed to give the sets of RemoteReads or RemoteWrites generated by each alternative. \nFor the case of RemoteReads analysis, all tuples from all alter-natives are propagated out of the conditionals. \nThis is because we are optimistically propagating reads as early as possible, since it is safe to read \nspurious fields, and never use their value. However, we must be more con- servative since it is certainly \nnot safe to include spurious writes. Thus, for RemoteWrites we only include tuples that occur in all \nalternatives. To reflect the conditional nature of the control flow, we adjust the frequencies in the \ntuples. Our simple scheme simply divides the cur-rent frequency by the number of alternatives, although \nclearly either static or dynamic branch prediction in-formation would be very valuable here. When moving \ntuples out of conditionals, tuples referring to the same location are merged by summing their adjusted \nfrequen- cies and taking the union of their definition sets. Loop statements are handled by first analyzing \nthe body of loop giving the RemoteReads/RemoteWrites valid for the top bottom of the loop body. We then \npropagate all tup i es that can not be killed by the loop. Tuples propagated outside of the loop have \ntheir fre-quencies increased, corresponding to the expected num-ber of times the loop will execute. To \ndemonstrate RemoteReads analysis for loops, con-sider the example in Figure 7, statement S2. The analysis \nof the body (statements S9.. S15) results in the RemoteReads set {(p-web, 1, S15), (t+y, 1! S12), (t-kc, \n1, Sll), (p+y, 1, SlO), (p-u, 1, SS)}. Since p is written in the body, all tuples with p must be killed, \nand only {(t-y, 1, S12), (t+c, 1, Sll)} are gen- erated by the loop. The loop also writes the pointer \nclose, and so it kills the tuples (close+y, 1, S6) and (cZose+z, 1, S3) that were valid at S3. Combining \nthe remaining tuples at S3, {(t+z, 1, S4), (t+y, 1, S7)}, with the frequency-adjusted tuples generated \nby the loop {(t+~, 10, Sll), (t+y, 10, S12)}, ives the set {(t+z, 11, Sll : S4), (t+y, 11, S12 : S7)T \nat S2. This set propagates to Sl as well. 4.2 Communication Selection After possible-placement analysis, \neach statement is as- sociated with a set of remote communication expres-sions (RCEs), which can be safely \nplaced before/after the statement. Based on this information we have de- veloped some heuristics for \nplacing the communications, and for selecting the correct kind of communications. In compiling with split-phase \nremote memory oper-ations it is usually beneficial to place these operations as early ae possible. This \nallows some overlap of com- munication with computation. Thus, we follow an ear- liest placement policy \nfor placing remote reads. This is lBy moving tuples out of conditionals and loops we may move a pointer \ndereference to a spot in the program that will cause a deref-erence that would not have occurred in the \noriginal program. In the transformation stage we ensure that it is safe to issue such a dereference. \n/* computation of communichtion read expressions for a statement */ fun collectCommSet(stmt, accessType) \n= case (type(stmt)) of <Basic> => return (collectCommSetBasic(stmt, accessType)); <Seq> => if (accessType \n== READ) return(collectCommReadsSeq(stmt)); else /* must be a WRITE */ return(collectCommWritesSeq(stmt)); \n<Loop> => return(collectCommSetLoop(stmt, accessType)); <If> ==> return(collectCommSetIf(stmt, accessType)); \n. . . /* Other stmts, switch...*/ /e collect remote communication expressions for a basic statement \n*/ fun collectCommSetBasic(stmt, accessType) = if ((accessType == READ) &#38;&#38; isRemoteAccess(rhs(stmt))) \nreturn <basevar(rhs(stmt)),field(rhs(stmt)),l,label(stmt)>; else if ((accessType == WRITE) &#38;&#38; \nisRemoteAcess(lhs(stmt))) return <basevar(lhs(stmt)),~eld(lhs(stmt)),I,label(stmt)>; return EMPTY; /e \ncomputation of communication read expressions for a statement * sequence, using a backward propagation \nscheme */ fun colIectCommReadsSeq(stmtSeq) = [sg:s-1] = getStmts(stmtSeq); currCommReadSet = collectCommSet(s-n, \nREAD); foreach stmt s-i in [s n:s-21 /* bac k ward propagation from s-n to s-1 */ j = i -1; /e subscript \nfor the predecessor statement in the sequence */ predCommReadSet = collectCommSet(sj, READ); foreach \ncommExpr (p,f,n,d) in currCommReadSet if (varWritten(p, sj)) /* base variable p itself is written */ \ncontinue; /* commExpr cannot be propagated above sj */ if (accessedViaAlias(p, f, d, sj, WRITE)) continue; \n/* p->j possibly written via an alias, say t->j: can t be propagated *I addToSet((p,f,n,d), PredCommReadSet); \n/* propagate commExpr to sj */ CurrCommReadSet = predCommReadSet; return(commReadSet(s-1)); /* computation \nof communication write expressions for a statement * sequence, using a forward propagation scheme */ \nfun collectCommWritesSeq(stmtSeq) = [s-l:sp] = getStmts(stmtSeq); currCommWriteSet = collectCommSet(s-1, \nWRITE); foreach stmt s-i in [s l:s-n-l] /* f orword propagation from s-1 to s-n */ j = i + 1; /* subsc:pt \nfor the successor statement in the sequence */ succCommWriteSet = collectCommSet(sj, WRITE); foreach \ncommExpr (p,f,n,d) in CurrCommWriteSet if (varWritten(p, sj)) /* base variable p itself is written */ \ncontinue; /* commExpr cannot be propagated above sj */ if ( accessedViaAlias(p, f, d, sj, READ ) 11 accessedViaAlias(p, \nf, d, sj, WRITE ) ) continue; /* p-->j possibly written via an alias, say t->j: can t be propagated */ \naddToSet((p,f,n,d), succCommWriteSet); /* propagate commExpr to sj */ currCommWriteSet = succCommWriteSet; \nreturn(commWriteSet(sg)); Figure 5: Possible Placement Analysis -Basic Rules achieved via a top-down \ntraversal of the SIMPLE repre-(p-+ f) n, Dlist), a decision is made whether or not to to sentation. At \nthe beginning of the traversal a hash table insert, the remote read at this point. If the frequency is \ngenerated that is used to contain all remote memory n is 1 or more and it is safe to place a dereference \nto operations that have already been selected. Each re-p at this point in the program, then this is a \ncandidate mote memory operation is a triple (p, f, d , also written for inclusion.2 If the tuple is a \ncandidate, then an entry Wf, d , where p is a pointer, f is a Aeld, and d is the labe 1of the statement \ncontaining the memory refer- There are several ways of ensuring that the dereference is valid. One method \nis to check that there exists some dereference to p on ence. Initially the hash table is empty. At each \nstate- all program paths starting at S;. Another method is to use a nilnessment, Si, the RemoteReads \nset is examined, and any analysis to determine which pointers are definitely not nil. In our run-tuples \nalready in the hash table are removed from the time system we also have the option of issuing a remote \noperation to a RemoteReads set. For each remaining access of the form potentially-invalid address. In \nthis case one could speculatively issue the remote operation, even for an invalid address. /NQ computation \nof communication read/write sets for an if stmt : the return C+CcommSet consists of commExprs that can \nbe moved out of the if stmt */ fun collectCommSetIf(iBtmt, accessType) = commIfSet = EMPTY; commThenSet \n= collectCommSet(ifStmt.thenpart, accessType); commElseSet = collectCommSet(ifStmt.elsepart, accessType); \nif (accessType == READ) foreach commExpr (p,f,n,d) in commThenSet /* merge then set */ n = adjustFrequency(n, \nifStmt); addToSet((p,f,n,d), commIfSet); for-each commExpr (p,f,n,d) in commElseSet /* merge else set \n*/ n = adjustFrequency(n, ifStmt); addToSet((p,f,n,d), commIfSet); return (commIfSet); else /* if (accessType \n== WRITE) =N/ foreach commExpr (p,f,n,d) in commThenSet  if ((p,f,nl,dl) exists in commElseSet ) /* \nSame field expression is written in else part */ n = adjustFrequency(n, ifStmt); addToSet((p,f,n,d), \ncommIfSet); nl = adjustFrequency(n1, ifStmt); addToSet((p,f,nl,dl), commIfSet); return (commIfSet); /* \ncommExprs that can be moved below the if stmt */ /* computation of communication readfwrite sets for \na loop: the return * commSet consists of commExprs that can be moved out of the loop */ fun collectCommSetLoop(loopStmt, \naccessType) = commLoopSet = EMPTY; commBodySet = collectCommSet(loopStmt.body, accessType); if (accessType \n== READ) foreach commExpr (p,f,n,d) in commBodySet if (varWritten(p, IoopStmt) 11 accessedViaAlias(p, \nf, d, IoopStmt, WRITE)) continue; /* cannot move it above the loop */ n = adjustFrequency(n, IoopStmt); \naddToSet((p,f,n,d), commloopset); eke /* if (accessType == WRITE) */ if (executesOnce(loopStmt)) foreach \ncommExpr (p,f,n,d) in commBodySet  if (varWritten(p, IoopStmt) 11 accessedViaAlias(p, f, d, IoopStmt, \nREAD) 11 accessedViaAlias(p, f, d, IoopStmt, WRITE)) continue; /* cannot move it below the loop */ n \n= adjustFrequency(n, loopstmt); addToSet((p,f,n,d), commLoopSet); return(commLoopSet); /* commExpr8 \nthat can be moved above/below the loop */ fun adjustFrequency(freq, stmt) = if (isLoopStmt(stmt)) return \n(freq * 10); /* moving commh pr out of a loop */ if (isIfStmt(stmt)) r&#38;Urn (freq / 2); /* moving \ncommExpr out of an if statement */ if (isSwitchStmt(stmt)) n = numberOfCaseStmts(stmt); / + number of \ncase stmts involved *f return (freq / n); /* moving commExpr out of a switch statement */ return (freq); \n/* return original value */ Figure 6: Possible Placement Analysis -Compound Rules for each statement \nlabel in Dlist is made into the hash cost model based on the number of words needed, and table. the size \nof the block to determine the best choice. Note After all the candidates at a program point are se- that \nif a pointer p is only used for reads, then a block lected we then determine if some of them should be \nmove is safe, even if we read spurious fields. blocked. This choice depends upon the target architec-Consider \nthe example in Figure 8(a) (the same pro- ture. For example, for our target architecture, pipelin-gram \nas used in Figure 7 to illustrate possible-placement ing is better for two remote accesses, but blocked \ncom-analysis). In placing the communication, the program is munication is better for three or more accesses. \nIf the processed starting at statement 5 1. At this point there structure being read is very large compared \nto the num- are two entries for pointer t , each with a frequency count ber of fields actually required, \nthen the tradeoff shifts of 11, and it is safe to dereference t at this point. Thus, slightly towards \npipelined communication. We use a both of these tuples are selected as candidates. Since SI: p = head; \n-((t-%z,ll,Sll : S4),(t+y, 11,S12 : S7)) SZ: while (p!=IiULL) {(t-+z,ll,Sll : S4),(t-+Y, ll,S12: S7)) \nS9: { ax = p-)x ; {(p~ne~t,1,S15),(t-fY,1,S12),(t-t~,l,Sl1),(~~Y,l,Sl~),(~--f~,l,S~)~ Sl 0: ay = p-)y \n; {(p~ne~t,1,S15),(t-fy,1,S12),(t~~,l,Sl~),(p~Y,l,Sl~)) Sll: bx=t-)x; Sl.?: s13: dist = f(ax,ay,bx,by); \n{(p+next,l,s15)} s14: if (dist < epsilon) close = p; {(p+nezt,l,S15)} s15: p = p->next ; {(p+nezt, 1, \nS15)) I S8: diffy = cy -ty; {} Figure 7: Example propagation there are only two reads, they are pipelined \n(as shown in Figure 8(b)). Entries for (t-+x, Sll), (t--+x, S4), (t-+y,S12), and (t+y,S7) are made in \nthe hash table. At, statement S2 there will be no remaining remote tu-ples (after removing all entries \nin the hash table), and so nothing is done. At statement S9 there are three remaining tuples! all referencing \npointer p, and thus a blocked commumcation is selected. For statements SlO to S15 there are no remaining \ntuples, and the next, non- empty list is at statement S3. At statement S3 there are two pipelined reads \nvia pointer close. Handling the insertion of RemoteWrites is somewhat more difficult. In this case we \nmust balance the cost of delaying some writes in order to expose opportunities for blocked writes. Further, \nit is not safe to write spu- rious fields in a block move. We handle these situations by propagating \nspecial RemoteFill tuples using the same algorithm as for RemoteReads. These tuples ensure that all fields \nin a struct will be read before a blocked write is inserted. 5 Experimental Results In order to evaluate \nour approach, we have used EARTH-MANNA distributed-memory parallel system as our target architecture. \nIn this section, we provide a brief description of the architecture, and the experi- mental results we \nobtained. 5.1 The EARTH-MANNA Architecture In the EARTH model, a multiprocessor consists of mul- tiple \nEARTH nodes and an interconnection network [13, 171. As illustrated in Figure 9, each EARTH node consists \nof an Execution Unit (EU) and a Synchroniza- tion Unit (SU), linked together by buffers. The SU and EU \nshare a local memory, which is part of a distributed memory architecture in which the aggregate of the \nlocal memories of all the nodes represents a global memory address space. The EU processes instructions \nin an active thread, where an active thread is initiated for execution when the EU fetches its thread \nid from the ready queue. The EU executes a thread to completion before moving to another thread. It interacts \nwith the SU and the network by placing messages in the event queue. The SU fetches these messages, plus \nmessages coming from of RemoteRead Sets remote processors via the network. The SU responds to remote \nsynchronization commands and requests for data, and also determines which threads are to be run and adds \ntheir thread ids to the ready queue. Memory operations are supported as split-phase transactions -computation \nsynchronizes on the com-pletion of a remote access, not on the issue of the OF- eration. EARTH-MANNA \nsupports split-phase priml-tives for reading/writing scalar data fields (char, integer, float, double), \nand also primitives for reading/writing , blocks of data. Local Memory Network Figure 9: The EARTH architecture \nOur experiments have been done using the EARTH- MANNA parallel machine[3]. Each MANNA node con- sists \nof two Intel i860 XP CPUs, clocked at 50MHz, 32MB of dynamic RAM and a bidirectional network in-terface \ncapable of transferring SOMB/S in each direc- tion. The two processors on iach node are mapped to the \nEARTH EU and SU. The EARTH runtime system supports efficient re-mote operations. Typically, the cost \ndepends on the location of the data that is referenced. Table I shows the cost of communication in two \nextreme cases on EARTH-MANNA, Sequential and Pipelined. The se-quential value indicates how long it takes \nto perform the complete operation, including context switching. In the pipelined case, operations are \nissued as fast as pos- sible, without the need to synchronize before issuing the next operation. Obviously, \nthe pipelined numbers are lower, as the EU, SU and network can all work in parallel. Therefore it is \nalways better to pipeline these operations. The trade-off between pipelining and block- moving the fields \nthat are allocated together depends on the size of data. Even though a block-move instruction Sf :comml \n=t-)x ; {(t+z,ll,Sll : S4),(My, 11,512 : S7)) Sl :comm2 = t->y ; Sl: p = head; Sl: p = head7 S2: while \n(p!=lULL) S2: while (p!=NLL) S9 : { blkmov(p,tbcomml,sizeof(POI~T) ; s9: ax = bcomm1.x; SlO: = bcomm1.y; \n Slf: bx =t-)x ; Sll: 2 = comml; S12: by = t->y ; S12: by = comm2; s13: dist flax,ay,bx,by); s13: dist \n= f(ax,ay,bx,by); 514: if (dist < epsilon) close = p; s14: if (dist < epsilon) close = p; s15: p = p->next \n; s15: p = bcomml.next; ) s3 :comm3 = close->x ;c ose)-ty, 1 S6), (close+z, 1, S3)) s3 :comm4 = close->y \n;s3 !! l= close-ix ; s4: tx =t-)x; s3: cx = comm3; S5: diffx = cx -tx; q: tx = commi; S6: cy = close->y \n; S5: diffx = cx -tx; S6: cy = comm4; 57: ty = t->y ; s7: ty = comm2; S8: diffy = cy -ty; S8: diffy = \ncy -ty; (a) selected RemoteRead sets (b) after transformation Figure 8: Applying Communication Selection \ncate the proportion that are read-data, write-data and [ EARTH Sequential 1 Pipelined blkmovs.3 It is \nclear that in all cases the total num- Operation Remote Remote ber of communication operations reduces. \nThe number Read word 7109ns 1908ns of read-data and write-data operations reduce because Write word \n6458 ns 1749ns Blkmov word 9700ns 2602 ns of redundant communication elimination and blocking. The number \nof blkmov operations increases because some individual read-data/write-data operations were combined \ninto new blkmov operations. Table I: Cost of communication on EARTH-MANNA is more expensive for one \nword, we have found that a block-move is better when three or more words can be moved together. Thus, \nin our experiments we used a threshold of three to determine when to issue pipelined operations and when \nto block them. 5.2 Experimental results We have experimented with five benchmarks from the Olden suite \n181, described in Table II. All five bench- marks use c/ ynamic data structures (trees and lists). Thus \nthe benchmark suite is suitable to evaluate our communication analysis focused on pointers. The benchmarks \nwere all written in EARTH-C, they use the best data distribution strategy we have discov- ered to date \nfor each benchmark, and they exploit the parallelism available in an efficient way. We performed our \nexperiments using the EARTH-McCAT compiler, comparing the performance with and without commu-nication \noptimizations. We refer to the unoptimized programs as the simple versions, and the optimized pro-grams \nas optimized versions. 5.2.1 Dynamic Counts of Communication Operations Figure 10: Improvement on dynamic \ncommunication In Figure 10, we compare the number of remote com-counts munication operations for the \nsimple and optimized ver-sions. The number under each benchmark name gives 5.2.2 Performance Improvement \nthe total number of communication operations in mil- lions. Each benchmark has two bars, the left bar \nrep- In Table III we provide the performance improvementresents the number of communication operations \nper-achieved for each benchmark, via communication opti-formed by the simple version, normalized to 100. \nThe mization.right bar shows the number of remote operations per- Note that the unoptimiaed versions \ncontain some blkmovs because formed by the optimized version. In each bar we indi- the compiler inserts \nblkmovs for assignments to entire structs. Table II: Benchmark Programs The first data column gives the \nsequential execution time for each benchmark. This time is measured for a purely sequential version of \nthe benchmark running on 1 node of the MANNA machine. In these versions all data is local, there is no \nparallelism., and there are no calls to any EARTH runtime operations. Thus, this is a truly sequential \nprogram, with no extra overhead. The next two columns give the times for the sim-ple and optimized parallel \nversions of the benchmark, for 1, 2, 4, 8 and 16 processors. This is followed by two columns that give \nthe speedup of the simple and optimized versions over the sequential version. Finally, the last column \ngives the performance improvement due to communication analysis. Note that communication analysis gives \nsome performance improvement for all benchmarks. In general the performance improvement increases as \nthe number of processor increases. We dis- cuss each benchmark in more detail below. Power: This benchmark \nimplements the power system optimization problem. It uses a four-level tree structure with different \nbranching widths at each level. Communication optimization achieves up to 6.9% speed-up for this benchmark. \nThe main benefit comes from blocking. It is computation-intensive benchmark, with functions operating \non a particular node of the tree. These functions typically read the fields from a node into scalars \nat different points, perform computa-tion, and write the values back into the field. Since all field \naccesses are with respect to a given node, communication optimization is able to place all the read accesses \nearly, and all the write accesses late in the function. Subsequently, these accesses are blocked, as \nthey correspond to fields within a specific node. An example code fragment from this benchmark is shown \nin Figure 11(a), which illustrates this observation. Perimeter: This benchmark computes the perimeter \nof a quad-tree encoded raster image. The unit square image is recursively divided into four quadrants \nuntil each one has only one point. The tree is then traversed bottom-up to compute the perimeter of each \nquadrant. It is an irregular benchmark, and each computation requires accesses to tree nodes which may \nnot be phys- ically close to each other. Thus it is a communication- intensive benchmark, and the benefits \nof communica- tion optimization are more visible for it. We are able to achieve upto 15% speed-up over \nthe simple version. The main optimization applied for this benchmark is also blocking. An example code \nfragment from perime-ter is shown in Figure 11(b). Here the blkmov replaces three remote read operations. \nThe optimization is ap- plied within a recursive function, where the program spends most of the time. \nHence substantial speed-up is obtained. Health: This benchmark simulates the Colombian health-care system \nusing a 4-way tree. Each village has four child villages, and a village hospital, treating pa-tients \nfrom the villages in the same subtree. At each time step, the tree is traversed, and patients, once as- \nsessed, are either treated or passed up to the parent tree node. The 4-way tree is evenly distributed \namong the processors and only top-level tree nodes have their children spread among different processors. \nThis benchmark has relatively few remote data ac-cesses. Thus the speed-up obtained via communica-tion \noptimization is not as significant. The communi-cation optimizations applicable to this benchmark in-clude \npipelining, and redundancy elimination, as illus- trated via the code fragment taken from it, shown in \nFigure 11(c). Tsp: This benchmark solves the traveling salesman problem using a divide-and-conquer approach \nbased on close-point algorithm. This algorithm first searches a suboptimal tour for each merges subtours \ninto bigger ones. built as a circular linked list sitting on top of the root nodes of subtrees. Similar \nto perimeter, this benchmark is irregular in nature and performs a significant time in data accesses. \nWe are able to obtain upto 11.98% speed-up for it. The benchmark mainly benefits from redundant communi-cation \nelimination and communication pipelining. Voronoi: The voronoi benchmark computes a Voronoi Diagram for \na random set of points. The points are gen- erated and stored in a binary tree, and the algorithm computes \nthe Voronoi Diagrams of the two subtrees re- cursively, and merges them to form the final diagram. The \nmerge phase walks along the convex hull of the two sub-diagrams, alternating between in an irregular \nfashion, so the benchmark spends a significant time in data accesses. Therefore, we obtain upto 15.48% \nspeed- up by communication optimization. This mainly comes from redundant communication elimination and \nblock- ing. 6 Related Work The closest related work is previous research at McGill on techniques to reduce \ncommunication overhead in par- allel EARTH-C programs that use pointer-based dy- namic data structures \n[22, 201. The locality analysis presented in [22] focuses on eliminating pseudo-remote memory operations, \ni.e., op- erations that are assumed to be remote by the com-piler, but actually access the local memory. \nUsing the high-level data distribution information provided by the programmer, this analysis identifies \npointers in the EARTH-C program, that can be declared as local. A Benchmark Sequential Simple Optimized \nSimple Optimized Optimized C EARTH-C EARTH-C Speedup Speedup vs. Simple -u (=I (set) set) (%impr) power \n1 proc 62.4 67.76 66.76 0.92 0.93 1.48 2 procs 35.96 34.41 1.74 1.81 4.31 4 procs 18.58 17.58 3.36 3.55 \n5.38 8 procs 9.77 9.12 6.39 6.84 6.65 16 procs 5.23 4.86 11.93 12.84 7.07 -I tsp 1 proc 23.8 26.95 26.26 \n0.88 0.91 2.56 2 procs 14.02 13.56 1.70 1.76 3.28 4 procs 7.71 7.33 3.09 3.25 4.93 8 procs 4.67 4.29 \n5.10 5.55 8.14 16 procs 3.77 3.32 6.31 7.17 11.93 -I health 1 proc 143.72 144.09 144.05 0.99 0.99 0.03 \n2 procs 84.25 SO.72 1.71 1.78 4.19 4 procs 48.32 44.78 2.97 3.21 7.33 8 procs 42.74 37.69 3.36 3.81 11.82 \n16 procs 33.88 28.84 4.24 4.98 14.88 I perimeter 1 proc 5.45 7.32 6.75 0.74 0.81 7.79 2 procs 3.67 3.35 \n1.49 1.63 8.72 4 procs 2.16 1.94 2.52 2.81 10.19 8 procs 1.12 1 0.98 11 4.87 1 5.56 11 12.50 16 procs \n0.75 I 0.63 II 7.27 I 8.65 II 16.00 - -I I I voronoi 1 proc 3.98 1 7.27 1 6.78 It 0.55 I 0.59 11 6.74 \n1 2 procs 3.23 2.85 1.23 1.40 11.76 4 procs 2.52 2.13 1.58 1.87 15.48 8 procs 1.31 1.17 3.04 3.40 10.69 \n16 procs 1.04 0.88 3.83 4.52 15.38 I Table III: Performance Improvement Results local pointer is assumed \nto be always pointing to lo-programmer, i.e., the communication that cannot be ef- cal memory by the \nEARTH-C compiler, and thus they fectively substituted by computation migration. Instead are not mapped \nto remote memory operations. This of using a software caching scheme which requires run-problem is orthogonal \nto the communication analysis time checks, our analysis decides at compile-time which presented here. \nIn this paper we are reducing the com-pointer dereferences are reused, and which references munication \ncost for accesses that might be remote. can be blocked together. An Interesting point would Tang et. \nal [20] use standard compiler optimiza-be to consider how the software caching scheme could tions like \ncommon subexpression elimination, loop-benefit from our analysis. and location-invariant removal to eliminate \nredundant Another form of communication optimization is pointer dereferences, and examine the effect \nof these prefetching. The most relevant work is compiler-based optimizations on the quality of threaded \ncode produced prefetching for recursive data structures by Luk and by the EARTH-C compiler. The communication \nanal-Mowry [16]. This work was directed towards reduc-ysis presented in this paper can also reduce redundant \ning the memory latency time for superscalar proces-pointer dereferences (for example, when remote mem-sors. \nThe basic idea is to automatically insert prefetch ory accesses are moved out of loops). However, in \nthis instructions for pointer references using three different paper we are also concerned about the \nplacement of the schemes, including a scheme whereby nodes ointed to pointer dereferences, pipelining \nremote operations, and by some pointer p are greedily prefetched for exam- Pblocking of associated remote \noperations. ple, prefetching the nodes pointed to by p + left and Another approach for compilation of \ndynamic data p + right as soon as p + value is accessed. Our anal-structure based applications on distributed \nmemory ma-ysis is not trying to prefetch along a traversal chain, chines was proposed by Carlisle and \nRogers [5, 41. They rather we are concentrating on moving pointer derefer-propose the Olden runtime system, \nthat uses a trade ences early, and on blocking related derefences together off between software caching \nof remote data and com-(i.e. we would read the complete node pointed to by putation migration, depending \non the data distribution p at the same time, we would not speculatively deref-and the amount of communication \nrequired. This de-erence any of those fields.) Further, in our case we are cision is guided, in part, \nby programmer-specified path-concerned with distrubuted memory parallel processors, afinity hints for \nrecursive data structures. If access-and so we have a different cost model. In our case it is ing remote \ndata is chosen, a software caching scheme always a good idea to move accesses early (our results is used \nto minimize the overhead of the communica-are written to memory and not a cache), and, in gen- tion. \nIn our approach, the programmer makes the choice eral, it is too expensive for us to speculatively prefetchof \nwhether or not to migrate computation using ex-pointer references. plicit constructs such as OOWNERd3F \nand @HOME, which For array-based scientific computations, a signif-enable the programmer to invoke functions \nat a given icant amount of work has been done on communi-processor. Therefore, our communication optimization \ncation optimization for distributed memory compila-focuses on further reducing the overhead of communi- \ntion [l, 2,6,21]. The approach proposed by Chakrabarti cation that is considered necessary by the EARTH-C \net. al [6] is the most recent. They propose a global Branch Compute-Branch(br, theta-l, . ...) iP . . \n. blkmov(br,LbcommT, sizeof(branch)); if ((next != 0)) i. . . . bcomm7.D.P = (temp-258 + tamp,259); \n. . . bcomm7.D.Q = (temp-260 + temp-261); 1 else C bcomm7.D.P = tmp.P; bcomm7.D.q = tmp.Q; ) temp-263 \n= bcomm7.R; temp-262 = (temp-263 * temp-263); temp-266 = bcomm7.X; temp-265 = (temp-266 * temp-266); \n bcomm7.alpha = (a / temp-313); b&#38;m7.beta = (b / temp-316); t.mp-317 = bcomm7.D; blkmov(Cbcomm7, \nbr. sizeof(branch)); . . . ) (a) poser int R-sum-adjacent(p. ql. 42, size) t . . . blkmov(p,tbcomm,sizeof(quad~truct.)) \n; temp-110 = bcomm.color; if ((temp-ii0 == 2)) (: switch (91) ( case 0: pi = bcomm.nn; break; case 1: \npi = bcomm.ne; break; . . . 1 switch (q2) { case 0: p2 = bcomm.nu; break; case 1: p2 = bcomm.ne; break; \n . . . x = R,sum,adjacent(pl, ql, q2, size-l); y = R-sum-adjacent(p2, ql, q2, size-l); ) else { temp-112 \n-bcomm.color; if ((temp-112 == 1)) (b)perimeter void check-patients,inside(village, list) i. . . . con11116 \n= (+village).hosp.freepersonnel; while ( (list != 0) ) { D = (*list).natient: commi = (*list).foruard; \nCO,,,, 6 -(*D).timeleft: comms = comms -1; (*p).timeleft = conenS; temp-66 = conenS; if ((temp-66 -= \n0)) { t = comm6; come16= (t + 1); 1 -(&#38;(+village).hosp.inside); R,removeList(l, p); 1 = (t(*village).returned); \n. . . > list = comml; 1 (*village).hosp.freepersonnel = comm6 ) (c)health scheduling approach for communication \noptimization of FSO/HPF programs. Our strategy is similar to their approach in the scheduling aspect. \nOne differ- ence is that they consider only read remote communi- cations, while we optimize both read \nand write remote accesses. Further, we follow the earliest placement pol- icy. Chakrabarti et. al show \nthat in some cases late placement can expose more opportunities for other op- timizations like message \ncombining for arrays. We are studying this interaction in the context of dynamic data structure based \napplications. In the set of benchmarks presented in this paper, we have not found any case where earliest \nplacement policy inhibits other commu- nication optimizations. Agrawal et. al l] indicated that interprocedural \npar- tial redundancy e imination can be beneficial for elimi- I nating redundant communication for array-based \nscien- tific applications. This observation also applies to pro- grams that use dynamic data structures. \nIn our tsp benchmark, one of the pointer parameters passed to the function distance remains invariant \nacross several calls to the function, and all the field accesses with respect to this pointer can be \nplaced before the first call by inter- procedural partial redundancy elimination. Currently, we achieve \nthis effect via function inlining. Krishnamurthy and Yelick [15] present communica-tion optimizations \nin the context of compiling explicitly parallel programs to Split-C programs. Their optimiza-tions include \nmessage pipelining by converting remote read/writes into their split-phase analogues, eliminat-ing acknowledgement \ntraffic, and reusing values from re-mote memory accesses. Their source language includes shared scalar \nvariables and distributed arrays, but does not include global pointers. The emphasis of their work is \nan optimization framework that handles explicitly parallel programs that may read/write the same mem- \nory locations, and thus they must correctly handle in-terfering parallel computations. In contrast, we \nhave a simpler parallel programming model, where our paral-lel threads do not interfere on ordinary remote \nmemory accesses. However, we do allow global pointers, and in fact the focus of our work is optimizing \nprograms using global pointers. Presumably the strengths of these two approaches could be combined to \nhandle both the more general parallel programming model and global point-ers. For shared-memory models, \ntransformations similar to communication optimization, can be used to reduce the synchronization overhead \nof the program [7]. Diniz and Rinard [7 present an approach that focuses on coa- lescing multip 1e critical \nsections that acquire and release the same lock multiple times, into a sin le critical region that acquires \nand releases the lock (i once. on y To this end, they perform lock movement and lock cancellation transformations, \nwhich try to reduce the frquency with which the program acquires and releases locks. These transformations \nare similar in spirit to the redundant communication elimination optimization. The elimination of redundant \ncode has been exam-ined quite rigorously for scalar computations. As an example, Knoop et. al. optimize \nthe computation by optimal code motion [14]. They issue the computation as late as possible to avoid \nunnecessary register pressure while maintaining computational optimality. They fo-cus on scalar variables, \nwhile we are interested in pointer dereferences. Further their algorithms treat each com- putation independently, \nwhile we consider the interac- tion between tuples in determining the best placement, and we consider \nthe costs of different communication strategies to decide between pipelining and blocking. 7 Conclusions \nand Further Work In this paper we have presented a communication anal-ysis framework that is composed \nof possible-placement analysis and communication selection. The fundamen-tal idea is that we wish to \nmove remote read operations as early as possible in order to allow for overlap between communication \nand computation. Further, we wish to pipeline or block remote reads and writes when benefi-cial. We presented \npossible-placement analysis as a struc- tured analysis on the SIMPLE representation of the EARTH-McCAT \ncompiler. In this analysis RemoteRead tuples are propagated optimistically upwards, and Re-motewrite \ntuples are propagated conservatively down-wards. In order to determine when it is safe to prop-agate \ntuples, both stack and heap read/write sets are required. The communication selection transformation \nuses the results of possible-placement analysis. The goal of the analysis is to locate the earliest point \nfor remote reads, and to generate either pipelined or blocked com-munication. For remote writes, the \ncommunication is delayed if this enables blocked communication. We implemented these techniques in the \nEARTH-McCAT compiler, and experimented with a set of five benchmarks These results show that the optimizations \nimprove performance from about 2% to 16% over the unoptimized programs. This work fits into the overall \nframework of the EARTH-McCAT compiler project where we are devel- oping techniques to automatically generate \nlow-level threaded programs from high-level parallel C programs. In this paper we have focused on one \ncomponent -the communication optimization. By adding this compo-nent to the compiler we get one step \ncloser to closing the gap between compiler-generated threaded programs and hand-coded threaded programs. \nOur next step is to study the interaction of commu- nication analysis with locality analysis and the \nthread generation algorithm. We would also like to add tech- niques for finding the best organization \nfor fields within each struct. By placing those fields that are accessed remotely located close to one \nanother, we can further improve the efficiency of the blocked communication. References PI Gagan Agrawal, \nJoel Saltz, and Raja Das. Interprocedural partial redundancy elimination and its application to dis-tributed \nmemory compilation. In Proc. of the ACM SIG-PLAN 95 Conj. on Programming Language Design and Im- plementation, \npages 258-269. PI Saman P. Amarasinghe and Monica S. Lam. Communica- tion optimization and code generation \nfor distributed mem- ory machines. In Proc. of the ACM SIGPLAN 93 Conj. on Programming Language Design \nand Implementation, pages 126-138, Albuquerque, N. Mex., Jun. 1993. U. Bruening, W. K. Giloi, and W. \nSchroeder-Preikschat. La-tency hiding in message-passingarchitectures. In Proc. of the [31 Martin C. \nCarlisle. Olden: Parallelking Programs with Dy-[41 namic Data Structure8 on Distributed-Memory Machines. \nPhD thesis, Princeton University Department of Computer Science, June 1996. Martin C. Carlisle and Anne \nRogers. Software caching and computation migration in Olden. In Proc. of the Fifth ACM SIGPLAN Symp. \non Principles &#38; Practice of Parallel Pro-gramming, pages 29-38, Santa Barbara, Calif., Jul. 1995. \n t51 Soumen Chakrabarti, Manish Gupta, and Jong-Deok Choi. Global communication analysis and optimization. \nIn Proc. of the ACM SIGPLAN 96 Conj. on Programming Lan-guage Design and Implementation, pages 68-78, \nPhiladel-phia, Penn., May 1996. P31 Pedro Diniz and Martin Rinard. Synchronization transfor- 171 mations \nfor parallel computing. In Conj. Rec. of the 24th ACM SIGPLAN-SIGAGT Symp. on Principles of Program-ming \nLanguages, pages 187-200, Paris, France, Jan. 1997. k31 Maryam Emami, Rakesh Ghiya, and Laurie J. Hendren. \nContext-sensitive interprocedural points-to analysis in the presence of function pointers. In Proc. of \nthe ACM SIG-PLAN 94 Conf. on Programming Language Design and Im- plementation , pages 242-256. PI Ana \nM. Erosa and Laurie J. Hendren. Taming control flow: A structured approach to eliminating goto statements. \nIn Proc. of the 1994 Intl. Conj. on Computer Languages, pages 229-240, Toulouse, France, May 1994. Rakesh \nGhiya. Putting pointer analysis to work. PhD thesis, McGill U., MontrCal, Que., Nov. 1997. 1111 Rakesh \nGhiya and Laurie J. Hendren. Putting pointeranaly-sis to work. In Conj. Rec. of the 25th Ann. ACM SIGPLAN-SIGACT \nSymp. on Principles of Programming Languages, pages 121-133, Jan. 1998. I121 Laurie J. Hendren, Xinan \nTang, Yingchun Zhu, Shereen Gho-brial, Guang Ft. Gao, Xun Xue, Haiying Cai, and Pierre Ouel-let. Compiling \nC for the EARTH multithreaded architecture. Intl. J. of Parallel Programming, 25(4):305-337, Aug. 1997. \n[I31 Herbert H. J. Hum, Olivier Maquelin, Kevin B. Theobald, Xinmin Tian, Guang R. Gao, and Laurie J. \nHendren. A study of the EARTH-MANNA multithreaded system. Intl. DOI J. of Parallel Programming, 24(4):319-347, \nAug. 1996. Jens Knoop, Oliver Riithing, and Bernhard Steffen. Opti-mal code motion: Theory and practice. \nACM Trans. on Programming Languages and Systems, 16(4):1117-1155, Jul. 1994. 1141 Arvind Krishnamurthy \nand Katherine Yelick. Optimizing parallel programs with explicit synchronization. In Proc. of the ACM \nSIGPLAN 95 Conj. on Programming Language Design and Implementation , pages 196-204. iI61 Chi-Keung Luk \nand Todd C. Mowry. Compiler-based prefetchingfor recursive data structures. In Proc. of the Seu- enth \nIntl. Conj. on Architectural Support for Programming Languages and Operating Systems, pages 222-233, \nCam-bridge, Mass., Oct. 1996. 1151 Olivier Maquelin, Guang R. Gao, Herbert H. J. Hum, Kevin B. Theobald, \nand Xin-Min Tian. Polling Watchdog: Combining polling and interrupts for efficient message han-dling. \nIn Proc. of the 29rd Ann. Intl. Symp. on Computer Architecture, pages 178-188, Philadelphia, Penn., May \n1996. P71 [18] Anne Rogers, Martin C. Carlisle, John H. Reppy, and Lau-rie J. Hendren. Supporting dynamic \ndata structures on distributed-memory machines. ACM Tran~. on Program-ming Languages and Systems, 17(2):233-263, \nMar. 1995. [19] Bhama Sridharan. An analysis framework for the McCAT compiler. Master s thesis, McGill \nU., Montrhal, Qu&#38;, Sep. 1992. [20] Xinan Tang, Rakesh Ghiya, Laurie J. Hendren, and Guang R. Gao. \nHeap analysis and optimizations for threaded programs. In Proc. of the PACT 97, pages 14-25, San Fran-cisco, \nNov. 1997. North-Holland Pub. Co. [21] Reinhard van Hanxleden and Ken Kennedy. Give-N-Take -a balanced \ncode placement framework. In Proc. of the ACM SIGPLAN 94 Conf. on Programming Language Design and Implementation \n, pages 107-120. 8th Id. Parallel Processing Symp., pages 704-709, Can&#38;n, [22] Yingchun Zhu and Laurie \nHendren. Locality analysis for Mexico, Apr. 1994. IEEE Comp. Sot. parallel C programs. In Proc. of the \nPACT 97, pages 2-13, San Francisco, Nov. 1997. North-Holland Pub. Co.   \n\t\t\t", "proc_id": "277650", "abstract": "This paper presents algorithms for reducing the communication overhead for parallel C programs that use dynamically-allocated data structures. The framework consists of an analysis phase called <i>possible-placement analysis</i>, and a transformation phase called <i>communication selection</i>.The fundamental idea of <i>possible-placement analysis</i> is to find all possible points for insertion of remote memory operations. Remote reads are propagated upwards, whereas remote writes are propagated downwards. Based on the results of the possible-placement analysis, the <i>communication selection</i> transformation selects the \"best\" place for inserting the communication, and determines if pipelining or blocking of communication should be performed.The framework has been implemented in the EARTH-McCAT optimizing/parallelizing C compiler, and experimental results are presented for five pointer-intensive benchmarks running on the EARTH-MANNA distributed-memory parallel architecture. These experiments show that the communication optimization can provide performance improvements of up to 16% over the unoptimized benchmarks.", "authors": [{"name": "Yingchun Zhu", "author_profile_id": "81100415796", "affiliation": "School of Computer Science, McGill University, Montreal, Quebec, Canada H3A 2A7", "person_id": "P305847", "email_address": "", "orcid_id": ""}, {"name": "Laurie J. Hendren", "author_profile_id": "81100646110", "affiliation": "School of Computer Science, McGill University, Montreal, Quebec, Canada H3A 2A7", "person_id": "P169482", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/277650.277723", "year": "1998", "article_id": "277723", "conference": "PLDI", "title": "Communication optimizations for parallel C programs", "url": "http://dl.acm.org/citation.cfm?id=277723"}