{"article_publication_date": "05-01-1998", "fulltext": "\n The Implementation and Evaluation of Fusion and Contraction in Array Languages* E Christopher Lewis \nCalvin Lint Lawrence Snyder University of Washington, Seattle, WA 98195-2350 USA tuniversity of Texas, \nAustin, TX 78712 USA {echris,snyder}@cs.washington.edu, lin@cs.utexas.edu Abstract Array languages such \nas Fortran 90, HPF and ZPL have many benefits in simplifying array-based computations and expressing \ndata parallelism. However, they can suffer large performance penalties because they introduce intermediate \narrays-both at the source level and during the compila-tion process-which increase memory usage and pollute \nthe cache. Most compilers address this problem by simply scalar-izing the array language and relying \non a scalar language compiler to perform loop fusion and array contraction. We instead show that there \nare advantages to performing a form of loop fusion and array contraction at the array level. This paper \ndescribes this approach and explains its advantages. Experimental results show that our scheme typically \nyields runtime improvements of greater than 20% and sometimes up to 400%. In addition, it yields superior \nmemory use when compared against commercial compilers and exhibits com-parable memory use when compared \nwith scalar languages. We also explore the interaction between these transforma-tions and communication \noptimizations. 1 Introduction Array languages such as Fortran 90 (FSO) [l], High Per-formance Fortran \n(HPF) [13] and ZPL [24] have become important vehicles for expressing data parallelism. Though they simplify \nthe specification of array-based calculations, they also present a potential problem: Large temporary \nar-rays may need to be introduced, either by the programmer or by the compiler. For example, the F90 \nstatements in Figure l(a) use temporary array R to cache a computation, while the Fortran 77 equivalent \nin Figure l(b) uses only the scalar variable s, which can be viewed ins a contracted form of the full \narray R. Similarly, there are cases where an array language compiler will insert temporary arrays to \npreserve array language semantics. In both cases, these array tem-poraries increase memory usage, degrade \nperformance by polluting the cache, and therefore impede the acceptance of This research w&#38;s supported \nin part by DARPA Grant E30602-97-I-0152 and NSF Grant CCR-9710284. P.rmis.icn to make digital or hsrd \ncopies of .II or p.rt of this work for p.non.l or cla..rccm us. is grsnted without 1.0 prcvid.d th.t \nccpiss sr. not made or distributed foe profit or commsrcisl sdvsn-mgc .nd th.t copi.. b..r this notic. \nsnd ths full citstion on th. first PWS. To eopy cthsrwisd. to republish. to pat on sswsrs w to rsdistributs \nto lit@. r.quirss prior sp.cific psrmissicn sndlcr s 1.0. SIGPLAN 99 Mcntr..l. Csnsd. 0 1999 ACM 0-99791-9974/99/0006...(6.00 \n array languages, despite their other advantages. There are two ways to solve this problem. The first \nis to scalarize the array language source (i.e., produce scalar loop nests for each array statement) \nand rely on a scalar language compiler to remove the array temporaries using its existing scalar level \noptimizations. Specifically, the scalar compiler must fuse loops to enable contraction, as shown in Figure \nl(b). The second approach is to optimize at the array level prior to scalarization (i.e., perform analyses \nand transformations on array statements directly). Since fusion and contraction are mature and well understood \ntransforma-tions, the first approach appears a natural choice because it simplifies the array language \ncompilation process and lever- ages existing compiler technology. However, we believe that the first \napproach is inferior for several reasons. First, the reality is that scalar languages do not require \nprogrammers to introduce large temporary arrays, so scalar language com-pilers do not bother to perform \ncostly transformations that will not benefit typical human generated code. Second, re-moving array temporaries \nat the scalar level solves the prob- lem at a greater conceptual distance from the source of the problem \nand at a greater cost. Most importantly, it is im- practical to implement an integrated optimization \nstrategy when some optimizations (e.g., communication pipelining) are performed at the array level while \nothers (e.g., array contraction) are subsequently performed at the scalar level. Our work supports earlier \nclaims that there are perfor-mance benefits to performing analyses and transformations at the array level \n[6,21]. In particular, this paper makes the following contributions. We explain how fusion and contrac- \ntion can be performed at the array level. We refer to the former as statement fvsion because array statements, \nnot loops, are the fused entities. We provide empirical evidence that our approach is superior to those \nused by several com-mercial compilers. We measure the benefits of various array level fusion and contraction \nstrategies, both in terms of ex- ecution time and memory usage, and we find that the com-mon practice \nof contracting only compiler introduced arrays is insufficient. In addition, we show that our array level \napproach produces code that is comparable to that of hand- written scalar programs. Finally, we show \nthat performance suffers when compilers do not use an integrated strategy for optimizing communication \nand performing fusion. This paper is organized as follows. Sections 2 and 3 define the representations \nwe use and the problem we solve. Section 4 describes our solution to the problem. Section 5 evaluates \nthe implementation of statement fusion and array contraction in the ZPL compiler, and the final two sections \npresent related work and give conclusions. do j=l.n R(i,:)=AA(i.:)*D(i-1,:) s = AA(ij)*D(i-1,j) D(i,:)=l.O/(DD(i,:)-AA(i-l,:)*R(i,:) \nD(ij)=l/(DD(i,j)-AA(i-l,j)*S Rx(i,:)=Rx(i.:)-Rx(i-l,:)*R(i,:) Rx(i,j)=Rx(i,j)-Rx(i-lj)*s Ry(i.:)=Ry(i.:)-Ry(i-l,:)*R(i.:) \nRy(iJ)=Ry(i.j)-Ry(i-l,j)*s enddo Figure 1: Illustration of unnecessary array allocation (R) in an array \nlanguage using a code fragment from the tridiago- nal systems solver component of the SPEC CFP95 Tomcatv \nbenchmark. 2 Representations This section describes our array statement normal form and array level dependence \nrepresentation. The representation we describe will be used in defining the fusion for contraction problem \nand in implementing its solution. 2.1 Normalized Array Statements We define a normalized array statement \nto be an element-wise array operation that has the following properties: (i) the same array (or aliasing \narrays) may not be both read and written, (ii) the statement contains arrays of a common rank, and (iii) \nthe extent of the array statement s com-putation is defined by an index set, called a region, and all \narray references are specified as constant offsets from this index set. The final property implies that \nfor each array ref-erence, the elements of its subscript are separable (i.e., an index variable may appear \nin only a single element of a sub- script) and a particular index variable appears in the same position \nof all subscripts. A normalized array statement has the following form. [R] f(AlQdl, AzQdz,.. . , A,Qds) \n The indices of array Ai involved in the computation are those of the region R=[l..nl,l..nz,. . . ,l..n,] \noffset by the integer r-tuple di=(diI,. . ., dip), i.e., (l+di,..nl+di,,. . , l+di,..n,+di,j. Figures \n2(a) and (b) contain F90 array statements and their normal form equivalents (note that A % AOO, where \n0 is the null or zero vector). Though only normalized statements can participate in fusion and contrac- \ntion, unnormalized statements do not prevent independent normalized statements from being optimized. \nThis normal form is an appropriate representation for array statements because the data volume of each \nterm in a single array statement is the same (i.e., they are con-formable). Conformability permits most \nF90 and ZPL array statements to be normalized, either directly or by the in-troduction of compiler temporaries \nthat are often later con-tracted. Furthermore, the normal form serves as an effective internal representation \nwhen compiling for parallel machines because it makes the alignment of arrays explicit. All array references \nare perfectly aligned except for vector offsets, so normalized statements will compile to highly efficient \nparal-lel code [7]. Compiler generated communication primitives need not be normalized because they are \nnot candidates for fusion or contraction. Not coincidentally, the normal form closely resembles the core \nof the ZPL source language. 1 A(l:m.l:n) = B(O:m-1.1~) 1 [l..m.l..n] A := B(Q(-LO); 2 C(l:m.l:n) = A(l:m,O:n-1) \n2 [l..m,l..n] C := A@(O,-1): 3 B(l:m.l:n) = A(O:m-1,2x+1) 3 (l..m,l..n] B := A@(-1,l); (b) do j = n.l,-1 \ndo i = m.l,-I A(ij) = B(i-1,j) B(i,j) = A(i-l.j+l) enddo enddo do i = l,n do j = 1.m C(ij) = A(ij-1) \nenddo enddo (d) Figure 2: Four different representations of the same array computation: (a) Fortran \n90, (b) normalized array state-ments, (c) Fortran 77, and (d) array statement dependence graph. 2.2 \nThe Array Statement Dependence Graph In this section, we review the concept of data dependence, and we \nmodify existing mechanisms to represent depen-dences between normalized array statements. Data depen-dences \n[26] represent ordering constraints on statements in a program. A flow or true dependence requires that \na vari- able assignment precede a read to the same variable, and an anti-dependence requires the reverse. \nAn output dependence requires that one assignment to a variable precede another assignment to the same \nvariable. Transformations that re-order dependent statements (i.e., move the dependence tar-get before \nits source) are illegal because they violate the dependence and do not preserve correctness. Data dependence \nis also used to represent ordering con-straints on iterations in the iteration space of a loop nest. \nThe iteration space associated with a loop nest has a dimen- sion for each loop in the nest. Loop transformations \nsuch as loop interchange or loop reversal are only legal if they preserve the data dependences in the \niteration space. Dis-tance vectors serve as a static analysis tool to represent data dependences concisely \nin an iteration space. Definition 1 A distance vector is on integer n-hple, d = (dl,dz,... , dn), representing \na dependence between the iter- ations of a rank n iteration space, where the source of the dependence \nprecedes the target by di iterations in loop i (I is the outermost), for 1 5 i 5 n. Note that a negative \nor zero value implies that the target precedes the source or that they are in the same iteration, respectively. \nA distance vector is lexiwgraph~cally nonnegative if it is a null vector or if its leftmost non-zero \nelement is positive. A lexicographically nonnegative distance vector is said to be legal because the \ndependence source precedes the target in the loop that carries the dependence. If a distance vector is \nnot lexicographically nonnegative, then the target of the dependence precedes the source in the loop \nthat carries the dependence, which is clearly illegal. Distance vectors are inappropriate for use in \narray level compilation because they are derived from loop nests, which are not created until after our \ntransformations have been performed. As a result, we introduce a variant of the dis-tance vector, called \nthe unconstrained distance vector, to represent array level data dependences between normalized array \nstatements. Definition 2 An unconstrained distance vector is an in-teger n-tuple, u = (u~,uz, . . . ,un), \nrepresenting a depen-dence between two normalized n-dimensional array state-ments, where the source of \nthe dependence precedes the target by ui iterations of the loop that iterates over dimension i (if both \nstatements appear in the same loop nest). Unconstrained distance vectors are constructed by subtract- \ning the dependence s target offset vector from its source off-set. For example, the unconstrained distance \nvectors that arise from the dependences in the code in Figure 2(b) are (O,O)-(0, -1) = (0,l) and (0,0)-(-l, \n1) = (1, -1) for array A and (-1,O) -(O, 0) = (-1,O) for array B. The lexicograph- ical nonnegativity \nof an unconstrained distance vector has no bearing on the legality of the dependence it represents. Because \nscalarization of a normalized statement gener-ates a single loop to iterate over the same dimension of \nall arrays in its body, we can characterize dependences by di-mensions of the array rather than dimensions \nof the iteration space. Thus ui is the distance of the dependence along ar-ray dimension i. Unconstrained \ndistance vectors are more abstract than traditional (constrained) distance vectors be-cause they separate \nloop structure from dependence repre- sentation. Though unconstrained distance vectors are not fully \ngeneral, they can represent any dependence that ap- pears in our normal form. We represent code using \nthe array statement dependence graph. Definition 3 An array statement dependence graph (ASDG), G = (V, \nE), is a labeled, acyclic, directed graph, where vertices, vi, represent statements, edges repre- sent \ndata dependences between statements, and each edge, (w~,va) E E, is labeled, l(w~,va), with a set of \n(vari-able name, unconstrained distance vector, dependence type E {pow, anti, output}) tuples. An ASDG \nis guaranteed to not contain cycles because it represents a single basic block at the array statement \nlevel. An edge from v1 to 212, (vr,vz) E E, in an ASDG indicates that the target statement v2 is dependent \non the source statement VI. The label on each edge in the ASDG describes the dependences the edge represents \nby naming the variables that induce the dependences and the associ-ated unconstrained distance vectors \nand dependence types. Figure 2(d) contains the ASDG that corresponds to the nor-malized array statements \nin 2(b). If after scalarization, the source and target of a depen- dence appear in the single loop nest, \na conventional (con-strained) distance vector may be constructed from an uncon- strained one given a \ndescription of the loop nest structure. Definition 4 A loop structure vector is an integer n-tuple, p= \n(pl,P2,... ,p,), that describes the dimension and direc- tion of each loop in an n-deep loop nest. Loop \ni (I is the outermost loop in the loop nest) iterates over dimension Jp;] in the direction of the sign \nof pi, positive denoting increas-ing. A loop structure vector is a permutation of (fl, f2,. . . , ztn). \nThe loop structure vec-tor that describes the loop nests in Figure 2(c) are (-2, -1) and (1,2). In the \nfirst nest, the outer loop iterates over the second dimension and the inner loop iterates over the first \ndimension, both in a decreasing direction. A constrained distance vector, d = (dl,da,. . . ,dn), is constructed \nfrom an unconstrained one, u, and a loop struc-ture vector, p, by letting di = fiulvi 1, for 1 5 i < \nn. Consider array statements 1 and 3 in Figure 2(b). If p = (-2,-l), the unconstrained distance vectors \n(-1,0) and (1,-l) become (0,l) and (1,-l), respectively, when constrained. The constrained distance vectors \nare lexico-graphically nonnegative, so the dependences of the code in Figure 2(b) are preserved by the \nfirst loop nest in 2(c) result- ing from loop structure vector p. There are no constraints on the structure \nof the second loop nest because it does not contain statements that depend on each other. A fusion partition \ndescribes a particular fusing of the statements in an ASDG. Definition 5 A fusion partition, P = (Pi,&#38;, \n. . . , PI), of an ASDG, G = (V, E), is a partitioning of the nodes of G into 1 disjoint sets, PI, Pa,. \n. . , 9, called fusible clusters such that the following conditions hold: (i) all statements in a single \ncluster operate under the same region, (ii) all unconstrained distance vectors on intra-fusible-cluster \nJaw dependences are null vectors (i.e., VPi and 211,~~ E Pi, if (x,u,fiow) E l(vl, va) then u is a null \nvector), (iii) there are no inter-fusible-cluster cycles, and (iv) a loop structure vector exists for \neach fusible cluster that preserves all intra- fusible-cluster dependences. Upon scalarization, all the \nstatements in a fusible cluster are implemented with a single loop nest. The statements in each loop \nnest and the loop nests themselves are ordered by a topological sort using intra-and inter-fusible cluster \ndependence edges, respectively. The first condition above ensures that all the statements in a single \ncluster have the same (i.e., conformable) loop bounds. The second condition ensures that a loop carried \nflow dependence will not inhibit parallelism. The final two conditions ensure that inter-and intra-fusible-cluster \ndependences are preserved, respectively. An algorithm to decide the final condition is described in detail \nin Section 4.2. The trivial fusion partition of an ASDG is one in which there is exactly one statement \nin each fusible cluster. Given a particular fusion partition we can decide for what arrays contraction \nhas been enabled. Definition 6 Given a fusion partition, P= (PI,&#38;,... ,Pi), of an ASDG, G = (V, E), \nan array, x, is contractible if the following conditions hold: (i) the source and target of all dependences \ndue to x appear in the same fusible cluster (i.e., V(vr, ~2) E E, if (2, u, t) E I(wI, va), then VI E \nPi and v2 E Pi for some i, 1 5 i 5 l), and (ii) the un-constrained distance vectors of all data dependences \ndue to z are null vectors (i.e.., V(vi, ~2) E E, if (z,u, t) E l(v~, va), then u is a null vector). These \nconditions ensure that all references to x will appear in a single loop nest upon scalarization, and \nthere will be no loop carried dependences due to x. The latter condition may be relaxed when the dependence \nis along a dimension of the array that is not distributed [4], but here we assume that all dimensions \nare distributed. This terminology is borrowed from Gao et al. [12], who corkkred a similar problem. See \nSection 6. Problem INPUT G = (V, E) : an array statenlent dependence graph There are two reasons to \nperform statement fusion: to en- able the elimination of arrays by contraction and to improve utilization \nof the data cache by exploiting inter-statement reuse. For the first goal, we seek a fusion partition, \nP, for an ASDG, G, that enables the maximum elimination of ar- ray element references by contraction. \nThe number of array element references eliminated by the contraction of array 3: (called reference weight, \nw(z, G)) is a function of the num-ber of times it is referenced at the array level and the region sizes \nover which these references occur. We call the sum of the reference weights of all contracted arrays \nthe wntrac-tion benefit of a fusion partition. For the second goal, we seek a fusion partition that maximizes \nthe number of arrays without inter-fusible-cluster dependences. The intuition is that while intra-cluster \ndependences are potential sources of cache reuse, we must be careful not to pollute the cache with the \nincreased references that come with excessive fu-sion. When all references to an array appear in a single \nloop nest, all other loop nests are spared the cache burden of references to the array. Both problems \nare provably NP-complete, so we present approximate solutions in the next section. 4 Solution This section \npresents algorithms for performing statement fusion to enable contraction and exploit locality. Because \neliminating entire arrays conserves memory and can result in enormous performance improvements, we perform \nfusion for contraction first. We also describe the details of scalar- ization. 4.1 Statement Fusion \nOur algorithm to fuse statements to enable array contraction appears in Figure 3. It takes as input an \nASDG, G, and it returns a fusion partition P = (PI, P2,. . . , PI) containing 1 clusters. Initially, \nP is the trivial fusion partition (line 1). The algorithm considers each variable,2 zi, that appears \nin the input array statement dependence graph in order of decreasing weight, w(zi,G). As a result, arrays \nthat have potentially the largest single impact on the total contraction benefit are considered first. \nIn line 5, set c is assigned all the fusible clusters that contain references to variable zd. The fusion \nof all the statements in the fusible clusters in c might introduce inter-fusible-cluster cycles, so c \nbecomes the union of itself and the fusible clusters that are on inter- fusible-cluster cycles using \nthe GROW function (line 6). This guarantees that there will be no dependence cycles, for they prevent \nfusion. If variable 2; is contractible and a fusion partition is produced by combining all the fusible \nclusters in c (by Definitions 6 and 5), fusion is performed. The union of all fusible clusters in c is \ntaken and assigned into the Pk with the smallest value k in c. The counter 1 is decremented to indicate \nthat there are fewer clusters. The FUSION-FOR-CONTRACTION algorithm uses three auxiliary routines. Function \nGROW(C, G) returns all fusible clusters not in c that are reachable by a dependence path from a cluster \nin c and that have a dependence path to a cluster in c. These are the fusible clusters that will be on \naFor simplicity, we describe the algorithm as operating on array variables. In reality, it operates on \narray variable definitions, so that different references to the same array in disjoint live ranges can \nbe optitniaed separately. OUTPUT P=(5,9,..., pl) : a fusion partition of G FUSION-FOR-CONTRACTION(G) \n1 P + trivial partition of G 2 I+ PI 3 5 + array vars in G sorted by decreasing weight w 4 for i + 1 \nto 1~1 { consider KU Zi for contraction } 5 c c {PjlPj contains a reference to variable Zi} fi c + c \nu GROW(C. Gl ; if CONTRACTIB~E?(&#38;,C,G) and FUSION-PARTITION?(C,G) 8 k + smallest j for Pj E c 9 Ph \n+-uzecz 10 1 + I -(ICI - 1) 11 return P Figure 3: Algorithm to find a fusion partition that enables contraction \nin an ASDG. an inter-fusible-cluster dependence cycle if the clusters in c are fused. This function s \nrunning time is O(e), where e is the number of edges in G. The FUSION-PARTITION?(C, G) and CONTRACTIBLE?(Z, \nc, G) predicates test the conditions in Definitions 5 and 6, respectively. They both run in O(e) time. \nThe former function can ignore inter-cluster cycles because line 6 guarantees they will not exist. It \nalso calls FIND-LOOP-STRUCTURE (described in the next section) to decide whether condition (iv) of Definition \n5 is met. If there are r arrays in G, the total running time for FUSION-FOR- CONTRACTION is O(re). The \nalgorithm to perform fusion for locality enhance- ment is identical to that in Figure 3, except that \nthe CON- TRACTIBLE? predicate in line 7 is eliminated. We try to fuse all statements that reference the \narray that will have the greatest single locality benefit, which is analogous to the contraction benefit. \nNext, we will describe the process by which an ASDG is scalarized given a fusion partition. 4.2 Scalarization \nScalarization generates a loop nest for each fusible cluster in a fusion partition, where the loop nests \nand the state- ments in the loop nests are ordered by a topological sort using inter-and intra-fusible-cluster \ndependences, respec- tively. The only work remaining is deciding the structure of each loop nest, i.e., \nthe direction in which and dimension over which each loop iterates. This information is encoded in a \nloop structure vector (Definition 4) for each fusible clus- ter. Intra-cluster dependences constrain \nthe structure of the loop nest that will implement its statements (i.e., the loop nest must preserve \nthese dependences). When the depen- dences do not fully constrain the structure of the loop nest, we \nwill favor the loop structure that best exploits spatial locality. The algorithm to find a loop structure \nvector given a set of unconstrained distance vectors from intra-fusible-cluster array-level dependences \nappears in Figure 4. FIND-LOOP- STRUCTURE consists of a doubly nested loop. The outer loop (line 3) iterates \nover the loops of the target loop nest, and the inner loop iterates over the dimensions of the arrays. \nThe loop body matches loops to array dimensions (lines 7 through 11). We consider target loops from outer \nto inner because when a dimension is assigned to a loop, the de- pendences that are carried in that loop \ndo not constrain the structure of the inner loops (thus set C is pruned in line 10). INPUT C : a set \nof m unconstrained distance vectors, each of size n OUTPUT p : a loop structure vector of size n (loop \ni iterates over array dimension lpi1 in the direction of the sign of pi) FIND-LOOP-STRUCTURE(C) 1 for \nj c1to n { initialize unassigned mask } 2 bj + true { bj = true + array dimension j has not yet been \nassigned to a loop} 3 for i -1 to n { iterate over loops } 4 for j + 1 to n { iterate over array dimensions \n} 5 if h; +l ifVuEC,Uj 20 6 d+ -1 ifVuEC,2ljIOand3uEC,zlj<O 0 otherwise 7 if a!#0 { can loop i iterate \nover dimension j? } 8 bj + false 9 pi + jd 10 C + C -{U E ClUj Z 0) 11 break out ofj loop 12 return NOSOLUTION \n{ no dimension found for loop i } 13 return p Figure 4: Algorithm to find a legal loop structure vector \ngiven a set of unconstrained distance vectors from intra-fusible-cluster data dependences. We consider \ndimensions from 1 to n so that inner loops will be matched with higher array dimensions to exploit spatial \nlocality (assuming row-major allocation), if allowed by the constraints. If there are e dependences, \nthe running time of lines 6 and 10 is O(e), so FIND-LOOP-STRUCTURE runs in O(n2e) time. Because the rank \nof the arrays, n, is typi- cally very small and effectively constant [23], the algorithm is essentially \nlinear, O(e), in the number of dependences. 5 Evaluation This section evaluates our algorithm for statement \nfu- sion and array contraction-as implemented in the ZPL compiler-by comparison to commercial FSO/HPF \ncompil- ers and hand coded C code. Furthermore, we examine the transformations effect on memory use and \ntheir relative im- pact on runtime performance. Finally, we evaluate how their interaction with communication \noptimizations effect perfor- mance. The benchmark programs we use to evaluate our trans- formations represent \ntypical parallel array language pro- grams. The SP application and EP kernel belong to the NAS parallel \nbenchmark suite [2, 31. SP solves sets of un- coupled scalar pentadiagonal systems of equations; it is \nrep- resentative of portions of CFD codes. EP generates pairs of Gaussian random deviates, and it is \nconsidered embarrass- ingly parallel. EP characterizes the peak realizable FLOPS of a parallel machine. \nTomcatv is a SPEC CFP95 bench- mark that performs vectorized mesh generation. The Simple code solves \nhydrodynamics and heat conduction equations by finite difference methods [lo]. The Fibro application \nuses mathematical models of biological patterns to simulate the dynamic structure of fibroblasts [ll]. \nWe use the Cray T3E, IBM SP-2 and Intel Paragon in our evaluation. The T3E is a distributed shared memory \nma- chine, while the other two are message passing distributed memory machines. The T3E we use has 94 \nnodes, each con- taining a 450 MHz DEC Alpha 21164, 8 and 96 KB Ll and L2 data caches, respectively, \nand 256 MB memory. The SP-2 we use has 144 nodes, each containing a 120 MHz POWER2 Super Chip (PZSC), \n128 KB data cache and 256 MB mem- B(l:n,l:m) = A(1:n.l:m)tA(1:n,l:m) 0)C(l:n,l:m) = A(l:n,l:m)*A(l:n,l:m) \nB(l:n.l:m) = A(O:n-l,l:m)tA(O:n-1,l:m) (2)C(l:n,l:m) = A(l:n,l:m)*A(l:n,l:m) B(l:n,l:m) = A(O:n-l,l:m)tC(O:n-l.l:m) \n(3) C(l:n,l:m) = A(1:n.l:m)*A(1:n.l:m) A(l:n.l:m) = A(l:n,l:m)tA(l:n,l:m) (4) A(l:n,l:m) = A(O:n-l,l:m)+A(O:n-1,l:m) \n(5) B(l:n,l:m) = A(l:n,l:m)+A(l:n,l:m) (6) C(l:n,l:m) = B(l:n,l:m) B(l:n,l:m) = A(1:n.l:m)tA(1:n,l:m)+C(O:n-1,l:m) \n(7) C(l:n,l:m) = B(l:n,l:m) Tl(l:n,l:m) = B(l:n.l:m) T2(1:n,l:m) = B(l:n,l:m) ( 3) A(l:n,l:m) = A(2:n+l,l:m) \n+ Tl(Z:n+l,l:m) + T2(2:ntl,l:m) Figure 5: Code fragments to exercise Fortran 90 and HPF compilers. ory. \nThe Paragon we use has 18 nodes, each containing a 75MHz Intel i860 processor, 8 KB data cache and 32 \nMB of memory. 5.1 Comparison to Commercial Compilers In order to assess the state of the art, we determine \nhow aggressively current commercial array language compilers perform statement fusion and array contraction. \nWe exam- ine compilers for F90 and HPF (a parallel superset of F90) because F90 is the array language \nto which the greatest de- velopment effort has been devoted. The developers of commercial compilers do \nnot advertise the specific optimizations that their products perform, SO we infer their ability to perform \nstatement fusion and array contraction by studying compiler output for a set of care- fully selected \ncode fragments, shown in Figure 5. In all cases, arrays B, Tl and T2 are not live beyond the given code \nfrag- ments. The fragments in (l), (2) and (3) test a compiler s ability to perform statement fusion \nto exploit temporal lo- cality. The fragments differ in the data dependences they contain. The fragments \nin (4) and (5) test a compiler s abil- ity to eliminate compiler temporaries, and (6) and (7) test the \nsame for user temporaries, in this case array B. Frag- ment (8) contains two user arrays that can be \ncontracted if contraction of the compiler array for the third statement is sacrificed. The fragment tests \nwhether a compiler prop- erly weighs this tradeoff. Figure 6 summarizes whether each compiler properly \nfused (and in some cases contracted) each code fragment. First, observe that the PGI and IBM compilers \nappear not to perform any statement fusion (i.e., each array state- ment compiles to a single loop nest). \nThe implementors hoped to leverage the optimizations performed by the back end Fortran 77 compiler, which \ndoes in fact perform fusion. Unfortunately, the back end compiler does not perform con- traction because \nit was not designed to compile scalarized array language programs. Most of the compilers success- fully \neliminate compiler temporaries. This is not surpris- ing given that it requires only a simple local analysis, \nbut additional experiments (Section 5.4) show that this trans- compiler wer trade-arrag language scalar \nfusion tempe tempa application w/o conk. to contr. % change long. compiler (1) (2) (3) (4) (5) (6) (7) \n(27 EP 22(0/22) 0(0/O) -100.0 1 PGI HPF 2.1 d 4 FIT&#38;C wwJ) Wl) -87.5 1 IBM XLHPF 1.2 SP 181(18/163) \n56(0/56) -69.1 48 APR XHPF 2.0 ,/ ,/ Tomcatv 19(4/X) 7(0/7) -63.2 7 :: d Cray F90 2.0.1.0 J J d J J \nSimple 85(20/65) 32(0/32) -62.4 32 ZPL 1.13 J Fibro 49(0/49) 27(0/27) -44.9 na dd/JdddJ Figure 6: Observed \nbehavior of five array language compil-ers. A d indicates that the compiler produced the proper fused/contracted \ncode (as described in the running text). formation alone is not sufficient. Though the APR com-piler \nappears to performs fusion for locality and compiler array contraction, it is unable to fuse loops that \ncarry anti-dependences. Finally, notice that the Cray F90 compiler appears to perform both statement \nfusion and array contraction, but there are circumstances under which it fails. The com-piler is unable \nto fuse statements where the resulting loop nest would contain loop carried anti-dependences. As a re- \nsult, fusion does not occur in either (3) or (7), in the latter case inhibiting contraction. We also \ninfer that the compiler considers contraction of compiler and user temporary ar-rays separately, since \nit contracts the compiler temporary in (8) at the expense of contracting the two user temporaries. The \nCray compiler probably never inserts compiler tempo-raries when a single statement does not require it, \neven if this transformation would enable the contraction of multiple other arrays. The technique we describe \nalways inserts com-piler arrays, and it treats compiler and user arrays together as candidates for contraction. \nIf a single statement does not truly require a compiler array, our algorithm is guar- anteed to contract \nit unless a more favorable contraction is performed that prevents it. 5.2 Comparison to Hand-coded A \nsuccessful array language compiler will produce scalar code comparable to that of a skilled scalar language \npro-grammer. We now compare code produced by the ZPL compiler with equivalent programs written in a scalar \nlan-guage. Figure 7 summarizes for each of the six benchmarks the number of static arrays appearing in \nthe compiled code with and without array contraction. Note that within each code, nearly all arrays are \napproximately the same size. We see that all compiler-generated arrays have been eliminated. The benefit \nof this is that a programmer can better compre-hend the memory use of their code when the compiler only \ninfrequently introduces arrays. Figure 7 shows a substantial reduction in the number of static arrays. \nAll the arrays are eliminated in EP, and in all but one of the other benchmarks more than half are eliminated. \nThe final column in Figure 7 gives the number of arrays that appear in equivalent scalar language codes. \nThe scalar language codes are all publicly available C or Fortran 77 programs written by third parties. \nThe compiler-generated code has the same or fewer arrays on all the benchmarks except SP, which highlights \na deficiency in our current algo-rithm. As we have described contraction, an array is con- tracted to \na scalar or left as is. SP contains a great many opportunities to contract arrays to lower dimensional \nar-rays. Though the resulting arrays cannot be manipulated in registers, they conserve memory and make \nbetter use of Figure 7: Static arrays contracted (categorized as com-piler/user arrays). Fibro was developed \nin ZPL, so no equiv- alent scalar version exists. the cache. Despite this shortcoming, SP still benefits \nfrom a substantial performance improvement, as we see in Sec-tion 5.4. 5.3 Effect on Memory Usage and \nProblem Size While the preceding section uses static array counts to sug- gest that contraction conserves \nmemory, here we employ dy-namic data to discover more precisely how memory conser-vation from array contraction \nenables larger problems to be solved in a fixed amount of memory. The degree by which contraction allows \nlarger problems to be solved is an im-portant issue for memory bound applications. We assume the following \nof a single program on a particular machine: (i) all arrays are the same size, which we call the problem \nsize, (ii) all array elements are the same size, and (iii) a constant amount of memory is available for \narray allocation independent of problem size. The degree by which the max-imum problem size scales due \nto contraction is the ratio of the maximum problem size after and before contraction, z. Given the above \nassumptions and that maximum problem size is inversely proportional to the maximum number of si- multaneously \nlive arrays, 1, the scaling factors becomes 2. We subtract 1 and multiply by 100 to convert the maximum \nproblem size scaling factor to percent change, C(lb, 1,) = 100 x &#38;-k The first columns of Figure \n8 give the dynamic lb and I,, values and the calculated C value for each benchmark. To confirm the above \nanalysis, we experimentally deter-mine for each benchmark the largest problem size that fits on a single \nnode of the Cray T3E and the IBM SP-2. Both machines have operating system facilities to limit the pro-cess \nsize, so we found the largest problem size that does not result in a memory allocation failure. Columns \nseven and ten of Figure 8 give the change in problem size, both along one dimension of the problem domain \nand in total data vol-ume. The experimental data shows that these applications respect the above assumptions, \nfor the C value accurately predicts the change in problem volume. The one exception is Frac on the SP-2, \nwhich violates assumption (ii). EP, in which all arrays are eliminated, clearly benefits the most from \ncontraction because the contracted form uses a con-stant amount of memory, independent of the problem \nsize. The other applications changes in problem size vary from 10% to 274% along a single dimension or \n25% to 1300% in total volume. IBM SP-2 maximum application lb la c w 0 contr. w/ contr. EP 22 0 2 s Frac \n8 1 707.0 15312 57Yos Tomcatv 19 7 171.4 9292 15302 Fibro 49 27 81.5 583 7902 SP 23 17 35.3 743 81 Simple \n40 32 25.0 640 715a Figure 8: Effect of contraction on maximum achievable  5.4 Run-time Performance \nThis section considers the runtime performance impact of array contraction and statement fusion. Though \nwe discuss only the relative effect of these transformations, other stud-ies have shown that the ZPL \ncompiler produces code that performs within 10% of hand coded C plus message passing and generally better \nthan HPF [8, 1 7, 18, 201. In order to better understand the performance contribu-tions of fusion and \ncontraction, we measure execution time using several incrementally different optimization strategies. \nbaseline : no fusion or contraction transformations are per- formed fl : fusion is performed to enable \nthe contraction of com-piler arrays, but contraction is not performed cl : fusion is performed to enable \nthe contraction of com-piler arrays, and contraction is performed f2 : cl plus fusion is performed to \nenable contraction of user arrays, but the contraction is not performed f3 : cl plus fusion is performed \nto improve locality (as de- scribed in Section 4) c2 : cl plus fusion is performed to enable contraction \nof user arrays, and contraction is performed cS+fS : c2 plus fusion is performed to improve locality \n(as described in Section 4) c2+f4 : cdi-f3 plus all legal fusion (by a greedy pair-wise algorithm) Figures \n9, 10 and 11 show the percent improvement of each transformation over baseline for each benchmark for \na varying number of processors on the Cray T3E, IBM SP-2 and Intel Paragon. Execution times are the best \nof three trials on the T3E and Paragon and of at least six trials on the SP-2, a machine that suffers \nfrom great performance variance from trial to trial. So that we may neutralize the effect of communication \nmasking all other performance char-acteristics on large processor sets, we scale the problem sizes with \nthe number of processors (i.e., the amount of data per processor remains constant as the number of processors \nin-creases). These graphs demonstrate that performing contraction on both compiler and user arrays in \narray languages is es- sential. The predominant characteristic of the graphs is that c2 dominates the \nother transformations. The elimination of a large portion of the compiler and user arrays by contrac-tion \ndrastically improves temporal locality, always resulting in a significant performance boost (up to 400% \non one appli- cation). Fibro on the SP-2 does not benefit from contraction for large number of processors \nbecause of interactions with problem size on single IBM SP-2 and Cray T3E nodes. problem size % change \nvol Crav T3E maximum w/o contr. w/ contr. Drobhl size % change vol I dw)274.3(1300.7) 2l I co I cdco) \n64.7(171.2) 35.5(83.6) 9.5(31.1) 11.7(24.8) communications optimizations discussed in the next section. \nIn the larger applications, contraction of only compiler ar-rays, cl, provides a substantive performance \nenhancement (up to 30%), but it is only a fraction of the potential con-traction benefit. The smaller \nbenchmarks, such as Fibro, EP and Frac, require no compiler arrays, so they do not benefit from fl and \ncl. Clearly, transformation cl does not sufficiently address the problem of unnecessary temporary arrays \nin array languages. For a number of programs, transformations f.2and f3 pro- duce noticeable slowdown. \nIt appears that they increase ca-pacity and conflict misses in programs that are particularly sensitive \nto memory system performance, such as Tomcatv and Fibro. Transformation c2+f4 generally results in no \nimprovement beyond c2+f3, and frequently produces signif-icantly less improvement versus baseline (3% \nversus 16% for Fibro on the T3E). SP is the one exception, because arbi-trary fusion enhances spatial \nlocality of independent state-ments. Our fusion algorithm instead fuses dependent state-ments to enhance \ntemporal locality. We leave to future work the extension of our algorithm for spatial locality sensitivity. \nThe lesson is that fusion should not be performed arbitrarily in an array language. As the number of \nprocessors, p, varies, certain trends become evident. The improvement due to contraction in EP and Frac \nis effectively independent of the number of processors because these codes scale nearly perfectly with \np. The improvement due to fusion and contraction grows with p for some programs, such as Simple and Tomcatv \non the SP-2, when the transformations improve portions of the program that make up a larger fraction \nof total execution time as p grows (i.e., the transformations improve portions of the code that do not \nscale well with p). The performance improvement for a transformation de-creases with p when the transformation \nimproves a portion of the code that makes up a smaller fraction of total exe-cution time as p increase. \nThis happens when some other segment of the code is not scaling well and consumes a larger fraction of \ntotal execution time as p increases. SP exhibits this behavior because only potions of the code that \nscale well benefit from the transformations. When both scaling and non-scaling segments of a code benefit \nfrom the trans-formations, machines characteristics (e.g., the relative costs of cache misses, communication \nand floating point opera-tions) dictate the trends. This is exemplified by Tomcatv, which shows level, \nincreasing and decreasing trends on the three machines in our experiments. 5.5 Interaction with Communication \nOptimization In this section, we demonstrate that statement fusion inter-acts with communication optimizations \nand for this reason should be performed at the array level. Some optimizations cannot be performed practically \nat the scalar level because H 60 % 60 40 g 1 40 7.0 40 20 20 % 0 1 4 16 number ot proces%~ro 64 ' -1\"' \n, 4 16 number ot pmcaewxo 64 1 4 numbu 16 ot pmeerrors 64 Figure 9: Benchmark performance on Cray T3E. \nNegative bars represent slowdown. SP , 60 Simple Tomcatv number ot proceoeore number of pmcaeor~ Figure \n10: Benchmark performance on IBM SP-2. Negative bars represent slowdown. SP Simple Tomcatv , 60, mfl \n&#38;g cl f2 Fibro .-I Frac , mp , m-sp, c2 0 d+fJ c2+f4 liumbet of iumber ot pmcrro tiumbrr ot Figure \n11: Benchmark performance on Intel Paragon. Negative bars represent slowdown. 57 they interact with other \ntransformations that can only oc-cur at the array level. If an optimization that interacts with array \nlevel transformations is relegated to a scalar compiler, either the array level transformations must \nunderstand and reason about the optimization behavior of the scalar com-piler or vice versa. It is unlikely \nthat scalar compilers can un- derstand the optimization strategy of all the compilers that compile to \nit, so the array compiler must consider scalar optimizations when performing array transformations, ef-fectively \nmoving the scalar transformations into the array compiler. To achieve efficient parallel execution, compilers \nmust often perform aggressive communication optimizations [9], such as redundancy elimination, message \ncombining and pipehning. In some cases, these communication optimiza-tions are at odds with fusion for \ncontraction. For example, pipelining hides latency by separating the send and receive portions of communication \nwith computation, but fusion may collect into a single loop some of the statements that could be used \nto hide latency, potentially disabling overlap. The experiments presented thus far resolve this conflict \nby favoring fusion, i.e., fusion is never prevented by communi- cation optimizations. We consider an \nalternative strategy in which communication optimizations are favored, i.e., fusion cannot be performed \nif it reduces the benefit of communi- cation optimization. Message vectorization never conflicts with \nfusion, so it is always performed. As the amount of fusion increases, the potential for con-flict with \ncommunication optimization grows. The preceding section demonstrates that c%+f4 is not a valuable transfor-mation, \nso we use the c2+f3 transformation. On the T3E, when favoring communication optimizations over fusion \nfor contraction, Simple, Tomcatv, SP and Fibro suffer a slow- down of 25.4%, 22.7%, 9.6% and 5.1%, respectively. \nOn the SP-2, they slowdown by 31.8%, 66.5%, 10.5% and -10.6%, respectively. On the Paragon, they slowdown \nby 7.5%, 8.5%, 5.0% and 0.9%. The first three programs slowdown sig-nificantly because the communication \noptimizations disable a large number of array contraction opportunities without producing comparable \ncommunication benefits. Only one fusion for locality opportunity and no contraction opportu-nities are \nlost by favoring communication optimizations in Fibro. It slows down little and in one case it speeds \nup, because of the additional communication optimization. EP and Frac do not slowdown because they are \nsmall codes that do not benefit from communication optimization, with or without fusion. We have not \ndemonstrated that favoring contraction is optimal, but we have shown that if a choice is to be made, \nfusion for contraction should be favored. This suggests that it would be very difficult to perform communication \nopti-mizations if fusion and contraction occur after scalarization. The communication transformations \nwould have to under-stand contraction well enough to optimize without disabling it, since it is unlikely \nthat the scalar compiler could reason about communication primitives once they are scalarized. The Fibro \ndata suggests that there are delicate tradeoffs that only an integrated approach to fusion and communica- \ntion optimization can address, which would further compli-cate performing fusion at the scalar level. \nFurthermore, we expect to find that integration will become even more impor-tant on machines with low \ncost synchronization in hardware (e.g., SGI Origin, Sun ElOOOO). Thus, these results sup-port our claim \nthat these optimizations for array languages should be performed at the array level. 6 Related Work The \nproblem of optimizing array languages at the array level has recently received attention by others. Hwang \net al. de-scribe a scheme for array operation synthesis [14]. Multi-ple instances of element-wise F90 \narray operations such as MERGE, CSHIFT, and TRANSPOSE are combined into a single operation, reducing \ndata movement and intermediate storage. Their work does not address the inter-statement intermediate \narray problem except to substitute an inter-mediate array s use by its definition. This statement merge \noptimization [15] enables more operation synthesis, but it is not always possible, and it potentially \nintroduces redun-dant computation and increases overall program execution time. Roth and Kennedy have \nindependently developed a similar array based data dependence representation for F90, and they describe \nits use in scalarization [21]. They do not address the fusion for contraction problem. Loop fusion in \nthe context of scalar programming lan-guages such as Fortran 77 is well understood [26]. Though most \nwork only considers pairwise fusion, some research ad-dresses collective loop fusion, as we do. Sarkar \nand Gao [22] transform loop nests by loop reversal, interchange and fusion to enable array contraction. \nThey target multiprocessors and exploit pipelining by executing producer and consumer loops on different \nprocessors, so they are free to ignore all but flow dependences. Because we instead distribute itera-tion \nspaces, preservation of all types of dependences is crit- ical to our solution. Gao et al. [12] describe \nanother tech-nique for loop fusion based on a maxflow algorithm. The technique requires its input loop \nnests to be identically con-trolled, and it does not perform loop reversal nor interchange to enable \nadditional fusion. Furthermore, it is unclear what the algorithm does when a potentially contractible \narray is consumed by multiple loop nests. Our collective scheme performs reversal, interchange and fusion \nsimultaneously to enable contraction. Carr and Kennedy recognized the importance of keeping array values \nin scalars through scalar replacement [4], which is similar to array contraction in that some array references \nbecome scalar references, but array allocation is not elimi-nated (i.e., memory usage is not reduced). \nTheir focus is in recognizing the opportunity in a scalar loop nest, while ours is in enabling the opportunity \nin an array language compiler via statement fusion. Many techniques for improving locality by loop transfor-mations \nhave appeared in the literature [5,16,19,25]. Much of this work addresses the issue of managing the conflicting \ngoals of improving locality without sacrificing parallelism. This is a far less important issue in an \narray language com-piler, for the compiler can assume that only the loops that it generates need to be \nparallelized; user loops can remain sequential. In this paper we have assumed that all dimen-sions of \nall arrays are distributed and are a potential source of parallelism. 7 Conclusion This paper has shown \nhow statement fusion can be per-formed at the array level to enable array contraction and to enhance \nlocality. We have introduced, for array state-ments, a normal form and data dependence machinery that \nleverages array language properties. We have empirically demonstrated that our array-level transformations \nproduce substantial performance improvements, both in execution time and in memory usage. We have found \nthat the common technique of only contracting compiler arrays is insufficient for achieving high performance. \nFinally, we have shown that fusion and contraction should be performed at the array level, i.e., before \nscalarization, because they must precede or be integrated with communication optimizations for best performance. \nAcknowledgments. We thank Sung-Eun Choi, Bradford Chamberlain, Jerry Roth, and the anonymous reviewers \nfor their comments and insights on early drafts of this paper. This research was supported by a grant \nof HPC time from the Arctic Region Supercomputing Center. This research was conducted using the resources \nof the Cornell Theory Center, which receives major funding from the National Sci-ence Foundation and \nNew York State, with additional sup- port from the National Center for Research Resources at the National \nInstitutes of Health, IBM Corporation, and other members of the center s Corporate Partnership Program. \nReferences Jeanne C. Adams, Walter S. Brainerd, Jeanne T. Martin,PI Brian T. Smith, and Jerrold L. Wagener. \nFortmn 90 Hand- book. McGraw-Hill, New York, NY, 1992. PI D. Bailey, E. Barszcz, J. Barton, D. Browning, \nR. Carter, L. Dagum, R. Fatoohi, S. Fineberg, P. Frederickson, T. La&#38;ski, R. Schreiber, H. Simon, \nV. Venkatakrish-nan, and S. Weeratunga. The NAS parallel benchmarks (94). Technical report, RNR Technical \nReport RNR-94-007, March 1994. David Bailey, Tim Harris, William Saphir, Rob van der Wi- jngaart, Alex \nWoo, and Maurice Yarrow. The NAS parallel benchmarks 2.0. Technical report, NAS Report NAS-95-020, December \n1995. I31 Steve Carr and Ken Kennedy. Scalar replacement in the presence of conditional control flow. \nSoftware -Practice and Ezperience, 24(1):51-77, January 1994. [41 [51 Steve Carr, Kathryn S. McKinley, \nand Chau-Wen Tseng. Compiler optimizations for improved data locality. In Pro-ceedings of the International \nConference on Architectural Support for ProgmmminJ Languages and Operating Sys-terns, October 1994. San \nJose, CA. PI B. Chamberlain, S. Choi, E Lewis, C. Lin, L. Snyder, and W. D. Weathersby. Factor-join: \nA unique approach to com- piling array languages for parallel machines. In David Sehr, Uptal Banerjee, \nDavid Gelernter, Alexandru Nicolau, and David Padua, editors, Proceedings of the Ninth International \nWorkshop on Languages and compilers for Parallel Corn-pa&#38;g, pages 481-500. Springer-Verlag, August, \n1996. PI Bradford L. Chamberlain, Sung-Eun Choi, E Christopher Lewis, Calvin Lin, Lawrence Snyder, and \nW. Derrick Weath- ersby. ZPL s WYSIWYG performance model. To appear in Third International Workshop on \nHigh-Level Parallel Pro-gramming Models and Supportive Environments, 1998. PI Bradford L. Chamberlain, \nSung-Eun Choi, E Christopher Lewis, Calvin Lin, Lawrence Snyder, and W. Derrick Weath- ersby. The csse \nfor high level parallel programming in ZPL. To appear in IEEE Computational Science and Engineer&#38;g, \n1998. PI Sung-Eun Choi and Lawrence Snyder. Quantifying the effect of communication optimizations. In \nInternational Confer-ence on Parallel Processing, August 1997. I101 W. Crowley, C. P. Hendrickson, and \nT. I. Luby. The SIMPLE code. Technical Report UCID-17715, Lawrence Livermore Laboratory, 1978. Marios \nD. Dikaiakos, Calvin Lin, Daphne Manoussaki, and Diana E. Woodward. The portable parallel implementation \nof two novel mathematical biology algorithms in ZPL. In Ninth Innternational Conference on Supercomputing, \n1995. WI G. Gao, R. Olsen, V. Sarkar, and R. Thekkath. Collective loop fusion for array contraction. \nIn Uptal Banerjee, David Gelernter, Alexandru Nicolau, and David Padua, editors, Proceedings of the Fifth \nInternational Workshop on Lan-guages and Compilers for Parallel Computing, pages 281- 1111 295. Springer-Verlag, \n1992. High Performance Fortran Forum. High Performance For- tran Langauge Specification, Version 2.0. \nJanuary 1997. [I31 1141 Gwan Hwan Hwang, Jenq Keun Lee, and Da Ching Ju. An array operation synthesis \nscheme to optimize Fortran 90 pro- grams. In Proceedings of the ACM SIGPLAN Symposium on Principles and \nPractice of Parallel Programming, July 1995. [I51 Dz-Ching Ju. The Optimization and Pamllelization of \nAr- my Language Programs. PhD thesis, University of Texas- Austin, August 1992. P 31 Ken Kennedy and \nKathryn S. McKinley. Optimizing for parallelism and data locality. In International Conference on Supercomputing, \npages 323-334, July 1992. 1171C. Lin, L. Snyder, R. Anderson, B. Chamberlain, S. Choi, G. Forman, E. \nLewis, and W. D. Weathersby. ZPL vs. HPF: A comparison of performance and programming style. Tech- nical \nReport 95-11-05, Department of Computer Science and Engineering, University of Washington, 1994. Dl \nCalvin Lin and Lawrence Snyder. SIMPLE performance re-sults in ZPL. In Keshav Pingali, Uptal Banerjee, \nDavid Gel- ernter, Alexandru Nicolau, and David Padua, editors, Work-shop on Languages and Compilers \nfor Parallel Computing, pages 361-375. Springer-Verlag, 1994. WI Naraig Manjikian and Tarek S. Abdelrahman. \nFusion of loops for parallelism and locality. IEEE transactions on par- allel and distributed systems, \n8(2):193-209, February 1997. PO1 Ton A. Ngo. The Role of Perfomance Models in Paral-lel Programming and \nLanguages. PhD thesis, University of Washington, Department of Computer Science and En- gineering, 1997. \nPI Gerald Roth and Ken Kennedy. Dependence analysis of For- tran90 array syntax. In Proceedings of the \nInt 1 Conference on Parallel and Distributed Processing Techniques and Ap-plications (PDPTA g6), August \n1996. 1221 Vivek Sarkar and Guang R. Gao. Optimization of array accesses by collective loop transformations. \nIn International Conference on Supercomputing, pages 194-205, June 1991. P31 Zhiyu Shen, Zhiyuan Li, \nand Pen-Chung Yew. An empirical study of Fortran programs for parallelizing compilers. IEEE Transactions \non Parallel and Distributed Systems, 1(3):356-364, July 1990. 1241 Lawrence Snyder. Programming Guide \nto ZPL. MIT Press, 1998. to appear. P51 Michael E. Wolf and Monica S. Lam. A data locality opti-mizing \nalgorithm. In Proceedings of the SIGPLAN SI Con-ference on Program Language Design and Implementation, \nJune 1991. 1261 M. Wolfe. High Performance Compilers for Parallel Com-puting. Addison-Wesley, Redwood \nCity, CA, 1996.  \n\t\t\t", "proc_id": "277650", "abstract": "Array languages such as Fortran 90, HPF and ZPL have many benefits in simplifying array-based computations and expressing data parallelism. However, they can suffer large performance penalties because they introduce intermediate arrays---both at the source level and during the compilation process---which increase memory usage and pollute the cache. Most compilers address this problem by simply scalarizing the array language and relying on a scalar language compiler to perform loop fusion and array contraction. We instead show that there are advantages to performing a form of loop fusion and array contraction <i>at the array level</i>. This paper describes this approach and explains its advantages. Experimental results show that our scheme typically yields runtime improvements of greater than 20% and sometimes up to 400%. In addition, it yields superior memory use when compared against commercial compilers and exhibits comparable memory use when compared with scalar languages. We also explore the interaction between these transformations and communication optimizations.", "authors": [{"name": "E. Christopher Lewis", "author_profile_id": "81541868056", "affiliation": "University of Washington, Seattle, WA", "person_id": "PP39038821", "email_address": "", "orcid_id": ""}, {"name": "Calvin Lin", "author_profile_id": "81451598438", "affiliation": "University of Texas, Austin, TX", "person_id": "PP14088871", "email_address": "", "orcid_id": ""}, {"name": "Lawrence Snyder", "author_profile_id": "81100168271", "affiliation": "University of Washington, Seattle, WA", "person_id": "PP14069254", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/277650.277663", "year": "1998", "article_id": "277663", "conference": "PLDI", "title": "The implementation and evaluation of fusion and contraction in array languages", "url": "http://dl.acm.org/citation.cfm?id=277663"}