{"article_publication_date": "09-19-2011", "fulltext": "\n Typed Self-Interpretation by Pattern Matching Barry Jay University of Technology, Sydney Barry.Jay@uts.edu.au \nAbstract Self-interpreters can be roughly divided into two sorts: self-recog\u00adnisers that recover the \ninput program from a canonical represen\u00adtation, and self-enactors that execute the input program. Major \nprogress for statically-typed languages was achieved in 2009 by Rendel, Ostermann, and Hofer who presented \nthe .rst typed self\u00adrecogniser that allows representations of different terms to have dif\u00adferent types. \nA key feature of their type system is a type:type rule that renders the kind system of their language \ninconsistent. In this paper we present the .rst statically-typed language that not only allows representations \nof different terms to have differ\u00adent types, and supports a self-recogniser, but also supports a self\u00adenactor. \nOur language is a factorisation calculus in the style of Jay and Given-Wilson, a combinatory calculus \nwith a factorisation operator that is powerful enough to support the pattern-matching functions necessary \nfor a self-interpreter. This allows us to avoid a type:type rule. Indeed, the types of System F are suf.cient. \nWe have implemented our approach and our experiments support the theory. Categories and Subject Descriptors \nD.3.4 [Processors]: Inter\u00adpreters; D.2.4 [Program Veri.cation]: Correctness proofs, formal methods; F.3.2 \n[Semantics of Programming Languages]: Opera\u00adtional semantics General Terms Languages, Theory Keywords \nself-interpretation, pattern matching 1. Introduction An interpreter implements a programming language, \nand a self\u00adinterpreter is an interpreter written in the language that it imple\u00adments. Self-interpreters \nare popular and available for Standard ML [34], Haskell [28], Scheme [1], JavaScript [12], Python [33], \nRuby [41], .-calculus [2, 4, 5, 21, 25, 26, 32, 35], and many other lan\u00adguages [23, 38, 42]. A self-interpreter \nenables programmers to eas\u00adily modify, extend, and grow a language [31], do other forms of meta-programming \n[8], and even derive an algorithm for normali\u00adsation by evaluation [6]. These self-interpreters can be \nroughly divided into two sorts: self-recognisers that recover the input program from a canonical representation, \nand self-enactors that execute the input program. While we will review the rich literature on self-interpretation \nin Permission to make digital or hard copies of all or part of this work for personal or classroom use \nis granted without fee provided that copies are not made or distributed for pro.t or commercial advantage \nand that copies bear this notice and the full citation on the .rst page. To copy otherwise, to republish, \nto post on servers or to redistribute to lists, requires prior speci.c permission and/or a fee. ICFP \n11, September 19 21, 2011, Tokyo, Japan. Copyright c &#38;#169; 2011 ACM 978-1-4503-0865-6/11/09. . . \n$10.00 Jens Palsberg University of California, Los Angeles palsberg@ucla.edu Sections 2 and 9, two highlights \nare papers by Mogensen [25], and by Berarducci and Bohm [5], that each de.ned a single program representation \nfor an untyped language that supports both a self\u00adrecogniser and a self-enactor, with proofs of correctness. \nNow consider statically typed languages. Most previous work on self-interpreters for these give all program \nrepresentations the same, universal type. The use of a universal type ignores the type of the input program \nand thereby misses an important opportunity for static type checking of self-interpreters. Major progress \nwas achieved in 2009 when Rendel, Ostermann, and Hofer [31] pre\u00adsented the .rst self-recognizer for a \nstatically-typed language in which representations of different terms can have different types. The challenge \nRendel, Ostermann, and Hofer left open the prob\u00adlem of typing a self-enactor. Additionally, their type \nsystem has a type:type rule that renders the kind system of their language incon\u00adsistent. Our results \nIn this paper we present the .rst statically-typed lan\u00adguage in which representations of different terms \ncan have different types and in which we can program and statically type both a self\u00adrecogniser and a \nself-enactor. Our language uses System F types and so has no rule asserting type:type. Our approach differs \nfrom previous work by adopting a pattern-matching perspective that we summarize next. The representation \nof a term t is a data structure 't so it is natural to consider an interpreter as a pattern-matching \nfunction in which each evaluation rule left . right of the source language can be represented by a case, \nwith a pattern derived from left and a body derived from right. So the fundamental question becomes how \nto represent pattern\u00admatching. Berarducci and Bohm achieved this by considering how to solve equations \ninside lambda calculus but now there is a more direct and powerful method, in the pattern calculus of \nJay and Kesner [16, 18, 19]. Although it may be possible to achieve our main goals in pure pattern calculus \n(or even static pattern calculus), this paper adopts a simpler and more direct approach. Recent work \non factorisation calculus by Jay and Given-Wilson [17] supports combinatory calculi that are more expressive \nthan traditional combinatory calculi (based on S and K), in being able to analyse the internal structure \nof any normal form, e.g. to recover X from SKX. More generally, they can de.ne pattern-matching functions \nthat are powerful enough to interpret themselves. This compares well with previous approaches in which \nfunctions at one level are analysed by functions at the next higher level, as in the higher-order polymorphic \n.-calculus F. and even F. * which adds the rule type:type. The pure factorisation calculus [17] is a \ncombinator calculus with just two operators, S and F , where S is known from SK\u00adcombinators, and F is \na factorization operator that is able to de\u00adcompose compounds (e.g. closed normal forms) into their compo\u00adnents. \nIn this paper we add a constructor B to block evaluation, and together S, F , and B are suf.cient to \nrepresent programs in an untyped manner and to de.ne an untyped self-recogniser and an untyped self-enactor. \nAn additional bene.t of this approach is that there is a term that decides equality of representations \nof combi\u00adnators (closed terms). By comparison, such a term is not known to exist for higher-order abstract \nsyntax of untyped .-calculus when the meta-language is the untyped .-calculus itself.  We meet the goals \nof static type checking by adding three more operators, the traditional operator K, a .xpoint operator \nY and an operator E that tests for equality of operators. Although the types are relatively simple, being \nthose of System F, they are used in two unusual ways. First, the F operator cannot be typed with Hindley-Milner \ntypes alone. Rather, it takes an argument of polymorphic type, since the types of components are not \ndetermined by the type of their compound. Second, the operator E doesn t have a princi\u00adpal type scheme, \nfor reasons that can be traced back to the typing of pattern-matching functions in pattern calculus. \nThis creates dif.\u00adculties when interpreting E itself, which are overcome by replacing explicit references \nto E in patterns by binding symbols which are shown to match with E only. We have implemented the entire \napproach and performed exper\u00adiments that support our theory. The rest of this paper. We will discuss \nthe nature of self\u00adinterpretation and closely related work (Section 2), and we will de.ne our language \n(Section 3), syntactic sugar (Section 4), self\u00adrecogniser (Section 5), self-enactor (Section 6), type \nsystem (Sec\u00adtion 7), including proofs that our self-interpreters type check, and experimental results \n(Section 8). We also discuss additional related work (Section 9). 2. The Nature of Self-Interpretation \nThis section .xes the terminology for the various sorts of self\u00adinterpreters to be considered. De.nitions \nhave been chosen so that they apply as widely as possible, i.e. both to .-calculi and other rewriting \nsystems, and to programming languages with their eval\u00aduation strategies. Since the calculi emphasise \nstatic interpreters, while the programming language community emphasise dynamic ones, we propose new \nnames for the various special cases. Self-interpretation involves two steps. The .rst is a process of \nquotation that transforms the syntax of a term t into a value or nor\u00admal form ' t (pronounced quote t \n) ready for interpretation. The second is the application of a self-interpreter to ' t to produce some\u00adthing \nthat has the same meaning as t. Researchers have identi.ed two sorts of meaning here: a static approach \nthat focuses on pro\u00adgram structure, and a dynamic approach that focuses on program behaviour. Let us \nexamine each of them in turn. A self-recogniser is a self-interpreter unquote that can recog\u00adnise a term \nfrom its quotation, by reversing the quotation process: Self-Recogniser: unquote( ' t) t (1) where \ndenotes behavioural equivalence. For example, quotation may add tags that block evaluation, which are \nthen removed by unquote. The .rst self-recogniser is due to Kleene [21] who in\u00adtroduced a notion of quotation \nand unquote for pure .-calculus, and established (1) as a consequence of \u00df-equality. Barendregt [2] cited \nKleene s paper and used the term self-interpreter for any .\u00adterm unquote that satis.es unquote( ' t)=\u00df \nt. Barendregt [2], Mogensen [25, 26], Berarducci and B\u00a8 ohm [5], and Bel [4] all pre\u00adsented .-terms unquote \nthat satisfy Equation (1). The name self\u00adrecognisers seems apt because the interpretation can recover, \nor recognise, (something equivalent to) the original term. The process of quotation tends to cause code \nexpansion, with unquote( ' t) being a much larger program than t. This problem is mitigated if the self-recogniser \nis strong in the sense that: Strong Self-Recogniser: unquote( ' t) -. * t where -. * denotes reduction \nin a calculus, or a small-step oper\u00adational semantics of a programing language. Among the examples above, \nthose of Mogensen [25, 26] and Berarducci and B\u00a8 ohm [5] are strong. A self-enactor is a self-interpreter \nenact that mimics evalua\u00adtion: Self-Enactor: t . * v implies enact( ' t) ' v. (2) That is, if t evaluates \nto a value v then enact( ' t) is behaviourally equivalent to ' v. Note that this account requires knowledge \nof the values v and the evaluation process . *. Also, a self-enactor cannot be a self-recogniser unless \n' v v which usually fails. That is, preservation of meaning by a self-enactor is different to that of \na self-recogniser. That said, a self-enactor enact can be combined with a self-recogniser unquote to \nproduce a self-interpreter that maps t to unquote(enact( ' t)). The .rst use of self-enactors is due \nto Mogensen [25] who proved the existence of such a term in pure .-calculus. He used the term self-reducer \nfor any .-term enact that satis.es Equation (2). Mogensen [25], Berarducci and B\u00a8ohm [5] (who preferred \nthe term reductor), and Song, Xu, and Qian [35] presented .-terms enact that satisfy Equation (2). The \nname self-enactor seems apt because Equation (2) implies that enact must do work to turn a quotation \ninto action. The technical details of Mogensen s paper [25] strongly suggest that a self-enactor is more \ncomplex than a self-recogniser. This makes sense since, unlike a self-recogniser, a self-enactor must \ndo actual evaluation. A strong self-enactor satis.es the following stronger require\u00adment: Strong Self-Enactor: \nt . * v implies enact( ' t) . *' v. Among the examples already mentioned, only that of Berarducci and \nB\u00a8 ohm is a strong self-enactor. The self-interpreters for Standard ML [34], Haskell [28], Scheme [1], \nJavaScript [12], Python [40], and Ruby [41] all do evaluation. The documentations suggest that the programmers \nintended them to be self-enactors or, in some case, a self-enactor followed by an application of a self-recognizer \nto print a value rather than a representation of a value. A desirable quality of program representation \nis that we can decide equality of program representations by a term equal, as in: Equality of Representations: \nj '' ' true if ' s = ' t equal( s, t) . ' false otherwise. Some quotation mechanisms support that; some \ndon t. Even the presence of unquote doesn t guarantee the existence of equal. For example, many of the \nabove papers represent .-terms by higher-order abstract syntax with a meta-language that itself is the \nuntyped .-calculus. For such representations, no term equal is known to exist. Binding operations complicate \nthe issues because in .-calculus, closed terms are built from open terms. Kleene [21] avoided the problems \nwith open terms by representing programs with only closed terms that are built from combinators, that \nis, small, closed .-terms. In general, combinatory calculi present an easier equality-checking problem. \n3. Blocking Factorisation Calculus 3.1 Overview Our language is a combinatory calculus called the blocking \nfactori\u00adsation calculus which has all the properties above: decidable equal\u00adity of quotations, a self-recogniser \nand a self-enactor. It is a factori\u00adsation calculus in the sense of Jay and Given-Wilson [17]. Factori\u00adsation \ncalculi are more expressive than traditional SK-combinators [17], which may seem surprising since SK-calculus \nis combina\u00adtorially complete [20]. However, its factorisation operator F can decompose an identity function \nSKX to recover the value of the combinator X, something that cannot be done using S and K alone. Note \nthat the corresponding logic would be unsound if F could decompose an arbitrary application, e.g. to \nrecover X from KKX. To ensure soundness, the reduction rules for F require its .rst argument to be factorable, \nthat is, a partial application of an op\u00aderator. As a result, the factorisation calculus is con.uent, \nwhich im\u00adplies soundness of the corresponding logic. This expressive power supports analysis of normal \nforms such as quotations, including decidable equality of program representations. More generally, it \nsupports pattern matching that is expressive enough to support self\u00adinterpreters.  3.2 Syntax We assume \na countable set of variables (meta-variables w, x, y, z). The operators (meta-variable O) of our language \nare given by: (Operator) O ::= Y | K | S | F | E | B. Each operator has an arity, given by 1, 2, 3, 3, \n4 and 8, respec\u00adtively, that helps us de.ne the intended semantics. S and K take their usual meanings \nfrom combinatory logic. Y is a .xed-point operator. In a pure setting it could be de.ned from S and K \nbut this interpretation will not support the typing. F is the factorisation op\u00aderator used to decompose \ncompounds, as described later. Perhaps surprisingly, it cannot be de.ned in terms of S and K [17]. E \nis an equality operator that takes four arguments; two to compare, and two alternative results, chosen \naccording to whether the compared arguments are the same operator or not. B is used to block evalua\u00adtion. \nThe terms (meta-variables p, q, r, s, t, u) of our term calculus are given by: (Term) t ::= x | O | tt. \nTerms are generated by the variables and operators and they are closed under application. The free variables \nof a term are all the variables that occur in the term, since there are no binding operations. A term \nt avoids a variable x if x is not a free variable of t. Term substitutions s are de.ned and applied in \nthe usual manner. The substitution of the term ui for the variable xi for 1 = i = n in the term t may \nbe denoted [u1/x1,u2/x2,...,un/xn]. A combinator is a term built without any variables, the collec\u00adtion \nof which forms the corresponding combinatory calculus [15]. Conceptually, the combinatory calculus is \nmore fundamental than the term calculus, but to de.ne operations such as pattern-matching on combinators \nrequires the larger, term calculus, so that our efforts will focus there. A term is factorable if it \nis a partial application of an operator, as determined by its arity. That is, Ot1 ... tk is a partial \napplication of operator O if k is strictly less than the arity of O. In particular, all operators are \nfactorable. A compound is a factorable application. t t ' r -. r ' u -. u ' '' ' t -. t ru -. ru ru -. \nru Figure 1. The Reduction Relation  3.3 Reduction Semantics The reduction rules are: Y t t (Y t) K \ns t s S s t uF O s tF (p q) s tE O O s tE p q s t  s u (t u) s t p q s t if O is an operator if p \nq is a compound if O is an operator otherwise, if p and q are factorable. That is, Y is the .xed-point \noperator, K eliminates its second argument, and S duplicates its third argument, all as usual. The factorisation \noperator F branches according to the value of its .rst argument. If this is a compound then apply the \nthird argument to the components, else return the second argument. The equality operator E decides equality \nof operators. If the .rst two arguments are the same operator then return the third argument, else if \nthe .rst two arguments are both factorable (whether equal or not) then return the fourth argument. Note \nthat there are no reduction rules for B, which may thus be thought of as a constructor. The reduction \nrelation -. is obtained by applying the reduc\u00adtion rules to arbitrary sub-terms, as described in Figure \n1. As usual in rewriting, the transitive closure of a relation is denoted by (-)+ as in -.+ and the re.exive, \ntransitive closure by (-) * as in -. * . If t -. * t ' then t reduces to t ' . The basic properties of \nthe calculus are easily established. THEOREM 3.1. Reduction is con.uent. Proof. If pq is a compound then, \nby inspecting the arities, it is clear that it is not an instance of any reduction rule. Hence, any reduction \nof pq is a reduction of p or of q which implies that there are no critical pairs [37] involving F (pq) \nst. Similarly, there are no other critical pairs. 0 THEOREM 3.2. Every combinator in normal form is factorable. \nProof. The proof is by induction on the structure of the combinator. For example, if it is of the form \nF pst then induction implies p is a factorable form, so that a reduction rule applies. Similar remarks \napply to combinators of the form Epqst. The other cases are straightforward. 0 This theorem provides \na form of progress property, in that eval\u00aduation of the operators, especially F and E, cannot get blocked. \nNote, however, that if O does not have a normal form then F O KK cannot become an instance of a rule. \n4. Syntactic Sugar For the purpose of practical programming, particularly of our two self-interpreters, \nwe use .ve forms of syntactic sugar: identity operator, written I,  .-abstraction, written . * x.s \nand, in typewriter font, x -> s,  let binding, written letx =sint,  let rec binding, written let rec \nx = t and  extensions, written p -> s | t.  We de-sugar terms with such constructs before executing \nthem. De-sugaring maps closed terms to closed terms. The type-writer font is used when the emphasis is \non programming, rather than the calculus.  4.1 Identity, .-abstraction, let, and let rec We de-sugar \nI to the combinator SKK. One of the oldest results on computability is that .-abstraction can be de.ned \nby SK-terms (e.g. [15]). The de.nition of . * x.t is as follows. . * x.x = I . * x.t = Kt if t avoids \nx . * x.t x = t if t avoids x . * x.(ru)= S(. * x.r)(. * x.u) otherwise . The use of .-contraction in \nthe third line above is not theoretically necessary but makes a big difference in the size of the resulting \nterm. LEMMA 4.1. For all terms s and u and variable x there is a reduction (. * x.s) u -. * [u/x]s. Proof. \nThe proof is by induction on the structure of the term s. If s is x then (. * x.s)u = Iu -. * u =[u/x]s. \nIf s avoids x then (. * x.s)u = Ksu -. s =[u/x]s. If s is of the form tx where t avoids x then (. * x.s)u \n= tu =[u/x]s. Otherwise, if s is of the form s1s2 then * ** (. x.s)u = S(. x.s1)(. x.s2)u -. (. * x.s1)u((. \n* x.s2)u) -. * [u/x]s1([u/x]s2) =[u/x]s by two applications of induction. 0 We de-sugar the syntax let \nx= uin t to (. * x.t)u and we de-sugar let rec f = t to Y (. * f.t), as usual.  4.2 Extensions A pattern-matching \nfunction of the form p1 . s1 | p2 . s2 ... | pn . sn | x . s can be built as a sequence of extensions \nof the form p ->s|t by declaring the vertical bar to be right-associative, and replacing the .nal case \nby . * x.s. In such an extension, the .rst subterm p is a pattern, which is a term in normal form. While \ngeneralisa\u00adtions are possible, our notion of extension is suf.cient for typed self-interpretation, and \nalready generalises the usual approaches to pattern matching in functional programming. Traditionally, \npat\u00adtern matching is a technique for destructing values of a given al\u00adgebraic data type, each pattern \nbeing headed by one of the type s constructors. In contrast, we allow patterns such as (yx) that is not \nheaded by any constructor, but rather denotes an arbitrary com\u00adpound data structure. Our notion of pattern \nmatching is widely ap\u00adplicable; for example, it is easy to program an equality checker for normal forms. \nWe use the following recursive function to de-sugar extensions: x . s | r = . * x.s O . s | r = . * x.E \nO xs (rx)= S(S(EO)(Ks))r pq . s | r = . * x.F x (rx) (. * y.(p . (q . s | r ' y) | r ' ) y) (where r \n' = . * y.. * z.r (yz)= S(Kr)) where x is chosen fresh. The .rst two rules are clear enough. The third \nde.nes matching against an applicative pattern pq by match\u00ading the components of the argument against \np and then against q. The complexity of the term is caused by the need to handle the various sorts of \nmatch failure. The intended semantics is given by de.ning matching. A match is either a successful match \ngiven by Some s where s is a substi\u00adtution, or a match failure None. The disjoint union l of successful \nmatches is the successful match obtained from the disjoint union of their substitutions, if this exists. \nAll other disjoint unions are None. The matching {u/p} of a pattern p against a term u is de.ned by the \nrules {u/x} = Some [u/x] {O/O} = Some [] {u1 u2/p1 p2} = {u1/p1}l {u2/p2} if u1 u2 is factorable {u/p} \n= None otherwise, if u is factorable {u/p} = unde.ned otherwise corresponding to those of static pattern \ncalculus [16]. LEMMA 4.2. Extensions satisfy the following derived reduction rules: (p . s | r) u -. \n* ss (if {u/p} = Some s) (p . s | r) u -. * ru (if {u/p} = None). Proof. The proof is a routine induction \non the structure of the pattern, given Lemma 4.1. 0 This style of pattern matching, also known as path \npolymor\u00adphism [16, 18, 19], cannot be expressed in pure .-calculus or even in a combinator calculus with \nthe operators Y , S, K, B. So, our calculus has the operator F and the novel operator E, and we make \nthem play key roles when we de-sugar pattern matching. For example, to unblock reduction we will use \nunblock de.ned by B x->x |x ->x which de-sugars to the combinator Bx . x | x . x ** '' = . x.F x (Ix)(. \ny.(B . (x . x | ry) | r ) y) * *' = . x.F x (Ix)(. y.(B . I | r ) y) = . * x.F x (Ix)(B . I | r ' ) = \n. * x.F x (Ix)(S(S(EB)(KI))r ' ) = S(SFI)(K(S(S(EB)(KI))(S(KI)))) where r ' = S(KI). 5. Self-Recognisers \n 5.1 Quotation Quotation for both our self-interpreters is given by ' x = x ' O = BO ' '' (st)= st Clearly, \nquoted terms are always normal forms, whose internal structure can be examined by factorising. THEOREM \n5.1. There is a decidable equality of quotations of closed terms. Proof. The booleans are given by K \n(true) and KI (false) as usual. The required equality term is  '' ' t t t . t ' r . r r u . r ' u p \n. p F p . F p ' p . p ' E p . E p ' q . q ' E p q . E p q ' Figure 2. Call-by-Name Evaluation let rec \nequal = x1 x2 -> ( y1 y2 -> (equal x1 y1) (equal x2 y2) (K I) | y -> K I) |x-> |y->E xy K(K I) It decides \nequality of arbitrary closed normal forms, be they quo\u00adtations or not. If applied to two compounds then \nit checks equality of both components (by applying the boolean (equal x1 y1) to (equal x2 y2) and (K \nI) to represent the conditional for con\u00adjunction). Alternatively, if the .rst argument is an operator \nthen E is applied. 0  5.2 A Self-Recogniser De.ne unquote by let rec unquote = B x -> x | y x -> (unquote \ny) (unquote x) |x-> x THEOREM 5.2. unquote is a strong self-recogniser with respect to -. * . Proof. \nThe proof is a straightforward induction on the struc\u00adture of t. If t is a variable or operator then \nunquote( ' t) -. * ' '' t. If t is an application t1 t2, then t is a compound t1 t2 (since all quotations \nare headed by B). Hence unquote( ' t)= ' '' unquote( ' t1 t2) -. * t1 t2 by two applications of induction. \n0 6. Self-Enactors 6.1 Evaluation We choose a call-by-name semantics, given by an evaluation rela\u00adtion \n. as de.ned in Figure 2. There is not much scope for variation here; the operator F behaves like other \nbranching constructs, such as conditionals, being eager in its .rst argument but deferring eval\u00aduation \nof the other two; and E evaluates its .rst two arguments to factorable forms, as required to support \nits reduction. To de.ne behavioural equivalence requires a notion of value and of context. De.ne a value \nto be a term that is irreducible with respect to .. A term t has a value if there is a value v such that \nt . * v. Usually, a context is described as a term with a hole in it but our terms contain free variables \nthat cannot be bound, so a context C[-] here must also allow a term substitution s that is to be applied \nto the term that .lls the hole. Now, two terms t1 and t2 are behaviourally equivalent (written t1 t2) \nif, for any context C[-], the term C[t1] has a value if and only if C[t2] does. The following lemma will \nbe our main tool in establishing behavioural equivalence. LEMMA 6.1. If t1 -. t2 then t1 t2. Proof. \nThe proof is by case analysis on the reduction rules. 0 Before giving the self-enactor for the blocking \nfactorisation calculus that we will type, the approach can be illustrated by a pair of simpler examples. \n 6.2 A Self-Enactor for SK-calculus Consider the interpretation of SK-calculus in the blocking factori\u00adsation \ncalculus. There are two natural approaches to the represen\u00adtation of a rule left . right as a case, namely \nthe reduction approach and the meta-circular approach (pace [32]). The reduc\u00adtion approach represents \nthe rule by the case | '' left . enact right where the left-and right-hand sides of the rule have been \nquoted. For the reduction rule for S, this yields | B S x3 x2 x1 -> enact (x3 x1 (x2 x1)) The meta-circular \napproach replaces ' right above by an applica\u00adtion of the operator that is the subject of the rule. For \nS this yields |B Sx3x2x1->enact(Sx3 x2x1) Although the meta-circular approach is sometimes more concise, \nand so will be preferred, it won t always be applicable. The interpretation of SK-calculus is thus given \nby the combi\u00adnator enactSK de.ned by let rec enactSK = let enact1 = B K x2 x1 -> enactSK (K x2 x1) | \nB S x3 x2 x1 -> enactSK (S x3 x2 x1) | x1 -> x1 in x2 x1 -> enact1 (enactSK x2 x1) | x1 -> x1 The function \nenact1 tries to perform one step of the evaluation. It is a pattern-matching function with one case for \neach reduction rule of the calculus, which then performs a recursive call to enactSK. This stops if no \nreduction rule can be applied, as indicated by the default identity function. This handles partially \napplied operators. The pattern-matching function for enactSK itself has two cases: that for a compound \nenacts the left-hand component and then reduces the whole by enact1. For example, enactSK (K S K) = enactSK \n(B K (B S) (B K)) -. * enact1 (enactSK (B K (B S)) (B K)) -. * enact1 (B K (B S) (B K)) -. * enactSK \n(B S) -. * BS = S Note that the evaluation is lazy. To make it eager the special case for enactSK must \nbe changed to x2 x1 -> enact1 (enactSK x2 (enactSK x1))  6.3 An Explicit Self-Enactor Figure 3 displays \na self-enactor for the blocking factorisation cal\u00adculus that mentions E explicitly, but will resist typing \nlater on. The case for Y uses the reduction approach, as the meta-circular ap\u00adproach as the term Y x1 \nreduces to x1 (Y x1) instead of the in\u00adtended x1 (B Y x1). The operators K and S are handled using the \nmeta-circular approach, as before. The rules for F and E are also meta-circular, which proves to be more \nconcise than writing out all of the alternative elaborations of the rules. The role of evalop can  let \nrec enactexp = letunblock= Bx ->x |x ->x in let evalop = x -> unblock (enactexp x) in let enact1 = B \nY x1 -> enactexp (x1 (B Y x1)) | B K x2 x1 -> enactexp (K x2 x1) | B S x3 x2 x1 -> enactexp (S x3 x2 \nx1) | B F x3 x2 x1 -> enactexp (F (evalop x3) x2 x1) |BEx4x3x2x1 -> enactexp (E (evalop x4) (evalop x3) \nx2 x1) | x1 -> x1 in x2 x1 -> enact1 (enactexp x2 x1) | x1 -> x1 Figure 3. A Self-Enactor that Handles \nE Explicitly be illustrated by an example. Consider enactexp (F K S K) = enactexp (B F (B K) (B S) (B \nK)) -. * enact1 (B F (B K) (B S) (B K)) -. * enactexp (F (evalop (B K)) (B S) (B K) If evalop (B K) were \nreplaced by enact (B K) then F would be applied to BK when it should be applied to K. So evalop is used \nto unquote operators while leaving everything else unchanged. Similar remarks apply to the interpretation \nof E. Note that there is no case for B in enact1 as it has no reduction rules.  6.4 An Implicit Self-Enactor \nThe type machinery developed in Section 7 is not able to type patterns that contain E. Even though this \nhas only arisen once, in the self-enactor in Figure 3. it creates a major technical challenge: we want \nthe self-enactor to use a pattern-matching function with one case per construct in the language, but \nat the same time we aren t allowed to use E in a pattern! We overcome this dif.culty by applying the \ndictum of Sherlock Holmes: Eliminate all other factors, and the one which remains must be the truth. \nSherlock Holmes [11]. That is, by .rst giving cases for all the other .ve operators (includ\u00ading a dummy \ncase for B) we can infer the presence of E without naming it explicitly in a pattern. The resulting self-enactor \nin which E is handled implicitly is in Figure 4.  6.5 Correctness LEMMA 6.2. If v is a factorable form \nthen enact( ' v) -. *' v. Proof. The proof is by a straightforward case analysis on the nature of factorable \nforms, since none of the special cases of enact1 apply. 0 LEMMA 6.3. If t1 -. t2 then enact( ' t1) and \nenact( ' t2) have a common reduct. Proof. The proof is by induction on the length of the reduction. If \nt1 t2 then routine case analysis shows that there is a reduction enact ' t1 -.+ enact ' t2. For example, \nif t1 is EOOsr and t2 is s then ' t1 is the term BE (BO)(BO) ' s ' r and and so enact t1 -. * enact (EOO \n' sr) -. enact s. evalop (B O) -. * -. * -.+ unblock (enact (B O)) unblock (B O) (by LeO mma 6.2) ' ' \n' let rec enact = let unblock = B x -> x | x -> x in let evalop = x -> unblock (enact x) in let enact1 \n= B Y x1 -> enact (x1 (B Y x1)) | BK x2x1-> enact(K x2x1) | BS x3x2x1 ->enact (Sx3x2 x1) | B F x3 x2 \nx1 -> enact (F (evalop x3) x2 x1) | BB x4x3x2 x1->BBx4 x3x2x1 | Bx5x4x3x2x1 -> enact (x5 (evalop x4) \n(evalop x3) x2 x1) | x1 -> x1 in x2 x1 -> enact1 (enact x2 x1) | x1 -> x1 Figure 4. A Self-Enactor that \nHandles E Implicitly Again, if t1 is EOusr where u is a factorable form other than O and t2 is r then \n' t1 is the term BE (BO) ' u ' s ' r. If u is an operator O1 (other than O) then evalop (BO1) -. * O1 \nas '' ''' before, and so BE (BO) ' usr -. * EOO1 sr -. r as required. Similarly, if u is some compound \nthen evalop ' u ' '' reduces to a compound q and so BE (BO) usr -. * EOq ' s ' r -. ' r as required. \nIf t1 is an application r1 u1 and r1 -. r2 then, by induc\u00adtion, enact ' r1 and enact ' r2 have a common \nreduct r3. Hence ' '' enact t1 reduces to the term enact1 (enact r1 u1) which has a common reduct with \nenact1 (enact ' r2 ' u1) which is a reduct of enact ' (r2 u1). A similar argument applies if u1 -. u2. \nIf t1 is some F psr and p1 -. p2 then, by induc\u00adtion, enact ' p1 and enact ' p2 have a common reduct \np3. Thus enact( ' t1) and enact( ' t2) both reduce to enact (F (unblock p3) ' s ' r) . Similar arguments \napply if t1 is of the form Epsrq. LEMMA 6.4. Let t be a term. If enact ' t reduces to a factorable form \nv then v is a quotation of some t1 such that t -. * t1. Proof. The proof is by induction upon the length \nof the reduction to v. If t is an operator O then the only factorable form enact ' t can reduce to is \n' O as required. If t is an application ru then any reduction of enact ' t produces '' ' the term enact1 \n(enact ru). Now if enact t is to produce a factorable form then enact ' r must reduce to a factorable \nform which, by induction, is some ' v1 where v1 is factorable. Now consider the cases of enact1 in turn. \nIf v1 is Y then the whole reduces to enact ( ' u (BY ' u)) which is enact ' t1 where t1 = u (Yu) arises \nfrom the reduction of t. Now apply induction to t1. Similar arguments apply if v1 is of the form Kx2 \nor Sx3 x2. Suppose that v1 is of the form Fx3 x2. If the whole is to produce a factorable form then enact \n' x3 must produce a fac\u00adtorable form which, by induction, must be a quotation ' p1 where x3 -. * p1. \nIf p1 is an operator O then evalop ' p1 reduces to O and the whole reduces to enact ' x2 so induction \napplies as Fx3 x2 x1 -. x2. Alternatively, if p1 is a compound p2 p3 then the whole reduces to enact \n( ' u ' p2 ' p3) to which induction can be applied. Suppose that v1 is of the form Bx4 x3 x2. Then the \nwhole produces ' (v1 u) which is a quotation of a reduct of t. Suppose that v1 is of the form x5 x4 x3 \nx2. Then it must be that x5 is E. Now proceed as before.  Otherwise, v1 u is a factorable form and the \nwhole reduces to ' (v1 u) as required. 0 THEOREM 6.5. If t is a term such that enact ' t reduces to some \nfactorable form n then t reduces to some factorable form v. Con\u00adversely, if t reduces to some factorable \nform v then enact ' t re\u00adduces to ' v. Hence enact is a self-enactor for the blocking factori\u00adsation \ncalculus. Proof. If enact ' t has a factorable form, then apply Lemma 6.4. Conversely, suppose that t \n-. * v where v is factorable. By Lemma 6.3, enact ' t and enact ' v have a common reduct and Lemma 6.2 \nimplies that the latter term evaluates to ' v which is normal, as required. 0 7. Static Type System 7.1 \nOverview We approach typing from a Curry-style perspective, in that the terms are .xed in advance, with \ntypes merely used to describe terms. This has several consequences, which will be noted when appropriate. \nOur type system uses the types of System F [13], given by T ::= X | T . T |.X.T where X,Y Z,... are meta-variables \nfor type variables, and U and T are meta-variables for types. These are much simpler than those of F. \n*. The key enabler is the ability to factorise functions in situ, without rising a level in the type \nhierarchy. Each operator O other than E has a principal type Ty[O] of the form Y :(X . X) . X K : X \n. Y . X S :(X . Y . Z) . (X . Y ) . X . Z F : X . Y . (.Z.(Z . X) . Z . Y ) . Y B : X . X.  For later \nconvenience, these types are not quanti.ed, but the type variables are typically assumed fresh. The types \nfor Y, S and K are all standard. Indeed, there is an embedding of Curry-style System F [3] into the blocking \nfactori\u00adsation calculus. Hence, the undecidability of type inference for System F [39] carries over to \nhere. However, we have designed and implemented a partial type inference algorithm that can type check \nour self-interpreters and also catch some mistakes in self\u00adinterpreters. The operator B has type X . \nX. A consequence of the type of B is that our notion of quotation is type-preserving: if a program has \ntype T , then its representation has type T , too. This shows that different quotations may have different \ntypes. An alternative would be to follow Rendel, Ostermann, and Hofer and introduce a new type Expr T \nof program representations so that terms cannot be confused with their representations; we leave this \nfor future work. Following Jay and Given-Wilson [17], the type for F contains a quanti.ed argument type \n.Z.(Z . X) . Z . Y. The variable Z is used to represent the unknown type of the second component of a \ncompound. This is unnecessary when every pattern is headed by a constructor that determines the types, \nbut knowing that the pattern xz has type X conveys no information about the type Z of z. It is easy to \nspecify a type scheme for E, namely X . X . Y . Y . Y but this is not suf.ciently general to type the \npattern-matching functions of interest, as each case may have a type that specialises the default type \nwith respect to its pattern. For example, consider an extension of the form O . s | r where r : U . T \nand s : S. Its de-sugared form is . * x.E O xs (rx)= S(S(EO)(Ks))r. Now this should have type U . T so \ntake x : U. Then E must have type Ty[O] . U . S . T . T. Of course, this is type-safe if S = T but, following \nthe approach developed in pattern calculus [16], it is enough that any solution of Ty[O]= U also solves \nS = T . De.ne {T1 = T2} to be the most general uni.er of T1 and T2. This is computed in the obvious manner, \nusing a-conversion to align quanti.ed type variables. Returning to our example, S = {Ty[O]= U}T and so \nE must have the type E : Ty[O] . U .{Ty[O]= U}T . T . T for any operator O other than E. It is clear \nfrom this that E cannot have a principal type, and so is excluded from this analysis. This is good enough \nfor the Curry-style, but from the perspec\u00adtive of the Church-style, or types-as-propositions, this is \nall very ad hoc, and yet it is not clear how the situation might be better managed. The typing suggests \nthat E be replaced by a family of operators EO for each operator O. Yet each of these would in turn require \nan equality operator, even though they would not have prin\u00adcipal types. We leave such considerations \nto future work. There remains the challenge of typing patterns involving E itself. To date, we have not \nfound a technique that works, and so will con.ne attention to the self-enactor in Figure 4 in which E \ndoes not appear explicitly. A .nal issue concerns the typing of lambda-abstractions. When type-level \noperations are explicit in System F then the standard approach to instantiating a quanti.ed type asserts \nthat if t : .X.T then t : {U/X}T for any type U. However, given f : S ..X.T then the instantiation of \nX is achieved by .rst applying f to some fresh variable x : S then instantiating at U and .nally abstracting \nwith respect to x to get .x.f xU : S .{U/X}T . In the Curry-style, this becomes .x.f x : S .{U/X}T . \nMitchell [24], and later Remy [30], considered the consequences of adding .\u00adcontraction to System F. \nFor us, the situation is not quite the same, as . * x.f x is de.ned to be f. So we require a type-derivation \nrule of the form t : S ..X.T . t : S .{U/X}T More generally, we need a subsumption rule with respect \nto a type instantiation relation -which generalises the usual type manipula\u00adtions.  7.2 Typing Rules \nA context is given by a sequence . of type variables, so that the judgments take the form . f T1 -T2 \nwhich asserts that T2 is an instance of T1 in context .. For example, . f T -.X.T whenever X is not free \nin .. The rules for the type instantiation order -are given in Fig\u00adure 5, where FV(S) is the free type \nvariables of S. A type context G is a sequence of distinct, typed term variables x1 : T1,...,xn : Tn \nas usual. The free type variables FV(G) of G is the union of the free type variables of each type Ti \nappearing within it. The type derivation rules are given in Figure 6.  X . . . f.X.T -[U/X]T . f T -.X.T \n., FV(S) f T1 -T2 . f S2 -S1 . f S . T1 -S . T2 . f S1 . T -S2 . T Figure 5. Type Instantiation THEOREM \n7.1. If G f t : T and t -. u then G f u : T . Proof. Consider the reduction EOOst -. s. Since U is a \ntype for O and . = {U = Ty[O]} exists it follows that the domain of . can be limited to the free type \nvariables of Ty[O] so that t : {Ty[O]= U}T = T as required. That the other reduction rules preserve typing \nis routine. 0  7.3 Derived Typing Rules LEMMA 7.2. The following rule can be derived for abstractions \nG,x : U f t : T . G f . * x.t : U . T Proof. The proof is by induction on the structure of the type deriva\u00adtion \nfor t. If the last step in the derivation uses a type instanti\u00adation FV(G,x : U) f T1 -T then it follows \nthat FV(G) f U . T1 -U . T so induction applies. The remaining pos\u00adsibilities follow the structure of \nt. If t is x then T is U and so . * x.x = I : .X.X . X -U . U = U . T as required. If x is not free in \nt then . * x.t = Kt and G f Kt : U . T as required. If t is of the form rx where x is not free in r then \n. * x.r x is r. Now the type derivation for t ends with some G,x : U f r : U1 . T G,x : U f x : U1 . \nG,x : U f rx : T It follows that FV(G,x : U) f U -U1 and so FV(G,x : U) f U1 . T -U . T by contravariance \nof the order with respect to argument types, which yields the desired typing for r. Otherwise, if t is \nan application t1 t2 then there are types T1 and T2 such that G f t1 : T2 . T and G f t2 : T2. By two \napplications of induction, it follows that G f . * x.t1 : U . T2 . T and G f . * x.t2 : U . T2 whence \n. * x.t = S(. * x.t1)(. * x.t2) has type U . T as required. 0 The intended typing rules for extensions \ndepend upon patterns taking their most general types, as described by type judgments of the form .; B \nf p : P where . is as before and B (big beta) is a type context in which each term takes a mono-type. \nThe rules are presented in Figure 7. In the last rule it is implicit that the type variables in .1, .2 \nand X are distinct. Note that E does not have a principal type, and so cannot appear in patterns. THEOREM \n7.3. The following rule can be derived for extensions G f r : U . T .; B f p : P.(G,B) f s : .T . n (FV(G) \n. FV(U . T )) = {} . = {U = P } . G f p . s | r : U . T Proof. The proof is by induction upon the structure \nof p. If p is a variable then apply Lemma 7.2. If p is an operator O other than E then p . s | r = . \n* x.E O xs (rx): U . T as required. If p is an application p1p2 then p . s | r = . * x.F x (rx) '' ' \n(. * y.(p1 . (p2 . s | ry) | r ) y) where r = S(Kr). This has type U . T if there is derivation of G,y \n: Z . U f (p1 . (p2 . s | r ' y) | r ' ) y : Z . T x : T . G G f x : T G f O : Ty[O] FV(Ty[O]) n FV(U \n. T )= {} G f E : Ty[O] . U .{Ty[O]= U}T . T . T G f t : U . T G f u : U G f tu : T G f t : T1 FV(G) \nf T1 -T2 G f t : T2 Figure 6. Type Rules for Terms or, equivalently G,y : Z . U f p1 . (p2 . s | r ' \ny) | r ' :(Z . U) . Z . T. Now the typing of the pattern p1 p2 is of the form .1; B1 f p1 : P1 .2; B2 \nf p2 : P2 . = {P1 = P2 . X}. .1, .2; .(B1,B2) f p1 p2 : .X Hence, it is enough to prove that .1(G,y : \nZ . U, B1) f p2 . s | r ' y : .1(Z . T ) where .1 = {P1 = Z . U }. Since r ' y has the desired type, \nthis hold s if .2(.1(G,B1,B2)) f s : .2(.1T ) where .2 = {P2 = .1Z}. Further, we have the premise {U \n= .X}(.(G,B1,B2)) f s : {U = .X}(.(T )) . Hence, it is enough to show that the restrictions of the compositions \n.2 . .1 and {U = .X}. . to G,B1,B2 and T are the same. Now the former is the most general solution of \nP1 = Z . U and Z = P2 or, equivalently, of P1 = P2 . U and Z = P2. Similarly, the latter is the most \ngeneral solution of P1 = P2 . X and X = U or, equivalently, of P1 = P2 . U and X = U. As neither restriction \ninvolves X or Z it follows that both are simply {P1 = P2 . U}. 0 COROLLARY 7.4. The following rule can \nbe derived for extensions G f r : .X.X . X .; B f p : X G,B f s : X FV(G) n . = {}G f p . s | r : .X.X \n. X . Proof. Instantiate the type of r to be Y . Y for some fresh variable Y and apply the theorem with \n{Y = X} mapping Y to X. 0 Note that although the corollary above will be suf.cient to type our self-interpreters, \nit is not clear how to prove the corollary without .rst proving the more general theorem, since the typing \nof an extension with a compound pattern requires uni.cation to handle the quanti.ed type of the third \nargument of F .  7.4 Type Checking the Self-Interpreters THEOREM 7.5. We have G f t : T if and only \nif G f ' t : T . Proof. Each direction is straightforward by induction on t. 0 THEOREM 7.6. The function \nequal de.ned in Section 5 has typing \u00d8f equal : .X..Y.X . Y . Bool  X; x : X f x : X FV(Ty[O]) f O : \nTy[O] .1; B1 f p1 : P1 .2; B2 f p2 : P2 . = {P1 = P2 . X}.1, .2,X; .(B1,B2) f p1 p2 : .X Figure 7. Type \nRules for Patterns where Bool = .Z.Z . Z . Z. Proof. This is a consequence of derived type inference \nrule for extensions. The calculations are relatively straightforward since the body of each case has \nthe same type Bool. 0 THEOREM 7.7. The self-recogniser unquote de.ned in Section 5 has typing \u00d8 f unquote \n: .X.X . X . Proof. Apply Corollary 7.4. 0 THEOREM 7.8. The self-enactor de.ned in Figure 4 has type \n\u00d8 f enact : .X.X . X . Proof. The proof is by repeated applications of Corollary 7.4. 0 Notice that \nwe can easily type check self-applications of unquote and enact, such as enact( enact).  7.5 Adequacy \nA weakness of our approach is that the type system does not distinguish terms from their quotations. \nA more re.ned approach associates to each type T a new type form Expr T to type its quoted expressions. \nThen quotation is said to be adequate [31] if each closed normal form of type Expr T is the quotation \nof some term. Future work may well adapt the self-interpreters given here to make them adequate. In the \nmeantime, observe that there is not much scope for confusion, as there is a simple test for being a quotation, \ngiven by let rec isquote = B (x y) -> false | B x -> true | x y -> isquote x &#38;&#38; (isquote y) | \nx -> false where &#38;&#38; is conjunction. Similarly, given an interpretation of strings there is a \npretty printer for quotations given by let rec pretty_print = B Y -> \"Y\" | B K -> \"K\" | B S -> \"S\" | \nB F -> \"F\" | B E -> \"E\" | B B -> \"B\" | x y -> pretty_print x ^ \"(\" ^ pretty_print y ^ \")\" | x -> \"<not \na quotation>\" 8. Experimental Results We have two implementations of both reduction and desugaring, one \nin bondi [7] and one in Scheme. Type inference in the bondi interpreter con.rms that all exam\u00adples have \nthe expected types. For example, enact in Figure 4 has the same type as the polymorphic identity function. \nThe inference algorithm adapts the standard techniques by adding a rule for typ\u00ading extensions. This \nis delicate as it is not obvious how any type substitutions required to infer a type for the body of \nthe exten\u00adsion can be incorporated into the result without forcing the default to take the type of the \nspecial case. For our purposes, it is suf.\u00adcient to merely check the type of the body, without propagating \nany changes. In converting the (well-typed) extensions into combinators, it is worth adopting some optimisations \nwhen de-sugaring extensions with compound patterns. A common situation concerns px . s | r which, when \nde-sugared, reduces to S (SFr)(K(p . . * x.s | (S(Kr))) . Otherwise, when desugaring |pq . s | r the \ndefault term r appears three times. When this is inef.cient, the extension p . s | r will be interpreted \nby its \u00df-expansion (. * x.(p . s | x)) r to avoid the copying. After de-sugaring, unquote is: Y(S(K(S(S(K \nS)(S F))(S(K K)(S(K(S(S(E B)(K(S K K))))) (S(K S) K)))))(S(K(S(S F(S K K))))(S(K K)(S(S(K S) (S(K(S(K \nS)))(S(K K))))K)))) which is built from 50 operators. The combinator for enact is shown in Figure 8; \nit uses 1185 operators. Both work .ne in all our experiments, which have tested all of the cases in the \nextensions used in de.ning enact. 9. Related Work on Typed Self-Interpretation A major source of dif.culties \nfor static type checking is that pro\u00adgrams must be of function type, while their quotations must be data \nstructures, amenable to analysis. The issues are well illustrated by Naylor s [28] self-interpreter for \nHaskell that has type: [(FunId, Exp)] . Exp . The input is a list of function de.nitions that each pairs \na function identi.er with an expression, and the output is also an expression. The key thing to note \nis that the type Exp is a tagged union of integers, variables, abstractions, applications, etc: data \nExp = App Exp Exp | Lambda VarId Exp | Fun FunId | Var VarId | Int Int | Lam (Exp -> Exp) This type \nsupports pattern-matching of the traditional kind (driven by the structure of an algebraic data type) \nbut also brings some disadvantages too. Note that, although it is straightforward to de\u00adcide equality \nof the .ve forms of expression that are used to rep\u00adresent input programs, the presence of arbitrary \nHaskell functions (tagged by the constructor Lam) within expressions will complicate any analysis of \ninterpretations. More signi.cant for the typing is that the resulting quotation process gives all program \nrepresenta\u00adtions the same type Exp, which severely limits the usefulness of static type checking. The \nself-interpreter uses tagging and untag\u00adging operations at every step of computation, which amounts to \nlittle more than dynamic type checking. Others have used tags in a similar manner, including Rossberg \nin his self-interpreter [34] for Standard ML, and L\u00a8 aufer and Odersky [22] in their self-interpreter \nfor a typed version of the SK combi\u00adnator calculus. Taha, Makholm, and Hughes [36], and also Danvy and \nL\u00b4 opez [10], showed how to eliminate super.uous tags.  Y(S(S(S(KS)(S(KK)(S(S(KS)(S(KK)(S(K(S(K(S(SF(SKK))))))(S(K(S(KK)))(S(K(S(S(KS)(S(KK)(S(KS)K)))))K)))))(S(S(KS)(S(KK)(S \n(K(S(S(KS)(SF))))(S(K(S(KK)))(S(S(KS)(S(KK)(S(K(S(S(KS)(SF))))(S(K(S(KK)))(S(S(KS)(S(K(S(KS)))(S(S(KS)(S(K(S(KS)))(S(K \n(S(K(S(KS)))))(S(K(S(K(S(K(S(EB)))))))(S(K(S(K(S(KK)))))(S(S(KS)(S(KK)(S(KS)(S(KK)(S(KS)(S(K(S(EY)))(S(KK)(S(S(KS)K)(K \n(S(SKK)(BY)))))))))))(K(S(KS)K))))))))(K(S(KK)(S(KS)K))))))(K(K(SKK))))))))(K(S(KS)K)))))))(S(S(KS)(S(KK)(S(K(S(S(KS)( \nSF))))(S(K(S(KK)))(S(K(S(S(KS)(S(K(SF))(S(KS)K)))))(S(K(S(KK)))(S(S(KS)(S(KK)(S(K(S(S(KS)(SF))))(S(K(S(KK)))(S(S(KS)(S \n(K(S(KS)))(S(S(KS)(S(K(S(KS)))(S(K(S(K(S(KS)))))(S(K(S(K(S(K(S(EB)))))))(S(K(S(K(S(KK)))))(S(S(KS)(S(KK)(S(KS)(S(KK)(S \n(KS)(S(K(S(EK)))(S(KK)(S(S(KS)(S(KK)(S(KS)K)))(KK)))))))))(K(S(KS)K))))))))(K(S(KK)(S(KS)K))))))(K(K(SKK))))))))(K(S(K \nS)(S(KK)(S(KS)K)))))))))))(S(S(KS)(S(KK)(S(K(S(S(KS)(SF))))(S(K(S(KK)))(S(K(S(S(KS)(S(K(SF))(S(KS)K)))))(S(K(S(KK)))(S \n(K(S(S(KS)(S(K(SF))(S(KS)(S(KK)(S(KS)K)))))))(S(K(S(KK)))(S(S(KS)(S(KK)(S(K(S(S(KS)(SF))))(S(K(S(KK)))(S(S(KS)(S(K(S(K \nS)))(S(S(KS)(S(K(S(KS)))(S(K(S(K(S(KS)))))(S(K(S(K(S(K(S(EB)))))))(S(K(S(K(S(KK)))))(S(S(KS)(S(KK)(S(KS)(S(KK)(S(KS)(S \n(K(S(ES)))(S(KK)(S(S(KS)(S(KK)(S(KS)(S(KK)(S(KS)K)))))(KS)))))))))(K(S(KS)K))))))))(K(S(KK)(S(KS)K))))))(K(K(SKK)))))) \n))(K(S(KS)(S(KK)(S(KS)(S(KK)(S(KS)K)))))))))))))))(S(S(KS)(S(K(S(K(S(S(KS)(SF))))))(S(K(S(K(S(KK)))))(S(K(S(K(S(S(KS)( \nS(K(SF))(S(KS)K)))))))(S(K(S(K(S(KK)))))(S(K(S(K(S(S(KS)(S(K(SF))(S(KS)(S(KK)(S(KS)K)))))))))(S(K(S(K(S(KK)))))(S(S(KS \n)(S(K(S(KS)))(S(K(S(KK)))(S(K(S(K(S(S(KS)(SF))))))(S(K(S(K(S(KK)))))(S(S(KS)(S(K(S(KS)))(S(K(S(K(S(KS)))))(S(S(KS)(S(K \n(S(KS)))(S(K(S(K(S(KS)))))(S(K(S(K(S(K(S(KS)))))))(S(K(S(K(S(K(S(K(S(EB)))))))))(S(K(S(K(S(K(S(KK)))))))(S(S(KS)(S(K(S \n(KS)))(S(K(S(KK)))(S(K(S(KS)))(S(K(S(KK)))(S(K(S(KS)))(S(K(S(K(S(EF)))))(S(K(S(KK)))(S(S(KS)(S(KK)(S(KS)(S(KK)(S(KS)(S \n(KK)(S(KS)K)))))))(K(S(KF))))))))))))(K(K(S(KS)K))))))))))(K(K(S(KK)(S(KS)K))))))))(K(K(K(SKK))))))))))(K(K(S(KS)(S(KK \n)(S(KS)(S(KK)(S(KS)K)))))))))))))))(S(K(S(K(S(S(KS)(SF))(S(KK)(S(S(KS)(S(K(SF))(S(KS)K)))(S(KK)(S(S(KS)(S(K(SF))(S(KS) \n(S(KK)(S(KS)K)))))(S(KK)(S(S(KS)(S(K(SF))(S(KS)(S(KK)(S(KS)(S(KK)(S(KS)K)))))))(S(KK)(S(K(S(S(KS)(SF))(S(KK)(S(S(KS)(S \n(S(KS)(S(K(S(KS)))(S(K(S(K(S(EB)))))(S(K(S(KK)))(S(K(S(K(S(S(EB)(K(BB)))))))(S(KS)K))))))(S(KK)(S(KS)K))))(K(SKK)))))) \n(S(KS)(S(KK)(S(KS)(S(KK)(S(KS)(S(KK)(S(KS)K)))))))))))))))))))(S(K(S(K(S(SF(SKK))))))(S(K(S(KK)))(S(K(S(K(S(SF(S(K(SKK \n))))))))(S(K(S(KK)))(S(K(S(K(S(SF(S(K(S(K(SKK))))))))))(S(K(S(KK)))(S(K(S(K(S(SF(S(K(S(K(S(K(SKK))))))))))))(S(K(S(KK) \n))(S(K(S(K(S(SF(S(K(S(K(S(K(S(K(SKK))))))))))))))(S(K(S(KK)))(S(S(KS)(S(K(S(KS)))(S(K(S(K(S(EB)))))(S(K(S(KK)))(S(S(KS \n)(S(KK)(S(KS)(S(KK)(S(KS)(S(KK)(S(KS)(S(KK)(S(KS)(S(KK)(S(KS)K)))))))))))(K(S(S(KS)(S(K(S(KS)))(S(K(S(K(S(KS)))))(S(K( \nS(K(S(KK)))))(S(K(S(S(KS)K)))K)))))(S(KK)K))))))))(K(K(S(K(S(K(S(K(S(K(S(K(SKK))))))))))))))))))))))))))))))))(S(K(S(S \n(KS)K)))K))(K(S(SF(SKK))(K(S(S(EB)(K(SKK)))(S(K(SKK)))))))) Figure 8. enact de-sugared In the examples \nabove, tags arise as constructors of an algebraic data type of expressions. Certainly, our approach is \nnot tagged in this sense. Rather, intensionality is built into the calculus in a fundamental way. Since \nequality is decidable for normal forms, including operators, there is no need for any additional tagging. \nBefore these efforts, Hagiya [14] presented a self-interpreter for a .-calculus which implicitly de.nes \na type system and does dynamic type checking. While typing is unnecessary for self-interpretation in \ngeneral, we are inspired by typability-preserving compilers [27] that com\u00adpile a typed source program \nto a typed target program and enable implementers to catch compiler errors by type checking the target \nprogram. An entirely different approach to typed interpretation is to use polymorphic types instead of \na single universal type for all program representations. Signi.cant progress in this direction was made \nby Pfenning and Lee [29] who studied the polymorphic .-calculus and presented an interpreter for F2 written \nin F3 as well as an interpreter for F. written in F+ . , and by Carette, Kiselyov, and Shan [9] who wrote \ntagless interpreters in OCaml and Haskell for a simply-typed .-calculus. They showed the viability of \nprogram representations with polymorphic types, without presenting self-interpreters. The literature \ncontains just one example (as far as we know) of a self-interpreter for a statically-typed language without \na universal type. Rendel, Ostermann, and Hofer [31] presented a self-recogniser for F* ., which is an \nextension of the higher-order polymorphic .-calculus F. that has a type:type rule. The self\u00adrecogniser \nhas type: . X.(Expr X) . X Their notion of quotation is type-monomorphic in that if ' s and ' t have \nthe same type, then s and t have the same type too. Equiva\u00adlently, terms of distinct types yield representations \nof distinct types. The use of type-monomorphic quotation is a radical departure from the use of a single \nuniversal type for all program representations, where all ' s and ' t have the same type, irrespectively \nof the types open the problems of writing an equality checker and a self-enactor. Their results inspired \nour work. Rendel, Ostermann, and Hofer discuss the notion of typed self\u00adrepresentation, that is, representing \nterms of a programming lan\u00adguage in the language itself [31, Section 2], and list .ve desirable properties. \nLet us evaluate how many of those properties our lan\u00adguage, type system, and self-interpreters have. \nWe list each prop\u00aderty in this font, followed by our evaluation. 1. Representation. There is a family \nof types (Expr T ) such that ' t has type (Expr T ) if and only if t has type T . We use (Expr T )= T \nand the stated equivalence is our Theorem 7.5. 2. Adequacy. Every term s of type (Expr T ) corresponds \nto a term t of type T , which means that for every s as above there exists a t such that s = ' t. Our \nde.nition of quotation doesn t have this property; the reason is that we use (Expr T )= T so an unquoted \nterm of type T also has type (Expr T ). 3. First class interpretations. It is possible to express operations \non quoted terms so that they are well-typed for all terms of type (Expr T ), without the need to refer \nto any speci.c such terms.  Our language doesn t have this property; rather, we use pattern matching \npervasively. 4. Self interpretation. There is a family of contexts evalT () such that evalT ( ' t) is \nobservationally equivalent to t if t has type T . This property is essentially Equation (1) which is \nimplied by our Theorem 5.2. 5. Re.ection. ' t exhibits the intensional structure of t in a useful way. \nOur self-enactor is a good example of how we can make good use of the intensional structure of a quoted \nterm. In summary, our language, type system, and self-interpreters have three of those properties (1,4,5), \nwhile our language intentionally doesn t have property 3, and we leave property 2 for future work. The \napproach of Rendel, Ostermann, and Hofer can be charac\u00adterised as follows. A function at one level of \nthe type hierarchy can be tagged to become a data structure at the next level. This requires of s and \nt. They presented a self-recogniser (not strong), and left a countable sequence of levels. By contrast, \nthe ability to factorise means that functions (in normal form) are already data structures without any \nneed to tag them or shift levels. Hence, the types of System F are good enough. The other notable difference \nis that both applications of terms to types, and type abstractions, are explicit in their work but implicit \nhere. This saves us from having to factorise type applications, but at the cost of type inference being \nundecidable. 10. Future Work The use of factorisation to support self-interpreters raises many interesting \nquestions about foundations and self-interpretation, as well as several practical questions. When pattern-matching \nis driven by the de.nition of an alge\u00adbraic data type then it is easy to decide whether a pattern-matching \nfunction covers all cases, but here all cases must include ev\u00adery factorable form of the calculus. Such \nanalyses of coverage await development. In practice, such open-ended, or extensible functions prove quite \nuseful. For example, a pretty-printer of type .X.X . String may have default behaviour that produces \nan exception, but as new types are declared, new cases are added for the new term forms. It seems likely \nthat the calculus without the .xpoint operator Y is strongly normalising, for reasons similar to those \nfor query calculus [16] which extends System F with generic queries for searching and updating. A more \ninteresting challenge is whether the results in this paper can be applied to a strongly normalising calculus. \nAfter all, if the basic calculus is strongly normalising, why shouldn t the interpretations be so too? \nTo put it another way, can the Y operator be replaced by something that is strongly normalising? By contrast, \nthe denotational semantics of factorisation is quite undeveloped. For example, there is not yet an account \nof com\u00adpounds and atoms in category theory. Open questions within self-interpretation include the following. \nIs there a self-interpreter that is adequate, in the sense of Rendel, Ostermann, and Hofer? This seems \nplausible, at the price of making everything somewhat more obscure. Is there a self-interpreter for a \nlanguage with decidable type-checking? This is a harder question, since it is not clear how to factorise \nthe application of a term to a type. Practical questions include the following. Does the calculus ad\u00admit \nan ef.cient implementation? This seems plausible, since fac\u00adtorisation is a formalisation of the car \nand cdr of Lisp. Can these techniques be applied to .-abstractions without .rst converting to combinators? \nHow easy is it to adapt the given self-interpreters to explore alternative interpretations, e.g. to handle \nclosures? 11. Conclusion The blocking factorisation calculus is statically typed and supports a quotation \nmechanism that preserves types and supports both a typed self-recogniser and a typed self-enactor. Building \non the ground-breaking work of Rendel, Ostermann, and Hofer, it brings the status of self-interpreters \nfor typed calculi close to the standard set for pure .-calculus by Mogensen and then Berarducci and Bohm. \nFuture work may develop strong self-recognisers and self\u00adenactors, and support types of the form Exp \nT for representations that are distinct from the type T of source programs. The self-recogniser and self-enactors \ndeveloped for the block\u00ading factorisation calculus have a very natural development as pattern-matching \nfunctions. Each evaluation rule becomes a case of the one-step reducer enact1 with the evaluation strategy \ncaptured by the nature of the recursion within which this is embedded. It will be easy enough to modify \nthe strategy, or the reduction rules to suit evolving tastes. In a sense, all self-interpreters can be \nseen as encodings of such pattern-matching functions. Further, we anticipate using this approach to model \nvarious pro\u00adgram transformations, e.g. to produce code in continuation-passing style, and also evaluation \nstrategies involving, say, closures. In gen\u00aderal, this work opens up new possibilities for the interpretation \nof typed programming languages during compiler construction. More generally, this work illustrates some \nof the expressive power that the pattern-matching approach brings to bear when one is able to analyse \ninternal structure with the same facility used to apply functions. Acknowledgments. The second author \nthanks Oleg Kiselyov, Torben Mogensen, Klaus Ostermann, Frank Pfenning, Andreas Rossberg, Jeff Siskind, \nAaron Stump, Walid Taha, and Mitch Wand for discussions about the state of the art of self-interpretation. \nWe thank Thomas Given-Wilson, Shu-yu Guo, Mohsen Lesani, Todd Millstein and Jose Vergara for helpful \ncomments on a draft of the paper. References [1] Harold Abelson, Gerald Jay Sussman, and Julie Sussman. \nStructure and Interpretation of Computer Programs. MIT Press, 1985. [2] Henk Barendregt. Self-interpretations \nin lambda calculus. J. Funct. Program, 1(2):229 233, 1991. [3] HP Barendregt. Handbook of Logic in Computer \nScience (vol. 2): Background: Computational Structures: Abramski,S (ed), chapter Lambda Calculi with \nTypes. Oxford University Press, Inc., New York, NY, 1993. [4] Michel Bel. A recursion theoretic self \ninterpreter for the lambda\u00adcalculus. http://www.belxs.com/michel/#sel.nt. [5] Alessandro Berarducci and \nCorrado B\u00a8ohm. A self-interpreter of lambda calculus having a normal form. In CSL, pages 85 99, 1992. \n[6] Mathieu Boesp.ug. From self-interpreters to normalization by evalu\u00adation. In Olivier Danvy, editor, \nProceedings of Workshop on Normal\u00adization by Evaluation, 2009. [7] bondi programming language. www-staff.it.uts.edu.au/~cbj/ \nbondi. [8] Reg Braithwaite. The signi.cance of the meta-circular inter\u00adpreter. http://weblog.raganwald.com/2006/11/signi.cance-of-meta\u00adcircular \n22.html, November 2006. [9] Jacques Carette, Oleg Kiselyov, and Chung chieh Shan. Finally tagless, partially \nevaluated: Tagless staged interpreters for simpler typed languages. Journal of Functional Programming, \n19(5):509 543, 2009. [10] Olivier Danvy and Pablo E. Mart\u00b4inez L\u00b4opez. Tagging, encoding, and Jones optimality. \nIn Proceedings of ESOP 03, European Symposium on Programming, pages 335 347. Springer-Verlag (LNCS), \n2003. [11] Sir Arthur Conan Doyle. The Sign of the Four. Lippincott s Monthly Magazine, February 1890. \n[12] Brendan Eich. Narcissus. http://mxr.mozilla.org/mozilla/ source/js/narcissus/jsexec.js, 2010. [13] \nJ-Y. Girard, Y. Lafont, and P. Taylor. Proofs and Types. Tracts in Theoretical Computer Science. Cambridge \nUniversity Press, 1989. [14] Masami Hagiya. Meta-circular interpreter for a strongly typed lan\u00adguage. \nJournal of Symbolic Computation, 8(6):651 680, 1989. [15] R. Hindley and J.P. Seldin. Introduction to \nCombinators and Lambda\u00adcalculus. Cambridge University Press, 1986. [16] Barry Jay. Pattern Calculus: \nComputing with Functions and Struc\u00adtures. Springer, 2009. [17] Barry Jay and Thomas Given-Wilson. A combinatory \naccount of internal structure. Journal of Symbolic Logic, 2011. To appear. http://www-staff.it.uts.edu.au/~cbj/Publications/ \nfactorisation.pdf.  [18] Barry Jay and Delia Kesner. First-class patterns. Journal of Functional Programming, \n19(2):191 225, 2009. [19] C.B. Jay. The pattern calculus. ACM Transactions on Programming Languages and \nSystems (TOPLAS), 26(6):911 937, November 2004. [20] S.C. Kleene. Introduction to Methamathematics. van \nNostrand, 1952. [21] Stephen C. Kleene. .-de.nability and recursiveness. Duke Math. J., pages 340 353, \n1936. [22] Konstantin L\u00a8aufer and Martin Odersky. Self-interpretation and re.ec\u00adtion in a statically \ntyped language. In Proceedings of OOPSLA Work\u00adshop on Re.ection and Metalevel Architectures. ACM, October \n1993. [23] Oleg Mazonka and Daniel B. Cristofani. A very short self-interpreter. http://arxiv.org/html/cs/0311032v1, \nNovember 2003. [24] J.C. Mitchell. Polymorphic type inference and containment. Informa\u00adtion and Computation, \n1985. [25] Torben \u00c6. Mogensen. Ef.cient self-interpretations in lambda calcu\u00adlus. Journal of Functional \nProgramming, 2(3):345 363, 1992. See also DIKU Report D 128, Sep 2, 1994. [26] Torben \u00c6. Mogensen. Linear-time \nself-interpretation of the pure lambda calculus. Higher-Order and Symbolic Computation, 13(3):217 237, \n2000. [27] Greg Morrisett, David Walker, Karl Crary, and Neal Glew. From Sys\u00adtem F to typed assembly \nlanguage. In Proceedings of POPL 98, 25th Annual SIGPLAN SIGACT Symposium on Principles of Programming \nLanguages, pages 85 97, 1998. [28] Matthew Naylor. Evaluating Haskell in Haskell. The Monad.Reader, 10:25 \n33, 2008. [29] Frank Pfenning and Peter Lee. Metacircularity in the polymorphic .-calculus. Theoretical \nComputer Science, 89(1):137 159, 1991. [30] Didier R\u00b4emy. Simple, partial type-inference for System F \nbased on type-containment. In Proceedings of the tenth ACM SIGPLAN in\u00adternational conference on Functional \nprogramming, ICFP 05, pages 130 143, New York, NY, USA, 2005. ACM. [31] Tillmann Rendel, Klaus Ostermann, \nand Christian Hofer. Typed self\u00adrepresentation. In Proceedings of PLDI 09, ACM SIGPLAN Confer\u00adence on \nProgramming Language Design and Implementation, pages 293 303, June 2009. [32] John C. Reynolds. De.nitional \ninterpreters for higher-order program\u00adming languages. In Proceedings of 25th ACM National Conference, \npages 717 740. ACM Press, 1972. The paper later appeared in Higher-Order and Symbolic Computation, 11, \n363 397 (1998). [33] Armin Rigo and Samuele Pedroni. Pypy s approach to virtual machine construction. \nIn OOPSLA Companion, pages 044 953, 2006. [34] Andreas Rossberg. HaMLet. http://www.mpi-sws.org/ ross\u00adberg/hamlet, \n2010. [35] Fangmin Song, Yongsen Xu, and Yuechen Qian. The self-reduction in lambda calculus. Theoretical \nComputer Science, 235(1):171 181, March 2000. [36] Walid Taha, Henning Makholm, and John Hughes. Tag \nelimination and Jones-optimality. In Proceedings of PADO 01, Programs as Data Objects, Second Symposium, \npages 257 275, 2001. [37] Terese. Term Rewriting Systems, volume 53 of Tracts in Theoretical Computer \nScience. Cambridge University Press, 2003. [38] John Tromp. Binary lambda calculus and combinatory logic. \nIn Kolmogorov Complexity and Applications, 2006. A Revised Version is available at http://homepages.cwi.nl/ \ntromp/cl/LC.pdf. [39] J. B. Wells. Typability and type checking in the second-order .\u00adcalculus are equivalent \nand undecidable. In Proceedings of LICS 94, Ninth Annual IEEE Symposium on Logic in Computer Science, \n1994. [40] Wikipedia. Pypy. http://en.wikipedia.org/wiki/PyPy, 2010. [41] Wikipedia. Rubinius. http://en.wikipedia.org/wiki/Rubinius, \n2010. [42] Tetsuo Yokoyama and Robert Gl\u00a8uck. A reversible programming lan\u00adguage and its invertible self-interpreter. \nIn Proceedings of PEPM 07, ACM Symposium on Partial Evaluation and Semantics-Based Pro\u00adgram Manipulation, \n2007.    \n\t\t\t", "proc_id": "2034773", "abstract": "<p>Self-interpreters can be roughly divided into two sorts: self-recognisers that recover the input program from a canonical representation, and self-enactors that execute the input program. Major progress for statically-typed languages was achieved in 2009 by Rendel, Ostermann, and Hofer who presented the first typed self-recogniser that allows representations of different terms to have different types. A key feature of their type system is a type:type rule that renders the kind system of their language inconsistent.</p> <p>In this paper we present the first statically-typed language that not only allows representations of different terms to have different types, and supports a self-recogniser, but also supports a self-enactor. Our language is a factorisation calculus in the style of Jay and Given-Wilson, a combinatory calculus with a factorisation operator that is powerful enough to support the pattern-matching functions necessary for a self-interpreter. This allows us to avoid a type:type rule. Indeed, the types of System F are sufficient. We have implemented our approach and our experiments support the theory.</p>", "authors": [{"name": "Barry Jay", "author_profile_id": "81317495121", "affiliation": "University of Technology, Sydney, Australia", "person_id": "P2801414", "email_address": "Barry.Jay@uts.edu.au", "orcid_id": ""}, {"name": "Jens Palsberg", "author_profile_id": "81100375570", "affiliation": "University of California, Los Angeles, USA", "person_id": "P2801415", "email_address": "palsberg@ucla.edu", "orcid_id": ""}], "doi_number": "10.1145/2034773.2034808", "year": "2011", "article_id": "2034808", "conference": "ICFP", "title": "Typed self-interpretation by pattern matching", "url": "http://dl.acm.org/citation.cfm?id=2034808"}