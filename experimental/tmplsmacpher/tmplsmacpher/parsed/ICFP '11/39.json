{"article_publication_date": "09-19-2011", "fulltext": "\n A Kripke Logical Relation for Effect- Based Program Transformations Jacob Thamsborg IT University of \nCopenhagen, Denmark thamsborg@itu.dk Abstract We present a Kripke logical relation for showing the correctness \nof program transformations based on a type-and-effect system for an ML-like programming language with \nhigher-order store and dynamic allocation. We show how to use our model to verify a number of interesting \nprogram transformations that rely on effect annotations. Our model is constructed as a step-indexed model \nover the standard operational semantics of the programming language. It extends earlier work [7, 8] that \nhas considered, respectively, dy\u00adnamically allocated .rst-order references and higher-order store for \nglobal variables (but no dynamic allocation). It builds on ideas from region-based memory management \n[21], and on Kripke logical re\u00adlations for higher-order store, e.g. [12, 14]. Our type-and-effect system \nis region-based and includes a region-masking rule which allows to hide local effects. One of the key \nchallenges in the model construction for dynamically allo\u00adcated higher-order store is that the meaning \nof a type may change since references, conceptually speaking, may become dangling due to region-masking. \nWe explain how our Kripke model can be used to show correctness of program transformations for programs \nin\u00advolving references that, conceptually, are dangling. Categories and Subject Descriptors D.3.1 [PROGRAMMING \nLANGUAGES]: Formal De.nitions and Theory; F.3.1 [LOGICS AND MEANINGS OF PROGRAMS]: Specifying and Verifying \nand Reasoning about Programs General Terms Languages, Theory, Veri.cation Keywords Kripke logical relation, \nultrametric spaces, step-indexed model, type and effect system, program transformations, effect masking, \ndangling pointers 1. Introduction A type system for a programming language classi.es programs ac\u00adcording \nto properties that the programs satisfy. An effect system is a type system that, in particular, classi.es \nprograms according to which side effects the programs may have. A variety of effect systems have been \nproposed for higher-order programming lan\u00adguages, e.g., [15, 18, 21], see [17] for a recent overview. \nEffect Permission to make digital or hard copies of all or part of this work for personal or classroom \nuse is granted without fee provided that copies are not made or distributed for pro.t or commercial advantage \nand that copies bear this notice and the full citation on the .rst page. To copy otherwise, to republish, \nto post on servers or to redistribute to lists, requires prior speci.c permission and/or a fee. Lars \nBirkedal IT University of Copenhagen, Denmark birkedal@itu.dk systems can often be understood as specifying \nthe results of a static analysis, in the sense that it is possible to automatically infer types and effects. \nEffect systems can be used for different purposes: they were originally proposed by Lucassen and Gifford \n[18] for paral\u00adlelization purposes but they have also, e.g., been used as the basis for implementing \nML using a stack of regions for memory man\u00adagement [9, 21]. In a recent series of papers, Benton et. \nal. have argued that another important point of effect systems is that they can be used as the basis \nfor effect-based program transformations, e.g., compiler optimizations, [5 8]. The idea is that certain \npro\u00adgram transformations are only sound under additional assumptions about which effects program phrases \nmay, or rather may not, have. For example, in a higher-order language with references it is only sound \nto hoist an expression out a lambda abstraction if it is known that the expression neither allocates \nnew references, nor reads or writes references. While it is intuitively clear that effect information \nis important for validating program transformations, it is surprisingly challeng\u00ading to develop semantic \nmodels that can be used to rigorously jus\u00adtify effect-based transformations. In earlier work, Benton \net. al. de\u00adveloped semantic relational models of effect systems for a higher\u00adorder language with dynamically \nallocated .rst-order references (ground store) [7] and for a higher-order language with global vari\u00adables \nfor higher-order store (but no dynamic allocation) [8]. In this paper we present a Kripke logical relations \nmodel of a region-based effect system for a higher-order language with higher\u00adorder store and dynamic \nallocation, i.e., with general ML-like refer\u00adences. As pointed out in [8], this is a particularly challenging \nexten\u00adsion and one that is important for soundness of effect-based trans\u00adformations for realistic ML-like \nlanguages. We now explain what the main challenges are; in Section 3 we give an intuitive overview of \nhow we address these challenges. The main challenge arises from effect masking: Our region\u00adbased effect \nsystem includes an effect masking rule that allows one to hide local uses of effects. This makes it possible \nto view a com\u00adputation as pure even if it uses effects locally and makes the ef\u00adfect system stronger \n(it can justify more program transformations). To model effect masking we need to model references that, \ncon\u00adceptually speaking, become dangling because they point into a re\u00adgion that is masked away. We say \nconceptually speaking because in the real operational semantics there is no deallocation of refer\u00adences \n(the operational semantics is completely standard); but in our model we have to reason as if regions \ncould actually be allocated and deallocated. Here is a simple concrete example: Consider the following \nexpression e, typed as indicated: let x = ref. 7 in let y = refs x in ICFP 11, September 19 21, 2011, \nTokyo, Japan. {wrs} Copyright &#38;#169; 2011 ACM 978-1-4503-0865-6/11/09. . . $10.00 .z.y := x : 1 . \n1, {als} c  The program allocates two references, binds them to x and y, and then returns a trivial \nfunction from unit to unit that assigns x to y. The types indicate that x will be bound to a location \nin region ., say location 0, and that y will be bound to a location in region s, say location 1. At location \n0 the value 7 is stored and at location 1 the location 0 is stored. The so-called latent effect {wrs} \nof the function type of the whole expression describes which effects the function may have when called. \nFinally the entire expression has the effect {als} as it allocates a location in region s. It performs \nallocation in region . as well, but this effect has been masked out: since no . appears in the return \ntype, the expression cannot leak location 0, and we can, conceptually, deallocate all locations in region \n. once the computation has run; this is what the masking rule captures. This particular example, however, \nwas chosen to stress test the masking rule, as location 0 is in fact leaked: it is in the function closure \nthat the expression returns. The function neither reads not writes location 0, but it does write it to \nthe heap; our model must be able to cope with such conceptually dangling pointers. On the level of types, \nthe meaning of the type ref. int of x changes: after the allocation of values for x and y, it contains \nlocation 0, but what should it contain after region . has been masked out? If, e.g., ref. int is taken \nto be empty, then the function is most likely no longer well-typed. We explain our answer to this question \nin Section 3. Note that the effect annotations in the types are just annotations; the operational semantics \nis completely standard and regions only exist in our semantic model, not in the operational semantics. \nFur\u00adther note that the issues in the above example do not arise for a language with only ground store. \nAnother challenge arises from the fact that since our language includes dynamically allocated general \nreferences, the existence of the logical relation is non-trivial; in particular, the set of worlds must \nbe recursively de.ned. Here we build on our earlier work [12] and de.ne the worlds as a solution to a \nrecursive metric-space equation. One may worry whether our resulting model now becomes much more complicated \nthan the earlier, already non-trivial, mod\u00adels of Benton et. al. [7, 8]. As we shall explain in Section \n8, that is not the case. Indeed, our model is arguably a bit simpler even though it applies to a richer \nlanguage. Moreover, it also becomes simpler to verify equivalences using the model. Our relational model \nis built directly over the operational se\u00admantics, using our metric approach to step-indexing. We could \nalso have de.ned the model using a denotational model of the program\u00adming language; see the discussion \nin Section 8 here we preferred the operational approach because it is perhaps more widely acces\u00adsible. \nWe use our model to validate a number of interesting effect\u00adbased program transformations that, to the \nbest of our knowledge, have not been proved correct before, see Section 7. To focus the presentation \non the core challenges, we here con\u00adsider a monomorphically typed higher-order programming lan\u00adguage \nwith general references, but leave out universal and exis\u00adtential types as well as recursive types. However, \nwe want to stress that since our semantic techniques (step-indexed Kripke logical re\u00adlations over recursively \nde.ned worlds) do indeed scale well to uni\u00adversal, existential, and recursive types, e.g. [12, 14], it \nis straight\u00adforward to extend our model to a language with such types.1 We conjecture that it is also \npossible to extend our model to richer effect systems involving region and effect polymorphism, but we \nhave not done so yet. 1 With type abstractions as values, we would have a latent effect on a polymorphic \ntype, which would thus be of the form .ea.t. Throughout the paper we aim to explain the underlying intu\u00aditions \nof the technical de.nitions and how the different concepts interact. In particular, we have included \na couple of selected proof cases, which serve to illuminate central points. A version of the pa\u00adper with \nappendices containing more proof cases can be found on\u00adline at www.itu.dk/people/thamsborg/longcarnival.pdf. \n2. Language and Effect System The expressions and values of our model language are de.ned in the grammar \nbelow. We use e to range over the set of expressions E, v to range over the set of values V, l to range \nover a countably in.nite set of locations L, and n to range over integers. e ::= x | l | n | () | (e1,e2) \n| p1 e | p2 e | fix f(x).e | e1e2 |ref e | e1 := e2 | ! e v ::= l | n | () | (v1,v2) | fix f(x).e Note \nthat expressions are not explicitly typed (we could also have worked with an explicitly typed language). \nThe operational semantics is given by a small-step operational semantics in the standard way. We omit \nthe full de.nition; suf.ce it to say that heaps H are .nite maps from locations to closed values, and, \nfor e .E, jl h .H and j = 0, we write (e, h).(e,hl) if (e, h) reduces to l (e,hl) in j steps, counting \nall reduction steps. We write irr(e, h) to state that no further reductions of (e, h) are possible, either \nbecause e is in fact a value or because we are stuck. To make some of the later proofs a little bit simpler, \nwe assume that the allocation of a new location by evaluation of ref e is deterministic. Runtime errors \n(such as trying to look up a non-existing location or trying to apply a number as a function) are modelled \nby the evaluation getting stuck. Our effect system ensures that programs are well-typed in the standard \nsense of not getting stuck and, moreover, tracks depen\u00addencies and side-effects of computations on the \nheap. The typing rules for our effect system are given in Figure 1. Types are de.ned by the following \ngrammar: e t ::= int | 1 | t1 \u00d7 t2 | t1 . t2 | ref. t, where . ranges over a countably in.nite set of \nregion variables RV and e is ranges over the set of effects. An effect is a .nite set of primitive effects, \nand a primitive effect is of one of the forms rd., wr., or al.. The primitive effect rd. speci.es a read \neffect on region .. Likewise wr. speci.es a write effect and al. speci.es an allocation effect (that \nlocations may be allocated in region .). e Note that, as usual, the function type t1 . t2 includes a \nlatent effect e; this is the effect that the function may have when called. The effect type judgement \nde.ned in Figure 1 takes the form . | G f e : t, e, with G a .nite map from program variables to types, \ne an expres\u00adsion, t a type, e an effect, and . a .nite set of region variables, including all the region \nvariables in G, t, and e. The effect system is fairly standard. Note the inclusion of the effect masking \nrule T-MASKING; here e - . is de.ned as e - {rd., wr., al.} and .,. denotes the union of . and {.}. We \nwrite FRV(G,t ) for the region variables in the types of G and in t . The masking rule expresses that \nto mask out primitive effects on . from e, the region variable . must not be free in G, nor in t . The \nidea is that in this case the remaining part of the computation cannot access . and thus we may hide \nthe effects on .. Also note that there is a standard notion of effect subtyping, given by .nite set inclusion, \nwhich also yields a notion of subtyping (de.ned by the last three rules in the .gure).  . | G f e1 : \nref.t, e1 . | G f e2 : t, e2 . | G f e : t,e . . . . | G f e1:=e2 : 1,e1 . e2 .{wr.} . | G f ref e : \nref.t, e .{al.} . | G f e1 : t1,e1 . | G f e2 : t2,e2 . | G f e : t1 \u00d7 t2,e . | G f e : t1 \u00d7 t2,e . | \nG f (e1,e2): t1 \u00d7 t2,e1 . e2 . | G f p1 e : t1,e . | G f p2 e : t2,e ee . | G,f:t1 . t2,x:t1 f e : t2,e \n. | G f e1 : t1 . t2,e1 . | G f e2 : t1,e2 e . | G f fix f(x).e : t1 . t2, \u00d8 . | G f e1e2 : t2,e . e1 \n. e2 .,. | G f e : t,e ./. FRV(G,t) T-MASKING . | G f e : t, e - . . | G f e : t1,e1 . | t1 = t2 e1 \n. e2 T-SUB . | G f e : t2,e2 . | t1 = tl . | t2 = tl . | t1 l = t1 . | t2 = tl e1 . e2 12 2 e1e2 . \n| t = t . | t1 \u00d7 t2 = t1 l \u00d7 t2 l . | t1 . t2 = tl . tl 12 Figure 1. Effect System Consider the example \nexpression e from the introduction. Note that it is well-typed (let-expressions are de.nable as usual) \nsince {wrs} the judgement {s}|\u00d8f e : 1 . 1, {als} is derivable in the effect system. The last rule applied \nis the T-MASKING rule, which allows us to hide the local effect on .. Speci.cally, we apply the masking \nrule like this: {wrs} {s, .}|\u00d8f e : 1 . 1, {al., als} {wrs} {s}|\u00d8f e : 1 . 1, {als} Thus we hide the \nallocation and use of the local reference bound to x. Effect masking makes it possible to do more optimizations: \nConsider the familiar example of an ef.cient implementation .b of the .bonacci function using two local \nreferences. We can use the \u00d8 masking rule to give it type and effect int . int, \u00d8. This allows us to \nview the imperative implementation as pure, and thus, e.g, by Theorem 7.1 we .nd that it is sound to \noptimize two identical calls to .b into one call. This sounds like a simple optimization, but the point \nis that a compiler can perform it automatically, just based on the effect types and our model justi.es \nthat it is sound to do so. See [7, 8] for more examples. Note though, that these earlier works either \nhave ground store (only integers in memory) or no dynamic allocation, so many non\u00adtrivial uses of memory \ncannot be expressed. By contrast, we can write, say, a function that sorts a (functional) list of integers \nby doing an internal (imperative) heap sort, i.e., by building a heap in fresh memory, and returning \na sorted list. Using the masking rule, we could type such a function as a pure function with no effects, \nand would know, e.g., that sorting the same list twice serves no purpose. 3. Overview of the Technical \nDevelopment Our point of departure is the Kripke logical relation reading of it knows the types of values \nstored at the allocated locations of the heap.2 We then index type interpretations by worlds; this makes \nit possible to interpret ref t as the set of allocated locations that hold values of type t. A computation \nhas the precondition that all locations hold well-typed values according to the world, and this property \nmust be re-established after running the computation, i.e., it also serves as a postcondition . As more \nlocations are allo\u00adcated, the world grows and the interpretation of types is set up to grow as well: \nit is a crucial property that interpreting types in future worlds yields more (or at least no fewer) \nvalues, this we refer to as type monotonicity. Barring the masking rule, it is a mostly straightforward, \nif lengthy, exercise to extend this approach to the present region\u00adbased type-and-effect system. The \nworlds have to be partitioned into regions and interpreting ref. t only considers region . of the world \nfor locations that hold values of type t. Also, computations now have effects and hence their behavior \nis more restricted: as precondition they only assume well-typedness of values at loca\u00adtions in the regions \nwith read effects; as postcondition they are required to have performed only well-typed writes and allocations \nand, importantly, only in such regions as permitted, respectively, by the write and allocation effects. \nThe masking rule, however, introduces a new dimension to the development of worlds: in addition to adding \nnew locations with types to existing regions as sketched above, we may now introduce new regions as well \nas mask out existing ones. One can loosely think of masking out region . as deallocating the locations \nof that region. A slightly revised intuition is that we just stop caring about region .: we expect never \nto touch it again and so we may safely relinquish control. On the level of worlds, we discard the region, \ni.e., lose the locations and the associated types. Whether the locations are actually deallocated or \njust .oat around in the surroundings does not matter: from our point of view they are gone. As argued \nby example above, this leaves us the issue of dangling pointers. Phrased more concretely: how should \nthe interpretation of types for an ML-like programming langauge that has been explored 2 In general, \nworlds may contain complex invariants of the heap; they extensively by the authors and others [2, 11, \n12, 14]. The common may even vary over time and may thus be represented by state transition approach \nis to augment the semantics be it operational or deno-systems [14]. Here, we need only the invariant \nthat values stored at locations tational with worlds that keep track of the layout of the heap: belong \nto certain types. types cope with region masking? What is, say, the interpretation of ref. t in a world \nwhere region . has been masked out? A natural choice is the empty set after all, we know nothing about \nregion . so what locations could we possibly choose. We, however, take a different approach: We generally \ninterpret a type as all values with the properties you expect from that type. The more properties, the \nfever values and vice versa. A value v in ref. t is a reference to a location that we used to but no \nlonger control. The value at the location may have changed or it may even have been deallocated: v is \nconceptually dangling. What can we reasonably do with a dangling pointer? We can neither read nor write \nit, indeed, we can do only things that go for all values, e.g., put it into a pair, project it out, etc. \nIn other words, we expect no properties of dangling pointers and correspondingly interpret ref. t as \nthe set of all values. We take a similar stand on functions: interpreting a function type that has effects \non masked out regions gives the set of all values; such functions are somehow dangling too. Running such \na function relies on preconditions outside our control, so we make no promises about the behavior. This \napproach goes well with type monotonicity: the interpreta\u00adtion of types should not shrink under masking \nand, indeed, mask\u00ading out region . enlarges ref. t to hold all values. There is a catch, though, since \nfuture worlds may introduce new regions as well, and reintroducing region . would not do. This is obvious \nfrom type monotonicity; a more conceptual explanation, however, is this: As\u00adsume that some function reads \nregion . and that region . holds location l that stores values of some type. To run the function, we \nneed to establish the precondition that a value of the proper type is stored at location l, because the \nfunction could very well read l. We then mask out region ., losing all information about l in the process, \nand afterwards reintroduce it. To run the function now, we verify that all locations in region . hold \nvalues of appropriate type this is, after all, the precondition of the function but as l is no longer \nin ., we cannot expect proper behavior of the function. We solve this issue by tracking, in the worlds, \nthe regions that have been masked out and prohibit their reintroduction. One viewpoint is that, as the \nworld develops over time, a region goes through a life-cycle: Initially, it is unknown to the world, \nbut at some point it gets initialized, joining the set of live regions. With further development of the \nworld, locations with associated types are added to the region. This proceeds until the region is masked \nout; it loses all content and is moved to the set of dead regions. And this is a one-way street: once \nyou are gone, you can t come back. Local Reasoning Conceptually, a world does not describe the entire \nheap, just the part of it that the program sees or controls. On the level of de.nitions, this comes down \nto a frame property of our computations: a computation in a world runs in a world-adhering heap extended \nwith a frame, and the latter remains unchanged. In addition, a computation may allocate locations that \nare not tracked by the ensuing world; these locations are conceptually transferred to the surroundings, \nthey become part of the frame of the following computation. This is the fate of locations in regions \nthat are masked out. In summa, we achieve a form of local reasoning by quantifying over frames, similarly \nto models of separation logic for higher\u00adorder languages [10]. 4. Metric Spaces and Type-World Circularity \nAs argued above, we intend to augment our semantics with worlds that track the layout of the heap; hence \nwe are faced with the construction of such worlds. De.ning them directly will not do since, loosely, \na world holds semantic types whilst semantic types are parameterized over worlds; this is the type-world \ncircularity observed already by Ahmed in her thesis [1]. In recent work, Reus, Schwinghammer, St\u00f8vring, \nYang and the authors [12] have proposed a general solution to such circularities, applying metric-space \ntheory. The notion of worlds we require here is suf.ciently simple that this solution is applicable off-the-shelf, \nso to speak. So we omit the machinery of the construction here and just present the worlds that is the \nresult; after all, this is presently our object of interest. Details are given in the appendix of the \nlong version of the paper. In addition, we will largely ignore the fact that we actually deal in metric \nspaces and not just plain sets. We emphasize that this is just for presentation purposes, worlds and \ntypes are metric spaces with certain properties; this is necessary to solve the circularity and must \nbe taken into account, e.g., when interpreting types, see the appendix of the long version of the paper. \nWe face an additional challenge here: As described above, we intend to track dead regions to avoid recycling \nthem. But if an expression is well-typed by the masking rule, then in terms of the development of worlds, \nit initializes and eventually masks out the very same region on each evaluation, and maybe we would like \nto run it more than once. Our solution to this is to introduce a layer of indirection, following ideas \nof earlier work [9, 21]: On the level of syntactic types, we have the region variables RV introduced \nabove. In the worlds, however, we work with a countably in.nite set of region names RN . To interpret \ntypes we have region environments, i.e., injective maps RV -.n RN with adequate domains; on each evaluation \nof an expression that performs masking, we then map the same region variable to a fresh region name. \nIn the textual explanations below, however, we purposely blur the distinction between region variables, \nregion names and the regions themselves. The de.nition of worlds and types is given in Figure 2; the \nordering on worlds relies on the world transitions given in Figure 3. Both warrant a few comments. ParBij(X, \nY, Z) are .nite partial bijections between X and Y decorated with elements from Z; we write dom1(P ) \nfor the set of .rst coordinates and dom2(P ) for the set of second coordinates. Worlds W have two components: \nthe .rst is the live regions, these are partial bijections; the second the dead regions. No region can \nbe both live and dead, nor can any location belong to more than one region. Worlds may develop over time \naccording to the transitions in Figure 3. The .rst transition adds a location pair with associated type \nto a live region; this corresponds to an actual allocation in the operational semantics and is a standard \nnotion of world extension. The second and third transitions are orthogonal to the .rst. They give the \nregion dynamics and have no counterpart in the operational semantics; they are, however, intimately connected \nto the masking rule. The .rst initializes a new empty region, the second masks out a live one, losing \nthe partial bijection in the process. After being masked out, a region is considered dead and cannot \nbe initialized once more. The re.exive, transitive closure of the transitions is a preorder on worlds. \nWe require of our types T that they are monotone with respect to that preorder and the standard set-theoretic \ninclusion on URel(V); this is the type monotonicity. For any set X, URel(X) is the set of indexed, downwards \nclosed relations on X, i.e., URel(X)= {R . N\u00d7 X \u00d7 X | .(k, x1,x2) . R. .j <k. (j, x1,x2) . R}. The downwards \nclosure is essential, it prevents values from .eeing types as the operational semantics progresses.3 \nOne minor issue remains: the types that decorate the partial bijections that are the regions belong to \nT and must be coerced into T by the isomor\u00ad 3 The set URel(X) has a natural metric [12] and the functions \nin T are not only monotone but also non-expansive; see the appendix of the long version of the paper \nfor details.  T W = {(., .) . (RN -.n ParBij(L, L, T)) \u00d7P.n (RN ) | dom(.) n . = \u00d8. .r, s . dom(.).r \n= s . dom1(.(r)) n dom1(.(s)) = \u00d8 = dom2(.(r)) n dom2(.(s))} w . w l .. .n . N. .w0,w1,...,wn . W.w = \nw0 . w l = wn . .0 = i < n. (.r . RN . .l1,l2 .L. .\u00b5 . TT.wi .al(r,l1,l2,\u00b5) wi+1) . (.r . RN .wi .reg(r) \nwi+1) . (.r . RN .wi .mask(r) wi+1) URel(V),. : TT~1 T T = W .mon = 2 Figure 2. Our semantic footing: \nworlds and types. The former comes equipped with the preorder that is the re.exive, transitive closure \nof the transitions in Figure 3 (., .) .al(r,l1,l2,\u00b5) (.l,.) .. r . dom(.) . l1 ./dom1(.(r)) . l2 ./dom2(.(r)) \n. dom(.l) = dom(.) . .l(r)= .(r) .{(l1,l2,\u00b5)}. .s . dom(.) \\{r}..l(s)= .(s). (., .) .reg(r) (.l,.) .. \nr .RN \\ (dom(.) . .) . dom(.l) = dom(.) .{r}. .l(r)= \u00d8..s . dom(.)..l(s)= .(s). (., .) .mask(r) (.l,.l) \n.. r . dom(.) . dom(.l) = dom(.) \\{r}. .l = . .{r}..s . dom(.) \\{r}..l(s)= .(s). Figure 3. The three \nworlds transitions. We have r . RN in all of them and additionally l1,l2 .L and \u00b5 . TTin the .rst. Transitions \nare, once parameters are given, partial maps from W to W. phism . before they can be applied to a world. \nThis isomorphism is what we get from the solution to the type-world circularity. A bit of world-related \nnotation will come in handy: For a world w =(., .) we set dom(w) = dom(.) and |w| = dom(.) . ., i.e., \ndom(w) is the set of live regions and |w| is the combined set of live and dead regions. We write dom1(w) \nfor the union of all .rst coordinates of all live regions; dom2(w) is de.ned similarly. For r . dom(w) \nwe abuse notation and write w(r) for .(r). 5. Types and the Logical Relation The relational, world-indexed \ninterpretation of types is given in Figure 4; the interpretation of function types relies on the inter\u00adpretation \nof computations given in Figure 5. It is worthwhile to comment a bit on this. Note that both the interpretation \nof types and computations take a number of parameters, the requirements on those are given in the captions \nof the .gures. Overall, we follow the intuition laid out in Section 3. Integer, unit and product types \nare standard. Looking at the reference type there are three cases: If we interpret ref. t in a world \nwhere R(.) is unknown, we get the empty set. This we never do; it is just a dummy clause that is neutral \nwith respect to type monotonicity, but we need it since we, for technical convenience, want interpreted \ntypes to be applicable to all worlds. The middle case is the proper one, here R(.) is a live region and \nwe go through it in search for location pairs that hold values of a type semantically identical to t \n. The quanti.cation over future worlds ensure type monotonicity and the k-equality is necessary for the \nstep-indexed setup; both are quite standard. The latter means that the sets we compare are equal if we \nrestrict to elements with index strictly less than k. In the last case we look for references to a masked \nregion; these are conceptually dangling pointers that we make no assumption about, hence we return all \n(pairs of) values. The function type proceeds along the same lines. The middle case is the proper one \nand it too is quite standard, except that we quantify only over future worlds in which R(FRV(e)) remain \nlive. In worlds where this fails, we make no promise about the behavior. The interpretations of computations \nis crucial. Note .rst that rds e are all region variables with read effects in e, wrs e and als e are \nde.ned similarly. PRe w gives the precondition on heaps that computations running in world w with effects \ne have: that all location pairs in all regions with read effects do, in fact, hold values of the appropriate \ntype. If, now, the precondition holds and the left hand side terminates, then so does the right hand \nside, and there is a future world w l such that the results are in the desired type at w l and the resulting \nheaps satisfy the postcondition QRe w, w l . The latter states, that any writes to existing locations \nare of the correct type and are permitted by a write effect; also any newly allocated locations tracked \nby w l hold well-typed values and are in regions with an allocation effect. That the postcondition speaks \nof the initial heaps as well as the .nal ones is seen in, e.g., Hoare Type Theory [19] as well. Also, \nsince we do local reasoning, there may be parts of the heap outside our control; these are the frames \nf1 and f2, they must remain unmodi.ed. And the computations may allocate locations that the future world \ndoes not track, these are the additional frames f1 l and f2l . Finally, it is worthwhile to remark that \nthe region-dynamics of computations is quite restricted: the future world w l must have the same live \nregions as w. In other words, computations cannot mask out existing regions and if they initialize any \nnew regions, they are obliged to mask them out before they terminate. Here we take inspiration from work \non region-based memory management [9,  [1]R w = {(k, (), ()) | k . N} [int]R w = {(k, n, n) | k . N. \nn . Z} w = {(k, (v1,1,v2,1), (v1,2,v2,2)) | (k, v1,1,v1,2) . [t1]R w . (k, v2,1,v2,2) . [t2]R . .. [t1 \n\u00d7 t2]R w} \u00d8 \u00d8 {... .. . .. . . R(.) / .|w| k =(.\u00b5)(w l ) [ref. t ]R (k, l1,l2) |.\u00b5 . TT. (l1,l2,\u00b5) \n. w(R(.)) ..w l w. [t ]R w l {(k, v1,v2) | k . N. v1,v2 . V} R(.) . dom(w) w = R(.) .|w|\\ dom(w). R(FRV(e)) \nr |w| . .. .w l w. R(FRV(e)) . dom(w l) . (k, fix f(x).e1, [t1 eR . t2] R w l . R(FRV(e)) . dom(w) \n .j = k. .(v1,v2) .V\u00d7V. (j, v1,v2) . [t1] w = .... fix f(x).e2) . . .. R l (j, (fix f(x).e1) v1, (fix \nf (x).e2) v2) . [Te t2]w {(k, v1,v2) | k . N. v1,v2 . V} dom(w) . R(FRV(e)) .|w|. Figure 4. Interpretation \nof types. We require R : RV -.n RN injective with FRV(t ) . dom(R) and get [t]R : W .mon URel(V). 21] \nwhere regions are allocated and deallocated following a stack discipline. Proposition 5.1. We have [t \n]R . T, i.e., it is a non-expansive and monotone map from W to URel(H). The proof of this proposition \nis deferred to the appendix of the long version of the paper. When interpreting a type, we use region \nenvironments with pos\u00adsibly excessive domains. Clearly, the value of the region environ\u00adments outside \nthe region variables of the type does not matter; this is captured in the following lemma that we need \nbelow: Lemma 5.2 (Environment Extension). Let R1,R2 : RV -.n RN be injective with dom(R1) . FRV(t ) . \ndom(R2). If we have R1(.)= R2(.) for all . . FRV(t), then [t]R1 = [t]R2 . The logical relation on expressions \nis de.ned in Figure 6. . is a syntactic over-approximation of all region variables; together with the \ncondition R :. y .|w| it ensures that we deal in live and dead regions only, not unknown ones. As for \nfunctions and computations, we require that the regions of the effects are live. The logical relation \nis asymmetrical: the left hand side approx\u00adimates the right hand side. We write . | G |= e1 ~ e2 : t, \ne if the approximation goes both ways and consider the computations equivalent in that case. Our logical \nrelation and subtyping are sound in the following, standard, sense: Theorem 5.3 (Compatibility). The \nlogical relation in Figure 6 is compatible with the typing rules of Figure 1. That is, the formation \nof expressions according to the typing rules respects the logical relation. Proposition 5.4. Interpreting \nsubtypes gives subsets, i.e., we have that . | t1 = t2 implies [t1]R w . [t2]R w for any region environment \nR :. y . RN and any world w . W. The proof of the theorem relies naturally on the proposition. The proofs \nare deferred to the next section and the appendix of the long version of the paper. We have the Fundamental \nLemma as corollary: Lemma 5.5. (Fundamental) . | G f e : t, e =. . | G |= e : e : t, e. Another standard \ncorollary is contextual equivalence: Two equivalent computations that are placed inside the same closing, \ninteger context, will co-terminate with the same value, when run in any two heaps. Observe, though, that \nthe context must be typed according to the typing rules here, i.e., with effect annotations. 6. Soundness \nIn this section, we present some of the cases of the proof of the Compatibility Theorem stated just above. \nFirst off, two lemmas to handle some of the details, then we give detailed proofs of lookup, masking, \nand .xed points. See the appendix of the long version of the paper for more cases. Lemma 6.1 (Precondition \nStrengthening). l RR e . e=. Pe w . Pe; w. Lemma 6.2 (Postcondition Weakening). l R l R l e . e=. Qe \nw, w . Qe; w,w . Lemma 6.3 (Lookup). . | G |= e1 : e2 : ref. t, e implies . | G |=! e1 : ! e2 : t, e \n.{rd.}. Proof. We unroll the de.nition of the logical relation: Let k . N, w . W, R :. y.V|G| be arbitrary. \nAssume .|w| and .1,.2 R(FRV(e .{rd.})) . dom(w) and that (k, .1,.2) . [G]R w. We now must show that (k, \n! e1[.1/G], ! e2[.2/G]) . [Te.{rd.} t ]R w. We proceed to unroll the de.nition of computations: Let j \n= ll ll k, e1 .E and h1,h2,f1,f2,g1 .H be arbitrary. Assume (k, h1,h2) . PRe.{rd.} w, that jll ll (! \ne1[.1/G],h1 \u00b7 f1).(e1 ,g1 ) ll ll and irr(e1 ,g1 ). By the operational semantics, there must be 0 = i \n= j, e1 l .E and g1 l .H, such that ill ll j-i ll ll (e1[.1/G],h1 \u00b7 f1).(e1,g1), (! e1,g1).(e1 ,g1 ) \nand irr(e1l ,g1l ) holds. The assumption of the lemma and Precondi\u00adtioning Strengthening gives us w l \nw with dom(w l) = dom(w) as well as e l 2 .E and g2l ,hl 1,hl 2,f1l ,f2 l such that * ll (e2[.2/G],h2 \n\u00b7 f2).(e2,g2), l hll hll with g1 = 1 \u00b7 f1 l \u00b7 f1, g2 = 2 \u00b7 f2 l \u00b7 f2 and (k - i, e1l ,e2) . [ref. t]R \nw l as well as (k - i, h1,h2,h1l ,hl 2) . QR w, w l . Let us ponder the fact (k - i, el 1,e2l ) . [ref. \nt]R w l . By assumption, R(.) . dom(w) = dom(w l) and so we must have e  .L and \u00b5 . TTsuch that e l \nl1, e 2 [Tet ].j = k. .e w = R l ll ). Since l1 . dom1(w(.\u00b5)(w ) = ww = dom(h and(R(.)) t[]entirelefthandsidereductionmustlooklike \nl 1) . dom(g1) we must have i<j since the alternative ll ll disproves irr(e1 ,g 1 ). Indeed, we must \nhave i = j - 1 and the l l l 1).(h1(l1),g 1), l 2), 2) we get a similar l l 1 .E. .h1,h2,f1,f2,g R (k, \nh1,h2) . Pe w . 1 .H. 1)1,f2 .H. \u00b7 f2 . l ll j-1 (! e1[.1/G],h1 .(! l1,g 1(l1) and g 1. Moreover, since \nl l \u00b7 f1) j (e1,h1 \u00b7 f1).(e 2 .E. .g 1,g 1). irr(e1,g 2,h1,h2,f lll l l ll =. ll ll in particular e = \nh = g 1 1 l l.w. e .w * .(l,g 2 (e2[.2/G],h2 \u00b7 f2) l ) . 2). l dom(w) = dom(w and l2 . dom2(w l) l = \ndom(h2) . dom(g *l reduction on the right hand side (e2,h2 = h l \u00b7 f )f\u00b7 2l l .(e 2,g lll * .(! l,g 2 \nl l l 2).(h2(l2),g 2). By Postcondition Weakening we are left to verify only that \u00b7 f1 . g = h \u00b7 f litsuf.cestoprove \n-(kj,h()![./G],hf\u00b7e2222l.(l )) t[]22 1(l1),hbranch on whether (l1,l2,\u00b5) g 1 1 1 2 2 2 l-(kj,h,h,h,h121 \nl l R . 2) . Qe w, w l k-j+1 l-(kj,h (.\u00b5)(w l), RR 1,e w 2) . [t ] l l l R ww = (k - j, e l). We now \nl lookup existedprior l. Since [t ] 1(l1),h2(l2)) . (.\u00b5)(w . w(R(.)) holds, i.e., whether to the entire \ncomputation locations we R (k,h1,h2) . Pe w .. l or were allocated along the way. If (l1,l2,\u00b5) . w l((R(.)) \n\\ 2(l2)) l..dom(h)=dom()dom(h)=dom() ,weget -(R(.))(kj,hwww1122 l,h1l l) directly . 1(l1),h2) . QR \ne (.\u00b5)(w l. If, on the other hand, from (k - i, h1,h2,hw, w .. . rds e. .(l1,l2,\u00b5) . w(R(.)). it is \nthe case that (l1,l2,\u00b5) . w(R(.)) holds, then we get k> 0 . (k - 1,h1(l1),h2(l2)) . (.\u00b5)(w)(k - 1,h1(l1),h2(l2)) \n. (.\u00b5)(w) from PRe.{rd.} because of the read effect rd.. If the locations are not written during the \ncom\u00ad l l l,h1l l l 1(l1) and h2(l2) 2(l2), we get 1,h2) . Q 1(l1),h2(l2)) . (.\u00b5)(w dom(h1) = dom1(w) \n. dom(h2) = dom2(w) . hand, they are written, then we still get this, but this time from 2) = dom2(w \nll putation, i.e., if h1(l1) hh = = l R .. (k,h1,h2,h l) immediately. If, on the other w, w (k - j, h \ne l (k - i, h1,h2,h2) . QRe w, w . 1) = dom1(w .l1 . dom(h1).h1(l1)= h ll l.)dom(h l.) dom(h 1(l1) \n. .. . wrs e. .(m1,l2,\u00b5) . w(R(.)).l1 = m1 . l Lemma 6.4 (Masking). .,. | G |= e1 : e2 : t, e implies \n. | G |= e1 : e2 : t, e - . provided that ./. FRV(G,t). 1(l1),h.l2 . dom(h2).h2(l2)= h ll l.(l ))(.\u00b5)(w22 \nk> 0 . (k - 1,h . ) Proof. We unroll the de.nition of the logical relation: Let k . N, l.(l )22 .V|G| \nbe arbitrary. Assume R w. We w . W, R :. y.|w| and .1,.2 R(FRV(e - .)) . dom(w) and that (k, .1,.2) . \n[G] .. . wrs e. .(l1,m2,\u00b5) . w(R(.)) . l2 = m2 . 2(l2)) . (.\u00b5)(w now must show that l(l),h11 l .r . \ndom(w). .(l1,l2,\u00b5) . w l k> 0 . (k - 1,h . ) R w. l (r) \\ w(r).r . R(als e) . l l l (k, e1[.1/G],e2[.2/G]) \n. [Te-. t]We proceed to unroll the de.nition of computations: Let j = k, 1) and that l 1(l1),h2(l2)) \n. (.\u00b5)(w Figure 5. Interpretation of computations with pre-and postcon\u00ad l.-k> 0(k 1,h l ) l E and h1,h2,f1,f2,g \nH be arbitrary. Assume that . . e 1 1 j PR e-. w, that (e1,h1 \u00b7 f1). l . (e (k, h1,h2) 1,g 1,g ditions. \nThere are implicit disjointness requirements on heaps: the heap compositions must all be well-de.ned. \nIn all three cases we have R : RV -.n RN injective and w R w l irr(e 1). We now have to prove a range \nof things; to do so we make use of the assumptions of the lemma. Pick r . RN \\|w| and de.ne w l. Let \nRl = R[. . r] be the corresponding extension l . W by the transition l . W. For [Te t ] w .reg(r) w \nwe require FRV(e, t) . dom(R) and R(FRV(e)) . dom(w). of R. We have Rl :.,. .|w l|, Rl PRe w is de.ned \nfor FRV(e) . dom(R) and R(FRV(e)) . R; l, the latter by Environment Extension and l ) and (FRV(e)) . \ndom(w (k, .1,.2) . [G] monotonicity of semantic types. But then the assumption of the lemma buys us that \nw dom(w), QRe w, w l additionally requires w l . W and w w with dom(w) = dom(w l ). R; l l (k, e1[.1/G],e2[.2/G]) \n. [Te t] w. . | G |= e1 : e2 : t, e Observe, now, that (k, h1,h2) . PRe ; w l because of type mono\u00adtonicity \nand since w (R(.)) = \u00d8. This means, that we have .. ll l with dom(w ll ll e ) as well as .k . N. .w \n. W. .R :. y.|w|. ..1,.2 .V|G| .E, ) = dom(w ww. * l h 2,h l 1,h\u00b7 f ll 2,f1,f\u00b7 f1, g l 2 l l 2,g \n2), g = l .(e g R(FRV(e)) . dom(w) . (k, .1,.2) . [G] R w. (k, e1[.1/G],e2[.2/G]) . [Te t ] R w. R; \nw ll and lll suchthat .H (),hf\u00b7e222ll-f(kj,e\u00b7 ,2 ll \u00b7 f 2 1,e 2 Figure 6. The logical relation. We \nrequire FRV(G, t, e) . . and, mask out region r. We have w l,h1 l 2) . [t] De.ne w lll . W by the transition \nw ll .mask(r) w lll, i.e., we lll l ll 2) . QR; w ,w . e w by transitivity. Also as always, FV(e1,e2) \n. G. lllll dom(w ) = dom(w ) \\{r} = dom(w l ) \\{r} = dom(w).  By Environment Extension and monotonicity \nwe get All that is left is gathering the consequences. ll R; ll R lll (k - j, e1,e ww. 2) . [t ] . [t \n]f lll Write hl 1 = h 1 \u00b7 1 with h 1,f1 .H and dom(f1 ) = dom1(w ). And similarly, write hl 2 = h 2 \n\u00b7f2 with h 2,f 2 .H and dom(f2 )= lll dom2(w ). It remains to prove that (k - j, h1,h2,h1,h2) . QRe-. \nw, w lll; note that we have transferred to the frame whatever locations that were allocated in region \nr. Take l1 . dom(h1) and assume h1(l1)= h1 (l1). We get s . wrs e and (m1,l2,\u00b5) . w l(R(s)) with l1 = \nm1 and ll ll k - j> 0 . (k - j - 1,h1(l1),h2(l2)) . (.\u00b5)(w ). Since w l(R(.)) = w l(r)= \u00d8, we conclude \ns = . and so s . wrs e - . and (l1,l2,\u00b5) . w(R(s)); all that remains is an application of type monotonicity. \nWrites to the right hand side heap is treated similarly. Assume, .nally, that there is s . dom(w) and \n(l1,l2,\u00b5) . w lll(s) \\ w(s). Since s = r we have (l1,l2,\u00b5) . w ll(s) \\ w l(s) and so there is s . R(als \ne) and ll ll k - j> 0 . (k - j - 1,h1(l1),h2(l2)) . (.\u00b5)(w ). Since we had s = r = R(.), we must have \ns . R(als e - .) as well; type monotonicity takes us the last way if k - j> 0 happens to hold. e Lemma \n6.5 (Fix). . | G,f : t1 . t2,x : t1 |= e1 : e2 : t2,e e implies . | G |= fix f(x).e1 : fix f(x).e2 : \nt1 . t2, \u00d8. Proof. We unroll the de.nition of the logical relation: Let k . N, w . W, R :. y.|w| and \n.1,.2 .V|G| be arbitrary. The assumption R(FRV(\u00d8)) . dom(w) gives us nothing, but we do get that (k, \n.1,.2) . [G]R w. Write e1 l = fix f(x).e1[.1/G] and e l 2 = fix f(x).e2[.2/G], we must show that ll eR \n(k, e1,e 2) . [T\u00d8 t1 . t2] w. As both expressions are, in fact, values it will suf.ce to show (k, el \n1,e l eR w. 2) . [t1 . t2]Notice .rst that R(FRV(e)) .|w|. If R(FRV(e)) . dom(w) fails, then there is \nnothing left to show. Assume, hence, that the inclusion holds, we aim to prove by induction that for \nall 0 = j = k we have ll eR (j, e1,e 2) . [t1 . t2] w. The base case is easy, since there is no termination \nin 0 steps here. So assume the above for 0 = j<k, we must prove that ll eR (j +1,e 1,e w. 2) . [t1 . \nt2]is good too. Pick w l w with R(FRV(e)) . dom(w l). Let i = j +1 and (v1,v2) . V\u00d7V with (i, v1,v2) \n. [t1]R w l be arbitrary. We must show l R l (i, el 1v1,e w. 2v2) . [Te t2] ll ll So, let il = i, e1 \n.E and h1,h2,f1,f2,g 1 .H be arbitrary and assume (i, h1,h2) . PRe w l and i; l ll ll (e1v1,h1 \u00b7 f1).(e1 \n,g 1 ) ll ll with irr(e1 ,g 1 ). Clearly, il > 0, so the reduction must go like (e l 1v1,h1 \u00b7 f1).(e1[.1/G,e \nl 1/f, v1/x],h1 \u00b7 f1) i;-1 ll ll .(e1 ,g 1 ) It is now time to make use of the assumption. Since il -1 \n= i-1 = j, we can apply the induction hypothesis to get ll l R l (i-1,e1[.1/G,e 1/f, v1/x],e2[.2/G,e \n2/f, v2/x]) . [Te t2] w. 7. Applications We now show applications of our logical relations model: we \nverify four effect-based program transformations. These transformations are also considered in [7], but \nonly for a language with ground store. Theorem 7.1 (Idempotent Computation). A computation with dis\u00adjoint \nread and write effects and no allocation effects is idempotent. More precisely, assume that we have . \n| G f e : t, e with rds e n wrs e = \u00d8 = als e. Then it holds that . | G |= let x = e in let y = e in \n(x, y) ~ let x = e in (x, x): t \u00d7 t, e. Proof. We just prove that the left hand side approximates the \nright hand side, the other way round proceeds similarly. Let k . N, w . W, R :. y.|w| and .1,.2 .V|G| \nbe arbitrary. Assume that R(FRV(e)) . dom(w) and that (k, .1,.2) . [G]R w. We set e1 = let x = e[.1/G] \nin let y = e[.1/G] in (x, y) and e2 = let x = e[.2/G] in (x, x) and have to prove (k, e1,e2) . [Te t \n\u00d7 t]R w. We proceed to unroll the de.nition of computations: Let j = k, lll lll e1 .E and h1,h2,f1,f2,g \n1 .H be arbitrary. Assume that (k, h1,h2) . PRe w, that jlll lll (e1,h1 \u00b7 f1).(e1 ,g 1 ) lll lll and \nthat irr(e1 ,g 1 ). By the de.nition of the operational semantics there must be 0 = i = j, e l 1 .E and \ng1 l .H such that the above reduction can be split into ill (e1,h1 \u00b7 f1).( let x = e1 in let y = e[.1/G] \nin (x, y),g 1) j-i lll lll .(e1 ,g 1 ) with ill (e[.1/G],h1 \u00b7 f1).(e1,g 1) and irr(e1l ,g 1l ). From \nthe Fundamental Lemma we get (k, e[.1/G],e[.2/G]) . R ll w. But then we get ww with dom(w) = dom(w ) \nas [Te t] well as e l 2 .E and g2l ,hl 1,hl 2,f1l ,f2 l .H such that * ll (e[.2/G],h2 \u00b7 f2).(e2,g 2), \nll l g1 = hl 1 \u00b7f1 l \u00b7f1, g2 = hl 2 \u00b7f2 l \u00b7f2, (k-i, h1,h2,hl 1,h2l ) . QRe w, w l R l and (k - i, el \n1,e w l. The latter implies e1 .V and so we 2) . [t ] lll lll must have i<j as the alternative would \ninvalidate irr(e1 ,g 1 ). ll ll Indeed, there must be 0 = il = j - i - 1, e1 .E and g1 .H such that we \ncan split the last j - i steps further (let x = e l 1 in let y = e[.1/G] in (x, y),g 1l ) .(let y = e[.1/G] \nin (e1l , y),g 1l ) i; lll ll .(let y = e1 in (e1, y),g 1 ) j-i-1-i; lll lll .(e1 ,g 1 ) with i; l ll \nll (e[.1/G],g 1).(e1 ,g 1 ) ll ll and irr(e1 ,g 1 ).  - l - ll \u00b7 fll (e[.1/G],h1 \u00b7 f1)(e1,hl 1 \u00b7 f1 \nl \u00b7 f1)(e[.1/G],hl 1 \u00b7 f1 l \u00b7 f1)(e1 ,hll 11 \u00b7 f1 l \u00b7 f1) P Re w w ; Q Re w,w ; P Re w ; R w ;; Q Re \nw ;;; ,w t - l \u00b7 fl - l \u00b7 fll (e[.2/G],h2 \u00b7 f2)(e2,h2 l 2 \u00b7 f2)(e[.2/G],h2 \u00b7 f2)(e2,h2 l 2 \u00b7 f2) R t \n Figure 7. Illustrated proof of the Idempotent Computation Theorem. Now for something odd: we reset the \nright hand side to the initial state. More precisely, we argue that (k - i - 1,hl 1,h2) . PRe w l, this \nis the crux of the entire proof: Notice initially, that not only is it the case that dom(w) = dom(w l), \nwe also have .r . dom(w).w(r)= w l(r) since als e = \u00d8; in particular we get dom2(w) = dom2(w l). Combining \nnow the facts .. . rds e. .(l1,l2,\u00b5) . w(R(.)). k> 0 . (k - 1,h1(l1),h2(l2)) . (.\u00b5)(w) and .l1 . dom(h1).h1(l1)= \nh1l (l1) . .. . wrs e. .(m1,l2,\u00b5) . w(R(.)).l1 = m1 . ll l k> 0 . (k - 1,h1(l1),h2(l2)) . (.\u00b5)(w ), with \nrds e n wrs e = \u00d8 buys us .. . rds e. .(l1,l2,\u00b5) . w l(R(.)). k - i - 1 > 0 . (k - i - 2,h1l (l1),h2(l2)) \n. (.\u00b5)(w l). Loosely speaking, the locations we are permitted to read held values of the correct type \nfrom the beginning and were not changed by the computation seen so far. We proceed to use the fact that \n(k - i - 1,e[.1/G],e[.2/G]) . [Te t]R l w by a second application of the Fundamental Lemma. lll lll This \nyields ww with dom(w ) = dom(w ) as well as ll ll e2 .E and g2 ,h1 ll,hll 2 ,f 1 ll,f2 ll .H such that \n* ll ll (e[.2/G],h2 \u00b7 f2).(e2 ,g 2 ), ll hll fll ll hll fll f l l ll ll g1 = 1 \u00b7 1 \u00b7 1 \u00b7 f1, g2 = 2 \u00b7 \n2 \u00b7 f2, (k - i - 1 - il,hl 1,h2,hll 2 ) . QRe w ,w ll and (k - i - 1 - il ,e 1 ,e 1 ,hll 2 ) . R w ll. \nThe latter implies e ll 1 .V and so we must have il < [t ] lll lll j - i - 1 since an equality would \ncon.ict with irr(e1 ,g 1 ). Even more precisely, we must have j - i - 1 - il =1 and the .nal step j lll \nlll in the entire reduction (e1,h1 \u00b7 f1).(e1 ,g 1 ) must be ll l ll lllll (let y = e1 in (e1, y),g 1 \n).((e1,e 1 ),g 1 ), lll lll lll ll in particular e1 =(e1,e 1 ) and g1 = g1 . We immediately get * ll \nll ll (e2,h2 \u00b7 f2).((e2 ,e 2 ),g 2 ). In the proof, it now remains to prove (k - j, h1,h2,hll 1 ,hll \n2 ) . l ll ll ll R QR w, w ll and that (k - j, (e1,e 1 ), (e2 ,e 2 )) . [t ]w . Notice ee initially, \nthat by the determinism of the operational semantics and lll llllll the fact that e2,e 2 .V we have e2 \n= e2 and g2 = g2 . Since dom2(w l) = dom2(w ll) by als e = \u00d8 we furthermore get = hll = fll hl 22 and \nf2 l 2 . Also recall k - j = k - i - il - 2. On the .rst obligation, we take l1 . dom(h1) and assume \nhll hll that h1(l1)= 1 (l1). If also h1l (l1)= 1 (l1) then the desired QR l ll follows from (k - i - \n1 - il,hl 1 ,hll 2 ) . w ,w . 1,h2,hll e hll Otherwise, we must have h1(l1)= hl 1(l1)= 1 (l1), but then \n(k - i, h1,h2,hl 1,h2l ) . QRe w, w l paves the way. And the second obligation is met by recalling (k \n- i, el 1,e l R w l and (k - 2) . [t ] ll ll R ll i - 1 - il ,e 1 ,e w . 2 ) . [t] Theorem 7.2 (Commuting \nComputations). Two computations commute if neither reads a region that the other writes, and there is \nno region they both write. More precisely, assume that we have . | G f e : t, e, . | G f e l : t l,el \nwith rds e n wrs el = rds el n wrs e = wrs e n wrs el = \u00d8. Then we have the following equivalence: . \n| G |= let x = e in let y = e l in (x, y) ~ let y = e l in let x = e in (x, y): t \u00d7 tl,e . el . Proof. \nAgain we give the details only one way: that the left hand side approximates the right hand side. Let \nk . N, w . W, R :. y.|w| and .1,.2 .V|G| be arbitrary. Assume that R(FRV(e . el)) . dom(w) and that (k, \n.1,.2) . [G]R w. Set e1 = let x = e[.1/G] in let y = e l[.1/G] in (x, y) and e2 = let y = e l[.2/G] in \nlet x = e[.2/G] in (x, y); we have to prove (k, e1,e2) . [Te.e t \u00d7 t l]R w. We proceed to unroll the \nde.nition of computations: Let j = k, lll lll e1 .E and h1,h2,f1,f2,g 1 .H be arbitrary. Assume that \n(k, h1,h2) . PRe.e; w, that jlll lll (e1,h1 \u00b7 f1).(e1 ,g 1 ) lll lll and that irr(e1 ,g 1 ). By the de.nition \nof the operational semantics there must be 0 = i = j, e1 l .E and g1 l .H such that the above reduction \ncan be split into illl (e1,h1 \u00b7 f1).( let x = e1 in let y = e [.1/G] in (x, y),g 1) j-i lll lll .(e1 \n,g 1 ) with ill (e[.1/G],h1 \u00b7 f1).(e1,g 1) and irr(e1l ,g 1l ). From the Fundamental Lemma we get (k, \ne[.1/G],e[.2/G]) . R w. By Precondition Strengthening we get a whole range [Te t] of stuff, but we need \nonly the following two facts: that e1 l .V and that for l1 . dom(h1) such that there is r . R(rds el) \nand (m1,l2,\u00b5) . w(r) with m1 = l1 we have g1l (l1)= h1(l1). The latter is a consequence of wrs e n rds \nel = \u00d8. We must have i<j as the alternative would invalidate lll lll ll irr(e1 ,g 1 ). Indeed, there \nmust be 0 = il = j - i - 1, e .E 1  i+1 i;+1 -ll \u00b7 f \u00b7 f l = hl \u00b7 fl -l ll \u00b7 fll \u00b7 f \u00b7 fl (e1,h1 \u00b7 f1) \n(let y = e [.1/G] in (e1, y),h 11 1 \u00b7 f1 11 \u00b7 f1)((e1,e 1 ),hll 11 11 \u00b7 f1) QR ; PR w e; w,w QR ;;; \nPR e.e; w,w e.e; w ;; PR QR w,w e e; w - l \u00b7 fl = h \u00b7 f \u00b7 fl -ll l \u00b7 fll \u00b7 f \u00b7 fl (e2,h2 \u00b7 f2)( let \nx = e[.2/G] in (x, e2 ),hl 22 \u00b7 f2 222 \u00b7 f2)((e2 ,e 2),hll 22 22 \u00b7 f2) * * Figure 8. Illustrated proof \nof the Commuting Computations Theorem. Most of the dashed lines are slightly off, i.e., they do not point \nat the exact right subheaps. and g1 ll .H such that we can split the last j - i steps further into (let \nx = e1 l in let y = e l[.1/G] in (x, y),g 1l ) .(let y = e l[.1/G] in (e1l , y),g 1l ) i; lll ll .(let \ny = e1 in (e1, y),g 1 ) j-i-1-i; lll lll .(e1 ,g 1 ) with i; ll llll (e [.1/G],g 1).(e1 ,g 1 ) ll ll \nand irr(e1 ,g 1 ). Much as in the previous proof, we now ditch our right hand side progress, but unlike \nthat, we also lose our new future world. Notice .rst that the Fundamental Lemma gives us (k - i - 1,e \nl[.1/G],e l[.2/G]) . [Te t lR w, we would like to apply that. l = h \u00b7h. ] Write g1 11 for h 1,h. 1 .H \nwith dom(h 1) = dom1(w); the crucial observation now is that we have (k-i-1,h ,h2) . PRe; w. 1 Hence \nwe get w l w with dom(w) = dom(w l) as well as e2 l .E and g2l ,h1 ll,hl 2,f1 ll,f2 l .H such that * \n(e l[.2/G],h2 \u00b7 f2).(e l 2,g 2l ), ll = hll fll l g11 \u00b71 \u00b7f1 . , g2 = hl 2 \u00b7f2 l \u00b7f2, (k-i-1-il,h 1,h2,h1 \nll,hl 2) . ll l R l e; w, w l and (k - i - 1 - il ,e 1 ,e 2) . [tlw .QR Much as we have argued before, \nwe now know that ]e ll 1 .V, hence j - i - 1 - il =1 and the entire left hand side reduction must look \nlike jl ll ll (e1,h1 \u00b7 f1).((e1,e 1 ),g 1 ), lll lll lll ll in particular, e1 =(e1,e 1 ) and g1 = g1 \n. We have now, in some sense, considered the situation from the point of view of e l, it is time to turn \nthe tables. There are h2 ,f2 with dom(h) = dom2(w) such that hl = h\u00b7 f. So we have 2 222 (k, h1,h 2) \n. PRe w by arguments as above, in particular we apply wrs el n rds e = \u00d8. Now, we still have (k, e[.1/G],e[.2/G]) \n. R ll ll w and so there is ww with dom(w) = dom(w ) as [Te t] ll ll well as e2 .E and g2 ,hl 1,hll 2 \n,f1l ,f2 ll .H such that l *llll (e[.2/G],h2 \u00b7 f2 \u00b7 f2 \u00b7 f2).(e2 ,g 2 ), l ll = hll fll g1 = h1 l \u00b7fl \nf1, g22 \u00b7f2 \u00b7f2 l \u00b7f2, (k -i, h1,h 2,hl 2 ) . 1 \u00b72 \u00b71,hll QR ll R h w, w ll and (k - i, e1l ,e w . Write \nhl = \u00b7 f e 2 ) . [t]1 11 l ll for f.H, in particular g1 = h\u00b7 f\u00b7 f1 l \u00b7 f1 and g1 = 1 1 1 hll \u00b7 fll 11 \n\u00b7 f1 \u00b7 f1 l \u00b7 f1; this is notation we need soon. Observe .rst, though, that we have the right hand side \nreduction * lll ll (e2,h2 \u00b7 f2).((e2,e 2 ),g 2 ). lll lll) We now build a world w with dom(w = dom(w) \nand lll lll with both ww l and ww ll, i.e., a common future world. The natural choice is for the dead \nregions of w lll to be the dead lll(r) regions of both w l and w ll. For r . dom(w) we set w = w l(r) \n. w ll(r), but we must take care not to wreck the partial bijections nor their mutual disjointness: Let \nr, s . dom(w) and ll llll take (l1l ,l2l ,\u00b5 ) . w (r) \\w(r) and (l1 ll,l2 ll ,\u00b5 ) . w (s) \\ w(s); it \nlll lll will suf.ce to show l1 l = 1 and l2 l = 2 . Now, we know that l1 l . dom1(w l) = dom(h1 ll), \nin particular we have l1 l ./dom(f1 ). Also l1 l ./dom1(w) = dom(h1 ). But l1 ll . dom1(w ll) = dom(h1l \n) h lll lll and since h1 l = 1 \u00b7 f1 we must have l1 l = 1 . Proving l2 l = 2 proceeds similarly. It \nshall now suf.ce to show ll ll R lll (k - j, h1,h2,h\u00b7 f,h\u00b7 f) . Qe.e; w, w 1 12 2 as the remaining obligations \nare discharged already. We have llllll dom1(w ) = dom1(w ) . dom1(w ) ll = dom(h1 ) . dom(h1 \u00b7 f1 ) = \ndom(hll 1 \u00b7 f1 ) and get dom2(w lll) = dom(hll 2 \u00b7 f 2 ) similarly. So take l1 . hll hll dom(h1) and \nassume that h1(l1)= 1 (l1). If h 1(l1)= 1 (l1) we get . . wrs el and (m1,l2,\u00b5) . w(R(.)) with l1 = m1 \nand (k - j, hll 1 (l1),hl 2(l2)) . (.\u00b5)(w l). And since wrs e n wrs e = \u00d8 hl we know that hll 2 (l2)= \n2(l1). If, on the other hand, h1(l1)= hll 1 (l1) then we must have h1(l1)= h 1(l1) and we get . . wrs \ne and (m1,l2,\u00b5) . w(R(.)) with l1 = m1 and (k - i - 1,h1 (l1),hll 2 (l2)) . (.\u00b5)(w ll). It remains to \nconsider allocation. So take r . dom(w) and (l1,l2,\u00b5) . w lll(r) \\ w(r). By the construction of w lll \nwe get (l1,l2,\u00b5) . w l(r) \\ w(r) or (l1,l2,\u00b5) . w l(r) \\ w(r); in both cases we proceed similarly, so \nassume the former holds. Then we know r . R(als el) and that (k - j, h1 ll(l1),hl 2(l2)) . (.\u00b5)(w l). \nBut as l2 ./dom2(w) we must have l2 . dom(f2 ) and so (hll \u00b7 f )(l2)= f (l2)= h2l (l2) and we are done. \n22 2 In the remaining two applications we can prove only approxima\u00adtion, not equivalence, because of \nthe possibility of non-termination. We defer the proofs to the appendix of the long version of the paper. \nTheorem 7.3 (Neutral Computation). A computation that per\u00adforms no writes and has return type unit approximates \nthe trivial computation. More precisely, having . | G f e : 1,e with wrs e = \u00d8 implies . | G |= e : () \n: 1, e. Theorem 7.4 (Pure Lambda Hoist). A pure computation evaluated as part of a function can, up to \napproximation, be evaluated once and the result cached. More precisely, having . | G f e : t1, \u00d8, . | \nG,y : t2,x : t1 f e l : t3, e.  gives us l e .y. let x = e in e : t2 . t3, \u00d8 8. Discussion 8.1 Work \nby Benton et. al An important point of reference for our work is the relational model by Benton et. al. \n[7] of an effect system for a higher-order langauge with dynamic allocation and ground store, i.e., only \nintegers in the heap. Indeed, apart from our extension to higher-order store, the type systems and examples \nconsidered are roughly the same. Having said so, we remark that our take on the issue of masking is novel; \nin particular it is different from that of Benton et. al. Their approach does not scale easily, if at \nall, to the higher-order store setting: The pivot is the Masking Lemma [7, Lemma 3], stating that the \ninterpretation of both types and computations in a world are preserved up to equality under masking, \nprovided that the region masked out is not, syntactically, in the type respectively in the computation. \nCombined with ground store, this makes short work of soundness of the masking rule. The Masking Lemma, \nhowever, does not scale easily to a higher-order store setting. Consider the computation from the in\u00adtroduction: \nThe returned function has latent effect {wrs} and cor\u00adrespondingly writes to location 1 in region s. \nBut the values stored at location 1 must be of type ref. int, and that depends on the type int associated \nwith location 0 in region .. In other words, the interpretation of a type may depend on regions that \ndo not oc\u00adcur syntactically in the type; this is the antithesis to the Masking Lemma. As described above, \nwe take the different approach that inter\u00adpretations of types should grow (or at least not shrink) under \nany application of masking, and only when we actually perform reads, writes and allocations do we require \nthat the regions in question are still live. This also means that we can get by without a silent region \n[7, Sections 5 and 6]. This is a designated region of the world that tracks inaccessible parts of the \nheap in an untyped way; in loc.cit. it is necessary for the Masking Lemma to hold for computations. That \nwe have no silent region is not just a technical convenience: it means that our model permits any action \non locations that have been masked out, including garbage collection or ownership trans\u00adfer. The locations \nhave left the world and we make no further as\u00adsumptions on them, whatsoever. By contrast, locations in \na silent region are still in the world and computations may assume that they remain allocated, even if \nthe stored value cannot be changed. Indeed, computations may actually access such locations in exten\u00adsionally \ninvisible ways: we could, say, read a location in the silent region as long as we make no use of the \nread value. Benton et. al.: Higher Order Store In more recent work, Benton et. al. [8] have given a relational \nmodel for a language with higher order store. The language has no dynamic allocation though, nor is there \nany masking rule. Unlike our approach, however, they work with a denotational semantics of the language; \nin particular they prove existence of the logical relation by solving a non-trivial mixed-variance domain \nequation, extending the standard technique by Pitts [20] to do so. One advantage is transitivity of their \nlogical relation; that is something we do not get. This is a general issue with step-indexed models observed \n.rst by Ahmed [3]; there are .xes, but a more proper take is to reason in a logic suited for step-indexing \nas done, e.g., by Dreyer et. al. [13]. We are convinced, however, that using the techniques developed \nby St\u00f8vring and the authors [11], we could also de.ne a model based on a denotational semantics of the \nprogramming language, hence achieving transitivity. Preservation of All Store Relations Throughout their \nwork, Ben\u00adton et. al. interpret computations by requiring preservation of all relations on heaps that \nrespect the effects of the computations, i.e., that ensure well-typed reads at locations with read effects \nand is closed under well-typed writes to locations with write effects. Our approach is more simple-minded; \nwe do, in some sense, just pre\u00adserve one relation. But it is unclear what the additional relations buys, \nwe know of no equivalences that fail due to our approach; indeed the two alternatives may very well be \nequivalent. What is clear, however, is that losing the many relations, as well as the absence of a silent \nregion, simplify proofs considerably: writing out all details, as we do, obviously generates some mileage \nbut in essence one may argue in simple diagrams as illustrated.  8.2 On Expressiveness Consider the \nfollowing variant of the so-called awkward example, e0e1 discussed in detail in [14]. Let t =(1 . 1) \n. int with e0 = {rd., wr.} and e1 = {rd., wr., rds, wrs} and de.ne expressions e1 and e2, which both \nhave type t and effect {als}, by e1 = let x = refs 0 in .f.(x := 0; f(); x := 1; f(); ! x) e2 = let x \n= refs 0 in .f.(f(); f(); x := 1;! x). We have used the subscript s to indicate that x will have the \ntype refs int. Since . and s are distinct, f cannot read / write the reference bound to x; this ensures \nthat both the left and the right hand side functions always return 1. Indeed we can show that e1 and \ne2 are contextually equivalent. We remark that if, on the other hand, we had typed e1 and e2 with the \nsame region variable instead of two distinct region variables, then our semantic model would not be expressive \nenough to show that e1 and e2 are contextually equivalent. For that, we could extend our model using \nideas from the model for non-effect-annotated types in [14]. In summa, effect information can be used \nto restrict the applicable contexts and can thus make it easier to show two expressions equivalent (for \nthose restricted contexts). Note that a standard uni.cation-based algorithm for inferring effects, would \ninfer the type t for e1 with . and s distinct. 9. Conclusion and Future Work We have presented a solution \nto the open problem of constructing a relational model for an effect system for a higher-order language \nwith dynamically allocated higher-order store. We have demon\u00adstrated that the model can be used to rigorously \njustify effect-based program transformations. Future work includes extending the model to region and \nef\u00adfect polymorphism, as found, e.g., in [21]. Moreover, we think our model could be adapted to a situation \nwhere the conceptual regions do not necessarily follow a stack-discipline [4, 16] by a slight modi\u00ad.cation \nof the interpretation of computations. In this paper we have focused on a relational model for verifying \neffect-based program transformations. We believe that the ideas of our model construc\u00adtion can also be \nused to construct models for other applications of effect systems for languages with higher-order store. \nAcknowledgments We thank Nick Benton, Lennart Beringer, Martin Hofmann and Andrew Kennedy for useful \ndiscussions. Kasper Svendsen and Filip Sieczkowski gave useful comments on a late draft of the paper. \nThis research was funded in part by the ToMeSo project from the Danish Agency for Science, Technology \nand Innovation (grant number 274 08 0412).  References [1] A. Ahmed. Semantics of Types for Mutable \nState. PhD thesis, Princeton University, 2004. [2] A. Ahmed, D. Dreyer, and A. Rossberg. State-dependent \nrepresenta\u00adtion independence. In Proceedings of POPL, 2009. [3] A. J. Ahmed. Step-indexed syntactic logical \nrelations for recursive and quanti.ed types. In P. Sestoft, editor, ESOP, volume 3924 of Lecture Notes \nin Computer Science, pages 69 83. Springer, 2006. ISBN 3\u00ad540-33095-X. [4] A. Aiken, M. F\u00a8ahndrich, and \nR. Levien. Better static memory man\u00ad agement: Improving region-based analysis of higher-order languages. \nIn Prcoeedings of PLDI, 1995. [5] N. Benton and P. Buchlovsky. Semantics of an effect analysis for exceptions. \nIn Proceedings of TLDI, 2007. [6] N. Benton, A. Kenney, M. Hofmann, and L. Beringer. Reading, writ\u00ading \nand relations: Towards extensional semantics for effect analyses. In Proceedings of APLAS, 2006. [7] \nN. Benton, L. Beringer, M. Hofmann, and A. Kennedy. Relational semantics for effect-based program transformations \nwith dynamic al\u00adlocation. In Proceedings of PPDP. ACM, 2007. [8] N. Benton, L. Beringer, M. Hofmann, \nand A. Kennedy. Relational se\u00admantics for effect-based program transformations: Higher-order store. In \nProceedings of PPDP. ACM, 2009. [9] L. Birkedal, M. Tofte, and M. Vejlstrup. From region inference to \nvon Neumann machines via region representation inference. In Proceedings of POPL, 1996. [10] L. Birkedal, \nN. Torp-Smith, and H. Yang. Semantics of separation\u00adlogic typing and higher-order frame rules for algol-like \nlangauges. Logical Methods in Computer Science, 2(5:1):1 33, 2006. [11] L. Birkedal, K. St\u00f8vring, and \nJ. Thamsborg. Realizability semantics of parametric polymorphism, general references, and recursive types. \nIn Proceedings of FOSSACS, 2009. [12] L. Birkedal, B. Reus, J. Schwinghammer, K. St\u00f8vring, J. Thamsborg, \nand H. Yang. Step-indexed Kripke models over recursive worlds. In Proceedings of POPL, pages 119 132, \n2011. [13] D. Dreyer, A. Ahmed, and L. Birkedal. Logical step-indexed logical relations. In LICS, pages \n71 80. IEEE Computer Society, 2009. ISBN 978-0-7695-3746-7. [14] D. Dreyer, G. Neis, and L. Birkedal. \nThe impact of higher-order state and control effects on local relational reasoning. In Proceedings of \nICFP, 2010. [15] D. Gifford and J. Lucassen. Integrating functional and imperative pro\u00adgramming. In ACM \nConference of LISP and Functional Programming, 1986. [16] F. Henglein, M. Makholm, and H. Niss. A direct \napproach to control\u00ad.ow sensitive region-based memory management. In Prcoeedings of PPDP, 2001. [17] \nF. Henglein, H. Makholm, and H. Niss. Effect types and region-based memory management. In B. Pierce, \neditor, Advanced Topics in Types and Programming Languages. MIT Press, 2005. [18] J. Lucassen and D. \nGifford. Polymorphic effect systems. In Proceed\u00adings of POPL, 1988. [19] A. Nanevski, G. Morrisett, and \nL. Birkedal. Polymorphism and sepa\u00adration in hoare type theory. In J. H. Reppy and J. L. Lawall, editors, \nICFP, pages 62 73. ACM, 2006. ISBN 1-59593-309-3. [20] A. M. Pitts. Relational properties of domains. \nInf. Comput., 127(2): 66 90, 1996. [21] M. Tofte and J.-P. Talpin. Implementation of the typed call-by-value \n.-calculus using a stack of regions. In Proceedings of POPL, 1994.  \n\t\t\t", "proc_id": "2034773", "abstract": "<p>We present a Kripke logical relation for showing the correctness of program transformations based on a type-and-effect system for an ML-like programming language with higher-order store and dynamic allocation.</p> <p>We show how to use our model to verify a number of interesting program transformations that rely on effect annotations.</p> <p>Our model is constructed as a step-indexed model over the standard operational semantics of the programming language. It extends earlier work [7, 8]that has considered, respectively, dynamically allocated first-order references and higher-order store for global variables (but no dynamic allocation). It builds on ideas from region-based memory management [21], and on Kripke logical relations for higher-order store [12, 14].</p> <p>Our type-and-effect system is region-based and includes a region-masking rule which allows to hide local effects. One of the key challenges in the model construction for dynamically allocated higher-order store is that the meaning of a type may change since references, conceptually speaking, may become dangling due to region-masking. We explain how our Kripke model can be used to show correctness of program transformations for programs involving references that, conceptually, are dangling.</p>", "authors": [{"name": "Jacob Thamsborg", "author_profile_id": "81418598508", "affiliation": "IT University of Copenhagen, Copenhagen, Denmark", "person_id": "P2801458", "email_address": "thamsborg@itu.dk", "orcid_id": ""}, {"name": "Lars Birkedal", "author_profile_id": "81100622053", "affiliation": "IT University of Copenhagen, Copenhagen, Denmark", "person_id": "P2801459", "email_address": "birkedal@itu.dk", "orcid_id": ""}], "doi_number": "10.1145/2034773.2034831", "year": "2011", "article_id": "2034831", "conference": "ICFP", "title": "A kripke logical relation for effect-based program transformations", "url": "http://dl.acm.org/citation.cfm?id=2034831"}