{"article_publication_date": "09-19-2011", "fulltext": "\n Just do It: Simple Monadic Equational Reasoning Jeremy Gibbons and Ralf Hinze Department of Computer \nScience, University of Oxford Wolfson Building, Parks Road Oxford, OX1 3QD, England http://www.cs.ox.ac.uk/{jeremy.gibbons,ralf.hinze}/ \nAbstract One of the appeals of pure functional programming is that it is so amenable to equational reasoning. \nOne of the problems of pure functional programming is that it rules out computational effects. Moggi \nand Wadler showed how to get round this problem by us\u00ading monads to encapsulate the effects, leading \nin essence to a phase distinction a pure functional evaluation yielding an impure im\u00adperative computation. \nStill, it has not been clear how to reconcile that phase distinction with the continuing appeal of functional \npro\u00adgramming; does the impure imperative part become inaccessible to equational reasoning? We think not; \nand to back that up, we present a simple axiomatic approach to reasoning about programs with computational \neffects. Categories and Subject Descriptors D.2.4 [Software/Program Veri.cation]: Correctness proofs, \nProgramming by contract; F.3.1 [Specifying and Verifying and Reasoning about Programs]: Asser\u00adtions, \nLogics of programs, Pre-and post-conditions, Speci.cation techniques; F.3.3 [Studies of Program Constructs]: \nFunctional constructs. General Terms Languages, Theory, Veri.cation. Keywords Monads, equational reasoning, \nLawvere theories, alge\u00adbraic speci.cation. 1. Introduction Pure functional programming languages are \ngood for equational reasoning [36]: non-strict semantics supports algebraic manipula\u00adtion through the \nprinciple of substitution of equals for equals . But on the face of it, purity is very limiting: many \nprograms of inter\u00adest involve impure computational effects, such as mutable state and nondeterminism. \nFamously, Moggi [16] and Wadler [38] showed how monads can be used to neatly encapsulate these effects. \nA pure program can assemble a representation of an effectful computation as a plain value, which can \nthen be executed at the top level instead of being printed out Haskell is the world s .nest imperative \npro\u00adgramming language [21]. Permission to make digital or hard copies of all or part of this work for \npersonal or classroom use is granted without fee provided that copies are not made or distributed for \npro.t or commercial advantage and that copies bear this notice and the full citation on the .rst page. \nTo copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speci.c \npermission and/or a fee. ICFP 11, September 19 21, 2011, Tokyo, Japan. Copyright c &#38;#169; 2011 ACM \n978-1-4503-0865-6/11/09. . .$10.00. Nevertheless, little work has been done on the combination of equational \nreasoning and programs exhibiting computational ef\u00adfects. Current practice involves simulating an effectful \ncomputation as a pure function, and conducting the reasoning on this pure value; for example, a recent \npaper by Hutton and Fulger [9] presents a proof of correctness of a stateful computation using equational \nrea\u00adsoning in terms of pure state-transforming functions. This works, but it is unsatisfactory in a number \nof ways: for one thing, it breaches the abstraction boundary provided by the monadic inter\u00adface, inhibiting \nany reuse of proof efforts across programs using different classes of effect; and for another, not all \ncomputational effects can be adequately simulated in this way. In this paper, we present a simple axiomatic \napproach to equa\u00adtional reasoning with monadic programs, preserving the monadic abstraction. The key \nidea is to exploit the algebraic properties of both the generic monad interface (the return and bind \noperators, and their associativity and unit laws) and the speci.c operations used to support a particular \nclass of effects (for example, the put and get operations of mutable state, or the choice points in non\u00addeterministic \ncomputations). In essence, this is an approach based more on the algebraic theory interpretation of universal \nalgebra within category theory [13] than on the later monad interpretation [14] more familiar to functional \nprogrammers. Either can be used to model computational effects in a pure setting [10], with nearly equivalent \nexpressiveness, but the algebraic theory interpretation emphasizes the speci.cs of a particular class \nof effects, whereas the monad interpretation focusses on the general notion of effectful computations \nand says little about speci.c effects. In passing, we present (what we believe to be) a novel approach \nto reasoning about programs that exploit both nondeterministic and probabilistic choice. The monad of \nnondeterminism (that is, the list functor, or more accurately the .nite powerset functor) is very familiar \nto functional programmers [38, 20]. Less well known in functional programming circles, but nevertheless \nwell established in programming language semantics, is the monad of probability distributions [5, 11, \n27, 3]. We show that these two monads com\u00adbine neatly, providing a simple uni.ed model of nondeterminism \nand probability, supporting the same simple equational reasoning as any other monad. Models for this \ncombination of effects have been given before (see for example [15]), but we believe that none are as \nstraightforward as ours. The remainder of the paper is structured as follows. We warm up in Section 3 \nwith a discussion of a simple problem involving effect\u00adful computation counting the disc moves in the \nTowers of Hanoi puzzle. In Sections 4, 5, and 6 we consider some more interest\u00ading effects: nondeterminism, \nexceptions, and mutable state, respec\u00adtively. In Section 7 we look at programs that combine two classes \nof effect, nondeterminism and state; and in Section 8 we look at probabilistic computations, and their \ncombination with nondeter\u00adminism. Finally, we return to Hutton and Fulger s tree relabelling problem \nin Section 9, and Section 10 concludes. But we start in Section 2 with some background on monads. 2. \nBackground As is well known [16, 38], the categorical notion of a monad pre\u00adcisely expresses an abstraction \nof sequential computations, which is necessary for controlling the consequences of computational ef\u00adfects \nin a lazy functional programming language. The return and bind methods model the identity and sequential \ncomposition of computations, respectively. They are captured in the following type class: class Monad \nm where return :: a . ma (>>=) :: ma . (a . mb) . mb  The two operations are required to satisfy three \nlaws, corresponding to the monoidal properties of sequential composition: return x >>= k = kx mx >>= \nreturn = mx (mx >>= k) >>= k' = mx >>=(. x . kx >>= k') We will make use of two important specializations \nof the operations associated with a monad, which more precisely match the idea of the identity computation \nand sequential composition: skip :: Monad m . m () skip = return () (>>) :: Monad m . ma . mb . mb mx \n>> my = mx >>= const my We will also make signi.cant use of the derived operator liftM :: Monad m . \n(a . b) . ma . mb liftM f mx = mx >>= return . f The reader may enjoy checking that the unit and associativity \nproperties imply that liftM satis.es the map laws of a functor: liftM id = id liftM (f . g)= liftM f \n. liftM g An uncurried version of Haskell s liftM2 can be de.ned as liftM f . pair, where pair sequences \na pair of monadic computations: pair :: Monad m . (ma,ma) . m (a,a) pair (mx,my)= mx >>= . x . my >>= \n. y . return (x,y) 2.1 Imperative functional programming The return and bind operations of the Monad \nclass are suf.cient to support a very convenient monad comprehension or do nota\u00adtion for computations, \noffering an elegant imperative programming style [37, 23]. The body of a do expression consists of a \nnon-empty sequence of quali.ers, which may be expressions, generators, or local de.nitions except for \nthe last quali.er, which must be an expression. In this paper, we restrict attention to patterns consisting \nof simple variables or tuples of such; then the do notation can be de.ned in terms of just >>= and ordinary \nlet (and return, which is usually needed for the .nal expression): do {e} = e do {e ; es} = e >> do {es} \ndo {x . e ; es} = e >>= . x . do {es} do {let decls ; es} = let decls in do {es} For example, an alternative \nde.nition of pair is pair (mx, my)= do {x . mx ; y . my ; return (x,y)} (More generally, the pattern \nmatch against a generated value might be refutable; then some mechanism for handling failed matches is \nnecessary. In Haskell, the Monad class has an additional operation fail for this purpose; we will do \nwithout.) The do notation makes it clearer that the monad laws are not awkward impositions, but properties \nessential for manipulating se\u00adquential compositions and blocks: do {y . return x ; ky} = do {kx}do {x \n. mx ; return x} = do {mx}do {x . mx ; y . kx ; k' y} = do {y . do {x . mx ; kx} ; k' y} where, in the \nthird law, x is not free in k'. The .rst two laws state that return is a unit of sequential composition, \nand the third that nested blocks can be .attened, given appropriate care over bound variables. 3. A counter \nexample: Towers of Hanoi The Monad class speci.es only the very basic general-purpose plumbing of sequential \ncomposition. To obtain any interesting computational effects, one must augment the interface with ad\u00additional \nmethods. In this paper, we will consistently do this by de.ning subclasses of Monad, rather than by declaring \ninstances of Monad with additional operations; and we will studiously program and reason according to \nthe interface, not to a particular implemen\u00adtation. For example, here is a class of monads supporting \na simple effect of counting: class Monad m . MonadCount m where tick :: m ()  And here is a program \nfor solving the Towers of Hanoi problem it ticks the counter once for each move of a disc. hanoi :: MonadCount \nm . Int . m () hanoi 0 = skip hanoi (n + 1)= hanoi n >> tick >> hanoi n We claim that hanoi n = rep \n(2n - 1) tick where rep repeats a unit computation a .xed number of times: rep :: Monad m . Int . m \n() . m () rep 0 mx = skip rep (n + 1) mx = mx >> rep n mx (This is a type specialization of the function \nreplicateM in the Haskell standard library.) Note that rep 1 mx = mx rep (m + n) mx = rep m mx >> rep \nn mx The veri.cation of hanoi is by induction on n. The base case is trivial; for the inductive step, \nwe assume the result for n, and calculate: hanoi (n + 1) = [[ de.nition of hanoi ]] hanoi n >> tick >> \nhanoi n = [[ inductive hypothesis ]] rep (2n - 1) tick >> tick >> rep (2n - 1) tick = [[ property of \nrep ]] rep ((2n - 1)+ 1 +(2n - 1)) tick = [[ arithmetic ]] rep (2n+1 - 1) tick  This is a very simple \nexample, because the additional algebraic structure in MonadCount is free no laws were imposed on tick, \nand the only equivalences between counting programs are those induced by the monoidal structure of sequential \ncomposition. In the rest of the paper, we will look at more interesting classes of effect. 4. Nondeterministic \ncomputations Nondeterministic programs are characterized by the ability to choose between multiple results, \nand (sometimes) by the possi\u00adbility of returning no result at all. We express these as two separate subclasses \nof Monad. 4.1 Failure We consider failure .rst, because it is the simpler feature: class Monad m . MonadFail \nm where fail :: ma  The fail operation is a left zero of sequential composition: fail >> m = fail but \nnot a right zero a computation of the form mx >> fail might yield some effects from mx before failing, \nand so be different from fail alone. Note that this is a different design from that in Haskell standard \n[22], which provides a fail method in the Monad class itself (as discussed in Section 2.1), as well as \nan mzero method as part of a more extensive MonadPlus class (which we will discuss below) but which is \nsilent on the properties expected of fail. There is a proposal dating from 2006 on the Haskell Wiki [39] \nto reform the standard along the lines we have chosen, but the situation is still open. We will make \nsigni.cant use of the function guard, which checks that a boolean condition holds, and fails if it does \nnot: guard :: MonadFail m . Bool . m () guard b = if b then skip else fail and, later on, a related \nfunction that takes a predicate: assert :: MonadFail m . (a . Bool) . ma . ma assert p mx = do {x . mx \n; guard (px) ; return x}  4.2 Choice Choice is modelled by the class MonadAlt: class MonadAlt m where \n( 0) :: ma . ma . ma subject to the axiom that 0 is associative, and composition dis\u00adtributes leftwards \nover it: (m 0 n) 0 p = m 0 (n 0 p) (m 0 n) >>= k =(m >>= k) 0 (n >>= k) Following [7], and in contrast \nto the additive monads of Gon\u00adcharov et al. [6], we do not always require composition to distribute rightwards \nover choice: in general, m >>= . x . k1 x 0 k2 x =(m >>= k1) 0 (m >>= k2) As was the case above with \nfail on the right of a composition, this is a dubious proposition when m has non-idempotent effects, \nsuch as writing output: on the left-hand side above, the effects happen once, and on the right, twice. \nSometimes, though, we do want this distributivity property; Section 7 is a case in point. 4.3 Nondeterminism \nWe model nondeterministic programs as those using the combina\u00adtion of failure and choice features: class \n(MonadFail m, MonadAlt m) . MonadNondet m where There are no additional operations; the body of this \nclass def\u00adinition is empty, and the operations of MonadNondet are just those of MonadFail (namely fail) \ntogether with those of MonadAlt (namely 0). However, there are additional laws, beyond those of MonadFail \nand MonadAlt considered in isolation: fail should be a unit of 0, giving a monoidal structure altogether. \nThe obvious model of the speci.cation (indeed, the initial one) is given by lists: instance Monad [] \nwhere return a =[a] mx >>= k = concat (map k mx) instance MonadFail [] where fail =[] instance MonadAlt \n[] where (0) = (++) However, sometimes we do not want to treat the ordering of ele\u00adments as signi.cant, \nso we might sometimes impose an additional axiom that 0 is commutative, in which case .nite bags form \nthe initial model. And sometimes we do not want to treat multiplic\u00adity of results as signi.cant either, \nso we add the axiom that 0 is idempotent; then .nite sets form the initial model. Note that MonadNondet \nis rather like the Haskell standard s MonadPlus class [22]. However, as we shall see in Section 5, there \nis an alternative interpretation of the same monoidal structure intuitively, as exceptions rather than \nnondeterminism. The signa\u00adtures of the operations are the same, but the laws are different, so the two \ninterpretations should most de.nitely not be confused with each other. (In particular, it makes no sense \nat all to suppose that the catch operator in exception handling is commutative!) 4.4 Permutations Here \nis a simple example of a nondeterministic program. The function select takes a non-empty list and nondeterministically \nchooses an element, returning that element and the remaining list; it fails on the empty list. select \n:: MonadNondet m . [a] . m (a, [a]) select [] = fail select (x : xs)= return (x,xs) 0 do {(y,ys) . select \nxs ; return (y,x : ys)} Using select, the function perms nondeterministically generates a permutation \nof a (.nite) list. perms :: MonadNondet m . [a] . m [a] perms [] = return [] perms xs = do {(y,ys) . \nselect xs ; zs . perms ys ; return (y : zs)}  We will exploit the laws of nondeterminism in Section \n7 and 8. 5. Exceptional computations Before we explore the rami.cations of reasoning about nondeter\u00adminism \nmonadically, it is worth pausing to consider again the im\u00adportance of the laws associated with a class. \nThere is another in\u00adterpretation of the signature of the class MonadNondet, namely a monad with an imposed \nmonoidal structure; but the additional laws are quite different, and hence so of course is the computational \nbe\u00adhaviour. We have in mind the class of effect called exceptions. As with nondeterminism, there is \na special constant fail, which is a left zero of composition. There is also a binary operator, which \nwe will call catch, which forms a monoid in conjunction with fail. We capture the latter by means of \na distinct subclass MonadExcept of MonadFail: class MonadFail m . MonadExcept m where catch :: ma . ma \n. ma  The idea is that fail raises an exception, and that catch m h executes body m, but passes control \nto the handler h if an exception is raised during the execution of m. This operational intuition is expressed \naxiomatically by the laws that catch and fail form a monoid: catch fail h = h catch m fail = m catch \nm (catch h h')= catch (catch m h) h' that unexceptional bodies need no handler: catch (return x) h = \nreturn x and that exceptions are left-zeros of sequential composition: fail >> mx = fail No other properties \nare expected; in particular, we expect that in general distributivity of composition over exception handlers \nwill fail: (catch m h) >>= k = catch (m >>= k)(h >>= k) exceptions raised by k on the left-hand side \nmay be erroneously handled on the right-hand side. Conversely, with nondeterminism there is no requirement \nthat pure computations are left zeros of 0. 5.1 Fast product We can illustrate reasoning with exceptions \nvia the fast product algorithm, which multiplies the elements of a list of integers, but raises an exception \nif it .nds a zero, subsequently catching the exception and returning zero for the overall product. We \nde.ne fastprod :: MonadExcept m . [Int] . m Int fastprod xs = catch (work xs)(return 0) where work is \nspeci.ed by work :: MonadFail m . [Int] . m Int work xs = if 0 . xs then fail else return (product xs) \n And we calculate: fastprod xs = [[ de.nition of fastprod ]] catch (work xs)(return 0) = [[ speci.cation \nof work ]] catch (if 0 . xs then fail else return (product xs)) (return 0) = [[ lift out the conditional \n]] if 0 . xs then catch fail (return 0) else catch (return (product xs)) (return 0) = [[ laws of catch, \nfail, and return ]] if 0 . xs then return 0 else return (product xs) = [[ arithmetic: 0 . xs =. product \nxs = 0 ]] if 0 . xs then return (product xs) else return (product xs) = [[ redundant conditional ]] return \n(product xs) Thus, fastprod is pure, never throwing an unhandled exception. Moreover, work may be re.ned \nseparately, to eliminate the multiple traversals; one can use the universal property of foldr to derive \nwork = foldr next (return 1) where next n mx = if n 0 then fail else liftM (n\u00d7) mx  The two steps of \nthe calculation concerning conditionals depend only on pure reasoning: for non-bottom boolean b, we have: \nif b then fx else fy = f (if b then x else y) if b then x else x = x 6. Stateful computations Perhaps \nthe foremost class of effect that springs to mind is that of stateful computations. These are captured \nby the interface class Monad m . MonadState s m | m . s where get :: ms put :: s . m ()  Here, the class \nMonadState s m denotes monads m involving func\u00adtions that transform a state of type s; the declaration \nincludes a functional dependency, declaring that m determines s. The opera\u00adtion get yields a copy of \nthe current state, and put overwrites the state with a new value (returning unit); they are related by \nfour ax\u00adioms: put s >> put s ' = put s ' put s >> get = put s >> return s get >>= put = skip get >>= \n. s . get >>= ks = get >>= . s . kss 6.1 Eight queens As an example of a stateful computation, we consider \na few pro\u00adgrams for the n queens problem. First, we start with a purely func\u00adtional implementation, implicitly \nplacing one queen in each column of the board, and explicitly assigning the rows of the queens from the \npermutation of the column numbers, so that by construction it is only the diagonals that are potential \nthreats. The main data structure we manipulate is a pair of lists recording the up-diagonals and down-diagonals \nthreatened by the queens to the right of a given column, where a queen in position (c,r) threatens up-diagonal \nr -c and down-diagonal r + c. For notational brevity, we de.ne type Square a =(a,a) square :: (a . b) \n. (Square a . Square b) square f (a,b)=(fa, fb) and write a2 and f 2 for Square a and square f , respectively. \nThe essence of the program is the following function test, which takes a (column, row) position for a \nqueen, and lists of already threatened up-and down-diagonals, and returns a pair consisting of a boolean \nfor whether this queen position is safe, and updated lists of threatened up-and down-diagonals. test \n:: Int2 . [Int]2 . (Bool,[Int]2) test (c,r)(ups,downs)= (u ./ups . d ./downs, (u : ups,d : downs)) where \n(u,d)=(r - c,r + c)  Predicate safe1 checks a putative placement of queens for safety, using a fold \nfrom right to left over the list of column-row pairs. The carrier (Bool,[Int]2) of the fold consists \nof a boolean indicating whether the queens to the right, which have already been checked, are safe, and \nthe up-and down-diagonals under threat from the queens considered so far. safe1 :: [Int]2 . [Int2 ] . \n(Bool,[Int]2) safe1 = foldr step1 . start1 start1 :: [Int]2 . (Bool,[Int]2)  start1 updowns =(True,updowns) \nstep1 :: Int2 . (Bool,[Int]2) . (Bool,[Int]2) step1 cr (restOK, updowns)=(thisOK . restOK, updowns') \nwhere (thisOK,updowns')= test cr updowns To allow convenient expression below of the relationship be\u00adtween \nsafe1 and some later variants, we have factored out the initial value ([ ], [ ]) for the threatened diagonals, \nand have not projected away the .nal diagonals; so the actual test for whether a list crs of column-row \npairs is safe is fst (safe1 ([ ],[ ]) crs). In particular, the safe arrangements of n queens can be computed \nin a generate\u00adand-test fashion, nondeterministically choosing row lists as permu\u00adtations and discarding \nthe unsafe ones: queens :: MonadNondet m . Int . m [Int] queens n = do {rs . perms [1..n] ; guard (fst \n(safe1 empty (place n rs))) ; return rs} place n rs = zip [1..n] rs empty = ([], [ ])  6.2 Queens, statefully \nRather than do the whole safety computation as a pure function, we now consider a version that uses a \nstateful computation to build up the sets of up-and down-diagonals threatened by the queens considered \nso far. The function returns a boolean, while statefully constructing the lists of threatened diagonals. \nsafe2 :: MonadState [Int]2 m . [Int2 ] . m Bool safe2 = foldr step2 start2 start2 :: MonadState [Int]2 \nm . m Bool start2 = return True step2 :: MonadState [Int]2 m . Int2 . m Bool . m Bool step2 cr k = do \n{b'. k ; uds . get ; let (b,uds')= test cr uds ; put uds' ; return (b . b')} In Figure 1, we prove using \nthe axioms of get and put that safe2 crs = do {uds . get ; let (ok,uds')= safe1 uds crs ; put uds' ; \nreturn ok}  by showing that this do expression satis.es the universal property of the fold in safe2. \nThis explains how the stateful computation safe2 relates to the pure function safe1. However, in order \nto revise the de.nition of queens to use safe2 instead of safe1, we need to reconcile the different computational \nsettings: perms is nondeter\u00administic, and safe2 is stateful. We could .atten the stateful com\u00adputation \nto a pure function: if we assume also the existence of a rei.cation function run :: MonadState s m . \nma . (s . (a,s)) then we have run (safe2 crs) updowns = safe1 updowns crs and obtain a plug-in replacement \nfor the use of safe1 in queens. We will not do that; instead, we will lift the two components perms and \nsafe2 into one uni.ed setting, providing both nondeterministic and stateful effects. 7. Combining effects \nOne distinct advantage of the axiomatic approach to reasoning with effects is that it is straightforward \nto take reasoning conducted in one setting and reuse it in another that is, indeed, the general bene.t \nof programming to an interface . In the case of the n-queens problem, we have a nondeterministic program \ngenerating candidate solutions (namely, permutations of the rows of the chessboard), and a stateful program \ntesting those candidates (namely, the safety check acting on a pair of lists of diagonals). To that end, \nwe name the combination of effects: class (MonadState s m,MonadNondet m) . MonadStateNondet s m | m . \ns  There are no additional operations; MonadStateNondet s oper\u00adations are just those of MonadState (namely, \nget and put) to\u00adgether with those of MonadNondet (namely, fail and 0). But, as with MonadNondet itself \nearlier, we need to specify how the two classes of effect interact. In this case, we want backtrackable \nstate, with nondeterministic behaviour taking priority over stateful behaviour a failing computation \nshould discard any accumulated stateful effects, and choice points should be explored from a com\u00admon \nstarting state. This is captured equationally by stipulating that failure is a right zero of sequential \ncomposition: m >> fail = fail and that composition distributes rightwards over choice: m >>= . x . k1 \nx 0 k2 x =(m >>= k1) 0 (m >>= k2) Recall that the MonadNondet class already requires fail tobea left \nzero of composition, and composition to distribute leftwards over choice. It would be unreasonable to \nrequire the additional two laws in general, because not all effects are backtrackable; but this is not \nan issue for state transformations. The alternative is to keep calm and carry on state persists over \nfailure, and is threaded linearly through choice points but this is not what we want for the n-queens \nproblem. (Failure happens in selecting from the empty list when generating a permutation, and when discovering \nthat a putative queen position is already threatened, and in both cases we should abandon that alternative \nand try an alternative. Choices take place between permutations, and each permutation should be checked \nstarting from a clean slate.) The types s . [(a,s)] and s . ([a],s) are both models of stateful and nondeterministic \ncomputations, but only the .rst is a model of backtrackable state. A crucial consequence of failure being \na left and right zero of composition (and skip being a left and right unit) is that guards commute with \nanything: guard b >> m = m >>= . x . guard b >> return x Indeed, the same applies for any expression \ninvolving only the effects of MonadFail: if mx has type MonadFail m . ma and my has type MonadStateNondet \nm . mb, then do {x . mx ; y . my ; return (x,y)} = do {y . my ; x . mx ; return (x,y)}  7.1 Queens, \nstatefully and nondeterministically Now, from the axiom of state that get >>= put = skip, we have queens \nn = do {s . get ; put s ; queens n} and from that we can calculate using basic properties of monads, \ntogether with the fact that perms is independent of the state and so commutes with put, that queens n \n= do {s . get ; rs . perms [1..n] ; put empty ; ok . safe2 (place n rs) ; put s ; guard ok ; return rs} \n This program is a little clumsy, but it does accurately capture the re\u00adlationship with the non-stateful \nversion of queens: when interpreted For the empty list, we have: do {uds . get ; let (ok,uds')= safe1 \nuds [] ; put uds' ; return ok} = [[ de.nition of safe1 ]] do {uds . get ; let (ok,uds')= start1 uds ; \nput uds' ; return ok} = [[ de.nition of start1 ]] do {uds . get ; let (ok,uds')=(True,uds) ; put uds' \n; return ok} = [[ lifting the let; get >>= put = return () ]] return True = [[ de.nition of safe2 ]] \nsafe2 []  while for non-empty lists, we have: do {uds . get ; let (ok,uds')= safe1 uds (cr : crs) ; \nput uds' ; return ok} = [[ de.nition of safe1 asa foldr ]] do {uds . get ; let (ok,uds')= step1 cr (safe1 \nuds crs) ; put uds' ; return ok} = [[ introduce a let ]] do {uds . get ; let (b' ,uds'')= safe1 uds \ncrs ; let (ok, uds')= step1 cr (b' ,uds'') ; put uds' ; return ok} = [[ de.nition of step1 ]] do {uds \n. get ; let (b' ,uds'')= safe1 uds crs ; let (b,uds''')= test cr uds'' ; let (ok,uds')=(b . b' ,uds''') \n; put uds' ; return ok} = [[ put s >> put s ' = put s ' ]] do {uds . get ; let (b' ,uds'')= safe1 uds \ncrs ; let (b,uds''')= test cr uds'' ; let (ok,uds')=(b . b' ,uds''') ; put uds'' ; put uds' ; return \nok} = [[ move two of the lets ]] do {uds . get ; let (b' ,uds'')= safe1 uds crs ; put uds'' ; let (b,uds''')= \ntest cr uds'' ; let (ok,uds')=(b . b' ,uds''') ; put uds' ; return ok} = [[ put s >> get >>= k = put \ns >> ks ]] do {uds . get ; let (b' ,uds'')= safe1 uds crs ; put uds'' ; uds'''' . get ; let (b,uds''')= \ntest cr uds'''' ; let (ok,uds')=(b . b' ,uds''') ; put uds' ; return ok} = [[ associativity of >>= ]] \n do {b'. do {uds . get ; let (ok,uds'')= safe1 uds crs ; put uds'' ; return ok} ; uds'''' . get ; let \n(b,uds''')= test cr uds'''' ; let (ok,uds')=(b . b' ,uds''') ; put uds' ; return ok} = [[ de.nition of \nstep2 ]] step2 cr (do {uds . get ; let (ok, uds'')= safe1 uds crs ; put uds'' ; return ok}) Figure 1. \nProving the relationship between safe2 and safe1 in a stateful setting, it amounts to remembering the \ncurrent state s, executing a stateful version of queens, and restoring the original state s again. 7.2 \nQueens, exploratively There is a more natural generate-and-test algorithm for the n\u00adqueens problem: rather \nthan computing a boolean ok for whether an arrangement of queens is safe, and then asserting ok, we explore \nthe possible threats to each queen in turn, and immediately aban\u00addon this arrangement if ever two queens \ncon.ict. How does this algorithm relate to the previous one? Very straightforwardly, as it turns out. \nSince guards commute with anything, in the version of the queens program from Section 7.1, the put s \nand guard ok in par\u00adticular commute, and we have: queens n = do {s . get ; rs . perms [1..n] ; put empty \n; ok . safe2 (place n rs) ; guard ok ; put s ; return rs} Let us therefore de.ne safe3 crs = safe2 crs \n>>= guard so that queens n = do {s . get ; rs . perms [1..n] ; put empty ; safe3 (place n rs) ; put \ns ; return rs}  Now, let us investigate safe3. Recall that safe2 is a fold. Moreover, (>>=guard) combines \nwith that fold using the fusion property, as we now show. The initial value is simple: start2 >>= guard \n= guard True = skip For the inductive step, we have: step2 cr k >>= guard = [[ de.nition of step2 ]] \ndo {b'. k ; uds . get ; let (b, uds')= test cr uds ; put uds' ; return (b . b')} >>= guard = [[ do-notation \n]] do {b'. k ; uds . get ; let (b, uds')= test cr uds ; put uds' ; guard (b . b')}= [[ guard distributes \nover conjunction ]] do {b'. k ; uds . get ; let (b, uds')= test cr uds ; put uds' ; guard b ; guard b'}= \n[[ guards commute with anything ]] do {b'. k ; guard b' ; uds . get ; let (b,uds')= test cr uds ; put \nuds' ; guard b}= [[ associativity of >>= ]] do {(k >>= guard) ; uds . get ; let (b,uds')= test cr uds \n; put uds' ; guard b}= [[ de.nition of step3 (see below) ]] step3 cr (k >>= guard) where we de.ne step3 \ncr m = m >> do {uds . get ; let (b,uds')= test cr uds ; put uds' ; guard b} Therefore, by fold fusion, \nwe have safe3 crs = foldr step3 skip We have derived this exploratory version of the n-queens algo\u00adrithm \nfrom the merely stateful and nondeterministic one using plain old-fashioned equational reasoning. Of \ncourse, it is still a rather inef.cient algorithm; we have simply used the problem as a vehicle for demonstrating \nthe modularity achievable using the ax\u00adiomatic approach to specifying effects. 8. Probabilistic computations \nThe observation that probability distributions form a monad is fairly well known; the .rst published \ndescription appears to date from 1981 [5], but (as with so many things) the credit seems to go back to \nan observation by Lawvere twenty years earlier [12]. Jones and Plotkin [11] use this theory to construct \na powerdomain of probability distributions, allowing them to give a semantics to recursive programs with \nprobabilistic features; Ramsey and Pfeffer [27] use it to de.ne a probabilistic lambda calculus for .nitely \nsup\u00adported distributions; Erwig and Kollmansberger [3] describe a little monadic Haskell library for \nprogramming with .nitely supported distributions. We suppose a type Prob of probabilities (say, the rationals \nin the closed unit interval), and de.ne a type class of .nitely supported probability distributions by: \nclass Monad m . MonadProb m where choice :: Prob . ma . ma . ma  The idea is that choice p mx my behaves \nas mx with probability p and as my with probability 1 - p. From now on, we will write \u00afp for 1-p, and \nfollowing Hoare [8], write choice in in.x notation, mx < p r my , because this makes the laws more transparent. \nWe have two identity laws: mx < 0 r my = my mx < 1 r my = mx a skewed commutativity law: mx < p r my \n= my < p\u00afr mx idempotence: mx < p r mx = mx and quasi-associativity: mx < p r (my < q r mz)=(mx < r \nr my) < s r mz .= p = rs . s\u00af= p\u00afq\u00af (As informal justi.cation for the associativity law, observe that \nthe likelihoods of mx,my,mz on the left are p,pq\u00af,p\u00afq\u00af, and on the right are rs, \u00afs, and a little algebra \nshows that these are pairwise rs, \u00afequal, given the premise.) Moreover, bind distributes leftwards and \nrightwards over choice: (mx < p r my) >>= k =(mx >>= k) < p r (my >>= k) mx >>= . x . k1 x < p r k2 x \n=(mx >>= k1) < p r (mx >>= k2) where, in the second law, x is assumed not to occur free in p. For example, \nhere is a function to generate a uniform distribu\u00adtion from a .nite list of outcomes: uniform :: MonadProb \nm . [a] . ma uniform [x]= return x uniform (x : xs)= return x < 1/length (x:xs) r uniform xs Note that \nuniform xs is side-effect-free (uniform xs >> m = m), because bind distributes leftwards over choice. \n8.1 The Monty Hall Problem As an example, consider the so-called Monty Hall Problem [28], which famously \ncaused a controversy following its discussion in Marilyn vos Savant s column in Parade magazine in 1990 \n[35]. Vos Savant described the problem as follows (quoting a letter from a reader, Craig F. Whitaker): \nSuppose you re on a game show, and you re given the choice of three doors: Behind one door is a car; \nbehind the others, goats. You pick a door, say No. 1, and the host, who knows what s behind the doors, \nopens another door, say No. 3, which has a goat. He then says to you, Do you want to pick door No. 2? \nIs it to your advantage to switch your choice? Implicitly, the car is equally likely to be behind each \nof the three doors, the car is the prize and the goats are booby prizes, the host always opens a door, \nand it always differs from the one you pick and always reveals a goat, and you always get the option \nto switch. We might model this as follows. There are three doors: data Door = A | B | C deriving (Eq, \nShow) doors :: [Door] doors =[A, B,C]  First, the host hides the car behind one of the doors, chosen \nuni\u00adformly at random: hide :: MonadProb m . m Door hide = uniform doors  Second, you pick one of the \ndoors, also randomly: pick :: MonadProb m . m Door pick = uniform doors  Third, the host teases you \nby opening one of the doors not the one that hides the car, nor the one you picked to reveal a goat, \nchoosing randomly among the one or two remaining doors: tease :: MonadProb m . Door . Door . m Door tease \nh p = uniform (doors \\\\ [h,p])  (Here, the expression xs\\\\ys denotes the list of those elements of xs \nabsent from ys.) Fourth, the host offers you the choice between two strategies either to switch to the \ndoor that is neither your original choice nor the opened one: switch :: MonadProb m . Door . Door . m \nDoor switch p t = return (head (doors \\\\ [p, t]))  or to stick with your original choice: stick :: MonadProb \nm . Door . Door . m Door stick p t = return p  (In either case, you know p and t but not h.) Here s \nthe whole game, parametrized by your strategy, returning whether you win the car: play :: MonadProb m \n. (Door . Door . m Door) . m Bool play strategy = do h . hide --host hides the car behind door h p . \npick --you pick door p t . tease h p --host teases you with door t (= h,p) s . strategy p t --you choose, \nbased on p and t return (sh) --you win iff your choice s equals h We will show below that the switching \nstrategy is twice as good as the sticking strategy: play switch = uniform [True,True,False] play stick \n= uniform [False, False,True]  The key is the fact that uniform choices are independent, in the sense \nthat choosing consecutively from two uniform distributions is equivalent to choosing simultaneously from \ntheir cartesian product: pair (uniform x,uniform y)= uniform (cp x y) where cp :: [a] . [b] . [(a,b)] \ncp x y = [(a,b) | a . x,b . y]  We omit the straightforward proof by induction. Expanding de.nitions \nand exploiting independence of uniform choices, we have play strategy = do {(h,p) . uniform (cp doors \ndoors) ; t . tease h p ; s . strategy p t ; return (sh)} So we calculate: play stick = [[ de.nition \nof play ]] do {(h,p) . uniform (cp doors doors) ; t . tease h p ; s . stick p t ; return (sh)}= [[ stick \np t = return p ]] do {(h,p) . uniform (cp doors doors) ; t . tease h p ; return (ph)}= [[ t unused, and \nuniform side-effect-free; liftM ]] liftM (uncurry ( ))(uniform (cp doors doors)) = [[ naturality of uniform; \nde.nition of cp, ]] uniform [True, False,False,False,True,False,False, False,True] = [[ simplifying: \nthree Trues, six Falses ]] uniform [True, False,False] and play switch = [[ de.nition of play ]] do \n{(h,p) . uniform (cp doors doors) ; t . tease h p ; s . switch p t ; return (sh)}= [[ switch p t = return \n(the (doors \\\\ [p,t])) see below ]] do {(h,p) . uniform (cp doors doors) ; t . tease h p ; s . return \n(the (doors \\\\ [p,t])) ; return (sh)}= [[ return is left unit ]] do {(h,p) . uniform (cp doors doors) \n; t . tease h p ; return (h the (doors \\\\ [p, t]))}= [[ case analysis on hp see below ]] do {(h,p) . \nuniform (cp doors doors) ; if (hp) then return False else return True}= [[ lift out conditional; booleans \n]] do {(h,p) . uniform (cp doors doors) ; return (h p)} = [[ de.nition of liftM ]] liftM (uncurry ( \n)) (uniform (cp doors doors)) = [[ naturality of uniform; de.nition of cp, ]] uniform [False,True,True,True,False,True,True, \nTrue, False] = [[ simplifying: three Falses, six Trues ]] uniform [False,True,True] For the second step \nabove, note that t is by construction distinct from p, and so doors \\\\ [p, t] is a singleton; we therefore \nintroduce the function the such that the [a]= a. Now for the case analysis. For the case h = p, we have: \ndo {t . tease h p ; return (h the (doors \\\\ [p, t]))} = [[ using h = p ]] do {t . tease h p ; return \n(h the (doors \\\\ [h, t]))} = [[ h is not in doors \\\\ [h,t] ]] do {t . tease h p ; return False} = [[ \nt unused, and uniform x side-effect-free ]] do {return False}  And for the case h = p, we have: do {t \n. tease h p ; return (h the (doors \\\\ [p,t]))} = [[ de.nition of tease ]] do {t . uniform (doors \\\\ [h,p]) \n; return (h the (doors \\\\ [p,t]))} = [[ h = p, so doors \\\\ [h,p] is a singleton ]] do {let t = the (doors \n\\\\ [h,p]) ; return (h the (doors \\\\ [p,t]))} = [[ h = p, and t = h,p; so t, h,p distinct ]] do {let t \n= the (doors \\\\ [h,p]) ; return (hh)} = [[ t unused ]] do {return True}  This concludes our proof that \nvos Savant was right, and that the many mathematics PhDs who wrote in to Parade magazine chas\u00adtizing \nher were at best thinking about a different problem. 8.2 Probability and nondeterminism One might argue \nthat a more accurate representation of the Monty Hall scenario allows the host a nondeterministic rather \nthan prob\u00adabilistic choice in hiding and teasing: Monty is in charge, and no\u00adbody says that he has to \nplay fair. The interaction of nondeterministic and probabilistic choice is notoriously tricky [40, 15], \nbut it turns out to be mostly straightfor\u00adward to give a model in terms of monads. We combine the two \nclasses MonadAlt (interpreted commutatively and idempotently) and MonadProb of effects: class (MonadAlt \nm,MonadProb m) . MonadAltProb m where As before, there are no new operations. But there is an additional \nlaw, relating the two kinds of choice probabilistic choice should distribute over nondeterministic: m \n< p r (n1 0 n2)=(m < p r n1) 0 (m < p r n2) Intuitively, no freedom is lost from the program on the \nleft by making the nondeterministic choice before the probabilistic one rather than afterwards. As a \nconsequence of this distribution property, the semantic model is roughly as sets of distributions, as \nfor example with the probabilistic predicate transformer work of McIver and Morgan [15]. But not quite \nsets; in order to retain the idempotence of probabilistic choice mx < p r mx = mx we have to live with \nequivalence up to convex closure [33] that is, if a computation may yield any two distributions d, d', \nthen it may also yield their convex combination r \u00d7d +r\u00af\u00d7d' for any r with 0 . r . 1. This is intuitively \nreasonable; if d,d' are possible outcomes of an experiment, then a sequence of experiments may yield \nany combination of ds and d's. The alternative solution, promoted by Varacca [34], is to abandon idempotence; \nthis is unappealing to us, because it means that unused choices, such as Monty s tease in the face of \nthe stick strategy, cannot be discarded. As a simple example of a computation that mixes nondetermin\u00adistic \nand probabilistic features, consider the basic operations of a fair coin toss [15]: coin :: MonadProb \nm . m Bool coin = return True < 1/2 r return False and an arbitrary boolean choice: arb :: MonadAlt \nm . m Bool arb = return True 0 return False sequentially combined in either order: arbcoin,coinarb :: \nMonadAltProb m . m Bool arbcoin = do {a . arb ; c . coin ; return (ac)} coinarb = do {c . coin ; a . \narb ; return (ac)} These two differ; informally, in coinarb the nondeterministic choice can depend on \nthe result of the coin toss, whereas in arbcoin it cannot and of course, the fair probabilistic choice \ndoes not de\u00adpend on the arbitrary nondeterministic choice either otherwise it wouldn t be fair. As sets \nof distributions (here represented as weighted lists), arbcoin has two possible outcomes, both being \na 50 50 distribution, so really only a single possible outcome: arbcoin = {[(True, 1/2),(False, 1/2)], \n[(False, 1/2),(True, 1/2)]} whereas coinarb offers four possible outcomes a 50 50 distribu\u00adtion, in \ntwo different ways, or always False or always True: coinarb = {[(True, 1/2),(False, 1/2)], [(False, 1/2),(True, \n1/2)], [(False, 1/2),(False, 1/2)],[(True, 1/2),(True, 1/2)]} whose convex closure is in fact the set \nof all boolean distributions; that is, the nondeterministic choice in arbcoin provides no .exibil\u00adity, \nbut the one in coinarb can engineer any distribution whatsoever. Returning to the Monty Hall problem, \nwe could allow the host to make nondeterministic rather than probabilistic choices: hide :: MonadAlt \nm . m Door hide = arbitrary doors tease :: MonadAlt m . Door . Door . m Door tease h p = arbitrary (doors \n\\\\ [h,p]) where arbitrary :: MonadAlt m . [a] . ma arbitrary = foldr1 (0) . map return As it happens, \nmaking this change has no effect on the game. The .rst two choices the host s choice of where to hide \nthe car, and your initial choice of door can still be combined, because bind distributes leftwards over \nnondeterministic choice: pair (hide,pick) = [[ let k = . h . liftM (h,) pick ]] hide >>= k = [[ de.nition \nof hide ]] (return A 0 return B 0 return C) >>= k = [[ distributivity ]] (return A >>= k) 0 (return B \n>>= k) 0 (return C >>= k) = [[ return is left unit ]] kA 0 kB 0 kC = [[ de.nition of k ]] liftM (A,) \npick 0 liftM (B,) pick 0 liftM (C, ) pick (where, for brevity, we have written (A,) for the function \n. x . (A, x)). The remainder of the reasoning proceeds just as before, and the conclusion is still that \nthe strategy switch wins two times in three, and stick only one time in three. 9. Tree relabelling Finally, \nwe turn our attention to the question that inspired our interest in reasoning about monadic programs \nin the .rst place. Hutton and Fulger [9] present a nice problem involving effectful computation with \nmutable state, concerning relabelling of trees. Given is a polymorphic datatype of trees, data Tree a \n= Tip a | Bin (Tree a)2 (we continue to use the (-)2 shorthand for pairs from Section 6.1) and a type \nof symbols newtype Symbol = ... deriving (Eq) with which to relabel. The problem is to prove that for \nsome suitable de.nition of a function relabel that takes trees t :: Tree a to relabelled trees u :: Tree \nSymbol, we have distinct (labels u) for each such u, where labels :: Tree Symbol . [Symbol] labels (Tip \na)=[a] labels (Bin (t,u)) = labels t ++ labels u distinct :: [Symbol] . Bool distinct [] = True distinct \n(l : ls)= l ./ls . distinct ls For calculational convenience, in this section we will use uncurried \nversions of some familiar operators: add :: Int2 . Int add = uncurry (+) cat :: [a]2 . [a] cat = uncurry \n(++) disjoint :: Eq a . [a]2 . Bool disjoint = null . uncurry intersect We will also write simply M \nin place of liftM throughout Sec\u00adtion 9, for brevity. 9.1 Fresh symbols We ll assume a class of monads \nsupporting fresh symbols: class Monad m . MonadFresh m where fresh :: m Symbol  The operation fresh \nis not completely unconstrained; it has to gen\u00aderate symbols that are indeed fresh. We specify this by \nasserting that any sequence of fresh symbols will be distinct. That is, given an operation to generate \na given number of fresh symbols: symbols :: MonadFresh m . Int . m [Symbol] symbols n = sequence (replicate \nn fresh) we require that assert distinct . symbols = symbols Note that we are again combining classes \nof effect, this time of MonadFresh and MonadFail: the two sides of the equation speci\u00adfying fresh have \nthe stronger type quali.cation MonadFreshFail m, where class (MonadFresh m,MonadFail m) . MonadFreshFail \nm As with the combination of state and nondeterminism, to relate the two classes of effect we add the \naxiom that failure is a right zero of composition; that is, we again take a backtracking interpretation, \nand indeed, one might think of MonadFresh as modelling a kind of stateful computation (where the state \nis the store of fresh symbols). As it turns out, the only property we will require of the predicate distinct \nis that it is segment-closed. Predicate p is segment-closed if p (x ++ y)=. px . py The signi.cance \nof this property is that assert p can be promoted through list concatenation: there is another predicate \nq on pairs of symbol lists such that assert p . M cat . pair = M cat . assert q . pair . (assert p)2 \n In particular, when p = distinct, then q = disjoint suf.ces: the concatenation of two lists is all \ndistinct if both lists are all distinct, and they are also disjoint. For the remainder of the section, \nwe will use only p and q, not distinct and disjoint themselves. Abstracting out the relevant properties \nof the predicate is helpful, because it means that the reasoning to follow can be generalized to other \nproblems. For example, we might suppose that the Symbol type is not just an equality type, but an enumeration, \nand we want to prove that tree relabelling yields a contiguous segment of the enumeration of all symbols \ninformally, that no fresh symbols are wasted. Then the same condition on symbols suf.ces, but where the \npredicate p is is a contiguous segment of the enumeration this is segment closed, with q being the predicate \non pairs are adjacent segments : q (xs,ys)= null xs . null ys . (succ (last xs) head ys)  9.2 Tree relabelling \nTree relabelling is a tree fold, using fresh at each tip. Given a fold combinator foldt :: (a . b) . \n(b2 . b) . Tree a . b foldt f g (Tip a)= fa foldt f g (Bin (t,u)) = g (foldtf gt,foldtf gu)  then we \nde.ne relabel :: MonadFresh m . Tree a . m (Tree Symbol) relabel = foldt (M Tip . const fresh)(M Bin \n. pair)  Extracting the distinct symbol list from a tree is another fold, but this time within MonadFail: \ndlabels :: MonadFail m . Tree Symbol . m [Symbol] dlabels = foldt (return . wrap)(M cat . assert q . \npair)  where wrap a =[a] makes a singleton list. Our claim is that the composition of dlabels and relabel \nnever fails the symbol list satis.es the predicate, and we always get a proper symbol list of the appropriate \nlength: dlabels relabel = symbols . size where m n = join . Mm . n is Kleisli composition, and size \n:: Tree a . Int size = foldt (const 1) add  We justify that claim in the next section. 9.3 Verifying \ntree relabelling First, we show that relabelling a tree and extracting its distinct symbols fuse to a \nsingle fold. We have (dlabels relabel) . Tip = [[ Kleisli composition ]] join . M dlabels . relabel \n. Tip = [[ de.nition of relabel as a fold ]] join . M dlabels . M Tip . const fresh = [[ functors; de.nition \nof dlabels as a fold ]] join . M (return . wrap) . const fresh = [[ functors, monads ]] M wrap . const \nfresh and (dlabels relabel) . Bin = [[ Kleisli composition ]] join . M dlabels . relabel . Bin = [[ \nde.nition of relabel as a fold ]] join . M dlabels . M Bin . pair . relabel2 = [[ functors; de.nition \nof dlabels as a fold ]] join . M (M cat . assert q . pair . dlabels2) . pair . relabel2 = [[ functors; \nnaturality of pair and join ]] M cat . join . M (assert q . pair) . pair . (M dlabels . relabel)2 = [[ \ncommutativity of assertions see below ]] M cat . assert q . join . M pair . pair . (M dlabels . relabel)2 \n= [[ join and pairs see below ]] M cat . assert q . pair . (join . M dlabels . relabel)2 = [[ Kleisli \ncomposition ]] M cat . assert q . pair . (dlabels relabel)2 and so, by the universal property of fold, \ndlabels relabel = foldt drTip drBin where drTip = M wrap . const fresh drBin = M cat . assert q . pair \n There are two non-trivial steps. The .rst is commutativity of as\u00adsertions join . M (assert q)= assert \nq . join which follows from guards commuting with anything. The second is the join and pairs step join \n. listM pair . pair = pair . join2 This does not hold in general. It does hold for commutative mon\u00adads; \nbut our MonadFreshFail is not commutative. However, it also holds on pairs (mmx,mmy) in the special case \nthat the inner com\u00adputation in the mmx has effects only from MonadFail, because those effects commute \nwith those of mmy. In our case, mmx = M dlabels (relabel t), and indeed dlabels introduces only the possi\u00adbility \nof failure, not any statefulness. Now we show that symbols . size is the same fold. We have symbols . \nsize . Tip = [[ de.nition of size as a fold ]] symbols . const 1 = [[ property of symbols ]] M wrap . \nconst fresh  and symbols . size . Bin = [[ speci.cation of symbols ]] assert p . symbols . size . Bin \n= [[ de.nition of size ]] assert p . symbols . add . size2 = [[ properties of symbols; functors ]] assert \np . M cat . pair . (symbols . size)2 = [[ assert p distributes over concatenation ]] M cat . assert q \n. pair . (assert p . symbols . size)2 = [[ speci.cation of symbols ]] M cat . assert q . pair . (symbols \n. size)2  and so, again by the universal property of fold, symbols . size = foldt drTip drBin also. \n(The two properties of symbols referred to are that symbols . const 1 = M wrap . const fresh symbols \n. add = M cat . pair . symbols2  which follow easily from the de.nition of symbols by simple equa\u00adtional \nreasoning.) 10. Conclusions 10.1 Related work We were inspired to write this paper by reading Hutton \nand Fulger on reasoning about effects [9]. We learned about the tree relabelling problem from them, although \nthe same problem is also used as Example 4.4 in the APPSEM 2000 lecture notes on monads and effects by \nBenton, Hughes and Moggi [1]. That particular problem admits a number of different approaches not only \nin abstracting from the speci.cs of the class of computational effects, as we have done here, but also \nin abstracting from the pattern of control, which we discuss in a related paper [4]. Hoare Type Theory \n[17] is a technique for introducing Hoare triples into a dependently typed language, allowing pre-and \npost\u00adconditions to be tracked by the type checker. It is based around a type constructor {P} x : A {Q} \nof Hoare triples, denoting com\u00adputations that, when run in a heap satisfying precondition P, will return \na result x of type A and an updated heap that together satisfy postcondition Q. It has been embedded \ninto the Coq proof system as an axiomatic extension called Ynot [18], and used as the basis for an implementation \nof separation logic [19]. Similarly, Schr\u00a8oder and Mossakowski [29] develop a monad-independent Hoare \nlogic within the HASCASL algebraic speci.cation language, and use this logic for reasoning about dynamic \nreferences. Either of these ap\u00adproaches makes a sound basis for certi.ed development of effectful programs \nin the Hoare Floyd style. Like we do, Swierstra [31] also starts with Hutton and Fulger s tree relabelling \nproblem, and with the observation that it is unfortu\u00adnate to have to expand the de.nitions of return \nand bind for a par\u00adticular monad in order to reason about a program using that monad. He uses these as \na springboard for taking a lightweight approach to Hoare Type Theory, based on the state monad but not \ninvolv\u00ading a heap. Roughly speaking, rather than a single monolithic type State s a =(s . (a,s)) of state \ntransformers (for .xed state type s and varying return type a), Swierstra introduces a family of types \nHoareState p a q indexed by pre-and postconditions, such that the input must be a state satisfying the \nprecondition p, and the out\u00adput will a pair establishing the postcondition q. This allows him to conduct \nHoare-Floyd-style reasoning for programs in the state monad, such as the tree relabelling problem; but \nthe approach does not seem to be directly applicable to other monads. Peyton Jones [21] discusses the \nawkward squad of teletype I/O, concurrency, exceptions, and interfacing to foreign functions, all of \nwhich are effects wrapped up in the Haskell IO monad. He gives an operational semantics for the .rst \nthree, but does not discuss reasoning about programs exploiting those classes of effect. Swierstra and \nAltenkirch [32] provide a simple functional implementation of teletype I/O, mutable state, and concurrency, \nusing the free monad generated by the algebra of the operations supporting each class of effect so in \nfact, taking an approach more closely aligned with algebraic theories, like we do. (Swierstra s doctoral \nthesis [30] extends this work to software transactional memory and distributed arrays too.) 10.2 Hoare-style \nreasoning As we were developing the calculational approach to reasoning about effectful programs in this \npaper, we explored a technique owing more to Hoare triples than to algebraic speci.cations. In the end, \nwe concluded that the Hoare-style technique was less convenient than the algebraic one exhibited in the \nrest of the paper, and (at least for all the examples we considered) unnecessary. Nevertheless, for completeness, \nwe outline the technique here, and explain why we think it did not work so well. The essence of the Hoare-style \ntechnique is the use of asser\u00adtions, so for this section we work within MonadFail. For notational brevity, \nwe will usually write p! for guard p . We ll also write mx {p} for the assertion that monadic computation \nmx establishes boolean postcondition p, a shorthand for do {mx ; p!} = do {mx} in the simple case that \nmx returns unit. More generally, mx may return a meaningful result; more generally still, we might make \nan assertion s1; ... ; sn {p} about a sequence of quali.ers. If these n statements stmts = s1; ... ; \nsn contain m generators with patterns u1,...,um, the returns should be of the m-tuple pats =(u1,...,um) \nof those patterns; so the assertion is a shorthand for the equality: do {stmts ; p!; return pats} = do \n{stmts ; return pats} (We assume for simplicity that no binding shadows any other, although this doesn \nt actually cause a problem. When there is just one generator, the return should be the value of that \npattern; when there are no generators, the return should be the empty tuple. A similar construction is \nused by Erk\u00a8 ok and Launchbury [2].) Some monadic operations are particularly amenable to manipu\u00adlation, \nbecause they are discardable, copyable, and relatively com\u00admutable; we call them queries. These characteristic \nproperties of ' queries q, q are expressed as follows: do {. q ; return ()} = return () do {x . q ; \ny . q ; return (x,y)} = do {x . q ; return (x,x)} do {x . q ; y . q ' ; return (x,y)} = do {y . q ' ; \nx . q ; return (x,y)} For example, the get operation of the state monad is a query. But if we were to \nextend the MonadCount class in Section 3 with an operation reset to reset the counter, this would be \ncopyable but not discardable; in a similar latch monad, set and reset methods would be copyable but not \nrelatively commutable; and the prob\u00adabilistic operation coin in Section 8.2 is discardable but not copy\u00adable. \n(Schr\u00a8oder and Mossakowski [29] call queries pure ; however, in general, queries are not pure in the \nsense of being instances of return.) To illustrate the assertional style, recall the Towers of Hanoi \nproblem from Section 3. We work here with an extended version of the MonadCount type class, offering \nalso an operation to yield the current value of the counter: class Monad m . MonadCount m where tick \n:: m () total :: m Int  Now we no longer intend the free interpretation of this interface; an implementation \nis constrained by the facts that total is a query, and that tick increments the total: m . total ; tick \n; n . total {n = m + 1} The operation total is intended a ghost variable , used only for reasoning and \nnot for execution. We can therefore consider the same program as before: hanoi :: MonadCount m . Int \n. m () hanoi 0 = skip hanoi (n + 1)= do {hanoi n ; tick ; hanoi n} The correctness property of hanoi \nis that t . total ; hanoi n ; u . total {u-t 2n-1} The general approach to discharging such a proof \nobligation is to expand out de.nitions until the .nal assertion is evidently redun\u00addant and can be eliminated. \nThe proof is by induction on n; the base case is straightforward: do {t . total ; hanoi 0; u . total \n; (u-t 20-1)!; return (t,u)}  = [[ de.nition of hanoi ]] and do {t . total ; skip ; u . total ; (u-t \n20-1)!; return (t,u)}= [[ skip is a unit of composition ]] do {t . total ; u . total ; (u-t 20-1)!; return \n(t,u)}= [[ total is a query ]] do {t . total ; (t-t 20-1)!; return (t,t)}= [[ arithmetic; True! is just \nskip ]] do {t . total ; return (t,t)} = [[ reversing .rst three steps ]] do {t . total ; hanoi 0; u . \ntotal ; return (t,u)}  For the inductive step, we assume the statement for n, and calculate: do {t . \ntotal ; hanoi (n + 1) ; u . total ; (u-t 2n+1-1)!; return (t, u)}= [[ de.nition of hanoi ]] do {t . total \n; hanoi n ; tick ; hanoi n ; u . total ; (u-t 2n+1-1)!; return (t, u)}= [[ inserting some queries ]] \ndo {t . total ; hanoi n ; u '. total ; tick ; t '. total ; hanoi n ; u . total ; (u-t 2n+1-1)!; return \n(t,u)}= [[ inductive hypothesis; property of tick ]] do {t . total ; hanoi n ; u '. total ; (u '-t = \n2n-1)!; tick ; t '. total ; (t ' = u ' + 1)!; hanoi n ; u . total ; (u-t ' = 2n-1)!; (u-t 2n+1-1)!; return \n(t,u)} = [[ arithmetic: .nal guard follows from the others ]] do {t . total ; hanoi n ; u '. total ; \n(u '-t = 2n-1)!; tick ; t '. total ; (t ' = u ' + 1)!; hanoi n ; u . total ; (u-t ' = 2n-1)!; return \n(t,u)} = [[ reversing .rst two steps ]] do {t . total ; hanoi (n + 1) ; u . total ; return (t, u)} This \nproof is signi.cantly more verbose than the one in Sec\u00adtion 3, because it phrased indirectly. Calculations \ntend to have a rightwards-pointing triangular ( r ) shape de.nitions are ex\u00adpanded, the .nal assertion \neliminated, and de.nitions contracted again and the second half of each calculation is a mirror image \nof the .rst half. Perhaps the reader feels that the Towers of Hanoi problem is too trivial to form the \nbasis of any verdict. We felt so too, and although we could easily see the simpler solution for that \nproblem, it took a long time to .nd the corresponding solution shown in Section 9 for the tree relabelling \nproblem. As with the Towers of Hanoi, we extended the monad with a ghost operation: class Monad m . MonadFresh \nm where fresh :: m Symbol used :: m (Set Symbol)  so that used is a query, returning the set of symbols \nused so far, and fresh returns and uses up some fresh symbols: x . used ; n . fresh ; y . used {x . y \n. n . y-x} The problem is then to prove that for some suitable de.nition of a function relabel :: MonadFresh \nm . Tree a . m (Tree Symbol) we have u . relabel t {distinct u} for every t, where distinct :: Tree \nSymbol . Bool distinct (Tip a)= True labels :: Tree Symbol . Set Symbol labels (Tip a)= {a} labels (Bin \n(t,u)) = labels t . labels u As it happens, the correctness condition is too weak to support an inductive \nargument, and it has to be strengthened to x . used ; t '. relabel t ; y . used {distinct t'. x . y . \nlabels t'. y-x} The resulting calculation was rather longwinded, and we much prefer the proof presented \nin Section 9. As well as the verbosity of the Hoare-style approach, it seems a little odd that we used \nonly postconditions and not preconditions. We might de.ne the Hoare triple {p} m {q} as a shorthand for \nthe identity do {p!; x . m ; q!; return x} = do {p!; m} But note that this is equivalent to the earlier \nnotation for assertions, since {p} m {q} = p!; m {q} and in particular {True} m {q} = m {q} Others \nhave come to the same conclusion; for example, Schr\u00a8oder and Mossakowski s monad-independent Hoare logic \n[29] simi\u00adlarly de.nes Hoare triples in terms of global evaluation formulae , which are analogous to \nour assertions. 10.3 Discussion We have presented an approach to effectful functional program\u00adming that \ntreats classes of effects as an abstract datatype, with an algebraic speci.cation capturing its operations \nand equations. We didn t start out this way, but it turns out that the approach to which we have been \nled has less to do with monads as introduced into functional programming by Moggi [16] and Wadler [38], \nand more to do with so-called Lawvere theories [13]. Fundamentally, monads arise from adjunctions classically, \nbe\u00adtween algebraic constructions such as term algebras in one direc\u00adtion, and an underlying forgetful \nfunctor in the other direction. Lawvere theories arise more directly from equational theories of operations \nand their laws. Both were developed as categorical for\u00admulations of universal algebra [10], but Lawvere \ntheories start off with the associated operations and their equational properties, whereas with monads \nthese are a secondary notion. Indeed, the ad\u00adjunction classically arising from the construction of the \nfree model of an equational theory yields precisely the monad in Moggi s sense [24]; but the additional \noperations and equations to support a class of effects play little part in the story that starts with \nmonads. Nev\u00adertheless, the monadic view makes a convenient interface for pro\u00adgramming, and of course \nis embedded within the design of pro\u00adgramming languages such as Haskell; so we do not wish to aban\u00addon \nit. (Power [10] points out that monads are in fact slightly more general than Lawvere theories: in particular, \nthe monads for con\u00adtinuations and for partiality do not arise from operations and their equations in \nthe same way as the other monads familiar to func\u00adtional programmers.) There is an intriguing duality \nin the Lawvere theory approach between algebraic operations and effect handlers [25]. For exam\u00adple, the \ncatch operation of MonadExcept is an effect handler in this sense; technically it doesn t form an operation \nof a Lawvere theory, because it doesn t have the right distributivity properties with respect to >>=. \nOperations and handlers can be seen as effect constructors and effect deconstructors , respectively; \nand the han\u00ad distinct (Bin (t,u)) = distinct t . distinct u . (labels t n labels u 0/) dlers can be de.ned \nas folds over the initial algebra induced by the free model of the algebraic operations [26]. Given \nthe amenability of folds to equational reasoning, we feel that this development has consequences worthy \nof further investigation. Acknowledgements We gratefully acknowledge the many helpful comments from mem\u00adbers \nof the Algebra of Programming research group at Oxford, es\u00adpecially Richard Bird, and from participants \nof the IFIP WG 2.8 meeting in Marble Falls and the European Workshop on Com\u00adputational Effects in Ljubljana, \nall of which have improved the presentation of this paper. This work was partially supported by UK Engineering \nand Physical Sciences Research Council grant EP/G034516/1 on Reusability and Dependent Types. References \n[1] N. Benton, J. Hughes, and E. Moggi. Monads and effects. In G. Barthe, P. Dybjer, L. Pinto, and J. \nSaraiva, editors, APPSEM 2000, volume 2395 of LNCS, pages 42 122. Springer, 2002. [2] L. Erk\u00a8ok and J. \nLaunchbury. Recursive monadic bindings. In ICFP, pages 174 185. ACM, September 2000. [3] M. Erwig and \nS. Kollmansberger. Probabilistic functional program\u00adming in Haskell. J. Funct. Prog., 16(1):21 34, 2006. \n[4] J. Gibbons and R. Bird. Effective reasoning about effectful traversals. Work in progress, Mar. 2011. \n[5] M. Giry. A categorical approach to probability theory. In Categorical Aspects of Topology and Analysis, \nvolume 915 of LNM, pages 68 85. Springer, 1981. [6] S. Goncharov, L. Schr\u00a8oder, and T. Mossakowski. Kleene \nmonads: Handling iteration in a framework of generic effects. In CALCO, volume 5728 of LNCS, pages 18 \n33, 2009. [7] R. Hinze. Prolog s control constructs in a functional setting: Axioms and implementation. \nIntern. J. Found. Comput. Sci., 12(2):125 170, 2001. [8] C. A. R. Hoare. A couple of novelties in the \npropositional calculus. Z. Math. Logik Grundlag. Math., 31(2):173 178, 1985. [9] G. Hutton and D. Fulger. \nReasoning about effects: Seeing the wood through the trees. In TFP, May 2008. [10] M. Hyland and J. \nPower. The category theoretic understanding of universal algebra: Lawvere theories and monads. Electron. \nNotes Theoret. Comput. Sci., 172:437 458, 2007. [11] C. Jones and G. Plotkin. A probabilistic powerdomain \nof evaluations. In LICS, pages 186 195, 1989. [12] F. W. Lawvere. The category of probabilistic mappings. \nPreprint we haven t managed to obtain a copy of this manuscript, 1962. [13] F. W. Lawvere. Functorial \nSemantics of Algebraic Theories. PhD thesis, Columbia University, 1963. Also available with commentary \nas Theory and Applications of Categories Reprint 5. [14] S. Mac Lane. Categories for the Working Mathematician. \nSpringer-Verlag, 1971. [15] A. McIver and C. Morgan. Abstraction, Re.nement and Proof for Probabilistic \nSystems. Springer Verlag, 2005. [16] E. Moggi. Notions of computation and monads. Inform. &#38; Comput., \n93(1), 1991. [17] A. Nanevski, G. Morrisett, and L. Birkedal. Hoare Type Theory, polymorphism and separation. \nJ. Funct. Prog., 18(5,6):865 911, 2008. [18] A. Nanevski, G. Morrisett, A. Shinnar, P. Govereau, and \nL. Birkedal. Ynot: Dependent types for imperative programs. In ICFP, pages 229 240, 2008. [19] P. W. \nO Hearn, H. Yang, and J. C. Reynolds. Separation and information hiding. In POPL, pages 268 280, 2004. \n[20] B. O Sullivan, J. Goerzen, and D. Stewart. Real World Haskell. O Reilly, 2008. [21] S. Peyton Jones. \nTackling the awkward squad: Monadic input/output, concurrency, exceptions, and foreign-language calls \nin Haskell. In T. Hoare, M. Broy, and R. Steinbr\u00a8 uggen, editors, Engineering Theories of Software Construction, \nvolume 180 of NATO Science Series, pages 47 96. IOS Press, 2001. [22] S. Peyton Jones. Haskell 98 Language \nand Libraries: The Revised Report. Cambridge University Press, 2003. [23] S. Peyton Jones and P. Wadler. \nImperative functional programming. In POPL, pages 71 84, 1993. [24] G. Plotkin and J. Power. Notions \nof computation determine monads. In FOSSACS, volume 2303 of LNCS, pages 342 356, 2002. [25] G. Plotkin \nand J. Power. Algebraic operations and generic effects. Appl. Cat. Struct., 11(1):69 94, 2003. [26] G. \nD. Plotkin and M. Pretnar. Handlers of algebraic effects. In ESOP, volume 5502 of LNCS, pages 80 94, \n2009. [27] N. Ramsey and A. Pfeffer. Stochastic lambda calculus and monads of probability distributions. \nIn POPL, pages 154 165, 2002. [28] J. Rosenhouse. The Monty Hall Problem: The Remarkable Story of Math \ns Most Contentious Brain Teaser. Oxford University Press, 2009. [29] L. Schr\u00a8oder and T. Mossakowski. \nHASCASL: Integrated higher-order speci.cation and program development. Theoretical Comput. Sci., 410(12-13):1217 \n1260, 2009. [30] W. Swierstra. A Functional Speci.cation of Effects. PhD thesis, University of Nottingham, \nNovember 2008. [31] W. Swierstra. A Hoare logic for the state monad. In TPHOLs, volume 5674 of LNCS, \npages 440 451. Springer-Verlag, 2009. [32] W. Swierstra and T. Altenkirch. Beauty in the beast. In Haskell \nWorkshop, pages 25 36, 2007. [33] R. Tix. Continuous D-Cones: Convexity and Powerdomain Construc\u00adtions. \nPhD thesis, Technische Universit\u00a8at Darmstadt, 1999. [34] D. Varacca and G. Winskel. Distributing probability \nover non\u00addeterminism. Math. Struct. Comput. Sci., 16(1):87 113, 2006. [35] M. Vos Savant. Ask Marilyn. \nParade Magazine, 9th September 1990. See also http://www.marilynvossavant.com/ articles/gameshow.html. \n[36] P. Wadler. A critique of Abelson and Sussman: Why calculating is better than scheming. SIGPLAN Not., \n22(3):8, 1987. [37] P. Wadler. Comprehending monads. Math. Struct. Comput. Sci., 2(4):461 493, 1992. \n[38] P. Wadler. Monads for functional programming. In M. Broy, editor, Program Design Calculi: Proceedings \nof the Marktoberdorf Summer School, 1992. [39] A. Yakeley, et al. MonadPlus reform proposal. http:// \nwww.haskell.org/haskellwiki/MonadPlus_reform_ proposal, Jan. 2006. [40] W. Yi and K. G. Larsen. Testing \nprobabilistic and nondeterministic processes. In Protocol Speci.cation, Testing and Veri.cation XII, \npages 47 61, 1992.     \n\t\t\t", "proc_id": "2034773", "abstract": "<p>One of the appeals of pure functional programming is that it is so amenable to equational reasoning. One of the problems of pure functional programming is that it rules out computational effects. Moggi and Wadler showed how to get round this problem by using monads to encapsulate the effects, leading in essence to a phase distinction - a pure functional evaluation yielding an impure imperative computation. Still, it has not been clear how to reconcile that phase distinction with the continuing appeal of functional programming; does the impure imperative part become inaccessible to equational reasoning? We think not; and to back that up, we present a simple axiomatic approach to reasoning about programs with computational effects.</p>", "authors": [{"name": "Jeremy Gibbons", "author_profile_id": "81339501222", "affiliation": "University of Oxford, Oxford, United Kingdom", "person_id": "P2801361", "email_address": "Jeremy.Gibbons@cs.ox.ac.uk", "orcid_id": ""}, {"name": "Ralf Hinze", "author_profile_id": "81332504302", "affiliation": "University of Oxford, Oxford, United Kingdom", "person_id": "P2801362", "email_address": "ralf.hinze@cs.ox.ac.uk", "orcid_id": ""}], "doi_number": "10.1145/2034773.2034777", "year": "2011", "article_id": "2034777", "conference": "ICFP", "title": "Just do it: simple monadic equational reasoning", "url": "http://dl.acm.org/citation.cfm?id=2034777"}