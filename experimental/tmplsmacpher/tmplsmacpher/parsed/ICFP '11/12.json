{"article_publication_date": "09-19-2011", "fulltext": "\n Implicit Self-Adjusting Computation for Purely Functional Programs Yan Chen Joshua Dun.eld Matthew A. \nHammer Umut A. Acar Max Planck Institute for Software Systems {chenyan, joshua, hammer, umut}@mpi-sws.org \nAbstract Computational problems that involve dynamic data, such as physics simulations and program development \nenvironments, have been an important subject of study in programming languages. Building on this work, \nrecent advances in self-adjusting computation have de\u00adveloped techniques that enable programs to respond \nautomatically and ef.ciently to dynamic changes in their inputs. Self-adjusting programs have been shown \nto be ef.cient for a reasonably broad range of problems but the approach still requires an explicit pro\u00adgramming \nstyle, where the programmer must use speci.c monadic types and primitives to identify, create and operate \non data that can change over time. We describe techniques for automatically translating purely functional \nprograms into self-adjusting programs. In this implicit approach, the programmer need only annotate the \n(top-level) input types of the programs to be translated. Type inference .nds all other types, and a \ntype-directed translation rewrites the source program into an explicitly self-adjusting target program. \nThe type system is related to information-.ow type systems and enjoys decidable type inference via constraint \nsolving. We prove that the transla\u00adtion outputs well-typed self-adjusting programs and preserves the \nsource program s input-output behavior, guaranteeing that trans\u00adlated programs respond correctly to all \nchanges to their data. Using a cost semantics, we also prove that the translation preserves the asymptotic \ncomplexity of the source program. Categories and Subject Descriptors D.1.1 [Programming Tech\u00adniques]: \nApplicative (Functional) Programming; F.3.3 [Logics and Meanings of Programs]: Studies of Program Constructs \nGeneral Terms Algorithms, Languages, Performance 1. Introduction Dynamic changes are pervasive in computational \nproblems: physics simulations often involve moving objects; robots interact with dy\u00adnamic environments; \ncompilers must respond to slight modi.ca\u00adtions in their input programs. Such dynamic changes are often \nsmall, or incremental, and result in only slightly different output, so computations can often respond \nto them asymptotically faster than performing a complete re-computation. Such asymptotic im\u00adprovements \ncan lead to massive speedup in practice but tradition- Permission to make digital or hard copies of all \nor part of this work for personal or classroom use is granted without fee provided that copies are not \nmade or distributed for pro.t or commercial advantage and that copies bear this notice and the full citation \non the .rst page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires \nprior speci.c permission and/or a fee. ICFP 11, September 19 21, 2011, Tokyo, Japan. Copyright &#38;#169; \n2011 ACM 978-1-4503-0865-6/11/09. . . $10.00 ally require careful algorithm design and analysis (e.g., \nChiang and Tamassia [1992]; Guibas [2004]; Demetrescu et al. [2005]), which can be challenging even for \nseemingly simple problems. Motivated by this problem, researchers have developed language\u00adbased techniques \nthat enable computations to respond to dynamic data changes automatically and ef.ciently (see Ramalingam \nand Reps [1993] for a survey). This line of research, traditionally known as incremental computation, \naims to reduce dynamic prob\u00adlems to static (conventional or batch) problems by developing com\u00adpilers \nthat automatically generate code for dynamic responses. This is challenging, because the compiler-generated \ncode aims to handle changes asymptotically faster than the source code. Early propos\u00adals [Demers et al. \n1981; Pugh and Teitelbaum 1989; Field and Teitelbaum 1990] were limited to certain classes of applications \n(e.g., attribute grammars), allowed limited forms of data changes, and/or yielded suboptimal ef.ciency. \nSome of these approaches, however, had the important advantage of being implicit:they re\u00adquired little \nor no change to the program code to support dynamic change conventional programs could be compiled to \nexecutables that respond automatically to dynamic changes. Recent work based on self-adjusting computation \nmade progress towards achieving ef.cient incremental computation by providing algorithmic language abstractions \nto express computations that re\u00adspond automatically to changes to their data [Ley-Wild et al. 2008; Acar \net al. 2009]. Self-adjusting computation can deliver asymp\u00ad totically ef.cient updates in a reasonably \nbroad range of problem domains [Acar et al. 2007, 2010a], and have even helped solve chal\u00ad lenging open \nproblems [Acar et al. 2010b]. Existing self-adjusting computation techniques, however, require the programmer \nto pro\u00adgram explicitly by using a certain set of primitives [Carlsson 2002; Ley-Wild et al. 2008; Acar \net al. 2009]. Speci.cally the program\u00ad mer must manually distinguish stable data, which remains the same, \nfrom changeable data, which can change over time, and operate on changeable data via a special set of \nprimitives. As a result, rewriting a conventional program into a self-adjusting pro\u00adgram requires extensive \nchanges to the code. For example, a purely functional program will need to be rewritten in imperative \nstyle using write-once, monadic references. In this paper, we present techniques for implicit self-adjusting \ncomputation that allow conventional programs to be translated au\u00adtomatically into ef.cient self-adjusting \nprograms. Our approach consists of a type system for inferring self-adjusting computation types from \npurely functional programs and a type-guided transla\u00adtion algorithm that rewrites purely functional programs \ninto self\u00adadjusting programs. The type system hinges on a key observation connecting self\u00adadjusting computation \nto information .ow [Pottier and Simonet 2003; Sabelfeld and Myers 2003]: both involve tracking data de\u00ad \npendencies (of changeable data and sensitive data, respectively) as well as dependencies between expressions \nand data. Speci.\u00adcally, we show that a type system that encodes the changeability of data and expressions \nin self-adjusting computation as secrecy of information suf.ces to statically enforce the invariants \nneeded by self-adjusting computation. The type system uses polymorphism to capture stable and changeable \nuses of the same data or expres\u00adsion. Our type system admits a constraint-based formulation where the \nconstraints are a strict subset of those needed by traditional information-.ow type systems. Consequently, \nas with information .ow, our type system admits an HM(X) inference algorithm [Oder\u00adsky et al. 1999] that \ncan infer all type annotations from top-level type speci.cations on the input of a program. For this \nwork, determination of types via type inference is not an end unto itself but a means for translating \npurely functional programs into self-adjusting programs. To achieve this, we .rst present a set of compositional, \nnon-deterministic transformation rules. Guided by the types, the rules identify the set of all change\u00adable \nexpressions that operate on changeable data and rewrite them into the self-adjusting target language. \nWe then present a deter\u00administic translation algorithm that applies the compositional rules judiciously, \nconsidering the type and context (enclosing expres\u00adsions) of each translated subexpression, to generate \na well-typed self-adjusting target program. Taken together, the type system, its inference algorithm, \nand the translation algorithm enable translating purely functional source programs to self-adjusting \ntarget programs using top-level type an\u00adnotations on the input type of the source program. These top-level \ntype annotations simply mark what part of the input data is sub\u00adject to change. Figure 1 illustrates \nhow source programs written in Level ML, a purely functional subset of ML with level types, can be translated \nto self-adjusting programs in the target language AFL, a language for self-adjusting computation with \nexplicit prim\u00aditives [Acar et al. 2006]. We prove three critical properties of the approach. Type soundness. \nOn source code of a given type, the trans\u00adlation algorithm produces well-typed self-adjusting code of \na corresponding target type (Theorem 6.1).  Observational equivalence. The translated self-adjusting \npro\u00adgram, when evaluated, produces the same value as the source program (Theorem 6.5).  Asymptotic \ncomplexity. The time to evaluate the translated program is asymptotically the same as the time to evaluate \nthe source program (Theorem 6.9).  Type soundness and observational equivalence together imply a critical \nconsistency property: that self-adjusting programs respond correctly to changing data (via the consistency \nof the target self\u00adadjusting language [Acar et al. 2006]). The third property shows that the translated \nprogram takes asymptotically as long to evaluate (from scratch) as the corresponding source program. \nIn addition, it places a worst-case bound on the time taken to self-adjust via change propagation, which \ncan and often does take signi.cantly less time when data changes are small. To prove this complexity \nresult, we use a cost semantics [Sands 1990; Sansom and Peyton Jones 1995] that enables precise reasoning \nabout the complexity of Type inference Evaluation e : t Level ML e  in k steps Type Observational \nType-Directed Soundness Equivalence Evaluation Translation  w AFL e. : t. in T(k) steps Figure 1. \nVisualizing the translation between the source language Level ML and the target language AFL, and related \nproperties. the explicit self-adjusting-computation mechanisms employed in the target language and thus \ncan be applied broadly. Paper guide. We .nd it better to give an overview of the proposed approach by \nfocusing on the translation problem and working back to the type system in a top-down manner (Section \n2). The details of the translation algorithm and our theorems, however, rely on the type system. We therefore \ntake a more bottom-up approach in the rest of the paper: we .rst present the static semantics (the syntax \nand the type system) (Sections 3 and 4), and then describe the target language AFL (Section 5) and the \ntranslation (Section 6). Finally, we discuss related work (Section 7) and conclude. Due to space restrictions, \nwe include all the proofs in the appendix [Chen et al. 2011]. 2. Overview We present an informal overview \nof our approach via examples. First we brie.y describe explicit self-adjusting computation, as laid out \nin previous work, and which we use as a target language. Then we outline our proposed approach. 2.1 Explicit \nSelf-Adjusting Computation The key concept behind explicit approaches is the notion of a modi.able (reference), \nwhich stores changeable values that can change over time [Acar et al. 2006]. The programmer operates \non modi.ables with mod, read,and write constructs to create, read from, and write into modi.ables. The \nrun-time system of a self\u00adadjusting language uses these constructs to represent the execution as a graph, \nenabling ef.cient change propagation when the data changes in small amounts. As an example, consider \na trivial program that computes x 2 +y: squareplus: int * int . int fun squareplus (x, y) = let x2=x \n*x in let r= x2 +y in r  To make this program self-adjusting with respect to changes in y, while leaving \nx unchanging or stable, we assign y the type int mod (of modi.ables containing integers) and read the \ncon\u00adtents of the modi.able. The body of the read is a changeable ex\u00adpression ending with a write. This \nfunction has a changeable arrow type .: C the evaluation time. We do not, however, prove tighter bounds \non squareplus_SC: int * int mod . int C the complexity of self-adjustments; this would be beyond the \nscope of this paper. We intend to complete an implementation of our approach as an extension of Standard \nML and the MLton compiler [MLton]. However, we expect the proposed approach could be implemented in other \nlanguages such as Haskell, where self-adjusting libraries also exist [Carlsson 2002]. In general, since \nour approach simply generates target code, it is agnostic to implementation details of C fun squareplus_SC \n(x, y) = let x2=x *x in read y as y in let r=x2 +y in write(r)  The read operation delineates the code \nthat depends on the changeable value y, and the changeable arrow type ensures a crit\u00adical consistency \nproperty: . -functions can only be called within  the context of a changeable expression. If we change \nthe value of y, change propagation can update the result, re-executing only the read and its body, reusing \nthe computation of the square x2. Suppose we wish to make x changeable while leaving y stable. We need \nto read x and place x2 into a modi.able (because we can only read within the context of a changeable \nexpression), and immediately read back x2 and .nish by writing the sum. (To avoid creating this modi.able \nwould require further structural changes to the code.) squareplus_CS: int mod * int . int C fun squareplus_CS \n(x, y) = let x2 = mod (read x as x in write(x * x )) in read x2 as x2 in let r= x2 +y in write(r) As \nthis example shows, rewriting even a trivial program can require modi.cations to the code, and different \nchoices about what is or is not changeable lead to different code. Moreover, if we need SC and squareplus \n CS for instance, if we want to pass squareplus to various higher-order functions we must write, and \nmaintain, both versions. Conservatively treating all data as changeable would require writing just one \nversion, but treating all data as modi.able can in\u00adtroduce unacceptably high overhead. At the other extreme, \nmaking everything stable requires no rewriting, but forgoes the bene.ts of change propagation. Instead, \nwe take an approach where data is modi.able only where necessary. squareplus  2.2 Implicit Self-Adjusting \nComputation To make self-adjusting computation implicit, we use type infor\u00admation to insert reads, writes, \nand mods automatically. The user annotates the input type of the program; we infer types for all ex\u00adpressions, \nand use this information to guide a translation algorithm. The translation algorithm returns well-typed \nself-adjusting target programs. The translation requires no expression-level annotations. For the example \nfunction squareplus above, we can automat\u00adically derive squareplus SC and squareplus CS from just the \ntype of the function (expressed in a slightly different form, as we discuss next). Level types. To uniformly \ndescribe source functions (more gener\u00adally, expressions) that differ only in their changeability , we \nneed a more general type system than that of the target language. This type system re.nes types with \nlevels S (stable) and C (changeable). The type intd is an integer whose level is d; for example, to get \nsquaresum CS we can annotate squaresum s argument with the type intC \u00d7 intS . Level types are an important \nconnection between information\u00ad.ow types [Pottier and Simonet 2003] and those needed for our translation: \nhigh-security secret data (level H) behaves like change\u00adable data (level C), and low-security public \ndata (level L) behaves like stable data (level S). In information .ow, data that depends on secret data \nmust be secret; in self-adjusting computation, data that depends on changeable data must be changeable. \nBuilding on this connection, we develop a type system with several features and mechanisms similar to \ninformation .ow. Among these is level polymorphism; our type system assigns level-polymorphic types to \nexpressions that accommodate various changeabilities . (As with ML s polymorphism over types, our level \npolymorphism is prenex.) Another similarity is evident in our constraint-based type inference system, \nwhere the constraints are a strict subset of those in Pottier and Simonet [2003]. As a corollary, our \nsystem admits a constraint\u00ad based type inference algorithm [Odersky et al. 1999]. Translation. The main \npurpose of our type system is to support translation. Given a source expression and its type, translation \nin\u00ad datatype a list = nil | cons of a * a list inc : int . int fun inc (x) = x+1 map :(a . \u00df) . a list \n. \u00df list fun mapfl = case l of nil . nil | cons(h,t) . cons(f h, map f t) mapPair : (int list * int list) \n. (int list * int list) fun mapPair (l,a) = (map inc l, map inc a) Figure 2. Function mapPair in ML datatype \na listd = nil | cons of a *(a listd) mapPair : ((intS listC ) * (intC listS )) . ((intS listC ) * (intC \nlistS )) S ... (* inc, map, mapPair same as in Figure 1. *) Figure 3. Function mapPair in Level ML, \nwith level types serts the appropriate mod, read,and write primitives and restruc\u00adtures the code to \nproduce an expression that is well-typed in the tar\u00adget language. The type system of the target language, \nwhich is ex\u00adplicitly self-adjusting, is monomorphic in the levels or changeabil\u00adity, while the implicitly \nself-adjusting source language is polymor\u00adphic over levels. Consequently, translation also needs to monomor\u00adphize \nthe source code. Our translation generates code that is well\u00adtyped, has the same input-output behavior \nas the source program, and is, at worst, a constant factor slower than the source program. Since the \nsource and target languages differ, proving these proper\u00adties is nontrivial; in fact, the proofs critically \nguided our formula\u00adtion of the type system and translation algorithm. A more detailed example: mapPair. \nTo illustrate how our trans\u00adlation works, consider a function mapPair that takes two integer lists and \nincrements the elements in both lists. This function can be written by applying the standard higher-order \nmap over lists. Fig\u00adure 2 shows the purely functional code in an ML-like language for an implementation \nof mapPair, with a datatype a list,an incre\u00adment function inc, and a polymorphic map function. Type signa\u00adtures \ngive the types of functions. To obtain a self-adjusting mapPair, we .rst decide how we wish to allow \nthe input to change. Suppose that we want to allow insertion and deletion of elements in the .rst list, \nbut we expect the length of the second list to remain constant, with only its elements changing. We can \nexpress this with the versions of the list type with different changeability: a listC for lists of a \nwith changeable tails;  a listS for lists of a with stable tails.  Then a list of integers allowing \ninsertion and deletion has type intS listC, and one with unchanging length has type intC listS . Now \nwe can write the type annotation on mapPair showninFig\u00adure 3. Given only that annotation, type inference \ncan .nd appropri\u00ad ate types for inc and map and our translation algorithm generates self-adjusting code \nfrom these annotations. Note that to obtain a self-adjusting program, we only had to provide types for \nthe func\u00adtion. We call this language with level types Level ML. Target code for mapPair. Translating \nthe code in Figure 3 pro\u00ad duces the self-adjusting target code in Figure 4. Note that inc and map have \nlevel-polymorphic types. In map inc l we increment sta\u00ad Levels d, e ::= S |C |a datatype a list_S = \nnil | cons of a * a list_S datatype a list_C = nil | cons of a *(a list_C) mod Types t ::= intd |(t1 \n\u00d7t2)d |(t1 +t2)d |(t1 . t2)d e Constraints C, D :: = a = true |false |.&#38; a.C |C .D |\u00df |a =\u00df |d < \nt (* inc specialized for stable data *) inc_S : int S . int funS inc_S (x) = x+1 Type schemes s ::= t \n|.a&#38;[D].t Figure 5. Levels, constraints, types, and type schemes . int funC inc_C (x) = read x as \nx in write (x +1) . But some type assignments are preferable, especially when one inc : .d. intd intd \n d considers constant factors. Choosing C levels whenever possible is val inc = select {d=S . inc_S \nalways a viable strategy, but treating all data as changeable results | d=C . inc_C} in more overhead. \nAs in information .ow, where we want to con\u00ad (* inc specialized for changeable data *) inc_C : int C \n sider data secret only when absolutely necessary, inference yields (a . funS map_SC fl = (* map for \nstable heads, changeable tails *) S . (a list_C) mod . (\u00df list_C) mod S \u00df) map_SC : S principal typings \nthat are minimally changeable, always preferring mod (read l as x in S over C. case x of nil . write \nnil  3. From Information Flow Types to SAC | cons(h,t) . write (cons(f h, map_SC f t))) Self-adjusting \ncomputation separates the computation and data into : . \u00df) . (a list_S) . (\u00df S C (amap_CS list_S) two \nparts: stable and changeable. Changeable data refers to data S funS map_CS fl = (* map for changeable \nheads, stable tails *) case l of nil . nil | cons(h,t) . let val h = mod (f h) in cons(h , map_CS f t) \nmap : .dh,dt. (a . \u00df) . a listdt . \u00df listdt dh S S val map = select {dh=S, dt =C . map_SC | dh=C, dt \n=S . map_CS} mapPair : ((int list_C) mod * (int mod) list_S) that can change over time; all non-changeable \ndata is stable. Sim\u00adilarly, changeable expressions refers to expressions that operate (via elimination \nforms) on changeable data; all non-changeable ex\u00adpressions are stable. Evaluation of changeable expressions \n(that is, changeable computations) can change as the data that they operate on changes: changes in data \ncause changes in control .ow. These distinctions are critical to effective self-adjustment: previous \nwork shows that it suf.ces to track and remember changeable data and evaluations of changeable expressions \nbecause stable data and eval\u00aduations of stable expressions remain invariant over time. Previous work \ntherefore presents languages that enable the programmer to . ((int list_C) mod * (int mod) list_S) S \n separate stable and changeable data, and type systems that enforce the correct usage of these constructs. \nIn this section, we describe the self-adjusting computation types that we infer for purely functional \nprograms. A key insight behind our approach is that in information-.ow type systems, secret (high\u00ad funS \nmapPair (l, a) = (map[dh=S,dt =C] inc[d=S] l, map[dh=C,dt =S] inc[d=C] a) Figure 4. Translated mapPair \nwith mod types and explicit level polymorphism. security) data is infectious: any data that depends \non secret data it\u00adself must be secret. This corresponds to self-adjusting computation: we increment changeable \nintegers, data that depends on changeable data must itself be changeable. In ble integers, and in map \ninc a so the type inferred for inc must be generic: .d. intd . intd.Our addition, self-adjusting computation \nrequires expressions that in\u00ad d translation produces two implementations of inc, one per instanti\u00adspect \nchangeable data elimination forms to be changeable. To ation (d=S and d=C): inc S and inc C (in Figure \n4). Sine we want encode this invariant, we extend function types with a mode,which to use inc with the \nhigher-order function map, we need to generate is either stable or changeable; only changeable functions \ncan in\u00ada selector function that takes an instantiation and picks out the spect changeable data. This \nadditional structure preserves the spirit appropriate implementation:   inc : .d. intd . intd d val \ninc = select {d=S . inc_S | d=C . inc_C} of information .ow-based type systems, and, moreover, supports \nconstraint-based type inference in a similar style. The starting point for our formulation is Pottier \nand Simonet  [2003]. Our types include two (security) levels, stable and change\u00ad able. We generally \nfollow their approach and notation. The two key differences are that (1) since Level ML is purely functional, \nwe need no program counter level pc ; (2) we need a mode e on In mapPair itself, we pass a level instantiation \nto the selector: inc[d=S]. (This instantiation is known statically, so it could be replaced with inc \nS at compile time.) function types. Observe how the single annotation on mapPair led to dupli-Levels. \nThe levels S (stable)and C (changeable) have a total order:  S =SC =CS =C cation of the two functions \nit uses. While inc S is the same as the original inc, the changeable version inc C adds a read and a \nwrite. Note also that the two generated versions of map are both  different from the original. The interplay \nof type inference and translation. Given user an\u00adnotations on the input, type inference .nds a satisfying \ntype assign\u00adment, which then guides our translation algorithm to produce self\u00adadjusting code. In many \ncases, multiple type assignments could sat\u00adisfy the annotations; for example, subsumption allows any \nstable type to be promoted to a changeable type. Translation yields target code that satis.es the crucial \ntype soundness, operational equiva\u00adlence, and complexity properties under any satisfying assignment. \n To support polymorphism and enable type inference, we allow level variables a, \u00df to appear in types. \nTypes. Types consist of integers tagged with their level, prod\u00aducts1 and sums with an associated level, \nand arrow (function) types. Function types (t1 . t2)d carry two level annotations e and d. e 1 In Pottier \nand Simonet [2003], product types are low-security (stable) be\u00adcause pairing adds no extra information. \nIn our setting, changeable products give more control over the granularity of change propagation. Values \nv ::= n|x |(v1,v2)|inl v |inr v |fun f(x)=e d =d t1 <: t(t1 \u00d7t2)t2 <: td =d 1 2 (subProd) (subInt) Expr. \ns e ::= v |.(x1,x2)|fst x|snd x| rd 1 \u00d7t d intd <: intd ( <: t 2 case xof {x1 .e1 ,x2 .e2}|apply(x1,x2) \n|let x=e1 in e2 Figure 9. Abstract syntax of the source language Level ML d =d . dd t1 <: t 1 +t tt<:2 \n. 1 2 (subSum) rd d ( (t1 +t2)d =d t<: 2 e=e t1 <: t1 t2 <: t 2 (subArrow) tions for the variables. \nOur constraints C, Dinclude level-variable d 2) ence composes into conjunctions of satis.ability predicates \n.&#38; a.C. The subtyping and lower bound relations de.ned in Figures 6 Figure 6. Subtyping and 7 consider \nclosed types only. For type inference, we can extend these with a constraint to allow non-closed types. \n. d e t2) . (t1 <: (tt comparisons =and level-type comparisons d<t, which type infer\u00ad 1 e A (ground) \nassignment, written f, substitutes concrete levels S d =d d =d (<-Int) (<-Prod) and C for level variables. \nAn assignment fsatis.es a constraint C, d< intdd < (t1 \u00d7t2)d written f fC, if and only if C holds true \nafter the substitution of variables to ground types as speci.ed by f. We say that C entails d =d d =d \n D, written C I D, if and only if every assignment fthat satis.es (<-Arrow) (<-Sum) d< (t1 t2)d< (t1 \n+t2) Figure 7. Lower bound of a type SS intS O.S. (t1 . t2)S O.S. (t1 \u00d7t2)O.S. (t1 +t2)O.S. e . CC intC \nO.C. (t1 e t2)C O.C. (t1 \u00d7t2)O.C. (t1 +t2)O.C. d1 d2 intd1 \" intd2 (t1 +t2)\" (t1 +t2) d1 d2 (t1 \u00d7t2)\" \n(t1 \u00d7t2)(t1 . t2)d1 \" (t1 .t2)d2 ee e Figure 8. Outer-stable and outer-changeable types, and equality \nup to outer levels The mode e is the level of the computation encapsulated by the function. This mode \ndetermines how a function can manipulate changeable values: a function in stable mode cannot directly \nma\u00adnipulate changeable values; it can only pass them around. By con\u00adtrast, a changeable-mode function \ncan directly manipulate change\u00adable values. The outer level dis the level of the function itself, as \na value. We say that a type is ground if it contains no level variables. Subtyping. Figure 6 shows the \nsubtyping relation t<: t,which is standard except for the levels. It requires that the outer level of \nthe subtype is smaller than the outer level of the supertype and that the modes match in the case of \nfunctions: a stable-mode func\u00adtion is never a subtype or supertype of a changeable-mode func\u00adtion. (It \nwould be sound to make stable-mode functions subtypes of changeable-mode functions, but changeable mode \nfunctions are more expensive; silent coercion would make performance less pre\u00addictable.) Levels and types. \nWe rely on several relations between levels and types to ascertain various invariants. A type t is higher \nthan d, written d < t, if the outer level of the type is at least d.In other words, d is a lower bound \nof the outer level(s) of t. Figure 7 de.nes this relation. We distinguish between outer-stable and outer\u00adchangeable \ntypes (Figure 8). We write t O.S. if the outer level of t is S. Similarly, we write t O.C. if the outer \nlevel of t is C. Finally, two types t1 and t2 are equal up to their outer levels, written t1 \" t2,if \nt1 =t2 or they differ only in their outer levels. Constraints. To perform type inference, we extend levels \nwith level variables a and \u00df, and use a constraint solver to .nd solu- C also satis.es D. We write f(a) \nfor the solution of a in f,and [f]t for the usual substitution operation on types. For example, if arr \n((r(S f(a)=S then [f]inta +intC =intS +intC. Type schemes. A type scheme s is a type with universally \nquan\u00adti.ed level variables: s = .a&#38;[D].t. We say that the variables a&#38;are bound by s. The type \nscheme is bounded by the constraint D, which speci.es the conditions that must hold on the variables. \nAs usual, we consider type schemes equivalent under capture-avoiding renaming of their bound variables. \nGround types can be written as type schemes, e.g. intC as .\u00d8[true].intC . 4. Source Language 4.1 Static \nSemantics Syntax. Figure 9 shows the syntax for our source language Level ML, a purely functional language \nwith integers (as base types), products, and sums. The expressions consist of values (integers, pairs, \ntagged values, recursive functions), projections, case expres\u00adsions, function applications, and let bindings. \nFor convenience, we consider only expressions in A-normal form, which names inter\u00admediate results. A-normal \nform simpli.es some technical issues, while maintaining expressiveness. Constraint-based type system. \nWe could de.ne types as t ::= int |t1 \u00d7t2 |t1 +t2 |t1 .t2 Such a type system would be completely standard. \nInstead, we use a richer type system that allows us to directly translate Level ML pro\u00adgrams into self-adjusting \nprograms in AFL. This constraint-based type system has the level-decorated types, constraints, and type \nschemes in Figure 5 and described in Section 3. After discussing the rules themselves, we will look at \ntype inference (Section 4.2). Typing takes place in the context of a constraint formula C and a typing \nenvironment G that maps variables to type schemes: G::= \u00b7| G,x : s. The typing judgment C;G fe e : t \nhas a constraint C and typing environment G, and infers type t for expression e in mode e. Beyond the \nusual typing concerns, there are three important aspects of the typing rules: the determination of modes \nand levels, level polymorphism, and constraints. To help separate concerns, we discuss constraints later \nin the section at this time, the reader can ignore the constraints in the rules and read C;Gfe e: t as \nGfe e: t, read C I d< t2 as d< t2, and so on. The mode of each typing judgment affects the types that \ncan be used directly by the expression being typed. Speci.cally, the mode discipline prevents the elimination \nforms from being applied  Under constraint C and source typing environment G, source expression e has \ntype t G(x)= .aa[D].t C I .a\u00df/a \u00df.[aa]D (SInt) (SVar) C;Gfe n :intS \u00df/a\u00df/aC . [aa]D;Gfe x :[aa]t C;Gfe \nv1 : t1 C;Gfe v2 : t2 (SPair) C;Gfe (v1,v2):(t1 \u00d7t2)S C;Gfe v : t1 (SSum) C;Gfe inl v :(t1 +t2)S C;G,x \n: t1,f :(t1 . t2)S fe e: t2 C I e< t2 e (SFun) C;Gfe (fun f(x)=e):(t1 . t2)S e C;GfS x1 :intd1 C I d1 \n=d2 C;GfS x2 :intd2 C I d1 = e . :int \u00d7 int . int (SPrim) C;Gfe .(x1,x2): intd1 C;G fS x :(t1 \u00d7t2)d \nC I d =e (SFst) C;G fe fst x:t1 C;G fe e1 : t. C;G,x :t. fe e2 :t C I t<: tC I t. \" t (SLetE) C;Gfe \nlet x=e1 in e2 : t C .D;G fS v1 : t. C;G,x : .a&#38;[D].t. fe e2 : t a&#38;nFV(C,G) =\u00d8 C I t<: tC I \nt. \" t (SLetV) C ..&#38;e let xv1 in e2 : t a.D;Gf= C;Gfx1 :(t1 . t2)d S e C I e. =e C;GfS x2 :t1 C \nI d< t2 (SApp) C;Gfe apply(x1,x2): t2 C;GfS x :(t1 +t2)d C;G,x1 :t1 fe e1 : t C I d =eC I d< tC;G,x2 \n:t2 fe e2 : t (SCase) C;Gfe case xof {x1 .e1 ,x2 .e2}:t Figure 10. Typing rules for Level ML to changeable \nvalues in the stable mode. This is a key principle of the type system. No computation happens in values, \nso they can be typed in ei\u00adther mode. The typing rules for variables (SVar), integers (SInt), pairs (SPair), \nand sums (SSum) are otherwise standard (we omit the symmetric judgment inr v). Rule (SVar) instantiates \na variable s polymorphic type. For clarity, we also make explicit the renaming of the quanti.ed type \nvariables a&#38;to some fresh \u00df&#38;(which will be instantiated later by constraint solving). To type \na function (SFun), we type the body in the mode e speci.ed by the function type (t1 . t2)d, and require \nthe result type t2 to be higher than the e mode, e< t2. As a result, a changeable-mode function must \nhave a changeable return type. This captures the idea that a changeable\u00admode function is a computation \nthat depends on changeable data, and thus its result must accommodate changes to that data. Primi\u00adtive \noperators .take two stable integers and return a stable integer result. As is common in Damas-Milner-style \nsystems, when typing let we can generalize variables in types (in our system, level variables) to yield \na polymorphic value only when the bound expression is a value. This value restriction is not essential \nbecause Level ML is pure, but facilitates adding side effects at a later date. In the .rst case (SLetE), \nthe expression bound may be a non-value, so we do not generalize and simply type the body in the same \nmode as the whole let, assuming that the bound expression has the speci.ed type in any mode e. .2 We \nallow subsumption only when the subtype and supertype are equal up to their outer levels, e.g. from a \nbound expression e1 of subtype intS to an assumption x : intC.This simpli.es the translation, with no \nloss of expressiveness: to handle .. deep subsumption, such as (intSS intS)S <: (intSS intC)C , we \ncan insert coercions into the source program before typing it with these rules. (This process could easily \nbe automated.) In the second case (SLetV), when the expression bound is a value, we type the let expression \nin mode e by typing the body in thesamemode e, assuming that the value bound is typed in the stable mode \n(the mode is ignored in the rules typing values). As in (SLetE), we allow subsumption on the bound value \nonly when the types are equal up to their outer level. Because we are binding a value, we generalize \nits type by quantifying over the type s free level variables. Function application, ., fst,and case are \nthe forms that elim\u00adinate values of changeable type. An application is typed in the mode e. of the function \nbeing applied because changeable func\u00adtions can operate on changeable values; the typing mode must match \n(e. = e). Furthermore, the result of the function must be higher than the function s level: if a function \nis itself changeable, (t1 . t2)C, then it could be replaced by another function and e thus the result \nof this application must be changeable. (Due to let\u00adsubsumption, checking this in (SFun) alone is not \nenough.) The rule (SCase) types a case expression, in either mode e, by typing each branch in e. The \nmode e must be higher than the level d of the scrutinee to ensure that a changeable sum type is not inspected \nat the stable mode. Furthermore, the level of the result t must also be higher than d: if the scrutinee \nchanges, we may take the other branch, requiring a changeable result. Rule (SFst) enforces a condition, \nsimilar to (SCase), that we can project out of a changeable tuple of type (t1 \u00d7t2)C only in changeable \nmode. We omit the symmetric rule for snd. Our premises on variables, such as the scrutinee of (SCase), \nare stable-mode (fS), but this was an arbitrary decision; since (SVar) is the only rule that can derive \nsuch premises, their mode is irrelevant. 4.2 Constraints and Type Inference Many of the rules simply \npass around the constraint C.An imple\u00admentation of rules with constraint-based premises, such as (SFun), \nimplicitly adds those premises to the constraint, so that C = ... .(e < t2). Rule (SLetV) generalizes \nlevel variables instead of type variables, with the occurs check a&#38;nFV(C,G)=\u00d8. Standard techniques \nin the tradition of Damas and Milner [1982] can infer types for Level ML. In particular, our rules and \nconstraints fall within the HM(X) framework [Odersky et al. 1999], permitting inference of principal \ntypes via constraint solving. As always, we cannot infer the types of polymorphically recursive functions. \nUsing a constraint solver that, given the choice between as\u00adsigning S or C to some level variable, prefers \nS, inference .nds principal typings that are minimally changeable. Thus, data and computations will only \nbe made changeable and incur tracking overhead where necessary to satisfy the programmer s annota\u00adtion. \nThis corresponds to preferring a lower security level in in\u00adformation .ow [Pottier and Simonet 2003]. \n 2 In the target language, bound expressions must be stable-mode, but the translation puts changeable \nbound expressions inside a mod, yielding a stable-mode bound expression. Levels d,e ::= S |C Types t \n::= int |t mod |t1 \u00d7t2 |t1 + t2 |t1 . t2 e Type schemes s ::= .a&#38;[D].t Typing G::= \u00b7|G,x: s |G,x \n: t environments Variables x ::= x|x[a&#38;= d] &#38; Values v ::= n |x|f|(v1,v2) |inl v |inr v | SC \nfunS f(x)= e|funC f(x)= e|&#38; select {(a&#38;i = di) .ei} i Expressions e ::= eS |eC Stable eS ::= \nv |.(x1,x2) |fst x |snd x | S expressions applyS(x1,x2) |let x = eS in e| case xof {x1 .eS ,x2 .eS}| \nmod eC C C Changeable e::= applyC(x1,x2) |let x= eS in e| expressions case xof {x1 .eC ,x2 .eC}| read \nxas yin eC |write(x) Figure 11. Types and expressions in the target language AFL  4.3 Dynamic Semantics \nThe call-by-value semantics of source programs is de.ned by a big-step judgment e .v, read e evaluates \nto value v . Our rules in Figure 13 are standard; we write [v/x]e for capture-avoiding substitution of \nv for the variable xin e.  5. Target Language The target language AFL (Figure 11) is a self-adjusting \nlanguage with modi.ables. In addition to integers, products, and sums, the target type system makes a \nmodal distinction between ordinary types (e.g. int) and modi.able types (e.g. int mod). It also distin\u00adguishes \nstable-mode and changeable-mode functions. Level poly\u00admorphism is supported through an explicit select \nconstruct and an explicit polymorphic instantiation. In Section 6, we describe how polymorphic source \nexpressions become selectsin AFL. The values of the language are integers, variables, polymorphic variable \ninstantiation x[a&#38;= &#38;d], locations f (which appear only at runtime), pairs, tagged values, stable \nand changeable functions, and the select construct, which acts as a function and case expression on levels: \nif xis bound to select {(a = S) .e1 |(a= C) .e2}then x[a= S] yields e1. The symbol xstands for a bare \nvariable x or an instantiation x[a&#38;= &#38;d]. We distinguish stable expressions eS from changeable \nexpres\u00adsions eC. Stable expressions create purely functional values; applyS applies a stable-mode function. \nThe mod construct evaluates a changeable expression and writes the output value to a modi.able, yielding \na location, which is a stable expression. Changeable ex\u00adpressions are computations that end in a write \nof a pure value. Changeable-mode application applyC applies a changeable-mode function. The let construct \nis either stable or changeable according to its body. When the body is a changeable expression, let enables \na changeable computation to evaluate a stable expression and bind its result to a variable. The case \nexpression is likewise stable or changeable, according to its case arms. The read expression binds the \ncontents of a modi.able xto a variable yand evaluates the body of the read. The typing rules in Figure \n12 follow the structure of the expres\u00ad sions. Rule (TSelect) checks that each monomorphized expression \nei in a select has type I[&#38;d/&#38;a]tI,where [&#38;d/&#38;a]t is a source-level  Under store typing \n. and target typing environment G, target value v has type scheme s for all d&#38;i such that a&#38;= \nd&#38;i I D .; G fS ei : I[d&#38;i/&#38;a]tI (TSelect) .; G fS select {d&#38;i .ei}:.a&#38;[D].t i  \nUnder store typing . and target typing environment G, target expression ee has target type t .(f)= t \n(TLoc) (TInt) .; G fS f: t .; G fS n: int G(x)= t G(x)=.a&#38;[D].t (TPVar) (TVar) .; G fS x : t .; \nG fS x[a&#38;= &#38;d]: I[&#38;d/&#38;a]tI .; G fS v1 : t1 .; G fS v2 : t2 (TPair) .; G fS (v1,v2): \nt1 \u00d7t2 . .; G,x : t1,f :(t1 e t2) fe e: t2 (TFun) . .; G fS fune f(x)= e:(t1 e t2) .; G fS v : t1 .; \nG fS x : t1 \u00d7t2 (TSum) (TFst) .; G fS inl v : t1 + t2 .; G fS fst x : t1 .; G fS x1 : int .; G fS x2 \n: int f.: int \u00d7int .int (TPrim) .; G fS .(x1,x2): int .; G fS e1 S : s .; G,x : s fe e2 : t. (TLet) \n.; G fe let x = e1 S in e2 : t. .; G fS x1 :(t1 . ) .;G fS x2 : t1 e t2 (TApp) .; G fe applye(x1,x2): \nt2 .; G,x1 : t1 fe e1 : t .; G fS x : t1 + t2 .; G,x2 : t2 fe e2 : t (TCase) .; G fe case xof {x1 .e1 \n,x2 .e2}: t .; G fC e: t .; G fS x : t (TMod) (TWrite) .; G fS mod e: t mod .; G fC write(x): t .; G \nfS x1 : t1 mod .; G,x : t1 fC e2 : t2 (TRead) .; G fC read x1 as xin e2 : t2 Figure 12. Typing rules \nof the target language AFL polymorphic type with the levels &#38;d substituted for the variables a&#38;, \nand I-I translates source types to target types (see Section 6.1). Rule (TPVar) is a standard rule for \nvariables of monomorphic type, but rule (TVar) gives the instantiation x[a&#38;= &#38;d],ofa variable \nxof polymorphic type, the type I[&#38;d/&#38;a]tI matching the monomorphic expression from the select \nto which xis bound. 5.1 Dynamic Semantics For the source language, our big-step evaluation rules (Figure \n13) are standard. In the target language AFL, our rules (Figure 14) model the evaluation of a .rst run \nof the program: modi.ables are created, written to (once), and read from (any number of times), but never \nupdated to re.ect changes to the program input. Both sets of rules permit expressions that are not in \nA-normal form, enabling  Source expression eevaluates to v e1 .v1 e2 .v2 (SEvValue) (SEvPair) v .v (e1,e2) \n.(v1,v2) e1 .v1 e.ve2 .v2 .(v1,v2)= v . (SEvSum) (SEvPrimop) inl e.inl v .(e1,e2) .v . e.(v1,v2) e1 \n.v1 [v1/x]e2 .v2 (SEvFst) (SEvLet) fst e.v1 let x = e1 in e2 .v2 e.inl v1 [v1/x1]e1 .v (SEvCaseLeft) \ncase eof {x1 .e1 ,x2 .e2}.v e1 .fun f(x)= e e2 .v2 [(fun f(x)= e)/f][v2/x]e.v (SEvApply) apply(e1,e2) \n.v Figure 13. Dynamic semantics of source Level ML programs standard capture-avoiding substitution. To \nsimplify the translation, we call instantiations x[a&#38;= &#38;d] values, even though x[a&#38;= &#38;d] \ndoes not evaluate to itself. So we distinguish machine values w which do evaluate to themselves from \nvalues v. The only difference is that machine values do not include x[a&#38;= &#38;d]. Machine w ::= \nn|x|f|(w,w) |inl w |inr w | values fune f(x)= e e |select {(a&#38;i = di) .ei} &#38; i  6. Translation \nWe specify the translation from Level ML to the target language AFL by a set of a rules. Because AFL \nis a modal language that dis\u00adtinguishes stable and changeable expressions, with a corresponding type \nsystem (Section 5), the translation is also modal: the transla\u00ad tion in the stable mode 1.produces a \nstable AFL expression eS , S and the translation in the changeable mode 1.produces a change- C able expression \neC . It is not enough to generate AFL expressions of the right syn\u00adtactic form; they must also have the \nright type. To achieve this, the rules are type-directed: we translate a source expression e at type \nt. But we are transforming expressions from one language to another, where each language has its own \ntype system; translating some e : t cannot produce some e . : t,but some e . : t. where t. is a target \ntype that corresponds to t. To express this vital property, we need to translate types, as well as expressions. \nWe developed the translation of expressions and types together (along with the proof that the property \nholds); the translation of types was instru\u00admental in getting the translation of expressions right. To \nunderstand how to translate expressions, it is helpful to .rst understand how we translate types. 6.1 \nTranslating Types Figure 15 de.nes the translation of types via two mutually recursive functions from \nLevel ML types to AFL types. The .rst function, ItI, tells us what type the target expression eS should \nhave when we translate e in the stable mode, e : t1. eS. We also use it S to translate the types in the \nenvironment G. The second function, ItI-C, makes sense in two related situations: translating the type \nt of an expression e in the changeable mode (e : t1. eC)and C translating the codomain of changeable \nfunctions. In the stable mode, values of stable type can be used and cre\u00adated directly, so the stable \ntranslation IintS I of a stable integer is  In the store ., target expression e evaluates to wwith updated \nstore .. (TEvMachineValue) .fw .(.fw) .fe1 .(.1 fw1) .1 fe2 .(.2 fw2) (TEvPair) .f(e1,e2) .(.2 f(w1,w2)) \n.fe.(.. fw) (TEvSum) .finl e.(.. finl w) .fe1 .(.1 fw1) .1 fe2 .(.2 fw2) .(w1,w2)= w . (TEvPrimop) .f.(e1,e2) \n.(.2 fw ) .fe.(.. f(w1,w2)) (TEvFst) .ffst e.(.. fw1) .fe1 .(.1 fw1) .1 f[w1/x]e2 .(.2 fw2) (TEvLet) \n.flet x = e1 in e2 .(.2 fw2) .fe.(.1 finl w1) .1 f[w1/x1]e1 .(.2 fw) (TEvCaseLeft) .fcase eof {x1 .e1 \n,x2 .e2}.(.2 fw) .fe1 e .(.1 ffune f(x)= e e) .1 fe e 2 .(.2 fw2) .2 f[(fune f(x)= e e)/f][w2/x]e e \n.(.3 fw) (TEvApply) .fapplye(e1e ,e2e) .(.3 fw) .fe.(.. fw) (TEvWrite) .fwrite(e) .(.. fw) C .fe1 .(.1 \nff) .1 f[.1(f)/x]e.(.2 fw) (TEvRead) C .fread e1 as x . in e.(.2 fw) .1 fe1 .(.. fw) (TEvSelectE) &#38;&#38; \n.f(select {...,d .e1,...})[a&#38;= d] .(.. fw) .feC .(.. fw) (TEvMod) .fmod eC .((.,f .w) ff) Figure \n14. Dynamic semantics for .rst runs of AFL programs just int. In contrast, a changeable integer cannot \nbe inspected or di\u00adrectly created in stable mode, but must be placed into a modi.able: IintC I = int \nmod. The remaining parts of the de.nition follow this pattern: the target type is wrapped with mod if \nand only if the outer level of the source type is C. When we translate a changeable-mode function type \n(with C below the arrow), its codomain is translated output-changeable : I(t1 . t2)S I = It1I . It2I-C.The \nCC reason is that a changeable-mode function can only be applied in the changeable mode; the function \nresult is not placed into a mod\u00adi.able until we return to the stable mode, so putting a mod on the codomain \nwould not match the dynamic semantics of AFL. The second function ItI-C de.nes the type of a changeable \nexpression e that writes to a modi.able containing t, yielding a changeable target expression eC. The \nsource type has an outer C, so when the value is written, it will be placed into a modi.able and have \nmod type. But while evaluating eC, there is no outer mod. Thus the translation ItI-C ignores the outer \nlevel (using the S  function -, which replaces an outer level C with S), and never intCS = intS (t1 \n\u00d7t2)CS =(t1 \u00d7t2)S (t1 +t2)CS =(t1 +t2)S (t1 . t2)CS =(t1 . t2)S ee IintS I = int IintC I = int mod I(t1 \n. t2)S I = It1I .It2I S (S ) I(t1 . t2)C I = It1I .It2Imod SS . t2)S I -C I(t1 = It1I .It2I C (C ) I(t1 \n. t2)C I = It1I .It2I-Cmod CC I(t1 \u00d7t2)S I = It1I \u00d7It2 I I(t1 \u00d7t2)C I =(It1I\u00d7It2I)mod I(t1 +t2)= It1I \n+It2 I S I I(t1 +t2)C I =(It1I+It2I)mod S I t I if t O.C. ItI-C = ItI if t O.S. I\u00b7I = \u00b7 ItIf = I[f]tI \n-C -C IG,x : .\u00d8[true].tI = IGI,x :ItIItIf = I[f]tIIG,x :.a&#38;[D].tI = IGI,x :.a&#38;[D].t IGIf = I[f]GI \n-C Figure 15. Stabilization of types t S; translations ItIand ItIof types; translation of typing environments \nIGI returns a type of the form (\u00b7\u00b7\u00b7 mod). However, since the value being returned may contain subexpressions \nthat will be placed into modi.ables, we use I-I for the inner types. For instance, I(t1 +t2)d I-C =It1 \nI+It2I. These functions are de.ned on closed types types with no free level variables. Before applying \none of these functions to a type found by the constraint typing rules, we always need to apply the satisfying \nassignment f to the type, so for convenience we write ItIf for I[f]tI, and so on. Because the translation \nonly makes sense for closed types, type schemes .a&#38;[D].t cannot be translated. The translation IGI \ntherefore translates only monomorphic types t; type schemes are left alone (except for replacing the \nsymbol . with .) until instantiation. Once instantiated, the type scheme is an ordinary closed source \ntype, and can be translated by rule (TVar).  6.2 Translating Expressions We de.ne the translation of \nexpressions as a set of type-directed rules. Given (1) a derivation of C;G fe e : t in the constraint\u00adbased \ntyping system and (2) a satisfying assignment f for C,itis always possible to produce a correctly typed \nstable target expres\u00adsion eS and a correctly typed changeable target expression eC (see Theorem 6.1 below). \nThe environment Gin the translation rules is a source typing environment, but must have no free level \nvariables. Givenanenvironment G from the constraint typing, we apply the satisfying assignment f to eliminate \nits free level variables before using it for the translation: [f]G. With the environment closed, we need \nnot refer to C. Many of the rules in Figure 17 are purely syntax-directed and are similar to the constraint-based \nrules. One exception is the (Var) rule, which needs the source type to know how to instantiate the level \nvariables in the type scheme. For example, given the poly\u00admorphic x : .a[true].(inta . inta)S, weneed \nthetypefrom a C;G fe x :(intC . intC)S so we can instantiate ain the trans- C lated term x[a=C]. Under \nsource typing G, G e \" (x \u00bb x : te ) renaming the head x in e to x: t yields expression e Gfx1 :t . \n(LPrimop1) Gf.(x1,x2) (x1 \u00bbx1 : t f.(x1,x2)) Gfx2 :t (LPrimop2) Gf.(x1,x2) (x2 \u00bbx2 . : t f.(x1,x2. )) \nGfx : t (LFst) G ffst x (x\u00bbx : t ffst x ) Gfx1 : t (LApply) G fapply(x1,x2) (x1 \u00bbx . :t fapply(x . ,x2)) \nGfx : t (LCase) Gfcase xof {x1 .e1 ,x2 .e2} (x\u00bbx . :t fcase x . of {x1 .e1 ,x2 .e2}) Figure 16. Renaming \nthe variable to be read (elimination forms) Our rules are nondeterministic, avoiding the need to decorate \nthem with context-sensitive details. Our algorithm in Section 6.3 resolves the nondeterminism through \ntype information. Stable rules. The rules (Int), (Var), (Pair), (Fun), (Sum), (Fst) and (Prim) can only \ntranslate in the stable mode. To translate to a changeable expression, use a rule that shifts to changeable \nmode. Shifting to changeable mode. Given a translation of ein the sta\u00adble mode to some eS, the rules \n(Write) and (ReadWrite) at the bot\u00adtom of Figure 17 translate ein the changeable mode, producing an eC. \nIf the expression s type t is outer stable (say, intS), the (Write) rule simply binds it to a variable \nand then writes that variable. If t is outer changeable (say, intC ) it will be in a modi.able at runtime, \nso we read it into r . and then write it. (The let-bindings merely satisfy the requirements of A-normal \nform.) Shifting to stable mode. To generate a stable expression eS based on a changeable expression eC, \nwe have the (Lift) and (Mod) rules. These rules require the source type t to be outer changeable: in \n(Lift), the premise t S = t. requires that t S is de.ned, and it is de.ned only for outer changeable \nt; in (Mod), the requirement is explicit: ft O.C. (Mod) is the simpler of the two: if e translates to \neC at type t,then e translates to the stable expression mod eC at type t. In (Lift), the expression is \ntranslated not at the given type t but at its stabilized t S, capturing the shallow subsumption in the \nconstraint typing rules (SLetE) and (SLetV): a bound expression of type t0 S can be translated at type \nt0 S to eS, and then promoted to type t0 C by placing it inside a mod. Reading from changeable data. \nTo use an expression of change\u00adable type in a context where a stable value is needed such as pass\u00ading \nsome x : intC to a function expecting intS the (Read) rule generates a target expression that reads the \nvalue out of x : intC into a variable x . : intS. The variable-renaming judgment Gfe (x \u00bbx . : t fe )takes \nthe expression e, .nds a variable xabout to be used, and yields an expression e . with that occurrence \nre\u00adplaced by x . . For example, G f case xof ... (x \u00bb x . : t f case x . of ...). This judgment is derivable \nonly for apply, case, fst,and ., because these are the elimination forms for outer\u00adchangeable data. For \n.(x1,x2), we need to read both variables, so we have one rule for each. The rules are given in Figure \n16.  Under closed source typing environment G, e G e : t'.esource expression e is translated at type \nt e in mode e to target expression ee G(x)=.a&#38;[D].t (Int) (Var) Gfn :intd 1.n Gfx :[&#38;d/&#38;a]t1.x[a&#38;=&#38;d] \nSS Gfv1 :t1 1.v1 . Gfv2 :t2 1.v2 . SS (Pair) S Gf(v1,v2):(t1 \u00d7t2)1.(v1,v2) S G,x :t1,f :(t1 . t2)S fe:t2 \n1.e e ee (Fun) . S 1. e Gffun f(x)=e:(t1 e t2)S fune f(x)=e Gfv :t1 1.v . S (Sum) Gfinl v :(t1 +t2)S \n1.inl v . S Gfx :(t1 \u00d7t2)S 1.x S (Fst) Gffst x :t1 1.fst x S Gfx1 :intS 1.x1 Gfx2 :intS 1.x2 SS (Prim) \nGf.(x1,x2):intd 1..(x1,x2) S Gfx1 :(t1 . t2)S 1.x1 Gfx2 :t1 1.x2 e SS (App) Gfapply(x1,x2):t2 1.applye(x1,x2) \ne G,x1 :t1 fe1 :t1.e1 . e Gfx:(t1 +t2)S 1.x G,x2 :t2 fe2 :t1.e . S e 2 (Case) Gf case xof {x1 .e1 ,x2 \n.e2}:t 1.case xof {x1 .e1 . ,x2 .e2. } e S Gfe:t . eC t =t. Gfe:t . eC t O.C. CC (Lift) (Mod) Gf e :t \n. mod eC Gf e :t . mod eC SS Gfe1 :t. 1.eS G,x:t. fe2 :t1.e . S e 2 . (LetE) Gflet x =e1 in e2 :t1.let \nx=eS in e e 2 a For all dai s.t. aa=di I D, G,x :.aa[D].t. f e :t .e e Gf v :[dai/aa]t . e S i (LetV) \na Gf let x =v in e :t .let x =select {(aa=di). ei}i in e e Gfe (x\u00bbx :tfe ) t. O.C. . S . C . 1 G,x : \ntfe :t1.eGfx:t.x CS (Read) C Gfe:t1.read xas x . in e C Gfe:t1.eS t O.S. S (Write) Gfe:t1.let r =eS in \nwrite(r) C Gfe:t1.eS t O.C. S (ReadWrite) Gfe:t1.let r =eS in read ras r . in write(r ) C Figure 17. \nMonomorphizing translation Monomorphization. A polymorphic source expression has no di\u00adrectly corresponding \ntarget expression: the map function from Sec\u00adtion 2 corresponds to the two functions map SC and map CS.Given \na polymorphic source value v : .a&#38;[D].t, the (LetV) rule trans\u00ad lates v once for each instantiation \nd&#38;i that satis.es the constraint D (each d&#38;i such that a&#38;= d&#38;i I D). That is, we translate \nthe value at source type [d&#38;i/&#38;a]t. This yields a sequence of source expres\u00adsions e1,...,en for \nthe n possible instances. For example, given .a[true].t, we translate the value at type [S/a]t. yielding \ne1 and at type [C/a]t. yielding e2. Finally, the rule produces a select ex\u00ad pression, which acts as a \nfunction that takes the desired instance d&#38;i and returns the appropriate ei. Since (LetV) generates \none function for each satisfying d&#38;i,it can create up to 2n instances for n variables. However, dead-code \nelimination can remove functions that are not used. Moreover, the functions that are used would have \nbeen handwritten in an explicit setting, so while the code size is exponential in the worst case, the \nsaved effort is as well. 6.3 Algorithm The system of translation rules in Figure 17 is not deterministic. \nIn fact, if the wrong choices are made it can produce painfully inef.cient code. Suppose we have 2: intC \n, and want to translate it to a stable target expression. Choosing rule (Int) yields the target expression \n2. But we could use (Int), then (ReadWrite) which generates an eC with a let,a read and a write then \n(Mod), which wraps that eC in a mod. Clearly, we should have stopped with (Int). To resolve this nondeterminism \nin the rules would complicate them further. Instead, we give the algorithm in Figure 18, which examines \nthe source expression e and, using type information, applies the rules necessary to produce an expression \nof mode e. 6.4 Properties Given a constraint-based source typing derivation and assignment f for some \nterm e, there are translations from e to (1) a stable eS and (2) a changeable eC, with appropriate target \ntypes: Theorem 6.1 (Translation Type Soundness). If C;Gfe e:t and fis a satisfying assignment for C then \n. (1) there exists eS such that [f]Gfe:[f]t1S eS and \u00b7;IGIf fS eS :ItIf and, if eis a value, then eS \nis a value;  (2) there exists eC such that [f]Gfe:[f]t1.eC  C -C and \u00b7;IGIf fC eC :ItIf . The proof \n(in the appendix [Chen et al. 2011]) is by induction on the height of the given derivation of C;Gfe e \n:t. If the conclud\u00ading rule was (SLetE), we use a substitution property (Lemma A.2) for each d&#38;i \nto get a monomorphic constraint typing derivation; that derivation is not larger than the input derivation, \nso we can apply the induction hypothesis to get a translated ei. . The proof constructs the same translation \nderivations as the algorithm in Figure 18 (in fact, we extracted the algorithm from the proof). We also \nprove that running a translated program gives the same result as running the source program. Theorem \n6.5 states that in an initially empty store \u00b7, if evaluating the translated program e . yields v . with \nnew store .,then eevaluates to v where v corresponds to [.]v . (the result of substituting values in \nthe store .. for locations appearing in v ). To de.ne this correspondence, we use a device somewhat sim\u00adilar \nto logical relations: a relation 9 on source and target ex\u00ad . : t pressions, allowing us to show that \nif e : t 9 e then v : t 9 [.]v . : t.Both e and e . must be closed. Our de.ni\u00adtion is weaker than the \nequivalence relations used in logical rela\u00adtions proofs: apply(id,4) 9 4, for example. It does not attempt \nto equate all programs that have the same meaning, but only particu\u00ad function trans (e, e)= case (e, \ne) of |(n, S) . Int |(x, S) . Var |((v1,v2), S) . Pair(trans(v1 , S), trans(v2 , S)) S |(fun f(x)=e. \n:(t1 . t2), S) . Fun(trans(e, e)) e |(inl v, S) . Sum(trans(v, S)) d |(fst (x :(t1 \u00d7 t2), e) . case (d, \ne) of |(S,S) . Fst(trans(x, S)) |(S,C) . if t1 O.S. then Write(trans(e, S)) else ReadWrite(trans(e, S))) \n S |(C,C) . Read(LFst, trans(fst (x. :(t1 \u00d7t2)), C), trans(x, S)) |(.(x1 :intS,x2 :intS), S) . Prim(trans(x1 \n, S), trans(x2, S)) |(.(x1 :intS,x2 :intS), C) . Write(trans(e, S)))) |(.(x1 :intC,x2 :intC), C) . Read(LPrimop1, \n Read(LPrimop2, Write(trans(.(x1,x), S)))) 2 x:t |(let =e1 :t. in e2, e) . LetE(if t. O.S. then trans(e1, \nS) else (if =t. then Mod(trans(e1 , C)) t. else Lift(trans(e1 , C))), trans(e2, e)) |(let x:.aa[D].t. \n=v1 :t. in e2, e) . let variants = all di such that aa=di I D in a a let f= .s. if t. O.S. then trans(v1 \n, S) =t. else (if t. then Mod(trans(v1 , C)) else Lift(trans(v1 , C))) in LetV(map f variants, trans(e2, \ne)) . d(e |(apply(x1 :(t1 t2),x2), e) . case , d, e) of e |(S,S,S) . App(trans(x1 , S), trans(x2, S)) \n|(C,S,C) . App(trans(x1 , S), trans(x2, S)) |(S,S,C) . if t2 O.S. then Write(trans(e, S)) else ReadWrite(trans(e, \nS)) |(e. ,C,C) . Read(LApply, S trans(apply(x. :(t1 . t2),x2), C), e trans(x1, S)) |(C,S,S) . Mod(trans(e, \nC)) |(e. ,C,S) . Mod(trans(e, C)) |(case x :t of {x1.e1 ,x2.e2}, e) . if t O.S. then Case(trans(x, S), \ntrans(e1, e), trans(e2, e)) else Read(LCase, . : t S trans(case xof {x1.e1 ,x2.e2}, C), trans(x,S)) |(x \n:t, C) . if t O.S. then Write(trans(e, S)) else ReadWrite(trans(e, S)) |(fun f(x)=e, C)|(inl v, C) |(n, \nC)|((v1,v2), C) . Write(trans(e, S)) Figure 18. Translation algorithm lar Level ML terms to AFL terms \nthat are similarly structured, but have overhead (mod, write, etc.). Thus, integers are related to inte\u00adgers, \npairs are related if their components are related, and so forth. The de.nition essentially ignores mod \nand write and ignores the mode in applye. Since translated programs can have extra read and let expressions, \nthese are substituted out in the relation, so that 39 let x =3in x. Functions are related if, given related \nar\u00adguments, they produce related results. Note that we will not induct over this relation; neither the \nterm, nor the type, gets smaller. We also relate substitutions: a source substitution sand a target \nsubstitution s, s=v1/x1,...,vn/xn s=w1/x1,...,wn/xn are related at their environments G= x1 : t1,...,xn \n: tn and G. =x1 :t1. ,...,xn :t. , written n (v1/x1,...,vn/xn):(x1 :t1,...,xn :tn) 9 (w1/x1,...,wn/xn):(x1 \n:t. 1,...,xn :t. ) n if, for all kfrom 1to n,we have vk :tk 9 wk :tk. The key lemma (Lemma 6.3) is that \nif e1.e ,the target e program e . is related to e. Combined with Theorem 6.4, which shows that related \nprograms evaluate to related values, this means that the translated program e . evaluates to the same \nvalue that e does. (Actually, e . is a value related to that value; only at intd/int, and products thereof, \nare they identical.) We begin by de.ning a store substitution operation: De.nition 6.2. The store substitution \n[.]eis de.ned as an ordinary substitution, except for e=f, in which case [.]f=[.](mod .(f)). For example, \n[f1 .1,f2 .2](f1,f2)=(mod 1,mod 2). Proofs and several other lemmas can be found in the ap\u00adpendix [Chen \net al. 2011]. Lemma 6.3 (Relation of Translation). If G f e : t1.e . and e \u00b7fs:Gand \u00b7fs:IGI and s:G9 \ns:IGI then [s]e :t 9 [s]e . :t. where t. =ItI if e=S, and t. =ItI-C if e=C. Theorem 6.4 (Generalized \nTranslation Soundness). If e:s 9 [.]e . :s. and D::.fe . .(.. fw) then D. ::e.vwhere v :s 9 [.]w :s. \n. Translation soundness now follows from Lemma 6.3 and Theo\u00ad rem 6.4: Theorem 6.5 (Translation Soundness). \nIf \u00b7f e : t1.e . and e \u00b7fe . .(.. fw),then e.v where v :t 9 [.]w :t. . Finally, we extend Theorem 6.5 \nto further show that the size W(D)of the derivation of the target-language evaluation is within a constant \nfactor of the size W(D)of the derivation of e . v. We need a few de.nitions and intermediate results, \nwhich can be found in the appendix. The proof hinges on classifying the keywords added by the translation, \nsuch as write, as dirty : a dirty keyword will lead to applications of the dirty rule (TEvWrite) in the \nevaluation derivation; such applications have no equivalent in the source-language evaluation. We then \nde.ne the head cost HC of terms and derivations, which counts the number of dirty rules applied near \nthe root of the term, or the root of the derivation, without passing through clean parts of the term \nor derivation. Just counting all the dirty keywords in a term would not rule out a \u00df-reduction duplicating \na particularly dirty part of the term. By de.ning head cost and proving that the translation generates \nterms with bounded head cost including for all subterms we ensure that no part of the term is too dirty; \nconsequently, substituting a subterm during evaluation yields terms that are not too dirty. De.nition \n6.6. Aterm e is shallowly k-bounded if HC(e) = k. Aterm e is deeply k-bounded if every subterm of e (including \ne itself) is shallowly k-bounded. Similarly, a derivation D is shal\u00adlowly k-bounded if HC(D) = k, and \ndeeply k-bounded if all its subderivations are shallowly k-bounded. Theorem 6.7. If trans (e,.) =e . \nthen e . is deeply 6-bounded. e : s 9 e : s Source expression e at type [schema] s is related to target \nexpression e . at type [schema] s. n :intd 9 n :int  ( d 9 ( e1,e2):t 1 1 and e2 :t2 9 e2 :tif e1 :t1 \n9 e \u00d7t  1 :t:t e1,e2):(t1 \u00d7t2)(inl e):(t1 +t2)d 9 (inl e 1 2 if e :t1 9 e ):t+t   if e :(t \u00d7t2)d \n9 e :t. \u00d7t 2 (fst e):t 9 (fst e 1 :t C e :t 9 (mod eC):t. mod if e :t 9 e:t. S e :t 9 (write(eS)):t. \nif e :t 9 e:t. 1 and e2 :t2 9 e2 :t 9 .( if e1 :t1 9 e.(e1,e2):t e1,e    2 9 (read e if e1 :t1 \n9 e S mod and for all v :t1 9 w :t S as x in eC):t [e1/x]e :t2 :t 1,  C we have [v/x]e :t2 9 [w/x]e:t \n e1,e2):t 2 1 :t . :.a&#38;[D].t . 1 and e2 :t1 9 e2 :t 9 applye( d 9 e if e1 :(t1 . e t2) . apply( \ne1,e2):t2 t e  &#38;&#38;d]:I[&#38; 9 e [a&#38;= if e :.a&#38;[D].t 9 ed/&#38;a]tI e :[v :.a&#38;[D].t \nd/&#38;a]t 9 select {d&#38;i .ei}9 (fune f(x)=e if for all i we have v :[&#38;d/&#38;a]t 9 ei :I[&#38;d/&#38;a]tI \n:.a&#38;[D].t i . 1, d e if for all v :t1 9 w :t (fun f(x)=e):(t1 . e t2) ):t t 1 e   ([(fun f(x)=e)/f][v/x]e):t2 \n9 ([(fune f(x)=e e)/f][w/x]e e):t  9 (let x =e1 in e2):t 2 1 :tfor all v :t1 9 w :t if e1 :t1 9 e (let \nx =e1 in e2):t2 and 1 1,  . e 2 :t [v/x]e2 :t2 9 [w/x]  9 (let x =e1 in e2):t 1 :tfor all v :t1 9 \nw :t if e1 :t1 9 e and [e1/x]e2 :t2 .,xe21 2 1 1,  . e 2 :t [v/x]e2 :t2 9 [w/x]  t1 +t2)d 9 e case \ne . of { .xe1 1 case e of {x1 .e1 ,x2 .e2}):t 9 ( if e :(and ( :t+t  for all v :tk 9 w :t k,  k :t. \nfor k .{Figure 19. Correspondence of source and target expressions, used in Theorem 6.5 and other results \n ek :t 9 [w/xk] 1 , 2}[v/xk]e Given D :: . f e . (.. f w) Theorem 6.8 (Cost Result). proach, however, \nis domain-speci.c and only works for certain pro\u00ad ** 1 * * * grams (e.g., functions cannot return arbitrary \nvalues): it is unsound where for every subderivation D f e . (.2 f w :: . ) * ) = k, then the number \nof dirty rule in general. of D(including D), HC(D applications in Dis at most k W (D). k+1 Information \n.ow and constraint-based type inference. A num-The cost theorem follows from Theorem C.5 (in the appendix) \nber of information .ow type systems have been developed to check a generalization of Theorem 6.4 and \nTheorem 6.8: security properties, including the SLam calculus [Heintze and Riecke 1998], JFlow [Myers \n1999] and a monadic system [Crary Theorem 6.9. If Dderives \u00b7ftrans(e, e) .(.. fw)then D et al. 2005]. \nOur type system uses many ideas from Pottier and derives e .v where v :t 9 [.]w :t. and W (D)=7W(D ). \n Simonet [2003], including a form of constraint-based type infer- Acar et al. [2006] proved that given \na well-typed AFL program, change propagation updates the output consistently with an initial run. Using \nTheorems 6.1 and 6.5, this implies that change propaga\u00ad tion is consistent with an initial run of the \nsource program.  7. Related Work Incremental computation. Self-adjusting computation provides an approach \nto incremental computation, which has been studied extensively [Ramalingam and Reps 1993; Demers et al. \n1981; Pugh and Teitelbaum 1989; Abadi et al. 1996]. Key techniques behind self-adjusting computation \ninclude dynamic dependence graphs, which allows a fully general change propagation mechanism [Acar et \nal. 2006], and a form of memoization that allows inexact compu\u00ad tations to be reused via memoized computations \nthat are (recur\u00adsively) self-adjusting [Acar et al. 2009]. Programming-language features allow writing \nself-adjusting programs but these require syntactically separating stable and changeable data, as well \nas code that operates on such data [Acar et al. 2006, 2009; Ley-Wild et al. 2008; Hammer et al. 2009]. \nDITTO [Shankar and Bodik 2007] shows the bene.ts of eliminating user annotations. By customiz\u00ading dependency \ntracking for invariant checking programs, DITTO provides a fully automatic incremental invariant checker. \nThe ap\u00adence [Odersky et al. 1999], and is also broadly similar to other systems that use subtyping constraints \n[Simonet 2003; Foster et al. 2006]. Cost semantics. To prove that our translation yields ef.cient self\u00adadjusting \ntarget programs, we use a simple cost semantics. The idea of instrumenting evaluations with cost information \ngoes back to the early 90s [Sands 1990]. Cost semantics is particularly important in lazy [Sands 1990; \nSansom and Peyton Jones 1995]) and parallel languages [Spoonhower et al. 2008] where it is especially \ndif.cult to relate execution time to the source code, as well as in self\u00adadjusting computation [Ley-Wild \net al. 2009].  8. Conclusion This paper presents techniques for translating purely functional programs \nto programs that can automatically self-adjust in re\u00adsponse to dynamic changes to their data. Our contributions \nin\u00adclude a constraint-based type system for inferring self-adjusting\u00adcomputation types from purely functional \nprograms, a type-directed translation algorithm that rewrites purely functional programs into self-adjusting \nprograms, and proofs of critical properties of the translation: type soundness and observational equivalence, \nas well as the intrinsic property of time complexity. Perhaps unsurprisingly, the theorems and their \nproofs were critical to the determination of the type systems and the translation algorithm: many of \nour initial attempts at the problem resulted in target programs that were not type sound, that did not \nensure observational equivalence, or were asymptotically slower than the source. These results take \nan important step towards the development of languages and compilers that can generate code that can \nrespond automatically to dynamically changing data correctly and asymp\u00adtotically optimally, without substantial \nprogramming effort. Re\u00admaining open problems include generalization to imperative pro\u00adgrams with references, \ntechniques and proofs to determine or im\u00adprove the asymptotic complexity of dynamic responses, and a \ncom\u00adplete and careful implementation and its evaluation. Acknowledgments We thank the anonymous ICFP \nreviewers, as well as Arthur Chargu\u00b4eraud, for their useful comments on the submitted version of this \npaper.  References M. Abadi, B. W. Lampson, and J.-J. L\u00b4evy. Analysis and caching of dependencies. \nIn International Conference on Functional Programming, pages 83 91, 1996. U. A. Acar, G. E. Blelloch, \nand R. Harper. Adaptive functional programming. ACM Trans. Prog. Lang. Sys., 28(6):990 1034, 2006. U. \nA. Acar, A. Ihler, R. Mettu, and O. S\u00a8umer. Adaptive Bayesian inference. In Neural Information Processing \nSystems (NIPS), 2007. U. A. Acar, G.E.Blelloch,M. Blume,R.Harper, andK.Tang\u00adwongsan. An experimental \nanalysis of self-adjusting computa\u00adtion. ACM Trans. Prog. Lang. Sys., 32(1):3:1 3:53, 2009. U. A. Acar, \nG. E. Blelloch, R. Ley-Wild, K. Tangwongsan, and D. T\u00a8urko.glu. Traceable data types for self-adjusting \ncomputa\u00adtion. In Programming Language Design and Implementation, 2010a. U. A. Acar, A. Cotter, B. Hudson, \nand D. T\u00a8urko.glu. Dynamic well\u00adspaced point sets. In Symposium on Computational Geometry, 2010b. M. \nCarlsson. Monads for incremental computing. In International Conference on Functional Programming, pages \n26 35, 2002. Y. Chen, J. Dun.eld, M. A. Hammer, and U. A. Acar. On\u00adline appendix to Implicit Self-Adjusting \nComputation for Purely Functional Programs, 2011. http://www.mpi-sws.org/ ~joshua/Chen11appendix.pdf. \n Y.-J. Chiang and R. Tamassia. Dynamic algorithms in computa\u00ad tional geometry. Proceedings of the IEEE, \n80(9):1412 1434, 1992. K. Crary, A. Kliger, and F. Pfenning. A monadic analysis of infor\u00admation .ow security \nwith mutable state. Journal of Functional Programming, 15(2):249 291, Mar. 2005. L. Damas and R. Milner. \nPrincipal type-schemes for functional programs. In Principles of Programming Languages, pages 207 212. \nACM, 1982. A. Demers, T. Reps, and T. Teitelbaum. Incremental evaluation of attribute grammars with application \nto syntax-directed editors. In Principles of Programming Languages, pages 105 116, 1981. C. Demetrescu, \nI. Finocchi, and G. Italiano. Handbook on Data Structures and Applications, chapter 36: Dynamic Graphs. \nCRC Press, 2005. J. Field and T. Teitelbaum. Incremental reduction in the lambda calculus. In ACM Conf. \nLISP and Functional Programming, pages 307 322, 1990. J. S. Foster, R. Johnson, J. Kodumal, and A. Aiken. \nFlow\u00adinsensitive type quali.ers. ACM Trans. Prog. Lang. Sys., 28: 1035 1087, 2006. L. Guibas. Modeling \nmotion. In J. Goodman and J. O Rourke, editors, Handbook of Discrete and Computational Geometry, pages \n1117 1134. Chapman and Hall/CRC, 2nd edition, 2004. M. A. Hammer, U. A. Acar, and Y. Chen. CEAL: a C-based \nlanguage for self-adjusting computation. In Proceedings of the 2009 ACM SIGPLAN Conference on Programming \nLanguage Design and Implementation, June 2009. N. Heintze and J. G. Riecke. The SLam calculus: programming \nwith secrecy and integrity. In Principles of Programming Lan\u00adguages (POPL 98), pages 365 377, 1998. R. \nLey-Wild, M. Fluet, and U. A. Acar. Compiling self-adjusting programs with continuations. In Proceedings \nof the Interna\u00adtional Conference on Functional Programming, 2008. R. Ley-Wild, U. A. Acar, and M. Fluet. \nA cost semantics for self\u00adadjusting computation. In Proceedings of the 26th Annual ACM Symposium on Principles \nof Programming Languages, 2009. MLton. MLton web site. http://www.mlton.org. A. C. Myers. JFlow: practical \nmostly-static information .ow con\u00adtrol. In Principles of Programming Languages, pages 228 241, 1999. \nM. Odersky, M. Sulzmann, and M. Wehr. Type inference with constrained types. Theory and Practice of Object \nSystems,5 (1):35 55, 1999. F. Pottier and V. Simonet. Information .ow inference for ML. ACM Trans. Prog. \nLang. Sys., 25(1):117 158, Jan. 2003. W. Pugh and T. Teitelbaum. Incremental computation via function \ncaching. In Principles of Programming Languages, pages 315 328, 1989. G. Ramalingam and T. Reps. A categorized \nbibliography on incre\u00admental computation. In Principles of Programming Languages, pages 502 510, 1993. \nA. Sabelfeld and A. C. Myers. Language-based information-.ow security. IEEE J. Selected Areas in Communications, \n21(1), 2003. D. Sands. Calculi for Time Analysis of Functional Programs.PhD thesis, University of London, \nImperial College, September 1990. P. M. Sansom and S. L. Peyton Jones. Time and space pro.ling for non-strict, \nhigher-order functional languages. In Principles of Programming Languages, pages 355 366, 1995. A. Shankar \nand R. Bodik. DITTO: Automatic incrementalization of data structure invariant checks (in Java). In Programming \nLanguage Design and Implementation, 2007. V. Simonet. Type inference with structural subtyping: A faithful \nformalization of an ef.cient constraint solver. In APLAS, pages 283 302, 2003. D. Spoonhower, G. E. Blelloch, \nR. Harper, and P. B. Gibbons. Space pro.ling for parallel functional programs. In Interna\u00adtional Conference \non Functional Programming, 2008.  \n\t\t\t", "proc_id": "2034773", "abstract": "<p>Computational problems that involve dynamic data, such as physics simulations and program development environments, have been an important subject of study in programming languages. Building on this work, recent advances in self-adjusting computation have developed techniques that enable programs to respond automatically and efficiently to dynamic changes in their inputs. Self-adjusting programs have been shown to be efficient for a reasonably broad range of problems but the approach still requires an explicit programming style, where the programmer must use specific monadic types and primitives to identify, create and operate on data that can change over time.</p> <p>We describe techniques for automatically translating purely functional programs into self-adjusting programs. In this implicit approach, the programmer need only annotate the (top-level) input types of the programs to be translated. Type inference finds all other types, and a type-directed translation rewrites the source program into an explicitly self-adjusting target program. The type system is related to information-flow type systems and enjoys decidable type inference via constraint solving. We prove that the translation outputs well-typed self-adjusting programs and preserves the source program's input-output behavior, guaranteeing that translated programs respond correctly to all changes to their data. Using a cost semantics, we also prove that the translation preserves the asymptotic complexity of the source program.</p>", "authors": [{"name": "Yan Chen", "author_profile_id": "81361601201", "affiliation": "Max Planck Institute for Software Systems, Kaiserslautern and Saarbr&#252;cken, Germany", "person_id": "P2801385", "email_address": "chenyan@mpi-sws.org", "orcid_id": ""}, {"name": "Joshua Dunfield", "author_profile_id": "81100605091", "affiliation": "Max Planck Institute for Software Systems, Kaiserslautern and Saarbr&#252;cken, Germany", "person_id": "P2801386", "email_address": "joshua@mpi-sws.org", "orcid_id": ""}, {"name": "Matthew A. Hammer", "author_profile_id": "81330491901", "affiliation": "Max Planck Institute for Software Systems, Kaiserslautern and Saarbr&#252;cken, Germany", "person_id": "P2801387", "email_address": "hammer@mpi-sws.org", "orcid_id": ""}, {"name": "Umut A. Acar", "author_profile_id": "81100077236", "affiliation": "Max Planck Institute for Software Systems, Kaiserslautern and Saarbr&#252;cken, Germany", "person_id": "P2801388", "email_address": "umut@mpi-sws.org", "orcid_id": ""}], "doi_number": "10.1145/2034773.2034792", "year": "2011", "article_id": "2034792", "conference": "ICFP", "title": "Implicit self-adjusting computation for purely functional programs", "url": "http://dl.acm.org/citation.cfm?id=2034792"}