{"article_publication_date": "09-19-2011", "fulltext": "\n Geometry of Synthesis IV Compiling Af.ne Recursion into Static Hardware Dan R. Ghica Alex Smith Satnam \nSingh University of Birmingham University of Birmingham Microsoft Research d.r.ghica@cs.bham.ac.uk ais523@cs.bham.ac.uk \nsatnams@microsoft.com Abstract Abramsky s Geometry of Interaction interpretation (GoI) is a logical-directed \nway to reconcile the process and functional views of computation, and can lead to a data.ow-style semantics \nof pro\u00adgramming languages that is both operational (i.e. effective) and denotational (i.e. inductive \non the language syntax). The key idea of Ghica s Geometry of Synthesis (GoS) approach is that for cer\u00adtain \nprogramming languages (namely Reynolds s af.ne Syntactic Control of Interference SCI) the GoI processes-like \ninterpretation of the language can be given a .nitary representation, for both internal state and tokens. \nA physical realisation of this representa\u00adtion becomes a semantics-directed compiler for SCI into hardware. \nIn this paper we examine the issue of compiling af.ne recursive programs into hardware using the GoS \nmethod. We give syntax and compilation techniques for unfolding recursive computation in space or in \ntime and we illustrate it with simple benchmark-style examples. We examine the performance of the benchmarks \nagainst conventional CPU-based execution models. Categories and Subject Descriptors B.5.2 [Hardware]: \nDesign Aids Automatic Synthesis General Terms Languages, Theory, Design 1. Introduction Why compile recursive \nfunctions into FPGA hardware? A few years ago the case for compiling recursive functions into FPGA circuits \nwas less compelling because these chips had limited com\u00adputational resources and no specialized on-chip \nmemory, so a stack would have to be made using precious .ip-.ops. Modern FPGAs can implement million-gate \ncircuits and can contain thousands of independent dual-port memory blocks (e.g. block-RAMs on Xilinx \nFPGAs which are around 36 Kbits in size) which are ideal for im\u00adplementing stacks and other local computational \nstate. Since there are many independent memories it is possible to have many cir\u00adcuit sub-components \nexecuting in parallel without one single stack acting as a memory bottle-neck. So now that we have effective \nelements for implementing recur\u00adsive computations on FPGAs are there any applications that actu\u00adally \nneed recursive descriptions? Certainly. As FPGAs have grown in capacity there has been an increasing \ndesire to map computa- Permission to make digital or hard copies of all or part of this work for personal \nor classroom use is granted without fee provided that copies are not made or distributed for pro.t or \ncommercial advantage and that copies bear this notice and the full citation on the .rst page. To copy \notherwise, to republish, to post on servers or to redistribute tions that were previously implemented \nin software for execution on a regular processor onto FPGAs instead, in order to improve performance \nor reduce energy consumption. Recently FPGAs have gained the capability to access large amounts of external \nmemory using multiple memory controllers and this allows us to manipulate dynamic data-structures like \ntrees with FPGA gates and exploit the thousands of on-chip independent memories as caches. With dy\u00adnamic \ndata-structures one naturally has the desire to express com\u00adputations with recursive algorithms (e.g. \nbalancing trees and insert\u00ading and removing nodes). Example applications include line-speed network packet \nprocessing which may require building up auxiliary dynamic data structures (e.g. trees) which are used \nto help classify and route packets, such as Netezza s data warehousing project1 or IBM s DataPower accelerated \nXML parser2. Recursive procedures, functions and methods have been used to describe the behaviour of \ndigital circuits in a variety of pro\u00adgramming languages ranging from mainstream hardware descrip\u00adtion \nlanguages like VHDL to experimental embedded domain spe\u00adci.c languages like Lava [BCSS98] for many years. \nMost systems typically implement recursion by inlining it at compile time to pro\u00adduce descriptions that \nare unfolded in space. These descriptions as\u00adsume the depth of the recursion to be determinable at compile \ntime. A speci.c example of such a recursive function is one that adds the elements of an array by using \na recursive divide and conquer strategy yielding an adder-tree implementation. At compile time the size \nof the array has to be known but not the value of the array elements. Our goal is to allow recursive \ndescriptions of circuits which un\u00adfold in time and space where the depth of recursion is not known at \ncompile time. This in turn allows us to describe recursive functions that can operate over dynamic data \nstructures. We aim to research and develop techniques for translating a sub-set of recursive func\u00adtions \ninto circuits where the depth of recursion is dependent on the dynamic values. Domain speci.c hardware \ndescription langauges sometimes of\u00adfer limited support for recursion. For example, Contessa [TL07] is \nused for .nance applications but only supports tail recursion. There has also been considerable interest \nin describing hardware using embedded domain speci.c langauges i.e. by designing a li\u00adbrary that captures \nimportant aspects of circuit design. For exam\u00adple, the JHDL system [BH98] provided a system coded in \nJava for performing structural hardware design. The Lava system provides a combinator library that makes \nit easier to descibe circuit layout and behaviour [HD08]. All of these systems allow recursive descrip\u00adtions \nbut they all require the descriptions to be in-lined. Even Blue\u00adspec [Nik04] which is a modern hardware \ndescription language based on functional programming concepts allows the speci.cation to lists, requires \nprior speci.c permission and/or a fee. 1 http://www.netezza.com/data-warehouse-appliance-products/ ICFP \n11 September 19 21, 2011, Tokyo, Japan. Copyright &#38;#169; 2011 ACM 978-1-4503-0865-6/11/09. . . $10.00 \n2 http://www-01.ibm.com/software/integration/datapower/ c of recursive functions but does not support \nthe synthesis of dynamic recursion. Sklyarov describes a mechanism for compiling recursive C++ descriptions \ninto FPGA circuits [Skl04] using hierarchical .nite state machines which have been applied to the design \nof Huffman encoder circuits and sorting networks. Sklyarov s technique has the advantage of dealing with \ngeneral recursion but it has the drawback of requring centralized control and it produces rather slow \ncircuits running at only 25 MHz. Our approach is more modular, does not require centralized control and \nproduces faster circuits. The knapsack and Kinght s Tour problems were described as recursive hardware \nalgorithms by Maruyama, Takagi and Hoshino [MTH99] although this technique is limited to loop unrolling \nand pipeling and the use of a stack to hold state information. Ferizis and Gindy show how recursive functions \ncan be imple\u00admented by using dynamic recon.guration [FG06] to unroll recur\u00adsive function calls at run-time \nwhich avoids the need for a stack. However, this approach is very limited because it replicates the body \nof the recursive function, which is likely to quickly exhuast the available FPGA resources, and it also \nrelies on immature dy\u00adnamic recon.guration technology. Contribution. This paper gives the most general \nmethod to date for compiling recursive programs into static hardware. The main language-level restriction \nwe impose, the use of an af.ne type system, is unavoidable because of computational reasons non\u00adaf.ne \nfunctions do not have a .nite state model. We describe a compiler to sequential circuits and we illustrate \nit with a typical benchmark for recursive programs, computing Fibonacci numbers. 2. Geometry of Synthesis \n 2.1 Objectives, methodology, motivation Higher-level synthesis (HLS) is the production of gate-level \nde\u00adscription of circuits from behavioural descriptions given in higher\u00adlevel programming languages. Current \nHLS tools and techniques have signi.cant limitations. One speci.c limitation is the weak treatment of \nfunction calls, which are typically implemented by inlining. This restriction is limiting the more general \napplication of high level synthesis tech\u00adnology to a wider class of algorithms than bene.t from dynamic \nprocedure calls and recursion. By inlining function calls at source-code level, which in hard\u00adware corresponds \nto replication of circuitry, current tools generate code which is often inef.cient. Also, ordinary programming \nlan\u00adguages interface via function calls with libraries and with run-time services. Lack of support for \nproper function calls makes such in\u00adteroperability impossible, which means that the entire system must \nbe developed in one single programming language. Separate com\u00adpilation, foreign function interfaces and \napplication binary inter\u00adfaces, standard facilities in modern compilers, cannot be supported without \na proper function mechanism. The Geometry of Synthesis (GoS) approach [Ghi07, GS10, GS11] enables full \nand proper implementation of ordinary pro\u00adgramming languages, with an emphasis on correct and effective \nhandling of function calls. Through static analysis the compiler can decide whether the circuit for a \nparticular function must be re\u00adinstantiated or can be reused, depending on the desired trade-offs between \narea/power and latency/throughput. More importantly, code can be compiled without requiring all de.nitions \nof all func\u00adtions to be available at compile time. Appropriate hardware inter\u00adfaces are generated for \nthe missing functions so that a complete functional circuit can be synthesised later, in a linking stage. \nThis means that pre-compiled libraries can be created, distributed and commercialised with better protection \nfor intellectual property. Fi\u00adnally, code written in one language can interact with code written in other \nlanguages so long as certain interface protocols are respected by the components. Moreover, these protocols \ncan be enforced via the type system at the level of the programming language. Designs developed directly \nin conventional hardware description languages, subject to respecting the interface protocols, can also \nbe used from the programming language via function calls. This allows the reuse of a vast existing portfolio \nof specialised, high-performance de\u00adsigns. Run-time services: certain circuits, which manage physical \nresources such as memory, network interface, video or audio inter\u00adface, etc., cannot be meaningfully \nreplicated as the resource itself cannot be replicated. Such services can also be used from ordinary \nprogramming languages via function calls. Note the technology fully supports so-called higher-order \nfunc\u00adtions, i.e. functions which operate on functions as argument or re\u00adsult. Such functions play an \nimportant role in the design of highly ef.cient parallel algorithms, such as Map-Reduce. These method\u00adological \nconsiderations are discussed at length elsewhere [Ghi11]. 2.2 Theoretical background Abramsky s Geometry \nof Interaction (GoI) interpretation is a logical-directed way to reconcile the process and and functional \nviews of computation, and can lead to a data.ow-style semantics of programming languages that is both \noperational (i.e. effective) and denotational (i.e. inductive on the language syntax) [AJ92]. These ideas \nhave already been exploited in devising optimal compilation strategies for the lambda calculus, a technique \ncalled Geometry of Implementation [Mac94, Mac95]. The key idea of the Geometry of Synthesis (GoS) approach \nis that for certain programming languages, of which Reynolds s af.ne version of Idealized Algol (called \nSyntactic Control of Interfer\u00adence [Rey78, Rey89]) is a typical representative, the process-like interpretation \ngiven by the GoI can be represented in a .nite way. This .nitary representation applies both to the internal \nstate of the token machine and the tokens themselves. Subsequent developments in game semantics (see \n[Ghi09] for a survey) made it possible to give interpretations of a large variety of computational features \n(state, control, names, etc.) in an interactive way which is compatible with the GoI framework. Such \ncomputa\u00adtional features often have themselves a .nite-state representation, as noted in [GM00]. A physical \nrealisation of the .nite-state representation of a GoI\u00adstyle semantics then becomes a semantics-directed \ncompiler for suitable languages, such as SCI, directly into hardware [Ghi07]. The hardware itself can \nbe synchronous or asynchronous [GS10]. Subsequent work also showed how programs of more expressive type \nsystems, such as full Idealized Algol, can be systematically represented into SCI as an intermediary \nlanguage via type inference and serialization [GS11]. As a .nal observation, the somewhat surprising \nway in which higher order abstraction and application are compiled into circuits is best understood via \nthe connection between GoI and monoidal categories, especially compact-closed categories [KL80], since \nsuch categories are an ideal formal setting for diagrammatic repre\u00adsentation [Sel09]. 3. Syntactic Control \nof Interference Reynolds s Idealized Algol (IA) is a compact language which com\u00adbines the fundamental \nfeatures of imperative languages with a full higher-order procedural mechanism [Rey81]. This combination \nmakes the language very expressive, for example allowing the en\u00adcoding of classes and objects. Because \nof its expressiveness and elegance, IA has attracted a great deal of attention from theoreti\u00adcians [OT81]. \nThe typing system (Basic) Syntactic Control of Interference (SCI) is an af.ne version of IA in which \ncontraction is disal\u00adlowed over function application and parallel execution. SCI was initially proposed \nby Reynolds as a programming language which would facilitate Hoare-style correctness reasoning because \ncovert interference between terms is disallowed [Rey78, Rey89]. SCI turned out to be semantically interesting \nand it was studied exten\u00adsively [Red96, OPTT99, McC07, McC10]. The restriction on con\u00adtraction in SCI \nmakes it particularly well suited for hardware com\u00adpilation because any term in the language has a .nite-state \nmodel and can therefore be compiled as a static circuit [Ghi07, GS10]. The primitive types of the language \nare commands, memory cells, and bounded-integer expressions s ::= com |var |exp. The type constructors \nare product and function: . ::= . \u00d7 . | . . . | s. Terms are described by typing judgements of the form \nx1 : .1,...,xk : .k f M : ., where we denote the list of identi.er type assignments on the left by G. \nBy convention, if we write G, G' it assumes that the two type assignments have disjoint sets of identi.ers. \nThe term formation rules of the language are those of the af.ne lambda calculus: Identity x : . fx : \n. G fM : .' Weakening x : ., G fM : .' , G' fM : .''G,x : ., x' : .' Commutativity ' G,x: .' ,x : ., \nG' fM : .'' G,x : .' fM : . Abstraction G f.x.M : .' .. G fM : . ..' . f N : . Application G, . fMN : \n.' G fMi : .i Product G f(M1,M2): .1 \u00d7.2 Importantly, contraction (identi.er reuse) is allowed in product \nformation but not in function application. The constants of the language are described below: n : exp \nare the integer constants; skip : com is the only command constant ( no-op ); asg : var \u00d7exp . com is \nassignment to memory cell, denoted by := when used in in.x notation; der : var .exp is dereferencing \nof memory cell, also denoted by ! ; seq : com \u00d7s . s is command sequencing, denoted by ; when used in \nin.x notation if s = com then the resulting expression is said to have side-effects; op : exp \u00d7exp . \nexp stands for arithmetic and logic operators; if : exp \u00d7com \u00d7com . com is branching; while : exp \u00d7com \n.com is iteration; newvar :(var . s) .s is local variable declaration in block command or block expression. \nLocal variable binding is presented with a quanti.er-style type in order to avoid introducing new variable \nbinders in the language. Local variable declaration can be sugared into a more familiar syntax as newvar(.x.M) \n=newvar x in M. We can reuse identi.ers in products, so conventional imper\u00adative program terms such as \nx:=!x+1 or c;c can be written as asg(x, add(der(x), 1)) and seq(c, c) respectively. One immediate consequence \nof the af.ne-type restriction is that nested applica\u00adtion is no longer possible, i.e. terms such as f \n: com . com f f(f(skip)) are illegal, so the usual operational unfolding of recur\u00adsion no longer preserves \ntyping. Therefore, the rec. operator in its most general formulation must be also eliminated. An appropriately \nadapted recursion operator will be presented in the next section. Despite its restrictions SCI is still \nexpressive enough to allow many interesting programs. Its .nite state model makes it perfectly suited \nfor hardware compilation [Ghi07, GS10]. The operational semantics of SCI is essentially the same as that \nof IA, which is standard. In this paper we are interested in compiling SCI to se\u00adquential hardware; the \nGoS method relies essentially on the exis\u00adtence of a game-semantic model for the programming language. \nThe existing game [Wal04] and game-like [McC10] models of SCI are asynchronous, and using them to create \nsynchronous hardware raises some technical challenges, which are addressed in previous work on round \nabstraction [GM10, GM11]. The compiler is de.ned inductively on the syntax of the lan\u00adguage. Each type \ncorresponds to a circuit interface, de.ned as a list of ports, each de.ned by data bit-width and a polarity. \nEvery port has a default one-bit control component. For example we write an interface with n-bit input \nand n-bit output as I = (+n, -n). More complex interfaces can be de.ned from simpler ones using concate\u00adnation \nI1@I2 and polarity reversal I- = map (.x.-x)I. If a port has only control and no data we write it as \n+0 or -0, depending of polarity. Note that obviously +0 = -0 in this notation! An interface for type \n. is written as [.], de.ned as follows: [com] = (+0, -0) [exp] = (+0, -n) [var] = (+n, -0, +0, -n) [. \n\u00d7.' [. ..'- @[.' ] = [.]@[.' ] = [.]]. ] The interface for com has two control ports, an input for starting \nexecution and an output for reporting termination. The interface for exp has an input control for starting \nevaluation and data output for reporting the value. Variables var have data input for a write request \nand control output for acknowledgment, and control input for a read request along with data output for \nthe value. Tensor and arrow types are given interpretations which should be quite intuitive to the reader \nfamiliar with compact-closed cat\u00adegories [KL80]. The tensor is a disjoint sum of the ports on the two \ninterfaces while the arrow is the tensor along with a polarity\u00adreversal of the ports in the contravariant \nposition. Reversal of po\u00adlarities gives the dual object in the compact closed category. Diagrammatically, \na list will correspond to ports read from left\u00adto-right and from top-to-bottom. We indicate ports of \nzero width (only the control bit) by a thin line and ports of width n by an additional thicker line (the \ndata part). For example a circuit of interface [com . com] =(-0, +0, +0, -0) can be written in any of \nthese two ways:  The unit-width ports are used to transmit events, represented as the value of the port \nbeing held high for one clock cycle. The n\u00adwidth ports correspond to data lines. The data on the line \nis only considered to be meaningful while there is an event on the control line. Below, for each language \nconstant we will give the asyn\u00adchronous game-semantic interpretation and its low-latency syn\u00adchronous \nrepresentation [GM11] and the (obvious) circuit imple\u00admentation. For the purpose of giving the game-semantics \nand its representation, the circuit interface will correspond to the set of moves; the semantics is a \nset of traces over the possible moves. We denote a move/event on port k of interface I =(p1,...,pm) by \nnk, where n is the data value; if the data-width is 0 we write * k. We use (m, m) to indicate the simultaneity \nof move/event m and m '. We use m \u00b7 m ' to indicate concatenation, i.e. move/event m ' happens after \nm, but not necessarily in the very next clock cycle. We de.ne m m ' = {(m, m ' ),m \u00b7 m ' }. We de.ne \nthe game se\u00admantic interpretation by [-]g and the synchronous representation by [-]s. The notation pc- \napplied to a set means closure under pre.x-taking. Skip skip : com, [com] = (+0, -0) skip g = pc{*1 \n\u00b7*2} [ skip] s = pc{(*1, *2)}. The circuit representation is: Intuitively the input port of a command \nis a request to run the command and the output port is the acknowledgment of successful completion. In \nthe case of skip the acknowledgment is immediate. Integer constant k : exp, [exp] = (+1, -n) = pc{*1 \n\u00b7 k2} [k]g = pc{(*1,k2)}. [k]s The circuit representation is: Intuitively the input port of an expression \nis a request to evaluate the expression and the output port is the data result and a control signal indicating \nsuccessful evaluation. In the case of a constant n the acknowledgment is immediate and the data is connected \nto a .xed bit pattern. Sequential composition seq : com \u00d7 exp . exp com \u00d7 exp . exp] =(-0, +0, -0, +n, \n+0, -n)[ seq]g = pc{*5 \u00b7* 1 \u00b7* 2 \u00b7* 3 \u00b7 k4 \u00b7 k6 | k . Z} [seq]s = pc{(*5, * 1) *2 \u00b7* 3 (k4 \u00b7 k6)| k . \nZ}. The circuit representation is: Above, D denotes a one-clock delay (D-.ip-.op). A sequencer SEQ propagates \nthe request to evaluate a command in sequence with an expression by .rst sending an execute request to \nthe command, then to the expression upon receiving the acknowl\u00adedgment from the command. The result of \nthe expression is propagated to the calling context. Note the unit delay placed between the command acknowledgment \nand the expression re\u00adquest. Its presence is a necessary artifact of correctly represent\u00ading asynchronous \nprocesses synchronously and cannot be opti\u00admised away [GM11]. Assignment and dereferencing asg : var \n\u00d7 exp . com var \u00d7 exp . com] =(-n, +0, -0, +n, -0, +n, +0, -n) asg g = pc{*7 \u00b7*5 \u00b7 k6 \u00b7 k1 \u00b7*2 \u00b7*8 | \nk . Z} [ asg] s = pc{(*7, * 5) k6 \u00b7 k1 (*2, * 8)| k . Z}. der : var . exp, [var . exp =(-n, +0, -0, \n+n, +0, -n)]der g = pc{*5 \u00b7*3 \u00b7 k4 \u00b7 k6 | k . Z}[ der] s = pc{(*5, *3) (k4,k6)| k . Z} The circuit representations \nare, respectively: The variable type has four ports: writing data (n bits), acknowl\u00adedging a write (0 \nbits), requesting a read (0 bits) and providing data (n bits). Assignment is a sequencing of an evaluation \nof the integer argument with a write request to the variable; the unused variable ports are grounded. \nDereferencing is simply a projection of a variable interface onto an expression interface by propagating \nthe read-part of the interface and blocking the write part. Operators op : exp \u00d7 exp . exp exp \u00d7 exp \n. exp] =(-0, +n, -0, +n, +0, -n) \u00b7 k '' op g = pc{*5 \u00b7* 1 \u00b7 k2 \u00b7* 3 \u00b7 k ' | op(k, k ' )= k '' . Z} 46 \n[ op] s = pc{(*5, *1) k2 \u00b7*3 (k4 ' \u00b7 k6)| k '' . Z}. The circuit representation is: R above is a register. \nThe input control of port 2 is connected to the load pin of the register. The (combinatorial) circuit \nOP implements the operation. Note that the value of the .rst opera\u00adtor is saved in the register because \nexpressions can change their value in time due to side-effects. Branching if : exp \u00d7 exp \u00d7 exp . exp \nexp \u00d7 exp \u00d7 exp . exp] =(-0, +n, -0, +n, -0, +n, +0, -n)[ if]g = pc{*7 \u00b7*1 \u00b7 02 \u00b7*5 \u00b7 k6 \u00b7 k8, *7 \u00b7*1 \n\u00b7 k2 ' \u00b7*3 \u00b7 k4 \u00b7 k8 | k ' =0,k . Z} [if]s = pc{(*7, *1) 02 \u00b7*5 (k6,k8), (*7, *1) k2 ' \u00b7*3 (k4,k8) | \nk ' =0,k . Z} The corresponding circuit is:  Above, Mux is a (combinatorial) n-bit multiplexer which \nselects one data path or the other depending on the control signal. X is a merge of two control signals \n(or or exclusive-or) and T is ade\u00admultiplexer which propagates the input control signal to the .rst or \nsecond output, depending on whether the data value is zero or nonzero. As before, the delay D is necessary \nfor correctness considerations. Iteration while : exp \u00d7 com . com] exp \u00d7 com . com] =(-0, +n, -0, +0, \n+0, -0) [ while]g = pc{*5 \u00b7 (*1 \u00b7 02 \u00b7*3 \u00b7*4) * \u00b7*1 \u00b7 k2 \u00b7*6 | k . Z \\{0}} [while]s = pc{(*5, *1) 02 \n\u00b7*3 *4 \u00b7 (*1 02 \u00b7*3 *4) * \u00b7*1 k2 \u00b7*6, (*5, *1) k2 \u00b7*6 | k . Z \\{0}} The circuit is: The iterator will \nkeep executing the second argument as long as the .rst argument is zero. State The local variable binder \nis a higher-order constant. newvar :(var . com) . com (var . com) . com] = (+n, -0, +0, -n, -0, +0, +0, \n-0) [ newvar]g = pc{*7 \u00b7* 5 \u00b7 v \u00b7* 6 \u00b7* 8 * | v .k1 \u00b7*2 \u00b7 (*3 \u00b7 k4) *} k.Z [newvar]s = pc{(*7, *5) v \n (*6, *8) * | v .(k1, *2) (*3,k4) *} k.Z The circuit with this behaviour is basically just a register: \n In addition to the constants of the language we also interpret struc\u00adtural rules (abstraction, application, \nproduct formation) as construc\u00adtions on circuits. In diagrams we represent bunches of wires (buses) as \nthick lines. When we connect two interfaces by a bus we assume that the two interfaces match in number \nand kind of port perfectly. In general a term of signature x1 : .1,...,xk : .k f M : . will be interpreted \nas a circuit of interface [.1 \u00d7\u00b7 \u00b7\u00b7\u00d7 .k . .]. Abstraction Semantically, in both the original game semantics \nand the synchronous representation the abstraction G f .x : ..M : . ' is interpreted by the currying \nisomorphism. Similarly, in circuits the two interfaces for this circuit and G,x : . f M : . ' are isomorphic. \nApplication To apply a function of type G f F : . . . ' to an argument . f M : . we simply connect the \nports in [.] from the two circuits:  Product formation Unlike application, in product formation we allow \nthe sharing of identi.ers. This is realised through special circuitry implementing the diagonal function \n.x.(x, x) for any type .. Diagrammatically, the product of terms G f Mi : .i is: The diagonal circuit \nis behaviourally similar to a stateful multiplexer-demultiplexer. It routes an input signal from the \ninterfaces on the right to the shared interface on the left while storing the source of the signal in \na set-reset register. From the semantic model we know that any output signal in the shared interface \nis followed by an input signal in the same interface, which is routed to the originating component using \nthe demul\u00adtiplexer T . SR registers are needed for all initial questions and T blocks use the registers \nfor the matching question. In the simplest case, for dcom the circuit looks like this: Note that this \ndiagonal introduces a unit delay, which is not strictly necessary for correctness. A lower-latency diagonal \nthat correctly handles instant feedback without the delay D can be implemented, but is more complex. \nStructural rules Finally, we give constructions for commutativity, weakening and identity. They are represented \nby the circuits below: Commutativity is rearranging ports in the interface, weakening is the addition \nof dummy ports and identities are typed buses. Example. The GoS approach allows the compilation of higher\u00adorder, \nopen terms. Consider for example a program that exe\u00adcutes in-place map on a data structure equipped with \nan iterator: .f : exp . exp.init; while(more)(curr := f(!curr); next): com, where init : com, curr : \nvar, next : com, more : exp. The interface of the iterator consists of an initialiser, access to the \ncurrent ele\u00adment, advance to the next element and test if there are more ele\u00adments in the store. Since \nSCI is call-by-name all free identi.ers are thunks. The block diagram of the circuit is given in Fig. \n1: The full schematic of the circuit for in-place map is also given in Fig. 1; for clarity we have identi.ed \nwhat ports correspond to what identi.ers. The ports on the right correspond to the term type com. Note \nthat we can optimise away the diagonal for variable identi.er curr be\u00adcause the .rst instance is used \nfor writing while the second one for reading. 4. Unfolding .nite recursion in space In its simplest instance \nrecursion can be seen simply as an unfold\u00ading of a circuit de.nition. Such recursive de.nitions can only \napply to well-founded de.nitions as in.nite unfoldings cannot be synthe\u00adsised. To support .nite recursion \nvia unfolding we augment the SCI type system with a rudimentary form of dependent typing.  Figure 1. \nIn-place map schematic and implementation First, it is convenient to add linear product (tensor) explicitly \nto the type system: G ' : . ' G, G '' G f M : . f M ' f M . M : . . . ' We also add a very simple form \nof dependent types, .{N} which is de.ned as .{0} = 1,.{N} = . . .{N - 1}, where 1 is the unit type (the \nempty interface). The language of indices N consists of natural number constants, subtraction and division \n(over natural numbers). This will guaran\u00adtee that recursive de.nitions have .nite unfoldings. Note that \nsince . is adjoint to . in the type system, the following three types are isomorphic: ' '' . .\u00b7 \u00b7\u00b7. . \n. . . . . \u00b7 \u00b7\u00b7 . . . . . .{N}. .. For example, an N-ary parallel execution operator can be recur\u00adsively \nde.ned, for example, as: par{1} = .x : com.x : com . com par{N} = .x : com.(x || par{N - 1}) : com . \ncom{N - 1}. com . com{N}.com. Recursive de.nitions in this dependent-type metalanguage are elaborated \n.rst into SCI by unfolding the de.nitions until all in\u00addices {N} are reduced, after which the normal \ncompilation process applies. Although this approach is technically very simple it is surpris\u00adingly effective. \nFor example, it allows the de.nition of sorting net\u00adworks using programming language syntax so that the \nelaborated SCI programs synthesise precisely into the desired sorting network. 4.1 Batcher s Bitonic \nSort Bitonic Sort [Bat68] is a well known algorithm for generating op\u00adtimal sorting networks. The de.nition \nof the algorithm is struc\u00adtural, i.e. it describes how the network is constructed from sub\u00adcomponents, \nrather than behavioural, i.e. indicating the way input data is processed into output data, as is the \ncase with most soft\u00adware algorithms. As a consequence, mapping Batcher s descrip\u00adtion of a sorting network \ninto a parallelisable program usually needs to change the point of view from the network to the individual \nele\u00adment. This is quite subtle and it renders the algorithm unrecognis\u00adable3. The computational element \nof a sorting network is a compare\u00ad and-swap circuit CS(m, n) = (m, n) (n, m) if m = n if m > n. A sorting \nnetwork is a circuit formed exclusively from CS circuits. A simple, but inef.cient, sorting network is \nthe odd-even transpo\u00adsition network, which for 4 elements is: In SCI the CS box can be implemented as \ndef CS = .m:exp..n:exp.if m<n then m . n else n . m With CS : exp . exp . exp . exp exp . exp . exp . \nexp. Let SC be the converse circuit: def SC = .m:exp..n:exp.if m>n then m . n else n . m. Note that the \ntype system allows the < operator to be given a parallel implementation; we can also use the special \nCS circuit, packaged like this so that it conforms to the type signature: The standard de.nition of \nBatcher s algorithm as a recursively speci.ed sorting network is this: 3 See http://www.tools-of-computing.com/tc/CS/Sorts/ \nbitonic_sort.htm and the CUDA implementation at http: //developer.download.nvidia.com/compute/cuda/sdk/ \nwebsite/samples.html.  .{2N} is the upside-down .{N} circuit. The other circuits involved in the recursive \nde.nition are: u (up-sort), d (down-sort), mu (merge-up) and md (merge-down). First let us introduce \nsyntactic sugar for function composition, f . g = .x : ..f(g(x)). In SCI the Batcher s bitonic sorting \nnetwork is de.ned by the following program: u{1} =d{1} = .x:exp.x mu{1} = md{1} = .x:exp.x u{N} = mu{N}. \n(u{N/2}. d{N/2}) d{N} = md{N}. (u{N/2}. d{N/2}) mu{N} = (mu{N/2}. mu{N/2}) . up{N} md{N} = (md{N/2}. \nmd{N/2}) . down{N} up{N} =(.x:exp{N/2-1}..z:exp{N/2-1}..y:exp..u:exp. x . y . z . u) . (CS . up{N/2 - \n2}) . (.x:exp..y:exp{N/2-1}..z:exp..u:exp{N/2-1}. x . z . y . u) down{N} =(.x:exp{N/2-1}..z:exp{N/2-1}..y:exp..u:exp. \nx . y . z . u) . (SC . up{N/2 - 2}) . (.x:exp..y:exp{N/2-1}..z:exp..u:exp{N/2-1}. x . z . y . u) Note \nthat above the . operator needs to be elaborated with the proper type, which is a trivial exercise in \ntype inference. A bitonic sorter is u{N} for N a power of 2. This program elaborates to a SCI program \nwhich then synthe\u00adsises to the standard bitonic sorting network. Note that this ap\u00adproach is very similar \nto the approach that structural functional hardware description languages such as Lava4. However, because \nour syntax is based on compact-closed combinators they match conventional programming language syntax \nbased on abstraction and application. In the case of Lava, the underlying syntax is set (al\u00adbeit not \nexplicitly) in a traced-monoidal framework which is some\u00adwhat more rigid [JSV96]. 5. Unfolding af.ne \nrecursion in time Sometimes a recursive unfolding of a circuit is not desired or not possible and we \nmay wish to have genuine run-time recursive behaviour. First note that unrestricted recursion in SCI \nis not possible as the unfolding of the recursive calls can violate the typing rule requiring disjointness \nof free identi.ers between a function and its argument, as in .x(F ) -. F (.x(F )). Therefore .x-point \ncan only be applied to closed terms: f F : . . . .x-point f .x(F ): . The recursive unfolding of the \n.x-point combinator suggests that the circuit representation of the .x-point circuit should be equiva\u00adlent \nto the following in.nite circuit: Ri is implemented as follows:  and SRi analogously. The registers \nmust be replaced with small addressable memory elements. Now we can rewrite our in.nite unfolding of \nthe .x-point like this: The solid/hollow arrows indicate the change of polarity of the bus [.]. Obviously, \nwhat we would like is something like this: A tempting idea is to fold the circuit for [F ] onto itself \nby using a FIX circuit very similar to the diagonal. However, this is not possible: some of the constituent \ncircuits of [F ] are stateful and their states can be different in different instances of F . The relevant \nstateful circuits are: R used in the implementation of operators and local variables. SR used in the \nimplementation of the diagonal. The other stateful circuit is D, but while the FIX block is executing, \nits state is the same for all instances of [F ]. As a .rst step towards implementing recursion we replace \nall occurrences of registers R and SR with indexed versions Ri and SRi: 4 See http://raintown.org/lava/ \nfor a Bitonic sort implementation. Every instance of [F ] now uses a different index, but is otherwise \nidentical. This means that we can replace the .xed indices with a counter and fold all the instances \ninto one single instance, indexed by the counter. The value of the counter will indicate what virtual \ninstance of [F ] is active and will be used as an index into the registers. The .x-point circuit will \nincrease this global counter whenever a recursive call is made and decrease it when a recursive return \nis made. When the counter is 0 the recursive return will be to the environment.  Circuit C is an up-down \ncounter. For example, for exp . exp the .x-point combinator which can be applied to function .f : exp \n. exp..n : exp.M is (data lines not shown):  for each input and output port in .. If the port is pure \ncontrol then the data line can be omitted. Correctness and limitations. Except for recursion, the imple\u00admentation \nof the compiler is a hardware instantiation of the syn\u00adchronous representation of the game semantics \nand is correct by construction as explained in a series of papers [McC02, GM10, GM11]. The detailed proof \nof correctness of the .x-point construc\u00adtor is beyond the scope of this paper, but the step-by-step construc\u00adtion \ngiven in this section mirrors the structure of the proof of cor\u00adrectness. There are two limitations which \nplay an important role in the correct implementation of .x-point: Concurrency. The SCI type system allows \nconcurrency in general, but concurrency inside the term to which the .x-point construc\u00adtor is applied \nmust be disallowed. It is crucial for the correctness of the implementation that only one virtual instance \nof the re\u00adcursive function, as indicated by the recursion depth counter, is active at any one moment \nin time. If the recursive call is used in a concurrent context this can no longer be guaranteed. Note \nthat recursively implemented functions can run in a parallel environ\u00adment, only internally parallelism \nis disallowed. For simplicity we disallow all parallelism, but a more careful analysis which only bans \nthe use of the recursive call in a parallel context is possible. Nested recursion. For simplicity we \nassume that recursive de.ni\u00adtions do not contain other recursive de.nitions. This is tech\u00adnically possible, \nby applying the same methodology and trans\u00adforming the recursion counter into an indexed array of recursion \ncounters. 5.1 Over.ow detection Recursive calls assume an idealized machine model with no bound on resources. \nIn physical machines that cannot be the case so re\u00adcursion calls that run too deep can over.ow resources. \nOn a con\u00adventional CPU-based architecture this leads to an abnormal termi\u00adnation of the program, triggered \neither by the run-time environ\u00adment (memory management modules or the operating system). A synthesised \nprogram, in contrast, runs in isolation and runtime er\u00adrors cannot be detected or processed. In our implementation, \nover\u00ad.ow would manifest simply by the counter rolling over, which would lead to active instances of the \nrecursive functions being mis\u00adidenti.ed. This is problematic because it will not give an error but will \nproduce the wrong result (similar to integer over.ow in C). It is important therefore to build into the \nlanguage an error de\u00adtection and handling mechanisms by providing the .x-point opera\u00adtor with an over.ow \nhandler. The syntax is f F : . . . G f M : com G f .x F with M : . The implementation:  The .x-point \noperator is, by design, aware of the maximum size of the counter. When a recursive call is about to increase \nbeyond the maximum size, instead of propagating the signal back to [F ] it will issue a special error \nsignal to command [M] which is the error handler. The control output of [M] is either ignored (grounded) \nor connected to a special global error port. 5.2 Tail recursion Iteration (while) is included in the \nrecursion-free fragment of SCI and it can be readily generalised to a higher-order tail-recursion operator. \nBecause tail-recursive calls do not need to percolate back through each instance of the function the \ncounter C and the instance-management apparatus are no longer necessary. The tail\u00adrecursive operator \nTAIL. becomes simply:  Note that the return from the recursive call is immediately propa\u00adgated to the \ncalling environment. Also, there is no need for the tail recursion operation to be applied to closed \nterms, as the tail unfolding is sequential rather than nested application: G f F : . . . G f tail.F : \n. 6. Performance and benchmarks At this early stage our research is mainly qualitative, demonstrating \nthe possibility of compiling recursive functions in static hardware. We will give two benchmark-style \nexamples to show that even in the absence of any compiler optimisations the performance is promising. \nWe choose a naive Fibonacci number calculation as a farily common benchmark of recursion performance \nand we exam\u00adine the call-by-name and the call-by-value versions. This is not a realistic ef.cient implementation \nbut is an excellent contrived ex\u00adample for creating a large number of recursive calls in an arbitrarily \nnested pattern. As this is the .rst, to our knowledge, attempt to give a general method for synthesising \nrecursion in time (recursion in space has been explored thoroughly by Lava) we are forced to compare \nagainst execution on a conventional CPU-based architecture. It is dif.cult to abstract away from the \nfact that we use not only different devices (CPU versus FPGA) but also different compilers and run\u00adtime \nenvironments. CPU We use an Intel Core 2 Duo processor running at 2.4 GHz on a machine with the Mac OS \nX 10.5 operating system, the Marst5 Algol 60 compiler and the gcc 4.0.1 C compiler. FPGA We use a Altera \nStratix III EPSL150 device, pro\u00adgrammed using Quartus 10.0, set on a Terasic DE3 board, op\u00aderating at \na default 50 MHz. We compute the maximum safe clock frequency using the TimeQuest timing constraint tool, \npart of the Quartus tool-set, and we scale execution time accordingly. In order to normalise the performance \ncomparison we will con\u00adsider two main metrics: Relative throughput We measure the relative throughput \nbetween the two devices computing from exactly the same source code. We take into account execution time \nand resource utilisation, which indicate the amount of parallelism that the device can support. Note \nthat on the FPGA parallel execution has no addi\u00adtional overhead or contention over resources, even in \nthe case of recursive functions, as no global stack is used in the imple\u00admentation. Relative throughput \nper transistor We further normalise the rel\u00adative throughput of the two computations relative to the \ntransis\u00adtor count, in order to obtain a fairer measure of performance. On both these measures the circuits \nwe synthesise perform better than CPU-based execution. 5 http://www.gnu.org/software/marst/ 6.1 Fibonacci, \ncall-by-name We use the simple-minded implementation of Fibonacci numbers in order to generate a very \nlarge number of recursive calls from small and simple code. This is appropriate as we do want to focus \nour benchmarking on the effectiveness of the implementation of recursion. In traditional Algol60 syntax \nthe program is: integer procedure fib(n); integer n; begin ifn-1= 0thenfib:=1 else if n-2 = 0 then fib \n:= 1 else fib := fib(n-1) + fib(n-2) end fib; The benchmark results are as follows: CPU FPGA Time 35.5 \ns 50 s Clock 2.4 GHz 137 MHz Cycles 85.2 B 6.85 B Transistors 300 M 400 M Utilisation 50% 2% Tx (rel.) \n5.62% 1,774.9% Tx/trans (rel) 7.49% 1,331.1% Note that the execution time on the FPGA is larger than \non the CPU. This is to be expected, as the code is purely sequential and uses very little memory. This \nis precisely the code that CPUs are highly optimised to execute fast, and this is on top of the CPU working \nwith a much faster clock. However, one core of this dual-core CPU is fully occupied by this computation, \nleading to a 50% utilisation of resources. We could compute two instances of .b in parallel with no extra \noverhead. On the FPGA, the area occupied by the synthesised circuit is only 2%, which means we can run \nup to 50 instances of this circuit in parallel, with no overhead. Note that the utilisation bound (2%) \nis on total ALUTs rather than on memory (1%), which means that the overhead needed to handle recursion \nis manageable. Using spatial unfolding we can only run .b to a depth of recursion of about 5-6 (the expansion \nis exponential) whereas our temporal unfolding has a depth of 256 (and reduced use of resources). A relative \ncomparison of execution time versus utilisation gives the FPGA 1,774.9% (almost 18\u00d7) total throughput \ncompared to the CPU (conversely, the CPU has 5.62% of the FPGA throughput). Images of block diagram and \nresource utilisation density for the synthesised circuit are in Fig. 2. Note the row of RAM blocks, three \nof which are used to implement the stateful elements of recursion they are the only memory overhead needed \nfor implementing re\u00adcursion. It is fair to take into account the fact that this particular FPGA has a \nlarger transistor count than the corresponding CPU. Normal\u00adising throughput by this factor still gives \nthe FPGA over 13\u00d7 total computational throughput per transistor. 6.2 Fibonacci, call-by-value The CBN \nevaluation makes the purely functional Fibonacci func\u00adtion staggeringly inef.cient. An improvement on \nthe naive algo\u00adrithm forces the evaluation of the argument inside the function: integer procedure fib(n); \ninteger n; begin integer n0, n1, n2; n0 := n; ifn0-1=0then fib:=1 else if n0-2 = 0 then fib := 1  Figure \n2. Synthesised FPGA layout for fib: block diagram and resource utilisation density (zoom-in) else begin \nn1 := fib(n0-1); n2 := fib(n0-2); fib:=n1 +n2end end fib; The results are now better in absolute terms \nboth on the CPU and the FPGA. The maximum clock frequency for the CBV fib is smaller, but a similar relative \nthroughput advantage for the FPGA version still occurs. CPU FPGA Time 2.8 s 4.0 s Clock 2.4 GHz 119 MHz \nCycles 6.78 B .48 B Transistors 300 M 400 M Utilisation 50% 2% Tx (rel.) 5.7% 1,735.9% Tx/trans (rel) \n7.6% 1,301.9% As a sanity check, we can compare this implementation with a call\u00adby-value implementation \nas used by more wide-spread compilers; Ocaml v3.10 computes fib(36) on the same CPU in 0.6 seconds, substantially \nbetter than Algol60 and Marst. However, the overall throughput of the FPGA remains higher. In Fig. 3 \nwe can see run-time snapshots taken with the Signal-Tap tool of the two circuits, indicating current \ncalculated value, current value of the internal recursion counter and next value being calculated. We \ncan notice in the CBN version the recursion counter cycling all the way back to 0 each time an argument \nneeds to be (re)evaluated, whereas in the CBV version the argument is picked up from the local variable. \n 6.3 Expressiveness The fact that the recursion operator can only be applied to syntac\u00adtically closed \nfunctions is not a major restriction in terms of ex\u00adpressiveness. Af.ne recursion has been studied in \nthe context of closed recursion in the lambda calculus [AFFM07] and is in fact the style of recursion \nused in Goedel s System T [AFFM10], where it is called iteration. We prefer af.ne recursion as iteration \nmay incor\u00adrectly suggest repeated serial application rather than true, nested, recursion. As a test of \nthe expressiveness of the compiler we programmed Escardo s implementation of the Bailey-Borwein-Plouffe \nspigot al\u00adgorithm for the computation of the n-th digit of p [BBP97]. The implementation is written in \nHaskell and it uses higher-order func\u00adtionals to give precise representations to real numbers as streams \nof digits [Esc09]. The program was successfully compiled to VHDL and synthesised on the same Altera-based \nFPGA-board. Note that our language does not offer direct support for lists and streams, which needed \nto be coded up in the standard way into the underly\u00ading lambda calculus. This test was done only to evaluate \nthe expressiveness of the compiler, as without direct semantic support for lists or streams the run-time \nperformance cannot be realistically assessed. Even so, the fact that ALUT utilisation stood at only 12% \nand the BRAM utilisation at only 8% indicates that the overall footprint is small and the overhead imposed \nby recursion manageable.  Figure 3. Run-time snapshots of CBN and CBV fib 7. Summary We have shown \na systematic approach for compiling programs us\u00ading af.ne recursion into correct circuits implemented \non FPGAs which operate between the 110 MHz to 140 MHz range (so the critical path is respectable) and \nthese circuits can use far fewer cy\u00adcles to compute a result than the corresponding software imple\u00admentation. \nHowever, unlike the software version we can systemat\u00adically explore space and time trade-offs by unrolling \nfunction calls through effectively inlining which in turn can increase through\u00adput at the cost of area. \nOur initial preliminary results are encour\u00adaging and we are developing the system further by allowing \nthe processing of off-chip dynamic data-structures and streams. By un\u00adderstanding how to effectively \nsynthesize recursive descriptions we get one step closer to the ability to transform programs into circuits \nfor implementations that have superior computational throughput or reduced energy consumption. References \n[AFFM07] Sandra Alves, Maribel Fern\u00b4andez, M\u00b4ario Florido, and Ian Mackie. The power of closed reduction \nstrategies. Electr. Notes Theor. Comput. Sci., 174(10):57 74, 2007. [AFFM10] Sandra Alves, Maribel Fern\u00b4ario \nFlorido, and Ian andez, M\u00b4Mackie. G\u00a8odel s system tau revisited. Theor. Comput. Sci., 411(11-13):1484 \n1500, 2010. [AJ92] Samson Abramsky and Radha Jagadeesan. New foundations for the Geometry of Interaction. \nIn IEEE Symposium on Logic in Computer Science (LICS), pages 211 222, 1992. [Bat68] Kenneth E. Batcher. \nSorting networks and their applications. In AFIPS Spring Joint Computing Conference, pages 307 314, 1968. \n[BBP97] D. Bailey, P. Borwein, and S. Plouffe. On the rapid computation of various polylogarithmic constants. \nMathematics of Compu\u00adtation, 66(218):903 914, 1997. [BCSS98] Per Bjesse, Koen Claessen, Mary Sheeran, \nand Satnam Singh. Lava: Hardware design in Haskell. In The International Con\u00adference on Functional Programming \n(ICFP), New York, NY, USA, 1998. ACM. [BH98] P. Bellows and B. Hutchings. JHDL: an HDL for recon.gurable \nsystems. In IEEE Symposium on FPGAs for Custom Computing Machines, Apr 1998. [Esc09] Martin H. Escardo. \nComputing with real numbers represented as in.nite sequences of digits in haskell. In Computability and \ncomplexity analysis, Ljubljana, Slovenia, 2009. (code available at the author s web page). [FG06] George \nFerizis and Hossam El Gindy. Mapping recursive func\u00adtions to recon.gurable hardware. In Field Programmable \nLogic and Applications, 2006. FPL 06. International Conference on, pages 1 6, 2006. [Ghi07] Dan R. Ghica. \nGeometry of Synthesis: a structured approach to VLSI design. In ACM Symposium on Principles of Program\u00adming \nLanguages (POPL), pages 363 375, 2007. [Ghi09] Dan R. Ghica. Applications of game semantics: From software \nanalysis to hardware synthesis. In IEEE Symposium on Logic in Computer Science (LICS), pages 17 26, 2009. \n[Ghi11] Dan R. Ghica. Functions interface models for hardware com\u00adpilation. In ACM/IEEE Ninth International \nConference on For\u00admal Methods and Models for Codesign (MEMOCODE), 2011. (forthcoming) [GM00] Dan R. Ghica \nand Guy McCusker. Reasoning about Ideal\u00adized Algol using regular languages. In The International Col\u00adloquium \non Automata, Languages and Programming (ICALP), pages 103 115, 2000. [GM10] Dan R. Ghica and Mohamed \nN. Menaa. On the composition\u00adality of round abstraction. In The International Conference on Concurrency \nTheory (CONCUR), pages 417 431, 2010. [GM11] Dan R. Ghica and Mohamed N. Menaa. Synchronous game se\u00admantics \nvia round abstraction. In The International Conference on Foundations of Software Science and Computation \nStruc\u00adtures (FoSSaCS), pages 350 364, 2011. [GS10] Dan R. Ghica and Alex Smith. Geometry of Synthesis \nII: From games to delay-insensitive circuits. Electr. Notes Theor. Comput. Sci., 265:301 324, 2010. [GS11] \nDan R. Ghica and Alex Smith. Geometry of Synthesis III: Re\u00adsource management through type inference. \nIn ACM Sympo\u00adsium on Principles of Programming Languages (POPL), pages 345 356, 2011. [HD08] Scott Hauck \nand Andr\u00b4e DeHon, editors. Recon.gurable Com\u00adputing, chapter Specifying Circuit Layout in FPGAs. Systems \non Silicon. Morgan Kaufmann Publishers, 2008. [JSV96] Andr\u00b4e Joyal, Ross Street, and Dominic Verity. \nTraced monoidal categories. Mathematical Proceedings of Cambridge Philo\u00adsophical Society, 119:447 468, \n1996. [KL80] G. M. Kelly and M. L. Laplaza. Coherence for compact closed categories. Journal of Pure \nand Applied Algebra, 19:193 213, 1980. [Mac94] Ian Mackie. The Geometry of Implementation. PhD thesis, \nImperial College, University of London, 1994. [Mac95] Ian Mackie. The geometry of Interaction machine. \nIn ACM Symposium on Principles of Programming Languages (POPL), pages 198 208, 1995. [McC02] Guy McCusker. \nA fully abstract relational model of Syntac\u00adtic Control of Interference. In The Conference on Computer \nScience Logic (CSL), pages 247 261, 2002. [McC07] Guy McCusker. Categorical models of syntactic control \nof interference revisited, revisited. LMS Journal of Computation and Mathematics, 10:176 216, 2007. [McC10] \nGuy McCusker. A graph model for imperative computation. Logical Methods in Computer Science, 6(1), 2010. \n[MTH99] Tsutomu Maruyama, Masaaki Takagi, and Tsutomu Hoshino. Hardware implementation techniques for \nrecursive calls and loops. In The International Conference on Field Programmable Logic and Applications \n(FPL), pages 450 455, 1999. [Nik04] Rishiyur Nikhil. Bluespec SystemVerilog: Ef.cient, correct RTL from \nhigh-level speci.cations. Formal Methods and Mod\u00adels for Co-Design (MEMOCODE), 2004. [OPTT99] Peter W. \nO Hearn, John Power, Makoto Takeyama, and Robert D. Tennent. Syntactic control of interference revisited. \nTheor. Comput. Sci., 228(1-2):211 252, 1999. [OT81] Peter O Hearn and Robert D. Tennent. Algol-like languages. \nBirkhauser, Boston, 1981. [Red96] Uday S. Reddy. Global state considered unnecessary: An intro\u00adduction \nto object-based semantics. Lisp and Symbolic Compu\u00adtation, 9(1):7 76, 1996. [Rey78] John C. Reynolds. \nSyntactic control of interference. In ACM Symposium on Principles of Programming Languages (POPL), pages \n39 46, 1978. [Rey81] John C. Reynolds. The essence of Algol. In Proceedings of the 1981 International \nSymposium on Algorithmic Languages, pages 345 372. North-Holland, 1981. [Rey89] John C. Reynolds. Syntactic \ncontrol of inference, part 2. In The International Colloquium on Automata, Languages and Programming \n(ICALP), pages 704 722, 1989. [Sel09] Peter Selinger. New Structures for Physiscs, chapter A survey of \ngraphical languages for monoidal categories. Springer Lecture Notes in Physics, 2009. [Skl04] Valery \nSklyarov. FPGA-based implementation of recursive algorithms. Microprocessors and Microsystems, 28(5-6):197 \n 211, 2004. Special Issue on FPGAs: Applications and Designs. [TL07] D.B. Thomas and W. Luk. A domain \nspeci.c language for recon.gurable path-based monte carlo simulations. In In\u00adternational Conference on \nField-Programmable Technology (ICFPT), pages 97 104, 2007. [Wal04] Matthew Wall. Games for Syntactic \nControl of Interference. PhD thesis, University of Sussex, 2004.    \n\t\t\t", "proc_id": "2034773", "abstract": "<p>Abramsky's Geometry of Interaction interpretation (GoI) is a logical-directed way to reconcile the process and functional views of computation, and can lead to a dataflow-style semantics of programming languages that is both operational (i.e. effective) and denotational (i.e. inductive on the language syntax). The key idea of Ghica's Geometry of Synthesis (GoS) approach is that for certain programming languages (namely Reynolds's affine Syntactic Control of Interference - SCI) the GoI processes-like interpretation of the language can be given a finitary representation, for both internal state and tokens. A physical realisation of this representation becomes a semantics-directed compiler for SCI into hardware. In this paper we examine the issue of compiling affine recursive programs into hardware using the GoS method. We give syntax and compilation techniques for unfolding recursive computation in space or in time and we illustrate it with simple benchmark-style examples. We examine the performance of the benchmarks against conventional CPU-based execution models.</p>", "authors": [{"name": "Dan R. Ghica", "author_profile_id": "81100060125", "affiliation": "University of Birmingham, Birmingham, United Kingdom", "person_id": "P2801409", "email_address": "d.r.ghica@cs.bham.ac.uk", "orcid_id": ""}, {"name": "Alex Smith", "author_profile_id": "81467661642", "affiliation": "University of Birmingham, Birmingham, United Kingdom", "person_id": "P2801410", "email_address": "ais523@cs.bham.ac.uk", "orcid_id": ""}, {"name": "Satnam Singh", "author_profile_id": "81100280060", "affiliation": "Microsoft Research, Cambridge, United Kingdom", "person_id": "P2801411", "email_address": "satnams@microsoft.com", "orcid_id": ""}], "doi_number": "10.1145/2034773.2034805", "year": "2011", "article_id": "2034805", "conference": "ICFP", "title": "Geometry of synthesis iv: compiling affine recursion into static hardware", "url": "http://dl.acm.org/citation.cfm?id=2034805"}