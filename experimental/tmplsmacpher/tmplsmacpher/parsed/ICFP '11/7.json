{"article_publication_date": "09-19-2011", "fulltext": "\n Pushdown Flow Analysis of First-Class Control Dimitrios Vardoulakis Olin Shivers Northeastern University \n dimvar@ccs.neu.edu Abstract Pushdown models are better than control-.ow graphs for higher\u00adorder .ow \nanalysis. They faithfully model the call/return structure of a program, which results in fewer spurious \n.ows and increased precision. However, pushdown models require that calls and returns in the analyzed \nprogram nest properly. As a result, they cannot be used to analyze language constructs that break call/return \nnesting such as generators, coroutines, call/cc, etc. In this paper, we extend the CFA2 .ow analysis \nto create the .rst pushdown .ow analysis for languages with .rst-class control. We modify the abstract \nsemantics of CFA2 to allow continuations to escape to, and be restored from, the heap. We then present \na summarization algorithm that handles escaping continuations via a new kind of summary edge. We prove \nthat the algorithm is sound with respect to the abstract semantics. Categories and Subject Descriptors \nF.3.2 [Semantics of Pro\u00adgramming Languages]: Program Analysis General Terms Languages Keywords pushdown \n.ow analysis, .rst-class continuations, re\u00adstricted continuation-passing style, summarization, higher-order \nfunctional language 1. Introduction Function call and return is the fundamental control-.ow mecha\u00adnism \nin higher-order languages. Therefore, if a .ow analysis is to model program behavior faithfully, it must \nhandle call and return well. Pushdown models of programs [15, 17, 22] enable .ow anal\u00ad yses with unbounded \ncall/return matching. These analyses are more precise than analyses based on control-.ow graphs. Pushdown \nmodels require that calls and returns in the analyzed program nest properly. However, many control constructs, \nsome of them in mainstream programming languages, break call/return nesting. Generators (e.g., in Python \n[14] or JavaScript [8]) are func\u00adtions that are usually called inside loops to produce a sequence of \nvalues one at a time. A generator executes until it reaches a yield statement, at which point it returns \nthe value passed to yield to its calling context. When the generator is called again, execution resumes \nat the .rst instruction after the yield. Coroutines (e.g., in Simula67 [3] or Lua [11]) can also suspend \nand resume their execu\u00adtion, but are more expressive than generators because they can spec\u00adify where \nto pass control when they yield. First-class continuations Permission to make digital or hard copies \nof all or part of this work for personal or classroom use is granted without fee provided that copies \nare not made or distributed for pro.t or commercial advantage and that copies bear this notice and the \nfull citation on the .rst page. To copy otherwise, to republish, to post on servers or to redistribute \nto lists, requires prior speci.c permission and/or a fee. ICFP 11, September 19 21, 2011, Tokyo, Japan. \nCopyright &#38;#169; 2011 ACM 978-1-4503-0865-6/11/09. . . $10.00 shivers@ccs.neu.edu reify the rest \nof the computation as a value. Continuations allow complex control .ow, such as jumping back to functions \nthat have already returned. Undelimited continuations (call/cc in Scheme [20] and SML/NJ [1]) capture \nthe entire stack. Delimited continu\u00ad ations [4, 7], found in Scala [16] and some Schemes, capture part \nof the stack. Continuations can express generators and coroutines, as well as multi-threading [18, 25] \nand Prolog-style backtracking. All these operators provide a rich variety of control behaviors. Un\u00adfortunately, \nwe cannot currently use pushdown models to analyze programs that use them. We rectify this situation \nby extending the CFA2 .ow analy\u00adsis [22] to languages with .rst-class control. In this article, we make \nthe following contributions. CFA2 is based on abstract interpretation of programs in contin\u00aduation-passing \nstyle (abbrev. CPS). We present a CFA2-style abstract semantics for Restricted CPS, a variant of CPS \nthat allows continuations to escape, but also permits effective rea\u00adsoning about the stack [23]. When \nwe detect a continuation that may escape, we copy the stack into the heap (sec. 4.3). We prove that the \nabstract semantics is a safe approximation of the actual run-time behavior of the program (sec. 4.4). \n In pushdown .ow analysis, each state has a stack of unbounded size. Hence, the state space is in.nite. \nAlgorithms that explore the state space use a technique called summarization. First\u00adclass control causes \nthe stack to be copied into the heap, so our analysis must also deal with in.nitely many heaps. We show \nthat it is not necessary to keep continuations in the heap during summarization; we handle escaping continuations \nusing a new kind of summary edge (sec. 5.3).  When calls and returns nest properly, execution paths \nsatisfy a property called unique decomposition: for each state s in a path, we can uniquely identify \nanother state s' as the entry of the procedure that contains s [17]. In the presence of .rst-class control, \na state can belong to more than one procedure. We allow paths that are decomposable in multiple ways \nand prove that our analysis is sound (sec. 5.4).  If continuations escape upward, a .ow analysis cannot \ngener\u00adally avoid including extra, spurious control .ows that degrade precision. What about continuations \nthat are only used down\u00adward, such as exception handlers or continuations captured by call/cc that never \nescape? We show that CFA2 can avoid spu\u00adrious control .ows for downward continuations (sec. 5.5).  2. \nWhy pushdown models? Finite-state .ow analyses, such as k-CFA, approximate programs as graphs of abstract \nmachine states. Each node in such a graph represents a program point plus some amount of abstracted envi\u00adronment \nand control context. Every path in the graph is considered a possible execution of the program. Thus, \nexecutions are strings in a regular language.  12 2 3 4 5 6  Figure 1. Control-.ow graph for a \nsimple program Finite-state analyses do not handle call and return well. Re\u00admembering only a bounded \nnumber of pending calls, they must inevitably include spurious paths in which a function is called from \none program point and returns to a different one. Execution traces that match calls with returns are \nstrings in a context-free language. Therefore, by abstracting a program to a pushdown automaton (or equivalent), \nwe can use the stack to eliminate call/return mismatch. The following examples illustrate the advantages \nof pushdown models. 2.1 Data-.ow information The following Scheme program de.nes the apply and identity \nfunc\u00adtions, then binds n1 to 1 and n2 to 2 and adds them. At program point (+ n1 n2), both variables \nare bound to constants; we would like a static analysis to be able to discover this. (define app (. (f \ne) (f e))) (define id (. (x) x)) (let* ((n1 (app id 1)) (n2 (app id 2))) (+ n1 n2)) Fig. 1 shows the \ncontrol-.ow graph for this program. In the graph, the top level of the program is presented as a function \ncalled main. Function entry and exit nodes are rectangles with sharp corners. Inner nodes are rectangles \nwith rounded corners. Each call site is represented by a call node and a corresponding return node, which \ncontains the variable to which the result of the call is assigned. Each function uses a local variable \nret for its return value. Solid arrows are intraprocedural steps. Dashed arrows go from call sites to \nfunction entries and from function exits to return points. There is no edge between call and return nodes; \na call reaches its corresponding return only if the callee terminates. A monovariant analysis, such as \n0CFA, considers all paths to be valid executions. Thus, we can bind n1 to 2 by calling app from 4 and \nreturning to 3. Also, we can bind n2 to 1 by calling app from 2 and returning to 5. At point 6, 0CFA \nthinks that each variable can be bound to either 1 or 2. (For polyvariant analyses, we can create similar \nexamples.) On the other hand, if we only consider paths that respect call/return matching, there is no \nspurious .ow of data. At 6, n1 and n2 are bound to constants. 2.2 Stack-change calculation Besides data-.ow \ninformation, pushdown models also improve control-.ow information. Hence, we can use them to accurately \ncalculate stack changes between program points. With call/return matching, there is only one execution \npath in our example: 121314 121314 89 1011 89 1011 12 34 567 In contrast, 0CFA thinks that the program \nhas a loop (there is a path from 4 to 3). Many optimizations require accurate information about stack \nchange. For instance: Most compound data are heap allocated in the general case. Examples include: closure \nenvironments, cons pairs, records, objects, etc. If we can show statically that such a piece of data \nis only passed downward, we can allocate it on the stack and reduce garbage-collection overhead.  Continuations \ncaptured by call/cc may not escape upward. In this case, we do not need to copy the stack into the heap. \n In object-oriented languages, objects may have methods that are thread-safe by using locks. An escape \nanalysis can eliminate unnecessary locking/unlocking in the methods of thread-private objects.  Such \noptimizations are better performed with pushdown models. 2.3 Fake rebinding It is possible that two \ncontrol-related references to the same vari\u00adable are always bound in the same run-time environment. If \na .ow analysis cannot detect this, it will additionally consider execution traces in which the two references \nare bound to different abstract values. We call this phenomenon fake rebinding [22]. (define (compose-same \nf x) (f (f x))) In compose-same, both references to f are always bound in the same environment (the top \nstack frame). However, if multiple clo\u00adsures .ow to f, a .nite-state analysis will consider traces that \ncall one closure at the inner call site and a different closure at the outer call site. CFA2 forbids \nthese paths because it knows that both refer\u00adences are bound in the top frame it correctly relates environment \nand control structure.  2.4 Broadening the applicability of pushdown models Pushdown-reachability algorithms \nuse a dynamic-programming technique called summarization. Summarization relies on proper nesting of calls \nand returns. If we call function app from location 2 in our example, summarization knows that, if the \ncall returns, it will return to location 3, not location 5. What if the call to app is a tail call, in \nwhich case the return point is effectively in a different procedure from the call site? We can handle \nthis by creating cross-procedure summaries [22]. In languages with exceptions, the return point may be \ndeeper in the stack. We can transform this case into ordinary call/return nesting and handle it precisely \nwith CFA2. Instead of thinking of an exception as a single jump deeper in the stack, we can return to \nthe caller, which checks if it can handle the exception and if not, it passes it to its own caller and \nso on. Functions return a pair of values, one for normal return and one for exceptional return. The JavaScript \nimplementation of CFA2 [5] uses this technique for exceptions. But what if the return point has already \nbeen popped off the stack, as is the case when using .rst-class control constructs? Push\u00addown models \ncannot currently analyze such programs, so we have to fall back to a .nite-state analysis and live with \nits limitations. In  v . Var = UVar + CVar u . UVar = a set of identi.ers k . CVar = a set of identi.ers \n. . Lab = ULab + CLab l . ULab = a set of labels . . CLab = a set of labels lam . Lam = ULam + CLam ulam \n. ULam ::= (.l(u k) call) clam . CLam ::= (.. (u) call) call . Call = UCall + CCall UCall ::= (f e q)l \nCCall ::= (q e). g . Exp = UExp + CExp f, e . UExp = ULam + UVar q . CExp = CLam + CVar pr . Program \n= ULam Figure 2. Partitioned CPS the rest of this paper, we show how to generalize pushdown models to \n.rst-class control. 3. Restricted CPS Preliminary de.nitions In this section we describe our CPS lan\u00adguage. \nCompilers that use CPS [1, 10, 21] frequently partition the terms in a program into two disjoint sets, \nthe user and the continu\u00adation set, and treat user terms differently from continuation terms. We adopt \nthis partitioning here (Fig. 2). Variables, lambdas and calls get labels from ULab or CLab. Labels are \npairwise distinct. User lambdas take a user argument and the current continuation; continuation lambdas \ntake only a user argument. We assume that all variables in a program have distinct names. Then, the de.ning \nlambda of a variable v, written def .(v), is the lambda term that contains v in its list of formals. \nFor any term g, iu.(g) is the innermost user lambda that contains g. Concrete syntax enclosed in [\u00b7] \ndenotes an item of abstract syntax. Functions with a ? subscript are predicates, e.g., Var?(e) returns \ntrue if e is a variable and false otherwise. We use two notations for tuples, (e1,...,en) and (e1,...,en), \nto avoid confusion when tuples are deeply nested. We use the latter for lists as well; ambiguities will \nbe resolved by the context. Lists are also described by a head-tail notation, e.g., 3 :: (1, 3, -47). \nHandling .rst-class control In CPS, we can naturally express .rst-class control without using special \nprimitives: when contin\u00aduations are captured by user closures, they may escape. Escaping continuations \ncomplicate reasoning about the stack. To enable reasoning about the stack in the presence of .rst-class \ncontrol, we have previously proposed a syntactically-restricted variant of CPS, called Restricted CPS \n(abbrev. RCPS) [23]. De.nition 1 (Restricted CPS). A program is in Restricted CPS iff a continuation \nvariable can appear free in a user lambda in operator position only. In RCPS, continuations escape in \na well-behaved way: after a continuation escapes, it can only be called; it cannot be passed as an argument \nagain. For example, the CPS-translation of call/cc, which is (.(f cc) (f (.(v k) (cc v)) cc)), is a valid \nRCPS term. Terms like (.(x k) (k(.(y k2) (y 123 k)))) are not valid. We can transform this term (and \nany CPS term) to a valid RCPS term by .-expanding to return the free reference to operator position: \n(.(x k) (k (.(y k2) (y 123 (.(u) (k u)))))). Why do we distinguish these very similar terms? Because, \naccording to the Orbit policy (cf. sec. 4.1), their stack behaviors differ. In the . . State = Eval + \nApply Eval = UEval + CEval UEval = UCall \u00d7 BEnv \u00d7 VEnv \u00d7 Time CEval = CCall \u00d7 BEnv \u00d7 VEnv \u00d7 Time Apply \n= UApply + CApply UApply = UClos \u00d7UClos \u00d7CClos \u00d7VEnv \u00d7Time CApply = CClos \u00d7 UClos \u00d7VEnv \u00d7Time Clos = \nUClos + CClos d . UClos = ULam \u00d7 BEnv c . CClos =(CLam \u00d7 BEnv)+ halt \u00df . BEnv = Var -Time ve . VEnv \n= Var \u00d7 Time -Clos t . Time = Lab * (a) Concrete domains (g, \u00df) Lam?(g) A(g, \u00df, ve) . ve(g, \u00df(g)) Var?(g) \n[UEA] ([(feq)l], \u00df, ve,t) . (proc, d, c, ve,l :: t) proc = A(f, \u00df, ve) d = A(e, \u00df, ve) c = A(q, \u00df, ve) \n[UAE] (proc, d, c, ve,t) . (call,\u00df ' , ve ' ,t) proc =([(.l(uk) call)],\u00df) \u00df ' = \u00df[u . t][k . t] ve ' \n= ve[(u, t) . d][(k, t) . c] [CEA] ([(qe). ], \u00df, ve,t) . (proc, d, ve,. :: t) proc = A(q, \u00df, ve) d = \nA(e, \u00df, ve) [CAE] (proc, d, ve,t) . (call,\u00df ' , ve ' ,t) proc = ([(.. (u) call)],\u00df) \u00df ' = \u00df[u . t] \nve ' = ve[(u, t) . d] (b) Concrete semantics Figure 3. Concrete semantics and domains case of the .rst \nterm, when execution reaches (y 123 k), we must restore the environment of the continuation that .ows \nto k, which may cause arbitrary change to the stack. In the second case, when execution reaches (y 123 \n(.(u) (k u))), a new continuation is born and no stack change is required. Thus, RCPS forces all exotic \nstack change to happen when calling an escaping continuation, not in other kinds of call sites. Concrete \nsemantics Execution in RCPS is guided by the seman\u00adtics of Fig. 3. In the terminology of abstract interpretation \n[2], this semantics is called the concrete semantics. In order to .nd proper\u00adties of a program at compile \ntime, one needs to derive a computable approximation of the concrete semantics, called the abstract se\u00admantics \n(cf. sec. 4). Execution traces alternate between Eval and Apply states. At an Eval state, we evaluate \nthe subexpressions of a call site before performing a call. At an Apply, we perform the call. The last \ncomponent of each state is a time, which is a sequence of call sites. Eval-to-Apply transitions increment \nthe time by re\u00adcording the label of the corresponding call site. Apply-to-Eval transitions leave the \ntime unchanged. Thus, the time t of a state reveals the call sites along the execution path to that state. \n Times indicate points in the execution when variables are bound. The binding environment \u00df is a partial \nfunction that maps variables to their binding times. The variable environment ve maps variable/time pairs \nto values. To .nd the value of a variable v, we look up the time v was put in \u00df, and use that to search \nfor the ac\u00adtual value in ve. By pairing variables with times, we allow a single variable to have multiple \nbindings at run time. Let s look at each transition individually. At a UEval state over [(f eq)l], we \nuse the function A to evaluate the simple expressions f , e and q: lambdas are paired up with \u00df to become \nclosures, while variables are looked up in ve using \u00df. We add the label l in front of the current time \nand transition to a UApply state (rule [UEA]). From UApply to Eval, we bind the formals of a procedure \n([(.l(uk) call)],\u00df) to the arguments and jump to its body. The new binding environment \u00df ' extends the \nprocedure s environment, with u and k mapped to the current time. The new variable envi\u00adronment ve ' \nmaps (u, t) to the user argument d, and (k, t) to the continuation c (rule [UAE]). The remaining two \ntransitions are similar. We use halt to denote the top-level continuation of a program pr. The initial \nstate I(pr) is ((pr, \u00d8), input, halt, \u00d8, ()), where input is a closure of the form ([(.l(uk) call)], \n\u00d8). The initial time is the empty sequence of call sites. 4. The CFA2 abstraction In this section, we \nll extend the abstract semantics of CFA2 to han\u00addle .rst-class control. The semantics uses two binding \nenviron\u00adments, a stack and a heap; we also use the stack for return-point information. We show the actual \ntransition rules in section 4.3; the main difference from the previous semantics is that continuations \ncan now be copied to, and restored from, the heap. Before that, we discuss how to manage the stack in \nRCPS (sec. 4.1) and how to decide whether a variable reference will be looked up in the stack or the \nheap (sec. 4.2). In section 4.4, we prove that the abstract semantics is an approximation of the concrete \nsemantics. The CFA2 abstraction only takes us halfway to a computable analysis: the abstract state space \nis in.nite, so we cannot explore it by enumerating all states. We tackle this problem in section 5. 4.1 \nStack-management policy The Orbit compiler [9, 10] compiles a CPS intermediate representa\u00ad tion to .nal \ncode that uses a stack. Orbit views continuations as clo\u00adsures whose environment record is a stack frame. \nTo decide when to push and pop the stack, we follow Orbit s policy. The main idea behind Orbit s policy \nis that we can manage the stack for a CPS program in the same way that we would manage it for the original \ndirect-style program: For every call to a user function, we push a frame for the arguments.  We pop \na frame at function returns. In CPS, user functions return by calling the current continuation with a \nreturn value.  We also pop a frame at tail calls. A UCall call site is a tail call in CPS iff it was \na tail call in the original direct-style program. In tail calls, the continuation argument is a variable. \n When a continuation is captured by a user closure, we copy the stack into the heap.  When we call \na continuation that has escaped, we restore its stack from the heap.  4.2 Stack/heap split The stack \nin CFA2 is more than a control structure for return-point information; it is also an environment structure \nit contains bind\u00adings. CFA2 has a novel approach to variable binding: two references to the same variable \nneed not be looked up in the same binding en\u00advironment. We split references into two categories: stack \nand heap references. In direct-style, if a reference appears at the same nest\u00ading level as its binder, \nthen it is a stack reference, otherwise it is a heap reference. For example, (.1(x)(.2(y)(x (x y)))) \nhas a stack reference to y and two heap references to x. Intuitively, only heap references may escape. \nWhen we call a user function, we push a frame for its arguments, so we know that stack references are \nalways bound in the top frame. When control reaches a heap reference, its frame is either deeper in the \nstack, or it has been popped. We look up stack references in the top frame, and heap references in the \nheap. Stack lookups below the top frame never happen (Fig. 4b). When a program p is CPS-converted to \na program p ', stack (resp. heap) references in p remain stack (resp. heap) references in p '. All references \nadded by the transform are stack references. We can give an equivalent de.nition of stack and heap refer\u00adences \ndirectly in CPS, without referring to the original direct-style program. Labels can be split into disjoint \nsets according to the in\u00adnermost user lambda that contains them. For the CPS translation of the previous \nprogram, (.1(x k1) (k1 (.2(y k2) (x y (.3(u) (x u k2)4))5))6) these sets are {1, 6} and {2, 3, 4, 5}. \nThe label to variable map LV (.) returns all the variables bound by any lambdas that belong in the same \nset as ., e.g., LV (4) = {y, k2, u} and LV (6) = {x, k1}. We use this map to model stack behavior, because \nall continuation lambdas that belong to a given user lambda .l get closed by extending .l s stack frame \n(cf. section 4.3). Notice that, for any ., LV (.) contains exactly one continuation variable. Using LV \n, we give the following de.nition. De.nition 2 (Stack and heap references). Let . be a call site that \nrefers to a variable v. The predicate S?(., v) holds iff v . LV (.). We call v a stack reference.  Let \n. be a call site that refers to a variable v. The predicate H?(., v) holds iff v/. LV (.). We call v \na heap reference.  v is a stack variable, written S?(v), iff all its references satisfy S?.  v is a \nheap variable, written H?(v), iff some of its references satisfy H?.  For instance, S?(5, y) holds because \ny .{y, k2, u} and H?(5, x) holds because x / .{y, k2, u}.  4.3 Abstract semantics The CFA2 semantics \nis an abstract machine that executes a program in RCPS (Fig. 4). The abstract domains appear in Fig. \n4a. An ab\u00ad stract user closure (member of the set t UClos) is a set of user lamb\u00addas. An abstract continuation \nclosure (member of t CClos) is either a continuation lambda or halt. A frame is a map from variables \nto abstract values, and a stack is a sequence of frames. All stack oper\u00adations except push are de.ned \nfor non-empty stacks only. A heap is a map from variables to abstract values. In contrast to the previous \nsemantics of CFA2, the heap can contain continuation bindings. Fig. 4c shows the transition rules. First-class \ncontrol shows up in two of the rules, [UCEA]. UAE] and [UOn transition from a UEvaltstate to a UApply \nstate (rule t[U UEA]), we .rst evaluate f, e and q. We evaluate atomic user  . . t . . UClos \u00d7t UEval \n= UCall \u00d7 Stack \u00d7 Heap t UApply = ULam \u00d7 tCClos \u00d7Stack\u00d7Heap . . t CEval = CCall \u00d7 Stack \u00d7 Heap . . CApply \n= CClos \u00d7 t ttUClos \u00d7 Stack \u00d7 Heap d . t UClos = Pow(ULam) c . t CClos = CLam + halt fr, tf . Frame \n=(UVar -UClos)+(CVar -CClos) tt st . Stack = Frame* h . Heap =(UVar -UClos)+ t (CVar -CClos \u00d7 Stack))Pow(t \n(a) Abstract domains pop(tf :: st) st push(fr, st) fr :: st (tf :: st)(v) tf (v) (tf :: st)[u . d ] tf \n[u . d ]:: st (b) Stack operations terms using A u . We non-deterministically choose one of the lamb\u00addas \nthat .ow to f as the operator in the t UApply state.1 The change to the stack depends on q and f. If \nq is a variable, the call is a tail call so we pop the stack (case 1). If q is a lambda, it evaluates \nto a new closure whose environment is the top frame, hence we do not pop the stack (cases 2, 3). Moreover, \nif f is a lambda or a heap reference then we leave the stack unchanged. However, if f is a stack reference, \nwe set f s value in the top frame to {ulam}, pos\u00adsibly forgetting other lambdas that .ow to f. The strong \nupdate to the stack prevents fake rebinding for stack references (cf. sec. 2.3): when we return to c \n, we may reach more stack references of f. These references and the current one are bound at the same \ntime. Therefore, they must also be bound to ulam. In the UApply-to-Utransition (rule [U tEval UAE]), \nwe push a frame for the procedure s arguments. If u is a heap variable, we update its binding in the \nheap with all lambdas in d . If k is a heap variable, we have a possibly escaping continuation. We save \nc in the heap and also copy the stack, so that we can restore it if c gets called later. In a tCApply \ntransition (rule [U CEval-to-tCEA]), we are prepar\u00ading for a call to a continuation so we must reset \nthe stack to the stack of its birth. When q is a lambda, it is a newly created clo\u00adsure so the stack \ndoes not change. When q is a stack reference, the t CEval state is a function return and the continuation \ns environment is the second stack frame. Therefore, we pop a frame before calling c . When q is a heap \nreference, we are calling a continuation that . .. .. {e} Lam?(e) may have escaped. The stack change \nsince the continuation capture st(e) S?(., e) can be arbitrary. We non-deterministically pick a pair \n( c, st ' ) from Au (e, ., st, h) h(q), jump to c and restore st ', which contains bindings for the \nh(e) H?(., e) stack references in c . In the CApply-to-UCAE]), the top frame tEval transition (rule [U \nis the environment of [(.. (u) call)]; stack references in call need this frame on the top of the stack. \nHence, we do not push; we extend the top frame with the binding for the continuation s parameter u. If \nu is a heap variable, we also update the heap. [U ' UEA] ([(feq)l], st, h) r (ulam, d, c, st , h) ulam \n.Au (f, l, st, h) d = Au (e, l, st, h) st(q) Var?(q) c = q . .. Lam?(q) Example Let s see how the abstract \nsemantics works on a pro\u00ad gram with call/cc. Consider the program pop(st) Var?(q) ' Lam?(q) . (H?(l, \nf) . Lam?(f)) (call/cc (.(c) (somefun (c 42)))) st = st .. st[f .{ulam}] Lam?(q) . S?(l, f) where somefun \nis an arbitrary function. We use call/cc to capture the top-level continuation and bind it to c. Then, \nsomefun will '' . .. [U UAE] ([(.l(uk) call)], d, = push([u . c, st, h) r (call, st , h ) never be called, \nbecause (c 42) will return to the top level with 42 ' d][k . c ], st) as the result. st h(u) . d (v \n= u) . H?(u) The CPS translation of call/cc is h ' (v)= h(k) .{( c, st)} (v = k) . H?(k) (.1(f cc) (f \n(.2(x k2) (cc x)) cc)) h(v) o/w The CPS translation of its argument is .. [U ' CEA] ([(qe). c, d, st \nd = , h) (.3(c k) (c 42 (.4(u) (somefunCPS u k))))  . .. ], st, h) r ( Au (e, ., st, h) t The initial \nstate I (pr)=(.1, {.3}, halt, (), \u00d8) is a UApply. {(q, st)} Lam?(q) (We abbreviate lambdas by their labels.) \nWe push a frame and ( c, st ' ) . S?(., q) jump to the body of .1. Since cc is a heap variable, we save \nthe .. {(st(q), pop(st))} h(q) H?(.,q) continuation and the stack in the heap, producing a heap h with \na single element [cc .{(halt, ())}], and tUEval state ' d, st, h) r (call, st [U ' ) , h ([(f .2 cc)], \n([f .{.3}][cc . halt]), h). .2 is essentially a continuation rei.ed as a user value. We tail call to \n.3, so we pop the stack, producing t UApply state CAE] ([(.. (u) call)], st ' = st[u . d ] h(u) . d (v \n= u) . H?(u) ' h (v)= h(v) o/w (.3, {.2}, halt, (), h). (c) Abstract semantics 1 An abstract execution \nexplores one path, but the algorithm that searches Figure 4. Abstract semantics and relevant de.nitions \nthe state space considers all possible executions.  ).). |([(g1 ...gn], \u00df, ve,t)|ca =([(g1 ...gn], \ntoStack(LV (.), \u00df, ve), |ve|ca ) |(([(.l(uk) call)],\u00df), d, c, ve,t)|ca =([(.l(uk) call)], |d|ca , |c|ca \n, st, |ve|ca ) () c = halt where st = toStack(LV (.),\u00df ' , ve) c =([(.. (u ' )call ' )],\u00df ' ) |(([(.. \n(u) call)],\u00df), d, ve,t)|ca =([(.. (u) call)], |d|ca , toStack(LV (.), \u00df, ve), |ve|ca ) |(halt, d, ve,t)|ca \n=(halt, |d|ca , (), |ve|ca ) |([(.l(uk) call)],\u00df)|ca = {[(.l(uk) call)]} |([(.. (u) call)],\u00df)|ca = [(.. \n(u) call)] |halt|ca = halt  |ve|ca = { (u, |ve(u, t)|ca ):(u . UVar) . H?(u)}.{ (k, makecs(ve(k, t), \nve)) : (k . CVar) . H?(k)} tt (halt, ()) c = haltwhere makecs(c, ve) ([(.. (u ' )call)], toStack(LV (.), \n\u00df, ve)) c = ([(.. (u ' )call)],\u00df) . .([ ui . d i ][k . halt]) ve(k, \u00df(k)) = halt toStack({u1,...,un,k}, \n\u00df, ve) . [ ui . d i ][k . [(.. (u) call)]] :: st ve(k, \u00df(k)) = ([(.. (u) call)],\u00df ' ) where di = |ve(ui,\u00df(ui))|ca \nand st = toStack(LV (.),\u00df ' , ve) Figure 5. From concrete states to abstract states g g where g . (halt \n+ Lam + Call) (a1,...,an) (b1,...,bn) iff for 1 . i . n, ai bi d 1 d 2 iff d 1 . d 2 h1 h2 iff h1(v) \nh2(v) for each v . dom(h1) tf 1 :: st1 tf 2 :: st2 iff tf 1 tf 2 . st1 st2 () () tf 1 tf 2 iff tf 1(v) \ntf 2(v) for each v . dom(tf 1) Figure 6. The relation on abstract states We next push a frame and jump \nto the body of .3: ([(c 42 .4)], ([c .{.2}][k . halt]), h). This is a non-tail call, so we do not pop: \n(.2, {42},.4, ([c .{.2}][k . halt]), h). We push a frame and jump to the body of .2: ([(cc x)], ([x .{42}][k2 \n. .4], [c .{.2}][k . halt]), h). As cc is a heap reference, we ignore the current continuation and stack \nand restore (halt, ()) from the heap: (halt, {42}, (), h). The program terminates with value {42}.  \n4.4 Correctness of the abstract semantics run-time behavior. First, we de.ne a map |\u00b7|ca from concrete \nto ab\u00adstract states. Next, we show that if . transitions to . ' in the concrete semantics, the abstract \ncounterpart |.|ca of . transitions to a state . ' which approximates |. ' |ca . Therefore, each concrete \nexecution, i.e., sequence of states related by ., has a corresponding abstract execution that computes \nan approximate answer. The map |\u00b7|ca appears in Fig. 5. The abstraction of an Eval state . of the form \n([(g1 ...gn], \u00df, ve,t) is an U. with ).Eval state the same call site. Since . does not have a stack, \nwe must expose stack-related information hidden in \u00df and ve. Assume that .l is the innermost user lambda \nthat contains .. To reach ., control passed from a UApply state . ' over .l. According to our stack t \npolicy, the top frame contains bindings for the formals of .l and any temporaries added along the path \nfrom . ' to . . Therefore, the domain of the top frame is a subset of LV (l), i.e., a subset of LV (.). \nFor each user variable ui . (LV (.) n dom(\u00df)), the top frame contains [ui .|ve(ui,\u00df(ui))|ca ]. Let k \nbe the sole continuation variable in LV (.). If ve(k, \u00df(k)) is halt (the return continuation is the top-level \ncontinuation), the rest of the stack is empty. If ve(k, \u00df(k)) is ([(.. (u) call)],\u00df ' ), the second frame \nis for the user lambda in which .. was born, and so forth: proceeding through the stack, we add a frame \nfor each live activation of a user lambda until we reach halt. The abstraction of a UApply state over \n([(.l(uk) call)],\u00df)is a UApply state . whose operator is [(.l(uk) call)]. The stack t of . represents \nthe environment in which the continuation argument was created, and we compute it using toStack as above. \nAbstracting a CApply is similar to the UApply case, only now the top frame is the environment of the \ncontinuation operator. Note that the abstraction maps drop the time of the concrete states, since the \nabstract states do not use times. In this section, we show that the abstract semantics simulates the \nThe abstraction of a user closure is the singleton set with the concrete semantics, which means that \nthe execution of a program corresponding lambda. The abstraction of a continuation closure is under the \nabstract semantics is a safe approximation of its actual the corresponding lambda. The abstraction |ve|ca \nof a variable environment is a heap, which contains bindings for the user and the continuation heap variables. \nEach heap user variable is bound to the set of lambdas of the closures that can .ow to it. Each heap \ncontinuation variable k is bound to a set of continuation-stack pairs. For each closure that can .ow \nto k, we create a pair with the lambda of that closure and the corresponding stack. The relation . 1 \n. 2 is a partial order on abstract states and can be read as . 1 is more precise than . 2 (Fig. 6). Tuples \nare ordered pointwise. Abstract user closures are ordered by inclusion. Two stacks are in iff they have \nthe same length and the corresponding frames are in . We can now state the simulation theorem. The proof \nproceeds by case analysis on the concrete transition relation; full details can be found in the .rst \nauthor s forthcoming dissertation. Theorem 3 (Simulation). If . . . ' and |.|ca . , then there exists \n' '' . such that . r . and |. ' |ca . . 5. Exploring the in.nite state space Pushdown-reachability algorithms, \nincluding CFA2, deal with the unbounded stack size by using a dynamic-programming technique called summarization. \nThese algorithms work on transition systems whose stack is unbounded, but the rest of the components \nare bounded. Due to escaping continuations, we also have to deal with Eval UUClos \u00d7 H UApply = ULam \u00d7 \ntHeap U= tUClos \u00d7 HHeap CApply CClos \u00d7 tStack \u00d7 H Ht Frame = UVar -UClos HFrame Stack = H H= UVar -UClos \n Heap t (a) Local domains |(call, st, h)|al =(call, |st|al , |h|al ) |(ulam, d, c, st, h)|al =(ulam, \nd, |h|al ) |( c, d, st, h)|al = ( c, d, |st|al , |h|al ) E||st=al \u00d8 st = ()tf I UVar st = tf :: st ' \n|h|al = h I UVar (b) Abstract to local maps = Call \u00d7 H Stack \u00d7 H Heap . .. .. in.nitely many heaps. \n Au (e, ., tf , h) 5.1 Overview of summarization {e} Lam?(e) tf (e) S?(., e) h(e) H?(., e) We start with \nan informal overview of summarization. Assume that a program is executing and control reaches the entry \nof a procedure. We start computing inside the procedure. While doing so, we are visiting several program \npoints inside the procedure and possibly calling (and returning from) other procedures. Sometime later, \nwe reach the exit and are about to return to the caller with a result. The intuition behind summarization \nis that, during this computation, the return point was irrelevant; it in.uences reachability only after \nwe return to the caller. Consequently, if from a program point n with an empty stack we can reach a point \nn ' with stack s ', then from n with stack s we can reach n ' with stack append(s ' ,s). Let s use summarization \nto .nd which nodes of the graph of Fig. 1 are reachable from node 1. We .nd reachable nodes by recording \npath edges, i.e., edges whose source is the entry of a procedure and target is some program point in \nthe same procedure. Path edges should not be confused with the edges already present in the graph. They \nare arti.cial edges used by the analysis to represent intraprocedural paths, hence the name. Node 1 goes \nto 2, so we record the edges (1, 1) and (1, 2). From 2 we call app, so we record the call (2, 8) and \njump to 8. In app, we .nd path edges (8, 8) and (8, 9). We .nd a new call (9, 12) and jump to 12. Inside \nid, we discover the edges (12, 12), (12, 13) and (12, 14). Edges that go from an entry to an exit, such \nas (12, 14), are called summary edges. We have not been keeping track of the stack, so we use the recorded \ncalls to .nd the return point. The only call to id is (9, 12), so 14 returns to 10 and we .nd a new edge \n(8, 10), which leads to (8, 11). We record (8, 11) as a summary also. From the call (2, 8), we see that \n11 returns to 3, so we record edges (1, 3) and (1, 4). Now, we have a new call to app. Reachability inside \napp does not depend on its calling context. From the summary (8, 11), we know that 4 can reach 5, so \nwe .nd (1, 5). Subsequently, we .nd the last two path edges, which are (1, 6) and (1, 7). During the \nsearch, we did two kinds of transitions. The .rst kind includes intraprocedural steps and calls; these \ntransitions do not shrink the stack. The second is function returns, which shrink the stack. Since we \nare not keeping track of the stack, we .nd the [UEA] (H[(f eq)l], tf , h) > (ulam, d, h) ulam .Au (f, \nl, tf , h) d = Au (e, l, tf , h) ' [UAE] (H[(.l(uk) call)], d, h) > (call, [u . d ], h ) h(u) . d (v \n= u) . H?(u) h ' (v)= h(v) o/w [H], tf , h) d, tf , h) CEA] ([(clam e). > (clam, d = Au (e, ., tf , \nh) '' [CAE] (H[(.. (u) call)], d, tf , h) > (call, tf , h ) tf ' = tf [u . d ] h(u) . d (v = u) . H?(u) \nh ' (v)= h(v) o/w (c) Local semantics Figure 7. Local semantics and relevant de.nitions target nodes \nof the second kind of transitions in an indirect way, by recording calls and summaries. We show a summarization-based \nalgorithm for CFA2 in section 5.3. The next section describes the local semantics, which we use in the \nalgorithm for transitions that do not shrink the stack.  5.2 Local semantics Summarization operates \non a .nite set of program points. Since the abstract state space is in.nite, we cannot use abstract states \nas program points. For this reason, we introduce local states (Fig. 7a) and de.ne a map |\u00b7|al from abstract \nto local states (Fig. 7b).  The local semantics (Fig. 7) describes executions that do not touch the \nrest of the stack (i.e., executions where functions do not return). A H] has no successor in CEval state \nwith call site [(ke). this semantics. Since functions do not call their continuations, the local frames \nand heaps contain only user bindings. Local steps are otherwise similar to abstract steps. Note that \nthere is no provision for .rst-class control in the local transitions; they are identical to the previous \nones [22]. The metavariable . ranges over local states. We de.ne the map |\u00b7|cl from concrete to local \nstates to be |\u00b7|al .|\u00b7|ca . Summarization distinguishes between different kinds of states: entries, exits, \ncalls, returns and inner states. CPS lends itself nat\u00adurally to such a categorization. The following \nde.nition works for all three state spaces: concrete, abstract and local. De.nition 4 (Classi.cation \nof states). A UApply state is an Entry control is about to enter the body of a function.  A CEval state \nis an Exit-Ret when the operator is a stack reference a function is about to return a result to its context. \n A CEval state is an Exit-Esc when the operator is a heap reference we are calling a continuation that \nmay have es\u00adcaped.  A CEval state where the operator is a lambda is an Inner state.  A UEval state \nis an Exit-TC when the continuation argument is a variable at tail calls control does not return to the \ncaller.  A UEval state is a Call when the continuation argument is a lambda.  A CApply state is a Return \nif its predecessor is an exit, or an Inner state if its predecessor is also an inner state. Our algo\u00adrithm \ndoes not distinguish between the two kinds of CApplys; the difference is just conceptual.   5.3 The \nCFA2 algorithm The algorithm for CFA2 appears in Fig. 8. Its goal is to compute which local states are \nreachable from the initial state of a program. For readers familiar with the previous algorithm: the \nmain ad\u00addition is the handling of Exit-Esc states (lines 25-33). Escaping continuations also require \nchanges to the handling of entries and tail calls. Entries are now a separate case, instead of together \nwith UCEvals. Last, the Propagate function takes an CApplys and inner Hextra argument. Structure of the \nalgorithm The algorithm uses a workset W, which contains path edges and summaries to be examined. An \nedge ( .1,. 2) is an ordered pair of local states. We call . 1 the source and . 2 the target of the edge. \nAt every iteration, we remove an edge from W and process it, potentially adding new edges in W. We stop \nwhen W is empty. An edge ( .1,. 2) is a summary when . 1 is an entry and . 2 is either an Exit-Ret or \nan Exit-Esc, not necessarily in the same pro\u00adcedure. Summaries carry an important message: each continuation \nthat can be passed to . 1 can .ow to the operator position of . 2. The algorithm maintains several sets. \nThe results of the analysis are stored in the set Seen. It contains path edges (from a procedure entry \nto a state in the same procedure) and summary edges. The target of an edge in Seen is reachable from \nthe source and from the initial state. Summaries are also stored in Summary. Escapes contains Exit-Esc \nstates. If the continuation parameter of a user lambda is a heap variable, entries over that lambda are \nstored in EntriesEsc. Final records .nal states, i.e., CApplys that call U halt with a return value for \nthe whole program. Callers contains triples (. 1,. 2,. 3), where . 1 is an entry, . 2 is a call in the \nsame procedure and . 3 is the entry of the callee. TCallers contains triples (. 1,. 2,. 3), where . 1 \nis an entry, . 2 is a tail call in the same procedure and . 3 is the entry of the callee. The initial \nstate I (pr) is de.ned as |I(pr)|cl . The helper function succ( .) returns the successor(s) of . according \nto the local semantics. Summaries for .rst-class continuations Perhaps surprisingly, even though continuations \ncan escape to the heap in the abstract semantics, we do not need continuations in the local heap. We \ncan handle escaping continuations with summaries. Consider the ex\u00adample from section 4.3. When control \nreaches [(cc x)], we want to .nd which continuation .ows to cc. We know that def .(cc) is .1. By looking \nat the single UApply over .1, we .nd that halt t .ows to cc. This suggests that, for escaping continuations, \nwe need summaries of the form ( .1,. 2) where . 2 is an Exit-Esc over a call site [(ke). ] and . 1 is \nan entry over def .(k). Edge processing Each edge ( .1,. 2) is processed in one of six ways, depending \non . 2. If . 2 is a return or an inner state (line 12), then its successor . 3 is a state in the same \nprocedure. Since . 2 is reachable from . 1, . 3 is also reachable from . 1. If we have not already recorded \nthe edge ( .1,. 3), we do it now (line 44). If . 2 is a call (line 14) then . 3 is the entry of the callee, \nso we propagate ( .3,. 3) instead of ( .1,. 3) (line 16). Also, we record the call in Callers. If an \nexit . 4 is reachable from . 3, it should return to the continuation born at . 2 (line 18). The function \nUpdate is responsible for computing the return state. We .nd the return value d by evaluating the expression \ne4 passed to the continuation (lines 48-49). Since we are returning to ..2 , we must restore the environment \nof its creation, which is tf 2 (possibly with stack .ltering, line 50). The new state . is the corresponding \nreturn of . 2, so we propagate ( .1,. ) (lines 51-52). If . 2 is an Exit-Ret and . 1 is the initial state \n(lines 19-20), then . 2 s successor is a .nal state (lines 53-54). If . 1 is some other entry, we record \nthe edge in Summary and pass the result of . 2 to the callers of . 1 (lines 22-23). Last, consider the \ncase of a tail call . 4 to . 1 (line 24). No continuation is born at . 4. Thus, we must .nd where . 3 \n(the entry that led to the tail call) was called from. Then again, all calls to . 3 may be tail calls, \nin which case we keep searching further back in the call chain to .nd a return point. We do the backward \nsearch by transitively adding a cross-procedure summary from . 3 to . 2. Let . 2 be an Exit-Esc over \na call site [(ke). ] (line 25). Its predecessor . ' CApply. To reach is an entry or a U.2, the algorithm \nmust go through . '. Hence, the .rst time the algorithm sees . 2 is at line 7 or 13, which means that \n. 1 is an entry over iu.([(ke). ]) and ( .1,. 2) is not in Summary. Thus, the test at line 26 is true. \nWe record . 2 in Escapes. We also create summaries from entries over def .(k) to . 2, in order to .nd \nwhich continuations can .ow to k. We make sure to put these summaries in Summary (line 29), so that when \nthey are examined, the test at line 26 is false. When . 2 is examined again, this time ( .1,. 2) is in \nSummary. If . 1 is the initial state, . 2 can call halt and transition to a .nal state (line 30). Otherwise, \nwe look for calls to . 1 to .nd continuations that can be called at . 2 (line 32). If there are tail \ncalls to . 1, we propagate summaries transitively (line 33). If . 2 is an entry over [(.l(uk) call)], \nits successor . 3 is a state in the same procedure, so we propagate ( .1,. 3) (lines 6-7). If k is a \nheap variable (lines 8-9), we put . 2 in EntriesEsc (so that it can be found from line 29). Also, if \nwe have seen Exit-Esc states that call k, we create summaries from . 2 to those states (line 11). If \n. 2 is a tail call (line 34), we .nd its successors and record the call in TCallers (lines 35-37). If \na successor of . 2 goes to an exit, we propagate a cross-procedure summary transitively (line 41). Moreover, \nif . 4 is an Exit-Esc, we want to make sure that ( .1,. 4) is in Summary when it is examined. We cannot \ncall Propagate with true at line 41 because we would be mutating Summary while iterating over it. Instead, \nwe use a temporary set which we unite with Summary after the loop (line 42).  01 Summary, Callers, TCallers, \nEntriesEsc, Escapes, Final .- \u00d8 02 Seen, W .- {(I (pr), I(pr))}03 while W \u00d8 = 04 remove (. 1, . 2) from \nW 05 switch . 2 06 case . 2 of Entry 07 for the . 3 in succ( .2), Propagate(. 1, . 3, false) 08 . 2 \nof the form ([(.l(uk) call)], d, h) 09 if H?(k) then insert . 2 in EntriesEsc 11 for each . 3 in Escapes \nthat calls k, Propagate(. 2, . 3, true) 12 case . 2 of CApply, Inner-CEval 13 for the . 3 in succ( .2), \nPropagate(. 1, . 3, false) 14 case . 2 of Call 15 for each . 3 in succ( .2) 16 Propagate(. 3, . 3, false) \n17 insert (. 1, . 2, . 3) in Callers 18 for each (. 3, . 4) in Summary, Update(. 1, . 2, . 3, . 4) 19 \ncase . 2 of Exit-Ret if . 1 = I (pr) then Final(. 2) 21 else 22 insert (. 1, . 2) in Summary 23 for \neach (. 3, . 4, . 1) in Callers, Update(. 3, . 4, . 1, . 2) 24 for each (. 3, . 4, . 1) in TCallers, \nPropagate(. 3, . 2, false) 25 case . 2 of Exit-Esc 26 if(. 1, . 2) not in Summary then 27 insert . 2 \nin Escapes 28 . 2 of the form ([(ke). ], tf , h) 29 for each . 3 in EntriesEsc over def .(k), Propagate(. \n3, . 2, true) else if . 1 = I (pr) then Final(. 2) 31 else 32 for each (. 3, . 4, . 1) in Callers, Update(. \n3, . 4, . 1, . 2) 33 for each (. 3, . 4, . 1) in TCallers, Propagate(. 3, . 2, true) 34 case . 2 of Exit-TC \n35 for each . 3 in succ( .2) 36 Propagate(. 3, . 3, false) 37 insert (. 1, . 2, . 3) in TCallers 38 \nS .- \u00d8 39 for each (. 3, . 4) in Summary insert (. 1, . 4) in S 41 Propagate(. 1, . 4, false) 42 Summary \n.- Summary . S Propagate(. 1, . 2, esc) 43 if esc then insert (. 1, . 2) in Summary 44 if(. 1, . 2) \nnot in Seen then insert (. 1, . 2) in Seen and W Update(. 1, . 2, . 3, . 4)  45 . 1 of the form ( (.l1 \n(u1 k1) call1)] ,d1, h1) 46 . 2 of the form ([ (fe2 (..2 (u2) call2))l2 ], tf 2, h2)  47 . 3 of the \nform ( (.l3 (u3 k3) call3)] ,d3, h2) 48 . 4 of the form ([ (k4 e4).4 ], tf 4, h4) 49 d .- A u (e4,.4, \ntf 4, h4) tf 2[f .{[(.l3 (u3 k3) call3)]}] S?(l2,f) tf .- tf 2 H?(l2,f) . Lam?(f)  51 . .- ([(..2 (u2) \ncall2)], d, tf , h4) 52 Propagate(. 1, . , false) Final(. ) 53 . of the form ([(ke). ], tf , h) 54 insert \n(halt, A u (e, ., tf , h), \u00d8, h) in Final Figure 8. CFA2 workset algorithm  5.4 Soundness The local \nstate space is .nite, so there are .nitely many path and summary edges. We record edges as seen when \nwe insert them in W, which ensures that no edge is inserted in W twice. Therefore, the algorithm always \nterminates. We obviously cannot visit an in.nite number of abstract states. To establish soundness, we \nrelate the results of the algorithm to the abstract semantics: we show that if a state . is reachable \nfrom I(pr), then the algorithm visits |. |al (cf. theorem 8). First-class continuations create an intricate \ncall/return structure, which complicates reasoning about soundness. When calls and returns nest properly, \nan execution path can be decomposed so that for each state . , we can uniquely identify another state \n. ' as the entry of the procedure that contains . [17]. When we add tail calls into the mix, unique decomposition \nis still possible [24]. However, in the presence of .rst-class control, a state can be\u00adlong to more than \none procedure. For instance, suppose we want to .nd the entry of the procedure containing . in the following \npath * *' '*' I(pr) r . c r . e r . c r . e r . r . where . ' is an Exit-Esc over [(ke). ], . e and \n. e ' are entries over def .(k), . c and . c ' are calls. The two entries have the form . e =(def .(k), \nd, c, st, h) ' ' '' d ' . e =(def .(k), ,c , st , h ) Both c and c ' can .ow to k and we can call either \nat . '. If we choose to restore c and st, then . is in the same procedure as . c. If '' ' we restore \nc and st , . is in the same procedure as . c. However, it is possible that c = c ' and st = st ', in \nwhich case . belongs to two procedures. Unique decomposition no longer holds. For this reason, we now \nde.ne a set of corresponding entries for each state, instead of a single entry [22]. De.nition 5 (Corresponding \nEntries). Let p = . e r * . where . e is an entry. We de.ne CEp( .) to be the smallest set such that: \n if . is an entry, CEp( .)= {. }  if p = . e r * . 1 r + . , . is an Exit-Esc over [(ke). ], . 1 is \nan entry over def .(k), then . 1 . CEp( .).  if p = . e r * . 1 r . , . is neither an entry nor an Exit-Esc, \n. 1 is neither an Exit-Ret nor an Exit-Esc, then CEp( .)= CEp( .1).  ** + t if p = . e r . 1 r . 2 r \n. 3 r . 4 r . , . is a CApply of the form ( c, d, st, h), . 4 is an Exit-Esc, . 3 . CEp( .4) and has \nd '' * the form (ulam, ,c, st, h ), . 2 . CEp( .3), . 1 is a Call, then CEp( .1) . CEp( .). t if p = \n. e r * . 1 r . 2 r + . 3 r . , . is a CApply, . 3 is an Exit-Ret, . 2 . CE * p( .3), . 1 is a Call, \nthen CEp( .1) . CEp( .). For each state . , we also de.ne CEp* ( .) to be the set of entries that can \nreach an entry in CEp( .) through tail calls. De.nition 6. Let p = . e r * . where . e is an entry. We \nde.ne CEp* ( .) to be the smallest set such that: CEp( .) . CE * p( .)  if p = . e r * . 1 r . 2 r \n* . , . 2 . CEp( .), . 1 is a Tail Call, then CE * p( .1) . CE * p( .).  Note that if . is an Exit-Esc \nover [(ke). ], a procedure that contains . has an entry . ' over iu.([(ke). ]). Thus, . ' is not in CEp( \n.) because iu.([(ke). ])= def .(k). For all other states, CEp( .) is the set of entries of procedures \nthat contain . . The following lemma relates the stack of a state with the stacks of its corresponding \nentries. Lemma 7. Let p =I (pr) r * . where . = (..., st, h). 1. If . is a .nal state then CEp( .)= \u00d8. \n 2. If . is an entry then CEp( .)= \u00d8. (Thus, CE * p( .)= \u00d8.) Let . e . CE * p( .), of the form (ulam, \nd, c, ste, he). Then, st = ste and the continuation argument of . is c . 3. If . is an Exit-Esc then \nits stack is not empty and CEp( .)= \u00d8. (We do not assert anything about the stack change between a state \nin CEp* ( .) and . ; it can be arbitrary.) 4. If . is none of the above, then CEp( .)= \u00d8.   Let . \ne =([(.l(uk) call)], d, c, ste, he). If . e . (CE * p( .) \\ CEp( .)) then there is a frame tf such that \nst = tf :: ste, and  there is a variable k ' such that tf (k ' )= c . If . e .CEp( .) then there is \na frame tf such that st = tf ::ste, dom(tf ) . LV (l), tf (u) d , and tf (k)= c .  The proof of lemma \n7 proceeds by induction on the length of the path p. We now state the soundness theorem. Its proof and \nthe proof of lemma 7 can be found in the .rst author s forthcoming dissertation. Theorem 8 (Soundness). \nIf p =I (pr) r * . then, after summarization: If . is a .nal state, then |. |al . Final.  If . is not \n.nal and . ' . CEp( .), then (|. ' |al , |. |al ) . Seen.  If . is an Exit-Ret or Exit-Esc and . ' . \nCE * p( .), then (|. ' |al , |. |al ) . Seen.  CFA2 without .rst-class control is complete, which means \nthat there is no loss in precision when going from abstract to local states [24]. The algorithm of Fig. \n8 is not complete; it may compute .ows that never happen in the abstract semantics. Consider the code: \n(define esc (.(f cc) (f (.(x k) (cc x)) cc))) (esc (.1(v1 k1) (v1 \"foo\" k1)) (.(a) (halt a))) (esc (.2(v2 \nk2) (k2 \"bar\")) (.(b) (halt b))) In this program, esc is the CPS translation of call/cc. The two user \nfunctions .1 and .2 expect a rei.ed continuation as their .rst argument; .1 uses that continuation and \n.2 does not. The abstract semantics .nds that {\"foo\"} .ows to a and {\"bar\"} .ows to b. However, the workset \nalgorithm thinks that {\"foo\",\"bar\"}.ows to b. At the second call to esc, it connects the entry to the \nExit-Esc state over [(cc x)] at line 11, which is a spurious .ow.  5.5 Various approaches to downward \ncontinuations In RCPS, the general form of a user lambda that binds a heap continuation variable is (.1(u \nk) (...(.2(u2 k2) (...(k e). ...)) ...)) where .1 contains a user lambda .2, which in turn contains a \nheap reference to k in operator position. During execution, if a closure over .2 escapes upward, merging \nof continuations at [(k e). ] is unavoidable. However, when .2 is not passed upward, the abstract semantics \nstill merges at [(k e). ]. A natural question to ask is how precise can CFA2 be for downward continuations, \nsuch as exception handlers or continuations captured by call/cc that never escape. In both cases, we \ncan avoid merging.  In section 2.4, we described how the JavaScript implementa\u00ad tion of CFA2 handles \nexception throws precisely. Another way to achieve this is by uniformly passing two continuations to \neach user function, the current continuation and an exception handler [1]. Consider a user lambda [(.(u \nk1 k2) (...(k2 e). . . . ))]where S?(., k2) holds. Every Exit-Ret over [(k2 e). ] is an excep\u00adtion throw. \nThe handler continuation lives somewhere on the stack. To .nd it, we propagate transitive summaries for \ncalls, as we do for tail calls. When the algorithm .nds an edge ( .1,. 2) where . 2 is an Exit-Ret over \n[(k2 e). ], it searches in Callers for a triple ( .3,. 4,. 1). If the second continuation argument of \n. 4 is a lambda, we have found a handler. If not, we propagate a summary ( .3,. 2), which has the effect \nof looking for a handler deeper in the stack. Note that the algorithm must keep these new summaries separate \nfrom the other summaries, so as not to confuse exceptional with ordinary control .ow. For continuations \ncaptured by call/cc that are only used down\u00adward, we can avoid merging by combining .ow analysis and \nescape analysis. Consider the lambda at the beginning of this subsection. During .ow analysis, we track \nif any closure over .2 escapes up\u00adward. We do this by checking for summaries ( .1,. 2), where . 1 is \nan entry over .1. If .2 is contained in a binding reachable from . 2 [12, sec. 4.4.2], then .2 is passed \nupward and we use the heap to look up k at [(k e). ]. Otherwise, we can assume that .2 does not escape. \nHence, when we see an edge ( .1,. 2) where . 1 is an entry over .2 and . 2 is an Exit-Esc over [(k e). \n], we treat it as an exception throw. We use the new transitive summaries to search deeper in the stack \nfor a live activation of .1, which tells us what .ows to k. 6. Related work The CFA2 workset algorithm \nis in.uenced by the functional ap\u00adproach of Sharir and Pnueli [17] and the tabulation algorithm of Reps \net al. [15]. CFA2 extends these algorithms to .rst-class func\u00ad tions, introduces the stack/heap split \nand applies to control con\u00adstructs that break call/return nesting. Traditional summary edges describe \nintraprocedural entry-to-exit .ows. We have created sev\u00aderal kinds of cross-procedure summaries for the \nvarious control pat\u00adterns. Summaries for tail calls describe .ows that do not grow the stack. Summaries \nfor exceptions describe .ows that grow the stack; the source of the summary may be deeper in the stack \nthan the tar\u00adget. Finally, summaries for .rst-class control describe .ows with arbitrary stack change. \nThe four different kinds of summaries can be conceptually uni.ed because they serve a common purpose: \nthey connect a continuation passed to a user function with the state that calls it. Earl et al. have \nproposed a pushdown higher-order .ow anal\u00adysis that does not use frames [6]. Instead, their analysis \nallocates all bindings in the heap with context, in the style of k-CFA. For k =0, their analysis runs \nin time O(n 6), where n is the size of the program. Like all pushdown-reachability algorithms, Earl et \nal. s analysis records pairs of states (.1,.2) where .2 is same-context reachable from .1. However, their \nalgorithm does not classify states as entries, exits, calls, etc. This has two drawbacks compared to \nthe tabulation algorithm. First, they do not distinguish between path and summary edges. Thus, they have \nto search the whole set of edges when they look for return points, even though only sum\u00admaries can contribute \nto the search. More importantly, path edges are only a small subset of the set S of all edges between \nsame\u00adcontext reachable states. By not classifying states, their algorithm maintains the whole set S, \nnot just the path edges. In other words, it records edges whose source is not an entry. In the graph \nof Fig. 1, some of these edges are (2, 3), (2, 6), (5, 7). Such edges slow down the analysis and do not \ncontribute to call/return matching, because they cannot evolve into summary edges. In CFA2, it is possible \nto disable the use of frames by classifying each reference as a heap reference. The resulting analysis \nhas similar precision to Earl et al. s analysis for k =0. We conjecture that this variant is not a viable \nalternative in practice, because of the signi.cant loss in pre\u00adcision. While there is extensive literature \non .nite-state higher-order .ow analysis, little progress has been made in taming the power of call/cc \nand general continuations. Might and Shivers s .CFA [12, 13] introduced a notion of frame strings to \ntrack stack mo\u00ad tion; these strings provide a notational vocabulary for describing and distinguishing \nvarious sorts of control transfer: recursive call, tail call, return, primitive application, as well \nas the more exotic control acts that are constructed with .rst-class control operators. However, the \nexpressiveness of this device is brought low by its eventual regular-expression-based abstraction. Once \nabstracted, it loses much of its ability to reason about unusual patterns of con\u00adtrol .ow. We suspect \nthat the in.nite-state analytic framework pro\u00advided by CFA2 could be the missing piece that would enable \na .CFA-based analysis to be computed without requiring precision\u00addestroying abstractions. Shivers and \nMight have also shown how functional coroutines can be constructed with continuations, and then exploited \nto fuse pipelines of online transducers together into ef.cient, optimized code [19]. Being able to apply \nthe power of pushdown models such as CFA2 to the transducer-fusion task raises interesting new possibilities. \nFor example, suppose we had a coroutine generator with a recursive control structure one that walks a \nbinary tree producing the elements at the leaves. We wish to connect this tree\u00adwalking generator to a \nsimple iterative coroutine that adds up all the items it receives. Is a pushdown .ow analysis powerful \nenough to fuse the composition of these two coroutines into a single, recursive tree traversal, instead \nof an awkward, heavyweight implementation that ping-pongs back and forth between two independent stacks? \n7. Conclusions In this paper, we generalize the CFA2 .ow analysis to .rst-class control. We propose an \nabstract semantics that allows stacks to be copied to the heap, and a summarization algorithm that handles \nthe in.nitely many heaps with a new kind of summary edges. With these additions, CFA2 becomes the .rst \npushdown model that analyzes .rst-class control constructs. Moreover, CFA2 can now analyze the same language \nfeatures as k-CFA, and do it more accurately. Thus, implementors of higher-order languages can use CFA2 \nas a drop-in replacement of k-CFA. We also revisit the idea of path decomposition to accommodate states \nthat belong to multiple procedures and prove our analysis sound. We show a program for which the abstract \nsemantics gives a different result from the local semantics and conclude that our new summarization algorithm \nis not complete. We are not certain that .rst-class control unavoidably leads to incompleteness; we plan \nto investigate if changes to the algorithm can make it complete. How\u00adever, it is possible that the abstract \nsemantics describes a machine strictly more expressive than pushdown systems, and that reacha\u00adbility \nfor this machine is not decidable. Acknowledgements This article reports on work supported by the Mozilla \nFoundation, and by the Defense Advanced Research Projects Agency under Air Force Research Laboratory \n(AFRL/Rome) Contract No. FA8650\u00ad10-C-7090 and Cooperative Agreement No. FA8750-10-2-0233. The views expressed \nare those of the authors and do not re.ect the of.cial policy or position of the Department of Defense \nor the U.S. Government.  References [1] A. W. Appel. Compiling with Continuations. Cambridge University \nPress, 1992. [2] P. Cousot and R. Cousot. Abstract interpretation: a uni.ed lattice model for static \nanalysis of programs by construction or approxima\u00adtion of .xpoints. In Proceedings of the Fourth Annual \nACM Sympo\u00adsium on Principles of Programming Languages, pages 238 252, Jan. 1977. [3] O.-J. Dahl and K. \nNygaard. SIMULA an ALGOL-based simulation language. Communications of the ACM, 9(9):671 678, 1966. [4] \nO. Danvy and A. Filinski. A functional abstraction of typed contexts. Technical Report 89/12, University \nof Copenhagen, 1989. [5] Doctor JS: A set of static-analysis tools for JavaScript. https:// github.com/mozilla/doctorjs. \n[6] C. Earl, M. Might, and D. Van Horn. Pushdown control-.ow analysis of higher-order programs. In Proceedings \nof the 2010 Workshop on Scheme and Functional Programming, 2010. [7] M. Felleisen. The theory and practice \nof .rst-class prompts. In Proceedings of the Fifteenth Annual ACM Symposium on Principles of Programming \nLanguages, pages 180 190, 1988. [8] JavaScript generators. https://developer.mozilla.org/en/ JavaScript/New_in_JavaScript/1.7. \n[9] D. Kranz. ORBIT: An Optimizing Compiler for Scheme. PhD thesis, Yale University, 1988. [10] D. Kranz, \nR. Kelsey, J. Rees, P. Hudak, J. Philbin, and N. Adams. Orbit: an optimizing compiler for Scheme. In \nProceedings of the 1986 SIGPLAN Symposium on Compiler Construction, pages 219 233, 1986. [11] The Lua \nprogramming language. http://www.lua.org. [12] M. Might. Environment Analysis of Higher-Order Languages. \nPhD thesis, Georgia Institute of Technology, 2007. [13] M. Might and O. Shivers. Analyzing the environment \nstructure of higher-order languages using frame strings. Theoretical Computer Science, 375(1 3):137 168, \nMay 2007. [14] Python generators. http://www.python.org/dev/peps/pep-0255. [15] T. W. Reps, S. Horwitz, \nand S. Sagiv. Precise interprocedural data.ow analysis via graph reachability. In Proceedings of the \nTwenty-Second Annual ACM Symposium on Principles of Programming Languages, pages 49 61, 1995. [16] T. \nRompf, I. Maier, and M. Odersky. Implementing .rst-class poly\u00admorphic delimited continuations by a type-directed \nselective CPS\u00adtransform. In Proceeding of the 14th ACM SIGPLAN International Conference on Functional \nProgramming, pages 317 328, 2009. [17] M. Sharir and A. Pnueli. Two approaches to interprocedural data\u00ad.ow \nanalysis. In S. Muchnick and N. Jones, editors, Program Flow Analysis: Theory and Applications. Prentice \nHall, 1981. [18] O. Shivers. Continuations and threads: Expressing machine concur\u00adrency directly in advanced \nlanguages. In O. Danvy, editor, Proceed\u00adings of the Second ACM SIGPLAN Workshop on Continuations (CW \n1997), Technical report BRICS-NS-96-13, University of \u00c5rhus, pages 2:1 15, Paris, France, Jan. 1997. \n[19] O. Shivers and M. Might. Continuations and transducer composition. In Proceedings of the 2006 ACM \nSIGPLAN Conference on Program\u00adming Language Design and Implementation (PLDI 2006), pages 295 307, Ottawa, \nCanada, June 2006. [20] M. Sperber, R. K. Dybvig, M. Flatt, A. van Straaten, R. Findler, and J. Matthews, \neditors. Revised6 Report on the Algorithmic Language Scheme. Cambridge University Press, 2010. [21] G. \nL. Steele. Rabbit: A Compiler for Scheme. Master s thesis, MIT, 1978. [22] D. Vardoulakis and O. Shivers. \nCFA2: A context-free approach to con\u00adtrol-.ow analysis. In Proceedings of the 19th European Symposium \non Programming (ESOP 2010), volume 6012 of Lecture Notes in Com\u00adputer Science, pages 570 589, Paphos, \nCyprus, Mar. 2010. Springer. [23] D. Vardoulakis and O. Shivers. Ordering multiple continuations on the \nstack. In Proceedings of the 2011 ACM SIGPLAN Workshop on Partial Evaluation and Program Manipulation \n(PEPM 2011), pages 13 22, Austin, Texas, Jan. 2011. [24] D. Vardoulakis and O. Shivers. CFA2: A context-free \napproach to control-.ow analysis. Logical Methods in Computer Science, 7(2:3): 1 39, May 2011. [25] M. \nWand. Continuation-based multiprocessing. In Conference Record of the 1980 LISP Conference, pages 19 \n28, Stanford, 1980.   \n\t\t\t", "proc_id": "2034773", "abstract": "<p>Pushdown models are better than control-flow graphs for higher-order flow analysis. They faithfully model the call/return structure of a program, which results in fewer spurious flows and increased precision. However, pushdown models require that calls and returns in the analyzed program nest properly. As a result, they cannot be used to analyze language constructs that break call/return nesting such as generators, coroutines, call/cc, <i>etc</i>.</p> <p>In this paper, we extend the CFA2 flow analysis to create the first pushdown flow analysis for languages with first-class control. We modify the abstract semantics of CFA2 to allow continuations to escape to, and be restored from, the heap. We then present a summarization algorithm that handles escaping continuations via a new kind of summary edge. We prove that the algorithm is sound with respect to the abstract semantics.</p>", "authors": [{"name": "Dimitrios Vardoulakis", "author_profile_id": "81461660787", "affiliation": "Northeastern University, Boston, MA, USA", "person_id": "P2801374", "email_address": "dimvar@ccs.neu.edu", "orcid_id": ""}, {"name": "Olin Shivers", "author_profile_id": "81100129912", "affiliation": "Northeastern University, Boston, MA, USA", "person_id": "P2801375", "email_address": "shivers@ccs.neu.edu", "orcid_id": ""}], "doi_number": "10.1145/2034773.2034785", "year": "2011", "article_id": "2034785", "conference": "ICFP", "title": "Pushdown flow analysis of first-class control", "url": "http://dl.acm.org/citation.cfm?id=2034785"}