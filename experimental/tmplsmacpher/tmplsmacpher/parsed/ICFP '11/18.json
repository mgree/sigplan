{"article_publication_date": "09-19-2011", "fulltext": "\n Parsing with Derivatives A Functional Pearl Matthew Might David Darais Daniel Spiewak University of \nUtah University of Wisconsin, Milwaukee might@cs.utah.edu, david.darais@gmail.com dspiewak@uwm.edu Abstract \nWe present a functional approach to parsing unrestricted context\u00adfree grammars based on Brzozowski s \nderivative of regular expres\u00adsions. If we consider context-free grammars as recursive regular ex\u00adpressions, \nBrzozowski s equational theory extends without modi.\u00adcation to context-free grammars (and it generalizes \nto parser combi\u00adnators). The supporting actors in this story are three concepts famil\u00adiar to functional \nprogrammers laziness, memoization and .xed points; these allow Brzozowski s original equations to be \ntranslit\u00aderated into purely functional code in about 30 lines spread over three functions. Yet, this \nalmost impossibly brief implementation has a draw\u00adback: its performance is sour in both theory and practice. \nThe culprit? Each derivative can double the size of a grammar, and with it, the cost of the next derivative. \nFortunately, much of the new structure in.icted by the derivative is either dead on arrival, or it dies \nafter the very next derivative. To eliminate it, we once again exploit laziness and memoization to transliterate \nan equational theory that prunes such debris into working code. Thanks to this compaction, parsing times \nbecome reasonable in practice. We equip the functional programmer with two equational theo\u00adries that, \nwhen combined, make for an abbreviated understanding and implementation of a system for parsing context-free \nlanguages. Categories and Subject Descriptors F.4.3 [Formal Languages]: Operations on languages General \nTerms Algorithms, Languages, Theory Keywords formal languages, parsing, derivative, regular expres\u00adsions, \ncontext-free grammar, parser combinator 1. Introduction It is easy to lose sight of the essence of parsing \nin the minutiae of forbidden grammars, shift-reduce con.icts and opaque action tables. To the extent \nthat understanding in computer science comes from implementation, a deeper appreciation of parsing often \nseems out of reach. Brzozowski s derivative upsets this calculus of effort and understanding to make \nthe construction of parsing systems accessible to the common functional programmer. Permission to make \ndigital or hard copies of all or part of this work for personal or classroom use is granted without fee \nprovided that copies are not made or distributed for pro.t or commercial advantage and that copies bear \nthis notice and the full citation on the .rst page. To copy otherwise, to republish, to post on servers \nor to redistribute to lists, requires prior speci.c permission and/or a fee. ICFP 11, September 19 21, \n2011, Tokyo, Japan. Copyright c &#38;#169; 2011 ACM 978-1-4503-0865-6/11/09. . . $10.00 The derivative \nof regular expressions [1], if gently tempered with laziness, memoization and .xed points, acts immediately \nas a pure, functional technique for generating parse forests from arbitrary context-free grammars. Despite \neven because of its simplicity, the derivative transparently handles ambiguity, left\u00adrecursion, right-recursion, \nill-founded recursion or any combina\u00adtion thereof. 1.1 Outline After a review of formal languages, we \nintroduce Brzozowski s derivative for regular languages. A brief implementation high\u00adlights its rugged \nelegance.  As our implementation of the derivative engages context-free languages, non-termination emerges \nas a problem.  Three small, surgical modi.cations to the implementation (but not the theory) laziness, \nmemoization and .xed points guarantee termination. Termination means the derivative can recognize arbitrary \ncontext-free languages.  We generalize the derivative to parsers and parser combinators through an equational \ntheory for generating parse forests.  We .nd poor performance in both theory and practice. The root \ncause is vestigial structure left in the grammar by earlier derivatives; this structure is malignant: \nthough it no longer serves a purpose, it still grows in size with each derivative.  We develop an optimization \ncompaction that collapses gram\u00admars by excising this mass. Compaction, like the derivative, comes from \na clean, equational theory that exploits laziness and memoization in its transliteration to working code. \n  In this article, we provide code in Racket, but it should adapt readily to any Lisp. All code and \ntest cases within or referenced from this article (plus additional implementations in Haskell and Scala) \nare available from: http://www.ucombinator.org/projects/parsing/ 2. Preliminary: Formal languages A \nlanguage L is a set of strings. A string w is a sequence of characters from an alphabet A. (From the \nparser s perspective, a character might be a token/terminal.) Two atomic languages arise often in formal \nlanguages: the empty language and the null (or empty-string) language: The empty language \u00d8 contains \nno strings at all: \u00d8 = {} . The null language E contains only the length-zero null string: E = {w} where \nlength(w)=0.  Or, using C notation for strings, E = {\"\"}. For convenience, we may use the symbol E to \nrefer to both the null language and the null string. Given an alphabet A, there is a singleton language \nfor every char\u00adacter c in that alphabet. Where it is clear from context, we use the character itself \nto denote that language; that is: c ={c} . 2.1 Operations on languages Because languages are sets, set \noperations like union apply: {foo}.{bar, baz} = {foo, bar, baz} . Concatenation (.) appends the product \nof the two languages: L1 . L2 = {w1w2 : w1 . L1 and w2 . L2} . The nth power of a language is the set \nof strings of n consecutive words from that language: Ln = {w1w2 ...wn : wi . L for 1 = i = n} . And, \nthe non-empty repetition of a language (its Kleene star) is the in.nite union of all its powers: 8 i \nL= L. i=0 2.2 Regular languages and context-free languages If a language is non-recursively de.nable \nfrom atomic sets using only union, concatenation and repetition, that language is regular. If we allow \nmutually recursive de.nitions, then the set of de\u00adscribable languages is exactly the set of context-free \nlanguages. (Even without Kleene star, the resulting set of languages is context\u00adfree.) We assume, of \ncourse, a least-.xed-point interpretation of such recursive structure. For instance, given the language \nL: L =({x}. L) . E. The least-.xed-point interpretation of L is a set containing a .nite string of every \nlength (plus the null string). Every string contains only the character x. [The greatest-.xed-point interpretation \nof L adds an in.nite string of x s.]  2.3 Encoding languages To represent the atomic and complex languages \nin code, there is a struct for each kind of language: (define-struct empty {}); \u00d8 (define-struct eps \n{}); E (define-struct char {value}) (define-struct cat {left right}) ; left . right (define-struct alt \n{this that}) ; this . that (define-struct rep {lang}) ; lang* Example In code, the language: Lab = Lab \n.{a, b} . E, becomes: (define L (alt (cat L (alt (char a) (char b))) (eps))) 3. Brzozowski s derivative \nBrzozowski de.ned the derivative of regular expressions in his work on the recognition of regular languages \n[1]. The derivative of a language L with respect to a character c is a new language that has been .ltered \nand chopped Dc(L): 1. First, retain only the strings that start with the character c.  2. Second, chop \nthat .rst character off every string. Formally:  Dc(L)= {w : cw . L} . Examples Db {foo, bar, baz} \n= {ar, az} Df {foo, bar, baz} = {oo} Da {foo, bar, baz} = \u00d8.  3.1 Recognition with the derivative The \nsimplicity of the derivative s de.nition masks its power. If one can compute successive derivatives of \na language, it is straightfor\u00adward to determine the membership of a string within a language, thanks \nto the following property: cw . L iff w . Dc(L). To determine membership, derive a language with respect \nto each character, and check if the .nal language contains the null string: if yes, the original string \nwas in; if not, it wasn t. 3.2 A recursive de.nition of the derivative Brzozowski noted that the derivative \nis closed over regular lan\u00adguages, and admits a recursive implementation: For the atomic languages: Dc(\u00d8)= \n\u00d8 Dc(E)= \u00d8 Dc(c)= E '' Dc(c)= \u00d8 if c = c. For the derivative over union: Dc(L1 . L2)= Dc(L1) . Dc(L2). \nThe derivative over Kleene star peels off a copy of the language: Dc(L)= Dc(L) . L. For the derivative \nof concatenation, we must consider the pos\u00adsibility that the .rst language could be null: Dc(L1 . L2)= \nDc(L1) . L2 if E . L1 Dc(L1 . L2)=(Dc(L1) . L2) . Dc(L2) if E . L1 We can express concatenation without \na conditional through the nullability function: d. This function returns the null language if its input \nlanguage contains the null string, and the empty set otherwise: d(L)= \u00d8 if E . L d(L)= E if E . L. Thus, \nwe can equivalently de.ne concatenation: Dc(L1 . L2)=(Dc(L1) . L2) . (d(L1) . Dc(L2)).  3.3 Nullability \nof regular languages Conveniently, nullability may also be computed using structural recursion on regular \nlanguages: d(\u00d8)= \u00d8 d(E)= E d(c)= \u00d8 d(L1 . L2)= d(L1) . d(L2) d(L1 . L2)= d(L1) . d(L2) d(L )= E. A recursive \nimplementation of the Boolean variant of the nullability function is straightforward: (define (d L) (match \nL [(empty) #f] [(eps) #t] [(char _) #f] [(rep _) #t] [(alt L1 L2) (or (d L1) (d L2))] [(cat L1 L2) (and \n(d L1) (d L2))])) Examples A couple examples illustrate the derivative on regular languages: Df {foo, \nbar} = {oo}.{foo, bar} Df {foo, bar} .{frak} = {oo}.{foo, bar} .{frak}.{rak} .  3.4 An implementation \nof the derivative As the description of a regular language is not recursive, it is straightforward to \ntransliterate the derivative into working code: (define (D c L) (match L [(empty) (empty)] [(eps) (empty)] \n[(char a) (if (equal? c a) (eps) (empty))] [(alt L1 L2) (alt (D c L1) (D c L2))] [(cat (and (? d) L1) \nL2) (alt (D c L2) (cat (D c L1) L2))] [(cat L1 L2) (cat (D c L1) L2)] [(rep L1) (cat (D c L1) L)])) \nMatching a regular language L against a consed list of characters w is straightforward: (define (matches? \nw L) (if (null? w) (d L) (matches? (cdr w) (D (car w) L)))) 4. Derivatives of context-free languages \nSince a context-free language is a recursive regular language, it is tempting to use the same code for \ncomputing the derivative. From the perspective of parsing, this has two chief drawbacks: 1. It doesn \nt work. 2. It wouldn t produce a parse forest even if it did. The .rst problem comes from the recursive \nimplementation of the derivative running into the recursive nature of context-free grammars. It leads \nto non-termination. The second comes from the fact that our regular implementation recognizes whether \na string is in a language rather than parsing the string. We tackle the termination problem in this section, \nand the parsing problem in the next. Example Consider the following left-recursive language: L = L .{x}. \nE. If we take the derivative of L, we get a new language: Dx L = Dx L .{x} . E. Mathematically, this \nis sensible. Computationally, it is not. The code from the previous section recurs forever as it attempts \nto compute the derivative of the language L. 4.1 Step 1: Laziness Preventing the implementation of the \nderivative from making an in.nite descent on a recursive grammar requires targeted laziness. Speci.cally, \nit requires making the .elds of the structs cat, alt and rep by-need.1 With by-need .elds, the computation \nof any (po\u00adtentially self-referential) derivatives in those .elds gets suspended until the values in \nthose .elds are required. 4.2 Step 2: Memoization With laziness, we can compute the derivative until \nit requires nulla\u00adbility (as in concatenation or testing membership). Nullability ea\u00adgerly walks the \nstructure of the entire language. Thus, nullability fails to terminate on a derived language such as \nthe one above. We need the derivative to return a .nite (if lazily explored) graph. By memoizing the \nderivative, it ties the knot when it re-encounters a language it has already seen: (define/memoize (D \nc L) #:order ([L #:eq] [c #:equal]) (match L [(empty) (empty)] [(eps) (empty)] [(char a) (if (equal? \na c) (eps) (empty))] [(alt L1 L2) (alt (D c L1) (D c L2))] [(cat (and (? d) L1) L2) (alt (D c L2) [(cat \nL1 L2) (cat (D c L1) L2)] [(rep L1) (cat (D c L1) L)])) The define/memoize form above de.nes a derivative \nfunction D that memoizes .rst by pointer equality on the language and then by value equality on the character. \n 4.3 Step 3: Fixed points The computation of nullability is more challenging than the com\u00adputation of \nthe derivative because it isn t looking for a structure; it s looking for a single answer: Yes, it s \nnullable, or No, it s 1 Lisp implementations that do not support lazy .elds have to provide them transparently \nwith macros, delay and force. not. As such, laziness and memoization can t help side-step self\u00addependencies \nthe way they did for the derivative. Consider the nul\u00adlability of the left-recursive language L: d(L)=(d(L) \n.\u00d8) . E. To know the nullability of L requires knowing the nullability of L. For decades, this problem \nhas been solved by interpreting the nullability of L as the least .xed point of the nullability equations. \nTo bare only the essence of nullability, we can hide the com\u00adputation of a least .xed point behind a \npurely functional abstrac\u00adtion: define/fix. The define/fix form uses Kleene s theorem to compute the \nleast .xed point of a monotonic recursive de.nition, and it allows the prior de.nition of nullability \nto be used with little change: (define/fix (d L) #:bottom #f (match L [(empty) #f] [(eps) #t] [(char \n_) #f] [(rep _) #t] [(alt L1 L2) (or (d L1) (d L2))] [(cat L1 L2) (and (d L1) (d L2))])) The #:bottom \nkeyword indicates from where to begin the iterative ascent toward the least .xed point. The define/fix \nform de.nes a function mapping nodes in a graph (V,E) to values in a lattice X, so that given an instance: \n(define/fix (fv) #:bottom .X body) After this de.nition, the function f : V . X is a least .xed point: \nf = lfp(.f..v.body), which is easily computed with straightforward iteration: lfp(F )= F n(.V .X ) for \nsome .nite n.  4.4 Recognizing context-free languages No special modi.cation is required for the matches? \nfunction. It works as-is for recognizing context-free languages. With access to laziness, memoization \nand a facility for comput\u00ading .xed points, we were able to construct a system for recognizing any context-free \nlanguage in less than 30 lines of code. 5. Parsers and parser combinators Using standard techniques from \nfunctional programming, we lifted the derivative from regular languages to context-free languages. If \nrecognition of strings in context-free languages were our goal, we would be done. But, our goal is parsing. \nSo, our next step is to generalize the derivative to parsers. This section reviews parsers and parser \ncombinators. (For a more detailed treatment, we refer the reader to [15, 16].) In the next section, we \nexplore their derivative. A partial parser p is a function that consumes a string and produces partial \nparses of that string. A partial parse is a pair containing the remaining unparsed input, and a parse \ntree for the pre.x. The set P(A, T ) contains the partial parsers over alphabet A that produce parse \ntrees in the set T : P(A, T ) . A * .P(T \u00d7 A * ). A (full) parser p consumes a string and produces all \npossible parses of the full string. The set LPJ(A, T ) contains the full parsers over alphabet A that \nproduce parse trees in the set T : LPJ(A, T ) . A * .P(T ). Of course, we can treat a partial parser \np . P(A, T ) as a full parser: LpJ(w)= {t :(t, E) . p(w)}, by discarding any partial parse that did not \nexhaust the input. 5.1 Simple parsers Simple languages can be implicitly promoted to partial parsers: \nA character c converts into a partial parser for exactly itself: {(c, w ' )} w = cw ' c = .w. \u00d8 otherwise. \nThe null string becomes the consume-nothing parser: E = .w. {(E, w)} . The empty set becomes the reject-everything \nparser: \u00d8= .w. {} .  5.2 Combining parsers Parsers combine in the same fashion as languages: The union \nof two parsers, p, q . P(A, X), combines all parse trees together, so that p . q . P(A, X): p . q = .w.p(w) \n. q(w). The concatenation of two parsers, p . P(A, X) and q . Q(A, Y ), produces a parser that pairs \nthe parse trees of the individual parsers together, so that p . q . P(A, X \u00d7 Y ): ''' ''' p . q = .w.{((x, \ny),w ):(x, w ) . p(w), (y, w ) . q(w )} In effect, the .rst parser consumes a pre.x of the input and \nproduces a parse tree. It passes the remainder of that input to the second parser, which produces another \nparse tree. The result is the left-over input paired with both of those parse trees. A reduction by function \nf : X . Y over a parser p . P(A, X) creates a new partial parser, p . f . P(A, Y ): p . f = .w.{((f(x),w \n' ):(x, w ' ) . p(w)} A reduction parser maps trees from X into trees from Y . In code, a new struct \nrepresents reduction parsers: (define-struct red {lang f}) Once again, the .eld lang should be lazy. \n  5.3 The nullability combinator A special nullability combinator, d, simpli.es the de.nition of the \nderivative over parsers. It becomes a reject-everything parser if the language cannot parse empty, and \nthe null parser if it can: d(p)= .w. {(t, w): t .LpJ(E)} . We can add a new kind of language node to \nrepresent these: (define-struct d {lang}) Once again, the .eld lang is lazy. (Please note that d is \nno longer the function from the previous section.) 5.4 The null reduction parser To implement the derivative \nof parsers for single characters: the null reduction partial parser, E . S, is handy. This parser can \nonly parse the null string; it returns a set of parse trees stored within: E . S = .w. {(t, w): t . S} \n. A new struct provides null-reduction nodes: (define-struct eps* {trees})  5.5 The repetition combinator \nIt is easiest to de.ne the Kleene star of a partial parser p . P(A, T ) in terms of concatenation, union \nand reduction, so that p . P(A, T * ): p =(p . p ) . .(head, tail).head : tail . E . {{O} . The colon \noperator (:) is the sequence constructor, and {O is the empty sequence. 6. Derivatives of parser combinators \nIf we can generalize the derivative to parsers and over parser com\u00adbinators, then we can construct parse \nforests using derivatives. But .rst, we must consider the question: What is the derivative of a parser? \nIntuitively, the derivative of a parser with respect to the charac\u00adter c should be a new parser. It should \nhave the same type as the original parser; that is, if the original parser consumed the alphabet A to \nconstruct parse trees of type X, then the new parser should do the same. Formally: Dc : P(A, T ) . P(A, \nT ). But, how should the derived parser behave? It should act as though the character c has been consumed, \nso that if the string w is supplied, it returns parses for the string cw. However, it also needs to strip \naway any null parses that come back. If it didn t strip these, then null parses containing cw would return \nwhen trying to parse w with the derived parser. It is nonsensical for a partial parser to expand its \ninput. Thus: Dc(p)= .w.p(cw) - (LpJ(E) \u00d7{cw}). To arrive at a framework for parsing, we can solve this \nequation for the partial parser p in terms of the derivative: Dc(p)= .w.p(cw) - (LpJ(E) \u00d7{cw}) iff Dc(p)(w)= \np(cw) - (LpJ(E) \u00d7{cw}) iff p(cw)= Dc(p)(w) . (LpJ(E) \u00d7{cw}). Fortunately, we ll never have to deal with \nthe left-over null parses in practice. With a full parser, these null parses are discarded: LpJ(cw)= \nLDc(p)J(w). Given their similarity, it should not surprise that the derivative of a partial parser resembles \nthe derivative of a language: The derivative of the empty parser is empty: Dc(\u00d8)= \u00d8. The derivative of \nthe null parser is also empty: Dc(E)= \u00d8. The derivative of the nullability combinator must be empty, \nsince it at most parses the empty string: Dc(d(L)) = \u00d8. The derivative of a single-character parser is \neither the null reduction parser or the empty parser: E .{c} c = c ' Dc(c ' )= \u00d8 otherwise. This rule \nis important: it allows the derived parser to retain frag\u00adments of the input string within itself. Over \ntime, as successive derivatives are taken, the parser is steadily transforming itself into a parse forest \nwith nodes like this. The derivative of the union is the union of the derivative: Dc(p . q)= Dc(p) . \nDc(q). The derivative of a reduction is the reduction of the derivative: Dc(p . f )= Dc(p) . f. The derivative \nof concatenation requires nullability, in case the .rst parser doesn t consume any input: Dc(p . q)=(Dc(p) \n. q) . (d(p) . Dc(q)). The derivative of Kleene star peels off a copy of the parser: Dc(p )=(Dc(p) . \np ) . .(h, t).h : t The rules are so similar to the derivative for languages that we can modify the \nimplementation of the derivative for languages to arrive at a derivative suitable for parsers: (define/memoize \n(D c L) #:order ([L #:eq] [c #:equal]) (match L [(empty) (empty)] [(eps* T) (empty)] [(d _) (empty)] \n[(char a) (if (equal? a c) (eps* (set c)) (empty))] [(alt L1 L2) (alt (D c L1) (D c L2))] [(cat L1 L2) \n(alt (cat (D c L1) L2)) (cat (d L1) (D c L2))] [(rep L1) (cat (D c L1) L)] [(red L f) (red (D c L) f)])) \n (Because pairing and list-building in Lisps both use cons, there is no reduction around the derivative \nof repetition.) 6.1 Parsing with derivatives Parsing with derivatives is straightforward until the last \ncharacter has been consumed. To parse, compute successive derivatives of the top-level parser with respect \nto each character in a string. When the string is depleted, supply the null string to the .nal parser. \nIn code, the parse function has the same structure as matches?: (define (parse w p) (if (null? w) (parse-null \np) (parse (cdr w) (D (car w) p))))  The question of interest is how to de.ne parse-null, which pro\u00adduces \na parse forest for the null parses of its input. Yet again, an equational theory guides: L\u00d8J(E)= {}LE \n. T J(E)= T Ld(p)J = LpJ(E) Lp . qJ(E)= LpJ(E) .LqJ(E) Lp . qJ(E)= LpJ(E) \u00d7LqJ(E) Lp . fJ(E)= {f(t1),...,f(tn)} \nwhere {t1,...,tn} = LpJ(E) Lp J(E)=(LpJ(E)) *  A note on repetition The rule for repetition can mislead. \nIf the interior parser can parse null, then there are an in.nite number of parse trees to return. However, \nin terms of descriptiveness, one gains nothing by allowing the interior of a Kleene star operation to \nparse null Kleene star already parses null by de.nition. So, in practice, we can replace that last rule \nby: {{O} p cannot parse null Lp J(E)= unde.ned otherwise. What we have at this point are mutually recursive \nset constraint equations that mimic the structure of the nullability function for languages. Once again, \nthe least .xed point is a sensible way of interpreting these equations. Thus, Kleene s .xed-point theorem, \nvia define/fix, returns the set of full null parses: (define/fix (parse-null p) #:bottom (set) (match \nl [(empty) (set)] [(eps* T) T] [(d L) (parse-null L)] [(char _) (set)] [(alt p1 p2) (set-union (parse-null \np1) (parse-null p2))] [(cat p1 p2) (for*/set ([t1 (parse-null p1)] [t2 (parse-null p2)]) (cons t1 t2))] \n[(red p1 f) (for/set ([t (parse-null p1)]) (f t))] [(rep _) (set ())])) It assumes that the null parse \nof each node is initially empty. 7. Performance and complexity The implementation is brief. The code \nis pure. The theory is ele\u00adgant. So, how does this perform in practice? In brief, it is awful. We constructed \na parser for Python 3.1. On one-line examples, it returns interactively. Yet, it takes just under three \nminutes to parse a (syntactically valid) 31-line input. The culprit? The size of the grammar within the \nparser can grow exponentially with the number of derivatives. (The rule for concatenation is to blame.) \nSpeci.cally, the grammar can double in size under the derivative. The cost model for parsing with derivatives \nis: number of derivatives \u00d7 cost of derivative + cost of .xed point at the end. The cost of the derivative \nis proportional to the size of the current grammar. The cost of the .xed point is quadratic in the size \nof the grammar for unambiguous parses in the worst case. Thus, the worst-case complexity of parsing a \ngrammar of size G over an input of length n is: O(n2nG + (2nG)2)= O(22nG2). Considering this complexity, \nit is remarkable that our example .nished at all. That it .nished in three minutes is astonishing. 7.1 \nExample: Growth in the grammar A glance at run-time behavior on the left-recursive list grammar exposes \nthe nature of the problem. The image on the left represents the grammar at the start; the image on the \nright represents the grammar after ten derivatives:  If one were to zoom in on image on the right, the \nnode on the bot\u00adtom right is (empty). All of the inbound edges are from concate\u00adnation nodes all of these \nnodes can be discarded. 8. Compaction Another equational theory shows how to eliminate unnecessary structure. \nThe empty parser is an annihilator under concatenation and the identity under union; a null parser is \nthe identity under concatenation. It is possible to aggressively perform reductions as pieces of parse \ntrees emerge. Our implementation utilizes the following sim\u00adpli.cations; we use (.) in lieu of (=) to \nemphasize direction: \u00d8. p = p .\u00d8.\u00d8 \u00d8. p = p .\u00d8. p (E .{t1}) . p . p . .t2.(t1,t2) p . (E .{t2}) . p . \n.t1.(t1,t2) (E .{t1,...,tn}) . f . E .{f(t1),...,f(tn)} ((E .{t1}) . p) . f . p . .t2.(t1,t2) (p . f) \n. g . p . (g . f) \u00d8. E . {{O} . We can implement these simpli.cation rules in a memoized, recursive \nsimpli.cation function. When simpli.cation is deeply recursive and memoized, we term it compaction. If \nthe algorithm compacts after every derivative, then the time to parse the 31-line Python .le drops from \nthree minutes to two seconds. A graph of the size of the residual Python grammar with respect to each \nderivative hints as to why:  8.1 Complexity The worst-case complexity is unchanged: it is still exponential. \nHowever, we can hypothesize about its average performance given the observation that the grammar tends \nto stay roughly constant in size (until collapsing into a parse forest at the very end). The cost of \neach derivative remains proportional to the size of the original grammar. The cost of the .xed point \nat the end is negligible because the grammar has collapsed under compaction. Thus, we conjecture with \nreason that the cost of parsing with derivatives is O(nG) in practice (for unambiguous grammars), where \nn is the size of the string, and G is the size of the grammar. Even for the ambiguous expression grammar, \nrecognition appears to be O(nG) (while producing all parse trees is exponential). 9. Related work There \nhas been a revival of interest in Brzozowski s derivative, itself a specialization of the well-known \nleft quotient operation on languages. Owens, Reppy and Turon re-examined the derivative in light of lexer \nconstruction [13], and Danielsson [5] used it to prove the totality of parser combinators. The literature \non parsing is vast; there are dozens of methods for parsing, including but not limited to abstract interpretation \n[3, 4], operator-precedence parsing [9, 14], simple precedence parsing [7], parser combinators [15, 16], \nLALR parsing [6], LR(k) parsing [12], GLR parsing [17], CYK parsing [11, 20, 2], Earley parsing [8], \nLL(k) parsing, and recursive descent parsing [19]. packrat/PEG parsing [10, 18]. Derivative-based parsing \nshares full coverage of all context-free grammars with GLR, CYK and Earley. Derivative-based parsing \nis not easy to classify as a top-down or bottom-up method. In personal correspondence, Stuart Kurtz pointed \nout that when the grammar is in Greibach Normal Form (GNF), the algorithm acquires a parallel top-down \n.avor. For grammars outside GNF, while watching the algorithm evolve under compaction, one sees what \nappears to be a pushdown stack emerge inside the grammar. (Pushes and pops appear as the jagged edges \nin the graph to the left.) The most directly related work is Danielsson s work on total parser combinators \n[5]. His work computes residual parsers simi\u00adlar to our own, but does not detail a simpli.cation operation. \nAc\u00adcording to our correspondence with Danielsson, simpli.cation does exist in the implementation. Yet, \nbecause it is unable to memoize the simpli.cation operation (turning it into compaction), the imple\u00admentation \nexhibits exponential complexity even in practice. Acknowledgements We are grateful for the thousands \nof com\u00adments on reddit, hackernews, Lambda the Ultimate and elsewhere that thoroughly dissected and improved \nan earlier draft of this work. We also thank the ICFP 2011 and the ESOP 2010 review\u00aders for their thoughtful \nand detailed feedback. This research is partly supported by the National Science Foundation under Grant \nNo. 1035658, by the DARPA CRASH project GnoSys: Rais\u00ading the level of discourse in systems program, and \nby the Na\u00adtional Nuclear Security Administration under the Accelerating Development of Retor.table CO2 \nCapture Technologies through Predictivity project through DOE Cooperative Agreement DE-NA0000740. 10. \nConclusion Our goal was a means to abbreviate the understanding and imple\u00admentation of parsing. Brzozowski \ns derivative met the challenge: its theory is equational, its implementation is functional and, with \nan orthogonal optimization, its performance is not unreasonable. References [1] BRZOZOWSKI, J. A. Derivatives \nof regular expressions. Journal of the ACM 11, 4 (Oct. 1964), 481 494. [2] COCKE, J., AND SCHWARTZ, J. \nT. Programming languages and their compilers: Preliminary notes. Tech. rep., Courant Institute of Mathematical \nSciences, New York University, New York, NY, 1970. [3] COUSOT, P., AND COUSOT, R. Parsing as abstract \ninterpretation of grammar semantics. Theoretical Computer Science 290 (2003), 531 544. [4] COUSOT, P., \nAND COUSOT, R. Grammar analysis and parsing by abstract interpretation, invited chapter. In Program Analysis \nand Compilation, Theory and Practice: Essays dedicated to Reinhard Wilhelm, T. Reps, M. Sagiv, and J. \nBauer, Eds., LNCS 4444. Springer discretionary--Verlag, Dec. 2006, pp. 178 203. [5] DANIELSSON, N. A. \nTotal parser combinators. In Proceedings of the 15th ACM SIGPLAN international conference on Functional \nprogramming (New York, NY, USA, 2010), ICFP 10, ACM, pp. 285 296. [6] DEREMER, F. L. Practical translators \nfor LR(k) languages. Tech. rep., Cambridge, MA, USA, 1969. [7] DIJKSTRA, E. W. Selected Writings on Computing: \nA Personal Perspective. Springer, Oct. 1982. [8] EARLEY, J. An ef.cient context-free parsing algorithm. \nCommuni\u00adcations of the ACM 13, 2 (Feb. 1970), 94 102. [9] FLOYD, R. W. Syntactic analysis and operator \nprecedence. Journal of the ACM 10, 3 (July 1963), 316 333. [10] FORD, B. Packrat parsing: Simple, powerful, \nlazy, linear time. In Proceedings of the 2002 International Conference on Functional Programming (Oct. \n2002). [11] KASAMI, T. An ef.cient recognition and syntax-analysis algorithm for context-free languages. \nTech. rep., Air Force Cambridge Research Lab, Bedford, MA, 1965. [12] KNUTH, D. On the translation of \nlanguages from left to right. Information and Control 8 (1965), 607 639. [13] OWENS, S., REPPY, J., AND \nTURON, A. Regular-expression derivatives re-examined. Journal of Functional Programming 19, 02 (2009), \n173 190. [14] PRATT, V. R. Top down operator precedence. In POPL 73: Proceedings of the 1st annual ACM \nSIGACT-SIGPLAN Symposium on Principles of Programming Languages (New York, NY, USA, 1973), POPL 73, ACM, \npp. 41 51. [15] SWIERSTRA, D. S., PABLO, AND SARIAVA, J. Designing and implementing combinator languages. \nIn Advanced Functional Programming (1998), pp. 150 206. [16] SWIERSTRA, S. Combinator parsing: A short \ntutorial. In Language Engineering and Rigorous Software Development, A. Bove, L. Bar\u00adbosa, A. Pardo, \nand J. Pinto, Eds., vol. 5520 of Lecture Notes in Computer Science. Springer Berlin / Heidelberg, Berlin, \nHeidelberg, 2009, ch. 6, pp. 252 300. [17] TOMITA, M. LR parsers for natural languages. In ACL-22: Pro\u00adceedings \nof the 10th International Conference on Computational Linguistics and 22nd annual meeting on Association \nfor Compu\u00adtational Linguistics (Morristown, NJ, USA, 1984), Association for Computational Linguistics, \npp. 354 357. [18] WARTH, A., DOUGLASS, J. R., AND MILLSTEIN, T. Packrat parsers can support left recursion. \nIn PEPM 08: Proceedings of the 2008 ACM SIGPLAN Symposium on Partial Evaluation and Semantics\u00adbased Program \nManipulation (New York, NY, USA, 2008), ACM, pp. 103 110. [19] WIRTH, N. Compiler Construction (International \nComputer Science Series), pap/dsk ed. Addison-Wesley Pub (Sd). [20] YOUNGER, D. H. Recognition and parsing \nof context-free languages in time n3. Information and Control 10, 2 (1967), 189 208.      \n\t\t\t", "proc_id": "2034773", "abstract": "<p>We present a functional approach to parsing unrestricted context-free grammars based on Brzozowski's derivative of regular expressions. If we consider context-free grammars as recursive regular expressions, Brzozowski's equational theory extends without modification to context-free grammars (and it generalizes to parser combinators). The supporting actors in this story are three concepts familiar to functional programmers - laziness, memoization and fixed points; these allow Brzozowski's original equations to be transliterated into purely functional code in about 30 lines spread over three functions.</p> <p>Yet, this almost impossibly brief implementation has a drawback: its performance is sour - in both theory <i>and</i> practice. The culprit? Each derivative can <i>double</i> the size of a grammar, and with it, the cost of the next derivative.</p> <p>Fortunately, much of the new structure inflicted by the derivative is either dead on arrival, or it dies after the very next derivative. To eliminate it, we once again exploit laziness and memoization to transliterate an equational theory that prunes such debris into working code. Thanks to this compaction, parsing times become reasonable in practice.</p> <p>We equip the functional programmer with two equational theories that, when combined, make for an abbreviated understanding and implementation of a system for parsing context-free languages.</p>", "authors": [{"name": "Matthew Might", "author_profile_id": "81309498719", "affiliation": "University of Utah, Salt Lake City, UT, USA", "person_id": "P2801401", "email_address": "might@cs.utah.edu", "orcid_id": ""}, {"name": "David Darais", "author_profile_id": "81488671612", "affiliation": "University of Utah, Salt Lake City, UT, USA", "person_id": "P2801402", "email_address": "david.darais@gmail.com", "orcid_id": ""}, {"name": "Daniel Spiewak", "author_profile_id": "81470652038", "affiliation": "University of Wisconsin, Milwaukee, Milwaukee, WI, USA", "person_id": "P2801403", "email_address": "dspiewak@uwm.edu", "orcid_id": ""}], "doi_number": "10.1145/2034773.2034801", "year": "2011", "article_id": "2034801", "conference": "ICFP", "title": "Parsing with derivatives: a functional pearl", "url": "http://dl.acm.org/citation.cfm?id=2034801"}