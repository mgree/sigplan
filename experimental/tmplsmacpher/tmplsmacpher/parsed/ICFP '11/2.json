{"article_publication_date": "09-19-2011", "fulltext": "\n Lightweight Monadic Programming in ML Nikhil Swamy* Nataliya Guts Daan Leijen* Michael Hicks *Microsoft \nResearch, Redmond University of Maryland, College Park Abstract Many useful programming constructions \ncan be expressed as mon\u00adads. Examples include probabilistic modeling, functional reactive programming, \nparsing, and information .ow tracking, not to men\u00adtion effectful functionality like state and I/O. In \nthis paper, we present a type-based rewriting algorithm to make programming with arbitrary monads as \neasy as using ML s built-in support for state and I/O. Developers write programs using monadic values \nof type mt as if they were of type t, and our algorithm inserts the necessary binds, units, and monad-to-monad \nmorphisms so that the program type checks. Our algorithm, based on Jones quali.ed types, produces principal \ntypes. But principal types are sometimes problematic: the program s semantics could depend on the choice \nof instantiation when more than one instantiation is valid. In such situations we are able to simplify \nthe types to remove any ambigu\u00adity but without adversely affecting typability; thus we can accept strictly \nmore programs. Moreover, we have proved that this simpli\u00ad.cation is ef.cient (linear in the number of \nconstraints) and coher\u00adent: while our algorithm induces a particular rewriting, all related rewritings \nwill have the same semantics. We have implemented our approach for a core functional language and applied \nit successfully to simple examples from the domains listed above, which are used as illustrations throughout \nthe paper. Categories and Subject Descriptors D.3.2 [Programming lan\u00adguages]: Language Classi.cations \nApplicative (functional) lan\u00adguages; F.3.3 [Logics and Meanings of Programs]: Studies of Pro\u00adgram Constructs \nType structure General Terms Languages,Theory Keywords monad, type, rewriting, coherence, coercion 1. \nIntroduction The research literature abounds with useful programming construc\u00adtions that can be expressed \nas monads, which consist of a type con\u00adstructor m and two operations, bind and unit:1 bind : .a,\u00df. m \na . (a . m\u00df) . m\u00df unit : .a. a . ma Example monads include parsers [13], probabilistic computa\u00adtions \n[25], functional reactivity [3, 8], and information .ow track\u00ad 1 These operations must respect certain \nlaws; cf. Wadler [29] for details. Permission to make digital or hard copies of all or part of this work \nfor personal or classroom use is granted without fee provided that copies are not made or distributed \nfor pro.t or commercial advantage and that copies bear this notice and the full citation on the .rst \npage. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior \nspeci.c permission and/or a fee. ICFP 11, September 19 21, 2011, Tokyo, Japan. Copyright c &#38;#169; \n2011 ACM 978-1-4503-0865-6/11/09. . . $10.00 ing [26]. In a monadic type system, if values are given \ntype t then computations are given type mt for some monad constructor m. For example, an expression of \ntype IO t in Haskell represents a computation that will produce (if it terminates) a value of type t \nbut may perform effectful operations in the process. Haskell s Monad type class, which requires the bind \nand unit operations given above, is blessed with special syntax, the do notation, for programming with \ninstances of this class. Moggi [22], Filinksi [11], and others have noted that ML pro\u00adgrams, which are \nimpure and observe a deterministic, call-by-value evaluation order, are inherently monadic. For example, \nthe value .x.e can be viewed as having type t . mt ': the argument type t is never monadic because x \nis always bound to a value in e, whereas the return type is monadic because the function, when applied, \nproduces a computation. As such, call-by-value applica\u00adtion and let-binding essentially employ monadic \nsequencing, but the monad constructor m and the bind and unit combinators for sequencing are implicit \nrather than explicit. In essence, the explicit IO monad in Haskell is implicit in ML. While programming \nwith I/O in ML is lightweight, program\u00adming with other monads is not. For example, suppose we are inter\u00adested \nin programming behaviors, which are time-varying values, as in functional reactive programs [3, 8]. With \nthe following notation we indicate that behaviors can be implemented as a monad: expres\u00adsions of type \nBeh a represent values of type a that change with time, and bindp and unitp are its monadic operations: \nMonad(Beh, bindb, unitb) As a primitive, function seconds has type unit . Beh int, its result representing \nthe current time in seconds since the epoch. An ML program using Beh effectively has two monads: the \nimplicit monad, which applies to normal ML computations, and the user\u00adde.ned monad Beh. The former is \nhandled primitively but the latter requires the programmer to explicitly use bindb, unitb, function composition, \netc., as in the following example: bindb (bindb (seconds()) (fun s-> unitb (is_even s))) (fun y-> unitb \n(if y then 1 else 2)) The type of this entire expression is Beh int: it is time-varying, oscillating \nbetween values 1 and 2 every second. Instead of using tedious explicit syntax, we would like to over\u00adload \nthe existing syntax so that monadic constructs are implicit, e.g., as in the following program (call \nit Q) let y = is_even (seconds()) in ifythen1else 2 We can see that the programs are structurally related, \nwith a bind corresponding to each let and application, and unit applied to the bound and .nal expressions. \nWhile ML programming with one monad is tedious, program\u00adming with more than one is worse. Along with \ndifferent binds and units for each monad, the programmer may have to insert calls to morphisms, which \nare functions that lift one monad into another. For example, suppose that along with time-varying expressions \nlike seconds() we allowed time-varying probability distributions ex\u00adpressed as a monad BehPrb a (we show \nan example of this in Section 2). Given a morphism from the .rst monad to the second, i.e., from a time-varying \nvalue to a time-varying probability distri\u00adbution, the programmer must insert calls to it in the right \nplaces. This paper presents an algorithm for automatically converting an ML program, like Q, which makes \nimplicit use of monads, into one that, like P , makes explicit use of monads, with binds, units, and \nmorphisms inserted where needed. Our algorithm operates as part of polymorphic type inference, following \nan approach simi\u00adlar to Jones [14], which is the foundation of Haskell s type class inference. We use \nthe syntactic structure of the program to iden\u00adtify where binds, units, and morphisms may appear. Type \ninference introduces a fresh variable for the monad being used at these vari\u00adous points and, as in Jones, \nwe produce quali.ed types of the form .\u00af. contains free type and monad variables, and P ..P . t, where \n\u00afcontains morphism constraints. We prove our algorithm produces principal types. As it turns out, our \nbasic scheme of inferring principal types could be encoded within Haskell s type class inference algorithm, \ne.g., one could de.ne morphisms as instances of a Morphism type class and then rely on inference to insert \nthem as necessary. How\u00adever, this algorithm would reject many useful programs as poten\u00adtially ambiguous, \nin particular, those whose types contain quan\u00adti.ed variables that appear only in the constraints P but \nnot in the type t . In the general setting of Haskell, this is sensible, as different instantiations \nof these variables could produce programs with different semantics. By focusing our attention on monads \nwe sidestep this problem. We can exploit laws governing the behavior of morphisms to ensure that all \npotential instantiations of such vari\u00adables are coherent; i.e., the program semantics will not change \nwith different instantiations. As such, our algorithm employs a linear\u00adtime strategy to eliminate such \nvariables, thus simplifying the con\u00adstraints P with no adverse effects on typability. We have implemented \nour inference algorithm for a core func\u00adtional language. We demonstrate its utility by applying it to \nexam\u00adple programs that use monads implementing behaviors, probabilis\u00adtic computations, and parsers (cited \nabove). We also develop an example using a family of monads for tracking information .ows of high-and \nlow-security data [7, 26]. We prove that our rewriting algorithm produces programs accepted by FlowCaml, \na dialect of ML that performs information .ow tracking [24], and thereby show that rewritten programs \nare secure (enjoy noninterference [12]). In summary, this paper presents an extension to ML for monadic \nprogramming, providing the following bene.ts: Developers can de.ne their own monads and program with \nthem in a direct style, avoiding the tedium of introducing binds, units, and morphisms manually.  Monadic \noperations are inserted automatically in conjunction with a novel type inference algorithm. Our algorithm \nproduces quali.ed types which describe the monads and morphisms used by a piece of code, usefully revealing \nthe monadic structure of the program to the developer. Exploiting the morphism laws, we ef.ciently infer \ngeneral types and produce coherent rewritings.  The system is quite .exible: we have shown, via our \nprototype implementation, that it supports many varieties of monads and their combination.  The next \nsection presents an overview of our approach; our main technical results are contained in Sections 3 \n5; and applications and related work discussed in Sections 6 and 7, respectively. 2. Overview This section \npresents an overview of our approach through the development of a few examples. We start by considering \na single user-de.ned monad and see that this is relatively easy to handle. Then, we look at handling \nmultiple monads, which require the use of morphisms. With these examples we illustrate our type inference \nand rewriting algorithm and discuss its key properties. 2.1 Programming with a single user-de.ned monad \nAs mentioned earlier, pure ML computations may be seen as al\u00adready employing an implicit monad, e.g., \ncovering partiality, state, exceptions, and I/O. We call this implicit monad Bot, since it is the bottom \nelement of our monad hierarchy; in effect, its unit opera\u00adtor is the identity function, and bind is the \nreverse application. Our goal is to exploit the inherent monadic structure of ML to provide support for \nprogramming with multiple, user-de.ned monads with the same ease as programming, implicitly, with Bot. \nOf course, we aim to do this without breaking compatibility with ML. Let us illustrate our idea on an \nexample using the probability monad [25]. Expressions of type Prb a describe distributions over values \nof type a, with bindp and unitp as its bind and unit combinators, respectively: Monad(Prb, bindp, unitp) \n The probability monad can be used to de.ne probabilistic models. The following program P , based on \nthe classic example due to Pearl [23], is an example of model code we would like to write: let rain = \nflip .5 in let sprinkler = flip .3 in let chance = flip .9 in let grass_is_wet = (rain || sprinkler) \n&#38;&#38; chance in if grass_is_wet then rain else fail () This program uses two functions it does not \nde.ne: flip : .oat . Prb bool fail : .a.unit . Prb a The .rst introduces distributions: flip(p) is a \ndistribution where true has probability p and false has probability 1 - p. The second, fail, represents \nimpossibility. The .rst four lines of P de.ne four random variables: rain is true when it is raining; \nsprinkler is true when the sprinkler is running; chance is explained below; and grass is wet is true \nwhen the grass is wet. The probability distribution for the last is dependent on the distributions of \nthe .rst three: the grass is wet if either it is raining or the sprinkler is running, with an additional \nbit of uncertainty due to chance: e.g., even with rain, grass under a tree might be dry. The last line \nof P implements a conditional distribution; i.e., the probability that it is raining given that the grass \nis wet. Mathematically, this would be represented with notation Pr(rain | grass is wet). Unfortunately, \nin ML we cannot write the above code directly because it is not type-correct. For example, the expression \nrain || sprinkler applies the || function, which has type bool . bool . bool, to rain and sprinkler, \nwhich each have type Prb bool. Fortunately, our system will automatically rewrite P so that it is type-correct, \nproducing the code given below. bindp (flip .5) (fun rain-> bindp (flip .3) (fun sprinkler-> bindp (flip \n.9) (fun chance-> bindp (unitp ((rain || sprinkler) &#38;&#38; chance)) (fun grass_is_wet-> if grass_is_wet \nthen unitp rain else fail ())))) When there is only one user-de.ned monad to consider, a rewriting such \nas this one is entirely syntactic. Roughly, each let is replaced by a bindp and each let-bound expression \nthat is not already monadic is wrapped with unitp. By doing so, we keep to the monadic structure of sequencing \nimplemented primitively by ML for its Bot monad, except we have now interposed the Prb monad. Although \nnot shown above, function applications are han\u00addled similarly: we insert bindp on both the left-and right-hand \nsides, thereby echoing the call-by-value semantics of function ap\u00adplication in ML. Under this rewriting \nsemantics, we can give a type to the orig\u00adinal program even though it is not typable in ML our algorithm \ninfers the type Prb bool for the source program. The types we infer always have a particular structure \nthat mirrors the monadic struc\u00adture of ML. As in Moggi s computational lambda calculus, we note that \ninferred function types always have a monadic type in their co-domain and monadic types never appear \nto the left of arrows. This corresponds to the following intuition. Since values are always pure, under \na call-by-value semantics, function arguments must al\u00adways be effect free (hence, no monadic types to \nthe left of arrows). Furthermore, since in ML functions can have arbitrary effects, their co-domains \nare always monadic.  2.2 Programming with multiple monads Now suppose we wish to program with both probabilities \nand behaviors (introduced in Section 1). Perhaps we would like the probability of rain to change with \ntime, e.g., according to the seasons. Then we can modify P (call it P ') so that the argument to flip \nis a function rainprb of type unit . Beh .oat: let rain = flip (rainprb ()) in ... Again, this program \nfragment is ill-typed, because flip expects a .oat but we have passed it a Beh .oat. If our rewriting \nsystem is to be applied, what should be the type of rain? One might expect it to be Beh (Prb bool), since \nit is a time-varying distribution. However, this type is nonsensical. Just as ML does not support data \nstructures containing non-values (e.g., those having type List (Bot a)) it does not support com\u00adputations \nparameterized by non-values, e.g., expressions of type Bot (Bot a) and, for that matter, type Beh (Prb \na). Therefore, a programmer must construct a combined monad, BehPrb, along with morphisms from the individual \nmonads into the combined one, to ensure that the overall program s semantics makes sense. There are several \nstandard techniques for combining monads [17]; here, we can combine them by de.ning objects of type BehPrb \nt as a stream of distributions over t, with the obvious morphisms. Monad(BehPrb, bindbp, unitbp) p2bp \n: Prb I BehPrb b2bp : Beh I BehPrb To allow automatic type-directed insertion of morphisms, we as\u00adsume \nthat there is at most one morphism between each pair of monads. In general, a morphism f1,2 : m1 I m2 \nhas the type .a.m1 a . m2 a.2 (We implicitly consider the unit operation of monad m as the morphism Bot \nI m.) Given the above morphisms, our system rewrites P ' thus: bindpb (bindpb (b2bp (rainprb ())) (fun \nv1-> p2bp (flip v1))) (fun rain-> p2bp (bindp (flip .3) (fun sprinkler-> ... where ... is identical to \nthe corresponding part of the rewrit\u00ading for P , and the .nal type is BehPrb bool. Here, the result 2 \nMorphisms (just like the binds and units) are not part of the source program; as such, they are treated \nspecially and not subject to the positivity condition on monadic types. of rainprb() is lifted into \nBehPrb .oat and then bound to the current value v1, which is passed to flip to generate a distribu\u00adtion. \nThis value is in turn lifted into BehPrb bool and bound to boolean rain for the rest of the computation, \nwhose result, of type Prb bool, is lifted into BehPrb bool by application of p2bp. 2.3 Properties of \ntype inference and rewriting Our examples so far have involved inserting known morphisms, binds, and \nunits, producing monomorphic types. But our system is more general in that we can rewrite a function \nto abstract the monads and morphisms it uses. For such functions our algorithm infers quali.ed types \nof the form .\u00af. t, where P is a set ..P of constraints m1 I m2 where the mi could be constant, known \nmonads, like Beh, or abstracted, unknown monads \u00b5 that appear in the bound type variables .\u00af(along with \nthe usual type variables a). For example, the type of let composefgx=f(gx) inferred by our system is \n.a\u00df.\u00b51\u00b52\u00b5. (\u00b51 I \u00b5, \u00b52 I \u00b5) . (\u00df . \u00b52 .) . (a . \u00b51 \u00df) . a . \u00b5. (where two occurrences of Bot on arrow \ntypes have been elided for readability). The rewritten term will take as arguments a represen\u00adtation \nof the monads \u00b5, \u00b51, and \u00b52, and two morphism functions corresponding to the two morphism constraints. \nSection 4 shows that our algorithm infers principal types, i.e. most general types. By restricting the \nstructure of inferred types (e.g., no monadic types to the left of arrows), and by providing only a limited \nform of subtyping, we obtain morphism constraints P that can be ef.\u00adciently solved using a simple linear-time \nprocedure. A solution of constraints allows us to instantiate the monadic operators and the morphisms \nin the elaborated term. Last but not least, our algorithm enjoys coherence: any two rewritings of the \nsame program are semantically equivalent. Said differently, choosing a particular solution does not affect \nthe mean\u00ading of the program. Coherence allows us to accept programs that would otherwise be rejected \nas ambiguous by related systems that employ quali.ed types for type inference, e.g., Haskell s type-class \nmechanism. We achieve coherence by taking advantage of the fol\u00adlowing assumed properties of morphisms: \nf1,2 . unit1 = unit2 (1) f1,2 (bind1 e1 e2)= bind2 (f1,2 e1)(f1,2 . e2) (2) f2,3 . f1,2 = f1,3 (3) Properties \n(1 2) are the so-called morphism laws, and the third is the transitivity property. For instance, the \ncombination of Beh, Prb, and BehPrb in the example above leads to constraints that admit several valid \nsolutions. One of these solutions, shown below, directly lifts all the let bindings to the BehPrb monad, \ninstead of lifting parts of the computation to either the Beh or Prb monads. bindbp (bindbp (b2bp (rainprb \n())) (fun v1-> p2bp (flip v1))) (fun rain-> bindbp (p2bp (flip .3)) (fun sprinkler-> bindbp (p2bp (flip \n.9)) (fun chance-> bindbp (unitbp ((rain || sprinkler) &#38;&#38; chance)) (fun grass_is_wet-> bindpb \n(fail ()) (fun f-> unitpb (if_ grass_is_wet then rain else f)))))) Using the morphism laws, we can show \nthat the two rewritings are equivalent. However we might argue that the .rst rewriting, produced by our \nalgorithm, is more precise than this one; intuitively it applies morphisms as late as possible and uses \nthe simplest types t ::= a | Tt1...tn (n ? 0 is the arity of T ) | t1 . mt2 monadic types m ::= \u00b5 | \nM type variables . ::= a | \u00b5 constraints P ::= p1, ..., pn constraint p ::= m1 I m2 type schemes s .\u00af \n::= ..P . t environment G ::= \u00b7| G,c:s | G,x:s values v ::= x | c | .x.e expressions e ::= v | e1 e2 \n| let x =e1 in e2 Figure 1. Core language syntax. m1 I m2 . P P |= m I m (M-Taut) (M-Hyp) P |= m1 I m2 \nP |= m1 I m2 P |= m2 I m3 (M-Trans) P |= m1 I m3 P f p1 ... P f pn (M-Many) P f p1, ..., pn Figure 2. \nThe constraint entailment relation. . = m,t . =[./.1] P, p\u00af2 |= .p\u00af1 .\u00af2 . ftv(..\u00af1.p\u00af1 . t) (Inst) P \nf..\u00af1.p\u00af1 . t ? ..\u00af2.p\u00af2 . .t Figure 3. The generic instance relation over type schemes. monad as long \nas possible. As such, the types more precisely reveal the monads actually needed by a piece of code. \n3. Quali.ed types for monadic programs This section describes the formal type rules of our system. Figure \n1 gives the syntax of types, constraints, environments, and expres\u00adsions. Monotypes t consist of type \nvariables a, full applied type constructors Tt1 ... tn, and function types t1 . mt2. Function arrows \ncan be seen as taking three arguments where the m is the monadic type. We could use a kind system to \ndistinguish monadic types from regular types but, for simplicity, we distinguish them using different \nsyntactic categories. Monadic types m are either monad constants M or monadic type variables \u00b5. Since \ntypes can be polymorphic over the actual monad (which is essential to principal types) we also have monadic \nconstraints p of the form m1Im2, which states that a monad m1 can be lifted to the monad m2. Type schemes \nare the usual quali.ed types [14] where we can quantify over both regular and monadic type variables. \nIn the expression language, we distinguish between syntactic value expressions v, and regular expressions \ne. This is in order to impose the value restriction of ML where we can only generalize over let-bound \nvalues. Figure 2 describes the structural rules of constraint entailment, where P |= p states that the \nconstraints in P entail the constraint p. The entailment relation is monotone (where P ' . P implies \nP |= P '), transitive, and closed under substitution. We also require that morphisms between the monads \nform a semi-lattice. This re\u00adquirement is not essential for type inference but as shown in Sec\u00adtion 5 \nit is necessary for a coherent evidence translation. Using entailment, we de.ne the generic instance \nrelation P f s1 ? s2 in Figure 3. This is just the regular instance de.nition on type schemes where entailment \nis used over the constraints. In the common case where one instantiates to a monotype, the rule simpli.es \nto: . = m,t . =[./.] P |= .p\u00af (Inst-Mono) P f... \u00afp\u00af. t ? .t 3.1 Declarative type rules Figure 4 describes \nthe basic type rules of our system; we discuss rewriting in the next subsection. The rules come in two \nforms: the rule P | G f v : s states that value expression v is well typed with a type s, while the rule \nP | G f e : mt states that expression e is well-typed with a monadic type mt , in both cases assuming \nthe constraints P and type environment G. The rule (TI-Bot) allows one to lift a regular type t into \na monadic type Bot t . The rules for variables, constants, let-bound values, instantia\u00adtion, and generalization \nare all standard. The rule for lambda ex\u00adpressions (TI-Lam) requires a monadic type in the premise to \nget well-formed function types. An expression like .x.x therefore gets type a . Bot a where the result \nis in the identity monad. The application rule (TI-App) and let-rule (TI-Do) lift into an arbitrary result \nmonad. The constraint .i. P |= mi I m ensures that all the monads in the premise can be lifted to a common \nmonad m, which allows a type-directed evidence translation to the underlying monadic program. 3.2 Type \ndirected monadic translation As described in Section 2, we rewrite a source program while performing \ntype inference, inserting binds, units, and morphisms as needed. This translation can be elegantly described \nusing a type directed evidence translation [14]. Since the translation is entirely standard, we elide \nthe full rules, and only sketch how it is done. In Section 5 we do show the evidence translation for \nthe type inference algorithm W since it is needed to show coherence. Our elaborated target language is \nSystem F (but here we leave out type parameters for simplicity). For the declarative rules, we can de.ne \na judgment like P | G f e : mt r e which proves that source term e is given monadic type mt and elaborated \nto the well-typed output term e. Similarly, the entailment relation P |= m1 I m2 r f returns a morphism \nwitness f with type .a. m1 a . m2 a. As an example, consider the (TI-App) rule. The rule with a type \ndirected translation is de.ned as: P | G f e1 : m1 (t2 . m3 t ) r e1 P | G f e2 : m2 t2 r e2 .i.P |= \nmi I m r fi P | G f e1 e2 : mt r bindm (f1 e1)(.x:(t2 . m3 t). bindm (f2 e2)(.y:t2.f3 (xy))) The bindm \nevidence comes from the Monad(m, bindm, unitm) constraint. This constraint is left implicit in the type \nrules since it is always satis.ed. Much of the time the morphisms are identity functions and the binding \noperations will be in the Bot monad which can all be optimized away. An optimizer can make further use \nof the monad laws to aggressively simplify the target terms. 3.3 Compatibility with ML Figure 4 is backwards \ncompatible with the ML type system: it accepts any program that is accepted by the standard Hindley-Milner \ntyping rules [5] extended with the value restriction we P | G f v : sP | G f e : mt G(x) = s G(c) = \ns P | G, x:t1 f e : m t2 (TI-Var) (TI-Const) (TI-Lam) P | G f x : s P | G f c : s P | G f .x.e : t1 \n. m t2 P | G f v : t P | G f v : s P f s ? t P, \u00afp | G f v : t \u00af. . ftv(G, P ) (TI-Bot) (TI-Inst) (TI-Gen) \nP | G f v : Bot t P | G f v : t P | G f v : .\u00af.. \u00afp . t P | G f e1 : m1 (t2 . m3 t) P | G f e2 : m2 \nt2 .i.P |= mi I m (TI-App) P | G f e1 e2 : m t P | G f v : s P | G, x:s f e : m t P | G f e1 : m1 t1 \nP | G, x:t1 f e2 : m2 t2 .i.P |= mi I m (TI-Let) (TI-Do) P | G f let x = v in e : m t P | G f let x =e1 \nin e2 : m t2 Figure 4. The basic declarative type rules. P | G f e : mt P |= m I m ' (TI-Lift) P | G \nf e : m ' t Figure 5. The type rules extended with a lifting rule. write an ML derivation as G fML e \n: t. To compare the derivations in both systems, we need to translate regular ML function types to monadic \nfunction types, and we de.ne \\t ) as: \\a) = a \\Tt1 ... tn) = T \\t1) ... \\tn) \\t1 . t2) = \\t1). Bot \\t2) \n We can state compatibility with ML formally as: Theorem 1 (Compatibility with ML). For any well-typed \nML non\u00advalue expression e such that G fML e : t, we also have a valid monadic derivation in the Bot monad \nof the form \u00d8| G f e : Bot \\t). For any well-typed value v where G fML v : t , we have a monadic derivation \nof the form \u00d8| G f v : \\t ). The proof is by straightforward induction over typing deriva\u00adtions. We observe \nthat for a standard ML program, we only need the Bot monad which means we can always reason under an \nempty constraint set \u00d8. Assuming empty constraints, the instance relation and generalization rule coincide \nexactly with the Hindley-Milner rules. The other rules now also correspond directly. We show the case \nfor the App rule as an example. By the induction hypothe\u00adsis, we can assume the premise \u00d8| G f e1 : Bot \n\\t2 . t ) and the premise \u00d8| G f e2 : Bot \\t2). The .rst premise is equiva\u00adlent to \u00d8| G f e1 : Bot (\\t2). \nBot \\t)) by de.nition. Us\u00ading the tautology rule of entailment, we can also conclude that \u00d8|= Bot I Bot \nand therefore we can apply rule (TI-App) to derive \u00d8| G f e1 e2 : Bot \\t) which is the desired result. \n 3.4 Extensions Unfortunately, the basic type rules are fragile with respect to .\u00adexpansion. For example, \nconsider the following functions: id = .x.x iapp :(int . Beh int) . Beh int (* given *) iapp = .f.f 1 \nThe basic rules infer the type of id to be .a. a . Bot a, and we suppose the type of iapp is given by \nthe programmer. With these types both the applications iapp id and iapp (.x.x) are rejected because id \nand .x.x have the type a . Bot a where the monadic However, we can lift the monadic result type by using \n.\u00adexpansion and introducing an application node, e.g. the .-expanded expression iapp (.x.id x) is accepted \nsince the application rule allows one to lift the result monad to the required Beh monad. Since the monadic \ntypes only occur on arrows, the programmer can always use a combination of applications and .-expansions \nto lift a monadic type anywhere in a type. Fortunately, such manual .-expansion is rarely required: only \nwhen combining higher-order functions where automatic lifting is expected on the result type. The inferred \ntypes in the basic system are also often general enough to avoid need of it. For example, without annotation, \nthe inferred principal type for iapp is .a\u00b51\u00b52. (\u00b51 I \u00b52) . (int . \u00b51 a) . \u00b52 a where all the given applications \nare accepted as is without need for .-expansion. Lifting Nevertheless, it is possible to make the type \nrules more robust under .-expansion, where we extend the basic system with a general lifting rule (TI-Lift) \ngiven in Figure 5 which allows arbitrary lifting of monadic expressions. For example, the id function \nin this system has the inferred type .a\u00b5. a . \u00b5a. Using this new type, all the applications iapp id, \niapp (.x.x), and iapp (.x.id x) are accepted. The good news is that extending the system with (TI-Lift) \nis benign: we can still do full type inference and constraint solving as shown in later sections. The \nbad news is that some inferred types become slightly more complicated. For example, the type for compose \n(given in Section 2.3) would be .a\u00df.\u00b51\u00b52\u00b53\u00b54\u00b5. (\u00b51 I \u00b5, \u00b52 I \u00b5) . (\u00df . \u00b52 .) . \u00b53 ((a . \u00b51 \u00df) . \u00b54 (a \n. \u00b5.)) Structural subtyping To ensure robustness under .-expansion while retaining simple types we could \nintroduce a structural subtyping rule. In particular, besides (TI-Lift) we could also introduce the rule: \nP | G f e : tP |= t I t ' (TI-Subsume) P | G f e : t ' Note that the subsumption constraint is between \ntypes instead of between monads. The rules for subsumption are: ' '' P |= t1 I t1 P |= t2 I tP |= m I \nm 2 (S-Fun) ' '' P |= t1 . mt2 I t1 . mt2 type Bot doesn t match the expected monad Beh. P |= t I t \n(S-Taut) P | G f* v : tP | G f e : mt G(x)= sP |= s ? t G(c)= sP |= s ? t (TS-Var) (TS-Const) P | \nG f* x : tP | G f* c : t P | G f* v : tP | G f e1 : m1 (t . m3 t ' ) P | G f e2 : m2 t .i. P |= mi I \nm (TS-Bot) (TS-App) ' P | G f v : Bot tP | G f e1 e2 : mt P | G,x:t1 f e : mt2 P ' | G f* v : t ' s \n= Gen(G,P ' . t ' ) P | G,x:s f e : mt (TS-Lam) (TS-Let) P | G f * .x.e : t1 . mt2 P | G f let x = v \nin e : mt e1 = vP | G f e1 : m1 t1 P | G,x:t1 f e2 : m2 t2 .i. P |= mi I m (TS-Do) P | G f let x= e1 \nin e2 : mt2 Figure 6. The syntax-directed type rules. The generalization function is de.ned as: Gen(G,s)= \n.(ftv(s) \\ ftv(G)).s . P | G f* v : t (TS-Lift) P | G f v : mt Figure 7. The syntax directed type rules \nextended with a lifting rule which replaces the rule (TS-Bot). These rules are structural over arrows \nusing the usual co/contra\u00advariant typing.3 With these rules we can give functions like id and iapp fairly \nsimple types: id : .a. a . Bot a iapp : .\u00b5. (int . \u00b5 int) . \u00b5 int And under the subsumption rules we \ncan show that this type for id, .a. a . Bot a, is as general as the type .a\u00b5. a . \u00b5a. Unfortunately, \nit turns out that it is exceedingly dif.cult to solve constraints between arbitrary types, as opposed \nto constraints between just monadic types. Since the subsumption rules give rise to constraints between \ntypes, we cannot give a coherent constraint solving strategy that is still complete we either need to \nreject certain reasonable programs or we need to solve such constraints too aggressively leading to an \nincomplete inference algorithm. Thus, our implementation uses the simple strategy since neither lifting \nnor subsumption provide a satisfactory improvement.  3.5 Syntax-directed type inference Figure 6 presents \na syntax directed version of the declarative type rules. The rules come in two .avors, one for value \nexpressions P | G f* v : t , and one for general expressions P | G f e : mt . Since each .avor has a \nunique rule for each syntactical expression, the shape of the derivation tree is uniquely determined \nby the expression syntax. Just like the Hindley-Milner syntax directed rules, all instantiations occur \nat variable and constant introduction, while generalization is only applied at let-bound value expressions. \nWe can show that the syntax directed rules are sound and com\u00adplete with respect to the declarative rules. \nTheorem 2 (The syntax directed rules are sound and complete). f * derivation P | G f v : t , and similarly, \nfor any P | G f e : mt we have P | G f e : mt . Completeness: For any derivation on a value expression \nP | G f Soundness: For any derivation P | G v : t there exists a v : s there exists a derivation P ' \n| G f * v : t, such that G f (P ' |t) ? (P | s). Similarly, for any derivation P | G f e : mt , 3 Note \nthat when a system has higher-kinds, we need to ensure that the arrow is not a .rst-class type constructor. \n there exists a derivation P ' | G f e : m ' t ', such that G f (P ' |m ' t ' ) ? (P | mt). Both directions \nare proved by induction on the derivations. Follow\u00ading Jones [14], we use an extension of the instance \nrelation in order to de.ne an ordering of polymorphic type schemes and monadic types under some constraint \nset. We can de.ne this formally as: s1 = ... p\u00af. tP2 f Gen(G, ... (P1,p\u00af) . t) ? s2 G f (P1 | s1) ? (P2 \n| s2) \u00af\u00b5 = ftv(m1,t1,P1) \\ ftv(G) \u00b5, \u00afa] a, \u00af. =[m/\u00aft/\u00afP2 |= .P1 P2 |= .m1 I m2 .t1 = t2 G f (P1 | m1 \nt1) ? (P2 | m2 t2) Besides extending the instance relation to monadic types, the def\u00adinition of this \nquali.ed instance relation allows us speci.cally to relate derivations in the declarative system that \ncan end in a type scheme s, to derivations in the syntax directed system that always end in a monotype. \nFinally, the syntax directed rules for the declarative type rules extended with the rule (TI-Lift) can \nbe obtained by replacing the rule (TS-Bot) with the rule (TS-Lift) given in Figure 7. This ex\u00adtended \nsystem is also sound and complete with respect to the ex\u00adtended declarative rules. 4. Principal types \nThe standard next step in the development would be to de.ne an algorithmic formulation of the system \n(including a rewriting to out\u00adput terms) and then prove that the algorithm is sound and complete with \nrespect to the syntactical rules, thereby establishing the prin\u00adcipal types property. Interestingly, \nwe can do this by translation. In particular, we can show that the syntactical rules in Figure 6 di\u00adrectly \ncorrespond to the syntactical rules of OML in the theory of quali.ed types [14]. In the next subsection \nwe prove that for every derivation on an expression e in our syntactical system, there ex\u00adists an equivalent \nderivation of an encoded term [e] in OML and the other way around. Since OML has a sound and complete \ntype reconstruction algorithm, we could choose to reuse that as is, and thereby get sound and complete \ntype inference (and as a conse\u00adquence there exist principal derivations). Unfortunately, the OML type \nreconstruction algorithm (essen\u00adtially the Haskell type class inference algorithm) is not satisfactory, \nas it would reject many useful programs. Intuitively, this is because it conservatively rejects solutions \nto constraints that are reasonable in light of the morphism laws; since it is unaware of these laws it \ncannot take advantage of them. The next section develops an algo\u00adrithm that takes advantage of the morphism \nlaws to be both permis\u00adsive and coherent. 4.1 Translation to OML The translation between our system \nand OML is possible since we use the same instance and generalization relation as in the theory of quali.ed \ntypes. Moreover, it is easy to verify that our entailment relation over morphism constraints satis.es \nall the requirements of the theory, namely monotonicity, transitivity, and closure under substitution. \nThe more dif.cult part is to .nd a direct encoding to OML terms. First, we are going to assume some primitive \nterms in OML that correspond to rules in our syntactical system: bot : .a. a . Bot a do : .a\u00df\u00b51\u00b52\u00b5. (\u00b51 \nI \u00b5, \u00b52 I \u00b5) . \u00b51 a . (a . \u00b52 \u00df) . \u00b5\u00df app : .a\u00df\u00b51\u00b52\u00b53\u00b5. (\u00b51 I \u00b5, \u00b52 I \u00b5, \u00b53 I \u00b5) . \u00b51 (a . \u00b53 \u00df) . \u00b52 \na . \u00b5\u00df Using these primitives, we can give a syntactic encoding from our expressions into OML terms: \nx] * = x c] * = c [ .x.e] * = .x.[e] v] = bot [v] * e1 e2] = app [e1][e2]let x=v in e] = let x= [v] \n* in [e][ let x=e1 in e2] = do [e1][.x.e2] * (with e1 = v) We can now state soundness and completeness \nof our syntactic system with respect to encoded terms in OML, where we write P | G fOML e : t for a derivation \nin the syntax directed inference system of OML (cf. Jones [14], Fig. 4). Theorem 3 (Elaboration to OML \nis sound and complete). f * P | G fOML [v] * : t in OML. Similarly, when P | G f e : mt we have P | G \nfOML [e] : mt. Completeness: If we can derive P | G fOML [v] * : t , there also ex\u00adists a derivation \nP | G f * v : t , and similarly, whenever P | G fOML Soundness: Whenever P | G v : t we can also derive \n[e] : mt , we also have P | G f e : mt. The proof of both properties can be done by straightforward induc\u00adtion \non terms. As a corollary, we can use the general type recon\u00adstruction algorithm W from the theory of \nquali.ed types which is shown sound and complete to the OML type rules. Furthermore, it means that our \nsystem is sound, and we can derive principal types. Corollary 4. The declarative and syntactic type rules \nadmit prin\u00adcipal types. Again, the same results hold for the extended type rules with the (TI-Lift) and \n(TS-Lift) rules. The only change needed is that the lifting primitive now needs to be polymorphic to \nre.ect the (TS-Lift) rule, i.e. bot : .a\u00b5. a . \u00b5a.  4.2 Ambiguous types Following Theorem 3, we could \nencode our type inference algo\u00adrithm using the type class facility of a language like Haskell, em\u00adploying \na morphism type class that provides morphisms between monads. In particular: class Morph m n where lift \n:: m a -> n a app :: Morph m1 m, Morph m2 m, Morph m3 m, Monad m =>m1(a-> m2 b) ->m3a-> mb appmfmx=liftmf \n>>=\\f -> lift mx >>= \\x -> lift (f x) ... Type checking could now be implemented using the syntactical \nencoding into a Haskell program and running the Haskell type checker. Unfortunately, this approach would \nnot be very satisfac\u00adtory: it turns out that our particular morphism constraints quickly lead to ambiguous \ntypes that cannot be solved by a generic sys\u00adtem. In particular, Haskell rejects any types that have \nvariables in the constraints that do not occur in the type (which we call free constraint variables). \nRecall our function iapp :(int . Beh int) . Beh int. The expression [iapp (.x. id (id x))] has the Haskell \ntype .\u00b5. (Morph \u00b5 Beh) . Beh int where the type variable \u00b5 only occurs in the constraint but not in the \nbody of the type. Any such type must be rejected in a system like Haskell. In general, there could exist \nmultiple solutions for such free constraint vari\u00adables where each solution gives rise to a different \nsemantics. A common example in Haskell is the program show [] with the type Show a . string. In this \nexample, choosing to resolve a as char results in the string , while any other choice results in []. \nWe were initially discouraged by this situation until we realized that focusing only on morphism constraints \nconfers an advantage: the monad morphism laws allow us to show that any solution for the free constraint \nvariables leads to semantically equivalent programs; i.e., the evidence translations for each solution \nare coherent. Moreover, there is an ef.cient and decidable algorithm for .nd\u00ading a particular least solution. \nAt a high-level our algorithm works by requiring that the set of monad constants and morphisms between \nthem form a semi-lattice, with Bot as the least element, where all morphisms satisfy the monad morphism \nlaws. Another requirement that is ful.lled by careful design of the type system is that the only morphism \nconstraints are between monadic type con\u00adstants or monadic type variables, and never between arbitrary \ntypes. We can repeatedly simplify a given constraint graph by eagerly sub\u00adstituting free constraint variables \n\u00b5 with the least upper bound of their lower bounds when these lower bounds are constants. This simple \nstrategy yields a linear-time decision procedure. The next section presents the algorithm in detail and \nproves coherence. 5. Constraint simpli.cation and coherence This section presents an algorithmic formulation \n(a variation on the Hindley-Milner algorithm W) of our syntax-directed type inference system. The previous \nsection established that while the type recon\u00adstruction algorithm of Jones can infer principal types, \nthese types are frequently ambiguous and hence programs with these types must be rejected. The contribution \nof this section is a simple (linear time) procedure that can eliminate some ambiguous variables in the \nconstraints of a type in a coherent way. By performing constraint simpli.cation, the types inferred by \nour algorithm are intentionally not the most general ones. However, simpli.cation allows strictly more \nprograms to be accepted. Moreover, we can show that sim\u00adpli.cation is justi.ed, in that the typability \nof the program is not adversely effected by the simpli.ed type. Section 5.1 discusses the key algorithmic \ntyping and rules and illustrates elaboration of source terms to System F target terms. Section 5.2 gives \nour constraint solving algorithm. Finally, Sec\u00adtion 5.3 shows, by appealing to the morphism laws, that \nour solv\u00ading algorithm is coherent and does not introduce ambiguity into the semantics of elaborated \nterms. 5.1 Algorithmic rewriting The structure of our algorithm W closely follows Jones algorithm for \nquali.ed types [14], and includes an elaboration into a calculus with .rst-class polymorphism. We formulate \nour algorithm in a stylized way to facilitate the proof of coherence. We think of the constraints generated \nby our system as forming a directed graph, with nodes corresponding to monad type constants and variables, \nany type t ::= mt | t constraint p ::= Do(m1,m2,m) | App(m1,m2,m3,m) substitution . ::= \u00b7| a . t | \u00b5 \n. m | .. ' target types t ::= . | t1 t2 |...t | t1 . t2 target terms e ::= x | c | .x:t.e | e1 e2 | .a.e \n| e [t] Figure 8. Syntax of constraint bundles and a target language e. and edges represented by the \nmorphism relation m I m '. However, instead of simply producing constraints of the form mIm ' as in the \nsyntax-directed system, our algorithm groups related constraints together in bundles . Constraint bundles \ncome in two .avors, corresponding to the fragments of the typing derivation (and hence bits of program \nsyntax) that induced the constraints. The bundles allow us to reason that edges in constraint graph come \nin speci.c kinds of pairs or triples, thus syntactically restricting the shape of the graph and facilitating \nour coherence proof. Figure 8 gives the syntax of target terms e and types t, and alters the syntax of \nconstraints p to constraint bundles. As we will see shortly, the bundles arise from corresponding inference \nrules from Figure 9: Do(m1,m2,m) is induced by the monadic let\u00adbinding rule (W-Do) and App(m1,m2,m3,m) \nby the (W-App) rule. Substitutions . map type variables to types, and .1.2 denotes substitution composition. \nFigure 9 shows the key rules in our algorithm W, expressed as a judgment P | G e : t; . r e where the \nconstraints P , f. type t, substitution ., and target term e are synthesized. As shown in Figure 8, t \nis either t or mt ; as in the syntax-directed rules, . denotes one of two modes, * and ; and the target \nterm e is explicitly typed. The substitution . applies to the free type-level variables (a and \u00b5) in \nG. An invariant of the rules is that .(t)= t, .(P )= P , and .(e)= e. For simplicity, we omit types on \nformal parameters and instantiation of type parameters in elaborated terms e. We also assume that a morphism \nfrom a monad m to m ' is named fm,m! ; and the bind and unit of a monad m are bindm and unitm. The omitted \nrules are unsurprising. Rule (W-Bot) corresponds to the syntactic rule (TS-Bot). It switches modes from \n to * in its premise, produces the monadic type Bot t , and elaborates the term by inserting the unit \nfor Bot. Rule (W-App) elaborates each sub-term in its .rst two premises, and in the fourth and .fth premises, \ncomputes the most-general uni.er .3 of the formal parameter type of e1 and the value type of e2. We generate \nan App-constraint bundle which indicates that there is a morphism from each of .3.2\u00b51, .3.1\u00b52, and .3\u00b5 \n' to the result monad \u00b5. In the elaborated terms, f\u00b5i,\u00b5 stand for morphisms that will be abstracted (or \nsolved) at the nearest enclosing let; similarly the bind\u00b5 are the binds of the result monad. The rule \nfor monadic let-bindings, (W-Do), is nearly identical to (W-App) except that there is one fewer monad \nvariable. Finally, rule (W-Let) implements generalization. We rewrite the let-bound value v in the .rst \npremise, and compute the variables .\u00afover which we can soundly generalize. In the third premise, we compute \nthe variables \u00b5\u00afthat appear in the constraints P1 but are not free in the type t these variables are \ncandidates for constraint solve(\u00af ' simpli.cation. The judgment P1 -. \u00b5) P1; . ' simpli.es con\u00adstraints, \neliminating the ambiguous type variables \u00b5\u00afcoherently this judgment is discussed in the next subsection. \nThe last premise rewrites the body in a context in which x s type is generalized. In the conclusion, \nwe translate to an explicitly typed application form, where the let-bound value is elaborated to generalize \nover both its constraints and the type variables .\u00af.  5.2 Soundness and ef.ciency of constraint simpli.cation \nIntuitively, our algorithm views a constraint set P as a directed graph, where the nodes in the graph \nare the monad types, and the edges are introduced by the constraint bundles. For example, we view a bundle \nDo(m1,m2,m) as a graph with vertices for m1,m2 and m, and edges from m1 to m and m2 to m. In the discussion \nbelow, we informally use intuitions from this graphical view of P . For each edge between m and m ' in \nthe constraint graph, a solution to P must compute a speci.c morphism between m and m ' . We start our \ndescription of the algorithm with the de.nition of morphism-induced least-upper bounds. This de.nition \nis relative to an initial set of constraints P0 that de.ne the monad constants and primitive morphisms \nused to type a source program. De.nition 5 (Least-upper bound). With respect to an initial context P0, \ngiven a set of monad constants A = {M1,...,Mn}, we write lub(A)= M to mean that M is the least upper \nbound of the monad constants in A, i.e., .i.P0 |= Mi I M; and for any M ' such that .i.P0 |= Mi I M ', \nwe have P0 f M I M '. Although de.ned with regard to a particular initial context P0, we write lub(A) \nfor conciseness, leaving P0 implicit. Our constraint simpli.cation algorithm is straightforward. We limit \nour attention to cycle-free constraint graphs. Whenever a cycle is detected in the constraint graph, \nwe require every variable and constant in the cycle to be identical a constraint graph with a cycle containing \nmore than one constant cannot be solved and the program is rejected. Given a cycle-free graph we perform \na topological sort and then proceed to simplify the graph starting from the leaves. We con\u00adsider a variable \n\u00b5 only after all its children have been considered. All variables have lower bounds (in-edges), since \nvariables are in\u00adtroduced by (W-Do) and (W-App) and have lower bounds by con\u00adstruction. Besides, the \nnode corresponding to Bot has an out-edge to every other node. For each variable \u00b5 considered, if all \nits in\u00adedges are from monad constants A = {M1,...,Mn}, and if \u00b5 has some out-edge (needed for coherence, \nand discussed in the next sub-section), we assign to \u00b5 the constant lub(A), thus eliminating the variable \nand proceeding to consider the next variable, if any. Figure 10 presents a set of inference rules that \ncodi.es this solv\u00ading algorithm (omitting the cycle elimination phase, for simplic\u00ad solve(\u00af ' ity). \nThe judgment has the form P -. \u00b5)P ; .. It considers the free constraint variables \u00b5\u00afin P , replacing \nthem with monad con\u00adstants under certain conditions, returning the residual constraints P ' that cannot \nbe simpli.ed further. This judgment ensures that dom(.) . \u00b5\u00afand .P ' = P '. Thus, in the (W-Let) rule \nwe apply . ' to the body of e1 in the conclusion, in effect resolving any free morphism f\u00b5,\u00b5! to the \nspeci.c morphism determined by . '. Notice that since dom(.) . \u00b5\u00af, the premises of (W-Let) ensure that \nwe eliminate only those variables appearing in neither the .nal type nor the context. The inference rules \nmake use of a few auxiliary functions, de.ned to the right of Figure 10. First, for a constraint bundle \np, function up-bnd(p) is the type of the resulting monad. In contrast, lo-bnd(p) is the set of types \nin a constraint bundle from which we require morphisms. Both of these are lifted to sets of constraints \nin the natural way. We also de.ne .owsTo\u00b5 P , the set of constraints in P that have \u00b5 as an upper bound; \n.owsFrom\u00b5 P , the set of constraints that have \u00b5 as a lower bound. We now explain the rules in detail. \nRule (S-\u00b5) is the workhorse of the algorithm. In the .rst two premises, it selects some constraint p \nwhose upper bound \u00b5 is in the list of variables to be solved, \u00b5\u00af. The third premise checks that \u00b5 has \nan upper bound; i.e., it is the lower bound of at least one constraint in P (for coherence). The fourth \npremise de.nes A, the set of all of \u00b5 s lower bounds, and the P | G f * v : t; . r e P | G f. e : t; \n. r e (W-Bot) P, p | G f v : Bot t ; . r (unitBot e) P1 | G f e1 : \u00b51 t1; .1 r e1 P2 | G f e2 : \u00b52 t2; \n.2 r e2 \u00b5, \u00b5 ' , a, \u00df fresh .2t1 = .3(a . \u00b5 ' \u00df) .1t2 = .3aP =(.3.2P1), (.3.1P2), App(.3.2\u00b51,.3.1\u00b52,.3\u00b5 \n' ,\u00b5) . = .1.2.3 (W-App) P | G f e1 e2 : .3(\u00b5\u00df); . r .(bind\u00b5 (f\u00b51,\u00b5 e1)(.x: .bind\u00b5 (f\u00b52,\u00b5e2) .y: .(f\u00b5!,\u00b5(xy)))) \ne1 = vP1 | G f e1 : \u00b51 t1; .1 r e1 P2 | G,x:t1 f e2 : \u00b52 t2; .2 r e2 \u00b5, a, \u00df fresh .2t1 = .3(a) .1t2 \n= .3(\u00df) P =(.3.2P1), (.3.1P2), Do(.3.2\u00b51,.3.1\u00b52,\u00b5) . = .1.2.3 (W-Do) P | G f let x= e1 in e2 : .3(\u00b5\u00df); \n. r .(bind\u00b5 (f\u00b51,\u00b5e1) .x:a.(f\u00b52,\u00b5e2)) P1 | G f * v : t ; .1 r e1 .\u00af= ftv(P1 . t ) \\ ftv(G) \u00b5\u00af=(ftv(P1) \n\\ ftv(t )) n .\u00af solve(\u00af '' P1 -. \u00b5)P1; . ' ..P P2 | G,x:s f e : mt; .2 r e2 . = .1.2s = .\u00af1 . t (W-Let) \n P2 | G f let x = v in e : .1(mt ); . r .((.x: .e2) .\u00af..abstractConstraints(P1' ,. ' e1)) abstractConstraints((p, \nP ), e)= abstractConstraints(p, abstractConstraints(P, e)) abstractConstraints(Do(m1,m2,m), e)= .bindm: \n..fm1,m: ..fm2,m: .e where abstractConstraints(App(m1,m2,m3,m), e)= .bindm: ..fm1,m: ..fm2,m: ..fm3,m: \n.e abstractConstraints(Lift(m), e)= .unitm: .e Figure 9. Selected algorithmic rules for elaboration into \nSystem F (with types in elaborated terms omitted for readability). solve(\u00af\u00b5) solve(\u00af p, P -. P ' ; .P2,P1 \n-. \u00b5) P ' ; . where up-bnd(Lift(m)) = m solve(\u00af\u00b5)\u00b5) \u00b5) solve(\u00af solve(\u00af P -. P ; \u00b7 p, p, P -. P ' ; .P1,P2 \n-. P ' ; . up-bnd(App(\u00b7, \u00b7, \u00b7,m)) = m up-bnd(Do(\u00b7, \u00b7,m)) = m  solve(\u00af' up-bnds(P )= {up-bnd(p)} p.P \nP0 |= lub(lo-bnds(p)) I up-bnd(p) P -. \u00b5)P ; . S-M solve(\u00af lo-bnd(Lift(m)) = Bot \u00b5) ' p, P -. P ; . \nlo-bnd(App(m1,m2,m3, \u00b7)) = {m1,m2,m3}lo-bnd(Do(m1,m2, \u00b7)) = {m1,m2} ' up-bnd(p)= \u00b5\u00b5\u00af= \u00b5, \u00b5\u00af.owsFrom\u00b5 \nP = {} lo-bnds(P )= lo-bnd(p) p.P solve(\u00af \u00b5 !) A = lo-bnds(p, .owsTo\u00b5 P ) . =(\u00b5 . lub(A)) .P -. P ' \n; . ' .owsTo\u00b5 P = {p | p . P . \u00b5 = up-bnd(p)} S-\u00b5 solve(\u00af\u00b5) .owsFrom\u00b5 P = {p | p . P . \u00b5 . lo-bnd(p)} \np, P -. P ' ; .. ' solve(\u00af Figure 10. P -. \u00b5) P ' ; .: Simplifying constraints using the lub-strategy. \n.fth premise de.nes the substitution . as mapping \u00b5 to the least upper bound of A. Recall again that \nlubis only de.ned on constant monads, so A must contain no variables \u00b5 '; this requirement es\u00adsentially \nforces solving to proceed bottom up, with the leaves of the tree induced by P (following cycle elimination) \nsolved .rst. Fi\u00adnally, the sixth premise applies the substitution to the constraints P and proceeds to \nsolve them; the .nal substitution consists of the substitution . ' produced by this recursive step, composed \nwith .. Rule (S-M) checks that constraints involving only constants (e.g., those whose \u00b5 have been completely \nsubstituted for) are con\u00adsistent with the initial constraint set P0, in which case they can be dropped. \nFinally, the .rst three rules help with bookkeeping, indi\u00adcating that (1) constraints may remain unsimpli.ed; \n(2) duplicate constraints may be dropped; and (3) constraints may be permuted. This last rule is non-deterministic, \nbut it can be implemented easily using simple topological sort of a cycle-free constraint constraint \ngraph. The next de.nition and the following lemma establish that solve(\u00af ' P -. \u00b5) P ; . only produces \nsound solutions to a constraint set. The proof is straightforward. De.nition 6 (Sound solution). Given \nan initial context P0 and a constraint set P , a solution . to the constraints P is sound if and only \nif, for each \u00b5 in dom(.), we have {M1,...,Mn} = lo-bnds(.(.owsTo\u00b5 P )), and .i.P0 |= Mi I .\u00b5. Lemma \n7 (Constraint simpli.cation is sound). For all P0, P, \u00b5\u00af, solve(\u00af\u00b5) P ' ,., if we have P0 f P -. P ' \n; ., then . is sound for P . Theorem 8 (Constraint solving is linear time). Given a constraint set P \nand an initial context P0, there exists a O(|P |) algorithm to decide whether or not P is fully solvable, \nwhere a constraint set P solve(\u00af\u00b5) is fully solvable iff \u00b5\u00af= ftv(P ) implies P0 f P -. \u00b7; .. Proof. \n(sketch) The algorithm begins by detecting and eliminating cycles in the constraint graph. Doing so is \nlinear in number of ver\u00adtices and edges of the graph, i.e., O(|V | + |E|), where |V | and |E|are each \nat most three times the number of App(m1,m2,m3,m) constraints plus twice the number of Do(m1,m2,m) constraints. \nThen the algorithm sorts the graph topologically (also linear). Then it attempts to eliminate each constraint \nvariable in sorted order. Eliminating a single constraint variable takes constant time. This is because \nelimination amounts to the cost of computing the lubfor elements of a .nite lattice in P0 and all lubs \ncan be pre\u00adcomputed on P0 without dependence on P . After eliminating all eligible constraint variables \nwe are left with either a graph that has variables without upper bounds (in which case we answer no ) \nor we have a graph with only con\u00adstants. Checking for upper bounds in a variable-free graph is again \nlinear in the number of constraints, since each ordering can be an\u00adswered in constant time (again, pre-computed \non P0). One would also like to show that our constraint solving algo\u00adrithm does not solve constraints \ntoo aggressively. We de.ne an im\u00adprovement relation on type schemes (following the terminology of Jones \n[16]), as a lifting of the solving algorithm. De.nition 9 (Improvement of type schemes). Given a type \nscheme s = ..P1 . t and a set of type variables \u00af=(ftv(P1) \\ .\u00af\u00b5 solve(\u00af\u00b5) ftv(t )) n .\u00af. If P1 -. P2; \n. then we say s ' ..P2 . t is an = .\u00af improvement of s. We might conjecture at .rst that improvement \nof types is con\u00adsistent with the type instantiation relation. That is, if s ' is an im\u00adprovement of s, \nthen P0 f s ' = s and P0 f s = s '. How\u00adever, by eliminating free constraint variables, our solving algorithm \nintentionally makes s ' less general than s. For example, given s = .\u00b5.(M1 I \u00b5, \u00b5 I M2) . t , (where \n\u00b5 . ftv(t)) our al\u00adgorithm could improve this to s ' = M1 I M2 . t , and indeed further to t, if P0 |= \nM1 I M2. However, for an arbitrary constant \u00b5, it is not that case that P0 |= M1 I \u00b5, \u00b5 I M2, which is \nwhat is demanded by the instantiation relation. Nevertheless, the type improvement scheme is still useful \nsince improvement at generalization points does not impact the typability of the remainder of the program. \nTheorem 10 (Improvement is justi.ed). For all P, G, x, s, s ' , e, m, t , if we have P | G,x:s f e : \nmt , and s ' is an improvement of s, then P | G,x:s ' f e : mt . Intuitively, we can see this theorem \nholds because the improvement of a type s = ... P\u00af1 . t to ...P\u00af2 . t only effects the free constraint \nvariables: the actual type t is unchanged and if P |= P1 we always have P |= P2 too. At any instantiation \nof s, we can always substitute the improved type since the type t is the same and the improved constraints \nP2 are also entailed if the original constraints P1 were.  5.3 Coherence The effectiveness of our constraint-solving \nstrategy stems from our ability to eagerly substitute constraint variable \u00b5 with the least upper bound \nM of all the types that .ow to it. Such a technique is not admissible in a setting with general purpose \nquali.ed type constraints, particularly when the evidence for constraints (in this case our morphisms, \nbinds and units) has operational meaning. One may worry that by instantiating \u00b5 with some M ' = M where \nM IM ', we may get an acceptable solution to the constraint graph but the meaning of the elaborated programs \ndiffers in each case. This section shows that when the monad morphisms satisfy the morphism laws our \nconstraint improvement strategy is coherent, i.e., all admissible solutions to the constraints yield \nelaborations with the same semantics. So, any speci.c solution (including the one produced by the lub-strategy) \ncan safely be chosen. Our approach to showing coherence proceeds as follows: solve(\u00af 1. Given a constraint \nset P and a derivation P0 f P -. \u00b5) \u00b7; ., we call . the lub-solution to P . 2. We can see all other \nsolutions to P as being derived from the lub-solution by repeated local modi.cations to the lub-solution. \nA local modi.cation involves picking a single variable \u00b5 such that . = . ' (\u00b5 . M) and considering a \nsolution to the constraint set . ' P that assigns some other solution M ' = M  to \u00b5; i.e., we have some \nsolution .1 = . ' (\u00b5 . M ' ). We can iterate this process, generating the solution .i+1 from .i in this \nmanner. 3. We enumerate the ways in which .iP can differ from .i+1P , considering interactions between \npairs of constraint bundles (App/App, Do/Do, Do/App, App/Do, etc.). In each case, since each kind of \nconstraint bundle can be related to the ab\u00adstract syntax of elaborated programs, we can reason about \nthe differences in semantics that might arise from the .i and the .i+1 solutions. We show that when all \nthe morphisms satisfy the morphism laws, that the solutions are indeed equivalent. Our result applies \nonly to well-formed contexts, a notion de\u00ad.ned below. In the de.nition, requirements (1) and (2) ensure \nthat the monads and their morphisms are well-typed. Requirement (3) ensures that least-upper bounds are \nde.ned and that there is a Bot ~ monad. Our notion of term equivalence, written e1 = e2, is exten\u00adsional \nequality on well-typed, elaborated terms. Clauses (4) and (5) state that this equivalence is axiomatized \nby the transitivity prop\u00aderty and the morphism laws. De.nition 11 (Well-formedness of a context). The \nfollowing con\u00additions are required of well-formed contexts, P0, G: 1. For any pair of monad constants \nM we have bindM and unitM bound as constants in G, with appropriate types. 2. For all M1,M2, if P0 |= \nM1 I M2 then G contains a constant fM1,M2 bound at the type .a.M1 a . M2 a. 3. For any set of monad \nconstants A, there exists M such that lub(A)= M and Bot is a monad constant in G with P0 |= Bot I M, \nfor all M. 4. We assume that for all M1,M2,M3, if P0 |= M1 I M2 and  ~ P0 |= M2 I M3, then fM2,M3 . \nfM1,M2 = fM1,M3 . 5. We assume that for all M1,M2, e1, e2, t1, t2, such that P0 |= M1 I M2 and e1 : \nM1 t1 and e2 : M2 t2, we have fM1,M2 (bindM1 .x:t1.e2) ~bindM2 (fM1,M2 e1) .x:t1. e1 = (fM1,M2 e2)  \nThe following lemma establishes that in well-formed contexts, our algorithm produces well-typed System \nF terms. The proof is a straightforward induction on the structure of the derivation, where by {[G]} \nwe mean the translation of a source typing context to a System F context. Lemma 12 (Well-typed elaborations). \nGiven G such that P0, G is well-formed, e, t, ., e,., such that P | G f. e : t; . r e. Then there exists \nt such that {[.G]}fF abstractConstraints(P, e): t. Next, we formalize the notion of a local modi.cation \n. ' of a valid solution . to constraint set. Condition (1) identi.es the vari\u00adable \u00b5 which is the locus \nof the modi.cation. Conditions (2) and (3) establish the range of admissible solutions to \u00b5, and condition \n (4) asserts that the modi.ed solution . ' picks a solution for \u00b5 that is different than ., but still \nadmissible.  De.nition 13 (Local modi.cation of a solution). Given a solution .1 to a constraint set \n(p, P ), a local modi.cation to .1 is a solution .2 for which the following conditions are true: 1. There \nexists a variable \u00b5 and a constant M such that \u00b5 = up-bnd(p) and .1 = .1' (\u00b5 . M ). 2. There exists \nMlo = lub(.1' (lo-bnds(.owsTo\u00b5 P )))). Mlo is a lower-bound for \u00b5. 3. There exists {M1 hi,...,Mnhi} \n= .1' (up-bnds(.owsFrom\u00b5 P )). Each Mihi is an upper bound for \u00b5. 4. There exists a monad constant M \n' = M such that P0 f Mlo I  '' I Mhi ' M and .i.P0 f M i , such that .2 = .1' (\u00b5 . M )  Finally, we \nstate and sketch a representative case of the main result of this section: namely, that the lub-strategy \nis coherent when the morphisms form a semi-lattice and satisfy the morphism laws. Theorem 14 (Coherence \nof constraint solving). Given P0, G, P, e, t, ., .1,.2, e, ., \u00b5\u00af, such that 1. P0, G is well-formed and \nP is cycle-free. 2. For all \u00b5 . \u00b5\u00af, the set .owsFrom\u00b5 P is non-empty, i.e., \u00b5 has an upper bound. 3. \nP | G f. e : t; . r e. 4. There exists .1 such that dom(.1) . \u00b5\u00afand .1 is a sound solution for P . \n5. There exists .2, a local modi.cation of .1.  ~ Then, .1e = .2e. Proof. (Sketch) Since .2 is a local \nmodi.cation, we have (from condition (1) of De.nition 13) .1 = .1' (\u00b5 . M), for some \u00b5, M, . 1' , and \nP = p, P ', where \u00b5 = up-bnd(p) is the modi.ed variable. We proceed by cases on the shape of p. Case \np is an App bundle: We have .1' p = App(M1 lo,M2 lo,M3 lo ,\u00b5) (since from condition (2) of De.nition \n13, lub is only de.ned on monad constant). To identify the upper bounds of \u00b5, we con\u00adsider the constraints \nin .1' (.owsFrom\u00b5 P ' ), note that all the upper bounds must be constants (from condition (3)), and proceed \nby cases on the shape of each of the constraints p ' in this set. Sub-case p ' is an App bundle: Without \nloss of generality on the speci.c position of \u00b5, we have . ' p ' = App(m1, \u00b5, m3,Mhi), where Mhi is an \nupper-bound of p '. From the shape of the con\u00adstraints, we reason that we have a source term of the form \ne (e1 e2), that is elaborated to the term shown below, where e, e1, e2 are the elaboration of the sub-terms. \n1. bindMhi (fm1,Mhi e)(.x: .bindMhi 2. (f\u00b5,Mhi (bind\u00b5 (fMlo,\u00b5 e1)(.x1: . 3. bind\u00b5 (fMlo e2)(.x2: .(fMlo \n(x1 x2))))))  1 ,\u00b5 ,\u00b5 23 4. (.y: .fm3,Mhi (xy))) Under the solutions .1 and .2, the inner subterm at \nlines 2 and 3 (call it e) may differ syntactically, i.e., .1 e = .2 e. Speci.cally, the solution .1 chooses \n\u00b5 . M while .2 may choose \u00b5 . M ' , for M = M '. However, using two applications of the morphism laws, \n(condition (5) of De.nition 11), we can show that the e is extensionally equivalent to the term shown \nbelow. 2. bindMhi e1)(.x1: . (f\u00b5,Mhi . fMlo,\u00b5 1 3. bindMhi (f\u00b5,Mhi . fMlo,\u00b5 e2)(.x2: . 2 (f\u00b5,Mhi . fMlo,\u00b5(x1 \nx2)))) 3 Appealing to condition (4) of De.nition 11, the transitivity prop\u00aderty, we get that the term \nabove is extensionally equivalent to the term below (call it e '). 2. bindMhi (fMlo,Mhi e1)(.x1: . 1 \n3. bindMhi (fMlo,Mhi e2)(.x2: .(fMlo,Mhi (x1 x2)))) 2 3 We have e ~ e and hence .1 e ~e '. Since, \u00b5 . \nFV ( e ' ), we = ' = .1 ' ~' ~ have .1 e = .2 e ', and .2 e = .2 e, thus establishing .1 e = .2 e, as \nrequired.  5.4 Ambiguity and limitations of constraint solving Our constraint solving procedure is effective \nin resolving many common cases of free constraint variables in types that would oth\u00aderwise be rejected \nas ambiguous by Haskell. However, a limitation of our algorithm is that, for coherence of solving, we \nrequire free constraint variables to have some upper bound in the constraint set. (See condition (2) \nof Theorem 14.) A variable with no upper bound may admit several possible solutions, so the morphisms \nleading to these solutions differ and result in different program rewritings our algorithm rejects such \na program as ambiguous. We argue that for typical programs our constraint solving strat\u00adegy is still \neffective. Our experience shows that terms with an un\u00adbounded constraint variable either consist of dead \ncomputations that are never executed, or constitute top-level expressions. In the next paragraph we discuss \na particular example program with an ambiguous type, illustrating the former case. To deal with the latter \ncase, top-level expressions should have type annotations. All the examples in this paper are deemed unambiguous \nby our algorithm provided a top-level annotation. Out of the 5 example programs, 3 programs have types \nwith variables that do not appear in the .nal type. All of these types would be rejected by OML (or Haskell) \nas ambiguous but are accepted by our system since they all have con\u00adstraint variables with several distinct \nlower bounds and an upper bound, so we can instantiate them with the lub of the lower bounds. Consider \nthe following example, with a state monad ST and a primitive function read: int . ST char: let g= fun()-> \nletf =funx->funy-> let z=read xinreadyin letw =f0in () Here, the type inferred for f is .\u00b5. (ST I \u00b5) \n. int . Bot int . \u00b5 char. Because of the partial application f0, we must give g the type .\u00b5, \u00b5 ' . (ST \nI \u00b5, Bot I \u00b5 ' ) . unit . \u00b5 ' unit. Here, the constraint variable \u00b5 resulting from the partial application \nof f does not appear in the return type, while it appears in the constraints without an upper bound. \nPicking an arbitrary solution for \u00b5, say \u00b5 = ST or \u00b5 = IO, where P0 |= ST I IO, causes the sub-term w \nto be given differ\u00adent types. This is a source of incoherence, since our extensional equality property \nis only de.ned on terms of the same type. How\u00adever, pragmatically, the speci.c type chosen for w has \nno impact on the reduction of the program, and we conjecture that in all cases when this occurs, the \nunbounded constraint variable has no in.u\u00adence on the semantics of the program. As such, our implementation \nsupports a permissive mode, so that despite it technically being ambiguous, we can accept the program \ng, and improve its type to '' ' .\u00b5.Bot I \u00b5, . unit . \u00b5 unit, by solving \u00b5 = ST. 6. Implementation and \napplications We have implemented our inference algorithm for the core lan\u00adguage of Figure 1 extended \nwith standard features, including con\u00additionals and recursive functions. Our implementation is written \nin Objective Caml (v3.12) and is about 2000 lines of code. It follows our basic morphism insertion strategy \n(i.e., Figure 4), but also pro\u00advides an alternative typing mode that uses the (TI-Lift) rule (Fig\u00adure \n5). All rewritings shown in this paper, modulo minor readabil\u00adity improvements, were produced with our \nimplementation and run against matching monadic libraries. In this section we present programs using \ntwo additional mon\u00adads, to give further examples of the usefulness of our system: pars\u00ading and information \n.ow tracking. For the latter, our technical re\u00adport [27] further considers a source language extended \nwith muta\u00adble references, which for tracking information .ow requires param\u00adeterized monads. We can type \nrewritten programs using the Flow-Caml security type system [24] and thereby prove they are secure. 6.1 \nParsing example A parser can be seen as a function taking an input string and returning its unconsumed \nremainder along with a result of type a. We can apply this idea directly by implementing a parser as \na monad whose type Par a conveniently hides the input and output strings. Its bind and unit combinators \nhave names bindp and unitp, respectively. The token: char . Par unit parser parses a particular character, \nwhile choice: (unit . Par a) . (unit . Par a) . Par a returns the result of the .rst (thunki.ed) parser \nif it is successful, and otherwise the result of the second parser. As an example we shall write a parser \nthat computes the maxi\u00admum level of nested brackets in an input string: (rec nesting. fun ()-> let nonempty \n= fun ()-> token [ ; let n = nesting() in token ] ; let m = nesting() in max (n+ 1)min letempty=fun() \n->0 in choice (fun ()-> nonempty()) (fun ()-> empty()))() Interpreted as standard ML code, the above \nprogram is not type correct: the functions max and + are typed as int . int . int, which does not match \nwith the type of n and m of type Par int. In our system the example is well-typed where the term gets \ntype Par int. The type directed translation automatically inserts the binds for sequencing and units \nto lift the .nal result into the parser monad. The actual translation produced by our implementa\u00adtion \nis: (rec nesting. fun () -> let nonempty = (fun ()-> bindp (token [ ) (fun _ -> bindp (nesting()) (fun \nn -> bindp (token ] ) (fun _ -> bindp (nesting()) (fun m -> unitp (max (n + 1) m)))))) in letempty=fun() \n->0 in choice (fun () -> nonempty()) (fun () -> unitp (empty()))) ()  6.2 Information .ow We are interested \nin enforcing a con.dentiality property by track\u00ading information .ow. Data may be labeled with a security \nlevel, and the target independence property, called noninterference [6, 12], ensures that low-security \noutputs do not depend on high-security inputs. Ever since Abadi et al. showed how to encode information \n.ow tracking in a dependency calculus [1], a number of monadic encodings have been proposed [4, 7, 19, \n26]. We focus on a variant of the Sec monad [26] that wraps data protected at some security level for \na pure functional subset of ML. In the absence of side ef\u00adfects, we only have to ensure that data with \na certain con.dentiality level is not disclosed to lower-level adversaries (explicit .ows). Let us consider \na simple security lattice {. = L = H = T}. The information .ow monad SecH (resp., SecL) tracks data with \ncon.dentiality level H (resp., L) with monadic operators bindh,unith (resp., bindl,unitl). The may-.ow \nrelation is ex\u00adpressed via a morphism that permits public data at a protected level: labup : SecL I SecH \nThus data labeled L may be used in a context expecting data labeled H , but not vice versa. The following \nsmall example computes the interest due for a savings account, and the date of the last payment. Primitive \nsavings returns a secret, having type unit . SecH .oat, rate returns a public input having type unit \n. SecL .oat. add interest is a pure function computing the new amount of the account after adding interest, \nhaving type .oat . .oat . .oat. Finally, current date returns the current date, having type unit . int. \n add_interest (savings ()) (rate()) The rewriting lifts the low security rate to compute the high secrecy \nupdate for savings. The .nal type of the entire expression is SecH .oat. bindh (savings ()) (fun y -> \nbindh (labup (rate ())) (fun z -> unith (add_interest y z)))} Our extended technical report [27] gives \na proof of soundness with respect to FlowCaml for an information .ow state monad which subsumes the Sec* \nmonads; therefore they also soundly encode non-interference. 7. Related work Our work builds on Jones \ntheory of quali.ed types [14 16], which ensures principal types and coherence of the type inference for \nOML and is used to infer Haskell type classes. We adapt this approach for a practical monadic setting. \nThe key difference is that we make the solving procedure aware of morphism laws, in such a way that Jones \nrestriction on ambiguous types can be removed. Filinski previously showed that any individual monadic \neffect can be synthesized from .rst-class (delimited) continuations and a storage cell [11], and thereby \ncan be expressed in direct style with\u00adout explicit use of bind and unit. Kiselyov and Shan [18] apply \nthis representation to implement probabilistic programs as an ex\u00adtension to Objective Caml. While our \nsystem shares the same goals as these, it uses a different mechanism type-directed rewriting to insert \nmonadic operators directly, rather than requiring them to be implemented in terms of continuations. Filinski \nalso showed how to implement monads in a compos\u00adable way [9]: given implementations of individual monads, \nand an order in which they can be layered on top of each other, he gives a semantics to their compositions. \nHowever, Filinski s representa\u00adtion elides monadic types from terms, complicating program under\u00adstanding. \nFor example, the type of seconds in his system would be unit . int, not unit . Beh int. Our approach \nfully integrates monadic types with ML type inference and yields well-typed ML programs, therefore it \nis hopefully easy to understand by the pro\u00adgrammers. Our approach can also be seen as orthogonal, since \nwe leave the implementation of monads to the programmer, treating all monad operators as black boxes. \nThe lattice induced by our mor\u00adphism declarations corresponds to Filinski s layering structure be\u00adtween \nmonads. In his latest work on this topic [10], Filinski proposes an opera\u00adtional semantics for composing \nmonads; he re.ects monads in the types, as effects, and provides runtime guarantees for well-typed programs, \nby dynamically inserting the minimal number of binds and units, based on syntactic hints. Orthogonally, \nwe perform type inference from unannotated source code. We make the monadic types, operators, and morphisms \nexplicit in the rewritten program, which gives the programmer the option to review and assess the resulting \nprogram. Last but not least our approach supports poly\u00admorphism over monads, permitting us to abstract \nand generalize monads and morphisms. For example, the rewritten compose func\u00adtion, whose type is given \nin Section 2.3, would additionally take as arguments the monads and morphisms used by its body, akin \nto Haskell s dictionary-passing interpretation of type classes. We can view our rewriting algorithm as \na particular case of a more general strategy for type-directed coercion insertion, which supports automatic \ncoercion of data from one type to another, with\u00adout explicit intervention by the programmer. Most related \nto our ap\u00adproach is that of Luo [20, 21], which considers coercion insertion as part of polymorphic type \ninference. In Luo s system rewritings may be ambiguous: when more than one is possible, each may have \ndif\u00adferent semantics. Also, the system does not include quali.ed types, so coercions may not be abstracted \nand generalized, hurting expres\u00adsiveness. Our own prior work addressed the problem with ambigu\u00adity by \ncarefully limiting the form and position of coercions [28]. However, we could not scale this approach \nto a setting with poly\u00admorphic type inference, as even the simplest combinations of co\u00adercions admitted \n(syntactic) ambiguity. Our restriction to monads in the present work addresses this issue: we can prove \ncoherence by relying on the syntactic structure of the program to unambigu\u00adously identify where combinators \nshould be inserted, and when the choice of combinators is unconstrained, the morphism laws allow us to \nprove that all choices are equivalent. Benton and Kennedy developed MIL, the monadic intermediate language, \nas the basis of optimizations in their MLj compiler [2]. They observe, as we do, that ML terms can be \nviewed as having the structure of our types t (Figure 1) where monads do not appear to the left of arrows. \nWhile our approach performs inference and translation together, their approach suggests an alternative: \nconvert the source ML program into monadic form and then infer the binds, units, and morphisms. We know \nfrom our translation to Haskell that this approach can only work by informing the solver of monad morphisms. \n8. Conclusions Monads are a powerful idiom in that many useful programming disciplines can be encoded \nas monadic libraries. ML programs enjoy an inherent monadic structure, but the monad in question is hardwired \nto be the I/O monad. We set out to provide a way to exploit this structure so that ML programmers can \nprogram against monads of their choosing in a lightweight style. The solution offered by this paper is \na new way to infer monadic types for ML source programs and to elaborate these programs in a style that \nincludes explicit calls into monadic libraries of the programmer s choice. A key consideration of our \napproach is to provide programmers with a way to reason about the semantics of elaborated programs. We \nachieve this in two ways. First, the types we infer are informative in that they explicitly indicate \nthe monads involved. And, second, when our system accepts a program, we show that all possible elaborations \nof a program have the same meaning, i.e., our elaborations are coherent. We implement our system in a \nprototype compiler, and evaluate it on a variety of domains. We .nd our system to be relatively simple, \nboth to implement and to understand and use, and powerful in that it handles many applications of interest. \nAcknowledgements The authors would like to thank Gavin Bier\u00adman and Matt McCutchen for their early contributions \nto this work, and to Gavin for comments on this draft. Hicks and Guts were both supported by NSF grant \nCNS-0905419. References [1] M. Abadi, A. Banerjee, N. Heintze, and J.G. Riecke. A core calculus of dependency. \nIn POPL, volume 26, pages 147 160, 1999. [2] Nick Benton and Andrew Kennedy. Monads, effects and transforma\u00adtions. \nIn Electronic Notes in Theoretical Computer Science, 1999. [3] Greg Cooper and Shriram Krishnamurthi. \nEmbedding dynamic data.ow in a call-by-value language. In ESOP, 2006. [4] K. Crary, A. Kliger, and F. \nPfenning. A monadic analysis of informa\u00adtion .ow security with mutable state. Journal of functional program\u00adming, \n15(02):249 291, 2005. [5] L. Damas and R. Milner. Principal type-schemes for functional pro\u00adgrams. In \nPOPL, pages 207 212, 1982. [6] D.E. Denning. A lattice model of secure information .ow. Communi\u00adcations \nof the ACM, 19(5):236 243, 1976. [7] D. Devriese and F. Piessens. Information .ow enforcement in monadic \nlibraries. In TLDI, pages 59 72, 2011. [8] Conal Elliott and Paul Hudak. Functional reactive animation. \nIn ICFP, pages 263 273, 1997. [9] A. Filinski. Representing layered monads. In POPL, pages 175 188, 1999. \n [10] A. Filinski. Monads in action. In POPL, pages 483 494, 2010. [11] Andrzej Filinski. Representing \nmonads. In POPL, 1994. [12] J.A. Goguen and J. Meseguer. Security policy and security models. In Symposium \non Security and Privacy, pages 11 20, 1982. [13] Graham Hutton and Erik Meijer. Monadic Parsing in Haskell. \nJFP, 8(4), 1998. [14] Mark P. Jones. A theory of quali.ed types. In ESOP, 1992. [15] Mark P. Jones. Coherence \nfor quali.ed types. Technical Report YALEU/DCS/RR-989, Yale University, September 1993. [16] Mark P. \nJones. Simplifying and Improving Quali.ed Types. Technical Report YALEU/DCS/RR-1040, Yale University, \nJune 1994. [17] Mark P. Jones and Luc Duponcheel. Composing monads. Technical Report YALEU/DCS/RR-1004, \nYale University, 1993. [18] Oleg Kiselyov and Chung chieh Shan. Embedded probabilistic pro\u00adgramming. \nIn DSL, 2009. [19] P. Li and S. Zdancewic. Encoding information .ow in Haskell. In CSFW, pages 16 27, \n2006. [20] Z. Luo. Coercions in a polymorphic type system. MSCS, 18(4), 2008. [21] Z. Luo and R. Kie\u00dfling. \nCoercions in Hindley-Milner systems. In Proc. of Types, 2004. [22] Eugenio Moggi. Computational lambda-calculus \nand monads. In LICS, 1989. [23] Judea Pearl. Embracing causality in default reasoning (research note). \nArti.cial Intelligence, 35(2):259 271, 1988. [24] F. Pottier and V. Simonet. Information .ow inference \nfor ML. TOPLAS, 25(1):117 158, 2003. [25] Norman Ramsey and Avi Pfeffer. Stochastic lambda calculus and \nmonads of probability distributions. In POPL, pages 154 165, 2002. [26] Alejandro Russo, Koen Claessen, \nand John Hughes. A library for light-weight information-.ow security in haskell. In Haskell, 2008. [27] \nNikhil Swamy, Nataliya Guts, Daan Leijen, and Michael Hicks. Lightweight monadic programming in ML. Technical \nReport MSR\u00adTR-2011-039, Microsoft Research, May 2011. [28] Nikhil Swamy, Michael Hicks, and Gavin M. \nBierman. A theory of typed coercions and its applications. In ICFP, 2009. [29] Philip Wadler. The essence \nof functional programming. In POPL, 1992.    \n\t\t\t", "proc_id": "2034773", "abstract": "<p>Many useful programming constructions can be expressed as monads. Examples include probabilistic modeling, functional reactive programming, parsing, and information flow tracking, not to mention effectful functionality like state and I/O. In this paper, we present a type-based rewriting algorithm to make programming with arbitrary monads as easy as using ML's built-in support for state and I/O. Developers write programs using monadic values of type <i>m</i> &#964; as if they were of type &#964;, and our algorithm inserts the necessary binds, units, and monad-to-monad morphisms so that the program type checks. Our algorithm, based on Jones' qualified types, produces principal types. But principal types are sometimes problematic: the program's semantics could depend on the choice of instantiation when more than one instantiation is valid. In such situations we are able to simplify the types to remove any ambiguity but without adversely affecting typability; thus we can accept strictly more programs. Moreover, we have proved that this simplification is <i>efficient</i> (linear in the number of constraints) and <i>coherent</i>: while our algorithm induces a particular rewriting, all related rewritings will have the same semantics. We have implemented our approach for a core functional language and applied it successfully to simple examples from the domains listed above, which are used as illustrations throughout the paper.</p>", "authors": [{"name": "Nikhil Swamy", "author_profile_id": "81342513197", "affiliation": "Microsoft Research, Redmond, WA, USA", "person_id": "P2801363", "email_address": "nswamy@microsoft.com", "orcid_id": ""}, {"name": "Nataliya Guts", "author_profile_id": "81461655497", "affiliation": "University of Maryland, College Park, College Park, MD, USA", "person_id": "P2801364", "email_address": "guts@cs.umd.edu", "orcid_id": ""}, {"name": "Daan Leijen", "author_profile_id": "81100572466", "affiliation": "Microsoft Research, Redmond, WA, USA", "person_id": "P2801365", "email_address": "daan@microsoft.com", "orcid_id": ""}, {"name": "Michael Hicks", "author_profile_id": "81100060959", "affiliation": "University of Maryland, College Park, College Park, MD, USA", "person_id": "P2801366", "email_address": "mwh@cs.umd.edu", "orcid_id": ""}], "doi_number": "10.1145/2034773.2034778", "year": "2011", "article_id": "2034778", "conference": "ICFP", "title": "Lightweight monadic programming in ML", "url": "http://dl.acm.org/citation.cfm?id=2034778"}