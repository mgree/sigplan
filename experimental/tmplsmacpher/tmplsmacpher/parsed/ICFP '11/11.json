{"article_publication_date": "09-19-2011", "fulltext": "\n Balanced Trees Inhabiting Functional Parallel Programming Akimasa Morihata Kiminori Matsuzaki Tohoku \nUniversity Kochi University of Technology morihata@riec.tohoku.ac.jp matsuzaki.kiminori@kochi-tech.ac.jp \n Abstract Divide-and-conquer is an important technique in parallel program\u00adming. However, algebraic data \nstructures do not .t divide-and\u00adconquer parallelism. For example, the usual pointer-based im\u00adplementation \nof lists cannot ef.ciently be divided at their mid\u00addle, which prevents us from developing list-iterating \ndivide-and\u00adconquer parallel programs. Tree-iterating programs possibly face a similar problem, because \ntrees might be ill-balanced and list\u00adlike shapes. This paper examines parallel programming based on balanced \ntrees: we consider balanced-tree structures and develop recursive functions on them. By virtue of their \nbalancing nature, either bottom-up or top-down recursive functions exploit divide\u00adand-conquer parallelism. \nOur main contribution is to demonstrate the promiseof this approach.We proposeawayof systematically developingbalanced \ntrees from parallel algorithms, and then, we show that ef.cient parallel programs on them can be developed \nby equational reasoning powered by Reynolds relational parametric\u00adity.We consider functions that operate \neither lists or binary trees, and show that our methods can uniformly deal with both cases. The developed \nparallel programs are purely functional, correct by construction, and sometimes even simpler than known \nalgorithms. Categories and Subject Descriptors D.1.[Programming Tech\u00adniques]: Concurrent Programming \nParallel Programming; D.1.1 [Programming Techniques]: Applicative (Functional) Program\u00adming; E.1[Data \nStructures]:Trees General Terms Theory, Algorithms Keywords BalancedTree,Divide-and-conquerParallelism, \nEqua\u00adtional Reasoning,Parametric Polymorphism 1. Introduction Parallel programming is an important concept \nnow. Ef.cient paral\u00adlel programs are necessary to better utilize modern multi-core and multi-CPU machines. \nHowever, ef.cient parallel programs are dif\u00ad.cult to develop. There are several reasons for this,but \none is that ourfavorite data structures and programming idioms, namely linear lists, foldr and foldl, \nare not suitable for parallelism. They are in\u00adherently sequential.Parallel programming therefore requires \nother data structures and idioms. To see thegap between standard programs and those suitable for parallel \nevaluation, let us consider calculating the total sum of list elements. Permission to make digital or \nhard copies of all or part of this work for personal or classroom use is granted without fee provided \nthat copies are not made or distributed for pro.t or commercial advantage and that copies bear this notice \nand the full citation on the .rst page.To copyotherwise, to republish, to post on servers or to redistribute \nto lists, requires prior speci.c permission and/or a fee. ICFP 11, September 19 21, 2011,Tokyo, Japan. \nCopyright c &#38;#169; 2011ACM 978-1-4503-0865-6/11/09... $10.00 sumList [] =0 sumList (a : x)= a + sumList \nx Regrettably, this accustomed program is not a parallel program. There are no expressions that can be \nevaluated in parallel. To achieve good parallelism, we may think of the divide-and-conquer summation. \nsumList [] =0 sumList [a]= a sumList (x ++ y)= sumList x + sumList y We borrow++ from Haskell to denote \nlist concatenation. The in\u00adtended meaning of this pseudoprogram is If the input consists of more than \none element, we split it in the middle, simultaneously compute the summations of sublists, and then merge \nthe two re\u00adsults. However,itis nota real program because conventional func\u00adtional programming languages \ndo not support pattern-matching by ++. An actual program should be something like the following: sumList \n[] =0 sumList [a]= a sumList z = let (x, y)= splitAt (div (length z) 2) z in sumList x + sumList y Here, \nwe split the input list in the middle by using splitAt. Now this de.nition brings an opportunity for \nparallel evaluation we can calculate sum x and sum y in parallel. However, this is not satisfactory. \nsplitAt will form a bottleneck because its calculation cannot be parallelized. Note that we mightfacea \nsimilar problemeven when we are manipulating trees. Consider the summation on binary trees. sumTree (Tip \na)= a sumTree (Bin lar)= sumTree l + a + sumTree r This appears to be a good parallel program because \nwe can simul\u00adtaneously evaluate sumTree l and sumTree r,but it is not ef.cient in general. The program \ncannot achieveparallel speedups when the input is a list-like tree whose left subtrees are all leaves. \nAs seen, there aregaps between standard functional programs and parallel programs. One of the most criticalgaps \ninvolves data structures. The usual pointer-based structures such as lists are not suitable for parallel \nprogramming. Theydo not provide an ef.cient way of dividing themselves, and therefore, computations on \nthem cannot be ef.ciently divided into a reasonably sized tasks that can be evaluated in parallel. To \nresolve this problem, we focus on balanced trees.We adopt balanced trees as abstract data structures \nthat encapsulate divide\u00adand-conquer parallelism, and develop recursive functions on them as basic components \nfor functional parallel programming. Intu\u00aditively,this approach regards ++ asaconstructor. Then, sumList \nde\u00ad.nedby using ++ isa conventional structural recursion on the tree, and the .exibility of splitting \nthe input to the sublists (as x ++ y) enables us to let the tree balanced.  This approach, considering \nrecursive functions on balanced trees, has several strong points. First of all, it isolates parallel \nal\u00adgorithmic issues from the program description. The balanced trees implicitly guarantees parallel speedups \nwith respect to the number of processors, and we do not need to worry about parallelism when developing \nprograms. Moreover,we may be able to implement bal\u00adanced trees by known self-balancing trees, and then \ntheir dynamic rebalancing enables us to add elements to or removeelements from them without seriously \naffecting parallel speedups. Note that recognizing ++ as a constructor is not very new. This viewhasbeentakenbyHuetal.[1998,2002]for \nformalizingfunc\u00adtional parallel programming patterns, andabalanced-tree represen\u00adtation of lists is also \nemployed in an implementation of a parallel functional programming language [Fluet et al. 2008]. However, \nit has not been clari.ed whether this approach scales up. There are several questions that have not been \nresolved yet, such as whether it can deal with functions on structures other than lists, whether it can \nencapsulate complicated parallel algorithms, and whether we can effectively utilize them. In this paper, \nwe describe an experiment carried out to demon\u00adstrate the promiseof this approach.We proposea methodof \nsys\u00adtematically developing balanced tree structures and parallel pro\u00adgrams on them. The method primarily \nconsists of the following two parts. We propose a systematic way of relating parallel algorithms to \nbalanced trees. Given a parallel algorithm and its input, we re\u00adlate them to a tree structure on which \nthe parallel algorithm forms a structural recursion. Its height is guaranteed to be less than the number \nof parallel steps of performing the compu\u00adtation; therefore, it is balanced if the original algorithm \nex\u00adploits much parallelism. Moreover, .exibility in parallel algo\u00adrithms, such as .exibility in dividing \nthe underlying structure, brings rebalancing of the derived balanced trees.For instance, weshowthata \nparallel tree contraction algorithmby Abraham\u00adson et al. [1989], which has been well studied as a basis \nof de\u00adveloping ef.cient parallel tree operations, derives a variant of the directed topology tree [Frederickson \n1997], which is a self\u00adbalancing tree structure for retaining binary trees.  We demonstrate systematic \ndevelopment of parallel programs on the balanced trees. Ourvehicleis equational reasoningpow\u00adered by \nReynolds relational parametricity [Reynolds 1983; Wadler 1989]. It enables us to systematically develop \nparallel programs that are correctby construction. Most notably, we de\u00advelop new ef.cient, purely functional \nimplementations of the parallel tree accumulations by Gibbons et al. [1994], whose original implementations \nare involved intricate pointer manipu\u00adlations.  The most characteristic feature is that our method does \nnot rely on certain structures of computations or data.We demonstrate that it can uniformly deal with \ncomputations on both of lists (Section 3) and binary trees (Sections4and 5). 2. Preliminary We consider \ntyped, purely functional programs with the Hindley-Milner type system.Weassume all computations terminate \nwithout raising anyerrors regardless of their reduction orders. We use Haskell [Peyton Jones 2003] to \ndescribe programs. We use Greek letters, a,(,..., for type variables, and capi\u00adtal English letters, A, \nB, . . ., for meta variables ranging over types. id :: .a. a . a denotes the identity function. Operator \n(.) :: .a,(,'. (( . ') . (a . () . a . ' denotes the func\u00adtion composition, i.e., (f . g) x = f (gx). \nAn underline, -, denotes a don tcare pattern that matches any value and provides no binding. We adopt \nEREW PRAM (exclusive-read exclusive-write paral\u00adlel random access machines) as our parallel computation \nmodel. Unless otherwise stated, p denotes the number of processors. 3. Balanced Trees for Parallel List \nOperations First, let us consider parallel evaluation of list operations. We extract the computation \nstructure of divide-and-conquer summa\u00adtions (Section 3.1), develop a balanced-tree structure from it \n(Sec\u00adtion 3.2), con.rm that the structure supports ef.cient dynamic re\u00adbalancing (Section 3.3), and develop \nparallel programs on it (Sec\u00adtion 3.4). This section might appear to be rather trivial. In the next two \nsections, we will consider tree operations and see that this method scales up to a more complicated case. \n 3.1 List-homomorphism Scheme Our startingpointisthedivide-and-conquer summation.Weignore the empty list \nfor simplicity. sumList [a]= a sumList (x ++ y)= sumList x + sumList y Usually,a function calculatesvaluesby \naggregating informationin the input structure.We focus on this aggregation structure. sumList just extracts \nthe element from each singleton list, and merges the results of independent sublists by addition. By \nabstracting out the concrete computations, we obtain the following skeleton , known as list homomorphisms \n[Bird 1987]. hom :: .a,(. (( . ( . () . (a . () . [a] . ( hom (.) f [a]= fa hom (.) f (x ++ y)= hom (.) \nfx . hom (.) fy We assume associativity of . so as to make the latter equation meaningful. Note thatalist \nhomomorphism does not specify anystrategy for dividing the input list. It is natural to split the list \nin the middle. However, other strategies are worth considering in general, for example when computational \nunits are heterogeneous. In short, a list homomorphism represents a set of functions. Let us formally \nspecify functions that correspond to list homo\u00admorphisms. Apparently,each of these functions, say h,should \nhave the following polymorphic type1: h :: .(. (( . ( . () . (A . () . [A] . (. This polymorphic type \nis not suf.cient to characterize list homo\u00admorphisms.Forexample, the following hBad could have this poly\u00admorphic \ntypebut hBad (+) id .= sumList. = hom (+) id hBad (.) f [a]= fa hBad (.) f (a : x)= fa . (fa . hBad x) \nTherefore,we shouldimpose some restrictions.Anaturalideaisto exclude those that duplicate, discard, or \ndisorder elements. In other words, as the following equation describes, the function should be able to \nput back the original list if no computation is performed. h (++) (aa . [a]) x = x This requirement is \nactually suf.cient. Reynolds relational para\u00admetricity [Reynolds 1983; Wadler 1989] explains that h satis.es \n1Functions that have more generic types are also acceptable. Note that A does not need to be polymorphic. \nSpecializing h to A could be worthwhile because we may change the strategy of splitting the input according \nto A. For example, ifA consists of structures such as trees, we may take account of their sizes.  the \nfollowing property from its type. g (l . r)= gl . gr . g (h (.) fx)= h (.)(g . f) x Then, it is straightforward \nto prove h (+) id = sumList. h (+) id x = sumList x .{ the requirement } h (+) id x = sumList (h (++) \n(aa . [a]) x) . { parametricity } sumList (l ++ r)= sumList l + sumList r . sumList . (aa . [a]) = id \n. { de.nition of sumList } True We have successfully characterized list homomorphisms by us\u00adinga setof \nfunctions.We call them list-homomorphism schemes. DEFINITION 3.1. Polymorphic function h :: .(. (( . \n( . () . (A . () . [A] . ( is saidtobea list-homomorphism scheme if it satis.es h (++) (aa . [a]) x = \nx for any list x.  3.2 Join Lists Next, we derive a balanced tree structure. This is easy; supplying \nconstructors to list-homomorphism schemes is suf.cient. This tree structure is called join (or append) \nlists [Hu et al. 1998, 2002]. data JList a = Single a | Join (JList a)(JList a) Given a list, say x, \nwe construct its join list representation as h Join Single x by using a list-homomorphism scheme, h.We \ncan restore the original list from its join-list representation. j2l :: .a. JList a . [a] j2l (Single \na)=[a] j2l (Join lr)= j2l l ++ j2l r Interestingly, j2l provides an exact characterization of list\u00adhomomorphism \nschemes. Thisfact may justify their de.nition. THEOREM 3.2. Polymorphic function h :: .(. (( . ( . () \n. (A . () . [A] . ( isalist-homomorphismschemeifandonlyifthe following equation holds for any list x. \nj2l (h Join Single x)= x PROOF. Note that it is suf.cient to prove the following equation. h (++) (aa \n. [a]) x = j2l (h Join Single x) It can be proved from the parametricity. h (++) (aa . [a]) x = j2l (h \nJoin Single x) .{ parametricity } j2l (Join lr)= j2l l ++ j2l r . j2l (Single a)=[a] .{ de.nition of \nj2l } True Theorem 3.2 implies another view of join lists: Single and Join are the frozenrepresentations \n[K\u00a8ander uhnemannetal. 2001;Voigtl \u00a82002] of (++) and (aa . [a]), respectively, and j2l is the thawing \nfunction for them. Join lists crystallize the computational structure of a list-homomorphism scheme. \nIt is worth noting that there are equivalent join lists that repre\u00adsent the same list, i.e., thyresult \nin the same list by j2l.For exam\u00adple, both of the following two join lists Join (Join (Single 1) (Single \n2)) (Single 3) Join (Single 1) (Join (Single 2) (Single 3)) represent [1, 2, 3] and are thus equivalent. \nThis .exibility corre\u00adspondstothe possibilityof choosingalist-homomorphism scheme. Infact, this .exibility \nis thekeyto ef.cient parallel computations. We can use divide-and-conquer parallelism by taking a balanced \nrepresentation from among those that are equivalent. 3.3 Balancing Join Lists It is not dif.cult to \nobtain a balanced join-list representation of a list. Given a list homomorphism scheme that .nishes its \ncomputa\u00adtion in k parallel steps, we can obtain a join list whose height is k. Therefore, the following \ntheorem holds. THEOREM 3.3. Given a list of length n implemented by an array, we can obtain its join \nlist representation whose height is llog2 n. in time O(n/p + log p). Actually, we can do more we can \nef.ciently modify the struc\u00adture while maintaining its balance. It is apparent from the de.nition of \nj2l and Theorem 3.2 that join lists are only sensitive to the order of leaves. Therefore, we can implement \njoin lists by self-balancing binary search trees.Forexample,byusing biased search trees [Bent et al. \n1985], we can prove the following theorem. THEOREM 3.4. Given a list of length n, there exists its join \nlist representation whose height is O(log n) and that can both remove its sublist and append another \nlist in time O(log n), which results in a join list representation of height O(n). Note that the time \ncomplexity of rebalancing, O(log n), is ade\u00adquate even though it is not a parallel algorithm. Even if \ncomputa\u00adtions on join lists .nish in time O(n/p), which is really ideal, the rebalancing cost is not \ndominant if p . O(n/ log n). Another balanced tree structure called a rope [Boehm et al. 1995] is adopted \nin a functional parallel programming language, Manticore [Fluet et al. 2008]. Compared to biased search \ntrees, ropes have smaller hidden constants and support more ef.cient concatenation, while their rebalancing \nis more costly.  3.4 Developing Programs on Join Lists Because we can keep join lists balanced, either \nbottom-up or top-down recursive functions on them will achieve good parallel speedups. However, we should \nnot use arbitrary recursive func\u00adtions. If functions do not respect the balancing operations, we can\u00adnot \ntake advantage of the balancing nature. In other words, we require that each function should result in \nthe same value for all equivalent join lists. We propose implementing join lists as an abstract datatype \nthat possesses several higher-order functions that form a safe interface. It forces us to respect the \nequivalence on join lists. An additional bene.t is that it conceals the concrete structure of the tree \nand thus, can implicitly balance itself. The main issue is the interface design. Here we refer to algorith\u00admic \nskeletons [Cole 1989], which are well-designed parallel pro\u00adgramming patterns. Standard list-operating \nskeletons include map, .lter, foldr, and scanr.We develop their implementations on join lists as follows. \nGiven function g::[A].B,wewouldliketo obtain its join-listversion gJ ::JList A.B that simulates the computation \nof g on join lists. gJ = g . j2l From this characteristic equation, we work out a recursive de.ni\u00adtion \nof gJ by equational reasoning. Note that gJ developed in this manner results in the same value for all \nequivalent join lists. It is worth noting that our derivations are routine for fusion (also known as \ndeforestation) transformation based on the classic unfolding-folding method [Burstall and Darlington \n1977; Wadler 1990]: we unfold function de.nitions so as to distribute their com\u00adputations and then try \nto match the expression to the characteristic equationsoastoobtainarecursive de.nition.Therearenoparallel \nprogramming issues.  3.4.1 List Homomorphisms Several skeletons, for example map and .lter, are instances \nof list homomorphisms. It is easy to implement list homomorphisms on join lists.From the strategy above, \nwe can immediately obtainthe following join-list version. homJ (.) f (Single a)= fa homJ (.) f (Join \nxy)= homJ (.) fx . homJ (.) fy 3.4.2 foldr Amore nontrivial example isfoldr. foldr fe [a]= f ae foldr \nfe (a : x)= fa (foldr f ex) foldr isnotalways suitableforevaluationonjoin lists.Wederive a suf.cient \nconditionbyworking out its join-listversion. foldrJ fe (Single a)= { de.nition of foldrJ }foldr fe (j2l \n(Single a)) = { de.nitions of j2l and foldr } f ae foldrJ fe (Join lr) = { de.nition of foldrJ }foldr \nfe (j2l (Join lr)) = { de.nition of j2l }foldr fe (j2l l ++ j2l r) = { claim: foldr fe (x ++ y)= foldr \nf (foldr f ey) x }foldr f (foldr fe (j2l r)) (j2l l) = { let foldr.f xe = foldrJ f ex } J (foldr.J fl \n. foldr.J fr) e Con.rming the claim is straightforward. Now the reasoning shows that foldr.J rather than \nfoldrJ is more suitable for bottom-up paral\u00adlel evaluation on join lists. foldrJ can be evaluated in \na bottom-up manner if we can ef.ciently implement the function composition operation. THEOREM 3.5 (Context \npreservation thereom [Chin et al. 1998; Morihata and Matsuzaki 2010]). foldrJ fe can be computed by a \nbottom-up divide-and-conquer parallel algorithm provided that thereexistsa setof constant-time functions \nF suchthat f1,f2 .F implies f1 . f2 .F.  3.4.3 scanr Sofar, we have considered bottom-up computations \non join lists. As the last example, we consider scanr and demonstrate that it results in a more interesting \nparallel program that includes a top\u00addown sweep on join lists. scanr fe [-] =[e] scanr fe (- : x)=[foldr \nf ex]: scanr f ex We may consider developing scanrJ such that scanrJ fe = scanr fe . j2l.Yet, it seems \nmore useful if scanrJ returns join lists. Thus, we de.ne scanrJ as follows. j2l . scanrJ fe = scanr fe \n. j2l Let us now work out scanrJ. j2l (scanrJ fe (Single -)) = { de.nition of scanrJ }scanr fe (j2l (Single \n-)) = { de.nitions of j2l and foldr } [e] 0 22 = 10 12+0  6 4 12 4+12 12 0 AAAAAAAA A AA A A A A \n4 2 3 1 7 5 2+16 161+12 125+00 Figure 1. Parallel evaluation ofscanrJ (+) 0 on a join-list repre\u00adsentation \nof [4, 2, 3, 1, 7, 5]:a bottom-up calculation of foldr.J (+) (the left picture) is followed by a top-down \ndistribution (the right picture) of the calculated values. j2l (scanrJ fe (Join lr)) = { de.nition of \nscanrJ }scanr fe (j2l (Join lr)) = { de.nition of j2l }scanr fe (j2l l ++ j2l r) = { claim (described \nbelow) }scanr f (foldr fe (j2l r)) (j2l l) ++ scanr fe (j2l r) = { de.nition of foldr.} J scanr f (foldr.J \nfre)(j2l l) ++ scanr fe (j2l r) = { de.nition of scanrJ }j2l (scanrJ f (foldr.J fre) l) ++ j2l (scanrJ \nf er) = { de.nition of j2l }j2l (Join (scanrJ f (foldr.J fre) l)(scanrJ f er)) The claim is scanr fe \n(x ++ y)= scanr f (foldr f ey) x ++ scanr f ey, and can be easily proved. In summary, we obtain the following \nrecursive de.nition. scanrJ fe (Single -)= Single e scanrJ fe (Join lr) = Join (scanrJ f (foldr.J fre) \nl)(scanrJ f er) The program suggests the following parallel evaluation of scanrJ.We .rst calculate foldr.J \nf for each subtree in a bottom-up manner. To carry out this calculation, we assume the premise of Theorem \n3.5. Then, we distribute the calculated value in the top\u00addown manner: we pass those from the parent and \nthe right subtree to the right and left subtrees, respectively. Finally, we update the leafvalues. Figure1depictsthe \nprocedure.It .nishes parallel steps proportionally to the height of the join lists. 4. Balanced Trees \nfor Parallel Tree Operations Next, we show that our method scales up to binary trees. data Tree a( = \nTip a | Bin (Tree a() ( (Tree a() As brie.y discussed in the introduction, the naive divide-and\u00adconquer \nevaluation, namely processing each subtrees in parallel, is not generally effective. It requires O(h) \nparallel steps where h is the height of the input. In other words, when the input is monadic like lists, \nit shows little parallel speedups. List-liketrees often appear in practice.For instance, syntax trees \nand XMLs are usually list-like. We would like to develop a balanced tree structure that retains (possibly \nlist-like) binary trees so as to exploit more parallelism. We focus on parallel tree contraction algorithms \n[Abrahamson et al. 1989; Miller and Reif 1985; Reif 1993], which we review in Section 4.1.Parallel tree \ncontraction algorithmsprovidea schedul\u00adingfor processinga treeofsize n in O(log n) parallel steps regard\u00adlessoftheshapeofthe \ntree. Thereforetheycouldbeagood starting point for developing a balanced tree structure. We formalizea \nparallel tree contraction algorithm usingafam\u00adily of polymorphic functions (Section 4.2), derive a balanced \ntree structure (Section 4.3), and see that existing self-balancing trees  Figure 2. AShunt operation \napplied to the star-marked leaf. can implement the derived tree structure (Section 4.4). We de\u00advelop \nimplementations of algorithmic skeletons on this balanced tree structure in the next section. 4.1 Parallel \nTree Contraction Parallel tree contraction algorithms [Miller and Reif 1985; Reif 1993] collapse a tree, \nnamely to reduce a tree to a leaf, through simultaneous applicationsofprimitive contraction operations. \nThe Shunt contraction algorithm [Abrahamson et al. 1989; Kosaraju and Delcher 1988; Reif 1993] is one \nthat uses Shunt as the primi\u00adtive contraction operation. Figure2describes the Shunt operation applied \nto the star-marked leaf. Shunt is an operation to a leaf, in whichtheleafandits parentis removed.In ordertokeepthe \nwhole structure as a tree, the node remaining after the contraction is con\u00adnected to its grandparent. \nThe following is a functional description of a Shunt operation. shunt :: .a, (. Tree a( . Tree a( shunt \n(Bin t - (Tip -)) = t shunt (Bin (Tip -) - t)= t The Shunt contraction algorithm providesagood scheduling \nfor processing trees. THEOREM 4.1 ([Abrahamson et al. 1989; Kosaraju and Delcher 1988]). We can completely \ncollapse a tree of size n using Shunt operations in time O(n/p + log p). Aparallel tree contraction \nalgorithm can implement manycom\u00adputations on trees by aggregating information in trees according to contraction \noperations [Abrahamson et al. 1989; Gibbons et al. 1994; Matsuzaki 2007; Matsuzaki et al. 2006a,b; Miller \nand Reif 1985; Reif 1993]. For example, by using the following addS in\u00adstead of shunt, we can calculate \nthe total sum of the values in the tree in parallel. addS :: Tree Int Int . Tree Int Int addS (Bin (Bin \nt1 bt2) a (Tip c)) = Bin t1 (b + a + c) t2 addS (Bin (Tip b) a (Bin t1 ct2)) = Bin t1 (b + a + c) t2 \naddS (Bin (Tip b) a (Tip c)) = Tip (b + a + c) We do not present the concrete procedures of parallel \ntree con\u00adtraction algorithms here; other sources such as [Reif 1993] can be consulted for details.We \nnote that theyare based on intensive use of arrays and pointers, and moreover, theycompletely destroythe \ninput tree as a side effect; therefore, it seems disadvantageous to implement them in purely functional \nsettings.  4.2 Shunt Contraction Scheme Now, let us formalize Shunt-based programsin termsofafamily \nof polymorphic functions. Recall that each Shunt operation aggre\u00adgates information from three nodes. \nAs seen inaddS,there are three cases: the left child is an internal node; the right is an internal node; \nand none of the children are internal nodes. Therefore, we specify aggregating operationsbythe following \nrecord type, where a and ( respectivelydenotethetypesofvaluesofleavesand internal nodes. data Shunt a( \n= Shunt {left :: ( . ( . a . (, right :: a . ( . ( . (, none :: a . ( . a . a} .a .. ... .. ..a.. .b \n..c...b ..c.. ... ... = .... .. Figure 3. The computation of connectL: A Shunt operation is applied \nto the right leaf that retains a tree c. a and b are contexts retained in the parent and the left sibling. \nThe contraction yields a larger context. We write ([\u00dfL,\u00dfR,\u00dfN]) as a shorthand of Shunt {left = \u00dfL, right \n= \u00dfR, none = \u00dfN}. We intend to capture the Shunt contraction by functions that have the following polymorphic \ntype. scs :: .a, (. (A . a) . (B . () . Shunt a( (1) . Tree AB . a Function scs prepares initial values \nfor leaves and internal nodes using the .rst and second parameters, respectively, and then, it aggregates \nthe values using the third parameter. The result type is a because parallel tree contraction results \nin a leaf. For example, we intend to implement the summation on trees, say sumTree, as follows: sumTree \n= scs id id ([add3 , add3 , add3 ]), where add3 bac = b + a + c. Indeed, following scsBU , which corresponds \nto the naive bottom-up computation, have the poly\u00admorphic type (1) and scsBU id id ([add3 , add3 , add3 \n]) calculates the total sum. scsBU fg ([\u00dfL,\u00dfR,\u00dfN]) (Tip a)= fa scsBU fg ([\u00dfL,\u00dfR,\u00dfN]) (Bin lar)= \u00dfN b \n(ga) c where b = scsBU fg ([\u00dfL,\u00dfR,\u00dfN]) l c = scsBU fg ([\u00dfL,\u00dfR,\u00dfN]) r Merging occurs only when both childrenbecome \nleaves, and hence it uses only \u00dfN. Yet,the polymorphictype(1)isnotsuf.cient.Wewouldlike to forbid discarding, \nduplicating, and shuf.ing subtrees.Toexpress this requirement, we consider reconstructing the input tree. \nscs Tip hole connect t = t The arguments, hole and connect, are de.ned as follows. hole prepares a context, \ni.e., a tree with a hole, and connect constructs a larger context or tree by combining smaller contexts \nand trees. type Context a( =(Tree a (, Tree a() . Tree a( hole :: .a, (. ( . Context a( hole a = a(l, \nr) . Bin lar connect :: .a, (. Shunt (Tree a()(Context a() connect = ([connectL, connectR, connectN]) \nwhere connectL bac = at . a (b t,c) connectR bac = at . a (b,c t) connectN bac = a (b, c) Figure3showsthe \nbehaviorof connectL. It connects two contexts, a and b, and tree c, which are respectively retained in \nthe parent, the left child, and the right child. To summarize, we formalize the Shunt contraction as \nfollows. DEFINITION4.2 (Shunt-contraction scheme). Afunctionscs of the polymorphic type (1) is said to \nbe a Shunt-contraction scheme if scs Tip hole connect t = t holds for any tree t.  Figure 4. Two equivalent \nShunt trees.We omitT and H ;a, The following lemma follows from the relational parametricity. LEMMA 4.3. \nGiven function h, assume that there exists h such that the following three equations hold. h (\u00dfL bac)= \n\u00dfL (hb)(ha)(hc) h (\u00dfR bac)= \u00dfR (hb)(ha)(hc) h (\u00dfN bac)= \u00dfN (hb)(ha)(hc) Then the following equation holds \nfor any function scs of the polymorphic type (1). h . scs fg ([\u00dfL,\u00dfR,\u00dfN]) = scs (h . f)(h . g) ([\u00dfL,\u00dfR,\u00dfN]) \nLemma 4.3 is useful for reasoning about Shunt-based parallel programs.Forexample, sumTree = scs id id \n([add3 , add3 , add3 ]) follows for anyShunt-contraction scheme scs. sumTree = scs id id ([add3 , add3 \n, add3 ]) .{ scs is a Shunt-contraction scheme } sumTree . scs Tip hole connect = scs id id ([add3 , \nadd3 , add3 ]) .{ Lemma 4.3 with letting ht = sumTree (t (Tip 0, Tip 0)) } sumTree . Tip = id . h . \nhole = id . h (connectL bac)= add3 (hb)(ha)(sumTree c) . h (connectR bac)= add3 (sumTree b)(ha)(hc) . \nsumTree (connectN bac) = add3 (sumTree b)(ha)(sumTree c) .{ unfolding de.nitions (note Tip 0 does not \naffect the sum) } sumTree (a (b, c)) = sumTree c + sumTree (a (Tip 0, Tip 0)) + sumTree b The obtained \nequation can be read as follows: the total sum of a large tree, a (b, c), can be calculated in a divide \nat the middle and conquer manner, namely adding the total sums of the upper part, a, and subtrees b and \nc. This evidently holds.  4.3 Shunt Trees Next, we derive a balanced tree structure from Shunt-contraction \nschemes.We nameita Shunt tree and de.ne as follows2. data ST a( = T a | N (ST a()(ST a()(ST a() data \nST a( = H ( | L (ST a()(ST a()(ST a() | R (ST a()(ST a()(ST a() Shunt trees consist of two mutually-de.ned \ndatatypes. ST repre\u00adsents a binary tree, while ST represents a context. The following functions, s2t \nand s2c, provide the meaning of Shunt trees. s2t :: .a, (. ST a( . Tree a( s2t (T a)= Tip a s2t (N bac)= \nconnectN (s2t b)(s2c a)(s2t c) 2T, H, L, R, and N respectively abbreviate Tip, Hole, Left, Right, \nand None. The stands for the hole. c, d, and e are wrapped by H , while b, f, g, h, and i are by T. \ns2c :: .a, (. ST a( . Context a( s2c (H a)= hole a s2c (L bac)= connectL (s2c b)(s2c a)(s2t c) s2c (R \nbac)= connectR (s2t b)(s2c a)(s2c c) By using the constructors and a Shunt-contraction scheme scs, we \ncan transformatree t to its Shunt-tree representation as follows. scs TH ([L , R , N]) t The following \ntheorem justi.es the construction of Shunt trees. THEOREM 4.4. Function scs of the polymorphic type (1) \nis a Shunt-contraction scheme if and only if the following equation holds for any tree t. s2t (scs TH \n([L , R , N]) t)= t PROOF. It is suf.cient to con.rm the following equation. s2t (scs TH ([L , R , N]) \nt)= scs Tip hole connect t It is evident from the de.nition of s2t and Lemma 4.3. AShunt tree crystallizes \na Shunt contraction scheme. Given a Shunt-contraction scheme scs and a binary tree t, the Shunt tree \nscs TH ([L , R , N]) t describes how scs will process t. The constructors denote what operations we should \napply. As the case of join lists, there are many Shunt trees that are equivalent in the sense that theycorrespond \nto the same binary tree. Figure4shows anexample. The left and the right Shunt trees are equivalent and \ncorrespond to the middle binary tree.  4.4 Balancing Shunt trees To fully exploit divide-and-conquer \nparallelism, we would like to .nd a balanced Shunt tree among the equivalent ones. One way to obtain \na balanced Shunt tree is to use the Shunt contraction algorithm.It calculateabalanced Shunt treein parallel. \nTHEOREM 4.5. Given a tree of n nodes, we can .nd its Shunt tree representation whose height is at most \n2llog n. in time O(n/p + log n). PROOF SKETCH. Use the Shunt contraction scheme that corre\u00adsponds to \nthe known algorithm [Abrahamson et al. 1989;Kosaraju and Delcher 1988]. The complexity and the height \nareevident from the algorithm. Moreover, it is possible to support dynamic rebalancing. Here we use directed \ntopology trees [Frederickson 1997]. Directed topol\u00adogy trees provide self-balancing representations of \nbinary trees. LEMMA 4.6 ([Frederickson 1997]). The height of each directed topology tree of n nodes is \nO(log n). LEMMA 4.7 ([Frederickson 1997]). Given directed topology trees of n nodes, their link (merging \ntwo trees by adding an edge) and cut (removing an edge and thereby obtaining two trees) can be performed \nin time O(log n).  mapTree :: .a,a ,(,( . (a . a ) . (( . ( ) . Tree a( . Tree a( fg (Tip a)= Tip (fa) \nmapTree Bin (mapTree mapTree fg (Bin t1 at2)= fgt1)(ga)(mapTree fgt1) zipTree :: .a,a ,(,( . Tree a( \n. Tree a( . Tree (a, a )((, ( ) (Tip a)(Tip b)= Tip (a, b) zipTree (Bin t1 at2)(Bin u1 bu2)= zipTree \nBin (zipTree t1 u1)(a, b)(zipTree t2 u2) reduce :: .a, (, '. (a . ') . (' . ( . ' . ') . Tree a( . ' \nreduce fg (Tip a)= fa reduce fg (Bin t1 at2)= g (reduce fgt1) a (reduce fgt1) uAcc :: .a, (, '. (a . \n') . (' . ( . ' . ') . Tree a( . Tree '' uAcc fg (Tip a)= Tip (fa) uAcc fg (Bin t1 at2)= Bin (uAcc fgt1)(reduce \nfg (Bin t1 at2)) (uAcc fgt2) dAcc :: .a, (, '. (' . ( . ') . (' . ( . ') . Tree a( . ' . Tree '' dAcc \nfL fR (Tip -) e = Tip e dAcc fL fR (Bin t1 at2) e = Bin (dAcc fL fR t1 (fL ea)) e (dAcc fL fR t2 (fR \nea)) Figure 5. De.nitions of tree skeletons. Shunt trees can be implemented by using directed topology \ntrees. Here we just show consequences, and will give a detailed explanation in Appendix A. THEOREM4.8. \nWecan implementaShunttreebyadirected topol\u00adogy tree so that its height is O(log n) and link and cut operations \nfor it take time O(log n), where n is the number of its nodes. 5. Developing Parallel Programs on Shunt \nTrees We have developed balanced Shunt trees. As their interfaces, we employ algorithmic skeletons that \noperate trees [Gibbons et al. 1994; Skillicorn 1996], called tree skeletons, and develop their Shunt-treeversionsby \nequational reasoning. Figure5summarizes the de.nitions of tree skeletons. mapTree and zipTree are tree \nversions of the well-known list operations. reduce is similar to foldr, and aggregates values in the \ntree. uAcc is similar to scanr, and puts the intermediate result of reduce on each node. dAcc update \nnode values while traversing a tree from its root to leaves. Their usefulness has been demonstrated through \nseveral examples [Matsuzaki et al. 2006b; Skillicorn 1997, 1996], including dynamic programming and XML \nprocessing. Implementations of mapTree are apparent.We con\u00ad and zipTree sider the others. 5.1 Shunt-based \nPrograms Before considering tree skeletons, we showthat our approach is not less expressive than a Shunt-contraction-based \napproach. THEOREM 5.1. Given function h, h = scs \u00dfT \u00dfH ([\u00dfL,\u00dfR,\u00dfN]) holds for any Shunt contraction scheme \nscs if and only if hS = h . s2t holds for function hS de.ned as follows. hS (T a)= \u00dfT a hS (N bac)= \u00dfN \n(hS b)(hS a)(hS c) hS (H a)= \u00dfH a hS (L bac)= \u00dfL (hS b)(hS a)(hS c) hS (R bac)= \u00dfR (hS b)(hS a)(hS c) \n PROOF. We prove this theorem as follows. h = scs \u00dfT \u00dfH ([\u00dfL,\u00dfR,\u00dfN]) .{ claim1(explained below) } h = \nhS . scs TH ([L , R , N]) .{ Theorem 4.4 } h . s2t . scs TH ([L , R , N]) = hS . scs TH ([L , R , N]) \n.{ claim2(explained below) } h . s2t = hS The .rst claim is straightforward from Lemma 4.3. The sec\u00adond \nclaim holds because any Shunt trees can be generated by scs TH ([L , R , N]) t with appropriate scs and \nt. This theorem states that Shunt-contraction-based program h can be implemented on Shunt trees by hS \nand hS. Since they are bottom-up recursive programs, their parallel evaluation is easy.  5.2 Tree Reduction \nAs the .rst skeleton, let us consider the tree reduction, reduce.We would like to develop its Shunt-tree \nversion, reduceS. reduceS :: .a, (, '. (a . ') . (' . ( . ' . ') . ST a( . ' reduceS fgs = reduce fg \n(s2t s) Since Shunt trees include ST , we also need to develop the ST version, say reduceS. It calculates \nthe tree reduction from the context and the value of the missing subtree. reduceS :: .a, (, '. (a . ') \n. (' . ( . ' . ') . ST a( . (', ') . ' reduceS fgs (reduce f gl, reduce fgr) = reduce fg (s2c s (l, r)) \n Let us develop recursive de.nitions of reduceS and reduceS. reduceS fg (T a)= { de.nition of reduceS \n} reduce fg (s2t (T a)) = { de.nitions of s2t and reduce } fa reduceS fg (N bac) = { de.nition of reduceS \n}reduce fg (s2t (N bac)) = { de.nition of s2t }reduce fg (s2c a (s2t b, s2t c)) = { de.nition of reduceS \n}reduceS fga (reduce fg (s2t b), reduce fg (s2t c)) = { de.nition of reduceS }reduceS fga (reduceS f \ngb, reduceS fgc) = { abstraction } let a = reduceS fga b = reduceS fgb c = reduceS fgc in a (b ,c ) Weintroduced \nthe last step so as to makeour attention to the divide\u00adand-conquer parallelism explicit.  considered \nessentially the same condition as a suf.cient condition T a algorithm. Matsuzaki et al. [2006a] discussed \nmore involved ap\u00adplications including dynamic programming and XML querying by (b, c) fa \u00dfH a \u00dfL fb fa \nc H aN L A AA b fa A AA c A AA fb fa A AA cb fc for implementing tree operations by using the Shunt \ncontraction using matrices to encode F. fa \u00dfR bfa R fc fa A AA A AA Figure 6. Implementation of reduce \nfg on Shunt trees: from the values given from its children, the value of the node is calculated andis \nsenttoits parent;thevariablesusedhere correspondto those in Theorem 5.2. Next, we deal with reduceS. \nIn the following, we use lred and rred to denote reduce fgl and reduce fgr, respectively. reduceS fg \n(lred ,rred )(H a) = { de.nition of reduceS }reduce fg (s2c (H a)(l, r)) = { de.nition of s2c }reduce \nfg (Bin lar) = { de.nition of reduce } glred arred reduceS fg (lred ,rred )(L bac) = { de.nition of reduceS \n}reduce fg (s2c (L bac)(l, r)) = { de.nition of s2c }reduce fg (s2c a (s2c b (l, r), s2t c)) = { de.nition \nof reduceS }reduceS fga (reduce fg (s2c b (l, r)), reduce fg (s2t c)) = { de.nition of reduceS and reduceS \n}reduceS fga (reduceS fgb (lred ,rred ), reduceS fgc) = { abstraction } let a = reduceS fga b = reduceS \nfgb c = reduceS fgc in (ah . a (b h,c )) (lred ,rred ) reduceS fg (lred ,rred )(R bac) = { similar to \nthe case of L } let a = reduceS fga b = reduceS fgb c = reduceS fgc in (ah . a (b,c h)) (lred ,rred ) \nThe recursive de.nition implies a suf.cient condition for its divide-and-conquer evaluation. The condition \nis in spirit very sim\u00adilartothatfor Theorem3.5:Wecanachieveef.cient parallel com\u00adputation if we can ef.ciently \nmerge and evaluate closures. THEOREM 5.2. reduceS fg :: ST AB . C can be computed by a bottom-up divide-and-conquer \nparallel algorithm provided that there exist a set of constant-time functions F. ((C, C) . C) and three \nconstant-time functions \u00dfH :: B .F,\u00dfL :: F .F .C .F, \u00dfR :: F. C .F.F de.ned as follows. \u00dfH a = a(b, c) \n. gbac \u00dfL fb fa c = ah . fa (fb h, c) \u00dfR bfa fc = ah . fa (b, fc h) In Theorem 5.2, each function in \nF represents a closure raised by reduceS.We depict the implementation in Figure 6. The premise of Theorem \n5.2, namely existence set of constant\u00adtime functions F, may appears to be arti.cial. Infact, similar \nno\u00adtions have been used as a basis for developing ef.cient parallel al\u00adgorithms. A classic application \nis parallel evaluation of an arith\u00admetic expression that consists of +, -, \u00d7, and /, where F = {(ax y. \n(a \u00d7 (x . y)+ b)/(c \u00d7 (x . y)+ d)) | a, b, c, d . R ...{+, -, \u00d7,/}} [Brent1974]. Abrahamson et al. [1989] \n 5.3 Upward Accumulation Next, let us consider the upward accumulation skeleton, uAcc,pro\u00adposedby Gibbonset \nal. [1994].We derivea parallel implementa\u00adtion of uAcc by developing uAccS and uAccS, characterized as \nfollows. s2t (uAccS fgs)= uAcc fg (s2t s) s2c (uAccS fgs (reduce fg l, reduce fg r)) (uAcc f gl, uAcc \nfgr)= uAcc fg (s2c s (l, r)) Since uAcc relies on reduce, uAccS takes the reduce values of the missing \nsubtrees as arguments. Wewill calculate their recursivede.nitions.Weuse some short\u00adhand notations in \nthe following derivation: we omit function param\u00adeters f and g;we abbreviatereduce to red;we letlred \n= red f gl, rred = red f gr, luA = uAcc f gl, and ruA = uAcc fgr;we omit the case of R . s2t (uAccS (T \na)) = { de.nitions of uAccS, s2t, and uAcc } Tip (fa) = { de.nition of s2t }s2t (T (fa)) s2t (uAccS \n(N bac)) = { de.nitions of uAccS and s2t }uAcc (s2c a (s2t b, s2t c)) = { de.nitions of uAccS, uAccS, \nand redS }s2c (uAccS a (redS b, redS c)) (s2t (uAccS b), s2t (uAccS c)) = { de.nition of connectN } connectN \n(s2t (uAccS b)) (s2c (uAccS a (redS b, redS c))) (s2t (uAccS c)) = { de.nition of s2t } s2t (N (uAccS \nb)(uAccS a (redS b, redS c)) (uAccS c)) s2c (uAccS (H a)(lred ,rred )) (luA,ruA) = { de.nitions of uAccS, \ns2c, and uAcc } Bin luA (glred arred ) ruA = { de.nition of s2c } s2c (H (glred arred )) (luA,ruA) s2c \n(uAccS (L bac)(lred ,rred )) (luA,ruA) = { de.nitions of uAccS and s2c } uAcc (s2c a (s2c b (l, r), s2t \nc)) = { de.nitions of uAccS, uAccS, redS and redS } s2c (uAccS a (redS b (lred ,rred ), redS c)) (s2c \n(uAccS b (lred ,rred )) (luA,ruA), s2t (uAccS c)) = { de.nition of connectL } connectL (s2c (uAccS b \n(lred ,rred ))) (s2c (uAccS a (redS b (lred ,rred ), redS c))) (s2t (uAccS c)) (luA,ruA) = { de.nition \nof s2c } s2c (L (uAccS b (lred ,rred )) (uAccS a (redS b (lred ,rred ), redS c)) (uAccS c)) (luA,ruA) \n We have developed the following recursive de.nitions.  h h \u00dfH a fb\u00dfL fa \u00dfR fa L R H a H aN L R AAA \nhA A A A AAA h A Afa A A A AA fb fa A A A Afa A AA fc h fc H a T a a = fa a = .H ah N A A A A (b, c) \n(fb h, c)(b, fc h) Figure 7. Implementation of uAcc fg on Shunt trees: given the e e value h fromitsparent,valuesaresenttoits \nchildren;notethatthis must follow reduce fg depicted in Figure 6, and a, b, c, fb and fc are those in \nthat .gure. H eT e e N eL eR e AAA AAA AAA e AAA L eL eR e AeR A A A AA e R eL e uAccS fg (T a)= \nT (fa) uAccS fg (N bac) = N (uAccS fgb) (uAccS fga (reduceS f gb, reduceS fgc)) (uAccS fgc) uAccS fg \n(H a)(lred ,rred )= H (glred arred ) uAccS fg (L bac) h = L (uAccS f gbh) (uAccS fga (reduceS f gbh, \nreduceS fgc)) (uAccS fgc) uAccS fg (R bac) h = R (uAccS fgb) (uAccS fga (reduceS f gb, reduceS f gch)) \n(uAccS f gch) These recursive de.nitions show that we can perform uAccS in a divide-and-conquer manner \nif reduceS can. Moreover, they suggest an implementation similar to scanr: we .rst calculate reduceS \nin a bottom-up manner, as in Figure 6, and then distribute the calculated values by a top-down sweep, \nas in Figure 7. THEOREM 5.3. There exists a two-sweeps divide-and-conquer parallel algorithm for uAccS \nfg, provided that the premises of Theorem 5.2 hold. It is worth noting that, in contrast to reduce, it \nis nontrivial to implement uAcc with the Shunt contraction, because it is hard to capture the top-down \nsweep by Shunt contraction schemes.  5.4 Downward Accumulation Gibbons et al. [1994] also introduced \nanother skeleton, called downward accumulations, dAcc.We can derive its Shunt-tree ver\u00adsion as being \nsimilar to uAcc. The derivation is routine and there\u00adfore we show it in Appendix B. The following is \nthe developed program. dAccS fL fR e (T -)= T e dAccS fL fR e (N bac) = N (dAccS fL fR eL b) a (dAccS \nfL fR eR c) where (a, (eL,eR)) = dAccS fL fR ea dAccS fL fR e (H a)=(H e, (fL e a,fR ea)) dAccS fL fR \ne (L bac) =(L ba (dAccS fgeR c), (eL,eR)) where (a, (eL,eR)) = dAccS fL fR ea (b, (eL,eR)) = dAccS fL \nfR eL b dAccS fL fR e (R bac) =(R (dAccS fgeL b) a c, (eL,eR)) where (a, (eL,eR)) = dAccS fL fR ea (c, \n(eL,eR)) = dAccS fL fR eR c Note that the implementation on ST , namely dAccS, returns twoadditionalvalues \nthat are accumulation parameters for process\u00ading missing subtrees. As similar to the case of uAcc, the \nwhole (eL,eR)= fa e (eL,eR)= fa e (eL,eR)= fa e Figure 8. Implementation of dAcc fL fR on Shunt trees: \nthe top\u00addown phase (the lower picture) follows after the the bottom-up phase (the upper picture); in \neach phase, each node receives values from either its parent or its children and sends calculated values \nto others;thevariables used here correspondto thosein Theorem 5.4. computation can be performed in parallel \nif these accumulation pa\u00ad rameters can be calculated in a bottom-up sweep. THEOREM 5.4. There is a two-sweep \ndivide-and-conquer parallel algorithm for dAccS fL fR :: ST AB . ST CC, provided that thereexista setof \nconstant-time functions F. (C . (C, C)) and three constant-time functions \u00dfH :: B .F, \u00dfL :: F.F.F, and \n\u00dfR :: F.F.F, de.ned as follows. \u00dfH a = ae . (fL e a,fR ea) \u00dfL fa fb = ae . let (eL,eR)= fa e in fb eL \n\u00dfR fa fc = ae . let (eL,eR)= fa e in fc eR We depict the implementation in Figure 8.We .rst prepare the \naccumulation parameters and then performatop-down sweep so as to distribute them and thereby update the \nnode values. 6. Conclusion, Related Work, and Future Work We have discussed developments and applications \nof balanced tree structures in purely functional parallel programming.We have de\u00adveloped balanced trees \nby extracting and then freezing the com\u00adputational structures of parallel computations. The balanced \ntrees enable us to naturally implement rather complicated parallel algo\u00adrithms. Moreover,the balanced \ntrees areinfactvariantsofknown self-balancing tree structures, and hence, we can ef.ciently obtain well-balanced \nones. Although we have considered only programs that operate lists and binary trees, our method should \nbe able to copewiththoseon non-binarytreesbyfollowingthestudyofMori\u00adhata et al. [2009]. Furthermore, \nsince our method does not use any tree-speci.c properties, we believe that it can be extended to other \nparallel algorithms. Because balanced trees suggest strategies for distributing the input tree to each \nprocessor, their use would be effective on distributed-memory environments. We are developing balanced\u00adtree-based \nimplementations of developed parallel programs so as to con.rm their effectiveness. 6.1 Functional Parallel \nProgramming and Algorithmic Skeletons Our initial objective was to formalize and to provide purely func\u00adtional \nimplementations to algorithmic skeletons, especially data parallel skeletons. Data parallel skeletons \nsimultaneously perform the same operations on each element of large data. All skeletons we have considered \nare data parallel skeletons. Other examples of data parallel skeletons include list comprehension operations \nin NESL [Blelloch et al. 1994] and Google s MapReduce [Dean and Ghemawat 2004]. The effectiveness of \ndata parallel skeletons, especially those for lists or arrays (called list skeletons), are well recognized, \nand most of the modern parallel functional program\u00adming languagessupportlistskeletons[Fluetetal.2008;Kelleretal. \n2010; Peyton Jones et al. 2008].  An unresolved issue is the implementation of the underlying data. \nAs discussed in the introduction, the usual implementation of lists and trees can hardly support ef.cient \nparallel computations, and therefore, data parallel skeletons are usually implemented on a special data \nstructure. Designing of the data structure is a crucial task. It will determine what kind of skeletons \nand operations we can support and what parallel computational environments we can use. List skeletons \nare usually implemented based on arrays; how\u00adever,this implementation cannot support ef.cient structural \nmodi.\u00adcations. Manticore [Fluetetal.2008] adoptsa balanced tree called a rope. Tree skeletons, which \nare usually implemented on paral\u00adlel tree contraction algorithms [Gibbons et al. 1994; Matsuzaki 2007; \nMatsuzakietal. 2006b; Skillicorn 1996],haveasimilardif.\u00adculty. The original Shunt contraction algorithm \n[Abrahamson et al. 1989;Kosaraju and Delcher 1988] uses pointer-based implementa\u00adtion of trees; however, \nthis prohibits us from using the algorithm on distributed-memory environments. Matsuzaki [2007] used \nthe m\u00adbridge technique [Gazit et al. 1987; Reif 1993] so as to implement tree skeletons on distributed-memory \nenvironments; however, this implementation cannot support ef.cient structural modi.cations. Our method \nprovides a systematic way of developing imple\u00admentations of both underlying data structures and skeletons \non them.To the best of our knowledge, it is the .rst method of sys\u00adtematically developing skeletons. \nOur derivations result in balanced trees, which are suitable for most of the parallel computation envi\u00adronments \nand which support ef.cient structural updates. Our im\u00adplementations of skeletons are correct by construction. \nMoreover, our two-sweep algorithms for uAcc and dAcc are simpler than the existing ones [Gibbons et al. \n1994; Matsuzaki 2007].  6.2 Parallel Tree Contractions and Balanced Trees From parallel tree contraction \nalgorithms, we have developed Shunt trees, which are a variant of directed topology trees. Some researches \nhave also pointed out the relationship between self\u00adbalancing trees and parallel tree contractions. Frederickson \n[1997] proposed directed topology trees. He com\u00adpared them with parallel tree contraction, and included \nthe follow\u00adingexplanation. Note that rake and compress are primitivecon\u00adtraction operations used in the \noriginal parallel tree contraction by Miller and Reif [1985]. Since the rake-and-compress operations \ncited here were not designed to handle dynamic situations, there is no .exibility in choosing which operations \nto apply. Thus there is only one tree that can result from applying the operations to a giventreeduringone \niteration.Thislackof.exibilitywould cause global restructuring after an update, and thus would be quite \ninef.cient. Indeed, we have introduced .exibility of applying contraction op\u00aderations. The key to the \n.exibility is the introduction of a set of polymorphic functions that are observationally characterized. \nAcar et al. [2004] introduced a machine model that supports in\u00adcremental evaluation based on computational \ntraces. Theyapplied their methodtoarandomizedvariantofthe parallel tree contraction algorithmby Miller \nand Reif [1985], and obtaineda self-balancing tree structuretheynamedRC trees. Their methodisvery interesting \nbecause theydiscovered a new self-balancing tree structure, while we have mainly focused on reasoning \nand development of paral\u00adlel programs and have used existing structures for making our bal\u00adanced trees \nself-balanced.Acombinationofthetwomethods could be fruitful, though we did not use the RC trees because \ntheydo not guarantee the worst-case time.  6.3 Formalization of Parallel Algorithms We have formalized \nparallel algorithms using a family of poly\u00admorphic functions, i.e., list homomorphism schemes and Shunt \ncontraction schemes. This was inspired from those who formal\u00adize domain-speci.c procedures using polymorphic \nfunctions. Day et al. [1999] pointed out that Knuth s 0-1 principle [Knuth 1998] can be rephrased in \nterms of Reynolds relational parametricity. Voigtl\u00a8ander [2008, 2009] demonstrated several applications \nof parametricity. Here we have added another application, namely the development of parallel programs. \nWe have developed balanced trees from these schemes. The un\u00adderlyingideaisahylomorphism [Meijeretal.1991].Ahylomor\u00adphism \nis a function expressed by a composition of two functions: an anamorphism, which is a structural corecursion \nthat generates an intermediate structure,andacatamorphism, whichisastructural recursion that consumesthe \nintermediate structure.By virtueofin\u00adduction/coinduction on the intermediate structure,hylomorphisms \nare suitable for reasoning and manipulation [Hu et al. 1996, 1997; Takano and Meijer 1995]. Usually, \nthe intermediate data structures ofhylomorphismsare algebraicdata structures.Here,wehaveused balanced \ntrees, which are abstract data structures, as the interme\u00addiate data structures and showed that parametricity \nenables us to reason about suchhylomorphisms. Acknowledgments The authors are grateful for Zhenjiang \nHu for his collaborating discussions, and Tachio Terauchi and Takeshi Tsukada for their encouragements. \nThe authors are also grateful for anonymous reviewers of ICFP 11, POPL 10, and SPAA 09 for their helpful \ncomments. In particular, a similarity between Shunt trees and known self-balancing trees was originally \nsuggested by reviewers ofSPAA 09. The second authoris supportedby the JSPS Grant-in-Aid forYoung Scientists \n(B) 22700037. References K.R. Abrahamson,N. Dadoun,D.G. Kirkpatrick,andT.M. Przytycka.A simple parallel \ntree contraction algorithm. J. Algorithms,10(2):287 302, 1989. U. A. Acar, G. E. Blelloch, R. Harper, \nJ. L.Vittes, and S. L. M.Woo. Dy\u00adnamizing static algorithms, with applications to dynamic trees and his\u00adtory \nindependence. In Proc. theFifteenth AnnualACM-SIAM Sympo\u00adsium on Discrete Algorithms, SODA2004, pages \n531 540. SIAM, 2004.  R.P. Brent. The parallelevaluationof general arithmeticexpressions. J. ACM, 21(2):201 \n206, 1974. S.W. Bent, D. D. Sleator, and R. E.Tarjan. Biased search trees. SIAMJ. Comput., 14(3):545 \n568, 1985. R. S. Bird. An Introduction to the Theory of Lists. In Logic of Programming and Calculi of \nDiscrete Design, volume 36 of NATOASI SeriesF, pages 3 42. Springer-Verlag, 1987. G. E. Blelloch, J. \nC. Hardwick, J. Sipelstein, M. Zagha, and S. Chatterjee. Implementation of a portable nested data-parallel \nlanguage. J.Parallel Distrib. Comput., 21(1):4 14, 1994.  H.-J. Boehm, R. R. Atkinson, and M.F. Plass. \nRopes: An alternative to strings. Softw., Pract. Exper. 25(12): 1315 1330, 1995. R. M. Burstall and J. \nDarlington. Atransformation system for developing recursive programs. J.ACM, 24(1):44 67, 1977.  W.-N.Chin,A.Takano,andZ.Hu.Parallelizationvia \ncontext preservation. In Proc. the 1998 International Conference on Computer Languages, ICCL 98, pages \n153 162. IEEE Computer Society, 1998. M. I. Cole. Algorithmic Skeletons: Structural Management of Parallel \nComputation. MIT Press, 1989. N. A. Day, J. Launchbury, and J. Lewis. Logical abstractions in Haskell. \nIn Proc. the 1999 HaskellWorkshop. Utrecht University Department of Computer Science,Technical Report \nUU-CS-1999-28, October 1999. J. Dean and S. Ghemawat. MapReduce: Simpli.ed data processing on large clusters. \nIn Proc. 6th Symposium on Operating System Design and Implementation (OSDI 2004), pages 137 150. USENIX, \n2004. M. Fluet, M. Rainey, J. H. Reppy, and A. Shaw Implicitly-threaded par\u00adallelism in Manticore. In \nProc. the 13thACM SIGPLAN International Conference on Functional Programming, ICFP 2008, pages 119 130. \nACM, 2008. G. N. Frederickson. A data structure for dynamically maintaining rooted trees. J. Algorithms, \n24(1):37 65, 1997. H. Gazit, G. L. Miller, and S.-H. Teng. Optimal tree contraction in the EREW model. \nIn Proc. the PrincetonWorkshop on Algorithms, Archi\u00adtectures, andTechnology Issues for Modelsof Concurrent \nComputation, pages 139 156. Plenum Press, 1987. J. Gibbons,W. Cai, and D. B. Skillicorn. Ef.cient parallel \nalgorithms for tree accumulations. Sci. Comput. Program., 23(1):1 18, 1994. Z. Hu, H. Iwasaki, and M. \nTakeichi. Deriving structural hylomorphisms from recursivede.nitions. In Proc. the 1stACM SIGPLAN International \nConference on Functional Programming, ICFP 96, pages 73 82.ACM, 1996. Z. Hu, H. Iwasaki, M. Takeichi, \nand A. Takano. Tupling calculation eliminates multiple data traversals. In Proc. the 2nd ACM SIGPLAN \nInternational Conference on Functional Programming, ICFP 97, pages 164 175.ACM, 1997. Z.Hu,M.Takeichi,andW.-N. \nChin.Parallelizationin calculational forms. In POPL 98: Proc. the 25th ACM SIGPLAN-SIGACT Symposium on \nPrinciples of Programming Languages, pages 316 328.ACM, 1998. Z. Hu, H. Iwasaki, and M.Takeichi. An \naccumulative parallel skeleton for all. In Programming Languages and Systems, 11th European Symposium \non Programming, ESOP 2002, Proceedings, volume 2305 of Lecture Notes in Computer Science, pages 83 97. \nSpringer, 2002. G. Keller, M. M. T. Chakravarty, R. Leshchinskiy, S. L. P. Jones, and B. Lippmeier. \nRegular, shape-polymorphic, parallel arrays in haskell. In  Proc. the 15thACM SIGPLAN International \nConference on Functional Programming, ICFP 2010, pages 261 272.ACM, 2010. D. Knuth. The Art of Computer \nProgramming,volume3. AddisonWesley Longman, second edition edition, 1998. S. R. Kosaraju and A. L. Delcher. \nOptimal parallel evaluation of tree\u00adstructured computations by raking. In VLSI Algorithms and Architec\u00adtures, \n3rd Aegean Workshop on Computing, AWOC 88, Proceedings, volume 319 of Lecture Notes in Computer Science, \npages 101 110. Springer, 1988. A.K\u00a8uhnemann, R. Gl\u00a8uck, and K. Kakehi. Relating accumulative and non-accumulative \nfunctional programs. In Rewriting Techniques and Applications, 12th International Conference, RTA 2001, \nProceedings, volume 2051 of Lecture Notes in Computer Science, pages 154 168. Springer, 2001. K. Matsuzaki. \nEf.cient implementation of tree accumulations on distributed-memory parallel computers. In Computational \nScience -ICCS 2007,7th International Conference,Proceedings,PartII, volume 4488 of Lecture Notes in Computer \nScience, pages 609 616. Springer, 2007. K. Matsuzaki, Z. Hu, and M.Takeichi. Towards automatic parallelization \nof tree reductions in dynamic programming. In SPAA 2006: Proc. the 18th AnnualACM SymposiumonParallel \nAlgorithmsandArchitectures, pages 39 48.ACM, 2006a. K. Matsuzaki,Z.Hu,andM.Takeichi.Parallelskeletonsfor \nmanipulating general trees. Parallel Comput., 32(7-8):590 603, 2006b.  E. Meijer,M.M.Fokkinga, andR.Paterson. \nFunctional programming with bananas, lenses, envelopes and barbed wire. In Functional Programming Languages \nand Computer Architecture, 5thACM Conference, Proceed\u00adings,volume 523 ofLectureNotes in Computer Science,pages \n124 144. Springer, 1991. G. L. Miller and J. H. Reif. Parallel tree contraction and its application. \nIn 26th Annual Symposium onFoundationsof Computer Science, pages 478 489. IEEE, 1985. A. Morihata and \nK. Matsuzaki. Automatic parallelization of recursive functions using quanti.er elimination. In Functional \nand Logic Pro\u00adgramming, 10th International Symposium, FLOPS 2010, Proceedings, volume 6009 of Lecture \nNotes in Computer Science, pages 321 336. Springer, 2010. A. Morihata, K. Matsuzaki, Z. Hu, and M. Takeichi. \nThe third homo\u00admorphism theorem on trees: Downward&#38; upward lead to divide-and\u00adconquer. In Proc. the \n36thACM SIGPLAN-SIGACT Symposium on Prin\u00adciples of Programming Languages, POPL 2009, pages 177 185.ACM, \n2009. S. Peyton Jones, editor. Haskell 98 Language and Libraries: The Revised Report. Cambridge University \nPress, Cambridge, UK, 2003. S. L. Peyton Jones, R. Leshchinskiy, G.Keller, and M. M.T. Chakravarty. Harnessing \nthe multicores: Nested data parallelism in haskell. In IARCS Annual ConferenceonFoundationsof SoftwareTechnologyand \nTheoret\u00adical Computer Science, FSTTCS 2008, volume 08004 of Dagstuhl Sem\u00adinar Proceedings. Internationales \nBegegnungs-undForschungszentrum fuer Informatik (IBFI), 2008. J. H. Reif, editor. Synthesis of Parallel \nAlgorithms. Morgan Kaufmann Publishers, 1993. J. C. Reynolds. Types, abstraction and parametric polymorphism. \nInf. Process., 83:513 523, 1983. D. B. Skillicorn. Structured parallel parallel computation in structured \ndocuments. J. UCS, 3(1):42 68, 1997. D. B. Skillicorn. Parallel implementation of tree skeletons. J. \nParallel Distrib. Comput., 39(2):115 125, 1996. A.Takano and E. Meijer. Shortcut deforestation in calculational \nform. In Conference Recordof FPCA 95: SIGPLAN-SIGARCH-WG2.8 Confer\u00adence on Functional Programming Languages \nand Computer Architec\u00adture, pages 306 313.ACM, 1995. J.Voigtl\u00a8Concatenate, In Proc. ander. reverse and \nmap vanish for free. the Seventh ACM SIGPLAN International Conference on Functional Programming (ICFP \n02), pages 14 25, 2002. J.Voigtl\u00a8ander. Much ado about two (pearl): a pearl on parallel pre.x computation. \nIn Proc. the 35th ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, POPL 2008, pages \n29 35. ACM, 2008. J.Voigtl\u00a8ander. Bidirectionalization for free! (pearl). In Proc. the 36th ACM SIGPLAN-SIGACT \nSymposium on Principles of Programming Languages, POPL 2009, pages 165 176.ACM, 2009. P. Wadler. Theorems \nfor free! In FPCA 89 Conference on Functional Programming Languages and Computer Architecture, pages \n347 359. ACM, 1989.  P.Wadler. Deforestation:Transforming programs to eliminate trees. Theor. Comput. \nSci., 73(2):231 248, 1990. A. Implementing Shunt Tree by Directed Topology Tree We .rst review directed \ntopology trees proposed by Frederickson [1997]. The following is the datatype of directed topology trees \nthat represent binary trees, Tree a(. data DTT a( = Leaf a | Rake (DTT a()(DTT a() data DTT a( = Rake \nL ( (DTT a() | Rake R (DTT a() ( | Compress (DTT a()(DTT a()  Given a binary tree, a directed topology \ntree is a binary tree that retains recursive merging of the given tree. DTT and DTT respectively represent \na tree and a cluster one of whose subtrees are missing. Rake,Rake L ,andRake R mergeatreetoits parent \ncluster. Compress merges two adjunct clusters each of which has a unique child cluster. Note that the \nnames of the merging operations are different from thosegivenby Frederickson [1997].We borrow these names \nfrom the tree contraction algorithmby Miller and Reif [1985] so as to explicitly show the relationship \nbetween directed topology trees and the tree contraction. The following function d2t converts directed \ntopology trees to binary trees. d2t :: DTT a( . Tree a( d2t (Leaf a)= Tip a d2t (Rake d d)= d2t d (d2t \nd) d2t :: DTT a( . Tree a( . Tree a( d2t (Rake L bd)= at . Bin tb (d2t d) d2t (Rake R db)= at . Bin \n(d2t d) bt d2t (Compress d1 d 2)= d2t d1 . d2t d2 Directed topology trees must satisfy some structural \nproperties. Here we omit them because they are important only for guarantee\u00ading height and ef.cient updating \nand thus do not affect our discus\u00adsion. Th following function d2s establishes a relationship between \ndirected topology trees and Shunt trees. d2s :: DTT a( . ST a( d2s (Leaf a)= T a d2s (Rake d d)= case \nd2s d of (c, Left s) . N (d2s d) cs (c, Right s) . N sc (d2s d) d2s :: DTT a( . (ST a (, Either (ST a()(ST \na()) d2s (Rake L bd) =(H b, Left (d2s d)) d2s (Rake R db) =(H b, Right (d2s d)) d2s (Compress d 1 d2 \n)= let (c2,s2)= d2s d 2 in case d2s d1 of (c1, Left s1) . (L c2 c1 s1,s2) (c1, Right s1) . (R s1 c1 \nc2,s2) It is not dif.cult to see s2t . d2s = d2t;moreover, we can effec\u00adtively retain Shunt trees by \ndirected topology trees because local modi.cations of directed topology trees are translated to local \nmod\u00adi.cation of Shunt trees. These observations justify Theorem 4.8. B. Developing Shunt-tree Version \nof dAcc We consider the Shunt-tree version of dAcc characterized as fol\u00adlows. Note that dAccS returns \naccumulation parameters so as to process the missing subtrees. s2t (dAccS fL fR es)= dAcc fL fR e (s2t \ns) let (s, (eL,eR)) = dAccS fL fR es in s2c s (dAcc fL fR eL l, dAcc fL fR eR r) = dAcc fL fR e (s2c \ns (l, r)) We develop their de.nitions as follows, where we omit the function parameters and the case \nof R . s2t (dAccS e (T -)) = { de.nitions of dAccS and s2t }dAcc e (Tip -) = { de.nitions of dAcc and \ns2t }s2t (T e) s2t (dAccS e (N bac)) = { de.nitions of dAccS and s2t }dAcc e (s2c a (s2t b, s2t c)) = \n{ de.nitions of dAccS and dAccS } let (a, (eL,eR)) = dAccS ea in s2c a (s2t (dAccS eL b), s2t (dAccS \neR c)) = { de.nition of s2t } let (a, (eL,eR)) = dAccS ea in s2t (N (dAccS eL b) a (dAccS eR c)) let \n(s, (eL,eR)) = dAccS e (H a) in s2c s (dAcc eL l, dAcc eR r) = { de.nitions of dAccS and s2c }dAcc e \n(Bin lar) = { de.nition of dAcc }Bin (dAcc (fL ea) l) e (dAcc (fR ea) r) = { de.nition of s2c }s2c (H \ne)(dAcc (fL ea) l, dAcc (fR ea) r) let (s, (eL,eR)) = dAccS e (L bac) in s2c s (dAcc eL l, dAcc eR r) \n= { de.nitions of dAccS and s2c }dAcc e (s2c a (s2c l (l, r), s2t r)) = { de.nitions of dAccS and dAccS \n}let (a, (eL,eR)) = dAccS ea (b, (eL,eR)) = dAccS eL b in s2c a (s2c b (dAcc eL l, dAcc eR r), s2t (dAccS \neR c)) = { de.nition of s2c }let (a, (eL,eR)) = dAccS ea (b, (eL,eR)) = dAccS eL b in s2c (L ba (dAccS \neR c)) (dAcc eL l, dAcc eR r)    \n\t\t\t", "proc_id": "2034773", "abstract": "<p>Divide-and-conquer is an important technique in parallel programming. However, algebraic data structures do not fit divide-and-conquer parallelism. For example, the usual pointer-based implementation of lists cannot efficiently be divided at their middle, which prevents us from developing list-iterating divide-and-conquer parallel programs. Tree-iterating programs possibly face a similar problem, because trees might be ill-balanced and list-like shapes. This paper examines parallel programming based on balanced trees: we consider balanced-tree structures and develop recursive functions on them. By virtue of their balancing nature, either bottom-up or top-down recursive functions exploit divide-and-conquer parallelism. Our main contribution is to demonstrate the promise of this approach. We propose a way of systematically developing balanced trees from parallel algorithms, and then, we show that efficient parallel programs on them can be developed by equational reasoning powered by Reynolds' relational parametricity. We consider functions that operate either lists or binary trees, and show that our methods can uniformly deal with both cases. The developed parallel programs are purely functional, correct by construction, and sometimes even simpler than known algorithms.</p>", "authors": [{"name": "Akimasa Morihata", "author_profile_id": "81372591868", "affiliation": "Tohoku University, Sendai-shi, Miyagi-prefecture, Japan", "person_id": "P2801383", "email_address": "morihata@riec.tohoku.ac.jp", "orcid_id": ""}, {"name": "Kiminori Matsuzaki", "author_profile_id": "81316489698", "affiliation": "Kochi University of Technology, Kami-shi, Kochi-prefecture, Japan", "person_id": "P2801384", "email_address": "matsuzaki.kiminori@kochi-tech.ac.jp", "orcid_id": ""}], "doi_number": "10.1145/2034773.2034791", "year": "2011", "article_id": "2034791", "conference": "ICFP", "title": "Balanced trees inhabiting functional parallel programming", "url": "http://dl.acm.org/citation.cfm?id=2034791"}