{"article_publication_date": "09-19-2011", "fulltext": "\n Frenetic: A Network Programming Language Nate Foster Rob Harrison Michael J. Freedman Cornell University \nPrinceton University Princeton University Christopher Monsanto Jennifer Rexford Princeton University \nPrinceton University Abstract Modern networks provide a variety of interrelated services includ\u00ading routing, \ntraf.c monitoring, load balancing, and access control. Unfortunately, the languages used to program today \ns networks lack modern features they are usually de.ned at the low level of abstraction supplied by the \nunderlying hardware and they fail to provide even rudimentary support for modular programming. As a result, \nnetwork programs tend to be complicated, error-prone, and dif.cult to maintain. This paper presents Frenetic, \na high-level language for program\u00adming distributed collections of network switches. Frenetic provides \na declarative query language for classifying and aggregating net\u00adwork traf.c as well as a functional \nreactive combinator library for describing high-level packet-forwarding policies. Unlike prior work in \nthis domain, these constructs are by design fully com\u00adpositional, which facilitates modular reasoning \nand enables code reuse. This important property is enabled by Frenetic s novel run\u00adtime system which \nmanages all of the details related to installing, uninstalling, and querying low-level packet-processing \nrules on physical switches. Overall, this paper makes three main contributions: (1) We an\u00adalyze the state-of-the \nart in languages for programming networks and identify the key limitations; (2) We present a language \ndesign that addresses these limitations, using a series of examples to moti\u00advate and validate our choices; \n(3) We describe an implementation of the language and evaluate its performance on several benchmarks. \nCategories and Subject Descriptors D.3.2 [Programming Lan\u00adguages]: Language Classi.cations Specialized \napplication lan\u00adguages General Terms Languages, Design Keywords Network programming languages, domain-speci.c \nlanguages, functional reactive programming, OpenFlow 1. Introduction Today s networks consist of hardware \nand software components that are closed and proprietary. The dif.culty of changing these components has \nhad a chilling effect on innovation, and forced network administrators to express policies through complicated \nand frustratingly brittle interfaces. As discussed in recent a New York Permission to make digital or \nhard copies of all or part of this work for personal or classroom use is granted without fee provided \nthat copies are not made or distributed for pro.t or commercial advantage and that copies bear this notice \nand the full citation on the .rst page. To copy otherwise, to republish, to post on servers or to redistribute \nto lists, requires prior speci.c permission and/or a fee. ICFP 11, September 19 21, 2011, Tokyo, Japan. \nAlec Story David Walker Cornell University Princeton University Times article [30], the rise of data \ncenters and cloud computing have brought these problems into sharp relief and led a number of networks \nresearchers to reconsider the fundamental assumptions that underpin today s network architectures. In \nparticular, signi.cant momentum has gathered behind Open-Flow, a new platform that opens up the software \nthat controls the network while also allowing packets to be processed using fast, commodity switching \nhardware [31]. OpenFlow de.nes a standard interface for installing .exible packet-forwarding rules on \nphysical network switches using a programmable controller that runs sep\u00adarately on a stock machine. The \nmost well-known controller plat\u00adform is NOX [20], though there are several others [1, 8, 25, 39]. OpenFlow \nis supported by a number of commercial Ethernet switch vendors, and has been deployed in several campus \nand backbone networks. Using OpenFlow, researchers have already created a va\u00adriety of controller applications \nthat introduce new network func\u00adtionality, like .exible access control [9, 33], Web server load bal\u00ad \nancing [21, 40], energy-ef.cient networking [22], and seamless virtual-machine migration [18]. Unfortunately, \nwhile OpenFlow and NOX now make it possible to implement exciting new network services, they do not make \nit easy. OpenFlow programmers must constantly grapple with several dif.cult challenges. First, networks \noften perform multiple tasks, like routing, access control, and traf.c monitoring. Unfortunately, decoupling \nthese tasks from each other and implementing them independently in separate modules is effectively impossible, \nsince packet-handling rules (un)installed by one module often interfere with overlapping rules (un)installed \nby other modules. Second, the OpenFlow/NOX interface is de.ned at a very low level of abstraction. For \nexample, the OpenFlow rule algebra di\u00adrectly re.ects the capabilities of the switch hardware (e.g., bit \npat\u00adterns and integer priorities). Simple high-level concepts such as set difference require multiple \nrules and priorities to implement cor\u00adrectly and more powerful wildcard rules are a limited hardware \nresource that programmers must manage by hand. Third, controller programs only receive events for packets \nthe switches do not know how to handle. Code that installs a forward\u00ading rule might prevent another, \ndifferent event-driven call-back from being triggered. As a result, writing programs for Open\u00adFlow/NOX \nquickly becomes a dif.cult exercise in two-tiered programming programmers must simultaneously reason \nabout the packets that will processed on switches and those that will be processed on the controller. \nFourth, because a network of switches is a distributed system, it is susceptible to various kinds of \nrace conditions. For example, a common NOX programming idiom is to handle the .rst packet of each network \n.ow on the controller and install switch-level rules to handle the remaining packets. However, such programs \ncan be susceptible to errors if the second, third, or fourth packets in a c Copyright &#38;#169; 2011 \nACM 978-1-4503-0865-6/11/09. . . $10.00  .ow arrive before the appropriate switch-level rule is computed \nand installed on the switches in the network. To address these challenges, we present Frenetic, a new \npro\u00adgramming language for networks. Frenetic is organized around two levels of abstraction: (1) a set \nof source-level operators for con\u00adstructing and manipulating streams of network traf.c, and (2) a run-time \nsystem that handles all of the details of installing and uninstalling low-level rules on switches. The \nsource-level operators draw on previous work on declarative database query languages and functional reactive \nprogramming (FRP) and are carefully con\u00adstructed to support the following key principles: Declarative \nDesign. Where possible, we consider what the pro\u00adgrammer might want, rather than how the hardware implements \nit. Hence, in many cases, we provide intuitive, high-level primitives, even though they are not directly \nsupported by the hardware. Modular Design. We have designed Frenetic s primitives to have limited network-wide \neffects and semantics that can be stated in\u00addependently of the context in which they are used. This facilitates \nbuilding modular programs with reuseable parts. Single-tier Programming. Frenetic programmers do not \nhave to worry that installing packet-handling rules may prevent the con\u00adtroller from analyzing other \ntraf.c. On the contrary, Frenetic sup\u00adports a see-every-packet abstraction which guarantees that every \npacket is available for analysis, thereby side-stepping the many complexities of today s two-tiered programming \nmodel. Race-free Semantics. Because Frenetic queries supply the run\u00adtime system with information about \nwhat programmers want, the run-time can suppress super.uous packets that arrive at the con\u00adtroller due \nto network race conditions. Automatic race detection and packet suppression simpli.es the programming \nmodel. Cost Control. In general, a danger of adopting high-level, declar\u00adative features is that it may \ndif.cult for users to understand or con\u00adtrol the computational costs of the abstractions they use. To \navoid this pitfall, Frenetic gives programmers guidance concerning the costs of programming constructs. \nIn particular, the query language is carefully de.ned so that the core query logic can be executed on \nnetwork switches, thereby keeping most packets in the fast path. The above principles make Frenetic programs \nrobust, compact, easy to write, easy to understand and easy to modify. Hence, to summarize, this paper \nmakes the following contributions: Analysis of OpenFlow/NOX dif.culties (Section 3): Using our combined \nexpertise in programming languages and networks, we identify weaknesses of the current model that modern \npro\u00adgramming language principles can overcome.  Frenetic language design (Section 4): Applying ideas \nfrom the disparate .elds of database query languages and functional re\u00adactive programming, we present \nand analyze our design choices for Frenetic, a language for programming OpenFlow networks.  Frenetic \nimplementation (Section 5): We describe Frenetic s implementation architecture, paying particular attention \nto the run-time system the enabling technology that allows us to raise the level of abstraction without \nsacri.cing performance.  Evaluation (Section 6): We discuss several applications imple\u00ad mented in Frenetic \nand NOX and compare them on several met\u00adrics: lines of code, controller load, and total traf.c. The results \ndemonstrate that Frenetic programs are more concise than their NOX counterparts and yet achieve comparable \nperformance.  2. Background on OpenFlow and NOX This section presents the main features of OpenFlow \nand NOX. To keep the presentation simple, we have elided a few details that are Integers n Rules r ::= \n(pat, pri, t, [a1,...,anD] Patterns pat ::= {h1 : n1,...,hk : nk} Priorities pri ::= n Timeouts t ::= \nn | None Actions a ::= output(op) | modify(h, n) Headers h ::= in port | vlan | dl src | dl dst | dl \ntype | nw src | nw dst | nw proto | tp src | tp dst Ports op ::= n | flood | controller Figure 1. OpenFlow \nSyntax. Pre.xes dl, nw, and tp denote data link (MAC), network (IP), and transport (TCP/UDP), respectively. \nnot important for understanding Frenetic. Readers interested in a complete description may consult the \nOpenFlow speci.cation [3]. Overview. In an OpenFlow network, a centralized controller manages a distributed \ncollection of switches. While packets .owing through the network may be processed by the centralized \ncontroller, doing so is orders of magnitude slower than processing those pack\u00adets on the switches. Hence, \none of the primary functions of the controller is to con.gure the switches so that they process the vast \nmajority of packets and only a few packets from new or unexpected .ows need to be handled on the controller. \nCon.guring a switch primarily involves installing entries in its .ow table: a set of rules that specify \nhow packets should be pro\u00adcessed. A rule consists of a pattern that identi.es a set of packets, an integer \npriority that disambiguates rules with overlapping pat\u00adterns, an optional integer timeout that indicates \nthe number of sec\u00adonds until the rule expires, and a list of actions that speci.es how packets should \nbe processed. For each rule in its .ow table, the switch maintains a set of counters that keep track \nof basic statistics concerning the number and total size of packets processed. Rules are de.ned formally \nby the grammar in Figure 1. A pat\u00ad tern is a list of pairs of header .elds and integer values, which \nare interpreted as equality constraints. For instance, the pattern {nw src : 10.0.0.1, tp dst : 80} matches \npackets from source IP address 10.0.0.1 going to destination port 80. We use stan\u00addard notation for values \nin common header .elds e.g., writing 10.0.0.1 instead of 167772161. Any header .elds not ap\u00adpearing in \na pattern are unconstrained. We call rules with uncon\u00adstrained .elds wildcard rules. OpenFlow Switches. \nWhen a packet arrives at a switch, the switch processes it in three steps: 1. It selects a rule from \nits .ow table whose pattern matches the packet. If there are no matching rules, the switch sends the \npacket to the controller for processing. Otherwise, if there are multiple matching rules, it picks the \nexact-match rule (i.e., the rule whose pattern matches every header .eld in the packet) if one exists, \nor a wildcard rule with highest priority if not. 2. It updates the byte and packet counters associated \nwith the rule. 3. It applies the actions listed in the rule to the packet in order, or drops the packet \nif the list is empty.  The action output(op) instructs the switch to forward the packet out on port \nop, which can either be a physical switch port n or one of the virtual ports flood or controller, where \nflood for\u00adwards it out on all physical ports (except the ingress port) and controller sends it to the \ncontroller. The action modify(h, n) instructs the switch to rewrite the header .eld h to n. The list \nof actions may contain both output and modify actions e.g., [modify(nw src, 10.0.0.1), output(2), output(controller)] \n  Figure 2. Simple Topology. rewrites the source IP address of the packet to 10.0.0.1 and then outputs \nit on switch port 2 and sends it to the controller. NOX Controller. The controller manages the set of \nrules installed on the switches in the network by reacting to network events. Most controllers are currently \nbased on NOX, which is a simple operating system for networks that provides primitives for managing events \nas well as functions for communicating with switches [20]. NOX de.nes a number of events:  packet in(switch, \nport, packet), triggered when switch for\u00adwards a packet received on physical port to the controller; \n  stats in(switch, xid, pattern, packets, bytes), triggered when switch returns the packets and bytes \ncounters in response to a request for statistics about rules contained in pattern. The xid parameter \nrepresents an identi.er for the request.  .ow removed(switch, pattern, packets, bytes), triggered when \na rule with pattern exceeds its timeout and is removed from switch s .ow table. The packets and bytes \nparameters contain the values of the counters for the evicted rule.  switch join(switch), triggered \nwhen switch joins the network.  switch exit(switch), triggered when switch exits the network.  port \nchange(switch, port, up), triggered when the link at\u00adtached to a given physical port on switch goes up \nor down. The up parameter represents the new status of the link.  NOX also provides functions for sending \nmessages to switches: install(switch, pattern, priority, timeout, actions), installs a rule with the \ngiven pattern, priority, timeout, and actions in the .ow table of switch.  uninstall(switch, pattern), \nremoves all rules contained in pattern from the .ow table of switch.  send(switch, packet, action), \nsends the given packet to switch and applies action to it there.  query stats(switch, pattern), issues \na request for statistics from all rules contained in pattern on switch and returns a request identi.er \nxid that can be used to match up the asyn\u00adchronous response from the switch.  The program running on \nthe controller de.nes a handler for each of the events built into NOX, but may otherwise be structured \nas an arbitrary program. Example. To illustrate the use of OpenFlow, consider a controller program written \nin Python that implements a simple repeater hub. Suppose that the network has a single switch connected \nto a pool of internal hosts on port 1 and a wide-area network on port 2, as shown in Figure 2. The switch \njoin handler below invokes the repeater when the switch joins the network. The repeater function installs \nrules on switch s that instruct the switch to forward packets from port 1 to port 2 and vice versa. def \nswitch_join(switch): repeater(switch) def repeater(switch): pat1 = {in_port:1} pat2 = {in_port:2} install(switch,pat1,DEFAULT,None,[output(2)]) \ninstall(switch,pat2,DEFAULT,None,[output(1)]) Note that both calls to install use the DEFAULT priority \nlevel and use None as the timeout, indicating that the rules are permanent. 3. Analysis of OpenFlow/NOX \nDif.culties OpenFlow provides a standard interface for manipulating the rules installed on switches, \nwhich goes a long way toward making net\u00adworks programmable. However, the programming model currently \nprovided by NOX has several de.ciencies that make it dif.cult to use in practice. This section presents \nfour of the most substantial dif.culties that arise when writing programs for OpenFlow/NOX. For concreteness, \nwe focus on the NOX controller but other con\u00adtrollers for OpenFlow such as Onix [25], Beacon [1], and \nNet\u00ad tle [39] suffer from similar issues. 3.1 Interactions Between Concurrent Modules The .rst issue \nis that NOX programs do not compose. Suppose that we want to extend the repeater hub to monitor the total \nnumber of bytes of incoming web traf.c. Rather than counting the web traf.c at the controller, a monitoring \napplication could install rules for web traf.c, and periodically poll the byte and packet counters associated \nwith those rules to collect the necessary statistics: def monitor(switch): pat = {in_port:2,tp_src:80} \ninstall(switch,pat,DEFAULT,None,[]) query_stats(switch,pat) def stats_in(switch,xid,pattern,packets,bytes): \nprint bytes sleep(30) query_stats(switch,pattern) The monitor function installs a rule that matches all \nincoming packets with TCP source port 80 and issues a query for the counters associated with that rule. \nThe stats_in handler receives the re\u00adsponse from the switch, prints the byte count to the console, sleeps \nfor 30 seconds, and then issues the next query. Ideally, we would be able to compose this program with \nthe repeater program to obtain a program that forwards packets and monitors traf.c: def repeater_monitor_wrong(switch): \nrepeater(switch) monitor(switch) Unfortunately, naively composing the two programs in this way will \nnot work due to interactions between the rules installed by each program. In particular, because the \nprograms install overlapping rules on the switch, when a packet arrives from port 80 on the source host, \nthe switch is free to process the packet using either rule. But using the repeater rule will not update \nthe counters needed for monitoring, while using the monitor rule will break the repeater program because \nits list of actions is empty (i.e., packets will be dropped). To obtain the desired behavior, we have \nto manually combine the forwarding logic from the .rst program with the monitoring policy from the second: \n def repeater_monitor(switch): pat1 = {in_port:1} pat2 = {in_port:2} pat2web = {in_port:2,tp_src:80} \ninstall(switch,pat1,[output(2)],DEFAULT) install(switch,pat2web,[output(1)],HIGH) install(switch,pat2,[output(1)],DEFAULT) \nquery_stats(switch,pat2web) Performing this combination is non-trivial: the pat2web rule needs to include \nthe output(1) action from the repeater program, and must be installed with HIGH priority to resolve the \noverlap with the pat2 rule. In general, composing NOX programs requires careful, manual effort on the \npart of the programmer to preserve the semantics of the original programs. This makes it nearly impossible \nto factor out common pieces of functionality into reusable libraries and also prevents compositional \nreasoning about programs.  3.2 Low-Level Programming Interface Another dif.culty stems from the low-level \nnature of the program\u00adming interface, which is derived from the features of the switch hardware rather \nthan being designed for ease of use. This makes programs unnecessarily complicated, as they must describe \nlow\u00adlevel details that do not affect the overall behavior of the program. For example, suppose that we \nwant to extend the repeater and mon\u00aditoring program to monitor all incoming web traf.c except traf\u00ad.c \ndestined for an internal server (connected to port 1) at address 10.0.0.9. To do this, we need to express \na logical difference of patterns, but OpenFlow patterns can only directly express positive constraints. \nThus, to simulate the difference between two patterns, we have to install two overlapping rules on the \nswitch, using prior\u00adities to disambiguate between them. def repeater_monitor_noserver(switch): pat1 = \n{in_port:1} pat2 = {in_port:2} pat2web = {in_port:2,tp_src:80} pat2srv = {in_port:2,nw_dst:10.0.0.9,tp_src:80} \ninstall(switch,pat1,DEFAULT,None,[output(2)]) install(switch,pat2srv,HIGH,None,[output(1)]) install(switch,pat2web,MEDIUM,None,[output(1)]) \ninstall(switch,pat2,DEFAULT,None,[output(1)]) query_stats(switch,pat2web) This program uses a separate \nrule to process web traf.c going to the internal server pat2srv matches incoming web packets going to \nthe internal server, while pat2web matches all other incoming web packets. It also installs pat2srv at \nHIGH priority to ensure that the pat2web rule only processes (and counts!) packets going to hosts other \nthan the internal server. Describing packets using the low-level patterns supported by OpenFlow switches \nis cumbersome and error-prone. It forces pro\u00adgrammers to use multiple rules and priorities to encode \npatterns that could be easily expressed using natural operations such as negation, difference, and union. \nIt adds unnecessary clutter to programs and further complicates reasoning about their behavior. 3.3 \nTwo-Tiered System Architecture A third challenge stems from the two-tiered architecture used in NOX, \nwhere a controller program manages the network by in\u00adstalling and uninstalling switch-level rules. This \nindirection forces the programmer to specify the communication patterns between the controller and switch \nand deal with tricky concurrency issues such as coordinating asynchronous events. Consider extending \nthe orig\u00adinal repeater program to monitor the total amount of incoming traf\u00ad.c by destination host. Unlike \nthe previous examples, we cannot install all of the rules we need in advance because, in general, we \nwill not know the address of each host a priori. Instead, the con\u00adtroller must dynamically install rules \nfor the packets seen at run time. def repeater_monitor_hosts(switch): pat = {in_port:1} install(switch,pat,DEFAULT,None,[output(2)]) \ndef packet_in(switch,inport,packet): if inport == 2: mac = dstmac(packet) pat = {in_port:2,dl_dst:mac} \ninstall(switch,pat,DEFAULT,None,[output(1)]) query_stats(switch,pat) The repeater_monitor_hosts function \ninstalls a single rule that handles all outgoing traf.c. Initially, the .ow table on the switch does \nnot contain any entries for incoming traf.c, so the switch sends all packets that arrive at port 2 to \nthe controller. This causes the packet_in handler to be invoked; it processes each packet by installing \na rule that handles future packets. Note that the controller only sees one incoming packet per host the \nrule processes future traf.c to that host directly on the switch. As this example shows, NOX programs \nare actually imple\u00admented using two programs one on the controller and another on the switch. While this \ndesign is essential for ef.ciency, the two-tiered architecture makes applications dif.cult to read and \nreason about, because the behavior of each program depends on the other e.g., installing/uninstalling \nrules on the switch changes which packets are sent up to the controller. In addition, the con\u00adtroller \nprogram must specify the communication patterns between the two programs and deal with subtle concurrency \nissues e.g., if we were to extend the example to monitor both incoming and out\u00adgoing traf.c, the controller \nwould have to issue multiple queries for the statistics for each host and synchronize the resulting callbacks. \nAlthough NOX makes it possible to manage networks using ar\u00adbitrary general-purpose programs, its two-tiered \narchitecture forces programmers to specify the asynchronous and event-driven inter\u00adaction between the \nprograms running on the controller and the switches in the network. In our experience, these details \nare a sig\u00adni.cant distraction and a frequent source of bugs.  3.4 Network Race Conditions One of the \ncorollaries of NOX s explicit two-tier programming model is that programs are susceptible to subtle network \nrace con\u00additions. For example, a common NOX programming idiom is to analyze the .rst packet of every \n.ow and calculate an action to apply to all future packets in the same network .ow. In fact, this is \nhow the repeater monitor hosts example described in the previous subsection worked. Unfortunately, our \nstatement that the packet in handler processes each packet by installing a rule that handles all future \npackets to the same host was a simpli.cation. The installed rule usually handles all future packets but \nnot al\u00adways! If a new packet in the same .ow arrives before the switch has been able to install the new \nrule, that new packet will also be sent up to the controller. Consequently, if the controller routines \nare not carefully crafted to be idempotent when receiving multiple unexpected packets in the same .ow, \nthey will fail. 4. Frenetic Language Design Frenetic is a new, domain-speci.c language for programming \nOpenFlow networks. It is embedded in Python and comprises two integrated sublanguages: (1) a limited, \nbut high-level and declara\u00adtive network query language, and (2) a general-purpose, functional and reactive \nnetwork policy management library. The language of\u00adfers a number of features that make programming more \nconvenient  Queries q ::= Select(a) * Where(fp) * GroupBy([qh1,..., qhn]) * SplitWhen([qh1,..., qhn]) \n* Every(n) * Limit(n) Aggregates a ::= packets | sizes | counts Headers qh ::= inport | srcmac | dstmac \n| ethtype | vlan | srcip | dstip | protocol | srcport | dstport | switch Patterns fp ::= true fp() | \nqh fp(n) | and fp([fp1 ,..., fpn ]) | or fp([fp1 ,..., fpn ]) | diff fp(fp1 , fp2 ) | not fp(fp) Figure \n3. Frenetic query syntax including a single-tier, see-every-packet abstraction; strong com\u00adpositionality \nproperties; a clear cost model; and a simple, race-free semantics. In the following subsections, we present \nthe main fea\u00adtures of the language and explain its semantic properties in detail. 4.1 The Network Query \nLanguage The network query sublanguage allows Frenetic programs to read the state of network. To implement \nthese reads ef.ciently, the Fre\u00adnetic run-time system changes the state of the network by installing \na variety of low-level rules on switches. However, from the high\u00adlevel, abstract viewpoint of the Frenetic \nprogrammer, these reads and their implementation have no observable effect on network state. As a result, \nqueries compose perfectly both with each other and with the operations in the policy management library. \nThe key challenge in the design of Frenetic s query sublanguage involves .nding a balance between expressiveness, \nsimplicity, and control over cost. For example, the cost of evaluating a query can be de.ned as the number \nof packets that must be diverted from the fast path in the network and processed on the controller. Manag\u00ading \nthis cost is important because the latency of processing a di\u00adverted packet is orders of magnitude worse \nthan processing it in hardware. Moreover, if many packets are diverted, the link between the switches \nand controller can become a bottleneck. Consequently, we deliberately limit the expressiveness of Frenetic \nquery language to ensure that it has a simple, easy-to-understand cost model pro\u00adgrammers can depend \non. Basic Concepts. Frenetic queries include constructs for .ltering the set of all packets in the network \nusing high-level patterns, sub\u00addividing this set by grouping on the basis of one or more header .elds, \nfurther splitting these sets by arrival time or whenever a header .eld changes value, limiting the number \nof values returned, and aggregating by number or size of packets. The result produced by a query is an \nevent stream a data structure that represents an in.nite, discrete, time-indexed stream of values. Though \nFrenetic is embedded in Python, an untyped language, it is useful to under\u00adstand the types of events \nand event-driven programs.1 The type a E denotes events carrying values of type a. For example, packet \nE is an event of packets and (switch \u00d7 int) E is an event of pairs of switch identi.ers and integers. \nThe syntax of Frenetic queries is given in Figure 3. Each top\u00ad level clause is optional, except for the \nSelect, which identi.es the type of event returned by the query an event carrying packets, byte counts, \nor packet counts. In Python code, we use the in.x op\u00ad 1 Though it is not central to this paper, we have \nimplemented a dynamic typechecker for Frenetic that checks the types of operators dynamically. erator \n* to combine clauses. We brie.y explain the main syntactic elements below and follow up with illustrative \nexamples. A Select(a) clause aggregates the results returned by the rest of the query using method a, \nwhere a may be one of packets (return the packets themselves), counts (return the number of packets) \nor bytes (return the sum of the sizes of the packets). A Where(fp) clause .lters the results, retaining \nonly those packets satisfying the .lter pattern fp. Simple query patterns de.ne sets of packets on the \nbasis of packet header .elds such as switch (switch), port (inport), source MAC address (srcmac), destina\u00adtion \nIP address (destip) and others. More complicated .lter pat\u00adterns can be constructed using natural set-theoretic \noperations such as intersection (and fp), union (or fp), difference (diff fp), and complement (not fp). \nThese high-level patterns are compiled to OpenFlow-representable patterns by Frenetic. A GroupBy([qh1,..., \nqhn]) clause subdivides the set of queried packets into subsets based on header .elds qh1 through qhn. \nFor example, grouping by srcip and srcport results in one subset for all packets with source IP 10.0.0.1 \nand TCP source port 80, a second subset for all packets with source IP 10.0.0.2 and TCP source port 80, \na third subset for all packets with source IP 10.0.0.1 and source port 21, etc. A SplitWhen([qh1,...,qhn]) \nclause, like a GroupBy, sub\u00addivides the set of selected packets into subsets. However, whereas GroupBy \nproduces one subset for all packets with particular val\u00adues for the given header .elds, SplitWhen does \nnot it gener\u00adates a new subset each time the value of one of the given .elds changes. For example, suppose \na query splits on source IP address, and packets with source IPs 10.0.0.1, 10.0.0.2 and 10.0.0.1 arrive \nin sequence. In this case, SplitWhen generates three subsets (the .rst and third packets are put in separate \nsets, because their IP addresses differ from the address of the preceding packet). If the arrival order \nwas different, perhaps 10.0.0.1, 10.0.0.1, 10.0.0.2, then only two subsets would be generated. An Every(n) \nclause partitions packets by time, grouping pack\u00adets that arrive within the same n-second window together. \nFinally, a Limit(n) clause limits the number of packets in each subset to n. The most common limit is \n1. Example Query. To get a taste of the Frenetic query language, consider the following web monitoring \nquery, designed for the single-switch repeater network presented in the previous section. def web_query(): \nreturn \\ (Select(sizes) * Where(inport_fp(2) &#38; srcport_fp(80))) * Every(30)) The in.x operator &#38; \nused in this query desugars into and fp. When registered with the run-time system, it selects all packets \narriving on physical port 2 and from TCP source port 80. It sums the sizes of all such packets every \n30 seconds and returns an event stream carrying integers as a result. The results of such queries may \nbe used in a variety of ways in Frenetic programs for traf.c analysis, for security monitoring and for \ndecisions about the forwarding policy. For now, all we will do is pipe the results to a printer: def \nweb_stats(): web_query() >> Print() Query Composition. To illustrate the modularity properties of Frenetic \nprograms, let us carry the example a step further and ex\u00adtend it to monitor incoming traf.c by host. \nAs shown in Section 3.1, implementing this program in NOX is dif.cult we cannot run the two smaller programs \nside-by-side because the rules for monitor\u00ading web traf.c overlap with the rules for monitoring traf.c \nby host.  Extending the Frenetic program, however, is simple. The following query summarizes the total \nvolume of traf.c arriving on physical port 2, grouped by destination host, every 60 seconds. def host_query(): \nreturn (Select(sizes) * Where(inport_fp(2)) * GroupBy([dstmac]) * Every(60)) This query may be composed \nwith the web query using the Merge operator, a generic combinator that transforms a pair of events into \nan event of pairs of optional values. def all_stats(): Merge(host_query(),web_query()) >> Print() The \nprogrammer who writes this program needs not know the de\u00adtails of the individual query routines, as neither \nquery can interfere with the results produced by the other. Why is that? Unlike NOX, Frenetic supports \nthe abstraction that queries merely read network state and do not modify it (even though the underlying \nrun-time sys\u00adtem will, in fact, modify the state of the network by installing rules on switches). Moreover, \nby design, Frenetic supports a program\u00adming model in which every query can see every packet in the network. \nThus, installing one query in the run-time does not silently inhibit any other queries from seeing certain \npackets. Note that the host query and the web queries operate at different frequencies 60 seconds vs. \n30 seconds. Implementing this functionality in Fre\u00adnetic is as easy as declaring the desired intervals. \nImplementing it in NOX, on the other hand, would be dif.cult, as the programmer would have to code tedious \nbookkeeping routines in event handlers to keep track of which statistics to collect at which times. Frenetic \ns run-time system does this bookkeeping automatically. Hence, our design has changed query composition \nfrom a challenging, error\u00adprone enterprise to a completely trivial one. Race-Free Semantics. One of the \nmost basic network programs is a learning switch, which discovers the identity of the hosts con\u00adnected \nto each of its ports by recording the source MAC addresses contained in incoming packets. The following \nquery could be used to implement the core functionality of a simple learning switch: def learning_query(): \nreturn (Select(packets) * Where(true_fp()) * GroupBy([srcmac]) * SplitWhen([inport]) * Limit(1)) def \nconnection_printer(): learning_query() >> Print() When learning query is executed, it generates an event \nthat in\u00adcludes one packet for each distinct source MAC, unless the port associated with that source MAC \nchanges (which might happen if a host, such as a laptop, were to move). This program is unre\u00admarkable \nexcept that it prints each new connection that it discov\u00aders exactly once because the query is limited \nto return one packet. Achieving the same effect in NOX is surprisingly tricky because of network race \nconditions. In the time it takes for a NOX program to generate and install a rule to suppress packets \n2, 3, 4 with the same source MAC, those packets might already have arrived at the switch, be en route \nto the controller and be about to be processed by the handler. Consequently, the NOX programmer will \nhave to remember to implement complex, error-prone bookkeeping if she wants to get it right. Such races \naffect the implementation of the Frenetic run-time system as well, but they are handled invisibly (and \nonce-and-for-all) at that level, and are not exposed to the pro\u00adgrammer. Unfortunately, the NOX implementation \ncannot mimic Frenetic as it does not have access to the high-level, semantic in\u00adformation expressed in \nthe queries that allows Frenetic to squash super.uous packets. The Query Cost Model. In order for programmers \nto use Frenetic effectively, they must have an understanding of the cost of applying the basic operations \nin the language. In particular, it is important that they have an understanding of the number of packets \nthat must be diverted from the fast path in the network and sent to the controller due to a query. The \ncost of executing a Frenetic query can be understood in terms of micro.ows i.e., sets of related packets \nthat have identical header .elds and arrive at the same switch. To illustrate recall the simple web query \nde.ned earlier: def web_query(): return \\ (Select(sizes) * Where(inport_fp(2) &#38; srcport_fp(80))) \n* Every(30)) An example of a micro.ow pertinent to this query is the one represented by a tuple that \ncontains in port 2, srcport 80, vlan 0, dl src0, and so on, with a speci.c value for each header .eld. \nAnother micro.ow pertinent to the query is the one with in port 2, srcport 80, vlan 1, dl src0, and so \non. Note the difference between the two .ows is only in the value of the vlan .eld. Clearly, the total \nnumber of micro.ows is enormous, but a single micro.ow may contain arbitrarily many packets so there \nare dramatically fewer inhabited micro.ows i.e., .ows for which the network actually witnesses a packet. \nA statistics query, such as the web query above, measures the counts or sizes of a particular stream \nof packets. Such a query diverts one packet per inhabited micro.ow to the controller. After that single \npacket has been diverted, the run-time system installs a rule on the switch for processing subsequent \npackets in that micro.ow.2 Every 30 seconds, the system gathers statistics for the query, not by diverting \nadditional packets, but by querying the counters maintained by the switches. There are two additional \nconsiderations in this cost analysis for statistics queries. First, if multiple statistics queries are \ninterested in information about the same micro.ow, then the costs are shared no matter how many statistics \nqueries are interested in a micro.ow at most one packet will diverted to the controller. Second, if the \nunderlying forwarding policy changes then the installed micro.ow rules must be uninstalled as the actions \nassociated with the rules may be wrong. The reason is that the rules used to collect statistics on the \nswitch are also used to perform forwarding and may, for example, be forwarding the given micro.ow out \non one port in the old policy and a different port in the new policy. Thus, when the policy changes, \nadditional packets may be diverted from the fast path as the network adapts to the change. The above \nanalysis applies speci.cally to statistics queries, as statistics can be tabulated on switches and collected \nlater by the controller. Packet queries are different because every packet that appears in the result \nof a packet query must go to the con\u00adtroller. Hence packet queries without a Limit clause are inherently \nexpensive in effect, the switch hardware cannot be used at all be\u00adcause every packet in each pertinent \nmicro.ow must be diverted to the controller. With a Limit clause, the costs are reduced. For example, \nwith a Limit(1), as in the query used in the learning 2 Of course, due to network race conditions and \nthe non-zero latency of switch-controller communication, it may be the case that prior to installing \nthe new rule, a few additional packets in the same micro.ow hit the switch and are diverted to the controller. \nHence, to be perfectly accurate, one packet modulo network race conditions is diverted from the fast \npath.  switch, the cost of a packet query is similar to the cost of an analo\u00adgous statistics query. \nDeep Packet Inspection. To implement deep packet inspection in Frenetic, one only needs to write a query \nthat returns the packets to inspect e.g., the following query returns all web traf.c: def web_packets_query(): \nreturn (Select(packets) * Where(srcport_fp(80))) def dpi(): web_packets_query() >> analyze_packet() \nOf course, as just explained, unrestricted packet queries such as this one do not make effective use \nof switch hardware and divert many packets to the controller. However, this is not a limitation of the \nFrenetic design, it is a limitation of the popular OpenFlow platform on which Frenetic sits. In the future, \nOpenFlow switches may well be extended to allow ef.cient querying of additional bits of every packet \nin hardware. When such extensions are available, we anticipate it will be straightforward to extend the \nFrenetic query language to support deep packet inspection ef.ciently. For now, to maintain a clear cost \nmodel for Frenetic queries i.e., one where cost depends on the number of micro.ows, not the number of \npackets in a micro.ow (except for packets returned by the query) we do not support deep packet inspection \nin queries themselves. Summary. The Frenetic query language supports a collection of orthogonal, high-level \nquery operators. The Frenetic run-time sys\u00adtem supports the abstraction that these operators read, but \ndo not modify network state. The key consequence of this abstraction is that queries compose seamlessly \nwith one another. The Frenetic run-time system also suppresses super.uous packets that occur due to race \nconditions in the underlying network, giving queries a sim\u00adple race-free semantics. Finally, Frenetic \nqueries have a simple, clear cost model that depends primarily on the number of inhab\u00adited micro.ows, \nnot the number of packets within a micro.ow.  4.2 The Network Policy Management Library Frenetic programmers \nmanage the policy that governs the forward\u00ading of packets through the network using a combinator library \nfor functional, reactive programming (FRP). The library design is in\u00adspired by Yampa [12] (a language \nfor programming robots) and its implementation is based on the strategy used in FlapJax [32] (a library \nfor web programming). However, there is still signi.cant novelty in applying these old ideas to a new \ndomain. In addition, Frenetic s query language, its representation of network state in the run-time system, \nand its library of FRP combinators, are all care\u00adfully designed to work well together. Basic Concepts. \nOne of the basic operations performed by a Frenetic program is to construct packet-forwarding rules for \nin\u00adstallation on switches. These rules are created using the Rule constructor, which takes a pattern \nand a list of actions as argu\u00adments. Patterns are similar to the .lter patterns used in the query language \nthe only difference is that rule patterns do not mention switches. Actions include forwarding through \na particular port p (forward(p)), .ooding through all ports (flood()), sending the packet to the controller \n(controller()), and modifying header .eld f to a new value v (modify(f,v)). There is no explicit drop \naction. The empty list is interpreted as a directive to drop packets. To associate rules with switches, \nFrenetic programs must create network policies. We represent policies in Python as dictionaries mapping \nswitches to lists of rules. Frenetic programs control the installation of policies in a net\u00adwork over \ntime by generating policy events. Policy events are in.\u00adnite, time-indexed streams of values, just like \nthe events generated from queries that we saw in the previous subsection. In addition Events Seconds \n. int E SwitchJoin . switch E SwitchExit . switch E PortChange . (switch \u00d7 int \u00d7 bool) E Once . a . \na E Basic Event Functions >> . a E . a\u00df EF . \u00df E Lift . (a . \u00df) . a\u00df EF >> . a\u00df EF . \u00df. EF . a. EF ApplyFst \n. a\u00df EF . (a \u00d7 .)(\u00df \u00d7 .) EF ApplySnd . a\u00df EF . (. \u00d7 a)(. \u00d7 \u00df) EF Merge . (a E \u00d7 \u00df E) . (a option \u00d7 \u00df \noption) E BlendLeft . a \u00d7 a E \u00d7 \u00df E . (a \u00d7 \u00df) E BlendRight . \u00df \u00d7 a E \u00d7 \u00df E . (a \u00d7 \u00df) E Accum . (. \u00d7 (a \n\u00d7 . . .) . a. EF Filter . (a . bool) . aa EF Listeners >> . a E . a L . unit Print . a L Register . policy \nL Send . (switch \u00d7 packet \u00d7 action) L Rules and Policies Rule . pattern \u00d7 action list . rule MakeForwardRules \n. (switch \u00d7 port \u00d7 packet) policy EF AddRules . policy policy EF Figure 4. Selected Frenetic Operators. \n# query returning one packet per source IP def src_ips() = return (Select(packets) * Where(inport_fp(1)) \n* GroupBy([srcip]) * Limit(1)) # helper to add switch to a port-packet pair def add_switch(port,packet): \nreturn (switch(header(packet)),port,packet) # parameterized load balancer def balance(balancer): return \n\\ (src_ips() >> # (IP*packet) E ApplyFst(balancer) >> # (port*packet) E Lift(add_switch) >> # (switch*port*packet) \nE MakeForwardRules() >> # policy E AddRules()) # policy E Figure 5. A Parameterized Load Balancer to \npolicy events and query-generated events, Frenetic also contains the primitive events Seconds, which \ncontains the number of sec\u00adonds since the epoch, SwitchJoin and SwitchExit, which con\u00adtains the identi.ers \nof switches joining or leaving the network, and PortChange, which contains triples comprising a switch, \na port number, and a boolean value. In this last event, the boolean value indicates whether the given \nport on the switch is enabled. Frenetic also contains Listeners, which represent event con\u00adsumers. One \nexample of a listener is the primitive Print listener, which consumes string events by printing them \nto the console. An\u00adother example is the Send listener, which consumes events car\u00adrying a switch, packet, \nand action list by sending each packet to the switch and applying the actions to it there. The Register \nlis\u00adtener applies a network policy to a network. The type of listeners of events a E is written a L. \n Frenetic programs analyze or transform events using event func\u00adtions. The type of event functions from \na E to \u00df E is written a\u00df EF. Many such event functions are based on standard oper\u00adators found in previous \nwork on FRP. For example, Merge, which we saw in previous sections, transforms a pair of events into \nan event of pairs of options. Lift(f) transforms an ordinary function f of type (a . \u00df) into an event \nfunction of type a\u00df EF that applies f to each value in its input event. Frenetic also supplies a derived \nlibrary of event functions useful speci.cally in a network\u00ading context. For example, MakeForwardRules \nconverts an event of triples containing a switch, port number, and packet into a for\u00adwarding policy that \nforwards packets with the same header out the given port. AddRules folds over the values in its incoming \npolicy event by repeatedly merging the policies it receives and returning, at each time step, the total \naccumulated policy so far. In the following paragraphs, we will further explain these con\u00adcepts using \nexamples. For reference, Figure 4 lists a selected set of the most important Frenetic operators and their \ntypes. Note that the composition operator >> is overloaded and can be used to com\u00adpose events with event \nfunctions, event functions with other event functions, and events with listeners. A First Example. The \nsimplest forwarding program just installs static packet-forwarding rules. The Frenetic program below \nmimics the NOX repeater hub presented in Section 3: rules = [Rule(inport_fp(1),[forward(2)]), Rule(inport_fp(2),[forward(1)])] \ndef repeater(): (SwitchJoin() >> Lift(lambda switch:{switch:rules}) >> Register()) The network policy \nin this program contains two rules. The .rst matches all packets arriving at port 1 and forwards them \nout port 2. Conversely, the second matches packets arriving on port 2 and forwards them out port 1. The \nrepeater function passes the SwitchJoin event stream to a lifted function that builds an event carrying \ndictionaries with switches as keys and the list of rules as the corresponding value. It then pipes this \npolicy to the Register listener, which installs it in the run-time system. One of the .rst things to \nnotice about this example is that it composes effortlessly with the network monitoring programs developed \nin the previous subsection: def repeater_web_monitor(): repeater() all_stats() Unlike the NOX code we \nsaw before, in Frenetic there is no need to rewrite and interleave overlapping monitoring code and forwarding \npolicy code. Because Frenetic presents the abstraction that queries read, but do not modify the network, \nthese reads do not interfere with the forwarding policy. Conversely, because queries see every packet \n, forwarding does not interfere with the semantics of a query (though, of course, sending packets along \na monitored link does affect the results of a query). Under the hood, the Frenetic run\u00adtime system manages \nthe interactions between the OpenFlow rules generated by queries implementation and the rules generated \nby the network policy. A Simple Load Balancer. A load balancer is a switch that re\u00adceives traf.c on its \nincoming ports and multiplexes that traf.c out its outgoing ports. Our load balancer will multiplex traf.c \nbased on source IPs: traf.c from the same source IP will be forwarded through the same output port; traf.c \nfrom different source IPs may be forwarded through different output ports. # Filter away rules involving \n# elements of ip_list from policy def filter_ips(ip_list,policy): secure_policy = policy for ip in ip_list: \nsecure_policy = delete_ip(ip,secure_policy) return secure_policy # Filter away rules involving # elements \nof bad_ips() from policyE def secure(policyE): return (BlendLeft({},bad_ips(),policyE) >> Lift(filter_ips)) \n# Apply the load balancer followed # by the security filter def secure_balance(): (secure(balance(weighted_balancer())) \n>> Register()) Figure 6. Securing the weighted balancer. The bad ips event and delete ip function are \nelided. Figure 5 presents the code for the core load balancing algo\u00ad rithm. The code uses a query (de.ned \nby src ips) to gener\u00adate an event with one value for each new source IP in a packet arriving on port \n1. The main routine, balance, takes an argu\u00adment balancer, which is an event function that transforms \nIP ad\u00addresses into ports (we assume traf.c will be multiplexed through ports 2 through OUTPORTS). The \nbalance function itself runs the src ips query to generate an event for each new IP address seen, runs \nthe balancer to determine the appropriate port through which to forward those packets, and uses library \nfunctions to construct the network policy as the result. The balancer can be instantiated in many different \nways. For example, the programmer might assume a uniform distribution of traf.c across IP addresses and \nhash each source IP to a port, def hash_balancer(): return Lift(lambda ip,port:hash_ip_to_port(ip)) or \nthey might implement round-robin load balancing: def rr_balancer(): next = lambda ip,port:(port%(OUTPORTS-1))+2 \nreturn (Accum(1,next)) Yet another possibility is to monitor the load on the switch and implement the \nload balancer using dynamic traf.c levels. The ip monitor program below queries the packet counts by \nIP ad\u00address every INTERVAL seconds. Then weighted balancer pipes the result of the query into an event \nfunction weighted choice (whose de.nition is elided), that selects the next port to forward through based \non current traf.c levels. def ip_monitor(): return (Select(counts) * Where(inport_fp(1)) * GroupBy([srcip]) \n* Every(INTERVAL)) def weighted_balancer(): return (ip_monitor() >> weighted_choice()) Any of the above \nbalancing functions can be used in conjunction with the generic balancer as follows.  Figure 7. Frenetic \narchitecture. def balance_switch(): balance(weighted_balancer()) >> Register() Interestingly, while \ncreating this parameterized load balancer with Frenetic is a relatively straight-forward exercise in \nfunctional pro\u00adgramming, simulating it in NOX is substantially more dif.cult. The crux of the problem \nis that the parameterized balancing algorithm (the function balance) cannot be de.ned in NOX without \nrisk\u00ading interference from the monitoring rules needed by components such as weighted balance. The simplest \nNOX solution is likely to make multiple copies of the code one for each separate bal\u00adancing function \nand handle interfering rules manually. Frenetic s run-time system handles all such interference automatically. \nComposing Forwarding Decisions. The previous examples illus\u00adtrate composition of queries with each other \nand with a single pol\u00adicy module. It is also possible to compose a routine that computes a forwarding \npolicy with separate routines that transform or alter the policy. A typical example is a security module \nthat prevents known bad source IPs from sending traf.c, as shown in Figure 6. Frenetic s functional style \nmakes such examples easy to code. It is typically much more dif.cult to compose the forwarding policies \ncomputed by different NOX modules, unless those modules act on completely disjoint sets of packets. Summary. \nMost network programs involve a combination of monitoring and forwarding. Because queries can always \nsee ev\u00adery network packet independently of the forwarding policies ex\u00adpressed by other modules, monitoring \nand policy components com\u00adpose seamlessly in Frenetic. Moreover, because of Frenetic s func\u00adtional style, \npost-facto application of policy modi.ers, such as our security module, is trivial. Overall, it is far \neasier to write simple, modular, reuseable programs in Frenetic than it is in NOX. 5. Frenetic Implementation \nFrenetic provides high-level abstractions that free programmers from having to reason about a host of \nlow-level details involving the underlying switch hardware. However, the need to deal with these low-level \ndetails does not just disappear because programs operate at a higher level. The rubber meets the road \nin the implementation, which is described in this section. We have implemented a complete working prototype \nof Fre\u00adnetic as an embedded combinator library in Python. Figure 2 de\u00ad picts its architecture, which \nis organized around three main pieces: the language itself, the run-time system, and NOX. The use of \nNOX is convenient but not essential we could also use any other con\u00adtroller as a back-end. function packet \nin(packet, inport) isSubscribed := false actions := [] for (query, event, counters, requests) . subscribers \ndo if query.matches(packet.header) then event.push(packet) isSubscribed := true for rule . rules do if \n(rule.pattern).matches(packet.header) then actions.append(rule.actions) if isSubscribed then send packet(packet, \nactions) else install(packet.header, DEFAULT, None, actions) .ows.add(packet.header) function stats in(xid, \npackets, bytes) for (query, event, counters, requests) . subscribers do if requests.contains(xid) then \ncounters.add(packets, bytes) requests.remove(xid) if requests.is empty() then event.push(counters) function \nstats loop() while true do query := next stats() counters.reset() for pattern . .ows do if query.matches(pattern) \nthen xid := stats request(pattern) requests.add(xid) sleep(next stats window()) Figure 8. Frenetic run-time \nsystem handlers The central piece of the implementation is the run-time system, which sits between the \nhigh-level program and NOX. It manages all of the bookkeeping related to installing and uninstalling \nrules on switches and also generates the necessary communication patterns between switches and the controller. \nTo do all of this, the run-time maintains several global data structures: policy, a dictionary from \nswitches to sets of high-level rules that describes the current packet-forwarding policy,  .ows, a set \nof low-level rules currently installed on the switches in the network, and  subscribers, a set of tuples \ncontaining a de.ning query, an event for that subscriber, byte and packet counts, and a list of outstanding \nstatistics requests.  To translate the high-level forwarding policy registered in the run\u00adtime into \nswitch-level rules, the run-time uses a simple strategy that reacts to .ows of network traf.c as they \noccur. At the start of the execution of a program, the .ow table of each switch in the network is empty, \nso every packet is sent to the controller and passed to the packet in handler. When it receives a packet, \nthis function .rst iterates through the set of subscribers and propagates the packet to each subscriber \nwhose de.ning query includes the packet in its result. Next, it traverses the policy and collects up \nthe list of actions speci.ed in all rules. Finally, it processes the packet in one of two ways: If there \nare no subscribers for the packet, then it installs a switch-level rule that processes future packets \nwith the same header .elds without involving the controller. Or, if there are subscribers for the packet, \nthen the run-time sends the packet back  Connectivity HUB LSW LFLSW Heavy Hitters HUB LSW LFLSW Web \nStats HUB LSW LFLSW NOX Lines of Code Controller Traf.c (kB) Aggregate Traf.c (kB) 20 12.8 69.2 55 13.5 \n42.3 75 31.3 64.1 110 9.3 57.2 198 10.3 36.1 * 104 4.5 14.1 135 4811 9.0 * Frenetic Lines of Code Controller \nTraf.c (kB) Aggregate Traf.c (kB) 6 9.1 65.6 30 12.0 41.0 58 12.4 41.5 29 11.1 55.0 53 10.6 36.4 81 10.9 \n36.9 13 4.5 13.6 37 5.1 9.20 65 5.8 9.9 Table 1. Experimental results. to the switch and applies the \nactions there, but does not install a rule, as doing so would prevent future packets from being sent \nto the controller (and, by extension, the subscribers that need to be supplied with those packets). In \neffect, this strategy dynamically unfolds the forwarding policy expressed in the high-level rules into \nswitch-level rules, moving processing off the controller and onto switches in a way that does not interfere \nwith any subscriber. The run-time uses a slightly different strategy to implement ag\u00adgregate statistics \nsubscribers, making use of the byte and packet counters maintained by the switches. The run-time system \nexecutes a loop that waits until the window for a statistics subscriber expires. At that point, it traverses \nthe .ows set and issues a request for the byte and packet counters from each switch-level rule whose \npat\u00adtern matches the query, adding the request identi.er to the set of outstanding requests maintained \nfor this subscriber in subscribers. The stats_in handler receives the asynchronous replies to these requests, \nadds the byte and packet counters to the counters main\u00adtained for the subscriber in subscribers, and \nremoves the request id from the set of outstanding requests. When the set of outstanding requests becomes \nempty, it pushes the counters, which now contain the correct statistics, onto the subscriber s event \nstream. Figure 8 gives pseudo-code for the NOX handlers used in the Frenetic run-time system. These algorithms \ndescribe the basic be\u00adhavior of the run-time, but elide some additional complications and details3 that \nthe actual implementation has to deal with such as spu\u00adrious packets that get sent to the controller \ndue to race conditions between the receipt of a message to install a rule and the arrival of the packet \nat the switch. The other piece of the Frenetic implementation is the library of FRP operators themselves. \nThis library de.nes representations for events, event functions, and listeners, as well as each of the \nprimitives in Frenetic. Unlike classic FRP implementations, which support continuous streams called behaviors \nas well as discrete streams called events, Frenetic focuses almost exclusively on dis\u00adcrete streams. \nThis means that the pull-based strategy used in most previous FRP implementations, which is optimized \nfor behaviors, is not a good .t for Frenetic. Accordingly, our FRP library uses a push-based strategy \nto propagate values from inputs to outputs. The run-time system s use of exact-match rules follows the \napproach used in Ethane [9] and many OpenFlow-based applica\u00ad tions [18, 21], and is well-suited for dynamic \nsettings. Moreover, exact-match rules use the plentiful conventional memory (e.g., SRAM) many switches \nprovide, as opposed to the small, expensive, power-hungry Ternary Content Addressable Memories (TCAMs) \nneeded to support wildcards. Still, wildcard rules are more concise 3 For example, when the forwarding \npolicy changes, some of the rules installed on switches may be stale and must be uninstalled. But when \nthe run-time uninstalls a rule on a switch, the byte and packet counters associated with the switch-level \nrule must not be lost. Thus, the Frenetic run-time de.nes a flow_removed handler that receives the counters \nfor and well-suited for static settings. We plan to develop a proactive, priority-based wildcard approach \nas part of Frenetic s run-time in the near future. Longer term, we plan to extend the run-time to adaptively \nselect between exact-match and wildcard rules, depend\u00ading on the capabilities of the switches in the \nnetwork. 6. Evaluation To evaluate our design for Frenetic, we implemented several sim\u00adple applications \nin Frenetic and compared them against equivalent NOX programs on three metrics: lines of code, traf.c \nto controller, and total traf.c. The lines of code metric gives a measure of the complexity of each program, \nas well as the savings from code reuse when modules are composed. The controller traf.c measures the \ntotal amount of communication between the switch and controller, which quanti.es the overhead of managing \nswitch-level rules using a run-time system. Finally, the aggregate traf.c metric measures the total amount \nof traf.c on every link in the network. Setup and Methodology. We ran our experiments using the Mininet \nvirtualization environment [26] on a Linux host with a 2.4GHz Intel Core2 Duo processor and 4GB of RAM. \nMininet does not provide performance .delity but does give accurate traf.c measurements. For the lines \nof code metric, we counted up to 80 characters of properly-indented Python excluding whitespace. We used \nWireshark to tally controller and total traf.c. Microbenchmarks. We compared the performance of Frenetic \nagainst NOX using the following microbenchmarks: All-Pairs Connectivity: each host sends and receives \nICMP (ping) packets to/from all other hosts. This benchmark tests whether the forwarding policy establishes \nbasic connectivity.  Web Statistics: each host generates a single request to a web server and the controller \nmonitors the aggregate HTTP traf\u00ad.c every .ve seconds. This tests the performance of simple monitoring \na common network administration task.  Heavy Hitters: each host sends and receives ICMP packets to/from \na variety of other hosts in the network. The controller collects per-host statistics and reports the \ntop-k traf.c sources. This illustrates a more sophisticated monitoring application.  Note that none \nof these microbenchmarks specify the underly\u00ading policy used to forward packets in the network. We ran \neach microbenchmark using several different policies: Hub: The hub (HUB) policy .oods packets received \non one port out on all other ports, except the port the packet arrived on.  Learning Switch: The learning \nswitch (LSW) policy dynam\u00adically learns the association between hosts and ports as it sees traf.c. It \n.oods packets to unknown destinations but outputs  uninstalled rules and adds them to the counters \nmaintained on the controller. packets to known hosts on the port the host is connected to. 180 135 90 \nController Traf.c (kB) NOX Frenetic 2040 1530 1020 510 0 Controller Traf.c (kB)  45 25 50 # Hosts # \nHosts # Hosts (a) All-Pairs Connectivity (c) Heavy Hitters (b) Web Statistics Figure 9. Scalability experimental \nresults. Connectivity Multi-get NOX Controller Traf.c (kB) 5.9 3.2 Aggregate Traf.c (kB) 34.8 30.1 Frenetic \nController Traf.c (kB) 12.0 11.8 Aggregate Traf.c (kB) 41.0 38.9 Table 2. Wildcard experimental results. \nLoop-Free Learning Switch: The loop-free learning switch (LFLSW) learns the host-port mapping and the \nnetwork topol\u00adogy using custom protocols of our own design. From these two pieces of information, it \ncalculates a spanning tree and uses this to avoid forwarding loops when .ooding packets. Results. The \nresults of our experiments are given in Table 1. They demonstrate a few key points. First, on these benchmarks, \nFre\u00adnetic performs comparably with hand-written NOX programs de\u00adspite being implemented using a run-time \nsystem. Second, Frenetic provides substantial code savings to the network programmer. In particular, \nFrenetic s compositional semantics allowed us to eas\u00adily compose the monitoring modules with each of \nthe forward\u00ading policies the size of each composition is exactly the sum of the sizes of the inputs (the \nmonitoring queries for Web Stats and Heavy Hitters are 23 and 7 lines, respectively) unlike the NOX programs, \nwhich had to be manually refactored to correctly imple\u00adment each version of the microbenchmark.4 Finally, \nthe aggregate traf.c statistics for LFLSW on the connectivity experiment demon\u00adstrate that by using Frenetic, \nprogrammers can write sophisticated network programs that actually consume less network capacity than \nhand-written NOX programs. The reason for this difference is that the Frenetic LFLSW dynamically reacts \nto network events while the NOX version uses periodic polling to discover the network topology, which \nproduces more total traf.c on the network. These microbenchmarks demonstrate that Frenetic s run-time \nsystem achieves adequate performance in some common scenarios. But they are far from comprehensive. There \nare certainly many sit\u00aduations where Frenetic s run-time system does not perform as well as hand-written \nNOX programs e.g., when the optimal implemen\u00adtation of the forwarding policy uses wildcard rules. To \ndemonstrate such a situation, we implemented a wildcard learning switch which is similar to the standard \nMAC learning switch distributed with NOX but installs wildcard rules instead of micro.ow rules. More \nspeci.cally, the controller installs .ow table entries that con\u00adstrain only the learned source and destination \nMAC addresses, but leave all other header .elds as wildcards. In situations where two hosts communicate \nacross multiple distinct micro.ows sharing a common source and destination MAC address, the wildcard \nlearn\u00ading switch will perform better, according to these metrics. Table 2, compares the wildcard learning \nswitch to the Frenetic run-time sys\u00adtem on the connectivity benchmark and another benchmark called multi-get, \nwhich generates multiple concurrent HTTP requests to different TCP ports i.e., two hosts with the same \nsource and desti\u00adnation MACs communicate using multiple distinct micro.ows. As the results show, the \nNOX application which uses wildcards signif\u00adicantly outperforms the reactive, micro.ow based approach \nused in Frenetic. We are currently working to extend the run-time system to support wildcard rules. Scalability \nExperiments. For each microbenchmark, we also conducted a scalability experiment to evaluate whether \nFrenetic programs would continue performing comparably to NOX pro\u00adgrams as the number of hosts in the \nnetwork grows. In each ex\u00adperiment, we used a single switch running the learning switch forwarding policy, \nbut scaled the number of hosts up from 1 to 50. The results in Figure 9 con.rm that Frenetic performance \nscales comparably and in many cases better than NOX. We hypothesize a simple reason for this difference: \na common NOX idiom, which we used in our implementations of the NOX bench\u00admarks, is to install rules \nwith timeouts. This ensures that rules self-destruct without the programmer having to perform extra bookkeeping \nto remember all of the installed rules. However, such timeouts result in additional packets being sent \nto the controller, both in .ow removed messages and for subsequent .ow setups. In contrast, Frenetic \ns run-time system reacts to changes in the for\u00adwarding policy and manages the set of installed rules \nautomatically, obviating the need for timeouts. Further Experience. In addition to the quantitative benchmarks \ndiscussed so far, we have implemented a collection of network util\u00adities in Frenetic to validate our \nlanguage design. This list of pro\u00adgrams ranges from essential network functions to novel applica\u00adtions \nthat implement new functionality. Frenetic s modular design makes it easy to build new tools out of simpler, \nreuseable parts. Code for these examples is hosted on the Frenetic web site [2]. Discovery. Discovers \nthe network topology.  Spanning Tree. Computes a spanning tree from the topology.  All-Pairs Shortest-Path \nRouting. Uses the topology to com\u00adpute a forwarding policy based on shortest paths.  Load Balancer. \nConnects incoming traf.c to one of several replica servers. Can be instantiated with many heuristics \nto balance incoming traf.c across back-end servers.  Fault-tolerant Routing. Connects incoming traf.c \nto one of  4 In fact, refactoring the benchmarks to use the loop-free learning switch was suf.ciently \ndif.cult that we did not complete it, despite the fact that several replica switches, organized into \nseveral layers. When a NOX provides a topology module and we had already implemented hub switch goes \ndown, traf.c is routed through the other switches and learning switch versions of the benchmarks. in \nthe same layer.  Address Resolution Protocol (ARP) Server. Implements ARP in the network, by maintaining \na global view of the IP-MAC address mapping.  Dynamic Host Con.guration (DHCP) Server. Implements DHCP \nto bootstrap network hosts with logical (IP) addressing information.  Memcached Query Router. Connects \nclients to virtual servers implementing a key-value store. The switch translates between the virtual \naddresses assigned to servers and the servers phys\u00adical addresses. When servers fail, it reassigns its \nvirtual ad\u00addresses to another server; when new servers becomes available, virtual addresses from other \nservers are remapped to it.  Scan-Free Learning Switch. Generalized learning switch. De\u00adtects and blocks \nmalicious hosts that scan the network.  DDoS Defense. Detects anomalies in the amount of traf.c sent \nover the network and drops packets from the offending hosts.  7. Related Work This paper extends preliminary \nwork by some of the authors, which was presented at a workshop on programmable network devices [19]. \nThe earlier paper did not describe a run-time system, query language, or any signi.cant applications, \nand did not provide an evaluation of the language s design or its implementation. The OpenFlow platform \nprovides a uniform interface for pro\u00adgramming physical network switches [3, 30, 31]. Many other plat\u00ad \nforms for programming network devices have also been proposed. The Click modular router [24] shares the \ngeneral goal of mak\u00ad ing network devices programmable and, like Frenetic, emphasizes modularity as an \norganizing design principle. But Click exclusively targets software switches (implemented as a Linux \nkernel module) while Frenetic can be used with physical switches (implemented using special-purpose hardware). \nRouteBricks [15] attempts to ob\u00ad tain better performance from software switches implemented us\u00ading stock \nmachines. Bro [35] and Snortran [16] allow programmers to express rich packet-.ltering and monitoring \npolicies for secur\u00ading networks and detecting intrusions while Shangri-La [10] and FPL-3E [14] compile \nhigh-level packet-processing programs down to special packet-processing hardware and FPGAs. The key dif\u00adference \nbetween Frenetic and all of these systems is that they are limited to a single device. Thus, they do \nnot address the issue of how to program a collection of interconnected switches. The Frenetic implementation \nuses the NOX controller [20], which provides convenient C++ and Python APIs for handling raw events and \ncommunicating with switches. Several other OpenFlow controllers have also been proposed. Beacon [1] is \nsimilar to NOX but provides a Java API. Maestro [8] provides a modular mecha\u00ad nism for managing network \nstate using programmer-de.ned views. It is also multi-threaded, which increases throughout dramatically. \nOnix [25] provides abstractions for partitioning and distributing network state onto multiple distributed \ncontrollers, which addresses the scalability and fault-tolerance issues that arise when using a centralized \ncontroller. SNAC [4] provides high-level patterns (sim\u00ad ilar to Frenetic s .lter patterns) for specifying \naccess control poli\u00adcies as well as a graphical monitoring tool but is not a general programming environment. \nThe Flow Management Language [23] also provides a high-level pattern language for specifying security \npolicies in OpenFlow networks [23]. Frenetic s event functions are modeled after functional reactive \nlanguages such as Yampa and others [17, 32, 34, 36], and many of our primitives are borrowed directly \nfrom these languages. Fre\u00adnetic s push-based implementation of the functional reactive com\u00adbinators is \nbased on FrTime [11] and is also similar to adaptive functional programming [5]. The Flask [29] language \napplies func\u00ad tional reactive programming to sensor networks in a staged lan\u00adguage. The key differences \nbetween Frenetic and all of these lan\u00adguages are in the application domain (networking as opposed to \nanimation, robotics, and others) and in the design of our query lan\u00adguage and run-time system, which \nuses the capabilities of switches to avoid sending packets to the controller. At a high level, Frenetic \nis also similar to streaming languages such as StreamIt [38], CQL [6], Esterel [7], Brooklet [37], etc. \nThe FRP operators used in Frenetic are more to our taste, but one could easily build a system that retained \nthe main elements of our design (e.g., the query language and the run-time system) but used different \nconstructs for processing streams of network events. The Nettle [39] language also uses FRP combinators \nto pro\u00ad gram OpenFlow switches. A Nettle program takes a stream of raw OpenFlow events as input (e.g., \nswitch join, port change, packet in, etc.) and produces a stream of raw OpenFlow messages as out\u00adput \n(e.g., install, uninstall, query stats, etc.). Although Nettle and Frenetic appear super.cially similar \nboth use FRP for OpenFlow networks a closer inspection reveals substantial differences. The most important \ndifference is that Nettle operates at a lower level of abstraction than Frenetic: it is an effective \nsubstitute for NOX while Frenetic sits on top of NOX (and, in the future, could potentially sit on top \nof Nettle). Nettle does not offer any analog of Frenetic s query language or its run-time system and \nso Nettle programs work in terms of low-level OpenFlow concepts such as switch-level rules, priorities, \nand timeouts. As such it suffers from all of the limita\u00adtions of NOX discussed in Section 3 e.g., Nettle \nprograms cannot be easily composed and are susceptible to network race conditions. NDLog, an extension \nof Datalog developed by Loo, Heller\u00adstein, et al. has been used to specify and implement routing pro\u00adtocols, \noverlay networks, and services such as distributed hash ta\u00adbles [27, 28]. Both Frenetic and NDLog use \nhigh-level languages to program networks, but there are some important differences. One is NDLog s focus \non routing protocols and overlay networks, whereas Frenetic programs can be used to implement .ner-grained \npacket\u00adprocessing including rewriting header .elds. Another difference is that NDLog programs are written \nin an explicitly distributed style while Frenetic offers the programmer the abstraction of a central\u00adized \nview of the network. This dramatically changes the way that programs must be written: an NDLog programmer \ncrafts a sin\u00adgle query that is evaluated on every router in the network while a Frenetic programmer writes \na program from the omniscient per\u00adspective of the controller and run-time system distributes low-level \nrules to the switches in the network. Finally, deploying NDLog in a production network would require \ndeep changes to the way that switches are built, as it requires each switch to run a custom Data\u00adlog \nengine. Frenetic targets OpenFlow, which is already supported by several vendors, and so can be deployed \nimmediately. One of the main challenges in the implementation of Frenetic is splitting work between the \n(powerful but slow) controller and the (fast but limited) switches. Gigascope [13], a stream database \nfor monitoring networks, addresses the same problem but, unlike Frenetic, only supports querying traf.c \nand cannot be used to control the processing of packets in the network. 8. Conclusions and Future Work \nThis paper describes the design and implementation of Frenetic, a new language for programming OpenFlow \nnetworks. Frenetic ad\u00addresses some serious problems with the OpenFlow/NOX program\u00adming model by introducing \na collection of high-level and composi\u00adtional operators for querying and transforming streams of network \ntraf.c. A run-time system handles all of the details related to in\u00adstalling and uninstalling low-level \nrules. An experimental evalua\u00adtion demonstrates that the performance of Frenetic s run-time sys\u00adtem is \ncompetitive with hand-written OpenFlow/NOX programs.  We are currently working to extend Frenetic in \nseveral direc\u00adtions. One thread of work is developing security applications for performing authentication \nand access control, and for ensuring iso\u00adlation between logical networks that share a common physical \nin\u00adfrastructure. We are also designing a new run-time system that gen\u00aderates rules from the registered \nsubscribers and forwarding rules eagerly. We plan to compare the tradeoffs between different rule\u00adgeneration \nstrategies empirically. Acknowledgments. We wish to thank Matthew Meola, Mark Re\u00aditblatt, and Minlan \nYu for many helpful discussions, and the anony\u00admous ICFP reviews for their insightful comments. Our work \nis supported in part by ONR grants N00014-09-1-0770 Networks Op\u00adposing Botnets and N00014-09-1-0652 Fabric: \nA Higher-Level Ab\u00adstraction for Building Secure Distributed Applications. Any opin\u00adions, .ndings, and \nrecommendations are those of the authors and do not necessarily re.ect the views of the ONR. References \n[1] Beacon: A java-based OpenFlow control platform. See http:// www.beaconcontroller.net, Nov 2010. [2] \nThe Frenetic language. See http://www.frenetic-lang.org/, Nov 2010. [3] OpenFlow. See http://www.openflowswitch.org, \nNov 2010. [4] SNAC. See http://snacsource.org/, 2010. [5] Umut A. Acar, Guy E. Blelloch, and Robert Harper. \nAdaptive func\u00adtional programming. TOPLAS, 28:990 1034, November 2006. [6] Arvind Arasu, Shivanth Babu, \nand Jennifer Widom. The CQL contin\u00aduous query language: Semantic foundations and query execution. The \nVLDB Journal, 15:121 142, Jun 2006. [7] G\u00b4 erard Berry and Georges Gonthier. The Esterel synchronous \npro\u00adgramming language: Design, semantics, implementation. Science of Computer Programming, (2):87 152, \n1992. [8] Zheng Cai, Alan L. Cox, and T. S. Eugene Ng. Maestro: A system for scalable OpenFlow control. \nTechnical Report TR10-08, Rice University, Dec 2010. [9] Martin Casado, Michael J. Freedman, Justin Pettit, \nJianying Luo, Natasha Gude, Nick McKeown, and Scott Shenker. Rethinking en\u00adterprise network control. \nTrans. on Networking., 17(4), Aug 2009. [10] Michael K. Chen, Xiao Feng Li, Ruiqi Lian, Jason H. Lin, \nLixia Liu, Tao Liu, and Roy Ju. Shangri-la: Achieving high performance from compiled network applications \nwhile enabling ease of programming. In PLDI, pages 224 236, Jun 2005. [11] Gregory H. Cooper and Shriram \nKrishnamurthi. Embedding dynamic data.ow in a call-by-value language. In ESOP, pages 294 308, 2006. [12] \nAntony Courtney, Henrik Nilsson, and John Peterson. The Yampa arcade. In Haskell Workshop, pages 7 18, \nAug 2003. [13] Chuck Cranor, Theodore Johnson, Oliver Spataschek, and Vladislav Shkapenyuk. Gigascope: \nA stream database for network applications. In SIGMOD, pages 647 651, 2003. [14] Mihai Lucian Cristea, \nClaudiu Zissulescu, Ed Deprettere, and Herbert Bos. FPL-3E: Towards language support for recon.gurable \npacket processing. In SAMOS, pages 201 212. Jul 2005. [15] Mihai Dobrescu, Norbert Egi, Katerina Argyraki, \nByung-Gon Chun, Kevin Fall, Gianluca Iannaccone, Allan Knies, Maziar Manesh, and Sylvia Ratnasamy. RouteBricks: \nExploiting parallelism to scale soft\u00adware routers. In SOSP, Oct 2009. [16] Sergei Egorov and Gene Savchuk. \nSNORTRAN: An Optimizing Com\u00adpiler for Snort Rules. Fidelis Security Systems, 2002. [17] Conal Elliott \nand Paul Hudak. Functional reactive animation. In ICFP, pages 163 173, Jun 1997. [18] David Erickson \net al. A demonstration of virtual machine mobility in an OpenFlow network, Aug 2008. Demo at ACM SIGCOMM. \n[19] Nate Foster, Rob Harrison, Matthew L. Meola, Michael J. Freedman, Jennifer Rexford, and David Walker. \nFrenetic: A high-level langauge for OpenFlow networks. In PRESTO, Nov 2010. [20] Natasha Gude, Teemu \nKoponen, Justin Pettit, Ben Pfaff, Mart\u00b4in Casado, Nick McKeown, and Scott Shenker. NOX: Towards an op\u00aderating \nsystem for networks. SIGCOMM CCR, 38(3), 2008. [21] Nikhil Handigol, Srinivasan Seetharaman, Mario Flajslik, \nNick McK\u00adeown, and Ramesh Johari. Plug-n-Serve: Load-balancing web traf.c using OpenFlow, Aug 2009. Demo \nat ACM SIGCOMM. [22] Brandon Heller, Srini Seetharaman, Priya Mahadevan, Yiannis Yiak\u00adoumis, Puneet Sharma, \nSujata Banerjee, and Nick McKeown. Elastic-Tree: Saving energy in data center networks. In NSDI, Apr \n2010. [23] Timothy L. Hinrichs, Natasha S. Gude, Martin Casado, John C. Mitchell, and Scott Shenker. \nPractical declarative network manage\u00adment. In WREN, pages 1 10, 2009. [24] Eddie Kohler, Robert Morris, \nBenjie Chen, John Jannotti, and M. Frans Kaashoek. The Click modular router. ACM Transactions on Computer \nSystems, 18(3):263 297, Aug 2000. [25] Teemu Koponen, Martin Casado, Natasha Gude, Jeremy Stribling, \nLeon Poutievski, Min Zhu, Rajiv Ramanathan, Yuichiro Iwata, Hi\u00adroaki Inoue, Takayuki Hama, and Scott \nShenker. Onix: A distributed control platform for large-scale production networks. In OSDI, Oct 2010. \n[26] Bob Lantz, Brandon Heller, and Nick McKeown. A network in a laptop: Rapid prototyping for software-de.ned \nnetworks. In HotNets, pages 1 6, 2010. [27] Boon Thau Loo, Tyson Condie, Joseph M. Hellerstein, Petros \nMani\u00adatis, Timothy Roscoe, and Ion Stoica. Implementing declarative over\u00adlays. SIGOPS, 39(5):75 90, 2005. \n[28] Boon Thau Loo, Joseph M. Hellerstein, Ion Stoica, and Raghu Ra\u00admakrishnan. Declarative routing: \nExtensible routing with declarative queries. In SIGCOMM, pages 289 300, 2005. [29] Geoffrey Mainland, \nGreg Morrisett, and Matt Welsh. Flask: Staged functional programming for sensor networks. In ICFP, pages \n335 346, 2008. [30] John Markoff. Open networking foundation pursues new standards. The New York Times, \nMar 2011. See http://nyti.ms/eK3CCK. [31] Nick McKeown, Tom Anderson, Hari Balakrishnan, Guru Parulkar, \nLarry Peterson, Jennifer Rexford, Scott Shenker, and Jonathan Turner. Open.ow: Enabling innovation in \ncampus networks. SIGCOMM CCR, 38(2):69 74, 2008. [32] Leo A. Meyerovich, Arjun Guha, Jacob Baskin, Gregory \nH. Cooper, Michael Greenberg, Aleks Brom.eld, and Shriram Krishnamurthi. Flapjax: A programming language \nfor Ajax applications. In OOPSLA, pages 1 20, 2009. [33] Ankur Nayak, Alex Reimers, Nick Feamster, and \nRuss Clark. Reso\u00adnance: Dynamic access control in enterprise networks. In WREN, Aug 2009. [34] Henrik \nNilsson, Antony Courtney, and John Peterson. Functional reactive programming, continued. In Haskell Workshop, \npages 51 64, Oct 2002. [35] Vern Paxson. Bro: A system for detecting network intruders in real\u00adtime. \nComputer Networks, 31(23 24):2435 2463, Dec 1999. [36] John Peterson, Paul Hudak, and Conal Elliott. \nLambda in motion: Controlling robots with Haskell. In PADL, Jan 1999. [37] Robert Soul\u00b4e, Martin Hirzel, \nRobert Grimm, Bu.gra Gedik, Henrique Andrade, Vibhore Kumar, and Kun-Lung Wu. A universal calculus for \nstream processing languages. In ESOP, pages 507 528, 2010. [38] William Thies, Michal Karczmarek, and \nSaman Amarasinghe. Streamit: A language for streaming applications. In International Con\u00adference on Compiler \nConstruction, pages 179 196, Apr 2002. [39] Andreas Voellmy and Paul Hudak. Nettle: Functional reactive \npro\u00adgramming of OpenFlow networks. In PADL, Jan 2011. [40] Richard Wang, Dana Butnariu, and Jennifer \nRexford. OpenFlow\u00adbased server load balancing gone wild. In Hot-ICE, Mar 2011.   \n\t\t\t", "proc_id": "2034773", "abstract": "<p>Modern networks provide a variety of interrelated services including routing, traffic monitoring, load balancing, and access control. Unfortunately, the languages used to program today's networks lack modern features - they are usually defined at the low level of abstraction supplied by the underlying hardware and they fail to provide even rudimentary support for modular programming. As a result, network programs tend to be complicated, error-prone, and difficult to maintain.</p> <p>This paper presents Frenetic, a high-level language for programming distributed collections of network switches. Frenetic provides a declarative query language for classifying and aggregating network traffic as well as a functional reactive combinator library for describing high-level packet-forwarding policies. Unlike prior work in this domain, these constructs are - by design - fully compositional, which facilitates modular reasoning and enables code reuse. This important property is enabled by Frenetic's novel run-time system which manages all of the details related to installing, uninstalling, and querying low-level packet-processing rules on physical switches.</p> <p>Overall, this paper makes three main contributions: (1) We analyze the state-of-the art in languages for programming networks and identify the key limitations; (2) We present a language design that addresses these limitations, using a series of examples to motivate and validate our choices; (3) We describe an implementation of the language and evaluate its performance on several benchmarks.</p>", "authors": [{"name": "Nate Foster", "author_profile_id": "81444600818", "affiliation": "Cornell University, Ithaca, NY, USA", "person_id": "P2801424", "email_address": "jnfoster@cs.cornell.edu", "orcid_id": ""}, {"name": "Rob Harrison", "author_profile_id": "81477640650", "affiliation": "Princeton University, Princeton, NJ, USA", "person_id": "P2801425", "email_address": "robh2@cs.princeton.edu", "orcid_id": ""}, {"name": "Michael J. Freedman", "author_profile_id": "81410595669", "affiliation": "Princeton University, Princeton, NJ, USA", "person_id": "P2801426", "email_address": "mfreed@cs.princeton.edu", "orcid_id": ""}, {"name": "Christopher Monsanto", "author_profile_id": "81488673230", "affiliation": "Princeton University, Princeton, NJ, USA", "person_id": "P2801427", "email_address": "chris@monsan.to", "orcid_id": ""}, {"name": "Jennifer Rexford", "author_profile_id": "81100508545", "affiliation": "Princeton University, Princeton, NJ, USA", "person_id": "P2801428", "email_address": "jrex@cs.princeton.edu", "orcid_id": ""}, {"name": "Alec Story", "author_profile_id": "81488671258", "affiliation": "Cornell University, Ithaca, NY, USA", "person_id": "P2801429", "email_address": "avs38@cornell.edu", "orcid_id": ""}, {"name": "David Walker", "author_profile_id": "81100426485", "affiliation": "Princeton University, Princeton, NJ, USA", "person_id": "P2801430", "email_address": "dpw@cs.princeton.edu", "orcid_id": ""}], "doi_number": "10.1145/2034773.2034812", "year": "2011", "article_id": "2034812", "conference": "ICFP", "title": "Frenetic: a network programming language", "url": "http://dl.acm.org/citation.cfm?id=2034812"}