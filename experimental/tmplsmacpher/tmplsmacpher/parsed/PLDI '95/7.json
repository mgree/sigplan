{"article_publication_date": "06-01-1995", "fulltext": "\n Corpus-based Static Branch Prediction Brad Calder, Dirk Grunwald, Donald Lindsay, James Martin, Michael \nMozer, and Benjamin Zom Department of Computer Science Campus Box 430 University of Colorado Boulder, \nCO 80309-0430 USA Abstract this paper, we describe ESP and its successful application to the problem \nof program-based branch prediction. Correctly predicting the direction that branches will take is Branch \nprediction is the process of correctly predicting increasingly important in today s wide-issue computer \narchi-whether branches will be taken or not before they are actually tectures. The name program-based \nbranch prediction is given executed. Branch prediction is important, both for computer to static branch \nprediction techniques that base their predic-architectures and compilers. Compilers rely on branch pre\u00adtion \non a program s structure. In this paper, we investigate diction and execution estimation to implement \noptimizations a new approach to program-based branch prediction that uses such as trace-scheduling [12, \n13] and other profile-directed a body of existing programs to predict the branch behavior optimization \n[8, 9]. in a new program. We call this approach to program-based Wide-issue computer architectures rely \non predictable con\u00adbranch prediction, evidence-based static prediction, or 13SP, trol flow, and failure \nto correctly predict a branch results in The main idea of ESP is that the behavior of a corpus of pro-delays \nfor fetching and decoding the instructions along the grams can be used to infer the behavior of new programs. \nIn incorrect path of execution. The penalty for a mispredicted this paper, we use a neural network to \nmap static featores as-branch may be several cycles long. For example, the mis\u00adsociated with each branch \nto the probability that the branch predict penalty is 4 to 5 cycles on the Digital Alpha AXP will be \ntaken. ESP shows significant advantages over other 21064 processor, In previous studies, we found that \ncondi\u00adprediction mechanisms. Specifically, it is a program-based tional branches in C programs were executed \napproximately technique, it is effective across a range of programming lan-every 8 instructions on the \nAlpha architecture [7], Current guages and programming styles, and it does not rely on the wide-issue \narchitectures can execute four or more instntctions use of expert-defined heuristics. In this paper, \nwe describe per cycle. As a result, such architectures are likely to exe\u00adthe application of ESP to the \nproblem of branch prediction cute branch instructions every two cycles or less and effective and compare \nour results to existing program-based branch pre-branch prediction on such architectures is extremely \nimpor\u00ad dictors, We also investigate the applicability of ESP across tant. Many approaches have been taken \nto branch prediction, computer architectures, programming languages, compilers, some of which involve \nhardware [5, 23] while others involve and run-time systems. Averaging over a body of 43 C and software \n[3, 6, 11]. Software methods usually work in tan-Fortran programs, ESP branch prediction results in a \nmiss rate dem with hardware methods, For example, some architectures of 20%, as compared with the 25cZ0miss \nrate obtained using have a likely bit that can be set by a compiler if a branch is the best existing \nprogram-based heuristics. determined to be likely taken by a compiler. Compilers typically rely on two \ngeneral approaches for branch prediction. Profile-based methods use program profiles 1 Introduction to \ndetermine the frequency that branch paths are executed. Fisher and Freudenberger showed that profile-based \nbranch In this paper, we propose a new technique for program-based prediction can be extremely successfid \nin reducing the number branch prediction based on a general approach that we have in\u00adof instructions \nexecuted between mis-predicted branches [11]. vented called Evidence-based Static Prediction (ESP). Our \nre-The main drawback of profile-based methods is that additional sults show that using our new approach \nresults in better branch work is required on the part of the programmer to generate the prediction than \nall existing program-based techniques. In ad\u00adprogram profiles, dition, our ESP approach is very general, \nand can be applied Program-based branch prediction methods attempt to pre\u00ad to a wide range of program \nbehavior estimation problems. In dict branch behavior in the absence of profile information and are based \nonly on a program s structure, Some of these tech- Permission to copy without fee all or part of this \nmaterial is niques use heuristics based on local knowledge that can be granted provided that the copies \nare not made or distributed for encoded in the architecture [14, 18]. Other techniques rely on direct \ncommercial advantage, the ACM copyright notice and the applying heuristics based on less local program \nstructure in an title of the publication and its date appear, and notice is given effort to predict branch \nbehavior [3]. In this paper, we describe that copying is by permission of the Association of Computing \na new approach to program-based branch prediction that does Machinery.To copy otherwise, or to republish, \nrequires a fee and/or specific permission. SIGPLAN 95La Jolla, CA USA @ 1995 ACM 0-89791 -697 -2/95/0006 \n...$3.50 79 not rely on such heuristics. Our branch prediction relies on a general program-based prediction \nframework that we call ESP. The main idea of ESP is that the behavior of a corpus of pro\u00adgrams can be \nused to infer the behavior of new programs. That is, instead of using a different execution of a program \nto pre\u00addict its own behavior (as is done with profile-based methods), we use the behavior of a large \nbody of different programs (the training set, or corpus) to identify and infer common behavior. Then \nwe use this knowledge to predict branches for programs that were not included in the training set. In \nparticular, in this paper we use a neural network to map static features associ\u00adated with each branch \nto the probability that the branch will be taken. Branch prediction using ESP has several important advan\u00adtages \nover existing program-based branch prediction methods. First, because the technique automatically maps \nstatic features of branches to a probability that they will be taken, our tech\u00adnique is suitable for \nprogram-based branch prediction across different languages, compilers, and computer architectures. Existing \ntechniques rely on compiler-writer defined heuristics that are based on an intuition about common programming \nid\u00adioms. Second, given a large amount of static information about each branch. the technique automatically \ndetermines what parts of that information are useful, Thus, it does not rely on trial\u00adand-error on the \npart of the compiler writer searching for good heuristics. Finally, our results show that ESP branch \nprediction outperforms existing heuristic program-based branch predic\u00adtion techniques over a body of \n43 C and Fortran programs. In particular, our heuristics have an average overall miss rate of 20%, which \ncompares to the 25% miss rate of the best existing heuristic technique, and the 8% miss rate of the perfect \nstatic predictor. This paper has tbe following organization. In Section 2 we discuss previous approaches \nto program-based branch pre\u00addiction and other knowledge-based approaches to program op\u00adtimization. In \nSection 3 we discuss the details of our ESP branch prediction method, Section 4 describes the methods \nwe used to evaluate and compare ESP prediction with previous approaches, and Section 5 presents our results. \nWe summarize our conclusions in Section 6 and also discuss possible future directions to take with this \nresearch. 2 Background In this section, wediscuss the existing approaches to static branch prediction \nand also discuss other knowledge-based ap\u00adproaches to compiler optimization. 2.1 Program-Based Branch \nPrediction Methods One of the most simple program-based methods for branch pre\u00addiction is called backward-taken/forward-not-taken \n(BTFNT). This technique relies on the heuristic that backward branches are usually loop branches, and \nas such are likely to be taken. One of the main advantages of this technique is that it relies solely \non the sign bit of the branch displacement, which is al\u00adready encoded in the instruction. While simple, \nBTFNT is also quite successful. Our results in Section 5 show it has an over\u00adall miss rate in our experiments \nof sq~o, Any more sophisti\u00adcated program-based prediction techniques must do better than BTFNT to be \nviable. To facilitate program-based methods for branch predic\u00adtion, some modem architectures provide \na branch-likely bit in each branch instruction [1], In these architectures, compilers can employ either \nprofile-based [11 ] or program-based tech\u00adniques to determine what branches are likely to be taken, In \nrecent work, Ball and Lams [3] showed that applying a number of simple program-based heuristics can significantly \nimprove the branch prediction miss rate over BTFNT on tests based on the conditional branch operation. \nA complete summary of the Ball and Lams heuristics is given in Table 1 (as described in [22]). Their \nheuristics use information about the branch opcode, operands, and characteristics of the branch successor \nblocks, and encode knowledge about common programming idioms. Two questions arise when employing an approach \nlike that taken by Ball and Lams. First, an important question is which heuristics should be used. In \ntheir paper, they describe seven heuristics that they considered successful, but also noted that We tried \nmany heuristics that were unsuccessful, [3] A sec\u00adond issue that arises with heuristic methods is how \nto decide what to do when more than one heuristic applies to a given branch. This problem has existed \nin the artificial intelligence community for many years and is commonly known as the ev\u00adidence combination \nproblem. Ball and Lams considered this problem in their paper and decided that the heuristics should \nbe applied in a fixed order; thus the first heuristic that applied to a particular branch was used to \ndetermine what direction it would take. They determined the best fixed order by conducting an experiment \nin which all possible orders were considered. We call using this pre-determined order for heuristic combination \nthe A Priori Heuristic Combination (APHC) method. Using APHC, Ball and Larus report an average overall \nmiss rate on the MIPS architecture of 20%. In a related paper, Wu and Lams refined the APHC method of \nBall and Lams [22]. In that paper, their goal was to deter\u00admine branch probabilities instead of simple \nbranch prediction. Whereas with branch prediction, the goal is to determine a single bit of information \nper branch (likely versus unlikely), with branch probabilities, the goal is to determine the numeric \nprobability that a branch is taken or not taken. Wu and Lams abandoned the simplistic evidence combination \nfunction of APHC in favor of an evidence combination function borrowed from Dempster-Shafer theory [1 \nO, 17]. We call this form of ev\u00adidence combination Dempster-Shafer Heuristic Combination (DSHC). By making \nsome fairly strong independence assump\u00adtions, the Dempster-Shafer evidence combination function can produce \nan estimate of the branch probability from any number of sources of evidence. For example, if one heuristic \nindicates that a branch is likely to be taken with probability X%, while another says it is likely to \nbe taken with probability Y%, then DSHC allows these two probabilities to be combined. The probabilities \nX% and Y% that Wu and Lams use are taken directly from the paper of Ball and Lams [3]. We refer to a \nDSHC algorithm based on this data as DSHC(B&#38;L). Because the goal of Wu and Lams was to perform program\u00adbased \nprofile estimation, they give no results about how the DSHC method works for program-based branch prediction. \nOne of the contributions of our paper is that we quantify the effectiveness of the DSHC method for branch \nprediction. Wagner et al. [21] also used heuristics similar to those of Ball and Lams to perform program-based \nprofile estimation. 80 Heuristic Heuristic. Name Description Loop Branch Predict that the edge back \nto the loop s head is taken and the edge exiting the loop is not taken. Pointer If a branch compares \na pointer against null or compares two pointers, predict the branch on false condition as taken. Opcode \nIf a branch checks an integer for less than zero, less than or equal to zero, or equal to a constant, \npredict the branch on fake condition. Guard If a register is an operand of the branch comparison, the \nregister is used before being defined in a successor block, and the successor block does not post-dominate \nthe branch, predict the successor block as taken. Loop Exit If a comparison is inside a loop and no successor \nis a loop head, predict the edge exiting the loop as not taken. Loop Header Predict the successor that \ndoes not post-dominate and is a loop header or a loop pre-header as taken. Call Predict the successor \nthat contains a call and does not post-dominate the branch as taken. Store Predict the successor that \ncontains a store instruction and does not post-dominate the branch as not taken. Return Predict the successor \nthat contains a return as not taken. Tablel: Summary of the BatVLaras Heuristics They also applied theheuristics \nina fixed order. llreyreport branch prediction miss rate results similar to those of Ball and Lams. \n2.2 Kncrwledge-Base Approaches to Optimization Our ESP method relies on collecting data from a corpus \nof pro\u00adgram behavior and using that data to perform program-based prediction. There is little other work \nin compiler optimization that has taken this approach. We summarize the work we are aware of here. In \n[2], Balasundaram et al, address a somewhat different program-based estimation problem, The authors wanted \nto make compile-time decisions about data partitioning across a parallel computer. They report on the \nidea of using profile data to train an estimator. This training, an offline step, generates code which \nis then incorporated into their compiler. Training only needs to bedoneonceper compilation target, and \nis reported to be better than using a parametrized theoretical model. While the strategy they employ \nis similar to ESP, their application domain is quite different. In addition, our results show that this \ngeneral approach of knowledge-based training can be used to enhance a wide class of optimization based \non program behavior estimation. 3 Evidence-based Branch Prediction In this section, we propose a general \nframework for program\u00ad based prediction. Our method, ESP, is generally described as follows. A body of \nprograms and program input is gathered (the corpus). Particular static information (the static feature \nser) about important static elements of the corpus (e.g., instruc\u00adtions) are recorded. The programs in \nthe corpus are executed, and the corresponding dynamic behavior is associated with each static element \n(e.g., the number of times a branch is taken and not-taken is associated with each branch). At this point, \nwe have accumulated a body of knowledge about the relation\u00adship between static program elements and dynamic \nbehavior. This body of knowledge can then be used at a later time to pre\u00addict the behavior of instructions \nwith similar static features for programs not in the corpus. With this broad definition of our framework \nin mind, we now describe how we apply this general framework to the specific problem of branch prediction. \n3.1 ESP Branch Prediction In applying ESP to the problem of branch prediction, we instan\u00adtiate the above \nframework in the following way. The static pro\u00adgram elements we are interested in are the program branch \nin\u00adstructions. For this study, we consider only two-way branches. For each branch instruction in the \nprogram text, we record a large static feature set (see Table 2). Some of the features are properties \nof the branch instruction itself (e.g., the brancb opcode), others are properties of the registers used \nto define the register in the branch instmction (e.g., the opcode of the instructions that defined them), \nwhile others are properties of the procedure that the branch is in (leaf versus non-leaf). The existence \nof some features is dependent on the values of other features. For example, feature 4 is only meaningful \nif feature 3 has an RA operand. We call such features dependent static features. We chose the feature \nset shown in Table 2 based on several criteria. First, we encoded information that we believed would \nlikely be predictive of behavior. This information included some of the information used to define the \nBaWLarus heuristics (e.g., information about whether a call appears in a successor of the branch). Second, \nwe encode other information that was easily available. For example, since the opcodes that define the \nbranch instruction register are readily available, we include them as well. Similarly, information about \nthe procedure type Feat. Feature Num. Name 1 Br. opcode 2 Br. direction 3 W. operand opcode 4 RA opcode \n5 RB opcode 6 Loop header 7 Language 8 Procedure type 9-16 9 Br. dominates 10 Br. postdominates 11 Succ. \nEnds 12 Succ. Loop 13 Succ. Backedge 14 Succ, Exit 15 Succ. UseDef 16 Succ Call 17-24 Table Feature \nDescription The opcode of branch instrrtction. F Forward branch, B Backwards branch The opcodeof the \ninstruction that defines the register used in the branch instruction (or ?, if the branch operand is \ndefined in a previous basic block). If the instruction in (3) uses an RA register, this is the opcode \nof the instruction that defines thatregister(? otherwise). If the instruction in (3) uses an RB register, \nthis is the opcode of the instruction that defines that register (? otherwise). LH the basic block is \na loop header, NLH -not a loop header The language of the procedure the branch is in (C or FORT). The \nbranches procedure is a Leaf, NonLeaf or calls itself recursively (CallSelf) Features of the Taken Successor \nof the Branch D basic block dominates this successor, or ND does not dominate PD the successor basic \nblock post-dominates the basic block with the branch, or NPD does not post-dominate Branch type ending \nsuccessor basic block, possible values (5T fall through, CBR conditional branch, UBR unconditional \nbranch, BSR branch sub\u00adroutine, JUMP jump, IJUMP indirect jump, JSR jump subroutine, IJSR indirect \njump subroutine, RETURN, COROUTINE, LASTJUMP_KIND, or NOTHING) LH the successorbasic block is a loop \nheader or unconditionally passes control to a basic block which is a loop header, NLH not a loop header \nLB the edge getting to the successor is a loop back edge, NLB not a loop back edge LE the edge getting \nto the successor is a loop exit edge, NLE not a loop exit edge UBD the successor basic block has a \nuse of a register before defining it and that register was used to determine the destination of the current \nconditional branch instruction. NU no use before def in successor PC the successor basic block contains \na procedure call or unconditionally passes control to a basic block with a procedure call, NPC no procedure \ncall down here Features of the Not Taken Successor of the Branch As above features 9 16 2: Stat:c Feature \nSet Used nr the ESP Branch Prediction Study. 82 is readily available. We note that the feature set listed \nhere is the only one we have yet tried. We have made no effort to identify a particularly good feature \nset, and our positive results suggest that such feature tuning is unnecessary. Having defined the static \nfeature set, we then determine the static feature set for each branch in the corpus of programs. We next \nrun the programs in the corpus and collect information about how often each branch is taken and not taken, \nThe goal is to associate two pieces of dynamic information with each branch instruction: how frequently \nthe branch was executed and how often was it taken. Because execution frequency is program dependent, \nwe normalize the branch frequency by the total number of branches executed in the program. We compute \nthe normalized branch weight by dividing how many times the branch was executed by the total number of \nbranches executed by the program (resulting in a number between zero and one). Finally, we associate \nwith each branch instruction in the corpus its static feature set, its normalized branch weight, and \nits branch probability (percentage of the time the branch was taken). 3.1.1 Prediction using Neural Nets \nOur goal is to have a system that can predict the branch prob\u00adability for a particular branch from its \nstatic feature set. This system should accurately predict not just for the programs in the corpus, but \nalso for previously unseen programs, One way of doing such prediction is via a feedforward neural network \n[19], A feedforward neural network maps a numerical input vector to a numerical output. Here, the input \nvector consists of the feature values in the static feature set, and the output is a scalar indicating \nthe branch probability. Figure 1 depicts the branch prediction neural network. A neural network is composed \nof processing units, depicted in the Figure by circles. Each processing unit conveys a scalar value known \nas its activi~. The activity pattern over the bottom row of units is the input to the network, The activity \nof the top unit is the output of the network. Activity in the network flows from input to output, through \na layer of intermediate or hidden urrits, via weighted connections. These connections are depicted in \nthe figure by links with arrows indicating the direction of activity flow, This is a standard neural \nnetwork architecture. We also use a fairly standard neural network dynamics in which the activity of \nhidden unit i, denoted hi, is computed as: h; = tanh(~ Wijzj + hi), where ~j is the activity of input \nunit j, is the connection Wij weight from input unit j to hidden unit i, bi is a bias weight associated \nwith the unit, and tanh is the hyperbolic tangent function, Similarly, the output unit activity, denoted \ny, is computed from the hidden unit activities: y = .5 tanh( 7Jihi+a)+l, E i where vi is the connection \nweight from hidden unit i to the output unit and a is a bias weight associated with the output unit. \nThe tanh tirnction is normalized to achieve an activity range of [0, 1] for the output unit. The input-output \nbehavior of the neural network is deter\u00ad mined by its free parameters, the weights w and v and biases \nb and a. These parameters are set by an algorithm known as backpropaga~ion [1 6]. This is a gradient \ndescent procedure for adjusting the parameters such that performance of the network on a training corpus \nis optimized. The standard measure of performance is the sum of squared errors, E = ~(yk tk)z, k where \nk is an index over examples in the training corpus, yk is the actual output of the network when training \ninput k is presented, and tk is the target output the output indicated for that example in the training \ncorpus. In this application, however, we have a different criterion for good performance, We want to \nminimize two sorts of errors, missed branches (MB) and branches incorrectly taken (BIT). MB occur when \nthe predictor says that the branch will be taken with probability less than.5 when the branch is in reality \ntaken; BIT occur when the predictor says that the branch will be taken with probability greater than.5 \nwhen the branch is in reality not taken. If the network output for example k, is binary-1 if the predicate \nthe branch probability is greater than .5 is believed to be true, O otherwise-then the total number of \nerrors due to MB for example k is EAZB = (1 -~k)tknk, where nk is the normalized branch weight, The \nproduct tknk gives the (relative) number of cases where the branch is taken. All of these branches are \nmissed if yk = O (or equivalently, 1 ~k = 1). Similarly, the total number of errors due to BIT is EBIT \n= Vk(l tk)nk. Because these two types of errors have equal cost, the total error is simply E=~EMB+EBIT= \n~nk[yk(l -r!k)+i!k(l -yk)]. kk This is used as the error measure to be minimized by the neural net training \nprocedure. That is, the free parameters in the neural net are adjusted such that the network will produce \noutputs yk such that E is minimized. Note that this does not require that the network accurately predict \nbranch probabilities per se, as we were assuming previously.l Each input unit s activity is normalized \nover the training set to have zero mean and standard deviaiion 1. The same normal\u00adization is applied \nfor test cases. We deal with nonmeaningful 1In the above discussion, we assumed that tbc network output \nwill be eirber O or 1. However, the output must be continuous-valued in order to apply gradient\u00adbased \ntraining procedures. Thus, we use the continuous activation rule for y presented earlier, and simply \ninterpret the continuous output us the network s confidence that the true branch probability is gmarcr \nthan 5. 83 output (branch probability) hidden layer input (static feature set)  Figure 1: The branch \nprediction neural network. Each circle represents a processing unit in the network, and the links between \nunits depict the flow of activity. dependent static features by setting their input activity to Oafter \nmiss rate and tbe pointer heuristic had a miss rate of 89~o. the normalization step; this prevents the \nnonmeaningful fea-These results show that applying heuristics based on intuition tures from having any \neffect on the computation, and is equiva-is both difficult and can often result in incorrect conclusions. \nlent to gating the flow of activity from these features by another Thus, new betrristics will be required \nfor new architectures, feature that indicates the relevance of the dependent features programming languages, \nand even compilers. for a particular example. We use a batch training procedure A second problem with \nbettristic approaches is determining in which weights are updated following a sweep through the how to \ncombine them when more than one apply to the same entire training corpus, and an adaptive learning rate \nprocedure situation. While Wu and Larus attempted to solve this problem wherein the learning rate for \nthe network is increased if error using Dempster-Shafer theory, our results show that using the drops \nregularly or is decreased otherwise, Momentum is not DSHC method results in slightly bigher miss rates \nthan the used. Training of the network continues until the thresholded more ad hoc APHC method. This \nis likely a result of the error of the net no longer decreases. By thresholded error. we strong independence \nassumptions embodied in the Dempster\u00admean the error computed when the output is first thresholded Shafer \nevidence combination function [15]. to values O or 1. This achieves a form of early stopping, and Our \nESP method addresses these two disadvantages di\u00adthereby helps to prevent overfitting. rectly. Instead \nof relying on experts to think of heuristics and to test them to determine if they are effective, our \nmethod extracts features associated with predictable behavior automat\u00ad 3.1.2 Discussion ically. The \nESP method also has disadvantages, as well. First, In Section 2, we noted that there were two inherent \nproblems a corpus of programs must be available. For our results in with heuristic-based approaches to \nprogram-based prediction. Section 5, we initially had only 8 C programs to examine, Our First there is \nthe problem of determining what heuristics to average, ESP prediction results for these 8 programs were \nthe use. In particular, the search for successful heuristics requires same as the APHC and DSHC results, \nAfter we increased tbe a significant amount of effort and cannot be easily automated. corpus of 8 C programs \nto 23 C programs, the average mispre-Furthermore, the effectiveness of particular heuristics (e.g., diction \nrate for ESP was 5% lower than the average miss rates the return heuristic which predicts the successor \nwithout tbe for the APHC and DSHC techniques, Second, our approach return instruction to be taken) will \ndepend on the programming requires that the feature set be defined. Our results indicate language, compiler, \nprogramming style, and architecture being that having too much information does not degrade the ESP used, \npredictions (we have not investigated the impact of not having For example, one might think that tbe \nreturn heuristic is enough data in the feature set), Third, our current implemen\u00adlikely to be more effective \nwhen applied to languages, such as tation of ESP requires that tbe neural net be trained. Such Scheme, \nwhere recursion is the most commonly used mecha-training requires someone who understands neural nets \nfairly nism for performing iteration. Likewise, the pointer heuristic, well, probably at the level of \na person who has taken a course which assumes pointer comparisons to null and for equality in neural \nnets. We envision that if the ESP approach becomes will fail, is more likely to be applicable in a pointerfrrl \nlan-sufficiently widespread, then took that facilitate such training guage like Scheme. We found, however, \nthat when we applied would be made available. We also note that preliminary re\u00adthese heuristics to three \nScheme programs (boyer, corewar, sults we have obtained using decision trees instead of neural and sccomp, \nall compiled with the Scheme-to-C compiler)z, networks are comparable to the neural net results presented \nthe results show that the return heuristic had an average 56% here. Moreover, decision trees are easier \nto use and the knowl\u00ad edge they encode can be automatically translated into simple if-then rules. In \nthe future, we plan to investigate how the ESP approach works for lan\u00ad guages such as CH and Scheme as \nwell. 84 Evaluation Methods To perform our evaluation, we collected information from 43 C and Fortran \nprograms. During our study, we instrumented the programs from the SPEC92 benchmark suite and other pro\u00ad \ngrams, including many from the Perfect Club [4] suite. We used ATOM [20] to instntment the programs. \nDue to the strt~c\u00ad ture of ATOM, we did not need to record traces and could trace very long-running programs, \nThe programs were compiled on a DEC 3000-400 using the Alpha AXP-21064 processor using either the DEC \nC or FORTRAN compilers. Most progra)ms were compiled using the standard OSF/1 V 1.2 operating sys\u00ad tems; \nother programs were compiled using different compilers and different versions of the operating system. \nMost programs were compiled with standard optimization (-O). Each program was run once to collect information \nabout branch frequency and the percentage of taken branches. For the SPEC92 programs, we used the largest \ninput distributed with the SPEC!?2 suite, Table 3 shows the basic statistics for the programs we instrumented. \nThe first column lists the number of instruc\u00adtions traced and the second column gives the percentage \nof instructions that are conditional branches, The third COIUmn gives the percentage of conditional branches \nthat are taken, The columns labeled Q-50 , Q-75 , Q-90 , Q-95 , Q-99 , and Q-1 00 show the number of \nbranch instmction sites that contribute 50, 75, 90, 95, 99 and 100% of all the executed conditional branches \nin the program. The next column Static shows the total number of conditional branch sites in each pro\u00adgram. \nThus, in Alvinn, two branch instructions constitute over 90% of all executed branches and correctly predicting \nthese two conditional branches is very important. The ATOM instrumentation tool provides a concrete rep\u00adresentation \nof the program, and we used this information to construct a control flow graph. Using the control How \ngraph, we computed the dominator and post-dominator trees, Follow\u00ading this, we determined the natural \nIoop headers and applied the same definition of natural loops used by Ball and Lams to determine the \nloop bodies [3], We used ATOM to reproduce the Ball and Larus APHC resuks, and to generate the static \nfeature sets with the corresponding branch probabilities whlich are used to train the neural net for \nESP, For ESP, we did not use the information gathered about a given program to predict the branches for \nthat same program; rather, we used a cross validation study. We took all of the programs, except the \none program for which we want to gather prediction results and fed the corpus of programs into the tneu\u00adral \nnet. We then use the neural net s branch probabilities to predict branches for that program not inchtded \nin the corpus. This provides a conservative estimate of how well ESP will perform since we are predicting \nthe behavior of a program that the neural net has not seen, For the ESP results shown in Section 5, we \nperformed the cross validation breaking the programs into two groups -C programs and FORTRAN pro\u00adgrams. \nWe performed cross validation feeding the feature sets for 22 of the C programs at a time into the neural \nnet, predicting branches for the 23rd C program not included in initial 22. We did the same for FORTRAN \nprograms feeding into the neural net the feature sets for 19 of the 20 programs in order to predict branches \nfor the 20th program. 5 Results We now compare the prediction accuracy of a priori heuris\u00adtic combination \n(APHC) branch prediction [3], the Dempster-Shafer heuristic combination (DSHC) proposed by Wu and Lanrs \n[22], and our ESP technique. Following this, we show that the APHC and DSHC techniques are sensitive \nto differ\u00adences in system architecture and compilers. 5.1 Comparison: APHC, DSHC and ESP Table 4 shows \nthe branch misprediction rate for the meth\u00adods we implemented. The first column shows the results for \nthe BTFNT architecture, the second column shows the re\u00adsults for our implementation of the Ball and Lams \nheuristics, and the third and fourth columns show the results when ap\u00adplying Dempster-Shafer to those \nheuristics. In implement\u00ading DSHC, we use both the original prediction rates specified in [3], DSHC(B&#38;L), \nand the prediction rates produced by our implementation, DSHC(Ours), Later, we compare the simi\u00adlarity \nbetween these two sets of prediction heuristics as seen in Table 6, The fifth column in Table 4 shows \ntbe results for our ESP method and the last column shows the results for the perfect static profile prediction. \nTable 4 reveals sev\u00aderal interesting points, First, the overall average shows that the Dempster-Shafer \nmethod performs no better than the fixed order of heuristics. Wu and Lanrs [22] said When more than one \nheuristic applies to a branch, combining the probabilities estimated by the ap\u00adplicable heuristics shouldproduce \non overal[branch probability that is more accurate than the individ\u00adual probabilities. However, there \nwas no comparison to the earlier results of Ball and Lams. In 6 cases (flex, sort, mdl j sp2, CSS, NAS, \nTFS), the Dempster-Shafer method is more than 5% worse than the simple APHC ordering, while the APHC \nor\u00addering method is 5% worse in only three cases (wdi f f, SDS, LWS). The intuition in [22] was correct; \nhowever, the Dempster-Shafer theory does not combine the evidence well enough to improve branch prediction, \nThe ESP technique per\u00adforms significantly better than the Dempster-Shafer and the APHC method in 15 cases \n( burg, flex, gzip, indent, od, perl, siod, sort, tex, wdiff, fpppp, su2cor, tomcatv, LWS, and TIS), \nand has significantly worse per\u00adformance in only one case (mall-j sp2 ). We feel that the ESP results \nmaybe improved by expanding the feature sets used. We used a limited number of features in the feature \nset to distinguish branches, primarily using the features described by Ball and Lams. To extend the set \nof features, we need to determine what new features (e.g., infor\u00admation from the control dependencegraph) \nwe want to include, capture that information during program instrumentation, and pass those features \nto the neural net, This is a simple. process, but we have only examined a small set of the possible features. \nRather than rely on intuition about the appropriate features (e.g,. by using the Ball and Lartrs predictors), \nwe should pro\u00advide as much information to the neural network as possible and let it decide the importance. \n85 # Insn s % Cond onditio d Brar Qua] x Program Traced Branches %Taken Q2m Qm Q-90 @3\u00ad m Q-1OO -sziiF \nbc 93,395,683 10.06 42.43 41 97 160 204 273 753 1,956 bison 6,344,388 10,02 76.83 16 89 197 311 654 1,348 \n2,905 burg 721,029 12.17 62.32 30 84 153 220 465 802 1,766 flex 15,458,984 12.89 68.37 29 102 190 260 \n421 1,204 2,969 grep 745,131 19.35 72.40 6 25 94 196 422 910 3,310 gzip 309,547,166 11.08 60.75 3 13 \n29 36 49 342 2,476 indent 32,569,634 14.72 51.91 27 74 159 244 457 1,065 2,272 od 210,341,272 12.88 45.72 \n30 56 76 84 118 433 1,702 perl 181,256,552 10.26 39.89 28 88 233 342 719 2,690 12,288 sed 85,604,071 \n10.63 65.55 16 59 91 109 151 863 2,570 siod 28,750,877 13.04 56.85 14 38 95 128 186 684 2,156 sort 10,301,164 \n14.01 59.12 13 24 51 63 77 352 1,810 tex 147,820,930 7,58 57,47 39 111 259 416 790 2,365 6,050 wdiff \n76,185,396 13.21 53.65 7 11 19 24 29 502 1,618 yacr 1,017,126,630 19.24 70.73 11 33 88 127 345 1,673 \n3,442 alvinn 5,240,969,586 8,93 97.77 2 2 2 3 102 430 1,622 compress 92,629,658 12.31 68.25 4 7 12 14 \n16 230 1,124 ear 17,005,801,014 4.97 90.13 2 4 6 8 32 530 1,846 eqntott 1,810,540,418 10,78 90.30 2 2 \n14 42 72 466 1,536 espresso 513,008,174 15.96 61.90 44 104 163 221 470 1,737 4,568 gcc 143,737,915 12.60 \n59,42 245 804 1,612 2,309 3,724 7,640 16,294 li 1,355,059,387 11.30 47.30 16 33 52 80 127 556 2,428 Sc \n1,450,134,411 17.99 66.88 14 41 94 153 336 1,471 4,478 doduc 1,149,864,756 6.94 48,68 3 40 175 231 296 \n1,447 7,073 fpppp 4,333,190,877 2.44 47,74 10 28 51 73 109 744 6,260 hydro2d 5,682,546,752 6.02 73.34 \n14 43 74 111 230 1,613 7,088 mdljsp2 3,343,833,266 10.12 83.62 6 10 14 16 23 1,010 6,789 nasa7 6,128,388,651 \n2,51 79.29 8 21 55 94 277 1,083 6,581 ora 6,036,097,925 5.25 53.24 5 8 11 12 17 641 5,899 spice 16,148,172,565 \n11.51 71.63 2 12 38 63 116 1,762 9,089 su2cor 4,776,762,363 3.34 73.07 8 15 26 34 60 1,569 7,246 swm256 \n11,037,397,884 1.65 98.42 2 2 3 3 13 795 6,080 tomcatv 899,655,317 3.35 99.28 3 4 5 7 7 515 5,474 wave5 \n3,554,909,341 4.37 61.79 18 40 82 132 276 1,331 8,149 APS 1,490,454,770 3.99 50.64 44 123 283 357 x 1,617 \n8,926 Css 379,319,722 7.32 55.63 32 109 211 262 467 2,202 9,670 LWS 14,183,394,882 7,92 66,34 3 9 18 \n26 38 1,148 6,927 NAS 3,603,798,937 3.43 60.67 5 14 34 69 125 1,663 7,614 Ocs 5,187,329,629 3.02 88.57 \n3 10 46 79 197 1,447 7,084 SDS 1,108,675,255 6.77 53.05 9 25 43 67 169 1,669 7,585 TFS 1,694,450,064 \n3.17 77.42 15 38 122 220 464 1,598 7,270 TIS 1,722,430,820 5.27 51.08 8 20 31 36 66 863 6,292 Wss 5,422,412,141 \n4.76 62,36 41 145 275 344 533 1,756 7,592 Table 3: Measured attributes of the traced programs. 86 Branc \n%edictio hiss Ka Program BTFNT APHC DSHC DSHC im Perfect (B&#38;L s)(B&#38;L s) (Ours) bc 40 37 353575 \n14 bison 52 15 16 1614 4 burg 53 35 33 3226 9 flex 43 33 39 3819 9 grep 42 27 23 2219 12 gzip 33 32 33 \n3320 9 indent 42 27 28 27 19 6 od 44 44 404030 8 perl 35 39 36 3626 4 sed 45 22 222325 5 siod 50 34 32 \n3327 10 sort 44 35 41 4221 8 tex 43 37 38 3630 13 wdiff 42 32 11 114 3 yacr 32 14 11 1214 6 OtherCAvg \n43 31 29 29 T 8 . alvinn 2 2 221 0 ear 10 8 8887 compress 44 25 26 28 30 14 eqntott 47 7 776 2 espresso \n34 24 23 23 32 15 gcc 48 34 35 3431 12 Ii 43 26 252728 12 Sc 39 29 312924 9 . SPECCAvg 34 . 19 20 20 \nx 9 dodoc 23 19 20 1916 5 fpppp 42 53 52 52 35 11 hydro2d 28 17 16 16 12 4 mdljsp2 69 41 62 62 64 10 \nnasa7 8 12 12115 3 ora 46 18 181818 5 spice 16 16 18 1414 7 su2cor 17 21 20 20 12 10 swm256 1 1 111 1 \ntomcatv 44 44 44 44 1 1 wave5 19 27 24 2321 6 . SPEC Fortran Avg 29 24 26 25 m 6 . APS 28 30 34 3126 \n10 Css 39 29 403633 9 Lws 38 32 25 2518 16 NAS 42 12 222212 4 Ocs 465542 SDS 18 32 25 1921 12 TFS 12 \n10 151311 6 TIS 18 26 25 2216 16 Wss 32 28 26 2625 11 . Perf Club Avg 26 23 24 22 18 10 . Overall Avg \n34 25 26 25 T 8 - Table 4: Comparison of using Heuristics in Ball and Lams ordering, Dempster-Shafer \nTheory and ESP. The first column shows the misprediction rate of the BTFNT approach. The second column \nshows the miss rate for our implementation of the APHC method of Ball and Lams. We computed the Dempster-Shafer \nmiss rates, shown in column three, with the same values for the heuristics used by Wu and farm as well \nas the values we computed, shown in column four, The fifth column is the miss rate for the ESP technique, \nwhile the last cohrmu is the miss rate for perfect static profile prediction. In each case, smaller vahres \nare better. 87  5.2 Cross-Architectural Study of A Priori Heuristics In the paper by Ball and Lams [3], \na number of prediction heuristics were described. These heuristics were the founda\u00ad tion for the prediction \nscheme in both the study by Ball and Lams and the study by Wu and Lams. In the study by Wu and Larus, \nthe values given in [3] were used for the Dempster- Shafer combination method, even though the study \nby Wu and Lams used a different architecture, compiler and runtime sys\u00ad tem. We wondered how sensitive \nthese metrics were to differ\u00ad ences in architecture, compiler, rrmtime system and selection of programs. \nWe use the CFG, dominator, post-dominator and loop infor\u00ad mation to implement the same heuristics in \n[3], summarized in Table 1. Our implementation results for these heuristics are shown in Table 5. This \ntable shows detailed information about how the branch heuristics performed for each program. Some of \nthe programs in our suite were also used in the ear\u00ad lier study by Ball [3], and the values in parenthesis \nshow the equivalent metrics recorded in that study. In general, the val\u00ad ues are quite similar, but there \nare some small differences that we believe arise from different runtime libraries. For exam\u00ad ple, a binary-buddy \nmemory allocator would not contain any loops, while a coalescing implementation may contain several loops. \nThese library routines are part of the native operating system, and not part of the distributed benchmark \nsuite. Note that there are considerable differences, in the percentage of non-loop branches, particularly \nin eqntott. Some of these differences are caused by libraries and rrtntime systems, but others can be \nattributed to architectural features. For example, the Alpha has a conditional move operation that obliviates \nthe need for many short conditional branches, reducing the number of conditional branches that are executed. \nTable 5 further demonstrates that our implementation of the heuristics listed in [3] appear to be correct. \nThe loop miss rates are roughly the same, the heuristics cover approximately the same percentage of branches \nand the overall branch prediction miss rates are similar. There are some differences, but after some \ninvestigation, we have attributed most of these to different architectures, operating systems and compilers. \nTable 6 shows the comparison of the overall averages for the heuristics comparing the Ball and Laros \nresults on the MIPS architecture to our results on the Alpha. This table also shows the probablilites \nused in the DSHC results shown in Table 4. The B&#38;L miss rates were used for the DSHC(B&#38;L) probabilities \nand our Overall miss rates in Table 6 were used for the DSHC(Ours) probabilities in Table 4. We felt \nthat the differences seen in Table 6 were to be expected, because the two studies used a different collection \nof programs with different compilers that implement differ\u00adent optimizations for different architectures \nand used different runtime libraries. Table 6 supports our position that at least some of Ball and Larus \nheuristics are quite language dependent. First, we point out that pointers are very rare in FORTRAN, \nand as such the great success of the Pointer heuristic in FORTRAN is of little consequence because it \napplies to very few branches, Next, we see that while the Store heuristic appears successful in our FORTRAN \nprograms, it performs much worse in our C programs. Conversely, the Loop Header heuristic performs well \nin C programs, but poorly in FORTRAN programs. Over\u00adall, four of the nine heuristics show a difference \nof greater than Ncde27 9 LDT :0,0(R22) FABS F8,F11 FABS F,?O,F1O CMmLT FIO,F11,Fl 1 FBNE F1 1,Ncde30 \n/ 29 FMov F20.FS j z, BIS R31.R21,R12 BIS R31,R7,R13 j    c1 b D 30 LDT F26,0(R20) FABS F9,F28 FABS \nF26,F14 CMFTLT F14,F28,F28 FBNE F28,Ncde32 ; 21 31 FMOV F26.FY \\ BIS R31,R21,R14 : BLS R31,R7,R15 : :21 \n33 ADDL R7,#l,R7 : CMPLE R7,R27,R26 : BNE R26,Ncde34 8 &#38;l&#38;gigg;:l BNE W, Ncde28 Figure 2: Sample \ncode fragment from TOMCATV benchmark that continues most of the branches in the program. The numbers \non the edges indicate the percentage of all edge transitions attributed to a particular edge. The dotted \nedges indicate taken branches. 10% in their miss rates when our C and Fortran programs are compared. \n5.2.1 The Influence of Architectures In certain cases, we had slightly different implementations of heuristics \nthan Ball and Lams because the Alpha architecture did not allow us to implement the heuristics as originally \nstated. For example, with respect to the Opcode heuristic, the Alpha architecture has two types of branch \ninstructions; one compares floating point numbers to zero and the other integer numbers to zero, The \nconditional branch instructions always compare a register to zero. On the MIPS architecture, the branch \nif equal (BEQ) and branch if not-equal (BNE) instructions compares two registers, To accomplish the same \ntask on the Alpha, an earlier comparison must be made between the two registers, and the resulting value \nis then compared to zero, 88  Loop Branches Non-Loop anches Miss Rate Miss Rate man+ %Branches Miss \nRate Miss Rat; Overall For Loops Branches Covered By For With Miss Heuristics Hueristics Default Rate \nr= 80 30 36 = 37 bison 12 64 84 15 18 15 burg 22 66 80 39 42 35 flex 15 60 58 38 46 33 grep 9 60 89 36 \n39 27 gzip 4 48 31 45 62 32 indent 27 69 77 23 27 27 od 56 83 74 43 42 44 perl 43 69 80 34 38 39 sed \n19 54 78 19 25 22 siod 34 74 29 34 34 sort 17 63 : 50 45 35 tex 33 51 78 40 41 37 wdiff 11 65 100 44 \n44 32 yacr 4 37 85 24 31  14 Other C Avg 23 F  75 34 38 . 31 alvinn 0 3 65 40 42 2 compress 8 (12) \n57 (66) 80 (90) 38 (39) 38 (40) 25 (30) ear 2 17 96 41 41 eqntott 2 (3) 11 (49) 75 (5) 40 (37) 45 (50) \n: (26) espresso 17 (18) 45 (37) 73 (44) 26 (25) 33 (26) 24 (21) gcc 25 (22) 72 (73) 79 (79) 33 (32) 37 \n(37) 34 (33) li 28 (28) 61 (62) 87 (90) 22 (25) 25 (2$) 26 (28) Sc 10 64 76 40 40 . 29 SPEC C Avg 11 \nT 79 35 38 19 cfodtsc 10 (8) 42 (52) 69 (92) 23 (31) 31 (33) 19 (21) fpppp 28 (34) 70 (86) 61 (82) 63 \n(40) 64 (42) 53 (41) hydro2d 3 52 88 25 31 17 mdljsp2 9 81 33 38 49 41 nasal 3 (1) 24 (10) 66 (95) 33 \n(29) 38 (32) 12 (4) m-a 3 64 57 27 spice 9 (9) 23 (21) 61 (75) ;; (33) 38 (36) i: (14) str2cor 1 44 78 \n46 47 21 swm256 1 1 65 9 13 1 ~omcatv 1 (1) 43 (38) 100 (loo) 99 (1) 99 (2) 44 (1) wave5 10 50 82 45 \n44 27 SPEC Fortran Avg 7 45 69 39 44 . 24 APS 26 52Y 62 25 33 30 Css 22 62 57 35 34 29 LWS 15 60 62 \n26 44 32 YAS 5 74 38 10 14 12 DCS 3 10 54 15 31 6 SDS 22 36 58 26 48 32 rFS 24 14 23 10 I Is 2; 40 : \n20 32 26 Wss 18 40  56 33 43  28 Perf Club Avg 16 44 56 23 34 23 Common Avg 13 (14) r\u00ad (49) 75 (75) \n$1 (29) 45 (33) 26 (22) 3verall Avg 15 50. 70 33 38 25 Table 5: Results for the Promarn-Based Heuristic \nAtsoroaches. The first column lists the miss rate for looP branches. The second column shows .. the \npercentage of non-loop branches. The. third column shows the dynamic percentage of non-loop branches \nthat can be predicted using one of the heuristics, while the fourth colnmn shows the miss rate achieved \nwhen using those berrristics. For example, 80% of the non-loop branches in compress can be predicted \nusing some heuristic, and those heuristics have a 3870 miss rate. Branches that can not be predicted \nusing the heuristics are predicted using a uniform random distribution. The fifth column shows the prediction \nmiss rate for the execution of all non-loop branches, combining the predictions from the heuristics and \nthe random predictions. Lastly, the sixth column lists the misprediction rate when both loop and non-loop \nbranches are included. 89 Bran Heuristic B&#38;L (MIPS) = LoopBranch 12% 17% 12% 15% Pointer 40% 58% \n1% 55% Call 22% 23% 44% 31% Opcode 16% 33% 29% 32% Loop Exit 20% 28% 30% 29% Return 28% 29% 30% 30% Store \n45% 52% 30% 42% Loop Header 25% 33% 48% 40% Guard 38% 34% 31% 33% Table6: Companion of Branch Miss Rates \nfor Prediction Heuristics. These averages =forall theprograms wesimulated andaprogrmis only included \ninaheuristic's average iftheheuristic applies toatlemt However, our implementation of the heuristics \ntook these factors into account, constructing an abstract syntax tree from theprogram binary andusing \nthat to determine the outcome of the conditional branch. Clearly, determining this informa\u00adtionat compile \ntime would simplify theanalysis, because we could usemore information from the program. However, both \nBall and Larus [3] and our study used binary instrumentation, so we felt that other factors must also \ncontribute to the pre\u00addiction differences. Reexamined oneprogram for which the Ball and Larus heuristics \nprovided good prediction accuracy, tomcatv in more detail, since our implementation of those heuristics \nprovided worse prediction accuracy (see Table 5). On the Alpha, tomcatv spends 99% of its execution in \none procedure. Furthermore, most of the basic block transitions in that procedure involve three basic \nblocks, shown in Figure 2. The edge from block 32 a 28 is a loop back edge, and our heuristics indicate \nthis correctly, However, the remaining two conditional branches only match the guard heuristic in the \nheuristics described by Ball and Lams. However, their study indicated that tomcatv benefited from the \nstore heuristic, which predicts that basic blocks with store instructions fol\u00adlowing a conditional branch \nare not taken. By comparison, on the Alpha, none of the successors of block 28 (blocks 29 and 30) or \nblock 30 (blocks 31 and 32) contain store instruc\u00adtions. This difference may be attributed to different \nregister scheduling or register saving conventions, requiring a store on the MIPS, but not on the Alpha, \nThe guard heuristic still applies, but predicts both branches in blocks 28 and 30 incorrectly. 5.2.2 \nThe Influence of Compilers and Optimization To further validate our belief that the choice of compilers \nin\u00adfluences the prediction accuracy of the various heuristics, we compiled one program, espresso, with \nthe following com\u00adpilers: cc on OSF/1 V1 .2, cc on OSF/I V2.0, the DEC GEM C compiler and the Gnu C compiler. \nThe results are shown in Table 7. In terms of the overall miss rate, the compilers all show different \nbehavior. Also note that the DEC GEM C com\u00adpiler produced significantly fewer loop branches, and resulted \nin a program approximately 15% faster than the other compil\u00aders. The GEM compiler unrolled one loop in \nthe main routine, l%ofthe bmrschesintheprogram. inserting more forward branches and reducing the dynamic \nfrequency of loop edges. This simple optimization changed the characteristics of the branches in the \nprogram and the efficacy of the APHC branch prediction technique. The difference caused by loop-unrolling \nis significant if we want to use branch probabilities after tradi\u00adtional optimization have been applied, \nHowever, many pro\u00adgrammers unroll loops by hand and other programmers use source-to-source restructuring \ntools, such as KAP or VAST. The differences evinced by these applications may render the fixed ordering \nof heuristics inappropriate for some programs. Our validation study confirmed an underlying assumption \nin our work: heuristic-based branch prediction rates vary with programs, program style, compiler, architecture, \nand runtime system. Rather than choosing a set of heuristics based on tbe intuition of a few people, \nwe have devised a program-based prediction mechanism that can be adapted to the techniques, style and \nmechanisms of different programmers, languages and systems. Furthermore, the corpus-based approach means \nour prediction technique can be customized to specific groups or customers. 6 Summary Branch prediction \nis very important in modem computer ar\u00adchitectures. In this paper, we investigate methods for static \nprogram-based branch prediction, Such methods are impor\u00adtant because they do not require complex hardware \nor time\u00ad consuming profiling. We propose a new, general approach to program-based behavior estimation \ncalled evidence-based static prediction (ESP), We then show how our general ap\u00adproach can be applied \nspecifically to the problem of program\u00adbased branch prediction. The main idea of ESP is that the behavior \nof a corpus of programs can be used to infer the behavior of new programs. In this paper, we use a neural \nnet\u00ad work to map static features associated with each branch to the probability that the branch will \nbe taken, ESP has the following advantages over existing program\u00adbased approaches to branch prediction. \nFirst, instead of being based on heuristics, it is based on a corpus of information about actual program \nbehavior and structure, We have observed that the effectiveness of heuristic approaches to branch prediction \n90  Program 0/s Compiler Loop Branches Non-Loop Branches Overall Perfect Miss Rate ~ ettristic Miss \nRate Miss Rate Branches Miss Rate mm~ Table7: Compdson of Accuracy ofhdiction Heuristics Using Dlffemnt \nCompilem can be architecture, compiler, and language dependent. Thus, ESP can be specialized easily to \nwork with new and different programming languages, compilers, computer architectures, or runtime systems. \nItisour hopethat itcaneven becurstomized for specific application domains, or workgroups with a moclest \namount of effort. Second, the ESP approach does not require careful consid\u00aderation when deciding what \nfeatures to include in the training data. The neural net we use is capable of ignoring information that \nis irrelevant and such information does not degrade the per\u00adformance of the predicted branch probabilities. \nOn the other hand, with heuristic methods, tial-and-error is often required to find heuristics that are \neffective. Finally, we have shown that the ESP approach results in branch prediction miss rates that \nare better than the best program-based heuristic approaches, Over a collection of 43 C and Fortran programs, \nthe overall miss rate of ESP branch prediction was 20?10,which compares against the 25% miss rate using \na fixed ordering of the Ball and Lams heuristics (the best heuristic method), and the overall 8% miss \nrate of the perfect static-profile predictor. We see many future directions to take with this work. Cur\u00adrently, \nthe neural network we use not only provides a prediction for each branch, but also provides its estimate \nof the bramcb probability. If that probability is > so~. we estimate that the branch will be taken. Our \nnext goal will be to incorporate this branch probability data to perform program-based profile estimation \nusing ESP. It is simple to add more features into our training information; for example, we plan on indicating \nbranches in library subroutines, since that those subroutines may have similar behavior across a number \nof programs. We also plan to gather large bodies of programs in other pro\u00adgramming languages, such as \nC++ and Scheme, and evaluate how ESP branch prediction works for those languages. We are also interested \nin seeing how effective other classification tech\u00adniques, such as memory-based reasoning or decision \ntrees, will be for J3SP prediction, Finally, we are interested in comparing the effectiveness of using \nESP prediction techniques against using profile-based methods across a range of optimization problems, \nWe also see other possible uses of the ESP approach that supplement profile-based prediction techniques. \nWe expect that organizations and workgroups might use their own pro\u00adgrams to train the ESP system. They \ncould then use program\u00adbased information for most compilations, and useprofile-based information for \nperformance-critical compilations. Likewise, computer vendors may provide several trained ESP predictors, \nbased on program type or language. Acknowledgements We would like to thank Alan Eustace and Amitabh Srivastava \nfor developing ATOM, and James Larus for motivating this paper. Brad Calder was supported by an ARPA \nFellowship in High Performance Computing administered by the Institute for Advanced Computer Studies, \nUniversity of Maryland. This work was funded in part by NSF grant No. ASC-9217394, NSF grant No. CCR-9404669, \nARPA contract ARMY DABT63-94\u00adC-0029 and a software grant from Digital Equipment Corp. References [1] \nR. Alverson, D. Crdlahan, D, Cummings, B. Koblenz, A. Porter\u00adfield, and B. Smith, The tera computer system. \nIn hrrernational Conference on Supercomputing, pages 1-6, June 1990. [2] Vasanth Balasundaram, Geoffrey \nFox, Ken Kennedy, and Ulrich Kremer. A static performance estimator to guide data partitioning decisions. \nIn Third ACM S[GPL4N Symposium on Principles&#38; Practice of Parallel Programming, pages 213-223, July \n1991. [3] Thomas Bsdl and James R. f.ants. !draach prediction for free. In Proceedings of the S1GPL4N \n93 Conference on Programming Language Design and Implementation, pages 300-313, June 1993. [4] M. Berry. \nThe Perfect Club Benchmarks: Effective performance evaluation of supercomputers. The international Journal \nojSu\u00adpercomputerApplications, 3(3):.5-40, Fall 1989. [5] Brad Calder and Dirk Grunwald. Fast &#38; accurate \ninstruction fetch and branch prediction. In 21s? Annual International Symposium on Computer Architecture, \npages 2-11. ACM, April 1994. [6] Brad Calder and Dirk Gmnwafd. Reducing branch costs via branch alignment. \nIn Six htrernational Conference on Archi\u00adtectural Support for Programming Languages and Operating Systems, \npages 242-251. ACM, 1994. [7] Brad Catder, Dirk Grunwald, and Benjamin Zom. Quantifying behavioral differences \nbetween C and C++ programs. Journal of Programming Languages, 2(4), 1995. Also available as Univer\u00adsity \nof Colorado Technical Report CU-CS-698-94. [8] P. P. Chang and W, W. Hwu. profile-guided automatic inline \nexpansion for C programs. Software Practice and Experience, 22(5):349-376, 1992, [9] P. P. Chang, S. \nA. Mahlke, and W. W. Hwu. Using profile infor\u00admation to assist classic compiIer code optimization. Softiare \nPractice and Experience, 21 (12):1301 1 321,1991. [10] A. P. Dempster. A generalization of bayesian inference. \nJournal of the Royal Statistical Society, 30:205 247, 1968. [it] J. A. Fisher and S. M. Frerrdenberger. \nPredicting conditional branch dkections from previous runs of a program. In Pro\u00ad ceedings of the F@h \nInternational Conference on Architectural 91 Support for Programming Languages and Operating Systems \n(ASPLOS-V), pages 85-95, Boston, Mass., October 1992. ACM. [12] Joseph A. Fisher. Tmce scheduling: A \ntechnique for global microcode compaction. IEEE Transactions on Computers, C\u00ad30(7):478-490, JUiy 1981. \n[13] Wen-mei W. Hwu and Pohua P. Chang. Achieving high instruc\u00adtion cache performance with an optimizing \ncompiler. In 16th An\u00adnual International Symposium on Computer Architecture, pages 242-251. ACM, 1989. \n[14] Scott McFarling and John Hennessy. Reducing the cost of branches. In 13th Annual International Sytnposiunr \nof Crmt\u00adpurer Architecture, pages 396-403. Association for Computing Machinery, 1986. [15] Judea Pearl. \nProbabilistic Reasoning in Intelligent Syslerns: Net\u00adworks of Plausible Inference. Morgan Kmrfmaan, San \nMateo, CA, 1988. [16] D. E. Rumelhart, G. E. Hinton, and R J. Williams. Parallel dis\u00adtributedprocessing: \n.Erplorations in the microstructure of cogni\u00adtion. Volume I: Foundations, chapter f-earning internal \nrepresen\u00adtations by error propagation, pages 318-362. MIT Press/Bradford Books, Cambridge, MA, 1986. \nD. E. Rumelhart and J. L. Mc-Clelland, editors. [17] G. Shafer. A Mathematical Theory of Evidence, Prhrceton \nUni\u00adversity Press, Princeton, NJ, 1976. [18] J. E. Smith. A study of branch prediction strategies. In \n8th Annual International Symposium of Computer Architecture, pages 135\u00ad 148. ACM, 1981. [19] P. Smolensky, \nM. C. Mozer, and D. E. Rumelhast, editors. Math\u00ademaacal perspectives on neural networks. ErIbaum, t 994. \nIn press. [20] Amitabh Srivastava and Alan Eustace. ATOM: A system for building customized program analysis \ntools. In Proceedings of the SIGPLAN 94 Conference on Programming Language Design and lmplementatlon, \npages 196-205, ACM, 1994, [21] Tlm A. Wagner, Vance Maverick, Susan Graham, and Michael Harrison. Accurate \nstatic estimators for program optimization, In Proceedings of the SIGPL4N 94 Conference on Programming \nLunguage Design and Implementation, pages 85-96, Orlando, Florida, June 1994. ACM. [22] Youfeng Wu and \nJames R, Lattrs. Static branch frequency and program profile analysis, In 27th International Symposium \non Microarchitecture, San Jose, Ca, November 1994, IEEE. [23] Tse-Yu Yeh and Yale N. Patt. A comparison \nof dynamic branch predictors that use two levels of branch history. In 20th Annual JnternationalSymposium \non Computer Architecture, pages 257\u00ad266, San Diego, CA, May 1993. ACM.  \n\t\t\t", "proc_id": "207110", "abstract": "<p>Correctly predicting the direction that branches will take is increasingly important in today's wide-issue computer architectures. The name <italic>program-based</italic> branch prediction is given to static branch prediction techniques that base their prediction on a program's structure. In this paper, we investigate a new approach to program-based branch prediction that uses a body of existing programs to predict the branch behavior in a new program. We call this approach to program-based branch prediction, <italic>evidence-based static prediction</italic>, or ESP. The main idea of ESP is that the behavior of a <italic>corpus</italic> of programs can be used to infer the behavior of new programs. In this paper, we use a neural network to map static features associated with each  branch to the probability that the branch will be taken. ESP shows significant advantages over other prediction mechanisms. Specifically, it is a program-based technique, it is effective across a range of programming languages and programming styles, and it does not rely on the use of expert-defined heuristics. </p><p>In this paper, we describe the application of ESP to the problem of branch prediction and compare our results to existing program-based branch predictors. We also investigate the applicability of ESP across computer architectures, programming languages, compilers, and run-time systems. Averaging over a body of 43 C and Fortran programs, ESP branch prediction results in a miss rate of 20%, as compared with the 25% miss rate obtained using the best existing  program-based   heuristics.</p>", "authors": [{"name": "Brad Calder", "author_profile_id": "81100088945", "affiliation": "Department of Computer Science, Campus Box 430, University of Colorado, Boulder, CO", "person_id": "PP17000250", "email_address": "", "orcid_id": ""}, {"name": "Dirk Grunwald", "author_profile_id": "81100218381", "affiliation": "Department of Computer Science, Campus Box 430, University of Colorado, Boulder, CO", "person_id": "PP15026204", "email_address": "", "orcid_id": ""}, {"name": "Donald Lindsay", "author_profile_id": "81100191527", "affiliation": "Department of Computer Science, Campus Box 430, University of Colorado, Boulder, CO", "person_id": "P68563", "email_address": "", "orcid_id": ""}, {"name": "James Martin", "author_profile_id": "81406597036", "affiliation": "Department of Computer Science, Campus Box 430, University of Colorado, Boulder, CO", "person_id": "PP14151811", "email_address": "", "orcid_id": ""}, {"name": "Michael Mozer", "author_profile_id": "81339518819", "affiliation": "Department of Computer Science, Campus Box 430, University of Colorado, Boulder, CO", "person_id": "PP39065299", "email_address": "", "orcid_id": ""}, {"name": "Benjamin Zorn", "author_profile_id": "81100190820", "affiliation": "Department of Computer Science, Campus Box 430, University of Colorado, Boulder, CO", "person_id": "PP40024661", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/207110.207118", "year": "1995", "article_id": "207118", "conference": "PLDI", "title": "Corpus-based static branch prediction", "url": "http://dl.acm.org/citation.cfm?id=207118"}