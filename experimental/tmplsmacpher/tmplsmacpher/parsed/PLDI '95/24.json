{"article_publication_date": "06-01-1995", "fulltext": "\n Tile Size Selection Using Cache Organization and Data Layout Stephanie Coleman Kathryn S. McKinley scoleman \n@d,?d.cumb, remet. corn mckinley@cs. umass. edu Intermetncs, Inc., 733 Concord Ave. Computer Science, \nLGRC, University of Massachusetts Cambridge, MA 02138 Amherst, MA 01003 Abstract When dense matrix computations \nare too large to fit in cache, previous research proposes tiling to reduce or eliminate capac\u00adity misses. \nThis paper presents a new algorithm for choosing problem-size dependent tile sizes based on the cache \nsize and cache line size for a direct-mapped cache. The algorithm elim\u00adinates both capacity and self-interference \nmisses and reduces cross-interference misses. We measured simulated miss rates and execution times for \nour algorithm and two others on a. va\u00adriety of problem sizes and cache orgamzations. At higher set associativity, \nour algorithm does not always achieve the best performance. However on direct-mapped caches, our algorithm \nimproves simulated miss rates and measured execution times when compared with prewous work. 1 Introduction \nDue to the wide gap between processor and memory speed in current architectures, achieving good performance \nrequires high cache efficiency. Compiler optimizations to improve data lc~cal\u00adity for uniprocessors is \nincreasingly becoming a critical part of achieving good performance [CMT94]. One of the most well\u00adknown \ncompiler optimizations is tiling (also known as blocking). It combines strip-mining, loop permutation, \nand skewing to en\u00adable reused data to stay in the cache for each of its uses, i.e., accesses to reused \ndata are moved closer together in the iteration space to eliminate capacity misses. Much previous work \nfocuses on how to do the loop nesi re\u00adstructuring step in tiling [CK92, CL95, IT88, GJG88, WL91, W0189]. \nThis work however ignores the effects of real caches such as low associativity and cache line size on \nthe cache per\u00adformance of tiled nests. Because of these factors, performance for a given problem size \ncan vary wildly with tile size [LRW91]. In addition, performance can vary wildly when the same tile sizes \nare used on very similar problem sizes [LRW91, NJL94]. These results occur because low associativity \ncauses interference misses in addition to capacity misses. Permission to copy without fee all or part \nof this material i:s granted provided that the copies are not made or distributed for direct commercial \nadvantage, the ACM copyright notice and i!he title of the publication and its date appear, and notice \nis given that copying is by permission of the Association of Computing Machinery.To copy otherwise, or \nto republish, requires a fee andlor specific permission. SIGPLAN 95La Jolla, CA USA @ 1995 ACM 0-89791 \n-697-2/95/0006... $3.50 In this paper, we focus on how to choose the tile sizes given a tiled nest. As \nin previous research, our algorithm targets loop nests in which the reuse of a single array dominates. \nGiven a problem size, the Tile Size Selection (TSS) algorithm selects a tile size that eliminates self-interference \nand capacity misses for the tiled array in a direct-mapped cache. It uses the data layout for a problem \nsize, cache size, and cache line size to generate potential tile sizes. If the nest accesses other arrays \nor other parts of the same array, TSS selects a tile size that minimizes expected cross interferences \nbetween these accesses and for which the working set of the tile and other accesses fits in a fully associative \nLRU cache. We present simulated miss rates and execution times for a variety of tiled nests that illustrate \nthe effectiveness of the TSS algorithm. We compare these results to previous algorithms by Lam et al. \n[LRW91] and Esseghir [Ess93]. On average, TSS achieves better miss rates and performance on direct-mapped \ncaches than previous algorithms because it selects rectangular tile sizes that use the majority of the \ncache. In some cases, it achieves significantly better performance, If the problem size is unknown at \ncompile time, the additional overhead of computing problem-dependent tile sizes at runtime is negligible. \nWe show that because TSS effectively uses the majority of the cache and its runtime overhead is negligible, \ncopying is unnecessary and significantly degrades performance. Section 2 compares our strategy to previous \nresearch. In Sec\u00adtion 3, we briefly review the relevant terminology and features of caches, reuse, and \ntiling, Section 4 describes the tile size selection algorithm, TS S. It generates a selection of tile \nsizes without self-interference misses in a direct-mapped cache us\u00ading the array size, the cache size, \nand the cache line size. It selects among these tile sizes to generate the largest tile size that fits \nin cache and that minimizes expected cross-interference misses from other accesses. Section 5 presents \nsimulation and execution time results that demonstrate the efficacy of our ap\u00adproach and compares it \nto the work of Esseghir and Lam et al. [Ess93, LRW91 ]. 2 Related Work Several researchers describe methods \nfor how to tile nests [BJWE92, CK92, CL95, IT88, GJG88, W0189]. None of this work however addresses interference, \ncache replacement poli\u00adcies, cache line size, or spatial locality which are important factors that determine \nperformance for current machines. More recent work has addressed some of these factors for selecting \ntile sizes [Ess93, LRW91]. Esseghm selects tile sizes for a variety of tiled nests [Ess93]. HIS algorithm \nchooses the maximum number of complete columns that fit in the cache. This algorithm leaves one large \ngap of unused cache. All of his experiments were performed on the RS6000 (64K, 128 byte line, 4-way set \nassociative). For this cache orgamzation, Esseghlr s strategy shghtl y out performs the TSS algorithm \nby a factor of 1.03. However, when compared to TSS or Lam etal. [LRW91] on an 8K cache with 1, 2, or \n4-way set associative caches using matrices that are relatively large with respect to cache size (e.g., \n300 x 300), Esseghir s algorithm results in significantly higher miss rates. For example, TSS out performs \nit on the DEC Alpha (8K, 32 byte hne, dmect mapped) for matrix multiply by an average factor of 2 (Section \n5). Lam et al. present cache performance data for tiled matrix multiply and describe a model for evaluating \ncache interference [LRW91]. The model evaluates reuse for one variable, and quan\u00adtifies self-interference \nmisses for matrix multiply as a function of tile size. They show choosing a tde size that uses a fixed \nfrac\u00adtion of the cache performs poorly compared to tde sizes that are tadored for a problem and cache \nsize. They present an algorlthm which chooses the largest size for a square tile that avoids self in\u00adterference \nbased on array size. Square tiles use a smaller portion of the cache and result in higher miss rates \nand execution times when compared to the data-dependent rectangular tiles chosen by our algorithm (Section \n5). TSS consistently improves execu\u00adtion times over Lam etal, by an average factor of 1,12 on the DEC \nAlpha and a smaller factor of 1.02 on the RS6000, Esseghir, Lam et al,, and Temam et al [TGJ93] all recom\u00admend \ncopying as a method to avoid self-interference and cross\u00adinterference misses. Copying also requires knowledge \nof array sizes which may not be available until runtime. It makes perfor\u00admance much more predictable \nfor varying tile sizes. However, computing the tile sizes at runtime with any of TSS, Esseghir, or Lam \netal. has no noticeable impact on performance. TSS achieves slgmficantly better performance than copying \nbecause it uses the majority of the cache, eliminates self interference, and minimizes cross interference \n(see Section 5). 3 IBackground 3.1 Cache Memory Tiling can be applied to registers, the TLB, or any \nother level of the memory hierarchy. In this paper, reconcentrate on tiling forthefirst level ofcache \nmemory. Thecache isdescribed by its size, line size, and set associatlvity [Smi82]. Unless otherwise \nindicated, we assume adirect-mapped cache. Wedlwdec ache misses into three categories, Compulsory misses \noccur when acachehn eisreferencedfor the first time. Without prefetching, these misses areun\u00adavoidable. \nCapacity misses occur when aprogram s working set size is larger than the cache size and are defined \nwith respect to a fully associative LRUcache, Ifacache linecontaining data that wdl be reused is replaced \nbefore lt is reused, a capacity miss occurs when the displaced data is next referenced, The miss is classified \nas a capacity miss only if It would occur inafully LRU cache. Otherwise, itisclassified as an interference \nmiss. Interference misses occur when acacheline thatcontains data that will be reused is replaced by \nanother cache line. An interference miss is distmgtri shed from a capacity miss be\u00adcause not all the \ndata in the cache at the point of the miss on thedisplaced data will be reused. Intuitively, interference \nmisses occur when there is enough room for all the data that will be reused, but because of the cache \nreplacement policy data maps tothe same location, Interference misses onarrays can redivided into two \ncategories. o Self-interference misses result when an element of the same array causes the interference \nmiss. Cross-interference misses result when an element of a different array causes the interference miss. \n 3.2 Reuse The two sources of data reuse are temporalreuse, multiple ac\u00adcessesto thesame memory location, \nand spatial reuse, accesses tonearby memory locations that share a cache line. Without loss of generality, \nwe will assume Fortran s column-major storage. Tiling only benefits loopnests withtemporal reuse. Wewill \nalso take advantage of spatial locality in tiled nests. 3.3 Tiling Tding reduces the volume of data \naccessed between reuses of an element, allowing a reusable element to remain in the cache until thenext \ntimeitis accessed. Consider thecode formatrlxmultiply in Figure l(a) and its corresponding reuse patterns \ndlustratedm Figure 2(a). Thereference Y(J,K)ls loop-invarlant withrespect tothe I loop. Each iteration \nof the Iloopalso accesses one row each of X and Z. Therefore, 2*N + N2 elements are accessed periteration \nof the I loop. Between each reuse of an element of Y there are N distinct elements of Z accessed on the \nJ loop, Nelements of the X array on the Kloop, and N2 -1 elements (a) Matrix Multiply DO 1=1, N DO K=l, \nN R = X(K,I) DO J=I, N Z(J,I) = Z(J,I) + R * Y(J,K) vuJ (b) Tiled Matrix Multiply DO KK=l, N,TK DO JJ=l, \nN,TJ DO 1=1, N DO K = KK, MIN(KK+TK-l,N) R = X(K.1) DO J = JJ, MIN(JJ+TJ-l,N) Z(J,I) = Z(J,I) + R * \nY(J,K) Figure 1: (b)x Yz Figure 2: Iteration space traversal in (a) untiled and (b) tiled matrix multiply \nof the Y array. If the cache is not large enough to hold this Reuse Factor Footprint many elements, then \nthe reusable Y data will be knocked out of the cache, and the program will have to repeat a costly memory \naccess. Previous research has focused on how to transform a nest into ~ a tiled version to eliminate \nthese capacity misses [CK92, CIL95, Z(J,I) OITK IO/1 TJ /TJll IT88, GJG88, W0189]. We assume as input \na tiled nest produced Table 1: Reuse Factor and Footprint for Tded Matrix Multiply by one of these methods \nand turn our attention the selection of tile sizes for the nest. For example, tiled matrix multiply appears \nin Figure l(b) and 4 Tile Size Selection its corresponding reuse pattern m Figure 2(h). In the tiled \nnest, one iteration of the I loop now accesses only TK + TJ + TK*TJ Given a target array reference, we \nnow show how to select a tile elements. Between reuse of an element of Y, the J and K loops size for \nthe reference. access TK distinct elements of X, TJ elements of Z, and TK * TJ 4.1 Detecting and Eliminating \nSelf Interference elements of Y, We call the portion of an array which is referenced In this section, \nwe describe how to detect and ehminate self\u00ad by a loop the~ootpr-int of that array reference [W0189]. \nWe call interference misses when choosing a tile size. We compute a the number of times the same element \nis referenced by a loop selection of tile sizes that exhibit no self interference and no the reuse jactor. \nWe call the innermost loop that has not been capacity misses. Factors such as cross interference and \nworking strip mined and interchanged the target loop, the 1 loop in matrix set size determine which size \nwe select. We use the cache size, multiply; the target nest accesses a tile of data. Inspection of the \nline size, and the array column dimension. We only select the array accesses, loop nesting, and loop \nbounds of the tiled tile sizes in which the column dimension is a multiple of the nest determines the \nfootprint and reuse factor [W0189]. Table 1 cache line size. illustrates these quantities for the version \nof tiled matrix multiply Consider the layout of a 200x 200 array Y m a direct-mapped in Figure l(b). \ncache that can hold 1024 elements of Y as illustrated in Figure 3. The largest tile with the most reuse \non the I loop is the access Without loss of generality, we assume the first element of array Y to Y. \nWe therefore target this reference to fit and stay in ca(:he. falls in the first position of the cache. \nSets are defined as groups We want to choose TK and TJ such that the TK x TJ submatrix of consecutive \nrows whose staring positions differ by N. The of Y will still be in the cache after each iteration of \nthe I loop first set therefore consists of rows 1 through 6. Let CS be the and there is enough room in \nthe cache for the working set size cache size in elements, CLS the cache line size in elements, and \noftheIloop, TKx TJ+TK +TJ. N the column dimension (the consecutively stored dimension). The number of \ncomplete rows that fit in the cache is simply RowsPerSet = [CS/Nj. (1) 1 I 1 200 400 60Q , 80Q 10QO \n, I 1024 I 1st set I I I 152 I I I I I 3rd set I Is H I 208 I I I I 9th set Figure 3: Column Layout for \na 200 x200 Array in a 1024 Element Cache For Figure 3, RowsPerSet = 5 for a tile of 200x 5 (Esseghir \nselects this tile size [Ess93]). A 200x 5 tile uses 97~0 percent of a 1024 element cache, but leaves \na single contiguous gap. If N evenly divides CS, we also select this tde size. Otherwise, we look for \na smaller column dimension with a larger row size that does not incur interference which combine to use \na higher percentage of the cache. We use the Euclidean algorithm [Kob87] to generate poten\u00adtial column \ndimensions. The Euclidean algorithm finds the g.c.d. (a, b) a > b, in 0(log3 (a)) time. It computes . \n a glb+rl b= q2?J + 7-2 . ?-1 q s?-z + 7-3 #-1 ,.._ k+lhk+l q r +7\u00ad until a remainder divides a previous \nremainder. Remainders are always decreasing. For our purposes, a = C S the cache size, and b = N the \ncolumn dimension. Each remainder is a potential column size. Given our example with a = 1024 and b = \n200, Euclid generates the following. 1024 = 5* 200+24 200 = 8*24+8 Since 8 divides 24, 8 = g.c, d. (1024, \n200) and it terminates. We begin with an initial column size of b = N. We must reduce the column size \nto at least r 1 before additional rows will not incur interference. Look at the 6*h row s starting position \nin Figure 3. Even if we reduce the column size from 200 to 25, no additional rows will fit because when \nthe 6th row is of width 25 or greater it interferes with the first row. When the column size equals r-l, \n24 in this example, it becomes possible to fit more rows. (When rl > N rl, only one more row fits with \na column size of rl as opposed to N. Otherwise, the number of rows increases by at least lCS/Nj.) The \nstarting positions of the first and second set will differ by SetDiff = N r 1. The difference between \nsubsequent sets will eventually become Gap = N mod SetDiff. The number of rows is determined by the point \nat which the difference changes from r 1 to Gap. The algorithm for computing the number of rows for a \ncolumn size which is a Euclidean remamder appears in Figure 4. The algorithm diwdes the cache into two \nsections: (1) the rl gap at the end of the first set and (2) the rest of the cache. It divides naturally \nbecause of the pattern in section 2. If an additional row starts a new set in section 2 and does not \ninterfere with previous sets then at least an additional RowsPerSel will not interfere. A space of size \nGap occurs at the end of the cache (in section 1 between the last row and the end of the cache) and eventually \noccurs between starting positions in section 2. For each of section 1 and 2, we thus compute (a) the \nnumber of rows of colSize that fit between the starting addresses differing by SefDiff, and (b) the number \nof rows that fit in the spaces of size Gap. Since we only use Euclidean column sizes, RowsPerGap = [ \nGap/ colSize j. For section 2 of the cache, RowsPerN is the number of rows that fit between two complete \ncolumns of size N, dictated by SetD&#38; RowsPerSetDtjF is the number of rows of colSize that fit between \nrows that have starting positions with differences of SetDtjj( Since columns are Euclidean numbers, it \nis the mini\u00admum distance allowed between starting positions of columns m different sets. RowsPerGap is \nthe number of rows that fit in the Gap. This pattern repeats RowsPerSet times. The number of rows that \nfit in section 2 of the cache is thus (RowsPerSetDiff + RowsPerN + RowsPerGap) * RowsPerSet, When SetDiff \n< rl patterns of rows fit m section 1, the pattern rows total RowsPer-SetDi&#38; * 1 r 1 / SetDiff ]. \nThe total is thus any pattern rows plus RowsPerGap. Returning to the example m Figure 3, RowsPerSet = \n5, rl == 24, SetDzjf= 176, and RowsPerN = 1. Given a column size of 24, RowsPerSetD#f = 7 and RowsPerGap \n= 1. Thus rowSize = 7*1*5+ 1*5+0+1 =41, foratile size of24x41, 4.2 Cache Line Size To take advantage \nspatial locality, we choose column sizes that are multiples of the cache line size m terms of elements, \nCLS. We assume the start of an array is aligned on a cache line boundary. After we find the row size, \nwe simply adjust colSize as follows. colSize t~ colSize mod CLS = O, or col.$ize = l~colSize = column \nlength { ~ CL$ colSize ~ CLS otherwise  Input: C.Y Cache Size, M Column Dimension, colSize = r k: Euchdean \nremamder output: rowSiz,e: max rows without interference Invariants: Row.~PerSet = ql = [CS/Nj T.1 .CS \nmod N SetD(ff= N rl RowsPerN = [N/SetDif] Gap = Nmod SetDiff procedure ComputeRows (coLSu,e) if (cob \nize = N) return Row.$Pe?Set else if (coMize == r 1 &#38; coL 7ize > SetD~fl return RowsPerSet + 1 else \nRow.vPerSetD(fl= ~ SetD@ / colSiz,e j Row.~PerGup = [ Gap f colSize j rowsize = Row.vPerSetDif * Ro w.rPerN \n* Row.rPerSet + Row.vPerGap * RowsPerSet + RowsPerSetD(ft * 1 rl / SetDZff j + RowsPerGup  return rowSize \nendif Figure 4: Row Sizes for Euclidean Column Sizes If colSize is equal to the length of the column, \nwe do not adjust it to a multiple of the line size.  4.3 Minimizing Cross Interference In this section, \nwe compute worst case cross-interference misses for tiled nests. We use footprints to determine the amount \nof data accessed in the tile and choose a tile size that has a working set size that fits in the cache \nand minimizes the number of expected cross-interference misses. Consider again matrix multiply from Figure \n1. On an iteration of the I loop (from Table 1), we access TK x TJ elements of Y, TJ elements of Z, and \nTK elements of X. If we completely fill up the cache with array Y, then every reference to Z causes a \ncross interference miss on the next reference to the victimized element of Y. Another cross interference \nmiss will occur on the next reference to the same element of Z. There are potenti~dly 2 * TJ cross interferences \nbetween Z and Y. Despite the Fact that the element of X that is reused in the J loop is register\u00adallocated, \nit still uses a cache line on its first access. This access causes a cross-interference miss on the next \nreference to any victimized element of Y. We ignore the cross interference misses of Z caused by X because \nthey happen very infrequently and are hard to predict. In the worst case, the number of cross\u00adinterference \nmisses, CM, that will occur as a result of a TK x TJ tile size in matrix multlple is CIM=2*TJ+TK. (2) \nThe cross-interference rate, CIR, is thus CIR = CIiW/(TJ * Th ), (3) the number of cross interference \nmisses per element of Y in a single iteration of the I loop. To compute this rate in general, we use \nthe footprints for the target nest for array accesses other than procedure TSS(CS, CLS, N, M) Input: \nC S: cache size, CLS: cache line size, N: column length, M: row length Output: tile size = bestColx be.vtRow \n bestCol = oldCol = N bestRow = rowSize = CS/N colSize = bestCol mod N while (colsSize > CLS &#38; oldCol \nmod colSize # 0 &#38; rowSize < M) rowSize = computeRows (colSize) tmp = colSize adjusted to a multiple \nof C LS if ( WSet (tmp, rowSize) > WSet (be.wCol, bestRow) &#38; WSet (trrzp, rowSize) < CS &#38; CIR \n(trrzp, rowSize) < CIR (bestCol, bestRow) be.uCol = tmp bestRow = rowSize endif tmp = colSize colSize \n= oldCol mod colSize oldCol = tmp endwhile if necessary, adjust besfCol to meet the working set size \nconstraint end TSS Figure 5: Tile Size Selection Algorithm the target. Each footprint is then multiplied \nby 2 if they do not reduce to one on the next inner loop in the nest. We also minimize expected cross \ninterference by selecting tile sizes such that the working set will fit in the cache, e.g., for matrix \nmultiply the working set size constraint is TJ*TK+TJ+l*CLS<CS. (4) (Since X is register allocated, we \nreduce its footprint to CLS.) The left-hand side of equation 4 is exactly the amount of cache need for \nan fully-associative LRU cache. In general, the working set is simply the sum of the footprints for the \ntarget loop which accesses the tile. We use the cross-interference rate and working set size con\u00adstraint \nto differentiate between tile sizes generated by the algo\u00adrithm described in Section 4.1. As the algorithm \niterates, we select a new tile size without self interference if its working set size is larger than \na previous tile size and still fits in cache and the new size has a lower CIR than the previous size. \n(Section 5 demonstrates that these tile sizes result in lower miss rates for direct-mapped caches.) If \nthe tile we select does not meet these constraints, we de\u00adcrease colSize by CL-$ until it does. Both \nthis phase and the self interference phase result in numerous gaps through out the cache, rather than \none large gap. Because cross-interfering ar\u00adrays typically map all over the cache, multiple gaps minimize \nthe expected interference, e.g., the columns of Z are more likely to map to one of many gaps rather than \none larger gap. 4,4 Tile Size Selection Algorithm The pseudo code for the TSS algorithm which completely \navoids self interference, capacity misses, and minimizes ex\u00adpetted cross interference appears in Figure \n5, We begin by selecting the column dlmenslon which is the maximum colSize and determine the maximum \nrowSize without self interference. In the pathological case, the column length evenly divides the cache \nand the algorithm terminates (this case often occurs with power of 2 array sizes), If the tile sizes \nare larger than the array or the bounds of the Iteration space, there is no need to tile. In addition, \nif any of the dimensions of the tiles are larger than the bounds of the iteration space, the tales are \nadjusted accordingly. For simplicity, these checks are omitted from Figure 5. The whiIe loop iterates \nfinding potential tile sizes without self interference using Euclidean numbers as candidates for column \ndimensions. After determining the number of rows that wdl not interfere for the given column size, the \ncolumn size is adjusted to a multiple of the CLS. If any newly compute tile size has a larger working \nset size that is also less than CS and for which the (7IR is less than the previous best tile size, we \nset this tile to be the best. If the initial tile size does not meet the working set size constraint \nand no subsequent tile size does either, we use the initial tile size, but reduce lts column size by \nCLS until it meets the constraint. 4.5 Set Associativity Set associativity does not affect the tile size \npicked by the al\u00adgorithm for a particular cache size. As expected, increasing the set associatiwty usually \ndecreases the miss rate on a particular tile size because more cross interferences, if they exist, are \nelim\u00adinated. Our results in Section 5 confirm this expected benefit from set associativity and illustrate \nthat increasing set associa\u00adtiwty causes the differences in miss rates between distinct tde sizes to \nbecome less extreme,  4.6 Translation Lookaside Buffer A translation lookaside buffer (TLB) IS a fast \nmemory used for storing virtual to physical address mappings for the most recently reference page entries. \nAll addresses referenced in the CPU must be translated from virtual to physical before the search for \nan element is performed, If the mappmg for the element is not In the TLB, a TLB miss occurs, causing \nthe rest of the system to stall until the mapping completes. A TLB miss can take anywhere from 30 to \nmore than 100 cycles, depending on the machine and the type of TLB miss. To avoid a TLB misses, tile \nsizes should enable the TLB to hold all the entries required for the tile. In general, the height (column \nin Fortran) of the tile should be much larger than the width of the tile, Since a TLB miss can cause \na cache stall, ensuring that no TLB misses occur is more important than the cross and self interference \nconstraints. The tile size therefore needs to be constrained such that the number of non-consecutive \nelements accesses (i.e. rows) is smaller than the number of page table entries in the TLB. 5 IResults \nFor our experiments, we used the following tiled ker\u00adnels on double precision (16 byte) two dimensional \narrays. mm Matrix Mrrltlply, tiled in 2 dimensions sor Successive Over Relaxation, tiled in 1 &#38; 2 \ndimensions lud LU Decomposition, tiled in 1 &#38; 2 dimensions hv23 Lwermore loou 23, bled in 1 dimension \n The point and tiled versions of SOR, LUD, and Llvermore loop 23 appear in Appendix A. We used square \narrays except for Livermore loop 23. The original Llvermore loop 23 is 101x 7. We increase both its bounds \nby a factor of 3 to preserve its shape and make it worthwhile to tile, Matrix multiply and Llvermore \nloop 23 incur self and cross interference. SOR and LUD op\u00aderate on a single array and thus by definition \nincur only self interference. Some of self interference for LUD results from non-contiguous accesses \nand we minimize it the same way we do cross interference for matrix multiply. We selected problem sizes \nof 256 x 256 to illustrate the patho\u00adlogical case, and 300x 300 and 301 x 301 to illustrate the effect \na small change in the problem size has on selected tde sizes and performance. We used these relatively \nsmall sizes in order to obtain timely simulation results. For execution results, we added a problem size \nof 550x550. We expect, and others have demon\u00adstrated, more dramatic improvements for larger array sizes. \n 5.1 Simulation Results for Tiled Kernel We ran simulations on these programs using the Shade cache simulator. \nWe used a variety of cache parameters: a cache size of 8K and 64K; set-associatiwty of 1, 2, and 4; and \na cache line size of 32, 64, and 128 [C0194]. Of these, we present cache pa\u00adrameters corresponding to \nthe DEC Alpha Model 3000/400 (8K, 32 byte line, direct-mapped) and the RS/6000 Model 540 (64K, 128 byte \nline, 4-way) with variation m lme size and associatiwty, We also executed the kernels on these machines. \nIn Table 2, we show simulated miss rates in an 8K cache for double precision arrays (16 byte) for the \nuntiled algorithm and the version tiled with TSS. We present results for set assoclativities of 1, 2, \nand 4 and cache line sizes of 32 bytes and 128 bytes. TSS achieves significant improvement in the miss \nrates for most of these kernels. On average, it improves miss rates by a factor of 8.6. It improves SOR2D \nby a factor of 66.21 on a 8K, 4-way, 32 byte hne cache. The improvement for 32 byte lines is higher, \na factor of 9.5, than that for 64 byte lines, 7,62, because the longer line sizes benefit these kernels \nall of which have good spatial locality. If the dramatic improvements of SOR2D are ignored, our algorlthm \nimproves miss rates by an average of 2.3 (2.8 on 32 byte hnes and 1.8 on 128 byte hnes). Two more trends \nfor tded kernels are ewdent m this table. The first trend is to be expected: even though TSS selects \ntile sizes for direct-mapped caches, these tile sizes always improve their performance when assoclativity \nis higher. The second trend is that for the untlled kernels and direct-mapped cache, the 128 byte lines \nhave higher miss rates than 32 byte lines for all but MM and LIV23. The interference from the larger \nhne size plays an important role. For 4-way caches, all of the untiled kernels have lower miss rates \nfor 128 byte lines because the increased set associativity has overcome the interference. Miss Rate \nTile Size Miss Rate  m ~ ~~~~~~ ss mm 1 32 300X300 2.325 16x29 0.613 3.79 mm 2 32 300X300 1,699 16x29 \n0.257 6.61 mm 4 32 300X300 1.679 16x29 0.197 8.52 mm 1 128 300X300 1.176 16x29 1.139 1.03 mm 2 128 300X300 \n0.458 16x29 0.274 1.67 mm 4 128 300X300 0.426 16x29 0.322 1.32 sorl D 1 32 300X300 1.214 300x86 1.204 \n1.01 sorl D 2 32 300X300 0.901 300x86 0.882 1.02 sorlD 4 32 300X300 0.927 300x86 0.880 1.05 sor 1D 1 \n128 300X300 1.980 300x86 1.506 1.32 sorlD 2 128 300X300 0.992 300x86 0.262 3.79 sorlD 4 128 300X300 0.255 \n300x86 0.255 1.00 sor2D 1 32 300X300 1.214 88x3 0.370 3.28 sor2D 2 32 300X300 0.901 88x3 0.015 60.07 \nsor2D 4 32 300X300 0.927 88x3 0.014 66.21 sor2D 1 128 300X300 ] 1.980 88x3 1.267 1.56 sor2D 2 128 300X300 \n0.992 88x3 0.017 58.35 sor2D 4 128 300X300 0.248 88x3 0.005 49,60 ludlD 1 32 300X300 ] 1.482 300x2 0.746 \n1.99 hrdlD 2 32 300X300 1,002 300x2 0.439 2.28 ludlD 4 32 300X300 0.990 300x2 0.398 2.49 ludl D 1 128 \n300X300 1.947 300x2 1.010 1.93 ludlD 2 128 300X300 0.366 300x2 0.166 2.21 ludl D 4 128 300X300 0.302 \n300x2 0.117 2.58 lud2D 1 32 300X300 1.482 16x29 0.471 3.15 lud2D 2 32 300X300 1,002 16x29 0.302 3.32 \nlud2D 4 32 300X300 0.990 16x29 0.271 3.65 lud2D 1 128 300X300 1.947 16x29 0.463 4.21 1ud2D 2 128 300X300 \n0.366 16x29 0.203 1.80 lud2D 4 128 300X300 0.302 16x29 0.183 1.65 liv23 1 32 303x21 6.061 64x21 5,850 \n1.04 liv23 2 32 303x21 5.713 64x21 5.258 1.09 liv23 4 4.761 64x21 liv23 1 128 4.161 64x21 liv23 2 liv23 \n4 Average Improvement for 32 byte lines &#38; Average Improvement for i128 byte ~ lines : : : Table \n2: Miss rates in 8K cache for double precision element ( 16 byte) arrays 5.2 Comparing Algorithms In \nboth simulations and execution results, we compare our tile sizes to those chosen by Lam et cd. [LRW91] \nand Esseghir s algorithms [Ess93 ]. We use the algorithms presented in their pa\u00adpers to compute the tile \nsizes for the different cache and data set. LRW generates the largest square tiles without self interference. \nEsseghir chooses the column length, N for the column tde size and [CS/N=] for the row size. For 1 dimensional \ntiling, simply choosing the correct number of complete columns of size N suf\u00adfices and as a result comparisons \nare uninteresting. We therefore consider matrix multiply, SOR2D, and LUD2D since they are tiled in 2 \ndimensions and the algorithms usually produce differ\u00adent tile sizes. We compare the results for 8K and \n64K caches separately since they have slightly different behaviors. 5.2.1 SK Caches Table 3 presents \nsimulated miss rates for an 8K, 32 byte line cache with associatlvities of 1, 2, and 4 for TSS, LRW, \nEsseghir, and the untiled kernels. For each kernel, we present the selected tde size (the actual parameters \nto the tiled algorithm), the work\u00ading set size (lV.Yet), and the simulated miss rates. The working set \nsize is presented to demonstrate the cache efftclency of the selected tile sizes. Notice that for SOR2D, \nLRW S tile sizes are not square as expected. We reduce the number of rows to account for the working \nset size of SOR2D. LRW has lower miss rates than TSS on LUD2D and is dra\u00admatically lower on SOR2D, TSS \nhas lower miss rates on MM. On average, TSS slightly Improves miss rates by a factor of 1.03 over LRW, \nexcluding arrays of size 256x256, (We ex\u00adclude this case since paddmg is probably a better solutlon to \npathological interference). TSS has consistently lower simulated miss rates than Esseghir, on average \na factor of 6.66 (excluding arrays of size 256x256, including them 5.34). For example, SOR2D 301x301, \n4-way, TSS improves miss rates by a factor of 16.6 over Esseghir. These results hold for larger line \nsizes as well [C0194]. TSS s lower simulated miss rates translate mto better per\u00adformance. Table 4 presents \nexecution time results on the DEC Alpha (first level cache: 8K, direct-mapped, 32 byte line; second level \ncache: 5 12K, banked). We measured the execution times for TSS with and without computmg the tile sizes \nat runtime, It made no measurable impact on performance. The simulated miss rates and execution times \nfor SOR2D 256 x 256 do not agree (LRW should be best), nor do the miss rates and execution times for \nLUD2D when TSS is compared to Esseghir (TSS should be sigruficantly better). We believe these inconsistencies \nresult due to a combination of two factors: the difference in working set sizes and the Alpha s large \nsecond level cache (5 12K). TSS always has at least as good cache efficiency in terms of working set \nsize as the other algorithms. Esseghir often uses too big of a working set, resulhng in interference. \nLRW uses a small working set (often around 50%) because they are hrnlted to square tales, TSS may therefore \nget more benefit from the second level cache. TSS always improves or matches execution time when com\u00adpared \nto LRW or Esseghm s algorithm. On average, TSS im\u00adproved over LRW by a factor of 1.12 when the pathological \ncases with array sizes of 256x256 are excluded. TSS Improved over Esseghir on average by a more significant \nfactor of 1.37, and by 2.01 on matrix multiply (again excluding array sizes of 256x256). 5.2.2 64K Caches \nTables 5 and 6 show the same type of results as the previous two tables, but for variants of the RS/6000 \norganization (64K, 4-way, 128 byte line). The simulated miss rates and execution times showed more variations \nthan those for the 8K cache. These inconsistencies probably result because the simulator uses an LRU \nreplacement pohcy and the RS6000 uses a quicker, but unpublished replacement pohcy. TSS still achieves \nlower miss rates more often (a factor of 1.19 without 256 x 256 arrays), but Esseghir s algorithm has \nlower miss rates for most of the 4-way simulated results, Esseghlr s algorlthm out performs TSS and LRW \nin execution times on the RS6000 (for TSS, by a factor of 1.03). This result probably stems from set \nassociativity and efficiency of the working set size. For this cache organization, Esseghir tends to \nencounter less interference because the working set sizes either fit in cache or are only slightly larger \nthan the cache. When Esseghir encounters cross interference, the 4-way associativity is now more likely \nto over come It through the cache, rather than the tile sizes. For miss rates, LRW achieves lower miss \nrates TSS by a factor of 1.17 (excluding 256 x 256 arrays), but most of this comes from SOR2D. When SOR2D \nis excluded, TSS has lower miss rates by a factor of 1.10 (excluding 256x256 arrays). TSS however continues \nto out performs LRW on the RS/6000, as it dld on the Alpha, but by less. In both Table 5 and 3, the square \ntde sizes use significantly less of the cache and are probably therefore not as effective. 5.3 Copying \nTo demonstrate that copying is unnecessary to achieve good per\u00adformance, we compared execution times \nfor tiled matrix multiply using tile sizes chosen by TSS to execution times for code gen\u00aderated by Esseghir \ns Tale-and-Copy algorlthm [Ess93]. These results appear in Table 7. The execution times for TS S include \ncomputmg the tile sizes at runtime. Tile sizes for Esseghlr s code were calculated using the formula \nTS = <-S 2 where TS is the tile size. We also used Esseghlr s algorithm Table 7: Matrix multlply execution \ntimes (seconds) on the Alpha EEEEiEE~~~:::   ::: ::::: wE3RElEE~:: :: :: : ~: ~ ::::: E : :~ E3Ha3ERE:: \n80::: ~~~ sor2D 1 256 1.966 256x 1 768 0.527 2x2 10 0,404 256x 1 768 0.527 sor2D 2 256 1.943 256x 1 \n768 0,477 2x2 10 0.253 256 X 1 768 0.477 sor2D 4 256 1,816 256x 1 768 0.098 2x2 10 0,123 256x 1 768 0.098 \nsor2D 1 300 1.214 88x3 440 0.370 16x14 256 0.076 300X 1 900 0.409 sor2D 2 300 0.901 88x3 4401 0.015 16x14 \n256 0.006 300X 1 900 0.138 sor2D 4 300 0.927 88x3 4.40, ()()14 16x 14 256 0.004 300X 1 900 0,166 EEEEEE! \n~ 7: 8~ :: := EEEE%EE ~ :: ::: :: : ::: EEEEIEE :: ~: : : : : : ~ ~:7 2~=: :~~ Table 3: Miss rates and \ntile sizesin8K(512 element) cache with 32 byte (2 element) lines for N x N arrays. Untiled TSS LRW Esseghir \nSpeedup Kernel NxN Time Tile Size Time Tile Size Time Tile Size Time LRWITSS ESS/TSS mm 256 1.85 170x2 \n1.68 2x2 5.69 256x2 1.70 3.39 1.01 mm 300 3.36 16x29 1,88 16x16 1,88 300X 1 3.47 1.00 1,85 mm 301 3.41 \n28x17 1.82 1 7X17 2.09 301 x 1 3.64 1.15 2.00 mm 550 26.03 18x27 11.85 18x18 12.95 512x1 25.67 1.09 2.17 \nsor2D 256 1.53 256x 1 1.50 2x2 2.60 256x 1 1.50 1.73 1.00 sor2D 300 2,35 88x3 2.07 16X 14 2.18 300X 1 \n2.10 1.05 1.01 sor2D 301 2.42 90X3 2.10 1 7X15 2.27 301X1 2.11 1.08 1.00 sor2D 550 9.81 38x12 7.06 18x16 \n7.42 512x1 8.87 1.05 1.26 hrd2D 256 1.23 170x2 1.13 2:X2 3.64 256x2 1.11 3.22 0.98 hrd2D 300 1,88 16x29 \n1.76 16X16 1.76 300X 1 1.60 1.00 0.91 lud2D 301 1.91 30x12 1.64 1:7X17 1.76 301X1 1.61 1.73 0.98 hrd2D \n550 13.48 18x27 10.32 18x18 10.36 512X1 11.92 1.00 1.15 Average Improvement 1.54 1.27 Average Improvement \nwithout 256 x256 cases 1,12 1.37 Table 4: Execution Times in seconds on the DEC Alpha (direct-mapped, \n8K, 32 byte lines) 287 Untiled TSS LRW Esseghir Size MISS Tile WSet Miss Tile WSet Miss Tile WSet Miss \nKernel Sets NxN Rate Size Size Rate Size Size Rate Size Size Rate mm 1 256 4.547 240x16 4088 0,566 16X16 \n280 0.647 256x16 4360 0.569 mm 2 256 4,441 240x16 4088 0.040 16x16 280 0.080 256x16 4360 0.045 mm 4 256 \n4.439 240x16 4088 0.036 16X16 280 0.029 256x 16 4360 0.030 mm 1 300 0.534 88x41 3704 0.133 41 X41 1730 \n0.193 300X13 4208 0.218 mm 2 300 0.419 88x41 3704 0.042 41 X41 1730 0.043 300X13 4208 0.042 mm 4 300 \n0.419 88x41 3704 0.042 41X41 1730 0.030 300X 13 4208 0.036 mm 1 301 0.509 112x26 3086 0.081 53x53 2870 \n0.172 301X13 4222 0.198 mm 2 301 0.524 112x26 3086 0.045 53x 53 2870 0.026 301X13 4222 0.038 mm 4 301 \n0.419 112x26 3086 0.045 53x53 2870 0.023 301X13 4222 0.036 sor2D 1 256 0.080 256x14 4096 0,157 16x14 \n256 0.041 256x14 4096 0.157 256 sor2D 2 0.010 256x14 4096 0.005 16x14 256 0.001 256x16 4096 0.005 sor2D \n4 256 0.001 256x 14 4096 0.003 16x14 256 0.006 256x16 4096 0.003 sor2D 1 300 0.064 300X11 3900 0.161 \n41X39 1681 0.031 300X11 3900 0.161 sor2D 2 300 0.008 300X11 3900 0,003 41X39 1681 0.001 300X11 3900 04003 \nsor2D 4 300 0.001 300X11 3900 0,003 41X39 1681 0001 300X11 3900 0.003 sor2D 1 301 0,064 301X11 3913 0.161 \n53X51 2809 0.033 301X11 3913 0.161 sor2D 2 301 0.008 301X11 3913 0.003 53X51 2809 0.001 301X11 3913 0.003 \nsor2D 4 301 0.001 301X11 3913 0.003 53X51 2809 0.001 301X11 3913 0.003 lud2D 1 256 0.601 240x16 4096 \n0.245 16X16 288 0.251 256x16 4368 0.244 lud2D 2 256 0.307 240x16 4096 0.034 16X16 288 0.053 256x16 4368 \n0.030 lud2D 4 256 0.305 240x16 4096 0,038 16X16 288 0,053 256x16 4368 0.034 lud2D 1 300 0.330 88x41 3737 \n0.075 41 X41 1763 0.070 300X13 4213 0.065 lud2D 2 300 0.319 88x41 3737 0.027 41X41 1763 0.030 300X13 \n4213 0.038 lud2D 4 300 0.318 88x41 3737 0.026 41 X41 1763 0.028 300X13 4213 0.038 112x26 2915 lud2D 1 \n301 0.333 3050 0.050 53x53 0.080 301X13 4227 0.064 lrrd2D 2 301 0.321 112x26 3050 0.028 53x53 2915 0.029 \n301X13 4227 0.037 lud2D 4 301 0.522 112x26 3050 0.027 53x53 2915 0.028 301X13 4227 0,038 Table 5: Miss \nrates and tile sizes m a 64K (4096 element) cache, 128 byte (8 element) lines Untiled TSS LRW Esseghir \nSpeedup Kernel NxN Time Tile Size Time Tile Size Time Tile Size Time LRWITSS ESSITSS ~40x16 mm 256 2.08 \n1.79 16X16 2.15 256x16 1.76 120 0.98 mm 300 3.37 88x41 2,89 41X41 2.99 300X13 2.82 1.03 0.98 mm 301 3.42 \n112x26 2,87 53x53 2.97 301X13 2.89 1.03 1.01 mm 550 20.80 56x64 18.03 58x58 18.07 550X7 17.76 1.00 0.99 \n256 ~,~7 sor2D 256x14 2.21 16x14 230 256x16 2,21 1.01 1.00 r sor2D 300 3.12 300X11 3.05 41X39 3.06 300X11 \n3,05 1.00 1,00 sor2D 301 3.14 301X11 3.05 53X51 3.06 301X11 3.05 1.00 1,00 sor2D 550 10.62 550X5 10.27 \n58x56 10.37 550X5 10.27 1.01 1.00 lud2D 256 1.27 240x16 O 82 16X16 104 256x16 0,79 1.27 0.96 lud2D 300 \n2.02 88x41 1.38 41X41 1.47 300X13 1.29 1.07 0.93 lud2D 301 2.06 112x26 1435 53x53 1.45 301X13 1.28 1.07 \n0.95 lud2D 550 12.87 56x64 8.70 58x58 8.65 550X7 7.94 0.99 0.91 Average Improvement 1.06 0.98 Average \nImprovement without 256 x256 cases 1.02 0.97 Table 6: Execution Times in seconds on the RS/6000 (64K, \n4-way, 128 byte hnes) 288 to select rectangular tile sizes, but these execution times were longer, TSS \nsignificantly speeds up performance as compared to Esseghir s Tile-and-Copy algorithm. Because copying \nis an expensive run-time operation. tiling alone performs significantly better, even on the pathological \nIV = 256 case where there is severe self interference. Summary This paper presents a new algorithm for \nchoosing problem-size dependent tile sizes using the cache size and line size. Its run\u00adtime cost is negligible \nmaking it practical for selecting tile :sizes and deciding when to tile whether or not array sizes are \nun\u00adknown at compile time. This algorithm performs better than prewous algorithms on direct-mapped caches. \nIt also performs well for caches of higher associatlvity when matrices are larger in comparison to the \ncache size. It obviates the need for copy\u00ading. As compilers for scalar and parallel compilers increasl \nngly turn their attention to data locality, tiling and other data locality optimization will only increase \nin importance. We have shown the tile size selectlon algorithm to be a dependable and effective component \nfor use in a compiler optimization strategy that seeks to use and manage the cache effectively. Acknowledgements \nWe would like to thank Sharad Singhai for assisting m the initial experiments on this work and James \nConant and Susan Landau for their insights on the Euclidean Algorithm. References [BJWE92] F. Bodin, \nW. Jalby, D. Windheiser, and C. Eisenbeis. A quantitative algorlthm for data locality optimiz\u00adation. \nIn Code Generation-Concepts, Tools, Tech\u00adtziques. Springer-Verlag, 1992. [CK92] S. Carr and K. Kennedy. \nCompiler blockability of numerical algorithms. In Proceedings of Superconz\u00adputing 92, Minneapolis, MN, \nNovember 1992. [CL95] S. Carr and R. B. Lehoucq, A compiler-blockable algorithm for QR decomposition. \nIn Proceedings of the Eighth SIAM Conference on Parallel Process\u00ading for Scient@c Computing, San Francisco, \nCA, February 1995. [CMT94] S. Carr, K. S. McKinley, and C. Tseng. Compiler op\u00adtimization for improving \ndata ]ocahty. In Proceed\u00adings of the Sixth International Conference on A rchi\u00adtectural Support for Programming \nLmzgaages and Operating Systems, San Jose, CA, October 1994 [CO194] S. Coleman. Selecting tile sizes \nbased on cache and data organization. Master s thesis, Dept. of Computer Science, University of Massachusetts, \nAmherst, September 1994. [Ess93] K. Esseghir. Improving data locality for caches. Master s thesis, Dept. \nof Computer Science, Rice University, September 1993. [GJG88] [IT88] [Kob87] [LRW91 ] [NJL94] [Smi82] \n[TGJ93] [WL91] [W0189] D. Gannon, W. Jalby, and K. Gallivarz. Strategies for cache and local memory management \nby global pro\u00adgram transformation. Journal of Parallel and Dis\u00ad tributed Computing, 5(5):587 61 6, October \n1988. F. Irigoin and R. Triolet. Supernode partitioning. In Proceedings of the Fl~teenth Annual ACM Sympo\u00adsium \non the Principles of Programming Languages, San Diego, CA, January 1988. Neal Koblitz. Graduate Texts \nin Mathematics. Springer-Verlag, New York, 1987. M. Lam, E. Rothberg, and M. E. Wolf. The cache performance \nand optlmizations of blocked al\u00adgorithms. In Proceedings of the Fourtit Interna\u00adtional Conference on \nArchitectural Support for Pro\u00adgramming Languages and Operating Systems, Santa Clara, CA, April 1991. \nJ. J. Navarro, T. Juan, and T. Lang. Mob forms: A class of multilevel block algorithms for dense linear \nalgebra operations. In Proceedings of the 1994 ACM International Conference on Supercomputing, pages \n354-363, Manchester, England, June 1994. A. J, Smith. Cache memories. Cotnputing Surveys, 14(3):473 530, \nSeptember 1982. 0. Temam, E. Granston, and W. Jalby. To copy or not to copy: A compile-time technique \nfor assessing when data copying should be used to eliminate cache contlicts. In Proceedings of Supercontputing \n93, Portland, OR, November 1993. M. E. Wolf and M. Lam. A data locality optimiz\u00ading algorithm. In Proceedings \nof the SIGPLAN 91 Conference on Programming Language Design and Implementatiorz, Toronto, Canada, June \n1991. M. J. Wolfe. More iteration space tiling. In Proceed\u00adings of Supercornputing 89, pages 655 664, \nReno, NV, November 1989. A Tiled Kernels  A.1 SOR Successive over relaxation is a five-spot stencd on \na two dimen\u00adsional array, The point algorithm with the best locality appears in Figure 6(a). The tiled \nversions for one and two dimensions appear in Figure 6(b) and (c), respectively. (a) SOR DO K=l, N DOJ= \nK+l, N DO I=2, N-1 A(I,J) = 0.2( A(I,J) + A(I+I,J) + A(I-l,J) + A(I,J+l) + A(I,J-1) wu JJ (b) sorld: \nSOR Tiled in One Dimension DO K=l, N DO H=2, N-l, T1 DO J=2, N-1 DO I = II, MIN(N-I, II+TI-1) A(I,J) \n= 0.2(A(I,J) + A(I+l,J) + A(I-I,J) + A(I,J+l) + A(I,J-1) uu0 (c) sor2d: SOR Tiled in IWO Dimensions \nDO JJ=2, N-t, TJ DO K=l, N DO 11=2, N-1. TI DO J = JJ, MIN(N-1. JJ+TJ-1) DO I = II, MIN(N-1, II+TI-1) \nA(I,J) = 0.2(A(I,J) + A(I+I,J) + A(I-1 ,J) + A(I,J+l) + A(I,J-1) Figure 6: A.2 Livermore Loop 23 Livermore \nLoop 23 is also a stencil kernel, but over 4 distinct arrays. The point and one dimensional tiled versions \nappear in Figure 7 respectively. (a) Livermore Loop 23 DO J=2, M DO K=2, N QA= ZA(K,J+l)*ZR(K,J) +ZA(K,J-I)*ZB(K,J) \n+ ZA(K+l ,J)*ZU(K,J) +ZA(K-1,J)*ZV(K,J) +ZZ(K,J) ZA(K,J)= ZA(K,J) +fw*(QA -ZA(K,J)) v u u (b) liv23: \nLivermore Loop 23 Tiled in One Dimension DO KK = 2,N,TK DO J= 2,M DO K = KK,MIN(KK+TK-1,N) QA= ZA(K,J+I)*ZR(K.J) \n+ZA(K,J-l)*ZB(K,J) + ZA(K+l ,J) ZU(K,J) +ZA(K-1,J)*ZV(K,J) +ZZ(K,J) ZA(K,J)= ZA(K,J) +fw*(QA -ZA(K,J)) \nFigure 7:  A.3 LUD LUD decomposes a matrix A into two matrices, L and U, where U is upper triangular \nand L N a lower triangular unit matrix. The point algorithm for LUD appears in Figure 8(a). The ver\u00adsion \nof LUD tiled in one dimension in Figure 8(b) 1staken from (a) LUD DO K=l, N DOJ= K+l, N A(J,K) = A(J,K)/A(K,K) \nDOI= K+l, N A(I,J) = A(I,J) -A(I,K) * A(K,J) 0v u (b) Iudld: LUD Tiled in One Dimension DO KK= 1,N, TK \nDO K = KK, MIN(N,KK+TK-1) DOI= K+I, N A(I,K) = A(I,K)/A(K,K) DOJ= K+l, N DOI= K+l, N A(I,J) = A(I,J) \n-A(I,K) * A(K,J) DO J= KK+TK, N DO I= KK+l, N DO K = KK, MIN(MIN(N,KK+TK-1),1-1) A(I,J) = A(I,J) -A(I,K) \n* A(K,J) uuv (c) lud2d: LUD TIIed in Two Dimensions DOJJ= 1, N, TJ DO H=l, N,T1 DO K=I, N DO J = MAX(K+I,JJ), \nMIN(N,JJ+BJ-1) DO I = MAX(K+I,II), MIN(N,II+TI-1) IF ((JJ<K+l) &#38; (K-tl<JJ+TJ-1) &#38; (J= MAX(K+I,JJ)) \nA(I,K) = A(I,K)/A(K,K) END IF A(I,J) = A(I,J) -A(I,K) * A(K,J) Figure 8: Carr and Kennedy [CK92]. The \nversion tiled in two dimensions m Figure 8(c) IS a modified version of the one Wolf and Lam use [WL91 \n]. Their original version targeted C s row-major stor\u00adage and we modified it to target Fortran s column-major \nstorage. 290 \n\t\t\t", "proc_id": "207110", "abstract": "<p>When dense matrix computations are too large to fit in cache, previous research proposes tiling to reduce or eliminate capacity misses. This paper presents a new algorithm for choosing problem-size dependent tile sizes based on the cache size and cache line size for a direct-mapped cache. The algorithm eliminates both capacity and self-interference misses and reduces cross-interference misses. We measured simulated miss rates and execution times for our algorithm and two others on a variety of problem sizes and cache organizations. At higher set associativity, our algorithm does not always achieve the best performance. However on direct-mapped caches, our algorithm improves simulated miss rates and measured execution times when compared with previous work.</p>", "authors": [{"name": "Stephanie Coleman", "author_profile_id": "81100290331", "affiliation": "Intermetrics, Inc., 733 Concord Ave., Cambridge, MA", "person_id": "P267882", "email_address": "", "orcid_id": ""}, {"name": "Kathryn S. McKinley", "author_profile_id": "81100402805", "affiliation": "Computer Science, LGRC, University of Massachusetts, Amherst, MA", "person_id": "P157900", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/207110.207162", "year": "1995", "article_id": "207162", "conference": "PLDI", "title": "Tile size selection using cache organization and data layout", "url": "http://dl.acm.org/citation.cfm?id=207162"}