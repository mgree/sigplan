{"article_publication_date": "06-01-1995", "fulltext": "\n Interprocedural Partial Redundancy Elimination and Its Application To Distributed Memory Compilation \nGagan Agrawal and Joel Saltz and Raja Das UMIACS and Department of Computer Science University of Maryland \nCollege Park, MD 20742 (301)-405-2756 {gagan,saltz,raja}@cs. Abstract Partial Redundancy Elimination \n(PRE) is a general scheme for suppressing partial redundancies which en\u00adcompasses traditional optimizations \nlike loop invariant code motion and redundant code elimination. In this paper we address the problem \nof performing this op\u00adtimization interprocedurally. We use interprocedural partial redundancy elimination \nfor placement of com\u00admunication and communication preprocessing state\u00adments while compiling for distributed \nmemory parallel machines. Introduction Partial Redundancy Elimination (PRE) is a well known technique \nfor optimizing code by suppressing partially redundant computations. It encompasses traditional optimizations \nlike invariant code motion and redun\u00addant computation elimination. It is widely used in optimizing compilers \nfor performing common subex\u00adpression elimination and strength reduction. More re\u00adcently, it has been \nused for more complex code place\u00adment tasks like placement of communication state\u00adments while compiling \nfor parallel machines [12, 15]. A number of schemes for partial redundancy elimina\u00adtion have been proposed \nin literature [10, 11,20,19, 25], but are largely restricted to optimizing code within a single procedure. \nAll these schemes perform data flow analysis on Control Flow Graph (CFG) of the proce\u00addure. In this paper, \nwe address the problem of per\u00adforming partial redundancy elimination interprocedu\u00adrally. There are several \ndifficulties in extending the ex\u00adisting intraprocedural algorithms for application on a full program, \nrather than full program representation efficient data flow analysis, *Thk work was supported 9213821 \nand by ONR under a single procedure. First, a is required which will allow while maintaining sufficient \nby NSF under grant No. ASC contract No. NOO014-93-1-0158. The authors assume all responsibility for \nthe contents of the paper. Permission to copy without fee all or part of this material is granted provided \nthat the copies are not made or distributed for direct commercial advantage, the ACM copyright notice \nand the title of the publication and its date appear, and notice is given that copyin is by permission \nof the Association of Computing Machinery. ? o copy otherwise, or to republish, requires a fee and/or \nspecific permission. SIGPLAN 95La Jolla, CA USA (3 1995 ACM 0-89791 -697-2/95/0006 ...$3.50 umd.edu \nprecision to allow useful transformations and to ensure safety and correctness of transformations. Renaming \nof formal parameters across procedures must be correctly done while propagating data flow information. \nCall\u00ading context of procedures must be correctly accounted for, also correctness and safety must be maintained \nif a procedure is called at multiple sites with different sets of parameters. We have developed an Interprocedural \nPartial Re\u00addundancy Elimination algorithm (IPRE). Our method is applicable to arbitrary recursive programs \nand arbi\u00adtrary control flow within each procedure. We have used interprocedural partial redundancy elimination \nfor optimizing placement of communication statements and communication preprocessing state\u00adments in distributed \nmemory compilation. We have implemented our scheme usin the the existing For\u00ad 7 tran D compilation system \n[17 as infrastructure. We have shown significant performance gains by optimizing placement of communication \npreprocessing statements. The rest of this paper is organized as follows. In Section 2, we discuss how \ninterprocedural partial re\u00addundancy elimination is required for placement of com\u00admunication statements \nand communication preprocess\u00ading statements while compiling for distributed memory parallel machines. \nIn Section 3, we briefly review an in\u00adtraprocedural partial redundancy elimination scheme. The program \nrepresentation used in interprocedural data flow analysis is stated in Section 4. Interprocedu\u00adral partial \nredundancy elimination is presented in Sec\u00adtion 5. In Section 6, we present experimental results to evaluate \nthe effectiveness and cost of our scheme. In Section 7, we mention related work. W e conclude in Section \n8. Intraprocedural partial redundancy scheme is presented in the appendix. 2 Distributed Memory Compilation \nIn compiling programs for execution on distributed memory parallel machines? an important considera\u00ad \ntion is optimizing commumcation between processors. Since the existing machines have relatively large \ncom\u00ad munication latencies, communication overhead can be reduced if message aggregation is done, i.e. \neach pro\u00ad cessor sends a small number of large messages. There are several cases in which the set of \ndata elements to be communicated between the processors cannot be deter\u00ad mined at compile-time. This \ncan happen because data 258 Real Integer X(m), IA(u) Y(m) ! data arl ays ! indirection array C c Build \nthe required schedule DS = Sched(.. parameters ..) Communicate data using the schedule Call Data.Move(Y,DS) \nbuild above forall X(i) i = = 1, n X(i) + Y(IA(i)) do X(i) lOi= = X(i) 1, n-local + Y(IAlocal(i)) Figure \nin the 1: left Compiling and SPMD an parallel loop code is shown which in the accesses right. data through \nan indirection array. Sequential code is shown is accessed using indirection arrays, data distribution \nmay not be known at compile-time, number of proces\u00adsors on which the program is to be run is not known \ntill runtime or due to the presence of symbolic loop bounds and strides in a parallel loop. In these \ncases, communication can be optimized by placing a prepro\u00adcessing statement, which determines the set \nof data elements to be communicated between the processors at runtime. The preprocessing statement stores \nthis information in a data-structure called communication schedule [24]. A collective communication routine \nthen performs the data movement, usil?g the information in the communication schedule. Tlus ensures that \nfor a parallel loop, each processor packages the set of data elements it wants to send to any other processor \nin a single message. In Figure 1, we show how SPMD code can be gen\u00aderated for the given loop in which \ndata is accessed through an indirection array (IA). A communication schedule is generated by a call to \nSche{, which ana\u00adlyzes the contents of array IA to determme the exact communication required. The required \ndata elements are sent or received by the Data-Move primitive. The reason for separation of these two \nphases of communication is that the result of preprocessing can be used for communicating several times. \nCompiler analysis has been developed for analyzing the data access patterns associated with a given parallel \nloop and inserting calls to appropriate communication pre\u00adprocessing routines and collective communication \nrou\u00adtines [2, 3, 9]. After such an initial analysis at a sin\u00adgle parallel loop or a single procedure \nlevel, placement of these statements must be optimized interprocedu\u00adrally. Large scientific and engineering \napplications of\u00adten present opportunities for reusing communication schedule several times and it is \nimportant to do this optimization to obtain reasonable performance. VVe, therefore, identify two optimization \nproblems, comrn u\u00adnicatzon schedule generation placement and communi\u00adcation placement. Partial redundancy \nelimination can be applied interprocedurally for solving both these op\u00adtimization problems. Partial redundancy \nelimination encompasses loop invariant code motion and redundamt code elimination and has been widely \nused intraproce\u00addurally to improve the runtime performance of codes. We believe that it can be applied \ninterprocedurally to optimize placement of communication preprocessing statements and communication statements. \n3 Intraprocedural Redundancy Elimination The cletails of interprocedural redundancy elimination we present \nare derived from the intraprocedural node based method of Dhamdhere [10], also referred to as Modified \nMorel Renvoise Algorithm (M MRA). De\u00adtailed data flow equations and the meaning of the terms used are \ngiven in the appendix. All terms used are for a particular computation, e.g AVINC (i) is the availabil\u00adity \nof the computation C at the beginning of node i. Whenever there is no scope for ambiguity, the subscript \nis dropped (as in the equations given in this paper). PRE considers snbexpressions or computafzons as \ncandidates for placement. Transparency of a basic block means that the none of variables involved in \nthe computation are modified in the basic block. Based upon transparency, two properties, availability \nand par\u00adtial availability are computed for beginning and end of each basic block (denoted respectively \nas AVIN, PAVIN, AVOUT and PAVOUT for each basic block). Availability of a computation at a point p in \na proce\u00addure means that this computation is currently placed at all the paths leading to p and if this \ncomputation is placed at p, it will have the same result as the last com\u00adputation on any of the paths. \nPartial availability is a weaker property, which means that the computation is currently placed on at \nleast one control flow path lead\u00ading to p and if it is placed at p, it will have the same result as the \nlast computation on at least one of the paths. A computation placed at p is partially redun\u00addant if it \nis partially available at p. Next, for each basic block in the program, proper\u00adties PPIN (possible placement \nat the beginning) and PPOUT(possible placement at the end) are computed. PPIN reflects the fact that \nit is feasible and profitable to hoist the computation occuring in this node (or a computation which \nhas been hoisted into this node). PPOUT indicates that it is safe to place the computa\u00adtion at the exit \nof this node. INSERT determines if a computation is to be inserted at the end of the a block as a result \nof the optimization and DEL determines if the computation in this node has become redundant and can be \ndeleted. 4 Program Representation 4,1 Definition We assume that a variable is either global to the entire \nprogram or is local to a single procedure. We further 259 assume that all parameters are passed by reference. \nWe do not consider the possibility of aliasing in our discus\u00adsion. Each procedure has one or more return \nstatements, which end the invocation of this procedure. We define a basic block to consist of consecutive \nstatements in the program text without any procedure calls or return statements, and no branching except \nat the beginning and end. ~ procedure can then be partitioned into a set of basic blocks, a set of call \nstatements and a set of return statements. Each call statement is a call site of the procedure invoked \nthere. In general, a procedure can be invoked at several call sites in the program. For the purpose of \nperforming interprocedural PRE on the full program, we have defined the following rep\u00adresentation. Intuitively, \nthe idea is to construct blocks of code within each procedure, A block of code com\u00adprises of basic blocks \nwhich do not have any call state\u00adment between them, In the directed graph we define below, each edge \ne corresponds to a block of code B(e). The nodes of the graph help clarify the control flow re\u00adlationships \nbetween the blocks of code. Full Program Represent at ion: ( FPll) is a di\u00adrected Multigraph G = (V, \nII), where the set of nodes V consists of an entry node and a return node for each procedure in the program. \nFor procedure i, the entry node is denoted by Si and the return node is denoted by ri. Edges are inserted \nin the following cases: 1. Procedures i and j are invoked by procedure k at call sites Csl and CS2 respectively \nand there is a path in CFG of k from Csl to Csz which does not include any other call statements. Edge \n(rt, s ) exists in this case. This edge is said to be associate J with call site cs~ at its start and \nwith call site CS2 at its end. The block of code B(e) consists of basic blocks of procedure k which \n2. Procedure z invokes procedure j at call site es and there is a path in CFG of i from the start node \nof procedure i to cs which does not include any other call statements. In this case, edge (si, sj ) exists. \nThis edge is said to be associated with call site es at its end. The block of code l?(e) consists of \nbasic blocks of procedure i which may be visited in any control flow path p from start of i to CS, such \nthat the path p does not include any other call statement. 3. Procedure j invokes procedure i at call \nsite es and there is a path in CFG of j from call site cs to a return statement within procedure j which \ndoes not include any other call statements. In this case, edge (ri, rj ) ex\u00adists. This edge is said to \nbe associated with call site cs at its start, The block of code l?(e) consists of basic blocks of procedure \nj which maybe visited in any con\u00adtrol flow path p from cs to a return statement of j, such that the path \np does not include any call statements.  4. In a procedure i, there is a possible flow of control from \nstart node to a return statement, without any call statements. In this case, edge (s;, r~) exists. The \n may be visited in any control flow path p from Csl to CS2, such that the path p does not include any \nother call statements. Program Foo a=l Do i = 1, 100 Call P(a,b) . ..CS1 Call Q(c) . ..CS2 Enddo Call \nQ(a) . ..CS3 Call P(a)c) . ..CS4 if cond then Call Q(a) . ..CS5 Endif Call R(a,c) . ..cs6 End Procedure \nP(x,y) Sched(x,y) ..other computat~ons . . End Procedure Q(z) z = . ..Z... End Procedure R(y,z) Sched(y,z) \n..other computations . . End Figure 2: An Example Program. A call site number is marked for each call \nsite block of code B(e) consists of basic blocks of procedure i which may be visited in any control flow \npath p from start of i to a return statement in i, such that the path p does not include any call statements. \nAn example program and its FPR are shown in Fig\u00adures 2 and 3 respectively. In Figure 3, the blocks of \ncodes B(4), B(9) and B(n) comprise of all basic blocks in procedures P, Q and R respectively. Block of \ncode corresponding to all other edges comprise of basic blocks from the main procedure. e.g. B(1) comprises \nof statement a = 1 and the loop header, B(2) com\u00adprises of the end of the do loop and the loop header. \nA block of code is a unit of placement in our anal\u00adysis, i.e. we initially consider placement only at \nthe beginning and end of a block of codel Note that a ba\u00adsic block in a block of code may or may not \nbe visited along a given control fiow path from source to sink of the edge, and similarly, a basic block \nmay belong to several blocks of code. This is taken into account dur\u00ading intraprocedural analysis done \nfor determining final local placement ~which we discuss in Section 5.5. The availablhty of the following \ninformation is as\u00adsumed during our interprocedural analysis phase. For each edge in FPR, we compute all \nthe variables which are modified in the block of code corresponding to this edge. This information is \nused by the TRANSC fUllc\u00ad 1This is different from intraprocedural PRE in which place\u00adment is considered \nat beginning and end of node (basic block) of the graph. 260 Foo //0 L Cs1 Csl { 4 / [P CS4 11 1 /\\ \nl\\Foo ) -~ o Procedure Entry Node / -\\ I) \\ Procedure Return Node -~ Figure 3: FPR for Program in Left. \nEdge numbers and call sites at which edges start/end (whenever applica\u00adble) aremarked in the Figure. \ntion defined later. For each procedure in the program, we also compute the list of variables modified \nby the procedure or any of the procedures invoked by this pro\u00adcedure. In absence of aliasing, this information \ncan easily be computed by flow-insensitive interprocedural analysis in time linear to the size of call \ngraph of the program [7]. This information is used by the CMODC, function defined later. 4.2 Candidates \nfor Placement We consider only the placement of pure functions. A pure function takes a number of parameters \nas input and produces a single result or output, without any side-effects or change in the value of inputs. \nIn gen\u00ad eral, any subexpression can also be viewed as a pure function. In practice, one may want to focus \non place\u00ad ment of only certain high cost functions, like commu\u00ad nication statements and communication \npreprocessing statements in the case of distributed memory compila\u00ad tion. A particular invocation of \na pure function is consid\u00adered for hoisting out of the procedure only if none of the parameters of the \npure function is modified along any path from the start of the procedure to this invocation of the pure \nfunction and the invocation of pure function is not enclosed by any conditional or loop. (This can be \ngeneralized by considering slice of the pure function, but we do not discuss this possibility here). \nA particu\u00adlar invocation of a pure function is referred to as can\u00addidate if it is considered for interprocedural \nplacement. We refer to the list of parameters of this pure function as the list of influencers of the \ncandidate. 5 Interprocedural Partial Redundancy Elimination We now present the IPRE scheme we have developed. \nWe use the terms edge and the block of code corre\u00adsponding to it interchangeably in this section. Given \nthe full program representation we described in Section 4, the major difficulties in applying data flow \nanalysis for PRE are: 1. A procedure having a candidate for placement (or a procedure invoking such a \nprocedure) may be invoked at multiple call sites with different sets of actual pa\u00adrameters, leading to \ndifferent sets of influencers. (e.g. in the code shown in Figure 2, procedure P is invokes at two call \nsites with different parameters). While con\u00adsidering placement of the candidate outside the pro\u00adcedure \nit is originally placed, it must be ensured that only the computation of the candidate with correct set \nof influencers is visible during each invocation of the procedure. 2. For placement of a candidate at \na certain point in a certain procedure, besides safety and profitability of the placement, it is also \nrequired that all influencers of the candidate are visible inside that procedure, i.e.  each of them \nis either a global variable, a formal pa\u00adrameter or a local variable. (e.g. in the code shown in Figure \n2, no placement will be possible inside proce\u00addure Q). 261 T /h Inf12 Q q~ @ Figure 4: Lattice used \nin 3, If a procedure is invoked at several call sites in the program, our program representation shows \npaths from edges ending at a call site calling this procedure to the edges starting at other call sites \nfor this procedure. (e.g. in Figure 3 there is a path from edge 6 to edge 9 to edge 2. Edge 6 ends at \ncall site CS5 whereas edge 2 starts at call site CS2). These paths are never taken and the data flow \nanalysis must not lose accuracy because of such paths in the graph. 4. Transparency of blocks of code \ncannot be deter\u00admined before starting the solution of data flow equa\u00adtions, since it is not known what \nare the local variables which need to be unmodified for the propagation of data flow information. 5.1 \nLattice for Data Flow Problems  We assume that the result of the computation of a can\u00addidate is always \nplaced in a global store, i.e. it is not passed along as an actual parameter at the call sites. Consider \na procedure p which has a candidate C for placement and is invoked at call sites CS1 and CS2 with different \nsets of parameters. Our scheme cannot place this candidate at a point from which there are paths leading \nto CS1 and csz and these paths do not have any further computation of C. This restriction must be incorporated \nwhile propagating availability and while considering locations for possible placement (PPIN and PPOUT). \nIf a candidate is available or if its placement is possible, it is always with a list of influencers, \nwhich will be used in placing the computation (i.e. if it is decided that the candidate is to be vlaced \nat this loca\u00ad . tion). For this purpose, we use a three-level lattice for the data flow problems. The \nlattice is shown in Figure 4. Each middle element in the lattice refers to a list of influencers? i.e. \nInfli = < VI, V2, . . . . Vn >. We define the following functions on this lattice: V and A are standard \nbinary join and meet operators. For ease in c m the data flow problems presenting our data flow equations, \nwe use ~ and ~ as confluence join and meet operators i.e. for computing join and meet respectively over \na set of elements. ~ is a unary operator which returns T when applied to a list of influencers and 1 \nwhen applied to T or 1. w is a binary non-commutative operators whose definition is as follows: 5.2 Terminology \nWe further use the following terms to describe the data flow equations in this paper. We had defined \nour pro\u00adgram represenation earlier in Section 4. In our Full Program Representation (FPR), the entry \nnode corre\u00adsponding to the main procedure is referred to as BEGIN node in the graph and similarly, the \nreturn node cor\u00adresponding to the main is referred to as the END node in the graph. The set of procedure \nreturn nodes is represented by X and the set of procedure entry nodes is represented by S. Consider an \nedge e = (v, w). The source node of e (i.e. the node v) is also referred to as So(e) and the sink node \nof e (i.e. the node w) is also referred to as Si(e). We use pred(e) to refer to the set of edges whose \nsink node is v. We denote by succ(e) the set of edges whose source node is w. If the sink of the edge \ne is a procedure entry node, then the call site with which the edge e is associated at its end is denoted \nby S/(e). We use sum (e) to refer to the set of edges which are associated with the call site S/(e) at \ntheir start. Alternatively, if the source of the edge e is a procedure return node, then the call site \nwith which the edge e is associated at the start is denoted by So (e). We refer by pred (e) the set of \n 262 edges which are associated with the call site So (e) at their end. Consider any edge e whose source \nis a procedure en\u00adtry node. The set cobeg(e ) comprises of edges whose source is the same as the source \nof edge e, If an edge e has a procedure return node as the source and if es is the call site with which \nthe edge e is associated at its start, then the set cobeg(e) comprises of the edges which are associated \nwith the call site es at their start. Next, consider any edge e whose sink is a procedure return node, \nThe set coend(e) comprises of the edges whose sink is the same as the sink of the edge e. If an edge \ne has a procedure entry node as the source and if cs is the call site with which the edge e is associated \nat its end, then the set coen cl(e) comprises of edges which are associated with the call site es at \ntheir end. The sets pred(e), preci (e), succ(e), sum (e), cobeg(e) and coend(e) for edges in the Graph \nshown in Figure 3 are shown in Figure 5. At any call site es, the set of actual parameters passed is \nap.~ and the jth actual parameter is apc$ (j). The set of formal parameters of the procedure invoked \nat the call site cs is ~pc~. (Clearly, this set is the same for all call sites which invoke this procedure). \nThe jth formal parameter is denoted by ~pc$ (j). The se~ of global variables in the program is gv. 5.3 \nAvailability and Partial Availability The equations for computing availability and partial availability \nare given in Figure 7. In computing avail\u00adability, all unknowns are initialized with T. This state means \nthat the candidate may be available, but we do not yet know what will be the list of influencers if it \nis available. Bottom element in the lattice means that the candidate is not available. Initially, the \nlocal data flow property ANTLOC(i) of the edges in the graph is determined. (For a block of code, ANTLOC \nmeans that there is a definition of this candidate inside the block.) In Section 4.2, we had discussed \nhow procedures are marked with candidates for placement. Consider an edge i whose source is a procedure \nentry node SP. If a candidate C is marked in the procedure p and -L (or false) when there is no occurrence \nof the candidate at procedure p. for placement from the procedure p with the list of influencers InflC, \nwe set ANTLOCC(i) = Inflc For renaming of formal parameters at call sites, we define two functions RNM1 \nC$ ancl RNM&#38; (see Fig\u00adure 6). Suppose a candidate is available at a call site cs with a list of influencers \nInfli. The function RNMIC, determines if this candidate can be available inside the procedure invoked \nat m, and if so, with what list of in\u00adfluencers. If any of influencers is neither a global vari\u00adable \nnor an actual parameters at es, RNMIC~ returns 1, otherwise, each actual parameter in the list is re\u00adplaced \nby corresponding formal parameter, RNMIC, [T] and RNML, [1] are defined to be T and 1 respectively, Suppose \na candidate is available at the return of a pro\u00adcedure and let cs be one of the call sites which invoke \nthis procedure. RNM~,, determines if this candidate will be available after the entry of the edges which \nstart at call site cs. If any of the influencers of the candi\u00addate inside the procedure is neither a \nglobal variable, nor a formal parameter, then RNM2C, returns J_. Oth\u00aderwise, each formal parameter is \nreplaced by the actual parameter at call site es. The equations for propagation of availability can be \nexplained as follows (see Figure 7). Consider an edge e whose source is a procedure entry node. A candi\u00addate \nwill be available at the entry of this edge e if the following holds: This candidate should be available \nat the exit of any edge p which ends at this procedure entry node (i.e. p c pred(e)), and furthermore, \nafter renaming (i.e. applying RNMIS,,(P)), the list of influ\u00adencers with which the candidate is available \nshould be the same for all such edges. If an edge e has a procedure return node So(e) as source, e is \nassociated with call site So (e) at its start. The set pred(e) comprises of edges whose sink is node \nSo(e) and the set pred (e) comprises of edges which are associated with the call site So (e) at their \nend. Note that even if the candidate is available at the end of all the edges p (p e pred (e)) and none \nof the influencers is modified inside the procedure, the candidate may not be available inside the procedure. \nThis can happen for two reasons, all influencers of the candidate may not be visible inside the proceclure, \nor the procedure may be invoked at multiple call sites ancl the candidate may not be available at other \ncall sites. In all other cases, ANTLOC(i) is set to 1. The following functions are used in our data \nflow equations. TRANSe [Infli] of an edge e in the graph re\u00adturns the list Infli if none of the influencers \nin the list Infli is modified in the block of code associated with this edge. If any of these influencers \nis modified, this function returns 1. TRANS. [T] and TRANS. [1] are defined to be T and 1 respectively. \nFor a call site es which invokes procedure p, CMODC. [Infli] returns the list Infli if none of the influencers \nin the list Infli is mod\u00adified by the procedure p (or by a procedure invoked by p). Otherwise CMODc, \n[Infl~] returns 1. CMODC, [T] always returns T and CMODC$ [1] always returns 1. OCRC (CS) determine if \nthe procedure p (or any proce\u00addure invoked by p) includes any occurrence of the can\u00addidate C. (Clearly, \nthis will be the same for all call sites which call procedure p). Whenever there is no scope for ambiguity, \nwe drop the subscript C. OCR(CS) returns T or true when there is an occurrence of the candidate If there \nis no definition of the candidate in the pro\u00ad cedure (CM ODSO,(.) does not return 1), then AVIN ( e) \nis determined by AVOUT at the edges belonging to pred (e). If there is any definition of the candidate \nin the procedure, then AVIN( e) is determined by AVOUT at the edges belonging to pred(e). Note that this \nstep preserves calling context of the procedure, i.e. accu\u00adracy in data flow analysis is not lost if \na procedure is invoked at multiple call sites. Equation 4 determines availability of a candidate at the \nend of an edge or block of code. If there is a com\u00adputation of the candidate in the edge with list of \ninflu\u00adencers Infli, then AVOUT is Infli if none of the influ\u00adencers is modified along this edge. If there \nis no com\u00adputation inside the edge (i.e. ANTLOC is 1), then the computation is available at the exit \nof the edge only if it is available at the entry of the edge and if none of the influencers is modified \nalong the edge. In computing partial availability, all unknowns are 263 e cobeg(e) coe:j(e) Pz4.E.L \n1  ;9 52,7 1:2 7 6,10 3 3 : 1,:,3 5,:s0 4 4 541;292;7 5 5 64 3986,106 7959 2,7 8 9 6I11 ;2 8 8;0 5,6,7 \n2,3,7 ?04 i 11 126;0 8;0 11 8,10 12 il 11 12 11 8;10 12 12 L Figure 5: pred(e), pred (e), succ(e) and \nsucc (e) sets for Graph in Figure 3 TI(Vi) = { vi fPcS (~) if if v% E Vt = gv aPcs(j) RNM1.s[< ?)1, . \n. . . V,, >] = { 1 < Tl(Lq),. ... T l(u~) > if 3i, (2L @ gv) other~~ise A (~j , VZ # ~Pcs(.7 )) (1) Tz(v, \n) = { vi ape.(j) if if v, V, E = gv fp.$ (j) RNM2C, [<Vi,...,%>] = { L < To,..., TZ(Vn) > if 21i, (v, \notherwise @ gv) A (Vj , I), # fpc, (j)) (2) Figure 6: Renaming functions initialized with 1. The equations \nfor computing par\u00adtial availability (Equations 5 and 6) are very, similar to corresponding equations \nfor computing avadability, except that join operator is used instead of meet oper\u00adator. The role of partial \navailability is to suggest profitabil\u00adity of transformations., it does not effect correctness and safety \nof transformations. Partial availability does not always guarantee that redundant code motion will not \noccur. We have therefore, used a simple method for de\u00adtermining partial availability, which may not always \nbe accurate. Inaccuracy comes in for two reasons. CMOD and TRANS. functions return 1 whenever one of \nthe influencers is modified in one of the basic blocks, this basic block may not occur in all the control \nflow paths taken. Secondly, calling context is not always preserved in propagating partial availability \ninformation. Precise computation of partial availability can be expensive, it will require a detailed \nrepresentation like Myer s Super-Graph [22] and use of stacks and/or graph reachability for preserving \ncalling context [23]. Our computation of partial availability still allows loop invariant code motion \nand redundant computation elimination. Some other optimizations which can be obtained by suppres\u00adsion \nof partially redundant computations may not be achieved because of this simple solution. 5.4 Data Flow \nAnalysis for Placement The data flow equations for determining placement of computations are shown in \nFigure 8. We briefly explain some of the key terms in these equations. In computing PPIN in the intraprocedural \ncase, the product term PPOUT + AVOUT ensures availability of the candidate at the entry of the node in \nthe op\u00adtimized program. PPOUT means that the candidate will be available as a result of the placements \ndeter\u00admined by the scheme. AVOUT means that the candi\u00addate is available in the original program. In the \ninter\u00adprocedural case, the same candidate can be available with more than one list of influencers. In \ncomputing PPIN in the interprocedural scheme, we use the term PPOUT M AVOUT (Equation 8). If PPOUT is \nset to a list of influencers InflZ then, after the placement determined by the scheme, the candidate \nwill be avail\u00ad able with set of influencers Inf12, even if it is available with a different list of influencers \nbefore the optimized placements. If PPOUT is 1 and AVOUT is Infll, then the candidate will be available \nwith the same set of in\u00adfluencers Inflj even after the placement. The rational behind the equation for \ndetermining INSERT is as follows. We decide to insert a candidate with the set of influencers Infl; at \nthe end of a block of code e, if PPOUT(e) is Infli, AVOUT(e) is not Infli and either PPIN (e) is not \nInfll or one of the influencers in the list Infl, is modified in this block of code. The term 264 if \nSo(e) is BEGIN node if So(e) C~i p~pred(e) ( RNMIsiJ(F) [AVOUT(P) ] ) AVIN(e) = (3) cMODsoJ(.)[ ~n,cnre~ \n(el AVOUTW 1 if (So(e) c 7?) A ( -I OCR(So (e) ) ) ~-r....,./ { RNM2s. (.)[APePTe~1 if (so(e) c 72) A \n( OCR (so (e))) (e)AVOUT(P) AVOUT(e) = (4) A.ecoemi(e) (TRANSCIANTLOC(C) u AVIN(C)I) if So(e) is BEGIN \nnode + pepred(e) ( RNMIs~Lp) [ PAVOUT(P) ] ) if So(e) c ~ PAVIN ( e) = (5) CMODso(e)[V l,,~pre(l (e) \nPAVOUT(p ) ] if (So(e) c 7?) A ( 10CR(So (e) ) ) RNk12sO(e)[V l,epre~(e) PAVOUT(P) 1 if (So(e) E %!) \nA (oCR(S o (e) ) ) 1 PAVOUT( e) = TRANSe[ANTLOC(e) u PAVIN(e)] (6) Figure 7: Data Flow Equations for \nAvailability and Partial Availability if Si(e) is END node ~,$acc(e) ( RNM2S0,($) [PPIN(s) ] ) if Si(e) \nE R i PPOUT(e) = (7) CMODsi(~)[~j,csu.c (e) PPIN(s ) ] if (Si(e) ES) A ( mOCR(Si (e) ) ) RNMlsi/(,)[A,~~UcC(e) \nppIN~s) ] if (Si(e) E S) A ( OCR(Si (e) ) ) { TEMP1(z) = &#38;cObeg(e) (ANTLOC(C) M TRANS. [ppOUT(c)] \n) TEMP2(2) = PPOUT(t) w AVOUT(z) if So(e) is BEGIN node #AVIN(e) A TEMP1(e) A if So(e) E$ ~pepred(e) \n( NM1Si (P) [TEMP2(P) 1) PPIN(e) = PAVIN(e) A TEMP1( e) A (8) ( (CMODSoJ( ,)[ &#38;cpred (e) TEMP2(p \n) ]) if (so(e) E l?) A ( 10CR(S o (e) ) ) PAVIN(e) A TEMPl(e) A i ( RNk~zso/(.)[Apepre,l(e) TEMP2(P) \n1)) if (So(e) E X?) A ( OCR (So (e) ) ) INSERT(e) = PPOUT(e) A l(PPOUT(e) A AVOUT(e)) A (9) (=(PPIN(e) \nA PPOUT(e)) V 7TRANS,[PPOUT(e)]) DEL(e) = ANTLOC(e) A PPIN(e) (lo) Figure 8: Data Flow Equations for \nPlacement 1 ( PPOUT(e) A AVOUT e)) will return T whenever of the call sites which call procedure p. Let \nF I?lN be PPOUT(e) and AVOUT(e \\ are not set to the same list Inj7i for any edge starting at call site \ncs and, further, of influencers Infli. let there be no modification to any of the influencers in In determining \nplacement (PPIN and PPOUT), we the call to procedure p. Then, no placement of the can\u00ad pr~serve the calling \ncontext of the procedures by using didate will be done tn any block of code inside call to a simple method, \nthe same that we used for computing procedure p. availability. It can be shown that the safety of place\u00adment \nis maintained through this method. Lemma 1 Conszder any procedure p such that the pro-The initial value \nof the PPIN and PPOUT are set to cedure p or any of the procedures invoked by it do not T. The desired \nsolution is the largest solution and can have any occurrence of the candidate C. Let cs be one be found \nby iterative method. 5.5 Final Local Placement The performance achieved by the compiled code (be- WTe \nhave so far considered the block of code associated with a single edge of FPR as the unit of placement. \nThe final placement of the candidates which have to be inserted depends upon further intraprocedural \nanalysis and is not necessarily at the end of blocks of code. Consider an edge e for which INSERT(e) \nis true. A number of edges end the same procedure return node or the same call site as the edge e and \nINSERT may not be true for all of them. Since all these edges have the same succ(e) and swcc (e) sets, \nthey have the same value of PPOUT e) and AVOUT(e). Therefore, the difference in the va \\ ue of INSERT(e) \ncomes because of the differ\u00adence in the value of PPIN(e) or TRANS. [PPOUT(e)]. For determining the final \nplacement, the control flow graph is traversed backwards from the call site cs or the procedure return \nstatement. Along any such traversal path, we identify the first basic block which belongs to the blocks \nof code for which INSERT is true but does not belong to the blocks of code for which INSERT is not true. \nLet bl be such a basic block and let b2 be its successor which belongs to the block of code for which \nINSERT is true. A new basic block is inserted between the basic blocks bl and b2 and the candidate is \ninserted in this new basic block. It can be shown that the fol\u00adlowing property is maintained by this \nscheme. Lemma 2 No tnsertton is made in any block of code for whleh INSERT ts 1. Using the above two \nlemmas, the correctness and safety properties of the interprocedural scheme can be established in the \nsame way as the correctness and safety of the original intraprocedural scheme [20]. Theorem 1 (Correctness) \nAfler insertion of new computations, the computation of the candtdate C be\u00adcomes redundant in an edge \nsatisfying ANTLOCC = Infli and PPINC = Infli. Theorem 2 (Safety) G onszder any edge of FPR An which a \nnew computation C M znserted. Every path starting from sink of thts edge ~ncludes a computation which \nwill be deleted, before tncludtng any edge m whtch a new occurrence of this computation will be added. \nThe solution of data flow properties for the program shown in Figure 2 is shown in Figure 9. The optimized \nprogram is shown in Figure 10. 6 Discussion 6.1 Effectiveness of the Scheme We have implemented a preliminary \nversion of our scheme using the existing Fortran D compilation sys\u00adtem developed at Rice University [17] \nas the neces\u00adsary infrastructure. We studied the effectiveness of our scheme in compiling an Euler solver \nfor unstructured grids [8], a code which accesses data through indirec\u00adtion arrays in parallel loops. \nThe existing compiler for irregular applications [9, 14] generated calls to PARTI routines for communication \npreprocessing and collec\u00ad tive communication [24], but did not perform any in\u00adterprocedural placement \nof these statements. fore interprocedural optimization) and the code af\u00adter interprocedural optimization \nis presented in Fig\u00adure 11. The experimental results show that interproce\u00addural placement of communication \npreprocessing state\u00adments is a must for obtaining reasonable performance. When the program is run on \na small number of proces\u00adsors, the communication cost is small and therefore, the performance difference \nmade by interprocedural place\u00adment of communication statements is small. However, when the same data \nis distributed over a large number of processors, the communication time becomes a sig\u00adnificant part \nof the total execution time of the program. Then, performing interprocedural placement of com\u00admunication \nstatements also makes a substantial differ\u00adence on the total execution time of the program. 6.2 Cost \nof the Scheme There are two issues in evaluating the cost of our scheme: the number of edges in the graph \nconstructed, and the number of iterations required for data flow equations to be solved. In the worst \ncase, each proce\u00addure may contribute edges quadratic in the number of statements in the program. In practice, \nwe expect this to be much smaller than the number of basic blocks in full program, e.g. the Euler code \nwe experimented with had 9 procedures, and a total of 1400 lines of code. The resulting graph had 16 \nedges, whereas the total number of basic blocks in the program was 117. In future, we plan to do this \nmeasurement for a number of different codes. The number of iterations required for data flow equa\u00adtions \nto converge is, in the worst case, proportional to the number of edges in the graph. If the number of \negdes in the graph is small, the time required for solu\u00ad tion will be small. 7 Related Work We are aware \nof two efforts on performing interproce\u00addural partial redundancy elimination. Morel and Ren\u00advoise briefly \ndiscuss how their scheme can be extended interprocedurally [21]. Their solution is hueristic in nature, \nand no formal details are available for their in\u00adterprocedural scheme. Their work 1s restricted to the \nprograms whose call graph is acyclic. They also do not consider the possibility that the procedure having \na candidate for placement may be invoked at multiple call sites with different set of parameters and \ndo not maintain accuracy of solutions when procedures are in\u00advoked at multiple call sites. Knoop et al. \nextend a scheme for performing ear\u00adliest possible code motion interprocedurally [18]. The main limitation \nof their work is that if any of the influ\u00adencers of a candidate is a formal parameter, then the candidate \n1s not considered for placement outside pro\u00adcedure boundary (since no renaming of influencers is done). \nIn the example presented in this paper, as well as in the Euler code we used for our experiments, their \nscheme will not perform any code motion. They do not consider the possibility of using any compact rep\u00adresentation \nfor the full program. Also, we believe that our effort is the first one to report an implementation and \napplication of interprocedural partial redundancy elimination. In our earlier work, we had outlined using \ninterprocedural partial redundancy elimination for dis\u00ad 266 Edge AVIN AVOUT PAVIN PAVOUT PPOUT PPIN \nDEL INSERT 1 -L -L 1 1 <alb> 1 1<alb> 2 <ajb> 1 <a, b> <a, b> <ajb> <ajb> -L 1 3 J-L -L 1 <a,c> -L J-<a,c> \n4 1 <xjy> <X, y> <X, y> <Z, y> <xly> <xzy> J\u00ad5 <a, b> <a, b> <LL, b> < (1,1>> <u%b> <a)b> 1-1\u00ad6 <U, c> \n<fl, c> <U, c> <a, c> -L <(L, c> J-1 7 <a, b> <a, b> <a, b> <ajb> <a, b> < (L,b> 1 1 81 1 _L 1 <a)c> \n-L 1<ajc> 911.-1-L 1L-1-L 10 <a, c> -1-<alc> <ajc> <a, c> <a)c> J-1 11 1 <yjz> <y, z> <y, z> <y,%> <yjz> \n<y)z> 1 12 <a, c> <a, c> <a, c> <alc> 1-<ajc> 1 1 Figure 9: Solution of Data Flow Properties for the \nGraph Program Foo Procedure P(x,y) a=l ..oiher computations . . Sched(a,b) End Doi = 1,100 Call P(a,b) \n Call Q(c) Procedure Q(z) Enddo z = . ..2... Call Q(a) End Sched(a,c) Call P(a,c) If cond then Call Q(c) \nProcedure R(y,z) Sched(a,c) . . other computations . . Endif End Call R(a,c) End Figure 10: O~timized \nVersion of Promam. Note that further Intraprocedural Analysis is required at call sites CS1 and CS6 to \nde~ermine final placement tributed memory compilation [1], but no formal details 8 Conclusions of the \nscheme or empirical evaluation was presented. In this paper we have addressed the problem of per-We compare \nour work with efforts on other flow\u00ad forming partial redundancy elimination interprocedu\u00adsensitive interprocedural \nproblems. Several different rally. This problem was initially motivated by the program representations \nhave been used for different problem of placement of communication preprocess\u00adflow-sensitive interprocedural \nproblems. Myer has sug\u00ad ing statements in distributed memory compilation. We gested concept of SuperGraph \n[22] which is constructed have developed an interprocedural partial redundancy by linking control flow \ngraphs of subroutines by insert\u00ad elimination (IPRE). Our algorithm is applicable on ar\u00ading edges from \ncall site in the caller to start node in bitrary recursive programs. callee. The total number of nodes \nin SuperGraph can Acknowledgements get very large and consequently the solution may take We thank Bill \nPugh for critically reading earlier ver\u00ad much longer time to converge. Several ideas in the dle\u00ad sions \nof this paper and suggesting several improve\u00ad sign of our representation are similar to the ideas used \nments to the scheme and its presentation. We have in Callahan s Program Summary Graph [6] and Inter\u00ad \nimplemented this scheme as a part of the D system be\u00ad procedural Flow Graph used by Soffa et al. [16]. \nFIAT ing developed under the leadership of Ken Kennedy at has been introduced as a general framework \nfor per- Rice university. We gratefully acknowledge our debt to forming interprocedural analysis [13], \nbut is more tar\u00ad the implementers of the interprocedural infrastructure geted towards flow-insensitive \nproblems. Interval based (FIAT) and the existing Fortran D compiler. approach for solving interprocedural \ndata flow equa\u00adtions has been investigated in (4]. Recompilation in a References compiler performing \ninterprocedural analysis has been investigated in [5]. [1] Gagan Agrawal and Joel Saltz. Interprocedural \ncom\u00ad Euler Solver on 10K mesh: 20 iterations No. Oi No 1P opt. I P opt. M opt. Procs. preproc. stints. \ncomm. stint. 2 47.74 4 26.77 8 17.45  16 12.35 32 12.72 Figure 11: Effectiveness of Interprocedural \nplacement munication optimizations for distributed memory com\u00adpilation. In Proceedings of the 7th Workshop \non Lan\u00adguages and Compilers for Parallel Computing, pages 283 299, August 1994. Also available as University \nof Maryland Technical Report CH-TR-3264. [2] Ga,gan Agrawal, Alan Sussman, and Joel Saltz. Conl\u00adpiler \nand runtime support for structured and block structured applications. In Proceedings Supercomput\u00adtng \n93, pages 578 587. IEEE Computer Society Press, November 1993. [3] Gagan Agrawal, Alan Sussman, and Joel \nSaltz. An in\u00adtegrated runtime and compile-time approach for paral\u00adlelizing structured and block structured \napplications. IEEE Transactions on Parallel and Distributed SyS \u00adtems, 1994. To appear. Also available \nas Univer\u00adsity of Maryland Technical Report CS-TR-3143 and UMIACS-TR-93-94. [4] Michael Burke. An interval-based \napproach to exhaus\u00adtive and incremental iuterprocedural data-flow anal\u00adysis, ACM Transactions on Programming \nLanguages and Systems, 12(3):341 395, JLdy 1990. [5] Michael Burke and Linda Torczon. Interprocedural \noptimization: Eliminating unnecessary recompilation. ACM Transactions on Programming Languages and Systems, \n15(3):367-399, July 1993. [6] D. Callahan. The program summary graph and flow\u00adsensitive interprocedural \ndata flow analysis. In Pro\u00adceedings of the SIGPLA N 88 Conference on Program Language Design and Implementation, \nAtlanta, GA, June 1988. [7] K. Cooper and K. Kennedy. Interprocedural side-effect analysis in linear \ntime. In Proceedings of the SIGPL.4N 88 Conference on Program Language Deszgn ancl Im\u00adplementation, Atlanta, \nGA, June 1988. [8] R. Das, D. J. Mavriplis, J. Salt., S. Gupta, and R. Pon\u00adnusamy. The design and implementation \nof a paral\u00adlel unstructured Euler solver using software primitives. AIAA Journal, 32(3):489-496, March \n1994. [9] Raja Das, Joel Saltz, and Reinhard von Hanxleden. Slicing analysis and indirect access to distributed \nar\u00adrays. In Proceedings of the 6th Workshop on Languages and Cornptlers for Parallel Computing, pages \n152 168. Springer-Verlag, August 1993. Also available as Uni\u00adversity of Maryland Technical Report CS-TR-3076 \nand UMIACS-TR-93-42. 30.26 14.99 9.85 7.41 8.92 schemes. [10] [11] [lq [13] [14] [15] [16] [17] [18] \n29.75 14.27 8.51 5.56 5.09 All numbers are in Seconds, on Intel Paragon D.M. Dhamdhere and H. Patil. \nAn elimination algo\u00adrithm for bidirectional data flow problems using edge placement. ACM Transactions \non Programming Lan\u00adguages and Systems, 15(2):312-336, April 1993. K. Drechsler and M. Stadel. A solution \nto a prob\u00adlem with Morel and Renvoise s Global optimiza\u00adtion by suppression of partial redundancies . \nACM  Transucttons on Progrommmg Languages and Systems, 10(4):635-640, October 1988. Manish Gupta, Edith \nSchonberg, and Harini Srini\u00ad vasan. A unified data flow framework for optimiz\u00ad ing communication. In \nProceedings of Languages and Compilers for Parallel Computing, August 1994. Mary Hall, John M Mellor \nCrummey, Alan Carle, and Rene G Rodrignez. FIAT: A framework for interpro\u00adcedural analysis and transformations. \nIn Proceedings of the 6th WorLxhop on Languages and Compilers for Parallel Con2put2ng, pages 522 545. \nSpringer-Verlag, August 1993. Reinhard v. Hanxleden. Handling irregular problems with Fortran D -a preliminary \nreport. In Proceed\u00adings of the Fourth Workshop on Compders for Parallel Computers, Delft, The Netherlands, \nDecember 1993. Also available as CRPC Technical Report CRPC\u00adTR93339-S. Reinhard von Hanxleden and Ken \nKennedy. Give-n\u00adtake a balanced code placement framework. In Pro\u00adceedings of the SIGPLAN 94 Conference \non Program\u00adming Language Design and Implementationj pages 107-120. ACM Press, June 1994. ACM SIGPLAN \nNo\u00adtices, I ol. 29, No. 6. Mary Jean Harroid and Mary LOU Soffa. Efficient com\u00adputation of interprocedural \ndefinition-use chains. ACM Transactions on Programming Languages and Systems, 16(2):175-204, March 1994. \nSeems Hiranandani, Ken Kennedy, and Chau-Wen Tseng. Compiling Fortran D for MIMD distributed\u00ad memory \nmachines. Communications of the ACM, 35(8):66-80, August 1992. J. Knoop and Steffan B. Efficient interprocedural \nbit\u00advector data flow analyses: A uniform interprocedural framework. Technical report, Dept. of Computer \nSci\u00adence, University of I(iel, September 1993. 268 [19] J. Knoop, O. Riithing, and B. Steffen. Lazy \ncode mo\u00adtion. In Proceedings of the ACM SIGPLA N 92 (Con\u00ad jerence on Program Language Design and Implement\u00adation, \nSan Francisco, CA, June 1992. [20] E. Morel and C. Renvoise. Global optimizatiorl by suppression of partial \nredundancies. Contrnunlcatzons of the ACM, 22(2):96 103, February 1979. [21] E. Morel and C. Renvoise. \nInterprocedural elimina\u00adtion of partial redundancies. In Program Flow A nol\u00ad ysis: Theory and Applications. \nPrentice Hall, Engle\u00ad wood Cliffs, NJ, 1981, [22] E. Myers, A precise interprocedural data flow aJgo\u00adrithm. \nIn Conference Record of the Eighth ACM Symp\u00ad osium on the Principles of Programming Languages, pages \n219-230, January 1981, [23] Thomas Reps, Snsan Horowitz, and Mooly Sagiv. Pre\u00ad cise interprocedural dataflow \nanalysis via graph reach\u00ad ability. In Conference Record of the Fourteenth Annual ACM SIGA CT/SIGPLAiV \nSymposium on Princ~~ples of Programming Languages, January 199.5. [24] Joel Saltz, Kathleen Crowley, \nRavi Mirchandaney, and Harry Berryman. Run-time scheduling and execution of loops on message passing \nmachines. Journal of ,Par\u00ad allel and Distributed Computing, 8(4):303 312, A,pril 1990. [25] A. Sorkin. \nSome comments on (A solution to a problem with Morel and Renvoise s Global optimiza\u00ad tion by suppression \nof partial redundancies . ACM Transactions on Programming Languages and Systems, 11(4):666-668, October \n1989. Appendix A.1 Intraprocedural Partial Redundancy Elimination The first partial redundancy elimination \nscheme was presented by Morel and Renvoise [20]. This scheme has been further extended and refined by \nDhamdhere 10, Drechsler [11], Knoop et. al, [19] and Sorkin 25. The details of interprocedural redundancy \nelimination we present are derived from the intraprocedural node II based method of Dhamdhere [10], \nalso referred to as Modified Morel Renvoise Algorithm (MMRA). The data flow equations used in the scheme \nare presented in Figure 12, The terms used in the data flow equations are explained below. Local data \nflow properties: ANTLOC(Z): Node i contains an occurrence of com\u00ad putation C not preceded by a definition \nof any of its operands. COMP(Z): Node i contains an occurrence of compu\u00adtation C not followed by a definition \nof any of its operands. TRANS( z): Node i does not contain a definition of any operand of computation \nC, Global data flow properties: AVIN(i)/AVOUT(i): Computation C is available at the entry/exit of node \ni. PAVIN(i)/PAVOUT( t): Computation C is partially available at the entryjexit of node i. PPIN(i)/PPOUT(i): \nComputation of C may be placed at entry lexit of node i. false if z is entry block AVIN(i) = { l-l P~Pre~(t)AvOUT(p) \notherwise AVOUT(i) = COMP(Z) + TRANS(i).AVIN(i)) false if i is entry blockPAVIN(i) = ~PcP,.e~[,)PAVOUT(P) \notherwise{ PAVOU I (Z) = COMP(i) + TRANS(Z),PAVIN(Z) PPIN(z) = PAVIN(Z). ( ANTLOC(Z) + TRANS(i) . PPOUT(Z)) \n~Pept..~(,l(ppOUT(P) + AVOUT(P)). false if i is exit block PPOUT(Z) = I-I ~e,UCC(,)PPIN(s)) otherwise \n{ INSERT(Z) = PPOUT(Z) =AVOUT(i). (-PPIN(i) + mTRANS(i)) DEL(i) = ANTLOC(Z),PPIN(i) Figure 12: MMRA \nscheme for Intraprocedural Partial Redundancy Elimination DEL(z): Occurrence of C in node i is redundant \nINSERT( z : A computation of C should be placecl at the exit o ) node i. We now briefly explain the rational \nbehind the key equations, A computation is available at the entry of a basic block if it is available \nat the exit of all the prede\u00adcessor basic blocks. A computation is available at the end of a basic block \nif it is available at the beginning of the basic block and none of the operands are mod\u00adified in the \nbasic block, or, alternatively, there is an occurrence of this computation in this basic block, not followed \nby any definition of the operands. A compu\u00adtation is partially available at the entry of a basic block \nif it is partially available at the exit of at least one pre\u00addecessor block. The equations for placement \ncan be ex\u00adplained as follows. In computing PPOUT, the ~ term ensures safety in placing an expression \nat the exit of the node. The ~ term in computing PPIN ensures avail\u00adability of the expression at the \nentry of this node in the optimized program, The term PAVIN determines the profitability of hoisting \na computation out of this node, This term avoids redundant code hoisting for al\u00admost all cases for any \nreal program, however, it does not guarantee. In the original MMRA scheme [10], an additional term is \nused to further prevent redundant code hoisting, this term still does not guarantee that no redundant \ncode hoisting occurs. For simplicity, we C1Onot include this additional term in our presentation. \n\t\t\t", "proc_id": "207110", "abstract": "<p>Partial Redundancy Elimination (PRE) is a general scheme for suppressing partial redundancies which encompasses traditional optimizations like loop invariant code motion and redundant code elimination. In this paper we address the problem of performing this optimization interprocedurally. We use interprocedural partial redundancy elimination for placement of communication and communication preprocessing statements while compiling for distributed memory parallel machines.</p>", "authors": [{"name": "Gagan Agrawal", "author_profile_id": "81100288827", "affiliation": "UMIACS and Department of Computer Science, University of Maryland, College Park, MD", "person_id": "P93478", "email_address": "", "orcid_id": ""}, {"name": "Joel Saltz", "author_profile_id": "81100475190", "affiliation": "UMIACS and Department of Computer Science, University of Maryland, College Park, MD", "person_id": "PP17001496", "email_address": "", "orcid_id": ""}, {"name": "Raja Das", "author_profile_id": "81100143228", "affiliation": "UMIACS and Department of Computer Science, University of Maryland, College Park, MD", "person_id": "PP14060491", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/207110.207157", "year": "1995", "article_id": "207157", "conference": "PLDI", "title": "Interprocedural partial redundancy elimination and its application to distributed memory compilation", "url": "http://dl.acm.org/citation.cfm?id=207157"}