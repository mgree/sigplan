{"article_publication_date": "06-01-1995", "fulltext": "\n Selective Specialization for Object-Oriented Languages Jeffrey Dean, Craig Chambers, and David Grove \nDepartment of Computer Science and Engineering University of Washington Seattle, WA 98195 ~dean,chambers, \ngrove}@cs.Washington.edu Abstract Dynamic dispatching is a major source of run-time overhead in object-oriented \nlanguages, due both to the direct cost of method lookup and to the redirect effect of preventing other \noptimization. To reduce this overhead, optimizing compilers for object-oriented languages anrdyze the \nclasses of objects stored in program variables, with the goal of bounding the possible classes of message \nreceivers enough so that the compiler can uniquely determine the target of a message send at compile \ntime and replace the message send with a direct procedure call. Specialization is one important technique \nfor improving the precision of this static class information: by compelling multiple versions of a method, \neach applicable to a subset of the possible argument classes of the method, more precise static information \nabout the classes of the method s arguments is obtained. Previous specialization strategies have not \nbeen selective about where this technique is applied, and therefore tended to significantly increase \ncompile time and code space usage, particularly for large applications. In this paper, we present a more \ngenertd framework. for speciahzation in object-oriented languages and describe a goal\u00addirected specialization \nrdgorithm that makes selective decisions to apply specialization to those cases where it provides the \nhighest benefit. Our results show that our algorithm improves the performance of a group of sizeable \nprograms by 657. to 275% while increasing compiled code space requirements by only 4~o to 10%. Moreover, \nwhen compared to the previous state-of-the-art specialization scheme, our algorithm improves performance \nby 11YO to 67% while simultaneously reducing code space requirements by 65% to 73%.  Introduction Dynamic \ndispatching is a serious run-time performance bottleneck for programs written in an object-oriented style. \nThe expense of dynamically-dispatched calls (also known as virtual function calls or message sends) arises \nboth from the direct cost of performing method lookup and from the indirect opportunity cost of the loss \nof other optimization, such as inlining and interprocedural analysis. To 2woid these costs, optimizing \ncompilers for object-oriented languages strive to statically bind as many message sends to called methods \nas possible, Static binding requires information about the possible classes that could be stored in each \nprogram variable, so that the set of invocable methods can be determined if only one method c;m be invoked \nand no method lookup error is possible, the send can be statically bound, i.e., replaced with a dhect \nprocedure call to the target method. Once statically bound, the call becomes amenable to further optimizations \nsuch as inline expansion. Permission to copy without fee all or part of this material is granted provided \nthat the copies are not made or distributed for direct commercial advantage, the ACM copyright notice \nand the title of the publication and its date appear, and notice is given that copying is by permission \nof the Association of Computing Machinery.To copy otherwise, or to republish, requires a fee and/or specific \npermission. SIGPLAN 95La Jolla, CA USA (3 1995 ACM 0-89791 -697-2/95/0006 ...$3.50 One technique for \nimproving the precision of class information, and indirectly to support more static binding, is to compile \nmultiple specialized versions of a method, each applicable to only a subset of the possible argument \nclasses of the method. By choosing the specializing subsets judiciously, many message sends can be statically \nbound that could not be in the unspecialized version. Some compilers for object-oriented languages implement \na restricted form of specialization called customization, in which a specialized version of a method \nis compiled for each possible receiver class, and methods are never specialized for arguments other than \nthe receiver [Chambers et al. 89]. Although this strategy yields substantial performance improvements, \nit can also lead to a substantial increase in code size, especially for large programs with deep inheritance \nhierarchies and a large number of methods. In this paper, we present a goal-directed algorithm that combines \ndynamic profile data with static information about where in the class hierarchy methods are defined to \nidentify profitable specializations. The algorithm also is suitable for use in object-oriented languages \nwith multi-methods, unlike customization. Our results show that the algorithm simultaneously increases \nprogram performance and decreases the compiIed code space requirements over several alternative compilation \nand specialization strategies, including customization. The next section prowdes an example motivating \nwhy specialization is an important optimization technique and discusses why existing specialization techniques \nare inadequate. Section 3 describes our algorithm for selective specialization, Section 4 provides performance \nresults for our algorithm, comparing it to a variety of alternative schemes, and Section 5 discusses \nrelated work. 2 A Motivating Example To illustrate the issues involved in specialization, Figure 1 presents \nan example of a Set class h~erarchy, with different subclasses for different set representations. The \nabstract base class provides operations to perform generic set operations such as union, intersection, \nand overlaps testing,, and relies on dynamically\u00addispatched do and includes messages, with versions of \nthese operations implemented in each subclass. Although writing the code in this fashion, where many \noperations are factored into the abstract Set class, offers software engineering advantages, it leads \nto inefficient code if compiled naively. Compiling a single version of the overlaps method that is general \nenough to apply to all possible Set representations results in code that has substantial dynamic dispatching \noverhead. In such code, the * In our syntax, k (argl: type ) { .. code ) is a closnre that accepts oneargument,and1.(type) \n:type ISastatictypedeclarationforsucha closure. se 1 f is the name of the recewer of a method. Other \narguments of the form Class: : arg represent methods that are dispatched on multiple arguments (multi-methods). \nAlternatively, in a singly-dispatched language, they could be written in a double-dispatching style [Ingatls \n86], Dynamically-dispatched messagesendsareshown in this font. class Set [T] { method overlaps (set2:Set[T]) \n:bool { self.do(k(elem:T) { if set2. ineludes(elem) then return true; )) ; return false; } method includes \n(elem:T) :bool { --A default includes implementation: subclasses can override to provide a more ej?cient \nimplementation self.do(k(e2:T){ i-f elem.egual(e2) then return true; }); return false; } .. otherset \noperationssuchas intersection, union, etc. ... } class ListSet[T] subclasses Set[T] ( method do(body:A(T) \n:void) :void { ,4. code toiterate over elementsof t,evaluating closure body on each element ... } } class \nHashSet[T] subclasses Set[T] { method do(body:k(T) :void) :void { . . . } method includes (elem:T) :bool \n{ --Amoreejjicient implementationof includes:hashthe elementandseeif itisinthatbucket class BitSet[T] \nsubclasses Set[T] { method do(body:k(T) :void) :void { . . . } method kcludes(elem:T) :bool { --A more \nefllcient implementation of inc ludes: test the appropriate bit positcon } ... Provide more efficientversions \nofoverlaps, union, intersection, etc. when oper-sting ontwoBitSets method overlaps (Bi.tSet[T] ::set2) \n:bool { . . . ) } Figure 1: A set abstraction hierarchy self.do(k(elem) {. . .}) message must be dynamically \ndispatched, since which implementation of do will be invoked cannot be determined statically and in fact \ncan vary at run-time. Furthermore, within the closure, tbe send of set2. includes(elem) must also be \ndynamically dispatched, since there is insufficient static information about the class ofs e t 2 to enable \nstatic binding to a single includes method. The direct cost of performing the dynamic dispatching is \nsubstantial, but having the messages be dynamically dispatched also prevents other optimization, such \nas inlining, which often has an even greater effect on code quality. For example, because the do message \nsend was not able to be statically-bound and inlined, the closure argament to do must recreated atrtrn-time \nand invoked as aseparate procedure for each iteration. Customization is a simple specialization scheme \nused in the implementations of some object-oriented langaages, including Self [Chambers &#38; Ungar 89, \nHolzle &#38; Ungar 94]. Sather [Lim &#38; Stolcke 91], and Trellis [Kilian 88]: aspecialized version \nofeach method is compiled for each class inheriting the method. Within the customized version of a method, \nthe exact class of the receiver is known, enabling the compiler to statically bind messages sent to the \nreceiver formal parameter (self). In our example, a specialized version of overlaps would be compiled \nforListSet andfIashSet, and this wouldallowthe self.clo(k (elm){...}) tobestatically\u00adbound totheappropriate \nimplementation in each of these versions (BitSet prowdes an overriding version of overlaps, and therefore \nuses this method definition for customization purposes). For example, in the ListSet version of overlaps, \nthe self.do(l(elem) {. . .}) message cangetinlined down toa simple while loop. Other operations defined \non Set, such as union and intersection, would be similarly customized. Because sends to self tend to \nbe fairly common in object-oriented programs, customization is effective at mcreasmg execution performance: \nSelf code runs 1.5 to 5 times faster as a result of customization, and customization was one of the single \nmost lmportrmt optimization included in the Self compiler [Chambers 92], Lea hand-simulated customization \nin C++ fora Matrix class hierarchy, showing an order-of-magnitude speedup, and argaed for the reclusion \nof customization in C++ implementations [Lea 90], Unfortunately, this simplestrategyfor specialization \nsuffers fromthe twin problems of overspecialization and underspecialization, because specialization is \ndone without regard for its costs and benefits. In many cases, mtrltlple specialized versions of a method \nare virtually idenfiical and could be coalesced without a sigmficant impact on program performance. For \nlarge programs with deep inheritance hierarchies and many methods, producing a specialized vers]on of \nevery method for every potential receiver class leads to serious code exploslon. In the presence of large, \nreusable libraries, we expect applications to use only a subset of the avarlable classes and operations, \nand some of those only infrequently, and consequently simple custornzatlon is likely to be impractical. \nIn systems employing dynamic compilation, such as the Self system [Chambers &#38; Ungar 89], customization \ncan be done lazily by delaying the creation of a specdized version of a method until the particular speciahzed \ninstance is actually needed at runtime, if at all. This strategy avoids generating code for class x method \ncombinations that are never used, but such systems can still have problems with overspecialization if \na method is invoked with a large number of distinct receiver classes during a program s execution or \nif a method is invoked only rarely for particuhw receiver classes. Moreover, dynamic compilation may \nnot be a suitable framework for some programming systems because of the need for an optimizing compiler \nand some representation of the program source to be included in the program s runtime system. In addition \nto overspecialization, the simple customization appro,ach suffers from underspecialization, because methods \nare never specialized on arguments other than the receiver. In many cases, considerable benefits can \narise from specializing a method for particular argument classes. In our example, specializing the overlaps \nmethod for a particular class of the set2 argument cotrldhave considerable performance benefits, since \nit would allow the s et2 . includes ( elem) message inside the loop to be statically-bound and potentially \ninlined. By specializing on an argument other than the receiver, clients of overlaps will have to be \ncompiled to choose the appropriate specialized version of over laps, perhaps by inserting additional \nclass tests or table lookups at run-time, but such tests occur only once per overlaps operation. Compared \nto incurring a dynamic dispatch for every element of the set, this overhead to select the appropriate \nspecialization can be minor. In effect, specialization can hoist dynamic dispatches from frequently executed \nsections of code to less frequently executed sections, often across multiple procedure boundaries. Selective \nSpecialization To address the problems of overspecialization and underspecialization, our work focuses \non techniques for specializing only where the benefits are large. Rather than specializing exhaustively, \nour algorithm exploits dynamic profile data to specialize heavily-used methods for their most beneficial \nargulment classes, preserving a single, general-purpose version of the method that works for the remaining \ncases. Moreover, our algor~thm identifies when a specialization can be shared by a group of classes without \ngreat loss in performance, further reducing code space costs. Our algorithm is based on a general framework \nin which a method can be specialized for a tuple of class sets, one class set per fcrrnrd argument, including \nthe receiver. For example, in the Set example we would describe one potential specialization of the overlaps \nmethod using the tuple < {ListSet, HashSet} , {HashSet} >. An argument class set can include multiple \nclasses when a specialization is shared by several classes. This framework has two advantages over previous \nschemes: Methods can be specialized to obtain more precise information about any formal argument, not \njust the receiver argument as with customization.  Multiple classes can share a single specialized method, \nrather than generating a specialized copy for each individual class.  However, this general model places \nfew constraints on specialization, requiring guidance in order to determine which potential specializations \nare profitable. Our algorithm relies on two sources of information to guide the specialization process: \nWe exploit information about the classes and methods defined in the program to identify groups of classes \nthat still allow static binding of message. e We use gpro f -style profile information to select the \nmost important specializations in program hot spots. The algorithm will be illustrated with the class \nhierarchy and method definitions shown in Figure 2 and the weighted call graph AlXt2 ( ) B::m2 ( ) ,. \nC ISF G &#38; I J method method A: :m3(arg2:A) A::m4(arg2:A) { self. {self m4(arg2) ; .~(); ar92. } \nn&#38;20; } Figure 2: Example class hierarchy + Specializable call arc - -+ Non-specializable call arc \n. Dynamically-dispatched cntl site o Statically-bound call site Figure 3: Example call graph shown in \nFigure 3. The class hierarchy consists of nine classes, with methodmo implemented onlyinclasses A, E,and \nG,methodm20 implemented only in classes A and B, and methods m3 and m4 implemented only in class A. The \nshaded regions show the equivalence classes that all invoke the same implementation of m andm2 (note \nthat theillustrations in Figure 2are two views of the same underlying inheritance hierarchy). Thealgonthmis \nshown in Figure 1. Set operations ontnples are defined to operate pairwiseon thetuple elements. The algorithm \nrelies on the presence of a weighted program call graph constructed from the profile data that describes, \nfor each call site in the program, the set of methods invoked and the number of times each was invoked \n(a call site can have multiple arcs due to dynamic dispatching) .Givenan arcinthecrdl graph, Caller(arc) \ngives the calling method, Callee(arc) gives the called method, CallSite(arc) identifies the message send \nsite within the caller, and Weight(arc) gives the execution count of the arc. For the arc labelled tx \nin Figure 3, Cal~er(tx) is A: :m4, Call@e(rX)is B: :m2, CallSite(@ is the send arg2 . m2 ( ) within m4, \nand Weight(a) is .550. The algorithm exploits information about the class hierarchy through the Applicable \nClasses function. ApplicableClasses~eth] returns the tuple of the set of classes for each formal argument \nfor which the method rneth could be invoked (excluding classes that bind to overriding methods). The \nshaded eqtuvalence classes regions in the example above identify the ApplicableClasses for each of the \nm and m2 methods. For example, ApplicableClasses[me thod E: : m ( ) I= <{ E,H,I )>. For singly dispatched \nlanguages, computing ApplicableClasses for each method is fairIy straightforward. For multiply.dispatchec[ \nlanguages, there are some subtleties in performing this computation efficiently; details of how this \ncan be done can be found elsewhere [Dean et al. 95]. Profile info: Caller(a), Catlee(a), CallSite(a), \nand Weight(a) give caller. callee, call site, and execution count for arc a. in call graph. Source info: \nApplicableClas$esume thod m ( f ~, f ~, ... . fn) n= n-tuple of sets of classes for f ~, f ~, ... . fn \nfor which m might be invoked Pas~ThroughArgs[ msgarg~, .... argn) inmethod m(f1,f2, ....fm) 1={<fi~~+ \napes>If~o,~ =ar9~p.., ( argl, ] %. ~~~ff~r~%~[fz. f oo (x, fl ) ir~ method m ( f 1, f 2 ) ~= {<1+3 >s2+1>] \n(the receiver is argument 1) Input: SpecializationThreshold, the minrmum Weight(arc) for an arc to be \nconsidered for specialization output: specializationsme~h: set of tuples of sets of classes for which \nmethod m should be speciahzed specializePi-ogramo = foreach method meth do Specializationsmeth:= ApplicableClasses \n~meth~; foreach method meth do speciahzeMethod(meth); specializeMethod(meth) = foreach arc s.t. Caller(arc) \n= meth and isSpecializableArc(arc) do if Weight(arc) > SpecializationThreshold then addSpecialization(meth, \nneededInfoForArc(arc)); isSpecializableArc(arc) returns bool = return PassThroughArgs [ CallSite(arc)l \n+ @ and Applicable Classes [ Caller(arc)] # neededInfoForArc(arc); neededInfoForArc(arc) returns Tuple[Set[Class]] \n= return neededInfoForArc(arc, ApplicableClasses~ Callee(arc)~ ), neededInfoForArc(arc, calleeInfo) returns \nTuple[Set[Class]] = needed:= ApplicableClasses~ Caller(arc)]; foreach <&#38;os-+ apes> G PassThroughArgs[ \nCallSite(arc)] do neededhc)$ := needed fi<),Yn calleeInfoaP<,.7; return needed  addSpecialization(meth, \nspecTuple) = foreach e.ristingspec G Specializationsmeth do if specTuple n e.xistingSpec # @ then $?t?Ch&#38;@hsme~h:= \n$?ecdizatlons~.~h V (existingSpec n specTuple); foreach arc s.t. CalLee(arc) = meth do cascadeSpecializations(arc, \nspec); cascadeSpecializations(arc, calleeSpec) = if PassThroaghArgs[ CallSite(arc)] # 0 and Applicable \nClasses [ Caller(arc)] = neededInfoFo?A vc(arc) and Welght(arc) > SpecializationThreshold then callerSpec \n:= neededInfoForArc (arc, calleeSpec); .. . if callerSpec # @ and callerSpec ~ Speclallzaaonscalle, \n(urc) then addSpecialization (CaGler(arc), callerSpecj; Figure 4: Selective specialization algorithm \n96 The goal of the algorithm is to eliminate dynamic dispatches for calls from some method by specializing \nthat method for particular classes of its formal parameters. By providing more precise information about \nthe classes of a method s formals, the algorithm attempts to make more static information available to \ndynamically-dispatc hed call sites within the method to enable the call sites to be statically bound \nin the specialized version. The simplest case in which specializing a method s formal can provide better \ninformation about a call site s actual occurs when the formal is passed directly as an actual parameter \nin the call; we call such a call site a pass\u00addwough call site (a similar notion is found in the jump \nfunctions of Grove and Torczon [Grove &#38; Torczon 93]). Our algorithm focuses on pass-through dynamically-dispatched \ncall sites, which we term specializable call sites, as the sites that can potentially benefit from specialization. \nFor the overlaps example, the sends of do and includes are specializable call sites, since obtaining \nmore precise reformation about the classes of se 1 f and set 2 can enable static binding of these message \nsends. In summary, our algorithm visiti each method in the call graph. The algorithm searches for high-weight, \ndynamically-dispatched, pass\u00adthrough calls from the method, i.e., those call arcs that are both possible \nand profitable to optimize through specialization of the enclosing method. The algorithm computes the \ngreatest subset of classes of the method s formals that would support static binding of the call arc, \nand creates a corresponding specialization of the method if such a subset of classes exists. Much of \nthe remainder of this section examines key aspects of the algorithm in more detail, focusing on the following \nissues: How is the set of classes that enable specialization of a calll arc computed? This is computed \nby the neededInfoForArc function, as discussed in Section 3.1. How should specializations for multiple \ncall sites in the same method be combined? This is handled by the addSpecialization routine and is discussed \nin Section 3,2. If a method m is specialized, how can we avoid converting statically-bound calls to \nm into dynamically-bound calls? Cascading specializations upwards (the cascadeSpecializations routine) \ncan solve the problem in many cases, and is discussed in Section 3,3. When is an arc important to specialize? \nThe algorithm currently uses a very simple heuristic, and Section 3,4 discusses the tradeoffs involved. \n At run-time, how is the right specialized version chosen? This is discussed in Section 3.5. What is \nthe interaction between the specialization algorithm and first-class nested functions? Section 3,6 considers \nthis question. This section concludes with a discussion of some related issues: How does the need to \nexamine the whole class hierarchy interact with separate compilation? How expensive is gathering and \nmanaging the necessary profile information? Can specialization be adapted to work in a system based \non dynamic compilation? 3.1 Computing Specializations for Call Sites The algorithm visits each high-weight \npass-through arc leaving a method. For each such arc, it determines the most general class set tuple \nfor the paw-through formals that would allow static binding of the call arc to the callee method. This \ninformation is computed by the neededInfoForArc function, which maps the Applicable Classes for the callee \nroutine back to the caller s formals using the ma)pping contained in the PassZ hroughArgs for the call \nsit~ if no specialization is possible, then the caller s ApplicableClasses is returned unchanged. As \nan example, consider arc et from the call graph in Flgore 3. For this arc, the caller s Applicable Classes \nis <{ A, B,,.. , J], {A, B,..., J }>, the callee s ApplicableClasses tuple is <{ B, E,H,I }>, and the \nPassTh-oug/rArgs mapping for the call site is (<2-+1>), so neededhrfoForArc(~) is <{ A,B,..., J), {B,E,H,I \n}>. This means that within the specialized version, the possible classes of arg2 are restricted to be \nin { B,E,H, I }; this information is sufficient to statically-bind the message send of m2 to B: : m2 \n( ) within the specialized version. The rseededhzfoForA rc(~) tuple will be added to the set of specializations \nfor m4 (Specializations. 3.2 Combining Specializations for Distinct Call Sites Different call arcs within \na single method may generate different class set tuples for specialization. These different tuples need \nto be combined somehow into one or more specialization tuples for the method as a whole. Deciding how \nto combine specializations for different call sites in the same method N a difficult problem, Ideally, \nthe combined method specialization(s) would cover the combinations of method arguments that are most \ncommon and that lead to the best specialization, but it is impossible, in general, to examine only the \narc counts in the call graph and determine what argument tuples the enclosing method was called with. \nIn our example, we can determine that within m4, the class ofs e 1 f was in {A,B,c,D,F} 625 times and \nin {E,H,T } 375 times, and that the class of arg2 was in {B, E,H, I } 550 times and in {A,c,D,F,G,J) \n450 times, but we cannot tell in what combinations the argument classes appeared. Because we want to \nbe sure to produce specializations that help the high-benefit call arcs, our algorithm covers all the \nbases, producing method specializations for all plausible combinations of arc specializations. This is \nthe function of the addSpecialization function: given a new arc specialization tuple, it forms the combination \nof this new tuple with rdI previously-computed speciahzation tuples (including the initial unspecialized \ntuple) and adds these new tuples to the set of specializations for the method. For example, given two \nmethod specmlization class set tuples <Al, . . ., An> and <A1nB1, . . ., AnnBn>, adding a new arc specialization \ntuple <Cl, . . . , Cn> leads to four method specialization tuples for the method: <Al, . . ., An>, <A1nB1 \n, . . . , A@Bn~, <A1nC1 , . . . , A#Wn>, and <A1nB1n C1 , . . ., A@ BJK fl> (assuming none of these intersections \nare empty: tuples containing empty class sets are dropped). For the example in Figures 2 and 3, nine \nversions of m4 would be produced, including the original unspecialized version, assuming that all four \noutgoing call arcs were above threshold. Although this approach can in principle produce a number of \nspecializations for a particular method that is exponential in the number of speciahzable call arcs emanating \nfrom the method, we have not observed this behavior in practice. For the benchmarks described in Section \n4, we have observed an average of 1.9 specializahons per method receiving any specializations, with a \nmaximum of 8 specializations for one method. We suspect that in practice we do not observe exponential \nblow-up because most call sites have only one or two high-weight specializable call arcs and because \nmethods tend to have a small number of formal arguments that are highly polymorphic. Were exponentird \nblow-up to become a problem, the profile information could be extended to maintain a set of tuples of \nclasses of the actual parameters passed to each method during the profiling run. Given a set of potential \nspecializations, the set of actual tuples encountered during the profiling run could be used to see which \nof the specializations would actually be invoked with high frequency. Of course, it is likely to be more \nexpensive to gather profiles of argument triples than to gather simple catl arc and count information. \n3.3 Cascading Specializations space during specialization. Alternatively, the algorithm could be Before \na method is specialized, some of its callers might have been able to statically-bind to the method. When \nspecialized versions of a method are introduced, however, it is possible that the static information \nat the call site will not be sufficient to select the appropriate speclafization. This is the case with \nthe arc from m3 to m4 in the example: m3 had static information that both its arguments were descendants \nof class A, which was sufficient to statically-bind to m4 when there was only a single version of m4, \nbut not after mulhple versions of m4 are produced, In such a case, there are two choices: these statically-bound \ncalls could be left unchanged, having them call the general-purpose version of the routine, or the statically-bound \ncall could be replaced with a dynamically-bound call that selects the appropriate specialization at run-time. \nThe right choice depends on the amount of optimization garnered through specialization of the callee \nrelative to the increased cost of dynamically dispatching the call to the speciahzed method. In some \ncases, this conversion of statically-bound calls into dynamically-dispatched sends can be avoided by \nspecializing the calling routine to match the specialized callee method. Tlm is the purpose of the cascadeSpecializations \nfunction. Given a specialization of a method, it attempts to specialize statically-bound pass-through \ncallers of the method to provide the caller with sufficient information to statically-bind to the speciahzed \nversion of the method. cascadeSpecializations first checks to make sure the call arc was statically bound \n(with respect to the pass-through arguments) and of high weight; if the call arc is dynamrcafly bound, \nthen regular specialization through speciulizedMethod will attempt to optimize that arc. For example, \nwhen specializing the m4 method for the tuple <{ A,B,C,D,F), {A, C,D,F,G,J)>, the arc from the m3 method \nis identified as a target for cascaded specmlization, but none of the other three callers are. If the \ncalling arc passes this first test, cascadeSpecializations computes the class set tuple for which the \ncafler should be specialized in order to support static binding of the cafl arc to the speciahzed version. \nFor this example, the computed class set tuple for m3 (callerSpec in the algorithm) lS <{ A, C,D,F ], \n{A,c,D,F}>. If the afgorithm determines that the cafl site can call the specialized version, and that \nspecialization of the caller is necessarY to enable static binding, the afgorithm recursively specmhzes \nthe caller method [if that specialization hasn t already been created). This recursive specialization \ncan set off ripples of specialization running upwards through the call graph afong statically-bound pass-through \nhigh-weight arcs. Recursive cycles of stahcally-bound pass-through arcs do not need special treatment, \nother than the check to see whether the desmed specialization has afready been created.   3.4 Improved \nCost-Benefit Analysis The algorithm as presented currently uses a very simple heuristic for deciding \nwhat to specialize: if the weight of a specializable arc is larger than the spectalizatiorzl%reshoki \nparameter, then the arc will be considered for specialization. In our implementation, the specialization \nThreshold is 1,000 inyocatlons. There are several shortcomings of this simple approach. First, the code \nspace increase recurred by specializing is not considered. A more intelligent heuristic could compute \nthe set of specializations that would be necessary to statically bind a particular arc, and factor this \nreformation into the decision-making process. Second, it treats all dynamic dispatches as equafly costfy. \nDue to the indirect effects of optimization such as inhning, the benefits of statically binding some \nmessage sends can be much higher than others. A more sophisticated heuristic could estimate the performance \nbenefit of static binding, taking into account post-inlining optimizations [Dean &#38; Chambers 94]. \nThird, the heuristic has no global view on the consumption of prowded with a fixed space budget, and \ncould visit arcs in decreasing order of weight, specializing until the space budget was consumed, We \ninitiafly implemented the simple heuristic using the specializationThreshold to get the algorithm up \nand running, but had expected to implement a more sophisticated heuristic later that would make better \ntimcdspace tradeoff decisions. However, in our system, the tradeoff implemented by the simple heuristic \nhas been more than adequate, as indicated by the empirical results repot-ted in Section 4. 3.5 Selecting \nthe Appropriate Version of a Specialized Method At run-time, message lookup needs to select the appropriate \nspecialized version of a method. In singly-dispatched languages like C++ and Smafltafk, existing message \ndispatch mechanisms work fine for selecting among methods that are specialized only on the receiver argument. \nAllowing specialization on arguments other than the receiver, however, can create multi-methods (methods \nrequiring dispatching on multiple argument positions) in the implementation, even if the source language \naflows only singly-dispatched methods, If the runtime system does not already support multi-methods, \nit must be extended to support them. A number of efficient strategies for multi-method lookup have been \ndevised, including trees of single dispatch tables [Kiczales &#38; Rodriguez 89], compressed multi\u00admethod \ndispatch tables [Chen et al. 94, Amiel et al. 94], and polymorphic reline caches extended to support \nmultiple arguments [Holzle et al. 91]. The choice of a multi-method dispatching mechamsm is orthogonal \nto our specialization algorithm, however,  3.6 Specializing Nested Methods In the presence of lexically-nested \nfirst-class functions, a message can be sent to a formal not of the outermost enclosing method but to \na lexically-nested function (such as a closure that accepts arguments). Although our current implementation \nonly considers the outermost method for specialization, in principle there is no reason that the algorithm \ncould not produce multiple specialized versions of nested methods. At run-time, the representation of \na closure could not contain a direct pointer to the target method s code, but instead would contain the \naddress of a stub routine that performed the necessary dispatching for the specialized argument posltlons. \n3.7 Other Issues In this subsection we address a variety of issues relating to the implementation of \nthe algorithm, 3.7.1 Whole Program Analysis and Incremental Compilation Examination of the complete \nclass hierarchy of the program, an underlying component of our specialization afgorithm, might seem to \nbe in conflict with incremental compilation: the compiler generates code containing embedded assumptions \nabout the structure of the program s class inheritance hierarchy and method definitions, and these assumptions \nmight change whenever the class hierarchy is ahered or a method is added or removed. (The atgonthm does \nnot depend on the implementations of the methods, other than the pass\u00adthrough information about cafl \nsites.) To help provide incremental compilation in the face of this whole-program analysm, our compder \nmaintains fine-grained dependency information to selectively recompile those pieces of the program that \nare invalidated as a result of some change to the class hierarchy or the set of methods in the program. \nThe dependency information forms a directed, acyclic graph, with nodes representing pieces of information, \nand edges representing dependencies. The dependency graph is constructed incrementally during compilation. \nWhenever a portion of the compilation process uses a piece of information that could change, the compiler \nadds an edge to the dependency graph from the node representing the information used to the node representing \nthe client of the information. When changes are made to the source program, the compiler computes what \nsource dependency nodes have been affected and propagates invalidations downstream from these nodes. \nThis invalidates afl information (including compiled code modules) that depended on the changed source \ninformation. Further details of this dependency representation and measurements of the amount of recompilation \nrequired for a 3-week period of programming can be found elsewhere [Chambers et al. 95]. 3.7.2 Gathering \nand Managing Profile Information Obtaining the profile data needed by the specialization afgorithm requires \nthat the program executable be built with the appropriate instrumentation. To enable long profiling runs \nand profiling of typl~cal application usage, profiling should be as inexpensive as possible, since otherwise \nit may not be feasible to gather profile information. Computing counts for statically-bound arcs can \nbe done b? inserting counters at statically-bound call sites. The expense of profiling dynamically-dispatched \nmessage sends depends in large part on the rtrn-time system s message dispatching mechanism. Some systems, \nincludlng the Cecil system in which we implemented the algorithm, use polymorphic in line caches [Holzle \net al. 91]: call-site-specific association lists mapping receiver classes to target methods. To gather \ncatl-site-specitic profile data, counter increments are added to each of the cases, and the counters \nfor the PICS of all call sites are dumped to a file at the end of a program run. The run-time overhead \nof this profiling for our Cecil benchmarks (described in Section 4) is 15-50?70.In other systems that \nuse method dispatckdng tables, such as C++ and Modula-3 implementations, additional code could, be insetted \nto maintain profile information at message send sites. To make managing profile information more convenient, \nour compiler maintains a persistent internal database of profile information that is consulted transparently \nduring compilations. Additionally, for object-oriented programs we have observed that the kind of profile \ninformation needed to construct this call graph remains fairly constant across different inputs to a \nprogram and even as the program evolves [Garrett et al. 94], so profiling can be done relatively infrequently \nand reused across many compilations.  3.7.3 Applicability to a Dynamic Compilation Environment Our algorithm \nis suitable for use in a dynamic compilation environment such as Self. The Self system initially compiles \nmethods without optimization but installs counters to detect the heavily-used methods. When a counter \nexceeds a threshold, the system recompiles portions of the code using optimization to adapt the code \nto program hot spots. Our specialization algorithm could be used in such a system. The unoptimized code \nwould keep track of the target methods for each call site and the counts (essentially arcs in the call \ngraph), and the appropriate localized portion of the call graph could be constructed as necessary to \nmake specialization decisions during the recompilation process. Section 4 provides results showing that, \neven for a system that compiles code lazily, our approach could lead to a significant reduction in code \nspace requirements over a system that employs simple customization.   4 Performance Evaluation To evahtate \nthe effectiveness of our algorithm, we implemented several different specialization schemes in the context \nof the Vortex compiler for Cecil, a pure object-oriented language based on multi\u00admethods [Chambers 93]. \nThe implementation of our algorithm constructs a weighted call graph from profiles of the program and \nthen generates a list of specialization directives using our algorithm. The compiler then executes the \ndirectives to produce the specialized versions of methods. We compared our specialization scheme with \nseveral alternative schemes, described in Table 1. Our base configuration performs Table 1: Compiler \nConfigurations Configuration Description Base Intraprocedural class analysis, inlining, splitting, CSE \n&#38; constant propagation&#38; folding, dead code elimination (to optimize away unneeded closure creations), \nand hard-wired class prediction for a small number of common messagessuch as if and +. This set of optimization \nN roughly comparable to that performed by the Self-91 compiler [Chambers &#38;Ungar91]. The compiler \nproduces one compiled version for each source method. Cust Base + simple customization: specialize each \nmethod for each inheriting class for the receiver argument. This corresponds to the approach take by \nthe implementations of Sather, Trellis, and Self, cust-MM Base + customization extended to multi-methods \nby custonrizmg on each possible combination of dispatched arguments. The code space requirements of this \nconfiguration make it impracticrrt for statically-compiled systems CHA Base + class hierarchy analysis. \nThe class hierarchy of the whole program M examined to enable conversion of dynamically-bound calls to \nstatically\u00adbound calls when the compiler detects that there are no overriding methods [Dean et al. 95]. \nSelective CHA + our profile-guided selective specmfization atgorithm. significant intraprocedural optimizations, \nseeking to temper the effects of Cec~l s pure language model. W; measured the effectiveness of these \nschemes in terms of both code space used and nmtime performance on four Cecil benchmark programs, described \nin Table 2. Table 2: Benchmarks Program Linesa Description Richards 400 Operating system task queue simulation \nlnstSched 2,400 A MIPS assembly code instruction scheduler Typechecker ll,ooob Typechecker for the Cecil \nlanguage I Compiler I37,500b IOptimizing compiler for the Cecil language a. In addition to the 8,500-line \nstandard library b. The typechecker and compiler share approximately 12,000 lines of code.  To evaluate \nthe performance of the algorithm, we measured both the dynamic number of dynamic dkpatches in each version \nof the programs as well as the bottom-line execution speed. The results are shown in Figure 5, All data \nhave been normalized to the Base configuration for each program. For the bottom-line execution speed \ngraph, taller bars indicate better performance for the code space usage graphs, shorter bars indicate \nmore compact compiled code. For the selective algorithm, we used one set of inputs to the Typechecker \nand Compiler benchmarks for gathering the profiles and a different set of inputs for measuring the resulting \nspecialized  Number of Dynamic Dispatches Execution Speed 35 I 1 Base Cust ,: Cust-MM CHA Selective \nFigure 5: Number of dynamic .,...$ ,. . -~ .> 143 30.5 196.1 4.0 r 3.5 3.0 25 2.0 1.5 Richards lnstSched \nTypechecker Compiler ., Source methods Cust : Cust-MM Selective B Figure 6: Number of routines programs. \nIn practice, the choice of input did not seem to have a significant impact on performance, however. Simple \ncustomization eliminated 35-6 1 % of the dynamic dispatches in the benchmark programs over the baseline. \nCustomizing for all combinations of dispatched arguments for multi-methods eliminated slightly more dispatches, \nranging from 41 -62Y0. Selective specialization did best of all, eliminating 54-669% of the dispatches; \nclass hierarchy analysis, a necessary component of our selective specialization algorithm, alone eliminated \n33-54% of the dispatches. Translated to bottom-line performance measurements, customization sped up the \nprograms by 26-125% (by 38-131 Yoif customizing on all dispatched arguments). Our selective specialization \nalgorithm performed considerably better, speeding the programs by 65-275%. dispatches and execution \nspeed ., . , , ..-. . , ,: * <. ~.>. ,.,. ,x. ? WV. ../ x. .,.*.,..,. -. <.. : .,,,. ,J% s . . .._, . \n,.J  Dynamic 4.0 3.5 3,0 2.5 0 % 2.0 cc 1.5 1.0 0.5 0.0 Richards lnstSched Typechecker Compiler Invoked \nmethods Cust ~ Cust-MM Selective B compiled (static and dynamic) Class hierarchy analysis by Itself \nsped up the programs by 24-70%, accounting for roughly a third of the benefit attributable to our algorithm. \nThe space requirements for,the various versions of the benchmarks are summarized in Figure 6. We present \ntwo different versions of the space requirements: the first is the number of specialized methods that \nare produced in a statically-compiled system such as Sather, * The runtlme performance values for Cust-MM \nwere derived from an executable that contained only those specialized methods that were invoked during \nprogram execution. Cust-MM is practical only for dynamic compilation systems, due to its prohibmve code \nspace increase in statically-compiled systems 100 Trellis, or Cecil, and the second is the number that \nwould be procluced in a dynamic compilation system such as Self, where only the methods invoked at run-time \nget compiled and specialized. Compile time numbers are not reported, but the compile time required should \nbe roughly proportional to the number of specialized methods compiled, Our selective specialization algorithm \nproduces fewer specializations than either of the simple customization strategies, In staticxdly\u00adcompiled \nsystems, receiver-oriented customization increases the code space required for the benchmarks by a factor \nof three to four. Our selective specialization algorithm increased code space by only 4-10% for the benchmarks, \ndespite achieving the highest run. time performance. For larger programs, selective specialization may \nbe the only practical specialization strategy in a statically-compiled system.  Related Work The implementations \nof Self [Chambers&#38; Ungar 91], Trellis [Kilian 88], and Sather [Lim &#38; Stolcke 91] use customization \nto provide the compiler with additional information about the class of the receiver argument to a method, \nallowing many message sends within each customized version of the method to be statically-bound. All \nof this previous work takes the approach of always specializing on the exact class of the receiver and \nnot specializing on any other arguments. The Trellis compiler merges specializations after compilation \nto save code space, if two specializations produced identical optimized code; compile time is not reduced, \nhowever. As discussed in Section 2, customization can lead to both overspecialization and underspecialization. \nOur approach is more effective because it identifies sets of receiver classes that enable static binding \nof messages and uses profile data to ignore infrequently-executed methods (thereby avoiding overspecialization), \nand because it allows specialization on arguments other than just the receiver of a message (preventing \nunderspecialization for arguments). Cooper, Hall, and Kennedy present a general framework for identifying \nwhen creating multiple, specialized copies of a procedure can provide additiomd information for solving \ninterprocedural dataflow optimization problems [Cooper et al. 92]. Their approach begins with the program \ns call graph, makes a forward pass over the catl graph propagating cloning vectors which represent the \ninformation available at call sites that is deemed interesting by the called routine, and then makes \na second pass merging cloning vectors that lead to the same optimization. The resulting equivalence classes \nof cloning vectors indicate the specializations that should be created. Their framework applies to any \nforward dataflow analysis problem, such as constant propagation. Our work differs from their approach \nin sever-al important respects. First, we do not assume the existence of a complete call graph prior \nto analysis. Instead, we use a subset of the real call graph derived from dynamic profile information. \nThis is important, because a precise call graph is difficult to compute in the presence of extensive \ndynamically dispatched messages [Palsberg &#38; Schwartzbach 91, Plevyak &#38; Chien 94, Pande &#38; \nRyder 94]. Second, our algorithm is tailored to object-oriented languages, where the information of interest \nis derived from the specializations of the arguments of the called routines. Our algorithm consequently \nworks backwards from dynamically-dispatched pass-through call sites, the places that demand the most \nprecise information, rather than proceeding in two phases as does Cooper et al. s algorithm. Finally, \nour algorithm exploits profile information to select only protiVdble specializations, Procedure specialization \nhas been long incorporated as a principal technique in partial evaluation systems [Jones et al. 93]. \nRuf, Katz, and Weise [Ruf &#38; Weise 91, Katz &#38; Weise 92] address the probtem of avoiding overspecialization \nin their FUSE partial evaluator. Their work seeks to identify when two specializations generate the same \ncode. Ruf identifies the subset of information about arguments used during specialization and reuses \nthe specialization for other call sites that share the same abstract static information. Katz extends \nthis work by noting when not all of the information conveyed by a routine s result is used by the rest \nof the program. Our work differs from these in that we are working with a richer data and language model \nthan a functional subset of Scheme and our algorithm exploits dynamic profile information to avoid specializations \nwhose benefits would be accrued only infrequently. 6 Conclusions We are exploring a number of areas in \nthis line of research. First, we are working on applying the specialization algorithm to other languages, \nsuch as C++ and Modula-3, to examine its performance across a variety of language models. We are also \ninvestigating ways of specializing callers for the return values of the called methods, so that knowledge \nof the class of the return value can be propagated to the caller, perhaps by returning to different locations \nbased on the class of the return vahre. On a larger scale, we are exploring how different optimization \ntechniques for optimizing object-oriented programs interact. Specialization is only one technique for \nreducing dynamic\u00addispatching overhead in object-oriented programs. A number of other techniques have \nalso demonstrated substantial performance improvements for programs with extensive dynamic dispatching, \nincluding whole-program class hierarchy analysis [Dean et al. 95], profile-guided class prediction [Garrett \net al. 94, Holzle &#38; Ungar 94], and interprocedural class inference [Palsberg &#38; Schwartzbach 91, \nPlevyak &#38; Chien 94]. All of these techniques are striving to eliminate dynamic dispatches and indirectly \nenable other optimizations, such as inlining, so it seems clear that the performance benefits of combining \nall of these techniques will not be strictly additive. We are currently exploring how these different \noptimization techniques interact. We have presented a general framework for specialization in object\u00adoriented \nlanguages and an algorithm that combines static analysis and profile information to identify the most \nprofitable specializations. Compared to a base configuration that does no specialization, our algorithm \nimproves the performance of a suite of realistic programs ranging from 400 to 50,000 lines by 65 % to \n275~0 while increasing compiled code space requirements by only 4% to 10%. Compared to customization, \nthe previous state-of-the-art specialization technique for object-oriented languages included in several \nlanguage implementations, our algorithm improves performance by 2070 to 68 ZO, while simultaneously reducirrg \ncode space requirements by 65% to 73%. As a consequence of its more judicious application of specialization, \nour algorithm is appropriate for specializing on multiple arguments of a method and for use in object-oriented \nlanguages with multi-methods.  Acknowledgments Thk research is supported in part by a NSF Research Initiation \nAward (contract number CCR-921099O), a NSF Young Investigator Award (contract number CCR-9457767), a \ngrant from the Office of Naval Research (contract number NOO014-94-1-1136), and gifts from Sun Microsystems, \nIBM, Pure Software, and Edison Design Group. Mark Linton and the anonymous referees provided helpful \ncomments that improved the presentation of this paper. Stephen North and Eleftherios Koutsofios of AT&#38;T \nBell Laboratories provided us with dot, an automatic graph layout program. An early version of this work \nappeared in the 1994 Workshop on Partial Evaluation and Semantics-Based Program Manipulation (PEPM 94). \n101 Other papers on the Cecil programming language and the Vortex optimizing compiler are available \nvia anonymous f tp from cs.Washington.edu: /pub/chambers and via the World Wide Web URL http:l/www.cs.washington.edu/research/projects/cecil. \n References [Amiel et al. 94] Eric Amlel, Olivier Gruber, and Eric Simon. Optimizing Multi-Method Dispatch \nUsing Compressed Dispatch Tables. In Proceedings 00PSLA 94, pages 244-258, Portland, Oregon, October \n1994. [Chambers &#38; Ungar 89] Craig Chambers and David Ungar. Customization: Optimizing Compiler Technology \nfor Self, A Dynamically-Typed Object-Oriented Programming Language. SIGPLAN Notices, 24(7):146-160, July \n1989. In Proceedings of the ACM SIGPLAN 89 Conference on Programming Language Design and Implementation. \n[Chambers &#38; Ungar 91] Craig Chambers and David Ungar, Making Pure Object-Oriented Languages Practical. \nIn Proceedings 00PSLA 91, pages 1-15, November 1991. Published as ACM SIGPLAN Notices, volume 26, number \n11. [Chambers 92] Craig Chambers. The Design and Implementation of the SELF Compile~ an Optimizing Compiler \nfor Object-Oriented Programming Lunguages. PhD thesis, Stanford University, March 1992. [Chambers 93] \nCraig Chambers. The Cecil Language: Specification and Rationale. Technical Report TR-93-03-05, Department \nof Computer Science and Engmeermg. University of Washington, March 1993. [Chambers et al. 89] Craig Chambers, \nDavid Ungar, and Elgin Lee. An Efficient Implementation of SELF a Dynamically-Typed Object-Oriented \nLanguage Based on Prototypes. In Proceedings OOPSLA 89, pages 49-70, October 1989. Published as ACM SIGPLAN \nNotices, volume 24, number 10. [Chambers et al. 95] Craig Chambers, Jeffrey Dean, and David Grove. A \nFramework for Selective Recompilation in the Presence of Complex Intermodule Dependencies. In 17th International \nConference on Software Engineering, Seattle, WA, April 1995. [Chen et al. 94] Weimin Chen, Volker Turau, \nand Wolfgang Klas, Efficient Dynamic Look-up Strategy for Multi-Methods. In M. Tokoro and R. Pareschi, \neditors, Proceedings ECOOP 94, pages 408-431, Bologna, Italy, July 1994. Springer-Verlag. [Cooper et \nal. 92] Keith D. Cooper, Mary W. Hall, and Ken Kennedy. Procedure Cloning. In Proceedings of 1992 IEEE \nInternational Conference on Computer Lunguages, pages 96\u00ad105, Oakland, CA, April 1992. [Dean &#38; Chambers \n94] Jeffrey Dean and Craig Chambers. Towards Better Inlining Decisions Using Inlining Trials. In Proceedings \nof the ACM Conference on LISP and Functional Programming 94, pages 273 282, Orlando, FL, June 1994. [Dean \net al. 95] Jeffrey Dean, David Grove, and Craig Chambers. Optimization of Object-Oriented Programs Using \nStatic Class Hierarchy Analysis. In Proceedings ECOOP 9S, Aarhus, Denmark, August 1995. Springer-Verlag. \n[Garrett et al. 94] Charlie Garrett, Jeffrey Dean, David Grove, and Craig Chambers. Measurement and Application \nof Dynamic Receiver Class Distributions. Technical Report UW-CS 94-03\u00ad05, University of Washington, March \n1994. [Grove &#38; Torczon 93] Dan Grove and Linda Torczon. InterProcedural Constant Propagation: A Study \nof Jump Function Implementations. SIGPL4N Notices, 28(6):90 99, June 1993. In Proceedings of the ACM \nSIGPLAN 93 Conference on Programming Lunguage Design and Implementation. [Holzle &#38; Ungar 94] Urs \nHolzle and Dawd Ungar. Optimizing Dynamically-Dispatched Calls with Run-Time Type Feedback. SIGPLAN Notices, \n29(6):326 336, June 1994. In Proceedings of the ACM SIGPLAN 94 Conference on Programming Language Design \nand Implementation. [Holzle et al. 91] Urs Holzlej Craig Chambers, and David Ungar. Optimizing Dynamically-Typed \nObject-Oriented Languages With Polymorphic Inline Caches. In P. America, editor, Proceedings ECOOP 91, \nLNCS 512, pages 2 1 38, Geneva, Switzerland, July 1991. Sprmger-Verlag. [Ingalls 86] Daniel H. H. Ingalls. \nA Simple Technique for Handling Multlple Polymorphism. In Proceedings 00PSLA 86, pages 347-349, November \n1986. Published as ACM SIGPLAN Notices, volume 21, number 11. [Jones et al. 93] Neil D. Jones, Carstein \nK. Gomarde, and Peter Sestoft. Partial Evaluation and Automatic Program Generation. Prentice Hall, New \nYork, NY, 1993. [Katz &#38; Weise 92] M. Katz and D. Weise. Towards a New Perspective on Partial Evaluation. \nIn Proceedings of the Workshop on Partial Evaluation and Semantics-Based Program Manipulation 92, pages \n29 36. Yale University, 1992. [Kiczales &#38; Rodriguez 89] Gregor Kiczales and Luis Rodriguez. Efficient \nMethod Dispatch in PCL. Technical Report SSL 89-95, Xerox PARC Systems Sciences Laboratory, 1989. [Kilian \n88] Michael F. Kilian. Why Trellis/Owl Runs Fast. Unpublished manuscript, March 1988. [Lea 90] Doug Lea. \nCustomization in C++, In Proceedings of the 1990 Usenix C+ + Conference, San Francisco, CA, April 1990. \n[Lim &#38; Stolcke 91] Chu-Cheow Lim and Andreas Stolcke. Sather Language Design and Performance Evaluation. \nTechnical Report TR 91-034, International Computer Science Institute, May 1991. [Palsberg &#38; Schwartzbach \n91] Jens Palsberg and Michael I. Schwartzbach. Object-Oriented Type Inference, In Proceedings 00PSL4 \n91, pages 146 161, November 1991. Published as ACM SIGPLAN Notices, volume 26, number 11. [Pande &#38; \nRyder 94] Hemant D. Pande and Barbara G. Ryder. Static Type Determination for C++. In Proceedings of \nSixth USENIX C++ Technical Conference, 1994. [Plevyak &#38; Chien 94] John Plevyak and Andrew A. Chien. \nPrecise Concrete Type Inference for Object-Oriented Languages. In Proceedings 00PSLA 94, pages 324-340, \nPortland, Oregon, October 1994. [Ruf &#38; Weise 91] E. Ruf and D. Weise. Using Types to Avoid Redundant \nSpecialization. In Proceedings of the Symposium on Partial Evaluation and Semantics-Based Program Manipulation \n91, pages 321-333. ACM, 1991. \n\t\t\t", "proc_id": "207110", "abstract": "<p>Dynamic dispatching is a major source of run-time overhead in object-oriented languages, due both to the direct cost of method lookup and to the indirect effect of preventing other optimizations. To reduce this overhead, optimizing compilers for object-oriented languages analyze the classes of objects stored in program variables, with the goal of bounding the possible classes of message receivers enough so that the compiler can uniquely determine the target of a message send at compile time and replace the message send with a direct procedure call. <italic>Specialization</italic> is one important technique for improving the precision of this static class information: by compiling multiple versions of a method, each applicable to a subset of the possible argument classes of the  method, more precise static information about the classes of the method's arguments is obtained. Previous specialization strategies have not been selective about where this technique is applied, and therefore tended to significantly increase compile time and code space usage, particularly for large applications. In this paper, we present a more general framework for specialization in object-oriented languages and describe a goal directed specialization algorithm that makes selective decisions to apply specialization to those cases where it provides the highest benefit. Our results show that our algorithm improves the performance of a group of sizeable programs by 65% to 275% while increasing  compiled code space requirements by only 4% to 10%. Moreover, when compared to the previous  state-of-the-art specialization scheme, our algorithm improves performance by 11% to 67% while simultaneously reducing code space requirements by 65% to 73%.</p>", "authors": [{"name": "Jeffrey Dean", "author_profile_id": "81100248818", "affiliation": "Department of Computer Science and Engineering, University of Washington, Seattle, WA", "person_id": "PP39034418", "email_address": "", "orcid_id": ""}, {"name": "Craig Chambers", "author_profile_id": "81100528252", "affiliation": "Department of Computer Science and Engineering, University of Washington, Seattle, WA", "person_id": "PP39047060", "email_address": "", "orcid_id": ""}, {"name": "David Grove", "author_profile_id": "81100575938", "affiliation": "Department of Computer Science and Engineering, University of Washington, Seattle, WA", "person_id": "PP39049261", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/207110.207119", "year": "1995", "article_id": "207119", "conference": "PLDI", "title": "Selective specialization for object-oriented languages", "url": "http://dl.acm.org/citation.cfm?id=207119"}