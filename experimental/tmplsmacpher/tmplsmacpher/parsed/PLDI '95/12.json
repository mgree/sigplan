{"article_publication_date": "06-01-1995", "fulltext": "\n Scheduling and Mapping: Software Pipelining\u00adin the Presence of Structural Hazards * Erik R. Altman R, \nGovindarajan Guang R. Gao DePt. of Electrical Engineering Dept. of Computer Science School of Computer \nScience McGill University Memorial Univ. of Newfoundland McGill University Montreal, H3A 2A7, CANAI)A \nSt. John s, AIB 3X5, CANADA Montreal, H3A 2A7, CANADA erik@macs.ee. mcgill.ca govindtlcs.mun.ca gao@cs.mcgill.ca \n Abstract Recently, soflware pipelining method~; based on an ILP (Integer Linear Programming) framework \nhave been successfully applied to derive rate-optimal schedules for architectures involving clean pipelines \n pipelines without structural hazards. The pro,blem for architec\u00adtures beyond such clean pipelines remains \nopen. One challenge is how, under a unijied ILP framework, to si\u00admultaneously represent resource constraints \nfor unclean pzpeltnes, and the assignment or mapping of operations from a loop to those pipelines. In \nthis paper we provtde a framework which does exactly this, and m additton constructs rate-optimal sofiware \npipelined schedules. The proposed formulation and a solution method have been implemented and tested \non a set of 1066 loops taken from various scientific and integer benchmark suztes. The formulation found \na rate-optimal sched\u00ad ule for 75% of the loops, and required a median time of only 2 seconds per loop \non a Spare l[J/30.  Introduction Software pipelining overlaps operations from differ\u00adent loop iterations \nin an attempt to fully exploit instruction-level parallelism. To be successful on real machines, the \nfunction unit constraints of those ma\u00adchines must be taken into account. A variety of soft\u00adware pipelining \nalgorithms [1, 5, 7, 11, 13, 16, 17, 18, 20, 24, 27, 26] have been proposed which operate un\u00adder resource \nconstraints. An excellent survey of these algorithms can be found in [19]. This work was supported by \nresearch grants from NSERC (Canada) and MICRONET Network Centers of Excellence (Canada). Permission to \ncopy without fee all or part of this material is granted provided that the copies are not made cwdistributed \nfor direct commercial advantage, the ACM copyright notice and the title of the publication and its date \nappear, and Inotice is given that copyin is by permission of the Association of Computing Machinery. \nf o copy otherwise, or to republish, requires a fee and/or specific permission. SIGPLAN 95La JoHa, CA \nUSA @ 1995 ACM 0-89791 -697-2/95/0006 ...$3.50 Integer Linear Programming (ILP)-based and other approaches \nhave been proposed for finding an optimal periodic schedule on machines with simple resource us\u00adage [6, \n9, 25]. However, this work has assumed that function units are either completely pipelined or have no \npipelining at all. Furthermore, it did not address the issue of mapping, that is the assignment of instructions \nto specific function units. For example if a loop has 3 divide instructions, a, b, and c and the architec\u00adture, \n2 Divide units, X and Y, the mappmg determines whether a is executed on X or Y, and likewise for b and \nc. Earlier work based on the ILP approach [6, 9] implicitly assumes that the mapping is done at run\u00adtime, \nbased on which of the function units are available. However, as we shall show, for non-pipelined function \nunits and function units with structural hazards, there are significant restrictions on mapping restrictions \nwhich preclude some schedules which would otherwise be legal. As a result, we require that the FU assign\u00adment \n(mapping) be fixed at compile-time. In this paper, we aim to eliminate these earlier re\u00adstrictions. The \napproach outlined here is a unified framework which: (1) finds the fastest possible sched\u00adule, (2) handles \npipelines with arbitrary structural hazards, and (3) correctly maps instructions to these pipelines at \ncompile time. Specifically, We extend our ILP method to architectures which are either non-pipelined \nor pipelined but contain\u00ading arbitrary structural hazards, namely unclean pipelines. We show that the \nmapping required can be identified as an instance of the circular\u00adarc graph coloring problem [10] and \ncan be repre\u00adsented by integer linear constraints and smoothly integrated into our overall ILP framework. \n We have implemented this extended framework and have performed tests on 1066 loops found in typical \nbenchmark programs such as the SPEC92, the NM kernels, linpack, and the livermore loops. In these experiments, \nwe obtained an op\u00adtimal schedule for greater than 75% of the loops.  We ran the experiments on a Spare \n10/30, which required a median of 2 seconds and a geometric mean of only 5 seconds per loop to compute \nthese schedules. To our knowledge, this is the first re\u00ad port of experimental results on optimal ILP-based \nschedules for pipelines with structural hazards on a large number of real loops. We hope that these results \nwill help the compiler community in the assessment of ILP approaches. In spite of these experimental \nresults, more experi\u00admental work in the future is required to study the prac\u00adticality of an ILP approach \nin a real compiler. Nonethe\u00adless, we believe that it is very useful to compiler design\u00aders to establish \nan optimal bound so as to compare and improve their heuristic methods of software pipelining. The rest \nof this paper is organized as follows. In the following section we present the necessary back\u00adground \nand motivation for using an ILP approach for software pipelining. In Section 3, we motivate our ap\u00adproach \nto the scheduling problem in the presence of un\u00adclean pipelines with an example, Section 4 deals with \nthe formulation of the scheduling and mapping prob\u00adlems for non-pipelined execution units. In Section \n5 we extend our formulation to function units with arbi\u00adtrary structural hazards. Experimental results \ninvolv\u00ading 1066 loops are reported in Section 6. We compare our work with other related work in Section \n7. Con\u00adcluding remarks are presented in Section 8. 2 Background and Motivation Soflware ptpelzwing has \nbeen proposed as an efi\u00adcient method for loop scheduling for high-performance VLIW and super-scalar architectures. \nSoftware pipelining derives a static parallel schedule a pe\u00adriodic pattern that overlaps instructions \nfrom dif\u00adferent iterations of a loop body. The performance of a software-pipelined schedule is measured \nby the z nttia\u00ad tton rate of successive iterations. The problem of finding, under resource constraints, \nan optimal schedule (i.e. with the fastest possible or rate-optimal initiation rate) is NP-hard. Various \nheuristic solution methods [7, 13, 22, 26] have been proposed to solve the problem efficiently. In this \npaper, we are interested in solution meth\u00adods based on ILP. In contrast to heuristic scheduling algorithms \nwhich find approximate (or suboptimal) so\u00adlutions, ILP methods are based on a formulation where an objective \noptimality function is specified. A software pipelined schedule can be represented in a linear periodic \nform [23], under which an instruction i in iteration j is initiated at time j * T + t,.T is the period \n(or inatiatzon interval often abbreviated as II in the literature) of the schedule. The value ti is \na constant and indicates the time at which i first ex\u00adecutes. As a result, the mathematical formulation \nof software pipelining using an ILP framework appears to be quite natural. The basic formulation requires \nthe casting of precedence constraints into linear form which is straightforward [24]. In [18], it was \ndemonstrated that register storage requirements can also be repre\u00adsented in a simple linear form and \nintegrated easily into the basic ILP framework. In [9], we extended this framework further to include \nfunction unit constraints (also in linear form) for hardware pipelines without structural hazards. In \nthe next section, we present an example which demonstrates this framework. Of course, finding optimal \nsolutions for an ILP prob\u00adlem is NP-Hard. However, we feel that a clearly stated optimality objective \nin the problem formulation is im\u00adportant. A given loop is likely to have many good schedules from which \nto choose, and optimality crite\u00adria are essential to guide the selection of the best ones. There is experimental \nevidence that ILP approaches may produce better schedules than many heuristic methods for software pipelining \n[9]. One interesting question is: will cleverly designed ex\u00adhaustive search methods be superior to an \nILP solver in terms of efficiency? Although we have lately been working on exploiting such alternatives \n[2], it is still too early to make a conclusion. In the meantime, we note that significant progress has \nbeen made in developing efficient ILP algorithms. We believe that ILP methods for software pipelining \nshould take advantage of such developments and further improve their performance. In particular, analyzing \nthe structure of the constraints in the ILP approach to the software pipelining prob\u00adlem appears to be \nan important direction to pursue, although it is largely beyond the scope of this paper. In the rest \nof this paper, we discuss how to extend the ILP framework to handle hardware pipelines with structural \nhazards.  3 Scheduling Unclean Pipelines In this section, we use a motivating example to illus\u00adtrate \nthe scheduling problems in the presense of un\u00adclean pipelines. In section 3.1, we address the issues \nwhen the execution units are non-pipelined a special, but very important case of unclean pipelines. \nIn section 3.2, we examine execution units which are pipelined but have structural hazards. 3.1 Non-pipelined \nExecution Units We use the simple example loop in Figure 1 as a running example throughout. Both C language \nand instruction-level representations are given in Fig\u00ad ure l(b) while the Data Dependence Graph (DDG) \nis   r!?!!!zl  (a) Dependence Graph depicted in Figure 1(a). For purpose of discussion in sider an \narchitecture with 31nteger Point (FP) units and 1 Load/Store that the Integer units perform and likewise \nthe FP units erations. Further let the add (iO) be 1 cycle, while i3, and i4) be 2 cycles, takes 2 cycles \nand store assume that all function perform execution that of Assume (i5) takes 1 cycle.1 Lastly we units \nare the next operation cannot be initiated the previous execution is completed. that the minimum initiation \ninterval iterations of a loop is constrained by dependence and available resources Figure 1: An this \nsection, we con\u00adunits, 2 Floating\u00adunit. We assume all integer operations all floating point op\u00adtime of \nan Integer an FP multiply (i2, also that load (ii) non-pipelined, i.e. on an FU until It is well known \nTmin of successive both loop-carr~ed [4, 13, 16, 19, 20]. The loop-carried dependence put a lower bound, \nTdeP, on T. The value of TdeP is determined by the critical cycle(s) in the loop [23]. Specifically sum \nof instruction execution times Tdep= sum of dependence distances along the critical cycle(s), For the \nDDG in Figure I(a), Td.P is 2 corresponding to the self loop at instruction i2. Another lower bound TT~S \non source constraints. Consider (1) the loop that use FU type r (e.g. are Fr of type r in the architecture, \nexecution time of all instructions cycles, then r- T is enforced by re\u00adall the operations in FP unit), \n(2) there and (3) the total in FU type risTr 7 where the maximum is taken over all FU types r. In our \nexample the T,e. bound is given by both the FP 1These latency values are assumed to make the illustrations \nin this section compact. More realistic values for the execution times based on PowerPC-604 [14] are \nassumed in the experimen\u00adtal section. iO: vr33 =vr33 + vr32 % vr33 is address of a[i] % il: vr34 = load \nm(vr33) % vr34 = a[i] % i2 : vr35 = vr35 * vr34 %vr35=s% i3: vr36 = vr35 * vr35 % vr36 stores s * s % \ni4: vr37 = vr36 * vr34 % vr37 = new a[i] % i5: store(vr37, m(vr33)) branch toiOif i<n enddo : (b) Program \nRepresentation Example Loop unit and the Load/Store unit, That is, T res = rnax(TInt J FP ~ Load/Store \n)  = ax(w+w+l) = max(l,3,3) The shortest initiation of T~eS and l_dep. However schedule with a period \nTlb = 3 interval Tlb is the maximum there may or may not exist a satisfying the given resource constraints. \nTnin schedule exists 2. There are several time fixed mapping is often impractical is the minimum period \nfor which a reasons that we require a compile\u00adof instructions to function units. It for an instruction \nto be switched from one function unit to another during the course of its execution. For instance, since \na specific operation (or part) of the instruction word is linked to a func\u00adtion unit in VLIW architectures, \nit becomes necessary to have a jired assignment from an instruction to FU. This fixed instruction (task) \nto, FU mapping may ako be essential in architectures with multiple register sets (such as the Multiflow \nTrace dedicated to one (or a group To illustrate the significance constraint, examine Schedule ule is \nfor the DDG in Figure the linear schedule form T. i t~~= 1,t&#38;! = 3, We use the notation tion of instruction \nil step. Since the FUS instructions continuing ti3 =5, ..il to continues machine [3]), with each of) \nexecution units. of this fixed assignment A in Table 1. This sched\u00ad1(a) and is obtained from + t., with \nT=3, &#38;O = O, ti4= 7, and tis= 9. indicate that the execu\u00adfrom the previous time are assumed to be \nnon-pipelined, from previous time step, con\u00ad tinue to engage the FU. Schedule A has a prolog (from 2In \nsome cases an initiation rate better than #--can be 77?*. obtained by unrolling the loop. We do not \nconsider unrolling in this paper, although the techniques lpresented here can be used with unrolled loops \nas well. o1234 5 Iteration = O io il _-i 1 i2 ..i2 i3 Iteration = 1 io il --i 1 Iteration = 2 Iteration \n= 3 Iteration = 4 Table 1: Schedule A for o123 45 Iteration = O iO il __i 1 i2 --i2 i3 Iteration = 1 \niO il Iteration = 2 Table 2: Alternative Schedule B for [0, 6]), a repetitive pattern ([7, 9]), and an \nepilog. Note that the resource constraints are met in the repeti\u00adtive pattern no more than 3 Integer, \n2 FP, and 1 Load/Store unit are needed at any one time. (Times 7 and 8: 2 FP and 1 Load/Store; Time 9: \n1 In\u00adteger, 2 FP, 1 Load/Store.) In other words, Sched\u00adule A satisfies both the dependence constraints \nand the resource constraints of our architecture. Unfortunately, Schedule A does not satisfy the fixed \nFU assignment criteria. To see this, consider the repet\u00aditive pattern starting at time step 7. If we \nassign the first FP unit to instruction i4 at time step 7, and the second FP unit to i3 at time step \n8, then we have the first FP unit free at time step 9 and the second FP unit free at time 7 (or time \nstep 10, taking the time steps with modulo 3). Thus we cannot make a fixed FU as\u00adsignment for instruction \ni2. In fact there does not exist any schedule with a period T = 3 which satisfies the fixed FU assignment \nand requires only 2 FP units (in addition to 1 Integer and 1 Load/Store unit). An important challenge \nhere is that the problem of instruction-to-FU-mapping and resource-constrained software pipelining must \nbe formulated within a uni\u00adfied framework. This leads to the first problem to be formulated and solved \nin thm paper. Problem 1: Given a DDG and an archi\u00adtectural configuration with non-pipelined ex\u00adecution \nunits, construct a schedule that has the shortest initiation interval, using only the available resources, \nand mapping each im struction i to the same function unit FU(i) in every iteration. One important observation \nwe have made is to identify Time Steps 6 7 8 91011 12 13 _i3 i4 -i4 i5 i2 _.i2 i3 ._i3 i4 __i4 i5 io \nil -i1 i2 --i2 i3 --i3 i4 iO i1 --i 1 i2 --i3 iO il the Motivating Example Time Steps 6 7 8 9 10 11 \n12 13 14 __i3 i4 _i4 i5 __i 1 i2 _i2 i3 .i3 i4 ._i4 iO il -i 1 i2 i2 i3 __i3 the Motivating Example \n(with T = 4) the FU assignment problem as a circular-arc coloring problem [10]. In Section 4.2 we introduce \nthis coloring as a linear constraint to the basic ILP formulation for handling non-pipelined FUS which \nwill be presented in Section 4. Finally, under fixed FU assignment, Schedule B shown in Table 2 is a \nT = TmLn = 4 schedule (one with the shortest initiation interval) for the example architecture, with \n3 Integer units, 2 FP units and 1 Load/ Store unit. 3.2 Pipelines with Structural Hazards In this subsection \nwe illustrate how the software pipelining problem is altered by the presence of struc\u00adtural hazards in \nfunction unit pipelines. In such a func\u00adtion unit, the resource usage of different stages of the pipeline \nis generally represented by a reservation ta\u00adble [15]. Once again we consider the example loop shown \nin Figure 1 with an architecture consisting of 3 Inte\u00adger units of latency 1, 2 E P units of latency \n3, and 1 Load/Store unit of latency 3. However, we now as\u00adsume that the FP and Load/Store units have \nstruc\u00adtural hazards. In order to make the discussion simple, we assume both the FP unit and the Load/Store \nunit to be 3-stage pipelines with the simple reservation ta\u00adbles shown in Figure 2(a) and Figure 2(b). \nSchedule C in Figure 2(c) is compatible with this architecture. Unlike Schedules A and 1?, Schedule C \ndepicts only when an operation is initiated. When haz\u00adards are present, resource usage is more complicated, \nand we instead represent it by a set of resource-usage tables as shown in Figure 2(d) for the FP unit. \n Time Steps 012 Stagel x Stage 2 x Stage 3 x   EIEEl (a) FP U~it Reservation Tables Time Steps o \n123 4 56 7 8 9101112 15 141516 Iteration = O io il i2 i3 i4 i5 Iteration = 1 io il i2 i3 i4 Iteration \n= 2 iO i1 i2 i3 Iteration = 3 io il i2 Iteration = 4 iO (c) Schedule C for Pipelines with Structural \nHazards Stage 1 Stage 2 Stage 3 Time Steps Time Steps Time Steps 13141516 13141516 13141516 Iter. = O \nIter. =1 i4 i4 i3 i4 i4 Iter.=2 i3 i2 23 a2 i2 23 Iter. =3 i2 T+.. A L.Cl. x u II I IIu (d) Resource \nUsage Tables for FP Unit Figure 2: Reservation Tables, Schedule C, and corresponding Resource Usage Tables \nThe way in which the FP resource usage tables in resource usage tables allows us in Section .5 to solve \na Figure 2(d) are derived from the FP reservation table generalization of Problem 1: in Figure 2(a) and \nSchedule C in Figure 2(c) is straight- Problem 2: Given a 13DG and an archi\u00ad forward. First consider \nStage 1 of the FP unit. In the tecture with structural hazards, construct a reservation table in Figure \n2(a) it is used only at time schedule that has the shortest initiation in- O. Thus in the resource usage \ntable for FP Stage 1 terval, using only the available resources, and in Figure 2(d), each FP operation \n(i2, i3, and i4) oc\u00admapping each instruction i to the same func\u00ad curs at the same time as in Schedule \nC . (For software tion unit FU(i) in every iteration. pipelining, we look only at the periodic pattern \nfrom [13, 16].) Stage 2 however, is used starting one cycle Combining our ILP approach with the resource \nus\u00ad after the start of the operation, Thus each FP opera\u00adage table provides the key to an lLP formulation \nfor tion is rotated one to the right from the resource usage Problem 2. As can easily be seen, the fixed \nFU assign\u00ad table of Stage 1. Since i2 was at time 16 in Stage 1, it ment problem in the presence of structural \nhazards is wraps around to time 13 in Stage 2. The reservation also related to the coloring problem. \nIn fact, this can table indicates that FP Stage 3 is used both one and be derived by an extension of \nthe formulation of the two cycles after the start of a floating point operation. non-pipelined case discussed \nearlier. This means that the resource usage table for Stage 3 has operations at the same times as Stage \n2 and in addi\u00adtion has them rotated one place to the right. Stage 3 is 4 Scheduling Non-Pipelined the \nonly one in which multiple instructions use a stage Execution Units at the same time: i2 and i3 both \nuse Stage 3 at time 13, while i3 and i4 do so at time 16. This being the In this section, we develop \na formulation for the soft\u00ad case, both FP units in the architecture are required. ware pipelining problem \nfor architectures involving This mapping of reservation tables and schedules to non-pipelined execution \nunits. The formulation is based on our earlier work on integer programming for\u00admulation for clean pipelines \n[9]. The basic ILP for\u00admulation reported in [9] is extended to non-pipelined architectures in Section \n4.1. In Section 4.2, we de\u00advelop a graph coloring formulation which guarantees a schedule with fixed \nFU assignment for each instruction. The result of this integration is a unified formulation for scheduling \nand mapping. 4.1 The Elasic ILP Formulation As discussed in Section 2, we consider linear periodic schedules \nof the form j T + t!. The repetitive pattern in the software pipelined schedule is represented suc\u00adcinctly \nin our ILP formulation by a T x N matrix A, where AJ is the number of nodes in the DDG. A is a 1-0 matrix \nwith at,, = 1 if and only if instruction i is scheduled to begin at time step tin the repetitive pat\u00adtern. \nOtherwise at,, = O. As will be seen shortly, the A matrix captures the modulo-reservation table [16, \n20] in a form that is suitable for representing resource con\u00adstraints m a linear form. The t, variables \nin our linear periodic schedule form can be related to the A matrix as 7 = T. ~ + ~TranspOse x [(), 1,, \n. .,T _ l]mansp se (1) where K and T are N-element vectors. Intuitively kt corresponds to the iteration \nfrom which the first oc\u00adcurrence of i in the repetitive pattern comes, while AmanSPO e ~ [.. .]~anspose \nis the offset of that i from the start of the pattern. The T, K, and A matrices for Schedule B are de\u00adpicted \nin Figure 3. For example, the 1 in the 4th row, 3rd column of A indicates that i2 begins execution at \nthe end of the Schedule B s loop pattern (times [8, 11] in Figure 2). Notice that the A matrix specifies \nonly the time step (in the repetitive pattern) at which an instruction is zntttated. To determine resource \nrequirements with T = [0, 1, 3, 5, 7, ll]~an po , K = [0, O, 0, 1, 1, 2]manSp0se, and [100000~ 010100 \n / (= I:60ps X (T =4) 000000 I Figure 3: T, K, A, and U matrices for Schedule B non-pipelined function \nunits, we need to know not just when each instruction is znzttated, but also how long each executes. \nIf an instruction i with latency di is initiated at time step tin the repetitive pattern, it will execute \nuntil time step (t+ di 1)mod T. Since the FU is non-pipelined, no other instructions may use it for \nthis entire period. Intuitively this converts the modulo reservation table d which indicates only when \nan operation starts to the matrix U which indicates the entire time during which an operation uses a \nnon-pipelined FU, as illustrated in Figure 3 for Schedule B. Formally, U is derived from A as follows: \n(d, lj Ut,i = aL[t_(j mod ~j,i, v t c [O, T 1], and Yi E V E 1=0 (2) Notice that if all instructions \nhave delay d, = 1 cycle, then Ut,i = at,~ 3. Since, ~~= 01 at,, = 1 Ut,, E {O, 1}. To compute the resource \nrequirement at time step t, we need to add the ut, i s for all i. However since different instructions \nmay need different types of function units, we compute the resource requirement for each type of function \nunit separately. To accomplish this, we con\u00adsider Z(r): the set of all instructions that require a function \nunit of type r. If R, function units of type r are required for the schedule then, clearly, The ILP formulation \nattempts to minimize a weighted sum of the R, for the various function unit types. The weight (or cost) \nassociated with FU type r is C, and may reflect for example, the criticality of the FU for the loop. \nIn this way it can act as a heuristic to help guide the ILP. The objective function, then, is Lastly, \nas shown in [9, 18, 23], the dependence con\u00adstraints specified by the DDG can be enforced on the schedule \nby where d~ is the latency of instruction i and mij is the dependence distance from z to j. Further, \nin or\u00adder to guarantee that each instruction is scheduled ex\u00adactly once in the repetitive pattern, we \nrequire addi\u00adtional constraints on the A matrix [9]. These con\u00adstraints are shown in Equations (9) and \n(10). The 3since clean pipelines can initiate a new operation in each cycle, the resource usage for an \ninstruction is, conceptually, for only one cycle. Hence m those cases, again, Ut,, = at,, . Time Step \nThe usage of FP units is shown in Figure 4(b). Note that the function unit used by i2 wraps around from \nTime Steps time 2 to O. This is a problem. At time 2, i2 begins 01 II 2 executing on the function unit \nthat was used by i4 at i4 .i4 Ii!i  EEEl (a) (b) Figure 4: A Repetitive Pattern and its (partial) \nRe\u00adsource Usage complete integer programming formulation is shown below. The formulation minimizes the \nobjective func\u00adtion subject to the dependency constraints of Equa\u00adtion (8) and the resource constraints \nof Equation(5). [ILP Formulation for Non-Pipelined Architec\u00adtures] h 1 minimize ~ C, ~R. 7.=0 subject \nto E ut,, <Rr, VrGIO, h l], WE[0,7 l] (5) ~~l(r) (,d, -1) Ut,i = U((t_[)mod ~),, b?~[(),~-l],!f~~~ E \n1=0 (6) T= T.~+Am nSPoSex [0,1, . . ..l]ma SPoSeSe (7) tJ t%>di T.mij V(i, j) c E (8) T 1 a~,i=l di \nEIO,lV -1] (9) E t=o ti >0, ki > O,at,i ~ O and ~i,i >0 are integers (10) vi G[o, N-l], vtE[o, T-1] \n 4.2 Coloring Formulation for Fixed FU Assignment Unfortunately, the ILP formulation presented in the \nprevious section sometimes underestimates the number of function units required to execute a loop under \nfixed FU assignment. For inst ante, the schedule in Table 1 depicts FP operations in the loop kernel \n(times 7, 8, and 9) in boldface. Clearly at each of these times only two boldface FP instructions are \nexecuting. Since this loop kernel is repeatedly executed, we map times 7, 8, and 9 to 0, 1, 2 as shown \nin Figure 4(a). times O and 1. Since each instruction is supposed to use the same FU on every iteration, \nthis causes a problem at time O, when i2 is still executing on the FU needed by i4. The problem is that \nEquation 5 notes only the number of FU S in use at one time, i.e. the number of solid horizontal lines \npresent at each of the 3 time steps in Figure 4(b). This problem bears a striking similarity to the prob\u00adlem \nof assigning variables with overlapping lifetimes to different registers. In particular, it is a circular \narc col\u00adoring problem [10]. We must ensure that the two frag\u00adments corresponding to i2 get the same color, \na fact represented by the dotted arc in Figure 4(b). In addi\u00adtion the arcs of i2 overlap with both i3 \nand i4, meaning i2 must have a different color than either. Similarly i3 and i4 must have different colors \nthan each other. The trick is how to express the coloring problem in a format compatible with the rest \nof the ILP formulation. One natural way of representing this coloring is ut ~.+ ut,j 1 lCi-Cj l>-z (11) \n The u variables are merely the U matrix from Sec\u00adtion 4.1, which represents the resource usage pattern \nof Figure 4(b). The value of ut,$ is 1 if instruction i is executing at time t and is O if it is not. \nThe ut,~ is analogous for instruction j. Here the colors ci and Cj are the colors representing the FUS \nfor instructions i and j. Both c, and Cj are positive integers, and they must be dtfferent tf nodes i \nand j overlap during loop execution. If instructions i and j overlap, then both ~t)i and ut)j will be \n1 at those times where the overlap occurs. At such times the value of the right hand side is ~ = ~, thus \nforcing Ci and c, to be different, as desired. On the other hand, if instructions i and j never overlap, \nthen ut,i or ut,jor both will be O at all times t. This gives the right hand side a value of O or ~, \nand no matter the choice of colors (including ci = cj ), Inequality (11) holds ~ Alas, absolute value \nis not a linear operation, so Inequality 11 does not fit with the rest of our ILP framework. To surmount \nthis problem, we adopt an approach outlined by Hu [12]. We introduce a set of W.j,j integer, O-1 variables, \nwith one such variable for each pair of nodes using the same type of function unit. Roughly speaking \nthese W1,1 variables represent the sign of Ci Cj. Using them, Equation (11) can be represented with \nthe following equations: Ut,t + Ut,j -1 C% cj > N W<lj (12) 2 I<ck<lv vkE[o,N 1] (14) Time Steps 01 \nwhere IV, the number of nodes in the DDG, is an upper Stage 1 1 0 bound on the number of colors. Stage2 \n0 1 Stage 3 11  EIEE Theorem 4.1 Equatzons 12 -14 requare that two (b) (T= 2) nodes i and j be asszgned \ndtfferent colors (mapped to different functzon untts) Zf and only if they overlap. For a proof, please \nsee [2] For our ILP formulation we require that there be at least as many function units of each type \nas colors. Hence we replace Equation 5 with: R, > C, vi &#38;I(r), Yr E [O, h 1] (15) The complete ILP \nformulation with fixed FU assign\u00adment (i.e. for Problem 1) is obtained by adding Equa\u00adtions (12), (13), \nand (15) to the formulation in Sec\u00adtion 4.1 and eliminating Equation (5) as redundant. 5 Scheduling \nPipelines with Structural Hazards Having provided a formulation to solve Problem 1 s twin requirements \nof a rate optimal software pipelined schedule and fixed FU assignment for non-pipelined ex\u00adecution units, \nwe now extend that formulation to solve Problem 2 s requirement that this be done for func\u00adtion units \nwith structural hazards. 5.1 ILP Formulation In order to represent the usage of the various stages of \na pipeline by each instruction, we use reservation tables [15]. A reservation table has s stages and \nd columns, where d is the execution time of an instruction in the pipeline For simplicity sake we assume \nthat an FU of a particular type r has a single reservation table describing its use of various stages. \nThis assumption is only to simplify discussion and is Dot a limitation of the formulation, as will be \ndiscussed at the end of this section. The number of time steps over which a particular stage s in a pipeline \nis used (not necessarily continuously) during the execution of an instruction is denoted by d,. Clearly, \nfor fixed FU assignment, T ~ cl,. Further, it is necessary that no stage in a pipeline is used by an \ninstruction at two time steps that are equal mod T. Such a constraint, known as the modulo scheduling \nconstraint [5, 11, 19], is necessary for fixed FIT assignment. In the subsequent discussion we assume \nthat the modulo scheduling constraint is always satisfied for the given T. Cases where the mod-UIO scheduling \nconstraint is not satisfied is beyond the scope of this paper. In the roughly 10?10 of cases where the \nmodulo scheduling constraint is violated, we apply Figure 5: Modified Reservation Tables the technique \ndescribed in [8]. Due to space restric\u00adtions, we omit details of that technique here. For each function \nunit whose execution time c1 < T, we extend the reservation table of the function unit to T columns by \nadding (T d) zero (column) vectors. Since d < T, each entry in the rnodtjied reservation ta\u00adble is at \nmost 1. When d > T, we fold the reservation table for the function unit to T columns such that the t-th \ncolumn in the original reservation table is merged with the (t mod T)-th column in the modified reserva\u00adtion \ntable. The merge operation results in adding the respective entries in the two columns. Use of the mod-U1O \nscheduling constraint guarantees that each entry in the modified reservation table is at most 1. As an \nexample, the FP unit whose reservation table is in Fig\u00adure 2(a) has execution time d = 3. Figure 5 shows \nits modified reservation table when T = 2 and T = 4. The s-th row in the modified reservation table of \nfunction unit type r specifies the usage of stage s. Let us denote this row by RT~, a vector of length \nT. An entrY RT; [t] is 1 if stage s is required t time steps af\u00ad ter the initiation of an instruction \n(mod T ). Using this vector, and the A matrix we can define the resource usage matrix U for pipelines \nwith hazards. Unlike in the non-pipelined case, here the resource usage matrix [J is defined for each \nstage s of the pipeline. How\u00adever, as before, Us is a 2-dimensional matrix defined over all time steps \nin the repetitive pattern and for all instructions that execute on this FU type. (T-1) (16) S[t)i] = \n~ ((t-l) mod T),i Tj[i], 1=0 There is a similarity between Equations (2) and (16). The introduction \nof RT~ facilitates selectively choos\u00ading appropriate az,, s. A randomly generated A matrix with two instructions \nand its corresponding U3 matrix for stage 3 of the FP unit (see Figure 5(a)) are shown in Figure 6. The \nRT vector is also provided for clarity. Using the resource usage matrix defined by Equa\u00adtion 16, the \nresource requirement of st age s in function unit type r at time step tin the repetitive pattern is R;(i) \n= ~ Us[t, i] iEZ(r) Thus the number of FUS of type r required for the schedule is computed by taking \nthe maximum of R; (t) [ILP Formulation for Pipelines with Hazards] /(rmspose = h 1  EEEml ~~:TranspOse \n= 0(1 [110 I us[t,i]T - 0\u00b0 = EEIHl Figure 6: Example A, RT~, and corresponding Us. over all time steps \nt in the repetitive pattern and over all stages s. That is, if R, is the number of FU S of type r required \nfor this schedule, then R. z R;(t) Vs E [0, (s. 1)] and VIE [O, T 1] (17) For the example shown in Figure \n6, it can be seen that stage 3 of the FP unit can be shared by the instructions corresponding to the \ntwo rows of U. As before, we minimize a weighted sum of Rr for each function unit of type r. Lastly, \nwe adopt the coloring formulation developed in Section 4.2 to ensure fixed FU assignment. The complete \nformulation for the software pipelining problem with structural hazards is shown in Figure 7. We briefly \nmention how our formulation can handle FUS with multiple reservation tables. This is impor\u00adtant, because, \nin practice, an FP unit may support many operations, such as FP-add, FP-muItiply, and FP-divide, each \nhaving a different resource usage pat\u00adtern. In this case, the resource usage table RT is as\u00adsociated \nwith the instruction rather than the function unit. That is, for each instruction i, we use the RT8S \nwhich is determined from the appropriate reservation table. 6 Experimental Results In this section we \nprovide results of experiments using our framework, Due to the lack of commercial proces\u00adsors containing \npipelined function units with hazards, we were forced to generate our own. However, we have tried to \nmake them as plausible as possible by matching the execution latency and throughput to the PowerPC\u00ad604 \narchitecture. In particular, we started with fully pipelined function units as in the real 604, but added \nan extra hazard use at time step 2 in Stage 1 of the Load unit and at time step 2 of Stage 4 of the FP \nMult unit as depicted in Table 3. We re-emphasize that since our approach handles hazards, architects \nof future systems may opt to save chip space by using minimize ~ C. Rr r=(l subject to ~ U [t,i] < R, \n(18) iEz(? ) vsE[o+sr l], vtE[o, T ], and VTC [O)h -I] c~< Rr Vi EZ(r), and r E [O,h 1] (19) U [t, i]+u \n[tjj] 1 _Nw,, Ci cj > (20) %J 2 U [t, i] + U [t, j] 1 CJ Ct >  N(l wij)(21) 2 Vijj EZ(r), Vt E [0,2 \n 1], and VrE [O, h 1] T = T. ~ +Aman po e x [0, 1, . . . . T_ l]fi wose (22) T 1 E a,,i=l WEIO,N-1] \n(23) t=o tj t~>di T.mij V(i, j) c E (24) (T-1) U [t,i]= ~ U((t_/) }~od T),, R~;[O (25) 1=0 VtEIO, T \nl], Vi EZ(r), VsEIO, sr l], rE [Oh-l] Figure 7: ILP formulation for Pipelines with Hazards pipelines \nwith hazards. As in the real PowerPC-604, the FP divide unit is non-pipelined with latency 17. Other \nunits remain fully pipelined. The Integer ALU has a latency of 1, the Floating Point Add and the Store \nunits are assumed to be fully pipelined with la\u00adtency 3. Finally, there are assumed to be 2 Integer ALU \ns and one each of the other types. We ran our experiments on a set of 1066 single-basic\u00adblock inner loops \ntaken from SPEC92 (integer and floating point), linpack, l:ivermore, and the lJAS kernels. The DDG s \nfor the loops were obtained by instrumenting a highly-optimizing research compiler. The DDG s varied \nwidely in size from 1 node to 1301 nodes with a median of 10 nodes, a geometric mean of 12, and an arithmetic \nmean of 19. To solve the ILP s, we used the commercial program, CPLEX. In order to deal with the fact \nthat our ILP approach Load Units FP Mult Units Table 3: Reservation Tables used in the experiments \ncan take a very long time on some loops, we adopted the following approach. First, we limited CPLEX to \n3 minutes in trying to solve any single ILP, i.e. a maxi\u00admum of 3 minutes was allowed to find a schedule \nat a given T. Second, for each loop, at most 6 different T values, from [Tt~, Tlb + 5], were tried until \na schedule was found. Using this approach and these parameters, we were able to obtain optimal solutions \nin the large majority of cases. However, in the future we plan to more systematically investigate how \nthe values of these parameters effect the optimality of the results. To be specific, our approach found \nan optimal sched\u00adule T = T min for at least 75910 of the loops. In fact, the optimal schedule is at the \nlower bound (T = Tlb) of what is possible in 69~0 of the loops as summarized in Table 4. As the gap indicates, \n74 or 670 of the schedules were Tmin but not Tl~. That is there was no feasible so\u00adlution to the ILP \nat T~b meaning that the lower bound was not tight in those cases. This fraction is similar to what others \nhave found [13, 16] for function units without structural hazards. The third column of Ta\u00adble 4 is the \narithmetic mean of the number of nodes in the DDG. As indicated by the third column our ILP approach \ndoes produce good schedules when the mean number of nodes in the DDG is less than 20. Table 5 indicates \nhow far the schedule found was from a possible Tmzn schedule. We say a posszble TmZn here since there \nis no evidence. CPLEX 3 minute time limit expired without indicating whether or not a schedule exists \nfor this Tmln period. As can be seen from Table 5, for 51 loops our approach found sched\u00adules, but cannot \ntell whether these are optimal. Another important consideration in evaluating these schedules is the \nnumber of registers they require. More than 85yo have fewer than 64 variable lifetimes while 8170 have \nfewer than 32 variable lifetimes. These num\u00adbers ignore the fact that some of the variable lifetimes \nmight be able to share the same physical register. In addition, they lump together both integer and floating \npoint values. Note that we can easily include register requirements in the ILP framework as demonstrated \nin [9], but for simplicity we choose not to do so in this Number Initiation Mean # Nodes of Loops Interval \nin DDG 735 T=Tlb 6 85 T= T/b+l 10 20 T= Tlb+2 16 4 T= T/b+3 18 11 T= T(b+4 17 5 T= T{b+5 Table 4: Scheduling \nPerformance for Schedules Found within Time Limit T Tmin 011 \\213\\ 415 #of Schedules ]785] 301518 15 \n13 Table 5: Difference between T and Tmin for Schedules Found within Time Limit discussion. 4 How long \ndid it take to get these schedules? The median time was 2 seconds, while the geometric mean was 5 seconds. \nFull results are summarized in Table 6. The times to find a schedule in Table 6 may not be con\u00adsidered \nfast enough for some real compilers. However, we noticed in the course of our experiments that many loop \nbodies occur repeatedly in different programs. To be precise we found that out of our set of 1066 loops, \nonly 455 are unique. One loop body was common to 73 different loops! This being the case, the compiler \ncould use our ILP approach to precompute optimal schedules for the most commonly occurring loops. This \nscheme could also be tailored to individual users by adding new loops to the database as the compiler \nencounters them. In fact, the ILP computation could be run in the background, so that the user may get \nnon-optimal code the first time his/her code is compiled, but on later compilations the desired schedule \nwould be in the database. 7 Related Work Software pipelining has been extensively studied [1, 4, 5, \n7, 13, 16, 17, 18, 20, 21, 24, 26, 2 i ]. Rau and Fisher provide a comprehensive survey of these works \nin [19]. As stated in [19], software pipelining methods vary in several aspects: (1) whether or not they \ncon\u00adsider finite resources, (2) whether they model simple 4Because of the way our compiler performed \nmemory dkam\u00adbiguation, some of our DDG s contain edges with very large de\u00adpendence dktances-more than \nthe number of iterations in tbe loop. Such edges result in very large, but fictitious register re\u00adquirements. \nTime (in Sec.) I <1 I 1-2 I 2-5 I 5-10 I 10-20 I 20-30 I 30-60 I 60-120 I 120-240 I 240-300 300-600 >600 \nNumber of Loops I 345 I 79 I 137 ) 28 I 39 13/57/50 111 I s,= Table 6: Time (in seconds) required for \nscheduling or complex resource usage, (3) whether the algorithm is one-pass, iterative, incremental, \nor exhaustive search based. In particular, in terms of resource constraints, the work reported in [1, \n24, 18], does not consider any resource constraint while the methods reported in [7 , 11, 17, 20, 26, \n27] deal with function unit con\u00adstraints but with simple resource usage. Both function unit and register \nresource constraints are considered in [13, 5]. Software pipelining methods for complex us\u00adage patterns \nwith limited function units was dealt with by [4, 16]. The methods proposed in these works that aim to \nbe implementable in a practical compiler all use heuristic approaches. In this paper, we develop a clear \nmathematical for\u00admulation of the software pipelining problem. The use of an integer programming method \nmay or may not be acceptable in a practical compiler. The proposed method is important for performance-critical \nkernel loops and is also useful in the context of a scheduling testbed where our optimal scheduling method \ncan be used to judge how well other existing/newly proposed heuristics perform, and hence to improve \nthem. In [9] we compared three heuristic algorithms with an ILP scheduling method for clean pipelines, \nIn [6] Feautrier independently gave an ILP formulation similar to our earlier work. Our method is unique \nin that it com\u00adbines scheduling and mapping in a unified framework and attempts to achieve an optimal \nsolution. An ad\u00advantage of our method is that it can be extended to handle multi-function pipelines as \nwell. It can incorpo\u00adrate minimizing buffers (logical registers) as in [18] or minimizing the maximum \nnumber of live values at any time step in the repetitive pattern, as in the method proposed by Eichenberger, \nDavidson and Abraham [5].  8 Conclusions In this paper we have proposed a method to formulate scheduling \nand mapping problems in a unified frame\u00adwork. Our method can handle non-pipelined execu\u00adtion units and \npipelines with structural hazards. The schedules obtained are optimal (in terms of initiation interval \n) and run with a minimum number of function units (for a fixed FU assignment of each instruction in the \nloop). The proposed method can also handle multi-function pipelines. We have implemented our scheduling \nmethod and run experiments on a set of 1066 loops taken from various benchmark suites such as SPEC92, \nthe NAS kernels, linpack and livermore. Our initial experiments with scheduling these loops on an extended \nPowerPC-604 architecture were encourag\u00ading, yielding rate-optimal schedules for more than 75~o of the \nloops. The scope of periodic schedules and the ILP-based solution framework can be extended to a large \nvariety of loops with conditionals. For example, conditionals can be handled by techniques discussed \nin [27], while code size reduction can be achieved when using special hardware support such as rotating \nregister files and predicated execution [21]. Furthermore, register con\u00adstraints can be integrated in \nthe framework as demon\u00adstrated in [9]. While the use of integer programming methods in practical compilers \nmay not always be acceptable, we discovered that many loop bodies are common to a large number of loops, \nmaking it possible to pre-compute optimal schedules for these loops. This database could also be automatically \nupdated with an individual user s loops. Ii addition the ILP ap\u00adproach can be used to evaluate other \nheuristic methods, and also to derive optimal schedules for performance\u00adcritical applications where the \nuser is willing to pay a high compilation cost for savings in runtime. An alternative to ILP is to enumerate \nall possible schedules. For some loops this works well our initial results suggest that 75 %0 of loops \nhave a relatively fi\u00adnite number of schedules. However, efficient enumera\u00adtion of schedules is a significantly \nmore demanding task in terms of programming effort, both because of the al\u00adgorithms required and the \nlack of off-the-shelf software to assist the effort. Many loops have a large number of optimal schedules \n but for those that do not, enumer\u00adation can take considerably more time than the ILP approach to find \nan optimal schedule Note that as soon as an optimal schedule is found, both approaches normally stop, \neven if all schedules have not been enu\u00admerated. In the future we plan to investigate the use of constraint \nprogramming languages for performing soft\u00adware pipelining. Acknowledgements Kemal Ebcio~lu, Mayan Moudgill, \nand Gabriel M. Sil\u00ad berman were instrumental in completing this paper. We wish to thank Qi Ning and \nthe anonymous referees for their helpful suggestions. We are thankful to IBM for its tech\u00ad nical support, \nand acknowledge the Natural Science and Engineering Research Council (NSERC) and MICRONET, Network Centres \nof Excellence, support of this work.  References [1]A. Aiken and A. Nicolau. Optimal loop parallelization. \nIn Proc. of the SIGPLAN 88 Conf. on Programming Language Design and Implementat~on, pages 308-317, Atlanta, \nGeorgia, Jun. 22 24, 1988. [~1E.R Altman. Two Approaches for Optimal Software Pipelzning with Resource \nConstraints (In Preparation). PhD thesis, McGill U., Montr+al, Qu6., 1995. [3] R. P. Colwell, R. P. Nix, \nJ. J. O Donnell, D. B. Pap\u00adworth, and P. K. Rodman. A VLIW architecture for a trace scheduling compiler. \nIEEE Trans. on Computers, pages 967-979, Aug. 1988. [4] J. C. Dehnert and R. A. Towle. Compiling for \nCydra 5. Journal of Supercomprding, 7:181-227, May 1993. [5] A. E. Eichenberger, E. S. Davidson, and \nS. G. Abra\u00adham. Minimum register requirements for a modulo schedule. In Proc. of the 27th Ann. Intl, \nSymp. on Iifwroarchitecture, pages 75-84, San Jose, Calif., Nov. 30-Dec.2, 1994. [6] P. Feautrier. Fine-grain \nScheduling under Resource Constraints. In Seventh Annual Workshop on Lan\u00adguages and Compilers for Parallel \nComputing, Ithaca, USA, August 1994. [7] F. Gasperoni and U. Schwiegelshohn. Efficient algo\u00adrithms for \ncyclic scheduling. Res. Rep. RC 17068, IBM T. J. Watson Res. Center, Yorktown Heights, 1991. [8] R. Govindarajan, \nE. R. Altman, and G. R. Gao. Co\u00adscheduling hardware and software pipelines. ACAPS Technical Memo 921 \nSchool of Computer Science, McGill University, Montreal, Que., 1995. [9] R. Govindarajan, E. R. Altman, \nand G. R. Gao. Llinimizing regisler requirements under resource\u00adconstrained rate-optimal software pipelining. \nIn Proc. Of the Ann. Intl. Symp. on Microarchitecture, Y 7th pages 85-94, San Jose, Calif., Nov, 30-Dec,2, \n1994. [IO] L. J. Hendren, G. R. Gao, E. R. Altman, and C. Muk\u00aderji. A register allocation framework \nbased on hier\u00adarchical cyclic interval graphs. In U. Kastens and P. Pfahler, editors, Proc. of the Intl. \nL onf. on Com\u00adpder Construction, number 641 in Lee. Notes in Comp. Sci., pages 176-191. Springer-Verlag, \nOct. 1992. [11] P.Y.T. HSU. Highly concurrent scalar processing. Technical report, tJniversity of Illinois \nat Urbana-Champagne, Urbana, IL, 1986. Ph, D. Thesis, [12] T. C. Hu. Integer Programming and Network \nFlows, page 270. Addison-Wesley Pub. Co., 1969. [13] R. A. Huff. Lifetime-sensitive modulo scheduling. \nIn Proc. of the SIGPLAN 93 C onf. on Programming Lan\u00adguage De.szgn and Implementation, pages 258 267, \nAl\u00adbuquerque, N. Mex., Jun. 23 25, 1993. [14] IBM/Motorola. PowerPC 604 RISC Microprocessor Technical \nSummary, 1994. [15] P. M. Kogge. The Architecture of Pzpelzned Computers. l!vIcGraw-Hill Book Company, \nNew York, N. Y., 1981. [16] M. Lam. Software pipelining: An effective scheduling technique for VLIW machines. \nIn Proc. of the SIG-PLAN 88 Conf. on Programming Language Design and Implementation, pages 318 328, Atlanta, \nGeorgia, Jun. 22-24, 1988. [17] S-M. Moon and K. Ebcio~lu. An efficient resource\u00adconstrained globaf scheduling \ntechnique for superscalar and VLIW processors. In Proc. of the l?5th Ann, lntl. Symp. on Microarchitecture, \npages 55-71, Portland, Ore., Dec. 1-4, 1992. [18] Q. Ning and G. R. Gao. A novel framework of register \nallocation for software pipelining. In Conf. Rec. of the Twentieth Ann. ACM SIGPLAN-SIGACT Symp. on Prmctples \nof Programming Languages, pages 29 42, Charleston, South Carolina, Jan. 10-13, 1993. [19] B. R. Rau \nand J. A. Fisher. Instruction-level parallel processing: History, overview and perspective. J. of Supercomputing, \n7:9-5o, May 1993. [20] B. R. Rau and C. D. Glaeser. Some scheduling techniques and an easily schedulable \nhorizontal ar\u00adchitecture for high performance scientific computing. In Proc. of the l~th Ann. Microprogramming \nWork., pages 183-198, Chatham, Mass., Oct. 12 15, 1981. [21] B. R. Rau, M. Lee, P. P. Tirumalai, and \nM. S. Schlansker. Register allocation for software pipelined loops. In Proc. of the SIGPLAN 92 Conf. \non Program\u00adming Language Destgn and Implementation, pages 283-299, San Francisco, Calif., Jun. 17 19, \n1992. [22] B. R. Rau. Iterative modulo scheduling: An algo\u00adrithm for software pipelining loops. In Proc. \noj the 27th Ann. Intl. Symp. on Mzcroarchitecture, pages 63 74, San Jose, Calif., Nov. 30-Dec.2, 1994. \n[23] R. Reiter. Scheduling parallel computations. J. of the .4 CM, 15(4):590-599, Oct. 1968. [24] V. \nVan Dongen, G. R. Gao, and Q. Ning. A polyno\u00admial time method for optimaJ software pipelining. In Proc. \nof the Conf. on Vector and Parallel Processing, CONPAR-92, number 634 in Lee. Notes in Comp. Sci., pages \n613-624, Lyon, France, Sep. 1-4, 199 2. [25] S. R. Vegdahl. A Dynamic Programming Technique for Compacting \nLoops. In Proc. of the 25th Ann. Intl. Symp. on Microarchitecture, pages 180-188, Portland, Ore., Dec. \n1 4, 1992. [26] J. Wang and E. Eisenbeis. A new approach to software pipelining of complicated loops \nwith branches. Res. rep., INRIA, Rocquencourt, France, Jan. 1993, [27] N. J. Warter, J. W. Bockhaus, \nG. E. Haab, and K. Subramanian. Enhanced modulo scheduling for loops with conditional branches. In Proc. \nof the 25th Ann. 1nt2. Symp. on Mtcroarchitecturej pages 170-179, Port\u00adland, Ore., Dec. I 4, 1992. \n\t\t\t", "proc_id": "207110", "abstract": "<p>Recently, software pipelining methods based on an ILP (Integer Linear Programming) framework have been successfully applied to derive rate-optimal schedules for architectures involving clean pipelines - pipelines without structural hazards. The problem for architectures beyond such clean pipelines remains open. One challenge is how, under a unified ILP framework, to simultaneously represent resource constraints for unclean pipelines, and the assignment or mapping of operations from a loop to those pipelines. In this paper we provide a framework which does exactly this, and in addition constructs rate-optimal software pipelined schedules. The proposed formulation and a solution method have been implemented and tested on a set of 1006 loops taken from various scientific and integer benchmark suites. The formulation found a rate-optimal schedule for 75% of the loops, and required a median time of only 2 seconds per loop on a Sparc 10/30.</p>", "authors": [{"name": "Erik R. Altman", "author_profile_id": "81100139431", "affiliation": "Dept. of Electrical Engineering, McGill University, Montreal, H3A 2A7, CANADA", "person_id": "P78695", "email_address": "", "orcid_id": ""}, {"name": "R. Govindarajan", "author_profile_id": "81100039585", "affiliation": "Dept. of Computer Science, Memorial Univ. of Newfoundland, St. John's, A1B 3X5, CANADA", "person_id": "PP31077886", "email_address": "", "orcid_id": ""}, {"name": "Guang R. Gao", "author_profile_id": "81100134147", "affiliation": "School of Computer Science, McGill University, Montreal, H3A 2A7, CANADA", "person_id": "P100128", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/207110.207128", "year": "1995", "article_id": "207128", "conference": "PLDI", "title": "Scheduling and mapping: software pipelining in the presence of structural hazards", "url": "http://dl.acm.org/citation.cfm?id=207128"}