{"article_publication_date": "06-01-1995", "fulltext": "\n Register Allocation Using Lazy Saves, Eager Restores, and Greedy Shuffling Robert G. Burger Oscar Waddell \nR. Kent Dybvig Indiana University Computer Science Department Lindley Hall 215 Bloomington, Indiana 47405 \n{burger, owaddell,dyb) @cs.indiana.edu Abstract This paper presents a fast and effective linear intraprocedu\u00ad \nral register allocation strategy that optimizes register usage across procedure calls. It capitalizes \non our observation that while procedures that do not cent ain calls (syntactic leaf routine~) account \nfor under one third of all procedure ac\u00ad tivations, procedures that actually make no calls ( eflectiue \nleaf routines) account for over two thirds of all procedure activations. Well-suited for both caller-and \ncallee-save reg\u00ad isters, our strategy employs a lazy save mechanism that avoids saves for all effective \nleaf routines, an eager restore mechanism that reduces the effect of memory latency, and a greedy register \nshuffling algorithm that does a remarkably good job of minimizing the need for temporaries in setting \nup procedure calls. Introduction Register allocation, the complex problem of deciding which values will \nbe held in which registers over what portions of the program, encompasses several interrelated sub-problems. \nPerhaps the most well-known of these is to decide which vari\u00adables to assign to registers so that there \nis no conflict [5]. Another involves splitting live ranges of variables in order to reduce conflicts. \nThese problems have been addressed for both intraprocedural and interprocedural register allocation. \nOptimizing register usage across procedure calls is also an important problem, but up to now it has been \naddressed primarily in terms of interprocedural analysis. In this paper we describe a fast and effective \nlinear in\u00adtraprocedural register allocation strategy that optimizes reg\u00adister usage across procedure \ncalls. In conjunction with lo\u00adcal register allocation, this strategy results in performance within range \nof that achieved by interprocedural register al\u00adlocation. Furthermore, it is successful even in the presence \nof anonymous procedure calls, which typically cause inter\u00adprocedural register allocation techniques to \nbreak down. This material is based in part upon work supported under National Science Foundation Grant \nCDA-9312614 and a NSF Graduate Re\u00ad search Fellowship. Permission to copy without fee all or part of \nthis material is granted provided that the copies are not made or distributed for direct commercial advantage, \nthe ACM copyright notice and the title of the publication and its date appear, and notice is given that \ncopying is by permission of the Association of Computing Machinery.To copy otherwise, or to republish, \nrequires a fee and/or specific permission. SIGPLAN 95La Jolla, CA USA Q 1995 ACM 0-89791 -697-2/95/0006 \n...$3.50 Our compiler dedicates a set of registers to be used to hold procedure arguments, including \nthe actual parameters and ret urn address. Any unused registers (including regis\u00adt ers cent aining non-live \nargument values) are available for intraprocedural allocation, both for user variables and com\u00adpiler \ntemporaries. Other registers are used for local register allocation and to hold global quantities such \nas the stack pointer and allocation pointer. Three costs must be minimized in order to optimize reg\u00adister \nusage across procedure CSUS: the cost of saving live registers around calls, the cost of restoring saved \nregisters before they are used, and the cost of shuffling argument registers when setting up the arguments \nto a call, some of which may depend upon the old argument values. It is easy for these costs to exceed \nthe benefits of register allocation, especially with many registers dedicated to procedure argu\u00adments. \nWe analyzed the run-time call behavior for a large va\u00adriety of Scheme programs. While procedures that \ncontain no calls (syntactic leaf routines) account for under one third of all procedure activations, \nprocedures that actually make no calls ( efiectiue leaf routines) account for over two t birds. We capitalize \non this fact by using a laz y save mechanism that avoids saves for all effective leaf routines. In addition, \nwe reduce the effect of memory latency by employing an tea\u00adger restore mechanism. Although this approach \nsometimes introduces unnecessary restores, we found that a lazy mech\u00adanism did not improve performance \nand added considerable compile-time overhead. Finally, we employ a greedy reg\u00adister shuffling algorithm \nthat does a remarkably good job of minimizing the need for temporaries in setting up procedure calls. \nSection 2 presents our lazy save, eager restore, and greedy shuffling algorithms. Section 3 describes \nour implement a\u00ad tlon.of the algorithms. Section 4 discusses the performance , ch ara.tit eristics of \nour implementation. Section 5 describes rdatkd work. Section 6 summarizes our results and discusses possibilities \nfor future work. 2 Save and Restore Placement For purposes of discussion we describe our strategy for \nsave and restore placement in terms of caller-save registers and the simplified language of expressions \nbased on Scheme [8] below. In Section 2.4 we explain how our st rat egy applies to callee-save registers. \n Benchmark Lines Description Benchmark Calls Breakdown 1 Compiler 30,000 Chez Scheme recompiling itself \nCompiler 33,041,034 i { DDD 15,000 hardware derivation system [3] DDD 86,970,102 deriving Scheme machine \n[4] Similix 33,891,834 / 1 Similix 7,000 self-application of the SoftScheme 11,153,705 Similix [2] partial \nevaluator boyer 914,113 SoftScheme 10,000 Wright s soft t yper [15] browse 1,608,975checking a 2,500 \nline program 3 cpst ak 159,135 ctak 302,621 Table 1. dderiv 191,219 destruct 236,412 E-+x div-iter \n1,623 + true div-rec 140,738 4 false -+ call fft 677,886 + (seq El Ez)  fprint 43,715 + (if El E2 Es) \n fread 23,194 I fxt ak 63,673 done, so there are no assignment expressions. All constants fxtriang 5,817,460 \nare reduced to either true or false. For simplicity, we ignore We assume that assignment conversion has \nalready been puzzle 912,245 the operator and operands of procedure calls by assuming t ak 111,379 they \nhave been ordered in some way ancl placed in a series of seq expressions whose last entry is call. t \nald 537,205 t akr 111,380 2. I Lazy Save Placement tprint 41,940 t raverse-init 1,268,249 t remes: the \ncallee-save registers used in a procedure are traverse 7,784,102 saved on entry, whereas the caller-save \nregisters live after The natural strategy for save placement involves two ex\u00ad triang 1L790)492 ~ a call \nare saved right before the call. The natural callee\u00adsave strategy saves too soon in the sense that it \nmay intro-Average duce unnecessary saves when a path through the procedure does not use the registers, \nThenatural caller-save strategy saves too late in the sense that it may introduce redundant Table 2. \nDynamic call graph summary saves when a path contains multiple calls. Our unified strat\u00adegy optimises \nthe Placement of registers saves between these m syntactic leaf nodes two extremes for both callee-and \ncaller-save registers. m non-syntactic leaf nodesWe tooled the Chez Scheme compiler to insert code to \nm non-syntactic internal nodes count procedure activations in a variety of programs and found that syntactic \nleaf routines (those that contain no m syntactic internal nodes callsl) on average account for under \none third of all acti\u00advations. We then retooled the compiler to determine how many activations actually \nmake no calls. These eflective leaj routine~ account for an average of more than two thirds We then present \nan improved algorithm that handles these of procedure activations. Ourlazy save strategy thus caters \ncases. to effective leaf routinee, saving registers only when a call is inevitable. Because of assignment \nconversion, variables 2.1.1 A Simple Save Placement Algorithmneed to be saved only once. In order to \nminimize redundant saves, therefore, our strategy saves registers as soon as a We first define the function \nS[l?], the set of registers that call is inevitable. Table 2 gives the results of our measure\u00adshould \nbe saved around expression E, recursively on the mentsfor a set of benchmarks described in Table 1 and \nfor a structure of our simplified expressions:Scheme version of the Gabriel benchmark suite [10]. Effec\u00adtive \nleaf routines are classified as syntactic and non-syntactic S[z]= 0 leaf nodes. Non-syntactic internal \nnodes are activations of S[true] = 0 procedures that have paths without calls but make calls at S[faise] \n= 0 run time, and syntactic internal nodes are those that have ~[cd] = {T I T is live after the caU} \nno paths without calls. S[(seq El E,)] = S[E] U S[-EZ] First we present a simple algorithm for determining \nlazy S[(if 1% E, Es)] = SIE1] U (S[.1%] n S[l%]) save placement. Next we demonstrate a deficiency involving \nshort-circuit boolean operations within if test expressions. We save register T around expression E \nif. r c S[E]. By 1Because tail calls in Scheme are essentially ~umps, they are not intersect ing S [Ez \n] with S [Es] in the if case, only those regis\u00adconsidered calls for this purpose. ters that need to be \nsaved in both branches are propagated, which yields a lazy save placement. The union operator in Next, \nconsider the two paths for which (if .EI Ez Es) the seq case places the saves as soon as they are inevitable. \nevaluates to true: El is true and E, is true, or El is false It can be shown that this placement is never \ntoo eager; i.e., and ES is true. Similarly, there are two paths for fa Ise, as if there is a path through \nany expression E without making a call, then S[-E]=O.  2.1.2 Short-Circuit Boolean Expressions Short-circuit \nboolean operations such as and and or are modeled as if expressions. As a result, if expressions nested \nin the test part occur frequently. Consider the expres\u00adsion (if (and z call) y call), which is modeled \nby (if (if z call false) g call). There is no path through this expres\u00adsion without making a call, so \nwe would like to save all the live variables around the outer if expression. Unfortunately, the above \nalgorithm is too lazy and would save none of the registers, regardless of which registers are live after \nthe calls: S[(if (if z call false) y call)] = S[(if z call false)] U (S[V] n S[call]) = (S[z] u (S[call] \nn S[false])) U (0 n S[call]) =Ou(S[dl]n O)u O = 0 To correct this deficiency, we must be sensitive to \ncondi\u00adtionals and constants, especially when they occur in a test context, 2.1.3 The Revised Save Placement \nAlgorithm The basic principle is to consider the paths through the con\u00adtrol flow graph. Along a given \npath we take the union of the registers that need to be saved at each node. Then we take the intersection \nof all the paths to determine the registers that need to be saved regardless of the path taken. In order \nto facilit ate this process, we define two functions recursively: St[E], the set of registers to save \naround E if E should eval\u00aduate to true, and Sj [E], the set of registers to save around E if E should \nevaluate to fa Ise. Register r is saved around E iff. r E S,[E] nS~[E]. The base cases are defined as \nfollows, where R is the set of all registers: S,[z] = 0 sf[z]=0 St[true] = @ Sf[true] = R &#38;[false] \n= R Sf [false] = @ St [call] = {r I r is live Sf [call] = {~ I 2-is live after the call} after the call} \nSince it is impossible that true should evaluate to false, and vice versa, we define these cases to be \nR so that any impossible path will have a save set of R, the identity for intersection. Thus, impossible \npaths will not unnecessarily rest rict the resuIt. Now we define the recursive cases. Intuitively, the \nset S~[(seq El Ez )] is the set of registers to save if the seq expression evaluates to true. There are \ntwo possible paths: El is true and E2 is true, or El is false and E2 is true. Thus, S,[seq] = (S,[J%] \nu S,[E2]) n (Sf[131] u &#38;[&#38;]) = (St[El] n Sj [El]) U St[-EZ]. The case for Sj [(seq El EZ )] is \nsimilar, as the diagram illustrates: El (seq El l%)tf S,[seq] = (S,[E1] n Sf[El]) U St[E,] Ez Sf [seql= \n(S,[EI] n Sf [EL]) u Sf[..E2] tf ! the diagram illustrates:  07 ft Stpf] = (S,[E1] u S,[EZ]) n (St[E,] \nU St[Es]) E2 s S~ [if] = (S,[lh] u Sf[E2]) n tf tf (S~[E,] U Sf[Es]) Our example A = (if (if z call \nfalse) y call) now yields the desired result. Let L be the set of live registers after A. Let II = (if \nz call false). S,[B] = (0u({Y}uL))n (0 uR) ={y}u L S,[A] = (St[B] u 0) n (sj[B] u L) =L s~[E!] ~ JOu({y} \nu L)) n(0u0) Sj[A] = (S,[B] u 0) n (Sj[B] u L) =L We see that although no registers would be saved around \nthe inner if expression (since S~[l?] n Sf [B] = 0), all the live registers would be saved around the \nouter if as desired. It is straightforward to show that the revised algorithm is not as lazy as the previous \nalgorithm, i.e., that S[E] ~ St[E] n Sf [E] for all expressions E. It can also be shown that the revised \nalgorithm is never too eager; i.e., if there is a path through any expression E without calls, then S~[E] \nn Sj[E] =0. Figure 1 shows the control graphs for not and the short\u00adcircuit boolean operators and and \nor and the derived equa\u00adtions for these operators. 2.2 Eager Restore Placement We considered two restore \nstrategies based on the question of how soon a register T should be restored: eager: as soon as T might \nbe needed, i.e., if T will possibly be referenced before the next call, and o lazy: as soon as T will \nbe needed, i.e., if T w~ certainly be referenced before the next call. We present three abbreviated control \nflow diagrams to demonstrate the differences in the two approaches. The reg\u00adister save regions are indicated \nby a rounded box. Calls are indicated by squares, references by circles, and restores by dashes across \nthe control flow lines. Control enters from the top. Figures 2a and 2b demonstrate how the eager approach \nintroduces potentially unnecessary restores because of the joins of two branches with different call \nand reference be\u00adhavior. Figure 2C shows an instance where even the lazy approach may be forced to make \na potentially unnecessary restore. Because the variable is referenced outside of its en\u00adclosing save \nregion, there is a path that does not save the variable. Consequently, the register must be restored \non exit of the save region. S,[(not l?)] = S,[(if E false true)] E = (st[E] u R) n (Sf[a] u 0) ~)tf Sf[(not \nE)] = = St[E] Sf[(if E false true)] h = (S,[,3] u 0) n (sj[E] u R) = S,[E] E1f W S,[(and El E2)] = S~[(if \n= (&#38;[EI] t = St[El] Sf [(and EI E2 )] = Sf [(if ESf W = (St[E1] t = (S,[E1] F EI + S,[(or El Ez \n)] = S~[(if = (St[El] f = Si[l?,] S~[(or El &#38;)]= Sf[(if Ez + = (St[EI] f = Sf[E1] F Figure 1.  \ncall eager lazy call eager, lazy ref  + 1 a. b. Figure 2. El Ez false)] u St[E,]) n (Sf[EI] u R) U \nS,[E,] El E2 false)] u S,[E,]) n (Sf[EI] u 0) U Sf[E,]) n S,[El] EI true Ez)] u 0) n (Sf [E,] u St[EZ]) \nn (Sf[EI] u St[E,]) El true E2)] u R) n (Sj[l%] u Sf[EZ]) U S1[ES] Aref c. In summary, the eager approach \nwould immediately re-2.4 Cnllee-Save Registers store any register possibly referenced before the next \ncall. It is straightforward and can be easily implemented in a single linear pass through the abstract \nsyntax tree. A bottom-up pass through the tree could compute the variable reference information and insert \nthenecessary restores in parallel. Be\u00adcause the restores occur early, they may reduce the effect of memory \nlatency. The lazy approach would restore whenever a reference is inevitable before the next call or when \nthe register is live on exit from the enclosing save region. Its main advantage is that most unnecessary \nrestores can be avoided. Unfor\u00adtunately, this approach is less straightforward and requires more passes \nthrough the abstract syntax tree. We implemented both approaches early on and found that the eager approach \nproduced code that ran just as fast as the code produced by the lazy approach. We concluded from this \nthat the reduced effect ofmemory latency offsets the cost of unnecessary restores. 2.3 Greedy Shuffling \nWhen setting up arguments to a call, the new argument val\u00adues may depend on the old argument values; \nthus, argument register shuffling is sometimes necessary. For example, con\u00adsider the call ~(y, z), where \nat the time of the call z is in argument register 41 and y in uz. In order to call j, y must be assigned \nto al and z to a~, requiring a swap of these two argument registers. By not fixing the order of evaluation \nfor arguments be\u00adfore register allocation, the compiler can determine an order\u00ading that requires a minimal \namount of shuffling, and some\u00adtimes it can avoid shuffling altogether. For example, the call j(z +1/, \nV+ 1, Y+z), where z is in register al, y is in register az, and z is in register as, can be set up without \nshuffling by evaluating y + 1 last. A left-to-right or right-to-left or\u00addering, however, would require \na temporary location for this argument. Because we do not fix the order of evaluation of argu\u00adments early \non, we cannot compute liveness information in a pass before the shuffling. Since register allocation \ndepends on liveness information, we must perform register allocation, shuffling, and live analysis in \nparallel. Our shuffling algorithm first partitions the arguments into those that contain calls ( complez) \nand those that do not (simple). We use temporary stack locations for all but the last of the complex \narguments, since making a call would cause the previous arguments to be saved on the stack any\u00adway. We \npick as the last complex argument one on which none of the simple arguments depend (if such a complex \nar\u00adgument exists), since it can be evaluated directly into its argument register. Ordering the simple \narguments is a problem of optimiz\u00ading parallel assignments, as noted in [9, 12]. We build the dependency \ngraph and essentially perform a topological sort. If we detect a cycle, we find the argument causing \nthe largest number of dependencies and remove it, placing it into a t em\u00adporary location, in hopes of \nbreaking the cycle. We then con\u00adtinue the process until all arguments have been processed. This greedy \napproach to breaking cycles may not always result in the minimal number of temporaries. We have ob\u00ad . \nserved that our algorithm, however, finds the optimal order\u00ading for a vast majority of call sites (see \nSection 3.1). We now describe how our lazy save strategy applies to callee\u00adsave registers. As we noted \nearlier, the natural register save placement strategy of saving on entry all callee-save regis\u00adters used \nin a procedure introduces unnecessary saves when a path through the procedure does not use the registers. \nOur effective leaf routine measurements indicate that Daths with\u00adout calls are executed frequently. Along \nthese paths caller\u00adsave registers can be used without incurring any save/restore overhead. Using caller-save \nregisters along paths without calls, our approach delays the use of callee-save registers to paths where \na call is inevitable. By adding a special caller-save return-address register, ret, the revised save \nplacement algorithm can be used to de\u00adtermine which expressions will always generate a call. This return \nre~ister is used to hold the return address of the cur\u00adrent procedure and must be saved and restored \naround calls. Consequently, if ret E St [E] U Sf [E], then E will inevitably make a call, but if ret \n@ St [E] U Sf [E], then E contains a path without any calls. Chow [6] describes a related technique called \nshrink wrapping to move the saves and restores of callee-save regis\u00adters to regions where the registers \nare active. His technique, however, is applied after variables have been assigned to callee-save registers. \nConsequently, his approach int reduces unnecessary saves and restores when it assigns a variable used \nalong a path without calls to a callee-save register when a caller-save register could be used instead. \nOur approach uses inevitable-call regions to guide the placement of vari\u00adables into registers and may \ndecide to keep a variable in a caller-save register until a call is inevitable, at which point it may \nbe moved into a callee-save register. 3 Implementation We allocate n registers for use by our register \nallocator. Two of these are used for the return address and closure pointer. For some fixed c < n 2, \nthe first c actual parameters of all procedure calls are passed via these registers; the remaining parameters \nare passed on the stack. When evaluating the ar\u00adguments to a call, unused registers are used when shuffling \nis necessary because of cycles in the dependency graph. We also fix a number 1< n 2 of these registers \nto be used for user variables and compiler-generated temporaries. The lazy save placement algorithm requires \ntwo linear passes through the abstract syntax tree. The first pass per\u00adforms greedy shuffling and live \nanalysis, computes St [E] and S~ [E] for each expression, and introduces saves. The second pass removes \nredundant saves. The eager restore placement algorithm requires one lin\u00adear pass. It computes the possibly \nreferenced before the next call sets and places the restores. This pass can be run in parallel with the \nsecond pass of the lazy save placement algorithm, so the entire register allocation process requires \njust two linear passes. 3.1 Pass 1: Save Placement The first pass processes the tree bottom-up to compute \nthe live sets and the register saves at the same time. It takes two inputs: the abstract syntax tree \n(T) and the set of registers live on exit from T. It returns the abstract syntax tree annotated with \nregister saves, the set of registers live on entry to T, St [Tl, and Sf [T1. Liveness information is \ncollected using a bit vector for the registers, implemented as an n-bit integer. Thus, the union operation \nis logical or, the intersection operation is 3.2 Pass 2: Save Elimination and Restore PIace\u00ad logical \nand, and creating the singleton {r} is a logical shift ment left of I for r bits. Save expressions are \nintroduced around procedure bodies and the then and else parts of if expressions, unless both branches \nrequire the same register saves. When we encounter a call, we use the following greedy shuffling algorithm: \n 1. We build the dependency graph for the register ar\u00adguments. Since calls destroy the argument registers, \nbuilding thedependency graph involves traversing the tree only down to calls. After thle order of evalua\u00adtionhas \nbeen determined, thearguments are traversed again by the current pass) which will visit the nodes only \none more time, since they require no shuffling. Thus, the overall pass is linear since all nodes in the \ntree are visited at most twice. 2. We partition the register arguments into those that do not cent ain \ncalls (simple) and those that do (complex). 3. We search through the complex arguments for one on which \nnone of the simple arguments depend. If one is found, it is placed onto the simple list. If not, the \nlast complex argument is placed onto the simple list. The remaining complex arguments are evaluated and \nplaced into temporary stack locations, since evaluation of complex arguments may require a call, causing \nthe previous arguments to be saved on the stack anyway. 4. We look for an argument in the simple list \nthat has no dependencies on the remaining argument registers. If we find one, we push it onto a to be \ndone last stack of arguments. Then we remove it from the de\u00adpendency graph and simple list and repeat \nstep 4 until all arguments have been assigned. Once all have been assigned, we evaluate the arguments \nin the to be done last stack directly into their argument registers. We conclude by assigning all the \nremaining argument reg\u00adist ers from the corresponding t empc)rary locations. 5. If all arguments have \ndependencies on each other, we greedily-pick the one that causes the most dependen\u00adcies and evaluate \nit into a temporary location, in hopes of breaking all the cycles. After removing it from the dependency \ngraph and simple list, we continue with step 4. We use other argument registers as tempo\u00adraries when \npossible; otherwise, we use the stack.  This algorithm is 0(n3 ), where n is the number of argu\u00adment \nregisters. Fortunately, n is fixed and is usually very small, e.g., six in our current implementation. \nConsequently, it does not affect the linearity of the pass. Furthermore, n is limited in practice by \nthe number of simple register argu\u00adments, and the operations performed involve only fast inte\u00adger arithmetic. \nFinding the ordering of arguments that min\u00adimizes the number of temporaries is NP-ccJmplete. We tried \nan exhaustive search and found that our greedy approach works optimally for the vast majority of all \ncases, mainly because most dependency graph cycles are simple. In our benchmarks, only 7% of the call \nsites had cycles. Furthe.r\u00adrnvre, the greedy algorithm was optimal for all of the call sites in all of \nthe benchmarks excluding our compiler, where it was optimal in all but six of the 20,245 call sites, \nand in these six it required only one extra temporary location. The second pass processes the tree to \neliminate redundant saves and insert the restores. It is significantly simpler than the first pass. It \ntakes three inputs: the abstract syntax tree (T), the current save set, and the set of registers possibly \nreferenced after T but before the next call. It returns two outputs: the abstract syntax tree with redundant \nsaves elim\u00adinated and restores added, and the set of registers possibly referenced before the next call. \nWhen a save that is already in the save set is encoun\u00adtered, it is eliminated. Restores for possibly \nreferenced reg\u00adisters are inserted immediately after calls. Our earlier example demonstrates why there \nmay be re\u00addundant saves. Let s assume that we have a procedure body of (seq (if (ifs call false) y call) \nz). Then the first pass of the algorithm would introduce saves as follows: (save (z) (seq (if (if z (save \n(s y) call) false) ~save (s) call)) x)) Two of the saves are redundant and can be eliminated. The result \nof the second pass would be: (save (z) (seq (if (if z (save (y) (restore-after call (z y))) false) ~restore-after \ncall (z))) x)) 4 Performance To assess the effectiveness of our register allocator we mea\u00adsured its \nability to eliminate stack references and its im\u00adpact on execution time for the set of programs described \nin Table 1 and for the Gabriel benchmarks. We also exam\u00adined the effect of our lazy save placement versus \nthe two natural extremes, early and late. Although the early strategy eliminates all redundant saves, \nit generates unnec\u00adessary saves in non-syntactic leaf routines. Because the late save strategy places \nregister saves immediately before calls, it handles all effective leaf routines well. This late strategy, \nhowever, generates redundant saves along paths with mul\u00adtiple calls. Our lazy strategy strikes a balance \nbetween the two extremes by saving as soon as a call is inevitable. Con\u00adsequently, it avoids saves for \nall effective leaf routines and at the same time eliminates most redundant saves. For the baseline and \ncomparison cases local register al\u00adlocation was performed by the code generator, eight regis\u00adters were \nglobally allocated to support our run-time model (which provides in-line allocation, fast access to free \nvari\u00adables, etc.), and our greedy shuffling algorithm was em\u00adployed. Thus our baseline for comparison \nis a compiler that already makes extensive use of registers. We were able to collect data on stack references \nby modifying our compiler to instrument programs with code to count stack accesses. Table 3 shows the \nreduction in stack references and CPU time when our allocator is permitted to use six argument registers. \nThe table also gives the corresponding figures for the early and late save strategies. On average, the \nlazy save approach eliminates 72~o of stack accesses and increases Table 3. Reduction of stack references \nand resulting .. 5Deedun for three different save st rat egies given six argument registers relative \nto the baseline of no argument registers. For both baseline and comparison cases local register allocation \nwas performed by the code generator, several registers were globally allocated to support the run-time \nmodel, and our greedy shuffling algorithm was employed. Lazy Save Early Save Late Save stack ref performance \nstack ref performance stack ref performance Benchmark reduction increase reduction increase reduction \nincrease Compiler 72% 30% 64% 26% 63% 13% DDD 68% 47% 59% 43% 63% 44% Similix 69% 26% 57% 18% 51% 9~o \nSoft Scheme 71% 22% 55% 14% 43% 5% boyer 66% 54% 49% 46% 60% 219 0 browse 72% 39% 60% 3570 70% 56% cpst \nak 77% 3570 59% 26% 77% 2070 ctak 71% 85% 50% 2070 70% 64% dderiv 56% 52% 46% 42% 44% 47% destruct 88% \n33% 82% 34% 87% 36% div-iter 100% 133% 80% 99% 99% 130% div-rec 77% 30% 65% 29% 76% 38% fft 71% 19% 71% \n2% 67y0 3% fprint 68% 15% 48% 9% 61% 16% fread 67% 39% 56% 22% 58% 24% fit ak 62% 35% 32% 14$ , 45% \n24% fxtriang 76% 43% 599 0 47% 767 0 15% puzzle 74% 34% 68% 32% 74% 34% t ak 72% 109% 52% 92% 50% 93% \nt akl 86% 67% 58% 41% 83% 67% t akr 72% 15% 52% 3% 50% 4% t print 63% 11% 41% 8% 56% 11% traverse-init \n76% 38% 70% 37% 74% 44% traverse 50% 29% 48% 28% 48% 30% triang 83% 41% 71% 32% 75% 48~o Average 72% \n43% 58% 32% 65% 36% cc -03 I gcc -03 I Chez Scheme Speedup 070 5% 14% Table 4. Performance comparison \nof Chez Scheme against the GNU and Alpha OSF/1 optimizing C compilers for t ak (26, 18, 9) benchmark \n(results are normalized to the Alpha OSF/1 compiler). Early Save Lazy Save Speedup Ca.llee cc -03 1.292s \n0.676s 91% save gcc -03 1.233s 0.772s 60% Caller-save by hand 0.990s 0.638s 55% Table 5. Execution times \nof optimized C code fort ak (26, 18, 9) using early and laz y save strategies for callee-save registers, \nand hand-coded assembly using lazy saves for caller-save registers. 136 run-time performance by 43 YO, \na significant improvement over both the early (58~0/32~0) and late (65~0/36Yo) save approaches. To compare \nour strategy against other register allocators, we timed Scheme-generated code for the call-intensive \ntak benchmark against fully optimized code generated by the GNU and Alpha OSF/1 C compilers. Table 4 \nsummarizes these results using the Alpha C compiler as a baseline. We chose the t ak benchmark because \nit is short and isolates the effect of register save/restore strategies fcm calls and because the transliteration \nbetween the Scheme and C source code is immediately apparent. Our Scheme code actually out\u00adperforms optimized \nC code for this benchmark despite the additional overhead of our stack overflc)w checks II 11 and ,.! \npoorer low-level instruction scheduling. Part of the performance advantage for the Scheme ver\u00adsion is \ndue to our compiler s use of caller-save registers, which turns out to be slightly better fcu this benchmark. \nThe remaining difference is due to the lazy save strategy used by the Scheme compiler versus the early \nsaves used by both C compilers. To study the effectiveness of our save strategy for both caller-and callee-save \nregisters, we hand-modified the op\u00adtimized assembly output of both C compilers to use our lazy save technique. \nIn order to provide another point of comparison, we hand-coded an assembly version that uses caller-save \nregisters. Table 5 compares the original C com\u00adpiler times with the modified versions and the hand-coded \nversion. In all cases the lazv save strateirv is clearlv bene\u00adficial and brings the perfor~ance of the \n&#38;llee-save C code within range of the caller-save code. To measure the effect of the additional register \nallocation passes on compile time, we examined profiles of the compiler processing each of the benchmark \nprograms and found that register allocation accounts for an average of 770 of overall compile time. This \nis a modest percentage of run time for a compiler whose overall speed is very good: on an Alpha 3000/600, \nChez Scheme compiles itself (30,000 lines) in un\u00adder 18 seconds. In contrast, the Alpha OSF/1 C compiler \nrequires 23 seconds to compile the 8,500 lines of support code used by our implement at ion. While 7% \nis acceptable overhead for register allocation, the com,piler actually runs faster with the additional \npasses since it is self-compiled and benefits from its own rezister allocator. We also ran the benc~marks \nwith several other variations in the numbers of parameters and user variables permitted to occupy registers, \nup through six apiece. Performance increases monotonically from zero through six registers, al\u00adthough \nthe difference between five and six registers is mini\u00admal. Our greedy shuffling algorithm becc~mes important \nas the number of argument registers increa~ies. Before we in\u00adst ailed this algorithm, the performance \nactually decreased after two argument registers. 5 Related Work Graph coloring [5] has become the basis \nfor most mod\u00adern register allocation strategies. Several improvements to graph coloring have been made \nto reduce expense, to de\u00adtermine which variables should receive highest priority for available registers, \nand to handle interprocedural register allocation. Steenkiste and Hennessy [14] implemented a combined \nintraprocedural and interprocedural register allocator for Lisp that assigns registers based on a bottom-up \ncoloring of a simplified interprocedural control flow gr aph. They handle cycles in the call graph and \nlinks to anonymous procedures bv introducing additional saves and restores at rxocedure call boundaries. \nUsing a combinatio n of int raprocedural and int erprocedural register allocation, they are able to eliminate \n 88% of st ack accesses: atmroxirnatelv 51~0 via intramocedu\u00ad .. ral register allocation and the remainder \nvia interprocedural allocation. Our figure of 72% appears to compare favorably since we do not perform \nany interprocedural analysis; differ\u00adences in language, benchmarks, and architecture, however, make direct \ncomparison impossible. Steenkiste and Hennessy found that an average of 36% of calls at run time are \nto (syntactic) leaf routines; this is similar to our findings of an average sightly below one third. \nThey did not identify or measure the frequency of calls to effective leaf routines. Chow and Hennessy \n[7] present an intraprocedural algo\u00adrithm that addresses certain shortcomings of straightforward graph \ncoloring. In their approach, coloring oft he register in\u00adterference graph is ordered by an estimate of \ntotal run-time savings from allocating a live range to a register, normal\u00adized by the size of the region \noccupied. For live ranges with equal total savings, priority goes to the shorter in hopes that several \nshort live ranges can be allocated to the same regis\u00adter. In order to improve procedure call behavior, \nincoming and outgoing parameters are pre-colored with argument registers. The priority-coloring algorithm \nis able to make effective use of caller-save registers for syntactic leaf proce\u00addures, preferring callee-save \nregisters for the rest. Chow [6] extends the priority-based coloring algorithm to an interprocedural \nregister allocator designed to mini\u00admize register use penalties at procedure calls. He provides a mechanism \nfor propagating saves and restores of callee\u00adsave registers to the upper regions of the call graph. In \nthis scheme, saves and restores propagate up the call graph until they reach a procedure for which incomplete \ninformation is available due to cycles in the call graph, calls through func\u00adtion pointers, or procedures \nfrom external modules. Such re\u00adstrictions render this approach ineffective for typical Scheme programs \nwhich rely on recursion and may make extensive use of first-class anonymous procedures. Chow also claims \nthat interprocedural register allocation requires a large num\u00adber of registers in order to have a noticeable \nimpact; the 20 available to his algorithm were inadequate for large bench\u00admarks. Clinger and Hansen [9] \ndescribe an optimizing compiler for Scheme which achieves respect able performance through a combination \nof aggressive lambda-lifting and parallel as\u00adsignment optimization. Lambda lifting transforms the free \nvariables of a procedure into extra arguments, decreasing closure creation cost and increasing the number \nof argu\u00adments subject to register allocation. Parallel assignment optimization then attempts to make \npassing arguments in registers as efficient as possible by selecting an order of eval\u00aduation that minimizes \nregister shuffling. Their shuffling al\u00adgorithm is similar to ours in that it attempts to find an ordering \nthat will not require the introduction of tempo\u00adraries but differs in that any cycle causes a complete \nspill of all arguments into temporary stack locations. Although [12] describes a register shuffling algorithm \nsimilar to ours, de\u00adtails regarding the selection of the node used to break cycles are not given. Shao \nand Appel [13, I.] have developed a closure con\u00adversion algorithm that exploits control and data flow \ninfor\u00admation to obtain extensive closure sharing. This sharing enhances the benefit they obtain from \nallocating closures in registers. Graph-coloring global register allocation with careful lifetime analysis \nallows them to make flexible and ef\u00adfective use of callee-and caller-save registers. Since the order \nof argument evaluation is fixed early in their continuation\u00adpassing style compiler, they attempt to eliminate \nargument register shuffling with several complex heuristics including choosing different registers for \ndirect-cdled functions. 6 Conclusions and Future Work In this paper we have described a fast linear intraprocedu\u00adral \nregister allocation mechanism based on lazy saves, eager restores, and greedy shuffling that optimizes \nregister usage across procedure calls. The lazy save technique generates register saves only when procedure \ncalls are inevitable, while attempting to minimize duplicate saves by saving as soon as it can prove \nthat a call is inevitable. We have shown that this approach is advantageous because of the high percent\u00adage \nof effective leaf routines. The eager restore mechanism restores immediately after a call all registers \npossibly refer\u00adenced before the next call. While a lazy restore mechanism would reduce the number of \nunnecessary restores, we found that restoring as early as possible compensates for unneces\u00adsary restores \nby reducing the effect of memory latency. The greedy shuffling algorithm orders arguments and attempts \nto break dependency cycles by selecting the argument caus\u00ading the most dependencies; it has proven remarkably \nclose to optimal in eliminating the need for register shuffling at run time. Our performance results \ndemonstrate that around 72~o of stack accesses are eliminated via this mechanism, and run-time performance \nincreases by around 43% when six registers are available for parameters and local variables. Our baseline \nfor comparison is efficient code generated by an optimizing compiler that already makes extensive use \nof global registers and local register allocation. This is within range of improvements reported for \ninterprocedural register allocation [14, 6]. Although the compiler now spends around 7% of its time on \nregister allocation, the compiler actually runs faster since it is self-compiled and benefits from its \nown register allocation strategy. Our effective leaf routine statistics suggest a simple strat\u00adegy for \nstatic branch prediction in which paths without C&#38; are assumed to be more likely than paths with \ncalls. Prelim\u00adinary experiments suggest that this results in a small (2 3~o) but consistent improvement. \nOther researchers have investigated the use of lambda lifting to increase the number of arguments available \nfor placement in registers [13, 9]. While lambda lifting can easily result in net performance decreases, \nit is worth in\u00advestigating whether lambda lifting with an appropriate set of heuristics such as those \ndescribed in [13] can indeed in\u00adcrease the effectiveness of our register allocator without sig\u00adnificantly \nincreasing compile time. Acknowledgement: TLe authors would like to thank Mike Ashley for his helpful \ncomments on an earlier version of this paper. References [1] Andrew W. Appel and Zhong Shao. Callee-save \nreg\u00ad isters in continuation-passing style. Lisp and Symbolic Computation, 5(3):191 221, 1992. [2] Anders \nBondorf. Similiz Manual, Sy8tem Version 5.0. DIKU, University of Copenhagen, Denmark, 1993. [3] Bhaskar \nBose. DDD A transformation system for Dig\u00adit al Design Derivation. Technical Report 331, Indiana University, \nComputer Science Department, May 1991. [4] Robert G. Burger. The Scheme Machine. Technical Report 413, \nIndiana University, Computer Science De\u00adpartment, August 1994. [5] G. J. Chaitin, M. A. Auslander, A. \nK. Cocke, M. E. Hopkins, and P. W. Markstein. Register allocation via coloring. Computer Languages, 6:47-57, \n1981. [6] F. Chow. Minimizing register usage penalty at proce\u00addure calls. In Proceedings of the SIGPLAN \n88 Confer\u00adence on Programming Language Design and Implemen\u00adtation, June 1988. [7] Fred C. Chow and John \nL. Hennessy. The priority\u00adbased coloring approach to register allocation. Trans\u00adactions on Programming \nLanguages and Systems, 12(4):501 536, October 1990. [8] William Clinger and Jonathan Rees (editors). \nRevised4 report on the algorithmic language Scheme, LISP Pointers, IV(3) :1-55, July-September 1991. \n[9] William D. Clinger and Lars Thomas Hansen, Lambda, the ultimate label, or a simple optimizing compiler \nfor Scheme. In Proceedings of the 199-4 ACM Conference on LISP and Functional Programming, pages 128 \n139, 1994. [10] Richard P. Gabriel. Performance and Evaluation of LISP Systems. MIT Press series in computer \nsystems. MIT Press, Cambridge, MA, 1985. [11] Robert Hieb, R. Kent Dybvig, and Carl Bruggeman. Representing \ncontrol in the presence of first-class con\u00adtinuations. In Proceedings of the SIGPLA N 90 Con\u00adference \non Programming Language Design and Imple\u00admentation, pages 66 77, June 1990. [12] David Kranz. Orbit: \nan optimizing compiler for Scheme. Technical Report 632, Yale University, Com\u00adputer Science Department, \n1988. [13] Zhong Shao and Andrew W. Appel. Space-efficient closure represent at ions. In Proceedings \nof the 1994 ACM Conference on LISP and Functional Program\u00adming, pages 150 161, 1994. [14] P. A, Steenkiste \nand J. L. Hennessy. A simple interpro\u00adcedural register allocation algorithm and its effective\u00adness for \nLisp. Transactions on Programming Languages and Systems, 11(1):1 32, January 1989. [15] Andrew K. Wright \nand Robert Cart wright. A practi\u00adcal soft type system for Scheme. In Proceedings of the 1994 ACM Conference \non LISP and Functional Pro\u00adgramming, pages 250-262, 1994.  \n\t\t\t", "proc_id": "207110", "abstract": "<p>This paper presents a fast and effective linear intraprocedural register allocation strategy that optimizes register usage across procedure calls. It capitalizes on our observation that while procedures that do not contain calls (<italic>syntactic leaf routines</italic>) account for under one third of all procedure activations, procedures that actually make no calls (<italic>effective leaf routines</italic>) account for over two thirds of all procedure activations. Well-suited for both caller-and calle-save registers, our strategy employs a &#8220;lazy&#8221; save mechanism that avoids saves for all effective leaf routines, an &#8220;eager&#8221; restore mechanism that reduces the effect of memory latency, and a &#8220;greedy&#8221; register shuffling algorithm that does a remarkbly  good job  of minimizing the need for temporaries in setting up procedure calls.</p>", "authors": [{"name": "Robert G. Burger", "author_profile_id": "81100576913", "affiliation": "Indiana University Computer Science Department, Lindley Hall 215, Bloomington, Indiana", "person_id": "PP14199444", "email_address": "", "orcid_id": ""}, {"name": "Oscar Waddell", "author_profile_id": "81100242452", "affiliation": "Indiana University Computer Science Department, Lindley Hall 215, Bloomington, Indiana", "person_id": "P212431", "email_address": "", "orcid_id": ""}, {"name": "R. Kent Dybvig", "author_profile_id": "81100181541", "affiliation": "Indiana University Computer Science Department, Lindley Hall 215, Bloomington, Indiana", "person_id": "PP14073331", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/207110.207125", "year": "1995", "article_id": "207125", "conference": "PLDI", "title": "Register allocation using lazy saves, eager restores, and greedy shuffling", "url": "http://dl.acm.org/citation.cfm?id=207125"}