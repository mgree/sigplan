{"article_publication_date": "06-01-1995", "fulltext": "\n Accurate Static Branch Prediction by Value Range Propagation Jason R. C. Patterson Oasonp@fit.qut. \nedu. au) School of Computing Science Queensland University of Technology Brisbane, Qld 4001, Australia \nPhone: +61-75-316191 Abstract The ability to predict at compile time the likelihood of a particular \nbranch being taken provides valuable information for several optimizations, including global instruction \nscheduling, code layout, function inlining, interprocedural register allocation and many high level optimization. \nPrevious attempts at static branch prediction have either used simple heuristics, which can be quite \ninaccurate, or put the burden onto the programmer by using execution profiling data or source code hints. \nThis paper presents a new approach to static branch prediction catled value range propagdion. This method \ntracks the weighted value ranges of variables through a program, much like constant propagation. These \nvalue ranges may be either numeric or symbolic in nature. Branch prediction is then performed by simply \nconsulting the value range of the appropriate variable. Heuristics are used as a fallback for cases where \nthe value range of the variable cannot be determined statically. In the process, value range propagation \nsubsumes both constant propagation and copy propagation. Experimental results indicate that this approach \nproduces significantly more accurate predictions than the best existing heuristic techniques. The valae \nrange propagation method can be implemented over any factored dataflow representation with a static single \nassignment property (such as SSA form or a dependence flow graph where the variables have been renamed \nto achieve single assignment). Experimental results indicate that the technique maintains the linear \nruntime behavior of constant propagation experienced in practice. 1 Introduction The ability to predict \nat compile time the probability of a particular branch being taken provides valuable information for \nseveral optimizations, including global instruction scheduling, code layout, function inlining, interprocedural \nregister allocation and many high level optimizations. For most of these Permission to copy without fee \nall or part of this material is granted provided that the copies are not made or distributed for direct \ncommercial advantage, the ACM copyright notice and the title of the publication and its date appear, \nand notice is given that copyin is by permission of the Association of Computing Machinery. ? o copy \notherwise, or to republish, requires a fee and/or specific permission. SIGPLAN 95La Jolla, CA USA G 1995 \nACM 0-89791 -697-2/95/0006 ...$3.50 optimizations branch predictions are merely an extra piece of useful \ninformation, but for global instruction scheduling and instruction cache optimizations the accuracy of \nthe branch predictions can make or break the optimization. Unfortunately, it is often difficult to predict \nwhich branches will and won t be taken through a particular program. This is not because conditional \nbranches are unpredictable in nature, however. Indeed conditional branches show surprisingly predictable \nbehavior. Obviously some branches will depend on the particular input being processed, but in most cases \nthe branching behavior of a program is largely independent of its input. It has been observed that most \nconditional branches take one direction most of the time, irrespective of the particular input being \nprocessed and even the type of program [FisherFreudenberger92]. Up to now there have basically been three \napproaches to obtaining branch prediction data for use in optimization: heuristics based on nesting \nand coding styles  real execution profiling  source code hints supplied by the programmer  Most heuristic \napproaches try to identify common programming practices. They are often based on dubious assumptions \nand are sometimes no better than random guesses. A simple example is the 90/50 rule, which predicts that \nbackward branches are taken 90% of the time and forward branches 50%. This is obviously far too crude \nto base important decisions on. It is possible to predict loop controlling branches fairly accurately \nusing simple nesting heuristics and loop analysis, but simple heuristics perform very poorly for the \nnon-loop branches which dominate the dynamic branch count of many programs [Smith81, Wal191 ]. More sophisticated \nheuristic approaches base their decisions on the datatypes and type of comparison used in the branch \nand the code in the target basic block [BallLarus93, Wagner+94]. Although these approaches are much better \nthan trivial heuristics, their accuracy is still only mediocre. These approaches can atso be somewhat \nhit-and-miss, since programs which use programming styles that don t fit the heuristics result in very \npoor predictions. Execution profiling is based on instrumenting a program with code to collect information \nsuch as basic block execution counts, which can then be used to determine branch probabilities [McFarlingHennessy \n86, BallLarus92]. The instrumented version of the program must be run to collect the profile data, so \nprogrammer intervention is required. The input used during these test runs can have an effect on the \nresulting profile. 67 As you would expect, execution profiling is much more accurate than heuristics, \nusually within a few percent of the common case runtime behavior for most branches [FisherFreudenberger92]. \nHowever, it is not perfect. Unfortunately researchers often use the same input for obtaining the profile \nand measuring the effectiveness of optimizations based on it. This gives an unrealistically good picture \nof the situation, since in practice a program is optimized based on a small number of profile runs, then \nrtm with many different input sets. This different input data causes the profile to be a less-than-perfect \nmatch to the input. Despite this, execution profiling is still easily the most accurate method for obtaining \nbranch prediction data. There is, of course, a serious weakness to execution profiling -it is too inconvenient \nfor all but the most performance aware programmers to bother doing . This is unfortunate, because profiling \ncan help identify higher level performance problems as well. Sadly it is the case that many programmers \ndon t even bother identifying performance critical routines, let alone tuning their code for better performance. \nAnother problem with execution profiling is that it is not practical for some pieces of code, such as \nparts of an operating system kernel. The final method of obtaining branch predictions is through source \ncode hints. Approaches based on programmer supplied hints are both inconvenient and potentially inaccurate. \nThey do, however, provide a way to handle certain difficult situations where neither compile time prediction \nnor execution profiling are adequate. Like the register declarations in the C language, such hints are \nindicative of a lack of suitable compiler technology more than anything else. Clearly, it would be nice \nto be able to predict branching behavior as accurately as execution profiling does, but without requiring \nany programmer intervention. This paper describes a new approach to static branch prediction which moves \ntowards this objective. The technique is called value range propagation. It uses the mechamsm of constant \npropagation [WegmanZadeck9 1] and some of the ideas from range analysis [Harrison77]. Vcdw range propagation \nextends standard constant propagation to track the weighted value ranges of variables in a program. These \nvalue ranges may be either numeric or symbolic in nature. Branch predictions are then made based on these \nvalue ranges. In tbe process, value range propagation subsumes both constant propagation and copy propagation. \n 2 Related Work Constant propagation is a well-understood problem whose goal is to identify expressions \nthat are constant for every possible execution of a program and can therefore be evaluated at compile \ntime rather than runtime [Kildal173]. This is a simple forward dataflow problem -information about the \nconstantness of expressions is simply propagated around the control flow graph until a fixed point is \nreached. There is a strong similarity between constant propagation and our objective. Instead of finding \nthe expressions which are constant, we want to determine the weighted range of values an expression can \nhave during the execution of a program. Branches based on this variable can then be accurately predicted \nsimply by examining this weighted value range. The technique outlined in this paper uses the mechanism \nof constant propagation with conditional branches developed by Mark Wegman and Ken Zadeck [WegmanZadeck91 \n]. Their approach uses the s~atic single assignment (SSA) representation, which was originally developed \nas a dataflow representation geared towards propagating values through a program. It therefore provides \nan ideal platform for this kind of analysis. Using SSA form, constant propagation can be performed in \nessentially linear time. In SSA form every variable has only one assignment, and therefore each use has \na single definition. In essence, the clef-use chains (called SSA edges) become a one-to-many rather than \na many-to-many relationship. For a detailed description of the SSA representation see [Cytron+91 ]. The \ntransformation into SSA form involves two steps. In the first step special assignment operations called \n#-functions are inserted at joins in the program where alternative versions of a variable meet. These \n@-functions occur at the entrance to basic blocks. and have the form: a = @(b, c, ...) The meaning of \nthis is that if control reaches this block via the fwst in-edge, a is assigned the value of b, if via \nthe second in\u00adedge, a is assigned the value of c, and so on. In the second step the variables are renamed \nso that each assignment to a particular variable has a unique name. This achieves the static single assignment \nproperty -each variable now has a single definition point (either in a real assignment or in a O-function). \nAlthough this paper describes the value range propagation method using the SSA form and it s associated \nterminology, it can be implemented using any factored dataflow representation with a static single assignment \nproperty. In fact, my actual implementation uses a dependence flow graph [JohnsonPingali93] where the \nvariables have been renamed to achieve single assignment.  3 Algorithm Overview 3.1 Objective The objective \nof any branch prediction algorithm is to determine the likelihood of taking each conditional branch in \na program. Most heuristic methods simply mark each branch as either taken or not-taken . This is useful \nbut not ideal. What we would really like to know is the probability of a branch being taken. This is \nwhat value range propagation produces -the out-edges of each conditional branch are marked with probabilities \nranging from O (never taken) to 1 (always taken). Using probabilities rather than a simple yes or no \nanswer lets us accurately assess a series of branches to determine the degree of speculation involved \nin moving a particular instruction (in global instruction scheduling), or the likelihood of executing \na particular basic block (for cache optimizations). 3.2 Propagation Terminology The heart of the value \nrange propagation method is the propagation of the weighted range of values each expression can have \naround the control flow graph until a fixed point is reached. In formal terms, the objective of any propagation \nalgorithm is to determine an output assignment for each variable in a program. These output assignments \nrepresent some knowledge about the value of each variable, and are normally described in terms of a multi-level \nlattice of possible values, The highest level in this lattice is top (T), which represents an as yet \nundetermined value. Below this are the levels which represent some known property of the variable. Finally, \nthe lowest level is bottom (l), which indicates that the variable contains a value that cannot be predicted \nat compile time. Propagation algorithms start with the optimistic assignment of T for each variable, \nand proceed by lowering the assignments (in terms of the lattice) as more information is gathered about \nthe program. Eventually a fixed point is reached and the process terminates. At this point no T assignments \nwill remain, so each variable will either have some information assigned to it, or will be undefined \n(1). Information is gathered about particular variables by symbolically executing program statements \nand applying the meet operator (~) at joins in the program @\u00adfunctions). The meet operation must be defined \nsuch that the result is never higher than the operands, otherwise the algotithm might never terminate. \n 3.3 Value Range Propagation In standard constant propagation there is only one middle level (consrant), \nand the rules for expression evaluation and meet operations are very simple (Figure 1). If a variable \nhas a constant as its output assignment after propagation, the variable will hold this constant value \nfor all possible executions of the program. Although this is useful for constant folding and determining \nreachable code, this information is of no use for branch prediction. anyn T. any anynl-.l. Cl nCj =Ci(ifi=j) \nG nCj =l(ifi#j) Figure 1. The standard constant propagation lattice. In order to predict what proportion \nof the time a branch will be taken, we really need to know what is in the variable that the branch is \nbased on. Not only do we need to know the values that might be in the variable, but also how often each \nvalue actually occurs at runtime. Take a moment to consider the code segment below (Figure 2). For fun, \nwork out how often block A is executed. for (x=O; x-d O; ++x) ( if(x>7){y= l;) else{ y= x;) .. if (y== \n1) { ... Biock A ... } ... ) Figure 2. A simple example, To the human reader it is obvious that block \nA is executed 30% of the time. Subconsciously you probably used the following logic in coming to this \ndecision: . x is evenly distributed across the values O-9 the test x>7 will succeed 20% of the time \n if the test succeeds y will have the value 1  if the test fails y has a 1-in-8 (1 z.s~o) chance of \nhaving the value 1  after the test v has a (0.2 1 + 0.8 *O. 125) = 3070 chance of having the va~ue 1 \n  The value range propagation method uses exactly this kind of logic to determine branch probabilities. \nAs with standard constant propagation, a simple worklist algorithm is used as the mechanism for forward \ndataflow analysis. Two working lists are maintained: the Flow WorkList, which is a worklist of control \nflow graph edges, and the SSA WorkList, which is a worklist of SSA edges (clef-use chains). The major \ndifferences between value range propagation and standmd constant propagation are: value ranges are propagated \nrather than constants the rules for expression and o-function evaluation are somewhat more complicated \nloop carried expressions are detected and handled specially (so that we don t actually execute the loops!) \neach control flow graph edge has a probability associated with it rather than a~ is~exec;ted flag - \nTbe algorithm proceeds as follows: 1 Initialize the FlowWorkList to contain the out-edges from the start \nnode, the SSAWorkList to be empty, all probabilities to be O, and all ranges to be T. Mark the out-edges \nof the start node with a probability of 1 (ie 100%), 2 If both working lists are empty, terminate. Otherwise \nextract the next item from one of the lists. Execution can proceed by processing items from either list, \nbut experience has shown that prefeming to select from the FlowWorkList tends to cause information to \nbe gathered more quickly and therefore reduces the running time of the algorithm. 3 If the item is a \ncontrol flow graph edge, visit the target node. If this node has never been visited before, evaluate \nevery expression in the node, otherwise evaluate only the o\u00adfunctions. For each expression, if the new \nresult (value range) differs from the old result for that expression (SSA variable), add the SSA edges \nstarting at that expression to the SSAWorkList. 4 If the item is a @-function, and one or more of the \nnode s in\u00adedges are back edges (as identified by a depth first traversal from the start node), then there \nis a loop in the SSA edge chain starting at this @function. In other words, this is a loop carried variable. \nAttempt to derive its value range. Loop carried variable derivation is described later in this paper. \nIf derivation succeeds, mark the expression as derived (derived expressions are not to be re-evaluated). \nIf derivation fails, mark it as impossible to derive, so that derivation will not be re-invoked for \nit. 5 If the item is a @-function which has not been marked as derived, and the probability of executing \nany of the in-edges to this node is non-zero, evaluate that @-function. The evaluation of a o-function \nis simply the merging of the appropriate ranges according to the current branch probabilities for each \nin-edge 1. If the new result differs from the old result for that @-function, add the SSA edges starting \nat that @function to the SSAWorkList. 6 If the item is an expression which has not been marked as derived, \nand the probability of executing any of the in-edges to this node is non-zero, evaluate the expression. \nIf the new result differs from the old result for that expression, add the SSA edges starting at that \nexpression to the SSAWorkList. 7 If the item is a conditional branch instruction which has not been marked \nas derived, then determine the probability of taking the branch by examining the relevant variable s \nvalue range. If the new probability differs from the old probability for this branch, mark the out-edges \nwith the new probabilities and add any changed out-edges to the FlowWorkList. 8 Goto 2.  3.4 Range Representation \nChoosing a flexible but efficient representation for the value range of a variable is a difficult but \ncritical part of implementing the value range propagation algorithm. An ideal representation would provide \na space efficient encoding for common cases such as single values, evenly distributed ranges and arithmetic \nsequences, whilst simultaneously allowing the flexibility to represent less common ranges such as geometric \nsequences and unusual sequences such as prime numbers. To make things even more complex, many ranges \ncan only be specified relative to others, eg: k is greater than y + 2 . Such symbolic ranges must also \nbe handled efilciently. At one extreme, it is possible to use a completely general symbolic representation \nfor range information. This may be appropriate for formal program verification systems, but it would \nbe far too inefficient for practical use in a compiler. Using too simplistic a representation, on the \nother hand, yields results not much better than constant propagation. Obviously a tradeoff between accuracy \nand efficiency is necessary. From a practical point of view, value range analysis is probably of most \nuse in tracking the arithmetic sequences of loop control variables, dense ranges, and constants of various \ntypes. In the method described by this paper, the value contained by a particular variable is represented \nby a set of weighted ranges where the total range is the union of all the individual ranges, Each individual \nrange is defined by: a probability a lower bound an upper bound a stride (arithmetic step size) 1 The \nprobabdity of an m-edge horn an unconditional branch (or fall through) is tire sum of the probabilities \nof the m-edges of the first node with equivalent control dependencies to the node which ends with the \nuncorrditlonal branch (ie the earliest node which dominates it and which it postdominates). In other \nwords, the sum of the probabditms of the edges wh[ch lead h the node being executed. An even distribution \nis assumed within each range, so uneven distributions must be represented by multiple rarrges. The notation \nused in this paper to describe a set of ranges is as follows: { P[L:U:S]) ... } In this notation P represents \nthe probability of the particula range applying at runtime, L and U give the lower and upper bounds of \nthe range, and S is the stride. In addition to ranges between numeric lower and upper bounds with numeric \nstrides, it is also necessary to handle common cases of symbolic ranges to achieve high levels of accuracy \nin branch predictions. This can be achieved by simply allowing each number in a range definition to be \ndefined as: SSA Variable operator Corrstanl For numeric values the variable component is NULL2, and for \npurely symbolic values the constant component is +0. This representation allows the simple case of values \ndefllned relative to a single variable to be handled (eg: x+2), but not values which represent potentially \ncomplex operations between multiple variables (eg: x+y). As a result, range operations and comparisons \nare kept simple. Although this only allows meaningful operations and compmisons between variables which \nshare a single common ancestor , it achieves most of the practical benefits of symbolic analysis without \nhaving to implement a full-blown theorem tester to determine relationships between ranges. As mentioned \nabove, each variable is represented by a set of ranges. In order to eliminate the possibility of a single \nvariable s set of ranges growing indefinitely, it is necessary to place an upper limit on the number \nof ranges used, In other words, a give-up point needs to be identified. In practice a relatively small \nnumber of ranges is adequate, normally no more than four for programs with normal control flow. Using \nfour ranges per variable allows us to handle merges from up to two levels of conditional branching without \nlosing accuracy.  3.5 Range Operations The method described in this paper propagates information about \nthe value ranges of variables through a program in the same way that constants are propagated in traditional \nconstant propagation. As a result, it is necessary to extend the expression evaluation mechanism of constant \npropagation to handle value ranges. For example, the optimizer must be able to evaluate expressions such \nas: ( 0.7[32:256:1], 0.3[3:21 :3] } + { 0.6[1 6:1 00:4], 0.4[8:8:0] ) = ( 0.42[48:356:1], 0.28[40:264:1] \n0.18[19:121 :1], 0.12[11 :29:3] } Implementing these range operations is relatively straightforward. \nEfficiency is important here because unlike constant propagation, expressions may need to be re-evaluated \nmany times before a fixed point is reached. If expressions become very large, applying a technique such \nas Reif and Lewis s tree representations 2 By ~LL I meanwhateverthe null concept for the representation \nbeing used is. In my implementation, for example, It m virtual register O. [ReifLewis77] maybe useful \nto avoid redundant re-computations. Unfortunately this technique implies a significant memory overhead, \nso practical uses are limited to very high-cost expressions. The observant reader might have noticed \nthat the above example actually loses information. Consider the calculation of 0.3[3:21 :3] + 0.6[1 6:1 \n00:4]. The result shown above is 0.18[1 9:1,21 :1], which represents any number from 19 to 121, In reality, \nhowever, there is no combination of a number from the range 0.3[3:21 :3] and a number from the range \n0.6[16: 100:4] that will total 20. This indicates a limitation of our range representation -achieving \nmore accuracy in this example would require either a more sophisticated range representation or a very \nlarge number of very small ranges. Although this loss of accuracy leads to less accurate branch predictions, \nproviding additional accuracy would greatly increase the running time of the algorithm. Some range operations \nare problematic for other reasons. Load operations, for example, typically result in a range of 1 unless \ndetailed alias analysis information is available. As a result, conditional branches based on a value \nloaded from memory often cannot be predicted using value range propagation. Instead, heuristics similar \nto those in [BallLarus93] must be used. Depending on the quality of the alias analysis being performed \nand the type of program being optimized, this might occur anywhere from 10% to 90% of the time. To determine \nprobabilities rather than simple taken or not taken predictions the heuristics must be weighted and combined \nusing a technique such as [WULWUS94].  3.6 Loop Carried Expressions The standard constant propagation \nalgorithm propagates values without explicitly handling loops in the program. For constant propagation \nthis is acceptable because an expression can only be evaluated twice. For example, a loop control variable \nis first assigned the constant of its initial value, then when the incremented version of this value \nis propagated around from the bottom of the loop the values are different so the variable is assigned \nJ_. If we used this approach for value range propagation, each loop would execute as many times during \npropagation as it would at runtime, because the value range of the loop control variable would gradually \nbe expanded until the loop termination test succeeded. Although this would work correctly, it would make \nwdue range propagation far too slow for practical use. To avoid this, it is necessary to explicitly identify \nloop carried expressions and determine their value ranges using a different approach. Loop carried SSA \nvariables are those with loops in their SSA edge chain. That is, one or more of the in-edges to a @-function \nis a back edge (as identified by a depth first traversal from the start node). This is easily identified \nduring value range propagation (it was step 4 in the algorithm description). Having identified a loop \ncarried expression, it is desirable to determine its value range without having to execute the loop. \nThis can be done by examining the expression s derivation [Harrison77]. A variable s deriv~ion is the \nset of operations performed on that variable during the loop. In its most general form, a derivation \nwould be a set of symbolic equations which specify how the variable may be computed from its previous \nvalue and the values of other variables. For practical purposes, however, this would be too complicated \nand would gain little over a simpler approach. In practice, it is only necessary to identify simple inductive \nderivations such as loop counters. This can be done by simply processing the operations in the derivation \nand matching them to a template such as: new value = old value ~ set of possible increments assert (new \nvalue between spec@c bounds) The first part of the template matches the loop increment operation(s), \nand the second part matches the termination test(s). This template can then be combioed with the initial \nvalue of the variable to produce a final value range. If the set of operations in a derivation cannot \nbe matched to such a template, then we can simply allow the propagation algorithm to determine the value \nrange by executing the loop. Thus one should view derivation matching as an efficiency optimization. \nOrdy the simple derivation processing mentioned above is actually essential for reasonable performance, \nbut adding more templates and more powerful derivation processing reduces the need for brute force propagation \nto do the work. 3.7 Interprocedural Analysis Standard constant propagation is often considered difficult \nto extend to interprocedural cases because solving the fully general case where constants may be propagated \nthrough intermediate procedures requires symbolic information to be manipulated rather than simple constants. \nIn contrast, since we are already performing relatively sophisticated symbolic analysis for intra \u00adprocedural \nvalue range propagation, it is straightforward to extend our analysis to handle interprocedural cases \nas well. In fact a significant amount of branch prediction accuracy would be lost if we did not handle \ninterprocedural cases, because the branching behavior of many procedures depends greatly on the values \nof their parameters. Interprocedural constant propagation is usually described in terms of a set of jump \nfunctions associated with each call site [Callahan+86, GroveTorczon93]. These jump functions give the \nvalues of the actual parameters used in each call of a function, and the value of each formal parameter \nis simply found by merging the values of the jump functions at the various call sites3. In our case, \nthe jump fimctions map directly to the range representations for the parameters in the call, and the \npropagation algorithm remains the same. In essence, the entire program is treated almost as if it were \none huge control flow graph. One particularly important extension of interprocedural value range propagation \nis the judicious use of procedure cloning for critical procedures [CooperHallKennedy92]. Procedure cloning \ninvolves duplicating a critical procedure which is not inlined but which is called in two (or more) significantly \ndifferent contexts so that each copy may be optimized in a different way. Thus the distinct copies can \ntake advantage of their specific calling contexts. Since the calling context has a large impact on the \nbranching behavior, this leads to substantially more accurate predictions. Many other optimization may \nalso benefit from procedure cloning [MetzgerStroud93]. 3 fioc~we re~rn =IUM are handled similarly, using \nremmj~PtictiOnS  3.8 Putting It All Together As a summary. consider our earlier example program (Figure \n2). The control flow graph in SSA form is shown below complete with SSA edges (F~g~e 3). . Xo=o   \nE&#38;z~~ I ;, . Ixl = O(xo, x5) .......... w . ....... ..... I X2>7 . . . . .[ X5=X4+1 [ I control \nflow edge SSA edge Figure 3. Our simple example in SSA form. Notice the assertion along the true edge \nof the Xl < 10 branch. Valuable information can often be derived from the equality tests controlling \nbranches, so assertions such as this one are placed in the graph after conditional branches to assert \nspecific properties of a variable4. Now consider the application of the value range propagation algorithm. \nInitially, all branches are marked with probabilities of O and alt variables are assigned T. The starting \nedge is marked with 100% and selected for processing. This causes the evaluation of all the expressions \nin the first block, which includes both the assignment to XO and the unconditional branch (or fall through). \nThe assignment to XO yields a range of 1[0:0:0]. Since this is not Variables created by assertton area \nspecial case when merged in @-functions. The merging of an assertion-derived variable with it s parent \nvariable, or of all the assertion-derived variables of a common parent, results m the value raage ofthe \nparent variable. the same as T, the SSA edge starting at this expression is added to the SSAWorkList. \nThe unconditional branch (or fall through] is marked with a 10070 probability, and is added to the FlowWorkList. \n The out-edge of the first block, or perhaps the SSA edge starting at xO, is selected next, causing the \n@function assignment to x 1 to be evaluated. One of the in-edges to this @onction is a back edge, so \nwe attempt to derive the @function s value range. In this case derivation will succeed and result in \na range of 1[0:10:1]. Again this is different from T, so the two SSA edges starting at this expression \nare added to the SSAWorkList. The branch on the expression xl <10 is evaluated next. .%nce xl has the \nrange 1[0 10: 1], the branch is predicted as 91% taken. As a result, the true out-edge is marked with \na 91910probability and the false edge a 9% probability, Since the probabilities of both edges have changed, \nboth edges are added to the FlowWorkLkt. This process continues until the FlowWorkLkt becomes empty. \nAt this stage the SSAWorkList still has many entries however, and the analysis is far from complete. \nIn fact this typically represents the point at which each expression has only been evaluated once. One \nby one, each SSA edge is now removed from the SSAWorkList, and each in turn causes further information \nto be propagated around the graph. Although our simple example terminates rather quickly, in general \nnew edges may be added to either or both worklists, and the value ranges and branch probabilities may \nbe further refined. Eventually these re-evaluations begin to produce the same results as the results \npreviously calculated. In these cases no edges are added to the working lists, so the lists gradually \nbecome shorter. The process slowly winds down and the FlowWorkList and SSAWorkList become empty. At this \nstage a fixed point has finally been reached. The final value ranges and branch probabilities for our \nsimple example are shown in Figure 4. Vahe Ranges Xo { 1[0:0:0] ) xl {1[0:10:1]} x2 {1[0:9:1]} x3 {1[0:7:1]} \nx4 {1[0:9:1]} x5 {1[1:10:1]} yo {1[1:1 :0]} yl (1[0:7:1]} y2 ( 0.8[0:7:1], 0.2[1 :1:0] } Branch Probabilities \nxl <10 9170 x2>7 2070 y2==1 30% Figure 4. Results for our simple example, 4 Algorithm Efficiency Standard \nconstant propagation can only lower (in the lattice sense) a variable twice, first to a constant and \nthen to 1. The asymptotic time complexity of standard constant propagation is therefore O(E.V). This \nis a worst case result, suggesting that every variable is defined in every basic block. Both intuition \nand empirical evidence suggest that in practice it is closer to O(E) (ie linear in the size of the program). \nThe method proposed in this paper adds a significant amount of processing to that of standard constant \npropagation. Unlike constant propagation, the lattice for value rarrge propagcuion has a conceptually \ninfinite number of middle levels, and expressions inside loops may need to be re-evaluated many times \nbefore. a fixed point is reached. The expression evaluation itself is also more complex than in constant \npropagation -up to R 2 operations are performed per expression evaluation, where R is the maximum number \nof ranges used to represent a variable (normally four). In order to guarantee termination, it is necessary \nto show that each variable can only be lowered a finite number of times. Placing an upper bound on the \nnumber of value ranges constituting a variable s representation has exactly this effect, but using this \nas a proof of termination is somewhat unsettling since it merely guarantees that any particular loop \nmight only iterate several million times ! A more realistic estimate of the algorithm s efficiency can \nbe provided by noting three observations: Non-loop dependent ranges are only evaluated once.  Almost \nall loop dependent ranges are either derived or are completely narrowed after a single evaluation because \nthe variables they depend on are derived.  Many problematic ranges cannot be represented and quickly \nbecome J-. For example, using the restricted representation described in this paper even a geometric \nsequence cannot be represented. In these cases the propagation algorithm will attempt to enumerate all \nthe values in the range. Due to the finite nature of the range representation, this leads to either an \noverly broad representation (as was the case in our earlier range operation example), or J_. In either \ncase no further refinement is possible. Noting the above observations, it is reasonable to expect value \nrange propagation to exhibit similar runtime behavior to that of constant propagation. This is indeed \nthe case -value range propagation is slower than constant propagation, but still linear in the size of \nthe program. Figures 5 and 6 show the runtime behavior for a collection of 50 programs including the \nSPEC92 suite, various other benchmarks and numerous UNIX utilities (evaluation sub-operations take essentially \nconstant time).   l&#38;LI--\u00ad 0 ,0000 40000 ,0000 ,0000 ,000,0 4 *,,,, ,,0000 ,,0000 ,,00,0 ,00,00 \nFigure 5. Number of expression evaluations versus number of instructions. ,0,000 . .,0000 4..000 350000 \n30000, 260000 ,00000 . . t 60000 . . .4 ,00,0 .\u00ad .-. 60000 9 * . A.!.! o 0 20000 40000 60000 60000 \n!00000 !.0000 ,40.0. (.0000 ,,00.0 ,00,,0 Figure 6. Number of expression evaluation sub-operations versus \nnumber of instructions. 5 Results To determine the accuracy of the value range propagation approach \nfor real programs, branch predictions determined using value range propagation were compared to the results \nof execution profiling and also to the results achieved using simple and complex heuristics (the 90/50 \nrule and the [B allLarus93] heuristics combined as in ~uLarus94] to produce probabilities). Different \ninputs were used to collect the execution profiles and the actual observed behavior , reflecting the \nnormal use of execution profiles found in practice. Heuristics similar to those of [BallLarus93] were \nused in cases where the value range propagation algorithm encountered a branch with a variable whose \nvalue range was 1. The SPEC92 benchmarks were used for the analysis. The resulting branch predictions \nwere analyzed in terms of how far each branch s predicted probability deviated from its actual behavior. \nThis involved determining the difference between the predicted probability for each branch and the actual \nprobability observed for that branch when the program was given the SPEC reference inputs. The analysis \nwas done in both an unweighed context, where each branch contributed equally, and in a context where \neach branch was weighted according to its execution count. This analysis is somewhat more detailed than \nthat of other branch prediction studies, which normally consider only taken or not\u00adtaken results. That \nis, a branch is predicted as taken or not-taken, and if at runtime it is taken (or not-taken) more than \nhalf the time, the prediction is considered correct. In our case we are interested in determining how \naccurate our predictions are at the finer level of branch probabilities, not simply branch directions. \nSince we will be speculating on the usefulness of scheduling instructions based on these predictions, \nit is important to know exactly how close to the real behavior our predictions are. Consider, for example, \nthe decision of whether to speculatively move an instruction up through two conditional branches. If \neach branch is taken 60% of the time, our instruction will only be useful 36% of the time. If the branches \nare simply predicted as taken, however, you could be led to believe that the instruction would be useful \nmost of the time. Figures 7 and 8 show the weighted and unweighed results for the SPECint92 and SPECfp92 \nbenchmark suites. Each benchmark is weighted equally within its suite. The graphs plot the percentage \nof branches predicted to within a given error margin. 73  SPECint92 Unweighed ----------------- ....- \n..,............:......,,, .......... .........- ,.,..:..-.. ..,,:,....... .......... -_ -- . ........ \n ...........-. _-a- --- -\u00ad ...... . --\u00ad --\u00ad ~..:---- \u00ad o <1 <3 <5 <7 <9 <11 <13 <15 <17 <19 <21 <23 <25 \n<27 <29 <31 <33 <35 <37 <39 Error in Percentage Points (*) SPECint92 Weighted .. --. e*--- \u00ad..\u00ad . .* \n... ...... ..........-,...:....-. _ .C .,.. -_ #-* . ...-.  -\u00ad.<---\u00ad o <1 <3 <5 <7 <9 <11 <13 <15 \n<17 <19 <21 <23 <25 <27 <29 <31 <33 <35 <37 <39 Error in Percentage Points @) Figure 7. Results for SPECint92 \n(unweighed &#38; weighted by execution count). Legend . Executmn Profihng  Ball &#38; Lams s Heuristics \nValue Range Propagation . . 90/50 Rule Value Range Propagation Random Predictions (numeric ranges only) \n74  SPECfp92 Un weighted ---------------------=-----------------\u00ad . . ------ --\u00ad. --- ---- -- - - - \n- -\u00ad <1 <3 <5 <7 <9 <11 <13 <15 <17 <19 <21 <23 <25 <27 <29 <31 <33 <35 <37 <39 Error in Percentage Points \n(~) SPECfp92 Weighted 1--- ---- ---- ---- .--- ---- ----------------------. . -.~.----- --  ---- - \n\u00ad -#\u00ad/  I -------\u00ad  -\u00ad <1 <3 <5 <7 <9 <11 <13 <15 <17 <19 <21 <23 <25 <27 <29 <31 <33 <35 <37 <39 \nError in Percentage Points (f) Figure 8. Results for SPECfp92 (unweighed &#38; weighted by execution \ncount). Legend -----. Execution Profiling . -Ball &#38; Larus s Heuristics Value Range Propagation \n..-. 90/50 Rule Value Range Propagation Random Predictions (numeric ranges only) The enor margin plotted \nis between O and 40 percentage points. This range was chosen to accentuate the important part of the \ngraph (the low error margins). Techniques which cannot predict a substantial percentage of branches to \nwithin *40~o are doing very poorly indeed, so displaying the remaining 40-100 error range would add little \nmeaning to the graphs. Since the graphs plot the percentage of branches predicted to within a given \nerror margin, better predictions result in lines which move more quickly towards the top of the graph. \nFor reference, the perfect static predictor would mark each branch with the same probability as was observed \nin the trial runs, and would have a graph with a horizontal line across the top, indicating that 100% \nof the branches were predicted to within +()% of tie observed behavior. As you would expect, execution \nprofiling does extremely well in all cases. It does almost as well as the perfect static predictor for \nnumeric code (SPECfp92), with an astonishing 92% of branches predicted to within *1% of their actual \nbehavior! For integer and pointer intensive code the results are slightly worse but still very impressive \n(SPECint92). Interestingly, the weighted integer results for execution profiling are significantly worse \nthan the unweighed results. This may be because many of the branches which get heavily weighted in SPECint92 \nare controlled by the size of the input, and the SPEC feedback collection inputs (input. short) are much \nshorter than the reference inputs (input.ref). As the figures show, the predictions based on value range \npropagation are quite accurate. Results are shown for the value range propagation algorithm both with \nand without symbolic ranges. In both cases they are better than the heuristic methods. Adding symbolic \nranges substantially increases the overall accuracy of the method because many more branches can be predicted \nwithout resorting to heuristics. Whether adding even more sophisticated symbolic analysis would result \nin a significant increase in accuracy or not is an open question. Unfortunately, even a small increase \nin the scope of the symbolic range representation would require a large amount of implementation effort. \nInterestingly, the value range propagation method is significantly more accurate fornumeric code than \nfor integer and pointer code. This is largely because numeric code often has a very simple branching \nstructure, with most branches depending on loop control variables. This reduces the number of cases where \nbranches are based on variables whose value range is _L, which in turn reduces the amount of time when \nheuristics are used. As expected, the 90/50 rule does most poorly, particularly in the unweighed case. \nThis is largely due to the inaccuracy of the 50 part of the 90/50 rule. Backward branches are usually \ntaken, so 90% is often quite close to the actual behavior of these branches, but forward branches depend \ngreatly on the particular test being performed. Most branches take one direction with high probability, \nand very few forward branches me a 50/50 split. As a result, the graph for the 90/50 rule has a sudden \nincrease at about the 50 % error mark (not shown in the figures), since all the branches which the 90/50 \nrule predicts as 507. taken are suddenly accounted for (50% + 50% covers the entire range of possibilities! \n). 6 Applications One of the most pleasing features of the approach outlined in this paper is that \nfor those who already compute the SSA form or a similar representation, this approach can be added to \nan optimizer as an extension of constant propagation. So with relatively little effort, quite accurate \nbranch predictions are suddenly available. These can then be used in a host of optimizations. The three \nmain optirnizations which benefit from branch predictions are: Global Instruction Scheduling Superscalar \nand VLIW processors issue several instructions every cycle. To fully utilize such processors, compilers \nmust find several independent instructions to issue for each cycle, or otherwise let the processor sit \npartially idle. Due to the relatively small size of most basic blocks, it is usually necessary to look \nbeyond basic block boundaries and perform some form of global scheduling with speculative execution [BernsteinRodeh91] \nor trace scheduling [Fisher81, Ellis85]. Good branch predictions are of great benefit here since the \ndegree of speculation involved in moving a particular instruction can be accurately assessed. Code Layout, \nCache Optimization &#38; Inlining On many modern processors, an instruction cache miss or pipeline flush \ndue to an unexpected change of direction in the instruction stream will cost several cycles. Even a correctly \npredicted branch may cost more than straight line code. As a result, compilers must pay careful attention \nto the way they lay out their generated code. This usually means placing related pieces of code close \nto each other, inlining simple function calls and coding likely paths as straight-line code with branches \nto less likely code which is placed out-of-line [McFarling89, PettisHansen90]. The objective here is \nto reduce the number of branches encountered at runtime and to improve the I-cache hit rate so that fewer \nmisses occur, This approach can consistently make an I-cache appear 2 or 3 times as large as it does \nunder current practice. Interprocedural Register Allocation Modern processors typically have large register \nsets which, if used well, can dramatically reduce the memory reference overhead. Unfortunately, the frequent \noccurrence of function calls in most programs significantly reduces the effectiveness of the register \nset. Register windows are a hardware solution to this problem, but interprocedural register allocation \nwhich takes into account the probabilities of function calls can make much better use of a given register \nset [Wal186, Wal188, Wal191, SteenkisteHennessy89]. In addition to the three optirnizations mentioned \nabove, several traditional high level optimizations can also benefit from knowledge of frequently executed \npaths by using tail duplication to create what are effectively larger basic block structures [ChangMahlkeHwu91 \n]. A similar mechanism can also be used to improve the accuracy of branch prediction itself [Kral194]. \nBranch probabilities can also be used to control the order of applying other optimization phases, as \nis done in coagulation [Karr84, Morris91 ]. In this case what we want to know is the execution frequencies \nof functions and basic blocks, not the 76 probabilities of branches. This information can be obtained \nby using a Markov state transition model [Wagner+94], or by propagating frequencies around the control \nflow graph until a fixed point is reached [WuLarus94]. Optirnizations can then be applied in descending \norder of execution frequency. This is particularly effective for optimizations which allocate a limited \nresource, such as register allocation, since the most frequently executed code is processed first and \nis therefore more likely to get the resources it needs. In addition to using the branch probabilities \ncalculated from value range propagation, the value ranges themselves can also be used in some simple \narray access optimizations. Of particular note are: Alias Analysis for Array Accesses Using value range \npropagation it is sometimes possible to show that the ranges of the indices of two array accesses cannot \noverlap. As a result, these two accesses cannot alias each other. This analysis is much more limited \nthan sophisticated data dependency analysis techniques such as Banerjee s Inequalities [Banerjee88]. \nHowever it does offer a simple false-dependency breaking mechanism for compilers which don t implement \nthe more sophisticated methods. Elimination of Array Bounds Checks For languages which require (or compilers \nwhich implement) dynamic array bounds checking, many array bounds checks can be shown to be redundant \nby value range propagation. The elimination of these tests in combination with existing test minimization \ntechniques such as [Gupta93] can greatly reduce the overhead of array bounds checking. Finally, value \nrartge propagation itself can be viewed as an optimization. Viewed in this context, value range propagation \nsubsumes both constant propagation and copy propagation. If a variable s final value range is a single \nconstant such as 1[7:7:0], then the variable s value is constant for all possible executions of the program \nand can therefore be evaluated at compile time. Similarly, a variable x whose value range is the single \nsymbolic range of another variable such as 1~ :yO] is simply a copy of y. As such, all references to \nx can be replaced by references to y, and x can be eliminated. Just as constant and copy propagation \nidentify unreachable code, so does vahe range propagation -branches to unreachable code have a probability \nof O. 7 Conclusion This paper presented a new approach to accurate static branch prediction called vtdue \nrange propagation. The technique tracks the weighted value ranges of variables through a program, much \nlike constant propagation. These value ranges may be either numeric or symbolic in nature. Branch prediction \nis then performed by simply consulting the value range of the appropriate variable. In the process, value \nrange propagation subsumes both constant propagation and copy propagation. Both numeric and symbolic \nranges must be handled to achieve high levels of accuracy in branch predictions, and a tradeoff between \naccuracy and efficiency is required. For practical purposes a simple range representation capable of \nhandling arithmetic sequences is sut%cient, and a set of four ranges per variable is adequate for most \nprograms with typical control flow. A representation which handles symbolic ranges defined relative to \na single variable achieves most of the benefits of symbolic analysis at a reasonable cost in implementation \neffort. Whether more sophisticated symbolic analysis would result in more aecwate predictions or not \nis an open question. To avoid executing loops in the program, loop carried expressions must be detected \nand handled specially. This can be done by matching a loop carried variable s ab-ivation to a set of \ntemplates which identify common looping scenarios. Variables whose derivations don t match a template \ncan still be handled by allowing the propagation algorithm to execute the loop. Experimental results \nindicate that the value range propagation approach gives predictions which are substantially more accurate \nthan the best current heuristic approaches. Heuristics must still be used in the value range propagation \nmethod for branches with variables whose ranges are impossible to determine statically, such as loads \nfrom memory. The value range propagation method can be implemented over any factored dataflow representation \nwith a static single assignment property (such as SSA form or a dependence flow graph where the variables \nhave been renamed to achieve single assignment). The method can be implemented as an extension of standard \nconstant propagation for those who already compute the SSA form or a similar representation. It maintains \nthe linear runtime behavior of standard constant propagation experienced in practice. Acknowledgments \nI would like to thank my PhD supervisor, Dr. John Gough, for providing many helpful comments during this \nwork and for reviewing various drafts of this paper. References [BallLams92] Thomas Ball and James R. \nLams. Optimally Profiling and Tracing Programs. Proceedings of the 19th AnnuaJ Symposium on Principles \nof Programnung Languages, January 1992, pages 59-70. [BallLams93] Thomas Ball and James R. Larus. Branch \nPrediction For Free. Proceedings of the SIGPLAN 93 Conference on Programmmg Language Design and Implementation, \nJune 1993, pages 30&#38;313. [Ban.rjee88] Utpal BanerJee. Dependence Analysis for Supercotnputing. Khrwer \nAcademic Pubfishers, 1988. [BernsteinRodeh91] Dawd Bernstein and Michael Rodeh. Global Instruction Scheduling \nfor Superscalar Machines. Proceedings of the SIGPLAN 91 Conference on Programming Language Design and \nImplementation, June 1991, pages 241-255. [CaIlahan+86] David Callahan, Keith D. Cooper, Ken Kennedy \nand Linda Torczon. [nterprocedural Constant Propagation. Procmdings of the SIGPLAN 86 Conference on Compiler \nConstruction, June 1986, pages 152-161. [ChangMahlkeHwu91] Pohua P. Chang, Scott A. Mahlke and Wen-Mei \nW. Hwu. Using Profile Information to Assist Cksic Code Optimization. Software Practice aad Experience \n21(12), December 1991, pages 1301-1321. [CooperHallKe.nedy92] Keith D. Cooper, Mary W. Hall and Ken Kennedy. \nProcedure Cloning, IEEE 1992 International Conference on Computer Languages, April 1992, pages 96-105. \n77 [Cytron+91] Ron Cytron, Jeanne Ferrante, Barry K. Rosen, Mark N. Wegman and F. Kenneth 7adeck. Efficiently \nComvutirw Static Sinde Assi.enment Form and the Controf Depe&#38;errce Gra~h. ACM T;ansact;ons on Programming \nLanguages and Systems 13(4), October 1991, pages 451-490. [Ellis85] John R. Ellis. Bulldog: A Compiler \nfor VLfW Architectures. PhD Thesis, Yale University, February 1985. Also available from MIT Press, 1986. \n[Fisher81] Joseph A. Fisher. Trace Scheduling: A Technique for Global Mtcrocode Compaction. JEEE Transactions \non Computers 30(7), July 1981, pages 478-490. [FisherFreudenberger92] Joseph A. Fisher and Stefan M. \nFreudenberger. Predicting Conditional Branch Dwecttorrs From Previous Runs of a Program. Proceedings \nof the 5th International Conference on Architectural Support for Programmmg Languages and Operating Systems \nOctober 1992, pages 85-95. [GroveTorczorr93] Dan Grove and Linda Torczort. Interprocedural Constant Propagation: \nA Study of Jump Function Implementations. Proceedings of the SIGPLAN 93 Conference on Programmmg Language \nDesign and Implementation, June 1993, pages 90-99. [Gupta93] RaJlv Gupta. Optimizing Array Bound Chzcks \nUsing Flow Analysis. ACM Letters on Programming Languages and Systems 2(1-4), March-December 1993. pages \n135-150. [Harrison77] Wllharn H. Harrrson. Compiler Analysts of the Value Ranges for Variables. IEEE \nTransactions on Software Engineering 3(3), May 1977, pages 243-250. [JohnsonPingalr93] Richard Johnson \nand Kershav Pingali. Dependence-Based Program Analvsis. Proceedirrm of the SIGPLAN 93 Conference on Programming \nLangu;ge Design and Implementation, June 1993, pages 78-89. [Karr84] Michael Kam. Code Generation by \nCoagufatiorr. Proceedings of the SIGPLAN 84 Symposmm on Compiler Construction, June 1984, pages 1-12. \n[Ki1dal173] G. A. KildaJ1. A Unij ied Approach to Global Program Optimization. Proceedings of the First \nAnnual Symposium on Prmclples of Programmmg Languages, October 1973, pages 194-206. [Kral194] Andreas \nKraH. [mrrrovirw Semi-Static Branch Predcctzon bv Code Replication. Proc~eding; of the S IGPLAN 94 Conference \non Programmrrrg Language Design and Implementation, June 1994, pages 97-106. [McFarhng89] Scott McFarling. \nProgram Optimization for Instruction Caches. Proceedin~s of the 3rd International SvmDosium on Architectural \nSupport f;r Programming Languages aid ~perating Systerns, April 1989, pages 183-191. [McFarlingHennessy86] \nScott McFarling and John L. Henness y. Reducing the Cost of Brarsches. Proceedings of the 13th Annual \nSymposium on Computer Architecture, June 1986, pages 396-403. [MetzgerStroud93] Robert Metzger and Sean \nS troud. Interprocedural Constant Propagation: An Empirical Study. ACM Letters on Programming Languages \nand Systems 2(1-4), March-December 1993, pages 213\u00ad 232. [MorrIs91 ] W. G. Morris. CCG: A Prototype Coagulating \nCode Generator. Proceedlrrgs of the SIGPLAN 91 Conference on Programming Language Design and Implementation, \nJune 1991, pages 45-58. [PettisHartsen90] Karl Pettis and Robert C. Hansen. Profile Guided Code Positioning. \nProceedings of the SIGPLAN 90 Conference on Programming Larrguage Design and Implementation, June 1990, \npages 16-27. [ReifLewls77] John H. Reif and Harry R. Lewis. Symbolic Evaluation and the Global Value \nGraph. Proceedings of the 4th Annual Symposium on Principles of Programrmng Languages, January 1977, \npages 104118. [Smith81] James E. Srmth. A Study of Branch Prediction Strategies. Proceedings of the 8th \nAnnual Symposmm on Computer Architecture, May 1981, pages 135-148. [SteenkrsteHennessy89] Peter A. Steerrkiste \nand John L. Hennessy. A Simple Interprocedural Register Allocation Algorithm and Its Effectiveness for \nLISP. ACM TransactIons on Programming Languages and Systems 11(l), January 1989, p~= 1-32. ~agner+94j \nTim A. Wagner, Vance Maverick, Swan L. Graham and Michael A. Harrison. Accurate Static Estmrutors for \nProgram Optimzzatlon. Proceednrgs of the SIGPLAN 94 Conference on Programming Language Design and Implementation, \nJune 1994, pages 85-96. ~afJ86] David W. Wall. Global Register Allocation at Link Time. Proceedings of \nthe SIGPLAN 86 Symposium on Compd.r Construction, June 1986, pages 264-275. wal188] Dawd W. WaJ1. Register \nWindows vs Register Allocation. proceedings of the SIGPLAN 88 Conference on Programming Language Design \nand Implementation, June 1988, pages 67-78. ~al191] Dawd W. WaH. Predicting Program Behavior Using Real \nor Estimated Profiles. Proceedirrm of the SIGPLAN 91 Conference on Programrmng Language De;gn and Implementation, \nJune 1991, pages 59-70. wegmanZadeck91 ] Mark N. Wegmarr and F. Kenneth Zadeck. Constant Propagation \nwith Conditional Branches. ACM Transactions on Programming Languages and Systems 13(2), April 1991, pages \n181-210. [WULMUS94] Youfeng Wu and James R. Larus. Static Branch Frequency and Program Profile Analysis. \nProceedings of the 27th International Symposium on Microarchitecture, November 1994, pages 1-11. 78 \n \n\t\t\t", "proc_id": "207110", "abstract": "<p>The ability to predict at compile time the likelihood of a particular branch being taken provides valuable information for several optimizations, including global instruction scheduling, code layout, function inlining, interprocedural register allocation and many high level optimizations. Previous attempts at static branch prediction have either used simple heuristics, which can be quite inaccurate, or put the burden onto the programmer by using execution profiling data or source code hints.</p><p>This paper presents a new approach to static branch prediction called <italic>value range propagation</italic>. This method tracks the weighted value ranges of variables through a program, much like constant propagation. These value ranges may be either numeric of symbolic in nature.    Branch prediction is then performed by simply consulting the value range of the appropriate variable. Heuristics are used as a fallback for cases where the value range of the variable cannot be determined statically. In the process, <italic>value range propagation</italic>subsumes both constant propagation and copy propagation.</p><p>Experimental results indicate that this approach produces significantly more accurate predictions than the best existing heuristic techniques. The <italic>value range propagation</italic> method can be implemented over any &#8220;factored&#8221; dataflow representation with a static single assignment property (such as SSA form or a dependence flow graph where the variables have been renamed to achieve single assignment). Experimental results indicate that   the technique maintains the linear runtime behavior of constant propagation experienced in practice.</p>", "authors": [{"name": "Jason R. C. Patterson", "author_profile_id": "81100562354", "affiliation": "School of Computing Science, Queensland University of Technology, Brisbane, Qld 4001, Australia", "person_id": "P135486", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/207110.207117", "year": "1995", "article_id": "207117", "conference": "PLDI", "title": "Accurate static branch prediction by value range propagation", "url": "http://dl.acm.org/citation.cfm?id=207117"}