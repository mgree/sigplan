{"article_publication_date": "06-01-1995", "fulltext": "\n Unifying Data and Control Transformations for Distributed Shared-Memory Machines Michal Cierniak Wei \nLi Department of Computer Science University of Rochester Rochester, {ciemi-ak, wei}@cs Abstract We \npresent a unified approach to locality optimization that employs both data and control transformations. \nData trans\u00adformations include changing the array layout in memory. Control transformations involve changing \nthe execution order of programs. We have developed new techniques for com\u00adpiler optimizations for distributed \nshared-memory machines, although the same techniques can be used for sequential ma\u00adchines with a memory \nhierarchy. Our compiler optimizations are based on an algebraic rep\u00adresentation of data mappings and \na new data locality model. We present a pure data transformation algorithm and an al\u00adgorithm unifying \ndata and control transformations. While there has been much work on control transformations, the opportunities \nfor data transformations have been largely rle\u00adglected. In fact, data transformations have the advantage \nof being applicable to programs that cannot be optimized wiith control transformations. The unified algorithm, \nwhich per\u00adforms data and control transformations simultaneously, offers improvement over optimizations \nobtained by applying data and control transformations separately. The experimental results using a set \nof applications on a parallel machine show that the new optimizations improve performance significantly. \nThese results are further analyzed using locality metrics with instrumentation and simulation. This work \nwas supported in part by an NSF Research Inltlation Award (CCR-9409 120) and ARPA contract F19628-94-C-0057. \nPermission to copy without fee all or part of this material is granted provided that the copies are not \nmade or distributed for direct commercial advantage, the ACM copyright notice and the title of the publication \nand its date appear, and notice is given that copying is by permission of the Association of Computing \nMachinery.To copy otherwise, or to republish, requires a fee and/or specific permission. SIGPLAN 95La \nJolla, CA USA 0 1995 ACM 0-89791 -697-2/95/0006... $3.50 NY 14627 .rochester. edu 1 Introduction Many \nusers find it easier to write parallel programs using a shared-memory programming model. As a result, \nmany mod\u00adern parallel machines are built with a global shared memory space, although the physical memory \nhas to be distributed for scalability. Examples include research projects such as the Stanford DASH and \nFlash, the MIT Alewife, commer\u00adcial machines such as the KSR 1 and KSR2 from Kendall Square Research \nand the T3D from Cray Research. Develop\u00adment is continuing on distributed memory message-passing machines \n(e.g. the Intel Paragon and TMC CM-5) and net\u00adwork (cluster) of workstations, yet even for these machines \nthere are efforts to implement a shared-memory program\u00adming model through runtime and kernel-level emulation \nof shared memory. Representative systems include Ivy [21], Munin [8] and others [27]. Most shared-memory \nmachines, both hardware and soft\u00adware based, rely on data caching to exploit data locality and reduce \ncommunication. Cache coherence must be main\u00adtained for multiple copies of the same data on multiple pro\u00adcessors. \nA large body of work now exists in the area of coherence protocols for large-scale multiprocessors. These \nprotocols vary from hardware-only implementations [25] to software shared memory emulations for message-passing \nmachines [27]. There are hybrid techniques that incorpo\u00adrate both migration and remote reference on NUMA \nma\u00adchines [10, 5, 29]. To reduce the cost of data coherence and communication, data locality must be \nexploited. When locality is exploited by only run-time, kernel, or hardware-level policies that observe \nprogram behavior from below, false sharing becomes a major problem. Informally, false sharing can be \ndescribed as follows: two or more pro\u00adcesses access non-overlapping portions of the same coher\u00adence block \n(at least one of them with writes), causing unnec\u00ad essary coherence traffic and data movement. False \nsharing has been found to be a serious obstacle to high performance on distributed shared memory machines \n[12, 4, 6]. Previous work on compiler algorithms for cache locality has been on loop transformations, \ni.e., control transforma\u00adtions. Wolf and Lam [32] focus on loop tiling of the inner\u00admost loops as a means \nof achieving cache locality. They try all possible subsets of the loops in the loop nest. The subset \nthat can be brought into the innermost position, and has the best objective function from the reuse vector \nspace model is chosen to be tiled. The techniques by Ferrante, Sarkar, and Thrash [14] estimate the number \nof distinct cache lines used by a given loop in a loop nest. Given this estimate, they compute the number \nof cache misses for a loop nest. Carr, McKinley and Tseng [7] developed a simple memory model that can \nbe used for both perfectly and imperfectly nested loops. They used loop permutations, fusion and distribution \nfor transformations. Gannon, Jalby and Gallivan [15] in\u00adtroduced the notion of uniformly generated data \nreuse, and proposed the notion of windows to capture data reuse. Eisen\u00adbeis, Jalby, Wlndheiser and Bodin \n[13] used the windows to develop a strategy to explicitly manage the data transfers to local memory, \nPorterfield [28] studied the problem of esti\u00admating the number of cache lines for uniprocessor machines, \nwhen the cache line size is 1. L1 [24] developed a new reuse model and algorithms that can optimize for \nnot only dense matrix algorithms but also banded-matrix algorithms. The work by Eggers and Jeremiassen \n[12] and by Bian\u00adchini and LeBlanc [4] showed that for some programs, pro\u00adgram restructuring and data \nrestructuring can eliminate or reduce false sharing so that performance can be improved. However, these \ntransformation techniques are all performed by hand on specific application programs. Dubois et al. describe \na hardware mechanism to eliminate misses due to false sharing, but at the expense of very large amounts \nof other communication [11 ]. While data transformations are new in compiling for dis\u00adtributed shared \nmemory machines, automatic data distribu\u00adtion for distributed message passing machines has been in\u00advestigated \nby Balasundararn and others [2], by Hudak and Abramham [18] for sequentially iterated parallel loops, \nby Knobe et al. [19] for SIMD machines, by Li and Chen [20] for index domain alignment, by Ramanujam \nand Sadayap\u00adpan [30] who find communication-free partitioning of arrays in fully parallel loops, by Gupta \nand Banerjee [16] with a constraint-based approach, by Anderson and Lam [1] on data alignment and parallelism, \nby Chatterjee, Gilbert, Schreiber and Teng [9] on array alignment and by Bau, Kodukula, Kotl\u00adyar, Pingali \nand Stodghill [3] on a clean linear algebra solu\u00ad tion to the alignment problem. However, the data mapping \nissues are sufficiently different on distributed shared memory machines. For example, neither data reuse \nnor false sharing is considered in any of the above approaches, since they are usually irrelevant for \nmessage passing distributed memory machines. In this paper, we present an algebraic representation of \ndata mappings, a data locality model, a new data transforma\u00adtion algorithm for locality, and a unified \napproach to local\u00adity improvement with both data and control transformations for distributed shared memory \nmachines. The experimen\u00ad real AIIOOO, 1000], B[1OOO,1000] for i =1, 1000 forj = 1, 1000 A[i, j] =2* B[ij \nj] endfor endfor (a) Original program real A[1OOO, 1000], B[1OOO,1000] forj = 1, 1000 for i =1, 1000 \nA[i, j] =2* B[i, j] endfor endfor (b) After loop interchange Figure 1: A Simple Example tal results using \na set of applications on a parallel machine show that the new optimizations improve performance sig\u00adnificantly. \nThese results are further analyzed using locality metrics with instrumentation and simulation. The rest \nof the paper is organized as follows. In Sec\u00adtion 2, data and control transformations are compared to \nshow that neither is superior to the other, and a unified ap\u00adproach incorporating both data and control \ntransformations simultaneously is desired. In Section 3, an algebraic repre\u00adsentation of data mappings \nis described. The representation is used to define a locality model in Section 4. With the locality model, \noptimizations with data transformations are developed in Section 5. The unified approach is discussed \nin Section 6. Experimental methodology and results are pre\u00adsented in Section 7. Finally, conclusions \nare in Section 8. 2 Motivation In this section,we will demonstrate the need for both control and data \ntransformations by providing examples for which neither of those approaches alone will produce the best \nlo\u00adcality. Furthermore, we will show that applying data and control transformations separately will not \ngenerate the best code either. Therefore a unified approach is desired. The differences between control \nand data transformations will be illustrated with simple examples. The program in Figure 1a will be used \nto show that in some cases control and data transformations can lead to similar performance gains. Assume \nthat arrays A and B have the default column major layout. We can see that they both have poor locality \nin this loop nest. 2.1 Control Transformations -We can ignore data dependence. Again, if we make sure \nthat none of the programming language As discussed in Section 1, the problem of control transfor\u00adconstructs \ndiscussed in Section 2.2 (pointer arith\u00admations, i.e., loop transformations is relatively well unders\u00admetic \netc.) is used for the arrays we want to remap, tood. We will not describe details of any of the known \ndata transformations are always legal. approaches [32, 7, 15, 13, 24]. -They can be used to optimize \nexplicitly parallel Different transformations may be used to improve data programs. For programs with \nuser level paral\u00adlocality. For the code fragment from Figure la, a simple IIoop lelism, loop transformations \nare generally not pos\u00adinterchange resulting in the code shown in Figure lb, will sible (except for the \nsequential portions of the pro\u00adimprove the locality of data accesses, gram). The reason is that the programmer \nhas al\u00adready decided the parallelization. Often, synchro\u00ad 2.2 Data Transformations nization primitives \nare inserted which seriously limit or make impossible any loop transformations. Data locality of the \ncode fragment from Figure 1a, can also be improved by a data transformation. If, without changing the \ncode, we decide to use row major layout for both amays, we will again obtain the optimal locality as \nin Section 2.1. The way some of the current programs are written ma:y be fori=l, n a serious obstacle \nin applying data transformations. Existing forj = 1, n programming languages allow programs whose correctness \nfork =1, n relies on the particular array layout. Mechanisms that can C[i, j] = C[i, j] + A[i, k] * B[k,j] \nbe blamed for that include pointer arithmetic and creating endfor aliases of different types by the use \nof common blocks or endfor type casts. endfor fori=l, n 2.3 Comparisons forj = 1,n Each of the approaches \ndescribed in Sections 2.1 and 2.2 has fork= 1, n some advantages and disadvantages. Before arguing that \nthe F[i, j] = F[i, j] i-E[i, k]* C[k, j] unified approach is needed let us discuss some advantages of \nendfor pure control and data transformations. endfor endfor Control transformations -Loop transformations \nare well understood. A lot of work has been done to develop loop transformat-Figure 2: Multiplication \nof Three Matrices: Original Code ions for locality and some of the algorithms hi~ve been implemented \nsuccessfully. For some programs, neither data nor code transformations alone will result in the best \npossible data reuse. We will -They may improve temporal locality. Data trans\u00addemonstrate this on the \nexample shown in Figure 2. Thisformations cannot achieve that. program computes a product of three square \nmatrices: F = -They work in some cases in which data transfor- EAB. mations cannot be applied because \nof conflicts be- Assume default column major mapping (similar reasoning tween different references to \nthe same array. As may be used for row major mapping). There are three pos\u00adan example, consider the code \nfragment created sible approaches to optimizing this program. Consider all of by concatenating loop nests \nfrom Figure la and them to show that for this example, we need both data and Figure 1b. The first loop \nnest requires row major transformations. control and the second one column major mapping. We Data transformations: \nIf we do not apply loop trans\u00adcan however improve locality by performing loop formations, the algorithm \nfrom Section 5 will show that interchange in one of the loop nests. the best locality is achieved when \narrays A and E have o Data transformations row major organization, arrays B and F have column major organization, \nand the array C cannot have optimal -They work for imperfectly nested loops. Although locality in both \nloop nests (the optimal mapping for the loop transformation can be applied to many in\u00adfirst loop nest \nis row major and for the second loop nest stances of imperfect loop nests, they cannot be it is column \nmajor). used for some complex loop nests. As long as the conditions of Section 2.2 are met, we can always \nLoop transformations: If we assume that all arrays change the mapping of an array. have column major \norganization, the best locality can linear mapping function is represented as a vector, where the forj \n= 1,n for i= 1,n fork=l, n C[i, j] = C[i, j] + A[i, k]* B[k,j] endfor endfo r endfor forj = 1,n fori=l, \nn fork=l, n F[i, j] = F[i, j] + E[i, k] * C[k, j] endfor endfo r endfor Figure 3: Multiplication of Three \nMatrices: Control and Data Transformations be achieved if we apply loop interchange in both loop nests \nto make the i loop innermost [24]. o Data and Loop Transformations: We can use the ap\u00adproach presented \nin Section 6 to generate code with even better locality. The code is shown in Figure 3 and the following \ndata mappings must be used: C, B and F column major; A and E row major. Note that this result cannot \nbe obtained by applying data and loop transformations one after another (in any order). From the above \ndiscussion, we conclude that applying control and data transformations separately will not give the best \nprogram. A unified approach that utilizes control and data transformations at the same time becomes necessary. \n 3 Algebraic Representation of Data Mappings We will first present a formal representation of array \nmap\u00adpings. The representation will be used in the locality model described in Section 4. 3.1 Representing \ndata mappings We consider data mappings for arrays of any dimensionality. We first discuss linear mappings \nand extend them later to affine mappings. A mapping is a function from a vector of array subscripts to \nan offset from the start of the memory block allocated for the array. Let us define a subscript vector \nto be the vector of array subscripts, where the ith element in the vector is the subscript from the ith \ndimension of the amay reference. A product of the subscript vector and the mapping vector is the offset. \nFor example, if we assume that the array was declared as real A[n, n] ; and consider a reference to an \narray element A [i, j ] , the subscript vector S is () ~ A mapping corresponding to a row major organization \nis a n ()i vector m = 1 To apply the mapping, we compute the inner-product of the subscript vector and \nthe mapping vector. For our example, sTrn= (ij) ; =in+j () () which indeed corresponds to the row major \nmapping. Col\u00ad1 umn major mapping would be represented as a vector n ., and would produce the offset i \n-I-nj. Linear mappings as described above are convenient for lo\u00adcality analysis. The actual mappings \nused in code generation should be extended to include a constant offset. Note that we can use the linear \npart of an affine mapping for locality analysis and use full affine mappings in the generated code. This \nis possible, because the constant offset does not change the locality properties of array accesses if \nwe do not consider inter-an-ay interferences. For compact representation, we can include the offset in \nthe mapping vector. Now, subscripts and mapping vectors for n-dimensional arrays will be comprised of \nn + 1 elements. For the example above and the constant offset of O, we have n (ijl) 1 =in+j o () This \nmapping vector describes arrays whose subscripts range from Oto n 1, as in the programming language \nC. In Fortran arrays have column major organization and subscripts range from 1 ton, so the analogous \nexpression is ()1 (ij 1) -(nn+ 1) 3.2 Constraints on mapping =n(j 1)+i 1 vectors 3.2.1 Unambiguity A \ndata important mapping must condition is satisfy that it certain is a one conditions. The to one mapping. \nmost This is needed to ensure that every subscript vector corresponding to a legal array reference maps \nto a unique offset. Not every mapping vector satisfies the unambiguity condi\u00adtion. An obvious observation \nis that for all dimensions of size greater than one, the corresponding element in the mapping vector \nmust not be zero. As we show below, other constraints exist. Without loss of generality, we use two-dimensional \nar\u00adrays to illustrate tbe ideas. It is straightforward to generalize tbe results to arrays of any dimensionality. \nSquare matrices Let us consider tbe following declaration (in the rest of this section, we assume that \nsubscripts lower bound is O) real A[n, n] ; a Let themapping vectorbem = b Let a,b > 0. () c We will \nshow that atleastone of a, b must be greater than or equal to n. Assume that a < n and b < n. Then, the \nsubscript vectors S1 = (b O I)T and S2 = (O a l)T both maptothe offset ab+c. Vectors S1 and S2are legal \nbecause a,b < n. Therefore at least one of a, bmust be greater than or equal to n. The condition on the \nsign of a and b can be relaxed. Rather than having a, b >0, we only require a, b # O. With this as\u00adsumption, \nsimilar reasoning shows that at least one of Ia \\, lb I must be greater than or equal to n. Note that \nthis is a necessary but not sufficient condition. Rectangular matrices For an array real A[n, k] ; Let \nthe mapping vector be m = (a b C)T. Let a, b >0. We will show that we must have either a z k or b z n. \nAssume that a < k and b < n. Then, the subscript vectors S1 = (b OI)T and S2 = (Oa I)T both map to the \noffset ab + c. Vectors S1 and S2 correspond to legal references to the array A because a and b are botb \nwithin bounds for the subscripts of A. Therefore we must have either a ~ k or b~n. This result can be \ngeneralized to matrices of any dimen\u00adsionality. 3.2.2 Dense mappings In most applications, we want the \nmappings to be dense. That is if the array contains n elements, we want offsets for all valid array references \nbe in the range 0, ., ., n 1. Note that the dense mapping is not required for correctness. On the other \nhand the unambiguity condition considered in Section 3.2.1 must be satisfied for the mapping to be valid. \nAll references allowed Let us consider tbe following array of size s = kn as an example real A[n, k] \n; Let the mapping vector be m = (a b C)T. Let us first assume for simplicity that a, b > 0 and c = O. \nWe have shown in Section 3.2.1 that a ~ k or b ~ n. Let us consider the reference A [ n 1 ] [ k 1 ] for \nboth cases. The offset < is a(n l)+ b(k 1). Themaximum offset is~mlX = nm 1. a~k:Ifb~2wehave~~ k(n \nl)+2k 2= k(n + 1) 2. We see that< > ~~:,X provided that k >2. Therefore we must have b <2, i.e., b \n= 1 and a = k.  b > n: Similarly, we must have a = 1 and b = n.  We can see that for two-dimensional, \nsquare arrays, for a given constant offset c, there are only two legal, linear map\u00adpings that are dense. \nOne of them, ( 1 n O)T, corresponds to column major orientation, the other one (k 1 O)T, corre\u00adsponds \nto row major orientation. Similarly, it can be shown that for an d-dimensional array, there are exactly \nd! legal, dense, linear mappings. Banded matrices If we have more information about the range of array \nsubscripts we may be able to allocate less memory than indicated by the product of the sizes in all dimensions \nof the acray. One case that frequently appears in scientific applications is that of a banded matrix. \nA banded matrix has non-zero elements only along the diagonals of the matrix. A matrix A has lower bandwidth \np if Aij = O for i > j + p and upper bandwidth q ifAij =Ofor j >i+ q. There is no need to store the zeros \nin memory. For a matrix 1ike that we can allocated memory using one of the mappings described below. \nAssume that we have the following declaration: real A[n, n] ; We can establish that this array contains \na banded matrix by program analysis or by use of programmer annotations. Assume that array A bas a lower \nbandwidth p and an upper bandwidth q. If there are no accesses to elements outside tbe band de\u00adfined \nby p and q,the following four mappings can be used: (P+q, 1, P)Tj  (l, P+ql q)Tj  (1 n, n, pn)~, and \n (n, 1 n, qn)T.  All of these mapping will result in memory being allocated for only the n(p + q + \n1) non-zero elements.  4 Locality Model We want to optimize programs for execution time. To exploit \nthe memory hierarchy, data reuse has to be maximized. To simplify our discussion, we will consider computers \nwith large main memory and smaller, but faster cache memory. Cache hit ratio [17] is one metric for quantifying \ndata reuse. The hit ratio for a given run of a program is a non-trivial function of machine parameters, \noperating system policies, load on the machine and the access pattern of the program itself. 209 A more \nmachine-independent metric, which can be used consider group reuse. Therefore expressions in subscripts \nof in compilers, is reference distance. We measure how many an array reference can be described by an \naccess matrix [22]. different cache lines are accessed between two references to In our exam~le: the \nsame cache line. We observe that this reference distance can be used to guide program optimization. As \na locality model, we propose a stride vector to measure the locality of array references in a loop nest. \nIn Section 5, this model is used for data optimizations. In Section 6, we show how to integrate the stride \nvectors formalism with trans\u00adformation matrices, which are commonly used to represent loop transformations. \nIn this paper we propose away to com\u00adpactly represent loop transformations and data mappings. 4.1 Reference \ndistance We use reference distance as a metric of the quality of data locality. This metric is not as \naccurate for a given machine as the cache hit ratio, but it has the advantage of being relatively independent \nfrom machine parameters. Reference distance for a given memory access is defined to be the number of \ndistinct cache lines accessed since the last access to the same cache line (or O if the cache line has \nnot been accessed before). To analyze the program behavior, we create a histogram for all interesting \nreferences. The goal of the locality opti\u00admization is to decrease the distances for critical references. \nNote that to decrease the distance for some references, we may have to increase the distance for others. \n 4.2 Stride vectors Our approach to representing data reuse for different data mappings uses a new concept \nof a stride vector instead of the more traditional reuse vectors [32, 24]. Elements of the stride vector \ngive us information about data reuse. If an element is O, then this loop carries temporal reuse, if an \nelement is less than the size of a cache line, then the loop has spatial reuse. real A[n, n]: for i =1, \nrtJ2 forj = 1, n/2 A[i, ifj] = ... Figure 4: lZxample 1 Consider the array reference in the example shown \nin Fig\u00adure 4. The indexing function is defined by an affine mapping and can be used to compute the subscripts \nvector: Let u be a vector of loop variables. For our example / .\\ u= . . With this notation, the subscript \nvector can be .? computed as: S = Au. For our example: ( ) =Au=(:  N:)=(a We have already seen that \nthe subscript vector can be used to compute the offset for an array reference < = ST m. If we assume \nrow major mapping for our example, the offset is (= STmr= (i, i+j) ~ =(n+l)~+j () The stride is the difference \nbetween offsets for this reference in two subsequent iterations (i. e., the stride for loop i assumes \ninstances of the reference with the loop variable of i differ by the step size and all other loop variables \nare constant). In our case, the stride in the inner loop is 1 and the stride in the outer loop is (n \n+ 1). We can represent this information more concisely with the stride vector. In our case the stride \n() n+l vector is Vr = 1 We can comp;te the st~ide vector from the mapping vector and the access matrix: \nv = AT m. Let us consider mc = (1, n)T and mT = (n, I)T which represent column major and row major mappings \nrespectively. c=ATmc=(: X)=(nn T=ATmT=(: m=(n~ ) Note that the formula for the offset can be rewritten \nas: ~ = (Ati)Tm = uTATm = UTV 4.3 Optimizations 4.3.1 Desired stride vectors Let us examine our two \nstride vectors v. and VCdefined ear\u00adlier. None of them shows temporal reuse, but row major mapping will \ncause spatial reuse in the inner loop and col\u00ad umn major mapping will have poor reuse. To exploit spatial \nlocality, we want the stride 1 in the inner loop. Therefore, row major mapping is preferable for this \narray reference and this loop nest. We consider separately optimal stride vectors for unipro\u00adcessor and \nparallel programs. Let w be a stride vector: v] l-q IJ= We will use the linear part of the indexing \nfunction. The . . . constant part of the affine mapping can be ignored if we do not N H where N is the \ndepth of the loop nest enclosing this array reference. 4.3.2 Optimizing for sequential programs For \nthe best data reuse, inner loops should have better data reuse than outer loops. This increases the likelihood \nof cache hits. So, visoptimalifv, ~vz+l fori = 1,. ,., N 1. 4.3.3 Optimizing for parallel programs For \nparallel programs the optimality condition is different. Good locality for parallel programs means good \ndata reuse on one processor and little false sharing between proces\u00adsors. In terms of stride vectors, \nwe want all elements that correspond to sequential loops to satisfy the condition frc}m Section 4,3.2 \nand at the same time each of those elements should be less than any element corresponding to a parallel \nloop. If possible, strides in all parallel loops should be greater than the coherency unit for a given \nmachine to avoid false sharing. To simplify the notation, we assume in this paper that in each loop nest \nall parallel loops are the outermost loops, so that the condition from Section 4.3.2 can be used. Note \nth~at the more general case can be handled by small modifications of the algorithms presented in the \nfollowing sections.  4.4 Application of stride vectors Stride vectors can be used to decide which data \nmappings and loop transformations should be used to optimize data reuse, Section 5 presents an algorithm \nfor deciding optimal map\u00adpings for arrays if only data transformations are performed. Next, Section 6 \nshows how to apply both control and data transformations to improve data locality. 5 Data Transformations \nIn this section we will show how the best mapping for an array can be chosen if we apply data transformations \nonly. 5.1 Single reference per array Let us consider the simple example from Figure 4. The 10 access \nmatrix is A = We want to decide the () 11 mapping for A. From the discussion in Section 3.2.1, we know \nthat there are only two choices for linear mappings: mr = (n, 1)T for row major and m. = (1, rt)T for \ncolumn major. As shown in Section 4.2, we can now compute stride veo tors for both mappings. From the \nstride vectors we see that row major mapping is preferable for this array reference and this loop nest, \nAs discussed in Section 3.2.2, there are d! different dense mappings, where d is the number of dimensions \nof the array, Factorial is a fast growing function, but we have rarely seen an array in a real-life application \nwith more than 5 dimensions. The vast majority of arrays used in scientific applications have less than \n3 dimensions. Therefore the brute-force algortihm that tries all possible dense, linear mappings and \nuses the optimality condition from Section 4.3.2 to find the mapping that results in the best stride \nvector is fast in practice. 5.2 Multiple references per array The problem may become significantly harder \nif there are many references for one array. It is possible that different references would require conflicting \nmappings for optimum data locality. In that case, we must use some strategy to resolve the conflict. \nWe can for instance. Choose the default mapping in case of any conflict. This is a simple way of ensuring \nthat the proposed data mapping will always be at least as good as the default mapping.  Give priorities \nto different references of the same ar\u00adray and satisfy the locality requirements for the highest priority \nreference first.   6 The Unified Approach In this section we will show how to unify loop transformations \nwith the data transformation approach presented in Section 5. We present the framework for representing \ncontrol and data transformations, which can be used to design algorithms for deciding the combination \nof loop transformation and data mappings that results in good data locality. It is hard to find an optimal \nsolution in this case and therefore the algorithm presented here is a heuristic. However, in most common \npatterns of code, the generated code will possess the optimal data locality. 6.1 Linear Loop Transformations \nThe control transformation we use here is the linear loop transformation theory developed in [23]. This \nframework makes it possible to employ complex program transforma\u00adtions to improve data locality and eliminate \nfalse sharing for banded matrix programs, This transformation theory is based on the use of integer lattices \nas the model of loop nests and the use of non-singular matrices as the model of loop transformations. \n() n+l Consider two nested loops. The points in the iteration ~=Vr= ATm space of this loop can be modeled \nas integer vectors in the 1 two dimensional space 232, where Z is the set of integers. n+l For example, \nthe iteration (t = 2, j = 3) can be represented WC= ATmC = n by the vector (2, 3). In general, points \nin the iteration space() 211 of a loop nest of depth n can be represented by integer vectors from the \nspace 2Zn We will focus on transformations that can be represented by linear, one-to-one mappings from \nthe iteration space of the source program to the iteration space of the target program. Linear, one-to-one \nmappings between iteration spaces can be lmodeled using integec non-singular matrices. Perform\u00ading a \nsequence of transformations corresponds to composing the mappings between iteration spaces, which, in \nturn, can be lmodeled as the product of the matrices representing the individual transformations. The \nissues of code generation and legality testing are described in [23]. The framework has been used for \nreducing non-local memory accesses on NUMA parallel machines [22].  6.2, Single array reference To \ncapture the data locality for array accesses, we will use the stride vector introduced in Section 4.2. \nTo compute the stride vector in the generated code, we have to apply the loop transformation to the array \nreference. Mathematically, we have to multiply the transformation matrix T by the access matrix A. Let \nus consider again the example from Figure 4. Assume that we want to apply loop interchange to obtain \nthe codle shown in Figure 5. real A[n, n]; for J =1, rr/2 for I = 1,n/2 AII, I+J] = ... Figure 5: Example \n1 after loop interchange Loop interchange for this loop nest is represented by the 01 matrix: T = 10 \n() Recall that the stride vector in the original code can be computed as ATna where m is the mapping \nvector for array A. We can compute the subscript vector in terms of the new loop indices S = AT-lU where \nu contains loop indices of the transformed loop nest. For our example, =(ww(:)=(,fJ) rheoffset for an \narray reference in the transformed code is ~ = STm = (AT-lu)~m = uT(T-])~ATm If all steps are unitary: \n( = UTW (see Section 4.2). Hence, the stride vector is v = (T 1)TATm. For our example and row major mapping, \nTo verify the correctness, we can compute the new stride vector directly. The access matrix in the code \nin Figure 5 is Az=~~..(. ) Hence, the stride vector for row major mapping is r=Afmr=(:   W)=(M which \nmatches the previous result. We want to find a loop transformation and an array layout that will result \nin the best data locality. More formally, we want to find a transformation matrix T and a mapping vector \nm that will result in a stride vector ZIwith elements in decreasing order. At the same time the transformation \nmatrix must satisfy data dependence and the mapping vector must represent a legal mapping. We use nonsingular \ntransformation matrices, therefore, we can rewrite v = (T l)TATm as, TTV = ATm In principle, we could \nfind the optimal solution by solving this equation (with the constraints for legality of transforma\u00adtion \nand mapping). This optimization is hard to solve. We propose a heuristic that is sufficient for many \ncommon patterns of code. The search space of possible loop transfor\u00admations is potentially infinite. \nIn our algorithm, we assume that the transformation matrix contains only values O and 1. We also assume \nthat we know the value of the stride vector v beforehand. We construct the matrix T row by row by trying \nall possible values for this row and all dense, linear mappings. We verify if the rows constructed up \nto the current step satisfy the above equation and the data dependence. This idea can be realized in \na two step algorithm. In the first step, we find the desired stride vector in the target code for every \narray reference. Note that because of the relation between different references to the same array, not \nall references will have the ideal stride vector as described in Section 4.3.1. In the second step, we \nconstruct the matrix TT row by row. 6.3 Multiple array references A similar algorithm can be used to \nsolve the most general problem of multiple array references in multiple loop nests. The search space \nis larger in this case than for the single array reference. If there are many arrays, we initially consider \nall possible mappings for every array. As the transformation matrices are being built, we constrain, \nthe set of acceptable mappings for every array. As in Section 5.2, the unified algorithm for multiple \narray references must implement conflict resolution. 7 Experiments In this section, we first describe \nthe experiment methodology and the applications used. Then we present the results and analysis.  T=(w(wo=(w \n 212 7.1 Methodology We performed three types of experiments. Native timings: Running the application \non the desired computer is the ultimate test of the optimizations pro\u00adposed here.  Hit (miss) ratios: \nWe use simulation and instrume,ata\u00adtion to find the fraction of memory accesses that were satisfied in \nthe cache. We show that our optimization increase hit ratio (and, of course, decrease miss ratio).  \nReference distance: We instrument the programs to measure reference distances for all array accesses. \n In our experiments we used an SGI Challenge. This ma\u00adchine has local caches ( lMB) on each node. Hardware \nsup\u00adports cache coherence. We present execution times as the results. To obtain hit ratios for uniprocessor \nexecutions, we use instrumentation for efficiency the performance impact of the instrumentation is significant \nbut not as large as the im\u00adpact of the detailed simulation. We use a simulator built with Mint [31 ] \nto obtain classification of misses into false sharing and other misses for parallel executions. Mint \nis a relatively fast simulator, but the level of detail causes a larger slowdown than the instrumentation. \nInstrumentation inserts a function call after every array references. The instrumentation code uses the \naddress of this reference to update instrumentation information. Note that because of the overhead of \ninstrumentation and simulation, the data sizes used in our experiments were smaller than data used in \nthe native runs. The total size of the data in each simulated run was smaller than the actual cache in \nSGI Challenge. Therefore, to show differences re\u00adsulting from better locality of a program, we had to \nsimulate machines with smaller caches than the physical configuration of our hardware. 7.2 Application \nsuite We have used a set of eight programs in our experiments. Three of those applications are sequential \nFortran programs which are automatically parallelized. The other five are ex\u00adplicitly parallel programs. \nThe code shown in Figure 6, is a simple benchmark that can benefit from unified data locality improvements. \nThe second program, MxMxM, is the example from Fig\u00adure 2 which was discussed in Section 2.3. This program \ncomputes the product of three square matrices. The third benchmark economics is a larger applica\u00adtion \nfrom the domain of economics modeling. It calculates the amount of goods shipped between supply and demand \nmarkets given supply and demand prices, transportation costs and tariffs [26]. We also use five explicitly \nparallel kernels: matrix mul\u00ad tiply (MxM), transitive closure of a graph (closure), ma\u00ad trix inversion \n(inversion), Gaussian elimination (Gauss), and Warshall-Floyd all-pairs shortest paths algorithm (Floyd]. \n fori=l, n forj =I,n A[i, j] = B[j, i]*C[i, j] + D[i, j]+ LOG(E[j, i]) endfor endfor fori=l, n forj = \n1,n B[i, j] = A[j, i]+ E[ij j] endfor endfor Figure 6: A simple benchmark  7.3 Execution times Executiontimes \ncodeanddatatransformations+-- IiF J 01 12345678 # processors Figure 7: Execution times for the simple \nbenchmark Execution times for the simple benchmark are shown in Fig\u00adure 7. We can see that both data \ntransformations and loop transformations alone improve performance of the program, but unified optimization \nresult in shorter execution times than either of those transformations alone. Unified trans\u00adformations \ndecrease the execution time by about 40% as compared to loop tranformations only. Figure 8 shows that \nthe unified set of optimization pro\u00adduces not only the best performance for the uniprocessor case, but \nalso the most scalable parallel program. Each speedup is relative to the optimized version of the sequential \npro\u00adgram, rather than the original un-optimized version. There\u00adfore speedups for one processor are always \n1 for all four versions of the program, even though Figure 7 demonstrates that the uniprocessors times \nof the optimized versions are different. The effect of scheduling (assigning iterations to proces\u00adsors) \nwas also eliminated as much as possible. The best possible scheduling was used in the un-optimized case \nso that there is no additional improvement in the optimized case 213 Speedups 81 A perfectspeedup \noflginal +--\u00ad 7 datatransformations+ codetransformations-+ codeanddatatransformations* 6\u00ad 5\u00ad 4\u00ad 3\u00ad 2\u00ad \n1 1 2345676 # processors Figure 8: Speedups for the simple benchmark dueto scheduling. Thescheduling \nalgorithm used in paral\u00adlel programs whose timings are presented in Figures 7 and 8 blocked iterations \nof parallel loops to minimize false sharing. If a naive scheduling algorithm, e.g. interleaving, were \nused in the original version, locality optimizations would have a bigger impact on the program behavior \nand the relative im\u00adprovements in performance would be larger. We can see that the version with unified \noptimizations scales better than any of the other three implementations. Even the uniprocessor execution \nbenefits from our approach, but the improvement increases with the number of processors. Executiontimes-MxMxM \n160~ I originalprogram+-\u00ad140 datatransformations+ \u00adcodetransformations+ 120 codeanddatatransformations+ \n100 i 80 \u00ad 40 \u00ad 20 I o! 12345678 # processors Figure 9: Execution times for the MxMxM program For \nthe double matrix multip~y, we have applied all three possibilities of locality optimization: data transformation \nalone. loop transformation alone and the unified transfor\u00admation (see the discussion in Section 2.3). \nWe can see in Figure 9 that for this program we have to apply both data and control transformations to \nobtain the shortest execution time (17% faster than loop transformations only). Again, the opti\u00admization \nhelp for both uniprocessor and parallel executions. Figure 10 shows the execution times for the economics \nprogram. We can see that applying transformations cuts the Executiontimes-economics 50 original + 45 \ndatatransformations+ 40 codetransformations* \u00adcodeanddatatransformations+ 35 k ~ 25 ~ ; .\u00ad 20 \u00ad15 \u00ad10 \n\u00ad 5 0 12345678 # processors Figure 10: Execution times for the economics program execution time by \nmore than half. Applying both data and code transformations produced the code 8% faster than the code \nwith loop transformations only. For some explicitly parallel programs we can only apply data transformations. \nThe code cannot be changed because the programmer has already decided on the parallelization and inserted \nsynchronization primitives that prevent the com\u00adpiler from using loop transformations. All five explicitly \nparallel kernels used in our experiments have this property. Therefore, we only show the performance \nof the original program and of the program with data transformations. transiive closure-speedup 61 A \nperfectspeedup 7 600vertices,original + 600ve!tices,datatransformations+ 1000veflicesrorlgmal~ 6 -1000 \nvetices, datetransformations+ 5 4 3\u00ad 2 1, I 01 1 2 3 45676 numberof processors Figure 11: Transitive \nclosure: speedup Figure 11 shows speedups of one of those kernels tran\u00adsitive closure for two data \nsizes: for graphs of 600 and 1000 vertices. As is expected, data size must be large enough to provide \nsufficient parallelism on larger numbers of proces\u00adsors. For this program speedups on up to three processors \nare good for the optimized version, but when more processors are used, the executions for 1000 vertices \nshow better speedups than executions for 600 vertices. Similar performance gains were observed for other \nexplic\u00aditly parallel applications. Figure 12 shows the execution time 214 Execution time improvement \nConsequently, we had to simulate caches with smaller sizes than the actual hardware present in our machines. \nWe varied Ongmd! W htdlmnhfomwm  the cache size from 16 Kbytes to 128Kbytes, the cache line 8 0 .= \nMxM Floyd Ck)s rc G.uw mvcrwn application Figure 12: Improvements in execution times for selected applications. \nof the optimized code as a fraction of the execution time of the original program. The applications are \na matrix multiply of two 1024 x 1024 matrices (MxM), shortest paths calcula\u00adtion of a graph with512 vertices \n(Floyd), transitive closure of a graph with 350 vertices (closure), Gaussian elimination for 400 equations \n(Gauss) and matrix inversion of a 500 x 500 matrix (inversion). All applications are parallel programs \nand were executed on 8 processors. Instrumentation results discussed in Section 7.4 shows that most of \nthe improvement in this case comes from eliminating false sharing. 7.4 Cache hit ratios Misses -direct-mapped \nI 16K 32K 64K cache size cache size was 32 bytes. Memory accesses classification Cachehll~ Fdlscshwingmnw \nOLhermmscs 2/<,ng 211M1, 41(YQ 4111Wh # processors/optimization Figure 14: Transitive closure: classification \nof memory ac\u00adcesses Figure 14 shows the breakdown of memory accesses in the transitive closure program \non 2 and 4 processors. For the original program (the left bar in each group) the number of misses and \nspecifically false sharing misses increases as the number of processors increases. For both numbers of \nprocessors, data transformations almost completely eliminate misses. Figure 11 shows that the performance \nimpact is very dramatic. 7.5 Reference distances o Orlgmd The experiments presented in this section \nshow that the opti\u00ad o d,,,  fr, \\lo n,,lc) , mization discussed in this paper decrease reference distances \n128K Figure 13: Normalized miss numbers for MxMxM Figure 13 shows numbers of misses for MxMxM for different \ncache sizes. We assume a direct-mapped cache. To make the graph more readable, miss numbers were normalized \nfor each cache size so that the unoptimized code had one unit of cache misses. As discussed in Section \n7.1 the instrumentation incurs a significant overhead in terms of the execution time. Therefore, we had \nto restrict ourselves to smaller data sizes. for many array references. Reference distance is a useful \nmet\u00adric, because it is not very closely tied to machine parameters and yet it can be used to successfully \npredict (qualitatively rather than quantitatively) the impact of locality optimiza\u00adtion. The cache line \nsize in all experiments in this section was 32 bytes. Figure 15 presents numbers of references whose \nreference distance falls into a given interval. We have divided the distances into intervals that best \nshow the effect of data trans\u00adformations on the economics program. We can see that the transformations \nincreased the number of references in the 5 10 interval and decreased the number of references with larger \ndistances in the 101 500 interval. References with other distances remained basically unchanged. Larger \nreference distance means higher probability that the cache line has been evicted since its because of \nthe significant impact of the had to run the program on small data sets all reference distances are small. \nFigure 16 shows reference distance MxMxM program from Figure 2 and its last use. Recall that instrumentation, \nwe which explains why intervals for the three optimized ver\u00ad 215 Reference Distances 180,1XX orIg!nJ \nIacml ~ DmTmmmmmn 140,WU )20SW n : 1( II,KX1: 2 ~ 80,C XI 2 60SUX), mtxxl 2(NYX ,,L Mm .. 0-4 5.10 \nI I-I(K )01-500 501-5326 DistanceInterval Figure 15: Reference distances forthe economics program. Reference \nDistances -MxMxM 9,003,0m2 o o-1 X,ooo,ocm @z 3-1oo 7,00il,oeo @ 101.5000 (ioco.000 z s S,oeaom 2 # \n4,00il,oe41 Ct 3,0eo,o(x 2,000%000 I ,Ooweo o Orlgllml (I m!IJns UMJCmm, code ml dm Optimization Figure \n16: Reference distances for MxMxM sions which were described in Section 2.3. The data here are presented \nin a slightly different way than for the economics program in Figure 15, because we compare four versions \nof the program instead of two. In the optimized code, the number of references in intervals corresponding \nto small distances increases and at the same time the number of references with large distances decreases \n(the total number of references does not change). The shifts of reference distances correlate well with \ndecreases in execution times presented in Figure 9, 8 Conclusions We have presented a unified approach \nto locality optimiza\u00adtion that employs both data and control transformations. We have developed new techniques \nfor compiler optimizations for distributed shared-memory machines, although the same techniques can be \nused for sequential machines with a mem\u00adory hierarchy. Our compiler optimizations are based on an algebraic \nrep\u00adresentation of data mappings and a new data locality model. We present a pure data transformation \nalgorithm and an algo\u00adrithm unifying data and control transformations. The unified algorithm, which performs \ndata and control transformations simultaneously, offers improvement over optimizations ob\u00adtained by applying \ndata and control transformations sepa\u00adrately. The experiments show that the new optimizations improve \nprogram performance significantly. 9 Acknowledgments We would like to thank Tom LeBlanc and Leonidas \nKon\u00adtothanassis for their helpful comments. References [1]J. M. Anderson and M. Lam. Global optimizations \nfor parallelism and locality on scalable parallel machines. Proceedings of ACM SIGPLAN 93 Conference \non Pro\u00adgramming Language Design and Implementation, .Tune 1993. [2] V. Balasundaram, G. Fox, K. Kennedy, \nand U. Kremer. An interactive environment for data partitioning and distribution. In Proc. 5th Distributed \nMemory Comput. Conf, April 1990. [3] D. Bau, I. Kodukula, V. Kotlyar, K. Pingali, and P. Stodghill. Solving \nalignment using elementary linear algebra. Proceedings of the Seventh Annual Workshop on Languages and \nCompilers for Parallel Computing, 1994. [4] R. Bianchini and T. J. LeBlanc. Software caching on cache-coherent \nmultiprocessors. In Proceedings of the Fourth IEEE Symposium on Parallel and Distributed Processing, \npages 521 526, December 1992. [5] William J. Bolosky, Robert P. Fitzgerald, and Michael L. Scott. Simple \nbut effective techniques for NUMA mem\u00adory management. In Proceedings of the Twelfth ACM Symposium on \nOperating Systems Principles, pages 19 31, Litchfield Park, AZ, December 1989. [6] William J. Bolosky \nand Michael L. Scott. False shal\u00ad ing and its effect on shared memory performance. In Proceedings of \nthe Fourth Usenix Symposium on Expe\u00adriences with Distributed and Multiprocessor Systems, pages 57 7 1, \nSan Diego, CA, September 1993. Also available as MSR-TR-93-1, Microsoft Research Labo\u00adratory, September \n1993. [7] S. Carr, K. McKinley, and C.-W. Tseng. Compiler op\u00adtimization for improving data locality. \nIn Proceedings of the 6th International Conference on Architectural Support for Programming Languages \nand Operating Systems, pages 252 262, October 1994. 216 [8] J. B. Carter, J. K. Bennett, and W. Zwaenepoel. \nImple\u00admentation and performance of Munin. In Proceedings of the Thirteenth ACM Symposium on Operating \nSys\u00adtems Principles, pages 152 1 64, Pacific Grove, CA, October 1991. [9] S. Chatterjee, J. R. Gilbert, \nR. Schreiber, and S. Tmg. Automatic array alignment in data-parallel programs. Proceedings of ACM Symposium \non Principles of Pro\u00adgramming Languages, 20, 1993. [10] Alan L. Cox and Robert J. Fowler, The implementation \nof a coherent memory abstraction on a NUMA multipro\u00adcessor: Experiences with PLATINUM. In Proceedings \nof the Twelfth ACM Symposium on Operating Systems Principles, pages 32-44, Litchfield Park, AZ, Decem\u00adber \n1989. [11] M. Dubois, J. Skeppstedt, L. Ricciulli, K. Ramamurthy, and P, Stenstrom. The detection and \nelimination of useless misses in multiprocessors. In Proceedings of the International Symposium on Computer \nArchitec\u00adture, pages 88 97, San Diego, CA, May 1993. [12] S. J. Eggers and T. E. Jeremiassen. Eliminating \nfalse sharing. In Proc. 1991 International Conference on Parallel Processing, 1991. [13] C. Elsenbeis, \nW. Jalby, D. Windheiser, and F. Bodin. A strategy for array management in local memory. In Proc. 3th \nAnnual Workshop on Languages and Compil\u00aders, August 1990. [14] J. Ferrante, V. Sarkar, and W. Thrash. \nOn estimating and exchange cache effectiveness. In Proceedings of the Fourth Workshop on Languages and \nCompilers for Parallel Computing, August 1991. [15] D. Gannon, W. Jalby, and K. Gallivan. Strategies \nfor cache and local memory management by global pro\u00adgram transformations. Journal of Parallel and Llis\u00adtributed \nComputing, 5:587 616, 1988. [16] M. Gupta and P. Banerjee. Demonstration of automatic data partitioning \ntechniques for parallelizing compilers on multicomputers. IEEE Transactions on Parallel and Distributed \nSystems, 3:179-193,1992. [17] J. Hennessy and D. Patterson. Computer Architecture: A Quantitative Approach. \nMorgan Kaufmann, 199C1. [18] D. Hudak and S. Abraham. Compiler techniques for data partitioning of sequentially \niterated parallel loc)ps. In Proc. ACM Int. Cor$ Supercomputing, June 1990. [ 19] K. Knobe, J. Lukas, \nand G, Steele. Data optimi za\u00adtion: Allocation of arrays to reduce communication on SIMD machines. Journal \nof Parallel and Distributed Computing, 8: 102 1 18, February 1990. [20] J. Li and M. Chen. Index domain \nalignment: Minimiz\u00ading cost of cross-referencing between distributed arrays. Technical report, Yale University, \n1989, [21] K. Li and P. Hudak. Memory coherence in shared vir\u00adtual memory systems. ACM Transactions on \nComputer Systems, November 1989. [22] W. Li and K. Pingali. Access normalization: Loop restructuring \nfor NUMA compilers. ACM Transactions on Computer Systems, 11 (4):353 375, November 1993. [23] W. Li \nand K. Pingali. A singular loop transformation framework based on non-singular matrices. Interna\u00adtional \nJournal of Parallel Programming, 22(2), April 1994. [24] Wei Li. Compiler cache optimization for banded \nma\u00adtrix problems. In 9th ACM International Conference on Supercomputing, July 1995. [25] D. J. Lilja. \nCache coherence in large-scale shared\u00admemory multiprocessors: Issues and comparisons. Computing Surveys, \n25(3):303 338, September 1993. [26] A. Nagurney, C. F. Nicholson, and P. M. Bishop. Spatial price equilibrium \nmodels with discriminatory ad val\u00adorem tariffs: formulation and comparative computation using variational \ninequalities. In Recent Advances in Spatial Equilibrium Modeling: Methodology and Ap\u00adplications. Springer-Verlag, \nHeidelberg, 1995. forth\u00adcoming. [27] B. Nitzberg and V. Lo. Distributed shared memory: A survey of issues \nand algorithms. Computer, 24(8):52 60, August 1991. [28] A. Porter field. Software Methods for Improvement \nof Cache Pe@ormance on Supercomputer Applications. PhD thesis, Rice University, May 1989. [29] R. P. \nLaRowe, Jr. and C. S. Ellis. Experimental compar\u00adison of memory management policies for NUMA multi\u00adprocessors. \nACM Transactions on Computer Systems, 9(4):3 19 363, November 1991. [30] J. Ramanujam and P. Sadayappan. \nCompile-time tech\u00adniques for data distribution in distributed memory ma\u00adchines. IEEE Transactions on \nParallel and Distributed Systems, 2, October 1991. [31] Jack E. Veenstra and Robert J. Fowler. Mint: \na front end for efficient simulation of shared-memory multi\u00adprocessors. Proceedings of the Second International \nWorkshop on Modeling, Analysis and Simulation of Computer and Telecommunication Systems (MASCOTS 94), \npages 201 207, January 1994. [32] M. Wolf and M. Lam. A data locality optimizing al\u00adgorithm. In Proc. \nACM SIGPLAN 91 Conference on Programming Language Design and Implementation, pages 30-44, June 1991. \n 217  \n\t\t\t", "proc_id": "207110", "abstract": "<p>We present a unified approach to locality optimization that employs both data and control transformations. Data transformations include changing the array layout in memory. Control transformations involve changing the execution order of programs. We have developed new techniques for compiler optimizations for distributed shared-memory machines, although the same techniques can be used for sequential machines with a memory hierarchy.</p><p>Our compiler optimizations are based on an algebraic representation of data mappings and a new data locality model. We present a pure data transformation algorithm and an algorithm unifying data and control transformations. While there has been much work on control transformations, the opportunities for data transformations have been largely neglected. In fact, data transformations have the advantage of being applicable to programs that cannot be optimized with control transformations. The unified algorithm, which performs data and control transformations <italic>simultaneously</italic>, offers improvement over optimizations obtained by applying data and control transformations separately.</p><p>The experimental results using a set of applications on a parallel machine show that the new optimizations improve performance significantly. These results are further analyzed using locality metrics with instrumentation and simulation.</p>", "authors": [{"name": "Micha&#322; Cierniak", "author_profile_id": "81100326251", "affiliation": "Department of Computer Science, University of Rochester, Rochester, NY", "person_id": "P196161", "email_address": "", "orcid_id": ""}, {"name": "Wei Li", "author_profile_id": "81452598957", "affiliation": "Department of Computer Science, University of Rochester, Rochester, NY", "person_id": "PP95031205", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/207110.207145", "year": "1995", "article_id": "207145", "conference": "PLDI", "title": "Unifying data and control transformations for distributed shared-memory machines", "url": "http://dl.acm.org/citation.cfm?id=207145"}