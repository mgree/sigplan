{"article_publication_date": "06-01-1995", "fulltext": "\n Global Code Motion Global Value Numbering Cliff Clickt Hewlett-Packard Laboratories, Cambridge Research \nOffice One Main St., Cambridge, MA. 02142 cliffc @hpl.hp.com (61 7) 225-4915 1. Introduction We believe \nthat optimizing compilers should treat the machine-independent optimizations (e.g., conditional constant \npropagation, global value numbering) and code motion issues separately. Removing the code motion requirements \nfrom the machine-independent optimiza\u00adtion allows stronger optimizations using simpler algo\u00adrithms. Preserving \na legal schedule is one of the prime sources of complexity in algorithms like PRE [18, 13] or global \ncongruence finding [2, 20]. We present a straightforward near-linear-time2 al\u00adgorithm for performing \nGlobal Code Motion (GCM). Our GCM algorithm hoists code out of loops and pushes it into more control \ndependent (and presumably less fre\u00adquently executed) basic blocks. GCM is not optimal in the sense that \nit may lengthen some paths; it hoists con\u00adtrol dependent code out of loops. This is profitable if the \nloop executes at least once; frequently it is very profit\u00adable. GCM relies only on dependence between \ninstruc\u00adtions; the original schedule order is ignored. GCM moves instructions, but it does not alter \nthe Control Flow Graph (CFG) nor remove redundant code. GCM benefits from CFG shaping (such as splitting \ncontrol-dependent edges, or inserting loop landing pads). GCM allows us to use a simple hash-based technique \nfor Global Value Numbering (GVN). i Thl~ work has ~n supported by ARPA through ONR want Noool 4-91\u00ad J-1989 \nand was done at the Rice University Computer Science Depar% ment Modern microprocessors with super-scalar \nor VLIW execution already reqmre global scheduhng, 2 We recpure a dominator tree, which requires O(cc(rz,n)) \nrime m build It is essentially linear in the size of the control flow graph, and very fast to build in \npractice. Permission to copy without fee all or part of this material is granted provided that the copies \nare not made or distributed for direct commercial advantage, the ACM copyright notice and the title of \nthe publication and its date appear, and notice is given that copyin is by permission of the Association \nof Computing Machinery. Yo copy otherwise, or to republish, requires a fee and/or specific permission. \nSIGPLAN 95La Jolla, CA USA 0 1995 ACM 0-89791 -697 -2/95/0006 ...$3.50 GVN attempts to replace a set \nof instructions that each compute the same value with a single instruction. GVN finds these sets by looking \nfor algebraic identities or instructions that have the same operation and identical inputs. GVN is also \na convenient place to fold constants; constant expressions are then value-numbered like other expressions. \nFinding two instructions that have the same operation and identical inputs is done with a hash-table \nlookup. In most programs the same variable is assigned many times; the requirement for identical inputs \nis a re\u00adquirement for identical values, not variable names. Hence GVN relies heavily on Static Single \nAssignment (SSA) form [12]. GVN replaces sets of value-equivalent instructions with a single instruction, \nhowever the single replacement instruction is not placed in any basic block (alternatively, think of \nGVN selecting a block at random; the resulting program is clearly incorrect). A following pass of GCM \nselects a correct placement for the instruc\u00adtion based solely on its dependence edges. Since GCM ignores \nthe original schedule, GVN is not required to build a correct schedule. Finally, GVN is fast, requiring \nonly one pass over the program. 1.1 Related Work Loop-invariant code motion techniques have been around \nawhile [1]. Aho, Sethi, Unman present an algo\u00adrithm that moves loop-invariant code to a pre-header before \nthe loop. Because it does not use SSA form sig\u00adnificant restrictions are placed on what can be moved. \nMultiple assignments to the same variable are not be hoisted. They also lift the profitability restriction, \nand allow the hoisting of control dependent code, They note that lifting a guarded division may be unwise \nbut do not present any formal method for dealing with faulting in\u00adstructions. Partial Redundancy Elimination \n(PRE) [18, 13] re\u00admoves partially redundant expressions when profitable. Loop-invariant expressions are \nconsidered partially re\u00addundant, and, when profitable, will be hoisted to before the loop. Profitable \nin this case means that no path is lengthened. Hoisting a computation that is used on some paths in a \nloop but not all paths may lengthen a path, and is not hoisted. GCM will hoist such control-dependent \nexpressions. In general this is very profitable. However, like all heuristics, this method sometimes \nlengthens pro\u00adgrams. GCM will move code from main paths into more control dependent paths. The effect \nis similar to Partial Dead Code Elimination [16]. Expressions that are dead on some paths but not others \nare moved to the latest point where they dominate their uses. Frequently this leaves no partially dead \ncode. GVN is a direct extension of local hash-table-based value-numbering techniques [1, 11]. Since it \nis global instead of local, it finds all the redundant expressions found by the local techniques. Local \nvalue-numbering techniques have been generalized to extended basic blocks in several compilers. GVN uses \na simple bottom-up technique to build partitions of congruent expressions. Alpern, Wegman and Zadeck \npresent a technique for building partitions top-down [2]. Their method will find congruences amongst \ndependence that form loops. Because GVN is a bottom-up technique it cannot find such looping congru\u00adences. \nHowever, GVN can make use of algebraic identi\u00adties and constant folding, which AlW/Z s technique can\u00adnot. \nAlso GVN runs in linear time with a simple one\u00adpass algorithm, while A/W/Z use a more complex O(rt log \nn) partitioning algorithm. Cliff Click presents ex\u00adtensions to A/W/Zs technique that include algebraic \nidentities and conditional constant propagation [10]. While this algorithm finds more congruences than \nGVN it is quite complex. Compiler writers have to trade off the expected gain against the higher implementation \ncost. PRE finds lexical congruences instead of value cori\u00adgruences [18, 13]. As such, the set of discovered \ncongru\u00adences (whether they are taken advantage of or not) only partially overlaps with GVN. Some textually-congruerlt \nexpressions do not compute the same value. However, many textually-congruent expressions are also value\u00adcongruent. \nDue to algebraic identities, many value cong\u00adruences are not textually equal. In practice, GVN finds \na larger set of congruences than PRE. Rosen, Wegman and Zadeck present a method for global value numbering \n[19]. Their technique and the one presented here find a near] y identical set of redun\u00addant expressions. \nHowever, the algorithm presented here is simpler, in part, because the scheduling issues are handled \nby GCM. Also GVN works on irreducible graphs; the R/WIZ approach relies heavily on program structure. \nFor the rest of this section we will discuss the inter\u00admediate representation used throughout the paper. \nWe present GCM next, in Section 2, both because it is useful as an optimization by itself and because \nGVN relies on it, In Section 3 we present our implementation of GVN. In Section 4 we present numerical \nresults showing the benefits of the GVN-GCM combination over more tradi\u00adtional techniques (e.g., PRE). \n 1.2 Program Representation We represent programs as CFGS, directed graphs with vertices representing \nbasic blocks, and edges repre\u00adsenting the flow of contro13 [1]. Basic blocks contain sequences of instructions, \nelementary operations close to what a single functional unit of a machine will do. In\u00adstructions are \nrepresented as assignments from simple (single operator) expressions (e.g., a := b + c ). We require \nthe program to be in SSA form [12]. We make extensive use of use-clef and clef-use chains. Use-clef chains \nare explicit in the implementa\u00adtion. All variables are renamed to the address of the structure that represents \nthe defining instruction; essen\u00adtially variable names in expressions become pointers to the expression \nthat defines the variable. This converts sequences of instructions into a directed graph, with the vertices \nbeing instructions and the edges representing data flow. It is not a DAG because SSA ~-functions are \nhandled like other expressions and can have backedges as inputs. This is similar in spirit to the value \ngraph [2] or an operator-level Program Dependence Graph [15]. In order to make the representation compositional \n[8], we need a $-function input that indicates under what condi\u00adtions values are merged. It suffices \nto make @functions keep a pointer to the basic block they exist in. We show an example in Figure 1. Both \nGCM and GVN rely solely on dependence for correct behavior, the original schedule is ignored (although \nit may be used as an input to the code\u00adplacement heuristic). To be correct, then, all depend\u00adence must \nbe explicit. LoAos and STORES need to be threaded by a memory token or other dependence edge [3, 15, \n23]. Faulting instructions keep an explicit control input. This input allows the instruction to be scheduled \nin its original basic block or after, but not be\u00adfore. Most instructions exist without knowing the basic \nblock they started in,4 Data instructions are treated as not residing in any particular basic block. \nCorrect behav\u00adior depends solely on the data dependence. An instruc\u00ad 3 Our implementation treats basic \nblocks as a special kind of instruction smular to Ferrante, Ottenstein and Warren s regwn nodes [151. \nThis allows us to simplifi several optimization algorithms unrelated to this ~aper. Our presentation \nwill use the more traditional CFG. The exceptions are instructions which can cause exceptions: loads, \nstores, division, subroutine catls, etc. Start: - : mioI / T1 1 m loop: \\ b:=a+l iz:=il+b c:=i2X2 \ncc := i2 < 10 br ne loop Figure 1 A loop, and the resulting graph to be scheduled tion, the instructions \nthat depend on it, and the instruc-4. Schedule all instructions late. We place instruc\u00ad tions on which \nit depends exist in a sea of instructions, with little control structure. The sea of instructions is \nuseful for optimization, but does not represent any traditional intermediate repre\u00adsentation such as \na CFG. We need a way to serialize the graph and get back the CFG form. We do this with a simple global \ncode motion algorithm. The global code motion algorithm schedules the free-floating instructions by attaching \nthem to the CFG S basic blocks. It must preserve any existing control dependence (divides, subroutine \ncalls and other possibly faulting operations keep a dependence to their original block) and all data \ndependence. While satis~lng these restrictions the scheduler uses a flexible and simple heu\u00adristic: place \ncode outside as many loops as possible, then on as few execution paths as possible.  2. Global Code \nMotion We use the following basic strategy: 1. Find the CFG dominator tree, and annotate basic blocks \nwith the dominator tree depth. 2. Find loops and compute loop nesting depth for each basic block.  \n3, Schedule (select basic blocks for) all instructions early, based on existing control and data depend\u00adence. \nWe place instructions in the first block where they are dominated by their inputs. This schedule has \na lot of speculative code. with ex\u00adtremely long live ranges. tions in the last block where they dominate \nall their uses. 5, Between the early schedule and the late schedule we have a safe range to place computations. \nWe choose the block that is in the shallowest loop nest possible, and then is as control dependent as \npos\u00ad sible. We cover these points in more detail in the sections that follow. We will use the loop in \nFigure 1 as a run\u00adning example. The code on the left is the original pro\u00adgram. On the right is the resulting \ngraph (possibly after optimization), Shadowed boxes are basic blocks, rounded boxes are instructions, \ndark lines represent con\u00adtrol flow edges, and thin lines represent data-flow (data dependence) edges. \n2.1 Dominators and Loops We find the dominator tree by running Lengauer and Tarjan s fast dominator finding \nalgorithm [17], We find loops using a slightly modified Tarjan s Testing flow graph reducibility [21 \n], Chris Vick has an excellent write-up of the modifications and on building the loop tree [22]. We used \nhis algorithm directly. The dominator tree and loop nesting depth give us a handle on expected execution \ntimes of basic blocks. We use this information in deciding where to place instruc\u00adtions in the following \nsections. The running time of these algorithms is nearly linear in the size of the CFG. In practice, \nthey can be computed quite quickly. We show the dominator tree and loop tree for our running example \nin Figure 2. CFG Dominator Loop Tree Tree Figure 2 CFG, dominator tree and loop tree  2.2 Schedule \nEarly Our first scheduling pass greedily places instructions as early as possible. We place instructions \nin the first block where they are dominated by their inputs.5 The only exception is instructions that \nare pinned into a particular block because of control dependence. These include PHI instructions, BRANCH/JUMP, \nSTOP/RETUE~N instructions (these end specific blocks). Faulting in\u00adstructions have an explicit control \ninput, so they cannot be moved before their original basic block.6 This sched\u00adule has a lot of speculative \ncode, with extremely long live ranges. We make a post-order DFS over the inputs, starting at instructions \nthat already have a control dependence ( pinned instructions). After scheduling all inputs to an instruction, \nwe schedule the instruction itself. We place the instruction in the same block as its deepest domina\u00adtor-tree \ndepth input. If all input definitions dominate the instruction s uses, then this block is the earliest \ncorrect location for the instruction. forall instructions i do if i is pinned then II Pinned instructions \nremain i.visit := True; II ... in their original block forall inputs x to i do II Schedule inputs to \npinned Schedule_Early( x ); II... instructions II Find earliest legal block for instruction i Schedule_Early( \ninstruction i ) { if i. visit = True then II Instruction is already return; II ... scheduled early? \ni. visit:= True; II Instruction is being visited now i. block;= roo~ II Start with shallowest dominator \n forall inputs x to i do II Schedule_Early( x ); II Schedule all inputs early if i. block .dom_depth \n<x. block .dom_depth then  i. block:= x. block; II Choose deepest dominator input 1 5 For some apphcatlons \n(e.g., trace scheduhng) the algorithm is not greedy enough. It does not move instmctions past $-functions. \n6 They can move after their original block, which only preserves partial correctness. 1. int first:= \nTRUE, x, sumo:= O; 2. while( predo ) {  2.1 suml := +( sumo, sumz ); 3. if( first ) 4. {first:= FALSE; \nX:= ..,; ] 5.1 sumz := sum I +x,  5.2 ) 1 1 c1 22 8 33 4 54 h CFG Dominator 5 Tree b u Figure 3 x s \ndefinition does not dominate it s use One final problem: it is easy (and occurs in practice) to write \na program where the inputs do not dominate all the uses. Figure 3 uses x in block 5 and defines it in \nblock 4. The dominator building algorithm assumes all execution paths are possible; thus it may be possible \nto use x without ever defining it. When we find the block of the ADD instruction in line 5.1, we discover \nthe deepest input comes from the assignment to x in block 4 (the other input, sum], comes from the PHI \nin block 2). Plac\u00ading the ADD instruction in block 4 is clearly wrong; block 4 does not dominate block \n5. Our solution is to observe that translation to SSA leaves in some extra PHI instructions, We show \nthe same code in Figure 4, but with the extra PHI instructions visible. In essence, the declaration of \nX. initializes it to the undefined value T,7 This value is then merged with other values assigned to \nx, requiring PHI instructions for the merging. These PHI instructions dominate the use of x in block \n5, This problem can arise outside loops as well. In Figure 5, the PHI instruction on line 3 is critical. \nWith the PHI instruction removed, the computation of ~(x) on line 4 will be scheduled early, in block \n2. However, block 2 does not dominate block 4, and y is live on exit from block 4. In fact, there is \nno single block where the inputs to ~ dominate ~s uses, and thus no legal schedule. The PHI instruction \ntells us that it s all right; the pro\u00adgrammer wrote code such that it appears that we can reach ~ without \nfirst computing it s inputs, 7 Pronounced top ; this notation is taken from the literature on constant \npropagation. 1. intfht:= TRUE, xo:= T, sumo = 0; 2. while( p-edo ) {  2. I sum] := $( Surno, sun n \n); 2.2 xl := $( X(),x3); 3. if(fzrst ) 4, { first:= FALSE; X2 := ..; ]  5. X3 := $( xl, x2); 5.1 sumz \n:= suml + X3; 5.2 } B$ 1 22 33 4 54 !iitl Dominator 5 CFG Tree - Figure 4 Now x s definitions dominate \nit s uses These PHI instructions cannot be removed. They hold critical information on where the programmer \nhas as\u00adsured us that values will be available. In our implemen\u00adtation of GVN, the optimization that would \nremove this PHI instruction is the one where we make use of the fol\u00adlowing identity: x =$(x, T). We ignore \nthis identity. In Figure 6, scheduling early moves the add % := a + 1 out of the loop, but it moves \nthe multiply c := iz x 2 into the loop. The PHI instruction is pinned into the block, The add iz := i, \n+ b uses the PHI, so it cannot be scheduled before it. Similarly, the multiply and the com\u00adpare use the \nadd, so they cannot be placed before the loop. The constants are all hoisted to the start of the pro\u00adgram. \n2.3 Schedule Late Scheduling early hoists code too much. Much of the code is now speculative, with long \nlive ranges. Witness the constants in Figure 6. They are all hoisted to the start of the program, no \nmatter where their first use is. To correct this, we also schedule late: we find the last place in the \nCFG we can place an instruction. Between the early schedule and the late schedule will be a legal range \nto place the instruction. We want to find the lowest common ancestor (LCA) in the dominator tree of all \nan instruction s uses. Find\u00ading the LCA could take O(n) time for each instruction, but in practice it \nis a small constant. We use a post-order DFS over uses, starting from the pinned instructions. After \nvisiting (and scheduling late) all of an instruction s children, we schedule the instruction itself. \nBefore scheduling, the instruction is in its earliest legal block; 1. int xo := T, zo:= .... 1.1 if( \nsometimeo ) 2. xl:= ...; 3. x,:= $( X(),xl );  3.1 if( sometime_latero ) 4. y := Z()+f(x2); 5, ... \n...  PY &#38;1 1 2 32 3 54 4 PDominator Tree 5 CFG Figure 5 The PHI instruction on line 3 is critical \n10 a READ() 1 ) m Start: i. [ [ \\ loop: X_ LEY!Y2L I/ I 1/ I v/ = Figure 6 Our loop example, after scheduling \nearly the LCA of its uses represents its latest legal block. The earliest block dominates the latest \nblock (guaranteed by the PHI instructions we did not remove). There is a line in the dominator tree from \nthe latest to the earliest block; this represents a safe range where we can place the in\u00adstruction. We \ncan place the instruction in any block on this line. We choose the lowest (most control dependent) position \nin the shallowest loop nest. For most instructions, uses occurs in the same block as the instruction \nitself. For PHI instructions, however, the use occurs in the previously block of the correspond\u00ad ing \nCFG edge. In Figure 5 then, the use of XOoccurs in block 1 (not in block 3, where the@ is) and the use \nof xl occurs in block 2. forall instructions i do if i is pinned then II Pinned instructions remain i. \nvisit:= True; II . in their original block forall usesy of i do II Schedule pinned insts outputs Schedule_Late( \ny ); //Find latest Iegat block for instruction i Schedule_Late( instruction i ) { if i. visit= True \nthen It Instruction is already return; II ... scheduled late? i. visit:= True; II Instruction is being \nvisited now Block lca := NU~, II Start the LCA empty forall uses y of i do { II Schedule all uses first \nSchedule_Late( y ); II Schedule all inputs late Block use := y.block; // Use occurs in y s block if y \nis a PHI then { // .. except for PHI instructions II Reverse dependence edge from i toy Pick j so that \nthe jth input of y is i II Use matching block from CFG use := y.block. CFG_pred[j]; 1 II Find the least \ncommon ancestor lca := Find_LCA( lea, use ); 1 ..use the latest and earliest blocks to pick final position \n 1 We use a simple linear search to find the least corn\u00admcm ancestor in the dominator tree. II Least \nCommon Ancestor Block Find_LCA( Block a, Block b ) { if( a = NULL) return b; // Trivial case // While \na is deeper than b go up the dom tree while( a.dom_depth < b.dom_depth ) a := a.immediate_dominatou \nII While b is deeper than a go up the dom tree while( b.dom_depth e a.dom_depth ) b:= b.immediate_dominator; \nwhile(a*b){ II While not equal a := a.immediate_dominator; II.. .go up the dom tree b := b.immediate_dominato~ \nII . .go up the dom tree } return a; II Return the LCA  ) 2.4 Selecting a Block In the safe range for \nan instruction we may have many blocks to choose from. The heuristic we use is to pick the lowest (most \ncontrol dependent) position in the shallowest loop nest. Primarily this moves computations out of loops. \nA secondary concern is to move code off frequently executed paths into if statements. In Figure 5, if \nthe computation of Z. in block 1 is only used in block 4 then we would like to move it into block 4. \nWhen we select an instruction s final position we affect other instructions latest legal position. In \nFigure 7, when the instruction b := a + 1 is scheduled before the loop, the latest legal position for \n1 is also before the loop. To handle this interaction, we select instructions final positions while we \nfind the latest legal position. Here is the code to select an instruction s final position: .,. Found \nthe LCA, the latest legal position for this inst. . We already have the earliest legal block. Block best:= \nlea; // Best place for i starts at the lca while lcu # i.block do // While not at earliest block do... \nif lca.loop_nest < best.loop_nest then // Save deepest block at shallowest nest best:= lea; lca := lca.immediate_dominato~ \n// Go up the dom tree ) i. block := bes~ II Set desired block The final schedule for our running example \nis in Figure 7. The MUL instruction s only use was after the loop. Late scheduling starts at the LCA \nof all uses (the last block in the program), and searches up the domina\u00ad tor tree for a shallower loop \nnest. In this case, the last block is at nesting level O and is chosen over the original block (nesting \nlevel 1). After the multiply is placed after the loop, the only use of the constant 2 is also after the \nloop, so it is scheduled there as well. The compare is used by the IF instruction, so it cannot be scheduled \nafter the loop. Similarly the ADD is used by both the compare and the PHI instruction. 3. Global Value \nNumbering Value numbering partitions the instructions in a program into groups that compute the same \nvalue. Our technique is a simple hash-table based bottom-up method [1 1]. We visit each instruction once, \nusing a hash-table to determine if the instruction should be in the same partition as some previously \nvisited instruction, Previously this methed has been used with great success for local value-numbering. \nWe extend to global value\u00adnumbering by simply ignoring basic block boundaries. We use the following algorithm: \n 1) Visit every instruction using a reverse postorder (RPO) walk over the dependency edges. 2) At each \ninstruction attempt to fold constants, use algebraic identities or find an equivalent instruction via \na hash-table lookup. The hash table allows us to assign the same value-number to equivalent instructions \nin constant time. The hash table is not reset at basic block bounda\u00adries. Indeed, basic blocks play no \npart in this phase. We use a IWO walk because, in most cases, the in\u00adputs to an instruction will have \nall been value-numbered before value-numbering the instruction itself. The ex. ceptions arise because \nof loops. Because we do not reach a fixed point, running GVN a second time may be profit\u00adable. In our \nexperience, one pass is usually sufficient. Start: i. := O a := reado I a Start: c1:= 1 ADD b I@i b:=a+cl \nClo:= 10 1 /+ v i / PHI i] loop: \\ loop: il :=$(io, iz) iz ADD i2:=i1+b cc := iz< Clo > cc branch \nne loop BRANCH 153 TI1 JI I C 2:= 2 c:=i2xc2 return c Figure 7 Our loop example, 3.1 Value-Numbering \nValue-numbering attempts to assign the same value number to equivalent instructions. The bottom-up hash\u00adtable \nbased approach is amenable to a number of exten\u00adsions, which are commonly implemented in local value\u00adnumber \nalgorithms. Among these are constant folding and recognizing algebraic identities. When we value\u00adnumber \nan instruction, we perform the following tasks: 1) Determine if the instruction computes a constant result. \nIf it does, replace the instruction with one that directly computes the constant. Instructions that compute \nconstants are value-numbered by the hash table below, so all instructions generating the same constant \nwill even\u00adtually fold into a single instruction. Instructions can generate constants for two reasons: \neither all their inputs are constant, or one of their inputs is a special constant. Multiplication by \nzero is such a case. There are many others and they all interact in useful ways.g 2) Determine if the \ninstruction is an algebraic iden\u00adtity on one of its inputs. Again, these arise for several reasons; either \nthe instruction is a trivial CoPY, or one a By treating basic blocks as a special kind of instmction, \nwe are able to constant fold basic blocks like other instructions. Constant folding a basic block means \nremovmg constant tests and unreachable control flow edges, which m turn simplifies the dependent o-functions \nafter scheduling late input is a special constant (add of zero) or both inputs are exactly the same (the \nt) or MAX of equal value;). We replace uses of such instructions with uses of the copied value. The current \ninstruction is then dead and can be removed. 3) Determine if the instruction computes the same value \nnumber as some previous instruction. We do this by hashing the instruction s operation and inputs (which \ntogether uniquely determine the value produced by the instruction) and looking in a hash table. If we \nfind a hit, then we replace uses of this instruction with uses of the previous instruction. Again, we \nremove the redundant instruction. If the hash table lookup misses, then the instruction computes a new \nvalue. We insert it in the table. Commutative instructions use a hashing (and compare) function that \nignores the order of inputs. While programmers occasionally write redundant code, more often this arises \nout of a compiler s fi-ont end during the conversion of array addressing expressions into machine instructions. \n3.2 Example We present an extended example in Figure 8, which we copied from the Rosen, Wegman and Zadeck \nexample [19]. The original program fragment is on the left and the SSA form is on the right. We assume \nevery original read(ao, bo,co,do,lo,mo,so,to) I al :=$( ao, a3) dl :=$( do, d3) ml := O( mo, m3) s, := \n@( so, S3) 1I 11:= coX b. dz := CO l:=cxb d:=c m2:=ll+4 12 := d2 X b. m:=l+4 l:=dxb a2 := co S2:= al \nX b. a;=c s:=axb tz :=s2+1 t:=s i-1 L a3 := $( a2, al) x:=axb d3 := @( dl, d2) y:=x+l 13 := ~ 1,, 12) \n I m3 := $( mz, ml) S3 := @( .$1, SJ ts:= ~ tl, tJ X. :=a3x b. yo:=xo+l Figure 8 A larger example, also \nin SSA form variable is live on exit from the fragment. For space rea-shown on the right. Coming out \nof SSA form requires a sons we do not show the program in graph form. new variable name as shown on the \nright in Figure 9. We present the highlights of running GVN here. We visit the instructions in reverse \npost order. The 4. Experiments reado call obviously produces a new value number. The ~-functions in the \nloop header block (al, d,, rm, SI, and We converted a large test suite into a low-level in\u00adtl) are not \nany algebraic identities and they do not conn-termediate language, lLOC [6, 5]. The ILOC produced by \npute constants. They all produce new value numbers, as translation is very naive and is intended to be \noptimized. does 11 := co x bo and mz := 11 + 4 . Next az is an We then performed several machine-independent \noptimi\u00adidentity on co. We follow the clef-use chains to remap zation at the ILOC level. We ran the resulting \nlLOC on a uses of a2 into uses of CO. simulator to collect execution cycle counts for the lLOC virtual \nmachine. All applications ran to completion onRunning along the other side of the conditional we the \nsimulator and produced correct results. discover that we can replace d2 with co. Then 12 := co x bo has \nthe same value number as 11and we replace uses ILOC is a virtual assembly language. The current of 12with \n11. The variables S2and t2produce new values. simulator defines a machine with an infinite register set \nIn the merge after the conditional we find 13 := ~(11, lJ , and 1 cycle latencies for all operations, \nincluding Lo~s, which is an identity on 11. We replace uses of 13occur-STORESand JUMPS. A single register \ncan not be used for ring after this code fragment with 11. The other $-both integer and floating-point \noperations (conversion functions all produce new values. We show the resulting operators exist). ILOC \nis based on a load-store archi\u00adprogram in Figure 9. At this point the program is incor-tecture; there \nare no memory operations other than LOAD rect; we require a round of GCM to move 1, := co x bo and STORE. \nILOC includes the usual assortment of logi\u00ad to a point that dominates its uses. cal, arithmetic and floating-point \noperations. The simulator is implemented by translating ILOC out of the loop. The final code, still in \nSSA form, is into C. The C code is then compiled and linked as a na-GCNl moves the computations of 11, \nmz, XO, and ,YO read(ao, bo,Co,do,lo,rno,,so,to) I read(ao, bo, co, do, l., mo, so,to) ~1 := Co X b. \nread(a, b, c, d, 1,m,s, t) l:=cxb +* m2;= 11+4 m2:=l+4 al :=$( ao, a3) v v v v d, := ()( do, dJ al :=+( \nao, as) ml := $( mo, ms) dl := ()( do, d3) J-1:=0( ~o, S3) ml := $( mo, ms) t, := $( to, f3) S1 := Q( \nso, S3) 1, := co x b~ m2 :=11+4 I S2:=al X bo tz:=sz+l t] := O( to,t3) S2 := al x b. a:=c m := m2 s:=axb \nt:=s+l tz:=sz+l d:=c a3 := $( co, al) d3 := 1$( dl, co) a3 := $( co, al) ms := 4( m2., ml) d3 := $( dl, \nCo) s,:= +( s,, s,) m3 := $( mz, ml) t~ := $( t~, tJ S3 := ($( s], SJ Xo := a3 x b. tq := ($( tl, tJ \nylj:=xo+l m 7x:=axb y:=x+l 1,, . ... Figure 9 After GVN on the left, after GCM in the middle, back out \nof SSA form on the right tive program. The executed code contains annotations internally with a naive \ntranslation out of SSA at the end. that collect statistics on the number of times each ILOC Thus, most \noptimization insert a large number of copies, routine is called and the dynamic cycle count. We followed \nwith a round of coalescing to remove the copies [4, 9]. This empties some basic blocks that only The \nsimulated ILOC clearly represents an unrealistic hold copies. We also split control-dependent blocks \nand machine model. We argue that while the simulation is some of those blocks go unused. We ran a pass \nto re\u00ad overly simplistic, it makes measuring the separate effects move the empty blocks. Besides the \noptimization al\u00ad of optimizations possible. We are looking at machine\u00adready discussed, we also used the \nfollowing: independent optimization, Were we to include memory hierarchy effects, with long and variable \nlatencies to CCP: Conditional Constant Propagation is an im\u00admemory and a fixed number of functional units \nwe would plementation of Wegman and Zadeck s algorithm [24]. obscure the effects we are trying to measure. \nIt simultaneously removes unreachable code and replaces computations of constants with load of constants. \nAfter 4.1 The Experiments constant propagation, CCP folds algebraic identities that use constants, such \nas x x 1 and x + O, into simple copy The test suite consists of a dozen applications with a operations. \n total of 52 procedures. All the applications are written in FORTRAN, except for cplex. Cplex is a large \nconstraint GCF: Global Congruence Finding implements Alp\u00adsolver written in C. Doduc, tomcatv, matrix300 \nand ern, Wegman, and Zadeck s global congruence finding fpppp are from the Spec89 benchmark suite.9 The \nre-algorithm [2]. Instead of using dominance information, maining procedures come from the Forsythe, \nMalcom available expressions (AVAIL) information is used to and Moler suite of routines [14]. remove \nredundant computations. AVAIL is stronger than dominance, so more computations are removed [20]. All \nexperiments start by shaping the CFG: splitting control-dependent edges and inserting loop landing pads. \nPRE: Partial Redundancy Elimination implements All experiments end with a round of Dead Code Elimi-Morel \nand Renvoise s algorithm. PRE is discussed in nation (DCE) [12]. Most of the phases use SSA form Section \n1.1. 9At the time, our tools were not up to handling the entue Spec suite, Reassociation: This phase \nreassociates and redis\u00adtributes integer expressions allowing other optirnizations to find more loop invariant \ncode [6]. This is important for addressing expressions inside loops, where the rapidly varying innermost \nindex may cause portions of a con~\u00adplex address to be recomputed on each iteration. Reas\u00adsociation can \nallow all the invariant parts to be hoisted, leaving behind only an expression of the form base+indexxstride \nin the loop. Our experiments all follow this general strategy: 1. Convert from FORTRAN or C to naive \nILOC. Re\u00adshape the CFG. 2. Optionally reassociate expressions. 3. Run either GVN-GCM or run GCF-PRE-CCP \nonce or twice, 4. Clean up with DCE, coalescing and removing empty blocks. 5. Translate the ILOC to \nC with the simulator, then compile and link.  6. Run the compiled application, collecting statistics. \nVariations on this Strategy-are outlined below in each experiment. 4.2 GVN-GCM VS. GCF-PRE-CCP We optimized \nwith the GVN-GCM combination and compared it to running one pass each of GCF, PRE and CCP. This represents \nan aggressive optimization strat\u00adegy with reasonable compile times. Each of GCF, PRE and CCP can find \nsome congruences or constants that GVN cannot. However, constants discovered with GVN can help find congruences \nand vice-versa. The separate passes have a phase-ordering problem; constants found with CCP cannot then \nbe used to find congruences with PRE. In addition, GCM is more aggressive than PRE in hoisting loop invariant \nexpressions. The table in Figure 10 shows the results. The first two columns are the application and \nprocedure name, the next column shows the runtime cycles for GVN-GCM. The fourth column is cycles for \nGCF-PRE-CCP. Per\u00adcentage speedup is in the fifth column. 4.3 GVN-GCM vs. GCF-PRE-CCP repeated We compared \nthe GVN, GCM combination against two passes of GCF, PRE and CCP. We ran DCE after each set; the final \nsequence being GCF, PRE, CCP, DCE, GCF, PRE and CCP. As before we finish up with a pass of DCE, coalescing \nand removing empty blocks. The sixth column in Figure 10 shows the runtime cycles for the repeated passes \nand column seven shows the percentage speedup over GVN-GCM. Roughly halve the gain over a single pass \nof GCF-PRE-CCP is lost, This indicates that the second pass of GCF and PRE could make use of constants \nfound and code removed by CCP, 4.4 Reassociate, then GVN-GCM vs. GCF-PRE -CCP We first reassociate expressions. \nThen we ran the GVN-GCM combination and compared it to running one pass of GCF, PRE and CCP. As before \nwe finish up with a pass of DCE, coalescing and removing empty blocks. In Figure 10, column eighth is \nthe runtime cycle count for reassociating, then running the GVN-GCM combination. Column nine is reassociation, \nthen GCF-PRE-CCP. Column ten is the percentage speedup. The average speedup is 5.970 instead of 4.370. \nReassociation provides more opportunities for GVN-GCM than it does for one pass of GCF, PRE and CCP. \n4.5 Reassociate, then GVN-GCM vs. GCF-PRE -CCP repeated We first reassociate expressions. Then we ran \nthe GVN-GCM combination and compared it to running two passes of GCF, PRE and CCP. The final sequence \nbeing Reassociate, GCF, PRE, CCP, DCE, GCF, PRE and CCP. As before we finish up with a pass of DCE, coa\u00adlescing \nand removing empty blocks, Column eleven in Figure 10 shows the runtime cy\u00adcles for the repeated passes \nand column twelve shows the percentage speedup over GVN-GCM. Again, roughly halve the gain over a single \npass of GCF-PRE-CCP is lost. This indicates that the second pass of GCF and PRE could make use of constants \nfound and code removed by CCP. 5. Conclusions We have implemented two fast and relatively straightforward \noptimization. We ran them both on a suite of programs with good results. By separating the code motion \nfrom the optimization issues we are able to write a particularly simple expression of Global Value Numbering. \nBesides finding redundant expressions, GVN folds constants and algebraic identities. After running GVN \nwe are required to do Global Code A40\u00adtion. Besides correcting the scheduled produced by GVN, GCM also \nmoves code out of loops and down into nested conditional branches. GCM requires the dominator tree and \nloop nesting depth, It then makes two linear-time passes over the program to select basic blocks. Running \ntime is essen\u00adtially linear in the program size, and is quite fast in practice. GVN requires one linear-time \npass over the program and is also quite fast in practice. Together these optimization provide a net gain \nover using GCF, PRE and CCP, and are very simple to implement. Reassociate Reassociate Reassociate Application \n;VN-GCM GCF-PRE-CCP GCF-PRE-CCP GVN-GCM GCF-PRE-CCP GCF-PRE-CCP Routim Once speedup Repeated speedup \nOnce speedup Repeated speedup 689,059 898,775 23.3% 821,399 16.1% 689,931 929,271 25.8% 8~4, J89 163% \ndoduc parol 536,685 637,325 15.8% 560,180 4,2% 513,005 689,495 25.6% 559,255 8.3% rkf45 rkfs 56,024 66,254 \n15.4% 66,254 15.47. 58,945 61,654 4.4% 62,526 5.7% fpppp gamgen 134,227 158,644 15.4% 151,039 11,1% 79,057 \n102,630 23.070 83,032 4.8% doduc prophy 553,522 646,101 14 3% 590,164 6.2% 617,902 834,681 26.0% 648,916 \n4.8% doduc pastern 625,336 721,850 13.4% 654,365 4 d!io 598,399 769,328 22.2% 651,012 8.1% doduc ddeflu \n1,329,989 1,485,024 10,4% 1,434,149 7,3% 1,332,394 1,541,800 13,6% 1,474,460 96% doduc yeh 342,460 379,567 \n9 8% 379,567 98% 342,460 362,090 54% 354,328 3.3% doduc deblco 479,318 529,876 9.59 . 497,174 3.6% 440,884 \n552,856 20.3% 508>216 13.2% doduc deseco 2,182,909 2,395,579 8.9% 2,249,694 3.070 2,175,324 2,671,044 \n18.6% 2,324,804 6.4% doduc mtegr 389,004 426,138 8.7% 383,403 -1 5% 401,214 487,928 17.8% 391,913 -2,4% \ndoduc cardeb 168,165 183,520 8.4% 157,620 -6.7% 159,840 169,090 5.5% 151,330 -5 6% doduc coeray 1,487,040 \n1,622,160 8 3% 1,622,160 8 3% 1,487,040 1,606,384 7.4% 1,606,384 7.470 !doduc bilan 536,432 580,878 7.7% \n545,127 1.6% 529,957 686,143 22,8% 582,127 9 o% matrix300 sgemv 496,000 536,800 7,6% 536,400 7.5% 400,000 \n401,200 0 3% 400,400 0 1% rkf45 rkf45 1,475 1,575 6.3% 1,575 6.3% 1,475 1,475 0.090 1,475 0.0% doduc \ninitbx 2,602 2,776 6.3% 2,606 0.2% 2,558 3>091 17 2% 2,591 1 3% doduc dccera 921,068 981,505 6.2% 981,505 \n6.2% 921,068 975,215 5.6% 975>215 5.6% fpppp twldrv ?6,291,270 81,100,769 5.9% 80,988,348 5.8% 79,039,949 \n81,989,205 3,6% 81,464,531 3.0% sewd sphrte 882 937 5 9% 937 5.9% 801 837 4,3% 829 34% doduc heat 578,646 \n613,800 57% 613,800 5.7% 560,418 610,080 8.l% 608,964 8.0% doduc orgpar 23,69 2 24,985 5.2% 24,060 1 \n5% 21,287 22,950 7.2% 21,840 2.5% doduc drepvi 680,080 715,410 4.9% 689,325 1.3% 697,840 741,055 5.8% \n720,890 3.2% fpppp fmtgen 926,059 973>331 4 9% 966,891 42% 923,419 960,776 3,9% 952,581 3.1% doduc repvid \n486,141 507,531 42% 492,837 1,4% 453,033 514,041 119% 496,557 8.8% doduc uudeb 863 898 3.9% 838 -3.0% \n735 877 16.2% 844 129% urand urnnd 550 563 2.3% 563 2 3% 555 564 1.6% 564 1,6% doduc drrgl 295,501 301,516 \n2.0% 298,926 1.1% 297,334 347,379 14.4% 347,379 14.4% cplex xload 3,831,744 3,899,518 1,7% 3,899,518 \n1,7% 3,831,744 3,899,518 1 7% 3,899,518 1,7% doduc colbur 751,074 763,437 1 6% 765,868 1 9% 761,478 804,580 \n5.4% 797,365 4.5% matrix300 sgemrtr 8,664 8,767 1.2% 8,666 0 o% 7,928 8,022 1.2% 7,920 -o 1% rkf45 fehl \n134,640 135,960 1.0% 134,640 0.0% 130,416 137,808 5.4% 137,808 5.4% cplex xaddrov ,6,345,264 16,487,352 \n0.9% 16,499,836 0.9% 15,539,173 16,487,352 5 8% 16,499,836 5.8% doduc si 9,924,360 9,981,072 0,6% 9,981,072 \n0 6% 9,300,548 9,470,684 1 8% 9.470,684 1 8% cplex xielem 9,738,262 19,791,979 0.3% 19,904,514 0.8% 19,755,998 \n19,718,353 -0.2% 19,956,683 1.0% fpppp fmtset 1,072 1,074 0.2% 1,074 0 2% 948 952 0.4% 952 0.4% doduc \nSupp 2,564,382 2,567,544 0.1% 2,567,544 0.1% 2,564,382 2,561,220 -o 1% 2,567,544 0 1% doduc iniset 56,649 \n56,684 0.1% 56,655 0,0% 47,316 47,498 0.4% 47,469 0 3% fpppp fpppp !6,871,102 26,883,090 0 o% 26,877,096 \n0.0% 26,871,102 26,871,102 0.0% 26,865,108 0.09 0 matrkx300 Saxpy 3,340,000 13,340,000 0.0% 13,340,000 \n0 o% 10,480,000 10,480,000 0 o% 10,480,000 0 o% doduc subb 1,763,280 1,763,280 0.0% 1.763,280 0.0% 1,763,280 \n1,763,280 0 o% 1,763,280 0 o% doduc x21y21 l,~53,980 1,253,980 0.0% 1,355,263 7.5% 1,162,343 1,162,343 \n0.0% 1,162,343 0.0% solve decomp 642 641 -0.2% 631 -1 7% 655 651 -0,6% 60 I -9 o% svd svd 4,596 4,542 \n-1.2% 4,547 -1.1% 4,612 4,407 -4.7% 4,135 -11.5 % cplex chpwot 4,156,117 4,097,458 -1,4% 4,097,548 -1.4% \n4,194,263 4,137,514 -1 4% 4,137,604 -1 4% zeroin zeroin 740 729 -1.5% 729 -1.5% 909 731 -24.4% 738 -23 \n2% tomcatv tomcatv ?485E+08 2,436E+08 -2 o% 2433E+08 -2 1% 1 952E+08 1,957E+08 0.3% 1,952E+08 0.0$%0 \nfmin fmm 908 876 -3.7% 902 -0.7% 1,017 885 -14 9% 901 -12 9% fpppp efill 2,046,512 1,951,843 -4 9% 1,898,546 \n-7.8% 2,791,920 2,305,816 -21,1% 2,471,638 -13.0% doduc lhbtr 75,708 71,616 -5 7% 77,010 1.7 % 71,430 \n84,266 15 2% 79,9!38 10 7% doduc saturr 63,426 59,334 -6.9% 58,962 -7 6% 63,426 60,078 -5 6% 59,706 -6.Z?ZO \nTOTAL 1.416E+08 4.432E+08 0.4% 4.423E+08 o 2% 3.873E+08 3,938E+08 1.6% 3.918E+08 1.1% Average 8,659,662 \n8,690,153 4 3% 8,672,843 24% 7,593,844 7,721.122 5,9% 7,681,987 2,2% Figure 10 GVN-GCM vs. GCF-PRE-CCP \n[1] [2] [3] [4] [5] [6] [7] [8] [9] [10] [11] [12] Bibliography A. Aho, R, Sethi, and J. Unman. Compilers \n Principles, Techniques, and Tools. Addison-Wesley, 1985. - B. Alpern, M. N. Wegman, and F. K. Zadeck. \nDetecting equality of variables in programs. In Conference Record of the Fifteenth ACM Sympo\u00adsium on \nthe Principles of Programming Lan\u00adguages, 1988. R. A. Ballance, A. B. Maccabe, and K. J. Otten\u00adstein. \nThe program dependence web: A represen\u00adtation supporting control-, data-and demand\u00addriven interpretation \nof imperative languages. In Proceedings of the SIGPLAN 90 Conference on Programming Languages Design \nand Implementa\u00adtion, June 1990. P. Briggs, Register Allocation via Graph Coloring. Ph.D. thesis, Rice \nUniversity, 1992. P. Briggs. The Massively Scalar Compiler Project. Unpublished report. Preliminary \nversion available via ftp://cs.rice.edu/public/prestonloptirnizer/ shared.ps. Rice University, July 1994. \n P. Briggs and K. Cooper. Effective partial redun\u00addancy elimination, In Proceedings of the SIGPLAN 94 \nConference on Programming Lan\u00adguages Design and Implementation, June 1994. P. Briggs and T. Harvey. Iloc \n93. Technical re\u00adport CRPC-TR93323, Rice University, 1993. R. Cartwright and M. Felleisen, The semantics \nof program dependence, In Proceedings of the SIGPLAN 89 Conference on Programming Lan\u00adguages Design and \nImplementation, June 1989. G. J. Chaitin. Register allocation and spilling via graph coloring, In Proceedings \nof the SIGPLAN 82 Symposium on Compiler Construction, June 1982. C. Click, Combining Analyses, Combining \nOptimi\u00adzatiorzs. Ph.D. thesis, Rice University, 1995. J. Coeke and J. T. Schwartz. Programming lan\u00adguages \nand their compilers. Courant Institute of Mathematical Sciences, New York University, April 1970. R. \nCytron, J. Ferrante, B. K. Rosen, M. N. Weg\u00adman, and F. K, Zadeck, An efficient method of computing static \nsingle assignment form. In Con\u00ad ference Record of the Sixteenth ACM Symposium on the Principles of Programming \nLunguages, Jan. 1989. [13] K. H. Drechsler and M. P. Stadel. A solution to a problem with Morel and \nRenvoise s Global opti\u00admization by suppression of partial redundancies . ACM Transactions on Programming \nLanguages and Systems, 10(4):635 640, Oct. 1988. [14] G. E. Forstyhe, M. A. Malcom, and C. B. Moler. \nComputer Methods for Mathematical Computa\u00adtions. Prentice-Hall, Englewood Cliffs, New Jer\u00adsey, 1977. \n[15] J. Ferrante, K. J. Ottenstein, and J. D. Warren. The program dependence graph and its use in op\u00adtimization. \nACM Transactions on Programming Languages and Systems, 9(3):3 19-349, July, 1987. [16] J. Knoop, O. Riithing, \nand B. Steffen. Partial dead code elimination. In Proceedings of the SIGPLAN 94 Conference on Programming \nLanguages De\u00adsign and Implementation, June 1994. [17] T. Lengauer and R. E. Tarjan. A fast algorithm \nfor finding dominators in a flowgraph. ACM Transac\u00adtions on Programming Languages and Systems, 1(1):121-141, \nJuly, 1979. [18] E. Morel and C. Renvoise. Global optimization by suppression of partial redundancies. \nCommunica\u00adtions of the ACM, 22(2):96-103, Feb. 1979. [19] B. K. Rosen., M. N. Wegman, and F. K. Zadeck, \nGlobal Value Numbers and Redundant Computa\u00adtions. In Conference Record of the Fifteenth A CM Symposium \non the Principles of Programtning Languages, Jan. 1988. [20] T. Simpson, Global Value Numbering. Unpub\u00adlished \nreport. Available from ftp://cs,rice.edu/ public/preston/optimizer/gval.ps. Rice University, 1994. [21] \nR. E. Tarjan. Testing flow graph reducibility. Journal of Computer and System Sciences, 9:355\u00ad365, 1974. \n[22] C. Vick, SSA-Based Reduction of Operator Strength. Masters thesis, Rice University, pages 11-15, \n1994. [23] D. Weise, R. Crew, M. Ernst, and B. Steensgaard, Value dependence graphs: Representation without \ntaxation. In Proceedings of the 21st ACM SIGPLAN Symposium on the Principles of Pro\u00adgramming Languages, \n1994. [24] M. N. Wegman and F. K. Zadeck. Constant propagation with conditional branches. ACM Transactions \non Programming Languages and Systems, 13(2): 181-210, April 1991.  \n\t\t\t", "proc_id": "207110", "abstract": "", "authors": [{"name": "Cliff Click", "author_profile_id": "81100637282", "affiliation": "Hewlett-Packard Laboratories, Cambridge Research Office, One Main St., Cambridge, MA", "person_id": "PP39051884", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/207110.207154", "year": "1995", "article_id": "207154", "conference": "PLDI", "title": "Global code motion/global value numbering", "url": "http://dl.acm.org/citation.cfm?id=207154"}