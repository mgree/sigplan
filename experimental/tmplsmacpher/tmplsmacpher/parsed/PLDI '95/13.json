{"article_publication_date": "06-01-1995", "fulltext": "\n Balanced Scheduling with Compiler Optimizations that Increase Instruction-Level Parallelism Jack L, \nLO and Susan J. Eggers Department of Computer Science and Engineering University of Washington { -j 10, \neggers} @cs . washi-rlgton. edu  Abstract Traditional list schedulers order instructions based on an \noptimistic estimate of the load latency imposed by the hardware and therefore cannot respond to variations \nin memory latency caused by cache hits and misses on non-blocking architectures. In contrast, bal\u00adanced \nscheduling schedules instructions based on an estimate of the amount of instruction-level parallelism \nin the program, By scheduling independent instructions behind loads based on what the program can provide, \nrather than what the implementation stip\u00adulates in the best case (i.e., a cache hit), balanced scheduling \ncan hide variations in memory latencies more effectively. Since its success depends on the amount of \ninstruction-level paral\u00adlelism in the code, balanced scheduling should perform even better when more \nparallelism is available. In this study, wecombinebal\u00adanced scheduling with three compiler optimization \nthat increase instruction-level parallelism: loop unrolling, trace scheduling and cache locality analysis. \nUsing code generated forthe DEC Alpha by the Multiflow compiler, we simulated a non-blocking processor \narchitecture that closely models the Alpha 21164. Our results show that balanced scheduling benefits \nfrom all three optimizations, pro\u00adducingaverage speedups tbat range from 1.15to 1.40, across the optimizations. \nMore importantly, because of its ability to tolerate variations in load interlocks, it improves its advantage \nover tradi\u00adtional scheduling, Without theoptimizations, balanced scheduled code is, on average, l.05times \nfaster than that generated byatra\u00additional scheduler; with them, its lead increases to 1.18. 1 Introduction \nTraditional instruction schedulers order load instructions based on an optimistic assumption that all \nloads will be cache hits. On most machines, this optimistic estimate is accurate, because theproces\u00ad \nsors block, i.e., stall the pipeline, on cache misses. Blocking pro\u00ad cessors simplify thedesign of thecode \nscheduler, by enabling all load instructions to be handled identically, whether they hit or miss in the \ncache. The traditional blocking processor model has recently been chal\u00ad lenged by processors that do \nnot block on loads [ER94] [McL93] [Asp93] [DA92] [Gwe94a] [Gwe94b] [Mip94] [CS95]. Rather than stalhng \nuntil a cache miss is satisfied, they use lockup-free caches [Kro81] [FJ94] to continue executing mstructlons \nto hide the latency of outstanding memory requests. On these non-block\u00ad ing architectures, instruction \nlatency is exposed to the compiler permission to copy without fee all or part of this material is flrantec \n,-, .. ~.-..... ~ provided that the copies are not made or distributed for direct commercial advantage, \nthe ACM copyright notice and the and notice is given titie of the publication and its date aPPear . \nthat copying is by permission of the Association of Computing Machinerv.To.. . . .... ., corw otherwise, \nor to republish, requires a fee and/or spe~fic permission. SIGPLAN 95La Jolla, CA USA 0 1995 ACM 0-89791 \n-697-2/95/0006 ...$3.50 and becomes uncertain: not only will the processor see both cache hits and misses, \nbut each level in the memory hierarchy will also introduce a new set of Iatencies. Instead of handling \nall loads iden\u00adtically, a non-blocking processor s code scheduler could arrange instructions behind loads \nmore intelligently to produce more effi\u00adcient code schedules. Unfortunately, a traditional instruction \nscheduler fails to exploit thk opportunity because of its optimistic and architecture-based estimates; \nits resulting schedules may be therefore intolerant of variations in load latency. Balanced scheduling \n[KE93] is an algorithm that can generate schedules that adapt more readily to the uncertainties in memory \nlatency. Rather than being determined by a predefine, architecttrr\u00adally-based value, load latency estimates \nare based on the number of independent instructions that are available to hide the latency of a particular \nload. Using these estimates as load weights, the baf \u00adanced scheduler then schedules instructions normally, \nPrevious work has demonstrated that balanced schedules show speedups averaging 8% for several Perfect \nClub benchmarks for two differ\u00adent cache hitimiss ratios, assuming a workstation-like memory model in \nwhich cache misses are normally distributed [KE93]. Since its success depends on the amount of instruction-level \nparal\u00adlelism in a program, balanced scheduling should perform better when more parallelism is available. \nIn this study, we combine bal\u00adanced scheduling with three compiler optimizations that increase instruction-level \nparallelism. Two of them, loop unrolling and trace scheduling, do so by giving the scheduler a larger \nwindow of instructions from which to select. The third, locality analysis, more effectively utilizes \nthe amount of instruction-level parallelism that is currently available. Loop unrolling generates more \ninstruction-level parallelism by duplicating loop iterations a number of times equal to the unrolling \nfactor. The technique increases basic block size by eliminating branch overhead instructions for all \niterations but the last. Conse\u00adquently, more instructions are available to hide load Iatencies and more \nflexibility is offered to the scheduler. Trace scheduling increases the amount of instruction-level parallelism \nby permitting instruction scheduling beyond basic blocks [FERN84] [LFK+93]. It does this by assigning \nexpected execution frequencies to edges of the control flow graph, and optimizing the schedules along \nthe execution paths with the highest frequencies. Basic blocks along these paths (or traces) are grouped \ntogether, and instructions are scheduled, sometimes speculatively, as though the trace were a sin\u00adgle \nbasic block. In order to preserve data dependencies and cor\u00adrectness, some code motion is restricted, \nor may only occur if additional compensation code is added. The third optimization, locality analysis, \nidentifies potential temporal and spatial reuse in array accesses within a loop and transforms the code \nto exploit it[MLG92]. When used in conjunction with balanced scheduling, it enables load instructions \nto be selectively balanced scheduled. If the compiler can determine that a load instruction is a hit, \nthen the optimistic latency estimate is correct, and should be used, This frees more instructions to \nhide the latencies of other loads (com\u00adpiler-determined cache misses, as well as loads with no reuse \ninformation), which are balanced scheduled. The original work on balanced scheduling [KE93], which com\u00adpared \nit to the traditional approach and without ILP-enhancing optimizations, relied on a stochastic model \nto simulate cache behavior and network congestion. It included load Iatencies for a singIe-level cache \nhierarchy, but assumed single-cycle execution for all other multi-cycle instructions, and modeled an \ninstruction cache that always hit and an ideal, pipelined floating point umt. By examining the effects \nof balanced scheduling independent of the particular latencies of other multi-cycle instructions, the \nsimple model allowed us to understand the ramdications of changing just one aspect of code scheduling. \nHowever, executions on a realistlc processor and memory architec\u00adture are needed to see how well balanced \nscheduling will perform in practice. Real machines have longer load latencies (i .e,, more cycles), both \nbecause of their more complete memory system [a cache hierarchy and a TLB) and their faster processor \ncycle time, relative to the memory system; they also execute instructions that require multiple, fixed \nlatencies. The former should help balanced scheduling relative to traditional scheduling, as long as \nthere 1s sufficient instruction-level parallelism in the code, because the bal\u00adanced technique is more \nhkely to hide the additional load latencies. However, the latter, should dilute its benefits. To study \nthe effects of ILP-enhancing optimization on balanced scheduling and to evaluate the impact of processor \nand memory features that affect its ability to hide load latencies, we incorpo\u00adrated balanced scheduling \nand locality analysis into the Multiflow compiler (which already does loop unrolhng and trace scheduling). \nUsing code generated for the DEC Alpha and a simulator that models the AXP 21164, we simulated the effects \nof all three opti\u00admization on both balanced and traditional scheduling, using Per\u00adfect Club [BCK+89] \nand SPEC92 [Dix92] benchmarks. Our results confirm the hypothesis. Balanced scheduling interacts well \nwith all three optimization, producing average speedups that range from 1.15 to 1.40. More importantly, \nits performance advan\u00adtage relative to traditional scheduling increases when combined with the optimizations. \nWithout ILP-enhancing optlmizations, bal\u00adanced scheduled programs execute 1.05 times faster than those \nscheduled traditional y; with them, its performance advantage rises as high as 1.18. The rest of this \npaper provides more detail and analysis to support these results. Section 2 briefly reviews the balanced \nscheduling algorithm and contrasts it to traditional mstructlon scheduhng. Section 3 contains a discussion \nof the three compder optlmtza\u00adtlons, in more detail than was presented above, and with examples for both \ntrace scheduling and locality analysis. Section 4 describes the methodology and experimental framework \nfor the studies. In section 5, we present the results from applying all three compiler optimization and \nanalyze the factors that account for the perfor\u00admance improvements. Finally, in section 6, we conclude. \n Balanced Scheduling Although balanced scheduling is anew algorithm forcalculatmg load instruction weights, \nit fits cleanly within the framework of a traditional list scheduler. This section describes the behavior \nof a traditional scheduler, and then explains how balanced scheduling differs. Instruction scheduling \nseeks to minimize pipeline stalls due to data and control hazards. Given the data and control dependence \npresent in the instruction stream and theestlmates formstructlon latencies, the scheduler statically \ndetermines an instruction order that avoids as many interlocks as possible. The list scheduling approach \n[HGS3] [GM86] [Sit78] to this minimization problem relies on a code DAG, instruction Iatencies, and heuristics \nfor instruction selection, Thecode DAGrepresents theinstructlons to be scheduled and the data dependence \nbetween them. Instruction latencies (also known as instruction weights) correspond to the number of cycles \nrequired before the results of the instructions become available. In traditional list scheduling, these \nlatencies have architecturally-fixed and optimistic values. (For example, as mentioned above, the weight \nof load instructions is the time of a cache hit.) Given the code DAG and the instruction weights, the \nscheduler traverses the DAG and generates an ordering for the instructions by applying the set of heuristics \nfor instruction selec\u00adtion, many of which are based on the instruction weights. The traditional list \nscheduler can be very effective for an architec\u00adture with a blocking processor. In this model, the weights \naccu\u00adrately reflect the actual Iatencies that a processor must hide. Varlatlons in actual load latencies \n(due to either cache misses in the various levels of the memory hierarchy or congestion in the inter\u00adconnect) \naremasked from the compiler, since the processor stalls after a number of cycles equal to the optimistic, \narchitecturally\u00addefined latency. This is not the case for non-blocking processors. Because the pro\u00adcessor \nis free to continue instruction execution while a load access is in progress, instruction schedulers \nhave an opportunity to hide load Iatencies by carefully selecting a number of instructions to place behind \nloads. Traditional schedulers, with their fixed assumptions about the time to execute loads, are unable \nto take advantage of this opportunity. The balanced scheduler, however, can produce schedules that tolerate \nvariations in memory latency by making no architectural assumptions about load weights. Instead of using \nfixed (and architecturally-optimistic) latency esti\u00admates for all load instructions, the balanced scheduler \nvaries the instruction latency forloads by basing it on the amount ofinstroc\u00adtion-level parallelism available \nin the code. We characterize this measurement as load-level parallelism, and we use it for the weight \nof the load instruction instead of the fixed instruction latency. Working with one basic block at a time, \nthe balanced scheduler calculates thewelght foreach load separately, as afunc\u00adtion of the number of independent \ninstructions that may initiate execution while the load is being serviced and the number of other loads \nthat could hldetheirlatencies with these same instructions. When loads are independent and therefore \ncan be scheduled in sequence without stalling, independent instructions can be used to hide thelatencies \nof them all. Forexample, inthecode DAGof Figure l,loads LOand Ll areindependent ofnon-load instructions \nXl and X2, and the scheduler could issue them in an LO L1 Xl X2 order. However, when loads appear in \nseries, there lS less load\u00adlevel parallelism tohidetheir latencles. Forexample, Xl and X2 can be used \nto hide the latency of either L2 or L3, but not both. The overall effect of the balanced scheduler is \nto schedule the indepen\u00addent instructions behind loads ina balanced fashion, and propor\u00adtlonal to their \nability to hide the latency of particular loads. The balanced scheduling approach has three potential \nadvantages over traditional schedulers. First, it hides more latency for loads that have more load-level \nparallelism available to them. Second, it distributes load-level parallelism across all the loads in \norder to be more tolerant of memory latency variability. Third, since it bases its load weight estimates \non the code rather than the architecture, it produces schedules that are independent of the memory system \nimplementation. Xo placed in the schedule must also be placed above the join, then a copy of that instruction \nin the off-trace path, to ensure that the instruction is executed regardless of the path taken. Code \nmotion /zT%m can also be done speculatively. For example, the scheduler might LO L1 L2 Xl X2 wish to \nmove an instruction in block 4 above the split in block 2.  w/ x3 Figure 1 Initial results [KE93] \nindicated that balanced scheduling produced speedups averaging 8 %0 for selected programs in the Perfect \nClub suite, when simulated on a stochastic model of a microprocessor with 1990 processor and memory latencies, \na bus interconnect and cache hit rates of 80% and 95%, Code for these studies was gener\u00adated using gcc \n[Sta]. 3 The Compiler Optimization Balanced scheduling utilizes load-level parallelism to hide the longer \nload latencies exposed by non-blocking processors. By applying techniques to increase load-level parallelism, \nthe bal\u00adanced scheduler should be able to generate schedules that are even more tolerant of uncertain \nmemory latency, In this study, we ana\u00adlyzed the effect of three such techniques: loop unrolling, trace \nscheduling, and locality analysis. 3.1 Loop Unrolling Loop unrolling increases the size of basic blocks \nby duplicating iterations a number of times equal to the unrolling factor. It con\u00adtributes to increased \nperformance in two ways, First, by creating multiple copies of the loop body, it reduces conditional \nbranch and loop indexing overhead from all but the last copy. Second, the con\u00adsequent increase in the \nsize of the basic block can expose more instruction-level parallelism, thereby providing more opportunities \nfor code scheduling. 3.2 Ikace Scheduling Trace scheduling [FERN84] [LFK+93] enables more aggressive \nscheduling by permitting code motion across basic block bound\u00adaries. Guided by estimated or profiled \nexecution frequencies for each basic block, it creates traces of paths through each procedure. The trace \nscheduler then picks a trace, in order of decreasing execution frequencies, and schedules the basic blocks \nin the trace as if they were a single basic block. Code motion takes into account the effects of scheduling \ninstructions across conditional jumps (splits) and merges (joins), following specific rules for when \ninstructions must be copied or cannot be moved, The final schedule effectively combines into a single \nblock what would have been multiple basic blocks if generated by a traditional scheduler. Figure 2 illustrates \na simple example of trace scheduling. The trace scheduler Identifies basic blocks 1, 2,4, and 5 as a \nsingle trace (trace A), and block 3 forms its own trace (trace B). Assume that trace A is scheduled first \nand therefore is known as the on-trace path (trace B M the off-trace path). Trace A is viewed by the \nscheduler as a single basic block: it uses traditional basic block code motion, as long as data dependence \nare maintained. However, in order to move instructions across splits or joins, compensation code may \nneed to be inserted. For example, if an instruction from block 5 is This operation becomes speculative, \nbecause it is not supposed to be executed in the off-trace path. Rather than adding instruction overhead \nin the off-trace path to undo its effects, speculative motion is restricted to safe operations only, \ni.e., it is not permitted if the instruction writes memory or sets a variable which is live in the off\u00adtrace \npath. Trace A @ o Trace B Figure 2  3.3 Locality Analysis Although memory latency can be uncertain because \nof cache behavior, this variability is not entirely random. For example, many numeric programs have regular \nmemory access patterns within loops. On different loop iterations, an array reference may access two \nelements of an array that reside in the same cache line; the first will miss in the cache, but the second \nwill hit because of spatial locality. If the compiler can determine cache behavior, it can treat cache \nhits and misses differently. Cache hits can be scheduled using the tradi\u00adtional scheduling scheme, because \ntheir actual load latency matches the optimistic, architecture-defined estimate. Then, because we don \nt need to hide the latency of hits, more indepen\u00addent instructions are available for loads that will \nmiss. These loads, as well as other loads for which locality analysis cannot determine cache behavior \n(e.g., those that are not array references within loops), can be scheduled via balanced scheduling. We \nidentify spatial and temporal locality by using the locality anal\u00adysis algorithm that Mowry, Lam and \nGupta developed for selective cache prefetching [MLG92], We apply their algorithm to determine cache \nhit and miss behavior for load instructions in inner loops. When the indices of array accesses are linear \nfunctions of the loop indices, their algorithm can statically determine the relationship between array \nreferences with respect to cache behavior, identify\u00ading both spatial and temporal reuse within cache \nblocks. Assume that the arrays in our examples (Figures 3-5) are laid out in row-major order. An array \nreference has spatial reuse if, on consec\u00adutive iterations of the inner loop, the reference accesses \nmemory locations that map to the same cache line. For example, in the code sequence of Figure 3, reference \nA[i] U] has spatial reuse in the inner loop. In order to exploit this locality, however, the scheduler \nneeds to identify the first reference to the cache line as a cache miss and the others as cache hits. \nThis cannot be done in the loop in Figure 3, where there is exactly one load instruction correspond\u00ading \nto the A[i] U] reference. However, if the inner loop is unrolled, the load in the first unrolled copy \ncan be marked as a cache miss, and the loads in the other copies as cache hits, Figure 3 for (i=O; i \n< n; i++) for (j=O; j < n; j++) C[i][j] = A[i][j]+B[i] [O]; Figure 4 for (i=O; i < n; i++) { for (j=O; \nj < n-(n%4); j+=4) { C[i][j] = A[i][j] + B[i] [O]; C[i][j+ll = A[il[j+ll + B[il[Ol; C[i][j+2] = A[i][j+2] \n+ B[i] [O]; C[i][j+3] = A[i][j+3] + B[i] [O]; } if (j<n) { C[i][j] = A[i][j] + B[i] [O]; j++; if (j<n) \n{ C[i] [j] = A[il [j] + B[i][Ol; j++; if (j<n) { C[i][j] = A[il[jl + B[il[Ol; j++; } } } } Figure 5 for \n(i=O; i < n; i++) { C[i][O] = A[i][O] + B[i] [O]; for (j =1; j <n; j++) C[i][j] = A[i][j] + B[il[Ol \nWe align the arrays on cache-hne boundaries, and assume the 32 byte cache line size of the Alpha 21164 \nfirst-level cache. For arrays with double-word elements, each cache line contains four elements of the \narray. Because of spatial reuse, references to A[i] [0], A[i][4], etc., will be cache misses, while references \nA[i][l], A[i][2], A[1][3], A[i][5], A[i][6], A[i][7], etc., will hit. When the number ofloop iterations \nis unknown statically, we postcondition the loop to ensure that the first unrolled copy still corresponds \nto a cache miss, This means that the code to handle extra loop itera\u00adtions is placed after the code for \nunrolling the loop, as in Figure 4. Note that we cannot simply use another for loop for the extra itera\u00adtions, \nbecause we must be able to mark the load instructions as cache hits or misses. For temporal reuse, a \ncode transformation is also applied. An array reference has temporal reuse when the reference accesses \nthe same location in different iterations of the loop. In Figure 3, array refer\u00adence B[i] [0] has temporal \nreuse in the inner loop, The first iteration of the loop will cause a cache miss, while the others will \nresult in hits. By peeling off the first iteration of the loop, we identify the load instruction for \nB[i][O] in the first iteration as a cache miss. The rest of the loop 1s unchanged, and since all successive \nrefer\u00adences to B[i][O] in the loop will now be cache hits, we do not need to apply balanced scheduling \nto its load. Figure 5 shows the peeled version of the original loop of Figure 3. In Figure 3, the inner \nloop actually exhibits both types of reuse, so the two transformations would be applied in conjunction. \nFirst, the loop would be peeled, and then the remaining iterations would be unrolled.  4 Methodology \nTo examine the interactions between balanced scheduling and the three compiler optimizations, we ran \na set of experiments that measured the effects of individual optimizations on balanced and traditionally \nscheduled code. We also compared the balanced scheduled code with that produced when combining the optimiza\u00ad \ntion with a traditional list scheduler. The experiments simulate program execution of all code on an \narchitectural model that closely resembles the DEC Alpha 21164, The code was generated using the Multiflow \ncompiler. Section 4.1 describes our workload, section 4.2 the compiler changes and section 4,3 the simulator \nand the metrics it produced. Program Lang. Description ARC2D Fortrarr Two-dimensional fluid flow problem \nsolver using Euler equations BDNA Fortran Simulation of hydration structure and dynamics of nucleic acids \n DYFESM Fortran Stnrctural dynamics benchmark to solve dis\u00adplacements and stresses MDG Fortratr IMolecular \ndynamic simulation of flexible water molecules I II Lattice-gauge QCD simulation Two-electron integral \ntransformation Trains a neurat network using back propaga\u00ad ~ dnasa7 II Fortran I Matrix manipulation \nroutines I doduc Fortran Monte Cado simulation of the time evolu\u00adtlon of a nuclear reactor component \n ear c Simulates the propagation of sound in the human cochlea hydro2d Fortrnn Solves hydrodynamical \nNav:er Stokes equa\u00adtions to compute gatactical jets mdljdp2 Fortrau Chemical appbcation program that \nsolves equations of motion for atoms ora Fortran Traces rays through an optical system com\u00adposed of \nspherical and plamu surfaces spice2g6 Fortran Circrut simulation package su2c0r Fortran Computes masses \nof elementary pamcles in the framework of the Quark-Gluon theory swm256 Fortran Solves shallow water \nequations using finite difference equations tomcatv 11 Fortran lVectorizedmeshgenerationprogram I Table \n1: The workload.  4S Workload Our experiments use benchmarks (Table 1) taken from the Perfect Club [BCK+89] \nand SPEC92 [Dix92] suites (the Multiflow com\u00adpiler has both Fortran and C front ends), We chose numeric \npro\u00adgrams because their high percentage of loops make them appropriate testbeds for optimizations targeted \nfor loops. Arrays in the C program are laid out in row-major orde~ the Fortran bench\u00ad marks have a column-major \ndata layout 4.2 The Compiler By modifying the scheduling module (Phase 3) of the MultiHow compiler, \nwe scheduled instructions by using either a traditional algorithm or balanced scheduling. The list scheduler \nfor the bal\u00adanced and traditional schedulers compared in this study is top\u00addown, and selects instructions \nfrom the ready list in a prioritized order. The priority of an instruction is simply the sum of the instruction \ns weight and the maximum priority of its successors. Ties are broken by the following heuristics, listed \nin order of prefer\u00adence. The first heuristic aims to control register pressure by select\u00ading the instruction \nwith the largest difference between consumed and defined registers. The second heuristic selects the \ninstruction that would expose the largest number of successors in the code DAG. The final heuristic selects \nthe instruction that was generated first in the original order of instructions. As another aid in control\u00adling \nregister pressure when applying balanced scheduling, we lim\u00adited load weights to a maximum of 50.1 Our \nversion of the Multiflow compiler generates code for DEC Alpha processors. For the loop unrolling experiments, \nwe selected two unrolling fac\u00adtors, 4 and 8. We disabled loop unrolling when the unrolled block reached \n64 instructions (4) or 128 (8), and did not unroll loops with more than one internal conditional branch.2 \nUsing the methodology established in other trace scheduling per\u00adformance studies [FGL93], we first profiled \nthe programs to deter\u00admine basic block execution frequencies. This information guided the Multiflow compiler \nin picking traces. Since traces do not cross the back edges of loops, we unrolled them, using the same \nunrolling factor as in the loop unrolling studies. To gain maximum flexibility of code motion, we also \npermitted speculative code motion. 1 In our machine model (described in the next section), the maximum \nload latency is 50 cycles, and occurs when a load must be satisfied by main memory. Hence, there is no \nneed to hide more than 50 cycles for any load instruction. Memory System Component Size First level, \non-chip data cache 8KB First level, on-chip instruction cache 8KB Second level, on-chip unified cache \n96KB Thkd-level, off-chip unified cache lMB Main memory TLB, instruction 48 entries TLB, data 64 entries \nWe added a locality analysis module to Multiflow s Phase 2. As in Mowry, Lam and Gupta [MLG92], a predicate3 \nis created for each array reference that exhibits reuse and is associated with the refer\u00adences s load \ninstruction. Predicates are used to determine both the unrolling factor (which depends on the cache block \nand array ele\u00adment sizes)4 and whether loads should be hits or misses. Depen\u00addence arcs were added in \nthe code DAG between each miss load and its corresponding hit loads to prevent the latter from floating \nabove the miss during scheduling. 4.3 Simulation Framework The compiled benchmarks were simulated by \nusing an execution\u00addriven simulator that emulates the D13C Alpha 21164 in most respects. The simulator \nmodels the 21 164 s pipelined functional units, 3-level cache hierarchy, first-level lockup-free cache, \nboth the instruction and data TLBs, branch and computed jump predic\u00adtion and the instruction and memory \nhierarchy latencies. (The lat\u00adter are listed in Tables 2 and 3.) Unlike the 21164, we assume single instruction \nissue, since we would like to understand fully balanced scheduling s ability to exploit load-level parallelism \nbefore applying it to multiple-issue processors that need ILP to execute efficiently, The simulator produces \nmetrics for execution cycles and number of instructions. Cycle metrics measure total cycles, interlock \ncycles for both loads and instructions with fixed latencies, and dynamic instruction execution. Instruction \ncounts are obtained for long and short integers, long and short floating point operations, loads, stores, \nbranches, and spill and restore instructions. All met\u00ad 2 Using the Alpha s conditional move instruction, \nthe Multiflow compiler does predicated execution on simple conditional branches. Our limit on the number \nof conditional branches only affects the conditions within loops that could not be predicated. Loop unrolling \ncan have limited benefit when applied to loops with internal branches, because in the unrolled loop body, \nthe internal branches limit the expansion of basic block size. Unrolling these loops can be more beneficial, \nhowever, when trace scheduling is also applied, because of its ability to schedule beyond basic block \nboundaries. 3 Predicates contain the loop index, loop depth, stride of access and type of locality. 4 \nRecall that locality analysis requires some loop unrolling to differentiate between load references that \nhit or miss. We only unroll those loops that contain arrays that exhibit reuse. Asso- Access ciativ. \nBlock Size Latency Memory ity (bytes) (hit) Update Policy 1 32 2 write-through 1 32 1 n.a. 3 64 8 writeback \n4 64 14 writeback n.a. n.a. 50 n.a --.. -\u00ad.. ---- Table 2: Memory hierarchy parameters. 19% reduction \nin interlock cycles due to fixed latency instructions; and the rest was a 23?Z savings in load interlock \ncycles. The best Instruction type Latency speedups were seen for programs whose frequently executed loops \ncould be unrolled the full extent of the unrolling facto~ when full integer op 1 unrolling did not occur, \nimprovements were smaller. For example, integer multiply 8 frequently executed loops in doduc, mdljdp2, \nspice2g6 and swm256 contained multiple conditionals and consequently were load 2 not unrolled. For these \nbenchmarks, the change in dynamic store 1 instruction count (column 6 in Table 4) was minimal when unrolling \nby 4, reflecting the paucity of unrollable loops. Unrolling FP op (excluding dlwde) 4 also had limited \neffects on the ora benchmark, in which most of the FP div (23 bit fraction) 17 execution time is spent \nin a large, loop-free subroutine. Finally, m FP div (53 bit fraction) 30 the BDNA benchmark, many of \nthe loops were large enough that the iteration instruction limit for unrolling disabled the optimi\u00ad branch \n2 zation. 1 Considering only the other eleven programs for which loop Table 3: Processor latencies. \nunrolling was universally applied, balanced scheduling was able to eliminate on average 3370 of interlock \ncycles, producing average rics are calculated for each optimization, for both scheduling tech\u00adspeedups \nof 1.29. niques. Unrolling by 8 brought additional benefit (an average speedup of  5 Effect of the \nOptimization 1.28 relative to no unrolling), but not as great, and not for all programs. For some programs \nthe more aggressive unrolling could This section details the results of our experiments and provides \nnot be completely appliedz; in most cases, it increased registerexplanations for our findings. We evaluate \neach optimization and pressure, and the independent instructions, now relatively fewer in compare its \neffects on balanced scheduling relative to the tradi\u00adnumber, were less able to hide the latency of the \nadditional spilltional scheduling approach. loads. Consequently, for most programs, load interlock cycles \nand 5.1 Loop Unrolling Loop unrolling with an unrolling factor of 4 improved the perfor-1 These blocks \nwere large enough to exploit load-level parallelism mance of all balanced scheduled programs, by an average \nspeedup without loop unrolling, corroborated by the comparison of of 1.19 (Table 4), About half of the \nbenefit was due to a 1170 traditional and balanced scheduling (later in this section), in decrease in \nthe number of executed instructions (integer and branch which BDNA had one of the largest relative reductions \nin instructions related to branch overhead); a fifth was caused by a interlock cycles. Total Cycles Dynamic \nInstruction Count Load Interlock Cycles (Millions) Speedup (Millions) Percentage Decrease (Millions) \nPercentage Decrease No LOOP No r,00p No Loop Benchmark Unrolting Unroll by 4 Unroll by 8 Unrolling Unroll \nby 4 Unroll by 8 Unrolling Unroll by 4 Unroll by 8 ARC213 17844.8 1.26 1.55 77049 11.4% 147% 321 34.3% \n202% BDNA 43163 1.03 105 3155,1 01% 1.3% 51.6 27.9% 21,1 % DYFESM 10495 1,32 136 776.4 124% 146% 194 \n768% 722% MDG 24346.2 1.10 1.16 15344 7.9% 11.7% 184.8 11.0% -12 1% QCD2 14372 1.14 117 11694 16 8% 185% \n321 343% 20,2% TRFD 3633.1 1,34 I 31 2482.8 17 0 % 136% 1412 55 9% 561% alvinn 8290.8 125 125 5283,8 \n36 6 % 40,3% 707.8 113% 10.2% dnasa7 220299 176 185 85324 225% 25690 16825 57.6% 61 1% doduc 19441 102 \n099 11645 41% 4 o% 481 0 6% 6.4% ear 22289.4 1.13 1..34 15776,9 17,1% 31,29 0 1421 -13,6% -28 9% hydro2d \n12782.3 152 175 74047 20.2% 26.5% 511.8 66.6% 62,5% mdtjdp2 4966.6 101 101 34905 0 4% O6% 133 3 o% 2.8% \nora 8183,9 1,00 1.00 5034,3 0.0% o o% 0.0 .... .... spice2g6 43659.1 101 1.04 24157.2 3,2% 3,970 12862.5 \n1.19 0 0,3% su2c0r 8032.5 110 120 6024.7 49% 94% 2943 62% 27.4% swm256 20470.8 1,00 I 44 13611 I o 4% \n109% 35000 1.9% 83 O% tomcatv 1896.1 1,27 1,26 1232,8 10.2% 11.3% 264,7 21,3% 3,9% AVERAGE 1.19 1.28 \n10.9% 14.0% 23.3 % 26.1 % Table 4: Balanced scheduling: Speedup in total cycles and percentage decrease \nm dynamic instruction count and load interlock cycles for unrolling factors of 4 and 8, relative to no \nunrolling. BS speedup vs. TS % reduction in Ioadl interlock cycles % of total cycles due to load interlock \ncycles No LU LU 4 LU 8 Benchmark No LU LU 4 LU 8 No LU LU 4 LU 8 BS TS BS TS BS TS ARC2D 1.33 1.52 1.78 \n53,3% 66.7% 77.4% 18.9% 30.3% 15,4% 30.4% 12.3% 30.6% BDNA 1.01 1,01 1.09 75.8% 806% 791% 1.2% 4,9% 0970 \n4.5% 1.0% 4.4% DYFESM 0.90 0,96 0,97 34,570 83.7% 80.6% 1.8% 3,1% O6% 3 6% 0.7% 3 7% MDG 098 0.99 0.99 \n64.6% 70.0% 65.9% 0.8% 2,2% 0.7% 2.5% 1.0% 2.9% QCD2 0.98 1,01 0,99 64% 359% 34.5% 22% 2.4% 1,770 2.6% \n2.1% 3.2% TRFD 0.93 0.96 0.98 51,8% 78.0% 80.8% 3,9% 8.7% 2,3% 10.8% 2.2% 11 8% alvinn 0.93 0.99 0.99 \n14.9% 3.2% 3,7% 8.5% 10.8% 9.4% 9,8% 9.6% 10.0% drrasa7 1.18 1.76 1.84 73.0% 87,2% 88.0% 7.6% 23.9% 5.7% \n25.290 5.5% 24.8% doduc 1,00 1,00 1,01 44,590 45.6% 47.4% 2,5% 45% 2.5% 46% 23% 4.3% ear 0.93 0.95 0.95 \n44.2% 399% 28.2% 0.6% 1.2% 0.8% 1,4% 1.1% 1.6% hydro2d 0.99 1,07 1.11 68.4% 887% 87.0% 4.0% 12.87. 2.0% \n17.0% 2.6% 18,2% mdljdp2 1.03 1.03 1.03 54.0% 55.4% 55.3% 2.7% 57% 2690 57% 26% 5.7% ora 1,00 100 1.00 \n----\u00ad ----\u00ad ..... 0.0% 0.0% 0.0% 0.0% 0.090 0,0% spice2g6 1.02 1,01 1,01 75% 85% 8.0% 29,5% 31 4% 295% \n32.1% 30.5% 327% su2c0r 1,18 1.22 1,26 87.8% 88.8% 90.4% 3.7% 25.3% 3.8% 27.5% 3.2% 26.5% swm256 1.22 \n1.22 1.63 67.8% 68.4% 94.4% 17.1% 43.5% 16.8% 43.6% 4.2% 45.6% tomcatv 1.22 1.36 137 72.4% 759% 72.7% \n14.090 41.3% 14.070 42.9% 16.97. 45.3% AVERAGE 1.05 1.12 1.18 51.3% 61.0% 62.1 % 7.0% 14.8% 6.4% 15.5% \n5.8% 16.0% Table 5: Balanced scheduling (BS) vs. traditional scheduling (TS) for loop unrolling: Total \ncycles speedup, percentage improvement in load interlock cycles, and load interlock cycles as a percentage \nof total cycles, the dynamic instruction count continued to drop, but by less (on of51 Yo,61%, and 62?Z0fewer \nload interlock cycles than traditional average, another 2.8% for load interlock cycles, for a total scheduling \nwhen unrolling O, 4, and 8 times, respectively (again, reduction of 26Y0, and another 3 %0for dynamic \ninstruction count, Table 5). The result was that only 7%, 6% and 6% of total execution for a total reduction \nof 14%). The three exceptions were TRFD, cycles were lost to load interlocks, while traditional scheduling \ns doduc and tomcatv, for which overall performa~ce dropped relative penalty was 15%, 16% and 16%. to \nunrolling by 4. In TRFD the increase in spill instructions offset Although balanced scheduling is, on \naverage, a better match thanthe reduction in branch overhead; in doduc, the more aggressive traditional \nlist scheduling with techniques that increase basic block unrolling increased code size, contributing \nto a degradation in size, such as loop unrolling, the average speedups mask a wideinstruction cache performance; \nand in tomcatv, interlock cycles divergence in the programs ability to benefit from it. For particular \nincreased because of the increase in spill code. benchmarks, traditional scheduling even outperformed \nbalanced The more important loop unrolling results, however, were found scheduling. In these programs, \nbalanced scheduling continued to be when comparing balanced and traditional sched~ling. The combi-more \nsuccessful at reducing load interlock cycles. However, it did nation of balanced scheduling and loop \nunrolling produced better this at the expense of hiding interlock cycles caused by multi-cycle average \nspeedups than coupling loop unrolling with traditional list instructions with fixed latencies, such as \ninteger multiply and scheduling. Without loop unrolling, balanced scheduled codes floating point instructions. \nThe traditional scheduler, on the other executed on average 1.05 times faster than the traditional approach \nhand, gave preference to the fixed latency operations. (Table 5); with it, balanced scheduling s advantage \nrose to 1.12 The benchmarks in question exhibited two characteristics thatwhen unrolling by 4, and to \n1.18 when by 8. (In other words, in limited the effectiveness of balanced scheduling. First, they had \ncontrast to balanced scheduling s 1.19 and 1.28 average improve\u00adsmaller basic blocks, and hence, less \nload-level parallelism. ments when unrolling by 4 and 8, traditional scheduling provided Second, as a \npercentage of total cycles, load interlock cycles werespeedups of only 1.12 and 1.15.) In all cases (all \nprograms and small and were outnumbered by non-load interlock cycles. In these unrolling optimization \nlevels) dynamic instruction counts between cases, when non-load interlock cycles dominated total interlockthe \ntwo scheduling approaches were almost identical. Balanced cycles and insufficient parallelism existed, \nbalanced schedulingscheduling s advantage stemmed entirely from its ability to hide sacrificed hiding \nnon-load interlocks to hide the latency of loadload latencies: in particular, the balanced code induced \nan average instructions. When more parallelism was exposed by unrolling loops, the frequency and magnitude \nof these performance degrada\u00ad2 There were e~~e~ti~~s, however. For example, the 64 instruction tions \nwere reduced. Nonetheless, the results argue for either incor\u00adlimit on unrolling by 4 prevented swm256 \nfrom being fully porating multi-cycle instructions with fixed latencies into the unrolled; the higher \nlimit with an unrolling factor of 8 allowed balanced scheduling algorithm or developing heuristics to \nstatically choose between the two schedulers on a basic block basis. more unrolling and therefore provided \nmore benefit. Speedup over BS alone Lu 4, MS, LU 8, TrS, Benchmark TrS LU 4, TrS LU 8, M LA LU 4, LA \nLU 8, LA LA LA ARC2D 1 13z 135 172 117 138 1.54 1.42 1.67 BDNA 1,02 103 1.07 ..\u00ad ... ... .-. . .. DYFESM \n0,98 0.44 047 109 131 1.32 1.21 1.21 MDG 1.04 1.11 1.13 1.15 1,16 1.17 1,27 127 QCD2 101 1.18 1,20 104 \n1.16 1.17 123 1.19 TRFD 0.99 1,55 149 134 1.36 136 1,44 1.45 alvmn 097 124 1.25 101 125 1.25 1.24 1.25 \ndnasa7 111 1.76 2,19 1,20 1.75 1,83 177 211 doduc 101 110 056 097 0.99 099 0.67 ... ear 096 120 141 1.03 \n1.12 1.31 1.24 141 hydro2d 1,02 1.55 1.76 1,44 1.79 182 1.79 195 mdljdp2 088 089 089 100 1,OQ I(XI 0,89 \n0,88 ora 106 106 106 100 1,00 1.00 1,06 1,06 spice2g6 1.05 1,18 1,14 1.02 102 104 112 112 su2c0r 0.95 \n109 1,25 113 117 1,14 . .. swm256 101 1.14 1.48 123 1.60 1.60 1.65 165 tomcatv 101 1.29 1.34 1.53 1.39 \n139 140 140 AVERAGE 1.01 1.19 1.26 1,15 1.28 1.31 1.29 1.40 Table 6: Speedups over balanced scheduling \nalone for combinations of loop unrolhng by 4 and 8 (LU 4 and LU 8), trace scheduling (TrS) and locality \nanalysis (LA). Speedup of Balanced Scheduling over lkaditiomd Scheduling 1 No trace scheduling llace \nscheduling Benchmark NoLU LU 4 LU 8 LU 4 LU 8 ARC2D 1 3? 152 1,78 1.62 1.99 BDNA 101 1,01 1,09 1.09 1,13 \nDYFESM 090 096 0,97 0.85 0.85 MDG 098 0.99 0.99 097 097 QCD2 098 1,01 099 096 0.99 TRFD 093 096 098 1.01 \n1.01 alvinn 093 0.99 0,99 0.97 098 dnasa7 118 176 184 1.83 2,17 doduc 100 100 101 1.06 1,02 ear 093 095 \n0,95 091 0.92 hydro2d 099 1.07 I.11 1,09 1.06 mdljdp2 103 103 103 0.91 0,91 ora 1.00 1.00 1 (XI 1,00 \n1.00 splce2g6 1.02 101 101 1.14 1.06 SU2C01 118 I 22 126 ... ... swm256 1.22 122 1.63 137 164 tomcatv \n122 136 1,37 138 143 AVERAGE 1.05 1.12 1.18 1.14 1.16 Table 7: Balanced scheduling (BS) vs. traditional \nscheduling (TS): Total cycles speedup for loop unrolling alone and trace scheduhng and loop unrolling \n158 5.2 Trace Scheduling Trace scheduling alone brought little benefit for this workload (Table 6, column \n2). Three factors limited its effectiveness. First, traces do not cross back edges of loops, and therefore \ntrace schedul\u00ading does not schedule beyond loop boundaries. Since the bulk of execution time in our applications \nwas spent in loops, there was lit\u00adtle opportunity to apply it effectively. Second, because it optimizes \nthe most frequently executed traces at the expense of less fre\u00adquently executed paths, trace scheduling \nis most effective when there are dominant paths of execution. DYFESM and doduc, in par\u00adticular, have \nfew dominant paths, and consequently trace schedul\u00adinghadeither limited effectiveness oreven degraded \nperformance, Third, programs such as BDNA already had extremely large basic blocks, and therefore trace \nscheduling was not needed to increase available parallelism, Forthese reasons, when evaluating the inter\u00adaction \nbetween trace scheduling and the two code scheduling tech\u00adniques, we first unrolled the loops. Averaged \nacross the entire workload, the speedups of trace sched\u00aduling and loop unrolling were 1.19 with an unrolling \nfactor of 4, and 1.26 when unrolling by 8 (again, Table 6).1 For programs where trace scheduling could \naccurately pick traces and string together several nonloop basic blocks, the wins were larger, on average \n1.29 and 1.43 for unrolling by 4 and 8, respectively. Included in this group were programs for which \nloop unrolling alone had been less beneficial; by effectively increasing basic block size in both the \nloops with conditionals andthesequential code, trace scheduling improved their performance. Consequently, \nthe combination of loop unrolling and trace scheduling sometimes brought benefits to balanced scheduling \nthat were greater than the sum of the individual effects. As with theloop unrolling experiments, thecomparison \ntotradi\u00adtional scheduling came out in balanced scheduling s favor (Table 7). When combined with trace \nscheduling and an unrolling factor of 4, balanced scheduled programs executed 1.14 times faster than \ntradi\u00adtional scheduling with the same optimization, The comparable speedup fortrace scheduling mdanunrolling \nfactor of8 was 1.16. As before, the improvements stemmed from balanced scheduling s ability tohideload \ninterlock cycles more effectively: their decrease rose to 65% and56%, resulting inonly5%o ftotale xecutioncycles \nwasted on load interlocks for the two degrees of unrolling. Tradi\u00adtional scheduling with trace scheduling \nand unrolling by 4 and 8, on the other hand, was still left with 15% of total cycles lost to load interlocks. \nOn an individual basis, however, traditional scheduling was sometimes superior to balanced scheduling, \nas in some of the loop unrolling experiments, because non-load interlock cycles were hidden more effectively. \nAs a final note, we observe that the overall performance gains due to trace scheduling are smaller than \nthose obtained in the original trace scheduling studies. This discrepancy can be attributed to the differing \nmachine models. Trace scheduling was first applied to VLIW machines, where instruction issue bandwidth \nwas not a bottleneck, However, inoursingle-issue model, where instruction issue bandwidth can be a scarce \nresource, speculative execution, lBecause of speculative execution and poor trace selection, the dynamic \ninstruction count for DYFESMmore than doubled for both balanced and traditional scheduling on a single-issue \nmachine. On a wider-issue machine (superscalar or VLIW), this effect would be smaller, because of theincreased \ninstruction issue bandwidth. If we had excluded DYFESM from the results for balanced scheduling plus \ntrace scheduling and loop unrolling, speedups would have been 1.23 (4) and 1.31 (8). (See also the discussion \nat the end of this section.) with its increased instruction count, can stress it. 5.3 Locality Analysis \nIn contrast to the first two optimization, which increase the amount of instruction-level parallelism \nin the code, locality analy\u00adsis allows the balanced scheduler to exploit the existing parallel\u00adism more \neffectively. Locality analysis should improve performance for two inter-related reasons. First, by relying \non more accurate information about memory latency (cache hits and misses), less uncertainty should exist, \nand the scheduler will pre\u00adsumably generate abetter schedule. Specifically, since we do not apply bahmced \nscheduling to load instructions that are cache hits, independent instructions that would normally be \nused to hide their latency can reapplied to loads that will miss, Second, since the spatial locality \nanalysis relies on (limited) loop unrolling, basic block sizes should increase, consequently giving the \nbalanced scheduler more flexibility in using independent instructions to hide load Iatencies, Taken together, \nthese effects should decrease inter\u00adlock cycles over applying balanced scheduling alone. Program speedups \nfrom locality analysis averaged 1,15 (Table 6), with the percentage gain divided almost evenly between \ndecreases in dynamic instruction count (branch overhead and spill code) and non-load interlock cycles \n2. The gains were less than those achieved by the optimization that increased basic block size (loop \nunrolling and loop unrolling coupled with trace scheduling), because there were fewer opportunities where \nlocality analysis could reapplied. Themost striking exception wastomcatv, which has very sequential accesses \nto large, read-only arrays: speedup for this program was 1.5. In general, however, four factors limited \nthe effectiveness of the locality analysis algorithm. First, in order to exploit locality, the compiler \nmust be able to determine how an array is aligned relative to cache lines. When subsections of arrays \narepassed asparameters toasubroutine, this is not possible. The second factor is the presence of index \nexpressions that introduce irregularity into the memory reference pattern. When array refer\u00adences contain \nindex expressions that are not exclusively composed of affine functions of loop induction variables, \nreuse cannot be identified statically. Third, some benchmarks pass multi-dimen\u00adsional arrays as parameters \nto subroutines, but the array dimen\u00adsions are dependent on other subroutine parameters. The cache alignment \ncharacteristics of these arrays are therefore unknown at compile time. Fourth, even though a load may \nbe identified as a cache hit, it may actually turn out to bea cache miss because of interference with \nanother load reference.  5.4 PuttingI tAllTogether Considering these results across optimizations (rather \nthan across programs for a particular optimization, as we have done previ\u00adously), balanced scheduled \ncode consistently produced speedups over that generated by traditional scheduling. Its performance improvements \nstemmed from its ability to hide load interlocks more effectively. With only two exceptions, balanced \nscheduled code produced fewer load interlocks than that of the traditional scheduler for all programs, \non all levels of optimization3. The differences ranged from two to three times as many interlocks (averaged \nacross all programs for a particular optimization) for the traditional scheduler (see summary results \nin Table 8). 2 The bulk of load interlock cycles had been reduced by balanced scheduling. 3 except locality \nanalysis. Since traditional scheduling relies on a single load latency, it has no counterpart in locality \nanalysis. Relative to traditional Relative to balanced Load interlock cycles remaining scheduling with \nthe same scheduling with no other after applying the optimization: optimization: optimization (% of total \ncycles) Percentage Percentage decrease in decrease in Program load interlock Program load interlock Balanced \nTraditional Optimization in addition to balanced scheduling speedup cycles speedup cycles scheduling \nscheduling No optumzatlom 105 51 na na 7 15 Loop unrolhng by 4 I 12 61 1,19 23 6 16 Loop unrolling by \n8 1.18 62 128 26 6 16 I r race scheduhng with loop unrolhng by 4 II 1.14 I 65 \\ 119 I 42 5 15 IT race \nscheduhng with loop unrolling by 8 II 116 \\ 56 I 1,26 I 34 5 15 Table 8: Summary comparison of balanced \nscheduling and traditional scheduling. Speedup relative to bal-Speedup relative arced scheduling with \nnu to locality analysis unrolting and no trace Optimization alone scheduling Locahty analysls na 115 \nLocahty analysls w!tb loop unrolhng by 4 1.11 1.28 Locahty analysls with 10opunrolhng by 8 1.14 131 Locality \nmalysls with trace schedubng and loop unrolling by 4 112 1.29 Locality analysls wnh trace scheduling \nand loop unrolling by 8 121 1.40 Table 9: Summary comparison of locality analysis results. As we applied \noptimizations that increased instruction-level paral-such as Predicated execution and array dependence \nanalysis to lelism more aggressively (i.e., no optimization through trace sched-disambig~ate between \nloads and stores: tha~ benefit code sched\u00aduling and loop unrolling with an unrolling factor of 8), balanced \nuling by exposing more load-level parallelism. These optimizations scheduling was able to extend its \nadvantage over traditional sched-are included m the Multiflow compiler. For the four programs that uling \nby exploiting the additional instruction-level parallelism. The both studies have in common, we estimate \nthat balanced scheduling percentage of load interlock cycles in balanced scheduled code had a 10% advantage \nover traditional scheduling with the simple dropped, while in traditionally scheduled code it remained \nconstant model, but only 4T0 when modeling the 21164. Recall that the or rose slightly. The consequence \nwas a consistent rise in speedup 21164 has a longer memory latency, ranging from 2 to 50 cycles, of balanced \nscheduling over the traditional approach. depending on the memory hierarchy level, and multi-cycle floating \npoint and integer multiply instructions. Therefore there are more Locality analysis contributed additional \nspeedup when applied latencies of several types to hide, some of which (the fixed latencyalong with the \nother optimization. When used with loop unrolling, arithmetic operations) balanced scheduling does not \nyet address. speedups of 1.28 and 1.31 were obtained over balanced scheduling Despite these differences, \nthis study validates the earlier conclusion alone, for unrolling factors of 4 and 8, respectively (Tables \n6 and 9). that balanced scheduling M on average superior to traditional sched-When trace scheduling was \nalso applied, these speedups reached uling. 1.29 and 1.40.   6 Conclusion 5.5 Simulating Real Architectures \nModern processors with lockup-free caches provide new opportu-Examining results from the original comparison \nof balanced and nities for better instruction scheduling by exposing the variations intraditional scheduling \n(without optimization) [KE93] illustrates load latency due to the memory hierarchy. Unfortunately, tradi\u00adboth \nthe limitations of simple architecture models and the benefits tional scheduling techniques cannot take \nadvantage of this oppor\u00ad of a very optimizing compiler for execution cycle analysis, There tumty, because \nthey assume that load instruction weights are fixed, were several methodological differences between \nthe two studies. best-case latency estimates. Balanced scheduling, on the otherFirst, the original work \nrelied on a stochastic model, rather than hand, is able to hide variable latencies by basing load instructionexecution-driven \nsimulation, to simulate cache behavior and weights on a measure of load-level parallelism in the program,network \ncongestion. Second, it included first-level cache load rather than using the fixed and optimistic latency \nestimates. latencies, but assumed single-cycle execution for all other multi\u00adcycle instructions, modeling \nan instruction cache that always hit Our studies have shown that the balanced scheduler can increase \nits and an ideal, pipelined floating point unit, and excluding a cache advantage over a traditional scheduler \nwhen more instruction-level hierarchy and TLB. This work incorporates all factors; fixed, multi\u00adparallelism \nis available, By applying loop unrolling, trace sched\u00ad cycle instruction latencies, in particular, proved \nto have a marked uling, and locality analysis m conjunction with the balanced effect on code scheduling \nperformance. Finally, the previous study scheduler, load interlock cycles were reduced to as low as 5 \n%0of used code generated by gee, which omits several optimlzatlons, total cycles when averaged across \nan entire workload. In contrast, traditionally scheduled code wasted no less than 15~o of its execution \ncycles waiting for load instructions, when the same optimizations were applied. The ability of the balanced \nscheduler to hide load latency translated into significant performance gains. Average speedups of balanced \nscheduled code over traditionally scheduled code increased from 1.05 without the ILP-enhancing optimizations \nto as much as 1.18 when they were used. Of all three optimization, loop unrolling produced the most \nsi,g-nlf\u00adicant benefits for balanced scheduled code, achieving performance improvements of 1.19 and 1.28 \nover no unrolling. Relative to tradi\u00adtional scheduling, balanced scheduling increased its edge from from \n1.05 (no unrolling) to 1.12 (unrolling by 4) to 1.18 (unrolling by 8). While loop unrolling increases \nparallelism within loop bodies, trace scheduling exposes it in unrolled loops that contain conditionals \nand in the sequential portions of code. However, since loops dominate execution in our workload, trace \nscheduling was only beneficial when used in conjunction with loop unrolling. The combination produced \nbalanced schedules that executed 1.19 times faster when unrolling by 4, and 1,26 times faster when unrolling \nby 8, over balanced scheduling alone. Furthermore, they increased balanced scheduling s lead relative \nto traditional scheduling to 1.14 (4) and 1.16 (8), Locality analysis provided a performance improvement \nof 1.15 over balanced scheduling alone. The figure reflects both the relatively small amount of array \nreuse in our workload and the benefits of the limited loop unrolling that is required to implement the \nalgorithm. Combining locality analysis with the other optimiza\u00adtion improved the speedups, The best case, \nof course, occurred when unrolling by 8, trace scheduhng, and locality analysis were all used: here average \nspeedups reached 1.40. Although our results demonstrate that the balanced scheduler 1s more effective \nthan traditional scheduling in takhg advantage of the additional instruction-level parallelism generated \nby the three optimization for most programs, there are cases where traditional scheduling does better. \nBecause it uses optimistic estimates for load latency, it saves instruction-level parallelism for multi-cycle \nopera\u00adtions. When (fixed latency) multi-cycle operations dominate loads and there is insufficient instruction-level \nparallelism to hide instruction Iatencies, traditionally scheduled code can execute faster. Having shown \nthat balanced scheduling is a good match with optimizations that increase instruction-level parallelism, \nwe intend to examine its effects on wider-issue (superscalar) processors that require considerable instruction-level \nparallelism to perform well, We also plan to investigate new techniques to better handle fixed, non-load \ninterlock cycles within the framework of the balanced scheduling algorithm.  Acknowledgments We would \nlike to thank John O Donnell from Equator Technolo\u00adgies, Inc., Michael Adler, Tryggve Fossum and Geoff \nLowney from Digital Equipment Corp., Stefan Freudenberger from HP Labs, and Robert Nix from Silicon Graphics \nInc. for access to the Multiflow compiler source and technical advice in using and alter\u00ading it; both \nwere invaluable. We also thank Dean Tullsen for the use of his Alpha simulator. This research was sponsored \nby NSF PYI Award #MIP-9058-439, ARPA contract #NOOO14-94-1136, and Digital Equipment Corporation.  References \nlAspY5J [BCK+89] [CS95] [DA92] [Dix92] [ER94] [FERN84] [FGL93] [FJ94] [GM86] [Gwe94a] [Gwe94b] [HG83] \n[KE93] [Kro81] 1. .4Sprey. I errormance Ieatures 01 the t A / 1UU micro\u00ad processor. IEEE Micro, 13(3):22 \n35, June 1993, M. Berry, D. Chen, D. Kuck, S. Lo, Y. Pang, L. Pointer, R. Roloff, A. Sameh, E. Clementi, \nS. Chin, D, Schneider, G. FOX, P. Messina, D. Walker, C. Hsiung, J. Schwarzmeier, K. Lue, S. Orszag, \nO. Johnson F. Seidl and, R. Goodrum, and J. Martin. The Perfect Club: Effective performance evaluation \nof supercomputers, [nternalional Journal of Supercom\u00adputer Applications, 3(3):5-40, December 1989. R.P. \nColwell and R.L. Steck. A 0.6um BiCMOS Pro\u00adcessor with Dynamic Execution. In IEEE International Solid-State \nCircuits Conference 95, page 176-177, February 1995. K. Diefendorf and M, Allen. Organization of the \nMo\u00adtorola 88110 superscalar RISC microprocessor. IEEE Micro, 12(2):5 1-61, April 1992.  K.M. Dixit, \nNew CPU benchmark suites from SPEC. In COMPCON, pages 305 3 10, February 1992. J. Edmondson and P. Rubinfeld. \nAn overview of the 21164 Alpha AXP microprocessor. In Hot Chips W, pages 1-8, August 1994.  J.A. Fmher, \nJ.R. Ellis, J. Ruttenberg, and A. Nicolau. Parallel processing: A smart compiler and a dumb ma\u00adchine. \nIn A CM SIGPLAI!J 84 Symposium on Compiler Construction, pages 36-46, June 1984. S.M. Freudenberger, \nT.R. Gross, and P.F. Lowney. Avoidance and suppression of compensation code in a trace scheduling compiler. \nTechnical Report HPL-93\u00ad35, Hewlett Packard, 1993. K.I. Farkas and N.P. Jouppi. Complexity/performance \ntradeoffs with non-blocking loads. In 21th Annual In\u00adternational Symposium on Computer Architecture, \npages 211-222, April 1994. P,B. Gibbons and S.S. M[uchnick. Efficient instruction scheduling for a pipelined \narchitecture. In ACM SIG-PLAN 86 Symposium on Compiler Conswuction, pag\u00ades 11 16, July 1986. L. Gwennap. \nUltraSparc Unleashes SPARC Perfor\u00admance. Microprocessor Report, pages 1 8, October 3, 1994. L. Gwennap. \n620 Fills Out PowerPC Product Line. Mi\u00adcroprocessor Report, pages 12 1 6, October 24, 1994. J.L. Hennessy \nand T.R. Gross. Postpass code optimiza\u00adtion of pipeline constraints. ACM Transactions on Pro\u00adgramming \nLanguages and Systems, 5(3):422-448, July 1983. D.R. Kerns and S,J. Eggers. Balanced scheduling: In\u00adstruction \nscheduling when memory latency is uncer\u00adtain, In ACM SIGPLAN 93 Conference on Program\u00adming Language Design \nand Implementation, pages 278-289, June 1993. D. Kroft. Lockup-free instruction fetch/prefetch cache \n  organization. In 8th Annual International Symposium [LFK+93] [MCL93] [Mip94] [MLG92] [Sit78] [Sta] \non Computer Architecture, pages 8 1 87, May 1981. P.F. Lowney, S.M. Freudenberger, R,J. Karzes, W.D. \nLiechtenstein, R.P. Nix, J.S. O Donnell, and J.C. Rut\u00adtenberg. The Multlflow trace scheduling compiler. \nThe Journal of Supercomputing, 7:5 1 143, 1993. E. McLellan. The Alpha AXP architecture and 21064 processor. \nIEEE Micro, 13(3):36-47, June 1993,  MIPS Technologies, Inc. Mips Open RISC Technology RI 0000 Microprocessor \nTechnical Brief. October 1994, T.C. Mowry, M.S. Lam and A. Gupta, Design and evaluation of a compiler \nalgorithm for prefetching. In F@h International Conference on Architectural Sup\u00adport for Programming \nLunguages and Operating Sys\u00adtems, pages 62 73, October 1992. R.L. Sites. Instruction Ordering for the \nCray -1 Com\u00adputer. Technical Report 78-CS-023, Univ. of Califor\u00adnia, San Diego, July 1978. R. StaHman. \nThe GNU Project Optimizing C Compiler. Free Software Foundation.   \n\t\t\t", "proc_id": "207110", "abstract": "<p>Traditional list schedulers order instructions based on an optimistic  estimate of the load latency imposed by the hardware and therefore cannot respond to variations in memory latency caused by cache hits and misses on non-blocking architectures. In contrast, balanced scheduling schedules instructions based on an estimate of the amount of instruction-level parallelism in the program. By scheduling independent instructions behind loads based on what the program can provide, rather than what the implementation stipulates in the best case (i.e., a cache hit), balanced scheduling can hide variations in memory latencies more effectively.</p><p>Since its success depends on the amount of instruction-level parallelism in the code, balanced scheduling should perform even better when more parallelism is available. In this study, we combine balanced scheduling with three compiler optimizations that increase instruction-level parallelism: loop unrolling, trace scheduling and cache locality analysis. Using code generated for the DEC Alpha by the Multiflow compiler, we simulated a non-blocking processor architecture that closely models the Alpha 21164. Our results show that balanced scheduling benefits from all three optimizations, producing average speedups that range from 1.15 to 1.40, across the optimizations. More importantly, because of its ability to tolerate variations in load interlocks, it improves its advantage over traditional scheduling. Without the optimizations, balanced scheduled code is, on average, 1.05 times faster than that generated by a traditional scheduler; with them, its lead increases to 1.18.</p>", "authors": [{"name": "Jack L. Lo", "author_profile_id": "81100147376", "affiliation": "Department of Computer Science and Engineering, University of Washington", "person_id": "P130565", "email_address": "", "orcid_id": ""}, {"name": "Susan J. Eggers", "author_profile_id": "81100262930", "affiliation": "Department of Computer Science and Engineering, University of Washington", "person_id": "PP15027685", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/207110.207132", "year": "1995", "article_id": "207132", "conference": "PLDI", "title": "Improving balanced scheduling with compiler optimizations that increase instruction-level parallelism", "url": "http://dl.acm.org/citation.cfm?id=207132"}