{"article_publication_date": "06-01-1995", "fulltext": "\n Optimizing Parallel Programs with Explicit Synchronization Arvind Krishnamurthy and Katherine Yelick \nComputer Science Division University of California, Berkeley * Abstract: We present compiler analyses \nand optimizations for explicitly parallel programs that communicate through a shared address space. Any \ntype of code motion on explicitly parallel programs requires a new kind of analysis to ensure that operations \nreordered on one processor cannot be ob\u00adserved by another. The analysis, based on work by Shasha and \nSnir, checks for cycles among interfering accesses. We improve the accuracy of their analysis by using \nadditional information from post-wait synchronization, barriers, and locks. We demonstrate the use of \nthis analysis by optimizing remote access on distributed memory machines. The opti\u00admization include message \npipelining, to allow multiple out\u00adstanding remote memory operations, conversion of two-way to one-way \ncommunication, and elimination of communica\u00adtion through data re-use. The performance improvements are \nas high as 20-35% for programs running on a CM-5 mul\u00adtiprocessor using the Split-C language as a global \naddress layer. Introduction Optimizing explicitly parallel shared memory programs re\u00ad quires new types \nof static analysis to ensure that accesses reordered on one processor cannot be observed by another. \nIntuitively, the parallel programmer relies on the notion of sequential consistency the parallel execution \nmust behave as if it were an interleaving of the sequences of memory operations from each of the processors \n[12]. If only the lo\u00ad cal dependencies within a processor are observed, the pro\u00ad gram execution might, \nnot be sequentially consistent [15]. To guarantee sequential consistency under reordering trans\u00ad formations, \na new type of analysis called cycle defecihon is required [18]. This work was supported m part by the \nAdvanced Research Projects Agency of the Department of Defense monitored by the Of\u00adfice of Naval Research \nunder contract DABT63-92-C-O026, by the De\u00adpartment of Energy under contract DE-FG03-94ER25206, and by \nthe National Science Foundation, The information presented here does not nec. memly reflect the ~omt, \non o. the poky of the Government ..cI ~0 ~ffi~]~l ~ndor,ge~~nt ~h~uld be inferred Permission to copy \nwithout fee all or part of this material is granted provided that the copies are not made or distributed \nfor direct commercial advantage, the ACM copyright notice and the title of the publication and its date \nappear, and notice is given that copying is by permission of the Association of Computing Machinery.To \ncopy otherwise, or to republish, requires a fee and/or specific permission. SIGPLAN 95La Jolla, CA USA \n@ 1995 ACM 0-89791 -697-2/95/0006,.. $3.50 hutudly Data= Flag=O Proc 1 Proc: 2 Write Data=l 1( Read Flag \n1 J Y 1 Write Flag=l Read Data 1 Figure 1: If the read of Flag returns 1, the read of Data should see \nthe new value. An example to illustrate sequential consistency is shown in Figure 1. The program is indeterminate \nin that the read of Flag may return either O or 1, and if it is O, then the read to Data may return either \nO or 1. However, if 1 has been read from Flag, then 1 must be the result of the read from Data. If the \ntwo program fragments were analyzed by a sequential compiler, it might determine that the reads or writes \ncould be reordered, since there are no local dependencies. If either pair of the accesses is reordered, \nthe execution in which Flag is 1 and Data is O, might result. Even if the compiler does not reorder the \nshared mem\u00adory accesses, reordering may take place at many levels in a multiprocessor system. At the \nprocessor level, a superscalar may issue an instruction as soon as all its operands are avail\u00adable, so \nwrites to different locations might be issued in the order the values become available. Most processors \nhave write buffers, which allow read operations to overtake write operations that have been issued earlier. \nIn fact, on the Su\u00adperSparcs [20] the write-buffer itself is not guaranteed to be FIFO. Reordering may \nalso take place at the network level in distributed memory multiprocessors, because some net\u00adworks adaptively \nroute packets to avoid congestion. Even if packets do not get reordered, two access sent to two different \nprocessors may be handled out of order, since latencies may vary. Also, on a machine like DASH [13], \nwith hardware caching, writes do not wait for all invalidations to complete, so remote accesses might \nappear to execute in reverse-order. These architectural features usually come with support to ensure \nsequential consistency, such as a memory barrier or a write-buffer flush to enforce ordering between \nmemory op\u00aderations, or a test for completion of a remote operation. However, these new instructions must \nbe inserted by the compiler. If a standard uniprocessor compiler is used for generating code, these special \ninstructions would not be au\u00adtomatically inserted. The cycle detection problem is to detect access cycles, \nsuch as the one designated by the figure-eight in Figure 1. In 196 addition to observing local dependencies \nwithin a program, a compiler must ensure that accesses issued by a single pro\u00adcessor in a cycle take \nplace in order. Cycle detection is necess\u00adary for most optimizations involving code motion, whether the \nprograms run on physically shared or distributed mem\u00adorv and whether thev have dvnamic or static thread \ntcre\u00adat~on. Cycle detecti:n is not-necessary for automatically parallelized sequential programs or data \nparallel programs with sequential semantics, because every pair of accesses has a fixed order, which \nis determinable at compile-time. The additional problem for explicitly parallel programs comes di\u00adrectly \nfrom the possibility of non-determinism, whether or not the programmer chooses to use it. In spite of \nthe semantic simplicity of deterministic pro\u00adgramming models, for performance reasons many applica\u00adtions \nare written in au explicitly parallel model. As we no\u00adticed with our toy example, uniprocessor compilers \nare ill\u00adsuited to the task of compiling explicitly parallel progralms, because they do not have information \nabout the semantics of the communication and synchronization mechanisms. As a result, they either generate \nincorrect code or miss oppor\u00adtunities for optimizing communication and synchronization, and the quality \nof the scalar code is limited by the inability to move code around parallelism primitives [15]. We present \noptimizations for multiprocessors with phys\u00ad ically distributed memory and hardware or software support \nfor a global address space. As shown in table 1, a remote ~eference on such a machine has a long latency \n[2][21][1.3]. However, most of this latency can be overlapped with local computation or with the initiation \nof more communication, especially on machines like the J-Machine and *T, with their low overheads for \ncommunication startup. CM-5 T3D DASH Remote Access 400 85 110 Local Access 30 23 26 Table 1: Access latencies \nfor local and remote memory modules expressed in terms of machine cycles. Three important optimizations \nfor these multiprocessors are overlapping communication, eliminating ronnd-t rip mes\u00adsage traffic, and \navoiding communication altogether. The first optimization, message pipeliningj changes remote read and \nwrite operations into their split-phase analogs, get and put. In a split-phase operation, the initiation \nof an access is separated from its completion [6]. The operation to force completion of outstanding split-phase \noperations comes in many forms, the simplest of which (called sync or ~erwe) blocks until all outstanding \naccesses are complete. To i m\u00adreprove communication overlap, puts and gets are moved back\u00adwards in the \nprogram execution and syncs are moved for\u00adward. The second optimization eliminates acknowledgement traffic, \nwhich are required to implement the sync operation for puts. A final optimization is the elimination \nof remcke accesses by either re-using values of previous accesses or up\u00addating a remote value locally \nmultiple times before issuing a write operation on the final vaJue. Cycle detection was first described \nby Shasha and Snir [18] and later extended by Midkiff, Padua, and Cytron to handle array indices [16]. \nIn previous work, we showed that by restricting attention to Single Program Multiple Data (SPMD) programs, \none could significantly reduce the com\u00adplexity of cycle detection [11]. The primary cent ribution of \nthis paper is improved cycle detection that makes use of syn\u00adchronization information in the program. \nShasha and Snir s analysis, when applied to real applications, discovers a huge number of spurious cycles, \nbecause cycles are detected be\u00adtween accesses that will never execute concurrently due to synchronization. \nWe use synchronization analysis to elimi\u00adnate these spurious cycles. The rest of the paper is organized \nas follows. The source programming language is described in section 2. We present basic terminology in \nsection 3 and a brief summary of Shasha and Snir s result in section 4. In section 5, we present our \nnew algorithms that incorporate synchronization analysis, and in sections 6 and 7, we give code generation \nand opti\u00admization for distributed memory machines. Section 8 esti\u00admates the potential payoffs of our \napproach by optimizing some application kernels. Related work is surveyed in sec\u00adtion 9 and conclusions \ndrawn in section 10. 2 Programming Language Our analyses are designed for explicitly parallel shared \nmem\u00adory programs. We have implemented them in a source-to\u00adsource transformer for a subset of Split-C \n[6]. Split-C is an explicitly parallel SPMD language for pro\u00adgramming distributed memory machines using \na global ad\u00address space abstraction. The parallel threads interact through reads and writes on a shared \naddress sDace that contains dis\u00ad . tributed arrays and shared objects accessible through wide pointers. \nThe most important feature of Split-C as a target is its support for split-phase (or non-blocking) memory \nop\u00aderations. Given pointers to global objects src 1 and dest 2, and local valnes src2 and dest 1 of the \nsame type, the split\u00ad phase versions of read and write operations on the global objects are expressed \nas: get (dest 1, srci) put (dest2, src2) /* unrelated computation */ sync () ; In the first assignment \nstatement, a get operation is per\u00adformed on src 1, and in the second, a put is performed on dest2. Neither \nof these operations are guaranteed to com\u00adplete (the values of dest 1 and dest2 are undefined) until \nafter the sync statement. A get operation initiates the read of a remote location, but it does not wait \nfor the value to be fetched. Similarly, a putoperation does not wait for the acknowledgement that the \nwrite occurred on the remote pro\u00adcessor. The sync operation delays the execution for previous non-blocking \naccesses to complete. On a distributed mem\u00adory machine, the get and put operations are implemented using \nlow-level messages sent across the interconnection net\u00adwork. Therefore, split-phase operations facilitate \ncommuni\u00adcation overlap, but the sync construct provides less control than one might want, because it \ngroups all outstanding puts and gets from a single processor. Split-C also provides finer grained mechanisms \nin which a sync object, implemented by a counter, is associated with each memory operation. Examples \noft hese are given in Section 6. Split-C also provides a store operation, which is a vari\u00adant of the \nput operation. A store operation generates a write to a remote memory location, but does not acknowl\u00adedge \nwhen the write operation completes. It exposes the ef\u00adficiency of one-way communication in those cases \nwhere the communication pattern is well understood. By transforming a put to a store, we not only reduce \nnetwork contention by 197 E, Ez Possible S System Contract 1 Gwen a shared memory program, a B correct \nmachine must produce only sequentially consistent Write X executions }or that program. Read Y 0Read Y \nWrite Y m Read X =[a Write Y Read Z Read Z Read X H Figure 2: The figure shows ashared memory execution \nand one possible total order on operations. Executions in which the W~ite to Y appears to happen before \nthe Write to X would not be legal. reducing the number of packets but also reduce the process\u00ading time \nspent by the processors in generating and handling the acknowledgement traffic. The source language differs \nfrom Split-C in two key as\u00adpects. First, the source language does not provide split\u00adphase operations; \nall accesses to shared memory are block\u00ading. This design choice stems from the observation that split-phase \noperations are good for performance but hard to use. Second, the global address space abstraction is \npro\u00advided in the source language only through a distributed ar\u00adray construct and through shared scalar \nvalues; the global address space cannot be accessed through global pointers, which are supported by Split-C. \nDisallowing global pointers allows us to implement our analysis techniques without full\u00adblown pointer \nalias analysis. However, there are no restric\u00ad tions imposed on the use of pointers into the local address \nspace. The type system prevents the creation of local point\u00aders into the global address space, and this \nallows us to ignore local memory accesses and local pointers in our analysis. Our source language design \nallows us to write meaningful parallel programs with optimized local computational ker\u00adnels and global \nphases that communicate using either shared variables or distributed arrays. Sequential Consistency A \nparallel execution E on n processors is given by n se\u00adquences of instruction executions El, . . . . En. \nWe consider two types of executions: a shared memory execution allows only atomic reads and writes to \nshared variables; a weak memory execution may contain split-phase as well as atomic operations. Given \na processor execution E, = U1, . . . . am, we asso\u00ad ciate with E, a total order al < az < ... < an. In \na shared memory execution, reads and writes happen atomically in the order they are issued, with no predetermined \nordering between accesses issued by different processors. Therefore, the program executzon order, E, \nis the union of these E, s. An execution E is sequentially consistent if there exists a total order S \nof the operations in E, i.e., E C S, such that S is a correct sequential execution where the reads must \nre\u00adturn the value of the most recent preceding write [12]. For example, in Figure 2, if the read to 1 \nreturns a new value written by El, then the read of X must also return the value written by El 1. ] This \nprogram might come from a case k which Y IS acting as a (presence flag for the value being wr]tten mto \nX This contract does not specify the behavior of programs with put and get or other non-blocklng memory \noperations. In order to extend the system contract for programs with weak memory accesses, rather than \nrelying on a particular instruction set with non-blocking memory operations and synchronizing accesses, \nwe use a more general framework proposed by Shasha and Snir [18], A delay set D specifies some pairs \nof memory accesses as being ordered, which says that the second operation must be delayed until the first \none is complete. For example, in Figure 3, El specifies the accesses issued by a processor, and D1 specifies \nthe delay constraints for executing the accesses. A sync operation, which is one particular mechanism \nfor expressing delay con\u00adstraints, could be introduced to prevent the get operation from being initiated \nbefore the puts complete. In general, given an execution E, D is a subset of the ordering given by E, \ni.e., D L E. A weak memory execution given by E and D is weakly consistent if there exists a total order \nS of the accesses in D, i.e., D q S, such that S is a correct sequential execution. System Contract 2 \nGiven a weak memory program, a cor\u00ad rect machine must produce only weakly consistent executions for that \nprogram. E is a dynamic notion based on a particular execution. During compilation, we approximate E \nby the program order P, defined as the transitive closure of the n program control flow graphs, one per \nprocessor. The compiler computes a delay set D, which is a subset of P. We say that a delay set D is \nszsficzent for P if, on any machine that satisfies the second system contract, all possible executions \nof P are sequentially consistent. Note that if we take D to be P, which means that we block on every \nremote memory access, it forces our machine to produce a sequentially consistent execution. Our goal \nduring program analysis is to find a much smaller D that still ensures sequential consistency. 4 Shasha \nand Snir s Algorithm A violation of sequential consistency occurs when the (hap\u00adpens before relation, \nwhich is E U S, contains a cycle. An example is shown in Figure 1. In this case, the figure-eight formed \nby the arrows is the cycle that violates sequential consistency. All violations are due to such cycles, \nalthough in general the cycles may extend over more than two proces\u00adsors and involve as many as 2n accesses. \nThe cross-processor edges in the cycles are conflicting (read-write or write-write) accesses to the same \nvariable by two different processors. We define the conflict set C to be a conservative approxima\u00adtion \nto these interferences: C contains all unordered pairs of shared memory operations al, aZ, such that \nal and az are issued by different processors, both access (or could ac\u00adcess) the same shared variable, \nand at least one is a write operation. Shasha and Snir proved that there exists a minimum delay set, \nDs&#38;s, that can be defined by considering cycles in P u C. The primary idea is that if P U C does \nnot contain any cycles (as in Figure 4), then EuS would not contain any cycles since P U C is a conservative \ncompile-time superset of E U S. We reformulate their result with the following definitions. 198 Figure \n3: A weak memory execution with D containing, some may execute in either order. However, an execution \nin which would be illegal. Definition 1 /ipath[al, . . ..u~]~P uCtsasimplep&#38;th. if for any accessa~ \nin the path, i~ah is an access on processor Pk, then the following hold: 1. Ifa,+l is also on P~l then \nfor all other accesse:~ al (~#~andj#i-Fl), aJ zsnoton pk. 2. If at-l anda,+l exist (i # 1 andi # n) \nanrl[a,-1, w] c C and [a,, a,+l] E C, then for all j # i, al is not on Pk.  Thus, except for the end-points \nof the path, a simple path is one that visits each processor at most once, with at most two accesses \nper processor during a visit. A special case of simple paths points to a potential violation of sequential \nconsistency. Definition 2 Given an edge [am, al] in some Pk, a path [a~,... , am] E P UC is called a \nback-path, for [am, al] if [al, . . . . am] is a simple path. Shasha and Snir define a particular delay \nset, denoted here Dsa ~, which is sufficient and in some sense minimal. Definition 3 DSaS = {[a,, aj] \nc Pl[a,, a~] has a back-path tn PU C}. Theorem 1 [18] DSkS is suflcient. The minimaMy results on DS&#38;S \nsays that given straight\u00adline code without explicit synchronization, if a pair of ac\u00adcesses in Ds&#38;s \nis allowed to execute out of order (i.e., is omitted from the delay set when the program is run), there \nexists a weakly consistent execution of that program that is not sequentially consistent. This notion \nof minim alit y is not as strong as one would like, because it ignores the ex\u00adistence of control structures \nand synchronization constructs that prevent certain access patterns. In the section that jfol\u00ad10WS, we \nextend this framework to include synchronization analysis, which is critical in reducing spurious delay \ned,ges in real parallel programs. lnitlally Data= Flag=O Proc: 1 Proc: 2 I Read , Data 1 J J Write Flag=l \nRead Flag 1 Figure 4: Example of a parallel program that does not re\u00adquire any delay constraints. Put \nx Get Z Get X R of the edges in E. Since the puts in El are not ordered, they the get of Z appears to \nhappen before either one of the puts 5 Using Synchronization Information The delay set computation described \nin the previous section does not analyze synchronization constructs and is there\u00adfore overly conservative. \nIt is correct to treat synchroniza\u00adtion constructs as simply conflicting memory accesses, but this ignores \na valuable source of information, since synchro\u00adnization creates mutual exclusion or precedence constraints \non accesses executed by different processors. For example, Figure 5 shows two program segments that access \nthe vari\u00adables X and Y. The delay set DS&#38;S contains edges between these access, prohibiting overlap. \nHowever, if synchroniza\u00adtion behavior is taken into account, the delays are seen to be unnecessary, because \nthe post will be delayed for the writes to complete and the reads cannot be started until the wait completes. \nIn this section, we modify the delay set computation to incorporate synchronization analysis for three \nconstructs: post-waits, barriers, and locks. Our synchronization analy\u00adsis presumes that synchronization \nconstructs can be matched across processors, and we describe runtime to ensure this dy\u00adnamically. This \nanalysis only helps if the programmer uses the synchronization primitives provided by the language. If \nthe programmer builds synchronization using reads and writes, we would not be able to detect the synchronization. \nIn this case our algorithm is still correct, but we would not be able to prune the delay set. 5.1 Analyzing \nPost-Wait Synchronization Post-wait synchronization is used for producer-consumer de\u00adpendencies. The \nconsumer executes a wait, which blocks un\u00adtil the producer executes a post. This creates a strict prece\u00addence \nbetween the operations before the post and the opera\u00adtions after the wait2. We start by considering two \nexamples that use post-wait synchronization, and then present the modified delay set construction. In \nour discussion, we use a precedence relation R that captures the happens-before property of accesses. \nDefinition 4 A precedence relation R is a set of ordered pairs of accesses, [al, az], such that al is \nguaranteed to com\u00adplete before az is initiated. Consider the computation of the delay set for the pro\u00adgram \nin Figure 5. DS&#38;S is {[a,, a,], [a,, as], [a,, m], [a., a.], [a~, a~], [a,, a~]}, which will force \ncompletion of al before the 2 In our analysls, we assume that Lt k illegal to post more than once on \nan event variable. 199 al Write X Wait F a4 a2 Write Y Read Y a5 o o a3 Post F ReadX a6 o o  oBEo \nFigure 5: The post-wait synch~onization enables the order\u00ading of conflict edges, which results in fewer \nback-paths. initiation of a2 and as before @. The semantics of post-wait synchronization require a precedence \nedge from aa to a4, which eliminates one direction of the conflict edge between as and al and leads to \na smaller delay set. Starting with the delay set {[a2, as], [al, as], [a4, as], [a4, aG]}, we can order \nthe other conflict edges [al, aG] and [U2, as] by transitivity, and thus destroy the remaining back-paths. \nAs this example illustrates, the delay set and precedence relation will be computed through a process \nof refinement. Initially, the precedence relation contains only those edges that directly link a post \nand a wait. We then create an ini\u00adtial delay set D1 with those edges from DSXS that involve at least \none synchronization construct. This says that some de\u00adlay edges those involving synchronization are more \nfun\u00addamental than others. Once D1 is computed, the prece\u00addence relation R is expanded to include the \ntransitive clo\u00adsure of itself and D1. The example provides two key insights into how we could use synchronization \ninformation. First, by providing a directionality to a conflict edge, we impose more restrictions on \nthe interleaving of accesses from differ\u00adent processors, which results in a smaller delay set. Second, \nthe precedence relation R serves as the catalyst for initiating the refinement process. The process \nof invalidating a back-path does not always involve directing a conflict edge. R could be used for re\u00admoving \ncertain accesses that are not qualified to appear in back-paths, and thus decrease the numbe~ of back-paths \nthat we discover. In Figure 6, there are simple paths from a3 to al and from (&#38;j to al. Furthermore, \nas and al are synchronization accesses, so [al, as] and [a4, aG] belong to the initial delay set DI. \nThis information, when combined with the precedence edge [U3, al], implies that al p~ecedes ae for any \nexecution of the program. Since a simple-path to al corresponds to a runtime execution where all the \nac\u00adcesses in the sequence execute before al, a~ will neve~ occur in a simple-path to al. We therefore \nremove a~ while de\u00adtermining the existence of simple-paths to al. Removal of a6 destroys the simple-path \nfrom az to U1, which otherwise al o az o a3 [+/ Post F o Figure 6: An example where synchronization \nanalysis dis\u00adqualifies certain accesses from appearing in back-paths. void fooo ( barr~ero ; for (i=O; \nl<n; i++) { bi~riero ; .. barriero ; } Figure 7: Inaccuracies in analysis of barrier statements would \nhave resulted in [al, az] being added to the delay set. Using these examples as motivation, we propose \na gen\u00aderal scheme for finding a delay set. We initially compute the delay set D1, which relates synchronizing \naccesses to non\u00adsynchronizing accesses, and combine it with direct prece\u00addenceedges toobtain complete \nprecedence information. For post-wait operations, this combining process requires the dominator tree \nof the control flow graph. A node uis said to dominatea node v if u appears on every path from the entry \nnode of the graph to u. (Domination information is efficiently represented using a dominator-tree, which \nstores only the closest dominators. ) We now present the modified algorithm for computing the delay set. \n1 Compute the dominator tree. 2 d, Compute initial delay restrictions DI by restricting the simple-path \nalgorithm from the previous section to pairs that include one synchronization access. 3. Compute the \nset of precedence edges, RI, between matching post and wait constructs. 4. For every pair of access \nstatements al and az, check whether there exists two other statements bl and b2 that satisfy the following \nconstraints.  (a) U1 dominates bl and bz dominates az, (b) [al, bl]CDland [bQ, a,] ED~, and (C) [b,,bz]<R, \n Add [al, a2] to Rif bl and bz exist. 5. The original conflict set C contained unordered pairs. Order \nthe pairs that have a precedence as follows: Let Cl= C {[az, al] l[al, a2]: R}. 6. Let D= DIU{[a,, aj]~Pl \n[a,, aJ]hasaback-pathin Pucl}.  By eliminating accesses and ordering conflict edges be\u00adfore checking \nfor back-paths, wereduce thenumber of back\u00adpaths that are discovered. There is a corresponding decrease \nin the size of the delay set, which results in improvements in execution times of the programs. 5.2 Analyzing \nBarrier Synchronization Barrier statements can be used to separate the program into different phases \nthat do not execute concurrently. The analysis for barriers is similar to that of post-wait synchro\u00adnization, \nsince crossing a barrier introduces a precedence re\u00adlation. As before, we add the delay edges between \naccesses and barriers before compute the delay set for the rest of the program. 200 To use barriers for \ncomputing precedence, we need to line up the barriers, which is undecidable in general. Fig\u00adure 7 shows \na sample program whereit is likely that the bar\u00adr~er statements do line up during program execution, \nbut to prove that assertion at compile-time, our analysis needs to prove that the function ~oo gets called \nby all the processors at the same time and the loop inside the function executes the same number of iterations. \nRather than adding sophis\u00adticated analysis to Iine up barriers [10], we use a simple run\u00adtime solution \nthat works well for many real programs, We add a run-time check to each barrier to determine whether \nthese are the ones lined up during compilation. The com\u00adpiler produces two copies of the code, one with \npipelining optimizations and the other without any optimizations, If the processors are indeed synchronized \nand executing the barrier operations as predicted, the optimized version of the code is run. This approach \nto analyzing barriers also allows us to overcome separate compilation issues for many lceal programs. \n 5.3 Lock Based Synchronization We can extend our synchronization analysis to locks, even though there \nare no strict precedence relations implied by the use of locks. We again compute D1 for pairs of accesses \nthat include a synchronization construct. We then deter\u00admine the set of accesses guarded by a lock. An \naccess a is said to be guarded by the lock 1, if the following conditions hold: 1. a is dominated by \na lock 1 operation (which we will call bl ), and there are no intervening unlock 1 operations. 2. a \ndominates unlock 1 operation, which we will call hz. 3. [ZIl,a] c DI and [a,bz] < DI  If access statements \naI and az are guarded by the lc)ck 1, we remove all other access statements that are guarc~ed by the \nsame lock before checking for a simple-path from az to al. This is a valid operation by the following \nreasoning. Ifa2, hl, bz, ..., bk, al is a simple-path, then the accesses cor\u00adresponding to bl, b2, . \n. . . bk must occur after az and before aI. It follows from our definition of being guarded by a Iclck \nthat none of bl, b2, . . . . b~ can be guarded by the same lc,ck and still appear in a violation sequence. \nThis improvement to the delay set construction allows accesses within critical regions to be overlapped. \n6 Code Generation The notion of a delay set can be used to generate code for a variety of memory models, \none of which is the put, get model provided by Split-C. In this section we describe our source level \ntransformer for Split -C, which uses cycle det ec\u00adtion and synchronization analysis. The input to the \ncode generation phase is the control flow graph, the delay set computed by the back-path recc,g\u00adnition \nalgorithm, and the use-defgraph for each processor s variable access (obtained through standard sequential \ncom\u00adpiler analysis). As we mentioned in section 4, to compute the delay set, we can use a conservative \napproximation to the conflict set C without affecting the correctness of our analysis. This is crucial \nto our analysis since there are in\u00ad herent inaccuracies in the array analysis that is required to generate \nconflict edges for array accesses. During the code generation process, both the delay con\u00adstraints and \nlocal dependencies must be observed. The gen\u00aderated code contains put, get, and store constructs, as \nwell as various types of sync statements. Normally, a sync state\u00adment will force completion of all previous \nputs and gets from the issuing processor. However, Split-C also provides a mechanism called sgnchroniz!ng \ncounter-s to wait for the completion of a subset of outstanding accesses. The pro\u00adgrammer specifies a \ncounter when issuing puts and gets, and again when issuing the sync, which will wait for only those accesses \nwith a matching counter. The first step in code generation is to split remote ac\u00adcesses into an initiation \nand synchronization. A remote read of X into y is transformed into get_ctr(y, X, counter) fol\u00adlowed by \nsync.ctr (counter), where counter is either a new or reused synchronizing counter. This transformation \nis al\u00adways legal, but analysis is needed to move the two opera\u00adtions away from each other, thereby allowing \ncommunication overlap. Separating Initiation from Completion, Figure 8 shows an example of code that \nmight be generated after calculating delay and clef-use edges. The synchronization of the get on X has \nbeen moved away from the initiation, but because of the conditional, two synchronizing points are needed. \nThe duplication may result in unnecessary overhead, but is legal because sync-cntr operations are idempotent, \nThe algorithm for moving a sync-ctr operation a~g~~ away from its corresponding init i at ion a,n, ~ \ninvolves repeated applications of the following rules: 1. If a~ync is at the end of a basic block, propagate \nCZ.yn. to all the successors of the basic block and continue the motion on each copy of a,g~~. 2. If \nasu~c is in the middle of a basic block, let a be the  .. operation the immediately follows it. (a) \nIf there is a delay or clef-use constraint of the form [a,~,,, a ], terminate the movement of a,,~.. \n (b) If a is another copy of the synchronization a~,~~, merge the two a~g~c operations. (c) Otherwise, \nmove a,,~c past a .  The above algorithm moves sync-et r operations as far away from initiation as \npossible. As shown in Figure 8, this may not be the best strategy, since it can lead to multiple synchronization \npoints in the same control path. Similarly, if a sync-ctr is propagated into a loop body, it will be \nex\u00adecnted in every iteration, even though the first execntion is sufficient. Our compiler uses heuristics \nto avoid some of these cases, since data on average executions of control paths and machine parameters \nwould be needed to choose the best placement. One-Way Communication Optimization. Another trans\u00adformation \nthat provides large performance benefits on some machines is to transform two-way communication into \none\u00adway communication. The put operation has an acknowl\u00adedgement that signals the completion of the operation. \nIf there are no delay constraints that require the completion of a put access, we can transform the access \ninto a one-way operation, called a store. One way to ensure completion of stores is with a global synchronization \npoint that stalls until all stores across the machine have completed. This transformation is usually \nvalid if the sync-ctr operation propagates all the way to a barrier synchronization. 201 get_ctr(&#38;x, \n&#38;X, ctrl) x=x y.z :Y=2 if (fooo) { if (fooo) { ~ sync_ctr(ctrl) ; % y.x+l y=x+l; } } [ Z=l sync_ctr(ctrl) \n; Z=l Source code Target code Fimme 8: Code Generation: Urmer case letters are shared de~ay edge, and \nthe dashed lin~ IS a def-use edge. E, EI barrier Put x Get X barrier z Get X ~ Figure 9: Barrier synch~onization \nguarantees that X is read\u00adonly in the second phase. Figure 10: Xcanbe cachedby Ez since the updates toX \nare guaranteed to be complete. Eliminating Remote Accesses Delay sets are needed for any transformation \nthat involves code motion on explicitly parallel code. In this section we consider a second class of \ntransformations for distributed memory machines, which lead to the elimination of remote accesses through \na kind of common-subexpression elimina\u00adtion. Consider two accesses al and az within a single basic block \nthat aregets tothe same variable X. If X-is not be\u00ading written concurrently, then the second get can \nbe elim\u00adinated. Two examples are shown in Figure 9. In the first case, there is a barrier that marks \nthe transition to.k-being read-only, and in the second, the post-wait synchronization ensures that the \ngets are issued only after the put is com\u00ad plete. The synchromzation analysis described in Section 5 \nidentifies these synchronization regions and orders the con\u00adflict edges between the gets and puts to \nX. Since there will not be a delay edge between the two gets to X, the second one can be eliminated. \nMutually exclusive access is sufficient but not necessary for elimination of repeated gets. It may be \npossible to reuse a previously read vaIue even when there are inter\u00advening global accesses, as long as \nit is legal to move the second get up to the point of the first one. The algorithm used for remote access \nelimination is essentially the reverse variables. lower case letters are local variables. the solid \nline is a of that used for sync-ctr propagation: the second get is moved backwards in the code until \nit reaches a operation that shares a delay edge or local dependence. If this propa\u00adgation is successful, \nwe will end up with a sequence like: get(locall, X, counterl) ... sync. cntr(counterl) get (loca12, X, \ncoumter2) ... sync _cntr(counter2) At this point, the second get is replaced by a local assign\u00adment of \nlocallto loca12, and the second sync-cntriselim\u00adinated, along with any of its copies. The examples presented \nso far eliminate redundant reads, which is similar to saving a value in register. The tech\u00adnique can \nbe applied to a variety of other communication\u00adeliminating optimizations as illustrated in Figure 11. \nFor simplicity, they are shown as transformations on the higher level code, with temporaries introduced \nto minimize con\u00adflicts during code motion. Reading a remote variable that has recently been written can \nbe avoided if the written value is still available. When a thread issues two successive writes to the \nsame variable, the earlier writes can be buffered in a local variable and the final value written to \nthe remote copy at a later point. This is equivalent to using write-back cache, rather than a write-through \ncache. 8 Experimental Results We quantify the benefits of our approach by studying the effect of the \noptimizations on a set of application kernels that use a variety of synchronization mechanisms. A brief \ndescription of the applications is given below: Ocean: This benchmark is from the Splash benchmark suite \n[19], and studies the role of eddy and boundary currents in large-scale ocean movements. The primary \ndata structure is a grid, and the core of this application is a stencti-like computation. Ei?13D: Em3d \nmodels the propagation of electromagnetic waves through objects in three dimensions [14]. The com\u00adputation \nconsists of a series of leapfrog integration steps: on alternate half time steps, changes in the electric \nfield are calculated as a linear function of the neighboring magnetic field values and vice versa. Epithelial \nCell Simulation: Biologists believe that the geometric structure of the embryo emerges from a few sim\u00adple, \nlocal rules of cell movement. This application is a cell aggregation simulation that allows scientists \nto posit such 202 .. Value Reuse Wnte.back Value Propogmon  l Figure 11: A set of transformations \nto eliminate remote accesses that are similar in spirit to standard uniprocessor op\u00ad timization like \ncomm on subexpression elimination. (Jpper case letters denote global v~riables, and lower c~se ones loc-ti \nvariables. 1.0 .75 = Unoptimized .5 = Pipelined communication .25 0 One-way communication . Ocean \nEM3D Epithel Cholesky Health Figure 12: The figure gives theexecution times, normalized sothatthe execution \ntime of thecode generated without analyzing synchronization constructs is 1. Thus, a relative speed of \n0.5 corresponds to a factor of 2 speedup. 35-, I t , r , r rules. At each time-step of the simulation, \na Navier-Stclkes solver calculates the fluid flow over a large grid by perform-30 ing 2-D FFTs. 25 -Cholesky: \nCholesky computes the factors of a symruet\u00adric matrix. The primary data structure is a lower triangu\u00ad20 \n\u00ad lar mat rix, which is distributed in a blocked-cyclic fashion. The computation is structured in a \nproducer-consumer st,yle, 15 and synchronize ation is ensured using post-wait operations 10 \u00ad on flags. \nHealth: This benchmark is from the Presto application 5 suite. Health simulates the Colombian health \nservice sys\u00adtem, which has an hierarchical service-dispensing system. 0 051015202530 3540 Exclusive access \nto shared data structures is guaranteed by Processors the use of locks. The prototype compiler automatically \nintroduces the mes-Figure 13: Speedup curves for the Epithelial application sage pipelining and one-way \ncommunications optimizaticms kernel with varying degrees of optimization. As expected, for all the applications. \nThe execution times of these appli-the optimized versions scale better with processors cations were improved \nby 20-35 YOthrough message-pipelining and one-way communication optimizations. These were mea\u00adsured on \na 64 processor CM-5 multiprocessor. The relative 9 Related Work speedups should be even higher on machines \nwith lower com\u00admunication startup costs or longer relative Iatencies. Fig-Most of the research in optimizing \nparallel programs has ure 12 gives the performance results of these experiments. been for data parallel \nprograms. In the more general con-The base program is the executable generated after apply\u00ad trol parallel \nsetting, Midkiff and Padua [15] describe eleven ing Shasha and Snir s cycle analysis. Our synchronization \ndifferent instances where standard optimizations (like code anal ysis results in much smaller delay sets, \nwhich in turn motion and dead code elimination) cannot be directly ap\u00adenables greater applicability of \nthe message pipelining opti-plied. Analysis for these programs is based the pioneering on mization. work \nby Shasha and Snir [18], which was later extended by As a result of introducing the message pipelining \nopti-Midkiff et al [16] to handle array based accesses. mization, the speedup characteristics of the \nprogram chain ges. We analyze the synchronization[5] accesses in the pro-Figure 13 shows that the efficiency \nof a parallel prOgraLm gram and obtain precedence and mutual exclusion informa\u00adincreases when we transform \nblocking operations by asyn-tion regarding remote accesses. Others have proposed algo\u00ad chronous operations. \nThe increase in efficiency is a direct rithms for analyzing synchronization constructs in the con\u00ad result \nof the reduction in either the time spent waiting for text of framing data-flow equations for parallel \nprograms, remote accesses to complete or the overhead of sending mes-where strict precedence information \nis necessary [4] [8]. Our sages. algorithm for analyzing post-wait synchronization is simi\u00ad lar in spirit; \nhowever, we can also exploit mutual-exclusion 203 info~mation on accesses. Also related search that proposes \nweaker memory approaches change the programmer s gramming conventions under ensured. Our work shifts \nthis to the compiler. Our analysis piling weak memory programs code motion is legal, which is instructions. \nCompilers and runtime guages like HPF and Fortran-D which burden could since critical systems to our \nwork is the re\u00admodels [1, 7]. Those model by giving pro\u00ad sequential consistency is from the programmer \nalso be used for com\u00adit can determine when for generating prefetch for data parallel lan\u00ad [9] implement \nmessage pipelin\u00ad ing optimizations and data re-use. The Parti runtime system and associated HPF compiler \nuses a combination of com\u00adpiler and runtime analysis to optimize communication [3], and these optimizations \nhave also been studied in the con\u00adtext of parallelizing compilers [17]. However, as discussed earlier, \ncompiling data parallel programs is fundamentally different from compiling explicitly parallel programs. \nFirst, in a data parallel program, it is the compiler s responsibility to map parallelism of degree n \n(the size of a data structure) to a machine with PROCS processors, which can sometimes lead to significant \nruntime overhead. Second, the analysis problem for data parallel languages is simpler, because they have \na sequential semantics resulting in only directed conflict edges. Standard data-dependence techniques \ncan be used in data parallel language to determine whether code-motion or pipelining optimizations are \nvalid. 10 Conclusions We presented analyses and optimizations for explicitly par\u00adallel programs on distributed \nmemory multiprocessors. The main optimization is masking latency of remote accesses by message pipelining \nand prefetching. Other optimizations similar to subexpression elimination and constant propaga\u00adtion are \nalso enabled by the analysis. We have a prototype compiler that implements tified the potential payoff \na set of application kernels. are as high as 3,5% on the ment expected on future cation startup. A new \nform of analysis explicitly parallel programs these optimization, and we quan\u00adof a few of these optimizations \non The performance improvements CM-5, with even better improve\u00adarchitectures with lower communi\u00ad called \ncycle detection is needed for in a general (not data-parallel) execution model. The analysis computes \nthe constraints on access order to ensure a sequentially consistent program\u00adming model. We improved on \nthe accuracy of the analysis by using synchronization information. This analysis is im\u00adportant, since \nmost parallel programs reduce the number of conflicting accesses through synchronizing operations. We \nalso show how to use this analysis to generate code for an abstract machine language, Split-C. References \n[I] S. V. Adve and M. D. Hill. Weak Ord.rin~-A New Definition. In 17th International Symposium on ComptiteT \nArchitecture, April 1990. [2] R. Arpaci, D. Culler, A. Krishnamurthy, S. Steinberg, and K. Yelick. Empirical \nEvaluation of the CRAY-T3D: A Com\u00adpiler Perspective. In International Symposium on Computer Architecture, \nJune 1995. [3] H. Berryman, J. Saltz, and J. Scroggs. Execution Time Sup\u00adport for Adaptive Scientific \nAlgorithms on Distributed Mem\u00adory Multiprocessors. Concurrency: Pmctice and Experience, June 1991. [4] \nD. Callahan and J. Subhlok. Static Analysis of Low-level Synchronization. In ACM SIGPLAN and SIGOPS WoTk\u00adshop \non Parallel and Distributed Debugging, May 1988. [5] W. W. Carlson and J. M. Draper. Distributed Data \nAccess in AC. In ACM Symposzum on Prtnczples and Practice of Parallel P.ogrammtng, July 1995. [6] D. \nE. Culler, A. Dusseau, S. C. Goldstein, A. Krishnamurthy, S. Lumetta, T. von Eicken, and K. Yelick. \nParallel Program\u00adming in Split-C. In Supercomp utang 93, Portland, Oregon, November 1993.  [7] K. Gharachorloo, \nD. Lenoskl, J. Laudon, A. Gupta, and J. Hennessy. Memory Consistency and Event Ordering in  Scalable \nShared-Memory Multiprocessors. In 17th Int erna \u00adtional , $ymposzum on Computer Ar chttecture, 1990. \n[8] D. Grunwald and H. Srinivasan. Data flow equations for Explicitly Parallel Programs. In ACM Symposium \non Prtn\u00adcaples and Practices of PaTallel Programming, June 1993. [9] S. Hiranandani, K. Kennedy, and \nC.-W. Tseng. Compiler Optimziations for Fortran D on MIMD Distributed-Memory Machines, In Proceedings \no.f the 1991 International Confer\u00adence on Supercomptiting, 1991. 1o] T. E. Jeremiassen and S. J. Eggers. \nReducing False Sharing on Shared Memory Multiprocessors through Compile Time Data Transformations. In \nACM Symposium on Principles and Practice of Parallel Programming, July 1995. 11] A. Krishnamurthy and \nK. Yelick. Optimizing Parallel SPMD Programs. In Languages and Compilers jor Parallel Com\u00ad puting, 1994. \n [12] L. Lamport. How to Make a Correctly Executes Multiprocess tions on Computers, C-28(9), [13] D. \nLenoski, J. Hennessy. tocol for the Symposaum  [14] N. K. Madsen. tegral Methods Orthogonal J. Laudon, \nK. Multiprocessor Computer that Programs. IEEE Transac-September 1979. Gharachorloo, A. Gupta, and The \nDirectory-Based Cache Coherence Pro-DASH Multiprocessor. In 17th Internattona[ on Computer Architectu~e, \nMay 1990. Divergence Preserving Discrete Surface In\u00adfor Maxwell s Curl Equations Using Non- Unstructured \nGrids. RIACS, February 1992. [15] S. Midkiff and D. Padua. Issues allel Programs. In Inter-national \ncessing -Vol II, 1990. [16] S. P. Midkiff, D. Padua, and R. Technical Report 92.04, in the Optimization \nof Par-Conference on Parallel Pro- G. Cytron. Compiling Pro\u00ad grams with User Parallelism. In Languages \nand Compiler-s for Parallel Computing, 1990. [17] A. Rogers and K. Pingali. Compiling for dktributed \nmem\u00adory architectures. IEEE Transactions on Parallel and Dis\u00adtributed Systems, March 1994. [18] D. Shasha \nand M. Snir. Efficient and Correct Execution of Parallel Programs that Share Memory. ACM Transactions \non Programming Languages and Systems, 10(2), April 1988. [19] J. P. Singh, W. D. Weber, and A. Gupta. \nSPLASH: Stanford parallel applications for shared memory. Computer Archi\u00adtecture News, March 1992. [20] \nThe SPARC Architecture Manual: Version 8. Spare Inter\u00adnational, Inc., 1992. [21] T. von Eicken, D. E. \nCuller, S. C. Goldstein, and K. E. Schauser. Active Messages: a Mechanism for Integrated Communication \nand Computation. In International Sym\u00adposium on ComputeT Architecture, May 1992. 204 \n\t\t\t", "proc_id": "207110", "abstract": "<p>We present compiler analyses and optimizations for explicitly parallel programs that communicate through a shared address space. Any type of code motion on explicitly parallel programs requires a new kind of analysis to ensure that operations reordered on one processor cannot be observed by another. The analysis, based on work by Shasha and Snir, checks for cycles among interfering accesses. We improve the accuracy of their analysis by using additional information from post-wait synchronization, barriers, and locks.</p><p>We demonstrate the use of this analysis by optimizing remote access on distributed memory machines. The optimizations include <italic>message pipelining</italic>, to allow multiple outstanding remote memory operations, conversion of two-way to one-way communication, and elimination of communication through data re-use. The performance improvements are as high as 20-35% for programs running on a CM-5 multiprocessor using the Split-C language as a global address layer.</p>", "authors": [{"name": "Arvind Krishnamurthy", "author_profile_id": "81100373233", "affiliation": "Computer Science Division, University of California, Berkeley", "person_id": "PP40026572", "email_address": "", "orcid_id": ""}, {"name": "Katherine Yelick", "author_profile_id": "81100059756", "affiliation": "Computer Science Division, University of California, Berkeley", "person_id": "P157665", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/207110.207142", "year": "1995", "article_id": "207142", "conference": "PLDI", "title": "Optimizing parallel programs with explicit synchronization", "url": "http://dl.acm.org/citation.cfm?id=207142"}