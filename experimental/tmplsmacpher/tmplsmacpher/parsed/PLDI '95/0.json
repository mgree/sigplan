{"article_publication_date": "06-01-1995", "fulltext": "\n Efficient Context-Sensitive Pointer Analysis for C Programs Robert P. Wilson and Monica S. Lam Computer \nSystems Laboratory Stanford University, CA 94305 http: //suif. Stanford. edu {bwilson, lam}@cs. Stanford. \nedu Abstract This paper proposes an efficient technique for con~ext\u00adsensitive pointer analysis that \nis applicable to real C pro\u00adgrams. For efficiency, we summarize the effects of pro\u00adcedures using partial \ntran$er func(ions. A partial transfer function (PTF) describes the behavior of a procedure assum\u00ading \nthat certain alias relationships hold when it is called. We cart reuse aPTF in many calling contexts \nas long as the aliases among the inputs to the procedure are the same. Our empiri\u00adcal results demonstrate \nthat this technique is successful a single PTF per procedure is usually sufficient to obtain com\u00adpletely \ncontext-sensitive results. Because many C programs use features such as type casts and pointer arithmetic \nto cir\u00adcumvent the high-level type system, our algorithm is based on a low-level representation of memory \nlocations that safely handles all the features of C. We have implemented our algo\u00adrithm in the SUIF compiler \nsystem and we show that it runs efficiently for a set of C benchmarks.  Introduction Pointer analysis \npromises significant benefits for optimizing and parallelizing compilers, yet despite much recent progress \nit has not advanced beyond the research stage. Several prob\u00adlems remain to be solved before it can become \na practical tool. First, the analysis must be efficient without sacrificing the accuracy of the results. \nSecond, pointer analysis algo\u00adrithms must handle real C programs. If an analysis only provides correct \nresults for well-behaved input programs, it will not be widely used. We have developed a pointer analysis \nalgorithm that addresses these issues. This research was suppcmed in part by ARPA contract DABT63-94-C\u00ad \n0054, an NSF Young Investigator award, and an Intel Foundation graduate fetIowship. Permission to copy \nwithout fee all or parl of this material is granted provided that the copies are not made or distributed \nfor direct commercial advantage, the ACM copyright notice and the title of the publication and its date \nappear, and notice is given that copying is by permission of the Association of Computing Machinery,To \ncopy otherwise, or to republish, requires a fee and/or specific permission. SIGPLAN 95La Jolla, CA USA \n@ 1995 ACM 0-89791 -697-2/95/0006 ...$3.50 The goal of our analysis is to identify the potential values \nof the pointers at each statement in a program. We represent that information usingpoints-to functions. \nReconsider heap\u00adallocated data structures as well as global and stack variables, but we do not attempt \nto analyze the relationships between individual elements of recursive data structures. Interproccdttral \nanalysis is crucial for accurately identify\u00ad ing pointer values. Only very conservative estimates are \npos\u00adsible by analyzing each procedure in isolation. One straight\u00adforward approach is to combine all the \nprocedures into a single control flow graph, adding edges for calls and returns. An iterative data-flow \nanalysis using such a graph is relatively simple but suffers from the problem of unrealizable paths, \nThat is, values can propagate from one call site, through the callee procedure, and back to a different \ncall site. Some algorithms attempt to avoid unrealizable paths by tagging the pointer information with \nabstractions of the calling con\u00adtexts [2, 12]. However, these algorithms still inappropriately combine \nsome information from different contexts. Emami et al. have proposed a context-sensitive algorithm that \ncompletely reanalyzes a procedure for each of its calling contexts [6]. This not only prevents values \nfrom propagating along unrealizable paths, but also guarantees that the analysis of a procedure in one \ncalling context is completely indepen\u00addent of alll the other contexts. A procedure may behave quite differently \nin each context due to aliases among its inputs, and a context-sensitive analysis keeps those behaviors \nsepa\u00adrate. Reanalyzing for every calling context is only practical for small programs. For larger programs, \nthe exponential cost quickly becomes prohibitive. Interval analysis, which has been successfully used \nto art\u00adalyze side effects for scalar and array variables in Fortran programs [7, 10], is an approach \nthat combines context sen\u00adsitivity and efficiency. This technique summarizes the effects of a procedure \nby a transferfunction. For each call site where the procedure is invoked, it computes the effects of \nthe pro\u00adcedure by applying the transfer function to the specific input parameters at the call site. This \nprovides context sensitivity without reanalyzing at every call site. Interval analysis relies on being \nable to concisely sum\u00admarize the effects of the procedures. Unfortunate y, pointer analysis is not amenable \nto succinct summarization. The ef\u00adfects of a procedure may depend heavily on the aliases that hold when \nit is called. Thus, the evaluation of a transfer func\u00adtion that summarizes the pointer assignments in \na procedure may be no simpler than running an iterative algorithm over the original procedure. We propose \na new technique that is completely context\u00adsensitive yet does not sacrifice efficiency. Our approach \nis based on the insight that procedures are typically called with the same aliases among their inputs. \nThus it is not necessary to completely summarize a procedure for all the potential aliases but only for \nthose that occur in the program. Our idea is to generate incomplete transfer functions that only cover \nthe input conditions that exist in the program. These incom\u00adplete transfer functions are made up of simple \npartial transfer functions (PTFs) that are only applicable to calling contexts that exhibit certain alias \nrelationships. We have developed an efficient technique that isolates the set of relevant aliases upon \nwhich a FTF definition is based. Our analysis embraces all the inelegant features of the C language that \nare hard to analyze but are commonly used. It safely handles arbitrary type casts, unions, and pointer \narithmetic. It also assumes that pointers maybe stored in any memory locations, regardless of the types \ndeclared for those locations. Since some of the standard library functions may change the values of pointers, \nwe provide the analysis with a summary of the potentitd pointer assignments in each library function. \nWe have implemented our algorithm in the SUIF compiler system and have measured the anatysis times for \na set of benchmark programs. Our empirical results show that our technique is successful in that it often \nneeds to generate only one PTF for each procedure in the program. This paper is organized as follows. \nWe first introduce the major concepts behind our approach and give an outline of our algorithm in Section \n2.1. We then describe our representation of memory locations and pointer values in Section 3. Next in \nSection 4, we explain the intraprocedural portion of our algorithm. Section 5 then describes how we compute \nthe effects of procedure calls. Finally, we discuss related work in Section 6 and present our experimental \nresults in Section 7. 2 Major Concepts This section describes the major concepts in the design of our \npointer analysis algorithm. We first introduce the general approach of using partial transfer functions \nas an efficient means 10 provide context sensitivity. We next describe the specific design of partial \ntransfer functions that we use in our algorithm. Finally, we provide a complete outline of our algorithm. \n 2.1 Partial Transfer Functions To provide some insight on how one might define transfer functions forpointeranalysis, \nconsider an informal summary f(p,q,r) { *P =*q; *q =*r; p int x,y, z; int *xO, *yO, *zO; main () ( \nXo=&#38;x; yo =&#38;y; 20=&#38;z; if (testl) S1 : f(&#38;xo, &#38;yo, &#38;z O); else if (test2) S2: \nf(&#38;zo, &#38;xo, &#38;y O); else S3 : f(&#38;xo, &#38;yo, &#38;x O); Figure 1: Example Program for \nthe very simple procedure f in Figure 1: The target of p points to whatever the target of q initially \npointed to. Case I If r and p do not point to any of the same locations, the target of q points to whatever \nthe target of r initially pointed to. Case II If r and p definitely point to exactly the same location, \nthen the target of q retains its original value. Case III If r and p may point to the same lo\u00ad cations \nbut their targets are not definitely the same, then the target of q may either retain its original value \nor point to whatever the target of r pointed to initially. This example illustrates two important points. \nFirst, the aliases among the inputs to a procedure determine its behav\u00adior. Given a particular set of \naliases, summarizing a procedure is relatively easy. Second, even for this simple two-statement procedure \nthe complete summary is fairly complex. Comput\u00ad ing a full summary for a procedure with cycles and recursive \ndata structures is prohibitively expensive. The main idea behind our algorithm is that we need not summarize \na procedure for all possible aliases among its inputs. Instead, we only need to find summaries that apply \nto the specific alias patterns that occur in the program. For our simple example, Case I applies to the \ncalls ats 1 ands 2 because they have the same alias pattern, even though they have totally different \nparameters. Case II applies to the call at S3. Thus it is not necessary to consider Case III for this \nparticular program. Our basic approach and its comparison with traditional interval analysis are illustrated \nin Figure 2. The traditional approach is to define a complete transfer function that maps the entire \ninput domain for a procedure to the corresponding outputs (Figure 2(a)). Instead, we develop a set of \npartial 7 Figure 2: (a) Transfer function fora procedure and (b) partiat transfer functions transfer \nfunctions (PTFs), each of which is applicable to only a subset of the input domain. As shown in Figure \n2(b), the domains of these partial functions are disjoint and the union of their domains does not necessarily \ncover all the possible inputs. Many potential inputs never occur in practice, and we only need PTFs for \nthe inputs that actuatly occur. That means the complexity of all the PTFs taken together is much lower \nthan that of the full transfer function,  2.2 Design of PTFs There is a trade-off between the complexity \nof the individ\u00adual PTFs and their applicability. By making a PTF more complicated, we can increase the \nsize of its input domain so that fewer PTFs need to be computed. The complete transfer function, which \ncovers all the possible inputs, is at one end of this trade-off. At the other extreme, each point in \nthe input space can have a separate PTF. One of these PTFs can only be reused when all of its inputs \nhave exactly the same values as in the context where it was created. The initial values specify the input \ndomain for the PTF. Whenever the analysis encounters another call to the procedure with the same input \nvalues, it can reuse the final values recorded in the PTF. This technique is commonly known as memorization. \nSince it is not the specific input values but their alias pat\u00adterns that determine the behavior of a \nprocedure, we use symbolic names called extended parameters to abstract the input values. An extended \nparameter represents the locations reached through an input pointer at the beginning of a pro\u00adcedure. \nEvery object is represented by at most one extended parameter. This is similar to the invisible variables \ndefined by Emami et al. [6]. For procedure f in Figure 1, we use the extended parameter l.p to represent \nthe location initially pointed to by p; I.p represents XO for the call ats 1 and z O for the call ats \n2. For the calls at s 1 and S2 in Figure 1, since none of the inputs are aliased, we create a new extended \nparameter for every location accessed. For the call at S3 on the other hand, both p and r point to the \nsame location, so we create only one extended parameter to represent the common location. When a pointer \nto a globat is passed into a procedure, we treat it as any other extended parameter. We do not necessarily \nwant to know the specific vatue of the pointer. For example, for the call at s 1 in Figure 1, the parameter \nl-p represents the global variable XO. If we used XO directly instead of the parameter, we would not \nbe able to reuse the same PTF for the call at S2. Since extended parameters represent global variables \nreferenced through pointers, we also use extended parameters to represent global variables that are referenced \ndirectly. If a global is referenced both directly and through a pointer input to a procedure, using the \nsame extended parameter for both references takes care of the alias between them. It is important to \nonly create the extended parameters that are relevant to the procedure. Aliases involving parameters \nthat are never referenced should not prevent us from reusing FTFs. Our solution is to create the extended \nparameters lazily. We only create extended parameters as they are refer\u00adenced. In contrast, Emami et \nal. create invisible variables for all input pointers that could potentially be accessed. Not only does \nthat require unnecessary overhead to create parameters that are never referenced, but it also limits \nthe opportunities for reuse. Extended parameters play three important roles: 1. Extended parameters make \nup part of the name space of a procedure. Each procedure has its own distinlct name space which consists \nof the extended pa\u00adrameters, local variables, and heap storage allocated by the procedure and its children. \nWe derive a parameter mapping at each call site to map the caller s name space to the cake s and vice \nversa. The initial points-to function for a PTF specifies the input domain. The aliases among the inputs \nto a PTF are the main part of the input domain specification. With our definition of extended parameters, \nthe initial points\u00adto function succinctly captures that information. For the alias in the call ats 3 \nin the example, the initial points-to function records that p and r point to the same extended parameter \nl.p as shown in Figure 4(a). Similarly, the totally disjoint points-to relationships in Figure 3(a) re\u00adflect \nthe lack of aliases for the other calls,   p -0 p--m (a) Initial Values (b) Finat Vatues Figure 3: \nParametrized PTF for Calls at S1 and S2 3. The final points-to function at the procedure exit summarizes \nthe pointer assignments. Given the ini\u00adtial points-to function, it is relatively easy to derive the points-to \nfunction at the procedure exit. Figures 3(b) and 4(b) show the final points-to functions produced for \nthe corresponding inputs. Since the analysis operates . 7 :h : l-p q l_q 2_q q l-q 2_q -0-0% (a) Initiat \nValues (b) Find Vatues Figure 4: Parametrized PTF for Call at S3 on the parametrized name space, the \nfinal points-to function summarizes the procedure parametrically. The parameter mapping for a particular \ncall site allows us to translate the summary back to the caJler s name space. Besides the aliases, the \ninput domain of a PTF is also defined by the values of function pointers passed in and used in calls \nwithin the procedure. The function pointer vatues affect the PTF summary because they determine what \ncode could be executed. The PTF design presented here is but one choice in the trade-off between the \ncomplexity of the PTFs and the sizes of their input domains. We have also explored another scheme that \nuses a separate extended parameter for each access path. That design increases reuse with considerable \ncost in com\u00adplexity, since multiple extended parameters may then refer to the same location. Furthermore, \nit requires additional analy\u00adsis to abstract the potentially infinite access paths to a finite set. Our \nexperience with that scheme suggests that such complexity is unnecessary for this analysis. 2.3 Aigorithm \nOutline Just as we create extended parameters lazily, we only create PTFs as they are needed. In this \nway, we do not compute unnecessary summaries. We begin by using an iterative data\u00adflow approach to find \nthe potential pointer values in the main procedure, When this iterative anatysis encounters a call to \na procedure with new input aliases, it recursively analyzes that procedure for the current context to \nproduce a new F TF. We use a stack to keep track of the current calling contexts, We update the initial \npoints-to functions and parameter mappings lazily during the iterative analysis. When we begin analyzing \na procedure, the initial points-to function is empty and the parameter mapping only records the actual \nvalues for the formal parameters. When we need to know the initiat value of an input pointer and it is \nnot already recorded, we add an entry to the initial points-to function. To check for aliases, we need \nto look up the initial values in the calling context. If the pointer has not yet been referenced in the \ncalling context either, we will add an entry to the caller s initial points-to function. The process \ncontinues recursively up the call graph until the pointer values are known. If the initial values are \nnot aliased with any existing parameters, we create anew extended parameter to represent those values \nand record that in the parameter mapping. Section 3.2 describes the situations where the initial values \nare aliawd with one or more existing parameters. When the iterative analysis encounters a procedure call, \nit needs to find the effects of the call on the points-to function. If the call is through a pointer \npassed as an input to one or more of the PTFs on the call stack, we add the potential values of that \npointer to the specifications of the input domains for those PTFs. For each potential callee procedure, \nwe first check if any of its lTFs apply. This involves building up a panrneter mapping and comparing \nthe input aliases to those recorded for the PTF. If the input aliases and function pointer vahtes match, \nwe use the parameter mapping to translate the final pointer values recorded in the PTF back to the current \ncontext. Otherwise, if the inputs do not match any of the existing PTFs, we reanalyze the procedure to \nproduce a new PTF.  3 Pointer Representations Since C programs commonly access memory using typecasts \nand other features to override the high-level types, our algo\u00adrithm is based on a low-level representation \nof memory. This allows us to handle the problematic features of C in a straight\u00adforward manner. Instead \nof using types to identify locations that may contain pointers, we assume that any memory lo\u00adcation could \npotentially hold a pointer. We also refer to locations within a bIock of memory by their positions, not \nby their field names. We define a new abstraction, the lo\u00adcation set, to specify both a block of memory \nand a set of positions within that block. For the most part, this low-level representation provides results \ncomparable to those based on high-level types. However, our conservative approach may occasionally lead \nto less accurate results. This is a price we are willing to pay in order to guarantee safe results for \nall input programs]. We divide memory into blocks of contiguous storage, whose positions relative to \none another are undefined. A block of memory maybe one of three kinds: a local variable, a locally aJlocated \nheap block, or an extended parameter. Global variables are treated as extended parameters, A spe\u00adcial \nlocal variable represents the return value of a procedure. Note that the heap blocks are only those allocated \nwithin a procedure or its cakes; heap blocks passed in from a calling context are considered to be extended \nparameters, We distinguish heap blocks based on their allocation con\u00adtexts, grouping together all the \nblocks allocated in each con\u00adtext. The minimat context information is simply the state\u00adment that creates \nthe block. Including the call graph edges along which the new blocks are returned, eliminating dupli\u00adcate \nedges in recursive cycles, can provide better precision for some programs [2]. While this scheme is a \ngood starting point, we have found that for some programs it produces far 1We do stilt place a few restrictions \non the inputs. For example, we currentty assume that pointers are not written out to files and then read \nin and later dereferenced. offset Figure 5: Members of a Location Set more heap blocks than we would \nlike. We are currently in\u00advestigating techniques to merge heap blocks that are elements of the same recursive \ndata structure in accordance with our goat to only distinguish complete data structures from one another. \nFor now, we limit the allocation contexts to only in\u00adclude the static allocation sites. That is sufficient \nto provide good precision for the programs we have analyzed so far. 3.1 Location Sets Our goal with regard \nto aggregates is to distinguish between different fields within a structure but not the different ele\u00adments \nof an array. That is, given an array of structures with fields x and y, the locations are partitioned \ninto two sets, one containing all field x data and one containing atl field y data. Our pointer anatysis \ncan be combined with data dependence tests to distinguish between different array elements. We represent \npositions within a block of storage using location sets. A location set is a triple (b, \\,s) where the \nbase b is the name for a block of memory, $ is an offset within that block, ands is the stride. That \nis, it represents the set of locations {~ + is I ic Z} within block b, as shown graphically in Figure \n5. The offsets and strides are measured in bytes. Expression Location Set scalar (scalar, O, O) strttct.F \n(Struct, f, o) array (array, O, O) array [i] (array, O,s) array [i].F (array, f,s) struct.F[i] (Struct, \nf % s, s) *(&#38;P +x) (p, o,1) z Table 1: Location Set Examples f= offset of F ands = array element \nsize  Table 1 shows the location sets that represent various ex\u00adpressions. Afield in a structure is \nidentified by its offset from the beginning of the structure. Except for array references, the stride \nis unused and is set to zero. A reference to an array element has astride equal to the element size. \nIf the elements of art array are structures, then the field information is cap\u00adtured by the offset. Since \nC does not provide array bounds checking, a reference to an array nested withiwa structure can access \nany field of the structure by using out-of-bounds actual values parameters Figure 6: Subsuming Existing \nParameters array indices. Although such out-of-bounds references are rare and non-standard, we believe \nit is still important to han\u00ad dle them conservatively. Thus we treat an array nested in a structure as \nif it completely overlaps the entire structure. This innplies that the offset will always be less than \nthe stride. We enforce that by always computing the offset modulo the stride whenever the stride is non-zero. \nWhen the position of a location within a block is entirely unknown, we set the location set stride to \none. This means that the location set includes all the locations within the blcck. This may occur due \nto pointer arithmetic. Although we reeognize simple pointer increments, which are commonly used to address \narray elements, to determine the location set strides, we do not attempt to evatuate more complex pointer \narithmetic. Instead, for each memory address input to an arithmetic expression, we add to the result \na location set with the same base object but with the stride set to one. This conservatively approximates \nthe results of any arithmetic ex\u00adpression. Because the positions of the blocks relative to one another \nare undefined, we need not worry about pointer arithmetic moving a pointer to a different block. In summary, \nour location set representation has several ad\u00advantages. Problems related to type casts and unions become \nirrelevant because we do not use the high-level types. It is also very easy to work with, especially \nwhen dealing with pointer arithmetic. 3.2 Extended Parameters As discussed in Section 2.2, every object \nis represented by at most one extended parameter. When adding an entry to the initial points-to function, \nwe first find the values of the pointer in the callling context. If the parameter mapping shows that \nan existing parameter already includes the same values, we reuse the old parameter instead of creating \na new one. For simplicity and efficiency, each initial points-to entry points to a single extended parameter. \nIn cases where the initiat values are aliased with more than one parameter, we create anew extended parameter \nthat subsumes all the aliased parameters, and we replace all references to the subsumed parameters. Likewise, \nwhen the initial values are aliased with an existing parameter but also include new values, we subsume \nthe aliased parameter with a new one. Figure 6 illustrates this. Parameter 1.a is created first to represent \nthe targets of a. When b is dereferenced later, we discover that its targets include the value represented \nby 1-abut also another vatuc. Thus, we create a new parameter 1-b that replaces a b a b a valid pointer. \nThe points-to functions record all the valid (X,o,o) (x,8,0) (l_b,-8,0) (l_b,O,O) pointers possibly contained \nin each location, and if there is I I,,.. 1 D only one possibility y, it is safe to assume that is the \nvalue being X.f l_b .... . dereferenced. Thus, we get the benefits of definite pointer aetuat vatues \nparameters Figure 7: Using Negative Offsets for Structures the old parameter la. This scheme reduces \nthe number of extended parameters with some loss of precision. That is an acceptable trade-off for our \npurposes, but subsuming parameters is not essential to our algorithm and could easily be omitted. Aliases \ninvolving fields of structures can be a bit more complicated. When updating the initial points-to ftmction, \nif we find a pointer to a field of an existing parameter, we can easily record that fact. However, if \nwe encounter a pointer to a field before a pointer to the enclosing structure, we will create an extended \nparameter to represent the field, and then the other pointer will have to point before the existing parameter. \nFortunately, our location sets solve this problem nicely. We simply allow the location set offsets to \nbe negative, as shown in Figure 7. Emami solves this problem by always creating parameters for structures \nbefore any other parameters [5], but that cannot work here because we create the parameters as they are \nreferenced.  3.3 Points-to Functions Both the domains and ranges of the points-to functions are expressed \nin terms of location sets. At each statement in a program, a points-to function maps the location sets \ncon\u00adtaining pointers to the locations that maybe reached through those pointers. Thus, a points-to function \nis a mapping from location sets to sets of location sets. It is important for the analysis to know which \nlocations may contain pointers. Since we do not use the high-level types and since the points-to functions \nonly contain entries for pointers that have already been referenced, we record sep\u00adarately for each block \nof memory the location sets that may hold pointers. Without that information, the analysis would have \nto conservatively evaluate every assignment as a poten\u00adtial pointer assignment. Although that would not \naffect the precision of the results (eventually the analysis finds which values may be pointers and removes \nany spurious results), we have found that it makes the analysis very inefficient. For a variety of optimization, \nit is important to know when a location definitely holds a certain pointer value. Within the pointer \nanalysis itself, that information can enable w-ong up\u00addates, where assignments overwrite the previous \ncontents of their destinations. Others have kept track of possible and definite points-to values separately \n[6], but that is unneces\u00adsary for our purposes. We only need that information at the point where a pointer \nis dereferenced. Since we assume that the input is legal, a location being dereferenced must contain \nvalues without the overhead of tracking them separately.  4 Analyzing a Procedure We use an iterative \ndata-flow analysis to find the points-to functions within a procedure. In this section, we only discuss \nthe process of analyzing procedures with no call statements. EvalProc (proc *pr, PTF *ptf ) I* iteratively \nanalyze a procedure / do { changed = FALSE; foreach cfgNode nd in pr. flowGraph { if (no predecessors \nof nd evaluated) continue; if (nd is meet) Eval Meet (rid, ptf) ; if (nd is assign) EvalAssign(nd, ptf) \n; if (nd is call) Eval Call (rid, ptf); } } while (changed) ; ) Figure 8: Intraprocedural Algorithm \n Figure 8 shows our data-flow algorithm. We simply iterate through all the nodes in a procedure s flow \ngraph until none of the points-to values change. We visit the nodes in reverse postorder, because it \nis important to know the input values before evaluating a node. This strategy is much simpler than a \nworklist algorithm, and it handles unstructured control flow with no extra effort. 4.1 Strong Updates \nStrong updates, where assignments overwrite the previous contents of their destinations, are an important \nfactor in the precision of pointer anrdysis. Unless we know that an as\u00adsignment will definitely occur \nand that it assigns to a unique location, we must conservatively assume that that location potentially \nretains its old values. Because strong updates make the node transfer functions non-monotonic, we introduce \nsome constraints on the order in which the flow graph nodes are evaluated in order to guar\u00adantee that \nthe algorithm terminates. Specifically, we never evaluate a node until one of its immediate predecessors \nhas been evaluated, and we never evaluate an assignment until its destination locations are known [1]. \nWe take advantage of the extended parameters to increase the opportunities for strong updates. A strong \nupdate is only possible if the destination of an assignment is a single location set representing a unique \nlocation. A location set is unique 6 if it has no stride and the base represents a unique block of memory. \nThe key is to recognize that an extended parameter representing the initial value of a unique pointer \ncan be a unique block even if that pointer has many possible values in the calling context. Since the \npointer can only contain one of those possibilities at any one time, the extended parameter is a unique \nblock within the scope of the procedure. Only when more than one location points to an extended parameter \nand the actual values for that parameter are not a single unique location must we mark the parameter \nas not unique. This greatly improves our ability to perform strong updates. Since a heap block represents \nall the storage allocated in a particular context, we assume that locally allocated heap blocks are never \nunique. On the other hand, local variables correspond directly to real memory locations so they are always \nunique blocks. 4.2 Sparse Representation Because our analysis is intcrprocedural and needs to keep the \nentire input program in memory at once, we have gone to considerable effort tousestorage space efficiently. \nSince we analyze heap data as well as global and stack variables, many possible memory locations could \nbe included in the points-to functions. Fortunately, the information stored in the points-to functions \nis very sparse. Pointers typically have only a few possible values, so we record the possibilities using \nlinked lists rather than bit vectors. Since the points-to functions usually do not change very much between \ntwo adjacent pro\u00adgram points, we also incorporate the sparse representation described by Chase et al. \n[1]. This scheme only records the points-to values that change at each node. Because of the sparse points-to \nfunction representation, looking up the values of a pointer requires searching back for the most recent \nassignment to that location. Beginning at the current node, we search back through the dominating fllow \ngraph nodesz. If we reach theprocedureentry when searching for an assignment to a formal or extended \nparameter, we compute the value of the initial points-to function for that parameter, if it has not already \nbeen recorded, as described in Section 3.2. This may add new extended parameters in the PTFs on the call \nstack. Since we only search for assignments in the dominating nodes, each meet node must contain SSA \n@functions [3] to identify the values to be assigned in it. We insert these @ functions dynamically as \nnew locations are assigned [1], The pseudo-code for handling a meet node is shown in Figure 9. For each \n#-function, we lookup the points-to values at each predecessor node and combine the results to get the \nnew points-to values. z~qcad of b fldistg skeleton trees [ 1], we just keep lists Of assignments sorted \naccording to a bottom-up traversal of the dominator tree. EvalMeet (c fgNode *rid, PTF *ptf) { /* ite~ate \nthrough the phi-functions */ foreach locSet dst in nd. phi Funcs ( locSet List SKCS; /* COmbine values \nfrom each predecessor / foreach cfgNode pred in nd. preds { J* look up points-to values */ srcs += lookup \n(ptf, dst, pred, NULL); ;* add points-to entry */ assign (ptf, dst, srcs, rid); ) } Figure9: Evaluating \nMeet Nodes  4.3 Evaluating Dereferences Because location sets may overlap, more than one location set \nmay refer to the same memory location. Values assigned to one location set must be observed by references \nto over\u00adlapping locations. Thus, when a pointer is dereferenced, we iterate through all of the overlapping \nlocations and look up in the current points-to function the values that have been assigned to each one. \nHowever, if the location being deref\u00aderenced is a unique location, values assigned to overlapping locations \nmay have been overwritten by a strong update. In that case, we first find the position of the most recent \nstrong update so that the lookup function will not look for assign\u00adments to overlapping locations prior \nto that point. Figure 10 summarizes this part of our algorithm. Eval Deref (locSet *v, cfgNode *rid, \nPTF *ptf ) ( locSet List result; cfgNode *str Upd = NULL; if (v is unique) ( / find the most recent strong \nupdate */ str[Jpd = findStrongUpdate (ptf, v, nd) ; } /* find the locations containing pointers */ locSet. \nList 10CS = v. base. ptrLocations; foreach locSet 10C in 10CS { if (lot overlaps v) { /* find values \nassigned to 10C before node nd but not before strUpd */ result += lookup (ptf, 10C, nd, str Upd) ; return \nresult; } Figure 10: Evaluating Dereferences  4.4 Evaluating Assignments An assignment node in a flow \ngraph specifies both the source and destination expressions, as well as the size of the value to be assigned. \nWhen building the flow graph from the interme\u00addiate representation of a program, we automatically convert \nthe assignments to a points-to form. That is, since a vari\u00adable reference on the right-hand side of an \nassignment refers to the contents of that variable, we add art extra dereference to each expression on \nthe right-hand side. Source and destina\u00adtion expressions may also involve pointer arithmetic. Simple \nincrements are included in the strides of the location sets. We simply keep a list of all the constant \nlocation sets and deref\u00aderence subexpressions found in other arithmetic expressions. The process of evaluating \nan assignment begins by finding the locations identified by the source and destination expres\u00adsions. \nTo evaluate an expression, we iterate through its con\u00adstant location sets and dereference subexpressions. \nConstant locations require no computation. Dereference expressions may include any number of nested dcreferences, \nwhich we evaluate one level at a time. For each destination location, we then update the points-to function \nto include the values from all the potential source locations. Figure 11 shows this process for the case \nwhere the size of the assignment is one word or Iess. EvalA.ssign (cfgNode *rid, PTF ptf) locSetList \ndsts = EvalExpr (rid. dsts, nd, ptf); locSet List srcs = EvalExpr (rid. srcs, nd, ptf); foreach loc$et \ndst in dsts { locSet List newSrcs = srcs; /* include the old values if this is not a strong update */ \nif (dst is not unique) newSrcs += lookup(ptf, dst, nd, NULL) ; assign(ptf, dst, newSrcs, rid); Flgurell: \nEvaluating Assignments In an aggregate assignment, where muItiple words are as\u00adsigned at once, all the \npointer fields from the sources are copied to the destinations. If a source location contains a pointer \nvalue at an offset within the range being copied, we add that pointer value at the corresponding offset \nto the des\u00adtination location. Interprocedural Algorithm We now describe how our algorithm handles procedure \ncalls. To evaluate the effects of a call on the points-to function, we need to find a FTF that applies \nin the calling context. We can either reuse an existing PTF or create a new one. Figure 12 outlines this \npart of our algorithm. Eval Call (c fgNode *rid, PTF *ptf) /* find G record function pointers values \n*/ procList targets = findCall Target s(nd, ptf); foreach proc pr in targets ( paramMap map; recordActuals(nd, \npr, map); PTF *tgtPTE ; if (pr is not on the call Stack) { tgtPTF = Get PTF (map, pr, nd, ptf); if (needVisit) \n{ push (pr, tgtPTF, map) onto callStack; EvalProc(pr, tgtPTF); pop callStack; I ] else { /* handle rec \nr~ive call */ tgtPTF = get PTF from call$tack; /* add new aliases and func ptf values */ updatePTFDomain \n(tgtPTF, map, nd, ptf); if (exit node of tgtPTF not reached) return; /* defer evaluation of nd */ } ApplySummary \n(tgtPTF, map, nd, ptf); Figure 12: Evaluating ProcedureCalls 5.1 Calls Through Pointers The first step \ninevahtatingp rocedurecatls is to determine the target procedures. Formost calls thetarget isaconstant \nand this is a trivial step, but the target may also be specified by a function pointer. Fortunately, \ncaIls through pointers are relatively easy to handle in pointer analysis, since the points-to functions \nrecord the possible pointer vahtes. Functionpointer values maybe expressed intermsofex\u00adtended parameters. \nThis is theonesituation wherewe need to know the specific vahtes represented by extended parame\u00adters. \nWhen artextended parameter isincluded as a potential targetofacall,we checkthepamrneter mappingsupthecall \ngraph until we find thevalues that it represents. Since the functionpointervalues arepartoftheinput domainspecifi\u00adcations \nforthe FTFs, we flag these extended parrtmeters as function pointers and record their values in the FTFs. \n 5.2 Testing ifa PTFApplies The input domain of a FTF is specified by its initiat points\u00adto function \nand by the values of the parameters used as call targets. To test if a PTF applies, we check if these \nthings are the same in the current context as they were when the PIT was created. We check theinitial \npoints-to function entries oneatatime, inthe order in which they were created. We then compare the values \nof the parameters that were used as call targets. If at any point they do not match, we give up and go \nonto thenext PTF. The basic steps of this process are shown in Figure 13. the calling context where eachPTF \nis created. If noneof Get PTF(paramMap *map, proc *pr, cfgNode *rid, PTF *ptf) PTF *home = NULL; / check \nthe existing PTFs / foreach PTF tgtPTF for pr { if (matchPTF(tgtPTF, map, nd, ptf)) { if (inputs have \nnew pointer locations) needVisit = TRUE; return tgtPT,F; } remove all extended parameters from map; /* \ncheck for the original context */ if (tgtPTF.home == (nd,ptf)) home = tgtPTF; } needVisit = TRUE; if \n(home) { updatePTFDomain (home, map, nd, ptf); return home; /* reuse original PTF */ } return allocatePTF(pr); \nFigure 13: Finding an Applicable PTF In the process of comparing the initial points-to function, weatsobuild \nupa parameter mapping. Ifthe PTFmatclhes, we can then use this mapping to apply the PTF in the current \ncontext. For each entry intheinitial points-to function, we find the actual values of thecorresponding \npointer in the current context and add the results to the parameter mapping. Just as when the PTF was \ncreated, this operation may create new initial points-to entries fortheothcr PTFs on the call stack. \nSometimes the aliases and function pointer values fora PTF match, but we need to extend the PTF because \nnew lo\u00adcations contain pointers. As described in Section 3.3, we record the location sets in each block \nof memory that may containpointers. Byallowingusto ignoreoperations thatdo not involve pointers, this \nmakes theanalysis more efficicmt. However, if an input location contains a pointer in the cur\u00adrent calling \ncontext, whereas it did not when the PTF was created, theresults inthe PTF may not recomplete. When thathappens, \nwereanalyze theprocedurc toextend thePTF. Note that theresulting PTFwill still reapplicable toall the \ncallingcontexts in theoriginalinput domain, sinceassuming that more locations may contain pointers is \nonly a matter of efficiency. If none ofthe existing PTFs are applicable, we needto reanalyze the procedure. \nIn general, wccrcate an empty PTF and then revisit the procedure to compute its summary. However, this \nsimple approach may waste alotofstora,ge. Dunngtheiterative analysisofaproced ure,wemayevaluate a call \nnode several times with different input values on each iteration. If we create a new PIT for each set \nof inputs, we will likely end upwitha lotof PTFs that only applyto intermediate results of theiteration. \nOursolution istorecord theexistingPTFs match, butoneof them wascreated inthe currentcontextduring anearlieriteration, \nweupdatethatPTF insteadofcreating anew one. 5.3 Aj@yinga PTF After we find an applicable PTF and a parameter \nmapping for that PTF in the current context, we translate the summary of the procedure back to the calling \ncontext. If the procedure call is through a pointer that has more than one potential vahte, we combine \nall the possible summaries and we do not perform any strong updates when applying them. The points\u00adto \nfunction at the procedure exit summarizes the effects of the entire procedure, so we simply translate \neach points-to entry and add the result to the points-to function at the call site. Local variables do \nnot exist in the calling context so we remove them when translating the points-to entries. 5.4 Recursive \nCalls We use art iterative approach to handling recursive calls. Be\u00adcause of calls through pointers, \nwe must identify the recursive cycles as the anatysis proceeds. Since we already keep a stack to record \nthe calling contexts, we can detect recursive calls by searching back to see if the call target is already \non the stack. Instead of creating a new PTF for a recursive call, we just use the summary from the PTF \nthat is already on the call stack. On the first iteration the summary maybe empty, and we may need to \ndefer evatttation of the recursive call. As long as there is some path that terminates the recursion, \nhowever, an approximate summary will eventually be provided. The iteration then continues until it reaches \na fixpoint. The PTF at the entry to a recursive cycle is an approxima\u00adtion for multiple calling contexts. \nWe combine the aliases and function pointer vahtes from each recursive call site with those recorded \nin the PTF. That may change the input do\u00admain for the PTF so that it no longer matches the original non-recursive \ncalling context. To avoid that problem, we record two separate input domains for these recursive PTFs. \nOne specifies the original input domain for the call from out\u00adside the recursive cycle, and the other \ncombines the inputs from all the recursive calls.  6 Related Work One of the most distinctive features \nof our algorithm is our conservative approach to handling all the features of the Clan\u00adguage. Most of \nthe previous work has made simplifying assumpticms that rule out things such as pointer arithmetic, type \ncasting, union types, out-of-bounds array references, and variable argument lists. Our analysis is based \non a points-to representation similar to the one described by Emami et al. [6], Other work has used alias \npairs. Choi et at. show how alias pairs can be com\u00adpactly represented using a transitive reduction strategy \n[2]. In that compact form, the alias pairs are not much different than a points-to representation. There \nare some differences in precision between using full alias pairs and points-to func\u00adtions, but neither \nis clearly superior [14]. We have found that the points-to function is a compact representation that \nworks well for analyzing C programs. Our scheme for naming heap objects is taken directly from Choi et \nal. Most other work has used k-lim ting, where some arbitrary limit is imposed on the length of pointer \nchains in recursive data structures [11]. Although k-limiting can sometimes provide more information, \nour algorithm is not intended to distinguish the elements of recursive data struc\u00adtures. That problem \nhas been addressed by a number of others [1, 4,8,9, 13]. Using symbolic names to represent the values \nof pointers passed into a procedure was first suggested by Landi and Ryder [12]. They use non-visible \nvariables to represent storage outside the scope of a procedure. Emami ct al. use invisible variables \nfor the same purpose. Our extended parameters are essentirdly the same as invisible variables, except \nthat we choose to subsume parameters so that each initial points-to entry contains a single extended \nparameter. Our work is most similar to the analysis developed by Emami et al. Their algorithm is based \non an invocation graph which has a separate node for each procedure in each calling context. The size \nof an invocation graph is exponential in the depth of the call graph, implying that their algorithm is \nalso exponential. Although our algorithm is still exponential in the worst case where every calling context \nhas different aliases, it performs well for real input programs where the aliases are usually the same. \nWe expect the precision of our results to be comparable to theirs, but there may be some dif\u00adferences. \nOur conservative hartdlingof C occasionally causes us to propagate pointer wdues to more locations. This \nis the price we pay to get safe results for atl inputs. Subsuming aliased parameters also gives up some \nprecision to improve efficiency. On the other hand, our results may be better be\u00adcause we use the extended \nparameters to increase the number of strong updates. Emami et al. mentioned the idea of using a memorization \nscheme similar to our algorithm. Our algorithm extends their design in several important ways. First, \nwe parametrize references to global variables to increase the opportunities for reusing PTFs in different \ncontexts. Second, we keep track of which pointers are actually referenced by a procedure. Thus, we avoid \nthe overhead of creating and updating irrelevant parameters, and we prevent differing aliases among those \nirrelevant parameters from limiting the reuse.  Experimental Results We have implemented our algorithm \nas part of the SUIF (Stanford University Intermediate Format) compiler sys\u00adtem [16]. SUIF is a flexible \ninfrastructure for compiler research. It includes a full ANSI C front end, so we are able to evrttuate \nour analysis with large, realistic application programs. The SUIF system also includes many components \nthat can benefit from points-to information. We have only just begun to take advantage of this, To illustrate \nthe poten\u00adtial, we report some preliminary results of using the points-to information to parallelize \nnumeric C programs. We present preliminary results for a number of bench\u00admarks, including several from \nthe SPEC benchmark suites. Two of the floating-point programs from SPECfp92, a lvinn and ear, are written \nin C and we include them both here. Of the SPECint92 integer programs, we present results for compress \nand eqnt ot t. Most of the other SPECint92 benchmarks contain set jmp / long jmp calls or asynchronous \nsignal handlers. We eventually plan to support set jmp/ long jmp calls in a conservative fashion, but \nwe do not know of any practical way to analyze programs that use asynchronous signal handlers, except \nfor simple cases where the signal handlers exit from the programs. We have also analyzed some Unix utilities, \ngrep and cliff, and a number of the benchmarks used by Landi and Ryder [12]. Proce- Analysis Avg. Benchmark \nLines dures (seconds) PTFs allroots 188 6 0.18 1.00 alvinn 272 8 0.22 1.00 grep 430 9 0.65 1,00 cliff \n668 23 2.13 1.30 lex315 776 16 0.93 1.00 compress 1503 14 1.45 1.00 loader 1539 29 1.70 1.03 football \n2354 57 6.70 1.02 compiler 2360 37 7.57 1.14 assembler 3361 51 5.82 1.08 eqntott 3454 60 9.88 1.33 ear \n4284 68 2.99 1.13 simulator 4663 98 15.54 1.39 Table 2: Benchmark and Analysis Measurements Since the \nfocus of our work has been making ~ointer anal\u00adysis efficient, we are primarily concerned wit~ ~he times \nre\u00adquired to analyze the various benchmarks. Table 2 shows the benchmark programs sorted by size. The \nfirst two columns list the number of source lines and the number of procedures encountered during the \nanalysis. The third column shows the analysis times in seconds for our algorithm running on a DECstation \n5000/260. These times do not include the over\u00adhead for reading the procedures from the input files, building \nflow graphs, and computing dominance frontiers. Neither do they include the time required to write the \nresults to the SUIF output tiles. Our pointer analysis is clearly fast enough to be practi\u00adcd for these \nprograms. In all cases, only a few seconds are required for the analysis. The amount of time can vary \nsignificantly, though, depending not only on the overall size of the input program but also on other \ncharacteristics of the program. For example, even though it is quite a bit larger than eqnt ot t, ear \nis much easier to analyze. In general, floating-point applications seem to be easy targets for pointer \nanalysis. More complex programs will require somewhat more analysis time. Besides the overall analysis \ntimes, we also measured some statistics to evaluate our use of PTFs. The results are very encouraging. \nThe last column of Table 2 shows the average number of PTFs per procedure. These averages are all close \nto one. Moreover, most of the situations where a procedure has more than one PTF are only due to differences \nin the off\u00adsets and strides in the initial points-to functions. Combining PTFs in those situations could \nimprove the efficiency with only a small loss in context-sensitivity. The compiler program is an interesting \ncaseto examine in more detail. This program is a small compiler that uses a recursive descent parser. \nIt has many procedure calls and a lot of them are recursive. Together these factors cause the invocation \ngraph described by Emami et al. to blow up to more than 700,000 nodes [15]. This is for a program with \nonly 37 procedures! For some larger applications, ~he exponential size of the invocation graph will be \ncompletely unreasonable to handle. Fortunately, our results show that it is unnecessary to reanalyze \neach invocation graph node. Points-to information is useful for many different compiler passes, but it \nis crucial for parallelization, Our first use of pointer analysis is to show that static analysis can \nbe used to parallelize loops in numerical C programs. The current SUIF parallelizer uses ourpoints-to \ninformation to determine if for\u00admal parameters can be aliased, It then detects parallel loops and generates \nSPMD (Single Program Multiple Data) code for multiprocessors. It has many of the standard analyses for \nrecognizing loop-level parallelism: constant propagation, in\u00adduction variable recognition, and data dependence \nanalysis. It also includes a few passes that are specific to parallelizing C programs: rewriting while \nloops as for loops where possible and rewriting pointer increments as array index cal\u00adculations. After \nparallelization, the compiler generates a C output file which contains calls to our run-time library. \nPercent Avg. Time Speedups Program Parallel Per Loop 2 Proc. I 4 Proc. alvinn 97.7 7.4 ms 1.95 3.50 IIem \nI 85.8 I 0.2 ms I 1.42 I 1.63 II Table 3: Measurements of Parallelized Programs We ran our parallelizer \nover alvinn and ear. We in\u00adstrumented our run-time system to measure the sequential execution time spent \nin the paral Ielizcd portions of the code. This measurement is shown as a percentage in the first col\u00ad \numn of Table 3. We also measured the sequential time spent in each invocation of each parallelized loop \nto determine the granularity of the parallelism. The averages of these times are shown in the second \ncolumn of Table 3. For both programs, the compiler managed to parallelize all the major loops. We ran \nthe generated code on two and four processors of an SGI 4D/380. The speedups are shown in Table 3. The \nparallelizcd a lvinn achieves very good speedups. The ear program performs reasonably well for two processors \nbut it does not speed up much more with four processors. This is not surprising since there is so little \ncom\u00ad putation in each parallelized loop and since the parallelized program suffers from a lot of false \nsharing. Here we have only shown the use of pointer analysis in a loop parallelizer for multiprocessors. \nThe same pointer information could be used to generate parallel code for superscalar and VLIW processors, \non which both a lvinn and ea r would perform very well, 8 Conclusion We have presented a fully context-sensitive \npointer analysis algorithm~ and have shown that it is very efficient for a set of C programs. This algorithm \nis based on the simple intuition that the aliases among the inputs to a procedure are the same in most \ncalling contexts. Even though it is difficult to sum\u00admarize the behavior of a procedure for all inputs, \nwe can find partial transfer functions for the input aliases encountered in the program. This allows \nus to analyze a procedure once and reuse the results in many other contexts. Even though our algorithm \nis still exponential in the worst case, we have so far found that it performs well. As long as most procedures \nare always called with the same alias patterns, our algorithm will continue to avoid exponential behavior. \nTO be safe, after reaching some limit on the number of PTFs per procedure, we could easily generalize \nthe PTFs instead of creating new ones. Our analysis can handle all the features of the C language. We \nmake conservative assumptions where necessary to en\u00adsure that our results are safe. Even though we may \noccasion\u00adally lose some precision due to these conservative assump\u00adtions, we believe it is important \nto handle the kinds of code found in real programs, even if they do not strictly conform to the ANS[ \nstandard. Our success so far has been encouraging, and our next step is to experiment with large, real-world \napplications. Our preliminary evaluation suggests that our approach may scale well for larger inputs, \nsince most procedures only require one PTF. We also need to use our pointer analysis in more compiler \noptimization to determine if the results obtained are sufficiently precise. Although there is more work \nto be done, we believe this is a major step towards making pointer analysis practical. Acknowledgements \nWe wish to thank Bill Landi for generously giving us his benchmark programs and Erik Ruf for providing \nsome mea\u00adsurements of those programs. Both they and the reviewers also gave us many helpful comments \nand suggestions. Many thanks to Jennifer Anderson, Shih-Wei Liao, Chris Wilson, and the rest of the SUIF \ngroup for their work on the SUIF parallelizer.  References [1] D. R. Chase, M. Wegman, and F. K. Zadcck. \nAnalysis of Pointers and Structures. In Proceedings of the ACM SIGPLAN 90 Conference on Programming Language \nDesign andImplementation, pages296-310, June 1990, [2] J.-D. Choi, M. Burke, and P. Carini. Efficient \nFlow-Sensitive InterProcedural Computation of Pointer-Induced Aliases and Side Effects. In Proceedings \nof the 20th Annual ACM Symposium on Principles of Pro\u00adgramming Languages, pages 232-245, Jan. 1993. [31 \nR. Cytron, J. Ferrante, B. K. Rosen, M. N. Wegman, and F. K. Zadeck. An Efficient Method of Computing \nStatic Single Assignment Form. In I roceedings of [he 16(h Annual ACM Symposium on Principles of Programming \nLanguages, pages 25-35, Jan. 1989. [4] A. Deutsch. InterProcedural May-Alias Analysis for Pointers: Beyond \nk-limiting. In Proceedings of the ACM SIGPLAN94 Conference on Programming Lan\u00adguage Design and Implementation, \npages 230-241, June 1994. [51 M. Emami. A Practical Intcrproccdural Alias Analysis for an Optimizing/Parallelizing \nC Compiler. Master s thesis, School of Computer Science, McGill University, Aug. 1993. [6] M. Emami, \nR. Ghiya, and L. J. Hendren. Context-Sensitive InterProcedural Points-to Analysis in the Pres\u00adence of \nFunction Pointers. In Proceedings of lhe ACM SIGPLAN 94 Conference on Programming Language Design and \nImplementation, pages 242 256, June 1994. [7] M. W. Hall, S. P. Amarasinghe, B. R. Murphy, and M, S. \nLam. Interproccdural Analysis for Parallel ization: Pre\u00adliminary Results. Technical Report CSL-TR-95-665, \nComputer Systems Lab, Stanford University, Stanford, CA 94305-4055, Apr. 1995. [8] W. L. Harrison III. \nThe InterProcedural Analysis and Automatic Parallclization of Scheme Programs. Lisp and Symbolic Computation, \n2(3): 176 396, Oct. 1989. [91 L. J. Hendren. Parallclizing Programs with Recursive Data Structures. IEEE \nTransactions on Parallel and Distributed Systems, 1(1):35-47, Jan. 1990. [10] F. Irigoin, P. Jouvelot, \nand R. Triolet. Semantical In\u00adterprocedural Parallelization: An Overview of the PIPS Project. In Proceedings \nof the 1991 AC14 International Conference on Supercomputing, pages 254 251, June 1991. [11] N. Jones \nand S. Muchnick, Flow Analysis and Opti\u00admization of Lisp-like Structures. In S. Muchnick and N. Jones, \neditors, Program Flow Analysis: Theory and Applications, pages 102 131. Prentice Hall, 1979. [12] W. \nLandi and B. G. Ryder. A Safe Approximate Algorithm for InterProcedural Pointer Aliasing. In Proceedings \nof the ACM SIGPLAN 92 Conference on Programming Language Design and Implementation, pages 235-248, June \n1992. 13] J. R. Larus and P. N. Hilfinger. Detecting Conflicts Be\u00adtween Structure Accesses. In Proceedings \nof the ACM SIGPLAN 88 Conference on Programming Language Design and Implementation, pages 21-34, June \n1988. 14] T. J. Marlowe, W. A. Landi, B. G. Ryder, J, D. Choi, M. G. Burke, and P. Carini. Pointer-Induced \nAliasing: A Clarification. ACM SIGPLAN Notices, 28(9):67-70, Sept. 1993. [15] E. Ruf. Personal communication, \nOct. 1994. [16] R. P. Wdson et al. SUIF An Infrastructure for Research on Parallelizing and Optimizing \nCompilers. ACM SIG-PLAN Notices, 29(12):31 37, Dec. 1994.  \n\t\t\t", "proc_id": "207110", "abstract": "<p>This paper proposes an efficient technique for context-sensitive pointer analysis that is applicable to real C programs. For efficiency, we summarize the effects of procedures using <italic>partial transfer functions</italic>. A partial transfer function (PTF) describes the behavior of a procedure assuming that certain alias relationships hold when it is called. We can reuse a PTF in many calling contexts as long as the aliases among the inputs to the procedure are the same. Our empirical results demonstrate that this technique is successful&#8212;a single PTF per procedure is usually sufficient to obtain completely context-sensitive results. Because many C programs use features such as type casts and pointer arithmetic to circumvent the high-level type system, our algorithm is based on a low-level representation of memory locations that safely handles all the features of C. We have implemented our algorithm in the SUIF compiler system and we show that it runs efficiently for a set of C benchmarks.</p>", "authors": [{"name": "Robert P. Wilson", "author_profile_id": "81332535699", "affiliation": "Computer Systems Laboratory, Stanford University, CA", "person_id": "PP39081155", "email_address": "", "orcid_id": ""}, {"name": "Monica S. Lam", "author_profile_id": "81100237956", "affiliation": "Computer Systems Laboratory, Stanford University, CA", "person_id": "PP14092336", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/207110.207111", "year": "1995", "article_id": "207111", "conference": "PLDI", "title": "Efficient context-sensitive pointer analysis for C programs", "url": "http://dl.acm.org/citation.cfm?id=207111"}