{"article_publication_date": "06-01-1995", "fulltext": "\n Storage Assignment to Stan Liao Srinivas Dcxadas MIT Department of EECS Cambridge, MA 02139-4307 Abstract \nDSP architectures typically provide indirect addressing modes with auto-increment and decrement. In addition, \nindexing mode is not available, and there are usually few, if any, general-purpose registers, Hence, \nit is necessary to use address registers and pe~orm address arithmetic to access automatic variables. \nSubsuming the address arithmetic into auto-increment and auto-decrement modes improves the size of the \ngenerated code. In this paper we present a formu~ation of the problem of optimal storage assignment such \nthat explicit instruc\u00adtions for address arithmetic are minimized. We prove that for the case of a single \naddress register the decision problem is NP-complete. We then generalize the problem to multiple address \nregisters. For both cases heuristic al\u00adgorithms are given. Our experimental results indicate an improvement \nof 3 1 Introduction Microprocessors such as microcontrollers and fixed\u00adpoint digital signal processors \n(DSPS) are increasingly being embedded into many electronic products, Such em\u00adbedded systems range from \nthe mundane (toasters, ther\u00admostats) to very sophisticated products (portable digital assistants, digital \ncellular phones, and laser printers). In fact, the use of microprocessors in embedded systems outnumbers \nthe use of processors in both the PC and the workstation market combined. Two trends are be\u00adcoming clear \nin the design of embedded systems. First, cost, power and reliability considerations are forcing de\u00adsigners \ninto taking the next step: incorporating all the electronics---microprocessor, program ROM and RAM, and \napplication-specific circuit components into a single Permission to copy without fee all or part of this \nmaterial is granted provided that the copies are not made or distributed for direct commercial advantage, \nthe ACM copyright notice and the title of the publication and its date appear, and notice is given that \ncopying is by permission of the Association of Computing Machinery.To copy otherwise, or to republish, \nrequires a fee and/or s~ecific permission. SIGPLAN 95La Jolla, CA USA 0 1995 ACM 0-89791 -697-2/95/0006 \n...$3.50 Decrease Code Size Kurt Keutzer Steve Tjiang Albert Wang Synopsys, Inc. Mountain View, CA 94043-4033 \n integrated circuit. Second, the amount of software in\u00adcorporated into embedded systems is growing larger \nand more complex, The first trend elevates code density to a new level of importance because program \ncode resides in on-chip ROM, the size of which translates directly into silicon area and cost. Moreover, \ndesigners often devote a signifi\u00adcant amount of time to reduce code size so that the code will fit into \navailable ROM; as exceeding on-chip ROM size could require expensive redesign of the entire IC [6, p. \n18] and even of the whole system. The second trend increasing software and system complexity mandates \nthe use of high-level languages (HLLs) in order to decrease development costs and time-to-market. However, \ncurrent compilers for microcontrollers and fixed-point DSPS gen\u00aderate poor code thus programming in a \nHLL can incur significant code performance and code size penalties. While optimizing compilers have proved \neffective for general purpose processors, the irregular datapaths and small number of registers found \nin embedded proces\u00adsors, especially fixed-point DSPS, remain a challenge to compilers. The direct application \nof conventional code optimization methods (e.g., [2]) has, so far, been un\u00adable to generate code that \nefficiently uses the features of fixed-point DSP microprocessors, We believe that generating the best \ncode for embedded processors will require not only traditional optimization techniques, but also new \ntechniques that take advantage of special architectural features and that decrease code size. This paper \npresents one of our efforts at developing such techniques: a data lay-out algorithm that decreases code \nsize. Many architectures such as the VAX, Motorola MC68000, TI TMS320C25, many embedded controllers and \ndigital signal processors provide addressing modes with auto-incrementiauto-decrement arithmetic. These \nad\u00addressing modes provide efficient sequential access of memory and increase code density because they \nsub\u00adsume address arithmetic instructions and result in shorter instructions in variable-length instruction \narchitectures, In particular, DSPS and embedded controllers are de\u00adsigned assuming software that runs \non them would make heavy use of auto-incrementiauto-decrement addressing. Sometimes, DSPS and controllers \nhave such a restricf,ed set of addressing modes that the set does not include a mode for indexing with \nan offset. Therefore, it is necm.\u00adsary to allocate a register and perform address arithmetic to access \nvariables. Subsuming the address arithmetic into auto-increment and auto-decrement modes improves bc)th \nperformance and size of the generated code. The placement of variables in storage has a significant impact \non the effectiveness of subsumption. Our compiler delays storage allocation of variables, moving it from \nihe front-end to the code generation step that selects wkh-ess\u00ading modes, thus increasing opportunities \nto use efficient auto-increment/auto-decrement modes. We formulate this delayed storage allocation as \nthe Offset Assignment Prob\u00adlem. First, we consider a simpler problem that we call simple offset assignment \n(SOA). A solution to the SOA problem assigns optimal frame offset to variables of a procedure assuming \nthat the target machine has a single indexing register with only the indirect, auto-increment, and auto-decrement \naddressing modes. For the SOA prc)b\u00adlem, we represent a procedure by its sequence of variable accesses. \nWe convert the access sequence into an access graph with weighted edges. We show that the SOA prc~b\u00adlem \nis equivalent to a linear graph covering problem of the access graph and that the decision problem for \nSOA is NP-complete. We derive an efficient heuristic algorithm for solving the covering problem, based \non Kruskal s spanning tree algorithm. To incorporate more realistic architectures, we extend the SOA \nproblem to the general offset assignment problem (GOA) that handles multiple index registers. We show \nhow the heuristics used for SOA are used to efficiently solve GOA. Experimental results show that our \ntechniques achieve significant improvements over existing techniques. The paper is organized as follows, \nIn Section 3 we formulate the simple offset assignment problem, and give a heuristic algorithm to solve \nit. We describe the gen\u00ad eral offset assignment problem in Section 4, and give a heuristic method to \ndetermine a good offset assign\u00ad ment. Preliminary experimental results are presented in Section 5. Extensions \nto the general framework are cle\u00adscribed in Section 7. In Section 8, we discuss related work and conclude \nwith directions for future research. 2 Processor Model and Notations For the purpose of exposition, we \nuse a simple proces\u00adsor model that reflects the addressing capabilities of most DSPS. The model is an \naccumulator-based machine. Each operation involves the accumulator and another operand from the memory. \nMemory access can occur only in\u00addirectly via a set of address registers, ARO through AR(lC 1). Furthermore, \nif an instruction uses ARi for indirect addressing, then in the same instruction ARi can be optionally \npost-incremented or post-decremented by one at no extra cost. If an address register does not point to \nthe desired location, it may be changed by adding or subtracting a constant, using the instructions ADAR \nand SBAR. Also, to initialize an address register, the LDAR instruction is used. We use * tARi), * (ARi) \n+, * (ARi) -to denote indi\u00ad rect addressing through ARi, indirect addressing with post\u00ad incremenq and \nindirect addressing with post-decrement, respectively. 3 Simple Offset Assignment In this section we \nassume that only one address reg\u00adister is used to address all variables. We describe the optimization \nproblem corresponding to assigning offsets to variables in a frame so as to obtain the most compact code. \nThis implies that we have to minimize the number of instructions whose sole function is setting ARO to \npoint to appropriate locations in the frame. 3.1 Example As an example illustrating the offset assignment \nprob\u00adlem, consider the C program in Figure 1(a). Assume that the offset assignment to the various variables \nis as shown in Figure 1(b). The assembly code for the C program is shown in Figure 1(c). In the assembly \ncode, the comment after an instruction indicates which variable ARO points to after the instruction is \nexecuted. The instructions SBAR and ADAR are used to change ARO to point to the frame location accessed \nin the next instruction. Assume that ARO initially points to the bottom of the frame, i.e., variable \na. The value of the variable a is loaded in the accumulator, and ARO is incremented in the first LOAD \ninstruction. In the second ADD instruction, the values in a and b are summed and stored in the accumulator; \nfurther, ARO is incremented. Next, using the instruction STORE the contents of the accumulator is stored \nin the location corresponding to variable c. When the assembly instructions corresponding to a = a + \nd are to be executed, we have to load a into the accumulator, but ARO points to f. Therefore, we have \nto subtract 5 from the contents of ARO using an explicit instruction SBAR ARO, 5. All in all, nine SBAR \nand ADAR instructions are required to execute the code of Figure l(a), given the offset assignment of \nFigure l(b). Now consider the offset assignment of Figure 2(b) for the same C code. Assume as before \nthat the ARO regis\u00adter points to variable a initially. We require the shorter assembly code sequence \nof Figure 2(c) to execute the C LDAR ARO, &#38;a ;a C=a+b; LOAD * (ARO)+ ;b ADD *( ARO)+ ;C f=d+e; STOR \n* (ARO)+ ;da=a+d; LOAD *( ARO)+ ; e c=d+a; ADD * (ARO) + ;f d= d+f +a; STOR *(ARO) SBAR AR0,5 ;a (a) \nLOAD * (ARO) ADAR AR0,3 ;d ADD * (ARO) SEAR AR0,3 ;a STOR * (ARO) ADAR AR0,3 ;d LOAD * (ARO) SBAR AR0,3 \n;a ADD * (ARO) f ADAR AR0,2 ;C STOR * (ARO)+ ;d e LOAD * (ARO) d ADAR AR0,2 ;f ADD * (ARO) c SBAR AROJ5 \n;a b All13 * (ARO) ADAR AR0,3 ;d ARO -+ a STOR * (ARO) (b) (c) Figure 1: (a) Code sequence (b) Offset \nassignment (c) Assembly code c=a+b; LDAR ARO, &#38;a ;a LOAD * (ARO) f=d+e; ADAR AR0,3 ;b a=a+d; ADD \n*( ARO) ;C c=d+a; STOR * (ARO) :d d=d+f+a; LOAD * (ARO) SEAR AR0,3 (a) ADD * (ARO)+ STOR * (ARO)+ ;a \nLOAD * (ARO)+ ;d ADD * (ARO) ;a STOR * (ARO)+ ;d b LOAD * (ARO) ;a ADD * (ARO)c ADAR mo,2 d STOR *( ARO) \nLOAD * (ARO) ARO a SBAR AR0,2 ;f f ADD * (ARO)+ ;a ADD * (ARO)+ ;d e STOR * (ARO) 43 (b) (c) Figure 2: \n(a) Code sequence (b) Different Offset assign\u00adment (c) Assembly code code of Figure 2(a). Only four SBAR \nand ADAR instruc\u00adtions are required to execute the code of Figure 2(a). We define the cost of an assignment \nto be the number of SBAR and ADAR instructions required. 3.2 Assumptions in SOA The simple offset assignment \n(SOA) problem involves assigning an offset to each of the local variables to min\u00adimize the number of \ninstructions required to execute a basic block under the following assumptions: A single address register. \nOne-to-one mapping of variables to locations. The basic block has a fixed evaluation order (sched\u00ad ule). \nWe will extend SOA to remove these restrictions in Sec\u00adtions 4 and 7. 3.3 Approach to the Problem Our \napproach to solving the SOA problem is to formu\u00adlate it as a well-defined combinatorial problem of graph \ncovering, called maximum weight path covering (MWPC), From a basic block we derive a graph, called an \naccess graph, that gives the relative benefits of assigning each pair of variables to adjacent locations. \nBy solving the MWPC problem, we can construct an assignment with minimum cost. We then show how to reduce \nan in\u00adstance of the Hamiltonian path problem into an instance of MWPC, demonstrating that a fast exact \nalgorithm for SOA will elude us. At the end of this section, we present a heuristic algorithm to solve \nfor SOA.  3.4 Access Sequence and Access Graph Given a code sequence C that represents a basic block, \nwe can uniquely define an access sequence for the block. Given an operation c = a op b, the access sequence \nis a b c. The access sequence for an ordered set of operations is simply the concatenated access sequences \nfor each operation in the appropriate order. The access sequence for the basic block of Figure 2(a) is \nshown in Figure 3(a). With the notion of the access sequence, it is easily seen that the cost of an assignment \nis equal to the number of adjacent accesses of variables that are not assigned to adjacent locations. \nFor instance, four address arithmetic instructions are required for the offset assignment in Fig\u00adure \n2, since the following two-symbol substrings of the access sequence refer to variables assigned to non-adjacent \nlocations: a b, d e, a c, and c1 f. The access graph G (V, E) is derived from an access sequence as follows: \nEach node v E V in the graph ab cde fada da cdfad b (a) c d 11 a f E e 1 w (b) Figure 3: (a) Access \nsequence (b) Access graph corresponds to a unique variable. An edge e= (vi, vj) E E between nodes v, \nand vj exists with weight w(e) if variables i and j are adjacent to each other w(e) times in the access \nsequence. Note that it does not matter if i is before j or if i is after j since we can both auto-increment \nand auto-decrement ARO during any lc~ad, store, or arithmetic instruction. The access graph for the basic \nblock of Figure 2(a) is shown in Figure 3(b). Thus, in term of the access graph, the cost of an assignment \nis equal to the sum of the weights of all edges that do not connect variables assigned to adjacent locations. \nFor the example in Figure 2, the edges (a,,b), (a,c), (d,e), and (d,f) are such edges, and these edges \nhave a total weight of four.  3.5 SOA and Maximum Weighted Path Covering Definition 3.1 A path P in \nG is an alternating sequence of nodes and edges [VI, el, V2, e2, . . .. em l, vm , 1 where ei = (vi, \nvi+ 1) ~ E, and no v, appears more than once on the path. Definition 3,2 Two paths are said to be disjoint \n~ they do not share any nodes. Definition 3.3 A disjoint path cover (henceforth cover) of a weighted \ngraph G is a subgraph C(V, E ) of G slxch that: For every node v in C, deg(v) ~ 2;  There are no cycles \nin C.  Note that the edges in C form a set of disjoint paths (some of which may contain no edges), \nhence the name. Definition 3.4 The weight of a cover C is the sum of the weights of all edges of C. The \ncost of a cover C is the sum of the weights of all edges in G but not in (7: cost(C) = ~ w(e). eGG,e$ZC \n189 (a) (b) Figure 4: (a) A disjoint path cover (b) an implied assignment with a cost of four Definition \n3.5 An offset assignment A is said to be im\u00ad plied by a cover C f edge e(u, v) E C implies variables \nu and v are adjacent in A. Definition 3.6 (MWPC) Given. an access graph G, find a cover C with maximum \nweight. This is equivalent to jinding a cover with minimum cost. We now show that solving the MWPC problem \nis equivalent to solving the simple offset assignment prob\u00adlem. Lemma 3.1 Given a cover C of G, all offset \nassignments implied by C have cost less than or equal to the cost of the cove~ Proof Let A be any assignment \nimplied by C, As seen in Section 3,4, the cost of the assignment is equal to the sum of the weights of \nall edges (u, v) such that [A(u) A(v)l > 1, where A(u) denotes the offset of variable u under assignment \nA. By Definition 3.5, these edges are a subset of edges in G but not in C. (There may well exist nodes \nu and v such that IA(u) A(v) I = 1 but (u, v) is not in C.) Thus the cost of this assignment is at most \nequal to that of C. Figure 4 gives an example of a cover and an implied assignment with cost less than \nthat of the cover. The edge (a, f) is not in the cover; but it does connect two variables assigned to \nadjacent locations. Thus, the cost of the cover is six, whereas the cost of this particular implied assignment \nis four. Comparing with the cover in Figure 5, it is evident that this cover is not optimal. Lemma 3.2 \nGiven any offset assignment A and an access graph G, there exists a disjoint path cover C which implies \nA and which has the same cost as A. Proof Given an assignment A, we construct a disjoint path cover C \nas follows: for each pair of nodes (u, v) such that A(u) = A(v) + 1, we pick the edge (u, v), 11 Figure \n5: Covering on access graph if it exists in G, to be included in C. C is a disjoint path cover because \nno node in C has a degree greater than two (a variable can have at most two neighbors) and there are \nno cycles (we are not considering memory wrap-around). Furthermore, C implies A by construction. The \nedges in G but not in C are exactly those which connect two nodes with non-adjacent assignments, and \nthus the cost of C is exactly equal to that of A. Theorem 3.1 Every offset assignment implied by an op\u00adtimal \ndisjoint path cover is optimal. Proof Let C be an optimal disjoint path cover C with cost c. Suppose \nthere is an assignment (not necessarily implied by C) with cost c < c. Since an offset assign\u00adment implies \nthe existence of a disjoint path cover with the same cost (Lemma 3.2), there is a disjoint path cover \nwith cost c which is less than c. This contradicts our assumption that C is an optimal cover. Hence, \nno assign\u00adment has a cost strictly less than c, and all assignments implied by C have cost c (Lemma 3.1). \ns Theorem 3.1 allows us to arrive at an optimal simple offset assignment by solving the corresponding \nmaximum weight path covering problem. Intuitively, an edge denotes the number of times two variables \nare accessed immedi\u00adately one after another and hence the number of address arithmetic instructions necessary \nif these two variables are not assigned to adjacent locations. Therefore, by select\u00ading a cover with \nthe maximum weight we minimize the number of address arithmetic instructions required. Consider the access \ngraph of Figure 5. The dark edges beginning from variable e and ending at variable b form a maximum weighted \npath covering (using a single path). This path corresponds to the offset assignment of Figure 2(b). The \nunselected edges in Figure 5 have a weight of 4. This means that the numbel of instructions required \nto explicitly manipulate ARO is 4. This is indeed true as seen in Figure 2(c). The following theorem \nshows that the corresponding decision problem for MWPC is NP-complete, Theorem 3.2 Given an access graph \nG and a number n, the problem of deciding whether there exists a cover with weight greater than or equal \nto n is NP-complete. Proof MWPC E NP, since we can verify the weight of a given cover in linear time. \nWe prove that MWPC is NP-complete by reduction from the Hamiltonian path problem. Given an undirected \ngraph H, we assign a unit weight to each edge. Now a Hamiltonian path exists if and only if there exists \na cover with weight equal to n 1. where n is the number of nodes. We have in essence reduced SOA to \nMWPC. To demonstrate that we have not reduced it to a harder problem, we need to show that given any \nweighted undi\u00adrected graph there is an access sequence that has the graph as its access graph, Theorem \n3.3 Given an undirected graph G(V, E) and a weighting function w : E ~ N, there exists an access se\u00adquence \nwith access graph G z G and weighting function W1=4W +2. Proof Select any node r in G as the root node, \nand perform a depth-first search on G. During the depth\u00adfirst search each edge (u, v) is traversed exactly \ntwice, once forward and once backward. Let T be the sequence in which the nodes are visited (including \nbacktracks). To construct the access sequence S from T, we insert W(U, v) occurrences of vu between every \npair of adjacent nodes UV in T. Thus, on the forward traversal of an edge (u, v) we actually go back \nand forth 2. W(U, v) + 1 times, and likewise on the backward traversal of the edge. Therefore, in the \nderived sequence, the number of times u and v are adjacent to each other is equal to 4 ~W(U, v) + 2. \nFurthermore, the access graph for the sequence is exactly G because by construction the only edges in \nthe graph are those connecting variables that appear adj scent to each other somewhere in the sequence. \nHence, the decision problem for SOA is NP-complete as well. We will need to develop efficient heuristic \nalgo\u00adrithms to solve SOA and MWPC for large problems. For small problems, a branch-and-bound procedure \nis feasible. 3.6 A Heuristic Algorithm for SOA We describe a heuristic algorithm for SOA/MWPC that is \nsimilar to Kruskal s maximum spanning tree algorithm [1]. The algorithm is greedy in that at each step \nthe edge with the largest weight is selected that does not yield a cycle and does not increase the degree \nof a node to more than two. The heuristic algorithm is shown in Figure 6. With careful implementation, \nthis algorithm can run in O (E log E) time. As our experimental results abdceabcdbdecdcbabcdbc I SOLVE-SOA(L) \n2{ 3 /* L = access sequence for basic block */ 4 G(V, E) ~ ACCESS-GRAPH(L); 5 E + sotied list of edges \nin E 6 in descending order of weight; 7 G (V , E ) : V i--V, E +-@; 8 while ([ E l<[Vl land~< ~){ 9 \nchoose e +--first edge in E;  10 B_E e; 11 if ( (e does not cause a cycle in G ) and 12 (e does not \ncause any node in V 13 to have degree > 2) ) 14 add eto E ; 15 else 16 discard e; 17 } 18 /* Construct \nan assignment from E */ 19 return CONSTRUCT-ASSIGNMENT( E ); 20 } Figure 6: Heuristic Algorithm for SOA \n demonstrate, this heuristic often produces a solution quite close to the optimal solution. As an example \nof applying the heuristic algorithm consider the access graph of Figure 3(c). We first pilck edges (a,d), \n(a, f) and (c,d). We reject (a,b) and (a,c) because each causes a cycle. Next, we pick (b,c). We reject \n(d,e) and (d, f ) and finally pick ( f,e), This results in the selection of the dark path of Figure 5 \nwhich is an optimal offset assignment. 4 General Offset Assignment Problem We describe the generalization \nof SOA to the case where there are k address registers, ARO through AR(k 1). In this generalization, \nwe make the following additional assumptions: 1. There is a fixed cost of introducing the use of an address \nregister. This set-up cost reflects the cost associated with initialization upon entry to the pro\u00adcedure \nand re-initialization after return from a canoe. 2. Each address register is used to point to a disjoint \nsubset of variables.  Definition 4.1 Let L be the access sequence of the basic block, and V be the \nset of variables in L. The access subsequence generated by W G V is the subsequence oj L consisting of \nvariables in W. (a) adeaddedad bcbcbccbbcbc 4d w a 13 e cost = o cost= 1 77 (b) (c) Figure 7: (a) Access \nsequece and graph (b) Access subsequence and graph generated by {a,d,e} (c) Access subsequence and graph \ngenerated by {b,c} Definition 4.2 (GOA) Given an access sequence L, the set of variables V, and the number \nof address registers k, jind a partition of V, II = {PI, Pz,. ... Pm}, where m < k, such that the total \ncost of the optimal SOA of the corresponding access subsequences plus the setup costs for using m registers \nis minimum. 4.1 Example of GOA Consider the access sequence and graph shown in Figure 7(a). The optimal \ncover is also shown, with a cost of six. Now consider allocating a second address register for the variables \nb and c. The access subsequences and graphs induced by this partition are shown in Figure 7(b) and (c). \nAssuming a setup cost of one, the cost of using two address registers on this partition is two. In this \ncase, there is an advantage in introducing a second address register. 4.2 A Heuristic Algorithm for \nGOA Clearly, an exact solution to this problem is too ex\u00adpensive to compute. Figure 8 gives a heuristic \nalgorithm for solving GOA. SUBSEQ(L,P) denotes the access subse\u00adquence of L generated by P. Our heuristic \nis to build up the partition blocks incrementally by repeatedly selecting a subset of nodes as a new \npartition block. The function SOLVE-GOA returns a collection of dis\u00adjoint ordered sets of variables which \nforms a partition of Hence, if a variable has a high penalty, then it may SOLVE-GOA(L, k) be beneficial \nto move it to another partition block. 2 L 3 /* L = access sequence of basic block */ 4 /* k = number \nof address registers */ 5 H + SOLVE-SOA(L); 6 if (k == 1) return {H}; 8 P A SELECT-VARIABLES(L); 9 LI \ne SUBSEQ(L, P); 10 Lz - SUBSEQ(L, L P); 11 HI e SOLVE-SOA(LI); 12 H2 +--SOLVE-SOA(L2); 13 if (setup-cost \n+ cost(lll ) + cost(EIZ) > cost(~)) 14 return {H}; 15 else 16 return {Hl } U SOLVE-GOA(L2, k 1): 17 \n% Figure 8: Heuristic Algorithm for GOA the set of all variables. The order of each subset gives an \noffset assignment. Given an access sequence L, SoLvE-GOA first computes the SOA of L. If there is only \none address register, the solution is simply the SOA. Oth\u00aderwise, SOLVE-GOA calls SELECT-VARIABLES to \nchoose a subset of the variables in L and solves SOA on the derived subsequences L1 and L2. If the cost \nof this split along with the setup cost is more expensive than that of H, there is no benefit in introducing \nthe new partition block and the current solution H is returned. Otherwise, it is advantageous to introduce \na new address register for this subset of variables, and SOLVE-GOA is recursively called for the remaining \nvariables. The procedure SELECT-VARIABLES selects a subset of variables for which a new partition block \nmay be created. It is important to note that on line 13 of the algorithm in Figure 8 we are making the \nassumption that, if allocating a new address register for the subset L1 returned by SELECT-VARIABLES \ndoes not reduce the cost, then further partitioning will not improve either. In other words, we assume \nthat if there is a good subset of variables, SELECT-VARIABLES will find it at the first opportunity. \nTo develop good heuristics for this procedure, we make the following observations: 1. If an access subsequence \nconsists of two variables, then the cost for this access subsequence is just the setup cost. No switching \ncost is incurred. 2. If a node in an access graph has more than two edges, the associated minimum penalty \nfor retaining the node in the graph is the sum of the weights on all edges except the two with the largest \nweights.  Thus, a simple heuristic for SELECT-VARIABLES is to select the two variables with the largest \npenalty, and the cost we have to pay for allocating a new address register is just the setup cost. However, \nas we have observed in our experiments (see Section 5), it is sometimes more profitable to select more \nthan two variables at once, and the best choice depends on the program itself. We are presently investigating \nmore powerful techniques for per\u00ad forming this variable selection. S Experiments and Results We have \nimplemented the heuristic algorithms of Sec\u00adtions 3 and 4. In addition, we have implemented a branch\u00adand-bound \nprocedure for solving SOA in order to evaluate the heuristic SOA algorithm. All the implementations han\u00addle \nnot on~y basic blocks, but entire procedures. Since our goal is to minimize static code size, we weigh \neach basic block equally. To construct the access graph for an entire procedure, we merge the access \ngraphs of each basic block, and also take control-flow edges into account as well. For example, if an \nedge exists from basic block A to basic block B, and the last variable in A is a and the first variable \nin B is b, we increase the weight on the edge between node a and b or create an edge if it does not exist. \nThis, however, makes the cost of the solution to MWPC only an approximate of the actual cost, as shown \nbelow. Table 1 shows our experimental results on ten routines typical of those found in DSP and embedded \napplications. The first five programs are core routines from a JPEG-MPEG package. The next three are \ngraphics routines from the xv program. Hufftree is a routine from gz ip that builds a Huffman encoding \ntree, and gnucrypt is the GNU implementation of the DES encryption algorithm, The column labeled decl. \norder gives the (overall) size of each program for the offset assignment based on the order in which \nthe variables are declared. Program sizes, along with the percentage reduction, obtained using the greedy \nheuristic of Figure 6 and using a branch-and\u00adbound procedure are shown next. The last columns show the \nresults of using GOA (Figure 8) with four address registers (k = 4). The column labeled b-b GOA uses \nthe same GOA algorithm, but calls the branch-and-bound procedure for SOA instead of the greedy SOA heuristic. \nIn all cases of GOA, a simple heuristic for SELECT-VARIABLES is used: select the m nodes with the heighest \npenalty. However, we have observed that no fixed value of m gives the best result overall, and in Table \n1 the best value of m is shown for each example (in the columns I program II decl. order II greedy SOA \nII b-b SOA greedy GOA b-b GOA  ,.-II II 4 l] # inst. II # ;nst. 1 % red. II # inst I % red. II # inst; \nI % red. I i+xl. II # ht. I % red./ # Sel. I chendct 756 730 3.5% 728 3.7% 651 13.9% 2 651 13.970 2 chenidct \n817 778 4.8% 776 5.0% 650 20.5% 3 650 20.5% 3 Ieedct 893 842 5.7% 841 5.8% 760 14.9% 4 760 14.9% 4 ileedct \n1017 949 6.7% 948 6.8% 819 19.5% 6 815 19.9% 6 jrev .4296 4070 5.3% 4057 5.6% 3387 21.2% 3 3387 21.2% \n3 readgif 648 599 7.6% 589I 9.1% 550 15.2% 5 551 15.0% 3 aWocrOp 549 534 2.7% 531 3.3% 520 5.3% 2 520 \n5.3% 2 smooth 4002 3841 4.0% 3837 4.1% 3621 9.5% 3 3617 9.6% 3 hufftree 956 914 4.4% 907 5.1% 856 10.5% \n3 858 10.3% 3 II 11 I 1! , ,, gnucrypt [~ 3188 II 3063 I 3.9% 1] 3065 ] 3.9% /] 2958 ~ 7.2~o ] 2 ]/ 2955 \n) 7.3% ] 2 Table 1: Experimental results marked # sel. ). Hence, there are many opportunities to improve \non the variable selection algorithm. For simple offset assignment, the reduction in code size ranges \nfrom 390 to 990. For general offset assignment, the reduction ranges from 5% to 205Z0. Thus, depending \non the program (number of variables and the way they ~. ( C Program are accessed), GOA can potentially \nachieve substantial improvements. In all cases the greedy heursitic arrives at solutions very close \nto the optimal. It is interesting to note that in a few ~SUIF Front End cases the greedy heuristic for \nSOA actually outperforms the branch-and-bound procedure. The reason for this is Machine that the heuristic \nand branch-and-bound procedure found Independent (, SUIF different optimal solutions for the path-covering \nproblem, Optimizations = , and in the presense of control-flow the access-graph model / Preliminary \n/ does not exactly reflect the real cost, which depends on Code // the actual offset assignment. Selection \n.~ / Dataflow Analyses 6 Our Compiler Framework &#38; ( TWIF Offset Assignment RegisterAllocation The \noptimization techniques described in this paper are incorporated into our framework for developing compilers \n&#38;  for embedded systems [3]. A diagram showing the stages of the compiler is shown in Figure 9. \nw  We use SUIF [12] as our front-end. Machine\u00adindependent optimizations such as global common subex\u00adpression \nelimination is carried out in SUIF. The SIJIF intermediate form is then translated into another internne\u00addiate \nform called TWIF, which is parametrized according Figure 9: A framework for developing compilers for \nem\u00adto the machine description. It is on this intermediate form bedded systems that instruction scheduling, \noffset assignment, and register allocation are performed [9], along with machine-specific dataflow analyses \nand related optimizations. (At the time of this writing we have only implemented the offset as\u00adsignment \nprocedure and the final code generation pass. Scheduling and register allocation are problems we are \nC=a b; ~=a-b; g=e+f; h. b+a; 7s% h=a+b; g=e+f; (a) (b) first: x L5 M first i Figure 10: Fragment of \na control-flow graph currently investigating and will be implemented in the near future.) Object code \nis then finally obtained through the final phase of code generation and peephole optimiza\u00adtion. Code \ncompression on object code [8] proves to be effective in further increasing the code density. 7 Extensions \n7.1 Offset Assignment for a Procedure The access graph model for offset assignment gives exact results \nfor basic blocks. However, in the presence of control-flow, modeling the exact cost of offset assignment \nis more difficult. To see this, consider the fragment of a control-flow graph shown in Figure 10, where \neach node denotes a basic block. The last variables accessed in blocks L 6 and L3 are j, and the first \nvariables accessed in blocks L 5 and L 1 are x and i, respectively. Since L3 can be followed by either \nL5 or L1, it is not obvious whether we should add an edge between j and x, or between j and i, or both. \nWhichever we choose, the graph does not model the cost exactly. For instance, if we do both, and it turns \nout that in the final assignment j is adjacent to i (say immediately below i), then according to the \naccess graph we need to pay a cost of two on the edge (j ,x) for this portion of the control-flow graph \ndue to the control-flow edges (L 6,L5) and (L3,L5). However, if at the end of L6 and L3 we do perform \nauto-increment after accessing j so that upon exit on either block the address register points to i, \nthen all we need to do is set the address register to point to x, thereby incurring a cost of one, rather \nthan two. It is interesting to note that, even though at the end of L 6 auto-incrementing the address \nregister does not make it point to x, we still perform auto-increment anyway. This is because in some \nmachines, such as the TMS320C25, setting the address register from an unknown value has a higher cost \nthan setting it from a known value. The former usually involves an immediate address which takes another \ninstruction word, whereas the latter requires only an address arithmetic instruction, Hence, it is preferable \nthat upon entering a basic block that the contents of the address register are known. Our current approach \nis to merge the access graphs for all basic blocks, with equal weighting, and to treat h h 12 ab c 21 \n 11 11 Lr fe (c) (d) Figure 11: (a) Schedule 1 (b) Schedule 2 (c) Access graph for Schedule 1 (d) Access \ngraph for Schedule 2 variables connected by control-flow edges (e.g., (j, i) and (j ,x) in the same way. \nThen, after offset assign\u00adment, a separate pass is used to determine whether auto\u00adincrement/decrement \nshould be used at the end of each basic block. If our goal is to optimize for execution speed, then this \nformulation will correctly reflect the actual cost (up to the accuracy of the execution frequency estimate \nof the basic blocks and control-flow edges). In this case, the access graph for each basic block is weighted \nby its estimated execution count (from either static estimation or profiling information), and likewise \nfor control-flow edges. 7.2 Multiple Memory Locations for Variables We can enforce static single assignment \nprior to deter\u00admining an offset assignment or we can perform a life-time analysis and merge variables \nwhich can share the same memory location. It may be possible that merging two variables with disjoint \nlife-times can lead to an assignment with lower cost,  7.3 GOA with Scheduling Scheduling has a strong \neffect on the sequence of variable accesses. In Figure 11(a) and (b) two different schedules for the \nsame C code are given. In Figure 11(c) and (d) the access graphs for the schedules of Figure 1l(a) and \n(b), respectively, are given. The schedule of Figure 1l(b) exploits the commutativity of the + operator \nand interchanges the operands a and b in the computation of h. The access graph of Figure 11(c) has a \ncost of 1 when a single address register is assumed. The access graph of Figure 11(d) has a cost of O. \n We are investigating techniques for scheduling that result in an access graph for which an offset assignment \nwith zero or low cost is guaranteed. 8 Conclusions and Ongoing Work Code generation for irregular datapaths \nand machines with severely restricted instruction sets, such as those used in DSP and embedded microprocessors, \nis a problem that has received relatively little attention to date. Previous work [4, 5, 7, 11] on VLIW \nmachines, microcode gen\u00aderation and application-specific instruction processors has covered the topic \nof irregular data paths but restricted addressing and code density has never been their primary concern. \nLiem et al. [10] presented techniques for gen\u00aderating compact code; however, the benchmark programs were \nquite small and it is not shown how their techniclues perform on larger, more realistic programs. With \nthe increasing use of embedded systems, code generation for them has become very important. In this paper \nwe presented algorithms that are able to exploit the addressing mode features of most DSP processors. \nOur initial results indicate that these algorithms can ob\u00adtain substantial improvements in code size \nbeyond those provided by conventional code generation techniques. There are many avenues for further \nwork in offset assignment as indicated in Section 7. We are also ad\u00addressing several other code optimization \nproblems that arise in irregular datapaths [9]. Conventional register al\u00adlocation is not possible for \nsome DSP processors since the number of general-purpose registers available could be very small, Minimizing \nthe number of accumulator spills becomes a relevant optimization problem. Finally, exploiting the different \nmode settings in instructions (e g., unsigned versus signed arithmetic) affords the possibility of generating \nmore compact code. 9 Acknowledgements The authors would like to thank Mahadevan Ganapathi for interesting \ndiscussions on the offset assignment prob\u00adlem, and Scott McFarling for careful reviewing of the lpa\u00adper, \nThis research was supported in part by the Advanced Research Projects Agency under contract NOO014-91 \n-J\u00ad 1698 and in part by a NSF Young Investigator Award with matching funds from Mitsubishi and IBM Corpora\u00ad \ntion. References [1] A. Aho, J, Hopcroft, and J, Unman. The Design and Analysis of Computer Algorithms. \nAddison-Wesley, 1974. [2] A. Aho, R. Sethi, and J. Unman. Compilers Princi\u00adples, Techniques and Tools. \nAddison-Wesley, 1986. [3] G. Araujo, S. Devadas, K. Keutzer, S. Liao, S. Ma\u00adlik, A. Sudarsanam, S. Tjiang, \nand A. Wang, Chal\u00adlenges in code generation for embedded processors. In P. Marwedel and G. Goossens, \neditors, Code Gen\u00aderation for Embedded Processors. Kluwer Academic Publishers, 1995. In press. [4] John \nR. Ellis. A Compiler for VLIW Architectures. MIT Press, 1985. [5] J. A. Fisher. Trace Scheduling: A Technique \nfor Global Microcode Compaction. IEEE Transactions on Computers, C-30(7 ):478J190, 1981. [6] J. G. Ganssle. \nThe Art of Programming Embedded Systems. San Diego, CA: Academic Press, Inc., 1992. [7] G. Goossens, \nJ. Rabaey, F, Catthoor, J Vanhoof, R. Jain, H. De Man, and J, Vandewalle, A Computer-Aided Design Methodology \nfor Mapping DSP Al\u00adgorithms onto Custom Multiprocessor Architectures. In Proceedings of IEEE International \nSymposium on Circuits and Systems, pages 924 925, May 1986. [8] S. Liao, S. Devadas, and K. Keutzer. \nCode Density Optimization for Embedded DSP Processors Using Data Compression Techniques, In Proceedings \nof the Chapel Hill Conference on Advanced Research in VLSI, March 1995. [9] S. Liao, S. Devadas, K. Keutzer, \nS. Tjiang, and A. Wang. Code Optimization Techniques for Em\u00adbedded DSP Microprocessors. In Proceedings \nof the 32nd Design Automation Conference, June 1995. [10] C. Liem, T. May, and P. Paulin. Instruction-Set \nMatching and Selection for DSP and ASIP Code Generation. In Proceedings of European Design and Test Conference, \nMarch 1994. [11] K. Rimey. A Compiler for Application-Spec@c signal Processors. PhD thesis, University \nof California, Berkeley, 1989. [12] R. Wilson, R. French, C. Wilson, S. Amarasinghe, J. Anderson, S, \nTjiang, S.-W. Liao, C.-W. Tseng, M. Hall, M. Lam, and J. Hennessy. SUIF: A Par\u00adallelizing and Optimizing \nResearch Compiler. Tech\u00adnical Report CSL-TR-94-620, Stanford University, May 1994.  \n\t\t\t", "proc_id": "207110", "abstract": "<p>DSP architectures typically provide indirect addressing modes with auto-increment and decrement. In addition, indexing mode is not available, and there are usually few, if any, general-purpose registers. Hence, it is necessary to use address registers and perform address arithmetic to access automatic variables. Subsuming the address arithmetic into auto-increment and auto-decrement modes improves the size of the generated code.</p><p>In this paper we present a formulation of the problem of optimal storage assignment such that explicit instructions for address arithmetic are minimized. We prove that for the case of a single address register the decision problem is NP-complete. We then generalize the problem to multiple address registers. For both cases heuristic algorithms are given. Our experimental results indicate an improvement of 3.</p>", "authors": [{"name": "Stan Liao", "author_profile_id": "81452604296", "affiliation": "MIT Department of EECS, Cambridge, MA", "person_id": "PP39075184", "email_address": "", "orcid_id": ""}, {"name": "Srinivas Devadas", "author_profile_id": "81100130021", "affiliation": "MIT Department of EECS, Cambridge, MA", "person_id": "PP39071020", "email_address": "", "orcid_id": ""}, {"name": "Kurt Keutzer", "author_profile_id": "81100164705", "affiliation": "Synopsys, Inc., Mountain View, CA", "person_id": "PP39030506", "email_address": "", "orcid_id": ""}, {"name": "Steve Tjiang", "author_profile_id": "81100257188", "affiliation": "Synopsys, Inc., Mountain View, CA", "person_id": "PP43118160", "email_address": "", "orcid_id": ""}, {"name": "Albert Wang", "author_profile_id": "81100213627", "affiliation": "Synopsys, Inc., Mountain View, CA", "person_id": "PP14083916", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/207110.207139", "year": "1995", "article_id": "207139", "conference": "PLDI", "title": "Storage assignment to decrease code size", "url": "http://dl.acm.org/citation.cfm?id=207139"}