{"article_publication_date": "06-01-1995", "fulltext": "\n APT: A Data Structure for Optimal Control Dependence Computation Keshav Pingali Department of Computer \nCornell University, Ithaca, Science NY 14853. GiDipartimento Universit?r anfranco Bilardi 2 di Elettronica \ned Informatica, di Padova, Padova, Italy. Abstract The control dependence relation is used extensively \nin re\u00adstructuring compilers. This relation is usually represented using the control dependence graph; \nunfortunately, the size of this data structure can be quadratic in the size of the pro\u00adgram, even for \nsome structured programs. In this paper, we introduce a data structure called the augmented post\u00addominator \ntree (A PT) which is constructed in space and time proportional to the size of the program, and which \ncan answer control dependence queries in time proportional to the size of the output. Therefore, APT \nis an optimal representation of control dependence. We also show that using APT, we can compute SSA graphs, \nas well as sparse dataflow evaluator graphs, in time proportional to the size of the program. Finally, \nwe put ATT in perspective by showing that it can be viewed as a factored representation of the control \ndependence graph in which jiltered search is used to answer queries. 1 Introduction Control dependence \nis a key concept in program optimiza\u00adtion and parallelization. Intuitively, a node n is control dependent \non a node c if c determines whether n is exe\u00adcuted. For example, in a conditional statement, statements \non the true and false sides of the conditional are control de\u00adpendent on the predicate. Since statements \nfrom opposite 1Keshav Pmgali (pingali @cs,cornell,edu) was supported by an NSF Presldentml Young Investigator \naward CCR-8958543, NSF grant CCR\u00ad9008526, ONR grant NOO014-93-1-0103, and a grant from Hewlett- Packard \nCorpomtlon. 2Gianfranco Bilardi (bdardl@art dei unipd.lt) was supported m part by the ESPRIT III Basic \nResearch Progrmmne of the EC under contract No 9072 (Project GEPPCOM) and by the Italian Ministry of \nUniversity and Reseiuch Permission to copy without fee all or part of this material is granted provided \nthat the copies are not made or distributed for direct commercial advantage, the ACM copyright notice \nand the title of the publication and its date appear, and notice is given that copying is by permission \nof the Association of Computing Machinery.To copy otherwise, or to republish, requires a fee and/or specific \npermission. SIGPLAN 95La Jolla, CA USA 0 1995 ACM 0-89791 -697 -2/95/0006 ...$3.50 sides of the conditional \nstatement are control dependent on the predicate in a complementary sense, it is more precise to say \nthat anode is control dependent on an edge; for example, we can say that statements on the true side \nof a conditional statement are control dependent on the true edge from the predicate. In the presence \nof nested control structures, mul\u00adtiway branches and unstructured flow of control, intuition is an unreliable \nguide, and it is better to use a formal, graph\u00adtheoretic definition of control dependence. This definition \nrequires the following concepts. Definition 1 A control flow graph G = (V, E) is a directed graph in \nwhich nodes represent statements, and an edge v + w represents possiblejlow of control from v to w. Set \nV contains distinguished nodes START and END such that every node is reachablefrom START , and END is \nreachable from every node in G. START has no predecessors and END has no successors. To simplify the \ndiscussion, we will follow standard prac\u00adtice and assume that there is an edge from START directly to \nEND in the control flow graph [FOW87]. Definition 2 A node w is said to postdominate a node v f eve?y \npath from ? to END contains w. Note that a node v is always postdominated by END and by itself. It can \nbe shown that postdominance is a transitive relation, and that its transitive reduction is a tree-structured \nrelation called the postdominator tree. The parent of a node in this tree is called the immediate postdominator \nof that node. The postdominator tree of a program can be con\u00adstructed in 0( IEIcY(]EI)) time using an \nalgorithm due to Tarjan and Lengauer [LT79], or in 0( IE \\) time using arather more complicated algorithm \ndue to Harel [Har85]. Control dependence can be defined formally as follows [FOW87]: Definition edge \n(u -+ 3 A node v) G E if w is said to be control dependent on 1. w p2. l~u, ost# dorninates u, then w \nv, and does not postdominate u,  END g START f c ed a/\\ b (b) Postdominator Tree --w  L!!LiEH END \n(c) Control Dependence Relation(a) Control Flow Graph Figure 1: A Program and its Control Dependence \nRelation Intuitively, this means that if control flows from node u to node o along edge u + v, it will \neventually reach node w; however, it is possible to reach END from u without passing through w. Thus, \nu is a decision-point that determines whether w will be executed. Let C denote the set of all pairs (c, \nw) such that node w is control dependent on edge e. We refer to the set C (~ E x V) as the control dependence \nrelation of the control flow graph G. Control dependence is used in many phases of modern compilers, \nsuch as dataflow analysis, loop transformations and code scheduling. An abstract view of these applications \nis that they require the computation of the following sets derived from C [CFS90]: o cd(e) = {w G V/(e, \nw) E C}, conds(w) = {e E E](e, w) E C}, and  cdequiv(w) = {v E Vlconds(l,) = conds(w)}  Set cd(e) \nis the set of nodes that are control dependent on edge e, while conds(w) is the set of control dependence \nof node w. These sets are used in scheduling instructions across basic block boundaries for speculative \nor predicated execution [Fis81, BR9 1]. They are also useful in merging program versions [HPR87], and \nin automatic paralleliza\u00adtion [ABC+ 88]. Set cdequiv(w) contains the nodes that have the same control \ndependence as node w. This infor\u00admation is useful in code scheduling because basic blocks with the same \ncontrol dependence can be treated as one large basic block, as is done in region scheduling [GS87]. The \nrelation cdequiv can also be used to decompose the control flow graph of a program into single-entry \nsingle-exit (SESE) regions, and this decomposition can be exploited to speed up dataflow analysis by \ncombining sb-uctural and fix\u00adpoint induction [JPP94, Joh94], and to perform dataflow analysis in parallel \n[JPP94, GPS90]. Figure 1 shows a small program and its control dependence relation. For any edge e, cd(e) \nis the set of marked nodes in the row corresponding to e. For any node w, conds(w) is the set of marked \nedges in the column corresponding to w. Fi\u00adnally, we see that cdequiv(c) = cdequiv(~) = {c, ~}, and cdequiv(a) \n= cdequiv(g) = {a, g}; all the other nodes are in cdequ iv sets by themselves. In this paper, we design \na data structure to represent the control dependence relation. Such a data structure must be evaluated \nalong three dimensions: preprocessing time T: the time required to build the data structure,  space \nS: the overall size of the data structure, and  qu e~ time Q: the time rewired to Produce the answer \nto cd, conds and cdequi.v queries.  The size of the control dependence relation gives an upper bound \non the space requirements of such a data structure. It is easy to show that the size of the relation \nis Q( IVI IEl ), even if we restrict our attention to structured programs. For exam\u00adple, for programs \nconsisting of n nested repeat-until loops, it can be verified that 1131= Q(n) and ICI = Q( n2); therefore, \nthe size of the control dependence relation can grow quadrat\u00adically with program size. It would be incorrect \nto conclude that quadratic space is a lower bound on the size of any rep\u00adresentation of the control dependence \nrelation. Note that the size of the postdominator relation grows quadratically with program size (consider \na chain of n nodes), but this relation can be represented using the postdominator tree, which can be \nbuilt in 0( lE/ ) space [LT79, Har85], and which provides proportional time access to the postdominators \nof a node. The explanation of the paradox is that postdominance is a transitive relation, and the postdominator \ntree, which is the transitive reduction of this relation, is a factored , compact representation of postdominance. \nThere is no point in build\u00ading a representation of the full relation because the factored relation is \nmore compact, and it answers queries optimally. Is there a factored representation of the control depen\u00addence \nrelation which can be built in 0( IEl ) space and 0( IE 1) preprocessing time, and which will answer \ncd, conds and cdequiv queries in time proportional to the size of the output? The standard representation \nof the control dependence re\u00adlation is the control dependence graph (CDG) [FOW87], which is best viewed \nas a bipartite graph in which the two sets of nodes in the bipartite graph are V and E, and in which \nthere is an undirected edge between node v and edge e if v is control dependent on e. Since this is a \nstraight-forward representation of the full relation, the size of the CD G is Q( IEI IVI ). There have \nbeen many efforts to construct more compact representations of the control dependence relation [FOW87, \nCFS90, Ba193, JP93, SGL94], and the lack of suc\u00adcess led Cytron, Ferrante and Sarkar to conjecture that \nany data structure that provided proportional time access to con\u00adtrol dependence sets must use space \nthat grows quadratically with program size [CFS90]. In this paper, we describe a data structure called \nthe aug\u00admented postdominator tree (AP7) which requires 0( IE[ ) space, is built in 0(\\ El) time3, and \nwhich is designed to pro\u00advide proportional time access to conds sets. This is clearly optimal to within \na constant factor. In fact, our approach incorporates a design parameter a( > O), under the control of \nthe compiler writer, representing a trade-off between time and space. A smaller value of a results in \nfaster query time at the expense of more memory for a larger data structure. Interestingly, the control \ndependence graph can be viewed as one extreme of this range of data structures, obtained when a is less \nthan 1/\\ El. Using existing algorithms for cd and cdeqyiv, we show that the .A7V data structure provides \nproportional time access to cd and cdequiv sets as well, completing the description of an optimal data \nstructure for control dependence. The rest of the paper is organized as follows. In Sec\u00adtion 2, we reformulate \nthe conds problem as a naturally stated graph problem called the Roman Chariots problem. In Section 3, \nwe describe the APT data structure and the algorithm for the conds problem. The data structure is a minor \naugmentation of the postdominator tree; the query procedure for conds sets performs a walk over parts \nof the postdominator tree. We also show how ATT can be used to answer cd and cdequiv queries. Some experimental \nre\u00adsults on a model problem and on the SPEC benchmarks are reported Section 4. In Section 5, we discuss \nan application of these techniques: we show how to perform d-function placement, the key step in SSA \ncomputation [CFR+ 9 1], in 0( \\.El ) time. Finally, in Section 6, we contrast our ap\u00adproach with dynamic \ntechniques like memorization [Mic68]; we also show that our approach can be viewed as an example of Chazelle \ns@tered search [Cha86]. 2 The Roman Chariots Problem We reformulate the control dependence problem as \na natu\u00adrally stated graph problem called the Roman Chariots prob\u00adlem, using the fact that nodes that \nare control dependent on an edge e in the control flow graph form a simple path in the postdominator \ntree [FOW87]. First, we introduce some convenient notation. Definition 4 Let T =< V,F > beatree. For \nv,w EV, the notation [v, w] represents the set of vertices on the simple path joining v and w in P. Similarly, \nthe notation [v, w) represents the set of vertices on the simple path joining v and w in P, not including \nw. (In particular [v, v) is empty.) For example, in the postdominator tree of Figure l(b), [d, a] denotes \nthe set {d, ~, c, a}, while [cl, a,) represents the set {d, ~, c}. This notation is similar to the standard \nmath\u00adematical notation for open and closed intervals of the line. 3We ~s~ume that [he postdominator relation \nis computed using HareI s algorithm; if the Lengauer and Tarjan algorithm is used, preprocessing time \nbecomes O(IEICZ(IE])), The following key theorem is due to Ferrante, Ottenstein and Warren [FOW87]. Theorem \n1 If(u A v) is an edge of the controljow graph, then 1. parent is an ancestor of v in the postdominator \ntree, and 2. cd(u + V) = [v, ~arrmt(u)).  Proofi Note that since no control-flow edge emanates from \nEND, the expression parent(u) is defined whenever (u + v) E K. 1. If parent(u) does not postdominate \nW, we can find a path v --+ ... + END which does not contain parent(u), Prefixing this path with the \nedge u + v, we obtain a path from u to END which does not contain parent(u), contradicting the fact that \nparent (u) postdominates u. 2. We show that cd(u + v) < [o, parent(u)). Let w be an element of cd( u \n+ 7)). From the definition of control dependence, w must postdominate v, so w is on the path [v, END] \nin the postdominator tree. From part (l), parent(u) is also on the path [v, END]. However, w cannot be \non the path [parent (v.), END] since in that case, it would be distinct from u and postdominate v.. Therefore, \nw must be on the path [v, parent(u)),  Conversely, assume that w is contained in the path [v, parent(u)). \nFrom part (l), it follows that w post\u00addominates v; it also follows that w does not postdominate parent(u), \nTherefore, if w # u, then w cannot post\u00addominate IL either. Therefore, w is control dependent on edgeu+ \nV. u Figure 2 shows the non-empty cd sets for the program of Figure 1(i). If [v, w) is a cd set, we \nwill refer to v and w as the bottom and top nodes of this set respectively, where the orientations of \nbottom and top are with respect to the tree4. The postdominator tree and the array of cd sets, together, \ncan be viewed as a compact representation of the control dependence relation since we can recover the \nfull control dependence relation by expanding each entry of the form [v, w) to the corresponding set \nof nodes by walking up the postdominator tree from v to w. The advantage of using the postdominator tree \nand cd sets, instead of the CDG, is that they can be represented in 0( IEl ) space, and as we will see, \ncan be built in 0( IEI) time. What is not obvious is how they can be used to answer control dependence \nqueries in proportional time that is the subject of the rest of the paper. 4As an aside, we remmk that \nthe bottom-closed, top-open representation for the sets has been chosen here since it is the most immediate \nto obtain in our application. In general, a closed set[b, t], in which t is an ancestor of b, is readily \nconverted into the equivalent half-open one [b, parent(t)), in constant time The conversion of set [b, \ni) into a closed one is less straightforward, and takes time proportional to the number of children of \nt,assuming that ancestorship can be decided in constant time However, if the conversion has to be performed \nfor a batch of half-open sets A, it can be accornphshed in time 0( IV I + IA ]) by a depth-first traversal \nof the tree. We do not need this conversion, v E abcde fg START-U  f-b .: C-d c-e a-b (a) Control Dependence \nRelation END E cd sets g START START-LZ [a, END) f f-b [b,g) ced C-d [d)f) c -e [e)f) aK k b a-b [b,c) \nTk (b) Postdominator Tree and cd sets Figure 2: Compact Representation of Control Dependence For the \npurpose of exposition, it is convenient to assume that the array of cd sets, which is indexed by CFG \nedges in Figure 2, is indexed instead by the integers 1.. m, where m is the number of CFG edges for which \nthe corresponding cd sets are non-empty. We will assume that the conversion from an integer (between \n1 and m) to the corresponding CFG edge and vice versa can be done in constant time. We can now reduce \nthe control dependence problem to a naturally stated graph problem. Roman Chariots Problem: The major \narteries of the Roman road system are organized as a rooted tree in which nodes represent cities, edges \nrepresent roads and the root of the tree is Rome 5. Public transportation is provided by chariots, and \nthe cities on each chariot route are totally ordered by the ancestor relation in the tree. Given a rooted \ntree T =< V, F, ROME > andan array A[l. .m] of chariot routes in which each route is specljied by its \nendpoints as Iv, w], where v is a descendant of w in T, design a data structure to answer the following \nqueries optimally. 1. cd(n): Enumerate the cities on route n. 2. conds(w): Enumerate the set of routes \nthat serve city w. 3. cdequiv(w): Enumerate the set of cities that are served by all and only the routes \nthat serve city w.  The control dependence problem is reduced to the Roman Chariots problem as follows, \nProcedure ConstructRoman-Chariots in Figure 3 takes a control flow graph as input, and 5A thorough ]jterature \nsearch failed to turn up any historical evidence to support this statement, but it is a matter of record \nthat all roads led to Rome [CicBC], just as in a tree rooted at Rome! Procedure ConstructRomanChariots(G:CFG):Tree, \nRouteArray; { 1: % G is the control flow graph, 2: T:= build-postdominator-tree(G); 3: .4 := [ ]; %Initialize \nto empty array 4: i:= o; 5: for each node u in Tin top-down order do 6: for each child c of u do 7: for \neach edge (c --i v) in G do 8: if f)is not u 9: then %append a cd set to end of A 10: i :=; +1; 11: .4[2] \n:= [v, U); 12: Note the correspondence between 13: edge c -+ II and index i; 14: endif 15: od 16: od \n17: return T, .4; } Figure 3: Constructing a Roman Chariots Problem returns the corresponding Roman \nChariots problem. As\u00adsuming the postdominator tree can be built in time O(IEI), procedure ConstructRomanChariots \ntakes time O(IEI), and space 0({,??1). Control dependence queries are handled as follows. o cd(?{ + v): \nIf o is parerd(u), return the empty set. Otherwise, let i be the index into array .4 for edge u -+ v. \nExecute the Roman Chariots query cd(i), e conds(w): Execute the Roman Chariots query conds (w), and \ntranslate each integer (between 1 amd m) returned by this query to the corresponding CFG edg,e. e cdequiv( \nw): Execute the corresponding Roman Char\u00adiots query cdequiv(w), The correctness of this reduction follows \nimmediately from Theorem 1 and Procedure ConstructRomanChari\u00adots, In the construction of Figure 3, the \ncd sets in .4 are sorted by decreasing top nodes; that is, if t 1 is a proper an\u00adcestor of t2 in the \npostdominator tree, then any cd set whose top node is tl is inserted in the array before any cd set whose \ntop node is t2. We will exploit this order when we build the APT structure in Section 3. Note that for \na general Roman Chariots problem (not arising from a control dependence problem), this sorting can be \ndone by a variation of Pro\u00adcedure ConstructRomanChariots, in time 0(1.41 + \\Vl ). This is within the \nbudget for preprocessing time given be\u00adlow. Therefore, we will assume without loss of generality that.4 \nhas been sorted in this way. The rest of the paper establishes the following result. Theorem 2 There \nis a data structure APT for a Roman Chariots problem (T =< V, F, ROME >, .4) which can be constructed \nin time T = O(IAI + (1+ l/a)lVl), and storedin space S = 0(1.41 + (1 + l/a)\\Vl), where a >0 is a design \nparameter By traversing APT, the following queries can be answered. * cd(c): Query answered in time proportional \nto output size. The time is independent of a. conds(w): Query answered in time 0((1 + ck)s) wheres is \noutput size. &#38; cdequiv(w ): Query answered in time proportional to output size. The time is independent \nof a. For the special case of a tree which is the postdominator tree of a control flow graph, (i) IAI \nS IEI, and (ii) IV I < IEl + 1, since the control flow graph is connected. Therefore, we have the following \nresult. Corollary 1 Given a CFG G = (V, E), structure APL? can be built in 0( [E\\) preprocessing time \nand space, and pro\u00advidesproportional time access to cd, conds and cdequiv sets. 3 APT: Solving the Roman \nChariots Problem Optimally The APT data structure to solve a Roman Chariots problem (T, .4) is an augmentation \nof the tree T, and is described incrementally in this section. 3.1 cd queries cd queries are easy: if \nthe query is cd(i), where i is between 1 and m (the size of .4), let [v, w) be the ith route in .4. Walk \nup the tree T from node v to node w, and output all nodes encountered in this walk, other than node w. \nThis takes time proportional to the size of the output. This algorithm is similar to one due to Ferrante \net al [FOW87]. 3.2 conds queries One way to answer conds queries is to examine all routes in array .4, \nand report every route whose bottom node is a descendant of the query node, and whose top node is a \nproper ancestor of the query node. This requires examining all routes in A, which is too slow. A better \napproach is to limit the search to routes whose bottom nodes are descendants of the query node, since \nthese are the only routes that can contain the query node. To fa\u00ad cilitate this, we will assume that \nat every node v, we have recorded all routes whose bottom node is v; then, the query procedure must visit \nthe subtree of the postdominator tree rooted at the query node, and examine routes recorded at these \nnodes. This is shown in Figure 4(a). The space taken by the data structure is S = 0( IV I + IA I), which \nis optimal. However, in the worst case, the query procedure must exam\u00adine all nodes and all routes (consider \nthe query conds (END)), so query time is Q = O(IAI + 11 1), which is too slow. To speed up query time, \nwe extend this idea as follows. Rather than store a route only at its bottom node, we can store the route \nat every node contained in the route, as in Figure 4(b). Given a query at node q, the query procedure \nsimply outputs all routes stored at that node; if IAq I is the size of this output, this takes time Q \n= 0(\\ .41 l), which is optimal. Unfortu\u00adnately, this strategy produces the control dependence graph in \ndisguise, and therefore blows up space requirements. For example, for the Roman Chariots problem arising \nfrom a nested repeat-until loop, the reader can verify that Q( 1.41) routes each contain Q( \\VI ) nodes \nand hence are represented in as many lists, requiring space S = Q( IV IIAl) overall, which is far from \noptimal. These considerations suggest that there is a space-time tradeoff in computing conds sets caching \ninformation at nodes on a route reduces query time, but increases storage requirements. Let us call the \ntwo extremes of caching no caching (store a route only at its bottom node), and full caching (store a \nroute at every node contained in the route). To explore the trade-off, we consider caching information \nat some but not necessarily all nodes on a route. Informally, we can say that the nodes in the tree are \npartitioned into zones, and all the nodes in a zone share cached information in the sense that even if \na route contains two or more nodes from a zone, it is cached at exactly one node in that zone. In Figure \n4(c), there are six zones induced by the following sets of nodes: {a, b,c}, {d}, {e}, {f, g}, {sTART} \nand {END}. Note that even though Chariot Route 1 contains both nodes a, and c, it is stored only at node \no.; similarly, it contains both f and g but is stored only at node ~. To quantify the space-time trade-off, \nwe must define zones formally. We require that a zone be formed by a subset of nodes in the tree, such \nthat these nodes and the edges in T between them form a tree. For simplicity, we will require that any \nnode v in a zone can be classified either as an interior node, which means that w and its children are \nall in the same zone, or as a boundary node, which means that v and its children are all in different \nzones; by convention, the leaves of the tree are boundary nodes. Intuitively, this rules out the possibility \nthat some but not all the children of a node are in the same zone as itself. In Figure 4, boundary nodes \nare shown as solid dots, while interior nodes are shown as hollow dots; for example, in Figure 4(c), \nnodes c and g are interior nodes, while all other nodes are boundary nodes, The details of our zone partitioning \nalgorithm will be given in Section 3.2.1. Assuming that zones are given to us, we can define how routes \nare cached in the tree. With each node v E V, we associate a list of routes L [v], defined formally as \nfollows. Definition 5 Zf v is an interior node, L[v] is the list of all routes whose bottom node is v; \nifv is a boundary node, L[v] is the list of all routes containing v. END Chariot Routes 1 g [U, END) \nSTART [M f2 3 Mf) cd (4; (3) 4 [e)f ) a/\\b 5 (1) {2,5) (a) No caching ENDEND , () {} (I)cso 0 g o{) \nSTART {l,2)f,a {l,2}f ,/ ,. START c (1,2)c9 ~t d eed ,, (4) {3)(4) {3) ~l~a o {ha /i b{2,51 b~l,j) \n(b) Full caching (c) Some caching: IX= 1 Figure 4: Zone Structure The motivation for this definition \nbecomes obvious if we think about the query procedure. When a node q is queried, the query procedure \nvisits all descendants of q that are in the same zone as q (call this set Zq), and for each visited node \nV, reportsallroutesinL[t)]whosetopendpoint t is a proper ancestor of q. For example, in Figure 4(c), \na query at node g results in visits to nodes g and ~. Using the definition of L [v], we can see that \nAg C Uuez, L [v]. A route containing a given query node q must originate at a descendant b of q; so if \nb < 21, then the route is in L [b], and otherwise, the route enters Z~ through some boundary node w, \nand is in L[w]. To avoid examining routes unnecessarily, we will assume that each list L [v] is sorted \nby top end point, from higher (closer to the root) to lower. Examination of routes in a list L [v] can \nterminate as soon as a route [b, t) not containing q is encountered; further routes on the list terminate \nat a descendant oft and do not contain the query node q. It follows immediately that the query time is \nproportional to the sum of the number of visited nodes and the number of reported routes. If we define \nthe subzone Zq associated with a node q to be the set of descendants of q that are in the same zone as \nq. The query time for node q can be written as follows: A simple implementation of this query procedure \nis given in Figure 5. Boundary nodes are distinguished from interior nodes by a boolean named Bnsiry? \nwhich is set to true for boundary nodes and to false for interior nodes; the algo\u00adrithm for determining \nwhich nodes are boundary nodes will Procedure Query(QueryNode); { 1: % APT data structure is global \nvariable; 2: % Query outputs list of routes numbers 3: Visit(QueryNode, QT/eryNode); } Procedure Visit(QueryNode, \nVisit Node); { 1: for each route i in L[VisitA ode] 2: in list order do 3: let .4(i) be [b, t); 4: \niftis a proper ancestor of Q~~cryAiode 5: then output i; 6: else break; %o exit from the loop 7: od \n; 8: if Vi.SitiVode is not a boundary node 9: then 10: for each child C of VisitNode 11: do 12: Visit(QveryA70de,C) \n13: od ; 14: endif; 15; } Figure 5: Query Procedure for conds be described in Section 3.2.1. In line \n4 of Procedure Visit, testing vvhether t is a proper ancestor of Qu erylNode can be done in constant \ntime as follows: since i and QueryNode are ordered by the ancestor relation, we can give each node a \nclfs (a[epth-first search) number, and establish ancestor\u00adship by comparing dfs numbers. Since dfs numbers \nare already assigned by postdominator tree construction algo\u00adrithms [LT79, Har85], this is convenient. \nAlternatively, we can use level numbers in the tree. 3.2.1 Algorithm for Determining Zones Let us first \nrequire that the following inequality holds for all nodes q; a is a design parameter. 12,1< Q1.4q]+ 1 \n(2) Intuitively, this means that the number of nodes visited when q is queried is at most one more than \nsome constant propor\u00adtion of the answer size (the additive term of 1 is required because a node may not \nbe contained in any route (1.4ql = 0), but the query procedure must nevertheless visit the queried node \nto determine this). This guarantees that the amount of work we do for a query is proportional to the \noutput size, provided the output is non-empty, as we can see by combining Equations (1) and (2); for \no: a constant, this is asymptotically optimal. Q = 0((1 +a)l.4,1) (3) Can we build zones so that Inequality \n2 is satisfied, with\u00adout blowing up storage requirements? One bit is required at each node to distinguish \nboundary nodes from interior nodes, which takes 0( IVI) space. The main storage over\u00adhead arises from \nthe need to list all overlapping routes at a boundary node, even if these routes originate at some other \nnode. This means that a route must be entered into the L[V] list of its bottom node, and of every boundary \nnode between its bottom node and top node. To keep storage requirements in check, our zone construction \nalgorithm builds zones in a bottom-up, greedy way, trying to making zones as large as possible without \nviolating Inequality 2. More precisely, a leaf node is always a boundary node. For a non-leaf node u, \nwe see if v and its children can be placed in the same zone without violating Inequality 2; if not, v \nis made a boundary node, and otherwise, v is made an interior node. Formalizing this intuitive description, \nwe can define subzones precisely. Definition 6 If node v is a leaf node or lZU\\) > (alAVl+l), thenv is \nabound\u00ad (l+xf,cch;ki,en(.) ary node and Z,j is {v}. Otherwise, v is an interior node and Z~ is {w} uu~childrerz(v) \nzt~. Note that the term (1 + xt,~.h,ildren(v) IZUI) is simply the number of nodes that would be visited \nby a query at node v if v is made an interior node. If this quantity is larger than (al.4,, I + 1), Inequality \n2 fails, so we make v a boundary node. Zones are simply maximal subzones: that is, subzones that are \nnot contained within a larger subzone. 3.2.2 Bounding Storage Requirements The definition of zones lets \nus bound storage requirements as follows. Denote by X the set of boundary nodes that are not leaves. \nIf v c (V X), then only routes whose bottom node is v are listed in .L[v]. Each route in A appears in \nthe list of its bottom node and, possibly, in the list of some other node in X. For a boundary node v, \nIL [v] I = 1.4.1. Hence, we have: (4) From Definition 6, if w E X, then IAVI < ~ \\Z,,l/ci, (5) uschildren(v) \n When we sum over v s X both sides of Inequality (5), we see that the right hand side evaluates at most \nto IV] /a, since all subzones Z. s involved in the resulting double summa\u00adtion are disjoint. Hence, ZVGX \nIAV I S IV [/a, which, used in Equation (4) yields: ~ ILIU]I <1.41 + \\v\\/ct. (6) Usv In conclusion, \nto store A7YT, we need 0(\\ V I) space for the postdominator tree, 0( IVI ) further space for the Bndry? \nbit and for list headers, and finally, from Inequality(6), 0( IAI + IV I/a) for the list elements. All \ntogether, we have S = 0(1.4\\ +(1 + l/ct)lVl), as stated in Theorem 2. We observe that design parameter \na embodies a tradeoff between query time (increasing with a) and preprocessing space (decreasing with \na). In fact, for a < 1/I Al, we obtain single-node zones (essentially, the control dependence graph since \nevery node has its overlapping routes explicitly listed) and, for CY~ \\V 1,we obtain a single zone (ignoring \nSTART and END and assuming \\A,, I > 0 for all other nodes, which is the case for the control dependence \nproblem). Small con\u00adstant values such as CY= 1 yield a reasonable compromise. Figure 4(c) shows the zone \nstructure of the running example fora = 1. 3.2.3 Preprocessing Algorithm We now describe an algorithm \nto construct the search struc\u00adture APT in linear time. The preprocessing algorithm takes three inputs: \ne Tree T for which we assume that the relative order of two nodes one of which is an ancestor of the \nother can be determined in constant time. For the control dependence problem, this is the postdominator \ntree. e The array of routes, .4, in which routes are sorted by top endpoint. For the control dependence \nproblem, this array is constructed by Procedure BuildSortedRoutes shown in Figure 3. Real parameter a \n20, which controls the space/query\u00adtime tradeoff, as described in the previous section. The preprocessing \nalgorithm consists of a sequence of few simple stages. 1. For each node v, compute the number of routes \nwhose top node (resp. bottom node) is v. Let b[v] (resp. t [v]) be the number of routes in .4 with bottom \n(resp. top) endpoint at v. To compute b[v] and i [v], two counters are set up and initialized to zero. \nThen, for each route in .4, the appropriate counters of its endpoints are in\u00adcremented. This stage takes \ntime 0( IV I + \\Al ), for the initialization of the 21VI counters, and for constant work done for each \nof the IAl routes. 2. Compute, for each node v, the size 1.4. I of the an\u00ad  swer set .4V. It is easy \nto see that 1A,, I = b[v] 1.4U1. This relation allows us to [v] + ~ucchtidren(v) compute the \\AV I values \nin bottom-up order, using the values of b[v] and t [v] computed in the previous step, intime O(lVl). \n 3. Determine boundary nodes. The objective of this step is to set, at each node, the value of a boolean \nvariable Bndry?[v] that identifies boundary nodes. Definition can be expressed in terms of subzone size \nz [v] = \\20 I as follows. /   -2 -3 tladcf g END START e d Chariot END () Routes 1 [(z END) g  k \n( ) START 2 [b,g) f/ {1,2) ,., ,, r m 3 ki,f ) ~ ,,. ; ;.,* ~ a ~> b j -b START+-a ~becbd 4 [e, (4] \n{3) /\\ (l)U b(2,5) 5 b,c) --.+ pxtdominator tree edges B - . control dependenceedges (a) Caching: \na = 1 (c) ,4ctual Implementation Node IJ q?)] t [1)] (I[v] 2[?)] Nzt Bndr~/ [v] Bndry?[v] L[v] a 1011f_ \n1 {1} b 202 1 1 {2,5} c o123 o ; {} d 1011 1 {3} f_ 1011 1 {4} f_ ; o22 1END 1 {1,2} o112END o ST A~~o001END \n1 ~ {} {} END o10 1 cc 1 {} (b) Values Computed During Preprocessing Figure 7: Construction of APT Structure \nIf v is a leaf or (1 + ~.,~chiidre,z(t~) 3[U]) > (cll.4,)1 + set W corresponding to p. This procedure \nensures that, 1), then v is a boundary node, and Z[V] is set to 1. in each list L [v], routes appear \nin decreasing order of Otherwise, v is an interior node, and Z[V] = (1 + top endpoint. Z?,echildren(v) \nz[~l) This stage takes time proportional to the number of append operations, which is ~Uev ]LIv]I = \no(l~l + Again, Z[V] and Bndry?[fl] are easily computed in bottom-up order, taking time O(IVI). Iv//a). \n 4. Determine, for each node t), the next boundary node In conclusion, we have shown that the preprocessing \ntime NziBndry[v] in the path from v to the root. If the par-T == 0(/.41 +(1 + l/a)lVl), as claimed. ent \nof v is a boundary node, then it is the next boundary Figure 6 shows the pseudo-code for building the \nsearch for v. Otherwise, v has the same next boundary as its structure APT. All the preprocessing, including \nconstruc\u00adparent. Thus, NztBndry[v] is easily computed in top-tion of the route array A, can be done in \none top-down and down order, taking 0( IV 1) time. A special provision one bottom-up walk of the postdominator \ntree, followed by is made for the root of T, whose next bounda3y is set one traversal of the route array. \nby convention to co, considered as a proper ancestor of any node in the tree. 3.3 cdequiv queries 5. \nConstruct list L[v] for each node v, By Definition 5, a To solve the cdequiv problem efficiently, we \nexploit an al\u00ad given route [b, t) appears in list L[v] for v c W, where gorithm of Johnson, Pearson \nand Pingali for identifying tree W contains b as well as all boundary nodes contained nodes contained \nin the same set of routes [JPP94, Section by [b, t). Specifically, let W = {to. = b, ZO1,.... Wk}, 3]. \nThis algorithm requires 0( [Al + IV I) time and space, where W, = NziBndry[wi-l], for i = 1,2, ..., k \nand During preprocessing, we execute this algorithm, and then Wk is the proper descendant of tsuch that \nt is a descen\u00adchain nodes in each equivalence class into a cycle, using a dant of lVzfBndry[wk]. field \nCat each node, which is made to point to the next node Lists L [v] s are formed by scanning the routes \nin A in in the cycle. Given a query cdequiv(v), we traverse the cy\u00adwhich routes have been entered in \ndecreasing order of cle associated with node v and output all nodes encountered top endpoint. Each route \np is appended at the end of in this traversal. (the constructed portion of) L [v] for each node v in \nthe Procedure PreProcessing(Ttree,A: RouteArray,a:real); { 1: 910b[v] /t [v]: number of routes with bottom/top \nnode t 2: for eachnode vinTdo 3: b[v] := t[v] := O; od 4: for each route [x, y) in A do 5: Increment \n6[$]; 6: Increment t[y]; 7: od; 8: %Determine boundary nodes. 9: for each node v in Tin bottom-up order \ndo 10: %Compute output size when v is queried. 11: (I(v] := b[v] -t[v] + ~l~~c~,t~~ren(t})~[?~]; 12: \nz[v]:= 1 + Z~GCM~.~~tV)Z[U]; %Tentativezonesize. 13; if (v is aleaf) or (z[v] > a * a[v] + 1) 14: then \n~o Begin a new zone 15: Bndry?[v] := true; 16: Z[v] := 1; 17: eke ToPut v into same zone as its children \n18: Bndry?[v] := false; 19: endif 20: od; 21: To Chain each node to the jirst boundary node that is an \nancestor 22: for each node v in T in top-down order do 23: if w is root of postdominator tree 24: then \nNztEIndr-y[v] := -m; 25: else if 13ndry?[parent(v)] 26: then N$tJ3ndry[v] := parent(v); 27: else Nxtl?ndry[v] \n:= Nztl?ndry[parent( v)]; 28: endlf 29: endif 30: od 31: % Add each route in A to relevant L[v] 32: fori \n:= 1 to IAI do 33: let .4[i] be [b, t); 34: w:=b; 35: while i is proper ancestor of w do 36: append i \nto end of list L[vJ]; 37! w := NxtBndry[w]; 38: od } Figure 6: Building the AT7 Structure An interesting \npoint to note is that the algorithm for cdequiv in [JPP94] uses the depth-first tree of the undi\u00adrected \nversion of the control-flow graph, in which the analogs of routes are back edges. This algorithm is based \non a non-trivial characterization of cdequiv classes in terms of cycle equivalence, a relation that holds \nbetween two nodes when they belong to the same set of cycles. This charac\u00adterization, which is remarkable \nin that it does not make any explicit reference to the postdominance relation, allows the cdequiv relation \nto be computed in less time than it takes to compute the postdominator tree. However, since post\u00addominator \ninformation is available in A PT, the reduction to cycle equivalence is not needed here. 3.4 Summary \nWe can summarize the data structure A P?_ for the Roman Chariots problem as follows, 1. 7 : tree that \npermits top-down and bottom-up traversals 2. .4: array of chariot routes of the form [v, w) where w \nis an ancestor of v in T 3. dfs[v]: djs number of node v 4. l?ndr~?[v]: boolean, Set to true if v is \na boundary node, and set to false otherwise 5. L[v]: list of chariot routes. If v is a boundary node, \nL[tI] is a list of all routes containing v; otherwise, it is a list of all routes whose bottom node is \nv. 6. C[v]: node. Node next to v in cdequiv equivalence class of v  Two aspects of our AK? implementation \nfor the control dependence problem are worth mentioning. Rather than use cd sets, we work with the corresponding \nCFG edges, and the conversion to cd sets is done on the fly, using the post\u00addominator tree (see Figure \n7(c)). This enables the output of conds queries to be produced directly without translation from integers \nto CFG edges, eliminating a data structure that would be needed for this translation. Finally, in pro\u00adcedure \nQuery of Figure 5, it is worth inlining the call to procedure Vkit, and eliminating ancestorship tests \non routes cached at the query node itselfi if full caching is performed, the overhead of a conds query \nin A P7, compared to that in the CDG, reduces to a single conditional test,  4 Experiments For control \ndependence investigations, the standard model problem is a nest of repeat-until loops, where the problem \nsize is the number of nested loops, n. Figures 8(a) and (b) show storage requirements as problem size \nis varied. The storage axis measures the total number of routes stored at all nodes of the tree. The \nstorage required for the CD G is n,(n + 3) which grows quadratically with problem size as expected. For \na fixed problem size, the storage needed for APT is between the storage needed for the CDG (full caching) \nand the storage needed if there is no caching (the dotted line at the bottom of Figures 8(a,b)). Consider \nthe graph for a = 1/32 in Figure 8(a). For small problem sizes (between 1 and31 ), storage requirements \nlook exactly like those of the CDG. For problem sizes larger than 63, storage requirements grow linearly. \nIn between these two regimes is a transitory region. A similar pattern can be observed in the graph for \no = 1/16, These results can be explained analytically as follows. From Equation 2, it follows that every \nnode is in a zone by itself if, for all nodes q! lz~l s CY141+ I <2. Thismeansthatfor all nodes L 1.4.TI< \nl/a. If the nesting depth is n, it is easy to verify that the largest value of lAql is (n+ 1). Therefore, \nif n < (1 /a) 1, all nodes are in zones by themselves, which is the case for the CDG. This analysis shows \nthe adaptive nature of the APT data structure. Intuitively, 1/a is a measure of the budget for space \n if the problem size is small compared with the budget, the algorithm performs full caching. As problem \nsize increases, full caching becomes more and more expensive, until at some point, zones with more than \none node start to appear, and the graph for APT peels away from the graph for the CDG. A similar analytical \ninterpretation is possible for Figure 8(b) which shows storage requirements for a > 1. Finally, Figure \n8(c) shows that for a fixed problem size, storage requirements increase as a decreases, as expected. \nThe dashed line is the minimum of the CDG size and the right hand side of Inequality 6 for n = 100; this \nis the computed upper bound on storage requirements, and it clearly lies above the graph of storage actually \nused. Figure 8(d) shows that for a fixed problem size, worst case query time decreases as ~ decreases. \nBecause actual query time is too small to measure accurately, we measured instead the number of routes \nexamined during querying (say r), and the number of nodes in the subzone of the query node, other than \nthe query node itself (say s). The y-axis is the sum (r+ 2s), where the factor of 2 comes from the need \nto traverse each edge in the subzone twice, once on the way down and then again on the way back up. Note \nthat each graph levels off at its two ends (for very small a and for very large a) as it should. It is \nimportant to note that the node for which worst-case query time is exhibited is different for different \nvalues of a. In other words, the range of query times for a fixed node is far more than the 5:1 ratio \nseen in Figure 8(d). Finally, Figures 8(e,f) show how preprocessing time varies with problem size, and \nwith a. These times were measured on a SUN-4. Note that for a > 1/8, preprocess\u00ading time is less than \nthe time to build the postdominator tree; even for very small values of a, the time to build the AlZ7_ \ndata structure is no more than twice the time to build the postdominator tree. This shows that preprocessing \nis relatively inexpensive. Real programs, such as the SPEC benchmarks, are less challenging than the \nmodel problem. Note that the ratio of storage required for full caching to the storage required for no \ncaching is simply the average number of nodes in a char\u00adiot route. In terms of the control dependence \nrelation, this is the average number of nodes that are control dependent on a CFG edge, considering only \nCFG edges with non-empty ccl sets, For the model problem of nested repeat-until loops, this number grows \nwith problem size. For procedures in the SPEC benchmarks, this number is fairly independent of problem \nsize, and is close to 2, as can be seen in Figure 9(a). This is because the height of the postdominator \ntree of most real prclgrams is quite small, and it is more or less indepen\u00addent of program size (for \nexample, in the SPEC benchmarks, most procedures have a postdominator tree height less than 25, and only \nprocedure iniset.f in doduc has a postdominator tree hei,ght of more than 75). Therefore, in practice, \nthe size of the C DG grows linearly with program size, as is seen in Figure 9(a). We conclude that the \nAKT data structure can reduce storage requirements for caching by about a fac\u00adtor of half, and this can \nbe seen in Figures 9(a,b). Query time was not significantly affected when Q was set to 1; for larger \nvalues of a, query time for a few nodes was affected, but on the whole, the effect was small, Finally, \nfor every procedure in the SPEC benchmarks, preprocessing time to construct APT is a small fraction of \nthe time to build the postdominator tree.  5 Applications The APT data structure can be used to design \nan optimal al\u00adgorithm for finding the dominance frontier of a node, which is the key step in conversion \nto Static Single Assignment (SSA) form [SS70, CFR+9 1]. In conversion to SSA form, dummy assignments \ncalled +$-functions are introduced into the program in such a way that every use of a variable is reached \nby at most one real or dummy assignment. The APT diita structure can be used to convert a program to \nSSA form in 0( IEl) time per variable; the related problem of computing sparse dataflow evaluator graphs \ncan also be solved using the same idea. First, we review the standard definition of dominance. Definiticm \n7 A node w is said to dominate a node v ifevery path from START to v contains w. It is well-known that \nany algorithm computing the post\u00addominance relation can be used to compute the dominance relation by \napplying this algorithm to the reverse controljlow graph, the control flow graph obtained by reversing \nall con\u00adtrol flow edges and interchanging START and END[LT79]. Definition 8 The edge dominance frontier \nof a node w in a control~ow graph is a set of edges, denoted by edf ( w), such (v -~ u) c edf(w) if 1. \nw dominates v, and 2. f w # u, then w does not dominate u.  This definition is similar to that of Cytron \netal [CFR+91]. Comparing this to Definition 3, it is clear that (v -+ u) 5000 / 4500 -/ / 4000 \u00ad , ( \n3500 \u00ad, / 3000 \u00ad/ %a / c&#38;G ~2500-r 150\u00ad$ / 2 2000 / // 100 \u00ad1500-/ / / 1000 \u00ad50 \u00ad 500 - ALPHA \n= ~ ok o 10 20 30 40 50 60 70 80 90100 Nesting Depth Nest,ng Depth (a)Storage vs. Nesting Level (a S \n1) (b) Storage vs. Nesting Level (a ~ 1) \\\\ 12000, 500 450 dep~ :100 \\ 1! actu I \\,predlcted 400\u00ad 10000 \n  l---v 350 8000:I / 6000 4000 \u00ad 2000 depth =32 ~6~ 24 68 log(ALPHA) log(A:PHA) (c) Storage vs. a (d) \nQuery Time vs. a 8 7 n 86 g epth =64 $5 P ~ 4 ----b - ----_____ _____ _____ _______ . 8 PDOM depth= 64k \n$3 epth =32 2 ------_ --_ __ _ _____ ____ PCTOt4:d3pih<?32 k~ \\ t -- detdh. ~ o o 102030405060708090100 \n-8 -6 -4 -2 0 2 4 6 Nesting Depth Log(ALPHA) (e) Preprocessing Time vs. Nesting Level (f) Preprocessing \nTime vs. a Figure 8: Experimental Results for Repeat-Until Loop Nests 42 800, r+ 1 I + Full Caching oOOF \nSome Caching ALPHA= 1 0. No Caching BFuII Caching +{ 10000 1 8 6000 #Some Caching ALPHA = 1 I No Caching \n1 1 % ~ 6000 In . - 4000 100-+ 2000 o 50 100 150 200 250 300 Program Size, Nodes 350 400 450 500 0 [ \nSPEC Floating Point Benchmarks Figure 9: Experimental Results for SPEC Benchmarks belongs to edf(w) \niff (u 4 v) belongs to conds(zu) in the reverse control flow graph. Therefore, an optimal data structure \nfor the dominance frontier problem is obtained by constructing an APT data structure on the reverse control \nflow graph (this data structure can be viewed as an aug\u00admented dominator tree on the forward control \nflow graph). Given a query for edf (w), node w is queried in the data structure using the query procedure \nof Figure 5, and each edge produced by the query procedure is reversed before being output. To find the \nSSA form of a program, a set of nodes s containing assignments to a variable is given, and it is de\u00adsired \nto find a set T such that T is closed in the following sense: if node t belongs to T, and (v + u) belongs \nto edf (t), then u belongs to t. This is called an iterated domi\u00adnance frontier computation, and dummy \nassignments called &#38;functions must be introduced at nodes in T which are not in S. In the reverse \ncontrol flow graph, this computation can be described equationally as follows. Extend the def\u00adinition \nof conds to sets of nodes in the natural way: if S is a set of nodes, let conds(s) be the set U,,c,$ \nconds(s). Let source be a function that, given an edge, returns the source of the edge; this function \ntoo can be extended to sets of edges in the natural way. The @function placement prob\u00adlem can be reduced \nto the following problem on the reverse C FG: given a set of nodes S, compute the smallest set of nodes \nT such that T = S U source(conds(T)), First, consider the problem of using the AP T data struc\u00adture to \ncompute conds(lV) where lV is a set of nodes known before any queries are made. To avoid visiting tree \nnodes repeatedly during querying, it is desirable to sort the nodes in N by level in the tree, and query \nthe nodes of N in bottom-up order. A node in the tree is marked when it is queried, and the query procedure \nof Figure 5 is modified so that it never visits nodes below a marked node. Therefore, the time for querying \nall the nodes in N is proportional to the sum of the number of nodes visited and the number of paths \nin the Lu lists of these nodes. In the worst case, every nocle in the tree is visited, in which case, \nusing Equation 6, we can see that the total time for querying is (7) Q = O(IVI+ IAI + lV1/a). Since 1.41 \n< ll?l in our context, this bound can be writ\u00adten as O(ll?l + (1 + I/a)/VI), which, for constant a, is \nproportional to program size. Sorting the set N initially by level can be done by a breadth-first walk \nof the tree, so it can be dlone in time 0( IV I). Therefore, we can find the set conds (N) for a set \nof nodes N in time proportional to program size. If N is given online, we cannot sort it before starting \nqueries. However, as long as nodes are presented for query\u00ading in order of decreasing level number, the \napproach dis\u00adcussed above can be used. To accomplish this in our context, we mainlain a priority queue \n[CLR92] of nodes that must be queried; the key for the priority queue is level number in the tree, Initially, \nthis priority queue contains only the nodes in S. At each step, a node w of highest level (farthest from \nthe root) is extracted from the priority queue, and a query is made in the APLT data structure with that \nnode; if there is a node u in source (conds(to )) but which is not in the output set, it is added to \nthe output set, and then inserted into the priority queue. From Theorem 1, it follows that the level \nof u is less than or equal to the level of to. Therefore, querying a node at level 1can never cause the \ninsertion of a node at a level strictly greater than 1into the priority queue. The priority queue can \nbe implemented using a heap, which gives O(log( k )) time per operation, where k, the number of keys, \nis the height of the tree [CLR92]; a more sophisticated data structure due to van Erode Boas et al gives \nO(log(log(k)) time per operation [VEBKZ77]. Prior\u00ad ity queues are more general than what we need since \nwe can guarantee that insertions always occur behind extractions. Sreedhar and Gao have pointed out that \nan array of size k suffices [Sf395]; in this array, each element is a linked list of nodes, at the corresponding \nlevel, that must be queried. A node at level 1is inserted into the priority queue by append\u00ading it to \nthe list of nodes at array element 1; this gives 0(1) time for each insertion. To extract a node with \nmaximum level number, the array elements are scanned by decreas\u00ading level number till a non-nil linked \nlist is found; this gives O(k) time for extractions. However, since insertions always occur behind extractions, \nthe extraction process always ad\u00advances monotonically through the array towards decreasing level number, \nso the total time for inserting and extracting nodes during the computation is bounded by 0( \\V\\). The \nalgorithm is described in Figure 10. The input to Procedure ~-placement is the set of nodes containing \nreal as\u00adsignments to some variable; the output is the set of nodes that should contain real or dummy \nassignments to the variable. The running time of the algorithm is proportional to the size of the program. \nThis is an improvement over the original algorithm of Cytron et al which has a worst-case time com\u00adplexity \nthat is quadratic in the size of the program [CFR+9 1]. Cytron and Ferrante have used path compression \nto design another algorithm with 0(1171a( l17\\)) complexity [CF93], but this algorithm is rather complicated. \nJohnson and Pingali have designed an 0(\\ El ) algorithm for ~-function place\u00adment [JP93], using the dependence \nflow graph [PBJ+9 1]. Just as building the SSA form can be reduced to an iterated conds computation, \nbuilding the dependence flow graph can be reduced to an iterated cdequiv computation, and can be done \nin 0( IE I) time per variable. However, this approach cannot be extended to compute sparse dataflow evaluator \ngraphs. Recently, Sreedhar and Gao have used a data structure called the D.1-graph to design a fast algo\u00adrithm \nthat takes 0( IE I) time for ~-function placement, and for computing sparse dataflow evaluator graphs. \nTheir al\u00adgorithm uses an 0( IEI ) query procedure for finding conds sets. In contrast, our approach requires \na small amount of preprocessing to build the A P7 data structure, the pay-off being proportional time \naccess to conds sets. The algorithm of Cytron et al can be viewed as one extreme of our algo\u00adrithm, when \nthere is full caching of dominance frontiers; similarly, the Sreedhar and Gao algorithm can be viewed \nas the other extreme of our algorithm, when no caching is performed. One final remark is in order although \nthe running time of the algorithm in Figure 10 is 0( IE I), the algorithm may not be optimal since its \nrunning time is not necessarily proportional to the size of the output. Is there an optimal algorithm \nfor SSA construction? 6 Conclusions and Related Work Control dependence was first defined by Ferrante, \nOtten\u00adstein and Warren [FOW87]. They also described the con\u00adtrol dependence graph, and gave an optimal \nalgorithm for cd queries, which used the postdominator tree to enumer\u00adate cd sets in proportional time. \nCytron, Ferrante and Sarkar described quadratic time algorithms for conds and Procedure @placement (N: \nset of nodes); , % A P T data structure on reverse CFG is global 2: Create a Priority Queue PQ; 3: Insert \nnodes in set N into PQ; 4: In tree 7 , mark all nodes belonging to set N; 5: 6: while PQ is not empty \ndo 7: q := ExtractMax(pQ); 8: output q; 9: QueryIncr(q); 10: od ; 11: Delete marks from nodes in T; \nProcedure QueryIncr(QueryNode); { 1: VisitIncr(QueryNode, QueryNode); } Procedure { 1: for 2: 3: 4: 5: \n6: 7: 8: 9: 10: 11: 12: 13: VisitIncr(QueryNode,VkitNode); each route i in L [VisitNode] in list order \ndo let .4[i] be [b, t); let reverse CFG edge corresponding to Route ibeu ~ b; if t is strict ancestor \nof QueryNode then if t{ is not marked then Mark u; Insert u into PQ; endif; else break; 9?0 exit from \nthe loop od ; 14: if VkitNode is not a boundary node 15: then 16: for each child C of VisitNode 17: do \n18: if C is not marked 19: then VkiitIncr(QueryNode,C); 20: od ; 21: endif; } Figure 10: Computing Iterated \nDominance Frontiers cdequiv queries [CFS90]. Sreedhar and Gao investigated the conds problem using their \nDJ-graph representation, but this approach did not reduce the asymptotic complexity of conds computation \n[SGL94]. The cdequiv problem for reducible control flow graphs was solved by Ball [Ba193] who needed \nboth dominator and postdominator informa\u00adtion in his solution; subsequently, Podgurski gave a linear\u00adtime \nalgorithm for~orward control dependence equivalence, which is a special case of general control dependence \nequiv\u00adalence [Pod93]. The general cdequiv problem was solved finally by Johnson, Pearson and Pingali \nwho designed an op\u00adtimal algorithm which required 0( IEl ) preprocessing time and space, and which enumerated \ncdequiv sets in pro\u00adportional time [JPP94]. This algorithm requires neither dominator nor postdominator \ninformation, and permits the computation of the cdequi.v relation in less time than it takes to compute \nthe postdominance relation ! There are many alternatives to the zone construction al\u00adgorithm given here. \nFor example, instead of searching the subtree below a query node for the bottom ends of chariot routes, \nwe can search the path from the query node to END for the top ends of relevant chariot routes. In general, \nthere is a trade-off between the sophistication of the query procedure and the amount of caching in AKT, \nfor a given query time. For example, we can use cdequiv information in answering conds queries. It can \nbe shown that the nodes in a cdequiv equivalence class are ordered by the ancestor relation in the postdominator \ntree [JPP94]. Given a query conds(v), we can answer instead the query conds(w) where w is the node in \nthe cdequiv class of v that is lowest is the tree; this lets the query procedure avoid examining nodes \non the path [v, w), which can be exploited during zone construction to reduce caching. Although we have \nused the term caching to describe APT, note that most caching techniques for search prob\u00adlems, such as \nmemorization and related ideas used in the the\u00adorem proving community [Mic68, SS93], perform caching \nat run time (query time). In contrast, caching in ART is performed during preprocessing, and the data \nstructure is not modified by query processing. This permits us to get a grip on storage requirements, \nwhich is difficult to do with run time approaches, Finally, we note that there is a deep connection between \nAPT, and the use of factoring to re\u00adduce the size of the CDG [CFS90]. Factoring identifies nodes that \nhave control dependence in common, and cre\u00adates representations which permit control dependence to be \nshared by multiple nodes. The simplest kind of factoring exploits cdequiv sets. If p nodes are in a cdequiv \nset, and have q routes in common, we can introduce a junction node, connect the q routes to the junction, \nand introduce edges from the junction to each of the p nodes. In this way, the number of edges in the \ndata structure is reduced from p * q top+ q. Exploitation of cdequiv information alone is not adequate \nto reduce the asymptotic size of the graph, but the idea of sharing routes can be extended for ex\u00adample, \nfactoring is possible when the routes containing a node VI area subset of the routes containing node \nV2. How\u00adever, no factorization to date has reduced worst-case space requirements, To place the APT data \nstructure in perspec\u00adtive, note that it can be viewed as a factored representation since a route is cached \njust once per zone, and that entry is shared by all nodes in the zone. However, there is an important \ndifference between the traditional approaches to factorization, and the one that we have adopted in APT. \nIn previous factorization, every route encountered during query processing is reported as output. In \nour approach, the query procedure may encounter some irrelevant routes which must be filtered out , but \nthere is a guarantee that the number of irrelevant routes encountered during query prc)cessing is at \nmost some constant fraction of the actual output. By permitting this slack in the query procedure, we \nare successful in reducing space and preprocessing time requirements without affecting asymptotic query \ntime. More generally, the approach to conds described in this paper can be viewed as an example of Chazelle \ns jiltered search [Cha86], a technique used in computational geometry to solve range search problems. \nIn these problems, a set of geometrical objects in R~ is given. A query is made in the form a connected \nregion in R~, and all objects intersecting this region must be enumerated. To draw the analogy, we can \nview the routes in our problem as geometric objects, and we can view the query node as the analog of \nthe query region; clearly, the conds problem asks for enumeration of all objects that intersect the query \nrange . Filtered search exploits the fact that to report k objects, it takes Q(k) time. Therefore, we \ncan invest O(k) time in an adaptive search technique that is relatively less efficient for large k than \nit is for small k. In our solution to the conds problem, nodes contained in a large number of routes \nare allowed to be in -zones with a large number of nodes; therefore, a quely at such a node may visit \na large number of nodes, but this overhead is amortized over the size of the output. Correspondingly, \nthe search procedure visits a small number of nodes if the query node has only a small amount of output. \nThis kincl of search procedure with adaptive caching may prove useful in solving other problems in the \ncontext of restructuring compilers. Acknowledgments: We thank Richard Johnson and V. Sreedhar for their \nimplementations, and for feedback. Richard also played a major role in designing the control dependence \nequivalence algorithm reported in PLDI 94. Paul Chew, Mayan Moudgill, Michael Wolfe, Eric Stoltz, Richard \nSchooler and Ruth Pingali caught errors in drafts of this paper. References [ABC+ 88] Frances Allen, \nMichael Burke, Ron Cytron, Jeanne Ferrante, Wilson Hsieh, and Vivek Sarkar. A frame\u00ad work for determining \nuseful parallelism. In Pro\u00ad ceedings of the 1988 International Conference on [Ba193] [BR91] [CF93] [CFR+91] \n[CFS90] [Cha86] [CicBC] [CLR92] [Fis81] [FOW87] [GPS90] [GS87] [Har85] Supercomputing, pages 207 215, \nSt. Malo, France, July 4-8, 1988. Thomas Ball. What s in a region? or computing control dependence regions \nin near-linear time for reducible control flow ACM Letters on Program\u00adming Languages and Systems, 2(1-4): \n1 1 6, March-December 1993. David Bernstein and Michael Rodeh. Global instruc\u00adtion scheduling for superscalar \nmachines In Pro\u00adceedings of the SIGPLAN 91 Conference on Pro\u00adgramming Language Design and Implementation, \npages 241-255, Toronto, Ontario, June 26-28, 1991. Ron Cytron and Jeanne Ferrante. Efficiently comput\u00ading \nqLnodes on-the-fly. In Proceedings of the Sixth Workshop on L.unguages and Compilers for Parallel Computing, \npages 461-476, August 1993. Published as Lecture Notes in Computer Science, number 768. R, Cytron, J. \nFerrante, B. K. Rosen, M. N. Wegman, and F. K. Zadeck. Efficiently computing static single assignment \nform and the control dependence graph. ACM Transactions on Programming Languages and Systems. 13(4):451490, \nOctober 1991. Ron Cytron, Jeanne Ferrante. and Vivek Sarkar. Compact representations for control dependence \nIn Proceedings of the SIGPLAN 90 Conference on Pro\u00adgramming Language Design and Implementation, pages \n337 35 1, White Plains, New York, June 20 22, 1990. Bernard Chazelle. Filtering search: A new approach \nto query answering. SIAM .loumal of Computing, 15:703-724, 1986. Marcus Tullius Cicero. Pro L. Cornelio \nBalbo Oratio 39. Published by Senate of Rome, Rome, 56 BC. Thomas Cormen, Charles Leiserson, and Ronald \nRivest. Introduction to Algorithms. The MIT Press, Cambridge, MA, 1992. Josh Fisher. Trace scheduling: \na technique for global microcode compaction. IEEE Transactions on Com\u00adputers, 7(3):478-490, 1981. J, \nFerrante, K, J. Ottenstein, and J. D. Warren. The program dependency graph and its uses in opti\u00admization, \nACM Transactions on Programmmg Lan\u00adguages and Systems, 9(3):3 19 349, June 1987. Rajiv Gupta, Lori Pollock, \nand Mary Lou Soffa. Par\u00ad allelizing data flow analysis. In Proceedings of the Workshop on Parallel Compilation, \nKingston, On\u00ad tario, May 6-8, 1990. Queen s University. Rajiv Gupta and Mary Lou Soffa Region scheduling. \nIn 2nd International Conference on Supercomputing, pages 141-148, 1987. D. Hare]. A linear time algorithm \nfor finding dom\u00adinators in flowgraphs and related problems, In Pro\u00adceedings of the 17th ACM Symposium \non Theory of Computing, pages 185 194, Providence, Rhode Is\u00adland, May 68, 1985. [HPR87] [Joh94] [JP93] \n[JPP94] [L~9] [Mic68] [PBJ+91] [Pod93] [SG95] [SGL94] [ss70] [ss93] [VEBKZ77] Susan Horowitz, Jan Prins, \nand Thomas Reps. In\u00adtegrating non-interfering versions of programs. In Conference Record of the 14th \nAnnual ACM Sym\u00adposium on Principles of Programming Languages, pages 133-145, Munich, West Germany, January \n21 23, 1987, Richard Johnson. EfJicient Program Analysis using Dependence Flow Graphs. PhD thesis, Cornell \nUni\u00adversity, August 1994. Richard Johnson and Keshav Pingali Dependence\u00adbased program analysis. In Proceedings \nof the SIG-PLAN 93 Conference on Programming Language Design and Implementation, pages 78 89, Albu\u00adquerque, \nNew Mexico, June 23-25, 1993. Richard Johnson, David Pearson, and Keshav Pin\u00adgali. The program structure \ntree: Computing control regions in linear time. In Proceedings of the SIG-PLAN 94 Conference on Programming \nLanguage Design and Implementation, pages 171-185, Or\u00adlando, Florida, June 2&#38;24, 1994. Thomas Lengauer \nand Robert Endre Tarj an. A fast al\u00adgorithm for finding dominators in a flowgraph. ACM Transactions on \nProgramming rems, 1(1):121 141, July 1979. Languages and Sys- D. Michie. Memo Nature, 218: 19 22, functions \nand April 1968. machine learning. Keshav Pingali, Micah Beck, Richard Johnson, Mayan Moudgill, and Paul \nStodghill. Dependence Flow Graphs: An algebraic approach to program dependencies. In Conference Record \nof the 18~h An\u00adnual ACM Symposium on Principles of Programming Languages, pages 67-78, January 1991. \nAndy Podgurski. Reordering-transformations that preserve control dependence. Technical Report CES\u00ad93-16, \nCase Western Reserve University, July 1993. Vugranam C. Sreedhar and Guang R. Gao. A lin\u00adear time algorithm \nfor placing q5-nodes, In Confer\u00adence Record of POPL 95: 22nd ACM SIGPLAN-SIGACT Symposium on Principles \nof Programming Languages, pages 62 73, San Francisco, California, January 1995. Vugranam C. Sreedhar, \nGuang R. Gao, and Yong\u00adfong Lee, DJ-graphs and their applications to flow. graph analyses. Technical \nReport ACAPS Memo 70, McGill University, May 1994. R, M. Shapiro and H. Saint. The representation of \nalgorithms. Technical Report CA-7002-1432, Mas\u00adsachusetts Computer Associates, February 1970, A. Segre \nand D. Scharstein. Bounded-overhead caching for definite-clause theorem proving. Journal of Automated \nReasoning, 11 :83 1 13, August 1993. P. Van Erode Boas, R. Kaas, and E, Zijlstra. Design and implementation \nof an efficient priority queue, Mathematical Systems Theory, 10:99 127, 1977. 46  \n\t\t\t", "proc_id": "207110", "abstract": "<p>The <italic>control dependence</italic> relation is used extensively in restructuring compilers. This relation is usually represented using the <italic>control dependence graph</italic>; unfortunately, the size of this data structure can be quadratic in the size of the program, even for some structured programs. In this paper, we introduce a data structure called the <italic>augmented post-dominator tree (APT)</italic> which is constructed in space and time proportional to the size of the program, and which can answer control dependence queries in time proportional to the size of the output. Therefore, <italic>APT</italic> is an <italic>optimal representation of control dependence</italic>. We also show that using <italic>APT</italic>, we can compute SSA graphs, as well as sparse dataflow evaluator graphs, in time proportional to the size of the program. Finally, we put <italic>APT</italic> in perspective by showing that it can be viewed as a factored representation of control dependence graph in which <italic>filtered search</italic> is used to answer queries.</p>", "authors": [{"name": "Keshav Pingali", "author_profile_id": "81100554731", "affiliation": "Department of Computer Science, Cornell University, Ithaca, NY", "person_id": "PP39048331", "email_address": "", "orcid_id": ""}, {"name": "Gianfranco Bilardi", "author_profile_id": "81100228925", "affiliation": "Dipartimento di Elettronica ed Informatica, Universit&#224; di Padova, Padova, Italy", "person_id": "PP40025096", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/207110.207114", "year": "1995", "article_id": "207114", "conference": "PLDI", "title": "APT: a data structure for optimal control dependence computation", "url": "http://dl.acm.org/citation.cfm?id=207114"}