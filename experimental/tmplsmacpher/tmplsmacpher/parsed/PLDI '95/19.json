{"article_publication_date": "06-01-1995", "fulltext": "\n The LRPD Test: Speculative Run-Time Privatization and Reduction Parallelization I?arallelization of \nLoops t with Lawrence University Rauchwerger and David Padua of Illinois at Urbana-Champaign Abstract \nCurrent parallelizing compilers cannot identify a significantfrac\u00adtion of parallelizable loops because \nthey have complex or statically insufficiently defined access patterns. As parallelizable loops arise \nfrequently in practice, we advocate a novel framework for their identification: speculatively execute \nthe loop as a do all, and ap\u00adply a fully parallel data dependence test to determine if it had any cross-iteration \ndependence; if the test fails, then the loop is re executed serially. Since, from our experience, a significant \namount of the available parallelism in Fortran programs can be exploited by loops transformed through \nprivatization and reductionparalleliza\u00adtion, our methods can speculatively apply these transformations \nand then check their validity at run-time. Another important contribu\u00adtion of this paper is a novel method \nfor reduction recognition which goes beyond syntactic pattetm matching: it detects at run-time if the \nvalues stored in art array participate in a reduction operation, even if they are transferred through \nprivate variables and/or are affected by statically unpredictable control flow, We present experimental \nresults on loops from the PERFECT Benchmarks which substanti\u00adate our claim that these techniques can \nyield significant speedups which are often superior to those obtainable by inspector/executor methods. \n Introduction To achieve a high level of performrmce for a particular program on today s supercomputers, \nsoftware developers are often forced to tediously hand-code optimization tailored to a specific ma\u00adchine. \nSuch hand-coding is difficult, increases the possibility of error over sequential programming, and the \nresulting code may not be portable to other machines. Restructuring, or parallelizing, compilers address \nthese problems by detecting and exploiting par\u00adallelism in sequential programs written in conventional \nlanguages. 2The authors are with the Center for Supercomputirsg Research &#38; Development, 1308 W. Main \nSt., Urbana, IL 61801, emait: rwerger, padual?csrd. uiuc. edu. Research supported m part by Intel and \nNASA Graduate Fellowships, and Army contract #DABT63-92\u00adC-0033. Th work is not necessamty representative \nof the posmions or policies of the Army or the Government. Permission to copy without fee all or part \nof this material is granted provided that the copies are not made or distributed for direct commercial \nadvantage, the ACM copyright notice and the title of the publication and its date appear, and notice \nis given that copying is by permission of the Association of Computing Machinery.To copy otherwise, or \nto republish, requires a fee and/or s~ecific Permission. SIGPLAN 95ia Jolla, CA USA Q 1995 ACM 0-89791 \n-697-2/95/0006 ...$3.50 218 Although compiler techniques for the automatic detection of par\u00adallelism \nhave been studied extensively over the last two decades (see, e.g., [25, 36]), current parallelizing \ncompilers cannot extract a significant fraction of the available parallelism in a loop if it has a complex \nand/or statically insufficiently defined access pattern. One major reason for this inability to statically \nparallelize some programs is that the most effective transformations, privatiza~iorz and reduc-Eion recognition, \ncannot be applied to a large class of applications that have irregular domains and/or dynamically changing \ninterac\u00adtions. Typical examples are complex simulations such as SPICE for circuit simulation, DYNA-3D \nand PRONTO-3D for structural mechanics modeling, GAUSSIAN and DMOL for quantum me\u00adchanical simulation \nof molecules, CHARMM and DISCOVER for molecular dynamics simulation of organic systems, and FIDAP for \nmodeling complex fluid flows [1 1]. Thus, in order to realize the full potential of parallel comput\u00ading \nit has become clear that static (compile time) analysis must be complemented by new methods capable of \nautomatically extracting parallelism at run time [9, 11, 14]. Run time techniques can suc\u00adceed where \nstatic compilation fails because they have access to the input data. For example, input dependent or \ndynamic data distribu\u00adtion, memory accesses guarded by run-time dependent conditions, and subscript expressions \ncan all be analyzed unambiguously at run-time. In contrast, at compile time the access pattern of some \nprograms cannot be determined, sometimes due to limitations in the current anrdysis algorithms but often \nbecause the necessary infor\u00admation is just not available, i.e., the access pattern is a function of the \ninput data. For example, most dependence analysis algorithms can only deal with subscript expressions \nthat are lines in the loop indices. In the presence of non linear expressions, a dependence is usually \nassumed. Also, generally compilers conservatively as\u00adsume data dependence in the presence of subscripted \nsubscripts. Although more powerful analysis techniques could remove this last limitation when the index \narrays are computed using only statically known values, nothing can be done at compile time when the \nindex arrays are a function of the input data [20, 30, 38]. Most previous approaches to run-time parallelization \nhave con\u00adcentrated on developing methods for constructing execution sched\u00adules for partially parallel \nloops, i.e., loops whose parallelization requires synchronization to ensure that the iterations are executed \nin the correct order. 1 These methods are centered around the extraction of an inspector loop that analyzes \nthe data access pattern off line, 1The only exception of which we are aware is our inspector method for \ndeal 1 parallelrzation [26]. Run time analysis techniques have also been used to detect access anomahes \nor race conditions in paratlel programs (see, e.g., [13, 24, 311). However, these rnerhods are generauy \nnot appropriate for run-time loop parallehzation since they are optunized for other purposes, e.g., for \nthem minimizing memory requirements is more unportant than speed. i.e., without side effects [8, 20,23,26, \n28,29,30,37, 38, 12]. The inspection phase of these schemes usually yields a partitioning of the set \nof iterations into subsets that can be executed in parallel. These subsets, sometimes called wavejkvsts, \nare scheduled sequentially by placing synchronization barriers between them. Unfortunately the distribution \nof the original loop into art irt\u00ad spector and executor loop is often not advantageous: if dte address \ncomputation of the array under test depends on the acttral data cclm\u00ad putation, as exemplified by Fig. \n1(a), then the inspector becomes both computationally expensive and has side-effects. This means that \nshared arrays would be modified during the execution of the inspector loop and saving the state of these \nvariables would be required -making the inspector equivalent to the loop itself. In ad\u00ad dition, the desirable \ngoal of exploiting coarse-grain parallelization, i.e., at the level of large complex loops, makes it \neven less likely that an appropriate inspector loop cart be extracted. 1.1 Speculative do all parallelization \nIn this paper we propose a novel framework for parallelizing do loops at run-time. The proposed framework \ndiffers conceptually from previous methods in two major points. Instead of finding a valid parallel execution \nschedule for the loop, we focus on the problem of simply deciding if the loop is fully parallel, that \nis, determining whether or not the loop has cross-iteration dependence. (This approach was also taken \nin [26].) Instead of dk.tributing the loop into inspector and executor 100IPS, we speculatively exe;ute \nthe-loop as a doall, i.e., execute all its iterations concurrently, and apply a run-time test to check \nif there were any cross-iteration dependence. If the run-time test fails, then we will pay a penalty \nin that we need to backtrack and re execute the loop serially. Compilers often transform programs to \noptimize performance. The two most effective transformations for increasing the amount of parallelism \nin a loop (i.e., removing certain types of data de\u00adpendence) are array privatization and reduction parallelization. \nKrothapalli and Sadayappart [15] proposed an inspector method for run-time privatization which relies \nheavily on synchronization, in\u00adserts an additional level of indirection into all memory accesses, and \ncalls for dynamic shared memory allocation. In our previews work [26] we gave art inspector method without \nthese drawbacks for determining whether a do loop can be executed as a deal.1, perhaps by privatizing \nsome shared variables. No previous run-time methods have been proposed for parallelizing reduction operations. \nIn this paper we present several new ideas: First, we advocate the use of run-time tests to validate \nthe execution of a loop that is speculatively executed in p~allel. The adv~tage of this approach is that \nthe computation of the loop is performed concurrently with lhe tests, i.e., the memory access pattern \ndoes not need to be extracted and analyzed separately as in inspector/executor methods.z In Sec\u00adtion \n5 we present experimental results on loops from the PERFECT Benchmarks which substantiate our claim that \nspeculative tech\u00adniques can yield significant speedups which are often superior to those obtainable by \ninspector/executor methods. Second, in addi\u00adtion to array privatization, the new techniques are capable \nof testing at run-time the validity of the powerful reduction parallelization 2If desired, all of our \nrun-time tests can be applied in inspector/executor mode. transformation. In particular, for an array \nelement (or section), our run-time methods are able to detect whether it participated exclu\u00adsively in \na reduction operation, or if all its accesses were either read-only or privatizable. If all the memory \nreferences in a do loop fall under any of these categories then the speculative concurrent execution \nof the loop was valid, i.e., the loop was indeed paral\u00adlel. The new algorithms consider only data dependence \ncaused by actual cross-iteration data-flow (a flow of vahses). Thus, they may potentially qtral@ more \nloops as parallel than the method in [26] which conservatively considered the dependence due to every \nmemory reference -even if no cross-iteration data-flow occurred at run-time. This situation could arise \nfor example when a loop reads a shared variable, but then only uses it conditionally. Another important \ncontribution of t.hk paper is a novel method for reduction recognition: in contrast to the static pattern \nmatching techniques employed by compilers until now, our method detects if the values stored in an array \nparticipate in a reduction operation, even if they are tmnsferred through private variables and/or are \naffected by statically unpredictable control flow. Our methods for speculatively executing do loops in \nparallel are described in Sections 3 and 4. In Section 5, we present some ex\u00adperimental measurements \nof loops from the PERFECT Benchmarks executed on the Alliant FX/80 and 2800. These measurements show \nthat the techniques presented in this paper are effective in produc\u00ading scalable speedups even though \nthe run-time analysis is done without the help of any special hardware devices. It is conceivable, and \nwe believe desirable, that future machines would include spe\u00adcial hardware devices to accelerate the \nrun-time analysis and in this way widen the range of applicability of the techniques and increase potential \nspeedups. 2 Preliminaries A loop can be executed in fully parallel form, without synchro\u00adnization, if \nand only if the desired outcome of the loop does not depend in any way upon the execution ordering of \nthe data accesses from different iterations. In order to determine whether or not the ex\u00adecution order \nof the data accesses affects the semantics of the loop, the data dependence relations between the statements \nin the loop body must be anrdyzed [6, 18,25,36, 39]. There are three possible types of dependence between \ntwo statements that access the same memory location: flow (read after write), anti (write after read), \nand output (write after write). Flow dependence express a fundamental relationship about the data flow \nin the program. Anti and output de\u00adpendence, also known as memory-related dependence, are caused by the \nreuse of memory, e.g., program variables. If there are flow dependence between accesses in different \niter\u00adations of a loop, then the semantics of the loop cannot be guaranteed if the loop is executed in \nfully parallel form. For example, the iter\u00adations of tie loop in Fig. 1(a) must be executed in order \nof iteration number because iteration i + 1 needs the value that is produced in iteration i, for 1 ~ \ni < n. In principle, if there are no flow dependence between the iterations of a loop, then the loop \nmaybe executed in fully parallel form. The simplest situation occurs when there are no anti, outpu~ or \nflow dependence. In thk case, all the iterations of the loop are independent and the loop, as is, can \nbe executed as a doa 11 (i.e., a fully parallel execution). If there are no flow dependence, but there \nare anti or output dependence, then the loop must be modhled to remove all these dependence before it \ndol=l, n doi=l, n/2 do i=l, n A(K(l)) = A(K(i)) + A(K(i-1)) Sl: tmp = A(2*i) doj=l, m = if (A(K(i)) .eq. \ntrue.) ................. endif enddo (a) then S2 : A(2*i) A(2*i-1) enddo (b) Figure cam be executed \nin parallel. Not all such situations can be handled efficiently. In order to remove certain types of \ndependence and execute the loop as a doa 11, two important and effective trans\u00adformations can be applied \nto the loop: privatization and reduction parallelization. l+ivatiza~ion creates, for each processor cooperating \non the exe\u00adcution of the loop, private copies of the program variables that give rise to anti or output \ndependence (see, e.g., [10, 21, 22, 32, 33]). The loop shown in Fig. 1(b), is an example of a loop that \ncan be executed in parallel by using privatization; the anti dependence between statement S2 of iteration \ni and statement S 1 of iteration i + I,, for I s i < n/Z, can be removed by privatizing the tempo\u00adrary \nvmiable tmp. h this paper, the following criterion is used to determine whether a variable maybe privatized. \nPrivatization Criterion: Let Abe a shared array (or array section) that is referenced in a loop L. A \ncan be privatized if and only if every read access to an element of A is preceded by a write access to \nthat same element of A within the same iteration of L. In general, dependence that are generated by accesses \nto variables rhat are only used as workspace (e.g., temporary variables) within an iteration can be eliminated \nby privatizing the workspace. However, according to the above criterion, if a shared variable is initialized \nby reading a value that is computed outside the loop, then that variable cannot be privatized. Such variables \ncould be privatized if a copy in mechanism for the external value is provided. The last value assignment \nproblem is the conceptual analog of the copy in problem. If a privatized variable is live after the termination \nof the loop, then the privatization technique must ensure that the correct value is copied out to the \noriginal (non privatized) version of that variable. It should be noted that the need for values to be \ncopied into or out of private variables occurs infrequently in practice. Reduction parallelization is \nanother important technique for transforming certain types of data dependent loops for concurrent execution. \nDefinition: A reduction variable is a variable whose value is used in one associative and commutative \noperation of the form z = x @ezp, where @ is the associative and commutative operator and z does not \noccur in ezp or anywhere else in the loop. Reduction variables are therefore accessed in a certain specific \npat\u00adtern (which leads to a characteristic data dependence graph). A sirm ple but typical example of a \nreduction is statement S 1 in Fig. 1(c). The operator @ is exemplified by the + operator, the access \npattern of array A(:) is read, modijj, write, and the function performed by the loop is to add a value \ncomputed in each iteration to the value stored in A(:). This type of reduction is sometimes called an \nup dare and occurs quite frequently in programs. There are two tasks required for reduction parallelization: \nrec\u00adognizing the reduction variable, and parallelizing the reduction operation. (In contrast, privatization \nneeds only to recognize pri\u00advatizable variables by performing data dependence analysis, i.e., it A(2*i-1) \nSl: A(,I)= A(j) + expo = tmp enddo enddo (c) 1: is contingent only on the access pattern and not on the \noperations.) Parallel metlods are known for performing reduction operations. One typical method is to \ntransform the do loop into a doall and enclose the access to the reduction variable in an unordered critical \nsection [14, 39]. Drawbacks of this method are that it is not scalable and requires synchronizations \nwhich can be very expensive in large multiprocessor systems. A scalable method can be obtained by noting \nthat a reduction operation is an associative and commutative recurrence and can thus be parallelized \nusing a recursive doubliig algoridun [16, 17, 19]. In this case the reduction variable is priva\u00adtized \nin the transformed doall, and the final result of the reduction operation is computed in an interprocessor \nreduction phase follow\u00ading the doall, i.e,, a scalar is produced using the partial results computed in \neach processor as operands for a reduction operation (with the same operator) across the processors. \nThus, the difficulty encountered by compilers in parallelizing loops with reductions arises not from \nfinding a parallel algorithm but fkom recognizing the reduction statements. So far this problem has been \nhandled at compile time by syntactically pattern matching the loop statements with a template of a generic \nreduction, and then performing a data dependence analysis of the variable under scrutiny to guarantee \nthat it is not used anywhere else in the loop except in the reduction statement [39].  3 Speculative \nParallel Execution of do Loops Consider a do loop for which the compiler cannot statically determine \nthe access pattern of a shared array A that is referenced in the loop. Instead of generating pessimistic, \nsequential code when it cannot unequivocally decide whether the loop is parallel. the compiler could \ndecide to speculatively execute the loop as a doa 11, and produce code to determine at run-time whether \nthe loop was in fact fully parallel. In addition, if it is suspected that some data dependence could \nbe removed by privatization and/or reduction parallelization the compiler may further speculatively apply \nthese transformations in order to increase the chances that the loop can be executed as a doa 11. If \nthe subsequent run-time test finds that the loop was not fully parallel, then it will be re-executed \nsequentially. In order to speculatively parallelize ado loop as outlined above we need the following: \nA mechanism of saving/restoring state: to save the original values of the program variables for the possible \nsequential re\u00adexecution of the loop. c An error (hazard) detection method: to test the validity of the \nspeculative parallel execution. o An automatable strategy: to decide when to use speculative parallel \nexecution. Saving/Restoring State. There are several ways to maintain back\u00adups of the program variables \nthat may be altered by the speculative parallel execution. If the resources (time and space) needed to \ncreate a backup copy are not too big, then a practical solution is check\u00adpointing prior to the speculative \nexecution. It might be possible to reduce this cost by identifying and checkpointing a point of mini\u00admum \nstate in the program prior to the speculative parallel execution. A more attractive solution is to privatize \nall shared variables, copy\u00adin (on demand) any needed external values, and copy-out any live values if \nthe test passes, thereby committing the results computed by the doall loop. This method could also yield \nbetter data lo\u00adcality and reduce the number of messages between processors (e.g., it would generate less \ncoherency traffic in a cache coherent dis\u00adtributed shared-memory machine). Note that privatized arrays \nneed not be backed up because the original version of the array will not be altered during the parallel \nexecution. Hazard Detection. There are essentially two types of errors (haz\u00adards) that could occur during \nthe speculative parailel execution: (i) exceptions and (ii) the presence of cross-iteration dependence \nin the loop. A simple way to deal with exceptions is to treat them as an invalid parallel execution, \ni.e., if an exception occurs, abandon the parallel execution, clear the exception flag, restore the vahtes \nof any altered program variables, and execute the loop sequentially. Below, we present techniques that \ncan be used to detect the presence of cross-iteration dependence in the loop and to test the valifiity \nof any privatization and/or reduction parallelization transformations that were applied. An Automatable \nStrategy. The main factors that the compiler should consider when deciding whether to speculatively paralle \nlize a loop are: the probability that the loop is a deal 1, the speedup obtained if the loop is a doall, \nand the slowdown incurred if the loop is not a deal 1. For example, the compiler might base its decision \non a ratio of the estimated run-time cost of an erroneous pwallel execution to the estimated run-time \ncost of a sequential execution. If this ratio is small, then significant performance gains would result \nfrom a successful (valid) parallelization of the loop, at the risk of increasing the sequential execution \ntime by only a small amount. In order to perform a cost/benefit analysis artcl to predict the parallelism \nof the loop, the compiler should use static analysis and run-time statistics (collected on previous executions \nof the loop or from different codes); in addhion, directives about the parallelism of the loop might \nprove useful. In Section 4.1 a complexity analysis of our run-time tests is presented that can be used \nto statically predict the minimum obtainable speedup and the maximum potential slowdown for a loop parallelized \nusing our tectilques. 3.1 Run-time data dependence analysis In this section we describe an efficient \nrun-time technique that can be used to detect the presence of cross iteration dependence in a loop that \nhas been speculatively executed in parallel. If there are any such dependence, then this test will not \nidentify them, it will only flag their existence. We note that the test need only be applied to those \nscalars and arrays that cannot be analyzecl at compile-time. In addition, if any shared variables were \nprivatized for the speculative parallel execution, then this test can determine whether those variables \nwere in fact validly privatized. An important source of ambiguity that cannot be anaiyzed stati\u00adcally \nand potentially generates overly conservative data dependence models is the run-time equivalent of dead \ncode. A simple example is when a loop first reads a shared array element into a local vari\u00ad able but \nthen only conditionally uses it in the computation of other shared variables. If the consumption of the \nread value does not ma\u00ad terialize at run-time, then the read access did not in fact contribute to the \ndata flow of the loop and therefore could not have caused a dependence. Since predicates seldom can be \nevaluated statically, the compiler must be conservative and conclude that the read access causes a dependence \nin every iteration of the loop. The test given here improves upon the Privatizing doall test described \nin [26] by checking only the dynamic data dependence caused by the actual cross-iteration flow of values \nstored in the shared arrays. Thk is accomplished using a technique we call dynamic dead reference elimination \nwhich is explained in detail following the description of the test. The most general version of the test, \nas applied to a privatized shared array A, is given below, i.e., it tests for all types of de\u00ad pendence, \nand also whether the array is indeed privatizable. If some of these conditions do not need to be verified, \nthen the test can be simplified in a straightforward manner, e.g., if the array was not privatized for \nthe speculative parallel execution, then all steps pertaining to the privatization check are omitted. \nThe Lazy (value-based) Privatizing doall Test (LPD Test) 1. Marking Phase. (Performed during the speculative \nparallel execution of the loop,) For each shared array A[l : s] whose dependencescannotbe determined \nat compile time, we declare read and write shadow arrays, Ar [1:s]and Aw [1 : s], respectively. In addition, \nwe declare a shadow array AnP[l : s] that will be used to flag array elements that cannot be vaiidly \nprivatized. Inhially, the test assumes that all array elements are privatizable, artd if it is found \nin any iteration that the value of an element is used (read) before it is redefined (written), then it \nwill be marked as not privatizable. The shadow arrays A., A~, and A~P are initialized to zero. During \neach iteration of the loop, all definitions or uses of the values stored in the shared array A are processed \n(a) Definitions (done when the value is written): set the element in Aw corresponding to the array element \nthat is modified (writ\u00adten). (b) Uses (done when the value that was read is used): if this array element \nis never modified (written) in this iteration, then set the corresponding element in A.. If the value \nstored in this array element has not been written in this iteration before this use (read access), then \nset the corresponding element in A~P, i.e., mark it as not privatizable. (c) Count the total number \nof write accesses to A that are marked in this iteration, and store the result in tw~ (A), where i is \nthe iteration number.  2. Analysis Phase. (Performed after the speculative parallel execu\u00adtion.) For \neach shared array A under scrutiny: (a) Compute (i) tw(A) = ~ twi(A), i.e., the total number of definitions \n(writes) that were marked by all iterations in the loop, and (ii) i!m(A) = mm(Aw [1 : s]), i.e., the \ntotal number of marks in AWII : s]. (b) If my(Aw [:] A A~[:]),3 i.e., if the marked areas are common \nanywhere, then the loop is nor a deal 1 and the phase ends. (Since we define (write) and use (read, but \ndo not define) values stored at the same location in different iterations, there is at least  one flow \nor anti dependence.) 9ang returns the OR of its vector operand s elements, i.e., any(v[l : 4) =(411v \nt42] v ...v +Z]). (c) Else if tw(A) = hn(A), then the loop is a doall (without privatizing the array \nA). (Since we never overwrite any memory location, there are no output dependence.) (d) Else if any(AW \n[:] A Amp[:]), then the array A is no~privatizable. Thus, the loop, as executed, is not a doall and the \nphase ends. (There is at least one iteration in which some element of A was used (read) before it was \nbeen modified (written).) (e) Otherwise, the loop was made into a do all by privatizing the shared array \nA. (We remove all memory-related dependence by privatizing this array.)  Dynamic dead reference elimination. \nWe now describe how the marking of the read and private shadow arrays, A, and Amp, can be postponed until \nthe value of the shared variable is actually used (Step l(b)). More formally, the references we want \nto identify are defined as follows. Definition: A dynamic dead read reference is a read access of a shared \nvariable that both (a) does not contribute to the computation of any other shared vari\u00adable, and (b) \ndoes not control (predicate) the references to other shared vari\u00adables.  The value obtained through \na dynamic dead read does not con\u00adtribute to the data flow of the loop. Ideally, such accesses should \nnot introduce false dependence in either the static or the run-time dependence analysis. If it is possible \nto determine the dead refer\u00adences at compile time then we can just ignore them in our analysis, Since \nthis is generally not possible (control flow could be input dependent) the compiler should identify the \nreferences that have the potential to be unused and insert code to solve this problem at run-time. In \nFig. 3.1 we give an example where the compiler can identify such a situation by following the clef-use \nchain built by using array names only. To avoid introducing false dependence, the marking of the read \nshadow array is postponed until the vahte that is read into the loop space is indeed used in the computation \nof other shared variables, In essence we are concerned with the flow of the values stored rather than \nwith their storage (addresses). We note that if the search for the actual use of a read value becomes \ntoo complex then it can be stopped gracetitlly at a certain deprh and a conservative marking of the shadow \narray can be inserted (on all the paths leading to a possible use). As can be observed from the example \nin Fig. 3.1, this method allows the LPD test to qualify more loops for parallel execution then would \nbe otherwise possible by just inspecting the memory references as in the original PD test [26]. In particular, \nafter marking and counting we obtain the results depicted in the tables. The loop fails the PD test since \nAu(:) A A,(:) is not zero everywhere (Step 2(b)). However, the loop passes the LPD test as AW(:)AAJ:) \nis zero everywhere, but only after privatization, since tw(d) # tin(A) and Aw (:) A AnP(:) is zero everywhere. \nPrivate shadow structures. The LPD test can take advantage of the processors private memories by using \nprivate shadow strictures for the marking phase of the test. Then, at the conclusion of the private marking \nphase, the contents of the private shadow structures are merged into the global shadow structures. Note \nthat since the order of the writes (marks) to an element of the shadow structure is not important, all \nprocessors can transfer their private shadow structures to the global structure without synchronization. \nIn fact, using private shadow structures enables some additional optimiza\u00adtion of the LPD test as follows. \nSince the shadow structures are private to each processor, the iteration number can be used as the mark. \nIn this way, no re irtitialization of the shadow structures is required between successive iterations, \nand checks such as has this element been written in this iteration? simply require checking if the corresponding \nelement in Aw is marked with the iteration number, Another benefit of the iteration number marks is that \nthey can double as time-stamps, which are needed for performing the last value assignment to any shared \nvariables that are live after loop termination. A processor-wise version of the LPD test. The LPD Test \ndeter\u00admines whether a loop has any cross-iteration data dependence. It turns out that essentially the \nsame method can be used to test whether the loop, as executed, has any cross processor data dependences.4 \nThe only difference is that all checks in the test refer to proces\u00adsors rather than to iterations, i.e., \nreplace iteration by processor in the description of the LPD test so that all iterations assigned to \na processor are considered as one super-iteration by the test. Note that a loop that is not fully parallel \ncould potentially pass the processor-wise version of the LPD test because data dependence among iterations \nassigned to the same processor will not be de\u00adtected. This is acceptable (even desirable) as long each \nprocessor executes its assigned iterations in increasing order.  3.2 Run time reduction parallelization \nAs mentioned in Section 2, there are two tasks required for reduction parallelization: recognizing the \nreduction variable, and paralle~izing the reduction operation. Of these, we focus our atten\u00adtion on the \nformer since, as previously noted, techniques are known for performing reduction operations in parallel. \nSo far the problem of reduction variable recognition has been handled at compile-time by syntactically \npattern matching the loop statements with a tem\u00adplate of a generic reduction, and then performing a data \ndependence analysis of the variable under scrutiny to validate it as a reduction variable [39]. There \nare two major shortcomings of such pattern matching identification methods. 1.The data dependence analysis \nnecessary to qualify a statement as a reduction cannot be performed statically in the presence of input-dependent \naccess patterns. 2. Syntactic pattern matching cannot identify all potential reduc\u00adtion variables (e.g., \nin the presence of subscripted subscripts). Below we show how each of these two difficulties can be overcome \nwith a combination of static and run-time methods, 3.2.1 The LRPD test: extending the LPD test for reduc\u00adtion \nvalidation In this section we consider the problem of verifying that a statement is a reduction using \nrun-time data dependence analysis. The poten\u00adtial reduction statement is assumed to syntactically pattern \nmatch the generic reduction tempIate z = z @ e xp; reduction statements that do not meet this criterion \nare treated in the next section. To verify that such a statement is a reduction we need to check that \nthe reduction variable z satisfies the definition given in Section 2, i.e., that z is only accessed in \nthe reduction statement, and that it does not appear in exp. Our basic strategy is to extend the LPD \ntest to check all statically unverifiable reduction conditions. We first consider how the test 4This \nfact was noted by Santosh Abraham[ 1]. do i=l, 5 z = A(K(i)) if (Bl(i) .eq. true.) then A{L(i)) = z \n+C(i) endif enddo B1(1:5)=(1 O 10 1) K(I:5)=(12341) L(1:5) =(22442) (a) Originaf shadow arrays PD test \n1 2 3 4 tw trrs Aw o 1 0 1 3 2 Am 1 1 1 1 A 1 1 1 1 do i=l,5 markread(K(i)) z = A.(K(i)) if (B l(i) \n.eq. true.) markwrite(L(i)) A(L(i)) = z +C(i) endif enddo (b) do i=l, 5 z = A(K(i)) if (B 1(i) .eq. Jrue.) \nthen markread(K(i)) markwrite(L(i)) A( L(I)) = z +C(il . . .. ., endif enddo (c) new LPD test Aw A, A \nshadow 1 2 o 1 1 0 1 0 atrays 3 4 0 1 1 0 1 0 tw 3 tm 2  AW(:) ~Ar(:) o 1 0 1 Aw(:) ~Ar(:) o 0 0 0 AW(:) \nA A~p(:) 01 0 1 Aw(:) A An=(:) o 0 0 0 (d) (e) Figure 2: The transformation of a do loop (a), using \nthe original version of the PD test (b), and the lazy version (c). The markwrite op&#38;ation marks the \nindicated element in the shadow army AW (A-r d Amp) according to the criteria given m-step 1(4 (l b)) \nof tie LpD test. dead read references are not marked in rhe LPD test, the array A fails tk PD test and \npassesthe LPD test, as shown in (d) and (e), respectively. then (markread) Since dynamic would be augmented \nto check only that the reduction variable is not accessed outside the single reduction statement. This \nsituation could arise if the reduction variable is an array element accessed through subscripted subscripts \nand the subscript expressions are not statically analyzable. For example, although statement S3 in the \nloop in Fig, 3(a) matches a reduction statement, it is still necessary to prove that the elements of \narray A referenced in S 1 and S2 do not overlap with those accessed instatement s3, i.e., that K(i) # \nR(j) and L(i) # R(j), for all I s i, j s n. Thus, the LRPD Itest must check at run-time that there is \nno intersection between the references in S3 and those in S1 and/or S2; in addkion it will be used to \nprove, as before, that any cross-iteration dependence in S1 and S2 are removed by privatization. To test \nthis new condition we use another shadow array Amc to flag the array elements that are not valid reduction \nvariables. Initially, all array elements are assumed to be valid reduction variables, i.e., An. [:] = \nfalse. k the marking phase of the test, i.e., during the speculative parallel execution of the loop, \nany array element defined or used outside the reduction statement is invalidated as a reduction variable, \ni.e,, its corresponding element in An= is set to true. As before, after the speculative parallel execution, \nthe analysis phase of the test is performed. Art element of A is a valid reduction variable if and only \nif it was not invalidated during the marking phase, i.e., it was not marked in d~a asnot a reduction \nvariable for any iteration, The other shadow arrays AnP, AW and A. are initialized, marked, and interpreted \njust as before. The LRPD test can also solve the case when the exp part of the RHS of the reduction statement \ncontains references to the array A that are different from the pattern matched LHS and cannot be statically \nanalyzed. To validate such a statement as a reduction we must show that no reference in exp overlaps \nwith those of the LHS, This is done during the marking phase by setting an element of Z&#38; to true \nif the corresponding element of A is referenced in exp. In summary, the LRPD test is obtained by modifying \nthe LPD test. The following step is added to the Marking Phase. 1(a ) Definitions and uses: if a reference \nto A is not one of the two known references to the reduction variable (i.e., it is outside the reduction \nstatement or it is contained in exp), then set the corresponding element of A~m to true (to indicate \nthat the element is not a reduction variable). (See Fig, 3(a) and (b).) In the Analysis Phase, Steps \n2(d) and 2(e) are replaced by the following. 2(a ) Else if I.ZrLy(AW [:] A Anp[:] A Anm[:]), then some \nelement of A written in the loop is neither a reduction variable nor privatizable. Thus, the loop, as \nexecuted, is not a deal and the phase ends. (There exist iterations (perhaps different) in which an element \nof A is not a reduction variable, and in which it is used (read) and subsequently modified.) 2(b ) Otherwise, \nthe loop was made into a doall by parallelizing reduction operations and privatizing the shared array \nA. (All data dependence are removed by these transformations.) If the analysis phase validates (passes) \nthe speculative parallel execution of the loop, then, as before, the last value assignments are performed \nfor any live shared variables, and the scalar result of each reduction is computed using the processors \npartial results in a reduction across the processors. (See Fig. 7.) (If reductions are implemented by \nplacing the reduction statements in unordered critical sections, then this last step is not necessary.) \nMultiple potential reduction statements. A more complicated situation is when the loop contains several \nreduction statements that refer to the same array A. In this case the type of the reduction operation \nperfo~ed on each element must be the s~e ~oughout the loop execution, e.g., a variable caunot participate \nin both a mul\u00adtiplicative and an addhive reduction since the resulting operation is not commutative and \nassociative and is therefore not parallelizable. The solution to this problem is to mark the shadow array \nAn= with the reduction type. Whenever a reference in a reduction statement is marked, the cument reduction \ntype (e.g., summation, multiplica\u00adtion) is checked with with previous one. If they are not the same, \nthe corresponding shadow element of A.= is set to true. h Fig 3(c) and(d), we show how a loop containing \ntwo potential reduction statements with different operators and an exp operand that contains references \nto the array under test can be transformed to perform a run-time dependence and reduction test. The subsequent \nanalysis of the shadow arrays will detect which elements were used in a reduction and which are privatizable \nor read-only. If any doi=l, n Sl: A(K(i)) = ....... S2: ............ = A(L(i)) S3: A(R(i)) = A(R(i)) \n+ doall I= 1, n markwrite(K(i)) markredux(K(i)) Sl: A(K(i)) = ....... markread(L(i)) markredux(L(i)) \nS2: ............ = A(L(i)) markwrite(R(i)) S3: A(R(i)) = A(R(i)) + enddoall doi=l, n Sl: A(S(i)) = A(S(i)) \n+ exp(X(i)) (c) S2: A(R(i)) = A(R(i)) + expo enddo (a) A_nx (:) = false. doall i= l,nexpo markwrite(R(i) \n) if (A_nx(R(l)) me. if (A_nx(R(i)) else A_nx(R(i)) = endif markread(X(i)) markredux(X(i)) true.) me. \n* then * ) marhedux(R(i)) (d)  Sl: A(R(i)) = A(R(i)) + exp(A(X(i))) markwrite(S(i)) if (A_nx(S (i)) \n.ne. true.) then if (A_nx(S(i)) .ne. + ) markredux(S(i)) else (b) expo A_nx(S(i)) = + endif S2: A(S(i)) \n= A(S(i)) + expo enddoall Figure ~: The transformation of the do loops in (a) and (c) M shown m (b) and \n(d), respectively. The markwrite (mark read) operation marks the mdlcated element m the shadow array \nAW (A, and Amp) according to the cntena given in Step 1(a) (1(b)) of the LPD test. The markredux operation \nsets the shadow array element of A.nx to tnre. In (d), the type of the reduction is tested by storing \nthe operator us Amx. element is found not to belong to one of these categories, then the speculative \nparallelization was incorrect and a sequential re execution must be initiated. As a final remark, we \nnote that a more aggressive irnpIementation could promote the type of a reduction at run-time: if a memory \nelement is first involved in a + reduction and then switches over to a * reduction and stays that way \nfor all the remaining references, then the speculative parallel execution can still yield valid partiaI \nresults on each processor. It is important to remember that a reduction type can be promoted in only \none direction (it cannot be demoted back to its initial type) and only once per loop invocation. Of course, \nthe reduction across processors must reflect the reduction operator promotion.   3.2.2 Static reduction \nrecognition and run-time check As mentioned at the beginning of this section, syntactic pattern matching \nis not a sufficiently powerful method to detect all the values that are subject to a reduction operation. \nIn particulm, syntactic pattern matching will fail to identify a reduction whenever all the references \non the RHS of the assignment look different from the reference on the LHS. Thus, if a statement is in \nfact a reduction, but the references on the LHS and/or the RHS are indirect, then syntactic pattern matchmg \nwill fail. This situation could arise naturally, e.g., through the use of temporary variables or subscripted \nsubscripts. In the latter case, it can only be determined at run-time if any of the array elements are \nreduction variables. In the following we show that a combination of static and run time techniques can \nbe used to successfully identify several types of potential reductions that could not be recognized with \npattern matching techniques. The general strategy is to speculate that every assignment to the array \nof interest is a potential reduction, unless proven otherwise statically or by other heuristics. At run-time \nthis assumption is then validated or invalidated on an element by element basis. Single statement reduction \nrecognition We first consider a single statement in which the references on the RHS are either dependent \non the array A (also referenced on the LHS) or are to values known to be independent of A, e.g., constants, \nloop invariants, or distinct global variables. The simplest case is when the RHS contains exactly one \nrefer\u00adence to A. Consider the potential reduction statement A( -R(i)) = A(X(i)) + ecp. If R(i) = X(i), \nfor some values of i, then the probability that the surrounding loop is parallel is increased. In this \ncase, the solution is simply to check this equality condition at run-time, and mark the shadow array \nAm= accordingly. The situation is a bit more complex when the RHS con\u00adtains multiple references to the \narray A. Consider the statement A(R(i)) = A(Xl(i)) + A(XZ(i)) + . . . + A(Xh(i)). This state\u00adment is \na reduction if and only if R(i) = Xj (i) for exactly one value of j (see Section 2). As the operation \nis commutative and associative, we cannot discount the possibility of a reduction. In this example, we \nmust check for equality between R(i) and every X,(i), I < j < k. If this equality condition is not met \nexactly once, then Am= (R(z)) is set to true (to indicate it was not a reduction). We note that a more \naggressive strategy could be taken when there are multiple references to A( R{ i) ) on the RHS: promote \nthe + re\u00adduction to a * reduction. However, as mentioned in Section 3.2.1, the reduction type can only \nbe promoted once in the entire loop. Fig. 4 shows the code generated for run-time validation when the \nRHS contains multiple references to A. In the interest of clarity, reduction type promotion is not shown. \nMultiple statement reduction recognition: Expanded Re\u00adduction Statements We now relax all restrictions \non the RHS and allow in it variables that are neither explicit functions of the array appearing on the \nLHS nor explicit loop invariant. Our goal is to uncover any possible link between the LHS and the RHS, \nif indeed one exists. The general A_nx(:) = false. doall i= l,n private integer count count = o Sl: \nS2: S3: do i=l, n A(K(i)) = ....... ,,.,.,,,,, = A(L(i)) A(S(i)) = A(R(i)) + A(T(i)) + A(X(i)) (a) Sl: \nmarkwrite(K(i)) markredux(K(i)) A(K(i)) = ......... enddo markread(L(i)) markredux(L(i)) function checkequal(x, \ny, et) S2: ......... = A(L(i)) markread(R(i)) (c) if (x .ne. y) then markread(T(i)) markredux(x) markread(X(i)) \nelse Ct=ct+l (b) markwrite(S(i)) checkequal(R(i), S(i), count) endif checkequaI(T(i), S(i), count) return \ncheckequal(X(i), S(i), count) end C type could be promoted if count= 3 if (count .eq. 1) markredux(S(i)) \nS3: A(S(i)) = A(R(i)) + A(T(i)) + A(X(i)) enddoall Figure 4: The code generated for the do loop in (a) \nis shown in (c). In (c), the procedure in (b) is called. The mark-x operations areas described in Fig. \n3. strategy of our methods is a fairly straightforward demand driven forward substitution of all the \nvariables on the RHS, a process by which all control flow dependence are substituted by data depen\u00ad dence \nas described in [2, 33]. Once this expression of the RHS is obtained it can be analyzed and validated \nby the methods described in the previous section. In the following we explain by way of ex\u00ad ~ple how \now new method can identify reductions by performing in essence a value based rather than a dependence \nbased analysis. In Fig. 5(a) statement S3 is first labeled at compile time as a potential reduction. \nThen, by following the clef-use chains of the variables on the RHS (i.e., z and y) within the scope of \nthe loop we find that in statement S 1 z may potentially carry the value of A(R(i)), while y is a constant \nwith respect to A. The algorithm then examines statement S 3 after forward substitution, but does not \nactually replace S 3 in the generated code. The substitution is done only for compiler analysis purposes. \nThis new version of s3, referred to as s33, is of the form: S33 : A(R(i)) = A(K(i)) + constant. Similarly, \nS5 becomes S55 : A(.L(~))I = A(K(i)) + constant. Next, we label the statement pairs (S1, s3) and (S 1, \nS 5 ) in the original loop as expanded reduction statements (ERSS). If we treat each ERS as a single \nreduction statement, then this problem is reduced to one treated above. The code generated for the run \ntime marking of the ERS is inserted for both sides of the statement (RHS and LHS ), but only in the same \nbasic block as the LHS. As we will see in a later example, this rule insures that both sides are marked \nwhen and if there is an assignment, i.e., it insures that a value is actually passed from the RHS to \nLHS, Any uses of values participating in the reduction that occur outside the ERS invalidate the ERS, \ni.e., set the corresponding element of the shadow array Am= to true. In the case of ERSS obtained through \nforward substitution, the value of the reduction reference may pass through several memory locations \n(intermediate variables) before reaching the statement of the LHS. As any use of an intermediate variable \nrepresents a use of a value that participates in the reduction, it invalidates the reduction for the \ncorresponding element of A. The uses can be obtained by following the clef-use chain within the scope \nof the loop. However, based on the dead reference elimination principle described in Section 3.1, only \nthose uses that contribute to the actual data-flow of the loop (when the value is passed on to a shared \nvariable or controls the access to a shared variable) are processed. If not all local variables carrying \nthe reduction value end up being used in the global data-flow within the loop, then we have either to \nverify that they (the local variables) are indeed not live after loop exit, or, if that is not possible, \nmake a conservative assumption (i.e., that all uses contribute to the data flow). In Fig. 5(a), statement \nS4 passes the value of A(K(i)) to the local variable t, which in turn passes it to A(_L(i)) in s5. The \nsame value is also passed to the shared variable B(i(i)) in S 6. Both uses (in S5 and S 6) should, in \nprinciple, invalidate An=(K(i)). On the other hand, statement S5 is another potential reduction of the \nsame type as in S3 and, thus only the use in S 6 needs to invalidate An=(K (i)). The transformed code \nis shown in Fig. 5(b). We note that if one of the intermediate variables is itself an array element addressed \nindirectly, then an additional run-time test must be performed. For example, if S1 and S3 in Fig. 5(a) \nwere of the form: S1 : X( N(i)) = A(K(i)) and S3 : A(R(i)) = X( P(i)) + y, then a value would be passed \nfrom S1 to S3 only if N(i) = F (i). However, if the array X is privatizable, and occurs only in these \ntwo statements, then the run-time testis not necessary, i.e., if N(i) = P(i), then A(K(i)) would be processed \nwith the read of X( F (i)) in s3, and otherwise no data flow would occur. Taking control flow into account. \nThe final situation we consider is when the forward substitution procedure must take into account conditional \nbranches and carry information into the expression of the ERS (see Fig. 6). The additional difficulty \npresented by this case is the fact that the exact form of the RHS is not known statically, What is known, \nhowever, is the set of all possible RHS forms, which can be computed by following all potential paths \nin the control flow graph. A direct approach uses a gated static single assignment (GSSA) [5, 34] representation \nof the program. In such a representation, scalar variables are assigned only once. At tie points of confluence \nof conditional branches a ~ function of the form @(B, Xl, Xz ) is used (in the GSSA representation) to \nselect one of the two possible definitions of a variable (Xl or X2), depending on the boolean S3: A(R(i)) \n= z+ y do i=l, n doi=l, n Sl: z = A(K(i)) Sl: w = A(M(i)) S2: y = constant S2: t = A(J(i)) (a) Sk t=z \nS5: A(L(i)) =t+y S6: if (exp) B(f(i)) = t enddo doall i= l,n Sl: z = A(K(i)) S2 y = constant markread(K(i)) \nmarkwrite(R(i)) if (K(i) .ne. R(i)) then markredux(K(i)) markredux(ll(i)) endif S3: A(R(i)) = z+ y Sk \nt=z (b) markwrite(L(i)) if (K(i) .ne. L(i)) then rn;~dnx(K(i)) markredux(L(i)) endif S5: A(L(i)) =t+y \nS6: if (exp) then rns&#38;edux(K(i)) B(f(i)) = t endif enddoall Figure 5: The code generated for the \ndo loop in (a) 1sshown m (b). The mark-x operations are as described in Fig. 3. expression B, By proceeding \nbackwards through the clef-use chains (which include the q$functions) it is easy to expand a scalar variable \nin terms of boolean expressions, other scalar variables, and array elements. In the example of Fig. 6, \nthe variable w in statement S 9 would be expanded as follows: w=+ =+ #(B3, t,A(M(i))) * 44B3) 4(B2, \nz> A(J(i)), A(M(i)))) =+ 4(B3, 4(B2, 4(B1, A(K(i))), A(L(0), A(J(i)))A(M(i)))) which means that the \nvalue of w is: A(K(i)) if (133A~2 A ~1) is true w. A(L(i)) A(J(i)) if if (B3 (B3 AB2 A -=Bl)k A 7~2) \nis true true (1) { A(M(z)) if (7B3) is true This compound equation can then be used to generate a mark \nread and a mark redux operation at statement S9 where w is read. To save unnecessary work, we only expand \nthose scalars that are on the RHS of assigmnents to shared variables or in po\u00adtential reduction statements \n(e.g., in the case of z in statement S 8). All other scalar references can be safely ignored. Fig. 6(b) \nshows the program in Fig. 6(a) after the insertion of the mark read and mark redux operations, which \nare based on the expansion of the scalar variables. The possible drawback of this approach is that the \nnumber of potential reductions and the number of terms in the logic expressions generated may be quite \nlarge. If this happens, we can gracefully degrade to a more conservative approach: test only some of \nthe expressions of the ERS and invalidate all the rest. S3: if (Bl) then S4: z = A(K(i)) (a) else S5: \nz = A(L(i)) endlf S6: if (B2) t=z S7; if (B3) w= t S8: if (B4) A(R(i)) = A(R(i)) + z S9: if (B5) Y(i) \n= w enddo doall i=l, n Sl: w = A(M(i)) S2: t = A(J(i)) S3: if (Bl) then S4: z = A(K(i)) else S5: z = \nA(fJj)) endif (b) S6: if (B2) t= z S7: if (B3) w= t S8: if (B4) then markread(Bl *K(i) + notBl *L(i)) \nmarkredux(B 1*K(i) + notB 1*L(i)) markwnte(R(i)) A(R(i)) = A(R(i)) + z endif S9: if (B5) then m ar~ead(B3*B2*Bl \n*K(i) + B3*B2*notB 1*L(i) + B3*notB2*J(i) + notB3*M(i)) markredux(B3*B2*Bl *K(i) + B3 *B2*notBl *L(i) \n + B3*notB2*J(i) + notB3*M(i)) Y(i) =w  endlf enddoall Figure 6: The code generated for the do loop \nin (a) is shown in (b). The mark-x operations are as described in Fig. 3. The expressions in the markread \nand markredux operations are abbreviations of i f then e 1 se statements representing the different assignments \nto z (S 8) and w (S 9) as in Equation 1. The operators * , + , and not represent logical and , or , and \ncomplement operators, respectively. It is important to note that the loop in Fig. 6 exemplifies the type \nof loop found in the SPICE2G6 program (subroutine LOAD) which can account for 70% of the sequential execution \ntime (h vectorization has dealt with before [35]). Finally we mention that reductions such as rein, max. \netc., would first have to be syntactically pattern matched, and then substituted by the min and ma-x \nfunctions. From this perspective, they are more difficult to recognize than simpler arithmetic reductions. \nHowever, after thk transformation, our techniques can be applied as described above.  4 Putting it All \nTogether Jn the previous sections we described run-time techniques that can be used for the speculative \nparallelization of loops. These techniques are automatable and a good compiler could easily insert them \nin the original code. In thk section, we give a brief outline of how a compiler might proceed when presented \nwith a do loop whose access pattern cannot be statically determined. A. At Compile Time. 1. A cost/benefit \nanalysis is performed using both static analysis (based on rhe asymptotic complexity of the LPRD test \ngiven below) and run-time collected statistics to determine whether the loop should be: (i) speculatively \nexecuted in parallel using the LRPD test, (ii) first tested for full parallelism, and then executed \nappropri\u00ad  ately (using an inspector/executor version of the LIRPD Test), or (iii) executed sequentially. \n2. Generate the code needed for the speculative uarallel execution. A parallel version of the origina;loop \nis a~gmented with, the markread, markwrite and markredux operations for the LRPD test; if necessary to \nidentify reduction variables, the [oop is also augmented as described in Section 3.2.2. In addition, \ncode is generated for the analysis phase of the LRPD Test, the potential sequential re-execution of the \nloop, and any necessary checkpointing/restoration of program variables. B. At Run-Time. 1. Checkpoint \nif necessary, i.e., save the state of program variables, 2. Execute the parallel version of the loop, \nwhich includes the marking phase of the test. 3. Execute the analysis phase of the test, which gives \nthe pass/fail result of the test. 4. If the test passed, then compute the final results of all reduction \noperations (from the processors partial results) and copy--out the values of any live private variables. \nIf the test failed, then restore the values of any altered program variables and execute the sequential \nversion of the loop. 5. Collect statistics for use in future runs, and/or for schedule reuse in this \nrun.  An example using iteration numbers as marks in private shadow arrays is shown in Fig, 7. If the \nspeculative execu Lion of the loop passes the analysis phase, then the scalar reduction re\u00adsults are \ncomputed by performing a reduction across the processors using the processors partial results. Otherwise, \nif the test fails, the loop is re-executed sequentially. 4.1 Complexity of the LRPD test The tirnerequiredby \nthe LRPD test is T (n,s, a, p) = O(mz/p+ log p), where p is the number of processors, n is the total \niteration count of the loop, s is the number of elements in the shared anray, and a is the (maximum) \nnumber of accesses to the shared array in a single iteration of the loop, We assume that the implementation \nof the test uses private shadow structures. The analysis below is valid for all variants of the LRPD \ntest. The marking phase (Step 1) takes O(n.a/p+s +log p) time, i.e., proportional to max(na/p, s, log \np) time. We record the read ~and write accesses, and the reduction and privatization flags in private \nshadow arrays using iteration number marks , In order to check whether for a read of an element there \nis a write in the same iteration, we simply check that element in the shadow array -a constant time operation. \nAll accesses can be processed in O(na/p) time, since each processor will be responsible for O(na/p) accesses. \nAfter all accesses have been marked in private storage, the private shadow arrays can be merged into \nthe global shadow arrays in 0(s + log p) time; the log p contribution arises from the possible write \nconflicts in global storage that could be resolved using software or hardware combining. The counting \nin Step 2(a) can be done in parallel by giving each processor slp values to add within its private memory, \nand then summing the p resulting values in global storage, which takes O(s/P + log p) time [19]. The \ncomparisons in Step 2(b) (2(d)) of AW with AT (with A.p and An=) take O(s/p + log p) time. If the loop \npasses the test, then the final result of each reduc\u00ad tion must be computed (unless the reduction was \nparallelized using unordered critical sections) and last value assignments must be per\u00ad formed for the \nlive private variables. If the reduction operation is parallelized using unordered critical sections, \nthen no overhead is incurred, i.e., the original sequential reduction operation and its transformed parallel \nversion require the same number of operations (within a small constant factor). However, if the reduction \nis paral\u00ad lelized using recursive doubling, then an overhead 0(s + log p) is incurred when the processors \npartial results are merged pair-wise into the scalar reduction results. Similarly, the private variables \nwith the latest time stamps (iteration number marks ) can be selected for last value assignment in time \n0(9+ log p). Hash tables. If s >> na/p, then the number of operations in the LRPD test does not scale \nsince each processor must always inspect every element of its private shadow stzucture when transfening \nit to the global shadow structure (even though each processor is respon\u00adsible for fewer accesses as the \nnumber of processors increases). Another related issue is that the resource consumption (memory) would \nnot scale. However, if shadow hash tables are used, then each processor will only have private shadow \ncopies of the array elements accessed in iterations assigned to it, which will increase the cost per \naccess by a small constant factor. Thus, if hash tables of size O(na/p) are used, then the complexity \nof the marking phase becomes O(n.a/p + log p). Similarly, using hash tables the analysis phase and any \nneeded last vahre assigmnents and/or processor-wise reduction operations can be performed in time O(na/p \n+ log p).  5 Experimental Results In this section we present experimental results obtained on two modestly \nparallel machines with 8 (Alliant FX/80 [3]) and 14 pro\u00adcessors (Alliant FX/2800 [4]) using a Fortran \nimplementation of our run-time library. The codes have been manually instnsmented with calls to the run-time \nlibrary. However, we remark that our results scale with the number of processors and the data size and \nthus they should be extrapolated for massively parallel processors (MPPs), the actual target of our run-time \nmethods. We considered seven do loops from the PERFECT Benchmarks [7] that could not be parallelized \nby any compiler available to us. Our results are summarized in Table 1. For each loop, we note the type \nof test applied: doall indicates cross-iteration dependence were checked (Lazy Doall (LD) test), privat \nindicates privatization was checked (LPD test), reduct indicates reduction parallelization was checked \n(LRD test). For each method applied to a loop, we give the speedup that was obtained, and the potential \nslowdown that would have been incurred if, after applying the method, the loop had to be re-executed \nsequentially. If the inspector/executor version of the LRPD test was applied, the computation performed \nby the inspector is shown in the table: the notation privatization indicates c Marking Phase c Analysis \nPhase dimension A(m), pA(m,procs) doall i=l ,n dimension A_w(m), pA_w(m,procs) A_w(l :m) = pA_w(l :m,i) \ndimension A_r(m), pA_r(m,procs) A_r(l :m) = pA_r(l :m,i) dimension A_nx(m), pA_rrx(m,procs) A_nx(l :m) \n= pA_nx(l:m,i) Initiahze(pA, pA_w, pA_r, p.Lnx) enddoall c original loop doall i=l,n result = test(A_w, \nA_r, A_nx) dimension A(l :m) private p if (result .eq. pass) then do i=l, n p = get_proc_ido c compute \nreduction Sl: A@(i)) = A@(i))+ expo pA_w(R(i), p) = i doall i=l, m S2: ......... = A(L(i)) Sl: pA(R(i), \np) = pA(R(i), p) + expo if (A_nx(i) .eq. false.) end do if (pA_w(L(i), p) .ne. i) A(i) = sum (pA(i, \n1:procs)) pA_r(L(i), p) = 1 enddoall (a) pA_nx(L(i), p) = .tnre. else S2, ........... = pA(L(i), p) c \nexecute the loop sequentially enddoall endif (b) (c) Figure 7: The sunphfied code generated for the do \nloop in (a) is shown us (b) and (c). Privatization is not tested because of a read before a write reference \nthe inspector verified that the shared array was privatizable and then cations. In the cases when the \ntest failed, we restored state, and dynamically privatized the array for the parallel execution, branch \nre executed the loop sequentially. The speedup reported includes predicate and subscript array mean that \nthe inspector computed both the parallel and sequential irtstantiations (Fig. 11). these values, and \nreplicates loop means that the inspector was work Loop 40 from SPICE is representative of the type of \nthe loop equivalent to the original 100p. contained in the LOAD subroutine, which accounts for 70% of \nthe sequential execution time. Since all the arrays are equivalence in Figures 8 through 14 the speedup \nand the potential slowdown In addition to the summary of results given in Table 1, we show to a global \nwork array, all accesses in the loop were shadowed in the LRD test, i.e., each array element was proven \nto be either ameasured for each loop as a function of the number of processors reduction variable, read \nonly, or independent (i.e., accessed in only was calculated using an optimally parallelized (by hand) \nversion one iteration). For this loop we used an inspector/executor version of the loop. The potential \nslowdown reported is the percentage of used. For reference, these graphs show the ideal speedup, which \n of the LRD test (instead of a speculative parallelization) because the execution time that would be \npaid as a penalty if the test had of complex memory management problems for the shadow arrays failed, \nand the loop was then executed sequentially. In cases where in the presence of highly irregular and sparse \naccess patterns. The extraction of a reduced inspector loop was impractical because of ideal speedup \nof loop 40 is not very large since the loop is small, complex control flow and/or inter-procedural problems, \nwe only irnbalanced between iterations, and traverses a linked list. The liied list traversal was parallelized \nusing techniques we developed for automatically parallelizing while loops [27]. Thus, although applied \nthe speculative methods. Whenevernecessary in the speculative executions, we performed the obtained speedup \nis modest, it represents a significant fractiona simple preventive backup of the variables potentially \nwritten in of the ideal speedup (see Fig. 14). Therefore, since loop 40 is one the loop. In some cases, \nthe cost of saving/restoring might be of the smallest loops in the LOAD subroutine, we expect to obtain \nsignificantly reduced by using another strategy. In order for our better speedups on the larger loops \n(since they have larger idealmethods to scale with the number of processors, the shadow arrays speedups). \nIn the camera-ready version of the paper, we will reportmust be distributed over the processor space, \nrather than replicated the speedups obtained on all loops in subroutine LOAD. on each processor (Section \n4.1). For this purpose, we tried using The speedups obtained for the loops from both OCEAN andhash tables. \nSince we had at most 14 processors, the extra cost of TRFD are modest because they are kernels. In the \ncase of the loop the hash accesses dominated the benefit of reducing the size of the from TRFD we were \nable to reuse tie schedule and improve ourshadow arrays. Thk was particularly true for the loops from \nthe results significantly. Because of the large data set accessed the loop OCEAN and TRFD Benchmarks. \nHowever, on a larger machine we from TRFD is the only case in which speculative execution proved would \nexpect the use of hash tables to pay off. Due to this problem, to be inferior to the inspector/executor \nmethod (saving state was a the results reported do not reflect the use of hash tables. significant portion \nof the execution time). The graphs show that in most cases the speedups scale with the number of processors \nand are a very significant percentage of the ideal speedup. When they do not scale, as mentioned above, \nwe believe that the use of hash tables (for MPPs) will preserve the scal\u00ad  6 Conclusion ability of our \nmethods. We note that with the exception of the TRFD loop (Fig. 10), the speculative strategy gives superior \nspeedups ver- In this paper we have approached the problem of parallelizing sus the inspector/executor \nmethod. For both methods the potential statically intractable loops at run-time from a new perspective \n\u00adslowdown is small, and decreases as the number of processors in\u00ad instead of determining a valid parallel \nexecution schedule for the creases. As expected, the potential slowdown is smaller for the loop, we speculate \nthat the loop is fully pruallelizable, a frequent oc\u00adinspector/executor method, currence in real programs. \nWe proposed efficient, scalable run-time We now make a few remarks about individual loops for which techniques \nfor veri~ing the correcmess of a speculative parallel ex-Table 1 does not give complete information. \necution, i.e., methods for checking that there were in fact no cross-The loop from TRACK is parallel \nfor only 90% of its invo-iteration dependence in the loop. From our previous experience Benchmark2 Experimental \nResults Subroutine potential Tested Description of Loop Inspector Loop Technique Speedup Slowdown (% \nof sequential execution time of program) (computation) MDG 14 processors pnvatizabon INTERF speculative \n11.55 ] 1.09 dc,all accesses to a privauzable vector guarded data accesses loop 1000 insp/exec 8.77 1.03 \n \u00ad pnvat by loop computed predicates (92% T,.q) branch predicate BDNA 14 processors privatization ACTFOR \n~ speculative 1.09 - doall accesses privatizable array indexed by a data accesses loop 240 insp/exec \n7.72 ] 1.04 pnvat subscript array computed inside loop (32% T, ,g) subscript array 8 processors TRFD \nspeculative .85 2.1-f doall small triangular loop accesses a vector data accesses INTGRL sched reuse \n1.93 2.17 indexed by a subscript array computed replicates loop loop 140 insp/exec 1.05 1.74 outside \nloop (5% T, ,q) sched reuse 2.10 1,74 TRACK accessesarray indexed by subscript array NLFILT 8 processors \ndoall computed outside loop, accesspattern not applicable loop 300 speculative 4.21 1.01 guarded by \nloop computed predicates (39% T.,q) ADM accessesprivatizable array thm aliases, RUN 14 processors doalf \narray re-dimensioned, access not applicable loop 20 speculative 9.01 1.02 pnvat pattern control flow \ndependent (44% T,,q) OCEAN 8 processors kemel-tike loop accesses a vector with data accesses FTRVMT speculative \n2.23 1.45 dealt inn-rime determined strides replicates loop loop 109 insplexec 2.14 1.30 26K invocations \naccount for437. 2 , .q SPICE traverses hnked list tenrunated by a NULL LOAD 8 processors doidf pointer, \nall referenced arrays equivalence data accesses loop 40 insplexec 2.75 1.09 reduct to a global work array \n Table 1: Summary of Experimental Results. with static analysis and parallelization of Fortran programs, \nwe huve found that the two transformations most effective in removing data dependence are privatization \nand reduction parallelization, Thus, our new run-time techniques for checking the validi~ of specula\u00adtive \napplications of these transformations increases our chance of extracting a significant fraction of the \navailable parallelism in even the most complex program. The methods in this paper employ a cle\u00adpendence \nanalysis based on the actual exchange (definition or use) of vahres rather than on the memory references \nthemselves. This approach leads to the exploitation of more parallelism than was pre\u00adviously possibly, \ne.g., our general method for reduction recognition that does not rely on syntactic pattern matching. \nOur experimental results show that the concept of run-time data dependence checking is a useful solution \nfor loops that cannot be analyzed sufficiently by a compiler. Both speculative and inspec\u00adtor/executor \nstrategies have been shown to be viable alternatives for even modestly parallel machines like the Alliant \nFX/80 and 28010. We would like to emphasize that our methods are applicable to all loops, without any \nrestrictions on their data or control flow. We believe that the significance of the methods presented \nhere will only increase with the advent of massively parallel processors (MPPs) for which the penalty \nof not parallelizing a loop could be a massive performance degradation. As we have shown, our run-time \ntests are efficient and scalable, and thus if the target machine has many (hundreds) processors, then \nthe cost of our techniques will become a very small fraction of the sequential execution time. b-I other \nwords, speculating that the loop is fully parallel has the poten\u00adtial to offer large gains in performance \n(speedup), while at the same time risking only small losses. To bias the results even more in our favor, \nthe decision on when to apply the methods should make use of 2Alt benchmarks are from tfre PERFECT Benchmark \nSuite run-time collected information about the fully parallel/not parallel nature of the loop. In addition, \nspecialized hardware features could greatly reduce the overhead introduced by the methods. Finally we \nbelieve that the true importance of this work is that it breaks the barrier at which automatic parallelization \nhad stopped: regulax, well behaved programs. We think that the use of aggres\u00adsive, dynamic techniques \ncan extract most of the available par\u00adallelism from even the most complex programs, making parallel computing \nattractive.  Acknowledgment We would like to thank Paul Petersen for his usefirl advice, and Wllliarn \nBlume, Gung-Chung Yang and Andrei Vladimirescu for identifying and clarifying applications for our experiments. \nSpecial thanks go to Nancy Amato for her careful review of the manuscript and insightful comments. References \n[1] S. Abraham. Private communication, 1994. [21 J. R. Allen, K. Kennedy, C. Porterfield, and J. Warren. \nConversion of control dependence to data dependence. In Proceedings of the I(lt)r ACM Symposiatrs on \nPrinciples of Programming Languages, pages 177-189, January 1983. [3] Athant Computer Systems Corporation. \nFXISeries Architecture Man\u00adual, 1986. [4] Albant Computers Systems Corporation. Al[iant FX12800 Series \nSys\u00adtem Description, 1991. [5] R. Ballance, A. Maccabe, and K. Ottenstent. The Program Depen\u00addence Web: \na Representation Supporting Control-Data-and Demand-Dnven Interpretation of Imperative Languages. In \nProceedings of the SIGPLAN 90 Conference on Programming Lmgmge Design and Irnplemenladon, pages 257-271, \nJune 1990. [6] U. Banerjee. Dependence Analysis for Supercomputing, Kluwer. Boston, MA., 1988. [7] M. \nBerry, D. Chen, P. Koss, D. Kuck, S. Lo, Y. Pang, R. Roloff, A. Sameh, E. Clementr, S. Chin, D. Schneider, \nG. Fox, P. Messina, D. Walker, C. Hsiung, J. Schwarzmeler, K. Lue, S. Orzag, F. Seidl, O. Johnson, G. \nSwanson, R. Goodrum, and J. Martin. The PER-FECT club benchmarks: Effective performance evaluation of \nsuper\u00adcomputers. Technical Report CSRD-827, Center for Supercomputmg Research and Development, University \nof Illinois, Urbana, IL, May 1989. [8] H. Benyman and J. Saltz. A manual for PARTI runtime prirmttves. \nInterim Report 90-13, ICASE, 1990. [9] W. Blrrme and R. Elgenmann. Performance Analysis of Parallelizing \nComprlers on the Perfect Benchmarks M Programs. LEEE Transac\u00adtions on Parallel and Distributed Systems, \n3 (6):643 656, November 1992. [10] M. Burke, R. Cytron, J. Ferrante, and W. Hsieh. Automatic generation \nof nested, fork-join parallehsm. Journal of Super-computing, pages 71-88,1989. [11] W. J. Camp, S. J. \nPlimpton, B. A. Hendrickson, and R. W. bland. Massively parallel methods for engineering and science \nproblems. Comm ACM, 37(4) !31-41, Apr d 1994. [12] D. K. Chen, P. C. Yew, and J. Torrellas. An efficient \nalgorithm for the rim-time parallelization of doacross loops. In Proceedings of Super\u00adcomputing 1994, \npages 518 527, Nov. 1994. [13] A. Dinmng and E. Schonberg. An empirical comparison of monitoring algorithms \nfor access anomaly detection. In Proc. of 2-rid ACM SIG-PLAN Symposium on Principles&#38; Practice of \nParallel Programming (PF OPP), pages 1-10,1990. [14] R. Elgenmann,J. Hoeffinger, Z. Li, and D. Padua. \nExperience rn the Automatic ParalJelization of Four Perfect-Benchmark Programs. Lecture Notes in Computer \nScience 589. Proceedings of the Fourth Workshop on Lmsguagesand Compilers for Parallel Compuiing, Santa \nClara, CA, pages 65 83, August 1991, [15] V. KrothapalJi and P. Sadayappan. An approach to synchromzation \nof parallel computing. In Proceedings of the 1988 International Confer\u00adence on Supercompu~mg, pages 573-581, \nJune 1988. [16] C. Kruskal. Efficient parallel algorithms for graph problems. In Pro\u00adceedings of the \n1985 International Conference on Parallel Processing, August 1985. [17] C. Knrskal. Efficient parallel \nalgorithms for graph problems. In Pro\u00adceedings of the 1986 International Conference on Parallel Processing, \npages 869 876, August 1986. [18] D. J. Kuck, R. H. Kuhn, D. A. Padua, B. Leasure, and M. Wolfe. Dependence \ngraphs and compiler opt.muzations. Jn Proceedings of the 8th ACM Symposium on Principles ofPrograrrrming \nLanguages, pages 207-218, January 1981. [19] F. Thomson I.aghton, introduction to Parallel Algorithms \nand Archi\u00adtectures: Arrays, Trees, Hypercubes. Morgan Kaufmann, 1992. [20] S. Leung and J. ZahorJan. \nIrnprowng the performance of runurne parallelization. In 4th PPOPP, pages 83 91, May 1993. [21] Zh,yuan \nL,. Array privatization for parallel execution of loops. In Proceedings of the 19th International Symposium \ncm Computer Archi\u00adtecture, pages 3 13 322, 1992. [22] D. E. Maydan, S. P. Amarasmghe, and M. S. Lam. \nData dependence and data-flow analysis of arrays. In Proceedings 5th Workshop on Pro\u00adgramming Languages \nand Compi[ers for Parallel Computing, August 1992. [23] S. Midkiff and D. Padua. Compiler algorithms \nfor synchromzation. L?Z?E Trans. CompuZ., C-36(12):1485-1495, 1987. [24] I. Nudler and L. Rudolph. Tools \nfor the efticlent developementof efti\u00adcient parallel programs. In Proc. 1st Israeli Conference on Computer \nSystem Engineering, 1988. [25] D. A. Padua and M. J. Wolfe. Advanced compiler optunizations for supercomputers, \nCommunications of the ACM, 29:1184-1201, De\u00adcember 1986. [26] L. Rauchwerger and D. Padua. The privatizing \ndoall test: A rnn\u00addrne technique for doall loop identification and array privatization. In Proceedings \nof the 1994 International Conference on Supercomputing, pages 33 43, July 1994. [27] Lawrence Rauchwerger \nand David A. Padua, Parallelizrng WHILE Loops forMultiprocessorSy stems. III Proceedurgsof 9thlnternattonal \nParallel Processing Symposium, April 1995. [28] J. Saltz and R, Mirchandaney, The preprocessed doacross \nloop. In Dr. H.D. Schwetrnan, editor, Proceedings of the 1991 International Conference on Parallel Processing, \npages 174 178. CRC Press, Inc., 1991. Vol. If -Software. [29] J. Saltz, R. Mmchandaney, and K. Crowley. \nThe doconsiderloop. In Proceedings of the 1989 International Conference on Supercomputing, pages 29-40, \nJune 1989. [30] J. Saltz, R. Mirchandaney, and K. Crowley. Run-time parallelization and scheduhrtg of \nloops. IEEE Trans. Comput, 40(5), May 1991. [31] E. Schonberg. On-the-fly detection of access anomalies. \nIn Proceed\u00adings of the SIGPLAN 1989 Conference on Programming Language Design and Implementation, pages \n285 297, Portland, Oregon, 1989. [32] P. Tu and D. Padua. Array privatization for shared and distributed \nmemory machines. In Proceedings 2nd Workshop on Languages, Com\u00adpilers, and Run-Time Envlronmentsfor Distributed \nMemory Machines, September 1992. [33] P. Tu and D. Padua. Automatic amay privatization. In Proceedings \n6t/r Annual Workshop on Languages and Compilers for Parallel Comput\u00ading, Portland, OR, August 1993. [34] \nPeng Tu and David Padua. GSA based demand-driven symbofic analysis. Technical Report 1339, University \nof Illinois at Urbana-Champaign, Cntr for Supercompuring Res &#38; Dev, February 1994. [35] A. Vladin-urescu, \nLSI Circuit Simulation on Vector Computers. PhD thesis, Electronics Research Laboratory, University of \nCalifornia, Berkeley, October 1982. Technical Rept. No. UCB/ERL M82/75. [36] M. Wolfe. Optimizing Compiiersfor \nSupercomputers. The MIT Press, Boston, MA, 1989. [37] J. Wu, J. Saltz, S. Hlranandam, and H. Berryman. \nRuntime compi\u00adlation methods for multicomputers. In Dr. H.D. Schwerman, editor, Proceedings of the 1991 \nInternational Conference on Parallel Pro. cessing, pages 26-30. CRC Press, Inc., 1991. Vol. If Software. \n[38] C. Zhu and P. C. Yew. A scheme to enforce data dependence on large mnkiprocessor systems. IEEE Trans. \nSoftw, Eng,, 13 (6):726-739,June 1987. [39] H. Zima. Supercompilersfor Parallel and Vector Computers. \nACM Press, New York, New York, 1991. SPeeduP d kmP MDG_lfWERF_lGCJ3 potentialSlwcfown of LwP MDG.lfWERF_lIMO \nw Numbaf of Prccwws (w?w3) w Numkw of wcms-wa (fwzscm) @&#38;up1si $ $.hUIOhn 1.63 1 Iz A ,1 40 e 8 7 \n8 5 4 ~1 = 7 234667 891011>2U ,4 2S4667S 91011121S14 Nurk6 Q(P NW M P _ SF%CU!AM . . . lNSPECT@EC ,..,, \nLX#L _ SPECUIATME ..\u00ad IN6PECVRE.2 Figure 8: SPaadupof kQP BDNA__0R_240 P7JtentiaiSlcwdclw! of Lwp BDNA.ACTFOR-240 \nw Numbe of Prcnxsawa(Ew%CO) w. Numbs+&#38; Prcceaaws(f=wsw) s+wdup1$ awdohll 133 12 . 1s3 ,, 1.4d 10 \nMO e 1a5 B lm 7 8 s + 1,,5 ?10 2 1,05 1 I.m J 2345878 930111218$4 2s4se7a s10il?213M NW 01P m NW mlP \n. _aEuu.Alr@s .--!NaPEcTfDEc _\u00ad low. _ SFECUIAT?f --- INSPECT= Figure 9: SpeB3uP of LmP TRFD.INTGRL-140 \npotentlelSlowdownof Imp TRFD_lNTGRL_140 w.. Number of Pr rs (FWSO) W. Numlw of Pmceaaom(wSo) milha?dwVaMSctmdulalw. \nSw.xhwl awllp . . 32 al 80 w 2s 27 26 26 2,4 23 1 . . . . ..---------\u00ad --- ---\u00ad 20 19 18 2 3 4 6 8 7 \n$ 17 N lnW d -m 2 3 4 6 a 7 e _sFewLm&#38; --- LWPECTIE.EC.-. IDFM NwnQ2,0! Fm NOSmbof, -M Ww4 le hum \n_ ,Tm ._. NwEcTJExEc Figure 10: spmdup of Loop TFfACK_NIHLT_SOO Potemlal Slowdcwm of LooP TRACK_NI-HLT_Wl \nVS. Numbw of Pmces!sora (FWSO) VS. Numberof Pmwsors (FWSO) Patialty ParallelLoop Padaliy Padel Loop awd. \nIm %~ w% lrm 4 lam lWU .3 1.W lJxa ?Uz5 ma mm ? a 4 6 8 7 M, ,* G4i%-. 8 2 s 4 6 0 7 N..\u00ad. m a _ SECU!ATW. \n. . low. _ SPEwlAmE Figure 11: 231 Sp3edup of LooP.43M.RUN.20 %tsmlal Slcwchvmof LOOPPQM_RUN_XJ w Numb \nof Proxsscxs (FX&#38;!J20) w Num&#38; of Prccsssms (IW%CY3) :~ 11 2 2246678 01011121a14 Nti o! P=* _ \nsFtCUMJNE ,..,, IDEM  Figure 12: Sw6duP of I.COP OCM.FTRVMT-W3 Potential Slowdown ofLWP 0CWf4_FTRVMT_l@ \nvs Numb of Prwezssors(FXX?O) vs Number of Prcwssors (BW?O) L%lti p M. t9 ~ 8 I _-J la ---- - \u00ad 6 17 \n4 %6 .\u00ad . , s , 2 . .... -------------?,4 . .... 1 93 , 3*5. 78 2346n7a Numb, d WI _ SFECUIA7NE ___ \nINSPECT(BEC .~..,, IDE4L _ SF5cwm ___ LNsPElx/mEc Figure 13: SPeeduPof LOOP SPICE-LOAD_40 putentlal Slwduivn \nof LCOP SpCE_LOAD_40 w Numkerof Prccsswrs (FX@O) w Numbar of Prcew.xs (W@O) Whlla LOOp -Lmkec IM Traversal \nMile Loop Linked M Trawamd Smadw 9.WZI. 4, 4.X 1.2s >. 1s ..> 124 .. l= .,.. .. ,= -.\\\\ 118 .. .. 11% \n~.k\u00ad 1,14 1,12 -.. . -+-...... 710 2 34687a T.Ja Nu.ter of m-m 2346a7a _ lNSFEGT~ ,,. . .. IDEAL Numb, \nd FTO-m Figure 14: 232 \n\t\t\t", "proc_id": "207110", "abstract": "<p>Current parallelizing compilers cannot identify a significant fraction of parallelizable loops because they have complex or statically insufficiently defined access patterns. As parallelizable loops arise frequently in practice, we advocate a novel framework for their identification: speculatively execute the loop as a doall, and apply a fully parallel data dependence test to determine if it had any cross-iteration dependences; if the test fails, then the loop is re-executed serially. Since, from our experience, a significant amount of the available parallelism in Fortran programs can be exploited by loops transformed through <italic>privatization</italic> and <italic>reduction parallelization</italic>, our methods can speculatively apply these transformations and then check their validity at run-time. Another important contribution of this paper is a novel method for <italic>reduction recognition</italic> which goes beyond syntactic pattern matching; it detects at run-time if the values stored in an array participate in a reduction operation, even if they are transferred through private variables and/or are affected by statically unpredictable control flow. We present experimental results on loops from the PERFECT Benchmarks which substantiate our claim that these techniques can yield significant speedups which are often superior to those obtainable by inspector/executor methods.</p>", "authors": [{"name": "Lawrence Rauchwerger", "author_profile_id": "81100504584", "affiliation": "University of Illinois at Urbana-Champaign", "person_id": "PP14176105", "email_address": "", "orcid_id": ""}, {"name": "David Padua", "author_profile_id": "81452612804", "affiliation": "University of Illinois at Urbana-Champaign", "person_id": "P63208", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/207110.207148", "year": "1995", "article_id": "207148", "conference": "PLDI", "title": "The LRPD test: speculative run-time parallelization of loops with privatization and reduction parallelization", "url": "http://dl.acm.org/citation.cfm?id=207148"}