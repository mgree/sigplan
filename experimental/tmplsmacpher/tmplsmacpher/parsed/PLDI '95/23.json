{"article_publication_date": "06-01-1995", "fulltext": "\n Elimination of Redundant Array Subscript Range Checks * Priyadarshan Kolte and Michael Wolfe Department \nof Computer Science and Engineering Oregon Graduate Institute of Science &#38; Technology {pkolte, mwolf \ne}@cse. ogi. edu Abstract This paper presents a compiler optimization algorithm to re\u00ad duce the run \ntime overhead of array subscript range checks in programs without compromising safety. The algorithm \nis based on partial redundancy elimination and it incorpo\u00ad rates previously developed algorithms for \nrange check op\u00ad timization. We implemented the algorithm in our research compiler, Nascent, and conducted \nexperiments on a suite of 10 benchmark programs to obtain four results: (1) the ex\u00ad ecution overhead \nof naive range checking is high enough to merit optimization, (2) there are substantial differences be\u00adtween \nvarious optimizations, (3) loop-based optimizations that hoist checks out of loops are effective in eliminating \nabout 98 % of the range checks, and (4) more sophisticated analysis and optimization algorithms produce \nvery marginal benefits. 1 Introduction Program statements that access elements of an array outside the \ndeclared array ranges introduce errors which can be dif\u00adficult to detect. Since compile-time checlang \nof whether all array accesses in a program are within the declared ranges is not possible in general, \nmany compilers offer the option of inserting run-time range checks into the compded program so that errors \ndue to array range violations are detected dur\u00ading execution. A range check compares the array subscript \nexpression with the array bounds and then traps (or raises an exception) if the subscript is not within \nthe declared array range. In spite of the advantage of compiler-inserted range checks in improving the \nreliability of programs, program\u00admers often prefer to compile their programs without range checking because \nthe execution overhead of the range checks is too high. Hence, compiler optimization that reduce the \nexecution overhead of range checks without compromising safety are useful. Range check optimization is \nespecially beneficial in safety-oriented languages such as Ada, where *This work is suPDOrted by NSF \nGrant CCR-9113S85 and gr=nk. from lntel Supercomputer Systems Division and the Oregon Advanced Computmg \nInstitute. Permission to copy without fee all or part of this material is granted provided that the copies \nare not made or distributed for direct commercial advantage, the ACM copyright notice and the title of \nthe publication and its date appear, and notice is given that copying is by permission of the Association \nof Computing Machinery.To copy otherwise, or to republish, requires a fee and/or specific permission. \nSIGPLAN 95La Jolla, CA USA Q 1995 ACM 0-89791 -697-2/95/0006 ...$3.50 range checking is not just a compiler \noption, but is required by the language definition. Although range checks are subject to traditional \ncom\u00adpiler optimizations such as constant propagation, common sub expression elimination, and invariant \ncode mot ion, range checks possess an interesting property that we study in this project: a range check \nC~ may imply a check Cl, and therefore performing C, makes performmg Cl unnecessary. For example, Figure \nl(a) shows a program with two state\u00adments, S1 and SZ, and four range checks, Cl Cq. Check CZ implies \ncheck C4 because of the mathematical fact (2 x iV < 10) ~ (2*N 1 ~ 10). Thus, check C4 IS redundant be\u00adcause \na [stronger check, C2, has [already been performed, The redundant check is eliminated and the optimized \npro\u00adgram has only three range checks as shown in Figure l(b). Further optimization exploits the fact \n(2 * N 1 > 5) a (2x iV ~ 5) At the program point where check Cl is per\u00adformed, the compiler deduces \nthat a stronger check, Cs, is guaranteed to be performed in the future. Hence the com\u00adpiler places the \nstronger check, Cs, before the weaker check, Cl, which makes check Cl redundant. The resulting pro\u00adgram \nhas only two range checks and is shown in Figure 1(c), Researchers have studied range check optimization \nas ap\u00adplications of automated program verification [8, 17], abstract interpretation [4, 5, 11], and data \nflow analysis [2, 9, 10]. Range check optimizers have also been implemented in sev\u00aderal compilers such \nas the IBM PL.8 compiler [13], the Alsys Ada compder [14], and the Karlsruhe Ada compiler [16]. This \nproject builds on previous work, especially on the algorithms present ed by Gupt a [9, 10]; our main \ncontribu\u00adtions are: use of partial redundancy elimination techniques [15, 12] for range check optimization; \n a study of the advantages of using induction variables [7, 18] in range check optimization;  an implementation \nof a range check optimizer in our research Fort ran compiler, Nascent; and  an experimental evaluation \nof the compile time cost and effectiveness of various optimizations on a suite of large programs.  We \npresent background and notation in $2. $3 describes our formulation and solution of the range check optimization \nproblem. 54 presents the results of our experiments. 85 discusses related work, and 56 concludes. Cl: \nC2: Sl: C3: C4: S2.: integer A[5.,1O] ,,, if (not (2*N ~ 5)) TRAP if (not (2*N < 10)) TRAP A[2*N] = o \nif (not (2* N-1 ~ 5)) TRAP if (not (2* N-1 ~ 10)) TRAP A[2*N-1] = 1 Cl: C2: Sl: C3: S2-: integer A[5..1o] \n... if (not (2*N ~ 5)) TRAP if (not (2*N ~ 10)) TRAP A[2*N] = o if (not (2* N-1 ~ 5)) TRAP A[2*N-1] = \n1 ... integer A[5.. 10] ... C3: if (not (2* N-I ~ 5)) TRAP C2: if (not (2*N ~ 10)) TRAP SI : A[2*N] = \no S2: A[2*N-1] = 1 ... . (a) Without any optimization (b) With some optimization (c) With more optimization \nFigure 1: Example of a program fragment without and with range check optimizations 2 Background and \nnotation We assume that the reader is familiar with the control ilow graph (CFG) abstraction of programs, \nnatural loops, clata flow analysis [1], and the Static Single Assignment (S/SA) representation of programs \n[6]. 2.1 Partial redundancy elimination Partial redundancy elimination (PRE) eliminates redundant computation \nof expressions in programs by moving invari\u00adant computations out of loops and also eliminating identical \ncomputations that are performed more than once on any ex\u00adecution path. Although we use terms defined \nby Morel and Renvoise [15] in this paper, our implementation of PRE uses the safe-earliest and latest-not-isolated \ntransformations de\u00ad veloped by Knoop, Ruthing, and Steffen [12] because tlhese transformations are conceptually \nsimpler and more efficient than the techniques originally proposed by Morel and Ren\u00advoise. The general \nstrategy of PRE transformations of pro\u00adgrams is as follows. 1. Identify sets of equivalent expressions. \nExpres\u00adsions in the program we partitioned into equivalence classes. Usually, syntactic equivalence is \nused to de\u00adtermine if two expressions are equivalent. 2. Solve data flow systems for availability y \nand an\u00adticipatability of expressions. An expression e is available at a program point p if some expression \nin the equivalence class of e, say e , has been computed on every path from the entry of the CFG to p \nand none of the operands of e have been redefined since  the computation of e . Intuitively, an expression \ne is available at point p if the value of e has always lbeen computed when program execution reaches \npoint p. Forward data flow analysis is used to determine avail\u00adability of expressions in programs. An \nexpression e is anticipatable at a program point p if e is computed on every path from p to the exit \nof the CFG before any of the operands of e are redefined. Intuitively, an expression e is anticipatable \nat point p if e is always guaranteed to be evaluated at some point after the execution of program point \nP. Backward data flow analysis is used to determine anticipatability of expressions in programs. 3. Determine \npoints in the program where new computations of expressions can be safely and profitably inserted. It \nis safe to insert an expression at a program point only if inserting the expression does not change the \nbehavior of the program, The principal observation is that if an expression e is anticipatable at a program \npoint p, then it is safe to insert a com\u00adputation of e at point p. It is profitable to insert a computation \nof expression e at program point p if e is not already available at point p and if the insertion of e \nat p makes some other computations of e (on the program execution path after point p) redundant. 4. \nEliminate redundant comput at ions. The compu\u00adtation of an expression e at program point p is redun\u00addant \nif e is available at p. A redundant computation is replaced by a copy of the result of the computation \nthat is available. The safe-earliest and latest-not-isolated transformations are two techniques for PRE. \nThey differ from each other in the placement of new computations that are inserted in step 3 above: the \nsafe-earliest strategy is to place compu\u00adtations as early in the program execution path as possible, \nwhereas the latest-not-isolated strategy places computations as late as possible to minimize register \npressure. 2,2 Canonical form of range checks Range checks of the form (if (not (subscript-expression \nS upper-bound-expression)) then TRAP) are expressed in the canonical form: (Check (range-expression < \nrange-constant)), where range-expression contains all the symbolic terms of the check and all the constants \nin the check are folded into the range-constant term; e.g., the check (if (not (i+l < 4* N)) then TRAP) \nis expressed as (Check (i 4*N < l)). The symbolic terms in range-expression are also listed in a canon\u00adical \norder whenever rearrangement of the terms is possible. Lower bound checks are expressed in canonical \nform after negating both sides of the inequality; e.g., the check (if (not (i+l z 4)) then TRAP) is expressed \nas (Check (-i < -3)). The canonical form is used so that semantically equiva\u00adlent range checks (which \nmay be syntactically different) fall into the same equivalence class in step 1 of the PRE al\u00ad gorithm. \nIntuition suggests that larger equivalence dames (fewer classes) usually increase the number of redundant \nex\u00adpressions. Thus, the intent and effect of the canonical form Program fragment SSA representation PRX \nINX Classification j=(l j~=o k=3 ko=3 m=5 m=5 for i = O to n 1 do fori=Oton ldo i h Linear jl = @(jo, \nj2) kl = @(ko, kz) j=j+l jz=jl+i h*(h+l)/2 Polynomial k=k+m kz=kl+m ?2 5*h+8 Linear A[k]=2*m+l A[k2]=2*m+l \n2*m+l 11 Invariant endfor endfor Figure 2: Example of induction variable analysls in the Nascent compiler \nof range checks is similar to the global reassociation tech\u00adnique described by Briggs and Cooper [3]. \nThe canonical form also simplifies the representation and manipulation of range checks within the compiler. \nWe use the canonical form to denote range check statements in the rest of the paper. 2.3 Range checks \nusing induction expressions The Nascent compiler uses SSA-based induction variable analysis techniques \nto associate induction expressions with all expressions in a program [7, 18]. Each loop in the pro\u00adgram \nis assigned a basic loop variable which assumes values 0,1,. . . for every loop iteration; each expression \nin the loop is associated with an induction expression which is a function C3 C4 of the basic loop variable. \nInduction expressions are classi\u00adfied as invariant, linear, polynomial, etc., depending on their Figure \n3: Check Implication Graph (CIG) of the program complexity. When possible, the trip counts of natural \nloops fragment shown in Figure 1(a) are also determined by the analysis. Figure 2 shows an examplel of \ninduction variable anal\u00adysis on a loop that has been assigned the basic loop vari\u00ad3 Range check optimizationable \nh. Induction variable analysis determines the induction expressions for all the program expressions; \ne.g., the pro-The goal of range check optimization is to reduce the exe\u00adgram expression kz is associated \nwith the induction expres\u00adcution overhead of range checking without affecting the be\u00adsion 5*h+8. In the \ntable in Figure 2, the column labeled havior of programs. A range check optimization preserves PRX shows \nsome of the program expressions, the column the behavior of a program if (1) an array range violation \nislabeled INX shows induction expressions associated with detected in the optimized program if and only \nif the arraythe program expressions, and the last column shows the clas\u00adrange violation 1s detected in \nthe unoptimized program andsifications of the induction expressions. Induction variable (2) a range violation \nin the optimized program is detectedanalysis also determines that the trip count of the loop is at compile-time \nor at run-time at a program execution point max(O,n). no later than the execution point at which the \nviolation inRange checks are created from either program expres\u00adthe unoptimized program is detected. \nsions (using the abstract syntax tree of the program) or Our solution to the range check optimization \nproblem takesfrom induction expressions (using induction variable analy\u00adthe program with range checks \nas input and optimizes It insis). When we need to distinguish between the two kinds of five steps: checks, \nwe use the abbreviation PRX-Check to mean a check that IS created from program expressions, and INX-Check \nto 1. Construct the check implication graph. A Check mean a check that is created from reduction expressions. \nFor Implication Graph (CIG) is a dkected graph in whichexample, if the upper bound of the array A in \nthe program the nodes represent range checks (all equivalent checksin Figure 2 is 100, then we can use \neither PRX-Check (kz share a node). There is an edge from node C, to node~ 100) or lNX-Check (5*h < 92) \nas the upper bound check C3 in the CIG only if check C, implies check Cj. Fig\u00ad for A[k]. ure 3 shows \nan example CIG. lFor clarity in this example, the loop index variable, 1,is not shown The CIG is required \nfor computing the as strong as in SSA form, and the loop is not decomposed into test and branch relation \non checks: check C, IS as strong us check Cj ifconstructs. there is a path (possibly a trivial path) \nfrom node Ci to node C, in the CIG. 2. Compute safe insertion points for checks, If checks that are \nas strong as check C are anticipatable at a program point, then it is safe to insert check C at the \nprogram point. Computing anticipatable checks requires backward data flow analysis. 3. Insert checks \nat safe and profitable program points. Usually it is safe to insert a check at multi\u00adple points in the \nprogram, and multiple checks can be inserted at each program point. Hence, the compiler has to determine \nthe optimal checks to reinserted at the optimal program points. There are at least five different schemes \nfor inserting these checks, which we describe in section 3.3. 4. Compute available checks and eliminate \nredund\u00adant checks. If checks that are as strong as check C are available at the program point where C \noccurs, then C is redundant and is eliminated from the pro\u00adgram. Computing available checks requires \nforward data flow analysis. 5. Eliminate compile time checks. Checks in the GIG that contain only compile \ntime constants are evaluated in this step. Compile-time checks that evaluate to FaIse are replaced by \nTRAP instructions and are reported to the programmer; checks that evaluate to true are eliminated from \nthe program.  The following subsections explain the parts of our solu\u00adtion in more detail. 3.1 Construction \nof the check implication graph A family is a set of range checks which have the same range\u00adexpression; \ne.g., Check (i+j < 10) and Check (i+j < 11) are in the same family. Within each family, the list of range \nchecks is ordered in increasing order of the range-constants of the checks. Thus, if C; and C~ belong \nto the same family and C, appears earlier in the list than C2, then C, is stronger than CJ. We optimize \nthe construction of the CIG by using fam\u00adilies of checks as nodes of the CIG instead of range checks. \nFor example, the CIG shown in Figure 3 after optimization consists of only two nodes: F 1 and FZ, where \nFI cent ains checks {C3, Cl}, and FZ contains checks {CZ, C4}. In addition to simplifying the detection \nof implications between checks in the same family, using families as nodes of the CIG helps discover \nimplications between checks in differ\u00adent families. Whenever we discover an implication between two checks, \nsay Ci/ (in family FI) and Cj, (in family FJ), we add an edge from node FI to node FJ and give edge (FI, \nFJ) a weight equal to range-con stant(C7/ ) range-constant(G,/ ). Then, for any two checks C, in family \nFI and C3 in family FJ, if range-constant(c, ) + weight(FI, FJ) < range-constant( cf), then we know that \ncheck C, is as strong as check Cj. For example, consider families Fs and Fq shown in Fig\u00adure 4. If we \nfind that Check (n < 6) * Check (m < 10), we add an edge from Fs to F1 with weight 4. Using this edge, \nit is simple to infer that Check (n < 1) is as strong as Check (m < 7), but we cannot infer that Check \n(n < 1) is as strong as Check (m < 3). If an edge E, has to be added between nodes in the CIG which \nalready has edge EJ between them, the weight of EJ is modified to be the minimum of the weights of E; \nand EJ. g-w F3 F4 Figure 4: Example of CIG with families as nodes 3.2 Computing available and anticipatable \nchecks Computing available checks is a forward data flow problem whereas computing anticipatable checks \nis a backward data flow problem. For both data flow problems, a check is killed by a definition of any \nof the symbols in its range-expression. For computing availability, a range check statement gen\u00aderates \na check C as well as all weaker checks (i.e., checks C such that C is as strong as C ). Thus, at merge \nnodes in the CFG, a check C is available after the merge if checks Cl and CZ are available on the inputs \nto the merge such that Cl is as strong as C and CZ is as strong as C. For anticipatability, we use similzw \nbut stronger condi\u00adtions for determining the set of checks generated and those ant icipat able before \na branch. A range check statement gen\u00aderates a check C and all weaker checks that are in the family of \nC. Thus, at branch nodes in the CFG, a check C is ant ici\u00adpatable before the branch if checks Cl and \nCZ are anticipat\u00adable after the branch such that Cl is as strong as C, Cz is as strong as C, and Cl, \nCq, and C are members of the same family. Since anticipatability determines where checks can be inserted, \nthe restriction that check implications are only within families ensures that a check is not inserted \nbefore the definition of one of the symbols in its range-expression. 3.3 Insertion of checks at safe \nand profitable program points This section presents five schemes for selecting program points for placing \nnew checks: no-insertion, safe-earliest and latest\u00adnot-isolated placement, check-strengthening, and preheader \ninsert ion. No-insertion is the simplest scheme in which steps 2 and 3 of our solution are skipped and \nno checks are inserted in the program. The only redundant checks are due to avail\u00adability and compile-time \nconstants (in steps 4 and 5). The safe-earliest and latest-not-isolated transforma\u00adtions, developed by \nKnoop et al. for PRE of arithmetic expressions [12], can be applied to the problem of range check placement \nif the anticipatable and available checks are computed as described in the previous subsection. The safe-earliest \nplacement is preferred to the latest-not-isolated placement because, unlike computation of arithmetic \nexpres\u00adsions, computing a check does not define any variable, and hence performing a check early has \nno effect on register pres\u00adsure. Furthermore, performing a check as early as possible increases the number \nof program points where that check becomes available, which in turn increases the number of other checks \nthat become redundant. Although the safe-earliest transformation always provides profitable placements \nfor arithmetic expressions, when it is applied to the problem of determining placements for range 273 \ninteger A[1..1o] integer A[1..1o] integer A[1.,1o] i = if ( ..) Check (i ... A[i]... else Check (i ... \nA[i+4],.. endif < < 10) 6) Check (i < 10) if (...) Check (i < 10) ...A[i]... else Check (i < 6) ...A[i+4 \n]... endif i=, . Check (i < 10) if (... ) ,..A[ i]... else Check (i < 6) ... A~+4]... endif (a) Original \nprogram fragment (b) After safe-earliest placement (c) After redundancy elimination Figure 5: Example \nof a program where safe-earliest placement is not always profitable integer A[1..1o] integer A[I..1o] \ninteger A[1..1o] Cond-check Cond-check ((1 ((1 < < 2*n), 2*n), k < 2*n 10) < 10) Cond-check Cond-check \n((1 ((1 < < 2*n), 2*n), k < 2*n 10) < 10) doj=lto2*n Check (k < 10) ... A[k]... Check (j < 10) ... A~]... \nenddo doj=lto2*n Check (k < 10) ...A[k]... Check (j < 10) ... A[j]... enddo doj=lto2m ... A[k]... ... \nA~]... enddo (a) Original program fragment (b) After preheader insertion (c) After redundancy elimination \n Figure6: Example ofaprogram with preheater insertion checks (using our anticipatability algorithm for \ndetermining safe program points), it is not guaranteed to produce prof\u00ad itable transformations. For example, \nthe transformation of the program in Figure 5(a) to that in 5(c) increases the num\u00ad ber of checks performed \non the path along the else branch. Check-strengthening, which was proposed by Gupta [9, 10], considers \nprogram points just before range checks in the CFG for inserting new checks. For each check C in the \nprogram, check-strengthening computes the strongest antic\u00adipatable check C that implies the check C, \ninserts C just before C, and eliminates C (the actual mechanism is to re\u00adplace C by C ). The optimization \nof the program m Figure 1(b) to the one in Figure l(c) is an example of strengthening, Check strengthening \ncan be viewed as a conservative form of the safe-earliest placement which misses some of the opti\u00admizat \nions, but which avoids the profitability problem shown in Figure 5. Preheader insertion is a simple and \neffective scheme for hoisting checks out of loops. If check C is anticipatable at the beginning of the \nloop body, and if the range-expression of check C is either invariant or linear in the index variable \nof the loop, then a conditional check, C , is inserted in the preheader of the loop; the check C is conditional \non the loop executing at least once. When the condition can be evalu\u00adat ed at compile time, an ordinary \ncheck is inserted instead of a conditional check. All loops in the program are pro\u00adcessed in an inner \nloop to outer loop manner so that checks from inner loops are hoisted to the outermost loop possible, \nFigure 6 shows an example of check placement performed by preheader insertion. Since Check (k < 10) is \nloop\u00adinvariant, it is hoisted out of the loop as Cond-check ((1 < 2*n), k < 10); this conditional check \nmeans: if (1 < 2*n) evaluates to true, then perform Check (k < 10). Since Check (j < 10) is linear in \nthe loop index vw~able, we perform loop-limit substitution of the index variable, j, to get Check (2*n \n< 10), which is then hoisted out of the loop as the conditional check Cond-check ((1 < 2*n), 2*n < 10). \nFigure 6(b) shows the conditional checks that are inserted in the preheader of the loop and Figure 6(c) \nshows the re\u00addundant checks eliminated from the loop. Preheader insertion is more effective in hoisting \nchecks out of loops than the safe-earliest placement for two rea\u00adsons: (1) safe-earliest placement does \nnot consider condi\u00adtional checks and (2) even when the check to be hoisted out of a loop is not conditional \n(i.e., it is known at compile time that the loop executes at least once), the control +IOW structure \nof w hi Ie loops prevents the check from being an\u00ad ticipatable at the loop preheader. (A CFG transformation \n such as loop rotation can help the safe-earliest placement in such cases by converting while loops into \nrepeat loops.) 3.4 Implementation in Nascent We have implemented the algorithm described in the previ\u00adous \nsubsections in our research Fortran compiler, Nascent. The range check optimizer offers six choices for \ninserting 274 new checks at safe program points: no insertion of checks, check strengthening only, safe-earliest \nplacement of checks, latest-not-isolated placement of checks, insertion of only loop invariant checks \nin preheaders of loops, and insertion of all checks that are linear (which includes loop invariant checks) \nin preheaders of loops. These options permit us to compare the various check placement schemes. The range \ncheck optimizer supports three kinds of im\u00adplications between checks: no implications between checks, \nimplications between checks in different families only, and all implications between checks (within and \nacross families). We use these options to investigate the importance of the implication property of checks \nfor the optimizations. The range check optimizer can create program expression checks (PRX-checks) from \nthe abstract syntax tree repre\u00adsentation of programs as well as induction expression checks (INX-checks) \nfrom the program representation produced by the induction variable analysis phase of Nascent. This op\u00adtion \nallows us to study the effects of using induction variable analysis on range check optimization. 4 Experimental \nresults This section presents experiments that answer the following questions about range check optimization: \n1. Is range check optimization really needed? 2. What is the effectiveness and cost of optimization? \n 3. Does induction variable analysis help? 4. Is the check implication property important?  We use \nthe dynamic counts of instructions as the mea\u00adsure of the execution times of programs. The C back-end \nof Nascent translates Fortran programs into instrumented C programs which are then compiled and executed \nusing their standard input data sets to obtain the dynamic counts of instruct ions. For our suite of \ntest programs, we chose 10 scientific programs from the Perfect, Riceps, and Mendez benchmarks because \nthese are known to contain substantial array-based computation [7]. The particular 10 programs chosen \nfor this study satisfied two criteria: (1) the C back-end of Nascent is not mature and these programs \nrequired no manual editing to fix the C programs, and (2) they had moderate disk space and computation \ntime requirements. The first five columns of Table 1 show the names of the programs and counts of source \nlines, subroutines, and nat\u00adural loops. The two columns labeled instructions show the static and dynamic \ncounts of instructions without range checking in the benchmark programs. Since the bench\u00admark programs \nwere naively translated without any opti\u00admization, the instruct ion counts represent the upper limit \non the number of non-range-check instructions. 4.1 Is range check optimization really needed? The first \nexperiment studies programs to determine whether range check optimization is necessary. In Table 1, the \ntwo columns labeled range checks show the static and dynamic counts of range checks needed for unoptimized \nrange check\u00ading of the programs. The final two columns of Table 1 show the ratios of the counts of range \nchecks to the counts of all other instructions. Since the minimum ratio of the dynamic counts is 22%, \nthe maximum ratio is 66%, and we expect one range check to translate into at least two instructions, \nwe es\u00adtimate that the overhead of executing range checks without any optimization is between 44% and \n132Y0. This is a pes\u00adsimistic estimate of the overhead of range checking because the programs are not \noptimized and we overestimated the number of non-range-check instructions. We conclude that the execution \noverhead of the range checks is high enough to need optimization. 4.2 What is the effectiveness and \ncost of optimization? The second set of experiments measure the effectiveness and compile time cost of \nseven check placement schemes on two kinds of checks: checks constructed from program ex\u00adpressions (PRX-Checks) \nand checks constructed from induc\u00adtion expressions (INX-Checks). The seven check placement schemes are: \n1. NI: redundancy elimination without any insertion of checks, 2. CS: check strengthening only, 3. \nLNI: latest-not-isolated placement, 4. SE: safe-earliest placement, 5. LI: preheader insertion of only \nloop invariant checks, 6. LLS: preheader insertion with loop-limit substitution of linear checks, and \n 7. ALL: loop-limit substitution followed by safe-earliest placement.  The columns of Table 2 show \nthe percentage of checks eliminated by the various optimization; these percentages are with respect to \nthe dynamic counts of range checks shown in Table 1. The last two columns show the com\u00adpilation times \nfor the 10 programs, which consist of a total of 26,307 lines of Fortran source code. These times were \nobtained by compiling Nascent using the -02 option of g++ and executing Nascent on a Sun SPARCcenter \n2000. The penultimate column, which is labeled Range , shows the CPU time (in seconds) taken by the range \ncheck optimiza\u00adtion phase, and the final column (labeled Nascent ) shows the wall clock time (in minutes \nand seconds) required by Nascent to parse, optimize, and generate C code for the 10 programs. The range \ncheck optimization phase takes a significant fraction of the compilation time (as much as computing SSA \nand performing induction variable analysis) because we did not fine tune the implementation for speed; \na more sophisticated implementation might halve the time required for range check opt imizat ion. When \nwe compare the percentage of PRX-Checks elim\u00adinated shown in the rows in Table 2, we find that check \nstrengthening (CS) is marginally better than no check in\u00adsertion (NI). The number of checks eliminated \nby the PRE check placement schemes (LNI and SE) are also very close to those eliminated by ordinary redundancy \nelimination (NI); the maximum improvement is 7% for dyfesm. As expected, safe-earliest placement (SE) \neliminates more checks than the latest-not-isolated placement (LNI), but not by a lot the maximum difference \nis 2. 9~0 for spec77. Both preheader placement schemes (LI and LLS) eliminate a much larger number of \nchecks than the other placement schemes. As ex\u00adpected, placing only loop-invariant checks in loop preheaders \n(LI) is not as good as performing loop-limit substitution of linear checks (LLS), and LLS eliminates \nalmost 30~0 more 275 instructions range checks check/instr (Yo) suite program lines subr loops static \ndynamic static dynamic static dynamic Mendez vortex 710 20 35 2,148 3,694x10b 672 95OX1O J 31 26 Perfect \narc2d 3,964 39 234 16,050 15,288x105 7,810 10,156x10b 48 66 bdna 3,980 42 276 16)236 3,712x106 4,177 \n803x 106 25 22 dyfesm 7,608 77 269 7,039 2,724x106 2,765 1,543 x 106 39 57 mdg 1,238 16 56 4,471 13,653x106 \n1,176 6,344x106 26 46 qcd 2,327 35 168 6,801 1,649x106 2,652 788x106 38 48 spec77 3,885 64 413 15,225 \n14,920x 106 5,538 7,124x106 36 48 trfd 485 7 79 2,052 3,939X106 292 2,332x10S 14 59 Riceps linpackd 797 \n11 41 1,738 135x lob 530 61x10b 30 45 simple 1,313 8 75 5,615 43,545x10S 2,738 26,255x106 48 60 Table \n1: Program characteristics of benchmark programs vortex arc2d bd na dyfesm mdg qcd spec77 trfd linpackd \nsimple Range Nascent NI 89.88 84.60 90.85 69.96 79.70 78.75 81.61 61.01 65.90 92.25 6.8 1:18 Cs 89,89 \n85.64 90.87 69.96 80.10 78.79 84.57 61.01 65.90 93.94 15.9 1:27 PRX- LNI 89.88 84.61 90.85 76.82 79.70 \n78.75 84.44 61.01 65.90 92.26 18.4 1:29 Checks SE 89.89 85.64 90.87 76.83 80.10 78.79 87.35 61.01 65.91 \n93.95 16.7 1:28 LI 89.88 84.60 90.85 69.96 79.70 78.75 81.61 61.01 65.90 92.25 11.0 1:22 LLS 99.99 99.96 \n98.44 98.74 98.53 97.00 96.67 98.74 99.73 99.97 13.0 1:24 ALL 99.99 99.96 98.44 98.73 98.54 97.06 98.24 \n98.74 99.73 99.97 24.0 1:35 NI 89.88 86.08 87.62 69.87 78.62 78.28 81.59 60.95 65.47 92.22 11.5 1:22 \nCs 89.89 87.63 87.64 69.87 79.02 78,28 84.54 60.95 65.47 93.92 23.4 1:36 INX- LNI 89.88 86.08 87.63 76.73 \n78.62 78.28 84.38 60.95 65.47 92.24 27.1 1:37 Checks SE 89.89 87.63 87.64 76.73 79.02 78.50 87.32 60.86 \n65.47 93.93 24.9 1:36 LI 89.88 94.21 90.87 77.99 78.83 85.19 87.53 81.77 67.63 95.96 17.9 1:29 LLS 99.99 \n98.96 95.81 98.17 97.65 96.57 99.70 98.66 99.72 99.96 21.1 1:32 ALL I 99.99 98.96 95.81 98.16 97.65 96.80 \n99.71 98.57 99.73 99.96 36.7 1:47 I Table 2: Percentage of checks eliminated by optimizat ions and time \nrequired for compilation. NI = redundancy elimination wit h no insertion of checks, CS = check strengthening, \nLNI = latest-not-isolated placement, SE = safe-earliest placement, LI = preheader placement of only loop \ninvariant checks, LLS = preheader placement with loop-limit substitution of linear checks, ALL = LLS \nfollowed by SE. Range = CPU time (in seconds) required by range optimization, Nascent = wall clock time \n(in minutes and seconds) required by Nascent for all 10 programs. checks than LI in the program dyfesm. \nFurther optimization such as loop-limit substitution followed by the safe-earliest placement (ALL) provides \na negligible improvement in the number of checks eliminated. Comparing the times required for various \nrange check optimizations shows that no insertion (NI) is fastest (obvi\u00adously!), the preheader insertion \nschemes (LI and LLS) are moderately expensive, and the PRE-based schemes (CS, LNI, SE) are the slowest. \nAs expected, within the PRE\u00adbased placement schemes, check strengthening (CS) is faster than safe-earliest \nplacement (SE), which is faster than the latest-not-isolated placement (LNI). Similarly, within the preheader \ninsertion schemes, LI is slightly faster than LLS. Based on the number of checks eliminated and the compile \ntime required, preheader insertion with loop-limit substitu\u00adtion of linear checks (LLS) is the clear \nwinner among all the check placement schemes  4.3 Does induction variable analysis help optimization? \nTable 2 shows a surprising result when we compare the num\u00adber of PRX-Checks eliminated with the corresponding \nnum\u00adber of INX-Checks: there are a number of cases such as bd na where a few more PRX-checks were eliminated \nthan the INX-checks. This result is unexpected because induc\u00adtion expressions should hold at least as \nmuch semantic in\u00adformation as the program expressions. one explanation is a particular weakness in the \nimplementation of induction variable analysis in Nascent that we have not repaired yet. Even so, range \ncheck optimization of INX-checks is never very bad compared to PRX-checks, and there is one case, LI \noptimization of trfd, where about 20% more checks were eliminated due to induction variable analysis; \nin this case, induction variable analysis could detect more loop invari\u00adant checks. Until we repair the \nimplementation of induction variable analysis in Nascent and further experiments indi\u00adcate otherwise, \nwe conclude that using induction variable analysis does not help range check opt imizat ion.  4.4 Does \nimplication between checks help optimization? The third experiment measures the effectiveness of the \ncheck implication property in detecting and eliminating redundant checks. We used variations of the check \nimplication graph to produce another three check placement schemes: 1.NI : redundancy elimination without \nany insertion of checks and with no implications between checks, 2. SE : safe-earliest placement with \nno implications be\u00adtween checks, and 3. LLS : preheader insertion with loop-limit substitution of linear \nchecks with no implications between checks in the same family, but with implications between checks \n 276 vortex ar-c2d bdna dyfesm mdg qcd spec77 trfd linpackd simple Range Nascent NI 89.88 84.60 90.85 \n69.96 79.70 78.75 81.61 61.01 65.90 92.25 6.8 1:18 NI 89.87 82.94 88.85 69.93 79.29 78.72 76.99 61.01 \n65.88 90.55 8.7 1:20 PRX\u00ad w 89.89 85.64 90.87 76.83 80.10 78.79 87.35 61.01 65.91 93.95 16.7 1:28 IIChecks \nSE LLS I 89.87 99.99 82.94 99.96 88.85 98.44 76.79 98.74 79.29 98.53 78.72 97.00 79.79 96.67 61.01 98.74 \n65.88 99.73 90.57 99.97 I 18.9 13.0 1:30 1:24 ! ] LLS 99.99 99.96 98.32 98.68 95.19 96.84 92.30 98.74 \n99.73 99.58 12,9 1:23 [ I NI 89.88 86.08 87.62 69.87 78.62 78.28 81.59 60.95 65.47 92.22 11.5 1:22 I \nNI 89.87 84.05 85.75 69.84 7!3.23 78.25 76.96 60.95 65.44 90.52 14.9 1:25 INX\u00ad 89.89 87.63 87.64 76.73 \n7(9.02 78.50 87.32 60.86 65.47 93.93 24.9 1:36 Checks % 89.87 84.05 85.75 76.70 73.23 78.48 79.74 60.86 \n65.44 90.55 30.5 1:41 LLS 99.99 98.96 95.81 98.17 9 7.65 96.57 99.70 98.66 99.72 99.96 21.1 1:32 LLS \n 99.99 98.96 95.81 98.11 9 7.65 96.54 99.64 98.66 99.72 99.95 22.0 1:32 Table 3: Percentage of checks \neliminated by optirnizations with and without implications between checks, and time required for compilation. \nNI = redundancy elimination with no insertion of checks, NI = NI with no implications between checks, \nSE = safe-earliest placement, SE = SE with no implications between checks, LLS = preheader placement \nwith loop-limit substitution of linear checks, LLS = LLS with no implications between checks within the \nsame family. Range = CPU time (in seconds) required by range optimization, Nascent = wall clock time \n(in minutes and seconds) required by Nascent for all 10 programs. in different families; this maintains \nimplications from conditional checks inserted in loop preheaders to the corresponding checks in the loop \nbodies. Table 3 shows the results of these three check placement schemes on the two kinds of checks. \nWhen we examine the number of checks eliminated without using check implica\u00adtions, we find a marginal \ndecrease (< 3%) in almost all cases. There is only one program, spec77, where 7% fewer checks were eliminated \nwithout use of the check implication property! Why do optimizations that use implication (NI and SE) \ntake less time than optimizations that do not use implication (NI and SE )? We did not modify the implementations \nof the anticipatability and availability data flow algorithms to take advantage of the fact that there \nare no implications between checks in NI and SE ; these implementations seau-ch the check implication \ngraph for implied checks. Hence, in the case of NI and SE , where no check implies any other, each check \nis inserted in a separate family, and the increase in the number of nodes of the CIG increases the time \nrequired by the data flow algorithms, which increases the time required for range check optimization. \nThe results of this experiment indicate that the prop\u00aderty of implication between checks is not very \nimportant for range check optimization the only important impli\u00adcations are those from checks inserted \nin loop preheaders to the corresponding checks in the loop bodies. 5 Related work Related work on range \ncheck optimization can be partitioned into two groups. The first group concentrates on the prob\u00adlem of \nidentifying range checks which can be evaluated. at compile-time and eliminated from the program; this \nincludes the automated program verification approach [8, 17] and the abstract interpret at ion approach \n[4, 5, 11, 14, 16]. The sec\u00adond group aims to reduce the execution overhead of range checks which cannot \nbe evaluated and eliminated at compile\u00adtime; this includes algorithms that perform data flow analy\u00adsis \nand insertion of checks [2, 9, 10, 13]. Our algorithms are in the second group. Suzuki and Ishihata [17] \nand German [8] used Floyd-Hoare logics and theorem proving techniques to verify the absence of array \nrange violations in programs. Two limita\u00adtions of the program verification approach are that it often \nrequires the programmer to supply assertions to aid the ver\u00adification proofs and that it is restricted \nto programs written in a structured manner (without goto statements). Hence, we feel that this approach \nis not directly applicable to the problem of automatic range check optimization of arbitrary programs. \nThe abstract interpretation algorithms [4, 5, 11, 14, 16] perform generation, propagation, and combination \nof asser\u00adtions about the bounds of variables to determine compile\u00adtime checks. The different algorithms \nvary in the sophis\u00adtication of the rules used for propagation and combination of the assertions: the \nrules implemented in the Karlsruhe Ada compiler [16] seem the simplest (and probably are the fastest) \nand those proposed by Cousot and Halbwachs [5] are quite complex. Since the algorithms in the abstract \ninter\u00adpretation approach and the program verification approach do not perform any insertion of checks \nin the program (steps 2 and 3 of our solution), they take advantage only of com\u00adpletely redundant checks \nand they miss opportunities for exploiting partially redundant checks. The main weakness of these algorithms \nis that they do not attempt to reduce the run time overhead of checks which cannot be evaluated at compile \ntime. Hence we expect the number of checks elimi\u00adnated by these algorithms to be less than algorithms \nwhich insert checks. It seems curious that both the implementations of Ada compilers [14, 16] use partial \nredundancy elimination for the optimization of most program expressions, but not for opti\u00admizing range \nchecks. Perhaps these compilers are conserva\u00adtive due to the Ada language requirement that the compiler \nis not permitted to move a computation to a program point which might change the exception handler that \nis invoked in case an exception occurs in the computation. Markstein, Cocke, and Markstein [13] presented \nthe first paper that addresses the problem of reducing the execution overhead of range-checks. They described \nan algorithm that is like a restricted form of preheader check insertion; the only checks that it considers \nfor preheader insertion are the checks present in articulation nodes in the loop body (be\u00ad cause these \nnodes post-dominate the loop entry nodes and dominate the loop exit nodes) and which have simple range \nexpressions. More recent approaches [9, 10] and our algo\u00ad 277 rithms handle checks with more complex \nrange expressions and use data flow analysis to relax the restriction about checks in articulation nodes. \nIn light of our experimental results, which show that the additional sophistication may not be cost effective, \nit would be interesting to implement the Markstein et al. algorithm in Nascent to compare its effectiveness \nwith the loop-limit substitution algorithm. The range check optimization algorithm presented in this \npaper is baaed on work by Gupta [9, 10]. We use a partial redundancy elimination framework to implement \nthe data flow analysis and optimizations described by Gupta. It is not clear how Gupta represents implications \nbetween checks in the compiler; we have proposed check implication graphs to denote and manipulate implications \nbetween checks. In contrast to Gupta s rules for determining whether loop-limit substitution is applicable \nto a check within a loop, we use induction expressions and induction type classifications (in\u00advariant \nand linear) produced by Nascent. Our experimental results match the limited results presented by Gupta. \nAsuru [2] also extends Gupta s range check optimization algorithms. Our loop-limit substitution technique \nis sim\u00adilar to his conservative expression substitution algorithm. His technique of loop guard elimination \nis a restricted form for exploiting implications between conditional checks in the check implication \ngraph. A weakness of Asuru s proposal to exploit range checks that post-dominate an array reference is \nthat it does not detect a range violation until after it occurs; we avoid this problem in our opt imizat \nions. Conclusions We have synthesized an algorithm for range check opti\u00admization from previously develop \ned t echruques, and we have shown the relationship of the previous techniques with this algorithm. We \nimplemented the range check optimizer and provided a thorough experimental evaluation of different al\u00adternatives \nfor range check optimizations on a suite of non\u00adtrivial Fortran programs. Our experimental results indicate \nthat insertion of range checks in programs is beneficial and that there are signifi\u00adcant differences \nin the number of checks eliminated by dif\u00adferent check placement schemes. Simple optimizations such as \npreheader insertion with loop-limit-substitution of linear checks greatly reduce the execution overhead \nof range checks with only a moderate increase in compilation time. Increas\u00ading the sophistication of \nthe analysis and optimization algo\u00adrithms increases compilation time, but does not necessarily produce \nappreciable improvements. Although our experimental results were obtained on a specific set of programs \nwritten in Fortran, we believe that they would be applicable to a larger variety of programs written \nin other languages. These results should improve the practical usefulness of range checking by encouraging \ncompiler writers to perform range check optimization and programmers to use range checking in production \nversions of programs. References [1] A. V. Aho, R. Sethi, and J. D. Unman. Cornpders, Princi\u00adples, Techniques, \nand Took. Addison Wesley, 1986. [2] J. M. Asuru. Optimization of array subscript range checks, ACM Letters \non Programming Languages and Systems, vol. 1, no. 2, 109-118, June 1992. [3] P. Briggs and K. D. Cooper. \nEffective partial redundancy elimination. Proceedings of the ACM SIGPLA N 94 Con\u00adference on Programming \nLanguage Design and Implemen\u00adtation, 159-170, June, 1994. [4] P. Cousot and R. Cousot. Abstract interpretation: \nA unified lattice model for static analysis of programs by construction or approximation of fixpoints, \nConference Record of the 4ih ACM Symposzum on Principles of Programming Languages, 238-252, January, \n1977. [5] P. Cousot and N. Halbwachs. Automatic discovery of linear restraints among variables of a program. \nConference Record of the @h ACM Symposium on Principles of Programming Languages, 84-96, January, 1978. \n[6] R. Cytron, J. Ferrante, B. K. Rosen, M. N. Wegman and F. K. Zadeck. Efficiently computing static \nsingle assignment form and the control dependence graph. ACM Transactions on Programming Languages and \nSystems, vol. 13, no. 4, pp. 451 490, October, 1991.  [7] M. P. Gerlek, E. Stoltz, and M. Wolfe. Beyond \ninduc\u00adtion variables: Detecting and classifying sequences using a demand-driven SSA form. ACM Transactions \non Program\u00adming Languages and Systems, to appear. [8] S. M. German. Automating proofs of the absence \nof common runtime errors. Conference Record of the @h ACM Sym\u00adposium on Principles of Programming Languages, \n105-118, January, 1978. [9] R. Gupta. A fresh look at optimizing array bound checking. Proceedings of \nthe ACM SIGPLAN 90 Conference on Pro\u00adgrammmg Language Design and Implementation, 272-282, June, 1990. \n[10] R. Gupta. Optimizing array bound checks using flow analy\u00adsis. ACM Letters on Programming Languages \nand Systems, vol. 2, nos. 1-4, 135-150, March-December, 1993. [11] W. H. Harrison. Compiler analysis \nfor the value ranges for variables. IEEE i ransactaons on Software Engineering, SE\u00ad3, 3, 243-250, May, \n1977. [12] J. Knoop, 0, Riithing, and B. Steffen. Lazy code motion. Proceedings of the ACM SIGPLAN 92 \nConference on Pro\u00adgrammmg Language Design and Implementation, 224-234, June, 1992. [13] V. Markstein, \nJ. Cocke, and P. Markstein. Optimization of range checking. Proceedings of the SIGPLA N 8.2 Sympo\u00adsium \non Compiler Construction, 114-119, June 1982. [14] E. Morel. Data flow analysis and global optimization. \nin Methods and tools for compiler construction, B. Lorho (cd.), Cambridge University Press, New York, \n289-315, 1984. [15] E. Morel and C. Renvoise. Global optimization by suppres\u00adsion of partial redundancies. \nCommunications of the A CM, vol. 2, no. 2, 96-103, February, 1979. [16] B. Schwarz, W. Kirchgassner, \nand R. Landwehr. An opti\u00admizer for Ada design, experiences and results. Proceedings SIGPLA N 88 Conference \nma P.oy-ammzng Language De\u00ad sign and Implementation, 175-185, June, 1988. [17] N. Suzuki and K. Ishihata. \nImplementation of an array bound checker. Conference Record of the 4bh ACM Sym\u00adposium on Principles of \nProgramming Languages, 132-143, January, 1977. [18] M. Wolfe. Beyond induction variables. Proceedings \nof the ACM SIGPLAN 92 Conference on Programming Language Design and Implementation, 162-174, June, 1992. \n 278  \n\t\t\t", "proc_id": "207110", "abstract": "<p>This paper presents a compiler optimization algorithm to reduce the run time overhead of array subscript range checks in programs without compromising safety. The algorithm is based on partial redundancy elimination and it incorporates previously developed algorithms for range check optimization. We implemented the algorithm in our research compiler, Nascent, and conducted experiments on a suite of 10 benchmark programs to obtain four results: (1) the execution overhead of naive range checking is high enough to merit optimization, (2) there are substantial differences between various optimizations, (3) loop-based optimizations that hoist checks out of loops are effective in eliminating about 98% of the range checks, and (4) more sophisticated analysis and optimization algorithms produce very marginal benefits.</p>", "authors": [{"name": "Priyadarshan Kolte", "author_profile_id": "81100298774", "affiliation": "Department of Computer Science and Engineering, Oregon Graduate Institute of Science & Technology", "person_id": "P228361", "email_address": "", "orcid_id": ""}, {"name": "Michael Wolfe", "author_profile_id": "81100031703", "affiliation": "Department of Computer Science and Engineering, Oregon Graduate Institute of Science & Technology", "person_id": "PP48026377", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/207110.207160", "year": "1995", "article_id": "207160", "conference": "PLDI", "title": "Elimination of redundant array subscript range checks", "url": "http://dl.acm.org/citation.cfm?id=207160"}