{"article_publication_date": "06-01-1995", "fulltext": "\n Efficient Building and Placing of Gating Functions * Peng Tu and David Padua Center for Supercomputing \nResearch and Development University of Illinois at Urbana-Champaign 1308 W. Main Street, Urbana, Illinois \n61801-2307 tu ,padua@csrd. uiuc. adu Abstract In this paper, we present an almost-linear time algorithm \nfor constructing Gated Single Assignment (GSA), which is SSA augmented with gating functions at @ nodes. \nThe gating functions specify the control dependence for each reaching definition at a @-node. We introduce \na new con\u00adcept of gating path, which is a path in the control flow graph from the immediate dominator \nu of a node o to v, such that every node in the path is dominated by w Previous algo\u00adrithms start with \n# function placement, and then traverse the control flow graph to compute the gating functions. By formulating \nthe problem into gating path construction, we are able to identify not only a ~ node, but also a gat\u00ading \npath expression which defines a gating function for the b node. Introduction The Gated Single-Assignment \n(GSA) program representa\u00ad tion is an extension of the Static Single Assignment (SSA) representation [CFR+ \n91]. GSA was introduced by Bal\u00adlance, Ma,ccabe and Ottenstein as a part of Program De\u00adpendence Web (PDW) \n[BM090]. GSA is a convenient representation for several program analysis and optimiza\u00adtion techniques, \nincluding constant propagation with con\u00additional branches [WZ91]; equality of symbolic expressions [AWZ88, \nHav93]; induction variable substitution [W0192]; symbolic dependence analysis [B E94] and demand-driven \nsymbolic analysis for array privatization [TP95, TP93]. In the SSA representation, @ functions of a single \ntype are placer! at the confluence nodes of a program flow graph to represent different definitions of \na variable reaching from different incoming edges. The condition under which a definition reachs a confluence \nnode is not represented in the d function. By contrast, in the GSA representation, several types of gating \njunctions are defined to represent the different condition classes at different confluence nodes. The \nresearch described was supported by contract DAB T63-92\u00adC-0033 from the Advanced Research ProJect Agency, \nThis work IS not necessarily representative of the positions or pohcles of the U. S. Army or the government. \nSIGPLAN 95La Jolla, CA USA 1995 ACM 0-89791 -697-2/95/0006 Some extra parameters are introduced in the \ngating func\u00adtions to represent the conditions. In this paper, we present an almost linear time algorithm \nto construct the GSA. The new algorithm is more efficient and simpler than the ex\u00adisting algorithms for \nGSA construction [BM090, Hav93]. Since SSA is a special case of GSA, it can also be used as an efficient \nalternative algorithm for SSA construction. The existing algorithms for building the GSA follow two steps. \nThe first step is the same ~ function placement pro\u00adcedure as in the SSA construction [CFR+ 91]. In the \nsec\u00adond step, the GSA conversion algorithms collect the control dependence of the definitions reaching \na ~ function and transforms the ~ function into a gating function. The orig\u00adinal GSA conversion algorithm \n[BM090] assumed a Pro\u00adgralm Dependence Graph (PDG) [FOW87] as its initiaJ rep\u00adresentation. Havlak developed \nanother algorithm [Hav93] to construct a variant of the GSA, known as Thinned GSA. Because it starts \nwith the program flow graph, it is, there\u00adfore. somewhat simrier. For each d function. both al\u00adgorithms \ntraverse tie control flow graph to find the gat\u00ading conditions for each reaching definition. To convert \na ~ function to a gating function, O(E) edges may be vis\u00adited (where E is the number of edges in the \nflow graph). Since the number of b functions in the momam is O(N) (where N is the number of nodes in \nthe p~og~am), and th~ same edge may be visited for every q$ function, the time complexity of these algorithms \nis O(E x N). The algorithm in this paper constructs and places the gating functions from a program control \nflow graph in a sin\u00adgle step. In our algorithm, SSA and GSA constructions are unified under a single \nprocess of gating path construction. It uses the path compression technique [Tar79] to reduce the total \nnumber of visits to the edges in the flow graph. Tarjan describes two ways to implement the path compres\u00adsion,, \nA simple method has an O(E log(lV)) time bound; a sophisticated off-line algorithm maintaining balanced \nsub\u00adtrees has an O(Ea(E, N)) time bound. Yet another on-line O(EW(E, N)) method, called stratified path \ncompression by Farrow [Far77, Tar79], can also be used. This GSA algo\u00adrithm is also almost as efficient \nas the best known algo\u00adrithms for @ function placement in SSA conversion. The rest of the paper is divided \ninto the following sec\u00adtions. In Section 2, we introduce some background and no\u00adtations. In Section 3, \nwe define the notation of gating path and discuss its relation to the dominance frontiers in the cent \next of ~ placement. Our gating path based GSA con\u00adstruction algorithm is presented in Section 4. In section \n5, we work through an example. Section 6 shows some timing results using the simple path compression \nimplement ation. Some conclusions are presented in Section 7. 2 Background and Notations Representing \ndata flow and control flow properties of pro\u00adgrams is an important issue in optimizing compilers for \nac\u00adcurate and efficient transformations. SSA form has been shown to be useful in capturing the data flow \ninforma\u00adtion required by some important program optimizations [AWZ88, RWZ88]. In the SSA form, each definition \nof a variable is given a unique new name, and each use of a variable is renamed to refer to a single \nreaching defini\u00adtion. tVhen several definitions of a variable, al, ag, . . . . am, reach a confluence \nnode in program control flow graph, a ~ function assignment statement, an = ~(al, u2, . . . . am), is \ncreated to merge them into a single new definition an. Hence, in the SSA representation every use of \na variable ex\u00adcept those in a ~ function has only one reaching definition that is identified by a unique \nvariable name. SSA captures the data flow information (clef-use chains) of a program in a compact form. \nAn efficient algorithm for constructing SSA with a min\u00adimal number of ~ functions was originally designed \nby Cytron, Ferrante, Rosen, Wegman and Zadeck [CFR+ 91]. The algorithm for placing the &#38;functions \nis 0(N2 ) in the worst case, but often appears to be linear when ap\u00adplied to real programs. Johnson and \nPingali [JP93, JPP94] proposed another algorithm to place ~ functions in O(E) time. Later, Cytron and \nFerrante proposed an almost linear time O(Ea(E)) using path compression. Recently, Sreed\u00adhar and Gao \n[SG94] have developed another O(E) time al\u00adgorithm. We should point out that although our algorithm also \nuses the path compression technique, our problem is more complicated and our approach is completely different \nfrom Cytron and Ferrante s. Whereas a ~ function represents the merge of multiple reaching definitions, \nit does not contain the condition that specifies which reaching definition will be the value of the function. \nGating functions were introduced by Ballance, Maccabe and Ottenstein [BM090] to capture the control conditions \nthat guard the paths to a ~ function. There are three types of gattng function: The y function, which \nis an if then else construct, captures the condition for each definition to reach a confluence node. \nFor instance, Xa = y(13, Xl, A-z) rep\u00adresents X3 = Xl if B and X2 if =B. The p function, which only appears \nat loop headers, selects the initial and loop-carried values. For instance, .Yz = p(.YO, X3 ) represents \nthat Xg s initial value is XO and its subsequent value is X3. The ~ function determines the value of \na variable at the e~lt of the end of the loop. In [BM090], the conversion to GSA is done after d placement. \nThe algorithm works by expanding each @ node into a GSA gating tree that contains the control in\u00adformation \nfor the different reaching definitions. The trans\u00adlation for each 4 node may potentially scan all the \nedges in the flow graph. Therefore, O(E) is its worst case time complexity. The translation for all the \n@ nodes is O(E x N) because there can be 0(.V) confluence nodes in a program. Paul Havlak introduced \na, variant of GSA, called Thinned GSA, and an algorithm for ~ translation that is similar to the original \nGSA algorithm with the same complex\u00adity. Both algorithms start from the ~ nodes after the SSA ~ placement. \nUsing the control dependence graph, they extract the related control information by walking through the \nflow graph along the paths that connect the definitions and the @ function. A program control flow graph \nCFG = (N, E) is a directed graph whose nodes N are the basic blocks in a program. Each edge u -v c E \nin the graph represents a possible flow of control from u to u. Two additional nodes, Entry and Exit, \nare added to the flow graph such that every entrance block of the program has an edge from the Entry \nnode and every exit block of the program has an edge to the Exit node. A node v domznates another node \nw, denoted as UBW, if every path from Entry to w contains v. A node u post\u00addominates another node w if \nevery path from w to Exit contains u. Node v strictly dominates w, denoted as v >> w, if v dominates \nw and v # w. Node v is the immediate dom\u00adinator of w, denoted as v = idorn(w), if v dominates w and every \nother dominator of w dominates v. Every node in a flow graph except Entry has a unique immediate dom\u00adinator. \nThe edges {Liom{w) _ UIW c N {Entry}} form a dominator tree such that v dominates w if and only if v \nis a proper ancestor of w in the dominator tree (where the word proper means v # w). The postdominator \ntree is defined similarly using the post-dominating relation. In the rest of the paper, the words predecessor, \nsuccessor, and path refer to the flow graph, and the words parent, child, ancestor, and descendant refer \nto the dominator tree. The dominance ~r-ontier [CFR+91] DI (A ) of a C FG node X is the set of nodes \nY c G FG such that X dom\u00adinates a predecessor P of Y but does not strictly dominate 1 -: DF(X) = {Y](3P \n--i Y)(X~P and X > Y)}. Given a set X of the CFG nodes, the set DF(X) is the union of the dominance frontiers \ndefined by each node in x: DF(,Y) = uz6xDF(X). The iterated domtnance frontier DF+(X) is the transitive \nclosure of DF(X): DF1(X) = DF(X); DF (X) = DF(X u DFC-l(X)). The iterated dominance frontier DF+ (X) \nfor a set of nodes X is defined as a union of individual iterated dominance frontiers. A fundamental \nresult proven in [CFR+ 91] states that if X is the set of assignment nodes for a variable V, then DF+ \n(X) is the minimum set of nodes that need ~ function assignment nodes for V. 3 Gating Paths and @-Function \nNodes In this section, we present another way to determine the set of nodes that need ~ function assignment \nnodes. We prove that the phi-nodes computed are the same as those us\u00ading the iterated dominance frontier \nalgorithm in [CFR+ 91]. This provides a,way to look at the problem from a different perspective. Definition \n1. Given a control flow graph CFG, a gattng path for a node v is a path in the CFG from idom(v) to v \ncontaining only the proper descendants of idom (v) in the dominator tree as intermediate nodes. 48 In \nother words, a gating path is a path from idom( u) to o in the CFG such that every node in the path is \ndominated by id07fZ(~). In the rest of this section, we prove a theorem that relates the gating paths \nto the placement of @functions. The Lemma 1 and its Corollary establish the existence of gating paths \nin CFG. Lemma 1.Foranypath d~ vin CFG, ifd>>v and d only occurs once in the path, d must dominate every \nnode in the path. Proof. If there is a node u in the path such that u is not dominated by d, i.e., d \n~ u z v, d ~ u, then the path Entry ~ u ~ v avoids the d. This is a contradiction of d>v. Corollary 1. \nFor every node v, there is a gating path from idom(v) to v. Proof. Since tdorn( v) > v, there is a path \nP from idorn(v) to v. Removing all the idom(v) : idorn (v) cycles, we obtain a new patil from idorn( \nv) to v where idorn(v) only occurs once. By Lemma 1, the idorn(v) dominates ev\u00ad exy node in the new path. \nHence the new path is a gating path. Lemma 2 and Lemma 3 establish the fact that dominance frontier relations \nonly exist among the nodes in the sibling subtrees under a common parent in the dominator tree of CFG. \nThat is, if v c DF+ (.Y), then r dom(v) B X and, therefore, idom(v) is a proper ancestor of A in the \ndomina\u00ad tor tree. Because X ~ U, X and v must belong to different subtrees under idorn(v). Lemma 2. If \nv s DF(X), then zdom(v) > .X . Proof. From the definition of DF(X), X # idorn(v). If idom(v) ~ .Y, then \nthere is a path from Entry to .Y that avoids the idorn(v). Because v e DI (X), there is a w s Pred(v), \nsuch that X>uY, but .Y $ v. Let X ~ w * v be a path from .k to v. Removing the .Y ~ X cycles in the path, \nwe obtain a path such that X appears only once. The idorn(v) must not be in the subpath X ~ w; otherwise, \nby Lemma 1, X >> idom(v) and, therefore, X >> v. Concatenating this path with the idom (u) avoiding path \nfrom Entry to X, we obtain a path from Entry to v which avoids idom(v). This is a contradiction. Therefore, \nwe have proven idom(v) > X. Lemma 3. If v E DF+(X), then idorn(v) > X. Proof. By applying Lemma 2, if \nv c DF1 (X), then tdom(u) >> .Y. Assume it is true for v 6 DF l. By induction, for v c DF (.Y), let v \nc DF(u), u c DF l (X). We have ~dorn(u ) >> X by induction premises and idom(v) > u by Lemma 1. Because \nidorn(v) > a implies idom(v)~idom(u), we obtain idom(o) B X. The Lemma 4-6 relates the gating paths to \nthe iterated dominance frontiers. Lemma 4. If v E DF(l )j then there is a gating path from dom (v ) to \nv passing through X. Proof. From the definition of DF(.T) and Corollary 1, there is a gating path X L \nw where w e Pred(v). From Lemma 1 and Lemma 2, we have zdorn( U) > X and, hence, a gating path from idom(v) \nto X. Because the path X ~ w contains only the proper descendants of X and, hence, of idorn(v), concatenating \nthe paths results in a, gating path idorn(u) LX ~ W v from idorn(v) to v passing through A . Lemma 5. \nIf v c DF+ (X ), then there is a gating path from zdorn(v) to v passing through .Y. IProof. Immediate \nby induction from Lemma 3 and Le;mma 4. ILemma 6. If there is a gating path from idom(v) through .Y to \nv where idorn(v) # .X, then v c DF+ (X). Proof. We prove this by induction on the number of con\u00ad fluence \nnodes on the subpath A ~ w-u. Since there is a gating path from idorn(v) through X to v and idom(v) # \nX, v must be a confluence node. Otherwise, w # idom(v) would be u s immediate dominator. Let the number \nof con\u00ad fluence nodes on the subpath X ~ v be n. 1, If n = 1, v is the only confluence node, In the path \nfrom Xtov, X~w -V, every intermediate node can have only one predecessor in the flow graph. Hence X~w. \nBecause idom(v) # X and idom(v) B X,X ~ v. Hence v G DF(X). 2. Assume the Lemma is true for n < i. For \nn = Z, let u be the second to last confluence node in X $ w ~ v, and the path Pti = idom(u) ~ u be the \nsubpath from idom(u) to u. If X is in Pu and A # idom(u), then Pu is a gating path for u passing through \nX with n,=k <i. Therefore, a 6 DFk< (X). Moreover, since there is only one non-confluence node from u \nto v, v E DF(u) = DFk+l(X). If X = idom(u) or X is not on P., then X~idom(u) >> u~w where w is the second \nto last node on X ~ idorn(u) &#38; u L w + v. Hence v ~ DF(X). This proves the Lemma. Using the results \nfrom Lemma 5 and Lemma 6, we obtain the fo!lowing Theorem for determining if a node u is in DF+(,%). \nTheorem 1. Given an initial set % of CFG nodes, for any node v in CFG, v ~ DF+ (X) if and only if there \nis a gating Path from zdorn(v) to v containing a node that belongs to 2? (i.e., there is a gating path \nidom(v) ~ X ~ v where X E X ). Proof. Immediate from Lemma, 5, Lemma 6, and the fact that DF+(X) = L.JXEXDF+ \n(X). Applying theorem 1, if we can compute a gating path expression for each node v in a C FG and determine \nif v has a gating path that contains nodes in X, we can determine if v needs a ~ function assignment. \nIn the next section, we present an algorithm which builds a gating path expression of v in such a way \nthat it is exactly the gating function in GSA if v E DF+(X). 4 Algorithm for GSA Construction Given a \nCFG(N, E), we can treat any path in the CFG as a string of edges in E, but not all such strings over \nE are paths in CFG. A path expression [Tar81b] P of type (u, v) is a simple regular expression over E \nsuch that every string in u(P) is a path from node u to node v (where u(P) represents the string generated \nby the regular expression P). Every subexpression of a path expression is also a path expression whose \ntype can be determined as follows. Let P be a path expression of type (u, u). If P = PI u Pz, then PI \nand P2 are path expressions of type (u, v).  If 1? = PI . P2, then there exists a unique node w such \nthat PI is a path expression of type (u, w) and P2 is a path expression of type (w, u).  If P = P?, \nthen u = v and PI is a path expression of type (u, v) = (u, u).  49 50 For instance, in the following \nstatements R = R, . R, : case (RI = 0) or (R2 == 0) return 0 if (B) then case RI = A Blockl return Rz \nelse case Rz = A Block return RI ,.. endif case RI = Y(B, RI,, RI~) \\ ~ .,there are two paths from \nthe if node to the endif node rep\u00ad return -Y(B, (Rlt R2), (Rlf R2)) ~esented by path expressions: pt \n= (t.f (1?) ~ Blockl = Note that in the case where R = RI U Rz and RI = endtf) and pf = (t.f (B) ~ B/ock2 \n- endtf). The nota- Y(B, RI,, RI,), RI and Rz must have the same type. That tion (a b -c) represents \na path of two edges from a to b is, R2 must have the same starting node as RI. Therefore, and from h \nto c. To simplify the discussion, we assume the Rz must be in the form of 7(B, R2t, Rz, ). endif is the \nentry of a basic block. The union of the two Back to the above example, we can obtain path expressions, \n(pt U p~ ), is of type (z.f (B), endif) and represents all paths from the if (B) to the endif. pt(if,endif)= \n7(B, A, 0) . A = ~(B, A,@); 4.1 Path Expressions Functions Represent ed as Gat ing P.f (~.f, endif) = \n= Y(B, ~(B, 0, O, A). A); A Different paths reaching a ~-function node are represented by path expressions. \nOur strategy is to define the sym\u00adbols used to represent the edges such that a path expres\u00ad P(zf, endif) \n= = = pt(if, Y(B, -Y(B, endif) U Pf(i.f, endif) A, @) U Y(B, @, A) A, A). sion takes the same form as \na gating function. Only the outgoing edges from conditional statements (or conditional edges) are necessary \nto unambiguously represent a path. We will represent paths using a form of gating functions Applying \nCytron et al. s renaming procedure [CFR+ 91] to insert variabIe names into a gating function R(u, v) \n= -y(B, R,, Rf ), we need to know from which predecessors of containing only cess of building the conditional \nedges. such gating functions, However, we also in the pro\u00adneed to use u that each of each path with Rt \nand Rf reach the predecessor v. This number is done by of v. If R = labeling RI uR2, the unconditional \nedges. We use a white space symbol A and the path R1 enters o from the ith predecessor of u, then to \nrepresent .P(Blockl, an endi~) unconditional = A and edge. P(Block2, For example, endij) = each of A \nrepre- RI RI is labeled with with a superscript i. This is done i. Hence, by labeling all the A in sents \nan unconditional edge. We now define the gating symbols for the edges of branch P(if, endi~) = p: (i~, \nendi~) U p~(i~, end~.f) statements. A branch statement like if(B) has two outgo\u00ad = ~(B, Al, O) Uy(~,O, \nA2) ing edges. Let s call them Bt and 13~. To build the gating = ~(B, Al, A2). function for a path, we \nuse the gating function notation to represent the edges. The Bt edge is represented by the ex- The superscript \nlabel can be used in the renaming pro\u00ad pression Y(B, A, 0), and the IIf edge is represented by the cedure \nto determine which parameter in a gating function expression 7(B, 0, A). Here we use another white space \nshould be substituted by the name on the top of the stack symbol easy to 0 to extend represent a the \nnotation branch that to statement is not taken. types with It is more for a variable. terested readers \nFor further should refer details of to [CFR+ this 91]. procedure, Consider, in\u00adfor than two outgoing \nbranches. In summary, a path expres\u00ad example, that Bioc.kl contains an assignment to a variable sion \nis represented as a gating function using the following A, which after renaming becomes A~l and that \nBlock2 has symbols: no assignment to A. Let the definition of A reaching the if statement be AO,,g. Then \nthe gating function for A takes A white space symbol .k represents an unconditional on the form: edge. \nAN.W = V(B, A1, A2). A white space symbol O represents an edge not taken at a branch node. In the renaming \nprocess, the reaching definition to v from A y expression -((B, el, ez, .... en) where only one e; is \npredecessor Hence, Al I in is the AB1, gating and from function predecessor is replaced 2 by is Ao~t9. \nA~l and A and all the from an n-way other branch e s are 0 statement represents the with condition i \ns edge P. A2 is replaced by AO~,9. The gatkg function Given a gating expression R(zL, v) representing \na path ANe~ = Y(B, AB1, Ao~,g) from node a to node v, the following equations define the properties of \nthe symbols that can be applied to simplify correctly reflects the reaching definitions from different \nI?(U, 2). paths. In summary, gating functions in GSA specify the condi- R= RI URz:case Rl=O tions for \neach reaching definition at a ~ node. These con\u00ad return Rz ditions are concatenation of conditional branches \non the case Rz = @ gating path of the q5-node. Path expression is a regular return RI expression representing \na path in the flow graph, Using case RI = y(B, RI,, Rlf) and the above representation of path expression, \nwe obtain a R2 = Y(B, R2,, R2f) path expression which is exactly the gating function for the return -y(B, \n(RI, U R2, ), (RIf u R2f )) ~ node. Algorithm: Building and placing gating functions input: The assignment \nnodes X initialize: foreach v ~ N do @(z)) ~ false GP(v) -0 G*(v) + @ ListP(v) + 0 if vEX X(v) -true \nfi od loop: foreach u ~ N in reverse rljn do derive: foreach v E children(u) do foreach e = (w, v) c \nE do ~dom node: if w == u then GP(u) -GP(tl) U (e) sibhng node: else (d,p(sufiroot(w), v)} - EVAL(e) \no(v) O(v) or ~ ListP( U) ,+ ListP(v) with p(stdwoot(w), v) ~ (* The Vvlth operator insert an element \nto a list *) od od segueftce: Topsort(childre n(u)) merge: foreach v ~ children(u) in Toporder do foreach \np(sukoot(w), v) C ListP(v) do mu entry: if suh oot(w) == v then G*(v) _ G*(u) I-Jp(subroot(w), v) gamma: \nelse GP(w) GP(V) U (GP(subToot(w)) . jO(SUbTOO~(W), v)) @(v) -Q(v) or O(sulwoot(w)) fi update: UPDATE(V, \nG+ (v)) LIN1f(u, v) od od od 4.2 Algorithm for Building Gating Functions In this section, we present \nan algorithm to construct the gating path expression and, therefore, the gating function for each node. \nOur algorithm assumes that a CFG and its dominator tree DT are given. We also assume that each node in \nthe CFG is assigned a depth-first number djn. The dominator tree can be computed in O(Ea(E, N)) time \nusing the dominator algorithm of Lengauer and Tar\u00adjan [LT79], or in 0(.E) time using a more complicated \nal\u00adgorithm of Harel [Har85]. The dfn can be computed in linear time [ASU86]. The djn number has the property \nof d~n(idom(v)) < d~n(v) for each node v # Entry. Each loop step of the algorithm processes a set of \nsibling nodes with a common parent u. The outer loop processes nodes in reverse d~n sequence. Since d~n(idom(v)) \n< d~n(v) all the siblings of v and their descendants must have already been visited by the outer loop \nbefore idom(v) is visited. In the derioe phase, the algorithm processes the set of sib\u00adlings children(u). \nDepending on the class to which an edge e = (w, v ) c E belongs, one of the following step is taken 1. \nIf e comes directly from v s immediate dominator (i.e. w = u = idom(v)), then the edge itself is a gating \npath from idom(v) to v. The algorithm updates the gating path of GP(v) with the union of e and the old \nGP(v). 2. If e comes from a node that is a descendant of a sibling of v (i.e. w # u and v > w), the algo\u00ad \nrithm calls EVAL(e) to compute a path expression p(subroot(zo), v). The subTooi(ro) is the root of the \nsubtree to which w belongs (i.e. the sibling of v dom\u00ad inating w). The path expression p(s-abroot(w), \nv] rep\u00adresents all paths from p(subroot(w)) to v which end with edge e and contain only the proper descendants \nof swbroot(w) as intermediate nodes. EVAL(e) also re\u00adturns true if one of the nodes along the path belongs \nto %, indicating by Theorem 1 that v is a phi function node. The gating expression returned from EVAL(e) \nis added to LzstP(v), which is a list containing all the lpaths starting from the siblings of v to v. \n3. If e comes from a node which is a descendant of v (i.e. w # u and v>>w), then it forms a loop with \nheader v. The algorith~ work. exactly as in etep 2 above. The the path expression computed by EVAL(e) \nis later used to update G*(v) to represent the path from the loop back edge e. G* (v) is used to build \na p function 51 for the loop. We will work through an example to illustrate the algo- Using Tarjan s \ntechnique for operations on a forest [Tar79], we define the following operations on the forest of subtrees \nin a dominator tree: EVAL(e): Let e = (w,.). If r = subroot(zo) -WI _ w~+ ... -Wk == w is the tree path \nfrom the root of tree containing w to w, then EVAL returns a path ex\u00adpression representing (R(r) R(w~). \nR.(wz).. . . . R(w). e) with each A superscripted by the predecessor number of e to o. It also returns \nthe value of (O(r) V X(T) V @(wl) v X(wl) ,.. V O(w) V X(W)) indicating whether there is a node in the \npath that belongs to .?. In this expression, V represents the logic or operation. O(z) is true if node \nr needs a pht function, and A (z) is true if x E A?. In the process, EVAL(e) performs path compression \nwhich updates the R s for the intermedi\u00adate nodes and relinks them directly to r.  LIN1{(u, v): If u \nand v are roots, combine the trees with roots u and v by making u the parent of v. LINK may also adjust \nthe tree and the R s to construct a balanced tree for the almost linear time algorithm.  UPDATE(V. ,,, \nP): If v is a root. , assimr \\, to Pif  R(v) v is not the immediate post-dominator of idom(rt); oth\u00aderwise, \nassign R(v) to A. If the CFG is reducible, then the paths between the sib\u00adling trees is cycle free. We \ncan then obtain a, topsort order among the siblings in ch iidren( u). Irreducible graphs can also be \nhandled by computing a path sequence for each dominator strong components. Due to limited space, we cannot \ndetail how to compute the path sequence. Inter\u00adested readers should refer to [Tar81a]. The two existing \nGSA algorithms only handle reducible graph, but the al\u00adgorithm here can be extended to handle irreducible \ngraph. In the merge phase, the algorithm follows the topsort order and computes for each child of u a \npath expression GP(v) representing all the gating paths of v. In the processes, if there is a p(o, v) \nin PLzst(n), indicating that u is a, loop header, step mu entry is executed. G*(u) is used to con\u00adstruct \na, p function for the loop header v. The algorithm completes the processing of the sibling set by executing \nUPDATE(U, GP(v)) and LINIT(u, v) for each child v of u. The LINK(u, v) operation is straightf\u00adorward. \nTwo path expressions are stored at each node v. GP(v) represents the gating path from idorn(v) to v. \nIt is the gating function placed at v if @(v) is true. R(v) represents the path expression from the parent \nof v to v in the tree where v currently belongs. R(v) is used by EVAL to compute the path expression \nfrom v to the root of tree where v currently belongs. R(zJ) may be changed by path compression. The UPDATE(V, \nGP(v)) is slightly different from the Tarj an s algorithm. Here, it contains an optimization step to \nsimplify subsequent path expressiorw. UPDATE(V, GP(u)) sets the value of the label l?(v) to GP(v). If \na v is the immediate post-dominator of idorn( u), then any path from idom(v) unconditionally passes through \nv. Therefore, we can set R(v) to A in the UPDATE op\u00aderation to represent the unconditional reach and, \nin this way, to simplify the path expressions for subsequent calls to EV-AL. This algorithm is a variant \nof Tarjan s fast algorithm for solving path problems using dominator strong components decomposition \n[Tar81 a]. Its correctness can be derived from the following Lemma, which we quote without proof here. \nrithm. Lemma 7. [Tar81a] For edges e = (w, v) in CFG such that w # u, the cor\u00adresponding path expression \nin the ListP(v) computed by the derive phase is an unambiguous path expres\u00adsion representing exactly \nthe paths from sutwoot(w) to v that end with e and contain only proper descendants of subroot(w) as intermediate \nnodes. b For each node v in CFG, GP(v ) as computed by the .. algorithm is an unambiguous path expression \nrepre\u00adsenting exactly the paths from idonz(v) to v that con\u00adtain only proper descendants of idorn(v) \nas intermedi\u00adate nodes. Lemma 7 in conjunction with Theorem 1 prove that the algorithm correctly builds \nand inserts the gating function for a CFG. Theorem 2. The gating path expression algorithm builds and \ninserts the gating functions correctly. The algorithm requires N time on the step initialize; N 1 calls \non UPDATE; N 1 calls on LINK; E calls on EVAL. The top sort at each major loop iteration sorts on disjoined \nsubsets. For the whole algorithm, the topsort time is the summation of individual subset sizes, which \nis O(N). The merge step takes O(N). Hence, the time complexity is O(Ea(E, N)) if the stratified path \ncompression is used to implement the forest operations, and O(E log N) if path compression is used. Because \nthe sequence of EVAL and LINA can be easily determined beforehand, the off-line al\u00adgorithm in [Tar79] \ncan also be used to achieve 0( Ea(13, N)) time complexity, Theorem 3. The time complexity of the algorithm \nis O(Ea(E, N)). For each node v, the algorithm computes the O(v), GP(v) and G* (v). If O(v) is true, \nthen a ,gating function is placed at v. Let X be the variable requiring the gating functions. The gating \nfunctions are built from GP(v) and G* (v) as follows: If G*(v) = 0, then v is not a loop header. The \n-y function X = GF (u) is placed at v.  If G* (v) # 0, then v is a, loop header, A p function X = p(GP(v), \nG*(v)) is placed at v and an ~ function X = q(=G*, X) is placed immediately after v,  The placement \nof -y function is straightforward. For a loop header node v, we construct a p function to select the \nfirst parameter for the first iteration of the loop and the second parameter for the rest. Hence, y s \nfirst parameter should be GP(v) and its second parameter should be G*(v). The ~ function determines the \nexiting value of a variable from V. The exiting paths of a loop are the paths that avoid all the back \nedges(i.e., avoid G*(v)). The ~G*(v) is constructed by reversing all the 0 s in G-(v) to A and the rest \nto 0 s. 5 Working through an Example We use the program in Figure 1 to illustrate how the al\u00adgorithm \nworks. Shown in Figure 2 is the dominator tree of the program. Each node is also labeled with its depth\u00adfirst \nnumber d~n. Solid edges are the dominator tree edges; dashed edges are nontree edges in the CFG. Note \nthat tree edges do not necessarily exist in the CFG. Edges from branch nodes are labeled by the branch \nconditions. 52 1: 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: read(A) if (P) then goto 5 if (Q) then \nA:=5 while (R) do A:=A+l enddo else if (T) then A:=A*3 else A :=A+6 endif endif write(A) Figure 1. A \nprogram example. 5? 1 red A 2 t ?(P) f :...%... ......... 6 8 while andif ?(Q) ~ ;:., ; t ,.,... ;. \n. ... f 9 10 7 A :-A+l 4 \\ .., j write A:-5 elm .! ...+.> .. #... ::......... ...... $./ 11 enddo \n?(T) fi~ i $.. ~,. % .-. .\u00ad 42 3 ............... . ~if A:-A*3 A:-A+6 w.. ------\u00ad ....--.. -. --.. \n.....--..... ---- Figure 2. Dominator tree of the example program. Let us first examine the situation \nwhen the algorithm processes node 11 (statement 9), i.e. a = if (T). Tvhe chil\u00addren of node 11 are nodes \n12,13,14 (statements 10,1 2,13). Node 12 (statement 10) has u as its only predecessor. Since u is its \nimmediate dominator, the idom node branch of the algorithm is executed and obtains GP(12) = Tt. Node \n13 (statement 12) also has a as its only predecessor. Simi\u00adlarly, the algorithm computes GP(13) = Tf. \nIn both cases, there is no assignment statement along the path after u. Hence, @(l I), 0(12) remains \nfalse, indicating that node 12 and 13 do not need rj-functions. Node 14 (statement 13) has two predecessors: \nnode 12 and 13 (statement 10 and 12). In both cases, the sibhng node branch of the algo\u00adrithm is executed. \nBoth nodes are roots of trees contain\u00ading no other nodes. Because node 12 (statement 10) con\u00adtains an \nassignment to A and the edge is an unconditional branch and node 12 (statement 10) is the first predeces\u00adsor \nof node 14 (statement 13), EVAL returns (true, Al). Similarly, EVAL returns Aa and true on the edge from \nnode 13 (statement 12). Hence 0(14) becomes true and LzstP(14) contains two paths starting from its siblings: \nnode 12 and 13 (statement 10 and 12). Following the 53 toporder to process the sibling set ensures that \nwhen merg\u00ading, paths in LMP(14), GP(12), GP(13) already count all the paths from the immediate dominator \nu. The result af\u00adter the merge phase for node 14 (statement 13) is hence (GP(12) A ) U (GP(13) . A ) \n= -y(T, A1, A2). It can then be used by renaming pass to make A14 = -Y(T, A12, A13). LINK is called to \nlink the subtree rooted at each child to u and form a larger subtree of the dominator tree. UPDATE is \ncalled on each child to store the gating path expression for future EVAL calls. Since node 14 (statement \n13) im\u00admediately post-dominates u, its gat ing pat h expression is set to A for future EVAL calls. A \nmore complicated situation happens when branch node 2 (statement ~) is processed (i.e., u = i.f (P)). \nNode 2 (statement 2) has three children in the dominator tree: node 3, 6 and 8 (statement 5, 14 and 3). \nIn the case of node 3 (statement 5), it has three predecessors: node 2, 5, and 9 (statement 2, 7 and \n4). Node 2 (statement z) is idom (3) (the immediate dominator of statement 5), hence GP(3) = Pt. The \npath from node 5 (statement 7) leads to node 3 (statement 5) itself, indicating a loop with node 3 (statement \n5) as the loop header. Therefore, GF (3) = Rt. The path from node 9 (statement 4) evalu\u00ad ates to node \n3 s (statement 5 s) sibling node 8 (statement 3) and the path is Q,. Hence, Q, is inserted into LzstP(3). \nIn the case of node 6 (statement 14), it has two predeces\u00ad sors: node 3 and 14 (statement 5 and 13). \nThe path from node 3 (statement 5) evaluates to its sibling node 3 (state\u00adment 5), which is Rf. The path \nfrom node 14 (statement 13) evaluates to its sibling node 8 (statement 3), which is Qf (note that EVAL \nevaluates to A for the subpath from node 14 to node 11 (statement 13 to statement 9) be\u00adcause R(14) is \nA (as explained in UPDATE(14, GP(14))). Hence, LzstP(6) contains Rf and Qf. Node 8 (statement 3) has \nonly node 2 (statement 2) as predecessor. Since idom(8) = 2, GP(8) equals Pf. The toporder is 8, 3, 6 \nsince node 3 has a path from 8 and node 6 has paths from both 8 and 3. Merge the paths for each node. \nGP(8) remains Pt. GP(3) becomes P, U (Pf ~Q,) = ~(P, A , 7(Q, A , 0)). Since G.P* (3) = Rt, a H function \nis also needed at node 3 (statement 5). The form of the ~ function is ,u(GP(3), GP*(3)) = K(7(P, A1, \n7(Q, A2, 0)), Y(R, A , 0)). For node 6 (statement 14), GP(6) becomes (P, ~ Rf ) U (Pj ~ Qf) = Y(P,Y(R, \n0, A ), ?(Q, 0, A2)). It is easy to verify that 0(3) and 0(6) are set to be true by the al\u00adgorithm indicating \nthey need ~ functions. The final re\u00adsult after renaming will be: for node 3 (statement 5), A3 := P(T(P,Ao, \n?(Q, A9, 0)), 7( R,A4, 0)); and for node 6 (statement 14), Y(P, 7(R, 0, As), -Y(Q, 0, AI*)). 6 :[mpkmentation \nand Measurement We implemented the algorithm using path compression in the POLARIS restructuring compiler \n[BEF+ 94]. The sim\u00adple algorithm uses only path compression and has a com\u00adplexi ty of O(E log N). Following \nare the timing results for the programs in the Perfect Benchmark [CKPK90]. Table 1 shows the characteristics \nof the programs in terms of num\u00adber of edges and number of subroutines. It also shows the total execution \ntime in seconds on a SUN-10 workstation for building the gating functions. Figure 3 shows the timing \nresults for all the subroutines versus the number of edges in the subroutines. Also plotted as a }.eference \nis the shape of the curve for 0.5 + 0.0015 x (Elog(E)) with the same E s as in the codes. Program / Edges \nARC2D II 2493 BDNA 4021 DYFESM 2593 FL052 2312 MDG 1150 MG3D 2999 OCEAN 238(J QCD 2019 SPEC77 3769 TRACK \n2144 TRFD 662 I Routines 42 45 77 30 18 28 38 36 64 32 10 Total Time 3.50 10.11 3.05 3.81 2.16 4.68 5.93 \n2.41 4.10 3.58 0.68 Table 1. Timings of some Perfect Benchmark codes Time for Perfect Benchmark Programs \n(Total 516 Subroutines) 61 I 0.5+0.001 5 EIC9(E) 5\u00ad 4\u00ad 3\u00ad 2\u00ad 1\u00ad 0 0 200 400 600 800 1000 Number of Edges \nFigure 3. Timings of the algorithm with path compression. Conclusions In this paper, we present an almost-linear \ntime algorithm to pIace and build gating functions in a single step for GSA construction. The algorithm \nis based on the well-known path compression technique [Tar79]. Our technique reveals the relationship \nbetween the gating path and the iterative dominance frontier in the flow graph and provides a new perspective \nof the SSA and paried with GSA algorithms, concise and more efficient. demonstrates that it is fast fect \nBenchmark suite. References GSA representation. As com\u00adthe new algorithm is more Our preliminary experiment \nfor the programs in the Per\u00ad [ASU86] A. V. Aho, R. Sethi, and J.D. Unman. Compilers: Principles, Techniques, \nand Tools. Addison-Wesley, 1986. [AWZ88] B. Alpern, M. N. Wegman, and F. K. Zadeck. Detecting equality \nof variables in programs. In Proc. of the 15tti ACM Symposium on Prin\u00adciples of Programming Languages, \npages 1 11, 1988. [BE94] [BEF+94] [BM090] [CFR+91] [CKPK90] [Far77] [FOW87] [Har85] [Hav93] [JP93] [JPP94] \n[LT79] William Blume and Rudolf Eigenmann. The range test: A dependence test for symbolic, non-linear \nexpressions. In Proceedings of Super\u00adcomputing 94, November 1994. Bill Blume, Rudolf Eigenmann, Keith \nFaigin, John Grout, Jay Hoeflinger, David Padua, Paul Petersen, Bill Pottenger, Lawrence Rauchw\u00aderger, \nPeng Tu, and Stephen Weatherford. Po\u00adlaris: The next generation in parallelizing com\u00adpilers. In Proc. \n7th Workshop on Programming Languages and Compilers for Parallel ComPut\u00ading, August 1994. R. Ballance, \nA. Maccabe, and K. Ottenstein. The program dependence web: a representa\u00adtion supporting control-, data-, \nand demand\u00addriven interpretation of imperative languages. In Proceedings of the SIGPLAN 90 Conference \non Programming Language Design and Imple\u00admentation, pages 257 271, June 1990. Ron Cytron, Jeanne Ferrante, \nBarry K. Rosen, Mark N. Wegman, and F. Kenneth Zadeck. Effi\u00adciently computing static single assignment \nform and the control dependence graph. ACM Trans\u00adactions on Programming Languages and Sys\u00adtems, 13(4):451-490, \nOctober 1991. George Cybenko, Lyle Kipp, Lynn Pointer, and David Kuck. Supercomputer performance eval\u00aduation \nand the perfect benchmarks. In Pro\u00adceedings of IC S1 Amsterdam, Netherlands, pages 162-174, March 1990. \nR. Farrow. Efficient on-line evacuation of func\u00adtions defined on paths in trees. Technical report, Rice \nUniversity, Dept. Math. Sci., 1977. J. Ferrante, K. J. Ottenstein, and J. D. Warren. The program dependency \ngraph and its uses in optimization. ACM Transactions on Pro\u00adgramming Languages and Systems, 9(3):319 \n349, June 1987. P. Harel. A linear time algorithm for finding dominators in flowgraphs and related problems. \nIn Proc. of the 17th ACM Symposium on Theory of Computing, May 1985. Paul Havlak. Construction of thinned \ngated single-assignment form. In Pro.. 6rd Workshop on Programming Languages and Compilers for Parallel \nComputing, August 1993. R. Johnson and K. Pingali. Dependence-based program analysis. In Proc. the SIGPLAN \n93 Conference on Program Language Design and Implementation, June 1993. R. Johnson, D. Pearson, and K. \nPingali. The program structure tree: Computing control re\u00adgions in linear time. In Proc. the SIGPLAN \n94 Conference on Program Language Design and Implementationj June 1994. Thomas Lengauer and Robert Endre \nTarjan. A fast algorithm for finding dominators in a flowgraph. ACM Transactions on Program\u00adming Languages \nand Sgstems, 1(1):121 141, July 1979.  54 [RWZ88] [SG94] [Tar79] [Tar81a] [Tar81b] [TP93] [TP95] [WO192] \n[WZ91] B. K. Rosen, M. N. Wegman, and F. K. Zadeck. Global value numbers and redundant compu\u00adtation. \nIn Proc. of the 15th ACM A$YT7tp08k&#38;~ on Principles of Programming Languages, pages 12-27, 1988. \nV.C. Sreedhar and G.R, Gao. Computing @ nodes in linear time using dj-graph. Techni\u00adcal Report Technical \nReport ,ACAPS TechnicaJ Memo 75, McGill University, School of Com\u00adputer Science, January 1994. Robert \nEndre Tarjan. Applications of path com\u00adpression on balanced trees. Journal of ACM, 26(4):690 715, October \n1979. Robert Endre Tarjan. Fast algorithm for solving path problems. Journcd of the ACM, 28(3):594\u00ad 614, \nJuly 1981. Robert Endre Tarjan. A unified approach to path problems. Journal oj the A CM, 28(3):577\u00ad593, \nJuly 1981. Peng Tu and David Padua. Automatic array privatization. In Pvoc. 6rd Workshop on Pro\u00adgramming \nLanguages and Compilers for Parai\u00adlel Computing, August 1993. Peng Tu and David Padua. GSA based demand\u00addriven \nsymbolic analysis for parallelizing com\u00adpilers. In Proc, ACM 1995 International Con\u00adference on Supercomputing, \nJuly 1995. Michael Wolfe. Beyond induction variables. Proc. the SIGPLA N 92 Conference on Pro\u00adgramming \nLanguage Design and Implementa\u00adtion, June 1992. M. N. Wegman and F. K. Zadeck. Constant propagation with \nconditional branches. ACM Transactions on Programming Languages and Sgsterns, 13(2):181-210, April 1991. \n \n\t\t\t", "proc_id": "207110", "abstract": "<p>In this paper, we present an almost-linear time algorithm for constructing Gated Single Assignment (GSA), which is SSA augmented with gating functions at &#248;-nodes. The gating functions specify the control dependences for each reaching definition at a &#248;-node. We introduce a new concept of <italic>gating path</italic>, which is path in the control flow graph from the immediate dominator <italic>u</italic> of a node <italic>v</italic> to <italic>v</italic>, such that every node in the path is dominated by <italic>u</italic>. Previous algorithms start with &#248;-function placement, and then traverse the control flow graph to compute the gating functions. By formulating the problem into gating path construction, we are able to identify not only a &#248;-node, but also a gating path expression which defines a gating function for the &#248;-node.</p>", "authors": [{"name": "Peng Tu", "author_profile_id": "81100439481", "affiliation": "Center for Supercomputing Research and Development, University of Illinois at Urbana-Champaign, 1308 W. Main Street, Urbana, Illinois", "person_id": "PP15032280", "email_address": "", "orcid_id": ""}, {"name": "David Padua", "author_profile_id": "81452612804", "affiliation": "Center for Supercomputing Research and Development, University of Illinois at Urbana-Champaign, 1308 W. Main Street, Urbana, Illinois", "person_id": "P63208", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/207110.207115", "year": "1995", "article_id": "207115", "conference": "PLDI", "title": "Efficient building and placing of gating functions", "url": "http://dl.acm.org/citation.cfm?id=207115"}