{"article_publication_date": "06-01-1995", "fulltext": "\n Simple and Effective Link-Time Optimization of lYIodula-3 Program hlaryF. Ferninclez Department of ClolmputerS \ncience,Princeton CTniversit~~, Princeton N.J, 0S544 mf f @cs. princeton. edu Abstract Modula-3 supports \ndevelopment of modular programs by separating an object s interface from its implemen\u00adtation. This separation \ninduces a runtime overhead in the implementation of objects, because it prevents the compiler from having \ncomplete information about a program s type hierarchy. This overhead can be re\u00adduced at link time, when \nthe entire type hierarchy be\u00adcomes available. P\\Te describe opportunities for link\u00adtime optimization \nof Moclula-3, present two link-time optimizations that, reduce the runtime costs of Modula\u00ad3 s opaque \ntypes and methods, and show how link-time optimization could provide C++ with the benefits of opaque \ntypes at no additional runtime cost. Our optimization techniques are implemented in mid, a retargetable \nlinker for the MIPS, SPARC, and In\u00ad tel -KM. mld links a machine-independent intermedi\u00adate code that \nis suitable for link-time optimization and code generation. Linking intermediate code simplifies implementation \nof the optimization and makes it pos\u00adsible to evaluate them on a wide range of architectures. mid s optimization \nare effective: they reduce the to\u00adtal number of instructions executed by up to 14% and convert as many \nas 7!3?70of indirect calls to direct calls. 1 Introduction Object-oriented languages have features that \nhelp de\u00advelop modular programs and libraries of reusable soft\u00adware, Opaque types and methods, two such \nfeatures in Modula-3, incur a runtime cost, because to implement them, the Modula-3 compiler must generate \ncode for various runtime computations and checks. Incomplete information at comptle time necessitates \nthese compu\u00adtations; their runtime overhead, however, can be re\u00adduced at hnk time when the entire program \nand its type hierarchy become available. Opaque types ancl methods are invaluable for de\u00adveloping libraries \nof reusable software. Opaque typing *This research was supported by a Francis Robbins Upton Fellowship \nin Science and Engineering and an IBM Graduate Research Fellowship. Permission to copy without fee all \nor part of this material is granted provided that the copies are not made or distributed for direct commercial \nadvantage, the ACM copyright notice and the title of the publication and its date appear, and notice \nis given that copying is by permission of the Association of Computing Machinery.To copy otherwise, or \nto republish, requires a fee and/or specific permission. SIGPLAN 95La Jolla, CA USA 0 1995 ACM 0-89791 \n-697-2/95/0006 ...$3.50 separates a type T s interface from its implementation and guarantees that clients \nthat declare subtypes of T can be compiled even when the source that defines T s representation is unavailable, \nas is often the case for libraries. Opaque typing also supports smart re\u00adcompdation: T s clients need \nnot be recompiled when T s representation is changed. Modula-3 also supports overriding of methods, which \npermits a subtype of T to redefine any method inherited from T. The stan\u00addard implementations of both \nfeatures incur runtime costs. Accessing the field of an opaque type requires two loads: one to determine \nthe field s runtime offset and a second to access the field. To support overriding, methods are implemented \nas indirect calls. The indi\u00adrect call may be inexpensive it might cost only one extra load to fetch \nthe procedure s address but it precludes other promising optirnizations, such as pro\u00adcedure inlining \nand specialization. One way to recover these runtime costs is to elim\u00adinate the features. The resulting \ndesign is similar to C++. The concrete representation of a C++ type T is revealed at compile time to \nT s clients. This revela\u00adtion makes accessing a field of an object as efficient as accessing a field \nof a structure, but it increases re\u00adcompilation because clients of T must be recompiled whenever T s \nrepresentation changes. Jf e describe opportunities for link-time optimization of Modula-3 and present \ntwo link-time optimization techniques. Daia-drluen stmpltfication is a new tech\u00adnique. Using a program \ns type hierarchy, it recovers completely the cost of opaque types and reduces the runtime overhead of \nmethods. It also reveals other opportunities for optimization, such as constant and type propagation \nancl procedure inlining and cloning. Profile-drzven optimization uses profile data to trans\u00adform those \nprocedures that can benefit most from opti\u00adrnizations made possible by data-driven simplification. Moreover, \nour techniques make it as easy to optimize procedures in libraries as those in applications. Data-driven \nsimplification and profile-driven opti\u00admization differ from other link-time optimizations, be\u00adcause they \nrequire high-level data, e.g., the types of objects and expressions, that are necessary for apply\u00ading \nour techniques but often missing from object code. Both optimizations require the entire program to be \navailable and thus cannot be applied at compile time. They are also machine independent and therefore \nbest applied to an intermediate code before code generation. Linking intermediate instead of object \ncode has two important benefits. First, it simplifies recognition of idiomatic expressions generated \nby the Modula-3 com\u00adpiler. The compiler generates intermediate-code idioms to create objects, to access \nfields of opaque objects, and to invoke methods. Often, these idioms can be recog\u00adnized and simplified. \nIdentifying idiomatic expressions would be difficult for a traditional linker presented with RISC object \ncode that has been reordered by instruc\u00adtion scheduling or with CISC object code that has com\u00adplex instruction \nformats and addressing modes. Sec\u00adond, it permits the use and evaluation of the same op\u00adtimization techniques \non a wide range of target archi\u00adtectures. Our optimization techniques are implemented in mid, a retargetable \nlinker for the MIPS, SPARC, and In\u00adtel 486. mld links mill, a machine-independent in\u00adtermediate code \nthat is suitable for optimization and code generation. To evaluate the effectiveness of our techniques, \nwe used m3, the DEC SRC Modula-3 v2.11 compiler [21], to compile five Modula-3 benchmarks. ms is not \na native compiler: it generates C and invokes a C compiler to generate object code. To produce our results, \nm3 invokes mlcc, an ANSI C compiler that gen\u00aderates mill. Although mld links Modula-3 programs, its implementation \nhas few dependencies on Modula-3 itself and required only modest changes to the ms com\u00adpiler (87 lines \nof new code) to produce the information needed by mid. mld could be used to evaluate link\u00adtime optimization \nof C++, for example, by using mlcc to compile C code generated by a C++ front end. mid s optimizations \nare simple and effective. Data\u00addriven simplification reduces the total number of in\u00adstructions executed \nby up to 11~0, and it converts as many as 79$Z0 of the indirect calls executed to di\u00adrect calls. Profile-driven \noptimization reduces the total number of instructions executed by up to 14 XO and the number of loads \nexecuted by up to 19%. 2 Opportunities for Optimization A Modula-3 module is composed of an tnterface, \nwhich specifies the procedures and abstract data types ex\u00adported by the module, and an trnplernentation, \nwhich includes the module s source code [27]. A module s interface is included by clients of the module. \nFig. 1 gives the exported interface for the module Hash and the first few lines of the modules Symbol \nand Hash. The Hash interface (Fig. 1, left) exports two abstract data types: HashT and HashTab. HashTab \n<: HashT de\u00ad clares HashTab to be a subtype of HashT, i.e., it has at least the same interface as HashT: \nlookup, insert, and delete. The subtype relation does not reveal the im\u00ad plementations of HashT or HashTab \nto clients or to the compiler; objects whose private data and methods are hidden in a module s implementation \nare called opaque. Symbol (Fig. 1, center) is a client of Hash. It declares SymbolTab as a subtype of \nHashTab, extending the def\u00adinition of HashTab by defining the field level and the methods enters cope \nand exits cope. SymbolTab over\u00adrides HashTab s definition of insert. Any invocation of insert by a SymbolTab \nobject calls SymInsert in\u00adstead of HashTab s insert. The implementation of Hash (Fig. 1, right) reveals \nthe concrete representa\u00adtion of HashTab but only within the Hash module. The revelation includes the \ndeclaration of HashTab s private data (cent ents ) and defines the procedure bindings of its methods. \nOpaque types are invaluable for developing modu\u00adlar programs and libraries of reusable software. They \nguarantee that a client module can be compiled even when the implementation of an imported opaque type \nis unavailable, as is often the case when an implemen\u00adtation module is provided in a library. For example, \nit is not necessary to recompile Symbol if HashTab s representation changes, because Symbol only depends \nupon HashTab s interface, not its implementation. Strict separation of an object s interface from its \nim\u00adplementation is not supported by all object-oriented languages. For example, C++ reveals an object \ns im\u00adplementation to its clients; Fig. 2 illustrates this design choice. The header file Hash. h is the \nloose equivalent of an interface. The declaration of the class HashTab exports the same method suite \nas the opaque type HashTab, but it also reveals the implementations of the object (the array contents) \nand its nonvirtual method lookup.1 These revelations help the C++ compiler im\u00adplement references to objects \nas efficiently as references to structures and permits compile-time inlining of non\u00advirtual methods whose \nimplementations are defined in the header file. It impedes development of modular programs and of upwardly \ncompatible libraries, how\u00adever, by creating dependencies between source mod\u00adules [11. Implementation \nchoices. Because the complete rep\u00adresentations of objects cannot be constructed at com\u00adpile time, the \nModula-3 compiler delays that construc\u00adtion until runtime. A runtime representation of types stores the \ninformation necessary for the delayed com\u00adputation. Fig. 3 shows the co,mpile-time (top) and runtime \n(bottom) representations of the object s and its type SymbolTab in the module Symbol. A type is represented \nby a type descriptor, which contains a unique typecode, the offsets and sizes of the data and methods \nof the type s instances, and the meth\u00adods themselves. SymbolTab-TC denotes the type de\u00adscriptor for the \ntype SymbolTab. Objects are repre\u00adsented by the type code of their type descriptor, their methods, and \ntheir data. In Fig. 3, the object s (left) 1 C++ has both virtual methods, which may be overridden, and \nnonvirtual methods, which may not. Nonvirtual methods are implement ed with direct calls. INTERFACE \nHash; MODULE Symbol; TYPE HashT = OBJECT FROM Hash IMPORT HashTab; METHODS TYPE SymbolTab = lHashTab \nlookup(key: TEXT): REFANY; level: INTEGER insert(key: TEXT; METHODS value: REFANY); enterscopeo:= Enter; \ndelete(key: TEXT) exitscopeo := Exit ENO ; OVERRIDES HashTab <: HashT insert := SymInsert END Hash. \nEND ; ... END Symbol. Figure 1: Moclu]a-3Hash // Hash.h // Symbol.c class HashTab { #include Hash.h \nvoid *contents[loO]; class SymbolTab public: int level; void *lookup(char *key) public: { return contents[hash(key)];} \n// overrides virtual void insert(char *, void *) ; void insert(char void delete(char *); void enterscopeo; \n} void e>citscopeo; } Figure2: fJ++ Hash contains a pointer to the methods of its type descrip\u00adtor(SymbolTab-TC->methods, \ncenter) anditsown data area. A type s methods are shared because they are immutable. After type initialization, \nthe methods for every object of type SymbolTab are the same, and all such objects share SymbolTab s methods. \nLink time is the earliest time at which the entire type hierarchy ofaprogram is known. lVithout a linker \nthat can use this information, program startup is the earli\u00adest point at which attributes of the concrete \nrepresen\u00adtations of all types, e.g., their sizes and the offsets to data and methods, can recomputed. \nIn the compile\u00adtime picture, the dotted lines indicate that the offsets to s sclata and methods are unlmown \nbecause s scon\u00adcrete representation depends on the sizes of HashTab s private data andmethocls. The compiler \nknows noth\u00ading about the structure of the shaded areas, because the module Symbol is compiled in isolation \nfrom the module that implements HashTab. The values of s s methods are also unknown at compile time. \nThese off\u00adsets and method bindings are computed at progri>m startup and are stored in SymbolTab s type \ndescriptor. At run time, the SymbolTab object s is represented as shown at the bottom: the object s sizes \nand offsets are initializedin SymbolTab-TC andits methods arebound. Once initialized, these values remain \nconstant. There are two sources of runtime overhead in the MODULE Hash; REVEAL HashTab = HashT BRANDED \nOBJECT OBJECT contents: ARRAY [1..101] OF REFANY OVERRIDES lookup := Lookup; insert := Insert; delete \n:= Delete END ; ... END Hash. and Symbol modules. // Hash.c #include Hash.h  : public HashTab{ // Implementations \nof // HashTab::insert // HashTab::delete HashTab::insert *, void *); and Symbol files. implementations \nof opaque typing and method invo\u00adcation. To access the fields and methods of opaque types, msemits code \nto fetch the appropriate offset at run time. For exarnple,i ntheexpression s.level, the offset oflevel \nis unknown at compile time, som3 gen\u00aderates: ((SymbolTab_fields *) (S + SymbolTab_TC->dataOffset))->level \n where SymbolTab_fields denotes the private fields of SymbolTab. The values of SymbolTab s methods are \nalsounlutown, sothemethod invocation s.lookup(key) is compiled into: ((HashT_methods *) ((*s) + HashT_TC->methodOffset))->lookup(key) \n; where (*s) accesses s s methods, and HashT_methods denotes the public methods ofHashTab. 3 Data-Driven \nSimplification mld uses data-driven simplification to simplify expres\u00adsions that refer to variables whose \nvalues are known to be constant after linking. This technique is sinl\u00adilar to partial evaluation and \nlets the linker simplify expressions that would otherwise be evaluated at run time. inld obtains the \nbindings between variables and SymbolTab_TC + typecode typecode s typecode . - dataOffset  . -.- . \nHashTab s dataSlze HashTab s private methods private data methodOffset 1 SymbolTab_TC-> - .   SymbolTab_TC-> \nmethodize dafaOffset meth odOffset SymbolTab s SymbolTab s E methods data ~ SymbolTab_TC -+ \\ typecodetypecode \ns-+-3 dataOffset.408 Hash.Lookup dataSize.412 SymbolTab_TC-> methodOffset.16 da,ao,f.,=408Rs y,:,,,:i,,6E~( \n methodSlze=24 t-==--l Figure 3: Modula-:3 compile-time and run-time representations of the SymbolTab \nobject s their link-time values from a bznd~ng file, the contents HashT-TC->methodS ize are replaced \nby 4 and 16, re\u00adof which reselmble assignments m C. A binding file con-spectively. Even though the runtime \naddresses of the tains assignments ofvaluestoglobals. A value maybe type descriptors are unknown, the \nbinding file speci\u00adan integer, floating-point number, string, the contents fies the contents of these \nstructures. Initializing the of a structure or union, the value of another global type hierarchy at link \ntime completely recovers the cost variable, or an untyped block of initialized bytes. of referencing \nthe fields of opaque objects. Ever-y ex- For a hfodula-3 program, the binding file is gener-pression \nof the form o + t->constjield + offset where o ated automatically by mldpp, a mill-code preproces-1s \nan object address, t is the address of o s type de\u00adsor, and contains the representation of the program \ns scriptor, and constjield is a field of t bound to a con\u00adtype hierarchy. ~ Fig. 4 depicts the compilation \nand stant at link time, is simplified to o + C where C is linking of our example ilIoclula-3 program \nusing mlcc, t->constfield + offset, The simplified expression is as mldpp, and mid. During linking, m3 \ninvokes mldpp, inexpensive as a reference to a field of a structure. which processes the mill object \nmodules for the appli-The mill idioms for accessing fields of opaque types cation and for the Modula-3 \nruntirne system (libm3. a). are generated by mlcc at compile time. mld uses code mldpp executes an initialization \nprocedure similar to generated by iburg [15] to match and rewrite mill id\u00adthe one executed by the Modula-3 \nrulltime system, but ioms at link time The first two iburg rules in Fig. 6 instead of initializing the \ntype hierarchy in memory, it match and rewrite the mill idioms for accessing fields emits a binding file \nthat clescribes the initiahzed type of opaque types. Nonterminals are in lower case; mill hierarchy,3 \noperators are in uppercase. tc and obj match type de- Fig. 5 gives part of the binding file produced \nby scriptors and objects, respectively. link_time_value mldpp for the program in Fig. 1. The first assign-searches \nthe binding file for the value at offset const ment, to *HashT-TC illustrates how mld uses binding in \nt c. cf ield mat ches an expression whose constant statements. At link time, mld symbolically simplifies \nvalue is bound at link time; f ieldaddr produces a sim\u00admill expressions that refer to variables defined \nin the plified addressing expression. mill s intermediate code binding file. For example, HashT-TC->dataOf \nfset and is based on the intermediate cocle used in lcc [14]. Of the 148 rules usecl by mld to simplify \nmill expressions, 2 A binding file is not restricted to type information; only 15 are specific to Modula-3. \nThe others specify any write-once data is permissible, e.g., an array after initialization. rules for \nconstant folding, strength reduction, and for 3 A pre-processing step to compute the binding file is \nnot simplifying addressing expressions. ~ necessary but helped in debugging; mld could compute this in\u00ad \nformation on the fly before applying optimization. 4 mlcc and mld use the same simplification rules. \nLfost are linking compilation ,- -A\u00ad 1I I I I * l __ r 1-. . --! -J d libm3.a Figure 4: Compiling \nand inking aNIodula-3 program using mlcc ancl mid. *HashT-TC = { *Hashl ib_TC = { typecode = 3; typecode \n= 4; dataSize = 4; dataSize = 408; dataOffset = 4; dataOffse% = 4; methodize= 16; methodize= 16; methodOffset \n= 4; methodOffset = 16; *methods = [ *methods =: [ # 4: overridden by subtype 4: Hash.._Lookup; *SymbolTab_TC \n= { typecode = 5; dataSlze = 412; dataOffset = 408; methodize= 24; methodOffset = 16; *methods = [ 4: \nHash__Lookup; # 8: overridden by subtype # 8: overridden by subtype 8: Symbol__SymInsert; # 12: overriddenby \nsubtype 12: Hash__Delete; 12: Hash-_Delete; 1; 1; h - i Figure 5: Example binding Converting method \ninvocations to direct calls. The information in binding files is not restricted to constant expressions; \ntheir contents can guide other transformations, such as with direct calls at link tion s.lookup(key), \nfor indirect call because the lookup will be overriddenin If lookup is overridden, replacing method invocations \ntime. The method invoca\u00ad example, is compile dinto an compiler does not know if asubtype ofSymbolTab. \nthen the runtime type of s  determines which procedure is bound to lookup, i.e., SymbolTab s lookupvalueor \nthatofitssubtype. Often this indirection is unnecessary. In this example, nei\u00adtherlookup nor delete is \noverridden; they are always bound to Hash.Lookup and Hash.Delete, respectively. mldpp executes a conservative \nalgorithm to iden\u00adtify methods that may be converted safely to direct calls. A formal definition of the \nalgorithm appears else\u00ad where [13]; we give an informal method invocation o.m, where t, is convertible \nif t initializes some procedure p or it inherits pertype, and if m svalue is not type oft. For each type \nt and used by mlcc to simplify expressions only a few rules for folding constants 3-specific idioms. \ndescription here. A o is an object of type method m s value to m s value from a su\u00ad overriclclen in any \nsub\u00admethod m for which at compile time: mld uses and for matching Modula 16: Symbol__Enter; 20: Symbol--Exit; \n1; }; file (# introduces comments). we can determine conservatively that m is bound to a procedure \np, mldpp emits a statement in the binding file assigning p to the offset of method m in t s meth\u00adods. \nThis assignment tells mld that any invocation of m may be converted to a direct call to p. For example, \nSymbolTab isa leaf type (it has no subtypes), so none of its methods are overridden. Each of its methods \nis initialized, either via inheritance or in its type cleclara\u00adtion, soeachof SymbolTab s methods inconvertible \nIn Fig. 5, SymbolTab s methods are initialized with the ap\u00adpropriate procedures; mld converts invocations \nof these methods to direct calls. Hashl?ab s insert method is not convertible because SymbolTab overrides \nthe defi\u00adnition of insert; thus its value (at offset 8) is unini\u00adtialized. The thircl rewrite rule in \nFig. 6 matches a method invocation and replaces a convertible method by a direct call. type produces \nthe compile-time type of an object.5 Benchmarks. Our benchmarks contain five programs written by active \nModula-3 users. Although the pro\u00adgram set is small, no program is trivial: they range from 3,000to 81,0001inesof \nModula-3; the Moclula-3 runtime system is an additional 61,000 lines. Finding non-proprietary programs \nthat use Modula-3 s object\u00ad 5Because this algorithm is conservative, we only need the compile-time, not \nthe runtime, type of o. cfield: INDIRI(ADDP(tc, const)) link_time_value (tc, const) fieldaddr: ADDP(ADDP(obj, \nconst), cfleld) ADDP(obj, const + cfield) method: CALLI(INDIRP(ADDP(INDIRP(INDIRP(obj) ), cfleld))) CALLI(link_time_value(type(obj) \n, cfield)) Figure 6: iburg rule sfor nlatchillg Nlodula-3idiol~ls i~l mill code Dynamic counts % dec. \n%dec. %clec. mclir Benchmark inst s loads calls m3f e 11.4 16.8 56.5 int erp 4.0 6.8 13,5 prover 4.0 \n60 79 0 pspec 2.6 38 214 m3pp 3.0 40 1.7 Table2. Results ofapplying clata-driven simplification. oriented \nfeatures was difficult, perhaps because users knowthefeatures incur aruutime cost For example, the Modula-3 \nruntimesystem itself uses few objects. Table 1 provides several static and dynamic mea\u00adsurements of the \nprograms; we chose metrics like those used to characterize the object-orieutedness of C++ programs [7]. \nOur static metrics include the percent\u00adage of type declarations that define objects and the percentage \nof total call sites that invoke methods The percentage ofmethocl invocations is less than halfthat reported \nfor C++ programs, indicating that methods probably are used infrequently. Dynamic measure\u00adments support \nthis hypothesis; the mean percentage of total procedure calls that are indirect 1 e., invoke methods, \n1s 24 %0, which is significantly less than the C++ mean of64% Because objects are not used ag\u00ad gressively \ninthese programs, reconsider the results of applying our optimizations to them a lower bound on the effectiveness \noflink-time optimization. Results. Table 2 sumrnarizesthe results of apply\u00adingdata-driven simplification \nto our benchmarks. Each program and the runtime system were cornpded into mill with ms and mlcc and linked \nwith mid. All mea\u00adsurements were taken on an unloaded DE(l 5000/240 running Ultrix V4.3 with 112 NIB \nmemory, a 64 KB direct-mapped instruction cache, a64 KB data cache, and a local disk. Usually mld generates \nan executable program directly by ernittingbinary code, but its bi\u00adnary code generator for the MIPS does \nnot sched\u00adule instructions. To produce these results, mld emits MIPS assembly code and invokes the MIPS \nassembler to schedule instructions and to generate an executable. Data-driven simplification and method \nconversion are effective. They reduce the number of instructions executed by :3 ll Y01 the number of \nloads executecl by Table 3: Dynamic statistics. Hot C70calls # Procs procs to hot Benchmark Inputs called \n(hb) procs m3fe 197 4100 19 (8) 31.1 int erp 42 1201 18 (6) 27.2 prover 3 91.5 13 (5) 31.7 ~ pspec 1169 \n16 (5) 15.8 m3pp 56 624 11 (5) 26.5 4 17%, and converts up to 79~0 of the dynamnc indn-ect calls to \ndirect calls. ~~e report exact instruction counts instead of elapsed execution tmnes because instruction \ncounts reflect precisely the effects of each transforma\u00adtion. Elapsed execution times are discussed in \nSec. 4. 4 Profile-Driven Optimization Data-driven simplification is a local transformation; it simplifies \nor eliminates expressions within a basic block. It reveals other opportunities for optimiza\u00adtion, such \nas propagation of newly identified constants e.g., the values of the fields typecodel dataSize, and dataOffset \n and the type descriptors them\u00adselves. Revealing thecomplete type hierarchy also per\u00admits type expressions \nto be simplified. For example, issubt ype (o, t) can be reduced to a constant boolean if the runtime \ntype of object o can be propagated from its creation point to its uses. Because mld links the complete \nprogram, it could apply global transforma\u00adtions to every proceclure, but dynamic program statis\u00adtics \nindicate that this is unnecessary Execution profiles of the benchmarks reveal that 2% or fewer of all \nprocedures called execute at least 5070 of all instructions executed. These procedures are the programs \n hot spots. In addition, more than 30% of all hot procedures are in hbraries. Table 3 sum\u00admarizes several \ndynamic statistics for the benchmarks, The second column gives the number of test inputs over which the \ndata was acquired; the third gives the total number of procedures that were called during ex\u00adecution; \nand the fourth gives the smallest number of procedures that together account for at least 505Z0 of the \ntotal execution time The number of hot proce\u00addures in libraries are parenthesized. Last, calls to the \nhot procedures account for a large percentage of total calls; the last column gives these percentages. \n c . % types Benchmark Description (Lines) objects m3fe Analysis program in the ms toolkit (81543) 68 \ninterp A PostScrip;interpreter ( 19000)  50 prover A theorem prover (4536) 44 pspec A program performance \nspecification checker (97S9) 40 m3pp A Modula-3 pretty printer (3072) 22 ,tic % calls to methods 21 \n11 12 1s 18 Dyn mic % calls iusts inclirect per call 40 82 8 142 34 87 33 147 4 113 Table 1: Summary \nof benchmark characteristics. We must minimize the cost of global optimizations, because mld already \npays for generating code for the entire program. These measurements indicate that broad application of \nglobal optirnizations is unneces\u00adsary; applying global optimizations only to hot pro\u00adcedures and at frequently \nexecuted call sites will be both effective and inexpensive. To test this hypothe\u00adsis, mld uses profiling \ndata generated by QPT [3] to se\u00adlect hot procedures and then applies constant ancl type propagation to \nthem. Our implementation of constant and type propagation uses standard, iterative data-flow analysis \nalgorithms [2] applied to mill code. Many global optimizations could be effective when applied to the \nselected procedures; we chose constant and tiype propagation for two reasons. First, data-driven simpli\u00adfication \nreveals constant-valued expressions, which can be propagated, and the complete type hierarchy, which \ncan be used to simplify type expressions, Second, our goal is not to introduce new global optimizations \nbut to demonstrate the value of existing optimization using information that is unavailable before link \ntime. Constant and type propagation often reveal useful information about procedures calling contexts, \ne.g., ar\u00adguments with constant values or whose runtime types are known. We apply targeted znltnmg and \ntargeted ciontng to procedures when their calling contexts are known. Targeted inlining attempts to inline \nthe pro\u00adcedures at frequently executed call sites in hot proce\u00addures, then applies constant and type \npropagation in the caller and its inlined callees. mld chooses candi\u00addate sites using information gathered \nfrom profiles of the program s execution; a description of the technique appears in [13]. Inlining is \nlimited to those sites where it will not create too many locals, which is an approx\u00adimate measure of \nregister pressure. Targeted cloning relies on the observation that pro\u00adcedures are often invoked with \narguments of the same type or the same value from multiple sites. Instead of inlining and specializing \na procedure at multiple sites, a copy of the procedure s text is made; the copy is spe\u00ad cialized using \nthe information about its arguments; and the clone is called in lieu of the original method. Tar\u00adgeted \ncloning applies constant and type propagation in hot procedures and their callers; if a call to a hot \nproce\u00addure or one of its callees has arguments with constant values or with known types, the procedure \nis cloned and is called in lieu of the original. The calling con\u00adtext of the original procedure is used \nto specialize the cloned version. Method cloning and specialization is a technique used in dynamically \ntyped, object-oriented languages [9]; it improves run time at a small cost in space. Results. Table 4 \nsummarizes the results of applying both data-driven simplification and profile-driven op\u00adtimization to \nour benchmarks. The static information provided in the table shows that application of global optimizations \nis limited: the maximum number of call sites inlined is 50; the maximum number of clones cre\u00adated is \n200. The third column gives the number of call sites at which a clone is called and indicates that clones \ncan be reused often, i.e., the calling context for a procedure is the same at multiple call sites. Profile-driven \noptimization reduces the number of instructions executed by 4 14 70, an additional 1 5% over data-driven \nsimplification alone. At best, an ad\u00additional 1.8% of the loads were eliminatecl. As noted above, these \nprograms do not use objects aggressively, so there are few opportunities in the programs hot spots for \npropagating type information and simpli\u00adfying type expressions. Program behavior is not the only obstacle \nto more inspiring results; several improve\u00adments are needed to mill code and mid. mill does not preserve \nas much type information as mld can use; mlcc s front end is based on that of lCC, which is de\u00adsigned \nfor C, not Modula-3, and it discards some type information early in compilation. A second obstacle is \nmid s register allocator, which was not designed to handle procedures with inlined callees. mid s code \ngenerators are based on those in ICC; Ice s register al\u00adlocator is machine independent and does not allocate \nacross basic blocks, which can help reduce spills in fre\u00adquently executed blocks. Both problems are limitations \nof our implementation and are not inherent to link-time optimization. Elapsed execution times. Reductions \nin the num\u00adber of instructions and loads executed are not reflected by comparable reductions in elapsed \nexecution time. Elapsed execution times range from 20Yc slower to 2YZ Name Description basehue No transformations \nare aonhecl. data driven Data-driven simplificatio;l appliecl using binding file. Data -drtven strnpltjicatton \nalso applied in the followtng four cases: mlinirrg only Applies relining at active call sites, but does \nnot apply global optimization to the caller or its callees. inlining+opts Applies irdining at active \ncall sites and global optmrlzations in the caller and mhnt d callees. targeted cloning Applies global \noptlmizatioqs to hot procedures, clones callees for which a calling context is known, but does not apply \ninlimng. all Applies both inltnlng+op~s and targeted clon~ng. Statti info Dynamic i o Tot al a.out ~ \nTotal Total indir Exec. size Inlined Clone insts. loads calls time Benchmark (MB) sites Clones Sites \n(Mill. ) d?c. (Mill. ) d~. ( Mill. ) c1 ?$. (sees) m3fe baseline 2.03 2536 611 13.31 113.4 data driven \n1.89 2,248 11.4 508 16.8 5.79 56.5 99.8 inlining only 1.89 48 2,227 12.2 511 16.4 ~~)~ 55.5 884 inlining+opts \n2.04 48 2,224 12.3 509 16.7 5.93 55.5 91.9 targeted clonir 2.04 200 249 2,258 10.9 509 16.7 5.83 56.2 \n99.8 all 2.04 48 200 249 2,183 13.9 497 18.6 5,93 55.5 89.3 lnt erp baseline 1.23 2,246 507 1.43 807 \ndata driven 1.20 2)156 4.0 473 6.8 1.23 13.5 80.3 inlining only 1.20 14 2,144 4.5 473 6.8 1.25 12.4 75.9 \ninlining+opts 1.20 14 2,136 4.9 471 7.1 1.25 12.7 77.6 targeted clonir 1.37 169 573 2;124 5.4 467 7.9 \n1.25 12.6 87.8 all - 1.36 14 169 573 2,108 6.2 466 8.2 1.25 12.6 96.5 smover baseline 0.63 8,121 1,964 \n32.90 246.7 data driven 0.62 7,797 4.0 1,845 6.0 6.91 79.0 269.4 inlining only 0.63 50 7>647 5.8 1,886 \n3.9 7.05 78.6 235.9 inlining+opts 0.69 50 7,580 6.7 1,893 3.6 6.96 78.8 2654 $;geted clonir 0.69 83 189 \n7,63:3 6.0 1,828 6.9 6.62 79.9 261.3 0.69 50 82 187 7,368 9.3 1,849 5.9 6.69 79.7 261.4 %l~~;ne 0.80 \n2>722 630 6.18 130.3 data driven 0.78 2,652 2.6 606 3.8 4.86 21.4 108.7 inlining only 0.79 25 2,634 3.2 \n606 3.7 4.87 21.2 123,4 inlining+opts 0.79 25 2.626 3.5 604 4.0 4.82 22.0 116.2 targeted clonir 0.86 \n200 350 2:631 3.4 602 4.4 4.85 21.5 124.6 all 0.86 25 200 370 2:608 4.2 601 4.6 4.81 22.2 140.1 m%pbaseline \n045 4,062 953 1.68 114.1 data chven 0.45 3,939 3.0 914 4.0 1.66 1.7 112.2 inlining only 0.45 13 3,908 \n3.8 906 4.4 1.66 1.3 115.0 inlining+opts 0.45 13 3,895 4.1 907 4.9 1.66 1.5 116.3 targeted clonir 0.46 \n13 33 3,944 2.3 918 3.6 1.67 0.7 100.5 all 0.46 13 15 44 3,850 5.2 898 5.7 1.64 2.4 105.9 Table 4: Static \nand dynamic characteristics of benchmark programs faster than the baseline case, but there is no correla\u00adtion \nbetween the number of instructions executed ancl run time. For the prover benchmark, one optimized version \nexecutes 9~o fetuer instructions than the base. line case but runs 6% sloue~ for the pspec bench\u00admark, \na version that executes only 3$7c0fewer instruc\u00adtions runs 16910 faster. We have cleterminecl that pro\u00adcedure \nplacement is the cause of this anomaly; execu\u00adtion times for various procedure layouts range from 9% \nslower to 1570 faster than the baseline case. Compa\u00adrable variations in elapsed times due to code and \ndata placement have been measured for SELF [19] when ex\u00adecuting on a SPARCktation-2 with a unified, direct\u00admapped \ncache. Both virtual-memory performance [34] and instruction cache and TLB miss ratios [25, 29] are known \nto be affected by procedure placement. For our benchmarks, poor (or better) instruction cache usage or \nan increase (or decrease) in TLB misses are possible explanations. Although this indicates that total \nin\u00adstructions executed is a poor predictor of runtime per\u00adformance for our benchmarks on the MIPS, the \nvalue of our techniques is simply masked, not invalidated, by the elapsed execution times. System Performance \n Linking intermediate code is slower than traditional linking because object code is generated for all \nmodules every time they are linked, mld compensates by using BURS-based code generators [17, 28], which \ncan select locally optimal code in as few as 50 instructions per tree node [16], and by emitting binary \ncode directly. mid s code generators are based on those used in ICC. To sim\u00adplify ret argeting, ICC s \ncode generators emit assembly and invoke native assemblers to generate binary code. This technique is \ninefficient and redundant for mid; as\u00adsembling assembly code for an entire program is both prohibitively \nslow and unnecessary, because mld has sufficient information to emit an executable directly. To serve \nmid, lCC S code generators were moclified to emit binary code; the resulting linker is twice as fast \nas the one that emits assembly. Writing the procedures to encode binary instructions on multiple targets \nis tedious and requires (almost) as much work as writing an assembler on each target. This task does \nnot prevent retargeting of mld to multiple ar\u00adchitectures. The procedures for emitting binary code on \nall three targets are generated automatically by a machine-code toolkit [30] from high-level specifications \nof the targets instruction sets. This technique simpli\u00adfies retargeting of mld dramatically and enables \neasy retargeting of mlcc and mld to any of lCC S UNIX\u00adbased targets. Even with these improvements, code \ngeneration re\u00adquires approximately 130psec per generated instruction on a DEC 5000 and dominates mid \ns total link time. Benchmark Link time Text size m3pp 29 281 prover 50 454 pspec 46 563 int erp 103 933 \nm3fe 185 1366 eqntott 3.4 46 li 6.5 99 espresso 13.6 232 gcc 51..4 992 Table 5: Link time in seconds \nand text size in KB Table 5 gives the time to link the Modula-3 bench\u00admarks and four of the SPEC benchmarks. \nWhen link\u00ading the SPEC benchmarks on a DEC 5000, code gem eration accounts for 42 80~0 of the link time; \nfor the Modula-3 benchmarks, it accounts for 48-58%. Seek\u00ading in and reading modules from large libraries \naccount for another 1OYO 35YO of link time for the Modula-3 pro\u00adgrams. Fast compilation can offset slow \nlinking. Because mlcc emits mill instead of assembly code, mlcc gen\u00aderates object code 1.5 3 times faster \nthan the compiler on which it is based, lCC, and the MIPS assembler. Although mlcc s speed cannot compensate \nfor slower linking in an edit-compile-link cycle of only a few mod\u00adules, it can compensate when every \nmodule is com\u00adpiled and linked. For example, when compiling and linking all modules of the four integer \nSPEC bench\u00admark programs, mlcc and mld are 1.2 1.5 times faster on a SPARCstation-2 with 64MB memory \nand 1.1 1.9 times faster on the MIPS than lcc and ld; avoiding the assembler yields most of this improvement. \nG 6 Lhk-time Optimization of C++ C++ compromises program modularity in favor of effi\u00adcient implementation \nof objects and nonvirtual meth\u00adods. This choice reflects the philosophy that C++ be A Better C [35], \ni.e., C++ provides object-oriented fea\u00adtures that can be implemented efficiently using exist\u00ading tools \non a wide range of targets. Numerous articles describe the effects of this design choice and present \ntechniques to improve modularity. Nackman et al. [26] describe programming conventions for using abstract \nbase classes to separate interfaces from implementa\u00adtions; Linton and Pan [24] provide a tool that enforces \nsuch conventions; and Baumgartner [4] proposes a lan\u00adguage extension to support ML-style signatures. \nOur results show that modularity need not be sacri\u00adficed for efficiency. We use mld to evaluate the effective\u00adness \nof link-time optimizations for Modula-3, but little 6 This comparison is unfair to the h,lIPS assembler, \nbecause the MIPS assembler schedules instructions but mld does not, of its implementation is dependent \non Modula-3, and the techniques themselves are language independent. For example, data-driven simplification \ncould provicle C++ with the benefits of opaque types at no additional runtime cost, ancl link-time inlining \nof methods could improve program modularity by reducing dependencies between source modules. As in Moclula-3, \nan implementation of opaque types m C++ requires compiler support and a runtime rep\u00adresentation of types. \nuntil recently, no single runtime type representation existed for C ++. Stroustrup ancl Lenkov [36] proposed \na user-extendible class, callecl Type-info, which has been adopted as a standard. Suggested uses include \nimplementations of garbage col\u00adlectors [12], customization of methocls [22], and simula\u00adtion of multimethocls \n[23], methods that are dispatched on both the types of the invoking object and the meth\u00adods arguments. \nWe cliscuss its use for implementing opaque types. The example in Fig. 7 provicles a posstble syntax \nfor opaque types in C++. We use this example to illus\u00adtrate how opaque types could be implemented if \nC++ provided them, we are not proposing a language exten\u00adsion. The opaque class HashTab defined in Hash. \nh declares the exported interface of a HashTab class, this interface is included by the client Symbol. \nUnlike the example in Fig. 2, HashTab s private data and its im\u00adplementation of lookup are not revealed \nto the con\u00adpiler when compiling Symbol. This definition enforces a separation of HashTab s interface \nfrom its implemen\u00adtation and affords the benefits of opaque types. The implementation of HashTab is revealed \nby the concrete class in Hash. c. Our implementation of opaque types in C++ is sim\u00adilar to that provided \nby Modula-3. We use Strous\u00adtrup and Lenkov s proposed runtime type represen\u00adt ation and object layout. \nThe type representation is extended to include the offsets and sizes of clata and methods. Fig. 8 depicts \nthe runtirne representation of a SyrnbolTab object s. SymbolTab-TI denotes the type information for SymbolTab. \nIt includes a pointer to SymbolTab s methods (a vtbll in C++ parlance) ancl a pointer to its parent type(s). \nThe object s is represented by multiple, aclj scent re\u00adgions, each one containing the information pertaining \nto its own or ancestor type. tJnlike the Modula-3 repre\u00adsentation, there are multiple pointers to vtbls, \neach one representing the methocls of the given type. This repre\u00adsentation is necessarily more complex \nthan Modula-3 s, because C++ supports multiple inheritance and permits casting of objects to ancestor \nand descendent types. T For example, by casting s to a HashTab, s can in\u00advoke HashTab s implementation \nof insert even though SymbolTab overrides its definition; s s vptrs provide 7An object and its vtbls \nare more complex than this, but this representation is adequate for our example. access to all of the \nmethods and data of SymbolTab s ancestors. The dotted lines inchcate that the offsets to data and methods \nare unknown at compile time, coll\u00adstant at run time, and computable at link tmle. This implementation \nrecluires compiler support to generate the idiomatic expressions necessary to access objects at run time. \nThe idioms are more complex than those for Modula-3; for example, the method invoca\u00adtion s->lookup would \nbe compiled into: ( (HashTab_method. *) (*(s + SymbolTab_TI->dat aOffset) + HashTab_TI->methodOf fset) \n)->lookup It also requires runtime support to walk a program s type hierarchy, allocate and initialize \nvtbls, compute the sizes of objects, and initialize the type representa\u00adtions. All of the overhead introduced \nby opaque classes can be eliminated at link time. As with Modula-3, a bind\u00ad ing file representing the \ninitialized C++ type hierarchy can be used by mld to eliminate the overhead of ref\u00aderencing fields and \nmethods. mld requires few changes to do this: an initiahzation procedure to generate a binding file and \nnew iburg rules to match C++ s id\u00adioms. The overhead introduced by making nonvlrtua] methods virtual \nand by hiding their implementations can also be eliminated This change induces two types of overhead: \nan indirect procedure call and the pre\u00advention of compile-time inlining. mid s algorithm for converting \nmethods trivially converts all these indirect calls to direct ones, and mld already implements in\u00adlining. \nFor example, the implementation of lookup in Hash. c is easdy inlined at link time at its call sites. \nIt is convertible and could be flagged for inlinmg using the inline directive or by a link-time flag. \nThe implementation outlined here necessarily omits many details, most notably those for implementing \nopaque classes that inherit from multiple base classes. Multiple inheritance raises semantic issues that \nmust be addressed in an implementation of opaque classes. For example, the C++ compiler can disamblguate \nin\u00advocations of methods derived from multiply inherited base classes, because it has access to the complete \ntype hierarchy. With opaque classes, the complete type hi\u00ad erarchy is not available until link time, \nso the linker must implement method disarnbiguation, just as the compiler does. 7 Related Work The Zuse \nTranslation System (ZTS) [10] linker is very similar in purpose and design to mid. The ZTS linker 1s \ndesigned to reduce the runtime overhead of the software-encapsulation features provided by Zuse, a language \nthat incorporates characteristics of Oberon, Modula-2, and Modula-3. ZTS transforms an interme\u00addiate \ncode and generates object code for the SUN-3 // Hash.h // Sylnbol.c // Hash.c opaque class HashTab { \n#include Hash.h #include Hash.h public: class Symbol.Tab : public HashTab{ concrete class HashTab { virtual \nvoid *lookup(char *key); int level; void *contents[100]; virtual void insert(char *, public: public: \nvoid *); void insert(char *, void *); void insert(char *, void *); virtual void delete(char *); void \nenterscopeo; void *lookup(char *key) } void exitscopeo; { return contents[hash(key)l; } void delete(char \n*); } Figure7: Example ~++ Hash and Symbol files using opaque classes. s+ vtbl --- vptr Hash Tab_TZ... \n..- .. SymbolTab_TI HashTab s HashTab s Type_info methods base class data SymbolTab_TI-> _________ \n: LJ > dataOffset dataOffset Vptr - dataSize SymbolTab s Hash Tab_lI-> __________ methodOffset data \nmethodQffset HasbTab s methodize methods SymbolTab_T[-> __________ methods methodOffset SymbolTab s parents \n+ { &#38;Hash Tab_TI, 0] methods Figure8: An experimental run-tirn erepresentation of the SymbolTab \nobject siuc++. at link time. lJnlike mid, ZTS can link native-object on the MIPS. It is unclear, however, \nthat this tech\u00adfiles as well as intermediate-code files but, only on its nique could be used on a CISC \nsuch as the Intel 486. single target. Although its goals are similar to those On the Intel 486, for example, \nthe method invocation ofmld, ZTS S intermediate code is not applicable to idiom ((*o) + t->methodOffset \n+ o.flset)->method is other languages and its implementation supports only compiled by lcc and gcc into \nfour distinct sequences one target. of instructions: they are 6 to 8 instructions each and use four different \naddressing modes. Most other optimizing linkers are intended for a fam\u00adily of related architectures [18, \n38] or use machine-level Converting method invocations to direct calls not representations such as register \ntransfers [5]. The ONI only enables inlining but also may improve perfor\u00adlinker [32] is more powerful \nthan most. OM applies mance on highly pipelined architectures on which mis\u00adoptirnizations to object files \ncompiled for the M1[PS predicted or unpredictable branch targets stall the or DEC Alpha architectures: \nit converts object code pipeline [31]. Calder et, al [6] use a unique name tech\u00adinto a higher-level representation, \napplies optimiza-nique for identifying convertible methods in C++ pro\u00adtion, and then re-generates object \ncode for its target. grams. At each indirect call site, they identify the sig-Some of OM S optimizations \nare target-independent, nature of the method in}-olied and check if the method s such as profile-driven \nprocedure inlining, andothms are signature is matched by a single procedure. SUCh a target-specific, \nsuch as converting two-instruction ad-method has only one binding and is trivially convert\u00address loadsintoone \ninstruction loadson the Alpha [33]. ible to a direct call. The technique is effective: it con-Transforming \nobject code is a powerful techniclue, be-verts 3 707G of indirect calls to direct ones, They esti\u00adcauseit \npermits optirnizationof code generated by any mate that when accurate target prediction of indirect conlpiler, \nIt might be possible to implement clata-calls is coupled with branch prediction, run tinle~ can driven \nsimplification in OM given adequate type ill-improve by 2 24(Z on an architecture like the Alpha. formation \nin (unscheduled) object code, because the Other effective techniques are used in pure object\u00adidioms for \nreferencing opaque-typed objects are simple orient,ed languages such as SELF [3Y] ancl Cecil [8], in \nwhich basic operations such as add are method invo\u00ad cations. SELF uses a customization technique that \ncaptures the possible bindings of methods in a runtinle\u00adgenerated trace; the trace is fed back to the \ncompiler to help it optimize call sites, e.g., by using type tests and direct calls for the most common \ncases and by inlining common targets [ 20]. Cecil [11] also uses profiling data to identify methods in \na program s hot spots and then specializes the methods on the types of the invoking ob\u00adject and the methods \narguments. Both SELF and Cecil are compiled on-the-fly, but their authors suggest that the techniques \ncould be effective for statically compiled languages that use similar object-oriented features ag\u00ad gressively. \nLink time might be a better target point for these optimizations: type tests can be added and in\u00ad lining \napplied without recompilation, and methods in libraries, for which source is often unavailable, can be \noptimized as well. Discussion Our results show that link-time optimization of an in\u00adtermediate code \nrecovers the cost of Modula-3 s opaque types and reduces the cost of methods while preserving the design \nbenefits of these features. The optimization techniques are simple, effective, and could be applied to \nC++. Unlike most optimizing linkers, mld transforms and links a high-level intermediate code; this permits \nevaluation of link-time optimization on several archi\u00adtectures, which is often difficult in experimental \npro\u00adgramming language research. Preliminary results of applying data-driven simplification to a set of \nsynthetic benchmarks on a DECpc (Intel 486 CPU) with 16MB memory show elapsed-time speedups of 10-2270, \nindi\u00adcating that this technique may be especially valuable on machines with small memories. Linking intermedi\u00adate \ncode also allows us to experiment with optimiza\u00adtion designed for other object-oriented languages. For \nexample, mld already provides most of the infrastruc\u00adture necessary for applying other high-level optimiza\u00adtion \nsuch as customization and specialization, Areas for future work include an experimental im\u00adplementation \nof opaque types in C++ and investigation of techniques for making intermediate-code hnking vi\u00adable during \nedit-compile-test cycles. Opaque types de\u00adcouple objects interfaces from their implementations, which \npresumably should clecrease development time of C++ programs by reducing the number of recompi\u00adlation \ndue to dependencies between source modules. We would like to modify several large C++ programs to use \nopaque types ancl then compare improvements in compile time with the costs of linking intermediate code. \nOur results indicate that, in its purest form, intermediate-cocle linking is too slow for use in devel\u00adopment, \nbecause object code must be generated for the entire program, making fast turn-around almost impossible. \nPossible techniques for reducing the cost of whole-program code generation include incremental linking \nof intermediate code or linking hybrid modules and libraries that contain some procedures in inter\u00ad mediate \ncode and others in object code. Using such techniques, we hope to demonstrate that tools like our retargetable, \noptimizing linker can b~ integrated into the software development cycle. Acknowledgements. Thanks to \nAdam Buchsbaum, .Jeffrey Dean, David Hanson, illukund Raghavachari, Norman Ramsey, and Anne Rogers for \ntheir helpful suggestions. Thanks to Brad Calder and the PLDI 95 referees for identifying additional \nreferences relevant to this work. References [1] E. Adams. The old man and the C. In F roceecizngs of \nthe Summer USENIX Conference, pages 15-26, 1984. [2] .4. Aho, R. Sethi, and J. Unman. Compilers -Prtnct\u00adples, \nTechniques and Tools, Addison Wesley, 1986, [3] T. Ball and J. R. Larus. Optimally profiling and tracing \nprograms. In Conference Record of the ,4 CM Symposium on Prznczples of Programming Languages, pages 59 \n7o, Albuquerque, NM, Jan. 1992. [4] G. Baumgartner and V. Russo. Implementing signa\u00adtures for C++. In \nProceeclzngs of the USENI,Y C++ Conference, pages 37-56, April 1994. [.5] M. E. Benitez and J. W. Davidson. \nA portable global optimizer and linker. Proceeclzngs of the SIGPLA N 88 Conference on Progrummtng Language \nDeszgn and Im\u00adplementation, SIGPL.4N Notices, 23(7) :32?9-338, July 1988. [6] B. Calder and D. Grunwald. \nReducing indirect func\u00adtion call overhead in C++ programs. In ACM Srynpo\u00ads~um on Pr!nclples of Programm~ng \nLanguages, pages 397-408, 1994. [i ] B. Calder, D, Grunwald, and B. Zorn, Quantifying behavioral differences \nbetween C and C++ programs. Journul of Programming Languages, 2(4), 1994, [8] C. Chambers. The Cecil \nlanguage Specification and rationale. Technical Report C SE-TR-93-03-05, Uni\u00adversity of Washington, \n1993. [9] C. Chambers and D Ungar. Making pure object\u00adoriented languages practical. In 00 PSL.4 Conference \nProceedings, pages 1-15, 1991. Published as SIGPLA N Notzces. 26(11). 1991. [10] C. S. Collberg. Flexible \nEncapwlatton. PhD thesis, Lund University, Sweden, 1992. [11] J. Dean, C. Chambers, and D, Grove. Selecti\\,e \nspe\u00adcialization for object-orlentecl languages. In SIGPL.4 JY Conference on Programming Lunguuge Deszgn \nand Im\u00adplementation, 1995. To appear, [12] D. Detlefs. Garbage collection and run-time typing as a C++ \nlibrary, In Proceedings of the USENI.Y C++ Conference, pages J-56, August 1992. [13] hf. Fernzindez. \nSimple and effective link-time optimiza\u00ad [29] K. Pettis and R. Hansen. Profile guided code position\u00ad \ntion of Moclula-3 programs. Technical Report TR-474\u00ad ing. In SIG PL.4N Conference on Programming Lan\u00ad \n94, Department of Computer Science, Princeton IJni\u00ad guage Design and Implementation, 1990. Also in SIG\u00ad \nversity, Nov. 1994. PLAN Notices, \\ ol. ?5, No. 6, June, 1990. [14] C. W. Fraser and D. R. Hanson. .4 \nRetai-getable [30] N. Ramsey and M. F. Fermindez. The New Jersey C Compiler: Design and Implement at~on. \n13en\u00ad machine-code toolkit. In Proceedings of the Wznter jamin/Cumrnings, Redwood City, CA, 1995. ISBN \n1995 USENI.Y Conference, pages 289 30?, New Or\u00ad 0-8053-1670-1. leans, LA, Jan. 1995. [15] C. JV. Fraser, \nD. R. Hanson, and T. A. Proebsting. [31] R L. Sites, editor. Alpha .4rchztecture Reference Man- Engineering \na simple, efficient code-generator gener\u00ad ual. Digital Press, 199?. ator. .4CL!I Letters on Programming \nLanguages and Systems, 1(3): 213-226, Sept. 1992. [32] A. Srivastava and D. W. Wall. A practical system \nfor interrnodule code optimization at link-time. Journal [16] C. W, Fraser and R. R. Henry. Hard-coding \nbottom\u00ad of Programming Languages, pages 1 18, March 1993. up code generation tables to save time and \nSpiice. Sof troare-Prac tice and Erperzence, 21(1):1-12, Jan. [33] A. Srivastava and D. W. Wall. Link-time \noptimization of address calculation on a 64-bit architecture. In SIG\u00ad 1991. PL.4AT Conference on Programming \nLanguage Design [I?] C. W. Fraser, R. R. Henry, and T. A. Proebsting. and fmplementat~on, pages 49 60, \n1994. BURG Fast optimal instruction selection and {ree parsing. SIGPLAN Nottces, 27(4):68-76, Apr. 1992. \n[34] J. W. Stamos. Static grouping of small objects to en\u00adhance performance of a paged virtual memory. \nACM [18] M. I. Himelstein, F. C. Chow, and K. Enderby. Cross- Transactions on Computer Systems, 2(2), \n1984. modnle optimizations: Its implementation and bene\u00adfits. In Proceedings of the Summer llSENI.Y Techn~cal \nConference, pages 347 356, Phoenix, AZ, June 1987. [35] B. Stroustrup. The C++ Programming Language. \nAd\u00addison Wesley, second edition, 1991. [19] U. Holzle. Adaptive optimization jor Self: Recorlcil\u00admg Htgh \nPerformance with Exploratory Programming. [36] B. Stroustrup and D. Lenkov. Run time type identi\u00adfication \nin C++. In Proceedings of the USEN1.Y C++ PhD thesis, Stanford University, August 1994. Conference, pages \n313-340, August 1992. [20] U. Holzle. Optimizing dynamically-dispatched calls [37] D. Ungar and R. B. \nSmith. SELF: The power of with run-time type feedback. In SIGPLA N Confere\u00ad simplicity. 00PSLA 87 Conference \nProceedings, SIG\u00ad nce on Programming Languuge Des~gn and Implernen - PL.4N Arotices, 22(12 ):22 i-241, \nDec. 1987. tuhon, pages 326 335, 1994. [38] D. \\V. \\Vall. Experience with a software-defined ma\u00ad [?1] \nB. Kalsow and E. Muller. SRC J1ocZula-.3 Verston chine architecture. .4Chf Transactions on Program\u00ad 2.11. \nDigital Equipment Corporation Systems Re\u00ad ming Languages and Systems, 14(3):299 338, 1992. search Center, \nJanuary 1992. [22] D. Lea. Customization in C++. In Proceedings of the [TSE.VI.Y C ++ Conference, pages \n301-314, April 1990. [23] D. Lea. Panel discussion: Run time type information and class design. In Proceedings \nof the USENIA C ++ Conference, pages 341-347, August 1992. [24] M. Linton and D. Pan. Interface translation \nand im\u00ad plementation filtering. In Proceedings of the 17SENI.Y C++ Conference, pages 227-236, April 1994. \n[25] S. McFarling. Procedure merging with instruction caches. In SIGPLA N Conference on Programming Lunguuge \nDeszgn and Implementation, pages 71-79, Toronto, Ontario, Canada, June 1991. [26] L. Nackman and J. Barton. \nBase-class composition with multiple derivation and virtual bases. In Pro\u00ad ceedings of the USENIX C++ \nConference, pages 57 71, April 1994. [?7] G. Nelson, editor. Systems Programmmg wzth Modulct\u00ad 3 . Prentice \nHall, 1991. [28] E. Pelegri-Llopart and S. L. Graham. Optimal code generation for expression trees: An \napplication of BITRS theory. In Conference Rfcord of the .4ChI Sympostum on Prtnctples of Programming \nLanguages, pages 294-308, San Diego, CA, Jan. 1988.  \n\t\t\t", "proc_id": "207110", "abstract": "<p>Modula-3 supports development of modular programs by separating an object's interface from its implementation. This separation induces a runtime overhead in the implementation of objects, because it prevents the compiler from having complete information about a program's type hierarchy. This overhead can be reduced at link time, when the entire type hierarchy becomes available. We describe opportunities for link-time optimization of Modula-3, present two link-time optimizations that reduce the runtime costs of Modula-3's opaque types and methods, and show how link-time optimization could provide C++ which the benefits of opaques types at no additional runtime cost.</p><p>Our optimization techniques are implemented in mld, a retargetable linker for the MIPS, SPARC, and Intel 486, mld links a machine-independent intermediate code that is suitable for link-time optimization and code generation. Linking intermediate code simplifies implementation of the optimizations and makes it possible to evaluate them on a wide range of architectures. mld's optimizations are effective: they reduce the total number of instructions executed by up to 14% and convert as many as 79% of indirect calls to direct calls.</p>", "authors": [{"name": "Mary F. Fern&#225;ndez", "author_profile_id": "81100384253", "affiliation": "Department of Computer Science, Princeton University, Princeton NJ", "person_id": "PP31075720", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/207110.207121", "year": "1995", "article_id": "207121", "conference": "PLDI", "title": "Simple and effective link-time optimization of Modula-3 programs", "url": "http://dl.acm.org/citation.cfm?id=207121"}