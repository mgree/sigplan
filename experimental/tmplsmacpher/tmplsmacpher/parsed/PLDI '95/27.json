{"article_publication_date": "06-01-1995", "fulltext": "\n Stack Caching for Interpreters M. Anton Ertl Institut fth-Computersprachen Technisclhe Universitat Wien \nArgentinierstrafie 8, A-104O Wien anton@mips. {complang. tuwien. ac. at Tel.: (+43-1)588014459 Fax.: \n(+43-1)5057838 Abstract An interpreter can spend a significant part of its exe\u00adcution time on accessing \narguments of virtual machine instructions. This paper explores two methods to reduce this overhead for \nvirtual stack machines by caching top\u00adof-stack values in (real machine) registers. The dynamic method \nis based on having, for every possible state of the cache, one specialized version of the whole interpreter; \nthe execution of an instruction usually changes the state of the cache and the next instruction is executed \nin the version corresponding to the new state. In the static method a state machine that keeps track \nof the cache state is added to the compiler. Common instructions exist in specialized versions for several \nstates, but it is not necessary to have a version of every instruction for every cache state. Stack manipulation \ninstructions are optimized away.  Introduction Interpreters are often used for implementing program\u00ad \nming languages. Their major advantages over compila\u00ad tion to native code are simplicity and portability. \nTheir major advantages over the generation of C code are compilation speed and flexibility (e.g., to \ngenerate addi\u00adtional code at run-time). Interpreters are still the dom\u00adinant implementation method of \ngeneral-purpose lan\u00adguages like Prolog, Forth and APL, probably the ma\u00adjority of special-purpose language \nimplementations are interpreters, and they are even used in special imple\u00ad ment ations of traditionally \ncompiled languages like C. In recent years many questions about inter\u00adpreters have been asked in the \nUsenet newsgroup Permission to copy without fee all or part of this material is granted provided that \nthe copies are not made or distributed for direct commercial advantage, the ACM copyright notice and \nthe title of the publication and its date appear, and notice is given that copying is by permission of \nthe Association of Computing Machinery.To copy otherwise, or to republish, requires a fee and/or specific \npermission, SIGPLAN 95La Jollar CA USA @ 1995 ACM 0-89791 -697 -2/95/0006 ...$3.50 comp. compilers. Efficiency \nwas a major concern; an\u00adother frequent question is whether to use a stack or a register architecture \nfor the virtual machine. The present paper deals with these issues. Section discusses general efficency \nissues; then we concentrate on a particular aspect of eilicency: accessing arguments of virtual machine \ninstructions. Our solution uses a stack machine that caches stack values in registers (Sec\u00adtion 3). We \npresent two methods for implementing this idea: either the interpreter (Section 4) or the compiler (Section \n5) keeps track of the cache state. Finally, em\u00adpirical results are presented (Section 6). The main original \ncontributions of this paper are the compiler-based static stack caching technique, the dis\u00adcussion of \ndifferent stack cache organizations, and the empirical evaluation. A note on terminology: unless otherwise \nnoted, the terms instruction and primitive refer to virtual ma\u00adchine instructions, cache refers to the \nstack cache im\u00adplemented in software, and the compiler is the program that generates the virtual machine \ncode. Some examples are written in MIPS assembly: regis\u00adter n is denoted by $n, the destination operand \nof an in\u00adstruction is usually the leftmost register, and comments start with #. 2 Interpreter efficiency \nSince we are interested in efficiency, we limit the discus\u00adsion to virtual machine interpreters, and \nwill not dis\u00adcuss, e.g., syntax tree interpreters. The interpretation of a virtual machine instruction \nconsists of three parts: accessing arguments of the instruction s performing the function of the instruction \n. dispatching (fetching, decoding and starting) the next instruction The first and third part constitute \nthe interpreter over\u00adhead. typedef void (* Inst)(); void add(Inst *ip, int *sp /* other regs */) { Sp[ll \n= Sp[ol+sp[ll ; (*lp)(ip+l, sp+l /* other registers */); } Inst program[] = { add /* . . . */ }; Figurel: \nDirect threading in C using tail calls typedef enum { add/*...*/ } Inst; void engineo { static Inst \nprogram[] = { acki /* . . . */ }; Inst *ip; mt *sp; for (;;) switch (*lp++) { case add: Sp[ll=sp[ol+sp[ll \n; Sp++ ; break; } } Figure2: Instruction dispatch using switch 2.1 Instruction dispatch The most efficient \nmethod for fetching, decoding, and starting the next primitive is direct threading [Be173]: Instructions \nare represented bythe addresses of the rou\u00adtine that implements them, and instruction dispatch consists \nof fetching that address and jumping to the routine. Unfortunately, direct threading cannot be im\u00adplementedin \nANSI C and other languages that do not have first-class labels and do not guarantee tail-call op\u00adtimization \n(Fig. lshowshow direct threading would be implemented in C using tail-calls). Two methods are usually \nused in C: a giant switch (Fig. 2) or calls (Fig. 3). In the first method instruc\u00adtions are represented \nby arbitrary integer tokens, and the switch uses the token to select the right routine; in this method \nthe whole interpreter, including the imple\u00admentations of all instructions, must be in one function. In \nthe second method every instruction is a separate function; this method is actually quite similar to \ndi\u00adrect threading (it just uses calls instead of jumps), so typedef void (* Inst)(); Inst *Ip; int *sp; \nvoid addo { SP[lI=SPIOI+SPIII ; S.p++ ; 1 Inst program[] = { add /* . . . */ }; void englneo { for (;;) \n(*lp++) (); } Figure3: Direct call threading lW $2,0($4) #get next instruction, $4=lnSt.ptr. addu $4,$4,4 \n#advance instruction pointer #execute next instruction j $2 #nop #branch delay slot Figure 4: Direct \nthreading in MIPS assembly Icallit direct call threading. Figure4, 5and 6show MIPS assembly code for \nthe threetechniques (direct call threading needed alittle source code twisting togetrea\u00adsonable scheduling). \nFig. 7shows the overhead ofthese techniques in cycles on two processors, the R3000, and the more deeply \npipelined R4000. The overhead varies dependingon how manydelayslots can refilled; usually it will be \nat the lower bound. The execution time penalty of the switch methodis caused by arangecheck, by atable \nlookup, and by the jump to the dispatch routine generated by most com\u00adpilers. The call method does not \nlook soslow)but itis usually even slower than the switch method: Every vir\u00adtual machine register, e.g., \ninstruction and stack point\u00aders, have to be kept in global or static variables. Most C compilers keep \nsuch variables in memory, causing at least a load and/or store for every virtual machine regis\u00adteraccessed \nin a primitive. Intheswitch method virtual machine registers can be kept in local variables, which are \ntranslated into real machine registers by good com\u00adpilers. Fortunately, there is a widely-available language \nwith first-class labels: GNU C(version 2.x); so direct thread\u00ading can be implemented portably (see Fig. \n8). If porta\u00adbility to machines without gcc is a concern, it is easy to I R3000 R4000 $L2: #for (;;) \nlW $3,0($6) #$6=instruction pointer #nop situ $2,$8,$3 #check upper bound bne 52,$0,$L2 addu $6,$6,4 \n#branch delay slot Sll $2,$3,2 #multiply by 4 addu $2,$2,$7 #add switch table base ($L13) lW $2,0($2) \n#nop j $2 #nop . . . $L13: #switch target table . word $L12 ... $L12: #add: ... j $L2 #nop Figure 5: \nSwitch dispatch in assembly add: ... j $31 #return engine: $;; : lW $2,1p #instruction pointer #nop lw \n$4,0($2) addu $3,$2,4 jal $31,$4 #call $4 Sw $3,ip #delay slot j $L3 #nop Figure 6: Direct call threading \nin assembly switch between direct threading and ANSI C conform\u00ading methods by using conditional compilation. \nIf the instructions are of constant length, dispatch\u00ading the next instruction can be performed in paraJlel \nwith the processing of the current instruction. This is direct 3-4 5-7 switch I12-13 18-19 call 9-1o \n17-18 Figure7: Cycles needed for instruction dispatch. Other costs vary with the dispatch method (see \ntext). typedef void *Inst; void engineo { static Inst progrem[] = { &#38;&#38;add /* . . . */ }; Inst \n*ip; int *sp; goto *ip++; add: Sp[ll=sp[ol+sp[il ; Sp++ ; goto *ip++; } Figure 8: Direct threading using \nGNU C s labels as values patch routine (e.g., instruction fetch) can be shifted to earlier instructions. \nHowever, this work is wasted ifthe virtual machine control flow changes (unless there are delayed branches \nin the virtual machine). 2.2 Semantic content The interpreter overhead can also be reduced byreduc\u00ading \nthe number of primitives executed, i.e., by increas\u00adingthesemantic content ofeach instruction. Combining \noften-used instruction sequences into one instructions a popular technique, as well as specializing aninstruc\u00adtionfor \nafrequent constant argument (eliminating the argument fetch and enabling optimizations in the native \ncode for the instruction). Care has to betaken that the resulting code expansion with its higher real \nmachine instruction cache miss-rate does not cancel out theben\u00adefits. Also, often the compiler must bemademorecom\u00adplex \nto make use of these instructions. On the other hand, optimizing compilers can make instructions with \nhigh semantic content useless (part of the RISC lesson). very useful for filling delay slots of both \nthe instruction dispatch routine and therest of the instruction. Wlhen 2.3 Accessing arguments codingin \nC, care must be taken to avoid potential de- In the hardware area, the contest between stack and pendences \ndueto aliasing (e.g., between instruction and register architectures has been decided for registerstack \npointer) that would prevent thecompilerfrom per\u00admachines.1 However, for interpretive implementations \nforming good scheduling. If an even higher amountof instruction-level parallelism is desired, a part \nof the dis-lFora dissenting opinion, read [Ko089]. lW $3,0($6) #get register numbers, lW $2,4($6) #$6=instruction \npointer lW $4,8($6) addu $3,$7,$3 #add reg. array base ($7) addu $2,$7,$2 lW $2,0($2) #load arguments \nlW $3,0($3) addu $4,$7,$4 addu $2,$2,$3 #perfcnm operation Sw $2,0($4) #store result Figure 9: Add in \na register architecture (without in\u00adstruction dispatch) addu $5,$4,$6 #$5=r3 $4=rl $6=r2 Figure lO: Unfolded \nadd (rlandr2intor3) the picture looks different: From the viewof the compiler writer, many languages \ncan be easily compiled for stack machine code. To achieve better performance with a register machine, \nthe compiler must perform optimizations, e.g., global reg\u00adister allocation (which needs data flow analysis). \nThis would eliminate one ofthe advantages ofusing an inter\u00adpreter, namely simplicity. Moreover, in an \ninterpreter the spi112 and move in\u00adstructions necessary in register architectures are much more time \nconsuming than in hardware, since each in\u00adstruction also has to execute an instruction dispatch. This \nisnot balanced by the fact that the other instruc\u00adtions also have to perform instruction dispatches, \nsince the other instructions usually have higher semantic con\u00adtent. I.e., the proportion of spill code \nis higher for vir\u00adtual register machines than for real register machines. Inhardware, the instruction \nandthe register numbers are decoded in parallel. A simple software implementa\u00adtionofaregister machine \nhasto fetch andlor decode the register numbers using separate instructions. Even with the amount of instruction-level \nparallelism that super\u00adpipelined and superscalar processors offer today andin the near future, this still \ncosts much time. Since hard\u00adware registers cannot be accessed in unindexed way,the virtual machines registers \nhave to be kept and accessed in memory, costing even more time. Fig. 9shows athree register add without \ninstruction dispatchon theMIPS architecture (10 cycles onthe R3000). There is an alternative implementation \nof a register If there are more values than the compiler can keep in regis\u00adters, some values have to \nbe stored into memory and loaded back later. This is called spillzng. lW $2,0($5) #get arguments lw \n$3,4($5) #$5=stack pointer addu $5,$5,4 #update stack pointer addu $2,$2,$3 #perform operation Sw $2,0($5) \n#store result Figure 11: Addinasimple stack implementation lW $2,4($5) #get other argument, $5=sp addu \n$5,$5,4 #update stack pointer addu $6,$6,$2 #perform operation, $6=tos Figure 12: Add, the top of stack \nis keptin aregister machine: The registers accessed can be encoded into the instruction by unfolding \nit, i.e., by creating aversion of the instruction for every combination of registers. The registers can \nthen be accessed directly, and therefore be kept in real machine registers, if there are enough3. Fig. \n10 shows one version of the add instruction. How\u00adever, this strategy causes code explosion, andwillprob\u00adably \nsuffer a severe performance hit on machines with small first-level caches: E.g., there would be 288 512 \nversions of every three-register instruction in a virtual machine with 8 registers (the lower bound is \nfor com\u00admutative operations); the add instruction alone would need 4.5 KB in a direct threaded implementation \non the MIPS architecture. The size of the first-level (real machine) instruction cache onthe R4000 is \njust 8KB. A simple stack machine does better than a simple register machine (see Fig. 11). It has the \nsame num\u00adber of operand fetches and stores; in addition, many instructions update the stack pointer. \nBut there is no fetchingldecoding to learn where the operands are. If there are enough registers, the \nnumber of operand fetches and stores can bereduced bykeepingn top-of\u00adstack valuesin registers (see Fig. \n12). This is not always beneficial; if an instruction takes z items from the stack and stores y items \nto the stack, keeping thetopn items in registers o is better than keeping just n 1 items, if z ~ n A \ny ~ n, due to fewer loads from and stores to the stack. is usually slower than keeping n 1 items, if \nx # Y A x < n A Y< n, due to additional moves between registers. 3However, the availability of registers \nshould not be taken for granted even on register-rich RISCS. E.g., when I tried to keep the top of stack \n(of Forth s stack-oriented virtual machine) m a register on the MIPS architecture, gcc (versions 2.3.3 \nand 2.4.5) spilled the return stack pointer to memory, an important internal register of the virtual \nmachine. is as fast as keeping n 1 items in the other cases. This holds for all machines where loads, \nstores, and moves cost more than zero cycles. Moreover, machines that can exploit a high amount ofinstruction-level \npar\u00adallelism can profit from the prefetching effect of keep\u00ading more items in registers. On a related \nnote, keeping one item in a register also speeds up floating-point and other long-latency instructions, \nwhere thestore back to the stack would expose the latency. Keeping one item in a register is never a \ndisadvan\u00adtage, if there are enough registers. Whether keeping two items is a good idea, depends on the \nvirtual ma\u00adchine and how it is used. E.g., for Forth it is not a good idea (see Section 6).  3 Stack \ncaching Keeping a constant number of items in registers is sim\u00adple, but causes unnecessary operand loads \nand stores. E.g., an instruction taking one item from the stack and producing no item (e.g., a conditional \nbranch) has to load an item from the stack, that will not be used if the next instruction pushes a value \non the stack (e.g., a literal). It would be better to keep a varying number of items in registers, on \nan on-demand basis, like a cache. This requires different implementations of an instruc\u00adtion for different \ncache states. Every allowed mapping of stack items to machine registers constitutes a cache state. There \nare several sensible options on the set of states allowed. Basically, we would like the set to be finite, \nso we can use finite state machines to describe the effect of executing or compiling instructions. The \nrelations of the states should minimize the amount of work nec\u00adessary for getting from one state to another. \nFig. 13 shows a three-state machine for stack caching in two registers. Transitions are shown for words \nwith various stack effects (due to space limitations not for all stack effects). In general, the selection \nof a set of states and transi\u00adtions for a given number of states and registers is an op\u00adtimization problem \nthat we leave for future work. Here we present just a few insights. 3.1 Stack pointer updates In addition \nto stack accesses, many stack pointer up\u00addates can be optimized away, too: The cache state can also contain \nthe information how much the contents of the stack pointer register differ from the actual value of the \nstack pointer. A good strategy that does not intro\u00adduce additional states is to let the difference correspond \nto the number of stack items in the cache (see Fig. 13). This means that the stack pointer need not be \nupdated .\u00ad--w --WW W--ww --WW ww -\u00ad W--ww w -\u00adww -- Figure 13: A simple cache state machine (transitions \nare marked wit h stack effects. w w --w represents an instruction that takes two word-sized items from \nthe stack and puts one result back on the stack (e.g., add).) addu $9,$8,$9 Figure 14: Add in stack \ncaching (starting in the full state of the three-state machine) in instruction implementations that can \naccess all stack items in registers, i.e., hopefully most of the time. Stack caching with stack pointer \nupdate minimization leads to code that is as good as that of the unfolded register machine (see Fig. \n14). 3.2 Minimal organization As a minimum, there should be one state for every num\u00adber of stack items \nin registers (as in Fig. 13). To mini\u00admize the amount of work, the bottom of the cached stack items should \nbe in the same register in all states; the other stack items should be allocated similarly. This ar\u00adrangement \nof states avoids the need to move stack items around on the bottom of the cache whenever something on \nthe top changes. 3.3 Overflows and underflows There is a movement cost, however: If something has to \nbe pushed when the cache is full, all stack items in the cache have to be moved to other registers. Fortunately, \noverflow Figure 15: Overflow transition in a minimal organiza\u00ad tion (the top-of-stack is rightmost, \n$9 contains the deep\u00ad est cached item) overflows are very rare if the cache is sufficiently large (if \nthe cache is small, there are not many moves). It can be made rarer by choosing an appropriate followup \nstate for overtlowing instructions: Choosing the full state as overflow followup state min\u00adimizes the \ntraffic between the stack cache and memory. But there are also other costs associated with overflow\u00ading: \nthe movement of stack items to other registers and the updating of the stack pointer. In particular, \non pro\u00adcessors where a move costs the same as a store, the tran\u00adsition to any state costs the same. So \nit can be better to choose a non-full state as the overflow follow-up state (see Fig. 15), in order to \nreduce the number of overflows (even though this increases the number of under flows a bit). Which state \nis the best, is probably best deter\u00admined empirically. While there are theoretical results [HS85], they \nare based on a random walk model, where pushes and pops occur equally likely irrespective of pre\u00advious \nevents. It is not clear that this model describes the behaviour of real programs, and our empirical re\u00adsults \nindicate that it does not (see Section 6). In the same way an optimal followup state for under\u00adflow can \nbe selected. It is probably useful to put at least the values in registers that the under flowing instruction \nproduces, i.e., the underflow followup state can be de\u00adpendent on the executed instruction. Note that \nthe optimal followup states for overflow and underflow depend on each other. I.e., if a suboptimal . \nFigure 16: Avoiding moves with additional states behaviour for under flows is selected, the corresponding \noverflow followup state will tend to be fuller than for the optimal underflow behaviour, in order to \nreduce the number of costly underflows. Another solution to the movement problem on over\u00adflows is to \nintroduce more states: instead of moving all stack items, just the bottom cached stack item is stored \nto memory and the register where it resided is reused to keep the top of stack. Of course, this new mapping \nof stack items to registers has to be represented in a new state. The moves would have to be performed \nwhen the new state is left. To avoid this, appropriate neighbors for this new state should be introduced. \nIf this approach is performed consequently, all such moves can be elimi\u00adnated, but the number of states \nis nearly multiplied by the number of cache registers. Combinations of both solutions to this problem \nare possible (see Fig. 16). 3.4 Stack manipulation instructions Stack manipulation instructions also \ncause moves in the minimal state machine. As before, these moves can be optimized away by introducing \nmore states. For stack shuffling instructions (e.g., swap and rot ), the extreme form of this approach \ncreates all assignments of stack items to registers where no register occurs twice. For duplicating instructions \n(e.g., dup and over), the ex\u00adtreme form results in an infinite number of cache states, since an unlimited \nnumber of such instructions causes an equally unlimited number of stack items to reside in Figure 17: \nA cache organization where one duplication is allowed (dup duplicates the top of stack, over dupli\u00ad cates \nthe second item, swap swaps the two top items, rot rotates the third item to the top, drop pops the top \nitem) the cache, and an infinite number of states is needed to record all these possibilities. If the \nnumber of cache states is to be limited, the number of duplications repre\u00adsented in the states has to \nbe limited. E.g., the number of stack items in the cache could be limited, the num\u00adber of duplicates \nof each item, or the total number of duplications. Figure 17 shows a two register cache or\u00adganization \nwhere one duplication is allowed. If there are several stacks, the simple solution is to treat them separately, \nwith separate caches (and sepa\u00adrate state machines). They can also be treated in a uni\u00adfied manner, sharing \nthe same set of registers. Moves between the stacks can again be optimized by introduc\u00ading additional \nstates.  3.5 Reducing the number of states In practice finiteness is not enough, there are also other \nlimits to the number of states. Figure 18 gives an idea of the number of states of various cache organizations \nwith a varying number of registers. The iminimal or\u00adganization has only one state for a certain number \nof stack items in registers; overflow move optimization removes the moves on overflow by introducing \nmore states; arbitrary shuffles optimizes shuffle instructions in a similar way, n + 1 stack items supports \nkeeping up to n + 1 stack items in n registers, in any order and with any kind of duplication; these \ntwo cases show that the number of states can grow explosively. One dupli\u00adcation is the minimal organization, \nextended with states that represent one (arbitrary) duplication of a stack item. Two stacks is the minimal \norganiza\u00adtion, combined with caching up to two items of another stack in the same registers, also in \na minimal organi\u00adzation. For organizations with many states, nearly all states will be rarely used. If \na smaller number of states is de\u00ad sired, many of these states can be eliminated. Transi\u00ad tions to such \nstates have to be rerouted, possibly incur\u00ad ring higher transition costs. However, these costs have to \nbe payed rarely, only when the state would have been used. This brings up the question of what transitions \nthere should be in the first place. The simplest criterion is the cost of the transition itself. However, \nthere are often several transitions costing the same (e.g., consider the overflow case in the minimal \norganization). In such cases a transition should be chosen to the node that has the smallest average \ntransition cost (e.g., a half-full state in the above-mentioned overflow case, because it minimizes the \ncostly overflows and underflows). Indeed, the cost of the transition should be considered to include \nthe average transition cost of the successor node.4 Or, even better, if the future is known, the actual \nfuture cost can be used to select the transition. The choice of transitions also influences the usage \ncounts of the states. It is desirable to have a strongly biased distribution of usage counts, in order \nto be able to eliminate many states, but also to achieve high real machine instruction cache hit rates. \nThis biasing can be achieved by selecting a specific state and choosing tran\u00adsitions that get closer \nto this canonical state if there is a choice. 3.6 Prefetching If stack item prefetching is desired, \nstates with too few stack items in registers should be forbidden. This will cause slightly higher memory \ntraffic: the prefetches will be useless if a number of pushes follows that causes the stack cache to \noverflow. In addition, on overflow the prefetched values have to be stored into memory, un\u00adless the cache \nstate also contains information about the prefetched values (corresponding to dirty bits in hard\u00adware \ncaches). Prefet thing more than one value can also introduce moves (an underflow variant of the overftow \nproblem). If it is used, prefetching should overcompen\u00adsate these costs by reducing the number of delay \nslots.  4 Dynamic stack caching In dynamic stack caching the interpreter maintains the state of the \ncache and the compiler need not even be aware of the existence of a cache. This means that there is a \ncopy of the whole interpreter for every cache state. The execution of an instruction can change the state \nof the cache, and the next instruction has to be executed in the copy of the interpreter corresponding \nto the new state. lThi~ infinitely recursive definition would result in infinite costs, but it is possible \nto shift the scale into a finite range. registers 123 4 5 6 7 81 n minimal 234 5 6 7 8 9 n+l overflow \nmove opt. 2510 17 26 37 50 65 nz+l arbitrary shuffles 2 5 16 65 326 1,957 13,700 109,601 ~;=o n!/i! n \n+ 1 stack items 3 15 121 1,356 19,531 335,923 6,725,601 153,391,689 one duplication 3 7 14 25 41 63 92 \n129 n(n + l)Z?2C~ + n + 1 two stacks 3 6 9 12 15 18 21 24 3n Figure 18: The number of cache states $L2: \n#add in state O: cache empty lW $4,0($6) #get arguments, lw $3,4($6) #$6=stack pointer lW $2,0($5) #get \nnext instruction, $5=instp addu $6,$6,8 #stack pointer update lW $2,4($2) #table lookup, next state: \n1 addu $5,$5,4 #advance lnStrCUtlOn pointer #jump to next instruction addu $4,$4,$3 #operation j $2 $L3: \n#add in state 1: tos in $4 lW $2,0($6) #get other argument lW $3,0($5) addu $6,$6,4 #stack pointer update \nlW $3,4($3) #next state: 1 addu $5,$5,4 j $3 addu $4,$4,$2 #operation $L4: #add in state 2: tos in $7, \nsecond in $4 lW $2,0($5) #nop lW $2,4($2) #next state: 1 addu $4,$4,$7 #operation j $2 addu $5,$5,4 \nFigure 19: Add in dynamic stack caching with table lookup This implies achange ofthe instruction dispatch \nrou\u00adtine. In a switch-based implementation, the instruction just has tojump to the appropriate copy of \nthe switch. For direct threading the changes are not so simple: The easy solution performs a table lookup \n(see Fig. 19). This costs a (real machine) load instruction on current RISC processors; to make bad news \nworse, this load instruc\u00adtionmay cost more than one cycle, since it increases the data dependence path \nlength of the instruction dispatch sequence, which will often become the critical path of an instruction, \nespecially if much of the rest has been optimized away (asin the add instate 2inFig. 19), On CISCsthe \nlookup may come for free (i486)or at little cost. The other solution is to store the instructions fora \nstateat afixed offset from the corresponding routinesin the other states. Then the address ofthe routine \nforan instruction can recomputed by adding the base address ofthe instruction and the offset of the state. \nThis costs a (real machine) add instruction on many processors, butmay come for free on others (SPARC) \n. The problem with this approach is that no portable language I know supports placing routines at specific \npoints in memory; what s worse, even some assemblers do not support it (e.g., the DecStation assembler). \nIf instruction dispatch becomes more expensive, dy\u00adnamic stack caching is probably not worth the trouble. \nE.g., none of the add implementations in Fig. 19 is faster (on both the R3000 and R4000) than the add \nin Fig. 12 with direct threading. Since the whole interpreter has to be replicated for ev\u00adery state, \nonly state machines with a few dozen states or less are practicable (depending on the size of the in\u00adterpreter \nand the (real machine) instruction cache). In other words, the stack cache should have the minimal organization, \nmaybe with a few frills like a bit of return stack5 caching, or, if there are few registers for caching, \none duplication, to make better use of them. Eliminat\u00ading the moves of stack manipulation instructions \ndoes not pay in many cases: The instruction dispatch has to be performed anyway, and the moves can often \nbe done in parallel, i.e., in the delay slots. Since the state of the cache is represented in only one \nvalue, i.e., the program counter of the processor, it is not possible to treat two caches (e.g., for \nan integer and a floating-point stack) with separate state machines in dynamic caching. Thestates of \nboth caches have to be represented in a single state machine. This multiplies their number and makes \nbig caches for more than one stack impractical.  5 Static stack caching Instatic stack caching the \ncompiler keeps track of the state of the cache and generates the code accordingly. This approach offers \nseveral bigadvantages over dy\u00adnamic stack caching: . There is no need for a special instruction dispatch \n5Languages with user-visible stacks (e. g., Forth) have a sepa\u00adrate stack for storing the return addresses \nof calls. routine and its possible performance disadvantages, Prog. Instr. loads updates rloads rupdates \ncalls direct threading can be used. compile 11,562,172 0.76 0.55 0.17 0.32 0.13 Stack manipulations \ncan be optimized away com\u00adpletely, i.e., not even an instruction dispatch is exe\u00adcuted. The compiler \njust notes the state transil;ion.  There is no need to replicate the whole interpreter for every state: \nThe implementation of the same instruction in many states can be the same, e.g., when the arguments of \nthe instruction are accessed in the same registers, but some other stack items re\u00adside in different registers \n(in dynamic stack caching they would have different instruction dispatch rou\u00adtines for continuing in \ndifferent states). More\u00adover, implementations of rarely used instructions for rarely used states can \nbe left out. The compiler will then generate code for a transition into a state for which the instruction \nis implemented.  Caches with several thousand states are fess ible, and probably even more with table \ncompression techniques. The compiler knows the future instruction stream and can generate optimal code \nfor it. Of course, there is also a disadvantage: It is not pos\u00adsible to execute the same code in different \nstates. The compiler has to reconcile the states of different co~ltrol flows at control flow joins. Apart \nfrom this fundamental problem there are also the practical problems of insuffi\u00adcient knowledge in the \ncompiler and avoiding compiler complexity. In particular, the compiler usually knows nothing about the \nstates of callers and callees. The traditional solution for the call problem is to have a calling convention. \nIn the case of stack caching this means that all procedures start in a specific state and return in a \nspecific (possibly different) state. The t ran\u00adsition into these states can be performed by the call \nand return instructions respectively. A simple solution for the control flow join problelm is to have \na control flow convention : at every basic block boundary (i.e., at every branch and branch target) the \ncode is in a canonical state. The transition into this state can be performed by the branch instructions. \nFor branch targets the transitions have to be performed by the instruction before the target. A slightly \nmore com\u00adplex, but faster solution is to have the branch perform the transition to the state at the branch \ntarget without causing a reset to a canonical state before the branch target. Due to the need for a calling \nconvention a return sl;ack cache cannot be used as effectively as in dynamic sl;ack caching. However, \na one-register return stack cache can be used to good effect: at the start of a procedure the register \nis filled with the return address. This is equiv\u00adalent to the leaf procedure optimization on RISCS. gray \n1,588,545 0.69 0.43 0.21 0.39 0.17 prims2x 5,766,854 0.75 0.43 0.18 0.34 0.16 cross 4,914,610 0,74 0.51 \n0.19 0.33 0.14 Figure 20: The measured programs and some of their characteristics: instructions, loads \nfrom (=stores to) the stack, stack pointer updates, return stack loads/stores, return, stack pointer \nupdates, and calls per instruction. Generating optimal code using knowledge of the next instructions \nin the basic block is possible in linear time using a two-pass algorithm, as a specialization of the \nap\u00adproach taken in tree pattern matching /PLG88, FHP91]. The first pass just determines which of the \npossible code sequences is optimal, the second pass then generates the code. Both passes use finite state \nmachines and are therefore fast. The usefulness of this technique depends on the organization of the \ncache state machine. It is only useful if there is more than one transition possible for an instruction \nfrom a given state and if choosing the right one requires foresight. From a certain point of view there \nis not much differ\u00adence between static stack caching and using a register architecture for the virtual \nmachine. Indeed, it can be seen as a framework to make virtual register machines more usable: It provides \nautomatic register allocation and spilling without lots of overhead instructions. It also provides principles \nfor keeping the number of dif\u00adferent implementations of an instruction small, if neces\u00adsary. And it provides \na simple, stack-based interface to the higher levels of the compiler. And the low level of the compiler \ndoes not have to handle the complexities of register allocation, it is just a simple and fast state machine. \nHowever, there is quite a bit of complexity in the generator that generates the instructions and the \ntables for the compiler.  6 Empirical results We instrumented a Forth system to collect data about the \nbehaviour of various stack caching organizations.e Several real-world applications were used as bench\u00admarks: \ninterpreting/compiling a 1800-line program (compile), running a parser generator on an Oberon grammar \n(gray), a text filter for generating C code from a specification of Forth primitives (prims2z), and a \ncross-compiler generating a Forth image for a computer with different byte-order (cross). Figure 20 shows \nthe number of loads from the stack (same as the number of stores to the stack), stack pointer updates \nand ex- GThe raw data is available at ftp://ftp.compl ang.tUv?ien. ac.at /pub/mist/stack-caching-data. \n /instruction cyclesl / 2\u00ad / instruction / // /// / / // loads+ stores 1-moves / -. .. . . -. ._. ._ \n. . _. . . -. .-, .-. / / / / / . OJ items in regs 0123456  Figure 21: Keeping a constant number of \nitems in reg\u00adisters: Memory accesses, moves, and stack pointer up-c  0 2i----- dates per instruction \nvs. number of items kept in regis\u00adters o 012345678910 ecuted (virtual) instructions for these programs \non an implementation without any kind of stack caching. It also gives the number of return stack loads/stores \nand stack pointer updates. The return stack is not consid\u00adered in the rest of the measurements. Applying \nreturn stack caching should have similar effects as for the data stack, with one exception: Most return \nstack accesses are simple pushes (on calls) or pops (on returns); there\u00ad fore, always keeping one return \nstack item in a register has virtually no effect. To compare the total argument access overhead of various \norganizations, the components have to be weighed and added. We used the following weights: loads, stores, \nmoves and stack pointer updates cost one cycle, instruction dispatches cost four cycles. Since the number \nof loads from and stores to the stack in memory is equal, we will only display their sum in the figures. \nThe figures display the total sum of all programs. First, we measured the effect of keeping a constant \nnumber of stack items in registers (see Fig. 21). It is easy to see that keeping one item in a register \nis best (see also Fig. 26): It significantly reduces the number of loads and stores. Keeping more items \nin registers re\u00adduces loads and stores, but introduces too many moves to be useful. Of course, the number \nof stack pointer up\u00addates cannot be reduced with this technique. On a Dec-Station (R3000) keeping one \nitem in a register causes a speedup of ll?Io for prims2x and 770 for CTOSS.7 Next, we measured dynamic \nstack caching on min\u00adimal organizations with a varying number of registers and varying overflow followup \nstates. We did not op\u00adtimize the underflow followup state; instead, we used 7The other programs run too \nfast to produce exact timings. Explicit register declarations were used to keep gcc from spilling important \nregisters. Figure 22: Dynamic Stack Caching: Argument access overhead in cycles/instruction of minimal \norganizations with different numbers of registers vs. overflow followup state the state that has those \nitems in registers that the un\u00adderflowing instruction produces. In Fig. 22 the lines represent the performance \nof cache organizations that use a specific number of registers, while varying the overflow followup state. \nE.g., the line labelled with 4 represents the organizations that use four registers; the lowest (optimal) \npoint on this line is for the organiza\u00adtion that uses state 3 as the overhead followup state, i.e., the \nstate where the registers contain the top three stack items and one register is free. The argument ac\u00adcess \noverhead is approximately halved for every register that is added.8 Another interesting result is that \nthe optimal overflow followup states are rather full, w bile we expected them to be about half-full, \nbased on the results in [HS85]. In this light, the underflow behaviour we used is probably close to optimal. \nFigure 23 shows how the components of the argu\u00adment access overhead vary for different overflow fol\u00adlowup \nstates of organizations with six registers. The fuller the overflow followup state, the more overflows \nthere are, increasing the number of moves. At the same time, the memory traffic decreases, since less \ndata, that would have fit into the cache, is stored and later loaded again. Although the number of overflows \nincreases, the number of stack pointer updates decreases, because the increase in overflows is outweighed \nby the decrease in under flows (note that under flows are usually one item 8This holds up to about 14 \nregisters, then the decrease slows down. /instruction loads+ stores  loads+ stores 0.05 moves   /instruction \nmoves updates . .\u00ad updates . .\u00ad dispatches I / I / / ------ I 0.025 - . . ---\u00ad / ---. _x- . . . . //. \n_________________________ ,_____ ////// . > / / \\ 0 0123456 . overflow to -,\u00ad>\u00ad -. _________ -----\u00ad \ncanonical state 123456 Figure 23: Dynamic Stack Caching: Memory accesses, moves, and stack pointer updates \nper instruction of Figure 25: Static Stack Caching: Memory accesses, organizations with 6 registers for \nvarying overflow fol-moves, stack pointer updates and instruction dispatches lowup states per (original) \ninstruction of organizations with 6 regis\u00adters for varying canonical states organizations that use a \nspecific number of registers, while varying the canonical state. I.e., the line labelled 6 4 represents \nthe organizations that use four registers. / The lowest point on this line is for using state 2 as the \ncanonical state, i.e., the state where two stack items are cached in two of the registers. Due to the \nhigh frequency of cache resets to the canonical state, the best canonical state (for organiza\u00adtions with \nmore than three registers) is the two-register state. It decreases the number of underflows fairly well \nwithout introducing too many moves on cache reset (see Fig. 25). Increasing the number of registers beyond \nfive has hardly any effect, because the cache is usually re\u00ad o~ canonical state set before it overflows \nfive registers. So the number of 0123456 loads, stores, moves, and updates stays at a certain level (about \n0.1 loads and stores and 0.2 moves and stack Figure 24: Static Stack Caching: Argument access over\u00adpointer \nupdates per instruction), whereas it approaches head in cycles per (original) instruction of various \norga-O in dynamic caching. However, static caching also re\u00ad nizations vs. canonical state duces the number \nof executed instructions. Note that Fig. 24 displays the overhead per original instruction; since instruction \ndispatch is not counted in the otherat a time, while overflows typically spill several items at figures, \nhere the dispatches that are optimized away area time). However, this does not hold for higher numbers \nsubtracted from the other overhead. of registers. For static stack caching we looked at organizations \nThe majority of cache resets in the programs we mea\u00adbased on minimal organizations, that also contained \nsured is caused by calls and returns. Indeed, in these states that represent the application of one stack \nma-programs every third or fourth instruction is a call or re\u00adnipulation word to a state of the minimal \norganization turn. So, the best way to reduce the number of cache re\u00ad(but only if the arguments of the \nstack manipulation sets and to increase static stack caching performance in word are already in registers). \nThese organizations were these programs would be procedure inlining. Note that combined with the control \nflow convention approach; we a lower number of cache resets will increase the num\u00adtried all the states \nof the minimal organization as canon-ber of useful registers and change the optimal canonical ical state \n(which also served as overflow followup state). state, asymptotically approaching the behaviour of dy-In \nFig. 24 the lines represent the performance of cache namic stack caching. Vconstant # of regs \\, dynamic \n--\u00ad_. .. \\ static i1 ;\\ dicts that (whether there was an overflow or not) in state 7 of a 10-register \ncache there is a higher probabil\u00adity of overflow than underflow. Obviously, the random walk model does \nnot describe the behaviour of these programs well. What about the other programs? In prims2x, the overflows \ndo not decrease for overflow fol\u00adlowup states below state 5, giving essentially the same picture. Only \nin gray this symptom does not appear in the 10-register cache. This is probably due to the fact that \ngray performs a graph walk using recursion. Figure 26: Comparison of the approaches: argument ac\u00adcess \noverhead in cycles/instruction vs. number of regis\u00adters used We did not evaluate the effect of reducing \nthe number of instances of the instructions. However, the distribu\u00adtion of the execution frequency of \nthe instructions (107o account for 90970 of the executed instructions) makes us believe that vast reductions \nare possible with little neg\u00adative impact on the execution time. Comparison. Figure 26 compares the three \nap\u00adproaches. For dynamic and static stack caching the best of the evaluated organizations for a specific \nnum\u00adber of registers was chosen. Note that the coincidence of the lines for dynamic and static stack \ncaching is partly an artifact of the weights we have chosen for the var\u00adious overheads, in particular, \nthe weight of instruction dispatch. E.g., if instruction dispatch costs five cycles, static stack caching \nrivals dynamic stack caching also for higher numbers of registers; if instruction dispatch costs even \nmore, static stack caching is better than dynamic stack caching everywhere, and its line would be partly \nbelow O (i.e., the dispatches optimized away outweigh the remaining argument access overhead). Finally, \nwe took a closer look at the empirical data in order to see how well the random walk model of [HS85] \ndescribes the behaviour of our programs: In cross and compile, the number of overflows is not re\u00ad duced \nbY changing the overflow followup state of the 10-register cache from a state with seven items in reg\u00adisters \n(state 7) to a state with fewer items in registers. This means that, after none of the overflows (and \nthere were 1110 in these programs with these cache organiza\u00adtions), more than three more stack items \nwere pushed before an underflow happened, In other words, there s a very strong tendency to go down after \ngoing up. By contrast, the random walk model assumes that the be\u00adhaviour is independent of previous behaviour. \nIt pre- Still, in less than 10 of the 279 overflows to state 5, gray overflows another time before under \nflowing. Until bet\u00ad ter models are available, stack-based designs have to be evaluated empirically. \n 7 Related work Much of the knowledge about interpreters is folk\u00adlore. The discussions in the Usenet \nnewsgroup comp. compilers [c. c] contain much folk wisdom and personal experience reports. Probably the \nmost complete current treatment on in\u00adterpreters is [DV90]. It also contains an extensive bib\u00adliography. \nAnother book that contains several articles on interpreter efficiency is [Kra83]. Most of the pub\u00adlished \nliterature on interpreters concentrates on decod\u00ading speed [Be173, Kli81], semantic content, virtual \nma\u00adchine design and time/space tradeoffs [Kli81, Pit87]. Stack caching has been used first in hardware \nstack machines [Bla77, HS85, HFWZ87, HL89, Ko089] and for speeding up procedure calls in processors designed \nat Bell Labs [DM82] and UC Berkeley (register win\u00addows) [HP90]. For interpreters, [DV90] proposed dy\u00adnamic \nstack caching with a minimal cache organization without stack pointer update minimization and with the \nfull state as followup state. They do not discuss other possible organizations and apparently they used \nonly the Sieve benchmark for their empirical evaluation. They report speedups (probably over an implementa\u00adtion \nthat does not keep any part of the stack in registers) of 16~0 for Forth on an 8086 with a two-register \ncache and 1770 for M-Code (a virtual machine for Modula-2) on an 68020 with a three register cache. 8 \nconclusion Apart from optimizing instruction dispatch and increas\u00ading the semantic content of the instructions, \nanother factor determines the performance of an interpreter: ac\u00adcessing the arguments of the instructions. \nFor inter\u00adpreters conventional register architectures do not enjoy the same advantages as on hardware \nimplementations. Their disadvantages are compiler complexity, slowness andlor big interpreters. The \nperformance of virtual stack machines can be im\u00ad [HFwz87] John R. Hayes, Martin E. Fraeman, proved by \ncaching stack items in registers. There is a Robert L. Williams, and Thomas Zaremba. large variety of \nstack cache organizations. Stack caclhing An architecture for the direct execution of can be employed \nin two ways: In dynamic stack caclhing the Forth programming language. In Ar\u00ad the interpreter keeps track \nof the state of the cache. A chitectural Support for Programming Lan\u00ad copy of the complete interpreter \nhas to be kept for every guages and Operating Systems (A SPLOS\u00ad state of the cache, making only cache \norganization with 11), pages 42-48, 1987. few states feasible. Moreover, on many processors dy\u00ad namic \nstack caching increases instruction dispatch time, [HL89] John Hayes and Susan Lee. The architec\u00ad eliminating \nthe speed advantage of stack caching. In ture of the SC32 Forth engine. Journal of static caching, the \ncompiler keeps track of the Ciiche Forth Application and Research, 5(4):493\u00ad state. This allows using \norganizations with more states, 506, 1989. it allows fast direct threading, and stack manipulation [HP90] \nJohn L. Hennessy and David A. Patterson. operations can often be optimized away completely, But Computer \nArchitecture. A Quantitative Ap\u00ad there is a bit of overhead for making the state conform proach. Morgan \nKaufman Publishers, 1990. to calling conventions and reconciling the cache states on control flow joins. \n[HS85] Makoto Hasekawa and Yoshiharu Shigei. High-speed top-of-stack scheme for inter- Acknowledgements \npreters: analysis. A management In International algorithm and Symposium its on Computer Architecture \n(ISCA), pages 48- Konrad Schwarz, Robert Bernecky, Andi Krall, Franz 54, 1985. Puntigam and Manfred Brockhaus, \nMarcel Hendrix, Ul\u00ad rich Neumerkel and the referees provided valuable com\u00ad [Kli81] Paul Klint. Interpretation \ntechniques. ments on earlier version of this paper. Marty Fraernan, Sofiware-Practice and Experience, \n11:963- John Hayes and Chris Bailey discussed about program 973, 1981. behaviour and their experiences \nwith the random walk model with me. [Ko089] Philip J. Koopman, Jr. Stack Computers. Ellis Horwood Limited, \n1989. References [Kra83] Glen Krasner, editor. Smalltalk-80: Bits of History, Words of Advice. Addison-Wesley, \n[Be173] James R. Bell. Threaded code. Communi\u00ad 1983. cations of the ACM, 16(6):370 372, 1973. [Pit87] \nThomas Pittman. Two-level hybrid in\u00ad [Bla77] Russell P. Blake. Exploring a stack archi\u00ad terpreter/native \ncode execution for com\u00ad tecture. IEEE Computer, 10(5) :30 39, May bined space-time eficiency. In Symposium \n1977. on Interpreters and Interpretive Techniques (SIGPLAN 87), pages 150-152,1987. [c. c1 comp. compilers. \nUsenet Newsgroup; archives available from [PLG88] Eduardo Pelegri-Llopart and Susan L. Gra\u00ad ftp://primost \n.cs. wise.edu. ham. Optimal code generation for expres\u00ad sion trees: An application of the BURS the\u00ad [DM82] \nDavid R. Ditzel and H. R. McLellan. Regis\u00ad ory. In Fifteenth Annual ACM Symposium ter allocation for \nfree: The C machine stack on Principles of Programming Languages, cache. In Symposium on Architectural \nSup\u00ad pages 294-308, 1988. port for Programming Languages and Sys\u00ad tems, pages 48 56, 1982. [DV90] Eddy \nH. Debaere and Jan M. Van Campen\u00ad hout, Interpretation and Instruction Path Coprocessing, The MIT Press, \n1990. [FHP91] Christopher W. Fraser, Robert R. Henry, and Todd A. Proebsting, BURG Fast Optimal Instruction \nSelection and Tree Parsing, 1991, Available via anony\u00ad mous ftp from kaese. cs .wisc. edu, file pub/lmrg. \nshar. Z.  \n\t\t\t", "proc_id": "207110", "abstract": "<p>An interpreter can spend a significant part of its execution time on accessing arguments of virtual machine instructions. This paper explores two methods to reduce this overhead for virtual stack machines by caching top-of-stack values in (real machine) registers. The <italic>dynamic method</italic> is based on having, for every possible state of the cache, one specialized version of the whole interpreter; the execution of an instruction usually changes the state of the cache and the next instruction is executed in the version corresponding to the new state. In the <italic>static method</italic> a state machine that keeps track of the cache state is added to the compiler. Common instructions exist in specialized versions for several states, but it is not necessary to have a version of every instruction for every cache state. Stack manipulation instructions are optimized away.</p>", "authors": [{"name": "M. Anton Ertl", "author_profile_id": "81100016086", "affiliation": "Institut f&#252;r Computersprachen, Technische Universit&#228;t Wien, Argentinierstra&#946;e 8, A-1040 Wien", "person_id": "PP39071533", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/207110.207165", "year": "1995", "article_id": "207165", "conference": "PLDI", "title": "Stack caching for interpreters", "url": "http://dl.acm.org/citation.cfm?id=207165"}