{"article_publication_date": "06-01-1995", "fulltext": "\n Avoiding Conditional Branches by Code Replication FRANK MUELLER Fachbereich Informatik Humboldt-Universitat \nzu Berlin Unter den Linden 6 10099 Berlin, Germany SUMMARY On-chip instruction caches are increasing \nin size. Com\u00adpiler writers are exploiting this fact by applying a vari\u00adety of optimization that improve \nthe execution perfor\u00admance of a program at the expense of increasing its code size. This paper describes \na new optimization that can be used to avoid conditional branches by replicating code. The central idea \nis to determine if there are paths where the result of a conditional branch will be known and to replicate \ncode to exploit it. Algorithms are described for detecting when branches are avoidable, for restructuring \nthe control flow to avoid these branches, and for positioning the replicated blocks in the restructured \ncode. The results indicate that the optimization can be frequently applied with reductions in both the \nnumber of instructions executed and total instruction cache work. INTRODUCTION This paper describes a \nnew approach for avoiding conditional branches by using code replication. This approach is accomplished \nin three steps for each loop level of a function. First, analysis is performed to determine whether any \nconditional branches can be avoided. Second, an algorithm is used to determine how the control flow can \nbe restructured to avoid conditional branches by replicating basic blocks. At this point heuristics are \nused to determine whether the transformation is worthwhile. Finally, new code is replicated and the physical \nposition of the basic blocks is established. RELATED WORK There are several optimizations that have been \ndevel\u00adoped in an attempt to improve performance despite the penalty of increasing code size. Inlining \nrep~aces a call by the body of the routine being invoked. Increased code size Permission to copy without \nfee all or part of this material is granted provided that the copies are not made or distributed for \ndirect commercial advantage, the ACM copyright notice and the title of the publication and its date appear, \nand notice is given that copying is by permission of the Association of Computing Machinery.To copy otherwise, \nor to republish, requires a fee and/or specific permission. SIGPLAN 95La JoHa, CA USA 63 1995 ACM 0-89791 \n-697-2/95/0006.,.$3.50 DAVID B. WHALLEY Department of Computer Science Florida State University Tallahassee, \nFL 32306-4019, U.S.A. e-mail: whalley@cs.fsu. edu phone: (904) 644-3506 results when a routine is inlined \nfrom more than one call site. Execution performance benefits often occur since the call and return are \navoided and more information is avail\u00adable for other optimizations [DaH88]. Loop unrolling replicates \nthe body within a loop. This optimization reduces the number of compare and conditional branch instructions \nexecuted by the loop and can result in more effective scheduling of instructions within the loop body \n[HeP90]. Replicating portions of basic blocks has been performed to avoid pipeline stalls for superscalar \nmachines [GoR90], Software pipelining replicates code associated with loops to produce a revised kernel \nof the loop that has fewer pipeline stalk [Jai9 1]. Detecting the dependence between loop iterations \nhas been described by Banerjee [Ban93]. This analysis has been used to reorder loop iterations to enhance \nperformance for vectorization, parallelization, and caching. Mueller and Whalley investigated avoiding \nuncondi\u00adtional jumps by code replication [MuW92]. Unconditional jumps were replaced with a replicated \nsequence of instruc\u00adtions that either reaches a return or falls into the block that positionally follows \nthe original unconditional jump. The growth in code size was minimized by choosing the short\u00adest path \nfor the replacement. A superoptimizer will generate an exhaustive set of bounded sequences of instructions \nwith the goal of finding a sequence that will produce the same effect as a more expensive sequence of \ninstructions. The more expensive sequence can then be recognized in a traditional optimizer and replaced \nwith the less expensive sequence. This tech\u00adnique has been used to eliminate conditional branches over \nshort instruction sequences in many instances on the IBM RS/6000 [GrK92]. The optimization described \nin thk paper can be viewed as partial redundancy elimination (PRE) of condi\u00adtional branches. PRE traditionally \nplaces copies of a com\u00adputation at other points in the control flow to force the orig\u00adinal copy to become \nfully redundant so it can be deleted [MoR79]. The transformation described in this paper dif\u00adfers since \nit involves restructuring the control flow by repli\u00adcating entire sequences of basic blocks. The research \ndescribed in the current paper was inspired by the work of Krall [Kra94]. Similar work was later published \nby Young and Smith [YoS94]. W-all often found a high correlation between the past results of branches \nand the future results of the same or different branches in a loop. This correlation typically increased \nwhen code was replicated in the loop to distinguish between different loop paths. The authors of this \npaper sus\u00adpected that the high correlation observed by Krall may indi\u00adcate that many paths in the replicated \ncode were not possi\u00adble due to data dependencies. Thus, many branches in the replicated code could potentially \nbe avoided. MOTIVATION While it has been shown that most unconditional jumps can be eliminated by code \nreplication [MuW92], it is obvious that fewer conditional branches can be avoided. One may be inclined \nto believe that such transformations would only rarely be possible. However, there are many common instances \nin programs that can be improved. The examples in this section are given in C code to more con\u00adcisely \ndepict the transformations performed. The control flow of the restructured C code segments would be compa\u00adrable \nto a restructured flow graph of basic blocks. Note that the actual implementation of this general optimization \nwas performed in the back end of a compiler and many other types of transformation instances were applied. \nI. Often, a flag is used by a programmer to exit a loop. For example, consider the following segments \nof code. The original loop will exit if either a general condition or a flag is false. The code to test \nthe flag variable is not within the loop after restructuring since the replication of the code associated \nwith statement C; allows this test to be avoided. The flag will be zero or nonzero depending upon the \ninitial path taken and all subsequent tests of the variable flag are eliminated. Other optimizations \nthat could be applied to further improve the code are depicted by italiciz\u00ading the code portions that \ncan be removed. The test of cndl after flag is set to zero can be removed since the result of the conditional \nbranch has no effect. The initial test of the f 1 ag variable would be avoided since the com\u00adparison \nis known to be true. In addition, the remaining assignments to flag can be removed, assuming that the \nvalue is not used after the loop. Finally, space for the local ORIGINAL AFTER RESTRUCTURING flag = 1; \nflag = 1; while (cndl &#38;&#38; flag) { if (cndl &#38;&#38; fldg) A; do { if (cnd2) { A; B; if (cnd2) \n{ flag = O; B; } flag = O; c; c; } if (cndl) break; break~ } c; } while (cndl) ; flag variable would \nnot be needed given that this was the on Iy place in the function in which it was used. 11. Often, conditions \nmay be tested that will result in a conditional branch always branching or always falling through once \nthe branch has a certain result. For instance, consider the following code segments. The left code seg\u00adment \ncontains a simple loop that will exit when both condi\u00adtions are false. Assume that the variable i is \nonly incre\u00admented in the loop. If the condition i < 100 is ever false, then it will always remain false \nfor the remainder of the loop. The right code segment shows the testing of the first condition can be \navoided after it becomes false. ORIGINAL AFTER RESTRUCTURING while (i < 100 I I somecnd) { whi,le (i \n< 100) { A; A; i++; i++; ) } while ( somecnd) { A; i++; } Similar behavior may be predicted for equality \nand inequality tests. Given that a variable is only incremented or only decremented for every iteration \nof a loop, an equal\u00ad ity test of that variable with a loop invariant value will only be true at most \nonce for the execution of the entire loop. Likewise, a test for inequality of that variable will only \nbe false at most once. III. A programmer may also often repeat conditions in i f statements to improve \nreadability. For instance, check\u00ading whether a pointer is not NULL may be done several times in different \ni f statements in the same loop. Consider the following segments of code. Assume that the variable p \nin the original code segment is not affected by the code in B ORIGINAL AFTER RESTRUCTURING while ( somecnd) \n{ while ( somecnd) { A; A; if {p &#38;&#38; *P == vail) if (p) { B; if (*p == vail) else B; c; else if \n(p &#38;&#38; *p == va12) c; D; if (*P == va12) else D; E; else )?; goto doE; }} else { c; doE: E; } \n1?; } code in C. IV. An invariant condition may be tested inside of a loop. In fact, loop invariant \nconditions are often generated due to applying other optimizations. Consider the nested loops in the \nfollowing left code segment. It appears there are no apparent invariant conditions. Yet, the inner loop \nwill be transformed to avoid executing an unconditional jump at the end of the loop body on each iteration. \nAs shown in the middle code segment, the test condition is also tested ini\u00adtially before the loop is \nentered. This condition is loop invariant in the outer loop and can be avoided by replicating code as \nshown in the right code segment. ORIGINAL AFTER OPTIMIZATION RESTRUCIWRE D do{ do{ A; A; A; i=O; for \n(i=O; i=O; if (O<N) { i-m: if (O<N) goto doB; i++) do{ do{ B; B;A: c; i++; i=O; } } do{ while (end) \n; while (i<N) ; doB: B; c; i++; }) while (end); while (i<N) ; C; } while (end) ; } 01s0 { goto doc ; \ndo { A; dot: C; } while (end) ; } A more traditional technique, called urrswitching, would simply test \nthe loop invariant condition initially and then enter one of two loops based on the result [LRS76]. Note \nthat the total code replicated is greater in the general approach described in this paper than it would \nbe for unswitching. This increase is due to only testing the invari\u00adant condition when it needs to be \nexecuted. Unlike unswitching, the general approach described in this paper will never increase the dynamic \nnumber of conditional branches executed. DETERMINING WHETHER BRANCHES CAN BE AVOIDED Analysis is performed \nto determine whether the con\u00additional branches in a loop can be avoided by replicating code. The compiler \nfirst calculates the set of registers and variables upon which a conditional branch (and its associ\u00adated \ncompare instruction) depends. This set was calculated by expanding the effects of the compare instruction \nassoci\u00adated with the conditional branch. For instance, consider the following SPARC instructions represented \nas RTLs r[l]=HI[_91; / sethi %hi (_g) , %gl */ r[8]=R[r [l]+ LO[_g]l; / Id [%91+%10 (_9) 1 ,%00 / IC=r[8]?5; \n/ * cmp %00,5 */ PC= IC<O, L20; 1 bl L20 *I The effect of the comparison can be expanded to: IC=RIHI \n[_g] +LO[_g] 1 ?5; In addition, for each basic blockin the loop the compiler determines the set of registers \nand variables that are affected by instructions within the current block. Thus, the compiler can determine \nthat a basic block updating the global variable g could affect the result of this conditional branch. \nUpdates to the registers r [1 ] (%gl) or r [ 8 ] (%00) would have no effect. The compiler next attempts \nto determine if there exists a path through a loop from the point immediately after a conditional branch \nis encountered to the same branch without the comparison associated with the branch being affected. If \na conditional branch is not affected in a path in which it is encountered, then that same path could \nbe taken again and the result of executing the conditional branch would not change. There are cases where \na conditional branch could be affected in each path it is encountered, but still could be avoided. One \ninstance is when a basic block can affect a conditional branch, but only if the result of the branch \nhad not already been in a specific direction (as depicted in Example II in the motivation section). Detecting \nthis situa\u00adtion requires remembering whether each branch was last taken or not. The processing of a block \nmay also make the result of a conditional branch known at that point. This situation may occur as the \nresult of updating a variable or register (as depicted in Example I in the motivation section) or when \nthe result of one conditional branch subsumes another (as shown in Example III in the motivation section). \nThe com\u00adpiler needs to detect if the conditional branch can be reached from this block without being \naffected again. The algorithm for determining which branches can be potentially avoided is shown in Figure \n1. Each block will have an in and an out state indicating the branches whose results are known at the \nbeginning and end of the block. A branch result can become known at a point in the flow graph due to \nthe branch being executed, another branch being executed whose result subsumes the branch, or the effects \nof a block. A known branch can become The actual algorithm is a bit more complicated. Two in and out \nstates were actuatly calculated for each block. Some effects will only make a branch result unknown If \nthe branch result had been in a specific direction. In these situations, only the out state of the block \nassociated with the specific direction affected will be the one updated. unknown due to an effect within \na block. A branch is potentially avoidable if it is in the block s in state and is not affected within \nthe block (or is made known due to an effect within the same block). DO FOR each block Bin the loop DO \nB->in := NULL, FOR each immediate predecessor P of B DO B->in := B->in u P->out. IF P contains a branch \nTHEN B->in:= B->in u (any branches that the transition from P to B subsumes). END IF END FOR B->out := \nB->in. B->out := B->out -(the branches that B affects). B->out := B->out u (the branches made known \nby the effects in B). IF B contains a branch THEN B->out := B->out U B. END IF END FOR WHILE any changes \nFigure 1: Finding Avoidable Branches Algorithm An example is given in Figure 2 to illustrate the algo\u00adrithm. \nFigure 2 (a) depicts the original loop. Blocks 2, 3, and 7 have a conditional branch. Block 4 affects \nthe condi\u00adtional branch in block 3 and block 6 affects the conditional branch in block 2, The conditional \nbranch in block 7 is affected by other instructions in the same block. Figure 2 (b) shows the branches \nthat are potentially known at the entry and exit point of each basic block. The branches for blocks 2 \nand 3 are avoidable since they are in the in states and not affected within their own blocks. While the \nbranch in block 7 is in the in state for that block, the branch is not avoidable since it is affected \nwithin block 7. Figures 2 (c) and 2 (d) will be discussed later in the paper. RESTRUCTURING THE CONTROL \nFLOW TO AVOID BRANCHES  Once it has been determined that one or more condi\u00adtional branches in a loop \ncan be avoided, the control flow within the loop can be restructured. An algorithm to accomplish the \nrestructuring is based on keeping a state in each block for each avoidable conditional branch in the \nloop. A block can inherit its state from a predecessor block or change its state due to an effect within \nthe block or due to being an immediate successor of an avoidable condi\u00adtional branch. A state associated \nwith a conditional branch can have one of three values: unknown, fall-through, or branch. An unknown \nstate indicates that it is not known whether or not the branch will be taken. A block will typi\u00adcally \nhave this state for a conditional branch if the branch (a) Original Loop (b) Potentially Known Branches \n2 in: 2,3,7 2 out: 2,3,7 3 in: 2,3,7 3 out: 2,3,7 F 4 in: 2,3,7 4 out: 2,7 5 in: 2,3,7 5 out: 2,3,7 F \n6 in: 2,3,7 6 out: 3,7 7 in: 2,3,7 7 out: 2,3,7 F (c) Restructured Control Flow (d) Positioned Code \n11 2 L II-l 6  a7(w r\u00ad 124 3 4 7 w P Figure 2: Restructuring a Loop to Avoid Branches has not been \nencountered previously in the control flow. A block will also have an unknown state for a conditional \nbranch if the block affects the conditional branch in a man\u00adner where the result of executing the branch \ncannot be pre\u00addicted. The state for a conditional branch within a block will be set to fall-through or \nbranch if the immediate prede\u00adcessor was the block containing the conditional branch. The state to be \nset depends upon whether the successor is a fall through or target of the branch. The state for a conditional \nbranch can also be set to a fall-through or branch when an effect in another portion of the loop causes \nthe result of the comparison to be known. Table 1 shows three such examples. Case I shows another block \nsetting one of the operands of the expanded compari\u00adson to a constant. Thus, the result of the conditional \nbranch can be determined to befall-through at that point. Case II illustrates that the state of a conditional \nbranch may not be changed even though a variable or register within the asso\u00ad ciated expanded comparison \nis updated. The other block r[2] > 76 will jump when it is known that r[2] > Case Decidable Effect Avoidable \nIc=r[8]?o: / cmp %00,0 */ Branch PC=IC==O,L1; / be L1 */ 1. Other r[8]=-l; /* move -1,%00 / Block Avoidable \nIC=r[2]?50; / cmp %92,50 */ Branch PC=IC>0,L2; / bg L2 *I II. Other r[21=r[21+l; / add l,%g2,%g2 */ \nBlock Avoidable IC=r[2]?76; / cmp %g2,76 */ Branch PC=IC>0,L4; /* bg L4 *I 111. Other IC=r[2]?83; / \ncmp %cJ2,83 / Block PC=IC<=0,L3; / ble L3 *I Table 1: Decidable Effects on Branches will have no effect \non the result of the conditional branch when the state is already branch. Case III depicts a situa\u00adtion \nwhere one conditional branch may also subsume another conditional branch. In other words, the direction \ntaken by one conditional branch may indicate the direction taken by another conditional branch in the \nsame loop. Assume the instrttctions in theotherblock areexecuted and the branchis not taken. The avoidable \nconditional branch will betakenifr [2] is not affected between the execution of the two branches since \nthe valuein r[21 is guaranteed to be greater than 83. Note that the conditional branch can\u00adnotbe avoided \nif the conditional branch in the other block is taken. In general, a conditional branch can only be poten\u00adtially \nsubsumed by another conditional branch when one argument of each comparison is identical and the other \nargument of each comparison is a constant or the same invariant value. Table 2 depicts the different \ncases when the result of one conditional branch subsumes another branch.2 Column 1 shows aknownresult \nfrom onecondi\u00adtional branch. This result is determined by not only the operands of the comparison andthe \nbranch relational oper\u00adator, but also by whether or not the branch was taken. For instance, the known \nresult after the conditional branch was not taken intheother block of Case IIIin Table 1 would be r[2] \n> 83. The second column in Table 2 depicts the condition associated with the branch to be subsumed. For \ninstance, the subsumable condition associated with avoid\u00adable branch in Case III of Table 1 would be \nr[21 > 76. The third and fifth columns of Table2 define the require\u00adments for the avoidable branch to \njump or fall through, respectively. A conditional branch with the condition 2 Note determining that a \nbranch can be avoided when It is encoun\u00adtered and not affected mapath (as shown m Figure 2)lsreallya \ncase of the branch subsuming itself. 83, as indicated by the jump requirement in the third row from the \nbottom of Table 2. The restructuring algorithm shown in Figure 3 will produce a dummy graph to efficiently \nrepresent the revised control flow of the loop. If it is later determined that the restructuring is worthwhile, \nthen the dummy graph will be used to modify the actual control flow of the function. The central idea \nof the algorithm in Figure 3 is that a new node will be added when no current node for that block exists \nwith the same set of states for the avoidable branches. Note that since each branch can have 3 states, \nthe upper bound for the code size increase is 0(3n), where n is the number of branches that can be avoided. \nIn practice, such increases have not been observed. However, to avoid an excessive code increase, a heuristic \nwas used to limit the value of n in a single loop. If more than n branches could be avoided in a loop, \nthen n branches are chosen based on the likelihood of being reached from the loop header. A conditional \nbranch in a node that has a known ~all-through or branch) state for that conditional branch will be eliminated \nin the restructured code. Note that the associated comp~ison and other dead instructions may be eliminated \nas well. Set the initial dummy node to be the header of the original loop with a state of unknown for \nall avoidable branches. Set the current dummy node to be this initial node, WHILE there are dummy nodes \nto process DO FOR each successor of the current dummy node DO Calculate the state of the successor. IF \na node associated with the successor exists with the same exact state for all avoidable branches THEN \nConnect the current dummy node to that existing node. ELSE Create a new dummy node with this state, connect \nthe current dummy node to it, and append it to list of dummy nodes. END IF END FOR Advance to the next \ndummy node to be processed. END WHILE Figure 3: Restructuring Algorithm Figure 2 (c) shows the restructured \ncontrol flow (dummy graph), The state is given to the left of each block, which is depicted with the \nblock number of the conditional branch and whether it fell through (F) or jumped (J) in the original \nloop. For instance, 2F3J indicates that the condi\u00adtional branch in block 2 will fall through next time \nand the branch in block 3 will jump. A basic block represented with a dashed box indicates that the conditional \nbranch (and typically its associated comparison) is unnecessary and will not be placed in the restructured \ncode. known subsumable jump fall through example example result branch requirement requirement I I V=cl \nV=C2 cl = C2 V=1O+VT cl #c2 V=1O--+-(V=I5) since 10= 10 since 10# 15 v#c2 cl *C2 v=lo+v #15 cl = C2 v=lo+-l(v \n#lo) since 10# 15 since 10= 10 -(c 1 re12 c2) V=1O+1(V>2O) since -I(1O > 20) v#cl c1 = C2 V*lO+=(V= \nIO) since 10= 10 v#c2 cl =C2 v#lo+v7:lo N/A NIA since 10= 10 v rell cl v re12C2 addeq(rell ) = addeq(re12) \nV>ll+v:m opp(noeq(rel 1), noeq(re12)) V>1O+7(V<1O) &#38;&#38; since 2 = 2 &#38;&#38; since opp( > , < \n) cl* addeq(rell) c2* &#38;&#38; 112 10+-1 -I(C1* addeq(re12)c2*) &#38;&#38;-(los lo-1) V=C2 N/A FJIA \n c 1 noeq(rell) C2 V22.0+=(V= I()) since 20>10 V*C2 c 1 noeq(rel 1) C2 V>20+V*1O NIA N/A since 20> 10I \n (1) v is a variable (2) c is a constant (3) rel is < , S . > , or 2 (4) opp(rel 1, re12) returns \ntrue when (x rel 1 y) &#38;&#38; (x re12 y) can never both be true (e.g. x > y&#38;&#38;x <y) (5) noeq(rel) \nreturns the relational operator without any equality (e.g. noeq( > ) and noeq( > ) both return > ) (6) \naddeq(rel) returns the relational operator with an equality (e.g. addeq( > ) and addeq( > ) both return \n 2 ) (7) c* is a constant that is adjusted by 1 in the appropriate direction if addeq(rel) != rel  \nTable 2: Subsumption Requirements Note that if block 3 is reached, then the conditional branch in block \n2 need never be executed for the remainder of the loop. Likewise, if block 5 is reached, then both the \ncortdi\u00adtional branches in blocks 2 and 3 can be subsequently avoided. AVOIDING BRANCHES NOT WITHIN INNERMOST \nLOOPS  The algorithm shown in Figure 3 was also extended to avoid branches that are not in the innermost \nloops of a program. The loops of a function are processed in decreas\u00ading order of their nesting level. \nOnce an inner loop has been processed, the effects of all of its blocks are unioned before processing \nthe next outer level loop. While process\u00ading the next outer loop, the inner loop is treated as if it \nwere a single block. The outermost level of a function is treated as a loop with no backedges. Only branches \nat the current loop level are considered candidates for being avoided. Furthermore, the effects of an \ninner loop are not currently used to make the states of branches at outer loop levels known. COMPRESSING \nTHE RESTRUCTURED GRAPH Occasionally, unnecessary nodes are introduced by the restructuring algorithm. \nFor instance, consider the con\u00adtrol flow of a function depicted in Figure 4 (a). Assume the conditional \nbranch in block 2 subsumes the conditional branch in block 7. If the branch in block 2 jumps to block \n7, then it is known that the branch in block 7 will transfer control to block 8. Likewise, if block 2 \nfalls through to block 3, then the branch in block 7 will transfer control to block 9. However, the execution \nof block 6 affects the brarlch in block 7. Figure 4 (b) shows the restructured con\u00adtrol flow using the \nalgorithm in Figure 3. Duplicate nodes for blocks 3,4, and 5 are generated since they have a differ\u00adent \nstate for the branch in block 7. But these duplicate nodes are unnecessary since the state of the branch \nwill be unknown upon transition to block 6. Once the loop has been restructured, the dummy graph is then \ncompressed to eliminate any unnecessary nodm. The algorithm for compressing the graph is shown in Figure \n5. The central idea is that the state of a branch in a node will become unknown unless it can reach a \nnode containing that branch with that state. Figure 4 (c) shows (a) Original Control Flow (b) Restructured \nControl Flow LEIL?J (c) Compressed~ontrol F1OW 1 2 3 45 6 F!) Figure 4: Eliminating Unnecessary Dummy \nGraph Nodes the control flow of Figure 4 (b) after compression.3 REPLICATION AND POSITIONING FOR THE \nRESTRUCTURED CODE Given the revised control flow represented in the dummy graph, the replication and \npositioning for the restructured code is accomplished in multiple stages. First, a set of heuristics \nare applied to determine if avoiding the conditional branches should be performed. Second, the blocks \nof theoriginal loop are replicated. Third, the blocks are positionally ordered to reduce the number of \nuncondi\u00adtional jumps in the replicated code. Finally, a number of optimizations are reapplied to the \nfunction in order to exploit the simplified control flow in the restructured code. 3 It maybe possible \nto perform this analysis on the control flow as\u00adsociated with the original loop and adjust the states \nof the dummy nodes as they are produced in the atgorithm shown in Figure 3. Thus, the last loop in Figure \n5 would not be required if unnecessary nodes are never intro\u00adduced. FOR each node in the dummy graph \nDO IF the node contains a branch and has a known state for that branch THEN Mark the node as reaching \nthat branch. END IF END FOR DO FOR each node in the dummy graph DO IF the node has a known state for \na branch and an immediate successor reaches that branch THEN Mark the node as reaching that branch. END \nIF END FOR WHILE any changes FOR each node in the dummy graph DO Set the state of the node as unknown \nfor any branch that is not marked as having been reached. IF another instance of the node exists with \nthe same state THEN Delete the node and adjust the transitions in the graph, END IF END FOR Figure 5: \nCompression Algorithm The first stage estimates the increase in size of the restructured code. The number \nof additional instructions for a loop was limited to avoid a significant increase in code size. When \nthis limit was exceeded, the analysis was reinvoked with a decremented number of avoided branches to \nfurther reduce the amount of replicated code. During the second stage, the restructured code is gen\u00aderated \nby replicating the corresponding blocks of the origi\u00adnal loop and adjusting the control flow according \nto the dummy graph representing the revised control flow, This includes the elimination of avoidable \nbranches and adjust\u00ading the control-flow transitions to enter and leave the restructured code instead \nof the original loop. During the third stage, the loop information within the revised control flow is \ncalculated. This information is subsequently used to adjust the positional order within the restructured \ncode by calling the procedure order, which is shown in Figure 6. This recursive procedure is initially \ninvoked with an empty list, the first block of the restruc\u00adtured code, and another empty list as parameters. \nThe out\u00adput of the algorithm is a list of blocks corresponding to a positional order such that unconditional \njumps are avoided when possible. The algorithm attempts to reduce the num\u00adber of unconditional jumps \nvia code positioning. While the restructured control flow reduces the number of conditional branches, \nit also introduces replicated blocks within new loops. It is imperative to find a good positioning of \nthese blocks or the benefit of avoided conditional branches may well be outweighed by the introduction \nof unconditional jumps to adjust the replicated control flow. PROCEDURE order(List, B, S-List) IF B not \nmarked as done AND none of the members of S-List dominate B THEN IF B is header of loop L AND there exists \nan unmarked successorof an exit block in L THEN B := unmarked successor of this exit block in L, END \nIF Mark B as done. S-list:= successors of B ordered by loop frequency, WHILE S-list not empty DO S := \nhead of S-1ist. S-list:= tail of S-list. order(List, S, S-list). END WHILE Insert B at the head of List. \nEND IF END PROCEDURE Figure 6: Positioning Algorithm The algorithm works as follows. The recursive proce\u00addure \norder terminates when all blocks are marked as done. The dominator check forces the recursion to backtrack \nalong the control flow when a block is encountered that is dominated by an unprocessed sibling block. \nThe dominator check provides the means to position if-then-else state\u00adments (even nested ones) before \nany blocks following the if-then-else construct. When a loop header is found, the algorithm follows the \ncontrol flow backwards to an exit block of the loop. It then processes an unmarked successor of the exit \nblock first. Thus, the algorithm attempts to process the exit block last, i.e. the exit block is positioned \nat the bottom of the loop. This avoids an unconditional jump at the bottom of the loop, The successor \nlist S-1ist of the current block is ordered in monotonically increasing loop frequency of the blocks. \nOn a tie of frequency, a block outside the current loop (that includes block B) appears first in the \nlist. This ordering ensures that the recursion is invoked on lower\u00adfrequency successor blocks first, \nthereby inserting these blocks in List before any higher frequency blocks. (Notice the post-recursion \naction to insert block B at the head of List.) As an example, consider the restructured control-flow \ngraph in Figure 2 (c). This graph contains a sequence of blocks, 5-7-2-3, in a separate loop. If block \n3 was position\u00adally the last block in the loop, it would contain an uncondi\u00adtional jump to block 5. The \nabove algorithm will eventually result in a call to order(listJ, 5, list2). The procedure deter\u00admines \nthat 5 is a loop header. It then follows the control flow backwards inside the loop to find block 2, \na successor of exit block 7. Thk results in a sequence of calls to order with blocks 3, 5, and 7, following \nthe control flow for\u00adwards. On the post-recursion action, these blocks are collected to yield the List \n= (2, 3, 5, 7/. This positional order avoids any unconditional jumps inside the loop, The resulting positioned \nloop is shown in Figure 2 (d). The dashed boxes indicate basic blocks where conditional branches were \neliminated, Notice that there are uncondi\u00adtional jumps following two instances of block 7 to block 8 \nas indicated by the dotted transitions. These jumps cannot be eliminated, but they have been moved outside \nof any loop within the restructured code. Thus, their execution frequency will be much less on average \ncompared to any instruction inside a loop. During the last stage, a number of standard optimiza\u00adtiorts \nare reapplied to the replicated code. This allows the compiler to take advantage of the simplified control \nflow due to the elimination of conditional branches. The absence of a branch and it s comparison operation \noften results in the elimination of a register assignment if the register was dead after the comparison. \nThe more effective reapplied optimizations include: dead code elimination (to delete dead assignments), \nbranch chaining (to minimize the over\u00adhead of branches from within the replicated code to it s sur\u00adrounding \ncode), global register allocation, common subex\u00adpression elimination, and code motion. The latter optimiza\u00adtion \nare applied to take advantage of the new loop struc\u00adtures within the replicated code. The effectiveness \nof avoiding conditional branches can only be fully exploited when these optimizations are reapplied. \nAt an earlier stage of this work, other basic block reordering algorithms were tested. It was found that \nthe benefit of avoided conditional branches (and their corre\u00adsponding compares) was sometimes outweighed \nby intro\u00adducing unconditional jumps on frequently executed paths. Thus, an increase in the number of \nexecuted instructions occasionally occurred. The algorithm described in Figure 6, on the other hand, \nyielded the best results for programs with different replication patterns by introducing fewer unconditional \njumps. RESULTS The optimization to avoid conditional branches was implemented in the compiler back-end \nVPO (Very Portable Optimizer) [BeD88]. The analysis and replication were performed after all other optimization \nhad been initially applied, except for filling delay slots, to maximize the bene\u00adfit of the traditional \noptimizations first.4 Measurements were collected on code generated by the compiler using EA!3E (Environment \nfor Architectural Study and Experi\u00admentation) [DaW91 ] on the SPARC architecture for a 4 Unstructured \nloops cun be introduced in the restructured code. Thus, loop optimizations should be initiatly applied \nbefore the restructur\u00ading optimization. Name banner cacheall cal ctags dhrystone join Od sched sdiff \nWc whetstone average Static Instructions Description Total Restruct banner generator +18,24Y0 +170,59Y0 \ncache simulator +3,08V0 +21.89Y0 calendar generator +21.51% +120.97 %0 C tags generator +10.53 %0 +35,49V0 \ninteger benchmark +8.60Y0 +18.23% relational join files +6.65Y0 +17.09~o octal dump +36.32 ZO +129.09% \ninstruction scheduler +34,25Y0 +87.0270 side-by-side file cliffs +1.25% +3.78Y0 word counter +39.22% \n+172,73?Z0 FP benchmark -0.89% -8.74% +16.25% +69.83Y0 Dynamic Instructions Total Restruct Branch -4.439io \n-5.43% -10.51% -2.23% -18.5870 -23.48Y0 -3.20% -12.48% -40.13% -1.67% -2.07% -5.91% -1.06% -7.44% -20.00% \n-7.91~o -9.87% -6.37% -9,57~o -12.18% -12.99Y0 -5.55% -8.39% -10.29% -4.15% -9.01% +0.0070 -11.llYO -12.82% \n-22.15% -6.81% -59.18910 -75.t)O~o -5.24% -14.31% -20.62% Hit Ratio Change 98.97% -0.34% 76.21% +().()3~o \n99.80% -0,35% 98.92% -0.40 70 84.62 %0 -0.70% 98.15% -0.79 %. 95.56% +1.60Y0 96.00% +1,22970 97.45% +0.09Y0 \n99.8970 -0.07% 100,00% -0.00% 95,05% +0.03?70 Cache Information Work -1.77% -2.30% -0.21% +1.54~o +1.56% \n-2.28% -18.89% -13.15~o -4.78% -10.52~o -6.45% -5.21% Table 3: Measurements number of C programs, which \nincluded benchmarks, UNIX utilities, and user applications. Table 3 shows the measurements for these \nprograms. Each program was tested with and without avoiding condi\u00adtional branches. The numbers in the \ntable represent the per\u00adcentage of change after applying the new optimization, Column 3 refers to the \nchange in program size. Column 4 shows the increased percentage of static instructions only within the \nrestructured code portions (loop or function level). Column 5 depicts the decrease in the total number \nof executed instructions. Column 6 illustrates the reduction of executed instructions only within the \nrestructured code. Column 7 reveals the dynamic change of branch instruc\u00adtions. Columns 8 and 9 report \nthe total hit ratio before the new optimization and the change in the hit ratio after apply\u00ading the new \noptimization, respectively. Finally, column 10 refers to the effect on cache work for a direct-mapped \nlkB instruction cache with a 16 byte line size. The cache work is calculated by the formula: cache work \n= cache hits + cache misses * miss penalty. The miss penalty was esti\u00admated at 10 cycles and a hit at \n1 cycle [Smi82]. The cache work is a better measurement than the hit ratio for the eval\u00aduation of optimizations \nwhen the number of executed instructions changes [DaH92]. The static measurements show that replication \nresults in an increase of code size of about 1670, depending on how many conditional branches could be \navoided. The original code portions increased by about on average 70% when they were restructured. The \ndynamic measurements indicate a savings of executed instructions of about s~o on average.5 The 5 l.he \n~he~s~~~~ &#38;n~hm~~k was reduced in code size due to a ch~n of unconditional jumps in whet 1(), accompanied \nby sets and tests of the same local variable, These chains were greatly simplified by the new opti\u00admization. \nFurthermore, the code positioning algorithm eliminated uncon\u00ad restructured code resulted in about 1490 \nfewer instructions executed compared to their original loops and 20?Z0fewer executed conditional branches. \nThe numbers indicate that the local savings of this new optimization can be substantial when the original \ncode portion is compared with the restructured code. The overall savings for a program depend on the \nexecution frequency of the restructured loop. It was surprising to find that even benchmark programs, \nsuch as dhrystone and whetstone, contained opportunities for the new optimization with respectable savings. \nThe hit ratio and it s change provides a general idea of the test programs caching performance. However, \nfor reasons mentioned previously, the cache work is a better indicator to evaluate the new optimization. \nThe cache work indicates that the reduced number of executed instructions outweighs the increase in code \nsize on average, even for a relatively small cache size of lkB. These results improved with larger cache \nsizes. Due to changes in the layout of basic blocks, the cache measurements may vary from pro\u00adgram to \nprogram. Thus, the average results seem more conclusive than the cache work of any single program. Figure \n7 shows the proportional benefit of the differ\u00adent techniques employed to avoid conditional branches. \nNot affected indicates branches that were encountered and not affected when reached on a subsequent loop \niteration (as shown in Figure 2). These cases account for about 1/3 ditional branches (due to gotos) \nin the restructured code, This resulted m an overall reduction of code size even after replication. For \nsdiff, the num\u00adber of compares and branches did not change, This was due to input data that never resulted \nin executing the restructured code portions where con\u00additional branches were avoided, The dynamic savings \nwere due to the ex\u00ad ecution of fewer unconditional jumps It was observed that restructuring the code \nprovided new opportunities to avoid unconditional jumps via code positioning. of the avoided branches. \nSubsumption means avoiding branches whose direction can be inferred from the result of other branches \n(as depicted in Case III of Table I) and accounts for over 1/5 of the savings. Branches avoided due to \nconstant comparisons imply that an expanded compari\u00adson was known due to an effect along a control-flow \npath (see Case I of Table 1) and are responsible for over 40% of the savings. In a few cases, branches \nwill follow the same direction since the result of the comparison can no longer be affected (as portrayed \nin Case II of Table 1). %Not .4ffectecl 20.il% Sulmmptloll 3 34(%Same D1rectloll 40 30% Constant Comparison \nFigure 7: Sources for Avoiding Branches A number of programs beyond the set in Table 3 were tested and \nit was found that some conditional branches could be avoided in every one of these programs, Yet, about \n1/3 of the programs resulted in an execution ben\u00adefit of 1% or less. It was also observed that the effec\u00adtiveness \nof avoiding conditional branches is highly data dependent. If branches are avoided in loops with high \nexecution frequencies, then the benefits can be quite high. This observation would suggest that avoiding \nconditional branches could be selectively applied where profiling data indicates that high benefits are \nmore likely. FUTURE WORK There are several areas that could be explored to pro\u00advide more opportunities \nfor avoiding conditional branches. The effects at each exit of an inner loop are not currently used to \navoid branches in outer loops. Yet, the results of inner loops are often tested in conditions in outer \nloops. The authors are considering applying the optimization to the control flow of an entire function \nall at once, rather than one loop at a time. Thus, the optimization could also be applied to functions \ncontaining unstructured loops. In addi\u00adtion, loops containing indirect jumps and associated jump tables \nare not currently restructured. Opportunities for avoiding conditional branches would increase if more \ninformation was available. For instance, flags are often declared as global variables. A call to many \nfunctions, such as print f, would not affect a global flag, However, the current analysis, which does \nnot perform interprocedural analysis, has to assume any global variable could be affected in an unknown \nmanner whenever any function is invoked. In addition, it was assumed that any variable could be updated \nwhenever a store through a pointer was encountered. InterProcedural and pointer anal\u00adysis would provide \nadditional opportunities for avoiding branches. CONCLUSIONS This paper described a general approach \nfor avoiding conditional branches by replicating code. The restructured code often contains simplified \ncontrol flow that allows other optirnizations to be applied more effectively. Vectorizing and parallelizing \ncompilers, in particular, may benefit from loops with fewer conditional branches. The optimization cculd \noften be applied and resulted in significant perfor\u00admance improvements for the code portions on which \nthe transformations were applied. The benefits of this opti\u00admization will improve as instruction cache \nsizes continue to increase. There are also promising future improvements that could be made to allow \na greater number of conditional brimches to be avoided. ACKNOWLEDGEMENTS The authors thank Jack Davidson \nfor allowing vpo to be used for this research. Ricky Benitez developed the ability in vpo to expand the \neffects of an RTL for use in other optimizations. This ability was used to expand the effects of compare \ninstructions, which proved quite useful for determining the set of registers and variables upon which \na conditional branch depends, Brad Calder, Emily Radiff, Randy White, and the anonymous reviewers pro\u00advided \nseveral helpful suggestions that improved the quality of the paper. REFERENCES [Ban93] U. Banerjee, \nLoop Transformations for Restructuring Compilers: The Foundations, Kluwer Academic Publishers, Norwell, \nMA (1993). [BeD88] M. E. Benitez and J. W. Davidson, A Portable Global Optimizer and Llnker~ Proceedings \nof the SIGPLAN 88 Symposium on Programming Language Design and Implementation, pp. 329-338 (June 1988). \n[DaH88] 3. Davidson and A. Holler, A Study of a C Function Inliner~ Sojiiare-Practice &#38; Expe\u00adrience \n18(8) pp. 775-790 (August 1988). [DaH92] J. W. Davidson and A. M. Holler, Subprogram Inlining: A Study \nof its Effects on Program Execution Time; IEEE Transactions on Soft\u00adware Engineering 18(2) pp. 89-102 \n(February 1992), [GOR90] [GrK92] [HeP90] [Jai91] [Kra94] [LRS76] [MoR79] [MuW92] [Smi82] [YOS94] J. W. \nDavidson and D. B. Whalley, A Design Environment for Addressing Architecture and Compiler Interactions; \nMicroprocessors and Microsystems 15(9) pp. 459-472 (November 1991). M. C. Golumbic and V. Rainish, Instruction \nScheduling beyond Basic Blocks: IBM Jour\u00adnal of Research and Development 34(1) pp. 93-97 (January 1990). \nT. Granlund and R. Kenner, Eliminating Branches using a Superoptimizer and the GNU C Compiler; Proceedings \nof the SIGPLAN 92 Conference on Programming Language Design and Implementation, pp. 341-352 (June 1992). \nJ. Hennessy and D. Patterson, Computer Archi\u00adtecture: A Quantitative Approach, Morgan Kaufmann, San Mateo, \nCA (1990). S. Jain, Circular Scheduling: A New Tech\u00adnique to Perform Software Pipelining, Pro\u00adceedings \nof the SIGPLAN 91 Symposium on Programming Language Design and Imple\u00admentation, pp. 219-228 (June 1991). \nA. Krall, Improving Semi-static Branch Pre\u00addiction by Code Replication; Proceedings of the SIGPLAN 94 \nSymposium on Programming Language Design and Implementation, pp. 97-106 (June 1994). P. M. Lewis, D. \nJ. Rosenkrantz, and R. E. Stearns, Compiler Design Theory, Addison-Wesley, Reading, MA (1976). E. Morel \nand C. Renvoise, Global Optimiza\u00adtion by Suppression of Partial Redundancies, Communications of the ACM \n22(2) pp. 96-103 (February 1979). F. Mueller and D. B, Whalley, Avoiding Unconditional Jumps by Code \nReplication; Proceedings of the SIGPLAN 92 Conference on Programming Language Design and Imple\u00admentation, \npp. 322-330 (June 1992). A. J. Smith, Cache Memories; Computing Surveys 14(3) pp. 473-530 (September \n1982). C. Young and M. D. Smith, Improving the Accuracy of Static Branch Prediction Using Branch Correlation; \nProceedings of the Sixth International Conference on Architectural Sup\u00adport for Programming Languages \nand Operat\u00ading Systems, pp. 232-241 (November 1994).  \n\t\t\t", "proc_id": "207110", "abstract": "", "authors": [{"name": "Frank Mueller", "author_profile_id": "81339518653", "affiliation": "Fachbereich Informatik, Humboldt-Universit&#228;t zu Berlin, Unter den Linden 6, 10099 Berlin, Germany", "person_id": "PP43141927", "email_address": "", "orcid_id": ""}, {"name": "David B. Whalley", "author_profile_id": "81100296923", "affiliation": "Department of Computer Science, Florida State University, Tallahassee, FL", "person_id": "P64341", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/207110.207116", "year": "1995", "article_id": "207116", "conference": "PLDI", "title": "Avoiding conditional branches by code replication", "url": "http://dl.acm.org/citation.cfm?id=207116"}