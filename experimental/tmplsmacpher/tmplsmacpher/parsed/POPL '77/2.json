{"article_publication_date": "01-01-1977", "fulltext": "\n Permission to make digital or hard copies of part or all of this work or personal or classroom use is \ngranted without fee provided that copies are not made or distributed for profit or commercial advantage \nand that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, \nto post on servers, or to redistribute to lists, requires prior specific permission and/or a fee.&#38;#169; \n1977 ACM 0-12345-678-9 $5.00 Machines with Multiregister Operations EXTENDED ABSTRACT A. V. Ahoand S. \nC. Johnson Bell Laboratories MurrayH ill, New Jersey J.D. Ullma~ Princeton University Princeton, New \nJersey Summary Previous work on optimal code generation has usually assumed that the underlying machine \nhas identical registers and that all operands fit in a single register or memory lo\u00adcation. This paper \nconsiders the more realistic problem of generating optimal code for ex\u00adpressions involving single and \ndouble length operands, using several models of register\u00adpair machines permitting both single and double \nword instructions. With register-pair machines a new phenomenon arises that is not present in optimal \ncode generation for sin\u00adgle register machines: Inanoptimal evaluation ofan expression it may benecessarytoos\u00adcillate \nback and forth between evaluating subexpressions of the expression. A linear-time optimal code generation \nalgorithm is derived for a register-pair machine in which all registers are interchangeable. The algorithm \nis based on showing that for this model there is an optimal evaluation sequence with limited oscillation \nbetween the sub\u00adtrees dominated by the children of a given node. Forother machine models including the \nfamiliar even-odd register-pair machine, optimal evaluation sequences can always require unlimited oscillation. \n1. Introduction In programming languages rich in expressions, like BLISS [W] or C [RKL1, we can find \nexpressions contain\u00ading many operators. In generating good object code for these languages, it is desirable \nto use registers to hold frequently-used data and addresses. Consequently, without exercising care in \nthe way available registers are used, there may not always be enough registers available to evaluate \nexpressions efficiently. Previous theoretical work on optimal code generation for expressions (e.g., \n[AJI, [B], [BLI, [N], [R], [SUI) has assumed that the underlying machine has N identical re\u00adgisters, \nthat each data object fits in a single register or memory location and that all operators produce a result \nin a single register. Urffortunately, few real machines have such a simple register architecture. Many \nhave special purpose registers for handling floating point operations, integer multiplication and division, \naddressing, condition codes, etc. In this paper we make a modest sally into this world of reality by \nconsidering the influence of operand tWork partially supported by NSF grant DCR-74-15255. size and register \nstructure on the complexity of code gen\u00ad eration. We relax the condition that data occupies one register \n or memory location by allowing operands and results of instructions to have size single or double, that \nis, taking one or two registers or memory locations. We shall call machines in which all instructions \noperate on single-sized operands single-register machines. Machihes with both sin\u00adgle and double instructions \nwill be called register-pair machines. A precise definition of these machine models is contained in Section \n2 of this paper. This paper considers optimal code generation algo\u00adrithms for register-pair machines. \nIn particular we are in\u00adterested in the optimal evaluation of expressions having no common subexpressions. \n(When the expressions are represented graphically, they are trees rather than dags [AU].) We exclude \nexpressions with common subexpres\u00adsions in this paper because in this case optimal code gen\u00aderation is \nalready difficult even on single register machines [BS, AJU]. With register-pair machines a new phenomenon \narises that is not present in optimal code generation for -single\u00adregister machines. Section 2. shows \nthat in an optimal evaluation of an expression on a register-pair machine it may always be necessary \nto oscillate back and forth r +-r op (ml, /7?,+1) between subexpressions of the expression. This oscilla\u00adtion \nis in direct contrast to the contiguity property of optimal programs for single-register machines [AJ]. \nFor certain types of register-pair machines Sections 3 and 4 show that we can bound this oscillation \nand thus derive a dynamic programming algorithm that generates optimal code for expressions in time that \nis linear in the input size of the expression. For other types of register\u00adpair machines, such as the \neven-odd register-pair machines, Section 5 shows that this oscillation can be\u00adcome arbitrarily large. \n2. The Machine Model Computers in existence today present a bewildering diversity of instruction repertoires. \nCertain operations such as multiplication and division often require double length data. The precise \ndetails of these operations, how\u00adever, vary widely from machine to machine. The multiplication instruction, \nfor example, may take as operands registers r and r and deliver the result in the register pair (r, r+l) \nor in the register pair (r 1, r) or possibly in register r with the contents of register r+l destroyed. \nRather than considering an intricate model which has sufficient complexity to mio ic every real machine, \nwe shall adopt a rather simple register-pair machine model in the hope that we can gain insight into \nthe effects of the architecture of the underlying machine on the complexity of code generation. We assume \na machine has N registers i-o, rl, , rN_l which are interchangeable with respect to all instructions \ninvolving single-length operands. The single-length in\u00adstructions have the usual form r + m [load] m \n+ r [store] r+ropr [operations]r+ropm } Here r and r refer to registers, and m to a memory loca\u00ad tion. \nWe also assume that there are instructions involving double-length operands. These instructions use two \nregis\u00adters or two memory locations to hold an operand. The re\u00adgister pairs that can be used to hold an \noperand are specified by a set P of permissible register pairs. For ex\u00adample, P might consist of all \npairs (r,, r,+l) such that i is even. The double-length instructions have the following form: (r, r ) \n+ (ml, m,+l) [double load] (m,, H7,+I) + (r, r ) [double store] (r, r ) -(r, r ) op (s, .s ) (r, r ) \n-(r, r ) op (m,, ml+{) (r, r ) + (r, r ) op s ,s#r, r (r, r ) -(r. r ) op trr r+rop (~, s ) r#s, .s \nIn these instructions (r, r ) and (s, s ) can be any register pairs in P and (m,, m,+l) is a pair of \nconsecutive memory locations used to hold a double-length operand. Note that in the operations, the destination \nis always the same as the left operand. If (r, r ) is a register pair in P, we shall refer to r as the \n/e/i register of the pair and r as the right register. To model real computers more closely we shall \nalso permit classes of extend and shorferl operations of the fol\u00adlowing form: (r, r )+ ELr (r, r ) +ERr \nr + SL (r, r ) r + SR(r, r ) where (r, r ) is any register pair in P. Finally we assume strong typing \nof operands; we do not permit the use of a double load or double store to move two single values. We \nshall consider three classes of register-pair machines in this paper. These classes are distinguished \nby the set P of register pairs usable in double-length in\u00adstructions. 1. UrlFeslricted Model P= {(r,, \nr,) I O<i, ,j<N l; /#j] 2. A ~jacen[ Model P= ((r,, r,+, ) I O</< N l}  3. Evetl-Odd Model  P = ((r,,, \nr~l+l) I O< I S\\(N-2)/2]) Although many other models are also possible, these three serve to illustrate \nsalient aspects of existing comput\u00aders with double-length instructions. Results pertaining to the unrestricted \nmodel can be used for machines where register-to-register moves and exchanges are much faster than other \noperations. The adjacent and even-odd models approximate machines such as the IBM System/370 and the \nPDP-11 where somewhat similar restrictions are placed on what registers can be used to hold the operands \nof multiply and divide instructions. Exarnp/e f. Let us consider how an expression might be evaluated \non an unrestricted machine with four registers r. through r3. The tree for an expression with which we \nshall deal is shown in Figure 1. We use circles to represent single values and rectangles for doubles. \nA se\u00adquence of machine instructions evaluating this expression tree with no stores is shown in Figure \n2. 0 It should be observed that the code of Figure 2 meets the restrictions of the adjacent machine but \nnot the even-odd machine (because E is loaded into an odd-even pair rather than an even-odd pair). In \nfact there is no way to evaluate Figure 1 on a four-register even-odd machine withoul using a store instruction. \nAnother important observation about the program of Figure 2 is that it begins working on the left subtree \nof the root then moves to the right subtree, then back and 22 Figure 1. Expression tree. (tl, r,) + c \n/* load Cinto (t O, r,) */ (r , r,) + (/73, I , )-L) /* compute n~ */ (rL r3)+ B (r2, r3) + (r2, r3)*(t \no, r,) I* compute t14*/ rj ---SR(rz, r3) I*compule tzj*I ro*F rO-ro+ G /*compute nla */ (r}, r2)-E \n(r,, rJ --(r,, rJ/r /* compute tl~ */ r2 + SR(rl, r2) /* compute llg */ (ro,rl)-A (ro, r,) + (rO, rl)+r~ \n/* compute )12 */ (rz, rs) + EL r2 I* compute t17*/ (r2, r~) -(r2, r3)*H /*compute n6 *I (m, r]) -(rO, \nri)+(r~, r~) /* compute tll */ Figure 2. Machine code to evaluate Figure I forth once more before evaluating \nthe root. This oscilla\u00adtion is necessary because if no STORE s are used, H3 re\u00adquires all four registers, \nand /?8 requires three. In fact, al] optimal programs evaluating Figure 1 on unrestricted re\u00adgister machines \nexhibit this oscillatory behavior, thus showing that this model does not satisfy the conditions for the \ncontiguity theorem of [AJI. It should be noted that the use of the SHORTEN opera[ors is not essential \nfor this example. We could re\u00adplace the subtrees of t~3 and })X by trees of singles requir\u00ading 4 and \n3 registers, respectively. for evaluation with no STORE s and (he same oscillation would appear in any \noptimal program using four registers. 3. Limited Oscillation in Unrestricted Machines This section shows \nthat the oscillations required to evaluate an expression tree for an unrestricted machine can be confined \nto LWO bounces between the subtrees of the root. This result is used in the next section to derive a \nlinear-time dynamic programming algorithm for the generation of optimal code for an unrestricted machine. \nThe va/ue of an instruction in a program is the expres\u00adsion which that instruction computes. The scope \nof an in\u00adstruction / is the subsequence of instructions from the in\u00adstruction following /, up to the \nlast instruction using the value of /. As a special case, if f computes the root and is not redefined, \nthen the scope of / is deemed to include the end of the program. A value is /ive at any instruc\u00ad tion \nwithin its scope. If the value of an instruction is never used, that instruction is use/e.ss and its \nscope is empty. The wM/f/~ of a program is the maximum over all instructions of the number of live values \nheld in registers after that instruction, counting doubles as 2 and singles as 1. Except for doubles, \nall these notions are as in [AJ], where more formal definitions appear. There are two properties of optimal \nprograms for single-register machines that were used by [AJI to obtain a dynamic programming code generation \nalgorithm, (1) Retraining, which allows us to take any optimal root program of width wand execute it \nusing a fixed set of w registers. That is, registers can be renamed at will.  (2) Rearratlgement which \nallows us to take any op\u00adtimal program for an expression tree Tand rear\u00adrange it into a sequence of subprograms \nPIP2 . P, P,+l such that each P,, 1 < i < r, evaluates and stores into memory the value of some node \nof Tand f,+] evaluates the root of T Except for the last instruction, there are no other stores in P,, \n1 < i < r. Moreover, for every node nof T, the portion of the subtree of 7with root n that omits proper \ndescendants of stored nodes is evaluated contiguously by a consecutive sequence of instructions in one \nof the P, s.  [n the unrestricted machine model, the renaming condi\u00adtion continues to hold, as all \nregisters are equivalent in their capabilities. The rearrangement condition, however, fails to hold, \nas we saw from Example 1. There is no op\u00adtimal program which evaluates each subtree of the root of Fig. \n1 contiguously. Nevertheless, we can prove the fol\u00adlowing modified version for programs with no STORE \nS; an obvious generalization analogous to the rearrangement theorem of [AJ] holds for programs with STORE \nS, Consider an expression tree T with root n, having two subtrees T] and T2. Roughly speaking, the following \nrearrangement theorem states that if T can be evaluated optimally with no stores, then T can be evaluated \nop\u00adtimally by a program which has one of the following forms: P,[ PI P21 P2P, [ PI P2P1 I P; P, P21 P; \nP;P]P2[ P;P; P2P, I whe~e PI and P2 evaluate subtrees T] and T2, respective\u00adly, leaving a result in a \nregister, and PI and P2 evaluate the remainder of T1 and T2, respectively. I is the instruc\u00adtion evaluating \nthe root tl. Thus an optimal program nev\u00ader needs more than two bounces between T, and T2. Theorem 1. \nLet P be a program with no STORE s and no useless instructions, computing some expression tree T on an \nN-register unrestricted machine. Suppose the root of T has children c1 and C2, where C2 may not exist. \nThen for each of c1 and C2 which is double and has a des\u00adcendant whose size is single, we can optionally \nselect a node m, which is a proper descendant of c,, (=1, 2, such that: (1) m, is single. (2) No proper \nancestor of m, except possibly the root of T is single, (3) We can find a program computing T with the \nsame number of instructions as P, of the same or smaller width as P, and having the form P] P2 PI, where \nP,, 1 <j<s, computes m,or c, into a register or registers, and / computes the  / 1 Moreover, P, is computed \nusing at most N ~.x (k) re\u00adk=l gisters, where x (k) is 2 if Pk computes some c1 and there is no mf. \nand x-(k) is 1 otherwise. That is, x(j) is the in\u00adcrease in the number of registers needed to hold the \nresults of PI, P2, , , P, over that needed to hold the results of P] ,P2, , P, ]. Proctf We shall actually \nprove a somewhat stronger result by induction on the length of P. The strengthening of the theorem necessary \nto form the inductive hypothesis is to permit P to compute T starting with the values of some leaves \nin registers, provided these leaves each meet the above conditions (l)-(2) on the m, s. P may also start \nwith the values of some of the C, S in registers even if they are doubles. Suppose the first instruction \nof P computes or loads a node n (perhaps a leaf) of the subtree of c,, The value of n is a double and \nat no time during the execution of P are fewer than two registers used for values in the subtree of c, \nuntil the root is evaluated. Then, using all N registers compute the value of c, using the instructions \nof P that serve that purpose. Call this sequence of instructions PI and suppose without loss of generality \nthat PI leaves its result in the last two registers, rN j and rA,_l. Rename the registers in the remaining \ninstructions of P so that only registers r. through rN-j are used. Since at least two of the N registers \nare always devoted to the subtree of c,, this renaming can be done. Let the resulting program be Q. By \nthe inductive hypothesis, we can put Q in the form P2 P, . P, I satisfying the conditions of the theorem \nand using N 2 registers. We may construct the desired pro\u00adgram PI P2 PL+, f by prepending PI to P2P3 \nP,/. At some time during the computation of P only one regis\u00adter is devoted to values of descendants \nof c,. Let n7, be the descendant of c, whose value is in a register the last time in P that one register \nis devoted to descendants of c,, It is not possible that some descendant of c, was initially in a register \nwhen P began, or else condition (2) in the theorem statement would be violated, Thus we may let PI be \nthe subsequence of P s instructions computing tT7, into some register, say N 1, Rename the registers \nused by the remaining instructions of P to avoid register N 1 until the value of m, is used. Since at \nleast one register is utilized for a descendant of m, until the value of m, is used, this renaming can \nbe done, Apply the algorithm re\u00adcursively to the remaining program and prepend F l as be\u00adfore. Example \n2. Consider the expression tree T in Figure 3. Theorem 1 implies that if T can be evaluated without STORES, \nthen in the worst (most oscillatory) case we can find singles ml and mj. no proper ancestor of which \nis single, such that an optimal program to evaluate T is nev\u00ad ,, ,, er more complex than P] P2 P} P21 \nor P2Pl P2PI, where P, and P2 are optimal programs to evaluate the subtrees of T with roots ml and n?2. \nPI and P2 are optimal programs evaluating the left and right subtrees of the root r, assum\u00ading ml and \nm2 have been evaluated and left in registers, / is the instruction evaluating the root r. 0 P, F lgure \n3. Expression {ree 7 4. Algorithm for Unrestricted Machines In this paper we measure the COS( of a program \nin terms of the number of instructions used. (The results can be generalized to apply (o more general \nadditive cost func[ions, if necessary. ) Theorem I [ells us that :In op\u00adtimal way [0 cornpule a tree \ninvolves working on each sub(ree of the root in turn, bretiking off work on a given subtree at mos( once. \nand leaving a single register holding u v:ilue for that subtree if and when we do. The dynamic programming \napproach of [AJ] can now be applied, pro\u00advided we compute the following COS[ vectors for each node /L \n (1) CI,,OI) = the cost of computing some singie\u00advalued descendtint )r~ of )] using I regis[ers and later \ncomputing (he bal~nce of the tree with root r] using j registers, including the register used to hold \nthe value of }}~. We can assume ,j< (, since o(herwise we could evaiua~e [he entire tree with root ~~without \noscillation using ,j regislers. If the tree with root )1 has no singles, then a,,())) can be taken to \nbe infinite,  (2) ~,(~~) = tbe cost of computing the tree with root }~ using 1 registers. If H is a \nleaf, ,6,()/) = O for till I.  To compute these values for a node H, suppose the a and (J values are \navailable for its children. We consider a list cl, Cz,,.. of children of the node )1, such that each \nsingle-sized child appears at mos( once, and each double\u00adsized child appears at most twice, The list \ncorresponds to an evaluation order in which ( 1) the children which do not appear are assumed to have \nbeen computed and stored, (2) the children which appear once are tissumed to be completely computed at \nthat time, and (3) the children which appear twice are assumed to have some single-valued subtree computed, \nand then later the entire subtree computed.  For each list, we shall describe the contribution to CII,( \nf~) and /31(t~) which might arise from computing the node )7 in the way described in the list. The actual \ncosts a,,()]) and ~l(t~) will be the minimum. over all lists, of the con\u00adtributed costs. The contribution \nto each cost of a non-Ieaf child c not appearing in the list is the cost of precom\u00adputing it and storing \nit; this is ~~(c) plus the cost of a single or double store. If c is the left descendant of H, it must \nalso be loaded, so this cost must be included. The contributions to a,,(~~) of a child c depend on whether \nit appears once or twice on the list and whether its sibling is a single or a double. A few cases should \nil\u00ad lustrate what is involved, 1, The list is clc~cl where C2 is a single. The cost of (his list is U,,, \n-l((l) + /3-l (cl) + cost of the instruction for /r. This cost reflects a computation in which we first \nevaluate a subtree of c1 (whose root is single) with 1 re\u00ad gister available, then compute the entire \nrighl subtree (whose root is (2) with ,/ 1 registers available (one being busy with [he descendant of \nc1). then return to evaluate the remainder of the left subtree with ,/ 1 registers, and finally evaluate \nH. If (2 is a double in this case, then the first term in the cost is al,l_2 (cl). 2. The list is CIC1CIC2. \nThe cost of this list is U,,, -l(cl) + ~,_l,,_l(c2) + cost of the instruction for H. Here we evaluate \na subtree of c1 with 1available registers, evaluate a subtree of c1 with ,j 1 available registers, complete \nthe evaluation of c1 with ,/ 1 available registers, then com\u00adplete the evaluation of C2 with ,) 2 available \nregisters, and finally evaluate )1. Similar formulas can be derived for the other cases. The cost /3,(t~) \nis the minimum taken over a,,()~) and the costs of the lists representing the ways of computing the tree \nwith root ~) without oscillation. For example, the list CICl, where both c} and C2 are singles is ~,(cl) \n+ B-1 (c2) + cost of the instruction for H. Clearly, given a list, the time required to compute a,, and \n/3, for a given i and ,j is bounded above by a constant. For binary operators, the number of lists is \nfinite (in fact, lists have length at most 4). Applying the same kind of dynamic programming considerations \nas [AJ], we can jus\u00adtify the following theorem: 7-heoretti 2, There is an algorithm to generate optimal \ncode for an unrestricted machine that is linear in the size of the expression, o Note that the above \nalgorithm is quadratic in the number of registers. If we permit operators of arbitrary ~rity /i, the \ncomplexity grows as 0((2k) !). (In effect, this is the number of lists which must be considered. ) 5. \nModels in Which Limited Contiguity Fails to Hold The result of Theorem 1 implies a modified version of \nthe contiguity theorem of [AJI, which in turn implies a polynomial :ilgorithm for optimal code generation. \nHow\u00adever, there are a number of machine models in which Theorem 1 can be shown no[ to hold. That is, \narbi~rary oscillation be~ween two subtrees may be necessary for op\u00ad[imal evaluation of an expression. \nFor example, consider the even-odd model. Le/t)nlu. For all in[egers u ~nd b with 2u+b < N, we can construct \na tree S,, ~ which can be evaluated with no STORE s into a single register if and only if at least u \neven-odd pairs and b additional registers are available. Pri)()/. The case u=O follows from the labeling \nalgorithm of [E, NI. The case b=O follows similarly, since i tree wi~h single-valued operands requiring \na regis[ers can be made to require u register pairs by making all i[s operands and operators double, \nand then shortening (he result. If u #0 and b#O, consider the ~ree of Figure 4. Since b >0, we can evaluate \n[he righ[ side and store the result in ti register which is not part of the u pairs, then evaluate {he \nleft. If we could evaluate Figure 4 with no stores us\u00ading fewer than u register-pairs we would violate \nwhat we know about $,,(), and if we used fewer thtin 2u+b regis\u00adters in total, we would viol~te what \nwe know of S~,~,,+h. 7/more))I 3. For every N there is an expression tree 7 with a node ~~wi~h two children \nc1 and C2 having the pro\u00adperty that any optimal program for 7 on an even-odd machine with 2N registers \nhas a subsequence of instruc\u00adtions /1,/2, , /2,v_l such that /1,/3, ~ evaluate des\u00adcendants of c1 and \n/2, /4, evaluate descendants of (2. That is, 2N 2 oscillations occur in the subtree of ~~. Pro?/ Figure \n5 shows the tree 7 Consider a program for 7 which does not store a result or move a value from one register \nto another. Surely any such program must be optimal. We mus[ first evaluate .S&#38;,o, or we shall never \nbe able to do so without STORE S. .SN,n must be computed in[o an even register, say register O, for we \nshall eventual\u00adly extend its value into the paired odd register. If we do the extension now, we can never \nevaluate .S~-l,l, since at least two registers will be tied up with the resulting value. Furthermore, \nthe result of S~-l,, must appear in register 1, the other half of the above pair, else there will not \nbe sufficient register-pairs to compute .SN-1,0. We are now in a situation where we cannot extend the \n value of SN,O in register O until we have computed the en\u00ad tire right side of Figure 5. Continuing in \nthis way, by symmetry of register-pairs, we must compute S,,0 in regis\u00adter 2(, v-/) and [hen S,_l I in \nregister 2( V-/)+l, for / = N I, N 2, , 3. Then we may compute S2 o in re\u00ad gister 2N 4, We now use the \nremaining 3 registers for S1 ,. arranging tha~ the result end up in register 21V 2. We then extend that \nvalue in~o regis~er 2N 1 and evalu\u00ad ate the right side of Figure 5 in regis(ers 2N 2 and 21V 1, Finally \nwe may extend S,,(I into register 2( N ))+1 for 2<1< N and evaluate the left side of Fig\u00adure 5 and the \nroot in registers O and 1. Thus every optimal program for 7 mus~ oscillate 2N 2 times back and forth, \nproving [he theorem. 0 It is interesting [o observe that even wi~h the adjacent\u00adregister constraint we \ncan produce subtrees to play the role of the .S, s in Theorem 3. For example, if there are N registers, \nthe tree of Figure 6 can be evaluated with no stores only if the result winds up in regis~er 0. Thus, \nif we permit trees like Figure 6, we can prove: 7/m~re)}~ 4. Theorem 3 also holds for adjacent register\u00adpair \nmachines. D 6. Summary and Suggestions for Further Work We have considered several models of register \npair machines and shown that there is an oscillatory behavior to the optimal evaluation of expressions \non these machines that is not present on single register machines. For the unrestricted model we were \nable to bound the os\u00adcillations to two and thus derive a linear-time dynamic programming algorithm. While \nthe dynamic programming algorithm is too expensive to implement in most compiler applications, its existence \nsuggests a more economical specialized linear-time algorithm could be constructed for particular machines \nin this class, For the even-odd and adjacent register-pair machines we showed the oscillations can be \nat least proportional to the number of registers. This result suggests that efficient optimal algorithms, \nif they exist, are unlikely to evolve from any dynamic programming considerations for these classes of \nregister~pair machines. There are a number of additional problems that would be worth solving to add \nto our understanding of code generation for register-pair machines. (1) Is the optimal code generation \nproblem polynomial or exponential for the (a) even-odd (b) adjacent register\u00adpair machines? Figure 4, \nThe tree .$a,b 26 ER Figure 5, The tree 7 (2) For what other sets of register pairs P, smaller cent register-pair \nmachines? than the unrestricted model, does the limited contiguity (5) How closely can optimality be \napproximated by a theorem hold. An interesting observation along these linear-or polynomial-algorithm \nfor the even-odd or ad~a\u00adlines is that if there is one additional register ra which can cent register-pair \nmachines? only be used as the left pair of a double, i.e., (6) How well can we generate code for register-pair \nP={(ra, ro), (r., rl), . . . (r~, r~-l)), then unlimited machines when expressions contain common subexpres\u00adbouncing \nis still required foroptimal evaluation ofexpres-sions? sions. (3) The trees exhibiting oscillatory \nbehavior for Bibliography Theorems 4 and 5 produce a number of bounces bounded by the number of registers. \nIs this the worst possible [AJI A. V. Aho and S. C, Johnson, Optimal Code We? If SO, it appears that \noptimal code generation could Generation for Expression Trees, JACM 23:3 be done in time polynomial \nin the tree size but exponen\u00ad (July 1976), 488-501, tial in the number of registers, [AJUIA. V. Aho, \nS. C. Johnson, and J. D. U1lman,(4) [f we could bound the number of oscillations by Code Generation for \nExpressions with Commonsome polynomial in the height of the tree, the algorithm Subexpressions, JACM \n24,1 (January 1977), to of Section 4 can be generalized to yield a polynomial algo\u00adappear. rithm, 1s \nthere such a bound for the even-odd or adja\u00ad o SL A EL N 2 ~L s EL SL EL i Figure 6. Tree requiring \nN registers with the result in register 0, [AU]A. V. Aho and J. D. Unman, Optimization of [N 1. Nakata, \n<On Compiling Algorithms for Arith-Straight Line Code, JIA M J. Computing 1>1. metic Expressions, Comm. \nACM 18:8 (August (1972), 1-19. 1967), 492-494. [B] J. C. Beatty, An Axiomatic Approach to Code [R] R. \nR. Redziejowski, On Arithmetic Expressions Optimization for Expressions, JACA4 19:4 (Oc-and Trees, \nComm. ACM 12:2 (February 1969), tober 1972), 613-640. 81-84. [BL1 J. L. Bruno and T. Lassagne, The Generation \nof [RKL]D. M. Ritchie, B. W. Kernighan, and M. E. Optimal Code for Stack Machines, JACM 22:3 Lesk, The \nC Programming Language, CSTR (JuiY 1975), 382-397. #31, Bell Laboratories, Murray Hill, N. J., 1975. \n[BSI J. Bruno and R. Sethi, Code Generation for a [SU1 R. Sethi and J. D. Unman, The Generation of One-Register \nMachine, JACA4 23:3 (July 1976), Optimal Code for Arithmetic Expressions, JACM 502-510. 17:4 (October \n1970), 715-728. [E] A. P. Ershov, On Programming of Arithmetic [W] W. Wulf, R. K. Johnsson, C. C. Weinstock, \nS. O. Operations, Dok/. A. N. USSR 118:3 (1958), Hobbs, and C. M. Geschke, The Dewgn Qf ur7 Op\u00ad427-430. \n(English translation in Comm. ACM 1:8 tlmizittg Compiler, American Elsevier, New York, (1958), 3-6.) \n1975. 28 \n\t\t\t", "proc_id": "512950", "abstract": "Previous work on optimal code generation has usually assumed that the underlying machine has identical registers and that all operands fit in a single register or memory location. This paper considers the more realistic problem of generating optimal code for expressions involving single and double length operands, using several models of register-pair machines permitting both single and double word instructions. With register-pair machines a new phenomenom arises that is not present in optimal code generation for single register machines: In an optimal evaluation of an expression it may be necessary to oscillate back and forth between evaluating subexpressions of the expression.A linear-time optimal code generation algorithm is derived for a register-pair machine in which all registers are interchangeable. The algorithm is based on showing that for this model there is an optimal evaluation sequence with limited oscillation between the sub-trees dominated by the children of a given node. For other machine models including the familiar even-odd register-pair machine, optimal evaluation sequences can always require unlimited oscillation.", "authors": [{"name": "A. V. Aho", "author_profile_id": "81100024612", "affiliation": "Bell Laboratories, Murray Hill, New Jersey", "person_id": "PP43126072", "email_address": "", "orcid_id": ""}, {"name": "S. C. Johnson", "author_profile_id": "81332506933", "affiliation": "Bell Laboratories, Murray Hill, New Jersey", "person_id": "PP43126474", "email_address": "", "orcid_id": ""}, {"name": "J. D. Ullman", "author_profile_id": "81100314798", "affiliation": "Princeton University, Princeton, New Jersey", "person_id": "PP43144682", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/512950.512953", "year": "1977", "article_id": "512953", "conference": "POPL", "title": "Code-generation for machines with multiregister operations", "url": "http://dl.acm.org/citation.cfm?id=512953"}