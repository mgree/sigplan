{"article_publication_date": "01-01-1977", "fulltext": "\n Permission to make digital or hard copies of part or all of this work or personal or classroom use is \ngranted without fee provided that copies are not made or distributed for profit or commercial advantage \nand that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, \nto post on servers, or to redistribute to lists, requires prior specific permission and/or a fee.&#38;#169; \n1977 ACM 0-12345-678-9 $5.00 MINIMAL AND OPTIMAL COMPUTATIONS OF RECURSIVE PROGRAMS. G6rard Berry, Ecole \ndes Mines de Paris and IRIA-LABORIA, Jean-Jacques L6vy, IRIA-JABORIA, 78150-Le Chesnay, France. apply. \nExtension towards the X calculus is done INTRODUCTION in [10]. Procedure call mechanisms have mainly \nbeen The lattice property of terms breaks down in studied in the framework of recursive programs general \n: two very different derivations may lead without assignments, for the simplicity of their to the same \nterm by syntactical accidents, which operational and denotational semantics (See Scott collapse two a \npriori different terms. In section I,[161, Nivat [141, Vuillemin [171). we take care of this fact by \nintroducing an equiva According to operational semantics, procedure lence and a preorder on derivations. \nWe give three calls act as textual rewritings ; computation rules characterizations of these relations. \nFor the main select at each computation step the occurrences of one, we extend the classical notion of \nresiduals unknown functions to be rewritten. A computation [41 by defining residuals of derivations by \nderiva\u00ad rule is called correct if the value it computes is tions, We show that derivation classes form \na latti\u00adthe one given by the denotational semantics. Correct\u00ad ce. ness and efficiency of computation \nrules have been In section II, we study the simple derivations studied in Vuillemin [17,18], Montangero-Pacini\u00addefined \nby Vuillemin with use of labels (named here Turini [13], Downey-Sethi [7]. The main results are comp2ete \nderivations). We give two characterizations well-known: innermost evaluation (call by value) is of them \nin the usual formalism. not correct, parallel outermost or full substitution Section III, is devoted \nto minimality and opti\u00ad are correct. Vuillemin [17,18] gives a sufficient mality results. Ordering infinite \nderivations as condition for a rule to be correct, (safety), later well as finite one, we construct Zeast \ncomputations extended by Downey-Sethi [7] into a necessary and of every syntactic approximation of the \ninfinite sufficient one (security). Vuillemin also studies a tree determined by the program. The associated \ncom\u00ad particular implementation of call by~name, the plete derivation are optimaz with respect to Vuille\u00addelay \nrule ; he shows its optimality with respect min s cost. Extension to interpretations is then to a reasonable \nimplementation cost, provided inter\u00adstraightforward, as soon as they satisfy a syntactic pretations \nsatisfy a sequentiazity condition. This condition. All classes of sequential interpretationsresult is \nin fact twofold : sequentiality allows considered in [2,6,11 ,171 do satisfy this condition. elimination \nof useless steps, and optimality follows The computation rule we use for constructing the by using sharing \nmechanisms in term implementation. optimal computations is very inefficient in general, The basis of \nall these studies is the following but reduces to the usual delay rules in sequential theorem (Vuillemin \n[17,] 81) : provided some restric\u00adinterpretations . tive conditions on programs are satisfied, the set \nof terms derivable from a given term is a lattice PRELIMINARIES under the derivation ordering. F-Algebras \nOui aim is to extend these results, for the three following reasons : first, though every Let F = {f,,f2, \n. . ..fm} be a set of @zction program can be transformed to match VuiJlemin s symbozs ; each symbol f. \nis given with its arity conditions, the transformations may affect the costs of computations : a more \ndirect proof can p(f.)20. Wedenoteby# thesetof k-ary symbols be investigated. Second, a direct generalization \nin k. The nullary symbols are called constants. A to the k-calculus is not straightforward, since A-terms \ndefinitely do not form lattices, Third, F-algebra is an object I = <D1,f~, ,,,, fl> where D1 the symbolic \n(or Herbrand) interpretation [51 is pf is a set and each f~ is a mapping from DT( 1) into not sequential \nin the sense of Vuillemin, and no optimality result is known for it. A morphism of F-algebras I and 1 \nis a mapping I Our point of view will be purely syntactic : we e : D1 + D1,, which preserves the operations \nfi : reconstruct the lattice property in derivations. We ~f.cFk ~;cDk ~ study minimal computation~ (i.e. \nfinite or infinite f:(i)) = f: (e(i)) 1 I derivations)in the symbolic interpretation, and transform them \ninto optimal ones, Eventually, we characterize interpretations to which similar results The Free F-algebra \n: Consider a denumerable set V={x ~ ,x ~,..., ,...1 % of variable symbols, and Vk={xl,x2, ..., ~}. ThenV \ngenerates the free-algebra M(F,Vi It contains V and is such that for any F algebra I, every mapping v: \nV + D extends in a unique way into a morphism (I,v) : M~F,V) + D .Elements of M(F,v) are called terms, \nand will belrepresented by well-formed\u00adformulae or trees. The morphism (I,v) determines the value of \na term for the interpretation I and the data mapping v, A substitution is a mapping u : v + M(F,v), extended \ninto a morphism u : M(F,V) + M(F V). We write o: ~/~, and for t=M(F,V), ut=t[~/~]. Occurrences of symbols \nin terms Let . denote concatenation and c be the empty word: For tcM(F,v) and seFuV, the set C(s,t) of \noccurren\u00adces of sin t is defined by : (i) t=xieV C(Xi,Xi)={E} C(s,xi)=O for s # x. 1 k (ii) t=f(;), \nfe Fk , C(s,t)=c u .U .C(s,tj)) ~=l((f,j) with c ={EI if s=f and C=@ otherwise ~: t=f(xl,f(x2,x,)). Then \nC(f,t) = {s, (f,2)} and C(xl,t) = {(f, l),(f,2)(f,2)}. The size IItll oftis the number of occurrences \nin t. Lemma 1 : let t =tEt /xiI and c cc(s,t ). Then either C=C.C with ccC(xi ,t) and c ~c(s,t ), or \nC ec(s,t) , Proof : by induction on the length of c. O Recursive program schemes. Derivation and residuals. \nConsider two disjoint sets F={fl,f2, . . ..fm}. of basic function symbols, . . ..l$Nl. of and $={$1,$2, \nunknown function symbols, given with the arities p(fi)ao and p(@i)20. We write M=M(Fu@,V) and N C(@,t)= \niglc($i,t). All terms implicitely belong to M, and since we shall only be interested in occurrences in \nC(p,t), we abbreviate ccc(~,t) into c in t. Given C,C in t, then c conta{ns c! written ~<~ r, if ciS \naprefix of c , and cand C are disjoint, written cIc , if neither CSC nor C SC. An occurrence c in t is \noutermost in t if it is not contained in another c in t. A recursive program scheme Z is a system of \nequations Z @i(x* ,x2>...>&#38;()))) = i 1 I i = 1,2 ,...,N and TieN(Fuo,V P(+i)) we consider a fixed \nprogram scheme E, thus omitted in notations. Let teM and cec($i,t), and let y be an additional variable. \nThere exists a unique decomposition t = tc[$i(~)/y] s.t. tc~M(FuO,Vu{y}) and C(y,tc)={c}. Then t detives \nimmediately into $ +by rewriting of c, written t $ t , iff t =tc[Ti[t/x]/y]. Given c ~c($lj,t ), three \ncases are possible, by lemma 1 : (i) C =c.cl with ClCC($j,Ti). Thenc is created by the derivation. (ii) \nc ~c($j,tc). Then c in t is a residua2 oft in t. (iii) C =C.C1.C2 with Cl~C(l,Ti) and c2cC($j,tk). Then \nc in t is a residual ofc, ($i,k).c2 in t, (See Church [4]). : o(x) = f(x,$(x))WEE t=f($($(x)),$(x)) (f~])f(f($(x) \n,$($(x))),$(x)) with =f(y,$(x)). Here (f,l)(f,2) is created, (f,2) (f,l) is a residual of (f,2), and \n(f, l)(f,l) and (f, l)(f,2)($, l) are residuals of (f,l). Given CcC($,t), $et C/c denote the set of residuals \nof the elements of C by immediate deriva tion of c, and let c /c abbreviate {c }/c. A derivation d : \nt ~ t is a sequence c1 2 k d : t+ t,+ = t . The concatenation 2 + k of d:t$t and d :t! $t is written \nd;dl and @ denotes the empty derivation. When t is assumed from the context, d can be written d=c1;c2;. \n,.;ck. Therefore c will denote either an occurrence or its derivation, unambiguously in each context. \nThe notion of residual extends to deriva\u00ad tions for CcC(O,t) by : C/(c;d) = (C/c)/d, and C/g = C. The \nset of all derivations starting from t is denoted by ~(t). If d=cl;c2; ,..;ck , then c.d abbreviates \nC.C l;c c2; ;c lc 1. EQUIVALENCE OF DERIVATIONS MODULO PERMUTATIONS. The preorder t ~ t is not an ordering \nif Vuillemin s conditions []51 are removed, and term classes do not even form lattices ; let Z: $(x) \n=x, $(x,y)=x and consider all derivations from !($($(x),y)).This example involves the derivations : $($(x)) \n$ O(X) and d2 : $($(x)) !l)$(x) 1 which give the same result by syntactical co~nci\u00addence. Using term \nspaces is the same as using deri vations spaces quotiented by t~e equivalence d~d if d and d are of the \nform t + t . Remark now that E/dl=@ and c/d ={=]. The example suggests to define 2 a finer equivalence \n:let d,d eD(t) be preequivalent, written d~dr , if d~d and c/d = c[d for all c in t. Then d14d2, Preequivalence, \nas a congruence for the concatenation, defines a preorder ~ by did iff ~d s.t. d;d =d . (The same definition \nwithx~ gives the image in derivations of the preorder + in terms) . However the equivalence generated \nby ~ is not =. Take Z: $(x,Y) = $(y,x),$(x)=f and d:$($(x),~(x)) $ $(llJ(x),O(X)). Let dl=d and d2=d;d. \nThen d1~d2~d1 since dI;d=d2 and d2;d~dl, but d =d 12 c is false. Therefore we define a finer equivalence \non derivations. I.1, Definition of the equivalence : The following permutation lemma is the basis of \nour constructions, 1.1.1. Lemma : Ifc are in a term t, then 1 C2 c1;(c2/c1)=c2;(c1/c2).(*) Proof : \nlet dI=cl;(c2/cl) and d2=c2;(cl/c2). Let c be in t. We have d1~d2 and c/dl=c/d2 by case ins\u00ad pection \non the relative positions of c, cl, C2. Thus n l=d2 1.1.2. Definition : The equivalence of derivations \nmodu20 pervnwtatzons, written -, is the least equiva lence satisfying : (i) Cl;(c2/cl)-c2;(cl/c2) (ii) \nif d-d , then d1;d;d2-dl;d ;d2 for any dl,d2.  Notice that -is the least congruence for concatenation \nwhich satisfies the permutation lermna, and that -is a refinement of =, i.e. dl-d2 implies Intuitively, \nif dl-d2, we can match d2 with l=d2 dl by successive uses of the permutation lemma. 1.2. Complete derivations \nof a set of occurrences : Let C be a set of occurrences in a term t. Following Curry &#38; Feys [6], \nwe are concerned with deriving all occurrences of C. This notion clearly makes sense when C is a set \nof disjoint occurrences (as previously seen). But this is more difficult when there are nested occurrences \nin C. 1.2.1. Definition : The set ~ of eompzete de~iva\u00adtzons of C is the set of finite and infinite derivations \ndefined by ~={c;(~)]ccC}. 1.2.2. Theorem : All derivations in ~ are finite and equivalent modulo permutations. \n Proof : If CCC, let IT(c,C) be the nesting level of= C (i.e. the number of prefixes of c in C). Let \nk=k(C) be the maximal nesting level in C and n. (C) be the number of occurrences in C at level i, C&#38;nsider \nthe k-tuple n(C)=<~(C),\\-l (C), . ..nl (C)>. Then one can check easily that n(C/c)< n(C) for any k c \nof C when < is the le~icographic ordering on N . Thus all derivations in C are finite. Furthermore, suppose \ndl,d2c~. We get dl-d2 by induction on n(C). The only problem is when dl=cl;d~ and d2=c2;d~. BY 1.1!1, \nwe have C/(cI;(c2/cl))=C/(c2;(cl/c2) )=C . Thus , if d c~ , then (c2/cl);d3c(C/cl) and 3 (cl/c2);d3e(~2). \nBy induction d~-(c2/cl);d3 and d~-(cl/c2);d3. Hence dl-d2 by 1.1.1. IJ This theorem is a strengthened \nform of Curry s Property E [61, because we replaced preequivalent by equivalent. The modification seems \nminor here but is crucial for what follows. 1,2,3. Notations : We consider ~ as an elementary step of \nderivation and write t &#38; t? for such a step,or simply C in a derivation context. We use c1 C2 capital \nletters for derivations t+ t + t . . ,+ t 12 n and therefore write D=C, ;C2;. ,.Cn. Hence D={dl;d2; . \n..dnldic~i}.the length IDI of D is the num\u00ad ber of nonempty derived sets and W is extended to sets. All \nthese notations are consistent because of theorem 1.2.2. The permutation lemma extends to sets as follows \n(and this corresponds to the so-called lemma of parallel movest in [61). 1.2.4. Lemma : If C ~, C2 are \nsets of occurrences in a term t, then C (C /C ) -c2;(c1/c2). 1 21 Proof : Both sets only contain complete \nderiva\u00ad . n ions f CIUC2 1.3. Residuals of derivations : 1.3.1, Definition : If D D co(t), the derivation \n 1 2 D21Dl residual ofD2 by D1 is clef: ned inductively by : (i) @/D=@  (ii) (D2;c2) /D, = (D2/D1);(c2/(D \nID2) )  This definition makes sense by induction on \\D1 1+ D21, taking as hypothesis that lD2/Dll<\\D21 \n and is illustrated by diagram 1, where each elemen\u00adtary square is an application of 1.2,4. 1.3.2. Lemma \n: 1) D/@ =D 2) (D2;D~)/D1=(D2/Dl);(D~/(Dl/D2) ) 3) D2/(Dl;D;)=(D2/Dl)/D~ -Proof, . by induction on IDI \nfor 1, by simultaneous InductIon on <max(\\D~l,lD~l),lD21> for 2 and 3.0 1.3.3. Lemma : If D1,D2e~(t), \nthen D1;(D2/Dl)-D2;(Dl/D2). Proof : by induction on IDI] + ID21. O This lemma is a generalisation of \nthe permuta tion lemma to derivations, but is also a strengthe\u00adned form of the Church-Rosser theorem \nand the proof technique is exactly the one used in [6]. But the advantage of considering equivalence \ninstead of preequivalence appears in terms of residuals of derivations, Equivalence exactly means consistency \nof residuals of derivations. The equivalence -defined in 1.1.2. can be generated using lemmas 1.2.4. \nor 1.3.3. instead of lemma I.1.l.  1.3.4. Lemma : If 11,-D2, then D1/C-D2/C. Proof : We have only to \nconsider Dl=Cl;(C2/Cl) and D =C ;(C /C ). Thus D /C=(C /C);D~ where 2212 11 D;=(C2/Cl)/(C/Cl). By lemma \n1,3.2., we get Df=C2/(Cl;(C/Cl))=C2/(C;(Cl /C)) by lemma 1.3.3. Therefore D~=(C2/C)/(C1/C) by 1,3.2. \nand D1/C=C~;(C~/C~) if C~=C1/C and C~=C2/C. Similarly Dq/C=C~;(C~/C~). Hence D,/C-Dq/C by 1,3,3. D (~)We~ons~de~ \n= extende~ to;ets of derivations of disjoint occurrences. 1.3.5 .Theorem : D,-D2 iff~D.D/D1=D/D2 iff \nD1/D2=D2/Dl=0. Proof : 1 => 2. By induction on IDI using 1.3.4. We .. emember hat l-Dz lmplles 1=D2 2 \n=> 3. Take D=DI or D=D2 and remark that D1/D1=D2/D2=@. 3 => 1. By induction on \\Dll+\\D21 using 1.3.2 \nand 1.3.3. D 1.3.6 Proposition : DI-D2 iff~D.D1/D-D2/D iff VD.D11D=D21D. Proof : 1 => 2. By induction \non IDI and lemma 1.3.4. 2 => 3. Because -is a refinement of =. 3 > 1. Take D=DI or D=D2 and apply 1,3.5, \nD 1.3.7. Proposition : The equivalence -is left sim plifiable. Formally, if D;D1-D;D2, then DI-D2. Proof \n: obvious from 1.3.5. since (D;D1)/(D;D2)=D1/D2. n 1.4. The lattice of derivation classes : We now define \nthe preorder on derivations.  1.4.1. Definition : D <D iff ~Ds.t. D D-D 12 1 2 1.4.2. Proposition : \nThe relation S is a preorder with -for associated equivalence and 131sD2 iff D1/D2=@. Proof : D<D since \nD-D. Assume D1<D2SD3. Then there are D and D; s.t. Dl;D~-D2 and D .D -D 1 2 2 3 ence Dl;D; ;D~-D s and \nDI<D Now suppose D1SD2. Then 3 DI;D-D2 for some D. Thus lemma 1.3.2 and theorem 1.3.5 implies (Dl;D)/D2=(Dl/D2);D \n=@. Therefore D1/D2=fl Conversely, if D,/D2=!?J, one has Dl;(D2/D1)-D2 by the permutation lemma, i.e. \nDl~D2. Now if Dl~D2~D1, we have D1/D2=D2/Dl=0 and thus DI-D2 by 1.3.5. D 1,4.3. Notation : Let [D] be \nthe equivalence class of D and [D(t)] be the set U(t)/-. 1.4.4. Theorem : The set [~(t)] with the quotient \nordering < is a upper-semilattice. Two elements [Dl] and [D.] have the l.u.b. ~Dl;(D2/DI)]=[D2;-(Dl/D2)l \nProof : One has DISD1 ;(D2/D1) andD2SD2;(D1/D2). Furthermore, let DIsD and D2sD. By 1.4.2, we have D1/D=D2/D=~. \nLemma 1.3.2 implies (Dl;(D2/D1))/D=(Dl/D);D =D where D =(D2/Dl)/(D/Dl). But D =D2/(Dl;(D/Dl))=D2/(D;(Dl/D) \n)=(D2/D)/(Dl/D) by 1.3.2, 1.3.3 and 1.3.5. Hence D =@ and Dl;(D2/Dl)<D by 1.4.2. D It is also shown in \n[3] that [Dl]and [D2] have a greater lower bound. 1.5, Standard derivations : Standard derivations, which \nwork in an out\u00adside in way and by convention from left to right for disjoint occurrences, are introduced \nin [61. The standarisation theorem in [6] allows any reduc tion to be standardised. But, although there \ncan be several standard derivations between two given terms, each equivalence class contains a unique \nstandard derivation, This will be our improved standardisation theorem. 1,5.1. Definition : For t=s(?) \nand di:ti+t~, let s(~)=((s, l) .dl);((s,2).d2) ;.. .((s,kdk)k) where k=p(s). The c?eritxztion d &#38; \nstandard if d=s;d and d is standard or if d=s(d) and di is standard for all i. 1.5.2. Notations : The \nset C of of occurrences is internal if c/C. The derivation D is internal if it only derives internal \nsets. Let Cn=C;E; . ..S. 1.5.3. Lemma : For any C, there are n and C inter\u00adnal s,t. n20 and C-cn;C), \nProof : by theorem 1.2.2, since we can consider any outside-in complete derivation of C.n 1.5.4. Theorem \n: There is a unique standard deriva tion in each equivalence class of [~(t)]. Proof : Existence (this \nproof comes from [12]). Consider any derivation D:t~t . With the above lemma, we can write D=Cl;C2;. \n..Cn where either Ci={E} or Ci is internal for all i. Let I(D) be .Q(D);D,, with D,, internal maximal \ns.t. D=D ;E Let n(D) be the number of internal Ci preceding some e-step. Formally n(D) is the number \nof i s.t. Ci is internal and ~j. l<i<j<n s.t. Cj={c}. Consider the triple < Ilt 11 ,n(D),L(D)>. We have \ntwo cases. First- L(D) ly D=c ;D2 with D2 internal. By induction on llt;llDj w: y 2-D;where is and D; \nstandard L(D) D-c ;D2. econdly D=D ~;c;s ;D2 with C and D2 internal and !L(D)>O. But C cannot create \ns and C;c is a complete derivation of CU{C}. Hence C; C-Cp; C with C internal by the above lemma and \np>O since EeCu{e}, Therefore D-D with L(D)-] D =D ~;c, P c ;e ;D2. Then n(D )<n(D) if C =@ or I(D)=l, \nand n(D )=n(D),i(D )<k(D) if C # @ and L(D)>l. By induction D -D where D is standard. Uniqueness. Let \nd,d :t% be standard s,t. d.-d , We prove d=d by induction on <Idl+ld 1, Ilt 11 >. The induction works \neasily when d=s(~),d =s(~ ) or d=c;dl,d =c;d; . Now suppose d=s;dl and d =s(~ ). Then s=$~O and E/d ={s}#@ \nwhich contradicts d-d by theorem 1.3.5. 0 218 1.6. Labelled derivations [9, 171 : We use Vuillemin s \nlabelling system [1711 to mark occurrences along derivations. Given an alpha bet E, let the set E* of \nwords on E be the set of labels . (In fact associativity is not necessary but convenient for notations~. \nA hzbelling p of a term t is a mapping p:C(Q,t)+E , A labelling v of the pro\u00adgram Z is a N-tuple (VI,V2, \n. ..VN) of labelings V, 1 of the T i. Given a labelled term (t,u) and an ele\u00ad mentary derivation d:t% \nwith ccC(@i,t), the corresponding step of labelled derivation d: (t,u)$(t ,u ) is defined by ; ~ (C )=p(C~) \nif C ~Cj/C P (c )=p(c)vi(c ,) if c creates c and c =c.c 1  For example, let t=$($(x)) and Z:$(x)=f(x,$($(x))) \nand let us write labels as exponents of occurrences in terms. Suppose (Z,v):$(x)=f(x,@a(+b(x))) and (t,v)=$c($d(x)). \nThen (t,~)$f($d(x),$ca(+cb(+d(x)))). 1.6.1. Notation : We write d= d! if d and d! are of the form d,d \n:(t,p)~(t ,~ ). The permutation lemma also holds for labelled derivations. 1.6,2. Lemma : If c c are \nin t, then 1 2 C1;(C z /c ~)= ~ c2;(cl/c2) for all U. Proof : by cases inspection. 0 1.6.3. Proposition \n: If d d then d ~ d for 1 2 lp2all u. Proof : obvious from 1,6.2, U 1,6.4. Definition : A labelling \np of a term t is consistent if the labels p(c) and p(c ) are incompa\u00adtible for any pair c and c of nested \noccurrences in t. A labelling w of X is consistent if all the vi are consistent. With our notations, \ni f v is consistent and C<C then v(c) lp(c ). We remark that IXIB implies ctylf36 and Ya\\66 for any Y \nand 8. The consistency of label\u00adings is a technical condition used to connect labelled terms and the \nequivalence -. 1.6.5. Lemma : If u,v are consistent and if (t,p)~(t ,p ), then p is consistent, Proof \n: by cases inspection on one step of labelled derivation. O 1.6.6, Proposition : If v,v are consistent, \nthen for any d there is one and only one d standard s.t. d= d . P s.t. d-d Hence d~ud by 1.6.3. Now \nsuppose that  Proof By 1.5.4, for any d, there is d Standard d and d are standard s.t. d~pd! . More \nprecisely, let d,d (t,~)~(t ,p ). We prove d=d by induction on <Idl+ d 1, Iltll >. If d=s(~),d =s(~ ) \nor d=c;dl, dt=E;d! the induction works easily by using 1.6.5. 1 Assume now that d=e;dl -and d =s(~ ). \nWe have s=$eO and t=$(~), t =$(~ ). Along d , we get B (C)=~(C). But d:($@),&#38;(T[l/l],~i)~(.t ,u ). \nConsider any c1 in T[~/~1. Either c,ec/s with c ~ internal to some t i and H, (cl)=p(c)lp(s) by the consistency \nhypothesis. Either c is created by E 1 and U(C)<U(CI). Therefore there is no c in T[~/~1 s.t. H1(c)~P(E). \nHence no occurence in t can have M(E) for label, which contradj.cts v(c)=u (s). O 1 1.6.7 Theorem : DI-D2 \niff Dl~pD2 for all U, iff Dl~uD21 fur SoIIIe consistent p,v. Proof : 1 => 2 by 1.6.3, 2 => 3 by instantiation, \n3 => 1 by 1.5.4 and 1.6.5. 0 11. FAMILY OF OCCURRENCES AND f-COl@LETE DERIVATIONS 11.1: Family of occurrences \n: In order to compute in an optimal way, must share duplicated objects (see [13,17]). ring residuals \nalong derivations is certainly necessary, but not sufficient, as shown by the following-example. we Sha- \nLet ~:@l(x)=f(x,x),$2(x)=g($2(x) ). Let d:t=4@$ 2(x))$f($2(x),@ 2(x) )$f(g(@2(x)),g(@2(x)))=t where \nC={(f, l),(f,2)}. The two occurrences CI=(f,l)(g,l) and c2=(f,2)(g, l) in t are nOt resi duals of the \nsame object along d. But it is easy to see that they have to be shared, This can be expressed in two \nways. The first alternative, used by Vuillemin [17], is to share them because they have the same label \nin any labelling of d. Intuiti\u00advely, residuals of shared occurrences are shared, but also occurrences \nwhich are created in the same way by shared occurrences. The second way, used here, does not involve \nlabels. There is a permutation d of d, namely d :t=$1($2(x))$ $l(g($2(x)) )sf(g($2(x)),g(~2(x) ))=t St. \nC =($l, l). Then CI,C2 are indeed residuals of a unique occurrence c=($l,l)(g, l) in $l(g($2(x))), and \ntherefore can be shared in d. The two approaches will be shown equivalent. Thus the behaviour of labels \nwill be explained within the usual formalism. 11.1.1 ,: Notation : From now on, any pair deriva\u00adtion-occurrence \nwritten (D,c) is an abbreviation for D:t% and c in t . We also consider pairs (D,C) when C is a set of \noccurrences in t . 11.1.2. Definitions : We consider the relations S and -(read has for residual and \nbelong to the same family as ) over pairs (D,c) defined as follows : (DI,CI)S(D2,C2) if~D s.t. (DI;D)-D2 \nand c2=cl/D. (DI,cI)-(D2,c2) if ~(D,c) s.t. (Dl,cl)~(D,c) and (D2,c2)~(D,c) or if ~(D,c) s.t. (DI,cI)-(D,c)-(D2,c2). \nHence the second relation is the symetric and transitive closure of the first one. It is straight\u00adforward \nto check that s is a preorder and -is an equivalence. Let us remark that,though we use the same symbols \nfor different relations, the context will always avoid any confusion. 11.1.3 : Proposition : The two \nabove relations are compatible with the equivalence of derivations w.r.t. permutations. Namely, if D1-D~ \nand D2-D~, we have (DI,CI)S(D2,C2) iff (D~,cl)~(D~,c2) and (DI,cI)-(D2,c2) iff (D~,cI)-(D~,c2). Proof \n: obvious since (D~;D)-D~ when D1-Di,D2-D~ and (DI;D)-D2. O 11.1.4 : Proposition : 1) (Dl,cl)g(D2,c2) \niff DlSD2 and c2ccI/(D2/Dl), 2) If DI-D2 and if (Di,Ci)S(D,C) for i=l,2, then C=c,12 3) If (D, ,cl)~(D2,c2) \nand D15DSD2, then there is a unique c s.t. (Dl>cl)~(D,c)~(D2>c2)> 4) If (Di,ci)s(D,c) for i=l,2, then \nthere are a unique c and some D s.t. D -l.u.b. ([Dl],~D2]) and (Di,ci)5(D ,c ).s(D,c) . Proof : obvious \nonce we remark that DI-D2 implies c/D1=c/D2 and that we have by definition c =C if 12 cccl/D and ccc2/D. \nD Consider again the above example. Let dl=(E;(f,l)), d2=(s;(f,2)) and d3=(01 ,1). Suppose c1 ,C2,C have \nthe same values as above, We have (dl,cl)-(d2,c2)-(d3,c), but (d3,c) seems more representative inside \nits family, because d3 does nothing but creating c and all the elementary steps of d3 are contained in \nthe derivations d, and d. 2 11.1.5. Definition : The derivation d generates (d,c) if d=o or d=d,;cl with \ndl generating (dl,cl) and c creating c. 1 11.1.6. Notation : If cl;d =d;c, let ~(d,c)=cl. 11.1.70 Proposition \n: If d generates (d,c), then there are dl and c1 s.t. d=~(d,c).dl and c=~(d,c)cl. Proof : obvious by \ninduction on dl. U 11.1.8. Notation : If d generates d,c) and ~(d,c)ec /d , then let (d,c)\\d =(c .d, \n,c cl) where d=~(d,c).dl and c=~(d,c)cl. This notation makes sense by 1.1.7. 11.1.9. Definition : For \nany pair (d,c), the eano\u00adnzca2 representative of (d,c), written (d,c)o, is defined bv : (@,c)a=(O,c) \n_((c ;;),c)o=(d,c)o\\c if c does not create g((d>c)o) ((c ;d),c)o=((c ;do),co) if (do,co)=(d,c)o and \nC creates ~((d,c)o) This definition means that we extract from d the only steps concerned by the creation \nof c. We first prove that the canonical representative definition is consistent with the equivalence \nof reductions modulo permutations. II. I.1O. Lemma : ((c*;(c2/cl)),c)o =((c2;(c1/c2)),c)o for any cl, \nc2, c. Proof : by cases inspection. D 11.1.1]. Lemma : If %((d2,c)o)=cl, if (dl,cl)o=(dl, c ) and if \n(d2,c)o=(cl .d~,clc ), then ~ ((dl;d2),c)o=((d; ;(c; .d~)),c~c ). Proof : by induction on ldll+ld21. \nU 11.1, ]2. Proposition : If dl-d2, then (d, ,c)o=(d2,c)o. Proof : by induction on the definition of \n1.1.2 u-he two above lemmas. D We now connect families of occurrences and canonical representatives \nin two steps, We first prove (d,c)o-(d,c). In fact we shall have a stronger property which is not necessary \nfor our theorem. Then we show that (d l>cl)-(d2,c2) implies (d1,cl)o=(d2,c2)o.  11. ].13. Definition \n: If D:t&#38;- and c is in t, then c and D are d~s~oint iff D=(Cl;C2; ..,Cn) and C, C i are disjoint \nfor any i s,t, ]Si<n$ Thus C/D={~}. 11.1.14. Lemma : If c,D are disjoint and CICC/D], then c and D/Dl \nare disjoint. Moreover I C1/(D/Dl)={cl} and cl=c/(Dl/D). Proof : by induction on ID], The only problem \nis when D=C. By hypothesis, we have c and C disjoint for any c in C. As residuals of disjoint occurrences \nare still disjoint, one has c and C/Dl disjoint too. I Thus cl/(C/Dl)={cl}, i.e. c cc/(D ;(C/D )). But \nsince 111 Dl;(C/Dl)-C;(Dl/C) and since residuals are consis-\u00ad tent with respect to permutations, we have \nc,cc/(C;(Dl/C)), i.e. ClCC/(Dl/C).O 11.1.15. Lemmas : 1) If ((dl;d2),c2)o=(dl,cl), then c2ecl/d2 .2) \nIf c2ecl/d2, then ((d ;d ) c ) =(dl,cl)o 12 20 3) If (d2,c2)o=(do,co), then ((d.1;d2),c2)o=((d1 ;do),co)o \nProof : by induction on \\dll+ld21. O 11.1,16. Proposition : For any (D,c), there is (Dl,cl) s.t. (D,c)s(DI,cI) \nand (D,C)OS(DI,C,). Proof : Consider any d in D. Then (D,c)o=(d,c)o by 11.1.12. Use an induction on \nId]. If d=o, then (d,c)o=(d,c)<(d,c). Now suppose d=c ;d . By induc\u00adtion there is (D ~,c;) St. (d ,c)&#38;(D~,c~) \nand (d ,c)S(D~,c;). Let (do,co)=(d,c)o and (d~,c~)=(d ,c)o. Let c2=~(d ,Co) and c~=~(dt,c~), o 00 We \nhave two cases. First, if C creates C then 2 do=c ;d~ and CO=C . Therefore, if Dl=cf;D; and o cl=c~, \nwe have (d ,CO)S(D1,CI) and (d,c)s(Dl,c,). o Secondly c does not create c;. Then (do,co)=(d~,c~)\\c , \ni.e. d~=c~,d!!,c~=cjcg and .0 do=c2.d ,co=c2c~ where c~ec2/c . Hence, as c2/c is a set of disjoint occurrences, \nthe derivations are disjoint for all c in c /cJ , But these c; o 2 2 derivations form do/c , Therefore \nd <d /c . More\u00ad00 over c and d =(do/c )/d~ are disjoint. Hence 03 c:ec:/d But by the permutation lemma, \nwe have 3 -d ;(c /do). By 11. ].12, we get ;d:;d3 o ((c ;d~;d3),c&#38;)o=((do;(c /do) ),C ) . By 11.1.15, \none has ((c ;d~;d3),c~)o=(do,co)0a~d c~eco/(c /do). Let d~eD /d , 0, Since (d~>c~)~(D-i>c;)> we have \nc;<c~/di by 11.1.4 and therefore c~<c~/(d;/d3) by 11.1.14 since c: and d are disjoint. Moreover by 3 \n 11.1.14 we get c;~cl/(d3/d;). In short if c =c 11 and D1=c ;D;;(d3/d~), then (d,c)S(Dl,cl) and (do,co)~(D, \n,cl). O 11.1.17. Theorem : (DI,cI)-(D2,c2) iff (DI,CI)O=(D2,C2)0. Proof : The previous proposition implies \n(D,C)O-(D,C). Hence if (DI,CI)O=(D2,C2)0, then (D1,c1)-(D2,c2).Conversely,Suppose(D1,C1)S(D2,C2) By \ndefinition, there is D s.t, (D ~;D)-Dp and c2ccl/D. Hence (DI,CI)O=((DI ;D),C2)0 by lemma 11.1,15 and \n((DI;D),C2)O=(D2,,C2)0 by proposition 11.1.12. Now when (DI,cI)-(D2,c2), we have by transitivity (DI,CI)O=(D2,C2)0, \nn Therefore two occurrences belongs to a same family iff they are generated in a same way. 11,1.18. \nProposition : Let (D,c)o=(do,co). Then do<D iff (do,co)<(D,c). Proof : corollary of 11.1.4 (lst clause) \nand 11.1.16, 11.1.4 (4th clause). O We now show the connection between families and labels. 11.1.19 \n: Definition : Given any term t, the labelling p is elementary if ~:L?(O,t)+E i.e. the range of u is \nthe set of letters E. 11.1,20 : Lemma : For any label o,, if IJ and v are invectives elementary labelings, \nthere is at most one (d,c) s.t. d: (t,p)~(t ,p ), d generates (d,c)and P (c)=a. Proof : by induction \non the length of a. O 11.1.21 : Theorem : Let Di=(t,p)~(ti,Ui) for i=l,2. Then (Dl,Cl)-(D2,C2) iff Vl(cl)=p2(c2) \nwhen p,v are injective elementary iff ~l(cl)=v2(c2) for all ~. Proof : First (DI,cI)-(D2,c2) implies \nPI(CI)=P2(C2) by the definitions. The converse fol\u00adlows from 11.1.20 and 11.1.17. 0 11.2. Complete derivations \n: 11,2.I. Definition : The set of C of occurrences is f-complete with respect to D, in short (D,C) ik \nf-complete, if C={c I(D,c )-(D,c)} when CCC. 11.2.2. Definition : (D,C) is d-complete if C is maximal \nSt. there are D!, D ,C with D-DJ;D I and C=c/D . 11.2.3. Definition : (D,C) is I.-complete if C={C lp \n(c )=U (c)} when CCC aid D:(t,p)~(t ,p ). 11.2.4. Definition : The derivation D is f complete (respectively \nd and !.-complete) if D=@ or D=DI;C when D ~ and (Dl,C) are f-complete (resp. d and g-complete). 111, \nLEAST AND OPTIMAL COMPUTATIONS11.2.5 : Lemma : If D is f complete, then (D,c)o~(D,c) for any c. 111,1, \nSemantics of recursive programs, Proof : by 11.1.18, it is sufficient to prove We define the operational \nsemantics in comple\u00addo-(D,c)o=(do,co). We use an induction on ID1. te F-algebras as in H4,17], The equivalence \nof this If D=@, then do=@~D. Otherwise D=D1;C1 with DI and semantics with denotational or algebraic semantics \nis shown in F14], (DI,C1) f-complete . If cccl/Cl, then 111,1,1, Definitions : ordered sets ,: let (D,c,L) \nbe an ordered set of least element i. ldfien they exists, there is only one c in C which creates c. Let \n(D,C)O=(DI,CI)O and by induction do~D,SD. Else, least upper bounds (l,u,h,) and greatest lower 1 1 bounds \n(g,l,b,) are denoted ctua and aria , A set ACD is directed if Va,ct cA, ~a cA s,t. aca and But , as (DI,CI) \nis f-complete, we get ~!ca!!, and is an ideaZ if it is directed and satis\u00adfies ~aeA,ycx cD, a ca implies \nct eA, An ordered set (d~>c;)=(DI,cl)o. One has do=d~;c~ by 11.1.11,15. (D, ,C1)O=(D1,C2)0 for all C2 \nin Cl by 11.1.17, By D is complete if every directed set AcD has a l.u.b.. induction d~~ . Hence Cl=c~/(Dl/d~) \nby 11.1.18 uA. A mapping h:D+D is monotonic if cxca implies 1 and 11.1.4. So do~D. D if and are complete \nhuh, continuous D D and huh for every directed set AcD, Given an ordered set D, a completion D* a 11.2.6 \n: Proposition : D is f complete iff D is of D is complete set containing D, up to morphism, and such \nthat for d-complete iff.D is k complete when the initial any complete D , every monotonic mapping h:U-+D \nex\u00adlabelings are injective and elementary. tends in a unique way into a continuous mapping hm:DM+D . \nAll completions of D are equal up to uni- Proof : obvious with the above lemma and que isomorphism, As \nan example, the completion by theorem 11.1.21. 0 ideals is the set Dm of nonempty ideals of D, orde\u00ad \n11.2.7. Notation : We write ~>t for an f complete red by inclusion, It contains acD represented by T(ct)={a \neDla ca} , and has {L} for least element, f complete derivation starting at t. derivation from t to t \n, We call FD(t) the set of Every mapping h:P+D extends by hm(J)=u{h(a),acJ}. The completion of (D,<) \nis denoted (Din,<), 11,2.8 : Proposition : The set [FO(t)l=FQ(t)/\u00ad 111.1.2, Definitions : ordered and \ncomplete F-alge\u00ad with the ~ ordering is a sublattice of [~(t)] with the same l.u.b. operation. %ras r143, \nAn ordered (complete) F-algebra is a F-alge- Proof : Let us remark that if (do,co)=(D2,c2)o bra I where \nD1 is ordered, ,has least element L and I and d <D,sD then there is c ~ St. 0 2 the m.4ppings f are \nmonotonic <continuous) , A mor\u00ad (do,co)S(Dl,cl)<(D2,c2 ) by 11.1.4 and 11,1.18. phism of ordered (complete) \nF-algebras is a mor\u00adof F-algebras is monotonic Suppose now that DI,D2 are f-complete, then it phism which \nalso (conti\u00adnuous), Let O be a new constant symbol, let is obvious by induction on ID21 that D1;(D2/Dl) \nMQ(F,V)=M(FU{Q},V) , also written M~. Let < be the is f complete for any D2 with use of 11.2,5. D the \nleast ordering on MQ s,t, Q<a for all a, and + a<; implies f(~) < f(~ ) for all f,~,~ , For exam\u00adple, \nf(~,g(~,~))<f(x,g(~~y) ) holds. Then ~~~ is the consider the f complete derivation ~ associated to d \ndefined by : 11.2.9. Definition : For any d in fl(t), we free ordered F-algebra generated by v, and its \ncom\u00ad pletion by ideals @~,K ,!l,f~> is the free comp2ete 5=0 d;c=d;C where C={c l(d,c)-(~,c )}  F-a2gebra \ngenerated by V, For any ordered (complete) F-algebra I, every This definition does not work nicely with \npermutations, but we will use it for the dis-data mapping v:V+D I extends in a unique way into a cussion \nabout the cost of derivations. It is morphism (I,v):Mti -bI. Intuitively, 0 is the syntac\u00ad straightforward \nto check that d is f-complete, tic undefined, an6 Pla contains infinite terms re\u00ad that d~~ and that D \n= ~ for any dcD when D is f complete, presented by all their finite approximmations. The morphism (I,v) \ngives the value of any term under the interpretation I of the function symbols and the data mapping v, \n111,1,3, Definitions : Gperationa2 semantics : to any tcM=P!(Fu@,V), we associate its syntactic value \n~(t)~MQ : (i) S(xi)=xi (ii) S(f~~))=f(S(~)) for all fcF  (iii) -S($(?))=0 for all $cO Cail program \na triple <X,I,V> there ~ is a scheme on F,O,V, the interpretation I is a complete most if.f every occurrence \nderived in d is outermostF-algebra, and v is a data mapping. Then t $ t im\u00adplies S(t)< S(t ) by induction \non Iltll , and the set in the corresponding term, {.S(t )lt $ t ] is directed inM~ ; its l,u.b. inl~~ \n111,3,2, Lemma : If Z(t)#Q, then there exists a least n20 such that S(En)+Q. Every standard or ou\u00ad is \ncalled E(t), the zhfinite term determined by the program. T?ze value P(t) computed by P on t is termost \nd.so(t) such that .$(d)#!2 begins with En, Mo\u00adreover Z(t)#Q is decidable, P(t)=(I,v)X(t)=u{ (I,v)(S(t \n))lt ~ t }, Remark that Proof : If X(t)#O, there exists d<~(t) standard ~that s(d)#fl. If t=f(?), then \nn=O. If t=$(~), Z(t)=P~(t) where F~=<Z,M~(F,V),vo>, with Vo:xi+xi, every standard or outermost derivation \n,must begin For d:t 1 t , we denote S([d3)=S(d)=S(t ) and with c, and the result follows by induction \non Idl, (I,v)rdl=(I,v)d=(I,v)t , If N denotes the cardinality of 0, then Z(t)#Q is equivalent to S(eN)#Q, \nwhich is decidable (see rlgl). n ~,2. Infinite derivations. Computations, 111.2.1 . Definitions : Let \n@ (t) be the set ~f fi\u00ad111,3.3,Definition : Let tcM and ael~ satisfy nite and infinite derivations from \nt. For 6ED (t). a<~(t). There exists a derivation ~.(t) which is let &#38;k denote the k first steps \nof 6, and let \u00ada least for a, standard, and outermost, S(15)=uk{S(6k)} and (I,v)6=uk(I,v)6k,D efine 6SQ6 \nto proof : by induction on Ilall , For a=!2, then ~=o, For a=f(~), by lemma 111.3,2, every stan\u00ad hold \niff ~k>o, dk >o s,t, 6kS6 k, , and let m be dard d such that S(d)#O is of the form en;d where the associated \nequivalence. Let @ (t)]=~m(t)/-m, sn:t~ f(?) and d =f(d~,d~, . . ..d~) for some let F&#38; ] be the class \nof 6. Clearly, &#38;sm&#38; implies d~~ll(ti). Then by induction, da(t)=cn;f(d~(t)) S(6)<S(6 ), and the \nnotations S([61) and (I,v) [61 make sense. where d=(t) = d (t,), . . ..d (tk) satisfies the al ak conditions. \nP111.2.2. Proposition : The structure <@(t)l,<m~ is a completion of <rU(t)],<>, and is a complete We \nnow extend this result to infinite deriva\u00ad lattice. tions. Notice that 62d holds iff ~6 s,t. 6wd;6 . \n Proof: Notice that [~m(t)l contains r~(t)] and that th? relation <m restricts to S on [Q(t)]. 111,3.4.Rotations \n: Let tcM and AEY~ such that Moreover, every [61 is the limit of the directed set { [6k]{k~O} c [~(t)]. \nThis completes the Proof of .. A+Z(t). Let S={anln2d}cMn be an increasing chain the first part. Using \nthe completion by ideals, it of limit A. Let 6S(t)=d (tO);da(t );...;? (tk-l) ;.,, is easy to see that \na completion of an upper-semi\u00adal21 ak lattice is a complete lattice. ~ where to=t and d (ti_lj:ti_l%. \nis constructed 1 aiwe can therefore abbreviate s and -m by < . as in 111.3.3. Let d~(t)=d (to) ;da(t \n) d (t and -, z 1  akk-l) al 111,2,3, Definition : Let the fuzz derivations Ki(t) be defined by \nKo(t)=@:t&#38;=to a~.d K i+,(t): t ~ ti:l IIT.3.5.Theorem : If S is a chain of limit A, then sot. K.~+}(t) \n= Ki(t);C(@,ti). Let K(t) be the 8S(t) is least for A and also for any A s.t. infinite full derivation. \n A<A <S(6s(t)). 111,2.4, Proposition :lX(t)]=U~rn(t)] Proof : Let 6 such that S(&#38;) >A!,and assume \nby induc\u00ad proof : by induction on [d!, show that TdlSFKldl (t)] ,, tilxlM:(t). Then d-d~(t);d for some \nd , Since @ . S(6 )>A, there exists some d <b such that .S(d )~ak+l 111.2,5. Definition : Let p Fe \na program and t be an initial &#38;erm, A computation of P on t is a deriva-Then 6 >d 2d (tk) implies \n6>6~(t);d (tk)=8p[t) .0 tion 869 (t). It is a computation of acD iff ak+ ] ak+l (I,v)d=xx. A computation \nis correct if itlcomputes Notice that a least syntactically correct deri\u00adP(t), syntaetica22y correct \nif it computes ~(tl in vation can he constructed from the chain .$(Ki(t)). : S(&#38;)=z(t), A computation \n6<&#38;(t) is Zeast for % 111,4 Optimal computations in $, CJCD if (I,v)6>cY and (I,v)6 oa implies 8 \n26 for From now on, we denote hy ~cFU(t) a f-complete eve;v ~ e~(t). derivation, and by %eFfl (t) a \nfinite or infinite Every syntactically correct derivation is cor f complete derivation (Definitions 11.2.4 \nand rect, The full derivation is correct by 111.2,4, 111.2.9 trivially extends to infinite derivations), \nThe cost of a derivation dc~(t) is its length 111.3 Least Computations in }~~. Id 1. But, as noticed \nbefore, a simple sharing tech\u00adnique applied to f-complete derivations allows to We restrict our attention \nto the symbolic in-derive all occurrences in a complete step at cost 1: terpretation Mmand construct \na least computation let X:$(x)=f(x,$(x)) ~, lf\\ of everv A<~(t). 111,3,1. Definition : A derivation dcl?(t) \nis outer\u00ad The cost of a f-complete derivation ~ is then the number 1~1 of nonempty set derived. This \nnotion of cost does not extend to infinite derivation ; we shall use a finer way of comparing derivations. \n111.4,1, Definition : Given tclf and d<u(t), the ~CZ\u00admz Ly F (d ,C is the equivalence class of (d ,c \n) such that d co(t) and (d,c)-(d ,c ), For d=cl;c2;.. .;ck, let di=c1;c2; ,,,;c. and define 1 F(d)={F(di,c \ni+l)lo~i<k}~ Let F(~)=~iF(~i), and let F(a) and F(3) be similarly defined. 111.4,2 Lemma : Let ~=Dl;Cl \n;D2;C2;D3cF21(t). Then F(DI,CI)+F((DI ;CI;D2),C2). Proof : Obvious from 11.2,5 and 11,1.18, n ~~efore \n1~1 is the_cardinality of F(a), and F(~)cF(d ) impli~s l~l<ld l,,We can_compare-infinite derivation by \nF(6)cF(6 ) for cost (6)<cost(6 ), 111,4,3. Definition : Let tc?4 and AeM~ such that A<Z(t), Then ~eFV(t) \nis opt~maZ for A i: it comp\u00adutes A and satfsfies F(6)cF(6 ) for any 8 ~FT(t) which computes A. We now \ntransform minimality results into opti\u00admality results, 111.4,4, Definitions : A derivati~n ~~FOm(t) is \noutermost if each set derived in 6 contains an ou\u00adtermost occurrence in the corresponding term, 111.4.5, \nLemma : Let c he outermost, assume ~ 2D;c, Then F(D,c)EF(~ ) Proof : We have c/(~ /D)=@. Then ~ is of \nthe form 1) ;C;D with cEC/(D/ti ~, otherwise c/(~ /D)={c}. Therefore ~(D,c)=F(D ,C) cF(d ). n 111.4.6. \nLemma : Let dc~(t), ~ eFQ(t) with d outer\u00ad most. Then ~ ~d implies F(d )ZF(d), Proof : Induction on Id] \nand lemma 111.4,5, Cl 111.4,7, Lemma : Let dell(t) outermost. Then ~ is ou\u00adoutermost and Ffa)=F(d). \nProof : Induction on Id] ,Let d=d ;c and ~=~ ;C, If c/(~ /d )=fj,then C=@ by 111,4,5 and 111,4,2. Otherwise \nc/(~ /d )={c} and CCC is outermost, So ~ is outermost, Now F(3)cF(d) and F(d)cF(d) follow by 11,2,9 and \n111,4.6, D 111.4,8. Proposition : Let ~,~ cFO(t) with ~ outer\u00admost. Then ~<~ holds iff F(~)cF(~ ). Proof \n: Induction on 1~1, Let ~=~l;C with CCC outermost, If Z<i , then F(~l,c) c F(~ ) by TII.4,5 and F(~)cF(~ \n) holds by induction, If F(d)cF(~ ), we get C/(~ /d)=@J by 111.4.5 and 111,4,2, and Z<Z1 hy induction \nand 1,4,2, n 111,4,9, Proposition : Let dcom(t) and ~ cFOm(t) with d outermost, Then 6<~ implies ~s~ \nand F(6)cF(: ). Proof : By definition of <, 111,4.6, 111.4,7 and 111,4.8. II 111,4,10. Theorem : Let \ntcM and AcM; such that A<X(t), Let S=~an,n21} be a chain of lj.mi.t A, let A eM~ be such that A<A <~(~S(t)). \nThen ~S(t) is least for A and A in ~~m, and is optimal for A and A . Proof:If ~(~)~A ,then S(~)>A and \n&#38;dS(t) by 111.3.5.But 8S(t) is outermost,Hence %~s(t) and F(~)>F(~Jt)) followby 111.4.9.0 As before, \nnotice that an optimal syntactical\u00adly correct derivation is constructible from S?(Ki(t)).  111.5. Least \nand optimal computations in interpre\u00ad tations. m Our results in MQ extend to interpretations, provided \nthe following condition is satisfied. 111,5,1, Definition : An interpretation I is pro\u00adjective if for \nevery V:V+D1 there exists a mapping :DIxM~+M~ such that C-l.v cT v(a,T)=g.l.b.{T <Tl (I,v)T >a} for \nac(I,v)T. Therefore computing a exactly means computing C~ v(a,Z(t))in ~ , Define optimaz for a as in \n11+,4.3. 111,5,2, Theorem : Let I projective and v:V+D Let 1 ~cp(t), let S be any chain in MQ of limit \ncl v(a,Z(t)), let ct ,a s.t, aca c(I,v)(dS(t)) and ac~ c(I,v) (~S(t)), Then 6S(t) is least for a and \na m in ~ (t), and ~S(t) is least and optimal for u and a in Flf (t). Proof : From 111,3,5 and 111,4,10, \nusing c1~,X(t))<S(6S(t) ) and C1 (a ,Z(t))<S~S (t))99 which trivially follow from the definition of c \n,0I,v Notice that when (I,v) is algebraic (see [51), the projectivity condition can he restricted to \nhold only on finite terms. Example : Non-projective interpretations typically - parallel functions as \nthe parallel or : 1 OF fit%t OF l%t and C1 V(tt,tt or tt) does not , exist.We give without proofs the \nfollowing examples of pro+ecti,7e i.nte.rmr~r.stinns . -StaDLe znteppretatzons [l,2] : write ~+~ if ~~ \ns.t. ~c$ and ~ +~ , Assuming continuous g.1.h. exist in PI, then I is L stable if every f satisfies Ijl,l \n, ii+; * fl(lnl )=fl(l)nfl(l ). Notice that M; is stable. -Sequential interpretations : several non \nequivalent definitions exist : a~=ai implies f(l )=f(l). No algebraicity condition is needed on D1.Here \nM ;.s not Seq!lem.t;al. Q -Mi2ner[111 : Either fl is con@tant, Or ai s.t. ai=l implies f(;)=l , and St. \nAQ ,a , ,a. >ai+l> fl(~) is sequential 12 1-1 ak step. In fact, in such sequential interpretations, \nfor each a.. Same situation as with Vuillemin s de-the computed values form a ch~in, and a stronger 1 \nfinition. optimality result holds : if 6 is progressively optimal, then for any ~ there exists izo such \nthat -Kalvz-,?zotkinr81 : These interpretations are (Z>v)~i~(I,v)~ and F(~i)cF(~). (see [171). Such an \nstable, and then projective.Here M; is sequential. optimality can not be expected if non comparable values \ncan be computed, as it is the case iri Mm, in Q  111.6. Progressively optimal computations and stable \ninterpretations or in Kahn-Plotkin sequen\u00adcomputation rules, tial interpretations. The theorem 111,5.2. \ndoes not suggest a prscti\u00ad cal way of performing optimal computations : we do nat want to first compute \nin trees and then adapt this computation in pointer structures, CONCLUSION ?{oreover we are not only \ninterested in the re\u00ad sult of an infinite computation, but also in the in-Under some fair conditions, \noptimal compu\u00ad termediate results i! produces (see [173), Call information step of 8 any &#38;O such \nthat tations of recursive programs have been studied at a purely syntactical level. Then by considering \nmi\u00ad(I,v)~i~(I,v)8i-1 . .. nimal derlvatlons, we left the Herhrand universe and dealt with the following \nproblem : how to re\u00adlata the < ordering on derivations to the cost of 111;6,1, Definition : A computation \n~ is ~rogres\u00ad sZVeZy optzma2 if ~i is optimal for (I,v)di at derivations ? This has been done here with \nuse of each information step i , outermost derivations which derive only crucial We give without proof \na result in [31, Define occurrences. Intuitively, any true permutations the covering relation~in M by \na~a if a@ <a of an outermost reduction could have worked, i.e. Q permutations which only use the permutati-on \nlemma implies a =a . Then a is obtained from a hy.~epla\u00ad, These derivations are cing an Q hy some f(fi). \nConstruct now t. and D in 1 hen c ~!+oslml ar to thec2 c +o computations in r167 very and safe the following \nway, hy a slight extension of 11.2.9 : when we consider a covering approximation step, although the safe \nderivations are defined in some (i) to=t, 5.=0 more semantical way, The securety conditions in~l are \ndirectly related to our s ordering since secure derivations are dominating the safe paral\u00adlel outermost \nderivation, Progressively optimal (ii) Fli+l =~i;da (ti):t $ ti+l for some af such that c ~,$( ~v)ti,ti~<ai<cl, \n$p(t), (t)) if s@ih an derivations seems interesting when values of deri\u00ad a.1 exists,5. 1+ 1 =~i otherwise. \nvations can he infinite objects as in [83 where coroutines are evaluated with a delay rule mecha\u00ad 111.6.2.Theorem: \nThe computation ~ is progressively nism, Finally, nearly all propositions and theo\u00ad optimal. rems in \nthis paper seems also true in the k-calcu\u00ad lus , This indicates that an axiomatized approach The covering \nai is used to designate some oc\u00adcould be taken (see [ 15]) although it seems to curence of $i in t. to \nhe suhsti.tuted, The cons\u00ad 1 the authors that such an approach could not bring truction of ~ can be \ndone directly in pointer struc\u00ad simplifications. tures. Let C(T)=cl v ((I,v)T,T).It is possible to construct \na pr~gressi.vely optimal correct compu\u00adtation (which is also optimal) in the following way: REFERENCES \nstarting from ti, perform full derivations to reach t such that (I,v)t n (I,v)t<, then choose a suita\u00ad \n[1] G, Berry, Bottom-up Computations of Recursive ble ai such that c(ti)~ai<clt ), and apply (ii) to \nProg?ams. R.A,I.R,O, Informatique Th60rique, Vol 10, n03, Mars 1976,reach t. Notice that optimal computations \nmay be 1+1 pp.47-82. finite, which is undecidable in general (but not in MM). In this case, our rule \nloops performing [21 G. Berrv and B. Courcelle, PPO~T ~ EquiVa- Q Zence and CanonicaZ Forms &#38; Sta\u00ad \nfull substitutions from some ti. bZe Discrete Interpretations. Notice also that this ~ rule is of no \npracti- Automata Languages and Programming, cal interest : we first perform full derivations, 3rd Coil, \n1976, Edinburgh University and then a posteriori select the useful steps. What Press, U.K. p,168 188. \nwe really need is a way of selecting ai~c(ti) from [31 G. Berry, CaZeuZs Minimaux et Gptimaux des c(ti), \nThis is the purpose of the sequentiality de-Programrnes R&#38;ursifs, to appear. finitions : Vuillemin \ns and Nilner s definitions imply ~acM~, ja >a s.t, ~A<M~, A~a, (I,v)AP (1,~}.a [41 A.Church, The calculi \nof Lambda Conversions, Annals of Math. Studies,nOG, implies *a , and a can be constructed from a. Princeton, \n1941 Jlyule!l In this ease, ouv reducee to Vuilleminl~ delay rule , Notice that when optimal computations, \n [5] B. Courcelle and M. Nivat, AZgebraic Families are finite, the delay rule loops producing useless \nof Interp~etat<ons, 17th FOCS, steps instead of looping in searching for the next Houston, 1976. r6] \nH.B.Curry and R.Feys,Combinatory Zogie,Vol 1, North Holland, 1958 r7 1 P.J.I)owney and R.Sethi,Correet \nComputation Ru2es-for Recursive Languages,-16th Annual s~posium on Foundatio~s of Computer SC, Berkeley,Oct. \n1975. [8 1 G.Kahn and G.plotkin,Concrete Data Types)To appear,tJniversity of Fdinburgh)A. I,Dept. r8bl \nG,Kahn and D.Mc Oueen,Coroutines and Networks of ParaZ~e~ Processes,To appear as IRIA-LAME?IA report. \n[9 1 J-J.L6vy,An a2gebraie interpretation of the A(3K-calculus and a Labelled ~-calculus, Proc,of the \nSymposium on -calculus and Computer SC Theory,Roma, Italy,March 1975, rlol J-J,LGvy,R&#38;ductions si?res \net optimales duns le A-ealcul,,To appear. rlll R,)~jlner,Fulzy Abstract Models of Typed A-eal\u00adculi,To \nappear in Theori.tical Computer SC. r127 G,Mitschke,The Standardisation Theorem for i-calculus, Swansea \n1975, r131 C,Montangero,G,Pacini and F.Turini,Graph Re\u00adpresentation and Computation Rules for TypeLess \nRecursive Program Schemes,Proc, 2nd Colloa.Automata,Languages and Program\u00adming,Saarbr~cken(l 974),Lect,Notes \nin Comp,Sc. 14. r141 M,Nivat,On the Interpretation of Recursive Program Sclzemes,Symposia Yathematica, \n[7.1 xv,Instituto Nazional.e di A1.ta Mate\u00ad matica, Italy,1975,P, 255-281 , r157 B,Rosen,Tree Manipulation \nSystems and Church-Rosser Theorems,JACM Vol 20 nO1 ,1973. r161 J),Scott,Gutline ofa Matlzematica2 Theory \nof Computation,P,R,G,mono,N 2,0xford 1970, r]77 J.Vuillemin,Proof Techniques for Recursive Programs,Ph.J), \nComp,Sc,Dept,Stanford(1973) r181 J,Vuillemin,Syntaxe,S 6mantique et Axiomatique d un Langage de Programnation \nSimp2e,Th?se d 6tat,lJniv.Paris YTTtl?74  \n\t\t\t", "proc_id": "512950", "abstract": "<p>Procedure call mechanisms have mainly been studied in theframework of recursive programs without assignments, for thesimplicity of their operational and denotational semantics (SeeScott [16], Nivat [14], Vuillemin [17]).</p><p>According to operational semantics, procedure calls act astextual rewritings ; \"computation rules\" select at each computationstep the occurrences of unknown functions to be rewritten. Acomputation rule is called <i>correct</i> if the value it computesis the one given by the denotational semantics. Correctness andefficiency of computation rules have been studied in Vuillemin[17,18], Montangero-Pacini-Turini [13], Downey-Sethi [7]. The mainresults are well-known: innermost evaluation (call by value) is notcorrect, parallel outermost or full substitution are correct.Vuillemin [17,18] gives a sufficient condition for a rule to becorrect, (safety), later extended by Downey-Sethi [7] into anecessary and sufficient one (security). Vuillemin also studiesparticular implementation of call-by-name, the <i>delay rule</i> ;he shows its <i>optimality</i> with respect to a reasonableimplementation cost, provided interpretations satisfy a<i>sequentiality</i> condition. This result is in fact twofold :sequentiality allows elimination of <i>useless steps,</i> andoptimality follows by using <i>sharing mechanisms</i> in termimplementation.</p><p>The basis of all these studies is the following theorem(Vuillemin [17,18]) : provided some restrictive conditions onprograms are satisfied, the set of terms derivable from a giventerm is a lattice under the derivation ordering.</p><p>Our aim is to extend these results, for the three followingreasons : first, though every program can be transformed to matchVuillemin's conditions, the transformations may affect the costs ofcomputations : a more direct proof can be investigated. Second, adirect generalization to the &#235;-calculus is notstraightforward, since &#235;-terms definitely do not formlattices. Third, the symbolic (or Herbrand) interpretation [5] isnot sequential in the sense of Vuillemin, and no optimality resultis known for it.</p><p>Our point of view will be purely syntactic : we reconstruct thelattice property in derivations. We study minimal computations(i.e. finite or infinite derivations)in the symbolicinterpretation, and transform them into optimal ones. Eventually,we characterize interpretations to which similar results apply.Extension towards the &#235;-calculus is done in [10].</p><p>The lattice property of terms breaks down in general : two verydifferent derivations may lead to the same term by syntacticalaccidents, which collapse two a priori different terms. In sectionI, we take care of this fact by introducing an <i>equivalence and apreorder on derivations.</i> We give three characterizations ofthese relations. For the main one, we extend the classical notionof residuals [4] by defining <i>residuals of derivations byderivations.</i> We show that derivation classes form alattice.</p><p>In section II, we study the \"simple derivations\" defined byVuillemin with use of labels (named here <i>completederivations</i>). We give two characterizations of them in theusual formalism.</p><p>Section III, is devoted to minimality and optimality results.Ordering infinite derivations as well as finite one, we construct<i>least</i> computations of every syntactic approximation of theinfinite tree determined by the program. The associated completederivation are <i>optimal</i> with respect to Vuillemin's cost.Extension to interpretations is then straightforward, as soon asthey satisfy a syntactic condition. All classes of sequentialinterpretations considered in [2,6,11,17] do satisfy thiscondition. The \"computation rule\" we use for constructing theoptimal computations is very inefficient in general, but reduces tothe usual delay rules in sequential interpretations.</p>", "authors": [{"name": "G&#233;rard Berry", "author_profile_id": "81100006256", "affiliation": "Ecole des Mines de Paris and IRIA-LABORIA, 78150-Le Chesnay, France", "person_id": "PP14015037", "email_address": "", "orcid_id": ""}, {"name": "Jean-Jacques L&#233;vy", "author_profile_id": "81100154846", "affiliation": "IRIA-LABORIA, 78150-Le Chesnay, France", "person_id": "PP31029729", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/512950.512971", "year": "1977", "article_id": "512971", "conference": "POPL", "title": "Minimal and optimal computations of recursive programs", "url": "http://dl.acm.org/citation.cfm?id=512971"}