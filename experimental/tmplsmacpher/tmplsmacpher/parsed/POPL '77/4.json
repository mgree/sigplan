{"article_publication_date": "01-01-1977", "fulltext": "\n Permission to make digital or hard copies of part or all of this work or personal or classroom use is \ngranted without fee provided that copies are not made or distributed for profit or commercial advantage \nand that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, \nto post on servers, or to redistribute to lists, requires prior specific permission and/or a fee.&#38;#169; \n1977 ACM 0-12345-678-9 $5.00 Applications of High Level Control Flow Barry K. Rosen Computer Sciences \nDepartment IBM Thomas J. Watson Research Center Yorktown Heights, New York 10598 ABSTRACT. Control flow \nrelations in a high level language pro\u00adgram can be represented by a hierarchy of small graphs that com\u00adbines \nnesting relations among statements in an ALGOL-like syntax with relevant perturbations caused by goto \nor leave statements. Applications of the new style of representation include denotation\u00adal semantics, \ndata flow analysis, source level compiler diagnostics, and program proving. 1. INTRODUCTION Of course \nwe can describe control flow in a program by tracing a path in the control flow graph of the program. \nBut what is the graph of a program in an ALGOL-like high level language? How is it related to the source \ntext? Can goto or leave statements be handled smoothly? How can sets of paths that are similar from the \nuser s viewpoint be concisely described? Suppose the program is changed. How can we update the graph \nand the results of analy\u00adsis by a program prover or an optimizing compiler? Must we start from scratch? \nQuestions like these have led to a new representa\u00adtion of control flow in high level language programs. \nSection 2 reviews the standard low level flowchart representa\u00adtion of control flow and contrasts it with \nthe use of a high level contro[ flow graph and reduced graplrs. Some applications of the new style of \nrepresentation are sketched in Sections 3-7. Section 3 shows how denotational semantics can cope with \ngoto while avoiding continuations. Section 4 globalizes local data flow -in\u00adformation and computes it \nbottom-up. Section 5 applies the new local information to global problems. Section 6 applies the previ\u00adous \nsections to the problem of generating concise but informative compiler diagnostics at source level [F076; \nOF76]. Section extends Owicki s method for proving partial correctness of asynch\u00adronous parallel programs \n[OW75; OW76; OG76] to cope with goto and leave. In the special case of sequential programs, a similar \nbut less general trick is proposed by Wang [Wa76]. High level control flow transcends the restriction \nto classical structured programming that formerly limited the usefulness of axiomatic systems for pro\u00adgram \nproving in the style introduced by Hoare [H069]. (The re\u00adstriction is only partially relaxed in [CH72].) \nTwo principles that have guided this work should be men\u00adtioned. The first is that COMPUTERS ARE NOT PEOPLE: \nthere is little reason to expect computer processing of large structures to be expedited by representations \nthat are natural when people process small structures by hand. For example, compare infix with postfix \nnotation. In this example computational simplicity corre\u00adlates poorly with intuitive naturalness, but \nit correlates well with mathematical elegance in the rules defining the formal notation. We have chosen \nto relate program texts to graphs in a way that is mathematically elegant (though perhaps unnatural for \nhand proc\u00adessing of small programs) in the hope of expediting computer processing of large programs. \nThe example program SUMFAC in the next section is small enough for hand processing and is just barely \nlarge enough to illustrate the points we wish to make. (Another program in this size range is cited in \nSection 7.) As is shown in Section 6, high level control flow is convenient for com\u00admunication between \na computer and a person as well as for proc\u00adessing by a computer alone. The second principle guiding \nthis work is a cautiously optimis\u00adtic version of Murphy s Law: WHATEVER CAN GO WRONG WILL, BUT NOT OFTEN, \nIn particular, well written programs in well designed languages will have few if any goto statements, \nbut even good programmers will use them now and then [Kn74]. We conclude that methods for proving or \noptimizing programs should be able to handle any use of goto, but that this ability should not contribute \nto the processing costs for the many programs free of goto. Moreover, the complexity of our methods should \nrise gradu\u00adally when anomalous statements are added to a large program. There should not be a drastic \nchange when a single goto or leave is added to a large program that previously used only single entry \nand exit control facilities. Such robustness is acheived by a mathemati\u00adcal approach that superficially \nresembles the informal control flow diagrams of [DDH72, Sec. 1.7] for classically structured programs \nbut also copes with nonclassical control flow. Ledgard and Mar\u00adcott y [LM75, Sec. 6] challenge our assumption \nthat classical struc\u00adtured programming is sometimes inadequate. Space does not per\u00admit a full discussion \nof the issues here. See [DEL76] for some recent results that support our assumption 2. ASSIGNING GRAPHS \nTO PROGRAMS The usual low level flowchart representation [U173, Sec. 1] is constructed by translating \na program into an intermediate text. Unlike the statemerrls of an ALGOL-like language with structured \nprogramming facilities, the instructions in intermediate text cannot be nested within each other. Certain \nsequences of instructions called basic blocks are grouped together to form the nodes of the graph. This \nrepresentation is a hierarchy with a graph at the top level and sequences of instructions at the level \nbelow. An example is shown in Figure 1 for a program SUMFAC written in ALGOL-Iike notation (supplemented \nby line numbers) atop the next col\u00adumn. Unlike the usual shallow hierarchy, the high level representa\u00adtion \nproposed here has the same depth as the depth of nesting of statements with~n statements in an ALGOL-like \nsyntax. AU places in the hierarchy are held by graphs, and each graph represents a portion of the control \nflow information in the program. These 01 SUMFAC: begin dcl MS, S integer external; 02 comment S will \nbe assigned the sum of the factors of MS; 03 dcl M, P, T, Q, R integer; 04 M:= MS; S:=l; P:=l; 05 NEWPRIME: \nP:= P+ l; T:=P; 06 AGAIN: Q := quo(M, P); R := rem(M, P); 07 TEST: ifR=O 08 then ZERO: [ T := mrrl(T, \nP); M := Q; MORE: goto AGAIN] 09 else NONZERO: 10 [CON1: if T # P then S := mul(S. quo(T-1, P-1)); 11 \nSEMICON: if P>Q then 12 PGQ: [CON2: if M >1 then S := mrd(S, M+l); 13 S:= S MS; 14 DONE: leave sUMFAC]; \n15 ANOTHER: goto NEWPRIME] 16 end - . ..... . graphs are related to each other by means of the high \nlevel corrfrol flow graph for the entire program. This graph is used by the theory to relate high level \nanalysis to concepts usually defined only for low level flowcharts. The smaller graphs in the hierarchy \nare called induced graphs. Semiformal descriptions of the high level control flow graph for a program \nare in [Ro76a, Sec. 2; R077, Sec. 2]. Induced graphs are constructed more formally in [R077, Sec. 4]. \nHere the presentation is rigorous and significantly more gener\u00adal. The sets (2.2. 1) later in this section \nare required to be single\u00adtons in [R077]. Let G be a finite directed graph with a set NG of nodes and \na set AG of arcs. An arc c has a source sc and a target tc, both of which are nodes. We identify c with \nthe pair (SC, tc). A nesting strucfure for G is a finite partially ordered set Z with a maximum m, together \nwith a set Na of nodes and a set Aa of arcs for each a in X. The following properties are required: Nrr \n= NG and AT = AG; (2.1.1) ~ < a in X implies (N8 s Na and A~ s As); (2.1.2) (c in AG has SC, tc in Nri) \nimplies (o is in As). (2.1.3)  For SUMFAC we take 2 to be the set of all <statement> nodes in the parse \ntree, with ~ < a iff tree node ~ is a descendant of tree node a. We deliberately confuse statement labels \nwith the tree nodes they identify, so that SUMFAC, TEST, ... are said to be in 2, with SUMFAC = rr and \nZERO s TEST < SUMFAC. For each a in X for SUMFAC, let Na = { enrering a, leaving a 1 u U ~<aNP and construct \nappropriate arcs. For example, there are arcs from entering TEST to eniering ZERO and to entering NONZERO, \narcs from feaving ZERO and from leaving NONZERO to leaving TEST, and an arc from entering MORE to entering \nAGAIN, The example of (2.1) just sketched for SUMFAC is the high level control flow graph for SUMFAC, \nwhich combines syntactic nesting with the details of control flow. A nesting structure wifh entrances \nand exits consists of G and Z as in (2.1) together with, for each a in X, sets of designated entrances \nand designated exiis DENTRtr s Na and DEXITa s Na (2.2.1) such that, whenevek /3 s tr in 2, DENTRa n \nND s DENTR~ and DEXITa n ND s DEXIT/3. (2.2.2) Entrance and exit sets are defined by ENTRa=DENTRa u{tc \nENrslc&#38;AG &#38;.sc GNa]; (2.2.3) EXITa=DEXITau {SC EN. I cCAG&#38;~tc CNa}. (2.2,4) For all statements \nin SUMFAC we take DENTRa = { entering a ] and DEXITa = { leaving a }. Other entrances and exits are added \nby escape or jump statements such as the leave (used as in [WU75, p. 145]) and the goto s in SUMFAC. \nWe can find ENTRtr and EXITa without constructing G: all we need is the parse tree and the ability to \ncorrelate the label in goto NEWPRIME with NEWPRIME as a member of 2. Given a nesting structure with entrances \nand exits, consider any a in 2, n in ENTRa, and p in EXITa. Let II(a, n, p) = 1 if there is a path from \nn to p in G touching only nodes of Na (2.3) and If(a, n, p) = O otherwise. These path bits can be computed \nbottom-up, beginning with choices of a that are minimal in >. Path bits for a can be determined from \npreviously computed path bits for parts P of a, where PARTa={ /3<al Noyin2has~< y< a}. (2.4) For exampIe, \nPART. for a = TEST is { ZERO, NONZERO ]. As in [R077, Sec. 4], we construct the induced graph Ga for \neach a in 2. The set of nodes in Ga is Nc% = No. U U ~ ~~A~~a (ENTR8 u EXITP) (2.5.1) where (2.5.2) For \nSUMFAC, Noa = { entering ~, leaving],Theset a ofreal arcs of Ga is RAGa={c CAalsc, tc~NGa}. (2.5.3) \nFor each ~ in PARTa there is a set of imaginary arcs IAG[/3] = {(n,p) InEENTR~ &#38;pEEXIT~ &#38;II(P, \nn,p) = 1}. (2.5.4) The total set of arcs is (2.5.5) G = AGa u U Pe PART. IAG4BI with the obvious definitions \nof sources and targets, (Some arcs are both real and imaginary.) Let al be the statement CON1 with one \npart ~1 . Then Ga ~ is shown in Figure 2 for i = 1. Now let a2 be CON2, so that Ga2 is shown in Figure \n2 for i = 2, More generally, if a is any one part conditional statement if.. then ~ in any program, and \nif ~ has only entering ~ as an entrance and leaving p as an e~i~, ~h~~ Ga is as shown in Figure 2. Similarly \nfor the other control structures of classical structured programming [DDH72, Sec. 1.7]. Escapes like \nleave and jumps like goto may be used within /3 or elsewhere in the program: Ga only depends on the relation \nbetween a and its parts. The induced graph construction automatically determines whether Ga is like the \nsimple diagrams of [DDH72, Sec. 1.7] or whether the should be possible to assign meaning bottom-up in \nan almost escapes and jumps in the program are relevant to cr. For example, G. for a = SEMICON is as \nshown in Figure 3 Figure 4 shows Grr for n = SUMFAC. and it is roughly twice as large as Figure 1. But \nFigure 1 presupposes the control flow analysis among instructions that identifies basic blocks and deletes \nanything not reachable from the entrance to SUMFAC. Similar analysis of Figure 4 leads to the compressed \ninduced graph shown in Figure 5. In general, whenever an induced graph is unpleasantly large, we can \ngo beyond [R077] by compressing it according to rules like those used to construct low level flowcharts. \nThe formal construction follows. Given a nesting structure with entrances and exits for a graph G, consider \nany a in Z and compress the induced graph Ga by applying rules until no further changes are possible. \nIf n is a node in Ga EXITa not reachable from at least one entrance to a, then delete n and all arcs \nc with sc = n or tc = n. (2,6) A node n in Ga is flow trivial iff both . n E ENTRa u EXITa (2.7.1) \nand there are unique arcs x, y with tx=nandsy=n. (2.7.2) If n is flow trivial then we bypass it: delete \nn, x, y and add an arc z from sx to ty. (2.7.3) Iterating the bypass operation (2.7 ) is like grouping \ninstructions into basic blocks. 3. DENOTATIONAL SEMANTICS It is natural to think of a program statement \na as meaning a (partial) function from storage states to storage states. In classical structured programming \nthe meaning of a is quite elegantly deter\u00admined by the meanings of its parts and by the production applied \nto generate a. The classical case is important but not universal: prac\u00adtical structured programming [Kn74; \nWU75; Za74] requires escapes and perhaps a few jumps. In a large program with a few jumps it classical \nway, without a major change in semantic style such as introduction of continuations [SW74]. Such a robus[ \nsemantic style is easily acheived with the help of Section 2, The key is simple: instead of asking what \na means, we ask what (a, n, p) means, where n is an entrance to a and p is an exit from a. In practical \nstructured programming the number of such triples is often larger than [ X [ , but not drastically larger, \nFor many of these triples the meanings follow from elementary obser\u00advations. In general, H(:, n, p) = \nO implies that (a, n, p) computes the empty map. In many programming languages the syntax is such that \n(P C EXITa and SC = p) implies ~ tc E Na, (3.1) so that (a, p, p) computes the identity map if p is \nin ENTRa n EXITa. In particular, only when H(a, n, p) = 1 and n # p is there any question about the meaning \nof (a, n, p) in SUMFAC. The 23 statements give rise to 24 triples in need of analysis here. Now consider \nany graph G together with a storage state trans\u00ad formation for each arc and a nesting structure with \nentrances and exits that satisfies (3. 1). We assume determinism: arcs with a common source are assigned \ntransformations with disjoint domains. Given the induced graph Ga, the transformation for each real arc \nin Ga, and previous] y determined meanings for all triples (/3, no, Po) such that ~ is a part of a, we \nfind that the meaning of (a, n, p) as a storage state transformation is determined. The usual least fixpoint \nconsiderations are required if Ga has cycles. Transforma\u00adtions are partial functions and, when ordered \nby inclusion between functions as sets of ordered pairs, have appropriate completeness properties. See \n[Ma74, Ch. 5; MR76] for details on the mathemat\u00adical machinery. In particular, the partial functions \nform a coherent [MR76, Def. 5.7] poset. 4. LOCAL DATA FLOW ANALYSIS For analysis to support a correctness \nproof or an optimization it is important to compute meanings that are much simpler than storage state \ntransformations. For example, we could let the mean\u00ading of (a, n, p) be the set of variables whose values \ncan be modified by an execution of a that euters at n and leaves at p, where can refers to all coutrol \nflow paths. Calf this set MOD (a, n, p). The previous section applies equally well to this kind of meaning. \nMore\u00adover, consider CON1 and CON2 in SUMFAC with induced graphs shown in Figure 2. It is easy to show \nthat MOD(ai,entering ai, leaving ai)= MOD(~i, enterirzg~i, leaving ~i). (4.1) The work of deriving (4.1) \nneed not be repeated for i = 1, 2 in SUMFACand, foreach program. Atproof time orcompile timewe need ouly \nrecognize that a: if...then ~ is a one part conditional statement with a related to ~ in the manner of \nclassical structured programming. Then the meaning of a can be derived from the meaning of ~ by plugging \ninto the equation MOD(a, entering a, leaving a) = MOD(~, errtering~, leaving~) (4.2) derived at the \ntime of language definition, when we draw Figure 2 without the i subscript. Now consider SEMICON in SUMFAC \nwith induced graph shown in Figure 3. Escapes and jumps perturb the iuduced graph, but only mildly. The \nstatement SEMICON is semiclassical [R077, Sec. 4], and the sets MOD( SEMICON, errtering SEMICON, p) for \np = leaving SEMICON and for p = entering DONE can still be found by plugging into equations de\u00adrived \nat the time of language de~lnition. In general, questions as to what statements can or cannot do to variables \nor expressions are local data flow questions. They can be asked of alt statements, not just the minimal \nones in 2 that corre\u00adspond roughly to instructions in an intermediate text. Local data flow questions \nultimately refer to the high level control flow graph G and to strictly local information about what \ncan or cannot hap\u00adpen when control flows along each arc. We can answer many such questions by considering \nthe smaller induced graphs and computing bottom-up. When a semiclassical statement is encountered we \ncan plug into a known equation without even considering the in\u00adduced graph. We used the elementary example \nof MOD and one part conditionals to illustrate the method. The potential savings are more apparent when \nthe equations like (4.2) for various questions and control operators are considered. Details are in [R077 \n] for three important questions: MOD (as above), PRE (the variables that can be preserved), and USE (the \nvariables that can be used). Of the 23 statements in SUMFAC, only T = SUMFAC fails to be semiclassical. \nOnly for r do we need to trace paths in an in\u00adduced graph at proof time or compile time. For the same \nreasons that the usual low level analysis techniques can use basic blocks rather than instructions as \nnodes, we can analyze the compressed induced graph instead. Whatever graph analysis technique one would \nuse in Figure 1 to determine whether SUMFAC can modify the value of its input MS, the same technique \ncan be used in Figure 5 instead, which is only half as large. Whh due cautiori, the local information \nderived for SUMFAC can then be applied to find local information about calls on SUMFAC in other programs. \nRecursive procedures lead to a chicken/egg problem: there are circular de\u00adpendencies in the family of \nequations like (4.2) derived for a pro\u00adgram with recursive procedures. It can be shown that correct and \nsharpest possible MOD, PRE, USE information is obtained by finding the least fixpoint of the family of \nequations. Details and a full discussion of the pitfalls are in [Ro76b]. 5. GLOBAL DATA FLOW ANALYSIS \nLocal data flow information was globalized in Section 4. When local information is available for all \ntriples (a, n, p) then high level versions of traditional global flow problems can be solved rapidly. \nAs with low level flowcharts, a global problem is an at\u00adtempt to assign information to nodes that summarizes \neither what can happen on some paths or what must happen on all paths. The set of paths considered, for \neach node n iu the high level control flow graph, is either the set of paths to n from nodes in DENTRr \nor the set of paths from n to nodes in DEXITn. (Recall (2.2.1 ).) The details for live variables are \nin [R077, Sec. 11], and it is clear that traditional constant propagation or available expressions [HU75, \nSec. 5.1] can be similarly handled. The more ambitious semilattice versions of these problems [GW76; \nKU76] are under study. As might be expected, situations where f (x Ay) # f(x) A f(y) are delicate. Such \nmmdis[ribu/ivi/y is fairly common [Ku76, Sec. 6], and it changes = to > in the Induced Graph Theorem \nof [R077, Sec. 4]. Despite this technical hurdle, we can find an acceptable assignment [GW76, p. 177] \nof information to nodes in the high level control flow graph by calculations with induced graphs only. \nWe are now attempting to generalize the symbolic analysis typified by (4.2) so as to move much of the \nwork back\u00adward from compile time to the time of language definition, as is done in [R077] for less ambitious \nflow problems. A new compiler style is beginning to emerge [Ca77; Ha77: Kn74; L076]. Source text and \nmachine code will both appear as dialects of an intermediate language incorporating high and low level \nconstructs, Compilation will consist of gradual expansion from relatively high to relatively low level \nwithin the intermediate language, with frequent interludes of analysis and optimization. In particular, \nspecial case code generation will be unnecessary because systematic optimization at all levels will do \nat least as well [Ca77]. These ambitious compilers will update data flow information to reflect program \nchanges due to optimization, so as to polish the code nearly as well as a good programmer could. The \nsyntax di\u00adrected high level analysis sketched here is particularly oriented toward such compilers. We \ntry to hold down the toral cost of initial analysis and later updating for well written programs. The \nwell known cost bounds deal only with initial analysis, and no explicit bounds on total cost are available \nfor any data flow analysis method. Another hindrance to a direct cost comparison becomes apparent in \nthe next section. High level analysis yields a great deal of information that is useful for diagnostics. \nTo extract this in\u00adformation from the results of low level analysis requires more work. 6. COMPILER DIAGNOSTICS \n Much of the information accumulated so far is of obvious usefulness in understanding and maintaining \na program. The ENTR and EXIT sets in (2.2) and the path bits m in (2.3) display the effects of any departures \nfrom classical structured program\u00adming. They do this concisely at source tevel for each statement. A \nlow level flowchart like Figure 1 conveys the control flow between basic blocks in one lump that has \nno simple correlation with the source text. For example, the statement TEST includes seven nodes in Figure \n1 and part of an eigth. Even though TEST is rather near the root of a large parse tree, it cannot be \ncorrelated with the usual shallow hierarchy unless we open up a node to reveal the sequence of instructions \nwithin a block: [Q+quo(M, P)#R+-rem(M, P)#R=O? ] In practice the situation will often be worse because-programs \nwill be larger and will have flowcharts that cannot be neatly arranged on a page. The local and global \ndata flow information from Sections 4 and 5 is clearly useful for diagnostic purposes or for program \nproofs [Ro76a, Sec. 6]. Fosdick and Osterweil [F076; 0F76] explain how to reinterpret global flow problems \nsuch as live variables to detect data flow anomalies such as an uninitialized use of a local variable. \nAs they imply, it is helpful to warn the programmer that B : = A+2 can be reached before A is initialized, \nbut much more helpful to couple the warning with an example path whereon this happens. Now suppose that \nB : = A+2 occurs in the context CONTEXT: [ FIDDLE: if...then A := O else MESS: [...]; THICKET: [...]; \nCHANGE: B := A+2 ] where MESS and THICKET are large and intricate statements such that, for y = MESS \nand for y = THICKET, = A E MOD(Y, entering y, leaving y). To trace an anomalous path from a point corresponding \nto entering CONTEXT to a point corresponding to errterirrg CHANGE in a low level flowchart would be confusing. \nIrrelevant details of paths through MESS and THICKET would be included. High level languages should relieve \nthe programmer of the need to read low level programs, at least when fine tuning of run time performance \nis not an immediate issue. The exclusive use of low level control and data flow analysis methods prechrdes \nsuch relief, unless we choose to have data flow diagnostics that are much less informative than is desirable. \nProgrammers can cope with exclusively low level diag\u00adnostics, but there is fittle appeal in a miraculous \nnew language, if the corresponding debugging tool talks back to me in octal or reformulate a crucial \npoint more abstractly and much more general\u00ad hexadecimal [Sp76, p. 294]. (Similar remarks apply to program \nproving when escapes and jumps are permitted.) In the induced graphs Ga for a = CONTEXT and a = FID-DLE \nwe can easily and naturally display short paths that character\u00adize all the anomalous paths in the full \nhigh level control flow graph. We begin in CONTEXT with the path from entering CONTEXT to entering FIDDLE \nto leaving FIDDLE (along an imaginary arc) to entering THICKET to leaving THICKET (along an imaginary \narc) to entering CHANGE, where the second imaginary arc cannot modify A. The first imagi\u00adnary arc can \nmodify A, but it can also preserve A. Therefore we provide more detail in FIDDLE with the path from entering \nFIDDLE to entering MESS to leaving MESS (along an imaginary arc) to leaving FIDDLE, where all arcs cannot \nmodify A. High level analysis leads directly to concise but informative diagnostics at source level when \nthe methods of [F076; 0F76] are applied to languages amenable to practical structured programming. 7. \nPROGRAM PROVING It is convenient to consider the original values of input varia\u00ad bles as persisting in \na special part of the storage state f of a compu\u00ad tation. Then partial correctness [Ma74, Sec. 3-1] with \nrespect to predicates P, Q on storage states is the property that, whenever a computation begins with \n$ such that P(f) and finishes with q , then Q(rr). Owicki [OW75; 0w76; 0G761 proposes a powerful method \nfor proving partial correctness of asynchronous parallel programs. Two assertions, a precondition and \na postcondition, are associated with each a in X, as in the style of proof introduced by Hoare [H069]. \nCarefully chosen assertions can lead to clear and convinc\u00ad ing proofs despite the pitfalls of asynchronous \nparallelism. We ly, so as to extend the applicability of the method. We assume that the maximum T in \nX satisfies DENTRm = { entering T ] and DEXITn = { leaving m }, (7.1) but none of the other constraints \nof classical structured program\u00adming are needed. A storage state predicate C[n] is to be assigned to \neach node n in the high level control flow graph G, in such a way that, for all states f, q and arcs \nc in G: P(f) implies Centering ~](.$); (7.2.1) C[leaving r](q) implies Q(q); (7.2.2) (C[sc](f) and ~ \n~c q) implies C[tc](q), (7.2.3) where ( ~c q means that $ can be changed to q when control flows along \nc. In sequential programming this would guarantee partial correctness, and we have so far just transcribed \n[F167] into the notation of Section 2. Now we impose the interference-free condition [OW75, Sec. 3.3]: \nfor all states .$, q and all arcs c in G such that c and n are in distinct parallel processes, (C[n](f) \nand C[SC](6) and f ~C T) implies C[n](~). (7.3) An assignment C from nodes to assertions that satisfies \n(7.3) as well as the Floyd conditions (7.2) will assure partial correctness, In [OW76; 0G76] the sharing \nof variables by processes is restricted in ways that expedite verification of (7.3), For us the precondition \nand postcondition for a in a Hoare style proof are Centering a] and C[leaving a]. By directly assign\u00ading \none assertion to each node in G rather than assigning fwo assertions to each statement in 2, we transcend \nthe restriction to classical structured programming. Wang [Wa76] proposes a similar trick for sequential \nprograms in the case where each a in Y, not just r, satisfies (7,1), Only if Noa in (2.5.2) is always \n{ entering a, leaving a ] will [Wa76] assign assertions to all nodes bf G. Consider the program FINDPOS \n~Ro76a, Sec. 3] that has been used to illustrate what is so hard about asynchronous parallelism. Owicki \npresents an elegant partial correctness proof for a similar program Findpos [OW75, Fig. 3.4] wherein \nthe effects of leave statements are simulated in classical structured programming. The 44 program Findpos \nis a shade less clear and distinctly less efficient than FINDPOS. The mathematical machinery in [OW75] \ncannot say, much less prove, that partial correctness of Findpos implies partial correctness of FINDPOS. \nWith (7.2) and (7.3) however, we can prove the partial correctness of FINDPOS directly by an easy adaptation \nof [OW75, Fig. 3.4]. The formulation (7.3) of interference-freeness is also simpler than the original \none. Adding [Ro76a, Lemma 5. 1] to the partial correctness proof leads to a total correctness proof more \nelegant than [Ro76a, Sec. 5]. REFERENCES Ca77. Carter, J.L. A case study of a new code generating techni\u00adque \nfor compilers. Comm. ACM (1977?) to appear. CH72. Clint, M., and Hoare, C.A.R. Program proving: jumps \nand functions. Acts Informatica 1 (1972),214-224. DDH72. Dahl. O. J., Dijkstra, E. W., and Hoare, C.A.R. \nStructured Programming. Academic Press, London and New York, 1972. DEL76. DeMillo, R. A., Eisenstat, \nS. C., and Lipton, R.J. Can structured programs be efficient? SIGPLAN Nolices vol. 11 num. 10 (October \n1976), 10-18. F167. Floyd, R.W. Assigning meanings to programs. Proc, Symp. Appl. Math. 19(1967),19-32. \nF076. Fosdick, L. D., and Osterweil, L.J, Data flow analysis in software reliability. TR CU-CS-087-76, \nComputer Science Dept., University of Colorado, Boulder, May 1976. GW76. Graham, S.L., and Wegman, M. \nAfastand usually linear algorithm for global flow analysis. X ACM 23 (1976), 172-202. Ha77. Harrison, \nW. Anew strategy for code generation the general purpose optimizing compiler. These Proceedings. HU75. \nHecht, M. S., and Unman, J.D. A simple algorithm for global flow data flow problems. SIAM J Computing \n4 (1975), 519-532. H069. Hoare, C.A.R. An axiomatic basis for computer program\u00adming. Comm. ACM 12 (1969),576-583. \nKU76. Kam, J. B., and Unman, J.D. Global data flow analysis and iterative algorithms. J. ACM 23(1976), \n158-171. Kn74, Knuth, D.E. Structured programming with goto statements. Computing Surveys 6(1974),261-302. \nL076. Loveman, D.B. Program improvement by source to source transformation. Proc. 3rd ACM Symp. on Principles \naf Programming Languages (January 1976),140-152. Ma74. Manna, Z. Mathematical Theory of Computation. \nMcGraw-Hill, New York, 1974. MR76. Markowsky, G., and Rosen, B.K. Bases for chain\u00adcomplete posets. IBM \nJ. Res. and Devel. 20 (1976), 138-147. 0F76. Osterweil, L. J., and Fosdick, L.D. DAVE: a validation, \nerror detection, and documentation system for FORTRAN programs. Software Practice and Experience (1976) \nto appear. 0w75. Owicki, S.S. Axiomatic proof techniques for parallel pro\u00adgrams. Tech. Rep. 75-251 (Ph.D. \nthesis), Computer Sci. Dept., Cornell U., Ithaca New York, July 1975. 0w76. Owicki, S.S. A consistent \nand complete deductive system for the verification of parallel programs. Proc. 8th Ann. ACM Symp. on \nTheory of Computing (May 1976), 73\u00ad 86. 0G76. Owicki, S. S., and Gries, D. Verifying properties of parallel \nprograms: an axiomatic approach. Comm. ACM 19 (1976), 279-285. Ro76a. Rosen, B,K. Correctness of parallel \nprograms: the Church-Rosser approach. Theoretical Computer Science 2 (1976), 183-207. Ro76b. Rosen, B.K, \nData flow analysis for procedural languages. IBM Research Report RC 5948, Yorktown Heights, April 1976. \nR077. Rosen, B.K. High level data flow analysis. Comm. ACM (1977?) to appear. SP76. Spier, M.J. Software \nmalpractice a distasteful experi\u00adence. Software Practice and Experience 6 (1976), 293\u00ad 299. SW74. Strachey, \nC., and Wadsworth, C.P. Continuations: a mathematical semantics for handling full jumps. Tech. Mono. \nPRG-11, Programming Res. Grp., Oxford U., Janu\u00adary 1974. U173. Unman, J.D. Fast algorithms for the elimination \nof common subexpressions. Acra Informatica 2 (1973), 191-213. Wa76. Wang, A. An axiomatic basis for proving \ntotal correctness of goto programs. BIT 16 (1976), 88-102. WU75. Wulf, W. A,, et al. The Design of an \nOptimizing Compi\u00adler. American Elsevier, New York, 1975. Za74. Zahn, C.T. A control statement for natural \ntop-down struc\u00adtured programming. Lecture Notes in Computer Sci. 19 (1974), 170-180. LM75. Ledgard, H. \nF., and Marcotty, M. A genealogy of control structures. Comm. ACM 18 ( 1975), 629-639. I . \\ \\ I l-f \n m . . Fig. 2. Induced graph Gai for al = CON1 and a2 = CON2 in SUMFAC. The sinuous dashed arc is imaginary. \nThe same picture with no i subscripts would be good for any a: if then ~ statement in classical structured \nprogramming. 13ht j  SEMTCON II / I JI . [ . Low level flowchart for SUMFAC. Each node is a basic block \nof instructions in an intermediate text (not shown) constructed in the usual way. For each block we indicate \nwhich line numbers in SUMFAC give rise to instructions in &#38;leav rj sEMICO~ 4 c)k(zv{ PGQ that block. \n Grounded arcs indicate entrances and exits. Fig. 3. Induced graph Ga for a = SEMICON in SUMFAC, Note \nthat leaving PGQ is not reachable from any entrance to SEMICON. This graph and the graphs shown in Figure \n2 belong to the infinite family of graphs induced by one part conditional statements a: if.. then /3 \nthat appear in programs with various uses of escapes and jumps, These graphs also belong to the infinite \nsubfamily derived from one part con\u00additionals that are semiclassical . In practical structured programming \nmost statements are semiclassical. 46 art 9 SUMFAC . L+ Fig. 4. Induced graph Gn for n = SUMFAC. Sinuous \ndashed arcs are imaginary but not real. Heavy arcs are contributed by escapes and jumps. Be\u00adcause two \nof these arcs pass control from TEST to other parts NEWPRIME and AGAIN of v, the statement n fails to \nbe semiclassical. Fig. 5. Compressed induced graph CGm for r = SUM-FAC. An arc from m to p in CGT summarizes \nthe net effect of a path from m to p in GrT that passes through only flow trivial nodes. ---,-.( , I \n[\\ @ G-iz-b\u00ad . . , 47 \n\t\t\t", "proc_id": "512950", "abstract": "Control flow relations in a high level language program can be represented by a hierarchy of small graphs that combines nesting relations among statements in an ALGOL-like syntax with <i>relevant</i> perturbations caused by <b>goto</b> or <b>leave</b> statements. Applications of the new style of representation include denotational semantics, data flow analysis, source level compiler diagnostics, and program proving.", "authors": [{"name": "Barry K. Rosen", "author_profile_id": "81100316668", "affiliation": "IBM Thomas J. Watson Research Center, Yorktown Heights, New York", "person_id": "P28116", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/512950.512955", "year": "1977", "article_id": "512955", "conference": "POPL", "title": "Applications of high level control flow", "url": "http://dl.acm.org/citation.cfm?id=512955"}