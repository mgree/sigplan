{"article_publication_date": "01-01-1977", "fulltext": "\n A New Strategy for Code Generation the General Purpose Optimizing Compiler Permission to make digital \nor hard copies of part or all of this work or personal or classroom use is granted without fee provided \nthat copies are not made or distributed for profit or commercial advantage and that copies bear this \nnotice and the full citation on the first page. To copy otherwise, to republish, to post on servers, \nor to redistribute to lists, requires prior specific permission and/or a fee.&#38;#169; 1977 ACM 0-12345-678-9 \n$5.00 William Harrison IBM Thomas J. Watson Research Center Yorktown Heights, New York 10598 ABSTRACT: \nThis paper presents a systematic approach to the problem of generating good code with a compiler that \nis easy to construct. A compiler structure is proposed which relies on interprocedural data flow analysis, \nglobal optimization, and an intermediate language schema to simplify the task of writing the code generating \nportions of a compiler without sacrificing code quality. This structure is contrasted with a more conventional \nstructure to explore the reasons why the new structure solves several problems inherent in the conventional \nstructure. Further advantages which accrue from the new structure are also presented. OVERVIEW Considerable \neffort has been spent over the last decade producing a systematic strategy for the syntax processing \ncomponent of compilers. During this time, however, no adequate framework has evolved for struc\u00adturing \nthe remaining components of an optimizing compiler. Recent advances ,z, ~ in program analysis have opened \nthe way to a new compiling strategy which may provide such a framework. This strategy is the general \npurpose optimizing compiler (GPO compiler) which is being developed by the Experimental Compiling System \ngroup at the IBM Research Laboratory, Yorktown Heights. The GPO compiler strategy has evolved from the \nsynthesis of four fundamental ideas: 1) the development of a schema for intermediate languages, 2) the \nexpression of language semantics through defining procedures which take the place of code generators, \ncode macros, and library source code, 3) the application of global data flow analysis to the defining \nprocedures and source programs, and 4) the repeated application of machine\u00adindependent optimizations. \nThe role of each of these is discussed in turn. Intermediate Language Schema Most compilers perform the \ntranslation from source to target in a series of stages using a different representation of the known \ninformation at each stage. These represent\u00adations, which include both text and symbol table informa\u00adtion, \nare often referred to as intermediate languages. In order to facilitate the application of global flow \nanalysis and optimization at any stage, it is imperative that a GPO compiler embody a schematic representation \ninto which all intermediate languages, from source to target, may be embedded. This representation, called \nthe IL Schema, meets several important objectives. It supports languages such as PL/1, ALGOL, COBOL, \nand FORTRAN. It makes the execution of a procedure integrated at compile-time equivalent to invocation \nof the procedure at run-time so that the compiler is free to chose whether or not to integrate any procedure \ninto any particular invocation, It introduces no ambiguities into control and data flow analysis that \nwere not characteristic of the source lan\u00adguage so that the quality of the code resulting from a source \nlanguage will not suffer from its translation through the schema. Finally, it contains a minimal number \nof language constructs with built-in semantics so that the largest number of constructs are available \nfor analysis and optimization. Several broad characterizations of this IL Schema can be made. It contains \nno dictionary holding particular information about the data types, bounds, or other attributes of variables. \nSuch information is expected to be held as the values of ordinary variables. The instruction format uniformly \nrepresents the invocation of procedures, whether they are primitives or source language proced\u00adures. \nNo expressions may appear as operands since the call by reference interpretation of these expressions \nat run-time may differ from the call by name interpreta\u00adtion imposed by procedure integration at compile-time. \nThere are no built-in constructs for exception handling nor are there even arithmetic or branching instructions. \nThe schematic language is somewhat higher than a conventional machine language in that details of storage \nmapping and address computation are suppressed. In addition, the IL schema provides for a naming convention \nwhich allows the variables containing type information to be associated with the variable they describe. \nIt also includes provision for describing the occurrence of block structure in a program, for describing \npossible storage equivalencing relations among variables, for indicating the occurrence of indirect addressing \nor its inverse, and for denoting program points as labels or entries. The following sample procedure \nfor generic addition of fixed or floating point numbers illustrates the use of this naming convention: \nADD: PROCEDURE(X,Y,Z); DO CASE X.TYPE EQ FIXED : FIXADD(X,Y,Z) ; FLOAT : FLOATADD(X,Y,Z); END; END; \nDefining Procedures In order to relate the semantics of a source language to the semantics of a target \nmachine, it is necessary for a compiler designer to specify at least one most general code sequence which \nimplements each source language operation. This is the same specification which would be used for an \ninterpreter of the operation. However, such a general definition ignores many possibilities for optimiza\u00adtion. \nThese possibilities arise because the context in which an operation is actually used may make the information \nneeded to interpret some parts of the operation sequence available at compile time. For example, the \nmost general code for referencing an array element in PL/I would include examination of subscript range \nenablement, the range checking itself, and the formation of a sum of products of the multipliers and \nsubscripts. In a GPO compiler, the semantics of a source level operation is expressed as a conventional \nprocedure whose invocation at run-time would always produce the correct result. These procedures, called \ndefining procedures, may be viewed as the code for an interpreter, or as macros whose expansion produces \nthe code desired from a compiler. By using procedure integration, the defining procedure for an operation \ncan be incorporated into a calling procedure. This involves performing by-name substitution for parameters \nand renaming of local varia\u00adbles. The integration of defining procedures merely eliminates the overhead \nof procedure call. In traditional compilers for large languages like PL/Is, the macro\u00adexpander or code-generator \ngenerally embodies the ability to execute tests at compile time in order to tailor the code sequences \nto the circumstances of their invocation. For example, the code generator for subscripting in a PL/I \ncompiler would usually test the enablement of subscrip\u00adtrange at compile-time, since the enablement of \nconditions is known statically. In a GPO compiler, the function of such an interpretive selection mechanism \nis performed by global data flow analysis and optimization. Such analysis and optimization is independent \nof the particular charac\u00adteristics of the language being compiled. For example, in a PL/I compiler, the \ntypes of data are always known at compile-time. Thus, at any invocation of the previously described ADD \nprocedure, the value of X.TYPE would have been established by the sequence of assignments to attributes \nof X made by the DECLARE statement for X. Data flow analysis relates the assignment to X.TYPE to its \nuse in ADD, allowing the compiler to determine which case applies at a particular invocation of ADD. \nAlthough the same ADD procedure might be employed in an compiler for a typeless language, the lack of \ndeclarations might cause the testing of X.TYPE to be deferred until execution time. Global Data Flow \nAnalysis Global data flow analysis is the process of determin\u00ading how information is generated and used \nin a program. R is then possible to base various optimizations on locally available information derived \nfrom the global analysis. For example, data flow analysis can be used to derive a relation between those \noperations which set the value of a variable and those operations which use that value. This makes it \npossible for the optimization called constant propagation to carry forward the results of interpreting \nan operation at compile-time by bringing those results directly to the operations which use them. The \ntechnology for performing such an analysis of a program is becoming well understood2,1A,16, However, \nbecause the semantics of all operations are expressed as procedures, it is necessary that the analysis \nof a program take cognizance of some summary of preceding analyses of the procedures the program invokes. \nRecent workl~] 131 has also estab. lished techniques for such interprocedural analysis. By using interprocedural \nanalysis, summary informa\u00adtion which is asserted for the most primitive operations encountered in the \ncompiling process can be mechanically constructed for the defining procedures built from them. Global \nOptimization In the development of a GPO compiler, a collection of optimization forms the basis which \nreplaces the manual analysis conventionally applied to determine special code generation cases. Certain \noptimizations, such as constant propagation and dead code elimmationz are vitally necessary in a GPO \ncompiler. Other optimizations, such as redundant expression elimination, range analysis6, variable propagation \n, and strength reduction may be part of any optimizing compiler. They assume greater importance in a \nGPO compiler since the code resulting from integrated procedures conttilns many redundtint presumably \ncorrespond closely to the source language operations. These operations would have been avoided by operations. \nNext, the resulting program is subjected to a human programmer presented with the task of writing repeated \ncontrol and data flow analysis, optimization, the original program entirely in a low-level language, \nIt and procedure integrations, The procedure integration has been demonstrated that the application of \nsuch component and the analysis component are directed by optimization to a procedural description of \nconcatenation (heavy lines) a library containing the defining procedures would, in a typical case, produce \na code sequence in which and their data flow summaries, This transformation 25 instructions are executed \nas compared to the 35 results in an IL/P program, i.e. one completely in terms of instructions required \nby an available optimizing compiler prtmitive operations, Although the choice of this primi\u00ador the 85 \ninstructions required by an interpreter, tive level is somewhat arbitrary, it is intended to be a regularized \nversion of the machine operations. The IL/P STRUCTURE OF THE COMPILER program result may be entered into \nthe library as a new As can be seen from Figure 1. a GPO compiler has defining procedure. Finally, the \nstorage mapping is three major transformatlorral components. performed. the lL/P is transformed to a \nmachine oriented First, the source language translator converts the variant of IL/P called RL and register \nallocation and final input form into its IL language, IL/S. The IL/S operators assembly take place. RL \npreserves the IL/P program but IL/P and IL/S Summaries IL/P Storage Mapping and -Analysis IL toRL Conversion \nI Instruction Scheduling, RegisterSummaries Allocation, Optimization and Final Expansions t Assembly \nProcedure Integration o IL/S IL/P Machine and Code Summary Figure 1 General Structure of a GPO Compiler \n31 annotates the operands of the IL/P program with whatev\u00ader addressing Information is needed. Register \nallocation is then performed, followed by final assembly. A small example will give some insight into \nthe interaction of optimizations with defining procedures to produce tailored code. For this example, \nwe will consider the translation of two lines of a PL/I program to the IBM System/370. The two PL/I statements \nare: 1. DECLARE I FIXED BINARY, A(10,1O) FIXED BINARY, B(2O,1O) FIXED BINARY; .. 2. A(I,l) = B(I,l); \nSince the emphasis in this example will be on tailoring the subscripting operation, the data type issues \nwhich would actually arise in PL/I will be ignored. T his is reasonable since it is clear that constant \npropagation of the FIXED BINARY attribute eliminates any conver\u00adsions. The translation of these two statements \ninto IL/PLI could then be as shown in Table 1. The SUBSCRIPT operation is a general PL/I sub\u00adscripting \nprocedure which computes a pointer to the array element whether the array is based, controlled, or parame\u00adter, \nor whether the bounds are known or unknown. Consider the integration of the subscript procedure into \nthe above program, as shown in Table 2. Examination of the optimization of the code resulting from statement \n2 shows that, in fact, optimal code can be derived automati\u00adcally from the general procedure. Constant \npropagation will drive all of the bounds and multiplier information from the declaration in statement \n1 into the operands of the code for statement 2, Furthermore, when constants are propagated to all input \narguments of an operation, the result can be computed and propagated further. By keeping the data flow \ninformation up to date, dead code elimination proceeds simultaneously. Table 2 shows the 1.1 MOVE 1,2 \nMOVE 1.3 MOVE 1.4 MOVE 1.5 MOVE 1.6 MOVE 1.7 MOVE 1.8 MOVE 1.9 MOVE 1.10 MOVE 1.11 MOVE 1.12 MOVE 2.1 \nSUBSCRIPT  2.2 SUBSCRIPT 2.3 MOVE results of constant propagation, with dead code marked by a bullet \n(.). It should also be noted (hat after constant propagation, all code resulting from sta[ernent 1 has \nbecome dead. Several other optimizing transformations can be interleaved with constant propagation. These \nare: range analysis, redundant expression elimination. and operation simplification. Operation simplification \nincludes collection of operator dependent transformations which are made when various operand criteria \nare met. For example. y=x+O * y = x is an operation simplifica\u00adtion. Table 3 shows the result of applying \nthese transfor\u00admations. A new data flow analysis pass is made to form the information needed for the \nvariable propagation optimi\u00adzation. These transformations are applied, opening the way for another round \nof optimization. In the case under consideration, however, optimization of the IL/P program is complete. \nTable 4 shows the final IL/P program corresponding to the sample input program. The process of integration \nand optimization has reduced the original IL/PLI program to an IL/370 program containing only machine-like \nprimitives. These primitives are selected to reflect, one-to-one, the semantics of the target machine \ns operators without the complex operand accessing structure present in most machines. This accessing \nstructure, which generally includes address formation, indexing, and indirection, is instead exposed \nas instructions in the code stream. This allows the optim\u00adizing transformations to ignore special addressing \ncharac\u00adteristics of the target machine. In a GPO compiler, storage mapping completes this process of \nexposing all computations by entering into the instruction stream all instructions needed for address \ncalculation. In the exa mple under discussion, this adds the instructions numbered 0.1 through 0.7 in \nTable 5. These A. LBOUNDI = 1 B, LBOUND1 = 1 A. HBOUND1 = 10 B. HBOUND1 =20 A. MULTIPLIER1 =20 B. \nMULTIPLIER1 =20 A. LBOUND2 = 1 B. LBOUND2 = 1 A. HBOUND2 = 10 B. HBOUND2 = 10 A. MULTIPLIER2 = 2 \n B. MULTIPLIER2 = 2 T1 =A(I,l) T2=B(I,1) indirect(T 1) = indirect Table 1 IL/PLI Version of Source \nProgram E # Code after Integration Code after Constant Propagation \\ I* 2.1.1 SUB T5 = l A. LBOUND2 SUB \nT5= 1-1 II  2.1.2 MULT T6 = A. MULTIPLIER2*T5 MULT T6= 2*0 I I 2.1.3 SUB T7 = I A. LBOUND1 SUB T7=I \n I I I 2.1.4 MULT T8 = A, MULTIPLIER1 *T7 IMULT T8 = 20*T7 I 2.1.5 ADD T9 = T6+T8 IADD T9 = O+T8 I 2.1.6 \nADD T1 = address-af(A)+T9 I unchan~ed I 2.2.1 SUB TIO = 1-B. LBOUND2 ]SUB TIO= l-1 I* 2.2.2 MULT T1 1 \n= B. MULTIPLIER2*T1 O IMULT T] I =2*O I* 2.2.3 SUB T12 = I B.LBOUND1 ISUB T12=I I I 2.2.4 MULT T13 = \nB. MULTIPLIER 1*T12 MULT T13=20*T12 2.2.5 ADD T14=T11+T13 IADD T14=O+T13 I 2,2,6 ADD T2 = address-o~(B)+T14 \nunchanged I I 2.3.1 MOVE indtrecdT 1) = indirect(T2 ) unchanged Table 2 Result of Constant Propagation \n# Code after Constant Propagation Code after Redundant Operation Expression Elimination Simplification \nand .1,3 SUB T7=I 1 unchanged .1.4 MULT T8 = 20*T7 unchanged ,.1.5 ADD T9 = O+T8 MOVE T9 = T8 :.1.6 ADD \nT 1 = address-of(A) +T9 unchanged !.2.3 SUB T12=I 1 T12is T7 !.2,4 MULT T13=20*T12 T13 is T8 ;.2.5 ADD \nT14=O+T13 MOVE T14=T8: !.2.6 ADD T2 = addres~-~~(B)+T14 unchanged !.3.1 MOVE indirect(T 1) = indirecl(T2 \n) unchanged Table 3 Result of Redundant Expression Elimination and Operation Simplification 33 instructions \nhave the form ADD Ti = DSA+offt or ADD T,= SI+off,, where the off, are the the offsets in the automatic \nstorage(DSA) or static storage assigned to variable i. Any references to address(i) in the program are \nchanged to T,. t This transformation occurs in two steps. Operation The process of converting the primitive \nIL/370 simplification changes the ADD to a MOVE and redun\u00addant expression elimination replacesT13 with \nT8. program to the corresponding RL/370 program is Code after Redundant Expression Elimination and # \nFinal Code After Subsumption/Renaming Operation Simplification 2.1.3 SUB T7=I 1 unchanged 2.1.4 MULT \nT8 = 20*T7 unchanged 2.1.5 MOVE T9 = T8 unchanged 2.1.6 ADD T1 = address-of(A)+T9 ADD T1 = address-of(A)+T8 \n2.2.5 MOVE T14=T8 unchanged 2.2.6 ADD T2 = address -of(B)+Tl 4 ADD T2 = address-of(B) +T8 2.3.1 MOVE \nindirect(T 1) = indirect unchanged Table 4 IL/P Code after Optimization # Code after Storage Mapping \nRL/370 Code 0.1 ADD T.= DSA+offA 0.2 ADD T,, = DSA+off,l 0.3 ADD T,= DSA+offl 0.4 ADD T., = DSA+off,, \n0.5 ADD T,g = DSA+off,, 0.6 ADD Tl=SI+off, 0.7 ADD T,,, = SI+off,(, 2.1.3 SUB T7=I 1 SUB DSA,offJT7]; \nDSA,offl[I]; SI,off, [l] 2.1.4 MULT T8 = 20*T7 MULT DSA,off~,[T8]; SI,off,,)[20]; DSA,offr,[T7] 2.1.6 \nADD T1 =T~+T8 2.2.6 ADD T2 = T,,+T8 2.3.1 MOVE indirect(T 1) = indirecf(T2 ) MOVE DSA,offA,T8[indirect( \nTl)]; DSA,offl,,T8[irzdirecdT l)] Table 5 Result of IL-RL Conversion 34 accomplished by gathering together \nthose operations Compiling Past the Machine Interface which can be absorbed into the operand semantics \nof the machine, For example, in addressing a single operand of an instruction, an IBM System/370 can \nperform the equivalent of the code sequence: ADD t=ll+k [for k<4095] ADD f= X+t reference indirect (t \n), The conversion of IL to RL is performed by the matching of such patterns in the data flow graph. When \napplied to the sample code described above, this process results in the the 3-instruction sequence of \nRL/370 code displayed in Table 5. Register allocation will convert this RL/370 code into a sequence of \nat most 5 machine instructions. COMPARISON WITH CONVENTIONAL COMPILERS The structure of a GPO compiler \nmay be compared with that of a conventional compiler. [L Schema In the GPO compiler strategy, the IL \nSchema replaces the collection of disparate levels of intermediate language present in a conventional \ncompiler. This single schematic representation provides for flexible support of multiple source or target \nlanguages. In addition, its use for the representation of input, output, and intermediate stages of compilation, \nas well as for the defining procedures themselves, makes it possible to write a single collection of \nanalysis and optimization algorithms. As an additional benefit of this flexibility, a GPO compiler is \nfree to perform various analyses and optimizations at more than one stage in the compiling process, making \nit unnecessary to find a single best time for optimization. Defining Procedures The GPO compiler strategy \nemploys a library which contains, in addition to the bodies of the defining proce\u00addures, a summary of \ntheir characteristics. The summary information is a uniform description of the data flow and other characteristics \nof all operations, primitive and expandable. Thus the analysis and optimization algor\u00adithms are totally \nindependent of the precise semantics of the operations. The fact that nonprimitive operations are merely \nprocedures in the same IL schematic framework makes it possible for most of the summary characteristics \nto be produced mechanically by the same data flow analysis which later uses them. Those summary items \nwhich, like commutivity, are difficult to mechanize will be supplied by assertions about the defining \nprocedures. A conventional compiler often performs peephole optimizationsz which detect the occurrence \nof certain machine-dependent code patterns which are subject to improvement. In a GPO compiler, a similar \npattern matching process takes place to convert the primitive IL program to its RL form. The pattern \nmatch in this case is not limited to small sections of control flow, but rather, is applied in the context \nof the more general data flow graph. This matching and substitution process is per\u00adformed to gather overexpanded \noperations back together into machine primitives. The overexpansion of operations often results from \nthe exposure of addressing calculations in the IL/P code. However, such a gathering compo\u00adnent will also \nproduce optimizations of any code which was overexpanded by the programmer before being input to the \ncompiler. Elimination of Special Casing In the GPO compiler strategy, the code generation phase of the \nconventional compiler has been replaced by a procedure integrator. Defining procedures in a GPO compiler \ncontain only those instructions needed to express the semantics of the individual operation which they \nimplement. It has been claimeds that code generators should provide for elaborate special casing. Special\u00adcasing \ncompilers require major re-examination and revision when language changes are made. In fact, the cost \nof a revised compiler often approaches that of the original. This requirement for major revision when \nlanguage changes are introduced seems to be inherent in the special-casing approach to optimization. \nThis is caused by the fact that the conceptual model used for the design and specification of each language \noperation does not match the representation of operation semantics used by the compiler designer, In \nparticular, the special-case compiler designer has distributed the semantics of each operation into the \ndescription of all others with which it may interact to produce special cases. This combinatorial effect \nmeans that although the compiler designer may correctly incorporate a language change into uses of the \nfeature specifically changed, it is difficult for him to locate all cases where the semantics of the \nchanged operation has been factored into the case analysis of other operations. What generally happens \nis that bugs develop in the compiler from outdated case analyses. These bugs are fixed by removing the \noffending special case and over time the quality of the compiler code degrades. The GPO compiler strategy \nof implementing only the general case operations and allowing the optimizations to derive the special \ncase code systematically overcomes the disadvan \u00adtages of speciil casing. ADDITIONAL ADVANTAGES In addition \nto reducing the problems of cost, com\u00adplexity, reliability, and code quality associated with existing \ncompiler technologies, the GPOcompiler str~tegy brings with it numerous additional advantages. Varying \nDegrees of Optimization It becomes possible to use the same body of program\u00adming to implement varying \ndegrees of optimization. For example, interpretive execution is achieved by totally or selectively avoiding \nthe integration of defining proce\u00addures. In the limiting case, all instructions In the IL/S form of a \nprogram are merely converted into subroutine calls to some interpreter library. On tbe other hand, a \nquick and dirty compilation is achieved by performing rudimentary constant propagations of the data type \nattributes. Simple examination of the summary informa\u00adtion of the defining procedures will indicate that \nno definitions of the value of these attributes occurs, so the constant propagation can be performed \nwithout extensive analysis. The cost of the alternating sequences of proce\u00addure integration and dead-code \nelimination which would result may be reduced to produce a quick and dirty compiler. This cost reduction \ncan be realized by applying a pre-integration of the defining procedures into one another with an accompanying \npre-analysis like that which has been suggested for conventional optimization ( 7. Pre-integration and \npre-analysis is also useful to the fully optimizing compiler because the alternative expansions represent \ntailored library routines from which a selection can be made according to circum\u00adstances. Proofs of Compiler \nCorrectness The GPO compiling strategy of using defining procedures assists in proving the correctness \nof an imple\u00admentation. The correctness of various optimizations may be demonstrated on an overall basis, \nand then a particular language implementation must only demonstrate, using IZ that the defining procedures \nsome accepted technique , are faithful to the language. Improved Programming Style Use of a GPO compiler \nalso contributes to the use of improved programming style by allowing a program to be constructed from \nmany small sub-programs without incurring the overhead associated with numerous proce\u00addure calls or with \noverly general procedures. Furthermore, the advantages of an abstract data definition can be realized \nwithout sacrificing the optimizations made possible by exploiting its underlying representation. Program \nDevelopmerrr Supporr In fact, the GPO compiling strategy can occupy a significant role in an overall \nprogramming support system. The same summary information and integration techni\u00adques applied to the library \nof defining procedures can be applied to a library of user programs. The compiler can verify that summaries \nof program behavior fit within their specified behavior. Optimization of Skslems The GPO compiler strategy \nopens the way to the optimization of large programming entities such as compilers, data managements, \nor operating systems. By compiling those components of a system which are totally internal, knowledge \nabout all invokers can be used to eliminate inhibitions to optimization which arise from not knowing \naliasing information about p~rameters and globals. ACKNOWLEDGEMENTS The Experimental Compiling System \nis being built upon a groundwork laid by Frances Allen, John Cocke, and Patricia Goldberg. Recognition \nshould be given to J. Lawrence Carter, Paul Loewner, David Lomet, Robert Tapscott, and Mark Wegman for \ntheir contributions to this project. My thanks are due to these people and especially to Mary Harrison \nfor their helpful suggestions in revision of this manuscript, REFERENCES 1) Allen, F.E. Interprocedural \ndata flow analysis. Proc. IFIP Congress ~, North Holland Publishing Co., Amsterdam, 1974, 398-402 2) \nAllen, F. E., and Cocke, J. A catalogue of optimizing transformations. QE@ m oPtimi~~tion of Compilers, \nRustin, R. (Ed.), Prentice-Hall, Englewoo~ Cliffs, N. J., 1972, 1-30 3) Barth, Jeffrey M. Interprocedural \ndata flow analysis based on transitive closure. Report UCB-CS-7644, University of California at Berkeley, \nSeptember 13, 1976 4) Carter, J.L. A case study of a new code generation technique for compilers Communications \nof the ACM, to be published 5) Elson, M., and Rake, S.T. Code generation technique for large language \ncompilers. IBM Systems Journal, Vol. 9 No, 3 (1970), 166-188 6) Harrison, William Compiler analysis of \nthe value ranges of variables. IBM Research Report, RC5544, T.J. Watson Research Laboratory, Yorktown \nHeights, NY., July 1975 7) H~rrison, William Formal semantics of a schematic intermediate language, \nIBM Research Report, RC6271, T.J. Watson Research Laboratory, Yorktown Heights, NY., November 1976 8) \nHerrick, Steven S., Donnely, David V. A survey of optimization techniques In compilers. National Technics] \nInformation Service, AD-AO 17-404, Rome Air Development Center, September 1975 9) Liskov, B. H., Zilles, \nS.N. Specification techniques for data abstractions, IEEE Transactions on Software Engineering, Vol. \n1 No, 1, March 1975, 7-fi 10) Lomet, David Predicting the effects of call point information on called \nprocedure optimization. Inter\u00adnal Memorandum, October 1975 11) Lomet, D. B. Data flow analysis in the \npresence of procedure calls. IBM Research Report, RC5728, T.J. Watson Research Laboratory, Yorktown Heights, \nNY., November 1975 12) Jones, C.B. Formal definition in compiler develop\u00adment. Technical Report, TR25. \n145, IBM Laboratory Vienna, February 1976 13) Rosen, Barry K. Dtita flow analysis for procedural languages. \nIBM Research Report, RC5948, T.J. Watson Research Laboratory, Yorktown Heights, NY., April 1976 14) Rosen, \nB. K. Data flow analysis for recursive PL/I programs. IBM Research Report, RC52 11, T.J. Watson Research \nLaboratory, Yorktown Heights, N. Y., January 1975 15) Spillman, T.C. Exposing side effects in a PL/I \noptimizing compiler. Proc. IFIP Congress 7 I North . , Holland Publishing Co,, Amsterdam, 1971, 376-381 \n16) Unman, J. D. Data Flow Analysis, Second USA Japan Computer Conference, ( 1975) 17) Urschler, G. Complete \nredundant expression elimina\u00ad tion in flow diagrams. IBM Research Report, RC4965, T.J, Watson Research \nLaboratory, Yorktown Heights, NY., August 1974 \n\t\t\t", "proc_id": "512950", "abstract": "This paper presents a systematic approach to the problem of generating good code with a compiler that is easy to construct. A compiler structure is proposed which relies on interprocedural data flow analysis, global optimization, and an intermediate language schema to simplify the task of writing the code generating portions of a compiler without sacrificing code quality. This structure is contrasted with a more conventional structure to explore the reasons why the new structure solves several problems inherent in the conventional structure. Further advantages which accrue from the new structure are also presented.", "authors": [{"name": "William Harrison", "author_profile_id": "81385594269", "affiliation": "IBM Thomas J. Watson Research Center, Yorktown Heights, New York", "person_id": "PP74022670", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/512950.512954", "year": "1977", "article_id": "512954", "conference": "POPL", "title": "A new strategy for code generation: the general purpose optimizing compiler", "url": "http://dl.acm.org/citation.cfm?id=512954"}