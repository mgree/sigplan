{"article_publication_date": "01-01-1977", "fulltext": "\n Permission to make digital or hard copies of part or all of this work or personal or classroom use is \ngranted without fee provided that copies are not made or distributed for profit or commercial advantage \nand that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, \nto post on servers, or to redistribute to lists, requires prior specific permission and/or a fee.&#38;#169; \n1977 ACM 0-12345-678-9 $5.00 An Interprocedural Data Flow Analysis Algorithm+ Jeffrey M. Barth Computer \nScience Division, EECS University of California, Berkeley Berkeley, California 94720 Abstract: A new \ninterprocedural data flow analysis algo\u00adrithm is presented and analyzed, The algorithm associates with \neach procedure in a program infor\u00admation about which variables may be modified, which may be used, and \nwhich are possibly preserved by a call on the procedure, and all of it subcalls. The algorithm is sufficiently \npowerful to be used on recursive programs and to deal with the sharing of variables which arises through \nreference parame\u00adters. The algorithm is unique in that it can com\u00adpute all of this information in a single \npass, not requiring a prepass to compute calling relation\u00adships or sharing patterns. A lower bound for \nthe computational complexity of gathering interpro\u00adcedural data flow information is derived and the algorithm \nis shown to be asymptotically optimal. The algorithm has been implemented and it is practical for use \neven on quite large programs. Introduction: A great deal of recent research has been devoted to developing \nalgorithms for intrapro\u00adcedural global flow analysis. [6,9)15,16,19] These methods generally assume that \nthe semantics of individual program statements are available. The purpose of global flow analysis is \nto determine what information is available at specific program statements by propagating the semantics \nof indivi\u00addual statements through the program in a manner which reflects the control structure. In practical \napplications of global flow analysis, such as pro\u00adgram optimization, verification, and documentation, \nsubroutine calls appear interspersed among program statements. The semantic effects of subroutine calls \nare not generally available. Traditionally, optimizers have treated calls as demonous black boxes which \nterminate the propagation of informa\u00adtion. The aim of interprocedural data flow analysis is to summarize \nthe semantic effects asso\u00adciated with subroutine calls, permitting global flow analysis to more effectively \npropagate infor mation through programs. We will motivate the study of interprocedural data flow ana,lysis \nwith an example code sequence: x := (y/z) + w; u := SOMEFUNCTION(x); v := y/z; +Research sponsored by \nNational Science Foundation Grant MCS74-07644-A02 Whether y/z can be considered a common subexpres\u00adsion \ndepends on the information that SOMEFUNCTION can not modify the values of y and z. This allows the information \nMy/z is available to be propagated through the call on SOMEFUNCTION. There are a wide variety of applications \nfor interprocedural data flow analysis, some of which are discussed in [5]. Interprocedural information \nthat is used at the point of call of a subroutine has been called fsummary t data flow information. [11] \nWith each function call, a summary of the variables that may be modified, that may be read, and that \nare possi\u00adbly preserved will be useful for intraprocedural analysis. [14] Precise definitions for these \nthree data flow problems will appear in the section on notation and problem definition. There are three \nfundamental difficulties asso\u00adciated with gathering summary information. In order to summarize a procedure, \nP, its body is examined. If P calls another procedure, Q, then the flow analysis of P requires a summary \nof Q. In nonrecursive programs, there is some ordering in which procedures can be examined which has \nthe pro\u00adperty that called subroutines are always analyzed in advance of the procedures which call them, \nThis ordering has been called the reverse invocation order by Allen. [2] In the case of recursive programs, \nthere is no ordering with this property. Thus, an interprocedural data flow analysis algo\u00adrithm which \nis to be used on recursive programs must include some method of circumventing this dif\u00adficulty. The second \nfundamental problem that an inter\u00adprocedural data flow analysis algorithm must cope with arises in programming \nlanguages which allow name or reference parameters. [10,12] Since these mechanisms introduce storage \nsharing among dif\u00adferent variables, called aliases, the determination of which variables may be modified \nby a statement is nontrivial. Suppose that r is a reference parameter in a procedure P which contains \nr :. u+v; as a statement. The summary information for P must reflect the fact that any variable potentially \nshared with r may be modified. It is fairly obvious that sharing can be analyzed by a prepass over the \nprogram. If a list of variables which are poten\u00adtially shared with r is available, the processing of \nthe above statement is no longer problematical. In order to perform the data flow analysis in a single \npass over the program this strategy is inadequate since there is no reason to believe that Rosen s method \nis the most highly sophisticated, all the aliases of r will have been exposed before capable of computing \narbitrary flow problems with a a Particular statement which modifies r is pro\u00ad high degree of precision, \neven in programs with cessed. complex sharing patterns. Another difficulty associated with gathering \nsummary information for recursive programs is in the correct treatment of variables. It is under\u00adstood \nthat separately declared variables are dis\u00adtinct even if their spellings coincide, but there is a more \nintrinsic problem. Recursive invocations of a procedure, P, have separate copies, called incarnations, \nOP P!s local variables. High quality summary data flow information for a procedure, Q, will not include \nthe possibility that Q may modify a variable local to P if the incarnation which may be modified is not \nthe same as the incarnation addressable at the point of call. The algorithm to be presented in this paper \nis sufficiently powerful to collect summary data flow information for recursive programs. It can be used \non programs with reference parameters and will deal with different incarnations of variables local to \nrecursive procedures. The data flow analysis technique described is strictly one pass in nature and can \nbe implemented in the first (parsing) pass of a compiler. An implementation is possible which utilizes \nbit vec\u00adtor operations, which are available as word opera\u00adtions on most hardware. The running time of \nthe algorithm is approximately the same as the running time for transitive closure, which is shown to \nbe a lower bound for the problem under reasonable assumptions. The algorithm simplifies in a natural \nway for use in languages with naming structures less general than Algol or for use on programs that do \nnot use recursion. Before embarking on any further study of interprocedural data flow analysis, it must \nbe made clear at what stage of an optimizing compiler this information is gathered. Interprocedural data \nflow information is used to determine that particular optimizing transformations do not affect the correctness \nof the program. Thus , it is necessary first to compute the data flow information and then to apply specific \noptimization. In the case of recursive programs, an entire pass over the program may be necessary to \nexpose the data flow informa\u00ad tion. For these reasons, an optimizing compiler should first compute summary \ndata flow information in a pass, and then attempt to find optimizing transformations in subsequent passes. \n Summary of previous work: The concepts of interprocedural data flow analysis have been developing for \nabout 6 years. The principal algorithms for computing summary information are due to Spillman (1971), \nAllen (1974), and Rosen(1975). [2,14,181 Each of the algorithms uses a different computational strategy. \nIn addition to this difference, each algorithm com\u00adputes somewhat different information. Spillman s technique \nis only suitable for computing !Imodifiesl! information, but does so for the complete PL/I language.. \nAllen s method is usable for computing !imodifies!i, !Iuses!!, and IlpreserveStr information, but is \npoorly suited for use on recursive programs. These techniques span a continuum of computa\u00adtional expense \nand expected quality of computed information. Which technique is most appropriate for a particular application \nwill depend on the value placed on the quality of information com\u00adputed. The present technique is computationally \nat least as simple as the easiest of the algorithms which preceded it. It does not produce information \nas precise as Rosen!s, and so will not be suitable for the most demanding of applications (verifica\u00adtion) \n. It does, however, attempt to stretch as far as possible the precision of calculated information within \nthe constraints of computational efficiency. The oldest interprocedural data flow analysis technique \nof which the author is aware was developed by Thomas Spillman. [18] His paper, published in 1971, deals \nwith the issue of exposing side effects of PL/I statements. Thus , the major emphasis of the work is \ndirected toward determining the patterns under which variables share storage. Included $n this analysis \nare language features such as call by reference parameters, pointer vari\u00adables (possibly with offsets), \nand ON conditions. His techniques are suitable for determining the side effects of procedure call statements \n(itmodi\u00adfies summary information) and for this reason are within the scope of interprocedural data flow \nanalysis. Spillman s method works essentially as fol\u00adlows : Each procedure is codified into a bit vector \nin which, loosely speaking, a bit represents the information whether the variable corresponding to its \nposition is modified in the procedure. Bit vectors for procedures are merged into one another in the \nnreverse invocation order (a procedure is not processed if possible until all procedures which it invokes \nare processed). Then, procedures are processed in the invocation order (reverse of above) to account \nfor aliasing effects. If recur\u00adsion is detected in the program, the above steps are iterated Until the \nbits stabilize. Cosmeticly, Spillman s method is the most similar to the techniques presented in this \npaper. We will use a similar representation of informa tion, but will be able to achieve substantially \nbetter results in the computation of modifies information. In addition, we will use this kind of representation \nto compute arbitrary data flow prob\u00adlems, which would not have been possible within the conceptual framework \navailable to Spillman. Although Spillman s method only requires a single pass over the program, it is \nnot suitable for com\u00adputing arbitrary interprocedural data flow informa\u00adtion. The interprocedural data \nflow analysis algo\u00adrithm developed by Frances Allen is the most widely used. [2,8] Originally, the algorithm \nwas limited to nonrecursive programs for which there is guaranteed to be a reverse invocation order. \nEach procedure is subjected to global flow analysis which determines its data flow properties given the \nknown data flow information associated with each statement. Allen and Schwartz have extended this basic \nalgorithm to handle recursive programs. [3] The essential idea is that a procedure, P, which calls another \nprocedure, Q, can be analyzed before Q if one is willing to make worst case assumptions about the data \nflow impact of Q. The information can be refined by reanalyzing P after a better approxima\u00adtion for Q \nhas been computed. AilCn does not discuss the problem of dif\u00adferent incarnations of variables in recursive \npro\u00adgrams. Her techniques do not naturally handle sharing of variables because the order in which procedures \nare examined is an order in which bind\u00adings at calling sites are always discovered after the called procedure \nhas been analyzed. The prepass which gathers the calling information can be used to collect sharing information \n[18], which allows Allen s method to base its actions on the most general sharing pattern for each given \nvari\u00adable . Other major work in interprocedural data flow analysis has been done by Rosen. [14,171 His \nmethod works on recursive programs and produces very precise information even in cases in which the sharing \npatterns of variables vary dependinp on the context. Unfortunately, his method is probably impractically \nslow and complicated for all but the most demanding of applications. The basic idea exploited by Rosen \nis that when analyzing (using global flow analysis) a procedure, P, with a call on a subprocedure, Q, \nthe information impact of Q can be exprwsed in a for!wla which summaries P. That is, the ~~]mmary o? \nP is m eruatlon with unk\u00adn owns for subprocedures called in P. Analyzing a program yields a set of eqllations \nwhich can be solved sj.mult.anmusly to obtain useful. rl?ta flow information. Sharing is handled by allowing \nformulas to be parametric, in some sense, in a sharing pattern. When an equation is used to summarize \ninformation at a point of call, the particular bjndings at. tb?t point of call are used to refine the \nformula. Incarnations of variables can be kept apart by renaming variables in eqllat.ions a. they are \n~a~~ed back into the procedure which created them. The technique used by Rosen to solve the simultaneous \nequations is a bit reduction strategy, in which maximal information is at first assumed. The equations \nare substituted into each other until the information, which is monotonically decreasing, stabilizes. \nHe proves that the information thus obtained is correct. A characteristic of this method is that the \niteration must be fully carried out, because partial solutions are incorrect until complete stabilization. \nRosen proposes that the equations may be simplified in advance by using various symbolic execution strategies \non the pro\u00adgram. Notation and Problem Definition: The approach taken to computing summary data flow information \nby this author is that the process is essentially one of computing relations over the domain of procedures \nxvariables. For example, ,,m~difiesll is a relation between procedures and the variables possibly modified \nas a summary effect of their invocation. The interprocedural data flow relations are comDuted by composition \nand transi\u00adtive closure of other relations which are easy to construct from the source program. These \neasy to construct relations are referred to as direct relations since they are constructed directly from \nthe program without considering subcalls. Rela\u00adtions which represent summary data flow information are \nreferred to as summary relations. The manner of presentation of the techniques will be to define various \nrelations. The notions of correctness and precision will then be supplied for summary relations . Formulas \nwill show how to compute summary relations from direct relations. The correctness of the summary relations \nwill be verified based on the definitions of the direct relations and the computational formula specified. \nIt will remain to show that the direct relations can be computed to satisfy their definitions easily. \nThis can be accomplished by illustrating an implementation of the data flow techniques (which is only \nvery briefly sketched in this paper) . It will be convenient to introduce some nota\u00adtional conventions \nwhich will be used throughout this paper. A formal definition of the summary data flow information that \nwe wish to compute will then be presented. We are considering the summary data flow prob\u00adlem for programs \nin Algol-like (static naming structure) languages. A Drogram is understood to be reasonably well formed, \ni.e. there is no inac\u00adcessible code. Subroutines are called explicitly at callin~ sites which are textually \nvisible, rather than being activated by the existence of some con dition. A variable is a declared entity \nwhose storage may be associated with subroutine entry. That is, if a procedure is called recursively, \na particular variable may be associated with several storage locations. Each such location is called \nan inCaMIatiOt3 of the variable. Def: Let ~ be the set of procedures in .a program to be analyzed. For \nthe exam\u00adples, assume that P, Q, R, and S are members of PP. Def: Let ~ be the set of variables in the \nprogram. For the examples, assume that r, u, v, w, x, y, and z are members of Vv. Separately declared \nvariables are distinct even if their spellings coin\u00adcide . Def: A (binary) relation is a set of or\u00addered \npairs. In accordance with standard notation, for relations A and B: t A* is the ref_le~i.,re t~~nsitive \nclosure of A, A+ is the transitive closure of A, A B is the composition of A and B, A is the set of pairs \nnot in A, A and B is the set of pairs in both A and B, A or B is the set of pairs in either A or B, TRANS(A) \nis the transpose of A, that is (a,a ) G A iff (a ,a) G TRANS(A), and (a,a ) G A is written interchange\u00adably \nwith a A a! or (a,af) in A. Def: Let ~ be a relation defined on PP x PP. A pair (P,Q) is in CALL if pro\u00adcedure \nP contains a call on procedure Q. Def: Let MUSTCALL be a relation defined on PP x PP. A pair (P,Q) is \nin MUSTCALL if procedure P contains a call on pro\u00adcedure Q on all paths of execution for which P terminates \nnormally. That is, a call on P must always be followed by a call on Q before P returns to its calling \nsite. timay 11 The notion of rmusttt in contrast to which distinguishes MUSTCALL from CALL is suffi\u00adciently \nimportant to warrant careful exposition. A !Imaytl relation which contains information only expresses \nthe possibility that the action suggested by the relation name will occur in the executing program. For \nexample, (P,Q) G CALL states the pos\u00adsibility that P may call Q. In contrast to this, [Imustl! information \nis appropriate when a certainty about the executing program is intended. If (P,Q) G MUSTCALL then the \ncall on Q must be executed as a subcall of P (in the case of normal termination of P for which summary \ndata flow information is mean\u00adingful). May information is usually used in the nega\u00adtive sense. The fact \nthat (P,Q) C CALL consti\u00adtutes definite information that P will not directly call Q. The fact that (P,Q) \nS CALL does not imply that P really will call Q, and so is of limited use for extracting important information. \nDef: Let DIRECTMOD be a relation defined on PP x VV. A pair (P,x) is in DIRECTMOD if procedure P contains \na statement which modifies the value of variable x. (may information) Def: Let DIRECTUSE be a relation \ndefined on PP x VV. A pair (P,x) is in DIRECTUSE if procedure P contains a use of the variable x, (may \ninformation) Def: Let DIRECTNOTPRE be a relation de\u00adfined on PP x VV. A pair (P,x) is in DIRECTNOTPRE \nif procedure P does not preserve the value of variable x on any path of execution for which P terminates \nnormally. That is, x must be set by an invocation of P. (must information) All of the above relations \nare direct in the sense that they contain information about effects of procedures ignoring indirect effects \ndue to sub\u00ad calls. The remaining relations to be defined con\u00ad tain summary information about the effects \nof pro\u00ad cedures and associated subcalls. Def: Let ~, ~, and ~ be relations defined on PP x VV which \nare the summary information that we wish to calculate: If (P,x) G MOD then procedure P, and any subcalls \nof P, will not modify the value of the variable x. If (P,x) S USE then procedure P, and any subcalls \nof P, will not use the variable x. If (P,x) G PRE then before P returns, the variable x will have been \nassigned. MOD, USE, and PRE are relations which contain may information. It will be more convenient to \ncalculate preserves information as must informa\u00adtion , so we define an additional summary data flow relation: \nDef: Let NOTPRE be a relation defined on PP x Vv. A pair (P,x) G NOTPRE implies that before P returns, \nthe variable will have been assigned. (NOTPRE is sim\u00adply PRE.) The above definition of USE is slightly \ndif\u00adferent from what is conventionally calculated in summary data flow analysis. Under this definition, \nif a variable is read it is considered a use!! of the variable. Traditionally, it would only have been \nconsidered a Iuserl if the value read from the variable could have been the value at the point of call. \nObviously, if a pair (P,x) S USE then the value of x is also not used as a summary effect. Thus, USE \ncan be substituted for traditional uses information in any application. Having made the definition this \nway, however, prevents certain pairs from being eliminated from USE. Specifi\u00adcally, consider the case \nin which a variable must be assigned before each program read action on it. By these definitions, there \nis no way to say that the variable is used, but not its value. It is understood that Summary information \nmust be correct with respect to any particular point of call of a procedure. The summary data flow rela\u00adtions \nare defined on PP x VV, where VV is the set of addressable incarnations of variables at the point of \ncall. The following example illustrates why this interpretation of summary information is critical: F- \n Declare x; Q P(); [ x unconditionally assigned by P; Q(); It is wrong to conclude that P NOTPRE x at \nits call within Q, since the incarnation of x which is addressable at this site is different from the \nincarnation assigned by the call on P. The relations defined above, and several to be introduced in later \nsections, are summarized in Table 1. Interprocedural data flow analysis is essen\u00adtially the process of \ncomputing the summary data flow relations. We must be somewhat careful in specifying what is expected \nof an interprocedural data flow analysis technique because the defini\u00adt ions of the relations do not \nexclude trivial (and useless) solutions. Several evaluation criteria of data flow analysis techniques \nwill be necessary. Def: We say that a summary data flow re\u00adlation, A, is correct fialarticular calling \n~ for a procedure, P, if: For may information -There is no instance of the following three conditions \noccurring simultaneously for any vari\u00adable, x: i. x is addressable at the calling site ii . (P,x) C A \niii, The call on P may have the summary effect A on the incarnation of x addressable at the time of call. \nFor must information -There is no instance of the following three conditions occurring simultaneously \nfor any vari\u00adable, x: i. x is addressable at the calling site ii. (P,x) GA iii. the call on P may fail \nto have the summary effect A on the incarnation of x which was address\u00adable at the time of the call. \nDef: We say that a summary data flow re\u00adlation, A, is correct if it is correct at every calling site \nin the program. We wish to define the precision of a calcu\u00adlated relation to capture the concept of the \namount of definite information available. In particular, for may information, the sparser the relation, \nthe Def: We say that a correct summary data flow relation is more ~recise than anoth\u00ader correctly calculated \nversion sf that relation if: more effects are known not to be possible. Rela\u00ad tions have a natural partial \nordering by set inclu sion. We define precision to be a meaningful com\u00ad parator between related elements \nin the partial order. For may information -the more precise relation is a subset of the pairs of the \nless precise relation. For must information -The more precise relation is a superset of the pairs of \nthe less pre\u00adcise relation. The notion of correct does not exclude the trivial solutions of all pairs \nfor may information and no pairs for must information. These solutions are correct, but are usually too \nimprecise to be use\u00ad ful . It is too stiff a criterion to ask for the most precise solution for a summary \ndata flow rela\u00adtion, since the determination of this will be unde\u00adcidable in general. Def: We say that \na summary relation is m ecise UD tO svmbolic execution if For may information -it is the most precise \ninformation possi\u00adble assuming that all condi\u00adtionally executed code is exe\u00adcutable and that all the \nvari\u00adables in the program are spelled distinctly. For must information -it is the most precise information \npossible assuming that any path of execution through procedures is possible and that all vari\u00adables in \nthe program are spelled distinctly. The condition pertaining to pairwise distinct vari\u00ad able spellings \nremoves a degenerate case in several Table 1 Direct/Summary/ Relation Domain Program Independent CALL \nPP x PP Direct May MUSTCALL PP x PP Direct Must DIRECTMOD PP x Vv Direct May DIRECTUSE PP x Vv Direct \nMay DIRECTNOTPRE PP x Vv Direct Must MOD PP F Vv Summary May USE PP x Vv Summary May PRE PP x Vv Summary \nMay NOTPRE PP x Vv Summary Must SCOPE PP x Vv Program Independent GENSCOPE PP x Vv Program Independent \nAFFECT Vv x Vv Direct May proofs, and will be explained in the first such instance. The part of the definition \npertaining to con\u00additional execution is somewhat different for may and must relations. Clearly, if all \npaths through a procedure are executable (the must condition), then all conditionally executed code is \nexecutable (the may condition). The converse is not true. Consider this procedure: P Declare w; Comment: \nw is local so that the assignments can t affect its value; [ L w := someexpression; IF W=OTHEN X := \nu+l ELSE Y := v+2; IF W= OTHEN y := u+3 ELSE X := v+4; Even assuming that all conditionally executed \ncode is executable, it would still be possible to con\u00adclude that neither x nor y is preserved by P (by \nsymbolically merging the THEN and ELSE parts of the conditional statements). We wish, however, to con\u00adsider \nmust information precise up to symbolic exe\u00adcution without requiring this kind of analysis. Ultimately \nwhat is required of an interpro\u00adcedural data flow analysis algorithm is that it produce provable correct \nrelations which are empir\u00adically sufficiently precise. When a technique is precise up to symbolic execution \nwe may be confi\u00addent that the information produced is of very high quality. Specific results of testing \nthe tech\u00adniques developed here will be included in [5]. Properties of MOD, USE, and PRE: The algorithm \nwhich will be presented computes MOD and USE as may information, but will compute NOTPRE as must information. \nTo obtain PRE, the complement of NOTPRE is calculated. This section will partially justify the use of \na different tech\u00adnique for computing PRE. There is a means of com\u00adputing PRE directly using a variation \nof the algo\u00adrithm presented here. An account of this appears in [5]. Although MOD a~d PRE are both may \ninformation, the manner in which they are collected intrapro\u00adcedurally differs. Consider straightline \ncode: x := u+ 1; Y:= v+1; ,, The first statement modifies x and the second statement modifies y. The \nMOD information for the two statements together is the union of the infor\u00ad mation for each statement: \nthey modify both x and The first statement may preserve all variables except x. The second may preserve \nall variables except y. The PRE information for the two state\u00ad ments together is the intersection of \nthe preserves information for each statement: they ma!? preserve all variables except x and y. In pictures, \nthis is summarized: Y. o Q Now consider code which is conditionally executed: IF booleanexpression THEN \nx := u + 1 ELSEy :=v+1; In this case the union of the may information for the internal statements is \nthe information for the entire statement for both MOD and PRE. In pictures this is summarized: ~yy MOD \nPRE For purposes of this paper, it must suffice to say that the algorithm presented has a hidden assumption \nthat the information composition func\u00adtion for straightline code is union. NOTPRE satis\u00adfies this requirement, \nwith the disadvantage of being must information. All forms of must informa\u00adtion require intersection \namong conditionally exe\u00adcuted statements: py )/ NOTPRE NOTPRE Calculating MOD and USE, no sharing: \nFor simplicity in this section, we assume that there is no mechanism, such as call by reference parameters, \nfor introducing sharing among vari\u00adables. Formulas which use CALL, DIRECTMOD, and DIRECTUSE to calculate \nMOD and USE are presented. A series of formulas will be presented which calcu\u00adlate summary data flow \nrelations to differing lev\u00adels of precision. In order to distinguish the com\u00adputed relations, we will \nwrite, for example, MOD/1.l to be the MOD relation as calculated by formula 1.1. Only the formulas for \nMOD are justi\u00adfied since the arguments in both cases are essen\u00adtially the same. Correct formulas for \nMOD and USE are easy to obtain: MOD := CALL* DIRECTMOD (1.1) USE := CALL* DIRECTUSE (1.2) Claim: MOD/1.l \nis correct. Justification: Since MOD is may informa\u00ad tion, we must justify the absence of any pair, \n(P,x), missing from the computed relation. Suppose that P-(CALL* DIRECTMOD) x. This says that P, and \nall procedures callable from P (directly or indirectly), do not contain a statement which modifies x. \nFrom this we conclude that P, in summary, does not modify x. Although the above formulas are correct, \nthey are extremely conservative in their treatment of variables. If any incarnation of x may be modified \nby a subcall of P, then P MOD/1.l x is calculated. If the incarnation of x which is modified by the subcall \nmust be. different from the one addressable at the point of call of P, then the pair (P,x) could have \nbeen eliminated from MOD, increasing the precision of the information. Refined formulas will be obtained \nfor MOD and USE by including Algol scoping rules in the calcu\u00adlations. The techniques presented can be \nmodified to accommodate languages with less general static naming structure. Def: The level of @ yrocedure \nis the static depth at which the procedure is defined. Declarations which occur any\u00adwhere within a procedure \nwill be associ\u00adated with the procedure invocation, rath\u00ader than with BEGIN block entry. The level of \n~ variable is the same as the level of the procedure to which it is lo\u00ad cal. (This differs somewhat from \nstan\u00ad dard Algol terminology.) Global vari\u00ad ables are at level O, the lowest naming level. When comparing \nlevels, it will usually be clearer to refer to lower levels as outer and higher levels as inner. Def: \nLet SCOPE be a relation defined on PP x Vv. A pair (P,x) is in SCOPE iff the level of x is strictly lower \nthan the level of P. That is, x is declared at a outer level from P. SCOPE is weaker than an addressability \nrelation, since it is possible that P SCOPE x, even though x is not addressable within P. It is shown \nin [51 that using SCOPE rather than an addressability relation does not degrade the calculated summary \ndata flow information. The following lemma will be used repeatedly in justifying formulas involving scoping \nrules: ScoDing Lemma: When computing summary data flow information, a call (direct or indirect) on a \nlevel n procedure, P, and all of its subcalls, can affect addressable variables at the original point \nof call only at levels O thru n-1. Proof: P, and its subcalls, may be able to address variables at levels \nwhich exceed n-1, but the variables at these levels will be new incarnations and are not addressable \nat the original calling site. Under the rules of static scoping, when a procedure is called, the variables \nwhich are addressable in the called pro\u00adcedure are a subset (not necessarily proper) of those addressable \nat the cal\u00adling site, plus new incarnations of local variables. In the body of P, the levels O thru n-1 \ncontain variables addressable at the original calling site, and level n contains new incarnations of \nlocal vari\u00adables for P. By applying the previous observation inductively, subcalls of P (direct or indirect) \ncan only affect a subset of the variables addressable in P plus new incarnations of local variables \nwhich were not addressable at the origi\u00ad nal calling site. Note that the above lemma is false in the \npresence of reference parameters. Formulas 2.1 and 2.2 produce summary data flow information which is \nmore refined than that pro\u00adduced by formulas 1.1 and 1.2: MOD := CALL* (DIRECTMOD and SCOPE) (2.1) USE \n:= CALL* (DIRECTUSE and scopE) (2.2) Claim: MOD/2.l is correct. Justification : (omitted, see [51). \n Formulas 2.1 and 2.2 reflect the observation that actions on local variables never affect sum\u00admary data \nflow information. These equations are sufficiently powerful to process programs in languages which do \nnot allow the nesting of naming levels except to distinguish locals and globals. For such languages the \nrelations computed are pre\u00adcise up to symbolic execution, a fact that will follow as a corollary to a \nmore general statement which is proven later in this section. SIMPL-T, C, and BLISS are languages which \nenforce this naming limitation. [8,13,201 A series of examples which illustrate program skeletons will \nbe useful in developing intuition as to the precision of various formulas used in calcu\u00adlating summary \ndata flow relations. We begin with two examples for which formulas 2.1 and 2.2 are more precise than \nformulas 1.1 and 1.2. For all the examples, the reader is to assume that subroutine calls are executed \nconditionally so that nonterminating recursion is avoided. P Declare x; x is modified; P(); [ Example \n1 The call on P does not modify the addressable incarnation of x at the point of call. This is the important \ncase of direct recursion. F These formulas produce precise data flow informa- Declare x; tion for all \nof the above examples. R Claim: MOD/4.l is correct. P(); [ Justification: (omitted, see [5]). x is modified; \nI FI(); Although which it is formulas possible 4.1 to construct and 4.2 are not examples precise for \nup to L symbolic execution, these formulas are the ones Example 2 recommended for use in practice. Formula \n4.1 fails to be completely precise on this ~omplicated exam- Here too, since x is local to P, ple: p \n (DIRECTMOD and scoPE) X. This example will fig\u00ad ure in a later discussion. Formulas 2.1 and 2.2 fail \nto produce data flow Q information which is precise up to symbolic execu Declare x; tion for the following \nexample:  P so; L[ Lr Q Declare x; R x is modified; x is modified; [ R(); R(); P(); P(); [ Q(); Q(); \nThe call on P within Q can not modifv the address-Example 3 able x thru the call on S but: The call on \nP within Q can not modify the currently P CALL* R, addressable incarnation of x as a consequence of R \n(DIRECTMOD and scorn) X, the scoping lemma. Plugging into formula 2.1, and P SCOPE x P CALL Q so P MoD/4.l \nx. Q CALL R R (DIRECTMOD and scopE) x Completely characterizing the effects of scop\u00ad ing rules on MOD \nand USE information will result in shows that P MOD/2.l x. We generalize from this formulas for them \nwhich are computationally less example to produce these formulas: efficient. It will be convenient to \nintroduce some notation for this characterization. MOD := (CALL* DIRECTMOD) and SCOPE (3.1) USE :. (CALL* \nDIRECTUSE) and SCOPE (3.2) Def: A call chain is an ordered sequence of procedures which are pairwise \nin the Formula 3.1 produces information for example 3 CALL relation. Thus , P CALL* S as a which is completely \nprecise since P SCOPE x. It result of P CALL Q, Q CALL R, and produces the same information as formula \n2.1 on R CALL S results in P,Q,R,S as a call example 1. Unfortunately, it fails to be as pre-chain. cise \nas formula 2.1 on example 2. Before develop\u00ading formulas which are completely precise on all of these \nexamples, we pause to prove the correctness Def: Let the call chain level be the of the third set of \nformulas. level of the outermost procedure in the call chain. In the example above, the Claim: MOD/3.l \nis correct. call chain level is min(level P, level Q, level R, level S). Justification: (omitted, see \n[5]). Def: Let MAXCHAINLEVEL be a !PP~ x /PPi The desirable properties of all of these for-matrix of \nintegers where rows and columns mulas can be combined: are selected by supplying procedure names. MAXCHAINLEVELIP,Q] \nis the maximum MOD := (CALL* (DIRECTMOD and SCOPE)) call chain level for all call chains from and SCOPE \n(4.1) P to Q. usE := (CALL* (DIRECTUSE and s120PE)) and SCOPE (4.2) Calculating MOD and USE can be accomplished \nwith these formulas: MOD := { (P,x) ! For some Q G PP, Corollary: For languages like BCPL which do not \nP CALL* Q, Q DIRECTMOD x, and allow the nesting of procedures, formula 2.1 is level x < MAXCHAINLEVELIP,Q] \n] (5.1) precise up to symbolic execution. USE := { (P,x) \\ For some QGPP, P CALL* Q, Q DIRECTUSE x, \nand level x < MAXCHAINLEVELIP,Q] } (5.2) The intuition which justifies the use of the maximum chain \nlevel in the formulas is that one may only be certain that some pair, (P,x), is absent from the computed \nMOD if the call chains which result in the modification of x all involve the call of a procedure at a \nlevel at least as low as x. If a call chain of maximum level contains a procedure at a level as low as \nx, then all other call chains must also. Claim: MOD/5.l is correct. Proof: Suppose that P MOD/5.l x. \nFor a contradiction , assume that P does modify the addressable x from some point of call. Since P can \nmodify x, there is some call chain, C, beginning with P, and ending with the procedure, Q, which directly \nmodifies x. The call chain lev\u00adel of C <. MAXCHAINLEVELIP,Q] by the de\u00adfinition of MAXCHAINLEVEL. The \nmodifica\u00adtion by Q of the same incarnation of x which is addressable at the call of P is only possible \nif level x < call chain level of C (by the scoping lemma). Thus , the level of x < MAXCHAINLEVELIP,Q] \nand all three conditions of formula 5.1 are satisfied, a contradiction. Claim: Formula 5.1 calculates \nMOD precisely up to symbolic execution. Proof: We must show that the elimination of any pair, (P,x), \nfrom MOD/5.l results in an incorrect summary relation ( assum\u00ading that all paths of conditional execu\u00adtion \nare executable) . Suppose that P MoD/5.l x. Look at any calling site for P. We know that there is some \ncall chain beginning with P and ending with a procedure Q which modifies some incarna\u00ad tion of x. Since \nall paths of condition\u00adal execution are executable, we may as\u00adsume that P calls Q through a call chain \nof maximal level. Since the level of this call chain exceeds the level of x, we know that no incarnation \nof x is created between the time P is called and the time Q modifies some incarnation of x. We also know \nthat the variables ad\u00addressable at the level of x from the cal\u00adling site are the same as the variables \naddressable at that level from Q (or in particular, not only is x addressable at the calling site of \nP , but it is the same incarnation of x which is modified by Q). We have proven that eliminating (P,x) \nfrom MOD/5.l produces incorrect in\u00adformation. (1) (1) If the spelling of x were not distinct from all \nother variables, it might be possible that at the calling site x IIis on the run time stackit but is \nnot addressable because a more local Proof: All call chains in such languages have a call chain level \nof 1. SCOPE selects effects on global (level O) vari\u00ad ables. It follows that all calculated pairs satisfy \nthe three conditions of formula 5.1. Having studied formulas for MOD and USE which are precise up to \nsymbolic execution, we are in a position to arque convincingly that formulas 4.1 and 4.2 should empirically \nbe good heuristic methods. Here once again are those formulas: MoD := (CALL* (DIRECTMOD and SCOPE)) and \nSCOPE (4.1) USE :. (CALL* (DIRECTUSE and SCOPE)) and SCOPE (4.2) We will claim, without proof that formula \n4.1 is the same as MOD:= { (P,x) i For some QC PP, P CALL* Q, Q DIRECTMOD x, and level x < min(level \nP, level Q) ] Thus, formula 4.1 differs from the chain level cal\u00adculation only in cases in which MAXCHAINLEVELI \nP ,Q] < min(level P, level Q). What this equation says is that if the highest level chain (innermost) \nfrom P to Q must go through some procedure less deeply nested than either P or Q, then formula 4.1 fails \nto be as precise as formula 5.1. This is a somewhat pathological condition which one can expect to arise \nrarely in practice. This matter has been studied empirically and the results are presented in [5]. Calculating \nNOTPRE, no sharing: It is possible to obtain formulas for calcu\u00adlating NOTPRE from MUSTCALL and DIRECTNOTPRE. \nIt turns out that the calculation of this summary data flow information involves very different considera\u00adtions \nthan those which applied to MOD and USE. An account of these methods appears in [5]. Sharing, MOD and \nUSE : The task of collecting summary data flow information is made somewhat more difficult by the introduction \nof reference parameters into the pro\u00adgram which is to be analyzed. A simple assignment statement like: \nx := u+ 1; can affect variables other than x. These aliasing effects happen in two,distinct ways, which \nwe name for future reference: _ _ variable has the same spelling. ... LL I Q Declare x; r I rPreference \nformal r) I r is modified; L I P(x); L Refmod Effect Modifying a reference Parameter results in the \nmodification of the actual parameter bound to it. In this example, we must determine that P MOD x. Q \nDeclare x; r I rR(reference formal r)  ~1c P x is modified; P(); R(x); Varmod Effect Modification \nof a variable may result in the modif\u00adication of reference parameters. Here, P MOD r must be deduced. \nIn this section, we will study sharing effects on the MOD relation. It should be understood that all \nthe reasoning applies equally well to USE information. The relation that will enable us h comPute summary \ndata flow information in the presence of sharing is: Def: Let AFFECT be a relation defined on Vv x Vv. \nA pair (r,x) is in AFFECT iff formal reference parameter r is directly bound to actual parameter x at \nsome point of call. A formula for MOD can now be obtained from formula 1.1 which will be correct in \nthe presence of reference parameters: MOD := CALL* DIRECTMOD AFFECT* TRANS(AFFECT)* (12) Intuitively, \nrefmod effects are accounted for by AFFECT* and varmod effects are computed by TRANS(AFFECT)*. (2) To \naid in the correctness proof of formula 12, and for subsequent formulas in this section, we use a lemma \nwhich requires the intro\u00adduction of a few new terms. Def: Let {x \\ r AFFECT* x} be called the set of \nactuals which may be aliased to r. (~) Formulas are numbered to be consistent with [5], thus formulas \n6 through 11 do not appear in this paper. Def: Let {r ~ x TRANS(AFFECT)* r} be called the &#38; ~ formals \nwhich maY be aliased to x. The lemma will attempt to formalize a rather simple idea which is best understood \nby looking at a series of diagrams. Consider nodes of these graphs to represent variables and directed \narcs to represent endpoints in the AFFECT relation. (A reverse arrow represents endpoints in the TRANS(AFFECT) \nrelation.) x Here we see a graph which is induced by a program in which formal reference parameters \nr and u are bound to actual parameter x. If both r and u are bound to x simultaneously, modifying r modifies \nr, x, and u. (The proof of the lemma contains an example of a program in which this occurs.) v x This \ngraph represents a program in which formal parameter r takes either x or y as its actual parameter. There \nis no way to bind both x and y to the same incarnation of formal parameter r, hence modifying x can not \nresult in the modification o-f Y. Aliasin~-: Altering a variable, r, may modify its set of actuals, and \nvariables which are in the sets of formals of these actuals. No other vari\u00ad able may be modified as a.sharing \neffect of the modification of r. Proof: The examples which illustrate rei%od and varmod effects prove \nmost of the first sentence of this lemma. The followipg example shows that formals of actuals can be \nmodified through sharing effects: 1[ Preference formal r) r is modified; HR(reference formal u) P(x); \nL-R(x); P modifies r. This modifies x, an actual of r (refmod). Since at the calling site of Pwithin \nR. u is a fonmal of x, u is modified by the call on P. What remains to prove is that no other kinds of \neffects can arise through shar\u00ading. In particular, it must be shown that no other actuals of formals \ncan be modified. That is, if r AFFECT x and r AFFECT U that modifying x will not modify u. At the moment \nthat the program modifies the value of x , it may be bound to some in\u00adcarnation of the reference parameter \nr. This incarnation of r can not be simul\u00adtaneously bound to x, because the incar\u00adnation of r is associated \nwith a particu\u00adlar call of the procedure to which it is a formal parameter. Since x affected r (varmod), \nthis call must have had actual parameter x, not u. The aliasing lemma relates to formula 12 in that \nAFFECT* TRANS(AFFECT)* has the impact of widening the modification of a variable first to its set of \nactuals and then widening this to the sets of formals for these actuals. In a diagram, modifying r results \nin the computation of the pos\u00ad sible m odifications of all the nodes in the shaded region: Claim: MOD/12 \nis correct. Justification: Follows from aliasing lem\u00adma and the proof of formula 1.1. Combining aliasing \neffects and scoping con\u00adsiderations is necessary for practical applica\u00adtions. It will turn out that the \nformulas derived are quite uniform in appearance, but the correct\u00adness arguments are quite different \nat each step of increasing complexity analogous to formulas 2.1, 3.1, and 4.1. For this reason, combining \nscoping and aliasing considerations will be done in stages which parallel the previous presentation of \nscop\u00ading. The notion of SCOPE must be recast because in the presence of reference parameters, the modifica\u00adtion \nof a local variable (formal parameter) may have global effects (see for instance the illustra\u00adtion of \nrefmod effects). (3) Def. Let GENSCOPE be a relation defined on PP x VV which will generalize SCOPE. \nA pair (P,x) is in GENSCOPE iff the level of x is strictly less than the level of P or x is a formal \nreference parameter of some (3) Recent work has resulted in an improved method for combining aliasing \nand scoping con\u00adsiderations. It is similar in spirit to the technique presented here , requires about \nthe same amount of computation, and results in more precise information. It is, however, consider\u00adably \nmore complex to present and obscures the intuitive notions emphasized here. procedure. This differs \nfrom SCOPE in that if x is a reference parameter, the pairs (P,x) are automatically in GENSCOPE for all \nprocedures, P. The first of the formulas which will combine scoping considerations and sharing effects \nfollows: MOD :. CALL* (DIRECTMOD and GENSCOPE) AFFECT* TRANS(AFFECT)* (13) Claim: MOD/13 is correct. \nJustification: (omitted, see [5]). Formula 3 is analogous to formula 2.1 The next equation echoes formula \n3.1: MOD := ( CALL* DIRECTMOD) and GENSCOPE) AFFECT* TRANS(AFFECT)* (14) Claim: MOD/14 is correct. Justification: \n(omitted, see [51). Combining formulas 13 and 14 produces the recommended formula for use on programs \nin languages that allow Algol scoping and reference parameters. MOD := ((CALL* (DIRECTMOD and GENSCOPE) \nand GENSCOPE) AFFECT* TRANS(AFFECT)* (15) Claim: MOD/15 is correct. Justification: (omitted, see [5]). \n Implementation: One of the major advantages claimed for the interprocedural data flow analysis algorithm \ndescribed in this paper is its strictly one ,pass nature. The idea behind the single pass implemen\u00adtation \nis that CALL , MUSTCALL, DIRECTUSE, DIRECTMOD, DIRECTNOTPRE, and AFFECT are easily con\u00adstructed from \nthe program before any interpro\u00adcedural information is available. In particular, since the order in which \nprocedures are examined is unimportant, it is not necessary to construct a call graph in advance of performing \nintraprocedural information gathering. It is also unnecessary to analyze the possible sharing relationships \nin advance, since they have no effect on any of the direct relations. Complexity: The computational complexity \nof the straight forward a bit vector implementation for this algo\u00adrithm is quadratic in IPP[ + !vv/. \nImplementing the algorithm on a machine with operations on words of fixed size results in an algorithm \nof approxi\u00admately cubic complexity. In this section, it will be shown that, under certain reasonable \nassump\u00ad tions, this is asymptotically the best possible algorithm for gatherin~ summary data flow informa\u00ad \ntion. We will consider programs with no sharing and no recursion. In order to rule out gathering trivial \nsummary information (no information), we assume that on a program with no loops, no condi\u00adtionals, no \nlocal variables, and no gotos, an algo\u00adrithm gathers information which is completely pre\u00adcise. The algorjthm \ndescribed has this property for both may and must information. Under these assumptions, it will be shown \nthat computing sum\u00admary information is asymptotically as complex as computing reflexive transit.~ve closure. \nIt is well known that the asymptotic complex\u00adity of computing reflexive transitive closure is the same \nas the complexity of boolean matrix multi\u00adplication. [11 Using a standard matrix represen\u00adtation for \nrelations, the algorithm presented can be made to run asymptotically as quickly as boolean matrix multiplication \nand transitive closure, plus the time necessary to scan the program once. Since the program scan is inevitable \nfor any conceivable algorithm, it will be argued that, at least in theory, the algorithm presented is \nthe fastest pos\u00adsible. The first observation necessary for the reduc\u00adtion to transitive closure is that \nthe computa\u00adtional complexity of computing the reflexive tran\u00adsitive closure of a cycle free graph is \nthe same as the complexity for arbitrary graphs. The reader may find a proof of this fact in the proof \nof Theorem 5.6 in Aho, Hopcroft, and Unman, although they do not state this fact. [1] Let M be an adjacency \nmatrix for some acyclic graph. The computation of the.reflexive transitive closure of M, M*, can be embedded \nin any of the summary data flow problems. We will produce a non\u00adrecursive program with the property that \nMOD for the program is an interpretation M*. The program consists of procedures, Pi, which are of the \nfol\u00adlowing form: r Pi xi is modified; L Call every procedure Pj for which M[i,j].l;  It is quite obvious \nthat Pi MOD x. iff M*[i,j] . 1. The program is nonrecursive b~cause the graph represented by M is acyclic. \nSuppose that M is an nxn matrix. The program constructed has :PPI = ivv! . n so the complexity of computing \nMOD expressed in IPPI + Ivv! is at least as great as the complexity of computing M* expressed in n (UP \nto a constant factor). It is particularly interesting to note that processing procedures in nonrecursive \nprograms using the reverse itivocation order (Allen [2]) does not lessen the computational complexity \nof the sum\u00admary data flow problem. Having shown that gathering sunmary data flow information in the no \nsharing and nonrecursive case is as complex as computing reflexive transitive closure, it follows that \nthe sharing and recursive cases are at least as complex. Conclusion: An implementation (written in PASCAL) \nfor this algorithm exists for PASCAL programs , and it appears to be quite inexpensive to use. In the \nmost general of terms, the data flow analysis of a medium or small program (up to 50 procedures and 300 \nvariables) will take about one third of the time it takes to compile the program using the standard translator. \nThe implementation was run on the PASCAL 6000/3.4 compiler, which is a program with about 6800 lines \nof code, 140 procedures and 770 vari\u00adables. [4,10] Performing the interprocedural data flow analysis \n(for MOD, USE, and PRE) took somewhat less time than it takes to compile the compiler. The space required \nwas about 10,000 (6o bit) words. The use of interprocedural data flow analysis in an optimizing compiler \nat these costs seems practical. In a system which is attempting to gen\u00aderate program diagnostics or automatic \ndocumenta\u00adtion, these costs are quite small compared to the surrounding system. The experience with this \nalgo\u00adrithm convincingly shows that interprocedural data flow analysis is well within what should be con\u00adsidered \npractical for use in programming language systems. Bibliography: [11 Aho, Alfred V., Hopcroft, John \nE., and Unman, Jeffrey D. ~ Design ~ Analvsis ~ Computer Al~orithms. Addison-Wesley Publishing Company, \nReading Mass. (1974). [21 Allen, F. E. Interprocedural data flow analysis, Proceedings ~ ConRress ~, \nNorth Holland Publishing Co,, Amsterdam (1974), 398-402. [3] Allen, F. E. and Schwartz, J. T. Determining \nthe data relationships in a collection of pro\u00adcedures. (unpublished detailed summary). [4] Ammann, Urs. \nCompiler for PASCAL 6000 -3.4. ETH, Institut Fuer Informatik, Zuerich (1974). [5] Barth, Jeffrey M. A \npractical interprocedural data flow analvsis algorithm and its applications. Ph.D. Disserta~ion, ~niversity \nof California, Berkeley (in preparat, on). [61 Graham, Susan L. and Wegman, Mark. A fast and usually \nlinear algor. thm for global flow analysis. Journal of the ~, vo1.23, No. 1 (Jan 1976), 172-202. [71 \nGries, David. .ComDiler Construction @ Di,aital ComDuters. Wiley, New York (1971), 39. [8] Hecht, Matthew \n.S. and Shaffer, Jeffrey B. Ideas on the design of a quad improver for SIMPL-T, part I: overview and \nintersegment analysis. Computer Science Technical Report TR-405 , University of Maryland, College Park \n(August 1975). [9] Hecht, Matthew S. and Unman, Jeffrey D. Analysis of a simple algorithm for global \nflow problems. Conference Record ~ m JK&#38;4 SIGACT/SIGPLAN SvmDosium w ~ Principles ~ Programming La \nn.guages, Boston, Mass. (October 1973), 202-217. [101 Jensen, Kathleen and Wirth, Niklaus. PASCAL User \nManual and Report. Springer Verlag Lecture Notes in Computer Science No. 18, Berlin (1974). [11] Lomet, \nDavid B. Data flow analysis in the presence of procedure calls. ~ Research ReDort RC5728, Thomas J. Watson \nResearch Center, Yorktown Height.s,New York (November 1975). [12] Naur, Peter. Revised report on the \nalgo\u00adrithmic language Algol-60. Communications of the ~ (January lg63). [13] Ritchie, Dennis M. C reference \nmanual. Bell Telephone Laboratories, Murray Hill, New Jersey (1975). [14] Rosen, Barry K. Data flow analysis \nfor recursive PL/I programs. ~ Research ReDort IiG5&#38;ll, Thomas J. Watson Center, Research Yorktown \nHeights, New York (January 1975). This report is superseded by [17]. [151 Rosen, Barry K. High level \ndata flow analysis, part 1 (classical structured program\u00adming). ~ Research ReDOrt RC5598, Thomas J. Watson \nResearch Center, Yorktown Heights, New York (August 1975). [161 Rosen, Barry K. High level data flow \nanalysis, part 2, (escapes and jumps). ~ Research ReRort RC5744, Thomas J. Watson Research Center, Yorktown \nHeights, New York (December 1975). [17] Rosen, Barry K. Data flow analysis for pro\u00adcedural languages. \nIBM Thomas J. Watson Research Center, Yorktown Heights, New York (1976). [181 Spillman, T. C. Exposing \nside effects in a PL/I optimizing compiler. Proceedings IFIP Confer\u00adence 1971, North Holland Publishing \nCompany, Amsterdam (1971), 376-381. [19] Tar.jan, Robert E. Solving path problems on directed graphs. \nStanford University Computer Sci\u00adence Department Technical Report STAN-CS-75-528, Palo Alto, Ca. (November \n1975). [20] Wulf, William A., et. al. Bliss: a basic language for implementation of system software for \nthe PDP-10. Carnegie Mellon University Computer Science Department Report (1970). \n\t\t\t", "proc_id": "512950", "abstract": "A new interprocedural data flow analysis algorithm is presented and analyzed. The algorithm associates with each procedure in a program information about which variables may be modified, which may be used, and which are possibly preserved by a call on the procedure, and all of it subcalls. The algorithm is sufficiently powerful to be used on recursive programs and to deal with the sharing of variables which arises through reference parameters. The algorithm is unique in that it can compute all of this information in a single pass, not requiring a prepass to compute calling relationships or sharing patterns. A lower bound for the computational complexity of gathering interprocedural data flow information is derived and the algorithm is shown to be asymptotically optimal. The algorithm has been implemented and it is practical for use even on quite large programs.", "authors": [{"name": "Jeffrey M. Barth", "author_profile_id": "81100581809", "affiliation": "University of California, Berkeley, Berkeley, California", "person_id": "P137548", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/512950.512962", "year": "1977", "article_id": "512962", "conference": "POPL", "title": "An interprocedural data flow analysis algorithm", "url": "http://dl.acm.org/citation.cfm?id=512962"}