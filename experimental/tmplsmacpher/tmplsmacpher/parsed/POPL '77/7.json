{"article_publication_date": "01-01-1977", "fulltext": "\n Permission to make digital or hard copies of part or all of this work or personal or classroom use is \ngranted without fee provided that copies are not made or distributed for profit or commercial advantage \nand that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, \nto post on servers, or to redistribute to lists, requires prior specific permission and/or a fee.&#38;#169; \n1977 ACM 0-12345-678-9 $5.00 APPLICATI CNS OF A GRAPH GRAMMAR FOR PRCKXAM CONTROL FLCW ANALYSIS Ken Kennedy;C \nLinda Zucconi;: Dept. of MathRice Houston, ematical University Texas Sciences 77001 I. Introduction \n A standard approach to the analysis of program structure for the purpose of code optimization is to \nconstruct the control flow graph which models the possible execution paths through the program. Various \ngraph algorithms can be applied to the control flow graph to produce data flow information, possible \nopti\u00admizations, etc. [A1,A2,AC:AU2,AU3,CS ,HUl,HU2.HU3,Kel~Ke2,Ke3,Ke4,Sc,U] . Studies of the form-of \ntypical control flow graphs indicate that such graphs tend to fall into a restricted subclass of general \ngraphs. For example, empirical investigations have shown tha c the vast majority of program graphs have \nno multiple\u00ad entry loops [AC,HU2,HU3,Knl]. The recent work on structured programming has suggested that \ngood programs fall into an even more restricted subclass. In fact, purists recommend that all programs \nbe synthesized from three basic control structures : sequential statements, if-then-else statements, \nand single-entry single-exit loops [Di,Wi]. Formal language theory [Hou] has given us a practical way \nto specify the set of strings which com\u00adprise a given language: via a grammar. It is then a natural idea \nto extend grammars from the strings to graphs in hopes of getting the same power of expression. Several \nresearchers have used this approach [FKZ,J2,RO]. In this paper we study the applicability of a grammatical \napproach to describing the set of control flow graphs which arise from good programs in the sense proposed \nby many programming practitioners. The resulting flow graph language contains all those programs constructed \naccording to the purists rules and also admits ~rograms with multiple-exit loops if such loops are constructed \nsensibly. The grammar we use is the ll~emi.structured flow graph gra~ar GSsFG which was studied originally \nin [FKZ]. There are several appealing properties of this grammar; perhaps the most important, from the \npoint-of-view of a compiler-writer, is the existence of a linear-time parsing algorithm which leads directly \nto a linear-time data flow analysis method [FKZ]. In the present work we summarize the results from [FKZI \nand address several new questions. First, how often do programs written by people with no knowledge of \nthe SSFG rules fall into the language defined In other words, is the language a natural one for programming? \nSecond, once a program has beenby GssFG? parsed according to Gs~FG do benefits other than fast data flow \nanalysis accrue? The paper is organized into three main sections. Section II introduces GssFG and the \nparsing algo\u00ad rithm from [FKZ]. Section III is devoted to an empirical study conducted by the authors \nin an attempt to answer the question of naturalness, described above. Section IV discusses several applications \nof the graph parse in a graph attribute grammar framework. The summary at the end of the paper includes \nsuggestions for further investigation. II. The Grammar The semi-structured flow graph grammar GSSFG consists \nof the nine rules depicted in Figure 1. ~: Work supported by the National Science Foundation, Division \nof Computer Research, Grant DCR73-03365-A01, Department of Mathematical Sciences, Rice University C5 \nTerminals Non Terminals single exit basic block computation region (start node) A!%basic block with test \ndecision region ++ Rules 1) = ..= 3) . H. $. .,= $ 05 c-blk ccd I .. .. 4) .. = 5) .. = $+ J+lA d-blk \nL!2!2E cd . .. = . 7) 8)  Jvjl% dcddd-loop  Figure 1. SSFG Note that the two terminal symbols represent \nstraight-line blocks of code with and without a conditional jump at the end. The two no;-terminais represent \n; computation region with one exit and a decision region which may do some computation but also makes \na decision at some point about which exit to take. The family 3SSFG of SSFG flow graphs is the set of \ngraphs which can be generated by applying the rules Note that rules 7, 8, and 9 allow various mirror \nimages. A graph r is said to be SSFG-redug~ f SSFG ble if Consider the spir~l graph depicted in Figure \n2. This is an example graph taken from m2] . Figure 2. Spiral Graph A derivation for the spiral graph \nis shown in Figure 3, Whether or not a particular flow graph is reduci\u00ad ble can be determined by applying \nthe parsing algorithm presented in [FKZI. If the given graph is a member of 7SSFG, the parsing algorithm \nreturns the sequence of productions required to reduce it to a single node; otherwise, the algorithm \nhalts with a report of failure. For completeness, we include an Outline of the parse algorithm: Algorithm \nP. SSFG Parse (outline). QIQ!.!2: 1) A graph r 2) A list L of unvisj ted nodes in straight order (the \nreverse of the order of last u visit by depth-first search). Q!U.PQ If r< 3~~FGa parse P otherwise, \nthe answer no. Method : kxm a: = entry node of r; Pr= 0; remove a from L. u visit: W+#null &#38; apply \nall reductions possible with a as header, set success to true if at least one - reduction takes place, \nfalse otherwise; add reductions to ; make a the unique r linked Predecessor of all its unvisited successors \n: i_&#38; success ~ g is linked to predecessor > a:=bthen else QL = @then a = null u else a: = next \nunvisited node from L ; u remove a from L u g Q od i-&#38; T is now a single computation node then \nreturn P else return no ~ r end Algorithm P, properly implemented , parsesarbitrary graphs in time linear \nin the size of the graph. Furthermore, the parse leads to a linear-time algorithm for data flow analysis \non members Of %SSFG, [FKZ]. It was shown in [FKZ] that the family %SSFG admits many graphs produced through \nthe use of control structures suggested as extensions for structured programming. Examples are Zahn s \ncontrol structure, [Za,Kn] Martin s natural set of control structures [w] and the counterexample of Ashcroft \nand Manna [AM] . From those results we were led to ponder the question: How many programs, written with \nor without structured concepts, fall naturally into 3SSFG? That question led us to the investigation \ndescribed in the next section. III. The Empirical Study For our study of the structure of typical application \nprograms we decided to look at FORTRAN programs since there exist numerous scientific applications programmed \nin that language and since the control structures do not particularly encourage good programming. The \nnext step was to acquire programs for the study. Members of the Rice faculty generously provided us with \nan ample supply which we subdivided into the following groups. 1) Sixty FORTRAN programs written by students \nin an engineering class where structured programming was not taught. The sample represented student solutions \nto two problems: a) find the roots of a cubic equation, b) compute the time to target and the required \nangle from the plane for a projectile with given velocity and range. 2) Two hundred twenty-seven subroutines \nfrom a large Chemistry Dept. application: to construct the wave function for a compound during molecular \nscattering using the stabilization method. The routines were as much as ten years old and many had been \nwritten at other universities. 3) Sixty-two Physics Dept. routines used to calculate eigenvalues and \nwave functions for certain Hamiltonians in a molecular structure application. 4) Seventy-six Biochemistry \nDept. routines comprising a large crystallographic system for analyzing X-ray diffraction data to deduce \nmolecular structure. cc =) + + + d-loop dc d-loop 3 a @ 7@ @ T7 7 7$ o0 LJ6Jc\u00ad d-loop d-blk6 =* % % \nFigure 3. Derivation of Spiral Graph 75 5) Five routines used by the Chemistry Dept. to determine mean \nsquare end-to-end separation of molecules via a Monte Carlo simulation of macro molecular structure. \n6) Thirty-six routines comprising two Mathematical Sciences Dept. applications: a) a finite element differential \nequation solver which uses a Galerkin procedure and isopara\u00admet ric elements. b) a collection of HI procedures \nfor solving quadratic homogeneous partial differential equations. 7) Thirty-four routines from two Geology \nDept. programs: a) a program which solves the free convection problem for a medium with variable viscosity \n.\u00ad by solving a system of coupled partial differential equations. b) a program which simulates the slow \nflow of liquids under given boundary conditions. All in 111, a total of five hundred routines were analyzed \nto determine how many were Cocke-Allen reducit .e and how many were SSFG-reducible. The results are summarized \nin Table 1 below. $ of Cocke-Allen Number Number of $ reducible of Cocke-Allen Cocke-Allen # SSFG-$ SSFG \nprograms SSFG Group programs reducible reducible reducible reducible reducible 1 60 58 97 55 92 94.8 \n2 227 221 97 199 88 94.3 3 62 60 97 5690 93 4 76 62 81.5 41 54 66 55480 000 636 33 92 3083 91 734 32 \n94 3088 94 Total 500 470 94 411 82 87.5 Table 1. Reducibility Analysis The last column, indicating the \npercentage of Cocke-Allen reducible programs which are also SSFG-reducible is worth comment. Since most \nfast data flow analysis routines work for Cocke-Allen reducible programs, one should compare the linear-time \nSSFG method with those for applicability. The last column in Table 1 indicates that the SSFG method would \nwork in nearly 90$ of those cases which could be handled by one of the fast non-linear methods [GW,Ke3,AU3]. \nOverall, more than four out of five of these programs, all written without the benefit of structured \nprogramming, were SSFG-reducible. Though we were pleased with this result, we were disturbed by the intractability \nof groups 4 and 5 , without which 88 $ of the programs would have been SSFG-reducible. We therefore undertook \na study of the programs in these groups to determine the reasons they failed to reduce so frequently. \nThe first step was to perform a static analysis on all the programs to determine average length and frequencies \nof various statement types. Tune results of this study appear in Table 2. Avg. {} Avg. # Avg. # of Avg. \n# Avg. # Avg. # computed Iabelled Group statements if-statements do-loops goto s goto s statements 1. \nStudent 41 2.6 .4 2 0 5 2. Chemistry 97 9 5 3 .28 13 3. Physics 66.5 5 6 2.5 .25 16  L. 13ioehem 200 \n31 11 151.5 69 5. Chemistry 320 50 10 20 3 54 6. MathSci 76 6 5 3 1 11 7. Geology 89 7 7 4 1 15 .- Table \n2. Static Analysis 76 From this table we can see that the programs in groups 4 and 5 were extremely long \nand contained many goto-statements. If we compare the statistics for group 4 with those for group 2, \nwhich reduced more frequently, we see that group 4 programs were three times as long (on the average) \nbut contained five times as many goto s, five times as many labelled statements and only twice as many \ndo-loops. It seems likely that many of the loops in group 4 were implemented with goto s rather than \ndo s, resulting in a fairly complex control flow structure. Our next step was to analyze programs in \ngroups 4 and 5 to determine specifically why they failed and to see if a richer grammar might reduce \nthem. For the purposes of this analysis, we defined two new grammars. The first, extended the SSFG grammar \nby adding two new rules depicted in Figure 4. 10) =)11) =+ I% 0 Figure 4. SSFGX extensions The resulting \ngrammar is called SSFG and an analysis of groups 4 and 5 showed that an additional 7 programs reduced \nunder these rules. x A more ambitious grammar admitting 3-exit regions was also considered. This grammar, \nnamed SSFG3, consists of rules 1-7 of SSFG and the nine rules depicted in Figure 5. Groups 4 and 5 were \nalso analyzed for reduction under this grammar. The results of these studies are summarized in Table \n3. I ;0 I $ Cocke-Allen Cocke-Allen which were Number of # SSFG SSFGX # SSFG3 SSFG3 # of Cocke-Allen \n# SSFG x reducible reducible reducible reducible reducibleI Programs reducible 4. Biochem I 76 62 41 \n47 76 58 93.5 I 5.Chemistry 5 5 0 1 25 3 75 Table 3. It can be seen that significant improvements in \nreduction percentages can be achieved by using the larger grammars. However, it is our belief that the \nprograms in group 4 and 5 have too many complex control structures and that increasing the complexity \nof our grammar is not an appropriate way to deal with such programs, We are currently investigating a \nsystematic code-copying approach which will be reported on in another paper. 77 new nonterminal (three \nexit region) rh ~ .. 8) 9) ,.= +-l .. _ 10) 11) .. J+7 .._ 12) 13) .. 1 m .. 14) 15) ,.= .,_ 16) .. Figure \n5. Additional rules for SSFG3 78 IV. Applications The grammatical description of program flow graphs \nallows a number of applications to be carried over from language theory. In particular, many algorithms \non the flow graph can be specified via attributes! on the graph grammar [Kn3,Kn4]. As an example, we \nshow how one might specify the analysis of profitabi\u00adlity, Simply stated, the problem is this. Suppose \nwe are given for each branch (x,y) in the program, the probability p(x,y) fhat that branch will be taken \nafter block x is executed. We wish to determine the expected frequency f(x) of execution of each block \nx in the program during a single execution of the whole program. Under certain simplifying Markov assumptions \n[CK], the expected frequencies can be expressed as follows: f(no) = 1 n the program entry node o (>k) \nf(x) = P(Y,x)f(Y) all other nodes E (y,x)EE A solution-finding method for the system of equations (~~) \nis described by the graph attribute grammar in Figure 6. There are several points to notice about this \ngraph grammar. 1) The attribute p is a synthesized attribute; that is, its value for composite regions \nis based on its value for nodes within the region. The values of p for terminal nodes are given. 2) The \nattribute ~ is an inherited attribute; that is, its value for nodes within a region is based upon its \nvalue for the region as a whole (and upon the values of ~). 3) Because of the dependence of ~ upon ~, \nwe are forced to evaluate ~ first. Thus the attribute grammar in Figure 8 gives rise to a two-pass algorithm. \nThe first pass moves forward through the parse, computing ~ for larger and larger regions; the second \npass moves backward through the parse, computing f for smaller and smaller regions. 4) The first value \nof f is the one for the whole program no which is 1 by assumption (the whole program is executed once). \nFigures 7 and 8 present an example profitability computation, with Figure 8 depicting the reduction pass \nand associated transition probability computations and Figure 9 depicting the production pass and frequency \ncomputations. From the considerations above and the example, we can see that the attribute grammar specification \ngives rise to the classical algorithm for profitability [CK]. Furthermore, by applying analogs of tech\u00ad \nniques in [w,m] we can compile these attribute grammars into efficiently-executing finite-state machines \nwhich use the parse (or its reverse) as input. The eventual result may be a system to generate graph\u00ad \nbased optimization algorithms. By analogy with [FKZ] these algorithms should be linear in the size of \nthe input program. v. Summary and Conclusions We have investigated the practicability of a graph grammar \nfor program control flow from two view\u00adpoints: its naturalness for describing programs and its applicability \nto compiler construction. An empirical study has shown that programmers (even those untutored in structured \nprogramming ) tend to write programs which are derivable using GSSFG. We might conclude that a typical \ngood pro\u00ad grammer would not find rules based upon G too restrictive, We have also shown a promising new \nway s!~r specifying global flow algorithms via an attributed This could lead to a system for the automatic \ngeneration of compiler optimizers. ersion f SSFG Acknowledgements We are particularly grateful to our \ncolleagues at Rice who provided us with sample programs: Biochemistry Dept: Professor Florante Quiocho \nand George Phillips. Chemistry Dept: Professor Edward F. Hayes, Professor Frederick T. Wall, Richard \nLiedtke, and John Chen. Geology Dept: Professor Jean-Claude De Bremaecker. Physics Dept. Professor Neal \nF. Lane, Tom Winter, and Allen Haggard. Mathematical Sciences Dept: Pr6fessor Mary F. Wheeler, Bruce \nDarlow, and N~resh Garg. Civil Engineering Dept: Professor Edward C. HoIt, Jr. We also appreciate helpful \ncomments given by Barry Rosen of IBM Research and Rodney Farrow of Rice. Rules Semantic Rules entry o) \nstart * f(x)=l 1) c-blk 3 w p(z,w)=l f(x)=f(z) ~) 4z w cc =$ 8 Y w p(z, w)=l. f(x)=f(z) f(y)=f(z) 3) \nz loop 3 8 x p(z, f(x)= w)=l f(z)/ (l-p(x, x)) 4) 5) 6) + w + z w l+z v w /+ z v w d =$ d-blk 3 cd * \n&#38;x w Ax v w Ax v w p(z, w)=l f (X)=f (z) p(z, v)=p(x, p(z,w)=p(x,w) f (X)=f (z) p(z,v)=p(y,v) p(z,w)=p(y,w) \nf (X)=f (z) f(y)=f(z) v) 7) !%z v w dc = @ Y v x w p(z,v)=p(x,y) p(z,w)=p(x,w) f(x)=f(z) f(y)=f(z)~:p(x,y) \n8) +-lz v w dd * I Rx Y p(z,v)=p(x,v)+p(x,y)$:p(y, p(z,w)=p(x,y)~:p(y,w) f(x)=f(z) f(y)=f(z)>~p(x,y) \nv) 9) r%z v w d 100p =) % v x w Y p(z,v)=p(x,v)/(1-p(x,y);:p(y,x) P(.z,w)=P(x,Y)~ P(Y,w)/(1-P(x,Y) ~P(Y, \nf(x)=f(z)/(1-p(x,y)~:p(y,x) f(y)=f(z)~~p(x,y)/(1-p(x,y)>~p(y, ) ) x)) z)) Figure 6. for Profitability \nttrib ted SSFG 80 enter dc .. 0 3 0.4 0.4 0.9 1.0 1.0 + exit 1 1.0 cc a 1234 23456 cc 56 + 1.0 ?234567 \n1.0 77 $!) + Figure 7. A Profitability Reduction Sequence f =1 1 f=l 1.0 f =1 123 1234567 cc a 456 cc \n= 23456 =1 1.0 1.0 f=l 7 + , f=l t i 1.0 e d-loop f=l 3 0.4 23456 0.4 0.6 7 f=l + + dc3 f=l. 17 * Figure \n8. Profitability Production Sequence REFERENCES [ASU] Aho, A.V., Sethi, R. and Ullmann, J.D., Code Optimization \nand Finite Church-Rosser Systems in w= Optimization X compilers; (R. Rustin, cd.) prentice-Hall, E wlewOOd \nCliffs, N.J., [AU1] Aho, A.V. and Ullmann, J.D., The Theory of Parsing, Translation and Compiling, Vol. \nI:Parsing, Prentice-Hall, Englewood C~fs, N.J., 1972. [AU2] Aho, A.V. and Unman, J.D., The Theory of \nParsin g, Translation and Compiling , Vol. II: Compiling, Prentice-Hall, Englewood ~ffs, N.J~ 1973. \n [AU3 ] Aho, A,V. and Unman, J.D., Node Listings for Reducible Flow Graphs, Proc. Seventh Annual ACM \nSymposium on Theory of Computing, Albuquerque, N.M., pp. 177-185 (May, 1975). [Al ] Allen, F.E., Control \nFlow Analysis, SIGPLAN Notices, Vol. 5, No. 7, pp. 1-19, July 1970. [A2] Allen, F.E., A Basis for Program \nOptimization, Proc. IFIP Conf. ~, North-Holland Publishing Co. Amsterdam, 1971. [AC] Allenj F.E. and \nCocke, J., Graph Theoretic Constructs for Program Flow Analysis, IBM Research Report RC 3923, T.J. Watson \nResearch Center, Yorktown Heights, N.Y., July 1972. [AM] Ashcroft, E. and Manna, Z., The Translation \nof Goto Programs Into While Programs, Proc. IFIP Conf. Q, North Holland Publishing Co., Amsterdam, 1971. \n[Be] Beatty, J.C., An Algorithm for Tracing Live Variables Based On A Straightened Program Graph, IBM \nTechnical Report TR 00.2503, IBM Poughkeepsie Laboratory, December 1973. [BJ] B6hm, C. and Jacopini, \nG. Flow Diagrams, Turing Machines, and Languages With Only Two Formation Rules, -, Vol. 19, No. 5, May \n1966. !Iprofitability Computations on program Flow Graphs, IBM Research Report [CK] Cocke, J. and Kennedy, \nK., RC 5123, T,J. Watson Research Center, Yorktown Heights, N.Y,, November 1974. [Cs] Cocke, J. and Schwartz, \nJ.T., Programming Languages and Their Compilers, New York University, New York, 1969. I!Notes on Structured \nProgramming, [Di] Dijkstra, E.W., in Dahl, Dijkstra and Hoare, Structured Pro\u00adgramming, Academic Press \n(1972). [EBA] Earnest, C.P., Balke, K.G. and Anderson, J., Analysis of Graphs by Ordering of Nodes, =, \nVol. 19, No. 1, Jan. 1972, pp. 23-42. !lGraPh Grammars and Global Program Data F1OW Analysis, [FKZ] Farrow, \nR., Kennedy, K., and Zucconi, L., to appear in Proceedings of the Seventeenth Annual IEEE Symposium on \nthe Foundations of Computer Science, October 1976. [FS] Friedman, D.P. and Shapiro, S.C., A Case For \nWhile-Until, SIGPLAN Notices, Vol. 9, No. 7, July 1974. [GW] Graham, S.L. and Wegman, M., A Fast and \nUsually Linear Algorithm for Global Flow Analysis, Conf. Record of the Second ACM Symposium on Principles \nof Programming Languages, Palo Alto, California  (Jan. 1975) pp. 22-34. [HU1 ] Hecht, M.S., and Unman, \nJ.D., Analysis of a Simple Algorithm for Global Data Flow Problems, Proc. ACM SIGACT/SIGPL4N Symposium \non Principles of Programming Languages, Boston, Mass., z lm. [HU2] Hecht, M.S., and Unman, J.D., Flow \nGraph Reducibility, SIAM J. Computing, Vol. 1, No. 2, . PP. 188-202, June 1972. [HU3 ] Hecht, M.S. and \nUnman, J.D., Characterizations of Reducible Flow Graphs, JACM, Vol. 21, No. 3, pp. 367-375, July 1974. \n[Jl] Jazayeri, M., On Attribute Grammars and the Semantic Specification of Programming Languages, Ph.D. \nThesis, Computing and Information Sciences Dept., Case Western Reserve University, (October 1974). [J2] \nJazayeri, M., Live Variable Analysis, Attribute Grammars, and Program Optimization, draft, Dept. of Computer \nScience, University of North Carolina, Chapel Hill, N.C. March 1975. [ka] Karpinski, R.H., An Unstructured \nView of Structured Programming, SIGPLAN Notices, Vol. 9, No. 3, March 1974. [Kel] Kennedy, K., A Global \nFlow Analysis Algorithm, International ~. Computer =., Section A, Vol. 3, pp. 5-15, Dec. 1971. [Ke2] \nKennedy, K., A Comparison of Algorithms for Global Flow Analysis, Technical Report 476-093-1, Dept. of \nMathematical Sciences, Rice University, Houston, Texas, Feb. 1974. [Ke3] Kennedy, K., Node Listings Applied \nto Data Flow Analysis, Conf. Record of the Second ACM Symposium on Principles of Programming Languages, \nPalo Alto, California, Jan. 1975. [Ke4] Kennedy, K., Use-definition Chains with Applications, Rice Technical \nReport 476-093-9, Dept. of Mathematical Sciences, Rice University, Houston, Texas, April 1975. 11A Deterministic \nAttribute Grammar Evaluator Based on Dynamic Sequencing, Technical Report 476-093-12, Dept. of Mathematical \nSciences, Rice lJniversitY, Houston, Texas, Oct. 1975. [Kw] Kennedy, K. and Warren, S.K., Automatic Generation \nof Efficient Evaluators for Attribute Grammars, to appear in Conf. Record of the Third ACM Symposium \non Principles of Programming Languages, [m] Kennedy, K. and Ramanathan, J., .. . . Atlanta, Ga., Jan. \n1976. [KP1] Kernighan, B.W. and Plauger, P.J., The Elements of Programming &#38; Style, McGraw-Hill \nBook CO., New York, 1974. [KP2] Kernighan, B.W. and Plauger, P.J., Programming Style: Examples and \nCounterexamples, Computing Surveys, Vol. 6, No. 4, Dec. 1974. [Ki] Kildall, G.A., A Unified Approach \nto Global Program Optimization, Proc. ACM SIGACT/SIGPLAN Symposium on Principles of Programming Languages, \nBoston, Mass., Oct. 1~. [Knl] Knuth, Donald E., An Empirical Study of Fortran Programs, Software Practice \nand Experience, VO1. 1, No. 2, 1971, pp. 105-134. [Kn2] Knuth, Donald E., Structured Programming with \nGOTO Statements, Computing Surveys, Vol. 6, No. 4, Dec. 1974. [Kn3] Knuth, Donald E., Semantics of Context-Free \nLanguages, Math. Systems Theory J. ~, pp. 127-145 (1968). !,semantic~ of Context-Free Languages: Correction, \n= Systems Theory ~ ~>[Kn4] Knuth, Donald E., p. 95 (1971). [LC] Lee, R.C.T. and Chang, S.K., Structured \nProgramming and Automatic Program Synthesis, SIGPLAN Notices, Vol. 9, No. 4, April 1974. [MT] Markowsky, \nG. and Tarjan, R.E., Lower Bounds on the Lengths of Node Sequences in Directed Graphs, IBM Research Report \nRC 5477, T.J. Watson Research Center, Yorktown Heights, N.Y., July 1975. [Ma] Martin, J.J., The Natural \nSet of Basic Control Structures, SIGPLAN Notices, Vol. 8, No. 12, Dec. 1973. [Ro] Rosen, B., Tree Manipulating \nSystems and Church-Rosser Theorems, ~, Vol. 20, No. 1, Jan. 1973. [Sa] Sanfield, S., The Scope of Variable \nConcept: the Key to Structured Programming?, SIGPLAN Notices, Vol. 9, No. 7, .JuIY 1974. [Se] Schaefer, \nM., ~ Mathematical Theory of Global Program Optimization, Prentice-Hall, Englewood Cliffs, N,J,, 1973, \n[Se] Sethi, Ravi, Testing for the Church-Rosser Property, JACM, Vol. 21, No. 4, Oct. 1974. [Sh] Shneiderman, \nB., The Chemistry of Control Structures, SIGPLAN Notices, Vol. 9, No. 12, Dec. 1974. [T] Tarjan, R.E., \nDepth-First Search and Linear Graph Algorithms, SIAM J. Computing, Vol. 1, No. 2, June 1972. [u] Unman, \nJ.D., Fast Algorithms for the Elimination of Common Subexpressions, Acts Informatica, vol. 2, pp. 191-213, \n1973. 34 [We ] Wegner, E., Control Structures for Programming Languages, SIGPLAN Notices, Vol. 10, No. \n2, Feb. 1975. [WR] Weiderman, No. 4, N.H. April and Rawson, 1975. B.M., Flowcharting Loops Without Cycles, \nI! SIGPLAN Notices, Vol. 10, [Wi] Wirth, Dec. N., On 1974. the Composition of Well-Structured Programs, \n Computing Surveys, Vol. 6, No. 4, [WRR] Wulf, W.A., Russell, D.B. Vol. 14, No. 12, Dec. and 1971. Haberman, \nA.N., BLISS: A Language for Systems Programming, -, [z] Zahn, C.T., Conference Structured Proceedings, \nControl Vol. in 44, Programming AFIPS Press Languages, (1975). Proceedings of the 1975 NCC, AFIPS  \n \n\t\t\t", "proc_id": "512950", "abstract": "A standard approach to the analysis of program structure for the purpose of code optimization is to construct the \"control flow graph\" which models the possible execution paths through the program. Various graph algorithms can be applied to the control flow graph to produce data flow information, possible optimizations, etc. [A1,A2,AC,AU2,AU3,CS,HU1,HU2.HU3,Ke1,Ke2,Ke3,Ke4,Sc,U]. Studies of the form of typical control flow graphs indicate that such graphs tend to fall into a restricted subclass of general graphs. For example, empirical investigations have shown that the vast majority of program graphs have no multiple-entry loops [AC,HU2,HU3,Kn1].The recent work on \"structured programming\" has suggested that \"good\" programs fall into an even more restricted subclass. In fact, purists recommend that all programs be synthesized from three basic control structures: sequential statements, if-then-else statements, and single-entry single-exit loops [Di,Wi].Formal language theory [HoU] has given us a practical way to specify the set of strings which comprise a given language: via a grammar. It is then a natural idea to extend grammars from the strings to graphs in hopes of getting the same power of expression. Several researchers have used this approach [FKZ,J2,Ro].In this paper we study the applicability of a grammatical approach to describing the set of control flow graphs which arise from \"good\" programs in the sense proposed by many programming practitioners. The resulting flow graph language contains all those programs constructed according to the purists' rules and also admits programs with multiple-exit loops if such loops are constructed sensibly. The grammar we use is the \"semi-structured flow graph\" grammar G<inf>SSFG</inf> which was studied originally in [FKZ]. There are several appealing properties of this grammar; perhaps the most important, from the point-of-view of a compiler-writer, is the existence of a linear-time parsing algorithm which leads directly to a linear-time data flow analysis method [FKZ].In the present work we summarize the results from [FKZ] and address several new questions. First, how often do programs written by people with no knowledge of the SSFG rules fall into the language defined by G<inf>SSFG</inf>? In other words, is the language a natural one for programming? Second, once a program has been parsed according to G<inf>SSFG</inf> do benefits other than fast data flow analysis accrue?The paper is organized into three main sections. Section II introduces G<inf>SSFG</inf> and the parsing algorithm from [FKZ]. Section III is devoted to an empirical study conducted by the authors in an attempt to answer the question of naturalness, described above. Section IV discusses several applications of the graph parse in a \"graph attribute grammar\" framework. The summary at the end of the paper includes suggestions for further investigation.", "authors": [{"name": "Ken Kennedy", "author_profile_id": "81100453545", "affiliation": "Rice University, Houston, Texas", "person_id": "PP40027435", "email_address": "", "orcid_id": ""}, {"name": "Linda Zucconi", "author_profile_id": "81100557138", "affiliation": "Rice University, Houston, Texas", "person_id": "PP14193426", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/512950.512958", "year": "1977", "article_id": "512958", "conference": "POPL", "title": "Applications of a graph grammar for program control flow analysis", "url": "http://dl.acm.org/citation.cfm?id=512958"}