{"article_publication_date": "01-21-2009", "fulltext": "\n SPEED: Precise and Ef.cient Static Estimation of Program Computational Complexity Sumit Gulwani Krishna \nK. Mehra Trishul Chilimbi Microsoft Research Microsoft Research India Microsoft Research sumitg@microsoft.com \nkmehra@microsoft.com trishulc@microsoft.com Abstract This paper describes an inter-procedural technique \nfor computing symbolic boundsonthe numberofstatementsaprocedureexecutes in terms of its scalar inputs \nand user-de.ned quantitative functions of input data-structures. Such computational complexity bounds \nfor even simple programs are usually disjunctive, non-linear, and involve numerical properties of heaps.We \naddress the challenges of generating these bounds using two novel ideas. We introduce a proof methodology \nbased on multiple counter instrumentation (each counter can be initialized and incremented at potentially \nmultiple program locations) that allows a given linear invariant generation tool to compute linear bounds \nindividually on these counter variables. The bounds on these counters are then composed together to generate \ntotal bounds that are non-linear and disjunctive. We also give an algorithm for automating this proof \nmethodology. Our algorithm generates complexity bounds that are usually precise not only in terms of \nthe computational complexity, but alsoin termsof the constantfactors. Next, we introduce the notion of \nuser-de.ned quantitative func\u00adtions that can be associated with abstract data-structures, e.g., length \nof a list, height of a tree, etc. We show how to compute bounds in terms of these quantitative functions \nusing a linear in\u00advariant generation tool that has support for handling uninterpreted functions.We show \napplication of this methodology to commonly used data-structures (namely lists, list of lists, trees, \nbit-vectors) using examples from Microsoft product code. We observe that a few quantitative functions \nfor each data-structure are usually suf\u00ad.cient to allow generation of symbolic complexity bounds of a \nvariety of loops that iterate over these data-structures, and that it is straightforward to de.ne these \nquantitative functions. The combination of these techniques enables generation of pre\u00adcise computational \ncomplexity bounds for real-world examples (drawn from Microsoft product code and C++ STL library code) \nfor some of which it is non-trivial to even prove termination. Such automatically generated bounds are \nvery useful for early detection of egregious performance problems in large modular codebases thatare \nconstantlybeingchangedbymultipledeveloperswhomake heavy use of code written by others without a good \nunderstanding of their implementation complexity. Permission to make digital or hard copies of all or \npart of this work for personal or classroom use is granted without fee provided that copies are not made \nor distributed for pro.t or commercial advantage and that copies bear this notice and the full citation \non the .rst page.To copyotherwise, to republish, to post on servers or to redistribute to lists, requires \nprior speci.c permission and/or a fee. POPL 09, January 18 24, 2009, Savannah, Georgia, USA. Copyright \nc &#38;#169; 2009ACM 978-1-60558-379-2/09/01... $5.00. Categories and Subject Descriptors D.2.4[Software \nEngineer\u00ading]: Software/ProgramVeri.cation; F.3.1[Logics and Meanings of Programs]: Specifying andVerifying \nand Reasoning about Pro\u00adgrams; F.3.2[Logics and Meanings of Programs]: Semantics of Programming Languages \nProgram analysis General Terms Performance,Veri.cation Keywords Symbolic Complexity Bounds,Termination \nAnalysis, Counter Instrumentation, Quantitative Functions 1. Introduction Modern software development \nhas embraced modular design and data abstraction. While this increases programmer productivity by enabling \ncode reuse, it potentially creates additional performance problems. Examples include hidden algorithmic \ncomplexity where a linear-time algorithm encapsulated insidea simple API callgives rise to quadratic \ncomplexity, when embedded inside an O(n) loop. Software performance testing attempts to address these \nissuesbut faces two fundamental limitations it is often too little or too late. First, due to resource \nconstraints a program is typically tested on only a small subset of its inputs and the performance problem \nmay not manifest on these inputs. Second, these performance tests are time consuming and are typically \nonly run periodically for large software projects. Consequently,manyperformance problems show up very \nlate in the software development process when it is hard to redesign/re-architect the system to correctly \n.x the prob\u00adlem, or even worse, after software ships. In this paper, we present a static analysis that \ncan compute symbolic complexity bounds for procedures in terms of theirinputs. Even though this information \ndoes not mirror the real running time of programs (which also depends on low-level architectural details \nlike caches and pipelines), it can be used to provide useful insights intohowamodule performsasafunctionofits \ninputsatan abstract level, and can produce early warnings about potential performance issues. The same \nanalysis can also be used for bounding other kinds of resources (e.g., memory) consumed by a procedure. \nThe hard part in computing complexity bounds is to bound the total number of loop iterations (or recursive \nprocedure call invocations). There are4challengesin computing these bounds. 1. Even for simple arithmetic \nprograms, these bounds are disjunc\u00adtive (i.e., theyinvolve use of Max operator, as in Example Dis1 in \nFigure 2 and Example NestedMultiple in Figure 4, for both of which the bound is Max(0,n - x0) + Max(0,m \n- y0)) and non-linear (Examples SimpleMultipleDep and NestedMultipleDep in Figure 4, for both of which \nthe bound is n \u00d7 (1 + m), assuming n, m = 0). This usually happensin presence of control .ow inside loops. \n 2. For complicated programs (Example Equals in Figure 1, Ex\u00adample Dis2 in Figure 2, Example 4 in Figure \n6, Example  Traverse in Figure 9), it is hard to even prove termination, and computing bounds ought \nto be an even harder task. 3. It is desirable to compute precise bounds (in terms of both the computational \ncomplexity as well as the constant fac\u00adtors) as opposed to computing any bound. Consider Example SimpleSingle2 \nin Figure3and Examples SimpleMultiple and SimpleMultipleDep in Figure 4. The termination ar\u00adgument for \neach of these examples (as provided by termi\u00adnation techniques based on disjunctively well-founded rela\u00adtions \n[27, 6, 1]) is the same: between any two successive (not necessarily consecutive) loop iterations either \nx increases and is bounded above by n, or y increases and is bounded above by m. This implies a total \nbound of (n + 1) \u00d7 (m + 1) (along with the observation that both x and y start out with a value of 0 \nand assuming n, m = 0)for each of these examples. In con\u00adtrast, our technique can produce precise bounds \nof Max(n, m), n + m, and n \u00d7 (1 + m) respectively for these examples. 4. For loops that iterate over \ndata-structures, expressing bounds requires use of some numerical functions over data-structures (e.g., \nlength of a list, height of a tree). and computing those bounds may require some sophisticated shape \nanalysis, which is a dif.cult task.  We address the .rst two challenges by using a counter instru\u00admentation \nmethodology (Section 3), wherein multiple counters are introduced at different cut-points (back-edges \nand recursive call sites) and are incremented and initialized at multiple locations (as opposed to simply \nusing a single counter for all cut-points). These counters are such that the individual bound on each \ncounter is lin\u00adear (and hence can be computed using a linear invariant genera\u00adtion tool). Furthermore, \nthese individual bounds can be composed together to yield a total bound, which may be disjunctive or \nnon\u00adlinear (Section 3.1).We describe an algorithm that produces sucha counter instrumentation scheme \n(Section 4). The third challengeis addressedby ensuring that our algorithm producesa counter-optimal \ninstrumentation scheme, which uses an optimal number of counters with an optimal number of dependen\u00adcies \nbetween them a criterion that we illustrate usually leads to precise estimation of bounds (Section 3.2). \nWe address the .nal challenge for abstract data-structures 1 by introducing the notion of user-de.nable \nquantitative functions, each of which is a numeric function over some tuple of data\u00adstructures. (Whenever \nwe use the term data-structure, we mean abstract data-structure.) The user speci.es the semantics of \nthese functions by means of annotating each data-structure method with how it may update some quantitative \nattribute of anyobject. Given such a speci.cation, we show how to use a linear invariant genera\u00adtion \ntool with support for uninterpreted functions to generate linear bounds on counter variables in terms \nof quantitative functions of input data-structures.We show application of this methodology to commonly \nused data-structures (namely lists, trees, lists of lists, bit-vectors) from Microsoft product code, \nwherein we observe that usuallyafewquantitativefunctionsaresuf.cienttoallowcomputa\u00adtion of precise loop \nbounds, and that it is straight-forward to de.ne these quantitative functions. We have implemented these \nideas inside a tool called SPEED that computes precise symbolic bounds for several real-life exam\u00adples \ndrawn from Microsoft product code as well as C++ Standard TemplateLibrary (STL) code.For someof theseexamples,even \nproving termination is non-trivial. Note that our technique for es\u00adtimating computational complexity \ndoes not assume program ter\u00admination. Instead, existence of an upper bound on the number of loop iterations \nprovides a free termination argument and may even 1those that are referenced and updated througha well-de.ned \ninterface yield a simpler and more ef.cient alternative to termination strate\u00adgies pursued in [1, 6], \nwhich rely on synthesizing ranking functions for loops [27, 5, 4]. We start with a brief description \nof our overall methodology along with some examples in Section 2. 2. Overall Methodology The basic idea \nof our methodology is to instrument monitor vari\u00adables (henceforthreferredtoas countervariables)to countthe \nnum\u00adber of loop iterations and then statically compute a bound on these countervariablesin termsof programinputsusinganinvariantgen\u00aderation \ntool. In principle, given a powerful invariant generation or\u00adacle,itissuf.cientto instrumentasingle countervariable \n(whichis initialized to 0 at the beginning of the procedure and is incremented by 1 at each back-edge) \nand then use the invariant generation ora\u00adcleto compute boundsonthe countervariable.However,evenfora \nsimple program, such an approach would require the invariant gen\u00aderation tool to be able to compute invariants \nthat are disjunctive, non-linear, and that can characterize sophisticated heap shapes. No suchinvariant \ngenerationtoolexists,andevenifitdid,itwouldnot be scalable. We present a two-tiered approach to address \nthe above chal\u00adlenges: (a) Introduce multiple counters, each of which may be ini\u00adtializedand incrementedat \nmultiple locations.Thisavoidstheneed for disjunctive and non-linear invariants. (b) Require the user \nto de\u00ad.ne some quantitative functions over abstract data-structures. This avoids the need for sophisticated \nshape analysis. Example 1 Consider the Equals procedure (taken from Mi\u00adcrosoft product code) for stringbuffer \ndata-structure StringBuffer in Figure1.Astringbufferis implemented asa listof chunks.A chunk consistsofa \ncharacter array str whose total size is bounded above by a global variable size and its len .eld denotes \nthe total number of valid characters in the array str. It is non-trivial to prove whether the outer loop \nin the Equals function even terminates. The only way to break out of the outer loop is when chunk1.str[i1] \n= chunk2.str[i2] (Line 7) or when (i1 < 0) . (i2 < 0) (Line 25). Since there is no information provided \nregarding the contents of chunk1.str and chunk2.str, we can restrict our attention to tracking (i1 < \n0) . (i2 < 0). The value of both i1 and i2 decreases in each iteration of the outer loop as well as in \nthe .rst inner loop; hence if this were the only update to i1 and i2,the outerloopwould terminate.However,notethatthe \nvalue of i1 and i2 may increase in the second and third inner loops (at Lines 15 and 22 respectively). \nInstead, consider the following (counter-intuitive) proof argument: The total number of times the second \ninner loop at Line 11 executes (or, more precisely, the back-edge (17, 11) is taken) is boundedabovebythe \nlengthoflist s1,denotedbyLength(s1). (This is because everytime the back-edge (17, 11) is taken, chunk1 \nadvances forward over the list s1.) This information can be obtained by computing a bound on counter \nvariable c1 that has been instrumented in the procedure to count the total number of times the back-edge \n(17, 11) is taken.  Similarly, the total number of times the third inner loop at Line 18 executes (i.e., \nthe back-edge (24, 18) is taken) is boundedabovebythe lengthoflist s2,denotedbyLength(s2), and this information \ncan be obtained by computing a bound on counter c2.  The number of times the .rst inner loop (at Line \n6) as well as the outer loop (at Line 5) executes for each iteration of the second innerloopor third \ninnerloopis boundedaboveby size. This information can be obtained by computing a bound on counter variable \nc3 that has been instrumented appropriately  int size; Equals(StringBuffer s1, StringBuffer s2) { 1 \nc1 := 0; c2 := 0; c3 := 0; 2 chunk1 := s1.GetHead(); chunk2 := s2.GetHead(); 3 Assume(0 = chunk1.len, \nchunk2.len < size); 4 i1 := chunk1.len - 1; i2 := chunk2.len - 1; 5 for(; ;) { 6 while (i1 = 0 . i2 = \n0) { 7 if (chunk1.str[i1] = chunk2.str[i2]) return 0; 8 i1 --; i2 --; 9 c3 := c3 + 1;  10 } 11 while \n(i1 < 0) { 12 chunk1 := s1.GetNext(chunk1); 13 if (chunk1 == null) break; 14 Assume(0 = chunk1.len < \nsize); 15 i1 := i1+ chunk1.len; 16 c1 := c1 + 1; c3 := 0; 17 } 18 while (i2 < 0) { 19 chunk2 := s2.GetNext(chunk2); \n20 if (chunk2 == null) break; 21 Assume(0 = chunk2.len < size); 22 i2 := i2+ chunk2.len; 23 c2 := c2 \n+ 1; c3 := 0; 24 } 25 if (i1 < 0) return (i2 < 0);if (i2 < 0) return 0; 26 c3 := c3 + 1; 27 } 28 return \n1; 29 } Figure1. The Equals method of String Buffer data-structure as im\u00adplemented in one of Microsoft \nsproduct code. The bold instrumen\u00adtation of counter variables c1,c2,c3 is part of our proof methodol\u00adogy,which \nimpliesaboundof Length(s1)+Length(s2)+size\u00d7 (1 + Length(s1) + Length(s2)) on the total number of loop \niter\u00adations for this procedure. in the procedure to count the total number of iterations of the .rst \ninner loop as well as the outer loop in between any two iterations of the second or third inner loops. \n(Note c3 is initialized to 0 whenever c1 or c2 is incremented). Hence, the total number of loop iterations \n(both inner and outer) is bounded aboveby Length(s1)+Length(s2)+size \u00d7(1+ Length(s1) + Length(s2)). Example \n2 Consider the example Dis1 shown in Figure 2. It is easy to see that the loop terminates because the \nif-branch is executed inside the loop until y<m, followed by execution of the then-branch until x<n. \nHowever, computing a bound on the numberofloop iterationsisabit subtle.We cannotsimplysaythat the loopexecutes \nfor (m-y0)+(n-x0) iterations since y0 may be greater than m (whichwould maketheexpression m-y0 negative) \nand/or x0 may be greater than n (whichwould maketheexpression n - x0 negative). Instead consider the \nfollowing proof argument. If the then-branch is ever executed, it is executed for at most m - y0 iterations. \nThe bound m - y0 canbe obtainedby com\u00adputing a bound on counter variable c1 (inside the if-branch), which \nhas been instrumented to count the number of iterations of the if-branch (or, equivalently, the number \nof times the back\u00adedge (5, 3) is taken).  If the else-branch is ever executed, it is executed for at \nmost n-x0 iterations. The bound n-x0 canbe foundbycomputing  Dis1(int x0,y0, n, m) { Dis2(int x0,z0,n){ \n1 c1 := c2 := 0; 1 c1 := c2 := 0; 2 x := x0; y := y0; 2 x := x0; z := z0; 3 while (x<n) 3 while (x<n) \n4 if (y<m) 4 if (z>x) 5 y := y +1; c1++; 5 x := x +1; c1++; 6 else 6 else 7 x := x +1; c2++; 7 z := z \n+1; c2++; 8 } 8 } Figure 2. These examples have disjunctive bounds (which require use of max operator). \nExample (a) is quite a simple example, yet its bounds are subtle: Max(0,n - x0)+ Max(0,m - y0). Example \n(b) is a more sophisticated example taken from [6], where it is used to motivate the technique of well-founded \ndisjunctive ranking functions for proving termination. Its timing bound is Max(0,n - x0)+Max(0,n-z0).The \nbold instrumentationof countervariables c1,c2 is part of our proof methodology that can can compute precise \nbounds for both examples with equal ease. a bound on counter variable c2 (inside the then-branch), which \nhas been instrumented to count the number of iterations of the then-branch (or, equivalently, the number \nof times the back\u00adedge (7, 3) is taken). Hence, the total number of loop iterations is bounded by Max(0,m \n- y0)+ Max(0,n - x0). Notethatinthisexample,itis importanttousedifferent countersto count the number \nof iterations of each branch (or,equivalently,each back-edge).Thisis becauseifthe same countervariablewouldhave \nbeenusedonboththe back-edges,thenalinearinvariant generation tool would not be able to compute bounds \non it, for the simple reason that the bound is disjunctive (i.e., it involves use of max operator). Also, \nconsider the example Dis2 in Figure 2, where the if-branch and then-branch alternate several times. This \nis a non\u00adtrivial example even from the viewpoint of proving termination. This example has been taken \nfrom [6], where it is used to motivate the technique of well-founded disjunctive ranking functions for \nproving termination. However, its timing bound can be estimated to be Max(0,n - x0)+ Max(0,n - z0) in \na manner very similar to that ofexample Dis1, wherein we compute bounds on the counters c1 and c2 at \nback-edges (5, 3) and (7, 3) respectively, andthen add these bounds after maxing them out with 0. Our \nmethodology for automating such proof arguments for computing timing bounds involves the following steps \nin the or\u00adder mentioned. 1. De.ning Quantitative/Numerical Attributes for data-structures. The user declares \nsome numerical-valued functions over data-structures.Forexample, lengthofa list, or heightofa tree. The \nsemanticsof these functionsis de.nedbyannotating each data\u00adstructure method with its effect on the numerical \nfunctions as\u00adsociated with relevant data-structures.Forexample, the delete method associated with a list \nreduces its length by 1.For pur\u00adpose of communicating the semantics of these functions to an invariant \ngeneration tool (used in the next step), we instrument each invocation of a data-structure method with \nits effect on the quantitative functions as de.ned by the user. This allows for treating the quantitative \nfunction as uninterpreted functions, which eases up the task of an invariant generation tool. Sec\u00adtion5 \ndescribes this methodologyin more detail along witha case-study of examples drawn from Microsoft product \ncode. 2. Generatingaproof structure. This correspondsto choosingaset of counter variables and for each \ncounter variable selecting the locations to initialize it to 0 and the locations to increment it  by \n1 (e.g., the bold counter instrumentation for the examples in Figures 1 and 2). The counters are chosen \nsuch that the given invariant generation tool can compute bounds on the counter variables at appropriate \nlocations in terms of the scalar inputs and quantitative functions of input data-structures. We use a \nlinear invariant generation tool, with support for handling uninterpreted functions, to generate these \nbounds. Section 3 describes the notion of a proof structure in more detail and also introduces the notion \nof an counter-optimal proof structure to enable generationof precise timing bounds. Section4describes \nan algorithm to generate a counter-optimal proof structure. 3. Composing the bounds on counter variables \nto obtain the .nal desired bound. Theorem 1 (in Section 3) describes how to compose bounds on counters \nto obtain a bound on the number ofloop iterations. Theorem4(in Section6) describeshowto obtain a bound \non the total cost of a procedure, given anycost\u00admetric that maps atomic program statements to some cost. \n3. Proof Structure Obtaininga proof structureinvolves choosingasetof fresh counter variables S and, for \neach counter variable, deciding the locations (from among the back-edges or procedure entry point) to \ninitialize it to 0 and deciding the back-edges to increment it by 1 along with the following constraints. \n Each back-edge q should be instrumented with an increment to some counter variable (denoted by M(q)). \n2  There should be no cyclic dependencies between counter vari\u00adables.Acountervariable c1 is said to \ndepend on another counter variable c2 if c1 is initialized to 0 at a back-edge where c2 is in\u00adcremented \nby 1.  The invariant generation tool is able to provide a symbolic bound B(q) at each back-edge q on \nthe counter variable M(q) in terms of the inputs to the procedure.  We now formally de.ne a proof structure. \nDEFINITION 1 (Proof Structure fora procedureP ). Let S bea set of counter variables and let M be a function \nthat maps eachback\u00adedge in P to some counter variable from set S. Let G be anyDAG structure over S .{r} \nwith r as the unique root node. Let B be a function that maps eachback-edge in P to some symbolic bound \nover inputs of P . Then, the tuple (S, M, G, B) is a proof-structure (withrespecttoa giveninvariantgeneration \ntool)ifforallback\u00adedges q in procedure P , the giveninvariantgeneration tool canbe used to establish \nbound B(q) on counter variable M(q) at q in the procedure Instrument(P, (S, M, G)). DEFINITION 2 (Instrument(P, \n(S, M, G))). Let P be some given procedure.We de.ne Instrument(P, (S, M, G)) to be the proce\u00addure obtained \nfrom P by instrumenting it as follows: Eachback-edge q in P is instrumented with an increment (by 1)to \ncounter variableM(q).  Eachback-edge q in P is instrumented with an initialization(to 0)of any counter \nvariablec' that is an immediate successor of  ' M(q) in G, i.e., (M(q),c) . G. The procedure entry point \nis instrumented with an initialization (to 0)of any counter variablec' that is an immediate successor \n' of the root node r in G, i.e., (r, c) . G. 2The effectiveness of our proof methodology can be increased \nby expand\u00ading shared paths in a loop body into disjoint paths, each with its own back\u00adedge. In essence, \nwhat we really require is to associate each acyclic path between two cut-points with a counter variable. \nEXAMPLE 1. The following tuple (S, M, G, B) is a proof struc\u00adture for procedure Equals showninFigure1. \nThisproof structure also corresponds to the textual argument for the complexity of pro\u00adcedure Equals \nin Section 2. S = {c1,c2,c3} M = {(27, 5) . c3, (10, 6) . c3, (17, 11) . c1, (24, 18) . c2}G = {(r, c1), \n(r, c2), (r, c3), (c1,c3), (c2,c3)}B = {(27, 5) . size, (10, 6) . size, (17, 11) . Length(s1), (24, 18) \n. Length(s2)} Above (27, 5) denotes the back-edge from program location 27 to location 5 (and so on). \nThe instrumented procedure Instrument(Equals, (S, M, G)) corresponds to bold instrumentation of the counter \nvariables c1,c2 and c3 as showninFigure1. 3.1 Computing BoundsfromaProof Structure Theorem1below describeshowto \ncomputea boundonthe total number of loop iterations in a procedure, given a proof structure. Laterin \nSection6,weshowhowtoextend Theorem1to compute symbolic bounds on the total cost of a procedure, given \nany cost\u00admetric that maps atomic program statements to some cost. Note that it is this process that introduces \ndisjunctions and non-linearity in our computation of bounds on loop iterations. THEOREM 1 (Bound on Loop \nIterations). Let (S, M, G, B) be a proof structure for procedure P . Then, U as de.ned below denotes \nan upper bound on the total number of iterations of all loops in procedure P . X U = TotalBound(c) c.S \nTotalBound(r) TotalBound(c) = = 0 Max {0} [ {B(q) | M(q) = c} \u00d7 0 X 1 A @1+ TotalBound(c') (c',c).G \nPROOF: We claim that TotalBound(c) denotes the sum of the number of traversals of all those back-edges \nq such that M(q)= c. This can be proved by induction on the topological orderof theDAG G. The total number \nof loop iterations is given by the sum of the number of traversals of each back-edge. The result nowfollows \nfromthefactthateach back-edgeismappedto some counter D EXAMPLE 2. For the proof structure shown in Example \n1, ap\u00adplication of Theorem1 yields: TotalBound(c1)= Length(s1), TotalBound(c2)= Length(s2), TotalBound(c3)= \nsize \u00d7 (1 + Length(s1)+ Length(s2)). Hence, U = Length(s1)+ Length(s2)+ size \u00d7 (1 + Length(s1)+ Length(s2)). \n 3.2 Counter-Optimal Proof Structure Aprocedure might have multiple proof structures. Some of these \nproof structures may be better than the others in terms of yielding a better bound on the total number \nof loop iterations (as de.ned in Theorem1).Wecande.neanoptimalproofstructuretobeone that yields a bound \nwhich is not larger than the bound provided by any other proof structure. However, there are several \npractical issues with this de.nition. It is not clear how to generate such an optimal proof structure, \nor even check whether a given proof structure is optimal. Even the simpler problem of comparing the bounds \nimplied by two proof structures is hard because it would involve reasoning about multiplication and max \noperators (Note thatthe boundsonloop iterationsgivenby Theorem1involve both multiplication and max operators). \nInstead we propose the notion of counter-optimal proof structure that is usually an optimal proof structure, \nand allows for ef.cient checking whether a given proof structureis counter-optimalor not.Wealsogivean \nalgorithm(in Section 4) to generate such a counter-optimal proof structure. Our de.nition for counter-optimal \nproof structure is inspired by the following two observations that hold for almost all of the examples \nthat we have come across in practice. Fewer counters usually lead to a better bound. Use of fewer counters, \nif possible, usually leads to generation of a better bound. This is illustrated by a variety of examples \ndescribed in Figure 3. Each of these examples have two back-edges. Use of a single counter for the two \nback-edges in each of the .rst three examples producesa boundof n. On the other hand, use of multiple \ncounters leads to bounds of 2n or n 2 depending on the relative ordering of the counters in theDAG G \n(i.e., depending on where the counters are initialized). However, note that we cannot always use a single \ncounter.Theexamplesin Figure4illustrate some situations when multiple counters can be used to compute \nbounds, but a single counter is not suf.cient. Lesser dependency between counters usually leads to a \nbet\u00adter bound. Multiple counters, when used, might need to have some dependency between them (in the \nDAG G). This is de.\u00adnitely the case when the total number of loop iterations are non\u00adlinear as is illustrated \nby the examples SimpleMultipleDep and NestedMultipleDep in Figure 4. However,in some case, multiple counters \nneed not have a dependencybetween them. This is usu\u00adally the case when the bounds are max-linear (i.e., \na disjunction of linear bounds) as is illustrated by the examples SimpleMultiple and NestedMultiple in \nFigure4.Useofextraneous dependencies usually leadsto imprecise bounds.Forexample,ifa dependency is introduced \namong the two counter variables (in case of exam\u00adples SimpleMultiple and NestedMultiple), then it leads \nto a quadratic bound as opposed to a linear bound. The above two observations motivate the following \ntransforma\u00adtions respectively over a triple (S, M, G) that is part of a proof structure (S, M, G, B). \nDEFINITION 3 (Node Merging Operation). Given a triple (S, M, G),and any two countersc1,c2 . S,none of \nwhichis tran\u00adsitively dependent on the other in G, the node merging operation ' ' ' yields a triple (S \n,M ,G ' ), where S ' = S -{c2}, M is same as M except that it maps those back-edges q to c1 that were \nmapped by M to c2, and G ' is obtained from G by removing the node c2 and adding the successors/predecessors \nof c2 to those of c1. DEFINITION 4 (Edge Deletion Operation). Given a triple (S, M, G), and an edge \n(c1,c2) . G, the edge deletion operation yields a triple (S ' ,M ' ,G ' ) where S ' = S, M ' = M and \nG ' = G -{(c1,c2)}. We are now ready to de.ne a counter-optimal proof structure. DEFINITION 5 (Counter-Optimal \nProof Structure). A proof struc\u00adture (S, M, G, B) is counter-optimal if It has a minimal number of counters. \nMore formally, any node merging operationover (S, M, G) yieldsatriplethatisnotpart of any proof structure. \n It has a minimal number of dependencies between counters. More formally, any edge deletion operation \nover (S, M, G) yields a triple that is not part of any proof structure.  Next, we describe how to generate \na counter-optimal proof structure. 4. Algorithmfor constructinga counter-optimal proof structure In this \nsection, we describe an ef.cient algorithm for constructing a proof structure, and in fact, one that \nis counter-optimal. Our algorithm runsintimethatinworst-caseis quadraticinthe number of back-edges (modulo \nthe time taken by the invariant generation tool). In contrast, note that the number of triples (S, M, \nG) is exponential in the number of back-edges. Hence, a naive full state space search to .nd anyproof \nstructure (S, M, G, B) would be too expensive. The algorithm strikes the right balance between two opposite \nchallenges. Introducing more counters and more dependencies between counters increases (not decreases) \nthe ability of an invariant generation tool to generate bounds on counters. It is always possible to \nmap each back-edge to a distinct counter, but the algorithm cannot simply make all counters depend on \nall other counters. This would lead to a cyclic dependencyof counters, and G would notbeaDAG, and hence \n(proofof) Theorem1 would break down. So the challenge is to .nd an appropriate set of acyclic dependencies \nbetween countersinDAG G. One may wonder if the dependencies between counters correspond to the nesting \nrelationship between the corresponding back-edges. This is unfortunately not the case; the example in \nFigure 1 aptly illustrates that the .ow-graph structure of the program can sometimes be quite misleading. \n To generate a counter-optimal proof structure, the algorithm would want to use a minimum number of \ncounters, and a mini\u00admum number of dependencies between counters.  The algorithm for constructing a \nproof structure (S, M, G, B) for a given procedure P is described in Figure 5. Each iteration of the \nloop in Line 8 attempts to map a new back-edge q to a counter. The algorithm .rst tries to use any existing \ncounter vari\u00adable (to ensure that the number of counter variables generated are optimal/minimal, which \nis one of the necessary requirements for a proof structuretobe counter-optimal).Ifitfails,the algorithm \ntries to introducea new countervariable c. The new counter c can be in\u00adtroduced in an exponential number \nof ways, each corresponding to choosing some subset of other existing counters as the immediate predecessors \nof c inDAG G. Thisexponential searchisavoidedby the following two observations. Creation of an additional \ndependency preserves the ability of the invariant generation tool to compute bounds.  The counter-optimality \nrequirement enforces minimality of de\u00adpendencies between counters.  Lines 17-21 make use of the above \ntwo observations to search for minimal dependencies for the new counter by starting with all possible \ndependencies and then removing them one by one if possible. However, it is possible that even adding \nall possible dependencies may not be suf.cient for the invariant generation toolto compute bounds at \nback-edge q. In that case, the algorithm postpones the decision of choosing a counter for the back-edge \nq for a future iteration of the outer loop in Line 6. There might be multiple iterations of the outer \nloop in Line6 since a back-edge q that could not be mapped to a counter in an earlier iteration of the \nloop in Line 8 may now be mapped to a counter since some new back-edges have been mapped to new counters \nin an earlier iteration. This allows for initializing the counter corresponding to back-edge q to 0 at \nthose back-edges, SimpleSingle(int n) x := 0; while (x<n) if (*) x := x +1; else x := x +1; SequentialSingle(int \nn) x := 0; while (x<n &#38;&#38; nondet()) x := x +1; while (x<n) x := x +1;  NestedSingle(int n) x \n:= 0; while (x<n) while (x<n &#38;&#38; nondet()) x := x +1; x := x +1; SimpleSingle2(int n,m) x := 0; \ny := 0; while (*) if (x<n) x++; y++; else if (y<m) x++; y++; else break; Figure 3. This set of simple \ndiverse examples demonstrate that use of fewer counters, whenever possible, leads to better bounds. Each \nof these examples have two back-edges. Use of single counter for the two back-edges in each of the .rst \nthree examples produces a bound of n. On the other hand, use of multiple counters leads to bounds of \n2n or n 2 depending on the relative orderingof thecountersin theDAG G (i.e., depending on where the counters \nare initialized). SimpleMultiple(int n, m) x := 0; y := 0; while (x < n) if (y < m) y := y + 1; else \nx := x + 1; NestedMultiple(int x0, y0, n, m) x := x0; y := y0; while (x < n) while (y < m &#38;&#38; \nnondet()) y := y + 1; x := x + 1; SimpleMultipleDep(int n, m) x := 0; y := 0; while (x < n) if (y < m) \ny++; else y := 0; x++; NestedMultipleDep(ix := 0; while (x < n) y := 0; x := x + while (y < m) y := y \n+ 1; nt n, m) 1; Figure 4. (a) This set of diverse examples demonstrate the need for multiple counters. \nThis is because if a single counter is used, then the invariants required to establish bounds are either \ndisjuntive(SingleMultiple or NestedMultiple)or non-linear(SingleMultipleDep or NestedMultipleDep). (b) \nAlso, the examples SingleMultipleDep and NestedMultipleDep demonstrate the need for creating a dependencyedge \nbetweentwo countersinDAG G (which corresponds to initializing one of the counters to 0 whenever the other \ncounter is incremented). Thisis becauseif multiple counters are used,but are not relatedin theDAG G, \nthen the invariant required to establish bounds is still non-linear. which in turn, may lead to the generation \nof a bound at back-edge q by the invariant generation tool. EXAMPLE 3. We brie.y illustrate the working \nof the algorithm on Equals procedureinFigure1 togenerate theproof structurein Example1.IftheloopinLine8startsoutby \nmappingback-edges corresponding to the outer loop and the .rst inner loop to some counter variable(s), \nit won t succeed. However, the loop would succeed in mapping the back-edges for the second and thirdinner \nloops to newcounters c1 and c2 respectively that need be dependent only on r; in this setting, invariant \ngeneration tool is able to bound c1 and c2 by Length(s1) and Length(s2) respectively at the respective \nback-edges. The algorithm then performs one more iteration of the outermost loop in line 6 and now the \nloop in Line 8 succeeds in mapping the back-edges corresponding to outer loop and the .rst inner loop \nto a new counter. The algorithm further realizes that they can be mapped to the same counter c3, but \nc3 should be dependent on r, c1,c2; in this setting, invariant generation tool is able to boundc3 by \nsize at both the back-edges. 4.1 Correctness It is not dif.cult to see that the algorithm generates \na counter\u00adoptimal proof structure (if it generates one). However, the inter\u00adesting part is that the algorithm, \nin spite of its greediness to be counter-optimal, does generateaproof structure,if thereexists one. The \nfollowing theorem establishes these properties. THEOREM 2. The algorithm alwaysgeneratesaproof structure,if \nthereexists one. Furthermore,if the algorithmgeneratesaproof structure,itgeneratesa counter-optimal one. \nPROOF: It is easy to see that if the algorithm generates a proof structure, then it generates a counter-optimal \none. This is because the algorithm generates a new counter for a back\u00adedge (Line 13) only when the back-edge \ncannot be mapped to anexisting counter(see conditionalin Line9). This implies that the new counter cannot \nbe merged with the existing counters. Hence, the proof structure generated has a minimal number of counters. \nAlso, the number of dependencies created for a new counter is minimal as is evident from the loop in \nLines 19-21 that exhaustively tries to remove anyredundant dependency. We now show that the algorithm \ngenerates a proof structure, if thereexists one. This follows from thefactin each iterationof the loop \nin Lines 8-21, the tuple (S, M, G, B) forms a exten\u00adsible partial-proof structure (as de.ned below in \nDe.nition 6), which canbeextendedto forma proof structure. Thisfact can be proved by induction on the \nnumber of loop iterations using Theorem3stated below, whose proofisgivenin the fullver\u00adsion of the paper \n[13]. D DEFINITION 6 (ExtensiblePartial-proof Structure). A partial-proof structure is a tuple (S, M, \nG, B) that satis.es all properties of a proof structure except we allow M and B to be partialfunctionsoverback-edges(in \nwhich casethe de.nitionof Instrument(P, (S, M, G)) is the same asin De.nition2 except that we only instrument \nthose back-edges that are mapped by M). A partial-proof structure (S, M, G, B) is extensible ifit canbe \nextended (by extending the maps M and B, as well as the DAG G)to yield a proof-structure. THEOREM 3. \nLet (S, M, G, B) be any extensible partial-proof structure. Let (S ' ,M ' ,G ' ,B ' ) be any partial-proof \nstructure such that the triple (S ' ,M ' ,G ' ) is obtained from (S, M, G) by a node merging operation \n(De.nition 3) or an edge deletion operation (De.nition 4). Then, (S ' ,M ' ,G ' ,B ' ) is anextensible \npartial-proof structure. 5. Quantitative Functions over Data-structures In this section, we introduce \nthe notion of user-de.nable quanti\u00adtative functions over abstract data-structures (those that are refer\u00adenced \nand updated through a well-de.ned interface). These func\u00adtions serve two purposes. They allow a linear \ninvariant generation tool with support for uninterpreted functions to discover linear bounds over counter \nvariables (in terms of these quantitative functions of input data\u00ad GenerateCounterOptimalProofStructure(ProcedureP \n) 1 S := \u00d8; 2 foreach back-edge q 3 M(q) . undefined; B(q) . undefined; 4 G := Empty DAG; 5 change := \ntrue; 6 while change 7 change := false; 8 foreach back-edge q s.t.M(q) is undefined 9 If .c . S s.t.((B \n' := Gen(S, M[q . c],G)) = .)  10 M(q) . c; B := B ' ; 11 change := true; 12 Else 13 Let c be some fresh \ncounter variable. S '' 14 := S .{c}; M = M[q . c]; G ' '' 15 := G .{(c ,c) | c . S, c ' = c}.{(r, \nc)}; 16 if (B ' := (Gen(S ' ,M ' ,G ' )) == .) continue; 17 (S, M, G, B) := (S ' ,M ' ,G ' ,B ' ); 18 \nchange := true; 19 foreach (c ' ,c) . G: 20 if ((B ' := Gen(S, M, G -{(c, c ' )})) = .) 21 G := G -{c \n' ,c)}; B := B ' ; 22 if . backedge q s.t.M(q) is undefined, then Fail 23 return (S, M, G, B); Gen(S, \nM, G) 1 P ' := Instr(P, (S, M, G)); 2 Run invariant generation tool on P ' to generate invariant Iq \nat any back-edge q in P ' . 3 := empty map; B ' 4 foreach back-edge q in P ' s.t. M(q) is defined 5 Iq \n' := Existentially eliminate all variables from Iq except counter M(q) and inputs. 6 If Iq ' implies \nan invariant of form M(q) = u, 7 B ' := B ' [q . u]; 8 Else return .; 9 return ; B ' Figure5. Algorithmto \nconstructacounter-optimal proof-structure for an input procedure P , if there exists one. The function \nGen(S, M, G) returns a map B that maps each back-edge q, s.t. M(q) is de.ned,to some boundon countervariable \nM(q) at back\u00adedge q in the procedure Instrument(P, (S, M, G)) (obtained by running the invariant generation \ntool and existentially eliminating the temporary variables from the invariant at q); if no bound could \nbe computed for anysuch back-edge q, it returns .. structures). The invariant generation tool need not \nbother about sophisticated heap shapes. They are very readable since they have been de.ned by the user \nherself. Hence, these allow the user to get a quick sense of the complexity of a procedure (which is \notherwise expressible using only a sophisticated logical formula over heap shapes). Each quantitative \nfunction is associated with a tuple of abstract data-structures. In that regard, a quantitative function \nis similar to a ghost .eld except that a quantitative function can be associated withatupleof data-structures,whileaghost \n.eldis associated with a single data-structure. EXAMPLE 4. Consider a List data-structure that maintains \na linked list of objects whose type is ListElement.Wecan associate a quantitative function Len with a \nList L (supposed to denote the length of list L)and we can associate a quantitative functionPos witha \npairof ListElement e and List L (supposed to denote the positionofalist-element e inside list L,ife belongs \nto L;otherwise it is don t care). The user annotates each method of an abstract data-structure with how \nit may affect the quantitative attributes of the input data\u00adstructures, and how it determines the quantitative \nattributes of the output data-structures. These effects are speci.ed in an imperative style by a sequence \nof (possibly guarded) assignments and assume statements using the program syntax except that the user \nmay also use quantitative functions (applied to appropriate arguments) wherever numeric variables can \nbe used. The user can use only thosevariablesthatareinscopeatthemethod declarationlevel(i.e., the inputs \nto the method, and the outputs of the method) with one exception.Wealsoallowforuseof freshvariablesontheleftsideof \nan assignment with the interpretation being that the assignment is for all possible instantiations of \nthat fresh variable. This is because a method may change the quantitative attributes associated with \nmultiple objects that are not in scope when the method is invoked. EXAMPLE 5. Figure 6(a) describes the \neffects of someList meth\u00adods on quantitative functions Len and Pos (more details in [13]). The method \nL1.MoveTo(e, L2) removes element e from list L1 and inserts it at beginning of list L2 (and has precondition \nthat e belongs to L1). Other methods are self-explanatory. The effect of method call L1.MoveTo(e, L2) \ninvolves decrementing Len(L1) and incrementing Len(L2) by1. Furthermore,inserting an element at the beginning \nof L2 increases position Pos(e ' ,L2) of all list\u00adelements e ' in L2 by 1. Stating this requires use \nof a free variable e '. Similarly, removal of an element from L1 decreases (by 1)posi\u00adtion Pos(e ' ,L1) \nof all list-elements e ' that are after e in L. Principles There aretwo principlestokeepin mind when \nde.n\u00ading these quantitative functions. (Precision) In general, de.ning more quantitative functions in\u00adcreases \nthe possibility that the invariant generation tool will be ableto computeabound.However,we observedthatforseveral \ncommonlyused data-structuresinthe sourcecodeofalargeMS product code, the number of quantitative functions \nrequired for computing bounds is small.  (Soundness) Whatever quantitative functions the user de.nes, \nthey are always sound from the tool s viewpoint since it takes the semantics of these functions is what \nthe user de.nes them to be. However, the user has an intended semantics for these func\u00adtions in her mind. \nIt is thus the user s responsibility to ensure that the user has conservatively estimated the effect \nof different methodsoverthe quantitativeattributesofdifferentobjectswith respect to the intended semantics. \n(A recent work [14] can be used to check the soundness of the user speci.cations w.r.t. the intended \nsemantics, if the intended semantics can be described as the size of some partition in an appropriate \nlogic.)  We carried out an exhaustive case-study of the source code of a large Microsoft product to \nidentify commonly used data-structures: lists, list of lists, bit-vectors, trees.We found that the above \nmen\u00adtioned principles are easy to satisfy for these data-structures. In particular, we found that a few \nquantitative functions are effective enough to express the timing complexity (as well as the invariants \nrequired to compute the timing complexity) of a variety of loops that iterateover the corresponding data-structure.We \nalso observe that it is easy and natural to write down the update to these quan\u00adtitative functions for \nthe methods supported by the corresponding data-structure. 5.1 Invariant Generation over Quantitative \nFunctions In order to allow for invariant generation over quantitative func\u00adtions, we need to communicate \nthe meaning of the quantitative List Operation Effect on Quantitative Functions e := L.Head() Assume(e \n= null . Len(L) = 0);Assume(e = null . Len(L) > 0);Pos(e, L) := 0; t := L.IsEmpty() Assume(t = true . \nLen(L) = 0);Assume(t = false . Len(L) > 0) e1 := L.GetNext(e2) Pos(e1, L) := Pos(e2, L) + 1;Assume(0 \n= Pos(e2, L) < Len(L)); L1.MoveTo(e, L2) Len(L1) := Len(L1) - 1;Len(L2) := Len(L2) + 1;Pos(e, L2) := \n0;Pos(e ' , L2) := Pos(e ' , L2) + 1; if (Pos(e, L1) < Pos(e ' , L1)) Pos(e ' , L1) := Pos(e ' , L1) \n- 1; (a) Semantics of Quantitative Functions Len and Pos Some looping patterns over lists (from Microsoft \nproduct code) Loop Invariant Complexity 1. for (e := f; e = null; e := L.GetNext(e)); c = Pos(e, L)-Pos(f, \nL) . Pos(e, L) = Len(L) Len(L)- Pos(f, L) 2. for (; !L.IsEmpty(); L.RemoveHead()); c = Old(Len(L)) - \nLen(L) . Len(L) = 0 Old(Len(L)) 3. for (e := L.Head(); e = null; ) tmp := e;e := L.GetNext(e);if (*)L.Remove(tmp); \nc = Pos(e, L) + Old(Len(L)) - Len(L) . Pos(e, L) = Len(L) Old(Len(L)) 4. T oDo.Init();Done.Init();L.MoveTo(L.Head(), \nT oDo); while (!T oDo.IsEmpty()) e := T oDo.Head();T oDo.RemoveHead(); Done.Insert(e); foreach successor \ns in e.Successors() if (L.Contains(s)) L.MoveTo(s, T oDo); c1 = Old(Len(L)) - Len(L) - Len(T oDo) . c1 \n= Len(Done) . Len(L) = 0 . Len(T oDo) = 0 Old(Len(L)) for (e := Done.Head(); e = null; e := Done.GetNext(e)); \nc2 = Pos(e, Done) . Pos(e, Done) = Len(Done)) .Len(Done) = Old(Len(L) Old(Len(L)) (b) Examples Figure \n6. InTable(b), Column1containsexamplesof looping patternsover lists fromMS product code. Column2describes \n(interesting partof)the inductiveloopinvariant (computedbyourinvariant generationtool)that relatesan \ninstrumentedloop counter c with appropriate quantitative attributes. Column3shows an upper bound on loop \niterations as obtained from theinvariantin Column2. functionstotheinvariant generationtool.Wedothisbyinstrument\u00ading \neach data-structure method call-site with the effect that it has on the quantitative functions of inputs \nand outputs of the method call. This is done by substituting the formals input and return parame\u00adtersinthe \nuser speci.cationbythe actualsatthecallsite.Theonly issue is with respect to the assignments that involve \nfree variables in the speci.cation. These can be handled by instantiating these assignments with all \nexpressions of appropriate type that are live at that program point. However, this (potentially expensive) \neager approach can be avoided by instantiating these assignments (dur\u00ading the invariant generation process) \nwith only those expressions that are present in the invariants computed by the (.ow-sensitive) invariant \ngeneration tool immediately before the method call site. The above instrumentation allows for treating \nthe quantitative functions as uninterpreted functions because their semantics has explicitly been encoded \nin the program. Now, we can simply use a linearinvariant generationtoolthathasbeenextendedwith support \nfor uninterpreted functions and aliasing 3 to compute linear invari\u00adants over quantitative functions. \nWe knowoftwo techniques thatextenda linearinvariant gener\u00adation tool with support for uninterpreted functions. \nAbstract Interpretation based technique. Gulwani and Tiwari have described a general mechanism for combining \nthe transfer functions of two given abstract interpreters to generate an ab\u00adstract interpreter than can \ndiscover invariants over combination of domains [16].We can use this methodology to combine an abstract \ninterpreter for linear arithmetic (such as the one based on polyhedron domain [7]) with an abstract interpreter \nfor un\u00adinterpreted functions [15].We have implemented our invariant generation tool using this methodology. \n3Aliasing is required to reason whether an update to an uninterpreted function such as Len(L1) can affect \nLen(L2): this is done by checking whether or not L1 and L2 are aliased. Constraint-based invariant generation \ntechnique. Beyer et al. have described how to extend constraint-based techniques for generating numerical \ninvariants to synthesis of invariants ex\u00adpressible in the combined theory of linear arithmetic and unin\u00adterpreted \nfunction symbols [2]. Unlike .xed-point computation based techniques like abstract interpretation, constraint-based \ntechniques are goal-directed and do not suffer from precision losses due to widening. However, these \nadvantages come at the cost of programmer speci.ed invariant templates. Our applica\u00adtion is a good .t \nfor such a scenario because we are indeed lookingforaspeci.ckindofinvariant,onethat relatestheloop counter \nwith quantitative functions. Examples Figure 6(b) shows some examples of looping patterns over lists \nfrom Microsoft product code. Column2of the table lists the interesting part of the inductive loop invariant \ngenerated by our invariant generation tool after these loops have been instrumented with a single counter \nc. Column3lists the bounds computed from these invariants (by existential elimination of all temporary \nvari\u00adables or variables that get modi.ed in the program). Some of these invariants use the term Old(t) \nthat referstothevalueof t at the be\u00adginning of the procedure (It is useful to create a copyof the quanti\u00adtativefunctions \nof input data-structures at the beginning since these might get destructively updated in the program). \nExample1iteratesovera list starting froma list-element f in list L and following the next links. Example2 \niteratesovera list by deleting its head in each iteration. Example3is more interest\u00ading and combines \nlist enumeration with destructive update of the list. Note that the inductive loop invariant (which is \nautomatically discovered) is a bit trickyin this case. Example4is the most challengingexample because \nthe while loop iterates over a T oDo list whose length may decrease as well as increase in each loop \niteration. Overall, elements are moved from the input list L to the T oDo list, which are then moved \nto Done list. However, the order in which the vertices are moved to Done list is the depth-.rst traversal \norder of the list-elements e, which are also graph nodes whose successors are given by the Successors \nmethod. Bounding the loop iterations of the outer while loop requires computing the non-trivial invariant \nc1 = 1+ Old(Len(L)) - Len(L) - Len(T oDo), which is something that is easily computed by our invariant \ngeneration tool. Also, note that it is easy to see that an upper bound on the number of loop iterations \nof the for-loop (after the while-loop) is the length of the Done list. However, computing this upper \nbound in terms of the inputs requires relating the length of the Done list in terms of the length of \nthe input list L;this relationship is:Len(Done) = Old(Len(L)). Discovering this relationship requires \ncomputing the loop invariant Len(Done) = 1+ Old(Len(L)) - Len(L) - Len(T oDo) in the .rst loop, which \nis again easily computed by our invariant generation tool. This illustrates another advantage of the \nquantitative functions in the overall process. The quantitative functions are not only useful forexpressing \nloop bounds,but also allow the invariant generation tool to relate numerical properties of different \ndata-structures, which is important to express the loop bounds in terms of inputs.  5.2 Composite Data-structures \nComposite data-structures like listof lists, array of lists (hastables) or n-ary trees, may have more \ninteresting quantitative attributes that can be associated with constituent data-structures. This hap\u00adpens \nwhen the quantitative attribute of a top-level data-structure may be a function of the quantitative attributes \nof the nested data\u00adstructures. A challenge that arises in such situations is that update of a nested \ndata-structure may not only affect the quantitative functions of the nested data-structure, but may also \naffect the quantitative functionsof the top-level data-structure.To addressthis, we pro\u00adpose de.ning \nanother functionatthelevelofanested data-structure thatmapsittothetop-level data-structureofwhichitisapartof.A \ndisadvantage of this approach is that it is not modular. However,we feel that this will notbea problemin \npractice since the annotations are only provided at the data-structure level. We illustrate this methodology \nfor some useful quantitative functions that can be associated with a list of lists, besides the functions \nLen and Pos de.ned earlier. Let L be anytop-level list of elements e, where each element e is a list \nof nodes f . TotalNodes(L): Sum of length of all lists e ', where e ' is an element of L. TotalNodes(L)= \nSum{Len(e ' ) | L.BelongsT o(e ' )} MaxNodes(L): Maximum length of any list e ', where e ' is an element \nof L. MaxNodes(L)= Max{Len(e ' ) | L.BelongsT o(e ' )} TotalPos(e, L): Sum of lengths of all lists e \n', where e ' lies before e in L (i.e., if e belongs to L, otherwise it is don tcare). TotalPos(e, L)= \nSum{Len(e ' ) | L.BelongsT o(e ' ) . Pos(e ' ,L) < Pos(e, L)} Note that the quantitativeattribute TotalNodes(L) \nof the top-level list L gets affected whenever anychange is made to list e. In order to appropriately \nupdate TotalNodes(L) in such cases, we propose introducing a function Owner that maps e to its top-level \nlist L. This idea is borrowed from the literature on ownership .elds [22]. Owner(e):Top-level list L \nto which the nested list e belongs. (If e is not a nested list, then Owner(()e) is don tcare.) Table \n(a) in Figure 7 describes the updates to these functions by some list operations. (For a more detailed \ntreatment, see [13].) Iterate(unsigned int a) {1 b := a; 2 while ( BitScanForward(&#38;id1,b)) //set \nall bits before id1 in b 3 b := b | ((1 << id1) - 1); 4 if ( BitScanForward(&#38;id2, ~ b)) break; //reset \nall bits before id2 in b 5 b := b &#38;(~((1 << id2) - 1)); 6 } Figure 8. One of many loops that iterate \nover bit-vectors taken from MS product code. The BitScanF orward(&#38;id, b) function returnsanon-zerovalueiffthe \nbit-vector b containsa 1 bit, in which case id is set to the position of the least signi.cant 1 bit. \nRows 1,2 show how these functions are affected by two list op\u00aderations wheninvokedovera listof lists.Row3showshow \nthese functions are affected by Remove operation when invoked over a nested list. Table(b)in Figure7shows \nsomeexamplesof looping patterns over list of lists from Microsoft product code. Example1 iterates over \nthe top-level list, and hence its complexity is simply the length of the top-level list. Example2iteratesoverthe \ntop-level list,but also processes all nodes in the nested lists, and hence its complexity is the sum \nof the top-level list and the lengths of all the nested lists, which can be expressed using the quantitative \nfunctions Len and TotalNodes. Example3.ndsa speci.cnodeinthe nestedlistand does this by walking over \nthe top-level list and then walking over an appropriate nested list. Its complexity can be expressed \nusing the quantitative function MaxNodes. Example4is most interesting sinceitwalksover the top-level \nlist L as well as all the nested lists deleting each element one by one. Its complexity is expressible \nusing the quantitative functions Len and TotalNodes. However, the interesting point to note here is that \nthe top-level list as well as the nested lists are being de\u00adstructively updated while they are being \ntraversed. The destructive update to a nested list e also requires an update to the quantitative attributes \nof the top-level list L, which is accessed using Owner(e).  5.3 Applicability of Quantitative Functions \nThe methodology of quantitativefunctions need not be restricted to recursive data-structures,but can \nalsobe appliedto data-structures like bit-vectors (which are otherwise hard to reason about). Bit\u00advectors \nhave quite a few interesting quantitative functions associ\u00adated with them. E.g., total number of bits: \nBits(a), total number of 1 bits: Ones(a), position of the least signi.cant 1 bit: One(a), etc. The full \nversion of the paper [13] describes these quantitative functionsin more detailandtheeffectof bit-wise \noperatorson these functions,andalsogivesexamplesofseveral looping patternsfrom Microsoft product code-base \nthat can be analyzed using these func\u00adtions. Figure 8 describes one of such examples Iterate, which masks \nout the least signi.cant consecutive chunk of 1s from b in each loop iteration. Our tool is able to compute \nthe inductive loop invariant 2c = 1+One(b)-One(a) . c = 1+Ones(a)-Ones(b) whentheloopis instrumentedwiththe \ncountervariable c. This im\u00adplies bounds of both Ones(a) as well as (Bits(a) - One(a))/2 on the total \nnumber of loop iterations. The full version of the paper [13] also describes interesting quantitative \nfunctions for trees, namely number of nodes in a tree, or the height of a tree, and also describes several \nlooping patterns from STL library code-base that can be analyzed using these quan\u00adtitative functions. \nOne such example is also provided in Figure 9. List Operation Effect on Quantitative Functions 1. e1 \n:= L.GetNext(e2) TotalPos(e1, L) := TotalPos(e2, L) + Len(e2);Assume(TotalPos(e1, L) = TotalNodes(L)); \nAssume(Len(e1) = MaxNodes(L));Owner(e1) := L; 2. L.Insert(e) TotalNodes(L) := TotalNodes(L) + Len(e);MaxNodes(L) \n:= Max(MaxNodes(L), Len(e)); TotalPos(e ' , L) := TotalPos(e ' , L) + Len(e);TotalPos(e, L) := 0;Owner(e) \n:= L; 3. e.Remove(f) TotalNodes(Owner(e)) := TotalNodes(Owner(e))-1; if (*) MaxNodes(Owner(e)):=MaxNodes(Owner(e))-1; \nif (Pos(e, Owner(e)) < Pos(e ' , Owner(e))) TotalPos(e ' , Owner(e)) := TotalPos(e ' , Owner(e)) - 1; \n (a) Semantics of Quantitative Functions TotalNodes,MaxNodes,TotalPos and the function Owner. Looping \npatterns over list-of-lists (from MS product code) Loop Invariant Complexity 1. for (e := L.Head(); e \n= null; e := L.GetNext(e)); c = Pos(e, L) . Pos(e, L) = Len(L) Len(L) 2. for (e := L.Head(); e = null; \ne := L.GetNext(e)) for (f := e.Head(); f = null; f := e.GetNext(f)); c = Pos(e, L) + TotalPos(e, L) . \nPos(e, L) = Len(L) . TotalPos(e, L) = TotalNodes(L) Len(L)+ TotalNodes(L) 3. for (e := L.Head(); e = \nnull; e := L.GetNext(e)) if (rand()) break; for (f := e.Head(); f = null; f := e.GetNext(f)); c = Pos(e, \nL) + Pos(f, e) . Pos(f, e) = Len(e) . Len(e) = MaxNodes(L) Len(L)+ MaxNodes(L) 4. for (e := L.Head(); \ne = null; e := L.RemoveHead()) for (f := e.Head(); f = null; f := e.RemoveHead()); c = Old(Len(L)) - \nTotalNodes(L) +Old(TotalNodes(L)) - Len(L) . Owner(e) = L Old(Len(L))+ Old(TotalNodes(L)) (b) Examples \n Figure 7. InTable (b), Column1contains someexamplesof looping patternsover listsof lists fromMS product \ncode. Column2describes (interesting part of) the inductive loop invariant, as computed by our tool, after \nthe back-edges in each example were all instrumented with the same counter c. Invariants shown are for \nouter loop (Examples 1,2,4) and for the second loop (Example 3). Column3shows an upper bound on loop \niterations as obtained from the invariant in Column 2. 6. Inter-procedural Computational Complexity Theorem \n1 described in Section 4 describes a strategy to com\u00adpute symbolic bound on the total number of loop \niterations given a proof structure.We can usea similar strategy to compute sym\u00adbolic bounds on the total \ncost of a procedure given any cost met\u00adric that maps atomic program statements (i.e., non procedure-call \nstatements)to some cost.Forexample,ifweassociateaunit cost with each statement, then we obtain a symbolic \nbound on the total number of statements executed by the procedure. If instead we as\u00adsociate each statement \nwith the amount of resources it consumes 4 (e.g., memory it allocates), we obtain a bound on the total \nresource consumption of the procedure. The cost of procedures in a program is computed in a bottom\u00adup \nordering of the call graph. Since the call graph can have cycles because of recursive procedures, we \ndecompose the call graph into a DAG of maximal strongly connected components (SCCs) and process thisDAGin \nreverse topological order. Non-recursive Procedures For a non-recursive procedure (not a part of anynon-trivial \nSCC), we use the approach mentioned below. We de.ne the cost IqI of a back-edge q between locations f \nand f ' to be the maximum of the costs of any (acyclic) path that starts at procedure entry or any counter \ninstrument location, and ends at f without visiting any other counter instrument location. We de.ne the \ncost of anyacyclic path to be the sum of the cost of all statements on the path and the cost of executing \nanyprocedure call. The cost Icall Q(vv)I of executing procedure call Q(vv) is obtained from the cost \nIQI of procedure Q, which has already been computed in terms of the formal parameters vy of procedure \nQ. We .rst replace the formal parameters vy in Q by actuals vv. Since vv might not be the inputs of the \nprocedure whose summary is 4Sometimes resources can also be released by a statement (as in case of memory \ndeallocation), in which case, one needs to obtain a lower bound L onthe amountof resource releasedbythe \nstatement,and then associate that statement with a cost of -L. being computed, we need a way to express \nIQI[vv/vy] as a function of procedure inputs vx.We do this by making use of the invariant (generated \nby the invariant generation tool) at the call site Icall (which relates the actuals vv with the procedure \ninputs vx)as follows. Icall Q(vv)I := Projectupper(IQI[vv/vy],Icall , vx) where the function Project(e, \nf, V ) denotes an upper bound upper on variable t thatis impliedby the formula .V ' : t = e . f, where \nV ' is the set of all variables that occur in e and f except V , and t is some fresh variable.5 See an \nexample below. EXAMPLE 6. Supposeweare computinga summaryforprocedure P (x1,x2) that calls a procedure \nQ(y1,y2) whose summary is already computed as y1 - 2y2. Suppose P calls Q with arguments v1,v2, where \nv1,v2 are such that (v1 - v2 = x1) . (v2 = x2). Then, the above Projectoperation helps estimate an upper \nupper bound on the cost of calling Q inside P as x1 - x2 by existentially quantifying out v1,v2 from \nthe formula .v1,v2[t = v1 - 2v2 . (v1 - v2 = x1) . (v2 = x2)] to obtain t = x1 - x2. Having de.ned the \ncost of a back-edge q (with respect to any given cost metric over statements), we now describe in the \ntheorem below how to compute the total cost of a non-recursive procedure. Proofof Theorem4follows easily \nfrom the proofof Theorem1. THEOREM 4. The total cost IP I of a procedure P given a cost metric for atomic \nstatements, and a proof structure (S, M, G, B) 5The function Projectisusedto computeboundoncomplexityofa \nupper procedure call Q(vv) in terms of the procedure inputs vx. (Note IQI has been computed in terms \nof formals vy.We replace that by actuals vv,but we still need to relate that to vx).Theexistential formula.V \n' : t = e.f formalizes the meaning of Project. Essentially, we want to eliminate variables upper froma \nboundsexpressionin presenceof someinvariants,butthe notionof existential eliminationisonly de.nedfor \nformulas.Useofvariablet allows usto modelthe problemofexistential elimination fromanexpressiontothe problem \nof existential elimination from a formula. Traverse(Tree T, TreeElt e) 1 y := e; Tree T ' ; 2 while (y \n= null) TreeElt e ' ; 3 c := c + 1; int c; 4 r := T.GetRight(y); Traverse ' (Tree T, TreeElt e) T '' \n5 Traverse(T,r); := T; e := e; c := 0; 6 y := T.GetLeft(y); Traverse(T, e); 7 c := c + 1; Figure 9. An \nexample of a recursive procedure Traverse that calls itself recursively inside a loop in presence of \nrecursive data\u00adstructures. The bold instrumentation of global counter variable c inside procedure Traverse \nand the construction of procedure Traverse ' is part of our proof methodology for bounding number of \nrecursive procedure call invocations as well as loop iterations in the procedure Traverse. can be computed \nas X IP I = (1+ TotalBound(c)) \u00d7 Max{IqI| M(q)= c} c.S where TotalBound(c) is as de.ned in Theorem 1. \nRecursive Procedures Let P1(vx),...,Pn(vx) be some set of mu\u00adtually recursive procedures.For each procedure \nPi(vx), we create a new procedure Pi ' (vx) that simply calls Pi(vx) after copying vx into global xv'. \nNow,to compute the complexity of anyprocedure Pi,we simply put the procedures Pi ' along with the procedures \nP1,...,Pn intoamodule, and marking procedure Pi ' as an entry point.Wenow runthe algorithminFigure5for \ngeneratinganoptimalproof struc\u00adture over this module with the following differences. The proof structure \nnowhas an additional requirement. The map M is required to map back-edges in all the procedures as well \nas the locations immediately before anyrecursive procedure call site to some global counter variable. \nA global counter variable is in\u00adcremented and initialized in Instrument(P, (S, M, G)) inexactly the same \nmanneras local countervariables with oneexception.If r is an immediate predecessor of c in theDAG G, \nthen c is initialized to 0 at the beginning of the entry point procedure Pi ' (as opposed to anyof the \nrecursive procedures). The correctness of the algorithm now follows from the observation that a global \ncounter introduced before a recursive call-site is counting how manytimes is that call\u00adsiteinvoked (just \nasa counter variable ata back-edge counts the number of iterations of the back-edge). Also, another difference \nis that the invariant generation tool is supposed to compute bounds on the global counter variables in \nterms of the globals vx ' (as opposed to the procedure inputs vx)in an inter-procedural setting. (After \ncomputation of these bounds, we v' simply replace x by vx). An intra-procedural invariant generation \ntool can be extended to an (context-insensitive) inter-procedural setting using the standard two phased \napproach. The .rst phase computes procedure summaries that relate the inputs of a proce\u00addure to the outputs. \nThe actual results are computed in the second phase, wherein inputs are propagated down the call graph \nfrom the entry points.We implemented this algorithm and wegive below an example of the invariants and \nbounds that were generated. EXAMPLE 7. Consider the recursive procedure Traverse inFig\u00adure 9 taken from \nC++ STL library code. The procedure calls it\u00adself recursively inside a loop to traverse a tree. The procedure \nhas been instrumented with a global variable c to keep track of the number of recursive procedure call \ninvocations and loop itera\u00adtions. The global counter c is initialized inside the entry procedure Traverse \n', whichsimply invokes Traverse after copying its ar\u00adguments T and e into globals T ' and e '. Let Nodes(e, \nT ) refer to a quantitative function that denotes the number of nodes be\u00adlow node e in tree T . The inter-procedural \ninvariant generation scheme described above computes the inter-procedural invariant '' c = 2(1 + Nodes(e \n,T ) - Nodes(e, T )) 6 (after computing the out-c proceduresummary c in = 2+2(Nodes(e, T )),whichin turn \nin '' requires the inductive loop invariant c - c = 2(Nodes(e ,T ) - Nodes(y, T )) at program point 3). \nExistential elimination of input parameter e yields the relation c = 2(1 + Nodes(e ' ,T ' )), thereby \nimplying a bound of 2(1 + Nodes(e, T )) (after we rename e ' back by e in the relation) on the recursive \ncall invocations and loop iter\u00adations. Having obtained an appropriate proof structure (which has counter \nincrements at recursive call-sites), we now use the same process that we used for non-recursive procedures \n(described in Theorem 4) to compute the total cost. The cost of a recursive call\u00adsite location s is de.ned \nin a similar way that we de.ne the cost of a back-edge. Note that now an acyclic path will not refer \nto anyof the recursive procedures P1,...,Pj , since we have instrumented a counter increment at those \nlocations (in the extended notion of proof structure for recursive procedures). 7. Preliminary Evaluation \nWehavebuilta prototype tool called SPEED that implements the ideas described in this paper and automatically \ncomputes symbolic complexity bounds of procedures (written in C/C++) in terms of the scalar inputs of \nthe procedure as well as anyuser-de.ned quan\u00adtitative functions. Our implementation uses the Microsoft \nPhoenix compiler infrastructure [26] for the front-end. We have imple\u00admented the abstract interpretation \nbased invariant generation tool in C#, as a Phoenix plug-in. Our tool operates by slicing a procedure \nwith respect to the statements that affect the number of loop iterations (by track\u00ading statements that \naffect the conditionals in loop exit nodes). The sliced version is usually much smaller, and is thus \na useful optimization for our invariant generation tool. We avoid inter\u00adprocedural invariant generation \nfor non-recursive procedures by simply inlining part of the called procedures that affect the number \nof loop iterations. This happens for a relatively very few cases. Our tool is able to automatically compute \nbounds for all the examples presented in this paper and takes on an average 0.25 seconds for each of \nthe examples. Even though the size of the examples as described in the paper is small, these examples \nare sliced versions of large procedures (ranging up to few hundreds of lines of code) from real code. \nWe have also run our tool on several other examples from Microsoft product code and C++ STL Library code. \nOur tool is able to compute precise bounds for more than half of the loops that we have seen in practice. \nAmong the examples that our tool is able to handle, there are several examples with loops whose nesting \ndepth goes up to 5, while their complexity is still linear or quadratic since they iterate over the same \nvariables in multiple loops. These are loops for which an unsound analysis that simply estimates the \ncost of a procedure by measuring the nesting depth would provide too imprecise results. The cases that \nwe have found our tool currently cannot handlefall into oneoftwo categories: 6In other words, the value \nof global counter c is proportional to the number of nodes that have been traversed, which is equal to \ndifference of total ' nodes Nodes(e ,T ' ) and nodes yet-to-be traversed Nodes(e, T ). Observe that this \ninductive invariant requires reference to original inputs with which Traverse was .rst invoked. This \nreference is provided by copying of the inputs T and e to fresh globalvariables T ' and e ' in procedure \nTraverse ' . Asimilar techniqueisusedin programveri.cationtoallowfor relatingpre and post states. Loop \nbounds depend on some environment assumption, discov\u00adering which requires a global analysis (while our \ninvariant gen\u00aderation tool performs only local analysis). These environment assumptions range from class \ninvariants (e.g., m> 0, required to prove bounds in case of while(i = n){i := i + m}, or some precondition \non the input (e.g., x<n, required to prove bounds in case of for(i := x; i = n; i++) to some contract \nbetween concurrentlyexecutingthreads,or someliveness prop\u00aderty guaranteed by the operating system (e.g., \na loop calling a low-level system API until it returns true).  Generating linear bounds on counter variables \nrequires path\u00adsensitive invariant generation (while our invariant generation toolis path-insensitive).Forexample, \n while (0=x=n) {if (b) x := x+t; else x:=x-t;} t := b? 1: -1; while (x = n) {if (b) x := x+t; else x \n:= x-t;} We also identi.ed an instance of a recursive procedureV where ourtoolfailedto computeabound,andthe \nreasonwas tracedtothe bound being exponential in size of the input tree. This complexity surprised the \ndeveloper who wrote that code. The looping pattern of the recursive function V(Tree T, TreeElement e) \nconsisted of if (e=null) { V(T ,T .GetLeft(e));V(T ,T .GetRight(e)); if (b){V(T ,T .GetLeft(e));V(T ,T \n.GetRight(e));}} We are currently working with some MS product groups to in\u00adcorporate SPEED into their \ncode check-in process. Theyare specif\u00adically interestedin creatingadatabaseofthe symbolic complexities \nof all the procedures in their huge code-base and then pointing out any signi.cant differences in symbolic \ncomplexity bounds when\u00adever anydeveloper checks in performance-critical code, e.g., code that is executed \nwithin atomic boundaries (probably higher up in the call graph), or code that touches hot data-structures. \n8. RelatedWork Type System Based Approaches Danielsson presented a type system targeted towards reasoning \nabout complexity of programs in lazy functional languages [9]. Crary and Weirich presented a type system \nfor reasoning about resource consumption, including time[8]. HughesandPareto proposedatypeandeffect systemon \nspace usage estimation based on the notion of sized types for a variant of ML such that well typed programs \nare proven to exe\u00adcute withinthegiven memory bounds[21].In these approaches,no effort is made to infer \nany bounds; instead they provide a mech\u00adanism for certifying the bounds once they are provided by the \nprogrammer. In contrast, our technique infers bounds. Hofmann and Jost statically infer linear bounds \non heap space usage of .rst\u00adorder functional programs running undera special memory mecha\u00adnism [20]. \nTheyuse linear typing and an inference method through linear programming to derive these bounds. Their \nlinear program\u00adming technique requires no .x-point analysis but it restricts the memory effects to a \nlinear form without disjunction. In contrast, our focus is on computing time bounds (which are usually \ndisjunc\u00adtive and non-linear) for imperative programs. Worst-case Execution Time Analysis There is a large \nbody of workon estimatingworst caseexecution time (WCET)inthe em\u00adbedded and real-time systems community \n[29, 30]. The WCET re\u00adsearch is more orthogonally focused on distinguishing between the complexity of \ndifferent code-paths and low-level modeling of ar\u00adchitectural features such as caches, branch prediction, \ninstruction pipelines.For establishing loop bounds, WCET techniques either require user annotation, or \nuse simple techniques based on pattern matching [19] or some simple numerical analysis. [17] describes \nan interval analysis based approach for automatic computation of loop bounds. However, it analyzes single-path \nexecutions of programs (i.e., using input data correspondingto oneexecution). Hence, their bounds are \nin real seconds, while our bounds are symbolic func\u00adtions of inputs. [18] determines loop bounds in synchronous \npro\u00adgrams and linearhybrid systemsby usinga relational linear analy\u00adsis to compute linear bounds on the \ndelay or timer variables of the system. In contrast, our focus is on automatic computation of loop bounds \n(in general purpose programs), which are usually disjunc\u00adtive, non-linear, and involve numerical properties \nof heap. None of the WCET techniques can automatically detect such bounds. Termination Techniques Recently,there \nhavebeen some newad\u00advances in the area of proving termination of loops based on discov\u00adering disjunctively \nwell-founded ranking functions [27] or lexico\u00adgraphic polyranking functions [4]. [6, 1] have successfully \napplied the fundamental result of [27] on disjunctively well-founded rela\u00adtions to prove termination \nof loops in real systems code. It may perhaps be possible to obtain bounds from certain kind of ranking \nfunctions given the initial state at the start of the loop. However, the ranking function abstraction \nis too weak to compute precise bounds. As illustrated in Introduction, programs with signi.cantly varying \ncomputational complexity are proved terminatingby [6,1] because of the same ranking function, and hence \nthere is no way to distinguish between the complexities of these procedures given the same termination \nproof. In contrast, our technique can generate more precise timing bounds. Symbolic Bound Generation \nGulavani and Gulwani have de\u00adscribed the design of a rich numerical domain to generate non\u00adlinear disjunctive \ninvariants [12], and theyhave applied it to gen\u00aderating bounds for timing analysis. However, their system \nrequires the user to describe the set of important expressions (over which the linear relationships are \nto be tracked) as well as the set of vari\u00adables that should occur within max operator for each loop. \nFurther\u00admore, their technique only applies to arithmetic programs. How\u00adever,theirworkcanbe betterusedasan \ninstanceofinvariantgener\u00adation tool within our framework (afterextension with uninterpreted functions). \nACE analyzes a functional language FP to derive a time\u00adcomplexity function [24] by repeatedly applying \na large library of rewrite rules to transform the step-counting version of the original recursive program \ninto a non-recursive bound function. However, bound generation is very sensitive to the order in which \nthe rules are applied and the rules required to produce a bound are speci.c to a programming practice. \nRosendahl describes a system to com\u00adpute complexity bounds of programs written in a .rst-order subset \nof LISP using abstract interpretation [28]. His system outputs a time-bound program that is not guaranteed \nto be in closed form. In addition,the techniqueonlyworksfor programs where recursionis controlled by \nstructural constraints, such as length of a list due to limitations of the analysis. [11] computes symbolic \nbounds by curve-.tting timing data obtained from pro.ling. Their technique has the advantage of mea\u00adsuring \nreal amortized complexity; however the results are not sound for all inputs. [25] presents a formalism \nfor user-de.ned resources on which bounds can be computed in terms of input data sizes. In contrast, \nwe focus on computational-complexity bounds and present a for\u00admalism for user-de.ned input data-structure \nsize measures for ex\u00adpressing computational-complexity bounds. Reducing pointers to integer programs \nOur notion of quanti\u00adtative functions is related to recent work on reducing (primarily singly linked \nlist-based) pointers to integer programs [10, 3, 23] after conducting alias analysis in a prepass. In \ncomparison, our approach is limited in two ways. It applies only to abstract data\u00adtypes, and aliasing \nhas to be taken care of by the uninterpreted functions domain.However, our approachhastwokeyadvantages: \nIt allows for arbitrary user annotations for any data-structure at run-time (as opposed to encoding the \nsemantics of the integer vari\u00adables fora speci.c data-structure inside the analysis). Furthermore, these \nannotations are not necessarily limited to pointer based data\u00adstructures, but also apply to say bit-vectors, \nwhich are otherwise quite complicated to reason about. Another technical advantage that our approachoffersis \nthat useof uninterpreted functionsavoids the need for eagerly creating (several) integer variables (since \ncon\u00adgruence based data-structures can represent uninterpreted function terms with common sub-expressions \nsuccinctly). 9. Conclusion This paper describes some fundamental principles for instrument\u00ading monitorvariables \nsuch that computing bounds on these monitor variables and appropriately composing them yields a bound \non the total number of loop iterations. The importance of instrumenting multiple counter variables, each \nof which may be initialized and incremented at multiple locations, lies in being able to use linear in\u00advariant \ngeneration tools to compute precise bounds that are disjunc\u00adtive as well as non-linear (which is usually \nthe case in presence of control-.owinside loops).In particular,(a)Weavoidthe problem of generating disjunctive \ninvariants by introducing multiple coun\u00adters.(b)Weavoid the problemof generating non-linearinvariants \nby introducing dependencies between counters, i.e. initializing a counter at multiplelocations corresponding \nto increment of other counters.(c)We address the issueof precisionby introducing min\u00adimal number of counters \nwith minimal number of dependencies. The paper also introduces the notion of quantitative functions over \nabstract data-structures that allowa linearinvariant generation tool with support for uninterpreted functions \nto compute bounds for loops that would otherwise require sophisticated shape analysis. Based on these \nideas, our tool is able to automatically gener\u00adate (with minimal user annotation for providing quantitative \nfunc\u00adtions) precise timing bounds for sophisticated real-lifeexamples for which it is non-trivial to \neven prove termination. Acknowledgements We thank EricKoskinen for reading an earlier draft of this paper \nand providing useful comments.We thank Lakshmisubrahmanyam Velaga for implementing the abstract interpreter \nfor uninterpreted functions, and a general combination framework for abstract inter\u00adpreters, which was \nused as part of the SPEED tool. References [1] J. Berdine, A. Chawdhary, B. Cook, D. Distefano, andP. \nO Hearn. Variance analyses from invariance analyses. InPOPL, 2007. [2] D.Beyer,T. Henzinger,R. Majumdar,andA. \nRybalchenko.Invariant synthesis for combined theories. In VMCAI 07, pages 378 394. [3] A. Bouajjani, \nM. Bozga, P. Habermehl, R. Iosif, P. Moro, and T.Vojnar. Programs with lists are counter automata. InCAV, \n2006. [4] A. R. Bradley,Z. Manna, and H. B. Sipma. The polyranking principle. In ICALP, pages 1349 1361, \n2005. [5] A. R. Bradley, Z. Manna, and H. B. Sipma. Termination analysis of integer linear loops. In \nCONCUR, pages 488 502, 2005. [6] B. Cook, A. Podelski, and A. Rybalchenko. Termination proofs for systems \ncode. In PLDI, pages 415 426, 2006. [7] P. Cousot and N. Halbwachs. Automatic Discovery of Linear Restraints \namong Variables of a Program. In POPL, 1978. [8] K. Crary and S.Weirich. Resource bound certi.cation. \nIn POPL, pages 184 198, 2000. [9] N. A. Danielsson. Lightweight semiformal time complexity analysis for \npurely functional data structures. In POPL, pages 133 144, 2008. [10] N. Dor, M. Rodeh, and M. Sagiv. \nCSSV: towards a realistic tool for statically detecting allbufferover.owsinC. In PLDI, 2003. [11] S. \nGoldsmith, A. Aiken, and D. S.Wilkerson. Measuring empirical computational complexity. In ESEC/SIGSOFT \nFSE, 2007. [12] B. S. Gulavani and S. Gulwani. Anumerical abstract domain based onexpression abstractionandmax \noperatorwith applicationintiming analysis. In CAV, pages 370 384, 2008. [13] S. Gulwani, krishna Mehra, \nandT. Chilimbi. Speed: Precise and ef.cient static estimation of program computational complexity. Technical \nReport MSR-TR-2008-95, Microsoft Research, 2008. [14]S.Gulwani,T.Lev-Ami,andM.Sagiv. Acombination framework \nfor tracking partition sizes. In POPL, 2009. [15] S. Gulwani and G. C. Necula. A Polynomial-Time Algorithm \nfor Global Value Numbering. In SAS, pages 212 227, 2004. [16]S.GulwaniandA.Tiwari. Combining Abstract \nInterpreters.In PLDI, pages 376 386, 2006. [17] J. Gustafsson, A. Ermedahl, C. Sandberg, and B. Lisper. \nAutomatic derivation of loop bounds and infeasible paths for wcet analysis using abstract execution. \nIn RTSS, pages 57 66, 2006. [18]N.Halbwachs,Y.-E.Proy,andP. Roumanoff.Veri.cationof real-time systems \nusing linear relation analysis. FMSD, 11(2), 97. [19] C. A. Healy, M. Sjodin, V. Rustagi, D. B. Whalley, \nand R. van Engelen. Supporting timing analysis by automatic bounding of loop iterations. Real-Time Systems, \n18(2/3):129 156, 2000. [20] M. Hofmann and S. Jost. Static prediction of heap space usage for .rst-order \nfunctional programs. In POPL, pages 185 197, 2003. [21] J. Hughes and L.Pareto. Recursion and Dynamic \nData-structures in bounded space: Towards Embedded ML Programming. In ICFP, 99. [22] K.R.M. Leino andP.M\u00a8uller. \nObjectinvariantsin dynamic contexts. In ECOOP, volume 3086 of LNCS, pages 491 516, 2004. [23] S. Magill, \nJ. Berdine, E. M. Clarke, and B. Cook. Arithmetic strengthening for shape analysis. In SAS, pages 419 \n436, 2007. [24] D. L.M\u00b4etayer. Ace: An Automatic Complexity Evaluator. ACM Trans. Program. Lang. Syst., \n10(2):248 266, 1988. [25] J. Navas, E. Mera,P.L\u00b4opez-Garc\u00b4ia, and M.V. Hermenegildo. User\u00adde.nable resource \nbounds analysis for logic programs. In ICLP, pages 348 363, 2007. [26] Microsoft Phoenix Compiler Infrastructure, \nhttp://research.microsoft.com/phoenix/. [27] A. Podelski and A. Rybalchenko. Transition invariants. In \nLICS, pages 32 41. IEEE, July 2004. [28] M. Rosendahl. Automatic Complexity Analysis. In FPCA, pages \n144 156,NewYork,NY, USA, 1989.ACM Press. [29] R. Wilhelm, J. Engblom, A. Ermedahl, N. Holsti, S. Thesing, \nD. Whalley, G. Bernat, C. Ferdinand, R. Heckmann, F. Mueller, I. Puaut,P. Puschner, J. Staschulat, andP. \nStenstrm. The Determi\u00adnation of Worst-Case Execution Times Overview of the Methods and Surveyof Tools. \nIn ACMTransactions on Embedded Computing Systems (TECS), 2007. [30] R.Wilhelm andB.Wachter. Abstract \ninterpretation with applications to timing validation. In CAV, pages 22 36, 2008.   \n\t\t\t", "proc_id": "1480881", "abstract": "<p>This paper describes an inter-procedural technique for computing symbolic bounds on the number of statements a procedure executes in terms of its scalar inputs and user-defined quantitative functions of input data-structures. Such computational complexity bounds for even simple programs are usually disjunctive, non-linear, and involve numerical properties of heaps. We address the challenges of generating these bounds using two novel ideas.</p> <p>We introduce a proof methodology based on multiple counter instrumentation (each counter can be initialized and incremented at potentially multiple program locations) that allows a given linear invariant generation tool to compute linear bounds individually on these counter variables. The bounds on these counters are then composed together to generate total bounds that are non-linear and disjunctive. We also give an algorithm for automating this proof methodology. Our algorithm generates complexity bounds that are usually precise not only in terms of the computational complexity, but also in terms of the constant factors.</p> <p>Next, we introduce the notion of user-defined quantitative functions that can be associated with abstract data-structures, e.g., length of a list, height of a tree, etc. We show how to compute bounds in terms of these quantitative functions using a linear invariant generation tool that has support for handling uninterpreted functions. We show application of this methodology to commonly used data-structures (namely lists, list of lists, trees, bit-vectors) using examples from Microsoft product code. We observe that a few quantitative functions for each data-structure are usually sufficient to allow generation of symbolic complexity bounds of a variety of loops that iterate over these data-structures, and that it is straightforward to define these quantitative functions.</p> <p>The combination of these techniques enables generation of precise computational complexity bounds for real-world examples (drawn from Microsoft product code and C++ STL library code) for some of which it is non-trivial to even prove termination. Such automatically generated bounds are very useful for early detection of egregious performance problems in large modular codebases that are constantly being changed by multiple developers who make heavy use of code written by others without a good understanding of their implementation complexity.</p>", "authors": [{"name": "Sumit Gulwani", "author_profile_id": "81100315615", "affiliation": "Microsoft, Redmond, WA, USA", "person_id": "P1300947", "email_address": "", "orcid_id": ""}, {"name": "Krishna K. Mehra", "author_profile_id": "81339516541", "affiliation": "Microsoft, Bangalore, India", "person_id": "P1300948", "email_address": "", "orcid_id": ""}, {"name": "Trishul Chilimbi", "author_profile_id": "81100578606", "affiliation": "Microsoft, Redmond, WA, USA", "person_id": "P1300949", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1480881.1480898", "year": "2009", "article_id": "1480898", "conference": "POPL", "title": "SPEED: precise and efficient static estimation of program computational complexity", "url": "http://dl.acm.org/citation.cfm?id=1480898"}