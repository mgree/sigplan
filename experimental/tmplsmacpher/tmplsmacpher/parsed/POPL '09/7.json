{"article_publication_date": "01-21-2009", "fulltext": "\n Modular Code Generation from Synchronous Block Diagrams Modularity vs. Code Size Roberto Lublinerman \nChristian Szegedy Stavros Tripakis Computer Science and Engineering Cadence Research Laboratories Cadence \nResearch Laboratories Pennsylvania State University Cadence Design Systems and Verimag Laboratory CNRS \n rluble@psu.edu szegedy@cadence.com tripakis@imag.fr Abstract We study modular, automatic code generation \nfrom hierarchical block diagrams with synchronous semantics. Such diagrams are the fundamental model \nbehind widespread tools in the embedded soft\u00adware domain, such as Simulink and SCADE. Code is modular \nin the sense that it is generated for a given composite block indepen\u00addently from context (i.e., without \nknowing in which diagrams the block is to be used) and using minimal information about the inter\u00adnals \nof the block. In previous work, we have shown how modular code can be generated by computing a set of \ninterface functions for each block and a set of dependencies between these functions thatisexportedalongwiththe \ninterface.Wehavealso introduceda quanti.ed notion of modularity in terms of the number of interface functions \ngenerated per block, and showed how to minimize this number, which is essential for scalability. Finally, \nwe have exposed the fundamental trade-offbetween modularity and reusability (set of diagrams the block \ncan be used in). In this paper we explore another trade-off: modularity vs. code size. We show that our \nprevious technique, although it achieves maximal reusability and is optimal in terms of modularity, may \nre\u00adsult in code replication and therefore large code sizes, something often unacceptableinan embedded \nsystem context.We proposeto remedy this by generating code with no replication, and show that this generally \nresultsin somelossof modularity.Weshowthatopti\u00admizing modularity while maintaining maximal reusability \nand zero replicationis an intractable problem (NP-complete).We also show that this problem can be solved \nusing a simple iterative procedure that checks satis.ability of a sequence of propositional formulas. \nWereport ona newprototype implementation andexperimental re\u00adsults. The latter demonstrate the practical \ninterest in our methods. Categories and Subject Descriptors D.2.3[Software Engineer\u00ading]: CodingTools \nandTechniques; D.3.2[Programming Lan\u00adguages]: Language Classi.cations Concurrent, distributed, and parallel \nlanguages, Data-.ow languages; D.3.4 [Programming Languages]: Processors Code generation GeneralTerms \nAlgorithms, Design, Languages Keywords Embedded software, Block diagrams, Synchronous languages, Code \ngeneration, Clustering, NP-complete Permission to make digital or hard copies of all or part of this \nwork for personal or classroom use is granted without fee provided that copies are not made or distributed \nfor pro.t or commercial advantage and that copies bear this notice and the full citation on the .rst \npage.To copyotherwise, to republish, to post on servers or to redistribute 1. Introduction Programming \nmay be art, craft or science. In any case, the tools used today have greatly advanced since the .rst \ndays of program\u00adming. By tools we mean programming languages and compilers, but also support tools such \nas debuggers, static analyzers, etc. The most important class of tools are probably the programming languages \nthemselves, since they are the primary means of captur\u00ading the intent of the designer (or programmer). \nUnderstandably, as these means become morepowerful,by raisingthelevelof abstrac\u00adtion, the designer s \nproductivity increases. In the .eld of embedded, real-time systems, like in manyother .elds, specialized \n(sometimes called domain-speci.c ) languages are used. Simulink from The MathWorks1 and SCADE from Es\u00adterelTechnologies2 \naretwosuccessful commercial productsin this .eld. They are especially widespread in the automotive and \navion\u00adics domains. These toolsofferamixof modeling/programming modelforthe design and implementation \nof embedded software. They provide environments which include graphical model editors, simulators and \ncode generators. Automatic generation of code that implements the semanticsofamodelisusefulindifferentcontexts:thecodecan \nbe usedfor simulation;butit canalsobe embeddedina real-time digital control system(X-by-wire).Infact, \nusagesof the latter type are increasingly adopted by the industry. Thus, these tools can be seen as programming \nlanguages, debuggers and compilers for the embedded software domain. The fundamental model behind notations \nsuch as the above is that of synchronous block diagrams. The latter are hierarchical data.ow diagrams \nwith a synchronous semantics, similar to that of synchronous languages such as Lustre (Caspi et al. 1987) \nor Esterel (Berry and Gonthier 1992). Hierarchyis a powerful concept, used extensively in graphical notations. \nHierarchyallowstobuild designsina modular manner, which is crucial for mastering complexity but also \nin order to address intellectual property (IP) issues. In a hierarchical block diagram,asetof atomic \nblocks canbe connectedto formadiagram, and this diagram can be then encapsulated into a macro (i.e., \ncomposite) block. The macro block can itself be connected with other blocksand further encapsulated.Anexampleofahierarchical \nblock diagram is shown in Figure 1. In such a context, modular code generation becomes a criti\u00adcal issue. \nBy modular we mean two things. First, code for a macro block should be generated independently from context,that \nis, with\u00adout knowing where (in which diagrams) this block is going to be used. Second, the macro block \nshould have minimal knowledge about its sub-blocks. Ideally, sub-blocks should be seen as black to lists, \nrequires prior speci.c permission and/or a fee. POPL 09, January 18 24, 2009, Savannah, Georgia, USA. \n1www.mathworks.com/products/simulink/ Copyright &#38;#169; 2009ACM 978-1-60558-379-2/09/01... $5.00 2www.esterel-technologies.com/products/scade-suite/ \n c Figure 1. A hierarchical block diagram consisting of a macro block P with three sub-blocks A, B, \nC. boxes supplied with some interface information. The second re\u00adquirement is very important for IP issues \nas mentioned above. Current code generation practice for synchronous block dia\u00adgrams is not modular: \ntypically the diagram is .attened, that is, hierarchyis removed and only atomic blocks are left. Thena \nde\u00adpendency analysis is performed to check for dependency cycles within a synchronous instant: if there \nare none, static code can be generatedbyexecuting blocksin anyorder that respects the depen\u00addencies. \nClearly, .attening destroys modularity and results in IP issues. It also impacts performance since the \nalgorithms compute on the entire .at diagram which can be very large. Moreover, the hierarchical structure \nof the diagram is not preserved in the code, which makes the code dif.cult to read and modify. Whyis \nit dif.cult to avoid .attening and have a truly modular code generation method? Let us illustrate the \nproblem with an ex\u00adample. Consider the macro block P shown in Figure 1. An straight\u00adforward way to generate \ncode for P is to generate a monolithic piece of code, in the form of a step function, as follows: P.step(x1, \nx2) returns (y1, y2) { (z1,z2) := A.step(x1); y1 := B.step(z1); y2 := C.step(z2, x2); return (y1, y2); \n} P.step() calls the step functions of the sub-blocks of P , to com\u00adpute the outputs y1,y2 of P , from \nthe inputs x1,x2. Now, suppose P is used as shown to the left of Figure 2: that is, we connect its output \ny1 to itsinput x2. Then we .nd that the functionP.step() generated above cannot be used: indeed, in order \nto call this func\u00adtion we need both x1 and x2,but x2 = y1, which is an output of the function!In otherwords,wehaveacyclic \ndependencybetween inputs and outputs. This dependency, however, is a false depen\u00addency: it is a result \nof the above code generation method, and not a property of the diagram. Indeed, .attening P as shown \nto the rightof Figure2 reveals this: there are nocyclic dependenciesin this .attened diagram, and we \ncan compute the output y2 from the input x1 by calling the step functions of blocks A, B, and C, in that \norder. Figure 2. Using block P of Figure 1. In (Lublinerman andTripakis 2008b) wehave solved this prob\u00adlem, \nby proposing a general framework for modular code gener\u00adation. This framework relies on the concept of \na pro.le for each block.The pro.le includesasetof interface functions thatevaluate different outputs \nof the block and may also update the state of the block (if the block has state). The fundamental difference \nwith the monolithic method described above is that the pro.le generally in\u00adcludes not one,but severalinterface \nfunctions.For instance,in the case of macro block P of Figure 1, we can generate the following two interface \nfunctions: P.get1(x1) returns y1 { P.get2(x2) returns y2 { (z1,z2) := A.step(x1); y2 := C.step(z2,x2); \ny1 := B.step(z1); return y2; return y1; } } The pro.le also includes a pro.le dependency graph that \nspeci.es constraints on the order in which these functions need to be called. For the above example, \nthis graph states that P.get1() must be executed before P.get2() (thus making the second output of A \navailable to C via internal variable z2). Our framework allows to quantify modularity of the generated \ncode, in terms of the number of interface functions that are pro\u00adduced: the fewer these functions, the \nmore modular the code is. Our framework also allows to explore the trade-offbetween mod\u00adularity and reusability \n(the ability to embed generated code in any context). As the above example illustrates, increasing modularity \nsometimes means sacri.cing reusability. Our notion of modularity has similarities but also differences \nfrom what is typically understood as modularity in programming. Itis similarinthe sensethatit capturesthe \nprincipleof abstractinga component by hiding all information, except what is necessary for the users \nof the component: block pro.les aim at capturing exactly this necessary information. On the other hand, \ncontrasting mod\u00adularity to reusability may appear surprising. It follows, however, from our choice to \nquantify modularity as inversely proportional to pro.le size. This choice is not as arbitrary as it may \nseem. Indeed, minimizing the size of pro.les is important for scalability. In a hi\u00aderarchical notation \nsuch as ours, the complexity of algorithms ap\u00adplied to a given block is a direct function of the size \nof the pro.les of its sub-blocks. Therefore, minimizing the number of interface functions is crucial \nin managing complexity as we move up in the hierarchyof blocks. Inourframework, adjustingthedegreeof \nmodularityisdoneby choosing a certain clustering algorithm. This clustering algorithm operates on the \nso-called scheduling dependency graph (SDG) of a given macro block P . The SDG of P consists of all interface \nfunc\u00adtions of sub-blocks of P connected together according to the inter\u00adnal diagram of P . Clustering \nsplits the SDG into a number of sub\u00adgraphs,andeach sub-graphgivesrisetoan interface functionfor P . Therefore, \nchoosing different clustering methods results in explor\u00ading different trade-offs. In (Lublinerman andTripakis \n2008b) we presented two clustering methods, the so-called dynamic method that achieves maximum reusability \nand is optimal with respect to modularity (that is, it generates the minimal number of interface functions \nneeded to achievemaximal reusability), and the so-called step-get method that generates at most twointerface \nfunctions, at the expense of reusability. The .rst contribution of this paper is the exploration of another \ntrade-off: between modularity and code size.We .rst show that the dynamic method may result in larger \ncode sizes. This is because the dynamic clustering algorithm allows sub-graphs to overlap, which in turn \nresults in code replication across interface functions. This leads us to study the problem of achieving \nmaximal reusability without code replication, that is, non-overlapping clustering tech\u00adniques. Our main \ntheoretical result is that optimal disjoint cluster\u00ading, that is, clustering into a minimal number of \nnon-overlapping sub-graphs without introducingfalse input-output dependencies,is an intractable problem \n(NP-complete). We also show that disjoint clustering generally results in loss of modularity, that is, \nan increase in the number of interface methods that need to be generated in order to achieve maximal \nreusability. Finally, we show how the optimal disjoint clustering problem can be solved usingaSATsolver.We \nproposea reductionof checking whethera disjoint clusteringexists withagiven numberof clusters K, to checking \nwhether a given propositional formula is satis.\u00adable. Then, optimal disjoint clustering can be solved \nby iteratively increasing K. The second major contribution of this work is a prototype im\u00adplementation \nof our framework and a set of experimental results. This implementation is new and includes the optimal \ndisjoint clus\u00adtering method proposed in this paper as well as the dynamic and step-get methods proposed \nin our previous work. Using this imple\u00admentation we experiment on a set of real Simulink examples com\u00ading \nfrom the Simulink demo suite and from industrial partners from the automotive domain.We .nd out that \nmanyof the problems that our modular code generation framework attempts to solve are not just theoretical \nproblems, but also arise in practice. In particular, we .nd examples that justify the interest in all \nthree clustering and code generation methods mentioned above. The rest of this paper is organized as \nfollows. In Section 2 we discuss relatedwork.In Sections3and4 we present the syn\u00adchronous block diagram \nnotation and our modular code generation framework:thisisasummaryofourpreviouswork.In Section5we illustrate \nthe trade-off between modularity and code size. In Sec\u00adtion6 we study the optimal disjoint clustering \nproblem and show thatitis NP-complete.In Section7weshowhowtosolvethisprob\u00adlem usingaSATsolver.In Section8we \nreport on our implementa\u00adtion andexperiments. Section9concludes this paper. 2. Related work There exists \na large body of work on automatic code generation for languages with synchronous semantics, e.g., see \n(Benveniste et al. 2003). Modular code generation, however, has been not as extensively studied. (Benveniste \net al. 2003) stated for this problem that a uni.ed treatment remains a research topic and this claim \nlargely remains true today. The need to depart from monolithic code generation into multi\u00adple functions \nfor each module or block has been acknowledged in previous works (Raymond 1988; Benveniste et al. 1997; \nHainque et al. 1999), mainly in the context of distribution of synchronous programs. These works, however, \nprovide incomplete solutions. In particular, they omit the concept of pro.le dependency graph, which \nis essential to reduce the number of interface functions by expressing dependencies among them. Also, \noptimality and maxi\u00admal reusability are not addressedin theseworks. Finally, clustering with overlapping, \nwhich is fundamental to achieve optimality, is not discussedinanyof theseworks.For instance, (Hainqueetal. \n1999) start from a very .ne-grain interface where every atomic op\u00aderator is mapped to an interface function, \nand then use methods to reduce the number of functions by clustering operators together. This approach \ngeneratesalarger numberof interface functionsthan our dynamic approach. Industrial tools such as Simulink \nor SCADE offer limited mod\u00adular code generation capabilities. In SCADE the main problem with modular \ncode generation, namely cyclic dependencies, is avoided by requiring that every feedback loop be broken \nby a unit-delay block at every level of the hierarchy. This is a major modeling restriction, as most \ndiagrams in practice exhibit feedback loops with no such delays at higher levels of the hierarchy. The \nsame restriction is imposed in (Biernacki et al. 2008). Indeed, a single step function is generated per \nblock in these works, but this is not enough to guarantee maximal reusability as explained above. Real-Time \nWorkshop Embedded Coder, a code generator for Simulink by The MathWorks, provides a feature called Function \nwith separate data ,but does not seem to generate multiple inter\u00adface functions per block, neither a \ndependency interface, both of which are essential as mentioned above. In (Mosterman and Ciol. 2004) a \nsolution reminiscent of our step-get method is presented and reported to be implemented in Simulink. \nThis solution generates only two interface functions per block (an output and an update function), thus \ncannot achieve maximal reusability. Indeed, maximal reusability requires in gen\u00aderal as many as n +1 \nfunctions, where n is the number of outputs ofa block (Lublinerman andTripakis 2008b). Less relatedaretheworks(MaffeisandLe \nGuernic1994;Aubry et al. 1996) which consider the problem of distribution of syn\u00adchronous programs. Distribution \ndoes not necessarily imply that the generated code is modular: for instance, one may look at the entire \nprogram (e.g., .atten it) in order to distribute it, which is the approach taken in the works above. \nTrade-offs between modularity and reusability or modularity andcodesizearenot discussedinanyofthe aforementionedworks. \nTo our knowledge, this paper is the .rst to study the problem of minimizing code size in the context \nof modular code generation for block diagrams. In this paper we use a simple, purely synchronous model \nof block diagrams, where all blocks run at the same rate. This is done for reasons of simplicity. In \n(Lublinerman andTripakis 2008a) we haveshownhowtoextendourmodularcode generationframework to triggered \nand timed block diagrams: these features allow one to describe multi-rate models. The optimal disjoint \nclustering method proposed in this paper can be readily used in triggered and timed block diagrams as \nwell. In this paper we consider diagrams that, if .attened, are acyclic. Cyclic diagrams can be handled \nusing the approach of (Malik 1994; Shiple et al. 1996), which is to check at compile-time that, despite \ncyclic dependencies, the diagram still has well-de.ned semantics. This, however, requires semantic knowledge \nabout each block, namely, what is the function that each block computes. Having such semantic knowledge \nis contrary to the requirement of treating blocks as black-boxes due to IP reasons. It is also possible \nto avoid such compile-time checks and rely on computing a .xpoint at run-time (Edwards and Lee July 2003; \nLee and Zheng 2007). However, this .xpoint may contain unde.nedvalues, which means such code cannot be \nused in safety-critical applications. 3. Synchronous block diagrams In this section we describe our notation \nwhich is a simple form of hierarchical block diagrams, and its semantics, which is syn\u00adchronous. The \nnotation: Weconsideranotationbasedonasetofblocks that can be connected to form diagrams (see Figure 1). \nBlocks are either atomic or macro (i.e. composite).Eachblockhasanumberofinput ports (possibly zero) and \na number of output ports (possibly zero). Diagrams are formedby connectingthe outputportofa block A to \nthe input port of a block B (B can be the same as A). An output port can be connected to more than one \ninput ports. However, an input port can only be connected to a single output. Amacro block encapsulates \na block diagram into a block. The blocks forming the diagram are called the internal blocks of the macro \nblock, or its sub-blocks. The connections between blocks ( wires ) are called signals. Upon encapsulation: \neach input port of the macro block is connected to one or more inputs of its internal blocks, or to an \noutput port of the macro block; and each output port of the macro block is connected to exactly one port, \neither an output port of an internal block, or an input of the macro block. Combinational, sequential, \nMoore-sequential blocks: Every atomic block A is pre-classi.ed as either combinational (state\u00adless) or \nsequential (having internal state). Some sequential blocks are Moore-sequential. Each output of a Moore-sequential \nblock only depends on the current state, but not on the current inputs. For example aunit-delay block \nthat stores the input and provides it as output in the next synchronous instant is a Moore-sequential \nblock. On the other hand, a block implementing an equation of the form yk = axk + bxk-1, where y is the \noutput stream, x is the input stream, a, b are constants and k is the index of the current synchronous \ninstant, is a non-Moore sequential block. The above de.nitions extend to macro blocks in a natural way. \nFlattening: Adiagram is.at ifit containsonly atomic blocks.A .attening procedure can be used to transform \na hierarchical block diagram into a .at one: (1) We start with the top-level diagram (whichmay consistofa \nsingle macro block).(2)Wepicka macro block A and we replace it by its internal diagram. While doing so, \nwe re-institute anyconnections that would be lost: If an input port p of A is connected externally to \nan output port q and internallyto an input port r, then we connectq to r directly. Similarly for output \nports of A. (3) If there are no more macro blocks left, we are done, otherwise, we repeat step (2). Block-based \ndependency analysis and acyclic diagrams: This type of dependencyanalysis is used only for the purpose \nof giving semantics to a diagram. We assume the diagram is .at. We con\u00adstruct a block-based dependency \ngraph. The nodes of this graph are all the blocks in the diagram. Notice that since the diagram is .at, \nall these blocks are atomic. For each block A that is not Moore-sequential, for each block B with some \ninput connected to an output of A, we add a directed edge from A to B.We say that a diagram is acyclic \nif, once we .attenit andbuild its block-based dependencygraph, we .nd that this graph has no cycles. \nSemantics: We assign semantics only to .at, acyclic diagrams. The semantics we use are standard synchronous \nsemantics used also in languages like Lustre. Each signal x of the diagram is inter\u00adpreted as a total \nfunction x : N . Vx, where N = {0, 1, 2, 3, ...}and Vx is a set of values: x(k) denotes the value of \nsignal x at time instant k. If x is an input this value is determined by the en\u00advironment, otherwise \nit is determined by the (unique) block that produces x. Sincethe diagramisacyclic thereexistsa well-de.ned \norder of .ring the blocks to compute the values of all signals at a given synchronous instant. 4. Modular \ncode generation In (Lublinerman andTripakis 2008b) wehave proposeda modular code generation scheme for \nthe diagrams described in the previous section. Here, we present a summary of this scheme and show the \ntrade-offs that can be explored with it. Modular code generation inputs and outputs: Our code gen\u00aderation \nscheme takes as inputs: 1. a macro block P with its internal block diagram; and 2. the pro.le of each \nsub-block of P (explained below).  It generates as outputs: 1. a pro.le for P . 2. the implementation(inacertain \nprogramming languagesuchas C++, Java, etc.) of each of the interface functions listed in the pro.le of \nP .  Pro.les: The pro.le of a block is essentially its interface. Both atomic and macro blocks have \npro.les. The pro.le of a block A contains: a list of interface functions and their signatures; and a \npro.le dependency graph (PDG) that describes the correct order in which these functions aretobe calledatevery \nsynchronous instant. The nodes of the PDG of A are the interface functions of A. Forexample, Figure3shows \nthe pro.lesof sub-blocksA, U, C of macro block P . Blocks A and C havea single interface function called \nstep() which takes the input and returns the output of these blocks. Block U has two interface functions: \nU.step() and U.get(). U.get() takes no input and returns the output of U. U.step() takes the input of \nU and returnsnooutput.Thisisacasewhere U isaMoore\u00adsequential block: its get() method returns the outputs \nand its step() method updates the state, given the inputs. The PDG of U shown in the .gure states that \nU.get() must be called before U.step(), at every synchronous instant. Code generation steps: Our modular \ncode generation scheme contains two major steps, explained brie.y and illustrated through the example \nof Figure 3: (1) Dependency analysis: this step determines whether there exists a valid execution order \nof the interface functions of the sub\u00adblocks of P .It consistsinbuildinga scheduling dependency graph3 \n(SDG) for the given macro block P and then checking that this graph does not contain any cycles. The \nSDG for P is built by connecting the PDGs of all sub-blocks of P .Forexample, the SDG for the block P \nshowninFigure3isshowninthe bottomleftofthe .gure. If the SDG contains a cycle then P is rejected: this \nmeans that modular code generationfails and P needs to be .attened (one or more levels). Otherwise, we \nproceed to the code generation step. (2) Pro.legeneration: this step involves several sub-steps, the \nmost important of which is clustering the SDG into a number of sub-graphs. How exactly to cluster the \nSDG is at the heart of our modular code generation scheme: choosing different clustering methods results \nin different specializations of the scheme, with different trade-offs as discussed below. An example \nof clustering is shown in Figure 3. The SDG of P is clustered in two sub\u00adgraphs, one containing functions \nU.step() and C.step(), and the other containing U.get() and A.step().  Once the SDG is clustered into \na set of sub-graphs, for each sub-graph G, we generate one interface function fG for P . Func\u00adtion fG \ncalls the functions included in G in the order speci.ed by G. This is a partial order, and any serialization \nof it into a total order is legal. An init() function is also generated for sequential blocks to initialize \ntheir state. In the example of Figure 3, the SDG of P is clusteredintwo sub-graphs. Eachof these sub-graphsgives \nrisetoan interface function.The left-mostsub-graphgivesrisetoa function calledP.step()andthe right-most \nonetoa function called P.get(). The implementation of these functions is shown below: P.get( ) returns \nP_out { P.step( P_in ) { U_out := U.get(); C_out := C.step(P_in); P_out := A.step(U_out); U.step( C_out \n); return P_out; } } After the generation of the interface functions, all that remains is to synthesize \na PDG for P . This PDG can be derived directly from the clustering: the nodes of the PDG are the interface \nfunctions for P generated above. The edges of the PDG are computed as follows. If sub-graph G1 depends \non sub-graph G2 (that is, there exists a node v1 in G1 and a node v2 in G2 such that v1 depends 3Note \nthat the scheduling dependencygraph (SDG) and the pro.le depen\u00addencygraph (PDG) are two different objects. \n Figure 3. Example of modular code generation. on v2 in the SDG of P )then interface functionfG1 depends \non fG2 . Clustering is done in such a way so as to guarantee that no cyclic dependencies are introduced \nbetween sub-graphs, therefore, the SDG is guaranteed to be acyclic. As an example, the SDG of block P \nof Figure 3 is shown at the bottom-right of the .gure. P.step() depends onP.get(), since U.step() depends \non U.get(). Modularity vs. reusability trade-off: As we mentioned above,by choosing different clustering \nalgorithms, we can explore different trade-offs.A most important trade-off is between modularity and \nreusability. Modularity can be made a quantitative notion in our framework: it can be measured in terms \nof the number of interface functions generated for a given block. The smaller this number, the higher \nthe degree of modularity. In that sense, the most modu\u00adlar code is one that has just one interfacefunction. \nThis de.nition is justi.ed by complexity considerations. The complexity of algo\u00adrithms such as cycle \ndetection or clustering, that our method uses, is a direct function of the sizes of the PDGs of the sub-blocks \nof a given macro block.Inturn,thesizeofthePDGofablockisafunc\u00adtion of the number of interface functions \ngenerated for this block (these function are the nodes of the PDG). Therefore, minimizing the number \nof generating functions is essentially for scalability. The price to pay for modularity is reusability.4 \nIf we generate too few interface functions, we may create additional,false input\u00adoutput dependencies, \nthat were not in the original diagram. These dependencies may resultinfalse dependency cycles when we \nlater embed macro block P inacertain diagram.Anexample illustrating this phenomenon has beengivenin the \nIntroduction (see Figures1 and 2). Another example is given here. Suppose we generate only 4Note that \npro.les and PDGs are essentially abstractions of the internals of a block. As with most abstractions, \ntheycan be more or less re.ned, that is, lose less or more information. The more information is lost, \nthe less usable the abstraction becomes. one, instead of two, interface function for block P of Figure \n3. In terms of clustering, this means we cluster the SDG of P into a single sub-graph that contains all \nnodes.Wewould then generatea single interface function for P , sayP.step(), as follows: P.step( P_in \n) returns P_out { U_out := U.get(); C_out := C.step(P_in); P_out := A.step(U_out); U.step( C_out ); return \nP_out; } The problem is that we have created a false dependency be\u00adtween the output of P and its input: \nthis dependency did not ex\u00adist in the original diagram because U is Moore-sequential. False input-output \ndependencies can be harmful in terms of reusability. For example, if a user ofP attempts to connect the \noutput of P to itsinput,toformafeedbackloop,thentheabovesingle-functionin\u00adterface cannotbe used.Wesaythat \nmaximalreusability is achieved when nofalse input-output dependencies are added during cluster\u00ading. In \n(Lublinerman andTripakis 2008b) wehave proposed the so\u00adcalled dynamic clustering method that achieves \nmaximal reusabil\u00adity witha minimal numberof interface functions.In that sense, this method is optimal. \nMoreover, the dynamic method is guaranteed to generate no more than n +1 interface functions for a block \nwith n outputs (no more than n functions if the block is combinational). In (Lublinerman andTripakis \n2008b) we have also proposed an\u00adother clustering method, called the step-get method, which gener\u00adates \nat most two (and often just one) interface functions per block. The step-get method privileges modularity \nbut obviously cannot guarantee maximal reusability.For sequential blocks, an init() func\u00adtion is added \nto initialize the state. P.get_1(x_1,x_2) returns y_1 { P.get_2(x_2,x_3) returns y_2 { if ( c = 0 ) \n{ if ( c = 0 ) { z_1 := A_1.step( x_2 ); z_1 := A_1.step( x_2 ); z_2 := A_2.step( z_1 ); z_2 := A_2.step( \nz_1 ); ... ... z_{n-1}:=A_{n-1}.step(z_{n-2}); z_{n-1}:=A_{n-1}.step(z_{n-2}); (z_B, z_C):=A_n.step(z_{n-1}); \n(z_B, z_C):=A_n.step(z_{n-1}); } } c := (c + 1) modulo 2; c := (c + 1) modulo 2; y_1 := B.step( x_1, \nz_B ); y_2 := C.step( z_C, x_3 ); return y_1; return y_2; } }  Figure 4. Example illustrating the trade-off \nbetween modularity and code size. Using the dynamic method, the SDG is clustered in two sub\u00adgraphs, as \nshown in part (c) of Figure 4. Notice that this is optimal. Indeed, clustering all nodes into a single \nsub-graph would create false dependencies from input x1 to output y2 and from input x3 to output y1, \nand that would result in loss of reusability. Also note that the dynamic clustering method may result \nin sub-graphs that overlap , that is, that share some nodes, as is the case with this example. This is \nessential in order to achieve a minimal number of sub-graphs. 5. Modularity vs. code size In this paper, \nwe explore another trade-off: modularity vs. code size. We illustrate this trade-off through an example, \nshown in Figure4.Part(a)ofthe .gureshowsa macro block P with n +2 sub-blocks, named A1, ..., An, B, C.Wesuppose \nthat each of these blocks has a single interface function step(), abbreviated by s. So A1.s stands for \nA1.step(), etc. The PDG of each of the sub-blocks then clearly consists of a single node. By connecting \nthe PDGs of the sub-blocks we form the SDG for P , as shown in part (b) of Figure 4. Figure 5. Code generated \nwith the dynamic method for the exam\u00adple of Figure 3. P.get_1(x_1) returns y_1 { P.internal( x_2 ) { \ny_1 := B.step(x_1, z_B); z_1 := A_1.step(x_2); return y_1; z_2 := A_2.step(z_1); } ... z_{n-1}:=A_{n-1}.step(z_{n-2}); \nP.get_2(x_3) returns y_2 { (z_B, z_C):=A_n.step(z_{n-1}); y_2 := C.step(z_C, x_3); } return y_2; } Figure \n6. Code generated with the optimal disjoint clustering method (this paper) for the example of Figure \n3. The code that is generated from this clustering is shown in Figure5.Two interface functions are generated \nfor P , one for each sub-graph.Thebodiesofthetwofunctionsare identicaluptotheir last two statements: \nthis is because both functions may need to evaluate the chain of blocks A1 to An. Internal persistent \nvariables z1, z2, and so on, hold the values of internal signals between blocks A1 and A2, A2 and A3, \nand so on.Variable c is a modulo-2 counter, initialized to zero, that serves to .ag whether sub-blocks \nA1 to An have or have not been .red in the current synchronous instant: if c=0 these blocks have not \nbeen .red; if c=1 theyhave. All these are automatically generated using the dynamic method: we refer \nthe reader to (Lublinerman and Tripakis 2008b) for the details. TheproblemwiththecodeofFigure5isthatit \ncontainsalotof replication: the .rst n +3 linesof the bodiesof functionsP.get 1() andP.get 2() are identical.In \ntheworst case, this phenomenon can result in code that is replicated m times, where m is the number of \noutputs of the block. This clearly results in code size blow-up that may be unacceptable for embedded \napplications where small footprint is often crucial. We can eliminate this replication phenomenon, by \ngiving up some modularity: instead of two sub-graphs, we can cluster the graph in three sub-graphs, as \nshown in part (d) of Figure 4. This clustering produces non-overlapping sub-graphs, thus resulting into \ninterface functions that share no code. The code generated from this alternative clustering is shown \nin Figure 6. Note that the code ofFigure6isnotonly smallerthanthecodeofFigure5,itisalso more ef.cient. \nIndeed, it avoids the use of counter c, which results in savings of memory, as well as time (to test \nthe condition c=0). The PDG for P generated for the clustering in three sub-graphs is shown in part (e) \nof Figure 4. Note that clustering (d) does not result in anyloss of reusability. It does not create any \nfalse input-output dependencies, and thus achieves maximal reusability, as does clustering (c). The different \nbetween the twoclusterings is that (c) privileges modularity instead of code size, while clustering (d) \nopts for code size at the expense of some modularity. In the rest of the paper we explore the idea of \noptimizing code size in depth. It is worth pointing out that the motivating example of Figure4isnotjustan \nacademicexamplebut actually occursin practice: see Figure 10 and related discussion in Section 8. 6. \nOptimal disjoint clustering From the discussion in the previous section, it becomes clear that the trade-offbetween \nmodularity and code size can be explored by tuning the clustering algorithm to use more or less overlapping. \nThe dynamic clustering method proposed in (Lublinerman and Tripakis 2008b) uses as much overlapping as \nnecessary in order to optimize modularity while maintaining maximal reusability. On the other hand, if \nwe want code of minimal size then we should use a clustering that results in disjoint (i.e., non-overlapping) \nsub-graphs. In this section weexplore this option.We show that, unfortunately, optimizing modularity \nwhile maintaining maximal reusability and disjoint sub-graphs is an intractable problem (NP-hard). WeformalizeaScheduling \nDependencyGraph (SDG) asa.nite directed acyclic graph G =(V, .), where V is the set of nodes and .. V \n\u00d7 V is the set of edges. Both sets are assumed to be '' non-empty. If (v, v) .. we write v . v. V is \npartitioned in three disjoint non-empty sets V = Vin . Vout . Vint (1) Vin contains the input nodes that \nhave no incoming edges: .v . Vin, .u . V : u . v. Vout contains the output nodes that have no outgoing \nedges: .v . Vout, .u . V : v . u.We also assume that output nodeshavea unique writer , thatis, for any \nv . Vout, the set {u . V | u . v} is a singleton (i.e., contains a single element). Vint containsthe \ninternal nodes.We assume that thereis no direct edge between an input node and an output node (if there \nis, we can add a new, dummy , internal node in-between). Notice that some internal nodes may have no \nincoming or no outgoing edges. This is necessary in order to model, for instance, SDGs like the one shown \nin Figure 3. In that example, there are four internal nodes, plus the input nodeP in and the output nodeP \nout. De.nition1 (Clustering). Let G =(V, .) bea directedgraph.A clustering of G is de.ned by a partition \nof V into k disjoint sets of nodes V = {V1, ..., Vk} suchthat .ki=1Vi = V . Each Vi is calleda cluster.We \nlift the relation . from nodes to clusters as follows: ' Vi .V Vj iffthere exist v . Vi and v' . Vj suchthat \nv . v. Given v . V , we denote by [v] the (unique) cluster Vi .V such that v . Vi.We denoteby . * (resp., \n. * V )the transitive closure of . (resp., .V ).We say that the clustering de.nedby V is valid if the \nfollowing conditions hold: 1. For any v . Vin . Vout, [v] is a singleton, that is, input and output nodes \nare clustered independently. '' 2. For any v . Vin and v' . Vout, if [v] . * [v] then v . * v. V That \nis, V does not introduce any input-output dependencies that were not already in G (note that v . * v' \nimplies [v] .V* ' [v] by de.nition). 3. The relation .V on clusters is acyclic (after dropping all self\u00adloops). \nWe say that the clustering de.ned by V is almost valid if Condi\u00adtions1 and2 above hold, while Condition3 \n(acyclicity) may not necessarily hold. The quotient of G w.r.t. V, denoted G/V , is the directed graph \nthat has clusters as nodes and whose edge relation is the relation .V on clusters. Validity captures \nthe fact that a clustering achieves maximal reusability,bynot introducingfalse input-output dependencies. \nThe acyclicity condition ensures that the resulting interface functions do not have cyclic dependencies. \nNotice that there always exists a valid clustering for anySDG: the one where every cluster contains a \nsingle node. Proposition 1. Validity of a clustering can be checked in polyno\u00admial time in the number \nof nodes of the SDG. Proof. Validity Condition1is trivial to check in linear time in the number of nodes. \nCondition 2 can be checked by computing the transitive closure of . on the nodes of the original graph, \nthen the transitive closure of the lifted relation .V on clusters, and then checking that the two relations \nare the same between inputs and outputs.Transitive closure canbe computed usinga simple al\u00adgorithm such \nas the all-pair shortest-path algorithm (Floyd 1962) which runs in O(|V |3) time, or other techniques \nwith better worst\u00adcase complexity, e.g., see (Fischer and Meyer 1971) and (Copper\u00adsmithandWinograd 1990).Theacyclicity \ncondition canbeveri.ed by explicitly computing the quotient graph and then checking for acyclicity usingTarjan \ns algorithm(Tarjan 1972) which runsin lin\u00adear time. The optimal disjoint clustering problem is, given \nan SDG G, .nd a valid clustering of G with minimum number of clusters. The decisionversionof this problemis,given \nG and an integer k,decide whether thereexistsavalid clusteringof G with at most k clusters. We will show \nthat the optimal disjoint clustering problem is NP\u00adhard. Before proving the result, we introduce some \nuseful concepts. Notice that because of validity Condition 1, input and output nodes are clustered in \nisolation in a valid clustering. Therefore, a valid clustering can be de.ned by a partition of Vint instead \nof V .We will be using this alternative de.nition in what follows. De.nition2 (Mergeability). Let G =(V, \n.) be an SDG and let V ' = {v1,...,vk}. Vint = {v1,...,vn} be a subset of internal nodes.Wecall V ' mergeable \nif the partition {V ', {vk+1},..., {vn}}de.nes an almost valid clustering. Two nodes v1,v2 . Vint are \ncalled mergeable, if the subset {v1,v2} is mergeable. The merge\u00adability graph of G, denoted M(G), is \nan undirected graph on the node set Vint, where we add an edge between v1 and v2 iff v1 and v2 are mergeable. \nLemma 1. Let G =(V, .) be an SDG and let V = {V1, ..., Vk}be a partition of Vint de.ning an almost valid \nclustering. Let V' = {V1, ..., Vi',V'', ..., Vk} be a re.nement of V, where Vi is split in i . V'' two \nnon-empty, disjoint subsets Vi'i = Vi. Then V' de.nes an almost valid clustering. Proof. Suppose V' is \nnot almost valid. Then, V' violates validity Condition 2 (Condition 1 holds by de.nition). This means \nthere ' must exist nodes v . Vin and v' . Vout such that [v] . * [v V' ] in G/V' butv . * v' in G. That \nis, there must exist a path ' s =[v] .V' U1 .V' \u00b7\u00b7\u00b7 Ul .V' [v] ' in G/V' . This means in turn that there \nexist nodes uj ,uj . Uj , for j =1, ..., l, such that ' '' v . u1,u 1 . u2, ..., u l . v. We distinguish \nthe following cases: 1) None of Vi',Vi''appear in s, that is, .j =1, ..., l : Uj = V'' Vi'. Uj i = . \nThen s is also a path of G/V , which implies that V is not almost valid, which is a contradiction. 2) \nOnly one of Vi',Vi''appear in s. Suppose, w.l.o.g., that only Vi'appears in s. Then,by replacing Vi'by \nVi we obtaina new path s' which is a path of G/V :this holds becauseVi'. Vi. This again implies that \nV is not almost valid, a contradiction. 3) Both Vi',Vi''appear in s, that is, s has the form: [v] .V' \nU1 .V' \u00b7\u00b7\u00b7 .V' Ul1 .V' Vi'.V' \u00b7\u00b7\u00b7 V '' .V ' i .V ' Ul2 .V ' \u00b7\u00b7\u00b7 .V ' Ul .V ' [v ' ]. Then the path s \n' =[v] .V U1 .V \u00b7\u00b7\u00b7 .V Ul1 .V Vi .V Ul2 .V \u00b7\u00b7\u00b7 .V [v ' ] isa pathof G/V :this holds because bothVi ' \n,V i '' are subsets of Vi. This again implies that V is not almost valid, a contradiction. Lemma 2. Let \nG =(V, .) be an SDG and let V = {V1, ..., Vk}be a partition of Vint de.ning an almost valid clustering. \nThen for all i =1, ..., k, for all v, u . Vi, v and u are mergeable. Proof. By Lemma 1, since V is almost \nvalid, anyre.nement of V intoa.nerpartitionisalsoalmostvalid.Inparticularthe re.nement ({v1}, ..., {v, \nu}, ..., {vn}) that groups only nodes v, u together and leaves all others isolated must be almost valid. \nTherefore, by De.nition 2, v and u are mergeable. An SDG G =(V, .) is called .at if there are no dependencies \nbetween its internal nodes, that is .v, v ' . Vint : v . v ' (2) Notice that in a .at SDG G, anyclustering \nof G that satis.es the .rst two validity conditions also satis.es the acyclicity condition, that is, \nit is a valid clustering. This is because the only edges allowed in a .at SDG are from input nodes to \ninternal nodes and from internal nodes to output nodes. Lemma 3. Let G =(V, .) be a .at SDG and let V \n= {V1, ..., Vk} be a partition of Vint into mergeable cliques, that is, for all i =1, ..., k, for all \nv, u . Vi, v and u are mergeable. Then V de.nes a valid clustering. Proof. Suppose V is notvalid. Then,validity \nCondition2mustbe violated (Condition1holdsbyde.nitionand Condition3cannotbe violatedina.at graph). This \nmeans there mustexist nodes v . Vin and v ' . Vout such that [v] . * [v ' ] (in G/V )butv. * v ' (in \nG). V Since v is an input, [v] . * [v ' ] implies there exists v1 . Vi, for V some i =1, ..., k, such \nthat: (1) v . v1 and (2) Vi . * [v ' ]. (2) V and thefact that G is.at togetherimplythat(3)thereexists \nv2 . Vi such that v2 . v '. (1) and v. * v ' together imply (4) v2 = v1. (1), (4) and v. * v ' together \nimply that v1 and v2 are not mergeable, which contradictsthehypothesis that Vi is a mergeable clique. \nWearenowreadytoproveourmain theoretical result.Weshow thatthe optimal disjoint clustering problemis NP-hardby \nreducing the partition-into-cliques problem to the former problem. Let us .rst recall the problemof partitioninga \ngraph into cliques.We are given an undirected graph G =(V, E),where V is the set of nodes and E is the \nset of edges, and a positive integer K =|V |. We want to decide whether V can be partitioned into k = \nK disjoint sets, V = V1 .\u00b7 \u00b7\u00b7. Vk, such that each Vi is a clique, that is, for any v, u . Vi, (v, u) \n. E. This problem is NP-hard (Gareyand Johnson 1978). Proposition 2. Optimal disjoint clustering is NP-hard. \nProof. Let G =(V, E) be an undirected graph.We constructa .at SDG Gf , using the construction technique \nillustrated in Figure 7. For each nodev of G we create three nodes in Gf :an internal node v . Vint, \nan input node v i . Vin and an output node v o . Vout. o We add inGf the edges v i . v and v . v . For \neach edge(u, v) . E, we create the following six nodes in ' ' '''' Gf : eu,ev . Vint, eu,e v . Vin, eu,e \nv . Vout.We also insert ' ''' '' in Gf the edges eu . eu, eu . eu, ev . ev, ev . ev . Additionally, we \ncreate the following four edges: u i . eu, eu . oo v , v i . ev, ev . u . Gf can be constructed in linear \ntime in the size of G. Let us study M(Gf ). First, note that a node eu produced by edge e of G is not \nmergeable with any other internal node. For instance, eu is not mergeable with u, since this would create \nthe ' . * o; dependency eu ueu is not mergeable with ev, since this ' . * '' would create the dependency \neu ev ;eu is not mergeable with v, since this would create the dependency v i . * e ''. Now, let us u \nfocus on two nodes u and v of G. Suppose, .rst, that (u, v) . E. Then u and v are not mergeable in Gf \n, since merging them would i oi create the dependency u . * v (or v . * u o). Next, suppose that (u, \nv) . E. Then u and v are mergeable in Gf . Indeed, in this case we have the picture shown in Figure 7. \nMerging u and v does o not add new dependencies, since u o already depends on v i and v already depends \non u i . It follows that two nodes u and v of G are mergeable in Gf iff theyare adjacent in G. This, \nand thefact that eu-type nodes are not mergeable with anyother node, implies that M(Gf ) is the disjoint \nunion of G anda setof 2|E| independent nodes eu,ev, ..., that must be clustered in isolation.We claim \nthat G can be partitioned into k cliques iff Gf admits a valid clustering of k +2|E| clusters (we are \nnot counting the clusters for the input and output nodes). Suppose Gf can be clustered into k +2|E| clusters. \nThen the nodes of G (that are internal nodes of Gf )must be clustered intok clusters V1, ..., Vk, since \nthe rest 2|E| clusters are reserved for the eu-type nodes. By Lemma 2, each Vi can only contain nodes \nthat are pairwise mergeable. Therefore, each Vi corresponds to a clique of M(Gf ), thus to a clique of \nG. Conversely, suppose that G can be partitioned into k cliques. Thus, M(Gf ) can be partitioned into \nk +2|E| (mergeable) cliques. Since Gf is .at, by Lemma 3, this partition de.nes a valid clustering for \nGf . Figure 7. The SDG sub-graph representing edge (u, v) of G in the proof of Proposition 2. Corollary \n1. Optimal disjoint clustering is NP-complete. Proof. The problem is NP-hard as shown in Proposition \n2. It is also in NP: non-deterministically choose a clustering, then check whether it is valid. The latter \ncan be done in polynomial time as shown in Proposition 1. 7. Reduction to SAT In this section, we show \nhow the optimal disjoint clustering prob\u00adlem can be solved by reducing it to a propositional satis.ability \nproblem. The latter can be solved in practice today using a SAT _ ^ solver. Before we present our method, \nwe need some technical re- Fk = Xbj sults. 1=j=k b . Vint Let G =(V, .) be an SDG with sets of input \nand output nodes Vin and Vout,respectively.ForasubsetV ' . V of nodes, we de.ne 1 0 BBBBB@ CCCCCA ' ''' \nIn(V )= {v . Vin |.v . V : v . * v } (3) ' ''' . ^^ Xbj .\u00acXb\u00a3 1 = i = k . * (4) Out(V ) = . V : v b \n. Vint {v . Vout |.vv} the set of dependent inputs and outputs of V ', respectively. We 1 = j = k i = \nj write In(v) and Out(v) instead of In({v}) and Out({v}), respec\u00adtively. . ^\u00b4 ` Xbj . Yoj Let V be \na partition of Vint.For any V ' .V, we de.ne b . Vint,o . Vout b . o '' V V ' . * {v . Vin | [v] . * \nV [v]} } (5) InV (V ) = 1 = j = k \u00b4 ` ^ . Xbj . Zij ' (6) OutV (V )= {v . Vout | V If v . InV (V ' ) \nthen we say that cluster V ' depends on input b . Vint,i . Vin node v. If v . OutV (V ' ) then we say \nthat output node v depends i . b on cluster V '. Notice that, by de.nition, In(V ' ) . InV (V ' ) and \n1 = j = k Out(V ' ) . OutV (V ' ). The inverse inclusions do not generally . \u00b4 ` ^ Xb1j . Xb2\u00a3 . (Yo\u00a3 \n. Yoj ) V '' hold, however. Also note that, by de.nition, if V ' .V* then InV (V ' ) . InV (V '' ) and \nOutV (V ' ) . OutV (V '' ). Finally, note b1,b2 . Vint,o . Vout b1 . b2 that for v . Vin and u . Vout, \n[v] . * .V V [u] iffthere exists V ' 1 = j = i = k such that v . InV (V ' ) and u . OutV (V ' ). . \u00b4 \n` ^ Xb1j . Xb2\u00a3 . (Zij . Zi\u00a3) Lemma 4. Let G =(V, .) be an SDG and let V = {V1,V2, ..., Vk}be a partition \nof Vint de.ning an almost valid clustering. Suppose b1,b2 . Vint,i . Vint b1 . b2 that InV (V1)= InV \n(V2) and OutV (V1)= OutV (V2) and let 1 = j = i = k ^ V ' = {V1 . V2,V3, ..., Vk}. V ' de.nes an almost \nvalid clustering. . ' \u00ac (Yoj . Zij ) Proof. V is a partition of internal nodes only, thus the clustering \ni . Vint,o . Vout i .* o 1 = j = k Figure 8. Encoding almost valid clusterings as propositional for\u00ad \nmulas. positive integer.We de.ne the propositional formula Fk as shown it de.nes satis.esvalidity Condition1by \nde.nition. Suppose there ' '' exist v . Vin and v . Vout, such that [v] . * [v ] butv. * v . V ' Then, \nthere must exist V ' ,V '' .V ', such that: (1) [v] .V ' V ' , V '' (2) V ' .V* ' , and (3) V '' .V ' \n[v ' ]. If both V ' = V1 . V2 and V '' = V1 . V2 then the clustering induced by V does not satisfy Condition2,which \ncontradictsthehypothesisthatitisalmostvalid. Thus, V ' = V1 . V2 or V '' = V1 . V2. V '' Case (a): V \n' = V1 . V2. (2) implies that either V1 . * or V V2 . * V V ''.Without loss of generality, we consider \nthe latter case. (1) and the fact that InV (V1)= InV (V2) imply [v] .V V2. (3) 'V V '' ' implies V '' \n.V [v ]. Thus, we have [v] .V V2 . * .V [v ], i.e., [v] . * [v ' ], which contradicts thehypothesis that \nV de.nes V an almost valid clustering. Case (b): V '' = V1 . V2. (2) implies that either V ' . * V1 or \nV V ' . * V V2.Without loss of generality, we consider the latter case. (3) and thefact that OutV (V1)= \nOutV (V2) imply V2 .V [v ' ]. (1) implies [v] .V V '. Thus, we have [v] .V V ' . * V2 .V [v ' ], V i.e., \n[v] . * [v ' ], which again contradicts the hypothesis that V V de.nes an almost valid clustering. Lemma \n5. Let G =(V, .) be an SDG and let V be a partition of Vint de.ning an almost valid clustering. Suppose \nV is optimalw.r.t. the number of clusters, that is, there is no partition V ' that has fewer elements \nthan V and also de.nes an almost valid clustering. Then V de.nes a valid clustering. Proof. V de.nes \nan almostvalid clustering, therefore,validity Con\u00additions1and2arealready satis.ed (De.nition1).Weneedtoprove \nthat Condition 3 (acyclicity) is also satis.ed. Suppose not. Then thereexist V1,V2 .V, i = j,such that \nV1 . *V V1. V V2 and V2 . * This implies that InV (V1)= InV (V2) and OutV (V1)= OutV (V2). But this means \nthat we can merge V1 and V2 and still obtain an almost valid clustering, with fewer elements (Lemma 4). \nThis con\u00adtradicts thehypothesis that V is optimal. We now propose the reduction to SAT. Let G =(V, .) \nbe a Scheduling DependencyGraph as before. Let k . Nbe an arbitrary in Figure 8, where Xbj ,Yoj ,Zij \nare boolean variables denoting that internal node b belongs to cluster j,that output node o depends on \ncluster j,and that clusterj depends on input node i,respectively. Fk encodes the following conditions \n(conjunctions appearing from top to bottom in Figure 8): 1. every cluster must contain at least one internal \nnode, 2. an internal node belongs to exactly only one cluster, 3. if an output o directly depends on \nan internal node b then o also depends on the cluster containing b, 4. if an internal node b directly \ndepends on an input node i then the cluster containing b also depends on input node i, 5. if an internal \nnode b2 directly depends on another internal node b1, then the cluster containing b2 will depend on every \ninput node that the cluster containing b1 depends upon (i.e., b1 . b2 . InV ([b1]) . InV ([b2])), 6. \nif an internal node b2 directly depends on another internal node b1 then every output that depends on \nthe cluster containing b2 will also depend on the cluster containing b1 (i.e., b1 . b2 . OutV ([b1]) \n. OutV ([b2])), and, 7. if an output o does not depend on an input i then it is not the case that a \ncluster j depends on i and o depends on cluster j.  Lemma 6. Let G =(V, .) be an SDG and k . N suchthat \nFk is satis.able. Let f bea satisfying assignment and V the clustering induced by V ' = {V1, \u00b7\u00b7\u00b7 ,Vk} \nwhere for b . Vint, b . Vj if and only if Xbj is true it f. Then: 1. .1 = j = k..o . Vout.Vj . * [o] \nimplies that Yoj is true in V f, V Vj implies that Zij is true in f. 2. .1 = j = k..i . Vin.[i] . * Proof. \nFollowsby inductiononthe lengthofthepathonthe depen\u00addence relation .V . Lemma 7. Let G =(V, .) be an \nSDG and k . N. Fk is satis.able if and only if there exists an almost valid clustering de.nedby V ', \na partition of Vint, suchthat |V ' | = k. Proof. (.)LetV ' beapartition de.ning an almostvalid clustering \nV such that |V ' | = k and let b . V and .1 = j = k.Vj .V ' . Set each boolean variable Xbj to true if \nb . Vj and to false otherwise. Note that the .rst two conjunction terms in Fk hold for any proper partition \nof size k. Set each variable Yoj to true if o . OutV (Vj ) and tofalse otherwise. Set eachvariable Zij \nto true if i . InV (Vj ) and tofalse otherwise. Observe now that the third to sixth conjunction terms \nin Fk encode the followingfacts: In(Vj ) . InV (Vj ), Out(Vj ) . OutV (Vj ), and .j, ., if Vj . * V V\u00a3 \nthen InV (Vj ) . InV (V\u00a3) and OutV (Vj ) . OutV (V\u00a3). Finally, the last term encodes the fact that .i \n. Vin,o . Vout,i . * o . .j : i . InV (Vj ) . o . OutV (Vj ). Thus the above is a satisfying assignment \nfor Fk. (.) Consider a satisfying assignment f to Fk and let V ' = {V1, \u00b7\u00b7\u00b7 ,Vk} where b . Vj if and \nonly if Xbj is true in f. The .rst two terms in Fk impose V ' to be a partition of size k. Let V = V \n' . Vint . Vout as before and note that V satis.es valid\u00adity Condition1. Suppose thatvalidity Condition2does \nnot hold. Hence .i . Vin,o . Vout such that i . * o and [i] . *V [o]. Moreover there exists j such that \n[i] . *V Vj and Vj . *V [o]. By Lemma 6 we have that Yoj and Zij are true in f but then f would not be \na satisfying assignment of Fk since this assignment falsi.es the last term in the conjunction. Thus, \nwe can solvethe optimal disjoint clustering problem using the following procedure: 1. initialize k := \n1; 2. build formula Fk; 3. check whether Fk is satis.able; 4. if Fk is satis.able, terminate and announce \nthat the optimal disjoint clustering has k clusters; the clustering can be directly obtained by the Xbj \nvariables: node b is in cluster j iffin the solution of Fk, Xbj is true; 5. otherwise increment k := \nk +1 and go to step 2.  The procedure is guaranteed to terminate since there always exists a valid clustering \nwith at most as manynodes as the nodes of the SDG. By Lemma 7, the procedure is guaranteed to produce \nan almostvalid clustering, whichis alsoof minimal size.ByLemma5, this almost valid clustering is also \nvalid. 8. Tool and experiments We have implemented the code generation techniques proposed in (LublinermanandTripakis \n2008b)andin this paperina proto\u00adtype tool written in Java. The tool takes as input a Simulink model (.mdl \n.le)aswellaspro.lesofblocks(inourownASCII format). The tool generates as output pro.les for the macro \nblocks in the Simulink model as well as Java code that implements the interface functionsforeachofthe \nmacro blocks.Thetoolalso generatesvar\u00adious statistics such as those that are presented below. Three clustering \nand corresponding code generation methods are currently implemented in the tool: the step-get and the \ndynamic methods proposedin (LublinermanandTripakis 2008b)andtheop\u00adtimaldisjoint clusteringmethodproposedinthispaper.Tosolvethe \nlatter problem, the tool employs the SAT-based method described in Section 7. In our implementation we \nusedthe SAT4J SATsolver (see http://www.sat4j.org). We have experimented using our tool on a set of Simulink \nex\u00adamples. Some of these examples are described in The Mathworks web-site for Simulink under Demos and \nthey are available with the Simulink tool.Two otherexamples are real-world models pro\u00advidedby industrial \npartners from the automotive domain.For con\u00ad.dentiality reasons we cannot reveal the source neither the \nnature of theseexamples.We will referto themas X1 and X2 inthe rest of this section. Our experiments \nhad multiple objectives. First, we wanted to validate the need for multiple interface functions per block, \nwhich isthe fundamentalhypothesis justifyingthisandourpreviouswork. Indeed, we found that this need arises \nin practice and it is not just a theoretical problem. In most of the examples we tried, there are feedback \nloops between macro blocks, at different levels of the hierarchy. One such example is an engine timing \nmodel coming from the Simulink demo suite. The top-level diagram for this ex\u00adampleisshownin Figure9 (picture \ncopyright The Mathworks).5 The top-level diagram contains a number of macro blocks: Throt\u00adtle&#38; Manifold \n, Compression and so on. Notice that there are multiple feedback loops between these blocks. However, \nwhen the hierarchy is .attened these loops are broken by blocks such as unit-delays.Amonolithic code \ngeneration method that generatesa single interface function per blockwouldfail on all theseexamples, \nbecause of these feedback loops. Thus, multiple interface functions per block are needed in practice \nas well as in theory. As a second objective for the experiments, we wanted to com\u00adpare the three clustering \nand code generation methods implemented in the tool. Finally, we wanted to see how the SAT-based method \nperforms in practice in terms of execution time. The results of our experiments are summarized in Table \n1. Although the optimal disjoint clustering problem and our SAT\u00adbased method have exponential worst-case \ncomplexity, in every example we have tried, execution took only a few seconds on a standard laptop computer \n(IBM T43p with a 2.26 GHz Intel Pentium processor and2 GB of RAM runningWindows XP and Java 6). Notice \nthat this is the time it takes to process and generate code for all blocks of each example. The greatest \nSAT problem submitted to the SATsolver among all the examples that we tried was a formula with 851 boolean \nvariables and 773 clauses. This formula was produced for the top-level block of example X2 when checking \nwhethera disjoint clustering with4clustersexists. Notice that modern SAT solvers can easily handle problems \nof much greater size. Table1shouldbe readas follows.Everyrow containsthe infor\u00admationfor oneexample.Inthe \ncolumns,we list:the nameoftheex\u00adample;the total numberof blocksintheexample (including atomic and macro \nblocks); the number of macro blocks; the number of blocks that are of type Combinational (C), Non-Moore-Sequential \n(NS), and Moore-Sequential (MS); the maximum number of out\u00adput ports that some block from the entire \nmodel has; the maximum number of sub-blocks that some (macro) block has; and the results we obtainedby \nrunning the three code generation methods on these examples. The three methods are: step-get (S-G), dynamic \n(Dyn) and op\u00adtimal disjoint clustering (ODC).For each method, and eachexam\u00adple, we report the total number \nof interface functions generated for 5This model correspondstothe Engine1 rowinTable1.  Figure 9. An \nengine timing model in Simulink model name ABS Autotrans Climate Engine1 Engine2 Power window X1 X2 total \n27 42 65 55 73 75 82 112 no. blocks macro C,NS,MS 3 1,0,2 9 4,0,5 10 4,0,6 11 2,1,8 13 3,2,8 14 6,2,6 \n16 2,5,9 16 7,9,0 max no. outputs 1 2 4 2 2 3 3 5 max no. sub-blocks 13 11 29 12 13 11 14 14 total no. \nintf. func. S-G Dyn ODC 4 4 4 fails 13 14 12 14 14 18 18 18 20 20 20 20 21 21 19 19 19 22 24 24 total \ncode size (ELOC) S-G Dyn ODC max red. 57 57 57 fails 108 101 14:6 144 165 144 42:26 132 140 132 19:11 \n180 188 180 19:11 180 199 183 32:16 182 182 182 245 342 261 108:27 Table 1. Experimental results all \nthe macro blocks in the example, as well as the total code size generated for these functions, measured \nin effective lines of Java code (ELOC). Effective means that we do not count comments, Java class constructors, \nand other similar overhead lines of code (these canbe different when usinga different language).We also \ndo not count init() functions and the corresponding lines of code. The last column labeled max red. corresponds \nto the maximum reduction achieved by ODC and is explained later in the section. By studyingTable1, we \ncan make the following observations: First, it is interesting to see that the step-get method succeeds \nin eliminating the feedback loops and corresponding dependency cy\u00adcles between macro blocks in almost \nall cases: the exception is the Autotrans example, modeling an automotive transmission sys\u00adtem. In this \nexample the step-get method fails, that is, results in a (false) dependency cycle in the SDG of one of \nthe macro blocks. As already mentioned, the step-get method generates at most two interface functions \nper block. More precisely,it generates twointer\u00adface functions for Moore-sequential blocks (thathaveinputs) \nanda single interface function for all other blocks (Lublinerman andTri\u00adpakis 2008b). The explanation \nwhy step-get succeeds on most of theexamples liesin thefact that mostof thecyclesin theexamples are broken \nby Moore-sequential blocks.For these blocks,twoin\u00adterface functions is all that is needed to achieve \nmaximal reusabil\u00adity, and the step-get method provides exactly this. In conclusion, theexamplesshow boththe \ninterestbut alsothe limitationsofthe step-get method, which justi.es the need for more elaborate meth\u00adods, \nin particular Dyn and ODC. The second observation is that ODC indeed achieves code size reduction in \nall cases except two (the ABS and X1 examples). Moreover,inallbutone case(the Autotrans example)this \nreduc\u00adtion comesatnoexpensein termsof modularity:thatis,the number of interface functions remains the \nsame. In the case of Autotrans only one extra interface function is generated. The reason why ODC can \nreduce code size without modularity penalty is because the overlapping that the dynamic method creates \ncan sometimes be eliminated without introducing an additional interface function. This happens when sub-graph \nG1 overlaps with sub-graph G2, and the set of inputs that G1 dependsuponisasubsetofthesetofinputs that \nG2 depends upon.In this case theoverlapping area canbe re\u00admoved from G2,and the dependencyG1 . G2 can \nbe added. This isvalid,since G2 requiresmore inputs than G1,therefore whenever the inputs for G2 are \navailable, G1 can be executed before G2. The above phenomenon occurs in all the examples except Au\u00adtotrans \n, where an additional sub-graph (and corresponding in\u00adterface function) needs to be generated in order \nto preserve the input-output dependencies. This occurs for the Transmission Ra\u00adtio macro block, the internal \ndiagram of which is shown in Fig\u00adure 10. It is interesting to observe that the structure of this diagram \nis identical to the structure of diagram shown in Figure 4(a). Indeed, blocks B and C of the latter .gure \ncorrespond to the two Product blocks of Transmission Ratio and block An corresponds to the Look-UpTable \nblock. This indicates that the motivatingexam\u00adpleof Figure4is not just an academicexamplebut actually \noccurs in practice. One may claim that the reduction that ODC achieves in code size is a small percentage \nof the overall size. This is true in most of theexamples presentedinTable1,but notin allof them:in X2 \na reduction of almost 25% on the total code size is achieved. More\u00adover, in the context of modular code \ngeneration, it is meaningful to measure not only the reduction achieved in the entire set of blocks, \nbut also in individual blocks. Indeed, these blocks are likely to be used not only in the current model, \nbut in other models as well. The last column of Table 1, labeled max red. shows the great\u00adest reduction \nachieved for some macro block in each example: the pair L1 : L2 is shown, where L1 is the ELOC generated \nby Dyn for this block, and L2 is the ELOC generated by ODC. It can be seen that the reduction in code \nsize for individual blocks can be dramatic, e.g., 75% in X2 , 50% in Power window , and so on. This justi.es \nthe practical interest in the ODC method. Finally, it is worth pointing out that the examples presented \nin Table1involve blocks witha relatively small numberof outputs each:from1to5at most. Becauseofthis,andthewaythedynamic \nmethod operates, the potential for overlap that can be removed is small on these examples. We expect \nthat the savings that can be achieved by ODC are greater in blocks with a greater number of outputs, \nbut experiments with larger examples are required to validate this expectation. 9. Conclusions This work \nextends our previous work on modular code generation for synchronous block diagrams.Inpreviousworkwehave \nstudied the trade-offbetween modularity and reusability. In this paper we studied another trade-off, \nbetween modularity and code size. We can reduce code size by avoiding the code replication that one of \nour previous methods introduces. However, this generally comes with a price in terms of modularity. Optimizing \nmodularity while avoiding replication and maintaining maximal reusability is a clus\u00adtering problem in \na directed acyclic graph. We showed that this problemis NP-complete, and thatit canbe solvedbya simple \nenu\u00admeration of the number K of available clusters and calling a SAT solver to check satis.ability of \na propositional formula for each K.We .nally reported on a new tool that implements the methods developed \nin this and our previous work. Experimental results ob\u00adtained by applying the tool on real Simulink models \nare encourag\u00ading and justify the interest in our approach. The experiments seem to suggest that the optimal \nnumber of clusters K is likely to be small in practice. References P. Aubry, P. Le Guernic, and S. Machard. \nSynchronous distribution of Signal programs. In 29th Intl. Conf. Sys. Sciences,pages 656 665. IEEE, 1996. \nA. Benveniste,P. Caspi, S.A. Edwards, N. Halbwachs,P. Le Guernic, and R. de Simone. The synchronous languages \n12 years later. Proc. IEEE, 91(1):64 83, January 2003. A. Benveniste,P. Le Guernic, andP. Aubry. Compositionality \nin data.ow synchronous languages: speci.cation &#38; code generation. Technical Report 3310, Irisa -Inria, \n1997. G. Berry and G. Gonthier. The Esterel synchronous programming language: Design, semantics, implementation. \nScience of Computer Programming, 19(2):87 152, 1992. D. Biernacki, J-L. Colaco, G. Hamon, and M. Pouzet. \nClock-directed modular code generation for synchronous data-.ow languages. In 2008 ACM SIGPLAN-SIGBED \nConference on Languages, Compilers, and Tools for Embedded Systems (LCTES 08).ACM, 2008. P. Caspi, D. \nPilaud, N. Halbwachs, and J. Plaice. Lustre: a declarative language for programming synchronous systems. \nIn 14thACM Symp. POPL, 1987. D. Coppersmith and S. Winograd. Matrix multiplication via arithmetic progressions. \nJ. Symbolic Comput., 9:251 280, 1990. S. Edwards and E. Lee. The semantics and execution of a synchronous \nblock-diagram language. Science of Computer Programming, 48:21 42(22), July 2003. M. Fischer and A. Meyer. \nBoolean matrix multiplication and transitive closure. IEEE 12th Symp. on Switching and Automata Theory, \npages 129 131, 1971. R.W. Floyd. Algorithm 97: Shortest path. Commun.ACM, 5(6):345, 1962. M.R. Garey \nand D.S. Johnson. Computers and Intractability. Freeman, 1978. O. Hainque, L. Pautet, Y. Le Biannic, \nand E. Nassor. Cronos: A Sepa\u00adrate Compilation Toolset for Modular Esterel Applications. In World Congress \non Formal Methods (FM 99), pages 1836 1853. Springer, 1999.  E.A. Lee and H. Zheng. Leveraging synchronous \nlanguage principles for heterogeneous modeling and design of embedded systems. In EM-SOFT 07: Proc. 7thACM&#38; \nIEEE Intl. Conf. on Embedded software, pages 114 123.ACM, 2007. R. Lublinerman andS.Tripakis. Modular \nCode Generation fromTriggered and Timed Block Diagrams. In 14th IEEE Real-Time and Embedded Technology \nand Applications Symposium (RTAS 08). IEEE CS Press, April 2008a. R. Lublinerman and S.Tripakis. Modularity \nvs. Reusability: Code Gener\u00adation from Synchronous Block Diagrams. In Design,Automation, and Testin Europe(DATE \n08).ACM, March 2008b. O. Maffeis and P. Le Guernic. Distributed Implementation of Signal: Scheduling \n&#38; Graph Clustering. In Formal Techniques in Real-Time andFault-Tolerant Systems, pages 547 566. Springer, \n1994. S. Malik. Analysisofcyclic combinational circuits. IEEETrans. Computer-Aided Design, 13(7):950 \n956, 1994. P. Mosterman and J. Ciol.. Interleaved execution to resolve cyclic depen\u00addencies in time-based \nblock diagrams. In 43rdIEEE Conf. on Decision and Control (CDC 04), 2004.  P. Raymond. Compilations\u00b4eede \nprogrammes Lustre. Master s thesis, epar\u00b4IMAG, 1988. In French. T.R. Shiple,G.Berry,andH.Touati. Constructiveanalysisofcyclic \ncircuits. In European Design andTest Conference (EDTC 96). IEEE Computer Society, 1996. R.Tarjan. Depth-.rst \nsearch and linear graph algorithms. SIAMJournal on Computing, 1:146 160, 1972.  \n\t\t\t", "proc_id": "1480881", "abstract": "<p>We study modular, automatic code generation from hierarchical block diagrams with synchronous semantics. Such diagrams are the fundamental model behind widespread tools in the embedded software domain, such as Simulink and SCADE. Code is modular in the sense that it is generated for a given composite block independently from context (i.e., without knowing in which diagrams the block is to be used) and using minimal information about the internals of the block. In previous work, we have shown how modular code can be generated by computing a set of interface functions for each block and a set of dependencies between these functions that is exported along with the interface. We have also introduced a quantified notion of modularity in terms of the number of interface functions generated per block, and showed how to minimize this number, which is essential for scalability. Finally, we have exposed the fundamental trade-off between modularity and reusability (set of diagrams the block can be used in).</p> <p>In this paper we explore another trade-off: modularity vs. code size. We show that our previous technique, although it achieves maximal reusability and is optimal in terms of modularity, may result in code replication and therefore large code sizes, something often unacceptable in an embedded system context. We propose to remedy this by generating code with no replication, and show that this generally results in some loss of modularity. We show that optimizing modularity while maintaining maximal reusability and zero replication is an intractable problem (NP-complete). We also show that this problem can be solved using a simple iterative procedure that checks satisfiability of a sequence of propositional formulas. We report on a new prototype implementation and experimental results. The latter demonstrate the practical interest in our methods.</p>", "authors": [{"name": "Roberto Lublinerman", "author_profile_id": "81317497568", "affiliation": "Pennsylvania State University, University Park, PA, USA", "person_id": "P1300932", "email_address": "", "orcid_id": ""}, {"name": "Christian Szegedy", "author_profile_id": "81328490410", "affiliation": "Cadence Design Systems, Berkeley, CA, USA", "person_id": "P1300933", "email_address": "", "orcid_id": ""}, {"name": "Stavros Tripakis", "author_profile_id": "81100410332", "affiliation": "Cadence Design Systems and CNRS, Berkeley, CA, USA", "person_id": "P1300934", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1480881.1480893", "year": "2009", "article_id": "1480893", "conference": "POPL", "title": "Modular code generation from synchronous block diagrams: modularity vs. code size", "url": "http://dl.acm.org/citation.cfm?id=1480893"}