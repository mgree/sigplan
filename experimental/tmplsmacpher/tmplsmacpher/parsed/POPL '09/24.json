{"article_publication_date": "01-21-2009", "fulltext": "\n Positive Supercompilation for a Higher Order Call-By-Value Language Peter A. Jonsson Johan Nordlander \nLule\u00b0 a UniversityofTechnology {pj, nordland}@csee.ltu.se Abstract Previous deforestation and supercompilation \nalgorithms may in\u00adtroduce accidental termination when applied to call-by-value pro\u00adgrams. This hides \nloopingbugs from the programmer, and changes the behavior of a program depending on whether it is optimized \nor not.We presenta supercompilation algorithm fora higher-order call-by-value language and we prove that \nthe algorithm both termi\u00adnates and preserves termination properties. This algorithm utilizes strictness \ninformation for deciding whether to substitute or not and comparesfavorably with previous call-by-name \ntransformations. Categories and Subject Descriptors D.3.4[Programming Lan\u00adguages]: Processors Compilers, \nOptimization; D.3.2[Program\u00adming Languages]: Language Classi.cations Applicative (func\u00adtional) languages \nGeneral Terms Languages, Theory Keywords supercompilation, deforestation, call-by-value 1. Introduction \nIntermediate lists in functional programs allow the programmer to writeclearand concise programs,butcarryacostatruntimesince \nlist cells need to be both allocated and garbage collected. Both deforestation (Wadler 1990) and supercompilation \n(S\u00f8rensen et al. 1996) are automatic program transformations which remove many of these intermediate \nstructures. In a call-by-value context these transformations are unsound, and might hide looping bugs \nfrom the programmer. Consider the program (.x.y) (3/z). This program could contain a division by zero, \nif the value of z is zero. ApplyingWadler s deforestation algorithm to the program will result in y,which \nis sound under call-by-name or call-by-need. Under call-by-value the division by zero in the original \nprogram has been removed, and hence the meaning of the program has been altered by the transformation. \nRemoving intermediate structuresina call-by-value languageis perhapseven more importantthaninalazy language \nsincethe en\u00adtire intermediate structure has to stay alive during the computation. Permission to make \ndigital or hard copies of all or part of this work for personal or classroom use is granted without fee \nprovided that copies are not made or distributed for pro.t or commercial advantage and that copies bear \nthis notice and the full citation on the .rst page.To copyotherwise, to republish, to post on servers \nor to redistribute to lists, requires prior speci.c permission and/or a fee. POPL 09, January 18 24, \n2009, Savannah, Georgia, USA. Copyright c &#38;#169; 2009ACM 978-1-60558-379-2/09/01... $5.00 Ohori and \nSasano (2007) sawthis need and presentedavery ele\u00adgant algorithm, for call-by-value languages, that removes \ninterme\u00addiate structures. Their algorithm sacri.ces some transformational power for algorithmic simplicity.Weexplorea \ndifferent partof the design space: a more powerful transformation at the cost of some algorithmic complexity. \nWe show how to construct a meaning\u00adpreserving supercompiler for pure call-by-value languages in gen\u00aderal \nand implement it in a compiler for a pure call-by-value lan\u00adguage (Nordlander et al. 2008). This is a \nnecessary .rst step towards supercompiling impure call-by-value languages, of which there are manyreadily \navailable today. Well known examples are OCaml (Leroy 2008), Standard ML (Milner et al. 1997) and F# \n(Syme 2008). Considering that F# is currently being turned into a product it is quite likely that strict \nfunctional languages will be even more popular in the future. One might think that our result should \nbe easily obtainable by modifyingacall-by-name algorithmto simply delay beta-reduction untilevery functionargumenthasbeen \nspecializedtoavalue.How\u00adever, it turns out that this strategy misses even simple opportunities to remove \nintermediate structures. The explanation is that eager specialization of function arguments risks destroying \nfold oppor\u00adtunities that might otherwise appear, something which may even prohibit complexity improvements \nto the resulting program. The novelty of our supercompilation algorithm is that it concen\u00adtrates all \ncall-by-value dependencies to a single rule that relies on the resultfromaseparate strictness analysisfor \ncorrectbehavior.In effect, our algorithm delays transformation of function arguments past inlining, much \nlikeacall-by-name scheme does, although only asfar as allowedby call-by-value semantics. The resultis \nan algo\u00adrithm that is able to improve a wide range of illustrative examples liketheexisting algorithmsdo,but \nwithoutthe riskof introducing arti.cial termination. The speci.c contributions of our work are: We provide \nan algorithm for positive supercompilation includ\u00ading folding, for a strict and pure higher-order functional \nlan\u00adguage (Section 4).  We prove that the algorithm terminates and preserves the se\u00admantics of the program \n(Section 5).  We show preliminary benchmarks from an implementation in theTimber compiler (Section 6). \n We outline variations of the algorithm that makes it perform better for certain programs (Section 7). \n We startoutwith someexamplesin Section2togivethe reader an intuitive feel of how the algorithm behaves. \nOur language of study is de.ned in Section 3, right before the technical contribu\u00adtions are presented. \n2. Examples Wadler (1990) uses the example append (append xs ys) zs and shows that his deforestation \nalgorithm transforms the program so that it saves one traversal of the .rst list, thereby reducing the \ncomplexity from 2|xs| + |ys| to |xs| + |ys|. append xs ys = case xs of [] . ys (x : xs) . x : append \nxs ys If we na\u00a8ively change Wadler s algorithm to call-by-value se\u00admantics by eagerly attempting to transform \narguments before at\u00adtacking the body, we do not achieve this improvement in complex\u00adity. An example from \na hypothetical deforestation algorithm that attacks arguments .rst is: f = append (append xs. ys ') zs \n. (Inline the body of append xs. ys . in the context append [] zs . and push down the context into each \nbranch of the case) f = case xs of [] . append ys. zs . (x : xs) . append (x : append xs ys') zs . (Transform \neach branchbut focus on the(x:xs)case) append (x : append xs ys') zs . (The expression contains an expression \nthat has already been seen. The subexpression x : append xs ys. is extracted for separate transfor\u00admation) \n x : append xs ys. x : case xs of  [] . ys . (x : xs ) . x : append xs. ys (A renaming of a previous \nexpression in the second branch) The end result from this transformation is: f = case xs of [] . h1 ys \n. zs . (x : xs) . h1 (h2 x xs ys ') zs . h1 xs ys = case xs of [] . ys (x : xs ) . x : h1 xs ys h2 \nx xs ys = x : case xs of [] . ys (x : xs ) . h2 x xs ys The intermediate structure from the input program \nis still there after the transformation, and the complexity remains at 2|xs| + |ys|! However, doing the \nexact opposite that is, carefully delaying transformationofargumentstoa functionpast inliningofitsbody \nactually leads to the same result asWadler obtains after trans\u00adforming append (append xs ys) zs. Thisisakeyobservationfor \nobtaining deforestation under call-by-value without altering seman\u00adtics, and our algorithm exploits it. \nExcept for thiskey difference, which is necessary to preserve semantics, our algorithm shares many of \nits rules with Wadler s algorithm.The transformationthatis commonly referredtoascase\u00adof-caseis crucialfor \nour algorithm,justlikeitisfora call-by-name algorithm. The case-of-case transformation is useful when \na case statement appears in the head of another case statement, in which case the outer case statement \nis duplicated and pushed into all branches of the inner case statement. Our algorithm also contains rules \nthat correspond to ordinary evaluation which eliminate case statements that have a known constructor \nin their head or to add two primitive numbers. The mechanism that ensures termination basically looks \nfor similar terms to ones that have already been transformed, and if that happens will stop the transformation \nby splitting the term into smaller terms that are transformed separately. The remaining rules of our \nalgorithm shifts the focus to the proper subexpression of expressions and ensures the algorithm does \nnot get stuck. We claim that our algorithm compares favorably with previ\u00adous call-by-name transformations, \nand proceed with demonstrat\u00ading the transformation of common examples. The results are equal to thoseofWadler \n(1990). Our .rstexampleis transformationof sum (map square ys).The functionsusedintheexamplesarede\u00ad.ned \nas: square x = x * x mapf xs = case xs of [] . ys (x : xs) . fx : mapf xs sum xs = case xs of [] . 0 \n (x : xs) . x + sum xs We start our transformation by allocating a new fresh function name(h0)to this \nexpression, inlining the body ofsum and substi\u00adtuting map square ys into the body of sum: case map square \nys of [] . 0 (x : xs ) . x + sum xs After inlining map and substituting the arguments into the body the \nresult becomes: case ( case ys of [] . [] (x : xs ) . (square x ): map square xs ) of [] . 0 (x : xs \n) . x + sum xs  We duplicate the outer case in each of the inner case s branches, usingtheexpressioninthe \nbranchesasheadofthat case-statement. Continuing the transformation on each branch with ordinary reduc\u00adtion \nsteps yields: case ys of [] . 0 (x : xs ) . square x + sum (map square xs ) Now inline the body of the \n.rst square and observe that the second argument to (+) is similartotheexpressionwe started with. We \nreplace the second parameter to(+) with h0 xs '. The result of our transformation is h0 ys, with h0 de.ned \nas: h0 ys = case ys of [] . 0 (x : xs ) . x . * x + h0 xs This new function only traverses its input \nonce, and no interme\u00addiate structures are created. If the expression sum (map square xs) or a renaming \nthereof is detected elsewhere in the input, a call to h0 will be inserted there instead. The work by \nOhori and Sasano (2007) can not fuse two suc\u00adcessive applications of the same function, nor mutually \nrecursive functions.We show that our algorithm can handle these two cases. We need the following new \nfunction de.nitions: mapsq xs = case xs of [] . [] '' ' (x: xs' ) . (x* x' ): mapsq xs f xs = case xs \nof [] . [] '' '' (x: xs) . (2 * x): g xs g xs = case xs of [] . [] '' ' (x: xs) . (3 * x): f xs' Transformingmapsq \n(mapsq xs) will inline the outer mapsq, substitute the argument in the function body and inline the inner \ncall to mapsq: case ( case xs of [] . [] '''' ' (x: xs) . (x* x): mapsq xs) of [] . [] '''' ' (x: xs) \n. (x* x): mapsq xs As previously, we duplicate the outer case in each of the inner case s branches, using \nthe expression in the branches as head of that case-statement. Continuing the transformation on each \nbranch by ordinary reduction steps yields: case xs of [] . [] ' ' '''' ' (x: xs) . (x* x* x* x): mapsq \n(mapsq xs) Thiswill encounterasimilarexpressiontowhatwe startedwith, and createa new function h1. The \n.nal result of our transformation is h1 xs, with the new residual function h1 that only traverses its \ninput once de.ned as: h1 xs = case xs of [] . [] ' ''' ' (x: xs' ) . (x* x* x* x' ): h1 xs For an example \nof transforming mutually recursive functions, consider the transformation of sum (f xs). Inlining the \nbody of sum, substituting its arguments in the function body and inlining the body of f yields: case \n( case xs of [] . [] '' '' (x: xs) . (2 * x): g xs) of [] . 0 ''' ' (x: xs) . x+ sum xs We now move \ndown the outer case into each branch, and perform reductions until we end up with: ''' ' case xs of {[] \n. 0; (x: xs) . (2 * x)+ sum (g xs) } ' We notice that unlike in previous examples,sum (g xs) is not similar \nto what we started transforming. For space reasons, we focus on the transformation of the rightmost expression \nin the last branch, sum (g xs' ), whilekeeping the functions already seen in mind.We inline the body \nof sum, perform the substitution of its arguments and inline the body of g: ' case ( case xsof [] . [] \n '''' '' (x: xs) . (3 * x): f xs'' ) of [] . 0 ''' ' (x: xs) . x+ sum xs We now move down the outer \ncase into each branch, and perform reductions: case xs' of [] . 0 '' (x: xs'' ) . (3 * x'' )+ sum (f \nxs'' ) Expressions e, f ::= || n | x | g | f e | .x.e | k e | e1 . e2 case e of {pi . ei} | let x = f \nin e letrec g = v in e p ::= n | k x Values v ::= n | .x.e | kv Figure 1. The language We noticeafamiliarexpressioninsum \n(f xs'' ), and fold when reaching it. Adding it all together gives a new function h2: h2 xs = case xs \nof [] . 0 ''' ' (x: xs) . (2 * x)+ case xsof [] . 0 '' '' (x: xs) . '' '' (3 * x)+ h2 xs Kort (1996) \nstudied a ray-tracer written in Haskell, and identi\u00ad.eda critical functioninthe innermostloopofa matrix \nmultiplica\u00adtion, called vecDot: vecDot xs ys = sum (zipWith (*) xs ys) Thisis simpli.edby our positive \nsupercompiler to: vecDot xs ys = h1 xs ys h1 xs ys = case xs of '' (x: xs) . case ys of '' (y: ys) . \n'' '' x* y+ h1 xsys . 0 . 0 The intermediate list between sum and zipWith is transformed away,and the \ncomplexity is reduced from2|xs|+|ys| to |xs|+|ys|(since this is matrix multiplication |xs| = |ys|). 3. \nLanguage Our language of study is a strict, higher-order, functionallanguage with let-bindings and case-expressions. \nIts syntax for expressions, values and patterns is shown in Figure 1. We let constructor symbols be denoted \nbyk. Let g range over a set G of global de.nitions whose right-hand sides are all values. The language \ncontains integer values n and arithmetic opera\u00adtions ., although these meta-variables can preferably \nbe under\u00adstood as ranging over primitive values in general and arbitrary op\u00aderations on these.We let \n+ denote the semantic meaning of .. We abbreviate a list of expressionse1 ...en as e, and a list of variables \nx1 ...xn as x. All functions have a speci.c arity and all applications must be saturated; hence .x.map \n(.y.y + 1) x is legal whereas map (.y.y + 1) is not. We denote the free variables of an expression e \nby fv(e), as de.ned in Figure 2. Along the same lines we denote the function names in an expression e \nas fn(e), de.ned in Figure 3. Aprogram is an expression with no free variables and all func\u00adtion names \nde.ned in G. The intended operational semantics is given in Figure 4, where [e/x]e' is the capture-free \nsubstitution of ' expressions e for variables x in e. A reduction context E is a term containing a single \nhole [], which indicates the next expression to be reduced. The expression E(e) is the term obtained \nby replacing the hole in E with e. E fv(x)= {x}fv(n)= \u00d8 fv(g)= \u00d8 fv(ke)= fv(e) fv(.x.e)= fv(e)\\{x}fv(fe)= \nfv(f) . fv(e) fv(let x = e in f)= fv(e) . (fv(f)\\{x}) fv(letrec g = v in f)= fv(v) . fv(f) S fv(case \ne of {pi . ei})= fv(e) . ((fv(ei)\\fv(pi)) fv(e1 . e2)= fv(e1) . fv(e2) Figure 2. Free variables of an \nexpression fn(x)= \u00d8 fn(n)= \u00d8 fn(g)= {g}fn(ke)= fn(e) fn(.x.e)= fn(e) fn(fe)= fn(f) . fn(e) fn(let x = \ne in f)= fn(e) . fn(f) fn(letrec g = v in f) =(fn(v) . fn(f))\\{g} S fn(case e of {pi . ei})= fn(e) . \n((fn(ei)) fn(e1 . e2)= fn(e1) . fn(e2) Figure 3. Function names of an expression Reduction contexts E \n::= [] |E e | (.x.e) E| k E |E. e | n .E | case E of {pi . ei}| let x = E in e Evaluation relation E(g) \n.. E(v)if (g = v) . G (Global) E((.x.e) v) .. E(let x = v in e) .. E(case k v of {ki xi . ei}) .. E([v/x]e)E([v/x]e)E([v/xj \n]ej )if k = kj (App) (Let) (KCase) E(case n of {ni . ei}) .. E(ej )if n = nj (NCase) E(n1 . n2) .. E(n)if \nn = n1 + n2 (Arith) Figure 4. Reduction semantics denotes a list of terms with just a single hole, evaluated \nfrom left to right. If a variable appears no more than once in a term, that term is said to be linear \nwith respecttothatvariable.LikeWadler (1990), we extend the de.nition slightly for linear case terms: \nno variable may appear in both the selector and a branch, although a variable may appear in more than \none branch. The de.nition of append is linear, although ys appears in both branches. We encode letrec \nas an application containing .x, where .x is de.ned as fix = .f.f (.n.fixf n) De.nition 3.1. Letrec is \nde.ned as: ' def ' letrec h = .x.e in e =(.h.e )(.y.fix (.h..x.e) y) strict(x)= {x}strict(n)= \u00d8 strict(g)= \n\u00d8 strict(ke)= strict(e) strict(.x.e)= \u00d8 strict(fe)= strict(f) . strict(e) strict(let x = e in f)= strict(e) \n. (strict(f)\\{x}) strict(letrec g = v in f)= strict(f) T strict(case e of {pi . ei})= strict(e) . ((strict(ei)\\fv(pi)) \nstrict(e1 . e2)= strict(e1) . strict(e2) Figure 6. The strict variables of an expression By de.ning letrec \nas syntactic sugar for other primitives we introduce an implicit requirement that the right hand side \nof letrec statements must not contain any free variables except h. This is not a limitation since functions \nthat contain free variables can be lambda lifted (Johnsson 1985) to the top level. 4. Higher OrderPositive \nSupercompilation Itistimetomakethe intuitiondevelopedin Section2more formal. Our supercompiler is de.ned \nas a set of rewrite rules that pattern\u00admatch on expressions. This algorithm is called the driving algo\u00adrithm,andis \nde.nedin Figure5.Two additional parameters appear as subscriptstotherewrite rules:amemoizationlist . \nandadriving context R. The memoization list holds information about expres\u00adsions already traversed and \nis explained more in detail in Section 4.1. The driving context R is smaller than E, and is de.ned as \nfol\u00adlows: R ::= [] |R e | case R of {pi . ei} | R. e | e .R Interestingly this de.nition coincides with \nthe evaluation contexts for a call-by-name language. The reason our algorithm still pre\u00adserves a call-by-value \nsemantics is that beta-reduction (rule R10) results in a let-binding, whose further specialization in \nrule R14 depends on whether the body expression f is strict in the bound variable x or not. In principle, \nan expression e is strict with regards to a variable x if it eventually evaluates x;in other words, ife \n.. ... .. E(x). Such information is not computable in general, although call-by\u00advalue semantics allows \nfor reasonably tight approximations. One such approximation is given in Figure 6, where the strict variables \nof anexpression e are de.nedasallfreevariablesof e except those thatonlyappearunderalambdaornotinsideall \nbranchesofacase. There is an ordering between rules; i.e., all rules must be tried in the order they \nappear. Rules R11 and R20 are the defaultfall\u00adback cases which extend the given driving context R and \nzoom in on the next expression to be driven. The program is turned inside\u00adout by moving the surrounding \ncontext R into all branches of the case-statement through rules R16 and R19. Rule R14 has a simi\u00adlar \nmechanism for let-statements. Notice how the context is moved out of the recursive call in rule R5, whereas \nrule R7 recursively applies the driving algorithm to the full new term R(n), forcing a re-traversal of \nthe new term in hope of further reductions. Meta\u00advariablea inruleR8andruleR19 standsforan annoying expres\u00adsion; \ni.e., an expression that would be further reducible were it not for a free variable getting in the way. \nThe grammar for annoying expressions is: a ::= x | n . a | a . n | a . a | ae Some expressions should \nbe handled differently depending on context. If a constructor application appears in an empty context, \nthereisnotmuchwe candobuttodrivetheargumentexpressions D[n]R,G,. D[x]R,G,. D[g]R,G,. D[ke][],G,. D[xe]R,G,. \nD[.x.e][],G,. D[n1 . n2]R,G,. D[e1 . e2]R,G,. D[ge]R,G,. D[(.x.f) e]R,G,. D[ee]R,G,. D[let x = v in f]R,G,. \nD[let x = y in f]R,G,. D[let x = e in f]R,G,. D[letrec g = v in e]R,G,. D[case x of {pi . ei}]R,G,. D[case \nkj e of {ki xi . ei}]R,G,. D[case nj of {ni . ei}]R,G,. D[case a of {pi . ei}]R,G,. D[case e of {pi . \nei}]R,G,. = R(n) (R1) = R(x) (R2) = Dapp(g, )R,G,. (R3) = k D[e][],G,. (R4) = R(x D[e][],G,.) (R5) = \n(.x.D[e][],G,.) (R6) = D[R(n)][],G,., where n = n1 + n2 (R7) = D[e1][],G,. .D[e2][],G,., if e1 . e2 = \na (R8) D[e2]R(e1.[]),G,., if e1 = n or e1 = a D[e1]R([].e2),G,., otherwise = Dapp(g, e)R,G,. (R9) = D[let \nx = e in f]R,G,. (R10) = D[e]R([] e),G,. (R11) = D[R([v/x]f)][],G,. (R12) = D[R([y/x]f)][],G,. (R13) \n= D[R([e/x]f)][],G,., if x . strict(f) andflinearw.r.t x (R14) let x = D[e][],G,. in D[R(f)][],G,., otherwise \n= D[R(e)][],G',., where G ' = G. (g, v) (R15) = case x of {pi .D[R([pi/x]ei)][],G,.} (R16) = D[let xj \n= e in ej ]R,G,. (R17) = D[R(ej )][],G,. (R18) = case D[a][],G,. of {pi .D[R(ei)][],G,.} (R19) = D[e]R(case \n[] of {pi.ei}),G,. (R20) Figure 5. Driving algorithm (rule R4). On the other hand -if the application \noccurs at the head of a case expression, we may choose a branch on basis of the constructor and leave \nthe arguments unevaluated in the hope of .nding fold opportunities further down the syntax tree (rule \nR17). Theargumentationis analogousforlambda abstractions:if there isasurrounding context we performa \nbeta reduction, otherwise we drive its body. Notice that the primitive operations ranged over by . can \nnot be unfolded and transformed like ordinary functions can. If the arguments of a primitive operation \nare annoying our algorithm will simply leave the primitive operation in place (rule R8). If we had a \nperfect strictness analysis and could decide whether an arbitrary expression will terminate or not, the \nonly difference in results between our algorithm and a call-by-name counterpart would be for the non-terminating \ncases. In practice, we have to settle for an approximation, such as the simple analysis de.ned in Figure \n6. One may speculate whether the transformations thus missed will have adverse effects on the usefulness \nof our algorithm in practice.We believe wehaveseen clear indications that thisis not the case, and that \ncrucialfactor instead is the ability to inline functionbodies irrespectiveofwhetherargumentsarevaluesornot. \nOur algorithmalways inlines functions unlessthe algorithmde\u00adtects a risk of non-termination. Avoiding \nto inline an expression that couldbe inlined willgive semantically equivalent,but syntac\u00adtically different \noutput from our algorithm. When the twoprograms are executed on a modern processor theywill also most \nlikely per\u00adform differently. Supero (Mitchell and Runciman 2008, Sec. 3.2) has a more advanced inlining \nstrategy, something we leave for fu\u00adture work to investigate. 4.1 Application Rule In the driving algorithm \nrule R3 and rule R9 refer to Dapp(), de.ned in Figure 7. Dapp() can be inlined in the de.nition of the \ndriving algorithm, it is merely given a separate name for improved clarity of the presentation. Figure \n7 contains some new notation: we use = to denote equality of two expressions up to renaming of variables \nand == for syntactical equivalence of expressions. GEN(e1,e2)= (.t, used . ( S used ' )) where (tg,. \n1' ,. 2' )= msg(e1,e2), (t ' ,s)= split e1 used ' = {u |( ,u) . S},. = {e |(e, ) . S} (t, used) = D[tg][],G,., \nif tg= x D[t '][],G,., otherwise S = D[.1'][],G,., if tg= x D[s][],G,., otherwise Figure 8. Generalization \nCare needs to be taken to ensure that recursive functions are not inlined forever. The driving algorithmkeepsa \nrecordof previously seen applications in the memoization list .;whenever it detects an expression that \nis equivalent (up to renaming of variables) to a pre\u00adviousexpression, the algorithm createsanewrecursivefunction \nhn for some n. Whenever such an expression is encountered again, a call to hn is inserted. This is not \nsuf.cient to guarantee termination of the algorithm,but the mechanism is crucial for the complexity improvements \nmentioned in Section 2. To ensure termination, we use the homeomorphic embedding relation S to de.ne \na predicate called the whistle . When the predicate holds for an expression we say that the whistle blows \non that expression. The intuition is that when e S f, fcontains all subexpressions of e, possibly embeddedin \notherexpressions.For anyin.nite sequence e0,e1,... there exists iand jsuchthat i<j and ei S ej . This \ncondition is suf.cient to ensure termination. In order to de.ne the homeomorphic embedding we need a \nde.nitionof uniform terms analogoustotheworkby S\u00f8rensenand Gl\u00a8 uck (1995), which we adjust slightly to \n.t our language. De.nition 4.1 (Uniform terms). Let s range over the set N . X . K .{caseof, let, letrec, \nprimop, lambda, apply}, and let caseof(e), let(e), letrec(v, e), primop(e), lambda(e), and apply(e) denote \na case, let, recursive let, primitive operation, lambda abstraction or application for all subexpressions \ne, e and v.Thesetof termsTisthe smallestsetofarityrespecting symbol applications s(e). Dapp(g, e)R,G,. \n=(h ' x, {(h ' , Nothing)}), where x = fv(R(ge)) Dapp(g, e)R,G,. = (x ' , {(h ' , Just R(g e))}), where \nx ' fresh Dapp(g, e)R,G,. = GEN(R(g e), t) Dapp(g, e)R,G,. = GEN(R(g e), head(W )) (letrec h = .x.e ' \nin h x, used), (e ' , used), where (g = v) .G, (e if .(h ' ,t) ...t = R(ge) if .(h ' ,t) ...t S R(ge)and \nt == .(R(ge)) if .(h ' ,t) ...t S R(ge)ifW = \u00d8 if found = \u00d8 otherwise ' , used)= D[R(ve)][],G,. ' x = \nfv(R(ge)), . ' = . . (h, R(ge)), h fresh, W = {e|(n, Just e) . used, n == h}found = {n|(n, Nothing) . \nused, n == h} Figure 7. Driving of applications De.nition 4.2 (Homeomorphic embedding). De.ne S as the \nsmallest relation on T satisfying: e S fi for somei x S y, n1 S n2,, e S s(f1,...,fn) e1 S f1,...,en \nS fn s(e1,...,en) S s(f1,...,fn) Examples of the homeomorphic embedding are shown in Figure 9. Whenever \nthe whistle blows, our algorithm splits the input ex\u00adpression into strictly smaller terms that are driven \nseparately in the empty context. This might expose new folding opportunities, and allows the algorithm \nto remove intermediate structures in subex\u00adpressions. The design follows the positive supercompilation \nas out\u00adlined by S\u00f8rensen (2000), except that we need to substitute the transformed expressions back instead \nof pulling them out into let\u00adstatements, in order to preserve strictness. Our algorithm is also more \ncomplicated because we perform the program extraction im\u00admediately instead of constructing a large tree \nand extracting the program in a separate pass. Splittingexpressionsis rather intricate,andtwomechanisms \nare needed, the .rst is the most speci.c generalization that entails the smallest possible loss of knowledge, \nand is de.ned as: De.nition 4.3 (Most speci.c generalization). An instance of a term e is a term of \nthe form .e for some substitution ..  A generalization of two terms e and f is a triple (tg,.1,.2), \nwhere .1,.2 aresubstitutions suchthat .1tg = e and .2tg = f.  Amost speci.c generalization (msg) of \ntwo terms e and fis a generalization(tg,.1,.2) suchthat forevery othergeneraliza\u00adtion (t ' g,. 1' ,. \n2' ) of e and fit holds that tg is an instance of t ' g.  We refer totg as the ground term.For background \ninformation and an algorithm to compute most speci.c generalizations, see Lassezetal.(1988).Figure9also \ncontainsexamplesofthemsg. The most speci.c generalization is not always suf.cient to split expressions. \nFor some expressions it will return the ground term as a variable, and the respective .s equal to the \ninput terms. If this happens, a function split is needed, de.ned as: De.nition 4.4 (Split). Fort . T \nwe de.ne split(t) by: split(s(e1,...,en)) = (s(x1,...,xn), [e1/x1,...,en/xn]) with x1,...,xn fresh. Alternative \n2 of Dapp() is for upwards generalization, and alternatives3and4 are for the downwards generalization. \nThis is exempli.ed below. The generalization algorithm is shown in Figure D[ append xs xs] (*) (Put(h0, \nappend xs xs)in. and transform according to the rules of the algorithm) case xs of [] . xs ''' ' (x \n: xs ) .D[ x ] : D[ append xs xs] (Focus on D[ append xs ' xs] and recall that . con\u00adtains append xs \nxs so alternative 2 of Dapp() is triggered and the transformation returns (x, {(h0, Just (append xs ' \nxs))}). This returns all the way up to (*) and restarts the transformation there with W = {append xs \n' xs}) D[ append xs xs] = (Generalize the expression with append xs ' xs) [D[ xs ' ]/x, D[ xs ' ]/y] \nD[ append x y] = letrec h0 xs ys = case xs of [] . ys '' ' ' (x : xs ) . x : h0 xs ys in h0 xs xs Figure \n10. Example of upwards generalization 8. If the ground term tg isavariable the algorithm drives the output \nfrom split, otherwise it will drive the output from the msg. All the examples of how our algorithm works \nin Section 2 eventually terminate through a combination of alternative 1 and alternative4(found = \u00d8)ofDapp(). \nThe second alternative of Dapp() in combination with the fourth alternative(W = \u00d8)is useful when transforming \nfunction calls that have the same parameter appearing twice, for example append xs xs as shown in Figure \n10. For space reasons we have omitted several intermediate steps that do not contribute to the un\u00adderstanding \nof the current discussion. The third alternative is used when terms are growing in some sense. An example \nof reverse with an accumulating parameter is shown in Figure 11 with the de.nition of reverse as: rev \nxs ys = case xs of [] . ys '' '' (x : xs ) . rev xs (x : ys) e f tg .1 .2 e S Just e x [e/x] [Just \ne/x] Right e S Right (e, e ' ) Right x [e/x] [(e, e ' )/x] fac y S fac (y - 1) fac x [y/x] [(y - 1)/x] \n Figure 9. Examples of the homeomorphic embedding and the msg D[ rev xs [] ] (Put(h0, rev xs [])in . \nand transform the program according to the rules of the algorithm) case xs of [] . [] '' '' (x : xs ) \n.D[ rev xs (x : [])] (Focus on the second branch and recall that. contains rev xs [] so alternative3of \nDapp() is triggered and the expression is generalized) D[ rev xs ' (x ' : [])] (Generalize the expression \nwith rev xs []) [D[ (x ' : [])]/zs]D[ rev xs ' zs] (Put(h1, rev xs ' zs)in. and transform according to \nthe rules of the algorithm) = letrec h1 xs ys = case xs of [] . ys '' '' (x : xs ) . h1 xs (x : ys) in \nh1 xs ' (x ' : []) (Putting the two parts together) case xs of [] . [] (x ' : xs ' ) . letrec h1 xs \nys = case xs of [] . ys (x ' : xs ' ) . h1 xs ' (x ' : ys) in h1 xs ' (x ' : []) Figure 11. Example of \ndownwards generalization To actually perform the generalization both upwards and down\u00adwards the driving \nalgorithm must propagate information in both directions, hence the return type of D[] must be changed \nto a pair, which inturn requires glue code for all the rules in Figure 5. This glue code simply merges \nthe second component of the return val\u00adues. The following is an example of a Haskell implementation of \nrule R4 with the return type changed to a pair and the return values from the subcomputations are merged \nand propagated upwards: drive r rho (ECon name args)= (ECon name args ' , concat used) where (args ' \n, used)= unzip (map dr args) dr = drive emptyContext rho 5. Correctness The problem with previous deforestation \nand supercompilation al\u00adgorithms in a call-by-value context is that they might change ter\u00admination propertiesof \nprograms.We prove that our supercompiler both terminates and does not alter whethera program terminates \nor not. The complete proofs are available from a companion techni\u00adcal report (Jonsson and Nordlander \n2008),but they do not reveal anything unexpected. 5.1 Termination In order to prove that the algorithm \nterminates we show that each recursive application of D[] in the right-hand sides of Figure 5 and 7 has \na strictly smaller weight than the left-hand side. The weight of an expression is one plus the sum of \nthe weight of its subexpressions, where variables, primitive numbers and function names have weight two. \nThe weight of a fresh variable not in the initial input is one. De.nition 5.1. The weight of a variable \nx in the initial input, a primitive number n, and a function name g is 2. The weight of a fresh variable \nnot in the initial input is 1. The weight of any P composite expression(n = 1)is|s(e1,...,en)| =1+ n \n|ei|. i=1 De.nition 5.2. LetSbea set witharelation =. Then (S, =) is a quasi-order if = is re.exive and \ntransitive. De.nition 5.3. Let (S, =) beaquasi-order. (S, =) isawell-quasi\u00adorder if, for every in.nite \nsequence s0,s1,... . S, there are i<j with si = sj The weight of the entire transformation is a triple \nthat contains the maximum length of the memoization list . denoted by N, the weight of the term being \ntransformed and the weight of the current term in focus. That such an N exists follows from the homeomorphic \nembeddingisawell-quasi-orderand Kruskal sTree Theorem (Dershowitz 1987): Theorem 5.4 (Kruskal s Tree \nTheorem). If S is a .nite set of function symbols, then any in.nite sequence t1,t2,... of terms from \nthe setS contains two terms ti and tj with i<j such that ti S tj . We need to show that the memoization \nlist . only contains elements that were in the initial input program: Lemma 5.5. The second component \nof the memoization list, .,can only contain termsfrom the setT. De.nition 5.6. The weight of a call to \nthe driving algorithm is |D[e]R,G,.| =(N -|.|, |R(e)|, |e|) Tuples must be ordered for us to tell whether \nthe weight of a term actually decreases fromdrivingit.We usethe standardlexical order between tuples. \nWith these de.nitions in place, we can formulate a lemma that the weight is decreasing in each step of \nour algorithm. Lemma 5.7. For each rule R: D[e]R,G,. = e1 in Figure 5 andFigure7 and each recursive application \nD[e ' ]R ' ,G,. ' in e1, |D[e ' ]R ' ,G,. ' | < |D[e]R,G,.| Lemma 5.8 (Totality). For all well-typedexpressionse,D[e]R,G,. \nis matchedbya unique ruleinFigure5. Proposition 5.9 (Termination). The driving algorithm D[] termi\u00adnates \nfor all well-typed inputs. Proof. The weight of the transformation is de.ned because the homeomorphic \nembedding is a well-quasi-order combined with Kruskal s Tree Theorem. Lemma 5.5 guarantees that the memo\u00adization \nlist . only contains terms from the initial input. By Lemma 5.7 the weightof the transformation decreases \nfor each step andby Lemma 5.8 we know that each recursive application will match a rule. Since < is well-founded \nover triples of natural numbers the system will eventually terminate.  5.2 Total Correctness We de.ne \nthe standard notions of operational approximation and equivalence.Ageneral contextCwhich has zero or \nmore holes in the place of some subexpressions is introduced. De.nition 5.10 (Operational Approximation \nand Equivalence). e operationally approximates e , e re ', if for all contexts C Proof. We sketch on \nthe proof for the .rst half of rule R14. All the remaining rules have similar proofs except for rule \nR9, where the proofis similarin structuretotheproofbySands(1996,p.24).We have that .(D[let x = e in f]R,G,.)= \n.(D[R([e/x]f)][],G,.). Evaluating the input term willeventually yielda context E() witha term: R(let \nx = e in f) ..r R(let x = v in f ) .. R([v/x]f) ..s E(v), and evaluating the input to the recursive call \nyields (re\u00admember thatfis linearw.r.tx): R([e/x]f) ..s E(e) ..r E(v). These two resulting terms are syntactically \nequivalent, and there\u00adfore cost equivalent. By Lemma 5.13 their ancestor terms are cost equivalent, R(let \nx = e in f) SC R([e/x]f), and cost equivalence implies strong improvement. By the induction hy\u00adpothesis \nR([e/x]f) Cs .(D[R([e/x]f)][],G,.), and therefore R(let x = e in f) Cs .(D[let x = e in f]R,G,.). 6. \nBenchmarks We provide measurements from a set of common examples from the literature on deforestation. \nWe show that our positive super\u00adcompiler does remove intermediate structures, and can improve the performanceby \nan orderof magnitude for certain benchmarks.We have left out the full details of the instrumentation \nof the runtime such that C[e], C[e ] are closed, if evaluation of C[e] termi\u00ad nates then so does evaluation \nof C[e ]. system and the transformed result of each benchmark for space rea\u00ad~'' r sons,but they areavailableina \nseparate report (Jonsson 2008). e is operationally equivalent to e , e = e ', if e re and ee e e e All \nmeasurements were performed on an idle machine running The correctness of deforestation in a call-by-name \nsetting has in an xterm. Each test was run 10 consecutive times and the best previously been shown by \nSands (1996) using his improvement theory. Notice that improvement C below is not the same as the homeomorphic \nembedding S de.ned previously.We use Sands s de.nitions for improvement and strong improvement: De.nition \n5.11 (Improvement, Strong Improvement). e is improved by e , e C e ', if for all contexts C such that \nC[e], C[e ] are closed, if computation of C[e] terminates using nfunction calls, then computation of \nC[e ] also terminates, and uses no more than n function calls. ' ~' eis strongly improvedby e , e Cs \ne ', iff e C e and e = e . We usee ..k v to denote that e evaluates to v using kfunction calls, and anyother \nreduction rule as manytimes as it needs, and e ' ..=k v ' to denote that e evaluates to v with at most \nkfunction calls and anyother reduction rule as manytimes as it needs. De.nition 5.12 (Cost equivalence). \nThe expressions e and e are ' '' cost equivalent, e SC e iff e C e and e C e Cost equivalence implies \nstrong improvement. If two terms evaluatewiththe samecosttotwocostequivalentexpressions,then the initial \nterms are also cost equivalent: Lemma 5.13 (Sands (1996)). If e1 ..r e ' 1 and e2 ..r e ' 2 then (e1 \n' SC e2 ' . e1 SC e2). With these de.nitions in place, total correctness for a transfor\u00admation can be \nstated: Theorem 5.14 (Sands). If e Cs e ', a transformation that replaces e by e is totally correct. \nImprovement theory in a call-by-value setting requires Sands s operational metatheory for functional \nlanguages (Sands 1997). Proposition 5.15 (Total Correctness). Let e be an expression, and . an environment \nsuchthat the range of . contains only closed expressions, and  fv(e) n dom(.)= \u00d8  then e Cs .(D[e][],G,.). \nresult was selected. The best result was selected since it must ap\u00adpear under the minimum of other activity \nof the operating system. The number of allocations and total allocation size remains con\u00adstant over all \nruns. The raw data for the time, size and allocation measurements are shown in Table 1. The time column \nis number of clockticks from the RDTSC instruction available in Intel/AMD processors, and the binary \nsize is in bytes as reported by ls. The total number of allocations and the total memory size allocated \nby the program are displayed in each column. The binary sizes are slightly increased by the supercompiler, \nbut the runtimes are allfaster. The main reason for the performance improvement is the removal of intermediate \nstructures, reducing the amount of memory allocations. The work on Supero by Mitchell and Runciman (2008) \nshows that there are open problems for supercompiling large Haskell programs. These problems are mainly \nrelating to speed, both of the compiler, and of the transformed program. When theypro.led Supero, they \nfound that the majority of the time was spent in the homeomorphic embedding test. Our algorithm performs \nthe test on a smaller part of the tree, so there is reason to believe that less time will be spent in \nthe test for our algorithm. The complexity of the homeomorphic embedding has been investigated separately \nby Narendran and Stillman (1987) and they give an algorithm of complexity O(size(e) \u00d7 size(f)) to decide \nwhether e S f .We expect the same problems that Mitchell and Runciman observed to appear in a call-by-value \ncontext as well, and intend to investigate them now that we have a theoretical foundation for our algorithm. \n 6.1 DoubleAppend As previously seen, appending three lists saves one traversal over the .rst list.ThisisanexamplebyWadler \n(1990),andthe interme\u00addiate structure is fused away by our supercompiler. Three strings of 9000 characters \neach were appended to each other into a 27 000 characters long string. The number of allocations goes \ndown, and one iteration over the .rst string is avoided. The binary size in\u00adcreases 1316 bytes, on a \nbinary of roughly 90k. Time Binary size Allocations Alloc Size Benchmark Before After Before After Before \nAfter Before After Double Append 105 844 704 89 820 912 89 484 90 800 270 035 180 032 2160 280 1440 256 \nFactorial 21 552 21 024 88 968 88 968 9 9 68 68 Flip a Tree 2131 188 237 168 95 452 104 704 20 504 57 \n180 480 620 Sum of Squares of a Tree 276 102 012 28 737 648 95 452 104 912 4194 338 91 29 360 496 908 \nKort s Raytracer 12 050 880 7969 224 91 968 91 460 60 021 17 320 144 124 Table 1. Time, space and allocation \nmeasurements 6.2 Factorial There are no intermediate lists created in a standard implementa\u00adtion, so \nanyperformance improvements come from inlining or re\u00adductions. One recursion and a couple of reductions \nare eliminated, thereby slightly reducing the runtime. The allocations remain the same and the .nal binary \nsize remains unchanged. 6.3 FlipaTree Flippinga treeis anotherexamplebyWadler (1990),andjustlike Wadler \nwe perform a double .ip (thus restoring the original tree) before printing the total sumof allleaves. \nThe supercompiler man\u00adages to eliminate the double .ip. The total number of allocations and the total \nsize of allocations is reduced. The runtime also goes down by an order of magnitude! The binary size \nincreases close to 10% however. 6.4 Sumof SquaresofaTree Computing the sum of the squares of the data \nmembers of a tree is the .nalexamplebyWadler (1990). Almost all allocations are removedby our supercompiler,butthe \nbinary sizeis increasedby nearly 10%. The runtime is improved by an order of magnitude.  6.5 Kort s \nRaytracer The inner loop of a Raytracer (Kort 1996) is extracted and trans\u00adformed. The total runtime, \nthe number of allocations, the total size of allocations and the binary size are all decreased. 7. Extensions \nThere are several ways the driving algorithm can be extended to makeit morepowerful.We show that with \nanextended Let-rulein combination with a disabled whistle for closed expressions we can evaluate manyclosed \nexpressions. If the let-expression contains no free variables we can drive either the right hand side \nor the body and see what the result is. Another option is to augment rule R14 with a second and third \nalternative as shown in Figure 12. The reasoning behind this is that a closed expression contains all \ninformation needed to evaluate it, thus a fold should be unnec\u00adessary. If the expression diverges, then \nso does the .nal program at that point. The immediate question following from the above is whether this \nis bene.cial for expressions that are not closed, a question we haveno de.nite answerto.Anexampleofthe \nbene.tofdrivingthe body is bodyEx as shown below. bodyEx = let x = e in case [] of {[] . x;(x : xs) . \ny} D bodyEx ] = D let x = e in case [] of { [] . x;(x : xs) . y }] = D[ e ] We can see how the body becomes \nstrict after driving it, which opens up for further transformations. Duplicating code to enable further \ntransformations might some\u00adtimes be bene.cial. Removing the linearity constraint of rule R14 could enable \nfurther transformations,but it could just as well turn out to have duplicated work, forcing multiple \nevaluations of the same expression, as well as growth in code size. The current al\u00adgorithm has no means \nof .nding a suitable trade-off. Obtaining a more re.ned behavior in this respect is left for future work. \n8. RelatedWork There exists much literature concerning algorithms that remove intermediate structures \nin functional programs. However, most of it is in a call-by-name or call-by-need context which makes \nit a different, yet dif.cult, problem. We therefore start our survey of related work with one call-by-value \ntransformation, and then look at the related transformations from call-by-name and call-by-need contexts. \n8.1 Lightweight Fusion Lightweight Fusion (Ohori and Sasano 2007) works by promoting afunction through \nthe .x point operator and guarantees termination by limiting each function to be inlined at most once. \nTheyimple\u00admentthe transformationinavariantofa compilerfor StandardML and present some benchmarks. The \nalgorithm is proven correct for a call-by-name language. It is explicitly mentioned that their goal is \nto extend the transformation to work for an impure call-by-value functional language. Comparing lightweight \nfusion to our positive supercompiler is somewhat dif.cult, the algorithms are not very similar in them\u00adselves. \nComparing results of the algorithms is more straightforward the restriction to only inline functions \nonce makes lightweight fu\u00adsion unable to handle successive applications of the same function and mutually \nrecursivefunctions, something the positivesupercom\u00adpiler handles gracefully. Considering the early stage \nof their work, we still .nd it an interesting approach that seems to solve a lot of problems.  8.2 Deforestation \nDeforestation, removing intermediate structures from programs, was pioneered by Wadler (1990) for a .rst \norder language more than .fteen years ago. The initial deforestation had support for higher order macros, \nincapable of fully emulating higher order functions. Marlow andWadler (1992) addressed the restriction \ntoa .rst\u00adorder language when they presented a deforestation algorithm for a higher order language. This \nwork was re.ned in Marlow s (1995) dissertation, where he also related deforestation to the cut\u00adelimination \nprinciple of logic. Chin (1994) has also generalised Wadler s deforestation to higher-order functional \nprograms by us\u00ading syntactic properties to decide which terms that can be fused. Both Hamilton (1996) \nand Marlow(1995) haveproven that their deforestation algorithms terminate. More recentworkby Hamilton \n(2006) extends deforestation to handle a wider range of functions, with an easy to recognise treeless \nform, giving more transparency for the programmer. Alimarine and Smetsers (2005) have improved the producer \nand consumer analyses in Chin s (1994) algorithm to be based on D[let x = e in f]R,G,. = D R([e/x]f) \n[],G,., if x . strict(f) andflinearw.r.tx D R([v/x]f)] [],G,., if D e][],G,. = v ' '' D[ R([e/x]f )][],G,., \nif D[ f][],G,. = f,x . strict(f ) andf linearw.r.tx let x = D[e][],G,. in D[R(f)][],G,., otherwise Figure \n12. Extended Let-rule semantics rather than syntax. They show their algorithm can re\u00admove much of the \noverhead introduced from generic programming (Hinze 2000). Whileallthisworkis algorithmically rather \ncloseto oursdueto the close relation between deforestation and positive supercompi\u00adlation, it is in a \ncall-by-name or call-by-need context.  8.3 Supercompilation Closely related to deforestation is supercompilation \n(Turchin 1979, 1980, 1986a,b). Supercompilation both removes interme\u00addiate structures, achieves partial \nevaluation as well as some other optimisations. In partial evaluation terminology, the decision of when \nto inline is taken online. The initial studies on supercompila\u00adtion were for the functional language \nRefal(Turchin 1989). The positive supercompiler (S\u00f8rensen et al. 1996) is a vari\u00adant which only propagates \npositive information, such as equali\u00adties. The propagation is done by uni.cation and the work high\u00adlights \nhow similar deforestation and positive supercompilation re\u00adally are. Narrowing (Albert andVidal 2001)is \nthe functional logic programming community equivalent of positive supercompilation but formulated asaterm \nrewriting system. Theyalso deal with non\u00addeterminism from backtracking, which makes the algorithm more \ncomplicated. Strengthening the information propagation mechanism to prop\u00adagate not only positive,but \nalso negative information, yields per\u00adfect supercompilation (Secher 1999; Secher and S\u00f8rensen 2000). \nNegative information is the opposite of positive information, in\u00adequalities. These inequalities can be \nused to prune branches that are certainly not taken in case-statements for example. More recently, Mitchell \nand Runciman (2008) have worked on supercompiling Haskell. They report runtime reductions of up to 55% \nwhen their supercompiler is used in conjunction with GHC. The positive supercompiler by S\u00f8rensen et al. \n(1996) is the immediate ancestor of our work, but we extended it to a higher\u00adorder languageand convertedittoworkon \ncall-by-value languages. 8.4 GeneralizedPartial Computation GPC (Futamura and Nogi 1988) uses a theorem \nprover to extract additional properties about the program being specialized. Among these properties are \nthe logical structure of a program, axioms for abstract data types, and algebraic properties of primitive \nfunctions. Earlywork on GPCwas performedbyTakano (1991). Atheorem prover is used on top of the transformation \nand when\u00adeveratestis encounteredthe theoremproververi.es whetheroneor more branches can be taken. Information \nabout the predicate which was tested is propagated along the branches that are left in the re\u00adsulting \nprogram.The reasonGPCissuchapowerful transformation is because it assumes the unlimited power of a theorem \nprover. Futamura et al. (2002) has applied GPC in a call-by-value set\u00adtinginasystem called WSDFU(Waseda \nSimplify-Distribute-Fold-Unfold), reporting manysuccessful experiments where optimal or near optimal \nresidual programs are produced. It is unclear whether WSDFU preserves termination behaviour or if it \nis a call-by-name transformation applied to a call-by-value language. We note that the rules for the \n.rst order language presented by Takano (1991) arevery similar to the positive supercompiler,but the \ntheorem prover required might exclude the technique as a can\u00addidate for automatic compiler optimisations. \nThe lack of termina\u00adtion guarantees for the transformation might be another obstacle. Considering the \nsimilarity of GPC and positive supercompilation it should be straight forward to convert GPC to work \non a call-by\u00advalue language, which makes it rather close to our work. 8.5 OtherTransformations Considering \nthe vast amount of research conducted on program transformations, we only brie.y survey other related \ntransforma\u00adtions. 8.5.1 Partial Evaluation Partial evaluation (Jones et al. 1993) is another instance \nof Burstall and Darlington s (1977) informal class of fold/unfold transforma\u00adtions. If the partial evaluation \nis performed of.ine, the process is guided by program annotations that tells when to fold, unfold, instantiateand \nde.ne. Binding-TimeAnalysis(BTA)isaaprogram analysis that annotates operations in the input program based \non whether theyare statically known or not. Partial evaluation does not remove intermediate structures, \nsomething we deem necessary to enable the programmer to write programs in the clear and concise listful \nstyle. Both deforesta\u00adtion and supercompilation simulate call-by-name evaluation in the transformer, \nwhereas partial evaluation simulates call-by-value. It is suggested by S\u00f8rensen et al. (1994) that this \nmight affect the strength of the transformation. 8.5.2 Short Cut Deforestation Short cut deforestation \n(Gill et al. 1993; Gill1996) takes a differ\u00adent approach to deforestation, sacri.cing some generality \nby only working on lists. The idea is that the constructors Nil and Cons can be replaced by a foldr consumer, \nand a special function build is used for the transformation to recognize the producer and enforce the \ntype re\u00adquirement. Lists using build/foldrcan easily be removed with the foldr/build rule: foldr f c \n(build g)= gf c. This shifts theburden from the compiler on to the programmer or compiler writer to makesure \nlist-traversing functions are written using buildand foldr, thereby cluttering the code with information \nfor the optimiser and making it harder to read and understand for humans. Gill implemented and measured \nshort cut deforestation in GHC using the no.b benchmark suite (Partain 1992). Around a dozen benchmarksimprovedby \nmorethan5%,averagewas3%andonly one example got noticeably worse, by 1%. Heap allocations were reduced, \ndown to half in one particular case. The main argument for short cut deforestation is its simplicity \non the compiler side compared to full-blown deforestation. GHC as of today contains a variant of the \nshort cut deforestation imple\u00admented by use of the rewrite rules (Jones et al. 2001) available in GHC. \nGhani and Johann (2008) have generalized the foldr/build rule to a fold/superbuild rule that can eliminate \nintermediate structures of inductive types without disturbing the contexts in which theyare situated. \n 8.5.3 Type-inference Based Short Cut Deforestation Type-inference can be used to transform the producer \nof lists into the abstracted form required by short cut deforestation, and this is exactly what Chitil \n(2000) does. Given a type-inference algorithm which infers the most general type, Chitil is able to determine \nthe list constructors that need to be replaced in one pass. From the principal type property of the type \ninference algorithm Chitil was able to deduce completeness of the list abstraction algo\u00adrithm. This completeness \nguarantees that if a list can be abstracted from a producer by abstracting its list constructors, then \nthe list abstraction algorithm will do so. Theimplicationsof the completenessis thata foldr consumer \ncan be fused with nearly anyproducer.A reason list constructors might not be abstractable from a producer \nis that theydo not occur in the producerexpressionbutin the de.nitionofa function which is called by \nthe producer. A worker/wrapper scheme proposed ensures that these list constructors are moved to the \nproducer to make the list abstraction possible. Chitil compared heap allocation and runtime between the \nshort cut deforestation in GHC 4.06 and a program optimised with the type-inference based short cut deforestation. \nThe example in ques\u00adtion was the n-queens problem, where n was set to 10 in order to make I/O time less \nsigni.cant than a smaller instance would have. Heap allocation went from 33 to 22 megabytes and runtime \nfrom 0.57 seconds to 0.51 seconds. The completeness property and the fact that the programmer does not \nhave to write any special code in combination with the promising results from measurements suggests type-inference \nbased short cut deforestation is a practical optimisation. 8.5.4 Zip Fusion Takano and Meijer (1995) \nnoted that the foldr/build rule for short cut deforestation had a dual. This is the destroy/unfoldr rule \nused in Zip Fusion (Svenningsson 2002) which has some interesting properties. It can remove all argument \nlists from a function which con\u00adsumes more than one list. The method described by Svennings\u00adson will \nremove all intermediate lists in zip [1..n] [1..n], one of the main criticisms against the foldr/build \nrule. The technique can also remove intermediate lists from functions which consume their lists using \naccumulating parameters, a known problematic case when fusing functions that most techniques can not \nhandle. The destroy/unfoldr rule is de.ned as: destroy g (unfoldr psi e)= g psi e The method is simple, \nand can be implemented the same way as short cut deforestation. It still suffers from the drawback that \nthe programmer or compiler writer has to make sure the list traversing functions are written using destroy \nand unfoldr. 9. Conclusions A positive supercompiler, for a higher-order call-by-value lan\u00adguage, that \nincludes folding has been presented.Wehave provenit correct. The adjustment to the algorithm for preserving \ncall-by-value se\u00admanticsisnewandworks surprisinglywellformanyexamplesthat were intended to show the usefulness \nof call-by-name transforma\u00adtions. 9.1 FutureWork We believe that the linearity restriction of rule R14 \nin the proof of correctnessisnot necessaryforthe soundnessofour algorithm,but havenot foundawaytoproveityet.Wewillinvestigatehowthe \nconcept of an inline budget may be used to obtain good balance between code size and inlining bene.ts. \nMore work could be done on strictness analysis component of our supercompiler. We do not intend to focus \non that subject, though; instead we hope that the modular dependencyon strictness analysis will allow \nour supercompiler to readily take advantage of general improvements in the area. Acknowledgments The \nauthors would like to thank Simon Marlow, Duncan Coutts and Neil Mitchell for valuable discussions. We \nwould also like to thankViktor Leijon and the anonymous referees for providing useful comments that helped \nimprove the presentation and contents andGerm\u00b4anVidalforexplaining narrowingtous. References E. AlbertandG.Vidal.The \nnarrowing-driven approachto functionallogic program specialization. New Generation Comput, 20(1):3 26, \n2001. A. Alimarine and S. Smetsers. Improved fusion for optimizing generics. In ManuelV. Hermenegildo \nand Daniel Cabeza, editors, Practical Aspects of Declarative Languages, 7th International Symposium, \nPADL 2005, LongBeach,CA,USA,January 10-11,2005,Proceedings,volume 3350 of Lecture Notes in Computer Science, \npages 203 218. Springer, 2005. ISBN 3-540-24362-3. R.M. Burstall and J. Darlington. Atransformation system \nfor developing recursive programs. Journalof theACM, 24(1):44 67, January 1977. W-N.Chin.Safefusionof \nfunctionalexpressionsII:Furtherimprovements. J. Funct. Program, 4(4):515 555, 1994. O. Chitil. Type-Inference \nBased Deforestation of Functional Programs. PhD thesis,RWTH Aachen, October 2000. N. Dershowitz. Termination \nof rewriting. Journal of Symbolic Computa\u00adtion, 3(1):69 115, 1987. Y. Futamura and K. Nogi. Generalized \npartial computation. In D. Bj\u00f8rner, A.P. Ershov, and N.D. Jones, editors, Partial Evaluation and Mixed \nComputation, pages 133 151. Amsterdam: North-Holland, 1988. Y. Futamura, Z.Konishi, and R. Gl\u00a8uck. Program \ntransformation system based on generalized partial computation. New Gen. Comput., 20(1): 75 99, 2002. \nISSN 0288-3635. N. Ghani andP. Johann. Short cut fusion of recursive programs with com\u00adputational effects. \nInP. Achten,P.Koopman, and M.T. Moraz\u00b4an, edi\u00adtors, DraftProceedingsofTheNinthSymposiumonTrendsin Functional \nProgramming (TFP), number ICIS R08007, 2008. A. Gill, J. Launchbury, and S.L. Peyton Jones. A short \ncut to deforesta\u00adtion. In Functional Programming Languages and Computer Architec\u00adture, Copenhagen, Denmark, \n1993, 1993. A. J. Gill. Cheap Deforestation for Non-strict Functional Languages. PhD thesis, Univ. of \nGlasgow, January 1996.  G.W. Hamilton. Higher order deforestation. In PLILP 96: Proceedings of the 8th \nInternational Symposium on Programming Languages: Imple\u00admentations, Logics, and Programs, pages 213 227, \nLondon, UK, 1996. Springer-Verlag. ISBN 3-540-61756-6. G. W. Hamilton. Higher order deforestation. Fundam. \nInformaticae, 69 (1-2):39 61, 2006. R. Hinze. Generic Programs and Proofs. PhD thesis, Habilitationsschrift, \nBonn University, 2000. T. Johnsson. Lambda lifting:Transforming programsto recursiveequations. In FPCA, \npages 190 203, 1985.  N.D. Jones, C.K. Gomard, andP. Sestoft. Partial Evaluation andAutomatic Program \nGeneration. Englewood Cliffs, NJ: Prentice Hall, 1993. ISBN 0-13-020249-5. S.L.Peyton Jones,A.Tolmach,andT. \nHoare. Playingbythe rules:Rewrit\u00ading as a practical optimisation technique in GHC. In Ralf Hinze, editor, \nProceedingsof the 2001ACM SIGPLAN HaskellWorkshop (HW 2001), 2nd September 2001, Firenze, Italy., Electronic \nNotes in Theoretical Computer Science,Vol59. UtrechtUniversity, September28 2001. UU\u00adCS-2001-23. P. A. \nJonsson. Positive supercompilation for a higher-order call-by-value language. Licentiate thesis, Lule\u00b0a \nUniversity of Technology, Sweden, Jun 2008. P. A. Jonsson and J. Nordlander. Positive Supercompilation \nfor a Higher Order Call-By-Value Language: Extended Proofs. Technical Report 2008:17, Department of Computer \nscience and Electrical engineering, Lule\u00b0a UniversityofTechnology, October 2008. J.Kort. Deforestationofa \nraytracer. Master s thesis, Universityof Amster\u00addam, 1996. J-L. Lassez, M. Maher, and K. Marriott. Uni.cation \nrevisited. In Jack Minker, editor, Foundations of Deductive Databases and Logic Pro\u00adgramming, pages 587 \n625. Morgan Kaufmann, 1988. X. Leroy. The Objective Caml system: Documentation and user s manual, 2008.WithD. \nDoligez,J. Garrigue,D.R\u00b4 emy, andJ.Vouillon.Available from http://caml.inria.fr (1996 2008). S. Marlow \nandP.Wadler. Deforestation for higher-order functions. In John Launchbury andPatrick M. Sansom, editors, \nFunctional Programming, Workshops in Computing, pages 154 165. Springer, 1992. ISBN 3-540\u00ad19820-2. S. \nD. Marlow. Deforestation for Higher-Order Functional Programs. PhD thesis, Department of Computing Science, \nUniversity of Glasgow, April 27 1995. R. Milner, M. Tofte, R. Harper, and D. MacQueen. The De.nition \nof StandardML, Revised edition. MIT Press, 1997. N. MitchellandC. Runciman.Asupercompilerfor corehaskell.InO. \nChitil et al., editor, SelectedPapersfrom theProceedingsof IFL 2007, volume 5083 of Lecture Notes in \nComputer Science, pages 147 164. Springer-Verlag, 2008. P. Narendran and J. Stillman. On the Complexity \nof Homeomorphic Em\u00adbeddings. Technical Report 87-8, Computer Science Department, State UniveristyofNewYorkat \nAlbany, March 1987. J. Nordlander,M. Carlsson,A.Gill,P. Lindgren,andB.vonSydow. The Timber home page, \n2008. URL http://www.timber-lang.org. A. Ohori and I. Sasano. Lightweight fusion by .xed point promotion. \nIn POPL 07: Proceedings of the 34th annual ACM SIGPLAN-SIGACT symposium on Principles of programming \nlanguages, pages 143 154, NewYork,NY, USA, 2007.ACM. ISBN 1-59593-575-4. W. Partain. The no.b benchmark \nsuite of haskell programs. In John Launchbury andPatrick M. Sansom, editors, Functional Programming, \nWorkshops in Computing, pages 195 202. Springer, 1992. ISBN 3-540\u00ad19820-2. D. Sands. Proving the correctness \nof recursion-based automatic program transformations. Theoretical Computer Science, 167(1 2):193 233, \n30 October 1996. D. Sands. From SOS rules to proof principles: An operational metathe\u00adory forfunctional \nlanguages. In Proceedings of the 24th AnnualACM SIGPLAN-SIGACT Symposium on Principles of Programming \nLan\u00adguages (POPL).ACM Press, January 1997. J.P. Secher. Perfect supercompilation. Technical Report DIKU-TR-99/1, \nDepartment of Computer Science (DIKU), University of Copenhagen, February 1999. J.P. Secher and M.H. \nS\u00f8rensen. On perfect supercompilation. In D. Bj\u00f8rner, M. Broy, and A. Zamulin, editors, ProceedingsofPerspectivesof \nSystem Informatics, volume 1755 of Lecture Notes in Computer Science, pages 113 127. Springer-Verlag, \n2000.  M.H. S\u00f8rensen. Convergence of program transformers in the metric space of trees. Sci. Comput. \nProgram, 37(1-3):163 205, 2000. M.H. S\u00f8rensenandR.Gl\u00a8uck. An algorithm of generalization in positivesu\u00adpercompilation. \nIn J.W. Lloyd, editor, International Logic Programming Symposium, pages 465 479. Cambridge, MA: MIT Press, \n1995. M.H. S\u00f8rensen, R. Gl \u00a8uck, and N.D. Jones. Towards unifying partial evalu\u00adation, deforestation, \nsupercompilation, and GPC. In D. Sannella, editor, Programming Languages and Systems ESOP 94. 5th European \nSym\u00adposiumonProgramming, Edinburgh,U.K.,April1994(Lecture Notesin Computer Science, vol. 788), pages \n485 500. Berlin: Springer-Verlag, 1994. M.H. S\u00f8rensen,R. Gl\u00a8uck, and N.D. Jones. A positive supercompiler. \nJournal of Functional Programming, 6(6):811 838, 1996. J. Svenningsson. Shortcut fusion for accumulating \nparameters&#38; zip-like functions. In ICFP, pages 124 132, 2002. D. Syme. The F# programming language, \nJun 2008. URL http:// research.microsoft.com/fsharp.  A.Takano. Generalized partial computationforalazy \nfunctional language. In Partial Evaluation and Semantics-Based Program Manipulation, New Haven, Connecticut \n(Sigplan Notices, vol. 26, no. 9, September 1991), pages 1 11.NewYork:ACM, 1991. A.Takano and E. Meijer. \nShortcut deforestation in calculational form. In FPCA, pages 306 313, 1995. V.F.Turchin. Asupercompiler \nsystem based on the language Refal. SIG-PLAN Notices, 14(2):46 54, February 1979. V.F.Turchin. Semantic \nde.nitions in Refal and automatic production of compilers. In N.D. Jones, editor, Semantics-Directed \nCompiler Gener\u00adation, Aarhus, Denmark (Lecture Notes in Computer Science, vol. 94), pages 441 474. Berlin: \nSpringer-Verlag, 1980.  V.F. Turchin. Program transformation by supercompilation. In H. Ganzinger and \nN.D. Jones, editors, Programs as Data Objects, Copenhagen, Denmark, 1985 (Lecture Notes in Computer Science, \nvol. 217), pages 257 281. Berlin: Springer-Verlag, 1986a. V.F. Turchin. The concept of a supercompiler. \nACM Transactions on Programming Languages and Systems, 8(3):292 325, July 1986b.  V.F.Turchin. Refal-5,Programming \nGuide&#38;Reference Manual. Holyoke, MA: New England Publishing Co., 1989. P.Wadler. Deforestation: transforming \nprograms to eliminate trees. Theo\u00adretical Computer Science, 73(2):231 248, June 1990. ISSN 0304-3975. \n    \n\t\t\t", "proc_id": "1480881", "abstract": "<p>Previous deforestation and supercompilation algorithms may introduce accidental termination when applied to call-by-value programs. This hides looping bugs from the programmer, and changes the behavior of a program depending on whether it is optimized or not. We present a supercompilation algorithm for a higher-order call-by-value language and we prove that the algorithm both terminates and preserves termination properties. This algorithm utilizes strictness information for deciding whether to substitute or not and compares favorably with previous call-by-name transformations.</p>", "authors": [{"name": "Peter A. Jonsson", "author_profile_id": "81100618840", "affiliation": "Lule&#229; University of Technology, Lule&#229;, Sweden", "person_id": "P1300994", "email_address": "", "orcid_id": ""}, {"name": "Johan Nordlander", "author_profile_id": "81100299897", "affiliation": "Lule&#229; University of Technology, Lule&#229;, Sweden", "person_id": "P1300995", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1480881.1480916", "year": "2009", "article_id": "1480916", "conference": "POPL", "title": "Positive supercompilation for a higher order call-by-value language", "url": "http://dl.acm.org/citation.cfm?id=1480916"}