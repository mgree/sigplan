{"article_publication_date": "01-21-2009", "fulltext": "\n Proving That Non-Blocking Algorithms Don t Block Alexey Gotsman Byron Cook Matthew Parkinson Viktor \nVafeiadis University of Cambridge Microsoft Research University of Cambridge Microsoft Research Abstract \nA concurrent data-structure implementation is considered non\u00adblocking if it meets one of three following \nliveness criteria: wait\u00adfreedom, lock-freedom,or obstruction-freedom. Developers of non\u00adblocking algorithms \naim to meet these criteria. However, to date their proofs for non-trivial algorithms have been only manual \npencil-and-paper semi-formal proofs. This paper proposes the .rst fully automatic tool that allows developers \nto ensure that their algo\u00adrithms are indeed non-blocking. Our tool uses rely-guarantee rea\u00adsoning while \novercoming the technical challenge of sound reason\u00ading in the presence of interdependent liveness properties. \nCategories and Subject Descriptors D.2.4 [Software Engineer\u00ading]: Software/Program Veri.cation; F.3.1 \n[Logics and Meanings of Programs]: Specifying and Verifying and Reasoning about Pro\u00adgrams General Terms \nLanguages, Theory, Veri.cation Keywords Formal Veri.cation, Concurrent Programming, Live\u00adness, Termination \n1. Introduction Non-blocking synchronisation is a style of multithreaded program\u00adming that avoids the \nblocking inherent to lock-based mutual ex\u00adclusion. Instead, alternative synchronisation techniques are \nused, which aim to provide certain progress guarantees even if some threads are delayed for arbitrarily \nlong. These techniques are pri\u00admarily employed by concurrent implementations of data structures, such \nas stacks, queues, linked lists, and hash tables (see, for ex\u00adample, the java.util.concurrent library). \nNon-blocking data structures are generally much more complex than their lock-based counterparts, but \ncan provide better performance in the presence of high contention between threads [38]. An algorithm \nimplementing operations on a concurrent data structure is considered non-blocking if it meets one of \nthree com\u00admonly accepted liveness criteria that ensure termination of the op\u00aderations under various conditions: \nWait-freedom [15]: Every running thread is guaranteed to com\u00adplete its operation, regardless of the execution \nspeeds of the other threads. Wait-freedom ensures the absence of livelock and starvation. Permission \nto make digital or hard copies of all or part of this work for personal or classroom use is granted without \nfee provided that copies are not made or distributed for pro.t or commercial advantage and that copies \nbear this notice and the full citation on the .rst page. To copy otherwise, to republish, to post on \nservers or to redistribute to lists, requires prior speci.c permission and/or a fee. POPL 09, January \n18 24, 2009, Savannah, Georgia, USA. Copyright c &#38;#169; 2009 ACM 978-1-60558-379-2/09/01. . . $5.00 \nLock-freedom [23]: From any point in a program s execution, some thread is guaranteed to complete its \noperation. Lock\u00ad freedom ensures the absence of livelock, but not starvation. Obstruction-freedom [16]: \nEvery thread is guaranteed to com\u00adplete its operation provided it eventually executes in isolation. In \nother words, if at some point in a program s execution we suspend all threads except one, then this thread \ns operation will terminate. The design of a non-blocking algorithm largely depends on which of the above \nthree criteria it satis.es. Thus, algorithm developers aim to meet one of these criteria and correspondingly \nclassify the algorithms as wait-free, lock-free, or obstruction-free (e.g., [14, 16, 25]). To date, proofs \nof the liveness properties for non-trivial cases have been only manual pencil-and-paper semi-formal proofs. \nThis paper proposes the .rst fully automatic tool that allows developers to ensure that their algorithms \nare indeed non-blocking. Reasoning about concurrent programs is dif.cult because of the need to consider \nall possible interactions between concurrently exe\u00adcuting threads. This is especially true for non-blocking \nalgorithms, in which threads interact in subtle ways through dynamically\u00adallocated data structures. To \ncombat this dif.culty, we based our tool on rely-guarantee reasoning [18, 29], which considers every \nthread in isolation under some assumptions on its environment and thus avoids reasoning about thread \ninteractions directly. Much of rely-guarantee s power comes from cyclic proof rules for safety; straightforward \ngeneralisations of such proof rules to liveness prop\u00aderties are unsound [1]. Unfortunately, in our application, \nwe have to deal with interdependencies among liveness properties of threads in the program: validity \nof liveness properties of a thread can depend on liveness properties of another thread and vice versa. \nWe resolve this apparent circularity by showing that (at least for all of the al\u00adgorithms that we have \nexamined) proofs can found that layer non\u00adcircular liveness reasoning on top of weak circular reasoning \nabout safety. We propose a method for performing such proofs by repeat\u00adedly strengthening threads guarantees \nusing non-circular reason\u00ading until they imply the required liveness property (Section 2). We develop \na logic that allows us to easily express these layered proofs for heap-manipulating programs (Sections \n3 and 4) and prove it sound with respect to an interleaving semantics (Section 6). In addition, we have \nfound that the rely and guarantee condi\u00adtions needed for proving algorithms non-blocking can be of a \nre\u00adstricted form: they need only require that certain events do not hap\u00adpen in.nitely often. This allows \nus to automate proving the liveness properties by a procedure that systematically searches for proofs \nin our logic with relies and guarantees of this form (Section 5). Using our tool, we have automatically \nproved a number of the published algorithms to be formally non-blocking, including chal\u00adlenging examples \nsuch as the HSY stack [14] and Michael s linked list algorithm [25]. Proofs for some of the veri.ed algorithms \nre\u00adquire complex termination arguments and supporting safety prop\u00aderties that are best constructed by \nautomatic tools.  2. Informal development We start by informally describing our method for verifying \nliveness properties and surveying the main results of the paper. Example. Figure 1 contains a simple \nnon-blocking implemen\u00adtation of a concurrent stack due to Treiber [33], written in a C\u00adlike language. \nA client using the implementation can call several push or pop operations concurrently. To ensure the \ncorrectness of the algorithm, we assume that it is executed in the presence of a garbage collector (see \n[17, Section 10.6] for justi.cation). We also assume that single word reads and writes are executed atomically. \nThe stack is stored as a linked list, and is updated by compare-and\u00adswap (CAS) instructions. CAS takes \nthree arguments: a memory address, an expected value and a new value. It atomically reads the memory \naddress and updates it with the new value when the ad\u00address contains the expected value; otherwise, it \ndoes nothing. In C syntax this might be written as follows: int CAS(WORD *addr, WORD v1, WORD v2) { atomic \n{ if (*addr == v1) { *addr = v2; return 1; } else { return 0; } } } In most architectures an ef.cient \nCAS (or an equivalent operation) is provided natively by the processor. The operations on the stack are \nimplemented as follows. The function init initialises the data structure. The push operation (i) allocates \na new node x;(ii) reads the current value of the top-of-the\u00adstack pointer S;(iii) makes the next .eld \nof the newly created node point to the read value of S;and (iv) atomically updates the top-of\u00adthe-stack \npointer with the new value x. If the pointer has changed between (ii)and (iv) and has not been restored \nto its initial value, the CAS fails and the operation is restarted. The pop operation is implemented \nin a similar way. Liveness properties of non-blocking algorithms. Notice that a push or pop operation \nof Treiber s stack may not terminate if other threads are continually modifying S: in this case the CAS \ninstruction may always fail, which will cause the operation to restart continually. Thus, the algorithm \nis not wait-free. However, it is lock-free: if push and pop operations execute concurrently, some operation \nwill always terminate. We note that an additional requirement in the de.nitions of the liveness properties \ngiven in Section 1 is that the properties have to be satis.ed under any scheduler, including an unfair \none that suspends some threads and never resumes them again: in this case the remaining threads still \nhave to satisfy the liveness properties. The properties form a hierarchy [10]: if an algorithm is wait-free, \nit is also lock-free, and if it is lock-free, it is also obstruction\u00adfree. Note also that even the weakest \nproperty, obstruction-freedom, prevents the use of spinlocks, because if a thread has acquired a lock \nand is then suspended, another thread may loop forever trying to acquire that lock. We .rst describe \nour approach for verifying lock-freedom. Reducing lock-freedom to termination. We show that the check\u00ading \nof lock-freedom can be reduced to the checking of termina\u00adtion in the spirit of [36]. Consider a non-blocking \ndata structure with operations op1,...,opn .Let op be the command that non\u00addeterministically executes \none of the operations on the data struc\u00adture with arbitrary parameters: op = if (nondet()) op1; else \nif (nondet()) op2; ... else opn; (2.1) struct Node { void init() { value_t data; S = NULL; Node *next; \n} }; Node *S; value_t pop() { Node *t, *x; void push(value_t v) { do { Node*t,*x; t=S; x = new Node(); \nif (t == NULL) { x->data = v; return EMPTY; do{ } t= S; x = t->next; x->next = t; } while(!CAS(&#38;S,t,x)); \n} while(!CAS(&#38;S,t,x)); return t->data; }} Figure 1. Treiber s non-blocking stack We denote non-deterministic \nchoice with nondet(). The de.nition of lock-freedom of the data structure requires that for all min any \n(in.nite) execution of the data structure s most general client C(m) de.ned below, some operation returns \nin.nitely often: m  C(m)= while (true) { op } i=1 We now show that this is the case if and only if for \nall k the following program C'(k) terminates: k C'(k)= op (2.2) i=1 The proof in the only if direction \nis by contrapositive: a non\u00adterminating execution of C'(k) can be straightforwardly mapped to an execution \nof C(k) violating lock-freedom in which the while loops make at most one iteration executing the same \noperations with the same parameters as in C'(k). For the if direction note that any in.nite execution \nof C(m) violating lock-freedom has only .nitely many (say, k) operations started: those that complete \nsuccessfully, those that are suspended by the scheduler and never resumed again, and those that do not \nterminate. Such an execution can then be mapped to a non-terminating execution of C'(k),in which the \noperations are completed, suspended or non-terminating as above. Thus, to check lock-freedom of an algorithm, \nwe have to check the termination of an arbitrary number of its operations running in parallel. Rely-guarantee \nreasoning and interference speci.cations. We prove termination of the program C'(k) using rely-guarantee \nrea\u00adsoning [18, 29]. Rely-guarantee avoids direct reasoning about all possible thread interactions in \na concurrent program by specifying a relation (the guarantee condition) for every thread restricting \nhow it can change the program state. For any given thread, the union of the guarantee conditions of all \nthe other threads in the program (its rely condition) restricts how those threads can interfere with \nit, and hence, allows reasoning about this thread in isolation. The logic we develop in this paper uses \na variant of rely\u00adguarantee reasoning proposed in RGSep [35] a logic for reason\u00ading about safety properties \nof concurrent heap-manipulating pro\u00adgrams, which combines rely-guarantee reasoning with separation logic. \nRGSep partitions the program heap into several thread-local parts (each of which can only be accessed \nby a given thread) and the shared part (which can be accessed by all threads). The partition\u00ading is de.ned \nby proofs in the logic: an assertion in the code of a thread restricts its local state and the shared \nstate. Additionally, the partitioning is dynamic, meaning that we can use ownership trans\u00adfer to move \nsome part of the local state into the shared state and vice versa. Rely and guarantee conditions are \nthen speci.ed with sets of actions, which are relations on the shared state determin\u00ading how the threads \nchange it. This is in contrast with the original rely-guarantee method, in which rely and guarantee conditions \nare relations on the whole program state. Thus, while reasoning about a thread, we do not have to consider \nlocal states of other threads. For example, using RGSep we can prove memory safety (no invalid pointer \ndereferences) and data structure consistency (the linked list is well-formed) of Treiber s stack [34]. \nThe proof con\u00adsiders the linked list with the head pointed to by the variable S to be in the shared state. \nWhen a push operation allocates a new node x, it is initially in its local state. The node is transferred \nto the shared state once it is linked into the list with a successful CAS instruction. The proof speci.es \ninterference between threads in the shared state with three actions, Push, Pop,and Id, with the following \ninformal meaning: Push corresponds to pushing an element onto the stack (a successful CAS in push); Pop \nto removing an element from the stack (a successful CAS in pop); and Id represents the identity ac\u00adtion \nthat does not change the shared state (a failed CAS and all the other commands in the code of the threads). \nProving lock-freedom. Using the splitting of the heap into local and shared parts and the interference \nspeci.cation for Treiber s stack described above, we can establish its lock-freedom as follows. As we \nshowed above, it is suf.cient to prove termination of a .xed but arbitrary number of threads each executing \na single push or pop operation with an arbitrary parameter. The informal proof of this (formalised in \nSection 4) is as follows: I. No thread executes Push or Pop actions in.nitely often. This is because \na Push or Pop action corresponds to a success\u00adful CAS, and once a CAS succeeds, the corresponding while \nloop terminates. II. The while loop in an operation terminates if no other thread executes Push or Pop \nactions in.nitely often. This is because the operation does not terminate only when its CAS always fails, \nwhich requires the environment to execute Push or Pop actions in.nitely often. Hence, every thread terminates. \nThe above proof uses rely-guarantee reasoning: it consists of proving several thread-local judgements, \neach of which establishes a property of a thread under an assumption about the interfer\u00adence from the \nenvironment. Properties of a parallel composition of threads are then derived from the thread-local judgements. \nThis is done by .rst establishing the guarantee provided by Statement I and then using it to prove termination \nof the operations. This pattern establishing initial guarantees and then deriving new guarantees from \nthem is typical for proofs of lock-freedom. We now con\u00adsider a more complicated example in which the \nproof consists of more stepsofthisform. Hendler, Shavit, and Yerushalmi [14] have presented an im\u00adproved \nversion of Treiber s stack that performs better in the case of higher contention between threads. Figure \n2 shows an adapted and abridged version of their algorithm. The implementation combines two algorithms: \nTreiber s stack and a so-called elimination scheme (partially elided). A push or a pop operation .rst \ntries to modify the stack as in Treiber s algorithm, by doing a CAS to change the shared top-of-the-stack \npointer. If the CAS is successful then the operation terminates. If the CAS fails (because of interference \nfrom another thread), the operation backs off to the elimination scheme. If this scheme fails, the whole \noperation is restarted. The elimination scheme works on data structures that are sepa\u00adrate from the list \nimplementing the stack. The idea behind it is that two contending push and pop operations can eliminate \neach other without modifying the stack if pop returns the value that push is trying to insert. An operation \ndetermines the existence of another operation it could eliminate itself with by selecting a random slot \npos in the collision array, and atomically reading that slot and overwriting it with its thread identi.er \nMYID. The identi.er of an\u00adother thread read from the array can be subsequently used to per\u00adform elimination. \nThe corresponding code does not affect the lock\u00adfreedom of the algorithm and is therefore elided in Figure \n2. The algorithm implements the atomic read-and-write operation on the collision array in a lock-free \nfashion using CAS1. This illus\u00adtrates a common pattern, when one lock-free data structure is used inside \nanother. An RGSep safety proof of the HSY stack would consider the data structures of the elimination \nscheme shared and describe in\u00adterference on the shared state using the actions introduced for Treiber \ns stack and two additional actions: Xchg (which denotes the effect of the successful operation on the \ncollision array de\u00adscribed above) and Others (which includes all the operations on the other data structures \nof the elimination scheme). Given this in\u00adterference speci.cation, the informal proof of lock-freedom \nof the algorithm is as follows: in a parallel composition of several threads each executing one push \nor pop operation, I. No thread executes Push or Pop actions in.nitely often. II. push and pop do not \nexecute the Xchg action in.nitely often if no other thread executes Push or Pop actions in.nitely often. \nThis is because a thread can only execute Xchg in.nitely often if its outer while loop does not terminate. \nThis can only happen if some other thread executes Push or Pop in.nitely often. III. push and pop terminate \nif no other thread executes Push, Pop,or Xchg actions in.nitely often. This is because in this case both \ninner and outer while loops eventually terminate. From Statements I and II, we get that no thread executes \nPush, Pop,or Xchg actions in.nitely often. Hence, by Statement III every thread terminates. The above \nproof is done in a layered style, i.e., starting from the weak guarantee provided by Statement I and \nstrengthening it using already established guarantees until it implies termination. This is informally \nillustrated in Figure 3 for the case of two operations (op1 and op2) running in parallel. The validity \nof the property of Thread 1 in the middle layer depends on the validity of the counterpart property of \nThread 2 and vice versa. However, it is unsound to remove the upper layer of Figure 3 and justify the \nguarantee in the middle layer by circular reasoning, i.e., by observing that a thread satis.es the guarantee \nif the other thread does. We have found that the proof method described above was applicable in all of \nthe examples of lock-free algorithms that we have considered. In the next two sections we develop a logic \nfor formalising proofs following the method. Automating lock-freedom proofs. The above informal proofs \nof lock-freedom use guarantee conditions of a restricted form that speci.es two sets of actions: those \nthat a thread can execute and those that it cannot execute in.nitely often. We have found that guarantee \nconditions of this form were suf.cient to prove lock\u00adfreedom for all the examples we considered. This \nobservation al\u00adlows us to automate proving lock-freedom of an algorithm by sys\u00adtematically searching \nfor termination proofs for a program consist\u00ad 1 Such an operation could be implemented with an atomic \nexchange instruc\u00adtion. The reason for implementing it with CAS is that in some architectures the atomic \nexchange instruction is either not available or slow. struct Node { value_t data; Node *next; }; Node \n*S; int collision[SIZE]; void push(value_t v) { Node *t, *x; x = new Node(); x->data = v; while (1) \n{ t=S; x->next = t; if (CAS(&#38;S,t,x)) { return; } // Elimination scheme // ... int pos = GetPosition(); \n// 0 = pos = SIZE-1 int hisId = collision[pos]; while (!CAS(&#38;collision[pos],hisId,MYID)) { hisId \n= collision[pos]; } // ... } } value_t pop() { Node *t, *x; while (1) { t=S; if (t == NULL) { return \nEMPTY; } x = t->next; if (CAS(&#38;S,t,x)) { return t->data; } // Elimination scheme // ... int pos \n= GetPosition(); // 0 = pos = SIZE-1 int hisId = collision[pos]; while (!CAS(&#38;collision[pos],hisId,MYID)) \n{ hisId = collision[pos]; } // ... } } Figure 2. The HSY non-blocking stack op1 op2 Thread 1 does not \n  Thread 2 does not execute Push or Pop in.nitely often _ execute Push or Pop in.nitely often _ Thread \n1 does not  Thread 2 does not execute Push, Pop,or execute Push, Pop,or Xchg in.nitely often Xchg \nin.nitely often  Thread 1 terminatesThread 2 terminates Figure 3. An informal proof argument where \nan arrow from state\u00adment A to statement B means that A is used as a rely condition while establishing \nthe guarantee B. ing of an arbitrary number of the algorithm s operations running in parallel: we search \nfor proofs that follow the pattern described above and use rely and guarantee conditions of the restricted \nform. Our proof search procedure performs a forward search, construct\u00ading proof graphs like the one in \nFigure 3 top-down. It is able to con\u00adstruct proofs that the programs C ' (k) terminate for all k at once, \nbecause our guarantee conditions are such that if several threads satisfy a guarantee, then so does their \nparallel composition. We now informally describe the procedure using the HSY stack as the run\u00adning example \n(the details are provided in Section 5). Consider a program consisting of an arbitrary number of the \nal\u00adgorithm s operations running in parallel. First, using existing tools for verifying safety properties \nof non-blocking algorithms [6], we can infer a splitting of the program state into local and shared parts \nand a set of actions describing how the operations change the shared state ({Push, Pop, Xchg, Others, \nId} for the HSY stack). The set de.nes the initial guarantee provided by every operation in the program \nthat ensures that the operation changes the shared state only according to one of the actions. Note that \nif several opera\u00adtions satisfy this guarantee, then so does their parallel composition. Hence, while \nchecking a property of an operation in the program, we can rely on its environment satisfying the guarantee. \nThe guar\u00adantee, however, is too weak to establish termination of the oper\u00adations. We therefore try to \nstrengthen it by considering every ac\u00adtion in turn and attempting to prove that no operation executes \nthe action in.nitely often in an environment satisfying the guarantee. In our running example, we will \nbe able to establish that the op\u00aderations do not execute the actions Push and Pop in.nitely often (but \nnot Xchg and Others). Again, if several operations satisfy the guarantee strengthened in this way, then \nso does their parallel com\u00adposition. Hence, we can check properties of the operations in the program \nassuming that their environment satis.es the guarantee. An attempt to prove their termination in this \nway fails again, and we have to strengthen the guarantee one more time. Namely, we try to prove that \nthe operations do not execute the remaining ac\u00adtions Xchg and Others in.nitely often. In this case, the \navailable guarantee is strong enough to prove that the Xchg action is not exe\u00adcuted in.nitely often. \nFinally, the strengthened guarantee allows us to establish termination of the operations. Proving obstruction-freedom \nand wait-freedom. Obstruction\u00adfreedom of an operation ensures its termination in an environ\u00adment that \neventually stops executing. Therefore, when proving obstruction-freedom, in.nite behaviours of the environment \nare ir\u00adrelevant and the necessary environment guarantee can always be Values = {...,-1,0,1,...} Locs \n= {1,2,...}Vars = {x,y,...,&#38;x,&#38;y,...} Stores = Vars .Values Heaps = Locs -.n Values S=Stores \n\u00d7Heaps Figure 4. Program states S represented by a safety property. For example, the operations of Treiber \ns stack guarantee that they modify the shared state accord\u00ading to one of the actions Push, Pop,and Id. \nTo prove obstruction\u00adfreedom of a push or pop operation2, it is thus suf.cient to prove its termination \nin an environment that executes only .nitely many such actions. This is true because, as in the proof \nof lock-freedom, in such an environment the CAS in the code of the operation will eventually succeed. \nIn general, we can automatically check obstruction-freedom by checking termination of every operation \nin the environment satisfying the safety guarantee inferred by the safety veri.cation tool and the additional \nassumption that it exe\u00adcutes only .nitely many actions. We describe this in more detail in Section 5. \nTo the best of our knowledge, loops in all practical wait-free non-blocking algorithms have constant \nupper bounds on the num\u00adber of their iterations. For example, Simpson s algorithm [32] con\u00adsists of straight-line \ncode only, and the number of loop iterations in the wait-free find operation of a non-blocking linked \nlist [37] is bounded by the number of distinct keys that can be stored in the list. For this reason, \ntermination of operations in wait-free algorithms can be justi.ed by considering only safety properties \nguaranteed by operations environment. The automatic check for wait-freedom is similar to the one for \nobstruction-freedom (see Section 5).  3. Specifying liveness properties Our logic for reasoning about \nliveness properties is a Hoare-style logic, which combines ideas from rely-guarantee reasoning and separation \nlogic. It generalises a recent logic for reasoning about safety properties of non-blocking algorithms, \nRGSep [35]. As any Hoare logic, ours consists of two formal systems: an assertion language and a proof \nsystem for Hoare triples. In this section, we describe the assertion language, de.ne the form of judgements \nin our logic, and show how to specify wait-freedom, lock-freedom, and obstruction-freedom in it. The \nnext section presents the logic s proof system. Programming language. We consider heap-manipulating pro\u00adgrams \nwritten in the following simple programming language. Commands C are given by the grammar C ::= C1; C2 \n|if (e) C1 else C2 |while (e) C |x = new() |x= e|x=*e1 |*e1 = e2 |delete e|atomic C where e ranges over \narithmetic expressions, including non\u00addeterministic choice nondet(). The command atomic C exe\u00adcutes C \nin one indivisible step. Programs consist of initialisa\u00adtion code followed by a top-level parallel composition \nof threads: C0;(C1d...dCn). To avoid side conditions in our proof rules, we treat each pro\u00adgram variable \nx as a memory cell at the constant address &#38;x. Thus, any use of x in the code is just a shorthand \nfor *(&#38;x). Similarly, we interpret .eld expressions x->next as *(x + offset of next).In our examples, \nwe also use other pieces of C syntax. Assertion language. Let p, qand rbe separation logic assertions: \n.e2 |p*q |false |p.q |.x.p|... p,q,r ::= emp |e1 2 This example is used here for illustrative purposes \nonly: obstruction\u00adfreedom of Treiber s stack follows from its lock-freedom. Separation logic assertions \ndenote sets of program states S repre\u00adsented by store-heap pairs (Figure 4). A store is a function from \nvariables to values; a heap is a .nite partial function from locations to values. We omit the standard \nformal semantics for most of the as\u00adsertions [30]. Informally, emp describes the states where the heap \nis empty; e1 .e2 describes the states where the heap contains a single allocated location at the address \ne1 with contents e2; p*qde\u00adscribes the states where the heap is the union of two disjoint heaps, one \nsatisfying pand the other satisfying q. The formal semantics of the assertion p*q is de.ned using a partial \noperation \u00b7on S such that for all (t1,h1),(t2,h2) .S (t1,h1) \u00b7(t2,h2)=(t1,h1 lh2) if t1 = t2 and h1 lh2 \nis de.ned, and (t1,h1)\u00b7(t2,h2) is unde.ned otherwise. Then u.[[p*q]] ..u1,u2.u= u1 \u00b7u2 .u1 .[[p]] .u2 \n.[[q]] As we argued in Section 2, while reasoning about concurrent heap-manipulating programs it is useful \nto partition the program state into thread-local and shared parts. Therefore, assertions in our logic \ndenote sets of pairs of states from S. The two components represent the state local to the thread in \nwhose code the assertion is located and the shared state. We use the assertion language of RGSep [35], \nwhich describes the local and shared components with separation logic assertions and is de.ned by following \ngrammar: P,Q::= p|p |P *Q|true |false |P .Q|P .Q|.x.P An assertion p denotes the local-shared state pairs \nwith the local state satisfying p; p the pairs with the shared state satisfying p and the local state \nsatisfying emp; P *Q the pairs in which the local state can be divided into two substates such that one \nof them together with the shared state satis.es P and the other together with the shared state satis.es \nQ. The formal semantics of P *Qis de.ned using a partial operation *on S2 such that for all (l1,s1),(l2,s2) \n.S2 (l1,s1) *(l2,s2)=(l1 \u00b7l2,s1) if s1 = s2 and l1 \u00b7l2 is de.ned, and (l1,s1) *(l2,s2) is unde.ned otherwise. \nThus, s .[[P *Q]] ..s1,s2.s = s1 *s2 .s1 .[[P]] .s2 .[[Q]] Note that by abuse of notation we denote the \nconnectives inter\u00adpreted by \u00b7and *with the same symbol *. It should be always clear from the structure \nof the assertion which of the two connectives is being used. We denote global states from S with small \nLatin let\u00adters (u, l, s), and local-shared state pairs from S2 with small Greek letters (s). Judgements. \nThe judgements in our logic include rely and guar\u00adantee conditions determining how a command or its environment \nchange the shared state. These represent languages of .nite and in.nite words over the alphabet S2 of \nrelations on shared states and are denoted with capital calligraphic letters (R,G,...). A word in any \nof the languages describes the sequences of changes to the shared state. Thus, relies and guarantees \ncan de.ne liveness proper\u00adties. This generalises the RGSep logic, in which rely and guarantee conditions \nde.ne safety properties and are therefore represented with relations on the shared state. Our proof system \nhas two kinds of judgements: R,Gf {P}C {Q}: if the initial state satis.es P and the environment changes \nthe shared state according to R,then the program is safe (i.e., it does not dereference any invalid pointers), \nchanges the shared state according to G, and the .nal state (if the program terminates) satis.es Q. R,(G1,G2) \nf{P}C1dC2 {Q}: if the initial state satis.es P and the environment changes the shared state according \nto R, then the program C1dC2 is safe, C1 changes the shared state according to G1, C2 changes the shared \nstate according to G2, and the .nal state (if both threads terminate) satis.es Q.Dis\u00adtinguishing the \nguarantee of each thread is crucial for liveness proofs done according to the method described in Section \n2. The informal de.nition of judgements validity given above as\u00adsumes a semantics of programs that distinguishes \nbetween the lo\u00adcal and the shared state. We sketch how such a semantics can be de.ned later in this section. \nIn Section 6 we give formal de.nitions of the semantics and the notion of validity, and relate them to \nthe standard ones, which consider the program state as a whole. Specifying rely and guarantee conditions. \nAs noted above, a rely or a guarantee condition de.nes sequences of atomic changes to the shared state. \nWe describe each such change with the aid of actions [35] of the form p.q,where pand q are separation \nlogic assertions. Informally, this action changes the part of the shared state that satis.es pinto one \nthat satis.es q, while leaving the rest of the shared state unchanged. Formally, its meaning is a binary \nrelation on shared states: [[p.q]] = {(s1 \u00b7s0,s2 \u00b7s0) |s1 .[p] .s2 .[q]} It relates some initial state \ns1 satisfying the precondition p to a .nal state s2 satisfying the postcondition q. In addition, there \nmay be some disjoint state s0 that is not affected by the action. For example, we can de.ne the three \nactions used in the proof of lock-freedom of Treiber s stack mentioned in Section 2 as follows: &#38;S.y \n. &#38;S.x*x.Node(v,y) (Push) &#38;S.x*x.Node(v,y) . &#38;S.y*x.Node(v,y) (Pop) emp . emp (Id) Here x.Node(v,y) \nis a shortcut for x.v *(x+1).y. Recall that the algorithm is assumed to be executed in the presence of \na garbage collector. Hence, the node removed from the list by Pop stays in the shared state as garbage. \nIn our examples, we syntactically de.ne rely and guarantee con\u00additions using linear temporal logic (LTL) \nwith actions as atomic propositions. Let False and True be the actions denoting the re\u00adlations \u00d8and S2 \nrespectively. We denote temporal operators al\u00adways , eventually , and next with o, 0,and 0, respectively. \nTheir semantics on in.nite words is standard [22]. The semantics on .nite words is de.ned as follows \n[20]: for m=0 d1 ...dm |= o. ..i. 1 =i=m.di ...dm |=. d1 ...dm |= 0. ..i. 1 =i=m .di ...dm |=. d1 ...dm \n|= 0. .m=2 .d2 ...dm |=. Note that here 0 is the weak next operator: it is true if there is no next state \nto interpret its argument over. For example, oR, where R .S2, denotes the language of words in which \nevery letter is from R(including the empty word), \u00aco0Rdenotes words which contain only .nitely many letters \nfrom R(including all .nite words), and 00False denotes exactly all .nite words. We specify termination \nof a command by requiring that it satisfy the guarantee 00False, i.e., we interpret termination as the \nabsence of in.nite computations. This is adequate for programs in the language intro\u00adduced above, since \nthey cannot deadlock. The semantics of triples in our logic makes no assumptions about the scheduler. \nIn particular, it can be unfair with respect to the command in the triple: the guarantee condition includes \nwords corresponding to the command being suspended and never executed again. For this reason, all rely \nand guarantee conditions in this paper are pre.x-closed, i.e., for any word belonging to a rely or a \nguarantee all its pre.xes also belong to it. Additionally, we require that relies and guarantees represent \nnonempty languages. Splitting states into local and shared parts. We now informally describe the split-state \nsemantics of programs that splits the pro\u00adgram state into local and shared parts (formalised in Section \n6). We partition all the atomic commands in the program into those that access only local state of the \nthread they are executed by and those that can additionally access the shared state. By convention, the \nonly commands of the latter kind are atomic blocks. For example, in the operations of Treiber s stack, \nall the commands except for CASes access only the local state. Further, we annotate each atomic block \nwith an action p . q determining how it treats the shared state, written atomicp.q C. These annotations \nare a part of proofs in our logic. For the logic to be sound, all the judgements used in a proof of a \nprogram have to agree on the treatment of the shared state. We therefore require that the same annotation \nbe used for any .xed atomic block throughout the proof. In the split-state semantics the command atomicp.q \nC exe\u00adcutes as follows: it combines the local state of the thread it is exe\u00adcuted by and the part of \nthe shared state satisfying p, and runs C on this combination. It then splits the single resulting state \ninto local and shared parts, determining the shared part as the one that satis\u00ad.es q. The new shared \nstate is thus this part together with the part of the shared state untouched by C. For the splittings \nto be de.ned uniquely (and for our logic to be sound), we require that pand qin all annotations be precise \nassertions [28]. An assertion r is precise if for any state uthere exists at most one substate satr(u) \nsatisfy\u00ading r: u = satr(u) \u00b7restr(u) for some restr(u). The assertions in all the actions used in this \npaper are precise. Let CASA(addr,v1,v2) be de.ned as follows: if (nondet()) atomicA { assume(*addr == \nv1); *addr = v2; return 1; } else atomicId { assume (*addr != v1); return 0; } where the assume command \nacts as a .lter on the state space of programs e is assumed to evaluate to 1 after assume(e) is ex\u00adecuted. \nThe above de.nition of CAS is semantically equivalent to the de.nition in Section 2, but allows different \naction anno\u00adtations for the successful and the failure cases. We annotate the CAS commands in the push \nand pop operations of Treiber s stack as CASPush(&#38;S,t,x) and CASPop(&#38;S,t,x), respectively. Similarly, \nwe annotate the CAS in the inner loop of the HSY stack as CASXchg (&#38;collision[pos],hisId,MYID). Specifying \nwait-freedom, lock-freedom, and obstruction-freedom. A non-blocking data structure is given by an initialisation \nroutine init and operations op1,...,opn on the data structure. We require that the initialisation routine \nsatisfy the triple {} {} oId,oTrue fempinit Inv for some data structure invariant Inv restricting only \nthe shared state: the routine creates an instance of the data structure in the shared state when run \nin isolation (i.e., in the environment that does not change the shared state). For Treiber s stack an \ninvariant maintained by all the operations on the data structure is that S points to the head of a linked \nlist. We can express this in our assertion language using an inductive predicate assertion lseg(x,y) \nof separation logic that represents the least predicate satisfying lseg(x,y) .(x= y.emp) .(.z.x = y.x.Node( \n,z) *lseg(z,y)) Thus, lseg(x, NULL)represents all of the states in which the heap has the shape of a \n(possibly empty) linked list starting from location x and ending with NULL. The invariant can then be \nexpressed as Inv = .x. &#38;S .x *lseg(x, NULL)*true (3.1) In our logic, we can express the liveness \nproperties of non\u00adblocking algorithms we introduced before as follows. Wait\u00adfreedom of an operation opi \nis captured by the triple R, 00False f{Inv}opi {true} (3.2) which ensures termination of the operation \nunder the interfer\u00adence from the environment allowed by the rely condition R. Obstruction-freedom of \nan operation opi can be expressed as R.00False, 00False f{Inv}opi {true} (3.3) Here R describes the allowed \ninterference from the operation s environment, and the conjunct 00False ensures that eventually all the \nthreads in the environment will be suspended. As we showed in Section 2, lock-freedom can be reduced \nto proving termination of several operations run in isolation, which is ensured by the validity of the \ntriples oId, 00False f{Inv}C ' (k){true} (3.4) for all k, where the program C ' (k)is de.ned by (2.2). \nNote that obstruction-freedom and wait-freedom are directly compositional properties and can thus be \nspeci.ed for every op\u00aderation separately. The speci.cation of lock-freedom considers all operations at \nonce, however, as we show in the next section, we can still reason about lock-freedom in a compositional \nway. 4. Compositional proof system for liveness and heaps To reason about judgements of the form introduced \nin the previ\u00adous section we need (i) a method for proving thread-local triples (i.e., those giving a \nspeci.cation to a single thread) and (ii) a proof system for combining thread-local triples into triples \nabout paral\u00adlel compositions. We describe an automatic method for proving thread-local triples in Section \n5 (the THREADLOCAL procedure and Figure 7). In this section, we present the second component a compositional \nproof system for reasoning about liveness properties of heap-manipulating programs, shown in Figure 5. \nWe explain the proof rules by example of formalising the informal proofs of lock\u00adfreedom from Section \n2. In Section 5 we show how to construct such proofs automatically, and in Section 6 we prove the proof \nrules sound with respect to an interleaving semantics. We .rst introduce two operations on languages \nused by the rules. Let L(A)denote the language of all .nite and in.nite words over an alphabet A. We \ndenote the concatenation of a .nite word a .L(A)and a word (either .nite or in.nite) \u00df .L(A)with a\u00df. \nThe safety closure CL(G)of a language G.L(A)is the smallest language de.ning a safety property that contains \nG [2]. In the setting of this paper, where all the languages considered are pre.x-closed, CL(G)can be \nde.ned as the set of words a such that every pre.x of a is in G: CL(G)={a .L(A)|.\u00df,.. a =\u00df. . \u00df .G} For \ntwo words a, \u00df .L(A), we denote the set of their fair interleavings with ad\u00df (we omit the standard de.nition \n[4]). We lift this to languages G1, G2 .L(A)as follows:  G1dG2 = {ad\u00df |a .G1 .\u00df .G2} 4.1 Proving lock-freedom \nof Treiber s non-blocking stack We start by proving termination of any two operations with arbi\u00adtrary \nparameters (which we denote with opi1 and opi2) running in RdCL(G2), G1 f{P1}C1 {Q1} RdCL(G1), G2 f{P2}C2 \n{Q2} PAR-C R, (G1, G2)f{P1 *P2}C1dC2 {Q1 *Q2} R, Gf{P }C {Q} P .P R .R G.G.Q .Q ' CONSEQ R ' , G.' \nf{P ' }C {Q ' } R, (G1, G2)f{P1 *P2}C1dC2 {true} RdG2, G1 ' f{P1}C1 {Q1} RdG1, G2 ' f{P2}C2 {Q2} '' \nPAR-NC R, (G1, G2)f{P1 *P2}C1dC2 {Q1 *Q2} R, (G1, G2)f{P }C1dC2 {Q} PAR-MERGE R, G1dG2 f{P }C1dC2 {Q} \n R, G f{P }C {Q ' }'' .'' '' }C {Q '' R , G f{P } CONJ R ' nR '' , G.' nG.'' f{P ' .P '' }C {Q ' .Q '' \n} Figure 5. Proof rules for reasoning about liveness properties of heap-manipulating programs. G.denotes \neither G or (G1, G2)de\u00adpending on whether the triple distinguishes between the guarantees provided by \nthe different threads. In the latter case operations on (G1, G2)are done componentwise. parallel and \nconsider the general case later. To prove this, we have to derive the triple oId, 00False f{Inv}opi1dopi2 \n{true} (4.1) for the data structure invariant Inv de.ned by (3.1). Statement I. Formally, the statement \nsays that every thread has to satisfy the guarantee G=o(Push .Pop .Id).\u00aco0(Push .Pop) where the actions \nPush, Pop,and Id are de.ned in Section 3. The .rst conjunct speci.es the actions that the thread can \nexecute, and the second ensures that it cannot execute the actions Push and Pop in.nitely often. In order \nto establish this guarantee, we do not have to make any liveness assumptions on the behaviour of other \nthreads; just knowing the actions they can execute (Push, Pop,and Id)is enough. We therefore use the \nrule PAR-C to establish G.Itis a circular rely guarantee rule [1] adapted for reasoning about heaps. \nIt allows two threads to establish their guarantees simultaneously, while relying on the safety closure \nof the other thread s guarantee that is being established. Note that without the safety closure the circular \nrules like this are unsound for liveness properties [1]. Note also that pre-and postconditions of threads \nin the premises of the rule are *-conjoined in the conclusion: according to the semantics of the assertion \nlanguage, this takes the disjoint composition of the local states of the threads and enforces that the \nthreads have the same view of the shared state. It is this feature of our proof rules that allows us \nto reason modularly in the presence of heap. Applying PAR-C with G1 =G2 =Gand R=CL(G),we get: CL(G)dCL(G), \nGf{Inv}opi1 {true}CL(G)dCL(G), Gf{Inv}opi2 {true} (4.2) CL(G), (G, G)f{Inv *Inv}opi1dopi2 {true *true} \nTaking the safety closure of Gremoves the second conjunct repre\u00adsenting the liveness part of G: CL(G)=o(Push \n.Pop .Id) Additionally, CL(G)dCL(G)=CL(G), so that the premises sim\u00adplifytotriples CL(G),Gf{Inv}opj {true},j \n.{i1,i2} (4.3) which ensure that the thread does not execute Push and Pop actions in.nitely often, provided \nthe environment executes only actions Push, Pop,and Id. We show how to discharge such triples in Section \n5. Their proof would formalise the informal justi.cation of Statement I given in Section 2 and would \nuse the annotations at atomic blocks introduced in Section 3 to determine the splitting of states into \nlocal and shared parts. Since Inv restricts only the shared state, Inv *Inv .Inv, hence, the conclusion \nof (4.2) is equivalent to CL(G),(G,G)f{Inv}opi1dopi2 {true} (4.4) Since G.CL(G), we can then apply a \nvariation on the rule of con\u00adsequence of Hoare logic, CONSEQ, which allows us to strengthen the rely \ncondition to G: G,(G,G)f{Inv}opi1dopi2 {true} (4.5) Statement II. Termination is captured by the guarantee \n00False, which says that eventually the program does not execute any tran\u00adsitions. To prove this guarantee, \nwe use the non-circular rely\u00adguarantee rule PAR-NC, which allows the .rst thread to replace its guarantee \nwith a new one based on the already established guarantee of the other thread, and vice versa. Note that \nthe .rst premise need only establish the postcondition true, since the post\u00adcondition Q1 *Q2 of the conclusion \nis implied by the other two premises. Applying PAR-NC with R = G1 = G2 = G and G1 ' =G2 ' =00False,we \nget: G,(G,G)f{Inv}opi1dopi2 {true} GdG,00False f{Inv}opi1 {true} (4.6) GdG,00False f{Inv}opi2 {true} \nG,(00False,00False)f{Inv}opi1dopi2 {true} We have already derived the .rst premise. Since GdG=G, we need \nto discharge the following thread-local triples (again postponed to Section 5): G,00False f{Inv}opj {true},j \n.{i1,i2} (4.7) We no longer need to distinguish between the guarantees of the two threads in the conclusion \nof (4.6). Hence, we use the rule PAR-MERGE, which merges the guarantees provided by the threads into \na single guarantee provided by their parallel composition: G,(00False,00False)f{Inv}opi1dopi2 {true} \nG,(00False)d(00False)f{Inv}opi1dopi2 {true} The conclusion is equivalent to G,00False f{Inv}opi1dopi2 \n{true} (4.8) from which (4.1) follows by CONSEQ. This proves termination of the two operations. Arbitrary \nnumber of operations. We can generalise our proof to an arbitrary number of operations as follows. First, \nnote that applying PAR-MERGE on (4.4), we get: CL(G),Gf{Inv}opi1dopi2 {true} (4.9) Hence, the proof for \ntwo operations establishes (4.9) and (4.8) given (4.3) and (4.7), i.e., it shows that the parallel composition \nopi1dopi2 preserves the properties (4.3) and (4.7) of its constituent operations. Note that this derivation \nis independent of the particular de.nitions of opi1 and opi2 satisfying (4.3) and (4.7). This allows \nus to prove by induction on kthat CL(G),Gf{Inv}opi1d...dopik {true} G,00False f{Inv}opi1d...dopik {true} \n(4.10) is derivable in our proof system for any k = 1.For k =1 the triples are established by (4.3) and \n(4.7). For the induction step, we just repeat the previous derivation with opi1 replaced by opi1d...dopik \nand opi2 replaced by opi(k+1). Applying CONSEQ to (4.10), we get (3.4), which entails lock\u00adfreedom of \nTreiber s stack. Note that instead of doing induction on the number of threads, we could have formulated \nour proof rules for kthreads. To simplify the presentation, we chose the minimalistic proof system. \n4.2 Proving lock-freedom of the HSY non-blocking stack The action Xchg used in the informal proof of \nlock-freedom of the HSY stack (Section 2) can be formally de.ned as follows: 0=i=SIZE -1.collision[i]. \ncollision[i]. (Xchg) The abridged data structure invariant is: .x.&#38;S .x*lseg(x,NULL)*true Inv = \n*@SIZE-1 i=0 collision[i]. *... (We elided some of the data structures of the elimination scheme.) We \nnow formalise the informal proof from Section 2 for two operations opi1 and opi2 running in parallel. \nStatement I. The statement requires us to establish the guarantee G=o(Push .Pop .Xchg .Others .Id).\u00aco0(Push \n.Pop) where Others describes the interference caused by the elimination scheme(elided).Asbefore,wecandothisusing \nPAR-C.Giventhe thread-local triples (4.3) with the newly de.ned opi1, opi2, G,and Inv, we can again derive \n(4.5) and (4.9), where CL(G)=o(Push .Pop .Xchg .Others .Id) Statement II. Now, provided that a thread \nsatis.es the guarantee G, we have to prove that the other thread satis.es the guarantee G ' =o(Push .Pop \n.Xchg .Others .Id).\u00aco0Xchg To this end, we use the non-circular rely-guarantee rule PAR-NC: G,(G,G)f{Inv}opi1dopi2 \n{true}GdG,G ' f{Inv}opi1 {true}GdG,G ' f{Inv}opi2 {true} G,(G ' ,G ' )f{Inv}opi1dopi2 {true} We thus \nhave to establish the following thread-local triples: G,G ' f{Inv}opj {true},j .{i1,i2} (4.11) We can \nnow use the conjunction rule, CONJ, to combine the guar\u00ad ' '' : antees Gand G into a single guarantee \nG G,(G,G)f{Inv}opi1dopi2 {true}G,(G ' ,G ' )f{Inv}opi1dopi2 {true} (4.12) G,(G '' ,G '' )f{Inv}opi1dopi2 \n{true} where G '' =G.G ' =o(Push .Pop .Xchg .Others .Id) .\u00aco0(Push .Pop .Xchg) This combines Statements \nI and II. Applying CONSEQ,we get: '' '''' G ,(G ,G )f{Inv}opi1dopi2 {true} procedure LOCKFREE(init,op) \n(Inv,G1):= SAFETYGUARANTEE(init,op) G2 := \u00d8 do G:= oG1 .\u00aco0G2 if THREADLOCAL(G,00False f{Inv}op {true}) \nreturn Lock-free G0 2 := G2 for each A .(G1 \\G02) do if THREADLOCAL(G,\u00aco0A f{Inv}op {true}) G2 := G2 \n.{A} while G02 = G2 return Don t know Figure 6. Proof search procedure for lock-freedom Thus, we have \nstrengthened the guarantee Gof Statement I to the guarantee G '' . Statement III. Finally, we can formalise \nStatement III by apply\u00ading the rule PAR-NC to establish termination: '' '''' G ,(G ,G ) f{Inv}opi1dopi2 \n{true} '' '' G dG ,00False f{Inv}opi1 {true} '' '' G dG ,00False f{Inv}opi2 {true} G '' ,(00False,00False) \nf{Inv}opi1dopi2 {true} provided we can establish the thread-local triples G '' ,00False f{Inv}opj {true},j \n.{i1,i2} (4.13) By PAR-MERGE we then get: G '' ,00False f{Inv}opi1dopi2 {true} (4.14) which proves the \ntermination of the two operations. Arbitrary number of operations. From the conclusion of (4.12), by \nPAR-MERGE we get: G,G '' f{Inv}opi1dopi2 {true} (4.15) Thus, the above derivation establishes (4.9), \n(4.15), and (4.14) given (4.3), (4.11), and (4.13). As before, this allows us to prove by induction on \nk that the following triples are derivable in our logic for any k =1: CL(G),Gf{Inv}opi1d...dopik {true} \nG,G '' f{Inv}opi1d...dopik {true} G '' ,00False f{Inv}opi1d...dopik {true} The last one implies lock-freedom \nof the HSY stack.  5. Automation In this section we describe our automatic prover for liveness prop\u00aderties \nof non-blocking concurrent algorithms. Our tool s input is aliveness property tobeproved and aprogram \nin aC-likelan\u00adguage consisting of the code of operations op1,...,opof a non\u00ad n blocking algorithm, together \nwith a piece of initialisation code init. We remind the reader that we denote with op the command, de.ned \nby (2.1), that non-deterministically executes one of the op\u00aderations opi on the data structure. We .rst \ndescribe how our tool handles lock-freedom. Proving lock-freedom via proof search. Recall that to prove \nlock-freedom, we have to prove termination of the program C ' (k) de.ned by (2.2) for an arbitrary k. \nAll rely and guarantee condi\u00adtions used in the examples of such proofs in Section 4 had a re\u00adstricted \nform oA1 .\u00aco0A2,where A1 and A2 are sets (disjunc\u00adtions) of actions and A2 . A1.Here A1 is the set of \nall actions that a thread can perform, whereas A2 is the set of actions that the thread performs only \n.nitely often. In fact, for all the non-blocking algorithms we have studied, it was suf.cient to consider \nrely and guarantee conditions of this form to prove lock-freedom. We prove termination of C ' (k) by \nsearching for proofs of triple (3.4) in our proof system in the style of those presented in Section 4 \nwith relies and guarantees of the form oA1 .\u00aco0A2. There are several ways in which one can organise such \na proof search. The strategy we use here is to perform forward search as explained informally in Section \n2. Figure 6 contains our procedure LOCKFREE for proving lock\u00adfreedom. It is parameterised with two auxiliary \nprocedures, whose implementation is described later: SAFETYGUARANTEE(init,op) computes supporting safety \nproperties for our liveness proofs, namely, a data structure in\u00advariant Inv such that {} {} oId,oTrue \nfempinit Inv(5.1) and an initial safety guarantee provided by every operation, which is de.ned by a set \nof actions G1 = {A1,...,An}such that oG1,oG1 f{Inv}op {Inv} (5.2) THREADLOCAL(R,Gf{Inv}op {true}) attempts \nto prove the thread-local triple R,Gf{Inv}op {true}valid. The no\u00adtion of validity of thread-local triples \nused by THREADLOCAL corresponds to the informal explanation given in Section 3 and is formalised in Section \n6. LOCKFREE .rst calls SAFETYGUARANTEE to compute the data structure invariant Inv and the safety guarantee \noG1. In our proofs of liveness properties, rely and guarantee conditions are then represented using LTL \nformulae with actions in G1 as atomic propositions. A side-effect of SAFETYGUARANTEE is that it an\u00adnotates \natomic blocks in op with actions from G1 as explained in Section 3. These annotations are used by the \nsubsequent calls to THREADLOCAL, which ensures that all thread-local reasoning in the proof of lock-freedom \nuses the same splitting of the program state into local and shared parts. Having computed the safety \nguarantee, we enter into a loop, where on every iteration we .rst attempt to prove termination of op \nusing the available guarantee. If this succeeds, we have proved lock-freedom. Otherwise, we try to strengthen \nthe guarantee oG1 . \u00aco0G2 by considering each action in G1 \\G2 and tryingtoprove that it is executed \nonly .nitely often using the current guarantee as a rely condition. If we succeed, we update the guarantee \nby adding the action to the set of .nitely executed actions G2. If we cannot prove that any action from \nG1 \\G2 is executed only .nitely often, we give up the search and exit the loop. This procedure scales \nbecause in practice the set of actions G1 computedby SAFETYGUARANTEE issmall.Thisisduetothefact that \nactions pq are local in the sense that pand q describe only the parts of the shared state modi.ed by \natomic blocks. It is possible to show that a successful run of LOCKFREE con\u00adstructs proofs of triples \n(3.4) for all k. We can construct the proofs for any number of threads uniformly because the guarantees \nGused in them are such that GdG = G. The construction follows the method of Section 4. The only difference \nis that the proofs con\u00adstructed by LOCKFREE .rst apply the rule PAR-C to triples (5.2) with G1 = G2 = \noG1.Since CL(oG1)= oG1, this establishes the initial safety guarantee oG1, which is then strengthened \nusing  R, Gf{Inv} op {true} ( ) Fair termination of (opdasyncR)dsync\u00acG ( SMALLFOOTRG [6] ) Abstract \ntransition system  Translation to arithmetic programs [3, 21] Equiterminating arithmetic program ( \n)   Valid/Don t know Figure 7. The high-level structure of the THREADLOCAL proce\u00addure for discharging \nthread-local triples the rule PAR-NC. In the proofs of Section 4, these two steps were performed with \none application of the rule PAR-C. We now describe the two auxiliary procedures used by LOCK-FREE SAFETYGUARANTEE \nand THREADLOCAL. The SAFETYGUARANTEE procedure. We implement the proce\u00addure using the SMALLFOOTRG tool \nfor verifying safety properties of non-blocking algorithms [6]. SMALLFOOTRG computes a data structure \ninvariant and an interference speci.cation by performing abstract interpretation of the code of init \nand op over an abstract domain constructed from RGSep formulae. This abstract interpre\u00adtation is thread-modular, \ni.e., it repeatedly analyses separate threads without enumerating interleavings using an algorithm similar \nto the one described in [12]. For the invariant and interference speci.ca\u00adtions computed by SMALLFOOTRG \nto be strong enough for use in liveness proofs, its abstract domain has to be modi.ed to keep track of \nthe lengths of linked lists as described in [21]. RGSep judgements can be expressed in our logic by triples \nwith rely and guarantee conditions of the form oA,where A is a set of actions. SMALLFOOTRGprovesthevalidityofRGSepjudgements \nthat, when translated to our logic in this way, yield (5.1) and (5.2). The THREADLOCAL procedure. We \nprove a thread-local triple R, Gf{Inv} op {true} using a combination of several existing methods and \ntools, as shown in Figure 7. For technical reasons, in this procedure we assume that R and G consist \nof only in.\u00adnite words and op has only in.nite computations. This can always be ensured by padding the \n.nite words in R and G with a special dummy action and inserting an in.nite loop at the end of op exe\u00adcuting \nthe action. To prove the triple R, Gf{Inv} op {true}: We .rst represent R and \u00acG as B\u00a8uchi automata, \nwhose tran\u00adsitions are labelled with actions from the set G1 computed by SAFETYGUARANTEE and apply the \nautomata-theoretic frame\u00adwork for program veri.cation [36]. This reduces proving the triple to proving \nthat the program (opdasyncR)dsync\u00acG termi\u00adnates when run from states satisfying the precondition Inv \nun\u00adder the fairness assumptions extracted from the accepting con\u00additions of the automata for R and \u00acG.Here \nopdasyncR is the asynchronous parallel composition interleaving the executions of op and the automaton \nR in all possible ways. The pro\u00adgram (opdasyncR)dsync\u00acG is the synchronous parallel compo\u00adsition of opdasyncR \nand the automaton \u00acG synchronising on ac\u00adtions of op. Intuitively, fair in.nite executions of the program \n(opdasyncR)dsync\u00acG correspond to the executions of op in an environment satisfying the rely R that violate \nthe guarantee G. Its fair termination implies that there are no such executions. To check fair termination \nof (opdasyncR)dsync\u00acG,we analyse it with the abstract interpreter of SMALLFOOTRG [6], which produces \nan abstract transition system over-approximating the program s behaviour. The interpreter uses the annotations \nat atomic blocks computed by SAFETYGUARANTEE to choose the splitting of the heap into local and shared \nparts.  Using the techniques of [3, 21], from this transition system we then extract an arithmetic program \n(i.e., a program with\u00adout the heap with only integer variables), whose fair termina\u00adtion implies fair \ntermination of (opdasyncR)dsync\u00acG. The arith\u00admetic program makes explicit the implicit arithmetic infor\u00admation \npresent in the heap-manipulating program that can be used by termination provers to construct ranking \nfunctions. For example, it contains integer variables tracking the lengths of linked lists in the original \nprogram.  Finally, we run a termination prover (TERMINATOR with fair\u00adness [8]) to prove fair termination \nof the arithmetic program.  We note that proofs of thread-local statements may be more complicated then \nthe ones in the examples of Section 2, which were based on control-.ow arguments. For example, for Michael \ns non\u00adblocking linked list algorithm [25] they involve reasoning about lengths of parts of the shared \ndata structure. Furthermore, the proofs may rely on complex supporting safety properties that en\u00adsures \nthat the data structure is well-formed. Automatic tool support is indispensable in constructing such \nproofs. Proving obstruction-freedom and wait-freedom. As we showed in Section 2, proving obstruction-freedom \nor wait-freedom of an operation in a non-blocking algorithm usually requires only safety guarantees provided \nby the operation s environment. In our tool, we use the guarantee oG1 inferred by SAFETYGUARANTEE. Namely, \nwe prove obstruction-freedom of an operation opi by establishing triple (3.3) with R = oG1 via a call \nto THREADLOCAL(oG1 .00False, 00False f{Inv} opi {true}) We can prove wait-freedom of an operation opi \nby establishing triple (3.2) with R = oG1 via a call to THREADLOCAL(oG1, 00False f{Inv} opi {true}) Experiments. \nUsing our tool, we have proved a number of non\u00adblocking algorithms lock-free and have found counterexamples \ndemonstrating that they are not wait-free. The examples we anal\u00adysed include a DCAS-based stack, Treiber \ns stack [33], the HSY stack [14], a non-blocking queue due to Michael and Scott [26] and its optimised \nversion due to Doherty et al. [9], a restricted double\u00adcompare single-swap operation (RDCSS) [13], and \nMichael s non\u00adblocking linked list [25]. In all cases except Michael s algorithm the tool found a proof \nof lock-freedom in less then 10 minutes. Veri.cation of Michael s algorithm takes approximately 8 hours, \nwhich is due to the unoptimised arithmetic program generator and the inef.cient version of the termination \nprover that we currently use. We have also tested our tool by proving the obstruction-freedom of the \nabove lock-free algorithms. (Obstruction-free algorithms that are not lock-free typically traverse arrays, \nhandling which is be\u00adyond the scope of the shape analysis that we use.) Additionally, we have checked \nthat the deletion operation of a linked list algorithm by Vechev and Yahav [37, Figure 2] is not obstruction-free \n(as ob\u00adserved by the authors), even though it does not use locks. We do not report any results for wait-free \nalgorithms in this paper. Operations consisting of straight-line code only are trivially wait-free. Proving \ntermination of wait-free find operations in non\u00adblocking linked lists mentioned in Section 2 requires \ntracking the keys stored in the list, which is not handled by our shape analysis. 6. Semantics and soundness \nWe give semantics to programs with respect to labelled transi\u00adtion systems with states representing the \nwhole program heap. The proof of soundness of our logic with respect to this global seman\u00adtics is done \nin two steps. We .rst show that, given a transition sys\u00adtem denoting a program, we can construct another \ntransition system operating on states that distinguish between local and shared heap, according to the \ninformal description given in Section 3. Interpre\u00adtation of judgements in this split-state semantics \nis straightforward. We then relate the validity of judgements in the split-state seman\u00adtics to the standard \nglobal notion of validity. The results in this sec\u00adtion do not follow straightforwardly from existing \nones, however, the techniques used to formulate the split-state semantics and prove the soundness theorems \nare the same as for the RGSep logic [35]. 6.1 Global semantics We represent denotations of programs as \na variant of labelled tran\u00adsition systems (LTS). DEFINITION 1(LTS). A labelled transition system (LTS) \nis a quadruple S =(S,T,F,T),where S is the set of non-erroneous states of the transition system,  T./S \nis a distinguished error state (arising, for example, when a program dereferences an invalid pointer), \n F .S is the set of .nal states, and  T is the set of transitions such that every t .T is associated \nwith a transition function ft :S .P(S) .{T},where P(S) is the powerset of S.  DEFINITION 2 (Computation \nof an LTS). A computation of an LTS (S,T,F,T) starting from an initial state u0 .S is a maxi\u00admal sequence \nu0,u1,...of states ui .S .{T}such that for all i there exists a transition t .T such that ui+1 = Tif \nft (ui)= T and ui+1 .ft (ui) otherwise. Given a thread C in the programming language of Section 3, we \ncan construct the corresponding LTS [C] in the following way. Let us assume for the purposes of this \nconstruction that the program counter of the thread is a memory cell at a distinguished address &#38;pc, \nimplicitly modi.ed by every primitive command. As the set of states S of the LTS we take the one de.ned \nin Figure 4. The .nal states are those in which the program counter has a distinguished .nal value. Every \natomic command in the thread, including atomic blocks, corresponds to a transition in the LTS. Conditions \nin if and while commands are translated in the standard way using assume commands. The transition functions \nare then just the standard postcondition transformers [30, 5]. The denotation of a parallel composition \nof threads is the paral\u00adlel composition of their denotations, de.ned as follows. DEFINITION 3 (Parallel \ncomposition of LTSes). The paral\u00adlel composition of two LTSes S1 =(S,T,F1,T1) and S2 =(S,T,F2,T2),where \nT1 nT2 = \u00d8, is de.ned as the LTS S1dS2 =(S,T,F1 nF2,T1 lT2). As follows from De.nitions 2 and 3, the \nparallel composition interleaves transitions from two LTSes on the same memory S without any fairness \nconstraints. Note that we can always satisfy T1 nT2 = \u00d8by renaming transitions appropriately. 6.2 Split-state \nsemantics We now show that given an LTS we can construct a split LTS that distinguishes between the local \nand the shared state. To this end, we assume a labelling function p that maps each transition in an LTS \nto either Local for operations that only access the local state, or Shared(pq) for operations that access \nboth the local and the shared state. Note that for a program C we can construct such a labelling pC from \nthe annotations we introduced in Section 3: com\u00admands outside atomic blocks are mapped to Local and annotations \nat atomic blocks give the parameters of Shared. Given a labelling p for an LTS (S,T,F,T), we can de.ne \nthe corresponding split LTS as (S2 ,T,F \u00d7S,T ' ),where T ' consists of fresh copies of transitions t \n' for every transition t . T.The program counter of a thread is always in its local state, hence, the \nset of states of the split LTS in which it has the .nal value is F \u00d7S. The transition functions for the \nsplit LTS are de.ned as follows. If p(t)= Local,then t ' executes t on the local state and preserves \nthe shared state: ftI (l,s)= ft (l) \u00d7{s}if ft (l)= T, and ftI (l,s)= Totherwise. If p(t)= Shared(pq), \nthen the execution of t ' follows the informal description of the execution of atomic blocks in Section \n3: ft' I (l,s)=  {(restq (u),satq(u) \u00b7restp(s)) |u.ft (l\u00b7satp(s))} if satp(s) is de.ned, ft (l\u00b7satp(s)) \n= T,and satq (u) is de.ned for all u .ft (l\u00b7satp(s));otherwise, t ' faults: ftI (l,s)= T. 6.3 Validity \nin the split-state semantics To de.ne validity of triples in the split-state semantics, we have to de.ne \nthe meaning of interleaving computations of a split LTS (S2 ,T,F \u00d7S,T) with actions of an environment \nchanging the shared state according to a rely condition R.L(S2).We repre\u00adsent these computations with \ntraces a .L(S2 \u00d7(S2 .{T}) \u00d7 ({e}.T)). The .rst two components of every letter in a trace de\u00ad.ne how the \nstate of the LTS changes. The third component de.nes if the change was made by a transition of the LTS \n(t .T)orthe environment (e). We require that the environment does not change the local state and does \nnot fault, i.e., all e-letters in a trace are of the form ((l,s),(l,s ' ),e). We often need to project \na trace awithout error states to a word that records how the shared state is changed by a particular \nset of transitions U .{e}.T. We de.ne such a projection ajU . L(S2) as the image of aunder the following \nhomomorphism 22 2 h :S\u00d7S\u00d7({e}.T) .L(S) '' (s,s ' ),t .U; h((l,s),(l ,s ),t)= e, otherwise where eis \nthe empty word. We write a.s if ais a nonempty trace and its last letter is of the form (s ' ,s,t) for \nsome s ' and t. DEFINITION 4 (Traces). For a rely condition R.L(S2) and a split LTS S =(S2 ,T,F \u00d7S,T),the \nset tr(S,R,s0) of traces of S executed in an environment satisfying Rstarting from an initial state s0 \n. S2 is de.ned as the set of traces a .L(S2 \u00d7(S2 . {T}) \u00d7({e}.T)) of the following two forms: .nite \nor in.nite traces a =(s0,s1,t0)(s1,s2,t1) ...,where si = T, aj{e} .R, and if ti = e,then si+1 .fti (si); \nand  .nite traces a = \u00df(sn,T,tn) for some \u00df =(s0,s1,t0) (s1,s2,t1) ...(sn-1,sn,tn-1) such that \u00dfj{e} \n.R, ftn (sn)= T, and if ti = e for i<n,then si+1 .fti (si).  The .rst case in this de.nition corresponds \nto safe traces, and the second to unsafe traces, i.e., those in which both the program and its environment \nstop executing after the program commits a memory fault (the treatment of the later case relies on Rbeing \npre.x-closed). Note that, since we assume that the scheduler is possibly unfair, the set of traces in \nthis de.nition includes those in which S is preempted and is never executed again. Hence, the set of \nprojections ajT of traces a.tr(S,R,s0) on the transitions of the LTS S, representing the guarantee condition \nprovided by S, is pre.x-closed. Let F0(C), respectively Ff (C),be the *-conjunction over all the threads \nin a program C of formulae &#38;pc .pc0, respectively &#38;pc .pcf ,where pc is the program counter of \nthe thread, pc0 is its initial value, and pcf is the .nal one. Note that F0(C) and Ff (C) do not restrict \nthe shared state. DEFINITION 5 (Validity). R,G|= {P}C {Q}. .s0 .[[P *F0(C)]]..a.tr(S,R,s0)..s. (a.s .s \n= T) . (safety) (a.s.s .(F \u00d7S) .s .[[Q*Ff (C)]]) . (correctness) (ajT .G) (guarantee) where S =(S2 ,T,F \n\u00d7S,T) is the split LTS constructed out of the LTS [C] using the labelling pC . R,(G1,G2) |= {P}C1dC2 \n{Q}. .s0 .[[P *F0(C1dC2)]]..a.tr(S1dS2,R,s0)..s. (a.s .s = T) . (safety) (a.s.s .((F1 nF2) \u00d7S) .s .[[Q*Ff \n(C1dC2)]]) . (correctness) (ajT1 .G1) .(ajT2 .G2) (guarantee) where S1 =(S2 ,T,F1 \u00d7S,T1) and S2 =(S2 \n,T,F2 \u00d7S,T2) are the split LTSes constructed out of the LTSes [C1] and [C2] using the labellings pC1 \nand pC2 , respectively. THEOREM 1. The proof rules in Figure 5 preserve validity. COROLLARY 1. If R,Gf{P} \nC {Q} is derived from valid thread-local triples using the rules in Figure 5, then R,G|= {P}C {Q}. 6.4 \nSoundness We now relate the notion of validity with respect to a split LTS to validity with respect to \nthe global LTS used to construct the split one. For a closed program (i.e., a program executing in isolation), \nwe can formulate a global notion of validity of triples without rely and guarantee conditions as follows. \nDEFINITION 6 (Validity with respect to a global LTS). For p,q . S and a command C such that [C] =(S,T,F,T) \nwe de.ne |= {p}C {q}if for all u0 .pand for any computation u0,u1,... of [C] we have ui = T, and if the \ncomputation is .nite and ending with a state u . F,then u . q. We de.ne |=[p] C [q] if |= {p}C {q}and \nevery computation of [C] starting from a state in pis .nite. THEOREM 2. Let [C] =(S,T,F,T) and S ' =(S2 \n,T,F \u00d7 S,T ' ) be a corresponding split LTS with respect to any labelling pC .Then R,G|= {P}C {Q}implies \n|= {.([[P *F0(C)]])}C {.([[Q*Ff (C)]])},  If R,00False |= {P}C {Q}implies |=[.([[P *F0(C)]])] C [.([[Q*Ff \n(C)]])],  where .(P)= {l\u00b7s|(l,s) .P}for any assertion P. Theorem 2 and Corollary 1 show that the provability \nof triple (3.4) from valid thread-local triples in the proof system of Section 4 implies that the program \nC ' (k) terminates, and hence, the corresponding algorithm is lock-free. Similar soundness results can \nbe formulated for obstruction-freedom and wait-freedom.  7. Related work Our proof system draws on the \nclassical circular and non-circular rely-guarantee rules for shared-variable concurrency [18, 29, 1] \nto achieve compositionality, and on separation logic (speci.cally, RGSep a combination of rely-guarantee \nand separation logic [35, 11, 34]) to achieve modular reasoning in the presence of heap. Its technical \nnovelty over previous rely-guarantee proof systems lies in our method of combining applications of circular \nand non\u00adcircular rules using judgements that distinguish between guarantees provided by different threads \nin a parallel composition. Colvin and Dongol [7] have recently proved the most basic non\u00adblocking algorithm, \nTreiber s stack [33], to be lock-free. They did this by manually constructing a global well-founded ordering \nover program counters and local variables of all the threads in the al\u00adgorithm s most general client. \nUnfortunately, their method requires each operation to have at most one lock-free loop, which rules out \nmore modern non-blocking algorithms, such as the HSY stack and Michael s list algorithm. Moreover, because \ntheir well-founded or\u00addering is over the whole program, their method is non-modular and does not scale \nto the more realistic examples of the kind we con\u00adsider in Section 5. In contrast, our method is modular, \nboth in the treatment of threads and heaps. We can reason about every thread separately under simple \nassumptions about its environment that do not consider parts of the heap local to other threads. Furthermore, \nour method is fully automatic. Kobayashi and Sangiorgi [19] have recently proposed a type\u00adbased method \nfor checking lock-freedom in p-calculus. Their pro\u00adgramming model and the notion of lock-freedom are \ndifferent from the ones used for non-blocking data structures, which makes their results incomparable \nto ours. Moore [27] presents a proof of a progress property for a non-blocking counter algorithm in the \nACL2 proof assistant. His proof is thread-modular, but the algo\u00adrithm considered is extremely simple. \nMcMillan [24] addresses the issue of circular dependencies among a class of liveness properties in the \ncontext of .nite-state hardware model checking. He takes a different approach from ours to resolving \nthe circularities by doing induction over time. 8. Conclusion Wait-freedom, lock-freedom, and obstruction-freedom \nare the es\u00adsential properties that make non-blocking algorithms actually non-blocking. We have proposed \nthe .rst fully automatic tool that allows their developers to verify these properties. Our success was \ndue to choosing a logical formalism in which it was easy to ex\u00adpress proofs about non-blocking algorithms \nand then observing that proofs of the liveness properties in it follow a particular pattern. We conclude \nby noting some limitations of our tool; lifting these presents interesting avenues for future work. First, \nwe prove the soundness of our logic with respect to an interleaving seman\u00adtics, which is inadequate for \nmodern multiprocessors with weak memory models. It happens that even proving safety properties of programs \nwith respect to a weak memory model is currently an open problem. Moreover, the published versions of \nconcurrent al\u00adgorithms assume a sequentially consistent memory model. In fact, most of non-blocking algorithms \nare incorrect when run on mul\u00adtiprocessors with weak memory models as published: one has to insert additional \nfences or (on x86) locked instructions for them to run correctly. In the future, we hope to address this \nproblem, building on a recent formalisation of weak memory model seman\u00adtics [31]. Second, our tool can \ncurrently handle only list-based algo\u00adrithms, because we use an off-the-shelf shape analysis. We believe \nthat the methods described in this paper should be applicable to more complicated data structures as \nwell, provided the necessary shape analysis infrastructure is available. The above-mentioned limitations \nnotwithstanding, this paper presents the .rst successful attempt to give modular proofs of liveness properties \nto complex heap-manipulating concurrent pro\u00adgrams. Acknowledgements. We would like to thank Josh Berdine, \nMike Dodds, Tim Harris, Michael Hicks, Andreas Podelski, Moshe Vardi, Eran Yahav, and the anonymous reviewers \nfor comments and discussions that helped to improve the paper.  References [1] M. Abadi and L. Lamport. \nConjoining speci.cations. ACM Trans. Program. Lang. Syst., 17(3):507 534, 1995. [2] B. Alpern and F. \nB. Schneider. De.ning liveness. Inf. Process. Lett., 21(4):181 185, 1985. [3] J. Berdine, B. Cook, D. \nDistefano, and P. W. O Hearn. Automatic termination proofs for programs with shape-shifting heaps. In \nCAV 06: Conference on Computer Aided Veri.cation, volume 4144 of LNCS, pages 386 400. Springer, 2006. \n[4] S. D. Brookes. Full abstraction for a shared-variable parallel language. Inf. Comput., 127(2):145 \n163, 1996. [5] C. Calcagno, P. O Hearn, and H. Yang. Local action and abstract separation logic. In \nLICS 07: Symposium on Logic in Computer Science, pages 366 378. IEEE, 2007. [6] C. Calcagno, M. J. Parkinson, \nand V. Vafeiadis. Modular safety checking for .ne-grained concurrency. In SAS 07: Static Analysis Symposium, \nvolume 4634 of LNCS, pages 233 248. Springer, 2007. [7] R. Colvin and B. Dongol. Verifying lock-freedom \nusing well-founded orders. In ICTAC 07: International Colloquium on Theoretical Aspects of Computing, \nvolume 4711 of LNCS, pages 124 138. Springer, 2007. [8] B. Cook, A. Gotsman, A. Podelski, A. Rybalchenko, \nand M. Y. Vardi. Proving that programs eventually do something good. In POPL 07: Symposium on Principles \nof Programming Languages, pages 265 276. ACM, 2007. [9] S. Doherty, L. Groves, V. Luchangco, and M. Moir. \nFormal veri.cation of a practical lock-free queue algorithm. In FORTE 04: Conference on Formal Techniques \nfor Networked and Distributed Systems, volume 3235 of LNCS, pages 97 114. Springer, 2004. [10] B. Dongol. \nFormalising progress properties of non-blocking programs. In ICFEM 06: Conference on Formal Engineering \nMethods, volume 4260 of LNCS, pages 284 303. Springer, 2006. [11] X. Feng, R. Ferreira, and Z. Shao. \nOn the relationship between concurrent separation logic and assume-guarantee reasoning. In ESOP 07: European \nSymposium on Programming, volume 4421 of LNCS, pages 173 188. Springer, 2007. [12] A. Gotsman, J. Berdine, \nB. Cook, and M. Sagiv. Thread-modular shape analysis. In PLDI 07: Conference on Programming Language \nDesign and Implementation, pages 266 277. ACM, 2007. [13] T. L. Harris, K. Fraser, and I. A. Pratt. A \npractical multi-word compare-and-swap operation. In DISC 02: Symposium on Distributed Computing, volume \n2508 of LNCS, pages 265 279. Springer, 2002. [14] D. Hendler, N. Shavit, and L. Yerushalmi. A scalable \nlock-free stack algorithm. In SPAA 04: Symposium on Parallelism in Algorithms and Architectures, pages \n206 215. ACM, 2004. [15] M. Herlihy. Wait-free synchronization. ACM Trans. Program. Lang. Syst., 13(1):124 \n149, 1991. [16] M. Herlihy, V. Luchangco, and M. Moir. Obstruction-free syn\u00adchronization: Double-ended \nqueues as an example. In ICDCS 03: International Conference on Distributed Computing Systems, pages 522 \n529. IEEE, 2003. [17] M. Herlihy and N. Shavit. The Art of Multiprocessor Programming. Morgan Kaufmann, \n2008. [18] C. B. Jones. Speci.cation and design of (parallel) programs. In IFIP Congress, pages 321 332. \nNorth-Holland, 1983. [19] N. Kobayashi and D. Sangiorgi. A hybrid type system for lock\u00adfreedom of mobile \nprocesses. In CAV 08: Conference on Computer Aided Veri.cation, volume 5123 of LNCS, pages 80 93. Springer, \n2008. [20] O. Lichtenstein, A. Pnueli, and L. D. Zuck. The glory of the past. In Conference on Logics \nof Programs, volume 193 of LNCS, pages 196 218. Springer, 1985. [21] S. Magill, J. Berdine, E. M. Clarke, \nand B. Cook. Arithmetic strengthening for shape analysis. In SAS 07: Static Analysis Symposium, volume \n4634 of LNCS, pages 419 436. Springer, 2007. [22] Z. Manna and A. Pnueli. The Temporal Logic of Reactive \nand Concurrent Systems: Speci.cation. Springer-Verlag, Berlin, 1992. [23] H. Massalin and C. Pu. A lock-free \nmultiprocessor OS kernel (Abstract). Operating Systems Review, 26(2):108, 1992. [24] K. L. McMillan. \nCircular compositional reasoning about liveness. In CHARME 99: Conference on Correct Hardware Design \nand Veri.cation Methods, volume 1703 of LNCS, pages 342 345. Springer, 1999. [25] M. M. Michael. High \nperformance dynamic lock-free hash tables and list-based sets. In SPAA 02: Symposium on Parallelism in \nAlgorithms and Architectures, pages 73 82. ACM, 2002. [26] M. M. Michael and M. L. Scott. Simple, fast, \nand practical non\u00adblocking and blocking concurrent queue algorithms. In PODC 96: Symposium on Principles \nof Distributed Computing, pages 267 275. ACM, 1996. [27] J. S. Moore. A mechanically checked proof of \na multiprocessor result via a uniprocessor view. Formal Methods in System Design, 14(2):213 228, 1999. \n[28] P. W. O Hearn. Resources, concurrency and local reasoning. Theor. Comput. Sci., 375(1-3):271 307, \n2007. [29] A. Pnueli. In transition from global to modular temporal reasoning about programs. In Logics \nand Models of Concurrent Systems, pages 123 144. Springer, 1985. [30] J. Reynolds. Separation logic: \nA logic for shared mutable data structures. In LICS 02: Symposium on Logic in Computer Science, pages \n55 74. IEEE, 2002. [31] S. Sarkar, P. Sewell, F. Zappa Nardelli, S. Owens, T. Ridge, T. Braibant, M. \nMyreen, and J. Alglave. The semantics of x86 multiprocessor machine code. This volume. [32] H. Simpson. \nFour-slot fully asynchronous communication mecha\u00adnism. IEE Proceedings, 137(1):17 30, 1990. [33] R. K. \nTreiber. Systems programming: Coping with parallelism. Technical Report RJ 5118, IBM Almaden Research \nCenter, 1986. [34] V. Vafeiadis. Modular .ne-grained concurrency veri.cation. PhD Thesis, University \nof Cambridge Computer Laboratory, 2008. [35] V. Vafeiadis and M. J. Parkinson. A marriage of rely/guarantee \nand separation logic. In CONCUR 07: Conference on Concurrency Theory, volume 4703 of LNCS, pages 256 \n271. Springer, 2007. [36] M. Vardi. Veri.cation of concurrent programs the automata\u00adtheoretic framework. \nAnn. Pure Appl. Logic, 51:79 98, 1991. [37] M. T. Vechev and E. Yahav. Deriving linearizable .ne-grained \nconcurrent objects. In PLDI 08: Conference on Programming Languages Design and Implementation, pages \n125 135. ACM, 2008. [38] I. William N. Scherer, D. Lea, and M. L. Scott. Scalable synchronous queues. \nIn PPoPP 06: Symposium on Principles and Practice of Parallel Programming, pages 147 156. ACM, 2006. \n \n\t\t\t", "proc_id": "1480881", "abstract": "<p>A concurrent data-structure implementation is considered non-blocking if it meets one of three following liveness criteria: wait-freedom, lock-freedom, or obstruction-freedom. Developers of non-blocking algorithms aim to meet these criteria. However, to date their proofs for non-trivial algorithms have been only manual pencil-and-paper semi-formal proofs. This paper proposes the first fully automatic tool that allows developers to ensure that their algorithms are indeed non-blocking. Our tool uses rely-guarantee reasoning while overcoming the technical challenge of sound reasoning in the presence of interdependent liveness properties.</p>", "authors": [{"name": "Alexey Gotsman", "author_profile_id": "81322494535", "affiliation": "University of Cambridge, Cambridge, United Kingdom", "person_id": "P1300918", "email_address": "", "orcid_id": ""}, {"name": "Byron Cook", "author_profile_id": "81323489213", "affiliation": "Microsoft Research, Cambridge, United Kingdom", "person_id": "P1300919", "email_address": "", "orcid_id": ""}, {"name": "Matthew Parkinson", "author_profile_id": "81406598777", "affiliation": "University of Cambridge, Cambridge, United Kingdom", "person_id": "P1300920", "email_address": "", "orcid_id": ""}, {"name": "Viktor Vafeiadis", "author_profile_id": "81100493655", "affiliation": "Microsoft Research, Cambridge, United Kingdom", "person_id": "P1300921", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1480881.1480886", "year": "2009", "article_id": "1480886", "conference": "POPL", "title": "Proving that non-blocking algorithms don't block", "url": "http://dl.acm.org/citation.cfm?id=1480886"}