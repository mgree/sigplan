{"article_publication_date": "01-21-2009", "fulltext": "\n Equality Saturation: a New Approach to Optimization * Ross Tate Michael Stepp Zachary Tatlock Sorin \nLerner Department of Computer Science and Engineering University of California, San Diego {rtate, mstepp, \nztatlock, lerner} @cs.ucsd.edu Abstract Optimizations in a traditional compiler are applied sequentially, \nwith each optimization destructively modifying the program to pro\u00adduce a transformed program that is \nthen passed to the next op\u00adtimization. We present a new approach for structuring the opti\u00admization phase \nof a compiler. In our approach, optimizations take the form of equality analyses that add equality information \nto a common intermediate representation. The optimizer works by re\u00adpeatedly applying these analyses to \ninfer equivalences between program fragments, thus saturating the intermediate representation with equalities. \nOnce saturated, the intermediate representation en\u00adcodes multiple optimized versions of the input program. \nAt this point, a pro.tability heuristic picks the .nal optimized program from the various programs represented \nin the saturated represen\u00adtation. Our proposed way of structuring optimizers has a variety of bene.ts \nover previous approaches: our approach obviates the need to worry about optimization ordering, enables \nthe use of a global optimization heuristic that selects among fully optimized programs, and can be used \nto perform translation validation, even on compil\u00aders other than our own. We present our approach, formalize \nit, and describe our choice of intermediate representation. We also present experimental results showing \nthat our approach is practical in terms of time and space overhead, is effective at discovering intricate \nop\u00adtimization opportunities, and is effective at performing translation validation for a realistic optimizer. \nCategories and Subject Descriptors D.3.4 [Programming Lan\u00adguages]: Processors Compilers; Optimization \nGeneral Terms Languages, Performance Keywords Compiler Optimization, Equality Reasoning, Interme\u00addiate \nRepresentation 1. Introduction In a traditional compilation system, optimizations are applied se\u00adquentially, \nwith each optimization taking as input the program pro\u00adduced by the previous one. This traditional approach \nto compilation has several well-known drawbacks. One of these drawbacks is that the order in which optimizations \nare run affects the quality of the * Supported in part by NSF CAREER grant CCF-0644306. Permission to \nmake digital or hard copies of all or part of this work for personal or classroom use is granted without \nfee provided that copies are not made or distributed for pro.t or commercial advantage and that copies \nbear this notice and the full citation on the .rst page. To copy otherwise, to republish, to post on \nservers or to redistribute to lists, requires prior speci.c permission and/or a fee. POPL 09, January \n18 24, 2009, Savannah, Georgia, USA. Copyright c . 2009 ACM 978-1-60558-379-2/09/01. . . $5.00 generated \ncode, a problem commonly known as the phase ordering problem. Another drawback is that pro.tability heuristics, \nwhich decide whether or not to apply a given optimization, tend to make their decisions one optimization \nat a time, and so it is dif.cult for these heuristics to account for the effect of future transformations. \nIn this paper, we present a new approach for structuring optimiz\u00aders that addresses the above limitations \nof the traditional approach, and also has a variety of other bene.ts. Our approach consists of computing \na set of optimized versions of the input program and then selecting the best candidate from this set. \nThe set of candidate optimized programs is computed by repeatedly inferring equiva\u00adlences between program \nfragments, thus allowing us to represent the effect of many possible optimizations at once. This, in \nturn, enables the compiler to delay the decision of whether or not an op\u00adtimization is pro.table until \nit observes the full rami.cations of that decision. Although related ideas have been explored in the \ncontext of super-optimizers, as Section 8 on related work will point out, super-optimizers typically \noperate on straight-line code, whereas our approach is meant as a general-purpose compilation paradigm \nthat can optimize complicated control .ow structures. At its core, our approach is based on a simple \nchange to the tra\u00additional compilation model: whereas traditional optimizations op\u00aderate by destructively \nperforming transformations, in our approach optimizations take the form of equality analyses that simply \nadd equality information to a common intermediate representation (IR), without losing the original program. \nThus, after each equality anal\u00adysis runs, both the old program and the new program are repre\u00adsented. \nThe simplest form of equality analysis looks for ways to instan\u00adtiate equality axioms like a * 0=0,or \na * 4=a << 2.How\u00adever, our approach also supports arbitrarily complicated forms of equality analyses, \nsuch as inlining, tail recursion elimination, and various forms of user de.ned axioms. The .exibility \nwith which equality analyses are de.ned makes it easy for compiler writers to port their traditional \noptimizations to our equality-based model: op\u00adtimizations can work as before, except that when the optimization \nwould have performed a transformation, it now simply records the transformation as an equality. The main \ntechnical challenge that we face in our approach is that the compiler s IR must now use equality information \nto represent not just one optimized version of the input program, but multiple versions at once. We address \nthis challenge through a new IR that compactly represents equality information, and as a result can simultaneously \nstore multiple optimized versions of the input program. After a program is converted into our IR, we \nrepeatedly apply equality analyses to infer new equalities until no more equalities can be inferred, \na process known as saturation. Once saturated with equalities, our IR compactly represents the various \npossible ways of computing the values from the original program modulo the given set of equality analyses \n(and modulo some bound in the case where applying equality analyses leads to unbounded expansion). Our \napproach of having optimizations add equality information to a common IR until it is saturated with equalities \nhas a variety of bene.ts over previous optimization models. Optimization order does not matter. The .rst \nbene.t of our ap\u00adproach is that it removes the need to think about optimization order\u00ading. When applying \noptimizations sequentially, ordering is a prob\u00adlem because one optimization, say A, may perform some \ntransfor\u00admation that will irrevocably prevent another optimization, say B, from triggering, when in fact \nrunning B .rst would have produced the better outcome. This so-called phase ordering problem is ubiq\u00aduitous \nin compiler design. In our approach, however, the compiler writer does not need to worry about ordering, \nbecause optimiza\u00adtions do not destructively update the program they simply add equality information. \nTherefore, after an optimization A is applied, the original program is still represented (along with \nthe transformed program), and so any optimization B that could have been applied before A is still applicable \nafter A. Thus, there is no way that apply\u00ading an optimization A can irrevocably prevent another optimization \nB from applying, and so there is no way that applying optimiza\u00adtions will lead the search astray. As \na result, compiler writers who use our approach do not need to worry about the order in which optimizations \nrun. Better yet, because optimizations are allowed to freely interact during equality saturation, without \nany considera\u00adtion for ordering, our approach can discover intricate optimization opportunities that \ncompiler writers may not have anticipated, and hence would not have implemented in a general purpose \ncompiler. Global pro.tability heuristics. The second bene.t of our ap\u00adproach is that it enables global \npro.tability heuristics.Even if there existed a perfect order to run optimizations in, compiler writers \nwould still have to design pro.tability heuristics for determining whether or not to perform certain \noptimizations such as inlining. Unfortunately, in a traditional compilation system where optimiza\u00adtions \nare applied sequentially, each heuristic decides in isolation whether or not to apply an optimization \nat a particular point in the compilation process. The local nature of these heuristics makes it dif.cult \nto take into account the effect of future optimizations. Our approach, on the other hand, allows the \ncompiler writer to design pro.tability heuristics that are global in nature. In particu\u00adlar, rather than \nchoosing whether or not to apply an optimization locally, these heuristics choose between fully optimized \nversions of the input program. Our approach makes this possible by sepa\u00adrating the decision of whether \nor not a transformation is applicable from the decision of whether or not it is pro.table.Indeed,using \nan optimization to add an equality in our approach does not indi\u00adcate a decision to perform the transformation \n the added equality just represents the option of picking that transformation later. The actual decision \nof which transformations to apply is performed by a global heuristic after our IR has been saturated \nwith equalities. This global heuristic simply chooses among the various optimized versions of the input \nprogram that are represented in the saturated IR, and so it has a global view of all the transformations \nthat were tried and what programs they generated. There are many ways to implement this global pro.tability \nheuristic, and in our prototype compiler we have chosen to imple\u00adment it using a Pseudo-Boolean solver \n(a form of Integer Linear Programming solver). In particular, after our IR has been saturated with equalities, \nwe use a Pseudo-Boolean solver and a static cost model for every node to pick the lowest-cost program \nthat com\u00adputes the same result as the original program. Translation validation. The third bene.t of our \napproach is that it can be used not only to optimize programs, but also to prove programs equivalent: \nintuitively, if two programs are found equal after equality saturation, then they are equivalent. Our \napproach can therefore be used to perform translation validation, a tech\u00adnique that consists of automatically \nchecking whether or not the optimized version of an input program is semantically equivalent to the original \nprogram. For example, we can prove the correctness of optimizations performed by existing compilers, \neven if our prof\u00aditability heuristic would not have selected those optimizations. Contributions. The \ncontributions of this paper can therefore be summarized as follows: We present a new approach for structuring \noptimizers. In our approach optimizations add equality information to a common IR that simultaneously \nrepresents multiple optimized versions of the input program. Our approach obviates the need to worry \nabout optimization ordering, it enables the use of a global op\u00adtimization heuristic (such as a Pseudo-Boolean \nsolver), and it can be used to perform translation validation for any compiler. Sections 2 and 3 present \nan overview of our approach and its ca\u00adpabilities, Section 4 makes our approach formal, and Section 5 \ndescribes the new IR that allows our approach to be effective.  We have instantiated our approach in \na new Java bytecode opti\u00admizer called Peggy (Section 6). Peggy uses our approach not only to optimize \nJava methods, but also to perform transla\u00adtion validation for existing compilers. Our experimental results \n(Section 7) show that our approach (1) is practical both in terms of time and space overhead, (2) is \neffective at discovering both simple and intricate optimization opportunities and (3) is effec\u00adtive at \nperforming translation validation for a realistic optimizer  Peggy is able to validate 98% of the runs \nof the Soot opti\u00admizer [35], and within the remaining 2% it uncovered a bug in Soot.  2. Overview Our \napproach for structuring optimizers is based on the idea of having optimizations propagate equality information \nto a common IR that simultaneously represents multiple optimized versions of the input program. The main \nchallenge in designing this IR is that it must make equality reasoning effective and ef.cient. To make \nequality reasoning effective, our IR needs to support the same kind of basic reasoning that one would \nexpect from sim\u00adple equality axioms like a * (b +c)=a * b +a * c, but with more complicated computations \nsuch as branches and loops. We have designed a representation for computations called Program Expres\u00adsion \nGraphs (PEGs) that meets these requirements. Similar to the gated SSA representation [34, 19], PEGs are \nreferentially transpar\u00adent, which intuitively means that the value of an expression de\u00adpends only on \nthe value of its constituent expressions, without any side-effects. As has been observed previously in \nmany contexts, referential transparency makes equality reasoning simple and effec\u00adtive. However, unlike \nprevious SSA-based representations, PEGs are also complete, which means that there is no need to maintain \nany additional representation such as a control .ow graph (CFG). Completeness makes it easy to use equality \nfor performing trans\u00adformations: if two PEG nodes are equal, then we can pick either one to create a \nwell-formed program, without worrying about the implications on any underlying representation. In addition \nto being effective, equality reasoning in our IR must be ef.cient. The main challenge is that each added \nequality can potentially double the number of represented programs, thus mak\u00ading the number of represented \nprograms exponential in the worst case. To address this challenge, we record equality information of \nPEG nodes by simply merging PEG nodes into equivalence classes. We call the resulting equivalence graph \nan E-PEG, and it is this E-PEG representation that we use in our approach. Using equivalence i:=0; i:= \n0; while (...) { while (...) { use(i * 5); use(i); i:=i +1; i:=i +5; if (...) { if (...) { i:=i +3; i:=i \n+15; }} }} (a) (b) Figure 1. Loop-induction-variable strength reduction: (a) shows the original code, \nand (b) shows the optimized code. classes allows E-PEGs to ef.ciently represent exponentially many ways \nof expressing the input program, and it also allows the equal\u00adity saturation engine to ef.ciently take \ninto account previously dis\u00adcovered equalities. Among existing IRs, E-PEGs are unique in their ability \nto represent multiple optimized versions of the input pro\u00adgram. A more detailed discussion of how PEGs \nand E-PEGs relate to previous IRs can be found in Section 8. We illustrate the main features of our approach \nby showing how it can be used to implement loop-induction-variable strength reduc\u00adtion. The idea behind \nthis optimization is that if all assignments to a variable i in a loop are increments, then an expression \ni*c in the loop (with c being loop invariant) can be replaced with i, provided all the increments of \ni in the loop are appropriately scaled by c. As an example, consider the code snippet from Figure 1(a). \nThe use of i*5 inside the loop can be replaced with i as long as the two increments in the loop are scaled \nby 5. The resulting code is shown in Figure 1(b). 2.1 Program Expression Graphs A Program Expression \nGraph (PEG) is a graph containing: (1) operator nodes, for example plus , minus , or any of our built-in \nnodes for representing conditionals and loops (2) data.ow edges that specify where operator nodes get \ntheir arguments from. As an example, the PEG for the use of i*5 in Figure 1(a) is shown in Figure 2(a). \nAt the very top of the PEG we see node 1, which represents the i*5 multiply operation from inside the \nloop. Each PEG node represents an operation, with the children nodes being the arguments to the operation. \nThe links from parents to children are shown using solid (non-dashed) lines. For example, node 1 represents \nthe multiplication of node 2 by the constant 5. PEGs follow the notational convention used in E-graphs \n[26, 27, 13] and Abstract Syntax Trees (ASTs) of displaying operators above the arguments that .ow into \nthem, which is the opposite convention typically used in Data.ow Graphs [11, 2]. We use the E-graph/AST \norientation because we think of PEGs as recursive expressions. Node 2 in our PEG represents the value \nof variable i inside the loop, right before the .rst instruction in the loop is executed. We use . nodes \nto represent values that vary inside of a loop. Intuitively, the left child of a . node computes the \ninitial value, whereas the right child computes the value at the current iteration in terms of the value \nat the previous iteration. In our example, the left child of the . node is the constant 0, representing \nthe initial value of i. The right child of the . node uses nodes 3, 4, and 5 to compute the value of \ni at the current iteration in terms of the value of i from the previous iteration. The two plus nodes \n(nodes 4 and 5) represent the two increments of i in the loop, whereas the f node (node 3) represents \nthe merging of the two values of i produced by the two plus nodes. As in gated SSA [34, 19], our f nodes \nare executable: the .rst (left-most) argument to f is a selector that is used to select between the second \nand the third argument. Our example doesn t use the branch condition in an interesting way, and so we \njust let d represent the PEG sub-graph that computes the branch condition. From a more formal point of \nview, each . node produces a sequence of values, one value for each iteration of the loop. The .rst argument \nof a . node is the value for the .rst iteration, whereas the second argument is a sequence that represents \nthe values for the remaining iterations. For example, in Figure 2, the nodes labeled 3 through 5 compute \nthis sequence of remaining values in terms of the sequence produced by the . node. In particular, nodes \n3, 4 and 5 have been implicitly lifted to operate on this sequence. PEGs are well-suited for equality \nreasoning because all PEG operators, even those for branches and loops, are mathematical functions with \nno side effects. As a result, PEGs are referentially transparent, which allows us to perform the same \nkind of equality reasoning that one is familiar with from mathematics. Even though all PEG operators \nare pure, PEGs can still represent programs with state by using heap summary nodes. Section 6 explains \nhow our Peggy compiler uses such heap summary nodes to represent the state of Java objects.  2.2 Encoding \nequalities using E-PEGs A PEG by itself can only represent a single way of expressing the input program. \nTo represent multiple optimized versions of the in\u00adput program, we need to encode equalities in our representation. \nTo this end, an E-PEG is a graph that groups together PEG nodes that are equal into equivalence classes. \nAs an example, Figure 2(b) shows the E-PEG that our engine produces from the PEG of Fig\u00adure 2(a). We \ndisplay equalities graphically by adding a dashed edge between two nodes that have become equal. These \ndashed edges are only a visualization mechanism. In reality, PEG nodes that are equal are grouped together \ninto an equivalence class. Reasoning in an E-PEG is done through the application of opti\u00admizations, which \nin our approach take the form of equality analy\u00adses that add equality information to the E-PEG. An equality \nanal\u00adysis consists of two components: a trigger, which is an expression pattern stating the kinds of \nexpressions that the analysis is inter\u00adested in, and a callback function, which should be invoked when \nthe trigger pattern is found in the E-PEG. The saturation engine continuously monitors all the triggers \nsimultaneously, and invokes the necessary callbacks when triggers match. When invoked, a call\u00adback function \nadds the appropriate equalities to the E-PEG. The simplest form of equality analysis consists of instantiating \naxioms such as a*0=0. In this case, the trigger would be a*0,and the callback function would add the \nequality a*0=0. Even though the vast majority of our reasoning is done through such declarative axiom \napplication, our trigger and callback mechanism is much more general, and has allowed us to implement \nequality analyses such as inlining, tail-recursion elimination, and constant folding. The following three \naxioms are the equality analyses required to perform loop-induction-variable strength reduction. They \nstate that multiplication distributes over addition, .,and f: (a +b)* m =a * m +b * m (1) .(a, b)* m \n=.(a * m, b * m) (2) f(a, b, c)* m =f(a, b * m, c * m) (3) After a program is converted to a PEG, a saturation \nengine repeatedly applies equality analyses until either no more equalities can be added, or a bound \nis reached on the number of expressions that have been processed by the engine. As Section 7 will describe \nin more details, our experiments show that 84% of methods can be completely saturated, without any bounds \nbeing imposed. Figure 2(b) shows the saturated E-PEG that results from apply\u00ading the above distributivity \naxioms, along with a simple constant folding equality analysis. In particular, distributivity is applied \nfour times: axiom (2) adds equality edge A, axiom (3) edge B, axiom (1) edge C, and axiom (1) edge D. \nOur engine also applies the constant folding equality analysis to show that 0* 5=0, 3* 5= 15and   \n               (c)  Figure 2. Loop-induction-variable Strength Reduction using PEGs: \n(a) shows the original PEG, (b) shows the E-PEG that our engine produces from the original PEG and (c) \nshows the optimized PEG, which results by choosing nodes 6, 8, 10, and 12 from (b). 1* 5=5. Note that \nwhen axiom (2) adds edge A, it also adds node 7, which then enables axiom (3). Thus, equality analyses \nessentially communicate with each other by propagating equalities through the E-PEG. Furthermore, note \nthat the instantiation of axiom (1) adds node 12 to the E-PEG, but it does not add the right child of \nnode 12, namely .(...)* 5, because it is already represented in the E-PEG. Once saturated with equalities, \nan E-PEG compactly represents multiple optimized versions of the input program in fact, it com\u00adpactly \nrepresents all the programs that could result from applying the optimizations in any order to the input \nprogram. For example, the E-PEG in Figure 2(b) encodes 128 ways of expressing the orig\u00adinal program (because \nit encodes 7 independent equalities, namely the 7 dashed edges). In general, a single E-PEG can ef.ciently \nrep\u00adresent exponentially many ways of expressing the input program. After saturation, a global pro.tability \nheuristic can pick which optimized version of the input program is best. Because this prof\u00aditability \ncan inspect the entire E-PEG at once, it has a global view of the programs produced by various optimizations, \nafter all other optimizations were also run. In our example, starting at node 1, by choosing nodes 6, \n8, 10, and 12, we can construct the graph in Fig\u00adure 2(c), which corresponds exactly to performing loop-induction\u00advariable \nstrength reduction in Figure 1(b). More generally, when optimizing an entire function, one has to pick \na node for the equivalence class of the return value and nodes for all equivalence classes that the return \nvalue depends on. There are many plausible heuristics for choosing nodes in an E-PEG. In our Peggy implementation, \nwe have chosen to select nodes using a Pseudo-Boolean solver, which is an Integer Linear Programming \nsolver where variables are constrained to 0 or 1. In particular, we use a Pseudo-Boolean solver and a \nstatic cost model for every node to compute the lowest-cost program that is encoded in the E-PEG. In \nthe example from Figure 2, the Pseudo-Boolean solver picks the nodes described above. Section 6.3 describes \nour technique for selecting nodes in more detail.  2.3 Bene.ts of our approach Optimization order does \nnot matter. To understand how our ap\u00adproach addresses the phase ordering problem, consider a simple peephole \noptimization that transforms i*5into i<<2+i. On the surface, one may think that this transformation should \nal\u00adways be performed if it is applicable after all, it replaces a multi\u00adplication with the much cheaper \nshift and add. In reality, however, this peephole optimization may disable other more pro.table trans\u00adformations. \nThe code from Figure 1(a) is such an example: trans\u00adforming i*5to i<<2+idisables loop-induction-variable \nstrength reduction, and therefore generates code that is worse than the one from Figure 1(b). The above \nexample illustrates the ubiquitous phase ordering problem. In systems that apply optimizations sequentially, \nthe qual\u00adity of the generated code depends on the order in which optimiza\u00adtions are applied. Whit.eld \nand Soffa [40] have shown experimen\u00adtally that enabling and disabling interactions between optimizations \noccur frequently in practice, and furthermore that the patterns of in\u00adteraction vary not only from program \nto program, but also within a single program. Thus, no one order is best across all compilation. A common \npartial solution consists of carefully considering all the possible interactions between optimizations, \npossibly with the help of automated tools, and then coming up with a carefully tuned sequence for running \noptimizations that strives to enable most of the bene.cial interactions. This technique, however, puts \na heavy burden on the compiler writer, and it also does not account for the fact that the best order \nmay vary between programs. At high levels of optimizations, some compilers may even run optimizations \nin a loop until no more changes can be made. Even so, if the compiler picks the wrong optimization to \nstart with, then no matter what optimizations are applied later, in any order, any number of times, the \ncompiler will not be able to reverse the disabling consequences of the .rst optimization. In our approach, \nthe compiler writer does not need to worry about the order in which optimizations are applied. The pre\u00advious \npeephole optimization would be expressed as the axiom i*5=i<<2+i. However, unlike in a traditional compi\u00adlation \nsystem, applying this axiom in our approach does not re\u00admove the original program from the representation \n it only adds information and so it cannot disable other optimizations. There\u00adfore, the code from Figure \n1(b) would still be discovered, even if the peephole optimization was run .rst. In essence, our approach \nis able to simultaneously explore all possible sequences of opti\u00admizations, while sharing work that is \ncommon across the various sequences. In addition to reducing the burden on compiler writers, remov\u00ading \nthe need to think about optimization ordering has two additional bene.ts. First, because optimizations \ninteract freely with no regard to order, our approach often ends up combining optimizations in unanticipated \nways, leading to surprisingly complicated optimiza\u00adtions given how simple our equality analyses are \nSection 3 gives such an example. Second, it makes it easier for end-user program\u00admers to add domain-speci.c \naxioms to the compiler, because they sum := 0; fo r (i := 0; i < 10; i++) { for (i := 0; i < 29; i += \n2) { for (i := 0; i < 10; i++) { for (j := 0; j < 10; j++) { use(i) for (j := 0; j < 10; j++) { use(i*10 \n+ j); } use(sum++); } use(i) (a) } } (b) } (c) (d) + eval pass 29 2 0 (e) 1 + 2 10 0 pass2 eval2 1 + \n2 1 0 (f) 1 +0 110 * 1 +0 2 + Figure 3. Various loops and their PEG representations. don t have to think \nabout where exactly in the compiler the axiom should be run, and in what order relative to other optimizations. \nGlobal pro.tability heuristics. Pro.tability heuristics in tradi\u00adtional compilers tend to be local in \nnature, making it dif.cult to take into account the effect of future optimizations. For example, con\u00adsider \ninlining. Although it is straightforward to estimate the direct cost of inlining (the code-size increase) \nand the direct bene.t of in\u00adlining (the savings from removing the call overhead), it is far more dif.cult \nto estimate the potentially larger indirect bene.t, namely the additional optimization opportunities \nthat inlining exposes. To see how inlining would affect our running example, con\u00adsider again the code \nfrom Figure 1(a), but assume that instead of use(i * 5), there was a call to a function f, and the use \nof i*5 occurred inside f.If f is suf.ciently large, a traditional inliner would not inline f, because \nthe code bloat would outweigh the call-overhead savings. However, a traditional inliner would miss the \nfact that it may still be worth inlining f, despite its size, be\u00adcause inlining would expose the opportunity \nfor loop-induction\u00advariable strength reduction. One solution to this problem consists of performing an \ninlining trial [12], where the compiler simulates the inlining transformation, along with the effect \nof subsequent op\u00adtimizations, in order to decide whether or not to actually inline. However, in the face \nof multiple inlining decisions (or more gen\u00aderally multiple optimization decisions), there can be exponentially \nmany possible outcomes, each one of which has to be compiled separately. In our approach, on the other \nhand, inlining simply adds an equality to the E-PEG stating that the call to a given function is equal \nto its body instantiated with the actual arguments. The result\u00ading E-PEG simultaneously represents the \nprogram where inlining is performed and where it is not. Subsequent optimizations then op\u00aderate on both \nof these programs at the same time. More generally, our approach can simultaneously explore exponentially \nmany pos\u00adsibilities in parallel, while sharing the work that is redundant across these various possibilities. \nIn the above example with inlining, once the E-PEG is saturated, a global pro.tability heuristic can \nmake a more informed decision as to whether or not to pick the inlined ver\u00adsion, since it will be able \nto take into account the fact that inlining enabled loop-induction-variable strength reduction. Translation \nValidation. Unlike traditional compilation frame\u00adworks, our approach can be used not only to optimize \nprograms, but also to establish equivalences between programs. In particular, if we convert two programs \ninto an E-PEG, and then saturate it with equalities, then we can conclude that the two programs are equivalent \nif they belong to the same equivalence class in the sat\u00adurated E-PEG. In this way, our approach can be \nused to perform translation validation for any compiler (not necessarily our own), by checking that each \nfunction in the input program is equivalent to the corresponding optimized function in the output program. \nFor example, our approach would be able to show that the two program fragments from Figure 1 are equivalent. \nFurther\u00admore, it would also be able to validate a compilation run in which i* 5= i<< 2+ i was applied \n.rst to Figure 1(a). This shows that we are able to perform translation validation regardless of what \noptimized program our own pro.tability heuristic would choose. Although our translation validation technique \nis intraprocedu\u00adral, we can use interprocedural equality analyses such as inlining to enable a certain \namount of interprocedural reasoning. This allows us to reason about transformations like reordering function \ncalls.  3. Reasoning about loops This section shows how our approach can be used to reason across nested \nloops. The example highlights the fact that a simple axiom set can produce unanticipated optimizations \nwhich traditional com\u00adpilers would have to explicitly search for. We start in Sections 3.1 and 3.2 by \ndescribing all PEG con\u00adstructs used to represent loops. We then show in Section 3.3 how our approach \ncan perform an inter-loop strength reduction opti\u00admization. 3.1 Single loop Consider the simple loop \nfrom Figure 3(a). This loop iterates 15 times, incrementing the value of i each time by 2. Assume that \nthe variable i is used inside the loop, and it is also used after the loop (as indicated by the two use \nannotations). The PEG for this code is shown in Figure 3(d). The value of i inside the loop is represented \nby a .node. Intuitively, this .node produces the sequence of values that i takes throughout the loop, \nin this case [0,2,4,...].The value of i after the loop is represented by the eval node at the top of \nthe PEG. Given a sequence sandanindex n, eval(s,n) produces the n th element of sequence s. To determine \nwhich element to select from a sequence, our PEG representation uses pass nodes. Given a sequence s of \nbooleans, pass(s) returns the index of the .rst element in the sequence that is true. In our example, \nthe = node uses the result of the . node to produce the sequence of values taken on by the boolean expression \ni = 29 throughout the loop. This sequence is then sent to pass, which in this case produces the value \n15, since the 15th value (counting from 0) of i in the loop (which is 30) is the .rst one to make i = \n29 true. The eval node then selects the 15th element of the sequence produced by the . node, which is \n30. In our previous example from Section 2, we omitted eval/pass from the PEG for clarity because we \nwere not Figure 4. E-PEG that results from running the saturation engine on the PEG from Figure 3(f). \nBy picking the nodes that are check\u00admarked, we get the PEG from Figure 3(e). interested in any of the \nvalues after the loop, the eval/pass nodes would not have been used in any reasoning. 3.2 Nested loops \nWe now illustrate, through an example, how nested loops can be encoded in our PEG representation. Consider \nthe code snippet from Figure 3(b), which has two nested loops. We are interested in the value of sum \ninside the loop, as indicated by the use annotation. The PEG for this code snippet is shown in Figure \n3(e). Each ., eval and pass node is labeled with a subscript indicating what loop depth it operates on \n(we previously omitted these subscripts for clarity). The node labeled suminner represents the value \nof sum at the beginning of the inner loop body. Similarly, sumouter is the value of sum at the beginning \nof the outer loop body. Looking at suminner , we can see that: (1) on the .rst iteration (the left child \nof suminner ), suminner gets the value of sum from the outer loop; (2) on other iterations, it gets one \nplus the value of sum from the previous iteration of the inner loop. Looking at sumouter , we can see \nthat: (1) on the .rst iteration, sumouter gets 0;on other iterations, it gets the value of sum right \nafter the inner loop terminates. The value of sum after the inner loop terminates is computed using a \nsimilar eval/pass pattern as in Figure 3(d). 3.3 Inter-loop strength reduction Our approach allows an \noptimizing compiler to perform intricate optimizations of looping structures. We present such an example \nhere, with a kind of inter-loop strength reduction. Consider the code snippet from Figure 3(c), which \nis equivalent to one we ve already seen in Figure 3(b). However, the code in 3(b) is faster because sum++ \nis cheaper than i * 10 + j. We show how our approach can transform the code in Figure 3(c) to the code \nin Figure 3(b). Figure 3(f) shows the PEG for i*10 + j, which will be the focus of our optimization. \nWe omit eval and pass in this PEG because they are not used in this example, except in one step that \nwe will make explicit. Figure 4 shows the saturated E-PEG that results from running the saturation engine \non the PEG from Figure 3(f). The checkmarks indicate which nodes will eventually be selected they can \nbe ignored for now. To make the graph more readable, we sometimes label nodes, and then connect an edge \ndirectly to a label name, rather than connecting it to the node with that label. For example, consider \nnode j in the E-PEG, which reads as .2(0, 1+ j).Rather than explicitly drawing an edge from + to j, we \nconnect + to a new copy of label j. In drawing Figure 4, we have already performed loop-induction variable \nstrength reduction on the left child of the topmost + from Function Optimize(cfg : CFG): CFG 1: let ir \n=ConvertToIR(cfg) 2: let saturated ir = Saturate(ir,A) 3: let best = SelectBest(saturated ir) 4: return \nConvertToCFG(best )  Figure 5. Optimization phase in our approach. We assume a global set A of equality \nanalyses to be run. Figure 3(f). In particular, this left child has been replaced with a new node i,where \ni = .1(0, 10 + i). We skip the steps in doing this because they are similar to the ones described in \nSection 2.2. Figure 4 shows the relevant equalities that our saturation engine would add. We describe \neach in turn. Edge A is added by distributing + over .2: i + .2(0, 1+ j)= .2(i +0, i +(1 + j)) Edge \nB is added because 0 is the identity of +, i.e.: i +0= i.  Edge C is added because addition is associative \nand commuta\u00adtive: i +(1 + j)=1+(i + j)  Edge D is added because 0, incremented n times, produces n: \n evale(ide, passe(ide = n)) = n where ide = .e(0, 1+ ide) This is an example of a loop optimization \nexpressible as a simple PEG axiom. Edge E is added by distributing + over the .rst child of eval2: eval2(j, \nk)+ i = eval2(j + i, k) Edge F is added because addition is commutative: j +i = i +j We use checkmarks \nin Figure 4 to highlight the nodes that Peggy would select using its Pseudo-Boolean pro.tability heuristic. \nThese nodes constitute exactly the PEG from Figure 3(e), meaning that Peggy optimizes the code in Figure \n3(c) to the one in Figure 3(b). Summary. This example illustrates several points. First, it shows how \na transformation that locally seems undesirable, namely trans\u00adforming the constant 10 into an expensive \nloop (edge D), in the end leads to much better code. Our global pro.tability heuristic is perfectly suited \nfor taking advantage of these situations. Second, it shows an example of an unanticipated optimization, \nnamely an optimization that we did not realize would fall out from the simple equality analyses we already \nhad in place. In a traditional compi\u00adlation system, a specialized analysis would be required to perform \nthis optimization, whereas in our approach the optimization sim\u00adply happens without any special casing. \nIn this way, our approach essentially allows a few general equality analyses to do the work of many specialized \ntransformations. Finally, it shows how our ap\u00adproach is able to reason about complex loop interactions, \nsome\u00adthing that is beyond the reach of current super-optimizer-based techniques.  4. Formalization of \nour Approach Having given an intuition of how our approach works through examples, we now move to a formal \ndescription. Figure 5 shows the Optimize function, which embodies our approach. Optimize takes four steps: \n.rst, it converts the input CFG into an internal rep\u00adresentation of the program; second, it saturates \nthis internal repre\u00adsentation with equalities; third, it uses a global pro.tability heuris\u00adtic to select \nthe best program from the saturated representation; .\u00adnally, it converts the selected program back to \na CFG. An instantiation of our approach therefore consists of three components: (1) an IR where equality \nreasoning is effective, along with the translation functions ConvertToIR and ConvertToCFG, (2) a saturation \nengine Saturate, and (3) a global pro.tability heuristic SelectBest. Future sections will show how we \ninstantiate these three components in our Peggy compiler. Saturation Engine. The saturation engine Saturate \ninfers equal\u00adities by repeatedly running a set A of equality analyses. Given an a equality analysis a \n.A,we de.ne ir1 .ir2 to mean that ir1 pro\u00adduces ir2 when the equality analysis a runs and adds some equali\u00adties \nto ir1.If a chooses not to add any equalities, then ir2 is simply thesameas ir1. We de.ne a partial order \n.on IRs, based on the equalities they encode: ir1 . ir2 iff the equalities in ir1 are a subset of the \nequalities in ir2. Immediately from this de.nition, we get: a (ir1 .ir2) .ir1 .ir2 (4) We de.ne an equality \nanalysis a to be monotonic iff: a'a''' (ir1 .ir2) .(ir1 .ir1) .(ir2 .ir2) .(ir1 .ir2) Intuitively, our \napproach addresses the phase ordering problem because applying an equality analysis a before b cannot \nmake b less effective, as stated in the following non-interference theorem. THEOREM 1. If a and b are \nmonotonic then: abb (ir1 .ir2) .(ir2 .ir3) .(ir1 .ir4) .(ir4 .ir3) The above follows immediately from \nmonotonicity and Property 4. We now de.ne ir1 .ir2 as: a ir1 .ir2 .. .a .A. (ir1 .ir2 .ir1= ir2) The \n.relation formalizes one step taken by the saturation engine. We also de.ne . * to be the re.exive transitive \nclosure of ..The . * relation formalizes an entire run of the saturation engine. Given a set A of monotonic \nequality analyses, if the saturation engine terminates, then it is canonizing, where canonizing means \nthat for any ir1, there is a unique ir2 with the following properties: (1) ir1 . * ir2 and (2) there \nis no ir3 such that ir2 . ir3.In this case the saturation engine computes this canonical saturated IR, \nwhich means that Optimize returns the same result no matter what order optimizations run in. Because \nin general saturation may not terminate, we bound the number of times that analyses can run. In this \ncase we cannot pro\u00advide the same canonizing property, but the non-interference theo\u00adrem (Theorem 1) still \nimplies that no area of the search space can be made unreachable by applying an equality analysis (a \nproperty that traditional compilation systems lack).  5. PEGs and E-PEGs The .rst step in instantiating \nour approach from the previous sec\u00adtion is to pick an appropriate IR. To this end, we have designed a \nnew IR called the E-PEG which can simultaneously represent mul\u00adtiple optimized versions of the input \nprogram. We .rst give a for\u00admal description of our IR (Section 5.1), then we present its bene.ts (Section \n5.2), and .nally we give a high-level description of how to translate from CFGs to our IR and back (Section \n5.3). 5.1 Formalization of PEGs and E-PEGs APEG is atriple (N , L, C ),where N is a set of nodes, L \n: N .F is a labeling that maps each node to a semantic function from a set of semantic functions F ,and \nC : N .list[N ] is a function that maps each node to its children (i.e. arguments). Types. Before giving \nthe de.nition of semantic functions, we .rst de.ne the types of values that these functions operate over. \nValues that .ow through a PEG are lifted in two ways. First, they are .-lifted, meaning that we add the \nspecial value .to each type domain. The .value indicates that the computation fails or does not terminate. \nFormally, for each type t ,we de.ne tb= t .{.}. Second, values are loop-lifted, which means that instead \nof representing the value at a particular iteration, PEG nodes represent values for all iterations at \nthe same time. Formally, we let Lbe a set of loop identi.ers, with each f .Lrepresenting a loop from \nthe original code (in our previous examples we used integers). We assume a partial order =that represents \nthe loop nesting structure: f<f' means that f' is nested within f. An iteration index i captures the \niteration state of all loops in the PEG. In particular, i is a function that maps each loop identi.er \nf .Lto the iteration that loop f is currently on. Suppose for example that there are two nested loops \nin the program, identi.ed as f1 and f2. Then the iteration index i =[f1 . 5,f2.3] represents the state \nwhere loop f1 is on the 5th iteration and loop f2 is on the 3rd iteration. We let I = L.N be the set \nof all loop iteration indices (where N denotes the set of non-negative integers). For i . I,we use the \nnotation i[f . v] to denote a function that returns the same value as i on all inputs, except that it \nreturns v on input f.The output of a PEG node is a map from loop iteration indices in I to values. In \nparticular, for each type t , we de.ne a loop-lifted version te= I .tb. PEG nodes operate on these loop-lifted \ntypes.  Semantic Functions. The set of semantic functions F is divided into two: F = Prims .Domain where \nPrims contains the primitive functions like f and ., which are built into the PEG representation, whereas \nDomain contains semantic functions for particular domains like arithmetic. Figure 6 gives the de.nition \nof the primitive functions Prims = {f, .e, evale, pass}. These functions are polymorphic in t ,in that \ne they can be instantiated for various t s, ranging from basic types like integers and strings to complicated \ntypes like the heap sum\u00admary nodes that Peggy uses to represent Java objects. The de.ni\u00adtions of evale \nand passe make use of the function monotonizee, whose de.nition is given in Figure 6. The monotonizee \nfunction transforms a sequence so that, once an indexed value is unde.ned, all following indexed values \nare unde.ned. The monotonizee function formalizes the fact that once a value is unde.ned at a given loop \niteration, the value remains unde.ned at subsequent iterations. The domain semantic functions are de.ned \nas Domain = {e op |op .DomainOp},where DomainOp is a set of domain operators (like +, *and -in the case \nof arithmetic), and e-lifted, and op is a .then loop-lifted version of op. Intuitively, the .-lifted \nversion of an operator works like the original operator except that it returns .if any of its inputs \nare ., and the loop-lifted version of an operator applies the original operator for each loop index. \nAs an example, the semantic function of + in a PEG is +e,and the semantic function of 1 is e1 (since \nconstants like 1 are simply nullary operators). However, to make the notation less crowded, we omit the \ntildes on all domain operators. Node Semantics. For a PEG node n .N , we denote its semantic value by \n[n]. We assume that [\u00b7] is lifted to sequences list[N ] in the standard way. The semantic value of n \nis de.ned as: [n] = L(n)([C (n)]) (5) Equation 5 is essentially the evaluation semantics for expressions. \nThe only complication here is that our expression graphs are recur\u00adsive. In this setting, one can think \nof Equation 5 as a set of recursive equations to be solved. To guarantee that a unique solution exists, \nwe impose some well-formedness constraints on PEGs. f : Be\u00d7te\u00d7te.te.e : te\u00d7te.te 8 > <if cond(i)= . \nthen . (if i(f)=0 then base(i) f(cond,t,f)(i)= if cond(i)= true then t(i) .e(base, loop)(i)= >if i(f) \n> 0 then loop(i[f .i(f) -1]) : if cond(i)= false then f(i) evale : te\u00d7Ne.tepasse : Be.Ne (( if idx(i)= \n. then . if I= \u00d8 then . evale(loop, idx)(i)= pass(cond )(i)= else monotonizee(loop)(i[f .idx(i)]) eif \nI= \u00d8 then min I where I= {i .N |monotonizee(cond)(i[f .i]) = true} where monotonizee : te.teis de.ned \nas: ( if .0 =i< i(f). value(i[f .i]) = . then . monotonizee(value)(i)= if .0 =i< i(f). value(i[f .i]) \n= . then value(i) Figure 6. De.nition of primitive PEG functions. The important notation: Lis the set \nof loop identi.ers, N is the set of non-negative integers, B is the set of booleans, I = L.N, tb= t .{.},and \nte= I .tb. DEFINITION 1. A PEG is well-formed iff: 1. All cycles pass through the second child edge of \na . 2. A path from a .e, evale,or passe to a .e. implies f ' =f or the path passes through the .rst \nchild edge of an evale. or passe.  3. All cycles containing evale or passe contain some .e. with  f \n' <f Condition 1 states that all cyclic paths in the PEG are due to looping constructs. Condition 2 states \nthat a computation in an outer-loop cannot reference a value from inside an inner-loop. Condition 3 states \nthat the .nal value produced by an inner-loop cannot be expressed in terms of itself, except if it s \nreferencing the value of the inner-loop from a previous outer-loop iteration. THEOREM 2. If a PEG is \nwell-formed, then for each node n in the PEG there is a unique semantic value [n] satisfying Equation \n5. The proof is by induction over the strongly-connected-component DAG of the PEG and the loop nesting \nstructure =. Evaluation Semantics. The meaning function [\u00b7] can be eval\u00aduated on demand, which provides \nan executable semantics for PEGs. For example, suppose we want to know the result of evale(x, passe(y)) \nat some iteration state i. To determine which case of evale s de.nition we are in, we must evaluate passe(y) \non i. From the de.nition of passe, we must compute the minimum i that makes y true. To do this, we iterate \nthrough values of i until we .nd an appropriate one. The value of i we ve found is the number of times \nthe loop iterates, and we can use this i back in the evale function to extract the appropriate value \nout of x. This example shows how an on-demand evaluation of an eval/pass sequence essentially leads to \nthe traditional operational semantics for loops. E-PEG Semantics. An E-PEG is a PEG with a set of equalities \nE between nodes. An equality between n and n ' denotes value equality: [n] = [n ' ].The set E forms an \nequivalence relation, which in turn partitions the PEG nodes into equivalence classes. Built-in Axioms. \nWe have developed a set of PEG built-in axioms that state properties of the primitive semantic functions. \nThese axioms are used in our approach as a set of equality analyses that enable reasoning about primitive \nPEG operators. Some important built-in axioms are given below, where denotes don t care : .e(A, B)= .e(evale(A, \n0),B) evale(.e(A, ), 0) = evale(A, 0) evale(evale(A, B),C)= evale(A, evale(B, C)) passe(true)=0 passe(.e(true, \n)) = 0 passe(.e(false,A)) = passe(A)+1 One of the bene.ts of having a well-de.ned semantics for primi\u00adtive \nPEG functions is that we can reason formally about these func\u00adtions. In particular, using our semantics, \nwe have proved all the axioms presented in this paper.  5.2 How PEGs enable our approach The key feature \nof PEGs that makes our equality-saturation ap\u00adproach effective is that they are referentially transparent, \nwhich in\u00adtuitively means that the value of an expression depends only on the values of its constituent \nexpressions [5]. In our PEG representation, referential transparency can be formalized as follows: \u00ab \nL(n)= L(n ' ). .(n, n ' ) .N 2 . .[n] = [n ' ] [C (n)] = [C (n ' )] This property follows from the de.nition \nin Equation (5), and the fact that for any n, L(n) is a pure mathematical function. Referential transparency \nmakes equality reasoning effective be\u00adcause it allows us to show that two expressions are equal by only \nconsidering their constituent expressions, without having to worry about side-effects. Furthermore, referential \ntransparency has the bene.t that a single node in the PEG entirely captures the value of a complex program \nfragment, enabling us to record equivalences between program fragments by using equivalence classes of \nnodes. Contrast this to CFGs, where to record equality between complex program fragments, one would have \nto record subgraph equality. Finally, PEGs allow us to record equalities at the granularity of individual \nvalues, for example the iteration count in a loop, rather than at the level of the entire program state. \nAgain, contrast this to CFGs, where the simplest form of equality between program fragments would record \nprogram-state equality. 5.3 Translating between CFGs and PEGs To incorporate PEGs and E-PEGs into our \napproach, we have developed the ConvertToIR and ConvertToCFG functions from Figure 5. We give only a \nbrief overview of these algorithms, with more details in a technical report [33]. Transforming a CFG \ninto a PEG. The key challenge in con\u00adstructing a PEG from a CFG is determining the branching structure \nof the CFG. We perform this task with a function that, given paths from one CFG basic block to another, \nproduces a PEG expression with f, eval,and pass operations, specifying which path is taken under which \nconditions. We use this to determine the break condi\u00adtions of loops and the general branching structure \nof the CFG. We also identify nesting depth, entry points, and back-edges of loops to construct . nodes. \nWe piece these components together with the instructions of each basic block to produce the PEG. Lastly, \nwe ap\u00adply some basic simpli.cations to remove conversion artifacts. Our conversion algorithm from CFG \nto PEG handles arbitrary control .ow, including irreducible CFGs (by .rst converting them so they become \nreducible). Transforming a PEG into a CFG. Intuitively, our algorithm .rst groups parts of the PEG into \nsub-PEGs; then it recursively converts these sub-PEGs into CFGs; and .nally it combines these sub-CFGs \ninto a CFG for the original PEG. The grouping is done as follows: f nodes are grouped together whose \nconditions are equal, thus performing branch fusion; . nodes are grouped together that have equal pass \nconditions, thus performing loop fusion. The pure mathematical nature of PEGs makes it easy to identify \nwhen two conditions are equal, which makes branch/loop fusion simple to implement. PEGs follow the insight \nfrom Click of separating code place\u00adment issues from the IR [9]. In particular, PEGs do not represent \ncode placement explicitly. Instead, placement is performed during the translation back to a CFG. As a \nresult, the translation from a CFG to a PEG and back (without any saturation) ends up perform\u00ading a variety \nof optimizations: Constant Propagation, Copy Prop\u00adagation, Common Subexpression Elimination, Partial \nRedundancy Elimination, Unused Assignment Elimination, Unreachable Code Elimination, Branch Fusion, Loop \nFusion, Loop Invariant Branch Hoisting/Sinking, Loop Invariant Code Hoisting/Sinking, and Loop Unswitching. \n  6. The Peggy Instantiation We have instantiated our approach in a Java bytecode optimizer called Peggy. \nRecall from Figure 5 that an instantiation of our approach consists of three components: (1) an IR where \nequal\u00adity reasoning is effective, along with the translation functions ConvertToIR and ConvertToCFG, \n(2) a saturation engine Saturate, and (3) a global pro.tability heuristic SelectBest. We now describe \nhow each of these three components work in Peggy. 6.1 Intermediate Representation Peggy uses the PEG \nand E-PEG representations which, as ex\u00adplained in Section 5, are well suited for our approach. Because \nPeggy is a Java bytecode optimizer, an additional challenge is to en\u00adcode Java-speci.c concepts like \nthe heap and exceptions in PEGs. Heap. We model the heap using heap summaries which we call s nodes. \nAny operation that can read and/or write some object state may have to take and/or return additional \ns values. Because Java stack variables cannot be modi.ed except by direct assignments, operations on \nstack variables are precise in our PEGs and do not involve s nodes. None of these decisions of how to \nrepresent the heap are built into the PEG representation. As with any heap sum\u00admarization strategy, one \ncan have different levels of abstraction, and Function Saturate(peg : PEG,A : set of analyses): EPEG \n1: let epeg = CreateInitialEPEG(peg) 2: while .(p, f) . A, subst . S . subst =Match(p, epeg) do 3: epeg \n:= AddEqualities(epeg,f(subst, epeg)) 4: return epeg Figure 7. Peggy s Saturation Engine. We use S to \ndenote the set of all substitutions from pattern nodes to E-PEG nodes. we have simply chosen one where \nall objects are put into a single summarization object s. Exceptions. In order to maintain the program \nstate at points where exceptions are thrown, we bundle the exception state into our abstraction of the \nheap, namely the s summary nodes. As a result, operations like division which may throw an exception, \nbut do not otherwise modify the heap, now take and return a s node (in addition to their regular parameters \nand return values). This forces the observable state at the point where an exception is thrown to be \npreserved by our optimization process. Furthermore, to preserve Java semantics, Peggy does not perform \nany optimizations across try/catch boundaries or synchronization boundaries. 6.2 Saturation Engine The \nsaturation engine s purpose is to repeatedly dispatch equality analyses. In our implementation an equality \nanalysis is a pair (p, f) where p is a trigger, which is an E-PEG pattern with free variables, and f \nis a callback function that should be run when the pattern p is found in the E-PEG. While running, the \nengine continuously monitors the E-PEG for the presence of the pattern p,and when it is discovered, the \nengine constructs a matching substitution,which is a map from each node in the pattern to the corresponding \nE-PEG node. At this point, the engine invokes f with this matching substitution as a parameter, and f \nreturns a set of equalities that the engine adds to the E-PEG. In this way, an equality analysis will \nbe invoked only when events of interest to it are discovered. Furthermore, the analysis does not need \nto search the entire E-PEG because it is provided with the matching substitution. Figure 7 shows the \npseudo-code for Peggy s saturation engine. The call to CreateInitialEPEG on the .rst line takes the input \nPEG and generates an E-PEG that initially contains no equality information. The Match function invoked \nin the loop condition performs pattern matching: if an analysis trigger occurs inside an E-PEG, then \nMatch returns the matching substitution. Once a match occurs, the saturation engine uses AddEqualities \nto add the equalities computed by the analysis into the E-PEG. A remaining concern in Figure 7 is how \nto ef.ciently implement the existential check on line 2. To address this challenge, we adapt techniques \nfrom the AI community. In particular, the task of .nd\u00ading the matches on line 2 is similar to the task \nof determining when rules .re in rule-based expert or planning systems. These systems make use of an \nef.cient pattern matching algorithm called the Rete algorithm [17]. Intuitively, the Rete algorithm stores \nthe state of partially completed matches in a set of FSMs, and when new infor\u00admation is added to the \nsystem, it transitions the appropriate FSM. Our saturation engine uses an adaptation of the Rete algorithm \nfor the E-PEG domain to ef.ciently implement the check on line 2. In general, equality saturation may \nnot terminate. Termination is also a concern in traditional compilers where, for example, inlin\u00ading recursive \nfunctions can lead to unbounded expansion. By using triggers to control when equality edges are added \n(a technique also used in automated theorem provers), we can often avoid in.nite ex\u00adpansion. The trigger \nfor an equality axiom typically looks for the left-hand-side of the equality, and then makes it equal \nto the right\u00adhand-side. On occasion, though, we use more restrictive triggers to avoid expansions that \nare likely to be useless. For example, the trig\u00adger for the axiom equating a constant with a loop expression \nused to add edge D in Figure 4 also checks that there is an appropriate pass expression. In this way, \nit does not add a loop to the E-PEG if there wasn t some kind of loop to begin with. Using our current \naxioms and triggers, our engine completely saturates 84% of the methods in our benchmarks. In the remaining \ncases, we impose a limit on the number of ex\u00adpressions that the engine fully processes (where fully processing \nan expression includes adding all the equalities that the expression triggers). To prevent the search \nfrom running astray and exploring a single in.nitely deep branch of the search space, we currently use \na breadth-.rst order for processing new nodes in the E-PEG. This traversal strategy, however, can be \ncustomized in the implementa\u00adtion of the Rete algorithm to better control the searching strategy in those \ncases where an exhaustive search would not terminate. 6.3 Global Pro.tability Heuristic Peggy s SelectBest \nfunction uses a Pseudo-Boolean solver called Pueblo [31] to select which nodes from an E-PEG to include \nin the optimized program. A Pseudo-Boolean problem is an integer linear programming problem where all \nthe variables have been restricted to 0 or 1. By using these 0-1 variables to represent whether or not \nnodes have been selected, we can encode the constraints that must hold for the selected nodes to be a \nwell-formed computation. In particular, for each node or equivalence class x, we de.ne a pseudo\u00adboolean \nvariable that takes on the value 1 (true) if we choose to evaluate x, and 0 (false) otherwise. The constraints \nthen state that: (1) we must select the equivalence class of the return value; (2) if an equivalence \nclass is selected, we must select one of its nodes; (3) if a node is selected, we must select its children \ns equivalence classes; (4) the chosen PEG is well-formed.  The cost model that we use assigns a constant \ncost Cn to each node n. In particular, Cn = basic cost (n) \u00b7 kdepth(n),where basic cost (n) accounts \nfor how expensive n is by itself, and kdepth(n) accounts for how often n is executed. We use depth(n) \nto denote the loop nesting depth of n,and k is simply a constant, which we have chosen as 20. Using Cn, \nthe objective function we P want to minimize is n.N Bn \u00b7 Cn,where N is the set of nodes in the E-PEG, \nand Bn is the pseudo-boolean variable for node n. Peggy asks Pueblo to minimize this objective function \nsubject to the well-formedness constraints described above. The nodes as\u00adsigned 1 in the solution that \nPueblo returns are selected to form the PEG that SelectBest returns. This PEG is the lowest-cost PEG \nthat is represented in the E-PEG, according to our cost model.  7. Evaluation In this section we use \nour Peggy implementation to validate three hypotheses about our approach for structuring optimizers: \nour ap\u00adproach is practical both in terms of space and time (Section 7.1), it is effective at discovering \nboth simple and intricate optimization opportunities (Section 7.2), and it is effective at performing \ntrans\u00adlation validation (Section 7.3). 7.1 Time and space overhead To evaluate the running time of the \nvarious Peggy components, we compiled SpecJVM, which comprises 2,461 methods. For 1% of these methods, \nPueblo exceeded a one minute timeout we imposed on it, in which case we just ran the conversion to PEG \nand back. We imposed this timeout because in some rare cases, Pueblo runs too long to be practical. The \nfollowing table shows the average time in milliseconds taken per method for the 4 main Peggy phases (for \nPueblo, a timeout counts as 60 seconds). CFG to PEG Saturation Pueblo PEG to CFG Time 13.9 ms 87.4 ms \n1,499 ms 52.8 ms All phases combined take slightly over 1.5 seconds. An end\u00adto-end run of Peggy is on \naverage 6 times slower than Soot with all of its intraprocedural optimizations turned on. Nearly all \nof our time is spent in the Pseudo-Boolean solver. We have not focused our efforts on compile-time, and \nwe conjecture there is signi.cant room for improvement, such as better pseudo-boolean encodings, or other \nkinds of pro.tability heuristics that run faster. Since Peggy is implemented in Java, to evaluate memory \nfoot\u00adprint, we limited the JVM to a heap size of 200 MB, and observed that Peggy was able to compile \nall the benchmarks without running out of memory. In 84% of compiled methods, the engine ran to complete \nsatu\u00adration, without imposing bounds. For the remaining cases, the en\u00adgine limit of 500 was reached, \nmeaning that the engine ran until fully processing 500 expressions in the E-PEG, along with all the equalities \nthey triggered. In these cases, we cannot provide a com\u00adpleteness guarantee, but we can give an estimate \nof the size of the explored state space. In particular, using just 200 MB of heap, our E-PEGs represented \nmore than 2103 versions of the input program (using geometric average).  7.2 Implementing optimizations \nThe main goal of our evaluation is to demonstrate that common, as well as unanticipated, optimizations \nresult in a natural way from our approach. To achieve this, we implemented a set of basic equality analyses, \nlisted in Figure 8(a). We then manually browsed through the code that Peggy generates on a variety of \nbenchmarks (including SpecJVM) and made a list of the optimizations that we observed. Figure 8(b) shows \nthe optimizations that we observed fall out from our approach using equality analyses 1 through 6, and \nFigure 8(c) shows optimizations that we observed fall out from our approach using equality analyses 1 \nthrough 7. With effort similar to what would be required for a compiler writer to implement the optimizations \nfrom part (a), our approach enables the more advanced optimizations from parts (b) and (c). Peggy performs \nsome optimizations (for example 15 through 19) that are quite complex given the simplicity of its equality \nanaly\u00adses. To implement such optimizations in a traditional compiler, the compiler writer would have \nto explicitly design a pattern that is speci.c to those optimizations. In contrast, with our approach \nthese optimizations fall out from the interaction of basic equality analy\u00adses without any additional \ndeveloper effort, and without specifying an order in which to run them. Essentially, Peggy .nds the right \nsequence of equality analyses to apply for producing the effect of these complex optimizations. In terms \nof generated-code quality, Peggy generates code whose performance is comparable to the code generated \nby Soot s in\u00adtraprocedural optimizations, which include: common sub-expression elimination, lazy code \nmotion, copy propagation, constant propa\u00adgation, constant folding, conditional branch folding, dead assign\u00adment \nelimination, and unreachable code elimination. However, in\u00adtraprocedural optimizations on Java bytecode \ngenerally produce only small gains (on the order of a few percent). In the rare cases where signi.cant \ngains are to be had from intraprocedural opti\u00admizations, Peggy excelled. Soot can also perform interprocedural \noptimizations, such as class-hierarchy-analysis, pointer-analysis, and method-specialization. We did \nnot enable these optimizations when performing our comparison against Soot, because we have not yet attempted \nto express any interprocedural optimizations. We (a) Equality Analyses Description 1. Built-in E-PEG \naxioms Axioms from Section 5.1 stating properties of primitive PEG nodes (f, ., eval, pass) 2. Basic \nArithmetic Axioms Axioms stating properties of arithmetic operators like +, -, *, /, <<, >> 3. Constant \nFolding Equates a constant expression with its constant value 4. Java-speci.c Axioms Axioms stating properties \nof Java-speci.c operators like .eld and array accesses 5. Tail-recursion Elimination Replaces the body \nof a tail-recursive procedure with a loop 6. Method Inlining Inlining based on intraprocedural class \nanalysis to disambiguate dynamic dispatch 7. Domain-speci.c Axioms User-provided axioms about certain \napplication domains (optional) (b) Optimizations Description 8. Constant Propagation and Folding Traditional \nConstant Propagation and Folding 9. Algebraic Simpli.cation Various forms of traditional algebraic simpli.cations \n10. Peephole Strength Reduction Various forms of traditional peephole optimizations 11. Array Copy Propagation \nReplace read of an array element by the expression it was previously assigned 12. CSE for Array Elements \nRemove redundant array accesses 13. Loop Peeling Pulls the .rst iteration of a loop outside of the loop \n14. LIVSR Optimization described in Section 2, namely Loop-induction-variable Strength Reduction 15. \nInterloop Strength Reduction Optimization described in Section 3 16. Entire-loop Strength Reduction Transforms \nentire loop into one operation, e.g. loop doing i incrs becomes plus i 17. Loop-operation Factoring Factors \nexpensive operations out of a loop, for example multiplication 18. Loop-operation Distributing Distributes \nan expensive operation into a loop, where it cancels out with another operation 19. Partial Inlining \nInlines part of a method in the caller but keeps the call for the rest of the computation (c) Domain-speci.c \nOptimizations Description 20. Domain-speci.c LIVSR LIVSR, but with domain operations like matrix/vector \naddition and multiply 21. Domain-speci.c code hoisting Code hoisting based on domain-speci.c invariance \naxioms 22. Domain-speci.c Redundancy Removal Removes redundant computations based on domain axioms 23. \nTemporary Object Removal Removes temporary objects created by calls to matrix/vector libraries 24. Specialization \nof Math Libraries Specializes vector/matrix algorithms based on, for example, the size of the vector/matrix \n25. Design-pattern Optimizations Removes overhead of certain design patterns, like the indirection or \ninterpreter pattern 26. Method Outlining Replaces code by call to a method performing the same computation, \nbut more ef.ciently 27. Specialized Redirection Replaces method call with call to a more ef.cient version \nbased on the calling context Figure 8. Optimizations performed by Peggy conjecture that interprocedural \noptimizations can be expressed as equality analyses, and leave this exploration to future work. With \nthe addition of domain-speci.c axioms, our approach en\u00adables even more optimizations, as shown in part \n(c). To give a .avor for these domain-speci.c optimizations, we describe two examples. The .rst is a \nray tracer application (5 KLOCs) that one of the au\u00adthors had previously developed. To make the implementation \nclean and easy to understand, the author used immutable vector objects in a functional programming style. \nThis approach however intro\u00adduces many intermediate objects. With a few simple vector axioms, Peggy is \nable to remove the overhead of these temporary objects, thus performing a kind of deforestation optimization. \nThis makes the application 7% faster, and reduces the number of allocated ob\u00adjects by 40%. Soot is not \nable to recover any of the overhead, even with interprocedural optimizations turned on. This is an instance \nof a more general technique where user-de.ned axioms allow Peggy to remove temporary objects (optimization \n23 in Figure 8). Our second example targets a common programming idiom in\u00advolving Lists, which consists \nof checking that a List contains an element e, and if it does, fetching and using the index of the element. \nIf written cleanly, this pattern would be implemented with a branch whose guard is contains(e) and a \ncall to indexOf(e) on the true side of the branch. Unfortunately, contains and indexOf would perform \nthe same linear search, which makes this clean way of writing the code inef.cient. Using the equality \naxiom l.contains(e)=(l.indexOf(e)= -1), Peggy can convert the clean code into the hand-optimized code \nthat programmers typi\u00adcally write, which stores indexOf(e) into a temporary, and then branches if the \ntemporary is not -1. An extensible rewrite system would not be able to provide the same easy solution: \nalthough a rewrite of l.contains(e) to (l.indexOf(e)= -1) would remove the redundancy mentioned above, \nit could also degrade perfor\u00admance in the case where the list implements an ef.cient hash-based contains. \nIn our approach, the equality simply adds information to the E-PEG, and the pro.tability heuristic can \ndecide after satura\u00adtion which option is best, taking the entire context into account. In this way our \napproach transforms contains to indexOf, but only if indexOf would have been called anyway. These two \nexamples illustrate the bene.ts of user-de.ned ax\u00adioms. In particular, the clean, readable, and maintainable \nway of writing code can sometimes incur performance overheads. User\u00adde.ned axioms allow the programmer \nto reduce these overheads while keeping the code base clean of performance-related hacks. Our approach \nmakes domain-speci.c axioms easier to add for the end-user programmer, because the programmer does not \nneed to worry about what order the user-de.ned axioms should be run in, or how they will interact with \nthe compiler s internal optimizations.  7.3 Translation Validation We used Peggy to perform translation \nvalidation for the Soot opti\u00admizer [35]. In particular, we used Soot to optimize a set of bench\u00admarks \nwith all of its intraprocedural optimizations turned on. The benchmarks included SpecJVM, along with \nother programs, com\u00adprising a total of 3,416 methods. After Soot .nished compiling, for each method we \nasked Peggy s saturation engine to show that the original method was equivalent to the corresponding \nmethod that Soot produced. The engine was able to show that 98% of methods were compiled correctly. Among \nthe cases that Peggy was unable to validate, we found three methods that Soot optimized incorrectly. \nIn particular, Soot incorrectly pulled statements outside of an intricate loop, trans\u00adforming a terminating \nloop into an in.nite loop. It is a testament to the power of our approach that it is able not only to \nperform opti\u00admizations, but also to validate a large fraction of Soot runs, and that in doing so it exposed \na bug in Soot. Furthermore, because most false positives are a consequence of our coarse heap model (sin\u00adgle \ns node), a .ner-grained model can increase the effectiveness of translation validation, and it would \nalso enable more optimizations. Our equality saturation engine can easily be extended so that, after \neach translation validation, it generates a machine-checkable proof of equivalence. With this in place, \nthe trusted computing base for our translation validator would only be: (1) the proof checker, (2) the \nbuilt-in axioms used in translation validation, most of which we have proved by hand, and (3) the conversion \nfrom Java bytecode to PEG.  8. Related Work Superoptimizers. Our approach of computing a set of programs \nand then choosing from this set is related to the approach taken by super-optimizers [24, 18, 4, 16]. \nSuperoptimizers strive to pro\u00adduce optimal code, rather than simply improve programs. Although super-optimizers \ncan generate (near) optimal code, they have so far scaled only to small code sizes, mostly straight line \ncode. Our ap\u00adproach, on the other hand, is meant as a general purpose paradigm that can optimize branches \nand loops, as shown by the inter-loop optimization from Section 3. Our approach was inspired by Denali \n[21], a super-optimizer for .nding near-optimal ways of computing a given basic block. Denali represents \nthe computations performed in the basic block as an expression graph, and applies axioms to create an \nE-graph data structure representing the various ways of computing the values in the basic block. It then \nuses repeated calls to a SAT solver to .nd the best way of computing the basic block given the equalities \nstored in the E-graph. The biggest difference between our work and Denali is that our approach can perform \nintricate optimizations involving branches and loops. On the other hand, the Denali cost model is more \nprecise than ours because it assigns costs to entire sequences of operations, and so it can take into \naccount the effects of scheduling and register allocation. Rewrite-based optimizers. Axioms or rewrite-rules \nhave been used in many compilation systems, for example TAMPR [6], ASF+SDF [36], the ML compilation system \nof Visser et al. [37], and Stratego [7]. These systems, however, perform transformations in sequence, \nwith each axiom or rewrite rule destructively updat\u00ading the IR. Typically, such compilers also provide \na mechanism for controlling the application of rewrites through built-in or user\u00adde.ned strategies. Our \napproach, in contrast, does not use strategies we instead simultaneously explore all possible optimization \nor\u00adderings, while avoiding redundant work. Furthermore, even with no strategies, we can perform a variety \nof intricate optimizations. Optimization Ordering. Many research projects have been aimed at mitigating \nthe phase ordering problem, including auto\u00admated assistance for exploring enabling and disabling properties \nof optimizations [39, 40], automated techniques for generating good sequences [10, 1, 22], manual techniques \nfor combining analy\u00adses and optimizations [8], and automated techniques for the same purpose [23]. However, \nwe tackle the problem from a different perspective than previous approaches, in particular, by simulta\u00adneously \nexploring all possible sequences of optimizations, up to some bound. Aside from the theoretical guarantees \nfrom Section 4, our approach can do well even if every part of the input program requires a different \nordering. Translation Validation. Although previous approaches to translation validation have been explored \n[30, 25, 42], our approach has the advantage that it can perform translation validation by using the \nsame technique as for program optimization. Intermediate Representations. Our maincontributionis an approach \nfor structuring optimizers based on equality saturation. However, to make our approach effective, we \nhave also designed the E-PEG representation. There has been a long line of work on developing IRs that \nmake analysis and optimizations easier to perform [11, 2, 34, 19, 15, 38, 9, 32, 29]. The key distinguishing \nfeature of E-PEGs is that a single E-PEG can represent many optimized versions of the input program, \nwhich allows us to use global pro.tability heuristics and to perform translation validation. We now compare \nthe PEG component of our IR with previous IRs. PEGs are related to SSA [11], gated SSA [34] and thinned\u00adgated \nSSA [19]. The \u00b5 function from gated SSA is similar to our . function, and the . function is similar to \nour eval/pass pair. However, unlike PEGs, all these variants of SSA are tried to an underlying CFG representation. \nProgram Dependence Graphs [15] and the Program Dependence Web [28] represent control information by grouping \ntogether oper\u00adations that execute in the same control region. However, these IRs are still statement \nbased, and maintain explicit control edges. Like PEGs, the Value Dependence Graph [38] (VDG) is a com\u00adplete \nfunctional representation. VDGs use . nodes (i.e. regular function abstraction) to represent loops, whereas \nwe use special\u00adized ., eval and pass nodes. These specialized nodes, combined with simple axioms about \nthem, allow us to perform intricate opti\u00admizations across loops, such as the optimization from Section \n3. Dependence Flow Graphs [29] (DFGs) are a complete and ex\u00adecutable representation of programs based \non dependencies. How\u00adever, DFGs employ a side-effecting storage model with an imper\u00adative store operation, \nwhereas our representation is entirely func\u00adtional, making equational reasoning more natural. Data.ow \nLanguages. Our PEG intermediate representation is related to the broad area of data.ow languages [20]. \nThe most closely related is the Lucid programming language [3], in which variables are maps from iteration \ncounts to possibly unde.ned val\u00adues, as in our PEGs. Lucid s .rst/next operators are similar to our . \nnodes, and Lucid s as soon as operator is similar to our eval/pass pair. However, Lucid and PEGs differ \nin their intended use and ap\u00adplication. Lucid is a programming language designed to make for\u00admal proofs \nof correctness easier to do, whereas Peggy uses equiva\u00adlences of PEG nodes to optimize code expressed \nin existing imper\u00adative languages. Furthermore, we incorporate a monotonize func\u00adtion into our semantics \nand axioms, which guarantees the correct\u00adness of our conversion to and from CFGs with loops. Theorem \nProving. Because most of our reasoning is performed using simple axioms, our work is related to the broad \narea of auto\u00admated theorem proving. The theorem prover that most inspired our work is Simplify [13], \nwith its E-graph data structure for represent\u00ading equalities [27]. Our E-PEGs are in essence specialized \nE-graphs for reasoning about PEGs. Furthermore, the way our analyses com\u00admunicate through equality is \nconceptually similar to the equality propagation approach used in Nelson-Oppen theorem provers [26]. \nExecution Indices. Execution indices identify the state of progress of an execution [14, 41]. The call \nstack typically acts as the interprocedural portion, and the loop iteration counts in our semantics can \nact as the intraprocedural portion. As a result, one of the bene.ts of PEGs is that they make intraprocedural \nexecution indices explicit. 9. Conclusion and future work We have presented a new approach to structuring \noptimizers that is based on equality saturation. Our approach has a variety of bene.ts over previous \ncompilation models: it addresses the phase ordering problem, it enables global pro.tability heuristics, \nand it performs translation validation. There are a variety of directions for future work. One direction \nis to extend Peggy so that it generates a proof of correctness for the optimizations it performs. Each \nstep in this proof would be the ap\u00adplication of an equality analysis. Since the majority of our analyses \nare axiom applications, these proofs would be similar to standard mathematical proofs. We would then \nlike to use these proofs as a way of automatically generating optimizations. In particular, by de\u00adtermining \nwhich nodes of the original PEG the proof depends on, and what properties of these nodes are important, \nwe will explore how one can generalize not only the proof but also the transforma\u00adtion. Using such an \napproach, we hope to develop a compiler that can learn optimizations as it compiles.  References [1] \nL. Almagor, K. D. Cooper, A. Grosul, T. J. Harvey, S. W. Reeves, D. Subramanian, L. Torczon, and T. Waterman. \nFinding effective compilation sequences. In LCTES, 2004. [2] B. Alpern, M. Wegman, and F. Zadeck. Detecting \nequality of variables in programs. In POPL, January 1988. [3] E. A. Ashcroft and W. W. Wadge. Lucid, \na nonprocedural language with iteration. Communications of the ACM, 20(7):519 526, 1977. [4] S. Bansal \nand A. Aiken. Automatic generation of peephole superoptimizers. In ASPLOS, 2006. [5] R. Bird and P. \nWadler. Introduction to Functional Programming. Prentice Hall, 1988. [6] James M. Boyle, Terence J. Harmer, \nand Victor L. Winter. The TAMPR program transformation system: simplifying the develop\u00adment of numerical \nsoftware. Modern software tools for scienti.c computing, pages 353 372, 1997. [7] M. Bravenboer, K. T. \nKalleberg, R. Vermaas, and E. Visser. Stratego/XT 0.17. A language and toolset for program transformation. \nScience of Computer Programming, 72(1-2):52 70, 2008. [8] K. D. Cooper C. Click. Combining analyses, \ncombining opti\u00admizations. Transactions on Programming Languages and Systems, 17(2):181 196, 1995. [9] \nC. Click. Global code motion/global value numbering. In PLDI, June 1995. [10] K. D. Cooper, P. J. Schielke, \nand Subramanian D. Optimizing for reduced code space using genetic algorithms. In LCTES, 1999. [11] R. \nCytron, J. Ferrante, B. Rosen, M. Wegman, and K. Zadeck. An ef.cient method for computing static single \nassignment form. In POPL, January 1989. [12] Jeffrey Dean and Craig Chambers. Towards better inlining \ndecisions using inlining trials. In Conference on LISP and Functional Programming, 1994. [13] D. Detlefs, \nG. Nelson, and J. Saxe. Simplify: A theorem prover for program checking. Journal of the Association for \nComputing Machinery, 52(3):365 473, May 2005. [14] E. Dijkstra. Go to statement considered harmful. pages \n27 33, 1979. [15] J. Ferrante, K. Ottenstein, and J. Warren. The program dependence graph and its use \nin optimization. Transactions on Programming Languages and Systems, 9(3):319 349, July 1987. [16] Christopher \nW. Fraser, Robert R. Henry, and Todd A. Proebsting. BURG fast optimal instruction selection and tree \nparsing. SIGPLAN Notices, 27(4):68 76, April 1992. [17] J. Giarratano and G. Riley. Expert Systems Principles \nand Programming. PWS Publishing Company, 1993. [18] Torbjorn Granlund and Richard Kenner. Eliminating \nbranches using a superoptimizer and the GNU C compiler. In PLDI, 1992. [19] P. Havlak. Construction of \nthinned gated single-assignment form. In Workshop on Languages and Compilers for Parallel Computing, \n1993. [20] W. M. Johnston, J. R. P. Hanna, and R. J. Millar. Advances in data.ow programming languages. \nACM Computing Surveys, 36(1):1 34, 2004. [21] R. Joshi, G. Nelson, and K. Randall. Denali: a goal-directed \nsuperoptimizer. In PLDI, June 2002. [22] L. Torczon K. D. Cooper, D. Subramanian. Adaptive optimizing \ncompilers for the 21st century. The Journal of Supercomputing, pages 7 22, 2002. [23] S. Lerner, D. Grove, \nand C. Chambers. Composing data.ow analyses and transformations. In POPL, January 2002. [24] Henry Massalin. \nSuperoptimizer: a look at the smallest program. In ASPLOS, 1987. [25] G. Necula. Translation validation \nfor an optimizing compiler. In PLDI, June 2000. [26] G. Nelson and D. Oppen. Simpli.cation by cooperating \ndecision procedures. Transactions on Programming Languages and Systems, 1(2):245 257, October 1979. [27] \nG. Nelson and D. Oppen. Fast decision procedures based on congruence closure. Journal of the Association \nfor Computing Machinery, 27(2):356 364, April 1980. [28] K. Ottenstein, R. Ballance, and A. MacCabe. \nThe program dependence web: a representation supporting control-, data-, and demand-driven interpretation \nof imperative languages. In PLDI, June 1990. [29] K. Pengali, M. Beck, and R. Johson. Dependence .ow \ngraphs: an algebraic approach to program dependencies. In POPL, January 1991. [30] A. Pnueli, M. Siegel, \nand E. Singerman. Translation validation. In TACAS, 1998. [31] H. Sheini and K. Sakallah. Pueblo: A hybrid \npseudo-boolean SAT solver. Journal on Satis.ability, Boolean Modeling and Computation, 2:61 96, 2006. \n[32] B. Steffen, J. Knoop, and O. Ruthing. The value .ow graph: A program representation for optimal \nprogram transformations. In European Symposium on Programming, 1990. [33] Ross Tate, Michael Stepp, Zachary \nTatlock, and Sorin Lerner. Translating between PEGs and CFGs. Technical Report CS2008\u00ad0931, University \nof California, San Diego, November 2008. [34] P. Tu and D. Padua. Ef.cient building and placing of gating \nfunctions. In PLDI, June 1995. [35] R. Vall\u00b4ee-Rai, L. Hendren, V. Sundaresan, P. Lam, E. Gagnon, and \nP. Co. Soot -a Java optimization framework. In CASCON, 1999. [36] M. G. J. van den Brand, J. Heering, \nP. Klint, and P. A. Olivier. Com\u00adpiling language de.nitions: the ASF+SDF compiler. Transactions on Programming \nLanguages and Systems, 24(4), 2002. [37] E. Visser, Z. Benaissa, and A Tolmach. Building program optimizers \nwith rewriting strategies. In ICFP, 1998. [38] D. Weise, R. Crew, M. Ernst, and B. Steensgaard. Value \ndependence graphs: Representation without taxation. In POPL, 1994. [39] Debbie Whit.eld and Mary Lou \nSoffa. An approach to ordering optimizing transformations. In PPOPP, 1990. [40] Deborah L. Whit.eld and \nMary Lou Soffa. An approach for exploring code improving transformations. Transactions on Programming \nLanguages and Systems, 19(6):1053 1084, November 1997. [41] B. Xin, W. N. Sumner, and X. Zhang. Ef.cient \nprogram execution indexing. In PLDI, June 2008. [42] Lenore Zuck, Amir Pnueli, Yi Fang, and Benjamin \nGoldberg. VOC: A methodology for the translation validation of optimizing compilers. Journal of Universal \nComputer Science, 9(3):223 247, March 2003. \n\t\t\t", "proc_id": "1480881", "abstract": "<p>Optimizations in a traditional compiler are applied sequentially, with each optimization destructively modifying the program to produce a transformed program that is then passed to the next optimization. We present a new approach for structuring the optimization phase of a compiler. In our approach, optimizations take the form of equality analyses that add equality information to a common intermediate representation. The optimizer works by repeatedly applying these analyses to infer equivalences between program fragments, thus saturating the intermediate representation with equalities. Once saturated, the intermediate representation encodes multiple optimized versions of the input program. At this point, a profitability heuristic picks the final optimized program from the various programs represented in the saturated representation. Our proposed way of structuring optimizers has a variety of benefits over previous approaches: our approach obviates the need to worry about optimization ordering, enables the use of a global optimization heuristic that selects among fully optimized programs, and can be used to perform translation validation, even on compilers other than our own. We present our approach, formalize it, and describe our choice of intermediate representation. We also present experimental results showing that our approach is practical in terms of time and space overhead, is effective at discovering intricate optimization opportunities, and is effective at performing translation validation for a realistic optimizer.</p>", "authors": [{"name": "Ross Tate", "author_profile_id": "81392610098", "affiliation": "University of Califorina, San Diego, San Diego, CA, USA", "person_id": "P1300990", "email_address": "", "orcid_id": ""}, {"name": "Michael Stepp", "author_profile_id": "81336493326", "affiliation": "University of Califorina, San Diego, San Diego, CA, USA", "person_id": "P1300991", "email_address": "", "orcid_id": ""}, {"name": "Zachary Tatlock", "author_profile_id": "81392605383", "affiliation": "University of Califorina, San Diego, San Diego, CA, USA", "person_id": "P1300992", "email_address": "", "orcid_id": ""}, {"name": "Sorin Lerner", "author_profile_id": "81100399150", "affiliation": "University of Califorina, San Diego, San Diego, CA, USA", "person_id": "P1300993", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1480881.1480915", "year": "2009", "article_id": "1480915", "conference": "POPL", "title": "Equality saturation: a new approach to optimization", "url": "http://dl.acm.org/citation.cfm?id=1480915"}