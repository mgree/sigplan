{"article_publication_date": "01-21-2009", "fulltext": "\n The Third Homomorphism Theorem onTrees Downward&#38;Upward LeadtoDivide-and-Conquer Akimasa Morihata \nKiminori Matsuzaki Zhenjiang Hu Masato Takeichi University of Tokyo University of Tokyo National Institute \nof University of Tokyo JSPS Research Fellow Informatics morihata@ipl.t.u-tokyo.ac.jp kmatsu@ipl.t.u-tokyo.ac.jp \nhu@nii.ac.jp takeichi@mist.i.u-tokyo.ac.jp Abstract Parallel programs on lists have been intensively \nstudied. It is well known that associativity provides a good characterization for divide-and-conquer \nparallel programs. In particular, the third ho\u00admomorphism theorem is not only useful for systematic development \nof parallel programs on lists,but it is also suitable for automatic parallelization. The theorem states \nthat if two sequential programs iterate the same list leftward and rightward, respectively, and com\u00adputethe \nsamevalue,then thereexistsadivide-and-conquer parallel program that computes the same value as the sequential \nprograms. While there have been many studies on lists, few have been done for characterizing and developing \nof parallel programs on trees. Naive divide-and-conquer programs, which divide a tree at the root and \ncompute independent subtrees in parallel, take time that is proportional to the height of the input tree \nand have poor scalability with respect to the number of processors when the input tree is ill-balanced. \nIn this paper, we develop a method for systematically con\u00adstructing scalable divide-and-conquer parallel \nprograms on trees, in which two sequential programs lead to a scalable divide-and\u00adconquer parallel program.We \nfocus on paths insteadof trees so as to utilize rich results on lists and demonstrate that associativity \npro\u00advides good characterization for scalable divide-and-conquer paral\u00adlel programs on trees. Moreover, \nwe generalize the third homomor\u00adphism theoremfromliststo trees.Wedemonstratetheeffectiveness of our method \nwith various examples. Our results, being general\u00adizations of known results for lists, are generic in \nthe sense that they work well for all polynomial data structures. Categories and Subject Descriptors \nD.1.3 [Concurrent Pro\u00adgramming]: Parallel programming; I.2.2 [Automatic Program\u00adming]: Program transformation; \nD.1.1[Applicative (Functional) Programming] General Terms Algorithm, Theory Keywords Divide-and-conquer, \nHuet s zippers, Polynomial data structures, The third homomorphism theorem Permission to make digital \nor hard copies of all or part of this work for personal or classroom use is granted without fee provided \nthat copies are not made or distributed for pro.t or commercial advantage and that copies bear this notice \nand the full citation on the .rst page.To copyotherwise, to republish, to post on servers or to redistribute \nto lists, requires prior speci.c permission and/or a fee. POPL 09, January 18 24, 2009, Savannah, Georgia, \nUSA. Copyright c &#38;#169; 2009ACM 978-1-60558-379-2/09/01... $5.00 1. Introduction What are little \nboys made of? Snips and snails, and puppy-dogs tails That s what little boys are made of! What are little \ngirls made of? Sugar and spice and all things nice That s what little girls are made of! (an old nursery \nrhyme) What are parallel programs on lists made of? Consider summing up the elements in a list [a1,a2,a3,a4,a5,a6,a7,a8] \nas an exam\u00adple. It is easy to derive sequential algorithms; both the rightward summation ((((((a1 + a2)+ \na3)+ a4)+ a5)+ a6)+ a7)+ a8 and the leftward summation a1 +(a2 +(a3 +(a4 +(a5 +(a6 +(a7 + a8)))))) are \nsequential algorithms. Can we derive a parallel algorithm for summation?Yes, the divide-and-conquer summation \n((a1 + a2)+(a3 + a4)) + ((a5 + a6)+(a7 + a8)) is a parallel algorithm, which divides the list at its \ncenter and computeseachpartin parallel.Whatisthekeytosuchdivide-and\u00adconquer parallel algorithms? Compare \nthe sequential algorithms with the divide-and-conquer algorithm. The only difference is the structure \nof the parentheses, and the associative law of +, namely a +(b + c)=(a + b)+ c, enables us to rearrange \nthe parentheses. This observation, i.e., an associative operator provides a divide\u00adand-conquer parallel \ncomputation, is formalized as the notion of list homomorphisms (Bird 1987). Function h is a list homomorphism \nif there exists associative operator . such that h (x + y)= hx . hy holds, where operator + denotes the \nconcatenation of two lists. What is nice about list homomorphisms is the scalability1: given a list of \nlength n, list homomorphisms yield linear speedup up to O(n/log n) processors. Scalability is one of \nthe most impor\u00adtant properties in parallel programming. If a parallel program has good scalability, it \nshows great speedup that other optimization techniques can barely achieve. If scalability is poor, parallelization \nis nothing but anti-optimization, because parallelization requires overheads such as communication and \nsynchronization. Then, what are list homomorphisms made of? It may be sur\u00adprising that parallel programming \non lists is made of writing two 1In this paper, we consider scalability with respect to numbers of proces\u00adsors \n(Foster 1995). This paper makes three main contributions: 8 8 8 Formalization of path-based computations \non trees:We intro\u00ad 9 9 9 duce path-based computations, which include downward and 6 upward computations. \nTo express path-based computations, 6 3 6 3    1 1 1   4 4 we borrow the notion of Huet s zippers \n(Huet 1997; McBride 2001). Spotlightingpathsisthekeytodeveloping parallelpro\u00ad  4 grams, because it \nmakes theories on lists applicable to trees. 7 5 5 7 7 5 Characterization of scalable parallel computations \non trees: We propose an algebraic characterization of scalable parallel programs on trees. The main idea \nis to consider divide-and\u00ad   conquer algorithms on one-hole contexts instead of those on trees. We \ncan evaluate programs that belong to our charac\u00adterization ef.ciently in parallel: given a tree of n \nnodes, they Figure 1. Aggressively dividing binary tree .nish in O(n/p + log p) steps of primitive operations \non an exclusive-read/exclusive-write parallel random access machine with p processors. It is worth noting \nthat this computational sequential programs, which proved as the third homomorphism the\u00adorem (Gibbons \n1996). The theorem states that if twosequential pro\u00adgrams iterate the same list leftward and rightward, \nrespectively,and compute the same value, then there exists a list homomorphism that computes the same \nvalue as these sequential programs. In otherwords,twosequential programs leadtoascalabledivide-and\u00adconquer \nparallel program. The third homomorphism theorem is useful for developing parallel programs, and automatic \nparalleliza\u00adtion methods have been proposed based on the theorem (Geser and Gorlatch 1999; Morita et \nal. 2007). In summary, scalable parallel programs on lists are made of sugar and spice and all things \nnice : associative operators and list homomorphisms, which are obtained from two sequential pro\u00adgrams. \nThen, what are parallel programs on trees made of?Forexam\u00adple, let us consider summing up all values \nin a tree. Someone may think of a divide-and-conquer parallel algorithm raised by subtree structures, \ni.e., computing independent subtrees in parallel. How\u00adever, such a naive parallel algorithm is generally \nnot scalable. Its speedup is limited by the height of the input tree, and thus, it has miserable scalabilityif \nthe input treeis ill-balanced.To obtain bet\u00adter scalability, we need to introduce more aggressive divisions, \nas outlined in Figure 1. In this case, aggressive divisions yield a scal\u00adable parallel algorithm, which \ncomputes the summation in logarith\u00admic time on the size of the tree with a suf.cient number of proces\u00adsors. \nAlthough we have successfully constructed a scalable divide\u00adand-conquer parallel summation algorithm \nbased on aggressive di\u00advisions, it is nontrivial to generalize the algorithm. What algebraic properties \nwill enable us to compute each part in parallel? Howcan we obtain a parallel program from sequential \nones? The nonlinear structure of trees makes it hard to develop parallel programs. There\u00adfore, it has \nbeen considered that scalable parallel programs on trees were made of snips and snails, and puppy-dogs \ntails . In this paper, we explain that scalable parallel programs on trees are infact made of sugar and \nspice and all things nice even though theyarea bit spicier than those on lists.We focus on the similarity \nbetween lists and paths, formalize scalable parallel programs on trees as path-based computations, and \ndemonstrate that associative computations on paths lead to scalable parallel programs. Moreover,we provethat \nthe following proposition holds, which is a tree version of the third homomorphism theorem that enables \nus to derive a scalable parallel program on trees from two sequential programs. If two sequential programs \niterate the same tree downward and upward, respectively, and compute the same value, then there exists \na scalable divide-and-conquer parallel program that computes the same value as these sequential programs. \ncomplexity implies theyhave good scalability in the sense that theyshow linear speedup up to O(n/log \nn) processors. The third homomorphism theorem on trees: We prove the third homomorphism theorem on trees. \nIt states that if a func\u00adtion is both downward and upward, then it can be evaluated ef.ciently in parallel. \nThe theorem is useful for systematically developing parallel programs. We will demonstrate its effec\u00adtiveness \nwith various examples. Our results are generic in the sense that they work well for all polynomial data \nstructures, which can capture a large class of algebraic data structures on functional languages; besides, \ntheyare generalizations of known results on lists. The rest of this paper is organized as follows. After \nthe prelimi\u00adnariesin Section2,weexplain our ideasand resultson node-valued binary trees in Sections 3 \nand 4. We develop our theory in Sec\u00adtion3and presentexamplesin Section4. After that, we generalize our \nresults to generic trees, namely polynomial data structures, in Section5.We discuss related and futureworksin \nSection6. 2. The Third Homomorphism Theorem 2.1 Basic de.nitions In this paper, we basically borrow notations \nof functional program\u00adming language Haskell (Peyton Jones 2003). The parentheses for function applications \nmay be omitted. Note that applications for functions havehigher priority than those for operators, thus \nfx\u00d8y means (fx) \u00d8 y. Operator o denotes a function composition, and its de.nition is (f o g)(x)= f(g(x)). \nUnderscore - is used to express don tcare pattern. x :: X means the type of x is X. Alist is denoted \nby brackets split by commas. The list concate\u00adnation operator is denoted by + . Note that + is associative. \n[A] denotes the type of lists whose elements are in A. The disjoint sum of twosets A and B is denotedbyEither \nAB. data Either ab = Left a | Right b We will writeL and R instead of Left and Right for shorthand. We \nwill use several standard functions in Haskell, and their de.nitions are given in Figure 2. 2.2 Right \nInverses Right inverses, which are generalizations of inverse functions, are useful for developing divide-and-conquer \nparallel programs (Gib\u00adbons 1996; Morita et al. 2007). De.nition 2.1 (right inverse). For functionf :: \nA -B,a right inverse of f, denoted by f., is a function satisfying the following equation. f o f. o f \n= f id x = x fst (a, b)= a map f [] =[] map f ([a]+ x)=[fa]+ map fx foldr (\u00d8) e [] = e foldr (\u00d8) e ([a]+ \nx)= a \u00d8 (foldr (\u00d8) ex) foldl (*) e [] = e foldl (*) e (x + [a]) = (foldl (*) ex) * a Figure 2. De.nitions \nof standard functions Two things are worth noting. First, as a right inverse exists for anyfunction,itis \nunnecessarytoworry aboutitsexistence. Second, arightinverseofafunctionis generallynot unique,and f. denotes \none of the right inverses of f. 2.3 List Homomorphisms and The Third Homomorphism Theorem List homomorphisms \nare an expressive computation pattern for scalable divide-and-conquer parallel programs on lists. De.nition \n2.2 (list homomorphism (Bird 1987)). Function h :: [A] -B is saidtobea list homomorphism if there exists \nfunction f :: A -B and associative operator (.) :: B -B -B such that h [] = .. h [a]= fa h (x + y)= hx \n. hy hold, where .. is the unit of .. Here, we write h = hom (.) f.2 Itisworth noting that associative \noperator . characterizesa list homomorphism. The associativity of . guarantees that the result of computation \nis not affected by where to divide the list. List homomorphisms are useful for developing parallel programs \non lists (Bird 1987; Cole 1994, 1995; Gibbons 1996; Hu et al. 1997a). The third homomorphism theorem \ndemonstrates a necessary and suf.cient condition of existence of a list homomorphism. Theorem 2.3 (the \nthird homomorphism theorem (Gibbons 1996)). Function h is a list homomorphism if and only if there exist \ntwo operators \u00d8 and * such that the following equations hold. h ([a]+ x)= a \u00d8 hx h (x + [a]) = hx * a \nThe third homomorphism theorem states that if we can com\u00adpute a function in both leftward and rightward \nmanners, then there existsadivide-and-conquer parallel algorithmtoevaluate the func\u00adtion.Whatthe theorem \nindicatesisnotonlytheexistenceof parallel programsbut alsoawayof systematicallydeveloping parallel pro\u00adgrams. \nThe following lemma plays a central role in parallelization. Lemma 2.4 (Gibbons (1996); Morita et al. \n(2007)). Assume that the following equations hold for function h. h ([a]+ x)= a \u00d8 hx h (x + [a]) = hx \n* a Then, h = hom (.) f holds, where . and f are de.ned as follows. fa = h [a] a . b = h (h. a + h. b) \nLemma 2.4 states that we can derive a parallel program from two sequential programs through a right inverse \nof h. 2We usually neglect .. because parallel computations on empty lists is useless. If .. is necessary, \nwe can prepareitby introducinga specialvalue that behaves as the unit. a 1 a 2 a 3  a 1 a 2 a 3 a 4 \na 5 a 4  ,,,, a 5 Figure 3. Azipper structure, which expresses a path from the root to the black leaf \n3. The Third Homomorphism Theorem on BinaryTrees Here, let us consider node-valued binary trees. data \nTree = Node Int Tree Tree | Leaf The goal is to formalize and prove the third homomorphism the\u00adorem on \nnode-valued binary trees, which consists of the three notions of downward computations, upward computations, \nand scalable divide-and-conquer computations on trees. To formalize these notions, we will focus on paths. \nGiven a path, the downward and upward computations are computations that are accomplished while walking \nthe path in downward and upward manners, re\u00adspectively. Scalable divide-and-conquer computations on trees \nare computations that split a path in the middle and compute each sub\u00adstructure in parallel. To specify \na path on a tree, we will borrow Huet s zippers (Huet 1997). 3.1 Zippers on BinaryTrees A zipper is a \nlist whose elements are contexts that are left after a walk. Based on walking downward from the root \nof a tree, we construct a zipper as follows: when we go down-right from a node, we add its left child \nto the zipper; when we go down-left, we addtheright childtothezipper.Forexample,Figure3showsthe correspondence \nbetween a zipper and a walk from the root to the black leaf. The zipper structures for node-valued binary \ntrees can be spec\u00adi.ed in the following type, where components of the Either type correspond to the left \nand the right child. type Zipper =[Either (Int, Tree)(Int, Tree)] Inthispaper,awalkusuallyendsataleaf.Then,azipper \nstoresthe whole tree. Function z2t below restores a tree from a zipper. z2t [] = Leaf z2t ([L (n, l)] \n+ r)= Node nl (z2t r) z2t ([R (n, r)] + l)= Node n (z2t l) r Whenawalk endsataleaf,azipper correspondstoatree. \nWhen a walk ends at an internal node, a zipper corresponds not to a tree buttoa one-hole contextofa tree. \nLookat Figure3again.On one hand, the zipper represents the tree with its path from the root to the black \nleaf. On the other hand, the zipper also represents the one-hole context in which the black circle represents \nthe hole.We will formalize scalable divide-and-conquer parallel programs on trees based on the second \nviewpoint, viewing a zipper as a one\u00adhole context.However,weusuallyregardazipperasatreeandcall the hole \nthe terminal leaf because a walk usually ends at a leaf. We would like to summarize the correspondences \namong a zip\u00adper,atree(ora one-hole context),andapath.Azipper corresponds toa tree witha terminal leaf(a \none-hole context) ora path from the roottothe terminal leaf.An initialsegmentofa zipper corresponds to \na one-hole context containing the root or a path from the root to anode.Atailsegmentofazipper correspondstoasubtree \ncontain\u00ading the terminal leaf (a one-hole context whose hole is the same as the hole of the original \none) or a path from a node to the terminal leaf. 3.2 Downward and Upward Computations on BinaryTrees \nNext, let us formalize downward and upward computations. Consider function sumTree below as an example, \nwhich sums up all values in a tree. sumTree Leaf =0 sumTree (Node nlr)= n + sumTree l + sumTree r First, \nwe would like to give its downward version. Since an initialsegmentofazipper correspondstoapathfromtheroottoan \ninternal node, function sumTreel below performs its computation downward from the root to the terminal \nleaf. sumTreel [] =0 sumTreel (x + [L (n, l)]) = sumTreel x + n + sumTree l sumTreel (x + [R (n, r)]) \n= sumTreel x + n + sumTree r Note that for computing summations for trees in the zipper, we use the function \nsumTree. Similarly, we can give its upward version. Since a tail segment of a zipper corresponds to a \npath from an internal node to the terminal leaf, function sumTreej below traverses a tree from its terminal \nleaf to its root. sumTreej [] =0 sumTreej ([L (n, l)] + r)= n + sumTree l + sumTreej r sumTreej ([R (n, \nr)] + l)= n + sumTreej l + sumTree r Here, sumTreej and sumTreel are equivalent to sumTree in the sense \nthat sumTreej = sumTreel = sumTree o z2t holds. However, computations on a path may require more information \nthan those on trees.Toformalize the correspondence between com\u00adputationsonpathsand thoseon trees,we introduceanotionof \npath\u00adbased computations. De.nition 3.1 (path-based computation on binary trees). Function h' :: Zipper \n-B is said to be a path-based computation of h :: Tree -A if there exists function . :: B -A such that \nthe following equation holds. . o h' = h o z2t This equation means that h' simulates the computation \nof h and the result is extracted by .. Note that z2t is a path-based compu\u00adtation of anyfunction; however, \nthis is useless in practice because no signi.cant computations are managed on paths. In other words, \nit is important to specify an appropriate path-based computation. Now, let us introduce downward and \nupward computations. De.nition 3.2 (downward computations on binary trees). Function h'::Zipper -B,whichisapath-based \ncomputationofh::Tree -A, is said to be downward if there exists operator (*) :: B -Either (Int,A)(Int,A) \n-B such that the following equations hold. h' (x + [L (n, t)]) = h' x * L (n,h t) h' (x + [R (n, t)]) \n= h' x * R (n,h t) De.nition 3.3 (upward computations on binary trees). Function h' :: Zipper -B, which \nis a path-based computation of h :: Tree -A, is said to be upward if there exists operator (\u00d8) :: Either \n(Int,A)(Int,A) -B -B such that the following equations hold. h' ([L (n, t)] + x)= L (n,h t) \u00d8 h' x h' \n([R (n, t)] + x)= R (n,h t) \u00d8 h' x       Figure 4. Recursive division on one-hole context: at \neach step, the one-hole context is divided at the node represented by a concentric circle. 3.3 Parallel \nComputations on BinaryTrees Next, let us consider scalable divide-and-conquer parallel compu\u00adtationson \ntrees.For scalabledivide-and-conquer computations,we would like to divide a tree at an arbitrary place \nand compute each part in parallel. However, as seen in Figure 1, splitting a tree does not yield two \ntrees of the original type: one is a tree of the original type,butthe otherisa one-hole context. Therefore,itisdif.cultto \nformalize recursive division on trees. Our idea is to consider recursive division on one-hole contexts, \ninsteadofthaton trees.Wedivideapathfromthe roottothehole, asFigure4.Ateachstep,we selectanodeonapathfromtherootto \nthe hole, divide the one-hole context into three one-hole contexts, i.e., the upper part, the lower part, \nand a complete subtree with the node. Apparently, we can divide the upper part and the lower part once \nagain; we can divide the complete subtree by taking off an arbitrary leaf and obtaining a one-hole context. \nThen, we can accomplish recursive division on a one-hole context. Now, let us characterize scalable divide-and-conquer \nparallel computations on node-valued binary trees. For parallel computa\u00adtion on trees, we require three \noperations: an operation (say .)that merges the results of two one-hole contexts, an operation (say f) \nthat takes the result of a complete tree and yields the result of the complete tree with its parent, \nand an operation (say .)that com\u00adputes a result of a complete tree from the result of a one-hole con\u00adtext. \nSincea one-hole context correspondstoa zipper, computation onaone-hole context canbe speci.edbypath-based \ncomputations. De.nition 3.4 (decomposition on binary trees). Adecomposition of function h :: Tree -A \nis triple (f, .,.) that consists of associative operator . :: B -B -B and two functions f :: Either (Int,A)(Int,A) \n-B and . :: B -A such that . o h' = h o z2t h' [] = .. h' [L (n, t)] = f (L (n,h t)) h' [R (n, t)] = \nf (R (n,h t)) h' (x + y)= h' x . h' y hold, where .. is the unit of .. In this case, h is said to be \ndecomposable. It is worth noting that the associativity of . is necessary to guarantee that the result \nof computation is not affected by where to divide the tree. It is also worth noting that a decomposition \ncan be seen as a list homomorphism on zippers: De.ne function f' as f' (L (n, t)) = f (L (n,h t)) and \nf' (R (n, t)) = f (R (n,h t)); then, h' = hom (.) f' holds. Actually decomposable functions can be ef.ciently \nevaluated in parallel, which is a consequence of theories of parallel tree con\u00adtraction (Miller and Reif \n1985; Cole andVishkin 1988; Gibbons and Rytter 1989; Abrahamson et al. 1989; Reif 1993). Theorem 3.5. \nIf (f, .,.) is a decomposition of function h and all of f, ., and . are constant-time computations, then \nh can be evaluated for a tree of n nodes in O(n/p + log p) time on an exclusive-read/exclusive-write \nparallel random access machine with p processors. Proof. It is not dif.cult to con.rm that h satis.es \nthe premise of Theorem 3.1 in (Abrahamson et al. 1989). Complexity implies good scalability of decomposable \nfunc\u00adtions, because we can obtain linear speedup up to O(n/log n) pro\u00adcessors. The function sumTree is \ndecomposable as the following equa\u00adtions show. sumPara = sumTree o z2t sumPara [] =0 sumPara [L (n, l)] \n= n + sumTree l sumPara [R (n, r)] = n + sumTree r sumPara (x + y)= sumPara x + sumPara y In other words, \nsumTree has a decomposition (f, +, id), where f is de.ned by f (L (n, v)) = n + v and f (R (n, v)) = \nn + v. It is not dif.cult to derive a parallel program for sumTree because of the associativity of +. \nHowever, it is generally dif.cult to derive an associative operator that provides a decomposition. 3.4 \nThe Third Homomorphism Theorem on BinaryTrees Finally, let us introduce the third homomorphism theorem \non node-valued binary trees, which demonstrates a necessary and suf\u00ad.cient condition of existence of \na decomposition. Theorem 3.6 ( the third homomorphism theorem on binary trees). Function h is decomposable \nif and only if there exists a path-based computation of h that is both downward and upward. Proof. We \nwill prove a stronger theorem later (Theorem 5.7). The statement of Theorem 3.6 is similar to the third \nhomomor\u00adphism theorem on lists. The observation underlying the theorem is that associative operators \nprovide parallel programs not only on listsbutalsoon trees.In addition,thefollowinglemmais usefulfor \ndeveloping parallel programs. Lemma 3.7. Assume that function h ', which is a path-based com\u00adputation \nof h so that . o h ' = h o z2t, is both downward and upward; then, thereexistsa decomposition (f, .,.) \nof h such that the following equations hold. h ' f (R (v,h t))= [R (v, t)] f (L (v,h t))= [L (v, t)] \nh ' h ' (h'. + h'. a . b = ab) Proof. We will prove a stronger lemma later (Lemma 5.6). Combining Theorem \n3.6 and Lemma 3.7, we can derive decom\u00adpositions of functions. Recall sumTree.Apath-based computation \nof sumTree (say st)is both downward and upward, because of st = sumTreel = sumTreej. Therefore, Theorem \n3.6 proves that there is a decomposition of sumTree. Lemma 3.7 shows a way of obtaining a decomposition \n(f, .,.). Here, . = id, be\u00adcause sumTree o z2t = id o st holds. Obtaining function f is easy as the following \ncalculation shows, where C is either L or R. f (C (n, sumTree t)) = { Lemma 3.7 }st [C (n, t)] = { de.nition \nof st }n + sumTree t Thus f (C (n, r)) = n + r. The last is associative operator .. Lemma 3.7 states \nthat a right inverse of st enables us to derive the operator. It is not dif.cult to .nd a right inverse. \nst . s =[L (s, Leaf )] Function st. above is certainly a right inverse of st, because st (st. s)= st \n[L (s, Leaf )] = s holds. Now we can obtain a de.nition of . as follows. a . b = { Lemma 3.7 } st (st. \na + st. b) = { de.nition of st. } st [L (a, Leaf ), L (b, Leaf )] = { de.nition of st } a + b We have \nobtained a decomposition ofsumTree, which is exactly the same as the one we previously showed. 4. Examples \nIn this section, we demonstrate howto develop scalable divide-and\u00adconquer parallel programs. Our development \nconsists of two steps. First, we seek an appropriate path-based computation that is both downward and \nupward. After that, we obtain a decomposition that brings scalable parallelism. 4.1 MaximumPathWeight \nLet us consider a small optimization problem as an initial example to compute the maximum weight of paths \nfrom the root to a leaf. For simplicity, we will assume that the value of each node is non\u00adnegative. \nThe following sequential program solves the problem. maxPath Leaf =0 maxPath (Node nlr)= n + max (maxPath \nl)(maxPath r) Our objective here is to develop a parallel program to solve the problem. First, we try \nto obtain a downward de.nition and think of the following program. maxPathl [] =0 maxPathl (x + [L (n, \nl)]) = max (maxPathl x)(pathWeight x + n + maxPath l) maxPathl (x + [R (n, r)]) = max (maxPathl x)(pathWeight \nx + n + maxPath r) pathWeight [] =0 pathWeight (x + [L (n, l)]) = pathWeight x + n pathWeight (x + [R \n(n, r)]) = pathWeight x + n Note that maxPathl is not downward, because it uses aux\u00adiliary function pathWeight \nthat computes the weight of the path from the root to the terminal leaf. The tupling transforma\u00adtion \n(Fokkinga 1989; Chin 1993; Hu et al. 1997b) is helpful in dealing with such situations. Consider function \nmaxPathl' , which computes the weight of the path to the terminal leaf together with the maximum path \nweight of the tree, namely maxPath 'l x = (maxPathl x, pathWeight x). Apparently maxPathl' is a path\u00adbased \ncomputation of maxPath;in addition, it is downward. maxPathl' [] = (0, 0) maxPathl' (x + [L (n, l)]) \n= let (m, w)= maxPathl' x in (max m (w + n + maxPath l),w + n) maxPathl' (x + [R (n, r)]) = let (m, w)= \nmaxPathl' x in (max m (w + n + maxPath r),w + n) Therefore, maxPathl' seems an appropriate path-based \ncomputa\u00adtion for maxPath, and we would like to derive its upward de.ni\u00ad w . . Function leftOdd below \nreturns the the leftmost odd number in the input tree if one exists; otherwise, it returns special value \n. that stands for emptiness. m - w .. .. mp. (m, w)=  Figure 5. Outline of mp .:closed circle represents \nterminal leaf. tion. Function maxPathj' below is the upward one. maxPathj' [] = (0, 0) maxPathj' ([L \n(n, l)] + x) = let (m, w)= maxPathj' x in (n + max m (maxPath l),n + w) maxPathj' ([R (n, r)] + x) = \nlet (m, w)= maxPath 'j x in (n + max m (maxPath r),n + w) Now that we have con.rmed that maxPath ' = \nmaxPathj' (say l mp)is both downward and upward, Theorem 3.6 proves the exis\u00adtenceof its parallel program.We \nderive this based on Lemma 3.7. Obtaining f is straightforward: f (C (n, m)) = (n + m, n), where C is \neither L or R. To obtain associative operator ., we would like to .nd a right inverse of mp. This is \nnot dif.cult, and function mp . below is a right inverse of mp, which is outlined in Figure 5. mp . (m, \nw)=[L (w, Node (m - w) Leaf Leaf )] Note that mp t =(m, w) implies m . w. Therefore, mp . is cer\u00adtainly \na right inverse of mp, because given tree [L (w, Node (m - w) Leaf Leaf )],where thereexists treet such \nthat mp t =(m, w), the maximum path weight of the tree is m and the path weight to the terminal leaf \nis w. Then, . is derived as follows. (m1,w1) . (m2,w2) = { Lemma 3.7 }mp (mp . (m1,w1)+ mp . (m2,w2)) \n= { De.nition of mp . }mp [L (w1, Node (m1 - w1) Leaf Leaf ), L (w2, Node (m2 - w2) Leaf Leaf )] = { \nDe.nition of mp } (max m1 (w1 + m2),w1 + w2) Lemma 3.7 guarantees the associativity of operator .. In \nsummary, we obtain the following parallel program. maxPath o z2t = fst o mp mp [] =(0, 0) mp [L (n, t)] \n=(n + maxPath t, n) mp [R (n, t)] =(n + maxPath t, n) mp (z1 + z2)= let (m1,w1)= mp z1 (m2,w2)= mp z2 \nin (max m1 (w1 + m2),w1 + w2) The parallel program we obtained above computes two values, while the sequential \nprogram only computes one value; thus, the parallel program is about two times slower than the sequential \none on single-processor machines. Since the parallel program is scal\u00adable,it will runfaster than the \nsequential oneif several processors areavailable; besides, we can reduce theoverheadby using the se\u00adquential \nprogram, as reported in (Matsuzaki and Hu 2006). 4.2 Leftmost Odd Number The next example is a small \nquery to .nd the leftmost odd number ina tree.Infact, this problem canbe solvedby .atteningthe tree into \na list and considering a divide-and-conquer algorithm on the list. Here we will derive a parallel program \nwithout such clever observation. leftOdd Leaf = . leftOdd (Node nlr)= case leftOdd l of .-if odd n then \nn else leftOdd r v -v In the downward computation of leftOdd,we need to determine whethera nearly-leafodd \nnumberis leftmostor not.Forthispur\u00adpose, we add additional information to the result: L v and R v cor\u00adrespond \nto an odd number v at the left and the right of the terminal leaf. leftOddl [] = . leftOddl (x + [L (n, \nl)]) = case leftOddl x of L v -L v a -case leftOdd l of .-if odd n then L n else a v -L v leftOddl (x \n+ [R (n, r)]) = case leftOddl x of L v -L v a -if odd n then R n else case leftOdd r of .-a v -R v Function \nleftOddl is a path-based computation of leftOdd:de.ne . by . . = . and . (Cv)= v where C is either L \nor R;then . oleftOddl = leftOdd oz2t holds.Next,wewouldliketo derive its upward de.nition. leftOddj [] \n= . leftOddj ([L (n, l)] + x) = case leftOdd l of .-if odd n then L n else leftOddj x v -L v leftOddj \n([R (n, r)] + x) = case leftOddj x of .-if odd n then R n else case leftOdd r of .-. v -R v a -a Now \nthat leftOddl = leftOddj (say lo)is both downward and upward, Theorem 3.6 proves that it is decomposable. \nLetusderivea parallel program.Itiseasyto.ndarightinverse of lo, and function lo. below is a right inverse. \nlo. . = [] lo. (L v)=[L (v, Leaf )] lo. (R v)=[R (v, Leaf )] Then Lemma 3.7 gives a decomposition of \nleftOdd after a small amount of calculation. We have omitted the calculation, because it is slightly \nboring though straightforward. The parallel program is shown in Figure 6. Thekey is distinguishing two \nkinds of odd number: those that are to the left of the terminal leaf and those that are to the right. \nWriting downward/upward programs is helpful for noticing such case analyses necessary for parallel computations. \n 4.3 Height The .nal example is computing the height of a tree. height Leaf =1 height (Node - lr) = 1+ \nmax (height l)(height r) leftOdd o z2t = . o lo . . = . . (L v)= v . (R v)= v lo [] = . lo [L (n, t)] \n= case leftOdd t of .-if odd n then L n else . v -L v lo [R (n, t)] = if odd n then R n else case leftOdd \nt of .-. v -R v lo (z1 + z2)= case (lo z1, lo z2) of (L v, -) -L v (R v, .) -R v (-,r) -r Figure 6. \nDivide-and-conquer parallel program for leftOdd This problem is similar to the maximum-path-weight problem, \nand we can specify downward and upward de.nitions in a similar way. heightl [] =(1, 1) heightl (x + [L \n- l]) = let (h,d)= heightl x in (max h (d + height l),d + 1) heightl (x + [R - r]) = let (h,d)= heightl \nx in (max h (d + height r),d + 1) heightj [] =(1, 1) heightj ([L - l]+ x)= let (h, d)= heightj x in (1 \n+ max h (height l),d + 1) heightj ([R - r]+ x)= let (h, d)= heightj x in (1 + max h (height r),d + 1) \nFunction heightl = heightj (say ht)computes the height of a tree in its .rst result, and its second result \nretains the depth of the terminal leaf. Function ht is a path-based computation of height, because fst \no ht = height o z2t holds. Let us parallelize it. The only nontrivial part is that to obtain an associative \noperator from its right inverse. In this case, differ\u00adent from the previous examples, we should de.ne \nright inverse ht. in a recursive manner because ht. (h, d) yields a tree of height h. Therefore, it .rst \nseems dif.cult to simplify the de.nition of ., even though the naivede.nitiona.b = ht (ht. a+ ht. b) \nis inef.\u00adcient. Actually, that simpli.cation is not too dif.cult. Let us look at Figure 7, which outlines \nthe tree ht. (h1,d1)+ ht. (h2,d2). The left and right trees correspond to ht. (h1,d1) and ht. (h2,d2), \nand the curved arrow corresponds to the concatenation operation on zippers. Now it is easy to see that \nthe height of this tree is max h1 (d1 + h2) and the depth of the terminal leaf is d1 + d2. In short, \nthe following gives a de.nition of .. (h1,d1) . (h2,d2)=(max h1 (d1 + h2),d1 + d2) Then, we obtain the \nfollowing parallel program for height. height o z2t = fst o ht ht [] =(1, 1) ht [L (n, t)] =(1+ height \nt, 2) ht [R (n, t)] =(1+ height t, 2) ht (z1 + z2)= let (h1,d1)= ht z1 (h2,d2)= ht z2 in (max h1 (d1 \n+ h2),d1 + d2) We have considered how to merge the results of substructures with the abstraction in Figure \n7. The most signi.cant thing is that Theorem 3.6 guarantees the correctness of the merging operation \nobtained from the abstraction. Theorem 3.6 proves that the results of ht, namely the height of the tree \nand the depth of the terminal  d1 h1  d2 h2   Figure 7. Outline of ht. (h1,d1)+ ht. (h2,d2):curved \narrowed line denotes plugging operation, which corresponds to concatena\u00adtion of two zippers. leaf, are \nsuf.cient for merging the results of two parts; thus, we can derive correct merging operation no matter \nwhat shape the trees are we image for ht. . 5. The Third Homomorphism Theorem on Polynomial Data Structures \nThis section discusses our generalization of the method so that it can deal with all polynomial data \nstructures, which can capture a large class of algebraic data structures on functional languages. 5.1 \nPolynomial Functors To describe our results, we use a few notions from category theory. Acategory consistsofa \nsetof objects anda setof morphisms.A functor (say F)is a morphism of categories, and it respects both \nidentity and composition. Fid = id F(f o g)= Ff o Fg We will consider the categorySet, where an object \nis a set and morphisms from object A to object B are all total functions from A to B.Afunctor gives a \nmapping between functions together with sets. Afunctor is said to be polynomial if it is made up from \nidentity functor I, constant functors such as !A where A is an object, and bifunctors + and \u00d7. Let two \noperators for morphisms \u00d7 and + be (f \u00d7 g)(x, y)=(f x,g y), (f + g)(1,x) = (1,f x), and (f + g)(2,y) \n= (2,g y). Then, the de.nitions of polynomial functors aregiven as follows, where F and G are polynomial \nfunctors, A and B are objects, and f is a morphism. (!A)B = A (!A)f = id IA = A If = f (F \u00d7 G)A = FA \n\u00d7 GA (F \u00d7 G)f = Ff \u00d7 Gf (F + G)A =({1}\u00d7 FA) . ({2}\u00d7 GA) (F + G)f = Ff + Gf The least .xed point of functor \nF,denotedby\u00b5F,is the smallest set that satis.es F(\u00b5F)= \u00b5F. The least .xed point exists for each polynomial \nfunctor. It is well known that the least .xed points of polynomial functors provide a good characterization \nof a large class of algebraic data structures (Backhouse et al. 1998), and these are called polynomial \ndata structures. For example, lists having elements of type A can be recognized as the least .xed point \nof functor L below, where 1 denotes a singleton set. L =!1 +!A \u00d7 I Node-valued trees having values of \ntype A can also be recognized as the least .xed point of functor T below. T =!1 +!A \u00d7 I \u00d7 I 5.2 ZippersforPolynomial \nData Structures Next, let us de.ne zippers for polynomial data structures.We will follow McBride(2001)whoshoweda \nsystematicderivationofzip\u00adpers based on derivatives of functors. The derivative of polynomial functor \nF, denoted by .F, is a polynomial functor de.ned as fol\u00adlows, where denotes a distinguishable element. \n.(!A) =!0 .(I) =!{ }.(F \u00d7 G)= F \u00d7 .G + .F \u00d7 G .(F + G)= .F + .G Functor .F corresponds to a one-hole \ncontext structure of F. The zipper structure of \u00b5F, denoted by ZF, is recognized as ZF = [.F(\u00b5F)]. To \nconvert zippers to trees, we use an operator (<F) :: ZF \u00ad\u00b5F -\u00b5F, whichis de.nedby z<F t = foldr (<F) \ntz where (<F) :: .F(\u00b5F) -\u00b5F -\u00b5F is the plugging-in opera\u00adtor (McBride 2001) de.ned as follows. a <!A \nt = a <I t = t (1, (a, b)) <F\u00d7G t =(a, b <G t) (2, (a, b)) <F\u00d7G t =(a <F t, b) (1,a) <F+G t = (1,a <F \nt) (2,b) <F+G t = (2,b <G t) We will omit the subscript of <F when it is apparent from its context. \nNote that operator < takes an additional tree to convert a zipper to a tree, because a zipper corresponds \nto a one-hole context. As we want to minimize the differences between zippers and trees, we will force \nthe difference between zippers and trees to be leaves.A set of leaves of \u00b5F, denoted by leaves\u00b5F, is \nformalized as follows. leaves\u00b5F =[ F] leaves [[!A] leaves = A [ I] leaves = 0 [ F \u00d7 G] leaves =[ F] leaves \n\u00d7 [ G] leaves [ F + G] leaves =({1}\u00d7 [ F] leaves ) . ({2}\u00d7 [ G] leaves ) 5.3 Sequential andParallel \nComputations on Zippers Now that the notion of zippers has been clari.ed, it is not dif.cult to provide \nde.nitions for downward and upward computations. De.nition 5.1 (path-based computation). Function h ' \n:: ZF -B is saidtobea path-based computation of function h :: \u00b5F -A if there exists operator . :: B -\u00b5F \n-A such that the following equation holds for all t E leaves\u00b5F. h ' h (z< t)= z . t De.nition 5.2 (downward \ncomputation). Function h ' :: ZF -B, whichisa path-based computationof function h :: \u00b5F -A, is said to \nbe downward if there exist operator (*) :: B -.FA -B and value e :: B such that the following equation \nholds. h ' z = foldl (*) e (map (.Fh) z) De.nition 5.3 (upward computation). Function h ' :: ZF -B, whichisa \npath-based computationof function h :: \u00b5F -A, is said to be upward if there exist operator (\u00d8) :: .FA \n-B -B and value e :: B such that the following equations holds. h ' z = foldr (\u00d8) e (map (.Fh) z) We \ncharacterize scalable divide-and-conquer parallel programs by recursive division on one-hole contexts \nas the same as the case of node-valued binary trees. De.nition 5.4 (decomposition). Adecomposition of \nfunction h :: \u00b5F -A is tuple (f, ., .) that consists of function f::.FA -B, associative operator . :: \nB -B -B, and operator . :: B \u00ad\u00b5F -A such that the following equation holds for all t E leaves\u00b5F. h (z< \nt)= hom (.)(f o .Fh) z . t One of the most dif.cult issues is to .nd a scalable divide-and\u00adconquer parallel \nalgorithm toevaluate such functions.We (Mori\u00adhataand Matsuzaki2008)devisedan algorithmby generalizingthe \ntree contraction algorithm on binary trees given by Abrahamson etal. (1989).Wehavenot discussedthisin \ndetailhere, becauseitis beyond the scope of this paper. Theorem 5.5. If (f, ., .) is a decomposition \nof function h and all of f, ., and . are constant-time computations, then h can be evaluated for a tree \nof n nodes in O(n/p + log p) time on an exclusive-read/exclusive-write parallel random access machine \nwith p processors. Proof. It is not dif.cult to con.rm that h satis.es the premise of Theorem9in (Morihata \nand Matsuzaki 2008). 5.4 The Third Homomorphism Theorem onPolynomial Data Structures We have shown \nthat list homomorphisms characterize scalable divide-and-conquer parallel programs on polynomial data \nstruc\u00adtures. Therefore, we can utilize parallelization methods on lists for free.For instance, the third \nhomomorphism theorem on treesisa direct consequence of that on lists. Lemma 5.6. Assume that function \nh ', which is a path-based com\u00adputation of h so that h (z< t)= h ' z . t holds for all t E leaves\u00b5F, \nis both downward and upward; then, there exists a de\u00adcomposition (f, ., .) of h such that f (.Fha)= h \n' [a] and (h'. a . b = h ' a + h'. b) hold. Proof. Since h ' is both leftward and rightward, . is associative \nand h ' = hom (.)(f o .Fh) holds from Lemma 2.4. Therefore, (f, ., .) forms a decomposition of h. Theorem \n5.7 ( the third homomorphism theorem on polynomial data structures). Function h :: \u00b5F -A is decomposable \nif and only if there exist operators (\u00d8) :: .FA -B -B, (*) :: B \u00ad.FA -B, and (.) :: B -\u00b5F -A, such that \nthe following equations hold for all t E leaves\u00b5F. h (z< t)= foldr (\u00d8) e (map (.Fh) z) . t = foldl (*) \ne (map (.Fh) z) . t Proof. The if partisa direct consequenceof Lemma5.6,andthe only if part is straightforward. \n6. Discussion We have developed a method for systematically constructing scal\u00adable divide-and-conquer \nparallel programs on polynomial data structures.Wehave focusedonpathsandexpressedpathsbyzip\u00adpers to utilize \nthe known theories on lists. We have proposed a characterization of scalable divide-and-conquer parallel \nprograms on polynomial data structures, proved the third homomorphism theorem on polynomial data structures, \nand demonstrated the ef\u00adfectiveness of our method with examples. Our motivation was to derive the third \nhomomorphism theo\u00adrem on trees. The theorem, whichwas .rst introducedbyGibbons (1996), is useful for \ndeveloping parallel programs, and automatic parallelization methods have been proposed (Geser and Gorlatch \n1999; Morita et al. 2007) based on the theorem. Our theorem is an exact generalization of the original \nthird homomorphism theorem, and our results arebuilt on list homomorphisms. Therefore,exist\u00ading automatic \nparallelization methods should be applicable. Oneoftheaimsofthis paperistoexplain ideasfor constructing \nscalable parallel programs on trees through the development of our theory.Parallel tree contraction, \nwhichwas .rst proposedbyMiller and Reif (1985), is known to be a useful framework for developing scalable \nparallel programs on trees, and many computations have been implemented on it (Cole and Vishkin 1988; \nGibbons and Rytter 1989; Abrahamson et al. 1989; Skillicorn 1996; Gibbons et al. 1994; Matsuzaki et al. \n2003; Matsuzaki 2007); however, parallel tree contraction is hard to use, because it requires a set of \noperations that satisfya certain condition.Infact, the requirements for decomposable functions is equivalent \nto the suf.cient condition for parallel tree contraction discussed in (Matsuzaki et al. 2003), and the \nthird homomorphism theorem on trees brings sets of operations that satisfy the condition. Moreover, our \nresults can deal with polynomial structures, even though most of the existing studies have only considered \nbinary trees. The third homomorphism theorem requires two sequential pro\u00adgrams, while conventional parallelization \nmethods generate a par\u00adallel program froma sequential program.Even though this require\u00adment may have \nits shortcomings, it is arguably true. There is gen\u00aderally little hope of obtaining a parallel program \nfrom a sequen\u00adtial program, because parallel programs are more complicated than sequential ones. In other \nwords, extra information is necessary to develop parallel programs from sequential ones. What the third \nho\u00admomorphism theorem provides is a systematic way of revealing such extra information. We have developed \nour methods on polynomial data structures, which correspond to trees of bounded degree. Regular data \nstruc\u00adturesareageneralizationof polynomialdata structures,and include trees of unbounded degree. Since \nMcBride (2001) demonstrated a systematic derivation of zipper structures for regular data struc\u00adtures, \nthere are no problems with formalizing path-based computa\u00adtions, downward computations, or upward computations \non regular data structures. However,it is dif.cult to achievescalability on reg\u00adular data structures.We \nhave required the operation to merge the results of siblings in O(1) time. As this requirement is not \nrealistic on regular data structures, it is necessary to parallelize this merging operation. In summary, \nit would be interesting to investigate the third homomorphism theorem on regular data structures. Acknowledgments \nThe authors are grateful to Nao Hirokawa, who proposed using Huet s zippers for formalizing computations \non paths. The authors are also grateful to anonymous referees of POPL 2009 for their helpful comments. \nThe .rst author is supported by Grant-in-Aid for JSPS research fellows 20 \u00b7 2411. References Karl R. \nAbrahamson, N. Dadoun, David G. Kirkpatrick, and Teresa M. Przytycka. A simple parallel tree contraction \nalgorithm. Journal of Algorithms, 10(2):287 302, 1989. Roland Carl Backhouse, Patrik Jansson, Johan Jeuring, \nand Lambert G. L.T. Meertens. Generic programming: An introduction. In Advanced Functional Programming, \npages 28 115, 1998. Richard S. Bird. An introduction to the theory of lists. In Logic of Programming \nand Calculi of Discrete Design, pages 3 42. Springer, 1987.NATOASI SeriesFVolume 36. Wei-Ngan Chin. Towards \nan automated tupling strategy. In Proceedings of the ACM SIGPLAN symposium on Partial evaluation and \nsemantics\u00ad based program manipulation, PEPM 93, pages 119 132, 1993. Murray Cole. Parallel programming, \nlist homomorphisms and the maxi\u00admum segment sum problem. In Parallel Computing: Trends and Ap\u00adplications, \nPARCO 1993, Grenoble, France, pages 489 492. Elsevier, 1994. Murray Cole. Parallel programming with list \nhomomorphisms. Parallel Processing Letters, 5:191 203, 1995. Richard Cole and Uzi Vishkin. The accelerated \ncentroid decomposition technique for optimal parallel tree evaluation in logarithmic time. Algo\u00adrithmica, \n3:329 346, 1988. MaartenM.Fokkinga.Tuplingand mutumorphisms.In Squiggolist,volume 1(4), 1989. IanFoster. \nDesigning and Building Parallel Programs. AddisonWesley, 1995. Alfons Geser and Sergei Gorlatch. Parallelizing \nfunctional programs by generalization. Journal of Functional Programming, 9(6):649 673, 1999. Alan Gibbons \nandWojciech Rytter. Optimal parallel algorithm for dynamic expression evaluation and context-free recognition. \nInformation and Computation, 81(1):32 45, 1989. Jeremy Gibbons. The third homomorphism theorem. Journal \nof Functional Programming, 6(4):657 665, 1996. Jeremy Gibbons,Wentong Cai, and David B. Skillicorn. Ef.cient \nparallel algorithms for tree accumulations. Science of Computer Programming, 23(1):1 18, 1994. ZhenjiangHu,HideyaIwasaki,and \nMasatoTakeichi.Formalderivationof ef.cient parallel programsbyconstructionof list homomorphisms. ACM \nTransactions on Programming Languages and Systems, 19(3):444 461, 1997a. Zhenjiang Hu, Hideya Iwasaki, \nMasato Takeichi, and Akihiko Takano. Tupling calculation eliminates multiple data traversals. In Proceed\u00adings \nof the 2nd ACM SIGPLAN International Conference on Functional Programming, ICFP 97, Amsterdam, The Netherlands, \npages 164 175. 1997b. G\u00b4erard P. Huet. The zipper. Journal of Functional Programming, 7(5): 549 554, \n1997. Kiminori Matsuzaki. Parallel Programming with Tree Skeletons. PhD thesis, Graduate School of Information \nScience and Technology, The UniversityofTokyo, 2007. Kiminori Matsuzaki and Zhenjiang Hu. Implementation \nof tree skeletons on distributed-memory parallel computers. Technical Report METR 2006-65, Departmentof \nMathematical Informatics,UniversityofTokyo, December 2006. Kiminori Matsuzaki, Zhenjiang Hu, and MasatoTakeichi. \nParallelization with tree skeletons. In Euro-Par 2003. Parallel Processing, 9th Inter\u00adnational Euro-Par \nConference, Klagenfurt, Austria, August 26-29, 2003. Proceedings,volume 2790 of Lecture Notes in Computer \nScience, pages 789 798. Springer, 2003. Conor McBride. The derivative of a regular type is its type of \none-hole contexts. Unpublished manuscript, 2001. Gary L. Miller and John H. Reif. Parallel tree contraction \nand its applica\u00adtion. In 26th Annual Symposium on Foundations of Computer Science, 21-23 October 1985, \nPortland, Oregon, USA, pages 478 489. 1985. Akimasa Morihata and Kiminori Matsuzaki. Atree contraction \nalgorithm on non-binary trees. Technical Report METR 2008-27, Department of Mathematical Informatics, \nUniversityofTokyo, 2008. Kazutaka Morita, Akimasa Morihata, Kiminori Matsuzaki, Zhenjiang Hu, and MasatoTakeichi. \nAutomaticinversion generatesdivide-and-conquer parallel programs. In Proceedings of the ACM SIGPLAN 2007 \nCon\u00adference on Programming Language Design and Implementation, San Diego, California, USA, June 10-13, \n2007, pages 146 155, 2007. Simon Peyton Jones, editor. Haskell 98 Language and Libraries: The Revised \nReport. Cambridge University Press, 2003. John H. Reif, editor. Synthesis of Parallel Algorithms. Morgan \nKaufmann Publishers, 1993. David B. Skillicorn. Parallel implementation of tree skeletons. Journal of \nParallel and Distributied Computing, 39(2):115 125, 1996.   \n\t\t\t", "proc_id": "1480881", "abstract": "<p>Parallel programs on lists have been intensively studied. It is well known that associativity provides a good characterization for divide-and-conquer parallel programs. In particular, the third homomorphism theorem is not only useful for systematic development of parallel programs on lists, but it is also suitable for automatic parallelization. The theorem states that if two sequential programs iterate the same list leftward and rightward, respectively, and compute the same value, then there exists a divide-and-conquer parallel program that computes the same value as the sequential programs.</p> <p>While there have been many studies on lists, few have been done for characterizing and developing of parallel programs on trees. Naive divide-and-conquer programs, which divide a tree at the root and compute independent subtrees in parallel, take time that is proportional to the height of the input tree and have poor scalability with respect to the number of processors when the input tree is ill-balanced.</p> <p>In this paper, we develop a method for systematically constructing scalable divide-and-conquer parallel programs on trees, in which two sequential programs lead to a scalable divide-andconquer parallel program. We focus on paths instead of trees so as to utilize rich results on lists and demonstrate that associativity provides good characterization for scalable divide-and-conquer parallel programs on trees. Moreover, we generalize the third homomorphism theorem from lists to trees.We demonstrate the effectiveness of our method with various examples. Our results, being generalizations of known results for lists, are generic in the sense that they work well for all polynomial data structures.</p>", "authors": [{"name": "Akimasa Morihata", "author_profile_id": "81372591868", "affiliation": "University of Tokyo, Tokyo, Japan", "person_id": "P1300963", "email_address": "", "orcid_id": ""}, {"name": "Kiminori Matsuzaki", "author_profile_id": "81316489698", "affiliation": "University of Tokyo, Tokyo, Japan", "person_id": "P1300964", "email_address": "", "orcid_id": ""}, {"name": "Zhenjiang Hu", "author_profile_id": "81100253989", "affiliation": "National Institute of Informatics, Tokyo, Japan", "person_id": "P1300965", "email_address": "", "orcid_id": ""}, {"name": "Masato Takeichi", "author_profile_id": "81100466948", "affiliation": "University of Tokyo, Tokyo, Japan", "person_id": "P1300966", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1480881.1480905", "year": "2009", "article_id": "1480905", "conference": "POPL", "title": "The third homomorphism theorem on trees: downward & upward lead to divide-and-conquer", "url": "http://dl.acm.org/citation.cfm?id=1480905"}