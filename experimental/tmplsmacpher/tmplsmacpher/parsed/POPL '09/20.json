{"article_publication_date": "01-21-2009", "fulltext": "\n Semi-Sparse Flow-Sensitive Pointer Analysis Ben Hardekopf Calvin Lin The University of Texas at Austin \n {benh,lin}@cs.utexas.edu Abstract Pointer analysis is a prerequisite for many program analyses, and \nthe effectiveness of these analyses depends on the precision of the pointer information they receive. \nTwo major axes of pointer analysis precision are .ow-sensitivity and context-sensitivity,and while there \nhas been signi.cant recent progress regarding scalable context-sensitive pointer analysis, relatively \nlittle progress has been made in improving the scalability of .ow-sensitive pointer analysis. This paper \npresents a new interprocedural, .ow-sensitive pointer analysis algorithm that combines two ideas semi-sparse \nanalysis and a novel use of BDDs that arise from a careful understanding of the unique challenges that \nface .ow-sensitive pointer analysis. We evaluate our algorithm on 12 C benchmarks ranging from 11K to \n474K lines of code. Our fastest algorithm is on average 197\u00d7 faster and uses 4.6\u00d7 less memory than the \nstate of the art, and it can analyze programs that are an order of magnitude larger than the previous \nstate of the art. Categories and Subject Descriptors D.3.4 [Processors]: Com\u00adpilers; F.3.2 [Semantics \nof Programming Languages]: Program Analysis General Terms Algorithms, Languages Keywords Pointer analysis, \nalias analysis 1. Introduction Almost all program analyses are more effective when given precise pointer \ninformation, and the scalability of such program analyses is often dictated by the precision of this \npointer information [46]. Two major dimensions of pointer analysis precision are .ow-sensitivity and \ncontext-sensitivity, which improve precision in complementary ways. A context-sensitive analysis respects \nthe semantics of pro\u00adcedure calls by analyzing each distinct procedure context indepen\u00addently, whereas \na context-insensitive analysis merges contexts to\u00adgether. A .ow-sensitive analysis respects the control-.ow \nof a pro\u00adgram and instead computes a separate solution for each program point, whereas a .ow-insensitive \nanalysis does not respect control\u00ad.ow and computes a single solution that conservatively holds for the \nentire program. Recently, the scalability of both .ow-insensitive pointer analy\u00adsis [4, 22, 23, 25, 40, \n41] and context-sensitive pointer analysis [10, 30, 33, 36, 39, 48, 52] has been greatly improved. In \ncontrast, there Permission to make digital or hard copies of all or part of this work for personal or \nclassroom use is granted without fee provided that copies are not made or distributed for pro.t or commercial \nadvantage and that copies bear this notice and the full citation on the .rst page. To copy otherwise, \nto republish, to post on servers or to redistribute to lists, requires prior speci.c permission and/or \na fee. POPL 09, January 18 24, 2009, Savannah, Georgia, USA. Copyright c . 2009 ACM 978-1-60558-379-2/09/01. \n. . $5.00 has been relatively little progress on improving the performance of .ow-sensitive pointer analysis. \nThis lack of progress is unfortu\u00adnate, because .ow-sensitive pointer analysis has been shown to be bene.cial \nto important problems, also known as clients,suchas se\u00adcurity analysis [7, 17], deep error checking [20], \nhardware synthe\u00adsis [51], and the analysis of multi-threaded programs [45], among others [3, 9, 18]. \nIn this paper, we present a new interprocedural, .ow-sensitive pointer analysis algorithm that signi.cantly \nimproves upon the state of the art. The algorithm as presented is context-insensitive, but it could be \nextended to add context-sensitivity using one of several available techniques. Indeed, there is evidence \nthat .ow-sensitivity and context-sensitivity conspire to improve the behavior of client analyses [20]. \nNevertheless, by concentrating exclusively on .ow\u00adsensitivity, we isolate its effects and directly address \nits particular challenges. 1.1 Challenges and Insights Flow-sensitive pointer analysis presents unique \nchallenges that hin\u00adder scalability. We will discuss these challenges in detail in Sec\u00adtion 2.3 after \ndescribing .ow-sensitive pointer analysis in Sec\u00adtion 2.1, but we summarize the challenges now so that \nwe can ex\u00adplain our insights for dealing with them. Traditional .ow-sensitive pointer analysis relies \non the standard iterative data.ow technique, which must conservatively and inef\u00ad.ciently propagate pointer \ninformation to all reachable program points in case any of those points uses that information. Many program \nanalyses have instead employed static single assignment (SSA) form to enable sparse analysis, which allows \ndata.ow in\u00adformation to .ow directly from variable de.nitions to their cor\u00adresponding uses [43]. These \ndef-use chains allow the analysis to avoid propagating information where it is not needed, greatly increasing \nanalysis ef.ciency. Unfortunately, the construction of these def-use chains requires pointer analysis \nto determine where variables are de.ned and used, so pointer analysis itself is unable to exploit this \ntechnique. In addition, .ow-sensitive pointer analysis has prohibitive mem\u00adory requirements and uses \nexpensive set operations. Previous work on pointer analysis has applied symbolic analysis, using binary \nde\u00adcision diagrams (BDDs) [6], to both reduce memory usage and de\u00adcrease the cost of set operations [4, \n48, 52]. Unfortunately, .ow\u00adsensitive analysis presents a challenge for symbolic analysis be\u00adcause of \nthe presence of strong updates. Strong updates enable the analysis to kill old pointer information when \na variable is assigned new information. Indirect strong updates (those involving mem\u00adory store instructions \nsuch as *x=y) are problematic for sym\u00adbolic analysis because they require that each program statement \nbe processed independently, something for which BDDs are not well\u00adsuited. Previous attempts at using \nBDDs for .ow-sensitive pointer analysis [52] have been forced to sacri.ce precision to achieve ac\u00adceptable \nperformance. Our algorithm overcomes these challenges with two insights: 1. There exists a subset of \nprogram variables (those that cannot be referenced indirectly via a pointer, called top-level variables) \nthat can be converted to SSA form without the bene.t of pointer information. For most programs, the majority \nof variables are top-level variables, so our solution employs a partial static sin\u00adgle assignment form \nthat performs a sparse analysis on top-level variables while using the standard iterative data.ow algorithm \nfor the remaining variables. 2. We can both preserve the precision bene.ts of strong updates and obtain \nmost of the performance bene.t of symbolic analy\u00adsis by con.ning the use of BDDs to the representation \nof pointer information, leaving the rest of the analysis unchanged. In ef\u00adfect, we use a partially-symbolic \npointer analysis.   1.2 Contributions This paper introduces a new .ow-sensitive pointer analysis algo\u00adrithm \nthat is signi.cantly more ef.cient both in terms of analysis time and memory usage than the previous \nstate of the art. More speci.cally, our contributions are as follows: We identify three major challenges \nthat limit the performance of .ow-sensitive pointer analysis, and we explain how all previous approaches \nto optimizing the analysis as well as our new algorithm target one or more of these challenges.  We \nintroduce a new type of .ow-sensitive pointer analysis called semi-sparse analysis, which signi.cantly \nimproves scal\u00adability. We further introduce two new optimizations for .ow\u00adsensitive analysis, Top-Level \nPointer Equivalence and Local Points-to Graph Equivalence, that are enabled by the use of semi-sparse \nanalysis.  We present the .rst use of BDDs for .ow-sensitive pointer analysis that allows the use of \nindirect strong updates. We also explore the BDD data structure s strengths and weaknesses compared to \nmore conventional data structures.  We compare our new semi-sparse analysis (SS) and semi-sparse analysisextendedwithourtwonewoptimizations \n(SSO)against a baseline algorithm for .ow-sensitive pointer analysis (IFS, standing for Iterative data.ow \nFlow-Sensitive analysis) based on the work of Hind and Pioli [28]. Our evaluation uses 12 C programs \nranging in size from 11K to 474K lines of code (LOC), and it considers two different data structures \nfor storing pointer information, namely, BDDs and sparse bitmaps. These two data structures are used \nto create two different versions of each algorithm: IFS, SS,and SSO.  When using sparse bitmaps to store \npointer information for both SSO and IFS,we .nd that SSO is 183\u00d7 faster than IFS and uses 47\u00d7 less memory. \nWhen using BDDs for both algorithms, we .nd that SSO is 114\u00d7 faster than IFS and uses 1.4\u00d7 less memory. \nOverall, our fastest analysis (SSO using BDDs) is 197\u00d7 faster, uses 4.6\u00d7 less memory, and can analyze \nprograms that have 323K lines of code, which is an order of magnitude larger than the baseline algorithm \n(IFS using sparse bitmaps). The rest of the paper is organized as follows. Section 2 ex\u00adplains .ow-sensitive \npointer analysis and identi.es the three major challenges that hinder scalability. Section 3 describes \nrelated work and it addresses these challenges. Section 4 describes partial static single assignment \nform, Section 5 introduces our new semi-sparse analysis and optimizations, and Section 6 explores the \nusefulness of BDDs. Section 7 gives a detailed evaluation of all the analyses, and Section 8 concludes. \n 2. Background This section brie.y describes .ow-sensitive pointer analysis and enumerates the major \nchallenges in making the analysis practi\u00adcal for large programs. Further details on the basic .ow-sensitive \npointer analysis algorithm are described by Hind et al [27]. 2.1 Flow-Sensitive Pointer Analysis Flow-sensitive \npointer analysis respects a program s control .ow and computes a separate solution for each program point, \nin con\u00adtrast to a .ow-insensitive analysis, which ignores statement order\u00ading and computes a single solution \nthat is conservatively correct for all program points. Traditional .ow-sensitive pointer analysis uses \nan iterative data.ow analysis framework, which employs a lattice of data.ow facts L, a meet operator \non the lattice, and a family of monotone transfer functions fi : L . L that map lattice elements to other \nlat\u00adtice elements. For pointer analysis the lattice elements are points-to graphs, the meet operator \nis set union, and each transfer function computes the effects of a program statement to transform an \ninput points-to graph into an output points-to graph. Analysis is carried out on the control-.ow graph \n(CFG), a directed graph G = .N,E. with a .nite set of nodes (or program points), N, corresponding to \nprogram statements and a set of edges E . N \u00d7 N corresponding to the control .ow between statements. \nTo ensure decidability of the analysis branch conditions are uninterpreted and branches are treated as \nnon-deterministic. Each node k of the CFG maintains two points-to graphs: INk, representing the incoming \npointer information, and OUTk,repre\u00adsenting the outgoing pointer information. Each node is associated \nwith a transfer function that transforms INk to OUTk, characterized by the sets GENk and KILLk, which \nrepresent the pointer informa\u00adtion generated by the node and killed by the node, respectively. The contents \nof these two sets depend on the particular program state\u00adment associated with node k, and the contents \ncan vary over the course of the analysis as new pointer information is accumulated (though the transfer \nfunction is still guaranteed to be monotonic). The analysis iteratively computes the following two functions \nfor all nodes k until convergence: INk = [ OUTx (1) x.pred(k) OUTk = GENk . (INk - KILLk) (2) The KILL \nset determines whether the analysis performs a strong or weak update to the left-hand side of an assignment. \nWhen the left-hand side de.nitely refers to a single memory location v,a strong update occurs in which \nthe KILL set is used to remove all points-to relations v . x prior to updating v with a new set of points-to \nrelations. If the left-hand side cannot be determined to point to a single memory location, then a weak \nupdate occurs: The analysis cannot be sure which of the possible memory locations should actually be \nupdated by the assignment, so to be conservative it must set KILL to the empty set to preserve all of \nthe existing points-to relations. An important aspect of any pointer analysis is the heap model, i.e., \nhow the conceptually in.nite-size heap is abstracted into a .nite set of memory locations. The most common \npractice, which we follow in this paper, is to treat each static memory allocation site as a single abstract \nmemory location (which may map onto multiple concrete memory locations during program execution).  2.2 \nThe Importance of Flow-Sensitive Pointer Analysis Some previous work has created a perception that the \nextra preci\u00adsion of .ow-sensitive pointer analysis is not bene.cial [28, 37], but as researchers attack \nnew program analysis problems, we believe that this perception should be questioned for the following \nreasons: Different client program analyses require different amounts of precision from the pointer analysis \n[26]. The list of client analy\u00adses that have been shown to bene.t from .ow-sensitive pointer analysis \nincludes several software engineering applications of growing importance, including security analysis \n[7, 17], deep error checking [20], hardware synthesis [51], and the analysis of multi-threaded programs \n[45], among others [3, 9, 18].  The precision of pointer analysis is typically measured in terms of \nmetrics that are averaged over the entire program. In cases such as security analysis and parallelization, \nthese metrics can be misleading a small amount of imprecision in isolated parts of the program can signi.cantly \nimpact the effectiveness of the client analysis, as demonstrated by Guyer et al [20]. Thus, two different \npointer analyses can have very similar average points\u00adto set sizes but very different impact on the client \nanalysis.  In a vicious cycle, the lack of an ef.cient .ow-sensitive pointer analysis has inhibited \nthe use of .ow-sensitive pointer analy\u00adses. The development and widespread use of a scalable .ow\u00adsensitive \npointer analysis would likely uncover additional client analyses that bene.t from the added precision. \n Several techniques [8, 17, 20, 21, 49] can improve the preci\u00adsion of .ow-sensitive pointer analysis, \nbut most of these tech\u00adniques greatly increase the cost of the pointer analysis, mak\u00ading an already non-scalable \nanalysis even more impractical. A signi.cantly more ef.cient .ow-sensitive pointer analysis algo\u00adrithm \nwould improve the practicality of such techniques, mak\u00ading .ow-sensitive pointer analysis even more useful. \n  Thus, we conclude that there are many reasons to seek a more scalable interprocedural .ow-sensitive \npointer analysis. 2.3 Challenges Facing Flow-Sensitive Pointer Analysis There are three major performance \nchallenges facing .ow-sensitive pointer analysis: 1. Conservative propagation. Without pointer information \nit is in general not possible to determine where variables are de.ned or used. Therefore, the analysis \nmust propagate the pointer infor\u00admation generated at each node k to all nodes in the CFG reach\u00adable from \nk in case those nodes use the information. Typically, however, only a small percentage of the reachable \nnodes actu\u00adally require the information, so most of the nodes receive the information needlessly. The \neffect is to greatly delay the con\u00advergence of equations (1) and (2). 2. Expensive transfer functions. \nEquations (1) and (2) require a number of set operations with complexity linear in the sizes of the sets \ninvolved. These sets tend to be large, with potentially hundreds to thousands of elements. This problem \nis exacerbated by the analysis conservative propagation which requires the nodes to needlessly re-evaluate \ntheir transfer functions when they receive new pointer information even when that informa\u00adtion is irrelevant \nto the node. 3. High memory requirements. Each node in the CFG must maintain two separate points-to \ngraphs, IN for the incoming in\u00adformation and OUT for the outgoing information. For large pro\u00adgrams that \nhave hundreds of thousands of nodes, these points-to graphs consume a signi.cant amount of memory. This \nproblem is also exacerbated by the analysis conservative propagation  which requires the IN and OUT \ngraphs to hold pointer informa\u00adtion irrelevant to the node in question. All of the work in improving \nthe scalability of .ow-sensitive pointer analysis can be seen as addressing one or more of these challenges. \nIn the next section we review past efforts at meeting these challenges before describing our own solution \nto the problem.  3. Related Work The current state of the art for traditional .ow-sensitive pointer \nanalysis using iterative data.ow analysis is described by Hind and Pioli [27, 28], and their analysis \nis the baseline that we use for evaluating our new techniques. Their analysis employs three major optimizations: \n1. Sparse evaluation graph (SEG) [11, 16, 42]. These graphs are derived from the CFG by eliding nodes \nthat do not manipu\u00adlate pointer information and hence are irrelevant to pointer analysis while maintaining \nthe control-.ow relations among the remaining nodes. There are a number of techniques for con\u00adstructing \nSEGs, which vary in the complexity of the algorithm and the size of the resulting graph. The use of SEGs \naddresses challenges (1) and (3) by signi.cantly reducing the input to the analysis. 2. Priority-based \nworklist. Nodes awaiting processing are placed on a worklist prioritized by the topological order of \nthe CFG, such that nodes higher in the CFG are processed before nodes lower in the CFG. This optimization \naims to amass at each node as much new incoming pointer information as possible before processing the \nnode, thereby addressing challenge (2) by reducing the number of times the node must be processed. 3. \nFiltered forward-binding. When passing pointer information to the target of a function call, it is unnecessary \nto pass every\u00adthing. The only pointer information that the callee can access is that which is accessible \nfrom a global or from one of the func\u00adtion parameters. Challenges (1) and (3) can thus be addressed by \n.ltering out the remaining information to add. Less informa\u00adtion is propagated unnecessarily, which leads \nto smaller points\u00adto graphs.  These optimizations speed up the analysis by an average of over 25\u00d7. The \nlargest benchmarks analyzed are up to 30,000 lines of code (LOC). To improve scalability, several non-traditional \napproaches to .ow-sensitive pointer analysis have been proposed. These ap\u00adproaches take inspiration from \na number of non-pointer-related program analyses which have addressed similar challenges using a sparse \nanalysis, including the use of static single assignment (SSA) form. Pointer analysis cannot directly \nmake use of SSA because pointer information is required to compute SSA form. Cytron et al [14] do propose \na scheme for incrementally computing pointer information while converting to SSA form; by incorporating \nthe minimum amount of pointer information necessary, this scheme re\u00adduces the size of the resulting SSA \nform. However, this technique does not speed up the computation of the pointer information it\u00adself. We \nnow describe two approaches that use SSAs for the actual computation of pointer information. Hasti and \nHorwitz [24] propose a scheme composed of two passes: a .ow-insensitive pointer analysis that gathers \npointer in\u00adformation and a conversion pass that uses the pointer information to transform the program \ninto SSA form. The result of the second pass is iteratively fed back into the .rst pass until convergence \nis reached. Hasti and Horwitz leave open the question of whether the resulting pointer information is \nequivalent to a .ow-sensitive anal\u00adysis; we believe that the resulting information is less precise than \na full .ow-sensitive pointer analysis. No experimental evaluation of this technique has been published. \n Chase et al [8] propose a technique that dynamically transforms the program to SSA form during the course \nof the .ow-sensitive pointer analysis. There is no experimental evaluation of this pro\u00adposed technique; \nhowever, a similar idea is described and experi\u00admentally evaluated by Tok et al [47]. The technique can \nanalyze programs that are twice as large as those that use iterative data.ow, enabling the analysis of \n70,000 LOC in approximately half-an\u00adhour. Unfortunately, the cost of dynamically computing SSA form limits \nthe scalability of the analysis. We cannot use a common infrastructure to compare Tok et al s technique \nwith ours, because their technique targets programs that begin in non-SSA form, whereas we use the LLVM \ninfrastruc\u00adture [32], which automatically transforms a program into partial SSA form as described in \nSection 4. While the comparison is im\u00adperfect due to infrastructure differences, our fastest analysis \n(SSO using BDDs) is 1,286\u00d7 faster and uses 11.5\u00d7 less memory on sendmail, the only benchmark common to \nboth studies. A different approach that primarily targets challenges (2) and (3) is symbolic analysis \nusing Binary Decision Diagrams (BDDs), which has been used with great success in model checking [2]. \nA number of papers have shown that symbolic analysis can greatly improve the performance of .ow-insensitive \npointer analysis [4, 48, 50, 52]. In addition, Zhu [51] uses BDDs to compute a .ow-and context-sensitive \npointer analysis for C programs. The analysis is fully symbolic (everything from the CFG to the pointer \ninformation is represented using BDDs) but not fully .ow-sensitive the anal\u00adysis cannot perform indirect \nstrong updates, so the KILL sets are more conservative (i.e., smaller) than a fully .ow-sensitive anal\u00adysis. \nSymbolic analysis is discussed in more detail in Section 6. Zhu does not show results for a .ow-sensitive, \ncontext-insensitive analysis, so we cannot directly compare his techniques with ours. There have been \nseveral other approaches to optimizing .ow\u00adsensitive pointer analysis that improve scalability by pruning \nthe input given to the analysis. Rather than improve the scalability of the pointer analysis itself, \nthese techniques reduce the size of its input. Client-driven pointer analysis analyzes the needs of a \npartic\u00adular client and applies .ow-sensitive pointer analysis only to por\u00adtions of the program that require \nthat level of precision [20]. Fink et al use a similar technique speci.cally for typestate analysis by \nsuccessively applying more precise pointer analyses to a program, pruning away portions of the program \nas each stage of precision has been successfully veri.ed [17]. Kahlon bootstraps the .ow\u00adsensitive pointer \nanalysis by using a .ow-insensitive pointer analy\u00adsis to partition the program into sections that can \nbe analyzed inde\u00adpendently [29]. These approaches can be combined with our new .ow-sensitive pointer \nanalysis to achieve even greater scalability. 4. Partial Static Single Assignment Form Static single \nassignment (SSA) form is an intermediate representa\u00adtion that requires each variable in a program to \nbe de.ned exactly once. Variables de.ned multiple times in the original representa\u00adtion are split into \nseparate instances, one for each de.nition. When separate instances of the same variable are live at \na join point in the control-.ow graph, they are combined using a f function, which takes the old instances \nas arguments and assigns the result to a new instance. One bene.t of SSA form is that each use of a variable \nis domi\u00adnated by exactly one de.nition, so it is trivial to match de.nitions with their corresponding \nuses, enabling sparse analyses. Thus, SSA form addresses all three major challenges identi.ed in Section \n2.3: It speeds up convergence, reduces the number of times transfer functions need to be evaluated, and \nreduces the sizes of the points\u00adto graphs stored at each node. int a, b, *c, *d; w1 = ALLOCa int* w=&#38;a; \n x1 = ALLOCb int* x=&#38;b; y1 = ALLOCc int** y = &#38;c; z1= y1 int**z= y; STORE 0 y1 c= 0; STORE \nw1 y1 *y= w; STORE x1 z1 *z= x; y2 = ALLOCd y = &#38;d; z2= y2 z= y; STORE w1 y2 *y= w; STORE x1 \nz2 *z= x; Figure 1. Example partial SSA code. On the left is the original C code, on the right is the \ntransformed code in partial SSA form. There are many known algorithms for converting a program into SSA \nform [1, 5, 13, 15]. However, the problem becomes more dif.cult when we consider indirect de.nitions \nthrough pointers. To correctly construct SSA form, we must know which variables are de.ned and/or used \nat each statement, which in turn requires pointer analysis. Even after pointer information becomes available, \nwe must either greatly complicate the SSA form [12] or sacri.ce much of its utility [31]. To overcome \nthese issues, modern compilers such as GCC [38] and LLVM [32] use a variant of SSA, which we refer to \nas partial SSA form. The key idea is to divide variables into two classes. One class contains variables \nthat are never referenced by pointers, so their de.nitions and uses can be trivially determined by inspection, \nand these variables can be converted to SSA using any algorithm for constructing SSA form. The other \nclass contains those vari\u00adables that can be referenced by pointers, and these variables are not placed \nin SSA form because of the above-mentioned complica\u00adtions. 4.1 LLVM Our semi-sparse analysis is implemented \nin the LLVM infrastruc\u00adture, so the rest of this section describes LLVM s internal repre\u00adsentation (IR) \nand its particular instantiation of partial SSA form. While the details and terminology are speci.c to \nLLVM, the ideas can be translated to other forms of partial SSA. LLVM s IR recognizes two classes of \nvariables: (1) top-level variables are those that cannot be referenced indirectly via a pointer, i.e., \nthose whose address is never exposed via the address\u00adof operator or returned via a dynamic memory allocation; \n(2) address-taken variables are those that have had their address ex\u00adposed and therefore can be indirectly \nreferenced via a pointer. Top\u00adlevel variables are kept in a (conceptually) in.nite set of virtual reg\u00adisters \nwhich are maintained in SSA form. Address-taken variables are kept in memory, and they are not in SSA \nform. Address-taken variables are accessed via LOAD and STORE instructions, which take top-level pointer \nvariables as arguments. These address-taken variables are never referenced syntactically in the IR; they \ninstead are only referenced indirectly using these LOAD and STORE in\u00adstructions. LLVM instructions use \na 3-address format, so there is at most one level of pointer dereference for each instruction. Figure \n1 provides an example of a C code fragment and its corresponding partial SSA form. Variables w, x, y,and \nz are top\u00adlevel variables and have been converted to SSA form; variables a, b, c,and d are address-taken \nvariables, so they are stored in memory and accessed solely via LOAD and STORE instructions. Because \nthe address-taken variables are not in SSA form, they can each be de.ned multiple times, as with variables \nc and d in the example. int **a, *b, c; a = &#38;b; a = ALLOCb b = &#38;c; t = ALLOCc c=0; STORE t a \nSTORE 0 t Figure 2. Example partial SSA code. On the left is the original C code, on the right is the \ntransformed code in partial SSA form. Because address-taken variables cannot be directly named, LLVM \nmaintains the invariant that each address-taken variable has at least one virtual register that refers \nonly to that variable. To illustrate this point, Figure 2 shows how a temporary variable, t, is introduced \nin the LLVM IR to take the place of the variable b, which in the original C code is referenced by a pointer. \nLLVM also treats global variables specially. Def-use chains for global variables can span multiple functions; \nhowever, in the presence of indirect function calls it is not possible to construct precise def-use chains \nacross function boundaries without pointer information. To address this issue, LLVM adds an extra level \nof indirection to each global variable: T glob becomes const T* glob,where T is the type of the global \ndeclared in the original program. The const pointers are initialized to point to an address\u00adtaken variable \nthat represents the original global variable. This modi.cation means that pointer information for global \nvariables is propagated along the SEG rather than relying on cross-function def-use chains. Note: The \nrest of this paper will assume the use of the LLVM IR, which means that any named variable is a top-level \nvariable and not an address-taken variable. 4.2 Advantages of Partial SSA For .ow-sensitive pointer \nanalysis, partial SSA form has several important implications which have not been previously identi.ed \nor explored: 1. The analysis can use a single global points-to graph to hold the pointer information \nfor all top-level variables. Since the variables are in SSA form, they will necessarily have the same \npointer information over the entire program. The presence of this global points-to graph means the analysis \ncan avoid storing and propagating the pointer information for top-level variables among CFG nodes. 2. \nDef-use information for top-level variables is immediately available, as in a sparse analysis. When pointer \ninformation for a top-level variable changes, the affected program statements can be directly determined, \nwhich can dramatically speed up the convergence of the analysis and reduce the number of trans\u00adfer functions \nthat must be evaluated. 3. Local points-to graphs, i.e., separate IN and OUT graphs for each CFG node, \nare still needed for LOAD and STORE state\u00adments, but these graphs only hold pointer information for address-taken \nvariables. The exclusion of top-level variables can signi.cantly reduce the sizes of these local points-to \ngraphs.   5. Semi-Sparse Analysis Semi-sparse analysis takes advantage of partial SSA form to greatly \nincrease the ef.ciency of the .ow-sensitive pointer analysis. In order to do so, we introduce a construct \ncalled the Data.ow Graph. We .rst describe the characteristics of the data.ow graph and how it is constructed, \nand we then describe the semi-sparse analysis itself, followed by the new optimizations enabled by partial \nSSA. Inst Type Example Def-Use Info ALLOC x = ALLOCi DEFtop COPY x = yz DEFtop, USEtop LOAD x = *y DEFtop, \nUSEtop, USEadr STORE *x = y USEtop, DEFadr, USEadr CALL x = foo(y) DEFtop, USEtop, DEFadr, USEadr RET \nreturn x USEtop, USEadr Table 1. Types of instructions relevant to pointer analysis. Instruc\u00adtions such \nas x=&#38;y are converted into ALLOC instructions, much like C s alloca. Def-Use Info describes whether \nthe instruction can de.ne or use top-level variables (DEFtop and USEtop, respectively) and whether it \ncan de.ne or use address-taken variables (DEFadr and USEadr, respectively). Recall that all named variables \nare, by construction, top-level. 5.1 The Data.ow Graph The data.ow graph (DFG) is a combination of a \nsparse evaluation graph (SEG) and def-use chains. This combination is required by the nature of partial \nSSA form, which provides def-use information for the top-level variables but not for the address-taken \nvariables. Without access to def-use information, an iterative data.ow analysis propagates information \nalong the control-.ow graph. As described in Section 3, the SEG is simply an optimized version of the \ncontrol-.ow graph that elides nodes that neither de.ne nor use pointer information. Since address-taken \nvariables do not have def\u00aduse information available, program statements that de.ne or use address-taken \nvariables must be connected via a path in the SEG so that variable de.nitions will correctly reach their \ncorrespond\u00ading uses. Since top-level variables have def-use information imme\u00addiately available, program \nstatements that de.ne or use top-level variables can be connected via these def-use chains. To construct \nthe DFG there are 6 types of relevant program statements, shown in Table 1. For each statement, the table \nlists whether it de.nes and/or uses top-level variables (DEFtop and USEtop, respectively) and whether \nthe statement de.nes and/or uses address-taken variables (DEFadr and USEadr, respectively). STORE instructions \nare labeled USEadr because weak updates require the updated variable s previous points-to set. CALL instructions \nare la\u00adbeled DEFadr because they can modify address-taken variables via the callee function. CALL and \nRET instructions are labeled USEadr because they need to pass the address-taken pointer information to/from \nthe callee function. COPY instructions can have multiple variables on the right-hand side, which allows \nit to accommodate SSA f functions. The DFG is constructed in two stages. In the .rst stage, a stan\u00addard \nalgorithm for creating an SEG (such as Ramalingam s linear\u00adtime algorithm [42]) is used. Only program \nstatements labeled DEFadr or USEadr are considered relevant; all others are elided. Then a linear pass \nthrough the partial SSA representation is used to connect program statements that de.ne top-level variables \nwith those that use those variables. Figure 3 shows the DFG correspond\u00ading to the partial SSA code in \nFigure 1. Theorem 1 (Correctness of the DFG). There exists a path in the DFG from all variable de.nitions \nto their corresponding uses. Proof. We proceed by cases based on the type of variable: Top-level: Def-use \ninformation for top-level variables is exposed by the partial SSA form; the DFG directly connects top-level \nvariable de.nitions to their uses, so the theorem is trivially true. Address-taken: All uses of a variable \ns de.nition must be reach\u00ad able from the statement that created the de.nition in the original Figure \n3. Example DFG corresponding to the code in Figure 1. Dashed edges are def-use chains; solid edges are \nfor the SEG.  control-.ow graph. The SEG preserves control-.ow informa\u00adtion for all statements that \neither de.ne or use address-taken variables. Therefore any use of an address-taken variable s def\u00adinition \nmust be reachable from the statement that created the de.nition in the SEG.  5.2 The Analysis The pointer \nanalysis itself is similar to that described by Hind and Pioli [27, 28]. The analysis uses the following \ndata structures: Each function F has its own program statement worklist StmtWorklistF . The worklist \nis initialized to contain all state\u00adments in the function that de.ne a variable (i.e., are labeled DEFadr \nor DEFtop).  Each program statement k that uses or de.nes address-taken variables (i.e., is labeled \nUSEadr or DEFadr) has two points-to graphs, INk and OUTk, which hold the incoming and outgoing pointer \ninformation for address-taken variables. Let Pk(v) be the points-to set of address-taken variable vin \nINk.  A global points-to graph PGtop holds the pointer information for all top-level variables. Let \nPtop(v) be the points-to set of top-level variable vin PGtop.  A worklist FunctionWorklist holds functions \nwaiting to be pro\u00adcessed. The worklist is initialized to contain all functions in the program.  The \nmain body of the analysis is listed in Algorithm 1. The outer loop selects a function from the function \nworklist, and the inner loop iteratively selects a program statement from that function s statement worklist \nand processes it, continuing until the statement worklist is empty. Then the analysis selects a new function \nfrom the function worklist, continuing until the function worklist is also empty. Each type of program \nstatement is processed as shown in Algorithms 5 10. These algorithms use the helper functions listed \nin Algorithms 2 4. The .. operator represents set update and duSEG -. and --. represent a def-use edge \nor SEG edge in the DFG, respectively. Algorithm 1 Main body of the semi-sparse analysis algorithm. Require: \nDFG = . N, E. while FunctionWorklist is not empty do F =SELECT(FunctionWorklist) while StmtWorklistF \nis not empty do k =SELECT(StmtWorklistF ) switch typeof (k): case ALLOC: processAlloc(F, k) case COPY: \nprocessCopy(F, k) case LOAD: processLoad(F, k) case STORE: processStore(F, k) case CALL: processCall(F, \nk) case RET: processRet(F, k) Algorithm 2 propagateTopLevel(F, k) if PGtop changed then du StmtWorklistF \n.. { n | k -. n . E} Algorithm 3 propagateAddrTaken(F, k) SEG for all { n . N | k --. n . E} do INn .. \nOUTk if INn changed then StmtWorklistF .. { n} Algorithm 4 .lter(k) return the subset of INk reachable \nfrom either a call argument or global variable  5.3 Optimizations Partial SSA form allows us to introduce \ntwo additional optimiza\u00adtion opportunities: top-level pointer equivalence and local points-to graph equivalence. \n 5.3.1 Top-level Pointer Equivalence Top-level Pointer Equivalence reduces the number of top-level vari\u00adables \nin the DFG, which reduces the amount of pointer information that must be maintained by the global top-level \npoints-to graph. In addition, it eliminates nodes from the DFG, which reduces the number of transfer \nfunctions that must be processed, speeding up convergence. The basic idea is to identify sets of variables \nthat have identical points-to sets and to replace each set by a single set repre\u00adsentative. Pointer equivalent \nvariables are those that have identical points\u00adto sets. More formally, let . be the points-to relation \nand . be the pointer equivalence relation; then . x, y, z. Variables : x. yiff x. z. y. z. Program variables \ncan be partitioned into disjoint sets based on the pointer equivalence relation; an arbi\u00adtrary member \nof each set is then selected as the set representative. By replacing all variables in a program with \ntheir respective set representatives and then eliding trivial assignments (e.g., x=x), Algorithm 5 processAlloc(F, \nk): [x= ALLOCi] PGtop .. {x .ALLOCi}propagateTopLevel(F, k) Algorithm 6 processCopy(F, k): [x=y z...] \nfor all v .right-hand side do PGtop .. {x .Ptop(v)}propagateTopLevel(F, k) Algorithm 7 processLoad(F, \nk): [x= *y] PGtop .. {x .Pk(Ptop(y))}OUTk .. INk propagateTopLevel(F, k) propagateAddrTaken(F, k) Algorithm \n8 processStore(F, k): [*x=y] if Ptop(x) represents a single memory location then // strong update OUTk \n. . (INk \\Ptop(x)) .{Ptop(x) .Ptop(y)} else // weak update OUTk . . INk .{Ptop(x) .Ptop(y)}propagateAddrTaken(F, \nk) Algorithm 9 processCall(F, k): [x = foo(y)] if foo is a function pointer then targets := Ptop(foo) \nelse targets := {foo}.lt := .lter(k) for all C .targets do for all call arguments a and corresponding \nparameters p do PGtop .. {p .Ptop(a)}propagateTopLevel(C, p) Let n be the SEG start node for function \nC INn .. .lt if INn changed then StmtWorklistC .. {n}if StmtWorklistC changed then FunctionWorklist .. \n{C}OUTk .. INk \\.lt propagateAddrTaken(F, k) Algorithm 10 processRet(F, k): [return x] callsites := \nthe set of CALL statements targeting F for all n .callsites do Let Fn be the function containing n OUTn \n.. OUTk propagateAddrTaken(Fn, n) if n is of the form r = F(...) then PGtop .. {r .Ptop(x)} propagateTopLevel(Fn, \nn) if StmtWorklistFn changed then FunctionWorklist .. {Fn} we can reduce the number of variables and \nthe size of the program that are given as input to the pointer analysis. This idea has been previously \nexplored for .ow-insensitive pointer analysis [23, 44]. A different approach that primarily targets challenges \n(2) and (3) is symbolic analysis using Binary Decision Diagrams (BDDs), which has been used with great \nsuccess in model checking [2]. A number of papers have shown that symbolic analysis can greatly improve \nthe performance of .ow-insensitive pointer analysis [4, 48, 50, 52]. In addition, Zhu [51] uses BDDs \nto compute a .ow-and context-sensitive pointer analysis for C programs. The analysis is fully symbolic \n(everything from the CFG to the pointer information is represented using BDDs) but not fully .ow-sensitive \nthe anal\u00adysis cannot perform indirect strong updates, so the KILL sets are more conservative (i.e., smaller) \nthan a fully .ow-sensitive analy\u00adsis. This work is explored in more detail in Section 6. Zhu does not \nshow results for a .ow-sensitive, context-insensitive analysis, so we cannot directly compare his techniques \nwith ours. Partial SSA form provides an opportunity to apply this optimization to .ow\u00adsensitive pointer \nanalysis as well. To do so, we must be able to iden\u00adtify pointer-equivalent variables prior to the pointer \nanalysis itself. Theorem 2 shows how we can identify top-level pointer-equivalent variables under certain \ncircumstances. Theorem 2 (Top-level pointer equivalence). A COPY statement of the form [x=y] .x . y. \nProof. Top-level variables are in SSA form, which means that they are each de.ned exactly once. Therefore, \nthe value of each top-level variable does not change once it is de.ned. Since x and y are top-level variables, \ntheir values never change. The COPY statement assigns x the value of y,so x . y. Theorem 2 says that \nvariables involved in a COPY statement with a single variable on the right-hand side are pointer equivalent, \nso they can be replaced with a single representative variable. The COPY statement (called a single-use \nCOPY) is then redundant and can be discarded from the DFG. When statements are discarded, any edges to \nthose statements must be updated to point to the successors of the discarded statement. If node n is \ndiscarded from DFG = .N, E.then the result is a new DFG = .N. , E..where: N. = N \\{n} E. = E \\{k .n}.{k \n. p |{k .n, n . p}.E} In Figure 3, y1 . z1 and y2 . z2. We can replace all occurrences of z1 with y1, \nreplace all occurrences of z2 with y2, and eliminate the nodes for [z1 = y1] and [z2 = y2]. The def-use \nedge from [y1 = ALLOCc] to [z1 = y1] is removed, and a new def-use edge is added from [y1 = ALLOCc] to \n[STORE x1 y1]. Similarly, the def-use edge from [y2 = ALLOCd ] to [z2 = y2] is removed, and a new def\u00aduse \nedge is added from [y2 = ALLOCd ] to [STORE x1 y2]. Figure 4 shows the optimized version of Figure 3. \nTheorem 3 (Correctness of the Transformation). The top-level pointer equivalence transformation preserves \nSSA form for top\u00adlevel variables. Proof. There are two characteristics of SSA form that the transfor\u00admation \nmust preserve: Every variable is de.ned exactly once. Let V beaset of pointer-equivalent variables found \nby the transformation and let S be the set of statements that de.ne these variables. S contains ex\u00adactly \none statement that is not a single-use COPY. S must contain at least one such statement because otherwise \nS forms a cycle in the def-use graph such that a variable is used before it is de.ned, which would violate \nSSA form. S cannot contain more than one suchstatementbecauseonlysingle-use COPYsareconsideredwhen Figure \n4. Figure 3 optimized using top-level pointer equivalence.  .nding equivalent variables. After the equivalent \nvariables are re\u00adplaced by their set representative, all of the single-use COPYsin S are deleted, leaving \nexactly one statement that de.nes the represen\u00adtative variable. Every de.nition dominates all of its \nuses. Every single-use COPY in S is dominated by a statement in S if a statement x = y . S,then x,y . \nV and by de.nition S must also contain the statement de.ning y. There is exactly one statement in S that \nis not a single-use COPY; therefore that statement must dominate all other statements in S. When the \nsingle-use COPYs are deleted, all of the edges pointing to those statements are updated as described \nabove therefore the remaining statement in S must dominate all statements in the program that used a \nvariable in V .  5.3.2 Local Points-to Graph Equivalence Local Points-to Graph Equivalence allows nodes \nin the DFG that are guaranteed to have identical points-to graphs to share a single graph rather than \nmaintain separate copies. This sharing can sig\u00adni.cantly reduce the memory consumption of the pointer \nanalysis, as well as reduce the number of times pointer information must be propagated among nodes. To \nidentify nodes with identical points-to graphs, we de.ne the notion of non-preserving nodes. The points-to \ngraphs that are local to nodes in the DFG (i.e., INk and OUTk) only contain pointer information for address-taken \nvariables. By the nature of partial SSA form, only STORE instructions and CALL instructions (which re.ect \nthe changes caused by STORE instructions in the callee function) can modify the address-taken pointer \ninformation; we call these nodes non-preserving. Other instructions may use this information (e.g., LOAD \nand RET instructions), but they propagate the pointer information through the DFG unchanged; we call \nthese nodes preserving. We say that non-preserving node p reaches node q (p . q) if there is a path in \nthe DFG from p to q, using only SEG edges, that does not contain a non-preserving node. There may be \na number of nodes in the DFG that are all reachable from the same set of non-preserving nodes; Theorem \n4 says that these nodes are guaranteed to have identical points-to graphs. Theorem 4 (Local points-to \ngraph equivalence). Let Nnp . Nbe the set of non-preserving DFG nodes. . p . Nnp and q,r . N : (p . r \n. p . q) . q and r have identical points-to graphs. Proof. Assume . q,r . N.(. p . Nnp : p . q . p . \nr),and that q and r do not have identical points-to graphs. Then one of the nodes (assume it is q) must \nhave received pointer information that the other did not. However, by construction of the partial SSA \nform, non-preserving nodes are the only places that can generate new pointer information for address-taken \nvariables (the only kind of variable present in the local points-to graphs). Therefore . p . Nnp.(p . \nq .\u00ac (p . r)). But this violates our initial assumption that both p and q are reachable from the same \nset of non-preserving nodes. Therefore, p and q must have identical points-to graphs. A simple algorithm \n(see Algorithm 11) can detect nodes that can share their points-to graphs. For each STORE and CALL node \nin the DFG, the algorithm labels all nodes that are reachable via a sequence of SEG edges without going \nthrough another STORE or CALL node with a label unique to the originating node. Since nodes may be reached \nby more than one STORE or CALL node, each node will end up with a set of labels. This process takes O(n3) \ntime, where n is the number of nodes in the SEG portion of the DFG. These labels represent the propagation \nof the unknown pointer information computed by the originating node. All nodes with an identical set \nof labels are guaranteed to have identical local points\u00adto graphs and can therefore share a single graph \namong them. Algorithm 11 Detecting nodes with equivalent points-to graphs. Require: DFG = . N,E. Require: \n. n . N : idn is a unique identi.er Require: Worklist = N while Worklist is not empty do n =SELECT(Worklist) \nSEG for all { k . N | k --. n . E} do if typeof (k) .{ STORE, CALL} then labeln .. { idk}   else labeln \n.. labelk if labeln changed then SEG for all { p . N | n --. p . E} do Worklist .. { p} By potentially \nsacri.cing a small amount of precision we can greatly increase the effectiveness of this optimization. \nCALL nodes turn out to be a large percentage of the total number of nodes in the DFG. By assuming that \ncallees do not modify address-taken pointer information accessible by their callers, thereby allowing \nAl\u00adgorithm11totreat CALL nodes exactly the same as all other non-STORE nodes, we can signi.cantly increase \nthe amount of sharing between nodes. This assumption is sound the optimization only causes nodes to share \npoints-to graphs, so if a callee does modify address-taken pointer information, the pointer information \nis prop\u00adagated to additional nodes that it otherwise wouldn t have reached. The effect of this assumption \non precision and performance is ex\u00adplored in Section 7.  6. Symbolic Analysis This section brie.y discusses \nthe pros and cons of using Binary De\u00adcision Diagrams (BDDs) for .ow-sensitive pointer analysis. BDDs \nare data structures for compactly representing sets and relations [6]. BDDs have several advantages \nover other data structures for this purpose: (1) the size of a BDD is only loosely correlated with the \nnumber of elements in the set that the BDD represents, meaning that large sets can be stored in very \nlittle space, and (2) the com\u00adplexity of set operations involving BDDs depends only on the sizes of the \nBDDs involved, not on the number of elements in the sets. Symbolic analysis takes advantage of these \ncharacteristics to per\u00adform analyses that would be prohibitively expensive both in time and memory using \nmore conventional data structures. There are a number of examples of symbolic pointer analyses in the \nlitera\u00adture [4, 48, 50, 51, 52]. These analyses are fully symbolic: all rel\u00adevant information is stored \nas either a set or relation using BDDs, and the analysis is completely expressed in terms of operations \non those BDDs. When applied speci.cally to .ow-sensitive pointer analysis [51], the relevant information \nis the control-.ow graph and the points-to relations; these are stored in BDDs and the trans\u00adfer functions \nfor the CFG nodes are expressed as BDD operations. Thus, the analysis essentially compute the transfer \nfunctions for all nodes in the CFG simultaneously, making the analysis very ef.\u00adcient. The strength of \nsymbolic analysis lies in its ability to quickly perform operations on entire sets. Its weakness is that \nit is not well-suited for operating on individual members of a set inde\u00adpendently from each other. This \nweakness directly impacts .ow\u00adsensitive pointer analysis. The KILL sets for indirect assignments, such \nas *x=y, cannot be ef.ciently computed on-the-.y because their contents depend not only on the pointer \ninformation computed during the analysis itself but also on the individual characteristics of the points-to \nset elements at the node in question, e.g., whether a particular element represents a single memory location \nor mul\u00adtiple memory locations (as would be true for a variable summa\u00adrizing the heap). Therefore a fully \nsymbolic .ow-sensitive pointer analysis must either process each indirect assignment separately, at prohibitive \ncost, or conservatively set all KILL sets for indirect assignments to the empty set, sacri.cing precision. \nWe propose an alternative to a fully symbolic analysis, which is to encode only a subset of the problem \nusing BDDs. For pointer analysis the most useful subset to encode is the set of points-to re\u00adlations, \nwhich is responsible for the vast majority of both memory consumption and set operations in the analysis. \nBy isolating the pointer information representation into its own source code mod\u00adule, we can easily substitute \na BDD-based implementation while leaving the rest of the analysis completely unchanged, including the \non-the-.y computation of KILL sets. In our experimental evalu\u00adation we study the effects of using BDDs \nto represent pointer infor\u00admation for both the baseline analysis (based on Hind and Pioli [28]) and our \nnew semi-sparse analysis. 7. Experimental Evaluation To evaluate our new techniques, we implement three \n.ow-sensitive pointer analysis algorithms: a baseline analysis based on Hind and Pioli [28] (IFS); semi-sparse \n.ow-sensitive analysis (SS); and the semi-sparse analysis augmented with our two new optimizations, top-level \npointer equivalence and local points-to graph equivalence (SSO). All the algorithms are .eld-sensitive \n(i.e., they treat each .eld of a struct as a separate variable) and for each algorithm we evaluate two \nversions, one that implements pointer information using sparse bitmaps and a second that uses BDDs. The \nbitmap versions of IFS, SS,and SSO .lter pointer informa\u00adtion at call-sites as described by Hind and \nPioli (see Section 3 and Section 5.2). The BDD versions of these algorithms do not use .l\u00adtering. The \ngoal of .ltering is to reduce the amount of pointer in\u00adformation propagated between callers and callees \nin order to speed up convergence and reduce the sizes of the points-to graphs. As mentioned earlier, \nwith the use of BDDs we don t need to worry about the sizes of the points-to graphs, and in fact for \nthe BDD versions the overhead involved in .ltering the pointer information overwhelms any potential bene.t. \nThe algorithms are implemented in the LLVM compiler infras\u00adtructure [32], and the BDDs use the BuDDy \nBDD library [35]. The algorithms are written in C++ and handle all aspects of the C lan\u00adguage except \nfor varargs. The source code for the various algo\u00adrithms is freely available at the authors website. \nThe benchmarks for our experiments are described in Table 2. Six of the benchmarks are taken from SPECINT \n2000 (the largest six applications from that suite) and six from various open-source applications. Function \ncalls to external code are summarized using hand-crafted function stubs. The experiments are run on a \n1.83 GHz processor with 2 GB of memory, using the Ubuntu 7.04 Linux distribution. 7.1 Performance Results \nTable 3 gives the analysis time and memory consumption of the various algorithms. These numbers include \nthe time to build the data structures, apply the optimizations, and compute the pointer analysis. For \nthe bitmap versions of these algorithms, memory is the limiting factor. IFS only scales to 20.5K LOC \nbefore running out of memory, SS scales to 67.2K LOC, and SSO scales to 252.6K LOC. For the two benchmarks \nthat IFS manages to complete, SS is 75\u00d7 faster and uses 26\u00d7 less memory, while SSO is 183\u00d7 faster and \nuses 47\u00d7 less memory. For the four benchmarks that SS completes, SSO is 2.5\u00d7 faster and uses 6.8\u00d7 less \nmemory. For the BDD versions of these algorithms, memory is not an issue and all three algorithms scale \nto 323.5K LOC. However, the two largest benchmarks (gdb and ghostscript) do not complete within our arbitrary \ntime limit of eight hours. For the ten bench\u00admarks that they do complete, SS is 44.8\u00d7 faster than IFS \nand uses 1.4\u00d7 less memory, while SSO is 114\u00d7 faster and uses 1.4\u00d7 less memory. Comparing the fastest \nalgorithm in our study (SSO using BDDs) with our baseline algorithm (IFS using bitmaps) using the two \nbenchmarks that IFS manages to complete, we have sped up .ow-sensitive analysis 197\u00d7 while using 4.6\u00d7 \nless memory. Figures 5 and 6 describe various analysis statistics to explain the relative performance \nof these algorithms. Figure 5 gives the percentage of points-to graphs that SS and SSO have compared \nto IFS (i.e., the number of points-to graphs maintained at each node summed over all the nodes). Figure \n6 gives the percentage of instructions that are processed by SS and SSO compared to IFS (i.e., the total \nnumber of nodes popped off of the statement worklists in Algorithm 1). For IFS the pointer-related instructions \nhave been grouped into basic blocks to reduce the number of points-to graphs that need to be maintained. \nThis grouping is not possible for SS and SSO because they have def-use chains between individual instructions. \nHowever, averaged over all the benchmarks, SS still has 24.6% fewer points-to graphs than IFS because \nonly nodes in the SEG portion of the data.ow graph require points-to graphs. Also recall that the points-to \ngraphs for SS and SSO only have to hold pointer information for address-taken variables, so they are \nmuch smaller than the points-to graphs for IFS. SSO reduces the number of points\u00adto graphs by another \n66.6% over SS using local points-to graph equivalence. The use of top-level def-use chains for semi-sparse \nanalysis pays off: averaged over all the benchmarks, SS processes 62.9% fewer instructions than IFS. \nSSO further reduces the number of instructions processed by 13.7% over SS. Name Description LOC Statements \nFunctions Call Sites 197.parser ex-050325 300.twolf 255.vortex sendmail-8.11.6 254.gap 253.perlbmk vim-7.1 \nnethack-3.4.3 176.gcc gdb-6.7.1 ghostscript-8.15 parser text processor place and route simulator object-oriented \ndatabase email server group theory interpreter PERL language text processor text-based game C language \ncompiler debugger postscript viewer 11.4K 34.4K 20.5K 67.2K 88.0K 71.4K 85.5K 323.5K 252.6K 226.5K 474.1K \n429.0K 33.6K 37.0K 45.0K 69.2K 69.3K 132.2K 184.6K 316.4K 356.3K 376.2K 484.3K 494.0K 99 325 107 271 \n273 725 726 1,935 1,385 1,159 3,801 4,815 774 2,519 331 4,420 3,203 6,002 8,597 15,962 23,001 19,964 \n37,119 18,050 Table 2. Benchmarks: lines of code (LOC) is obtained by running wc on the source. Statements \nreports the number of statements in the LLVM IR. The benchmarks are ordered by number of statements. \nbitmap BDD Name IFS SS SSO IFS SS SSO time mem time mem time mem time mem time mem time mem 197.parser \n80.25 888 1.28 53 0.52 15 7.24 142 0.64 142 0.48 142 ex-050325  OOM 15.74 198 7.33 39 7.95 142 0.66 \n143 0.46 142 300.twolf 72.28 415 0.82 32 0.34 12 6.41 143 0.46 144 0.32 143 255.vortex  OOM 33.37 1,275 \n11.70 81 14.39 150 0.97 151 0.78 150 sendmail-8.11.6  OOM  OOM 86.38 258 38.51 150 2.16 154 1.40 152 \n254.gap  OOM  OOM 191.72 518 68.66 167 2.50 168 2.34 166 253.perlbmk  OOM  OOM  OOM 1,477.05 280 \n50.22 182 21.25 177 vim-7.1  OOM  OOM  OOM 4,759.37 535 573.28 300 112.16 263 nethack-3.4.3  OOM \n OOM 4,762.07 1,648 3,435.48 423 13.68 225 5.37 220 176.gcc  OOM  OOM  OOM 2,445.27 595 39.71 234 \n9.37 226 gdb-6.7.1  OOM  OOM  OOM OOT  OOT  OOT ghostscript-8.15  OOM  OOM  OOM OOT  OOT  OOT \n Table 3. Performance: time (in seconds) and memory consumption (in megabytes) of the various analyses. \nResults under the bitmap columns are obtained using pointer information implemented using sparse bitmaps; \nthose under the BDD columns are obtained using pointer information implemented using BDDs. OOM means \nthe benchmark ran out of memory; OOT means it ran out of time (exceeded an eight hour time limit). 50100 \n % of Points-to Graphs Relative to Baseline 80 % of Baseline Instructions Processed SS 40 SSO 30 60 \n40 20 20 10 0 0 Figure 5. Number of points-to graphs maintained by SS and SSO Figure 6. Number of instructions \nprocessed by SS and SSO com\u00ad compared to IFS. Lower is better (fewer points-to graphs). pared to IFS. \nLower is better (fewer instructions processed).  7.2 Performance Discussion Semi-sparse analysis delivers \non its promise. Based on the num\u00adber of instructions processed and the reported ef.ciency, semi\u00adsparse \nanalysis signi.cantly speeds up convergence. When using bitmaps, the global top-level points-to graph \nsigni.cantly reduces memory consumption as well, especially when coupled with the top-level pointer equivalence \nand local points-to graph equivalence optimizations. However, there are some results which may be a bit \nsurprising; we highlight these results and explain them in this sec\u00adtion. First, note the memory requirements \nfor the BDD analyses as compared to the sparse bitmap analyses. We see for the smaller benchmarks that \nthe BDDs actually require more memory than the bitmaps, even though the premise behind BDDs is that they \nare more memory ef.cient. This discrepancy arises because of the implementation of the BuDDy library \nan initial pool of memory is allocated before the analysis begins, then expanded as necessary. As we \nlook at the larger benchmarks we see that the memory requirements for the BDD analyses rise much more \nslowly than that for the bitmaps, bearing out our initial premise. Second, the bitmap version of SSO \ncompletes for nethack-3.4.3, but runs out of memory for two benchmarks with fewer statements (253.perlbmk \nand vim-7.1). This showcases the dif.culty of pre\u00addicting analysis performance based solely on the input \nsize the actual performance of the analysis also depends on factors that are impossible to predict before \nthe analysis is complete, such as the points-to set sizes of the variables and how widely the pointer \nin\u00adformation is dispersed via indirect calls. Third, the time required for the SS and SSO BDD analyses \nto analyze 253.perlbmk, vim-7.1, gdb-6.7.1, and ghostscript-8.15 seem disproportionately long considering \nthe analysis times for the other benchmarks. There is one minor and one major reason for this anomaly. \nThe minor reason is speci.c to 253.perlbmk the .eld-sensitive solution has an average points-to set size \nover twice that of the .eld-insensitive solution. This result seems counter\u00adintuitive, since .eld-sensitivity \nshould add precision and hence reduce points-to set size. However, to account for the individual .elds \nof the structs, .eld-sensitive analysis increases the number of address-taken variables, in some cases \n(such as 253.perlbmk) making the points-to set sizes larger than for a .eld-insensitive analysis, even \nthough the analysis results are, in fact, more precise. With the exception of 253.perlbmk, all the other \nbenchmarks do have smaller points-to set sizes for the .eld-sensitive analysis. For the remaining three \nbenchmarks with disproportionately large analysis times (vim-7.1, gdb-6.7.1, and ghostscript-8.15), the \nmajor reason for the anomaly is the BDDs themselves. To con.rm this .nding, we measure the average processing \ntime per node for each of the benchmarks and .nd that these three benchmarks have a much higher time \nper node than the others. The main cost of pro\u00adcessing a node is the manipulation of pointer information, \nwhich points out a weakness of BDDs their performance is directly re\u00adlated to how well they compact the \ninformation that they are stor\u00ading, and it is impossible to determine apriori how well the BDDs will \ndo so. The performance of the pointer analysis can vary dra\u00admatically depending on this one factor. There \nare BDD optimiza\u00adtions that we have not yet explored, and these may improve per\u00adformance; these include \nthe re-arrangement of the BDD variable ordering, the use of don t care values in the BDD, and other formu\u00adlations \nof BDDs such as Zero-Suppressed BDDs (ZBDDs). Vari\u00adous other BDD-based pointer analyses have bene.tted \nfrom one or more of these optimizations [34, 48] While for now the BDD versions have superior performance, \nthere is still much that can be done to improve the bitmap ver\u00adsions. Memory is the critical factor, \nand most of the memory con\u00adsumption comes from the local points-to graphs. Even after apply- Time Memory \n6 4 2 0 Normalized Time/Memory Figure 7. Analysis time and memory usage (normalized to our baseline) \nfor the bitmap version of SSO without the assumption on CALLsversus SSO with the assumption i.e., SSOwithout \n/SSOwith. ing the local points-to graph equivalence optimization, a signi.\u00adcant number of the remaining \nlocal points-to graphs contain identi\u00adcal information further efforts to identify and collapse these \nlocal graphs ahead of time could have a dramatic impact on memory con\u00adsumption. For example, there are \nseveral possible schemes for dy\u00adnamically identifying and sharing identical bitmaps across multiple points-to \ngraphs. In addition, by combining semi-sparse analysis with dynamically computed static single assignment \nform [8, 47] we could greatly reduce the sizes of the local points-to graphs. We can decrease the cost \nof evaluating the transfer functions us\u00ading techniques such as the incremental evaluation of transfer \nfunc\u00adtions [19]. We believe that there is still signi.cant room for im\u00adprovement in the bitmap version \nof the SSO algorithm, which we plan to explore in future work.  7.3 SSO Precision The version of SSO \nused in these experiments makes use of the assumption discussed at the end of Section 5.3.2, i.e., that \ncallee functions do not modify address-taken pointer information accessi\u00adble by their callers. This assumption \nincreases the effectiveness of the optimizations (see Figures 7 and 8 for a comparison), but po\u00adtentially \nsacri.ces some precision. To test how much precision is lost we compute the thru-deref metric for SSO \nboth with and with\u00adout this assumption. The thru-deref metric examines each LOAD and STORE in the program \nand averages the points-to set sizes of the dereferenced variables, weighted by the number of times each \nvariable is dereferenced the larger the value, the less precise the pointer analysis. We .nd that our \nbenchmarks do not suffer a signi.cant precision loss by making this assumption; on average the thru-deref \nmetric increased by 0.1%, with a maximum increase of 0.2%.  8. Conclusion Flow-sensitive pointer analysis \nis an important enabling technology for program analysis. We have identi.ed the major challenges that \nstand in the way of scalable .ow-sensitive pointer analysis, and we have directly addressed these challenges \nwith our new semi-sparse analysis, thereby signi.cantly improving on the previous state of 5 4 3 2 1 \n0 Time Memory Normalized Time/Memory Figure 8. Analysis time and memory usage (normalized to our baseline) \nfor the BDD version of SSO without the assumption on CALLsversus SSO with the assumption i.e., SSOwithout \n/SSOwith. the art. We have also described how BDDs can be effectively used for a fully .ow-sensitive \npointer analysis without sacri.cing pre\u00adcision. Our techniques are 197\u00d7 faster and use 4.6\u00d7 less memory \nthan traditional .ow-sensitive pointer analysis. In the future we plan on further optimizing the analysis, \nim\u00adplementing a number of precision-enhancing features, and building various client analyses (such as \nsecurity and error-checking appli\u00adcations) to showcase the usefulness of our techniques. We believe that \n.ow-sensitive pointer analysis has an important position in the realm of program analysis and that our \nwork has made it possible for clients that use .ow-sensitive pointer information to scale to applications \nwith hundreds of thousands of lines of code.  Acknowledgments We thank Kathryn McKinley, Sam Guyer, \nMichael Hind, and the anonymous reviewers for their helpful comments on early versions of this paper. \nThis research was supported by Air Force Research Laboratory contract FA8750-07-C-0035 from the Disruptive \nTech\u00adnology Of.ce and from a grant from the Intel Research Council. References [1] J. Aycock and R. \nN. Horspool. Simple generation of static single\u00adassignment form. In 9th International Conference on Compiler \nConstruction (CC), pages 110 124, London, UK, 2000. Springer-Verlag. [2] T. Ball, R. Majumdar, T. D. \nMillstein, and S. K. Rajamani. Automatic predicate abstraction of c programs. In Programming Language \nDesign and Implementation (PLDI), pages 203 213, 2001. [3] R. Barua, W. Lee, S. Amarasinghe, and A. Agarawal. \nCompiler support for scalable and ef.cient memory systems. IEEE Trans. Comput., 50(11):1234 1247, 2001. \n[4] M. Berndl, O. Lhotak, F. Qian, L. Hendren, and N. Umanee. Points\u00adto analysis using BDDs. In Programming \nLanguage Design and Implementation (PLDI), 2003, pages 103 114. [5] G. Bilardi and K. Pingali. Algorithms \nfor computing the static single assignment form. Journal of the ACM, 50(3):375 425, 2003. [6] R. E. Bryant. \nGraph-based algorithms for Boolean function manipulation. IEEETC, C-35(8):677 691, Aug 1986. [7] W. Chang, \nB. Streiff, and C. Lin. Ef.cient and extensible security enforcement using dynamic data .ow analysis. \nIn Computer and Communications Security (CCS), 2008, pages 39 50. [8] D. R. Chase, M. Wegman, and F. \nK. Zadeck. Analysis of pointers and structures. In Programming Language Design and Implementation (PLDI), \npages 296 310, 1990. [9] P.-S. Chen, M.-Y. Hung, Y.-S. Hwang, R. D.-C. Ju, and J. K. Lee. Compiler support \nfor speculative multithreading architecture with probabilistic points-to analysis. SIGPLAN Not., 38(10):25 \n36, 2003. [10] B.-C. Cheng and W.-M. W. Hwu. Modular interprocedural pointer analysis using access paths: \nDesign, implementation, and evaluation. ACM SIG-PLAN Notices, 35(5):57 69, 2000. [11] J.-D. Choi, R. \nCytron, and J. Ferrante. Automatic construction of sparse data .ow evaluation graphs. In Symposium on \nPrinciples of Programming Languages (POPL), pages 55 66, New York, NY, USA, 1991. ACM Press. [12] F. \nChow, S. Chan, S.-M. Liu, R. Lo, and M. Streich. Effective representation of aliases and indirect memory \noperations in SSA form. In Compiler Construction, 1996, pages 253 267. [13] R. Cytron, J. Ferrante, B. \nK. Rosen, M. N. Wegman, and F. K. Zadeck. Ef.ciently computing static single assignment form and the \ncontrol dependence graph. ACM Transactions on Programming Languages and Systems, 13(4):451 490, 1991. \n[14] R. Cytron and R. Gershbein. Ef.cient accommodation of may-alias information in SSA form. In Programming \nLanguage Design and Implementation (PLDI), June 1993, pages 36 45. [15] R. K. Cytron and J. Ferrante. \nEf.ciently computing F-nodes on-the\u00ad.y. ACM Trans. Program. Lang. Syst, 17(3):487 506, 1995. [16] E. \nDuesterwald, R. Gupta, and M. L. Soffa. Reducing the cost of data .ow analysis by congruence partitioning. \nIn Compiler Construction, 1994, pages 357 373. [17] S. Fink, E. Yahav, N. Dor, G. Ramalingam, and E. \nGeay. Effective typestate veri.cation in the presence of aliasing. In International Symposium on Software \nTesting and Analysis, pages 133 144, 2006. [18] R. Ghiya. Putting pointer analysis to work. In Principles \nof Programming Languages (POPL), 1998, pages 121 133. [19] D. Goyal. An improved intra-procedural may-alias \nanalysis algorithm. Technical report TR1999-777, New York University, 1999. [20] S. Z. Guyer and C. \nLin. Error checking with client-driven pointer analysis. Science of Computer Programming, 58(1-2):83 \n114, 2005. [21] B. Hackett and R. Rugina. Region-based shape analysis with tracked locations. In Symposium \non Principles of Programming Languages, pages 310 323, 2005. [22] B. Hardekopf and C. Lin. The Ant and \nthe Grasshopper: Fast and accurate pointer analysis for millions of lines of code. In Programming Language \nDesign and Implementation (PLDI), pages 290 299, San Diego, CA, USA, 2007. [23] B. Hardekopf and C. Lin. \nExploiting pointer and location equivalence to optimize pointer analysis. In International Static Analysis \nSymposium (SAS), pages 265 280, 2007. [24] R. Hasti and S. Horwitz. Using static single assignment form \nto improve .ow-insensitive pointer analysis. In Programming Language Design and Implementation (PLDI), \n1998, pages 97 105. [25] N. Heintze and O. Tardieu. Ultra-fast aliasing analysis using CLA: A million \nlines of C code in a second. In Programming Language Design and Implementation (PLDI), pages 23 34, 2001. \n[26] M. Hind. Pointer analysis: haven t we solved this problem yet? In Workshop on Program Analysis for \nSoftware Tools and Engineering (PASTE), pages 54 61, New York, NY, USA, 2001. ACM Press. [27] M. Hind, \nM. Burke, P. Carini, and J.-D. Choi. Interprocedural pointer alias analysis. ACM Transactions on Programming \nLanguages and Systems, 21(4):848 894, 1999. [28] M. Hind and A. Pioli. Assessing the effects of .ow-sensitivity \non pointer alias analyses. In Static Analysis Symposium, pages 57 81, 1998. [29] V. Kahlon. Bootstrapping: \na technique for scalable .ow and context\u00adsensitive pointer alias analysis. In Programming language design \nand implementation, pages 249 259, 2008. [30] H.-S. Kim, E. M. Nystrom, R. D. Barnes, and W.-M. W. Hwu. \nCompaction algorithm for precise modular context-sensitive points\u00adto analysis. Technical report IMPACT-03-03, \nCenter for Reliable and High Performance Computing, University of Illinois, Urbana-Champaign, 2003. [31] \nC. Lapkowski and L. J. Hendren. Extended SSA numbering: introducing SSA properties to languages with \nmulti-level pointers. In CASCON 96: Proceedings of the 1996 conference of the Centre for Advanced Studies \non Collaborative research, page 23, 1996. [32] C. Lattner. LLVM: An infrastructure for multi-stage optimization. \nMaster s thesis, Computer Science Dept., University of Illinois at Urbana-Champaign, Dec 2002. [33] C. \nLattner and V. Adve. Data structure analysis: An ef.cient context\u00adsensitive heap analysis. Technical \nReport UIUCDCS-R-2003-2340, Computer Science Dept, University of Illinois at Urbana-Champaign, 2003. \n[34] O. Lhotak, S. Curial, and J. Amaral. Using ZBDDs in points-to analysis. In Workshops on Languages \nand Compilers for Parallel Computing (LCPC), 2007. [35] J. Lind-Nielson. BuDDy, a binary decision package. \n[36] A. Milanova and B. G. Ryder. Annotated inclusion constraints for precise .ow analysis. In ICSM 05: \nProceedings of the 21st IEEE International Conference on Software Maintenance (ICSM 05), pages 187 196, \n2005. [37] M. Mock, D. Atkinson, C. Chambers, and S. Eggers. Improving program slicing with dynamic points-to \ndata. In Foundations of Software Engineering, pages 71 80, 2002. [38] D. Novillo. Design and implementation \nof Tree SSA, 2004. [39] E. M. Nystrom, H.-S. Kim, and W. mei W. Hwu. Bottom-up and top-down context-sensitive \nsummary-based pointer analysis. In International Symposium on Static Analysis, pages 165 180, 2004. [40] \nD. Pearce, P. Kelly, and C. Hankin. Ef.cient .eld-sensitive pointer analysis for C. In ACM Workshop on \nProgram Analysis for Software Tools and Engineering (PASTE), pages 37 42, 2004. [41] D. J. Pearce, P. \nH. J. Kelly, and C. Hankin. Online cycle detection and difference propagation for pointer analysis. In \n3rd International IEEE Workshop on Source Code Analysis and Manipulation (SCAM), pages 3 12, 2003. [42] \nG. Ramalingam. On sparse evaluation representations. Theoretical Computer Science, 277(1-2):119 147, \n2002. [43] J. H. Reif and H. R. Lewis. Symbolic evaluation and the global value graph. In Principles \nof programming languages (POPL), pages 104 118, 1977. [44] A. Rountev and S. Chandra. Off-line variable \nsubstitution for scaling points-to analysis. ACM SIGPLAN Notices, 35(5):47 56, 2000. [45] A. Salcianu \nand M. Rinard. Pointer and escape analysis for multithreaded programs. In PPoPP 01: Proceedings of the \nEighth ACM SIGPLAN Symposium on Principles and Practices of Parallel Programming, pages 12 23, 2001. \n[46] M. Shapiro and S. Horwitz. The effects of the precision of pointer analysis. Lecture Notes in Computer \nScience, 1302:16 34, 1997. [47] T. B. Tok, S. Z. Guyer, and C. Lin. Ef.cient .ow-sensitive interprocedural \ndata-.ow analysis in the presence of pointers. In 15th International Conference on Compiler Construction \n(CC), pages 17 31, 2006. [48] J. Whaley and M. S. Lam. Cloning-based context-sensitive pointer alias \nanalysis. In Programming Language Design and Implementation (PLDI), pages 131 144, 2004. [49] R. P. Wilson \nand M. S. Lam. Ef.cient context-sensitive pointer analysis for C programs. In Programming Language Design \nand Implementation (PLDI), pages 1 12, 1995. [50] J. Zhu. Symbolic pointer analysis. In International \nConference on Computer-Aided Design (ICCAD), pages 150 157, New York, NY, USA, 2002. ACM Press. [51] \nJ. Zhu. Towards scalable .ow and context sensitive pointer analysis. In DAC 05: Proceedings of the 42nd \nAnnual Conference on Design Automation, pages 831 836, 2005. [52] J. Zhu and S. Calman. Symbolic pointer \nanalysis revisited. In Programming Language Design and Implementation (PLDI), pages 145 157, New York, \nNY, USA, 2004. ACM Press.  \n\t\t\t", "proc_id": "1480881", "abstract": "<p>Pointer analysis is a prerequisite for many program analyses, and the effectiveness of these analyses depends on the precision of the pointer information they receive. Two major axes of pointer analysis precision are flow-sensitivity and context-sensitivity, and while there has been significant recent progress regarding scalable context-sensitive pointer analysis, relatively little progress has been made in improving the scalability of flow-sensitive pointer analysis.</p> <p>This paper presents a new interprocedural, flow-sensitive pointer analysis algorithm that combines two ideas-semi-sparse analysis and a novel use of BDDs-that arise from a careful understanding of the unique challenges that face flow-sensitive pointer analysis. We evaluate our algorithm on 12 C benchmarks ranging from 11K to 474K lines of code. Our fastest algorithm is on average 197x faster and uses 4.6x less memory than the state of the art, and it can analyze programs that are an order of magnitude larger than the previous state of the art.</p>", "authors": [{"name": "Ben Hardekopf", "author_profile_id": "81331494259", "affiliation": "The University of Texas at Austin, Austin, TX, USA", "person_id": "P1300979", "email_address": "", "orcid_id": ""}, {"name": "Calvin Lin", "author_profile_id": "81451598438", "affiliation": "The University of Texas at Austin, Austin, TX, USA", "person_id": "P1300980", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1480881.1480911", "year": "2009", "article_id": "1480911", "conference": "POPL", "title": "Semi-sparse flow-sensitive pointer analysis", "url": "http://dl.acm.org/citation.cfm?id=1480911"}