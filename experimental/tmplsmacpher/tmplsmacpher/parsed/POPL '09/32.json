{"article_publication_date": "01-21-2009", "fulltext": "\n Focusing on Pattern Matching Neelakantan R. Krishnaswami Carnegie Mellon University neelk@cs.cmu.edu \nAbstract In this paper, we show how pattern matching can be seen to arise from a proof term assignment \nfor the focused sequent calculus. This use of the Curry-Howard correspondence allows us to give a novel \ncoverage checking algorithm, and makes it possible to give a rigorous correctness proof for the classical \npattern compilation strategy of building decision trees via matrices of patterns. Categories and Subject \nDescriptors F.4.1 [Mathematical Logic]: Lambda Calculus and Related Systems General Terms Languages, \nTheory Keywords Focusing, Pattern Matching, Type Theory, Curry-Howard 1. Introduction From the point \nof view of the semanticist, one of the chief at\u00adtractions of functional programming is the close connection \nof the typed lambda calculus to proof theory and logic via the Curry-Howard correspondence. The point \nof view of the workaday pro\u00adgrammer seems, at .rst glance, less exalted one of the most compelling features \nin actual programming languages like ML and Haskell is the ability to analyze structured data with pattern \nmatch\u00ading. But pattern matching, though enormously useful, has histor\u00adically lacked the close tie to \nlogic that the other core features of functional languages possess. The De.nition of Standard ML (Mil\u00adner \net al. 1997), for example, contains a suggestion in English that it would be desirable to check coverage, \nwith no clear account of what this means or how to accomplish it. Our goal is to rectify this discrepancy, \nand show that the se\u00admanticist ought to be just as interested in pattern matching as the programmer. \nWe show that pattern matching has just as strong a logical interpretation as everything else in functional \nprogramming, and that .lling in this part of the Curry-Howard correspondence en\u00adables simple correctness \nproofs of parts of the compiler (such as the coverage checker and the pattern compiler) that required \nconsider\u00adable insight and creativity before. Speci.cally, our contributions are: First, we give a proof \nterm assignment for a focused sequent calculus, which naturally gives rise to pattern matching. Then, \nwe show how to extend this calculus to properly model features of ML-like pattern languages such as as-patterns, \nor-patterns, Permission to make digital or hard copies of all or part of this work for personal or classroom \nuse is granted without fee provided that copies are not made or distributed for pro.t or commercial advantage \nand that copies bear this notice and the full citation on the .rst page. To copy otherwise, to republish, \nto post on servers or to redistribute to lists, requires prior speci.c permission and/or a fee. POPL \n09, January 18 24, 2009, Savannah, Georgia, USA. Copyright c &#38;#169; 2009 ACM 978-1-60558-379-2/09/01. \n. . $5.00 incompleteness, and the left-to-right priority ordering of ML\u00adstyle pattern matching. This \ncalculus also extends easily to encompass features like recursive and existential types.  Second, we \ngive a simple inductive characterization of when a pattern is exhaustive and non-redundant, and prove \nthat this algorithm is sound. This is of interest since it is a coverage test that does not involve examining \nthe output of a pattern compilation algorithm.  Third, we reconstruct the classical matrix-based method \nof compiling patterns into decision trees in terms of our for\u00admalism, and prove its correctness. This \ncorrectness result is stronger than prior results, since it is based directly upon the language s semantics. \n 2. An Introduction to Focusing 2.1 The Focused Sequent Calculus A rule of a logical system is called \ninvertible when the conclusion of the rule is strong enough to imply the premises; that is, when the \nrule can be read bidirectionally. For example, the right rule for implication in the sequent calculus \nis invertible: G,A f B G f A . B  We can give a proof that this rule is invertible by showing that the \nconclusion lets us deduce the premise: . . . G f A . B G,A f A G, A, B f B WEAKEN .L G,A f A . B G, \nA, A . B f B CUT G,A f B Since the conclusion can be deduced from the premises, and the premises follow \nfrom the conclusion, applying an invertible rule cannot change the provability of a sequent the same \ninformation is available in either case. (In contrast, a rule like sum-introduction is not invertible, \nsince the knowledge that A + B holds is less informative than knowing that, say, A holds.) This is problematic \nfor applications involving proof search, such as theorem proving. The fact that we can apply invertible \nrules at any time means that there are many equivalent proofs that differ only in the order that inversion \nsteps are made in, which increases the size of the theorem prover s search space with no bene.t. Andreoli \nintroduced the concept of focusing (Andreoli 1992) as a technique to reduce the nondeterminism in the \nsequent calculus. First, he observed (as had others (Miller et al. 1991)) that applications of invertible \nrules could be done eagerly, since inversion does not change provability. Then, he observed that each \nconnective in linear logic was either invertible on the left and non\u00adinvertible on the right (the positive \ntypes), or invertible on the right and non-invertible on the left (the negative types). Finally, he made \nthe remarkable observation that in a fully inverted sequent (i.e., one in which no further inversions \nare possible), it is possible to select a single hypothesis (on either the left or the right) and then \neagerly try to prove it, without losing completeness. This fact explains the name focusing; one can focus \non a hypothesis without losing completeness. We give a version of the focused sequent calculus for intuitionistic \nlogic below. First, we group the connectives. Types A ::= X | 1 | A \u00d7 B | A . B | 0 | A + B Positives \nP ::= X | 1 | A \u00d7 B | 0 | A + B Negatives N ::= X | A . B The types are unit 1, products A \u00d7 B, void \n0, sums A + B, function spaces A . B , and atomic types X. Sums and products1 are positive, and function \nspace is negative. Atoms are treated as either polarity, as convenient. This system has four judgements. \nFirst, we have a right focus phase G f A, in which we try to prove a positive formula on the right. As \nusual, G is taken to be a list of hypotheses, with the rules presented in such a way as to make the usual \nstructural properties (exchange, weakening and contraction) admissible. G f A G f A G f B G f A 1R \u00d7R \n+R1 G f 1G f A \u00d7 B G f A + B G f B G; \u00b7f N +R2 BLURR G f A + B G f N Since the leaves of a positive proposition \ncan contain negative for\u00admulas, the BLURR rule lets us transition to the two-context right inversion \njudgement G; . f A, in which a negative proposition on the right is inverted. Since the only negative \nconnective is implica\u00adtion, this means moving the left-hand-sides of arrows A . B into the context ., \nwhich can contain arbitrary positive or negative for\u00admulas. The . context is ordered the structural \nrules of exchange, contraction and weakening are not permitted in this context. G; . f A G; ., A f B \n.R G; . I P BLURL G; . f A . B G; . f P Once the right-hand-side is positive once again, we have to \ncon\u00adtinue with inversion on the left before we can resume focus. This brings us to the left-inversion \nphase G; . I P , which inverts all of the positive hypotheses on the left starting from the leftmost \nend of the pattern context, and moves negative hypotheses into G. The ordering of . forces the left-inversion \nrules to be applied from left to right, eliminating the irrelevant choice of which order to apply the \nleft-invertible rules from the calculus. G; . I P G,N;. I P G; . I P HYPL 1L G; N, . I P G; 1, . I P \nG; A, B, . I P \u00d7L 0L G; A \u00d7 B, . I P G; 0, . I P 1 Intutitionistic products can be either negative, with \nprojective elimina\u00adtions, or positive, with a let-binding elimination. This corresponds to the fact that \nintuitionistic products can be seen as corresponding to either one of the linear connectives A&#38;B \nor A . B. We choose the latter, since it has a let-binding elimination letpair (x, y)= e in e' and this \nis the more natural elimination to explain pattern matching. G; A, . I P G; B, . I P G f P +L FOCUSR \nG; A + B, . I P G; \u00b7 I P G I X G I P G; P I Q FOCUSL FOCUSLP G; \u00b7 I X G; \u00b7 I Q Once the pattern context \nis emptied, we can resume focus, either returning to right focus via FOCUSR, or by switching into the \nleft focus phase G I A, in which we focus on a hypothesis in the negative context. G I A A . GG I A . \nB G f A HYP .L G I A G I B Note that this focused sequent calculus is a restriction of the traditional \nsequent calculus: if we collapsed all of the judgements into a single judgement, elided the focus and \nblur rules that move us between judgements, and turned the semicolon separating the G and . contexts \ninto a comma, then each of these rules would be one of the standard rules of the sequent calclus2. So \nit is easily seen that every focused proof has a corresponding proof in the regular sequent calculus. \nThe completeness proof for focusing with respect to the sequent calculus is a little bit harder, and \ncan be found elsewhere (Liang and Miller 2007). 2.2 A Proof Term Assignment The next step on our path \nis to assign proof terms to this calculus. Positive Right e ::= ()| (e, e')| inl e | inr e | u Negative \nRight u ::= .p. u | r Arms (Positive Left) r ::= [] | [r | r'] | e | t | case(t, p . r) Applications \n(Neg. Left) t ::= x | te Patterns p ::= x |()| (p, p')| [] | [p | p'] Ordinary Contexts G ::= \u00b7| G,x \n: N Pattern Contexts . ::= \u00b7| .,p : A G f e : A G f e1 : A G f e2 : B 1R \u00d7R G f () :1 G f(e1,e2) : A \n\u00d7 B G f e : A G f e : B +R1 +R2 G f inl e : A + B G f inr e : A + B G; \u00b7f u : N BLURR G f u : N G; . \nf u : A G; ., p : A f u : B .R G; . I r : P BLURL G; . f .p. u : A . B G; . f r : P 2 In fact, the \n.L rule is the natural deduction elimination rule for implica\u00adtion, rather than a true left rule of the \nsequent calculus. We do this to keep the upcoming proof terms more familiar. G; . I r : A G,x : N;. I \nr : P G; . I r : P HYPL 1L G; x : N, . I r : P G; () :1, . I r : P G; p : A, p ' : B, . I r : P . '\u00b8 \n\u00d7L 0L G; p, p : A \u00d7 B, . I r : P G;[] : 0, . I [] : P '' G; p : A, . I r : P G; p : B, . I r : P +L G; \n[p | p ' ]: A + B, . I [r | r ' ]: P G f e : P G I t : X FOCUSR FOCUSL G; \u00b7 I e : P G; \u00b7 I t : X G I \nt : P G; p : P I r : Q FOCUSLP G; \u00b7 I case(t, p . r): Q G I t : A x : A . GG I t : A . B G f e : A HYP \n.L G I x : A G I te : B These rules exactly mimic the structure of the sequent calculus we just presented. \nThe proof terms for the right focus judgement G f e : A consists of all of the introduction forms for \nthe positive types. For the type 1, we have () ; for products A \u00d7 B, pairs ( e, e ') ; for sums A + B, \nthe injections inl e and inr e; and the empty type 0 has no introduction form. There is an interesting \ninteraction between the syntax and the typing judgement here; while the negative introductions u are \nincluded in the productions of the syntactic class e, the typing rule BLURR only permits us to use these \nat a negative type. (A similar story can be told for all of the other inclusions of r in u, and t in \nr.) Negative introduction forms that is, functions are typed with the judgement G; . f u : A, and the \nproof terms are lambda-terms .p. u. Our lambda terms differ in two respects from the usual presentation \nof functions in the lambda calculus, one minor and one major. The minor difference is that we need no \ntype annotation for the binder this is because focused calculi are inherently bi-directionally typed.3 \nThe major difference is that instead of a variable binder .x. u, we have a pattern as the binder for \na function. This will be common to all the binding forms of our language, and is where most of the novelty \nof this calculus resides. The introduction rule for functions . R moves the pattern from the proof term \ninto the right-hand side of the pattern context .. The reason we say right-hand-side in particular is \nbecause the pattern context is an ordered context we cannot make free use of the rule of exchange. So \na context p : A, p ' : B, . is considered to be different from a context p ' : B, p : A, .. This is unlike \nthe behavior of the ordinary variable context G, in which we identify permutations. Furthermore, instead \nof mapping variables to types, a pattern context assigns types to patterns in the previous example p \nand p ' are schematic variables. The patterns themselves range from the familiar to the unfamil\u00adiar. \nWe have a variable pattern x, unit pattern () , and pair patterns ( p, p ') , which all match the syntax \nof patterns from ML. However, we depart quite radically from ML when we reach the case of sum patterns. \nA pattern for a sum type A + B is a pattern [p | p ' ], which can be read as a pattern p for the left \nbranch, and a pattern p ' for 3 Concretely, the typing judgements form a well-moded logic program. All \nof the judgements take the contexts and proof terms as inputs. Bidirection\u00adality arises from the fact \nthat the type argument is an output of the GIt : A judgement, and an input of all of the others. STLC \nFocused STLC abort(t) case(t, [] . []) case(t, x1. t1, x. t2) case(t, [x1 | x2] . [t1 | t2]) letunit \n() = t in t ' case(t, () . t ' ) letpair (x, y) = t in t ' case(t, (x, y) . t ' ) let x = t in t ' \ncase(t, x . t ' ) Figure 1. Traditional and Focused Eliminations the right branch. This choice deviates \nfrom the ML practice of hav\u00ading separate patterns for each constructor, but it has the bene.t of ensuring \nthat a well-typed pattern will always be complete. After decomposing all of the implications, we have \na pattern context . and an arm r. Now, we come to the reason why the context must be ordered. Patterns \nand arms are separate: patterns live in the pattern context on the left of the turnstile I, and the body \nof the proof term, r, carries the arms. So we must regard . as an ordered context to track which branches \nof a sum-pattern [p | p ' ] correspond with which arms [r | r ' ]. This can be seen in the sum elimination \nrule +L given a pattern hypothesis [p | p ' ]: A + B at the left-most end of the context, and an arm \nbody [r | r ' ], we break it down into two premises, one of which uses the pattern/arm pair p : A/r, \nand the other of which uses p ' : B/r '. If we were free to permute the pattern hypotheses, then we would \nlose this correlation. As an aside, note that functions, as inhabitants of the negative type A . B, are \nonly bound with variable patterns. So the struc\u00adture of the sequent calculus naturally precludes decomposing \nfunc\u00adtions with pattern matching, showing that this is not an ad-hoc re\u00adstriction. Furthermore, each \nvariable in a pattern becomes a distinct addition to the context G, so repeating variables does not require \ntheir equality; it will shadow them instead. So the linearity restric\u00adtion on ML-style patterns can be \nseen as a restriction that forbids patterns that shadow their own bindings. After decomposing all of \nthe pattern hypotheses, we can either apply a function with the FOCUSL rule, or we can apply a function \nand case analyze the result with the FOCUSLP rule. The proof term of the FOCUSLP rule is case analysis, \nand this offers us an opportunity to examine how the pattern elimination compares with the traditional \nelimination constructs for sums and products. In Figure 1, we give a table with the traditional eliminator \non the left, and the focused elimination on the right. Observe that we have an elimination form for the \nunit type this is in analogy to the elimination for the type 1 in linear logic, which is the unit to \nthe tensor A . B. Since the syntax of patterns allows nesting, we can also express case expressions like \nthe following SML expression: case t of (Inl x, Inl u) => t1 | (Inr y, Inl u) => t2 | (Inl x, Inr v) \n=> t3 | (Inr y, Inr v) => t4 in our focused calculus: case(t, ( [x | y], [u | v]). [[t1 | t2] | [t3 \n| t4]]) Instead of writing a series of four disjuncts, we write a single pattern ( [x | y], [u | v]) \n, which is a pair pattern with two sum\u00adpatterns as subexpressions. The ordering in our context means \nthat the nested arm [[t1 | t2] | [t3 | t4]] will be decomposed by the left\u00adhand sum-pattern [x | y], \nyielding either [t1 | t2] or [t3 | t4], and then the result will be decomposed by the right-hand sum-pattern \n[u | v]. In other words, it is also possible to see this pattern calculus as a shorthand for writing \nnested case statements. 2.3 From Sequent Calculus to Programming Language All of the well-typed terms \nof this language are in \u00df-normal, .-long form. This property is enforced by the rules to transition between \nthe judgements, which limit at which types they can be applied. For example, the BLURR rule only allows \ndeparting the right positive judgment at a function types, which means that a tuple containing a function \ncomponent must have it as a lambda-abstraction. This restriction is extremely useful in a type theory \nintended for proof search, since this means that the system is cut-free. However, the Curry-Howard correspondence \ntells us that evaluation of func\u00adtional programs corresponds to the normalization of proofs. There\u00adfore, \nwe must add the Cut rule back into the calculus in order to get an actual programming language. This \nintroduces non-normal proofs into the calculus, whose corresponding lambda terms are non-normal forms \nthat we can use as programs that actually com\u00adpute results. To write .-short terms in our langauge, we \nrelax the restrictions on how long we retain focus and how far we decompose invertible types. We allow \nhypotheses of arbitrary type in G, and relax the BlurR, BlurL, FocusR, FocusL and FocusLP rules so that \nthey ac\u00adcept subterms at any type A, rather than just at positive or negative types. Then, to allow non \n\u00df-normal terms, we add a type-annotated term (e : A) (with rule ANNOT), which permits normal forms to \nappear at the heads of case statements and function applications. We also introduce a notion of value \nfor our language suitable for a call-by-value evaluation strategy, with strict sums and products and \nno evaluation under lambda-abstraction. Negative Elims t ::= x | (e : A) | te Values v ::= ()| (v, v \n' )| inl v | inr v | .p. u G f e : A ANNOT G I (e : A): A Now, we need to explain what substitution means \nfor our pattern context. Intuitively, the pattern associated with each hypothesis should drive how values \nare decomposed, an idea we formalize with the following relations: ' ' R((v/p))A u'. u L((v/p))A r'. \nr '' R((v/p))A u'. u L((v/p))A r'. r '' R((v/p))A .p ' .u'. .p ' .u R((v/p))A r'. r L((v/x))A r'. [(v \n: A)/x]r L((()/()))1 r'. r ' ''' L((v1/p1))A1 r'. r L((v2/p2))A2 r'. r '' L(((v1,v2) / (p1,p2)))A1\u00d7A2 \nr'. r ' L((v/p1))A r1 '. r ' L((inl v/[p1 | p2]))A+B [r1 | r2] '. r ' L((v/p2))B r2 '. r ' L((inr v/[p1 \n| p2]))A+B [r1 | r2] '. r These rules match a value against a pattern, decomposing the value. When we \nreach a pair pattern (p1,p2), we decompose the value (v1,v2) into two pieces, and sequentially match \nv1 against p1, and then do another pattern substitution on the result, matching v2 against p2. When we \nreach a sum-pattern [p1 | p2], we decide to take either the left branch or the right branch depending \non whether the value is inl v or inr v. Finally, when we reach a variable pattern, we substitute the \nvalue for the variable. Since variables are in the category of negative focus terms t, we wrap the value \nin a type annotation (v : A) to ensure that the result is type-correct. This de.nition of pattern substitution \nthat satis.es the following principle: PROPOSITION 1. If \u00b7f v : A, then: If G; p : A, . f u : C, and \nR((v/p))A u'. u ', then G; . f u ' : C.  If G; p : A, . I r : C, and L((v/p))A r'. r ', then G; . I \nr ' :  C. If we have a value v of the appropriate type, and a well-typed term r in a pattern context \nwith p at the leftmost end, then if the pattern substitution relation relates r to r ', we can deduce \nthat r ' is well-typed in the smaller pattern context. The following de.nition of pattern substitution \nsatis.es this principle: As an aside, it is evident that these rules de.ne a relation that is syntax-directed \nand total. So it appears we could have de.ned a pattern substitution function instead of using a more \nsyntactically heavyweight judgement. However, later on we will need to relax this condition, so we present \npattern substitution as a relation from the beginning. Equipped with an understanding of what pattern \nsubstitution is, we give the operational semantics below, with mutually recursive reduction relations \nfor each of the syntactic categories. The only novelties in this semantics are that we use pattern substitution \nin\u00adstead of ordinary substitution when reducing function applications and case statements, and that there \nis an extra rule to discard redun\u00ad dant type annotations. e1 .PR e ' 1 (e1, e2) .PR . e ' 1, e2\u00b8 e2 .PR \ne ' 2 (v1, e2) .PR . v1, e ' 2\u00b8 e .PR e ' e .PR e ' u .NR u ' inl e .PR inl e ' inr e .PR inr e ' u .PR \nu ' r .PL r ' t .NL t ' '' r .NR r case(t, p . r) .PL case(t ,p . r) '' L((v/p))A r'. rt .NL t '' (v \n: A) .PL v case((v : A),p . r) .PL r te .NL te '' e .PR ee .PR e '' (v : A) e .NL (v : A) e (e : A) .NL \n(e : A) ' R((v/p))A u'. u ' (.p. u : A . B) v .NL (u : B) This semantics validates the usual progress \nand preservation theorems. PROPOSITION 2 (Type Soundness). This language is sound. Progress holds: \n1. If \u00b7f e : A, then e .PR e ' or e is a value v. 2. If \u00b7; \u00b7f u : A, then u .NR u ' or u is a value \nv. 3. If \u00b7; \u00b7 I r : A, then r .PL r ' or r is a value v. 4. If \u00b7 I t : A, then t .NL t ' or t is a \nterm (v : A).   Type preservation holds: 1. If \u00b7f e : A and e .PR e ', then \u00b7f e ' : A 2. If \u00b7; \u00b7f \nu : A and u .NR u ', then \u00b7; \u00b7f u ' : A   3. If \u00b7; \u00b7 I r : A and r .PL r ', then \u00b7; \u00b7 I r ' : A 4. \nIf \u00b7 I t : A and t .NL t ', then \u00b7f t ' : A  3. From ML Patterns to Focused Patterns While our language \nis very pleasant theoretically, it is not yet ade\u00adquate to fully explain ML pattern matching. Consider \nan example from SML like: case e of (Inl x, Inl u) => e1 |was (Inr _,Inr_)=>e2 |z =>e3 This small example \nis doing quite a few things. First, it uses wildcard patterns and as-patterns, which have no analog in \nthe pat\u00adtern language we have described so far. Second, this example relies on implicit priority ordering \n we expect the .rst two patterns to be tested before the last. This is what ensures that even though \nthe variable pattern z (in the third clause) matches anything on its own, it will only serve as a catch-all. \nTo explain as-patterns and wildcards, we will extend the lan\u00adguage of patterns with the patterns T (for \nwildcard) and the and\u00adpattern p . p '. We extend the typing rules and pattern substitution relation below. \nG; . I r : B G; p : A, p ' : A, . I r : B TOP AND ' G; T : A, . I r : B G; p . p : A, . I r : B ' ''' \nL((v/p1))A r'. r L((v/p2))A r'. r '' L((v/T))A r'. r L((v/p1 . p2))A r'. r We can introduce a wildcard \nor and-pattern at any type. The typing rule TOP requires that the term be well-typed without the wildcard \nhypothesis, and the rule AND for the and-pattern p . p ' requires that we be well-typed in a context \nwith both p and p ' pattern hypotheses. If we erase the proof terms from the rules, we see that the rules \nfor these two patterns have a very clear logical interpretation: the rule TOP rule is the rule of weakening, \nand the AND rule is the rule of contraction. The pattern substitution for the T pattern simply throws \naway the value and returns the arm unchanged. The and-pattern p1 . p2 matches v against p1, and then \nmatches it a second time against p2. However, we are no closer to the goal of being able to account for \nthe priority ordering of conventional pattern matching. Looking once more at the example at the start \nof this section, it is clear that when we interpret the third clause the pattern Z we must also have \nsome way of saying match Z, but not if it matches the .rst or second clauses . If we had some sort of \nnegation operation on patterns, we could express this constraint, if we interpreted the ML pattern Z \nas the focused pattern z .\u00ac[ (Inl x, Inl u)] . \u00ac[ (yas(Inr ,Inr ))] . Here, we use negation to indicate \nan as-yet-unde.ned pattern negation, and the semantic brackets to indicate an as-yet-unde.ned translation \nof ML patterns into focused form. The idea is that we want a pattern that is z and not the .rst clause, \nand not the second clause. To make this possible, we start with the wildcard T and and\u00adpattern p . p \n' patterns, and add their duals to the language of patterns. That is, we add patterns . and p . p ' to \nour pattern language. ' Patterns p ::= ... |.| p . p ' Arms r ::= ... |.| r . r BOT G; . : A, . I . : \nB '' G; p : A, . I r : B G; p : A, . I r : B OR '' G; p . p : A, . I r . r : B '' L((v/p1))A r1 '. r \nL((v/p2))A r2 '. r '' L((v/p1 . p2))A r1 . r2 '. r L((v/p1 . p2))A r1 . r2 '. r The intended semantics \nof the false-pattern . is guaranteed fail\u00adure just as T matches successfully against anything, . will \nsuccessfully match nothing. This semantics is given by not giv\u00ading a rule for . in the pattern substitution \njudgement, which en\u00adsures there is no way for a false pattern to match. Likewise, when we match a value \nagainst the or-pattern p . p ', we will nondeter\u00administically choose one or the other pattern to match \nagainst. To implement this semantics, we give two rules in the pattern substi\u00adtution judgement for or-patterns, \none corresponding to taking the left branch and one for the right. So pattern substitution may now be \nunde.ned (if a false pattern appears), or it may give multiple results (if an or-pattern appears), revealing \nwhy we de.ned it in relational style, as a judgement. The OR typing rule says that p . p ' typechecks \nif the left-hand pattern p typechecks against the left-hand arm r, and the right\u00adhand pattern p ' typechecks \nagainst the right-hand arm r '. Despite the name, this pattern form is much more general than the or\u00adpatterns \nfound in functional programming languages neither is there a requirement that the two patterns bind \nthe same variables, nor do the two patterns have to share the same arm. Instead, this pattern form is \nbetter compared to the vertical bar separating case alternatives p1 . e1 |p2 . e2 | ... |pn . en. The \nBOT typing rule says that . is a valid pattern at all types. It is a syntactic marker for an incomplete \npattern match: there is no way for any value to successfully match against it. As a result, the progress \nlemma fails when the . pattern can be used without restriction (though type preservation still holds), \nbecause a false\u00adpattern can be used to block reduction in a well-typed program. This is an intentional \ndecision, made for two reasons. First, internalizing incompleteness as a form of type-unsafety makes \nit easy to prove the soundness of a coverage algorithm: we know that a coverage algorithm is sound if \nit is strong enough to make the progress lemma go through. Second, having false-and or-patterns allows \nus to de.ne the complement of a pattern for any pattern at a type A, we can de.ne another pattern that \nmatches exactly the values of type A the original does not. The negation operation has a very similar \nform to the de Morgan identities: \u00acx = . \u00ac() = . \u00ac(p, p ' ) = (\u00acp, T) . (T, \u00acp ' ) \u00ac[] =[] ' \u00ac[p | \np ' ] =[\u00acp |\u00acp ] \u00acT = . ' \u00ac(p . p ' )= \u00acp .\u00acp \u00ac. = T ' \u00ac(p . p ' )= \u00acp .\u00acp We can show that this de.nition \nis a genuine complement. PROPOSITION 3 (Pattern Negation). If G; p : A, . I r1 : C, G ' ; \u00acp : A, . \n' I r2 : C ', and \u00b7f v : A, then: L((v/p))r1 '. r1 ' or L((v/p ' ))r2 '. r2 ' AA It is not the case \nthat there exist r1' ,r2 ' such that L((v/p))r1 '. A r1 ' and L((v/\u00acp))r2 '. r2' . A So any value v will \nsuccessfully match against either p or \u00acp, but not both. The proof of this theorem is a routine induction, \nbut uses the deep operations de.ned in the next section. Now, we can explain how to interpret ML-style \ncase statements in terms of focused pattern matching. Suppose we have a case statement with the branches \nq1 . e1| ...qn . en, where we write q for ML pattern expressions: ML Patterns q ::= x |()| (q, q ' )| \ninl q | inr q || x as q First, we give a simple priority-free interpretation of each pattern, which de.nes \nthe semantic brackets [ q] used earlier to motivate our extensions to the pattern language: [ x] = x \n[ ()] = ()[ (q1,q2)] = ([ q1] , [ q2] )[ inl q] = [[[q] |.] [ inr q] =[.| [ q]]] [ ] = T [ x as q] = \nx . [ q] Next, we translate a series of disjunctive ML patterns q1 . e1| ... |qn . en into a pattern/arm \npair (p; r) as follows: translate(q1 . e1| ... |qn . en)= translate ' (q1 . e1| ... |qn . en; T) translate \n' (\u00b7; neg)=(.; .) --. translate ' (q1 . e1|-q . e; neg)= let p1 =[ q1] . neg let r1 = arm(p1;[ e1]]) \n---. let (p; r)= translate ' (q . e; \u00ac[ q1] . neg) (p1 . p; r1 . r) So q1 gets translated to [ q1] .T, \nq2 gets translated to [ q2] . \u00ac[ q1] .T, and so on, with all of the negated patterns being stored in \nthe accumulator argument neg. We then disjunctively join the patterns for each branch. Likewise, to translate \nthe ML term e1 to a pattern term r1, we .rst need to .nd the translation of the term on its own (which \nwe assume to be [ e1] ), and then expand it so that the sum-and or-patterns within p1 can .nd the term \n[ e1] where they expect to. We give the function to do this below: arm(\u00b7; r)= r arm(x, ps; r)= arm(ps; \nr) arm((), ps; r)= arm(ps; r) arm((p1,p2) , ps; r)= arm(p1,p2, ps; r) arm([], ps; r) = [] ' arm([p | \np ' ], ps;[r | r ' ]) = [arm(p, ps; r) | arm(p ,ps; r)] arm(T, ps; r)= arm(ps; r) '' arm(p . p ,ps; r)= \narm(p,p ,ps; r) arm(., ps; r)= . '' arm(p . p ,ps; r)= arm(p, ps; r) . arm(p ,ps; r) This function walks \ndown the structure of its argument, repli\u00adcating the or/sum structure of its pattern arguments with or/sum \narms, and placing r at the leaves of this tree. (Unsurprisingly, this function exactly mimics the way \nthat the left-inversion phase de\u00adcomposes the pattern context.) 4. Deep Operations on the Pattern Context \nIn order to explain features like pattern compilation, we need to be able to manipulate and reorder the \npattern context. For example, we would like to be able to simplify a pattern like [x |.] . [.| y] into \na pattern like [x | y]. However, the pattern context is ordered, in order to .x the order in which case \nanalyses are performed. So theorems about manipulating patterns deep inside the pattern context must \nalso have associated algorithms, which explain how to perform this restructuring. 4.1 Deep Inversion \nand Deep Introduction Lemmas We begin by showing that it is admissible to decompose pattern hypotheses \nanywhere in the context. Each of these lemmas gener\u00adalizes one of the positive left rules, allowing the \ndecomposition of a pattern hypothesis anywhere within the pattern context. To save space, the lemmas \nare given in inference rule style, with the double line indicating that the top and the bottom imply \neach other. G,x : A;., . ' I r : C G; ., . ' I r : C ===================== DVAR ==================== \nD1 G; .,x : A, . ' I r : C G; ., () :1, . ' I r : C G; .,p1 : A1,p2 : A2, . ' I r : C ================================= \nD\u00d7 G; ., (p1,p2) : A1 \u00d7 A2, . ' I r : C G; ., . ' I r : C ===================== DT G; ., T : A, . ' I \nr : C G; .,p1 : A, p2 : A, . ' I r : C ============================ D. G; .,p1 . p2 : A, . ' I r : C \n These lemmas show that the inversion and introduction principles for variable, unit, pair, and and-patterns \ncan be generalized to work deep within the pattern context are all admissible, and that doing so does \nnot change the shape of the associated arm. Next, we generalize the introduction and inversion principles \nassociated with the +L and .L rules, and say that it is admissible to apply left-rules for sums and ors \ndeep within the pattern context. However, unlike the case for pair-and and-patterns, doing so does require \nsubstantially restructuring the arms, because we may need to go deep within the arms to ensure that the \ncase splits in the proof terms still match the order in the context. As a result, we use auxilliary functions \nto restructure the proof terms appropriately. G; .,p1 : A1, . ' I OutL (.; r): C + G; .,p2 : A2, . ' \nI OutR (.; r): C + ================================== D+OUT G; ., [p1 | p2]: A1 + A2, . ' I r : C G; \n.,p1 : A, . ' I OutL .(.; r): C G; .,p2 : A, . ' I OutR .(.; r): C =============================== D.OUT \nG; .,p1 . p2 : A, . ' I r : C G; .,p1 : A1, . ' I r1 : C G; .,p2 : A2, . ' I r2 : C ==================================================== \nD+JOIN G; ., [p1 | p2]: A1 + A2, . ' I Join+(.; r1; r2): C G; .,p1 : A, . ' I r1 : C G; .,p2 : A, . \n' I r2 : C ================================================== D.JOIN G; .,p1 . p2 : A, . ' I Join.(.; \nr1; r2): C  Inthe D.JOIN and D+JOIN rules, we use the functions Join. and Join+ to properly combine \nthe two arms r1 and r2. Here is the de.nition of the Join+ function: Join+(\u00b7; r1; r2) =[r1 | r2] Join+(x \n: A, .; r1; r2)= Join+(.; r1; r2) Join+(() :1, .; r1; r2)= Join+(.; r1; r2) Join+((p1,p2) : A1 \u00d7 A2, \n.; r1; r2)= Join+(p1 : A1,p2 : A2, .; r1; r2) Join+([] : 0, .; r1; r2) =[] ' '' ' '' Join+([p1 | p2]: \nA + B, .; [r | r ]; [r | r ]) = 11 22 ' ' '''' [Join+(p1 : A, .; r ; r ) | Join+(p2 : B, .; r ; r )] \n1212 Join+(T : A, .; r1; r2)= Join+(.; r1; r2) Join+(p1 . p2 : A, .; r1; r2)= Join+(p1 : A, p2 : A, \n.; r1; r2) Join+(. : A, .; r1; r2)= . ' ''' '' Join+(p1 . p2 : A, .; r . r ; r . r )= 1 12 2 ' ' '''' \nJoin+(p1 : A, .; r ; r ) . Join+(p2 : a, .; r ; r ) 1212 This function walks down the structure of the \npattern contexts, destructuring the arms in lockstep (and hence their proof trees), until it reaches \nthe leaves of the derivation, where the goal patterns are at the leftmost position. Then it simply puts \nthe two derivations together according to the +L rule. Note that this function de.nition is apparently \npartial when the head of the context argument . is a sum pattern [p | p ' ] or an or-pattern p . p ', \nthen both of the term arguments must be either sum-case bodies [r | r ' ] or or-pattern bodies r . r \n' respectively. However, well-typed terms always satisfy this condition. (The de.nition of Join. is similar; \nthe only difference is that the base case becomes Join.(\u00b7; r1; r2)= r1 . r2.) Likewise, the D+OUT rule \nshows that we can take apart a term based on a case analysis it performs deep within its arm. The OutL \nfunction walks down the structure of the context to grab the correct branch of the case at each leaf: \nOutL (\u00b7;[r1 | r2]) + OutL (x : A, .; r) + OutL (() :1, .; r) +OutL ((p1,p2) : A1 \u00d7 A2, .; r) + OutL ([] \n: 0, .; r) + = r1 = OutL (.; r) + = OutL (.; r) += OutL (p1 : A1,p2 : A2, .; r) + = [] OutL ([p1 | p2]: \nA1 + A2, .; [r1 | r2]) = + [OutL (p1 : A1, .; r1) | OutL (p2 : A2, .; r2)] ++ OutL (T : A, .; r)= OutL \n(.; r) ++ OutL (p1 . p2 : A, .; r)= OutL (p1 : A, p2 : A, .; r) ++ OutL (. : A, .; r)= . +OutL (p1 . \np2 : A, .; r1 . r2)= + OutL (p1 : A, .; r1) . OutL (p2 : A, .; r2) ++ The other Out projections are identical, \nexcept with differing base cases as follows: OutR (\u00b7;[r1 | r2]) = r2 + OutL .(\u00b7; r1 . r2)= r1 OutR .(\u00b7; \nr1 . r2)= r2 Finally, we have generalized rules for the 0 type and false\u00adpattern. D0 G; ., [] : 0, . \n' I Abort0(.) : C D. G; ., . : A, . ' I Abort.(.) : C As expected, there are associated functions to \nensure that an abort or a . occurs at every leaf of the proof tree: Abort0(\u00b7) =[] Abort0(x : A, .) = \nAbort0(.) Abort0(() :1, .) = Abort0(.) Abort0((p1,p2) : A1 \u00d7 A2, .) = Abort0(p1 : A1,p2 : A2, .) Abort0([] \n: 0, .) =[] Abort0([p1 | p2]: A1 + A2, .) = [Abort0(p1 : A1, .) | Abort0(p2 : A2, .)] Abort0(T : A, \n.) = Abort0(.) Abort0(p1 . p2 : A, .) = Abort0(p1 : A, p2 : A, .) Abort0(. : A, .) = . Abort0(p1 . p2 \n: A, .) = Abort0(p1 : A, .) . Abort0(p2 : A, .) Abort.(.) is similar, with the .rst case returning .. \n 4.2 Coherence of the Deep Rules The deep inversions let us destructure terms based on a pattern hy\u00adpothesis \nanywhere in the pattern context, and the deep introductions let us re-construct them. Beyond the admissibility \nof the deep inver\u00adsion and introduction rules, we will need the following coherence properties: PROPOSITION \n4. The following equations hold for all ., r, r1, r2 Join.(.; r1; r2)= r if and only if r1 = Out.L (.; \nr) and r2 = OutR .(.; r), Join+(.; r1; r2)= r if and only if r1 = OutL +(.; r) and r2 = OutR (.; r), \n+ If G; ., [] : 0, . ' I r : C, then r = Abort0(.),  If G; ., . : A, . ' I r : C, then r = Abort.(.). \n  These properties give us a round-trip property if we take a term apart with a deep inversion and \nput it back together with a deep introduction (or vice-versa), then we get back the term we started with. \nIn addition to this, we will need some lemmas that let enable us to commute uses of Join and Out. PROPOSITION \n5. For all ..{+, .} and d .{L, R}, we have: ' ' r = Outd .(.,pL . pR : A, . ' .(.; Outd; r)) iff ' ' \nr = Outd; Outd .(.,pd : A, . '.(.; r)). '  r = Outd .(.,pL . pR : A, . ' ; Join.(.; r1; r2)) iff ' r \n= Join.(.; Outd ; r1); Outd ; r2)) .(.,pL : A, . '.(.,pR : A, . ' ' r = Outd ; r1; r2)) iff .(.; Join.(.,pL \n. pR : A, . ' ' r = Join.(.,pd : A, . '.(.; r1); Outd ; Outd .(.; r2)) ' '' r = Join.(.,pL . pR : A, \n. ' ; Join.(.; r1; r ); Join.(.; r2; r )) 12 ' iff r = '' Join.(.; Join.(.,pL : A, . ' ; r1; r2); Join.(.,pR \n: A, . ' ; r ; r )) 12 In addition to these equalities, we have another four similar cases where the \n.-functions are replaced with +-functions. The net effect of these equalities is that we can use the \ndeep inversions and introductions in any order, with the con.dence that the order will not change the \n.nal result term we get the same result term regardless of the path to its construction. 4.3 Deep Exchange \nand Substitution Since reordering the pattern context corresponds to doing case splits in different orders, \nit seems intuitively clear that reordering the pattern context should give rise to a new proof term that \nis in some sense equivalent to the old one. To make this idea precise, we do three things. First, we \nprove that the rule of exchange is admissible for the pattern context. Second, we extend the notion of \npattern substitution to include substitutions deep within the context, and not just at the leftmost hypothesis. \nFinally, we show that if we get the same result regardless of whether deep substitution follows an exchange, \nor preceeds it, which justi.es the intuition that moving the pattern hypothesis in the ordered context \ndidn t matter . 4.3.1 Exchange , . '' PROPOSITION 6 (Exchange). If G; .,p : A, . ' I r : C holds, then \nwe have G; ., . ' ,p : A, . ''IEx(.; p : A;. ' ; r): C. , . ''I Here, we assert that if we have a derivation \nof G; .,p : A, . ' r : C, then we can move the hypothesis p : A to the right, past the assumptions in \n. '. The proof term naturally gets altered, and so we must give a function Ex which computes the new \narm. Ex(.; x : A;. ' ; r)= r Ex(.; () : 1;. ' ; r)= r Ex(.; (p1,p2) : A1 \u00d7 A2;. ' ; r)= ' let r = Ex(.; \np1 : A1; p2 : A2, . ' ; r) Ex(.; p2 : A2;. ' ,p1 : A1; r ' ) Ex(.;[] : 0;. ' ; r)= Abort0(.) Ex(.; [p1 \n| p2]: A1 + A2;. ' ; r)= ' let r = Ex(.; p1 : A1;. ' ; OutL (.; r)) 1+ ' let r = Ex(.; p2 : A2;. ' ; \nOutR (.; r)) 2+ '' Join+(., . ' ; r1; r ) 2 Ex(.; T : A;. ' ; r)= r Ex(.; p1 . p2 : A;. ' ; r)= ' let \nr = Ex(.; p1 : A; p2 : A, . ' ; r) Ex(.; p2 : A;. ' ,p1 : A1; r ' ) Ex(.; . : A;. ' ; r)= Abort.(.) \nEx(.; p1 . p2 : A1 + A2;. ' ; r)= ' let r = Ex(.; p1 : A1;. ' ; Out.L (.; r)) 1 ' let r = Ex(.; p2 : \nA2;. ' ; OutR 2 .(.; r)) '' Join.(., . ' ; r ; r ) 12 This function is inductively de.ned on the structure \nof the pattern argument p, and it uses the deep inversion principles (and their associated functions \nOut) to break down the derivation for the recursive calls, and it uses the deep introduction functions \n(and their associated functions Join) to rebuild the arm for the new pattern context.  4.3.2 Deep Substitution \nThe statement of the deep pattern substitution lemma is a straight\u00adforward generalization of the substitution \nprinciple for the pattern context: PROPOSITION 7. If \u00b7f v : A, and G; .,p : A, . ' I r : C, and ' L((v/p)). \nr'. r ', then G; ., . ' I r : C. A Of course, we need to de.ne deep pattern substitution. In the de.nition \nbelow, note that it is nearly identical to the ordinary pattern substitution we simply index the relation \nby ., and use the Join* and Out** functions in the place of the [\u00b7|\u00b7] and \u00b7.\u00b7 constructors. L((v/x)). \nr'. [(v : A)/x]r L((()/())). r'. r A 1 ' ''' L((v1/p1)). r'. r L((v2/p2)). r'. r A1 A2 '' L(((v1,v2) \n/ (p1,p2))). r'. r A1\u00d7A2 '' L((v/p1)). (OutL (.; r)) '. r L((v/p2)). (OutR (.; r)) '. r A +A + '' L((inl \nv/[p1 | p2])). r'. r L((inr v/[p1 | p2])). r'. r A+BA+B ' ''' L((v/p1)). r'. r L((v/p2)). '. r A r A \n'' L((v/T)). r'. r L((v/p1 . p2)). r'. r AA '' L((v/p1)). (OutL . r L((v/p2)). (OutR . r A .(.; r)) 'A \n.(.; r)) ''' L((v/p1 . p2)). r'. r L((v/p1 . p2)). r'. r AA  4.3.3 Permuting Substitution and Exchange \nNow, we can show that we can permute the order in which we do substitutions and exchanges. That is, we \nwant to show that if we substitute a value for a pattern in a context after doing an exchange, we get \nthe same result as performing the exchange, and then substituting into the permuted context. We end up \nwith four cases to this lemma, depending on whether the target hypothesis of the exchange is to the right \nof the hypothesis to be substituted, the pattern to be substituted itself, or is a hypothesis to the \nleft of the substitution target. For clarity, we write these lemmas in inference rule style, putting \nthe premises above the line and the conclusion below. In the following two cases, we assume \u00b7f v : A: \n'' L((v/p)). r'. r G; .,p : A, . ' ,p : B, . '' I r : C A '' ' ) L((v/p)). Ex(.,p : A, . ' ; p : B;. \n'' ; r) '. Ex(., . ' ; p : B;. '' ; r A ' , . '' I r : C L((v/p)). r'. r G; .,p : A, . ' A .,. ' ' L((v/p))Ex(.; \np : A;. ' ; r) '. r A In the .rst case, we exchange the position of a hypothesis to the right of the \nsubstitution target, and in the conclusion we see that performing the substitution on the exchanged term \nyields the same result as performing exchange on the substitutand. In the second case, we see that exchanging \nthe target of a substitution does not change the result. In the next two cases, we assume that \u00b7f v : \nB: .. ,. '' L' , . '' ' : B, . ''' I r : C v/p '\u00b8\u00b8.,p:A,. ' r'. r G; .,p : A, . ' ,p B .. ,p:A,. '' \nL' v/p '\u00b8\u00b8.,. ' Ex(.; p : A;. ' ; r) '. Ex(.; p : A;. ' ; r ) B .. L'' , . ''' I r : C : B, . '' v/p \n'\u00b8\u00b8.,p:A,. ' r'. r G; .,p : A, . ' ,p B .. L' , . '' ' v/p '\u00b8\u00b8.,. ' Ex(.; p : A;. ' ,p : B, . '' ; r) \n'. Ex(.; p : A;. ' ; r ) B In the third case, the target of the exchange is to the left of the substitution \ntarget, but is not exchanged past it. In the fourth case, the exchange target is to the left of the substitution \ntarget, and the action of the exchange moves it to the right of the substitution target. Proving this \ncommutation involves proving it for Out and Join, and then working up. 4.3.4 Discussion In this section, \nwe have stated numerous technical lemmas. The rea\u00adson for going into this level of detail is to illustrate \nthat the overall structure of the metatheorems 1) is extremely regular and 2) con\u00adsists of statements \nof familiar logical principles. The only thing that makes these lemmas differ from the corresponding \nlemmas for the sequent calculus formulation is that we have to explicitly manage the proof terms. Even \nso, we can see the algorithms as nothing more than the constructive content of the proofs showing the \nadmissibil\u00adity of principles like inversion, introduction, exchange and substi\u00adtution. When we use these \nalgorithms as subroutines to implement coverage checking and pattern compilation, we will be able to \nsee exactly how directly they depend on logical proof transformations. 5. Coverage Checking In order \nto get useful answers out of the machinery of the previous sections, we .rst need some good questions. \nOne question is the question of coverage checking how can we ensure that a pattern match against a value \nwill always yield a unique result? This is an interesting question because the pattern language we have \nintroduced does not always answer these questions in the af.rmative. For example, the pattern [x |.] \n. [.| y] will never match against any value, and likewise, the pattern x . (T,y) can match in two different \nways. So we must .nd out how to check whether the pattern substitu\u00adtion relation de.nes a total function \nfor a particular pattern. Since a relation is functional when it is total and it has only one output \nfor each input, we will de.ne judgements to track both of these con\u00additions. We begin with the judgement \np det A, which should hold whenever there is at most one way any value of type A can match against the \npattern p. p det A x det A [] det 0 p1 det A1 [p1 | p2] de p2 det A2 t A1 + A2 () det 1 p1 det A1 p2 \ndet A2 (p1, p2) det A1 \u00d7 A2 T det A p1 det A p2 det A p1 . p2 det A p1 det Ap2 det Ap1,p2 fail A . det \nAp1 . p2 det A This judgement inductively follows the structure of the pattern, asking only that each \npattern s sub-components are deterministic, until it reaches the case for the or-pattern p . p ' . This \nis the dif.cult case, because both p and p ' might be deterministic, but the combination might not be. \nFor example, consider the pattern x .T. Each half is trivially deterministic, but because there are values \nthat can match either pattern, the or-pattern as a whole is not determinate. If we knew that there were \nno values that could match both pat\u00adterns, then we could know the whole pattern covers. To determine \nthis, we introduce the judgement p1,...,pn fail A, which holds when no value v of type A can match against \nall of the patterns pi. Now, we call out to p1,p2 fail A to establish the premise of the or\u00adpattern case \n we believe p1 . p2 det A when we know that there is nothing in the intersection of p1 and p2. We give \nthe de.nition . of -p fail A itself below: - . p fail A -. p1, -. p2 fail A -. p1, p, p ' , -. p2 fail \nA -. p1, T, -. p2 fail A -. p1, p . p ' , -. p2 fail A -. p1, ., -. p2 fail A -. -. -. ' -. p1, p, p2 \nfail Ap1,p ,p2 fail A -' . .- p1,p . p ,p2 fail A -. - -. -p1 fail A1 . . p2 fail A2 p1,p2 fail A -. \n- ---.-. - . p1, x, p2 fail A [] fail 0[p1 | p2] fail A1 + A2 -. - . p1 fail A1 p2 fail A2 ----. ----. \n(p1,p2) fail A1 \u00d7 A2 (p1,p2) fail A1 \u00d7 A2 Furthermore, we can use the fact that we have a syntactic negation \non patterns to turn our failure judgement into a coverage judgement. That is, if \u00acp fail A, then we know \nthat p must match against all values of type A which is precisely what we require of coverage! (\u00acp) \nfail A p covers A Now, we can make these informal claims precise, and show the soundness of these judgements. \nPROPOSITION 8 (Failure). If we have derivations p1,...,pn fail A  G; .,p1 : A, . . . , pn : A, . ' \nI r1 : C,  \u00b7f v : A  then it is not the case that there exist r2,...,rn+1 such that for all i .{1,...,n}, \nL((v/pi)). ' A ri . ri+1. The proof of this statement is via an induction on the failure judgement, and \nin each case we make use of the deep operations de.ned in the previous section. The most interesting \ncase of the proof is when we reach the ----. rule for decomposing a set of pair patterns (p1,p2). In \nthis case, we need to make use of the lemmas that allow us to commute ex\u00adchange and substitution we \nbegin with a context in the form ------------. ., (p1,p2) : A1 \u00d7 A2, . '. Inversion lets us rewrite the \ncontext to ----------. .,p1 : A1,p2 : A2, . '. Then, we use exchange to transform it into ----. ----. \nthe form .,p1 : A1,p2 : A2, . ', and appeal to the induction hy\u00adpothesis. This gives us enough evidence \nto construct the refutation we need only because we can reorder a series of exchanges and substitutions \nwithout changing the .nal result. PROPOSITION 9 (Coverage). If we have derivations \u00b7f v : A, ' p covers \nA, and G; .,p : A, . ' I r : C, then L((v/p)). r'. r . A This is an immediate consequence of the soundness \nof the failure judgement, and the semantics of pattern negation. PROPOSITION 10 (Determinacy). If we \nhave derivations \u00b7f v : A,  p det A,  G; .,p : A, . ' I r : C,  ' D1 :: L((v/p)). r'. r , A '' \nthen if D2 :: L((v/p)). r'. r , we know D1 = D2. A ' We write D :: L((v/p)). r'. r to indicate that we \nwant A to consider a particular derivation D of the pattern substitution of v into p. This means that \nour determinacy judgement ensures that there is at most one way to perform a pattern substitution for \neach value. We can now recover the progress lemma, by changing the rules governing the introduction of \npattern hypotheses: G; .,p : A f u : Bp covers Ap det A .R G; . f .p. u : A . B G I t : A G; p : A I \nr : Bp covers Ap det A FOCUSP G; \u00b7 I case(t, p . r): B PROPOSITION 11 (Progress, Redux). We have that: \n1. If \u00b7f e : A, then e .PR e ' or e is a value v. 2. If \u00b7; \u00b7f u : A, then u .NR u ' or u is a value \nv. 3. If \u00b7; \u00b7 I r : A, then r .PL r ' or r is a value v. 4. If \u00b7 I t : A, then t .NL t ' or t is a \nterm (v : A).  Finally, while we are discussing coverage, it is worth pointing out that the failure \njudgement also allows us to detect redundant or useless patterns, during the translation of ML patterns \nto focused patterns. As we compile each arm, we can check to see if the arm conjoined with the negation \nof its predecessor pattern fails, and if so, we know that the arm is redundant, and can signal an error. \n6. Pattern Compilation 6.1 What Is Pattern Compilation? If we implemented the focused pattern calculus \nin a naive way, we would observe that there are several inef.ciencies in the pat\u00adtern substitution algorithm, \narising from and-and or-patterns. And\u00adpatterns p . p ' can force the same value to be destructured mul\u00adtiple \ntimes. For example, a match of a value v against the pattern [x | y] . [u | v] will result in v being \ndestructured twice once to determine whether to take left or right branch for the pattern [x | y], and \nagain to determine whether to take the left or right branch for [u | v]. This is redundant, since one \ntest suf.ces to es\u00adtablish whether v is a left-or right-injection. Likewise, or-patterns can also introduce \ninef.ciency, because their naive implementation is via backtracking when a value v is matched against \na pattern p1 . p2, we will try to match p1 and if it fails, try matching p2. This can result in repeated \nre-tests if the cause of the failure is deep within the structure of a term. For example, suppose we \nmatch a value of the form inl inl v ' against a pattern like [[.|T] |T].[[T|.] |.]. Here, a naive left-to-right \nbacktracking algorithm will case analyze two levels deep before failing on the left branch, and then \nit will repeat that nested case analysis to succeed on the right branch. To avoid these inef.ciencies, \nwe want a pattern compilation algorithm. That is, we want to take an otherwise complete and deterministic \npattern and arm, and transform them into a form that does no backtracking and does not repeatedly analyze \nthe same term, and whose behavior under pattern substitution is identical to the original pattern and \narm. We can state this constraint by restricting our language of pat\u00adterns to one in which 1) failure \nand or-patterns do not occur, and 2) the only use of conjunctive patterns is in patterns of the form \nx . p. The .rst conditions ensures that pattern substitution will never fail or backtrack, and the second \ncondition ensures that and-patterns can never force the re-analysis of a term, since a variable pattern \ncan only trigger a substitution. The target sublanguage of patterns is given in the following grammar: \nc ::= x |()| (c1,c2)| [] | [c1 | c2] | T| x . [c1 | c2] | x .(c1,c2) Observe that a restricted pattern \nc corresponds to a series of primi\u00adtive let-bindings, pair bindings, and case statements on sums, each \nof which corresponds to a form in the usual lambda calculus. So we can formalize the pattern compilation \nas asking: is there an algorithm that can take a complete, deterministic pattern p with arm r, and translate \nit into a pattern c (with arm r '), such that for '' '''? all v, if L((v/p))r'. r if and only if L((v/p))r'. \nr AA  6.2 The Pattern Compilation Algorithm We give a pattern compilation algorithm in Figure 2, which \ntakes two arguments as an input. The .rst is a pattern context .. This argument will be used as the termination \nmetric for the function, and will get smaller at each recursive call. It also constrains the type of \nthe second argument. The second argument is a set S of pairs, with each pair consisting of a row of patterns \nqi and an arm r. Each row s pattern list is the same length as the metric argument, and the n-th pattern \nin the pattern list is either T or the same pattern as the n-th pattern in the metric list .. Additionally, \nthis set satis.es the invariant that for any sequence of values vi : Ai of the right types, there is \nexactly one element of S for which all of the vi can succeed in matching its qi. The set S is related \nto the matrix of patterns found in the traditional presentations of pattern compilation. We only need \na set instead of an array, because our alternation operator p . p ' is a form of non-deterministic choice. \nHowever, this also means we need the extra invariant that there is only one matching row for any value, \nbecause we have no ordering that would let us pick the .rst matching row. With this description in mind, \nwe can state the correctness theorems below: PROPOSITION 12 (Soundness of Pattern Compilation). If we \nhave that .= p1 : A1,...,pn : An ' '' S is a set of pairs such that for every (p1,...,p n; r ) . S, ' \n''' pi .{pi, T}, and G; p1 : A1,...,p n : An, . ' I r : C ' '' For all v1 : A1,...,vn : An there exist \n(p1,...,p n; r1) . '' S such that there exist r2 ...r n+1 such that for all i . {1 ...n}. L((vi/pi' ))r \n' '. r ' ii+1. Ai (c1,...,cn; r1)= Compile(.; S) then it holds that G; c1 : A1,...,cn : An, . ' I r1 \n' : C, and  For all v1 : A1,...,vn : An, if there exists a unique (p1' ,...,p n' ; r1) . S, such that \nthere exists a unique r2,...,rn+1, such that for all 1 = i = n, L((vi/pi' ))r ' '. ri' +1, then there \n iA exist r2 ...rn+1 such that for all 1 = i = n, L((vi/ci))ri '. A ' ri+1 and rn+1 = rn+1. PROPOSITION \n13 (Termination of Pattern Compilation). If we have that .= p1 : A1,...,pn : An ' '' S is a set of pairs \nsuch that for every (p1,...,p n; r ) . S, ' ''' pi .{pi, T}, and G; p1 : A1,...,p : An, . ' I r : C n \n' '' For all v1 : A1,...,vn : An there exists a unique (p1,...,p n; r1) . '' S such that there exist \nunique r2 ...r n+1 such that for all i .{1 ...n}. L((vi/pi' ))r ' '. ri' +1 iAi then there is a (c1,...,cn; \nr1)= Compile(.; S) Looking at Figure 2, we see that the compilation algorithm is recur\u00adsive, and at each \nstep it 1) breaks down the outermost constructor of the leftmost pattern in the pattern context ., 2) \nadjusts the set S to match the invariant, 3) recursively calls Compile, and 4) constructs the desired \nresult from the return value. Clearly, if . is empty, then there is no work to be done, and the algorithm \nterminates. If the .rst pattern is a unit pattern, then we know that the .rst element of each of the \npattern lists in S is either T or (). So, we can adjust the elements of S by using inversion to justify \ndropping the T and () patterns from the start of each of the pattern lists in S. Then, we can recursively \ncall the pattern compiler, and use the unit introduction rule to justify restoring the unit pattern to \nthe front of the list. Likewise, if the .rst pattern is a pair pattern (p1,p2), we can use inversion \nto justify splitting each of the pattern lists in S. If the .rst pattern in a pattern list is (p1,p2), \nthen we can send it to p1,p2, and if it is T, we can send it to T, T.4 (The intuition is that at a pair \ntype, T is equivalent to (T, T).) Then, after the recursive call we can use pair introduction to construct \nthe optimized pair pattern. If the .rst pattern is an abort pattern [], then we can return a pattern \nlist that starts with [] and is followed by a sequence of T patterns. This is .ne because there are no \nvalues of type 0, so the correctness constraint holds vacuously. We reach the .rst complex case when \nthe .rst pattern is a sum pattern [p1 | p2]. First, we de.ne the Left and Right functions, which take \nthe elements of S and choose the left and right branches, respectively. (If the head of the pattern list \nis a T pattern, it gets assigned to both sides, since (T, qs; r) with the head at a sum type is equivalent \nto ([T|T], qs;[r | r]).) After splitting, we call Compile on the left and right sets. How\u00adever, we can \nt just put them back together with a sum pattern, be\u00adcause we don t know that the tails of the two pattern \nlists are the same they arise from different calls to Compile. This is what the Merge function is for. \nGiven two pattern contexts and arms consist\u00ading of optimized patterns, it will .nd a new pattern context \nand a new pair of arms, such that the new arms are substitution equivalent to the old ones, but which \nare typed under the new pattern list. PROPOSITION 14 (Context Merging). If we have that G; .1,c1 : A, \n. ' 1 I r1 : C,  G; .2,c2 : A, . ' 2 I r2 : C, and  \u00b7f v : A,  then we may conclude that '' '  (c \n; r1; r2)= MergeA(.1; c1; r1;.2; c2; r2),  G; .1,c ' : A, . ' 1 I r1 ' : C,  G; .2,c ' : A, . ' 2 I \nr2 ' : C, .1 ' '' '' L((v/c ' ))r1 '. r1 if and only if L((v/c1)).1 r1 '. r1 , and AA .2 ' '' '' L((v/c \n' ))r2 '. r2 if and only if L((v/c2)).1 r2 '. r2 AA The notation Merge * in Figure 2 indicates that \nwe apply Merge to each element of the sequences cs1 and cs2. Its de.nition uses another auxilliary function \nWeaken. As its name might suggest, this is a generalized weakening lemma for the pattern context. 4 Here, \nwe de.ne a local function split((p; r)) to perform this decomposi\u00adtion. Also, we write map split(S) to \nindicate mapping over a set. PROPOSITION 15 (Extended Weakening). If G; ., . ' Ir : C and r ' = WeakenA(.; \nc; r), then G; .,c : A, . ' I r ' : C, and for all ' \u00b7f v : A, L((v/c)). r'. r. A If the .rst pattern \nis an and-pattern p1 .p2, we can use inversion to justify splitting each of the pattern lists in S. If \nthe .rst pattern in a pattern list is p1 . p2, then we can send it to p1,p2, and if it is T, we can send \nit to T, T. (As usual, the intuition is that T is equivalent to T.T.) Then, after the recursive call \nwe can use pair introduction to construct the optimized pair pattern. However, we re not quite done \nour context has two patterns c1 and c2, and if we combined them with an and-pattern then we would potentially \nbe forced to repeat tests, which we want to avoid. So we introduce an auxilliary function And, which \nsatis.es the following property: PROPOSITION 16 (Conjunction Simpli.cation). If we have that G; .,c1 \n: A, c2 : A, . ' I r : C and \u00b7f v : A, then (c; r ' )= AndA(.; c1; c2; r),  G; .,c : A, . ' I r ' : \nC, and  ' '' '' L((v/c)). r'. r if and only if L((v/c1 . c2)). r'. r . AA It is worth looking at the \npair pattern case in Figure 3 in a little more detail. It uses the exchange lemma to move the components \nof two pair patterns together to change (c1,c2) , (c1' ,c 2' ) into ' '' c1,c 1,c2,c 2 , so that the \nsubcomponents of the pair patterns can be conjoined together. This illustrates why it was necessary to \nprove that substitution and the deep pattern operations could commute: changing this order implies changing \nthe order that the patterns substitutions would be performed. This function follows the structure of \nthe two pattern arguments, and when it reaches the pair-pair or sum-sum cases, it uses the deep inversion, \nintroduction, and exchange algorithms to reorder the pattern hypotheses so that the tests can be merged. \nFinally, we reach the case where the head of the pattern con\u00adtext is the or-pattern p1 . p2. For each \nelement of S, the split\u00adting algorithm proceeds as follows. Each element of the form (p1 . p2,... ; r1 \n. r2) becomes two cases (p1, T,... ; r1) and (T,p2,... ; r2). We know via inversion that (p1,... ; r1) \nand (p2,... ; r2) are well-typed, and using top-introduction makes each element satisfy the compile invariant \nfor p1 : A, p2 : A, . . .. Fur\u00adthermore, any sequence of values v, v, . . . with a repeated value v will \nmatch one of these two elements exactly when the original would match v, . . .. So we can invoke Compile \nrecursively, and receive (c1,c2,... ; r) as a return value. As in the and-pattern case, we can combine \nthe c1 and c2 patterns with the And function to .nd a c that does the job. With the correctness of the \ncompilation algorithm established, it is easy to show that the coverage algorithm of the previous section \npermits us to con.dently invoke the pattern compiler. PROPOSITION 17 (Pattern Compilation). If we have \nthat p covers A and p det A,  G; p : A, . I r : C, and  \u00b7f v : A,  then it is the case that (c; r \n' )= Compile(p : A; {(p, r)}), '' ''' L((v/p))r'. r if and only if L((v/c))r'. r AA 7. Extensions and \nFuture Work This language is highly stylized, but some obvious extensions work out very easily. First, \nadding iso-recursive types \u00b5a.A is straight\u00adforward. If we have a term roll(e) as the constructor for \na recursive Compile(\u00b7; {(\u00b7; r)}) = (\u00b7; r) Compile(() :1, .; S) = let split((T, qs; r)) = {(qs; r)}split(((), \nqs; r)) = {(qs; r)} S let (cs; r)= Compile(.; map split(S)) ((), cs; r) Compile((p1,p2) : A1 \u00d7 A2, .; \nS) = let split((T, qs; r)) = {(T, T, qs; r)}split(((p1,p2) , qs; r)) = {(p1,p2, qs; r)} S let (c1,c2, \ncs; r)= Compile(p1 : A1,p2 : A2, .; map split(S)) ((c1,c2) , cs; r) --. Compile([] : 0,p : A; S) = - \n. ([] : 0, T; []) --. Compile([p1 | p2]: A1 + A2,p : A; S) = -. - . '' let Left((T,p ; r)) = {(T,p ; \nr)} -. - . '' Left(([p1 | p2],p ;[r1 | r2])) = {(p1,p ; r1)} -. - . '' let Right((T,p ; r)) = {(T,p ; \nr)} -. - . '' Right(([p1 | p2],p ;[r1 | r2])) = {(p2,p ; r2)} --. S -. let (c1, cs1; r1)= Compile(p1 \n: A1,p : A; map Left(S)) --. S -. let (c2, cs2; r2)= Compile(p2 : A2,p : A; map Right(S)) '' -. -. let \n(cs; r ; r )= Merge-* (c1 : A1; cs1; r1; c2 : A2; cs2; r2) . 12 A . -'' ([c1 | c2], cs;[r | r ]) 12 \nCompile(x : A, .; S) = let split((T, qs; r)) = {(qs; r)}split((x, qs; r)) = {(qs; r)} S let (cs; r)= \nCompile(.; map split(S)) (x, cs; r) Compile(T : A1, .; S) = let split((T, qs; r)) = {(qs; r)} S let \n(cs; r)= Compile(.; map split(S)) (T, cs; r) Compile(p1 . p2 : A1, .; S) = let split((T, qs; r)) = {(T, \nT, qs; r)}split((p1 . p2, qs; r)) = {(p1,p2, qs; r)} S let (c1,c2, cs; r)= Compile(p1 : A1,p2 : A1, .; \nmap split(S)) let (c; r ' )= AndA(\u00b7; c1; c2; r) (c, cs; r ' )  Compile(. : A1, .; S) = let split((T, \nqs; r)) = {(qs; r)}split((., qs; r)) = \u00d8 S let (cs; r)= Compile(.; map split(S)) (T, cs; r) Compile(p1 \n. p2 : A1, .; S) = let split((T, qs; r)) = {(T, T, qs; r)}split((p1 . p2, qs; r1 . r2)) = {(p1, T, qs; \nr1), (T,p2, qs; r2)} S let (c1,c2, cs; r)= Compile(p1 : A1,p2 : A1, .; map split(S)) let (c; r ' )= AndA(\u00b7; \nc1; c2; r) (c, cs; r ' ) Figure 2. Pattern Compilation type, then we can simply add a pattern roll(p) \nto the pattern lan\u00adguge. We believe supporting System-F style polymorphism is also straightforward. Universal \nquanti.cation .a. A is a negative con\u00adnective, and so does not interact with the pattern language. Existen\u00adtial \nquanti.cation .a. A is a positive connective with introduction form pack(A, e), and we can add a pattern \npack(a, p) to support its elimination. Features such as GADTs (Jones et al. 2006; Simonet and Pottier \n2007) and pattern matching for dependent types (Co\u00adquand 1992; Xi 2003; McBride 2003), are much more \ncomplicated. In both of these cases, matching against a term can lead to the dis\u00adcovery of information \nthat re.nes the types in the rest of the match. This is a rather subtle interaction, and deserves further \nstudy. Another direction is to treat the proof term assignment discussed here as a lambda calculus in \nand of itself, rather than as a program\u00ad AndA(.; T; c; r)= AndA(.; c; T; r)= (c; r) AndA(.; x . c1; c2; \nr)= AndA(.; c1; x . c2; r)= let (c ' ; r ' )= AndA(.; c1; c2; r) ''' ' if c = y . c then (c ' ;[x/y]r \n' ) else(x . c ' ; r ) And1(.; (); x; r)= And1(.; x; (); r)= (x; r) And0(.; []; x; r)= And0(.; x; []; \nr)= ([]; Abort0(.)) AndA\u00d7B (.; x; (c1,c2); r)= AndA\u00d7B(.; (c1,c2); x; r)= (x .(c1,c2); r) AndA+B(.; x;[c1 \n| c2]; r)= AndA+B(.; [c1 | c2]; x; r)= (x . [c1 | c2]; r) AndA(.; x; y; r)= (x;[x/y]r) And1(.; (); (); \nr)= ((); r) And0(.; []; []; r)= ([]; Abort0(.)) .\u00b8 '' AndA\u00d7B (.; (c1,c2); c1,c ; r)= 2 ''' ' let (c1 \n; r ' )= AndA(.; c1; c1; Ex(.,c1 : A; c2 : B; c1 : A; r)) '' ' let (c ; r '' )= AndB (.; c2; c ; r ' \n) . 2 \u00b82 '''' '' ) ( c1 ,c ; r 2 '' AndA+B(.; [c1 | c2]; [c | c ]; r)= 12 '''' ' let (c ; r )= AndA(.; \nc1; c ; OutL (.,c1 : A; OutL (.; r))) 11 1++ '''' ' let (c2 ; r )= AndB (.; c2; c2; OutR (.,c2 : A; OutR \n(.; r))) 2 ++ '' '' '''' ([c | c ]; Join+(.; r ; r )) 12 12 Figure 3. Conjunction Simpli.cation ming \nlanguage, as we have done here. For example, we can use the exchange function to generalize the pattern \nsubstitution to deal with open terms and study properties like con.uence and normal\u00adization. The presence \nof sums makes this a trickier question than it may seem at .rst glance; focusing seems to eliminate the \nneed for some, but not all, of the commuting conversions for sum types. Finally, we should more carefully \nstudy the implications for implementation. A simple ML implementation of the algorithms given in this \npaper can be found at the author s web site, but it is dif.cult to draw serious conclusions from it because \nso little engineering effort has gone into it. However, a few prelimary observations are possible. Imple\u00admenting \ncoverage checking is very easy it is roughly 80 lines of ML code. This closely follows the inference \nrules given above, with the the addition of memoization to avoid repeatedly trying to .nd failure derivations \nof the same sequence of patterns. This seems to suf.ce for the sorts of patterns we write by hand; it \nis not clear whether there are (easily-avoidable) pathological cases that machine-generated programs \nmight exhibit. A more-or-less direct transliteration of the pattern compiler is in the neighborhood of \n300 lines of ML. While it is reasonably fast on small hand-written ex\u00adamples, it should probably not \nbe used in a production compiler. In particular, the Merge* algorithm is implemented via iterated calls \nto Merge, which can result in an enormous number of redundant traversals of the pattern tree. This was \ndone to simplify the correct\u00adness proof, but a production implementation should avoid that. 8. Related \nWork We .rst learned to view pattern matching as arising from the in\u00advertible left rules of the sequent \ncalculus due to the work of Kesner et al. (1996), and Cerrito and Kesner (2004). We have extended their \nwork by building on a focused sequent calculus. This permits us to give a simpler treatment; the use \nof an ordered context allows us to eliminate the communication variables they used to link sum MergeA(.1; \nT; r1;.2; c2; r2)= MergeA(.2; c2; r2;.1; T; r1)= ' let r1 = WeakenA(.1; c2; r1) ' (c2; r1; r2) MergeA(.1; \nx; r1;.2; c2; r2)= MergeA(.2; c2; r2;.1; x; r1)= ' let r1 = WeakenA(.1; c2; r1) ' let (c; r2)= AndA(.2; \nx; c2; r2) '' (c; r1; r2) MergeA(.1; x . c1; r1;.2; c2; r2)= MergeA(.2; c2; r2;.1; x . c1; r1)= '' let \n(c ' ; r ; r )= MergeA(.1; c1; r1;.2; c2; r2) 12 '''' '' let (c ; r1 )= AndA(.1; x; c1; r1) '' '' let \n(; r2 )= AndA(.2; x; c2; r2) '' ; r '' ''  (c 1 ; r2 ) Merge1(.1; (); r1;.2; (); r2)= ((); r1; r2) Merge0(.1; \n[]; r1;.2; []; r2)= ([]; Abort0(.1); Abort0(.2)) .\u00b8 '' MergeA\u00d7B(.1; (c1,c2); r1;.2; c1,c ; r2)= 2 '''' \n' let (c1 ; r1; r2)= MergeA(.1; c1; r1;.2; c1; r2) '''''' '' ' '' '' let (c ; r ; r )= MergeB (.1,c : \nA; c2; r ;.2,c : A; c ; r ) . 2 \u00b812 1 1122 '''' '''' ( c1 ,c ; r ; r ) 2 12 '' MergeA+B (.1;[c1 | c2]; \nr1;.2;[c | c ]; r2)= 12 ''' ' let (c ; r ; r )= MergeA(.1; c1; OutL (.1; r1); 112+ ' .2; c1; OutL (.1; \nr2)) + '' '' '' let (c ; r ; r )= MergeA(.1; c2; OutR +(.1; r1); 212 ' .2; c ; OutR (.1; r2)) 2+ '' '' \n''' ''' ([c | c ]; Join+(.1; r1; r ); Join+(.2; r2; r )) 121 2 WeakenA(.; T; r)= r WeakenA(.; x; r)= \nr Weaken1(.; (); r)= r Weaken0(.; []; r)= Abort0(.) WeakenA(.; x . c; r)= WeakenA(.; c; r) WeakenA\u00d7B \n(.; (c1,c2); r)= WeakenB (.,c1 : A; c2; WeakenA(.; c1; r)) WeakenA+B(.; [c1 | c2]; r)= Join+(.; WeakenA(.; \nc1; r); WeakenB (.; c2; r)) Figure 4. Pattern Merge patterns and their bodies. Furthermore, our failure \nand nondeter\u00administic choice patterns permit us to explain the sequential pattern matching found in functional \nlanguages, coverage checking, and pattern compilation. Focusing was introduced by Andreoli (1992), in \norder to con\u00adstrain proof search for linear logic. Pfenning (in unpublished lec\u00adture notes) gives a simple \nfocused calculus for intuitionistic logic, and Liang and Miller (2007) give calculi for focused intuitionistic \nlogic, which they relate to both linear and classical logic. Neither of these have proof terms. Zeilberger \n(2007) gives a focused calculus based on Dummett s notion of logical harmony (Dummett 1991). This calculus \ndoes not have a coverage algorithm; instead coverage is a side-condition of his typing rules. Our pattern \nsubstitution is a restricted form of hereditary substitution, which Watkins et al. (2004) introduced \nas a way of re.ecting the computational content of structural proofs of cut admissibility (Pfenning 2000). \nIn his work on Ludics, Girard (2001) introduced the idea of the daimon, a sequent which corresponds to \na failed proof. Introducing such sequents can give a logical calculus certain algebraic closure properties, \nat the cost of soundness. However, once the requisite properties have been used, we can verify that we \nhave any given proof is genuine by checking that the undesirable sequents are not present. This is an \nidea we exploited with the introduction of the . and p1 . p2 patterns, which make our language of patterns \nclosed under complement. Zeilberger (2008) gives a higher-order focused calculus. In this style of presentation, \nthe inversion judgement is given as a single in.nitary rule, de.ned using the functions of the ambient \nmeta\u00adlogic, rather than the explicit collection of rules we gave. The virtue of their approach is that \nit defers questions of coverage and de\u00adcomposition order into the metalogic. However, this is precisely \nthe question we wanted to explicitly reason about. In real compilers, there are two classical approaches \nto compil\u00ading pattern matching, either by constructing a decision tree (de\u00adscribed by Cardelli (1984) \nand Pettersson (1992)) or building a backtracking automaton (described by Augustsson (1985)). Our calculus \nuniformly represents both approaches, since backtracking can be represented with the use of the nondeterministic \ndisjunction pattern p1 . p2 and the abort pattern [], and case splitting is rep\u00adresented with the sum-pattern \n[p1 | p2]. This lets us view pattern compilation as a source-to-source transformation, which simpli.es \nthe correctness arguments. Fessant and Maranget (2001) describe a modern algorithm for pattern compilation \nwhich operates over matrices of patterns. Their algorithm tries to make use of an ef.cient mix of backtracking \nand branching, whereas our compilation algorithm builds a pure decision tree. It might be possible to \n.nd a presentation of their ideas without having to explicitly talk about low-level jumps and gotos, \nby replacing p . p ' with a biased choice that always tries p .rst. Maranget (2007) also describes an \nalgorithm for generating warnings for non-exhaustive matches and useless clauses. This algorithm is a \nspecialized version of the decision tree compilation algorithm which returns a boolean instead of a tree. \nHowever, his correctness proof is not as strong as ours: Maranget de.nes a matching relation and shows \nthat a complete pattern will always succeed on a match, but the connection between the matching relation \nand the language semantics is left informal. Sestoft (1996) shows how to generate pattern matching code \nvia partial evaluation. This ensures the correctness of the compilation, but he does not consider the \nquestion of coverage checking. Jay (2004) has also introduced a pattern calculus. Roughly, he takes the \nview that datatypes are just subsets of the universe of program terms (like Prolog s Herbrand universe), \nand then allows de.ning programs to match on the underlying tree representations of arbitrary data. This \napproach to pattern matching is very expres\u00adsive, but its extremely intensional nature means its compatibility \nwith data abstraction is unclear. The work on the .-calculus (Cirstea and Kirchner 2001) is an\u00adother \ngeneral calculus of pattern matching. It treats terms similarly to Jay s pattern calculus. Additionally, \nit uses the success or failure of matching as a control primitive, similar to the way that false-and \nor-patterns work in this work. However, the focus in this paper was on the case where the nondeterminism \nis inessential, rather than exploring its use as a basic control mechanism. Acknowledgements. The author \nthanks Jonathan Aldrich, Robert Harper, Dan Licata, William Lovas, Frank Pfenning, Ja\u00adson Reed, John \nReynolds, Kevin Watkins, and Noam Zeilberger for encouragement and advice. This work was supported in \npart by NSF grant CCF-0541021, NSF grant CCF-0546550, DARPA contract HR00110710019 and the Department \nof Defense. References J.M. Andreoli. Logic Programming with Focusing Proofs in Linear Logic. Journal \nof Logic and Computation, 2(3):297, 1992. L. Augustsson. Compiling pattern matching. Proc. of a conference \non Functional Programming Languages and Computer Architecture, pages 368 381, 1985.  Luca Cardelli. \nCompiling a functional language. In LFP 84: Proceedings of the 1984 ACM Symposium on LISP and Functional \nProgramming, pages 208 217, New York, NY, USA, 1984. ACM Press. ISBN 0-89791\u00ad142-3. doi: http://doi.acm.org/10.1145/800055.802037. \nS. Cerrito and D. Kesner. Pattern matching as cut elimination. Theoretical Computer Science, 323(1-3):71 \n127, 2004. H. Cirstea and C. Kirchner. The rewriting calculus -Part I. Logic Journal of the IGPL, 9(3): \n2001. T. Coquand. Pattern matching with dependent types. Proceedings of the Workshop on Types for Proofs \nand Programs, pages 71 83, 1992. M. Dummett. The Logical Basis of Metaphysics. Duckworth, 1991. Fabrice \nLe Fessant and Luc Maranget. Optimizing pattern match\u00ading. In ICFP 01: Proceedings of the sixth ACM SIGPLAN \nInter\u00adnational Conference on Functional Programming, pages 26 37, New York, NY, USA, 2001. ACM Press. \nISBN 1-58113-415-0. doi: http://doi.acm.org/10.1145/507635.507641. J.Y. Girard. Locus Solum: From the \nrules of logic to the logic of rules. Mathematical Structures in Computer Science, 11(03):301 506, 2001. \n C. B. Jay. The pattern calculus. Transactions on Programming Languages and Systems 26(6):911-937, 2004. \n S.P. Jones, D. Vytiniotis, S. Weirich, and G. Washburn. Simple uni.cation\u00adbased type inference for \nGADTs. Proceedings of the eleventh ACM SIGPLAN International Conference on Functional Programming, pages \n50 61, 2006. D. Kesner, L. Puel, and V. Tannen. A Typed Pattern Calculus. Information and Computation, \n124(1):32 61, 1996.  Chuck Liang and Dale Miller. Focusing and polarization in intuitionistic logic. \nIn 16th EACSL Annual Conference on Computer Science and Logic. Springer-Verlag, 2007. URL http://www.cs.hofstra.edu/~cscccl/focusil.pdf. \nLuc Maranget. Warnings for pattern matching. Journal of Functional Programming, 2007. C. McBride. Epigram. \nTypes for Proofs and Programs, 3085:115 129, 2003. Dale Miller, Gopalan Nadathur, Frank Pfenning, and \nAndre Scedrov. Uni\u00adform proofs as a foundation for logic programming. Ann. Pure Appl. Logic, 51(1-2):125 \n157, 1991. R.R. Milner, Mads Tofte, Robert Harper, and David McQueen. The De.ni\u00adtion of Standard ML:(revised). \nMIT Press, 1997. Mikael Pettersson. A term pattern-match compiler inspired by .nite au\u00adtomata theory. \nIn Uwe Kastens and Peter Pfahler, editors, CC, volume 641 of Lecture Notes in Computer Science, pages \n258 270. Springer, 1992. ISBN 3-540-55984-1. F. Pfenning. Structural Cut Elimination I. Intuitionistic \nand Classical Logic. Information and Computation, 157(1-2):84 141, 2000. P. Sestoft. ML pattern match \ncompilation and partial evalua\u00adtion. Lecture Notes in Computer Science, 1110:446, 1996. URL citeseer.ist.psu.edu/sestoft96ml.html. \nVincent Simonet and Francois Pottier. A constraint-based approach to guarded algebraic data types. Transactions \non Programming Languages and Systems 29(1), 2007. K. Watkins, I. Cervesato, F. Pfenning, and D. Walker. \nA concurrent logical framework: The propositional fragment. Types for Proofs and Programs, pages 355 \n377, 2004. H. Xi. Dependently Typed Pattern Matching. Journal of Universal Com\u00adputer Science, 9(8):851 \n872, 2003. Noam Zeilberger. The logical basis of evaluation order. Thesis proposal, May 2007. Carnegie \nMellon, Pittsburgh, Pennsylvania. Available at http://www.cs.cmu.edu/~noam/research/proposal.pdf., 2007. \nNoam Zeilberger. Focusing and higher-order abstract syntax. In George C. Necula and Philip Wadler, editors, \nPOPL, pages 359 369. ACM, 2008. ISBN 978-1-59593-689-9.   \n\t\t\t", "proc_id": "1480881", "abstract": "<p>In this paper, we show how pattern matching can be seen to arise from a proof term assignment for the focused sequent calculus. This use of the Curry-Howard correspondence allows us to give a novel coverage checking algorithm, and makes it possible to give a rigorous correctness proof for the classical pattern compilation strategy of building decision trees via matrices of patterns.</p>", "authors": [{"name": "Neelakantan R. Krishnaswami", "author_profile_id": "81320491252", "affiliation": "Carnegie Mellon University, Pittsburgh, PA, USA", "person_id": "P1301016", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1480881.1480927", "year": "2009", "article_id": "1480927", "conference": "POPL", "title": "Focusing on pattern matching", "url": "http://dl.acm.org/citation.cfm?id=1480927"}