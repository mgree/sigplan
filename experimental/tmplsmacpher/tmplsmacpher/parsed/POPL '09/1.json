{"article_publication_date": "01-21-2009", "fulltext": "\n A Calculus of Atomic Actions Tayfun Elmas Shaz Qadeer Serdar Tasiran Koc\u00b8 University, .Istanbul, Turkey \nMicrosoft Research, Redmond, WA Koc\u00b8 University, .Istanbul, Turkey telmas@ku.edu.tr qadeer@microsoft.com \nstasiran@ku.edu.tr Abstract We present a proof calculus and method for the static veri.cation of assertions \nand procedure speci.cations in shared-memory con\u00adcurrent programs. The key idea in our approach is to \nuse atom\u00adicity as a proof tool and to simplify the veri.cation of assertions by rewriting programs to \nconsist of larger atomic actions. We pro\u00adpose a novel, iterative proof style in which alternating use \nof ab\u00adstraction and reduction is exploited to compute larger atomic code blocks in a sound manner. This \nmakes possible the veri.cation of assertions in the transformed program by simple sequential rea\u00adsoning \nwithin atomic blocks, or signi.cantly simpli.ed application of existing concurrent program veri.cation \ntechniques such as the Owicki-Gries or rely-guarantee methods. Our method facilitates a clean separation \nof concerns where at each phase of the proof, the user worries only about only either the sequential \nproperties or the concurrency control mechanisms in the program. We implemented ourmethodinatoolcalled \nQED.Wedemonstratethesimplicityand effectiveness of our approach on a number of benchmarks includ\u00ading \nones with intricate concurrency protocols. Categories and Subject Descriptors D.2.4 [Software Engineer\u00ading]: \nSoftware/Program Veri.cation assertion checkers, correct\u00adness proofs, formal methods; F.3.1 [Logics \nand Meanings of Pro\u00adgrams]: Specifying and Verifying and Reasoning about Programs assertions, invariants, \npre-and post-conditions, mechanical ver\u00adi.cation; D.1.3 [Programming Techniques]: Concurrent Program\u00adming \n parallel programming General Terms Languages, Theory, Veri.cation Keywords Concurrent Programs, Atomicity, \nReduction, Abstrac\u00adtion 1. Introduction This paper is concerned with the problem of statically verifying \nthe (partial) correctness of shared-memory multithreaded programs. This problem is undecidable and, in \ntheory, no harder than the prob\u00adlem of verifying single-threaded programs. In practice, however, it is \nsigni.cantly more dif.cult to verify multithreaded programs. For single-threaded programs, the undecidability \nof program ver\u00adi.cation is circumvented by the use of contracts pre-conditions, post-conditions, and \nloop invariants to decompose the problem into manageable pieces. These contracts need to refer only to \nthe Permission to make digital or hard copies of all or part of this work for personal or classroom use \nis granted without fee provided that copies are not made or distributed for pro.t or commercial advantage \nand that copies bear this notice and the full citation on the .rst page. To copy otherwise, to republish, \nto post on servers or to redistribute to lists, requires prior speci.c permission and/or a fee. POPL \n09, January 18 24, 2009, Savannah, Georgia, USA. Copyright c &#38;#169; 2009 ACM 978-1-60558-379-2/09/01. \n. . $5.00 locally visible part of the program state, and can usually be stated with little dif.culty. \nReasoning about multithreaded programs, on the other hand, requires signi.cantly more intellectual effort. \nFor example, invariant-based reasoning [1, 21] requires for each line1 of the program an annotation that \nis guaranteed to be stable un\u00adder interference from other threads. Writing such a speci.cation is challenging \nbecause of the need to consider the effect of all thread interleavings. The resulting annotations in \neach thread are often non-local and complicated because they refer to the private state of other threads. \nThe rely-guarantee approach [16] provides more .exibility in specifying the interference from the environment \nbut the complexity of the required annotations is still signi.cant. The fundamental dif.culty in reasoning \nabout multithreaded programs is the need to reason about concurrent execution of .ne\u00adgrained atomic actions. \nIn this paper, we introduce an iterative ap\u00adproach to proving assertions in multithreaded programs aimed \nat circumventing this dif.culty. In our approach, in each step of the proof, we rewrite the program by \nlocally transforming its atomic actions to obtain a simpler program. Each rewrite performs one of two \ndifferent kinds of transformations abstraction and reduction. Abstraction replaces an atomic action with \na more relaxed atomic action allowing more behaviors. Abstraction transformations in\u00adclude making shared \nvariable reads or writes non-deterministic, and prefacing atomic statements with extra assertions. Reduction \n[18] replaces a compound statement consisting of several atomic actions with a single atomic action if \ncertain non-interference conditions hold. This transformation has the effect of increasing the granular\u00adity \nof the atomic actions in the program. Abstraction and reduction preserve or expand the set of behaviors \nof the program, so that as\u00adsertions proved at the end of a sequence of transformations are valid in the \noriginal program. While reduction and abstraction have been studied in isolation, the iterative and alternating \napplication of abstraction and reduction is a distinguishing and essential aspect of our method. Abstraction \nand reduction are symbiotic. Reduction creates coarse-grained ac\u00adtions from .ne-grained actions and allows \na subsequent abstrac\u00adtion step to summarize the entire calculation much as precondi\u00adtions and post-conditions \nsummarize the behavior of a procedure in a single-threaded program. Conversely, suitably abstracting \nan atomic action allows us to reason that it does not interfere with other atomic actions, and later \napplication of reduction are able to merge it with other actions. The examples in Sections 2 and 6 show \nthat the combined iterative application of these techniques is a sur\u00adprisingly powerful proof method \nable to prove intricate examples correct by a sequence of simple transformations and few annota\u00adtions. \nIn most cases, we are able to simplify the program enough so that assertions in the .nal program can \nbe validated by purely se\u00adquential reasoning within a single abstract atomic action. In other cases, \nthe atomic actions in the program become large enough for 1 For brevity, we use the word line to refer \nto the granularity of atomically executed statements. a straightforward application of existing techniques \nsuch as rely\u00adguarantee reasoning. Another distinguishing feature of our approach is the possibil\u00adity \nof introducing assertions at any point during the sequence of transformations while deferring their proof \nuntil later, when large enough atomic blocks make their proof easy. In our framework, an\u00adnotating any \natomic statement with any assertion is a valid program abstraction. The interpretation is that the action \nis relaxed so that if the assertion is violated, the execution is made to go wrong. An\u00adnotating an action \nwith an assertion, e.g. one that indicates that it is not simultaneously enabled with another action, \nmay enable fur\u00adther reduction steps. In other proof methods, when an additional assertion is introduced, \none is forced to prove that it is valid and preserved under interference by other threads. In our approach, \nwe use the introduced assertion without .rst proving it and take fur\u00adther proof steps that simplify the \nprogram. Yet, the soundness of our method is not compromised as long as all assertions are proved eventually. \nWe have implemented our veri.cation method in a tool called QED. Our tool accepts as input a multithreaded \nprogram written in an extension of the Boogie programming language [2] and a proof script containing \na sequence of proof commands. A proof command is used for one of two purposes. First, it may provide \na high-level tactic for rewriting the input program using abstraction, reduction or a combination of \nthe two. Second, it may provide a concise speci.cation of the behavior of the current version of the \nprogram; common speci.cations include locking protocols and data invariants. After executing each step \nin the proof script, QED allows the user to examine the resulting program, intercept the proof, and give \nnew commands. The tool automatically generates the veri.cation conditions justifying each step of the \nproof and veri.es them using Z3 [6], a state-of-the-art solver for satis.ability\u00admodulo-theories. We \nhave evaluated QED by verifying a number of multithreaded programs with varying degree of synchronization \ncomplexity. These examples include programs using .ne-grained locking and non-blocking data structures. \nWe have found that the iterative ap\u00adproach embodied in QED provides a simple and convenient way of communicating \nto the veri.er the programmer s understanding of the computation and synchronization in the program. \nThe proofs in our method are invariably simpler and more intuitive than the proofs based on existing \napproaches. To summarize, this paper makes the following contributions: A novel proof technique for \nmultithreaded programs, based upon rewriting of the input program iteratively using abstrac\u00adtion and \nreduction, producing in the limit a program that can be veri.ed by sequential reasoning methods.  A \ntool QED that implements our proof method using a set of intuitive, concise, and machine-checked proof \ncommands.  Evaluation of our technique and tool on a variety of small to medium-sized multithreaded \nprograms.   2. Motivating examples In this section, we provide an overview of our method using sev\u00aderal \nexamples. We begin by illustrating reduction and abstraction and, in Section 2.1, present a nontrivial \ninteraction between them. In these examples, each line of code performs at most one access to a global \nvariable. For example, we split the increment of x in Fig\u00adure 1 and the assignment of newsize to currsize \n(lines 15 16) in Figure 4 into multiple lines. We use if(*) and while(*) to denote nondeterministic choice. \nThe statements assume e and assert e cause the execution to block or go wrong, respectively, if e eval\u00aduates \nto false; otherwise, they are equivalent to a skip statement. void inc() { void inc() { int t; int t; \nacquire(lock); [havoc t; x := x+1]; t:= x; } t := t+1; x:= t; release(lock); } Figure 1. Lock-based \natomic increment The statement havoc x assigns x a nondeterminstic value of proper domain. Reduction \nproduces a single atomic action from the sequential composition of two atomic actions if either the .rst \naction is a right mover or the second action is a left mover. An action R commutes to the right of X \nif the effect of thread t executing R followed by a different thread u executing action X can be simulated \nby .rst thread u executing X followed by thread t executing R.If R commutes to the right of all actions \nin the program, it is said to be right mover. Common cases where R is a right mover are if R and X do \nnot con.ict (i.e., read or write any common variables), if R is non-deterministic (i.e. independent of \nthe program store) or if R disables X. A left mover is de.ned similarly. Lock acquires are right movers \nand lock releases are left movers. Accesses to local variables and race-free accesses to shared variables \nare both right and left movers. In Figure 1, the procedure inc on the left is a lock-based imple\u00admentation \nfor atomically incrementing a shared variable x. Through an iterative application of reduction, our method \ncan transform the body of this procedure into a single atomic action as shown on the right and indicated \nby square brackets. In Figure 2, the procedure inc on the left uses the CAS (Compare\u00adAnd-Swap) primitive \nfor a lock-free implementation of the same behavior. CAS(x,t,t+1) atomically compares the value of x \nwith t, writing t+1 into x and returning true if the two are identical, and leaving x unchanged and returning \nfalse if the two are different. Note that this program has con.icting accesses to x that are si\u00admultaneously \nenabled, and is consequently more dif.cult to reason about than the previous version. However, through \nan iterative ap\u00adplication of abstraction and reduction, our method can just as easily show that this \nprocedure atomically increments x; the sequence of transformations are shown from left to right in Figure \n2. First, we perform a simple transformation (as in [10]) that peels out the last iteration of the loop, \nthereby arriving at the version of inc in Figure 2(b). Next, we argue that it is a valid abstraction \nto replace the read of x in t:= x with havoc t since this only increases the set of possible behaviors. \nWe do not expect this abstraction to lead to additional assertion violations since the result of the \nread action is later veri.ed by CAS(x,t,t+1); reading a value other than the actual value of x would \nsimply lead to a failed CAS operation. We also abstract each unsuccessful execution of CAS(x,t,t+1) inside \nthe loop with the statement skip to arrive at the version of inc in Figure 2(c). In the version of inc \nin Figure 2(c), every operation except for the last one accesses only local state and is consequently \nboth a right and a left mover. Therefore, we can use reduction to sum\u00admarize the entire loop with a single \nhavoc t statement to arrive at the version of inc in Figure 2(d). Finally, we reduce the resulting sequential \ncomposition to arrive at a single atomic action in the version of inc in Figure 2(e). Figure 3(a) shows \nprocedure add, a client of inc. Procedure add has a parameter n required to be at least 0 and calls inc \nrepeat\u00adedly in a loop n times. Once the procedure inc has been trans\u00adformed into an atomic increment \nof x, we can reason about add by replacing the call to inc with an atomic increment of x to ob\u00ad void \ninc() { void inc() { void inc() { void inc() { void inc() { int t; int t; int t; int t; int t; while \n(true) { while (*) { while (*) { havoc t; t:= x; t:= x; havoc t; if (CAS(x,t,t+1)) assume x!=t; skip; \n[havoc t; break; } } x := x+1]; } t:=x; havoc t; havoc t; [assume x==t; [assume x==t; [assume x==t; x:=t+1]; \nx := t+1]; x := t+1]; } } } } } (a) (b) (c) (d) (e) Figure 2. Lock-free atomic increment void add(int \nn) { void add(int n) { void add(int n) { while (0<n) { while (0<n) { [assert 0<=n; inc(); [x := x+1]; \nx := x+n; n:=n-1; n:= n-1; n:=0]; } } } } } (c) (a) (b) Figure 3. Client of inc tain the version in \nFigure 3(b). In this version, every action other than the atomic increment accesses only local variables. \nMoreover, atomic increments commute with each other. Therefore, every ac\u00adtioninthe second versionof inc \nis both a right and a left mover. Consequently, the entire loop is reducible to a single atomic action. \nNow, using purely sequential reasoning techniques and by writing an appropriate invariant for the loop \nin add, we perform abstraction on this atomic action. The resulting atomic action is semantically equivalent \nto the body of the version of add in Figure 3(c). 2.1 A device cache We have illustrated abstraction \nand reduction using a collection of small examples. We now demonstrate the symbiotic nature of these \ntwo techniques using a larger and nontrivial example. In the following, in order to make it easier to \nfollow the proof steps, sentences that constitute a proof step are indicated by (S1), (S2), etc. Device \ncache operation: Procedure Read in Figure 4 reads a number of bytes from a device s physical storage, \nmodeled by the variable device. Client threads call Read to request to read (up to) size bytes from device \ninto the output parameter buffer, starting from index start in the device. Read returns the number of \nbytes it was able to read (possibly less than size) from the device in the output parameter bytesread. \nIn order to make subsequent requests to Read faster, the implementation of Read caches the bytes read \ninto an (unbounded) memory buffer cache. In this example, the type int is the set of non-negative integers. \nThe reader can informally assume that all integers are initialized to 0. Initial states will be treated \nmore formally later in the paper. The variables device, cache and buffer are all integer-indexed maps \nstarting from 0. The variable currsize stores the number of bytes from the de\u00advice that are already available \nin the cache. When there are enough bytes in the cache (lines 2-4), Read jumps to COPY TO BUFFER to copy \nthe contents of cache to buffer (lines 18-22). In this case, the number of bytes read is the same as \nthe number of bytes requested by the client. If the cache does not contain all the bytes requested, there \nare two possibilities: If in line 5 a thread t observes newsize > currsize, this means another thread \nis in the process of copying bytes from the device to the cache as will be explained below. In this case, \nt can read the portion of cache between 0 and currsize (lines 18-21). If bytesread < size at return, \nthe client thread t may retry later for the rest of the bytes.  Otherwise, thread t reads the missing \nbytes from the device into the cache. The driver allows only one thread to read from the device and write \nto the cache. This is implemented by the following synchronization policy. The .rst thread that at\u00adtempts \nto read from the device sets the variable newsize to start + size (line 9), and then performs the actual \nread by jumping to READ DEVICE (line 11). While newsize is greater than currsize, no other thread attempts \nto access the section of cache and device between currsize and newsize.As ex\u00adplained above, during this \nperiod, another thread can read the portion of cache between 0 and currsize. The thread that jumped to \nREAD DEVICE sets currsize to newsize (lines 14-17), which makes the recently read bytes from the device \navailable for further calls to Read by all threads. Synchronization mechanisms: This example uses two \ndiffer\u00adent mechanisms to synchronize the client threads. A lock protects accesses to currsize and newsize, \nthereby allowing us to reduce the code blocks from lines 1-10 and lines 14-17 into atomic actions (S1). \nThis increased granularity of atomicity also allows us to introduce and prove the invariant currsize \n<= newsize.  Only one thread at a time is allowed to update newsize,make the jump to READ DEVICE from \nline 10, and update cache. We capture this synchronization by introducing an abstract lock variable al \nthat is associated with the locking predicate currsize < newsize.  Acquiring al: The lock al is available \nto a thread in a state where currsize == newsize. A thread increases newsize (line 9), making the locking \npredicate true, to acquire al.  Releasing al: A thread sets currsize to newsize (line 15 16), making \nthe locking predicate false to release al.   Thus, the lock al is acquired just before the jump to \nREAD DEVICE and held during the execution of lines 11 17. Speci.cation: We would like to verify that \nif the input pa\u00adrameters of Read satisfy 0 <= start &#38;&#38; 0 <= size at entry, then the output parameters \nof Read satisfy (forall x:int. start <= procedure Read (start : int, size : int) procedure Read (start \n: int, size : int) returns (buffer : [int]int, bytesread : int) returns (buffer : [int]int, bytesread \n: int) { { var i, j, tmp : int; var i, j, tmp : int; 1 acquire(); A [if (*) { 2 i := currsize; havoc \ni, size; 3 if (start + size <= i) { assume (size == 0 || start + size <= currsize); 4 release(); goto \nCOPY_TO_BUFFER; goto COPY_TO_BUFFER; 5 } else if (newsize > i) { } else { 6 size := (start <= i) ? i \n-start : 0; i := currsize; 7 release(); goto COPY_TO_BUFFER; assume newsize <= i &#38;&#38; i < start \n+ size; 8 } else { newsize := start + size; 9 newsize := start + size; goto READ_DEVICE; 10 release(); \ngoto READ_DEVICE; }] } // end if READ_DEVICE: READ_DEVICE: 11 while (i < start + size) { B while (i < \nstart + size) { 12 cache[i] := device[i]; C [assert currsize <= i; cache[i] := device[i]]; 13 i:=i +1; \nD i := i + 1; } } 14 acquire(); E [tmp := newsize; 15 tmp := newsize; currsize := tmp]; 16 currsize := \ntmp; 17 release(); COPY_TO_BUFFER: COPY_TO_BUFFER: 18 j:=0; F j := 0; 19 while(i < size) { G while (j \n< size) { 20 buffer[j] := cache[start + j]; H [assert start + j < currsize; buffer[j] := cache[start \n+ j]]; 21 j:=j +1; I j := j + 1; } } 22 bytesread := size; return; J bytesread := size; return; } } \nFigure 4. Original device cache program x &#38;&#38; x < start + bytesread ==> buffer[x] == device[x]) \nat exit. This task is straightforward if Read executes without any in\u00adterference. Our goal in the following \nis to convert the entire body of Read into an abstract atomic action that is strong enough to verify \nthe aforementioned property. Veri.cation: We will argue that lines and actions that access shared variables \nare of the desired mover types in order to allow us to convert the body of Read into an atomic action. \nWe will argue that the update of cache at line 12 is a right mover (S2),and  the read of cache at line \n20 is a left mover (S3)  Note that buffer is a local variable and device is immutable, so the only possible \ncon.ict among these lines could be due to the variable cache. The execution of line 12 is not simultaneously \nenabled in two different threads because of the lock al. The argument that line 12 does not con.ict with \nline 20 requires the introduction of assertions just prior to the execution of these actions. These assertions \nare shown on lines C and H in Figure 5; they are checked atomically with the execution of the action \n(shown by surrounding square brackets). These assertions capture why the update and read of cache do \nnot con.ict: from a state in which both hold, the update and read access different indices of cache! \nThe transformation of line 12 and line 20 in Figure 4 into line C and line H in Figure 5 introduces a \ndependency on the variable currsize because of the assertion annotations. Therefore, we must also argue \nthat lines C and H commute to the right and left respectively of the action updating currsize on line \nE. Due to the abstract lock al, line C is not enabled simultaneously with line E. Also, line H commutes \nto the left of line E when executed from a state satisfying the already proved invariant currsize <= \nnewsize. We now show how to transform the atomic action in lines 1\u00ad10 of Figure 4 into a right mover. \nIn its original form, this action Figure 5. Transformed device cache program does not commute to the \nright of the atomic action in lines 14-17. To see this, consider the scenario where lines 1-10 are executed \nby a thread t from a state in which currsize < (start + size) < newsize; consequently size is reduced \nto either 0 or currsize\u00adstart. Suppose that this were followed by another thread u execut\u00ading the action \nin lines 14-17 and increasing currsize to newsize. Clearly, this scenario is not equivalent to one in \nwhich u executes .rst followed by t, since in this case t would not modify size. To transform lines 1-10 \ninto a right mover, we perform two ab\u00adstractions to get the action shown on line A in Figure 5. We .rst \nab\u00adstract the check at line 5 of Figure 4 to a non-deterministic decision and enable lines 6-7 to run \neven when currsize == newsize (S4). This causes a thread to return fewer bytes than size even though \nit could jump to READ DEVICE and read all bytes it needs from device. The second abstraction is to update \nthe variable size nondetermin\u00adistically to a value less than or equal to that assigned by the orig\u00adinal \nprogram in the case when control jumps to COPY TO BUFFER (S5). This allows the code to read fewer than \nthe number of bytes available to it in the cache. This abstraction is justi.ed since the additional behavior \nin the transformed Read (Figure 5) might have occurred if the call had taken place while an update of \ncache was being executed by another thread. This abstraction is also safe from the point of view of proving \nthat the buffer contains a valid snap\u00adshot of device for the .rst bytesread bytes. The transformed Read \nin Figure 5 can now be reduced into an atomic action, because lines A-D are right movers and lines F-J \nare left movers (S6). Fi\u00adnally, we can prove the relevant condition about output parameters at exit using \npurely sequential reasoning techniques. We introduce and verify the invariant (forall x:int. 0 <= x < \ncurrsize ==> cache[x] == device[x]) once the code in Fig\u00adure 5 has been converted into a single atomic \naction (S7). A proof of Read in Owicki-Gries at the line-level granularity would require all introduced \ninvariants to be included in the annotations at several Atomic : a ::= .tt Stmt : s ::= a | .() | sO \n| s ; s | s D s | s l s Figure 6. The language lines. In addition, it would require facts about global \nvariables es\u00adtablished at assignments and conditional checks to be propagated through the annotations \nin subsequent lines. Our method can han\u00addle this example much more simply by introducing invariants such \nas those in (S7) when atomic blocks are large enough.   3. Preliminaries A program P is represented \nby a tuple P =(Var ,Main,Body). We refer to elements of the tuple using dotted notation (e.g., P.Main) \nand omit reference to P when clear from the context. Var is the set of uniquely-named global variables. \nThe program heap is modeled by using a map similarly to ESC/Java [11] and Boogie [2]. The statement Main \nis the body of the program. Body is a map from procedure names to statements to be executed when a procedure \nis called. Figure 6 shows the language that we use to describe pro\u00adgrams formally. In our language, we \nexpress each atomic statement (Atomic) with a gated action .t t. The store predicate . is the gate of \nthe action. If executed at a state that violates ., the action goes wrong . From states that satisfy \n., the set of state transitions allowed by this gated action are described by transition predicate t. \nStatements (Stmt) are either procedure calls (.()) or are built from atomic statements by sequential \n(;)orparallel (I) composition, non-deterministic choice (D) or looping (sO). We model argument passing \nusing global variables, so a call to .is simply denoted by .(). We sometimes express transition predicates \nin the compact no- V ' tation: (t)M = t . (x=x)where M is list of vari- x.Var\\M ables written by a and \nx' is a variable that refers to the value of x after the transition described by t is taken. For example, \nwe can express the Boogie statements assume e, assert e and x:= e (where e is an expression and x is \na variable) with the gated ac\u00ad ' tions true t(e)\u00d8, e t(true)\u00d8 and true t(x=e)x , respectively. Atoms(s), \nthe set of all gated atomic actions in s, is computed recursively as follows: Atoms(.tt) = {.tt} Atoms(sO \n) = Atoms(s) Atoms(s1; s2) = Atoms(s1) . Atoms(s2) Atoms(s1Ds2) = Atoms(s1) . Atoms(s2) Atoms(s1ls2) \n= Atoms(s1) . Atoms(s2) Atoms(.()) = \u00d8 We extend the de.nition of Atoms to programs as follows: S Atoms(P)=Atoms(Main). \nAtoms(Body(.)) ..Dom(Body) 3.1 Execution semantics The program state evolves over time as threads execute \ngated ac\u00adtions. Each thread has a unique identi.er from a set Tid. tmain is a distinguished thread id \nthat runs the statement P.Main, i.e., the program body. The operational semantics of our programming \nlan\u00adguage is given in Figure 7. To represent the fact that a statement s . Stmt is executed by a thread \nt . Tid,we use the dynamic statement t : s. The state\u00adments skip and error are also dynamic statements \nrepresenting a program that has terminated normally and with an assertion viola\u00adtion, respectively. The \nsemantics only allows dynamic statements (Dynamic) to be executed, so, before being executed, each static \nstatement must be labeled by the id of the executing thread. The gate and the transition predicate of \na gated action may refer to the current thread id through the special variable tid .Var . When the gated \naction .tt is being executed by a thread t . Tid, tis sub\u00adstituted for tid in both . and t. To represent \nthe creation of new threads by the parallel composition statement, we make use of two functions left,right \n:Tid . Tid. We assume that left and right together de.ne a tree over Tid with tmain being the root of \nthe tree. A state of P is a tuple (s,d)where sis the current store, and d is the dynamic statement representing \na partially executed program. We express a feasible state transition derivable from the operational semantics \nin Figure 7, as (s1,d1).(s2,d2),where d1 and d2 are dynamic statements. We denote the transitive closure \nof . with . *. The program starts with an arbitrary store and the dynamic statement tmain :P.Main. Let \n.[t/tid] and t[t/tid] be formulas obtained by substitut\u00ading t for tid in . and t, respectively. s F .[t/tid] \nstates that .[t/tid]is satis.ed given the valuation of the variables in s.Sim\u00adilarly, (s1,s2)Ft[t/tid]states \nthat t[t/tid]is satis.ed given the valuation of unprimed variables in s1 and primed variables in s2. \nWe will denote with (.tt)[t]the gated action .[t/tid]tt[t/tid]. The semantics of a gated action t : .tt \nis giveninthe rules ATOMIC and ERROR. We call transitions obtained from ATOMIC and ERROR atomic transitions \nof the program. ATOMIC states that if the current store satis.es .[t/tid], the store is modi.ed atomically \nconsistent with t[t/tid] and the current dynamic statement be\u00adcomes skip.Otherwise, ERROR states that \nthe execution goes wrong, where the store does not change but the statement becomes error. An execution \nof s is a sequence of feasible transition steps (s1,t : s) . (s2,d2) . \u00b7\u00b7\u00b7 . (sn,dn),where t is a thread \nid from Tid. An execution is terminating if it ends in skip or error. An execution is blocking if it \ncannot be extended. Clearly, any terminating execution is also a blocking execution. An execution succeeds \nif it ends in skip and fails (or goes wrong) if it ends in error. In the following, we de.ne Good(t,s,.)as \nthe set of pre-and post-store pairs associated with succeeding executions of s executed by thread tfrom \nstores satisfying .. Bad(t,s,.)is the set of pre-stores associated with failing executions. Formally, \nGood (t,s,.)= {(s1,s2)|s1 F., (s1,t :s). * (s2,skip)} Bad(t,s,.)= {s1 |s1 F., .s2.(s1,t :s). * (s2,error)} \nA statement s may go wrong from . if there exists a failing run of s from .,i.e. .t . Tid. Bad(t,s,.) \n\u00d8. A program P may = go wrong from . if P.Main may go wrong from .. s|V represents the projection of \ns to the set of variables V . Var . The projections of Good and Bad to V are de.ned as as follows: Good \n|V(t,s,.)= {(s1|V,s2|V)|(s1,s2).Good (t,s,.)} Bad|V(t,s,.)= {s1|V |s1 .Bad(t,s,.)} The de.nitions of \nGood and Bad are also extended to pro\u00adgrams: Good (P,.)and Bad(P,.)are shorthands for Good(tmain , P.Main,.)and \nBad(tmain ,P.Main,.), respectively.  4. The proof method In our approach, a proof of a program is performed \nby a sequence of steps each of which modi.es the current proof context. The proof context consists of \nthe current program P and a store predicate I. A proof context P,I speci.es a set of executions of P \nthat start from stores satisfying I. I not only constrains initial stores from which P is run, but also \nis an invariant guaranteed to be preserved by every atomic transition of P. We give a formal de.nition \nof Iin Section 4.1. A proof step is denoted P1,I1 --+ P2,I2.The core proof rules are given in Figure \n8. (s1,d1) .(s2,d2) Dynamic : d ::= skip |error |t: s |d ; d |d ld E ::= [\u00b7] |E ; d |E ld |d lE ATOMI \nC ERROR FORK s1 F .[t/tid] (s1,s2) F t[t/tid] s F \u00ac.[t/tid] u = left(t) w = right(t) (s1,t: .tt) .(s2,skip) \n(s,t : .tt) .(s,error) (s,t :(s1ls2)) .(s,u : s1lw : s2) EVALUATE EVALUATE-ERROR (s1,t: s1) .(s2,t: \ns2) s2 = error (s1,t: s) .(s2,error) LABEL (s1,E[t: s1]) .(s2,E[t: s2]) (s1,E[t: s]) .(s2,error) (s,t \n:(s1 ; s2)) .(s,t : s1; t: s2) PROC-CALL SEQUENTIAL Body(.)= s CHOOSE-FIRST CHOOSE-SECOND (s,skip ; \nt: s) .(s,t : s)(s,t : .()) .(s,t : s)(s,t :(s1 D s2)) .(s,t : s1)(s,t :(s1 D s2)) .(s,t : s2) LOOP-SKIP \nLOOP-ITER JOIN-FIRST JOIN-SECOND (s,t : s O) .(s,skip)(s,t : s O) .(s,t : s ; t: s O )(s,skip lt: s) \n.(s,t : s)(s,t : s lskip) .(s,t : s) Figure 7. The operational semantics P1,I1 P2,I2 AUX-ANNOTATE INVARIANT \nAtoms(P)= {.1 tt11,...,.n tt1 n} .1 tt21 fI ... .n tt2 n fI '' n .tn F I2 .I1 P fI2 a ./Var F (.1 .t11) \n..a..a .t1 ... F (.1 ) ..a..a .tn 22 P,I1 P,I2 P,I P[Var .Var .{a},.1 tt1 1 ..1 tt1 2 ,...,.n ttn \n1 ..n ttn 2 ],I SIMULATE REDUCE-SEQUENTIAL Ifa1 .a2 a2 fI P,Ifa1 : R or P,Ifa2 : L REDUCE-CHOICE P,I \n P[a1 .a2],I P,I P[a1; a2 .a1 .a2],I P,I P[a1Da2 .a1 .a2],I REDUCE-LOOP P,Ifa : m m .{R,L} .tt fI EXPAND-PARALLEL \n F . .t[Var/Var ' ] If(.tt .a) ..tt a3 = a1[left(tid)/tid] a4 = a2[right(tid)/tid] P,I P[aO P,I P[a1la2 \n..tt],I.(a3; a4)D(a4; a3)],I Figure 8. The core proof rules of our method Each proof step is governed \nby a proof rule which either rewrites the program or changes the program invariant. A rewrite of P is \ndenoted P[x. y]where an element x of the program is replaced with another element y of the same type. \nA rewrite may modify Var by adding new variables to it, or may replace statements in Main or Body by \nothers. A program is proved correct by a sequence of proof steps P, true --+ P1, I1 --+ P2, I2 \u00b7\u00b7\u00b7--+ \nPn, In if the .nal program Pn is one in which all the gated actions are validated.Agated action .tt is \nvalidated by showing that In .. is a valid formula. Since In is an invariant of Pn,noexecutionof Pn starting \nfrom a state in In causes an assertion violation. The soundness theorem (Theorem 1 below) asserts that \nin this case the initial program P cannot go wrong starting from a state in In. Soundness. The following \nlemma states that the application of each core proof rule given in Figure 8 preserves the soundness of \nassertion checking, i.e., failing runs of the original program are preserved after an application of \nany rule. In addition, the new program simulates succeeding runs of the original program from astore \ns as long as the former does not go wrong from s.We give a proof outline for Lemma 1 below. The complete \nproofs are given in our technical report [8]. LEMMA 1 (Preservation). Let P1, I1 --+ P2, I2 be a proof \nstep. Let V=P1.Var and X=P2.Var \\P1.Var . Then the following hold: 1. Bad(P1, .X. I2) . Bad|V(P2, I2) \n2. For each (s1,s2).Good (P1, .X. I2): (a) s1 .Bad|V(P2, I2), or (b) (s1,s2).Good |V(P2, I2)) PROOF SKETCH: \nWe prove that for each execution E1 of P1 start\u00ading from a store s1 in I2 and ending in state s2, there \nexists a wit\u00adness execution E2 of P2 from s1 that leads to s2 or goes wrong. To obtain E2, we provide \nin each case a sequence of local trans\u00adformations to E1. Each transformation is either the swap of two \nadjacent actions justi.ed by their mover types, or the replacement of an action by a more abstract one. \nThe correctness of the exe\u00adcution transformations are justi.ed by the antecedents of the proof rules. \nD The following theorem generalizes the lemma to an arbitrary number of proof steps and states the soundness \nof our proof method. THEOREM 1 (Soundness). Let P1, true --+ \u00b7\u00b7\u00b7 --+ Pn, In be a sequence of proof steps. \nLet V= P1.Var and X= Pn.Var \\P1.Var .If Pn cannot go wrong from In,then P1 cannot go wrong from In and \nGood (P1, .X. In).Good |V(Pn, In). The P1, I1 notation implicitly requires that I1 be an invariant of \nprogram P1. Furthermore, any transformation P1, I1 --+ P2, I2 has the property that I2 .I1. This connection \nbetween I1 and I2 becomes signi.cant while proving Theorem 1. In the rest of this section, we present \nthe proof rules of our system, which are the building blocks of the higher-level proof tactics presented \nin Section 5. We anticipate that most users of QED will reason at the level of these higher-level, combined \nsteps. In this section, we have made an attempt at motivating each proof rule by providing forward pointers \nto Section 5. 4.1 Invariants The second component of the proof context, I, is a store predicate that \nspeci.es the stores from which each execution of P preserves Ifor each atomic transition of P. A gated \naction .tt preserves I, denoted .tt fI, if from stores satisfying I, .tt either goes wrong or preserves \nIat the post-store, i.e., (..t) .(I.I ' ). Here, for a store predicate ., the store predicate . ' is \nobtained by replacing each free occurrence of each v.Var with v ' .Then Iis an invariant of a program \nP, denoted P fI, if all gated actions in Atoms(P) preserve I. Each proof starts from the proof context \nP,true. New program invariants are introduced by strengthening the current invariant by using INVARIANT. \n 4.2 Auxiliary variables The rule AUX-ANNOTATE adds a fresh auxiliary variable a to the program and replaces \neach gated action .i tt1 i in P with a new gated action .i tt2i. We require that .i tt2 i preserves the \ncurrent invariant. The new action t2 i speci.es how a is updated by the gated action (or left unmodi.ed) \nfor each value of a in the current state, but preserves the effect of t1 i on other variables. In particular, \nthe introduction of auxiliary variables cannot cause an action to block at a state sif the original action \ndid not. This proof rule is typically used to annotate gated actions with synchronization information; \nsee Section 5.2 for details. 4.3 Simulation The simulation relation, de.ned in this section, enables \nus to decide when SIMULATE can replace a gated action with another. Let I be a store predicate. .2 tt2 \nsimulates .1 tt1 from I, denoted If.1 tt1 ..2 tt2, if the following two conditions hold: 1. F I..2 ..1 \n 2. F I..2 .t1 .t2  The .rst condition above states that whenever .1 tt1 goes wrong from I, so does \n.2 tt2. The second condition states that whenever .2 tt2 does not go wrong from I, it simulates succeeding \nruns of .1 tt1 from I. Common ways of rewriting a gated action .1 tt1 to .2 tt2 are strengthening the \ngate, i.e., .2 ..1 and t1 = t2, or weakening the transition predicate, i.e., t1 . t2 and .1 = .2. In \nboth cases, the new action is an abstraction of the old one. While the former adds extra failing behaviors \nto the gated action by adding new assertions for the pre-store, the latter adds extra succeeding behaviors. \nBorrowing assertions. A special case of the simulation proof rule is the annotation of gated actions \nwith assertions. When a gated action .1 tt1 is annotated with an assertion ., it is transformed to the \ngated action (.1 ..) tt1. We use assertions to enable later applications of the simulation rule. Consider \nthe case where we would like to replace .1 tt1 with .2 tt2 but it is not the case that If.1 tt1 ..2 tt2 \nbecause the invariant Iis not strong enough. Instead of having to strengthen Iby reasoning about the \nentire program, we simply express our belief about the program state when .1 tt1 is reached using an \nassertion .. The new simulation check If(.1 ..) tt1 .(.2 ..) tt2 is more likely to pass. We often insert \nassertions to express beliefs about synchronization mechanisms; see Section 5.2 for details. Our method \nallows program transformations to proceed while the veri.cation of the introduced assertion is deferred \nto later proof stages. Borrowing assertions in this manner (instead of proving them right away) is a \npowerful tool, since in the transformed pro\u00adgram with larger atomic actions, validation of assertions \ncan often be done automatically. Validating gated actions. Another special case of SIMULATE is using \nthe invariant to validate gated actions, i.e. proving their their gates (assertions) are valid. We use \nthe following derived rule RELAX for the purpose of proving and eliminating the gates. RELAX F I.. P,I \nP[.tt .true tt],I A program is proved correct when all the gates of Atoms(P) are replaced with true. \nThe rule emphasizes the fact that validating assertions in .tt, in essence, is nothing more than sequential \nrea\u00adsoning where the invariant is used as the pre-condition to discharge the assertions in sequential \ncode represented by t. Here, .is ob\u00adtained by propagating the assertions in the sequential code to the \nbeginning of the code block using weakest preconditions [7].  4.4 Reduction P,Ifs : m LEFT-MOVER a= \n.tt .t,u.Tid..\u00df.Atoms(P): (t= u) .(If(\u00df[u] .a[t]) (a[t] .\u00df[u])) P,If.tt : L RIGHT-MOVER a= true t(..t) \n.t,u.Tid..\u00df.Atoms(P): (t= u) .(If(a[t] .\u00df[u]) (\u00df[u] .a[t])) P,If.tt : R Figure 9. Right and left movers \nOur reduction rules are based on Lipton s theory [18]. Figure 9 shows how we determine whether an action \nis a right or a left mover using the judgment P,Ifs : m. These judgments are used for reducing sequential \ncomposition, nondeterministic choice, loops, and parallel composition. The rules LEFT-MOVER and RIGHT-MOVER \nde.ne the mechanism by which we label atomic actions in the program as a left or right mover. These rules \ncorrespond to performing a pairwise commu\u00adtativity check with all other atomic actions and are performed \nau\u00adtomatically by QED.The rule LEFT-MOVER is straightforward; an action .tt is a left mover if it commutes \nto the left of every action in the program. The rule RIGHT-MOVER is similar; however, to es\u00adtablish whether \n.tt is a right mover, we check the commutativity of true t(..t) to the right of actions in the program. \nWe have introduced this asymmetry deliberately to increase the applicabil\u00adity of the RIGHT-MOVER rule \nin the case when the program contains blocking actions. To see why this was needed, observe that an ac\u00adtion \n.tt that may go wrong cannot commute to the right of an action \u00dfthat blocks. Sequential composition. \nThe REDUCE-SEQUENTIAL makes use of the .operator de.ned in terms of wp(t,.), the (sequential) weakest-precondition \nof . with respect to the transition t.It is applicable to a1; a2 if either a1 is a right mover or a2 \nis a left mover. P1, I1 P2, I2 INLINE-CALL Body(.)= s P, I P[.() .s], I REDUCE-RECURSIVE M= {.1, ..., \n.n}is closed under call There exists an annotation function Annot where for each .i .M: Annot(.i)=(mi,.i,ti) \n..i .M: .i tti fI P, I, Annot fBody(.i): mi P, I, Annot f{.i,ti}.i Finalizable(.i,.i) P, I P[Body(.1) \n..1 tt1,..., Body(.n) ..n ttn], I Figure 10. Rules for procedures .1 tt1 ..2 tt2 =(.1 .wp(t1,.2)) t (t1 \n.t2) ' wp(t, .)= .Var .t .. ' '' t1 .t2 = .Var .t1[Var '' /Var ' ] .t2[Var '' /Var ] Nondeterministic \nchoice. The REDUCE-CHOICE makes use of the .operator de.ned as follows. .1 tt1 ..2 tt2 =(.1 ..2) t (t1 \n.t2) Loops. REDUCE-LOOP reduces a loop aO to a single gated action .tt . Intuitively, . is a predicate \nthat is true at the beginning of the loop and t speci.es a relation between the beginning of the loop \nand the end of any iteration. The conditions needed to apply this rule are the following: 1) .tt preserves \nthe current invariant, 2) the body of the loop is a right or left mover, and 3) .tt simulates by zero \nor more iterations of the loop body a. Parallel composition. We reduce parallel statements composed of \ngated actions to sequential compositions using three rules. The .rst rule EXPAND-PARALLEL eliminates \nthe parallel composition by explicitly enumerating the two possible interleavings of the gated actions. \nThefollowingderivedrules REDUCE-PARALLEL-I/II exploit the gated actions being right/left movers to directly \neliminate the parallel composition. REDUCE-PARALLEL-I P, Ifa1 : L or P, Ifa2 : R a3 = a1[left(tid)/tid] \na4 = a2[right(tid)/tid] P, I P[a1la2 .a3; a4], I REDUCE-PARALLEL-II P, Ifa1 : R or P, Ifa2 : L a3 = a1[left(tid)/tid] \na4 = a2[right(tid)/tid] P, I P[a1la2 .a4; a3], I  4.5 Procedures In this section, we show how our proof \nmethod deals with proce\u00addures. The rules are given in Figure 10. The rule INLINE-CALL is particularly \nsimple; it simply inlines Body(.) at the call site. All the other rules discussed previously can be used \nto transform the body of a procedure iteratively making it smaller and simpler. In the limit, the procedure \nbody could be transformed to a single atomic action. Once the procedure body has been simpli.ed enough, \nit can be inlined at call sites without any signi.cant increase in the pro\u00adgram size. The rule INLINE-CALL \nis inadequate if there is recursion in the program. In this case, we use the proof rule REDUCE-RECURSIVE, \nwhich takes a set Mof procedures that are closed under the call relation and gives a mechanism whereby \nthe body of each method in Mis replaced by a gated action. To break circular dependencies, REDUCE-RECURSIVE \nrequires an annotation function Annot that P, Ifs : m BOTH-MOVER SUP-MOVER P, Ifs : L P, Ifs : R P, Ifs \n: nn m P, Ifs : B P, Ifs : m CALL-MOVER ATOMIC-NON-MOVER Annot(.)=(m, ., t) P, If.tt : A P, If.() : m \nCOMBINE-MOVERS LOOP-MOVER P, Ifs1 : m P, Ifs2 : n .{; , D, l} P, Ifs : m OO P, Ifs1 s2 : m n P, Ifs : \nm O De.nitions of m; n, mDn, mln,and m O m; n BRLA mDn BRLA mln BRLA m B BRLA B BRLA B BRLA B B R RRAA \nR RRAA R RR --RR LL -L -L LALA L L -L -LL AA -A -A AAAA A A ---A - Figure 11. Rules for deciding movers \nprovides a speci.cation for each procedure .i in Mand for each loop occurring in a .i as explained later \nin this section. To de.ne the procedure speci.cations provided by Annot,we must .rst introduce two new \nmover types atomic non-mover (A) and both-mover (B). The partial order .de.nes a lattice over movers \nas speci.ed as follows: B .L .A and B .R .A Figure 11 extends the judgment P, Ifs : m to all statements \nand all mover types [12]. The symbol -indicates that no conclusion about the mover type can be reached \nand rules in Figure 11 cannot be applied. The rule CALL-MOVER allows us to use the mover type annotation \nfor the body of a procedure at the call site. We call the statement s atomic if P, Ifs : m is proved \nfor any mover m .A. The function Annot is de.ned as follows: For each .i .M, Annot(.i) returns a tuple \n(mi,.i,ti), where mi .{A, L, R, B}speci.es the mover type of the body of .i,and .i and ti are the pre-condition \nand the post\u00adcondition of .i, respectively. If the rule REDUCE-RECURSIVE succeeds, then it is sound to \nreplace the body of each procedure .i .Mwith .i tti. While .i is a store predicate for the call point \nof the procedure, ti is a transition predicate and speci.es a relationship between the call and return \npoints of the procedure. For soundness of the REDUCE-RECURSIVE rule, we require that every execution \nof .i from .i can be extended to a blocking execution. We express this requirement using the predicate \nFinalizable(.i,.i). We currently assume that the programmer has ensured that this requirement is met \nand do not provide a mechanical check for it in our tool. This condition is similar to a termination \nrequirement, in fact, termination is a suf.cient condition for being .nalizable. We leave the mechanization \nof this check to future work. For each loop s O in the body of a procedure .i .M, Annot(s O) returns \na store predicate .. . is a loop invariant O for s . The above speci.cations are written only for loops \nand proce\u00adduresthatareprovedtobeatomicasrequiredby REDUCE-RECURSIVE. Therefore, the problem of deriving \nthese speci.cations is the same as providing loop invariants and procedure speci.cations for se\u00adquential \ncode. Given an annotation function Annot, REDUCE-PROCEDURE checks each procedure .i in three phases. \nFirst, it checks that the speci.cation of .i indicates an action that preserves the invari\u00adant. In the \nrule, this is described by the condition .i tti fI. Second, it checks that given the mover types for \nprocedures in Annot, the body of .i in P conforms to its mover type pro\u00advided by Annot. In the statement \nof the rule, this is expressed with the condition P, I, Annot f Body(.i): mi.The rules in Figure 11 are \nused in this phase; for brevity, we have elided the implicit argument Annot to all the judgments. Third, \nit veri.es that the body of .i implements the speci.cation described by the pre-condition .i and the \npost-condition ti. This is described by the condition P, I, Annot f{.i,ti}.i. Formally, P, I, Annot f \n{.i,ti}.i if for all t . Tid, the following two conditions hold: (1) (s1,s2) F ti for all (s1,s2) . Good(t, \nBody(.i),.i),and (2) Bad(t, Body(.i),.i)= \u00d8. The third requirement described above is veri.ed by generating \na veri.cation condition (VC) and checking its validity using an au\u00adtomatic theorem prover. Since the \nbody of .i is atomic, checking whether .i implements its speci.cation requires sequential reason\u00ading \nonly. We use a VC generation technique based on weakest pre\u00adconditions, similar to that implemented in \nthe Boogie veri.er [2]. To handle the use of the parallel composition operator within pro\u00adcedure bodies, \nwe de.ne the weakest precodition of s1Is2 with re\u00adspect to a post-store . as follows: wp(s1ls2,.)= wp(s1[left(tid)/tid],wp(s2[right(tid)/tid],.)) \nNotice that the weakest precondition for the parallel composition is similar to the one for the sequential \ncomposition [2]. Since s1Is2 is part of a procedure body that has been proved to be atomic, s1Is2 is \natomic as well. The correctness of the weakest precondition of parallel composition crucially depends \non this observation.  5. Implementation with high-level tactics The proof rules introduced in Section \n4 are low-level rules that are the building blocks of proofs. In this section, we will present higher-level, \nmore intuitive-to-use proof tactics: coarser-grained proof rules built out of lower-level ones. Figure \n12 summarizes the tactics, their usage and the low-level rules justifying the application of the tactic. \nBy construction, each tactic preserves the soundness of assertion checking. We implemented our proof \nmethod in an interactive tool called QED. QED uses the Boogie framework [2] as its front end and forwards \nthe validity checks to the Z3 SMT solver [6]. Our tool accepts as input a multithreaded program written \nin an extension of the Boogie programming language [2] and a proof script. The transformed program after \nthe application of each proof rule or tactic is available for the user to examine. If required, the tool \nautomatically generates the veri.cation conditions necessary to prove the antecedents of proof rules \nand checks them using Z3. The commands are rejected if this check fails. Proof strategy. We view a proof \nof a concurrent program as a sequence of steps, each applying a tactic presented in this section. We \nhave found the proof strategy sketched below to be a good start: while exist unveri.ed assertions : while \nnot done { do reduction with reduce stmt/loop/proc } eliminate assertions with check if exist unveri.ed \nassertions : repeat { do abstraction with abstract or mutex } repeat { introduce a speci.cation with \ninvariant, annot } In each iteration we .rst apply reduction and then verify as\u00adsertions. When there \nare still unveri.ed assertions, we do abstrac\u00adtion and introduce speci.cations, which allows reduction \nto obtain coarser atomic actions at the next iteration. In the following sec\u00adtions we elaborate on the \noperations of the tactics referred to above and give examples of how these tactics are used during the \nproof in Section 2.1. 5.1 Introducing invariants and speci.cations The tactic invariant . introduces \na new invariant into the proof context. The current invariant Iis replaced with the conjunction I... \nThe tactic fails to change the invariant if any gated action in Atoms(P) does not preserve I... The tactics \nannot pre, annot post and annot inv allow us to introduce speci.cations for loops and procedures. The \nspeci.ca\u00adtions can be introduced partially, where newly introduced spec\u00adi.cations are conjoined with \nthe existing speci.cations. This al\u00adlows the annotation function referred to in Section 4.5 to be de\u00ad.ned \npartially while the proof progresses. While annot pre ., . adds a new pre-condition, an assertion, ., \nannot post ., t adds a new post-condition t to the speci.cation of procedure .. The tactic annot mover \n., m (needed for REDUCE-PROCEDURE) speci.es that the body of . is of mover type m. The tactic2 annot \ninv s O,. adds a loop invariant . to the speci.cation of the loop s O . Example: The speci.cation for \nRead in Figure 4 is introduced in step S7 by the tactics annot pre Read,0 <=start &#38;&#38;0<= size \nand annot post Read,(forall x:int. start <= x &#38;&#38; x < start + bytesread ==> buffer[x] == device[x]). \n 5.2 Abstraction Abstraction is applied by adding either extra transitions or extra assertions to a \ngated action. We use the abstract tactic for the former by having a gated action read or write a nondeterministic \nvalue from/to a variable. The tactic mutex is used for the latter to infer extra assertions to the gated \nactions using synchronization information. Read and write abstractions. A read abstraction is performed \nby the tactic abstract read x,.tt . It makes the gated action .t t read a nondeterministic value from \nx at the beginning of the action thus making the operation of the action independent of the initial value \nof x when its execution starts. For this purpose, abstract read x,.tt replaces the given action .tt with \nthe following: P, I --+ P[.t (t )M ..tt], I j .t (.x.t )M if x .M t = ' .t (x = x ..x.t )M if x ./M A \nwrite abstraction is performed through the tactic abstract write x,.tt . It makes the gated action .tt \nwrite a nondeter\u00administic value to x at the end of the action. For this purpose, abstract write x,.tt \nreplaces the given action .tt with the following: 2 In our implementation, we assign each statement a \nunique label . The tactics that require a statement as a parameter are given the label of the statement. \n Tactic Usage Low-level rules invariant . Add a new invariant to the proof context. INVARIANT annot pre \n., . Introduce a pre-condition speci.cation to a procedure. annot post ., t Introduce a post-condition \nspeci.cation to a procedure. annot mover ., m annot inv sO,. Specify that the body of procedure . is \nof mover type m. Introduce a loop invariant speci.cation to a loop. reduce stmt reduce loop aO ,. t t \nApply reduction iteratively on the program body and procedure bodies. Reduce the loop to its previously \ngiven speci.cation. REDUCE-SEQUENTIAL/CHOICE/PARALLEL REDUCE-LOOP reduce proc .1, ..., .n Reduce the \nprocedures to their previously given speci.cations and do inlining. REDUCE-PROCEDURE, INLINE-CALL inline \n. Inline the body of the procedure . at all call sites. INLINE-CALL abstract read x,. t t Abstract the \nvalue of x at the entry of the action .tt. SIMULATE abstract write x,. t t Abstract the value of x at \nthe exit of the action .tt. SIMULATE assert ., . t t Add new assertion by strengthening the gate of the \naction .tt SIMULATE mutex f, x1, ..., xn Add assertions for a mutual exclusion access policy for variables \nx1, ..., xn AUX-ANNOTATE, INVARIANT, SIMULATE check . Validate assertions in the procedure body using \nsequential analysis and I. RELAX Figure 12. The high-level proof tactics.  P,I --+ P[.t(t)M ..tt],I \nt = (.x ' .t)M.{x} By the de.nition of simulation, both read and write abstractions are sound by construction. \nWe do not allow abstractions that violate the program invariant I. Example: We do two read abstractions \nin our running example. First, in step S4, we abstract the read of newsize for the action corresponding \nto the branch of the if statement at lines 5-7 of Figure 4. This allows Read to take this branch even \nthough newsize ==currsize holds, as a result, reading from the cache the available bytes (fewer than \nthe initially requested size) and returns them, although it could have fetched all the requested bytes \nfrom device. Second, in step S5, we abstract size at the beginning of the ac\u00adtion that spans lines 3-7 \nof Figure 4 (after reducing the .rst two branches of if to a single atomic action by REDUCE-CHOICE). \nThis allows Read to return fewer bytes than the original size.The as\u00adsumption (size == 0 || start + size \n<= i) obtained from the condition of if guarantees that the abstraction leaves size still in the safe \nbounds. These two abstractions allow us to prove that the branches of if are all right-movers and reduce \nthe entire statement to the one given at line 1 of Figure 5. Notice that, neither abstraction breaks \nthe speci.cation of Read and each corresponds to a behavior that could have occurred in a different interleaving. \nD Adding assertions. We use the tactic assert .,.tt to add the assertion p to the gated action to yield \n...tt. In the following, we describe a tactic that, using hints about mutual exclusion syn\u00adchronization \nin the program, infers appropriate assertions for gated actions. The tactic mutex f,x1,...,xn takes a \nstore predicate f and a set of program variables x1,...,xn . Var . The hint communicated through this \ntactic is that fspeci.es a mutual exclusion condition that holds at all the program states from which \na gated action reads from or writes to a variable xi. In this regard, f can be thought of as a high-level \ndescription of a locking discipline that can be implemented by any program variables. The tactic automatically \nadds to the program a fresh auxiliary variable a with domain Tid .{0}3, and generates the following invariant \nover Var .{a}: I =(a =0). f I associates the auxiliary variable with f. Intuitively, fis true whenever \nit is acquired and a stores the id of the thread that acquired f,and f is false whenever f is not acquired \nby any 3 We assume that 0is not an element of Tid, and represents no thread . thread and a stores 0. \nThe operation of the tactic includes 1) by AUX-ANNOTATE, adding a to the set of program variables, 2) \nby INVARIANT, adding the invariant I, and then 3) by annotating gated actions as follows: 1. Replace \nevery gated action .tt such that F (. . t) . (\u00acf.f ' ) with .t(t .(a ' = tid)). 2. Replace every gated \naction .tt such that F (..t) .\u00acf ' with (..(a = tid)) t(t .(a ' =0)). 3. Replace every gated action \n.tt such that F (..t) . (f. f ' ) with .t(t .(a ' = a)). 4. Replace every gated action .tt that read \nfrom or write to the variable xi with (..(a = tid)) tt.  The above operations are justi.ed by the rules \nAUX-ANNOTATE and SIMULATE. The assertion a = tid is the key to showing that actions annotated with a \n= tid are movers in later reduction steps because they are non-con.icting. Example: 1. The application \nof mutex in our running example of Sec\u00adtion 2.1 (in step S1) expresses the fact that newsize and currsize \nare protected by a variable lock, which is modi.ed by acquire and release primitives. We used the tactic \nmutex lock == true, currsize,newsize. The assertion a = tid were added to the lines accessing currsize \nand newsize between acquire and release. 2. In order to reduce the code in Figure 5 into one atomic \nac\u00adtion, it is crucial to prove that the lines between READ DEVICE and COPY TO BUFFER commute over each \nother. This enabled the reason\u00ading in S2 and S3. In fact, only one thread can execute these lines. Recall \nthat the synchronization mechanism in Read allows only the thread that establishes currsize < newsize \nto access the device. In order to prove that this is the case and to use this fact in later reduc\u00adtions, \nwe use the tactic mutex currsize < newsize, device.As a result, the assertion a = tid is added to the \nactions spanning the block of code between the labels READ DEVICE and COPY TO BUFFER.  D  5.3 Reduction \nReducing statements. The tactic reduce stmt is used to compute coarser atomicities in the program by \niteratively applying reduction rules in Figure 8. Example: In our running example, we apply reduce stmt \ntwice. The .rst reduction (in step S1), after using the mutex tactic as described in Section 5.2, combines \nbranches of the if statement between lines 3-4, 5-7 and 9-10 of Figure 4 to separate atomic blocks, each \nhaving the code at line 1-2 at the beginning. Then it merges the branches of if to a single atomic action. \nIn addition, the block at lines 14-17 is also reduced to a single action. Figure 5 shows the state of \nthe program at this point. The second reduction (in step S6), using the assertions at lines 3 and 8 reduces \nthe loops as described below and combines the entire body into a single action. D Reducing loops. The \ntactic reduce loop is used to reduce an entire loop to a gated action given with the same tactic. reduce \nloop s O .tt uses REDUCE-LOOP to reason about possi\u00adbility of reducing s O to .tt . Example: The loops \nin our running example at lines B-D and G-I of Figure 5 are reduced to single actions using the tac\u00adtic \nreduce loop. After steps S2 and S3, the body of the .rst loop is a right-mover, and the body of the latter \nis a left-mover. We use the speci.cation (currsize <= i) t( ((i <= i ) &#38;&#38; forall x:int. ((i -1) \n<= x <= start + size)==>(cache [x] == device[x])) )(i,cache) for the .rst loop and (start + i< currsize) \nt( (i <= i ) &#38;&#38; forall x:int. ((i -1) <= x <= size)==>(buffer [x] == cache[start + x]) )(i,buffer) \nfor the latter. Notice that, these gated actions specify the copying from device to cache and from cache \nto buffer properly. In addition, these actions are right-and left-movers themselves, and are used later \nin the reduce stmt tactic. D Reducing procedures. For the cases where the body of a pro\u00adcedure is small, \nwe provide the tactic inline ., which replaces all the calls to . with its body after doing proper substitutions \nfor formals. In other cases, the tactic reduce proc .1, ..., .n is used to eliminate calls to the procedures \n.1, ..., .n, which are closed under call. reduce proc does the checks speci.ed in the rule REDUCE-PROCEDURE \nwhere M = {.1, ....n}, and the existing speci.cations for loops and procedures given as described in \nSec\u00adtion 5.1 de.ne the annotation function Annot. The tactic fails if a procedure or loop does not implement \nits given speci.cation, or the body of a procedure is not of the given mover type. If all the checks \npass, it replaces all the calls to .1, ..., .n with their corresponding speci.cations. Example: Suppose \nthat the loops at lines B-D and F-I of Fig\u00adure 5 are implemented as separate procedures, and the loops \nare re\u00adplaced with the calls copy from device(start,size) and buffer := copy from cache(start,size), \nrespectively. In this case, we could still reduce the bodies of the procedures copy from device and copy \nfrom cache, the loops, to single actions by reduction. In this case, the speci.cations to be inlined \nat the call sites in Read will be similar to the loop speci.cations given above while describing reduce \nloop on Read. D  6. Experience In this section we present our experience with several benchmark algorithms4. \nWe were able to prove the benchmarks, except for the non-blocking stack, using our tool QED. All the \nproof steps driven by high-level tactics were fully mechanized; QED required only a few seconds to .nish \neach proof. We have generated and veri\u00ad.ed the veri.cation conditions for the non-blocking stack manu\u00adally; \ncurrently, we are trying to mechanize this proof as well. Our experience with the benchmarks is summarized \nin the following conclusions: Our proofs did not require complicated global invariants that capture possible \ninterference at each interleaving point. Instead, we attempted to achieve the correct level of atomicity \nby rea\u00adsoning locally about the effect of synchronization on individual 4 We will use pseudocode for \nbrevity; the veri.ed versions of the benchmarks in the Boogie programming language can be found at: http//home.ku.edu.tr/~telmas/popl09bench.tar.gz \nactions. The iterative use of reduction and abstraction was cru\u00adcial in this endeavor. The benchmarks, \nwhile operating with .ne-grained concur\u00adrency, ensure a coarse level of atomicity through a variety of \nsophisticated synchronization protocols. We were able capture these protocols with few uses of abstraction \nthrough the assert, mutex and rwlock tactics.  In many benchmarks, after aggressive use of reduction \nand ab\u00adstraction, the atomic blocks obtained were large enough so that global program invariants stated \nfor the transformed program were almost as simple as invariants for sequential programs.  In the remainder \nof this section, we discuss the proof of each benchmark at a high level. 6.1 Purity benchmarks apply_f() \n1 var x, fx : int; 2 acquire(m); 3 x := z; 4 release(m); 5 while(true) { 6 fx := f(x); 7 acquire(m); \n8 if(x == z) { 9 z := fx; release(m); 10 break; 11 } else { 12 x := z; release(m); 13 } 14 } Figure \n13. Optimistic concurrency control using transaction retry We veri.ed the examples in [10], thereby demonstrating \nthat our approach generalizes existing work on enforcing atomicity through abstractions. We were able \nto handle all the examples by simple applications of abstract read and abstract write on variables that \nare accessed but left unmodi.ed in the pure blocks. Consider an example (Figure 13) from Section 5 of \n[10]. This program reads a shared variable z in one critical section, performs a long com\u00adputation f \non its value, and then attempts to writes back the result into z in another critical section. The tactic \nmutex m==true, z allows the code block 2-4 and both branches of the if statement to be proved atomic. \nThe reads of z at line 3 and 12 are unnecessary for correctness and could therefore be abstracted. Notice \nthat the loop in this example and other examples in this section contain the break statement. Such loops \ncannot be translated directly to the loop statement in Figure 6. Instead of rewriting the programs to \neliminate break, we provide the user tactics to hoist the .nal, successful iteration out of the loop. \nIn this particular example, we hoist the successful if branch at lines 8-10 out of the loop. This is \na sound operation since every terminating execution of the loop contains these lines once at the end. \nWe apply similar transformations to the loops in other examples given below. The rest of the loop after \nthe above transformation does not touch global variables, and also implements a simple skip op\u00aderation. \nThus this portion of the program is a both-mover. Finally we apply reduction and convert the body of \napply f into a single atomic action that assigns f(z) to z. 6.2 Multiset Figure 14 shows a concurrent \nmultiset of integers with InsertPair and Delete operations. The implementation contains an array M of \ncells for storing the multiset elements; the elt .eld of the cell stores the element and the vld .eld \nindicates whether the value stored in elt is valid. Procedures acq and rel acquire and release FindSlot(x:int) \nInsertPair(x:int, y:int) returns r:int returns r:bool 1 for (i=0; i<N; i++) { 1 i := FindSlot(x); 2 acq(M[i]); \n2 if(i== -1) { 3 if (M[i].elt==nil &#38;&#38; !(M[i].vld)){ 3 r := false; return; 4 M[i].elt := x; rel(M[i]); \n4} 5 r := i; return; 5 j := FindSlot(y); 6 } else { rel(M[i]); } 6 if(j== -1) { 7 } r := -1; return; \n7 M[i].elt = nil; 8 r := false; return; Delete(x:int) 9} returns r:bool 10 acq(M[i]); 1 for (i=0; i<N; \ni++) { 11 acq(M[j]); 2 acq(M[i]); 12 M[i].vld = true; 3 if (M[i].elt==x &#38;&#38; M[i].vld){ 13 M[j].vld \n= true; 4 M[i].elt:=nil; M[i].vld:=false; 14 rel(A[i]); 5 rel(M[i]; r := true; return; 15 rel(A[j]); \n6 } else { rel(M[i]); } 16 r := true; return; 7 } r := false; return; Figure 14. The multiset data structure \n(M[i].lck), the lock of cell i.In[9],we proved that InsertPair and Delete are atomic using an abstraction \nmap from M to an ab\u00adstract speci.cation variable S representing the multiset contents. The variable S \nwas required to abstract away from the concrete values of the indices of M into which multiset elements \nare stored. Using our method, we transformed the bodies of InsertPair and Delete implementations to atomic \nactions, indicated by [...], given below. Thus, we replaced a proof based on abstraction map\u00adpings with \na simpler, layered correctness proof that ends with se\u00adquential reasoning. Delete(x:int) InsertPair(x:int, \ny:int) returns r:bool { returns r:bool { var i : int; var i,j : int; [havoc i; [havoc i,j; if(*) { assume \n0 <= i,j < N; r := false; return; if(*) { } else { r := false; return; assume0<=i<N; }else{ assume M[i].elt \n== x; assume M[i].elt == nil; assume M[i].vld; assume M[j].elt == nil; M[i].elt:=nil; M[j].vld:=false; \nassume !(M[i].vld); r := true; return; assume !(M[i].vld); }] M[i].elt:=x; M[i].vld:=true; } M[j].elt:=y; \nM[j].vld:=true; r := true; return; }] } We .rst proved the following speci.cation for FindSlot and inlined \nthe speci.cation at call points in InsertPair. This proof required reasoning similar to the example in \nFigure 13: we hoisted the last iteration of the loop in FindSlot, which allocates an empty slot, outside \nand performed abstractions on the other iterations. The abstractions were done on the elt and vld .elds \nof M[i].Then we used the tactic mutex (M[x].lck==true), M[x].elt, M[x].vld to reduce the code blocks \nbetween calls to acq and rel to atomic actions, including the succeeding iteration in FindSlot. FindSlot(x:int) \nreturns r:int { [havoc r; if(*) { r := -1; return; } else { assume (0 <= r < N &#38;&#38; M[r].elt == \nnil &#38;&#38; !(M[i].vld)); M[i].elt := x; return; }] } The tactic mutex (M[x].elt != nil &#38;&#38; \n!M[x].vld), M[x].elt, M[x].vld allowed us to capture the property that, once FindSlot returns an allocated \nslot, its elt and vld .elds are not modi.ed by other threads. This fact is essential for proving that \nthe atomic actions spanning the blocks 1-3 and 5-9 of InsertPair are right\u00admovers. Then we were able \nto merge the three atomic blocks (lines 1-3, 5-9 and 10-16) in InsertPair into a single atomic action. \nWe also introduced the invariant (forall x:int. 0 <= x &#38;&#38; x < N &#38;&#38; M[x].vld ==> M[x].elt \n!= nil) at the very end of the proof. It is crucial to introduce this invariant only after the body of \nInsertPair has been transformed into a single action because the individual actions in InsertPair do \nnot preserve this invariant. The correctness proof for a sequential multiset implementation would have \nrequired the same simple invariant. 6.3 Non-blocking algorithms Our experience with the following collection \nof non-blocking al\u00adgorithms demonstrates that our method can also be used to verify highly-concurrent \nalgorithms from the literature. rightpush(v) returns r:bool 1 while(true) { 2 k := oracle(right); 3 prev \n:= A[k-1]; 4 cur := A[k]; 5 if(prev.val != RN &#38;&#38; cur.val = RN) { 6 if(k = MAX + 1){r:= false; \nreturn; } 7 if (CAS(&#38;A[k-1], prev, <prev.val,prev.ctr+1>)) 8 if (CAS(&#38;A[k], cur, <v,cur.ctr+1>)) \n9 { r := true; return; } 10 } 11 } Figure 15. Example operation of obstruction-free deque Obstruction-free \ndeque. The double-ended queue (deque) from [14] provides four operations rightpop, rightpush, leftpop, \nand leftpush operation with similar designs, to insert/remove elements to/from both ends of the queue. \nFigure 15 shows the rightpush operation, which inserts an element into the right end of the deque. After \nreading two consecutive array elements in lines 3-4 to local variables prev and cur, rightpush inserts \nthe given value v if CAS operations at lines 7-8 both succeed. Because of the reads at lines 3-4, the \nCAS operations cannot be proved to be movers in the original program. We abstracted these reads using \nthe abstract read tactic, which made prev and cur point to ar\u00adbitrary elements. In addition, we abstracted \nthe write to k at line 2, using the abstract write tactic. This abstraction does not affect the correctness \nof the operation, since the function oracle can re\u00adturn any index from the deque; its purpose is to return \nthe optimum index for having fewer failing attempts. The above proof steps allowed us to reason about \nthe operation of two consecutive CAS operations. We then hoisted the part of the loop body with the successful \nCAS operations out of the loop. Then it was easy, by using the speci.cation of CAS, to prove that the \nremaining part of the loop was a skip operation. By adding the deque invariants indicated in the algorithm \ndescription in [14], we were able to prove that the CAS operations, which were hoisted out of the loop, \nare left-movers in the abstracted program. This gave us an atomic block that contains two CAS operations \nwith the desired behavior. We proved the other operations using a similar approach. Non-blocking stack. \nFigure 16 shows the pop and push oper\u00adations from Michael s non-blocking stack algorithm [19]. The op\u00aderations \nmake use of a hazard pointer per thread in order to solve the well-known ABA problem. [22] gave a proof \nof this algorithm using concurrent separation logic. Since the algorithm uses .ne\u00adgrained concurrency, \nthe proof in [22] required reasoning about in\u00advariants at each interleaving point throughout the code. \nUsing our method, we performed a simpler proof that transforms the bodies of pop and push to the following \natomic actions: pop() push(b:ref) returns r:ref returns r:bool 1vart,n:ref; 1vart:ref,n:int=1; 2 while \n(true) { 2 while(n <= THREADS) { 3 t:=TOP; 3 if (H[n]==b){ 4 if(t == null) break; 4 r := false; return; \n5 H[tid]:=t; 5 }n:=n+ 1; 6 if(t != TOP) continue; 6} 7 n := TL[t]; 7 while (true) { 8 if(CAS(&#38;TOP, \nt, n)) break; 8 t := TOP; 9} 9 TL[b] := t; 10 H[tid] := null; 10 if(CAS(&#38;TOP, t, b)) break; 11 r \n:= t; return; 11 } r := true; return; Figure 16. Michael s algorithm with hazard pointers pop() push(b:ref) \n{ returns r:ref { var m : ref, i : int; var m, n : ref; [assert b != nil; [havoc m,n; if(*) { if(*) { \nr := false; assume (TOP == nil); } else { r := nil; tl[b] = TOP; } else { TOP := b; r := true; assume \n(m != nil &#38;&#38; m == TOP); }] n := tl[m]; TOP := n; r := m; } }] } The proof of the non-blocking \nstack required the application of 11 tactics, in three of which we introduced invariants. Our proof strategy \nwas aimed at making the successful executions of the code at lines 7-8 of pop and9-10of push atomic. \nWe performed read abstractions on TOP at lines 4 and 6 of pop and 8 of push since these reads do not \naffect correctness. For each loop, we hoisted the loop iteration containing a successful CAS operation \noutside the loop. There is an ownership protocol implicit in the use of the stack. A thread should only \npush into the stack an element that it owns. The push operation transfers the ownership of the element \nto the stack. Ownership is transferred back to the thread that manages to pop that element from the stack. \nWe used an auxiliary variable owner , a map from stack elements to thread ids, to capture this ownership \ntransfer protocol. We also introduced assertions into the code to check that the ownership protocol is \nnot being violated. As with other examples, we delayed the introduction of invariants until the atomic \nactions in the program had become coarse enough; all invariants were consequently fairly simple. bakery(i:int) \n1 choosing[i] := 1; 2 number[i] := 1 + max(number[1],...,number[N]); 3 choosing[i] := 0; 4 for(i:=1; \ni<=N ; ++i) { 5 while(choosing[j] != 0) /*wait*/; 6 while(number[j] != 0 &#38;&#38; (number[j],j)<(number[i], \ni))/*wait*/; 7} 8c :=c+ 1; 9 assert c == 1; // critical section 10 c :=c-1 11 number[i] := 0; return; \nFigure 17. The bakery algorithm The bakery algorithm. The bakery algorithm (Figure 17) pro\u00advides non-blocking \ncritical sections [17]. We encoded the mutual exclusion property by adding a global variable c that counts \nthe number of threads in the critical section and adding the assertion c == 1 in the critical section. \nWe proved the property by obtaining a single action that spans the code between lines 1-7 that is enabled \nonly when (number[i],i) is greater that (number[k],k) for all k different from i. We abstracted the waiting \niterations of the for loop, which allowed us to eliminate the loop from the code. We ap\u00adplied the mutex \ntactic twice. The .rst application encoded the fact that that choosing[i]==0 prohibits the values of \nnumber[i] that are smaller than number[k] to be read by a different thread k at line 6. The second application \nof the mutex tactic modeled lines 1-7 as an acquire for a conceptual lock, and line 11 as the release \nof this lock. This conceptual lock protects c so that the code between lines 8-10 is atomic and the assertion \nnever fails.  7. Related work In this section, we compare our work with other approaches for compositional \nveri.cation of multithreaded programs. In a nutshell, our method is orthogonal and complementary to existing \nmethods that do not make direct use of reduction and abstraction, and sub\u00adsumes others that do. In the \nOwicki-Gries approach [21], each potential interleav\u00ading point in a program must be annotated with an \ninvariant that is valid under interference from other concurrently-executing ac\u00adtions. Rely-guarantee \nmethods [25, 5, 4] make this approach more modular by obviating the need to consider each pair of concur\u00adrent \nstatements separately. Instead, the guarantee and rely condi\u00adtions of a thread provide a summary for \ntransitions taken by this thread and the transitions taken by environment threads, respec\u00adtively. Both \nthese methods require the programmer to reason about interleavings of .ne-grained actions; consequently, \nthe required an\u00adnotations are complex. Concurrent separation logic [20, 3] has the ability to maintain \nseparation between shared and local memory dynamically. Similarly to our method, it enables sequential \nreason\u00ading for multithreaded programs. By converting the original program into a simpler program that \nuses coarse-grained atomic actions, our method enhances the applicability of all of these approaches. \nAt the same time, our method can bene.t from these approaches as well. For example, the ability of concurrent \nseparation logic to reason about dynamic ownership transfer of heap objects could be use\u00adful for establishing \nthat heap accesses are non-con.icting, thereby enabling the key step of reduction in our method. The \nsame sym\u00adbiotic relationship applies to the approach in [15] where method contracts and object invariants \nare used to specify sharing and own\u00adership constraints on Spec# objects. Several veri.cation approaches \nin the literature use reduction as a key ingredient [12, 24, 10, 23]. These approaches are different \nfrom ours in that (i) they are limited to simple synchronization dis\u00adciplines and (ii) can only reason \nabout commutativity of accesses that are not simultaneously enabled. [13] addresses the .rst issue by \nusing auxiliary variables and access predicates to enable wider application of reduction. In addition \nto regular locking primitives, [23] applies mover-analysis to non-blocking synchronization prim\u00aditives \nLL, SC and CAS under certain execution patterns of these prim\u00aditives. As demonstrated in this paper, \nour method supports intricate synchronization mechanisms naturally, with moderate annotation burden. \nFurther, our check for mover types is general and able to deduce that certain simultaneously enabled \naccesses commute. Ab\u00adstractions have been used as a mechanism to prove atomicity in the work on purity \n[10, 23]. The abstraction step in our method sub\u00adsumes purity and allows us to deduce pure code blocks \nthrough simple variable abstractions. 8. Conclusion We introduced a proof method that iteratively simpli.es \na program by rewriting it in terms of coarser-grained atomic actions. When applied iteratively, reduction \nand abstraction enable further use of each other and signi.cantly simplify programs. Our tool QED automates \nthe proof steps in our approach, and our experience suggests that our approach provides a useful strategy \nto simplify the veri.cation of assertions in concurrent programs. Future work includes extending our \nframework to verify programs written in C and Spec#, developing tactics to support more synchronization \nidioms such as barriers and events, and applying our method to larger veri.cation problems.  Acknowledgments \nThis research was supported by a career grant (104E058) from the Scienti.c and Technical Research Council \nof Turkey, the Turk\u00adish Academy of Sciences Distinguished Young Scientist Award (TUBA-GEBIP), and a research \ngift from the Software Reliability Research group at Microsoft Research, Redmond, WA. We would like to \nthank the Spec# team, particularly Rustan Leino and Mike Barnett, for their support with using the Boogie/Spec# \nframework. References [1] E. A. Ashcroft. Proving assertions about parallel programs. J. Comput. Syst. \nSci., 10(1):110 135, 1975. [2] M. Barnett, B.-Y. E. Chang, R. DeLine, B. Jacobs, and K. R. M. Leino. \nBoogie: A modular reusable veri.er for object-oriented programs. FMCO 05: 4th International Symposium \non Formal Methods for Components and Objects, pages 364 387, 2005. [3] S. Brookes. A semantics for concurrent \nseparation logic. Theor. Comput. Sci., 375(1-3):227 270, 2007. [4] J. W. Coleman and C. B. Jones. Guaranteeing \nthe soundness of rely/guarantee rules. Journal of Logic and Computation, 17(4):807 841, 2007. [5] F. \nS. de Boer, U. Hannemann, and W.-P. de Roever. A compositional proof system for shared variable concurrency. \nIn FME 97: 4th International Symposium of Formal Methods Europe, volume 1313, pages 515 532. Springer-Verlag, \n1997. [6] L. M. de Moura and N. Bj\u00a8orner. Z3: An ef.cient SMT solver. In TACAS 08: Proceedings of the \n14th International Conference on Tools and Algorithms for the Construction and Analysis of Systems, volume \n4963 of Lecture Notes in Computer Science, pages 337 340. Springer, 2008. [7] E.W.Dijkstra. A Discipline \nof Programming. Prentice Hall PTR, Upper Saddle River, NJ, USA, 1997. [8] T. Elmas, S. Qadeer, and S. \nTasiran. A calculus of atomic actions. Technical Report MSR-TR-2008-99, Microsoft Research, 2008. [9] \nT. Elmas, S. Tasiran, and S. Qadeer. VYRD: Verifying concurrent programs by runtime re.nement-violation \ndetection. In PLDI 05: Proceedings of the 2005 ACM SIGPLAN conference on Programming Language Design \nand Implementation, pages 27 37, New York, NY, USA, 2005. ACM Press. [10] C. Flanagan, S. N. Freund, \nand S. Qadeer. Exploiting purity for atomicity. IEEE Trans. Softw. Eng., 31(4):275 291, 2005. [11] C. \nFlanagan, K. R. M. Leino, M. Lillibridge, G. Nelson, J. B. Saxe, and R. Stata. Extended static checking \nfor Java. In PLDI 02: Proceedings of the ACM SIGPLAN 2002 Conference on Programming Language Design and \nImplementation, pages 234 245, New York, NY, USA, 2002. ACM Press. [12] C. Flanagan and S. Qadeer. Types \nfor atomicity. In TLDI 03: Proceedings of the 2003 ACM SIGPLAN International Workshop on Types in Language \nDesign and Implementation, pages 1 12, New York, NY, USA, 2003. ACM. [13] S. Freund and S. Qadeer. Checking \nconcise speci.cations for multithreaded software. Journal of Object Technology, 3(6):81 101, 2004. [14] \nM. Herlihy, V. Luchangco, and M. Moir. Obstruction-free syn\u00adchronization: Double-ended queues as an example. \nIn ICDCS 03: Proceedings of the 23rd International Conference on Distributed Computing Systems, pages \n522 529, Washington, DC, USA, 2003. IEEE Computer Society. [15] B. Jacobs, J. Smans, F. Piessens, and \nW. Schulte. A simple sequential reasoning approach for sound modular veri.cation of mainstream multithreaded \nprograms. Electron. Notes Theor. Comput. Sci., 174(9):23 47, 2007. [16] C. B. Jones. Development Methods \nfor Computer Programs including a Notion of Interference. PhD thesis, Oxford University, June 1981. [17] \nL. Lamport. A new solution of Dijkstra s concurrent programming problem. Commun. ACM, 17(8):453 455, \n1974. [18] R. J. Lipton. Reduction: a method of proving properties of parallel programs. Commun. ACM, \n18(12):717 721, 1975. [19] M. M. Michael. Hazard pointers: Safe memory reclamation for lock\u00adfree objects. \nIEEE Trans. Parallel Distrib. Syst., 15(6):491 504, 2004. [20] P. W. O Hearn, H. Yang, and J. C. Reynolds. \nSeparation and information hiding. In POPL 04: Proceedings of the 31st ACM SIGPLAN-SIGACT Symposium on \nPrinciples of Programming Languages, pages 268 280, New York, NY, USA, 2004. ACM. [21] S. Owicki and \nD. Gries. Verifying properties of parallel programs: an axiomatic approach. Commun. ACM, 19(5):279 285, \n1976. [22] M. Parkinson, R. Bornat, and P. O Hearn. Modular veri.cation of a non-blocking stack. SIGPLAN \nNot., 42(1):297 302, 2007. [23] L. Wang and S. D. Stoller. Static analysis for programs with non\u00adblocking \nsynchronization. In PPoPP 05: Proceedings of the ACM SIGPLAN 2005 Symposium on Principles and Practice \nof Parallel Programming, pages 61 71. ACM Press, June 2005. [24] L. Wang and S. D. Stoller. Runtime analysis \nof atomicity for multi\u00adthreaded programs. IEEE Transactions on Software Engineering, 32:93 110, Feb. \n2006. [25] Q. Xu, W. P. de Roever, and J. He. The rely-guarantee method for verifying shared variable \nconcurrent programs. Formal Aspects of Computing, 9(2):149 174, 1997.  \n\t\t\t", "proc_id": "1480881", "abstract": "<p>We present a proof calculus and method for the static verification of assertions and procedure specifications in shared-memory concurrent programs. The key idea in our approach is to use atomicity as a proof tool and to simplify the verification of assertions by rewriting programs to consist of larger atomic actions. We propose a novel, iterative proof style in which alternating use of abstraction and reduction is exploited to compute larger atomic code blocks in a sound manner. This makes possible the verification of assertions in the transformed program by simple sequential reasoning within atomic blocks, or significantly simplified application of existing concurrent program verification techniques such as the Owicki-Gries or rely-guarantee methods. Our method facilitates a clean separation of concerns where at each phase of the proof, the user worries only about only either the sequential properties or the concurrency control mechanisms in the program. We implemented our method in a tool called QED. We demonstrate the simplicity and effectiveness of our approach on a number of benchmarks including ones with intricate concurrency protocols.</p>", "authors": [{"name": "Tayfun Elmas", "author_profile_id": "81100211898", "affiliation": "Ko&#231; University, Istanbul, Turkey", "person_id": "P1300915", "email_address": "", "orcid_id": ""}, {"name": "Shaz Qadeer", "author_profile_id": "81100286660", "affiliation": "Microsoft Research, Redmond, WA, USA", "person_id": "P1300916", "email_address": "", "orcid_id": ""}, {"name": "Serdar Tasiran", "author_profile_id": "81100391292", "affiliation": "Ko&#231; University, Istanbul, Turkey", "person_id": "P1300917", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1480881.1480885", "year": "2009", "article_id": "1480885", "conference": "POPL", "title": "A calculus of atomic actions", "url": "http://dl.acm.org/citation.cfm?id=1480885"}