{"article_publication_date": "01-21-2009", "fulltext": "\n A Cost Semantics for Self-Adjusting Computation Ruy Ley-Wild * Carnegie Mellon University rleywild@cs.cmu.edu \nAbstract Self-adjusting computation is an evaluation model in which pro\u00adgrams can respond ef.ciently \nto small changes to their input data by using a change-propagation mechanism that updates computation \nby re-building only the parts affected by changes. Previous work has proposed language techniques for \nself-adjusting computation and showed the approach to be effective in a number of application areas. \nHowever, due to the complex semantics of change propaga\u00adtion and the indirect nature of previously proposed \nlanguage tech\u00adniques, it remains dif.cult to reason about the ef.ciency of self\u00adadjusting programs and \nchange propagation. In this paper, we propose a cost semantics for self-adjusting computation that enables \nreasoning about its effectiveness. As our source language, we consider a direct-style .-calculus with \n.rst\u00adclass mutable references and develop a notion of trace distance for source programs. To facilitate \nasymptotic analysis, we propose techniques for composing and generalizing concrete distances via trace \ncontexts (traces with holes). We then show how to translate the source language into a self-adjusting \ntarget language such that the translation (1) preserves the extensional semantics of the source programs \nand the cost of from-scratch runs, and (2) ensures that change propagation between two evaluations takes \ntime bounded by their relative distance. We consider several examples and ana\u00adlyze their effectiveness \nby considering upper and lower bounds. Categories and Subject Descriptors D.3.0 [Programming Lan\u00adguages]: \nGeneral; D.3.3 [Programming Languages]: Language Constructs and Features General Terms Languages. Keywords \nSelf-adjusting computation, cost semantics. 1. Introduction In many applications it can be important \nor even necessary to ef.\u00adciently update the output of a computation as the input undergoes small changes \nover time. This problem, broadly known as incre\u00admental computation, has been studied extensively in both \nthe algo\u00adrithms and programming languages communities. * This author was partially supported by a Bell \nLabs Graduate Fellowship and a gift from Intel. This author was partially supported by a gift from Intel. \nPermission to make digital or hard copies of all or part of this work for personal or classroom use is \ngranted without fee provided that copies are not made or distributed for pro.t or commercial advantage \nand that copies bear this notice and the full citation on the .rst page. To copy otherwise, to republish, \nto post on servers or to redistribute to lists, requires prior speci.c permission and/or a fee. POPL \n09, January 18 24, 2009, Savannah, Georgia, USA. Copyright c &#38;#169; 2009 ACM 978-1-60558-379-2/09/01. \n. . $5.00 Umut A. Acar Matthew Fluet Toyota Technological Institute at Chicago {acar,.uet}@tti-c.org \n In the algorithms community, researchers devised algorithms that are optimized to take advantage of \nspeci.c small input changes. Over the course of hundreds of papers (see Chiang and Tamassia 1992; Eppstein \net al. 1999; Agarwal et al. 2002 for surveys), impor\u00adtant advances have been made. Those results show \nthat it is often possible to update computations asymptotically faster (often by a linear factor) than \nre-computing from scratch. However, incremen\u00adtal algorithms can be dif.cult to design and analyze, especially \nfor sophisticated problems (e.g., 3D motion simulation (Guibas 1998)). These algorithms can also be dif.cult \nto implement and use, be\u00adcause of inherent complexity and non-compositionality. Over the same period \nof time, the programming languages com\u00admunity has made signi.cant progress on run-time and compile-time \napproaches to incremental computation (e.g., Demers et al. 1981; Pugh and Teitelbaum 1989; see Ramalingam \nand Reps 1993 for a survey). The goal of this line of work is to derive incremental pro\u00adgrams from static \nprograms automatically or semi-automatically. The idea is to maintain certain information during an execution \nthat can be used to ef.ciently update the output after changes to the input. Recent work on self-adjusting \ncomputation (e.g., Acar et al. 2006b,a; Ley-Wild et al. 2008b) proposed a general-purpose change-propagation \nmechanism that can closely match asymptotic performance bounds achieved by algorithmic techniques. Self\u00adadjusting \ncomputation has been shown to be effective in various applications (e.g., Acar et al. 2004, 2006a,c, \n2008c,b). For exam\u00adple, recent work (Acar et al. 2008b) proposed a solution to simulat\u00ading moving convex \nhulls in 3D, a problem that has resisted ad hoc approaches for a decade (Guibas 1998). Reasoning about \nthe effectiveness of self-adjusting programs, however, remains dif.cult. In particular, there is no cost \nmodel for self-adjusting computation. Previous applications of the approach often give only experimental \nresults to illustrate performance gains (e.g., Acar et al. 2006a,c, 2008b). Giving asymptotic bounds \nre\u00adquires integrating change propagation into the algorithm by consid\u00adering a low-level machine model \nakin to the RAM model (e.g., Acar et al. 2004). As a result, the bounds derived do not directly apply \nto the code as written. More importantly, the approach does not provide a source-level reasoning mechanism. \nThe main dif.culty in reasoning about a self-adjusting program is understanding how the program responds \nto changes to its data. One reason for this is the complexity of the update mechanism; another is the \nnature of previously proposed linguistic techniques. To see the .rst dif.culty, consider executing a \nprogram with some input and later changing the input. In self-adjusting compu\u00adtation, as the program \nexecutes, information about the execution (such as data and control dependencies) is recorded. After \nthe input is changed, the output is updated by performing change propaga\u00adtion to .nd the parts of the \ncomputation affected by the change, us\u00ading the recorded dependence information and updating stale com\u00adputation \nby re-executing code. When re-executing code, change propagation may reuse previous computations with \na form of com\u00adputation memoization. Since change-propagation re-executes parts .s s tution into two contexts \nis the same the distance between the sub\u00ad ssTTC + s2 1 sss ; e ; T s +  stituted traces themselves plus \nthe distance between the contexts. v ds s c We compile the source language into a self-adjusting target \ntranslation translation translation translation language. The target language has mutable modi.able references \nv v and is in continuation-passing style; its syntax combines ideas from v v .t ttTT2 1 t dt.O(ds) \nrecent work on imperative self-adjusting computation (Acar et al. C + stt + t; T t ; ev t.O(c 2008a) \nand on compiling self-adjusting programs (Ley-Wild et al. s) c tcanreplayatracefromapreviousrun(Te.g.,0 \n 2008b). Evaluation of a target expression (e t) takes place in the consistency context of a store (st) \nand yields a value (v t) and a trace (T t). ,t The semantics includes a change-propagation mechanism \n(it) that ,t t  tC+ TTt2 1 Figure 1. The left diagram illustrates the correspondence between tT;0 st \n; T t + ) in a store (st) to v dt produce a value and a trace that are consistent with a from-scratch \ntexecution,whilereusingtheworkfromtheinitialtrace(T0 give a cost semantics for the target language that \ncounts steps of ). We the source and target from-scratch runs and the consistency of change propagation \nin the target. The right diagram illustrates the evaluation (but not steps of change propagation). As \nin the source, correspondence between distance in the source and target, and the we de.ne a distance \nfor traces (et) and bound the time for change time for change propagation in the target. propagation \nby the distance between the computation traces before and after propagation. We connect the source and \ntarget languages by providing a compilation mechanism that translates source programs into target programs. \nThe adaptive cps (ACPS) translation extends recent work (Ley-Wild et al. 2008b) to support imperative \nreferences and yields provably ef.cient self-adjusting programs. In particular, we prove the following \nproperties of the translation (cf., Figure 1). Extensional semantics: The translation preserves the \nevalua\u00adtion of source programs (top left square).  Intensional semantics: The translation preserves \nthe asymp\u00adtotic cost of from-scratch runs (top left square).  Consistency of change propagation. Change \npropagation (in the target) preserves the extensional semantics of from-scratch runs (bottom left square). \n Trace distances. Translated programs have asymptotically the same trace distance as their source (top \nright square).  Change propagation time. Time for change propagation (in the target) coincides with \nsource trace distance (right diagram).  To prove the .rst two properties, we generalize a folklore \ntheo\u00adrem about cps to show that an ACPS-compiled program preserves the evaluation and asymptotic complexity \nof a source program. The ACPS translation is more complicated than the standard translation because it \nthreads continuations through the store. We give a sim\u00adple, structural proof of the consistency of change \npropagation by casting it as a full replay mechanism. This simpli.cation is made possible by the translation \nitself earlier work had to use step\u00adindexed logical relations for capturing the correspondence between \nstateful programs (Acar et al. 2008a). We prove the fourth prop\u00aderty by establishing a relation between \nthe traces of the source and the target programs. This property also bounds the time for change of the \nprogram code and reuses other parts of the execution, it is hard to reason about its complexity. In particular, \nthe user may need to reason about the contexts in which sub-expressions are evaluated to distinguish \nchanged and unchanged data, which can be dif.cult even with limited forms of computation reuse techniques \nsuch as lazy evaluation (e.g., Wadler and Hughes 1987; Sands 1990a,b). Other dif.culties arise from the \nnature of the previously pro\u00adposed linguistic facilities. These approaches require the program\u00admer to \nmark all data that change over time and identify their depen\u00addencies, delimit the static scope of the \noperation that reads change\u00adable data (essentially identifying control dependencies), and apply memoization \nby carefully considering whether the data dependen\u00adcies are local or non-local (Acar et al. 2006a). Depending \non the choice of the scope for the primitives and the use of memoization, the programmer may observe \ndrastically different performance. In this paper, we propose a cost semantics for self-adjusting computation. \nWe consider a natural source language, give a cost semantics for the language, and develop techniques \nfor reasoning about the similarity of executions. We then show techniques for compiling source programs \ninto a self-adjusting target language that preserves both the extensional (meaning) and the intensional \n(cost) semantics of the source programs. By offering a natural, high-level source language, we eliminate \nthe burden of restructur\u00ading a program for self-adjusting computation. By offering a cost se\u00admantics \nand a translation mechanism, we provide realistic source\u00adlevel reasoning techniques that guarantee performance. \nFigure 1 illustrates our approach. Our source language is a .\u00adcalculus with .rst-class references. Its \ncost semantics evaluates ex\u00adpressions (e s) in the context of stores (ss) in the usual way, and pro\u00adduces \na trace of the evaluation (T s) and a step count (c s). We quan\u00adtify the similarity between evaluations \nof source programs with a es ds = states that the distance between tracedistance (ssTT2 thetraces andssTT2 \n1 1 the edit distance between evaluations. To give an effective dis\u00adpropagation (the last property) by \nshowing that change propagation in the target takes time proportional to the target distance. is ds). \nIntuitively, the trace distance measures There are several properties of trace distance that we would \nlike to note. First, trace distance is a relation. By de.ning it relationally, we allow the approach \nto apply to any concrete implementation technique consistent with the semantics: our main theorems state \nthat our translation can match any source distance computed re\u00adlationally. Second, trace distance is \nsensitive to the choice of loca\u00adtions. This is because trace distance compares concrete evaluations. \nPrevious implementations of self-adjusting computations can often choose locations to minimize the trace \ndistance. Since our theorems can match any distance computed, they apply to existing implemen\u00adtations. \nThe problem of whether an implementation can ef.ciently achieve the minimum possible distance is not \nwell understood: this is undecidable in general but these impossibility results typically involve programs \nthat don t arise in practice. tance, we show that it suf.ces to record function calls and store operations \nin the trace. We don t record complete stores or evalu\u00adation contexts.1 Since our language is stateful, \nrecording complete stores would lead to a distance measure that overestimates distance signi.cantly; \nrequiring evaluation contexts would make reasoning cumbersome. To enable proving asymptotic bounds on \ndistance, in addition to just concrete evaluations, we develop a notion of trace contexts, which are \ntraces with holes that can be .lled with other traces. We prove that, under certain conditions, distance \nis additive under substitution: the distance between traces obtained via substi\u00ad 1 For some time, we \nthought that evaluation contexts, which describe how results are used, were necessary. We use evaluation \ncontexts to prove our meta-theoretic results, but they are not necessary for source-level reasoning. \nn GRn.nil nil.R P n Pc::R n .Rc GRn.nil nil.R P GR3.3::Rn mapA (\u00a3n) . \u00a3. n Pc::R n .R3 GR3.3::Rn mapA \n(\u00a3n) . \u00a3. n GR2.2::R3 b::Rc.Rb mapA (\u00a33) . \u00a3c P GR1.1::R3 Pa::Rc.Ra GR1.1::R2 Pa::Rb.Ra mapA (\u00a33) . \n\u00a3c mapA (\u00a32) . \u00a3b mapA (\u00a31) . \u00a3a mapA (\u00a31) . \u00a3a Figure 2. The abstract derivations for executions of \nmapA with inputs [1,3] (left) and [1,2,3] (right). Due to space restrictions, we refer the reader to \nthe companion technical report (Ley-Wild et al. 2008a) for the details of the proofs and Twelf code. \n2. An Overview of Derivation Distances We give a high-level overview of derivation distance and contexts. \nAs a simple example, we consider a map function. Our source language is a .-calculus with references. \nThis lan\u00adguage is general-purpose (Turing-complete) and expressive: it allows writing both structured \nprograms (e.g., iterative divide\u00adand-conquer list algorithms) as well as unstructured programs (e.g., \ngraph algorithms). In this language, we can de.ne linked lists and implement a map function for them \nas follows. datatype a cell = nil | :: of a * a list withtype a list = a cell ref fun map(f : a-> b)(l: \na list): b list= case !l of nil => ref nil | h::t => let mt = map f t in ref ((f h)::mt) end This essentially-standard \nimplementation of map with pointer\u00adbased lists is actually self-adjusting: using the techniques described \nin this paper (Section 6), we can compile it to a self-adjusting pro\u00adgram. The resulting self-adjusting \nprogram can be run with some input list. Afterwards, any of the contents of the references can be changed \nand the output can be updated via change propagation. For example, consider a specialization mapA of \nmap that maps integers to letters of the alphabet. Consider running mapA with input [1,3] to obtain [a,c] \nand then changing the input to [1,2,3] by writing a new cons cell into the .rst tail pointer. After this \nchange, we can run change change propagation to update the output to [a,b,c]. How fast would we expect \nchange propagation be after inserting an element into the input? Intuitively, we only need to translate \nthe new integer into a letter, which requires constant time, but we also need to .nd the right place \nto insert the element in the output it is not clear how much time that would take. Derivation Distance. \nWe develop techniques for reasoning about the effectiveness of change propagation by using derivation \ndis\u00adtance.2 The idea is to compare the evaluation derivations of a pro\u00adgram with two different, typically \nsimilar, inputs and compute the edit distance between these derivations. But what should the dis\u00adtance \nbetween evaluations be? Comparing evaluation derivations directly yields coarse distances. To see this, \nconsider comparing the derivation for the evaluation of mapA with inputs [1,3] and [1,2,3]. Since these \ninputs are represented in the store and since the store is threaded through the derivation, all of derivation \nsteps will be different stores won t match. Thus the distance between the derivations would be linear \nin the size of the input far larger than the constant that we expect. To realize the similarity between \nthe derivations, we exclude the store from the derivations and include the store operations in\u00adstead. \n(P stands for put (allocation); G stands for get (derefer\u00ad 2 In Section 3, we represent derivations with \ntraces and formally de.ne trace distance. Here, we use derivations because they are more intuitive. \nence).) Figure 2 shows the derivations of mapA with inputs [1,3] and [1,2,3]. The differences between \nthe derivations are high\u00adlighted: the two derivations differ only at steps that operate on the element \n2, which is what differs between the two runs. Note that the difference remains the same even if we add \nmore elements to these lists (e.g., [...,0,1,3,4,...] and [...,0,1,2,3,4,...]). Of course, it is possible \nto make the distance between deriva\u00adtions arbitrarily small when we suppress arbitrary parts of the derivation. \nWe prove that this distance is in fact realistic by de\u00adscribing how source programs may be compiled (Section \n6) to a target language (Section 5) with provable ef.ciency. Derivation Contexts. To reason about the \nasymptotic complexity bounds for distance, we need to compute distance for all (appropri\u00adately changed) \ninputs. This is dif.cult with the distance described above, which requires two concrete executions. To \nfacilitate asymp\u00adtotic analysis, we use derivation contexts (Section 3). A derivation context is a derivation \nwith one or more holes in it. We write Ve.v for a hole that expects an evaluation of e . v. We can obtain \na derivation from a derivation context by substituting a derivation for a hole. As an example, consider \nthe derivation, shown below, of mapA applied to the list [a1,...,am]@O where O represents an unspeci.ed \nlist. In the derivation ei (resp. oi) denotes the reference to the cons cell containing input ai (resp. \noutput for \u00dfi), and \u00dfi denotes the character to which ai is mapped. Given this deriva\u00adtion context, we \ncan substitute the list [1,3] for O and obtain the derivation for that input by substituting the derivation \nof [1,3] (shown in Figure 2) in place of the hole.3 mapA(Ro).oo Rm.am::Ro ( P\u00dfm::oo.om G . . . GR1.a1::R2 \nP\u00df1::o2.o1 mapA (\u00a32) . o2 mapA (\u00a31) . o1 Let D1[O] and D2[O] be derivation contexts and let D1.and D2.be \nderivations. We prove that the distance between D1[D1.] and D2[D2.] is the sum of the distances between \nD1[O] and D2[O] and between D1.and D2., for suitably-shaped contexts. This result enables generalizing \nconcrete distances to arbitrary inputs. For example, the above two analyses can be generalized and combined \nto show that the distance between derivations of mapA with inputs that differ by one element is constant. \nThis allows us to also derive asymptotic complexity bounds, which is generally dif.cult with concrete \ncost semantics (Section 4). 3. The Source Language (Src) The Src language is a simply-typed, call-by-value \n.-calculus with recursive functions and ML-style references. The language also in\u00adcludes natural numbers \nfor didactic purposes and can easily be ex\u00adtended with products, sums, recursive types, etc., but we \nomit them 3 Note that not all substitutions yield well-formed derivations. In particular, the choice \nof locations needs to be consistent. ; s; v . s; v; e;0 E; s; ez . s ; v ; T ; c E; s; caseN zero ez \n(x.es) . s ; v ; T ; c E; s; {vn / x} es . s ; v ; T ; c E; s; caseN (succ vn) ez (x.es) . s ; v ; T \n; c E[O ex]; s; ef . sf ; fun f .x.e; Tf ; cf E[(fun f .x.e ) O]; sf ; ex . sx; vx; Tx; cx E; sx; {vx \n/ x}{fun f .x.e / f } e . s ; v ; T ; c (fun f .x.e) vx.v E; s; ef ex . s ; v ; Tf \u00b7Tx \u00b7(ME (T )\u00b7e); \ncf + cx +1+ c \u00a3/. dom(s) s = s l{\u00a3 . v} v.R E; s; put v . s ; \u00a3; PE \u00b7e;1 \u00a3 . dom(s) s(\u00a3)= v E; s; get \n\u00a3 . s; v; GR.v\u00b7e;1 E \u00a3 . dom(s) s = s[\u00a3 . v] E; s; set \u00a3v . s ; zero; SR.v\u00b7e;1 E Figure 3. Src evaluation \nE; s; e . s ; v ; T ; c. as they provide no additional insight. Although Src has no oper\u00adational support \nfor self-adjusting computation (i.e., a mechanism for updating a computation under input changes), its \ndynamic se\u00admantics produces an execution trace that can be used to quantify similarities between runs \nas a distance. Src programs can be com\u00adpiled into Tgt programs (see Sections 5 and 6), whose semantics \ninclude a change propagation judgement that realizes updates and asymptotically matches Src distances. \nThe syntax of Src is given below, which de.nes types t , expres\u00adsions e, and values v, using metavariables \nf and x for identi.ers and e for locations. t ::= nat | tx . t | t ref e ::= v | caseN vn ez (x.es) | \nef ex | put v | get vR | set vR v v ::= x | zero | succ v | fun f.x.e | \u00a3 The dynamic semantics of memoizing \nfunctions fun f.x.e is in\u00adstrumented to identify opportunities for computation reuse. The reference primitives \nand scrutinee of caseN are restricted to value forms for technical simplicity. This restriction can be \navoided with syntactic sugar, for example the unrestricted dereference form get eR can be de.ned as (fun \nf.x.get x) eR. 3.1 Static, Dynamic, and Cost Semantics The (standard, hence omitted) typing judgement \nS; G f e : t as\u00adcribes the type t to the expression e in the store and variable typing contexts S and \nG. Figure 3 gives the dynamic and cost semantics of Src. The large-step evaluation relation E; s; e . \ns ; v ; T ; c re\u00adduces expression e in store s to value v in updated store s and yields an execution \ntrace T and a cost c. The trace internalizes the shape of an evaluation derivation and will be used to \nidentify the similar computations. The cost internalizes the size of a trace and will be used to relate \nthe constant slowdown due to compiling Src programs to Tgt programs. For the present time, we suggest \nthat the reader ignore the highlighted evaluation context E component; it is crucial for relating Src \nand Tgt distances (see Section 6), but is not necessary for reasoning about Src distance. We distinguish \nactive computation as work that may be used to identify similarities and differences in computation. \nEvaluation of reference primitives and application of memoizing functions yield active computation. Case-analysis \nand (in the presence of sums, products, etc.) other forms of \u00df-reduction are considered passive computation. \nAn evaluation derivation internalizes its size in a cost c as a natural number that quanti.es active \nwork. We do not ex\u00adplicitly quantify passive work because it is always bounded by a constant multiple \nof active work. Intuitively, since a Src program can only perform a bounded amount of computation between \nfunc\u00adtion calls, memoizing function actions suf.ce to account for all pas\u00adsive work; including actions \nfor passive work (e.g., case-analysis) would give a more accurate measure but isn t necessary for calcu\u00adlating \nasymptotic time complexity or distance. This property is for\u00admalized in the companion technical report \n(Ley-Wild et al. 2008a). A trace T is an interleaving of actions that internalizes the shape of an evaluation \nderivation: | GR.v | SR.v As ::= Pv.R EE E vf vx.v A ::= As | M (T ) E T ::= e | A\u00b7T Actions A serve \nas markers for active work and consist of store actions and memoizing function actions. Store actions \nAs include allocation (P), dereference (G), and update (S), which are labeled with the location e and \nvalue v involved in each operation. A vf vx.v memoizing function action ME (T ) is labeled with a function \nvf , argument vx, and result v; the delimited trace T identi.es the body of the function application \nfor reuse; as in the dynamic semantics, the highlighted evaluation context E can be ignored. Traces facilitate \nidentifying the similarities and differences be\u00adtween different runs of a program. More speci.cally, \nsince store mutation is the only kind of observable side effect in Src, refer\u00adence primitives uniquely \ndetermine the control .ow of a closed pro\u00adgram. Thus, by recording them in the trace, we can identify \nwhere program runs differ. Since memoizing functions identify explicitly similar computations by matching \narguments to function calls, they can be used to identify where program runs perform similar com\u00adputations. \nTherefore actions in traces are necessary and suf.cient to isolate the similarities and differences between \nprogram runs. Returning to the dynamic semantics (Figure 3), evaluation ex\u00adtends the trace and increments \nthe cost counter according to the kind of reduction. Cost grows in lock-step with the trace and could \nbe de.ned as the size of the trace, but we keep it explicit to relate the intensional semantics of the \nSrc and Tgt languages. A value reduces to itself, produces an empty trace, and has no cost. A case-analysis \nreduces according to the branch prescribed by the scrutinee; the trace and cost are unchanged, since, \nas noted above, case-analysis incurs only passive work. A function application reduces the function ef \nand argument ex to values and then evaluates the redex. An application concatenates the function, argument, \nand redex traces to represent the sequenc\u00ading of work; the redex trace is delimited by the memoizing \nfunction action to identify the scope of the function call; the cost of the traces are added and incremented \nby a unit of work for the \u00df-reduction. A reference allocation extends the store with a fresh location \nthat is initialized with the speci.ed value and returns the location. A dereference returns the location \ns value. An update changes the location s contents and returns zero. In each case, the trace is the singleton \naction corresponding to the primitive, and the work is 1. 3.2 Trace Distance Consider running a single \nprogram under two different stores: in\u00adtuitively, the executions will be identical up to the .rst differing \nstore primitive (viz. the run of mapA on the pre.x ...,0,1 from Section 2), after which the traces may \ncorrespond to different sub\u00adprograms (e.g., because an allocation produced different locations or a read \nfound different values). In terms of traces, they will have a common pre.x up to the .rst differing store \naction. Conservatively, the only similarity between two runs would be the common pre.x. Memoizing functions, \nhowever, enable recognizing similar compu\u00ad T1 8 T2 = dT1 E T2 = d search/nil search/synch e E e = (0, \n0) Mvf vx.v1 (T1)\u00b7T1 E Mvf vx.v2 (T2)\u00b7T2 = (1, 1) + d + d E1 E2 T1\u00b7T1 E T2 = dT1 E T2\u00b7T2 = d search/memo/L \nsearch/memo/R vf vx.vvf vx.v M(T1)\u00b7T1 E T2 = (1, 0) + dT1 E M(T2)\u00b7T2 = (0, 1) + d EE T1 E T2 = dT1 E \nT2 = d search/store/L search/store/R As \u00b7T1 ET2 = (1, 0) + dT1 E As \u00b7T2 = (0, 1) + d T1 8 T2 = dT1 E \nT2 = d synch/nil synch/store synch/search e 8 e = (0, 0) As \u00b7T1 8 As \u00b7T2 = dT1 8 T2 = d T1 8 T2 = dT1 \n8 T2 = d synch/memo vf vx.vvf vx.v M(T1)\u00b7T1 8 M(T2)\u00b7T2 = d + d EE Figure 4. Src search distance T1 \nE T2 = d (top) and synchronization distance T1 e T2 = d (bottom). tations that occur after two runs have \ndiverged (viz. the run of mapA on the post.x 3,4,...) because they identify the trace of the same function \napplied to the same argument. Nevertheless, even if two computations result from the same function application, \nthey may also have different traces and return different results due to inter\u00adactions with the store. \nMore generally, comparing two traces alter\u00adnates between searching for a point where traces align (i.e., \nmemo\u00adizing function application) and synchronizing the two similar traces until they again differ (i.e., \nstore actions). Distance is formally captured by the search distance T1 E T2 = d and synchronization \ndistance T1 e T2 = d judgements (given in Figure 4), de.ned by structural induction on the two traces. \nThe search mode can switch to synchronization if it encounters simi\u00adlar program fragments (as identi.ed \nby memoizing application ac\u00adtions), and the synchronization mode must switch to search mode if the trace \nactions differ at some point. Intuitively, the trace distance measures the symmetric difference between \ntwo traces (i.e., the size of trace segments that don t occur in both traces). Concretely, we quantify \ndistance d = (c1,c2) between traces T1 and T2 as a pair of costs, where c1 is the amount of work in T1 \nthat isn t shared with T2 and c2 is the amount of work in T2 that isn t shared with T1. We let d + d \ndenote pointwise addition for distance. Since traces approximate the shape of an evaluation deriva\u00adtion, \ntrace distance approximates a (higher-order) distance judge\u00adment on evaluation derivations that quanti.es \nthe dis/similarities between two runs (modulo the stores). The dynamic semantics of Tgt has nondeterministic \nallocation and memoization in order to avoid committing to an implementation; consequently, the de.ni\u00adtion \nof Src trace distance is a relation, but we will show that any dis\u00adtance derivable for Src programs is \npreserved in Tgt (Theorem 7). The search distance T1 E T2 = d accounts for traces that don t match, but \nswitches to synchronization mode if it can align memoization actions. The search distance between empty \ntraces is zero. Upon simultaneously encountering memoization actions vf vx.v1 vf vx.v2 M (T1)\u00b7T1 and \nM (T2)\u00b7T2 of the same function vf E1 E2 and argument vx (search/synch rule), the search distance can \nswitch to synchronizing the bodies T1 and T2, while separately searching for further synchronization \nof the tails T1 and T2. The cost of the synchronization and search are added to the cost of 1 for the \nmemoization match in each trace. Finally, skipping an action in search mode incurs a cost of 1 in addition \nto the distance between the tail of the trace (search/memo rules and search/store rules). Turning to \nthe synchronization distance, the T1 e T2 = d judgement attempts to structurally match the two traces. \nIdentical work in both traces incurs no cost, but synchronization returns to search mode either nondeterministically \nor when work cannot be reused because traces don t match. Synchronization mode is only meant to be used \non traces generated by the evaluation of the same expression under (possibly) different stores. The \nsynchronization distance between empty traces is zero. En\u00adcountering identical store actions allows distance \nto remain in syn\u00adchronization mode without cost (synch/store rule). Synchronizing a memoization action \n(synch/memo rule) requires the function call (function vf and argument vx) and return (result v) to match; \nthis allows the bodies as well as the tails to be synchronized separately and their distance compounded. \nNote that even if the bodies don t match completely and return to search mode, memoizing functions provide \na degree of isolation because tails can be matched indepen\u00addently. Synchronization falls back to search \nmode (synch/search rule) nondeterministically or necessarily when the actions are dis\u00adtinct (e.g., because \nstore or memo actions don t match). Observe that the de.nition of synchronization distance is quasi\u00adsymmetric: \nT1 eT2 = (c1,c2) iff T2 eT1 = (c2,c1); similarly for search distance. Furthermore, note that distance \nof Src programs is de.ned by induction on the two traces: both judgements tra\u00adverse traces left-to-right \neither matching work or accounting for skipping it. This means that common work consists of a subse\u00adquence \nof actions that appears in both traces, which precludes re\u00ad ()\u00b7Mgy.b ordering work. For example, comparing \nruns Mfx.a()\u00b7 and Mgy.b()\u00b7Mfx.a ()\u00b7 can only synchronize one of the calls, the other call must be skipped. \nThis restriction avoids having to search for permutations for matching computations and simpli.es the \nimplementation requirements of Tgt; however, this limitation obviously hinders the ef.ciency of self-adjusting \ncomputation for certain classes of computations. 3.3 Trace Contexts In order to reason compositionally \nabout distance, we de.ne a trace context T to be a trace with a hole; the formalization to multi-holed \ncontexts is straightforward and omitted for reasons of space. vf vx.vvf vx.v T ::= O | As \u00b7T | M (T )\u00b7T \n| M (T )\u00b7T EE Trace context distances T1 E T2 = d and T1 e T2 = d are obtained by lifting distance on \ntraces to trace contexts, extended with the following rules for holes (in the multi-hole analogue, holes \nare uniquely labeled and labels must also coincide): O E O = (0, 0) O 8 O = (0, 0) By requiring holes \nto coincide when comparing trace contexts, we can reason separately about context and trace distance, \nand then combine the results. Intuitively, the identity theorem for traces means a common suf.x subcomputation \nincurs no cost. The iden\u00adtity theorem for trace contexts and the substitution theorem show that a common \npre.x computation does not affect distance either: provided the hole in both trace contexts is immediately \nbounded by a memoization action of the same function and argument, context and trace distance can be \ncombined additively. The identity theo\u00adrems are proved by induction on the subject trace T or trace context \nT . Theorem 1 (Identity for Traces) For any trace T , T e T = (0, 0). Theorem 2 (Identity for Trace Contexts) \nFor any trace context T , vf vx.vvf vx.v T [M (O)\u00b7T ] e T [M (O)\u00b7T ]= (0, 0). EE Theorem 3 (Substitution) \nvf vx.v1 vf vx.v2 If T1[M (O)\u00b7T1] E T2[M (O)\u00b7T2]= d E1 E2 and T1 e T2 = d , vf vx.v1 vf vx.v2 then T1[M \n(T1)\u00b7T1] E T2[M (T2)\u00b7T2]= d + d . E1 E2 vf vx.v1 vf vx.v2 If T1[M (O)\u00b7T1] e T2[M (O)\u00b7T2]= d E1 E2 and \nT1 e T2 = d , vf vx.v1 vf vx.v2 then T1[M (T1)\u00b7T1] e T2[M (T2)\u00b7T2]= d + d . E1 E2 Proof: By simultaneous \ninduction on the .rst derivations. .  3.4 Trace Distance, Revisited The rules of Figure 4 are useful \nfor high level reasoning, but aren t rich enough to demonstrate a correspondence with Tgt trace dis\u00adtance. \nWe present an alternate rule system that subsumes the above system and serves as an intermediary for \nproving the preservation of distance under compilation. Failure Actions. The search/synch rule separately \nsynchronizes the bodies and searches the tails when it encounters matching mem\u00adoizing actions. While \nthis rule is useful, it precludes memoization between one body and another tail; for example, it doesn \nt allow splitting T1 as T11\u00b7T12 and synchronizing T11 with a pre.x of T2 and searching T12 against the \nrest of T2. The na\u00a8ive rule T1\u00b7T1 8 T2\u00b7T2 = d vf vx.v1 vf vx.v2 ME1 (T1)\u00b7T1 E M(T2)\u00b7T2 = (1, 1) + d E2 \nwould allow splitting both traces, but it is unsound because it may fully synchronize T1\u00b7T1 with T2\u00b7T2, \neven though the trace concate\u00adnation may not have been generated the same expression, violating the well-formedness \nof synchronization distance. We remedy this by introducing the failure action F.E v to explicitly force \nsynchro\u00adnization mode to switch back to search mode; it is labeled by an evaluation context E and result \nv, which are needed for technical reasons but can be ignored when reasoning about Src distance: .v A \n::= \u00b7\u00b7\u00b7| F E The revised system is obtained by removing the search/synch and search/memo rules from Figure \n4 and adding the rules in Figure 5. The new search/memo rules insert an explicit failure action between \nthe body and tail of a memoization action, and still incur a cost of 1 for failing to match. The search/fail \nrules allow search to skip a failure action without cost. Observe that, in Figure 4, a trace is subjected \nto synchronization if it is delimited by a memoization action and failure actions never occur in the \nscope of a memoization action, so failure actions never appear in synchronization mode. Therefore the \nsearch/memo and search/fail rules subsume the (replaced) search/memo rules: any distance derivable from \nthe failure-free deductive system is also derivable from the system with explicit failure. .v T1\u00b7F\u00b7T1 \nE T2 = d E search/memo /L vf vx.v M(T1)\u00b7T1 E T2 = (1, 0) + d E .v T1 E T2\u00b7F\u00b7T2 = d E search/memo /R vf \nvx.v T1 E M(T2)\u00b7T2 = (0, 1) + d E T1 E T2 = dT1 E T2 = d search/fail/L search/fail/R .v .v F\u00b7T1 E T2 \n= dT1 E F\u00b7T2 = d EE .v1 .v2 T1\u00b7F\u00b7T1 8 T2\u00b7F\u00b7T2 = d E1 E2 search/synch vf vx.v1 vf vx.v2 M(T1)\u00b7T1 E M(T2)\u00b7T2 \n= (1, 1) + d E1 E2 Figure 5. Additional rules for Src distance with explicit failure. The search/synch \nrule identi.es matching function applica\u00adtions and switches to synchronizing the concatenation of the \nbody, failure action, and tail. Since there are no new synchronization distance rules, leading failure \nactions force synchronization to switch to search (only the synch/search rule applies). Therefore the \nsearch/synch rule enables synchronizing part of T1 with T2 and then searching the remainder of T1 against \nT2 (after encounter\u00ading the failure action between T2 and T2). The search/synch rule subsumes the (replaced) \nsearch/sync rule. Evaluation Contexts. The evaluation contexts E in Src evalua\u00adtion and traces are necessary \nfor relating Src and Tgt traces in Sec\u00adtion 6, but can be ignored when reasoning about Src evaluation \nand distance (in the deductive systems with and without failure). An evaluation context is built up throughout \nevaluation (Figure 3) to capture the shape of the surrounding evaluation derivation, up to the nearest \nmemoizing function application: E ::= O |E ex | vf E The language restriction on the occurrence of expressions \navoids explicit forms for case-analysis or reference manipulation. The evaluation of a memoizing function \napplication extends the context for evaluating the function and argument expressions, but resets the \ncontext for evaluating the redex; passive \u00df-reduction (e.g., case\u00adanalysis) passes the context unchanged. \nThe accumulated context is used to label the actions with the current context and is used by the ACPS \ntrace translation to reify the continuation. Intuitively, contexts help identify if computation after \na mem\u00adoizing function application can be reused. The search/synch rule ignores the contexts of each trace, \nthe search/memo rules pass the context and result to the failure action. The synch/store and synch/memo \nrules formally require the contexts to be identical. vf vx.v1 Since synchronization begins at memoizing \nactions ME1 (T1) vf vx.v2 and ME2 (T2) (cf., search/synch), the bodies T1 and T2 re\u00adsult from the evaluation \nof the same expression in the same reset context (cf., application evaluation) but under (possibly) different \nstores. Synchronization distance inductively preserves the property that the two traces being compared \nresult from the same expression in the same context. In particular, the evaluation contexts and re\u00adsults \nmatch in the synch/memo rule, so the property holds for the tails justifying why they can be synchronized \nindependently of the bodies. Therefore, contexts in synchronization mode are necessar\u00adily equal, and \ncan be ignored when reasoning about Src distance. 4. Examples We consider several examples to show how \ntrace distance can be used to analyze the sensitivity of programs to small changes in their input. We \nsay that a program is O(f(n))-sensitive or O(f (n))\u00adstable for an input change if the distance between \nthe traces of that b::Rc.Rb \u00b7Pa::Rb.Ra E Pa::Rb.Ra T0 8 T0 =0 P= (2, 1) M R3.Rc (T0)\u00b7Pb::Rc.Rb \u00b7Pa::Rb.Ra \nE M R3.Rc (T0)\u00b7Pa::Rc.Ra = (3, 2) GR2.2::R3 \u00b7A2.b \u00b7M R3.Rc (T0)\u00b7Pb::Rc.Rb \u00b7Pa::Rb.Ra E M R3.Rc (T0)\u00b7Pa::Rc.Ra \n= (5, 2) M R2.Rb (GR2.2::R3 \u00b7A2.b \u00b7M R3.Rc (T0)\u00b7Pb::Rc.Rb )\u00b7Pa::Rb.Ra E M R3.Rc (T0)\u00b7Pa::Rc.Ra = (7, \n3) GR1.1::R2 \u00b7A1.a \u00b7M R2.Rb (GR2.2::R3 \u00b7A2.b \u00b7M R3.Rc (T0)\u00b7Pb::Rc.Rb )\u00b7Pa::Rb.Ra 8 GR1.1::R3 \u00b7A1.a \u00b7M \nR3.Rc (T0)\u00b7Pa::Rc.Ra = (8, 4) M R1.Ra (GR1.1::R2 \u00b7A1.a \u00b7M R2.Rb (GR2.2::R3 \u00b7A2.b \u00b7M R3.Rc (T0)\u00b7Pb::Rc.Rb \n)\u00b7Pa::Rb.Ra ) E M R1.Ra (GR1.1::R3 \u00b7A1.a \u00b7M R3.Rc (T0)\u00b7Pa::Rc.Ra )= (9, 5) Figure 6. Trace distance between \nmapA [1,2,3] and mapA [1,3]. program is O(f(n)) for inputs related by that change. In our anal\u00adysis, \nwe consider two kinds of changes: insertions/deletions that relate lists that differ by the existence \nof an element (e.g., [1,3] and [1,2,3]) and replacements that relate inputs that differ by the value \nof one element (e.g., [1,2,3] and [1,7,3]). We start with the map example that we considered informally \n(Section 2) and analyze its sensitivity to an insertion into/deletion from the input by comparing its \ntraces. When convenient, we visualize traces as derivations and analyze their relative distance under \na replacement. In our analysis, we consider two kinds of bounds: upper bounds and lower bounds. Our upper \nbounds state that the distance between the traces of a program with inputs related by some change can \nbe asymptotically bounded by some function of the input size un\u00adder the assumption that locations allocated \nin the computation (or mentioned in the trace) can be chosen to match nicely. Without the ability to \nmatch locations, it is not possible to prove interesting upper bounds, because two runs of the program \ncan differ by as much as the size of the traces if their locations are chosen from dis\u00adjoint sets. As \nwe discuss in Section 7, an implementation can often match locations, sometimes with programmer guidance. \nOur lower bounds state that the distance between traces of a program with inputs related by some change \ncannot be asymptotically smaller than a function of input size regardless of how we choose loca\u00ad fun \nreduce fidl = let fun redrl= case !l of nil=> refr | h::t => red (f(h,r)) t in red id l end fun reducePair \nf id l = let fun comp l = case !l of nil => ref nil | a::t => case !t of nil => ref (a::ref nil) | b::u \n=> ref (f(a,b)::(comp u)) fun recl= if !(lenLT (l,2)) then case !l of nil => id | h:: => h else rec (comp \nl) in rec l end fun msort l = if !(lenLT (l,2)) then l else let (a,b) = partition l sa=msort a sb=msort \nb in merge (sa,sb) end fun filter f l = case !l of nil => ref nil | h::t => if (f h) then h::(filter \nf t) else filter f t tions. Such lower bounds suggest but do not prove a lower bound on the running \ntime for change propagation (Section 7). Our analyses .t into one of the following patterns. Sometimes, \nwe start with two concrete inputs and show a bound on the distance between traces with these inputs. \nWe then generalize this bound to arbitrary inputs using the identity and substitution theorems (The\u00adorems \n1 and 3). Other times, using the identity and the substitution theorems, we write a recursive formula \nfor the distance between the traces of the program with inputs related by some change, and solve this \nformula to establish the bound. When analyzing our ex\u00adamples and using the identity and the substitution \ntheorems, we ignore contexts, because, as noted in Section 3, they are not needed for analysis. We use \nthe distance and the composition theorems in the informal style of traditional algorithmic analysis, \nbecause we have no meta-logical framework for reasoning about asymptotic properties of self-adjusting \nprograms (Section 7). Figure 7 shows the code for list-reduction and merge-sort (see Section 2 for the \ncode of map). The list-reduce and merge-sort implementations use several functions, whose code we omit \nfor brevity. The lenLT(l,i) function returns (in a reference) true iff the length of the list l is less \nthan the integer i. The partition function evenly splits a list into two and merge combines two sorted \nlists. All of these functions are O(1)-sensitive to replace\u00adments on average (for merge, we need to average \nover all permuta\u00adtions of the input to obtain this bound). To focus on the main ideas, we omit the analysis \nof these utility functions here, which are sim\u00adilar to that of the map function discussed below. 4.1 \nMap Recall the mapA function from Section 2. We analyze the sensi\u00adtivity of mapA to an insertion/deletion \nmore precisely by using trace distance. Figure 6 shows the derivation of the trace distance for mapA \nwith inputs L = [1,2,3] and L = [1,3]. We con\u00adsider derivations where the input locations are e1,e2,e3,e4 \nand the output locations are ea,eb,ec,en. In the derivations we use the notation M R.R (T ) as a shorthand \nfor the memoization ac\u00adtion MmapA R.R (T ). Similarly we write Ax.y as a shorthand for the memoization \naction Mfx.y() of the function f mapping in\u00adteger x to letter y, whose subtrace (body) we leave unspeci.ed \nand assume to be of length constant (it contributes one to the dis\u00adtance). We de.ne the tail trace T0 \ncommon to both executions as GR3.3::R4 \u00b7A3.c \u00b7M R4.Rn (GR4.nil nil.Rn )\u00b7Pc::Rd.Rc \u00b7P. When de\u00adriving \nthe distance, we combine consecutive applications of the same rule and use the fact that the synchronization \ndistance be\u00adtween a trace and itself is (0, 0). Having derived a constant bound for this example, we \ncan generalize the result to obtain an asymptotic bound for a change in one element in the middle of \nan arbitrary list. Consider the traces T1 and T2 for mapA(L1) and mapA(L2) where L1 = [x] and L2 = nil. \nThe distance between them is trivially constant for any x. We will now use the substitution theorem to \ngeneralize this result to arbitrary lists by showing how to extend the inputs lists with identical pre.xes \nand suf.xes without affecting the constant bound. Figure 7. Code for the examples. We consider extending \nthe input with the same suf.x. We start by replacing each of the sub-traces of the form M . () for the \nrightmost call to mapA in T1 and T2 with a hole to obtain the trace contexts T1 and T2. Let L3 be any \nlist and let T3 be the trace for mapA(L3). Note that the traces T1[T3] and T2[T3] are the traces for \nmapA(L1@L3) and mapA(L2@L3). By the identity theorem, the distance between T3 and itself is (0, 0). Since \nT3 starts with memoization action of the form M Ri.Ra (...), we can apply the substitution theorem, so \nthe distance between T1[T3] and T2[T3] is equal to the distance between T1[M Ri.Ra (O)] and T3[M Ri.Ra \n(O)], which is constant. Thus, we are able to append any suf.x to L1 and L2 without increasing their \ndistance. Symmetrically, we can extend these lists with the same pre.x. To see this, let L0 be a list \nand consider its trace T0 with mapA. Now de.ne the trace context T0 as the context obtained by replacing \nthe rightmost sub-trace in T0 of the form M . () with a hole. Now, sub\u00adstitute into this trace the traces \nT1[T3] and T2[T3] (i.e., T0[T1[T3]] and T0[T2[T3]]). By the identity and the substitution theorems, the \ndistance is equal to distance between of T1[T3] and T2[T3], which is constant. Thus, we can generalize \nconcrete examples to other lists by prepending and appending arbitrary lists, essentially obtaining any \ntwo lists related by an insertion/deletion. We conclude that mapA is constant sensitive for an insertion \ninto/deletion from its input.  4.2 Reduce The list-reduce function reduces a list to a value by applying \na given binary operator with a speci.ed identity element to the el\u00adements of the list. The standard accumulator-based \nimplementa\u00adtion, reduce: ( a * a -> a) -> a -> a list -> a ref shown in Figure 7, is not amenable to \nself-adjusting compu\u00adtation, because the distance can be as large as linear. To see this note that all \nintermediate updates of the accumulator depend on the previously-seen elements. Thus replacing the .rst \nelement will prevent all derivation steps from matching, causing the distance to be linear in the size \nof the input (in the worst case). Figure 7 shows another implementation for list-reduce, called reducePair. \nThis implementation applies the function comp re\u00adpeatedly until the list is reduced to contain at most \none element. Function comp pairs the elements of the input list from left to right and applies f to each \npair reducing the size of the input list by half. Thus, comp is called a logarithmic number of times. \nUsing the shorthand chk(e) . v for derivations of the form lenLT(e) . b Gb.v, the derivations for reducePair \ncan be represented with the following derivation context. comp(R).R1 rec(R1).r1 (( chk(\u00a3) . F rec(\u00a3) \n. r1 reducePair (f, id, \u00a3) . r1 To analyze the sensitivity of reducePair for a replacement operation, \nconsider evaluating reducePair with two lists related by a replacement. The recursive case for the derivations \nboth .t the derivation context given above. Note that the derivations for comp are related by a replacement. \nSince a replacement in the input causes the output of comp to change by a replacement as well, the recursive \ncalls to rec are related by a replacement as well. Furthermore, since the derivation for comp and rec \nboth start with memoized functions, we can apply the substitution theorem assuming that the comp returns \nits output in the same location. More precisely, we can write the sensitivity of rec to a replacement \nfor an input size of n as j .rec(n/2) + .comp(n/2) if n> 1 .rec(n)= 1 otherwise Since comp uses one element \nof the input to produce one element of the output, it is relatively easy to show that is is O(1) sensitive \nto replacement when f is O(1) (i.e., .comp(m) . O(1) for any m). By straightforward arithmetic, we conclude \nthat .rec(n) . O(log n). Since reducePair simply calls rec this implies that reducePair has logarithmic \nsensitivity to a replacement.  4.3 Merge sort We analyze the sensitivity of the merge-sort algorithm \nto replace\u00adment operations. The recursive case for the derivations of msort with inputs that differ in \none element, .t the following derivation context (function names are abbreviated). len(R).b part(R).(Ra,Rb) \nms(Ra).Rc ms(Rb).Rd mg(Rc,Rd).R( Gb.F ( ((( ms (\u00a3) . \u00a3 The derivation starts with a check for the length \nof the list being greater than one. In the recursive case, the list has more than one el\u00adement so the \nlenLT function returns false. Thus, we partition the input lists into two lists ea and eb of half the \nlength, sort them to obtain ec and ed, and merge the sorted lists. Since both evalu\u00adations can be derived \nfrom this context, the distance between the derivations is the distance between the derivations substituted \nfor the holes in the context. Consider the derivations substituted for each hole. Since lenLT and part \nare called with the input, the derivations for lenLT(e1) (and part(e1)) are related by replacement. As \na result, one of ea or eb are also related by replacement. Thus only one of the derivations ms(ea) or \nms(eb) are related by replacement and the other pair is identical. Consequently mg(ec,ed) derivations \nare related by replacement. Since all contexts belong to memoized function calls, we can apply the substitution \ntheorem by assuming that all related and identical functions calls in both evaluations return their results \nin the same locations. Thus, we can write the sensitivity of msort as .msort(n) = 2.msort(n/2)+.partition(n)+.merge(n). \nIt is easy to show that partition and lenLT functions are O(1) sensitive to replacements. Similarly, \nwe can show that merge is O(1) sensitive to replacements on average, if we take the average over all \npermutations of the input list. Thus, we obtain j .msort(n/2) + 1 if n> 1 .msort(n)= 1 otherwise This \nrecurrence trivially is bounded by 1+4c log n, so we conclude that msort is O(log n)-sensitive to replacement \noperations. 4.4 Filter As an example of another program that is not naturally stable we consider a standard \nlist .lter function filter, whose code is shown in Figure 7, for which we prove that there are inputs \nwhose traces are separated by a linear distance in the size of the inputs regardless of the choice of \nlocations. In other words, we will prove a lower bound for the sensitivity of filter. The reason for \nwhich filter is not stable is similar to that of the conventional implementation of reduce (Section 4.2), \nbut more subtle because it is primarily determined by the choice of locations rather than the computation \nbeing performed. To see why filter can be highly sensitive, it suf.ces to con\u00adsider a specialization, \nwhich we call filter0, that only keeps the nonzero elements. For example, with input lists L = [0,0,0] \nand L = [0,0,1], filter0 returns nil and [1], respectively. Since we are interested in proving a lower \nbound only, we can summa\u00adrize traces by including function calls and put operations only the omitted \nparts of the trace will affect the bound by a constant factor assuming that the .ltering functions takes \nconstant time. In par\u00ad ticular, using the shorthand M R.R (T ) for the memoization action Mfilter0 R.R \n(T ), the traces for .lter with L and L are respectively: R1.Rn (M R2.Rn (M R3.Rn (M R4.Rn (P M nil.Rn \n)))), and R1.Ra (M R2.Ra (M R3.Ra (M R4.Rn (Pnil.Rn )\u00b7P M1::Rn.Ra ))). Note that the distance between \nthese two traces is greater than 3 the length of the input because in the second trace three memo\u00adization \nactions return the location ea holding [1], whereas in the .rst trace en is returned. Since these locations \nare different, the memoization actions do not match and contribute to the distance. This example does \nnot lead to a lower bound, however, because we can give two traces for the considered inputs for which \nthe distance is one, e.g.,: R1.Rn (M R2.Rn (M R3.Rn (M R4.Rn (P M nil.Rn )))), and R1.Rn (M R2.Rn (M \nR3.Rn (M R4.R n (Pnil.R 1::R n M n )\u00b7P.Rn ))). The idea is to choose the locations in such a way that \nthe traces overlap maximally. It is not dif.cult to generalize this example for arbitrary lists of the \nform [0,...,0,0] and [0,...,0,1]. We obtain the worst-case inputs by modifying this example to prevent \nlocation choices from reducing the distance arbitrarily. Consider parameterized lists of the form L1(n) \n= [(0)n,0,(0)n] and L2(n) = [(0)n,1,(0)n], where 0n denotes n repeated 0 s. We will show that the distance \nbetween traces for any two such inputs is at least n +1 and thus linear in the size of the input, 2n \n+1. For example, the traces for L1(1)= [0,0,0] and L2(1) = [0,1,0] have the form: R1.Rn (M R2.Rn (M R3.Rn \n(M R4.Rn (P M nil.Rn )))), and R1.Ra (M R2.Ra (M R3.Rn (M R4.Rn (Pnil.Rn ))\u00b7P1::Rn M .Ra )). These traces \nhave distance greater than 2. Regardless of how we change the locations this distance will not decrease \nbecause the return locations of n memoization actions before and after the occurrence of 1 will have \nto differ. Thus, regardless of which location the other trace chooses to store the empty list, at least \nhalf the calls will have a differing location. We can generalize this example with n =3 to arbitrary \nlists by using our identity and substitution theorems as we did for the map example. Since the approach \nis essentially the same as with map, we leave it out here. Thus, we conclude that filter is O(n)-sensitive \nto a replacement. This example implies that a self-adjusting computation can do poorly with this implementation \nof filter. As with reduce, how\u00adever, we can give a stable implementation of filter by using a compress \nfunction similar to comp of reducePair that applies the .lter function to half of the remaining un.ltered \nelements. We can show that this implementation of filter is O(log n) sensitive un\u00adder suitable choice \nof locations. 5. The Target Language (Tgt) The Tgt language is a simply-typed, call-by-value .-calculus \nwith natural numbers and recursive functions, extended with mod\u00adi.able references and a memoization primitive. \nThe language is self-adjusting: its semantics includes evaluation and change\u00adpropagation judgements that \ncan be used to reduce expressions to values and adapt computations to input changes. Tgt extends the \nread-only modi.ables of (Ley-Wild et al. 2008b) with imperative update, a cost semantics for evaluation \nand change propagation, and a notion of trace distance. The syntax of Tgt is given below, which de.nes \ntypes t, expres\u00adsions e, values v, and adaptive commands ., using metavariables f S; G f v : t S; G f \nvl : t mod S; G f vk : t mod . res S; G f vk : t . res S; G f putk vvk : res S; G f getk vl vk : res \nS; G f vl : t mod S; G f v : t S; G f vk : nat . res S; G f setk vl vvk : res S; G f e : res S; G f v \n: t S; G f memo e : res S; G f halt v : res Figure 8. Tgt typing S; G f e : t (fragment). and x for \nidenti.ers and e for locations. t ::= nat | tx . t | t mod | res e ::= v | caseN vn ez (x.es) | ef vx \nv ::= x | . | zero | succ v | fun f.x.e | \u00a3 . ::= putk vvk | getk vR vk | setk vR vvk | memo e | halt \nv def . x.e = fun f.x.e with f/. FV(e) Tgt enforces a continuation-passing style (cps) discipline to \nhelp identify opportunities for reuse and computations for re-execution. Adaptive store commands have \nan explicit continuation vk iden\u00adtifying the computation that follows the command. The cps dis\u00adcipline \nalso restricts a function application ef vx to have a value argument. Modi.ables t mod are mutable references \nwith adap\u00adtive store commands putk, getk, and setk for allocation, deref\u00aderence, and update. The type \nres is an opaque answer type, while halt is a continuation that injects a .nal value into the res type. \n5.1 Static, Dynamic, and Cost Semantics Figure 8 gives a fragment of the static semantics of Tgt. The \ntyping judgement S; G f e : t ascribes the type t to the expression e in the store and variable typing \ncontexts S and G; the omitted rules are standard. Figure 9 gives the dynamic semantics. Evaluation uses \nand produces a trace T as a sequence of adaptive (store and memo) actions A, ending in a halt action: \nv.R As ::= Pvk | GR.v | SR.v vk vk A ::= As | Me T ::= Hv | A\u00b7T . T ::= .| T The large-step evaluation \nrelation T.; s; e .E T ; s ; v ; d (resp. T.; s; k .K T ; s ; v ; d ) reduces the expression e (resp. \nthe adaptive command .) under the store s, yielding the value v and the updated store s ; evaluation \nalso takes an (optional) reuse trace T. from a previous run, and produces an execution trace T for the \ncurrent run and a pair of costs d = (c, c ) for work c discarded from the reuse trace and new work c \nperformed for the current run. The auxiliary evaluation relation e . v reduces an expression e to a value \nv , independent of the store. The halt v command yields a computation s .nal value, with a cost of 1 \nfor the current run and a cost c = |T. | for work discarded from the reuse trace T., where the cost of \nan optional trace is: |.| =0 |Hv| =1 |A\u00b7T | =1+ |T | An adaptive store command uses the store (putk, \ngetk, and setk rules) and delivers the result to the continuation; the trace is extended with the corresponding \nstore action labeled by the location, value, and continuation involved, and incurs a cost of 1 for the \ncurrent run. A memoized expression memo e in Tgt has no special behavior when evaluated from scratch \n(memo/miss rule): it evaluates the body e and extends the trace with a memo action Me, incurring a cost \nof 1 for the current run. The memo/hit rule exploits the reuse trace and switches to change propagation. \nef . fun f .x.e v . v ez . v caseN zero ez (x.es) . v {vn / x} es . v caseN (succ vn) ez (x.es) . v {vx \n/ x} {fun f .x.e / f } e . v ef vx . v e . . \u00a3/. dom(s) sl = s l{\u00a3 . v} .. T ; s; . .T ; s ; v ; d \n|T.| = cT ; sl; vk \u00a3 .T ; s ; v ; d KE v.R T.; s; e .T ; s ; v ; dT.; s; halt v .Hv; s; v; (c, 1) T.; \ns; putk vvk .P\u00b7T ; s ; v ; (0, 1) + d EK K vk \u00a3 . dom(s) s(\u00a3)= v\u00a3 . dom(s) sl = s[\u00a3 . v] .. T ; s; vk \nv .E T ; s ; v ; dT ; sl; vk zero .E T ; s ; v ; d .GR.v SR.v T ; s; getk \u00a3vk .\u00b7T ; s ; v ; (0, 1) + \ndT.; s; setk \u00a3vvk .\u00b7T ; s ; v ; (0, 1) + d K vk K vk m T.; s; e .T ; s ; v ; dT ; e } Te; cTe; s T ; \ns ; v ; d E memo/miss memo/hit T.; s; memo e .Me\u00b7T ; s ; v ; (0, 1) + dT ; s; memo e .Me\u00b7T ; s ; v ; \n(c, 1) + d KK . Figure 9. Tgt reduction e . v and evaluation T ; s; . .K T ; s ; v ; d and T.; s; . \n.E T ; s ; v ; d . \u00a3/. dom(s) sl = s l{\u00a3 . v} \u00a3 . dom(s) s(\u00a3)= v\u00a3 . dom(s) sl = s[\u00a3 . v] T ; sl T ; s \n; v ; dT ; s T ; s ; v ; dT ; sl T ; s ; v ; d put/reuse get/reuse set/reuse v.Rv.R GR.v GR.v SR.v SR.v \nP\u00b7T ; s P\u00b7T ; s ; v ; d \u00b7T ; s \u00b7T ; s ; v ; d \u00b7T ; s \u00b7T ; s ; v ; d vk vk vk vk vk vk T ; s T ; s ; v \n; d iT l = .T ; s; . .T ; s ; v ; d K memo/reuse change Me\u00b7T ; s Me\u00b7T ; s ; v ; d Hv; s Hv; s; v; (0, \n0) T ; s T ; s ; v ; d Figure 10. Tgt change propagation T.; s i s ; v ; T ; d . m The memoization judgement \nT ; e } Te; c .nds a trace Te that corresponds to a previous run of e (under a (possibly) different store), \nincurring a cost c for discarding the pre.x of T up to Te: m T ; e . Te; c mm Me\u00b7T ; e } T ;1 A\u00b7T ; e \n} Te;1+ c The change propagation relation T ; s i T ; s ; v ; d (given in Figure 10) replays the execution \ntrace T under the store s, yielding the value v and the updated store s , with an updated execution trace \nT and a pair of costs d = (c, c ) for work c discarded from T and new work c performed for T . A halt \naction can be replayed without cost to obtain the (unchanged) .nal value. An adaptive action can be replayed \nwithout cost if the action is consistent with the current store (reuse rules), the tail of the trace \ncan be recursively change propagated and then extended with the same action. However, if a store action \nis inconsistent with the store (e.g., a speci.c location can t be allocated), then change propagation \nmust switch back to evaluation. Since adaptive actions capture their continuation, a trace T can be rei.ed \nback into an adaptive command iT l that represents the rest of the computation: v.R iP\u00b7T l = putk vvk \niMe\u00b7T l = memo e vk iGR.v\u00b7T l = getk \u00a3vk iHvl = halt v vk iSR.v\u00b7T l = setk \u00a3vvk vk Thus, change propagation \ncan reify and re-evaluate an inconsistent trace T (change rule), while keeping the trace T for possible \nreuse later. Note that the rei.ed putk (resp. getk) forgets the (stale) lo\u00adcation (resp. value). The \nchange rule does not, however, require the action to be inconsistent; this nondeterminism intentionally \navoids committing to particular allocation and memoization policies.  5.2 Consistency of Change Propagation \nSuppose we have a Tgt program e such that S; \u00b7f e : res and an initial store s1 such that f s1 :S l S1. \nWe can evaluate e under the store s1 and no reuse trace, yielding the initial result v1 and a trace T1: \n.; s1; e .E T1; s1; v1; d1. After this initial evaluation, we can consider another store s2 such that \nf s2 :S l S2 and update the output of the evaluation with respect to this store by applying change propagation \nto T1 under the store s2: T1; s2 i T2; s2; v2; d2. The consistency of change propagation asserts that \nthe result and trace obtained by change propagation are identical to those obtained by evaluation (recall \nthe bottom left square of Figure 1). We prove this consistency property for Tgt by giving a simple structural \nproof. Theorem 4 (Consistency of Change propagation) If .; s1; e .E T1; s1; v1; and T1; s2 i T2; s2; \nv2; , then .; s2; e .E T2; s2; v2; . If .;; .E T1;;; and T1; s2; e .E T2; s2; v2; , then .; s2; e .E \nT2; s2; v2; . Proof: The theorem must be strengthened with analogous state\u00adments for the .K relation. \nBy simultaneous induction on the second derivation of each statement. Proved in Twelf. . Recent work \ngave a similar consistency theorem, but with a dif\u00adferent language (Acar et al. 2008a). Compared to that \nwork, our proof is dramatically simpler. We achieve this by casting change propagation as a full replay \nmechanism that can re-allocate loca\u00adtions. In previous work, it was not possible to express change prop\u00adagation \nas a full replay mechanism change propagation could not re-allocate locations allocated in a previous \nrun. This required argu\u00ading that the results obtained by change propagation and evaluation are isomorphic \nby using step-indexed logical relations. 5.3 Trace Distance Reasoning about computation reuse achieved \nby change propaga\u00adtion is dif.cult. In this section, we introduce a notion of trace dis\u00adtance and show \nthat the cost of change propagation may be bounded by the distance between the input and the result traces. \nThe de.ni\u00adtion of distance is similar to the source at a high level. Indeed, in Section 6 we show that \nthey are asymptotically the same. As in Src, we de.ne a search distance T1ET2 = d that accounts for differences \nbetween traces until it .nds matching memoization actions, at which point it can use the synchronization \ndistance T1 eT2 = d that accounts for reuse between traces until they differ, T1 8 T2 = dT1 E T2 = dT1 \nE T2 = d Hv1 E Hv2 = (1, 1) Me\u00b7T1 E Me\u00b7T2 = (1, 1) + dA\u00b7T1 E T2 = (1, 0) + dT1 E A\u00b7T2 = (0, 1) + d T1 \n8 T2 = dT1 E T2 = d Hv 8 Hv = (0, 0) A\u00b7T1 8 A\u00b7T2 = dT1 8 T2 = d Figure 11. Tgt trace search distance \nT1 E T2 at which point it must return to the search distance. The distance d = (c1,c2) quanti.es the \ncost c1 of work in T1 that isn t shared with T2 and the cost c2 of work in T2 that isn t shared with \nT1. The search distance (given in Figure 11) between halt actions is 1 for each action, irrespective \nof the value returned. Two identical memo actions incur a cost of 1 each, but afford the possibility \nof switching from search to synchronization mode. Skipping an action incurs a cost of 1 for the corresponding \ntrace and forces distance to remain in search mode. Note that these last two rules allow memo actions \nto remain in search mode; identical memo actions are never forced to synchronize. Synchronization distance, \nas in Src, is only meant to be used on traces generated by the evaluation of the same expression under \n(possibly) different stores (though, there exists a synchronization distance between any two traces). \nThe synchronization distance between halt actions is (0, 0), and assumes both actions return the same \nvalue. Identical adaptive actions match without cost and allow distance to continue synchronizing the \ntail. Synchronization may return to search mode, either nondeterministically or because adaptive actions \ndon t match. Just as Src distance, Tgt distance judgements are quasi-symmetric and induce a ternary relation \ndue to the nondeterminism of memo matching. In light of the dynamic semantics, trace distance can be \ngiven an asymmetrical operational interpretation: the distance is the amount of work that must be discarded \nfrom one run and executed to produce the other run. (Intuitively, the asymmetry arises from the fact \nthat discarding work, while not free, is cheaper than performing work.) In particular, search distance \nhas an operational analogue realized by evaluation, while synchronization distance is realized by change \npropagation. A distance (c1,c2) between traces T1 and T2 intuitively means there is cost c1 for discarding \nunusable work from the reuse trace T1 and cost c2 for performing new work for T2, but any common work \nthat can be reused is free. This relation between distance and the dynamic semantics is formally captured \nby the following theorem (recall the bottom right diagram of Figure 1). Theorem 5 (Dynamic semantics \ncoincides with distance) If .; s1; e1 .E T1; s1; v1; and .; s2; e2 .E T2; s2; v2; , then T1 E T2 = d \niff T1; s2; e2 .E T2; s2; v2; d. If .; s1; e .E T1; s1; v1; and .; s2; e .E T2; s2; v2; , then T1 e T2 \n= d iff T1; s2 i T2; s2; v2; d. Proof: The theorem must be strengthened with analogous state\u00adments for \nthe .K judgement. By simultaneous induction on the second derivation of each statement. Proved in Twelf. \n. 6. Translation Program Translation. The adaptive primitives of Src programs are used to guide an adaptive \ncontinuation-passing style (ACPS) transformation into equivalent Tgt programs (given in Figure 12). The \ntype translation [t s= tt preserves the nat type, converts ] the function type to take a continuation \nargument, and converts the reference type to a modi.able type. The expression and value translations \n[e st = e t and [v sv t (the former using the Tgt k t ] v ] = value vk as an explicit continuation) are \nstandard cps conversions = d and synchronization distance T1 e T2 = d. [nat] = nat [tx . t] = [tx] . \n([t] . res) . res [t ref] = [t] mod = [v] vk vk [v] [caseN vn ez (x.es)] vk = caseN [vn] ([ez] vk)(x \n. [es] vk) [ef ex] vk [ef ] (. yf . = [ex] (. yx.(yf yx) vk)) [put v] vk = putk [v] vk [get vl] vk = \ngetk [vl] vk [set vl v] vk = setk [vl][v] vk = x [x] [zero] = zero [succ v] = succ [v]= \u00a3 [\u00a3] [fun f \n.x.e] = fun f .x.. yk. putk (. yr.memo (yk yr)) (. yl.memo ([e] (. yr.getk yl (. yk.yk yr)))) Figure \n12. Type translation [t s= t t (top) and term translations st s] [e ] vk = e t and [v ] = v t (bottom). \nfor natural numbers, while reference primitives are translated into Tgt adaptive store commands with \nan explicit continuation vk. The value translations (except for functions) are straightforward. The halt \nexpression is not in the image of the translation, but it can be used as an initial identity continuation \nid = . x.halt x for evaluating a cps-converted program. The metavariable y is used to distinguish identi.ers \nintroduced by the translation. The type translation is extended pointwise to Src store and variable typing \ncontexts S and G; the value translation is extended pointwise to Src stores s. The cps discipline in \nTgt facilitates identifying the scope of an adaptive store action as the rest of the computation, so \nchange propagating an inconsistent store action will re-execute the tail of the trace. Memoizing a function, \nhowever, in the presence of (pos\u00adsibly different) continuations and store mutation is subtle and cru\u00adcially \nrelies on two ideas: threading continuations through the store, and using explicit memo operations before \nand after the func\u00adtion body. First, the translation treats the continuation as change\u00adable data by threading \nit through the store during the function call (viz. putk in the function body and getk in the continuation). \nThis effectively shifts the continuation to the store, so the function call can memo match on its argument \neven if its continuation dif\u00adfers (provided the same location is used to store the continuation as in \nthe previous run). After the function body is change propagated without cost, the (new) continuation \nwill be resumed by reading it from the store and passing it the memoized result. Second, the translation \ninserts memo commands at the function call and return points in an attempt to isolate reuse of the function \nbody separately from reuse of the rest of the computation. Thus the continuation can memo match if the \nresult is the same, even if the function body had to re-execute due to an inconsistent store interaction. \nThe correctness and ef.ciency of the translation is captured by the fact that well-typed Src programs \nare compiled into (stat\u00adically and dynamically) equivalent well-typed Tgt programs with the same asymptotic \ncomplexity for initial runs (i.e., Tgt evalua\u00adtion with an empty reuse trace). Type preservation is standard \nand elided for reasons of space. More importantly, the evaluation and asymptotic cost of from-scratch \nruns of Src programs is preserved by compilation (recall the top right diagram of Figure 1). Theorem \n6 (Translation preserves extensional/intensional) If E; s0; e0 . s1; v1; T ; c0, and .; [s1] l sk; vk \n[v1] .E s2; v2; Tk; ( ,c1), then .; [s0] l sk; [e0] vk .E s2 l se; v2; T ; ( ,c2) and c0 + c1 = c2 = \n4c0 + c1 whence c2 . T(c0 + c1). Proof: By induction on the .rst derivation. . The store sk accounts \nfor locations free in the continuation vk, while the store se accounts for locations allocated for (the \ncontinuations of) memoizing functions. Instantiating this theorem with the identity continuation vk = \nid, we have that evaluation of a Src program coincides with (from-scratch) Tgt evaluation of its ACPS-translation. \nFurthermore, the adaptive work c2 . T(c0) in Tgt is proportional to the active work c0 in Src, because \nthe work of the identity continuation is constant. This means that the translation preserves the asymptotic \ncomplexity of from-scratch runs. Trace Translation. The Tgt trace of an ACPS-compiled Src pro\u00adgram is \nricher than its Src counterpart because Tgt traces have ex\u00adplicit continuations and the ACPS translation \nintroduces adminis\u00adtrative redices, threads continuations through the store, and inserts memoization \nat function call and return points. The Src dynamic semantics and distance, however, are suf.ciently \ninstrumented to translate Src traces into equivalent Tgt traces. An explicit Src evaluation context E \nis suf.cient to reify the current continuation [E] vk relative to an initial Tgt continuation vk: vk \n= vk E[eO x vk = E (. yf . [ex] (. yx.(yf yx) vk))[ vf E] vk [ E] (. yx.([vf ] yx) vk) = Moreover, since \nactive Src actions are instrumented with their local evaluation context, we can give a trace translation \n[T s] vk t Tk t of Src trace T s using the vk t as an initial continuation (to extend the local context \nE of actions) and suf.x Tkt . The translation of the empty trace and store actions is straightforward: \n= [PvE.R \u00b7T[e]] vvkk TTkk = TP[k vI.R \u00b7([T ] vk Tk) [EI vk[GR.vR.[vI \u00b7T ] vk Tk = G\u00b7([T ] vk Tk) EE I \nvk[SR.v\u00b7T ] vk Tk = S[R.[vI\u00b7([T ] vk Tk) E [EI vk Since a failure action is inserted at a function s \nreturn point, it is translated to the trace that follows the evaluation of a function body (cf., ACPS \nfunction translation): .v ] R.kw \u00b7M(([EI vk) [vI) [FE \u00b7Tvk Tk = G\u00b7([T ] vk Tk) ka where kw = . yr.memo \n(([E] vk) yr) ka = . yk.yk [v]Note that kw is the memoizing version of the original continuation that \nwas written to the store before the evaluation of the body and ka is the continuation of the getk command \nthat fetches the memoizing version of original continuation. The translation of a memoizing function \naction must account for writing the memoizing version of the original continuation to the store before \nmemoizing on the evaluation of the body: [M(fun f .x.e) vx.v(T )\u00b7T ] vk Tk = Pkw.R \u00b7M([e ] kr)\u00b7([T ] \nkr Tr) E km where kw = . yr.memo (([E] vk) yr) km = . yl.memo ([e ] (. yr.getk yl (. yk.yk yr))) e = \n{fun f .x.e / f }{vx / x} e kr = . yr.getk \u00a3 (. yk.yk yr) Tr = [F.v \u00b7T ] vk Tk E Note that kr is the \ncontinuation that fetches and invokes the mem\u00adoizing version of the original continuation; this is the \ncontinuation that is passed to the body. The body of the memoizing function ac\u00adtion is translated with \nrespect to kr and Tr, which is the translation of a failure action. Trace translation is syntax-directed, \nexcept for the choice of locations for continuations of memoizing functions; below we specify how these \nlocations are chosen. Given the trace translation, Theorem 6 can be strengthened to show that the if \nthe continuation vk is of the form [E] vk, then the Tgt evaluation trace T is [T ] vk Tk. Finally, Src \ndistance may be related to Tgt distance by trace translation (recall top right diagram of Figure 1). \nTheorem 7 (Src/Tgt distance soundness) Assume Tkt 1 e T t = ( ,c1), T t E T t = ( ,c2). k2 k1 k2 If \nT1 E T2 = ( ,c), then ([T1] vk t Tkt 1) E ([T2] vk t Tkt 2)= ( ,c ) and c = c = 4c + max{c1,c2}. If T1 \ne T2 = ( ,c), then ([T1] vkt 1 Tkt 1) e ([T2] vkt 2 Tkt 2)= ( ,c ) and c = c = 4c + max{c1,c2}. Proof \n(sketch): We de.ne a variant of Src s distance relation with precise accounting for memoization at function \ncall and re\u00adturn points. We show that the original Src distance bounds the precise Src distance by a \nconstant factor (the original version uses amortization to avoid precise accounting and to simplify reason\u00ading). \nNext, we preprocess the precise Src distance derivation by assigning matching fresh locations to memoization \nactions that synchronize, these are used by the trace translation for continua\u00adtions (this is always \npossible because stores and traces are .nite). Finally, we proceed by induction on the (instrumented) \nprecise Src distance derivation, using the trace translation to build an equiva\u00adlent Tgt distance derivation. \n. Corollary 8 (Src/Tgt distance soundness) Let T t be the identity continuation trace for Ti (i .{1, \n2}). idi If T1 E T2 = ( ,c), then ([T1] id T t id2)= ( ,c . T(c). id1) E ([T2] id T t ) and c If T1 \ne T2 = ( ,c), then ([T1] id T t id2)= ( ,c ) and c . T(c). id1) e ([T2] id T t Proof: The search distance \nT t id2 and synchronization dis\u00ad id1 E T t tance Tidt 1 id2 between the identity continuation traces \nare e T t constant, therefore the asymptotic bound c . T(c) follows by Theorem 7. . Note that since Src \nand Tgt distance are quasi-symmetric, an analogous theorem and corollary hold of the left component of \ndis\u00adtance. This means that change propagation has the same asymptotic time-complexity as trace distance. \nThe converse of the theorem does not hold: a distance may be derivable of Tgt traces which does not correspond \nto any derivable Src distance. This incompleteness is to be expected because memoization of a function \ncall and re\u00adturn in Tgt need not match in lock-step, whereas the synch/memo (resp. synch/search) Src \nrule requires both (resp. neither) to match in lock-step. 7. Discussion We brie.y remark on some limitations \nof our approach. Incompleteness. Soundness of the translation guarantees that any distance derivable \nin Src is also (up to a constant factor) derivable in Tgt. However, the Tgt proof system exhibits more \npossible dis\u00adtances: in Src, memoization requires matching both the function call and return points, \nwhile the ACPS translation into Tgt distin\u00adguishes memoization at the call and the return. Therefore, \nthere are more opportunities for switching between search and synchroniza\u00adtion in Tgt and there may be \nmore distance values derivable in Tgt than in Src. For example, in Tgt a function call memoization can \nmiss (i.e., remain in search mode) while the return can match (i.e., switch from search to synchronization \nmode), which is not possi\u00adble in Src. Consequently, any upper bound found using Src distance is preserved \nby compilation, but lower bound arguments on a Src program are not necessarily lower bounds on the Tgt \ndistance. Nondeterminism. The dynamic semantics and distance of Src and Tgt programs are nondeterministic \ndue to the freedom in choosing locations as well as deciding when memoization matches. This avoids having \nto commit to a particular implementation, but also means that any upper bound derived using the nondeterminis\u00adtic \nsemantics may not be realized by a particular implementation. In order for an implementation to realize \nan upper bound on distance, the allocation and memoization policies used in deriving the dis\u00adtance must \ncoincide with those of the implementation. In previous work (Ley-Wild et al. 2008b), we proposed both \nuser-speci.ed and compiler-generated mechanisms for de.ning allocation and mem\u00adoization policies, which \nsuf.ce for realizing the bounds derived in our examples. Ultimately, it would be useful to develop compilation \nand run-time techniques to automatically minimize the distance be\u00adtween the computations by considering \nall possible policies. Meta-logic. The proof system for distance applies to concrete traces, while in \nour examples we use it to reason schematically over classes of contexts and input changes. To fully formalize \nthe exam\u00adples, we would need a meta-logic that permits quanti.cation over contexts and classes of input \nchanges, and can express asymptotic bounds. Such a meta-logic could be extended with theorem-proving \ncapabilities which could automate .nding bounds on distance. 8. Related Work We brie.y review previous \nwork on incremental computation and cost semantics. Incremental and Self-Adjusting Computation Incremental \ncom\u00adputation has been studied extensively since the early 80 s. We brie.y mention a few approaches here \nand refer the reader to the survey by Ramalingam and Reps (1993) and some recent papers (e.g., Ley-Wild \net al. 2008b) for a more detailed set of references. Effective early approaches to incremental computation \neither use dependence graphs (Demers et al. 1981; Reps 1982; Yellin and Strom 1991) or memoization (e.g., \nPugh and Teitelbaum 1989; Abadi et al. 1996; Heydon et al. 2000). Self-adjusting computa\u00adtion generalized \ndependence graphs techniques by introducing dy\u00adnamic dependence graphs (Acar et al. 2006b), which enables \na change propagation algorithm update the structure of the computa\u00adtion based on data modi.cations, and \ncombining them with memo\u00adization (Acar et al. 2006a). Recent work showed that the approach can be generalized \nto support imperative updates (side effects to memory) (Acar et al. 2008a). Ley-Wild et al 2008b described \nhow to incorporate a version of the compilation technique used in this paper for a pure source language \ninto an existing compiler (ML\u00adton). That paper did not consider mutable references and provided no cost \nsemantics or effectiveness guarantees. Researchers proposed several implementations of self-adjusting \ncomputation. Carlsson (2002) present a Haskell implementation of the initial proposal to self-adjusting \ncomputation (Acar et al. 2006b). Shankar and Bodik 2007 use a variant of self-adjusting computation techniques \nfor the purpose of incremental invari\u00adant checking. Cooper and Krishnamurthi (Cooper and Krishna\u00admurthi \n2006) adapt the initial proposal for self-adjusting com\u00adputation (Acar et al. 2006b) to support Functional \nReactive Pro\u00adgramming (Elliott and Hudak 1997). Both approaches are sim\u00adilar to an alternative formulation \nof self-adjusting computation based on tracking dependences at the granularity of function calls and \nmemory locations that is described in the .rst authors the\u00adsis (Acar 2005). Shankar and Bodik s approach \nis further special\u00adized for incremental invariant checking and is unsound in the gen\u00aderal case: change \npropagation does not preserve the intensional (performance) and extensional (input-output behavior) semantics \nwith respect to from-scratch runs. These implementations all as\u00adsume purely functional programming (except \nfor the mutator) and often require support from a higher-order language (e.g., ML, Haskell, Scheme). \nRecent work made some progress on giving an implementation of self-adjusting computation in lower-level \nlanguages, C in particular (Hammer and Acar 2008). Self-adjusting computation has been applied, in several \nincar\u00adnations, to a number of problems from a reasonably broad set of application domains such as motion \nsimulation (Acar et al. 2006c, 2008b), machine learning (Acar et al. 2007), and other algorithmic problems \n(Acar et al. 2004, 2005, 2006a). It is possible to ana\u00adlyze the performance of change propagation for \na particular prob\u00adlem by using algorithmic analysis techniques. For example, ear\u00adlier work (Acar et al. \n2004) analyzed the performance of change propagation for tree contraction problem. Most applications \nof self\u00adadjusting computation, however, evaluated the effectiveness of the approach experimentally (e.g., \nAcar et al. 2006a). The examples that we consider in this paper con.rm these experimental .ndings. Cost \nSemantics This work builds on previous work on pro.l\u00ading or cost semantics for reasoning about resource \nrequirements of programs. The idea of instrumenting evaluations to generate cost information goes back \nto the early 90s (Sands 1990a; Rosendahl 1989). The approach has been shown to be particularly important \nin high-level languages such as lazy (e.g., Sands 1990a,b; Sansom and Jones 1995) and parallel languages \n(e.g., Blelloch and Greiner 1995, 1996; Spoonhower et al. 2008) where it is particularly dif.\u00adcult to \nrelate execution time to the source code. The idea of having a cost semantics construct a trace resembles \nthe techniques used for evaluation of parallel programs (Blelloch and Greiner 1996; Spoon\u00adhower et al. \n2008). The structure and use of our traces, however, dif\u00adfers signi.cantly from those used in parallel \nlanguages: we record store actions and compute distances, whereas they work in a pure setting and use \ntraces to reason about parallelism. In the context of incremental computation, we know of no other work \nthat offers a source-level cost semantics for reasoning about effectiveness of incremental update mechanisms. \n9. Conclusion Due to its complex semantics and the nature of previously pro\u00adposed linguistic facilities, \nreasoning about the effectiveness of self\u00adadjusting programs has been dif.cult, forcing previous work \nto re\u00adsort to experimental validation. This paper gives a high-level cost semantics for self-adjusting \ncomputation. The approach enables programming in a familiar set\u00adting, .-calculus with .rst-class references, \nand compiling such pro\u00adgrams into self-adjusting programs. The user can determine the re\u00adsponsiveness \nof compiled self-adjusting programs by computing a kind of edit distance between traces of source programs. \nThese traces consists of function calls and individual store operations. The user need not reason about \nevaluation contexts or global state. These results are made possible by (1) a compilation mechanism that \ncan translate ordinary code into self-adjusting code while pre\u00adserving its ef.ciency, and (2) by techniques \nfor matching evalua\u00adtion contexts appropriately without exposing them to the user for source-level reasoning. \nA common limitation of cost semantics-based approaches to performance analysis is that they often apply \nonly to concrete evaluations. We show that this need not be the case by providing techniques for generalizing \ntrace distances of concrete evaluations to arbitrary inputs, composing trace distances, and by reasoning \nwith trace contexts. For illustrative purposes, we derive asymp\u00adtotic bounds for several examples. We \nexpect these results to lead to a more formal and precise reasoning of effectiveness of self\u00adadjusting \nprograms as well as pro.ling tools that can infer concrete and perhaps asymptotic complexity bounds. \nReferences Mart\u00b4in Abadi, Butler W. Lampson, and Jean-Jacques L\u00b4evy. Analysis and Caching of Dependencies. \nIn Proceedings of the International Confer\u00adence on Functional Programming (ICFP), pages 83 91, 1996. \nUmut A. Acar. Self-Adjusting Computation. PhD thesis, Department of Computer Science, Carnegie Mellon \nUniversity, May 2005. Umut A. Acar, Guy E. Blelloch, Robert Harper, Jorge L. Vittes, and Mav\u00aderick Woo. \nDynamizing static algorithms with applications to dynamic trees and history independence. In ACM-SIAM \nSymposium on Discrete Algorithms (SODA), 2004. Umut A. Acar, Guy E. Blelloch, and Jorge L. Vittes. An \nexperimental analysis of change propagation in dynamic trees. In Workshop on Algorithm Engineering and \nExperimentation (ALENEX), 2005. Umut A. Acar, Guy E. Blelloch, Matthias Blume, and Kanat Tangwongsan. \nAn experimental analysis of self-adjusting computation. In Proceedings of the ACM SIGPLAN Conference \non Programming Language Design and Implementation (PLDI), 2006a. Umut A. Acar, Guy E. Blelloch, and Robert \nHarper. Adaptive functional programming. ACM Transactions on Programming Languages and Systems (TOPLAS), \n28(6):990 1034, 2006b. Umut A. Acar, Guy E. Blelloch, Kanat Tangwongsan, and Jorge L. Vittes. Kinetic \nAlgorithms via Self-Adjusting Computation. In Proceedings of the 14th Annual European Symposium on Algorithms \n(ESA), pages 636 647, September 2006c. \u00a8 tive Bayesian Inference. In Neural Information Processing Systems \n(NIPS), 2007. Umut A. Acar, Alexander Ihler, Ramgopal Mettu, and Ozg\u00a8ur S\u00a8umer. Adap- Umut A. Acar, Amal \nAhmed, and Matthias Blume. Imperative self\u00adadjusting computation. In Proceedings of the 25th Annual ACM \nSym\u00adposium on Principles of Programming Languages (POPL), 2008a. Umut A. Acar, Guy E. Blelloch, Kanat \nTangwongsan, and Duru T\u00a8glu.urko.Robust Kinetic Convex Hulls in 3D. In Proceedings of the 16th Annual \nEuropean Symposium on Algorithms (ESA), September 2008b. \u00a8 Umut A. Acar, Alexander Ihler, Ramgopal Mettu, \nand Ozg\u00a8umer. Adap\u00adur S\u00a8tive Inference on General Graphical Models. In Uncertainty in Arti.cial Intelligence \n(UAI), 2008c. Pankaj K. Agarwal, Leonidas J. Guibas, Herbert Edelsbrunner, Jeff Er\u00adickson, Michael Isard, \nSariel Har-Peled, John Hershberger, Christian Jensen, Lydia Kavraki, Patrice Koehl, Ming Lin, Dinesh \nManocha, Dim\u00aditris Metaxas, Brian Mirtich, David Mount, S. Muthukrishnan, Dinesh Pai, Elisha Sacks, Jack \nSnoeyink, Subhash Suri, and Ouri Wolefson. Al\u00adgorithmic issues in modeling motion. ACM Comput. Surv., \n34(4):550 572, 2002. Guy Blelloch and John Greiner. Parallelism in sequential functional lan\u00adguages. \nIn FPCA 95: Proceedings of the seventh international confer\u00adence on Functional programming languages \nand computer architecture, pages 226 237, 1995. ISBN 0-89791-719-7. Guy E. Blelloch and John Greiner. \nA provable time and space ef.cient implementation of nesl. In ICFP 96: Proceedings of the .rst ACM SIGPLAN \ninternational conference on Functional programming, pages 213 225. ACM, 1996. Magnus Carlsson. Monads \nfor Incremental Computing. In Proceedings of the 7th ACM SIGPLAN International Conference on Functional \npro\u00adgramming (ICFP), pages 26 35. ACM Press, 2002. Y.-J. Chiang and R. Tamassia. Dynamic algorithms in \ncomputational ge\u00adometry. Proceedings of the IEEE, 80(9):1412 1434, 1992. Gregory H. Cooper and Shriram \nKrishnamurthi. Embedding Dynamic Data.ow in a Call-by-Value Language. In Proceedings of the 15th Annual \nEuropean Symposium on Programming (ESOP), 2006. Alan Demers, Thomas Reps, and Tim Teitelbaum. Incremental \nEvaluation of Attribute Grammars with Application to Syntax-directed Editors. In Proceedings of the 8th \nAnnual ACM Symposium on Principles of Programming Languages, pages 105 116, 1981. Conal Elliott and Paul \nHudak. Functional Reactive Animation. In ICFP 97: Proceedings of the second ACM SIGPLAN international \nconference on Functional programming, pages 263 273. ACM, 1997. David Eppstein, Zvi Galil, and Giuseppe \nF. Italiano. Dynamic graph algorithms. In Mikhail J. Atallah, editor, Algorithms and Theory of Computation \nHandbook, chapter 8. CRC Press, 1999. Leonidas J. Guibas. Kinetic data structures: a state of the art \nreport. In WAFR 98: Proceedings of the third workshop on the algorithmic foundations of robotics, pages \n191 209, 1998. Matthew Hammer and Umut A. Acar. Memory Management for Self-Adjusting Computation. In \nThe 2008 International Symposium on Mem\u00adory Management, 2008. Allan Heydon, Roy Levin, and Yuan Yu. Caching \nFunction Calls Using Precise Dependencies. In Proceedings of the 2000 ACM SIGPLAN Con\u00adference on Programming \nLanguage Design and Implementation (PLDI), pages 311 320, 2000. Ruy Ley-Wild, Umut A. Acar, and Matthew \nFluet. A Cost Semantics for Self-Adjusting Computation. Technical Report CMU-CS-08-141, Department of \nComputer Science, Carnegie Mellon University, July 2008a. Ruy Ley-Wild, Matthew Fluet, and Umut A. Acar. \nCompiling self-adjusting programs with continuations. In Proceedings of the International Con\u00adference \non Functional Programming (ICFP), 2008b. William Pugh and Tim Teitelbaum. Incremental computation via \nfunction caching. In Proceedings of the 16th Annual ACM Symposium on Princi\u00adples of Programming Languages, \npages 315 328, 1989. G. Ramalingam and T. Reps. A Categorized Bibliography on Incremental Computation. \nIn Proceedings of the 20th Annual ACM Symposium on Principles of Programming Languages (POPL), pages \n502 510, 1993. Thomas Reps. Optimal-time incremental semantic analysis for syntax\u00addirected editors. In \nProceedings of the 9th Annual Symposium on Prin\u00adciples of Programming Languages (POPL), pages 169 176, \n1982. Mads Rosendahl. Automatic complexity analysis. In FPCA 89: Proceed\u00adings of the fourth international \nconference on Functional programming languages and computer architecture, pages 144 156. ACM, 1989. David \nSands. Calculi for Time Analysis of Functional Programs. PhD thesis, University of London, Imperial College, \nSeptember 1990a. David Sands. Complexity analysis for a lazy higher-order language. In ESOP 90: Proceedings \nof the 3rd European Symposium on Program\u00adming, pages 361 376. Springer-Verlag, 1990b. Patrick M. Sansom \nand Simon L. Peyton Jones. Time and space pro.ling for non-strict, higher-order functional languages. \nIn POPL 95: Pro\u00adceedings of the 22nd ACM SIGPLAN-SIGACT symposium on Principles of programming languages, \npages 355 366, 1995. Ajeet Shankar and Rastislav Bodik. DITTO: Automatic Incrementalization of Data Structure \nInvariant Checks (in Java). In Proceedings of the ACM SIGPLAN 2007 Conference on Programming language \nDesign and Implementation (PLDI), 2007. Daniel Spoonhower, Guy E. Blelloch, Robert Harper, and Phillip \nB. Gib\u00adbons. Space pro.ling for parallel functional programs. In Proceedings of the International Conference \non Functional Programming (ICFP), 2008. Philip Wadler and R. J. M. Hughes. Projections for strictness \nanalysis. In Proc. of Functional programming languages and computer architecture, pages 385 407. Springer-Verlag, \n1987. D. M. Yellin and R. E. Strom. INC: A Language for Incremental Computa\u00adtions. ACM Transactions on \nProgramming Languages and Systems, 13 (2):211 236, April 1991.  \n\t\t\t", "proc_id": "1480881", "abstract": "<p>Self-adjusting computation is an evaluation model in which programs can respond efficiently to small changes to their input data by using a change-propagation mechanism that updates computation by re-building only the parts affected by changes. Previous work has proposed language techniques for self-adjusting computation and showed the approach to be effective in a number of application areas. However, due to the complex semantics of change propagation and the indirect nature of previously proposed language techniques, it remains difficult to reason about the efficiency of self-adjusting programs and change propagation.</p> <p>In this paper, we propose a cost semantics for self-adjusting computation that enables reasoning about its effectiveness. As our source language, we consider a direct-style &#955;-calculus with first-class mutable references and develop a notion of trace distance for source programs. To facilitate asymptotic analysis, we propose techniques for composing and generalizing concrete distances via trace contexts (traces with holes). We then show how to translate the source language into a self-adjusting target language such that the translation (1) preserves the extensional semantics of the source programs and the cost of from-scratch runs, and (2) ensures that change propagation between two evaluations takes time bounded by their relative distance. We consider several examples and analyze their effectiveness by considering upper and lower bounds.</p>", "authors": [{"name": "Ruy Ley-Wild", "author_profile_id": "81351606631", "affiliation": "Carnegie Mellon University, Pittsburgh, PA, USA", "person_id": "P1300968", "email_address": "", "orcid_id": ""}, {"name": "Umut A. Acar", "author_profile_id": "81100077236", "affiliation": "Toyota Technological Institute at Chicago, Chicago, IL, USA", "person_id": "P1300969", "email_address": "", "orcid_id": ""}, {"name": "Matthew Fluet", "author_profile_id": "81100181338", "affiliation": "Toyota Technological Institute at Chicago, Chicago, IL, USA", "person_id": "P1300970", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1480881.1480907", "year": "2009", "article_id": "1480907", "conference": "POPL", "title": "A cost semantics for self-adjusting computation", "url": "http://dl.acm.org/citation.cfm?id=1480907"}