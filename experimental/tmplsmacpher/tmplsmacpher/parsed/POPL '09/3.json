{"article_publication_date": "01-21-2009", "fulltext": "\n A Model of Cooperative Threads Mart\u00b4in Abadi Microsoft Research, Silicon Valley and University of California, \nSanta Cruz abadi@microsoft.com Abstract We develop a model of concurrent imperative programming with \nthreads. We focus on a small imperative language with cooperative threads which execute without interruption \nuntil they terminate or explicitly yield control. We de.ne and study a trace-based denota\u00adtional semantics \nfor this language; this semantics is fully abstract but mathematically elementary. We also give an equational \ntheory for the computational effects that underlie the language, including thread spawning. We then analyze \nthreads in terms of the free alge\u00adbra monad for this theory. Categories and Subject Descriptors D.1.3 \n[Programming Tech\u00adniques]: Concurrent Programming Parallel programming General Terms Languages, Theory \nKeywords denotational semantics, monad, operational semantics, transaction 1. Introduction In the realm \nof sequential programming, semantics, whether oper\u00adational or denotational, provides a rich understanding \nof program\u00adming constructs and languages, and serves a broad range of pur\u00adposes. These include, for instance, \nthe study of veri.cation tech\u00adniques and the reconciliation of effects with functional program\u00adming via \nmonads. With notorious dif.culties, these two styles of semantics have been explored for concurrent programming, \nand, by now, a substantial body of work provides various semantic ac\u00adcounts of concurrency. Typically, \nthat work develops semantics for languages with parallel-composition constructs and various com\u00admunication \nmechanisms. Surprisingly, however, that work provides only a limited un\u00adderstanding of threads. It includes \nseveral operational semantics of languages with threads, sometimes with operational notions of equivalence, \ne.g., [7, 23, 20, 21]; denotational semantics of those languages seem to be much rarer, and to address \nmessage passing rather than shared-memory concurrency, e.g., [13, 19]. Yet threads are in widespread \nuse, often in the context of elaborate shared\u00admemory systems and languages for which a clear semantics \nwould be bene.cial. In this paper, we investigate a model of concurrent imperative programming with threads. \nWe focus on cooperative threads which Permission to make digital or hard copies of all or part of this \nwork for personal or classroom use is granted without fee provided that copies are not made or distributed \nfor pro.t or commercial advantage and that copies bear this notice and the full citation on the .rst \npage. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior \nspeci.c permission and/or a fee. POPL 09, January 18 24, 2009, Savannah, Georgia, USA. Copyright c &#38;#169; \n2009 ACM 978-1-60558-379-2/09/01. . . $5.00 Gordon Plotkin LFCS, University of Edinburgh and Microsoft \nResearch, Silicon Valley gdp@inf.ed.ac.uk execute, without interruption, until they either terminate \nor else ex\u00adplicitly yield control. Non-cooperative threads, that is, threads with preemptive scheduling, \ncan be seen as threads that yield control at every step. In this sense, they are a special case of the \ncooperative threads that we study. Cooperative threads appear in several systems, programming models, \nand languages. Often without much linguistic support, they have a long history in operating systems and \ndatabases, e.g., [22]. Cooperative threads also arise in other contexts, such as Internet services and \nsynchronous programming [4, 29, 9, 8, 5]. Most re\u00adcently, cooperative threads are central in two models \nfor program\u00adming with transactions, Automatic Mutual Exclusion (AME) and Transactions with Isolation \nand Cooperation (TIC) [18, 28]. AME is one of the main starting points for our research. The intended \nim\u00adplementations of AME rely on software transactional memory [27] for executing multiple cooperative \nthreads simultaneously. How\u00adever, concurrent transactions do not appear in the high-level op\u00aderational \nsemantics of the AME constructs [1]. Thus, cooperative threads and their semantics are of interest independently \nof the de\u00adtails of possible transactional implementations. We de.ne and study three semantics for an \nimperative language with primitives for spawning threads, yielding control, and block\u00ading execution. \n We obtain an operational semantics by a straightforward adap\u00adtation of previous work. In this semantics, \nwe describe the meaning of a whole program in terms of small-step transitions between states in which \nspawned threads are kept in a thread pool. This semantics serves as a reference point.  We also de.ne \na more challenging compositional denotational semantics. The meaning of a command is a pre.x-closed set \nof traces. Pre.x-closure arises because we are primarily interested in safety properties, that is, in \nmay semantics. Each trace is roughly a sequence of transitions, where each transition is a pair of stores, \nand a store is a mapping from variables to values. We establish adequacy and full-abstraction theorems \nwith respect to the operational semantics. These results require several non\u00adtrivial choices in the de.nition \nof the denotational semantics.  Finally, we de.ne a semantics based on the algebraic theory of effects. \nMore precisely, we give an equational theory for the computational effects that underlie the language, \nand analyze threads in terms of the free algebra monad for this theory. This de.nition is more principled \nand systematic; it explains threads with standard semantic structures, in the context of functional programming. \nAs we show, furthermore, we obtain our denotational semantics as a special case.  Section 2 introduces \nour language and Section 3 de.nes its operational semantics. Section 4 develops its denotational seman\u00adtics. \nSection 5 presents our adequacy and full-abstraction theorems (Theorems 5.10 and 5.15). Section 6 concerns \nthe algebraic theory b . BExp = ... e . NExp = ... C,D . Com = skip ||||||| x:= e (x. Vars) C; D if b \nthen C else D while b do C async C yield block Figure 1. Syntax. of effects and the analysis of the \ndenotational semantics in this monadic setting (Theorem 6.3). Section 7 concludes. The appendix outlines \nsome of the more interesting proofs. 2. The Language Our language is an extension of a basic imperative \nlanguage with assignments, sequencing, conditionals, and while loops (IMP [30]). Programs are written \nin terms of a .nite set of variables Vars, whose values are natural numbers. In addition to those standard \nconstructs, our language includes: A construct for executing a command in an asynchronous thread. Informally, \nasync C forks off the execution of C.This execution is asynchronous, and will not happen if the present \nthread keeps running without ever yielding control, or if the present thread blocks without .rst yielding \ncontrol.  A construct for yielding control. Informally, yield indicates that any pending thread may \nexecute next, as may the current thread.  A construct for blocking. Informally, block halts the execution \nof the entire program, even if there are pending threads that could otherwise make progress.  Thus, \nwe can for example write (Figure 2) a piece of code that spawns the asynchronous execution of x := 0, \nthen executes x := 1 and yields, then resumes but blocks unless the predicate x =0 holds, then executes \nx := 2. The execution of x := 0 async x:= 0; x:= 1; yield; if x=0 then skip else block; x:= 2 Figure \n2. Example command. may happen once the yield statement is reached. With respect to safety properties, \nthe conditional blocking amounts to waiting for x =0 to hold. More generally, AME s blockUntil bcan be \nwritten if b then skip else block. More elaborate uses of blocking are possible too, and sup\u00adported by \nlower-level semantics and actual transactional implemen\u00adtations [18, 1]. In those implementations, blocking \nmay cause a roll-back and a later retry at an appropriate time. We regard roll\u00adback as an interesting \naspect of some possible implementations, but not as part of the high-level semantics of our language, \nwhich is the subject of this work. G . State = Store \u00d7 ComSeq \u00d7 Com s . Store = Vars . Value n . Value \n= Nat T . ComSeq = Com * Figure 4. State space. We de.ne the syntax of the language in Figure 1. We do \nnot de\u00adtail the constructs on numerical expressions, nor those for boolean conditions, which are as usual. \nThus, our language is basically a fragment of the AME cal\u00adculus [1]. It omits higher-order functions \nand references. It also omits unprotected sections for non-cooperative code, particularly legacy code. \nNon-cooperative code can however be modeled as code with pervasive calls to yield. See Section 7 for \nfurther dis\u00adcussion of possible extensions to our language.  3. Operational Semantics We give an operational \nsemantics for our language. Despite some subtleties, this semantics is not meant to be challenging. It \nis given in terms of small-step transitions between states. Accordingly, we de.ne states, evaluation \ncontexts, and the transition relation. 3.1 States As described in Figure 4, a state G= \\ s,T,C) consists \nof the following components: a store s which is a mapping of the given .nite set Vars of variables to \na set Value of values, which we take to be the set of natural numbers;  a .nite sequence of commands \nT which we call the thread pool;  a distinguished active command C.  We write s[x n] for the store \nthat agrees with s except at x, . which is mapped to n. We write s(b) for the boolean denoted by b in \ns,and s(e) for the natural number denoted by ein s, similarly. We write T.T' for the concatenation of \ntwo thread pools T and T'. 3.2 Evaluation Contexts As usual, a context is an expression with a hole \n[], and an evaluation context is a context of a particular kind. Given a context C and an expression \nC, we write C [C] for the result of placing C in the hole in C . We use the evaluation contexts de.ned \nby the grammar: E =[] |E ; C  3.3 Steps A transition G -. G' takes an execution from one state to the \nnext. Figure 3 gives rules that specify the transition relation. According to these rules, when the active \ncommand is skip, a command from the pool becomes the active command. It is then evaluated as such until \nit produces skip, yields, or blocks. No other computation is interleaved with this evaluation. Each evaluation \nstep produces a new state, determined by decomposing the active command into an evaluation context and \na subexpression that describes a computa\u00adtion step (for instance, a yield or a conditional). In all cases \nat most one rule applies. In two cases, no rule applies. The .rst is when the active command is skip \nand the pool is empty; this situation corresponds to normal termination. The second is when the active \ncommand is blocked, in the sense that it has the form E [block]; this situation is an abnormal termination. \n\\ s,T,E [x:= e]) -. \\ s,T,E [skip; C]) -. \\ s,T,E [if b then C else D])-. \\ s,T,E [if b then C else D])-. \n\\ s,T,E [while b do C]) -. \\ s,T,E [async C]) -. \\ s,T,E [yield]) -. \\ s,T.C.T ' ,skip) -.  \\ s[x. n],T,E \n[skip]) if s(e)= n (Set) \\ s,T,E [C]) (Seq) \\ s,T,E [C]) if s(b)= true (Cond True) \\ s,T,E [D]) if s(b)= \nfalse (Cond False) \\ s,T,E [if b then (C; while b do C) else skip]) (While) \\ s,T.C,E [skip]) (Async) \n\\ s,T.E [skip],skip) (Yield) \\ s,T.T ' ,C) (Activate) Figure 3. Transition rules of the abstract machine. \nWe write G -. c G ' when G -. G ' via the Activate rule, and call this a choice transition. We write \nG -. a G ' when G -. G ' via the other rules, and call this an active transition. Active transitions \nare deterministic, i.e., if G -. a G ' and G -. a G '' then G ' =G '' .  4. Denotational Semantics \nNext we give a compositional denotational semantics for the same language. Here, the meaning of a command \nis a pre.x-closed set of traces, where each trace is roughly a sequence of transitions, and each transition \nis a pair of stores. The use of sequences of transitions goes back at least to Abra\u00adhamson s work [3] \nand appears in various studies of parallel com\u00adposition [2, 16, 10, 11]. However, the treatment of threads \nrequires some new non-trivial choices. For instance, transition sequences, as we de.ne them, include \nmarkers to indicate not only normal termi\u00adnation but also the return of the main thread of control. Moreover, \nalthough these markers are similar, they are attached to traces in different ways, one inside pairs of \nstores, the other not. Such de\u00adtails are crucial for adequacy and full abstraction. Also crucial to full \nabstraction is minimizing the information that the semantics records. More explicit semantics will typically \nbe more transparent, for instance, in detailing that a particular step in a computation causes the spawning \nof a thread, but will consequently fail to be fully abstract. Section 4.1 is an informal introduction \nto some of the details of the semantics. Section 4.2 de.nes transition sequences and estab\u00adlishes some \nnotation. Sections 4.3 and 4.4 de.ne the interpretations of commands and thread pools, respectively. \nSection 4.5 discusses semantic equivalences. 4.1 Informal Introduction As indicated above, the meaning \nof a command will be a pre.x\u00adclosed set of traces, where each trace is roughly a sequence of tran\u00adsitions, \nand each transition is a pair of stores. Safety properties which pertain to what may happen are closed \nunder pre.xing, hence the pre.x-closure condition. Intuitively, when the meaning of a command includes \na trace (s1,s1' )(s2,s2' ) ..., we intend that the command may start executing with store s1, transform \nit to s1' , yield, then resume with store s2, transform it to s2' , yield again, and so on. In particular, \nthe meaning of block will consist of the empty sequence e. The meaning of yield; block will consist of \nthe s is any store. Here, the pair (s,s) is a stutter that represents immediate yielding. If the meaning \nof a command C includes (s1,s1' ) ...(sn,sn' ) ''' ' and the meaning of a command Dincludes (sn,sn) ...(sm,sm), \none might naively expect that the meaning of C; D would con\u00ad ''' ' tain (s1,s1) ...(sn,sn) ...(sm,sm), \nwhich is obtained by con\u00adcatenation plus a simple local composition between (sn,sn' ) and (sn' ,sn'' \n). Unfortunately, this naive expectation is incorrect. In a trace (s1,s1' )(s2,s2' ) ..., some of the \npairs may represent steps taken by commands to be executed asynchronously. Those steps need not take \nplace before any further command D starts to exe\u00adcute. Accordingly, computing the meaning of C; D requires \nshuf\u00ad.ing suf.xes of traces in C with traces in D. The shuf.ing repre\u00adsents the interleaving of C s asynchronous \nwork with D s work. We introduce a special return marker Ret in order to indicate how the traces in C \nshould be parsed for this composition. In par\u00adticular, when C is of the form C1; async (C2), any occurrence \nof Ret in the meaning of C2 will not appear in the meaning of C. The application of async erases any \noccurrence of Ret from the meaning of C2 intuitively, because C2 does not return control to its sequential \ncontext. For example, the meaning of the command x:= n; yield; x:= n ' will contain the trace '' ' (s,s[x. \nn])(s ,s [x. n ]Ret) for every sand s ' . On the other hand, the meaning of the command x:= n; async \n(x:= n ' ); yield will contain the trace '' ' (s,s[x. n]Ret)(s ,s [x. n ]) for every s and s ' . The \ndifferent positions of the marker Ret correspond to different junction points for any commands to be \nexecuted next. If the meaning of C contains u(sn,sn ' Ret)u ' and the mean\u00ading of D contains (sn' ,sn'' \n)v, then the meaning of C; D con\u00adtains u(sn,sn'' )w,where w is a shuf.e of u ' and v. Notice that the \nmarker from u(sn,sn ' Ret)u ' disappears in this combination. '' ''' The marker in u(sn,sn)w, if present, \ncomes from (sn,sn)v.An analogous combination applies when the meaning of C contains ' ' ''' u(sn,sn Ret)u \nand the meaning of D contains (sn,sn Ret)v empty sequence e plus every sequence of the form (s,s),where \n(a trace that starts with a transition with a marker). Moreover, if Figure 5. Denotational semantics. \n[[skip]] = * [[x := e]] [[C; D]] = = {(s, s[x . n]Ret)Done | s . Store,s(e)= n}.[ C]] . [[D]] [[if b \nthen C else D]] [[while b do C]] = = {t | t . [[C]], non-empty, fst(t)(b)= true}. .{t | t . [[D]], non-empty, \nfst(t)(b)= false}..i[[(while b do C)i]] [[async C]] = async([ C]]c) [[yield]] = d(*) [[block]] = {e} \n the meaning of C contains a trace without any occurrence of the marker Ret, then this trace is also \nin the meaning of C; D:the ab\u00adsence of a marker makes it impossible to combine this trace with traces \nfrom D. An additional marker, Done , ends traces that represent com\u00adplete normally terminating executions. \nThus, the meaning of skip will consist of the empty sequence e and every sequence of the form (s, s Ret) \nplus every sequence of the form (s, s Ret)Done. Contrast this with the meaning of yield; block given \nabove. It is possible for a trace to contain a Ret marker but not a Done marker. Thus, the meaning of \nasync (block) will contain the empty sequence e plus every sequence of the form (s, s Ret), but not (s, \ns Ret)Done. More elaborately, the meaning of the code of Figure 2 will contain all traces of the form \n(s, s[1])(s[1],s[0])(s[0],s[2] Ret)Done wherewewrite s[n] as an abbreviation for s[x . n].These traces \nmodel normal termination after taking the true branch of the conditional if x =0 then x := 2 else block. \nThe meaning will also contain all pre.xes of those traces, which model partial executions including those \nthat take the false branch of the conditional and terminate abnormally. The two markers are somewhat \nsimilar. However, note that (s, s ' Ret) is a pre.x of (s, s ' Ret)Done,but (s, s ' ) is not a pre.x \nof (s, s ' Ret). Such differences are essential.  4.2 Transition Sequences A plain transition is a pair \nof stores (s, s ' ).A return transition is apair ofstores (s, s ' Ret) in which the second is adorned \nwith the marker Ret.A transition is a plain transition or a return transition. A transition sequence \nis a .nite (possibly empty) sequence, beginning with a sequence of transitions, of which at most one \n(not necessarily the last) is a return transition, and optionally followed by the marker Done if one \nof the transitions is a return transition. We write TSeq for the set of transition sequences. A pure \ntransition sequence is a .nite sequence of plain transi\u00adtions, possibly followed by a marker Done. Note \nthat such a se\u00adquence need not be a transition sequence in the sense above. It is proper if it is not \nequal to Done. We write PSeq for the set of pure transition sequences. We use the following notation: \n We typically let u, v,and w range over transition sequences or pure transition sequences, and let t \nrange over non-empty ones.  We write u =p v for the pre.x relation between sequences u and v (for both \nkinds of sequence, pure or not).  For a non-empty sequence of transitions t, we write fst(t) for the \n.rst store of the .rst transition of t.  For a transition sequence u, we write u c for the pure transition \nsequence obtained by removing the Ret marker, if present, from u.  We let t range over stores and stores \nwith return markers.  4.3 Interpretation of Commands Preliminaries We let Proc be the collection of \nthe non-empty pre.x-closed sets of transition sequences, and let Pool be the collection of the non-empty \npre.x-closed sets of pure transition sequences, where a set P is pre.x-closed if whenever u =p v . P \nthen u . P . We write P . for the least pre.x-closed set that contains P . Under the subset partial ordering, \nProc and Pool are both .-cpos (i.e., partial orders with sups of increasing sequences) with least element \n{e}. We interpret commands as elements of Proc.We use Pool as an auxiliary cpo; below it also serves \nfor the semantics of thread pools. We de.ne a continuous clean function -c :Proc . Pool by: P cc = {u \n| u . P } (Continuous functions are those preserving all sups of increasing sequences.) We de.ne the \nset of shuf.es of a pure transition sequence u with a sequence v as follows: If neither .nishes with \nDone, their set of shuf.es is de.ned as usual for .nite sequences.  If u does not .nish with Done, then \na shuf.e of u and v Done is a shuf.e of u and v. Similarly, if v does not .nish with Done, then a shuf.e \nof u Done and v is a shuf.e of u and v.  A shuf.e of u Done and v Done is a shuf.e of u and v followed \nby Done.  We write uw v for the set of shuf.es of u and v. We de.ne a continuous composition function \n. :Proc2 . Proc by: P . Q = {u(s, t )v |.s ' ,w, w ' . u(s, s ' Ret)w . P, (s '' ,t )w . Q, v . ww w \n' }.{u | u . P with no return transition} Composition is associative with two-sided unit, given by: * \n= {(s, s Ret)Done | s . Store}. We also de.ne a continuous delay function d:Proc . Proc by: d(P)= {(s,s)u \n| s . Store,u . P}.Thus, d(P) is P preceded by all possible stutters (plus e). Simi\u00adlarly, we de.ne a \ncontinuous function async : Pool . Proc by: async(Q)= {(s,s Ret)u | s . Store,u . Q}. Thus, for P . Proc, \nasync(Pc) differs from d(P) only in the placement of the marker Ret. Interpretation The denotational \nsemantics [[\u00b7]]:Com -. Proc maps a command to a non-empty pre.x-closed set of transition sequences. We \nde.ne it in Figure 5. There, the interpretation of loops relies on the following approximations: (while \nb do C)0 = block (while b do C)i+1 = if b then (C;(while b do C)i) else skip The 0-th approximant corresponds \nto divergence, which here we identify with blocking. We straightforwardly extend the semantics to contexts, \nso that [[C]] : Proc . Proc is a continuous function on Proc. This function is de.ned by induction on \nthe form of C, with the usual clauses of the de.nition of [[\u00b7]] plus [[[ ]]](P)= P. PROPOSITION 4.1. \n[[C[C]]] = [ C]]([ C]]). Therefore, if [[C]] . [[D]] then [[C[C]]] . [[C[D]]]. 4.4 Interpretation of \nThread Pools As an auxiliary de.nition, it is important to have also an interpre\u00adtation of thread pools \nas elements of Pool. We develop one in this section. Preliminaries We de.ne a continuous shuf.e operation \nw :(Pool)2 . Pool at this level by: [ Pw Q = uw v u.P,v.Q The shuf.e operation is commutative and associative, \nwith unit I = def {e,Done}. We de.ne the (left) action u\u00b7v of a pure transition sequence u on a transition \nsequence v, by setting u \u00b7 (s,t)v = {(s, t)w | w . uw v} and u \u00b7 e = {e} We then de.ne async:Pool \u00d7 Proc \n-. Proc by: [ async(P,Q)= u\u00b7v u.P,v.Q The use of the notation async for both a unary and a binary operation \nis a slight abuse, though in line with the algebraic theory of effects: see the discussion in Section \n6. In this regard note the equality async(P) . Q =async(P,Q) (and the equality [[yield]] . P =d(P) points \nto the corresponding relationship between d and [[yield]]). Interpretation We de.ne the semantics of \nthread pools by: [[C1,...,Cn]]= [ C1]]c w ... w [[Cn]]c (n = 0) intending that [[e]] = I. For any thread \npool T, Done . [[T]] iff T = e (because, for all C, Done ./[[C]]c and, for all P and Q, I . Pw Q iff \nI . P and I . Q). Further, we set [[T,C]] = async([ T]], [[C]]). LEMMA 4.2. For all P,Q . Pool and R \n. Proc we have: 1. async(Pw Q,R)= async(P,async(Q,R)) 2. async(I,R)= R  4.5 Equivalences An attractive \napplication of denotational semantics is in proving equivalences and implementation relations between \ncommands. Such denotational proofs tend to be simple calculations. Via ade\u00adquacy and full-abstraction \nresults (of the kind established in Sec\u00adtion 5), one then obtains operational results that would typically \nbe much harder to obtain directly by operational arguments. As an example, we note that we have the following \nequivalence: [[async (C; yield; D)]]= [ (async (C; async (D))]] This equivalence follows from three facts: \nWe have: [[yield; D]]c =[[async (D)]]c = {(s,s)u c | s . Store,u . [[D]]}.; whenever [[D1]]c =[[D2]]c \n, [[C; D1]]c =[[C; D2]]c; whenever [[D1]]c =[[D2]]c , [[async (D1)]]= [ async (D2)]].  This particular \nequivalence is interesting for two reasons: It models an implementation strategy (in use in AME) where, \nwhen executing C; yield; D,the yield causes a new asyn\u00adchronous thread for D to be added to the thread \npool.  It illustrates one possible, signi.cant pitfall in more explicit semantics. As discussed above, \nsuch a semantics might detail that a particular step in a computation causes the spawning of a thread. \nMore speci.cally, it might extend transitions with an extra trace component: a triple (s,u,t) might represent \na step from s to t that spawns a thread that contains the trace u. With such a semantics, the meanings \nof async (C; yield; D) and async (C; async (D)) would be different, since they have different spawning \nbehavior.  Many other useful equivalences hold. For instance, we have: [[x := n; x := n ' ]]= [ x := \nn ' ]] trivially. For every C,we also have: [[async (C); x := n]]= [ x := n; async (C)]] and, for every \nC and D,we have: [[async (C); async (D)]]= [ async (D); async (C)]] Another important equivalence is \n[[while (0 = 0) do skip]]= [ block]] Thus, the semantics does not distinguish an in.nite loop which never \nyields from immediate blocking. On the other hand, we have: [[while (0 = 0) do yield]] =[[ block]] The \ncommand while (0 = 0) do yield generates unbounded sequences of stutters (s, s). Alternative semantics \nthat would dis\u00adtinguish while (0 = 0) do skip from block or that would identify while (0 = 0) do yield \nwith block are viable, how\u00adever. We brie.y discuss those variants and others in Section 7.  5. Adequacy \nand Full Abstraction In this section we establish that the denotational semantics of Sec\u00adtion 4 coincides \nwith the operational semantics of Section 3, and is fully abstract. The adequacy theorem, which expresses \nthe coincidence, says that the traces that the denotational semantics predicts are exactly those that \ncan happen operationally. These traces may in general represent the behavior of a command in a context. \nAs a special case, the adequacy theorem also applies to runs, which are essentially traces that the command \ncan produce on its own, i.e., with an empty context. The full-abstraction theorem implies that, if two \ncommands C and Dhave the same set of traces denotationally, then they produce the same runs in combination \nwith every context. In other words, observing runs, we cannot distinguish C and D in any context. We \ncomment on other possible notions of observation, and the corresponding full-abstraction results, below. \nSection 5.1 de.nes runs precisely. Sections 5.2 and 5.3 present our adequacy and full-abstraction results, \nrespectively. 5.1 Runs A pure transition sequence generates a run if, however it can ' '' ''' ' be written \nas u(s,s )(s ,s )v,we have s = s '' .For such a pure transition sequence w =(s1,s2) ...(sn-1,sn),we set \nrun(w)= s1 ...sn and run(wDone) = s1 ...snDone.A transition sequence u generates a run if u c does, and \nthen we set run(u)= run(u c). If a pure transition sequence u generates a run, then it can be easily \nbe recovered from run(u):the run s1 ...sn maps back to (s1,s2) ...(sn-1,sn) and the run s1 ...snDone \nmaps back to (s1,s2) ...(sn-1,sn)Done Since each non-empty run contains at least two elements, this de.nition \napplies when n =0 and n = 2. We write runs(P) for the set of runs generated by (pure) transition sequences \nin P. 5.2 Adequacy LEMMA 5.1. The following equalities hold: 1. [[E[block]]]= [ block]] 2. [[skip; \nC]]= [ C]] 3. [[E[async D]]] = async([ D]]c ,E[skip]) 4. [[E[yield]]]c =async([ E[skip]]]c ,[[skip]])c \n 5. For all T  = e(equivalently Done.[[T]]), [ ' '' c ' '' [[T]] = {[[T .T ,C]] |T = T .C.T } LEMMA \n5.2. If C is blocked then, for all T, [[T,C]] = {e}. LEMMA 5.3. [[T,skip]] = {(s,s Ret)v |v .[[T]]}.. \nThe next lemma applies when C is neither skip nor blocked. LEMMA 5.4. Suppose that \\s,T,C)-.a \\s ' ,T \n' ,C ' ). Then, for '' '' c any s , (s,s )v .[[T,C]]c iff (s ' ,s '' )v .[[T ' ,C ' ]]. LEMMA 5.5. Suppose \nthat \\s,T,C)-.a * some \\s ' ,T ' ,skip)with u .[[T ' ]]c.Then (s,s ' )u.[[T,C]]c . For the proof of the \nconverse of this lemma, we proceed by an induction on the size of loop-free commands. We then extend \nto general commands by expressing their semantics in terms of the semantics of their approximations by \nloop-free commands. The size of a loop-free command is de.ned by structural recursion: |skip|= |block|=1 \n|x:= e|= |async C|= |yield|=2 |if b then C else D|= |C; D|= |C|+ |D| Note that if \\s,T,C)-.a \\s ' ,T \n' ,C ' )and C is loop-free, then so is C ' and, further, |C ' |<|C|. The approximation relation C S D \nbetween loop-free com\u00admands C and general commands D is de.ned to be the least such relation closed under \nall non-looping program constructs and such that, for any b, C, D,and i=0: C S D block S D (while b do \nC)i S (while b do D) This relation is extended to thread pools and contexts in the obvious way: we write \nT S T ' and CS C ' for these extensions. LEMMA 5.6. Suppose that T S U, C S D, and, further, that '' \n''' \\s,T,C)-.a \\s ,T ' ,C ). Then, for some U ,D with T ' S U ' ''' and C ' S D , \\s,U,D)-.a * \\s ,U \n,D ). Next we de.ne the approximants C(i) of a command C by induction on i and structural recursion on \nC, beginning with the case where Chas one of the forms skip, block, x:= e,or yield, when C(i) = C, and \ncontinuing with: (async C)(i) = async C(i) D)(i) C(i) D(i) (if b then C else = if b then else (C; D)(i) \n= C(i); D(i) C)(i) C(i) (while b do =(while b do )i For any C one shows that C(i) S C(i+1) S C. LEMMA \n5.7. 1. If C S Dthen [[C]] .[[D]]. 2. For any command D: [ (i) [[D]]= [[D]] i We can now establish the \nconverse of Lemma 5.5. LEMMA 5.8. Suppose that (s,s ' )u.[[T,C]]c. Then. for some T ' , \\s,T,C)-.a * \n\\s ' ,T ' ,skip)with u.[[T ' ]]c . LEMMA 5.9. 1. For any proper non-empty pure transition se\u00adquence u, \n(s,s ' )u . [[T,C]]c holds iff for some T ' ,C ' , \\s,T,C)-.a * -.c\\s ' ,T ' ,C ' )with u .[[T ' ,C ' \n]]c . 2. For any s, s ' , T, C, (s,s ' )Done .[[T,C]]c holds iff \\s,T,C)-.a * \\s ' ,e,skip). The following \nAdequacy Theorem for pure transition sequences is an immediate consequence of Lemmas 5.8 and 5.9: THEOREM \n5.10. 1. For n> 0, (s1,s1' ) ...(sn,sn' ) . [[T,C]]c iff there are Ti,Ci,(i =1,n) such that T1 = T, C1 \n= C, and \\si,Ti,Ci)-.a * -.c\\si' ,Ti+1,Ci+1),for 1 =i =n-1, and \\sn,Tn,Cn)-.a * some \\sn' ,T ' ,skip). \n2. For n> 0, (s1,s1' ) ...(sn,sn' )Done . [[T,C]]c iff there are Ti,Ci,(i =1,n) such that T1 = T, C1 \n= C, and \\si,Ti,Ci)-.a * -.c\\si' ,Ti+1,Ci+1),for 1 =i =n-1, and \\sn,Tn,Cn)-.a * \\sn' ,e,skip). As a corollary \nwe obtain an adequacy theorem for runs: COROLLARY 5.11. 1. For n = 2, s1 ...sn . runs([[T,C]]) iff there \nare Ti,Ci,(i =1,n-1) such that T1 = T, C1 = C, \\si,Ti,Ci)-.a * -.c\\si+1,Ti+1,Ci+1)(1 =i =n-2), and \\sn-1,Tn-1,Cn-1)-.a \n* some \\sn,T ' ,skip). 2. For n = 2, s1 ...snDone . runs([[T,C]]) iff there are Ti,Ci,(i =1,n -1) such \nthat T1 = T, C1 = C, and \\si,Ti,Ci)-.a * -.c\\si+1Ti+1,Ci+1)(1 = i = n-2), and \\sn-1,Tn-1,Cn-1)-.a * \\sn,e,skip). \n 5.3 Full Abstraction The .rst lemma in the proof of full abstraction bounds the nonde\u00adterminism of commands \nin semantic terms. LEMMA 5.12. For all C, u, and s,the set {t | u(s, t ) . [[C]]} is .nite. Intuitively, \nLemma 5.12 is useful because it implies that, at any point, there are certain steps that a command cannot \ntake, and in proofs those steps can be used as unambiguous, visible markers of activity by the context. \nThis lemma is somewhat fragile it does not hold once one adds certain nondeterministic choice operators \nto the language. An alternative argument that does not use the lemma relies on fresh variables instead. \nThe fresh variables permit an alternative de.nition of the desired markers. Full-abstraction results \ninvariably require some notion of obser\u00advation. Let us write obs(P ) for the observations that we make \non P . Proc. Equational full abstraction is that [[C]]= [ D]] if and only if, for every context C, obs([ \nC[C]]]) = obs([ C[D]]]).In other words, two commands have the same meaning if and only if they lead to \nthe same observations in every context of the language. The stronger inequational full abstraction is \nthat [[C]] . [[D]] if and only if, for every context C, obs([ C[C]]]) . obs([ C[D]]]). The dif.\u00adcult \npart of this equivalence is usually the implication from right to left: that if, for every context C, \nobs([ C[C]]]) . obs([ C[D]]]),then [[C]] . [[D]]. One possible candidate for obs(P ) is P c. This notion \nof obser\u00advation can be criticized as too .ne-grained. Nevertheless, we .nd it useful to prove full abstraction \nfor this notion of observation, with the following lemma. LEMMA 5.13. If [[C[C]]]c . [[C[D]]]c for every \ncontext C,then [[C]] . [[D]]. Another possible candidate for obs(P ) is runs(P ). Runs record more than \nmere input-output behavior, but much less than entire execution histories. We therefore .nd them attractive \nfor our pur\u00adposes. The following lemma connects runs to cleaning. LEMMA 5.14. If runs([[C[C]]]) . runs([[C[D]]]) \nfor every con\u00adtext C,then [[C]]c . [[D]]c . We obtain the following Full-abstraction Theorem: THEOREM \n5.15. [[C]] . [[D]] iff, for every context C, runs([[C[C]]]) . runs([[C[D]]]). One direction of the theorem \nfollows from Lemmas 5.13 and 5.14. The other is an immediate consequence of the compositionality of the \nsemantics (Proposition 4.1). Coarser-grained de.nitions of obs(P ) may sometimes be ap\u00adpropriate. For \nthose, full abstraction will typically require addi\u00adtional closure conditions on P , such as closure \nunder stuttering and closure under mumbling, much as in our work and Brookes s on parallel composition \n[2, 10].  6. Algebra The development of the denotational semantics in Section 4 is ad hoc, in that the \nsemantics is not related to any systematic approach. In the functional programming approach to imperative \nlanguages, commands have unit type, 1. Then, taking the monadic point of view [6], they are modeled as \nelements of T (1) for a suitable monad T on, say, the category of .-cpos and continuous functions. For \nparallelism one might look for something along the lines of the resumptions monad [15, 12, 17]. In the \nalgebraic approach to computational effects [24, 17], one analyses the monads as free algebra monads \nTL for a suitable equational or Lawvere theory L (meaning in the enriched sense, so that inequations \nare allowed, as are families of operations continuously parameterized over a cpo). As discussed in [15], \nresumptions are generally not fully ab\u00adstract when their domain equation is solved in a category of cpos. \nIf, instead, it is solved in a category of semilattices, increased ab\u00adstraction may be obtained. The \nsituation was analyzed from the al\u00adgebraic point of view in [17]. It was shown there that resumptions \narise by combining a theory for stores [24] with one for nonde\u00adterminism, one for nontermination, and \none for a unary operation d thought of as suspending computation. The difference between solving the \nequation in a category of semilattices or cpos essentially amounts to whether or not one asks that d, \nand the other operations, commute with nondeterminism. In [10], Brookes, using an apparently different \nand mathemati\u00adcally elementary trace-based approach, succeeded in giving a fully abstract semantics for \na language of the kind considered in [15]. However, in [19], Jeffrey showed that trace-based models of \ncon\u00adcurrent languages can arise as solutions to domain equations in a category of semilattices, thereby \nrelating the two approaches. We propose here to identify the suspension operation d with the operation \nof the same name introduced in Section 4.3; indeed this identi.cation was the origin of the de.nition \nof yield given there, and it is natural to further identify yield as the generic effect [25] corresponding \nto the suspension operation. These identi.cations are justi.ed by Theorem 6.3, below, and the discussion \nfollowing it. In Section 6.1 we carry out an algebraic analysis of resump\u00adtions. We show in Theorem 6.1 \nthat, imposing the commutations with nondeterminism just discussed, they do indeed correspond to a traces \nmodel, provided one uses the Hoare or lower powerdomain. (This powerdomain is a natural choice as we \nconsider only may semantics in this paper, and elements of such powerdomains are Scott closed, so downwards-closed, \na natural generalization of our pre.x-closedness condition.) In [10] Brookes imposed further clo\u00adsure \nconditions on his sets of traces, viz under stuttering and under mumbling, but these are not needed for \nour full-abstraction result. The missing ingredient in an algebraic analysis of Proc is then an account \nof async. In the denotational semantics of any command of the form async C,all Ret marking is lost from \nthe meaning of C, because of the application of the clean function, -c; further all the sequences in \n[[C]]c are proper. We propose to treat async as a generic effect, parameterized by an element of AProc, \nwhich we de.ne to be the sub-.-cpo of Pool of all non\u00adempty pre.x-closed sets of proper pure transition \nsequences. We think of such sets as modeling asynchronous threads, spawned by an active thread; the difference \nfrom Pool is that the latter also contains an element that models the empty thread pool. In order to \ngive the equations for the async operation it will, as one may expect, be useful to .rst have an algebraic \nanalysis of AProc; we carry out this analysis in Section 6.2. It turns out, as detailed in Theorem 6.2, \nthat AProc is similar to, but not quite, a resumptions cpo. Finally, we analyze processes in Section \n6.3, showing, in Theorem 6.3, that a process is a kind of double\u00adthread more precisely, a resumption \nthat returns not only a value but also an element of AProc. 6.1 Resumptions Our theory LRes for resumptions \nfollows [17] but is somewhat modi.ed, as we are interested only in may semantics and as we wish to allow \nin.nitely proceeding processes. The theory is a combination of several constituent theories which we \nconsider successively. The Lawvere theory LS of stores can be presented via a family of unary operations \nupdatex,n and a family of Nat-ary oper\u00adations lookupx (x . Vars, n . Nat). (A Nat-ary operation is a \ncountably in.nitary operation whose arguments are indexed by elements of Nat.) For any computation ., \nupdatex,n(.) is read as the computation that .rst updates x to n and then proceeds as .;for any Nat-indexed \ncollection (.n)n of computations, lookupx(.n)n is read as the computation that proceeds as .n if x has \nvalue n. The Lawvere theory LH for nondeterminism is that of the lower (aka Hoare) powerdomain, presented \nusing a binary operation .; the Lawvere theory LO for nontermination is the theory of a least element, \npresented using a constant O; and the Lawvere theory Ld for suspension is that of a unary operation d, \nwith no equations. See [24, 17] for more details of these theories, including an account of the equations \nfor stores and for Hoare powerdomains. For resumptions, continuing to follow [17], we wish the oper\u00adations \nof LS to commute with those of LH and LO (which auto\u00admatically commute with each other) and it is also \nnatural to have d commute with nondeterministic choice, but not with the operations of LS, as we wish \nto model interruption points, and not with O,as we want to be able to model in.nitely proceeding processes. \nWe therefore de.ne: LRes = LH .((LS .LO)+ Ld) and let TRes be the associated monad. (For any two theories \nL and L ' presented with disjoint signatures, the theories L + L ' and L .L ' are presented with the \nunion of the signatures of L and L ' and, in the former case, with the union of their equations and, \nin the latter case, with the union of their equations together with additional equations that say that \neach operation of the one theory commutes with each operation of the other.) We now give an elementary \ntrace-based picture of TRes(P ) for moderately general P .Let Q be a partial order. A Q-transition is \napairofstates (s, s ' x) in which the second is marked with an element x of Q;we let t range over stores \nand stores marked with an element of Q.A basic Q-transition sequence is a non-empty sequence consisting \nof plain transitions optionally followed by a Q-transition. Let =Q be the least preorder on the set of \nbasic Q\u00adtransition sequences which contains the pre.x relation =p and is such that, for any x, y in Q,if \nx =y then u(s, s ' x) =Q u(s, s ' y). One can show that =Q is a partial order and that u =Q v holds iff: \neither u =p v or else .w, x =y. u =p w(s, s ' x) .v = w(s, s ' y) We need a few notions concerning ideals \nin partial orders. An ideal in a partial order Q is a downwards-closed subset of Q;for any subset X of \nQ we write X.for {x .Q |.y .X. x =y}, the least ideal including X;and forany x . Q we write x . for {x}.. \nDownwards-closed sets, i.e., ideals, provide a suitable generalization of pre.x-closed sets when passing \nfrom sequences to general partial orders. An ideal I is directed if it is nonempty and any two elements \nof the ideal have an upper bound in the ideal. An ideal is denumerably generated if I = X .for some denumerable \nX . I. We write I.. (Q), respectively I.(Q), for the collection of all denumerably generated directed \nideals of Q, respectively all denumerably gener\u00adated ideals of Q, and we partially order them by subset; \nI.. (Q) is an .-cpo, indeed it is the free such over Q;and I. (Q) is the free .-cpo with all .nite sups \nover Q. Let Q-BTrans be the set of basic Q-transition sequences, par\u00adtially ordered as above. One can \nview I.(Q-BTrans) as an LRes \u00admodel with the following de.nitions of the operations: (updateRes)x,n(I)= \n{(s, t )u |(s[x .n],t )u .I} S (lookupRes)x(In)n = {(s, s ' )u .In |s(x)= n} n I .Res J = I .J ORes = \n\u00d8 dRes(I)= {(s, s)u |s .Store,u .I}.{(s, s)|s .Store} (We skip over the difference between the notion \nof an LRes-model and of an algebra satisfying equations.) The next theorem shows that the algebraic notion \nof resump\u00adtions can indeed be characterized in trace-based terms, speci.cally as ideals of basic Q-transition \nsequences. THEOREM 6.1. Viewed as an LRes-model, I.(Q-BTrans) is TRes(I.. (Q)). The unit . : I..(Q) .I.(Q-BTrans) \nis given by: .(I)= {(s,s x) |x .I}and, for any continuous f : I.. (Q) .I.(R-BTrans), its Kleisli extension \nf : I.(Q-BTrans) .I.(R-BTrans) is given by: f (I)= {u(s, t )v |.s ' ,x. u(s, s ' x) .I, (s ' ,t )v .f(x.)}.{u \n|u .I with no Q-transition} One can go further and obtain a closely related, if less elemen\u00adtary, picture \nof TRes(P ) for arbitrary P : one needs a notion of ideal that takes the .-sups of P into account. 6.2 \nAsynchronous Processes One might hope that AProc can be understood as a cpo of resump\u00adtions, and, indeed, \nproper pure non-empty transition sequences and basic {Done}-transition sequences are very similar. One \ncan associate to every pure transition sequence u(s, s ' )Done (re\u00adspectively non-empty pure transition \nsequence u not containing Done) the basic {Done}-transition sequence u(s, s ' Done) (re\u00adspectively u). \nUnfortunately, while the association is a bijection between proper pure non-empty transition sequences \nand basic {Done}-transition sequences, it does not respect the order, since u(s, s ' ) =p u(s, s ' )Done \nbut u(s, s ' )={Done}u(s, s ' Done). There is a related programming language phenomenon. Deno\u00adtationally, \nwe have the inclusion: [[(async (yield; block)); C]] .[[(async skip); C]] but not the inclusion: [[yield; \nblock]] .[[skip]] Operationally, as in the proof of the full-abstraction theorem, one can distinguish \n[[yield; block]] from [[skip]] using a sequential context which, however, is not available when the command \nis within an async. To solve this dif.culty we take the theory of asynchronous threads LAProc to be LRes \nextended by a new constant halt with an equation: d(O) =halt accounting for the additional inequalities \ndiscussed above. We can turn AProc into a model of LAProc by de.ning opera\u00adtions as follows: (updateAProc)x,n(X)= \n{(s, s ' )u |(s[x .n],s ' )u .X}. S (lookupAProc)x(X)n =({(s, s ' )u .Xn |s(x)= n}). n X .AProc Y = X \n.Y OAProc = {e}dAProc(X)= {(s, s)u |s .Store,u .X}.haltAProc = {(s, s)Done}. Note that haltAProc =[[skip]]c \n. The next theorem shows that the variant theory LAProc indeed captures AProc. THEOREM 6.2. AProc is \nthe initial LAProc-model, i.e., it is TAProc(0). Here TAProc is the free algebra functor associated to \nthe theory AProc. It is not hard to go on and obtain a general view of the monad TAProc using a suitable \nnotion of (proper) pure Q-transition sequences; however we omit the details as they are not needed for \nan account of processes. 6.3 Processes We turn to our algebraic account of Proc. The signature is that \nfor LRes together with two families of unary operation symbols asyncP and yield toP ,where P . AProc. \nThe .rst of these corresponds to the function of the same name de.ned above, but restricted to asynchronous \nthreads. The second corresponds to a slightly different version of async in which the .rst action is \nthat of the thread spun off, rather than that of the active command. We often .nd it convenient to write \nasyncP t and yield toP t as, respectively, P \u00b7t and t \u00b7P . We begin with a theory LSpawn for async and \nyield to which involves the other operations. The .rst group of equations for LSpawn concerns commutation \nwith .: (P .AProc P ' ) \u00b7x =(P \u00b7x) .(P ' \u00b7x) P \u00b7(x .y)= P \u00b7x .P \u00b7y (x .y) \u00b7P = x \u00b7P .y \u00b7P x \u00b7(P .AProc \nP ' )= x \u00b7P .x \u00b7P ' The second group of equations is for async: P \u00b7updatel,v(x) = updatel,v(P \u00b7x) P \u00b7lookupl(xn)n \n=lookupl(P \u00b7xn)n P \u00b7O=O P \u00b7d(x)= d(P \u00b7x) .d(x \u00b7P ) P \u00b7(P ' \u00b7x)=(Pw P ' ) \u00b7x The .rst three state that \nP \u00b7-commutes with another operation; the next concerns the interaction of async with suspension and brings \nin yield to; the last reduces two async s to one. The third, and last, group of equations is for yield \nto: x \u00b7(updateAProc)l,v(P ) = updatel,v(x \u00b7P ) x \u00b7(lookupAProc)l(Pn)n =lookupl(x \u00b7Pn)n x \u00b7OAProc =O x \n\u00b7dAProc(P )= d(x \u00b7P ) .d(P \u00b7x) x \u00b7haltAProc =d(x) The .rst three assert that x \u00b7-acts homomorphically \nwith respect to an operation; the next concerns the interaction with suspension; and the last concerns \nwhat happens when asynchronous threads halt. We take LProc to be LRes + LSpawn, i.e., the equations are \nthe ones just given for async and yield to, together with those for LRes. One might have expected to \nsee an equation with left-hand side P \u00b7(x \u00b7P ' ); indeed, we could have added the equation: ' '' P \u00b7(x \n\u00b7P )=(P \u00b7x) \u00b7P .(x \u00b7P ) \u00b7P However this equation is redundant, and can be proved from the others using \nthe algebraic induction principle of Computational Induction described in [26]. (One proceeds by such \nan induction on P ' , with a subinduction on P .) We now aim to give a picture of TProc(I.(Q)) like that \nof TRes(I.(Q)). Take the partial order Q-Trans of the Q-transition sequences to be that of the basic \n(Q \u00d7PSeq)-transition sequences. Note that one can regard Q-transition sequences as elements of a kind \nof double thread in which the .rst thread returns a value together with a second (asynchronous) thread. \nWe show that Q-Proc = def I.(Q-Trans) carries the free model of LProc on I.(Q).We view Q-Proc as a LRes-model \nas in Section 6.1. In order to give async and yield to we de.ne, abusing notation, left and right actions \nof PSeq on Q-Trans: \u00b7:PSeq \u00d7Q-Trans .Q-Proc \u00b7: Q-Trans \u00d7PSeq .Q-Proc The de.nitions are by mutual induction \non the length of the se\u00adquences involved. For all u .PSeq and v .Q-Trans, we put: u[Done] \u00b7(s, s ' )= \n{(s, s ' )u}. u \u00b7(s, s ' (x, v)) = {(s, s ' (x, w)) |w .uw v}. u \u00b7(s, s ' )v = {(s, s ' )w |w .u \u00b7v .v \n\u00b7u}. where [Done] indicates an optional occurrence of Done, and: v \u00b7e = \u00d8 v \u00b7(s, s ' )Done = {(s, s ' \n)v}. v \u00b7(s, s ' )u = {(s, s ' )w |w .u \u00b7v .v \u00b7u}. where, in the last line, u is required to be proper. \nFor P .AProc and I .Q-Proc, we put: S (asyncProc)P (I)= u \u00b7v Su.P,v.I (yield toProc)P (I)= v \u00b7u u.P,v.I \nWith these additional operations, Q-Proc is a model of LProc. Our main algebraic theorem characterizes \nfree models of a natural equational theory for resumptions with thread-spawning in terms of a kind of \ndouble-thread. The cpo of processes Proc is the free model over 1; this places cooperative threads within \nthe monadic approach to effects. THEOREM 6.3. Viewed as an LProc-model, I.(Q-Trans) is the free model \nover I.. (Q). The unit . : I.. (Q) .I.(Q-Trans) is given by: .(I)= {(s, s (x, Done)) |x .I}and, for any \ncontinuous f : I.. (Q) .I.(R-Trans), its Kleisli extension f : I.(Q-Trans) .I.(R-Trans) is given by: \nf (I)= {u(s, t )v |.s ' ,x,w. u(s, s ' (x, w)) .I, (s ' ,t )v .w .\u00b7f(x.)}.{u |u .I with no (Q \u00d7PSeq) \ntransition} As in the case of resumptions, one can go further and obtain a closely related, if less elementary, \npicture of TProc(P ) for arbi\u00adtrary P . We are .nally in a position to give our algebraic account of \nProc. There is an isomorphism . : Q-Trans . TSeq \\{e}, where Q = {Ret}, sending u =(s1,s1' ) ... (sn,sn' \n) to itself and u(s, s ' (Ret,v)) to u(s, s ' Ret)v. One then has an isomorphism I.(Q-Trans) ~e}.So Proc \ncan be = Proc, given by: I ..(I) .{seen as the free model of LProc on {Ret}. This algebra determines \nthe semantics of our language, and, in that sense, justi.es the previous, more ad hoc, account. First, \nwe have [[skip]] = .({Ret}) and I .J =({Ret}.J) (I),sothe Kleisli structure determines the semantics \nof skip and composi\u00adtion, as one would expect from the monadic point of view. Next, the update and lookup \noperations, together with the assumed primi\u00adtive natural number and boolean functions, determine the \nsemantics of assignment and conditionals; the d operation is that of the alge\u00adbra; and block is modeled \nby O. The semantics of spawning is de\u00adtermined by async together with the function -c :Proc .AProc, and \nit turns out that the latter is also determined by algebraic means. Speci.cally, one can regard AProc \nas a model of LProc, setting: asyncP (Q)= {(s, s ' )w |.u .P, (s, s ' )v .Q.w .uw v}. and yield toP (Q)= \nasyncQ(P ),and then -c is the extension of Ret .haltAProc to Proc. In the converse direction one can \nconsider adding missing al\u00adgebraic operations to the language, for example adding . and yield to via \nconstructs C or D and yield to C. The latter con\u00adstruct is to the binary yield to as async is to the \nbinary async. It generalizes yield, which is equivalent to yield to skip.Its operational semantics is \ngiven by the rule: \\s, T, E[yield to C])-.\\s, T.E[skip],C) One may debate the programming usefulness \nof such additional constructs, but they do allow one to express the equations used for the algebraic \ncharacterizations at the level of commands. For example, the equation P \u00b7 d(x)= d(P \u00b7 x) . d(x \u00b7 P ) \nbecomes: (async C); yield; D = (yield;(async C); D) or (yield;(yield to C); D)  7. Conclusion A priori, \nthe properties and the semantics of threads in general, and of cooperative threads in particular, may \nnot appear obvious. In our opinion, a huge body of incorrect multithreaded software and a relatively \nsmall literature both support this point of view. With the belief that mathematical foundations could \nprove bene.cial, the main technical goal of our work is to de.ne and elucidate the semantics of threads. \nFor instance, semantics can serve for validating reasoning principles; our work is only a preliminary \nbut encouraging step in this respect. Our initial motivation was partly practical we wanted to un\u00adderstand \nand further the AME programming model and similar ones. We also saw an opportunity to leverage developments \nin trace-based denotational semantics and in the algebraic theory of effects, and to extend their applicability \nto threads. As our results demonstrate, the convergence of these three lines of work proved interesting \nand fruitful. We focus on a particular small language with constructs for threads. Several possible extensions \nmay be considered. These in\u00adclude constructs for parallel composition, nondeterministic choice, higher-order \nfunctions, and thread-joining. More speculatively, they also include generalized yields, of the kind \nthat arise in the algebraic theory of effects, as discussed in Section 6. Importantly, our monadic treatment \nof threads indicates how to add higher-order functions to the semantics. Our results mostly carry over \nto these extensions. In some cases, small changes or restrictions are required. In particular, the full\u00adabstraction \nproof with nondeterministic choice would use fresh variables; the one for higher-order functions might \nrequire standard limitations on the order of functions, cf. [19]. Thus, our approach seems to be robust, \nand indeed as in the case of higher-order functions helpful in accounting for a range of language features. \nAnother possible direction for further work is the exploration of alternative semantics. For instance, \nwe could switch from the may semantics that we study to must semantics. We could also de.ne alternative \nnotions of observation. As suggested in Sec\u00adtion 5.3, some of the coarser notions of observation might \nre\u00adquire closure conditions, such as closure under stuttering and under mumbling. It would also be interesting \nto consider .ner notions of observation that distinguish blocking from divergence. To this end we could \nadd constructs such as orElse [14] and, in the seman\u00adtics, treat blocking as a kind of exception. Finally, \nwe could revisit lower-level semantics with explicit optimistic concurrency and roll\u00adbacks, of the kind \nemployed in the implementation of AME.  Acknowledgments Weare gratefulto Mart\u00b4in Escard\u00b4o and Martin \nHyland forcomments on this work. References [1] Mart\u00b4in Abadi, Andrew Birrell, Tim Harris, and Michael \nIsard. Semantics of transactional memory and automatic mutual exclusion. In Proceedings of the 35th ACM \nSIGPLAN-SIGACT Symposium on Principles of Programming Languages, pages 63 74, 2008. [2] Mart\u00b4in Abadi \nand Gordon D. Plotkin. A logical view of composition. Theoretical Computer Science, 114(1):3 30, June \n1993. [3] Karl Abrahamson. Modal logic of concurrent nondeterministic programs. In Gilles Kahn, editor, \nInternational Symposium on Semantics of Concurrent Computation, volume 70 of Lecture Notes in Computer \nScience, pages 21 33. Springer, 1979. [4] Atul Adya, Jon Howell, Marvin Theimer, William J. Bolosky, \nand John R. Douceur. Cooperative task management without manual stack management. In Proceedings of the \nGeneral Track: the 2002 USENIX Annual Technical Conference, pages 289 302, 2002. [5] Roberto Amadio and \nSilvano Dal Zilio. Resource control for synchronous cooperative threads. Theoretical Computer Science, \n358:229 254, 2006. [6] Nick Benton, John Hughes, and Eugenio Moggi. Monads and effects. In Gilles Barthe, \nPeter Dybjer, Lu\u00b4is Pinto, and Jo ao Saraiva, editors, Advanced Lectures from International Summer School \non Applied Semantics, volume 2395 of Lecture Notes in Computer Science, pages 42 122. Springer, 2002. \n[7] Dave Berry, Robin Milner, and David N. Turner. A semantics for ML concurrency primitives. In Proceedings \nof the 19th ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, pages 119 129, 1992. \n[8] G\u00b4erard Boudol. Fair cooperative multithreading. In Lu\u00b4is Caires and Vasco Thudichum Vasconcelos, \neditors, Concurrency Theory, 18th International Conference, volume 4703 of Lecture Notes in Computer \nScience, pages 272 286. Springer, 2007. [9] Fr\u00b4ederic Boussinot. Fairthreads: mixing cooperative and \npreemptive threads in C. Concurrency and Computation: Practice and Experience, 18(5):445 469, April 2006. \n[10] Stephen Brookes. Full abstraction for a shared-variable parallel language. Information and Computation, \n127(2):145 163, June 1996. [11] Stephen Brookes. The essence of parallel Algol. Information and Computation, \n179(1):118 149, 2002. [12] Pietro Cenciarelli and Eugenio Moggi. A syntactic approach to modularity in \ndenotational semantics. In Proceedings of the 5th Biennial Meeting on Category Theory and Computer Science, \n1993. [13] William Ferreira and Matthew Hennessy. A behavioural theory of .rst-order CML. Theoretical \nComputer Science, 216(1-2):55 107, 1999. [14] Tim Harris, Simon Marlow, Simon Peyton-Jones, and Maurice \nHerlihy. Composable memory transactions. In Proceedings of the 10th ACM SIGPLAN Symposium on Principles \nand Practice of Parallel Programming, pages 48 60, 2005. [15] Matthew Hennessy and Gordon D. Plotkin. \nFull abstraction for a simple programming language. In J. Be.cv\u00b4a.r, editor, 8th Symposium on Mathematical \nFoundations of Computer Science, volume 74 of Lecture Notes in Computer Science, pages 108 120. Springer, \n1979. [16] E. Horita, J.W.deBakker, and J.J.M.M. Rutten. Fully abstract denotational models for nonuniform \nconcurrent languages. Information and Computation, 115(1):125 178, 1994. [17] Martin Hyland, Gordon Plotkin, \nand John Power. Combining effects: Sum and tensor. Theoretical Computer Science, 357(1 3):70 99, 2006. \n[18] Michael Isard and Andrew Birrell. Automatic mutual exclusion. In Proceedings of the 11th USENIX \nworkshop on Hot Topics in Operating Systems, pages 1 6, 2007. [19] Alan Jeffrey. A fully abstract semantics \nfor a concurrent functional language with monadic types. In Proceedings, Tenth Annual IEEE Symposium \non Logic in Computer Science, pages 255 264, 1995. [20] Alan Jeffrey. Semantics for core Concurrent ML \nusing computation types. In Higher order operational techniques in semantics, pages 55 90. Cambridge \nUniversity Press, 1997. [21] Alan Jeffrey and Julian Rathke. A fully abstract may testing semantics for \nconcurrent objects. Theoretical Computer Science, 338(1 3):17 63, June 2005. [22] Microsoft. SQL Server \n2005 books online. CLR Hosted Environment, at http://msdn.microsoft.com/en-us/library/ms131047. aspx, \nSeptember 2007. [23] Prakash Panangaden and John H. Reppy. The essence of Concurrent ML. In Flemming \nNielson, editor, ML with Concurrency, chapter 1, pages 5 29. Springer, 1997. [24] Gordon Plotkin and \nJohn Power. Notions of computation determine monads. Lecture Notes in Computer Science, 2303:373 393, \n2002. [25] Gordon D. Plotkin and John Power. Algebraic operations and generic effects. Applied Categorical \nStructures, 11(1):69 94, 2003. [26] Gordon D. Plotkin and Matija Pretnar. A logic for algebraic effects. \nIn Proceedings, Twenty-Third Annual IEEE Symposium on Logic in Computer Science, pages 118 129, 2008. \n[27] Nir Shavit and Dan Touitou. Software transactional memory. In Proceedings of the 14th Annual ACM \nSymposium on Principles of Distributed Computing, pages 204 213, 1995. [28] Yannis Smaragdakis, Anthony \nKay, Reimer Behrends, and Michal Young. Transactions with isolation and cooperation. In Proceedings of \nthe 22nd Annual ACM SIGPLAN Conference on Object-Oriented Programming, Systems, Languages, and Applications, \npages 191 210, 2007. [29] J. Robert von Behren, Jeremy Condit, Feng Zhou, George C. Necula, and Eric \nA. Brewer. Capriccio: scalable threads for Internet services. In Proceedings of the 19th ACM Symposium \non Operating Systems Principles, pages 268 281, 2003. [30] Glynn Winskel. The Formal Semantics of Programming \nLanguages. The MIT Press, 1993. Appendix: Proofs This appendix gives some proofs of interest, to the \nextent that space permits. We begin with the proof of two main lemmas used in the proof of full abstraction. \nGiven two stores s and s ' , we de.ne a boolean expression check(s) as the conjunction of the formulas \nx = nfor every variable x,where nis the natural number s(x) (so check(s) is true in sand false elsewhere). \nWe also de.ne commands: goto(s) as the sequence of assignments x := n for every variable x,where nis \nthe natural number s(x);  (s . s ' ) as if check(s) then goto(s ' ) else block;  ''' ' ''' (s . s . \ns ) as (s . s ); yield;(s . s ); yield. Proof of Lemma 5.13: Letting P =[[C]] and Q =[[D]],we as\u00adsume \nthat P . Qand prove that there exists C such that [[C]](P)c . [[C]](Q)c . For this, choose a sequence \nw in P but not in Q.If w = w c, then we can take C to be []. Therefore, for the rest of the proof, we \nconsider the case w = w c . If w = w c,then w is of the form u(s,s ' Ret)v.We let ' '' '' C =[ ];(s . \ns ) where s does not appear in u or v and '' '' u(s,s ) . Q (so, by pre.x-closure, u(s,s )v . Q). Such \na choice of s '' is always possible by Lemma 5.12. Thus, [[C]](P) '' '' contains u(s,s Ret)v,and [[C]](P)c \ncontains u(s,s )v. Suppose that u(s,s '' )v is also in [[C]](Q)c, and that this is ' 'c '' because some \nsequence w is in [[C]](Q) and w = u(s,s )v. By the de.nition of the semantics of sequential composition, \nthis could arise in one of the following ways: w ' = u(s,s '' Ret)v, with w . Q. This contradicts w . \nQ. '''' '' w = u (s,s )v ' ,where u and v are of the same length as u and v, respectively, and s '' occurs \nmarked with Ret in either u ' or v ' . This contradicts the requirement that s '' does not appear in \nuor v. ' ''' ' w = u(s,s )v, w . Q,and w does not have a Ret marker. This contradicts the requirement \nthat u(s,s '' ) . Q. Proof of Lemma 5.14: Letting P =[[C]] and Q =[[D]],we assume that Pc . Qc and prove \nthat there exists C such that runs([[C]](P)) . runs([[C]](Q)). For this, choose a sequence w in Pc but \nnot in Qc . First, suppose that w is of the form (s1,s1' ) ...(sn,sn' ), with n> 0.We let C be async \n[]; mesh(w),where mesh(w) is the command yield; (s1 ' . s1 '' . s2); ...; (sn'-1 . sn''-1 . sn); (s ' \n. s '' ) nn where the stores si '' are all different from one another and from all other stores in w, \nand are such that ' ' ''' c (s1,s1) ...(si,si)(si,si ) . Q and ' ' '' '''' c (s1,s1) ...(si-1,si-1)(si-1,si)(si,si)(si,si \n) . Q Such a choice of stores si '' is always possible by Lemma 5.12. Since [[mesh(w)]] contains the \ntransition sequence: ''' '' '' ''' (s1,s1)(s1,s1 )(s1 ,s2) ...(sn-1,sn)(sn,sn Ret)Done we obtain that \n[[C]](P) contains the transition sequence: ' ''''' ' '' (s1,s1)(s1,s1)(s1,s1 )(s1 ,s2)(s2,s2) ...(sn-1,sn) \n' ''' (sn,sn)(sn,sn Ret) '''' '' ''' which generates the run s1s1s1s1 s2s2 ...sn-1snsnsn. Sup\u00adpose that \nthis run is also in runs([[C]](Q)). Therefore, there exists w ' . Qc such that ' ''''' ' '' (s1,s1)(s1,s1 \n)(s1 ,s2)(s2,s2) ...(sn-1,sn) ' ''' (sn,sn)(sn,sn) is a shuf.e of w ' with ''' '' '' ''' (s1,s1 )(s1 \n,s2) ...(sn-1,sn)(sn,sn)Done '' '' which we call w , or with a pre.x of w . We analyze the origin of \nthe transitions in the shuf.e: The transitions (si,si' ) must all come from w ' , since each of '' '' \nthe transitions in w contains one of the stores sj and, by choice, these are different from si and si' \n. Suppose that, up to some i- 1 <n, w ' starts like w,inother words as (s1,s1' ) ...(si-1,si'-1). Suppose \nfurther that, in the shuf.e up to this point, each transition (sj,sj' ) is followed im\u00ad ' '' '' mediately \nby the corresponding transitions (sj ,sj )(sj ,sj+1) from w '' . We argue that this remains the case \nup to n. We consider the next possible transition in the shuf.e, ''' ' namely (si-1,si-1). This transition \ncannot come from w because, by the choice of si''-1,we have that ' ' ''' c (s1,s1) ...(si-1,si-1)(si-1,si-1) \n. Q So this transition comes from w '' . One step further, in order to derive a contradiction, we  suppose \nthat the transition (si''-1,si) comes from w ' .So ' ' ''' w starts (s1,s1) ...(si-1,si-1)(si-1,si), \nand in fact ' ''' ' (s1,s1) ...(si-1,si-1)(si-1,si)(si,si), since, as noted above, the last transition \nhere must come from w ' . The next ' '' '' transition in the shuf.e is (si,si ). By the choice of si \n,we have that ' ' '' '''' c (s1,s1) ...(si-1,si-1)(si-1,si)(si,si)(si,si ) . Q So the transition (si' \n,si '' ) cannot come from w ' . Therefore, it must come from w '' . However, the next available transi\u00ad \n'''' ''' '' tion in w is (si-1,si),and (si,si ) and (si-1,si) must '' '' be different because si-1 and \nsi are different, by choice, from si ' and si. Thus, the assumption that the transition (si''-1,si) comes \nfrom w ' leads to a contradiction. This transition must come from w '' . Finally, suppose that, up to \nn, w ' starts like w,in other words as (s1,s1' ) ...(sn,sn' ), and that, in the shuf.e, each transi\u00adtion \n(sj,sj' ) is followed immediately by the corresponding ''''' '' '' transitions (sj,sj )(sj ,sj+1) from \nw . By the choice of s , n' ' ''' c ' '' we have that (s1,s1) ...(sn,s )(s ,s ) ./Qso (s ,s ) nnnnn comes \nfrom w '' , not from w ' . In sum, w ' = w, and therefore w . Qc, contradicting our assump\u00adtion that \nw . Qc . Next, suppose that wis of the form (s1,s1' ) ...(sn,sn' )Done. With the same C , we obtain that \n[[C ]](P) contains: ' ''''' ' '' (s1,s1)(s1,s1)(s1,s1 )(s1 ,s2)(s2,s2) ...(sn-1,sn) ' ''' (sn,sn)(sn,sn \nRet)Done '''' '' ''' which generates the run s1s1s1s1 s2s2 ...sn-1snsnsnDone. Suppose that this run is \nalso in runs([[C ]](Q)). Again, by the choice '' '' of s1 , ..., sn, this can be the case only if w is \nin Qc.(The argument for the contradiction may actually be simpli.ed in this case, because of the marker \nDone.) We continue with proofs of algebraic characterization theorems. We write .Cpo, .SL,and .CpoI. \nfor, respectively, the cate\u00adgories of .-cpos, .-cpos with all .nite sups, and the Kleisli cat\u00adegory of \nI .. The latter two have countable biproducts, given by cartesian product in the .rst case, and the sum \nof posets in the sec\u00adond; in particular, the copower Store \u00d7 P is the usual cartesian product of posets \nand we identify PStore with Store \u00d7 P. LEMMA 7.1. Suppose that R carries the free structure over P in \n.CpoI. which is a model of LS in .CpoI. and which has a map dR : R. . R.Then I .(R) carries the free \nstructure over I .(P) in .SL which is a model of LS in .SL, and which has a map d: M. . M and: S (updateI.(R))x,n(I)= \nSu.I (updateR)x,n(u) (lookupI.(R))l(f)= (lookupR)l(v,x) x.Nat,u.f(x) S dI.(R)(I)= u.I dR(u) . dR(. ) \nProof of Theorem 6.1: By Corollary 2 of [17], the free model (R,updateR,lookupR) of LS in .CpoI. together \nwith a morphism R. . R over a poset Q has carrier the solution of the following domain equation in .CpoI. \n: R ~S =(S\u00d7 (R. + Q)) by which we mean the initial such R, and where we abbreviate Stores to S. Since \ncountable copowers and powers coincide in .CpoI. , this can be rewritten as: R ~ =(S\u00d7 S) \u00d7 (R. + Q) As \nthe left adjoint Pos . .CpoI. preserves all colimits we can solve this domain equation by solving it \ninstead in Pos and that can be done by taking Rto be the least set such that R =(S\u00d7 S) \u00d7 (R. + Q) and \nthen imposing the evident inductively de.ned partial order. It is not hard to see that R. is Q-BTrans.The \nmap dR : R. . Rin .CpoI. is: inl.R.+QS R. -. R. + Q- ---. (S\u00d7 (R. + Q))= R and is given by: dR(x)= { \n((s,s),inl(x)) | s . S}. The map (updateR)l,v : R . Ris: updateR.+Q R = TS (R. + Q) --------. TS(R. + \nQ)= R and is given by: (updateR)l,v((s,s),x)= ((s,s[l . v]),x) . Similarly (lookupR)l : RV . Ris given \nby: (lookupR)l(v,((s,s),x)) = { ((s,s),x) | s(l)= v}. By Lemma 7.1 I .(R) then carries the free structure \nM over I .(Q) in .SL which is a model of LS in .SL and which has a map M. . M. By Theorem 8 of [17], \nto have a model of LRes is to have such a structure, so I .(R) carries the free model of LRes over I \n.(Q). There is an evident isomorphism Q-BTrans ~R, so the free = such model is also carried by I .(Q-BTrans). \nUsing the above and Lemma 7.1 one then veri.es that the operations are as required. For the formula for \nthe Kleisli extension, that f.. = f is evident and that the purported extension is a morphism of models \nof LRes is a calculation. Proof of Theorem 6.2: To have a model of LAProc in .Cpo is to have a model \nM of LS in .SL together with a map d: M. . M andanelement halt . M such that d(O) = halt.Further,to have \nsuch a map and element is to have a map (M +1). . M. With these observations the proof proceeds analogously \nto that of Theorem 6.1. Proof of Theorem 6.3: To show that I .(Q-Trans) is the free al\u00adgebra over I \n.. (Q) with unit . as above, we must show that for any LProc-model Aand any continuous function f: I \n.. (Q) . Athere is a unique morphism h: I .(Q-Trans) . A of models of LProc such that h. = f. We begin \nby showing uniqueness. To this end, suppose we are given such an A, f, and morphism h such that h. = \nf. Then for uniqueness it suf.ces to prove that h.TRes = g where g((x,u) . )= u\u00b7 A f(x),as h is a morphism \nof models of LProc. We calculate: h(.TRes ((q,u) . )) = h({ (s,s(q,u)) | s . Stores} ) = h(u .\u00b7{ (s,s(q,Done)) \n| s . Stores} ) = h(u .\u00b7 .(q . )) = u .\u00b7 h(.(q . )) = u .\u00b7 Af(q . ) For existence we are again given \nAand f and wish to construct a suitable h. To this end, with g as before, take h to be the TRes \u00adextension \nof g.Then h. = f and it remains to prove that h preserves async and yield to. It suf.ces to prove this \nfor individual transition sequences, by induction invoking LProc equations on A as necessary. The formula \nfor the Kleisli extension follows using the Kleisli formula of Theorem 6.1.  \n\t\t\t", "proc_id": "1480881", "abstract": "<p>We develop a model of concurrent imperative programming with threads. We focus on a small imperative language with cooperative threads which execute without interruption until they terminate or explicitly yield control. We define and study a trace-based denotational semantics for this language; this semantics is fully abstract but mathematically elementary. We also give an equational theory for the computational effects that underlie the language, including thread spawning. We then analyze threads in terms of the free algebra monad for this theory.</p>", "authors": [{"name": "Martin Abadi", "author_profile_id": "81100547147", "affiliation": "Microsoft Research, Silicon Valley, CA and University of California, Santa Cruz, Santa Cruz, CA, USA", "person_id": "P1300922", "email_address": "", "orcid_id": ""}, {"name": "Gordon Plotkin", "author_profile_id": "81100235459", "affiliation": "University of Edinburgh, Edinburgh, United Kingdom and Microsoft Research, Silicon Valley, CA", "person_id": "P1300923", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1480881.1480887", "year": "2009", "article_id": "1480887", "conference": "POPL", "title": "A model of cooperative threads", "url": "http://dl.acm.org/citation.cfm?id=1480887"}