{"article_publication_date": "01-21-2009", "fulltext": "\n Automatic Modular Abstractionsfor Linear Constraints David Monniaux CNRS/VERIMAG, Grenoble David.Monniaux@imag.fr \nAbstract We propose a method for automatically generating abstract trans\u00adformers for static analysis \nby abstract interpretation. The method focuses on linear constraints on programs operating on rational, \nreal or .oating-point variables and containing linear assignments and tests. In addition to loop-free \ncode, the same method also applies for obtaining least .xed points as functions of the precondition, \nwhich permits the analysis of loops and recursive functions. Our algorithms are based on new quanti.er \nelimination and symbolic manipulation techniques. Given the speci.cation of an abstract domain, and a \nprogram block, our method automatically outputs an implementation of the corresponding abstract transformer. \nIt is thus a form of program transformation. The motivation of our work is data-.ow synchronous program\u00adming \nlanguages, used for building control-command embedded systems,but it also applies to imperative and functional \nprogram\u00adming. Categories and Subject Descriptors F.3.1[Logics and Mean\u00adings of Programs]: Specifying \nandVerifying and Reasoning about Programs Invariants, Mechanical veri.cation, Pre-and post\u00adconditions; \nF.3.2[Logics and Meanings of Programs]: Seman\u00adtics of Programming Languages Program analysis; F.4.1[Math\u00adematical \nLogic and Formal Languages]: Mathematical Logic Mechanical theorem proving General Terms algorithms, \nlanguages, theory, veri.cation Keywords quanti.er elimination, abstract interpretation, program transformation, \nlinear inequalities 1. Introduction In program analysis, it is often necessary to prove or infer numer\u00adical \nproperties of programs, for instance, in order to prove cer\u00adtain relationships between array indices, \nor to prove the absence of over.ows. Static program analysis by abstract interpretation ob\u00adtains properties \nof variables, or of relationships between variables, representable in an abstract domain. Examples of \nclassical nu\u00admerical abstract domains for numerical properties include intervals (Cousot and Cousot 1976) \n to each variable x one attaches an interval [xmin,xmax] and convex polyhedra (Cousot and Halb- Permission \nto make digital or hard copies of all or part of this work for personal or classroom use is granted without \nfee provided that copies are not made or distributed for pro.t or commercial advantage and that copies \nbear this notice and the full citation on the .rst page.To copyotherwise, to republish, to post on servers \nor to redistribute to lists, requires prior speci.c permission and/or a fee. POPL 09, January 18 24, \n2009, Savannah, Georgia, USA. Copyright c . 2009ACM 978-1-60558-379-2/09/01... $5.00 wachs 1978) conjunctions \nof inequalitiesa1x1 +\u00b7\u00b7\u00b7+anxn = c are inferred. For each implemented numerical domain and each program \nin\u00adstruction,the static analyzer must provide an abstract transfer func\u00adtion, which maps the property \nbefore the instruction to a safe prop\u00aderty after the instruction (for forward analysis; the reverse is \ntrue of backward analysis).For instance,over the intervals, z=x+y is opti\u00admally abstracted as zmax = \nxmax +ymax and zmin = xmin +ymin; the transfer functions for polyhedra are more complex. While the designers \nof abstract interpreters generally strive so thatthe output property is optimal (the interval [zmin,zmax] \nde.ned aboveis the least possible one for the inclusion ordering), optimality is not pre\u00adserved by composition. \nConsider, for instance, y=x; z=x-y; with the precondition that x . [0, 1]. The interval for z, obtained \nfrom those for x and y by applying the rules of interval arithmetics, is [-1, 1];yet, the optimal interval \nis{0}. The reason for this loss of precision is that while the computation of the interval for z from \nthose for x and y is locally optimal, it does not take into account the relationship between x and y. \nOur initial target application was programs written in syn\u00adchronous data-.ow languages such asL USTRE \n(Caspietal. 1987), SIMULINK or SCADE (Caspi et al. 2003). In these languages, op\u00ad erators are built out \nof elementary operators, introducing many intermediate variables. Successions of small elementary operations \nmay also occur when analyzing low-level code, e.g. assembly (Bal\u00ad akrishnan and Reps 2004; Gopan and \nReps 2007) or Java byte\u00ad code, and they hamper certain static analysis methods due to the reduced size \nof the code window used for transfer functions (Lo\u00ad gozzo andF\u00a8ahndrich 2008). Analyzing .oating-point \ncode at the assembly level may actually be easier than analyzing higher-level programs, since the semantics \nof elementary .oating-point opera\u00adtions are usuallyfairly well-de.ned while the de.nition and com\u00adpiling \nprocesses of higher-level languages may leave signi.cant leeway (Monniaux 2008b). It is therefore important, \nfor such ap\u00ad plications, to be able to analyze program blocks as a whole and not as a succession of independent \noperations. In the above simple example, we could obtain better precision by using a relational abstract \ndomain linking the inputs and the outputs of the procedure. In general, though, the code fragment may \ncontain tests and loops (or, more generally, semantic .xed points), which complicates the matter (see \nSec. 3.4.3 for a short example whose semantics involves a .xed point). Ideally, for better precision, \nthe analyzer should provide a (hopefully optimal) abstract transfer function for each possible program \nblock (fragment of code without loops). However, the de\u00adsigners of the analyzer cannot include a hand-coded \nfunction for each possible program block to be analyzed, if only because the number of possible program \nblocks is in.nite. Also, the user might want to use abstract domains not pre-programmed in the analyzer. \nWe would like that abstract transfer functions be obtained auto\u00admatically from the de.nition of the abstract \ndomain and the source code (or semantics) of the program block. In this article, we show how to automatically \ntransform pro\u00adgram blocks without loops into an effectiveimplementation of their optimal abstract transfer \nfunction. This optimal transformer maps constraints on the block inputs to the tightest possible constraints \non the block output. This transformation is parametric in the ab\u00adstract domain used: it takes as inputs \nboth the program block and a speci.cation of the abstract domain, and outputs the corresponding transfer \nfunction. The same method applies for both forward and backward analysis by abstract interpretation, \nthough, for the sake of simplicity, the article focuses on forward analysis. For short, our analysis \nconsiders the exact transition relation of loop-free program fragments as anexistentially quanti.ed formula. \nFromthatformula,itisableto computethe optimal abstract trans\u00adformer for the fragment with respect to \na user-speci.ed abstract domain, or even for the least invariant of the fragment in that ab\u00adstract domain. \nThe user may specify any abstract domain in the wide class of template linear abstract domains (Colon \net al. 2003). Our method is based upon quanti.er elimination in the theory of rational linear arithmetic. \nIt has long been known that this theory admitted quanti.er elimination, but algorithms remained mostly \nimpractical. Recent improvements in SAT/SMT solving techniques have made it possible to perform quanti.er \nelimination on larger formulas (Monniaux 2008a). We also show how to obtain transfer functions for loops, \nwhich arealso optimalinacertain sense(theycomputethe least inductive invariant representable in the abstract \ndomain). In the beginning of the article, we focus on simple forward analysis of loop-free blocks, then \nsingle loops (or single .xed points), for programs dealing with real or rational variables. The same \nmethods apply to integer variables, at the expense of some added abstraction. We show in later sections \nhow to deal with various constructions, including nested loops and arbitrary control\u00ad.ow graphs, recursive \nprocedures and .oating-point computations. Our focus was indeed, originally, synchronous data-.ow programs \noperating over real (for modeling) or .oating-point (for execution) variables,butwe realizedthatthe same \ntechnique couldapplytoa wider spectrum of languages. Our analysis goes further than most constraint-based \nstatic anal\u00adysis (Sankaranarayanan et al. 2004, 2005) in that it computes the general form of the optimal \npostcondition or least inductive in\u00advariant as a function of the precondition parameters, not just for \nspeci.c values of those parameters. For a simple example, if the procedure is invoked on the interval \ndomain and the z := x + y operation, our transformation outputs zmin := xmin + ymin and zmax := xmax \n+ ymax. This is especially important since the func\u00adtion mapping the input parameters to the output parameters \nmay be non convex (a simple example is the abstraction of the absolute value with respect to intervals \nfrom Sec. 3.2). Intheabove case,the abstract transfer functionis linear,butin general it is only piecewise \nlinear. It can be expressed as a simple executable program, consisting only of tests and assignments \n(see an example at the end of Sec. 3.2). The analysis thus amounts to a program transformation from the \nconcrete to the abstract program. An advantage of obtaining the abstract transfer functions in such a \nform is that it can be compiled as an ordinary program and loaded back into the analyzer for maximal \nef.ciency. The abstract transfer function obtained by the analysis of a block may be retained for future \nuse, since it is valid in any context. An application of our transformation is therefore modular interprocedural \nanalysis. Wehave sofar considered analyzes where the constraints apply to program variables at a given \ncontrol point. It is also possible to consider relationships between variables at two different control \npoints, especially the entry and exit of procedures. This way, we can also analyze programs with recursive \nprocedures, including the famous McCarthy91 function (Manna and McCarthy1969; Manna and Pnueli 1970). \n Contrary to most analyzes of numerical properties based on ab\u00adstract interpretation, our analysis for \nloops does not use widening operators for .ndingover-approximationsof least.xed points.For instance, \nthe set of reachable states at the start of a loop (a loop invariant)is expressed as the least .xed point \nof the transition re\u00adlation that contains the input precondition. In widening-based an\u00adalyzes, over-approximations \nof the set of reachable states after 1, 2, 3, etc. loop iterations are computed, and the analyzer tries \nto ex\u00adtrapolate these results in order to obtain some candidate for being a loop invariant.For instance, \nan abstract analyzer based on inter\u00advals may obtain [1, 2], [1, 3], [1, 5], and, because the lower bound \nof the interval stays stable and the upper bound is unstable, may try [1, +8[. If [1, +8[ is stable under \nthe transition relation, then itisa safeinvariant, otherwise further wideningis needed.Widen\u00adingsareamajor \nsourceof imprecisioninmanystatic analyzersand their design is somewhat of a black art . While the soundness \nof the transition relation and the stability test ensure that the analy\u00adsis results are correct, and \nthe correct construction of the widening operator ensures termination,the qualityoftheover-approximation \nobtained (whetheritis closetothe actual leastinvariantorfarfrom it)dependsonvariousfactors.In contrast,ourmethodis \nguaranteed to yield least inductive invariants. In Sec.2, we recallfactsof formulasbuilt outof linear \ninequal\u00ad ities. In Sec. 3.1 we de.ne the class of abstract domains that we consider. In Sec. 3.2, we \nshow how we obtain optimal abstract transformers as logical formulas, and in Sec. 3.3 how to compile \nthese formulas into executable functions. In Sec. 3.4 we show how thesame processappliestoleast inductiveinvariants.InSec.4 \nwe show how to deal with various extensions to the admissible do\u00admains and operations: how to allow in.nite \nvalues for constraint parameters, how to allow some class of non-convex domains, how to partition the \nstate space, and how to model .oating-point com\u00adputations using real numbers. In Sec.5 we shall see how \nto deal with recursive procedures and arbitrary control-.ow graphs. 2. Linearformulas We consider logical \nformulas built out of linear inequalities. A linear expression isa sum a1v1 + \u00b7\u00b7\u00b7 + anvn where the ai \n. Q and the vi are variables. Qdenotes the .eld of rational numbers, R the .eldofreal numbers.Alinear \ninequalityisoftheform l> 0 or l = 0, wherel isalinearexpression. Linear inequalities canalways be scaled \nso that they use only integer coef.cients, as opposed to rationals. a = b = c is shorthand for a = b \n. b = c. Unquanti.ed formulas arebuilt outof atomic formulas (linear inequalities) using logical connectives \n. and .. l =0 means l = 0 .l = 0.Aformula is said to be in disjunctive normal form (DNF) if it is written \nas a disjunction C1 .\u00b7 \u00b7\u00b7. Cn, where each of the Ci is a conjunction Ai,1 .\u00b7\u00b7\u00b7. Ai,nj where the Ai,j \nare atomicformulas or negations thereof. Quanti.ed formulas are built out of the same, plus the universal \nand existential quanti.ers . and .. The Q-models (respectively, R-models) of a formula F are mappings \nm from the free variables of F to Q (respectively, R) such that m veri.es the formula; we then note m \n|= F . F is said to be true if every assignment is a model (a model is a mapping from the setofvariablesto \nQor R),satis.able ifithasamodel,and false or unsatis.able otherwise.Truth and satis.ability are equivalent \nif F has no free variables. We say that two formulasF and G with the same free variables are equivalent, \nnoted F = G, if theyhave the same models. Any formula is equivalent to a formula in disjunctive normal \nform, which can be obtained by repeated application of distributivity: a . (b . c) = (a . b) . (a . c). \nF is said to imply G, noted F = G, if all models of F are models of G. We say that F and G are equivalent \nmodulo assumptions T , noted F =T G, if F . T = G .T ;we de.ne similarlyF = T G as F .T = G .T . Equivalences \nmodulo assumptions are often used when simplifying formulas.For instance,ifweknowthata certain programisalways \nused in a context where T = a<b holds, and program analysis, at some point, generates the formula F = \n.xa = x = b, then this formula can be simpli.ed to G = true. The theory of linear inequalities admits \nquanti.er elimination: for anyformula F with quanti.ers, there exists a formula G with\u00adout quanti.ers \nsuch that G = F . Thereexist several algorithms that compute such a G from F . Ferrante and Rackoff(1975) \nproposed a doublyexponential method (Bradleyand Manna 2007, Sec. 7.3), which is too slow in practice; \nwe have since proposed another al\u00adgorithm that takes advantage of the recent improvements in satis\u00ad.ability \ntesting technology. (Monniaux 2008a) Our algorithm also allows conversion to disjunctive normal form, \nand formula simpli\u00ad.cation modulo assumptions. 3. Optimal AbstractionoverTemplate Linear Constraint Domains \n3.1 Template Linear Constraint Domains Let F be a formula over linear inequalities. We call F a do\u00admain \nde.nition formula if the free variables of F split into n pa\u00adrameters p1,...,pn and m state variables \ns1,...,sm. We note .F : Qn .P(Qm) de.nedby .F (p )= {ps . Qm | (pp, ps) |= F }. As an example, the interval \nabstract domain for 3 program vari\u00adables s1,s2,s3 uses6 parameters m1,M1,m2,M2,m3,M3;the formula is m1 \n= s1 = M1 . m2 = s2 = M2 . m3 = s3 = M3. In this section, we focus on the case where F is a conjunction \nL1(s1,...,sm) = p1 . \u00b7 \u00b7\u00b7 . Ln(s1,...,sm) = pn of linear inequalities whose left-hand side is .xed and \nthe right-hand sides are parameters. Such conjunctions de.ne the class of template linear constraint \ndomains (Colonetal. 2003).Particularexamples of abstract domains in this class are: the intervals (for \nanyvariable s, consider the linear forms s and -s);  the difference bound matrices (for any variables \ns1 and s2, consider the linear form s1 - s2);  the octagon abstract domain (for anyvariables s1 and \ns2, dis\u00adtinct or not, consider the linear forms \u00b1s1 \u00b1 s2)(Min\u00b4e 2001)  the octahedra (for any tuple \nof variables s1,...,sn, consider the linear forms \u00b1s1 \u00b7\u00b7\u00b7\u00b1 sn). (Claris\u00b4o and Cortadella 2004)  Remark \nthat .F is in general not injective, and thus one should distinguish the syntax of the values of the \nabstract domain (the vector of parameters p )and theirsemantics .F (p ). As an example, if one takes \nF to be s1 = p1 . s2 = p2 . s1 + s2 = p3, then both (p1,p2,p3) = (1, 1, 2) and (1, 1, 3) de.ne the same \nset for state variables s1 and s2. If pu = pv coordinate-wise, then .F (pu) . .F (pv), but the converse \nis not true due to the non\u00aduniqueness of the syntactic form. Take any nonempty set of states W . Qm.Take \nfor all i = 1,...,m: pi = supss. W Li(ps). Clearly, W . .F (p1,...,pm), and infact pp is such that .F \n(p ) is the least solution to this inclusion. pi belongs in general to R.{+8}, not necessarily to Q.{+8}. \nv (for instance, if W = {s1 | s12 = 2} and L1 = s1, then p1 =2). We have therefore de.ned an aF : P(Rm) \n. {.} . (R . {+8})n, and (aF ,.F ) form a Galois connection: aF maps any set to its best upper-approximation. \nThe .xed points of aF ..F are the normal forms.For instance, s1 = 1 . s2 = 1 . s1 + s2 = 2 is in normal \nform, while s1 = 1 . s2 = 1 . s1 + s2 = 3 is not.  3.2 Optimal AbstractTransformersforProgram Semantics \nWe shall consider the input-output relationships of programs with rationalorrealvariables.We .rst narrowthe \nproblemto programs without loops and consider programs consisting in linear arithmetic assignments, linear \ntests, and sequences. Noting a, b, . . . the val\u00adues of program variables a, b ... at the beginning of \nexecution and ' a,b' ,... the output values, the semantics of a program P is de\u00ad.ned as a formula [P \n] such that (a, b, . . . , a' ,b' ,... ) |= P if and ' only if the memory state (a,b' ,... ) can be \nreached at the end of an execution starting in memory state (a, b, . . . ): Arithmetic [a ' := L(a, b, \n. . . )+ K]F = a= L(a, b, . . . )+ K . b' = b . c= c . ... where K is a real constant and L is a linear \nform, and b, c, d . . . are all the variables except a; Tests [if c then p1 else p2] . =(c . [p1]F ) \n. (\u00acc . [p2]F ); ' = b' for all variables except a;  Non deterministic choice [a := random] .= b.c= \nc.... , Failure [fail] . = false; '' Skip [skip] .= a . b' = c . ... = a= b . c Sequence [P1; P2]F \n= .a,b'' ,... '' f1 . f2 where f1 is [P1]F where a' has been replaced by '' a, b' by b'' etc., f2 is \n[P2]F where a has been replaced by a, b by b'' etc. In addition to linear inequalities and conjunctions, \nsuch formu\u00adlas contain disjunctions (due to tests and multiple branches) and existential quanti.ers (due \nto sequential composition). Note that sofar, we have represented the concrete denotational semantics \nexactly. This representation of the transition relation usingexistentially quanti.ed formulasisevidentlyasexpressiveas \na representation by a disjunction of convex polyhedra (the latter can be obtained from the former by \nquanti.er elimination and conversion to disjunctive normal form), but is more compact in general. This \nis why we defer quanti.er elimination to the point where we compute the abstract transfer relation. Consider \nnowadomain de.nition formula F = L1(s1,s2,... ) = p1 .\u00b7 \u00b7 \u00b7.Ln(s1,s2,... ) = pn on the program inputs, \nwith param\u00ad eters p and free variables ps, and another F ' = L1'(s1,s2,... ) = '''' p1 .\u00b7 \u00b7\u00b7. L'(s1,s2,... \n) = pon the program outputs, with pa\u00ad nn rameters pp' and free variables sp' . Sound forward program \nanalysis consistsin derivingasafe post-condition fromaprecondition: start\u00ading from anystate verifying \nthe precondition, one should end up in the post-condition. Using our notations, the soundness condition \nis written .ps, sp' F . [P ] =. F ' (1) The freevariablesof this relation are pp and pp' :the formula \nlinks the valueofthe parametersoftheinput constraintsto admissiblevalues of the parameters for the output \nconstraints. Note that this sound\u00adness condition can be written as a universally quanti.ed formula, with \nno quanti.er alternation. Alternatively, it can be written as a conjunction of correctness conditions \nfor each output constraint pa\u00ad rameter: Ci'= .ps, sp' F . [P ] =. L'i(sp' ) = pi'. Let us take a simple \nexample: if P is the program instruction z := x + y, F = x = p1 . y = p2, F ' = z = p1, then ' = z= x \n+ y, and the soundness condition is .x, y, z (x = [P ] . ' p1 .y = p2 .z = x+y =. z = p1). Remark that \nthis soundness ' condition is equivalent to a formula without quanti.ers p1 = p1 + p2, which may be \nobtained through quanti.er elimination. Remark ' also that while any value for p1 ful.lling this condition \nis sound ' (for instance, p1 = 1000 for p1 = p2 =1), only one value is ' optimal (p1 =2 for p1 = p2 \n=1).An optimalvalueforthe output parameter pi is de.ned by Oi'= Ci'..qi (Ci'[qi/p'i]=. pi = qi' ). Again, \nquanti.er elimination can be applied; on our simple example, it yields p1 ' = p1 + p2. If there are n \ninput constraint parameters p1,...,pn, then the optimal value for each output constraint parameter pi \n' is de.ned by a formula Oi ' with n +1 freevariables p1,...,pn,pi' . This formula de.nesapartial function \nfrom Qn to Q, in the mathematical sense: for each choice of p1,...,pn, there exist at most a single pi' \n. The values of p1,...,pn for which there exists a corresponding p ' i make up the domain of validity \nof the abstract transfer function. Indeed, this functionisin general not de.nedeverywhere; consider for \ninstance the program: if (x>=10){y=random;} else{y =0;} '' ' If F = x = p1 and F = y = p1, then O1 ' \n= p1 < 10 . p1 =0, and the function is de.ned only for p1 < 10. At this point, we have a characterization \nof the optimal abstract transformer corresponding to a program fragment P and the input and output domain \nde.nition formulas as n formulas (where n is the number of output parameters) Oi ' each de.ning a function \n(in the mathematical sense) mapping the input parameters p to the output parameter pi' . Another example: \nthe absolute value function y := |x|, again with the interval abstract domain. The semantics of the operation \nis (x = 0 . y = x) . (x< 0 . y = -x); the precondition is x . [xmin,xmax] and the post-condition is y \n. [ymin,ymax]. Acceptable values for (ymin,ymax) are characterized by formula G = .xxmin = x = xmax =. \nymin =|x|= ymax (2) '' The optimalvalue for ymax is de.nedbyG..ymax G[ymax/ymax] ' =. ymax = ymax. Quanti.er \nelimination over this last formula gives as characterization for the least, optimal, value for ymax: \n(xmin + xmax = 0 . ymax = xmax). (xmin + xmax < 0 . ymax = -xmin). (3) We shall see in the next sub-section \nthat such a formula can be automatically compiled into code such as: if (xmin + xmax >= 0) { ymax = xmax; \n} else { ymax = -xmin; }  3.3 Generation of the Implementation of the Abstract Domain Consider formula \n3, de.ning an abstract transfer function. On this disjunctivenormal form we see that the function we \nhavede.ned is piecewise linear:several regions of the range of the input parame\u00adters are distinguished \n(here, xmin + xmax < 0 and xmin + xmax = 0), and on each of these regions, the output parameter is a \nlinear function of the input parameters. Given a disjunct (such as ymax = -xmin . xmin + xmax < 0), the \ndomain of validity of the dis\u00adjunct can be obtained by existential quanti.er elimination over the resultvariable \n(here .ymax (ymax = -xmin . xmin + xmax < 0)). The union of the domains of validity of the disjuncts \nis the domain of validity of the full formula. The domains of validity of distinct disjuncts canoverlap,butin \nthis case, since Oi ' de.nes a function in the mathematical sense, the functions de.ned by such disjuncts \ncoincide on their overlapping domains of validity. This suggests a .rst algorithm for conversion to an \nexecutable form: 1. Put Oi ' into quanti.er-free, disjunctive normal form C1 .\u00b7 \u00b7\u00b7. Cn. 2. For each disjunct \nCi, obtain the validity domain Vi as a con\u00adjunction of linear inequalities and solve for pi ' (obtain \npi ' as a linear function vi of the p1,...,pn). 3. Output the result as a cascade of if-then-else and \nassignments, as in the example at the end of Sec. 3.2.  Algorithm1TOITETREE(F, z, T ):turn a formula \nde.ningz as a function of the other free variables of F into a tree of if-then-else constructs, assuming \nthat T holds. D(= C1 .\u00b7 \u00b7\u00b7. Cn) . QELIMDNFMODULO({}, F, T ) for allCi . D do Pi . QELIMDNFMODULO(z, F, \nT ) endfor P . PREDICATES(P1,...,Pn) if P = \u00d8 then Ensure: .zF is always true O . SOLVE(D, z) else K \n. CHOOSE(P ) O . IfThenElse(K, TOITETREE(F, z, T . K), TOITETREE(F, z, T .\u00acK)) end if An if-then-else \ncascade may beinef.cient, since identical con\u00additions may have to be tested several times. We could of \ncourse factoroutallconditionsandassignthemto Booleanvariables,but then, some of the tests performed may \nactually not be needed.We therefore propose an algorithm forbuilding an if-then-else tree. The idea of \nthe algorithm is as follows: Each path in the if-then-else tree corresponds to a conjunction C of conditions \n(if one goes through the if branch of if (a) and the else branch of if (b), then the path corresponds \nto a .\u00acb).  The formula Oi ' is simpli.ed relatively to C, a process that prunes out conditions that \nare always or never satis.ed when C holds.  If the path is deep enough, then the simpli.ed formula becomes \na conjunction. One then solves this conjunction to obtain the computed variable (here, ymax)as a function. \n  Our algorithm TOITETREE(F, z, T ) (Alg. 1) uses a func\u00adtion QELIMDNFMODULO(pv, F, T ) that, given \na possibly empty vector of variables pv, a formula F and a formula T , outputs a quanti.er-free formula \nF ' in disjunctive normal form such that F ' =T .pv F and no useless predicates are used.PREDICATES(F \n) returns the set of atomic predicates of F . SOLVE(D, z) solves a minimal disjunction D of inequalities \nfor variable z, assuming that there is at most one solution for z for each choice of the other vari\u00adables; \none simple way to do that is to look for anyconstraint of the form z = L or z = L and output z = L.CHOOSE(P \n) chooses any predicate in P (one good heuristic seems to be to choose the most frequent in P1,...,Pn). \nLetustake,asasimpleexample, formula3.Wewishto obtain ymax as a function of xmin and xmax, soin the algorithmTOITE-TREE \nwe set z = ymax. C1 is the .rst disjunct xmin + xmax = 0 . ymax = xmax,C2 is the second disjunct xmin+xmax \n< 0.ymax = -xmin.We project C1 and C2 parallel to ymax, obtaining respec\u00adtively P1 =(xmin +xmax = 0) \nand P2 =(xmin +xmax < 0).We choose K to be the predicate xmin + xmax = 0 (in this case, the choice does \nnot matter, since P1 and P2 are the negation of each other). Figure 1. The least .xed point representable \nin the domain (lfp (a . f . .))is not necessarily the least approximation of the least .xed point(a(lfp \nf ))inside the abstract domain.For instance, if we take a program initialized by x . [-1, 1] and y =0, \nand at each iteration, we rotate the point by 45., the least invariant is an 8-point star, and the best \napproximation inside the abstract domain of intervals is the square [-1, 1]2. However,this square is \nnot an in\u00adductive invariant: no rectangle (product of intervals) is stable under the iterations, thus \nthere is no abstract inductive invariant. The .rst recursive call to TOITETREE is made in the context \nof T =(xmin + xmax = 0). Obviously, F . T = (ymax = xmax) . T and thus (.ymaxF ) . T = T . QELIMDNFMODULO(ymax, \nF, T ) will then simply output the formula true . It then suf.ces to solve for ymax in ymax = xmax. This \nyields the formula for computing the correct value of ymax in the cases where xmin + xmax = 0.  The \nsecond recursivecall is made in the context of T =(xmin+ xmax < 0. The result is ymax = -xmin, the formula \nfor computing the correct value of ymax in the cases where xmin + xmax < 0.  These two results are then \nreassembled into an if-then-else state\u00adment, yielding the program at the end of \u00a73.2. The algorithm terminates \nbecause paths of depth d in the tree of recursive calls correspond to truth assignments to d atomic predi\u00adcates \namong those found in the domains of validity of the elements of the disjunctive normal form of F . Since \nthere is only a .nite number of such predicates, d cannotexceed that number.Asingle predicate cannotbeassignedtruthvaluestwicealongthesamepath \nbecause the simpli.cation processinQELIMDNFMODULOerases this predicate from the formula.  3.4 Least \nInductive Invariants We have so far considered programs without loops. We shall now see that not only \ncan we compute the optimal abstract post\u00adcondition of a block as a simple, executable function of the \nparam\u00adeters of the precondition,but we can also compute the parameters of the least inductive invariant \nof a program block that is of the form speci.ed by the abstract domain.1 Beware that this least in\u00adductiveinvariant \nfoundinthe abstract domainisin generaldifferent from the least element of the abstract domain that includes \nthe least inductive invariant of the system (Fig. 1). 1In order to specify the least invariant, we would \nhave to quantify over all setsof states,then .lter thosewhich are inductiveinvariants. Thisis second\u00adorder \nquanti.cation, which we cannot handle.By restricting ourselves to invariantsofa certain shape, we replaceitby \n.rst order quanti.cation. 3.4.1 Stability Inequalities Consider a program fragment: while (c) { p; }.We \nhave do\u00admain de.nition formulas F = L1(s1,...,sm) = p1 . \u00b7\u00b7\u00b7 . Ln(s1,...,sm) = pn for the precondition \nof the program frag\u00adment, and F ' = L1' (s1,...,sm) = p1 ' .\u00b7\u00b7\u00b7. L ' (s1,...,sm) = npn ' for the invariant. \nDe.ne G = [c] . [p]. G is a formula whose free variables are '' '' s1,...,sm,s1,...,sm such that (s1,...,sm,s1,...,sm) \n|= G if and only if the state (s1' ,...,sm' ) can be reached from the state (s1,...,sm) inexactly one \niterationoftheloop.Aset W . Qm is said to be an inductive invariant for the head of the loop if .ps . \nW, .sp' (ps, sp') |= G =. sp' . W . We seek inductive ' invariants of the shape de.ned by F ', thus \nsolutions for p of the stability condition: .ps, sp' F ' . G =. F ' [sp' /ps]. (4) Not onlydo wewant \nan inductiveinvariant,but we alsowant the initial states of the program to be included in it. The condition \nthen becomes '' ' H =(.ps, F =. F ) . (.ps, sp' F . G =. F [sp'/ps]) (5) This formula links the values \nof the input constraint parameters p1,...,pn to acceptable values of the invariant constraint param\u00adeters \np1' ,...,pn' . In the same way that our soundness or correct\u00adness condition on abstract transformers \nallowed any sound post\u00adcondition, whether optimal or not, this formula allows anyinduc\u00adtive invariant \nof the required shape as long as it contains the pre\u00adcondition, not just the least one. The intersection \nof sets de.ned by p ' 1 and p ' 2 is de.ned by min(p ' 1,p ' 2). More generally,the intersectionofafamilyof \nsets, unbounded yet closed under intersection, de.ned by p ' . Z is de.ned by min{p ' | p ' . Z}.We take \nfor Z the set of accept\u00adable parameters p ' such that p ' de.nes an inductive invariant and .ps, F =. \nF ';that is, we consider only inductive invariants that contain the set I = {ps | F } of precondition \nstates. ' '' We deduce thatpi is uniquely de.ned by: pi = min(.p1,..., '' ' pi-1,pi+1,...,pn H) which \ncan be rewritten as '''' '' (.p1,...,pi-1,pi+1,...,pn H) . (.qp' H[qp' /p ' ]=. pi = qi) (6) The free \nvariables of this formula are p1,...,pn,pi' . This formula de.nes a function (in the mathematical sense) \nde.ning pi ' from p1,...,pn. As before, this function can be compiled to an exe\u00adcutable version using \ncascades or trees of tests. 3.4.2 Simple Loop Example Toshowhowthe method operatesin practice,letus \nconsider .rsta very simpleexample(something happens is a nondeterministic choice): int i=0; while (i \n<= n) { if (something_happens) { i=i+1; if (i == n) { i=0; } } } Let us abstract i at the head of the \nloop using an interval [imin,imax]. For simplicity, we consider the case where the loop is at least entered \nonce, and thus i =0 belongs to the invariant. For better precision, we model each comparison x .y over \nthe = integers as x>= y+1.x<= y-1;similar transformations apply for other operators. The formula expressing \nthat such an interval is e3 = random(); assume(e3 >= e3min &#38;&#38; e3 <= e3max); an inductive invariant \nis: olds1 = s1; if (random) { ' imin = 0 . 0 = imax ..i.i ((imin = i . i = imax. s1 = e3; (((i +1 = \nn - 1 . i +1 = n + 1) . i ' = i + 1). } else { '' ' ' (i+1 = n+1.i = 0).i = i)) =. (imin = i .i = imax)) \nif (e1 -olds1 < -e2) { s1 = olds1 -e2; (7) } Quanti.er elimination produces: (imin = 0 . imax = 0 . imax \n<n .-imin + n - 2 < 0). (imin = 0 . imax = 0 . imax - n +1 = 0 . imax <n) (8) The formulas de.ning optimal \nimin and imax are: imin = 0 . imin = 0 . n> 0 (9) (imax =0 ..n> 0 . n< 2) . (imax = n - 1 . imax = 1) \n(10) We note that this invariant is only valid for n> 0, which is unsurprising given that we speci.cally \nlooked for invariants containing the precondition i =0. The output abstract transfer function is therefore: \nif (n<=0){ fail(); } else { iMin = 0; if (n < 2) { iMax = 0; } else /* n >= 2 */ iMax = n-1; } } The \ncase disjunction n<2 looks unnecessary,butisa sideeffect oftheuseof rational numberstomodelaproblemovertheintegers. \nThe resulting abstract transfer function is optimal, but on such a simple case, one could have obtained \nthe same using polyhedra (Cousot and Halbwachs 1978) or octagons (Min \u00b4 e 2001). Let us now consider \nthe same program, simply replacing n by the constant 20. All implementations of intervals (and thus of \noctagons and polyhedra, since we only have one variable), will overshoot theimax = 19 target when using \nthe traditional widening and narrowing strategies: they will compute i . [0, 0], then . [0, 1],. [0, \n2] and widen to [0, +8[,and narrowing will not reduce the interval. Even if we replaced i == 20 by i \n>= 20, narrowing would stillfailto reduce the interval due to the nondeterministic choice since the concrete \ntransfer function f , mapping sets of states at the head of the loop to sets of states at the next iteration, \nis expansive: for all set of states W , W . f(W ). This is a well-known weakness of the widening/narrowing \napproach, and the workaround is a syntactic trick known as widening up to or widening with thresholds: \nfor all variables, the constants to which it is compared aregathered and used as widening steps (Blanchet \net al. 2003, Sec. 7.1.2). This syntactic approach fails if tests are more indirect, whereas our semantic \napproach is not affected.  3.4.3 Synchronous Data Flow Example: Rate Limiter To go back to the original \nproblem of .oating-point data in data\u00ad.ow languages, let us consider the following library block: a rate \nlimiter. When compiled into C, such a block in inserted in a reactive loop, as shown below, where assume(c) \nstands for if (c) {} else {fail();}: if(e1-olds1> e2){ s1 = olds1 + e2; } } ... } We are interested \nin the input-output behavior of that block: obtain bounds on the output s1 of the system as functions \nof bounds on the inputs(e1, e2, e3). Note that in this case, s1, e1, e2, e3 are streams, not single scalars. \nOne dif.culty is that the s1 output is memorized, so as to be used as an input to the next computation \nstep.The semanticsofsuchablockis thereforeexpressedasa.xed point. We wish to know the least inductive \ninvariant of the form s1min = s1 = s1max under the assumption that e1min = e1max . e2min = e2max . e3min \n= e3max. The stability condition yields, after quanti.er elimination and projection on s1max the condition \ns1max = e1max . s1max = e3max. Minimization then yields an ex\u00adpression that can be compiled to an if-then-else \ntree: if (e1max > e3max) { s1max = e1max; } else { s1max = e3max; } This result, automatically obtained, \ncoincides with the intuition that a rate limiter (at least, one implemented with exact arithmetic) should \nnot change the range of the signal that it processes. This program fragment has a rather more complex \nbehavior if all vari\u00adables and operations are IEEE-754 .oating-point, since rounding errors introduce \nslight differences of regimes between ranges of in\u00adputs (Sec. 4.4, 6). Rounding errors in the program \nto be analyzed introduce dif.culties for analyzes using widenings, since invari\u00adant candidates are likely \nto be almost stable , but not truly sta\u00adble, becauseof these errors.Again, thereexistworkarounds so that \nwidening-based approaches can still operate (Blanchet et al. 2003, Sec. 7.1.4). 4. Extensions to the \nAdmissible Domains and Operations The class of domains and program constructs of the preceding section \nmay seem too limited.We shall see hereafewextensions. 4.1 In.nities Consider the interval abstract domain, \nde.ned by x = p2 .-x = p1. The techniques explained in Sec. 3.1 allow only .nite bounds. Yet, it makes \nsense thatp1 and p2 could be equal to +8 so asto represent in.nite intervals. This can be easily achieved \nby a minor alteration to our de.nitions. Each parameter pi is replaced by two 8 i8 i bi is constrained \nto be in {0, 1} (if the quanti.er elimination procedure in use allows Boolean variables, and pparameters \np . p then p 8 i can 8 i = 0 meansbe taken as a Boolean variable); p while (true) { bi , p 8 i 1 \nmeans pi =+8. ... 8 i ), Li <pi becomesbecomes (p = pi e1 = random(); assume(e1 >= e1min &#38;&#38; \ne1 <= e1max); 8 i bi ). After this rewriting, all formulas are formulas of the theory of linear inequalities \nwithout in.nities and are amenable to the appropriate algorithms.  4.2 Non-Convex Domains Section 3.1 \nconstrains formulas to be conjunctions of inequalities of the form Li = pi. What happens if we consider \nformulas that may contain disjunctions? The template linear constraint domains of section 3.1 have a \nvery important property: they are closed under (in.nite) intersec\u00adtion; thatis,ifwehaveafamily p . W \n, then there exist p0 such T that .F (p )= .F (p 0) (besides, p0 = inf{p | p . W }). ps.W This is what \nenables us to request the least element that contains the exact post-condition, or the least inductive \ninvariant in the do\u00admain: we take the intersection of all acceptable elements. Yet, if we allownon-convexdomains, \nthere does not necessarily exist a least element .F (p ) such that S . .F (p ). Consider for instance \nS = {0, 1, 2} and F representing unions of two intervals ((-x = p1 . x = p2) . (-x = p3 . x = p4)) . \np2 = p3. There are two, incomparable, minimal elements of the form .F (p ): p1 = p2 =0 . p3 = -1 . p4 \n=2 and p1 =0 . p2 =1 . p3 = -2 . p4 =2. We consider formulasF built out of linear inequalitiesLi(s1, \n...,sn) = pi as atoms, conjunctions, and disjunctions. By in\u00adduction on the structure of F , we can show \nthat .F :(R . {-8})n .P(Rn) is inf-continuous; that is, for anydescending chain (p i)i.I such that limi \np i = p 8, then .F (p i) is decreasing T and = .F (p 8). The property is trivial for atomic i.I .F (p \ni) formulas, andis conservedby greatestlower bounds(.)as well as binary least upper bounds(.). Let us \nconsider a set S .P(Rn), stable under arbitrary inter\u00adsection (or at least, greatest lower bounds of \ndescending chains). S can be for instance the set of invariants of a relation, or the set of over-approximations \nof a set W . .F -1(S) is the set of suitable domain parameters; for instance, it is the set of parameters \nrepre\u00adsenting inductive invariants of the shape speci.ed by F , or the set of representable over-approximations \nof W . .F -1(S) is stable un\u00adder greatest lower bounds of descending chains: take a descending chain \n(p i)i.I ,then.F (limi p i)= ni.F (p i) . S byinf-continuity and stability of S. By Zorn s lemma, .F \n-1(S) has at least one min\u00adimal element. Let P [p ] be a formula representing .F -1(S) (Sec. 3.1 proposes \nformulas de.ning safe post-conditions and inductive invariants). The formula G[p ]= P [p ] ..p ' P [p \n'] . p ' = p =. p = p ' de.nes the minimal elements of .-1(S). For instance, consider p =(a, b, c, d), \nF =(-x = a . x = b) . (-x = c . x = d), representing unions of two intervals [-a, b].[-c, d].Wewant upper-approximationsoftheset \n{0, 1, 3};that isP [p ]= .x (x =0.x =1.x =3 =p, x]). . F [pWeadd the constraint that-a = b .b =-c .-c \n= d,so as not to obtain the same solutions twice (by exchange of (a, b) and (c, d)) or solutions with \nempty intervals.Byquanti.er eliminationover G, we obtain (a =0 . b =1 . c = -3 . d = 3) . (a =0 . b = \n0 . c = -1 . d = 3), that is, either [0, 0] . [1, 3] or [0, 1] . [3, 3].  4.3 DomainPartitioning Non-convex \ndomains, in general, are not stable under intersections and thus best abstraction problems admit multiple \nsolutions as minimal elements of the set of correct abstractions. There are, how\u00adever,non-convexabstract \ndomains that are stable under intersection and thus admit least elements as well as the template linear \ncon\u00adstraint domains of Sec. 3.1: those de.ned by partitioning of the state space. Consider pairwise disjoint \nsubsets (Ci)i.I of the state space Qm, and abstract domains stable under intersection (Si)i.I , Si .P(Ci). \nElements of the partitioned abstract domain are S `S\u00b4 unions si where si . Si. If si,j ] j.J is afamily \nof ele\u00ad i.Ii T `S \u00b4ST ments of the domain, then = j.Ji.I si,j ] i.Ij.J si,j ; that is, intersections \nare taken separately in each Ci. Take afamily (Fi[p ])i.I of formulas de.ning template linear constraint \ndomains (conjunctions of linear inequalities Li(s1,..., sn) = pi)andafamily(Ci)i.I of formulas such that \nfor all i and i ' , Ci . Cil is equivalent to false and C1 .\u00b7 \u00b7\u00b7. Cl is equivalent to true. F =(C1 . \nF1) . \u00b7 \u00b7\u00b7 . (Cl . Fl) then de.nes an an abstract domain such that .F is a inf-morphism. All the techniques \nof Sec. 3.1 then apply. 4.4 Floating-Point Computations Real-life programs do not operate on real numbers; \nthey operate on .xed-point or .oating-point numbers. Floating point operations have few of the good algebraic \nproperties of real operations; yet, they constitute approximations of these real operations, and the \nrounding error introduced can be bounded. In IEEE .oating-point (IEE 1985), each atomic operation (not\u00ading \n., e, ., 0, v for operations so as to distinguishthem from the operations +, -f , \u00d7, /, v over the reals) \nis mathematically de\u00ad.ned as the image of the exact operation over the reals by a round\u00ading function.2 \nThis rounding function, depending on user choice, maps each real x to the nearest .oating-point value \nrn(x) (round to nearest mode, with some resolution mechanism for non repre\u00adsentable values exactly in \nthe middle of two .oating-point val\u00adues), r-8(x) the greatest .oating-point value less or equal to x \n(round toward-8), r+8(x) the least .oating-point value greater or equal to x (round toward +8), r0(x) \nthe .oating-point value of the same sign as x but whose magnitude is the greatest .oating\u00adpoint value \nless or equal to |x| (round toward0). If x is too large to be representable, r(x)= \u00b18 depending on the \nsize of x The semantics of the rounding operation cannot be exactly rep\u00adresentedinsidethetheoryoflinear \ninequalities.3Asaconsequence, we are forced to use an axiomatic over-approximation of that se\u00admantics: \na formula linking a real number x to its rounded ver\u00adsion r(x). Min\u00b4 e (2004) uses an inequality |r(x) \n- x|= erel \u00b7|x| + eabs, where erel isa relative error and eabs is an absolute error, leaving asidethe \nproblemofover.ows.The relativeerrorisdueto rounding at the last binary digit of the signi.cand, while \nthe absolute error isduetothefact thatthe rangeofexponentsis .niteand thus that thereexistsaleast positive.oating-point \nnumber and some nonzero values get rounded to zero instead of incurring a relative error. Because our \nlanguage for axioms is richer than the interval linear forms usedby Min\u00b4e, we canexpress more precise \nproperties of .oating-point rounding.We recall brie.y the characteristics of IEEE-754 .oating-point numbers. \nNonzero .oating point numbers are represented as follows: x = \u00b1s.m where 1 = m< 2 is the mantissa or \nsigni.cand, which has a .xed number p of bits, and s =2e the scaling factor (Emin = e = Emax is the exponent). \nThe difference introduced by changing the last binary digit of the mantissa is \u00b1s.elast where elast =2-(p-1): \nthe unit in the last place or ulp. Such a decomposition is unique for a given number if we impose that \nthe leftmost digit of the mantissa is 1 this is called a normalized representation. Except in the case \nof numbers of very small magnitude, IEEE-754 always works with normalized 2We leave aside the peculiarities \nof some implementations, such as those of mostCcompilersoverthe 32-bit Intel platform where there are \nextended precisions types used for some temporary variables and expressions can undergo double rounding. \n(Monniaux 2008b) 3To be pedantic, since IEEE .oating-point formats are of a .nite size, the rounding \noperation could be exactly represented by enumeration of all possible cases; this would anyway be impossible \nin practice due to the enormous size of such an enumeration. representations. There exists a least positive \nnormalized number mnormal and a least positive denormalized number mdenormal, and the denormals are the \nmultiples of mdenormal less than mnormal. All representable numbers are multiples of mdenormal. Consider \nfor instance .oating-point addition or subtraction x = \u00b1a \u00b1 b. Suppose that 0 = x = mnormal. a and b \nare multiples of mdenormal and thus a - b is exactly represented as a denormalized number; therefore \nr(x)= x. If x>mnormal, then |r(x) - x|= erel.x. The cases for x = 0 are symmetrical. We can therefore \ncharacterize r(x)-x using linear inequalities through case analysis over x:Round+(a . b, a + b) (respectively, \nRound+(a e b, a - b)) holds, where Round+(r, x)=(x = mnormal . r = x) . (x>mnormal .-erel.x = r - x = \nerel.x (11) Round(r, x)=(x =0 . r = 0). (x> 0 . r = 0 . Round+(r, x)). (x< 0 . r = 0 . Round+(-r, -x)) \n(12) To each .oating-point expressione, we associated a rounded\u00adoff variable re,thevalueof whichwe constrainusingRound(re,e) \nor Round+(re,e). For instance, a expression e = a . b is re\u00adplaced by a variable re, and the constraint \nRound+(re,a + b) is added to the semantics. In the case of a compound expression e = ab+c,we introducee1 \n= ab,and we obtainRound+(re,re1 + c) . Round(re1 , ab). If we know that the compiler uses a fused multiply-add \noperator, we can use Round(re, ab + c) instead. 5. Complex control .ow We have sofar assumed no procedure \ncall, and at most one single loop. We shall see here how to deal with arbitrary control .ow graphs and \ncall graph structures. 5.1 Loop Nests In Sec. 3.4, we have explained how to abstract a single .xed point. \nThe method can be applied to multiple nested .xed points by replacing the inner .xed pointby its abstraction.For \ninstance, assume the rate limiter of Sec. 3.4.3 is placed inside a larger loop. One may replace it by \nits abstraction: if (e1max > e3max) { s1max = e1max; } else { s1max = e3max; } assume(s1 <= s1max); /* \nand similar for s1min */ Alternatively,we canextend our frameworktoan arbitrary con\u00adtrol .ow graph with \nnested loops, the semantics of which is ex\u00adpressed asa single .xed point.We may use the same method as \nproposed by Gulwani et al. (2008, \u00a72) and other authors. First, a cut set of program locations is identi.ed; \nany cycle in the control .ow graph must go through at least one program point in the cut set. In widening-based \n.xed point approximations, one classically applies widening at each point in the cut set.Asimple method \nfor choosing a cut set is to include all targets of back edges in a depth\u00ad.rsttraversalofthe control-.owgraph, \nstartingfromthestartnode; in the case of structured program, this amounts to choosing the head node of \neach loop. This is not necessarily the best choice with re\u00adspect to precision, though (Gulwani et al. \n2008, \u00a72.3); Bourdoncle (1992, Sec. 3.6) discusses methods for choosing such as cut-set. To each point \nin the cut set we associate an element in the ab\u00adstract domain, parameterized by a number of variables. \nThe values of these variables for all points in the cut-set de.nes an invariant candidate. Since paths \nbetween elements of the cut sets cannot con\u00adtain a cycle, their denotational semantics can be expressed \nsimply by anexistentially quanti.ed formula. Possible paths between each source and destination elements \nin the cut-set de.ned a stability condition (Formula 4). The conjunction of all these stability con\u00ad \nditions de.nes acceptable inductive invariants. As above, the least inductive invariant is obtained by \nwriting a minimization formula (Sec. 3.4). Let us take a simple example: i=0; while(true) { /* A */ \n if (choice()) { j=0; while(j <i) {/*B*/ /* something */ j=j+1; } i=i+1; if (i==20) { i=0; } } else \n{ /* something */ } } We choose program points A and B as cut-set. At program point A, we look for an \ninvariant of the form IA(i, j)= imin,A = i = imax,A, and at program point B, for an invariant of the \nform IB(i, j)= imin,B = i = imax,B . jmin = j = jmax . dmin = i - j = dmax (a difference-bound invariant). \nThe (somewhat edited for brevity) stability formula is written: .jIA(0,j) ..i.j ((IB (i, j) . j = i . \n(i +1 = 19. i+1 = 20.i+1 = 21)) . If[i+1 = 20,IA(0,j),IA(i+1,j)]). .i.j (IA(i, j) . IB(i, 0)) ..i.j \n((IB (i, j) . j<i) . IB (i, j + 1)) (13) Replacing IA and IB into this formula, then applying quanti\u00ad.er \nelimination, we obtain a formula de.ning all acceptable tu\u00adples (imin,A,imax,A,imin,B ,imax,B,jmin,jmax,dmin,dmax). \nOp\u00adtimal values are then obtained by further quanti.er elimination: imin,A = imin,B = jmin =0,imax,A \n= imax,B = 19, jmax = 20, dmin =1, dmax = 19. The same example can be solved by replacing 20 by another \nvariable n as in Sec. 3.4.2. 5.2 Procedures and Recursive Procedures Wehave sofar considered abstractionsof \nprogram blocks with re\u00adspectto setsof program states.Aprogram blockis consideredasa transformer from \na state of input program states to the correspond\u00ading set of output program states. The analysis outputs \na sound and optimal(inacertainway) abstract transformer,mappingan abstract set of input states to an \nabstract set of output states. Assuming there are no recursiveprocedures, procedure calls can beeasilydealtwith.Wecansimply \ninlinetheprocedureatthepoint of call,as donein e.g.ASTR\u00b4 EE(Blanchet et al. 2002, 2003; Cousot et al. \n2005). Because inlining the concrete procedure may lead to code blowup, we may also inline its abstraction, \nconsidered as a nondeterministic program. Consider a complex procedure P with input variable x and output \nvariable x.We abstract the procedure automatically with respect to the interval domain for the postcon\u00addition(mz \n= z = Mz);suppose we obtain Mz := 1000; mz := x then we can replace the function call by z<= 1000&#38;&#38;z>= \nx. Thisisaformof modular interprocedural analysis:considering the call graph, we can abstract the leaf \nprocedures, then those calling the leaf procedures and so on. This method is however insuf.cient for \ndealing with recursive procedures. In order to analyze recursiveprocedures, we need to abstract not setsof \nstates,but setsof pairsof states,expressing the input-output relationships of procedures. In the case \nof recursive procedures, these relationships are the least solution of a system of equations. To takea \nconcreteexample, let us consider McCarthy sfamous 91 function (Manna and McCarthy 1969; Manna and Pnueli \n1970), which, non-obviously,returns 91 for all inputs less than 101: int M(int n) { if (n > 100) { return \nn-10; } else { return M(M(n+11)); } } The concrete semantics of that function is a relationship R between \nits input n and its output r. It is the least solution of R .{(n, r) . Z2 | (n> 100 . r = n - 10). (n \n= 100 ..n2 . Z(n + 11,n2) . R . (n2,r) . R)} (14) We look for a inductive invariant of the form I = ((n \n= A) . (r - n = d) . (r - n = .)) . ((n = B) . (r = C)),a non\u00adconvex domain (Sec. 4.2). By replacing \nR by I into inclusion 14, and by universal quanti.cation over n, r, n2, we obtain the set of admissible \nparameters for invariants of this shape. By quanti.er elimination, we obtain (C = 91) . (d =.= -10) . \n(A = 101) . (B = 100) within a fraction of a second using MJOLLNIR (see Sec. 6). Inthiscase,thereisasingle \nacceptableinductiveinvariantofthe suggested shape. In general, there may be parameters to optimize, as \nexplained in Sec. 3.4. The result of this analysis is therefore a map from parameters de.ning sets of \nstates to parameters de.ning sets of pairs of states (the abstraction of a transition relation). This \nabstract transition relation (a subset of X \u00d7 Y where X and Y are the input and output state sets) can \nbe transformed into an abstract transformer in X. . Y . as explained in Sec. 3.2. Such an interprocedural \nanalysis may also be used to enhance the analysis of loops (Martin et al. 1998). 6. Implementations and \nExperiments We have implemented the techniques of Sec.3in quanti.er elim\u00ad ination packages, including \nM ATHEMATICA4 and REDUCE 3.85 + REDLOG6 in addition to our own package, MJOLLNIR (Monniaux 2008a).7 As \ntest cases, we took a library of operators for synchronous programming, having streams of .oating-point \nvalues as input and outputs. These operators are written in a restricted subset of C and take as much \nas 20 lines.A front-end based onCIL (Necula et al. 2002) converts them into formulas, then these formulas \nare processed and the corresponding abstract transfer functions are 4http://www.wolfram.com/ 5http://www.uni-koeln.de/REDUCE/ \n6http://www.algebra.fim.uni-passau.de/~redlog/ 7Source code and GNU/Linux/IA32 binaries of this implementa\u00ad \ntion are available from http://www-verimag.imag.fr/~monniaux/ download/automatic abstraction.zip. pretty-printed. \nSince for our application, it is important to bound numerical quantities, we chose the interval domain. \nFor instance, the rate limiter presented in Sec. 3.4.3 was ex\u00ad tracted from that library. Since this \noperator includes a memory (a variable whose value is retained from a call to the operator to the next \none), its data-.ow semantics is expressed using a .xed\u00adpoint. When considered with real variables, the \nresulting expanded formula was approximately 1000 characters long, and with .oat\u00ading pointvariables approximately \n8000 characterslong. Despite the lengthof these formulas, theycanbe processedbyMJOLLNIR ina matter of \nseconds. The result can then be saved once and for all. Analyzers such asA STR\u00b4 EE (Blanchet et al. \n2002, 2003; Cousot et al. 2005) must have special knowledge about such operators, otherwise the analysis \nresults are too coarse (for instance, the in\u00adtervalsdonotget stabilizedatall).TheASTREEdevelopment team \n\u00b4 therefore had to provide specialized, hand-written analyzes. In con\u00adtrast, all linear .oating-point \noperators in the librarywere analyzed within a fraction of a second using the method in the present arti\u00adcle, \nassumingthat .oating-pointvaluesinthe sourcecode werereal numbers. If one considered instead the abstraction \nof .oating-point computations using real numbers from Sec. 4.4, computation times did not exceed 17 seconds \nper operator; the formulas produced are considerably more complex than in the real case. Note that this \ncomputationisdone onceandforallforeach operator;astatic ana\u00adlyzer can therefore cache this information \nfor further use and need not recompute abstractions for library functions or operators unless these functions \nare updated. Our analyzer front-end currently cannot deal with non-numerical operations and data structures \n(pointers, records, and arrays). It is therefore not yet capable of directly dealing with the real control\u00adcommand \nprograms that e.g.A STR\u00b4 EEaccepts, which do not consist purely of numerical operators. We plan to integrate \nour analysis method into a more generic analyzer. Alternatively, we plan to adapt a front-end for synchronous \nprogramming languages such asSIMULINK,a tool widely usedby control/command engineers. The correctness \nof the methods described in this article does not rely on anyparticularity of the quanti.er elimination \nprocedure used, provided one also has symbolic computation procedures for e.g. putting formulas in disjunctive \nnormal form and simplifying them. The difference between thevarious quanti.er elimination and simpli.cation \nprocedures is ef.ciency; experiments showed that ours was vastly more ef.cient than the others tested \nfor this kind of application.For instance, our implementationwas ableto complete the analysis of the \nrate limiter of Sec. 3.4.3, implemented over the reals,in1.4s,andin17swiththe sameexampleover .oating-point \nnumbers, while REDLOG took 182 s for the former and could not .nish the latter, andMATHEMATICAcould analyze \nneither (out-of\u00admemory). On other examples, our quanti.er elimination procedure isfaster than the other \nones, or can complete eliminations that the others cannot (Monniaux 2008a). 7. RelatedWorks Thereisa \nsizeable amountofliterature concerning relational nu\u00admerical abstract domains; that is, domains that \nexpress constraints between numerical variables. Convex polyhedra were proposed in the 1970s (Cousot \nand Halbwachs 1978; Halbwachs 1979), and there have been since then manyimprovements to the technique; \na bibliography wasgatheredby Bagnaraetal. (2006). Algorithms on polyhedra are costly and thus a variety \nof domains intermediate between simple interval analysis and convex polyhedra were pro\u00adposed (Claris\u00b4e2001; \nSankaranarayanan oand Cortadella 2004; Min\u00b4etal.2005).Allthese domainscomputeinvariantsusinga widening \noperator (Cousot and Cousot 1992, 1976; Cousot and Halbwachs 1978). There is, however, no guarantee that \nthe resulting invariant is the best representable in the abstract domain, even with the use of narrowing \niterations; this is one difference with our proposal, which computes the best representable inductive \ninvariant. Another difference is that these domains are designed to work with numericalvalues for theinput \nconstraints, thus the computa\u00adtion must be done for every value of the input constraints param\u00adeters. \nUsing simple program transformations, theymay also apply to symbolic input constraints (constraint parameters \nbeing taken as extravariables),butin general this will leadtobad results;forin\u00adstance, the input-output \nrelationship for the rate limiter of Sec. 3.4.3 is not convex, while numerical abstract domains in the \nliterature are convex. In comparison the algorithm in this article can be run once to obtain a formula \nthat gives the best invariant depending on the input constraints, allowing modular analysis. Several \nmethods have been proposed to synthesize invariants without using widening operators (Colon et al. 2003; \nCousot 2005; Sankaranarayananetal. 2004).In commonwithus,theyexpressas constraints the conditions under \nwhich some parametric invariant shape truly is an invariant, then theyuse some resolution or simpli\u00ad.cation \ntechniqueover those constraints.Again, these methods are designed for solving the problem for one given \nset of constraints on the inputs, as opposed to .nding a relation between the output or .xed-point constraints \nand the input constraints. In some cases, the invariant may also not be minimal. Bagnaraetal. (2005a,b) \nproposedimprovementsoverthe clas\u00ad sical widenings on linear constraint domains (Halbwachs 1979). Gopan \nand Reps (2006) introduced lookahead widenings : stan\u00ad dard widening-based analysis is applied to a sequence \nof syntactic restrictions of the original program, which ultimately converges to the whole programs; \nthe idea is to distinguish phases or modes of operation in order to makethe widening more precise. Gonnord \nand Halbwachs (2006) haveproposed acceleration techniques for linear constraints. Thesedo not replace \nwidenings altogether,but theyal\u00adleviate the need for some of the costly workarounds to the impreci\u00adsion \nintroducedby widenings, such as delayed widening (Blanchet et al. 2003, Sec. 7.1.3). These address a \ndifferent problem from ours. On the one hand, neither improved widenings nor accelera\u00adtion guarantee \nthat the inductive invariant obtained at the end is the least one (indeed, theycan yield the top element \nT).8 Furthermore, the invariant that these methods obtain is not parametric in the pre\u00adcondition, contrary \nto the one that our method obtains. On the other hand, improved widenings work regardless of the form \nof the tran\u00adsition relation, which our method constrains to be piecewise linear. Some of the cited methods \noperate on general polyhedra, while our method constrains the shape of the polyhedra that are found to \na certain template. Gaubert et al. (2007); Gawlitza and Seidl (2007) proposed re\u00ad placing the usual widening/narrowing \niteration techniquesbya pol\u00adicy iteration (or strategy iteration)approach. Their approach con\u00adverges \nona .xed point,but not necessarily the least one. Their idea is to replace computing the least .xed point \nof a complex abstract operator(the point-wise minimumofafamilyof simpler operators) byasequenceofleast.xedpoint \ncomputationsforthesesimpleop\u00aderators. Their technique anyway needs to compute these latter least .xed \npoints, and it is possible that our method can help in that re\u00adspect. Techniques using quanti.er elimination \nfor generating nonlin\u00adear invariants for programs using nonlinear arithmetic have also been proposed \n(Kapur 2004) and shown capable of producing optimal invariants parameterized by input constraints (Monniaux \n2007). Quanti.er elimination in the theory of real closed .elds is, however, a very costly technique. \nExperimentally, the formulas generated by common implementations tend to grow huge (due 8Thereexist exact \naccelerationtechniquesbut these rather applyto discrete automata. to dif.cult simpli.cations) and both \ntime and space requirements growveryfastwiththe numberofvariables.Thisiswhy we con\u00adsidered the linear \ncase in the present article. Gulwani et al. (2008) have also proposed a method for gener\u00ad ating linear \ninvariants over integer variables, using a class of tem\u00adplates. The methods described in the present \narticle can be applied to linearinvariantsoverintegervariablesintwoways: eitherbyab\u00adstracting them using \nrationals (as in examples in Sec. 3.4.2, 5.1), either by replacing quanti.er elimination over rational \nlinear arith\u00admetic by quanti.er elimination over linear integer arithmetic, also known as Presburger \narithmetic. Quanti.er elimination over Pres\u00adburger arithmetic is however very expensive (Fischer and \nRabin 1974). Gulwani et al.instead chose to .rst consider integervari\u00ad ables as rationals, so as to be \nable to compute over rational con\u00advex polyhedra, then bound variables and constraint parameters so as \nto model them as .nite bit vectors, .nally obtaining a problem amenabletoSATsolving. Programvariables \nare .nite bitvectorsin most industrial programming languages, and parameters to useful invariantsover \nintegervariables are often small, thus their approach seems justi.ed.Wedo not see,however,howtheir method \ncouldbe applied to programs operating over real or .oating-point variables, which are the main motivation \nfor the present article. The idea of producing procedure summaries (Sharir and Pnueli 1981) as formulas \nmapping input bounds to output bounds is not new. Rugina and Rinard (2005), in the context of pointer \nanaly\u00ad sis (with pointers considered as a base plus an integer offset), pro\u00adposedareductionto linear \nprogramming. This reduction step, while sound, introduces an imprecision that is dif.cult to measure \nin ad\u00advance; our method, in contrast, is guaranteed to be optimal in a certain sense. Rugina and Rinard \ns method, however, allows some nonlinear constructs in the program to be analyzed. Martin et al. (1998) \nproposed applying interprocedural analysis to loops. Seidl et al. (2007) also produce procedure summaries \nas nu\u00admerical constraints. Our procedure summaries are implementations of the corresponding abstract \ntransformer over some abstract do\u00admain, while theirs outputs a relationship between input and out\u00adput \nconcrete values. Their analysis considers a convex set of con\u00adcrete input-output relationships, expressed \nas a simplices, a re\u00adstricted class of convex polyhedra. This restriction trades precision for speed: \nthe generator and constraint representations of simplices haveapproximatelythesamesize,whileingeneralpolyhedraexpo\u00adnential \nblowup can occur.Testsby arbitrary linear constraints can\u00adnot be adequately represented within this framework. \nSeidl et al. (2007, Sec. 4) propose deferring those constraints using auxiliary variables; this, however, \nloses some precision. Their analysis and ours are therefore incomparable, since theymake different choices \nbetween precision and ef.ciency. Lal et al. (2005) proposed an interprocedural analysis of numer\u00ad ical \nproperties of functions using weighted pushdown automata. The weights are taken in a .nite height abstract \ndomain, while the domains we consider have in.nite height. In earlierworkswehaveproposedamethodfor obtaining \ninput\u00adoutput relationships of digital linear .lters with memories, taking into account the effects of \n.oating-point computations (Monni\u00ad aux 2005). This method computes an exact relationship between bounds \non the input and bounds on the output, without the need for an abstract domain for expressing the local \ninvariant; as such, for this classof problems,itis more precisethanthe method from this article. This \ntechnique, however, cannot be easily generalized to cases where the operator block contains tests. 8. \nConclusion and FutureWork Writing static analyzers by hand has long been found tedious and error-prone. \nOne may of course prove an existing analyzer correct through assisted proof techniques, which removes \nthe possibility of soundness mistakes, at the expense of much increased tediousness. In this article, \nwe proposed instead effective methods to synthe\u00adsize abstract domainsby automatic techniques. The advantages \nare twofold: new domains can be created much more easily, since no programming is involved; a single \nprocedure, testable on indepen\u00addent examples, needs be written and possibly formally proved cor\u00adrect.To \nour knowledge, this is the .rst effective proposal for gen\u00aderating numerical abstract domains automatically, \nand one of the few methods for generating numerical summaries. Also, it is also the only method sofar \nfor computing summaries of .oating-point functions. Wehaveshown that .oating-point computations could \nbe safely abstracted using our method. The formulas produced are how\u00adeverfairlycomplexin this case, and \nwe suspect that furtherover\u00adapproximation could dramatically reduce their size. There is also nowadays \nsigni.cant interest in automatizing, at least partially, the tedious proofs that computer arithmetic \nexperts do and we think that the kind of methods described in this article could help in that respect. \nWehave sofarexperimented with smallexamples, because the original goal of this work was the automatic, \non-the-.y, synthesis of abstract transferfunctions for small sequences of code that could be more precise \nthan the usual composition of abstract of individ\u00adual instructions, and less tedious for the analysis \ndesigner than the method of pattern-matching the code for known operators with known mathematical properties.Afurther \ngoalis the precise anal\u00adysis of longer sequences, including integer and Boolean computa\u00adtions.WehaveshowninSec.4.3howitwas \npossibletopartitionthe state space and abstract each region of the state-space separately; but naive \npartitioning according ton Booleans leads to 2n regions, which canbe unbearably costly andis unneededin \nmost cases.We think that automatic re.nement and partitioning techniques (Jean\u00ad net 2003) could be developed \nin that respect. References Roberto Bagnara, Patricia M. Hill, Elena Mazzi, and Enea Zaffanella. Widening \noperators for weakly-relational numeric abstractions. InStatic Analysis (SAS), volume 3672 of LNCS, pages \n3 18. Springer, 2005a. DOI: 10.1007/11547662 3. Roberto Bagnara,PatriciaM.Hill,ElisaRicci,andEneaZaffanella. \nPrecise widening operators for convex polyhedra. Sci. Comput. Program., 58(1\u00ad2):28 56, 2005b. DOI: 10.1016/j.scico.2005.02.003. \nRoberto Bagnara,PatriciaM. Hill,and EneaZaffanella. TheParmaPolyhe\u00addra Library,version 0.9, 2006. URL \nhttp://www.cs.unipr.it/ppl. Gogul Balakrishnan and Thomas Reps. Analyzing memory accesses in x86 executables. \nIn Compiler Construction (CC), volume 2985 of LNCS, pages 5 23. Springer, 2004. DOI: 10.1007/b95956. \nBruno Blanchet, Patrick Cousot, Radhia Cousot, J\u00b4er ome Feret, Laurent Mauborgne, Antoine Min \u00b4De\u00ad e, \nDavid Monniaux, and Xavier Rival. sign and implementation of a special-purpose static program analyzer \nfor safety-critical real-time embedded software. In The Essence of Com\u00adputation: Complexity, Analysis,Transformation, \nnumber 2566 in LNCS, pages 85 108. Springer, 2002. DOI: 10.1007/3-540-36377-7 5. Bruno Blanchet, Patrick \nCousot, Radhia Cousot, J\u00b4er ome Feret, Laurent Mauborgne, Antoine Min \u00b4 e,David Monniaux, andXavierRival.Astatic \nanalyzer for large safety-critical software. In Programming Language Design and Implementation (PLDI), \npages 196 207.ACM, 2003. DOI: 10.1145/781131.781153. Franc\u00b8ois Bourdoncle. S\u00b4emantique des langages imp\u00b4eratifs \nd ordre sup\u00b4etation abstraite. Ecole polytechnique, \u00b4 Palaiseau, 1992. erieur et interpr \u00b4PhD thesis, \nAaron R. Bradley and Zohar Manna. The Calculus of Computation: De\u00adcision Procedures with Applications \ntoVeri.cation. Springer, October 2007. Paul Caspi, Daniel Pilaud, Nicolas Halbwachs, and John A. Plaice. \nLUS-TRE: a declarative language for real-time programming. In Principles of Programming Languages (POPL), \npages 178 188.ACM, 1987. DOI: 10.1145/41625.41641. Paul Caspi, Adrian Curic, Aude Maignan, Christos Sofronis, \nStavrosTri\u00adpakis, and Peter Niebert. From Simulink to Scade/Lustre to TTA: a lay\u00adered approach for distributed \nembedded applications. SIGPLAN notices, 38(7):153 162, 2003. DOI: 10.1145/780731.780754. CAV05. Computer \nAidedVeri.cation(CAV), number 4590 in LNCS, 2005. Springer. DOI: 10.1007/b138445. Robert Claris \u00b4 o and \nJordi Cortadella. The octahedron abstract domain. In Static Analysis (SAS), number 3148 in LNCS, pages \n312 327. Springer, 2004. Michael Colon, Sriram Sankaranarayanan, and HennySipma. Linear invari\u00adant generation \nusing non-linear constraint solving. In Computer Aided Veri.cation (CAV), number 2725 in LNCS, pages \n420 433. Springer, 2003. Patrick Cousot. Proving program invariance and termination by parametric abstraction, \nlagrangian relaxation and semide.nite programming. In VMCAI05, pages 1 24. DOI: 10.1007/b105073. Patrick \nCousot and Radhia Cousot. Abstract interpretation and application to logic programs. J. of Logic Programming, \n13(2 3):103 179, 1992. Patrick Cousot and Radhia Cousot. Static determination of dynamic proper\u00ad ties \nof programs. In Proceedings of the Second International Symposium on Programming, pages 106 130. Dunod,Paris, \nFrance, 1976. Patrick Cousot and Nicolas Halbwachs. Automatic discovery of lin\u00ad ear restraints among \nvariables of a program. In Principles of Pro\u00adgramming Languages (POPL), pages 84 96. ACM, 1978. DOI: \n10.1145/512760.512770. Patrick Cousot, Radhia Cousot,J\u00b4er ome Feret, Laurent Mauborgne, Antoine Min\u00b4e, \nDavid Monniaux, and Xavier Rival. The ASTREE analyzer. In \u00b4Programming Languages and Systems (ESOP), \nnumber 3444 in LNCS, pages 21 30, 2005. ESOP07. Programming Languages and Systems (ESOP), volume 4421 \nof LNCS, 2007. Springer. DOI: 10.1007/978-3-540-71316-6. Jeanne Ferrante and Charles Rackoff. A decision \nprocedure for the .rst order theory of real addition with order. SIAMJournal of Computation, 4(1):69 \n76, March 1975. Michael J. Fischer and Michael O. Rabin. Super-exponential complexity of Presburger arithmetic. \nIn Complexity of Computation, number7 in SIAM AMS proceedings, pages 27 42. American Mathematical Soci\u00adety, \n1974. \u00b4analysis by policy iteration on relational domains. In ESOP07, pages 237 252. DOI: 10.1007/978-3-540-71316-6. \nThomas Gawlitza and Helmut Seidl. Precise .xpoint computation through strategy iteration. In ESOP07, \npages 300 315. DOI: 10.1007/978-3\u00ad 540-71316-6 21. St\u00b4ephane Gaubert, Eric Goubault, AnkurTaly, and Sarah \nZennou. Static Laure Gonnord and Nicolas Halbwachs. Combining widening and acceler\u00ad ation in linear relation \nanalysis. In Static Analysis (SAS), volume 4134 of LNCS, pages 144 160. Springer, 2006. DOI: 10.1007/11823230 \n10. Denis Gopan and Thomas W. Reps. Lookahead widening. In Com\u00adputer AidedVeri.cation (CAV), volume 4144 \nof LNCS, pages 452 466. Springer, 2006. DOI: 10.1007/11817963 41. Denis Gopan and ThomasW. Reps. Low-level \nlibrary analysis and summa\u00ad rization. In Computer AidedVeri.cation (CAV), volume 4590 of LNCS, pages \n68 81. Springer, 2007. DOI: 10.1007/978-3-540-73368-3 10. Sumit Gulwani, Saurabh Srivastava, and Ramarathnam \nVenkatesan. Program analysis as constraint solving. In Programming Lan\u00adguage Design and Implementation \n(PLDI). ACM, 2008. DOI: 10.1145/1375581.1375616. Nicolas Halbwachs. D\u00b4etermination automatique de relations \nlin\u00b4eaires v\u00b4eri.\u00b4ees par les variables d un programme. PhD thesis, Universite\u00b4scienti.queetm\u00b4edicalede \nGrenoble, 1979. IEEE standardfor Binary .oating-point arithmetic for microprocessor sys\u00adtems. IEEE, 1985. \nANSI/IEEE Std 754-1985. Bertrand Jeannet. Dynamic partitioning in linear relation analysis. applica\u00adtiontotheveri.cationof \nreactive systems. Formal Methods in System Design, 23(1):5 37, July 2003. Deepak Kapur. Automatically \ngenerating loop invariants using quanti.er elimination. In ACA (Applications of Computer Algebra), 2004. \nAkash Lal, Gogul Balakrishnan, and Thomas Reps. Extended weighted pushdown systems. In CAV05, pages 343 \n357. DOI: 10.1007/11817963 32. Francesco Logozzoand ManuelF\u00a8ahndrich.Onthe relativecompletenessof bytecode \nanalysisversus source code analysis.In Compiler Construction (CC), volume 4959 of LNCS, pages 197 212. \nSpringer, 2008. DOI: 10.1007/978-3-540-78791-4 14. Zohar Manna and John McCarthy. Properties of programs \nand partial function logic. In Machine Intelligence, 5, pages 27 38. Edinburgh University Press, 1969. \nZohar Manna and Amir Pnueli. Formalization of properties of functional programs. J. ACM, 17(3):555 569, \n1970. DOI: 10.1145/321592.321606. Florian Martin, Martin Alt, Reinhard Wilhelm, and Christian Ferdinand. \nAnalysis of loops. In Compiler Construction (CC), volume 1383 of LNCS, pages 80 94. Springer, 1998. Antoine \nMin\u00b4e. The octagon abstract domain. In Reverse Engineering (WCRE), pages 310 319. IEEE, 2001. DOI: 10.1109/WCRE.2001.957836. \nAntoine Min\u00b4e. Relational abstract domains for the detection of .oating\u00ad point run-time errors. In Programming \nLanguages and Systems (ESOP), volume 2986 of LNCS, pages 3 17. Springer, 2004. David Monniaux. Compositional \nanalysis of .oating-point linear numerical .lters. In CAV05, pages 199 212. DOI: 10.1007/b138445. David \nMonniaux. Aquanti.er elimination algorithm for linear real arith\u00ad metic. In LPAR (Logic for Programming, \nArti.cial Intelligence, and Reasoning), LNCS. Springer, 2008a. David Monniaux. Optimal abstraction on \nreal-valued programs. In Static analysis (SAS), number 4634 in LNCS, pages 104 120. Springer, 2007. DOI: \n10.1007/978-3-540-74061-2 7. David Monniaux. The pitfalls of verifying .oating-point computations. ACM \nTransactions on programming languages and systems, 30(3):12, 2008b. DOI: 10.1145/1353445.1353446. George \nC. Necula, Scott McPeak, Shree P. Rahul, and Westley Weimer. CIL: Intermediate language and tools for \nanalysis and transformation of C programs. In Compiler Construction (CC), volume 2304 of LNCS, pages \n209 265. Springer, 2002. DOI: 10.1007/3-540-45937-5 16. Radu Rugina and Martin Rinard. Symbolic bounds \nanalysis for pointers, array indices, and accessed memory regions. ACMTrans. on Program\u00adming Languages \nand Systems (TOPLAS), 27(2):185 235, 2005. DOI: 10.1145/349299.349325. Sriram Sankaranarayanan, Henny \nSipma, and Zohar Manna. Constraint\u00ad based linear-relations analysis. In SAS, number 3148 in LNCS, pages \n53 68. Springer, 2004. Sriram Sankaranarayanan, HennySipma, and Zohar Manna. Scalable anal\u00ad ysis of linear \nsystems using mathematical programming. In VMCAI05, pages 21 47. DOI: 10.1007/b105073. Helmut Seidl, \nAndrea Flexeder, and Michael Petter. Interprocedurally analysing linear inequality relations. In ESOP07, \npages 284 299. DOI: 10.1007/978-3-540-71316-6 20. Micha Sharir and Amir Pnueli. Two approaches to inter-procedural \ndata\u00ad.ow analysis. In Program Flow Analysis: Theory and Application. Prentice-Hall, 1981. VMCAI05. Veri.cation, \nModel Checking and Abstract Interpretation (VM-CAI), number 3385 in LNCS, 2005. Springer. DOI: 10.1007/b105073. \n  \n\t\t\t", "proc_id": "1480881", "abstract": "<p>We propose a method for automatically generating abstract transformers for static analysis by abstract interpretation. The method focuses on linear constraints on programs operating on rational, real or floating-point variables and containing linear assignments and tests.</p> <p>In addition to loop-free code, the same method also applies for obtaining least fixed points as functions of the precondition, which permits the analysis of loops and recursive functions. Our algorithms are based on new quantifier elimination and symbolic manipulation techniques.</p> <p>Given the specification of an abstract domain, and a program block, our method automatically outputs an implementation of the corresponding abstract transformer. It is thus a form of program transformation.</p> <p>The motivation of our work is data-flow synchronous programming languages, used for building control-command embedded systems, but it also applies to imperative and functional programming.</p>", "authors": [{"name": "David P. Monniaux", "author_profile_id": "81100428259", "affiliation": "CNRS, Gi&#232;res, France", "person_id": "P1300950", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1480881.1480899", "year": "2009", "article_id": "1480899", "conference": "POPL", "title": "Automatic modular abstractions for linear constraints", "url": "http://dl.acm.org/citation.cfm?id=1480899"}