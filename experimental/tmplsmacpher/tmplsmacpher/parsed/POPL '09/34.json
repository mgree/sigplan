{"article_publication_date": "01-21-2009", "fulltext": "\n RelaxedMemoryModels: anOperationalApproach * G\u00b4erardBoudol GustavoPetri INRIASophiaAntipolis {Gerard.Boudol,Gustavo.Petri}@inria.fr \nAbstract Memory models de.ne an interface between programs written in some language and their implementation, \ndetermining which be\u00adhaviour the memory (and thus a program) is allowed to have in a given model. A minimal \nguarantee memory models should pro\u00advide to theprogrammeristhat well-synchronized, thatis,data-race free \ncode has a standard semantics. Traditionally, memory mod\u00adels are de.ned axiomatically, setting constraints \non the order in which memory operations are allowed to occur, and the program\u00adming language semantics \nis implicit as determining some of these constraints.Inthis work wepropose a new approach toformalizing \na memory model in which the model itself is part of a weak op\u00aderational semanticsfor a(possibly concurrent) \nprogramminglan\u00adguage. We formalize in this way a model that allows write opera\u00adtions to the store to \nbe buffered. This enables us to derive the or\u00addering constraints from the weak semantics of programs, \nand to prove, at theprogramminglanguagelevel, that the weaksemantics implements the usualinterleaving \nsemanticsfordata-racefreepro\u00adgrams, hence in particular that it implements the usual semantics for sequential \ncode. Categories and Subject Descriptors F.3.2[Logics and Meanings ofPrograms]:SemanticsofProgrammingLanguages; \nD.3.4[Pro\u00adgramming Languages]:Processors-Optimization General Terms Languages,Theory 1. Introduction \nOptimizingtheperformance ofcomputing systemshas alwaysbeen a concern for hardware and software technology, \nand therefore, given that the latency of memory operations is one of the key performance factors, it \nis not surprising that techniques have been developed very early to minimize the cost of these operations \nin program execution. As a typical optimization technique, shared memory multiprocessors use write buffers \nand caches [15], thus bene.ting from a parallel architecture where memory operations run concurrently \nwith otherinstructions.These techniques, as well as various optimizations performed by compilers, are \nnaturally intended topreserve the semantics of sequential code. However,problems arise when executing \nconcurrent code shar\u00ading the memory. In particular, many algorithms designed for syn\u00ad * Workpartially \nsupportedby theANR-SETI-06-010grant. Permission to make digital or hard copies of all or part of this \nwork for personal or classroomuseisgranted withoutfeeprovided that copiesarenot madeordistributed forpro.tor \ncommercial advantage andthat copiesbearthis notice andthefull citation onthe .rstpage.Tocopy otherwise,torepublish,topostonserversortoredistribute \ntolists, requiresprior speci.cpermission and/or afee. POPL 09, January18 24,2009,Savannah,Georgia,USA. \nCopyright c . 2009ACM978-1-60558-379-2/09/01. . .$5.00 chronizationpurposes(mutual exclusion,producer-consumer) \nfail to achieve their goal in an optimized implementation setting. A classical example (see for instance \n[3]) is with Dekker s mutual exclusion algorithm: .ag0 := false . .ag1 := false if .ag1 then if .ag0 \nthen critical section 0 critical section 1 that mayfail thatis,thereexistsanexecutionwhereboth threads \ncan enter their critical section , for various reasons. For instance, thehardwaremight allowtoreorderindependent \nstatements,mak\u00adingthe assignments .ag0 := false and .ag1 := false tohappen af\u00adter,orinparallelwiththe \nsubsequent conditionalbranchings.These assignments could also be delayed in some communication struc\u00adture(writebuffer \nor cache)before they actually affect the memory, there again opening the possibility for the conditional \nbranchings to read a wrong valuefromthe memory.Another standard example (againgivenin[3])isthe one of \naproducer-consumer algorithm. Namely, withtheprogram data := 1 . whilenot .agdoskip .ag := true r := \ndata one could get a wrong value for r if the assignment data := 1 is delayed with respect to .ag := \ntrue, or if r := data is specula\u00adtivelydone before while not .ag do skip.To compensate for these failures, \nthe hardware usually offers various synchronization fea\u00adtures: memorybarriers(fence), read-modify-write \natomicinstruc\u00adtions(test-and-set, compare-and-swap). The optimizations implemented in commercial hardware \nor software work .ne for sequential programs, but the failure of the standard interleaving semantics \nprompted researchers to under\u00adstand which semantics for shared variable concurrency is actually supported \nby these optimized hardware architectures and compil\u00aders. Then the notion of a weak, or relaxed memory \nmodel was in\u00adtroduced[15], to serve as an abstractinterface[4](or a contract, as wewill see)betweentheprogrammer \nand theimplementation.(In the following we shall use weak and relaxed as synonymous.) Amemory modelisintended \nto specify which values a read,in the execution of a program, can return, and therefore it de.nes what \nthepossible outcomes of a concurrent system are.There are usually more(possibly unwanted) outcomesfor \nsuch a system than with thestandardinterleaving semantics which,asamemory model,is knownasSequentialConsistency(SC)[27] \n,and memory mod\u00adels can be compared from this point of view. Numerous relaxed (w.r.t. SC)memory models \nhave been introduced: Weak Ordering (WO)[4, 15], Processor Consistency (PC)[21], Release Consis\u00adtency(RCsc \nandRCpc)[19],LocationConsistency(LC)[17], and many others(see[2] and[34]forasurvey).Thesemodels arefor\u00admulated,quite \nabstractly, as constraints on the ordering of memory operations. The problem of understanding the semantics \nsupported by shared memory architectures recently regained a lot of interest, promptedbythefact that \nsomeprogramminglanguages, e.g. JAVA, offer a multithreadingfacilityatthe applicationprogramminglevel. \nDe.ning an abstract memory model for a high-level programming languageis not an easy task, and,despite \nconsiderable effortshave been made to propose a revision of the original JAVA Memory Model(JMM, see[29]),thisis \nstill a controversial matter[6,14]. However, a consensus has emerged as regards the r ole of memory models: \na memory modelis nowgenerally conceived as a contract between the (application) programmer and the implementor \n[4]. The latter should use it as delineating the allowed optimizations, while the former is ensured that, \nprovided he/she writes prop\u00aderly synchronized programs,he/she willgetasemantics whichis equivalent to \nthe usual interleaving semantics. Well-synchronized programs 1 are also data-races free (DRF), and therefore \nthe pro\u00adgrammer s side of the contract memory models should provide is also known as the DRF guarantee. \nThis DRF guarantee is a good property to have, because the interleaving semantics pro\u00advides a reasonablebasisfor \ntheformal understanding, analysis and veri.cation of concurrentprograms(and thisis also aportability \nguarantee). Memory models are usually de.ned by means of various par\u00adtial orders relating actions, or \nmore precisely events in an execu\u00adtion.Thisisthe way theJMM[29] is(re)de.nedforinstance.A proof of theDRFguaranteeforthis \nmodel wasgivenin[29], and laterformalized usingproof assistants(Isabelle[5] and COQ [25]), which revealed \nsome subtleproblems.Some otherproofs of a sim\u00adilar result werepreviouslygivenin[4,19,20] usingdifferentfor\u00admalisms,butit \nseemsfair to say that all theseproofs,including the mostformal ones, only establish a very abstract version \noftheDRF guarantee,from whichthe notion of aprogram,in the sense ofpro\u00adgramming languages, is actually \nabsent. Our aim here is to give a proofoftheDRFguarantee,thatis, aproof ofthefactthat a relaxed memory \nmodelimplementstheinterleaving semanticsfordata-race freeprograms, at theprogramming language level.To \nthis end, we introduce a new way ofde.ning a memory model. In the approach we propose, the memory model \nis de.ned as partofa weak operational semantics fortheprogramminglanguage (as opposed to the strong, \ni.e. interleaving semantics). Our weak operational semantics is quite concrete: to formalize the memory \naccess reorderings supported by relaxed memory models, and to get a model similar to an architecture \ninvolving write buffers, we introduce a construct .B.T to describe (part of) a con.guration involvinga \nwritebuffer B, which,for some references(orpointers) p1,...,pk,separately recordssequences of valuestobewrittenin \nthe memory.Thatis, B is a mapping 11 kk B = {p1 .. v1 \u00b7\u00b7\u00b7 vn1 ,...,pk .. v1 \u00b7\u00b7\u00b7 vnk } where v1 i isthe \n.rst-invalueforpi, and vni i isthelast-in value.The syntaxfor thread systems with writebuffersisgivenby \nT ::= e |.B.T | (T . T ' ) where e is any program of the language. We should think of these as(downwards) \ntrees,with unary nodeslabeledby buffers,binary nodesputting togetherparallel components, and where eachleafis \nathread,thatis aprogram.The shared memory(or store) should be seen as sitting over the root of such trees. \nThe buffer B in .B.T collects the write operations issued from the thread system T, and is shared among \nall the threads in T. Then in our weak operational semantics a writeinstruction (p := v)does notdirectly \naffect the memory, but puts the value v for p in a buffer on the path to the store.Forinstance wehave, \nusing ML s notation !pfor 1We deviate here from the standard terminology, where well-synchro\u00adnized or \nproperlylabeled [19] isoftentakenassynonymousto data\u00adracefree. dereferencing apointer: x := 1;r0 := \n!y . y := 1;r1 := !x * ..x .. 1.r0 := !y ..y .. 1.r1 := !x Then the writes are propagated from the buffers \nto the store, in an asynchronous way(theprecise semantical rules aregiveninSection 4). For each pointer, \nthe propagation of values follows the FIFO order determined by the sequencing of thread instructions, \nbut is otherwiseperformedin aninterleaved manner, andis asynchronous with respect to the execution of \nthreads. That is, the propagation of writes is parallel to the execution of threads. A read operation \n!p returns the most recent value for p, as viewed from the thread issuing this operation, that is, the \nlast-in value for p in the closest buffer on the path from the thread reading p to the store, if any, \nor the value stored for p in the shared memory otherwise. Then, assuming that the memory initially contains \n{x .. 0,y .. 0}, a possible execution, where the buffered writes are delayed with respect to the execution \nof the threads,is thefollowing: x := 1;r0 := !y . y := 1;r1 := !x * ..x .. 1.r0 := 0 ..y .. 1.r1 := 0 \n That is, we get the outcome {r0 .. 0,r1 .. 0}, which we cannotgetby the usualinterleaving semantics.(Thisis \nsimilarto what happens with the mutual exclusion algorithm example given above.) In our model, there \nare queues of values to write in the memoryfor each reference, and notjust sequences of writes.Then the \nwrites on distinct references issued by a given thread may be reorderedinthepropagationprocess(thisiscalled \njockeying in [15]).Forinstance, consider thefollowingsystem of threads: r0 := !y; . x := 1; r1 := !xy \n:= 1 Then, starting with {x .. 0,y .. 0}, a possible outcome is {r0 .. 1,r1 .. 0}, since, representing \nterminationby (), wehave: r0 := !y;r1 := !x . x := 1;y := 1 * . r0 := !y;r1 := !x ..x .. 1,y .. 1.() \n`\u00b4 ..y .. 1. r0 := !y;r1 := !x ..x .. 1.() The various relaxations, that is, reorderings of memory operations \nthat are allowed in our model, are observed from the behaviour of threads dictated by the rules of the \nweak operational semantics, rather than being prescribed a priori in an axiomatic manner. Ac\u00adcording \nto the classi.cation of Adve and Gharachorloo s tutorial paper[3], we can saythat our modelimplements: \nW.R, that is reordering of a read of some reference w.r.t. writes ondistinct referencespreviously issuedby \nthe same thread; W.W, that is reordering of writes issued by a given thread on distinctreferences; Read \nothers write early, that is thepossibility to read a writeis\u00ad sued by another thread, even before this \nwrite has affected the memory; Read own write early, the same, with respect to the writesissued by agiven \nthread. On the other hand, our model does not implement R.RW, that is, the reordering of reads with respect \nto subsequent memory operationsinthe samethread(thisisexplainedinSection4).One canalsoseethat ourmodel \nmaintains writeatomicity inthesense of[3]. Tothebest of ourknowledge, withthe exception of[33] that we \nwill commentinSection6, most of the approaches about mem\u00adory models describe such a model usingpartial \norders,prescribing in an axiomatic way which arethe allowed(schemata of) execu\u00adtions,assetsofeventsequipped \nwithpartial orderrelations(see [29,34]forinstance, and[14]foran approach using the more ab\u00adstract con.guration \nstructures). However, this notion of an accept\u00adable executionisgenerally not used(with the exception \nof[13]) to give a weaksemantics toprograms.Thepaper[18]forinstance ar\u00adgues that high-level memory models \nshould be used to support an end-to-end approach; however,theproposed memory model(LC), which is based \non partial order execution semantics, is not related to the semantics of some high-level concurrent programming \nlan\u00adguage.In ourproofoftheDRFguarantee, weshall also use apartial order relation on events, to establish \nthat data-race free programs are well-synchronized, that is the fact that two con.icting actions performed \nconcurrentlyin a computation sequence of aDRFpro\u00adgram are separatedbya synchronization action(moreprecisely, \nan unlock action). However, here we directly extract this partial or\u00adderfromthestandardinterleavingsemanticsofthreads,using \ntrue concurrency techniques[11,12].Thenboth our memory model and our proof of the DRF guarantee are based \non descriptions of the strong (that is, interleaving) and weak semantics that follow the standard operational \nstyle which, we believe, is easier to un\u00adderstand, and toformally manipulate(as ourproof shows), than \nan axiomaticpartial order semantics. To conclude thisintroduction, we mustpoint out that wedo not claimweareproposing \nanewrelaxed memory model.Ourcontri\u00adbution rather is to propose using an operational approach to such \nmodels, as this allows us to give a proof of the DRF guarantee at the programming language level. Our \nproof uses arguments that, we think, couldbe adapted to other models than the one with write buffers \nwe are considering here. Inparticular, it would be interest\u00ading to apply similar techniques to more concrete \nmodels, closer to actualhardware designs,possibly with adifferentgeometry than a tree as regards the \ninterconnection network, and to the weak con\u00adcurrency semantics introduced by compiler optimizations \nfor se\u00adquential code. For such weak semantics one should also prove the DRFguarantee. The rest of the \npaper is organized as follows: in Section 2 we introduce ourlanguage, whichisCoreML(withouttyping) with \nsome concurrentprogramming constructs, and wede.ne the refer\u00adence(interleaving) semanticsofthislanguage \ntheoneaprogram\u00admer is supposed to understand. This section also de.nes the data\u00adracefreenessproperty.Then,inSection3 \nwe useBerry andL\u00b4evy s equivalencebypermutation of computations[7,28] to showthat DRF programs are properly \nsynchronized. In Section 4 we intro\u00adduce our main contribution, namely the weak operational seman\u00adtics \nof the language, involving write buffering. Then in Section 5, using the bisimulation method, we show \nour main result, namely that the weak semanticsdoes notintroduce unwanted outcomesfor DRFprograms.InSection6, \nwebrie.ydiscuss some relatedwork, and .nally conclude,indicating somepossiblefuturework. 2. TheLanguage \nOur language is basically Core ML (without typing), that is an imperative call-by-value .-calculus, enriched \nwith thread creation and a constructfor ensuring mutual exclusion.The syntaxis: e ::= v | (e0e1) expressions \n| (ref e) | (!e) | (e0 := e1) | (threade) | (with ldoe) v ::= x | .xe | () values where l is a lock, \nthat is a name from a given in.nite set Locks . As usual, . is a binder for the variable x in .xe, and \nwe shall consider expressions up to a-conversion, thatis up to the renaming ofbound variables.Thecapture-avoiding \nsubstitutionof e0 for the free occurrences of x in e1 is denoted {x ..e0}e1. We shall use some standard \nabbreviations like (let x = e0 in e1)for (.xe1e0), whichis alsodenoted e0 ;e1 whenever x does not occurfreein \ne1. We could easily add to the language standard constructs such as recursion, or conditionalbranching \nonboolean values. To state the operational semantics of the language, we have to extendit with run-time \nconstructs,intwo ways.First, weintroduce references (sometimes also referredto as memorylocations, mem\u00adory \naddresses, or pointers) p, q,. . . that are names from a given in.nite setRef (disjointfromLocks ).These \nare(run-time)values. Then we use the construct (holding l do e)to hold a lock for e. Asitis standardwithlanguagesinvolving \nconcurrency with shared variables, we follow a small-step style to describe the operational semantics, \nwhere an atomic transition consists in reducing a redex (reducible expression)within an evaluation context, \nwhilepossibly performing a side effect.The syntaxis then extended and enriched asfollows: p,q... .Ref \nreferences v ::= \u00b7\u00b7\u00b7 | p run-time values e ::= \u00b7\u00b7\u00b7 | (holding ldo e) run-time expressions r ::= (.xev) \nredexes | (ref v) | (!p) | (p := v) | (threade) | (with ldo e) | (holding ldo v) E ::= [] | (Ee) | (v \nE) evaluation contexts | (ref E) | (!E) | (E := e) | (v := E) | (holding ldo E) As usual, we denote \nby E[e]the expression resulting from .lling the hole in E by e.Every expression of the(run-time)languageis \neither a value, or a redexin apositiontobe reduced, orfaulty.More precisely, let us say that an expression \nis faulty if it has one of the followingforms: (ve)where the value v is not afunction .xe ' ;  (!v)or \nv := v ' where the value v is not a reference.  Then wehave: LEMMA 2.1. Foranyexpression e ofthe run-timelanguage, \neither e is a value, or there is a unique evaluation context E and a unique expression e ' which eitheris \na redex, orisfaulty, such that e = E[e ' ]. (Theproof,byinduction one,isimmediate.) The transitions in \nthe operational semantics go from one con\u00ad.guration to another.A con.guration is a triple (S,L,T)where \nS is the store, L is the lock context, and T is a thread system.Let us de.ne these components, and introduce \nsome notations.The store, alsocalledherethememory,isamappingfroma .niteset dom(S) of references to values.Wedenote \nby S[p := v]the store obtained from S by updating the value of the reference p to v. The lock context \nL is a .nite set of locks, those that are currently held by some thread.More concretely, this would be \nrepresented by a spe\u00adci.cpart ofthestore,giving toa .nitenumberoflocksavalue(not directly accessible \nfrom the language) from the set {free,busy}. However, in the relaxed semantics we will have to maintain \nthis distinctfromthestandard store.Finally T is a thread system.This couldbegiven some structure,like \na multiset, or aqueue ofthreads (that is, expressions), but for our purposes it will be convenient to \nhave some syntaxfor that, namely: T ::= e | (T . T ' ) That is, a thread system is a parallel composition \nof expressions. This syntax is rigid, in the sense that here parallel composition is not assumed tobe \ncommutative or associative. (S,L,T[E[(.xev)]]) . (S,L,T[E[{x ..v}e]]) (S,L,T[E[(ref v)]]) . (S .{p..v},L,T[E[p]]) \np .. dom(S) (S,L,T[E[(!p)]]) . (S,L,T[E[v]]) S(p)= v (S,L,T[E[(p:= v)]]) . (S[p := v],L,T[E[()]]) (S,L,T[E[(threade)]]) \n. (S,L,T[(E[()]. e)]) (S,L,T[E[(with ldo e)]]) . (S,L.{l},T[E[(holding ldo e)]]) l .. L (S,L,T[E[(holding \nldo v)]]) . (S,L-{l},T[E[v]]) Figure 1: the Interleaving Semantics We shall use the symbol C to denote \ncon.gurations. These con.gurations (S,L,T)will be quali.ed as strong, or sometimes standard, in what \nfollows (where there will also be a notion of weak con.guration). As usual, we shall assume we consider \nonly well-formed con.gurations, meaning that any reference that occurs somewherein the con.gurationbelongs \nto thedomain of the store, that is, it is bound to a value in the memory we shall not de.ne thisproperty, \nwhichispreservedinthe operational semantics, more formally.Forinstance,ife is an expression ofthe sourcelanguage, \ntheinitial con.guration (\u00d8,\u00d8,e)is well-formed. Ourlastingredientbeforede.ning theoperational semanticsis \nthe one of parallel evaluation context,givenby T ::= [] | (T. T) | (T . T) The operational semantics \nof thelanguage isgiven inFigure1.As one can see, this is the usual interleaving semantics. At each step \none non-deterministically chooses a thread of the form E[r]to re\u00adduce,ifany.Reducing a redexis always \nan atomic, and, apartfrom thepossible choice wehavewhen creating anewreference,deter\u00administic step. This \nsemantics is intended to serve as the reference semantics for aprogrammer writingin ourlanguage.Since \nthis se\u00admantics is quite standard, we do not further comment it. Let us just note that (with l do e)is \na structured synchronization con\u00adstruct(which weborrow from[31]), the semantics of whichis that it performs \na test-and-set operation on the lock l and, if this suc\u00adceeds, proceeds up to the termination of e, upon \nwhich the lock is released. * As usual, we denote by C . C ' the re.exive and transitive closure of the \ntransition relation, and we say that C ' is reachable * from C if C . C ' . Our main result will be established \nfor con.gurations that are reachable from an initial con.guration of theform (\u00d8,\u00d8,e)where e is an expression \nof the source language. Moregenerally, we shall consider regular con.gurations, where at most one thread \ncan hold a lock, and where a lock held by some threadisindeedin thelock context.Thisisde.ned asfollows: \nDEFINITION (REGULAR CONFIGURATION)2.2. Acon.guration C =(S,L,T)isregular ifand onlyifit satis.es (i)if \nT = T0[E0[(holdingldoe0)]]= T1[E1[(holdingldoe1)]] then T0 = T1 &#38;E0 = E1 &#38;e0 = e1 (ii)T = T[E[(holding \nldo e)]] . l . L For instance, any con.guration of the form (\u00d8,\u00d8,e)where e is an expression of the source \nlanguage is regular.Thefollowing should be obvious: REMARK 2.3. IfC is regular andC . C ' then C ' is \nregular. Apartfrom regularity,the onlyproperty ofprograms(thatis, ex\u00adpressions) we shall considerin thefollowingis \ndata-race freeness. Thisis a safetyproperty, meaning that one cannot reach a con.gu\u00adration wherethere \nare simultaneous accesses tothe same reference, one of thembeing a write access: DEFINITION (DRF PROGRAMS) \n2.4. A con.guration C = (S,L,T)involves a data-race if T = T[E[r]] = T ' [E ' [r ' ]]with T '' T ., \nwhere r and r = are both accesses to the same reference p, that is redexes (!p)or (p := v), at least \none of them is a write (thatis,an assignment).Acon.guration C is data-race free if one cannot reach from \nC a con.gurationthatinvolves adata-race.An expression e isdata-race free iftheinitial con.guration (\u00d8,\u00d8,e)is \ndata-racefree. To conclude this section, let us make a comment on the synchro\u00adnization constructs.We \ncould easily addtothelanguage a construct new lock,for creating a newlock, whichis a run-time value.Then \nwe would use (with e0 doe1)where e0 is an expression.We could also use explicit locking (lock e)and unlocking \n(unlock e)con\u00adstructs,but wethinkitis abetterdisciplineto use ablock-structured construct as the oneproposedhere. \nIt should intuitively be clear that if, in every computation of a given con.guration, two concurrent \ncon.icting accesses to the same memory location are separated by a synchronization action, then the con.gurationisdata-racefree.The \nnext sectionisdevoted to establish that the converseis true. 3. Concurrency,Con.ict andEventOrdering \nToprove our mainresultregarding DRFprograms, weneed toin\u00adtroduce a re.ned presentation of the operational \nsemantics, where we make explicit where a reduction occurs, and what reduction occurs. In doing this, \nwe take our inspiration from previous work on trueconcurrency semantics [11,12],whichinturnreliedon Berry \nandL\u00b4evy s notion of permutation equivalence of computa\u00adtions[7,28].Our aimhereis tobe able tode.nein \na rigorous way thenotionofthe happensbefore relation[26].Moregenerally, we aim at de.ning, with respect \nto a given execution sequence of a program, an ordering meaning that an event in the computation sequence \nintrinsically precedes another one, that is, we cannot make the latter occur before the former, whatever \nreordering we can make of the sequence, by commuting concurrent, non con.ict\u00ading steps.In order tode.ne \nthis notion, wehave tointroduce some technicalde.nitions.We shall use variouskinds of sequences inthe \nfollowing, whichwe shall collectivelydenoteby s,.... (andlater also .), andtherefore we .x a few notations \nregarding sequences: the empty sequence is always denoted e, and the concatenation of the sequence s \n' after the sequence s is denoted s \u00b7 s ' . The pre.x \u00b7 s '' orderingisdenoted =, thatis, s = s ' if \ns ' = s for some s '' . Thelength of s is |s|. Now, we .rst need a notion of occurrence, denoting a path \nfrom the root to a node in a binary tree made of expressions put in parallel. An occurrence is a sequence \nof symbols ., meaning on the left of a parallel composition, and ., meaning on the right. We denote by \nOcc the set of occurrences, that is the free monoid {.,.} * , and we use occ , or sometimes simply o, \nto range over Occ .For each thread system T and occurrence occ , wede.ne the subsystem(subtree) T/occ \nof T at occurrence occ if this is \u00df (S,L,T[E[(.xev)]]) --. (S,L,T[E[{x ..v}e]]) @T .p (S,L,T[E[(ref \nv)]]) --. (S .{p..v},L,T[E[p]]) p .. dom(S) @T rdp (S,L,T[E[(!p)]]) --. (S,L,T[E[v]]) S(p)= v @T wrp \n (S,L,T[E[(p := v)]]) --. (S[p := v],L,T[E[()]]) @T spw (S,L,T[E[(threade)]]) --. (S,L,T[(E[()]. e)]) \n@T l (S,L,T[E[(with ldo e)]]) --. (S,L.{l},T[E[(holding ldo e)]]) l .. L @T (S,L,T[E[(holding ldov)]]) \n--l. (S,L-{l},T[E[v]]) @T Figure 2: the Decorated Operational Semantics indeed anoccurrenceof asubtree \n,intheobviousway,thatis: REMARK 3.3. If o1 .o2, and T/o1 and T/o2 areboth de.ned, then for any T1 and \nT2 we have (T[o1 := T1])[o2 := T2]= T/e = T (T[o2 := T2])[o1 := T1]. (T . T ' )/. \u00b7 occ = T/occ We \nnowde.ne whatitmeansfor actions tobe con.icting.Withthe '' (T . T )/. \u00b7 occ = T/occ idea thatlocking( \nl )and unlocking (l)areboth write actionsin a (otherwise unde.ned),and wede.ne similarly T/occ .The(unique) \n sense, wehave: occurrence of the hole in a parallel context, that is occ satisfying DEFINITION (CONFLICT)3.4. \nThe con.ict relation # is the bi\u00ad T/occ=[], isdenoted @T.Whenever occ is an occurrence in T, nary relation \non actionsgivenby that is T/occ is de.ned, we denote by T[occ := T ' ] the thread S system obtained \nfrom T by replacing the subtree T/occ at occur-#= def {(.p,.p),(wrp,wrp),(wrp,rdp),(rdp,wrp)} p.Ref ' \n rence occ by T (we omit the formal de.nition, by induction on S occ , which shouldbe obvious). Withthe \nnotion ofan occurrence, we are able to say where,that is, in which thread, a reduction occurs: an occurrence \nis similar to athreadidenti.erin athread system(althoughin our setting this identity may dynamically \nchange, upon thread creation). Now to say what occurs, we introduce the notion of an action. There are \nseveralkindsof actions:performing a \u00df-reduction, creating a new reference p in the store, reading or \nwriting a reference, spawning a new thread, and taking or releasing a lock l. Then the syntax of actionsis \nasfollows: .{l,l}\u00d7{ l,l} l.Locks Thisis a symmetric relation whichinparticular, says that accesses to \nthe same reference in the store are con.icting and cannot be re\u00adordered, because this would in general \nresult in a different state, unless, obviously,both accesses are made to read the memory. We could prove, \nas in [11, 12], a diamond lemma, which is a conditional con.uence property saying that two concurrent \nand non-con.icting transitions from a given con.guration can be pushed-out (inthe sense of categorytheory)to \nclosethediagram, resulting in a common con.guration. This was the basis for de.n\u00ad a, b... ::= \u00df | .p \n| rdp | wrp | spw | l | l We denote by Act the set of actions. We now are in a position where we can \nformulate our re.ned, that is, decorated operational semantics.This takes theform of transitions a'' \n' (S,L,T)-(S ,T ) . ,L o where a is the action performed, and o the occurrence where it is done. This \nis described in Figure 2. The relation between the two presentations ofthe semantics shouldbe obvious: \nLEMMA 3.1. For any con.guration C, we have C . C ' if and a onlyif C -. C ' for some actiona and occurrence \no. o Two occurrencesin athreadsystem T are concurrent if,intuitively, theylead to non-overlapping parts \nof the tree T, orin other words, they aredivergingpaths.Thisis easilyformallyde.ned: DEFINITION(CONCURRENCY)3.2. \nTwooccurrenceso and o ' are concurrent, in notation o. o ' , if neither is pre.x of the other: \u00ac(o = \no ' )&#38;\u00ac(o ' = o). Itiseasyto seeforinstancethatifoccurrenceo2 follows occurrence o1 in a sequence \nof(decorated) transitions, then either o1 .o2 or o1 = o2, that is, o2 cannot be a strict pre.x of o1. \nOne can also observe that a thread system T involves a data-race if there are concurrent occurrences \nof accesses to the same referencein T, one ing the equivalence by permutation of computations in the \nabove mentioned papers. Here we shall use another property, which we call asynchrony (see [12] for references \nabout this terminology). This property asserts that two consecutive steps in a computation can be commuted \nif the actions are not con.icting, and the occur\u00adrences where they areperformed aredisjoint: a1a2 LEMMA \n(ASYNCHRONY) 3.5. If C -. C1 -. C2 where o1 o2 \u00ac(a1 #a2)and o1 .o2 then there exists a unique con.guration \n' a2' a1 C1 such that C -. C1 -. C2. o2 o1 PROOF: for this proof we let C =(S,L,T), C1 =(S1,L1,T1) \nand C2 =(S2,L2,T2). First we observe that if T = T1[E1[r1]] with T1 = T1[e1]= T2[E2[r2]], so that o1 \n=@T1 and o2 = @T2, with T2 = T2[e2],then wehave T/o2 = E2[r2]since o1 . o2. Then if a2 can be performed \nfrom (S,L,T)at occurrence o2, which onlydepends on S and L, with a2'' (S,L,T)-. (S1' ,L 1' ,T 1)= C1 \no2 where T1 ' = T[o2 := e2], then T1' /o1 = E1[r1]and we know by Remark 3.3 that, if the action a1 can \nbe performed at occurrence o1 from(S1' ,L 1' ,T 1' )(whichonlydepends on S1 ' and L ' 1),this will result \nin a con.guration of the form (S2' ,L 2' ,T 2' )where T2 ' = T2. of whichisawrite.Thefollowing shouldbeclear: \nIt remainstocheck that,thankstothehypothesis \u00ac(a1 #a2), the steps can indeed be permuted, and that S2 \n' = S2 and L ' 2 = L2. Thiscomputationisequivalenttothefollowing one: Then weproceedby a case analysis \nonthetransitions.Let usjust \u00df wrp examine afew cases and sketch the correspondingproof(allthe (S,\u00d8,T) \n-. (S,\u00d8,(e0 .(.zze ' 0)))- . (S ' ,\u00d8,(().(.zze ' 0)) numerous cases are actually equally easy).Inparticular, \nwedo not wrp \u00df (S '' (S '' --. ,\u00d8,(().(.zz()))-. ,\u00d8,(().()) investigate the cases where one of the actions \nhas no side effect, that is, one of the actions is \u00df or spw, where it is obvious that the two actions \ncanbe commuted. a1 = .p. In this case we cannot have a2 = rdp or a2 = wrp, sincethis wouldimplyo1 = o2 \n(forpdoes notoccurin T,sincethe con.guration (S,L,T)is well-formed),acontradiction.Ifa2 = .q we musthave \nq .psince p . dom(S1), andin this case = a2'' (S,L,T)-. (S1,L,T1) o2 with dom(S1' )= dom(S).{q}, and \ntherefore a1 can be per\u00adformed from the con.guration (S1' ,L,T1' ) since p .. dom(S1' ), andperforming \na1 resultsin (S2,L2,T2)(whereL2 = L).Allthe other cases are easy. a1 = wrp.In this case weknow that a2 \nis neitherwrp nor rdp.If, forinstance, a2 = wrq with q .= p, then wehave S1 = S[p := v1] and S2 = S1[q \n:= v2]for some values v1 and v2. Then if we let S1 ' S ' = S[q := v2], we clearly have S2 = 1[p := v1], \nand we easilyconcludein this case. wherewehavepermuted the .rsttwosteps.Wecannotgofurther, since the \nsecond step in this computation is con.icting with the third, whichinturnis not concurrent with thelast \none(theselast two steps are in program order since they are performed from the same thread). In this \nexample we can say that the .rst wrp inherently precedes (this will be formally de.ned below) the second \nsuch action, and also precedes the second \u00df, and that the .rst \u00df inherently precedes the second wrp and \nthe second \u00df. This example shows that a given action, say wrp, may have, in a given computation, like \n., several different relations with another action,like \u00df.Thenthe notion ofaction, evenifcomplementedwith \nits occurrence, is not the right basis to de.ne the ordering we are lookingfor.Wehavetointroduce the \nnotion ofan event, asfollows: DEFINITION (EVENTS) 3.7. An event in a computation . is a pair (a,o)i decorated \nby a positive integer i, where the action a is performed at occurrence o in the computation ., and i \nis less than the number of suchpairs (a,o)in . (thatis,(a,o)i is the i-th occurrence ofaction a performedat \noccurrence o in.).Wedenote a1 = l . In this case S1 = S and L1 = L .{l} with l .. L. byEvent(.)the set \nofeventsdeterminedbythe computation .. The action a2 cannot be neither l nor l, and itis easy to see \nthat Forinstance,for the computation . above, wehave it can be performed from (S,T,L), resulting in \n(S1' ,L,T1' ), and 1 112 therefore a1 can be performed from the latter con.guration. The Event(.)= {(wrp,.),(wrp,.),(\u00df,.),(\u00df,.)} \ncase where a1 = l is similar. We can now de.ne the equivalence by permutation of transitions on computations. \nAcomputation is a sequence of(decorated)tran\u00adsitions a1an . = C0 -. C1 \u00b7\u00b7 \u00b7 Cn-1 - . Cn o1 on ai+1 More \nformally, . is a sequence of steps Ci - -. Ci ' such that oi+1 Cj+1 = Cj ' for any j, but one should \nnotice that, given an ini\u00adtial con.guration C0, the sequence of actions and occurrences is enough to \ndetermine the whole computation (and among the ac\u00adtions, only the .p s are actually necessary). Then \nthe equivalence by permutation is the congruence (with respect to concatenation, which, on computations, \nis only partially de.ned) on such se\u00adquencesgeneratedby the asynchronyproperty: DEFINITION (EQUIVALENCEBY \nPERMUTATION)3.6. Theequi\u00advalencebypermutation of transitions is theleastequivalence . on computations \nsuch that a1a2 (i)ifC -. C1 -. C2 where \u00ac(a1 #a2)and o1 .o2 then o1 o2 a1a2a2' a1 C -. C1 -. C2 . C -. \nC1 -. C2 o1 o2 o2 o1 where C1 ' isdetermined asin theAsynchronyLemma; (ii).0 . .0 ' &#38;.1 . .1 ' . \n.0 \u00b7 .1 . .0 ' \u00b7 .1' . It should be clear that if . . . ' then |.| = |. ' |, and . and . ' perform the \nsame actions atthe same occurrences(andinthe same number for each of such pair), possibly in a different \norder. The main purpose of this de.nition is to allow us to formally de.ne an event ordering relation \nwith respect to a computation .. To introduce this last notion, let us .rst see an example. Let e0 =(p \n:= v), e0 ' =(p := v ' ) e1 =(.x(.zze ' 0)()) and T =(e0 . e1).Thenfor S satisfying p . dom(S)wehave \nwrp\u00df . =(S,\u00d8,T)- . (S ' ,\u00d8,((). e1))-. (S ' ,\u00d8,(().(.zze ' 0)) For agiven computation a1an . = C0 -. \nC1 \u00b7\u00b7\u00b7 Cn-1 - . Cn o1 on we denote by .(.)the sequence (a1,o1)1 \u00b7\u00b7 \u00b7 (an,on)k of events of. in temporal \norder, thatis, as theyappear successivelyin ..We can .nallyde.nethe event ordering determinedbya computation: \nDEFINITION (EVENT ORDERING) 3.8. Given a computation ., we say that an event (a,o)i . Event(.) inherentlyprecedes \n'' '' )j (a,o )j . Event(.)in ., in notation (a,o)i =. (a,o , if and only if in any . ' . ., the i-th \noccurrence of (a,o)precedes the j-thoccurrence of(a ' ,o ' ). Itshouldbe clear that thisisindeedan ordering, \nthatis, a re.exive, transitive and anti-symmetric relation. Moreover, two con.icting actions can neverbepermuted, \nand thereforeif i '' j .(.)= s0 \u00b7 (a,o)\u00b7 s1 \u00b7 (a,o )\u00b7 s2 ' '' )j with a #a then (a,o)i =. (a,o .For \ninstance, in the compu\u00adtation . above, wehave 1 12 (wrp,.)=. (wrp,.)=. (\u00df,.) and (\u00df,.)1 =. (wrp,.)1 \n The event ordering in a computation contains in particular the program order, that relates actions (a,o)and \n(a ' ,o ' )in temporal order, such that o = o ' (this ordering also takes into account the creation of \nredexes identi.edbyL\u00b4evy[28]in the .-calculus). To conclude this section weprove aproperty ofDRFprograms \nthat will be crucial in establishing our main result. This property shows in particular that if, in a \ncomputation starting from a DRF con.guration, two con.icting actions are performed concurrently, theninbetween \nthe two there mustbe a synchronization, thatis an (S '' (S '' action l of releasingalock.Let usde.ne: \nDEFINITION (WELL-SYNCHRONIZED)3.9. Acon.guration C is (2) (a '' ,o '' )h .=. (a ' ,o ' )j.In this case \nwe use theTransposition well-synchronized if,foranycomputation Lemmaabove,and concludeusing theinductionhypothesis. \na1an C = C0 -. C1 \u00b7\u00b7 \u00b7 Cn-1 - . Cn o1 on 4. TheWeakMemoryModel where ai #aj (withi <j)and oi .oj then \nthere exists h such Inthis section weintroduce our main contribution, namelyan oper\u00ad that i . h . j, \nah = l and oi = oh. Thisis similarto theDRF0property of[4].To show thatDRFpro\u00adgramsarewell-synchronized,we \n.rstneedalemma,statingthat,in a computation, two events, one of which precedes, in serialization order, \nthe other one, but not inherently, can be moved to occur in the reverse temporal order: LEMMA (TRANSPOSITION)3.10. \nLet . be a computation such that i '' j .(.)= s0 \u00b7 (a,o)\u00b7 s1 \u00b7 (a,o )\u00b7 s2 '' )j with (a,o)i .=. (a,o \n. Then there exists . ' . . such that '' \u00b7 s '' .(. ' )= s0 \u00b7 s1 ' \u00b7 (a,o )j \u00b7 (a,o)i 1 \u00b7 s2. PROOF: \nby induction on |s1|. If s1 = e, we have neither o = o ' ' '' nor a # a , since otherwise we would have \n(a,o)i =. (a,o )j , and therefore o. o ' and \u00ac(a # a ' ). Then by de.nition of the permutation equivalence \nwe can commute these two steps. If '' '' s1 =(a ,o )h \u00b7 . there are two cases: '' '' '' (a ,o )h .=. \n(a,o )j.In this case we applytwice theinduction '' '' '' hypothesis, to transpose .rst (a ,o )h and (a,o \n)j, and then the latter with (a,o)i . '' '' )h =. (a '' )j'' (a ,o ,o .Wedo nothave o = o , since otherwise \nwe would have (a,o)i =. (a ' ,o ' )j by transitivity, and therefore o. o '' . Similarly, it is impossible \nthat a # a '' , and then by de.nitionof thepermutation equivalence wecantranspose (a,o)i '' '' )h with \n(a ,o , and conclude using theinductionhypothesis. PROPOSITION 3.11. DRF regular con.gurations are well-syn\u00adchronized. \nPROOF: we show that for any computation . starting from a DRF regular con.guration C,if .(.)= s0 \u00b7 (a,o)i \n\u00b7 s1 \u00b7 (a ' ,o ' )j \u00b7 s2 '' '' ' )j with (a,o)i =. (a,o and o. o , then there exist l, o , k, .0 ational \nformulation of a relaxed memory model. As indicated in theIntroduction, we extend the syntax of thread \nsystems with a se\u00admanticalingredient, namelythe one ofa(write)buffer.Ourbuffers are not simply FIFO .les \nrecording the memory updates issued by a thread. We adopt a weaker memory model, where the writes re\u00adgardingeachreference \nareindependentlybuffered.Then a bufferB is a mapping from a .nite set dom(B)of references to sequences \nof values. We use the symbol s to denote sequences v1 \u00b7\u00b7\u00b7 vn of values.In such a sequence thehead(.rst-in \nvalue v1)is on theleft, andthe tailis on the right, so thatthelast-in valueis vn.Inparticu\u00adlar {p .. \nv} denotes abuffer which assigns to the single reference p the sequence made of the single value v. We \ndenote by W(B) the set ofreferencesfor whichthereisindeedsome write operation bufferedin B, thatis W(B)= \ndef { p| p . dom(B)&#38;B(p).= e} In order to formulate the weak semantics, we need some technical de.nitions \nregarding buffers. First, we denote by B[p. v] the buffer obtained from B by putting the value v at the \nend of the queue assigned to this reference in B. If there is no such queue, that is, if p .. dom(B), \nthis is simply B .{p .. v}. Then we denote by B . pthebufferobtainedby removing the .rst-invalue for \np in B. We shall only use this in the case where p . W(B), and therefore we consider thisis notde.ned \notherwise.The thread systems with writebuffers aredescribed asfollows: T ::= T |.B.T | (T. T) We extend \nthe notation W tothese trees,denotingbyW(T)the set ofreferencesfor whichthereis a write operationbufferedin \nT(the formal de.nition, which should be obvious, is omitted). A weak con.guration is a con.guration possibly \ninvolving write buffers, that is a triple of the form (S,L,T). We still only consider well\u00adformed (weak)con.gurations, \nand this means in particular that if p is in the domain of a buffer occurring in T, then we must have \np . dom(S).We needtogeneralizetheparallelcontextsinto weak contexts, asfollows: and .1 such that (a,o)i \n\u00b7 s1 \u00b7 (a ' ,o ' )j = .0 \u00b7 (l,o '' )k \u00b7 .1 with o = o '' .Weproceedbyinduction on |s1|. ' T ::= [] |.B.T \n| (T. T) | (T. T) s1 = e. In this case we must have a # a , since otherwise '' )j we could commute (a,o)i \nand (a,o , contradicting (a,o)i =. '' ' (a,o )j.Then weproceed by case on a #a .Itisimpossible that a \n= .p = a ' ,because one cannot create twice the same reference in a given computation. If a,a ' .{wrp,rdp} \nfor some p, where either a or a ' is wrp, then e is not data-race free, since o.o ' . As above, W(T)denotes \nthe set of references for which there is a write operation recorded in the context T. In the semantics \nwe will use the most recent value for a reference p along agivenpath in a weak con.guration. In fact \nwe shall only use this for the path leading to a read instruction (!p)in a weak context T.In order to \nde.nethe mostrecent valuefor pinT,we .rstde.nethesequence Finally theonlypossible caseis a,a ' .{l,l} \nfor some l.Since bu.(p,T)of valuesbufferedfor pin T, asfollows: one cannot acquire twice the same lock \nconsecutively in a compu-bu.(p,[])= e ' tation, either a or a is l. Moreover, since C is a regular con.gu\u00ad \nbu.(p,.B.T)= ration, the same holds for the con.guration performing (a,o)i, by ( B(p)\u00b7 bu.(p,T) ifp . \nW(B) bu.(p,T) otherwise ' Remark 2.3, and therefore either a = l, or a = l , a = l and ' o = o. '' '' \ns1 =(a ,o )h \u00b7 ..Wedistinguish again two cases. '' '' '' '' ' (1) (a ,o )h =. (a,o )j.If o .o then we \nuse theinduction hypothesis to conclude. Otherwise, we have o '' = o ' , and since ' '' '' o. o thisimplies \no .o,forit cannot be the case that o <o, '' ' ' whereas o = o wouldimplyo = o , contradictingo. o .Nowif \na#a '' we argue asinthebase case, andotherwise we can commute '' '' (a,o)i with(a ,o )h, and then applytheinductionhypothesis. \nbu.(p,(T. T))= bu.(p,T) bu.(p,(T. T))= bu.(p,T) Then the most recent valuefor pin (S,T)isde.ned asfollows: \nDEFINITION (MOST RECENT VALUE)4.1. Given a pair (S,T) of a store S and a weak context T,for anyreference \np . dom(S) the most recent value given to p in (S,T), denoted (S,T)(p), is the value v suchthat bu.(p,T)= \ns \u00b7 v,ifsuch a value exists, and S(p)otherwise. (S,L,T[E[(.xev)]]) - -. (S,L,T[E[{x ..v}e]]) (S,L,T[E[(ref \nv)]]) - -. (S .{p..v},L,T[E[p]]) p .. dom(S) T rdp (S,L,T[E[(!p)]]) - -. (S,L,T[E[v]]) (S,T)(p)= v ao \n-. (S ' ) (S,L,T) @ T wrp (S,L,T[E[(p := v)]]) - -. (S,L,T[.{p .. v}.E[()]]) @ T spw (S,L,T[E[(threade)]]) \n- -. (S,L,T[(E[()]. e)]) @ T l@ (S,L,T[E[(with ldo e)]]) - -. (S,L.{l},T[E[(holding ldo e)]]) l .. L \nT l@ (S,L,T[E[(holding ldo v)]]) - -. (S,L-{l},T[E[v]]) T T (S,L,.B.T) -.  (S[p := v],L,.B . p.T) B(p)= \nv \u00b7 s e (S,L,T[.B0..B1.T]) - -. (S,L,T[.B0[p. v]..B1 . p.T]) B1(p)= v \u00b7 s @ T (S,L,T[(.B.T. T ' )]) \n- -. (S,L,T[.{p .. v}.(.B . p.T. T ' )]) B(p)= v \u00b7 s @ T (S,L,T[(T..B.T ' )]) - -. (S,L,T[.{p .. v}.(T..B \n. p.T ' )]) B(p)= v \u00b7 s @ T (S,L,T[.B.T]) - -. (S,L,T[T]) W(B)= \u00d8 @ T Figure 3: the Weak Operational \nSemantics We denote by T thefactthatthereis nobuffered write(for any reference)in T on the path from \nthe root to the occurrence of the hole, thatis: T .def .p.bu.(p,T)= e One maynoticethatifT ,thenfor anyreference \npthe most recent value for pin (S,T)is the one in the store S.As a lastingredient we needin ordertode.nethe \nweaksemantics, weextendthe notion ofapath,i.e. occurrence, tothe weaksetting.We adda new symbol . to \ndenote a step through a buffer. Then the notion of subtree T/occ at occurrence occ is rede.nedin the \nobvious way, with .B.T/.\u00b7 occ =T/occ (the remainingclauses do not change). Similarly, the de.nition of \ntheoccurrence of theholeinaweak context T, stilldenoted @T, is extended in the obvious way, that is @.B.T \n= .\u00b7 @T. We now have all the technical de.nitions needed to de.ne the weak operational semantics, whichisdescribed \nas transitions ,L ' ,T ' de.ne: fence = def (letx = new lockin(with x do ())) Notice that, as usual, \nthe semantics of acquiring or releasing a lock arestill strong, thatis,these(write)operationsarenot delayed, \nand are atomic. In addition to that, the weak semantics involves operations to up\u00addate,inanasynchronousway(butrespecting \nthe programorder foreach reference),thememory.Thisisdoneby meansof the .rst fourundecoratedtransitions,wherenoactionisindicated \nwealso call these the silent transitions: the .rst one pushes a value from a topmost write buffer into \nthe store, and the other three transfer a buffered value to an upperlevel. Withthese rules,the write \noperationsissuedfrom agiventhread on different references are made independent, because a buffer records \nadistinctqueuefor each reference, and therefore they may be globallyperformed, thatis,.nallyreachthestore,inanyorder. \nSimilarly, the writes issued by different threads may update the store with no particular order. We let \nthe reader check that, for instance, apossible outcome of possibly without any actionlabel.ThisisgiveninFigure3.There \nare severalimportantdifferences with the reference semantics: (1) whenperforminga readoperation (!p),wetakethe \nvalue asthe most recent onefor palongthepaththatleadsto this operation. This value is taken from the \nstore only when there is no write buffered for p on the path to the store, that is bu.(p,T)= x := 1; \n` p0 := !y; p1 := !x ; \u00b4 y := 2 x := 3 p2 := !x is{p0 .. 2,p1 .. 3,p2 .. 1}.This exampleistakenfrom[34], \nas wellas thefollowingone: e. We could also adopt this as a condition for performing a read operation \n(!p), thus de.ning a stronger memory model2, without affectingthe correctness result. (2) a write operation \n(p := v)issued by a thread is delayed, that is,the value v isputin a writebufferforp.Suchan operationis \ntherefore asynchronous, thatis, non-blocking. (3) an unlock action now operates as a memory barrier, \nsince one cannot releasealock unless all thewritebuffersonthepath to the store are empty.Indeed,in an \nextendedlanguage, one could  2similar to theIBM370 model, and thePC model, see[3]. x := 1; . p2 := !x \n; p0 := !y; x := 2; p1 := !xy := 3 where, a possible outcome is {p0 .. 3,p1 .. 1,p2 .. 1}, as the reader \nmay check. An execution that is not permitted in our model, but is allowed in the JAVA memory model for \ninstance is the following one. Let us assume the store initially contains {x .. 0,y .. 0}. Then the outcome \n{p0 .. 1,p1 .. 1} is not possiblein our relaxed modelfor theprogram p0 := !y;x := 1 . p1 := !x ;y := \n1 because one of the statements p0 := !y or p1 := !x has to be issued .rst, and in our model this means \nthat !y or !x has to be evaluated, returning the value 0.Thatis, asin[15], the value to storein an assignmentisknown \nat the time ofissuance ofthe write operation, and the relaxation, w.r.t. the strong semantics, concerns \nmemory operations(redexes)of theform p := v and !p. In other words, afeatureof amemory modelbased on \nwritebuffersisthat it does not allow the reordering of reads after subsequent memory operations(R.RW \nfollowing the terminology of[3]). The alert reader may have noticed that our semantics is a little bit \ntoo rigid.Forinstance, withtheprogram (threadp0 := !x);(threadp1 := !x);x := 1 and the initial store \n{x .. 0}, the outcome {p0 .. 1,p1 .. 0} is not possible, because the second thread is always informed \nof the write x := 1 before the .rst one. As a remedy to this, we could adopt a non-deterministic semanticsfor \nthread creation, namely (S,L,T0[T1[E[(threade)]]]). (S,L,T0[(T1[E[()]]. e)]) However, theproof of our \nmain result would be more complicated with this rule, and therefore we do not consider it, since we are \nmainly interested in the semantics of DRFprograms.(The seman\u00adtics of programs that run into a data-race \nis sometimes considered unde.ned, see[8,31]forinstance.) Clearly, by modifying the architecture of the \nweak con.gura\u00adtions, we could describe different memory models. For instance, the write buffers could \nbe queues of pairs (p,v)meaning that an update for reference p has been issued, in which case reordering \nthe writesissuedfrom agiven threadisimpossible.One could also assumethatthereis onlyone, or a .nite number \nofbuffers atthetop ofthe con.guration, thus re.ectingstatic architectures witha .xed number of processors \neach with their own write buffers. All these models are stronger, that is, less relaxed than the one \nwe describe in Figure 3, and therefore they induce fewer weak behaviours for programs. ThelastruleinFigure3allows \none to remove emptybuffers.In this way, we mayreduce a weakcon.guration to a standardone.To see this, \nlet us denote by (S,L,T)-* . (S ' ,L ' ,T ' )the transition relationinductivelyde.nedbythefollowingrules: \nC -. C '' -. * C ' o ** C -. CC -. C ' andlet T/ occ be thebufferfound at node occ inT,ifany, thatis: \n.B.T/ e = B .B.T/ .\u00b7 occ =T/ occ (T. T ' )/ . \u00b7 occ =T/ occ (T. T ' )/ . \u00b7 occ =T ' / occ Then wede.ne \nthe size |T| of the thread system withbuffers Tas follows: |T| =0 |.B.T| = 2+.T. + |B| (|T0| + |T1|)(|T0| \n+|T1| +3)|(T0 . T1)| = 2+ 2 where P |B| = |B(p)| p.W(B) P .T. = |T/ o| {o|.B. T/ o=B} Then wehave LEMMA \n(TERMINATION) 4.2. (S,L,T) -. (S ' ,L,T ' ) . |T| > |T ' | (The proof is omitted.) One can also check \nthat if |T| > 0 then there is (S ' ,L,T ' )such that (S,L,T)-. (S ' ,L,T ' ), and there\u00adfore wehave: \nCOROLLARY 4.3. For any weak con.guration (S,L,T) there exists a standard con.guration (S ' ,L,T)such \nthat (S,L,T)-* . (S ' ,L,T). As a consequence, we observethatifthereis an unlockoperationto perform, \none can always clearthebuffersto reacha state where the lockcanbe released.In other words, thereis nodeadlockdue \nto an unlock operation.(Thereareobviously deadlocked con.gurations, where threads are stuck on trying \nto acquire busy locks.)We shall use thefollowingnotation: * (S,L,T). (S ' ,L,T) .def (S,L,T)-. (S ' ,L,T) \n Notice that,given C =(S,L,T), there are ingeneral several dis\u00adtinct stores S ' suchthat C. (S ' ,L,T),because \nthe writesbuffered atdisjoint occurrences, thatis,issuedbyconcurrent threads, canbe used to update the \nmemoryin anyorder. 5. Correctness To establish our correctness property, which says that the weak semantics \nagrees with the reference semantics for data-race free programs, we .rst introduce a property of weak \ncon.gurations that is called coherence, which means that there are no concurrent buffered writesforthe \nsame referenceinthe con.guration(there maybe concurrent writes,but theyconcern distinct references). \nDEFINITION (COHERENCE)5.1. Athread systemwithbuffers T iscoherent ifandonlyif o. o ' . W(T/ o)n W(T/ \no ' )= \u00d8 for anyo,o ' .Occ .We also saythat the con.guration (S,L,T)is coherent if Tis coherent. In \nother words, Tis coherentiffor anygiven reference pthe set { o | p . W(T/ o)} is totally ordered by \nthe pre.x order =. This property obviously holds for any standard con.guration (S,L,T),hence inparticular \nfor initial con.gurations of the form (\u00d8,\u00d8,e). It should be intu\u00aditively clear that the relation . is \ndeterministic for coherent con\u00ad.gurations. This is what we now prove. First we observe that the silent \ntransitionspreserve coherence: LEMMA 5.2. If T is coherent and (S,L,T) --. (S ' ,L ' ,T ' ) then T ' \nis coherent. occ PROOF: one checks that, for any reference q, if q . W(T ' / o)n W(T ' / o ' ) then o \n= o ' or o ' = o, by cases on the transition (S,L,T)--. (S ' ,L ' ,T ' ).If T= T[(.B.T0 . T1)]and T ' \n= occ T[.{p .. v}.(.B . p.T0 . T1)] we only consider the case of q = p. We see that, since T is coherent, \nif p . W(T ' / o) then either o = @T, or o =@T\u00b7.\u00b7 o ' withp . W(.B. p.T0/ o ' , and therefore the occurrences \nof writes for p in T ' are totally ordered w.r.t. the pre.x ordering. The case where T= T[(T0 ..B.T1)] \nand T ' = T[.{p .. v}.(T0 ..B . p.T1)]is similar, and all the other cases areimmediate. Next we show \nthat the silent transitions on coherent con.gurations arelocallycon.uent: LEMMA (LOCAL CONFLUENCE) 5.3. \nIf C is a coherent weak con.guration, andC - . C0 andC - . C1 then either C0 = C1 o0 o1 ** ' '' or there \nexists C suchthat C0 -. C andC1 -. C . PROOF: there are many cases to consider, which are all easy, and \ntherefore we only examine a few of them. For instance, we may disjoint occurrences, C =(S,L,T[(.B0.T0 \n..B1.T1)]and C0 =(S,L,T[.{p .. v}.(.B0 . p.T0 ..B1.T1)]) C1 =(S,L,T[.{q .. v ' }.(.B0.T0 ..B1 . q.T1)]) \nSince C is coherent, wehave p.q, andin this case welet = C ' =(S,L,T[.{p .. v,q .. v ' }.(.B0 . p.T0 \n..B1 . q.T1)]) ** and wehave C0 -. C ' and C1 -. C ' in three steps. When the two transitions C - . C0 \nand C - . C1 are o0 o1 performed at occurrences that are related by the pre.x order, we mayhaveforinstance \nC =(S,L,T[.B0..B1..B2.T])with C0 =(S,L,T[.B0[p. v]..B1 . p..B2.T]) C1 =(S,L,T[.B0..B1[q. v ' ]..B2 . \nq.T]) (wherepossiblyp = q).In this case welet C ' =(S,L,T[.B0[p. v]..(B1 . p)[q. v ' ]..B2 . q.T]) ** \nand wehave C0 -. C ' and C1 -. C ' in one step. As a consequence ofthelemmas4.2,5.2and5.3,the relation \n-. is con.uent on coherent con.gurations, and therefore COROLLARY 5.4. IfC is a coherent con.guration \nthen C . C0 &#38;C . C1 . C0 = C1 To establish our main result, we need a technical de.nition. We denoteby \np(o)the projection of the(weak)occurrence o,given as follows: p(e)= e p(.\u00b7 o)= p(o) p(. \u00b7o)= . \u00b7p(o) \np(. \u00b7o)= . \u00b7p(o) DEFINITION(TheBISIMULATIONRELATION)5.5. Foranygiven (strong) con.guration C, we de.ne \nthe relation R(C) between weak and strong con.gurations as follows: C ' R(C)C '' if and onlyifthere exists \na sequence ofweaktransitions a0an ' C0 = C -. * - . C1 \u00b7\u00b7\u00b7 -. * --. Cn = C o0 on such that ' a0' an' \n'' CC ---. C \u00b7 \u00b7 \u00b7 - --. C = C 0 = 1 n p(o0) p(on) is a valid sequence of(strong)transitions, withCi \n. Ci ' for alli. (Notice that sinceC is a standard con.guration, we actually have, a0 withthe notations \nofthede.nition, C - . C1.)We showthat,ifC o0 is aDRFcon.guration, the relation R(C)isindeedabisimulation. \nFirst, we observe that the weak semantics simulates the reference one: a ' '' '' '' PROPOSITION 5.6. \nIf C R(C)C and C -. C then there o '''' ' exist C and o such that C -* . -a. C with o = p(o ) and ' \no ''' '' ''' '' C = C , hence obviously C . C . It is easy to see that ''' '' C . C also holds in the \ncase where a = wrp, since there is onlyonebuffered write(on p)inC ''' . Toprovethat,conversely,the weaksemanticsdoes \nnotdeviatefrom thereference semantics asregardsdata-racefreeprograms,weneed thefollowinglemma: LEMMA \n5.7. LetC be a strongregular con.guration suchthat * a0* an C = C0 -. - . C1 \u00b7 \u00b7 \u00b7 -. --. Cn =(S,L,T) \no0 on with p . W(T/ o). Then there exists i such that ai = wrp with p(o) = p(oi), and for all i <j . \nn if p(oi) = p(oj) then aj .l = . PROOF: by induction on n. First we observe that, due to the hy\u00adpothesis \nW(T) .\u00d8, we must have n =0, since C is a standard con.gurations. If n =1, then it is easy to see that \nthe only possi\u00adbility,in order tohave W(T).\u00d8,is a1 == {p} = wrp withW(T)(ando = o1). Otherwise(n> 1), \nweproceed bycases on an.We notice that if an = l then o .= on. The lemma is obvious in the case where \n* an an = wrp and o = on. Otherwise, we have Cn-1 -. --. Cn, on andif Cn-1 =(S ' ,L ' ,T ' )wehave \np . W(T ' )with '' '' p . W(T/ o) ..o .p(o )= p(o)&#38;p . W(T / o ) and we conclude usingtheinductionhypothesis. \nNow we show that, for data-race free regular con.gurations, the second half of our bisimulation result \nholds. Moreover, we show that in the bisimulation scenario, the coherence property is pre\u00adservedby the \nweak semantics(notjustthe silenttransitions asin Lemma5.2): PROPOSITION5.8. IfCisaDRFregular con.guration,C \n' R(C)C '' ' ''' * a where C is coherent and C -. -. C then C is coherent and o a '' '' '' ''' there \nexists C such that C - -. C and C R(C)C . p(o) PROOF: wehave * a0* an ' C0 = C -. - . C1 \u00b7 \u00b7 \u00b7 -. --. \nCn = C o0 on and a0' an' '' C ---. C1 \u00b7 \u00b7 \u00b7 - --. Cn = C p(o0) p(on ) * a ' '' with Ci . Ci for all \ni.Let D be such that C -. D -. C .Then o D is coherent by Lemma 5.2. By Lemma 4.3 there exists D such \nthat D . D, hence C ' . D, and therefore D = C '' by Corollary 5.4.Weproceed byinduction on thelength \nof the sequence of -. \u00ad '' '' transitions from D to C . If this length is 0, that is, D = C , we have \np(o)= o since C '' is a strong con.guration, and either .a' = a wrp and D -. C , or a = wrp. In the \n.rst case, we may let o ' '' C R(C)C . '' ' '' C = C . In the second case there obviously exists C such \nthat PROOF:thisisimmediate,because if '' ''' a D -. C and C . C . Since there is exactly one write buffered \n* a0* an ' o C0 = C -. - . C1 \u00b7 \u00b7 \u00b7 -. --. Cn = C ' ''' in C , this con.gurationis coherent, and clearlyC \nR(C)C . o0 on '' Otherwise let D ' be such that D -. D ' -. * C . We showis such that ' o a a0' an' \n'' that there exist D and u such that D is coherent and D ' -. D C ---. C1 \u00b7 \u00b7 \u00b7 - --. Cn = C u p(o0) \np(on) with p(u)= p(o) (we shall then conclude using the induction ' ''' with Ci . Ci for all i, hence \nin particular C . C , then we have hypothesis regarding D ' ). We proceed bycases on the transitions \n* aa' ' '' ''' C -. C -. C , and in all cases except a = wrp we have D -. C and D -. D ' . ' There \nare manycasesto consider, most ofwhichareimmediate. We only examine the ones where a = wrp or rdp, that \nis D = (S,L,T)withT= T[E[r]]where r =(p := v)or r =(!p)and o =@T. r =(p := v).Wehave C ' =(S,L,T[.{p \n.. v}.E[()]]).Ifo. o ' ,let us consider the case where T = T ' [T0[.B0..B1.T ' ]. T1] with '' '' D =(S,L,T \n[T0[.B0[q. v ]..B1 . q.T]. T1[E[r]]]) ando =@T ' \u00b7 . \u00b7@T1.Assumethatq = p.Then p . W(T/ o ' \u00b7.) with \no ' =@T ' \u00b7 . \u00b7@T0 andbyLemma5.7there exists isuchthat ai = wrp andp(o ' \u00b7.)= p(oi), with aj .l for i \n<j . n if p(oi) = p(oj). = Then p(oi) .p(o), but this contradicts Proposition 3.11 since ai # a and C \nis data-race free and regular. Then it must be the case that q .p, andifwelet D =(S,L,T '' =)where THEOREM \n(CORRECTNESS)5.9. The weak memory model im\u00adplementsthe reference semanticsfordata-racefreeprograms.More \nprecisely, the strongcon.gurations reachablefrom a(strong)DRF regular con.guration C in the weak semantics \ncoincide with the con.gurations reachablefrom the same con.guration C in the ref\u00aderence semantics. Notice \nthat in particular the weak semantics correctly implements sequentialprograms, thatdo not use the (threade)construct. \n6. SomeRelatedWork In the Introduction we brie.y surveyed part of the literature on memory models, where \nour main source ofinspiration was[15]. Inthis areathe workthatisthe closestto oursis[33], wherethe authors \nintroduce a syntactic model for hardware architectures in\u00advolvingbuffers(FIFOqueues of write requests)and \ncaches, with rewritingrules which,in effect,de.ne an operational semanticsfor thebasic memory operations,decomposed \nusingcommit/reconcile steps. The target of this model is hardware design, and therefore the syntactic \nstructureis static, andtheprogramminglanguage side '' ' '' (DRFguarantee)is notinvestigated,butthe approachis \nneverthe\u00adless similar to ours. For further references about memory models, T= T [T0[.B0[q. v ]..B1 . \nq.T]. T1[.{p .. v}.E[()]]] a then we have D ' -. D. It remains to see that D is coherent. especially \nas regards performance issues, and particular hardware . T '' o '' models, we refer to[2,3].In the rest \nof this section webrie.ydis- Assume that p / o '' with o .o.Thenby Lemma 5.7 there cuss some works that \nare relatedto ours,though not alwaysdealing exists i such that ai = wrp and p(o '' ) = p(oi), with aj \n. = . l explicitlywithrelaxed memorymodels. for i<j . n if p(oi) = p(oj), but, as above, this contradicts \nProposition3.11. Still assuming o. o ' , let us consider the case where T = T ' [(.B.T ' . T1)]with o \n=@T ' \u00b7 . \u00b7@T1 and ''' ' D =(S,L,T [.{q .. v }.(.B . q.T . T1[E[r]])]) ' @T ' and o = \u00b7 .. Since q . \nW(T/ o ' ), we can show, using as in the previous case Lemma 5.7 and Proposition 3.11, that q .p = (since \notherwise this would contradict the assumption that C is DRF).Then weletin this case D =(S,L,T '' )where \nT '' ' ' = T [.{q .. v }.(.B . q.T ' . T1[.{p .. v}.E[()]])] a WehaveD -. Dwhere u =@T ' \u00b7.\u00b7 . \u00b7@T1, \nandwe conclude as u intheprevious case.Allthe other cases(witho. o ' or o ' = o)are easy.Asindicated \nabove, we conclude theproof of theProposition in the case where r =(p := v)using the induction hypothesis \nregarding D ' . r =(!p).Wehave C ' =(S,L,T[E[v]])where v =(S,T)(p). We only examine the case where T \n= T ' [(.B.T ' . T0)] with o =@T ' \u00b7 . \u00b7@T0, o ' =@T ' \u00b7 . and ''' ' D =(S,L,T [.{q .. v }.(.B . q.T \n. T0[E[r]])] (all the other cases are easy). Assume that q = p. Then p . W(T/ o ' ), and therefore by \nLemma 5.7 there exists i such that ai = wrp with p(o ' ) and i <j . n . aj .l, but = p(oi)= this contradictsProposition \n3.11, since p(oi).p(o)and ai #a. Then we musthave q .pin this case, anditis easy to see that = we thenhave \n(S,T '' )(p)= v =(S,T)(p)where A line of work which is related to ours, as regards the method we use \nand the kind of result we get, is the one on software transactional memory (STM). Several recent papers \n[1, 16, 30] studythistopicfrom an operationalpoint ofview,de.ninga strong and weak semantics for a high-level \nlanguage, and establishing a correctness resultfor aparticular class ofprograms.(The notions ofweakandstrongtransactions, \nor recoverable andnon-interfering atomic actions, also appearin[9,10].)So one can seethatthese works \nare similar to ours in spirit. Furthermore, since they are focusing on synchronization problems arising \nin STM, which is a sort of memorymodel,it wouldbeinterestingto see whether more formal connections could \nbe established. Some research in that directionhasbeeninitiatedin[22],where some oftheissues arising \nfrom the interaction between atomic blocks and relaxed memory models, and more speci.callytheJMM[29], \nareinvestigated. The paper [32] proposes a theory of memory models. Al\u00adthough it seems to us that this \nwork is more concerned with com\u00adpiler optimizationsthan with memory modelsproper(thetwo are quite oftenmixed),itproposes \nan approach similartoours, where allowedtransformations are speci.ed at the syntacticlevel.ADRF guarantee,therecalled \nthefundamentalproperty, isshown,fora class of programs restricted to steps, that is simultaneous asign\u00adments \nsimilar to p := e, on which transformations are de.ned (whereas our model deals withmemory operations \np := v). This work adopts an axiomatic stylehowever, where thepossible relax\u00adations of the strong semantics \nare determined a priori, relying on partial orders representing dependencies, and not by considering \na weak semantics. This sounds quite appropriate as regards the kind of optimizations that the authors \nconsider, and it would be worth investigating whether the two approaches could be combined. In a similar \nvein,let us mention the work[23],the overallaim ofwhich '' '' ' isto establishandend-to-endapproach \nto concurrentprogramming in C-. By this is meant a programming style where properties of T = T [.{q .. \nv }.(.B . q.T . T0)] @T '' Therefore if we let u = and D =(S,L,T '' [E[v]]) we programs are proved by \nmeans of separation logic, and where the a have D ' -. D and p(u)= p(o).ByLemma 5.2 D ' iscoherent, \nsemanticsunderconsideration(asregardslogicalpropertiesinpar\u00adu ticular)includes, or morepreciselyisintended \ntoinclude compiler hence soisD.We concludetheproof, as above, usingtheinduction optimizations for sequential \ncode. It seems to us that the use of hypothesisfor D ' . separation logic is a way to deal only with \nwell-synchronized pro-Asanobviousconsequenceof thepropositions5.6 and5.8(and of grams,and thereforethataproperty \nsimilartotheDRFguarantee Corollary5.4),we.nallyobtainthecorrectnessresult: shouldholdinthiscasetoo. 7. \nConclusionandFutureWork We have proposed a new approach to relaxed memory models, by formalizing such \na model by means of a weak operational seman\u00adtics.Thisallowed ustoprovethecorrectness of theweak memory \nmodel for data-race free programs. There are several directions in which this work could be extended. \nObserving that our model is less abstract than the ones that use partial orders of events, and more abstract \nthanparticular hardware architectures, we think two differentdirectionscouldbeexplored: .rst,wecould \ntry toextract a more abstract weak semantics from the operational presentation, using the same true-concurrency \ntechniques that we used for the strong case.Indeed,in[12] webuilt an event structure semantics (for CCS)in \nthat way. This wouldallow us to compare our model with others presented using partial orders, like [14, \n29, 34], and to see which con.gurations, axiomatically prescribed in a partial order approach, are allowed \nor not. Second, in the opposite direc\u00adtion, we couldtryto make our model closerto realhardwareimple\u00admentations. \nFor instance, changing our model to deal with buffers made of sequences of writes (p,v), thusforbidding \nthe W.W re\u00adlaxation, it seems that we get a model which is very close to the INTEL 64 architecture[24]. \nWe think it is easy to extend our model with some other syn\u00adchronization mechanisms, like volatile variables \nfor instance: this would mean having another way of creating references, vref e, which in the weak semantics \ncreates a strong reference, with atomic writes, that is, no write buffering. A more ambitious goal wouldbe,asindicatedintheprevious \nsection,todeal with atomic transactions. We could also model the prefetching of reads by al\u00adlowing the \nvalue stored at some pointer to be propagated down\u00adwardsinto the readbuffers, or simulatethebehaviour \nof a cacheby putting a value read from the store in a buffer close to the thread that issued the read \noperation. We think the correctness result still holdsin these cases,but theproofhas tobe adapted.Finally, \nas we suggested in the previous section, it would be interesting to inte\u00adgrate compiler optimizationsin \nour approach. References [1] M. ABADI, A. BIRRELL, T. HARRIS, M. ISARD, Semantics of transactional memory \nand automatic mutual exclusion, POPL 08 (2008)63-74. [2] S.V.ADVE,DesigningMemoryConsistencyModelsforShared-MemoryMultiprocessors, \nPhDThesis,Univ. ofWisconsin(1993). [3] S.A.ADVE,K.GHARACHORLOO,Shared memory consistency models: a tutorial, \nIEEEComputerVol.29No.12(1996)66-76. [4] S.ADVE,M.D.HILL,Weak ordering Anewde.nition, ISCA 90 (1990)2-14. \n[5] D.ASPINALL,J. S.EVC.\u00b4IK,FormalisingJava sdata racefree guarantee, TPHOLs 07,LectureNotesinComput.Sci.4732(2007) \n22-37. [6] D.ASPINALL,J. S.EVC.\u00b4IK,Javamemorymodel examples:good,bad and ugly, VAMP 07(2007). [7] G.BERRY,J.-J.L \n\u00b4 EVY, Minimal and optimal computations of recursiveprograms, J. ofACM26(1979)148-175. [8] H.-J.BOEHM,S.V.ADVE,FoundationsoftheC++concurrency \nmodel, PLDI 08(2008)68-78. [9] C.BLUNDELL,E.C.LEWIS,M.M.K.MARTIN,Subtletiesoftrans\u00adactional memory atomicity \nsemantics, IEEEComput.Architecture LettersVol.5No.2(2006). [10] G. BOUDOL, Atomic actions, INRIA Res. \nRep. 1026 and EATCS Bull.38(1989)136-144. [11] G. BOUDOL, I. CASTELLANI, A non-interleaving semantics \nfor CCS based on proved transitions, Fundamenta Informaticae XI (1988)433-452. [12] G. BOUDOL, I. CASTELLANI, \nFlow models of distributed com\u00adputations: three equivalent semantics for CCS, Information and Computation \nVol.114No.2(1994)247-314. [13] P.CENCIARELLI,A.KNAPP,B.REUS,M.WIRSING,Anevent\u00adbased structural operational \nsemantics of multi-threaded Java, in Formal Syntax and Semantics of JAVA, Lecture Notes in Comput. Sci.1523(1999)157-200. \n[14] P.CENCIARELLI,A.KNAPP,E.SIBILIO,TheJavamemorymodel: operationally, denotationally, axiomatically, \nESOP 07, Lecture NotesinComput.Sci.4421(2007)331-346. [15] M.DUBOIS,CH.SCHEURICH,F.BRIGGS,Memory accessbuffer\u00adingin \nmultiprocessors, ISCA 86(1986)434-442. [16] L. EFFINGER-D EAN, M. KEHRT, D. GROSSMAN, Transactional eventsforML, \nICFP 08(2008)103-114. [17] G.R.GAO,V.SARKAR,Locationconsistency anewmemory model and cache consistencyprotocol, \nIEEETrans. onComputers Vol.49No.8(2000)798-813. [18] G.R.GAO,V.SARKAR,Ontheimportanceof anend-to-end \nview of memory consistency in future computer systems, ISHPC 97, LectureNotesinComput.Sci.1336(1997)30-41. \n[19] K. GHARACHORLOO, D. LENOSKI, J. LAUDON, P. GIBBONS, A.GUPTA,J.HENNESSY,Memory consistency and event \nordering in scalable shared-memory multiprocessors, ACM SIGARCH ComputerArchitecture NewsVol.18No.3a(1990)15-26. \n[20] P.B.GIBBONS,M.MERRITT, K.GHARACHORLOO,Proving sequential consistency ofhigh-performance shared memories, \nACM Symp.onParallelAlgorithms andArchitectures(1991)292-303. [21] J.R.GOODMAN,Cache consistency and sequential \nconsistency, Techn.Rep.TR1006,University ofWisconsin(1991). [22] D. GROSSMAN, J. MANSON, W. PUGH, What \ndo high-level memory models meanfor transactions?, MSPC 06(2006)62-69. [23] A.HOBOR,A.W.APPEL,F.ZAPPA \nNARDELLI,Oraclesemantics for concurrentseparationlogic, ESOP 08,LectureNotesinComput. Sci.4960(2008)353-360. \n[24] INTEL CORP., Intel64 architecturememory ordering whitepaper, (2007). [25] M. HUISMAN, G. PETRI, \nThe Java memory model: a formal explanation, VAMP 07(2007). [26] L. LAMPORT, Time, clocks, and the ordering \nof events in a distributed system, CACMVol.21No.7(1978)558-565. [27] L. LAMPORT, How to make a multiprocessor \ncomputer that cor\u00adrectly executes multiprocessprograms, IEEETrans.onComputers Vol.28No.9(1979)690-691. \n[28] J.-J.L \u00b4 EVY, Optimal reductions in the lambda calculus, in To H.B.Curry:Essays onCombinatoryLogic,LambdaCalculus \nand Formalism(J.P.Seldin, J.R.Hindley, Eds),Academic Press(1980) 159-191. [29] J.MANSON,W.PUGH,S.A.ADVE,TheJava \nmemory model, POPL 05(2005)378-391. [30] K.F.MOORE,D.GROSSMAN,High-level small-step operational semanticsfor \ntransactions, POPL 08(2008)51-62. [31] J.C.REYNOLDS,Toward agrainlesssemanticsforshared-variable concurrency, \nFST-TCS 04, Lecture Notes in Comput. Sci. 3328 (2004)35-48. [32] V.SARASWAT,R.JAGADEESAN,M.MICHAEL,C.vonPRAUN,A \ntheory of memory models, PPOPP 07(2007)161-172. [33] X. SHEN, ARVIND, L. RUDOLPH, Commit-reconcile &#38; \nfences (CRF): a new memorymodel for architects and compiler writers, ISCA 99(1999)150-161. [34] R.C.STEINKE,G.J.NUTT,Auni.edtheory \nof shared memory consistency, JACMVol.51No.5(2004)800-849. \n\t\t\t", "proc_id": "1480881", "abstract": "<p>Memory models define an interface between programs written in some language and their implementation, determining which behaviour the memory (and thus a program) is allowed to have in a given model. A minimal guarantee memory models should provide to the programmer is that well-synchronized, that is, data-race free code has a standard semantics. Traditionally, memory models are defined axiomatically, setting constraints on the order in which memory operations are allowed to occur, and the programming language semantics is implicit as determining some of these constraints. In this work we propose a new approach to formalizing a memory model in which the model itself is part of a weak operational semantics for a (possibly concurrent) programming language. We formalize in this way a model that allows write operations to the store to be buffered. This enables us to derive the ordering constraints from the weak semantics of programs, and to prove, at the programming language level, that the weak semantics implements the usual interleaving semantics for data-race free programs, hence in particular that it implements the usual semantics for sequential code.</p>", "authors": [{"name": "G&#233;rard Boudol", "author_profile_id": "81100062958", "affiliation": "INRIA Sophia Antipolis, Sophia Antipolis, France", "person_id": "P1301026", "email_address": "", "orcid_id": ""}, {"name": "Gustavo Petri", "author_profile_id": "81392617680", "affiliation": "INRIA Sophia Antipolis, Sophia Antipolis, France", "person_id": "P1301027", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1480881.1480930", "year": "2009", "article_id": "1480930", "conference": "POPL", "title": "Relaxed memory models: an operational approach", "url": "http://dl.acm.org/citation.cfm?id=1480930"}