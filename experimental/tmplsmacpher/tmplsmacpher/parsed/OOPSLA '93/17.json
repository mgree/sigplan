{"article_publication_date": "04-01-1993", "fulltext": "\n Addendum to the Proceedings OOPSLA Washington, DC-26 September-7 October, Open Distributed Processing \nModerator: Oscar Nierstrasz University of Geneva Panelists: Alan Snyder-SunSoft, Inc. Antony S. Williams-Microsoft \nCorporation William Cook-Apple Computer Open Systems for Software: An Object-Oriented Solution Alan \nSnyder--SunSof, Inc. The computer industry is facing a software crisis. To remain competitive, organizations \nneed to continually improve the computer solutions they use to run the enterprise. However, constructing \nand maintaining enterprise wide application systems running in a heterogeneous distributed environment \nis too expensive and time consuming, resulting in a large backlog of unfulfilled needs. One of the most \npromising approaches to resolving this software crisis is the idea of constructing software systems by \ninterconnecting software component products developed by a wide variety of commercial software vendors \nand in-house developers. This approach allows customers to better leverage the work of others, reducing \ndevelopment costs. It offers maximum freedom of choice to the customer, enabling more flexible and configurable \nenterprise solutions. It encourages competition among software developers to produce innovative components \nthat maximize the value to customers. It enables the rapid evolution of software systems to meet fast \nchanging business requirements. For this component-based approach to be successful, several requirements \nmust be satisfied. It must be possible to interconnect components developed independently, using different \nprogramming languages, running on different kinds of machines in a distributed environment. It must be \npossible to replace individual components, again where the new component might be implemented by a different \ndeveloper, using a different programming language, running on a different machine, possibly of a different \nkind than the original component. It must be possible to introduce or replace existing components with \nnew components that provide extended capabilities without breaking existing software. Adding or replacing \ncomponents must be possible without stopping the system. It should be  1993 Panel Session possible \nto validate component interconnections for compatibility based on component attributes. Components should \nbe isolated from each other so that software errors can be confined and tracked to a particular component. \nIt must be possible to incorporate existing legacy applications with minimal or no modification. These \ncapabilities should apply not just to traditional application programs, but also to smaller functional \nunits, such as a file selection dialog, and even to basic system components, such as file systems and \nvirtual memory managers. At a higher level, it must be possible to interconnect data, not just programs. \nFor example, it must be possible to connect a specific invoice with a specific customer record. It must \nbe easy for system administrators and users to make interconnections and reconfigure components. For \nexample, it should be possible to move a collection of data to a different machine to improve performance \nwithout changing the other parts of the system that use the data. At SunSoft, we are developing an object-oriented \napplication framework to support this component based model of software, based on the concept of distributed \nobjects that can be located anywhere on a network to provide services. Objects have several characteristics \nthat make them a natural choice for supporting application interoperability and configurability in a \ndistributed, heterogeneous environment. The most important characteristic is abstraction or encapsulation. \nAn object possesses a well-defined interface. We define object interfaces using an interface definition \nlanguage, defined in the Object Management Group s CORBA specification. The interfaces defined in this \nlanguage serve as a formal contract between the clients that use objects and the software components \nthat implement the objects. This contract allows clients and components to be developed independently \nwith plausible assurance that they will work together. Based on declarations provided by clients and \nobject implementations, the interface conformance of a Addendum to the Proceedings OOPSLA 93 connection \ncan be checked when the connection is made. The current OMG interface definition language captures only \ntype signatures; we are working on extensions to capture behavioral specifications. Our object interfaces \nare pure operational interfaces. Operational interfaces allow the client of an object to be fully decoupled \nfrom the implementation of the object. The client uses the same invocation to interact with objects regardless \nof their location, be it in the same address space as the client or on another machine on the network. \nProcesses are started and dynamic libraries are linked automatically as needed without client involvement. \nThe OMG interface definition language uses language- independent abstract datatypes. Thus, clients and \nobjects can be written in different programming languages; the choice of programming language is not \nrevealed through the interface. We map an interface into the client or component s programming language \nusing a programming-language specific mapping. Thus, for example, a C++ client uses objects as if they \nwere C++ objects, and a C++ component developer implements objects by writing C++ classes. Another important \ncharacteristic is polymorphism, or the ability of a single client to use many different objects that \nhave different realizations but support the same interface. Because of this ability, customers can use \ndifferent implementations for different purposes or replace implementations incrementally in a large \nsystem. We have designed a set of standard interfaces for basic services. We expect a variety of components \nto be developed that implement these interfaces and provide differing levels of quality of service (differing \nin performance, reliability, and cost, for example). System integrators and customers can select components \nbased on the quality of service most appropriate to their needs. The OMG interface definition language \nsupports interface inheritance. Interface inheritance defines conformance relations between interfaces: \nif interface B inherits from interface A, objects that support B are compatible with clients that expect \ninterface A. Interface inheritance increases the flexibility of the system, without sacrificing the ability \nto validate connections. For example, if a component supporting interface A is replaced by a component \nsupporting interface B (containing additional operations), objects created by the new component can be \nused successfully by old clients (who were developed against interface A) as well as by new clients (who \nwere modified to take advantage of the new features in interface B). Our object technology uses persistent \nobject references to support reliable connections. Object references can be passed around the network \nand saved in persistent storage. An object reference retains its meaning to the client, even if the object \nimplementation is changed (because a component was replaced) or the object is relocated elsewhere in \nthe network. Object references are typed; clients can determine at run time whether an object reference \nsupports the interface the client expects. Object references can be used to access services or data. \nOur object technology is scalable. It supports relatively lightweight objects (the size of a C struct) \nall the way to large objects (the size of a file or a database). It supports simple single-threaded applications \nas well as multithreaded applications that are designed for concurrent use by multiple collaborating \nusers. It supports efficient access by clients in the same address space as well as reliable access across \nthe network. Our goal is to provide a uniform model for components at many levels of abstraction, from \noperating system components to large application programs. The key to supporting scalability is to design \nthe architecture to permit a wide variety of implementation technologies to be used. To retain this flexibility, \nwe avoid placing constraints on objects (such as universal object identifiers or universal object equality) \nthat excessively restrict possible implementations. In addition to the basic object model described above, \nwe define several standard interfaces to permit generic interconnection of objects. By supporting these \nstandard interfaces, components can be integrated with a wide variety of other components using standard \ntools and standard user interface actions such as drag and drop. We also provide implementations of key \nservices that we expect most applications to require. These implementations conform to standard interfaces, \nso that alternative components can be substituted or added easily. We are working with the Object Management \nGroup and other large system vendors (Hewlett-Packard and IBM) to achieve widespread adoption of a common \nobject model and common object interfaces. When these services become commonplace, the foundation will \nexist for component-based development of enterprise applications in heterogeneous distributed environments. \n The OLE 2.0 Object Model Antony S. Williams-Microsofi Corporation Microsoft s Object Linking and Embedding \n(OLE) version 2.0 is a system for &#38;;-phcation integration built on a distributed object model. In \nthis paper I discuss some of the distinguishing characteristics of the OLE architecture that are important \nfor achieving integration of separately evolving components. Background OLE enables integration between \nseparately developed applications, without programming. End users can combine objects created by different \napplications to construct custom composite objects in v,?rious user interface paradigms such as compound \ndocuments. With programming, custom applications can be built using other applications as components. \nOLE defines and implements an object model, a set of generic interfaces, and a set of services. Together \nwith services (objects) implemented by application vendors, this constitutes an environment of integrated \nbut modular software. OLE version 1 .O explored the compound document domain, and proved the viability \nof binding together executable code from different suppliers, giving early experience of real reuse of \n(coarse-grain) components. OLE version 2.0 introduces a distributed object model and supporting services. \nIn the domain of compound documents, OLE also defines some domain-specific interfaces, a storage Washington, \nD.C. September 26-October 1,1993 service oriented towards structured documents, and user interface style \nguidelines. These will not be discussed in this paper.  OLE Distinguishing features Since objects invoke \noperations on each other in an unrestricted fashion, it is not useful to characterize some objects as \nservers and others as clients. In OLE, client and server are roles that components play in particular \nrelationships and at particular moments. Component Object Model OLE defines a very precise notion of \nan interface, through which clients access the functionality of an object. An interface specifies the \ncontract between the client and the object. An interface formally specifies the names and signatures \nof the methods in the interface, and informally though precisely specifies the rules that clients and \nobject implementors must follow in order to conform to the contract. This model of interfaces is designed \nto separate the definition of the contract from the mechanisms by which objects implement their methods. \nThis hides implementation detail, and supports multiple providers of a given interface as well as multiple \nclients. In C++ an interface is an abstract class, with no data members and only pure virtual member \nfunctions. OLE interfaces have very little intrinsic overhead. Interfaces are suitable for fine-grain \ncomponents as well as coarse; the same paradigm is used for objects which can range in scale from push \nbuttons to multimedia publications or whole libraries. An interface is manifested in code as a (runtime) \ninterface identifier, through which clients and objects validate that they are implemented to the same \ncontract, and a binary standard calling convention which supports runtime binding of independently supplied \ncomponents. An interface pointer is a typed pointer through which methods in that interface can be called \non an object. OLE defines a model of a component as an object that is accessed through one or more interfaces. \nAll object references are interface pointers. Clients can instantiate objects with a simple call giving \na class identifier, or bind to existing objects by name. In both cases clients specify (with an interface \nidentifier) the desired interface. OLE takes care of locating the appropriate implementation code (dynamically \nloading in-process if required), establishing RPC connections etc., and returns an appropriate interface \npointer back to the client. Additionally, clients can obtain interface pointers by calling methods on \ninterface pointers they already have. This is discussed in the RPC section, below. Every interface includes \na method called QueryInterface through which a client can gain access to other interfaces on the same \nobject. QueryInterface is the negotiation/validation mechanism which allows objects to support multiple \ninterfaces, and removes the requirement for a class hierarchy of implementations. Clients can manage \npolymorphic collections of diverse objects, by referring to objects through an interface that they all \nimplement. QueryInterface allows interfaces to be added over time, so that clients and objects can increase \nin richness of function and integration, while preserving compatibility with older software. New function \nis exposed through new interfaces. Clients that can use the new function will query for the new interface. \nOlder objects will respond negatively to the query, and clients can fall back to old behavior. New objects \ncan implement both old and new interfaces, and satisfy both old and new clients. In many cases, the new \ninterface is a pure extension of the old, in which case the object implementor can share the state and \nimplementation that is common, and simply needs to check (in QueryInterface) for both interface identifiers. \nThe OLE Component Object Model also defines an optional technique for implementation reuse. A component \ncan instantiate another (sub-)component as a part of its implementation, and expose the interfaces of \nthe sub-component to external clients. This technique is called aggregation. Aggregation is completely \nan implementation choice and is opaque to external clients. This model of components is leveraged throughout \nthe OLE 2.0 implementation. All of the services and pieces of OLE 2.0 are components. For example, a \ncompound document container application dealing with a contained object is in fact calling an aggregate \ncomponent that has a presentation cache component, RPC components, etc. The component object model support \nof dynamic loading of components means that the OLE implementation is itself extensible at many points. \n Pseudo-objects Since clients only see interfaces, functionality may be exposed as objects with interfaces \neven if the implementation is not structured as objects. A spreadsheet application or a database may \nexpose some data as a table of values, even though the table may not exist as a separate dam structure. \nIn the case of a spreadsheet, the dam may be buried within a larger sheet, with formulae etc. in the \ncells. In a database, the table may not exist at all, being a virtual table that is constructed piecemeal \non demand. It is the complete separation of interface from implementation that permits this emulation \nof object- ness. It means that interesting and useful existing software need not be re-architected to \nbe integrated in the OLE model. Further, it means that there is no reason to lose performance of implementation, \nas would happen if (say) each cell in a spreadsheet were required to be implemented as a separate component. \n Remote Procedure Call The implementation of RPC in OLE is itself unremarkable. The currently released \nversion uses a local-machine only RPC system built on native inter- process communication mechanisms. \nThe distributed version uses the Open Software Foundation s DCE RPC. OLE RPC is integrated into the architecture \nin a way that gives an unprecedented degree of local/remote transparency. Clients really see no difference \nwhether the object is in-process, in another process on the local machine, or on a remote machine. The \nbinding calls hide all of the implementation details. Since OLE allows for interfaces to be added over \ntime, the RPC marshalling architecture has been made extensible. Marshallers for interfaces are just \ncomponents, and the Component Addendum to the Proceedings OOPSLA 93 Object Model provides dynamic loading \nof marshallers for new interfaces OLE RPC supports an explicit notion of destination contexts, which \nallows marshallers to ask certain questions about the destination for which data is being marshalled. \nSpecifically, OLE uses this to determine whether memory can be shared with the destination (as it can \nin a single machine). This permits exploitation of sharing where possible, for greater performance. Given \nthat marshallers are components, smart marshallers can be written and installed to exploit this in other \nways. Object references (interface pointers) are first class types; they can be transferred as parameters \nthrough method invocation (both input and output). Sending an interface pointer through RPC automatically \nsets up the RPC connection required for the destination to use the pointer. Marshalling interface pointers \npreserves identity. Smart marshalling removes redundant levels of indirection, so that the destination \nhas a direct connection to the object. In the (not so unusual) case that the pointer refers to an object \nin the address space of the destination, the resulting connection is a direct binding, with no RPC connections \ninvolved at all. In contrast to other distributed systems, which require binding to obtain an interface \npointer, OLE uses interface pointer marshalling to encapsulate low level binding.  Naming In a conventional \nsystem, names are the parameters for functions or methods that obtain access to objects, for example, \nopening a file. OLE deals with objects, and so names are used to obtain interface pointers to objects. \nOLE deals with many containment domains (file systems, databases, documents, etc.) and so deals with \nfederating namespaces that have different syntax and access methods. OLE defines an encapsulation called \na moniker that insulates clients from how an object is located, while providing access to the object \nonce bound. A moniker is a component object that is used where a name (e.g. a string) is used in a conventional \nsystem. A moniker does for naming what objects do for data: it provides a well-defined interfaces to \nits operations, encapsulates the representation, and hides the implementation details of binding. Monikers \nmay be composed and de-composed to support nested containment or sequential access paths, analogous to \npath names in a file system. Clients are unaffected whether a moniker is a composite or a single element. \nMonikers support persistence (they may be saved and re-loaded) and are used where persistent object references \nare needed. Each moniker encapsulates the type of container or access provider that it uses (either the \ncontainer with which this moniker is composed, or the environment such as a file system), the particular \nmethod to obtain access, and the parameters of the method call. Monikers provide the notion of reduction \n(evaluation to a canonical form), and binding (activating the object if necessary, and obtaining an interface \npointer). The interface to monikers therefore contains methods for . composition and decomposition . \nsaving and loading . reduction . biding . comparison and hashing As an example of use of monikers, \nconsider a spread sheet that is contained in a database, and which supports links to named ranges of \ndata within the sheet. The spreadsheet application is given the moniker for the spreadsheet (as well \nas access to the data). The data may actually be in a database of which the spreadsheet application is \nignorant. The spreadsheet application constructs a relative moniker to refer to a named range within \nthe sheet, and composes this moniker with that of the whole sheet. The resultant moniker can be given \nout to other applications, and used to locate and bind to the dam. Since monikers are components, new \nmoniker classes can be written and installed, to give access to new naming domains and object storage \nservices. For example, a new moniker type might encapsulate a query over a directory service to locate \nthe best source of a particular data service. This allows an application that supports data links to \nbe coupled to this more intelligent system without change to that application. Application Integration, \nnot Application Distribution William Cook-Apple Computer Discourse on the basic purpose of real-world \ndistributed computation is often lost amid discussions and comparisons of particular technologies and \nfeatures. What problem is a given system trying to solve? Who are its users and what scenarios characterize \ntheir interaction with the system? Unless we answer these questions, all the technology is without context. \nTo me the basic purpose is integration of applications . Applications are taken as the central building-blocks \nbecause they are the primary units of execution within existing systems. One should be able to integrate \napplications on a single machine, or across the network with equal ease. Applications are also the units \nof utility that users understand and purchase. It is these end users and their hired consultants who \nwill connect together the distributed components to fashion a system to satisfy their needs. This is \nvery different from another typical purpose: to distribute an application across a network. One difference \nis in the communication bandwidth: a distributed application typically needs very high real- time communication, \nwhile integrated applications can get by with much lower communication rates. Another difference is that \na distributed application is typically constructed by a software house and then deployed, while integrated \napplications are constructed for customers in the field. Washington, D.C. September 264ctober 1,1993 \nTo satisfy the need for application integration, Apple is shipping a range of tools: Apple Events: a \nhigh-level messaging system with application objects as targets and arbitrary data parameters. 0 Object \nModel: a structural object reference model with query capabilities. Standard Registry: suites of standard \nevents and object structures for use by all vendors. Terminology resources: mappings of application- \nspecific events and objects to human-readable form. Standard terminologies are supplied. AppleScript: \nan object-oriented scripting language capable of sending and receiving Apple Events defined in application \nterminologies. Open Scripting API: an API for compiling, loading/storing, and executing scripts. Scripting \nEnvironment: a simple script editor that can create double-clickable script applications. Other features, \nlike indirect naming services and broadcast messaging, are not included in the base system, but are being \nlayered on top by writing applications. The resulting system was released to developers this spring. \nIt combines the basic features of distributed computation, object-oriented programming, and database \nqueries into a synergistic package for integrating computational resources. One way to understand this \nviewpoint is to imagine every application as an object-oriented, query-driven command processor with \na common scripting language. Integration of applications into a system is done by writing scripts that \norchestrate the flow of work through an organization. Addendum to the Proceedings OOPSLA 93  \n\t\t\t", "proc_id": "260303", "abstract": "", "authors": [{"name": "Oscar Nierstrasz", "author_profile_id": "81100134506", "affiliation": "University of Geneva", "person_id": "PP39029114", "email_address": "", "orcid_id": ""}, {"name": "Alan Snyder", "author_profile_id": "81100167718", "affiliation": "SunSoft, Inc.", "person_id": "PP31030383", "email_address": "", "orcid_id": ""}, {"name": "Anthony S. Williams", "author_profile_id": "81100005346", "affiliation": "Microsoft Corporation", "person_id": "P20582", "email_address": "", "orcid_id": ""}, {"name": "William Cook", "author_profile_id": "81406596033", "affiliation": "Apple Computer", "person_id": "PP39027612", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/260303.260322", "year": "1993", "article_id": "260322", "conference": "OOPSLA", "title": "Open distributed processing (panel)", "url": "http://dl.acm.org/citation.cfm?id=260322"}