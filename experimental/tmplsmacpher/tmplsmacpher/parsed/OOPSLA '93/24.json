{"article_publication_date": "04-01-1993", "fulltext": "\n Addendum to the Proceedings OOPSLA Washington, DC-26 September-I October, 1993 Workshop Processes and \nMetrics for Object-Oriented Software Development Organized By: Steven C. Bilow, Tektronix, Inc. Doug \nLea, SUNY Oswego &#38; NY CASE Center Karl Freburger, Knowledge Systems Corporation Dennis de Champeaux, \nRational Report By: Steven C. Bilow, Tektronix, Inc. Doug Lea, SUNY Oswego &#38; NY CASE Center Introduction \n&#38; Workshop Overview On Sunday, 26 September, 1993 the OOPSLA 93 Workshop on Processes and Metrics \nfor Object Oriented Software Development was held in Washington DC. The workshop was structured in 3 \nsections consisting of specific, focused activities. Those sections were; (1) A series of formal paper \npresentations. (2) Four smaller special topic discussion groups facilitated by the workshop organizers. \n(3) A consensus building process, group discussion, and construction of a position statement. Summaries \nof these activities will constitute the body of this report.  Formal Presentations The first session \nof the day began with a brief introduction by Steve Bilow, an overview of the days activities by Doug \nLea, and 1 minute participant introductions. This was followed by the presentation of 8 position papers. \nThose papers were selected by the workshop organizers, from the body of 24 papers submitted, as the most \ninteresting, thought provoking, or universally relevant. The following were the papers selected. Sh am \nChidamber and Chris Kemerer, MI 7 Sloan School MOOSE: Metrics for Object Oriented Soff ware Engineering \nShyam Chidamber represented himself and Chris Kemerer in a presentation of the current status of the \nMOOSE metrics. The Chidamber and Kemerer metrics have generated a significant amount of interest and \nare currently the most well known suite of measurements for O-O software. In Shyam s quest to validate \nhis metrics he has spent 3 months interviewing software designers and several month collecting empirical \ndata from both C++ and Smalltalk projects. His principle points are that metrics must be theoretically \nrigorous and practically relevant. Toward those goals, the MOOSE metrics are beginning to show strong \nempirical validity. In response to Kenny Rubin s question about how the numbers resulting from the metrics \nshould/can be used, Shyam proposed calibrating the data against a large body of typical cases and then \nusing them to predict maintainability, effort, reuse, etc. One member of the group asked whether Shyam \nhad a feel for the relative quality of Smalltalk versus C++ and he provided the only rational answer \nthat you can t compare apples and oranges. One of the most important things we can learn from the work \nof Chris and Shyam is that it is extremely difficult to collect a body of empirical data large enough \nto rationally validate a set of measures. Sallie Henry and Mark Lettanzi, Virginia Tech. Measurement \nof Software Maintenance and  Reliability in the Object Oriented Paradigm Among the most interesting \nwork in the field of O-O metrics is that of Dr. Sallie Henry and her students. For this workshop, doctoral \nstudent Mark Lettanzi presented the status of his work on the prediction of maintenance and reliability. \nThe paper describes the results of three studies. One, a study of maintenance difficulty in procedural \nversus O-O software. Two, a relationship between reuse, productivity, and O-O techniques. Three, a modification \nand application of the MOOSE (C&#38;K) metrics for the purpose of predicting maintainability. Mark openly \nshared his work and proposed that there is far too little evidence to support the O-O industries claim \nof increased maintainability. His research effort, however, does support that claim even if not through \nempirically validated metrics. Addendum to the Proceedings OOPSLA 93 Mark Lorenz and Jeff Kidd, Hatteras \nSoftware O-O Metrics Positioy Paper Mark Lorenz and Jeff Kidd came to the workshop with metrics taken \nfrom their up and coming book on the subject. They made two very important points. First, they make a \nlarge distinction between what they term Project Metrics and Design Metrics. Project metrics include \nsuch items as schedule, staffing estimating, and nearness to completion. Design metrics, on the other \nhand include such items as method size, class size, use of inheritance, cohesion, etc. He showed a number \nof charts that illustrated distributions of some of his measures across several projects. He did not \nspeak to the relation of his proposed metrics to formal mathematics, but, his body of data is most interesting \nbecause it come from empirical data rather than theory. More important than the presentation of Mark \ns metrics was his statement that metrics should not drive design but, rather, should be used to pinpoint \nanomalies. Some may disagree with this premise but it does remind us of how easy it is to misuse measures. \nKathy Reinold, Bull HN Information Systems Processes and Metrics for Object-Oriented Software Development \nKathy Reinold brought to us the perspective of a software manager tasked with moving from a procedural \nworld to an object oriented one. Her paper dealt primarily with our old familiar friend-the line of code. \nWhile many of us shy away from KLOC as a measure of software, Kathy brought use the perspective of one \nforced to use what is available. She spoke of KLOC and function points throughout her talk as well as \nwhat she called object counting which the group decided was actually class counting. She presented charts \nillustrating her contention that Function Point Analysis and object counting have the ability to product \nreliable KLOC estimates, even in O-O projects. Linda Rising, Honeywell, Inc. An information Hiding Metric \nAnother quite interesting bit of work came from Linda Rising at HoneywelI. She has been working on measures \nof information hiding in Object-Based languages and brought the ADA perspective to the group. Unlike \nthe work of Lorenz and Kidd or Reinold, Linda s metric has its foundation in measurement theory, not \npractical application. She proposed two basic premises. One, that a module having more that one design \ndecision is poorer at information hiding than one containing only one design decision ( She did not, \nunfortunately, have time to define design decision ). TWO, the more entities that exist but are not required \nto implement the design decision the poorer the information hiding. Linda s work has been validated theoretically, \nsubjectively, experimentally, and in a case study of a large Ada program. Irene Brooks, Texas Instruments \nObject Oriented Metrics Collection and Evaluation with a Software Process Irene Brooks presented a paper \nbased on practical experience at Texas Instruments. She explained how TI has found measures of size, \ndefect density, and defect intensity to be very useful in the schedule management and defect control. \nShe commented that some traditional metrics like McCabe complexity measure do not appear to be useful \nfor O-O development. She also noted that it seems necessary to find adequate measures for polymorphism, \ninheritance, and the cohesion between class attributes. Her primary goals are repeatability of process, \nquality measurement, project management and configuration control techniques. Jerry Hamilton, McDonnell \nDouglas Metrics: Where Things Went Wrong Perhaps the most deeply revealing story of the day came from \nJerry Hamilton who summed up the misuse of metrics through practical example. Jerry s is the story of \nlast minute metrics and the result that unsubstantiated metrics can have on a project. The project began \nwith no metrics and only later did the management and customer decide that they need some measurements \nof progress. The project team decided to report, to the customer four categories of information. (1) \nthe number of classes defined, coded, and integrated, (2) the number of services designed, coded, and \nintegrated, (3) the number of classes remaining, and (4) the number of services remaining. As the project \nprogressed the class count climbed. This may have been due to many causes such as refactoring and re- \narchitecture but the customers perception was that the project was out of control. This resulted in management \ndeclaring NO MORE CLASSES. The rest of the story is painfully obvious.  Engineerin 00 Design Methods \ninto Repeatable % esign Processes The final formal presentation was by Xiping Song. His was the most \nprocess oriented paper to be accepted for presentation. Song s work is devoted to process repeatability. \nHis goal is to quantify existing design methodologies and generalize them into a set of processes, each \nof which is specific to a certain class of project, and which may be used consistently, and repeatably \nacross these projects. The work is rooted in what Leon Osterweil calls Process Programming. In other \nwords, a software process should be designed, coded, and executed in a manner quite similar to software \nitself. To apply this technique to 00 methods, Song began with an architecture based on the work of Booth. \nHe derived, partially from Booth s design examples, 7 variables according to which an 00 method could \nbe tailored for a given project. He tailored the Booth process into 2 different processes and developed \na supporting tool that allows the user to select the process according to a set of variables.  Discussion \nGroups Following the formal presentation of papers the group broke into 3 smaller groups each with a \nspecific focus. Steve Bilow led the group on Metrics and Management, Doug Lea led the Quality Metrics \ngroup, and Dennis de Champeaux s group discussed Processes and Micro-Processes. Due to space considerations \nthese sessions will not be described in detail. Summaries of these sessions may be obtained from the \nrespective workshop co-ordinator. Group Position Statement The final session was devoted to construction \nof lists summarizing the state of affairs in 00 process and metrics research and practice. The goal of \nthis list making process was to obtain some measure of, consensus from a very diverse group of 30 people. \nThis was probably the most important part of the workshop and, though the resulting lists are a bit overly \ngeneral, they do portray a group consensus relatively well. This process was facilitated by Doug Lea. \nThe first list is a set of metrics that are, without question or objection, valuable in 00 software development. \nUnsurprisingly, given the constraints of uniform applicability, they mainly consist of metrics useful \nin just about any engineering effort: Problem size and complexity Fraction of budget expended Fraction \nof functionality completed Amount of effort expended Schedule progress Number of iterations However, \nthree categories of 00-specific metrics were also listed: Interdependence and coupling among object (e.g., \ncounts of static and dynamic references) Class cohesion (e.g., measures of relatedness of methods within \na class) Effective use of inheritance (e.g., measures of relatedness and reuse in subclasses) In contrast, \nthe following list contains metrics that are of such dubious value and interpretation in OOSE, that they \nshould not be used, or used only with extreme caution: Lines of code written Lines of code reused Number \nof classes McCabe and Halsted complexity metrics In the middle ground lie categories of metrics that \nare considered important, but are in need of further development to be useful. (This list was composed \nby the Quality Metrics breakout group in a separate session.) Modularity Reusability Interference Operationalizations \nof customer requirements Customer views of quality Addendum to the Proceedings OOPSLA 93 Human factors \nProductivity and quality in non-linear models, participatory design methods, and other novel processes \n The group identified the following issues as the most pressing problems in current 00 development practices. \nLack of common lifecycle process definition Lack of corresponding management models and practices Inaccurate \nestimation Instability and unscalability of typical development processes Misunderstanding and misuse \nof 00 concepts and constructs by developers Lack of sufficient training and PR Lack of reusability, especially \nat the domain level Failure to use proven procedures such as design reviews The final list describes \nissues that are in most immediate need of further research to help solve practical development problems. \nThe relationship between easily measured quantities and desired results The relationship between measures \nof the size and complexity of the problem space and those of the solution Accounts of the essential differences \nbetween 00 and non-00 development Methods for accurately estimating development time and effort from \ninitial requirements Development of better macro-and micro process models Better theory of illness of \n00 programs that underlies quality metrics Better accounts of essential differences between 00 and non-00 \nmetrics Better ways to use metrics to control complexity Development of metrics and instrumentation that \nprogrammers find informative, not threatening Collection and evaluation of empirical dam of all sorts, \nespecially for metrics validation, development of norms, and assessment of the impact of reuse on productivity \nand quality  Conclusion In general, we believe that our workshop was both productive and interesting. \nThe combination of process and metrics was a mixed blessing. It increased the number of participants \nto the point where the effectiveness of participant interaction was strained. At the same time, it is \nvery important for process people and metricians to actively communicate since these 2 areas will continue \nto contribute much to each other. It appears that this workshop did quite a good job of promoting that \ninteraction. Participants Frank Armour Frank-Armour@mail.amsinc.com John Baker N/A Imran Bashir ibashir@cat.syr.edu \nSteve Bilow Steven. bilow @ tek. corn Irene Brooks elise@tasmania.works.ti.com Shyam Chidamber shyam@athena.mit.edu \nDennis de Champeaux ddc@rational.com Martin Fowler 100031.3311 @compuserve.com Karl Freburger freb@goldfuch.mv.com \nJerry Hamilton N/A Rachel Harrison rh@ecs.soton.ac.uk John Hogg hogg@ bnrca Fred Jewell N/A Cynthia Jones \nCynthia~Palladino~Jones@mail.amsinc.com Roger Kelly kelly@sde.mdso.vfge.com Jeff Kidd 71214.3120@compuserve.com \nRalph Kolewe rkolewe Beridani. corn Mark Lattanzi Eattanzi@cs.vt.edu Doug Lea dl @g. oswego. edu Mark \nLorenz 71214.3120@compuserve.com Marco Mulazzani M.Mulazzani@rcvie,co.at Jorgen Norberg norberg@vnet.ibm.com \nCarl Ponder oakhill!ponder@cs.utexas.edu Linda Rising rising%elhx.dnet@esu36.cfsat.honeywell.com Kenny \nRubin krubin Gparcplace. corn Radhika Seshan rads@houxa.att.com Xiping Song song@scr.siemens.com Todd \nStevens todds@cs.vt.edu Geree Streun streun@fischer.com David Tegarden tegarden@galllium.csusb.edu Laurel \nVon Gerichten laurel@goofy.att.com Kurt Welker wdk@inel.gov John Williams jwilliam@stsci.edu Russel Winder \nR. Winder@cs.ucl.ac.uk  \n\t\t\t", "proc_id": "260303", "abstract": "", "authors": [{"name": "Steven C. Bilow", "author_profile_id": "81100306733", "affiliation": "Tektronix, Inc.", "person_id": "P269670", "email_address": "", "orcid_id": ""}, {"name": "Doug Lea", "author_profile_id": "81100271749", "affiliation": "SUNY Oswego & NY CASE Center", "person_id": "PP48023830", "email_address": "", "orcid_id": ""}, {"name": "Karl Freburger", "author_profile_id": "81332499346", "affiliation": "Knowledge Systems Corporation", "person_id": "PP31031208", "email_address": "", "orcid_id": ""}, {"name": "Dennis de Champeaux", "author_profile_id": "81100265123", "affiliation": "Rational", "person_id": "PP31034638", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/260303.260337", "year": "1993", "article_id": "260337", "conference": "OOPSLA", "title": "Processes and metrics for object-oriented software development", "url": "http://dl.acm.org/citation.cfm?id=260337"}