{"article_publication_date": "04-01-1993", "fulltext": "\n OOPSLA  Washington, DC-26 September-I October, 1993 Experience Report 11 App/ying Software Testing \nPractices to an Object-Oriented Software Development Reported by Betty P. Chao Software Engineering Associates \nDonna M. Smith Los Alamos National Laboratory united in such a way that the fundamental logical Abstract \nbuilding blocks are classes and objects. Structured This paper describes the experiences in testing an \nprogramming uses procedural abstractions, whereas object-oriented software system. The testing activities \nobject abstractions are principal in object-oriented were integrated into a software quality assurance \nprogramming. Structured programming encapsulates program so that assurance practices such as inspections \nprocedures, whereas objects are encapsulated in object- were equally important as unit level and system \nlevel oriented programming. In light of the differences, can testings. The test approach employed object-based \nstandard testing practices be used to test object- testing with a focus on testing objects and their \noriented code? features. Results indicated areas such as test coverage Booth [3] has suggested that the \nuse of an object- and stopping criteria require further research in refining oriented paradigm does not \nchange any basic testing object-based testing. practices. Code inspections, unit testing, subsystem \ntesting, and integration testing all have useful roles in  Introduction object-oriented software testing. \nIn code inspections, Software testing is a mature area in software the focus is on the system s key abstractions \nand engineering that has evolved into an integrated, mechanisms, as embodied in classes and objects. \nUnit systematic discipline. The objective of software testing testing exercises not only the classes \nand objects but is to demonstrate that the software performs its the interactions among them. Subsystem \ntesting intended functions. While it is desirable to assure the focuses on testing the functionality \nof the entire correctness of software, testing, however, can merely subsystem. Finally, integration testing \nexercises the serve to demonstrate that the software is free from system-level tests. All throughout \nthe different phases bugs. In order to meet the objective of software of testing (from unit testing through \nsubsystem testing testing, systematic testing methodologies are required to integration testing), regression \ntesting also plays an for dividing the testing effort up into manageable pieces. important role whenever \nclasses and objects are added Numerous software testing guidelines and standards or modified. are in \nexistence, notably the IEEE Standard for The purpose of this paper is to describe the results of an Software \nTest Documentation [l] and the IEEE object-oriented testing effort within the framework of a Standard \nSoftware Unit Testing [2]. software quality assurance program. The software testing guidelines and standards \nwere developed in the midst of the structured programming Software Testing within a era. There are significant \ndifferences between structured Software Quality Assurance Program code versus object-oriented code. \nStructured programming encourages autonomy at the level of Although the quality of a software product \nis strongly procedures and discourages interaction through nonlocal dependent on the ability to test \nit, quality needs to be variables. In contrast, object-oriented programming built into the product. The \npurpose of a software quality discourages procedure autonomy and organizes assurance program is to assure \nthe quality of the procedures via collections of operations that share an deliverable software, the associated \ndocumentation, and object s state through their nonlocal variables. In the processes used to produce \nthe deliverable software. structured programming, the fundamental logical building As such, software \ntesting becomes an integral element blocks are algorithms with maximum use of global data. of the software \nquality assurance program. As shown in In object-oriented programming, data and operations are Figure \n1, testing practices such as document inspection Addendum to the Proceedings OOPSLA 93 and code inspection \nare equally important as unit testing and system testing. Inspections, such as the Fagan s inspection \nmethod [4], were used to detect errors in requirements, specifications, and software code. Requirements \nDesign Implementation Figure  Software System Description As depicted in Figure 2, the software system \nis composed of three software components that collectively allow the examination of issues related to \nthe movement and transformation of material as it moves through a processing facility. The software components \nare: 1. Object-oriented Discrete-Event Simulation- system which provides modeling of discrete- event \nand mixed discrete-continuous-event simulation.  2. Process Modeling System which is a simulation tool \nkit for moving and transforming material.  3. Application System which provides extensibility to the \nProcess Modeling System for specific needs such as generation of reports pertaining to material balances, \nequipment utilization, process dependencies, wastes, and resource utilization.  The software system \nis used on multiple platforms, including workstations, personal computers, and Macintoshes.The software \nsystem was developed using the Common Lisp Object System (CLOS). Inspections were also used to establish \nproperties such as maintainability, extensibility, and reusability of the code. Testing Pmct ices 1 \nI Object-Oriented Discrete Event Simulation-System (OW I Process Modeling Figure 2   Test Approach \nand Results The test approach is object-based; that is, the focus is on testing objects and their features. \nSpecifically, the testing,is aimed at CLOS constructs: defclass, defgeneric, defun, and defmethod. Table \n1 shows the number of CLOS constructs for each of the three software components. Washington, D.C. September \n264ctober 1,1993 ODS Process Application Modeling System System defclass cMy;neric defmethod lines of \ncode E 68 54 2200 1 54 12 172 1000 :, 26 1 1000 Table 1. Number of CLOS Constructs for each of the Three \nSoftware Components  ODS Tests. Since the ODS code provides the underlying simulation environment by \nwhich the other software components operated from, test coverage was deemed important. Comprehensive \ntest cases that included the classes and their functions/methods were exercised. Emphasis was also placed \non fault-based testing by exercising test cases that demonstrated the extent of a fault on not just the \nlocal object and its specific methods but the effect of the fault on the ODS as a whole. Another important \nODS testing consideration was the sequence by which the test cases were exercised. Utility modules that \nare required to load and close other modules were tested first. Then modules that contained the data \nstructure by which simulation events are processed were tested. Finally, simulation modules were tested. \n Lessons Learned from ODS Testing. The organization of the ODS modules and packages was an important \ndeterminant on test coverage. A considerable amount of time and effort was required to generate comprehensive \ntest cases because the partitioning of the software in different modules and packages was not obvious. \nA clear-cut software organization would help to expedite the generation of test cases. Fault-based testing \non object-oriented software appears to provide a clearer demonstration of the extent and breadth of the \nfault than procedural-based software. This is because the encapsulation and modularity properties of \nclasses allow faults to be readily identified and rectified. Whereas, faults in procedural-based software \nare notorious for detrimental effects on seemingly unrelated codes. Many of the ODS simulation modules \nprovide scaffolding to the Process Modeling System. Such scaffolds are difficult to test unless specific \nsoftware is written to exercise them. Rather than writing additional software, it was decided that successful \nexecution of the test cases for the Process Modeling System is sufficient to demonstrate the proper functioning \nof the scaffolds. Process Modeling System Tests. As can be seen in Table 1, the number of classes and \nfunctions/methods is considerable. Since it was infeasible to generate and execute test cases for all \ncombinations of classes and functions/methods, some classes were assumed to be independent of each other. \nFor example, the class, resources, is assumed to operate in the same fashion on all instances of the \nclass materials. Emphasis was also placed on testing those instances that were relevant to the application \nof interest, Hence, there were some instances that were deferred to later testing. Lessons Learned from \nProcess Modeling System Testing. The assumption of class independence appeared to be reasonable. A small \nset of test cases was exercised to demonstrate that resources operated in the same fashion on all instances \nof materials. Test coverage and stopping criteria were difficult to determine. For example, in moving \nparts through locations, one could generate a large set of test cases (i.e., moving 1 to n parts through \n1 to n locations). Frequently, the stopping criterion was dependent on the testing specialists previous \nexperiences on generating test cases. The scaffolds in the ODS simulation modules were assumed to function \nproperly by the successful execution of the Process Modeling System test cases. Application System Tests. \nThe application involved the generation of reports associated with plutonium processing. The reports \nprovided information such as material balances, equipment utilization, resource utilization, process \ndependencies, and waste generated from the processes. This set of tests was deemed as acceptance testing. \nIt should be noted that the successful execution of this set of tests is not sufficient to demonstrate \nuses for other applications. For example, the plutonium processing exercised instances of parts and bulk \ncontainers for the class, materials. Other instances such as streams were not exercised. Hence, additional \ntest suites are required to sufficiently demonstrate other uses of the software system.  Summary Integrating \nthe testing activities into a software quality assurance program was beneficial to the testing process. \nThis is because assurance practices such as inspections and writing testable requirements and specifications \nprovided a solid starting point from which the test approach and subsequent test cases were formulated. \nUsing object-based testing within the framework of CLOS constructs is intuitive. Also the systematic \nexecution of test from one software component to the next is reminiscent of established testing practices \nwhereby a progression from unit testing, subsystem testing, integration testing, and to acceptance testing \nis accomplished. The results reported here represent a rudimentary effort in object-based testing. Issues \nsuch as test coverage, stopping criteria, fault-based testing, need further research and refinement. \nIt is postulated that many other issues will surface. Considerable attention was paid by both researchers \nand practitioners in defining Addendum to the Proceedings OOPSLA 93 procedural-based testing. The contention \nis that object- based testing requires the same amount of attention. References: 111 IEEE S&#38;l 829-1983, \nStandard for Software Test Documentation, IEEE Publishers, New York, NY. PI IEEE Std 1008-1987, Standard \nSoftware Unit Testing (ANSI), IEEE Publishers, New York, NY. [31 Booth, G. Object-Oriented Design with \nApplications, The Benjamin/Cummings Publishing Company, Inc., Redwood City, CA, 199 1. r41 Fagan, M.E. \nDesign and Code Inspections to Reduce Errors in Program Development. IBM Systems Journal, 15,3, 1976, \n123-148. Contact information: Betty P. Chao Software Engineering Associates P.O. Box 500% -Albuquerque, \nNM 87 185 E-mail: SEABETTY@aol.com Donna M. Smith Los Alamos National Laboratory Los Alamos, NM 87545 \n- E-mail: smith@mimsy@lanl.gov Washington, D.C. September 26-October 1, 1993  \n\t\t\t", "proc_id": "260303", "abstract": "", "authors": [{"name": "Betty P. Chao", "author_profile_id": "81100133873", "affiliation": "Software Engineering Associates, P.O. Box 5006, Albuquerque, NM", "person_id": "P30032", "email_address": "", "orcid_id": ""}, {"name": "Donna M. Smith", "author_profile_id": "81100079903", "affiliation": "Los Alamos National Laboratory, Los Alamos, NM", "person_id": "P69177", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/260303.260317", "year": "1993", "article_id": "260317", "conference": "OOPSLA", "title": "Applying software testing practices to an object-oriented software development", "url": "http://dl.acm.org/citation.cfm?id=260317"}