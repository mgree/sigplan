{"article_publication_date": "04-01-1993", "fulltext": "\n Washington, DC-26 September-l October, 1993 Workshop 11 Efficient Implementation of Concurrent Object-Oriented \nPrograms Report by: L. V. Kale University of Illinois This full-day workshop was held on Monday prior \nto the The next paper to be presented was Distributedmain conference. The purpose of the workshop was \nto Execution of C++ Programs by Arun Chatterjee. Itexamine the efficiency of existing concurrent object-described \nthe ESP distributed object orientedoriented language implementations, discuss techniques environment. \nHe gave several examples of ESP for improving the efficiency of such implementations, and programs which \nare written in C++. ESP supports identify the challenges in reaching parity in efficiency. distributable \nobjects via a base class called An important goal of the workshop was to facilitate MessageCapable. Method \ninvocation on remote objectcommunication amongst researchers and practitioners is handled transparently \nby ESP. It also supports working in distinct communities, particularly those asynchronous method invocations. \n building applications using concurrent object-oriented languages and those building implementations \nof The final presentation of the first session was given by concurrent object-oriented languages. L.V. \nKale and was titled Medium Grained Execution in Concurrent Object Oriented Systems, co-authored by The \nworkshop focussed on all aspects of efficient Sanjeev Krishnan. This presentation sketched a brief implementation \nof concurrent object-oriented programs. outline of the Charm++ system. It is a message drivenThe topics \ndiscussed included: and multi-threaded system which helps tolerate Fine and Coarse-grained systems communication \nlatencies. It supports information sharing . Compilation techniques (static analysis and abstraction \nand dynamic load balancing and is available optimization) on distributed and shared memory machines, \nand workstation networks. He argued that medium grained Supporting data parallel computations . systems \nare currently desirable because they are easier Run time issues (load balancing, scheduling, and . to \nimplement and the explicit grain-size control they message driven execution) require can be selected \nindependent of the number of . Experience identifying efficiency problems processors or the architecture. \nParticularly with Large-scale concurrent object-oriented programming message driven execution. experience \n The second session, Compilation Techniques began The workshop included thirteen presentations with a \npresentation called Compiler and Library (organized into four sessions) and two panel Support for Aggregate \nObject Communications on discussions and was very well attended with a lively Distributed Memory Machines, \npresented by Jenq and participating audience. Kuen Lee in collaboration with Yunn Yen Chen. He first \ndescribed a language that is a dialect of PC++. PC++ The first session was titled Medium-grained supports \na collections construct for building distributed Systems. To begin this session, Andrew Grimshaw objects, \nand distributed pointers to refer to remote presented a paper titled Overhead Versus Application address \nspaces. He then described an shared memoryGranularity in Mentat, co-authored by W. Timothy and shared-object \ninterface called NAM, based on the Strayer Jon Weissman, and Emily West. He argued that notions of RPC \nand active messages. It supports the absolute wall-clock time of the overhead is not as several shared \nmemory and shared object operations, important as the relative cost of the overhead compared such as \nblock-read and migrate-object. Results from to the useful computation time. The presentation cited two \napplications carried out using this environment onquantitative performance data obtained on two an nCUBE \nmachine were then described. applications using the Mentat object oriented parallel processing system. \nThe data was obtained on a network Next, Birger Andersen presented Efficiency by Type of Sparcstations \nand on an Intel IPSC/860 and Guided Compilation. He argued that it is essential to demonstrated that \nthe overhead was a small percentage make use of different levels of type knowledge for of the useful \ncomputation for these applications. compile time optimization. He presented an overview of Addendum to \nthe Proceedings OOPSLA 93 the the possible levels of type knowledge, and what kinds of compile time optimizations \nmay be performed based on such knowledge. An example of of optimization in the context of the fine-grained \ndistributed object oriented language Ellie was also presented. Ellie aims at machine independent parallel \nprogramming, by allowing users to specify fine-grained programs, and having the compiler merge objects \nat compile time, and supporting object clustering and load balancing at run time. The Concert System-Compiler \nand Runtime Support for Efficient, Fine-Grained Concurrent Object-Oriented Programs was the third presentation \nand was made by Andrew Chien and co-authored by Vijay Karamcheti and John Plevyak. They described compilation \nand runtime techniques to make the fine-grained object-oriented languages efficient. Their approach is \nbased on aggressive static analysis, tuning the execution grain size by static optimization, speculative \noptimization and dynamic compilation. They implemented these techniques in the Concert system which has \nbeen operational on several sequential and parallel platforms including the CM-5 and Sum workstations. \nA high- performance run-time support system has also been implemented as part of Concert. In the final \npresentation of this session, Ian Angus presented a paper titled Class Specific Optimizations: The C++ \nCompiler can do more. He noted that C++ and COOP in general have made few inroads into the realm of scientific \ncomputing. Severe performance problems exist in using C++. Radical compiler improvements are necessary \nin order to overcome these problems. He gave an overview of some common efficiency problems and proposed \nclass-specific optimizations as a mechanism for overcoming them. The third session was Runtime System \nIssues. Kenjiro Taura presented a paper co-authored by Satoshi Matsuoka and Akinori Yonezawa titled An \nEfficient Implementation Scheme of Concurrent Object-Oriented Languages on Stock Multicomputers. The \npaper summarized their implementation of a concurrent object-oriented language on conventional multi-computers. \nThey argued that while dedicated hardware support for such languages is attractive, most conventional \nmachines do not provide such support. Their implementation optimizes intra-node scheduling by combining \nstack-based and queue-based scheduling. Message passing is optimized to communication via registers and \nthe stack whenever possible. The intra- node communication overhead was reduced by using specialized \nmessage handlers for each type of message, similar to those in Active Messages. Preliminary benchmark \nresults were presented on a 512 processor AP-1000. They demonstrated inter-node latency of less than \n10 micro-seconds, and good speedups for the benchmarks. The next presentation, Efficient Parallel Global \nGarbage Collection on Massively Parallel Computers, was by Tomio Kamada, Satoshi Matsuoka, and Akinori \nYonezawa. The paper presents an efficient algorithm for garbage collection, which takes advantage of \nreference locality, traverses references across nodes, and interrupts ongoing computation for a minimum \namount of time. The number of messages needed for termination detection marking by their GC algorithm \nis less than 10% of the number of mark messages, which is a significant improvement over previous algorithms. \nThe third paper, by Yutaka lshikawa, Atsushi Hori, Hiroki Konaka, Munenori Maeda, and Takashi Tomokiyo, \nwas MPC++: A Parallel Programming Language and its Parallel Objects Support. They presented the design \nof an extension of C++ called MPC++, which is a language targeted towards message-driven parallel machines, \nsuch as the RWC- 1. MPC++ does not directly provide parallel objects; however they have demonstrated \nhow parallel objects can be realized using MPC++ primitives. They have also demonstrated how runtime \nroutines for parallel objects, such as mutex and signal, can be supported using MPC++. They plan to implement \nMPC++ for RWC-1, CM-5, and Intel Paragon. The fourth and final session was titled Language Designs. Stephan \nMurer was the first presenter with a paper titled The psather DIST Class: Data-parallel Programming Beyond \nArrays. In this research, the Sather language is extended to provide support for implicitly expressing \nparallel computations using distributed complex data structures. The programming model provides a shared-memory \nview as pSather objects reside in a clustered shared address space with cheap intra-cluster accesses. \nThe DIST built-in class provides a directory data structure for coordinating chunks of a distributed \ndata structure across clusters. All data in one chunk are located within the same cluster. The dist data \nparallel statement specifies an unordered loop that iterates over all chunks of a distributed structure. \nResults of a prototype implementation on the CM-5 were presented. Next was Concurrent Object-Oriented \nin Sympal, presented by Yariv Aridor and co-authored with Shimon Cohen and Amiram Yehudai. Sympal uses \na pure parallel functional language as its host language (a side- effect free subset of Common LISP). \nSympal has an optimizing compiler that, for example, detects send- and-forget messages for which the \nreturn values are not used. The compiler also decides the placement of objects based on the user-provided \nobject graphs. It s been implemented on a multiprocessor and workstations. Performance of several applications \non the g-node multiprocessor were presented. The final paper, by Murat Karaorman and John Bruno, was \ntitled Design and Implementation Issues for Object-Oriented Concurrency. This was a broad paper that \naimed at describing the design and implementation issues for COOP systems. The observations were based \non the experience gained while introducing concurrency into Eiffel. This was done without modifying the \nlanguage. Issues relating to thread abstractions, coordination, and language and distribution were described. \nLibrary-based approaches were argued to be better than language extensions. In addition to the four technical \nsessions, there were two panel discussions. The first panel discussion, titled Who needs concurrent objects? \ntook place prior to the lunch break, and featured panelists Ian Angus, from Boeing, Allen Robinson, from \nSandia, and Arch Robison from Shell. The panel was moderated by Andrew Chien. The objective of this panel \nwas to focus on the application experience using concurrent object-oriented languages, and to identify \nthe major barriers for wider use of the such languages. Arch Robison identified the phases in which code \nmigrates (or may migrate) toward Concurrent Objects at organizations with large extant production codes. \nThe initial codes in languages such as Fortran first moves to C++ with explicit message passing, motivated \nby software engineering concerns. In the second phase, the message passing gets encapsulated (rather \nthan being low level, and explicit), driven by reuse and efficiency considerations. In the final phase, \nthe code may move to truly concurrent objects, where individual objects may be distributed, and present \nparallel abstractions in an object oriented fashion. However, C++ based object oriented style can be \nquite slow on some applications-from two to twelve times slowdowns are possible, and this is one hindrance \nfor its acceptance. Ian Angus stated that C++ programs need more memory, and are slower. He also noted \nthat OOP is used quite widely in Coarse-grained applications in distributed programming (networks), but \nnot widespread on tightly coupled parallel machines. Boundary conditions are also difficult to express \nin 00 programs. Efficiency, education, and a convincing argument that what you will have will work, are \nnecessary for acceptance of COOP style. Allen Robinson noted that at OONSKI 93 numerous applications \nwere listed which could benefit from COOP. C++ is the main-stream object-oriented language. Defining \nnew languages (or using others than C++) will not succeed easily. Efficiency is quite important-people \nwant to run machines somewhere near the peak efficiency of the machines. Linking in Fortran subroutines \nin such programs is useful for efficiency. The advantages of C++ are seen in representing mathematical \nentities as classes, in using overloaded operators, and polymorphism. Reuse is also a big win. For widespread \nacceptance, COOP needs efficient implementation of arithmetic operators, automatic load balancing, and \nevidence of its utility. In the discussion that followed, Arch noted that reducing a 80K program to 20K \n(i.e demonstrating the code size reduction) will also help prove the utility of COOP. Andrew Chien asked \nwhether any other parallel language meets the requirements put up by Allen. Allen responded by agreeing \nthat none does, and added that that is why most users use SPMD style with explicit message passing-rather \nthan any other languages or compilers. Prompted by a question on utility of COOPS for ease of modeling, \nIan said that it helps in some areas but doesn t in others (e.g. boundary conditions). Efficiency is \nan issue, and everything gets compared to Fortran. Also: software development costs (which COOP saves) \nare difficult to measure, whereas execution time can be easily measured, which leads to the focus on \nefficiency. Arun Chaterjee said that the commercial world is even more reluctant to switch to new languages \nthan scientific and engineering organizations. However, Kale noted that in certain application classes, \nproducts by Microsoft and Borland etc. have made C++ a de-facto standard. The second panel discussion \nwas called Efficient Implementation: Issues and Challenges, and took place at the end of the workshop. \nThe panel members were: Andrew Grimshaw, Andrew Chien, and L. V. Kale. The issues under discussion included \nevaluations of how efficient COOP systems currently are, what are key issues in eflicient implementation, \nare there any fundamental disadvantages to COOP style, the importance of portability, dam-parallelism, \netc. A major portion of the discussion revolved around the debate between fine-grained and medium-grained \nsystems. Andrew Grimshaw identified granularity (and its interaction with portability) as one of the \nkey issues for application efficiency. He noted that workstation networks and massively parallel machines \nare both useful for different category of applications, and so we must focus on both the platforms. He \nalso questions whether there is really a need for fine-grained languages, and hardware support for such \nlanguages. Sanjay Kale argued that message driven execution, instead of a blocking receive based communication, \nor RPC like blocking object invocation, is a must, to overlap communication and computation effectively, \nso as to attain desired efficiency. He noted that medium grained systems can serve as a back-end for \nfine- grained ones (assuming the compiler carries out clustering) and also serve as a usable system, \nuntil fine-grain compiler technology matures. However, fine-grained systems have one draw-back-they typically \ndo not support sophisticated message scheduling strategies, such as , because the overhead cannot be \nsustained with a fine- grained model. Yet, such strategies are essential in many classes of applications \nsuch as branch-and-bound etc. The COOP community should strive the enhance the modularity and potential \nfor re-use in their systems, build reusable libraries to realize this potential, and demonstrate reuse \nand utility in real-life applications. Andrew Chien agreed that workstations and MPPs can be good for \ndifferent applications. The latency issues separates the two classes of applications. On the efficiency \nissue, he said that the COOP technology must compare to the fastest way of expression, and make it efficient \nenough that it is not worth the trouble to use the low-level (fast) languages. Efficiency can be attained \nwith a better run-time system in a medium- grained approach, or with compiler technology in a fine- grained \napproach. In his opinion, the fine-grained approach is better because it provides a uniform model which \nis necessary for portability. He listed several challenges in the compiler and run-time technologies \nincluding support for separate compilation, exploitation of partial information, storage analysis and \ngarbage collection, etc. A lively discussion followed the presentations including discussion about fine-grained \nand medium-grained approaches. Andrew Grimshaw noted that tine-grained may be better, but compiler technology \nisn t going to be adequate in the short term. If a fine-grained language is helped by annotations one \ncould simply absorb those annotations in the language itself. However, Andrew Chien disagreed, as annotations \ncan be ignored and are optional and that this distinction is important. In response to a question about \ndefining grain sizes, Andrew Grimshaw ventured that small-grain is less Addendum to the Proceedings \nOOPSLA 93 than 25 instructions and coarse is more than 1 million instructions, with medium somewhere \nin between. It was commented that in the future, the focus may shift from efficiency to software productivity. \nWorkshop Organizers: L. V. Kale, University of Illinois (Coordinator) Ian Angus, Boeing Computer Services \nAndrew Chien, University of Illinois Akinori Yonezawa, University of Tokyo \n\t\t\t", "proc_id": "260303", "abstract": "", "authors": [{"name": "L. V. Kale", "author_profile_id": "81100348138", "affiliation": "University of Illinois", "person_id": "PP42051330", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/260303.260350", "year": "1993", "article_id": "260350", "conference": "OOPSLA", "title": "Efficient implementation of concurrent object-oriented programs", "url": "http://dl.acm.org/citation.cfm?id=260350"}