{"article_publication_date": "10-01-1996", "fulltext": "\n Simple and Effective Analysis of Statically-Typed Object-Oriented Programs Amer Diwan J. Eliot B. Moss \nKathryn S. McKinley* Department of Computer Science University of Massachusetts, Amherst, MA 01003 \nAbstract To use modern hardware effectively, compilers need ex-tensive control-flow information. Unfortunately, \nthe fre- quent method invocations in object-oriented languages obscure control flow. In this paper, we \ndescribe and eval- uate a range of analysis techniques to convert method invocations into direct calls \nfor statically-typed object-oriented languages and thus improve control-flow in-formation in object-oriented \nlanguages. We present simple algorithms for type hierarchy analysis, aggre-gate analysis, and interprocedural \nand intraprocedu-ral type propagation. These algorithms are also fast, O(lprocedures] * Cprocedurea np \n* up) worst case time (linear in practice) for our slowest analysis, where np is the size of procedure \np and vP is the number of vari- ables in procedure p, and are thus practical for use in a compiler. When \nthey fail, we introduce cause analysis to reveal the source of imprecision and suggest where more powerful \nalgorithms may be warranted. We show that our simple analyses perform almost as well as an oracle that \nresolves all method invocations that invoke only a single procedure. 1 Introduction Computer hardware \nand programming languages are on a collision course. Modern hardware exposes an increas- *The authors \ncan be reached electronically via Internet addresses {diwan,moss,mckinley}@cs.umass.edu. This work was \nsup-ported by the National Science Foundation under grants CCR-9211272 and CCR-9525767 and by gifts from \nSun Microsystems Laboratories, Inc. P~r~i~~io~ to make dWalhard copy of part or all of this work for \npersonal or classroom use is granted without fee provided that copies are not made or distributed for \nProfit or commercial advantage, the copyright notice, the title ?f @ Pubkation,and its date appear, and \nnotice is given that CoPYW 1s bY PermissIon pf ACM, Inc. To copy otherwise, to republish to Poston sewers, \nor to redistribute to lists, requires prior specific perm&#38;ion and/or a fee. OOPSLA 96 CA, USA 0 1996ACM \n0-89791-788~X/96/0010...$3.~ ing amount of implementation details to the compiler. The exposed detail \nincludes the pipeline, the memory hierarchy, and the functional parallelism of the proces- sor. To achieve \nthe processor s potential, the compiler must be able to predict the program s control-flow. In object-oriented \nlanguages, programmers use a type hierarchy and method invocations to improve code reuse and correctness. \nUnfortunately, method invocations ob-scure which procedure is actually being invoked. In dynamically-typed \nlanguages, frequent method look-up is costly in itself [6] but in statically-typed languages, it is typically \nnot a significant cost. For both static and dynamic languages however, method invocations inhibit optimization. \nIf analysis can resolve method in-vocations to direct calls, the compiler can then optimize effectively \nacross method invocation sites, replacing the method invocation with a direct call, a tailored call, \nor an inlined call. The additional control-flow information provides fodder for an optimizing compiler \nto improve performance. In this paper, we describe and evaluate the following analyses for resolving \nmethod invocations in a statically- typed object-oriented language: . Type hierarchy analysis determines \nthe procedures a method invocation may call by considering the types that implement that method. . Type \npropagation performs intraprocedural and in- terprocedural data-flow analysis for types. . Aggregate \nanalysis detects when an object or record field is restricted to a single type. Our type hierarchy analysis \nis the same as previous work. What differentiates our type propagation and ag- gregate analysis from \nprevious work is that we make simplifications to achieve a fast whole program analy-sis, 0 ((procedures\\ \n* ~~oced rer np *vp) time worst case (linear in practice) for the slowest analysis, where np is the size \nof procedure p and Q is the number of vari- ables in procedure p. We also introduce a cause analysis \nalgorithm that determines the reason when our analysis does not resolve a method invocation. Using the \ncause analysis, we present detailed experimental results that characterize which language and programming \nfeatures affect the procedure(s) called at run-time from a method invocation site and the ability of \nthe analysis to resolve the method invocations. We implemented the analyses for Modula-3 programs [ \n191 in the SRC Modula-3 compiler version 3.5 [16]. Modula-3 is a statically-typed object-oriented systems \nprogramming language similar (for present purposes) to C++ and even more so to Java. We evaluated the \nanalyses on ten Modula-3 programs ranging in size from 400 to 29,000 lines of non-blank, non-comment \nlines of code (76,122 total lines). Our results demonstrate that these simple algorithms are very effective \nat resolving statically monomorphic method invocation sites, i.e., those method invocation sites that \ncall a single procedure. Our techniques detect 92% of the method invocation sites in our test suite that \ncall one procedure at run time. Most method invocations are resolved by type-hierarchy analysis, but \nintraprocedural and interprocedu- ral type propagation, and aggregate analysis, also benefit individual \nprograms. Our cause analysis indicates that the primary reason for analysis failure is polymorphism, \ni.e., the method invocations call more than one proce-dure at run time and thus analysis alone cannot \nresolve them. Our results show that most polymorphic method , invocations are due to use of heap allocated \nstructures. For monomorphic sites, the primary cause of analysis failure is the loss of information at \ncontrol-flow merges, records, and heap allocated structures. Although there is room to improve the aggregate \nanalysis, our simple analysis resolves most of the few monomorphic calls that could be resolved by an \naggregate analysis. Our analyses are simple, fast, and effective enough to incorporate into existing \ncompilers for statically-typed object-oriented languages. The remainder of this paper is organized as \nfollows. Sections 2 and 3 review background material and further motivate these analyses. Section 4 describes \ntype hierar- chy analysis, type propagation, and aggregate analysis. Section 5 presents static and dynamic \nmeasurements of method invocations and our ability to resolve them. Sec-tion 6 gives the reasons for \nour failures and successes and compares our analyses to an oracle. Section 7 discusses how our results \napply to languages like C++. Section 8 reviews related work. Section 9 concludes.  2 Background: Polymorphism \nthrough subtyping Statically-typed object-oriented languages support poly- morphism through subtyping. \nA type S is a subtype of T if it supports all the behavior of T.Thus, the program can use an object of \ntype s whenever an object of type T is expected. In particular, a variable with declared type T may refer \nto objects that are subtypes of T,not just T. Consider the Modula-3 type hierarchy in Figure 1, which \ndefines a type T,and S,a subtype of T.Proce-dures mT,ms, and nS are defined elsewhere. S has all the \nbehavior of T (in particular, the m method) but it may have different implementations of T smethods (in \nthis case, mS instead of mT). S may support behavior not supported by T (in this example, the n method). \nInvok-ing the m method on a variable with declared type T may invoke one of three procedures: 1. mT, \nif the last object assigned to the variable had type T 2. mS, if the last object assigned had type S;and \n 3. error, if the last object assigned was NULL.  In general, invoking a method on a variable (the re- \nceiver) can call any procedure that overrides that method in the variable s declared type or any subtype \nof its de- clared type. The NULL type is a subtype of all objects in Modula-3 and overrides all methods \nwith anerror procedure. A polymorphic method invocation site calls more than one user procedure at run \ntime. For example, consider invoking the print method on each element of a linked list in a loop. If \nthe list links objects of different types, then the print method invokes different procedures depending \non the type of the list element. A monomorphic method invocation site always in- vokes the same user \nprocedure (or error).It may have the potential to be polymorphic but the polymorphism is TYPE T = OBJECT \n f: T; METHODS m := mT; END; (* S is a subtype of T *) TYPE S = T OBJECT METHODS n a- nS; WE&#38;DES \n m := mS; END; Figure 1: A Modula-3 Type Hierarchy never present at run time. To continue the linked \nlist ex- ample, if the list links objects of only one type, then the print method will always invoke \nthe same procedure. A method invocation is resolved if it is identified as being monomorphic. The goal \nof the analyses presented here is to resolve all the monomorphic sites.  Motivation Object-orientation \npromises many important software engineering benefits. There are however significant obstacles to obtaining \npeak performance from object-oriented programs on modern processors. In particular, small methods with \nfrequent method invocations hin-der compiler optimizations since the compiler does not know where control \nwill go next. Modern hardware, on the other hand, requires a sub- stantial amount of control-flow information \nto exploit the hardware fully. For example, the memory latency of some modem machines is around 70-100 \ncycles [ 1 l] and thus the compiler needs to know which instruction will be executed 70 cycles in advance \nin order to prefetch data or instructions most effectively. Other hardware features that require extensive \ncontrol-flow information to optimize for include multiple instruction issue per cycle and non-blocking \nloads. Figure 2 shows the cumulative distribution of the num- ber of SPARC instructions executed between \nmethod in-vocations for a run of M2 toM3, a Modula-3 program that converts Modula-2 programs to Modula-3. \nAtpoint (x, y ) , y% of the time there are fewer than x instruc- 80 - E 8 cii a 50 100 150 200 250 Instructions \nbetween method invocations Figure 2: Instruction distribution between method invo-cations tions between \nmethod invocations. The graph shows that more than 50% of the time there are fewer than 60 instructions \nbetween method invocations. These in-structions include those for passing parameters and for method lookup. \nWith such frequent unknown control transfers, it is unlikely that the compiler can fully exploit prefetch \ninstructions or instruction level parallelism. There are two ways of improving the control-flow information \nin the compiler. First, program analysis may reduce the set of possible procedures called at each method \ninvocation [l, 10, 121. This analysis is effective only on monomorphic method invocations. Second, a \nprogram may be transformed so that the performance- critical method invocations can be converted to direct \ncalls. An example is splitting, which duplicates code in order to improve type information [8]. Program \ntransfor-mations are effective on polymorphic and monomorphic method invocations. Since transformations \nmay degrade performance, a good strategy for a compiled language is first to analyze, and only then to \napply transformations to convert the re- maining, frequently executed, polymorphic method in-vocations. \nIn this paper, we focus on the first part of the solution: to develop and evaluate a range of analysis \ntechniques to resolve method invocations. 4 Analysis In this section we describe each analysis technique \nand introduce cause analysis. We use the type hierarchy of Figure 1 to illustrate the strengths and limitations \nof the analyses. We use a power set of the types for the lattice for our analyses; the initial type for \na variable or heap or record field or pointer reference is the empty set. 4.1 Type Hierarchy Analysis \nType hierarchy analysis bounds the set of procedures a method invocation may call by examining the type \nhierarchy declarations for method overrides. For each type T and each method m in T, type hierarchy analysis \nfinds all overrides of m in the type hierarchy rooted at T. These overrides are the procedures that may \nbe called when m is invoked on a variable of type T.Since NULL is a subtype of all objects in Modula-3 \nand it overrides all methods, type hierarchy analysis can never narrow down the possibilities to just \none; at best it determines a method is one procedure or the error procedure. Type hierarchy analysis \ndoes not examine what the program actually does, just its type and method decla-rations. Thus, it takes \ntime proportional to the num-ber of types and methods in the program, O(ITypesl * IMethodsl).  4.2 Intraprocedural \nType Propagation Intraprocedural type propagation improves the results of type hierarchy analysis by \nusing data flow analysis to propagate types from type events to method invocations within a procedure, \nType events create or change type information. The three distinguishing type events are allocation (v \nt NEW (t)), implicit and explicit type discrimination operators (IsType (v, T)), and assignment (v t \nu), which includes parameter bindings at calls. For example, consider the following code: 1 P := NEW \n(S); IF cond THEN 2 0 := NE W (T); 3 0-m 0; ELSE 4 0 := P; 5 0.m 0; END; 6 o.m 0; Statement 2 contains \na type event: an allocation. The al- location propagates the type T to o, and thus determines that the \nmethod invocation in statement 3 calls procedure mT.Similarly, statement 4 contains a type event: an \nas- signment. The type event propagates the type of p to o, and thus determines that the method invocation \nin state- ment 5 calls procedure mS. Finally, intraprocedural type propagation merges the types of o \nat the control merge before statement 6, yielding the type {T, S}.Thus the method invocation in statement \n6 cannot be resolved to a single procedure. Our implementation of type propagation propagates types only \nto scalars; it assumes the conservative worst case (the declared type) for the allocated types of record \nfields, object fields, array references, and pointer ac-cesses. We formulate intraprocedural type propagation \nas a data-flow problem similar to reaching definitions. We identify and propagate pairs of variables \nand sets of pos- sible types for the variables. All variables initially have the empty type. A statement \ns with a type event gener-ates and kills types as follows: GENTYPE (v t NEW(t)) = (v, t) GENTYPE(ZsType(v, \n7 )) = (v, TypeOflv) f~ 2 ) GENTYPE (V t 1~) = (v, TwO74) KILLTYPE(V t NEW(t)) = (21, TypeOflv)) KILLTYPE(V \nt u) = (T TypeWW T denotes a set of types and t denotes a single type. TypeOfretums the current types \nof a variable. ZsType is an explicit type discrimination event which checks if u s type is in T.Type \ndiscrimination may also be implicit. In particular, for each IsType, there is an implicit type discrimination \nevent for the false branch. The data-flow equations for a statement s are similar to the equations for \nreaching definitions: IN@) = U,~PRED(~) OUT(p) OUT(s) = GENTYPE(S) U (IN(s) -KILLTYPE( Our implementation \nstores the possible types of a vari- able as a set. Thus, the union and intersection operators are set \nunion and set intersection respectively. This prob- lem formulation is monotone and distributive. Since \nModula-3 programs are always reducible and type prop- agation is rapid [ 171, we use a 0 (n * V) solution, \nwhere n is the number of statements in a procedure and v is the With the type discrimination operations \nin Modula 3, a more precise, but non-monotone formulation is possible. As far as we know, no one has \ninvestigated this formulation. number of variables in the procedure and each step of the algorithm is \na bit vector operation, 4.3 Aggregate Analysis The goal of our aggregate analysis is to handle a common \nsituation efficiently: monomorphic use of a general data structure. Consider the linked list package \nagain. Our aggregate analysis detects when a program links objects of a single type and thus would resolve \nthe invocation of the print method on the list elements. The aggregate analysis circumvents the difficulty \nof analyzing records and heap allocated objects by merging all instances of an object or record type. \nFor example, v: T; v.f := <rhs>  propagates the types of <rhs> to the field f of all pos- sible types \nof v. The possible types of v may be deter- mined by another analysis (such as type propagation) or may \nbe conservatively approximated as T and its sub- types. This analysis discovers monomorphic uses of general \ndata structures. However, if the program allocates two distinct linked lists, one with elements of type \ns and the other with type T, aggregate analysis does not recognize that each list is homogeneous. It \ninfers the type {T, s , NULL } for the elements in both lists. The type of an object-typed field always \nincludes NULL since all fields in Modula-3 are initialized at al- location, and thus the first assignment \nto every object-typed field is always NULL. In order to propagate types to and from a field, aggregate \nanalysis requires that all assignments to that field be available for analysis. We perform aggregate \nanalysis in a single forward pass. For each procedure, it is O(n) time in the number of statements in \nthe procedure. 4.4 Interprocedural Qpe Propagation Interprocedural type propagation combines with in-traprocedural \ntype propagation to resolve more method invocations. It begins by building a call graph of the program. \nThe call graph has an edge from a method invocation to each possible target determined by the ear- lier \nintraprocedural analysis. The algorithm operates by maintaining a work list of procedures that need to \nbe analyzed. A procedure needs to be analyzed if new information becomes available about its parameters \nor about the return value of one of its callees. When in-terprocedural type propagation analyzes a procedure, \nit may put the callers and callees of the procedure on the work list and update the call graph. In particular, \nanaly-sis may eliminate some call graph edges if it refines the type of a method receiver. Interprocedural \ntype prop-agation maintains the work list in depth first order and terminates when the work list is empty. \nInter-procedural type propagation also keeps track of which procedures are called only via method invoca-tions \n(i.e., not called directly). For these procedures, it eliminates NULL as a possible type for the first \nargu-ment (self). If self isNULL, then error is invoked instead of this procedure. Interprocedural type \npropagation propagates types only to scalars, and it assumes the most conservative type (the declared \ntype) for all data accessed through pointer traversal. Interprocedural type propagation does not propagate \nside effects from calls and assigns the most conservative type for any variable changed by the call: \nthe declared type. Variables potentially changed by a call include variables declared in outer scopes, \nglobals, parameters passed by reference, and parameter aliases. A distinguishing characteristic of our \ninterprocedural . analysis is that, unlike related work ([l, 22, 20]), our analysis is context insensitive. \nRather than analyzing for every combination of call site and callee we merge the parameter types of all \ncall sites of a procedure, and the return types of all callees at a call site. This simplifi- cation \nyields a much faster analysis (quadratic instead of exponential) but at the cost of some accuracy. Consider \nthe following code: PROCEDURE Caller1 () = t := P (NEW (T)); t.m 0; PROCEDURE Caller2 ( ) = t := P \n(NEW (U); t.m 0; PROCEDURE P (0: T) : T = RETURN o; A context-sensitive analysis would analyze P separately \nfor each of its call sites and thus determine that the Analysis Eliminates NULL Complexity Type Hierarchy \nNo O(ITypesl * IMethodsl) Intraprocedural Type Propagation Yes OE, 7% * 5) Aggregate Analysis No W-L \n%) 1 Interprocedural Type Propa lgation Yes Table 1: Summary of analyses method invocation in Caller1 \nwill call mT and that in Cal ler2 will call mu.Our context-insensitive analysis instead merges the parameter \ntypes for each caller of P and thus does not resolve the method invocations in Caller1 and Caller2. We \nshow in Section 6 that for our benchmark suite, the loss in precision is not significant. If interprocedural \ntype propagation is invoked on an incomplete program, it makes conservative assumptions about the parameters \nof procedures that could be called from unavailable code, and about return values of un-available procedures. \nSince interprocedural type propagation may analyze each procedure multiple times (in particular, recursive \nor potentially recursive procedures), it may be substantially slower than intraprocedural type propagation. \nSince in-formation flows forward through parameters and back- wards from return values, the worst case \ncomplexity is 0( 1 procedures1 * Crocedurer nP * vP), where n is the number of statements in procedure \np and T+ the number of variables in procedure p. In practice, however, we have found it to be linear \nin the number of instructions, analyzing each procedure an average of 2 to 4 times.  4.5 Analyzing Incomplete \nPrograms An implicit assumption in the analyses described above is that the entire program (except for \nlibrary code) is available for analysis. Moreover, it is assumed that the library code does not create \nsubtypes of any types de-clared outside the library2. While we have extended our analyses to work with \nincomplete information, the re-sults presented here assume the entire program (except libraries) is available. \nIn future work, we will evalu-ate the effectiveness of the analyses on incomplete pro-grams. 2Libraries \nmay introduce subtypes of types declared outside the library if structural equality is used for types. \n 4.6 Ordering the analyses The ordering of our analyses can make a difference in the effectiveness. For \ninstance, if type propagation occurs only before aggregate analyses, then type propagation cannot propagate \nthe information exposed by aggregate analysis. We have not explored these interactions be-tween analyses \nexperimentally; instead we have chosen the following fixed ordering. 1. type hierarchy analysis 2. intraprocedural \ntype propagation 3. interprocedural type propagation 4. aggregate analysis 5. interprocedural and \nintraprocedural type propaga-tion where needed  4.7 Summary Table 1 summarizes the analyses. Eliminates \nNULL in-dicates whether the analysis can eliminate NULL as a possible type. In the Complexity column \nnP is the num-ber of statements in procedure p. These algorithms are simple and therefore fast, as shown \nin the Complexity column.  5 Results Section 5.1 describes the benchmark programs. Sec-tion 5.2 evaluates \nthe effectiveness of our analyses in converting method invocations to direct calls. Section 5.3 presents \nthe run-time improvements due to resolving these method invocations. Name Lines format 395 dformat 602 \nk-tree 77.6 slisp 1645 PP 2328 dom 6186  Method invocations Compile time Run time 37 47,064 95 30,775 \n13 714,619 223 67,253 24 458 222 293 1821 1808 430 4966  Description Text formatter Text formatter Builds \nand traverses a tree structure Small 1isD interpreter I 1 A pretty printer for Modula-3 programs 1 A \nsystem for building distributed applications A graDhica1 mail reader I Converts Modula-2 code to Modula-3 \nM3 v. 3.5.1 code generator + extensions Window system + small application Table 2: Benchmark Programs \n 5.1 Benchmark Programs For each of the benchmark programs, Table 2 gives the number of non-comment, \nnon-blank lines of code, the number of method invocations at compile time and at run time (for one run \nof the benchmark), and a brief description of the programs. 5.2 Converting Method Invocations to Direct \nCalls Figures 3 through 12 illustrate the percent of method invocations resolved by each analysis for \neach of the benchmark programs. The graphs have one bar for each level of analysis: tha: type hierarchy \nanalysis tha+tpa: tha plus intraprocedural type propagation tha+tpa+h: tha+tpa plus aggregate analysis \ntha+tpa+ip: tha plus interprocedural type propagation tha+tpa+ip+h: tha+tpa+ip plus aggregate analysis \nThe black regions in the bars corresponds to percent-age of method invocations at run time that the analyses \nresolves to exactly one procedure. The gray region cor-responds to method invocations that analysis resolves \nto one user procedure or error. The pair above the bar is the corresponding number of static call sites. \nIf a method invocation site is not executed at run time, it does not appear in these graphs. The figures \nillustrate that type hierarchy analysis and aggregate analysis are most effective analyses for these \nprograms and the effectiveness of the [I81 ---I other analyses is relatively small. Intraprocedural \nand interprocedural type propagation removes many NULL possibilities but resolves few selves. Thus that \nhave well as Modula-3 guages (such more effective type hierarchy less effective Aggregate additional \nmethod invocations by them- type propagation is useful for languages defined semantics for the NULL case \n(such and Java) but is less useful for other lan-as C++). Type propagation will also be if the whole \nprogram is not available since and aggregate analysis will become much because of the incomplete type \nhierarchy. analysis along with type propagation re- solves two method invocation sites in dom and 341 \nsites in m3cg (of which 88 are executed in the bench- mark run). Aggregate analysis is also effective \non trestle, resolving five method invocation sites, and on postcard, resolving 22 method invocation sites. \nHowever the resolved method invocations in trestle and postcard are not executed in the benchmark run \nand thus the impact of aggregate analysis does not ap-pearinthefigures. (Trestle andpostcard arelarge \nsystems and our inputs exercised only a part of them.) 5.3 Execution time improvement To judge the run-time \nimpact of the analyses, we ran our non-interactive benchmarks3 before and after resolution of method \ninvocations on a DEC 3000/400 workstation. In the first experiment, the compiler replaced method 3Because \nTrestle, postcard, and dom are interactive, we did not include them in this experiment. Figure 3: Analysis \nresults for format Figure 4: Analysis results for df ormat Figure 5: Analysis results for k-tree Lhl \n,hr+tpa dIa+,pa+h Gla+,pa+tp UI*+lpr+ip+h Figure 6: Analysis results for sl isp Figure 10: Analysis \nresults for M2 toM3 Figure 11: Analysis results for m3 cg 1h Ihsrqu *r+tp+h h+,p~rip ,r+,p+ip+li Figure \n12: Analysis results for Trestle invocations that resolved to exactly one user procedure with direct \ncalls. These are the method invocations that make up the black region in Figures 3 through 12. The compiler \ndid not convert method invocations that re-solved to one user procedure or error since that would be \ninconsistent with Modula-3 language semantics. We found that the execution time improvement averaged \nless than 2% for the benchmarks even when the com-piler inlined the frequently executed resolved method \ninvocations. In the second experiment, the compiler replaced method invocations that resolved to one \nuser procedure or error with direct calls. Ignoring the error pos-sibility is inconsistent with Modula-3 \nsemantics but it facilitates comparison with languages such as C++. We found that resolving the method \ninvocations improved performance by 0 to 9%, with an average improvement of 3%. When the compiler inlined \nthe frequently ex-ecuted resolved method invocations, the performance improvement ranged from 0 to 19%, \nwith an average of 6.5%. These results show that unlike dynamically-typed lan-guages, the direct cost \nof method invocations in statically typed-languages is small. The main cost of method in-vocations is \nindirect: method invocations obscure con-trol flow and thus inhibit compiler optimizations. We are currently \nimplementing and evaluating several op-timizations that exploit the information exposed by the analyses \ndescribed here.  6 Cause Analysis In Section 6.1 we describe our cause analysis technique and in Section \n6.2 we apply it to the benchmark pro-grams. 6.1 Technique In the absence of control and data merges, \nsuch as calls, analysis coulddetermine the allocated type of every vari- able. However, real programs \nintroduce potential poly-morphism by merging control and data as follows: . Control merges: after a conditional \nstatement at a call site with multiple targets (because of the returns) at a procedure with multiple \ncallers at the return of a procedure with multiple re-turn statements . Data merges: - at assignments \nthrough potential aliases (in-cludes heap allocated data, pointers, and array references) If a merge \nresults in the loss of type information and the affected variable is later used to invoke a method, then \nthat merge is the reason analysis failed to resolve the method invocation. The method invocation may \nac-tually be polymorphic, or the analysis may not be pow- erful enough to resolve it. For every method \ninvocation that our analyses do not resolve, our cause assignment algorithm finds the merges that result \nin the loss of type information for the receiver of the method invocation, The analyzer finds the merge \nby following use-defchains [2] to the point where information is lost. We use this information to expose \nthe reason when our analyses fail. The reason suggests what analyses or transformations may be effective \non the unresolved method invocations. For example, if a control merge Table 3: Cause of information loss \n Source Solution Record field More powerful aggregate analysis Object field More powerful aggregate analysis \nControl merge Context sensitive analysis Unavailable Analyze libraries obscures a type, a context sensitive \nanalysis may prevent this loss of information. The cause analysis identifies four sources of information \nloss: . Record: a merge of types in record fields or arrays (recall that the implementation propagates \ntypes only to scalars and to some extent to object fields), . Heap: a merge of types in the heap (includes \nobject fields and pointer references), . Control Merge: a merge of types due to a control merge, . Code \nUnavailable: a conservative type assumed due to unavailability of library code. Table 3 suggests the \ntechniques that may prevent the loss of information for each of the four causes of infor- mation loss. \n6.2 Results In Section 5.2, we demonstrated that the analyses resolve many method invocations to direct \ncalls. In this section, we address these questions: 1. How do our analyses compare to an oracle that \nresolves all monomorphic method invocations? 2. What transformations will be effective in convert- ing \nthe polymorphic method invocations to direct calls?  Figure 13 addresses the first question. Each bar \ngives the run-time data for one benchmark program. The height of a bar corresponds to the percentage \nof method invocations that always call the same procedure in a run of the benchmark4. Each bar has two \nregions, the black region corresponds to the method invocations re-solved by analysis and the gray region \ncorresponds to the unresolved monomorphic method invocations. The pair 4 We used two runs for pp with \ndifferent command-line parame-ters to expose the polymorphic method invocations. Figure 13: Monomorphic \nmethod invocations above each bar gives the number of static method invoca-tions corresponding to the \ntwo regions. The gray region is an upper bound on the truly monomorphic method in-vocations; (i.e., across \nall possible runs of the programs) and thus on how much better the oracle can do compared to our analyses. \nIt is an upper bound since method in-vocations may be polymorphic on a different program execution. Figure \n13 shows that, for all benchmarks except m3 cg and trestle, our analysis resolves the vast majority of \nmonomorphic method invocations; the analyses perform almost as well as the oracle. For df ormat, format, \nm2 t om3, pp, and s 1 i sp, our analyses perform as well as the oracle. Across all the benchmarks, the \noracle would resolve at most 7% more method invocation sites compared to our analyses. For the benchmarks \nwhere our analyses are less effective, Figure 14 indicates which analyses may be successful in resolving \nthese method invocations. Each bar in Figure 14 breaks down an unresolved region in Figure 13 into four \nregions, one for each cause of analysis failure. The numbers above each bar give the total number of \nmonomorphic method invocation sites. For m3cg the figure indicates that a more powerful aggregate analysis \nmay be successful in resolving more method invocations. On inspection of the source code of m3cg, we \nfound that an analysis would have to discover the semantics of a stack in order to do better than our \naggregate analysis. It is unlikely that any analysis would be able to discover the semantics of a stack \nand thus resolve more method invocations. For trestle, the primary cause of analysis fail-ure is control \nmerges. Thus a context sensitive analy-sis may be effective in resolving more method invoca-Figure 14: \nMonomorphic method tions. Tres t 1 e is the only benchmark where a context- sensitive analysis may be \nhelpful. Figure 15 addresses the second question: what trans- formations will be effective in converting \nthe polymor- phic method invocations to direct calls? Figure 15 presents data for the method invocation \nsites that call more than one procedure in a run of the benchmark and thus cannot be resolved by analysis \nalone. These method invocations are a lower bound on the polymor- phic method invocations since in another \nrun of the benchmark, additional method invocations may be poly- morphic. Figure 15 illustrates that \nmost run-time polymorphic method invocations arise because more than one type of object is stored in \na heap slot. Two techniques, ex-plicit type test [5, 151 and cloning combined with ag-gressive aggregate \nanalysis, may be able to resolve these method invocations. Merges in control are another im-portant cause \nof the run-time polymorphism, especially for tres t 1 e, and can be resolved by code splitting and cloning \n[7, 14, 91. From the static counts above the bars, we see that while the number of run-time polymorphic \nsites in the benchmarks is usually small, they are executed rela-tively frequently. For example, of the \n29 method in-vocation sites executed in a run of format, only 3 sites are polymorphic, but they comprise \nmore than 80% of the total method invocations executed. Across all the benchmarks, polymorphic sites \nare called 26 times more than monomorphic sites, Thus these Modula-3 invocations that are unresolved \nprograms have relatively few polymorphic method in-vocation sites, but they are executed very frequently. \nThis observation has an implication for optimizations: the number of method invocation sites where transfor-mation \nis needed is small and thus the code growth in-duced by transformations such as cloning is likely to \nbe negligible.   7 Applicability to Other Languages The analyses described here are language independent \nbut their usefulness depends on the language and the programming style. For example, some C++ program- \nming styles discourage the use of virtual functions un-less necessa$; in essence the style encourages \nthe pro- grammer to attempt type-hierarchy analysis manually. In such situations, the impact of type-hierarchy \nanalysis will be limited compared to Modula-3 programs, where all methods are virtual. We expect that \nour results will carry over to other statically typed object oriented lan-guages such as C++ $ the programs \nare written using only virtual methods. However the run-time improve-ment due to our analyses in C++ \nprograms may be greater since method invocations are more costly in languages that have multiple inheritance. \nSince dynamically-typed languages encourage a fundamentally different style of programming, we expect \nthat our results will not directly apply to them. Only virtual functions may be overridden in subtypes. \n i:l Record . Heap Control Merge H Code Unavailable Figure 15: Polymorphic  Related Work In this section, \nwe describe the related work on un-derstanding and analyzing object-oriented programs and distinguish \nour contributions. Femandez [ 121 and Dean et al. [lo] evaluate type hierarchy analysis for Modula-3 \nand Cecil respectively. They find that type hierarchy analysis is a worthwhile technique that resolves \nmany method invocations. Our work confirms these results. In addition to type hierarchy analysis, we \nevaluate a range of other techniques. Palsberg and Schwartzbach [20], Agesen and Holzle [l], and Plevyak \nand Chien [22] describe type inference for dynamically typed object-oriented languages. Age-sen and Holzle \ns, and Plevyak and Chien s analyses are more powerful than ours since they are context sensitive (polyvariant). \nThey are also more complex and expen- sive. Polyvariant analyses can be used in conjunction with transformations \nto resolve polymorphic method in-vocations. We focus solely on analysis here. Plevyak and Chien discuss \nreasons for loss of type information, but do not present any results. We present detailed data giving \nreasons for loss of type information. In work done concurrently with ours, Bacon and Sweeney [4] and \nAigner and Hijlzle [3] evaluate tech-niques for resolving method invocations in C++ pro-grams. Bacon \nand Sweeney evaluate three fast anal- Type propagation and type inference are terms that have been used \nto describe the same kinds of analysis in object-oriented languages. method invocations yses, including \ntype hierarchy analysis, for resolving method invocations in C++ programs. Unlike us, Bacon and Sweeney \nevaluate only flow insensitive analyses. Aigner and Holzle evaluate type feedback and type hi-erarchy \nanalysis and find that they are both effective at resolving method invocations. Pande and Ryder [21] \ndescribe a pointer analysis al-gorithm for C++ programs. Plevyak and Chien s type inference algorithm \nalso does some pointer analysis [22]. Both algorithms consider the control flow in a program and are \nthus more powerful than our simple aggregate analysis, which also deals with pointer analysis. How-ever, \nthey are also much slower than our aggregate analy-sis. On aSPARC-10, Pande and Ryder s algorithm takes \nas much as 23 minutes to analyze programs that are less than 1000 lines of code (median 36 seconds). \nOur aggre- gate analysis takes 38 seconds to analyze 28,977 lines of code on a DEC 3000/400. We show \nthat our simple analysis is effective and there is little to be gained by a more powerful analysis for \nour benchmarks. This result is partly due to Modula-3 s language semantics which restrict aliasing; a \nmore powerful alias analysis may be more useful for C++ than for Modula-3, but this need has not yet \nbeen demonstrated. Chambers [6], Calder and Grunwald [5], Holzle and Ungar [ 151, and Grove et al. [ \n131 describe transforma-tions for converting method invocations to direct calls, We focus solely on analysis \nhere. Shivers [23] describes and classifies a range of anal- yses to discover control flow in Scheme \nprograms. Our interprocedural type propagation is similar to References his OCFA. While Shivers focuses \non powerful (and slow) analyses---OCFA is the least powerful analysis he considers-we focus on simple \nand fast analyses. In-terprocedural type propagation is the most complicated analysis we consider. Another \nkey difference between our work and that of others is that we present results that give the reason when \nanalysis fails, and place upper bounds on how well more powerful analyses or transformations can possibly \ndo. Conclusions We describe and evaluate a range of analyses for object- oriented programs: type-hierarchy \nanalysis, intraproce-dural and interprocedural type propagation, and aggre-gate analysis. Aggregate analysis \nis a new technique and is simpler and faster than previous work. We demonstrate that our techniques are \nextremely effective at resolving method invocations in Modula- 3 programs. On average, our analyses resolve \nmore than 92% of the method invocation sites that are amenable to analysis and improve the run-time of \nthe benchmark programs by up to 19%. For method invocations that are unresolved by our analyses, we determine \nthe reason for analysis failure. The.failure reason suggests which other analyses and transformations \nmay be effective. The primary failure reason in our benchmarks is polymorphism: invocations called more \nthan one procedure and thus are not amenable to analysis alone. polymorphism is due to objects of different \nstored in heap slots. The other significant the method at run time Most of this types being reasons for \nanalysis failure are an insufficiently powerful aggregate analysis and lack of a context sensitive analysis. \nImprov-ing the aggregate analysis and adding a context sensitive analysis would resolve at most 7% more \nmethod invo-cation sites. 10 Acknowledgments We would like to thank Ole Agesen, Darko Stefanovic, and \nthe anonymous referees for comments on drafts of this paper. 111 Ole Agesen and Urs Holzle. Type feedback \nvs. concrete type inference: A comparison of optimization techniques for object-oriented languages. In \nProceedings of the Conference on Object-Oriented Programming Systems, Languages, and Applications, pages \n91-107, Austin, Texas, October 1995. ACM. PI Alfred V. Aho, Ravi Sethi, and Jeffrey D. Ullman. Compilers: \nPrinciples, Techniques, and Tools. Addison-Wesley, 1986. [31 Gerald Aigner and Urs HSlzle. Eliminating \nvirtual function calls in C++ programs. In European conference on object-orientedprogramming, Linz, Austria, \nJuly 1996. [41 David Bacon and Peter Sweeney. Fast static analysis of C++ virtual function calls. In \n0OPSL.A 96 Conference Proceedings: Object-Oriented Programming Systems, Languages, and Applications, \nSan Jose, CA, October 1996. ACM, ACM Press. Brad Calder and Dirk Grunwald. Reducing indirect function \ncall overhead in C++ programs. In Conference Record ofthe Twenty-First ACM Symposium on Principles of \nProgramming Languages, pages 397-408, Portland, Oregon, January 1994. t61 Craig Chambers. The design \nand evaluation of the SELF compiler, an optimizing compiler for object-oriented programming languages. \nPhD thesis, Stanford University, CA, March 1992. 151 Craig Chambers and David Ungar. Customization: \nOptimizing compiler technology for SELF, a dynamically-typed object-oriented programming language. In \n171 Proceedings of the ACM SIGPLAN 89 Conference on Programming Language Design and Implementation, pages \n146-160, Portland, Oregon, June 1989. ACM SIGPUN Notices 24,7 (July 1989). 181 Craig Chambers and David \nUngar. Iterative type analysis and extended message splitting: Optimizing dynamically-typed object-oriented \nprograms. In Proceedings of the ACM SIGPLAN 90 Conference on Programming Language Design and Implementation, \npages 150-164, White Plains, New York, June 1990. ACM SIGPLAN Notices 25,6 (June 1990). [91 Craig Chambers \nand David Ungar. Making pure object oriented languages practical. In Proceedings of the Conference on \nObject-Oriented Programming Systems, Languages, and Applications, pages l-l 5. Phoenix, Arizona, October \n199 1. ACM SIGPUN Notices 26, 11 (November 1991). [lOI Jeffery Dean, David Grove, and Craig Chambers. \nOptimization of object-oriented programs using static class hierarchy analysis. In Proceedings of European \nConference on Object-Oriented Programming, Aarhus, Denmark, August 1995. [I 11 Digital Equipment Corporation. \nDEC3000 300/400/500/600/800 Models: System Programmer k Manual, first printing edition, September 1993. \n[I21 Mary F. Femandez. Simple and effective link-time optimization of Modula-3 programs. In Proceedings \nof Conference on Programming Language Design and Implementation, La Jolla, CA, June 1995. SIGPLAN, ACM \nPress. [ 131 David Grove, Jeffery Dean, Charles Garrett, and Craig Chambers. Profile-guided receiver \nclass prediction. In Proceedings of the Conference on Object-Oriented Programming Systems, Languages, \nand Applications, pages 108-123, Austin, Texas, October 1995. ACM. [ 141 Mary Wolcott Hall. Managing \nInterproceduralOptimizations. PhD thesis, Rice University, Houston, Texas, April 1991. [ 151 Urs HSlzle \nand David Ungar. Optimizing dynamically-dispatched calls with run-time type feedback. In Proceedings \nof the ACM SIGPLAN 94 Conference on Programming Language Design and Implementation, pages 326-336. ACM, \nJune 1994. [ 161 Bill Kalsow and Eric Muller. SK Mod&#38;a-3 Version 3.5. Systems Research Center, Digital \nEquipment Corporation, Palo Alto, CA, 1995. [ 171 J. B. Kam and J. D. Ullman. Global data flow analysis \nand iterative algorithms. Journal ofthe ACM, 7(3):305-318,1976. [ 181 Farshad Nayeri, Benjamin Hurwitz, \nand Frank Manola. Generalizing dispatching in a distributed object system. In Proceedings of European \nConference on Object-Oriented Programming, Bologna, Italy, July 1994. [ 191 Greg Nelson, editor. Systems \nProgramming with Mod&#38;z-3. Prentice Hall, New Jersey, 199 1. [20] Jens Palsberg and Michael I. Schwartzbach. \nObject-oriented type inference. In Proceedings of the Conference on Object-Oriented Programming Systems, \nLanguages. and Applications, pages 146-162, Pheonix, Arizona, October 199 1. SIGPLAN, ACM Press. [21] \nHemant Pande and Barbara G Ryder. Static type determination and aliasing for C++. Technical Report LCSR-TR-250, \nRutgers University, July 1995. [22] J. Plevyak and A. Chien. Precise concrete type inference for object-oriented \nlanguages. In Proceedings of conference on object-oriented programming systems, languages, and applications, \npages 324-340. ACM, October 1994. [23] Olin Shivers. Control-jlow analysis of higher-order languages. \nPhD thesis, Carnegie Mellon University, Pittsburgh, PA, May 199 1.  \n\t\t\t", "proc_id": "236337", "abstract": "To use modern hardware effectively, compilers need extensive control-flow information. Unfortunately, the frequent method invocations in object-oriented languages obscure control flow. In this paper, we describe and evaluate a range of analysis techniques to convert method invocations into direct calls for statically-typed object-oriented languages and thus improve control-flow information in object-oriented languages. We present simple algorithms for <i>type hierarchy analysis, aggregate analysis, and interprocedural and intraprocedural type propagation</i>. These algorithms are also fast, <i>O</i>(|procedures| * &amp;sum;<inf>p</inf><sup>procedure</sup> <i>n</i><inf><i>p</i></inf> * <i>v</i><inf><i>p</i></inf>) worst case time (linear in practice) for our slowest analysis, where <i>n</i><inf><i>p</i></inf> is the size of procedure <i>p</i> and <i>v</i><inf><i>p</i></inf> is the number of variables in procedure <i>p</i>, and are thus practical for use in a compiler. When they fail, we introduce <i>cause analysis</i> to reveal the source of imprecision and suggest where more powerful algorithms may be warranted. We show that our simple analyses perform almost as well as an oracle that resolves all method invocations that invoke only a single procedure.", "authors": [{"name": "Amer Diwan", "author_profile_id": "81100202872", "affiliation": "Department of Computer Science, University of Massachusetts, Amherst, MA", "person_id": "PP15025608", "email_address": "", "orcid_id": ""}, {"name": "J. Eliot B. Moss", "author_profile_id": "81406593781", "affiliation": "Department of Computer Science, University of Massachusetts, Amherst, MA", "person_id": "PP39023945", "email_address": "", "orcid_id": ""}, {"name": "Kathryn S. McKinley", "author_profile_id": "81100402805", "affiliation": "Department of Computer Science, University of Massachusetts, Amherst, MA", "person_id": "P157900", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/236337.236367", "year": "1996", "article_id": "236367", "conference": "OOPSLA", "title": "Simple and effective analysis of statically-typed object-oriented programs", "url": "http://dl.acm.org/citation.cfm?id=236367"}