{"article_publication_date": "10-01-1996", "fulltext": "\n Code Reuse in an Optimizing Compiler Ali-Reza Adl-Tabatabai , Thomas Gross 12, and Guei-Yuan Lueh3 School \nof Co m puter Science 21nstitut fiir Computer Systeme ECE Department Carnegie Mellon University ETH Ziirich \nCarnegie Mellon University Pittsburgh, PA 152 13 CH 8092 Ziirich Pittsburgh, PA 15213 Abstract This \npaper describes how the cmcc compiler reuses code - both internally (reuse between different modules) \nand exter- nally (reuse between versions for different target machines). The key to reuse are the application \nframeworks developed for global data-flow analysis, code generation, instruction scheduling, and register \nallocation. The code produced by cmcc is as good as the code pro-duced by the native compilers for the \nMIPS and SPARC, al-though significantly less resources have been spent on cmcc (overall, about 6 man \nyears by 2.5 persons). cmcc is imple- mented in C++, which allowed for a compact expression of the frameworks \nas class hierarchies. The results support the claim that suitable frameworks facilitate reuse and thereby \nsignificantly improve developer effectiveness. Introduction A well-chosen set of application-specific \nframeworks results in significant code reuse, which is a requirement for con-cise code. In this paper, \nwe report on our experience with using this software technology for an optimizing compiler. Optimizing \ncompilers have a (well-deserved) reputation of being large and complex software systems. This complexity \nhas three direct consequences: (1) maintenance of the com-piler is expensive, (2) retargetting a high-quality \ncompiler to a different instruction set architecture is difficult, and (3) ex- A. Adl-Tabatabai s current \naddress: Intel Corporation, RN6-18. 2200 Mis- sion College Blvd. Santa Clara, CA 95052-8119. This research \nwas sponsored in part by the Advanced Research Projects Agency/IT0 monitored by SPAWAR under contract \nN00039-93-C-0152.  Permission to make digitalhard copy of part or all of this work for personal or classroom \nuse is ranted without fee provided that copies are not made or distributed for pro BIt or commercial \nadvantage, the copyright notic% the title of the publication and its date appear, and notice is given \nthat copying is by permission of ACM, Inc. To copy otherwise, to republish, to post on servers, or to \nredistribute to lists, requires prior specific permission and/or a fee. OOPSIA 96 CA, USA Q 1996 ACM \nO-89791 -788-x/96/001 0...$3.50 perimentation with or extension of the compiler platform is often a challenge. \nThe need to address the first two items is obvious, but in production compilers, the third issue is important \nas well. Given the complexities of current processors, no sin-gle heuristic produces consistently superior \nresults. Conse-quently, compilers try multiple heuristics and pick the best result. For example, the \nRS/6000 compiler [2] tries three different register-allocation heuristics, and then picks the al- location \nwith the least spill code. The SGI MIPSpro com-piler [29] uses 4 different scheduling strategies. Such \nan ap- proach to structuring the compiler requires a high degree of flexibility (i.e., the ability to \nmodify some part of the compiler while keeping the rest of the infrastructure unchanged). This flexibility \nis even more crucial in an aggressive returgetable compiler. Various researchers and practitioners have \nembraced the paradigm of object-oriented design and implementation to achieve modularity and to increase \ncode reuse [15, 281. Generic class libraries for basic data structures such as linked lists, stacks, \nqueues, heaps, and so on, provide a first level of code reuse [30, 261. Such basic data structures are \npervasive inside a compiler, and thus there is ample opportunity for code reuse based on libraries within \na compiler. However, there exists a second level of code reuse based on patterns or application frameworks \n[ 12, 3 1, 321. An application frame-work captures the control structure of a class of computations but \nleaves specification of the exact functionality to the client of the framework. For example, many optimization \nphases consist of one or more data-flow analysis passes followed by transformations based on the result \nof the analysis. Code can then be reused by developing a single application framework for global data \nflow. This framework factors out code that is common among different phases, such as iterations over \nthe flow graph that modify the In and Out sets of each ba-sic block. The framework is parameterized according \nto the characteristics of the flow analysis (e.g., the direction, conflu-ence operation, etc.); a client \nof the framework then provides the details (e.g., in the form of a function to initialize the In and \nOut sets). Our goal for the cmcc compiler was to develop a competi- tive optimizing C compiler that is \nuseful as a platform for ar- chitecture and compiler research. This goal implies the need to support \neasy retargetting (to generate code for different machines). Given the small number of people involved \n(the authors), we strive for high internal (between modules) and external (between versions for different \ntargets) code reuse. Our definition of code reuse is rather strict -we do not permit any modification \nto reused code, since the compiler supports multiple targets. If the reused code is included un-changed \nin all compilers, then all versions of the compiler can benefit from advances or changes. This situation \nis different from those environments where stand-alone applications are built: Two systems may start \nwith the same class library but then are permitted to change it according to their respec- tive needs. \nWe note that there is no generally accepted definition of reuse. Even changes of up to 25% of the source \ncode have been classified in the literature as reuse, with slight revi-sions [23]. Although such a form \nof reuse is better than developing code from scratch, it nevertheless violates our re- quirement that \nreuse is based on complete, unchanged use of the code base. In this paper, we report how application \nframe-works increase code reuse and ease compiler extensions (i.e., support the addition of new optimizations, \ndifferent orderings among transformations, and new targets). Since we do not allow any modification to \nthe reused code, our experience reported here makes a conservative statement about the reuse through \nframework technology. Other environments might report even higher degrees of reuse if slight modifications \nare allowed. In the next section, we briefly establish that cmcc is a competitive optimizing compiler. \nThen, in the subsequent sections, we describe the use of frameworks in the global data-flow analysis, \ncode generation, instruction scheduling, and register-allocation modules of the compiler. 2 The cmcc \ncompiler We developed an optimizing, retargetable compiler (the Carnegie Mellon C Compiler or cmcc) for \na complete pro-gramming language (ANSI C), with all the warts and sub-tleties that a real language implies. \nResults for language subsets (e.g., pointer-less languages) are difficult to extrapo- late to other compilers. \ncmcc is coded in C++ and produces assembly language for a variety of target machine architec-tures. It \nuses the ICC ANSI C front end [lo]. Table 1 lists the global optimizations performed by cmcc. The global \nregister allocator integrates live range split-ting [24] with a graph-coloring register allocator [6]. \nRegister allocation is done as part of code generation (although it is listed here as an optimization, \nit is not part of the global op-timizer). Loop unrolling and peeling Linear function test replacement \nInduction variable expansion Induction variable simplification Constant propagation and folding Induction \nvariable elimination Assignment propagation Partial dead code elimination Dead assignment elimination \nPartial redundancy elimination Strength reduction Branch optimizations Instruction scheduling Global \nregister allocation (using graph coloring) Register coalescing Table 1: Optimizations performed by cmcc \nMIPS 1 SUN program gee -02 1 cc -0 1 cc -0 Table 2: Performance of optimized code generated by cmcc, \nrelative to optimized code generated by gee (version 2.3.2) and MIPS cc on a DECstation 5000/2OO, and \nSUN cc on a SPARC. Table 2 contrasts the optimized code generated by cmcc with the optimized code produced \nby gee and the native MIPS cc compiler for the MIPS architecture, and with the code produced by the native \nSUN cc compiler for the SPARC architecture. (This table presents the performance relative to these compilers; \na number of less than one means that cmcc produces better code.) The sample programs are the C programs \nof the SPEC suite. Overall, even without serious performance tuning, cmcc produces code that is roughly \nof the same quality as the code produced by the other compilers. The global optimizer s interface to \ncmcc s code generator is encapsulated inside an abstract code generation class (this is an example of \nan inre@zce class [3 11). Each target s code generator inherits from this class. The register allocator \nand instruction scheduler are machine independent and operate on schedules of abstract instructions. \nTo test our claims of retargetability, we chose two extremes of the range of possible target machines. \ncmcc currently generates code for various RISC machines (MIPS, SPARC, DLX) and an experimental LIW machine \n(Intel iwarp). 3 The data-flow analysis framework The majority of global optimizations listed in Table \n1 are based on bit-vector data-flow analyses. Such optimizations are similar in structure, and we can \nreuse code by develop- ing an application framework (the data-flow analysis (DFA) application framework) \nfor such problems. 3.1 Description Informally, a DFA problem is defined by several parameters (see the \nappendix for a more detailed description): Direction The direction of a DFA can be either forward or \nbackward. Examples of forward analyses include avail-able expressions and reaching definitions. Example \nof backward analyses include anticipated expressions and live variables. Confluence The confluence operation \ncan be either the con- junction or disjunction operator, resulting in either an all paths or any paths \nproblem respectively. Available expressions and busy expressions are all paths prob-lems, while reaching \ndefinitions and live variables are any paths problems. Initial In and OZL~ sets The initial value of \nthe In (Out) bit vector at the source (sink) node in the control-flow graph depends on the data-flow \nproblem. For instance in live variables, global variables are considered live at the sink node and local \nvariables are not live, while in available expressions all expressions are considered not available at \nthe source node. The initial values of the In and Out bit vectors at all other nodes depend on the confluence \noperator. Gen and Kill sets The Gen and Kill sets are defined by the data-flow analysis problem. Transfer \nfunctions The transfer functions relate the In and Out sets at each node in the control-flow graph. The \nmost common transfer functions are Out = (In -Kill) + Gen for forward flow problems and In = (Out -Kill) \n+ Gen for backward flow problems. To solve a DFA problem, the transfer functions are repeatedly solved \nat each node in the control-flow graph until a fixed point is reached. To speed up this computation for \nforward flow problems, the transfer functions can be solved in reverse post order of the control-flow \ngraph. To speed up the solution for backward flow problems, the blocks can be visited in the reverse \norder of the forward flow case. In general, unidirectional DFA problems can be catego- rized into four \ntypes according to their direction and conflu- ence operator: (1) forward flow all paths, (2) forward \nflow any path, (3) backward flow all paths and (4) backward flow any path. DFA-based optimizations differ \nmainly in their Gen and Kill sets, the initial value of the In (Out) set at the source (sink) node in \nthe control-flow graph, and how the results of the data-flow analysis are used. cmcc s DFA framework \nis organized as a C++ class hi-erarchy, as depicted in Figure 1. A new DFA problem is defined as a C++ \nclass that inherits from one of four classes: FwdAndData-Flow, Fwd-Or-Data-Flow, BwdAndData-Flow, and \nBwd-Or-Data-Flow. Each one of these classes corresponds to one of the DFA prob-lem categories defined \nabove. These classes, however, are abstract classes; that is, they contain declarations of virtual functions \nwhose implementations must be supplied by inher- iting classes. To create a concrete class, an inheriting \nclass must specify a virtual function for initializing the Gen and Kill sets at each block, a virtual \nfunction for allocating bit vectors, and a virtual function for using the results of the DFA at each \nbasic block. To specify most DFA problems, one need only to define the three virtual functions described \nabove. However, not all DFA problems are so neatly similar, and some need to specify other parameters. \nSome problems need more complicated transfer functions; for instance, lazy code motion as described in \n[ 171, or bidirectional data-flow problems such as the classic partial redundancy algorithm of [25]. \nOther problems need to define the initial value of the Tn (Out) set of the source (sink) node (e.g., \nlive variables). To satisfy these types of problems, an inheriting class can over-ride the default virtual \nfunction for performing the transfer function at a block, and the default virtual function for initial- \nizing the In or Out set of the source or sink node (depending on the direction of the DFA). Figure 2 \nhighlights those member functions and fields that are of interest for this class. The GEN, KILL, IN and \nOUT arrays hold pointers to bit vectors for each basic block and are indexed by basic block numbers. \nBit vectors are allo-cated by the ini t ( ) method. f g-i t er is a generic iterator that iterates over \nbasic blocks. This member is initialized by inheriting classes, to an iterator that iterates in the for-ward \nor backward direction in the control-flow graph. The confluence () and transfer () methods perform the \nconfluence operation and transfer functions at a basic block. trans ( ) is a default transfer function \nthat can be used by inheriting classes. The fixed point computation is performed by the df a ( ) method; \nthe definition of this method is shown in Figure 10 in the appendix. When the fixed point is computed, \npropagate ( ) is called for each basic block. An inher-iting class uses the results of the data-flow \nanalysis in the propagate () method. In the next level of the hierarchy, there are the FwdData-Flow and \nBwdData-Flow classes, which in-herit from the Data-Flow class. In the appendix, Figure 11 shows Fwd-Data-Flow, \nthe other class is symmetric. Ewd-Data-Flow Fwd-DaLeFlow Bwd-Or-Data-Flow Bwd-And-Data_Flow Fwd-Or-Data-Flow \nFwd-And-Data-Flow Figure 1: Overview of class hierarchy. class Data-Flow { protected: BV **GEN,**KILL,**IN,**OUT,*tmp; \n Block *src-block, *sink-block;  Block-Iter *fg-iter; // iterates over blocks in CFG virtual void confluence(Block \n*b) = 0;  virtual int transferlint block-id) = 0; BV *trans (BV *in,BV *gen,BV *kill) {*tmp = *in;*tmp \n-= *kill;*tmp += *gen;return tmp;} virtual void init-Root0 = 0;  virtual void init....In-Out(BV *in,BV \n*out) = 0; virtual void init-Gen-Kill(Block *b,BV *gen,BV *kill) = 0; virtual int propagate(Block *) \n= 0; virtual BV* new-BV() = 0; public: Data-Flow(BlockJter *fg-iter);  void dfa() ; // computes the \nfixed point void init(); // allocate bit vectors 1;  Figure 2: The Data-Flow base class. These two classes \nspecialize the Data-Flow base class by 3.2 Reuse defining the direction of the DFA, and thus define all \nthe virtual functions in the Data-Flow classthat depend on Table 3 shows the size (in lines of C++ code) \nof the DFA thedirectionoftheDFA: confluence(),init-Root0 framework. These numbers do not include the \ncommon stan- and transfer ( ) . (Note that transfer ( ) is kept as a dard templates (e.g., lists, iterators, \netc.), since weconcentrate protected member so that inheriting classes can override on the importance \nof frameworks in this paper. Lines of code this method if necessary.) Since the confluence operation \nis are a simple metric to assess reuse. If the frameworks are not yet known, a new virtual function conf \nl-Op ( ) is de- reused by a different team, other metrics like development fined. time, rate and number \nof EC0 (engineering change orders) would provide more insight in the reusability of the system. The third \nlevel of the hierarchy adds the confluence opera-There are two noteworthy aspects: First, the code is \nex-tion to the direction, resulting in four classes ({Fwd, Bwd} x tremely compact (and also highly efficient). \nSecond, most of {Or, And}). F gI ure 3 provides an example of how the DFA the code is placed in the base \nclass, reflecting several itera- framework is applied to the available expressions problem. tions of \nrefinement and (re)factoring. As alluded to in Figure The few lines of code shown in this figure represent \nthe core 3, the use of the DFA framework also allows for compact code of the optimization. in the optimizer, \nsince the framework forms the foundation code generator: Inst and Code-Generator; all target machines \nmust inherit from these two abstract classes. Table 3: Size of DFA framework. for a wide range of optimizations. \nTable 4 lists the sizes of the parts of the optimizer that use the DFA framework. Optimization # of DFA \nLines steps of Code Assignment propagation 1 430 Dead assignment elimination 1 327 Assignment sinking \n2 570 Partial redundancy elimination 4 2506 Live range analysis 2 331 Debugger 5 1836 Table 4: Size \nof clients of the DFA framework. Overall, there are 10 global data-flow analysis passes in cmcc. In \naddition, 5 more data-flow analysis passes are done to implement algorithms for detecting the effects \nof op- timizations on source-level debugging [I]. Comparing the code for the various optimizations, and \nthe number of DFA steps for each optimization, we see that cmcc exhibits sig-nificant reuse. Another \nimportant aspect is not visible in the plain numbers: since DFA is so compact and efficient, we never \nthought twice about using DFA. As a result, the op-timizer is concise (and we even found new uses for \nDFA: enabling source-level debugging of optimized code).  4 Code generation framework There are two \nchallenges for the code generator: modeling the target machine and describing a way to select an appropri- \nate code sequence for each intermediate representation (IR) node. Moreover, since we would like our compiler \nto be re- targetable, we would also like the code generator to address these issues in a machine-independent \nmanner. 4.1 Description The machine-dependent aspects of the code generator are represented by a set \nof abstract classes that hide and encap- sulate target machine details. Code selection, register allo-cation, \ninstruction scheduling, and code emission then be-come totally machine independent by manipulating these \nab- stract classes. Machine-dependent aspects include machine instructions, register banks, instruction \nselection, and so on. We highlight here the two principal abstract classes in the The abstract Ins t \nclass is the code generator s machine-independent representation of an instruction. The machine-independent \nfunctions inside the code generator operate on trees and schedules of machine-independent instructions \nof type Inst. The Ins t class encapsulates all the information that is needed by the machine-independent \nphases of the code generator. Its interface is a set of methods that allow clients to query instruction \ncharacteristics such as the size of immedi- ate(s), the register bank of result register, the commutability, \nthe result latency, and so on. The Inst class also includes methods for code emission: class Inst { Inst \n*src [MAX-SRCS] ; unsigned n-srcs; Register *dst-reg; . . . public : virtual void emit(ostream&#38;) \n= 0; virtual Register-Bank *bank0 = 0; virtual int is-commutable0 = 0; virtual int imm-n-bits ( ) = 0; \nvirtual int inun-sign-extend0 = 0; . . . 1; Each target machine inherits from this base class and pro-vides \nthe definitions of this class pure virtual functions; Figure 4 shows the class hierarchy. Similar instructions \nare grouped into a shared class, simplifying the hierarchy (e.g., Mips-Convert-Inst contains all the \ndata-format conversion instructions of the MIPS architecture). A class X-Special captures those that \ndo not fit; note that there is no class MIPS-Special, all the MIPS instructions fit neatly into one of \nthe other classes. The MCode intermediate rep-resentation, which was developed independently as part \nof the Clarity C++ project, converged to an instruction set that closely resembles this group of instructions \n[22]. The constructor for each of these classes generates an in-struction (node) of the respective type. \nThe data element -Info keeps information about this class of instructions, such as the assembly opcodes, \nwhether an instruction ac-cepts immediate values, which register bank (if any) holds the operands, format \nstrings for the code emitter, instruction latency, and so on. This allows a class higher up in the hier- \narchy to implement most of the virtual functions of the Ins t class (see Figure 13 in the appendix for \nan example). The register allocator, scheduler, and emitter operate on objects of type Inst. Since the \nmachine-specific instruction class Avail-Exprs : public Fwd-And-Data-Flow { public: Avail-Exprs(Block \n*src-blk) : Fwd-And-Data-Flow(src-blk) (1 int propagate(Block*) {} void init-Gen-Kill(Block *block,BV \n*gen,BV *kill); BV *new-BV() (return new-Expr-BV();} I ; Figure 3: An example application MIPS-lnst \n-MIPS-Load-lnst -MIPS_Store-lnst -MIPS-Call-lnst -MIPS-Branch-lnst -MIPS-Jump-lnst -MIPS-Switch-lnst \nMIPS-Return-lnst _ MIPS-Compare-lnst _ MIPS-Add-lnst _ MIPS-Sub-lnst _ MIPS-Mul-lnst -MIPS-Div-lnst -MIPS-Convert-lnst \n-MIPS-Shift-lnst Figure 4: Hierarchy classes are all derived from Inst, this choice allows us keep these \nother modules machine independent. TheCode-Generator classhierarchy The abstract Code-Generator class \nencapsulates all func- tions that generate machine instructions; it includes methods to perform arithmetic \noperations, to copy a block of memory, to get argument registers, and so on. All code is generated through \nthis code generation class: whenever a code gener-ator function needs to generate code, it uses methods \nof the classCode-Generator. For example, the register alloca-tor uses this class to generate spill, caller-save, \nand callee-save code in a machine-independent manner. Figure 5 shows afragment ofthe Code-Generator class. \nA benefit of using an abstract code generator class is that the compiler back end can dynamically select \nthe target code generator, providing the mechanisms to implement afat com-piler (Section 7). That is, \nthe compiler object can contain multiple code generators at the same time. The template 56  of the DFA \nframework: Available expressions. IWARP-Load-lnst IWARP-Store-lnst IWARP-Call-lnst IWARP-Branch-lnst \nIWARPJump-lnst IWARP-Switch-lnst IWARP-Return-lnst IWARP-Compare-lnst IWARP-Add-lnst iWARP_Sub-lnst IWARP-Mul-lnst \nIWARP-Div-lnst IWARP-Convert-lnst IWARP-Shift lnst IIWARP-Speck-lnst of machine instruction classes. \nto Code-Generator-For-shown in Figure 6 allows to con- veniently declare machine-specific code generators. \nAs an example of how these abstract classes are used we consider the code selector. Code selection is \ndriven by the match ( ) function, which matches the nodes of the IR with instructions of the target machine. \nIR nodes are defined by the class Node, and the set of operators of the IR nodes are the same as the \nones defined by the ICC compiler [lo]. The function match ( ) takes an IR node and abstract code generator, \nand returns abstract instructions generated for the IR node: Inst *match(Node *n,Code-Generator&#38; \ncg); The code selection functionmatch ( ) includes the following code for selecting floating point conversion: \nreturn cg.cvtdf(match(n->left,cg)); // returns instruction that // converts left subtree  struct Code-Generator \n{ const MachineKind machine; Code-Generator(MachineKind mc) : machine {) . . . // many omitted virtual \nInst *or(Inst*,Inst*) = 0; // bitwise inclusive or virtual Inst *ld8(int is-signed,Inst*) = 0; // load \nbyte virtual Inst *cvtdf(Inst*) = 0; // convert double to float virtual Inst *cvtdi(Inst*) = 0; // convert \ndouble to integer . . .  Figure 5: Fragment of Code-Generator class. Each machine-specific code generator \nrealizes the virtual functions defined in the abstract Code-Generator class. For example, the MIPS and \niWarp code generators contain: Inst *MIPS-Code-Generator::cvtdf(Inst *src) ( return new MIPS-Convert-Inst \n(MIPS-Convert-Inst::cvtds,src); 1 Inst *IWARP-Code-Generator::cvtdf(Inst *src) { return new IWARP-Convert-Inst \n(IWARP_Convert_Inst::cvtd2f,src); I cvtds and cvtd2 f are names chosen by the implemen- tors of each \ncode generator respectively; these names are usu- ally chosen based on the mnemonics of the assembler. \nIn the above case, the two methods look almost identical, because both machines support a direct conversion \nfrom double to float. Conversions between from double to integer (cvtdi) are different. In this case, \non the MIPS the (integer) result ends up in a special register and must be moved to an integer register; \non the iWarp, there is only a single register bank: Inst *MIPS-Code-Generator::cvtdi(Inst *src){ src \n= new MIPS-Convert-Inst (MIPS-Convert-Inst::cvtdw,src); return new MIPS-Mov-Inst (MIPS-Mov-Inst::mffp-lo,src); \nI Inst *IWARP-Code-Generator::cvtdi(Inst *src){ return new IWARP-Convert-Inst (IWARP_Convert_Inst::cvtd2w,src); \n1  4.2 Reuse Tables 5 and 6 present the number of lines of code for the machine-specific parts and \nthe common part of the code gen- erator. Table 5: Size of code generators. The category Mist in Table \n6 includes the generic base classes for instructions (Ins t), registers (Register), reg- ister banks \n(Register-Bank), and so on. For each code generator, approximately 75% of the code is reused. The small \nnumber of lines for each code generator renders the use of code-generator generators unnecessary. Module \nLines of Code Code emitter 575 Code selector 1165 Scheduler 970 Register allocator 7640 Mist 4245 Total \n14595 Table 6: Size of generic code generator. Another data point to gauge the difficulty of producing \na code generator is that one of the authors developed the SPARC code generator in about a week (after \ndeveloping the MIPS code generator). The other code generators evolved as part of the compiler development, \nso their development time is not a meaningful metric. template <MachineKind MACHINE> struct Code-Generator-For-: \nCode-Generator { Code-Generator-For-() : Code-Generator(MACHINE) {} . . . Inst *or(Inst*,Inst*); Inst \n*ld8(int signed,Inst*); Inst *cvtdf(Inst*);  . f . 1 // Mips code generator declarations typedef Code-Generator-For-<MIPS> \nMIPS-Code-Generator; MIPS-Code-Generator -mips-code-generator; // Iwarp code generator declarations \n typedef Code-Generator-For-<IWARP> IWARP-Code-Generator; IWARP-Code-Generator -iwarp-code-generator; \n // Functions for selecting code generators Code-Generator&#38; mips-cg() {return-mips-code-generator;} \nCode-Generator&#38; iwarp-cg() {return -iwarp-code-generator;} Figure 6: Declarations of code generators. \n5 Scheduling framework by the implementation of the machine). Instruction scheduling is an important \noptimization for 5.1 Description pipelined and superscalar machines. Schedulers, however, are often handcrafted \nand fine-tuned, so this part of a compiler Scheduling must preserve two kinds of constraints: prece- \ndeserves attention by the developers of application-specific dence constraints and resource constraints. \nPrecedence con-frameworks. straints indicate the minimum latency between an instruction In the cmcc compiler, \ncode scheduling is a separate code that produces a value and instructions that use the value. We generator \nphase that takes and produces schedules of instruc- model the precedence constraint between two instructions \ntions. Like the other code generation phases, scheduling oper-with an arc that is annotated with the \nresult latency (in clock ates on the Ins t class, and is thus machine independent. The cycles) of the \nproducer instruction. Constructing precedence purpose of scheduling is to reorder instructions such that \nthe arcs is done in a machine-independent manner; the Inst target machines resources are used more effectively. \nThere-abstract class (see previous section) encapsulates all the nec- fore, the scheduler must accurately \nmodel the result latencies essary dependence and latency information. and resource requirements of each \ninstruction, while still be- Resource constraints ensure that there are enough ma-ing machine independent. \nMoreover, for multiple-issue (i.e., chine resources (e.g., decode slots, functional units, re-superscalar \nand VLIW) machines, the scheduler must ac-sult buses, etc.) for each scheduled instruction. Re-curately \nmodel the instruction fetch and decode constraints sources are machine dependent, so cmcc models ma-of \nthe target machine. For a VLIW machine, the scheduler chine resources with another abstract interface \nclass packs instructions (since each instruction contains multiple called the Resource class. This class \ndeclares two operations), while for a superscalar machine, the scheduler virtual functions that the scheduler \nuses for manipu-schedules instructions into an instruction window that models lating resources: struct-hazard \n(Resource*) and the instructions that are fetched and decoded as a unit by the subscribe(Resource*). \nThe structhazardo processor at runtime (the size of the window is determined function takes a request \nfor resources (at a particular slot in the schedule) and checks whether the resources are available. \nThe subscribe ( ) function books the request. Each target machine inherits from the Resource class and \ndefines the implementation of these two virtual functions. The Ins t and Resource abstract classes allow \nthe code scheduler to perform scheduling in a machine independent fashion. List scheduling is the basic \nlocal scheduling technique used in cmcc (and many other compilers); it consists of four steps: (1) initializing \na ready list that holds all instructions that are ready to be scheduled (ready instructions), (2) choosing \na ready instruction I based on a candidate selection heuristic, (3) inserting I into the schedule after \nmaking sure the resource constraints of 1 are satisfied, and (4) adding into the ready list those instructions \nwhose precedence constraints are satisfied by the scheduling of I.  The direction of code scheduling \nis determined by the order in which instructions are considered (i.e., placed on the ready list). If \ninstructions are placed on the ready list after their descendants are scheduled, the code scheduler is \na forward scheduler. In this case, the scheduler starts with the producers of values. If instructions \nare place on the ready list after their parents are scheduled, the scheduler works backward; that is, \nit starts with the consumers of values. The manner in which candidate instructions are selected and inserted \ninto the schedule in steps 2 and 3 of the list scheduling algorithm can be either instruction oriented \nor cycle oriented. An instruction-oriented scheduler chooses a ready instruction (using the candidate \nselection heuristic), inserts the instruction into a schedule slot such that the prece- dence and resource \nconstraints are satisfied, and then modi- fies the ready list. A cycle-oriented scheduler steps through \nthe schedule cycle-by-cycle. For each cycle, it chooses a ready instruction whose precedence and resource \nconstraints are satisfied in the current cycle (using a candidate selection heuristic to break ties if \nthere is more than one such instruc- tion), inserts the instruction into the schedule, and then mod- \nifies the ready list. The Scheduler class hierarchy is depicted in Figure 7. The Scheduler-FWD and Scheduler-BWD \nclasses spe- cialize the scheduler based on the direction (forward or back- ward). The bottom level of \nthe hierarchy determines whether the scheduler is cycle or instruction oriented. The Scheduler class \ndeclares two pure virtual func- tions that determine the candidate selection heuristic but are not defined \nin the class hierarchy: choose-cand ( ) and init-candheuristic 0. Clients define the candidate selection \nheuristic by defining these two functions. Among the possible candidate selection heuristics are maximum \nde- lay from root, number of precedent instructions, and result latency. Building one or more schedulers \nusing this framework is straightforward: simply inherit from one of the four bottom levels of the hierarchy \nand provide the candidate selection heuristic. In this manner, a compiler can mix-and-match from a number \nof different scheduling strategies and heuris- tics, and try different approaches for either different \ntarget machines (if they have different hardware features) or even different programs [29]. Note that \nthis scheduling framework almost completely covers the range of possible local list schedulers. 5.2 \nReuse Table 7 shows the size of the scheduler framework. The cat-egory Mist includes bookkeeping, computation \nof prece- dence arcs, and so on. Given the framework, specifying a scheduler requires less than 100 lines \nof code, as can be seen in Table 8, which lists the sizes of several schedulers built as part of the \nproject. Level 1 Lines of Code Base 282 FwdlBwd direction 60 Cycle/Inst. orientation 31 Mist 597 Total \n970 Table 7: Size of scheduler framework. Scheduler heuristic Kind Lines of Code Number of descendants \nFwd-Inst 56 Execution time Fwd-Cyc 55 Max path from root Bwd-Inst 66 Max bwd. operation latency Bwdlnst \n68 Max bwd. cycle latency Bwd-Cyc 69 Table 8: Sizes of different schedulers. The amount of code reuse \ndepends also on the aggressive- ness of the compiler and the tolerance of the user for compi- lation \nspeed. Clearly, as a compiler attempts to find the best schedule based on a number of heuristics [29], \nthis framework is reused multiple times in a single compiler. cmcc currently performs only one scheduling \nstep, but we have implemented several scheduling heuristics and are actively investigating the application \nof multiple scheduling heuristics to a single compilation unit. 6 Register-allocation framework Register \nallocation is an important optimization for modern processors that exhibit high memory access latencies. \nLike instruction schedulers, register allocators are usually hand-crafted and extensively fine-tuned. \nIn the cmcc compiler, register allocation is performed after code scheduling. The cmcc register allocator \ntakes a scheduled code sequence in which instructions use virtual registers and assigns physical registers \nbased on graph coloring. Figure 7: Hierarchy of scheduler classes. There exist a number of approaches \nto graph-coloring based register allocation; for example, Chaitin-style color-ing [7,2], optimistic coloring \n[4], priority-based coloring [9] and several modern approaches [3, 5, 27, 241 that take the program structure \ninto account when splitting live ranges. It is difficult and impractical to implement and experiment \nwith these different register-allocation approaches without a register-allocation framework. In addition, \na framework fa-cilitates afair comparison; that is, if an improvement is added, we would like ail register \nallocators to benefit from the im- provement rather than biasing in favor of a single approach. 6.1 Description \nWe identify seven phases in the register allocator: graph construction, Live-range coalescing, color \nordering, color assignment, graph reconstruction, spill-code insertion, and shufJle-code insertion (as \nillustrated by Figure 8). The graph- construction phase builds the interference graph for the input instruction \nsequence. The color-ordering phase heuristically determines the order in which live ranges are to be \nassigned colors. The color-assignment phase assigns colors to live ranges based on the ordering computed \nby the previous phase; this phase uses the interference graph to make sure that con-flicting live ranges \nare assigned different colors. If either of the color-ordering or color-assignment phases spills a live \nrange, the register allocator rebuilds the interference graph and restarts from the color-ordering phase. \nOur register allo-cator reserves no registers for spill code; that is, the register space is not divided \ninto local and global registers. There are two ways to reconstruct the interference graph: (1) spill \ncode (a load/store) is inserted into the schedule im-mediately before/after a use/definition, the interference \ngraph is thrown away, and the whole coloring process restarts from the first phase; (2) the existing \ninterference graph is modi- fied by producing a new small live range for each occurrence of a spilled \nlive range (no spill code is inserted; the newly produced live ranges span only one instruction), and \ncoloring proceeds with the modified graph. The first approach reuses the code for constructing the interference \ngraph. The second approach favors compilation speed, because the interference graph is not rebuilt from \nscratch. cmcc takes the second approach (i.e., it reuses the graph, to save compilation time). Spill \ncode and shuffle code, respectively, are inserted into the final schedule by the last two phases, spill-code \ninsertion and shuffle-code insertion. Shuffle code moves a data value between the different storage locations \nassigned to a split live range. There are two stacks that are used as the interface be-tween the color-ordering \nand color-assignment phases: (1) thecolor stack(C), and(2) the spill stack(S).Thecolor-ordering phase \ndetermines the order in which live ranges are assigned colors and pushes live ranges onto the C stack \nbased on the ordering-the higher the position within C, the higher the priority. The color-assignment \nphase pops live ranges from C and finds legal colors (i.e., non-conflicting colors) for them. The S stack \nis a pool for keeping spilled live ranges. If the color-ordering phase decides to spill live ranges, \nor the color-assignment phase fails to find legal colors for live ranges, those spilled live ranges are \npushed onto S. 6.2 Modeling existing register-allocation ap-proaches This section elucidates how some \nexisting register-allocation approaches fit into the register-allocation framework. A com- parative evaluation \nof the various approaches is beyond the scope of this paper. Chaitin-style coloring [7, 21 uses a simplijication \nprocess to determine the coloring order. Simplification removes all unconstrained live ranges (degree \n< N); when a node is re- moved, all edges incident upon the node are also removed, opening up further \nopportunities for simplification. If simpli- fication blocks (all nodes have degree 2 N), a least-cost \nlive range is spilled so that the simplification process can proceed. This approach makes spilling decisions \nin the color-ordering phase. Optimistic coloring [4] also uses the simplification process Reconstruction \nGraph Construction Figure 8: Structure of the register allocator. to determine the coloring order. When \nthe simplification pro-cess blocks, however, rather than spilling, optimistic coloring pushes a least-cost \nlive range onto the C stack, removing the live range from the residual graph. In this manner, a live \nrange that would have been spilled by the Chaitin-style ap-proach is given another chance at receiving \na register in the color-assignment phase. Optimistic coloring defers spill de-cisions until the color-assignment \nphase, which spills those live ranges that cannot be assigned legal colors. Priority-based coloring [9] \ndetermines the coloring order according to a priority function. This approach first seperates out the \nunconstrained live ranges since unconstrained live ranges can always be assigned a legal color. If a \nlive range cannot be assiged a legal color (because of conflicts with the neighbors colors), then priority-based \ncoloring splits this live range, The framework shown in Figure 9 is general enough to model the preceeding \napproaches to coloring-based register allocation . This framework also underlines the similarity of priority-based \ncoloring to optimistic coloring; the difference is that one color ordering is simplification based, whereas \nthe other is priority based. In addition, this framework can also model other approaches to live-range \nsplitting that take the program structure into account, such as the approaches taken by Tera [5], Multiflow \n[I I], and RAG [27], as well as splitting based on the SSA representation [3], and the integrated ap-proach \nto splitting and spilling provided by fusion-style col-oring [24]. A more detailed discussion of these \napproaches can be found in a technical report [24]. The class hierarchy of the register-allocation frame-work \nis depicted in Figure 9. There are two classes: (1) Graph-Coloring, which provides the fundamen-tal framework \nfor coloring-based register allocation, and (2) Graph-Builder, which is responsible for build-ing the \ninterference graph. The Graph-Coloring class (Figure 14 in the appendix) is the engine that drives the \nregister allocation. All phases are imple-mented in Graph-Coloring except the color-ordering and shuffle-code \ninsertion phases; these two phases are declared as virtual functions: color-ordering ( ) de-fines the \nmethod for determining the color ordering, and  i Priority colormg as modeled here differs m one aspect \nfrom the descrip- tion in [9]: it excludes live range splitting (Le., live ranges are spilled instead \nof split when the supply of colors is exhausted). shuffle-code-insertion ( ) defines the method for \ninserting shuffle code for the split live ranges. Each of the various approaches to register allocation \nis encapsu-lated in a class. The class Priority-Based inherits from Graph-Coloring and provides an implementation \nof color-ordering ( ) that computes the color ordering using a priority function. Simplification-Based \nalso inherits from Graph-Coloring and provides a version of color-ordering ( ) that uses the simplification \npro- cess to determine the color ordering. The shuf f lexo- de-insertion ( ) methods ofthe PriorityBasedand \nSimplification-Based classes are nap functions be- cause no live ranges are split by these approaches \nto register allocation. The class Fusion-Based [24] inherits from Simplification-Basedandaddsanimplementationof \nshuffle-code-insertion ( ) . The register-allocation framework allows live-range split-ting in the graph-construction \nphase; we view live range splitting as a way to build an interference graph that has fewer interferences, \nor potentially requires fewer overhead operations. The class Graph-Builder has two functions, construction \n( ) and reconstruction( ) , which correspond to the graph construction and reconstruction phases, respectively. \nconstruct ion ( ) is declared as a virtual function. The class Conventional inherits from Graph-Builder \nand constructs the conventional interfer- ence graph, which does not have split live ranges; priority-based, \nChaitin-style, and optimistic coloring use this kind of graph construction. The program structure based \nap-proaches to live range splitting require different mechanisms for building the interference graph; \nfor example, RAG needs the program dependence graph (PDG) structure [27], the Tera approach requires \nthe loop hierarchy [5], and the Mul- tiflow register allocator needs a trace picker [ 111. The class \nSplitting-Style provides the framework for splitting live ranges during construction of the interference \ngraph. The classes Single-Bb, Loop, Ssa, Pdg, and Trace all in- herit from Splitting-Style and provide \nthe appropriate method to construct the interference graph in the desired form. 6.3 Reuse Table 9 lists \nthe sizes of different coloring approaches. The Simplification Based and Priority Based entries give \nthe size of the code that invokes the simplification process and Graph-Builder Graph-Coloring Splitting-Style \nConventional Simplification-Based Priority-Based Figure 9: Class hierarchy of register-allocation framework. \nsorting routine, respectively. The Fusion Based category contains primarily the code for shuffle-code \nplacement. The Mist category consists of classes for live ranges, interfer-ence edges, assigned-color \ninformation, etc. Graph Coloring Lines of Code Base 2543 Simplification Based 40 Priority Based 71 Fusion \nBased 992 Mist 1859 Table9: Size of Graph-Coloring. Table 10 gives the size of the hierarchy of graph \nbuilders. Presently, the cmcc compiler implements the SingleEb and the Loop approaches to live-range \nsplitting (i.e., single basic blocks or loops define the extent of a live range unit; see [24] for more \ndetails). Graph builder Lines of Code Base 1281 Splitting Style 471 Conventional 18 Single Bb 19 Loop \n39 Table 10: Size of Graph-Builder.  Table 11 shows the size of each phase of register alloca-tion. \nThe two entries for Color ordering and Shuffle-code insertion are zero because these virtual functions \nmust be defined by each client (register allocator). Some phases that are not shown in Figure 9 are classified \ninto the Mist cat-egory (e.g., insertion of caller-save code, data-flow analysis for building the interference \ngraph, static estimation of ex-ecution frequency (if no profiling information is available), selecting \ndifferent register-allocation approaches, etc.) In addition to code reuse across different register al-locators \nwhich we obtain by basing a register allocator on the Graph-Builder and Graph-Coloring frame-Module Lines \nof Code Graph-Builder Graph reconstruction 291 Graph construction 990 Total (Base Table 10) 1281 Graph-Coloring \n Coalescing 63 Color ordering 0 Color assignment 554 Spill-code insertion 394 Shuffle-code insertion \n0 Mist 1532 Total (Base Table 9) 2543 Total 3824 I I I Table 11: Size of register-allocation framework. \n works, code is shared by the Splitting-Style and Simplicafication-Based classes. Both of these classes \nmanipulate the interference graph through simplifi-cation. Simplification determines color ordering and \nis also used to find the splitting points (for register allocators de-rived from Splitting-Style [24]). \nThe shared code for graph coloring/building of Table 12 is also organized as a framework. This structure \nallows different register-allocation approaches to use different heuristics to relax the graph con-straints \nwhen the process blocks. For example, Chaitin-style coloring spills the least-cost (spill-cost/degree) \nlive range, optimistic coloring chooses the least-cost live range but does not spill, fusion-style coloring \nsplits live ranges rather than spilling. Each method inherits from Base and provides the implementation \nof what to do when the simplification process blocks. Table 12 lists the sizes of this shared code. For \nexample, the code for Chaitin-style coloring consists of Graph-Coloring (3824) (Table 1 I>, Simplification-Based \n(40) andthe Misc category (1859) of Table 9, Conventional (18) (Table IO), and the code for the Simplification-Process \n(155)with Chaitin-style heuristics (20) (Table 12). Given this frame-work, 67 % of the code is part of \nthe common code base. Simplification Process l- Allocator 1 Lines of Code [71 [41 Base X X Chaitin Style \nX Optimistic X Splitting Spilling Table 12: Shared code for graph building and coloring.  One of the \nbenefits of this framework is that it allows us to evaluate different register-allocation heuristics \neasily. More-over, a fair comparison among different approaches is now possible; for example, when some \nenhancement of a heuris- tic is added to Simplification-Based, all approaches based on simplification \nbenefit from this improvement. 7 Management of multiple targets Most compilers maintain a separate executable \nfor each new target. This creates problems for version management and validation, as each target s compiler \nexecutable must be built and maintained separately. We avoid this problem by having a fat compiler, a \nsingle compiler executable that can gener-ate code for any target. This can be done by dynamically selecting \none of several implementations of a code genera-tion interface, a strategy that has also been used explicitly \nby other projects[ IO], but that falls out of our frameworks-based implementation for free. The class \nsystem for global data-flow analysis stabilized after the second revision. We are now at the third iteration \nof the code generator. Readers should not be discouraged by the number of iterations. Each iteration \nreduced the amount of code, a step we consider to be important for the long term. Also we fought at the \nsame time with different C++ compil- ers. In the process of moving from one compiler to another, on a \nvariety of platforms, we encountered more than one un-pleasant surprise. (We used, with different success, \ng++ on Suns and Alphas, the DEC Alpha C++ compiler, SGI s C++ ( DELTA compiler), MetroWerks Gold, and \nSymantec C++ (the last two on a 68K-based Macintosh).) Although it is hard to quantify the contribution \nof the changing compiler platforms to the development cost, we are beginning to see a stabilization of \nthe environments. 8 Related work Although there exists a large body of literature introducing application-specific \nframeworks, empirical studies are hard to come by. [14] describes experience for a different ap-plication \ndomain (protocol software for the signalling mod-ule). Although compilers provide a nice domain for the \nuse of object-oriented technology, there have been only a few re- ports. Most ofthese focus on front-end \nissues. [21] advocates a syntax-directed view of objects. The use of application frameworks has been \nreported in the context of a Smalltalk compiler: [ 131 discusses front-end issues (parsing, semantic \nanalysis), [ 161 presents a toolkit for constructing code optimizers for Smalltalk. It is interesting \nto contrast the objectives and decisions of [16] and cmcc. [ 161 aims to support the construction of \na range of machine- specific optimizers; it is based on RTL (the Register Transfer Language). cmcc uses \naggressive optimization technology in a machine-independent optimizer, register allocator, and instructions \nscheduler, and strives to keep the different code generators coupled, in the sense that they continue \nto benefit from improvements to the shared core compiler system. To the best of our knowledge, no empirical \naccount of the im- pact of the framework technology on code reuse in the RTL system has been published. \n(Another difference is that [ 161 deals with Smalltalk and cmcc with C.) [ 151 includes a brief description \nof a simple object diagram compiler, and some of the patterns described in [ 121 are derived from translation \nsystems. However, we are not aware of any investigation of frameworks for a compiler s global optimizer \nor retargetable back end, yet these components determine the success of an optimizing compiler. 9 Conclusions \nThe low-level objects to be used by an optimizing compiler are well understood -the usual data structures \nfrom an ad- vanced undergraduate algorithms class go a long way, and there exist a number of class libraries \nfor common data struc- tures. However, the design of frameworks for global data-flow analysis, instruction \nscheduling, and register allocation as well as the set of abstract classes that form the founda-tion \nfor the code generator, allow us to reuse code beyond low-level building blocks. These frameworks contribute \nsignificantly to the organi-zation of the compiler code. The code is clean and concise, easy to understand, \nand has been reused a large number of times: the code for the data-flow module has been reused 15 times, \nthe code generator code is reused 3 times, and more than 75% of the code for a code generator is actually \npart of the common code base. The domain of optimizing compil-ers provides a further example for the \npower of the idea of application frameworks. The abstract classes of the code generator play a crucial \nrole in our plan to build a retargetable system, but their im- portance extends this aspect. These abstract \nclasses allow us to make the compiler extensible. The code selector, register allocator, instruction \nscheduler, and code emitter are written to work on objects of type Ins t, although the actual objects \nareoftype MIPS-Inst, IWARP-Inst,or SPARC-Inst. By using this one base class, we are now in a position \nto ex- tend the compiler, for instance, to reorder phases or to add optimizations. Another, although \nmore indirect, metric to gauge the bene- fits of frameworks in this application domain is the mere fact \nthat a group of 2.5 persons was able to construct a competitive compiler. (And constructing this compiler \nwas not the only activity for this group.) Good advice on compiler construc-tion is hard to come by -the \ncmcc project has demonstrated how modern programming technology can be employed by a small team to produce \na high-quality optimizing compiler, in a short period of time. References Ul A. Adl-Tabatabai and T. \nGross. Source-level debugging of scalar optimized code. Conf on Prog. Language pages 33-43. ACM, May \nPI D. Bernstein, D. Q. In Proc. ACM SIGPLAN 96 Design and Implementation, 1996. Goldin, M. C. Golumbic, \nH. Krawczyk, Y. Mansour, I. Nahshon, and R. Y. Pin- ter. Spill code minimization techniques for optimizing \ncompilers. In Proc. ACM SIGPLAN 89 Con&#38; on Prog. Language Design and Implementation, pages 258-263. \nACM, July 1989. [31 P. Briggs. RegisterAllocation via Graph Coloring. PhD thesis, Rice University, April \n1992. [41 P. Briggs, K. D. Cooper, K. Kennedy, and L. Torczon. Coloring heuristics for register allocation. \nIn Proc. ACM SIGPIAN 89 Conf on Prog. Language Design and Im- plementation, pages 275-284. ACM, July \n1989. [51 D. Callahan and B. Koblenz. Register allocation via hierarchical graph coloring. In Proc. ACMSIGPLAN \n91 Conf on Prog. Language Design and Implementation, pages 192-203, Toronto, June 1991. ACM. [61 G. J. \nChaitin. Register allocation and spilling via graph coloring. In Proc. ACM SIGPLAN 1982 Symp. on Com-piler \nConstruction, pages 98-105, June 1982. In SIG- PLAN Notices, v. 17, n. 6. 171 G. J. Chaitin, M. A. Auslander,A. \nK. Chandra, J. Cocke, M. E. Hopkins, and P W. Markstein. Register alloca-tion by coloring. Research Report \n8395, IBM Watson Research Center, 198 1. F31 F. Chow. A Portable, Machine-independent Global Op-timizer \n-Design and Measurements. PhD thesis, Stan-ford University, 1984. [91 F. C. Chow and J. L. Hennessy. \nA priority-based col-oring approach to register allocation. ACM Trans. on Prog. Lang. Syst., 12:501-535, \nOct. 1990. UOI C. Fraser and D. Hanson. A Retargetable C Compiler: Design and Implementation. Benjamin/Cummings, \n1995. [Ill S. Freudenberger and J. Ruttenberg. Phase ordering of register allocation and instruction \nscheduling. In R. Giegerich and S. L. Graham, editors, Code Gen-eration -Concepts, Tools, Techniques, \npages 146170. Springer Verlag, 1992. [121 E. Gamma, R. Helm, R. Johnson, and J. Vlissides. De-sign Patterns, \nElements of Object-Oriented Sofhyare. Addison Wesley, 1995. [I31 J. Graver. The evolution of an object-oriented \ncom-piler framework. Software: Practice &#38; Experience, 22(7):5 19-535, July 1992. [I41 H. Huni, R. \nJohnson, and R. Engel. A framework for networked protocol software. In Proc. OOPSLA 95, pages 358-369. \nACM, 1995. II51 I. Jacobson, M. Christerson, P. Jonsson, and G. Overgaard. Object-Oriented Sofnyare Engineering. \nAddison-Wesley, 1992. [I61 R. Johnson, C. McConnell, and J. Lake. The RTL system: A framework for code \noptimization. In R. Giegerich and S. L. Graham, editors, Code Gen-eration -Concepts, Tools, Techniques, \npages 255-274. Springer Verlag, 1992. [I71 J. Knoop, 0. Ruthing, and B. Steffen. Lazy code motion. In \nProc. ACM SIGPLAN 92 Conf on Prog. Language Design and Implementation, pages 224-234. ACM, June 1992. \n1181 J. Knoop, 0. Ruthing, and B. Steffen. Lazy strength reduction. .I. ofProg. Languages, 1(1):71-91, \n1993. [I91 J. Knoop, 0. Ruthing, and B. Steffen. Optimal code motion: Theory and practice. ACM Trans. \non Prog. Lang. Syst., 16(4): 1117-l 155, July 1994. WI J. Knoop, 0. Ruthing, and B. Steffen. Partial \ndead code elimination. In Proc. ACM SIGPLAN 94 Conf on Prog. Language Design and Implementation, pages \n147-158. ACM, June 1994. 1211 K. Koskimies. Software engineering aspects in lan-guage implementation. \nIn Proc. 2nd Intl. Compiler-Compiler and High-Speed Compilation Workshop, Lec-ture Notes in Computer \nScience Vol. 371, pages 37-5 1, Berlin, 1989. Springer Verlag. [22] B. Lewis, L. Deutsch, and T. Goldstein. \nClarity Mcode: A retargetable intermediate representation for compi-  lation. In Proc. ACMBIGPLAN Workshop \non Inter- mediate Representations {IR 95), pages 119-128, San Francisco, Jan 1995. ACM. [23] J. Lewis, \nS. Henry, D. Kafura, and R. Schulman. On the relationship between the object-oriented paradigm and software \nreuse: An empirical investigation. Journal of Object-Oriented Prog., 5(4):35-41, 1992. [24] G. Lueh, \nT. Gross, and A. Adl-Tabatabai. Global register allocation based on graph fusion. Technical Report 96-106, \nCarnegie Mellon University, School of Computer Science, March 1996. [2.5] E. Morel and C. Renvoise. Global \noptimization by sup- pression of partial redundancies. Communications of the ACM, 22(2):96-103, Feb 1979. \n[26] S. Naeher et al. Leda, a library for efficient data types and algorithms. Avail. via FTP from sbsvax.cs.uni-sb.de, \n1994. [27] C. Norris and L. L. Pollock. Register allocation over the program dependence graph. In Proc. \nACM SIGPLAN 94 ConJ : on Prog. Language Design and Implementa- tion, pages 266-277. ACM, June 1994. [28] \nJ. Rumbaugh, M. Blaha, W. Premerlani, F. Eddy, and W. Lorensen. Object-Oriented Modeling and Design. \nPrentice Hall, 199 1. [29] J. Ruttenberg, G. Gao, A. Stoutchinin, and W. Lich- tenstein. Software pipelining \nshowdown: Optimal vs. heurisitic methods in a production compiler. In Proc. ACM SIGPLAN 96 Con5 on Prog. \nLanguage Design and Implementation, pages l-l 1. ACM, May 1996. [30] A. Stepanov and M. Lee. The standard \ntemplate library. Tech. Report HP Lab. HPL-94-34,1994. Available from http://www.cs.rpi.edu/-/ musser/stl.html. \n[31] B. Stroustrup. The C++ Programming Language. Addison-Wesley, 199 1. [32] Taligent Inc. Building \nobject-oriented frameworks. http:Nwww.taligent.corn/building-oofw.html, 1994.   A Data-flow analysis \ndetails Here we reproduce the details of the DFA framework from a technical report. A user familiar with \nthis kind of framework may skip this section, it is included for the benefit of those readers without \na background in compiler construction. At the topmost level of the DFA class hierarchy is the Data-Flowclass(Figure \n1). Figure2 highlights thosemem-ber functions and fields that are of interest for this class. The GEN, \nKILL, IN and OUT arrays hold pointers to bit vectors for each basic block and are indexed by basic block \nnumbers. Bit vectors are allocated by the ini t ( ) method. f g-i t er is a generic iterator that iterates \nover basic blocks. This member is initialized by inheriting classes, to an iterator that iterates in \nthe forward or backward direction in the control flow graph. The conf luence ( ) and transfer ( ) meth- \nods perform the confluence operation and transfer functions at a basic block. trans ( ) is a default \ntransfer function that can be used by inheriting classes. The initlioot ( ) method initializes the source \nor sink node s In or Out set, while the init-In-out ( ) method initializes all other nodes In and Out \nsets. The init-GenXill ( ) method initializes a block s Gen and Kill sets. newBV ( ) is a method that \nallocates a bit vector. This method is used by init ( ) to allocate bit vectors to the bit vector arrays. \nini t ( ) must be called before the data-flow analysis is performed. The fixed point computation is performed \nby the dfa ( ) method; the definition of this method is shown in Figure 10. When the fixed point is computed, \npropagate ( ) is called for each basic block. An inheriting class uses the results of the data-flow analysis \nin the propagate ( ) method. In the next level of the hierarchy, there are the FwdData-Flow and BwdData-Flow \nclasses,which inherit from the Data-Flow class. Figure 11 shows Fwd-Data-Flow, the other class is symmetric. \nThese two classes specialize the Data-Flow base class by defining the direction of the DFA, and thus \ndefine all the virtual functions in the Data-Flow classthatdepend on the di- rection of the DFA: confluence \n( ) , initRoot ( ) and transfer ( ) . (Note that the transfer ( ) is kept as a protected member so that \ninheriting classes can override this method if necessary.) These classes also initialize the flow-graph \niterator f g-i ter by allocating an iterator that iterates in the appropriate direction. The confluence \n( ) method defined in the Data-Flow classdependsboth on the direction and confluence operation of the \nDFA. Since the confluence operation is not yet known, a new virtual function conf 1-0~ ( ) is defined. \nNote that since the direction of the DFA is known, we either initialize the In set of the source basic \nblock or the Out set of the sink basic block. Therefore these classes define new virtual methods that \ninitialize these sets:init-Src-In ( ) and init-Sink-Out ( ) . The third level of the hierarchy adds the \nconflu- ence operation to the direction, resulting in four classes ({Fwd,Bwd} x {Or,And}); two of these \nare shown in Fig- ure 12. These classes inherit from the FwdData-Flow and BwdData-Flow classes and define \nthose virtual func- tions that depend on the confluence operation: conf l-Op ( ) and init_In-Out ( ) \n_ This leaves three virtual functions from the base Data-Flow class thatmust be defined by in- vo .d \nData-Flow::dfaO I Block *block; // // initialize in, out, gen and kill sets // fg-iter->resetO; \nwhile (block = fg-iter->next()] 1  init~Gen~Kill(block,GEN[block-~id],KILL[block-~id] ); init-In-Out(IN[block->id],OUT[block->id]]; \n init-RootO; // // compute the fixed point // int changed; do {changed = 0; fg-iter->reset(); while \n(block = fg-iter->next())  (confluence(block); changed += transfer(block->id);) } while (changed); \n // // Propagate results to basic blocks /i fg-iter->reset(];  while (block = fg-iter->next()) propagate(block); \n Figure 10: Fixed point computation. class Fwd-Data-Flow : public Data-Flow { void confluence(Block *b); \nvoid init-Root0 {init-SrcJn(IN[src-block->id]);} protected: virtual void confl-Op(BV*,BV*) = 0; virtual \nint transfer(int id)  {return *OUTrid] = *trans(IN[id],GEN[id],KILL[id]);] virtual void init-Src-In(BV \n*bv) {bv->make-empty();] public: Fwd-Data-Flow(Block *src-blk) : Data-Flow((new Fwd-Block-Iter(src-blk) \n)] {] 1; void Fwd-Data-Flow::confluence(Block *block) ( Block *pred; foreachsred-block(pred,block) \nconfl-Op(IN[block-zid],OUT[pred->id]);  Figure 11: The Fwd-Data-Flow class. heritingclasses: init-GenXill \n( ) , propagate ( ) , and new-BV(). #define or-confl(in,out) *(in) += *(out) #define and-confl(in,out) \n*(in) *= *(out) class Fwd-Or-Data-Flow : public Fwd-Data-Flow { void confl-Op(BV *in,BV *out) { or-confl(in,out);} \nvoid init-In-Out(BV *in,BV *out) {in->make-empty();out->make_empty();l public: Fwd-Or-Data-Flow(Block \n*src-blk) : Fwd-Data-Flow(src-blk) I} 1; class Fwd-And-Data-Flow : public Fwd-Data-Flow I void confl-Op(BV \n*in,BV *out) ( and-confl(in,out);l void init-In-Out(BV *in,BV *out) {in->make-fullO;out->make-fullo;) \npublic: Fwd-And-Data-Flow(Block *src-blk) : Fwd-Data-Flow(src-blk) 0 1; Figure 12: The Fwd-OrData-Flow \nand FwdAnd-Data-Flow classes. Figure 3 shows an example of how the DFA framework is used. This figure \nshows the class definition for the available expressions problem. In cmcc, this DFA is performed as part \nof partial redundancy elimination and its results are used to perform further analysis. Therefore the \npropagate ( ) method performs no action. If the available expressions DFA was being used to eliminate \ncommon subexpressions, propagate ( ) would be defined to iterate through the expressions inside a block \nand replace those expressions whose values are available with fetches from temporary vari-ables. The \ninit-Gen-Kill ( ) method (whose code is not shown) simply iterates through the expressions inside a basic \nblock, computing the block s gen and kill bit vectors. The new-Expr-EW ( ) function (used in new-BV ( \n) to allocate a new bit vector) returns a bit vector suitable for holding a set of expressions. A.1 \nOptimization internals The DFA framework provides the foundation for the global optimization phase. The \nglobal optimization phase first trans- forms the loop structure of the program by partially peeling loops \nand inserting loop preheader blocks. After these con-trol flow transformations, the optimizer performs \nconstant propagation and folding using an algorithm based on abstract interpretation; this phase cleans \nup tests that were peeled out of loops. Assignment propagation and dead code elimination are based on \nthe algorithms described in [ 81, and are repeated until they effect no more changes on the program. \nIn the next step, the optimizer performs partial redundancy elimination using the algorithm described \nin [ 191. Strength reduction is integrated with partial redundancy elimination using the al- gorithm \ndescribed in [1X]. Linear function test replacement and induction variable elimination are also performed \nduring partial redundancy elimination. FinalIy, partial dead code elimination is performed using the \nalgorithm described in [20]. Partial dead code elimination is repeated until no more changes are detected \nin the program.  B Code generator details Figure 13 shows how conversion instructions are dealt with \nin the MIPS code generator. struct MIPS-Convert-Inst : MIPS-Inst { public: enum Kind { cvtds,cvtdw, \n. . . n-cvt 1; // others ommitted const Kind kind; private: static const Info -Info[n-cvtl; public: \n MIPS-Convert-Inst(Kind,Inst*); const Info&#38; info0 {return -Info[kindl;l };  MIPS-Convert-Inst:: \nMIPS-Convert-Inst(Kind k,Inst *src) : MIPS-Inst(Inst::Convert,src), kind(k) I} const MIPS-1nst::Info \n MIPS-Convert-Inst:: -Info[MIPS-Convert-Inst::n-cvtl = {  (\"cvt.s.d\\t%x,%O\",\"%r\",MIPS_No_Imm,0,2,FP-REG), \n (\"cvt.w.d\\t%x,%O\",\"%r\",MIPS_No_Imm,0,2,FP-RBGI, . . . 1; Figure 13: Providing the machine-specific \ninformation.  C Register allocator details Figure 14 sketches the core engine of the register allocator. \n class Graph-Coloring ( public: Graph-Builder *const builder; Graph-Coloring(char*, All-Physical-Regs*, \nGraph-Builder*, Code-Generator*); \"Graph-ColoringO; void do-coloring(FGEdge-List&#38;, BlockList&#38;, \nBlock-List&#38;, Stack-Block&#38;, Reg-Set&#38;); private: void color-assignment(Physical-Reg-Bank*, \nVertex-Stack&#38;, Vertex-Stack&#38;); void spill-code-insertion(Block_List&#38;, Stack-Block&#38;); \nvoid coalescing(Block-List&#38;); virtual void color-ordering(Physical_Reg_Bank*, Vertex-Stack&#38;, \nVertex-Stack&#38;) = 0; virtual void shuffle-code-insertion(Block-List&#38; blocks, Stack-Block&#38; \nspill-block) {); 1; void Graph-Coloring::do-coloring(FGEdge-List&#38; unionorder, Block-List&#38; blocks, \n Stack-Block&#38; spill-block, Reg-Set&#38; reg-set) ( // construct the interference graph builder->construction(unionOrder, \nblocks, this); int any-spill = 1; while (any-spill) { any-spill = 0;  coalescing(blocks); // do live \nranges coalescing Vertex-Stack COLOR, SPILL; Physical-Reg-Bank *bank; for(int i = 0; i -z allghyregs-m-banks; \ni++) { bank = allqhyregs->bank(i); // check if there are available registers for coloring if(bank->avail-regs() \n== 0) continue; COLOR.clear(); SPILL.clear(); color-orderingtbank, COLOR, SPILL); color-assignmenttbank, \nCOLOR, SPILL); // physical register assignment if (SPILL.length()) any-spill = 1; if(any-spill) // reconstruct \nthe graph builder->reconstruction(blocks, allqhyregs); spill-code-insertion(blocks, spill-block); shuffle-code-insertion(blocks, \nspill-block); Figure 14: Details of Graph-Coloring.  \n\t\t\t", "proc_id": "236337", "abstract": "This paper describes how the cmcc compiler reuses code---both internally (reuse between different modules) and externally (reuse between versions for different target machines). The key to reuse are the application frameworks developed for global data-flow analysis, code generation, instruction scheduling, and register allocation.The code produced by cmcc is as good as the code produced by the native compilers for the MIPS and SPARC, although significantly less resources have been spent on cmcc (overall, about 6 man years by 2.5 persons). cmcc is implemented in C++, which allowed for a compact expression of the frameworks as class hierarchies. The results support the claim that suitable frameworks facilitate reuse and thereby significantly improve developer effectiveness.", "authors": [{"name": "Ali-Reza Adl-Tabatabai", "author_profile_id": "81100032153", "affiliation": "Intel Corporation, RN6-18. 2200 Mission College Blvd. Santa Clara, CA, and School of Computer Science, Carnegie Mellon University, Pittsburgh, PA", "person_id": "PP14023844", "email_address": "", "orcid_id": ""}, {"name": "Thomas Gross", "author_profile_id": "81332502168", "affiliation": "Institut f&#252;r Computer Systeme, ETH Z&#252;rich, CH 8092 Z&#252;rich, and School of Computer Science, Carnegie Mellon University Pittsburgh, PA", "person_id": "PP39072612", "email_address": "", "orcid_id": ""}, {"name": "Guei-Yuan Lueh", "author_profile_id": "81331498383", "affiliation": "ECE Department, Carnegie Mellon University, Pittsburgh, PA", "person_id": "PP33032555", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/236337.236342", "year": "1996", "article_id": "236342", "conference": "OOPSLA", "title": "Code reuse in an optimizing compiler", "url": "http://dl.acm.org/citation.cfm?id=236342"}