{"article_publication_date": "10-01-1996", "fulltext": "\n The Direct Cost of Virtual Function Calls in C++ Karel Driesen and Urs Hijlzle Department of Computer \nScience University of California Santa Barbara, CA 93 106 (karel,urs}@cs.ucsb.edu http://www.cs.ucsb.edu/(-karel,-ursJ \nAbstract. We study the direct cost of virtual function calls in C++ programs, assuming the standard implementation \nusing virtual function tables. We measure this overhead experimentally for a number of large benchmark \nprograms, using a combination of executable inspection and processor simulation. Our results show that \nthe C++ programs measured spend a median of 5.2% of their time and 3.7% of their instructions in dispatch \ncode. For all virtuals versions of the programs, the median overhead rises to 13.7% (13% of the instructions). \nThe thunk variant of the virtual function table implementation reduces the overhead by a median of 21% \nrelative to the standard implementation. On titure processors, these overheads are likely to increase \nmoderately. 1. Introduction Dynamic dispatch, i.e., the run-time selection of a target procedure given \na message name and the receiver type, is a central feature of object-oriented languages. Compared to \na subroutine call in a procedural language, a message dispatch incurs two kinds of overhead: a direct \ncost and an indirect cost. The direct cost of dynamic dispatch consists of the time spent computing the \ntarget function as a function of the run-time receiver class and the message name (selector). The ideal \ndispatch technique would find the target in zero cycles, as if the message send was a direct procedure \ncall. Thus, we define the direct cost of dynamic dispatch for a particular program P as the number of \ncycles spent on the execution of P, minus Permission to make digitalhard copy of part or all of this \nwork for personal or classroom use is granted without fee provided that copies are not made or distributed \nfor profit or commercial advantage, the copynght notice, the title of the publication and its date appear, \nand notice is given that copying is by permission of ACM, Inc. To copy otherwise, to republish to post \non servers, or to redistribute to lists, requires prior specific permlssion and/or a fee. OOPSLA 96 CA, \nUSA 0 1996 ACM O-69791 -788-W96/0010...$3.50 the number of cycles spent on the execution of an equivalent \nprogram Pi&#38;al in which all dispatches are replaced by direct procedure calls that magically invoke \nthe correct target function. In practice, Pideal may be impossible to construct, since some sends will \nhave varying targets at run time. However, a dispatch technique may reach ideal performance (zero overhead) \non some programs on a superscalar processor, as we will discuss in section 2.6. The indirect cost stems \nfrom optimizations that cannot be performed because the target of a call is unknown at compile time. \nMany standard optimizations such as interprocedural analysis require a static call graph to work well, \nand many intraprocedural optimizations are ineffective for the small function bodies present in object-oriented \nprograms. Thus the presence of dynamic dispatch hinders optimization, and consequently, the resulting \nprogram will run more slowly. Although indirect costs can be an important part of the total overhead \n[HU94] this study will mostly ignore them and instead focus on the direct costs. The aim of this study \nis to measure the direct cost of virtual function table lookup for a number of realistic C++ programs \nrunning on superscalar processors, and to identify the processor characteristics that most affect this \ncost. Unfortunately, it is hard to measure this cost directly since we cannot usually run program Pideal \n(the program without any dispatch code). Although it is fairly easy to count the number of instructions \nexecuted on behalf of dynamic dispatch, this measure does not accurately reflect the cost in processor \ncycles. On modern pipelined processors with multiple instruction issue the cost of an instruction may \nvary greatly. For example, on a 4-way superscalar processor with a branch penalty of 6, an instruction \ncan take anywhere between 0.25 and 7 cycles?. Therefore we measure the direct cost of virtual function \ntable lookup by simulating the execution of P Using an executable editor and a superscalar and Pideal. \nprocessor simulator, we compute the execution times of both programs, thus arriving at the direct cost \nof dispatch. In addition to allowing dispatch cost to be measured at all, simulation also facilitates \nexploring a broad range of possible processor implementations, thus making it possible to anticipate \nperformance trends on future processors. Our measurements show that on a processor resembling current \nsuperscalar designs, the C++ programs measured spend a median of 5.2% and a maximum of 29% of their time \nexecuting dispatch code. For version of the programs where every function was converted to a virtual \nfunction, the median overhead rose to 13.7% and the maximum to 47%. The rest of this paper is organized \nas follows: section 2 provides some background on virtual function table lookup and the aspects of superscalar \nprocessors that are relevant in this context. Section 3 discusses our experimental approach and the benchmark \nprograms, and section 4 presents the experiments, and section 5 discusses their results.  2. Background \nThis study concentrates on the dispatch performance of C++ programs on modern (superscalar) hardware. \nWhile we assume that the reader is familiar with the general characteristics of C++, we will briefly \nreview its most common dispatch implementations, virtual function tables and the thunk variant, and the \nsalient hardware characteristics of modern processors. Readers familiar with these topics may wish to \nskip to section 2.5. + In the absence of cache misses. 2.1 Virtual function tables C++ implements dynamic \ndispatch using virtual function tables (VFTs). VFTs were first used by Simula [DM73] and today are the \npreferred C++ dispatch mechanism [ES90]. The basic idea of VFTs is to determine the function address \nby indexing into a table of function pointers (the VFT). Each class has its own VFT, and each instance \ncontains a pointer to the appropriate VFT. Function names (selectors) are represented by numbers. In \nthe single-inheritance case, selectors are numbered consecutively, starting with the highest selector \nnumber used in the superclass. In other words, if a class C understands IM different messages, the class \nmessage selectors are numbered O..m-1. Each class receives its own dispatch table (of size m), and all \nsubclasses will use the same selector numbers for methods inherited from the superclass. The dispatch \nprocess consists of loading the receiver s dispatch table, loading the function address by indexing into \nthe table with the selector number, and jumping to that function. With multiple inheritance, keeping \nthe selector code correct is more difficult. For the inheritance structure on the left side of Figure \n1, functions c and e will both receive a selector number of 1 since they are the second function defined \nin their respective class. D multiply inherits from both B and C, creating a conflict for the binding \nof selector number 1. In C++ [ES90], the conflict is resolved by using multiple virtual tables per class. \nAn object of class D has two dispatch tables, D and DC (see Figure I).$ Message sends will use dispatch \ntable D if the receiver object is viewed as a B or a D and table DC if the receiver is viewed as a C. \nThe dispatch code will also adjust the receiver address before calling a method defined in C [ES90]. \nFigure 2 shows the five-instruction code sequence that a C++ compiler typically generates for a virtual \nfunction call. The first instruction loads the receiver object s VFT pointer into a register, and the \nsubsequent two instructions index into the VFT to load the target address and the receiver pointer adjustment \n(delta) for 1 We ignore virtual base classes in this discussion. Our benchmark suite contains only a \nfew instances of them, not enough to allow meaningful measurements. Virtual base classes introduce an \nextra overhead of a memory reference and a subtraction [ES90]. Figure 1. Virtual function tables (VFTs) \nCapital characters denote classes, lowercase characters message selectors, and numbers method addresses \n1: load [object-reg + #VFToffset], table-reg 2: load [table-reg + #deltaOffset], delta-reg 3: load [table-reg \n+ #selectorOffset], method-reg 4: add object-reg, delta-reg, object-reg 5: call method-reg Figure 2. \nInstruction sequence for VFT dispatch multiple inheritance. The fourth instruction adjusts the receiver \naddress to allow accurate instance variable access in multiple inherited classes. Finally, the fifth \ninstruction invokes the target function with an indirect function call. 2.1.1 Thunks Instructions 2 and \n4 in Figure 2 are only necessary when the class of the receiver has been constructed using multiple inheritance. \nOtherwise, the offset value loaded into the register delta-reg in instruction 2 is zero, and the add \nin instruction 4 has no effect. It would be convenient if we could avoid executing these useless operations, \nknowing that the receiver s class employs only single inheritance. Unfortunately, at compile time, the \nexact class of the receiver is unknown. However, the receiver s virtual function table, which stores \nthe offset values, knows the exact class. The trick is to perform the receiver address adjustment only \nafter the virtual function table entry is loaded. In the GNU GCC thunk implementation, the virtual function \ntable entry contains the address of a parameterless procedure (a thunk), that adjusts the receiver address \nand then calls the correct target function (see Figure 3). In the single inheritance case, the virtual \nfunction table entry points directly to the vf-table I thunk --+ add #offset to object-reg target I \nI I jump #method , *II I I I I vf-table target Figure 3. Thunk virtual f%nction tables, in the multiple \ninheritance case (above), and the single inheritance case (below) target function. Instead of always \nloading the offset value and adding it to the this pointer, the operation only happens when the offset \nis known to be non-zero. Since multiple inheritance occurs much less frequently than single inheritance, \nthis strategy will save two instructions for most virtual function calls?. Therefore, barring instruction \nscheduling effects, thunks should be at least as efficient as standard virtual function tables.  2.2 \nSuperscalar processors How expensive is the virtual function call instruction sequence? A few years ago, \nthe answer would have been simple: most instructions execute in one cycle (ignoring cache misses for \nthe moment), and so the standard sequence would take 5 cycles. However, on current hardware the situation \nis quite different because processors try to exploit instruction-level parallelism with superscalar execution. \nFigure 4 shows a simplified view of a superscalar CPU. Instructions are fetched from the cache and placed \nin an instruction buffer. During every cycle, the issue unit selects one or more instructions and dispatches \nthem to the appropriate functional unit (e.g., the integer unit). The processor may contain multiple \nfunctional units of the same type. For example, the processor in Figure 4 has three integer units and \nthus can execute up to three +In the GNU WC implementationfor SPARC excutables, one of the offset instructionsis \nusually replaced by a register move. The latter is necessary to pass the l&#38;s pointer in register \n%oO to the callee. instruction buffer 1 issue unit , S\\ /j * \\ : :, X\\ __ -\\ L  ,; 9 integer unit load/store \nunit : branch unit ~ FPU Figure 4. Simplified organization of a superscalar CPU integer instructions \nconcurrently. The number of instructions that can be dispatched in one cycle is called the issue width. \nIf the processor in Figure 4 had an issue width of four (often called four-way superscalar ), it could \nissue, for example, two integer instructions, one load, and a floating-point instruction in the same \ncycle. Of course, there is a catch: two instructions can only execute concurrently if they are independent. \nThere are two kinds of dependencies: data dependencies and control dependencies. Data dependencies arise \nwhen the operands of an instruction are the results of previous instructions; in this case, the instruction \ncannot begin to execute before all of its inputs become available. For example, instructions 2 and 3 \nof the VFT dispatch sequence can execute concurrently since they are independent, but neither of them \ncan execute concurrently with instruction 1 since they both use the VFT pointer loaded in instruction \n1. The second form of dependencies, control dependencies, result from the fact that some instructions \ninfluence the flow of control. For example, the instructions following a conditional branch are not known \nuntil the branch executes and determines the next instruction to execute (i.e., whether the branch is \ntaken or not). Therefore, even if an instruction after the branch has no data dependencies, it cannot \nbe executed concurrently with (or before) the branch itself. Both forms of dependencies may carry an \nexecution time penalty because of pipelining. Whereas the result of arithmetic instructions usually is \navailable in the next cycle (for a latency of one cycle), the result of a load issued in cycle i is not \navailable until cycle i+2 or i+3 (for a load latency L of 2 or 3 cycles) on most current processors even \nin the case of a first-level cache hit. Thus, instructions depending on the loaded value cannot begin \nexecution until L cycles after the load. Similarly, processors impose a branch penalty of B cycles after \nconditional or indirect branches: when a branch executes in cycle i (so that the branch target address \nbecomes known), it takes B cycles to refill the processor pipeline until the first instruction after \nthe branch reaches the execute stage of the pipeline and produces a result. To summarize, on ideal hardware \n(with infinite caches and an infinite issue width), the data and control dependencies between instructions \nimpose a lower limit on execution time. If N instructions were all independent, they could execute in \na single cycle, but if each of them depended on the previous one they would take at least N cycles to \nexecute. Thus, the number of instructions is an inaccurate predictor of execution time on superscalar \nprocessors. Even though actual processors do not have infinite resources, this effect still is significant \nas we shall see later in this paper. Figure 5 shows the dependencies between the instructions of the \nVFT dispatch sequence. At most two instructions can execute in parallel, and the minimum cost of the \nentire sequence is 2L for the chain of data dependencies between instructions 1, 3, and 5, and B for \nthe branch penalty for instruction 5, i.e., the 2L+B+l yg) 1: load [object-reg + #VFToffset], table-reg \n2: load [table-reg + #deltaOffset], delta-reg 3: load [table-reg + #selectorOffset], method-reg 4: add \nobject-reg, delta-reg, object-reg 5: call method-reg Figure 5. VFT execution schedules with cycle counts \nand assembly code. Dependencies are indicated with arrows. delay until the first instruction after it \ncan execute. Thus, the sequence s execution time on a processor with load latency L and branch penalty \nB is 2L + B + 1 cycles. In a previous study [DHV95] we approximated the dispatch cost of several techniques \nby analyzing the call sequence carefully and describing their cost as a function of load latency and \nbranch penalty, taking into account superscalar instruction issue. However, this approximation (e.g., \n2L + B + 1 for VFT dispatch) is only an upper bound on the true cost, and the actual cost might be lower. \nThe next few sections explain why. 2.3 Branch prediction Since branches are very frequent (typically, \nevery fifth or sixth instruction is a branch [HP95]) and branch penalties can be quite high (ranging \nup to 15 cycles on the Intel Pentium Pro processor [Mic9.5]), superscalar processors try to reduce the \naverage cost of a branch with branch prediction. Branch prediction hardware guesses the outcome of a \nbranch based on previous executions and immediately starts fetching instructions from the predicted path. \nIf the prediction is correct, the next instruction can execute immediately, reducing the branch latency \nto one cycle; if predicted incorrectly, the processor incurs the full branch penalty B. Predictions are \nbased on previous outcomes of branches. Typically, the branch s address is used as an index into a prediction \ntable. For conditional branches, the result is a single bit indicating whether the branch is predicted \ntaken or not taken, and typical prediction hit ratios exceed 90% [HP95]. For indirect branches, the prediction \nmechanism must provide a full target address, not just a taken/not taken bit. A branch target buJ?Zr \n(BTB) accomplishes this by storing the predicted address in a cache indexed by the branch address (very \nsimilar to a data cache). When the processor fetches an indirect branch, it accesses the BTB using the \nbranch instruction s address. If the branch is found, the BTB returns its last target address and the \nCPU starts fetching instructions from that address before the branch is even executed. If the prediction \nis wrong, or if the branch wasn t found, the processor stalls for B cycles and updates the BTB by storing \nthe branch and its new target address. BTBs affect the cost of the VFT dispatch sequence: if the virtual \ncall was executed previously, is still cached in the BTB, and invokes the same function as in the previous \nexecution, the branch penalty is avoided, reducing the sequence s cost to 2L + 1. 2.4 Advanced superscalar \nexecution Unfortunately, the truth is even more complicated. To improve performance, modern processors \nemploy two additional techniques that can decrease the performance impact of dependencies. First, instructions \nmay be executed out of order: an instruction I that is waiting for its inputs to become available does \nnot stall all instructions after it. Instead, those instructions may execute before I if their inputs \nare available. Additional hardware ensures that the program semantics are preserved; for example, if \ninstructions I, and I, write the same register, I, will not overwrite the result of I2 even if I, executes \nfirst. Out- of-order execution increases throughput by allowing other instructions to proceed while some \ninstructions are stalled. Second, speculative execution takes this idea one step further by allowing \nout-of-order execution across conditional or indirect branches. That is, the processor may speculatively \nexecute instructions before it is known whether they actually should be executed. If speculation fails \nbecause a branch is mispredicted, the effects of the speculatively executed instructions have to be undone, \nagain requiring extra hardware. Because branches are so frequent, speculating across them can significantly \nimprove performance if branches can be predicted accurately. Of course, the processor cannot look arbitrarily \nfar ahead in the instruction stream to find instructions that are ready to execute. For one, the probability \nof fetching from the correct execution path decreases exponentially with each predicted branch. Also, \nthe issue units must select the next group of instructions to be issued from the buffer within one cycle, \nthus limiting the size of that buffer. The most aggressive designs available today select their instructions \nfrom a buffer of about 30-40 instructions [Mic94][Mic95], so that instructions have to be reasonably \nnear the current execution point in order to be issued out-of-order. 2.5 Co-scheduling of application \ncode With speculative, out-of-order execution the cost of the VFT dispatch sequence is not only highly \nvariable (depending on the success of branch prediction), but it cannot be computed in isolation from \nits surrounding code. For example, if many other instructions precede the dispatch sequence, they could \nexecute during the cycles where the processor would otherwise lay idle waiting for the loads to complete. \nOr vice versa, the dispatch instructions could fit into empty issue slots of the rest of the basic block. \nThis co-scheduling of the application and dispatch code may reduce the overall cost significantly, possibly \nto the point where completely removing the dispatch code would not speed up the program at all (since \nall dispatch instructions fit into otherwise empty issue slots). Thus, at least in theory, a dispatch \nimplementation may reach zero overhead (i.e., adds no cycles to the execution time) even though it does \nintroduce extra instructions. 2.6 Summary While all of the processor features discussed above improve \nperformance on average, they also increase the variability of an instruction s cost since it depends \nnot only on the instruction itself (or the instruction and its inputs), but also on the surrounding code. \nMost processors sold today (e.g., the Intel Pentium and Pentium Pro processors, as well as virtually \nall RISC processors introduced since 1995) incorporate several or all of these features. As a result, \nit is hard to predict how expensive the average C++ virtual function call is on a current-generation \nPC or workstation. The experiments described in the rest of this paper aim to answer exactly this question. \n  3. Method This section describes how we simulated the execution of the C++ programs, what processor \nfeatures we assumed, and what benchmarks we used. 3.1 Simulation scheme Figure 6 shows an overview of \nour experimental approach: first, the C++ program compiled by an optimizing compiler (we used GNU gee \n2.6.3 and 2.7.2 with options -02 -msupersparc). Then, an application that uses the EEL executable editing \nlibrary [LS95] detects the dispatch instructions and produces a file with their addresses. Using this \nfile as well as a processor description, the superscalar processor simulator then runs the benchmark. \nbenchmark ~ c++ ~ / source i,. /,;A, ~ / _*. i ,, -; GCC ~\\ I Compiler , \\,_ ., t--, processor I I benchmark \n/ / description j executable ~ i :.;; 1 : EEL vf- \\\\ I processor / call marker \\ isimulator ,i \\ instructions \n~ simulated 1 in benchmark j ~ execution ; time data ~ Figure 6. Overview of experimental setup The \nsimulator can execute most SPARC programs using the shade tracing tool [CK93]. Shade always executes \nall instructions of the program so that programs produce the same results as if they were executed on \nthe native machine. Each instruction executed can be passed to a superscalar processor simulator that \nkeeps track of the time that would be consumed by this instruction on the simulated processor. Optionally, \nthe simulation of dispatch instructions can be suppressed (i.e., they are executed but not passed to \nthe timing simulator), thus simulating the execution of Pideal , the program using the perfect, zero-cost \ndynamic dispatch scheme. Although we currently use only benchmarks for which we have the source, this \nis not strictly necessary. Provided that the vf-call marker program detects all virtual calls correctly, \nany executable can be measured. The source language does not even have to be C++, as long as the language \nunder consideration uses VFT dispatch for its messages. Compared to a tool that detects dispatches at \nthe source code level, a tool based on binary inspection may be harder to construct, but it offers a \nsignificant advantage even beyond its source, compiler, and language independence. In particular, it \nis non-intrusive, i.e., does not alter the instruction sequence, and is thus more accurate. The vf-call \nmarker program detects the virtual function call code sequence discussed in section 2. This code sequence \nconsists of the five instructions in Figure 2 and any intervening register moves. They may appear in \ndifferent orderings (but with the correct dependencies), possibly spread out over different basic blocks. \nSince the code sequence is highly characteristic, the marker program is very accurate, detecting virtual \ncalls exactly for most programs.+ For three benchmarks the marker is slightly imprecise, erring by 0.4% \nor less. Only in ixx, 2.3% of the calls went undetected so that our measurements slightly underestimate \nthe direct dispatch cost for this benchmark. t We cross-checked this by using VPROF, a source-level virtual \nfunction profiler for CCC [Aig95]. 3.2 Benchmarks We tested a suite of two small and six large C++ applications \ntotalling over 90,000 lines of code (Table 1). In general, we tried to obtain large, realistic applications \nrather than small, artificial benchmarks. Two of the benchmarks (delta&#38;e and richards) are much smaller \nthan the others; they are included for comparison with earlier studies (e.g., [HU94, G+95]). Richards \nis the only synthetic benchmark in our suite (i.e., the program was never used to solve any real problem). \nWe did not yet test any programs for which only the executables were available. name ~ description lines \n:1 1 deltablue incremental dataflow constraint solver 1,000 I i eqn ~type-setting program for mathe- \n~ 8,300 I matical equations . I ~id1 ~SunSoft s IDL compiler (version 1.3) i 13,900 ~ 1 using the demonstration \nback end I which exercises the front end but ~produces no translated output. 1 ixx i IDL parser generating \nC++ stubs, 11,600~ I distributed as part of the Fresco library ; / 1 (which is part of Xl lR6). Although \nit I ~performs a function similar to IDL, the I ~program was developed Iindependently and is structured \nI differently. I lcom I optimizing compiler for a hardware 14,100 : I ~description language developed \nat the ~University of Guelph perky ~back-end optimizer that is part of the ~ 22,900 ~ ~Stanford SUIF \ncompiler system I i richards 1 simple operating system simulator ; 500 I i troff ~GNU groff version \n1.09, a batch-style ~ 19,200 ~ I.text formatting program 1 Table 1: Benchmark programs For every program \nexcept porkys we also tested an all-virtual version (indicated by -a$ suffix) which was compiled from \na source in which all member functions except operators and destructors were declared virtual. We chose \nto include these program versions in order to simulate programming styles that extensively use abstract \nbase classes defining virtual functions only (C++ s way of defining interfaces). For example, the Taligent \nCommonPoint frameworks $ Porky cannot be compiled as all virtual without a large effort of manual function \nrenaming. instruc- ~ virtual ~ tions per program version instructions calls virtual call original 40,427,339, \n615,100 i 65 deltablue i all-virtual 79,082,867 5,145,581 15 original 97,852,301: 100,207 976 eqn all-virtual \n108,213,587' 1,267,344! 85 original 91,707,462) 1,755,156; ;: idl all-virtual 99,531,814' 3,925,959 ~ \n I original 30,018,790, 101,025 ~ 297 ixx all-virtual 34,000,249; 606,463' 56 original 169,749,862 1,098,596i \n154 lcom all-virtual 175,260,461 2,311,705 75 original, 8,119,196 65,790 / 123 richards / all-virtual \n~ 15,506,753 1,146,217 13 original i 91,877,525 ; 806,312 113 troff all-virtual / 114,607,159 ) 3,323,572 \n34 perky original 748,914,861 3,806,797 196 Table 2: Basic characteristics of benchmark pro, grams (dynamic \ncounts) provide all functionality through virtual functions, and thus programs using CommonPoint (or \nsimilar frameworks) are likely to exhibit much higher virtual function call frequencies. Lacking real, \nlarge, freely available examples of this programming style, we created the all virtual programs to provide \nsome indication of the virtual function call overhead of such programs. These versions can also be used \nto approximate the behavior of programs written in languages where (almost) every function is virtual, \ne.g., Java or Modula-3. For each benchmark, Table 2 shows the number of executed instructions, the number \nof virtual function calls, and the average number of instructions between calls. All numbers are dynamic, \ni.e., reflect run-time execution counts unless otherwise mentioned. All programs were simulated in their \nentire length as shown in Table 2. Simulation consumed a total of about one CPU-year of SPARCstation-20 \ntime. 3.3 Processors Table 3 shows an overview of recently introduced processors. Since we could not \npossibly simulate all of Shipping date 95~ 95 95 95 1 95 kize ofBTB 0 0 0 64 : 512 j Size of BHP  2048 \ni 512 2048 512 I 0 ;Branch Penalty 1 4 4 5 l-3 11-15 Issue Width 1 4 : 5 4 4 3 Load Latency I 2 2 2 3 \n: Primary I-cacheb 1 16Kx2 ; 32Kx2 1 i 32Kx4 8K.x 8K Prima&#38; D-cache 16KxI 132Kx2 8KxI ~ 32Kx4 8K \n/ 1 &#38;t-of-order ~Speculative? ? u Y .Y Y Y Y Y Y y Y ; Table 3: Characteristics of recently introduced \nprocessors BTB = branch target buffer size; BHT = branch history table size (branch histories are used \nto predict the direction of conditional branches) b 16Kx2 means the cache is 16K bytes ? and 2.way associative \nthese processors and their subtle differences, we chose to model a hypothetical SPARC-based processor \nthat we dubbed P96 because it is meant to resemble the average processor introduced today. For our experiments, \nwe ran all benchmarks on P96 to obtain the base results for the dispatch overhead. To examine the effects \nof the most important processor features, we then varied each parameter while keeping all others constant. \nFinally, we also measured a few individual configurations that resemble existing processors (Table 3). \nP96-noBTB resembles the UltraSPARC in that it lacks a BTB, i.e., does not predict indirect branches. \nP96-Pro resembles the Pentium Pro in its branch configuration, having a very high branch penalty and \nrelatively modest branch prediction. Finally, P2000 is an idealized processor with essentially infinite \nhardware resources; we use it to illustrate the impact of the branch penalty on a processor that has \nvirtually no other limitations on instruction issue. It should be noted that none of these processors \nis intended to exactly model an existing processor; for example, the Intel Pentium Pro s instruction \nset and microarchitecture is very different from P96-Pro, and so the latter should not be used to predict \nthe Pentium Pro s performance on C++ programs. Instead, we use these processors to mark plausible points \nin the design space, and their distance and relationship to illustrate particular effects or trends. \n P96-P96-P20001 P20001~ 50% ~Processor P96 noBTB Pro bpl bpl0 ~ . . instructions 45% : Size of BTB 256 \n0 512 , 1024 1024 i Size of BHT 1024 1024 0 ~ 1024 1024 1 Branch Penalty 4 4 15 1 IO : Issue Width 4 \n4 4 32 : 32 1~ Load Latency 2 /Primary I-cache 32K, 2-way associative I Primary D-cache 32K, 2-way associative \n/Out-of-order ? Y Speculative? Y Table 4: Characteristics of simulated processors  4. Experimental \nResults This section first examine the cost of dynamic dispatch on the baseline architecture, P96, and \nthen examines the impact of individual architectural parameters (branch penalty/prediction, load latency, \nand issue width). 4.1 Direct cost on P96 4.1.1 Instructions and cycles First, we will examine the cost \nof dynamic dispatch on the baseline architecture, P96. Recall that we define the cost as the additional \ncycles spent relative to a perfect dispatch implementation that implements each dispatch with a direct \ncall. Figure 7 and Figure 8 show the results. On the standard benchmarks, the cost 14% ~ 29% . instructions \n! 12% ~ 1 cycles Figure 7. Direct cost of standard VFT dispatch (unmodified benchmarks) 40% I cycles \nc1 35% $ E 30% 8 o 25% Y 2 20% Figure 8. Direct cost of standard VFT dispatch (all-virtual benchmarks) \n varies from 1.4 % for eqn to 20% for deltablue, with a median overhead of 5.2 %. For the all-virtual \nversions, the overhead increases to between 4.7 % and 47% with a median overhead of 13%. The standard \nbenchmarks spend a median 3.7% of their instructions on dispatch, and the all-virtual versions a median \nof 13.7%. For the standard benchmarks the cycle cost is larger than the cost in the number of instructions \nexecuted; on average, it is a median 1.7 times larger. This difference confirms that the VFT dispatch \nsequence does not schedule well on a superscalar processor, compared to non-dispatch code. However, this \neffect varies substantially between benchmarks. The largest difference is found in eqn (2.8 times) and \ndeltablue (3.8 times). Since the dispatch sequence is always the same, this indicates that the instructions \nsurrounding a call can significantly affect the cost of virtual function lookup, or that virtual calls \nare more predictable in some programs than in others. We will explore these questions shortly. 4.1.2 \nThunks Figure 9 compares the cycle cost of standard and thunk implementations for the unmodified benchmarks?. \nThunks have a smaller cycle overhead than regular + Since GCC cannot compile idl, idl-av, and horn-av \nwith thunks, these benchmarks are missing from Figure 9 and Figure IO. . standard . standard 29%  50% \n~ 18% thunk I thunk 16% Figure 9. Cycle cost of VFT dispatch, standard and thunk variants (unmodified \nbenchmarks) tables for all benchmarks, using a median of 79% of the cycles of the regular implementation. \nFigure 10 shows the cycle cost for the all-virtual benchmarks. Here, thunks have 72% of the regular overhead. \nThe exact amount of the gain varies greatly between benchmarks. For example, the thunk overhead for ixx \nand deltablue is only 15% and 47% of the regular overhead, while for &#38;-off, thunks use almost as \nmany cycles as standard tables (98%). How can thunks, in some cases, improve dispatch performance by \nmore than a factor of two? One reason for the difference is the unnecessary receiver address adjustment \nthat is avoided with thunks (instructions 2 and 4 in Figure 5). In the thunk implementation, instructions \nthat depend on the receiver s address do not have to wait for the virtual function call to complete, \nif the target is predicted accurately. In contrast, in the standard implementation instructions 2 and \n4 create a dependency chain from instruction 1 to any instruction that needs the receiver s address. \nIn Figure 10. Cycle cost of VFT dispatch, standard and thunk variants-(all-virtual benchmarks) deltablue, \nthe added dependencies stretch the inner loop from 9 cycles (thunks) to 12 cycles (standard), where a \ndirect called implementation would use 8 cycles (all times exclude cache misses). Thus the overhead of \nthunks is only 25% of the overhead of standard tables for a large part of the execution, so that the \nremoval of only two instructions out of five can avoid more than half the virtual function call overhead \nin particular cases. This effect is particularly pronounced in all-virtual benchmarks that contain many \ncalls to accessor functions (i.e., functions that just return an instance variable). Another part of \nthe difference is due to memory hierarchy effects: with perfect caching +, thunk overhead for ixx and \ndeltablue rises to 48% and 54%. + By perfect caching we mean that there are no cache miss, not even for \ncold starts.  4.1.3 Generalization to other processors How specific are these measurements to our (hypothetical) \nP96 processor? Figure 11 compares the relative dispatch overhead of standard tables on P96 with that \nof the other processors listed in Table 3. Clearly, the processor configuration affects performance: \nlonger branch penalties combined with less ambitious branch prediction (P96-Pro) and the absence of a \nBTB (P96-noBTB) both impact dispatch performance negatively so that all programs spend a larger percentage \nof their time in dispatch code. Even P2000 with its 32-instruction issue CPU shows relative overheads \nthat are a median 28% higher than in P96. Thus, we expect future processors to exhibit higher dispatch \noverheads for most C++ programs. 80% ! I -6- P96 I P96-pro 2 P96-noBTB 3 P2OOOibp I 4~ P2000hp10 I / \n 0% Figure 11. Dispatch overhead in P96 vs. P96-noBTB and P96-Pro To explain these differences in more \ndetail, the next few sections present the effects of several processor characteristics on the direct \ncost of dynamic dispatch. In particular, we will investigate the impact of the branch penalty, the size \nof the branch target buffer (BTB), and the issue width. In each experiment, we vary the feature under \ninvestigation while keeping all other characteristics constant. To illustrate the trends, we show cost \nin two ways, each of them relative to P96. The first graph in each section compares absolute cost, i.e., \nthe number of dispatch cycles relative to P96. The second graph compares relative cost, i.e., the percentage \nof total execution time (again relative to P96) spent in dispatch. The two measurements are not absolutely \ncorrelated: if the absolute overhead increases, the relative cost may decrease if the rest of the application \nis slowed down even more than the dispatch code. Similarly, the absolute cost may decrease while the \nrelative cost increases because the absolute cost of the rest of the application decreases even more \nstrongly. 4.2 Influence of branch penalty Since one of the five instructions in the dispatch sequence \nis an indirect branch, the branch misprediction penalty directly affects the cost of virtual function \ndispatch. Since each dispatch contains a single indirect branch, we would expect the absolute overhead \nto increase proportionally to the number of mispredicted branches. And since the number of mispredictions \nis independent of the branch penalty, the cost should increase linearly with the branch penalty. A ~ \nz 5 ,~~ I deltablue j D d 1 , eqn E e ~ id1 I ii ixx ix Ix ~ lcom L ; 1 porky I P n/a richards R r 1 \ntroff T ;t Table 5: Benchmark abbreviations Figure 12 confirms this expectation (see Table 5 for the \none-letter abbreviations used in Figure 12 -17). For small branch penalties, the actual penalty can be \nsmaller than expected if the branch penalty is filled with instructions preceding the branch which have \nnot yet completed (e.g. because they are waiting for their R 1.4 0.5 K , 0 I 2 3 4 5 6 7 8 9 IO II 12 \nbranch penalty Figure 12. Overhead in cycles (relative to P96) for varying branch penalties inputs to \nbecome available). This effect appears to be small. The slope of the overhead lines increases with the \nBTB miss ratio, i.e., the fraction of mispredicted calls. Richards and @off have large BTB miss ratios \n(54% and 30%), which account for their steep cost curves. Most of the other benchmarks have a misprediction \nrate of 10% or less, which dampens the effect of branch penalty on cycle cost. Figure 13 shows that the \nfraction of execution time spent in dispatch can actually decrease with increasing branch penalty. For \nexample, tix has many indirect calls that are not part of virtual function calls, and these branches \nare very unpredictable (with a BTB miss ratio of 86%). Consequently, the relative overhead of virtual \ncalls in tix decreases with larger branch penalties since the cost of the rest of the program increases \nmuch faster. However, for most benchmarks the relative overhead differs less than 20% between the extreme \nbranch penalty values (0 and lo), indicating that the VFT branches are about as predictable as the other \nbranches 0.8 0.7 0.6 : r I 0 12 3 4 5 6 7 S 9 IO II 12 branch penalty Figure 13. Overhead in % of total \nexecution time (relative to P96) for varying branch penalties in the applications. Thus, the relative \ndispatch costs given earlier in Figure 7 and Figure 8 are quite insensitive to branch penalty variations. \n4.3 Influence of branch prediction As discussed in section 2.3, branch target buffers (BTBs) predict \nindirect (or conditional) branches by storing the target address of the branch s previous execution. \nHow effective is this branch prediction? Our baseline processor, P96, has separate prediction mechanisms \nfor conditional and indirect branches since the former can better be predicted with history-sensitive \n2-bit predictors [HP95]. Thus, varying the size of the BTB will affect only indirect branches, thus directly \nillustrating the BTB s effect on dispatch overhead. In general, smaller BTBs have lower prediction ratios \nbecause they cannot store as many individual branches. Recall that the processor uses the branch instruction \ns address to access the BTB (just like a load instruction uses the data address to access the data cache). \nIf the branch isn t cached in the BTB, it cannot be predicted. Figure 14. Overhead in cycles (relative \nto P96) for varying Branch Target Buffer sizes Naturally, the smaller the BTB, the fewer branches it \ncan hold, and thus the larger the fraction of branches that can t be predicted because they aren t currently \ncached in the BTB. Figure 14 confirms this expectation: in general, smaller BTBs increase dispatch overhead.+ \nApparently, a BTB size of 128 entries is large enough to effectively cache all important branches, as \nthe dispatch overhead does not decrease significantly beyond that BTB size. Figure 15 shows the dispatch \noverhead as a fraction of execution time. In general, the relative overhead varies in tandem with the \nabsolute overhead, i.e., smaller BTBs increase dispatch overhead. For processors with BTBs with 128 or \nmore entries, P96 should accurately predict the BTB s impact on dispatch performance. Finally, Figure \n16 shows the prediction ratio as a function of the BTB size. The ratio starts at zero (without a BTB, \nindirect branches cannot be predicted) and asymptotically reaches a final value around a BTB + For very \nsmall BTB sizes, the overhead can increase with a larger BTB. This is not a bug in our data. In very \nsmall BTBs, there are many conflict misses-branches are evicting each other from the BTB because the \nBTB cannot cache the working set of branches. With very small BTBs, this thrashing is so bad that removing \nthe virtual calls does not improve hit ratios in Pideal. However, at some point the BTB may be just large \nenough to hold most indirect branches in Pidealbut still not large enough to also hold the virtual function \ncalls. In this case, the difference in BTB effectiveness between Pideal and P suddenly becomes large, \nthus leading to a higher dispatch overhead. Figure 15. Overhead in % of total execution time (relative \nto P96) for varying Branch Target Buffer sizes size of 128. Generally, smaller benchmarks need fewer \nBTB entries to reach asymptotic behavior since they have fewer active call sites. r X f E; x~ R Figure \n16. Indirect branch prediction ratio as a function of BTB size The asymptotic prediction ratio corresponds \nto the hit ratio of an inline cache+ [DS84]. For some benchmarks, prediction works very well, with 90% \nor more of the calls predicted correctly. But several benchmarks (especially richards, ixx, eqn, and \ntrofl show much lower prediction ratios even with very large BTBs because their calls change targets \ntoo frequently. For example, the single virtual call in richards frequently switches between four different \nreceiver classes, each of which redefines the virtual function. No matter how large the BTB, such calls \ncannot be predicted well. The median prediction ratio for the standard benchmarks is only 65% vs. 91% \nfor the all-virtual versions; the latter are more predictable because many calls only have a single target \nand thus are predicted 100% correctly after the first call. 4.4 Influence of load latency Load latency \ninfluences dispatch cost since the VFT dispatch sequence contains two dependent load instructions. Thus, \nhigher load latencies should lead to higher dispatch overhead. Our measurements confirm this assumption: \ncompared to the baseline load latency of two, increasing the load latency to three increases absolute \ndispatch cost by a median of 5 1%; the relative cost increases by 3 1%. Similarly, with a load latency \nof one the absolute overhead decreases by 44% and the relative overhead by 37%. (Processors are unlikely \nto have load latencies larger than three, so we did not simulate these.) Clearly, load latency affects \nthe efficiency of dispatch code more than that of normal code sequences. Furthermore, it appears that \nthere are not enough surrounding application instructions to effectively hide the latency of the loads \nin the dispatch sequence, even for small load latencies. 4.5 Influence of issue width The final factor, \nissue width (i.e., the number of instructions that can be issued to the functional units in one cycle) \ncan also influence dispatch performance. Figure 17 shows that issue width has a strong impact t Since \nan inline cache stores a target separately for each call site, its hit rate mirrors that of a branch \ntarget buffer of infinite size with no history prediction bits. for small values. On a scalar processor \n(issuing at most one instruction per cycle), programs spend a much smaller fraction of their time in \ndispatch. Of course, absolute performance would be worse than on P96 since execution would consume many \nmore cycles (for example, lcom is three times slower on the one-issue processor than on the four-issue \nprocessor). With larger issue widths the relative overhead increases more slowly, reaching an asymptotic \nvalue of 26% (median) more than on P96. Thus, on wider-issue processors, the relative cost of dynamic \ndispatch will increase slightly because the application code benefits more from the additional issue \nopportunities than the dispatch c0de.t 1.x 1.6 1.4 I 0.6 0.4 i 0.2 0 I I 2 4 6 8 12 I6 Instruction Issue \nWidth Figure 17. Overhead in % of total execution time (relative to P96) for varying instruction issue \nwidths 4.6 Cost per dispatch In [DHV95] we predicted the cost of a single VFT dispatch to be 2L + B + \n1, i.e., two load delays plus a branch penalty; for P96, this adds up to 9 cycles. How accurate is this \nprediction? Figure 18 shows the cost in cycles per dispatch for all benchmarks. Clearly, the cost estimate \nof 9 cycles is too high, but that is not I For a few benchmarks (e.g., richards) the relative overhead \ndecreases with high issue widths. We assume that these benchmarks benefit from higher issue rates because \nthey allow critical dispatch instructions to start earlier, thus hiding part of their latency. surprising \nbecause the above model ignores the effects of branch prediction and co-scheduling of non-dispatch instructions. \nIn essence, a BTB reduces the effective branch penalty since the full penalty B is only incurred upon \na misprediction. The cost model could be improved by using the effective branch penalty Beff = B * btb-misprediction-ratio. \nFor the standard benchmarks, with a median misprediction ratio of 35%, this model predicts a cost of \n6.4 cycles, which still overestimates the real cost (median 3.9 cycles). Considering all benchmarks, \nthe median misprediction ratio of 11% results in an estimated cost of 5.4 cycles per dispatch, which \noverestimates the actual median of 2.8 cycles / dispatch by a factor of two. 11 Figure 18. Cycles per \ndispatch Dispatch cost varies widely: a single dispatch costs 2.1 cycles in lcom but 10.2 cycles in \ndeltablue, a difference of a factor of 4.8. This variation illustrates the combined effects of the factors \ndiscussed previously, such as the BTB hit ratio and the co-scheduling of application code. The dispatch \ncost of the all-virtual programs varies much less since the average cost is dominated by very predictable \nmonomorphic calls (i.e., call sites invoking the same function every time). 5. Discussion and Future \nWork What does this detailed dispatch performance analysis tell us? Will dispatch performance improve \nwith future hardware? Should programmers write their applications differently to improve performance? \nFirst, the median dispatch overheads we observed (5.2% for the standard benchmarks and 13.7% for the \nall-virtual versions) can be used as a bound on the dispatch performance improvements one can hope to \nobtain, for C++ programs, from better software or hardware. Thus, no matter how good a dispatch mechanism \nis, we cannot hope for much more than a performance improvement of around 5-10%. Any further improvement \nmust come from other optimizations such as customization or inlining [CUL89, HU94]. Given that better \noptimizing compilers are possible [AH96], it hardly seems appropriate for programmers to compromise the \nstructure of their programs to avoid dispatch. Many object-oriented systems use or could use VFT-like \ndispatch mechanisms (e.g., implementations of Java, Modula-3, Oberon-2, and Simula), and thus this study \nbears some significance for those languages as well. While the characteristics of typical programs may \ndiffer from the C++ programs measured here, the general trends should be similar. Together, the standard \nand all-virtual programs represent a wide spectrum of program behaviors and call frequencies, and thus \nwe expect many programs written in other languages to fall somewhere within that spectrum. Furthermore, \nthe dependency structure (and thus performance on superscalar processors) of many other dispatch mechanisms \n(e.g., selector coloring or row displacement) is similar to VFT, as we have shown in [DHV95]. Therefore, \nthe measurements presented here should apply to these dispatch mechanisms as well. Although simulations \nprovide accurate numbers, they are inordinately expensive and complicated. As discussed in section 4.6, \nthe analytical model for VFT dispatch cost developed in [DHV95] already predicts dispatch cost fairly \nwell using only two parameters. In future work, we intend to use the detailed results presented here \nas a starting point to construct a better model that would allow implementors or programmers to estimate \nthe dispatch cost in their application using a simple formula containing few processor-or application-specific \nparameters. Our results show that there may be room for better dispatch algorithms: a 5% or 10% improvement \nin performance for most programs is still significant. We hope that our measurements encourage others \nto search for better dispatch mechanisms. Previous work suggests that inline caching [DS84] should perform \nvery well on superscalar processors [DHV95], at least for call sites with low degrees of polymorphism. \nIn essence, inline caching is the software equivalent of an infinite branch target buffer (BTB) since \nit caches the last dispatch target by modifying the call. In addition, it contains only a single data \ndependency and thus schedules very well [DHV95]. A hybrid, adaptive dispatch implementation that employs \ninline caching where appropriate might considerably reduce dispatch cost in many programs and thus appears \nto be an attractive area for future work. Finally, will dispatch overhead increase in the future? We \nbelieve so, even though the effect is likely to be moderate. As Figure 17 showed, the relative overhead \nwill increase as processors issue more instructions per cycle. At an issue width of 16, the median overhead \nincreases by about 26%. Future processors might also have longer load latencies, further increasing dispatch \ncost. General compiler optimizations may also influence dispatch performance. Much current research focuses \non compilation techniques to increase instruction-level parallelism. If compilers successfully reduce \nexecution time on wide-issue processors, the effective dispatch overhead could further increase for programs \nwith unpredictable VFT calls. In summary, over the next few years, we expect the relative dispatch cost \nto rise, though the exact extent is hard to predict. 6. Related Work Rose [Ros88] analyzes dispatch performance \nfor a number of table-based techniques, assuming a RISC architecture and a scalar processor. The study \nconsiders some architecture-related performance aspects such as the limited range of immediates in instructions. \nMilton and Schmidt [MS941 compare the performance of VTBL-like techniques for Sather. Neither of these \nstudies take superscalar processors into account. The efficiency of message lookups has long been a concern \nto implementors of dynamically-typed, pure languages like Smalltalk where dispatches are more frequent \nsince these languages model even basic types like integers or arrays as objects. Dispatch consumed a \nsignificant fraction of execution time in early Smalltalk implementations (often 30% or more, even in \ninterpreted systems). Hash tables reduced this overhead to around 5% [CPL83]; however, 5% of a relatively \nslow interpreter still is a lot of time. The introduction of inline caching [DS84, UP871 dramatically \ndiminished this overhead by reducing the common case to a comparison and a direct call. A variant, polymorphic \ninline caches (PICs), extends the technique to cache multiple targets per call site [HCU91]. For SELF-93 \nwhich uses inline caching and PICs, Hljlzle and Ungar [HU95] report an average dispatch overhead of lo-15% \non a scalar SPARCstation-2 processor, almost half of which (6.4%) is for inlined tag tests implementing \ngeneric integer arithmetic. (This figure also includes other inlined type tests, not just dispatched \ncalls.) Given the large differences in languages, implementation techniques, and experimental setup, \nused, it is difficult to compare these results with those presented here. Calder et al. [CG94] discuss \nbranch misprediction penalties for indirect function calls in C++. Based on measurements of seven C++ \nprograms, they conclude that branch target buffers are effective for many C++ programs. For their suite \nof programs (which differs from ours), they measured an average BTB hit ratio of 91%, assuming an infinite \nBTB. In comparison, the hit ratios we observed were much lower, with a median hit ratio of only 65% for \nthe standard benchmarks. Grove et al. [G+95] also report more polymorphic C++ programs than Calder, which \nleads us to believe that Calder s suite of C++ programs may have been uncharacteristically predictable. \nSrinivasan and Sweeney [SS95] measure the number of dispatch instructions in C++ applications, but do \nnot calculate the relative dispatch overhead or consider superscalar issue. Much previous work has sought \nto improve performance by eliminating dispatches with various forms of inlining based on static analysis \nor profile information. Hijlzle and Ungar [HU94] estimate that the resulting speedup in SELFis five times \nhigher than the direct cost of the eliminated dispatches. Given the dispatch overheads reported here, \nthis ratio suggests significant optimization opportunities for C++ programs. Preliminary results from \nan optimizing C++ compiler confirm this assumption [AH96]. 7. Conclusions We have analyzed the direct \ndispatch overhead of the standard virtual function table (VFT) dispatch on a suite of C++ applications \nwith a combination of executable inspection and processor simulation. Simulation allows us to precisely \ndefine dispatch overhead as the overhead over an ideal dispatch implementation using direct calls only. \nOn average, dispatch overhead is significant: on a processor resembling current superscalar designs, \nprograms spend a median overhead of 5.2% and a maximum of 29% executing dispatch code. However, many \nof these benchmarks use virtual function calls quite sparingly and thus might underrepresent the actual \naverage C++ program. For versions of the programs where every function was converted to a virtual function \nto simulate programming styles that extensively use abstract base classes defining virtual functions \nonly (C++ s way of defining interfaces), the median overhead rose to 13.7% and the maximum to 47%. On \nfuture processors, this dispatch overhead is likely to increase moderately. On average, thunks remove \na fourth of the overhead associated with the standard implementation of virtual function calls. For some \nprograms the difference is much higher since thunks remove a data dependency chain that inhibits instruction \nlevel parallelism. To our knowledge, this study is the first one to quantify the direct overhead of dispatch \nin C++ programs, and the first to quantify superscalar effects experimentally. In addition to measuring \nbottom-line overhead numbers, we have also investigated the influence of specific processor features. \nAlthough these features typically influence the absolute dispatch cost considerably (i.e., the number \nof cycles spent in dispatch code), the relative cost (the percentage of total execution time spent in \ndispatch code) remains fairly constant for most parameters except for extreme values. Thus, the overheads \nmeasured here should predict the actual overhead on many current processors reasonably well. Since many \nobject-oriented languages use virtual function tables for dispatch, and since several other dispatch \ntechniques have identical execution characteristics on superscalar processors, we believe that our study \napplies to these languages as well, especially if their application characteristics fall within the range \nof programs studied here. Acknowledgments This work is supported in part by NSF grant CCR 96-24458, MICRO \ngrant 95-077, IBM Corporation, and Sun Microsystems. We would like to thank David Bacon, Harini Srinivasan, \nOle Agesen, and Gerald Aigner for their comments on earlier versions of this paper. Special thanks go \nto Kathryn O Brien for her support. Many thanks also go to the users of the CS department s 64-processor \nMeiko CS-2 (acquired under NSF grant CDA 92- 16202) who accommodated our (at times extensive) simulation \nworkload. 8. References [Aig95] Gerald Aigner. VPROF: A Virtual Function Call Profile1 for C++. Unpublished \nmanuscript, 1995. [AH961 Gerald Aigner and Urs HGlzle. Eliminating Virtual Func-tion Calls in C++ Programs. \nECOOP 96 Conference Proceedings, Linz, Austria, July 1996. [CG94] Brad Calder and Dirk Grunwald. Reducing \nIndirect Function Call Overhead in C++ Programs. In 21st Annual ACM Svmposiutn on Principles of Programming \nLanguages, p. 397-408, January 1994. [CUL89] Craig Chambers, David Ungar, and Elgin Lee. An Effi- cient \nImplementation of SELF, a Dynamically-Typed Object-Oriented Language Based on Prototypes. In OOPSLA 89 \nConference Proceedings, p. 49-70, New Orleans, LA, October 1989. Published as SIGPLAN Notices 24( IO), \nOctober 1989. [CK93] [CPL83] [DM73] [DSS4] [DHV95] [ES901 [G+95] [HP951 [HCU9 I 1 [HU94] [HU95] [Kra83] \n[Kro85] [LS95] [Mic94] [Mic95] Robert F. Cmelik and David Keppel. Shade: A Fast Instruction-Set Simulator \nfor Execution Profiling. Sun Microsystems Laboratories, Technical Report SMLI TR- 93-12, 1993. Also published \nas Technical Report CSE-TR 93-06-06, University of Washington, 1993. T. Conroy and E. Pelegri-Llopart. \nAn Assessment of Method-Lookup Caches for Smalltalk-Implementa-tions. In [Kra83]. O.-J. Dahl and B. Myrhaug. \nSimufa Implementation Guide. Publication S 47, NCC, March 1973. L. Peter Deutsch and Alan Schiffman. \nEfficient Imple-mentation of the Smalltalk-System. Proceedings of the 1 Ith Symposium on the Principles \nof Programming Languages, Salt Lake City, UT, 1984. Karel Driesen, Urs Holzle, and Jan Vitek. Message \nDispatch on Modern Compute! Architectures. ECOOP 95 Conference Proceedings, Arhus, Denmark, August 1995. \nMargaret A. Ellis and Bjarne Stroustrup. The Annotated C++ Reference Manual. Addison-Wesley, Reading, \nMA, 1990. David Grove, Jeffrey Dean, Charles D. Garrett, and Craig Chambers. Profile-Guided Receiver \nClass Predic-tion. In OOPSLA 9.5, Object-Oriented Programming Systems, Languages and Applications, p, \nlO8- 123, Austin, TX, October 1995. Hennessy and Patterson. Computer Architecture: A Quantitative Approach. \nMorgan Kaufmann, 1995. Urs Holzle, Craig Chambers, and David Ungar. Opti-mizing Dynamically-Typed Object-Oriented \nLanguages With Polymorphic lnline Caches. In ECOOP 91 Confer- ence Proceedings, Geneva, 199 I. Published \nas Springer Verlag Lecture Notes in Computer Science 512, Springer Verlag, Berlin, 1991. Urs Holzle and \nDavid Ungar. Optimizing Dynamically-dispatched Calls With Run-Time Type Feedback. In PLDI 94 Conjerence \nProceedings, pp. 326-335, Orlando, FL, June 1994. Published as SIGPLAN Notices 29(6), June 1994. Urs \nHolzle and David Ungar. Do Object-Oriented Languages Need Special Hardware Support? ECOOP 95 Conference \nProceedings, Arhus, Denmark, August 1995. Glenn Krasner. Smalltalk-80: Bits of History, Words oj Advice. \nAddison-Wesley, Reading, MA, 1983. Stein Krogdahl. Multiple Inheritance in Simula-like Languages. B/T \n25, pp. 3 18-326, 1985. James Larus and Eric Schnarr. EEL: Machine-lndepen-dent Executable Editing. In \nPLDI 95 Conference Proceedings, pp. 291-300, La Jolla, CA, June 1995. Published as SlGPLAN Notices 30(6), \nJune 1995. Microprocessor Report. HP PA8000 Combines Complexit?, and Speed. Volume 8, Number 15, November \n14, 1994. Microprocessor Report. Intel s P6 Uses Decoupled Superscalar Design. Volume 9, Number 2, February \n16, 1995. [MS941 S. Milton and Heinz W. Schmidt. Dynamic Dispatch in Object-Oriented Languages. Technical \nReport TR-CS-94-02, The Australian National University, Canberra, January 1994. [Ros88] John Rose. Fast \nDispatch Mechanisms for Stock Hard-ware. OOPSLA 88 Conference Proceedings, p. 27-35, San Diego, CA, November \n1988. Published as SIGPLAN Notices 23(1 I), November 1988. [SS95] Harini Srinivasan and Peter Sweeney. \nEvaluating Virtual Dispatch Mechanismsfir C++. IBM Technical Report RC 20330, Thomas J. Watson Research \nLaboratory, December 1995. [UPS71 David Ungar talk? In IEEE and David Patterson. What Price Computer \n20(l), January 1987. Small-  \n\t\t\t", "proc_id": "236337", "abstract": "We study the direct cost of virtual function calls in C++ programs, assuming the standard implementation using virtual function tables. We measure this overhead experimentally for a number of large benchmark programs, using a combination of executable inspection and processor simulation. Our results show that the C++ programs measured spend a median of 5.2% of their time and 3.7% of their instructions in dispatch code. For \"all virtuals\" versions of the programs, the median overhead rises to 13.7% (13% of the instructions). The \"thunk\" variant of the virtual function table implementation reduces the overhead by a median of 21% relative to the standard implementation. On future processors, these overheads are likely to increase moderately.", "authors": [{"name": "Karel Driesen", "author_profile_id": "81339497300", "affiliation": "Department of Computer Science, University of California, Santa Barbara, CA", "person_id": "PP42052304", "email_address": "", "orcid_id": ""}, {"name": "Urs H&#246;lzle", "author_profile_id": "81100400656", "affiliation": "Department of Computer Science, University of California, Santa Barbara, CA", "person_id": "P286890", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/236337.236369", "year": "1996", "article_id": "236369", "conference": "OOPSLA", "title": "The direct cost of virtual function calls in C++", "url": "http://dl.acm.org/citation.cfm?id=236369"}